Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 130?140, Dublin, Ireland, August 23-29 2014.A PAC-Bayesian Approach to Minimum Perplexity Language ModelingSujeeth BharadwajUniversity of Illinois405 N. Mathews Ave.Urbana, IL 61801, USAsbhara3@illinois.eduMark Hasegawa-JohnsonUniversity of Illinois405 N. Mathews Ave.Urbana, IL 61801, USAjhasegaw@illinois.eduAbstractDespite the overwhelming use of statistical language models in speech recognition, machinetranslation, and several other domains, few high probability guarantees exist on their generaliza-tion error.
In this paper, we bound the test set perplexity of two popular language models ?
then-gram model and class-based n-grams ?
using PAC-Bayesian theorems for unsupervised learn-ing.
We extend the bound to sequence clustering, wherein classes represent longer context suchas phrases.
The new bound is dominated by the maximum number of sequences represented byeach cluster, which is polynomial in the vocabulary size.
We show that we can still encouragesmall sample generalization by sparsifying the cluster assignment probabilities.
We incorporateour bound into an efficient HMM-based sequence clustering algorithm and validate the theorywith empirical results on the resource management corpus.1 IntroductionThe ability to predict unseen events from a few training examples is the holy grail of statistical languagemodeling (SLM).
Although the final test for any language model is its contribution to the performance ofa real system, task-independent metrics such as perplexity are popular for evaluating the general qualityof a model.
Standard algorithms therefore attempt to minimize perplexity on some previously unobservedtest set, assumed to be drawn from the same distribution as the training set.
This begets the question ofhow the test set perplexity is related to training set perplexity ?
every paper on SLM has an answer, withvarying levels of theoretical and empirical justification.The problem of data sparsity and generalization can be traced back to at least as early as Good (1953),and possibly Laplace, who recognizes that the maximum likelihood (ML) estimate of event frequencies(n-grams) cannot handle unseen events.
Smoothing techniques such as the add-one estimator (Lidstone,1920) and the Good-Turing estimator (Good, 1953) assign a non-zero probability to events that havenever been observed in the training set.
Recently, Ohannessian and Dahleh (2012) strengthened thetheory by showing that Good-Turing estimation is consistent when the data generating process is heavy-tailed.
In the context of this paper, smoothing was perhaps the first attempt to bound generalization error,in that it successfully guarantees a finite test set perplexity.It is evident that smoothing of the n-gram estimate alone is not sufficient.
Techniques that incorporatelower and higher order n-grams, such as Katz (1987) smoothing, Jelinek-Mercer (1980) interpolation,and Kneser-Ney (1995) smoothing, have become standard (Rosenfeld, 2000).
Chen and Goodman (1999)provide a thorough empirical comparison of smoothing methods and uncover useful relationships be-tween the test set cross-entropy (log perplexity) and the size of the training set, model order, etc.
ABayesian interpretation further explains why some of the techniques (don?t) work.
Teh (2006) discussesfundamental limitations of the Dirichlet process (Mackay and Peto, 1995) and proposes the hierarchi-cal Pitman-Yor language model as a better way of generating the heavy-tailed (power law) distributionsexhibited in natural language.This work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/130Instead of directly modeling a heavy-tailed distribution over words, class-based models address datasparsity by estimating n-grams over clusters of words.
Intuitively, clustering is a transformation of theevent space from the space of word n-grams, in which most events are rare, to the space of class n-grams,which is more densely measured and therefore requires fewer training examples.
Brown et al.
(1992)show that the clustering function that maximizes the training data likelihood must also maximize mu-tual information between adjacent clusters; although several useful clustering algorithms are based onthis principle, no provable guarantees currently exist.
Moreover, word transitions are never completelycaptured by the underlying class transitions, and some tradeoff between accurate estimation of frequentevents (word n-grams) and generalization to unseen events (class n-grams) is desired ?
class-based mod-els are therefore often interpolated with word n-grams using some of the previously described Bayesianmethods (Rosenfeld, 2000).Our survey of SLM techniques and their treatment of generalization error has been rather brief andcertainly not comprehensive.
We focus primarily on n-grams and related models since they have domi-nated SLM over the last several decades (Rosenfeld, 2000), and therefore serve as a good starting pointfor further analysis.
The existing literature suggests that apart from empirical validation and intuition,no provable guarantees exist on the generalization error of language models.
Bayesian techniques workwell only to the extent the prior assumptions are valid; in this paper, we present theoretical guaranteesthat hold irrespective of the correctness of the prior.Model selection approaches such as the Akaike Information Criterion (AIC) (Akaike, 1973) and itsvariants (Burnham and Anderson, 2002) quantify the tradeoff between complexity and goodness of fit.
Inthe context of a language model, it can be shown that test set cross entropy is approximately the trainingset cross entropy plus the number of model parameters.
Unfortunately, such bounds are loose and donot provide significant algorithmic insight ?
at best, they recommend the smallest model that works wellon the training set.
Chen (2009) obtained a very accurate relationship for exponential language modelsby estimating the test set performance with linear regression.
Although empirical, his approximationleads to better models based on l1+ l22regularization.
Exponential models are often motivated withthe minimum discrimination information (MDI) principle, which roughly states that of all distributionssatisfying a particular set of features, the exponential family is the centroid (minimizes distortion relativeto the farthest possible true distribution) (Rosenfeld, 1996).
This does not bound the generalization errorin the manner we wish to, but it is nevertheless a useful property that complements Chen?s observations.In this paper, we strive for the best of both worlds ?
we present PAC-Bayesian theory as a powerful toolfor deriving high probability guarantees as well as efficient and well-motivated algorithms.
In the nextsection, we state some useful PAC-Bayesian theorems.
In Section 3, we present our main results.
Weapply the PAC-Bayesian bounds to n-grams, class-based n-grams, and also sequence clustering, whereclasses represent longer context such as phrases.
We show that for sequence clustering, the bound isdominated by the maximum number of sequences represented by each cluster, and consequently requiresmany more training examples than a class-based model over words.
We address this issue by sparsifyingthe cluster assignment probabilities using the l?norm, 0 < ?
< 1, an effective proxy for the intractablel0norm.
In Section 4, we show how our bound can be incorporated into an HMM-based clusteringalgorithm.
In Section 5, we validate the theory presented in this paper with some empirical results on theresource management corpus.2 PAC-Bayesian BoundsPAC-Bayesian theory is a useful framework for combining frequentist bounds with the notion of a prior.Probably approximately correct (PAC) learning bounds the worst case generalization error of the best hy-pothesis selected from a hypothesis space ?
and therefore treats all hypotheses uniformly (Valiant, 1984).PAC-Bayesian bounds, however, place a prior over the hypothesis space while making no assumptions onthe data generating distribution (McAllester, 1998).
Thus, PAC-Bayesian bounds can both 1) incorporateprior information, and 2) provide frequentist guarantees on the expected performance.
They have beensuccessfully applied to classification settings such as the support vector machine (SVM) (McAllester,2003; Langford, 2005), yielding significantly tighter bounds.
Seldin and Tishby (2010) extend the frame-131work to include unsupervised learning tasks such as density estimation and clustering.
Since statisticallanguage modeling at its core is a discrete density estimation problem, we focus on the bounds developedby Seldin and Tishby (2010) and summarize key results in the following subsection.2.1 Unsupervised LearningGiven a d-dimensional product space X(1)?
... ?
X(d)and a collection of N samples, S, independentand identically distributed (i.i.d.)
according to some unknown distribution p(x1, ..., xd) over the productspace, we want to estimate p(x1, ..., xd) with some model q(x1, ..., xd).
In the case of clustering (e.g.class-based models), we make the following assumption on q(x1, ..., xd) [Note: we make no assumptionson the true distribution p(x1, ..., xd)]:q(x1, ..., xd) =?c1,...,cdq(c1, ..., cd)d?i=1q(xi|ci) (1)where ci= hi(xi) for some clustering function hi: X(i)7?
C(i).
We refer to them collectively as aclustering function h, h = {hi}di=1; hence h : X(1)?
...?X(d)7?
C(1)?
...?C(d).
We assume that theoriginal space X(1)?
... ?
X(d)has finite cardinality, with ni= |X(i)|, and likewise for the clusteredspace C(1)?
...?
C(d), where mi= |C(i)| is the number of clusters.
We define a hypothesis space,H, tobe the space of all possible clustering functions h H.For h  H, we define the distributions ph(c1, ..., cd) =?x1,...,xdp(x1, ..., xd)?di=1?
(hi(xi) = ci)and p?h(c1, ..., cd) =?x1,...,xdp?
(x1, ..., xd)?di=1?
(hi(xi) = ci), where p(x1, ..., xd) is the unknowntrue distribution, and p?
(x1, ..., xd) is the empirical (maximum likelihood) estimate.
The delta func-tion, ?
(arg), takes a value of 1 only when arg is true, and 0 otherwise.
We can extend tothe original space with the model assumption in Equation (1).
For example, ph(x1, ..., xd) =?c1,...,cdph(c1, ..., cd)?di=1q(xi|ci).The key difference between PAC learning and the PAC-Bayesian framework is the following notionof a random predictor, which is a distributionQ(h), learnt over the hypothesis spaceH.
Inference worksas follows: for a new sample (x1, ..., xd), we first draw a hypothesis h from H at random according tothe distribution Q(h).
We then return q(x1, ..., xd) according to the model described by Equation (1)and the clustering function h. The PAC-Bayesian framework therefore allows for a second level of aver-aging over Q, and we can define the induced distributions: pQ(c1, ..., cd) =?hQ(h)ph(c1, ..., cd) andp?Q(c1, ..., cd) =?hQ(h)p?h(c1, ..., cd).
Again, we can extend to the original space with pQ(x1, ..., xd)and p?Q(x1, ..., xd) using the model assumption in Equation (1).
Note that pQ(x1, ..., xd) is unknownsince p(x1, ..., xd) is unknown; but the goal is to bound some notion of generalization error, such as theKL-divergence KL(p?Q(x1, ..., xd)||pQ(x1, ..., xd)).The Change of Measure Inequality (CMI) (Seldin and Tishby, 2010) is central to almost every PAC-Bayesian bound, so we briefly state it here.
For any measurable function ?
(h) on H and for any distri-butions Q(h) and P(h):EQ(h)[?
(h)] ?
KL(Q||P) + lnEP(h)[e?
(h)](2)where KL(Q||P) = EQ(h)[lnQ(h)P(h)]is the KL-divergence betweenQ andP .
The proof is fairly straight-forward and is a direct consequence of rewriting ?
(h) as ln(e?
(h)Q(h)P(h)P(h)Q(h)).Seldin and Tishby (2010) apply the CMI with ?
(h) = N ?
KL(p?h(x1, ..., xd)||ph(x1, ..., xd)) andsimplify the KL-divergence term by recognizing that 1) {q(ci|xi)}di=1defines a distribution over allpossible clusterings, and hence Q = {q(ci|xi)}di=1; and 2) a specific P , which they call the prior, can bedefined without making any assumptions on the true distribution p(x1, ..., xd).
Note that P is not a priorin the Bayesian sense: 1) it indicates preference on the structure of the hypothesis, not an assumptionon the data generating distribution, although the latter could be a consequence of the former; 2) thebound holds regardless of P; and 3) the bound holds regardless ofQ, which is not necessarily the Bayesposterior.132The following prior on H makes no assumptions on p(x1, ..., xd).
We present a simplified version ofthe prior developed by Seldin and Tishby (2010):P(h) ?1exp[?di=1milnni+ nilnmi](3)The prior is based on a combinatorial argument.
In order to select a clustering function hifor somei, we first need to pick a cardinality profile (number of elements per cluster) for the miclusters; thereare nmiisuch profiles, hence the first term in the sum.
Next, given a cardinality profile, we need tobound the number of ways in which each of the nielements can be assigned to the clusters given theirsizes; there are at most mniipossibilities, hence the second term in the sum.
The CMI with ?
(h) =N ?
KL(p?h(x1, ..., xd)||ph(x1, ..., xd)), our modified prior, and a few information theoretic results leadto the following bound.PAC-Bayesian Clustering: For any distribution p over X(1)?
...?X(d)and an i.i.d.
sample S of sizeNaccording to p, with probability at least 1?
?, for all distributions of cluster functionsQ = {q(ci|xi)}di=1,the following holds:KL(p?Q(x1, ..., xd)||pQ(x1, ..., xd)) ?
?di=1nilnmi+K1N(4)where K1=?di=1milnni+ (M ?
1) ln(N + 1) + lnd+1?, and M =?di=1mi.
Although thisshows convergence, in applications such as language modeling, we are interested in directly bound-ing the test set perplexity or cross-entropy.
Seldin and Tishby (2010) smooth p?Q(x1, ..., xd) to boundEp(x1,...,xd)[?
ln p?Q(x1,...,xd)] and provide the following useful result based on Equation (4).Bound on Cross-Entropy: For any probability measure p overX(1)?...
?X(d)and an i.i.d.
sample S ofsize N according to p, with probability 1?
?
for all distributions of cluster functionsQ = {q(ci|xi)}di=1:Ep(x1,...,xd)[?
ln p?Q(x1, ..., xd)] ?
?I(p?Q(c1, ..., cd)) + ln(M)?
?di=1nilnmi+K12N+K2(5)where p?Q(x1, ..., xd) is now the smoothed empirical estimate induced by Q, I(p?Q(c1, ..., cd)) =?di=1H(p?Q(ci))?H(p?Q(c1, ..., cd)) is the multi-information of the clustering,M andK1are as definedin Equation (4), and K2is an additional term, K2?
I(p?Q(c1, ..., cd)), and the bound is non-negative.3 Language ModelsSince language modeling is yet another density estimation problem in which we want to minimize the testset perplexity, the bound in Equation (5) readily applies to both word n-grams and class-based n-grams.Note that the bounds are on cross-entropy, which is log perplexity, but we use the two terms almostinterchangeably.
We are now interested in estimating the unknown true distribution p(v1, ..., vn) overthe space Vn, where V is some vocabulary consisting of V = |V| words.
The degenerate case, d = 1,X(1)= Vn, is the case of word n-grams and results in a bound that is dominated by n1= |X(1)| = Vn.This suggests that the number of training samples, N , must be on the same order as Vnfor the bound(and hence the estimate) to be meaningful.It is also clear why class-based models are favored whenever they work.
In this case, d = n, X(i)= Vfor all 1 ?
i ?
d, and the bound in Equation (5) reduces to something linear in V (since ?i, ni=|X(i)| = V ).
Moreover, the clustering function is the same for all i ?
that is, word clusters do not dependon the position in the n-gram.
Assuming K word clusters, the number of training examples, N , onlyneeds to be on the order of Kn+ nV , achieving effective small sample generalization especially whenK << V .
In the following subsections, we extend the bound to sequences and present a unique approachto regularize the bound.1333.1 Sequence ClusteringWe have discussed two extreme cases, namely d = 1 and d = n, that correspond to word n-grams andclass-based n-grams, respectively.
In practice, they are often interpolated to retain the advantages ofboth, as shown in the following model:q(v1, ..., vn) = ?q(v1, ..., vn) + (1?
?
)?c1,...,cnq(c1, ..., cn)n?i=1q(vi|ci) (6)for some 0 < ?
< 1.
A Bayesian interpretation of the above model is to select between the n-gramand the class-based model with probabilities ?
and 1 ?
?, respectively.
In other words, for each n-gram (v1, ..., vn), we simply flip an ?-biased coin to decide on one of the two models.
In this paper,we interpolate across the entire spectrum, 1 ?
d ?
n, instead of just the extreme cases ?
that is, wecapture clusters over not just words, but also sequences of words (phrases).
Previous results by Deligneand Bimbot (1995), Ries et al.
(1996), and Justo and Torres (2007) indicate that clustering over phrasesis practically useful and leads to significant improvements.Suppose our goal is to estimate the probability of a trigram, for example, ?the cat sat.
?In the case of d = 1, we directly estimate the joint probability p(the, cat, sat).
In thestandard class-based model, where d = 3, we estimate with the model p(the, cat, sat) =?c1,c2,c3p(c1, c2, c3)p(the|c1)p(cat|c2)p(sat|c3).
The intermediate cases, such as d = 2 in this ex-ample, are often neglected.
The theory we subsequently develop interpolates over all four segmenta-tions, including the missing ones: p(the, cat, sat) =?c1,c2p(c1, c2)p(the cat|c1)p(sat|c2) as well asp(the, cat, sat) =?c1,c2p(c1, c2)p(the|c1)p(cat sat|c2).In general, an n-gram has 2n?1possible segmentations, as illustrated in the previous example.
Sup-pose f  F is a particular segmentation from the space of all possible segmentations, and we explicitlydefine it as the following mapping:f : Vn7?
X(1)?
...?X(d)(7)where 1 ?
d ?
n and f is simply a segmentation that does not modify the joint distribution; that is,p(v1, ..., vn) = p(x1, ..., xd).
If f is fixed a priori, we can immediately apply the bounds derived inEquation (5) over the segmented space X(1)?
...?
X(d).
This is the case where we decide on a model,such as the standard class-based model (d = n), and simply use it.An extension to the case of interpolated models is straightforward.
We modify the hypothesis spaceH to not only include all possible clusterings, but also all possible segmentations.
The new random pre-diction Q over H works as follows: given an n-gram (v1, ..., vn), draw a segmentation f  F accordingto the distribution pi = (pi1, ..., pi2n?1), where the segmentations are indexed by j = 1, ..., 2n?1(theordering does not matter), and pijis the probability of drawing segmentation j; pick a clustering as inthe random classifier described in Equation (5) for the new segmented space; and estimate q(v1, ..., vn)according to the model described by the previous steps.
The bound, in terms of pi, is given below.PAC-Bayes Sequence Clustering: For any probability measure p over Vn, and an i.i.d.
sample S ofsize N drawn according to p, with probability 1 ?
?
for all distributions of segmentations pi and for alldistributions of cluster functions Q:Ep(v1,...,vn)[?
ln p?Q(v1, ..., vn)] ?2n?1?j=1?
?K3(j) + ln(M(j))?
?d(j)i=1Vai(j)lnmi(j) +K1(j)2N?
?pij(8)K3(j) = ?I(p?Q(c1, ..., cd(j))) +K2(j)where ?j ?i, 1 ?
ai(j) ?
n, and ?j,?d(j)i=1ai(j) = n, and Vai(j)simply replaces niin Equation (5)for a given j.
The term K2(j) is from Equation (5).
Note that all terms such as mi(j), the number ofclusters corresponding to the space, their product M(j), and additional terms K1(j), K2(j) now dependon the segmentation j since X(i)and d(j) depend on j.134We can favor certain segmentations (e.g.
those that require few training examples), but note that thebound above is true regardless of the distribution over possible segmentations, pi.
Also, the bound isdominated by the exponent ai(j) and the constraint?d(j)i=1ai(j) = n. Hence, the bound is polyno-mial in V for all segmentations except the standard class-based setting where d(j) = n, in which case?i, ai(j) = 1.
For example, if d(j) = n ?
1 for some segmentation j, there exists some i such thatai(j) = 2 and hence represents clusters of bigrams.
If d(j) = n ?
2, there exists some segmentation j,and a space i such that ai(j) = 3, and so on until d(j) = 1, and this is the case of word n-grams wherea1(j) = n.3.2 Bound MinimizationImposing the restriction ?j ?i, ai(j) = 1 is simple, and although it can guarantee the small-samplebenefits of a standard class-based model, it is not a useful strategy for incorporating the constraint.
Sinceai(j) corresponds to the original space X(i)for a given j, restricting ai(j) would restrict X(i)to ana priori, fixed set of V elements.
To learn the best possible set of V elements, however, we need tominimize the effective size of X(i).
For example, suppose we are estimating trigrams over V3using thefollowing segmentation: X(1)= V and X(2)= V2?
i.e.
a bigram over clusters of words and clusters ofword bigrams.
The unconstrained bound is dominated by X(2).
We can restrict the effective size of X(2)by assigning zero probability to the vast majority of its elements, by constraining the hypothesis spaceto consider only cluster assignment functions q(xi|ci) in which n2<< V2of the elements have nonzeroprobability.
Thus, every word sequence in Vdcan be generated by the d = n segmentation, but everyother segmentation is constrained to generate at most a subset of Vdwith nonzero probability.We achieve this by imposing the restriction on the random predictor Q.
By Bayes rule, q(ci|xi) =q(xi|ci)q(ci)q(xi)and we can alternatively define Q as Q = {q(ci), q(xi), q(xi|ci)}di=1.
Our goal is to learna Q that minimizes the RHS of Equation (5), which includes maximizing the multi-information term,as well as constraining ni.
As expected, q(xi) controls the absolute size of X(i)and q(xi|ci) controlsthe effective size based on the clustering.
The dominant term in all of our bounds is ni(or ai, withni= Vai), which results from the second term in the prior defined in Equation (3), since it bounds thenumber of ways in which the niitems can be assigned to the miclusters.
Alternatively, we can representthis quantity with an upper bound,(?ci?q(xi|ci)?0)lnmi.
We can write q(xi) =?ciq(xi|ci)q(ci),and ni= ?q(xi)?0= ?
?ciq(xi|ci)q(ci)?0; by the triangle inequality and scale invariance of the l0norm, this is less than or equal to?ci?q(xi|ci)?0.
We therefore limit the upper bound,?ci?q(xi|ci)?0,by sparsifying q(xi|ci) for every cluster ci.The Optimization Problem: Given some segmentation, we want to find a random predictorQ ?
a class-based model over the fixed segmentation ?
such that the bound in Equation (5) is minimized, which isgiven by the following optimization problem:maximizeQI(p?Q(c1, ..., cd))subject to ?q(xi|ci)?0?
V, ?
ci C(i), i = 1, .
.
.
, d(9)Since such optimization problems are known to be NP-complete, we use a computationally tractableproxy.
The standard practice is to use the l1norm instead of the l0norm; although non-convex, we resortto the l?norm, 0 < ?
< 1, since q(xi|ci) is a probability vector with a fixed l1norm.
We therefore solvethe following problem:maximizeQI(p?Q(c1, ..., cd))subject to ?q(xi|ci)???
V, ?
ci C(i), i = 1, .
.
.
, d(10)We have shown that one way to regularize the bound for a non-trivial sequence clustering problem,regardless of whether the segmentation is fixed or if we are interpolating across all segmentations, isto sparsify the cluster assignment probabilities for every cluster.
There are many ways to sparsify aprobability vector (Pilanci et al., 2012; Kyrillidis et al., 2013), and we select the l?norm, 0 < ?
<1351, for its simplicity and success in other applications (Chartrand and Staneva, 2008).
Our approachguarantees manageable bounds on the test set cross-entropy for a general class of SLMs, without makingany assumptions on the true distribution p(v1, ..., vn).The Bayesian Connection A Bayesian interpretation of our regularization provides additional insightinto other successful models, such as the hierarchical Pitman-Yor language model (HPYLM).
In ourapproach, we impose the restriction ?q(xi|ci)???
V , 0 < ?
< 1, for every cluster ci.
It can beshown that this is equivalent to a sub-exponential prior on q(xi|ci) (Hastie et al., 2009).
Since q(xi) =?ciq(xi|ci)q(ci) and we make the assumption that q(xi|ci) is sub-exponential for every ci, we areconsequently assuming that q(xi) is also sub-exponential.
Although the PAC-Bayesian bounds holdregardless of the true distribution, our regularization technique implicitly assumes that it is heavy-tailed.The key to HPYLM?s success within the Bayesian setting is a better prior that matches the heavy-tailed distribution of natural language (Teh, 2006) ?
the regularization approach developed in this paperreassuringly corresponds to the assumption that the true distribution is heavy-tailed (sub-exponential).On the other hand, it may be possible to derive provable guarantees for HPYLM within the context ofour clustering model.
The main difference between HPYLM and the less successful Dirichlet process(DP) is the Chinese restaurant process, which assigns new tables (clusters) to customers (samples) muchmore aggressively in the former model than in the latter (Teh, 2006).
HPYLM therefore has far fewercustomers (samples) per table (cluster) than DP, resulting in significantly sparser q(xi|ci).4 An Efficient HMM AlgorithmThe hidden Markov model (HMM) is a popular tool for modeling sequences and has been used in severalspeech and language clustering tasks (Rabiner, 1989; Smyth, 1997; Li and Biswas, 1999).
Over its richhistory, several techniques, including regularization and sparsification of the HMM parameters, havebeen developed (Bicego et al., 2007; Bharadwaj et al., 2013).
The goal of this section is to show how ourbound easily fits into a well-established model such as the HMM.We can rewrite the standard class-based model by making a Markov assumption on q(c1, ..., cn):q(x1, ..., xd, c1, ..., cd) =d?i=1q(xi|ci)q(ci|ci?1) (11)where {xi}di=1is some segmentation of (v1, ..., vn)  Vn.
The HMM literature refers to cias the hiddenstate, q(xi|ci) as the observation probability, and q(ci|ci?1) as the state transition probability (Rabiner,1989).
If we consider each state of the HMM to be a cluster, then as before, q(ci|xi) = q(xi|ci)q(ci)q(xi)is a distribution over all possible clustering functions.
To solve the optimization problem described inEquation (10), we need to maximize the multi-information I(q(c1, ..., cn)) while satisfying the constraint?q(xi|ci)???
V .
We can rewrite the constrained optimization problem as an unconstrained problemusing a Lagrangian, and solve for q(xi|ci) with an l?regularized version of the expectation maximization(EM) algorithm, similar to Bharadwaj et al.
(2013).To maximize the multi-information term I(q(c1, ..., cd)) in Equation (10), we sparsify the state tran-sition probabilities q(ci|ci?1).
This provably works when we use l?regularization, 0 < ?
< 1 forsparsifying q(ci|ci?1).
The Renyi ?-entropy of a random variable with some probability distributionq is defined to be H?
(q) =?1?
?log ?q?
?and there are two useful results we use (Principe, 2010): 1)lim??1H?
(q) = H(q), whereH(q) is the Shannon entropy; and 2)H?
(q) is non-increasing in ?.
Thus,for ?
< 1,H?
(q) is an upper bound on the Shannon entropy.
Since l?regularization minimizes the Renyi?-entropy, which for 0 < ?
< 1 is an upper bound on the Shannon entropy, it effectively maximizes themutual information between ciand ci?1, given that I(q?Q(ci, ci?1)) = H(q?Q(ci))?H(q?Q(ci|ci?1)).Thus, we have shown that at least in the context of clustering, sparsifying both the observation prob-abilities and the state transition probabilities of an HMM using the l?prior directly minimizes general-ization error.136500 700 900 1100 1300 1500 1700 19007891011121314Training set size (# sentences)Test set cross?entropyHMMSparse HMMFigure 1: Test set cross-entropy of HMM vs l?-regularized (sparse) HMM as a function of the numberof training sentences5 ExperimentsWe test our approach on a subset of the resource management (RM) corpus (Price et al., 1993), whichconsists of naval commands that span approximately V = 1000 words.
First, we show that l?regular-ization works.
Figure 1 shows the estimated test set cross-entropy of an unregularized HMM and of anl?-regularized HMM as a function of the number of training sentences.
We vary the training set size from10 to 2000 sentences and test the models on 800 sentences; Figure 1 reports the average cross-entropyon brackets of training sizes ?
10-100, 110-200, and so on.
The l?-regularized HMM requires additionaltunable parameters such as the value of ?.
To simplify the search on a separate 300 sentence developmentset, we make a (rather restrictive) assumption that ?
for both the transition and observation probabilitiesis the same, and that ?
is independent of the size of the training set.
Our solutions are therefore not opti-mal, but adequate to demonstrate our claims.
To ensure that the cross-entropy is bounded, we smooth allestimates with add-one smoothing.
For small training datasets, the unregularized HMM learns modelsthat assign near-zero likelihood to some of the test sentences; hence, we only present results for trainingset sizes greater than 500 sentences.Like many other model selection results, Figure 1 suggests that model sparsity is essential when train-ing datasets are small.
In this example, about 900 sentences are required for the unregularized HMMto outperform the sparse HMM.
In the context of the theory developed in earlier sections, it was shownthat test set cross-entropy is proportional toniN, where N is the number of training examples.
In practicalsettings, N is fixed; hence, the only strategy for minimizing cross-entropy is to minimize ni.
Figure 1confirms that l?regularization successfully sparsifies q(xi|ci), the observation probabilities of the HMM,thereby minimizing ni.We also compare how the test set cross-entropy improves as a function of the training set size for fourdifferent models: 1) a baseline bigram model estimated over words; 2) a baseline class-based modelusing Brown?s algorithm (Brown et al., 1992) with K = 20 clusters, learnt over the entire dataset so thatit is also representative of knowledge-based approaches in which the true clusters are known a priori;3) l?-regularized HMM with 20 ergodic states; and 4) a special case of 3) in which the state transitionsare constrained to artificially form m1= 10 word clusters (10 states) and m2= 5 clusters that representword bigrams (10 states, where the 5 clusters are modeled with 2 left-to-right states each); therefore, themodel represents an interpolation between the standard class-based model and word bigrams, but is ofthe exact same complexity as 2) and 3).Figure 2 shows the estimated test set cross-entropy for each of the four models.
The values of ?used in our experiments are ?
= 0.7 for the words only case and ?
= 0.9 for sequences.
It is clear137200 400 600 800 1000 1200 1400 1600 1800 20006789101112Training set size (# sentences)Testsetcross?entropy1) word bigrams2) class?based (Brown)3) HMM (words only)4) HMM (sequences)Figure 2: Test set cross-entropy as a function of the number of training sentences for the four settingsfrom Figure 2 that l?regularization helps even in the case of a standard class-based model, the boundfor which is already linear in V .
With fewer than 100 sentences, l?regularization can both learn theclusters and estimate their transitions reasonably well, and surpasses Brown for training set sizes ofN ?
800 sentences.
Brown?s algorithm in 2) finds clusters such that pairwise mutual informationterms are maximized; in 3), we not only maximize the mutual information, but we also reduce theeffective V by ensuring that each cluster (or state) specializes and represents as few words as possible.As the number of training examples increases, estimates of class transitions indeed improve, but theclass-based assumption itself becomes too restrictive.
In 4), which represents an interpolated model,we see the tradeoff achieved by incorporating sequences: for small training sets, the model achievesbetter generalization than word bigrams, but is worse than the class-based model; and for larger trainingsets, the interpolated model learns better representations of high frequency events and outperforms theclass-based models represented by 2) and 3).The value of ?
in 3) is 0.7, whereas ?
in 4) is 0.9; this seems counter-intuitive at first, but note thata smaller ?
does not necessarily imply sparser observation probabilities; however, it implies a heavierdistribution in a Bayesian setting.
A Bayesian interpretation therefore suggests that in 4), the model itselfis better equipped to cope with heavy tails, whereas a more aggressive ?
is required in 3).6 ConclusionBy defining a random clustering model (a model in which there is a distribution over possible clusterassignments, e.g.
an HMM), it is possible to specialize published PAC-Bayesian cross-entropy boundsto the cases of n-gram and class-based n-gram estimation.
A distribution over segmentations allowsderivation of a cross-entropy bound on sequence clustering algorithms, which can be made useful bysparsifying the sequence cluster observation probabilities.
An efficient l?regularization technique canbe used to maximize sparsity, thereby minimizing the test set cross-entropy.AcknowledgementsWe are grateful to the SST Group at Illinois and the anonymous reviewers for valuable feedback.
Thanksalso to Jitendra Ajmera, Om Deshmukh, and Ashish Verma for their contributions to the clusteringalgorithm.
This work was supported by the NSF CDI Program Grant Number BCS 0941268 and AROW9111NF-09-1-0383; the opinions expressed in this work are those of the authors and do not necessarilyreflect the views of the funding agencies.138ReferencesHirotugu Akaike.
1973.
Information theory and an extension of the maximum likelihood principle.
In Proceedingsof the Second International Symposium on Information Theory, pages 267?281.Sujeeth Bharadwaj, Mark Hasegawa-Johnson, , Jitendra Ajmera, Om Deshmukh, and Ashish Verma.
2013.
Sparsehidden Markov models for purer clusters.
In Proceedings of the IEEE International Conference on Acoustics,Speech, and Signal Processing, pages 3098?3102.Manuele Bicego, Marco Cristani, and Vittorio Murino.
2007.
Sparseness achievement in hidden Markov models.In Proceedings of of the International Conference on Inage Analysis and Processing, pages 67?72.Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza, Jenifer C. Lai, and Robert L. Mercer.
1992.
Class-basedn-gram models of natural language.
Computational Linguistics, 18(4):467?479.Kenneth P. Burnham and David R. Anderson.
2002.
Model selection and multi-model inference: a practicalinformation-theoretic approach.
Springer.Rick Chartrand and Valentina Staneva.
2008.
Restricted isometry properties and nonconvex compressive sensing.Inverse Problems, 24:1?14.Stanley F. Chen and Josha Goodman.
1999.
An empirical study of smoothing techniques for language modeling.Computer Speech and Language, 13:359?394.Stanley F. Chen.
2009.
Performance prediction for exponential language models.
In Proceedings of NAACL HTL.Sabine Deligne and Frederic Bimbot.
1995.
Language modeling by variable length sequences: theoretical for-mulation and evaluation of multigrams.
In Proceedings of the IEEE International Conference on Acoustics,Speech, and Signal Processing, pages 169?172.I.J.
Good.
1953.
The population frequencies of species and the estimation of population parameters.
Biometrica,40(3):237?264.Trevor Hastie, Robert Tibshirani, and Jerome Friedman.
2009.
The Elements of Statistical Learning: Data Mining,Inference, and Prediction.
Springer.Frederick Jelinek and Robert L. Mercer.
1980.
Interpolated estimation of Markov source parameters from sparsedata.
In Proceedings of Workshop on Pattern Recognition in Practice, pages 381?397.Raquel Justo and M. Ines Torres.
2007.
Different approaches to class-based language models using word segments.Computer Recognition Systems 2, Advances in Soft Computing, 45:421?428.Slava M. Katz.
1987.
Estimation of probabilities from sparse data for the language model component of a speechrecognizer.
IEEE Transactions on Acoustics, Speech, and Signal Processing, 35(3):400?401.Reinhard Kneser and Hermann Ney.
1995.
Improved backing-off for m-gram language modeling.
In Proceedingsof the IEEE International Conference on Acoustics, Speech, and Signal Processing, pages 181?184.Anastasios Kyrillidis, Stephen Becker, Volkan Cevher, and Christoph Koch.
2013.
Sparse projections onto thesimplex.
JMLR: Workshop and Conference Proceedings, Proceedings of the 30th International Conference onMachine Learning, 28(2):235?243.John Langford.
2005.
Tutorial on practical prediction theory for classification.
The Journal of Machine LearningResearch, 6:273?306.Cen Li and Gautam Biswas.
1999.
Clustering sequence data using hidden Markov model representation.
InProceedings of the SPIE ?99 Conference on Data Mining and Knowledge Discovery, pages 14?21.G.J.
Lidstone.
1920.
Note on the general case of the Bayes-Laplace formula for inductive or a posteriori proba-bilities.
Transactions of the Faculty of Actuaries, 8:182?192.David J.C. Mackay and Linda C. Bauman Peto.
1995.
A hierarchical Dirichlet language model.
Natural LanguageEngineering, 1(03):289?308.David McAllester.
1998.
Some PAC-Bayesian theorems.
In COLT?
98 Proceedings of the eleventh annualconference on Computational Learning Theory, pages 230?234.David McAllester.
2003.
Simplified PAC-Bayesian margin bounds.
In COLT?
03 Proceedings of the sixteenthannual conference on Computational Learning Theory, pages 202?215.139Mesrob I. Ohannessian and Munther A. Dahleh.
2012.
Rare probability estimation under regularly varying heavytails.
JMLR: Workshop and Conference Proceedings, 25th Annual Conference on Learning Theory, 23(21):1?24.Mert Pilanci, Laurent El Ghaoui, and Venkat Chandrasekaran.
2012.
Recovery of sparse probability measuresvia convex programming.
In Advances in Neural Information Processing Systems (NIPS), volume 25, pages2429?2437.P.
Price, W.M.
Fisher, J. Bernstein, and D.S.
Pallett.
1993.
Resource Management RM 2.0.
In Linguistic DataConsortium, Philadelphia.Jose C. Principe.
2010.
Information Theoretic Learning.
Springer.Lawrence R. Rabiner.
1989.
A tutorial on hidden Markov models and selected applications in speech recognition.Proceedings of the IEEE, 77(2):257?286.Klaus Ries, Finn Dag Buo, and Alex Waibel.
1996.
Class phrase models for language modeling.
In ICSLP ?96Proceedings of the Fourth International Conference on Spoken Language, pages 398?401.Ronald Rosenfeld.
1996.
A maximum entropy approach to adaptive statistical language modeling.
Computer,Speech, and Language, 10:187?228.Ronald Rosenfeld.
2000.
Two decades of statistical language modeling: Where do we go from here?
Proceedingsof the IEEE, 88(8).Yevgeny Seldin and Naftali Tishby.
2010.
PAC-Bayesian analysis of co-clustering and beyond.
The Journal ofMachine Learning Research, 11:3595?3646.Padhraic Smyth.
1997.
Clustering sequences with hidden Markov models.
In Advances in Neural InformationProcessing (NIPS), volume 9, pages 648?654.Yee Whye Teh.
2006.
A hierarchical Bayesian language model based on Pitman-Yor processes.
In Proceedingsof the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Associationfor Computational Linguistics, pages 985?992.L.G.
Valiant.
1984.
A theory of the learnable.
Communications of the ACM, 27(11):1134?1142.140
