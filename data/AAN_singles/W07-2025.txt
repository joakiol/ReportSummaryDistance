Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 129?132,Prague, June 2007. c?2007 Association for Computational LinguisticsCU-TMP:Temporal Relation Classification Using Syntactic and Semantic FeaturesSteven Bethard and James H. MartinDepartment of Computer ScienceUniversity of Colorado at Boulder430 UCB, Boulder, CO 80309, USA{bethard,martin}@colorado.eduAbstractWe approached the temporal relation identi-fication tasks of TempEval 2007 as pair-wiseclassification tasks.
We introduced a va-riety of syntactically and semantically mo-tivated features, including temporal-logic-based features derived from running ourTask B system on the Task A and C data.We trained support vector machine modelsand achieved the second highest accuracieson the tasks: 61% on Task A, 75% on Task Band 54% on Task C.1 IntroductionIn recent years, the temporal structure of text has be-come a popular area of natural language processingresearch.
Consider a sentence like:(1) The top commander of a Cambodian resistanceforce said Thursday he has sent a team torecover the remains of a British mine removalexpert kidnapped and presumed killed byKhmer Rouge guerrillas almost two years ago.English speakers immediately recognize that kid-napping came first, then sending, and finally saying,even though before and after never appeared in thetext.
How can machines learn to do the same?The 2007 TempEval competition tries to addressthis question by establishing a common corpus onwhich research systems can compete to find tempo-ral relations (Verhagen et al, 2007).
TempEval con-siders the following types of event-time temporal re-lations:Task A Events1and times within the same sentenceTask B Events1 and document timesTask C Matrix verb events in adjacent sentencesIn each of these tasks, systems attempt to annotatepairs with one of the following relations: BEFORE,BEFORE-OR-OVERLAP, OVERLAP, OVERLAP-OF-AFTER, AFTER or VAGUE.
Competing systems areinstructed to find all temporal relations of thesetypes in a corpus of newswire documents.We approach these tasks as pair-wise classifi-cation problems, where each event/time pair isassigned one of the TempEval relation classes(BEFORE, AFTER, etc.).
Event/time pairs are en-coded using syntactically and semantically moti-vated features, and then used to train support vectormachine (SVM) classifiers.The remainder of this paper is structured as fol-lows.
Section 2 describes the features used to char-acterize event/time relations.
Section 3 explains howwe used these features to train SVM models for eachtask.
Section 4 discusses the performance of ourmodels on the TempEval data, and Section 5 sum-marizes the lessons learned and future directions.2 FeaturesWe used a variety of lexical, syntactic and semanticfeatures to characterize the different types of tempo-ral relations.
In each task, the events and times werecharacterized using the features:word The text of the event or time words1TempEval only considers events that occurred at least 20times in the TimeBank (Pustejovsky et al, 2003) corpus forthese tasks129ShhhhhhhhhhhhhQQQ(((((((((((((PPhhhhh(((((INForNPhhhh((((NPhhh((([TIMEthe quarter]VPXXXVBNendedNPXXX[TIMESept 30]NPDeltaVPhhhhh(((((VBD[EVENTposted]NPhhhhh(((((net income of $133 millionFigure 1: A syntactic tree.
The path between posted and the quarter is VBD-VP-S-PP-NP-NPpos The parts of speech2of the words, e.g.
this cru-cial moment has the parts of speech DT-JJ-NN.gov-prep Any prepositions governing the event ortime, e.g.
in during the Iran-Iraq war, thepreposition during governs the event war, andin after ten years, the preposition after governsthe time ten years.gov-verb The verb that governs the event or time,e.g.
in rejected in peace talks, the verb rejectedgoverns the event talks, and in withdrawing onFriday, the verb withdrawing governs the timeFriday.
For events that are verbs, this feature isjust the event itself.gov-verb-pos The part of speech2 of the governingverb, e.g.
withdrawing has the part of speechVBG.aux Any auxiliary verbs and adverbs modifying thegoverning verb, e.g.
in could not come, thewords could and not are considered auxiliariesfor the event come, and in will begin withdraw-ing on Friday, the words will and begin are con-sidered auxiliaries for the time Friday.Events were further characterized using the features(the last six use gold-standard TempEval markup):modal Whether or not the event has one of the aux-iliaries, can, will, shall, may, or any of theirvariants (could, would, etc.
).gold-stem The stem, e.g.
the stem of fallen is fall.gold-pos The part-of-speech, e.g.
NOUN or VERB.gold-class The semantic class, e.g.
REPORTING.gold-tense The tense, e.g.
PAST or PRESENT.gold-aspect The aspect, e.g.
PERFECTIVE.gold-polarity The polarity, e.g.
POS or NEG.Times were further characterized using the follow-ing gold-standard TempEval features:2From MXPOST (ftp.cis.upenn.edu/pub/adwait/jmx/)gold-type The type, e.g.
DATE or TIME.gold-value The value, e.g.
PAST REF or 1990-09.gold-func The temporal function, e.g.
TRUE.These gold-standard event and time features are sim-ilar to those used by Mani and colleagues (2006).The features above don?t capture much of the dif-ferences between the tasks, so we introduced sometask-specific features.
Task A included the features:inter-time The count of time expressions betweenthe event and time, e.g.
in Figure 1, there isone time expression, Sept 30, between the eventposted and the time the quarter.inter-path The syntactic path between the eventand the time, e.g.
in Figure 1 thepath between posted and the quarter isVBD>VP>S<PP<NP<NP.inter-path-parts The path, broken into three parts:the tags from the event to the lowest commonancestor (LCA), the LCA, and the tags from theLCA to the time, e.g.
in Figure 1 the parts areVBD>VP, S and PP<NP<NP.inter-clause The number of clause nodes along thesyntactic path, e.g.
in Figure 1 there is oneclause node along the path, the top S node.Our syntactic features were derived from a syntactictree, though Boguraev and Ando (2005) suggest thatsome could be derived from finite state grammars.For Task C we included the following feature:tense-rules The relation predicted by a set of tenserules, where past tense events come BEFOREpresent tense events, present tense events comeBEFORE future tense events, etc.
In the text:(2) Finally today, we [EVENT learned] thatthe space agency has taken a giant leapforward.
Collins will be [EVENT named]commander of Space Shuttle Columbia.130Since learned is in past tense and named is infuture, the relation is (learned BEFORE named).In preliminary experiments, the Task B system hadthe best performance, so we ran this system on thedata for Tasks A and C, and used the output to addthe following feature for both tasks:task-b-rel The relation predicted by combining theoutput of the Task B system with temporallogic.
For example, consider the text:(3) [TIME 08-15-90 (=1990-08-15)]Iraq?s Saddam Hussein[TIME today (=1990-08-15)] soughtpeace on another front by promising torelease soldiers captured during theIran-Iraq [EVENT war].If Task B said (war BEFORE 08?15?90)then since 08?15?90=1990?08?15=today,the relation (war BEFORE today) must hold.3 ModelsUsing the features described in the previous section,each temporal relation ?
an event paired with a timeor another event ?
was translated into a set of fea-ture values.
Pairing those feature values with theTempEval labels (BEFORE, AFTER, etc.)
we traineda statistical classifier for each task.
We chose sup-port vector machines3(SVMs) for our classifiers asthey have shown good performance on a variety ofnatural language processing tasks (Kudo and Mat-sumoto, 2001; Pradhan et al, 2005).Using cross-validations on the training data, weperformed a simple feature selection where any fea-ture whose removal improved the cross-validationF-score was discarded.
The resulting features foreach task are listed in Table 1.
After feature selec-tion, we set the SVM free parameters, e.g.
the ker-nel degree and cost of misclassification, by perform-ing additional cross-validations on the training data,and selecting the model parameters which yieldedthe highest F-score for each task4.3We used the TinySVM implementation fromhttp://chasen.org/%7Etaku/software/TinySVM/ and trainedone-vs-rest classifiers.4We only experimented with polynomial kernels.Feature Task A Task B Task Cevent-wordevent-pos X Xevent-gov-prep X Xevent-gov-verb X Xevent-gov-verb-pos X X 2event-aux X X Xmodal X Xgold-stem X X 1gold-pos X Xgold-class X X Xgold-tense X X Xgold-aspect X Xgold-polarity X Xtime-word Xtime-pos Xtime-gov-prep Xtime-gov-verb Xtime-gov-verb-pos Xtime-aux Xgold-typegold-value X Xgold-func Xinter-time Xinter-path Xinter-path-parts Xinter-clause Xtense-rules Xtask-b-rel X XTable 1: Features used in each task.
An X indicatesthat the feature was used for that task.
For Task C, 1indicates that the feature was used only for the firstevent and not the second, and 2 indicates the reverse.Strict RelaxedTask P R F P R FA 0.61 0.61 0.61 0.63 0.63 0.63B 0.75 0.75 0.75 0.76 0.76 0.76C 0.54 0.54 0.54 0.60 0.60 0.60Table 2: (P)recision, (R)ecall and (F)-measure ofthe models on each task.
Precision, recall and F-measure are all equivalent to classification accuracy.4 ResultsWe evaluated our classifers on the TempEval testdata.
Because the Task A and C models derived fea-tures from the Task B temporal relations, we first ranthe Task B classifer over all the data, and then ran theTask A and Task C classifiers over their individualdata.
The resulting temporal relation classificationswere evalutated using the standard TempEval scor-ing script.
Table 2 summarizes these results.Our models achieved an accuracy of 61% onTask A, 75% on Task B and 54% on Task C, thesecond highest scores on all these tasks.
The Temp-131Task Feature Removed Model AccuracyA- 0.663time-gov-prep 0.650gold-value 0.652polarity 0.655task-b-rel 0.656B- 0.809event-aux 0.780gold-stem 0.784gold-class 0.794C- 0.534event-gov-verb-2 0.522event-aux-2 0.525gold-class-1 0.526gold-class-2 0.527event-pos-2, task-b-rel 0.529Table 3: Feature analysis.
The ?-?
lines show theaccuracy of the model with all features.Eval scoring script also reported a relaxed measurewhere for example, systems could get partial creditfor matching a gold standard label like OVERLAP-OR-AFTER with OVERLAP or AFTER.
Under thismeasure, our models achieved an accuracy of 63%on Task A, 76% on Task B and 60% on Task C, againthe second highest scores in the competition.We performed a basic feature analysis where, foreach feature in a task, a model was trained with thatfeature removed and all other features retained.
Weevaluated the performance of the resulting modelsusing cross-validations on the training data5.
Fea-tures whose removal resulted in the largest drops inmodel performance are listed in Table 3.For Task A, the most important features were thepreposition governing the time and the time?s nor-malized value.
For Task B, the most important fea-tures were the auxiliaries governing the event, andthe event?s stem.
For Task C, the most importantfeatures were the verb and auxiliaries governing thesecond event.
For both Tasks A and C, the featuresbased on the Task B relations were one of the topsix features.
In general however, no single featuredominated any one task ?
the greatest drop in per-formance from removing a feature was only 2.9%.5 ConclusionsTempEval 2007 introduced a common dataset forwork on identifying temporal relations.
We framed5We used cross-validations on the training data to preservethe validity of the TempEval test data for future researchthe TempEval tasks as pair-wise classification prob-lems where pairs of events and times were assigneda temporal relation class.
We introduced a variety ofsyntactic and semantic features, including paths be-tween constituents in a syntactic tree, and temporalrelations deduced by running our Task B system onthe Task A and C data.
Our models achieved an ac-curacy of 61% on Task A, 75% on Task B and 54%on Task C. Analysis of these models indicated thatno single feature dominated any given task, and sug-gested that future work should focus on new featuresto better characterize temporal relations.6 AcknowledgmentsThis research was performed under an appointmentof the first author to the DHS Scholarship andFellowship Program, administered by the ORISEthrough an interagency agreement between DOEand DHS.
ORISE is managed by ORAU under DOEcontract number DE-AC05-06OR23100.
All opin-ions expressed in this paper are the author?s anddo not necessarily reflect the policies and views ofDHS, DOE, or ORAU/ORISE.ReferencesB.
Boguraev and R. K. Ando.
2005.
Timebank-driventimeml analysis.
In Graham Katz, James Pustejovsky,and Frank Schilder, editors, Annotating, Extractingand Reasoning about Time and Events, Dagstuhl Sem-inars.
German Research Foundation.T.
Kudo and Y. Matsumoto.
2001.
Chunking with sup-port vector machines.
In NAACL.I.
Mani, M. Verhagen, B. Wellner, C. M. Lee, andJ.
Pustejovsky.
2006.
Machine learning of temporalrelations.
In COLING/ACL.S.
Pradhan, K. Hacioglu, V. Krugler, W. Ward, J. H. Mar-tin, and D. Jurafsky.
2005.
Support vector learning forsemantic argument classification.
Machine Learning,60(1):11?39.J.
Pustejovsky, P. Hanks, R. Saur, A.
See, R. Gaizauskas,A.
Setzer, D. Radev, B. Sundheim, D. Day, L. Ferro,and M. Lazo.
2003.
The timebank corpus.
In CorpusLinguistics, pages 647?656.M.
Verhagen, R. Gaizauskas, F. Schilder, M. Hepple, andJ.
Pustejovsky.
2007.
Semeval-2007 task 15: Temp-eval temporal relation identification.
In SemEval-2007: 4th International Workshop on Semantic Evalu-ations.132
