Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 341?348,Queen Mary University of London, September 2009. c?2009 Association for Computational LinguisticsUnsupervised Classification of Dialogue Acts using a Dirichlet ProcessMixture ModelNigel Crook, Ramon Granell, and Stephen PulmanOxford University Computing LaboratoryWolfson BuildingParks Road, OXFORD, UKnigc@comlab.ox.ac.ukramg@comlab.ox.ac.uksgp@clg.ox.ac.ukAbstractIn recent years Dialogue Acts have be-come a popular means of modelling thecommunicative intentions of human andmachine utterances in many modern di-alogue systems.
Many of these systemsrely heavily on the availability of dialoguecorpora that have been annotated with Di-alogue Act labels.
The manual annota-tion of dialogue corpora is both tediousand expensive.
Consequently, there is agrowing interest in unsupervised systemsthat are capable of automating the annota-tion process.
This paper investigates theuse of a Dirichlet Process Mixture Modelas a means of clustering dialogue utter-ances in an unsupervised manner.
Theseclusters can then be analysed in terms ofthe possible Dialogue Acts that they mightrepresent.
The results presented here arefrom the application of the Dirichlet Pro-cess Mixture Model to the Dihana corpus.1 IntroductionDialogue Acts (DAs) are an important contribu-tion from discourse theory to the design of di-alogue systems.
These linguistics abstractionsare based on the illocutionary force of speechacts (Austin, 1962) and try to capture and modelthe communicative intention of human or ma-chine utterances.
In recent years, several dia-logue systems have made use of DAs for mod-elling discourse phenomena in either the DialogueManager (Keizer et al, 2008), Automatic SpeechRecogniser (Stolcke et al, 2000) or the Auto-matic Speech Synthesiser (Zovato and Romportl,2008).
Additionally, they have been used also inother tasks such as summarisation, (Murray et al,2006).
Therefore, a correct DA classification of di-alogue turns can bring benefits to the performanceof these modules and tasks.Many machine learning approaches have beenused to automatically label DAs.
They are usu-ally based on Supervised Learning techniquesinvolving combinations of Ngrams and HiddenMarkov Models (Stolcke et al, 2000; Mart?
?nez-Hinarejos et al, 2008), Neural Networks (Garfieldand Wermter, 2006) or Graphical Models (Ji andBilmes, 2005).
Relatively few approaches to DAclassification have been based on unsupervisedlearning methods.
Some promising results werereported by Anderach et al(Andernach et al,1997; Andernach, 1996) who applied KohonenSelf Organising Maps (SOMs) to the problem ofDA classification.
Although the SOM is nonpara-metric in the sense that it doesn?t require that thenumber of clusters to be found in the data be a pa-rameter of the SOM that is specified before clus-tering begins, it?s capacity to detect clusters is lim-ited to the size of the two-dimensional lattice ontowhich the clusters are projected, and the size ofthis lattice is determined prior to clustering.
Thispaper investigates the use of an unsupervised, non-parametric Bayesian approach to automatic DAlabelling: namely the Dirichlet Process MixtureModel (DPMM).
Specifically, the paper reports re-sults from applying the Chinese Restaurant Pro-cess (CRP), a popular approach to DPMMs, tothe automatic labelling of DAs in the Dihana cor-pus.
The Dihana corpus (J.M.Bened??
et al, 2006)has previously been used for the same task butwith a supervised learning approach (Mart?
?nez-Hinarejos et al, 2008).
The results reported hereindicate that, treating each utterance as a bag ofwords, the CRP is capable of automatically clus-341tering most utterances according to speaker, level1 and in some cases level 2 DA annotations (seebelow).2 The Dihana corpusThe Dihana corpus consists of human-computerspoken dialogues in Spanish about queuing infor-mation of train fares and timetables.
The acquisi-tion was performed using the Wizard of Oz (WoZ)technique, where a human simulates the systemfollowing a prefixed strategy.
User and systemutterances are different in nature, user utterancesare completely spontaneous speech whereas sys-tem utterances are based on pre-written patternsthat the WoZ selected according to what the usersaid in the previous turn, the current dialogue stateand the WoZ strategy.
There is a total of 900 dia-logues with a vocabulary of 823 words.
However,after applying a process of name entity recognition(cities, times, number, ...) and making the distinc-tion between system and user words there are 964different words.
The same process of name en-tity recognition was also used by Martinez Hinare-jos (Mart?
?nez-Hinarejos et al, 2008)2.1 Annotation schemeDialogues were manually annotated using a dia-logue act annotation scheme based on three lev-els (see Table 1).
The first level corresponds tothe general intention of the speaker (speech act),the second level represents the implicit informa-tion that is referred to in the first level and the thirdlevel is the specific data provided in the utterance.Using these three levels and making the distinc-tion between user and system labels, there are 248different labels (153 for the user and 95 for the sys-tem).
Combining only first and second level thereare 72 labels (45 for user and 27 for system), andwith only first level there are 16 labels (7 for userand 9 for system).Annotation was done at utterance level.
Thatis, each dialogue turn was divided (segmented)into utterances such that each one corresponds to aunique DA label.
An example of the segmentationand annotation of two turns of a dialogue can beseen in Figure 13 Dirichlet Process Mixture ModelsThis paper present a Dirichlet Process MixtureModel (DPMM) (Maceachern and Mu?ller, 1998;Escobar and West, 1995; Antoniak, 1974) for theLevel LabelsFirst Opening, Closing, Confirmation,Undefined, Not-understood, Waiting,Consult, Acceptance, RejectionSecond Departure-hour, Arrival-hour,Fare, Origin, Destination, Day,Train-Type, Service, Class, Trip-timeThird Departure-hour, Arrival-hour,Fare, Origin, Destination, Day,Train-Type, Service, Class,Trip-time, Order-number,Number-trains, Trip-typeTable 1: Set of dialogue act labels used in the Di-hana corpusautomatic, unsupervised clustering of the utter-ances in the Dihana corpus.
This approach treatseach utterance as a bag of words (i.e.
an unorderedcollection of words) (Sebastiani, 2002).
Utter-ances are clustered according to the relative countsof word occurrences that they contain so that utter-ances with similar histograms of word counts will,in general, appear in the same cluster.Bayesian methods for unsupervised data clus-tering divide into parametric and nonparametricapproaches.
Parametric approaches to clusteringsuch as Finite Bayesian Mixture Models (Mclach-lan and Peel, 2000) require prior estimation of thenumber of clusters that are expected to be foundin the data.
However, it is not always possible toknow this in advance and often it is necessary torepeat a modelling experiment many times over arange of choices of cluster numbers to find an op-timal number of clusters.
Sub-optimal choices forthe number of clusters can lead to a degradationin the generalisation performance of the model.Nonparametric approaches to mixture modelling,on the other hand, do not require prior estimatesof the number of clusters in the data; this is dis-covered automatically as the model clusters thedata.
Dirichlet Processes offer one approach to de-veloping Bayesian nonparametric mixture models.The remainder of this section briefly introducesDPMMs, beginning with a brief look at finiteBayesian mixture models which will serve as use-ful background for presenting the Chinese Restau-rant Process, the Dirichlet Process paradigm usedin this paper.342Speaker Utterance TranscriptionLevel 1 Level 2 Level 3S S1 Welcome to the railway information system.
How may I help you?Opening Nil NilU U1 Could you tell me the departure times from ValenciaQuestion Departure-hour OriginU2 to Madrid .Question Departure-hour DestinationFigure 1: An example of some turns from an annotated dialogue of DIHANA corpus.Figure 2: A 3-simplex with two examples pointsand the corresponding distributions3.1 Finite Bayesian Mixture ModelsA Dirichlet distribution is defined as a measureon measures.
Specifically, a Dirichlet distributiondefines a probability measure over the k-simplex.The k-simplex is a convex hull constructed so thateach point on the surface of the simplex describesa probability distribution over k outcomes:Qk = {(x1, .
.
.
, xk) : xi ?
0?i ?
{1 .
.
.
k},k?i=1xi = 1}Figure 2 shows a 3-simplex with two examplepoints and the corresponding distributions.
TheDirichlet distribution places a probability measureover the k-simplex so that certain subsets of pointson the simplex (i.e.
certain distributions) havehigher probabilities than others (Figure 3).
Theprobability measure in the Dirichlet is parame-terised by a set of positive, non-zero concentra-tion constants ?
= {?1, .
.
.
?k : ?i > 0}, writtenDirichletk(?1, .
.
.
?k).
The effects of differentvalues of ?
for the 3-simplex are shown in Figure3.The probability density function of the DirichletFigure 3: Three example Dirichlet Distributionsover the 3-simplex with darker regions showingareas of high probability: (a) Dirichlet(5,5,5), (b)Dirichlet(0.2, 5, 0.2), (c) Dirichlet(0.5,0.5,0.5).343distribution is given by:Dirichletk(?1, .
.
.
, ?k) = f(x1, .
.
.
, xk;?1, .
.
.
, ?k)= ?
(?ki=1 ?i)?ki=1 ?
(?i)k?i=1xai?1iwhere ?
(x) (=?
?0 t(x?1)e?tdt) extends the fac-torial function to the real numbers.
Since adraw from a Dirichlet distribution (written ?
?Dirichletk(?))
gives a distribution, a Dirichletcan be used as the prior for a Bayesian finite mix-ture model:?
?
Dirichletk(?1, .
.
.
, ?k)?
is a distribution over the k components ?
ofthe finite mixture model.
Each component ?zi isdrawn from a base measure G0 (?zi ?
G0).
Thechoice of distribution G0 depends on the natureof the data to be clustered; with data that is rep-resented using the bag of words model, G0 mustgenerate distributions over the word vocabulary.Hence the Dirichlet distribution is an appropriatechoice in this case:?zi ?
Dirichletv(?1, .
.
.
, ?v)where v is the size of the vocabulary.For each data point (utterance) xi a component?zi is selected by a draw zi from the multinomialdistribution ?
:zi ?
Multinomialk(?
)A suitable distribution F (?zi) is then used to drawthe data point (utterance).
In the bag of wordsmodel, the multinomial distribution is used todraw the words for each data point xi:xi ?
Multinomialv(?zi)A small example will illustrate this generativeprocess.
Imagine that there are just two typesof utterances with a vocabulary consisting sim-ply of the words A, B and C. A finite Bayesianmixture model in this case would first draw ?from a suitable Dirichlet distribution (e.g.
?
?Dirichlet2(0.5, 1)) as, for example, is shown inFigure 4(a).
Next the two components ?z1 and?z2 would be drawn from a suitable base distribu-tion G0 (e.g.
?z1 ?
Dirichlet3(1, 0.5, 0.5) and?z2 ?
Dirichlet3(0.5, 0.5, 1), see Figure 4(b)and 4(c)).
In this case, ?z1 will tend to generateFigure 4: An example finite Bayesian mixturemodel.
(a) The prior distribution over components?z1 (b) and ?z2 (c)utterances containing more occurrences of wordA than B or C, whilst ?z2 will tend to gener-ate utterances with more C?s than A?s or B?s.
Acomponent zi is then selected for each utterance(zi ?
Multinomialk(?)).
Note that in this ex-ample, the distribution ?
would lead to more utter-ances generated by ?z2 than by ?z1 .
Suppose thatfive utterances are to be generated by this modeland that the components for each utterance arez1 = 1, z2 = 2, z3 = 2, z4 = 1 and z5 = 2.The words in each utterance are then generatedby repeated draws from the corresponding com-ponent (e.g.
x1 = ACAAB, x2 = ACCBCC,x3 = CCC, x4 = CABAAC and x5 = ACC).3.2 Dirichlet ProcessesA Dirichlet Process can be thought of as an exten-sion of a Dirichlet distribution where the dimen-sions of the distribution are infinite.
The prob-lem with the infinite dimension Dirichlet distri-bution, though, is that its probability mass wouldbe distributed across the whole of the distribution.However, in most practical applications of mixturemodelling there will be a finite number of clusters.The solution is to have a process which will tendto place most of the probability mass at the be-ginning of the infinite distribution, thereby mak-ing it possible to assign probabilities to clusterswithout restricting the number of clusters avail-able.
The GEM stick breaking construction (thename comes from the first letters of Griffiths, En-gen and McCloskey (Pitman, 2002)) achieves pre-cisely this (Pitman and Yor, 1997).
Starting with344a stick of unit length, random portions ?
?k are re-peatedly broken off the stick, with each part thatis broken off representing the proportion of prob-ability assigned to a component:?
?k ?
Beta(1, ?)
?k =?k?1i+1 (1?
?
?i) ?
?
?kThe Dirichlet Process mixture model can nowbe specified as:?
?GEM(?)
?zi ?G0 zi ?
(1 .
.
.?
)zi ?Multinomial(?)
xi ?
F (?zi)3.3 Chinese Restaurant ProcessThe Chinese Restaurant Process (CRP) is a popu-lar Dirichlet Process paradigm that has been suc-cessfully applied to many clustering problems.
Inthe CRP, one is asked to imagine a Chinese restau-rant with an infinite number of tables.
The cus-tomers enter the restaurant and select, according toa given distribution, a table at which to sit.
All thecustomers on the same table share the same dish.In this paradigm, the tables represent data clusters,the customers represent data points (xi) and thedishes represent components (?z).
As each cus-tomer (data point) enters the restaurant the choiceof which table (cluster) and therefore which dish(component) is determined by a draw from the fol-lowing distribution:?i|?1, .
.
.
, ?i?1 ?1(?
+ i?
1)??i?1?j=1?
?j + ?G0?
?where ?
is the concentration parameter for theCRP.
The summation over the ?
?j ?s counts thenumber of customers sat at each of the occupiedtables.
The probability of sitting at an already oc-cupied table, therefore, is proportional to the num-ber of customers already sat at the table, whilst theprobability of starting a new table is proportionalto ?G0.
Figure 5 illustrates four iterations of thisinitial clustering process.Once all the customers (data points) have beenplaced at tables (clusters), the inference processbegins.
The posterior p(?,?, z|x) cannot be cal-culated exactly, but Gibbs sampling can be used.Gibbs sampling for the CRP involves iterativelyremoving a randomly selected customer from theirtable, calculating the posterior probability distri-bution across all the occupied tables together witha potential new table (with a randomly drawn dish,Figure 5: The first four steps of the initial cluster-ing process of the CRP.
The probability distribu-tion over the tables is also shown in each case.i.e.
component), and making a draw from that dis-tribution to determine the new table for that cus-tomer.
The posterior distribution across the tablesis calculated as follows:?i|?1, .
.
.
,?i?1,x?
1B??i?1?j=1?
?jp(xi|?j) + ?G0p(xi|?i)?
?where B = ?p(xk) +?i?1j=1 p(xi|?i) is the nor-malising constant.
After a predetermined numberof samples, the dish (component) of each occupiedtable is updated to further resemble the customers(data points) sitting around it.
In the bag of wordsapproach used here, this involves converting thehistogram of word counts in each customer (utter-ance) sitting at the table into an empirical distribu-tion H(xi), taking the average of these empiricaldistributions and modifying the dish (component)to further resemble this distribution:?i = ?i +?mimi?j=1H(xj)where ?
(0 ?
?
< 1) is the learning con-stant and mi is the number of customers around345table i.
The inference process continues to it-erate between Gibbs sampling and updating thetable dishes (components) until the process con-verges.
Convergence can be estimated by observ-ing n consecutive samples in which the customerwas returned to the same table they were takenfrom.4 ResultsThe CRP with Gibbs sampling was used to clus-ter both user and system utterances from the 900dialogues in the Dihana corpus.
Each utterance istreated as an independent bag of words where allinformation about the dialogue that it came fromand the context in which it was uttered is ignoredduring training.
Intra-cluster and inter-cluster sim-ilarity measures were used to evaluate the resultingclusters.
Intra-cluster similarity S?i is calculatedby averaging the Euclidean distance between ev-ery pair of data points in the cluster i:S?i =12mimi?i=1;j=1|xi ?
xj |Inter-cluster similarity S??
is calculated by sum-ming the Euclidean distance between the centroidsof all pairs of clusters:S??
=n?i=1;j=1|Ci ?
Cj |where Ci is the centroid of cluster i and n is thenumber of clusters.Two classification error measures were alsoused, one from the cluster (table) perspective E?,and the other from the perspective of the DialogueAct (DA) annotations (first level) of the Dihanacorpus E??.
The cluster classification error of ta-ble i is calculated by summing up the occurrencesof each DA on the table, finding the DA with thelargest total and allocating that DA as the correctclassification for that table Di.
The number offalse positives fpi for that table is the count of allcustomers (utterances) with DA annotations not inDi.
The number of false negatives fni is the countof utterances with label Di that occur on other ta-bles.
The cluster classification error for table i istherefore:E?i =1n(fpi + fni )The DA classification error E?
?i measures howwell DA i has been clustered, using the size of theClusterNo.
Ans Ask Clo Not Rej Und1 1 54 2 91 29 2 1 912 7 161 1 113 273 26 814 382 12 1 515 6 1 909 1 327 2217 47 39 1 118 73 1 319 1 420 131 115 1 3 122 270 29 3 323 135 8 2 225 83 31 1 428 247 16 1 429 349 6 1 1233 13 3 5 1 4 2541 202 45 1 2 346 4 149 6 251 1 2 451 124 896 1 1253 45 477 10Table 2: Clusters of user utterances, with thecounts for each level 1 speech act.
The largestcluster for each speech act is in bold.
The abbrevi-ations are: Und = Undefined, Ans = Answering,Ask = Asking, Clo = Closing, Rej = Rejection,Not = Not-understood.DA classN ci , the size of the largest cluster of utter-ances from that DA classM ci , and the total numberof utterances n in the corpus:E?
?i =1n(Nci ?M ci )Table 6 summarises the results from three sep-arate runs of the CRP, each increasing in numberof epochs.
It should be noted here that the Dihanacorpus has 72 DA categories, so the ideal numberof clusters discovered by the CRP would be 72.
Itshould also be noted that given an initial randomclustering, a good clustering algorithm will reduceintra-cluster similarity (S??
), increase inter-clustersimilarity (S??)
and reduce the classification errors(E??
and E???
).346Epochs (K) No.
Clusters S??
S??
E??
E??
?0 70 99703.6 243.74 0.05303 0.009791000 44 14975.4 217.56 0.01711 0.003851500 54 10093.7 336.15 0.01751 0.00435Figure 6: The results from three separate runs of the CRP on utterances from the Dihana corpus.
Clustersimilarity measures and classification error values are shown after 0 (i.e.
random clustering), 1000K, and1500K epochs.
S?
?, E??
and E???
are averaged values.Level 1 Level 2 ClusterNo.Answering Day 14Destination 22Fare 29Departure-hour 28, 41Asking Departure-hour,Fare 4Train-type 12Fare 49Departure-hour 51, 53Table 3: Clusters that have specialised on level 1and level 2 annotations.5 DiscussionThe first row of the table in Figure 6 shows thecluster similarity measures and classification er-rors after 0 epochs of the inference procedure (i.e.for a random clustering of utterances).
This gives abaseline for the measures and error values used insubsequent runs.
The second row of values showsthe results after a run of 1000K epochs of the in-ference procedure.
This run finds only 44 clustersbut has a much lower value for S??
than was foundin the random clustering, showing a significant in-crease in the similarity between utterances withineach cluster.
Surprisingly, the value for S??
is alsoreduced, showing that the differentiation betweenthe clusters formed at this stage is even lower thanthere was with the random clustering.
E??
and E??
?show suitable reductions indicating that the classi-fication errors are being reduced by the inferenceprocess.
The third row of values show that after1500K epochs 54 clusters have been found, intra-cluster similarity is increased beyond that for therandom clustering, but the classification errors re-main essentially the same as for the 1500K run.Although the 1500K epoch run found only 54clusters, it was able to clearly distinguish betweensystem and user utterances: with 30 clusters con-taining system utterances only, 22 clusters con-taining user utterances only and 2 clusters contain-ing instances of both.
Given that the system utter-ances in the Dihana corpus are generated from arestricted set of sentences, it is not surprising thatthese were easy to cluster and differentiate fromuser utterances.
However, the CRP was also ableto cluster user utterances well, which is more ofa challenge.
Table 2 shows the clusters that havespecialised on user utterances, with the counts ofthe level 1 annotations in each case.
The largestcluster for each level 1 annotation is shown in boldtypeface.
From here it can be seen that cluster 15has specialised on bothClosing and Rejection.
It isnot surprising that these fall within the same clus-ter since the words used in each are often the same(e.g.
?No thank you?
can act as either a closingstatement or a rejection statement).
Clusters 14,22, 29, 28 and 41 have specialised to the Answer-ing annotation, whilst clusters 4, 12 49, 51 and 53have specialised to Asking.
Table 3 shows howeach of these clusters have specialised to level 2annotations.
Cluster 14, for example, specialiseson the Answering:Day pair, whilst 22 specialiseson Answering:Destination pair.These initial results show that, at least for theDihana corpus, the DPMM can successfully clus-ter utterances into Speaker, Level 1, and Level2classes.
Whilst this looks promising, it must beacknowledged that the Dihana corpus is restrictedto train service inquiries and it remains unclearwhether this approach will generalise to other di-alogue corpora with a broader range of topics andwider vocabularies.
Future work will include in-vestigating the use of ngrams of words, syntacticfeatures, the DAs of previous utterances and ex-perimentation with other corpora such as Switch-board (Godfrey et al, 1992).AcknowledgmentsThis work was funded by the Companions project(www.companions-project.org) sponsored by the347European Commission as part of the InformationSociety Technologies (IST) programme under ECgrant number IST-FP6-034434.
We thank JeffBilmes (University of Washington) for many veryhelpful discussions about Dirichlet processes andtheir application.ReferencesToine Andernach, Mannes Poel, and Etto Salomons.1997.
Finding classes of dialogue utterances withkohonen networks.
In In Daelemans, pages 85?94.J.A.
Andernach.
1996.
A machine learning approachto the classification and prediction of dialogue utter-ances.
In Proceedings of the 2nd International Con-ference on New Methods in Language Processing,pages 98?109.Charles E. Antoniak.
1974.
Mixtures of dirichlet pro-cesses with applications to bayesian nonparametricproblems.
The Annals of Statistics, 2(6):1152?1174.J.L.
Austin.
1962.
How to do things with words.
Ox-ford: Clarendon Press.Michael D. Escobar and Mike West.
1995.
Bayesiandensity estimation and inference using mixtures.Journal of the American Statistical Association,90(430):577?588.Sheila Garfield and Stefan Wermter.
2006.
Call clas-sification using recurrent neural networks, supportvector machines and finite state automata.
Knowl.Inf.
Syst., 9(2):131?156.J.
J. Godfrey, E. C. Holliman, and J. Mcdaniel.
1992.SWITCHBOARD: telephone speech corpus for re-search and development.
In Proc.
ICASSP, vol-ume 1, pages 517?520 vol.1.Gang Ji and J. Bilmes.
2005.
Dialog act tagging usinggraphical models.
In Acoustics, Speech, and SignalProcessing, 2005.
Proceedings.
(ICASSP ?05).
IEEEInternational Conference on, volume 1, pages 33?36.J.M.Bened?
?, E.Lleida, A. Varona, M.J.Castro,I.Galiano, R.Justo, I.
Lo?pez, and A. Miguel.
2006.Design and acquisition of a telephone spontaneousspeech dialogue corpus in spanish: Dihana.
In FifthInternational Conference on Language Resourcesand Evaluation (LREC), pages 1636?1639, Genova,Italy, May.S.
Keizer, M. Gasic, F. Mairesse, B. Thomson, K. Yu,and S. Young.
2008.
Modelling user behaviourin the his-pomdp dialogue manager.
In IEEE SLT,pages 121?124, Dec.Steven N. Maceachern and Peter Mu?ller.
1998.
Esti-mating mixture of dirichlet process models.
Jour-nal of Computational and Graphical Statistics,7(2):223?238.C.
D.
Mart?
?nez-Hinarejos, J. M.
Bened?
?, andR.
Granell.
2008.
Statistical framework for a span-ish spoken dialogue corpus.
Speech Communica-tion, 50:992?1008.Geoffrey Mclachlan and David Peel.
2000.
Finite Mix-ture Models.
Wiley Series in Probability and Statis-tics.
Wiley-Interscience, October.Gabriel Murray, Steve Renals, Jean Carletta, and Jo-hanna Moore.
2006.
Incorporating speaker anddiscourse features into speech summarization.
InProceedings of the main conference on Human Lan-guage Technology Conference of the North Amer-ican Chapter of the Association of ComputationalLinguistics, pages 367?374, Morristown, NJ, USA.Association for Computational Linguistics.J.
Pitman and M. Yor.
1997.
The two-parameterPoisson-Dirichlet distribution derived from a stablesubordinator.
Annals of Probability, 25(2):855?900.J.
Pitman.
2002.
Combinatorial stochastic processes.Fabrizio Sebastiani.
2002.
Machine learning in au-tomated text categorization.
ACM Comput.
Surv.,34(1):1?47, March.Andreas Stolcke, Noah Coccaro, Rebecca Bates, PaulTaylor, Carol Van Ess-Dykema, Klaus Ries, Eliza-beth Shriberg, Daniel Jurafsky, Rachel Martin, andMarie Meteer.
2000.
Dialogue act modeling forautomatic tagging and recognition of conversationalspeech.
Comput.
Linguist., 26(3):339?373.E.
Zovato and J. Romportl.
2008.
Speech synthesisand emotions: a compromise between flexibility andbelievability.
In Proceedings of Fourth InternationalWorkshop on Human-Computer Conversation, Bel-lagio, Italy.348
