Efficiency, Robustness and Accuracyin Picky ChartDavid M. MagermanStanford  Un ivers i tyS tanford ,  CA 94305magerman@cs .s t  an ford .eduParsing*Carl WeirParamax SystemsPaol i ,  PA  19301we i r@prc .un isys .comABSTRACTTh is  paper  descr ibes  P icky ,  a probabi l i s t ic  agenda-basedchar t  pars ing  a lgor i thm which  uses  a techn ique  cal led p~'ob-abilistic prediction topredict which grammar rules are likelyto lead to an acceptable parse of the input.
Using a subopti-mal search method, "Picky significantly reduces the number ofedges produced by CKY-like chart parsing algorithms, whilemaintaining the robustness of pure bottom-up arsers andthe accuracy of existing probabilistic parsers.
Experimentsusing Picky demonstrate how probabilistic modelling can im-pact upon the efficiency, robustness and accuracy of a parser.1.
I n t roduct ionThis paper addresses the question: Why should we useprobabilistic models in natural anguage understanding?There are many answers to this question, only a few ofwhich are regularly addressed in the literature.The first and most common answer concerns ambigu~ity resolution.
A probabilistic model provides a clearlydefined preference nile for selecting among grammati-cal alternatives (i.e.
the highest probability interpreta-tion is selected).
However, this use of probabilistic mod-els assumes that we already have efficient methods forgenerating the alternatives in the first place.
While wehave O(n 3) algorithms for determining the grammatical-ity of a sentence, parsing, as a component of a naturallanguage understanding tool, involves more than simplydetermining all of the grammatical interpretations of aninput.
Ill order for a natural anguage system to processinput efficiently and robustly, it must process all intelligi-ble sentences, grammatical or not, while not significantlyreducing the system's efficiency.This observ~ttiou s ggests two other answers to the cen-tral question of this paper.
Probabilistic models offera convenient scoring method for partial interpretationsin a well-formed substring table.
High probability con-stituents in the parser's chart call be used to interpretungrammat.ical sentences.
Probabilistic models can also*Special I.hanks to Jerry Hobbs and F3ob Moo*re at S\[II forproviding access to their colllptllel's, and to Salim \]-/oukos, Pe-l:er Brown,  and Vincent and Steven Della Piel.ra ,-xt IF3M for theirinst.ructive lessons on probabi|isti,: modell ing of natural  I:mguage.be used for efficiency by providing a best-first searchheuristic to order the parsing agenda.This paper proposes an agenda-based probabilistic hartparsing algorithm which is both robust and efficient.
Thealgorithm, 7)icky 1, is considered robust because it willpotentially generate all constituents produced by a purebottom-up arser and rank these constituents by likeli-hood.
The efficiency of the algorithm is achieved througha technique called probabilistic prediction, which helpsthe algorithm avoid worst-case behavior.
Probabilisticprediction is a trainable technique for modelling whereedges are likely to occur in the chart-parsing process.
2Once the predicted edges are added to the chart usingprobabilistic prediction, they are processed in a stylesimilar to agenda-based chart parsing algorithms.
Bylimiting the edges in the chart to those which are pre-dicted by this model, the parser can process a sentencewhile generating only the most likely constituents giventhe input.In this paper, we will present the "Picky parsing al-gorithm, describing both the original features of theparser and those adapted from previous work.
Then,we will compare the implementation f ` picky with exist-ing probabilistic and non-probabilistic parsers.
Finally,we will report the results of experiments exploring how`picky's algorithm copes with the tradeoffs of efficiency,robustness, and accuracy.
32.
Probab i l i s t i c  Mode ls  in  "P ickyThe probabilistic models used ill the implementation of"Picky are independent of the algorithm.
To facilita.te thecomparison between the performance of "Picky and itspredecessor, "Pearl, the probabilistic model ilnplelnentedfor "Picky is similar to "Pearl's coring nlodel, the context-l 'pearl =-- probabil istic Earley-style parser  (~-Ear l ) .
"Picky =-probabil istic CI(Y-like parser ( 'P-CKY).2Some famil iarity with chart  parsing terminology is assumed inthis paper.
For terminological definitions, see \[9\], \[t0l, \[11\], or \[17\].3Sections 2 and 3, the descriptions of the probabil istie modelsused in ",Picky and the T'icky algorithn,,  are similar in contentto the corresponding sections of Magernmn and Weir\[13\].
Theexperimental  results and discussions which follow in sections .1-6~tre original.40free grammar with context-sensitive probability (CFGwith CSP) model.
This probabilistic model estimatesthe probability of each parse T given the words in thesentence S, P(TIS), by assuming that each non-terminaland its immediate children are dependent on the non-terminal's iblings and parent and on the part-of-speechtrigram centered at the beginning of that rule:P(TIS) ~- I I  P(A  --+ a\]C --~ 13A7, aoala2) (1)AETwhere C is the non-terminal node which immediatelydominates A, al is the part-of-speech associated with theleftmost word of constituent A, and a0 and a2 are theparts-of-speech of the words to the left and to the rightof al, respectively.
See Magerman and Marcus 1991 \[12\]for a more detailed description of the CFG with CSPmodel.3.
The  Pars ing  A lgor i thmA probabilistic language model, such as the aforemen-tioned CFG with CSP model, provides a metric for eval-uating the likelihood of a parse tree.
However, while itmay suggest a method for evaluating partial parse trees,a language model alone does not dictate the search strat-egy for determining the most likely analysis of an input.Since exhaustive search of the space of parse trees pro-duced by a natural language grammar is generally notfeasible, a parsing model can best take advantage of aprobabilistic language model by incorporating it into aparser which probabilistically models the parsing pro-cess.
"Picky attempts to model the chart parsing processfor context-free grammars using probabilistic prediction.Picky parses sentences in three phases: covered left-corner phase (I), covered bidirectional phase (II), andtree completion phase (III).
Each phase uses a differ-ent method for proposing edges to be introduced to theparse chart.
The first phase, covered left-corner, usesprobabilistic prediction based on the left-corner word ofthe left-most daughter of a. constituent to propose dges.The covered bidirectional phase also uses probabilisticprediction, but it allows prediction to occur from theleft-corner word of any daughter of a constituent, andparses that constituent outward (bidirectionally) fromthat daughter.
These phases are referred to as "cov-ered" because, during these phases, the parsing mech-anism proposes only edges that have non-zero proba-bility according to the prediction model, i.e.
tha.t havebeen covered by the training process.
The final phase,tree completion, is essentially an exhaustive search of allinterpretations of the input, according to the gra.mn\]a.r.However, the search proceeds in best-first order, accord-ing to the measures provided by the language model.This phase is used only when the probabilistic predictionmodel fails to propose the edges necessary to completea parse of the sentence.The following sections will present and motivate the pre-diction techniques used by the algorithm, and will thendescribe how they are implemented in each phase.3 .1 .
P robab i l i s t i c  P red ic t ionProbabilistie prediction is a general method for usingprobabilistic information extracted from a parsed corpusto estimate the likelihood that predicting an edge at acertain point in the chart will lead to a correct analysisof the sentence.
The Picky algorithm is not dependenton the specific probabilistic prediction model used.
Themodel used in the implementation, which is similar tothe probabilistic language model, will be described.
4The prediction model used in the implementation ofPicky estimates the probability that an edge proposedat a point in the chart will lead to a correct parse to be:P( A --+ otB\[3\]aoal ~.
), (2)where ax is the part-of-speech of the left-corner word ofB, a0 is the part-of-speech of the word to the left of al,and a~ is the part-of-speech of the word to the right ofa l .To illustrate how this model is used, consider the sen-tenceThe cow raced past the barn.
(3)The word "cow" in the word sequence "the cow raced"predicts NP  --+ det n, but not NP  --4 det  n PP ,since PP is unlikely to generate a verb, based on train-ing material, s Assuming the prediction model is welltrained, it will propose the interpretation of "raced"as the beginning of a participial phrase modifying "thecow," as inThe cow raced past the barn mooed.
(4)However, the interpretation of "raced" as a past par-ticiple will receive a low probability estimate relative tothe verb interpretation, since the prediction aodel onlyconsiders local context.4It is not necessary for ~he prediction model to be the same asthe language model used to evaluate complete analyses.
However,it is helpful if this is the ca.se, so that the probability estimates ofincomplete dges will be consistent with the probability estimatesof completed constituents.SThroughout this discussion, we will describe the predictionprocess using wo,-ds as the predictors of edges.
In the implementa-tion, due to sparse data concerns, only parts-of-speech are used topredict edges.
Give,, more robust estimation techniques, a prob-abilistic prediction model conditioned on word sequences i likelyto perform as well or better.41The process of probabilistic prediction is analogous tothat of a human parser recognizing predictive lexicalitems or sequences in a sentence and using these hints torestrict he search for the correct analysis of the sentence.For instance, a sentence beginning with a wh-word andauxiliary inversion is very likely to be a question, and try-ing to interpret it as an assertion is wasteful.
If a verb isgenerally ditransitive, one should look for two objects tothat verb instead of one or none.
Using probabilistic pre-diction, sentences whose interpretations are highly pre-dictable based on the trained parsing model can be ana-lyzed with little wasted effort, generating sometimes nomore than ten spurious constituents for sentences whichcontain between 30 and 40 constituents!
Also, in someof these cases every predicted rule results in a completedconstituent, indicating that the model made no incorrectpredictions and was led astray only by genuine ambigu-ities in parts of the sentence.3 .2 .
Exhaust ive  P red ic t ionWhen probabilistic prediction fails to generate the edgesnecessary to complete a parse of the sentence, exhaus-tive prediction uses the edges which have been generatedin earlier phases to predict new edges which might com-bine with them to produce a complete parse.
Exhaus-tive prediction is a combination of two existing types ofprediction, "over-the-top" prediction \[11\] and top-downfiltering.Over-the-top rediction is applied to complete dges.
Acompleted edge A -+ a will predict all edges of the formB -+ flAT.
6Top-down filtering is used to predict edges in order tocomplete incomplete dges.
An edge of the form A --4aBoBxB2f l ,  where a B1 has been recognized, will predictedges of the form B0 + 3' before B1 and edges of theform B2 --4 ~ after B1.3.3.
Bid i rec t iona l  Pars ingThe only difference between phases I and II is that phaseII allows bidirectional parsing.
Bidirectional parsing isa technique for initiating the parsing of a constituentfrom any point in that constituent.
Chart parsing algo-rithms generally process constituents from left-to-right.For instance, given a grammar uleA -+ B1B2.
.
-B , ,  (5)6In the implementation of "Picky, over-the-top rediction fi)rA --+ o' will only predict edges of the form B -+ A~'.
This liJnitaticmon over-the-top recliction is due to the expensive bookl~eepinginvolved in bidirectional parsing.
See the section on bidirectionalparsing for more details.a parser generally would attempt o recognize a B1, thensearch for a B2 following it, and so on.
Bidirectionalparsing recognizes an A by looking for any Bi.
Once aBi has been parsed, a bidirectional parser looks for a/3/-1 to the left of the Bi, a Bi+I to the right, and soon.Bidirectional parsing is generally an inefficient tech-nique, since it allows duplicate edges to be introducedinto the chart.
As an example, consider a context-freerule NP -+ DET N, and assume that there is a deter-miner followed by a noun in the sentence being parsed.Using bidirectional parsing, this NP rule can be pre-dicted both by the determiner and by the noun.
Theedge predicted by the determiner will look to the rightfor a noun, find one, and introduce a new edge consistingof a completed NP.
The edge predicted by the noun willlook to the left for a determiner, find one, and also intro-duce a new edge consisting of a completed NP.
Both ofthese NPs represent identical parse trees, and are thusredundant.
If the algorithm permits both edges to beinserted into the chart, then an edge XP --+ ~ NP/3 willbe advanced by both NPs, creating two copies of everyXP edge.
These duplicate XP edges can themselves beused in other rules, and so on.To avoid this propagation of redundant edges, the parsermust ensure that no duplicate dges are introduced intothe chart.
79icky does this simply by verifying every timean edge is added that the edge is not already in the chart.Although eliminating redundant edges prevents exces-sive inefficiency, bidirectional parsing may still performmore work than traditional left-to-right parsing.
In theprevious example, three edges are introduced into thechart to parse the NP -+ DET N edge.
A left-to-rightparser would only introduce two edges, one when thedeterminer is recognized, and another when the noun isrecognized.The benefit of bidirectional parsing can be seen whenprobabilistic prediction is introduced into the parser.Freqneatly, the syntactic structure of a constituent isnot determined by its left-corner word.
For instance,in the sequence V NP PP, the prepositional phrase PPcan modify either the noun phrase NP or the entire verbphrase V NP.
These two interpretations require differentVP rules to be predicted, but the decision about whichrule to use depends on more than just the verb.
The cor-rect rule may best be predicted by knowing the preposi-tion used in the PP.
Using probabilistic prediction, thedecision is made by pursuing the rule which has the high-est probability according to the prediction model.
Thisrule is then parsed bidirectionally.
If this rule is in factthe correct rule to analyze the constituent, hen no other42predictions will be made for that constituent, and therewill be no more edges produced than in left-to-right pars-ing.
Thus, the only case where bidirectional Parsing isless efficient han left-to-right parsing is when the pre-diction model fails to capture the elements of context ofthe sentence which determine its correct interpretation.3 .4 .
The  Three  Phases  o f  7~ickyCovered Le f t -Corner  The first phase uses probabilis-tic prediction based on the part-of-speech sequences fromthe input sentence to predict all grammar ules whichhave a non-zero probability of being dominated by thattrigram (based on the training corpus), i.e.P(A --4 BSlaoala2 ) > O i6)where al is the part-of-speech of the left-corner word ofB.
In this phase, the only exception to the probabilis-tic prediction is that any rule which can immediatelydominate the preterminal category of any word in thesentence is also predicted, regardless of its probability.This type of prediction is referred to as exhaustive pre-diction.
All of the predicted rules are processed using astandard best-first agenda processing algorithm, wherethe highest scoring edge in the chart is advanced.Covered B id i rect iona l  If an S spanning the entireword string is not recognized by the end of the firstphase, the covered bidirectional phase continues theparsing process.
Using the chart generated by the firstphase, rules are predicted not only by the trigram cen-tered at the left-corner word of the rule, but by thetrigram centered at the left-corner word of any of thechildren of that rule, i.e.V(A --+ ,~B*lbob~b2 ) > 0.
(7)where bl is the part-of-speech associated with the left-most word of constituent B.
This phase introduces in-complete theories into the chart which need to be ex-panded to the left and to the right, as described in thebidirectional parsing section above.Tree  Complet ion  If the bidirectional processing failsto produce a successful parse, then it is assumed thatthere is some part of the input sentence which is notcovered well by the training material.
In the final phase,exhaustive prediction is performed on all complete the-ories which were introduced in the previous phases butwhich are not predicted by the trigrams beneath t.heln(i.e.
V(rule \] trigram) = 0).In this phase, edges ~tre only predicted by their left-corner word.
As mentioned previously, bidirect.ionalparsing can be inefficient when the prediction model isinaccurate.
Since all edges which the pledictioa modelassigns non-zero probability have already been predicted,the model can no longer provide any information forfuture predictions.
Thus, bidirectional parsing in thisphase is very likely to be inefficient.
Edges already inthe chart will be parsed bidirectionally, since they werepredicted by the model, but all new edges will be pre-dicted by the left-corner word only.Since it is already known that the prediction model willassign a zero probability to these rules, these predictionsare instead scored based on the number of words spannedby the subtree which predicted them.
Thus, this phaseprocesses longer theories by introducing rules which canadvance them.
Each new theory which is proposed bythe parsing process is exhaustively predicted for, usingthe length-based scoring model.The final phase is used only when a sentence is so faroutside of the scope of the training material that noneof the previous phases are able to process it.
This phaseof the algorithm exhibits the worst-case xponential be-havior that is found in chart parsers which do not usenode packing.
Since the probabilistic model is no longeruseful in this phase, the parser is forced to propose anenormous number of theories.
The expectation (or hope)is that one of the theories which spans most of the sen-tence will be completed by this final process.
Dependingon the size of the grammar used, it may be unfeasibleto allow the parser to exhaust all possible predicts be-fore deciding an input is ungrammatical.
The questionof when the parser should give up is an empiricM issuewhich will not be explored here.Post -process ing:  Par t ia l  Pars ing  Once the finalphase has exhausted all predictions made by the gram-mar, or more likely, once the probability of all edgesin the chart falls below a certain threshold, Picky deter-mines the sentence to be ungrammatical.
However, sincethe chart produced by 7)icky contains all recognized con-stituents, sorted by probability, the chart can be used toextract partial parses.
As implemented, T'icky prints outthe most probable completed S constituent.4.
Why a New A lgor i thm?Previous research efforts have produced a wide vari-ety of parsing algorithms for probabilistic and non-probabilistie grammars.
One might question the needfor a. new algorithm to deal with context-sensitive prob-abilistic models.
However, these previous efforts havegenerally failed to address both efficiency and robust-hess effe(:ti rely.For noll-probabilistic grammar models, tile CKY algo-rithm \[9\] \[17\] provides efficiency and robustness in poly-nomia.1 time, O(6'n3).
C,I(Y can be modified to ha.n-43dle simple P-CFGs \[2\] without loss of efficiency.
How-ever, with the introduction of context-sensitive proba-bility models, such as the history-based grammar\[l\] andthe CFG with CSP models\[12\], CKY cannot be mod-ified to accommodate these models without exhibitingexponential behavior in the grammar size G. The linearbehavior of CKY with respect o grammar size is depen-dent upon being able to collapse the distinctions amongconstituents of the same type which span the same partof the sentence.
However, when using a context-sensitiveprobabilistic model, these distinctions are necessary.
Forinstance, in the CFG with CSP model, the part-of-speech sequence generated by a constituent affects theprobability of constituents that dominate it.
Thus, twoconstituents which generate different part-of-speech se-quences must be considered individually and cannot becollapsed.Earley's algorithm \[6\] is even more attractive than CKYin terms of efficiency, but it suffers from the same expo-nential behavior when applied to context-sensitive prob-abilistic models.
Still, Earley-style prediction improvesthe average case performance of en exponential chart-parsing algorithm by reducing the size of the searchspace, as was shown in \[12\].
However, Earley-style pre-diction has serious impacts on robust processing of un-grammatical sentences.
Once a sentence has been de-termined to be ungrammatical, Earley-style predictionprevents any new edges from being added to the parsechart.
This behavior seriously degrades the robustnessof a natural anguage system using this type of parser.A few recent works on probabilistic parsing have pro-posed algorithms and devices for efficient, robust chartparsing.
Bobrow\[3\] and Chitrao\[4\] introduce agenda-based probabilistic parsing algorithms, although nei-ther describe their algorithms in detail.
Both algo-rithms use a strictly best first search.
As both Chitraoand Magerman\[12\] observe, a best first search penalizeslonger and more complex constituents (i.e.
constituentswhich are composed of more edges), resulting in thrash-ing and loss of efficiency.
Chitrao proposes a heuristicpenalty based on constituent length to deal with thisproblem.
Magerman avoids thrashing by calculating thescore of a parse tree using the geometric mean of theprobabilities of the constituents contained in the tree.Moore\[14\] discusses techniques for improving the effi-ciency and robustness of chart parsers for unificationgrammars, but the ideas are applicable to probabilisticgrammars as well.
Some of the techniques proposed arewell-known ideas, such as compiling e-t, ra.nsitions (nullgaps) out of the grammar and heuristically controllingthe introduction of predictions.The Picky parser incorporates what we deem to be themost effective techniques of these previous works intoone parsing algorithm.
New techniques, uch as proba-bilistic prediction and the multi-phase approach, are in-troduced where the literature does not provide adequatesolutions.
Picky combines the standard chart parsingdata structures with existing bottom-up and top-downparsing operations, and includes a probabilistic versionof top-down filtering and over-the-top prediction.
Pickyalso incorporates a limited form of bi-directional pars-ing in a way which avoids its computationally expensiveside-effects.
It uses an agenda processing control mech-anism with the scoring heuristics of Pearl.Wi th  the exception of probabilistic prediction, most ofthe ideas in this work individually are not original to theparsing technology literature.
However, the combinationof these ideas provides robustness without sacrificing ef-ficiency, and efficiency without losing accuracy.5.
Resu l ts  o f  Exper imentsThe Picky parser was tested on 3 sets of 100 sentenceswhich were held out from the rest of the corpus duringtraining.
The training corpus consisted of 982 sentenceswhich were parsed using the same grammar that Pickyused.
The training and test corpora re samples from theMIT's Voyager direction-finding system.
7 Using Picky'sgrammar, these test sentences generate, on average, over100 parses per sentence, with some sentences generatedover 1,000 parses.The purpose of these experiments i to explore the im-pact of varying of Picky's parsing algorithm on parsingaccuracy, efficiency, and robustness.
For these exper-iments, we varied three attributes of the parser: thephases used by parser, the maximum number of edgesthe parser can produce before failure, and the minimumprobability parse acceptable.In the following analysis, the accuracy rate representsthe percentage of the test sentences for which the high-est probability parse generated by the parser is identicalto the "correct" pa.rse tree indicated in the parsed testcorpus, sEfficiency is measured by two ratios, the prediction ratioand the completion ratio.
The prediction ratio is definedas the ratio of number of predictions made by the parser7Spec ia l  thanks  to V ic tor  Zue  at  M IT  for  the  use of  the  speechdata  f rom MIT 's  Voyager  sys tem.8There  are  two except ions  to th is  accuracy  measure .
I f  t i leparser  generates  a p laus ib le  parse  for a sentences  wh ich  has  mul t i -pie p laus ib le  in t .e rpretat ions ,  the  parse  is cons idered  cc~rrcct.
Also.if the  parser  generates  a correct; pal'se~ I)ll~ the  parsecl  test  corpusconta ins  an  incor rect  parse  (i.e.
if there  is an  er ror  in the  answerkey) ,  the  parse  is cons idered  col-rect.44during the parse of a sentence to the number of con-stituents necessary for a correct parse.
The completionratio is the ratio of the number of completed edges tothe number of predictions during the parse of sentence.Robustness cannot be measured irectly by these ex-periments, since there are few ungrammatical sentencesand there is no implemented method for interpreting thewell-formed substring table when a parse fails.
However,for each configuration of the parser, we will explore theexpected behavior of the parser in the face of ungram-matical input.Since Picky has the power of a pure bottom-up arser,it would be useful to compare its performance and effi-ciency to that of a probabilistic bottom-up arser.
How-ever, an implementation of a probabilistic bottom-upparser using the same grammar produces on averageover 1000 constituents for each sentence, generating over15,000 edges without generating a parse at all!
Thissupports our claim that exhaustive CKY-like parsing al-gorithms are not feasible when probabilistic models areapplied to them.5 .1 .
Cont ro l  Conf igurat ionThe control for our experiments i  the configuration ofPicky with all three phases and with a maximum edgecount of 15,000.
Using this configuration, :Picky parsedthe 3 test sets with an 89.3% accuracy rate.
This isa slight improvement over Pearl's 87.5% accuracy ratereported in \[12\].Recall that we will measure the efficiency of a parserconfiguration by its prediction ratio and completion ratioon the test sentences.
A perfect prediction ratio is 1:1,i.e.
every edge predicted is used in the eventual parse.However, since there is ambiguity in the input sentences,a 1:1 prediction ratio is not likely to be achieved.
Picky'sprediction ratio is approximately than 4.3:1, and its ratioof predicted edges to completed edges is nearly 1.3:1.Thus, although the prediction ratio is not perfect, onaverage for every edge that is predicted more than onecompleted constituent results.This is the most robust configuration of Picky which willbe attempted in our experiments, ince it includes bidi-rectional parsing (phase II) and allows so many edges tobe created.
Although there was not a sufficient num-ber or variety of ungrammatical sentences to explorethe robustness of this configuration further, one inter-esting example did occur in the test sets.
The sentenceHow do I how do I get to MIT?is an ungranm~atical but interpretable sentence whichbegins with a restart.
The Pearl parser would have gen-erated no analysis tbr the latter part of the sentence andthe corresponding sections of the chart would be empty.Using bidirectional probabilistic prediction, Picky pro-duced a correct partial interpretation of the last 6 wordsof the sentence, "how do I get to MIT?"
One sentencedoes not make for conclusive evidence, but it repre-sents the type of performance which is expected fromthe Picky algorithm.5 .2 .
Phases  vs .
E f f i c iencyEach of Picky's three phases has a distinct role in theparsing process.
Phase I tries to parse the sentenceswhich are most standard, i.e.
most consistent with thetraining material.
Phase II uses bidirectional parsing totry to complete the parses for sentences which are nearlycompletely parsed by Phase I.
Phase III uses a simplis-tic heuristic to glue together constituents generated byphases I and II.
Phase III is obviously inefficient, since itis by definition processing atypical sentences.
Phase IIis also inefficient because of the bidirectional predictionsadded in this phase.
But phase II also amplifies the in-efficiency of phase III, since the bidirectional predictionsadded in phase II are processed further in phase III.Pred.
Comp.Phases Ratio RatioI 1.95 1.02I,II 2.15 0.94II 2.44 0.86I,III 4.01 1.44III 4.29 1.40I,II,III 4.30 1.28II,III 4.59 1.24Coverage %Error75.7% 2.3%77.0% 2.3%77.3% 2.0%88.3% 11.7%88.7% 11.3%89.3% 10.7%89.7% 10.3%Table 1: Prediction and Completion Ratios and accuracystatistics for Picky configured with different subsets ofPicky's three phases.In Table 1, we see the efficiency and accuracy of Pickyusing different, subsets of the parser's phases.
Using thecontrol parser (phases I, II, and II), the parser has a 4.3:1prediction ratio and a 1.3:1 completion ratio.By omitting phase III, we eliminate nearly half of thepredictions and half the completed edges, resulting ina 2.15:1 prediction ratio.
But this efficiency comes atthe cost of coverage, which will be discussed in the nextsection.By omitting phase II, we observe a slight reduction inpredictions, but an increase in completed edges.
Thisbehavior esults from the elimination of the bidirectionalpredictions, which tend to genera.re duplicate edges.Note that this configuration, while slightly more efficient,45is less robust in processing ungrammatical input.5 .3 .
Phases  vs .
AccuracyFor some natural anguage applications, uch as a natu-ral language interface to a nuclear reactor or to a com-puter operating system, it is imperative for the user tohave confidence in the parses generated by the parser.Picky has a relatively high parsing accuracy rate ofnearly 90%; however, 10% error is far too high for fault-intolerant applications.Phase No.I + II 238III 62Overall 300Accuracy Coverage %Error97% 77% 3%60% 12% 40%89.3% 89.3% 10.7%Table 2: 7~icky's parsing accuracy, categorized by thephase which the parser reached in processing the testsentences.Consider the data in Table 2.
While the parser has anoverall accuracy rate of 89.3%, it is.far more accurate onsentences which are parsed by phases I and II, at 97%.Note that 238 of the 300 sentences, or 79%, of the testsentences are parsed in these two phases.
Thus, by elimi-nating phase III, the percent error can be reduced to 3%,while maintaining 77% coverage.
An alternative to elim-inating phase III is to replace the length-based heuristicof this phase with a secondary probabilistic model of thedifficult sentences in this domain.
This secondary modelmight be trained on a set of sentences which cannot beparsed in phases I and II.5 .4 .
Edge  Count  vs .
AccuracyIn the original implementation of the Picky algorithm,we intended to allow the parser to generate dges un-til it found a complete interpretation or exhausted allpossible predictions.
However, for some ungrammati-cal sentences, the parser generates tens of thousands ofedges without terminating.
To limit the processing timefor the experiments, we implemented a maximum edgecount which was sufficiently large so that all grammat-ical sentences in the test corpus would be parsed.
Allof the grammatical test sentences generated a parse be-fore producing 15,000 edges.
However, some sentencesproduced thousands of edges only to generate an incor-rect parse.
In fact, it seemed likely tha,t there might bea correlation between very high edge counts and incor-rect parses.
We tested this hypothesis by varying themaximum edge count.In Table 3, we see an increase in efficiency and a decreaseMaximum Pred.
Comp.Edge Count Ratio Ratio15,000 4.30 1.351,000 3.69 0.93500 3.08 0.82300 2.50 0.86150 1.95 0.92100 1.60 0.84Coverage %Error89.3% 10.7%83.3% 7.0%80.3% 5.3%79.3% 2.7%66.0% 1.7%43.7% 1.7%Table 3: Prediction and Completion Ratios and accuracystatistics for 7~icky configured with different maximumedge count.in accuracy as we reduce the maximum number of edgesthe parser will generate before declaring a sentence un-grammatical.
By reducing the maximum edge count bya factor of 50, from 15,000 to 300, we can nearly cutin half the number of predicts and edges generated bythe parser.
And while this causes the accuracy rate tofall from 89.3% to 79.3%, it also results in a significantdecrease in error rate, down to 2.7%.
By decreasing themaximum edge count down to 150, the error rate can bereduced to 1.7%.5 .5 .
P robab i l i ty  vs .
AccuracySince a probability represents the likelihood of an inter-pretation, it is not unreasonable to expect he probabil-ity of a parse tree to be correlated with the accuracy ofthe parse.
However, based on the probabilities associ-ated with the "correct" parse trees of the test sentences,there appears to be no such correlation.
Many of thetest sentences had correct parses with very low probabil-ities (10-1?
), while others had much higher probabilities(10-2).
And the probabilities associated with incorrectparses were not distinguishable from the probabilities ofcorrect parses.The failure to find a correlation between probability a.ndaccuracy in this experiment does not prove conclusivelythat no such correlation exists.
Admittedly, the trainingcorpus used for all of these experiments i far smallerthan one would hope to estimate the CFG with CSPmodel parameters.
Thus, while the model is trained wellenough to steer the parsing search, it may not be suffi-ciently trained to provide meaningful probability values.6.
Conc lus ionsThere are many different applications of natural lan-guage parsing, and each application has a different costthreshold for efficiency, robustness, and accuracy.
'\['he"Pick), algorithm introduces a framework for integral.ing46these thresholds into the configuration of the parser inorder to maximize the effectiveness of the parser for thetask at hand.
An application which requires a high de-gree of accuracy would omit the Tree Completion phaseof the parser.
A real-time application would limit thenumber of edges generated by the parser, likely at thecost of accuracy.
An application which is robust to er-rors but requires efficient processing of input would omitthe Covered Bidirectional phase.The :Picky parsing algorithm illustrates how probabilis-tic modelling of natural anguage can be used to improvethe efficiency, robustness, and accuracy of natural lan-guage understanding tools.REFERENCES1.
Black, E., Jelinek, F., Lafferty, J., M~german, D. M.,Mercer, R. and Roukos, S. 1992.
Towards History-basedGrammars: Using Richer Models of Context in Prob-abilistic Parsing.
In Proceedings of the February 1992DARPA Speech and Natural Language Workshop.
Ar-den House, NY.2.
Brown, P., Jelinek, F., and Mercer, R. 1991.
BasicMethod of Probabilistic Context-free Grammars.
IBMInternal Report.
Yorktown Heights, NY.3.
Bobrow, R. J.
1991.
Statistical Agenda Parsing.
In Pro-ceedings of the February 1991 DARPA Speech and Nat-ural Language Workshop.
Asilomar, California.4.
Chitrao, M. and Grishman, R. 1990.
Statistical Parsingof Messages.
In Proceedings of the June 1990 DARPASpeech and Natural Language Workshop.
Hidden Valley,Pennsylvania.5.
Church, K. 1988.
A Stochastic Parts Program and NounPhrase Parser for Unrestricted Text.
In Proceedings ofthe Second Conference on Applied Natural LanguageProcessing.
Austin, Texas.6.
Earley, J.
1970.
An Efficient Context-Free Parsing Algo-rithm.
Communications of the A CM Vol.
13, No.
2, pp.94-102.7.
Gale, W. A. and Church, K. 1990.
Poor Estimates ofContext are Worse than None.
In Proceedings of theJune 1990 DARPA Speech and Natural Language Work-shop.
Hidden Valley, Pennsylvania.8.
Jelinek, F. 1985.
Self-orgmlizing Language Modeling forSpeech Recognition.
IBM Report.9.
Kasami, T. 1965.
An Efficient Recognition and Syn-tax Algorithm for Context-Free Languages.
ScientificReport AFCRL-65-758, Air Force Cambridge ResearchLaboratory.
Bedford, Massachusetts.10.
Kay, M. 1980.
Algorithm Schemata and Data Structuresin Syntactic Processing.
CSL-80-1~, October 1980.11.
Kimball, J.
1973.
Principles of Surface Structure Parsingin Natural Language.
Cognition, 2.15-47.12.
Magerman, D. M. and Marcus, M. P. 1991.
Pearl: AProbabilistic Chart Parser.
In Proceedings of the Euro-pean ACL Conference, Mavcli 1991.
Berlin, Germany.13.
Magerman, D. M. and Weir, C. 1992.
Probabilisti?
: Pre-diction and Picky Chart Parsing.
In Proceedings of the47February 1992 DARPA Speech and Natural LanguageWorkshop.
Arden House, NY.14.
Moore, R. and Dowding, J.
1991.
Efficient Bottom-UpParsing.
In Proceedings of the February 1991 DARPASpeech and Natural Language Workshop.
Asilomar, Cal-ifornia.15.
Sharman, R. A., Jelinek, F., and Mercer, R. 1990.
Gen-erating a Grammar for Statistical Training.
In Proceed-ings of the June 1990 DARPA Speech and Natural Lan-guage Workshop.
Hidden Valley, Pennsylvania.16.
Seneff, Stephanie 1989.
TINA.
In Proceedings of the Au-gust 1989 International Workshop in Parsing Technolo-gies.
Pittsburgh, Pennsylvania.17.
Younger, D. H. 1967.
Recognition and Parsing ofContext-Free Languages in Time n 3.
Information andControlVol.
10, No.
2, pp.
189-208.
