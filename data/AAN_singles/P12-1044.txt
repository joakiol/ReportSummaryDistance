Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 420?429,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsLearning Syntactic Verb Frames Using Graphical ModelsThomas LippincottUniversity of CambridgeComputer LaboratoryUnited Kingdomtl318@cam.ac.ukDiarmuid O?
Se?aghdhaUniversity of CambridgeComputer LaboratoryUnited Kingdomdo242@cam.ac.ukAnna KorhonenUniversity of CambridgeComputer LaboratoryUnited Kingdomalk23@cam.ac.ukAbstractWe present a novel approach for buildingverb subcategorization lexicons using a simplegraphical model.
In contrast to previous meth-ods, we show how the model can be trainedwithout parsed input or a predefined subcate-gorization frame inventory.
Our method out-performs the state-of-the-art on a verb clus-tering task, and is easily trained on arbitrarydomains.
This quantitative evaluation is com-plemented by a qualitative discussion of verbsand their frames.
We discuss the advantages ofgraphical models for this task, in particular theease of integrating semantic information aboutverbs and arguments in a principled fashion.We conclude with future work to augment theapproach.1 IntroductionSubcategorization frames (SCFs) give a compact de-scription of a verb?s syntactic preferences.
Thesetwo sentences have the same sequence of lexicalsyntactic categories (VP-NP-SCOMP), but the firstis a simple transitive (?X understood Y?
), while thesecond is a ditransitive with a sentential complement(?X persuaded Y that Z?):1.
Kim (VP understood (NP the evidence(SCOMP that Sandy was present)))2.
Kim (VP persuaded (NP the judge) (SCOMPthat Sandy was present))An SCF lexicon would indicate that ?persuade?is likely to take a direct object and sentential com-plement (NP-SCOMP), while ?understand?
is morelikely to take just a direct object (NP).
A compre-hensive lexicon would also include semantic infor-mation about selectional preferences (or restrictions)on argument heads of verbs, diathesis alternations(i.e.
semantically-motivated alternations betweenpairs of SCFs) and a mapping from surface framesto the underlying predicate-argument structure.
In-formation about verb subcategorization is useful fortasks like information extraction (Cohen and Hunter,2006; Rupp et al, 2010), verb clustering (Korho-nen et al, 2006b; Merlo and Stevenson, 2001) andparsing (Carroll et al, 1998).
In general, tasks thatdepend on predicate-argument structure can benefitfrom a high-quality SCF lexicon (Surdeanu et al,2003).Large, manually-constructed SCF lexiconsmostly target general language (Boguraev andBriscoe, 1987; Grishman et al, 1994).
However,in many domains verbs exhibit different syntacticbehavior (Roland and Jurafsky, 1998; Lippincottet al, 2010).
For example, the verb ?develop?has specific usages in newswire, biomedicine andengineering that dramatically change its probabilitydistribution over SCFs.
In a few domains likebiomedicine, the need for focused SCF lexiconshas led to manually-built resources (Bodenreider,2004).
Such resources, however, are costly, prone tohuman error, and in domains where new lexical andsyntactic constructs are frequently coined, quicklybecome obsolete (Cohen and Hunter, 2006).
Data-driven methods for SCF acquisition can alleviate420these problems by building lexicons tailored tonew domains with less manual effort, and highercoverage and scalability.Unfortunately, high quality SCF lexicons are dif-ficult to build automatically.
The argument-adjunctdistinction is challenging even for humans, manySCFs have no reliable cues in data, and some SCFs(e.g.
those involving control such as type raising)rely on semantic distinctions.
As SCFs follow a Zip-fian distribution (Korhonen et al, 2000), many gen-uine frames are also low in frequency.
State-of-the-art methods for building data-driven SCF lexiconstypically rely on parsed input (see section 2).
How-ever, the treebanks necessary for training a high-accuracy parsing model are expensive to build fornew domains.
Moreover, while parsing may aid thedetection of some frames, many experiments havealso reported SCF errors due to noise from parsing(Korhonen et al, 2006a; Preiss et al, 2007).Finally, many SCF acquisition methods operatewith predefined SCF inventories.
This subscribes toa single (often language or domain-specific) inter-pretation of subcategorization a priori, and ignoresthe ongoing debate on how this interpretation shouldbe tailored to new domains and applications, such asthe more prominent role of adjuncts in informationextraction (Cohen and Hunter, 2006).In this paper, we describe and evaluate a novelprobabilistic data-driven method for SCF acquisi-tion aimed at addressing some of the problems withcurrent approaches.
In our model, a Bayesian net-work describes how verbs choose their argumentsin terms of a small number of frames, which arerepresented as distributions over syntactic relation-ships.
First, we show that by allowing the infer-ence process to automatically define a probabilisticSCF inventory, we outperform systems with hand-crafted rules and inventories, using identical syntac-tic features.
Second, by replacing the syntactic fea-tures with an approximation based on POS tags, weachieve state-of-the-art performance without relyingon error-prone unlexicalized or domain-specific lex-icalized parsers.
Third, we highlight a key advantageof our method compared to previous approaches: theease of integrating and performing joint inference ofadditional syntactic and semantic information.
Wedescribe how we plan to exploit this in our futureresearch.2 Previous workMany state-of-the-art SCF acquisition systems takegrammatical relations (GRs) as input.
GRs ex-press binary dependencies between lexical items,and many parsers produce them as output, withsome variation in inventory (Briscoe et al, 2006;De Marneffe et al, 2006).
For example, a subject-relation like ?ncsubj(HEAD, DEPENDENT)?
ex-presses the fact that the lexical item referred to byHEAD (such as a present-tense verb) has the lexi-cal item referred to by DEPENDENT as its subject(such as a singular noun).
GR inventories includedirect and indirect objects, complements, conjunc-tions, among other relations.
The dependency rela-tionships included in GRs correspond closely to thehead-complement structure of SCFs, which is whythey are the natural choice for SCF acquisition.There are several SCF lexicons for general lan-guage, such as ANLT (Boguraev and Briscoe, 1987)and COMLEX (Grishman et al, 1994), that dependon manual work.
VALEX (Preiss et al, 2007) pro-vides SCF distributions for 6,397 verbs acquiredfrom a parsed general language corpus via a systemthat relies on hand-crafted rules.
There are also re-sources which provide information about both syn-tactic and semantic properties of verbs: VerbNet(Kipper et al, 2008) draws on several hand-builtand semi-automatic sources to link the syntax andsemantics of 5,726 verbs.
FrameNet (Baker et al,1998) provides semantic frames and annotated ex-ample sentences for 4,186 verbs.
PropBank (Palmeret al, 2005) is a corpus where each verb is annotatedfor its arguments and their semantic roles, coveringa total of 4,592 verbs.There are many language-specific SCF acquisi-tion systems, e.g.
for French (Messiant, 2008),Italian (Lenci et al, 2008), Turkish (Han et al,2008) and Chinese (Han et al, 2008).
These typ-ically rely on language-specific knowledge, eitherdirectly through heuristics, or indirectly throughparsing models trained on treebanks.
Furthermore,some require labeled training instances for super-vised (Uzun et al, 2008) or semi-supervised (Hanet al, 2008) learning algorithms.Two state-of-the-art data-driven systems for En-glish verbs are those that produced VALEX, Preiss etal.
(2007), and the BioLexicon (Venturi et al, 2009).421The Preiss system extracts a verb instance?s GRs us-ing the Rasp general-language unlexicalized parser(Briscoe et al, 2006) as input, and based on hand-crafted rules, maps verb instances to a predefinedinventory of 168 SCFs.
Filtering is then performedto remove noisy frames, with methods ranging froma simple single threshold to SCF-specific hypothesistests based on external verb classes and SCF inven-tories.
The BioLexicon system extracts each verb in-stance?s GRs using the lexicalized Enju parser tunedto the biomedical domain (Miyao, 2005).
Eachunique GR-set considered a potential SCF, and anexperimentally-determined threshold is used to fil-ter low-frequency SCFs.Note that both methods require extensive man-ual work: the Preiss system involves the a prioridefinition of the SCF inventory, careful construc-tion of matching rules, and an unlexicalized pars-ing model.
The BioLexicon system induces its SCFinventory automatically, but requires a lexicalizedparsing model, rendering it more sensitive to domainvariation.
Both rely on a filtering stage that dependson external resources and/or gold standards to selecttop-performing thresholds.
Our method, by contrast,does not use a predefined SCF inventory, and canperform well without parsed input.Graphical models have been increasingly popu-lar for a variety of tasks such as distributional se-mantics (Blei et al, 2003) and unsupervised POStagging (Finkel et al, 2007), and sampling methodsallow efficient estimation of full joint distributions(Neal, 1993).
The potential for joint inference ofcomplementary information, such as syntactic verband semantic argument classes, has a clear and in-terpretable way forward, in contrast to the pipelinedmethods described above.
This was demonstrated inAndrew et al (2004), where a Bayesian model wasused to jointly induce syntactic and semantic classesfor verbs, although that study relied on manuallyannotated data and a predefined SCF inventory andMLE.
More recently, Abend and Rappoport (2010)trained ensemble classifiers to perform argument-adjunct disambiguation of PP complements, a taskclosely related to SCF acquisition.
Their study em-ployed unsupervised POS tagging and parsing, andmeasures of selectional preference and argumentstructure as complementary features for the classi-fier.Finally, our task-based evaluation, verb clusteringwith Levin (1993)?s alternation classes as the goldstandard, was previously conducted by Joanis andStevenson (2003), Korhonen et al (2008) and Sunand Korhonen (2009).3 MethodologyIn this section we describe the basic components ofour study: feature sets, graphical model, inference,and evaluation.3.1 Input and feature setsWe tested several feature sets either based on, orapproximating, the concept of grammatical relationdescribed in section 2.
Our method is agnostic re-garding the exact definition of GR, and for examplecould use the Stanford inventory (De Marneffe et al,2006) or even an entirely different lexico-syntacticformalism like CCG supertags (Curran et al, 2007).In this paper, we distinguish ?true GRs?
(tGRs), pro-duced by a parser, and ?pseudo GRs?
(pGRs), aPOS-based approximation, and employ subscripts tofurther specify the variations described below.
Ourinput has been parsed into Rasp-style tGRs (Briscoeet al, 2006), which facilitates comparison with pre-vious work based on the same data set.We?ll use a simple example sentence to illustratehow our feature sets are extracted from CONLL-formatted data (Nivre et al, 2007).
The CONLLformat is a common language for comparing outputfrom dependency parsers: each lexical item has anindex, lemma, POS tag, tGR in which it is the de-pendent, and index to the corresponding head.
Table1 shows the relevant fields for the sentence ?We runtraining programmes in Romania and other coun-tries?.We define the feature set for a verb occurrence asthe counts of each GR the verb participates in.
Table2 shows the three variations we tested: the simpletGR type, with parameterization for the POS tagsof head and dependent, and with closed-class POStags (determiners, pronouns and prepositions) lexi-calized.
In addition, we tested the effect of limitingthe features to subject, object and complement tGRs,indicated by adding the subscript ?lim?, for a total ofsix tGR-based feature sets.While ideally tGRs would give full informa-422Index Lemma POS Head tGR1 we PPIS2 2 ncsubj2 run VV0 03 training NN1 4 ncmod4 programme NN2 2 dobj5 in II 4 ncmod6 romania NP1 7 conj7 and CC 5 dobj8 other JB 9 ncmod9 country NN2 7 conjTable 1: Simplified CONLL format for example sen-tence ?We run training programmes in Romania andother countries?.
Head=0 indicates the token is theroot.Name FeaturestGR ncsubj dobjtGRparam ncsubj(VV0,PPIS2) dobj(VV0,NN2)tGRparam,lex ncsubj(VV0,PPIS2-we) dobj(VV0,NN2)Table 2: True-GR features for example sentence:note there are also tGR?,lim versions of each thatonly consider subjects, objects and complementsand are not shown.tion about the verb?s syntactic relationship to otherwords, in practice parsers make (possibly prema-ture) decisions, such as deciding that ?in?
modifies?programme?, and not ?run?
in our example sen-tence.
An unlexicalized parser cannot distinguishthese based just on POS tags, while a lexicalizedparser requires a large treebank.
We therefore definepseudo-GRs (pGRs), which consider each (distance,POS) pair within a given window of the verb to bea potential tGR.
Table 3 shows the pGR features forthe test sentence using a window of three.
As withtGRs, the closed-class tags can be lexicalized, butthere are no corresponding feature sets for param(since they are already built from POS tags) or lim(since there is no similar rule-based approach).Name FeaturespGR -1(PPIS2) 1(NN1) 2(NN2) 3(II)pGRlex -1(PPIS2-we) 1(NN1) 2(NN2) 3(II-in)Table 3: Pseudo-GR features for example sentencewith window=3Whichever feature set is used, an instance is sim-ply the count of each GR?s occurrences.
We extractinstances for the 385 verbs in the union of our twogold standards from the VALEX lexicon?s data set,which was used in previous studies (Sun and Korho-nen, 2009; Preiss et al, 2007) and facilitates com-parison with that resource.
This data set is drawnfrom five general-language corpora parsed by Rasp,and provides, on average, 7,000 instances per verb.3.2 SCF extractionOur graphical modeling approach uses the Bayesiannetwork shown in Figure 1.
Its generative storyis as follows: when a verb is instantiated, an SCFis chosen according to a verb-specific multinomial.Then, the number and type of syntactic arguments(GRs) are chosen from two SCF-specific multino-mials.
These three multinomials are modeled withuniform Dirichlet priors and corresponding hyper-parameters ?, ?
and ?.
The model is trained viacollapsed Gibbs sampling, where the probability ofassigning a particular SCF s to an instance of verb vwith GRs (gr1 .
.
.
grn) is the productP (s|V erb = v,GRs = gr1 .
.
.
grn) =P (SCF = s|V erb = v)?P (N = n|SCF = s)?
?i=1:nP (GR = gri|SCF = s)The three terms, given the hyper-parameters andconjugate-prior relationship between Dirichlet andMultinomial distributions, can be expressed in termsof current assignments of s to verb v ( csv ), s toGR-count n ( csn ) and s to GR ( csg ), the corre-sponding totals ( cv, cs ), the dimensionality of thedistributions ( |SCF |, |N | and |G| ) and the hyper-parameters ?, ?
and ?
:P (SCF = s|V erb = v) = (csv+?
)/(cv+|SCF |?
)P (N = n|SCF = s) = (csn + ?
)/(cs + |N |?
)P (GR = gri|SCF = s) = (csgri +?
)/(cs + |G|?
)Note that N , the possible GR-count for an in-stance, is usually constant for pGRs ( 2 ?
window), unless the verb is close to the start or end of thesentence.423?
// V erbxSCF&&V erbii ?
ISCFi //Ni||SCFxNoo ?ooGRi SCFxGRoo ?ooFigure 1: Our simple graphical model reflecting subcategorization.
Double-circles indicate an observedvalue, arrows indicate conditional dependency.
What constitutes a ?GR?
depends on the feature set beingused.We chose our hyper-parameters ?
= ?
= ?
= .02to reflect the characteristic sparseness of the phe-nomena (i.e.
verbs tend to take a small number ofSCFs, which in turn are limited to a small numberof realizations).
For the pGRs we used a windowof 5 tokens: a verb?s arguments will fall within asmall window in the majority of cases, so there isdiminished return in expanding the window at thecost of increased noise.
Finally, we set our SCFcount to 40, about twice the size of the strictly syn-tactic general-language gold standard we describe insection 3.3.
This overestimation allows some flex-ibility for the model to define its inventory basedon the data; any supernumerary frames will act as?junk frames?
that are rarely assigned and hencewill have little influence.
We run Gibbs samplingfor 1000 iterations, and average the final 100 sam-ples to estimate the posteriors P (SCF |V erb) andP (GR|SCF ).
Variance between adjacent states?estimates of P (SCF |V erb) indicates that the sam-pling typically converges after about 100-200 itera-tions.13.3 EvaluationQuantitative: cluster gold standardEvaluating the output of unsupervised methods isnot straightforward: discrete, expert-defined cate-gories (like many SCF inventories) are unlikely toline up perfectly with data-driven, probabilistic out-put.
Even if they do, finding a mapping betweenthem is a problem of its own (Meila, 2003).1Full source code for this work is available at http://cl.cam.ac.uk/?tl318/files/subcat.tgzOur goal is to define a fair quantitative compari-son between arbitrary SCF lexicons.
An SCF lexi-con makes two claims: first, that it defines a reason-able SCF inventory.
Second, that for each verb, ithas an accurate distribution over that inventory.
Wetherefore compare the lexicons based on their per-formance on a task that a good SCF lexicon shouldbe useful for: clustering verbs into lexical-semanticclasses.
Our gold standard is from (Sun and Korho-nen, 2009), where 200 verbs were assigned to 17classes based on their alternation patterns (Levin,1993).
Previous work (Schulte im Walde, 2009;Sun and Korhonen, 2009) has demonstrated that thequality of an SCF lexicon?s inventory and probabil-ity estimates corresponds to its predictive power formembership in such alternation classes.To compare the performance of our feature sets,we chose the simple and familiar K-Means cluster-ing algorithm (Hartigan and Wong, 1979).
The in-stances are the verbs?
SCF distributions, and we se-lect the number of clusters by the Silhouette vali-dation technique (Rousseeuw, 1987).
The clustersare then compared to the gold standard clusters withthe purity-based F-Score from Sun and Korhonen(2009) and the more familiar Adjusted Rand Index(Hubert and Arabie, 1985).
Our main point of com-parison is the VALEX lexicon of SCF distributions,whose scores we report alongside ours.Qualitative: manual gold standardWe also want to see how our results line up witha traditional linguistic view of subcategorization,but this requires digging into the unsupervised out-424put and associating anonymous probabilistic objectswith established categories.
We therefore presentsample output in three ways: first, we show theclustering output from our top-performing method.Second, we plot the probability mass over GRs fortwo anonymous SCFs that correspond to recogniz-able traditional SCFs, and one that demonstrates un-expected behavior.
Third, we compared the out-put for several verbs to a coarsened version of themanually-annotated gold standard used to evaluateVALEX (Preiss et al, 2007).
We collapsed the orig-inal inventory of 168 SCFs to 18 purely syntacticSCFs based on their characteristic GRs and removedframes that depend on semantic distinctions, leav-ing the detection of finer-grained and semantically-based frames for future work.4 Results4.1 Verb clusteringWe evaluated SCF lexicons based on the eight fea-ture sets described in section 3.1, as well as theVALEX SCF lexicon described in section 2.
Table 4shows the performance of the lexicons in ascendingorder.Method Pur.
F-score Adj.
RandtGR .24 .02tGRlim .27 .02pGRlex .32 .09tGRlim,param .35 .08pGR .35 .10VALEX .36 .10tGRparam,lex .37 .10tGRparam .39 .12tGRlim,param,lex .44 .12Table 4: Task-based evaluation of lexicons acquiredwith each of the eight feature types, and the state-of-the-art rule-based VALEX lexicon.These results lead to several conclusions: first,training our model on tGRs outperforms pGRs andVALEX.
Since the parser that produced them isknown to perform well on general language (Briscoeet al, 2006), the tGRs are of high quality: it makessense that reverting to the pGRs is unnecessary inthis case.
The interesting point is the major perfor-mance gain over VALEX, which uses the same tGRfeatures along with expert-developed rules and in-ventory.Second, we achieve performance comparable toVALEX using pGRs with a narrow window width.Since POS tagging is more reliable and robust acrossdomains than parsing, retraining on new domainswill not suffer the effects of a mismatched parsingmodel (Lippincott et al, 2010).
It is therefore pos-sible to use this method to build large-scale lexiconsfor any new domain with sufficient data.Third, lexicalizing the closed-class POS tags in-troduces semantic information outside the scopeof the alternation-based definition of subcatego-rization.
For example, subdividing the indefinitepronoun tag ?PN1?
into ?PN1-anyone?
and ?PN1-anything?
gives information about the animacy ofthe verb?s arguments.
Our results show this degradesperformance for both pGR and tGR features, unlessthe latter are limited to tGRs traditionally thought tobe relevant for the task.4.2 Qualitative analysisTable 5 shows clusters produced by our top-scoringmethod, GRparam,lex,lim.
Some clusters are imme-diately intelligible at the semantic level and corre-spond closely to the lexical-semantic classes foundin Levin (1993).
For example, clusters 1, 6, and 14include member verbs of Levin?s SAY, PEER andAMUSE classes, respectively.
Some clusters arebased on broader semantic distinctions (e.g.
cluster2 which groups together verbs related to locations)while others relate semantic classes purely basedon their syntactic similarity (e.g.
the verbs in clus-ter 17 share strong preference for ?to?
preposition).The syntactic-semantic nature of the clusters reflectsthe multimodal nature of verbs and illustrates why acomprehensive subcategorization lexicon should notbe limited to syntactic frames.
This phenomenon isalso encouraging for future work to tease apart andsimultaneously exploit several verbal aspects via ad-ditional latent structure in the model.An SCF?s distribution over features can reveal itsplace in the traditional definition of subcategoriza-tion.
Figure 2 shows the high-probability (>.02)tGRs for one SCF: the large mass centered on di-rect object tGRs indicates this approximates the no-tion of ?transitive?.
Looking at the verbs most likelyto take this SCF (?stimulate?, ?conserve?)
confirms4251 exclaim, murmur, mutter, reply, retort, say,sigh, whisper2 bang, knock, snoop, swim, teeter3 flicker, multiply, overlap, shine4 batter, charter, compromise, overwhelm,regard, sway, treat5 abolish, broaden, conserve, deepen, eradi-cate, remove, sharpen, shorten, stimulate,strengthen, unify6 gaze, glance, look, peer, sneer, squint, stare7 coincide, commiserate, concur, flirt, inter-act8 grin, smile, wiggle9 confuse, diagnose, march10 mate, melt, swirl11 frown, jog, stutter12 chuckle, mumble, shout13 announce, envisage, mention, report, state14 frighten, intimidate, scare, shock, upset15 bash, falter, snarl, wail, weaken16 cooperate, eject, respond, transmit17 affiliate, compare, contrast, correlate, for-ward, mail, shipTable 5: Clusters (of size >2 and <20) producedusing tGRparam,lex,limthis.
Figure 3 shows a complement-taking SCF,which is far rarer than simple transitive but alsoclearly induced by our model.The induced SCF inventory also has some redun-dancy, such as additional transitive frames besidefigure 2, and frames with poor probability estimates.Most of these issues can be traced to our simplifyingassumption that each tGR is drawn independentlyw.r.t.
an instance?s other tGRs.
For example, if anSCF gives any weight to indirect objects, it givesnon-zero probability to an instance with only indi-rect objects, an impossible case.
This can lead toskewed probability estimates: since some tGRs canoccur multiple times in a given instance (e.g.
in-direct objects and prepositional phrases) the modelmay find it reasonable to create an SCF with allprobability focused on that tGR, ignoring all oth-ers, such as in figure 4.
We conclude that our inde-pendence assumption was too strong, and the modelwould benefit from defining more structure withinFigure 2: The SCF corresponding to transitive hasmost probability centered on dobj (e.g.
stimulate,conserve, deepen, eradicate, broaden)Figure 3: The SCF corresponding to verbs takingcomplements has more probability on xcomp andccomp (e.g.
believe, state, agree, understand, men-tion)instances.The full tables necessary to compare verb SCFdistributions from our output with the manual goldstandard are prohibited by space, but a few exam-ples reinforce the analysis above.
The verbs ?load?and ?fill?
show particularly high usage of ditransi-tive SCFs in the gold standard.
In our inventory, thisis reflected in high usage of an SCF with probabil-ity centered on indirect objects, but due to the inde-pendence assumptions the frame has a correspond-ing low probability on subjects and direct objects,despite the fact that these necessarily occur alongwith any indirect object.
The verbs ?acquire?
and?buy?
demonstrate both a strength of our approachand a weakness of using parsed input: both verbs426Figure 4: This SCF is dominated by indirect objectsand complements, catering to verbs that may takeseveral such tGRs, at the expense of subjectsshow high probability of simple transitive in ouroutput and the gold standard.
However, the Raspparser often conflates indirect objects and preposi-tional phrases due to its unlexicalized model.
Whileour system correctly gives high probability to ditran-sitive for both verbs, it inherits this confusion andover-estimates ?acquire?
?s probability mass for theframe.
This is an example of how bad decisionsmade by the parser cannot be fixed by the graphi-cal model, and an area where pGR features have anadvantage.5 Conclusions and future workOur study reached two important conclusions: first,given the same data as input, an unsupervised prob-abilistic model can outperform a hand-crafted rule-based SCF extractor with a predefined inventory.We achieve better results with far less effort thanprevious approaches by allowing the data to gov-ern the definition of frames while estimating theverb-specific distributions in a fully Bayesian man-ner.
Second, simply treating POS tags within asmall window of the verb as pseudo-GRs producesstate-of-the-art results without the need for a pars-ing model.
This is particularly encouraging whenbuilding resources for new domains, where com-plex models fail to generalize.
In fact, by integrat-ing results from unsupervised POS tagging (Teichertand Daume?
III, 2009) we could render this approachfully domain- and language-independent.We did not dwell on issues related to choosingour hyper-parameters or latent class count.
Both ofthese can be accomplished with additional samplingmethods: hyper-parameters of Dirichlet priors canbe estimated via slice sampling (Heinrich, 2009),and their dimensionality via Dirichlet Process priors(Heinrich, 2011).
This could help address the redun-dancy we find in the induced SCF inventory, with thepotential SCFs growing to accommodate the data.Our initial attempt at applying graphical modelsto subcategorization also suggested several ways toextend and improve the method.
First, the indepen-dence assumptions between GRs in a given instanceturned out to be too strong.
To address this, we couldgive instances internal structure to capture condi-tional probability between generated GRs.
Second,our results showed the conflation of several verbalaspects, most notably the syntactic and semantic.In a sense this is encouraging, as it motivates ourmost exciting future work: augmenting this simplemodel to explicitly capture complementary infor-mation such as distributional semantics (Blei et al,2003), diathesis alternations (McCarthy, 2000) andselectional preferences (O?
Se?aghdha, 2010).
Thisstudy targeted high-frequency verbs, but the use ofsyntactic and semantic classes would also help withdata sparsity down the road.
These extensions wouldalso call for a more comprehensive evaluation, aver-aging over several tasks, such as clustering by se-mantics, syntax, alternations and selectional prefer-ences.In concrete terms, we plan to introduce latent vari-ables corresponding to syntactic, semantic and alter-nation classes, that will determine a verb?s syntac-tic arguments, their semantic realization (i.e.
selec-tional preferences), and possible predicate-argumentstructures.
By combining the syntactic classes withunsupervised POS tagging (Teichert and Daume?
III,2009) and the selectional preferences with distribu-tional semantics (O?
Se?aghdha, 2010), we hope toproduce more accurate results on these complemen-tary tasks while avoiding the use of any supervisedlearning.
Finally, a fundamental advantage of a data-driven, parse-free method is that it can be easilytrained for new domains.
We next plan to test ourmethod on a new domain, such as biomedical text,where verbs are known to take on distinct syntacticbehavior (Lippincott et al, 2010).4276 AcknowledgementsThe work in this paper was funded by the Royal So-ciety, (UK), EPSRC (UK) grant EP/G051070/1 andEU grant 7FP-ITC-248064.
We are grateful to LinSun and Laura Rimell for the use of their cluster-ing and subcategorization gold standards, and theACL reviewers for their helpful comments and sug-gestions.ReferencesOmri Abend and Ari Rappoport.
2010.
Fully unsuper-vised core-adjunct argument classification.
In ACL?10.Galen Andrew, Trond Grenager, and Christopher Man-ning.
2004.
Verb sense and subcategorization: us-ing joint inference to improve performance on com-plementary tasks.
EMNLP ?04.Collin Baker, Charles Fillmore, and John Lowe.
1998.The Berkeley FrameNet project.
In COLING ACL ?98.David Blei, Andrew Ng, Michael Jordan, and John Laf-ferty.
2003.
Latent dirichlet alocation.
Journal ofMachine Learning Research.Olivier Bodenreider.
2004.
The Unified Medical Lan-guage System (UMLS): integrating biomedical termi-nology.
Nucleic Acids Research, 32.Bran Boguraev and Ted Briscoe.
1987.
Large lexiconsfor natural language processing.
Computational Lin-guistics, 13.Ted Briscoe, John Carroll, and Rebecca Watson.
2006.The second release of the RASP system.
In Proceed-ings of the COLING/ACL on Interactive presentationsessions.John Carroll, Guido Minnen, and Ted Briscoe.
1998.Can subcategorisation probabilities help a statisticalparser?
In The 6th ACL/SIGDAT Workshop on VeryLarge Corpora.K Bretonnel Cohen and Lawrence Hunter.
2006.
Acritical review of PASBio?s argument structures forbiomedical verbs.
BMC Bioinformatics, 7.James Curran, Stephen Clark, and Johan Bos.
2007.
Lin-guistically motivated large-Scale NLP with C&C andBoxer.
In ACL ?07.Marie-Catherine De Marneffe, Bill Maccartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InLREC ?06.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2007.
The infinite tree.
In ACL ?07.Ralph Grishman, Catherine Macleod, and Adam Meyers.1994.
Comlex syntax: building a computational lexi-con.
In COLING ?94.Xiwu Han, Chengguo Lv, and Tiejun Zhao.
2008.Weakly supervised SVM for Chinese-English cross-lingual subcategorization lexicon acquisition.
In The11th Joint Conference on Information Science.J.A.
Hartigan and M.A.
Wong.
1979.
Algorithm AS 136:A K-Means clustering algorithm.
Journal of the RoyalStatistical Society.
Series C (Applied Statistics).Gregor Heinrich.
2009.
Parameter estimation for textanalysis.
Technical report, Fraunhofer IGD.428Gregor Heinrich.
2011.
Infinite LDA implementing theHDP with minimum code complexity.
Technical re-port, arbylon.net.Lawrence Hubert and Phipps Arabie.
1985.
Comparingpartitions.
Journal of Classification, 2.Eric Joanis and Suzanne Stevenson.
2003.
A general fea-ture space for automatic verb classification.
In EACL?03.Karin Kipper, Anna Korhonen, Neville Ryant, andMartha Palmer.
2008.
A large-scale classification ofEnglish verbs.
In LREC ?08.Anna Korhonen, Genevieve Gorrell, and Diana Mc-Carthy.
2000.
Statistical filtering and subcategoriza-tion frame acquisition.
In Proceedings of the JointSIGDAT Conference on Empirical Methods in NaturalLanguage Processing and Very Large Corpora.Anna Korhonen, Yuval Krymolowski, and Ted Briscoe.2006a.
A large subcategorization lexicon for naturallanguage processing applications.
In LREC ?06.Anna Korhonen, Yuval Krymolowski, and Nigel Collier.2006b.
Automatic classification of verbs in biomedi-cal texts.
In ACL ?06.Anna Korhonen, Yuval Krymolowski, and Nigel Collier.2008.
The choice of features for classification of verbsin biomedical texts.
In COLING ?08.Ro Lenci, Barbara Mcgillivray, Simonetta Montemagni,and Vito Pirrelli.
2008.
Unsupervised acquisitionof verb subcategorization frames from shallow-parsedcorpora.
In LREC ?08.Beth Levin.
1993.
English Verb Classes and Alternation:A Preliminary Investigation.
University of ChicagoPress, Chicago, IL.Thomas Lippincott, Anna Korhonen, and Diarmuid O?Se?aghdha.
2010.
Exploring subdomain variation inbiomedical language.
BMC Bioinformatics.Diana McCarthy.
2000.
Using semantic preferences toidentify verbal participation in role switching alterna-tions.
In NAACL ?00.Marina Meila.
2003.
Comparing clusterings by the Vari-ation of Information.
In COLT.Paola Merlo and Suzanne Stevenson.
2001.
Automaticverb classification based on statistical distributions ofargument structure.
Computational Linguistics.Ce?dric Messiant.
2008.
A subcategorization acquisitionsystem for French verbs.
In ACL HLT ?08 Student Re-search Workshop.Yusuke Miyao.
2005.
Probabilistic disambiguation mod-els for wide-coverage HPSG parsing.
In ACL ?05.Radford M. Neal.
1993.
Probabilistic inference usingmarkov chain Monte Carlo methods.
Technical report,University of Toronto.Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-donald, Jens Nilsson, Sebastian Riedel, and DenizYuret.
2007.
The CoNLL 2007 shared task on de-pendency parsing.
In The CoNLL Shared Task Sessionof EMNLP-CoNLL 2007.Diarmuid O?
Se?aghdha.
2010.
Latent variable models ofselectional preference.
In ACL ?10.Martha Palmer, Paul Kingsbury, and Daniel Gildea.2005.
The Proposition Bank: an annotated corpus ofsemantic roles.
Computational Linguistics.Judita Preiss, Ted Briscoe, and Anna Korhonen.
2007.
Asystem for large-scale acquisition of verbal, nominaland adjectival subcategorization frames from corpora.In ACL ?07.Douglas Roland and Daniel Jurafsky.
1998.
How verbsubcategorization frequencies are affected by corpuschoice.
In ACL ?98.Peter Rousseeuw.
1987.
Silhouettes: a graphical aidto the interpretation and validation of cluster analysis.Journal of Computational and Applied Mathematics.C.J.
Rupp, Paul Thompson, William Black, and John Mc-Naught.
2010.
A specialised verb lexicon as the ba-sis of fact extraction in the biomedical domain.
In In-terdisciplinary Workshop on Verbs: The Identificationand Representation of Verb Features.Sabine Schulte im Walde.
2009.
The induction of verbframes and verb classes from corpora.
In CorpusLinguistics.
An International Handbook.
Mouton deGruyter.Lin Sun and Anna Korhonen.
2009.
Improvingverb clustering with automatically acquired selectionalpreferences.
In EMNLP?09.Mihai Surdeanu, Sanda Harabagiu, John Williams, andPaul Aarseth.
2003.
Using predicate-argument struc-tures for information extraction.
In ACL ?03.Adam R. Teichert and Hal Daume?
III.
2009.
Unsuper-vised part of speech tagging without a lexicon.
InNIPS Workshop on Grammar Induction, Representa-tion of Language and Language Learning.E.
Uzun, Y. Klaslan, H.V.
Agun, and E. Uar.
2008.Web-based acquisition of subcategorization frames forTurkish.
In The Eighth International Conference onArtificial Intelligence and Soft Computing.Giulia Venturi, Simonetta Montemagni, Simone Marchi,Yutaka Sasaki, Paul Thompson, John McNaught, andSophia Ananiadou.
2009.
Bootstrapping a verb lex-icon for biomedical information extraction.
In Com-putational Linguistics and Intelligent Text Processing.Springer Berlin / Heidelberg.429
