First Joint Conference on Lexical and Computational Semantics (*SEM), pages 477?481,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsUOW-SHEF: SimpLex ?
Lexical Simplicity Ranking based on Contextualand Psycholinguistic FeaturesSujay Kumar JauharResearch Group in Computational LinguisticsUniversity of WolverhamptonStafford Street, WolverhamptonWV1 1SB, UKSujay.KumarJauhar@wlv.ac.ukLucia SpeciaDepartment of Computer ScienceUniversity of SheffieldRegent Court, 211 PortobelloSheffield, S1 4DP, UKL.Specia@dcs.shef.ac.ukAbstractThis paper describes SimpLex,1 a LexicalSimplification system that participated in theEnglish Lexical Simplification shared task atSemEval-2012.
It operates on the basis ofa linear weighted ranking function composedof context sensitive and psycholinguistic fea-tures.
The system outperforms a very strongbaseline, and ranked first on the shared task.1 IntroductionLexical Simplification revolves around replacingwords by their simplest synonym in a context awarefashion.
It is similar in many respects to the task ofLexical Substitution (McCarthy and Navigli, 2007)in that it involves elements of selectional preferenceon the basis of a central predefined criterion (sim-plicity in the current case), as well as sensitivity tocontext.Lexical Simplification envisages principally a hu-man target audience, and can greatly benefit chil-dren, second language learners, people with low lit-eracy levels or cognitive disabilities, and in generalfacilitate the dissemination of knowledge to wideraudiences.We experimented with a number of features thatwe posited might be inherently linked with tex-tual simplicity and selected the three that seemedthe most promising on an evaluation with the trialdataset.
These include contextual and psycholin-guistic components.
When combined using an SVM1Developed by co-organizers of the shared taskranker to build a model, such a model provides re-sults that offer a statistically significant improve-ment over a very strong context-independent base-line.
The system ranked first overall on the LexicalSimplification task.2 Related WorkLexical Simplification has received considerablyless interest in the NLP community as comparedwith Syntactic Simplification.
However, there area number of notable works related to the topic.In particular Yatskar et al (2010) leverage therelations between Simple Wikipedia and EnglishWikipedia to extract simplification pairs.
Biran et al(2011) extend this base methodology to apply lexi-cal simplification to input sentences.
De Belder andMoens (2010), in contrast, provide a more generalarchitecture for the task, with scope for possible ex-tension to other languages.These studies and others have envisaged a rangeof different target user groups including children(De Belder and Moens, 2010), people with low liter-acy levels (Aluisio et al, 2008) and aphasic readers(Carroll et al, 1998).The current work differs from previous researchin that it envisages a stand-alone lexical simpli-fication system based on linguistically motivatedand cognitive principles within the framework of ashared task.
Its core methodology remains open tointegration into a larger Text Simplification system.3 Task SetupThe English Lexical Simplification shared task atSemEval-2012 (Specia et al, 2012) required sys-477tems to rank a number of candidate substitutes(which were provided beforehand) based on theirsimplicity of usage in a given context.
For example,given the following context with an empty place-holder, and its candidate substitutes:Context: During the siege , GeorgeRobertson had appointed Shuja-ul-Mulk,who was a boy only 12 years old andthe youngest surviving son of Aman-ul-Mulk, as the ruler of Chitral.Candidates: {clever} {smart}{intelligent} {bright}a system is required to produce a ranking, e.g.
:System: {intelligent} {bright} {clever,smart}Note that ties were permitted and that all candi-dates needed to be included in the system rankings.4 The SimpLex Lexical SimplificationSystemIn an approach similar to what Hassan et al (2007)used for Lexical Substitution, SimpLex ranks can-didates based on a weighted linear scoring function,which has the generalized form:s (cn,i) =?m?M1rm (cn,i)where cn,i is the candidate substitute to be scored,and each rm is a standalone ranking function thatattributes to each candidate its rank based on itsuniquely associated features.
Based on this scoring,candidates for context are ranked in descending or-der of scores.In the development of the system we experi-mented with a number of these features includingranking based on word length, number of syllables,scoring with a 2-step cluster and rank architecture,latent semantic analysis, and average point-wise mu-tual information between the candidate and neigh-boring words in the context.However, the features which were intuitively thesimplest proved, in the end, to give the best results.They were selected based on their superior perfor-mance on the trial dataset and their competitivenesswith the strong Simple Frequency baseline.
Thesestand-alone features are described in what follows.4.1 Adapted N-Gram ModelThe motivation behind an n-gram model for LexicalSimplification is that the task involves an inherentWSD problem.
This is because the same word maybe used with different senses (and consequently dif-ferent levels of complexity) in different contexts.A blind application of n-gram frequency search-ing on the shared task?s dataset, however, gives sub-optimal results because of two main factors:1.
Inconsistently lemmatized candidates.2.
Blind replacement of even correctly lemma-tized forms in context producing ungrammat-ical results.We infer the correct inflection of all candidates fora given context based on the appearance of the orig-inal target word (which is also one of the candidatesubstitutes) in context.
To do this we run a part-of-speech (POS) tagger on the source text and note thePOS of the target word.
Then handcrafted rules areused to correctly inflect the other candidates basedon this POS tag.To resolve the issue of ungrammatical textual out-put, we further use a simple approach of poppingwords in close proximity to the placeholder and per-forming n-gram searches on all possible query com-binations.
Take for instance the following example:Context: He was away.Candidates: {going} {leaving}where ?going?
is evidently the original word in con-text, but ?leaving?
has also been suggested as a sub-stitute (there are many such cases in the datasets).One of the possible outcomes of popping contextwords leads to the correct sequence for the lattersubstitute, i.e.
?He was leaving?
with the word?away?
having been popped.The rationale behind this approach is that if one ofthe combinations is grammatically correct, the num-ber of n-gram hits it returns will far exceed thosereturned by ungrammatical ones.The n-gram (2 ?
n ?
5) searches are performedon the Google Web 1T corpus (Brants and Franz,2006), and the number of hits is weighted by thelength of the n-gram search (such that longer se-quences obtain higher weight).
This may seem like478a simplistic approach, especially when the candidatewords appear in long-distance dependency relationsto other parts of the sentence.
However, it should benoted that since the Web 1T corpus only consists ofn-grams with n ?
5, structures that contain longerdependencies than this are in any case not consid-ered, and hence do not interfere with local context.4.2 Bag-of-Words ModelThe limitations of performing queries on the GoogleWeb 1T are that n-grams hits must be in strict lin-ear order of appearance.
To overcome this diffi-culty, we further mimic the functioning of a bag-of-words model by taking all possible ordering ofwords of a given n-gram sequence.
This approach,to some extent, gives the possibility of observing co-occurrences of candidate and context words in vari-ous orderings of appearance.
This results in a num-ber of inadequate query strings, but possibly a few(as opposed to one in a linear n-gram search) goodword orderings with high hits as well.As with the previous model, only n-grams with2 ?
n ?
5 are taken.
For a given substitute the totalnumber of hits for all possible queries involving thatsubstitute are summed (with each hit being weightedby the length of its corresponding query in words).To obtain the final score, this sum is normalized bythe actual number of queries.4.3 Psycholinguistic Feature ModelThe MRC Psycholinguistic Database (Wilson, 1988)and the Bristol Norms (Stadthagen-Gonzalez andDavis, 2006) are knowledge repositories that asso-ciate scores to words based on a number of psy-cholinguistic features.
The ones that we felt weremost pertinent to our study are:1.
Concreteness - the level of abstraction associ-ated with the concept a word describes.2.
Imageability - the ability of a given word toarouse mental images.3.
Familiarity - the frequency of exposure to aword.4.
Age of Acquisition - the age at which a givenword is appropriated by a speaker.We combined both databases and compiled a sin-gle resource consisting of all the words from bothsources that list at least one of these features.
It maybe noted that these attributes were compiled in simi-lar fashion in both databases and were normalized tothe same scale of scores falling in the range of 100to 700.In spite of a combined compilation, the coverageof the resource was poor, with more than half thecandidate substitutes on both trial and test sets sim-ply not being listed in the databases.
To overcomethis difficulty we introduced a fifth frequency featurethat essentially simulates the ?Simple Frequency?baseline, 2 but with scores that were normalized tothe same scale of the other psycholinguistic features.This composite of features was used in a linearweighted function with weights tuned to best perfor-mance values on the trial dataset.
This function sumsthe weighted scores for each candidate, and normal-izes this sum by the number of non-zero features (inthe worst-case scenario, ?
when no psycholinguisticfeatures are found ?
the scorer is equivalent to the?Simple Frequency?
baseline).
It is interesting tonote that the frequency feature did not dominate thelinear combination; rather there was a nice interplayof features with Concreteness, Imageability, Famil-iarity, Age of Acquisition and Simple Frequency be-ing weighted (on a scale of -1 to +1) as 0.72, -0.22,0.87, 0.36 and 0.36, respectively.4.4 Feature CombinationWe combined the three standalone models usingthe ranking function of the SVM-light package(Joachims, 2006) for building SVM rankers.
The pa-rameters of the SVM were tuned on the trial dataset,which consisted of only 300 example contexts.
Toavoid overfitting, instead of taking the single bestparameters, we took parameter values that were theaverage of the top 10 distinct runs.It may be noted that the resulting model makes noattempt to tie candidates, although actual ties may beproduced by chance.
But since ties are rarely usedin the gold standard for the trial dataset, we reasonedthat this should not affect the system performance inany significant way.2The ?Simple Frequency?
baseline scores each substitutebased on the number of hits it produces in the Google Web 1T479bline-SFreq w-ln n-syll psycho a-n-gram b-o-w pmi lsa SimpLexTrial 0.398 0.176 0.118 0.388 0.397 0.395 0.340 0.089 ?Test 0.471 0.236 0.163 0.432 0.460 0.460 0.404 0.054 0.496Table 1: Comparison of Models?
Scores5 Results and DiscussionThe results of the SimpLex system trained and tunedon the trial set, in comparison with the Simple Fre-quency baseline and the other stand-alone featureswe experimented with are presented in Table 1.
Thescores are computed through a version of the Kappaindex over pairwise rankings, and therefore repre-sent the average agreement between the system andthe gold-standard annotation in the ranking of pairsof candidate substitutes.Table 1 shows that while in isolation the featuresare unable to beat the Simple Frequency model, to-gether they form a combination which outperformsthe baseline.
The improvement of SimpLex overthe other models is statistically significant (statisti-cal significance was established using a randomiza-tion test with 1000 iterations and p-value ?
0.05).We believe that the reason why the context awarefeatures were still unable to score better than thecontext-independent baseline is the isolated focuson simplifying a single target word.
People tendto produce language that contains words of roughlyequal levels of complexity.
Hence in some casesthe surrounding context, instead of helping to dis-ambiguate the target word, introduces further noiseto queries, especially when its individual componentwords have skewed complexity factors.
A simul-taneous simplification of all the content words in acontext could be a possible solution to this problem.As an additional experiment to assess the impor-tance of the size of the training data in our simplifi-cation system, we pooled together the trial and testdatasets, and ran several iterations of the combina-tion algorithm with a regular increment of number oftraining examples and noted the effects it producedon eventual score.
Three hundred examples were ap-portioned consistently to a test set to maintain com-parability between experiments.
Note that this time,no optimization of the SVM parameters was made.The results were inconclusive, and contrary to ex-pectation, revealed that there is no general improve-ment with additional training data.
This could bebecause of the difficulty of the learning problem, forwhich the scope of the combined dataset is still verylimited.
A more detailed study with a corpus that isorders of magnitude larger than the current one maybe necessary to establish conclusive evidence.6 ConclusionThis paper presented our system SimpLex whichparticipated in the English Lexical Simplificationshared-task at SemEval-2012 and ranked first out of9 participating systems.Our findings showed that while a context agnosticfrequency approach to lexical simplification seemsto effectively model the problem of assessing wordcomplexity to a relatively decent level of accuracy,as evidenced by the strong baseline of the sharedtask, other elements, such as interplay of contextawareness with humanly perceived psycholinguisticfeatures can produce better results, in spite of verylimited training data.Finally, a more global approach to lexical sim-plification that concurrently addresses all the wordsin a context to normalize simplicity levels, may bea more realistic proposition for target applications,and also help context aware features perform better.AcknowledgmentThis work was supported by the European Com-mission, Education & Training, Erasmus Mundus:EMMC 2008-0083, Erasmus Mundus Masters inNLP & HLT program.ReferencesSandra M. Aluisio, Lucia Specia, Thiago A.S. Pardo, Er-ick G. Maziero, and Renata P.M. Fortes.
2008.
To-wards brazilian portuguese automatic text simplifica-tion systems.
In Proceeding of the eighth ACM sym-480posium on Document engineering, DocEng ?08, pages240?248, Sao Paulo, Brazil.
ACM.Or Biran, Samuel Brody, and Noemie Elhadad.
2011.Putting it simply: a context-aware approach to lexi-cal simplification.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Linguis-tics: Human Language Technologies, pages 496?501,Portland, Oregon, USA, June.
Association for Compu-tational Linguistics.Thorsten Brants and Alex Franz.
2006.
The google web1t 5-gram corpus version 1.1 ldc2006t13.John Carroll, Guido Minnen, Yvonne Canning, SiobhanDevlin, and John Tait.
1998.
Practical simplificationof english newspaper text to assist aphasic readers.
InProceedings of AAAI - 98 Workshop on IntegratingArtificial Intelligence and Assistive Technology, Madi-son, Wisconsin, July.Jan De Belder and Marie-Francine Moens.
2010.
Textsimplification for children.
In Proceedings of the SI-GIR workshop on Accessible Search Systems, pages19?26.
ACM, July.S.
Hassan, A. Csomai, C. Banea, R. Sinha, and R. Mi-halcea.
2007.
Unt: Subfinder: Combining knowledgesources for automatic lexical substitution.
In Proceed-ings of the Fourth International Workshop on SemanticEvaluations (SemEval-2007), pages 410 ?
413.
Asso-ciation for Computational Linguistics, Prague, CzechRepublic.Thorsten Joachims.
2006.
Training linear svms in lin-ear time.
In Proceedings of the 12th ACM SIGKDDinternational conference on Knowledge discovery anddata mining, KDD ?06, pages 217?226, New York,NY, USA.
ACM.Diana McCarthy and Roberto Navigli.
2007.
Semeval-2007 task 10: English lexical substitution task.
In Pro-ceedings of the 4th International Workshop on Seman-tic Evaluations (SemEval-2007), Prague, Czech Re-public, pages 48?53.Lucia Specia, Sujay Kumar Jauhar, and Rada Mihalcea.2012.
Semeval-2012 task 1: English lexical simplifi-cation.
In Proceedings of the 6th International Work-shop on Semantic Evaluation (SemEval 2012), Mon-treal, Canada.Hans Stadthagen-Gonzalez and Colin Davis.
2006.
Thebristol norms for age of acquisition, imageability, andfamiliarity.
Behavior Research Methods, 38:598?605.Michael Wilson.
1988.
Mrc psycholinguistic database:Machine-usable dictionary, version 2.00.
BehaviorResearch Methods, 20:6?10.Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-Mizil, and Lillian Lee.
2010.
For the sake of simplic-ity: unsupervised extraction of lexical simplificationsfrom wikipedia.
In Human Language Technologies:The 2010 Annual Conference of the North AmericanChapter of the Association for Computational Linguis-tics, HLT ?10, pages 365?368, Stroudsburg, PA, USA.Association for Computational Linguistics.481
