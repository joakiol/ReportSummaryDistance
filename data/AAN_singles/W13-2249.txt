Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 392?397,Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational LinguisticsDCU-Symantec at the WMT 2013 Quality Estimation Shared TaskRaphael Rubino?
?, Joachim Wagner?
?, Jennifer Foster?,Johann Roturier?
Rasoul Samad Zadeh Kaljahi??
and Fred Hollowood?
?NCLT, School of Computing, Dublin City University, Ireland?Center for Next Generation Localisation, Dublin, Ireland?Symantec Research Labs, Dublin, Ireland?
{rrubino, jwagner, jfoster}@computing.dcu.ie?
{johann roturier, fhollowood}@symantec.comAbstractWe describe the two systems submit-ted by the DCU-Symantec team to Task1.1.
of the WMT 2013 Shared Task onQuality Estimation for Machine Transla-tion.
Task 1.1 involve estimating post-editing effort for English-Spanish trans-lation pairs in the news domain.
Thetwo systems use a wide variety of fea-tures, of which the most effective are theword-alignment, n-gram frequency, lan-guage model, POS-tag-based and pseudo-references ones.
Both systems perform ata similarly high level in the two tasks ofscoring and ranking translations, althoughthere is some evidence that the systems areover-fitting to the training data.1 IntroductionThe WMT 2013 Quality Estimation Shared Taskinvolve both sentence-level and word-level qual-ity estimation (QE).
The sentence-level task con-sist of three subtasks: scoring and ranking transla-tions with regard to post-editing effort (Task 1.1),selecting among several translations produced bymultiple MT systems for the same source sentence(Task 1.2), and predicting post-editing time (Task1.3).
The DCU-Symantec team enter two systemsto Task 1.1.
Given a set of source English newssentences and their Spanish translations, the goalsare to predict the HTER score of each translationand to produce a ranking based on HTER for theset of translations.
A set of 2,254 sentence pairsare provided for training.On the ranking task, our system DCU-SYMCalltypes is second placed out of thirteen sys-tems and our system DCU-SYMC combine isranked fifth, according to the Delta Average met-ric.
According to the Spearman rank correlation,our systems are the joint-highest systems.
In thescoring task, the DCU-SYMC alltypes systemis placed sixth out of seventeen systems accord-ing to Mean Absolute Error (MAE) and third ac-cording to Root Mean Squared Error (RMSE).
TheDCU-SYMC combine system is placed fifth ac-cording to MAE and second according to RMSE.In this system description paper, we describe thefeatures, the learning methods used, the results forthe two submitted systems and some other systemswe experiment with.2 FeaturesOur starting point for the WMT13 QE shared taskwas the feature set used in the system we submit-ted to the WMT12 QE task (Rubino et al 2012).This feature set, comprising 308 features in to-tal, extended the 17 baseline features provided bythe task organisers to include 6 additional sur-face features, 6 additional language model fea-tures, 17 additional features derived from theMT system components and the n-best lists, 138features obtained by part-of-speech tagging andparsing the source sentences and 95 obtained bypart-of-speech tagging the target sentences, 21topic model features, 2 features produced by agrammar checker1 and 6 pseudo-source (or back-translation) features.We made the following modifications to this2012 feature set:?
The pseudo-source (or back-translation) fea-tures were removed, as they did not con-tribute useful information to our system lastyear.?
The language model and n-gram frequencyfeature sets were extended in order to cover1 to 5 gram sequences, as well as source andtarget ratios for these feature values.?
The word-alignment feature set was alsoextended by considering several thresholds1http://www.languagetool.org/392when counting the number of target wordsaligned with source words.?
We extracted 8 additional features from thedecoder log file, including the number of dis-carded hypotheses, the total number of trans-lation options and the number of nodes in thedecoding graph.?
The set of topic model features was reducedin order to keep only those that were shownto be effective on three quality estimationdatasets (the details can be found in (Rubinoet al(to appear), 2013)).
These features en-code the difference between source and targettopic distributions according to several dis-tance/divergence metrics.?
Following Soricut et al(2012), we employedpseudo-reference features.
The source sen-tences were translated with three differentMT systems: an in-house phrase-based SMTsystem built using Moses (Koehn et al2007) and trained on the parallel data pro-vided by the organisers, the rule-based sys-tem Systran2 and the online, publicly avail-able, Bing Translator3.
The obtained trans-lations are compared to the target sentencesusing sentence-level BLEU (Papineni et al2002), TER (Snover et al 2006) and the Lev-enshtein distance (Levenshtein, 1966).?
Also following Soricut et al(2012), one-to-one word-alignments, with and withoutPart-Of-Speech (POS) agreement, were in-cluded as features.
Using the alignment in-formation provided by the decoder, we POStagged the source and target sentences withTreeTagger (Schmidt, 1994) and the publiclyavailable pre-trained models for English andSpanish.
We mapped the tagsets of both lan-guages by simplifying the initial tags and ob-tain a reduced set of 8 tags.
We applied thatsimplification on the tagged sentences beforechecking for POS agreement.3 Machine LearningIn this section, we describe the learning algo-rithms and feature selection used in our experi-ments, leading to the two submitted systems forthe shared task.2Systran Enterprise Server version 63http://www.bing.com/translator3.1 Primary Learning MethodTo estimate the post-editing effort of translatedsentences, we rely on regression models built us-ing the Support Vector Machine (SVM) algorithmfor regression -SVR, implemented in the LIB-SVM toolkit (Chang and Lin, 2011).
To buildour final regression models, we optimise SVMhyper-parameters (C, ?
and ) using a grid-searchmethod with 5-fold cross-validation for each pa-rameter triplet.
The parameters leading to the bestMAE, RMSE and Pearson?s correlation coefficient(r) are kept to build the model.3.2 Feature Selection on Feature TypesIn order to reduce the feature and obtain morecompact models, we apply feature selection oneach of our 15 feature types.
Examples of featuretypes are language model features or topic modelfeatures.
For each feature type, we apply a featuresubset evaluation method based on the wrapperparadigm and using the best-first search algorithmto explore the feature space.
The M5P (Wangand Witten, 1997) regression tree algorithm im-plemented in the Weka toolkit (Hall et al 2009)is used with default parameters to train and eval-uate a regression model for each feature subsetobtained with best-first search.
A 10-fold cross-validation is performed for each subset and wekeep the features leading to the best RMSE.
Weuse M5P regression trees instead of -SVR be-cause grid-search with the latter is too computa-tionally expensive to be applied so many times.Using feature selection in this way, we obtain 15reduced feature sets that we combine to form theDCU-SYMC alltypes system, containing 102features detailed in Table 1.3.3 Feature BinarisationIn order to aid the SVM learner, we also experi-ment with binarising our feature set, i.e.
convert-ing our features with various feature value rangesinto features whose values are either 1 or 0.
Again,we employ regression tree learning.
We trainregression trees with M5P and M5P-R4 (imple-mented in the Weka toolkit) and create a binaryfeature for each regression rule found in the trees(ignoring the leaf nodes).
For example, a binaryfeature indicating whether the Bing TER score isless than or equal to 55.685 is derived from the4We experiment with J48 decision trees as well, but thismethod did not outperform regression tree methods.393Backward LMSource 1-gram perplexity.Source & target 1-grams perplexity ratio.Source & target 3-grams and 4-gram perplexity ratio.Target SyntaxFrequency of tags: ADV, FS, DM, VLinf, VMinf, semicolon, VLger, NC, PDEL, VEfin, CC, CCNEG, PPx, ART, SYM,CODE, PREP, SE and number of ambiguous tagsFrequency of least frequent POS 3-gram observed in a corpus.Frequency of least frequent POS 4-gram and 6-gram with sentence padding (start and end of sentence tags) observed in acorpus.Source SyntaxFeatures from three probabilistic parsers.
(Rubino et al 2012).Frequency of least frequent POS 2-gram, 4-gram and 9-gram with sentence padding observed in a corpus.Number of analyses found and number of words, using a Lexical Functional Grammar of English as described in Rubinoet al(2012).LMSource unigram perplexity.Target 3-gram and 4-gram perplexity with sentence padding.Source & target 1-gram and 5-gram perplexity ratio.Source & target unigram log-probability.DecoderComponent scores during decoding.Number of phrases in the best translation.Number of translation options.N -gram FrequencyTarget 2-gram in second and third frequency quartiles.Target 3-gram and 5-gram in low frequency quartiles.Number of target 1-gram seen in a corpus.Source & target 1-grams in highest and second highest frequency quartile.One-to-One Word-AlignmentCount of O2O word alignment, weighted by target sentence length.Count of O2O word alignment with POS agreement, weighted by count of O2O, by source length, by target length.Pseudo-ReferenceMoses translation TER score.Bing translation number of words and TER score.Systran sBLEU, number of substitutions and TER score.SurfaceSource number of punctuation marks and average words occurrence in source sentence.Target number of punctuation marks, uppercased letters and binary value if the last character of the sentence is a punctuationmark.Ratio of source and target sentence lengths, average word length and number of punctuation marks over sentence lengths.Topic ModelCosine distance between source and target topic distributions.Jensen-Shannon divergence between source and target topic distributions.Word AlignmentAveraged number of source words aligned per target words with p(s|t) thresholds: 1.0, 0.75, 0.5, 0.25, 0.01Averaged number of source words aligned per target words with p(s|t) = 0.01 weighted by target words frequencyAveraged number of target words aligned per source word with p(t|s) = 0.01 weighted by source words frequencyRatio of source and target averaged aligned words with thresholds: 1.0 and 0.1, and with threshold: 0.75, 0.5, 0.25 weightedby words frequencyTable 1: Features selected with the wrapper approach using best-first search and M5P.
These features areincluded in the submitted system alltypes.394Feature to which threshold t is applied t (?
)Target 1-gram backward LM log-prob.
?35.973Target 3-gram backward LM perplexity 7144.99Probabilistic parsing feature 3.756Probabilistic parsing feature 57.5Frequency of least frequent POS 6-gram 0.5Source 3-gram LM log-prob.
65.286Source 4-gram LM perplexity with padding 306.362Target 2-gram LM perplexity 176.431Target 4-gram LM perplexity 426.023Target 4-gram LM perplexity with padding 341.801Target 5-gram LM perplexity 112.908Ratio src&trg 5-gram LM log-prob.
1.186MT system component score ?50MT system component score ?0.801Source 2-gram frequency in low quartile 0.146Ratio src&trg 2-gram in high freq.
quartile 0.818Ratio src&trg 3-gram in high freq.
quartile 0.482O2O word alignment 15.5Pseudo-ref.
Moses Levenshtein 19Pseudo-ref.
Moses TER 21.286Pseudo-ref.
Bing TER 16.905Pseudo-ref.
Bing TER 23.431Pseudo-ref.
Bing TER 37.394Pseudo-ref.
Bing TER 55.685Pseudo-ref.
Systran sBLEU 0.334Pseudo-ref.
Systran TER 36.399Source average word length 4.298Target uppercased/lowercased letters ratio 0.011Ratio src&trg average word length 1.051Source word align., p(s|t) > 0.75 11.374Source word align., p(s|t) > 0.1 485.062Source word align., p(s|t) > 0.75 weighted 0.002Target word align., p(t|s) > 0.01 weighted 0.019Word align.
ratio p > 0.25 weighted 1.32Table 2: Features selected with the M5P-R M50binarisation approach.
For each feature, the cor-responding rule indicates the binary feature value.These features are included in the submitted sys-tem combine in addition to the features presentedin Table 1.regression rule Bing TER score ?
55.685.The primary motivation for using regressiontree learning in this way was to provide a quickand convenient method for binarising our featureset.
However, we can also perform feature selec-tion using this method by experimenting with vari-ous minimum leaf sizes (Weka parameter M ).
Weplot the performance of the M5P and M5P-R (opti-mising towards correlation) over the parameter Mand select the best three values of M .
To experi-ment with the effect of smaller and larger featuresets, we further include parameters of M that (a)lead to an approximately 50% bigger feature setand (b) to an approximately 50% smaller featureset.Our DCU-SYMC combine system was builtby combining the DCU-SYMC alltypes fea-ture set, reduced using the best-first M5P wrap-per approach as described in subsection 3.2, witha binarised set produced using an M5P regres-sion tree with a minimum of 50 nodes per leaf.This latter configuration, containing 34 featuresdetailed in Table 2, was selected according to theevaluation scores obtained during cross-validationon the training set using -SVR, as described inthe next section.
Finally, we run a greedy back-ward feature selection algorithm wrapping -SVRon both DCU-SYMC alltypes and DCU-SYMCcombine in order to optimise our feature sets forthe SVR learning algorithm, removing 6 and 2 fea-tures respectively.4 System Evaluation and ResultsIn this section, we present the results obtained with-SVR during 5-fold cross-validation on the train-ing set and the final results obtained on the testset.
We selected two systems to submit amongstthe different configurations based on MAE, RMSEand r. As several systems reach the same perfor-mance according to these metrics, we use the num-ber of support vectors (noted SV) as an indicatorof training data over-fitting.
We report the resultsobtained with some of our systems in Table 3.The results show that the submitted sys-tems DCU-SYMC alltypes and DCU-SYMCcombine lead to the best scores on cross-validation, but they do not outperform the systemcombining the 15 feature types without feature se-lection (15 types).
This system reaches the bestscores on the test set compared to all our systemsbuilt on reduced feature sets.
This indicates thatwe over-fit and fail to generalise from the trainingdata.Amongst the systems built using reduced fea-ture sets, the M5P-R M80 system, based on thetree binarisation approach using M5P-R, yieldsthe best results on the test set on 3 out of 4 offi-cial metrics.
These results indicate that this sys-tem, trained on 16 features only, tends to estimateHTER scores more accurately on the unseen testdata.
The results of the two systems based onthe M5P-R binarisation method are the best com-pared to all the other systems presented in thisSection.
This feature binarisation and selectionmethod leads to robust systems with few features:31 and 16 for M5P-R M50 and M5P-R M80 re-spectively.
Even though these systems do not leadto the best results, they outperform the two sub-mitted systems on one metric used to evaluate the395Cross-Validation TestSystem nb feat MAE RMSE r SV MAE RMSE DeltaAvg Spearman15 types 442 0.106 0.138 0.604 1194.6 0.126 0.156 0.108 0.625M5P M50 34 0.106 0.138 0.600 1417.8 0.135 0.167 0.102 0.586M5P M130 4 0.114 0.145 0.544 750.6 0.142 0.173 0.079 0.517M5P-R M50 31 0.106 0.137 0.610 655.4 0.135 0.166 0.100 0.591M5P-R M80 16 0.107 0.139 0.597 570.6 0.134 0.165 0.106 0.597alltypes?
96 0.104 0.135 0.624 1130.6 0.135 0.171 0.101 0.589combine?
134 0.104 0.134 0.629 689.8 0.134 0.166 0.098 0.588Table 3: Results obtained with different regression models, during cross-validation on the training setand on the test set, depending on the feature selection method.
Systems marked with ?
were submittedfor the shared task.scoring task and two metrics to evaluate the rank-ing task.On the systems built using reduced feature sets,we observe a difference of approximately 0.03ptabsolute between the MAE and RMSE scores ob-tained during cross-validation and those on the testset.
Such a difference can be related to train-ing data over-fitting, even though the feature setsobtained with the tree binarisation methods aresmall.
For instance, the system M5P M130 istrained on 4 features only, but the difference be-tween cross-validation and test MAE scores issimilar to the other systems.
We see on the fi-nal results that our feature selection methods is anover-fitting factor: by selecting the features whichexplain well the training set, the final model tendsto generalise less.
The selected features are suitedfor the specificities of the training data, but are lessaccurate at predicting values on the unseen test set.5 DiscussionTraining data over-fitting is clearly shown by theresults presented in Table 3, indicated by the per-formance drop between results obtained duringcross-validation and the ones obtained on the testset.
While this drop may be related to data over-fitting, it may also be related to the use of differ-ent machine learning methods for feature selec-tion (M5P and M5P-R) and for building the fi-nal regression models (-SVR).
In order to ver-ify this aspect, we build two regression modelsusing M5P, based on the feature sets alltypesand combine.
Results are presented in Table 4and show that, for the alltypes feature set, theRMSE, DeltaAvg and Spearman scores are im-proved using M5P compared to SVM.
For thecombine feature set, the scoring results (MAEand RMSE) are better using SVM, while the rank-ing results are similar for both machine learningmethods.The performance drop between the results onthe training data (or a development set) and thetest data was also observed by the highest rankedparticipants in the WMT12 QE shared task.
Tocompare our system without feature selection tothe winner of the previous shared task, we eval-uate the 15 types system in Table 3 using theWMT12 QE dataset.
The results are presented inTable 5.
We can see that similar MAEs are ob-tained with our feature set and the WMT12 QEwinner, whereas our system gets a higher RMSE(+0.01).
For the ranking scores, our system isworse using the DeltaAvg metric while it is bet-ter on Spearman coefficient.6 ConclusionWe presented in this paper our experiments for theWMT13 Quality Estimation shared task.
Our ap-proach is based on the extraction of a large ini-tial feature set, followed by two feature selectionmethods.
The first one is a wrapper approach us-ing M5P and a best-first search algorithm, whilethe second one is a feature binarisation approachusing M5P and M5P-R.
The final regression mod-els were built using -SVR and we selected twosystems to submit based on cross-validation re-sults.We observed that our system reaching the bestscores on the test set was not a system trained ona reduced feature set and it did not yield the bestcross-validation results.
This system was trainedusing 442 features, which are the combination of15 different feature types.
Amongst the systemsbuilt on reduced sets, the best results are obtained396System nb feat MAE RMSE DeltaAvg Spearmanalltypes 96 0.135 0.165 0.104 0.604combine 134 0.139 0.169 0.098 0.587Table 4: Results obtained with the two feature sets contained in our submitted systems using M5P tobuild the regression models instead of -SVR.System nb feat MAE RMSE DeltaAvg SpearmanWMT12 winner 15 0.61 0.75 0.63 0.6415 types 442 0.61 0.76 0.60 0.65Table 5: Results obtained on WMT12 QE dataset with our best system (15 types) compared to WMT12QE highest ranked team, in the Likert score prediction task.using the feature binarisation approach M5P-R80, which contains 16 features selected from ourinitial set of features.
The tree-based feature bina-risation is a fast and flexible method which allowsus to vary the number of features by optimising theleaf size and leads to acceptable results with a fewselected features.Future work involves a deeper analysis of theover-fitting effect and an investigation of othermethods in order to outperform the non-reducedfeature set.
We are also interested in finding a ro-bust way to optimise the leaf size parameter forour tree-based feature binarisation method, with-out using cross-validation on the training set withan SVM algorithm.AcknowledgementsThe research reported in this paper has beensupported by the Research Ireland EnterprisePartnership Scheme (EPSPG/2011/102 and EP-SPD/2011/135) and Science Foundation Ireland(Grant 12/CE/I2267) as part of the Centre forNext Generation Localisation (www.cngl.ie)at Dublin City University.ReferencesChih-Chung Chang and Chih-Jen Lin.
2011.LIBSVM: A Library for Support Vector Ma-chines.
ACM Transactions on Intelligent Sys-tems and Technology, 2:27:1?27:27.
Soft-ware available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H Witten.2009.
The WEKA Data Mining Software: anUpdate.
ACM SIGKDD Explorations Newsletter,11(1):10?18.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, et al2007.
Moses: Open sourcetoolkit for statistical machine translation.
In ACL,pages 177?180.Vladimir I Levenshtein.
1966.
Binary codes capableof correcting deletions, insertions and reversals.
InSoviet physics doklady, volume 10, page 707.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In ACL, pages 311?318.Raphael Rubino, Jennifer Foster, Joachim Wagner, Jo-hann Roturier, Rasul Samad Zadeh Kaljahi, and FredHollowood.
2012.
DCU-Symantec Submission forthe WMT 2012 Quality Estimation Task.
In Pro-ceedings of the Seventh WMT, pages 138?144.Raphael Rubino et al(to appear).
2013.
Topic Modelsfor Translation Quality Estimation for Gisting Pur-poses.
In Proceeding of MT Summit XIV.Helmut Schmidt.
1994.
Probabilistic part-of-speechtagging using decision trees.
In Proceedings of theInternational Conference on New Methods in Natu-ral Language Processing.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In AMTA, pages 223?231.Radu Soricut, Nguyen Bach, and Ziyuan Wang.
2012.The SDL Language Weaver Systems in the WMT12Quality Estimation Shared Task.
In Proceedings ofthe Seventh WMT, pages 145?151.Yong Wang and Ian H Witten.
1997.
Inducing ModelTrees for Continuous Classes.
In Proceedings ofECML, pages 128?137.
Prague, Czech Republic.397
