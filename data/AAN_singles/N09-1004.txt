Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 28?36,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsA Fully Unsupervised Word Sense Disambiguation Method UsingDependency KnowledgePing ChenDept.
of Computer and Math.
SciencesUniversity of Houston-Downtownchenp@uhd.eduChris BowesDept.
of Computer and Math.
SciencesUniversity of Houston-Downtownbowesc@uhd.eduWei DingDepartment of Computer ScienceUniversity of Massachusetts-Bostonding@cs.umb.eduDavid BrownDept.
of Computer and Math.
SciencesUniversity of Houston-Downtownbrownd@uhd.eduAbstractWord sense disambiguation is the process ofdetermining which sense of a word is usedin a given context.
Due to its importance inunderstanding semantics of natural languages,word sense disambiguation has been exten-sively studied in Computational Linguistics.However, existing methods either are brit-tle and narrowly focus on specific topics orwords, or provide only mediocre performancein real-world settings.
Broad coverage anddisambiguation quality are critical for a wordsense disambiguation system.
In this paper wepresent a fully unsupervised word sense dis-ambiguation method that requires only a dic-tionary and unannotated text as input.
Such anautomatic approach overcomes the problem ofbrittleness suffered in many existing methodsand makes broad-coverage word sense dis-ambiguation feasible in practice.
We evalu-ated our approach using SemEval 2007 Task7 (Coarse-grained English All-words Task),and our system significantly outperformed thebest unsupervised system participating in Se-mEval 2007 and achieved the performance ap-proaching top-performing supervised systems.Although our method was only tested withcoarse-grained sense disambiguation, it can bedirectly applied to fine-grained sense disam-biguation.1 IntroductionIn many natural languages, a word can representmultiple meanings/senses, and such a word is calleda homograph.
Word sense disambiguation(WSD)is the process of determining which sense of a ho-mograph is used in a given context.
WSD is along-standing problem in Computational Linguis-tics, and has significant impact in many real-worldapplications including machine translation, informa-tion extraction, and information retrieval.
Gener-ally, WSD methods use the context of a word forits sense disambiguation, and the context informa-tion can come from either annotated/unannotatedtext or other knowledge resources, such as Word-Net (Fellbaum, 1998), SemCor (SemCor, 2008),Open Mind Word Expert (Chklovski and Mihal-cea, 2002), eXtended WordNet (Moldovan and Rus,2001), Wikipedia (Mihalcea, 2007), parallel corpora(Ng, Wang, and Chan, 2003).
In (Ide and Ve?ronis,1998) many different WSD approaches were de-scribed.
Usually, WSD techniques can be dividedinto four categories (Agirre and Edmonds, 2006),?
Dictionary and knowledge based methods.These methods use lexical knowledge basessuch as dictionaries and thesauri, and hypoth-esize that context knowledge can be extractedfrom definitions of words.
For example, Leskdisambiguated two words by finding the pair ofsenses with the greatest word overlap in theirdictionary definitions (Lesk, 1986).?
Supervised methods.
Supervised methodsmainly adopt context to disambiguate words.A supervised method includes a training phaseand a testing phase.
In the training phase,a sense-annotated training corpus is required,from which syntactic and semantic features areextracted to create a classifier using machine28learning techniques, such as Support VectorMachine (Novischi et al, 2007).
In the fol-lowing testing phase, a word is classified intosenses (Mihalcea, 2002) (Ng and Lee, 1996).Currently supervised methods achieve the bestdisambiguation quality (about 80% precisionand recall for coarse-grained WSD in the mostrecent WSD evaluation conference SemEval2007 (Navigli et al, 2007)).
Nevertheless,since training corpora are manually annotatedand expensive, supervised methods are oftenbrittle due to data scarcity, and it is hard to an-notate and acquire sufficient contextual infor-mation for every sense of a large number ofwords existing in natural languages.?
Semi-supervised methods.
To overcome theknowledge acquisition bottleneck problem suf-fered by supervised methods, these methodsmake use of a small annotated corpus as seeddata in a bootstrapping process (Hearst, 1991)(Yarowsky, 1995).
A word-aligned bilingualcorpus can also serve as seed data (Ng, Wang,and Chan, 2003).?
Unsupervised methods.
These methods acquirecontextual information directly from unanno-tated raw text, and senses can be induced fromtext using some similarity measure (Lin, 1997).However, automatically acquired informationis often noisy or even erroneous.
In the mostrecent SemEval 2007 (Navigli et al, 2007), thebest unsupervised systems only achieved about70% precision and 50% recall.Disambiguation of a limited number of words isnot hard, and necessary context information can becarefully collected and hand-crafted to achieve highdisambiguation accuracy as shown in (Yarowsky,1995).
However, such approaches suffer a signifi-cant performance drop in practice when domain orvocabulary is not limited.
Such a ?cliff-style?
per-formance collapse is called brittleness, which is dueto insufficient knowledge and shared by many tech-niques in Artificial Intelligence.
The main challengeof a WSD system is how to overcome the knowl-edge acquisition bottleneck and efficiently collectthe huge amount of context knowledge.
More pre-cisely, a practical WSD need figure out how to createand maintain a comprehensive, dynamic, and up-to-date context knowledge base in a highly automaticmanner.
The context knowledge required in WSDhas the following properties:1.
The context knowledge need cover a largenumber of words and their usage.
Such arequirement of broad coverage is not trivialbecause a natural language usually containsthousands of words, and some popular wordscan have dozens of senses.
For example, theOxford English Dictionary has approximately301,100 main entries (Oxford, 2003), and theaverage polysemy of the WordNet inventory is6.18 (Fellbaum, 1998).
Clearly acquisition ofsuch a huge amount of knowledge can only beachieved with automatic techniques.2.
Natural language is not a static phenomenon.New usage of existing words emerges, whichcreates new senses.
New words are created,and some words may ?die?
over time.
It is esti-mated that every year around 2,500 new wordsappear in English (Kister, 1992).
Such dynam-ics requires a timely maintenance and updatingof context knowledge base, which makes man-ual collection even more impractical.Taking into consideration the large amount anddynamic nature of context knowledge, we only havelimited options when choosing knowledge sourcesfor WSD.
WSD is often an unconscious process tohuman beings.
With a dictionary and sample sen-tences/phrases an average educated person can cor-rectly disambiguate most polysemous words.
In-spired by human WSD process, we choose an elec-tronic dictionary and unannotated text samples ofword instances as context knowledge sources forour WSD system.
Both sources can be automat-ically accessed, provide an excellent coverage ofword meanings and usage, and are actively updatedto reflect the current state of languages.
In this pa-per we present a fully unsupervised WSD system,which only requires WordNet sense inventory andunannotated text.
In the rest of this paper, section2 describes how to acquire and represent the con-text knowledge for WSD.
We present our WSD al-gorithm in section 3.
Our WSD system is evaluatedwith SemEval-2007 Task 7 (Coarse-grained English29Figure 1: Context Knowledge Acquisition and Represen-tation ProcessAll-words Task) data set, and the experiment resultsare discussed in section 4.
We conclude in section 5.2 Context Knowledge Acquisition andRepresentationFigure 1 shows an overview of our context knowl-edge acquisition process, and collected knowledgeis saved in a local knowledge base.
Here are somedetails about each step.2.1 Corpus building through Web searchThe goal of this step is to collect as many as possi-ble valid sample sentences containing the instancesof to-be-disambiguated words.
Preferably these in-stances are also diverse and cover many senses of aword.
We have considered two possible text sources,1.
Electronic text collection, e.g., Gutenbergproject (Gutenberg, 1971).
Such collections of-ten include thousands of books, which are oftenwritten by professionals and can provide manyvalid and accurate usage of a large number ofwords.
Nevertheless, books in these collectionsare usually copyright-free and old, hence arelack of new words or new senses of words usedin modern English.2.
Web documents.
Billions of documents existin the World Wide Web, and millions of Webpages are created and updated everyday.
Such ahuge dynamic text collection is an ideal sourceto provide broad and up-to-date context knowl-edge for WSD.
The major concern about Webdocuments is inconsistency of their quality, andmany Web pages are spam or contain erroneousinformation.
However, factual errors in Webpages will not hurt the performance of WSD.Nevertheless, the quality of context knowledgeis affected by broken sentences of poor linguis-tic quality and invalid word usage, e.g., sen-tences like ?Colorless green ideas sleep furi-ously?
that violate commonsense knowledge.Based on our experience these kind of errorsare negligible when using popular Web searchengines to retrieve relevant Web pages.To start the acquisition process, words that needto be disambiguated are compiled and saved in atext file.
Each single word is submitted to a Websearch engine as a query.
Several search enginesprovide API?s for research communities to auto-matically retrieve large number of Web pages.
Inour experiments we used both Google and Yahoo!API?s to retrieve up to 1,000 Web pages for each to-be-disambiguated word.
Collected Web pages arecleaned first, e.g., control characters and HTML tagsare removed.
Then sentences are segmented simplybased on punctuation (e.g., ?, !, .).
Sentences thatcontain the instances of a specific word are extractedand saved into a local repository.2.2 ParsingSentences organized according to each word aresent to a dependency parser, Minipar.
Dependencyparsers have been widely used in ComputationalLinguistics and natural language processing.
Anevaluation with the SUSANNE corpus shows thatMinipar achieves 89% precision with respect to de-pendency relations (Lin, 1998).
After parsing sen-tences are converted to parsing trees and saved infiles.
Neither our simple sentence segmentation ap-proach nor Minipar parsing is 100% accurate, so asmall number of invalid dependency relations mayexist in parsing trees.
The impact of these erroneousrelations will be minimized in our WSD algorithm.Comparing with tagging or chunking, parsing is rel-atively expensive and time-consuming.
However, inour method parsing is not performed in real timewhen we disambiguate words.
Instead, sentences30Figure 2: Merging two parsing trees.
The number besideeach edge is the number of occurrences of this depen-dency relation existing in the context knowledge base.are parsed only once to extract dependency relations,then these relations are merged and saved in a localknowledge base for the following disambiguation.Hence, parsing will not affect the speed of disam-biguation at all.2.3 Merging dependency relationsAfter parsing, dependency relations from differentsentences are merged and saved in a context knowl-edge base.
The merging process is straightforward.A dependency relation includes one head word/nodeand one dependent word/node.
Nodes from differentdependency relations are merged into one as long asthey represent the same word.
An example is shownin Figure 2, which merges the following two sen-tences:?Computer programmers write software.?
?Many companies hire computer programmers.
?In a dependency relation ?word1 ?
word2?,word1 is the head word, and word2 is the depen-dent word.
After merging dependency relations, wewill obtain a weighted directed graph with a wordas a node, a dependency relation as an edge, andthe number of occurrences of dependency relation asweight of an edge.
This weight indicates the strengthof semantic relevancy of head word and dependentword.
This graph will be used in the following WSDFigure 3: WSD Procedureprocess as our context knowledge base.
As a fullyautomatic knowledge acquisition process, it is in-evitable to include erroneous dependency relationsin the knowledge base.
However, since in a large textcollection valid dependency relations tend to repeatfar more times than invalid ones, these erroneousedges only have minimal impact on the disambigua-tion quality as shown in our evaluation results.3 WSD AlgorithmOur WSD approach is based on the following in-sight:If a word is semantically coherent with its context,then at least one sense of this word is semanticallycoherent with its context.Assume that the text to be disambiguated is se-mantically valid, if we replace a word with itsglosses one by one, the correct sense should bethe one that will maximize the semantic coherencewithin this word?s context.
Based on this idea weset up our WSD procedure as shown in Figure 3.First both the original sentence that contains theto-be-disambiguated word and the glosses of to-be-disambiguated word are parsed.
Then the parsingtree generated from each gloss is matched with theparsing tree of original sentence one by one.
Thegloss most semantically coherent with the originalsentence will be chosen as the correct sense.
Howto measure the semantic coherence is critical.
Ouridea is based on the following hypotheses (assumeword1 is the to-be-disambiguated word):?
In a sentence if word1 is dependent on word2,and we denote the gloss of the correct sense ofword1 as g1i, then g1i contains the most se-mantically coherent words that are dependent31on word2;?
In a sentence if a set of words DEP1 are de-pendent on word1, and we denote the gloss ofthe correct sense of word1 as g1i, then g1i con-tains the most semantically coherent words thatDEP1 are dependent on.For example, we try to disambiguate ?company?in ?A large company hires many computer program-mers?, after parsing we obtain the dependency rela-tions ?hire ?
company?
and ?company ?
large?.The correct sense for the word ?company?
shouldbe ?an institution created to conduct business?.
Ifin the context knowledge base there exist the depen-dency relations ?hire ?
institution?
or ?institution?
large?, then we believe that the gloss ?an institu-tion created to conduct business?
is semantically co-herent with its context - the original sentence.
Thegloss with the highest semantic coherence will bechosen as the correct sense.
Obviously, the size ofcontext knowledge base has a positive impact on thedisambiguation quality, which is also verified in ourexperiments (see Section 4.2).
Figure 4 shows ourdetailed WSD algorithm.
Semantic coherence scoreis generated by the function TreeMatching, andwe adopt a sentence as the context of a word.We illustrate our WSD algorithm through an ex-ample.
Assume we try to disambiguate ?company?in the sentence ?A large software company hiresmany computer programmers?.
?company?
has 9senses as a noun in WordNet 2.1.
Let?s pick the fol-lowing two glosses to go through our WSD process.?
an institution created to conduct business?
small military unitFirst we parse the original sentence and twoglosses, and get three weighted parsing trees asshown in Figure 5.
All weights are assigned tonodes/words in these parsing trees.
In the parsingtree of the original sentence the weight of a node isreciprocal of the distance between this node and to-be-disambiguated node ?company?
(line 12 in Fig-ure 4).
In the parsing tree of a gloss the weightof a node is reciprocal of the level of this node inthe parsing tree (line 16 in Figure 4).
Assume thatour context knowledge base contains relevant depen-dency relations shown in Figure 6.Input: Glosses from WordNet;S: the sentence to be disambiguated;G: the knowledge base generated in Section 2;1.
Input a sentence S, W = {w| w?s part of speechis noun, verb, adjective, or adverb, w ?
S};2.
Parse S with a dependency parser, generateparsing tree TS ;3.
For each w ?W {4.
Input all w?s glosses from WordNet;5.
For each gloss wi {6.
Parse wi, get a parsing tree Twi;7. score = TreeMatching(TS , Twi);}8.
If the highest score is larger than a presetthreshold, choose the sense with thehighest score as the correct sense;9.
Otherwise, choose the first sense.10.
}TreeMatching(TS , Twi)11.
For each node nSi ?
TS {12.
Assign weight wSi = 1lSi , lSi is thelength between nSi and wi in TS ;13.
}14.
For each node nwi ?
Twi {15.
Load its dependent words Dwi from G;16.
Assign weight wwi = 1lwi , lwi is thelevel number of nwi in Twi;17.
For each nSj {18.
If nSj ?
Dwi19.
calculate connection strength sjibetween nSj and nwi;20. score = score + wSi ?
wwi ?
sji;21.
}22.
}23.
Return score;Figure 4: WSD AlgorithmThe weights in the context knowledge base are as-signed to dependency relation edges.
These weightsare normalized to [0, 1] based on the number of de-pendency relation instances obtained in the acquisi-tion and merging process.
A large number of occur-rences will be normalized to a high value (close to1), and a small number of occurrences will be nor-32Figure 5: Weighted parsing trees of the original sentenceand two glosses of ?company?Figure 6: A fragment of context knowledge basemalized to a low value (close to 0).Now we load the dependent words of each wordin gloss 1 from the knowledge base (line 14, 15 inFigure 4), and we get {small, large} for ?institu-tion?
and {large, software} for ?business?.
In thedependent words of ?company?, ?large?
belongs tothe dependent word sets of ?institution?
and ?busi-ness?, and ?software?
belongs to the dependent wordset of ?business?, so the coherence score of gloss 1is calculated as (line 19, 20 in Figure 4):1.0?1.0?0.7 + 1.0?0.25?0.8 + 1.0?0.25?0.9= 1.125We go through the same process with the secondgloss ?small military unit?.
?Large?
is the only de-pendent word of ?company?
appearing in the depen-dent word set of ?unit?
in gloss 2, so the coherencescore of gloss 2 in the current context is:1.0?
1.0?
0.8 = 0.8After comparing the coherence scores of twoglosses, we choose sense 1 of ?company?
as the cor-rect sense (line 9 in Figure 4).
This example illus-trates that a strong dependency relation between ahead word and a dependent word has a powerful dis-ambiguation capability, and disambiguation qualityis also significantly affected by the quality of dictio-nary definitions.In Figure 4 the TreeMatching function matchesthe dependent words of to-be-disambiguated word(line 15 in Figure 4), and we call this matching strat-egy as dependency matching.
This strategy will notwork if a to-be-disambiguated word has no depen-dent words at all, for example, when the word ?com-pany?
in ?Companies hire computer programmers?has no dependent words.
In this case, we developedthe second matching strategy, which is to match thehead words that the to-be-disambiguated word is de-pendent on, such as matching ?hire?
(the head wordof ?company?)
in Figure 5(a).
Using the dependencyrelation ?hire ?
company?, we can correctly choosesense 1 since there is no such relation as ?hire ?unit?
in the knowledge base.
This strategy is alsohelpful when disambiguating adjectives and adverbssince they usually only depend on other words, andrarely any other words are dependent on them.
Thethird matching strategy is to consider synonyms as amatch besides the exact matching words.
Synonymscan be obtained through the synsets in WordNet.For example, when we disambiguate ?company?
in?Big companies hire many computer programmers?,?big?
can be considered as a match for ?large?.
Wecall this matching strategy as synonym matching.The three matching strategies can be combined andapplied together, and in Section 4.1 we show theexperiment results of 5 different matching strategycombinations.334 ExperimentsWe have evaluated our method using SemEval-2007Task 07 (Coarse-grained English All-words Task)test set (Navigli et al, 2007).
The task organiz-ers provide a coarse-grained sense inventory cre-ated with SSI algorithm (Navigli and Velardi, 2005),training data, and test data.
Since our methoddoes not need any training or special tuning, neithercoarse-grained sense inventory nor training data wasused.
The test data includes: a news article about?homeless?
(including totally 951 words, 368 wordsare annotated and need to be disambiguated), a re-view of the book ?Feeding Frenzy?
(including to-tally 987 words, 379 words are annotated and needto be disambiguated), an article about some trav-eling experience in France (including totally 1311words, 500 words are annotated and need to be dis-ambiguated), computer programming(including to-tally 1326 words, 677 words are annotated and needto be disambiguated), and a biography of the painterMasaccio (including totally 802 words, 345 wordsare annotated and need to be disambiguated).
Twoauthors of (Navigli et al, 2007) independently andmanually annotated part of the test set (710 wordinstances), and the pairwise agreement was 93.80%.This inter-annotator agreement is usually consideredan upper-bound for WSD systems.We followed the WSD process described in Sec-tion 2 and 3 using the WordNet 2.1 sense repositorythat is adopted by SemEval-2007 Task 07.
All exper-iments were performed on a Pentium 2.33GHz dualcore PC with 3GB memory.
Among the 2269 to-be-disambiguated words in the five test documents,1112 words are unique and submitted to GoogleAPI as queries.
The retrieved Web pages werecleaned, and 1945189 relevant sentences were ex-tracted.
On average 1749 sentences were obtainedfor each word.
The Web page retrieval step took 3days, and the cleaning step took 2 days.
Parsing wasvery time-consuming and took 11 days.
The merg-ing step took 3 days.
Disambiguation of 2269 wordsin the 5 test articles took 4 hours.
All these steps canbe parallelized and run on multiple computers, andthe whole process will be shortened accordingly.The overall disambiguation results are shown inTable 1.
For comparison we also listed the re-sults of the top three systems and three unsuper-vised systems participating in SemEval-2007 Task07.
All of the top three systems (UoR-SSI, NUS-PT, NUS-ML) are supervised systems, which usedannotated resources (e.g., SemCor, Defense ScienceOrganization Corpus) during the training phase.
Ourfully unsupervised WSD system significantly out-performs the three unsupervised systems (SUSSZ-FR, SUSSX-C-WD, SUSSX-CR) and achieves per-formance approaching the top-performing super-vised WSD systems.4.1 Impact of different matching strategies todisambiguation qualityTo test the effectiveness of different matching strate-gies discussed in Section 3, we performed some ad-ditional experiments.
Table 2 shows the disambigua-tion results by each individual document with thefollowing 5 matching strategies:1.
Dependency matching only.2.
Dependency and backward matching.3.
Dependency and synonym backward matching.4.
Dependency and synonym dependency match-ing.5.
Dependency, backward, synonym backward,and synonym dependency matching.As expected combination of more matchingstrategies results in higher disambiguation quality.By analyzing the scoring details, we verified thatbackward matching is especially useful to disam-biguate adjectives and adverbs.
Adjectives and ad-verbs are often dependent words, so dependencymatching itself rarely finds any matched words.Since synonyms are semantically equivalent, it isreasonable that synonym matching can also improvedisambiguation performance.4.2 Impact of knowledge base size todisambiguation qualityTo test the impact of knowledge base size to dis-ambiguation quality we randomly selected 1339264sentences (about two thirds of all sentences) fromour text collection and built a smaller knowledgebase.
Table 3 shows the experiment results.
Overalldisambiguation quality has dropped slightly, which34System Attempted Precision Recall F1UoR-SSI 100.0 83.21 83.21 83.21NUS-PT 100.0 82.50 82.50 82.50NUS-ML 100.0 81.58 81.58 81.58TreeMatch 100.0 73.65 73.65 73.65SUSSZ-FR 72.8 71.73 52.23 60.44SUSSX-C-WD 72.8 54.54 39.71 45.96SUSSX-CR 72.8 54.30 39.53 45.75Table 1: Overall disambiguation scores (Our system ?TreeMatch?
is marked in bold)Matching d001 d002 d003 d004 d005 Overallstrategy P R P R P R P R P R P R1 72.28 72.28 66.23 66.23 63.20 63.20 66.47 66.47 56.52 56.52 65.14 65.142 70.65 70.65 70.98 70.98 65.20 65.20 72.23 72.23 58.84 58.84 68.18 68.183 79.89 79.89 75.20 75.20 69.00 69.00 71.94 71.94 64.64 64.64 72.01 72.014 80.71 80.71 78.10 78.10 72.80 72.80 71.05 71.05 67.54 67.54 73.65 73.655 80.16 80.16 78.10 78.10 69.40 69.40 72.82 72.82 66.09 66.09 73.12 73.12Table 2: Disambiguation scores by article with 5 matching strategiesshows a positive correlation between the amount ofcontext knowledge and disambiguation quality.
It isreasonable to assume that our disambiguation per-formance can be improved further by collecting andincorporating more context knowledge.Matching Overallstrategy P R1 65.36 65.362 67.78 67.783 68.09 68.094 70.69 70.695 67.78 67.78Table 3: Disambiguation scores by article with a smallerknowledge base5 Conclusion and Future WorkBroad coverage and disambiguation quality are crit-ical for WSD techniques to be adopted in prac-tice.
This paper proposed a fully unsupervisedWSD method.
We have evaluated our approach withSemEval-2007 Task 7 (Coarse-grained English All-words Task) data set, and we achieved F-scores ap-proaching the top performing supervised WSD sys-tems.
By using widely available unannotated textand a fully unsupervised disambiguation approach,our method may provide a viable solution to theproblem of WSD.
The future work includes:1.
Continue to build the knowledge base, enlargethe coverage and improve the system perfor-mance.
The experiment results in Section 4.2clearly show that more word instances can im-prove the disambiguation accuracy and recallscores;2.
WSD is often an unconscious process for hu-man beings.
It is unlikely that a reader exam-ines all surrounding words when determiningthe sense of a word, which calls for a smarterand more selective matching strategy than whatwe have tried in Section 4.1;3.
Test our WSD system on fine-grained SemEval2007 WSD task 17.
Although we only evalu-ated our approach with coarse-grained senses,our method can be directly applied to fine-grained WSD without any modifications.AcknowledgmentsThis work is partially funded by NSF grant 0737408and Scholar Academy at the University of HoustonDowntown.
This paper contains proprietary infor-mation protected under a pending U.S. patent.35ReferencesAgirre, Eneko, Philip Edmonds (eds.).
2006.
WordSense Disambiguation: Algorithms and Applications,Springer.Chklovski, T. and Mihalcea, R. 2002.
Building a sensetagged corpus with open mind word expert.
In Pro-ceedings of the Acl-02 Workshop on Word Sense Dis-ambiguation: Recent Successes and Future Directions,Morristown, NJ, 116-122.C.
Fellbaum, WordNet: An Electronic Lexical Database,MIT press, 1998Project Gutenberg, available at www.gutenberg.orgHearst, M. (1991) Noun Homograph Disambiguation Us-ing Local Context in Large Text Corpora, Proc.
7thAnnual Conference of the University of Waterloo Cen-ter for the New OED and Text Research, Oxford.Nancy Ide and Jean Ve?ronis.
1998.
Introduction to thespecial issue on word sense disambiguation: the stateof the art.
Comput.
Linguist., 24(1):2?40.Kister, Ken.
?Dictionaries defined?, Library Journal, Vol.117 Issue 11, p43, 4p, 2bwLesk, M. 1986.
Automatic sense disambiguation usingmachine readable dictionaries: how to tell a pine conefrom an ice cream cone.
In Proceedings of the 5th An-nual international Conference on Systems Documenta-tion (Toronto, Ontario, Canada).
V. DeBuys, Ed.
SIG-DOC ?86.Dekang Lin.
1998.
Dependency-based evaluation ofminipar.
In Proceedings of the LREC Workshop onthe Evaluation of Parsing Systems, pages 234?241,Granada, Spain.Lin, D. 1997.
Using syntactic dependency as local con-text to resolve word sense ambiguity.
In Proceedings ofthe 35th Annual Meeting of the Association For Com-putational Linguistics and Eighth Conference of theEuropean Chapter of the Association For Computa-tional Linguistics (Madrid, Spain, July 07 - 12, 1997).Rada Mihalcea, Using Wikipedia for Automatic WordSense Disambiguation, in Proceedings of the NorthAmerican Chapter of the Association for Computa-tional Linguistics (NAACL 2007), Rochester, April2007.Rada Mihalcea.
2002.
Instance based learning with au-tomatic feature selection applied to word sense disam-biguation.
In Proceedings of the 19th internationalconference on Computational linguistics, pages 1?7,Morristown, NJ.Dan Moldovan and Vasile Rus, Explaining Answers withExtended WordNet, ACL 2001.Roberto Navigli, Kenneth C. Litkowski, and Orin Har-graves.
2007.
Semeval-2007 task 07: Coarse-grained english all-words task.
In Proceedings of theFourth International Workshop on Semantic Evalua-tions (SemEval-2007), pages 30?35, Prague, CzechRepublic.Roberto Navigli and Paola Velardi.
2005.
Structural se-mantic interconnections: a knowledge-based approachto word sense disambiguation.
IEEE Transactions onPattern Analysis and Machine Intelligence (PAMI),27(7):10631074.Hwee Tou Ng, Bin Wang, and Yee Seng Chan.
Exploit-ing Parallel Texts for Word Sense Disambiguation: AnEmpirical Study.
ACL, 2003.Hwee Tou Ng and Hian Beng Lee.
1996.
Integrat-ing multiple knowledge sources to disambiguate wordsense: an exemplar-based approach.
In Proceedings ofthe 34th annual meeting on Association for Computa-tional Linguistics, pages 40?47, Morristown, NJ.Adrian Novischi, Muirathnam Srikanth, and AndrewBennett.
2007.
Lcc-wsd: System description for En-glish coarse grained all words task at semeval 2007.In Proceedings of the Fourth International Workshopon Semantic Evaluations (SemEval-2007), pages 223?226, Prague, Czech Republic.Catherine Soanes and Angus Stevenson, editors.
2003.Oxford Dictionary of English.
Oxford UniversityPress.Rada Mihalcea, available athttp://www.cs.unt.edu/ rada/downloads.htmlYarowsky, D. 1995.
Unsupervised word sense disam-biguation rivaling supervised methods.
In Proceedingsof the 33rd Annual Meeting on Association For Com-putational Linguistics (Cambridge, Massachusetts,June 26 - 30, 1995).36
