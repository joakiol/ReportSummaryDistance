Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 149?159,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsImproving Semantic Parsing via Answer Type InferenceSemih Yavuz1, Izzeddin Gur1, Yu Su1, Mudhakar Srivatsa2 and Xifeng Yan11University of California, Santa Barbara, Department of Computer Science2IBM Research{syavuz,izzeddingur,ysu,xyan}@cs.ucsb.edumsrivats@us.ibm.comAbstractIn this work, we show the possibility of infer-ring the answer type before solving a factoidquestion and leveraging the type informationto improve semantic parsing.
By replacing thetopic entity in a question with its type, we areable to generate an abstract form of the ques-tion, whose answer corresponds to the answertype of the original question.
A bidirectionalLSTM model is built to train over the abstractform of questions and infer their answer types.It is also observed that if we convert a ques-tion into a statement form, our LSTM modelachieves better accuracy.
Using the predictedtype information to rerank the logical formsreturned by AgendaIL, one of the leading se-mantic parsers, we are able to improve theF1-score from 49.7% to 52.6% on the WE-BQUESTIONS data.1 IntroductionLarge scale knowledge bases (KB) like Freebase(Bollacker et al, 2008), DBpedia (Auer et al, 2007),and YAGO (Suchanek et al, 2007) that store theworld?s factual information in a structured fash-ion have become substantial resources for people tosolve questions.
KB-based factoid question answer-ing (KB-QA) that attempts to find exact answers tonatural language questions has gained much atten-tion recently.
KB-QA is a challenging task due tothe representation variety between natural languageand structural knowledge in KBs.As one of the promising KB-QA techniques, se-mantic parsing maps a natural language questioninto its semantic representation (e.g., logical forms).Ranking F1 # Improved QsAgendaIL 49.7 -w/ Oracle Types@10 57.3 +234w/ Oracle Types@20 58.7 +282w/ Oracle Types@50 60.1 +331w/ Oracle Types@All 60.5 +345Table 1: What if the correct answer type is enforced?
On We-bQuestions, we remove those with incorrect answer types in thetop-k logical forms returned by AgendaIL (Berant and Liang,2015), a leading semantic parsing system, and report the newaverage F1 score as well as the number of questions with animproved F1 score.It uses a logical language with predicates closely re-lated to KB schema, and constructs a dictionary thatmaps relations to KB predicates.
The problem thenreduces to generating candidate logical forms, rank-ing them, and selecting one to derive the final an-swer.In this work, we propose an answer type pre-diction model that can improve the ranking ofthe candidate logical forms generated by seman-tic parsing.
The type of an entity, e.g., person,organization, location, carries very usefulinformation for various down-stream natural lan-guage processing tasks such as co-reference resolu-tion (Recasens et al, 2013), knowledge base popu-lation (Carlson et al, 2010), relation extraction (Yaoet al, 2012), and question answering (Lin et al,2012).
Although the potential clues for answer typefrom the question has been employed in the recentwork AgendaIL (Berant and Liang, 2015) at the lex-ical level, Table 1 suggests that there is yet a largeroom for further improvement by explicitly enforc-149ing answer type.
Inspired by this observation, weaim to directly predict the KB type of the answerfrom the question.
In contrast to a small set of pre-defined types as used in previous answer type pre-diction methods (e.g., (Li and Roth, 2002)), KBscould have thousands of fine-grained types.
Take?When did Shaq come into the NBA??
as a runningexample.
We aim to predict the KB type of its an-swer as SportsLeagueDraft.1The value of typing answers in a fine granularitycan be appreciated from two perspectives: (1) Sinceeach entity in a KB like Freebase has a few types,answer type could help prune answer candidates, (2)since each predicate in the KB has a unique typeschema, answer type can help rank logical forms.The key challenge of using answer types to re-rank logic forms and hence their corresponding an-swers, is that it shall be done before the answer isfound.
Otherwise, there is no need to further inferits type.
Inspired by the observation that the an-swer type of a question is invariant as long as thetype of the topic entity (Shaq) remains the same(DraftedAthlete), we define abstract ques-tion as the question where the topic entity mentionis replaced by its corresponding KB type.
For theaforementioned example, the best candidate abstractquestion is ?When did DraftedAthlete comeinto the NBA??
and the answer to this question isSportsLeagueDraft.
Hence, we can reduce theanswer type prediction task to abstract question an-swering.The first step in our method is question ab-straction, in which we generate candidate abstractquestions based on the context of question and itscandidate topic entities.
We build a bidirectionalLSTM network over the question that recursivelycomputes vector representations for the past andfuture contexts of an entity mention.
Based onthese context representations, we predict the righttype of the entity mention.
Next, in order to bet-ter utilize the syntactic features of the question,we convert the question form into a normal state-ment form by using dependency tree of the ques-tion.
For the running example, after perform-ing the conversion, the abstract question becomes?DraftedAthlete come when into the NBA?
?1KB type of answer (?1992 NBA Draft?)
in the context.We then construct a bidirectional LSTM neural net-work over this final representation of the questionand predict the type of the answer.
Using the in-ferred answer type, we are able to improve the resultof AgendaIL (Berant and Liang, 2015) on WebQues-tions (Berant et al, 2013) from 49.7% to 52.6%.2 BackgroundThe knowledge base we work with consists oftriples in subject-predicate-object form.
It can berepresented as K = {(e1, p, e2) : e1, e2 ?E , p ?
P}, where E denotes the set of entities (e.g.,ShaquilleOneal), and P denotes the set of bi-nary predicates (e.g., Drafted).
A knowledge basein this format can be visualized as a graph where en-tities are nodes, and predicates are directed edgesbetween entities.
Freebase is used in this work asthe knowledge base.
It has more than 41M entities,596M facts, and 24K types.Types are an integral part of the Freebaseschema.
Each entity e in Freebase has a setof categories (types) it belongs to, and thisinformation can be obtained by checking theout-going predicates (Type.Object.Type)from e. For example, ShaquilleOnealhas 20 Freebase types including Person,BasketballPlayer, DraftedAthlete,Celebrity, and FilmActor.
For a specificquestion involving ShaquilleOneal, amongthese types, only a few will be relevant.Each predicate in Freebase is from a subject en-tity to an object entity, and has a type signature.
Ithas a unique expected types for its subject and ob-ject, independent of the individual subject and ob-ject entities themselves.
For example, the predicatePeople.Person.Profession expects its sub-ject to be of Person type and its object to be ofProfession type.3 Question AbstractionThe type of the topic entity rather than the entity it-self is essential for inferring the answer type, whichis invariant as the topic entity changes within thesame class.
For example, independent of whichNBA player (with DraftedAthlete type) is thetopic entity of this question ?When did Shaq comeinto the NBA?, the type of the answer is always go-150Figure 1: Bi-directional LSTM model for question abstraction.Green circles represent the forward sequence?s hidden vectors,while the red circles denote the backward sequence?s.
shaq(the topic entity mention) is the single output node of the net-work.ing to be SportsLeagueDraft in Freebase.
Pre-dicting this distinct type among the large number ofcandidate types in Freebase is a challenging task.We propose a two-step solution for this problem.In the first step, we compute a confidence score foreach possible KB type for a given topic entity us-ing a bidirectional LSTM network.
The second stepprunes candidate types using the entity type infor-mation in Freebase.3.1 FormulationGiven a natural language question and its topic en-tity mention, question abstraction is to predict typesof the mention in the question context.
Formally, letq = (x1, x2, .
.
.
, xL) denote the question, m be thetopic entity mention in q, and T = {t1, t2, .
.
.
, tK}the set of all types in KB.
Given q and m, we com-pute a probability distribution o ?
RK?1 over T ,where ok denotes the likelihood of tk being the cor-rect type of m in q.3.2 Scoring Topic Entity Types with LSTMModel.
We formulate question abstraction as a clas-sification problem.
A bidirectional LSTM networkis built over q whose output is computed from thenodes that correspond to the words of m. Fig.
1 il-lustrates the model for the question ?When did Shaqcome into the NBA?
?Let u(x) ?
RD?1 denote the vector space em-bedding of word x.
Forward and backward outputs?
?h l,?
?h l ?
RDh?1 of bidirectional LSTM are recur-sively computed by?
?h l,?
?c l = LSTM(u(xl),?
?h l?1,?
?c l?1) (1)?
?h l,?
?c l = LSTM(u(xl),?
?h l+1,?
?c l+1) (2)as described in Graves (2012), where ?
?cl ,?
?cl ?RDh?1 stand for LSTM cell states.To encode the context of m to the final output, weapply an AVERAGE pooling layer when computingthe output.
For each output node r ?
[i, j] (i and jcorrespond to the starting and ending indices ofm inq), we compute final forward and backward outputsby?
?vr = AV G(?
?h1, .
.
.
,?
?hr) (3)?
?vr = AV G(?
?hr, .
.
.
,?
?hn), (4)where AV G stands for average pooling.We take the average of outputs at each outputnode?
?v = AV G(?
?vi , .
.
.
,?
?vj ) (5)?
?v = AV G(?
?vi , .
.
.
,?
?vj ) (6)as the forward and backward outputs of the wholenetwork.
The final representation v of the networkis obtained by concatenating ?
?v and?
?v .For question q, the probability distribution o overtypes is computed bys(q) = Whyv (7)o(q) = softmax(s(q)), (8)where Why ?
RK?
(2Dh) since v is the concatena-tion of two vectors of dimension Dh, where Dh isthe hidden vector dimension.Objective Function and Learning.
Given aninput question q with a topic entity mention m,LSTM network computes the probability distribu-tion o(q) ?
RK?1 as in (8).
Let y(q) ?
RK?1 de-note the true target distribution over T for q, where151yk(q) = 1/n if tk is a correct type, yk(q) = 0 oth-erwise, and n is the number of correct types.
Weuse the cross-entropy loss function between y(q) ando(q), and define the objective function over all train-ing data asJ(?)
= ?
?qK?k=1yk(q) log ok(q) +?2 ??
?2 ,where ?
denotes the regularization parameter, and?
represents the set of all model parameters to belearned.
We use stochastic gradient descent withRMSProp (Tieleman and Hinton, 2012) for mini-mizing the objective function.3.3 PruningLet Te represent the set of KB types for entity e. Wedefine the set of candidate types for entity mentionm asCm =?eTe,where e is a possible match of m in KB.
We onlyneed to score the types in Cm.
Once the hidden rep-resentation v is computed by LSTM, we use subma-trix Why[Cm] that consists of rows of Why corre-sponding to the types in Cm as the scoring matrix in(7).
This returns the final scores for candidate typesin Cm.4 Conversion to Statement FormThe objective of the conversion is to canonicalizequestion form into declarative statement (subject-relation-object) form.
We use a simple pattern-basedmethod that relies on dependency tree2 (Manning etal., 2014).
It decides whether the sub-trees of theroot need reordering based on their dependency re-lations3.Before obtaining the dependency tree, we retrievenamed entity (NER) tags of the question tokens.
Wereplace a group of question tokens corresponding anamed entity with a special token, ENTITY, to sim-plify the parse tree.
In Figure 2, the question is firsttransformed to ?what boarding school did ENTITYgo to??
Each question is represented by the root?s2We use Stanford CoreNLP dependency parser3http://universaldependencies.orgFigure 2: Conversion: red relations form the input patternPattern Conversion(cop, nsubj) (nsubj, root, cop)who was anakin skywalker?
anakin skywalker was who(dobj, aux, nsubj) (nsubj, root, dobj)what language does australians speak?
australians speak what language(dobj, aux, nsubj, nmod) (nsubj, root, dobj, nmod)what did edward jenner do for a living?
edward jenner do what for a living(nsubj, dobj) (nsubj, root, dobj)who played bilbo baggins?
who played bilbo baggins(advmod, aux, nsubj) (nsubj, root, advmod)where did benjamin franklin died?
benjamin franklin died whereTable 2: Top-5 most common patterns with mappings.dependency relations to its sub-trees in the originalorder, e.g., (dep, aux, nsubj, nmod).
We clus-ter all these sequences and detect the patterns thatappear at least 5 times in the training data.
Thesepatterns are then manually mapped to their corre-sponding conversion (pattern vs. mapping in Figure2).Once the recomposition order of the sub-trees isdetermined by the conversion mapping, we finalizethe reordering of the question tokens by keeping theorder of words within the sub-trees same as the orig-inal order in the question.
The example in Figure2 becomes ?ENTITY go to what boarding school?with its corresponding sub-tree conversion mapping(nsubj, root, nmod, dep).
If no mapping is cre-ated for a pattern, we keep the order of the wordsexactly as they occur in the original question form.The motivation behind conversion is to overcomethe potential semantic confusion stemming fromvarities in syntactic structures.
To exemplify, con-sider two hypothetical questions ?who plays X in152Figure 3: Bi-directional LSTM model over the final representa-tion of the question.
Green and red circles are corresponding toforward and backward hidden vectors, respectively.
The outputnode is when.Y??
and ?who does Z play in Y?
?, where X isa FilmCharacter, Y is a Film, and Z is aFilmActor, with answer types FilmActor andFilmCharacter, respectively.
With conversion,we aim to transform second question into ?Z playwho in Y?, while leaving the first one as it is.
Not-ing that the order of words affects the output of ouranswer type inference network, our intuition is to letthe model distinguish better between such questionsusing their syntactic structure in this way.5 Answer Type PredictionGiven a reordered question with topic entity mentionm, and a topic entity type te ?
T , our task is topredict a probability distribution o ?
RK?1 over theanswer types.A topic entity type te ?
T is described as a setof words, {xi}.
Let u(xi) ?
RD?1 represent thevector space embedding of xi, the representation ofte is computed by the average encoding,u(te) =1|{xi}|?xiu(xi).
(9)As the first step, we replace the words of entitymention m with topic entity type te, and obtain anew input word sequence r. te is treated as oneword and encoded by Eq.
9.
We construct a bidi-rectional LSTM network over this input sequencer, whose output node corresponds to the questionword.
The output of the network is a probability dis-tribution over types denoting the likelihood of beingthe answer type.
Figure 3 shows how the networkis constructed for the running example.
The sameaverage pooling described in Section 3.2 is appliedto obtain the final forward and backward output vec-tors ?
?v and?
?v from the output node (this time, sin-gle output node) of network.
The final output vec-tor v for prediction is obtained by concatenating ?
?v ,and ?
?v .
The distribution o is computed by a stan-dard softmax layer.
The learning is performed bythe same cross-entropy loss and objective functiondescribed in Section 3.2.6 Reranking by Answer TypeIn this section, we describe how to rerank logicalforms based on our answer type prediction model.Reranking Model.
Let l1, l2, .
.
.
, lN be the log-ical forms generated for question q by a semanticparser, e.g., AgendaIL.
Each logical form has a scorefrom the semantic parser.
Meanwhile, our answertype prediction model generates a score for the an-swer type of each logical form.
Therefore, we canrepresent each logical form li using a pair of scores:the score from semantic parser and the score fromour type prediction model.
Suppose we know whichlogical forms are ?correct?, using the two scoresas input, we train a logistic regression model withcross-entropy loss to learn a binary classifier for pre-dicting the correct logical forms.
We rerank the top-k logical forms using their probability computed bythe trained logistic regression model, and select theone with the highest probability.
Finally, we run theselected logical form against KB to retrieve the an-swer.
We select the optimal value of k from [1, N ]using the training data.
For AgendaIL on WebQues-tions, we find that k = 80 gives the best result.Training Data Selection.
We now discuss whichlogical forms are ?correct?, i.e., how to select thepositive examples to train the logistic regressionmodel.
Because a question can have more than oneanswer, we use the F1 score, the harmonic mean ofprecision and recall, to evaluate logical forms.
Weselect all the logical forms with F1 > 0 as the setof positive examples.
However, taking all the log-ical forms with F1 = 0 as negative examples will153not work well.
Even though the F1 score of a log-ical form is 0, its answer type could still be correct.Therefore, we use the following trick: If there is apositive example with answer type t, we do not treatany other logical form with answer type t as neg-ative example.
The logical forms having F1 = 0,with the aforementioned exception, are then selectedas the final set of negative examples.
Our empiricalstudy shows this trick works well.7 ExperimentsIn this section, we describe the datasets, model train-ing, and experimental results.7.1 Dataset and Evaluation MetricsDatasets.
To evaluate our method, we use the We-bQuestions dataset (Berant et al, 2013), which con-tains 5,810 questions crawled via Google SuggestAPI.
The answers to these questions are annotatedfrom Freebase using Amazon Mechanical Turk.
Thedata is split into training and test sets of size 3,778and 2,032 questions, respectively.
This dataset hasbeen popularly used in question answering and se-mantic parsing.The SimpleQuestions (Bordes et al, 2015) con-tains 108,442 questions written in natural languageby English-speaking human annotators.
This datasetis a collection of question/Freebase-fact pairs ratherthan question/answer pairs.
The data4 is split andprovided as training(75,910), test(21,687), and val-idation(10,845) sets.
Each question is mapped tothe subject, relation, and object of the correspondingFreebase fact.
This dataset is only used for trainingthe question abstraction model.Training Data Preparation.
Since WebQues-tions only provides question-answer pairs along withannotated topic entities, we need to figure out thetype information, which can be used as training data.We obtain simulated types as follows: We retrieve 1-hop and 2-hop predicates r from/to annotated topicentity e in Freebase.
For each relation r, we query(e, r, ?)
and (?, r, e) against Freebase and retrievethe candidate answers ra.
The F1 value of eachcandidate answer ra is computed with respect to theannotated answer.
The subject and object types ofthe relation r with the highest F1 value is selected4http://fb.ai/babi.as the simulated type for the topic entity and the an-swer.
When there are multiple such relations, weobtain multiple simulated types for topic entity andanswer, one from each relation.
We treat each ofthem as correct with equal probability.Candidate Logical Forms for Evaluation.
Toobtain candidate logical forms, we train AgendaIL(Berant and Liang, 2015) on WebQuestions withbeam size 200 using the publicly available code5 bythe authors.Evaluation Metric.
We report average F1 scoreof the reranked logical forms using the predicted an-swer types as the main evaluation metric.
It is a com-mon performance measure in question answering asquestions might have multiple answers.7.2 Experimental SetupWe use 50 dimensional word embeddings, which areinitialized by the 50 dimensional pre-trained wordvectors6 from GloVe (Pennington et al, 2014), andupdated in the training process.
Hyperparametersare tuned on the development set.
The size of theLSTM hidden layer is set at 50.
We use RMSProp(Tieleman and Hinton, 2012) with a learning rate of0.005 and mini-batch size of 32 for the optimization.We use a dropout layer with probability 0.5 for reg-ularization.
We implemented the LSTM networksusing Theano (Theano Development Team, 2016).Identifying Topic Entity.
We use Stanford NERtagger (Manning et al, 2014) to identify topic en-tity span for both training and test data.
For en-tity linking, annotated mention span is mapped to aranked list of candidate Freebase entities using Free-base Search API for the test data.
For the trainingdata, we use the gold Freebase topic entity linkingsof each question provided by WebQuestions, com-ing from its question generation process.Question Abstraction.
We first pre-train theLSTM model described in Section 3.2 on the Sim-pleQuestions dataset.
Then, we update the pre-trained model on the training portion of WebQues-tions data where the simulated topic entity types areused as true labels.
We use the detected topic en-tity mentions to obtain candidate matching entitiesin the KB using Freebase Search API.
We use top-5https://github.com/percyliang/sempre6http://nlp.stanford.edu/projects/glove/154Model F1(Berant et al, 2013) 35.7(Yao and Van Durme, 2014) 33.0(Berant and Liang, 2014) 39.9(Bao et al, 2014) 37.5(Bordes et al, 2014) 39.2(Yang et al, 2014) 41.3(Dong et al, 2015b) 40.8(Yao, 2015) 44.3(Berant and Liang, 2015) 49.7(Yih et al, 2015) 52.5(Reddy et al, 2016) 50.3(Xu et al, 2016) 53.3(Yih et al, 2015) (w/ Freebase API) 48.4(Yih et al, 2015) (w/o ClueWeb) 50.9(Xu et al, 2016) (w/o Wikipedia) 47.1Our Approach (w/o SimpleQuestions) 51.6Our Approach 52.6Table 3: Comparison of our reranking-by-type system with sev-eral existing works on WebQuestions.3 entities returned for the pruning step of QuestionAbstraction on the test examples.Answer Type Prediction.
We train Answer TypePrediction model using the simulated topic entityand answer types for each question.
We perform theanswer type prediction on test data using the pre-dicted topic entity type.7.3 ResultsOur main result is presented in Table 3.
Our systemadds 2.9% absolute improvement over AgendaIL,and achieves 52.6% in F1 measure.
Yih et al (2015)achieve 52.5% by leveraging ClueWeb and S-MART(Yang and Chang, 2015), an advanced entity linkingsystem.
Xu et al (2016) achieve 53.3% by lever-aging Wikipedia and S-MART.
If tested withoutClueweb/Wikipedia/S-MART, their F1 scores are48.4% and 47.1%, respectively.
When our method istested without using SimpleQuestions data for pre-training question abstraction module, it attains F1score of 51.6%.In Table 4, we present some question ex-amples where our method can select a bet-ter logical form.
Take the question ?whodid [australia] fight in the first world war?
?as an example.
Our topic entity type pre-diction module returns MilitaryCombatant,Method F1 Gain LossBase 50.3 69 47Base + Conv 50.5 96 56Base + Abs 52.2 184 87Base + Abs + Conv 52.6 203 93AgendaIL 49.7 - -Table 6: Ablation analysis of modules of our method.Gain/Loss columns denote the number of questions where theF1 score of our selected logical form is greater/less than that ofthe top ranked logical forms from AgendaIL.StatisticalRegion, and Kingdom as the top-3 results for the type of ?australia?
in this ques-tion, which indicates that it exploits the contextof this short question successfully.
The abstractquestion is ?
[military combatant] fight who in thefirst world war??
for which our system returnsMilitaryCombatant, MilitaryConflict,and MilitaryCommander as answer types withprobabilities 0.73, 0.25, and 0.005, respectively,MilitaryCombatant is indeed the right answertype.
This example shows the effect of abstraction inchanneling the context in the most relevant directionto find the right answer type.
In Table 5, we providea comparison of the selected logical forms based onAgendaIL rankings and our rankings.7.4 Ablation AnalysisIn this section, we evaluate the effect of individualcomponents of our model.
Note that the answer typeprediction model described in Section 5 can workindependently from question abstraction and formconversion.
We develop the following variants i)Base, ii) Base + Conversion, iii) Base + Abstrac-tion, iv) Base + Abstraction + Conversion, whereBase corresponds to a model that infers answer typeswithout employing abstraction or form conversion.We train/test each variant separately.
Table 6 showseach component contributes and question abstrac-tion does help boost the performance.Suppose we perform answer type prediction with-out question abstraction, and feed ?
[australia] fightwho in the first world war??
into the answertype prediction model (Base + Conversion).
Thepredicted answer type is Location.
Unfortu-nately, there is neither a 1-hop or 2-hop correct re-lation from/to Australia with the expected typeLocation nor a correct (with positive F1) candi-155Question Topic Entity Type Prediction Answer Type Prediction AgendaIL Answer Type F1 Gainwho inspired obama?
InfluenceNode InfluenceNode UsVicePresident 1.0what are some books that mark twain wrote?
Author WrittenWork InfluenceNode 0.3who won the league cup in 2002?
SportsAwardType SportsAwardWinner SportsLeagueSeason 1.0what type of government does france use?
Country FormOfGovernment Government 1.0where are the new orleans hornets moving to?
SportsTeam SportsFacility Location 1.0who did australia fight in the first world war?
MilitaryCombatant MilitaryCombatant MilitaryCommander 0.4what guitar does corey taylor play?
Musician MusicalInstrument Organization 0.33what region is turkey considered?
Location AdministrativeDivision Beer 0.93what country does rafael nadal play for?
Athlete Country OlympicDiscipline 1.0Table 4: Example questions where our type prediction helps select a better logical form.
The F1 gain shows the difference betweenthe F1 score of the logical form we select and the top ranked logical form from AgendaIL.Questions and Selected Logical Forms1.
what are some books that mark twain wrote?AgendaIL: (MarkTwain - Influence.InfluenceNode.InfluencedBy - ?
)Ours: (MarkTwain - Book.Author.WorksWritten - ?)2.
what guitar does corey taylor play?AgendaIL: (?
- Organization.Organization.Founders - CoreyTaylor)Ours: (CoreyTaylor - Music.GroupMember.InstrumentsPlayed - ?)3.
what type of government does france use?AgendaIL: (France - Government.GovernmentalJurisdiction.Government - ?
)Ours: (France - Location.Country.FormOfGovernment - ?
)Table 5: Comparison of selected logical forms for some examples.
Logical forms are simplified and canonicalized into (subject -predicate - object) format for better readability, where ?
corresponds to answer nodes.date logical form with the answer type Location.This shows that through question abstraction, a bet-ter logical form is selected for this question.To exemplify another benefit of question ab-straction, consider the question ?where does[marta] play soccer??
The top 3 entity link-ings via Freebase Search API for ?marta?
areMetropolitanAtlantaRapidTransit-Authority, Marta, and SantaMarta, wherethe correct entity is the second one.
Our questionabstraction system returns FootballPlayer asthe top topic entity type prediction that is indeedcorresponding to the correct entity.
Utilizing thecontext via question abstraction we are able torecover useful information when the entity linkingis uncertain.Table 6 also shows that the conversion to state-ment form also helps, especially together with Ab-straction.
In the above example, the model withoutConversion (Base + Abs) predicts the answer typefor ?where does [football player] play soccer?
asSportsFacility, whereas the full model, con-sidering Conversion as well, finds the answertype for ?
[football player] play soccer where?
asSportsTeam which is the better type in this case.7.5 Error AnalysisWe present a further analysis of our approach byclassifying the type inference errors made on ran-domly sampled 100 questions.
9% of the er-rors are due to inference at incorrect granular-ity (e.g., City instead of Location).
12% ofthe errors are the result of incorrect answer labels(hence incorrect answer types) or question ambigu-ity (e.g., ?where is dwight howard now??).
11% ofthem are incorrect, but acceptable inferences, e.g.,BookWrittenWork instead of BookEditionfor question ?what dawkins book to read first?
?39% of the errors are due to the sparsity problem:They are made on questions whose answer type ap-pears less than 5 times in the training data (e.g.,DayOfYear).
The remaining 29% of them are dueto incorrect question abstraction.
In most of thequestion abstraction errors, the predicted topic en-tity type is semantically close to the correct type.
Inother cases such as ?what did joey jordison play inslipknot??
where we predict FilmActor as thetopic entity type while Musician is the correctone.
In these cases, the answer type inference is notable to correct the abstraction error.
These 29% oferrors also contain the entity linking errors.1568 Related WorkFreebase QA has been studied from two differ-ent perspectives: grounded QA systems that workdirectly on KBs and general purpose ungroundedQA systems.
Kwiatkowski et al (2013) generatesKB agnostic intermediary CCG parses of questionswhich are grounded afterwards given a KB.
Bor-des et al (2014) uses a vector space embedding ap-proach to measure the semantic similarity betweenquestion and answers.
Yao and Van Durme (2014),Bast and Haussmann (2015) and Yih et al (2015)exploit a graph centric approach where a groundedsubgraph query is generated from question and thenexecuted against a KB.
In this work, we propose aneural answer type inference method that can be in-corporated in existing grounded semantic parsers asa complementary feature to improve ranking of thecandidate logical forms.Berant and Liang (2015) uses lambda DCS logi-cal language with predicates from Freebase.
In theirapproach, types are included as a part of unary lexi-con for building the logical forms from natural lan-guage questions.
However, no explicit type infer-ence is exploited.
We show that such informationcould indeed be useful for selecting logical forms.There have been a series of studies investigatingthe expected answer type of a question in differentcontexts such as Li and Roth (2002), Lally et al(2012), and Balog and Neumayer (2012).
Most ofthese approaches classify the questions into a smallset of types.
Even when the set of classes is morefine-grained, e.g., 50 classes in Li and Roth (2002),they cannot be used for our purpose as it would re-quire nontrivial mapping between these categoriesand a much larger number of KB types.
Further-more, these methods often rely on a rich set of handcrafted features and external resources.Sun et al (2015) uses Freebase types to learn therelevance of candidate answers to a given questionvia an association model.
Their model directly ranksthe answer candidates by utilizing types, whereasours ranks the logical forms via predicting answertype.
In this sense, we are able to take advantageof both logical form and type inference.
Su et al(2015) exploits answer typing to facilitate knowl-edge graph search, but their input is graph query in-stead of natural language question.
They predict an-swer types using additional relevance feedback forgraph queries, while our algorithm directly infersanswer types from input questions.
On the questionabstraction side, our work is related to a recent study(Dong et al, 2015a) which classifies entity mentionsinto 22 types derived from DBpedia.
They use amultilayer perceptron over a fixed size window anda recurrent neural network for the representations ofcontext and entity mention, respectively.
Instead, weuse a bidirectional LSTM network to exploit the fullcontext more flexibly.9 ConclusionIn this paper, we present a question answer type in-ference framework and leverage it to improve se-mantic parsing.
We define the notion of abstractquestion as the class of questions that can be an-swered by type instead of entity.
Question an-swer type inference is then reduced to ?question ab-straction?
and ?abstract question answering?, bothof which are formulated as classification problems.Question abstraction is performed by exploiting thetopic entity and its context in question via an LSTMnetwork .
A separate neural network is trained toexploit the abstraction to make the final question an-swer type inference.
Our method improves the rank-ing of logical forms returned by AgendaIL on theWEBQUESTIONS dataset.
In the future, we wouldlike to investigate how the abstraction and explicittype inference can be incorporated in the early stageof semantic parsing for generating better candidatelogical forms.AcknowledgementsWe would like to thank the anonymous reviewers fortheir valuable comments, and Huan Sun for fruit-ful discussions.
This research was sponsored in partby the Army Research Laboratory under cooperativeagreements W911NF09-2-0053, NSF IIS 1528175,and NSF CCF 1548848.
The views and conclu-sions contained herein are those of the authors andshould not be interpreted as representing the officialpolicies, either expressed or implied, of the ArmyResearch Laboratory or the U.S. Government.
TheU.S.
Government is authorized to reproduce and dis-tribute reprints for Government purposes notwith-standing any copyright notice herein.157ReferencesSo?ren Auer, Christian Bizer, Georgi Kobilarov, JensLehmann, Richard Cyganiak, and Zachary Ives.
2007.DBpedia: A nucleus for a web of open data.
In Inter-national Semantic Web Conference (ISWC).Krisztian Balog and Robert Neumayer.
2012.
Hier-archical target type identification for entity-orientedqueries.
In ACM International Conference on Infor-mation and Knowledge Management (CIKM).Junwei Bao, Nan Duan, Ming Zhou, and Tiejun Zhao.2014.
Knowledge-based question answering as ma-chine translation.
In Annual Meeting of the Associa-tion for Computational Linguistics (ACL).Hannah Bast and Elmar Haussmann.
2015.
More accu-rate question answering on freebase.
In ACM Inter-national Conference on Information and KnowledgeManagement (CIKM).Jonathan Berant and Percy Liang.
2014.
Semantic pars-ing via paraphrasing.
Annual Meeting of the Associa-tion for Computational Linguistics (ACL).Jonathan Berant and Percy Liang.
2015.
Imitation learn-ing of agenda-based semantic parsers.
Transactions ofthe Association for Computational Linguistics (TACL).Jonathan Berant, Andrew Chou, Roy Frostig, and PercyLiang.
2013.
Semantic parsing on freebase fromquestion-answer pairs.
In Empirical Methods on Nat-ural Language Processing (EMNLP).Kurt Bollacker, Colin Evans, Praveen Paritosh, TimSturge, and Jamie Taylor.
2008.
Freebase: A collabo-ratively created graph database for structuring humanknowledge.
In ACM SIGMOD International Confer-ence on Management of Data.Antoine Bordes, Sumit Chopra, and Jason Weston.2014.
Question answering with subgraph embeddings.ArXiv.Antoine Bordes, Nicolas Usunier, Sumit Chopra, and Ja-son Weston.
2015.
Large-scale simple question an-swering with memory networks.
ArXiv.Andrew Carlson, Justin Betteridge, Richard C. Wang, Es-tevam R. Hruschka, Jr., and Tom M. Mitchell.
2010.Coupled semi-supervised learning for information ex-traction.
In ACM International Conference on WebSearch and Data mining (WSDM).Li Dong, Furu Wei, Hong Sun, Ming Zhou, and Ke Xu.2015a.
A hybrid neural model for type classificationof entity mentions.
In International Joint Conferenceon Artificial Intelligence (IJCAI).Li Dong, Furu Wei, Ming Zhou, and Ke Xu.
2015b.Question answering over freebase with multi-columnconvolutional neural networks.
In Annual Meeting ofthe Association for Computational Linguistics (ACL).Alex Graves, 2012.
Supervised Sequence Labelling withRecurrent Neural Networks, pages 5?13.
SpringerBerlin Heidelberg.Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and LukeZettlemoyer.
2013.
Scaling semantic parsers with on-the-fly ontology matching.
In Empirical Methods onNatural Language Processing (EMNLP).Adam Lally, John M Prager, Michael C McCord,BK Boguraev, Siddharth Patwardhan, James Fan, PaulFodor, and Jennifer Chu-Carroll.
2012.
Questionanalysis: How watson reads a clue.
IBM Journal ofResearch and Development.Xin Li and Dan Roth.
2002.
Learning question classi-fiers.
In International Conference on ComputationalLinguistics (COLING).Thomas Lin, Mausam, and Oren Etzioni.
2012.
No nounphrase left behind: Detecting and typing unlinkableentities.
In Empirical Methods on Natural LanguageProcessing (EMNLP).Christopher D Manning, Mihai Surdeanu, John Bauer,Jenny Finkel, Steven J Bethard, and David McClosky.2014.
The stanford corenlp natural language process-ing toolkit.
In Annual Meeting of the Association forComputational Linguistics: System Demonstrations.Jeffrey Pennington, Richard Socher, and Christopher DManning.
2014.
Glove: Global vectors for word rep-resentation.
In Empirical Methods on Natural Lan-guage Processing (EMNLP).Marta Recasens, Marie catherine De Marneffe, andChristopher Potts.
2013.
The life and death of dis-course entities: Identifying singleton mentions.
Con-ference of the North American Chapter of the Associa-tion for Computational Linguistics: Human LanguageTechnologies (NAACL-HLT).Siva Reddy, Oscar Ta?ckstro?m, Michael Collins, TomKwiatkowski, Dipanjan Das, Mark Steedman, andMirella Lapata.
2016.
Transforming DependencyStructures to Logical Forms for Semantic Parsing.Transactions of the Association for ComputationalLinguistics (TACL).Yu Su, Shengqi Yang, Huan Sun, Mudhakar Srivatsa, SueKase, Michelle Vanni, and Xifeng Yan.
2015.
Exploit-ing relevance feedback in knowledge graph search.
InACM International Conference on Knowledge Discov-ery and Data Mining (SIGKDD).Fabian M. Suchanek, Gjergji Kasneci, and GerhardWeikum.
2007.
Yago: A core of semantic knowledge.In World Wide Web (WWW).Huan Sun, Hao Ma, Wen-tau Yih, Chen-Tse Tsai,Jingjing Liu, and Ming-Wei Chang.
2015.
Open do-main question answering via semantic enrichment.
InWorld Wide Web (WWW).158Theano Development Team.
2016.
Theano: A Pythonframework for fast computation of mathematical ex-pressions.
ArXiv.Tijmen Tieleman and Geoffrey E. Hinton.
2012.
Lec-ture 6.5 - RMSProp, COURSERA: Neural networksfor machine learning.
Technical Report.Kun Xu, Siva Reddy, Yansong Feng, Songfang Huang,and Dongyan Zhao.
2016.
Question answering onfreebase via relation extraction and textual evidence.In Annual Meeting of the Association for Computa-tional Linguistics (ACL).Yi Yang and Ming-Wei Chang.
2015.
S-mart: Noveltree-based structure learning algorithms applied to en-tity linking.
In Annual Meeting of the Association forComputational Linguistics (ACL).Min-Chul Yang, Nan Duan, Ming Zhou, and Hae-Chang Rim.
2014.
Joint relational embeddings forknowledged-based question answering.
In EmpiricalMethods on Natural Language Processing (EMNLP).Xuchen Yao and Benjamin Van Durme.
2014.
Informa-tion extraction over structured data: Question answer-ing with freebase.
In Annual Meeting of the Associa-tion for Computational Linguistics (ACL).Limin Yao, Sebastian Riedel, and Andrew McCallum.2012.
Unsupervised relation discovery with sense dis-ambiguation.
In Annual Meeting of the Association forComputational Linguistics (ACL).Xuchen Yao.
2015.
Lean question answering over free-base from scratch.
In The North American Chap-ter of the Association for Computational Linguistics(NAACL).Wen-tau Yih, MingWei Chang, Xiaodong He, and Jian-feng Gao.
2015.
Semantic parsing via staged querygraph generation: Question answering with knowl-edge base.
In Annual Meeting of the Association forComputational Linguistics (ACL).159
