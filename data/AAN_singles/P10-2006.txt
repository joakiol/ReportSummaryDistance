Proceedings of the ACL 2010 Conference Short Papers, pages 27?32,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsEfficient Path Counting Transducers for Minimum Bayes-Risk Decodingof Statistical Machine Translation LatticesGraeme Blackwood, Adria` de Gispert, William ByrneMachine Intelligence LaboratoryCambridge University Engineering DepartmentTrumpington Street, CB2 1PZ, U.K.{gwb24|ad465|wjb31}@cam.ac.ukAbstractThis paper presents an efficient imple-mentation of linearised lattice minimumBayes-risk decoding using weighted finitestate transducers.
We introduce transduc-ers to efficiently count lattice paths con-taining n-grams and use these to gatherthe required statistics.
We show that theseprocedures can be implemented exactlythrough simple transformations of wordsequences to sequences of n-grams.
Thisyields a novel implementation of latticeminimum Bayes-risk decoding which isfast and exact even for very large lattices.1 IntroductionThis paper focuses on an exact implementationof the linearised form of lattice minimum Bayes-risk (LMBR) decoding using general purposeweighted finite state transducer (WFST) opera-tions1.
The LMBR decision rule in Tromble et al(2008) has the formE?
= argmaxE??E{?0|E?|+?u?N?u#u(E?
)p(u|E)}(1)where E is a lattice of translation hypotheses, Nis the set of all n-grams in the lattice (typically,n = 1 .
.
.
4), and the parameters ?
are constantsestimated on held-out data.
The quantity p(u|E)we refer to as the path posterior probability of then-gram u.
This particular posterior is defined asp(u|E) = p(Eu|E) =?E?EuP (E|F ), (2)where Eu = {E ?
E : #u(E) > 0} is the sub-set of lattice paths containing the n-gram u at least1We omit an introduction to WFSTs for space reasons.See Mohri et al (2008) for details of the general purposeWFST operations used in this paper.once.
It is the efficient computation of these pathposterior n-gram probabilities that is the primaryfocus of this paper.
We will show how generalpurpose WFST algorithms can be employed to ef-ficiently compute p(u|E) for all u ?
N .Tromble et al (2008) use Equation (1) as anapproximation to the general form of statisticalmachine translation MBR decoder (Kumar andByrne, 2004):E?
= argminE??E?E?EL(E,E?
)P (E|F ) (3)The approximation replaces the sum over all pathsin the lattice by a sum over lattice n-grams.
Eventhough a lattice may have many n-grams, it ispossible to extract and enumerate them exactlywhereas this is often impossible for individualpaths.
Therefore, while the Tromble et al (2008)linearisation of the gain function in the decisionrule is an approximation, Equation (1) can be com-puted exactly even over very large lattices.
Thechallenge is to do so efficiently.If the quantity p(u|E) had the form of a condi-tional expected countc(u|E) =?E?E#u(E)P (E|F ), (4)it could be computed efficiently using countingtransducers (Allauzen et al, 2003).
The statis-tic c(u|E) counts the number of times an n-gramoccurs on each path, accumulating the weightedcount over all paths.
By contrast, what is neededby the approximation in Equation (1) is to iden-tify all paths containing an n-gram and accumulatetheir probabilities.
The accumulation of probabil-ities at the path level, rather than the n-gram level,makes the exact computation of p(u|E) hard.Tromble et al (2008) approach this problem bybuilding a separate word sequence acceptor foreach n-gram in N and intersecting this acceptor27with the lattice to discard all paths that do not con-tain the n-gram; they then sum the probabilities ofall paths in the filtered lattice.
We refer to this asthe sequential method, since p(u|E) is calculatedseparately for each u in sequence.Allauzen et al (2010) introduce a transducerfor simultaneous calculation of p(u|E) for all un-igrams u ?
N1 in a lattice.
This transducer iseffective for finding path posterior probabilities ofunigrams because there are relatively few uniqueunigrams in the lattice.
As we will show, however,it is less efficient for higher-order n-grams.Allauzen et al (2010) use exact statistics forthe unigram path posterior probabilities in Equa-tion (1), but use the conditional expected countsof Equation (4) for higher-order n-grams.
Theirhybrid MBR decoder has the formE?
= argmaxE?
?E{?0|E?|+?u?N :1?|u|?k?u#u(E?
)p(u|E)+?u?N :k<|u|?4?u#u(E?
)c(u|E)}, (5)where k determines the range of n-gram ordersat which the path posterior probabilities p(u|E)of Equation (2) and conditional expected countsc(u|E) of Equation (4) are used to compute theexpected gain.
For k < 4, Equation (5) is thusan approximation to the approximation.
In manycases it will be perfectly fine, depending on howclosely p(u|E) and c(u|E) agree for higher-ordern-grams.
Experimentally, Allauzen et al (2010)find this approximation works well at k = 1 forMBR decoding of statistical machine translationlattices.
However, there may be scenarios in whichp(u|E) and c(u|E) differ so that Equation (5) is nolonger useful in place of the original Tromble etal.
(2008) approximation.In the following sections, we present an efficientmethod for simultaneous calculation of p(u|E) forn-grams of a fixed order.
While other fast MBRapproximations are possible (Kumar et al, 2009),we show how the exact path posterior probabilitiescan be calculated and applied in the implementa-tion of Equation (1) for efficient MBR decodingover lattices.2 N-gram Mapping TransducerWe make use of a trick to count higher-order n-grams.
We build transducer ?n to map word se-quences to n-gram sequences of order n. ?n has asimilar form to the WFST implementation of an n-gram language model (Allauzen et al, 2003).
?nincludes for each n-gram u = wn1 arcs of the form:wn-11 wn2wn:uThe n-gram lattice of order n is called En and isfound by composing E ?
?n, projecting on the out-put, removing ?-arcs, determinizing, and minimis-ing.
The construction of En is fast even for largelattices and is memory efficient.
En itself mayhave more states than E due to the association ofdistinct n-gram histories with states.
However, thecounting transducer for unigrams is simpler thanthe corresponding counting transducer for higher-order n-grams.
As a result, counting unigrams inEn is easier than counting n-grams in E .3 Efficient Path CountingAssociated with each En we have a transducer ?nwhich can be used to calculate the path posteriorprobabilities p(u|E) for all u ?
Nn.
In Figures1 and 2 we give two possible forms2 of ?n thatcan be used to compute path posterior probabilitiesover n-grams u1,2 ?
Nn for some n. No modifica-tion to the ?-arc matching mechanism is requiredeven in counting higher-order n-grams since all n-grams are represented as individual symbols afterapplication of the mapping transducer ?n.Transducer ?Ln is used by Allauzen et al (2010)to compute the exact unigram contribution to theconditional expected gain in Equation (5).
For ex-ample, in counting paths that contain u1, ?Ln re-tains the first occurrence of u1 and maps everyother symbol to ?.
This ensures that in any pathcontaining a given u, only the first u is counted,avoiding multiple counting of paths.We introduce an alternative path counting trans-ducer ?Rn that effectively deletes all symbols ex-cept the last occurrence of u on any path by en-suring that any paths in composition which countearlier instances of u do not end in a final state.Multiple counting is avoided by counting only thelast occurrence of each symbol u on a path.We note that initial ?:?
arcs in ?Ln effectivelycreate |Nn| copies of En in composition whilesearching for the first occurrence of each u. Com-2The special composition symbol ?
matches any arc; ?matches any arc other than those with an explicit transition.See the OpenFst documentation: http://openfst.org280123u1:u1u2:u2?:??:??:??:??
:?Figure 1: Path counting transducer ?Ln matchingfirst (left-most) occurrence of each u ?
Nn.01324u1:u1u2:u2u1:?u2:??:??:??
:?Figure 2: Path counting transducer ?Rn matchinglast (right-most) occurrence of each u ?
Nn.posing with ?Rn creates a single copy of En whilesearching for the last occurrence of u; we find thisto be much more efficient for large Nn.Path posterior probabilities are calculated overeach En by composing with ?n in the log semir-ing, projecting on the output, removing ?-arcs, de-terminizing, minimising, and pushing weights tothe initial state (Allauzen et al, 2010).
Using ei-ther ?Ln or ?Rn , the resulting counts acceptor is Xn.It has a compact form with one arc from the startstate for each ui ?
Nn:0 iui/- log p(ui|E)3.1 Efficient Path Posterior CalculationAlthough Xn has a convenient and elegant form,it can be difficult to build for large Nn becausethe composition En ?
?n results in millions ofstates and arcs.
The log semiring ?-removal anddeterminization required to sum the probabilitiesof paths labelled with each u can be slow.However, if we use the proposed ?Rn , then eachpath in En ?
?Rn has only one non-?
output la-bel u and all paths leading to a given final stateshare the same u.
A modified forward algorithmcan be used to calculate p(u|E) without the costly?-removal and determinization.
The modificationsimply requires keeping track of which symbolu is encountered along each path to a final state.More than one final state may gather probabilitiesfor the same u; to compute p(u|E) these proba-bilities are added.
The forward algorithm requiresthat En?
?Rn be topologically sorted; although sort-ing can be slow, it is still quicker than log semiring?-removal and determinization.The statistics gathered by the forward algo-rithm could also be gathered under the expectationsemiring (Eisner, 2002) with suitably defined fea-tures.
We take the view that the full complexity ofthat approach is not needed here, since only onesymbol is introduced per path and per exit state.Unlike En ?
?Rn , the composition En ?
?Ln doesnot segregate paths by u such that there is a di-rect association between final states and symbols.The forward algorithm does not readily yield theper-symbol probabilities, although an arc weightvector indexed by symbols could be used to cor-rectly aggregate the required statistics (Riley et al,2009).
For large Nn this would be memory in-tensive.
The association between final states andsymbols could also be found by label pushing, butwe find this slow for large En ?
?n.4 Efficient Decoder ImplementationIn contrast to Equation (5), we use the exact valuesof p(u|E) for all u ?
Nn at orders n = 1 .
.
.
4 tocomputeE?
= argminE??E{?0|E?|+4?n=1gn(E,E?
)}, (6)where gn(E,E?)
=?u?Nn ?u#u(E?
)p(u|E) us-ing the exact path posterior probabilities at eachorder.
We make acceptors ?n such that E ?
?nassigns order n partial gain gn(E,E?)
to all pathsE ?
E .
?n is derived from ?n directly by assign-ing arc weight ?u?p(u|E) to arcs with output labelu and then projecting on the input labels.
For eachn-gram u = wn1 in Nn arcs of ?n have the form:wn-11 wn2wn/?u ?
p(u|E)To apply ?0 we make a copy of E , called E0,with fixed weight ?0 on all arcs.
The decoder isformed as the composition E0 ?
?1 ?
?2 ?
?3 ?
?4and E?
is extracted as the maximum cost string.5 Lattice Generation for LMBRLattice MBR decoding performance and effi-ciency is evaluated in the context of the NIST29mt0205tune mt0205test mt08nw mt08ngML 54.2 53.8 51.4 36.3k0 52.6 52.3 49.8 34.51 54.8 54.4 52.2 36.62 54.9 54.5 52.4 36.83 54.9 54.5 52.4 36.8LMBR 55.0 54.6 52.4 36.8Table 1: BLEU scores for Arabic?English maximum likelihood translation (ML), MBR decoding usingthe hybrid decision rule of Equation (5) at 0 ?
k ?
3, and regular linearised lattice MBR (LMBR).mt0205tune mt0205test mt08nw mt08ngPosteriorssequential 3160 3306 2090 3791?Ln 6880 7387 4201 8796?Rn 1746 1789 1182 2787Decoding sequential 4340 4530 2225 4104?n 284 319 118 197Totalsequential 7711 8065 4437 8085?Ln 7458 8075 4495 9199?Rn 2321 2348 1468 3149Table 2: Time in seconds required for path posterior n-gram probability calculation and LMBR decodingusing sequential method and left-most (?Ln) or right-most (?Rn ) counting transducer implementations.Arabic?English machine translation task3.
Thedevelopment set mt0205tune is formed from theodd numbered sentences of the NIST MT02?MT05 testsets; the even numbered sentences formthe validation set mt0205test.
Performance onNIST MT08 newswire (mt08nw) and newsgroup(mt08ng) data is also reported.First-pass translation is performed using HiFST(Iglesias et al, 2009), a hierarchical phrase-baseddecoder.
Word alignments are generated usingMTTK (Deng and Byrne, 2008) over 150M wordsof parallel text for the constrained NIST MT08Arabic?English track.
In decoding, a Shallow-1 grammar with a single level of rule nesting isused and no pruning is performed in generatingfirst-pass lattices (Iglesias et al, 2009).The first-pass language model is a modifiedKneser-Ney (Kneser and Ney, 1995) 4-gram esti-mated over the English parallel text and an 881Mword subset of the GigaWord Third Edition (Graffet al, 2007).
Prior to LMBR, the lattices arerescored with large stupid-backoff 5-gram lan-guage models (Brants et al, 2007) estimated overmore than 6 billion words of English text.The n-gram factors ?0, .
.
.
, ?4 are set accordingto Tromble et al (2008) using unigram precision3http://www.itl.nist.gov/iad/mig/tests/mtp = 0.85 and average recall ratio r = 0.74.
Ourtranslation decoder and MBR procedures are im-plemented using OpenFst (Allauzen et al, 2007).6 LMBR Speed and PerformanceLattice MBR decoding performance is shown inTable 1.
Compared to the maximum likelihoodtranslation hypotheses (row ML), LMBR givesgains of +0.8 to +1.0 BLEU for newswire data and+0.5 BLEU for newsgroup data (row LMBR).The other rows of Table 1 show the performanceof LMBR decoding using the hybrid decision ruleof Equation (5) for 0 ?
k ?
3.
When the condi-tional expected counts c(u|E) are used at all orders(i.e.
k = 0), the hybrid decoder BLEU scores areconsiderably lower than even the ML scores.
Thispoor performance is because there are many un-igrams u for which c(u|E) is much greater thanp(u|E).
The consensus translation maximising theconditional expected gain is then dominated byunigram matches, significantly degrading LMBRdecoding performance.
Table 1 shows that forthese lattices the hybrid decision rule is an ac-curate approximation to Equation (1) only whenk ?
2 and the exact contribution to the gain func-tion is computed using the path posterior probabil-ities at orders n = 1 and n = 2.30We now analyse the efficiency of lattice MBRdecoding using the exact path posterior probabil-ities of Equation (2) at all orders.
We note thatthe sequential method and both simultaneous im-plementations using path counting transducers ?Lnand ?Rn yield the same hypotheses (allowing fornumerical accuracy); they differ only in speed andmemory usage.Posteriors Efficiency Computation times forthe steps in LMBR are given in Table 2.
In calcu-lating path posterior n-gram probabilities p(u|E),we find that the use of ?Ln is more than twiceas slow as the sequential method.
This is due tothe difficulty of counting higher-order n-grams inlarge lattices.
?Ln is effective for counting uni-grams, however, since there are far fewer of them.Using ?Rn is almost twice as fast as the sequentialmethod.
This speed difference is due to the sim-ple forward algorithm.
We also observe that forhigher-order n, the composition En ?
?Rn requiresless memory and produces a smaller machine thanEn ?
?Ln .
It is easier to count paths by the finaloccurrence of a symbol than by the first.Decoding Efficiency Decoding times are signif-icantly faster using ?n than the sequential method;average decoding time is around 0.1 seconds persentence.
The total time required for lattice MBRis dominated by the calculation of the path pos-terior n-gram probabilities, and this is a func-tion of the number of n-grams in the lattice |N |.For each sentence in mt0205tune, Figure 3 plotsthe total LMBR time for the sequential method(marked ?o?)
and for probabilities computed using?Rn (marked ?+?).
This compares the two tech-niques on a sentence-by-sentence basis.
As |N |grows, the simultaneous path counting transduceris found to be much more efficient.7 ConclusionWe have described an efficient and exact imple-mentation of the linear approximation to LMBRusing general WFST operations.
A simple trans-ducer was used to map words to sequences of n-grams in order to simplify the extraction of higher-order statistics.
We presented a counting trans-ducer ?Rn that extracts the statistics required forall n-grams of order n in a single composition andallows path posterior probabilities to be computedefficiently using a modified forward procedure.We take the view that even approximate search0 1000 2000 3000 4000 5000 6000010203040506070sequentialsimultaneous ?Rntotaltime(seconds)lattice n-gramsFigure 3: Total time in seconds versus |N |.criteria should be implemented exactly where pos-sible, so that it is clear exactly what the system isdoing.
For machine translation lattices, conflat-ing the values of p(u|E) and c(u|E) for higher-order n-grams might not be a serious problem, butin other scenarios ?
especially where symbol se-quences are repeated multiple times on the samepath ?
it may be a poor approximation.We note that since much of the time in calcula-tion is spent dealing with ?-arcs that are ultimatelyremoved, an optimised composition algorithm thatskips over such redundant structure may lead tofurther improvements in time efficiency.AcknowledgmentsThis work was supported in part under theGALE program of the Defense Advanced Re-search Projects Agency, Contract No.
HR0011-06-C-0022.ReferencesCyril Allauzen, Mehryar Mohri, and Brian Roark.2003.
Generalized algorithms for constructing sta-tistical language models.
In Proceedings of the 41stMeeting of the Association for Computational Lin-guistics, pages 557?564.Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-jciech Skut, and Mehryar Mohri.
2007.
OpenFst: ageneral and efficient weighted finite-state transducerlibrary.
In Proceedings of the 9th International Con-ference on Implementation and Application of Au-tomata, pages 11?23.
Springer.Cyril Allauzen, Shankar Kumar, Wolfgang Macherey,Mehryar Mohri, and Michael Riley.
2010.
Expected31sequence similarity maximization.
In Human Lan-guage Technologies 2010: The 11th Annual Confer-ence of the North American Chapter of the Associ-ation for Computational Linguistics, Los Angeles,California, June.Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.Och, and Jeffrey Dean.
2007.
Large languagemodels in machine translation.
In Proceedings ofthe 2007 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning, pages 858?867.Yonggang Deng and William Byrne.
2008.
HMMword and phrase alignment for statistical machinetranslation.
IEEE Transactions on Audio, Speech,and Language Processing, 16(3):494?507.Jason Eisner.
2002.
Parameter estimation for prob-abilistic finite-state transducers.
In Proceedings ofthe 40th Annual Meeting of the Association for Com-putational Linguistics (ACL), pages 1?8, Philadel-phia, July.David Graff, Junbo Kong, Ke Chen, and KazuakiMaeda.
2007.
English Gigaword Third Edition.Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,and William Byrne.
2009.
Hierarchical phrase-based translation with weighted finite state trans-ducers.
In Proceedings of Human Language Tech-nologies: The 2009 Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics, pages 433?441, Boulder, Col-orado, June.
Association for Computational Linguis-tics.R.
Kneser and H. Ney.
1995.
Improved backing-off form-gram language modeling.
In Acoustics, Speech,and Signal Processing, pages 181?184.Shankar Kumar and William Byrne.
2004.
MinimumBayes-risk decoding for statistical machine trans-lation.
In Proceedings of Human Language Tech-nologies: The 2004 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, pages 169?176.Shankar Kumar, Wolfgang Macherey, Chris Dyer, andFranz Och.
2009.
Efficient minimum error ratetraining and minimum bayes-risk decoding for trans-lation hypergraphs and lattices.
In Proceedings ofthe Joint Conference of the 47th Annual Meeting ofthe Association for Computational Linguistics andthe 4th International Joint Conference on NaturalLanguage Processing of the AFNLP, pages 163?171, Suntec, Singapore, August.
Association forComputational Linguistics.M.
Mohri, F.C.N.
Pereira, and M. Riley.
2008.
Speechrecognition with weighted finite-state transducers.Handbook on Speech Processing and Speech Com-munication.Michael Riley, Cyril Allauzen, and Martin Jansche.2009.
OpenFst: An Open-Source, Weighted Finite-State Transducer Library and its Applications toSpeech and Language.
In Proceedings of HumanLanguage Technologies: The 2009 Annual Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics, Companion Vol-ume: Tutorial Abstracts, pages 9?10, Boulder, Col-orado, May.
Association for Computational Linguis-tics.Roy Tromble, Shankar Kumar, Franz Och, and Wolf-gang Macherey.
2008.
Lattice Minimum Bayes-Risk decoding for statistical machine translation.In Proceedings of the 2008 Conference on Empiri-cal Methods in Natural Language Processing, pages620?629, Honolulu, Hawaii, October.
Associationfor Computational Linguistics.32
