Proceedings of the 12th Conference of the European Chapter of the ACL, pages 238?245,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsEffects of Word Confusion Networks on Voice SearchJunlan Feng, Srinivas BangaloreAT&T Labs-ResearchFlorham Park, NJ, USAjunlan,srini@research.att.comAbstractMobile voice-enabled search is emergingas one of the most popular applicationsabetted by the exponential growth in thenumber of mobile devices.
The automaticspeech recognition (ASR) output of thevoice query is parsed into several fields.Search is then performed on a text corpusor a database.
In order to improve the ro-bustness of the query parser to noise in theASR output, in this paper, we investigatetwo different methods to query parsing.Both methods exploit multiple hypothesesfrom ASR, in the form of word confusionnetworks, in order to achieve tighter cou-pling between ASR and query parsing andimproved accuracy of the query parser.
Wealso investigate the results of this improve-ment on search accuracy.
Word confusion-network based query parsing outperformsASR 1-best based query-parsing by 2.7%absolute and the search performance im-proves by 1.8% absolute on one of our datasets.1 IntroductionLocal search specializes in serving geographi-cally constrained search queries on a structureddatabase of local business listings.
Most text-based local search engines provide two text fields:the ?SearchTerm?
(e.g.
Best Chinese Restau-rant) and the ?LocationTerm?
(e.g.
a city, state,street address, neighborhood etc.).
Most voice-enabled local search dialog systems mimic thistwo-field approach and employ a two-turn dia-log strategy.
The dialog system solicits from theuser a LocationTerm in the first turn followed by aSearchTerm in the second turn (Wang et al, 2008).Although the two-field interface has beenwidely accepted, it has several limitations for mo-bile voice search.
First, most mobile devices arelocation-aware which obviates the need to spec-ify the LocationTerm.
Second, it?s not alwaysstraightforward for users to be aware of the dis-tinction between these two fields.
It is com-mon for users to specify location information inthe SearchTerm field.
For example, ?restaurantsnear Manhattan?
for SearchTerm and ?NY NY?for LocationTerm.
For voice-based search, it ismore natural for users to specify queries in a sin-gle utterance1.
Finally, many queries often con-tain other constraints (assuming LocationTerm is aconstraint) such as that deliver in restaurants thatdeliver or open 24 hours in night clubs open 24hours.
It would be very cumbersome to enumerateeach constraint as a different text field or a dialogturn.
An interface that allows for specifying con-straints in a natural language utterance would bemost convenient.In this paper, we introduce a voice-based searchsystem that allows users to specify search requestsin a single natural language utterance.
The out-put of ASR is then parsed by a query parserinto three fields: LocationTerm, SearchTerm,and Filler.
We use a local search engine,http://www.yellowpages.com/, which accepts theSearchTerm and LocationTerm as two query fieldsand returns the search results from a business list-ings database.
We present two methods for pars-ing the voice query into different fields with par-ticular emphasis on exploiting the ASR output be-yond the 1-best hypothesis.
We demonstrate thatby parsing word confusion networks, the accuracyof the query parser can be improved.
We furtherinvestigate the effect of this improvement on thesearch task and demonstrate the benefit of tightercoupling of ASR and the query parser on searchaccuracy.The paper outline is as follows.
In Section 2, wediscuss some of the related threads of research rel-evant for our task.
In Section 3, we motivate theneed for a query parsing module in voice-basedsearch systems.
We present two different queryparsing models in Section 4 and Section 5 and dis-cuss experimental results in Section 6.
We sum-marize our results in Section 7.1Based on the returned results, the query may be refinedin subsequent turns of a dialog.2382 Related WorkThe role of query parsing can be considered assimilar to spoken language understanding (SLU)in dialog applications.
However, voice-basedsearch systems currently do not have SLU as aseparate module, instead the words in the ASR1-best output are directly used for search.
Mostvoice-based search applications apply a conven-tional vector space model (VSM) used in infor-mation retrieval systems for search.
In (Yu et al,2007), the authors enhanced the VSM by deem-phasizing term frequency in Listing Names andusing character level instead of word level uni/bi-gram terms to improve robustness to ASR errors.While this approach improves recall it does notimprove precision.
In other work (Natarajan etal., 2002), the authors proposed a two-state hiddenMarkov model approach for query understandingand speech recognition in the same step (Natarajanet al, 2002).There are two other threads of research liter-ature relevant to our work.
Named entity (NE)extraction attempts to identify entities of interestin speech or text.
Typical entities include loca-tions, persons, organizations, dates, times mon-etary amounts and percentages (Kubala et al,1998).
Most approaches for NE tasks rely on ma-chine learning approaches using annotated data.These algorithms include a hidden Markov model,support vector machines, maximum entropy, andconditional random fields.
With the goal of im-proving robustness to ASR errors, (Favre et al,2005) described a finite-state machine based ap-proach to take as input ASR n-best strings and ex-tract the NEs.
Although our task of query segmen-tation has similarity with NE tasks, it is arguablewhether the SearchTerm is a well-defined entity,since a user can provide varied expressions as theywould for a general web search.
Also, it is notclear how the current best performing NE methodsbased on maximum entropy or conditional ran-dom fields models can be extended to apply onweighted lattices produced by ASR.The other related literature is natural languageinterface to databases (NLIDBs), which had beenwell-studied during 1960s-1980s (Androutsopou-los, 1995).
In this research, the aim is to mapa natural language query into a structured querythat could be used to access a database.
However,most of the literature pertains to textual queries,not spoken queries.
Although in its full general-1?bestWCN QueryParsedQueryParserSpeech SearchASRFigure 1: Architecture of a voice-based search sys-temity the task of NLIDB is significantly more ambi-tious than our current task, some of the challeng-ing problems (e.g.
modifier attachment in queries)can also be seen in our task as well.3 Voice-based Search SystemArchitectureFigure 1 illustrates the architecture of our voice-based search system.
As expected the ASR andSearch components perform speech recognitionand search tasks.
In addition to ASR and Search,we also integrate a query parsing module betweenASR and Search for a number of reasons.First, as can be expected the ASR 1-best out-put is typically error-prone especially when a userquery originates from a noisy environment.
How-ever, ASR word confusion networks which com-pactly encode multiple word hypotheses with theirprobabilities have the potential to alleviate the er-rors in a 1-best output.
Our motivation to intro-duce the understanding module is to rescore theASR output for the purpose of maximizing searchperformance.
In this paper, we show promisingresults using richer ASR output beyond 1-best hy-pothesis.Second, as mentioned earlier, the query parsernot only provides the search engine ?what?
and?where?
information, but also segments the queryto phrases of other concepts.
For the example weused earlier, we segment night club open 24 hoursinto night club and open 24 hours.
Query seg-mentation has been considered as a key step toachieving higher retrieval accuracy (Tan and Peng,2008).Lastly, we prefer to reuse an existing localsearch engine http://www.yellowpages.com/, inwhich many text normalization, task specific tun-ing, business rules, and scalability issues havebeen well addressed.
Given that, we need a mod-ule to translate ASR output to the query syntax thatthe local search engine supports.In the next section, we present our proposed ap-proaches of how we parse ASR output includingASR 1-best string and lattices in a scalable frame-work.2394 Text Indexing and Search-based Parser(PARIS)As we discussed above, there are many potentialapproaches such as those for NE extraction we canexplore for parsing a query.
In the context of voicelocal search, users expect overall system responsetime to be similar to that of web search.
Con-sequently, the relatively long ASR latency leavesno room for a slow parser.
On the other hand,the parser needs to be tightly synchronized withchanges in the listing database, which is updatedat least once a day.
Hence, the parser?s trainingprocess also needs to be quick to accomodate thesechanges.
In this section, we propose a probabilis-tic query parsing approach called PARIS (parsingusing indexing and search).
We start by presentinga model for parsing ASR 1-best and extend the ap-proach to consider ASR lattices.4.1 Query Parsing on ASR 1-best output4.1.1 The ProblemWe formulate the query parsing task as follows.A 1-best ASR output is a sequence of words:Q = q1, q2, .
.
.
, qn.
The parsing task is tosegment Q into a sequence of concepts.
Eachconcept can possibly span multiple words.
LetS = s1, s2, .
.
.
, sk, .
.
.
, sm be one of the possiblesegmentations comprising of m segments, wheresk = qij = qi, .
.
.
qj , 1 ?
i ?
j ?
n + 1.
Thecorresponding concept sequence is represented asC = c1, c2, .
.
.
, ck, .
.
.
, cm.For a given Q, we are interested in searchingfor the best segmentation and concept sequence(S?, C?)
as defined by Equation 1, which is rewrit-ten using Bayes rule as Equation 2.
The priorprobability P (C) is approximated using an h-gram model on the concept sequence as shownin Equation 3.
We model the segment sequencegeneration probability P (S|C) as shown in Equa-tion 4, using independence assumptions.
Finally,the query terms corresponding to a segment andconcept are generated using Equations 5 and 6.
(S?, C?)
= argmaxS,CP (S,C) (1)= argmaxS,CP (C) ?
P (S|C) (2)P (C) = P (c1) ?m?iP (ci|ci?h+1i?1 ) (3)P (S|C) =m?k=1P (sk | ck) (4)P (sk|ck) = P (qij |ck) (5)P (qij |ck) = Pck(qi) ?j?l=i+1Pck(ql | ql?k+1l?1 ) (6)To train this model, we only have access to textquery logs from two distinct fields (SearchTerm,LocationTerm) and the business listing database.We built a SearchTerm corpus by including validqueries that users typed to the SearchTerm fieldand all the unique business listing names in thelisting database.
Valid queries are those queriesfor which the search engine returns at least onebusiness listing result or a business category.
Sim-ilarly, we built a corpus for LocationTerm by con-catenating valid LocationTerm queries and uniqueaddresses including street address, city, state, andzip-code in the listing database.
We also built asmall corpus for Filler, which contains commoncarrier phrases and stop words.
The generationprobabilities as defined in 6 can be learned fromthese three corpora.In the following section, we describe a scalableway of implementation using standard text indexerand searcher.4.1.2 Probabilistic Parsing using Text SearchWe use Apache-Lucene (Hatcher and Gospod-netic, 2004), a standard text indexing and searchengines for query parsing.
Lucene is an open-source full-featured text search engine library.Both Lucene indexing and search are efficientenough for our tasks.
It takes a few millisecondsto return results for a common query.
Indexingmillions of search logs and listings can be donein minutes.
Reusing text search engines allowsa seamless integration between query parsing andsearch.We changed the tf.idf based document-termrelevancy metric in Lucene to reflect P (qij |ck) us-ing Relevancy as defined below.P (qij |ck) = Relevancy(qij , dk) =tf(qij , dk) + ?N(7)where dk is a corpus of examples we collected forthe concept ck; tf(qij , dk) is referred as the termfrequency, the frequency of qij in dk;N is the num-ber of entries in dk; ?
is an empirically determinedsmoothing factor.2400 1gary/0.323cherry/4.104dairy/1.442jerry/3.9562crites/0.652christ/2.857creek/3.872queen/1.439kreep/4.540kersten/2.0453springfield/0.303in/1.346 4springfield/1.367_epsilon/0.294 5/1missouri/7.021Figure 2: An example confusion network for ?Gary crities Springfield Missouri?Inputs:?
A set of K concepts:C = c1, c2, .
.
.
, cK ,in this paper, K = 3, c1 =SearchTerm, c2 = LocationTerm,c3 = Filler?
Each concept ck associates with a textcorpus: dk.
Corpora are indexed usingLucene Indexing.?
A given query: Q = q1, q2, .
.
.
, qn?
A given maximum number of words in aquery segment: NgParsing:?
Enumerate possible segments in Q up toNg words long: qij = qi, qi+1, .
.
.
, qj ,j >= i, |j ?
i| < Ng?
Obtain P (qij |ck)) for each pair of ck andqij using Lucene Search?
Boost P (qij |ck)) based on the position ofqij in the query P (qij |ck) = P (qij |ck) ?boostck(i, j, n)?
Search for the best segment sequenceand concept sequence using ViterbisearchFig.3.
Parsing procedure using Text Indexer andSearcherpck(qij) =tf(qii ?
dis(i, j), dk) + ?N ?
shift(8)When tf(qij , dk) is zero for all concepts, weloosen the phrase search to be proximity search,which searches words in qij within a specific dis-tance.
For instance, ?burlington west virginia?
?5 will find entries that include these three wordswithin 5 words of each other.
tf(qij , dk) is dis-counted for proximity search.
For a given qij , weallow a distance of dis(i, j) = (j ?
i + shift)words.
shift is a parameter that is set empirically.The discounting formula is given in 8.Figure 3 shows the procedure we use for pars-ing.
It enumerates possible segments qij of a givenQ.
It then obtains P (qij |ck) using Lucene Search.We boost pck(qij)) based on the position of qij inQ.
In our case, we simply set: boostck(i, j, n) = 3if j = n and ck = LocationTerm.
Other-wise, boostck(i, j, n) = 1.
The algorithm searchesfor the best segmentation using the Viterbi algo-rithm.
Out-of-vocabulary words are assigned to c3(Filler).4.2 Query Parsing on ASR LatticesWord confusion networks (WCNs) is a compactlattice format (Mangu et al, 2000).
It aligns aspeech lattice with its top-1 hypothesis, yieldinga ?sausage?-like approximation of lattices.
It hasbeen used in applications such as word spottingand spoken document retrieval.
In the following,we present our use of WCNs for query parsingtask.Figure 2 shows a pruned WCN example.
Foreach word position, there are multiple alternativesand their associated negative log posterior proba-bilities.
The 1-best path is ?Gary Crites Spring-field Missouri?.
The reference is ?Dairy Queenin Springfield Missouri?.
ASR misrecognized?Dairy Queen?
as ?Gary Crities?.
However, thecorrect words ?Dairy Queen?
do appear in the lat-tice, though with lower probability.
The challengeis to select the correct words from the lattice byconsidering both ASR posterior probabilities andparser probabilities.The hypotheses in WCNs have to be reranked241by the Query Parser to prefer those that havemeaningful concepts.
Clearly, each business namein the listing database corresponds to a single con-cept.
However, the long queries from query logstend to contain multiple concepts.
For example, afrequent query is ?night club for 18 and up?.
Weknow ?night club?
is the main subject.
And ?18and up?
is a constraint.
Without matching ?nightclub?, any match with ?18 and up?
is meaning-less.
The data fortunately can tell us which wordsare more likely to be a subject.
We rarely see ?18and up?
as a complete query.
Given these observa-tions, we propose calculating the probability of aquery term to be a subject.
?Subject?
here specif-ically means a complete query or a listing name.For the example shown in Figure 2, we observe thenegative log probability for ?Dairy Queen?
to be asubject is 9.3.
?Gary Crites?
gets 15.3.
We referto this probability as subject likelihood.
Given acandidate query term s = w1, w2, ..wm, we repre-sent the subject likelihood as Psb(s).
In our exper-iments, we estimate Psb using relative frequencynormorlized by the length of s. We use the follow-ing formula to combine it with posterior probabil-ities in WCNs Pcf (s):P (s) = Pcf (s) ?
Psb(s)?Pcf (s) =?j=1,...,nwPcf (wi)where ?
is used to flatten ASR posterior proba-bilities and nw is the number of words in s. Inour experiments, ?
is set to 0.5.
We then re-rankASR outputs based on P (s).
We will report ex-perimental results with this approach.
?Subject?is only related to SearchTerm.
Considering this,we parse the ASR 1-best out first and keep theLocation terms extracted as they are.
Only wordalternatives corresponding to the search terms areused for reranking.
This also improves speed,since we make the confusion network lattice muchsmaller.
In our initial investigations, such an ap-proach yields promising results as illustrated in theexperiment section.Another capability that the parser does for bothASR 1-best and lattices is spelling correction.
Itcorrects words such as restaurants to restaurants.ASR produces spelling errors because the lan-guage model is trained on query logs.
We needto make more efforts to clean up the query logdatabase, though progresses had been made.5 Finite-state Transducer-based ParserIn this section, we present an alternate method forparsing which can transparently scale to take as in-put word lattices from ASR.
We encode the prob-lem of parsing as a weighted finite-state transducer(FST).
This encoding allows us to apply the parseron ASR 1-best as well as ASR WCNs using thecomposition operation of FSTs.We formulate the parsing problem as associat-ing with each token of the input a label indicatingwhether that token belongs to one of a businesslisting (bl), city/state (cs) or neither (null).
Thus,given a word sequence (W = w1, .
.
.
, wn) outputfrom ASR, we search of the most likely label se-quence (T = t1, .
.
.
, tn), as shown in Equation 9.We use the joint probability P (W,T ) and approx-imate it using an k-gram model as shown in Equa-tions 10,11.T ?
= argmaxTP (T |W ) (9)= argmaxTP (W,T ) (10)= argmaxTn?iP (wi, ti | wi?k+1i?1 , ti?k+1i?1 )(11)A k-gram model can be encoded as a weightedfinite-state acceptor (FSA) (Allauzen et al, 2004).The states of the FSA correspond to the k-gramhistories, the transition labels to the pair (wi, ti)and the weights on the arcs are ?log(P (wi, ti |wi?k+1i?1 , ti?k+1i?1 )).
The FSA also encodes back-offarcs for purposes of smoothing with lower order k-grams.
An annotated corpus of words and labels isused to estimate the weights of the FSA.
A samplecorpus is shown in Table 1.1. pizza bl hut bl new cs york cs new csyork cs2.
home bl depot bl around nullsan cs francisco cs3.
please null show null me null indian blrestaurants bl in null chicago cs4.
pediatricians bl open null on nullsundays null5.
hyatt bl regency bl in null honolulu cshawaii csTable 1: A Sample set of annotated sentences242The FSA on the joint alphabet is converted intoan FST.
The paired symbols (wi, ti) are reinter-preted as consisting of an input symbol wi andoutput symbol ti.
The resulting FST (M ) is usedto parse the 1-best ASR (represented as FSTs(I)), using composition of FSTs and a search forthe lowest weight path as shown in Equation 12.The output symbol sequence (pi2) from the lowestweight path is T ?.T ?
= pi2(Bestpath(I ?M)) (12)Equation 12 shows a method for parsing the 1-best ASR output using the FST.
However, a simi-lar method can be applied for parsing WCNs.
TheWCN arcs are associated with a posterior weightthat needs to be scaled suitably to be comparableto the weights encoded in M .
We represent the re-sult of scaling the weights in WCN by a factor of?
asWCN?.
The value of the scaling factor is de-termined empirically.
Thus the process of parsinga WCN is represented by Equation 13.T ?
= pi2(Bestpath(WCN?
?M)) (13)6 ExperimentsWe have access to text query logs consisting of 18million queries to the two text fields: SearchTermand LocationTerm.
In addition to these logs, wehave access to 11 million unique business listingnames and their addresses.
We use the combineddata to train the parameters of the two parsingmodels as discussed in the previous sections.
Wetested our approaches on three data sets, which intotal include 2686 speech queries.
These querieswere collected from users using mobile devicesfrom different time periods.
Labelers transcribedand annotated the test data using SearchTerm andLocationTerm tags.Data Sets Number of WACCSpeech QueriesTest1 1484 70.1%Test2 544 82.9%Test3 658 77.3%Table 2: ASR Performance on three Data SetsWe use an ASR with a trigram-based languagemodel trained on the query logs.
Table 2 shows theASR word accuracies on the three data sets.
Theaccuracy is the lowest on Test1, in which manyusers were non-native English speakers and a largepercentage of queries are not intended for localsearch.We measure the parsing performance in termsof extraction accuracy on the two non-filler slots:SearchTerm and LocationTerm.
Extraction accu-racy computes the percentage of the test set wherethe string identified by the parser for a slot is ex-actly the same as the annotated string for that slot.Table 3 reports parsing performance using thePARIS approach for the two slots.
The ?Tran-scription?
columns present the parser?s perfor-mances on human transcriptions (i.e.
word ac-curacy=100%) of the speech.
As expected, theparser?s performance heavily relies on ASR wordaccuracy.
We achieved lower parsing perfor-mance on Test1 compared to other test sets dueto lower ASR accuracy on this test set.
Thepromising aspect is that we consistently improvedSearchTerm extraction accuracy when usingWCNas input.
The performance under ?Oracle path?column shows the upper bound for the parser us-ing the oracle path2 from the WCN.
We prunedthe WCN by keeping only those arcs that arewithin cthresh of the lowest cost arc betweentwo states.
Cthresh = 4 is used in our experi-ments.
For Test2, the upper bound improvementis 7.6% (82.5%-74.9%) absolute.
Our proposedapproach using pruned WCN achieved 2.7% im-provement, which is 35% of the maximum poten-tial gain.
We observed smaller improvements onTest1 and Test3.
Our approach did not take advan-tage of WCN for LocationTerm extraction, hencewe obtained the same performance with WCNs asusing ASR 1-best.In Table 4, we report the parsing performancefor the FST-based approach.
We note that theFST-based parser on a WCN also improves theSearchTerm and LocationTerm extraction accu-racy over ASR 1-best, an improvement of about1.5%.
The accuracies on the oracle path and thetranscription are slightly lower with the FST-basedparser than with the PARIS approach.
The per-formance gap, however, is bigger on ASR 1-best.The main reason is PARIS has embedded a modulefor spelling correction that is not included in theFST approach.
For instance, it corrects nieman toneiman.
These improvements from spelling cor-rection don?t contribute much to search perfor-2Oracle text string is the path in the WCN that is closestto the reference string in terms of Levenshtein edit distance243Data Sets SearchTerm Extraction Accuracy LocationTerm Extraction AccuracyInput ASR WCN Oracle Transcription ASR WCN Oracle Transcription1-best Path 4 1best Path 4Test1 60.0% 60.7% 67.9% 94.1% 80.6% 80.6% 85.2% 97.5%Test2 74.9% 77.6% 82.5% 98.6% 89.0% 89.0% 92.8% 98.7%Test3 64.7% 65.7% 71.5% 96.7% 88.8% 88.8% 90.5% 97.4%Table 3: Parsing performance using the PARIS approachData Sets SearchTerm Extraction Accuracy LocationTerm Extraction AccuracyInput ASR WCN Oracle Transcription ASR WCN Oracle Transcription1-best Path 4 1best Path 4Test1 56.9% 57.4% 65.6% 92.2% 79.8% 79.8% 83.8% 95.1%Test2 69.5% 71.0% 81.9% 98.0% 89.4% 89.4% 92.7% 98.5%Test3 59.2% 60.6% 69.3% 96.1% 87.1% 87.1% 89.3% 97.3%Table 4: Parsing performance using the FST approachmance as we will see below, since the search en-gine is quite robust to spelling errors.
ASR gen-erates spelling errors because the language modelis trained using query logs, where misspellings arefrequent.We evaluated the impact of parsing perfor-mance on search accuracy.
In order to measuresearch accuracy, we need to first collect a ref-erence set of search results for our test utter-ances.
For this purpose, we submitted the hu-man annotated two-field data to the search engine(http://www.yellowpages.com/ ) and extracted thetop 5 results from the returned pages.
The re-turned search results are either business categoriessuch as ?Chinese Restaurant?
or business listingsincluding business names and addresses.
We con-sidered these results as the reference search resultsfor our test utterances.In order to evaluate our voice search system, wesubmitted the two fields resulting from the queryparser on the ASR output (1-best/WCN) to thesearch engine.
We extracted the top 5 results fromthe returned pages and we computed the Precision,Recall and F1 scores between this set of resultsand the reference search set.
Precision is the ra-tio of relevant results among the top 5 results thevoice search system returns.
Recall refers to theratio of relevant results to the reference search re-sult set.
F1 combines precision and recall as: (2* Recall * Precision) / (Recall + Precision) (vanRijsbergen, 1979).In Table 5 and Table 6, we report the search per-formance using PARIS and FST approaches.
Theoverall improvement in search performance is notData Sets Precision Recall F1ASR Test1 71.8% 66.4% 68.8%1-bestTest2 80.7% 76.5% 78.5%Test3 72.9% 68.8% 70.8%WCNTest1 70.8% 67.2% 69.0%Test2 81.6% 79.0% 80.3%Test3 73.0% 69.1% 71.0%Table 5: Search performances using the PARIS ap-proachData Sets Precision Recall F1ASR Test1 71.6% 64.3% 67.8%1-bestTest2 79.6% 76.0% 77.7%Test3 72.9% 67.2% 70.0%WCNTest1 70.5% 64.7% 67.5%Test2 80.3% 77.3% 78.8%Test3 72.9% 68.1% 70.3%Table 6: Search performances using the FST ap-proachas large as the improvement in the slot accura-cies between using ASR 1-best and WCNs.
OnTest1, we obtained higher recall but lower preci-sion with WCN resulting in a slight decrease inF1 score.
For both approaches, we observed thatusing WCNs consistently improves recall but notprecision.
Although this might be counterintu-itive, given that WCNs improve the slot accuracyoverall.
One possible explanation is that we haveobserved errors made by the parser using WCNsare more ?severe?
in terms of their relationship tothe original queries.
For example, in one particular244case, the annotated SearchTerm is ?book stores?,for which the ASR 1-best-based parser returned?books?
(due to ASR error) as the SearchTerm,while the WCN-based parser identified ?banks?as the SearchTerm.
As a result, the returned re-sults from the search engine using the 1-best-basedparser were more relevant compared to the resultsreturned by the WCN-based parser.There are few directions that this observationsuggests.
First, the weights on WCNs may needto be scaled suitably to optimize the search per-formance as opposed to the slot accuracy perfor-mance.
Second, there is a need for tighter cou-pling between the parsing and search componentsas the eventual goal for models of voice search isto improve search accuracy and not just the slotaccuracy.
We plan to investigate such questions infuture work.7 SummaryThis paper describes two methods for query pars-ing.
The task is to parse ASR output including 1-best and lattices into database or search fields.
Inour experiments, these fields are SearchTerm andLocationTerm for local search.
Our first method,referred to as PARIS, takes advantage of a genericsearch engine (for text indexing and search) forparsing.
All probabilities needed are retrieved on-the-fly.
We used keyword search, phrase searchand proximity search.
The second approach, re-ferred to as FST-based parser, which encodes theproblem of parsing as a weighted finite-state trans-duction (FST).
Both PARIS and FST successfullyexploit multiple hypotheses and posterior proba-bilities from ASR encoded as word confusion net-works and demonstrate improved accuracy.
Theseresults show the benefits of tightly coupling ASRand the query parser.
Furthermore, we evaluatedthe effects of this improvement on search perfor-mance.
We observed that the search accuracy im-proves using word confusion networks.
However,the improvement on search is less than the im-provement we obtained on parsing performance.Some improvements the parser achieves do notcontribute to search.
This suggests the need ofcoupling the search module and the query parseras well.The two methods, namely PARIS and FST,achieved comparable performances on search.One advantage with PARIS is the fast trainingprocess, which takes minutes to index millionsof query logs and listing entries.
For the sameamount of data, FST needs a number of hours totrain.
The other advantage is PARIS can easilyuse proximity search to loosen the constrain of N-gram models, which is hard to be implementedusing FST.
FST, on the other hand, does bettersmoothing on learning probabilities.
It can alsomore directly exploit ASR lattices, which essen-tially are represented as FST too.
For future work,we are interested in ways of harnessing the bene-fits of the both these approaches.ReferencesC.
Allauzen, M. Mohri, M. Riley, and B. Roark.
2004.A generalized construction of speech recognitiontransducers.
In ICASSP, pages 761?764.I.
Androutsopoulos.
1995.
Natural language interfacesto databases - an introduction.
Journal of NaturalLanguage Engineering, 1:29?81.B.
Favre, F. Bechet, and P. Nocera.
2005.
Robustnamed entity extraction from large spoken archives.In Proceeding of HLT 2005.E.
Hatcher and O. Gospodnetic.
2004.
Lucene in Ac-tion (In Action series).
Manning Publications Co.,Greenwich, CT, USA.F.
Kubala, R. Schwartz, R. Stone, and R. Weischedel.1998.
Named entity extraction from speech.
In inProceedings of DARPA Broadcast News Transcrip-tion and Understanding Workshop, pages 287?292.L.
Mangu, E. Brill, and A. Stolcke.
2000.
Finding con-sensus in speech recognition: Word error minimiza-tion and other applications of confusion networks.Computation and Language, 14(4):273?400, Octo-ber.P.
Natarajan, R. Prasad, R.M.
Schwartz, andJ.
Makhoul.
2002.
A scalable architecture for di-rectory assistance automation.
In ICASSP 2002.B.
Tan and F. Peng.
2008.
Unsupervised query seg-mentation using generative language models andwikipedia.
In Proceedings of WWW-2008.C.V.
van Rijsbergen.
1979.
Information Retrieval.Boston.
Butterworth, London.Y.
Wang, D. Yu, Y. Ju, and A. Alex.
2008.
An intro-duction to voice search.
Signal Processing Magzine,25(3):29?38.D.
Yu, Y.C.
Ju, Y.Y.
Wang, G. Zweig, and A. Acero.2007.
Automated directory assistance system - fromtheory to practice.
In Interspeech.245
