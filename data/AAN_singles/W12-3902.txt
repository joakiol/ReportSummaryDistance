Proceedings of the First Workshop on Multilingual Modeling, pages 11?17,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsLanguage-Independent Named Entity Identification using WikipediaMahathi BhagavatulaSearch andInformation Extraction LabIIIT Hyderabadmahathi.b@research.iiit.ac.inSantosh GSKSearch andInformation Extraction LabIIIT Hyderabadsantosh.gsk@research.iiit.ac.inVasudeva VarmaSearch andInformation Extraction LabIIIT Hyderabadvv@iiit.ac.inAbstractRecognition of Named Entities (NEs) is a dif-ficult process in Indian languages like Hindi,Telugu, etc., where sufficient gazetteers andannotated corpora are not available comparedto English language.
This paper details a novelclustering and co-occurrence based approachto map English NEs with their equivalent rep-resentations from different languages recog-nized in a language-independent way.
Wehave substituted the required language specificresources by the richly structured multilin-gual content of Wikipedia.
The approach in-cludes clustering of highly similar Wikipediaarticles.
Then the NEs in an English articleare mapped with other language terms in in-terlinked articles based on co-occurrence fre-quencies.
The cluster information and theterm co-occurrences are considered in ex-tracting the NEs from non-English languages.Hence, the English Wikipedia is used to boot-strap the NEs for other languages.
Throughthis approach, we have availed the structured,semi-structured and multilingual content ofthe Wikipedia to a massive extent.
Experi-mental results suggest that the proposed ap-proach yields promising results in rates of pre-cision and recall.1 IntroductionNamed entity recognition (NER) is an importantsubtask of information extraction that seeks tolocate and classify atomic elements in text intopredefined categories such as the names of persons,organizations, locations, etc.The state-of-art NER systems for English pro-duce near-human performance.
However, fornon-English languages the state-of-art NER systemsperform below par.
And for languages that have alack of resources (e.g., Indian Languages) a NERsystem with a near-human performance is a distantfuture.NER systems so far developed involved linguisticgrammar-based techniques as well as statisticalmodels.
The grammar-based techniques requirelinguistic expertise and requires strenuous effortsto build a NER system for every new language.Such techniques can be safely avoided when thereis a requirement to build a generic NER systemfor several languages (e.g., Indian Languages).Statistical NER systems typically require a largeamount of manually annotated training data.
Withthe serious lack of such manually annotated trainingdata, the task of high-performance NER systemprojects as a major challenge for Indian languages.This paper focuses on building a generic-purposeNE identification system for Indian languages.Given the constraints for resource-poor languages,we restrain from developing a regular NE Recogni-tion system.
However, the goal here is to identifyas many NEs available in Indian languages withoutusing any language-dependent tools or resources.Wikipedia is a free, web-based, collaborative,multilingual encyclopedia.
There are 283 languageeditions available as of now.
Wikipedia has bothstructured (e.g., Infoboxes, Categories, Hyperlinks,11InterLanguage links, etc.)
and semi-structured(content and organization of the page) information.Hence, the richly linked structure of Wikipediapresent across several languages (e.g., English,Hindi, Marathi) has been used to build and enhancemany NLP applications including NE identificationsystems.
However, the existing approaches thatexploit Wikipedia for recognizing NEs concentratesonly on the structured parts which results in lessrecall.
Our approach concentrates on exploitingstructured and semi-structured parts of Wikipediaand hence yielding better results.The approach used is simple, efficient, easilyreproducible and can be extended to any languageas it doesn?t use any of the language specificresources.2 Related WorkWikipedia has been the subject of a considerableamount of research in recent years includingGabrilovich and Markovitch (2005), Milne etal.
(2006), Zesch et al (2007), Timothy Weale(2006) and Richman and Schone (2008).
The mostrelevant work to this paper are Kazama and Tori-sawa (2007), Toral and Munoz (2006), Cucerzan(2007), Richman and Schone (2008).
More detailsfollow, however it is worth noting that all knownprior research is fundamentally monolingual, oftendeveloping algorithms that can be adapted to otherlanguages pending availability of the appropriatesemantic resources.Toral and Munoz (2006) used Wikipedia tocreate lists of NE?s.
They used the first sentenceof Wikipedia articles as likely definitions of thearticle titles, and used them in attempting to classifythe titles as people, locations, organizations, ornone.
Unlike the method presented in our paper,their algorithm relied on WordNet (or an equivalentresource in another language).
The authors notedthat their results would need to pass a manualsupervision step before being useful for the NERtask, and thus did not evaluate their results in thecontext of a full NER system.Similarly, Kazama and Torisawa (2007) usedWikipedia, particularly the first sentence of each ar-ticle, to create lists of entities.
Rather than buildingentity dictionaries, associating words and phrasesto the classical NE tags (PERSON, LOCATION,etc.
), they used a noun phrase following the verbforms ?to be?
to derive a label.
For example, theyused the sentence ?Franz Fischler ... is an Austrianpolitician?
to associate the label ?politician?
to thesurface form ?Franz Fischler?.
They proceeded toshow that the dictionaries generated by their methodare useful when integrated into an NER system.It is to be noted that their technique relies upon apart-of-speech tagger.Cucerzan (2007), by contrast to the above,used Wikipedia primarily for Named Entity Dis-ambiguation, following the path of Bunescu andPasca (2006).
As in our paper, and unlike the abovementioned works, Cucerzan (2007) made use ofthe explicit Category information found withinWikipedia.
In particular, Category and related listderived data were key pieces of information usedto differentiate between various meanings of anambiguous surface form.
Cucerzan (2007) did notmake use of the Category information in identifyingthe class of a given entity.
It is to be noted that theNER component was not the focus of their research,and was specific to the English language.Richman and Schone (2008) emphasized onthe use of links between articles of different lan-guages, specifically between English (the largestand best linked Wikipedia) and other languages.The approach uses English Wikipedia structurenamely categories and hyperlinks to get NEs andthen use language specific tools to derive multilin-gual NEs.The following are the majors differences be-tween any of the above approaches to the approachfollowed in this paper.?
No language resource has been used at anystage of NE identification, unlike the above ap-proaches that used at least one of the languagedependent tools like dictionary, POS tagger,etc.?
Our approach utilized several aspects ofWikipedia (e.g., InterLanguage links, Cate-12gories, Sub-titles, Article Text), which has beenby far the best exploitation of various structuralaspects of Wikipedia.?
Language-independent mapping of mul-tilingual similar content (i.e., the paral-lel/comparable topics or sentences of differentlanguages) can be used as a reference to anyfuture work.
Further details can be found inthe Section 4.2.3 Wikipedia StructureFrom Wikipedia, we exploited the following threemajor units:Category links: These are the links from anarticle to ?Category?
pages, represented in the formof [[Category:Luzerne County, Pennsylvania]],[[Category:Rivers of Pennsylvania]], etc.InterLanguage links: Links from an articleto a presumably equivalent article in another lan-guage.
For example, in the English language article?History of India?, one finds a set of links including[[hi: ]].
In almost all cases, the articleslinked in this manner represent articles on the samesubject.Subtitles of the document: These are consid-ered to be semi-structured parts of a Wikipediaarticle.
Every page in Wikipedia consists of atitle and subtitles.
Considering the data below thesubtitles, they can be referred as subparts of thearticle.
For example, the article regarding JimmyWales has subtitles ?Early life and education?,?Career?, etc.4 ArchitectureThe system architecture involves 3 main steps andare detailed as follows:4.1 Related Document Clustering:Hierarchical clustering outputs a hierarchy, a struc-ture that is more informative than the unstructuredset of clusters returned by flat clustering.
This paperdeals with large amounts of semi-structured dataand requires structured clusters as output ratherthan unstructured clusters.
Moreover, specifying thenumber of clusters beforehand is difficult.
Hence,we prefer Hierarchical clustering over Flat clus-tering in rest of the paper.
Bottom-up algorithmscan reach a cluster configuration with a betterhomogeneity than Top-Down clustering.
Hence,we prefer bottom-up clustering over top-downclustering.Within bottom-up clustering there are severalsimilarity measures that can be employed namelysingle-linkage, complete-linkage, group-averageand centroid-measure.
This single-link mergecriterion is local.
Priority is given solely to the areawhere the two clusters come closest to each other.Other, more distant parts of the cluster and theclusters?
overall structure are not taken into account.In complete-link clustering or complete-linkageclustering, the similarity of two clusters is thesimilarity of their most dissimilar members.
Incentroid clustering, the similarity of two clustersis defined as the similarity of their centroids.Group-average agglomerative clustering or GAACevaluates cluster quality based on all similaritiesbetween documents, thus avoiding the pitfalls ofthe single-link and complete-link criteria.
Hence,in this paper, we made use of the Group-averageagglomerative clustering.We have considered the English Wikipedia ar-ticles which contain InterLanguage links to Hindiarticles.
The English articles are clustered based onthe overlap of terms, i.e., the number of commonterms present between articles.
The clusteringalgorithm is detailed as follows:Initially, consider English Wikipedia data, eacharticle in the dataset is considered as a singledocument cluster.
Now, the distance between twoclusters is calculated usingSIM-GA(?i, ?j) = 1(Ni+Nj)(Ni+Nj?1)?dm??i??j?dn??i?
?j ,dm 6=dn~dm ?
~dnwhere ~d is the length-normalized vector of documentd, ?
denotes the dot product, and Ni and Nj are thenumber of documents in ?i and ?j , respectively.
Us-ing group average agglomerative clustering, the pro-13cess is repeated till we reach a certain threshold (setto 0.2) and thus the hierarchical clusters of Englishdata are formed.
In order to cluster documents ofother languages, we availed the InterLanguage linksand structure of English clusters.
The InterLanguagelinks are used in replicating the cluster structure ofEnglish Wikipedia articles across other language ar-ticles.
Therefore, we avoided the repetition of theclustering step for non-English articles.
These dif-ferent language clusters, being interconnected, arefurther utilized in our approach.4.2 Mapping related content within interlinkeddocuments:As the clustering technique used is hierarchical,the intermediate clustering steps are gathered andare called as subclusters.
For example, if twoclusters (say Diseases, Hospitals) are merged toform a cluster (say Medicine).
Then the Diseases,Hospitals are called subclusters for the Medicinecluster.We measured the average of cosine similaritiesbetween the subtitle lists of the articles in a givencluster.
If the average similarity exceeds a threshold(set to 0.72), it would mean the articles in the cluster(e.g., Diseases) all share similar subtitles.
Other-wise, we go for a subcluster, until the thresholdcriteria is met.
E.g., any two articles of the clusterDiseases share the common subtitles like Symptomsof Disease, Causes, Precautions, etc.
This isillustrated in figure 1.
As per our observation, thearticles of different languages pertaining to samecluster will have same subtitles but depicted indifferent languages.
The Hindi articles of cluster?Diseases?
share the same subtitles with those inEnglish.
This is illustrated in figure 2.In order to map subtitles across languages, ineach cluster, consider the non-English article withmaximum number of subtitles and its correspondingEnglish article.
A lookup in a bilingual dictionarydeveloped by Rohit et al (2010) would help inmapping certain subtitles.
The rest of the subtitlesare mapped based on their order of occurrences.The subtitles are likely to occur at the same orderin interlinked articles with high number of sub-titles.
The dictionary is expanded by adding theFigure 1: Subtitles of Cancer and Multiple Sclerosismapped subtitles obtained from such interlinkedarticles.
This process is repeated with the remaininginterlinked articles.
Rohit et al had developedthe bilingual dictionary availing Wikipedia titlesand abstract information.
Hence, their approachis language-independent and doesn?t hinder ouralgorithm from being applied to other languages.Consider each subtitle of an article in a clusterand collect its subtitle data from that article andfrom its corresponding interlinked article in Hindi.For example, consider the subtitle ?Causes?, collectthe subtitle data from an English article (say Cancer)and map it with the subtitle data from the Hindiequivalent page on Cancer.
We now have a mappingtitled ?Causes - Cancer?
for the Cancer articlesacross languages.
Repeat this for all articles andgroup the mappings of common subtitles.
Then, amajor group ?Causes?
is formed.
This group willnow have a set of mappings like ?Causes - Cancer?,?Causes - Multiple Sclerosis?, etc.
Thus the multi-lingual grouping and mapping is done.
This stepmaps similar content of different languages.
Thisis one of the important contributions of the paperwhich has the potential to be applied elsewhere.4.3 Term co occurrences model:Consider a map (e.g., ?Causes - Cancer?)
whichcontains both English and Hindi data.
Given thefact that the usage of English tools doesn?t hurt theextensibility of the approach to other languages, theEnglish data is annotated with Stanford NER andthe NEs are retrieved.
Hindi data is preprocessedby removing the stop words.
The stop words list isgenerated by considering words that occur above acertain frequency in the overall dataset.14Figure 2: Subtitles of Cancer article across languagesFor a given map and preprocessed data, everyEnglish NE is paired with every non-tagged Hindiword.
Attach a default weight (=1) for each pair.Hence, a pair may look like (tagged English word,non tagged Hindi word, 1).
This step is repeatedwith all other mappings present in a group (Ex:?Causes - Cancer?, ?Causes - Multiple Sclerosis?in the group ?Causes?).
On repeated occurrence ofthe same pair, weight of that pair increases (by 1).Finally, for a English NE term, the Hindi term withwhich it has highest frequency is identified.
Thenthe NE tag of English term is assigned to Hinditerm.
Hence, Hindi word is labeled.
This step isrepeated with the remaining English NEs and Hinditerms.For example, consider two small mappings,each with two English NEs and one sentencein Hindi.
Consider the first map, with ?Alexan-der/PERSON?, ?India/LOCATION?
as English NEsandas Hindi sentence.
Then each NE of English isattached with each Hindi word (except the stopwords) like Alexander - , Alexander -, Alexander - , India - , etc., in allcombinations.
Consider the second map with?Alexander/PERSON?, ?Philip/PERSON?
as En-glish NEs and as Hindisentence.
The pairs would be Alexander - ,Alexander - etc.
Hence, the maximum co oc-curred pair would be Alexander - (Alexanderin Hindi).
Then the NE tag of Alexander/PERSONis attached to /PERSON.
Similarly, for theremaining English NEs and Hindi terms, the max-imum co-occurred pair is identified and the Hinditerm is tagged.5 Evaluation and Experimental setup:As our approach requires InterLanguage links, weare only interested in a subset of English and HindiWikipedia articles which are interconnected.
Thereare 22,300 articles in English and Hindi Wikipediathat have InterLanguage links.
The output of Hierar-chical GAAC clustering on this subset was observedto be 345 clusters.
We have manually tagged Hindiarticles of 50 random clusters (as cluster size candictate accuracies) with three NE tags (i.e., Person,Organization, Location), resulting in 2,328 Hindiarticles with around 11,000 NE tags.
All furtherexperiments were performed on this tagged dataset.Precision, Recall and F-measure are the evaluationmetrics used to estimate the performance of oursystem.In order to compare our system performancewith a baseline, we have availed the Hindi NERsystem developed by Gali et al (2008) at LTRC(Language Technologies Research Center) 1 thatrecognizes and annotates Hindi NEs in a giventext using Conditional Random Fields (CRF) asthe sequential labeling mechanism.
Their systemis reproduced on our dataset with a 5-fold crossvalidation using spell variations, pattern of suffixesand POS tagging as the features.6 Experiments and Results:The experiments conducted are broadly classified asfollows:Experiment 1: Using the structure of Wikipedianamely Category terms, we can cluster the articleswhich are having similar category terms.
Anotherapproach for clustering is to consider the Wikipediapage as an unstructured page and then cluster the ar-ticles based on the similarity of words present in it.We have performed Hierarchical GAAC based clus-tering for these experiments.Experiment 2: Different clustering metrics willyield different accuracies for a given data.
Here, wewill measure which similarity metric is appropriate1http://ltrc.iiit.ac.in15for the dataset under study following a Category in-formation based clustering of articles.6.1 Experiment 1: Whether to use structure ofthe Wikipedia page:No Category: Clustering without using the Cate-gory information: As the first experiment, the arti-cles are clustered based on the article text and notusing the category terms.With Category: Clustering using the Category in-formation: In this experiment, the category termsare used for clustering the documents.
The F-measure suggests that category terms better capturethe semantics of an article when compared to thetext of the article.
Adding to the fact that categoryterms suggest a compact representation of an articlewhereas the text include noisy terms.
The compactrepresentation of articles has proved to be crucial byour next set of experiments.Precision Recall F-measureNER LTRC 64.9 50.6 56.81No Category 69.8 62.7 66.05With Category 73.5 64.3 68.59Table 1: Experiment to determine the impact of structurebased clustering6.2 Experiment 2: Similarity metrics forClusteringSLAC: Single-linkage Agglomerative Clustering:Single-linkage algorithm would make use of mini-mum distance between the clusters as similarity met-ric.
One of the drawback for this measure is that ifwe have even a single document related to two clus-ters, the clusters are merged.
In Wikipedia, we willnot have un-related documents, all the documentswill be having a certain overlap of terms with eachother.
Hence, the number of clusters formed are rel-atively less compared to other two similarity mea-sures.
Thus the measures of Precision, Recall andF-measure are quite less.CLAC: Complete-linkage Agglomerative Cluster-ing: Complete-linkage algorithm would make useof maximum distance between the clusters as simi-larity metric.
This results in a preference for com-pact clusters with small diameters over long.
Hence,the accuracies are improved.
The drawback is that itcauses sensitivity to outliers.GAAC: Group Average Agglomerative Clustering:Group Average is the average between single-linkage metric and complete-linkage metric.
Hence,covers the advantages of the both, overcoming thedrawbacks of both metrics to some extent.
Thus, theaccuracies have improved considerably over previ-ous experiments.Precision Recall F-measureNER LTRC 64.9 50.6 56.81SLAC 67.6 60.3 63.74CLAC 70.3 61.1 65.38GAAC 73.5 64.3 68.59Table 2: Experiment to evaluate similarity metrics7 Discussions:From the above results, we have made the follow-ing observations.
(I) Experiment 1: The Categoryinformation of Wikipedia was able to capture the se-mantics and represent the articles in a compact wayresulting in higher accuracies over the article textinformation.
(II) Experiment 2: As each cluster isprocessed independently while identifying NEs, thecompactness and uniformity of the clusters matterin our approach.
This is studied by considering dif-ferent similarity metrics while forming clusters.
Fi-nally, from the experiments we conclude that forma-tion of hard clusters matter more for better results ofthe approach.8 ConclusionsThis paper proposes a method to identify the NEsin Indian languages for which the availability of re-sources is a major concern.
The approach suggestedis simple, efficient, easily reproducible and can beextended to any other language as it is developed un-der a language-independent framework.
Wikipediapages across languages are merged together at subti-tle level and then the non-English NEs are identifiedbased on term-term co-occurrence frequencies.
Theexperimental results conclude that the use of Cate-gory information has resulted in compact represen-tations and the compactness of the clusters plays apredominant role in determining the accuracies ofthe system.16ReferencesDaniel M. Bikel and Richard Schwartz and Ralph M.Weischedel 1999.
An Algorithm that Learns What?sin a Name, volume 34.
Journal of Machine LearningResearch.Silviu Cucerzan 2007.
Large-scale named entity dis-ambiguation based on Wikipedia data.
In Proc.
2007Joint Conference on EMNLP and CNLL, pages 708?716.Evgeniy Gabrilovich and Shaul Markovitch 2007.
Com-puting semantic relatedness using Wikipedia-based ex-plicit semantic analysis.
In Proceedings of the 20th In-ternational Joint Conference on Artificial Intelligence,pages 1606?1611.Evgeniy Gabrilovich and Shaul Markovitch 2006.
Over-coming the brittleness bottleneck using wikipedia: en-hancing text categorization with encyclopedic knowl-edge.
proceedings of the 21st national conference onArtificial intelligence - Volume 2, pages 1301?1306.Evgeniy Gabrilovich and Shaul Markovitch 2005.
Fea-ture generation for text categorization using worldknowledge.
In IJCAI05, pages 1048?1053.Jun?ichi Kazama and Kentaro Torisawa 2007.
ExploitingWikipedia as External Knowledge for Named EntityRecognition.
Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning, pages 698?707.David Milne and Olena Medelyan and Ian H. Wit-ten 2006.
Mining Domain-Specific Thesauri fromWikipedia: A Case Study.
Proceedings of the 2006IEEE/WIC/ACM International Conference on Web In-telligence, pages 442?448.Antonio Toral and Rafael Munoz 2006.
A proposal toautomatically build and maintain gazetteers for namedentity recognition by using Wikipedia.
In EACL 2006.Timothy Weale 2006.
Utilizing Wikipedia Categories forDocument Classification.
Evaluation, pages 4.Torsten Zesch and Iryna Gurevych and Max Mu?hlha?user2007.
Analyzing and Accessing Wikipedia as a Lex-ical Semantic Resource.
Biannual Conference of theSociety for Computational Linguistics and LanguageTechnology.Alexander E. Richman and Patrick Schone 2008.
MiningWiki Resources for Multilingual Named Entity Recog-nition.
ACL08.Razvan Bunescu and Marius Pasca 2006.
Using En-cyclopedic Knowledge for Named Entity Disambigua-tion.
EACL?06.Karthik Gali and Harshit Surana and Ashwini Vaidya andPraneeth Shishtla and Dipti M Sharma.
2008 Aggre-gating Machine Learning and Rule Based Heuristicsfor Named Entity Recognition.
IJCNLP?08.Rohit Bharadwaj G, Niket Tandon and Vasudeva Varma.2010 An Iterative approach to extract dictionar-ies from Wikipedia for under-resourced languages.ICON?10.17
