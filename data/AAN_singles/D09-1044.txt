Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 420?429,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPMulti-Document Summarisation Using Generic Relation ExtractionBen HacheyCentre for Languate TecnologyMacquarie UniversityNSW 2109 AustraliaCapital Markets CRC LimitedGPO Box 970Sydney NSW 2001bhachey@cmcrc.comAbstractExperiments are reported that investi-gate the effect of various source docu-ment representations on the accuracy ofthe sentence extraction phase of a multi-document summarisation task.
A novelrepresentation is introduced based ongeneric relation extraction (GRE), whichaims to build systems for relation iden-tification and characterisation that can betransferred across domains and tasks with-out modification of model parameters.
Re-sults demonstrate performance that is sig-nificantly higher than a non-trivial base-line that uses tf*idf -weighted words and atleast as good as a comparable but less gen-eral approach from the literature.
Anal-ysis shows that the representations com-pared are complementary, suggesting thatextraction performance could be furtherimproved through system combination.1 IntroductionThe goal of summarisation is to take an informa-tion source, extract content from it, and presentthe most important content in a condensed form(Mani, 2001).
The field of automatic summarisa-tion (Mani, 2001; Sp?arck Jones, 2007) aims to cre-ate tools that address various summarisation taskswith minimal human intervention.
Extractive ap-proaches to automatic summarisation create rep-resentations of the source document that are gen-erally based on an easily identified text sub-unitsuch as sentences or paragraphs.
These represen-tations are then used to identify representative orotherwise important snippets of text to place in thesummary.Following Sp?arck Jones (2007), summarisationsystems can be characterised with respect to theirapproach to three main sub-tasks: 1) interpreta-tion, 2) transformation and 3) generation.
Theinput consists of the source document (or a col-lection of source documents in the case of multi-document summarisation).
The first step (interpre-tation) creates a representation of the source doc-ument by performing some level of interpretation.A simple approach here represents sentences bytheir tokens (i.e., as an unordered bag-of-words).The next step (transformation) is the compactionstep where the source representation is convertedinto the summary representation, e.g.
by identify-ing sentences whose words are most representativeof the full text.
Finally, in the last step (genera-tion), the output summary is created.
In the caseof sentence extraction, this includes various opera-tions to maximise coherence such as ensuring thatentity references are comprehensible and arrang-ing the sentences in a sensible order.The current work investigates several represen-tations of source documents.
In particular, an ap-proach from the literature based on atomic events(Filatova and Hatzivassiloglou, 2004) is comparedto a novel approach based on generic relation ex-traction (GRE), which aims to build systems forrelation identification and characterisation that canbe transferred across domains and tasks withoutmodification of model parameters (Hachey, 2009).The various representations are substituted in theinterpretation phase of a multi-document sum-marisation task and used as the basis for extract-ing sentences to be placed in the summary.
Sys-tem summaries are compared by calculating termoverlap with reference summaries created by hu-man analysts.2 MotivationIn seminal work on automatic summarisation,Luhn (1958) introduces a representation basedon content words.
These are defined as non-function words from the source document that areneither too frequent nor too infrequent.
Luhnuses frequency to weight content words and ex-420tracts sentences with the highest combined con-tent scores to form the summary.
Subsequent workadapted the tf*idf weighting scheme, where termfrequency (tf ) is combined with inverse documentfrequency (idf ), an inverse measure of term oc-currence across documents that serves to down-weight common words (Sp?arck Jones, 1972).
Inmodern work, tf*idf representations are often usedas simple but non-trivial baselines.
The problemis that these shallow features often break downwhere underlying linguistic content needs to becompared rather than just surface structure.The use of representations based on informa-tion extraction (IE) has been suggested as one ap-proach to capturing deeper semantic information.This is based on the notion that IE definitions oftypes for entities, relations and events provide alevel of abstraction that is appropriate for auto-matic summarisation.
Several approaches in theliterature have explored the use of IE-based rep-resentations for extractive summarisation: McK-eown et al (1998) incorporate patient character-istic templates for matching potential treatmentsto specific patients in a medical summarisationsystem; White and Cardie (2002) incorporate abootstrapped IE system based on Autoslog (Riloff,1996) for filling event templates; and Harabagiuand Maiorano (2002) incorporate a hybrid ap-proach that uses conventional supervised IE tech-niques for known topics and a more general ap-proach based on WordNet for unknown topics.1The problem with these systems is that they alluse supervised approaches to IE that require thatthe IE templates be known in advance and addi-tionally require significant investment in writingextraction rules or in annotating data for train-ing.
Where more general techniques are used, theystill require domain-specific resources, e.g.
Whiteand Cardie (2002) bootstrapping approach still re-quires that the extraction templates be known inadvance and Harabagiu and Maiorano (2002) ap-proach depends on the WordNet lexical database,for which coverage is not guaranteed for arbitrarydomains.Filatova and Hatzivassiloglou (2004) intro-duce methods using more general IE represen-tations that are not based on supervised learn-ing.
Given a named entity recogniser, the rep-1Comparable approaches using IE in the context ofabstractive?as opposed to extractive?summarisation includework by DeJong (1982), Hahn and Reimer (1999), White etal.
(2001) and Saggion and Lapalme (2002).resentation is automatically derived and consistsof <Ent,Connector, Ent> event triples, whereconnectors are verbs or action nouns that occur inbetween the two NEs.
Thus, the approach aims toperform a generic IE task that the authors refer toas atomic event extraction.
This representation isshown to outperform a tf*idf baseline on a multi-document summarisation task.
As we will see inSection 4.3 below, Filatova and Hatzivassiloglou?sapproach has three main shortcomings.
First, it fo-cuses exclusively on simple atomic events (i.e., en-tity mention pairs with an intervening verbal con-nector), meaning that it will not be able to ad-dress tasks where relations are at least as impor-tant as events (e.g., biographical summarisation).Second, it relies on exact matching between con-nectors, which is not capable of capturing latentsemantic similarities (e.g., between ?work for?
and?employed by?).
Third, its performance is subjectto the coverage of WordNet, which is used to iden-tify action nouns.Generic relation extraction (GRE) aims to buildsystems that can be transferred across domainsand tasks without modification of model param-eters (Hachey, 2009).
For relation identification(i.e., extraction of relation forming entity mentionpairs), this is achieved by using general rule-basedapproaches and, for relation characterisation (i.e.,assignment of types to relation mentions), this isachieved by using unsupervised machine learning.Hachey (2009) introduces a GRE approach thataddresses the shortcomings of the atomic event ap-proach mentioned above.
First, it models a type ofIE that includes relations.
Second, it uses a con-nector model based on latent Dirichlet alocation(Blei et al, 2003), which provides a mechanismfor capturing latent semantic similarities betweenconnectors.
Third, it does not rely on domain-specific resource like WordNet.
The GRE modelsused here do rely on dependency parsing.
How-ever, they still generalise across formal domainsas the relation identification and characterisationsystems, developed on news data, achieve compa-rable performance when applied directly to a rela-tion extraction task in the biomedical domain (seeHachey (2009) for details).
Furthermore, gram-matical relations obtained from dependency pars-ing provide a means for constraining relation iden-tification and supplying more linguistically mean-ingful features for relation characterisation.421c1c2c3c4c5t11 1 0 1 1t21 0 0 1 0t30 1 0 0 1t41 0 1 1 1Table 1: Text ?
concept matrix for set cover ap-proach to automatic summarisation (Filatova andHatzivassiloglou, 2004).3 Algorithm for Set Cover ExtractionFor the sake of comparison, the current evaluationadopts the Filatova and Hatzivassiloglou (2004)summarisation framework.
This defines an extrac-tion approach based on a mapping between textualunits and concepts.
To illustrate, consider the ma-trix in Table 1 where rows represent textual units(e.g., sentences, paragraphs) and columns repre-sent concepts (e.g., words, events, relations) in theinput text.
Each concept is either absent or presentin a given textual unit.
Additionally, each con-cept has a weight associated with it.
Looking atthe problem in this way makes it natural to for-mulate it as follows: the summary should selecttextual units such that there is maximal coverageof the salient conceptual units.2This is essentiallythe maximum coverage problem, which has beenshown to be reducible to the set covering problem,for which there are approximation algorithms inthe literature that run in polynomial time or better(Hochbaum, 1997; Bienstock and Iyengar, 2004).Filatova and Hatzivassiloglou define severalgreedy algorithms that can be parametrised interms of the general SUMMARISE function in Fig-ure 1, which takes the text ?
concept matrix Dand the maximum summary length k as input.
TheSUMMARISE function first initialises the summaryS to the empty set.
Then it enters a loop thatcontinues until the summary reaches the desiredlength.
Within the loop, a text unit is extracted andadded to the summary after which the text ?
con-cept matrix is updated The output of the algorithmis a set S comprising the text units that make upthe summary.
For the experiments reported here,the text units t are sentences and LENGTH(ti) re-2While not considered in the current experiments, a morediscourse-oriented approach could be derived within the setcover framework by down-weighting conceptual units thatoccur e.g.
in portions of the source documents that describebackground information, where text segments containingbackground information could be identified using a sentence-level rhetorical status classifier like that developed by Teufeland Moens (2002).SUMMARISE : D, k1 S ?
{}2 while?ti?SLENGTH(ti) < k3 tj?
EXTRACT(D)4 S ?
S ?
tj5 D ?
UPDATE(D, tj)6 return SFigure 1: Generalised function for Filatova andHatzivassiloglou (2004) approach to extractivesummarisation.EXTRACT: D1 cj?
argmaxcj?cols(D)?ti?rows(D)D[ti, cj]2 tk?
argmaxtk?rows(D)&D[tk,cj]>0SCORE(D, tk)3 return tkUPDATE: D, ti1 for each cj?
cols(D)2 if D[ti, cj] > 03 for each tk?
rows(D)4 D[tk, cj]?
05 D ?
DELETE(D, ti)6 return DFigure 2: Extraction and update functions for Fi-latova and Hatzivassiloglou (2004) modified adap-tive algorithm.turns the count of word tokens in sentence ti.Figure 2 contains the EXTRACT and UPDATEfunctions used here.3The EXTRACT function firstidentifies the concept cjnot yet covered in thesummary that has the highest overall weight in thetext ?
concept matrix D. Then it selects the textunit tkwith the highest score from among the textunits that contain concept cj.
The SCORE func-tion is the sum of concept weights for the giventext unit, i.e.
:SCORE : D, ti7?
return?cj?cols(D)D[ti, cj] (1)The UPDATE function in Figure 2 aims to min-imise redundancy in the summary by globallymaximising the number of conceptual units cov-ered in the output.
In addition to removing the rowrepresenting the extracted text unit from the text?concept matrixD, it iterates through the remainingtext units and assigns zero weights to all conceptsthat are covered by the extracted text unit.3The EXTRACT and UPDATE functions in Figure 2 corre-spond to Filatova and Hatzivassiloglou (2004) modified adap-tive algorithm and were found in preliminary experiments tobe the better than the simple greedy and adaptive greedy al-gorithms (see Hachey (2009) for details).422Bush worked as an oil lease negotiator for Amoco inDenver and later started his own oil company, JNB.tf*idf (TF)jnb:3.55, amoco:3.13, oil:3.05,negotiator:3.04, lease:2.58, denver:2.45,bush:2.44, worked:2.28, started:2.21,later:2.13, own:1.96, company:1.94,...event (EV)<PER bush,worked,XFN oil>:0.00023,<PER bush,worked,ORG amoco>:0.00011,<PER bush,worked,LOC denver>:0.00011,<XFN oil,started,ORG jnb>:0.00011,...relation (RL)<ORG amoco,rd94,LOC denver>:0.00039,<ORG amoco,rd505,LOC denver>:0.00039,<XFN oil,rd92,ORG jnb>:0.00002,<XFN oil,rd712,ORG jnb>:0.00002,...entity pairev(EE)<PER bush,XFN oil>:0.00244,<PER bush,LOC denver>:0.00122,<PER bush,ORG jnb>:0.00044,<LOC denver,XFN oil>:0.00033,...entity pairrl(ER)<ORG amoco,LOC denver>:0.00311,<ORG jnb,XFN oil>:0.00155Figure 3: Example sentence and various represen-tations of sentence content.4 ModelsFigure 3 contains an example sentence and its rep-resentations corresponding to the various modelsof sentence content explored here.4These are de-scribed in detail in the rest of this section.4.1 Baseline tf*idf Representation (TF)The baseline model represents sentences as tf*idf -weighted bags-of-words (TF).
Document frequen-cies for terms are derived from the same resourceused by Filatova and Hatzivassiloglou (2004)?afrequency list compiled from a large sample ofweb pages.
Term weighting is calculated usingtf*idf as:w(i, j) =?
(1 + log (tfi,j)) ?
log(Ndfi)(2)where tfi,jis the number of times term i occurs insentence j and dfiis the number of documents inwhich term i occurs.
An example sentence and itstf*idf representation can be seen in Figure 3.4The sentence was selected from document set d47 (fromthe data set described in Section 5.1 below), which containsarticles about Neil Bush and his role in the collapse of Sil-verado Savings and Loan during the U.S. Savings and Loancrisis of the 1980s and 1990s.4.2 Event Representation (EV)We also compare to Filatova and Hatzivassiloglou(2004) atomic events (EV).
This consists of<Enti, Connectorj, Entk> event triples, whereconnectors are verbs or action nouns (i.e., nounsthat are hyponyms of event or activity in Word-Net) that occur in between the two entity men-tions.
Given a named entity recogniser and a lex-ical resource (WordNet), these are derived auto-matically from the text as follows.
In the first step,all pairs of entity mentions that occur together in asentence are identified.
Next, the algorithm char-acterises the entity mention pairs using the con-nector words from the intervening context and dis-cards pairs without an intervening connector word.Event triple weighting is calculated by combin-ing entity pair and connector weights as:wev(i, j, k) = wne(i, k) ?
wcn(j, i, k) (3)where wne(i, k) is the weight of the entitypair <i, k> consisting of entities i and k andwcn(j, i, k) is the weight of connector j in the con-text of entity pair <i, j>.
wne(i, k) is calculatedas the normalised entity pair count, i.e.
:wne(i, k) =Cne(< i, k >)Cne(< ?, ?
>)(4)where Cne(<i, k>) is the count of mentions ofentity pair <i, k>5and Cne(<?, ?>) is the totalcount of entity mention pairs.
And, wcn(j, i, k) iscalculated as the normalised count of connector jin the context of the entity pair, i.e.
:wcn(j, i, k) =C<i,k>cn(j)C<i,k>cn(?
)(5)where C<i,k>cn(j) is the count of occurrences ofconnector j in the context of entity pair <i, k>and C<i,k>cn(?)
is the total count of connectors inthe context of entity pair <i, k>.
An examplesentence and its event representation can be seenin Figure 3.
Event triples generated include<PER bush,worked,ORG amoco> and<PER bush,started,ORG jnb>.Some erroneous event triples are also generated.The first error has to do with the fact that entities5Coreference between entity mentions is computed by ex-act string match after removing punctuation, converting toall lower case, and prefixing the entity type.
For example,the entity mention string ?JNB?
with type ORGANISATION isnormalised to ORG jnb.423include named entities identified in the pre-processing as well as the ten most frequent nounsin the document set.
In the example sentence fromFigure 3, the most frequent nouns include ?oil?but not ?negotiator?
or ?company?.
Therefore, ?oil?is labelled as an entity and extracted in a num-ber of triples such as <PER bush,worked,XFN oil> (as opposed to <PER bush,worked,XFN negotiator>).
Anotherproblem illustrated by the example sentence hasto do with the noisy nature of the surface-levelapproach to identifying entity mention pairs andconnectors which tends to generate many falsepositive events, e.g.
<ORG amoco,started,ORG jnb>.
If the algorithm was constrainedbased on the underlying grammatical structure,it should be able to identity that the argumentsof ?worked?
are ?Bush?
and ?Amoco?
(i.e.,<PER bush,worked,ORG amoco>) and that?worked?
does not describe an event involving?Amoco?
and ?JNB?.4.3 Relation Representation (RL)The focus of the current evaluation is a novel rep-resentation based on generic relation extraction(GRE).
As mentioned above, GRE is a minimallysupervised approach to the relation extraction taskthat aims to build systems for relation identifica-tion and characterisation that can be transferredacross domains and tasks without modification ofmodel parameters.
Relation mentions are identi-fied by taking pairs of entity mentions that have ei-ther 1) no more than two intervening words in thesurface order of the sentence or 2) no more thanone edge intervening on the shortest path througha dependency parse (see Hachey (2009) for detailsand experiments comparing different window con-figurations).
This is stricter than the Filatova andHatzivassiloglou approach in that entity mentionshave to occur much closer or be connected by asingle dependency relation.
At the same time, itis less strict in the sense that an action- or event-denoting word is not required in the context, whichmakes it a more general model of IE.Relation connectors are derived from amodel ofrelation types based on latent Dirichlet alocation(Blei et al, 2003) that incorporates word, entityand dependency path features from the context ofa relation-forming entity mention pair (see Hachey(2009) for details).
This outputs a topic distribu-tion for each entity mention pair that correspondsto the type of relation that is described.
This rep-resentation 1) models a type of generic IE that in-cludes relations, 2) uses a connector model that ab-stracts away from surface-level event descriptorsused by Filatova and Hatzivassiloglou (2004) and3) does not rely on domain-specific resources likeWordNet.6For the purpose of comparison, rela-tion triples are weighted in the same way as eventtriples using Equations 3 and 4 above.
However,the connector pair weighting is modified to use thedistribution over topics given by the LDA output.7Relation triples generated for the examplesentence in Figure 3 include <ORG amoco,rd94,LOC denver> and <ORG amoco,rd505,LOC denver>, where the connectors(i.e., rd94 and rd505) are identifiers that indexparticular topics from the LDA output.
Here,rd94 and rd505 index topics that correspondto located-in relations so the respective triplesboth describe located-in relations between Amocoand Denver.
Relation triples generated for theexample sentence also include <XFN oil,rd92,ORG jnb> and <XFN oil,rd712,ORG jnb>.
These are erroneous for the samereason as some of the event triples above (i.e., dueto the noise inherent in the approach to identifyingnominal entity mentions by identifying the tenmost frequent nouns in the document set).4.4 Entity Pair Representations (EE, ER)Finally, we investigate the performance of rep-resentations that do not model event or re-lation type information.
These are identicalto the EV and RL representations above, ex-cept they are <Ent,Ent> 2-tuples instead of<Ent,Connector, Ent> 3-tuples.
That is, entitypairs are included here provided that they meet therelation mention identification constraints.
Theyare weighted using the normalised entity paircount (Equation 4 above).
Relation-based entitypairs generated for the example sentence in Fig-ure 3 include<LOC denver,ORG amoco> and<ORG jnb,XFN oil>.6The GRE representation here does rely on dependencyparsing, however, Hachey (2009) shows that it is still directlyportable between the news and biomedical domains withoutmodification of model parameters.7Distributions for entity mention pairs tend to have a longuniform tail and only a few topics with higher probability.
Inconverting to a weighting scheme, topic representations hereare converted to a sparse representation where all topics inthe uniform tail are removed.4245 Experimental Setup5.1 DataThe experiments here use the multi-documentsummarisation data from the 2001 Document Un-derstanding Conference (DUC),8which is thesame data used by Filatova and Hatzivassiloglou(2004).
This comprises 30 test document sets,each of which include approximately 10 news sto-ries.
Each document set is collected by a humanand focuses on a particular topic.
Example topicsinclude the nomination of Clarence Thomas to theAmerican Supreme Court, Neil Bush?s role in thecollapse of Silverado Savings and Loan and theExxon Valdez oil spill.
Gold standard summariesare provided for each document set for summarylengths of 50, 100, 200 and 400 words.
This helpsto ensure that the systems are not over-tuned tospecific summary lengths.
For each summary task(i.e., all 120 document set?
summary length com-binations), there are three distinct gold standardsummaries created by different human analysts.Pre-processing includes sentence boundaryidentification, segmentation of words (tokenisa-tion), labelling words with part-of-speech tags,identification of noninflected base word forms(lemmatisation) from the LT-TTT tools (Grover etal., 2000).
It also includes dependency parsing us-ing Minipar (Lin, 1998) and automatic named en-tity recognition using the C&C tagger (Curran andClark, 2003) trained on the data from the MUC-7shared task (Chinchor, 1998).
Weights for the var-ious IE-based representations are calculated overeach input document set.5.2 EvaluationThe evaluation uses Rouge9to determine whichrepresentation selects content that overlaps mostwith human summaries.
Rouge estimates thecoverage of appropriate concepts (Lin, 2004) ina summary by comparing it to several human-created reference summaries.
Rouge-1 does soby computing recall based on macro-averaged un-igram overlap.
Rouge-SU4 does so by calculatingskip-bigram overlap where bigrams are allowed to8http://www-nlpir.nist.gov/projects/duc/index.html9Rouge stands for recall-oriented understudy for gistingevaluations.
While current versions also compute precisionand f-score of system summaries, the evaluation here usesrecall alone, which is sufficient when the length of the sum-maries being compared is the same.
Rouge can be obtainedfrom http://haydn.isi.edu/ROUGE/.1 50 100 200 400TF 0.0797 0.1113 0.1742 0.2467EV 0.1360 0.1776 0.2315 0.3019RL 0.1360 0.1766 0.2412 0.3014SU4 50 100 200 400TF 0.0173 0.0259 0.0442 0.0693EV 0.0376 0.0494 0.0692 0.0950RL 0.0356 0.0491 0.0701 0.0939Table 2: Comparison of Rouge scores for the tf*idf(TF), event (EV) and relation (RL).be composed of non-contiguous words (with asmany as four words intervening).
Rouge-SU4 alsoincludes unigrams to decrease the chances of zeroscores where there is no skip-bigram overlap.The configuration is based on comparisons be-tween Rouge and human judgements of contentcoverage (Lin, 2004), which suggest that Rouge-1 and Rouge-SU4 with stemming and removalof stop words are good measures for evaluatingmulti-document summarisation tasks, consistentlyachieving Pearson?s correlation scores above 0.72and as high as 0.9 for longer summaries.
PairedWilcoxon signed ranks tests across document setsare used to check for significant differences be-tween systems.
The paired Wilcoxon signed rankstest is a non-parametric analogue of the paired ttest.
The null hypothesis is that the two popula-tions from which the scores are sampled are iden-tical.6 ResultsCan extractive summarisation be improved us-ing representations based on generic informa-tion extraction?
Table 2 contains results fortf*idf (TF), event (EV) and relation (RL) repre-sentations.
Columns contain results for differentlengths of summary (50, 100, 200 and 400 words).The best representation for each summary lengthis in bold and representations that are statisticallydistinguishable from the best (i.e., p ?
0.05) areunderlined.
The results demonstrate unambigu-ously that the event and relation representationsoutperform the tf*idf representation, with stronglysignificant p-values less than 0.001 for both Rougemeasures and all summary lengths.
The event andrelation representations are indistinguishable forboth Rouge measures and all summary lengths.4251 50 100 200 400ER 0.1497 0.1929 0.2527 0.3123EE 0.1442 0.1705 0.2288 0.3061SU4 50 100 200 400ER 0.0419 0.0537 0.0786 0.1008EE 0.0364 0.0447 0.0643 0.0963Table 3: Comparison of Rouge scores for entitypairs based on relations (ER) and events (EE).How does entity pair identification for genericrelations compare to entity pair identificationfor atomic events?
Table 3 contains results forthe representations described in Section 4.4.
Rowscorrespond to entity pair identification for rela-tions (ER) and events (EE).10Results suggest thatthe entity pair model based on GRE data out-performs the entity pair model based on atomicevents, at least for medium sized summaries of100 and 200 words where ER is significantly betterthan EE for both Rouge measures.How do the event and relation representationsperform with respect to corresponding entitypair representations?
The scores for the entitypair representations reported in Table 3 are statisti-cally indistinguishable from those for correspond-ing relation and event representations in Table 2above.
This appears to be a mixed result for boththe relation representation introduced here andthe Filatova and Hatzivassiloglou event represen-tation.
And, while GRE is shown to have a positiveeffect on Rouge scores when compared to atomicevents, the same cannot be said of approachesto characterising relation and event types.
How-ever, as the correlation analysis (Section 7.1 be-low) demonstrates, RL and ER do not necessar-ily perform well on the same document sets.
Thissuggests that they are actually complementary tosome degree, meaning that a combined systembased on both representations would outperformRL and ER on their own.10In contrast to the results for the tf*idf, relation and eventrepresentations which use the modified adaptive algorithmdescribed above, results for entity pair representations use asimplified version of the EXTRACT function that picks thetext unit that has the highest score.
This performed signifi-cantly better than the modified adaptive algorithm (p ?
0.01)for all summary lengths for ER and was indistinguishable forEE.
See Hachey (2009) for details.7 Analysis and Comparison7.1 ComplementarityFigure 4 contains results for a correlation analy-sis comparing the various representations.
Thisalso includes a comparison to the human upperbound (HU), computed by leave-one-out cross val-idation.
Cells in the matrix contain the correla-tions values measured across document set Rouge-SU4 scores11using Spearman?s ?
rank correlationcoefficient (rS).
Here, high values mean that tworepresentations tend to perform well on the samedocument sets such that an ordering of documentsets by Rouge scores is similar for the representa-tions being compared.
In the figure, correlationstrength is represented by shading where light-toned squares indicate strong correlation (and thedarkest squares indicate weak negative correla-tion).
For example, the upper left cell contains rSbetween the TF and EV representations.
The foursquares correspond to rSvalues of -0.085, 0.199,0.245 and 0.267 respectively for summaries of 50,100, 200 and 400 words.The analysis illustrates a number of interest-ing points.
First, it demonstrates that none ofthe representations correlate highly with the hu-man upper bound, meaning that the automatic sys-tems do not necessarily do well on the documentsets that may be considered easier as measuredby human agreement using Rouge.
This suggeststhat task difficulty does not need to be consideredas a possible underlying cause of correlation be-tween the automatic systems.
The analysis alsoillustrates that there is no clear and consistent re-lationship between summary length and correla-tion values.
Some cells suggest that correlationmay have a monotonic linear relationship increas-ing with length (e.g., TF*EV) while others seemto suggest inverse linear (e.g., TF*RL), quadratic(e.g., EV*HU) and invariant (e.g., EV*EE) rela-tionships with length.Looking at correlation between automatic sys-tems (i.e., TF, EV, RL, EE and ER), correla-tion values closer to zero suggest that the sys-tems do well on different document sets and thata combined system might therefore be better.By this reasoning, the largest gains would comefrom combining TF with any other representation.Among the other automatic systems, the relation11Correlation across document set Rouge-1 scores showssimilar trends.426-0.10.10.30.50.7Rouge-SU4400200HU10050400200ER10050400200EE10050400200RL10050400200EV10050EREERLEVTFFigure 4: Comparison of representations using Spearman?s rs.
Row and column labels correspond totf*idf (TF), event (EV), relation (RL), event entity pair (EE), relation entity pair (ER) and human (HU)representations.
Lighter toned squares indicate stronger correlation.representation (RL) shows moderately high poten-tial for combination with its corresponding entitypair representation (ER) with Spearman?s rSval-ues in the range from 0.348 to 0.476.
This sug-gests that ER should not necessarily be consid-ered a simpler representation of the same infor-mation captured by RL when comparing results.The event representation (EV), by contrast, showsthe strongest correlation of any comparison withits corresponding entity pair representation (EE)with rSvalues in the range from 0.541 to 0.725.7.2 Error AnalysisFour document sets were considered for erroranalysis.
These were selected to cover differentrelative rankings of representations.
Rows in Ta-ble 4 give the document set ID and list the repre-sentations in order of their Rouge-SU4 scores.
In-spection of the corresponding document sets sug-gests that the different approaches compared hereare appropriate for different types of summarytasks.
Specifically, it suggests that relation andevent representations perform poorly on summari-sation tasks that are oriented towards sentiment,description or analysis.
However, they do wellon document sets that are oriented towards fac-tual information typical of information extractiontasks (though current representations do not cap-ture date, time or numeric information).
This sup-ports the notion from the previous section that thedifferent representations evaluated here are com-plementary.The document set (d06) for the summaries inFigure 5 illustrates a case where the relation andevent representations perform well with respectSet Rank 1 Rank 2 Rank 3d15 TF (0.046) RL (0.035) EV (0.023)d39 TF (0.033) EV (0.024) RL (0.014)d06 RL (0.094) EV (0.060) TF (0.016)d53 RL (0.078) TF (0.035) EV (0.020)Table 4: DUC 2001 document sets chosen for er-ror analysis and corresponding Rouge-SU4 scores.to tf*idf.
The gold standard summary describesa beating event, addressing the basic facts of theRodney King beating by Los Angeles police aswell as the political aftermath which consists pri-marily of an investigation and a summary of re-lated police brutality events.
The difference inperformance seems to be due to the fact that re-lations and events are central to all aspects ofthis summary and the relation and event represen-tations clearly do better than tf*idf at capturingthis information.
This summary also illustrates anunintended side-effect of the relation representa-tion where the generic relation identification algo-rithm finds relations between components of lex-ical compounds or multi-word phrases.
The rep-resentation for the third sentence in the RL sum-mary, for example, includes a relation betweenORG police and XFN chief in addition totrue positive relations e.g.
between ORG policeand PER darylgates and false positive re-lations e.g.
between PER tombradley andORG police.7.3 Comparison to Supervised ExtractionRelated work by Wong et al (2008) also com-pares representations for sentence extraction on427TF(0.016)(20/29)[S1] Mr. Williams likened the report to the Knapp Commission, a 1970s blue-ribbon study that exposedwidespread corruption in the New York Police Department and led to significant improvements there.
[S2]?There?s no doubt in our mind that the only reason they stopped Joe Morgan was because he is black andhe was the first black who happened to come by,?
said William Barnes, one of the attorneys representingthe former ballplayer.
[S3] Joseph McNamara, retired chief of San Jose?s department and now a fellowat Stanford University?s Hoover Institution, said he has been getting calls all summer from [END] citiesaround the country about racism and brutality in their departments.EV(0.060)(9/29)[S1] A high-ranking commission appointed after the beating, under the chairmanship of Mr WarrenChristopher, a lawyer and former deputy secretary of state, concluded that the Los Angeles police de-partment got results, in terms of arrests, but had developed a ?siege mentality that alienates the officerfrom the community?.
[S2] The images of Los Angeles police swinging nightsticks at King as he lay onthe ground, played repeatedly on national news programs, were burned into the national conscience andled to widespread calls for investigation of police brutality.
[S3] Besides recommending that Mr Gatesshould go, the Christopher commission urged a policy [END] of community policing with more foot pa-trols, as well as measures to discipline racist police officers and to improve the investigation of complaintsabout police brutality.RL(0.094)(3/29)[S1] Mr. Gates opposed the Police Corps because its members would not be professionals.
[S2] Shortlyafter Rodney King?s beating, a news program on ABC illustrating police brutality showed a still photoof police using a martial-arts weapon against a person being arrested, but there was no mention that theepisode involved Operation Rescue.
[S3] The report was issued yesterday by a commission appointed byMayor Tom Bradley and Police Chief Daryl Gates in the wake of the videotaped beating March 3 of ablack motorist, Rodney King, by Los Angeles police.
[S4] Investigations have been launched by the FBI,the Los [END] Angeles County district attorney?s office and the Long Beach Police Department.HU(0.400)(15/29)The most important of the many cases of police brutality reported in southern California 1989-1992, wasthe beating of Rodney King by four Los Angeles officers on March 3, 1991.
An investigating commissionoutlined steps for improvement of the police department and called for the resignation of Chief Gates.Gates did not resign until the following year after the acquittal of the four officers caused massive rioting.Other cases of police brutality arose in Minneapolis, Chicago and Kansas City.
Operation Rescue claimedthat its non-violent anti-abortion demonstrators were seriously injured by excessive police tactics in morethan [END] 50 cities.Figure 5: Example system and human (HU) summaries where relation (RL) and event (EV) representa-tions perform well with respect to the tf*idf (TF) representation: Police Brutality Document Set (d06).the DUC 2001 data.
However, it uses supervisedmachine learning (probabilistic support vector ma-chines) to derive a salience function while we fo-cus on unsupervised approaches that can be portedto new domains and tasks without annotation ortraining.
Interestingly, Wong et al?s results sug-gest that adding events to a word-based featureset increases the precision of supervised sentenceextraction but reduces the recall.
By contrast,the current results and analysis provide evidencethat word and generic IE-based representations arecomplementary when using unsupervised saliencefunctions for sentence extraction.The Wong et al (2008) paper also provides use-ful results for comparison to state-of-the art.
Onthe 200 word summarisation task, Wong et al re-port Rouge-1 scores of 0.352 and 0.344 respec-tively for word-based and event-based represen-tations.
On the same task, our unsupervised ap-proach achieves Rouge-1 scores of 0.174, 0.232,0.229, 0.241 and 0.253 respectively for the tf*idf,event, event entity pair, relation and relation en-tity pair representations.
Wong et al?s best overallscore is 0.396 using a representation that combinessurface, content and relevance features.8 ConclusionExperiments were presented that compare the ef-fect of various source document representationson the accuracy of automatic summarisation.
Thisserves as an extrinsic evaluation of generic relationextraction, a domain-neutral and fully portable ap-proach to relation identification and characterisa-tion.
Results demonstrate that GRE is an effectiverepresentation for sentence extraction for multi-document summarisation.
Performance for the re-lation representation is significantly better than anon-trivial tf*idf baseline across the range of sum-mary lengths explored.
Performance is also atleast as good as a comparable but less general rep-resentation based on event extraction.
Correlationanalysis suggests that different representations arecomplementary due to the fact that they performwell on different document sets.
Error analysissupports this conclusion, suggesting that the rela-tion and event representations perform poorly onsummarisation tasks that are oriented towards e.g.sentiment, description or analysis while they per-form well on tasks that focus on fact-oriented in-formation.428AcknowledgmentsThis work was supported by Scottish EnterpriseEdinburgh-Stanford Link grant R37588 as part ofthe EASIE project at the University of Edinburgh.It would not have been possible without the guid-ance of Claire Grover and Mirella Lapata.ReferencesDaniel Bienstock and Garud Iyengar.
2004.
Fasterapproximation algorithms for packing and coveringproblems.
Technical Report TR-2004-09, ColumbiaUniversity.David Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet alocation.
Journal of Ma-chine Learning Research, 3:993?1022.Nancy Chinchor.
1998.
Overview of MUC-7.
In Pro-ceedings of the 7th Message Understanding Confer-ence, Fairfax, VA, USA.James R. Curran and Stephen Clark.
2003.
Languageindependent NER using a maximum entropy tagger.In Proceedings of the 7th Conference on NaturalLanguage Learning, Edmonton, Alberta, Canada.Gerald DeJong.
1982.
An overview of the FRUMPsystem.
In Wendy G. Lehnert and Martin H. Ringle,editors, Strategies for Natural Language Process-ing, pages 149?176.
Lawrence Erlbaum Associates,Hillsdale, NJ.Elena Filatova and Vasileios Hatzivassiloglou.
2004.Event-based extractive summarization.
In Proceed-ings of the ACL Text Summarization Branches OutWorkshop, Barcelona, Spain.Claire Grover, Colin Matheson, Andrei Mikheev, andMarc Moens.
2000.
LT TTT?a flexible tokeni-sation tool.
In Proceedings of the 2nd InternationalConference on Language Resources and Evaluation,Athens, Greece.Ben Hachey.
2009.
Towards Generic Relation Extrac-tion.
Ph.D. thesis, University of Edinburgh.Udo Hahn and Ulrich Reimer.
1999.
Knowledge-based text summarization: Salience and general-ization operators for knowledge base abstraction.In Inderjeet Mani and Mark T. Maybury, editors,Advances in Automatic Text Summarization, pages215?232.
MIT Press, Cambridge, MA.Sanda M. Harabagiu and Steven J. Maiorano.
2002.Multi-document summarization with GISTexter.
InProceedings of the 3rd International Conference onLanguage Resources and Evaluation, Las Palmas,Spain.Dorit S. Hochbaum.
1997.
Approximating coveringand packing problems: set cover, vertex cover, in-dependent set and related problems.
In Dorit S.Hochbaum, editor, Approximation Algorithms forNP-Hard Problems, pages 94?143.
PWS PublishingCompany, Boston, MA.Dekang Lin.
1998.
Dependency-based evaluation ofMINIPAR.
In Proceedings of the LREC WorkshopEvaluation of Parsing Systems, Granada, Spain.Chin-Yew Lin.
2004.
ROUGE: a package for auto-matic evaluation of summaries.
In Proceedings ofthe ACL Text Summarization Branches Out Work-shop, Barcelona, Spain.Hans P. Luhn.
1958.
The automatic creation of litera-ture abstracts.
IBM Journal of Research and Devel-opment, 2(2).Inderjeet Mani.
2001.
Automatic Summarization.John Benjamins, Amsterdam/Philadelphia.Kathleen R. McKeown, Desmond A. Jordan, andVasileios Hatzivassiloglou.
1998.
Generatingpatient-specific summaries of online literature.
InProceedings of the AAAI Spring Symposium on In-telligent Text Summarization, Stanford, CA, USA.Ellen Riloff.
1996.
Automatically generating extrac-tion patterns from untagged text.
In Proceedingsof the 14th National Conference on Artificial Intelli-gence, Portland, OR, USA.Horacio Saggion and Guy Lapalme.
2002.
Generat-ing indicative-informative summaries with SumUM.Computational Linguistics, 28(4):497?526.Karen Sp?arck Jones.
1972.
A statistical interpretationof term specificity and its application in retrieval.Journal of Documentation, 28(1):11?21.Karen Sp?arck Jones.
2007.
Automatic summarising:The state of the art.
Information Processing andManagement, 43:1449?1481.Simone Teufel and Marc Moens.
2002.
Summariz-ing scientific articles ?
experiments with relevanceand rhetorical status.
Computational Linguistics,28(4):409?445.Michael White and Claire Cardie.
2002.
Selectingsentences for multidocument summaries using ran-domized local search.
In Proceedings of the ACLWorkshop on Automatic Summarization, Philadel-phia, PA, USA.Michael White, Tanya Korelsky, Claire Cardie, VincentNg, David Pierce, and Kiri Wagstaff.
2001.
Mul-tidocument summarization via information extrac-tion.
In Proceedings of the 1st International Con-ference on Human Language Technology Research,San Diego, CA, USA.Kam-Fai Wong, Mingli Wu, and Wenjie Li.
2008.
Ex-tractive summarization using supervised and semi-supervised learning.
In Proceedings of the 22nd In-ternational Conference on Computational Linguis-tics, Manchester, UK.429
