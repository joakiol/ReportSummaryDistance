Proceedings of NAACL HLT 2007, pages 484?491,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsA Comparison of Pivot Methods for Phrase-based Statistical MachineTranslationMasao Utiyama and Hitoshi IsaharaNational Institute of Information and Communications Technology3-5 Hikari-dai, Soraku-gun, Kyoto 619-0289 Japan{mutiyama,isahara}@nict.go.jpAbstractWe compare two pivot strategies forphrase-based statistical machine transla-tion (SMT), namely phrase translationand sentence translation.
The phrasetranslation strategy means that we di-rectly construct a phrase translation ta-ble (phrase-table) of the source and tar-get language pair from two phrase-tables;one constructed from the source languageand English and one constructed from En-glish and the target language.
We then usethat phrase-table in a phrase-based SMTsystem.
The sentence translation strat-egy means that we first translate a sourcelanguage sentence into n English sen-tences and then translate these n sentencesinto target language sentences separately.Then, we select the highest scoring sen-tence from these target sentences.
We con-ducted controlled experiments using theEuroparl corpus to evaluate the perfor-mance of these pivot strategies as com-pared to directly trained SMT systems.The phrase translation strategy signifi-cantly outperformed the sentence transla-tion strategy.
Its relative performance was0.92 to 0.97 compared to directly trainedSMT systems.1 IntroductionThe rapid and steady progress in corpus-based ma-chine translation (Nagao, 1981; Brown et al, 1993)has been supported by large parallel corpora suchas the Arabic-English and Chinese-English paral-lel corpora distributed by the Linguistic Data Con-sortium and the Europarl corpus (Koehn, 2005),which consists of 11 European languages.
How-ever, large parallel corpora do not exist for manylanguage pairs.
For example, there are no pub-licly available Arabic-Chinese large-scale parallelcorpora even though there are Arabic-English andChinese-English parallel corpora.Much work has been done to overcome the lackof parallel corpora.
For example, Resnik and Smith(2003) propose mining the web to collect parallelcorpora for low-density language pairs.
Utiyamaand Isahara (2003) extract Japanese-English parallelsentences from a noisy-parallel corpus.
Munteanuand Marcu (2005) extract parallel sentences fromlarge Chinese, Arabic, and English non-parallelnewspaper corpora.Researchers can also make the best use of exist-ing (small) parallel corpora.
For example, Nie?enand Ney (2004) use morpho-syntactic information totake into account the interdependencies of inflectedforms of the same lemma in order to reduce theamount of bilingual data necessary to sufficientlycover the vocabulary in translation.
Callison-Burchet al (2006a) use paraphrases to deal with unknownsource language phrases to improve coverage andtranslation quality.In this paper, we focus on situations where no par-allel corpus is available (except a few hundred paral-lel sentences for tuning parameters).
To tackle theseextremely scarce training data situations, we pro-pose using a pivot language (English) to bridge the484source and target languages in translation.
We firsttranslate source language sentences or phrases intoEnglish and then translate those English sentencesor phrases into the target language, as described inSection 3.
We thus assume that there is a parallelcorpus consisting of the source language and En-glish as well as one consisting of English and the tar-get language.
Selecting English as a pivot languageis a reasonable pragmatic choice because English isincluded in parallel corpora more often than otherlanguages are, though any language can be used as apivot language.In Section 2, we describe a phrase-based statisti-cal machine translation (SMT) system that was usedto develop the pivot methods described in Section3.
This is the shared task baseline system for the2006 NAACL/HLT workshop on statistical machinetranslation (Koehn and Monz, 2006) and consists ofthe Pharaoh decoder (Koehn, 2004), SRILM (Stol-cke, 2002), GIZA++ (Och and Ney, 2003), mkcls(Och, 1999), Carmel,1 and a phrase model trainingcode.2 Phrase-based SMTWe use a phrase-based SMT system, Pharaoh,(Koehn et al, 2003; Koehn, 2004), which is basedon a log-linear formulation (Och and Ney, 2002).
Itis a state-of-the-art SMT system with freely avail-able software, as described in the introduction.The system segments the source sentence into so-called phrases (a number of sequences of consecu-tive words).
Each phrase is translated into a targetlanguage phrase.
Phrases may be reordered.Let f be a source sentence (e.g, French) and e be atarget sentence (e.g., English), the SMT system out-puts an e?
that satisfiese?
= argmaxe Pr(e|f) (1)= argmaxeM?m=1?mhm(e, f) (2)where hm(e, f) is a feature function and ?m is aweight.
The system uses a total of eight featurefunctions: a trigram language model probability ofthe target language, two phrase translation probabil-ities (both directions), two lexical translation prob-1http://www.isi.edu/licensed-sw/carmel/abilities (both directions), a word penalty, a phrasepenalty, and a linear reordering penalty.
For detailson these feature functions, please refer to (Koehn etal., 2003; Koehn, 2004; Koehn et al, 2005).
To setthe weights, ?m, we carried out minimum error ratetraining (Och, 2003) using BLEU (Papineni et al,2002) as the objective function.3 Pivot methodsWe use the phrase-based SMT system described inthe previous section to develop pivot methods.
Weuse English e as the pivot language.
We use Frenchf and German g as examples of the source and targetlanguages in this section.We describe two types of pivot strategies, namelyphrase translation and sentence translation.The phrase translation strategy means that we di-rectly construct a French-German phrase translationtable (phrase-table for short) from a French-Englishphrase-table and an English-German phrase-table.We assume that these French-English and English-German tables are built using the phrase model train-ing code in the baseline system described in theintroduction.
That is, phrases are heuristically ex-tracted from word-level alignments produced by do-ing GIZA++ training on the corresponding parallelcorpora (Koehn et al, 2003).The sentence translation strategy means that wefirst translate a French sentence into n English sen-tences and translate these n sentences into Germanseparately.
Then, we select the highest scoring sen-tence from the German sentences.3.1 Phrase translation strategyThe phrase translation strategy is based on the factthat the phrase-based SMT system needs a phrase-table and a language model for translation.
Usually,we have the language model of a target language.Consequently, we only need to construct a phrase-table to train the phrase-based SMT system.We assume that we have a French-English phrase-table TFE and an English-German phrase-tableTEG.
From these tables, we construct a French-German phrase-table TFG, which requires estimat-ing four feature functions; phrase translation prob-abilities for both directions, ?(f?
|g?)
and ?(g?|f?)
andlexical translation probabilities for both directions,485pw(f?
|g?)
and pw(g?|f?
), where f?
and g?
are French andGerman phrases that are parts of phrase translationpairs in TFE and TEG, respectively.2We estimate these probabilities using the proba-bilities available in TFE and TEG as follows.3?(f?
|g?)
=?e??TFE?TEG?(f?
|e?)?(e?|g?)
(3)?(g?|f?)
=?e??TFE?TEG?(g?|e?)?(e?|f?)
(4)pw(f?
|g?)
=?e??TFE?TEGpw(f?
|e?)pw(e?|g?)
(5)pw(g?|f?)
=?e??TFE?TEGpw(g?|e?)pw(e?|f?)
(6)where e?
?
TFE?TEG means that the English phrasee?
is included in both TFE and TEG as part of phrasetranslation pairs.
?(f?
|e?)
and ?(e?|f?)
are phrasetranslation probabilities for TFE and ?(e?|g?)
and?(g?|e?)
are those for TEG.
pw(f?
|e?)
and pw(e?|f?)
arelexical translation probabilities for TFE and pw(e?|g?
)and pw(g?|e?)
are those for TEG.The definitions of the phrase and lexical transla-tion probabilities are as follows (Koehn et al, 2003).?(f?
|e?)
= count(f?
, e?)?f?
?
count(f?
?, e?
)(7)where count(f?
, e?)
gives the total number of timesthe phrase f?
is aligned with the phrase e?
in the par-allel corpus.
Eq.
7 means that ?(f?
|e?)
is calculatedusing maximum likelihood estimation.The definition of the lexical translation probabil-ity ispw(f?
|e?)
= maxa pw(f?
|e?,a) (8)pw(f?
|e?,a) =n?i=1Ew(fi|e?,a) (9)Ew(fi|e?,a) = 1|{j|(i, j) ?
a}|??
(i,j)?aw(fi|ej)(10)2Feature functions scores are calculated using these proba-bilities.
For example, for a translation probability of a Frenchsentence f = f?1 .
.
.
f?K and a German sentence g = g?1 .
.
.
g?K ,h(g, f) = log?Ki=1 ?
(f?i|g?i), where K is the number ofphrases.3Wang et al (2006) use essentially the same definition toinduce the translation probability of the source and target lan-guage word alignment that is bridged by an intermediate lan-guage.
Callison-Burch et al (2006a) use a similar definition fora paraphrase probability.w(f |e) = count(f, e)?f ?
count(f ?, e)(11)where count(f, e) gives the total number of timesthe word f is aligned with the word e in the par-allel corpus.
Thus, w(f |e) is the maximum likeli-hood estimation of the word translation probabilityof f given e. Ew(fi|e?,a) is calculated from a wordalignment a between a phrase pair f?
= f1f2 .
.
.
fnand e?
= e1e2 .
.
.
em where fi is connected to several(|{j|(i, j) ?
a}|) English words.
Thus, Ew(fi|e?,a)is the average (or mixture) of w(fi|ej).
This meansthat Ew(fi|e?,a) is an estimation of the probabil-ity of fi in a. Consequently, pw(f?
|e?,a) estimatesthe probability of f?
given e?
and a using the prod-uct of the probabilities Ew(fi|e?,a).
This assumesthat the probability of fi is independent given e?
anda.
pw(f?
|e?)
takes the highest pw(f?
|e?,a) if thereare multiple alignments a.
This discussion, whichis partly based on Section 4.1.2 of (Och and Ney,2004), means that the lexical translation probabilitypw(f?
|e?)
is another probability estimated using theword translation probability w(f |e).The justification of Eqs.
3?6 is straightforward.From the discussion above, we know that the prob-abilities, ?(f?
|e?
), ?(e?|f?
), ?(g?|e?
), ?(e?|g?
), pw(f?
|e?),pw(e?|f?
), pw(g?|e?
), and pw(e?|g?)
are probabilities inthe ordinary sense.
Thus, we can derive ?(f?
|g?),?(g?|f?
), pw(f?
|g?
), and pw(g?|f?)
by assuming thatthese probabilities are independent given an Englishphrase e?
(e.g., ?(f?
|g?, e?)
= ?(f?
|e?
)).We construct a TFG that consists of all French-German phrases whose phrase and lexical transla-tion probabilities as defined in Eqs.
3?6 are greaterthan 0.
We use the term PhraseTrans to denote SMTsystems that use the phrase translation strategy de-scribed above.3.2 Sentence translation strategyThe sentence translation strategy uses two inde-pendently trained SMT systems.
We first trans-late a French sentence f into n English sentencese1, e2, ..., en using a French-English SMT system.Each ei (i = 1 .
.
.
n) has the eight scores calcu-lated from the eight feature functions described inSection 2.
We denote these scores hei1, hei2, .
.
.
hei8.Second, we translate each ei into n German sen-tences gi1,gi2, .
.
.
,gin using an English-German486SMT system.
Each gij (j = 1 .
.
.
n) has the eightscores, which are denoted as hgij1, hgij2, .
.
.
, hgij8.This situation is depicted asf ?
ei (hei1, hei2, .
.
.
, hei8)?
gij (hgij1, hgij2, .
.
.
, hgij8)We define the score of gij , S(gij), asS(gij) =8?m=1(?emheim + ?gmhgijm) (12)where ?em and ?gm are weights set by performingminimum error rate training4 as described in Section2.
We select the highest scoring German sentenceg?
= argmaxgij S(gij) (13)as the translation of the French sentence f .A drawback of this strategy is that translationspeed is about O(n) times slower than those of thecomponent SMT systems.
This is because we haveto run the English-German SMT system n times fora French sentence.
Consequently, we cannot set nvery high.
When we used n = 15 in the experi-ments described in Section 4, it took more than twodays to translate 3064 test sentences on a 3.06GHzLINUX machine.Note that when n = 1, the above strategy pro-duces the same translation with the simple sequen-tial method that we first translate a French sentenceinto an English sentence and then translate that sen-tence into a German sentence.We use the terms SntTrans15 and SntTrans1 to de-note SMT systems that use the sentence translationstrategy with n = 15 and n = 1, respectively.4 ExperimentsWe conducted controlled experiments using theEuroparl corpus.
For each language pair de-scribed below, the Europarl corpus provides three4We use a reranking strategy for the sentence translationstrategy.
We first obtain n2 German sentences for each Frenchsentence by applying two independently trained French-Englishand English-German SMT systems.
Each of the translated Ger-man sentences has the sixteen scores as described above.
Theweights in Eq.
12 are tuned against reference German sentencesby performing minimum error rate training.
These weights arein general different from those of the original French-Englishand English-German SMT systems.types of parallel corpora; the source language?English, English?the target language, and the sourcelanguage?the target language.
This means that wecan directly train an SMT system using the sourceand target language parallel corpus as well as pivotSMT systems using English as the pivot language.We use the term Direct to denote directly trainedSMT systems.
For each language pair, we com-pare four SMT systems; Direct, PhraseTrans, Snt-Trans15, and SntTrans1.54.1 Training, tuning and testing SMT systemsWe used the training data for the shared task ofthe SMT workshop (Koehn and Monz, 2006) totrain our SMT systems.
It consists of three paral-lel corpora: French-English, Spanish-English, andGerman-English.We used these three corpora to extract a set ofsentences that were aligned to each other across allfour languages.
For that purpose, we used Englishas the pivot.
For each distinct English sentence, weextracted the corresponding French, Spanish, andGerman sentences.
When an English sentence oc-curred multiple times, we extracted the most fre-quent translation.
For example, because ?Resump-tion of the session?
was translated into ?Wiederauf-nahme der Sitzungsperiode?
120 times and ?Wieder-aufnahme der Sitzung?
once, we extracted ?Wieder-aufnahme der Sitzungsperiode?
as its translation.Consequently, we extracted 585,830 sentences foreach language.
From these corpora, we constructedthe training parallel corpora for all language pairs.We followed the instruction of the shared taskbaseline system to train our SMT systems.6 Weused the trigram language models provided with theshared task.
We did minimum error rate training onthe first 500 sentences in the shared task develop-ment data to tune our SMT systems and used the5As discussed in the introduction, we intend to use the pivotstrategies in a situation where a very limited amount of paralleltext is available.
The use of the Europarl corpus is not an accu-rate simulation of the intended situation because it enables us touse a relatively large parallel corpus for direct training.
How-ever, it is necessary to evaluate the performance of the pivotstrategies against that of Direct SMT systems under controlledexperiments in order to determine how much the pivot strate-gies can be improved.
This is a first step toward the use of pivotmethods in situations where training data is extremely scarce.6The parameters for the Pharaoh decoder were ?-dl 4 -b 0.03-s 100?.
The maximum phrase length was 7.4873064 test sentences for each language as our test set.Our evaluation metric was %BLEU scores, as cal-culated by the script provided along with the sharedtask.7 We lowercased the training, development andtest sentences.4.2 ResultsTable 1 compares the BLEU scores of the four SMTsystems; Direct, PhraseTrans, SntTrans15, and Snt-Trans1 for each language pair.
The columns SE andET list the BLEU scores of the Direct SMT sys-tems trained on the source language?English andEnglish?the target language parallel corpora.
Thenumbers in the parentheses are the relative scoresof the pivot SMT systems, which were obtainedby dividing their BLEU scores by that of the cor-responding Direct system.
For example, for theSpanish?French language pair, the BLEU score ofthe Direct SMT system was 35.78, that of thePhraseTrans SMT system was 32.90, and the rela-tive performance was 0.92 = (32.90/35.78).
Forthe SntTrans15 SMT system, the BLEU score was29.49 and the relative performance was 0.82 =(29.49/35.78).The BLEU scores of the Direct SMT systemswere higher than those of the PhraseTrans SMT sys-tems for all six source-target language pairs.
ThePhraseTrans SMT systems performed better thanthe SntTrans15 SMT systems for all pairs.
TheSntTrans15 SMT systems were better than the Snt-Trans1 SMT systems for four pairs.
Accordingto the sign test, under the null hypothesis that theBLEU scores of two systems are equivalent, findingone system obtaining better BLEU scores on all sixlanguage pairs is statistically significant at the 5 %level.
Obtaining four better scores is not statisticallysignificant.
Thus, Table 1 indicatesDirect > PhraseTrans > SntTrans15 ?
SntTrans1where ?>?
and ???
means that the differences ofthe BLEU scores of the corresponding SMT systemsare statistically significant and insignificant, respec-tively.7Callison-Burch et al (2006b) show that in general a higherBLEU score is not necessarily indicative of better translationquality.
However, they also suggest that the use of BLEU isappropriate for comparing systems that use similar translationstrategies, which is the case with our experiments.As expected, the Direct SMT systems outper-formed the other systems.
We regard the BLEUscores of the Direct systems as the upperbound.
TheSntTrans15 SMT systems did not significantly out-perform the SntTrans1 SMT systems.
We think thatthis is because n = 15 was not large enough to covergood translation candidates.8 Selecting the highestscoring translation from a small pool did not alwayslead to better performance.
To improve the perfor-mance of the sentence translation strategy, we needto use a large n. However, this is not practical be-cause of the slow translation speed, as discussed inSection 3.2.The PhraseTrans SMT systems significantly out-performed the SntTrans15 and SntTrans1 systems.That is, the phrase translation strategy is betterthan the sentence translation strategy.
Since thephrase-tables constructed using the phrase transla-tion strategy can be integrated into the Pharaoh de-coder as well as the directly extracted phrase-tables,the PhraseTrans SMT systems can fully exploit thepower of the decoder.
This led to better performanceeven when the induced phrase-tables were noisy, asdescribed below.The relative performance of the PhraseTransSMT systems compared to the Direct SMT systemswas 0.92 to 0.97.
These are very promising re-sults.
To show how these systems translated thetest sentences, we translated some outputs of theSpanish-French Direct and PhraseTrans SMT sys-tems into English using the French-English Directsystem.
These are shown in Table 3 with the refer-ence English sentences.The relative performance seems to be related tothe BLEU scores for the Direct SMT systems.
Itwas relatively high (0.95 to 0.97) for the difficult (interms of BLEU) language pairs but relatively low(0.92) for the easy language pairs; Spanish?Frenchand French?Spanish.
There is a lot of room forimprovement for the relatively easy language pairs.This relationship is stronger than the relationship be-tween the BLEU scores for SE/ET and those for thePhraseTrans systems, where no clear trend exists.Table 2 shows the number of phrases stored in thephrase-tables.
The Direct SMT systems had 7.3 to8A typical reranking approach to SMT (Och et al, 2004)uses a 1000?best list.488Source?Target Direct PhraseTrans SntTrans15 SntTrans1 SE ETSpanish?French 35.78 > 32.90 (0.92) > 29.49 (0.82) > 29.16 (0.81) 29.31 28.80French?Spanish 34.16 > 31.49 (0.92) > 28.41 (0.83) > 27.99 (0.82) 27.59 29.07German?French 23.37 > 22.47 (0.96) > 22.03 (0.94) > 21.64 (0.93) 22.40 28.80French?German 15.27 > 14.51 (0.95) > 14.03 (0.92) < 14.21 (0.93) 27.59 15.81German?Spanish 22.34 > 21.76 (0.97) > 21.36 (0.96) > 20.97 (0.94) 22.40 29.07Spanish?German 15.50 > 15.11 (0.97) > 14.46 (0.93) < 14.61 (0.94) 29.31 15.81Table 1: BLEU scores and relative performanceNo.
of phrases (?M?
means 106)Direct PhraseTrans common R PS?F 18.2M 190.8M 6.3M 34.7 3.3F?S 18.2M 186.8M 6.3M 34.7 3.4G?F 7.3M 174.9M 3.1M 43.2 1.8F?G 7.3M 168.2M 3.1M 43.2 1.9G?S 7.5M 179.6M 3.3M 44.1 1.9S?G 7.6M 176.6M 3.3M 44.1 1.9?S?, ?F?, and ?G?
are the acronyms of Spanish, French, andGerman, respectively.
?X?Y?
means that ?X?
is the source lan-guage and ?Y?
is the target language.Table 2: Statistics for the phrase-tables18.2 million phrases, and the PhraseTrans systemshad 168.2 to 190.8 million phrases.
The numbers ofphrases stored in the PhraseTrans systems were verylarge compared to those of Direct systems.9 How-ever, this does not cause a computational problem indecoding because those phrases that do not appear insource sentences are filtered so that only the relevantphrases are used during decoding.The figures in the common column are the numberof phrases common to the Direct and PhraseTranssystems.
R (recall) and P (precision) are defined asfollows.R = No.
of common phrases ?
100No.
of phrases in Direct system9In Table 2, the PhraseTrans systems have more than 10xas many phrases as the Direct systems.
This can be explainedas follows.
Let fi be the fanout of an English phrase i, i.e.,fi is the number of phrase pairs containing the English phrasei in a phrase-table, then the size of the phrase-table is s1 =?ni=1 fi, where n is the number of distinct English phrases.When we combine two phrase-tables, the size of the combinedphrase table is roughly s2 =?ni=1 f2i .
Thus, the relative sizeof the combined phrase table is roughly r = s2s1 =E(f2)E(f) ,where E(f) = s1n and E(f2) = s2n are the averages overfi and f2i , respectively.
As an example, we calculated theseaverages for the German-English phrase table.
E(f) was 1.5,E(f2) was 43.7, and r was 28.9.
This shows that even if anaverage fanout is small, the size of a combined phrase table canbe very large.P = No.
of common phrases ?
100No.
of phrases in PhraseTrans systemRecall was reasonably high.
However, the upperbound of recall was 100 percent because we useda multilingual corpus whose sentences were alignedto each other across all four languages, as describedin Section 4.1.
Thus, there is a lot of room for im-provement with respect to recall.
Precision, on theother hand, was very low.
However, translation per-formance was not significantly affected by this lowprecision, as is shown in Table 1.
This indicates thatrecall is more important than precision in buildingphrase-tables.5 Related workPivot languages have been used in rule-based ma-chine translation systems.
Boitet (1988) discussesthe pros and cons of the pivot approaches in multi-lingual machine translation.
Schubert (1988) arguesthat a pivot language needs to be a natural language,due to the inherent lack of expressiveness of artifi-cial languages.Pivot-based methods have also been used in otherrelated areas, such as translation lexicon induc-tion (Schafer and Yarowsky, 2002), word alignment(Wang et al, 2006), and cross language informationretrieval (Gollins and Sanderson, 2001).
The trans-lation disambiguation techniques used in these stud-ies could be used for improving the quality of phrasetranslation tables.In contrast to these, very little work has beendone on pivot-based methods for SMT.
Kauers etal.
(2002) used an artificial interlingua for spokenlanguage translation.
Gispert and Marin?o (2006)created an English-Catalan parallel corpus by auto-matically translating the Spanish part of an English-Spanish parallel corpus into Catalan with a Spanish-Catalan SMT system.
They then directly trained anSMT system on the English-Catalan corpus.
They489showed that this direct training method is superiorto the sentence translation strategy (SntTrans1) intranslating Catalan into English but is inferior toit in the opposite translation direction (in terms ofthe BLEU score).
In contrast, we have shown thatthe phrase translation strategy consistently outper-formed the sentence translation strategy in the con-trolled experiments.6 ConclusionWe have compared two types of pivot strategies,namely phrase translation and sentence translation.The phrase translation strategy directly constructs aphrase translation table from a source language andEnglish phrase-table and a target language and En-glish phrase-table.
It then uses this phrase table ina phrase-based SMT system.
The sentence transla-tion strategy first translates a source language sen-tence into n English sentences and translates these nsentences into target language sentences separately.Then, it selects the highest scoring sentence from thetarget language sentences.We conducted controlled experiments using theEuroparl corpus to compare the performance ofthese two strategies to that of directly trained SMTsystems.
The experiments showed that the perfor-mance of the phrase translation strategy was statis-tically significantly better than that of the sentencetranslation strategy and that its relative performancecompared to the directly trained SMT systems was0.92 to 0.97.
These are very promising results.Although we used the Europarl corpus for con-trolled experiments, we intend to use the pivot strate-gies in situations where very limited amount of par-allel corpora are available for a source and target lan-guage but where relatively large parallel corpora areavailable for the source language?English and thetarget language?English.
In future work, we willfurther investigate the pivot strategies described inthis paper to confirm that the phrase translation strat-egy is better than the sentence translation strategy inthe intended situation as well as with the Europarlcorpus.1010As a first step towards real situations, we conducted addi-tional experiments.
We divided the training corpora in Section4 into two halves.
We used the first 292915 sentences to trainsource-English SMT systems and the remaining 292915 onesto train English-target SMT systems.
Based on these source-ReferencesChristian Boitet.
1988.
Pros and cons of the pivot andtransfer approaches in multilingual machine transla-tion.
In Dan Maxwell, Klaus Schubert, and ToonWitkam, editors, New Directions in Machine Trans-lation.
Foris.
(appeared in Sergei Nirenburg, HaroldSomers and Yorick Wilks (eds.)
Readings in MachineTranslation published by the MIT Press in 2003).Peter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1993.
The mathematicsof statistical machine translation: Parameter estima-tion.
Computational Linguistics, 19(2):263?311.Chris Callison-Burch, Philipp Koehn, and Miles Os-borne.
2006a.
Improved statistical machine transla-tion using paraphrases.
In NAACL.Chris Callison-Burch, Miles Osborne, and PhilippKoehn.
2006b.
Re-evaluating the role of BLEU inmachine translation research.
In EACL.Adria?
de Gispert and Jose?
B. Mari no.
2006.
Catalan-English statistical machine translation without parallelcorpus: Bridging through Spanish.
In Proc.
of LREC5th Workshop on Strategies for developing MachineTranslation for Minority Languages.Tim Gollins and Mark Sanderson.
2001.
Improvingcross language information retrieval with triangulatedtranslation.
In SIGIR.Manuel Kauers, Stephan Vogel, Christian Fu?gen, andAlex Waibel.
2002.
Interlingua based statistical ma-chine translation.
In ICSLP.Philipp Koehn and Christof Monz.
2006.
Manual and au-tomatic evaluation of machine translation between eu-ropean languages.
In Proceedings on the Workshop onStatistical Machine Translation, pages 102?121, NewYork City, June.
Association for Computational Lin-guistics.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In HLT-NAACL.English and English-target SMT systems, we trained Phrase-Trans and SntTrans1 SMT systems.
Other experimental condi-tions were the same as those described in Section 4.
The tablebelow shows the BLUE scores of these SMT systems.
It indi-cates that the PhraseTrans systems consistently outperformedthe SntTrans1 systems.Source-Target PhraseTrans SntTrans1Spanish-French 31.57 28.36French-Spanish 30.18 27.75German-French 20.48 19.83French-German 14.38 14.11German-Spanish 19.58 18.67Spanish-German 14.80 14.46490Ref i hope with all my heart , and i must say this quite emphatically , that an opportunity will arise when thisdocument can be incorporated into the treaties at some point in the future .Dir i hope with conviction , and put great emphasis , that again is a serious possibility of including this in the treaties .PT i hope with conviction , and i very much , insisted that never be a serious possibility of including this in thetreaties .Ref should this fail to materialise , we should not be surprised if public opinion proves sceptical about europe , or evenrejects it .Dir otherwise , we must not be surprised by the scepticism , even the rejection of europe in the public .PT otherwise , we must not be surprised by the scepticism , and even the rejection of europe in the public .Ref the intergovernmental conference - to address a third subject - on the reform of the european institutions is also ofdecisive significance for us in parliament .Dir the intergovernmental conference - and this i turn to the third issue on the reform of the european institutions is ofenormous importance for the european parliament .PT the intergovernmental conference - and this brings me to the third issue - on the reform of the european institutionshas enormous importance for the european parliament .Table 3: Reference sentences (Ref) and the English translations (by the French-English Direct system) ofthe outputs of the Spanish-French Direct and PhraseTrans SMT systems (Dir and PT).Philipp Koehn, Amittai Axelrod, Alexandra BirchMayne, Chris Callison-Burch, Miles Osborne, andDavid Talbot.
2005.
Edinburgh system descriptionfor the 2005 IWSLT speech translation evaluation.
InIWSLT.Philipp Koehn.
2004.
Pharaoh: a beam search decoderfor phrase-based statistical machine translation mod-els.
In AMTA.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In MT Summit.Dragos Stefan Munteanu and Daniel Marcu.
2005.
Im-proving machine translation performance by exploit-ing non-parallel corpora.
Computational Linguistics,31(4):477?504.Makoto Nagao.
1981.
A framework of a mechani-cal translation between Japanese and English by anal-ogy principle.
In the International NATO Symposiumon Artificial and Human Intelligence.
(appeared inSergei Nirenburg, Harold Somers and Yorick Wilks(eds.)
Readings in Machine Translation published bythe MIT Press in 2003).Sonja Nie?en and Hermann Ney.
2004.
Statistical ma-chine translation with scarce resources using morpho-syntactic information.
Computational Linguistics,30(2):181?204.Franz Josef Och and Hermann Ney.
2002.
Discrimina-tive training and maximum entropy models for statis-tical machine translation.
In ACL.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51.Franz Josef Och and Hermann Ney.
2004.
The align-ment template approach to statistical machine transla-tion.
Computational Linguistics, 30(4):417?449.Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,Anoop Sarkar, Kenji Yamada, Alex Fraser, ShankarKumar, Libin Shen, David Smith, Katherine Eng,Viren Jain, Zhen Jin, and Dragomir Radev.
2004.
Asmorgasbord of features for statistical machine trans-lation.
In HLT-NAACL.Franz Josef Och.
1999.
An efficient method for deter-mining bilingual word classes.
In EACL.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In ACL.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In ACL.Philip Resnik and Noah A. Smith.
2003.
The webas a parallel corpus.
Computational Linguistics,29(3):349?380.Charles Schafer and David Yarowsky.
2002.
Induc-ing translation lexicons via diverse similarity measuresand bridge languages.
In CoNLL.Klaus Schubert.
1988.
Implicitness as a guiding princi-ple in machine translation.
In COLING.Andreas Stolcke.
2002.
SRILM - an extensible languagemodeling toolkit.
In ICSLP.Masao Utiyama and Hitoshi Isahara.
2003.
Reliablemeasures for aligning Japanese-English news articlesand sentences.
In ACL, pages 72?79.Haifeng Wang, Hua Wu, and Zhanyi Liu.
2006.
Wordalignment for languages with scarce resources usingbilingual corpora of other language pairs.
In COL-ING/ACL 2006 Main Conference Poster Sessions.491
