Proceedings of ACL-08: HLT, pages 308?316,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsA Joint Model of Text and Aspect Ratings for Sentiment SummarizationIvan TitovDepartment of Computer ScienceUniversity of Illinois at Urbana-ChampaignUrbana, IL 61801titov@uiuc.eduRyan McDonaldGoogle Inc.76 Ninth AvenueNew York, NY 10011ryanmcd@google.comAbstractOnline reviews are often accompanied withnumerical ratings provided by users for a setof service or product aspects.
We proposea statistical model which is able to discovercorresponding topics in text and extract tex-tual evidence from reviews supporting each ofthese aspect ratings ?
a fundamental problemin aspect-based sentiment summarization (Huand Liu, 2004a).
Our model achieves high ac-curacy, without any explicitly labeled data ex-cept the user provided opinion ratings.
Theproposed approach is general and can be usedfor segmentation in other applications wheresequential data is accompanied with corre-lated signals.1 IntroductionUser generated content represents a unique source ofinformation in which user interface tools have facil-itated the creation of an abundance of labeled con-tent, e.g., topics in blogs, numerical product and ser-vice ratings in user reviews, and helpfulness rank-ings in online discussion forums.
Many previousstudies on user generated content have attempted topredict these labels automatically from the associ-ated text.
However, these labels are often presentin the data already, which opens another interestingline of research: designing models leveraging theselabelings to improve a wide variety of applications.In this study, we look at the problem of aspect-based sentiment summarization (Hu and Liu, 2004a;Popescu and Etzioni, 2005; Gamon et al, 2005;Nikos?
Fine DiningFood 4/5 ?Best fish in the city?, ?Excellent appetizers?Decor 3/5 ?Cozy with an old world feel?, ?Too dark?Service 1/5 ?Our waitress was rude?, ?Awful service?Value 5/5 ?Good Greek food for the $?, ?Great price!
?Figure 1: An example aspect-based summary.Carenini et al, 2006; Zhuang et al, 2006).1 Anaspect-based summarization system takes as inputa set of user reviews for a specific product or ser-vice and produces a set of relevant aspects, the ag-gregated sentiment for each aspect, and supportingtextual evidence.
For example, figure 1 summarizesa restaurant using aspects food, decor, service, andvalue plus a numeric rating out of 5.Standard aspect-based summarization consists oftwo problems.
The first is aspect identification andmention extraction.
Here the goal is to find the setof relevant aspects for a rated entity and extract alltextual mentions that are associated with each.
As-pects can be fine-grained, e.g., fish, lamb, calamari,or coarse-grained, e.g., food, decor, service.
Sim-ilarly, extracted text can range from a single wordto phrases and sentences.
The second problem issentiment classification.
Once all the relevant as-pects and associated pieces of texts are extracted,the system should aggregate sentiment over each as-pect to provide the user with an average numeric orsymbolic rating.
Sentiment classification is a wellstudied problem (Wiebe, 2000; Pang et al, 2002;Turney, 2002) and in many domains users explicitly1We use the term aspect to denote properties of an objectthat can be rated by a user as in Snyder and Barzilay (2007).Other studies use the term feature (Hu and Liu, 2004b).308Food: 5; Decor: 5; Service: 5; Value: 5The chicken was great.
On top of that our service wasexcellent and the price was right.
Can?t wait to go back!Food: 2; Decor: 1; Service: 3; Value: 2We went there for our anniversary.
My soup was cold andexpensive plus it felt like they hadn?t painted since 1980.Food: 3; Decor: 5; Service: 4; Value: 5The food is only mediocre, but well worth the cost.Wait staff was friendly.
Lot?s of fun decorations.
?Food ?The chicken was great?, ?My soup wascold?, ?The food is only mediocre?Decor ?it felt like they hadn?t painted since1980?, ?Lots of fun decorations?Service ?service was excellent?,?Wait staff was friendly?Value ?the price was right?, ?My soup was coldand expensive?, ?well worth the cost?Figure 2: Extraction problem: Produce aspect mentions from a corpus of aspect rated reviews.provide ratings for each aspect making automatedmeans unnecessary.2 Aspect identification has alsobeen thoroughly studied (Hu and Liu, 2004b; Ga-mon et al, 2005; Titov and McDonald, 2008), butagain, ontologies and users often provide this infor-mation negating the need for automation.Though it may be reasonable to expect a user toprovide a rating for each aspect, it is unlikely thata user will annotate every sentence and phrase in areview as being relevant to some aspect.
Thus, itcan be argued that the most pressing challenge inan aspect-based summarization system is to extractall relevant mentions for each aspect, as illustratedin figure 2.
When labeled data exists, this prob-lem can be solved effectively using a wide varietyof methods available for text classification and in-formation extraction (Manning and Schutze, 1999).However, labeled data is often hard to come by, es-pecially when one considers all possible domains ofproducts and services.
Instead, we propose an un-supervised model that leverages aspect ratings thatfrequently accompany an online review.In order to construct such model, we make twoassumptions.
First, ratable aspects normally repre-sent coherent topics which can be potentially dis-covered from co-occurrence information in the text.Second, we hypothesize that the most predictive fea-tures of an aspect rating are features derived fromthe text segments discussing the corresponding as-pect.
Motivated by these observations, we constructa joint statistical model of text and sentiment ratings.The model is at heart a topic model in that it as-signs words to a set of induced topics, each of whichmay represent one particular aspect.
The model isextended through a set of maximum entropy classi-fiers, one per each rated aspect, that are used to pre-2E.g., http://zagat.com and http://tripadvisor.com.dict the sentiment rating towards each of the aspects.However, only the words assigned to an aspects cor-responding topic are used in predicting the ratingfor that aspect.
As a result, the model enforces thatwords assigned to an aspects?
topic are predictive ofthe associated rating.
Our approach is more generalthan the particular statistical model we consider inthis paper.
For example, other topic models can beused as a part of our model and the proposed class ofmodels can be employed in other tasks beyond senti-ment summarization, e.g., segmentation of blogs onthe basis of topic labels provided by users, or topicdiscovery on the basis of tags given by users on so-cial bookmarking sites.3The rest of the paper is structured as follows.
Sec-tion 2 begins with a discussion of the joint text-sentiment model approach.
In Section 3 we provideboth a qualitative and quantitative evaluation of theproposed method.
We conclude in Section 4 with anexamination of related work.2 The ModelIn this section we describe a new statistical modelcalled the Multi-Aspect Sentiment model (MAS),which consists of two parts.
The first part is based onMulti-Grain Latent Dirichlet Allocation (Titov andMcDonald, 2008), which has been previously shownto build topics that are representative of ratable as-pects.
The second part is a set of sentiment pre-dictors per aspect that are designed to force specifictopics in the model to be directly correlated with aparticular aspect.2.1 Multi-Grain LDAThe Multi-Grain Latent Dirichlet Allocation model(MG-LDA) is an extension of Latent Dirichlet Allo-cation (LDA) (Blei et al, 2003).
As was demon-3See e.g.
del.ico.us (http://del.ico.us).309strated in Titov and McDonald (2008), the topicsproduced by LDA do not correspond to ratable as-pects of entities.
In particular, these models tend tobuild topics that globally classify terms into productinstances (e.g., Creative Labs Mp3 players versusiPods, or New York versus Paris Hotels).
To com-bat this, MG-LDA models two distinct types of top-ics: global topics and local topics.
As in LDA, thedistribution of global topics is fixed for a document(a user review).
However, the distribution of localtopics is allowed to vary across the document.A word in the document is sampled either fromthe mixture of global topics or from the mixture oflocal topics specific to the local context of the word.It was demonstrated in Titov and McDonald (2008)that ratable aspects will be captured by local topicsand global topics will capture properties of revieweditems.
For example, consider an extract from a re-view of a London hotel: ?.
.
.
public transport in Lon-don is straightforward, the tube station is about an 8minute walk .
.
.
or you can get a bus for ?1.50?.
Itcan be viewed as a mixture of topic London sharedby the entire review (words: ?London?, ?tube?, ???
),and the ratable aspect location, specific for the localcontext of the sentence (words: ?transport?, ?walk?,?bus?).
Local topics are reused between very differ-ent types of items, whereas global topics correspondonly to particular types of items.In MG-LDA a document is represented as a setof sliding windows, each covering T adjacent sen-tences within a document.4 Each window v in docu-ment d has an associated distribution over local top-ics ?locd,v and a distribution defining preference for lo-cal topics versus global topics pid,v.
A word can besampled using any window covering its sentence s,where the window is chosen according to a categor-ical distribution ?d,s.
Importantly, the fact that win-dows overlap permits the model to exploit a largerco-occurrence domain.
These simple techniques arecapable of modeling local topics without more ex-pensive modeling of topic transitions used in (Grif-fiths et al, 2004; Wang and McCallum, 2005; Wal-lach, 2006; Gruber et al, 2007).
Introduction of asymmetrical Dirichlet prior Dir(?)
for the distribu-tion ?d,s can control the smoothness of transitions.4Our particular implementation is over sentences, but slidingwindows in theory can be over any sized fragment of text.
(a) (b)Figure 3: (a) MG-LDA model.
(b) An extension of MG-LDA to obtain MAS.The formal definition of the model with Kglglobal and K loc local topics is as follows: First,draw Kgl word distributions for global topics ?glzfrom a Dirichlet prior Dir(?gl) and K loc word dis-tributions for local topics ?locz?
- from Dir(?loc).Then, for each document d:?
Choose a distribution of global topics ?gld ?
Dir(?gl).?
For each sentence s choose a distribution over slidingwindows ?d,s(v) ?
Dir(?).?
For each sliding window v?
choose ?locd,v ?
Dir(?loc),?
choose pid,v ?
Beta(?mix).?
For each word i in sentence s of document d?
choose window vd,i ?
?d,s,?
choose rd,i ?
pid,vd,i ,?
if rd,i = gl choose global topic zd,i ?
?gld ,?
if rd,i= loc choose local topic zd,i?
?locd,vd,i ,?
choose word wd,i from the word distribution ?rd,izd,i .Beta(?mix) is a prior Beta distribution for choos-ing between local and global topics.
In Figure 3a thecorresponding graphical model is presented.2.2 Multi-Aspect Sentiment ModelMG-LDA constructs a set of topics that ideally cor-respond to ratable aspects of an entity (often in amany-to-one relationship of topics to aspects).
Amajor shortcoming of this model ?
and all other un-supervised models ?
is that this correspondence isnot explicit, i.e., how does one say that topic X is re-ally about aspect Y?
However, we can observe thatnumeric aspect ratings are often included in our databy users who left the reviews.
We then make theassumption that the text of the review discussing anaspect is predictive of its rating.
Thus, if we modelthe prediction of aspect ratings jointly with the con-struction of explicitly associated topics, then such a310model should benefit from both higher quality topicsand a direct assignment from topics to aspects.
Thisis the basic idea behind the Multi-Aspect Sentimentmodel (MAS).In its simplest form, MAS introduces a classifierfor each aspect, which is used to predict its rating.Each classifier is explicitly associated to a singletopic in the model and only words assigned to thattopic can participate in the prediction of the senti-ment rating for the aspect.
However, it has been ob-served that ratings for different aspects can be cor-related (Snyder and Barzilay, 2007), e.g., very neg-ative opinion about room cleanliness is likely to re-sult not only in a low rating for the aspect rooms,but also is very predictive of low ratings for the as-pects service and dining.
This complicates discoveryof the corresponding topics, as in many reviews themost predictive features for an aspect rating mightcorrespond to another aspect.
Another problem withthis overly simplistic model is the presence of opin-ions about an item in general without referring toany particular aspect.
For example, ?this product isthe worst I have ever purchased?
is a good predic-tor of low ratings for every aspect.
In such cases,non-aspect ?background?
words will appear to be themost predictive.
Therefore, the use of the aspect sen-timent classifiers based only on the words assignedto the corresponding topics is problematic.
Such amodel will not be able to discover coherent topicsassociated with each aspect, because in many casesthe most predictive fragments for each aspect ratingwill not be the ones where this aspect is discussed.Our proposal is to estimate the distribution of pos-sible values of an aspect rating on the basis of theoverall sentiment rating and to use the words as-signed to the corresponding topic to compute cor-rections for this aspect.
An aspect rating is typicallycorrelated to the overall sentiment rating5 and thefragments discussing this particular aspect will helpto correct the overall sentiment in the appropriate di-rection.
For example, if a review of a hotel is gen-erally positive, but it includes a sentence ?the neigh-borhood is somewhat seedy?
then this sentence ispredictive of rating for an aspect location being be-low other ratings.
This rectifies the aforementioned5In the dataset used in our experiments all three aspect rat-ings are equivalent for 5,250 reviews out of 10,000.problems.
First, aspect sentiment ratings can oftenbe regarded as conditionally independent given theoverall rating, therefore the model will not be forcedto include in an aspect topic any words from otheraspect topics.
Secondly, the fragments discussingoverall opinion will influence the aspect rating onlythrough the overall sentiment rating.
The overallsentiment is almost always present in the real dataalong with the aspect ratings, but it can be coarselydiscretized and we preferred to use a latent overallsentiment.The MAS model is presented in Figure 3b.
Notethat for simplicity we decided to omit in the figurethe components of the MG-LDA model other thanvariables r, z and w, though they are present in thestatistical model.
MAS also allows for extra unasso-ciated local topics in order to capture aspects not ex-plicitly rated by the user.
As in MG-LDA, MAS hasglobal topics which are expected to capture topicscorresponding to particular types of items, such Lon-don hotels or seaside resorts for the hotel domain.
Infigure 3b we shaded the aspect ratings ya, assumingthat every aspect rating is present in the data (thoughin practice they might be available only for some re-views).
In this model the distribution of the overallsentiment rating yov is based on all the n-gram fea-tures of a review text.
Then the distribution of ya, forevery rated aspect a, can be computed from the dis-tribution of yov and from any n-gram feature whereat least one word in the n-gram is assigned to theassociated aspect topic (r = loc, z = a).Instead of having a latent variable yov,6 we use asimilar model which does not have an explicit no-tion of yov.
The distribution of a sentiment rating yafor each rated aspect a is computed from two scores.The first score is computed on the basis of all the n-grams, but using a common set of weights indepen-dent of the aspect a.
Another score is computed onlyusing n-grams associated with the related topic, butan aspect-specific set of weights is used in this com-putation.
More formally, we consider the log-lineardistribution:P (ya = y|w, r, z)?exp(bay+?f?wJf,y+paf,r,zJaf,y), (1)where w, r, z are vectors of all the words in a docu-6Preliminary experiments suggested that this is also a feasi-ble approach, but somewhat more computationally expensive.311ment, assignments of context (global or local) andtopics for all the words in the document, respec-tively.
bay is the bias term which regulates the priordistribution P (ya = y), f iterates through all then-grams, Jy,f and Jay,f are common weights andaspect-specific weights for n-gram feature f .
paf,r,zis equal to a fraction of words in n-gram feature fassigned to the aspect topic (r = loc, z = a).2.3 Inference in MASExact inference in the MAS model is intractable.Following Titov and McDonald (2008) we use a col-lapsed Gibbs sampling algorithm that was derivedfor the MG-LDA model based on the Gibbs sam-pling method proposed for LDA in (Griffiths andSteyvers, 2004).
Gibbs sampling is an example of aMarkov Chain Monte Carlo algorithm (Geman andGeman, 1984).
It is used to produce a sample froma joint distribution when only conditional distribu-tions of each variable can be efficiently computed.In Gibbs sampling, variables are sequentially sam-pled from their distributions conditioned on all othervariables in the model.
Such a chain of model statesconverges to a sample from the joint distribution.
Anaive application of this technique to LDA wouldimply that both assignments of topics to words zand distributions ?
and ?
should be sampled.
How-ever, (Griffiths and Steyvers, 2004) demonstratedthat an efficient collapsed Gibbs sampler can be con-structed, where only assignments z need to be sam-pled, whereas the dependency on distributions ?
and?
can be integrated out analytically.In the case of MAS we also use maximum a-posteriori estimates of the sentiment predictor pa-rameters bay, Jy,f and Jay,f .
The MAP estimates forparameters bay , Jy,f and Jay,f are obtained by us-ing stochastic gradient ascent.
The direction of thegradient is computed simultaneously with running achain by generating several assignments at each stepand averaging over the corresponding gradient esti-mates.
For details on computing gradients for log-linear graphical models with Gibbs sampling we re-fer the reader to (Neal, 1992).Space constraints do not allow us to present eitherthe derivation or a detailed description of the sam-pling algorithm.
However, note that the conditionaldistribution used in sampling decomposes into twoparts:P (vd,i = v, rd,i = r, zd,i = z|v?, r?, z?,w, y) ?
?d,iv,r,z ?
?d,ir,z, (2)where v?, r?
and z?
are vectors of assignments ofsliding windows, context (global or local) and top-ics for all the words in the collection except for theconsidered word at position i in document d; y is thevector of sentiment ratings.
The first factor ?d,iv,r,z isresponsible for modeling co-occurrences on the win-dow and document level and coherence of the topics.This factor is proportional to the conditional distri-bution used in the Gibbs sampler of the MG-LDAmodel (Titov and McDonald, 2008).
The last fac-tor quantifies the influence of the assignment of theword (d, i) on the probability of the sentiment rat-ings.
It appears only if ratings are known (observ-able) and equals:?d,ir,z =?aP (yda|w, r?, rd,i = r, z?, zd,i = z)P (yda|w, r?, z?, rd,i = gl),where the probability distribution is computed as de-fined in expression (1), yda is the rating for the athaspect of review d.3 ExperimentsIn this section we present qualitative and quantita-tive experiments.
For the qualitative analysis weshow that topics inferred by the MAS model cor-respond directly to the associated aspects.
For thequantitative analysis we show that the MAS modelinduces a distribution over the rated aspects whichcan be used to accurately predict whether a text frag-ment is relevant to an aspect or not.3.1 Qualitative EvaluationTo perform qualitative experiments we used a setof reviews of hotels taken from TripAdvisor.com7that contained 10,000 reviews (109,024 sentences,2,145,313 words in total).
Every review wasrated with at least three aspects: service, locationand rooms.
Each rating is an integer from 1 to 5.The dataset was tokenized and sentence split auto-matically.7(c) 2005-06, TripAdvisor, LLC All rights reserved312rated aspect top wordsservice staff friendly helpful service desk concierge excellent extremely hotel great reception english pleasant helplocation hotel walk location station metro walking away right minutes close bus city located just easy restaurantslocal rooms room bathroom shower bed tv small water clean comfortable towels bath nice large pillows space beds tubtopics - breakfast free coffee internet morning access buffet day wine nice lobby complimentary included good fruit- $ night parking rate price paid day euros got cost pay hotel worth euro expensive car extra deal booked- room noise night street air did door floor rooms open noisy window windows hear outside problem quiet sleepglobal - moscow st russian petersburg nevsky russia palace hermitage kremlin prospect river prospekt kempinskitopics - paris tower french eiffel dame notre rue st louvre rer champs opera elysee george parisian du pantheon cafesTable 1: Top words from MAS for hotel reviews.Krooms top words2 rooms clean hotel room small nice comfortable modern good quite large lobby old decor spacious decorated bathroom sizeroom noise night street did air rooms door open noisy window floor hear windows problem outside quiet sleep bit light3 room clean bed comfortable rooms bathroom small beds nice large size tv spacious good double big space huge kingroom floor view rooms suite got views given quiet building small balcony upgraded nice high booked asked overlookingroom bathroom shower air water did like hot small towels door old window toilet conditioning open bath dirty wall tub4 room clean rooms comfortable bed small beds nice bathroom size large modern spacious good double big quiet decoratedcheck arrived time day airport early room luggage took late morning got long flight ready minutes did taxi bags wentroom noise night street did air rooms noisy open door hear windows window outside quiet sleep problem floor conditioningbathroom room shower tv bed small water towels bath tub large nice toilet clean space toiletries flat wall sink screenTable 2: Top words for aspect rooms with different number of topicsKrooms.We ran the sampling chain for 700 iterations toproduce a sample.
Distributions of words in eachtopic were estimated as the proportion of words as-signed to each topic, taking into account topic modelpriors ?gl and ?loc.
The sliding windows were cho-sen to cover 3 sentences for all the experiments.
Allthe priors were chosen to be equal to 0.1.
We used15 local topics and 30 global topics.
In the model,the first three local topics were associated to therating classifiers for each aspects.
As a result, wewould expect these topics to correspond to the ser-vice, location, and rooms aspects respectively.
Un-igram and bigram features were used in the senti-ment predictors in the MAS model.
Before apply-ing the topic models we removed punctuation andalso removed stop words using the standard list ofstop words,8 however, all the words and punctuationwere used in the sentiment predictors.It does not take many chain iterations to discoverinitial topics.
This happens considerably faster thanthe appropriate weights of the sentiment predictorbeing learned.
This poses a problem, because, in thebeginning, the sentiment predictors are not accurateenough to force the model to discover appropriatetopics associated with each of the rated aspects.
Andas soon as topic are formed, aspect sentiment predic-tors cannot affect them anymore because they do not8http://www.dcs.gla.ac.uk/idom/ir resources/linguistic utils/stop wordshave access to the true words associated with theiraspects.
To combat this problem we first train thesentiment classifiers by assuming that paf,r,z is equalfor all the local topics, which effectively ignores thetopic model.
Then we use the estimated parame-ters within the topic model.9 Secondly, we mod-ify the sampling algorithm.
The conditional prob-ability used in sampling, expression (2), is propor-tional to the product of two factors.
The first factor,?d,iv,r,z , expresses a preference for topics likely fromthe co-occurrence information, whereas the secondone, ?d,ir,z , favors the choice of topics which are pre-dictive of the observable sentiment ratings.
We used(?d,ir,z)1+0.95tq in the sampling distribution instead of?d,ir,z , where t is the iteration number.
q was chosento be 4, though the quality of the topics seemed tobe indistinguishable with any q between 3 and 10.This can be thought of as having 1 + 0.95tq ratingsinstead of a single vector assigned to each review,i.e., focusing the model on prediction of the ratingsrather than finding the topic labels which are good atexplaining co-occurrences of words.
These heuris-tics influence sampling only during the first itera-tions of the chain.Top words for some of discovered local topics, in-9Initial experiments suggested that instead of doing this?pre-training?
we could start with very large priors ?loc and?mix, and then reduce them through the course of training.However, this is significantly more computationally expensive.31301020304050607080901000  10  20  30  40  50  60  70  80  90  100RecallPrecisiontopic modelmax?ent classifiertopic modelmax?ent classifier01020304050607080901000  10  20  30  40  50  60  70  80  90  100RecallPrecisionmax?ent classifier1 topic2 topics3 topics4 topics01020304050607080901000  10  20  30  40  50  60  70  80  90  100RecallPrecision(a) (b) (c)Figure 4: (a) Aspect service.
(b) Aspect location.
(c) Aspect rooms.cluding the first 3 topics associated with the rated as-pects, and also top words for some of global topicsare presented in Table 1.
We can see that the modeldiscovered as its first three topics the correct associ-ated aspects: service, location, and rooms.
Other lo-cal topics, as for the MG-LDA model, correspond toother aspects discussed in reviews (breakfast, prices,noise), and as it was previously shown in Titov andMcDonald (2008), aspects for global topics corre-spond to the types of reviewed items (hotels in Rus-sia, Paris hotels) or background words.Notice though, that the 3rd local topic induced forthe rating rooms is slightly narrow.
This can be ex-plained by the fact that the aspect rooms is a centralaspect of hotel reviews.
A very significant fractionof text in every review can be thought of as a part ofthe aspect rooms.
These portions of reviews discussdifferent coherent sub-aspects related to the aspectrooms, e.g., the previously discovered topic noise.Therefore, it is natural to associate several topics tosuch central aspects.
To test this we varied the num-ber of topics associated with the sentiment predictorfor the aspect rooms.
Top words for resulting top-ics are presented in Table 2.
It can be observed thatthe topic model discovered appropriate topics whilethe number of topics was below 4.
With 4 topicsa semantically unrelated topic (check-in/arrival) isinduced.
Manual selection of the number of topicsis undesirable, but this problem can be potentiallytackled with Dirichlet Process priors or a topic splitcriterion based on the accuracy of the sentiment pre-dictor in the MAS model.
We found that both ser-vice and location did not benefit by the assignmentof additional topics to their sentiment rating models.The experimental results suggest that the MASmodel is reliable in the discovery of topics corre-sponding to the rated aspects.
In the next sectionwe will show that the induced topics can be used toaccurately extract fragments for each aspect.3.2 Sentence LabelingA primary advantage of MAS over unsupervisedmodels, such as MG-LDA or clustering, is that top-ics are linked to a rated aspect, i.e., we know ex-actly which topics model which aspects.
As a re-sult, these topics can be directly used to extract tex-tual mentions that are relevant for an aspect.
To testthis, we hand labeled 779 random sentences fromthe dataset considered in the previous set of experi-ments.
The sentences were labeled with one or moreaspects.
Among them, 164, 176 and 263 sentenceswere labeled as related to aspects service, locationand rooms, respectively.
The remaining sentenceswere not relevant to any of the rated aspects.We compared two models.
The first model usesthe first three topics of MAS to extract relevant men-tions based on the probability of that topic/aspect be-ing present in the sentence.
To obtain these probabil-ities we used estimators based on the proportion ofwords in the sentence assigned to an aspects?
topicand normalized within local topics.
To improve thereliability of the estimator we produced 100 sam-ples for each document while keeping assignmentsof the topics to all other words in the collection fixed.The probability estimates were then obtained by av-eraging over these samples.
We did not performany model selection on the basis of the hand-labeleddata, and tested only a single model of each type.314For the second model we trained a maximum en-tropy classifier, one per each aspect, using 10-foldcross validation and unigram/bigram features.
Notethat this is a supervised system and as such repre-sents an upper-bound in performance one might ex-pect when comparing an unsupervised model suchas MAS.
We chose this comparison to demonstratethat our model can find relevant text mentions withhigh accuracy relative to a supervised model.
It isdifficult to compare our model to other unsupervisedsystems such as MG-LDA or LDA.
Again, this isbecause those systems have no mechanism for di-rectly correlating topics or clusters to correspondingaspects, highlighting the benefit of MAS.The resulting precision-recall curves for the as-pects service, location and rooms are presentedin Figure 4.
In Figure 4c, we varied the numberof topics associated with the aspect rooms.10 Theaverage precision we obtained (the standard mea-sure proportional to the area under the curve) is75.8%, 85.5% for aspects service and location, re-spectively.
For the aspect rooms these scores areequal to 75.0%, 74.5%, 87.6%, 79.8% with 1?4 top-ics per aspect, respectively.
The logistic regressionmodels achieve 80.8%, 94.0% and 88.3% for the as-pects service, location and rooms.
We can observethat the topic model, which does not use any explic-itly aspect-labeled text, achieves accuracies lowerthan, but comparable to a supervised model.4 Related WorkThere is a growing body of work on summariz-ing sentiment by extracting and aggregating senti-ment over ratable aspects and providing correspond-ing textual evidence.
Text excerpts are usually ex-tracted through string matching (Hu and Liu, 2004a;Popescu and Etzioni, 2005), sentence clustering(Gamon et al, 2005), or through topic models (Meiet al, 2007; Titov and McDonald, 2008).
String ex-traction methods are limited to fine-grained aspectswhereas clustering and topic model approaches mustresort to ad-hoc means of labeling clusters or topics.However, this is the first work we are aware of thatuses a pre-defined set of aspects plus an associatedsignal to learn a mapping from text to an aspect for10To improve readability we smoothed the curve for the as-pect rooms.the purpose of extraction.A closely related model to ours is that of Mei etal.
(2007) which performs joint topic and sentimentmodeling of collections.
Our model differs fromtheirs in many respects: Mei et al only model senti-ment predictions for the entire document and not onthe aspect level; They treat sentiment predictions asunobserved variables, whereas we treat them as ob-served signals that help to guide the creation of top-ics; They model co-occurrences solely on the docu-ment level, whereas our model is based onMG-LDAand models both local and global contexts.Recently, Blei and McAuliffe (2008) proposed anapproach for joint sentiment and topic modeling thatcan be viewed as a supervised LDA (sLDA) modelthat tries to infer topics appropriate for use in agiven classification or regression problem.
MAS andsLDA are similar in that both use sentiment predic-tions as an observed signal that is predicted by themodel.
However, Blei et al do not consider multi-aspect ranking or look at co-occurrences beyond thedocument level, both of which are central to ourmodel.
Parallel to this study Branavan et al (2008)also showed that joint models of text and user anno-tations benefit extractive summarization.
In partic-ular, they used signals from pros-cons lists whereasour models use aspect rating signals.5 ConclusionsIn this paper we presented a joint model of text andaspect ratings for extracting text to be displayed insentiment summaries.
The model uses aspect ratingsto discover the corresponding topics and can thus ex-tract fragments of text discussing these aspects with-out the need of annotated data.
We demonstratedthat the model indeed discovers corresponding co-herent topics and achieves accuracy in sentence la-beling comparable to a standard supervised model.The primary area of future work is to incorporate themodel into an end-to-end sentiment summarizationsystem in order to evaluate it at that level.AcknowledgmentsThis work benefited from discussions with SashaBlair-Goldensohn and Fernando Pereira.315ReferencesDavid M. Blei and Jon D. McAuliffe.
2008.
Supervisedtopic models.
In Advances in Neural Information Pro-cessing Systems (NIPS).D.M.
Blei, A.Y.
Ng, and M.I.
Jordan.
2003.
LatentDirichlet alocation.
Journal of Machine Learning Re-search, 3(5):993?1022.S.R.K.
Branavan, H. Chen, J. Eisenstein, and R. Barzi-lay.
2008.
Learning document-level semantic proper-ties from free-text annotations.
In Proceedings of theAnnual Conference of the Association for Computa-tional Linguistics.G.
Carenini, R. Ng, and A. Pauls.
2006.
Multi-DocumentSummarization of Evaluative Text.
In Proceedings ofthe Conference of the European Chapter of the Asso-ciation for Computational Linguistics.M.
Gamon, A. Aue, S. Corston-Oliver, and E. Ringger.2005.
Pulse: Mining customer opinions from free text.In Proc.
of the 6th International Symposium on Intelli-gent Data Analysis, pages 121?132.S.
Geman and D. Geman.
1984.
Stochastic relaxation,Gibbs distributions, and the Bayesian restoration ofimages.
IEEE Transactions on Pattern Analysis andMachine Intelligence, 6:721?741.T.
L. Griffiths and M. Steyvers.
2004.
Finding scien-tific topics.
Proceedings of the Natural Academy ofSciences, 101 Suppl 1:5228?5235.T.
L. Griffiths, M. Steyvers, D. M. Blei, and J.
B. Tenen-baum.
2004.
Integrating topics and syntax.
In Ad-vances in Neural Information Processing Systems.A.
Gruber, Y. Weiss, and M. Rosen-Zvi.
2007.
HiddenTopic Markov Models.
In Proceedings of the Confer-ence on Artificial Intelligence and Statistics.M.
Hu and B. Liu.
2004a.
Mining and summarizingcustomer reviews.
In Proceedings of the 2004 ACMSIGKDD international conference on Knowledge dis-covery and data mining, pages 168?177.
ACM PressNew York, NY, USA.M.
Hu and B. Liu.
2004b.
Mining Opinion Featuresin Customer Reviews.
In Proceedings of NineteenthNational Conference on Artificial Intellgience.C.
Manning and M. Schutze.
1999.
Foundations of Sta-tistical Natural Language Processing.
MIT Press.Q.
Mei, X. Ling, M.Wondra, H. Su, and C.X.
Zhai.
2007.Topic sentiment mixture: modeling facets and opin-ions in weblogs.
In Proceedings of the 16th Interna-tional Conference on World Wide Web, pages 171?180.Radford Neal.
1992.
Connectionist learning of beliefnetworks.
Artificial Intelligence, 56:71?113.B.
Pang, L. Lee, and S. Vaithyanathan.
2002.
Thumbsup?
Sentiment classification using machine learningtechniques.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing.A.M.
Popescu and O. Etzioni.
2005.
Extracting productfeatures and opinions from reviews.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing (EMNLP).B.
Snyder and R. Barzilay.
2007.
Multiple Aspect Rank-ing using the Good Grief Algorithm.
In Proceedingsof the Joint Conference of the North American Chapterof the Association for Computational Linguistics andHuman Language Technologies, pages 300?307.I.
Titov and R. McDonald.
2008.
Modeling online re-views with multi-grain topic models.
In Proceedingsof the 17h International Conference on World WideWeb.P.
Turney.
2002.
Thumbs up or thumbs down?
Senti-ment orientation applied to unsupervised classificationof reviews.
In Proceedings of the Annual Conferenceof the Association for Computational Linguistics.Hanna M. Wallach.
2006.
Topic modeling; beyond bagof words.
In International Conference on MachineLearning.Xuerui Wang and Andrew McCallum.
2005.
A note ontopical n-grams.
Technical Report UM-CS-2005-071,University of Massachusetts.J.
Wiebe.
2000.
Learning subjective adjectives from cor-pora.
In Proceedings of the National Conference onArtificial Intelligence.L.
Zhuang, F. Jing, and X.Y.
Zhu.
2006.
Movie re-view mining and summarization.
In Proceedings ofthe 15th ACM international conference on Informationand knowledge management (CIKM), pages 43?50.316
