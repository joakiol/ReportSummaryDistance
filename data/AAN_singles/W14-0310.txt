Workshop on Humans and Computer-assisted Translation, pages 66?71,Gothenburg, Sweden, 26 April 2014.c?2014 Association for Computational LinguisticsThe ACCEPT Portal: An Online Framework for the Pre-editing andPost-editing of User-Generated ContentVioleta SeretanFTI/TIMUniversity of GenevaSwitzerlandVioleta.Seretan@unige.chJohann RoturierSymantec Ltd.Dublin, Irelandjohann roturier@symantec.comDavid SilvaSymantec Ltd.Dublin, IrelandDavid Silva@symantec.comPierrette BouillonFTI/TIMUniversity of GenevaSwitzerlandPierrette.Bouillon@unige.chAbstractWith the development of Web 2.0, alot of content is nowadays generated on-line by users.
Due to its characteristics(e.g., use of jargon and abbreviations, ty-pos, grammatical and style errors), theuser-generated content poses specific chal-lenges to machine translation.
This pa-per presents an online platform devoted tothe pre-editing of user-generated contentand its post-editing, two main types of hu-man assistance strategies which are com-bined with domain adaptation and othertechniques in order to improve the trans-lation of this type of content.
The plat-form has recently been released publiclyand is being tested by two main types ofuser communities, namely, technical fo-rum users and volunteer translators.1 IntroductionUser-generated content ?
i.e., information postedby Internet users in social communication chan-nels like blogs, forum posts, social networks ?
isone of the main sources of information availabletoday.
Huge volumes of such content are createdeach day, reach a very broad audience instantly.1The democratisation of content creation dueto the emergence of the Web 2.0 paradigm alsomeans a diversification of the languages used onthe Internet.2Despite its availability, the new con-tent is only accessible to the speakers of the lan-guage in which it was created.
The automatictranslation of user-generated content is thereforeone of the key issues to be addressed in the field ofhuman language technologies.
However, as stated1For instance, 58 million tweets are sent on aver-age per day (http://www.statisticbrain.com/twitter-statistics/).2See http://en.wikipedia.org/wiki/Languages_used_on_the_Internet for statistics.by Jiang et al.
(2012), despite the obvious bene-fits, there are relatively little attempts at translatinguser-generated content.The reason may lie in the fact that user-ge-nerated content is very challenging for machinetranslation.
As shown, among others, by Nagara-jan and Gamon (2011), there are several charac-teristics of this content that pose new process-ing challenges with respect to traditional content:informal style, slang, abbreviations, specific ter-minology, irregular grammar and spelling.
In-deed, Internet users are rarely professional writ-ers.3They often write in a language which is nottheir own, and sacrifice quality for speed, not pay-ing attention to spelling, punctuation, or grammarrules.The ACCEPT project4addresses these chal-lenges by developing a technology integratingmodules for automatic and manual content pre-editing, statistical machine translation, as wellas output evaluation and post-editing.
Thus, theproject aims to improve the translation of user-ge-nerated content by proposing a full workflow, inwhich the participation of humans is essential.The application scenario considered in theproject are user communities sharing specific in-formation on a given topic.
The project focuses,more specifically, on the following use cases:1. the commercial use case, in which the tar-get community is the user community builtaround a software company in order formembers to help each other with issues re-lated to products;2. the NGO use case, in which non-go-vernmental organisations such as DoctorsWithout Borders produce health-care contentfor distributions in areas of need.3Even when they are, as in the case of government agen-cies, the type of content produced (e.g., tweets) still poses?multiple challenges?
to translation (Gotti et al., 2013).4http://www.accept-project.eu/66The language pairs considered in the project areEnglish to French, German and Japanese, as wellas French into English for the first use case (in-volving technical forum information), and Frenchto and from English for the second use case (in-volving healthcare information).Past halfway into its research program, theproject has accomplished significant progress inthe main areas mentioned above (pre-editing, sta-tistical machine translation, post-editing, and eval-uation).
The ACCEPT technology has recentlybeen released to the broad public as an on-line framework, which demonstrates the differentmodules of the workflow and provides access toassociated software components (plug-ins, APIs),as well as to documentation.
The pre-editing tech-nology has been deployed on the targeted user fo-rum5, allowing users to check their messages be-fore posting them.
The post-editing technology isbeing used by a community of translators, whichprovide pro-bono translation services to the NGOsconsidered in our second use case.In this paper, we describe the framework by pre-senting its architecture and main modules (Sec-tion 2).
We discuss related work in Section 3 andconclude in Section 4.2 The FrameworkThe ACCEPT technology has been made acces-sible to a broad audience in the form of an on-line framework, i.e., an integrated environmentwhere registered users can perform pre-editing,post-editing and evaluation work.
The framework?
henceforth, the ACCEPT Portal ?
is hosted on acloud computing infrastructure and is available atwww.accept-portal.eu.2.1 Architecture of the FrameworkAs explained in Section 1, the ACCEPT techno-logy consists of the following main modules:1.
Pre-editing module;2.
Machine translation module,3.
Post-editing module,4.
Evaluation module.The typical workflow is incremental, but themodules are independent.
They can be used bothwithin and outside the portal, as they are built on aREST API facilitating integration.5https://community.norton.com/In the remaining of this section, we introduceeach of the framework modules.62.2 Pre-editing ModuleThe pre-editing module leverages existing ling-ware which provides authoring support rulesaimed at language professionals, by relying onshallow language processing (Bredenkamp et al.,2000).
The existing English checker and the lin-guistic resources on which it relies have been ex-tended and adapted to suit the type of data gener-ated by community users.
In particular, the soft-ware extension consisted of designing a numberof pre-editing rules aimed at source normalisation,for the purpose of making the input text easierto handle by the SMT systems.
In the case ofFrench, the pre-editing rules have been designedfrom scratch.
The pre-editing rules pertain to thelevels of spelling, grammar, style and terminology.They are defined using the original lingware?s ruleformalism and are incorporated into a server dedi-cated to the project.The rule development was corpus-driven andwas performed on data collected for this purpose.A stable set of pre-edition rules is available inthe portal for each of the domains and sourcelanguages considered (i.e., technical forum andheathcare data in English and French).
The rulesare described in detail in the project deliverableD 2.2 (2013).The rules proposed have been evaluated individ-ually and in combination (Roturier et al., 2012;Gerlach et al., 2013; Seretan et al., 2014).
Asa general observation, it is important to noticethat, for SMT, the improvement of the input textdoes not go hand in hand with the improvement oftranslation.
For example, in French the rule forcorrecting verbal forms to the subjunctive tensehad a negative impact since the subjunctive is notfrequent in the training data.
Conversely, it waspossible to define lexical reformulations which de-graded the quality of the input text, but had a po-sitive impact on translation quality.The combined impact of the rule applica-tion was measured in a variety of settings in alarge-scale evaluation campaign involving transla-tion students (Seretan et al., 2014).
As the rulesare divided into two major groups, those automati-cally applicable and those requiring human inter-6The MT module will be omitted, as it is not part of theportal.
The interested reader is referred to D 4.2 (2013).67Figure 1: The ACCEPT Pre-edit plug-in in action (screen capture)vention, the evaluation was carried out for the fullset of rules, as well as for the automatic rules only.In addition, the evaluation was performed in botha monolingual and a bilingual setting, i.e., withthe evaluators having or not access to the sourcetext, and it involved evaluation scales of differentgranularities.
The evaluation results showed a sys-tematic statistically significant improvement overthe baseline when pre-editing is performed on thesource content.
More details about the evalua-tion methodology and results can be found in theproject deliverable D 9.2.2 (2013).A data excerpt illustrating the impact of pre-editing on translation quality is presented in Ex-ample 1 below.
The simple correction of an ac-cented letter, du?
d?u, leads to the change of seve-ral target words, and to a much better translation ofthe input sentence.1.
a) Source (original):J?ai du m?absenter hier apr`es midi.b) Source (pre-edited):J?ai d?u m?absenter hier apr`es midi.c) Target (original):I have the leave me yesterday afternoon.d) Target (pre-edited):I had to leave yesterday afternoon.The pre-editing component of the ACCEPTtechnology is available as a JQuery plug-in, whichcan be downloaded and installed by Web applica-tion owners, so that it can be used with text areasand other text-bearing elements.
APIs and ac-companying documentation have also been madeavailable, so that the pre-editing rules can beleveraged in automatic steps, without the plug-in,across devices and platforms.
A demo site illus-trating the use of the plug-in in a TinyMCE envi-ronment is available on the portal (see Figure 1).The latest developments of the pre-editing mo-dule include the possibility for users to customisethe application of rule sets, in particular, to ignorespecific rules and to manage their own dictionary,in order to prevent the activation of checking flags.2.3 Post-editing ModuleThe post-editing module of the framework (seealso Roturier et al., (2013)) is designed to fulfilthe project?s objective of collecting post-editingdata in order to learn correction rules and, throughfeedback loops, to integrate them into the SMTengines (with the goal of automating correctionswhenever possible).
The project relies on the par-ticipation of volunteer community members, whoare subject matter experts, native speakers of the68Figure 2: The ACCEPT Portal showing the post-editing demo (screen capture)target language and, possibly, of the source lan-guage.
Accordingly, the post-editing environment(see Figure 2) provides functionalities for bothmonolingual and bilingual post-editing.The post-editing text is organised in tasks be-longing to post-editing projects.
The latter arecreated and managed by project administrators,by defining the project settings (e.g., source andtarget languages, monolingual or bilingual mode,collaborative or non-collaborative type7), upload-ing the text for each task8, inviting participants bye-mail, and monitoring revision progress.The post-editors edit the target text in asentence-by-sentence fashion.
They have accessto the task guidelines and to help documentation.The interface of the post-editing window displaysthe whole text, through which they can navigatewith next-previous buttons or by clicking on aspecific sentence.
Users can check the text theyare editing by accessing, with a button, the con-tent checking technology described in Section 2.2.Their actions ?
in terms of keystrokes and usage7In a collaborative editing scenario, users may see editsfrom other users and do not have to repeat them when work-ing on the same project task.
Conflicts are avoided by pre-venting concurrent access.8Currently, the JSON format is used for the input data.of translation options ?
and time spent editing arerecorded in the portal.9When they are done edi-ting, they can click on a button marking the com-pletion of the task.
At any time, they can interrupttheir work and save their results for later.Users can enter a comment on the post-editingtask they have performed.
The feedback elicitedfrom users include the difficulty of the task andtheir sentiment (Was it easy to post-edit?
Did youenjoy the post-editing task?).
For systematicallycollecting user feedback, the project administra-tors can specify on the project configuration pagea link to a post-task survey, which will be sent tousers after completing their tasks.The post-editing module includes a JQueryplug-in for deployment in any Web-based envi-ronment; a dedicated section of the portal; APIsenabling the use of the post-editing functionalityoutside the portal; and sample evaluation projectsfor several language pairs.The post-editing technology has been exten-sively used in specific post-editing campaigns in-volving translator volunteers and Amazon Me-chanical Turk10workers.
The campaigns, includ-9The post-editing data is exported in XLIFF format.10The integration was done via the ACCEPT API.69ing reports on post-task surveys, are documentedinter alia in deliverable D 8.1.2 (2013).
A notablefinding was that professional translators, who werereticent towards MT before the task, had a morepositive sentiment after post-editing and their mo-tivation to post-edit in the future increased.2.4 Evaluation ModuleThe role of the evaluation module is to support thecollection of user ratings for assessing the qualityof source, machine-translated and post-edited con-tent, and, ultimately, to support the developmentof the technology created in the project.This module groups several software compo-nents: an evaluation environment available as asection of the portal; APIs enabling the collectionof user evaluations in-context; and a third com-ponent which is a customisation of the Appraisetoolkit for the collaborative collection of humanjudgements (Federmann, 2012).As in the case of post-editing module, this mod-ule provides functionality for creating and man-aging projects.
Using the evaluation environ-ment/APIs, project creators can define questioncategories, add questions and possible answers,and upload evaluation data (in JSON format).
Fortraditional evaluation projects, the Appraise sys-tem is used instead.3 Related WorkTransforming the source text in order to betterfit the needs of machine translation is a well-investigated area of research.
Strategies likesource control, source re-ordering, or source sim-plification at the lexical or structural level havebeen largely explored; for reviews, see, for in-stance, Huhn (2013), Kazemi (2013), and Feng(2008), respectively.User-generated content has been investigatedin the context of machine translation in recentwork dealing specifically with spelling correc-tion (Bertoldi et al., 2010; Formiga and Fonol-losa, 2012); lexical normalisation by substitutingill-formed words with their correct counterpart,e.g., makn ?
making (Han and Baldwin, 2011);missing word ?
e.g., zero-pronoun ?
recovery andpunctuation correction (Wang and Ng, 2013).Rather than focusing on specific phenomena orWeb genres (i.e., tweets), we adopt a more gen-eral approach in which we address the problem ofsource normalisation at multiple levels ?
punctua-tion, spelling, grammar, and style ?
for any type oflinguistically imperfect text.Another peculiarity of our approach is that itis rule-based and does not require parallel datafor learning corrections.
In exchange, a limi-tation of our pre-editing approach is that it islanguage-dependent, as the underlying technologyis based on shallow analysis and is therefore time-expensive to extend to a new language.The post-editing technology differs from exist-ing (standalone or Web-based) dedicated tools ?e.g., iOmegaT11or MateCat12?
in that it is tai-lored to community users, and, consequently, itis lighter, it generates more concise reports, anda simpler interface replaces the grid-like formatfor presenting data.
Another specificity is that itis sufficiently flexible to be used in other environ-ments (e.g., Amazon Mechanical Turk, cf.
?2.3).4 ConclusionThe technology outlined in this paper demon-strates a specific case of human-computer interac-tion, in which, for the first time, several modulesare integrated in a full process in which humanpre-editors, post-editors and evaluators play a keyrole for improving the translation of communitycontent.
The technology is freely accessible in theonline portal, has been deployed on a major userforum, and can be downloaded for integration inother Web-based environments.
Since it is built ontop of a REST API, it is portable across devicesand platforms.
The technology would be useful toanyone who needs information instantly and relia-bly translated, despite linguistic imperfections.One of the main future developments concernsthe further improvement of SMT, by exploring,in particular, the use of text analytics and senti-ment detection.
In addition, by incorporating post-editing rules and developing techniques to changethe phrase table and system parameters dynam-ically, it will be possible to reduce the amountof error corrections that human post-editors haveto perform repeatedly.
Another major develop-ment (joint work with the CASMACAT Europeanproject) will focus on novel types of assistance fortranslators, aimed specifically at helping transla-tors by identifying problematic parts of the ma-chine translation output and signalling the para-phrases that are more likely to be useful.11http://try-and-see-mt.org/12http://www.matecat.com/70AcknowledgmentsThe research leading to these results has receivedfunding from the European Community?s SeventhFramework Programme (FP7/2007-2013) undergrant agreement no288769.ReferencesNicola Bertoldi, Mauro Cettolo, and Marcello Fe-derico.
2010.
Statistical machine translation of textswith misspelled words.
In Human Language Tech-nologies: The 2010 Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics, pages 412?419, Los Angeles,California.Andrew Bredenkamp, Berthold Crysmann, and MirelaPetrea.
2000.
Looking for errors: A declarative for-malism for resource-adaptive language checking.
InProceedings of the Second International Conferenceon Language Resources and Evaluation, Athens,Greece.2013.
ACCEPT deliverable D 2.2 Definition ofpre-editing rules for English and French (finalversion).
http://www.accept.unige.ch/Products/D2_2_Definition_of_Pre-Editing_Rules_for_English_and_French_with_appendixes.pdf.2013.
ACCEPT deliverable D 9.2.2: Survey ofevaluation results.
http://www.accept.unige.ch/Products/D_9_2_Survey_of_evaluation_results.pdf.2013.
ACCEPT deliverable D 4.2 Report onrobust machine translation: domain adap-tation and linguistic back-off.
http://www.accept.unige.ch/Products/D_4_2_Report_on_robust_machine_translation_domain_adaptation_and_linguistic_back-off.pdf.2013.
ACCEPT deliverable D 8.1.2 Data andreport from user studies - Year 2. http://www.accept.unige.ch/Products/D_8_1_2_Data_and_report_from_user_studies_-_Year_2.pdf.Christian Federmann.
2012.
Appraise: An open-source toolkit for manual evaluation of machinetranslation output.
The Prague Bulletin of Mathe-matical Linguistics (PBML), 98:25?35.Lijun Feng.
2008.
Text simplification: A survey.Technical report, CUNY.Llu?
?s Formiga and Jos?e A. R. Fonollosa.
2012.
Dea-ling with input noise in statistical machine transla-tion.
In Proceedings of COLING 2012: Posters,pages 319?328, Mumbai, India.Johanna Gerlach, Victoria Porro, Pierrette Bouillon,and Sabine Lehmann.
2013.
La pr?e?edition avecdes r`egles peu co?uteuses, utile pour la TA statistiquedes forums ?
In Actes de la 20e conf?erence surle Traitement Automatique des Langues Naturelles(TALN?2013), pages 539?546, Les Sables d?Olonne,France.Fabrizio Gotti, Philippe Langlais, and Atefeh Farzin-dar.
2013.
Translating government agencies?
tweetfeeds: Specificities, problems and (a few) solutions.In Proceedings of the Workshop on Language Anal-ysis in Social Media, pages 80?89, Atlanta, Georgia.Bo Han and Timothy Baldwin.
2011.
Lexical normali-sation of short text messages: Makn sens a #twit-ter.
In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics: Hu-man Language Technologies, pages 368?378, Port-land, Oregon.Jie Jiang, Andy Way, and Rejwanul Haque.
2012.Translating user-generated content in the social net-working space.
In Proceedings of the Tenth BiennialConference of the Association for Machine Transla-tion in the Americas (AMTA-2012), San Diego, Cali-fornia.Arefeh Kazemi, Amirhassan Monadjemi, and Moham-madali Nematbakhsh.
2013.
A quick review on re-ordering approaches in statistical machine transla-tion systems.
IJCER, 2(4).Tobias Kuhn.
2013.
A survey and classification of con-trolled natural languages.
Computational Linguis-tics.Meenakshi Nagarajan and Michael Gamon, editors.2011.
Proceedings of the Workshop on Languagein Social Media (LSM 2011).
Portland, Oregon.Johann Roturier, Linda Mitchell, Robert Grabowski,and Melanie Siegel.
2012.
Using automatic ma-chine translation metrics to analyze the impact ofsource reformulations.
In Proceedings of the Con-ference of the Association for Machine Translationin the Americas (AMTA), San Diego, California.Johann Roturier, Linda Mitchell, and David Silva.2013.
The ACCEPT post-editing environment: aflexible and customisable online tool to perform andanalyse machine translation post-editing.
In Pro-ceedings of MT Summit XIV Workshop on Post-edi-ting Technology and Practice, Nice, France.Violeta Seretan, Pierrette Bouillon, and Johanna Ger-lach.
2014.
A large-scale evaluation of pre-edi-ting strategies for improving user-generated contenttranslation.
In Proceedings of the 9th Edition of theLanguage Resources and Evaluation Conference,Reykjavik, Iceland.Pidong Wang and Hwee Tou Ng.
2013.
A beam-search decoder for normalization of social mediatext with application to machine translation.
In Pro-ceedings of the 2013 Conference of the North Ameri-can Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pages471?481, Atlanta, Georgia.71
