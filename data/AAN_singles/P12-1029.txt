Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 273?282,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsWord Sense Disambiguation Improves Information RetrievalZhi Zhong and Hwee Tou NgDepartment of Computer ScienceNational University of Singapore13 Computing Drive, Singapore 117417{zhongzhi, nght}@comp.nus.edu.sgAbstractPrevious research has conflicting conclu-sions on whether word sense disambiguation(WSD) systems can improve information re-trieval (IR) performance.
In this paper, wepropose a method to estimate sense distribu-tions for short queries.
Together with thesenses predicted for words in documents, wepropose a novel approach to incorporate wordsenses into the language modeling approachto IR and also exploit the integration of syn-onym relations.
Our experimental results onstandard TREC collections show that using theword senses tagged by a supervised WSD sys-tem, we obtain significant improvements overa state-of-the-art IR system.1 IntroductionWord sense disambiguation (WSD) is the task ofidentifying the correct meaning of a word in context.As a basic semantic understanding task at the lexi-cal level, WSD is a fundamental problem in naturallanguage processing.
It can be potentially used asa component in many applications, such as machinetranslation (MT) and information retrieval (IR).In recent years, driven by Senseval/Semevalworkshops, WSD systems achieve promising perfor-mance.
In the application of WSD to MT, researchhas shown that integrating WSD in appropriate wayssignificantly improves the performance of MT sys-tems (Chan et al, 2007; Carpuat and Wu, 2007).In the application to IR, WSD can bring two kindsof benefits.
First, queries may contain ambiguouswords (terms), which have multiple meanings.
Theambiguities of these query words can hurt retrievalprecision.
Identifying the correct meaning of theambiguous words in both queries and documentscan help improve retrieval precision.
Second, querywords may have tightly related meanings with otherwords not in the query.
Making use of these relationsbetween words can improve retrieval recall.Overall, IR systems can potentially benefit fromthe correct meanings of words provided by WSDsystems.
However, in previous investigations of theusage of WSD in IR, different researchers arrivedat conflicting observations and conclusions.
Someof the early research showed a drop in retrieval per-formance by using word senses (Krovetz and Croft,1992; Voorhees, 1993).
Some other experiments ob-served improvements by integrating word senses inIR systems (Schu?tze and Pedersen, 1995; Gonzaloet al, 1998; Stokoe et al, 2003; Kim et al, 2004).This paper proposes the use of word senses toimprove the performance of IR.
We propose an ap-proach to annotate the senses for short queries.
Weincorporate word senses into the language modeling(LM) approach to IR (Ponte and Croft, 1998), andutilize sense synonym relations to further improvethe performance.
Our evaluation on standard TREC1data sets shows that supervised WSD outperformstwo other WSD baselines and significantly improvesIR.The rest of this paper is organized as follows.
InSection 2, we first review previous work using WSDin IR.
Section 3 introduces the LM approach to IR,including the pseudo relevance feedback method.We describe our WSD system and the method of1http://trec.nist.gov/273generating word senses for query terms in Section4, followed by presenting our novel method of in-corporating word senses and their synonyms into theLM approach in Section 5.
We present experimentsand analyze the results in Section 6.
Finally, we con-clude in Section 7.2 Related WorkMany previous studies have analyzed the benefitsand the problems of applying WSD to IR.
Krovetzand Croft (1992) studied the sense matches betweenterms in query and the document collection.
Theyconcluded that the benefits of WSD in IR are not asexpected because query words have skewed sensedistribution and the collocation effect from otherquery terms already performs some disambiguation.Sanderson (1994; 2000) used pseudowords to intro-duce artificial word ambiguity in order to study theimpact of sense ambiguity on IR.
He concluded thatbecause the effectiveness of WSD can be negatedby inaccurate WSD performance, high accuracy ofWSD is an essential requirement to achieve im-provement.
In another work, Gonzalo et al (1998)used a manually sense annotated corpus, SemCor, tostudy the effects of incorrect disambiguation.
Theyobtained significant improvements by representingdocuments and queries with accurate senses as wellas synsets (synonym sets).
Their experiment alsoshowed that with the synset representation, whichincluded synonym information, WSD with an errorrate of 40%?50% can still improve IR performance.Their later work (Gonzalo et al, 1999) verified thatpart of speech (POS) information is discriminatoryfor IR purposes.Several works attempted to disambiguate termsin both queries and documents with the senses pre-defined in hand-crafted sense inventories, and thenused the senses to perform indexing and retrieval.Voorhees (1993) used the hyponymy (?IS-A?)
rela-tion in WordNet (Miller, 1990) to disambiguate thepolysemous nouns in a text.
In her experiments, theperformance of sense-based retrieval is worse thanstem-based retrieval on all test collections.
Her anal-ysis showed that inaccurate WSD caused the poorresults.Stokoe et al (2003) employed a fine-grainedWSD system with an accuracy of 62.1% to dis-ambiguate terms in both the text collections andthe queries in their experiments.
Their evalua-tion on TREC collections achieved significant im-provements over a standard term based vector spacemodel.
However, it is hard to judge the effectof word senses because of the overall poor perfor-mances of their baseline method and their system.Instead of using fine-grained sense inventory, Kimet al (2004) tagged words with 25 root senses ofnouns in WordNet.
Their retrieval method main-tained the stem-based index and adjusted the termweight in a document according to its sense match-ing result with the query.
They attributed the im-provement achieved on TREC collections to theircoarse-grained, consistent, and flexible sense tag-ging method.
The integration of senses into the tra-ditional stem-based index overcomes some of thenegative impact of disambiguation errors.Different from using predefined sense inventories,Schu?tze and Pedersen (1995) induced the sense in-ventory directly from the text retrieval collection.For each word, its occurrences were clustered intosenses based on the similarities of their contexts.Their experiments showed that using senses im-proved retrieval performance, and the combinationof word-based ranking and sense-based ranking canfurther improve performance.
However, the cluster-ing process of each word is a time consuming task.Because the sense inventory is collection dependent,it is also hard to expand the text collection withoutre-doing preprocessing.Many studies investigated the expansion effectsby using knowledge sources from thesauri.
Someresearchers achieved improvements by expandingthe disambiguated query words with synonyms andsome other information from WordNet (Voorhees,1994; Liu et al, 2004; Liu et al, 2005; Fang, 2008).The usage of knowledge sources from WordNet indocument expansion also showed improvements inIR systems (Cao et al, 2005; Agirre et al, 2010).The previous work shows that the WSD errors caneasily neutralize its positive effect.
It is importantto reduce the negative impact of erroneous disam-biguation, and the integration of senses into tradi-tional term index, such as stem-based index, is a pos-sible solution.
The utilization of semantic relationshas proved to be helpful for IR.
It is also interest-274ing to investigate the utilization of semantic relationsamong senses in IR.3 The Language Modeling Approach to IRThis section describes the LM approach to IR andthe pseudo relevance feedback approach.3.1 The language modeling approachIn the language modeling approach to IR, languagemodels are constructed for each query q and eachdocument d in a text collection C. The documentsin C are ranked by the distance to a given query qaccording to the language models.
The most com-monly used language model in IR is the unigrammodel, in which terms are assumed to be indepen-dent of each other.
In the rest of this paper, languagemodel will refer to the unigram language model.One of the commonly used measures of the sim-ilarity between query model and document modelis negative Kullback-Leibler (KL) divergence (Laf-ferty and Zhai, 2001).
With unigram model, the neg-ative KL-divergence between model ?q of query qand model ?d of document d is calculated as follows:?D(?q||?d)=?
?t?Vp(t|?q) logp(t|?q)p(t|?d)=?t?Vp(t|?q) log p(t|?d)?
?t?Vp(t|?q) log p(t|?q)=?t?Vp(t|?q) log p(t|?d) + E(?q), (1)where p(t|?q) and p(t|?d) are the generative proba-bilities of a term t from the models ?q and ?d, V isthe vocabulary of C, and E(?q) is the entropy of q.Define tf (t, d) and tf (t, q) as the frequencies of tin d and q, respectively.
Normally, p(t|?q) is calcu-lated with maximum likelihood estimation (MLE):p(t|?q) =tf (t,q)Pt?
?q tf (t?,q) .
(2)In the calculation of p(t|?d), several smoothingmethods have been proposed to overcome the datasparseness problem of a language model constructedfrom one document (Zhai and Lafferty, 2001b).
Forexample, p(t|?d) with the Dirichlet-prior smoothingcan be calculated as follows:p(t|?d) =tf (t, d) + ?
p(t|?C)?t?
?V tf (t?, d) + ?, (3)where ?
is the prior parameter in the Dirichlet-priorsmoothing method, and p(t|?C) is the probability oft in C, which is often calculated with MLE:p(t|?C) =Pd?
?C tf (t,d?)Pd??CPt?
?V tf (t?,d?)
.3.2 Pseudo relevance feedbackPseudo relevance feedback (PRF) is widely used inIR to achieve better performance.
It is constructedwith two retrieval steps.
In the first step, ranked doc-uments are retrieved from C by a normal retrievalmethod with the original query q.
In the second step,a number of terms are selected from the top k rankeddocuments Dq for query expansion, under the as-sumption that these k documents are relevant to thequery.
Then, the expanded query is used to retrievethe documents from C.There are several methods to select expansionterms in the second step (Zhai and Lafferty, 2001a).For example, in Indri2, the terms are first ranked bythe following score:v(t,Dq) =?d?Dq log(tf (t,d)|d| ?1p(t|?C)),as in Ponte (1998).
Define p(q|?d) as the probabilityscore assigned to d. The topm terms Tq are selectedwith weights calculated based on the relevancemodel described in Lavrenko and Croft (2001):w(t,Dq) =?d?Dq[tf (t,d)|d| ?
p(q|?d)?
p(?d)],which calculates the sum of weighted probabilitiesof t in each document.
After normalization, theprobability of t in ?rq is calculated as follows:p(t|?rq) =w(t,Dq)Pt?
?Tqw(t?,Dq).Finally, the relevance model is interpolated with theoriginal query model:p(t|?prfq ) = ?
p(t|?rq) + (1?
?
)p(t|?q), (4)where parameter ?
controls the amount of feedback.The new model ?prfq is used to replace the originalone ?q in Equation 1.Collection enrichment (CE) (Kwok and Chan,1998) is a technique to improve the quality of thefeedback documents by making use of an externaltarget text collection X in addition to the originaltarget C in the first step of PRF.
The usage of X issupposed to provide more relevant feedback docu-ments and feedback query terms.2http://lemurproject.org/indri/2754 Word Sense DisambiguationIn this section, we first describe the construction ofour WSD system.
Then, we propose the method ofassigning senses to query terms.4.1 Word sense disambiguation systemPrevious research shows that translations in anotherlanguage can be used to disambiguate the meaningsof words (Chan and Ng, 2005; Zhong and Ng, 2009).We construct our supervised WSD system directlyfrom parallel corpora.To generate the WSD training data, 7 parallel cor-pora were used, including Chinese Treebank, FBISCorpus, Hong Kong Hansards, Hong Kong Laws,Hong Kong News, Sinorama News Magazine, andXinhua Newswire.
These corpora were alreadyaligned at sentence level.
We tokenized Englishtexts with Penn Treebank Tokenizer, and performedword segmentation on Chinese texts.
Then, wordalignment was performed on the parallel corporawith the GIZA++ software (Och and Ney, 2003).For each English morphological root e, the En-glish sentences containing its occurrences were ex-tracted from the word aligned output of GIZA++,as well as the corresponding translations of theseoccurrences.
To minimize noisy word alignmentresult, translations with no Chinese character weredeleted, and we further removed a translation whenit only appears once, or its frequency is less than 10and also less than 1% of the frequency of e. Finally,only the most frequent 10 translations were kept forefficiency consideration.The English part of the remaining occurrenceswere used as training data.
Because multiple En-glish words may have the same Chinese transla-tion, to differentiate them, each Chinese translationis concatenated with the English morphological rootto form a word sense.
We employed a supervisedWSD system, IMS3, to train the WSD models.
IMS(Zhong and Ng, 2010) integrates multiple knowl-edge sources as features.
We used MaxEnt as themachine learning algorithm.
Finally, the system candisambiguate the words by assigning probabilities todifferent senses.3http://nlp.comp.nus.edu.sg/software/ims4.2 Estimating sense distributions for querytermsIn IR, both terms in queries and the text collectioncan be ambiguous.
Hence, WSD is needed to disam-biguate these ambiguous terms.
In most cases, doc-uments in a text collection are full articles.
There-fore, a WSD system has sufficient context to dis-ambiguate the words in the document.
In contrast,queries are usually short, often with only two orthree terms in a query.
Short queries pose a chal-lenge to WSD systems since there is insufficientcontext to disambiguate a term in a short query.One possible solution to this problem is to findsome text fragments that contain a query term.
Sup-pose we already have a basic IR method which doesnot require any sense information, such as the stem-based LM approach.
Similar to the PRF method,assuming that the top k documents retrieved by thebasic method are relevant to the query, these k docu-ments can be used to represent query q (Broder et al,2007; Bendersky et al, 2010; He and Wu, 2011).
Wepropose a method to estimate the sense probabilitiesof each query term of q from these top k retrieveddocuments.Suppose the words in all documents of the textcollection are disambiguated with a WSD system,and each word occurrence w in document d is as-signed a vector of senses, S(w).
Define the proba-bility of assigning sense s to w as p(w, s, d).
Givena query q, suppose Dq is the set of top k documentsretrieved by the basic method, with the probabilityscore p(q|?d) assigned to d ?
Dq.Given a query term t ?
qS(t, q) = {}sum = 0for each document d ?
Dqfor each word occurrence w ?
d, whose stem form isidentical to the stem form of tfor each sense s ?
S(w)S(t, q) = S(t, q) ?
{s}p(t, s, q) = p(t, s, q) + p(q|?d) p(w, s, d)sum = sum + p(q|?d) p(w, s, d)for each sense s ?
S(t, q)p(t, s, q) = p(t, s, q)/sumReturn S(t, q), with probability p(t, s, q) for s ?
S(t, q)Figure 1: Process of generating senses for query termsFigure 1 shows the pseudocode of calculating the276sense distribution for a query term t in q with Dq,where S(t, q) is the set of senses assigned to t andp(t, s, q) is the probability of tagging t as sense s.Basically, we utilized the sense distribution of thewords with the same stem form in Dq as a proxy toestimate the sense probabilities of a query term.
Theretrieval scores are used to weight the informationfrom the corresponding retrieved documents in Dq.5 Incorporating Senses into LanguageModeling ApproachesIn this section, we propose to incorporate senses intothe LM approach to IR.
Then, we describe the inte-gration of sense synonym relations into our model.5.1 Incorporating senses as smoothingWith the method described in Section 4.2, both theterms in queries and documents have been sensetagged.
The next problem is to incorporate the senseinformation into the language modeling approach.Suppose p(t, s, q) is the probability of tagging aquery term t ?
q as sense s, and p(w, s, d) is theprobability of tagging a word occurrence w ?
d assense s. Given a query q and a document d in textcollection C, we want to re-estimate the languagemodels by making use of the sense information as-signed to them.Define the frequency of s in d as:stf (s, d) =?w?d p(w, s, d),and the frequency of s in C as:stf (s, C) =?d?C stf (s, d).Define the frequencies of sense set S in d and C as:stf (S, d) =?s?S stf (s, d),stf (S,C) =?s?S stf (s, C).For a term t ?
q, with senses S(t, q):{s1, ..., sn},suppose V :{p(t, s1, q), ..., p(t, sn, q)} is the vectorof probabilities assigned to the senses of t andW :{stf (s1, d), ..., stf (sn, d)} is the vector of fre-quencies of S(t, q) in d. The function cos(t, q, d)calculates the cosine similarity between vector Vand vector W .
Assume D is a set of documentsin C which contain any sense in S(t, q), we definefunction cos(t, q) =?d?D cos(t, q, d)/|D|, whichcalculates the mean of the sense cosine similarities,and define function ?cos(t, q, d) = cos(t, q, d) ?cos(t, q), which calculates the difference betweencos(t, q, d) and the corresponding mean value.Given a query q, we re-estimate the term fre-quency of query term t in d with sense informationintegrated as smoothing:tf sen(t, d) = tf (t, d) + sen(t, q, d), (5)where function sen(t, q, d) is a measure of t?s senseinformation in d, which is defined as follows:sen(t, q, d) = ?
?cos(t,q,d)stf (S(t, q), d).
(6)In sen(t, q, d), the last item stf (S(t, q), d) calcu-lates the sum of the sense frequencies of t senses ind, which represents the amount of t?s sense informa-tion in d. The first item ?
?cos(t,q,d) is a weight of thesense information concerning the relative sense sim-ilarity ?cos(t, q, d), where ?
is a positive parame-ter to control the impact of sense similarity.
When?cos(t, q, d) is larger than zero, such that the sensesimilarity of d and q according to t is above the av-erage, the weight for the sense information is largerthan 1; otherwise, it is less than 1.
The more similarthey are, the larger the weight value.
For t /?
q, be-cause the sense set S(t, q) is empty, stf (S(t, q), d)equals to zero and tf sen(t, d) is identical to tf (t, d).With sense incorporated, the term frequency is in-fluenced by the sense information.
Consequently,the estimation of probability of t in d becomes queryspecific:p(t|?send ) =tf sen(t, d) + ?
p(t|?senC )?t?
?V tf sen(t?, d) + ?, (7)where the probability of t in C is re-calculated as:p(t|?senC ) =Pd?
?C tf sen (t,d?)Pd??CPt?
?V tf sen (t?,d?)
.5.2 Expanding with synonym relationsWords usually have some semantic relations withothers.
Synonym relation is one of the semantic re-lations commonly used to improve IR performance.In this part, we further integrate the synonym rela-tions of senses into the LM approach.Suppose R(s) is the set of senses having syn-onym relation with sense s. Define S(q) as the setof senses of query q, S(q) =?t?q S(t, q), and de-fine R(s, q)=R(s)?S(q).
We update the frequencyof a query term t in d by integrating the synonymrelations as follows:tf syn(t, d) = tf sen(t, d) + syn(t, q, d), (8)277where syn(t, q, d) is a function measuring the syn-onym information in d:syn(t, q, d) =?s?S(t)?
(s, q)p(t, s, q)stf (R(s, q), d).The last item stf (R(s, q), d) in syn(t, q, d) is thesum of the sense frequencies of R(s, q) in d. Noticethat the synonym senses already appearing in S(q)are not included in the calculation, because the infor-mation of these senses has been used in some otherplaces in the retrieval function.
The frequency ofsynonyms, stf (R(s, q), d), is weighted by p(t, s, q)together with a scaling function ?
(s, q):?
(s, q) = min(1, stf (s,C)stf (R(s,q),C)).When stf (s, C), the frequency of sense s in C, isless than stf (R(s, q), C), the frequency of R(s, q)in C, the function ?
(s, q) scales down the impactof synonyms according to the ratio of these two fre-quencies.
The scaling function makes sure that theoverall impact of the synonym senses is not greaterthan the original word senses.Accordingly, we have the probability of t in d up-dated to:p(t|?synd ) =tf syn(t, d) + ?
p(t|?synC )?t?
?V tf syn(t?, d) + ?, (9)and the probability of t in C is calculated as:p(t|?synC ) =Pd?
?C tf syn (t,d?)Pd??CPt?
?V tf syn (t?,d?)
.With this language model, the probability of a queryterm in a document is enlarged by the synonyms ofits senses; The more its synonym senses in a doc-ument, the higher the probability.
Consequently,documents with more synonym senses of the queryterms will get higher retrieval rankings.6 ExperimentsIn this section, we evaluate and analyze the mod-els proposed in Section 5 on standard TREC collec-tions.6.1 Experimental settingsWe conduct experiments on the TREC collection.The text collection C includes the documents fromTREC disk 4 and 5, minus the CR (CongressionalRecord) corpus, with 528,155 documents in total.
Inaddition, the other documents in TREC disk 1 to 5are used as the external text collection X .We use 50 queries from TREC6 Ad Hoc taskas the development set, and evaluate on 50 queriesfrom TREC7 Ad Hoc task, 50 queries from TREC8Ad Hoc task, 50 queries from ROBUST 2003(RB03), and 49 queries from ROBUST 2004(RB04).
In total, our test set includes 199 queries.We use the terms in the title field of TREC topics asqueries.
Table 1 shows the statistics of the five querysets.
The first column lists the query topics, and thecolumn #qry is the number of queries.
The columnAve gives the average query length, and the columnRels is the total number of relevant documents.Query Set Topics #qry Ave RelsTREC6 301?350 50 2.58 4,290TREC7 351?400 50 2.50 4,674TREC8 401?450 50 2.46 4,728RB03 601?650 50 3.00 1,658RB044 651?700 49 2.96 2,062Table 1: Statistics of query setsWe use the Lemur toolkit (Ogilvie and Callan,2001) version 4.11 as the basic retrieval tool, and se-lect the default unigram LM approach based on KL-divergence and Dirichlet-prior smoothing method inLemur as our basic retrieval approach.
Stop wordsare removed from queries and documents using thestandard INQUERY stop words list (Allan et al,2000), and then the Porter stemmer is applied to per-form stemming.
The stem forms are finally used forindexing and retrieval.We set the smoothing parameter ?
in Equation 3to 400 by tuning on TREC6 query set in a range of{100, 400, 700, 1000, 1500, 2000, 3000, 4000, 5000}.With this basic method, up to 10 top ranked docu-ments Dq are retrieved for each query q from theextended text collection C ?
X , for the usage ofperforming PRF and generating query senses.For PRF, we follow the implementation of Indri?sPRF method and further apply the CE technique asdescribed in Section 3.2.
The number of terms se-lected from Dq for expansion is tuned from range{20, 25, 30, 35, 40} and set to 25.
The interpolationparameter ?
in Equation 4 is set to 0.7 from range4Topic 672 is eliminated, since it has no relevant document.278Method TREC7 TERC8 RB03 RB04 Comb Impr #ret-relTop 1 0.2530 0.3063 0.3704 0.4019 - - -Top 2 0.2488 0.2876 0.3065 0.4008 - - -Top 3 0.2427 0.2853 0.3037 0.3514 - - -Stemprf (Baseline) 0.2634 0.2944 0.3586 0.3781 0.3234 - 9248Stemprf+MFS 0.2655 0.2971 0.3626?
0.3802 0.3261?
0.84% 9281Stemprf+Even 0.2655 0.2972 0.3623?
0.3814 0.3263?
0.91% 9284Stemprf+WSD 0.2679?
0.2986?
0.3649?
0.3842 0.3286?
1.63% 9332Stemprf+MFS+Syn 0.2756?
0.3034?
0.3649?
0.3859 0.3322?
2.73% 9418Stemprf+Even+Syn 0.2713?
0.3061?
0.3657?
0.3859?
0.3320?
2.67% 9445Stemprf+WSD+Syn 0.2762?
0.3126?
0.3735?
0.3891?
0.3376?
4.39% 9538Table 2: Results on test set in MAP score.
The first three rows show the results of the top participating systems, thenext row shows the performance of the baseline method, and the rest rows are the results of our method with differentsettings.
Single dagger (?)
and double dagger (?)
indicate statistically significant improvement over Stemprf at the95% and 99% confidence level with a two-tailed paired t-test, respectively.
The best results are highlighted in bold.
{0.1, 0.2, ..., 0.9}.
The CE-PRF method with thisparameter setting is chosen as the baseline.To estimate the sense distributions for terms inquery q, the method described in Section 4.2 is ap-plied with Dq.
To disambiguate the documents inthe text collection, besides the usage of the super-vised WSD system described in Section 4.1, twoWSD baseline methods, Even and MFS, are appliedfor comparison.
The method Even assigns equalprobabilities to all senses for each word, and themethod MFS tags the words with their correspond-ing most frequent senses.
The parameter ?
in Equa-tion 6 is tuned on TREC6 from 1 to 10 in incrementof 1 for each sense tagging method.
It is set to 7,6, and 9 for the supervised WSD method, the Evenmethod, and the MFS method, respectively.Notice that the sense in our WSD system is con-ducted with two parts, a morphological root and aChinese translation.
The Chinese parts not only dis-ambiguate senses, but also provide clues of connec-tions among different words.
Assume that the senseswith the same Chinese part are synonyms, there-fore, we can generate a set of synonyms for eachsense, and then utilize these synonym relations inthe method proposed in Section 5.2.6.2 Experimental resultsFor evaluation, we use average precision (AP) as themetric to evaluate the performance on each query q:AP(q) =PRr=1 [p(r)rel(r)]relevance(q) ,where relevance(q) is the number of documents rel-evant to q, R is the number of retrieved documents,r is the rank, p(r) is the precision of the top r re-trieved documents, and rel(r) equals to 1 if the rthdocument is relevant, and 0 otherwise.
Mean aver-age precision (MAP) is a metric to evaluate the per-formance on a set of queries Q:MAP(Q) =Pq?Q AP(q)|Q| ,where |Q| is the number of queries in Q.We retrieve the top-ranked 1,000 documents foreach query, and use the MAP score as the main com-paring metric.
In Table 2, the first four columns arethe MAP scores of various methods on the TREC7,TREC8, RB03, and RB04 query sets, respectively.The column Comb shows the results on the union ofthe four test query sets.
The first three rows list theresults of the top three systems that participated inthe corresponding tasks.
The row Stemprf shows theperformance of our baseline method, the stem-basedCE-PRF method.
The column Impr calculates thepercentage improvement of each method over thebaseline Stemprf in column Comb.
The last column#ret-rel lists the total numbers of relevant documentsretrieved by different methods.The rows Stemprf +{MFS, Even, WSD} are the re-sults of Stemprf incorporating with the senses gen-erated for the original query terms, by applying theapproach proposed in Section 5.1, with the MFSmethod, the Even method, and our supervised WSDmethod, respectively.
Comparing to the baselinemethod, all methods with sense integrated achieveconsistent improvements on all query sets.
Theusage of the supervised WSD method outperformsthe other two WSD baselines, and it achieves sta-279tistically significant improvements over Stemprf onTREC7, TREC8, and RB03.The integration of senses into the baseline methodhas two aspects of impact.
First, the morphologi-cal roots of senses conquer the irregular inflectionproblem.
Thus, the documents containing the irreg-ular inflections are retrieved when senses are inte-grated.
For example, in topic 326 {ferry sinkings},the stem form of sinkings is sink.
As sink is an irreg-ular verb, the usage of senses improves the retrievalrecall by retrieving the documents containing the in-flection forms sunk, sank, and sunken.Second, the senses output by supervised WSDsystem help identify the meanings of query terms.Take topic 357 {territorial waters dispute} for ex-ample, the stem form of waters is water and its ap-propriate sense in this query should be water ??
(body of water) instead of the most frequent senseof water ?
(H2O).
In Stemprf +WSD, we correctlyidentify the minority sense for this query term.
Inanother example, topic 425 {counterfeiting money},the stem form of counterfeiting is counterfeit.
Al-though the most frequent sense counterfeit ??
(not genuine) is not wrong, another sense counter-feit ??
(forged money) is more accurate for thisquery term.
The Chinese translation in the lattersense represents the meaning of the phrase in orig-inal query.
Thus, Stemprf +WSD outperforms theother two methods on this query by assigning thehighest probability for this sense.Overall, the performance of Stemprf +WSD is bet-ter than Stemprf +{MFS, Even} on 121 queries and119 queries, respectively.
The t-test at the confi-dence level of 99% indicates that the improvementsare statistically significant.The results of expanding with synonym relationsin the above three methods are shown in the lastthree rows, Stemprf +{MFS, Even, WSD}+Syn.
Theintegration of synonym relations further improvesthe performance no matter what kind of sense tag-ging method is applied.
The improvement varieswith different methods on different query sets.
Asshown in the last column of Table 2, the number ofrelevant documents retrieved is increased for eachmethod.
Stemprf +Even+Syn retrieves more rele-vant documents than Stemprf +MFS+Syn, becausethe former method expands more senses.
Overall,the improvement achieved by Stemprf +WSD+Syn islarger than the other two methods.
It shows thatthe WSD technique can help choose the appropriatesenses for synonym expansion.Among the different settings, Stemprf +WSD+Synachieves the best performance.
Its improvementover the baseline method is statistically significantat the 95% confidence level on RB04 and at the 99%confidence level on the other three query sets, withan overall improvement of 4.39%.
It beats the bestparticipated systems on three out of four query sets5,including TREC7, TREC8, and RB03.7 ConclusionThis paper reports successful application of WSDto IR.
We proposed a method for annotating sensesto terms in short queries, and also described an ap-proach to integrate senses into an LM approach forIR.
In the experiment on four query sets of TRECcollection, we compared the performance of a su-pervised WSD method and two WSD baseline meth-ods.
Our experimental results showed that the incor-poration of senses improved a state-of-the-art base-line, a stem-based LM approach with PRF method.The performance of applying the supervised WSDmethod is better than the other two WSD base-line methods.
We also proposed a method to fur-ther integrate the synonym relations to the LM ap-proaches.
With the integration of synonym rela-tions, our best performance setting with the super-vised WSD achieved an improvement of 4.39% overthe baseline method, and it outperformed the bestparticipating systems on three out of four query sets.AcknowledgmentsThis research is supported by the Singapore Na-tional Research Foundation under its InternationalResearch Centre @ Singapore Funding Initiativeand administered by the IDM Programme Office.ReferencesE.
Agirre, X. Arregi, and A. Otegi.
2010.
Document ex-pansion based on WordNet for robust IR.
In Proceed-ings of the 23rd International Conference on Compu-tational Linguistics, pages 9?17.5The top two systems on RB04 are the results of the sameparticipant with different configurations.
They used lots of webresources, such as search engines, to improve the performance.280J.
Allan, M. E. Connell, W.B.
Croft, F.F.
Feng, D. Fisher,and X. Li.
2000.
INQUERY and TREC-9.
In Pro-ceedings of the 9th Text REtrieval Conference, pages551?562.M.
Bendersky, W. B. Croft, and D. A. Smith.
2010.Structural annotation of search queries using pseudo-relevance feedback.
In Proceedings of the 19th ACMConference on Information and Knowledge Manage-ment, pages 1537?1540.A.
Broder, M. Fontoura, E. Gabrilovich, A. Joshi, V. Josi-fovski, and T. Zhang.
2007.
Robust classification ofrare queries using web knowledge.
In Proceedingsof the 30th International ACM SIGIR Conference onResearch and Development in Information Retrieval,pages 231?238.G.
Cao, J. Y. Nie, and J. Bai.
2005.
Integrating wordrelationships into language models.
In Proceedingsof the 28th International ACM SIGIR Conference onResearch and Development in Information Retrieval,pages 298?305.M.
Carpuat and D. Wu.
2007.
Improving statisti-cal machine translation using word sense disambigua-tion.
In Proceedings of the 2007 Joint Conferenceon Empirical Methods in Natural Language Process-ing and Computational Natural Language Learning,pages 61?72.Y.
S. Chan and H. T. Ng.
2005.
Scaling up wordsense disambiguation via parallel texts.
In Proceed-ings of the 20th National Conference on Artificial In-telligence, pages 1037?1042.Y.
S. Chan, H. T. Ng, and D. Chiang.
2007.
Word sensedisambiguation improves statistical machine transla-tion.
In Proceedings of the 45th Annual Meeting ofthe Association for Computational Linguistics, pages33?40.H.
Fang.
2008.
A re-examination of query expansion us-ing lexical resources.
In Proceedings of the 46th An-nual Meeting of the Association of Computational Lin-guistics: Human Language Technologies, pages 139?147.J.
Gonzalo, F. Verdejo, I. Chugur, and J. Cigarrin.
1998.Indexing with WordNet synsets can improve text re-trieval.
In Proceedings of the COLING-ACL Workshopon Usage of WordNet in Natural Language ProcessingSystems, pages 38?44.J.
Gonzalo, A. Penas, and F. Verdejo.
1999.
Lexicalambiguity and information retrieval revisited.
In Pro-ceedings of the 1999 Joint SIGDAT Conference on Em-pirical Methods in Natural Language Processing andVery Large Corpora, pages 195?202.D.
He and D. Wu.
2011.
Enhancing query transla-tion with relevance feedback in translingual informa-tion retrieval.
Information Processing & Management,47(1):1?17.S.
B. Kim, H. C. Seo, and H. C. Rim.
2004.
Informa-tion retrieval using word senses: root sense tagging ap-proach.
In Proceedings of the 27th International ACMSIGIR Conference on Research and Development inInformation Retrieval, pages 258?265.R.
Krovetz and W. B. Croft.
1992.
Lexical ambiguityand information retrieval.
ACM Transactions on In-formation Systems, 10(2):115?141.K.
L. Kwok and M. Chan.
1998.
Improving two-stagead-hoc retrieval for short queries.
In Proceedingsof the 21st International ACM SIGIR Conference onResearch and Development in Information Retrieval,pages 250?256.J.
Lafferty and C. Zhai.
2001.
Document language mod-els, query models, and risk minimization for informa-tion retrieval.
In Proceedings of the 24th InternationalACM SIGIR Conference on Research and Develop-ment in Information Retrieval, pages 111?119.V.
Lavrenko and W. B. Croft.
2001.
Relevance basedlanguage models.
In Proceedings of the 24th Interna-tional ACM SIGIR Conference on Research and De-velopment in Information Retrieval, pages 120?127.S.
Liu, F. Liu, C. Yu, and W. Meng.
2004.
An ef-fective approach to document retrieval via utilizingWordNet and recognizing phrases.
In Proceedingsof the 27th International ACM SIGIR Conference onResearch and Development in Information Retrieval,pages 266?272.S.
Liu, C. Yu, and W. Meng.
2005.
Word sense disam-biguation in queries.
In Proceedings of the 14th ACMConference on Information and Knowledge Manage-ment, pages 525?532.G.
A. Miller.
1990.
WordNet: An on-line lexi-cal database.
International Journal of Lexicography,3(4):235?312.F.
J. Och and H. Ney.
2003.
A systematic comparison ofvarious statistical alignment models.
ComputationalLinguistics, 29(1):19?51.P.
Ogilvie and J. Callan.
2001.
Experiments using theLemur toolkit.
In Proceedings of the 10th Text RE-trieval Conference, pages 103?108.J.
M. Ponte and W. B. Croft.
1998.
A language model-ing approach to information retrieval.
In Proceedingsof the 21st International ACM SIGIR Conference onResearch and Development in Information Retrieval,pages 275?281.J.
M. Ponte.
1998.
A Language Modeling Approachto Information Retreival.
Ph.D. thesis, Department ofComputer Science, University of Massachusetts.M.
Sanderson.
1994.
Word sense disambiguation and in-formation retrieval.
In Proceedings of the 17th Inter-national ACM SIGIR Conference on Research and De-velopment in Information Retrieval, pages 142?151.281M.
Sanderson.
2000.
Retrieving with good sense.
Infor-mation Retrieval, 2(1):49?69.H.
Schu?tze and J. O. Pedersen.
1995.
Information re-trieval based on word senses.
In Proceedings of the4th Annual Symposium on Document Analysis and In-formation Retrieval, pages 161?175.C.
Stokoe, M. P. Oakes, and J. Tait.
2003.
Word sensedisambiguation in information retrieval revisited.
InProceedings of the 26th International ACM SIGIRConference on Research and Development in Informa-tion Retrieval, pages 159?166.E.
M. Voorhees.
1993.
Using WordNet to disam-biguate word senses for text retrieval.
In Proceedingsof the 16th International ACM SIGIR Conference onResearch and Development in Information Retrieval,pages 171?180.E.
M. Voorhees.
1994.
Query expansion using lexical-semantic relations.
In Proceedings of the 17th Inter-national ACM SIGIR Conference on Research and De-velopment in Information Retrieval, pages 61?69.C.
Zhai and J. Lafferty.
2001a.
Model-based feedbackin the language modeling approach to information re-trieval.
In Proceedings of the 10th ACM Conferenceon Information and Knowledge Management, pages403?410.C.
Zhai and J. Lafferty.
2001b.
A study of smoothingmethods for language models applied to ad hoc infor-mation retrieval.
In Proceedings of the 24th Interna-tional ACM SIGIR Conference on Research and De-velopment in Information Retrieval, pages 334?342.Z.
Zhong and H. T. Ng.
2009.
Word sense disambigua-tion for all words without hard labor.
In Proceedingsof the 21st International Joint Conference on ArtificialIntelligence, pages 1616?1621.Z.
Zhong and H. T. Ng.
2010.
It Makes Sense: A wide-coverage word sense disambiguation system for freetext.
In Proceedings of the 48th Annual Meeting ofthe Association of Computational Linguistics: SystemDemonstrations, pages 78?83.282
