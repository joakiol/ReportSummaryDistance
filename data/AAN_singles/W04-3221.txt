Attribute-Based and Value-Based Clustering: An EvaluationAbdulrahman ALMUHAREB and Massimo POESIODepartment of Computer Science and Centre for Cognitive ScienceUniversity of EssexColchester, United Kingdom, CO4 3SQaalmuh@essex.ac.uk poesio@essex.ac.ukAbstractIn most research on concept acquisition fromcorpora, concepts are modeled as vectors ofrelations extracted from syntactic structures.In the case of modifiers, these relations oftenspecify values of attributes, as in (attrred); this is unlike what typically proposed intheories of knowledge representation, whereconcepts are typically defined in terms of theirattributes (e.g., color).
We comparedmodels of concepts based on values withmodels based on attributes, using lexicalclustering as the basis for comparison.
Wefind that attribute-based models work betterthan value-based ones, and result in shorterdescriptions; but that mixed models includingboth the best attributes and the best valueswork best of all.1 IntroductionIn most recent research on concept acquisitionfrom corpora (e.g., for lexicon construction),concepts are viewed as vectors of relations, orproperties, extracted from syntactic structures(Grefenstette, 1993; Lin, 1998; Curran and Moens,2002; Kilgarriff, 2003, and many others).
Theseproperties often specify values of attributes such ascolor, shape, or size: for example, the vector usedby Lin (1998) for the concept dog includes theproperty (dog adj-mod brown).
(We willuse the term values here to refer to any modifier.
)To our knowledge, however, no attempt has beenmade by computational linguists to use theattributes themselves in such vectors: i.e., to learnthat the description of the concept dog includeselements such as (dog color) or (dogsize).
This is surprising when considering thatmost models of concepts in the AI literature arebased on such attributes (Brachman and Levesque,1985).Two problems need to be addressed when tryingto identify concept attributes.
The first problem isthat values are easier to extract.
We found,however, that patterns like the X of the dog,already used in (Berland and Charniak, 1999;Poesio et al 2002) to find part-of relations (usingtechniques derived from those used in (Hearst,1998; Caraballo, 1999) to find hyponymyrelations) are quite effective at finding attributes.A second problem might be that instances of suchpatterns are less frequent than those used to extractvalues, even in large corpora such as the BritishNational Corpus (BNC).
But this problem, as well,is less serious when using the Web as a corpus(Kilgarriff and Schuetze, 2003; Keller and Lapata,2003; Markert et al submitted).We report on two experiments whose goal wasto test whether identifying attributes leads to betterlexical descriptions of concepts.
We do this bycomparing the results obtained by using attributesor more general modifiers ?
that we will simplycall values ?
as elements of concept vectors usedto identify concept similarities via clustering.
InSection 2, we discuss how Web data were used tobuild attribute- and value- based concept vectors,and our clustering and evaluation methods.
InSection 3, we discuss a first experiment using theset of concepts used in (Lund and Burgess, 1996).In Section 4, we discuss a second experiment using214 concepts from WordNet (Fellbaum, 1998).
InSection 5 we return to the notion of attribute.2 Methods2.1 Using Text Patterns to Build ConceptDescriptionsOur techniques for extracting conceptdescriptions are simpler than those used in otherwork in at least two respects.
First of all, we onlyextracted values expressed as nominal modifiers,ignoring properties expressed by verbalconstructions in which the concept occurred as anargument (e.g., Lin?s (dog obj-of have)).
(We originally made this simplification toconcentrate on the comparison between attributesand values (many verbal relations express morecomplex properties), but found that the resultingdescriptions were still adequate for clustering.
)Secondly, our data were not parsed or POS-taggedprior to extracting concept properties; our patternsare word-based.
Full parsing is essential whencomplete descriptions are built (see below) andallows the specification of much more generalpatterns (e.g., matching descriptions modified in avariety of ways, see below), but is computationallymuch more expensive, particularly when Web dataare used, as done here.
We also found that whenusing the Web, simple text patterns not requiringparsing or POS tagging were sufficient to extractlarge numbers of instances of properties with agood degree of precision.Our methods for extracting 'values' areanalogous to those used in the previous literature,apart from the two simplifications just mentioned:i.e., we just consider every nominal modifier asexpressing a potential property.
The pattern weuse to extract values is as follows:?
"[a|an|the] * C [is|was]"where C is a concept, and  the wildcard (*) standsfor  an unspecified value.
The restriction toinstances containing is or was to ensure that the Cactually stands for a concept (i.e., avoidingmodifiers) proved adequate to ensure precision.
Anexample of text matching this pattern is:?
?
an inexpensive car is ?The pattern we use for extracting conceptattributes is based on linguistic tests for attributesalready discussed, e.g., in (Woods, 1975).According to Woods, A is an attribute of C if wecan say [V is a/the A of C]: e.g., brown is a colorof dogs.
If no V can be found which is a value ofA, then A can not be an attribute for the concept C.This test only selects attributes that have values,and is designed to exclude other functions definedover concepts, such as parts.
But some of thesefunctions can be (and have been) viewed asdefining attributes of concepts as well; so for themoment we used more general patterns identifyingall relational nouns taking a particular concept asarguments.
(We return on the issue of thecharacterization of attributes below.)
Our patternfor attributes is shown below:?
"the * of the C [is|was]"where again C is a concept, but the wildcarddenotes an unspecified attribute.
Again, is/was isused to increase precision.
An example of textmatching this pattern is:?
?
the price of the car was ?Both of the patterns we use satisfy Hearst'sdesiderata for good patterns (Hearst, 1998): theyare (i) frequent, (ii) precise, and (iii) easy torecognize.
Patterns similar to our attribute patternwere used by Berland and Charniak (1999) andPoesio et al(2002) to find object parts only; aftercollecting their data, Berland and Charniak filteredout words ending with "ness", "ing", and "ity",because these express qualities of objects, and useda ranking method to rank the remaining words.
(An accuracy of 55% for the top 50 proposed partswas reported.)
We found that these patterns can beused to collect other sorts of 'attributes', as well.2.2 Web Data Collection through GoogleIn recent years there has been growing evidencethat using the Web as a corpus greatly reduces theproblem of data sparseness, and its size more thancompensates the lack of balance (e.g., (Keller andLapata, 2003)).
The benefits from using the Webover even large corpora like the BNC forextracting semantic relations, particularly whenusing simple text patterns, were informally pointedout in (Poesio, 2003) and demonstrated moresystematically by Markert et al(submitted).
Thesefindings were confirmed by our experiments.
Acomparison of numbers of instances of somepatterns using the Web and the BNC is shown inTable 1.Pattern Web BNC"the * of the *" 23,100,000 208,155"the * of the * is" 10,900,000 3,627"the * of the car is" 26,400 5Attribute"the * of the hat is" 2,770 1"the fast * is" 38,100 3"an electronic * is" 120,000 5"the * car is" 84,500 24 Value"the * hat is" 17,100 1Table 1:  Comparison of frequencies of somepatterns in BNC and the Web.
Web frequency isbased on Google countsWe collect our data from the Web using theGoogle search engine, accessed via the freelyavailable Google Web API1.
The API only allowsto retrieve the first 1,000 results per search request;to overcome this restriction, we use the dateragefeature of the Google search request.
This featureallows the user to fragment the search space into anumber of periods, hence retrieving only pages thathave been updated during a specified period.
In thetwo experiments presented here, we aimed tocollect up to 10,000 matches per search requestusing the daterage feature: we divided the searchspace into 100 days starting from January, 1990until mid 2004.
(The procedure we used does notguarantee collecting all the instances in theaccessed periods, because if there are more than1 Google Web API is available on the Web athttp://www.google.com/apis/1,000 instances in one period, then only the first1,000 instances will be collected.)
2Our requests to Google take the general form "s1* s2" (including the double quotes), where s1 and s2are two strings and the wildcard denotes anunspecified single word.
For example, the searchrequest "a * car is" catches instances such as: [ared car is], [a small car is], and [a sport car is].
Itis worth mentioning that Google does not payattention to punctuation marks; this is one area inwhich parsing would help.When receiving results from Google, we do notaccess the actual Web pages, but instead weprocess the snippets that are returned by Google.32.3 Clustering MethodsThe task that we use to compare conceptdescriptions is lexical acquisition via clustering.We experimented with clustering systems such asCOBWEB (Fisher, 1987) and SUBDUE (Cook andHolder, 2000) before settling on CLUTO 2.1(Karypis, 2002).
CLUTO is a general-purposeclustering tool that implements three differentclustering algorithms: partitional, agglomerative,and graph partitioning algorithms.
CLUTOproduces both flat and hierarchical clusters.
It usesa hard clustering technique, where each conceptcan be assigned to only one cluster.
The softwareallows to choose a similarity metric between a setincluding extended Jaccard and cosine.
CLUTOwas optimized to cluster data of large sizes in areasonable time.
The software also providesanalysis and visualization tools.In this paper, we use extended Jaccard, whichwas found to produce more accurate results thanthe cosine function in similar tasks (Karypis, 2002;Curran and Moens, 2003).
In CLUTO, theextended Jaccard function works only with thegraph partitioning algorithm.2.4 Evaluation MeasuresWe used two types of measures to evaluate theclusters produced by CLUTO using the conceptdescriptions discussed above, both of whichcompare the clusters produced by the system tomodel clusters.
Accuracy is computed by dividingthe number of correctly clustered concepts by thetotal number of concepts.
The number of correctlyclustered concepts is determined by examining2 Also, registered users of the API can send up to 1,000requests per day, but our daily limit was increased byGoogle to 20,000 requests per day.3 Snippets are text excerpts captured from the actualweb pages with embedded HTML tags.
We process thesnippets by removing the HTML tags and extracting thetargeted piece of text that was specified in the request.each system cluster, finding the class of eachconcept in the model clusters, and determining themajority class.
The cluster is then labeled with thisclass;   the concepts belonging to it are taken to becorrectly clustered, whereas the remainingconcepts are judged to be incorrectly clustered.In the contingency table evaluation (Swets,1969; Hatzivassiloglou and McKeown, 1993), theclusters are converted into two lists (one for thesystem clusters and one for the model clusters) ofyes-no answers to the question "Does the pair ofconcepts occur in the same cluster?"
for each pairof concepts.
A contingency table is then built,from which recall (R), precision (P), fallout, and Fmeasures can be computed.
For example, if themodel clusters are: (A, B, C) and (D), and thesystem clusters are: (A, B) and (C, D), the yes-nolists are as in Table 2, and the contingency table isas in Table 3.Question Model AnswerSystemAnswerDoes the pair (A, B) occur inthe same cluster?
Yes YesDoes the pair (A, C) occur inthe same cluster?
Yes NoDoes the pair (A, D) occur inthe same cluster?
No NoDoes the pair (B, C) occur inthe same cluster?
Yes NoDoes the pair (B, D) occur inthe same cluster?
No NoDoes the pair (C, D) occur inthe same cluster?
No YesTable 2: Model and the system answers for theco-occurrence questionModel Answer System AnswerYes  Noa b Yes11c d No22Table 3: The contingency table33.0caaR ?+=  50.0baaP =+=33.0dbbFallout ?+=  40.0PRPR2F ?+?
?=3 First Experiment: Using a Set of Conceptsfrom Lund and BurgessOne limitation of using Google is that even withan increased daily limit of 20,000, it wouldn?treally be feasible to attempt to cluster, say, all ofWordNet 100,000 noun concepts.
For this reason,we used much smaller sets of concepts in our twoexperiments.
The first set alowed us to compareour results with those obtained by Lund andBurgess (1996); the second set consisted of a largernumber of concepts from WordNet.Lund and Burgess (1996) used a set of 34concepts belonging to 3 different classes (animals,body parts, and geographical locations) to evaluatetheir method for acquiring lexical representations,HAL (Hyperspace Analogue to Language).
Lundand Burgess were able to correctly cluster all of theconcepts except for one body part, tooth, whichwas incorrectly clustered with animals.
In this firstexperiment, we used the 34 Lund and Burgessconcepts plus Italy, horse, and tongue (37 in total)to compare value-based and attribute-baseddescription when used for clustering, using conceptdescriptions collected using the methods describedabove.The input to clustering is a frequency table withconcepts as rows and values, attributes, or bothattributes and values as columns.
Each cell in thetable contains the frequency of co-occurrencebetween the concept and corresponding value orattribute.
Before clustering, the frequencies aretransformed into weighted values using the t test(Manning and Schutze, 1999).
(The t test wasfound by Curran and Moens (2002) to be the bestweighting method.)
The t test formula we used forattributes is shown below:2ji2jijij,iN)attribute,concept(CN)attribute(C)concept(CN)attribute,concept(Ct????????
???
(1)where N is the total number of relations, and C is acount function.
The values formula is similar.We use the CLUTO vcluster command forclustering, with parameters: similarity function =Extended Jaccard Coefficient, clustering method =Graph Partitioning, no.
of clusters = 3.Vector Size4Used Data500 1522 3044 4753 4969ValuesOnly 64.86% 94.59% - - 94.59%AttributesOnly 97.30% 97.30% - 97.30% -Attributes1522and Values1522- - 100.00% - -Table 4: Clustering accuracy with  values,attributes, and their combination, using differentvector sizes4 Here, we choose the top k features by their overallfrequency.Table 4 shows the accuracy of the producedclusters when using values, attributes, and thecombination with different vector sizes.
Theresults show that with concept descriptions oflength 500, attributes (97.30%) are much moreaccurate than values (64.86%).
With vectors ofsize 1522, the accuracy with attributes remains thesame, while the accuracy with values improves,but is still lower than the accuracy with attributes(94.59%).
This indicates that attributes have morediscriminatory power than values: an attributevector of size 500 is sufficient to produce a moreaccurate result than using a value vector of threetimes the size.
But perhaps the most interestingresult is that even though further  increasing thesize of pure attribute- and value- descriptions (to4753 and 4969, respectively) does not improveaccuracy, perfect accuracy can be obtained byusing vectors  of length 3044, including the 1522best attributes and the 1522 best values.
Thissuggests that while attributes are a good way ofgeneralizing across properties, not all properties ofconcepts can be viewed as attribute/value pairs(section 5; also (Poesio and Almuhareb,submitted)).4 Second Experiment: Using a Set ofConcepts from WordNetIn order to get a more realistic evaluation and abetter comparison with work such as (Lin, 1998;Curran and Moens, 2002), we also ran a secondexperiment using a larger set of concepts from theWordNet noun hierarchy (Fellbaum, 1998).
Wechose 214 relatively common concepts from 13different classes covering a variety ofsubhierarchies (see Appendix A).
Each classcontains a set of concepts that share a commonhypernym in the WordNet hierarchy.Model Answer Systems AnswerYes NoYes 1294 503 BooleanNo 387 20607Yes 1117 950 Frequency No 564 20160Table 5: The contingency table based on booleanand frequency for the combined attributes andvaluesThe frequencies for attributes and values wereagain collected as in the first experiment.However, these data were used in a different way.In determining the weight, we performed the t test5on boolean values instead of the original5 We consider only positive values of t.frequencies6, treating all positive frequencies as 1and everything else as 0.
This eliminates the effectof variations in frequencies in the original data, theintuition being that frequencies do not add to thesemantics of concepts: what we are interested in isthe fact that a concept has a given attribute/value,regardless of how many times we haveencountered this fact.
This approach is similar tothe approach adopted in (Hearst, 1998); see also(Curran and Moens, 2002) for a comparison ofmethods dealing with concept vectors based onraw frequencies or boolean values.
Thetransformed table is a binary table that containsonly zeros and ones in its cells.
Table 5 shows thecontingency table for clusters produced based onboolean and frequency for the combined data ofattributes and values; it shows that boolean data ismore accurate in the four cases.For clustering, as well, we used CLUTO in adifferent way.
Instead of asking CLUTO tocompute the similarities between the concepts, wecomputed them ourselves, using the version of theextended Jaccard similarity function used byCurran and Moens, as this version produces betterresults than the one used in CLUTO.
The twoversions of the extended Jaccard function areshown below:where tm,i and tn,i are the weighted co-occurrencevalues between concept m and concept n withattribute/value i, and computed as in equation (1).Measures UsedData7 Accuracy Recall Precision Fallout FValuesOnly 71.96% 58.48% 52.91% 04.14% 55.55%AttributesOnly 64.02% 59.90% 53.54% 04.14% 56.54%AttributesAnd Values 85.51% 76.98% 72.01% 02.38% 74.41%Table 6: Clustering evaluation based on values,attributes, and the combinationWe compute the similarity between each pair ofconcepts, produce a similarity matrix and send it toCLUTO for clustering.
We then call the scluster6 In equation (1), this will effect only C(concepti,attributej), other counts will not be effected.7 Here, we use full size vectors that contain all thefeatures.command of CLUTO with the followingparameters: clustering method = GraphPartitioning, no.
of clusters = 13.
The results of theevaluation are shown in Table 6.Value-based concept descriptions resulted inbetter clusters than attribute-based when measuredusing Accuracy (71.96% vs. 64.02%), but the othermeasures all indicate that attributes work slightlybetter than values: e.g., F=55.55% for values,56.64% for attributes.
The reason for thisdifference is that the Accuracy measure simplyevaluates if each concept is assigned to its correctcluster, while the remaining measures concernabout the relation between each pair of concepts(i.e., if they were assigned to the same cluster ornot).
But, just as in Experiment 1, the best resultsby any measure are again obtained when usingconcept descriptions containing the best 'attributes'and the best 'values'; this time, however, thedifference is much more significant: Accuracy is85.51%, F is 74.41%.Model ClusterSystemClusterBuildingDiseaseVehicleFeelingBodyPartFruitCreatorPublicationAnimalFurnitureClothF.RelationTime1 0 2 0 11 0 0 0 0 0 0 0 0 02 0 0 13 0 0 0 0 0 0 0 0 0 03 0 0 0 0 0 0 0 0 0 0 0 0 174 0 0 0 0 0 0 2 0 18 0 0 6 05 2 0 0 1 0 16 0 0 1 0 0 1 06 1 16 0 0 0 0 0 0 0 0 0 0 17 0 0 0 0 0 0 15 0 0 0 0 0 08 0 0 0 0 0 0 0 15 0 0 0 0 09 0 0 0 0 16 0 0 0 0 4 0 0 110 0 0 0 0 0 0 0 0 1 0 0 9 011 1 0 1 0 0 0 0 0 0 6 0 0 012 0 0 0 0 0 0 0 0 0 2 16 0 013 15 0 0 1 0 0 0 1 0 2 0 0 0Table 7: The confusion matrix for the clustersproduced using both attributes and valuesTable 7 shows the confusion matrix for theclusters produced using both attributes and values.A close inspection of these clusters reveals that'furniture' concepts were the less homogeneousbecause they were scattered among four differentclusters.
There are 14 'furniture' concepts; six ofthem (bookcase, cabinet, couch, cradle, desk andwardrobe) were grouped in a separate clusterwhich also contains two more concepts (pickupand greenhouse).
Four of the concepts (bed,lamp, seat, and table) were clustered with 'bodypart' concepts.
Two of the concepts (dresser andsofa) were clustered with 'cloth' concepts, and theremaining two concepts (chair and lounge) wereclustered with 'building' concepts.?????
?+?=+?=ii,ni,mi,ni,mii,ni,mnmCLUTOii,ni,mii,ni,mnmMoens&Curran))tt(tt()tt()concept,concept(sim)tt()tt()concept,concept(simTwo points should be noted about the furnitureconcepts.
First, at least two concepts (seat andlounge) have more than one sense in WordNet.Seat was clustered with body part concepts, whichis acceptable if we think of seat as "the fleshy partof the human body that you sit on" (WordNet,sense 2).
The same for lounge, which wasclustered with buildings, which is consistent withits second sense in WordNet: "a public room (as ina hotel or airport) with seating where people canwait".
This indicates that techniques fordifferentiating between different senses are needed?
e.g., using a soft clustering technique as in(Pereira et al 1993) instead of a hard clusteringtechnique.
Second, furniture concepts may nothave a common prototype that is shared by all ofthe member concepts.
This is a well knownproblem in the prototype theory of concepts(Laurence and Margolis, 1999).The greater compactness of attribute-basedrepresentations vs. value-based ones was moreevident in this second experiment.
We collected51,045 distinct values and 8,934 distinct attributes;the total number of value-concept relations is1,026,335, compared to 422,621 attribute-conceptrelations.5 Attributes and Values: A discussionAlthough our results suggest that trying toidentify attributes is beneficial, the notion of'attribute' is not completely clear, and has beenused in widely different ways in KnowledgeRepresentation literature.
An attempt of definingthe notion has been made by Guarino (1992), whoclassifies attributes into relational and non-relational attributes.
Relational attributes includequalities such as color and position, and relationalroles such as son and spouse.
Non-relationalattributes include parts such as wheel and engine.The Qualia Structure of the Generative Lexicon(Pustejovsky, 1991) is another attempt atidentifying "the essential attributes of an object asdefined by the lexical item".
Pustejovskyidentifies four roles: Constitutive Role (Guarino'sparts), Formal Role (Guarino's qualities), AgentiveRole (Guarino's relational roles), and Telic Role(not included in Guarino's classification).Our analysis of the attribute data shows that theattributes we found can be mapped in the four rolesof the Qualia structure.
Table 8 shows how wemanually mapped the top 50 attributes of theconcept car to the Qualia roles and the Guarino'sclasses.
This mapping is not trivial (e.g., a path isnot part of a car, and design can be regarded as aquality), but a variety of tests may help:Morphological and Ontological Tests:Dixon (1991) proposed a semantic classificationfor nouns.
According to Dixon, parts are concreteconcepts and mostly basic noun roots or rarelyderived from verbs, while qualities are abstractconcepts and many of them are basic noun roots orderived from adjectives, some derived from stems,and few derived from verbs.
Our observations alsosuggest that telic attributes are usually derivedfrom verbs.Attributes Test:  Since attributes can also beviewed as concepts (e.g., in WordNet), theythemselves should have some shared attributes.For example: since parts are concrete objects theyshould share attributes such as size, length, andgeometry.
Also, since qualities usually can beassigned values (e.g.
age (25)), then they shouldshare attributes such as range and average.Question Type Test:  Different types ofattributes tend to occur with different types ofquestions.
For example, relational role attributestend to occur with who-questions like "Who is thedriver of the car?"
and "Who is the manufacturer ofthe car?
"GuarinoClassQualiaRole Car AttributesPart Constitutive Rolefront, rear, interior, inside, side,body, trunk, exterior, underside,hood, back, nose, roof, engine,frame, floor, rest, silhouette,backseat, wheelbase, battery,chassis, pathQuality Formal Rolespeed, value, weight, price,velocity, color, condition,momentum, convenience,propulsion, look, inertia, state,model, history, balance, motion,performanceRelationalRoleAgentiveRole driver, owner- Telic Role8handling, use, search, design,benefitTable 8: The classification of the top 50attributes of the concept carIn future work, we plan to use some of thesetests to classify attributes, and possibly filter someof them; this might improve the discriminationpower of attributes.
Also, concepts may sharecertain Qualia, but differ in other respects: forexample, the chair concept and the man conceptshare some parts (e.g., arm, back, leg, and seat)and even some qualities (e.g., color, size, andshape) but differ in other levels (i.e., AgentiveRole, and Telic Role).8 Telic roles define purposes, functions, and activitiesthat are related to the concept.
Some valid telic roles forthe concept car would be: driving, selling, and buying.6 ConclusionsSimple text patterns were used to automaticallyextract both basic value-based and attribute-basedconcept descriptions for clustering purposes.
Ourpreliminary results suggest, first of all, that whenlarge amounts of data such as the Web areaccessed, these simple patterns may be sufficient tocompute descriptions rich enough to discriminatequite well, at least with small sets of conceptsbelonging to clearly distinct classes.
Secondly, wefound that even though attributes are fewer thanvalues, attribute-based descriptions need not be aslong as value-based ones to achieve as good orbetter results.
Finally, we found that the bestdescriptions included both attributes and moregeneral properties.
We plan to extend this workboth by refining our notion of attribute and byusing more sophisticated patterns working off theoutput of a parser.7 AcknowledgementAbdulrahman Almuhareb is supported by KingAbdulaziz City for Science and Technology(KACST), Riyadh, Saudi Arabia.
We want tothank Google for making their Web API availableto the research community and George Karypis forthe CLUTO clustering toolkit.ReferencesM.
Berland and E. Charniak.
1999.
Finding parts invery large corpora.
In Proc.
of the 37th  ACL,pages 57?64, University of Maryland.R.
J. Brachman and H. J. Levesque, editors.
1985.Reading in Knowledge Representation.
MorganKaufmann, California.S.
A. Caraballo.
1999.
Automatic construction of ahypernym-labeled noun hierarchy from text.
InProc.
of  the 37th  ACL.D.
J. Cook and L. B.
Holder.
2000.
Graph-baseddata mining.
IEEE Intelligent Systems, 15(2), 32-41.J.
R. Curran and M. Moens.
2002.
Improvementsin automatic thesaurus extraction.
In Proc.
of theACL Workshop on Unsupervised LexicalAcquisition, pages 59?66.R.
M. W. Dixon.
1991.
A New Approach toEnglish Grammar, on Semantic Principles.Clarendon Press, Oxford.C.
Fellbaum, editor.
1998.
WordNet: An electroniclexical database.
The MIT Press.D.
H. Fisher.
1987.
Knowledge acquisition viaincremental conceptual clustering.
MachineLearning, 2:139?172.G.
Grefenstette.
1993.
SEXTANT: Extractingsemantics from raw text implementation details.Heuristics: The Journal of KnowledgeEngineering.N.
Guarino.
1992.
Concepts, attributes andarbitrary relations: some linguistic andontological criteria for structuring knowledgebase.
Data and Knowledge Engineering, 8, pages249?261.V.
Hatzivassiloglou and K. McKeown.
1993.Towards the automatic identification ofadjectival scales: clustering adjectives accordingto meaning.
In Proc.
of the 31st ACL, pages 172?182.M.
A. Hearst.
1998.
Automated discovery ofWordNet relations.
In C. Fellbaum, editor,WordNet: An Electronic Lexical Database.
MITPress.G.
Karypis.
2002.
CLUTO: A clustering toolkit.Technical Report 02-017, University ofMinnesota.
Available at URL: http://www-users.cs.umn.edu/~karypis/cluto/.F.
Keller and M. Lapata.
2003.
Using the Web toobtain frequencies for unseen bigrams.Computational Linguistics, 29(3).A.
Kilgarriff and H. Schuetze.
2003.
Introductionto the special issue of Computational Linguisticson the web as a corpus.
ComputationalLinguistics.A.
Kilgarriff.
2003.
Thesauruses for NaturalLanguage Processing.
In Proc.
of the IEEE 2003International Conference on Natural LanguageProcessing and Knowledge Engineering (NLP-KE'03), Beijing.S.
Laurence and E. Margolis.
1999.
Concepts andCognitive Science.
In E. Margolis and S.Laurence, editors, Concepts: Core Readings.Cambridge, MA., Bradford Books/MIT Press,pages 3-81.D.
Lin.
1998.
Automatic retrieval and clustering ofsimilar words.
In Proc.
of COLING-ACL, 768-774.K.
Lund and C. Burgess.
1996.
Producing high-dimensional semantic spaces from lexical co-occurrence.
Behavior Research Methods,Instrumentation, and Computers, 28,  203-208.C.
D. Manning and H. Schuetze.
1999.Foundations of Statistical NLP.
MIT Press.K.
Markert, M. Nissim, and N. Modjeska.
2004.Comparing Knowledge Sources for NominalAnaphora Resolution.
Submitted.F.
Pereira, N. Tishby, and L. Lee.
1993.Distributional clustering of English words.
InProc.
of the 31st ACL, pages 183-190,Columbus, Ohio.M.
Poesio and A. Almuhareb.
2004.
Feature-basedvs.
Property-based KR: An EmpiricalPerspective.
Submitted.M.
Poesio, T. Ishikawa, S. Walde, and R. Vieira.2002.
Acquiring lexical knowledge for anaphoraresolution.
In Proc.
of  LREC, Las Palmas, June.M.
Poesio.
2003.
Associative descriptions andsalience.
In Proc.
of the EACL Workshop onComputational Treatments of Anaphora,Budapest.J.
Pustejovsky.
1991.
The generative lexicon.Computational Linguistics, 17(4), pages 409-441.J.
A. Swets.
1969.
Effectiveness of InformationRetrieval Methods.
American Documentation,20, pages 72-89.W.
A.
Woods.
1975.
What?s in a link: Foundationsfor semantic networks.
In Daniel G. Bobrow andAlan M. Collins, editors, Representation andUnderstanding: Studies in Cognitive Science,pages 35-82.
Academic Press, New York.Appendix A.
The 214 Concepts from the 13 WordNet Classes Used in Experiment 2Class ConceptsAnimal bear, bull, camel, cat, cow, deer, dog, elephant, horse, kitten, lion, monkey, mouse, oyster, puppy, rat, sheep, tiger, turtle, zebraBuilding abattoir, center, clubhouse, dormitory, greenhouse, hall, hospital, hotel, house, inn, library, nursery, restaurant, school, skyscraper, tavern, theater, villa, whorehouseCloth pants, blouse, coat, costume, gloves, hat, jacket, jeans, neckpiece, pajamas, robe, scarf, shirt, suit, trousers, uniformCreator architect, artist, builder, constructor, craftsman, designer, developer, farmer, inventor, maker, manufacture, musician, originator, painter, photographer, producer, tailorDisease acne, anthrax, arthritis, asthma, cancer, cholera, cirrhosis, diabetes, eczema, flu, glaucoma, hepatitis, leukemia, malnutrition, meningitis, plague, rheumatism, smallpoxFeeling anger, desire, fear, happiness, joy, love, pain, passion, pleasure, sadness, sensitivity, shame, wonderFruit apple, banana, berry, cherry, grape, kiwi, lemon, mango, melon, olive, orange, peach, pear, pineapple, strawberry, watermelonFurniture bed, bookcase, cabinet, chair, couch, cradle, desk, dresser, lamp, lounge, seat, sofa, table, wardrobeBody Part ankle, arm, ear, eye, face, finger, foot, hand, head, leg, nose, shoulder, toe, tongue, tooth, wristPublication atlas, book, booklet, brochure, catalog, cookbook, dictionary, encyclopedia, handbook, journal, magazine, manual, phonebook, reference, textbook, workbookFamilyRelationboy, child, cousin, daughter, father, girl, grandchild, grandfather, grandmother, husband,kid, mother, offspring, sibling, son, wifeTime century, decade, era, evening, fall, hour, month, morning, night, overtime, quarter, season, semester, spring, summer, week, weekend, winter, yearVehicle aircraft, airplane, automobile, bicycle, boat, car, cruiser, helicopter, motorcycle, pickup, rocket, ship, truck, van
