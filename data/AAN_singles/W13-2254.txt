Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 422?428,Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational LinguisticsMEANT at WMT 2013: A tunable, accurate yet inexpensivesemantic frame based MT evaluation metricChi-kiu LO and Dekai WUHKUSTHuman Language Technology CenterDepartment of Computer Science and EngineeringHong Kong University of Science and Technology{jackielo|dekai}@cs.ust.hkAbstractThe linguistically transparentMEANT andUMEANT metrics are tunable, simpleyet highly effective, fully automatic ap-proximation to the human HMEANT MTevaluation metric which measures seman-tic frame similarity between MT outputand reference translations.
In this pa-per, we describe HKUST?s submissionto the WMT 2013 metrics evaluationtask, MEANT and UMEANT.
MEANTis optimized by tuning a small numberof weights?one for each semantic rolelabel?so as to maximize correlation withhuman adequacy judgment on a devel-opment set.
UMEANT is an unsuper-vised version where weights for each se-mantic role label are estimated via an in-expensive unsupervised approach, as op-posed to MEANT?s supervised method re-lying on more expensive grid search.
Inthis paper, we present a battery of exper-iments for optimizing MEANT on differ-ent development sets to determine the setof weights that maximize MEANT?s accu-racy and stability.
Evaluated on test setsfrom the WMT 2012/2011 metrics evalua-tion, bothMEANT and UMEANT achievecompetitive correlations with human judg-ments using nothing more than a monolin-gual corpus and an automatic shallow se-mantic parser.1 IntroductionWe evaluate in the context of WMT 2013 theMEANT (Lo et al 2012) and UMEANT (LoandWu, 2012) semantic machine translation (MT)evaluation metrics?tunable, simple yet highly ef-fective, fully-automatic semantic frame based ob-jective functions that score the degree of similaritybetween the MT output and the reference transla-tions via semantic role labels (SRL).
Recent stud-ies (Lo et al 2013; Lo and Wu, 2013) show thattuningMT systems againstMEANTmore robustlyimproves translation adequacy, compared to tun-ing against BLEU or TER.In the past decade, the progress of machinetranslation (MT) research is predominantly drivenby the fast and cheap n-gram based MT eval-uation metrics, such as BLEU (Papineni et al2002), which assume that a good translation is onethat shares the same lexical choices as the ref-erence translation.
Despite enforcing fluency, ithas been established that these metrics do not en-force translation utility adequately and often fail topreserve meaning closely (Callison-Burch et al2006; Koehn and Monz, 2006).
Unlike BLEU,or other n-gram based MT evaluation metrics,MEANT adopts at outset the principle that a goodtranslation is one from which the human readersmay successfully understand at least the centralmeaning of the input sentence as captured by thebasic event structure?
?who did what to whom,when, where and why?
(Pradhan et al 2004).Lo et al(2012) show that MEANT correlatesbetter with human adequacy judgment than othercommonly used automatic MT evaluation metrics,such as BLEU (Papineni et al 2002), NIST (Dod-dington, 2002), METEOR (Banerjee and Lavie,2005), CDER (Leusch et al 2006), WER (Nie?enet al 2000), and TER (Snover et al 2006).
Re-cent studies (Lo et al 2013; Lo andWu, 2013) alsoshow that tuning MT system against MEANT pro-duces more robustly adequate translations on bothformal news text genre and informal web forumor public speech genre compared to tuning againstBLEU or TER.
These studies show thatMEANT isa tunable and highly-accurate MT evaluation met-ric that drives MT system development towardshigher utility.As described in Lo and Wu (2011a), the pa-422rameters in MEANT, i.e.
the weight for each se-mantic role label, could be estimated using simplegrid search to optimize the correlation with humanadequacy judgments.
Later, Lo and Wu (2012)described an unsupervised approach for estimat-ing the parameters of MEANT using relative fre-quency of each semantic role label in the referencetranslations under the situation when the humanjudgments for the development set are unavailable.In this paper, we refer the version of MEANT us-ing the unsupervised approach of weight estima-tion as UMEANT.In this paper, we present a battery of exper-iments for optimizing MEANT on different de-velopment sets to determine the set of weightsthat maximizes MEANT?s accuracy and stability.Evaluated on the test sets ofWMT 2012/2011 met-rics evaluation, MEANT and UMEANT achievea competitive correlation score with human judg-ments by nothing more than a monolingual corpusand an automatic shallow semantic parser.2 Related work2.1 Lexical similarity based metricsN-gram or edit distance based metrics such asBLEU (Papineni et al 2002), NIST (Dodding-ton, 2002), METEOR (Banerjee and Lavie, 2005),CDER (Leusch et al 2006), WER (Nie?en etal., 2000), and TER (Snover et al 2006) do notcorrectly reflect the similarity of the basic eventstructure?
?who did what to whom, when, whereand why??
of the input sentence.
In fact, anumber of large scale meta-evaluations (Callison-Burch et al 2006; Koehn and Monz, 2006) reportcases where BLEU strongly disagrees with humanjudgments of translation adequacy.Although AMBER (Chen et al 2012) shows ahigh correlation with human adequacy judgment(Callison-Burch et al 2012) and claims to pre-serve the simplicity of BLEU, the modifications itincurred on BLEU through four different n-grammatching strategies and several different penaltiesmakes it very hard to interpret and indicate whaterrors the MT systems are making.2.2 Linguistic feature based metricsULC (Gim?nez and M?rquez, 2007, 2008) isan automatic metric that incorporates several se-mantic similarity features and shows improvedcorrelation with human judgement of translationquality (Callison-Burch et al 2007; Gim?nezand M?rquez, 2007; Callison-Burch et al 2008;Gim?nez and M?rquez, 2008) but no work hasbeen done towards tuning an SMT system using apure form of ULC perhaps due to its expensive runtime.
Lambert et al(2006) did tune on QUEEN,a simplified version of ULC that discards the se-mantic features of ULC and is based on pure lexi-cal similarity.
Therefore, QUEEN suffers from theproblem of failing to reflect translation adequacysimilar to other n-gram based metrics.Similarly, SPEDE (Wang andManning, 2012) isan integrated probabilistic FSM and probabilisticPDA model that predicts the edit sequence neededfor the MT output to match the reference.
Sagan(Castillo and Estrella, 2012) is a semantic textualsimilarity metric based on a complex textual en-tailment pipeline.
These aggregated metrics re-quire sophisticated feature extraction steps; con-tain several dozens of parameters to tune and em-ploy expensive linguistic resources, like WordNetand paraphrase table.
Like ULC, these matricesare not useful in the MT system development cy-cle for tuning due to expensive running time.
Themetrics themselves are also expensive in trainingand tuning due to the large number of parametersto be estimated.
Although ROSE (Song and Cohn,2011) is a weighted linear model of shallow lin-guistic features which is cheaper in run time but itstill contains several dozens of weights that need tobe tuned which affects the portability of the metricfor evaluating translations across domains.Rios et al(2011) introduced TINE, an auto-matic recall-oriented evaluationmetric which aimsto preserve the basic event structure, but no workhas been done toward tuning an SMT systemagainst it.
TINE performs comparably to BLEUand worse than METEOR on correlation with hu-man adequacy judgment.3 MEANT and UMEANTMEANT (Lo et al 2012), which is the weightedf-measure over the matched semantic role labelsof the automatically aligned semantic frames androle fillers, outperforms BLEU, NIST, METEOR,WER, CDER and TER.
Recent studies (Lo et al2013; Lo andWu, 2013) also show that tuning MTsystem against MEANT produces more robustlyadequate translations than the common practice oftuning against BLEU or TER across different datagenres, such as formal newswire text, informalweb forum text and informal public speech.
Pre-423Figure 1: Examples of automatic shallow semantic parses.
The input is parsed by a Chinese automaticshallow semantic parser.
The reference and MT output are parsed by an English automatic shallow se-mantic parser.
There are no semantic frames for MT3 since there is no predicate.cisely, MEANT is computed as follows:1.
Apply an automatic shallow semantic parseron both the references and MT output.
(Fig-ure 1 shows examples of automatic shallowsemantic parses on both reference and MToutput.)2.
Applymaximumweighted bipartite matchingalgorithm to align the semantic frames be-tween the references and MT output by thelexical similarity of the predicates.3.
For each pair of aligned semantic frames,(a) Lexical similarity scores determine thesimilarity of the semantic role fillers.
(b) Apply maximum weighted bipartitematching algorithm to align the seman-tic role fillers between the reference andMT output according to their lexicalsimilarity.4.
Compute the weighted f-measure over thematching role labels of these aligned predi-cates and role fillers.Mi,j ?
total # ARG j of aligned frame i in MTRi,j ?
total # ARG j of aligned frame i in REFSi,pred ?
similarity of predicate in aligned frame iSi,j ?
similarity of ARG j in aligned frame iwpred ?
weight of similarity of predicateswj ?
weight of similarity of ARG jmi ?
#tokens filled in aligned frame i of MTtotal #tokens in MTri ?
#tokens filled in aligned frame i of REFtotal #tokens in REFprecision =?imiwpredSi,pred+?j wjSi,jwpred+?j wjMi,j?imirecall =?i riwpredSi,pred+?j wjSi,jwpred+?j wjRi,j?i riwheremi and ri are the weights for frame, i, in theMT/REF respectively.
These weights estimate thedegree of contribution of each frame to the overallmeaning of the sentence.
Mi,j and Ri,j are the to-tal counts of argument of type j in frame i in theMT and REF respectively.
Si,pred and Si,j are thelexical similarities of the predicates and role fillersof the arguments of type j between the referencetranslations and the MT output.
wpred and wj arethe weights of the lexical similarities of the predi-cates and role fillers of the arguments of typej be-tween the reference translations and the MT out-put.
There are in total 12 weights for the set of424semantic role labels in MEANT as defined in Loand Wu (2011b).For MEANT, wpred and wj are determined us-ing supervised estimation via a simple grid searchto optimize the correlation with human adequacyjudgments (Lo and Wu, 2011a).
For UMEANT,wpred and wj are estimated in an unsupervisedmanner using relative frequency of each semanticrole label in the reference translations when the hu-man judgments on adequacy of the developmentset were unavailable (Lo and Wu, 2012).In this experiment, we use a MEANT /UMEANT implementation along the lines de-scribed in Lo et al(2012) and Tumuluru et al(2012) but we incorporate a variant of the aggre-gation function proposed in Mihalcea et al(2006)for phrasal similarity of role fillers as it normal-izes the phrase length better than geometric meanas described in Tumuluru et al(2012).
In casethere is no semantic frame in the sentence, we treatthe whole sentence as a phrase and calculate thephrasal similarity, like the role fillers in step 3.1,as the MEANT score.4 Experimental setupWe tune the 12 weights for the set of semantic rolelabels in MEANT using grid search to maximizethe correlationwith human judgment on 6 develop-ment sets.
Following the protocol inWMT12 met-rics evaluation task (Callison-Burch et al 2012),we use Kendall?s correlation coefficient for thesentence-level correlation with human judgments.The GALE development set consists of 40 sen-tences randomly drawn from the DARPA GALEP2.5 Chinese-English evaluation set alg with theoutputs from 3 participating MT systems and thecorresponding human adequacy judgments.
TheWMT12-A development set consists of 800 sen-tences randomly drawn from the Czech-Englishtest set in WMT12 metrics evaluation task alongwith the output from 5 participating systems andthe corresponding human judgments.
Similarly,each of theWMT12-B,WMT12-C andWMT12-Ddevelopment sets consists of 800 randomly drawnsentences from the WMT12 metrics evaluationtest set on German-English, Spanish-English andFrench-English respectively.
The WMT12-E de-velopment set consists of 800 sentences out ofwhich 200 sentences were randomly drawn fromeach of WMT12-A, WMT12-B, WMT12-C andWMT12-D data set.We evaluated MEANT and UMEANT on 3groups of test sets.
The first group is the original(without partition) test data for each language pair(translated in English) in WMT12.
This group oftest sets is used for comparing MEANT?s perfor-mance with the reported results from other partic-ipants of WMT12.
The second group is the heldout subset of the test data for each language pair inWMT12.
The third group is the original set of testdata for each language pair in WMT11.
The lat-ter 2 groups are used for determining which set oftuned weights maximize the accuracy and stabilityof MEANT.5 ResultsTable 1 shows that the best and the worst sentence-level correlations reported in Callison-Burch et al(2012) on the original WMT12 test sets (withoutpartitioning) for translations into English, togetherthe sentence-level correlation of MEANT tunedon different development sets and UMEANT.
Thegrey boxes mark the results of experiments inwhich there was an overlap between parts of thedevelopment data and the test data.
A study of thevalues for the 12 weights associated with the se-mantic role labels show that a general trend of theimportance of different labels in MEANT: ?who?is always the most important; ?did?, ?what?,?where?, ?why?, ?extent?, ?modal?
and ?other?are quite important too; ?when?, ?manner?
and?negation?
fluctuate where they are quite impor-tant in some development sets but not quite im-portant in some development sets; ?whom?
is usu-ally not important.
Given the fact that MEANTemploys significantly less expensive linguistic re-sources and less sophisticated machine learning al-gorithm in tuning the parameters, the performanceof MEANT is very competitive with other partici-pants last year.Table 2 shows the sentence-level correlation onthe WMT12 held-out test sets and the originalWMT11 test sets of MEANT tuned on differentdevelopment sets and UMEANT together with theaverage sentence-level correlation on all test sets.The results show that MEANT tuning onWMT12-C development set achieve the highest sentence-level correlation with human judgments on aver-age.
UMEANT, the unsupervised wight estimatedversion of MEANT, achieves a very competitivecorrelation score when compared with MEANTtuned on different development sets.
As a result,425Table 1: The best and the worst sentence-level correlation reported in Callison-Burch et al(2012) on theoriginal WMT12 test sets (without partitioning) for translations into English together the sentence-levelcorrelation of MEANT tuned on different development sets and UMEANT.
The grey box marked resultsof experiments in which parts of the development data and the test data are overlapped.WMT12 cz-en WMT12 de-en WMT12 es-en WMT12 fr-enBest reported 0.21 0.28 0.26 0.26MEANT (GALE) 0.13 0.16 0.15 0.15MEANT (WMT12-A) 0.12 0.17 0.16 0.15MEANT (WMT12-B) 0.11 0.18 0.15 0.14MEANT (WMT12-C) 0.12 0.17 0.17 0.15MEANT (WMT12-D) 0.12 0.17 0.16 0.16MEANT (WMT12-E) 0.12 0.17 0.17 0.15UMEANT 0.12 0.17 0.16 0.14Worst reported 0.06 0.08 0.08 0.07Table 2: Sentence-level correlation on the WMT12 held-out test sets and the original WMT11 test setsof MEANT tuned on different development sets and UMEANT together with the average sentence-levelcorrelation on all test sets.WMT12 held-out WMT11 Averagecz-en de-en es-en fr-en cz-en de-en es-en fr-en -MEANT (GALE) 0.0657 0.1251 0.1762 0.1719 0.3460 0.1123 0.2416 0.1913 0.1788MEANT (WMT12-A) 0.0652 0.1117 0.1663 0.1540 0.3764 0.1101 0.2314 0.1944 0.1762MEANT (WMT12-B) 0.0458 0.1294 0.1556 0.1548 0.3992 0.1479 0.2571 0.2037 0.1867MEANT (WMT12-C) 0.0746 0.1278 0.1833 0.1592 0.3764 0.1324 0.2674 0.1882 0.1887MEANT (WMT12-D) 0.0628 0.1164 0.1826 0.1655 0.3802 0.1168 0.2339 0.1975 0.1820MEANT (WMT12-E) 0.0496 0.1353 0.1791 0.1619 0.3840 0.1101 0.2596 0.1851 0.1831UMEANT 0.0477 0.1333 0.1606 0.1548 0.3764 0.1257 0.2828 0.1913 0.1841we submitted two metrics to WMT 2013 metricsevaluation task.
One is MEANT with weightslearned from tuning on WMT12-C developmentsets and the other submission is UMEANT.6 ConclusionIn this paper, we have evaluated in the context ofWMT2013 the MEANT and UMEANT metrics,which are tunable, accurate yet inexpensive fullyautomatic machine translation evaluation metricsthat measure similarity between theMT output andthe reference via semantic frames.
Recent stud-ies show that tuning MT system against MEANTproduces more robustly adequate translations thanthe common practice of tuning against BLEU orTER across different data genres, such as formalnewswire text, informal web forum text and infor-mal public speech.
The weight for each seman-tic role label in MEANT is estimated by maximiz-ing the correlation with human adequacy judgmenton a development set.
UMEANT is a version ofMEANT in which weight for each semantic rolelabel is estimated in an unsupervised fashion us-ing the relative frequency of the semantic role la-bels in the reference.
We present the experimen-tal results for determining the set of weights thatmaximize MEANT?s accuracy and stability by op-timizing MEANT on different development sets.We disagree with the notion ?a good evaluationmetric is not necessarily a good tuning metric, andvice versa?
(Chen et al 2012).
Instead, we be-lieve that a good evaluation metric should be onethat is a good objective function to drive the devel-opment of MT systems towards higher utility.
Inother words, a good evaluation metric should cor-relate well with human adequacy judgment and atthe same time, be inexpensive in running time so asto fit into the MT pipeline to improve MT quality.Our results shows that MEANT is a good evalu-ation/tuning metric because it achieves a competi-tive correlation scorewith human judgments by us-ing less expensive linguistic resources and trainingalgorithms making it possible to tune MT systemagainst MEANT to improve MT quality.7 AcknowledgmentThis material is based upon work supported inpart by the Defense Advanced Research ProjectsAgency (DARPA) under BOLT contract no.HR0011-12-C-0016, and GALE contract nos.HR0011-06-C-0022 and HR0011-06-C-0023; bythe European Union under the FP7 grant agree-426ment no.
287658; and by the Hong Kong Re-search Grants Council (RGC) research grantsGRF620811, GRF621008, and GRF612806.
Anyopinions, findings and conclusions or recommen-dations expressed in this material are those of theauthors and do not necessarily reflect the views ofDARPA, the EU, or RGC.ReferencesSatanjeev Banerjee and Alon Lavie.
METEOR:An automatic metric forMT evaluation with im-proved correlation with human judgments.
InProceedings of the ACL Workshop on Intrinsicand Extrinsic Evaluation Measures for MachineTranslation and/or Summarization, pages 65?72, Ann Arbor, Michigan, June 2005.Chris Callison-Burch, Miles Osborne, and PhilippKoehn.
Re-evaluating the role of BLEU in Ma-chine Translation Research.
In Proceedings ofthe 13th Conference of the European Chapterof the Association for Computational Linguis-tics (EACL-06), pages 249?256, 2006.Chris Callison-Burch, Cameron Fordyce, PhilippKoehn, Christof Monz, and Josh Schroeder.
(Meta-) evaluation of Machine Translation.
InProceedings of the 2nd Workshop on StatisticalMachine Translation, pages 136?158, 2007.Chris Callison-Burch, Cameron Fordyce, PhilippKoehn, Christof Monz, and Josh Schroeder.Further Meta-evaluation of Machine Transla-tion.
In Proceedings of the 3rd Workshop onStatistical Machine Translation, pages 70?106,2008.Chris Callison-Burch, Philipp Koehn, ChristofMonz, Matt Post, Radu Soricut, and Lucia Spe-cia.
Findings of the 2012 Workshop on Statisti-cal Machine Translation.
In Proceedings of the7th Workshop on Statistical Machine Transla-tion (WMT 2012), pages 10?51, 2012.Julio Castillo and Paula Estrella.
Semantic TextualSimilarity for MT evaluation.
In Proceedings ofthe 7th Workshop on Statistical Machine Trans-lation (WMT 2012), pages 52?58, 2012.Boxing Chen, Roland Kuhn, and George Foster.Improving AMBER, an MT Evaluation Metric.In Proceedings of the 7th Workshop on Statis-tical Machine Translation (WMT 2012), pages59?63, 2012.George Doddington.
Automatic evaluation ofmachine translation quality using n-gram co-occurrence statistics.
In Proceedings of the2nd International Conference on Human Lan-guage Technology Research, pages 138?145,San Diego, California, 2002.Jes?s Gim?nez and Llu?s M?rquez.
Linguisticfeatures for automatic evaluation of heteroge-nous MT systems.
In Proceedings of the Sec-ond Workshop on Statistical Machine Transla-tion, pages 256?264, Prague, Czech Republic,June 2007.Jes?s Gim?nez and Llu?sM?rquez.
A smorgasbordof features for automaticMT evaluation.
In Pro-ceedings of the Third Workshop on StatisticalMachine Translation, pages 195?198, Colum-bus, Ohio, June 2008.Philipp Koehn and Christof Monz.
Manual andAutomatic Evaluation of Machine Translationbetween European Languages.
In Proceedingsof the Workshop on Statistical Machine Trans-lation (WMT-06), pages 102?121, 2006.Patrik Lambert, Jes?s Gim?nez, Marta R Costa-juss?, Enrique Amig?, Rafael E Banchs, Llu?sM?rquez, and JAR Fonollosa.
Machine Transla-tion system development based on human like-ness.
In Spoken Language Technology Work-shop, 2006.
IEEE, pages 246?249.
IEEE, 2006.Gregor Leusch, Nicola Ueffing, and HermannNey.
CDer: Efficient MT Evaluation UsingBlock Movements.
In Proceedings of the 13thConference of the European Chapter of the As-sociation for Computational Linguistics (EACL-06), 2006.Chi-kiu Lo and Dekai Wu.
MEANT: An Inexpen-sive, High-Accuracy, Semi-Automatic Metricfor Evaluating Translation Utility based on Se-mantic Roles.
In Proceedings of the Joint con-ference of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics : HumanLanguage Technologies (ACL-HLT-11), 2011.Chi-kiu Lo and Dekai Wu.
SMT vs. AI redux:How semantic frames evaluate MT more ac-curately.
In Proceedings of the 22nd Inter-national Joint Conference on Artificial Intelli-gence (IJCAI-11), 2011.Chi-kiu Lo and Dekai Wu.
Unsupervised vs.supervised weight estimation for semantic MTevaluation metrics.
In Proceedings of the 6thWorkshop on Syntax and Structure in StatisticalTranslation (SSST-6), 2012.427Chi-kiu Lo and Dekai Wu.
Can informal genres bebetter translated by tuning on automatic seman-tic metrics?
In Proceedings of the 14th MachineTranslation Summit (MTSummit-XIV), 2013.Chi-kiu Lo, Anand Karthik Tumuluru, and DekaiWu.
Fully Automatic Semantic MT Evaluation.In Proceedings of the Seventh Workshop on Sta-tistical Machine Translation (WMT2012), 2012.Chi-kiu Lo, Karteek Addanki, Markus Saers, andDekai Wu.
Improving machine translation bytraining against an automatic semantic framebased evaluation metric.
In Proceedings ofthe 51st Annual Meeting of the Association forComputational Linguistics (ACL-13), 2013.Rada Mihalcea, Courtney Corley, and Carlo Strap-parava.
Corpus-based and knowledge-basedmeasures of text semantic similarity.
In Pro-ceedings of the national conference on artificialintelligence, volume 21, page 775.
Menlo Park,CA; Cambridge, MA; London; AAAI Press;MIT Press; 1999, 2006.Sonja Nie?en, Franz Josef Och, Gregor Leusch,and Hermann Ney.
A Evaluation Tool for Ma-chine Translation: Fast Evaluation for MT Re-search.
In Proceedings of the 2nd InternationalConference on Language Resources and Evalu-ation (LREC-2000), 2000.Kishore Papineni, Salim Roukos, Todd Ward, andWei-Jing Zhu.
BLEU: a method for automaticevaluation of machine translation.
In Proceed-ings of the 40th Annual Meeting of the Associa-tion for Computational Linguistics, pages 311?318, Philadelphia, Pennsylvania, July 2002.Sameer Pradhan, Wayne Ward, Kadri Hacioglu,James H.Martin, and Dan Jurafsky.
Shallow Se-mantic Parsing Using Support Vector Machines.In Proceedings of the 2004 Conference on Hu-man Language Technology and the North Amer-ican Chapter of the Association for Computa-tional Linguistics (HLT-NAACL-04), 2004.Miguel Rios, Wilker Aziz, and Lucia Specia.
Tine:A metric to assess MT adequacy.
In Proceed-ings of the Sixth Workshop on Statistical Ma-chine Translation (WMT-2011), pages 116?122,2011.Matthew Snover, Bonnie Dorr, Richard Schwartz,Linnea Micciulla, and John Makhoul.
A studyof translation edit rate with targeted human an-notation.
In Proceedings of the 7th Conferenceof the Association for Machine Translation inthe Americas (AMTA-06), pages 223?231, Cam-bridge, Massachusetts, August 2006.Xingyi Song and Trevor Cohn.
Regression andRanking based Optimisation for Sentence LevelMachine Translation Evaluation.
In Proceed-ings of the 6th Workshop on Statistical MachineTranslation (WMT 2011), pages 123?129, 2011.Anand Karthik Tumuluru, Chi-kiu Lo, and DekaiWu.
Accuracy and robustness in measuringthe lexical similarity of semantic role fillersfor automatic semantic mt evaluation.
In Pro-ceeding of the 26th Pacific Asia Conferenceon Language, Information, and Computation(PACLIC-26), 2012.Mengqiu Wang and Christopher D. Manning.SPEDE: Probabilistic Edit Distance Metrics forMT Evaluation.
In Proceedings of the 7th Work-shop on Statistical Machine Translation (WMT2012), pages 76?83, 2012.428
