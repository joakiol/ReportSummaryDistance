Categorizing Unknown Words: Using Decision Trees to IdentifyNames and MisspellingsJ an ine  Too leNatura l  Language LaboratoryDepar tment  of Comput ing  ScienceS imon Fraser Univers i tyBurnaby,  BC,  Canada  VSA IS6toole@cs.sfu.caAbst rac tThis paper introduces a system for categorizing un-known words.
The system is based on a multi-component architecture where each component is re-sponsible for identifying one class of unknown words.The focus of this paper is the components hat iden-tify names and spelling errors.
Each componentuses a decision tree architecture to combine multipletypes of evidence about the unknown word.
The sys-tem is evaluated using data from live closed captions- a genre replete with a wide variety of unknownwords.1 In t roduct ionIn any real world use, a Natural Language Process-ing (NLP) system will encounter words that are notin its lexicon, what we term 'unknown words'.
Un-known words are problematic because a NLP systemwill perform well only if it recognizes the words thatit is meant o analyze or translate: the more words asystem does not recognize the more the system's per-formance will degrade.
Even when unknown wordsare infrequent, they can have a disproportionate ef-fect on system quality.
For example, Min (1996)found that while only 0.6% of words in 300 e-mailswere misspelled, this meant that 12% of the sen-tences contained an error (discussed in (Min andWilson, 1998)).Words may be unknown for many reasons: theword may be a proper name, a misspelling, an ab-breviation, a number, a morphological variant of aknown word (e.g.
recleared), or missing from thedictionary.
The first step in dealing with unknownwords is to identify the class of the unknown word;whether it is a misspelling, a proper name, an ab-breviation etc.
Once this is known, the proper ac-tion can be taken, misspellings can be corrected, ab-breviations can be expanded and so on, as deemednecessary by the particular text processing applica-tion.
In this paper we introduce a system for cat-egorizing unknown words.
The system is based ona multi- component architecture where each compo-nent is responsible for identifying one category ofunknown words.
The main focus of this paper is thecomponents hat identify names and spelling errors.Both components use a decision tree architecture tocombine multiple types of evidence about the un-known word.
Results from the two components arecombined using a weighted voting procedure.
Thesystem is evaluated using data from live closed cap-tions - a genre replete with a wide variety of un-known words.This paper is organized as follows.
In section 2we outline the overall architecture of the unknownword categorizer.
The name identifier and the mis-spelling identifier are introduced in section 3.
Perfor-mance and evaluation issues are discussed in section4.
Section 5 considers portability issues.
Section 6compares the current system with relevant preced-ing research.
Concluding comments can be found insection 6.2 Sys tem Arch i tec tureThe goal of our research is to develop a system thatautomatically categorizes unknown words.
Accord-ing to our definition, an unknown word is a wordthat is not contained in the lexicon of an NLP sys-tem.
As defined, 'unknown-ness' i  a relative con-cept: a word that is known to one system may beunknown to another system.Our research is motivated by the problems thatwe have experienced in translating live closed cap-tions: live captions are produced under tight timeconstraints and contain many unknown words.
Typ-ically, the caption transcriber has a five second win-dow to transcribe the broadcast dialogue.
Becauseof the live nature of the broadcast, there is no op-portunity to post-edit he transcript in any way.
Al-though motivated by our specific requirements, theunknown word categorizer would benefit any NLPsystem that encounters unknown words of differ-ing categories.
Some immediately obvious domainswhere unknown words are frequent include e-mailmessages, internet chat rooms, data typed in by callcentre operators, etc.To deal with these issues we propose a multi-component architecture where individual compo-nents specialize in identifying one particular type of173unknown word.
For example, the misspelling iden-tifier will specialize in identifying misspellings, theabbreviation component will specialize in identify-ing abbreviations, etc.
Each component will returna confidence measure of the reliability of its predic-tion, c.f.
(Elworthy, 1998).
The results from eachcomponent are evaluated to determine the final cat-egory of the word.There are several advantages to this approach.Firstly, the system can take advantage of existingresearch.
For example, the name recognition mod-ule can make use of the considerable research thatexists on name recognition, e.g.
(McDonald, 1996),(Mani et al, 1996).
Secondly, individual compo-nents can be replaced when improved models areavailable, without affecting other parts of the sys-tem.
Thirdly, this approach is compatible with in-corporating multiple components ofthe same type toimprove performance (cf.
(van Halteren et al, 1998)who found that combining the results of several partof speech taggers increased performance).3 The  Cur rent  Sys temIn this paper we introduce a simplified versionof the unknown word categorizer: one that con-tains just two components: misspelling identifica-tion and name identification.
In this section we in-troduce these components and the 'decision: compo-nent which combines the results from the individualmodules.3.1 The Name Identif ierThe goal of the name identifier is to differentiate be-tween those unknown words which are proper names,and those which are not.
We define a name as wordidentifying a person, place, or concept hat wouldtypically require capitalization i English.One of the motivations for the modular architec-ture introduced above, was to be able to leverageexisting research.
For example, ideally, we shouldbe able to plug in an existing proper name recog-nizer and avoid the problem of creating our own.However, the domain in which we are currently op-erating - live closed captions - makes this approachdifficult.
Closed captions do not contain any caseinformation, all captions are in upper case.
Exist-ing proper name recognizers rely heavily on case toidentify names, hence they perform poorly on ourdata.A second isadvantage of currently available namerecognizers i  that they do not generally return aconfidence measure with their prediction.
Someindication of confidence is required in the multi-component architecture we have implemented.
How-ever, while currently existing name recognizers areinappropriate for the needs of our domain, futurename recognizers may well meet these requirementsand be able to be incorporated into the architecturewe propose.For these reasons we develop our own name iden-tifier.
We utilize a decision tree to model the charac-teristics of proper names.
The advantage ofdecisiontrees is that they are highly explainable: one canreadily understand the features that are affectingthe analysis (Weiss and Indurkhya, 1998).
Further-more, decision trees are well-suited for combining awide variety of information.For this project, we made use of the decision treethat is part of IBM's Intelligent Miner suite for datamining.
Since the point of this paper is to describean application of decision trees rather than to ar-gue for a particular decision tree algorithm, we omitfurther details of the decision tree software.
Sim-ilar results should be obtained by using other de-cision tree software.
Indeed, the results we obtaincould perhaps be improved by using more sophisti-cated decision-tree approaches such as the adaptive-resampling described in (Weiss et al 1999).The features that we use to train the decision treeare intended to capture the characteristics of names.We specify a total of ten features for each unknownword.
These identify two features of the unknownword itself as well as two features for each of the twopreceding and two following words.The first feature represents the part of speech ofthe word.
Vv'e use an in-house statistical tagger(based on (Church, 1988)) to tag the text in whichthe unknown word occurs.
The tag set used is asimplified version of the tags used in the machine-readable version of the Oxford Advanced LearnersDictionary (OALD).
The tag set contains just onetag to identify nouns.The second feature provides more informative tag-ging for specific parts of speech (these are referredto as 'detailed tags' (DETAG)).
This tagset consistsof the nine tags listed in Table 1.
All parts of speechapart from noun and punctuation tags are assignedthe tag 'OTHER;.
All punctuation tags are assignedthe tag 'BOUNDARY'.
Words identified as nounsare assigned one of the remaining tags depending onthe information provided in the OALD (although theunknown word, by definition, will not appear in theOALD, the preceding and following words may wellappear in the dictionary).
If the word is identified inthe OALD as a common oun it is assigned the tag'COM'.
If it is identified in the OALD as a propername it is assigned the tag 'NAME'.
If the word isspecified as both a name and a common oun (e.g.
'bilF), then it is assigned the tag 'NCOM'.
Pronounsare assigned the tag 'PRON'.
If the word is in a listof titles that we have compiled, then the tag 'TITLE'is assigned.
Similarly, if the word is a member of theclass of words that can follow a name (e.g.
'jr'), thenthe tag 'POST ~ is assigned.
A simple rule-based sys-174COM common ounNAME nameNCOM name and common ounPRONOUN pronounTITLE titlePOST post-name wordBOUNDARY boundary markerOTHER not noun or boundaryUNKNOWN unknown nounTable 1: List of Detailed Tags (DETAG)Corpus frequencyWord lengthEdit distanceIspell informationCharacter sequence frequencyNon-English charactersTable 2: Features used in misspelling decision treetern is used to assign these tags.If we were dealing with data that contains caseinformation, we would also include fields represent-ing the existence/non-existence of initial upper casefor the five words.
However, since our current datadoes not include case information we do not includethese features.3.2 The Misspel l ing Identi f ierThe goal of the misspelling identifier is to differenti-ate between those unknown words which are spellingerrors and those which are not.
We define a mis-spelling as an unintended, orthographically incorrectrepresentation (with respect o the NLP system) of aword.
A misspelling differs from the intended knownword through one or more additions, deletions, sub-stitutions, or reversals of letters, or the exclusion ofpunctuation such as hyphenation or spacing.
Likethe definition of 'unknown word', the definition of amisspelling is also relative to a particular NLP sys-tem.Like the name identifier, we make use of a decisiontree to capture the characteristics of misspellings.The features we use are derived from previous re-search, including our own previous research on mis-spelling identification.
An abridged list of the fea-tures that are used in the training data is listed inTable 2 and discussed below.Corpus frequency: (Vosse, 1992) differentiatesbetween misspellings and neologisms (new words)in terms of their frequency.
His algorithm classi-fies unknown words that appear infrequently as mis-spellings, and those that appear more frequently asneologisms.
Our corpus frequency variable specifiesthe frequency of each unknown word in a 2.6 millionword corpus of business news closed captions.I~'ord Length: (Agirre et al, 1998) note thattheir predictions for the correct spelling of mis-spelled words are more accurate for words longerthan four characters, and much less accurate forshorter words.
This observation can also be found in(Kukich, 1992).
Our word length variables measuresthe number of characters in each word.Edit distance: Edit-distance isa metric for iden-tifying the orthographic similarity of two words.Typically, one edit-distance corresponds to one sub-stitution, deletion, reversal or addition of a charac-ter.
(Damerau, 1964) observed that 80% of spellingerrors in his data were just one edit-distance fromthe intended word.
Similarly, (Mitton, 1987) foundthat 70% of his data was within one edit-distancefrom the intended word.
Our edit distance featurerepresents the edit distance from the unknown wordto the closest suggestion produced by the unix spellchecker, ispell.
If ispell does not produce any sugges-tions, an edit distance of thirty is assigned.
In pre-vious work we have experimented with more sophis-ticated distance measures.
However, simple edit dis-tance proved to be the most effective (Toole, 1999).Character sequence frequency: A characteris-tic of some misspellings i that they contain charac-ter sequences which are not typical of the language,e.g.tlted, wful.
Exploiting this information is a stan-dard way of identifying spelling errors when using adictionary is not desired or appropriate, e.g.
(Hulland Srihari, 1982), (Zamora et al, 1981).To calculate our character sequence feature, wefirstly determine the frequencies of the two least fre-quent character tri-gram sequences in the word ineach of a selection of corpora.
In previous work weincluded each of these values as individual features.However, the resulting trees were quite unstable asone feature would be relevant o one tree, whereasa different character sequence feature would be rel-evant to another tree.
To avoid this problem, wedeveloped a composite feature that is the sum of allindividual character sequence frequencies.Non-English characters: This binary featurespecifies whether a word contains a character that isnot typical of English words, such as accented char-acters, etc.
Such characters are indicative of foreignnames or transmission noise (in the case of captions)rather than misspellings.3.3 Decision Making ComponentThe misspelling identifier and the name identifierwill each return a prediction for an unknown word.In cases where the predictions are compatible, e.g.where the name identifier predicts that it is a nameand the spelling identifier predicts that it is nota misspelling, then the decision is straightforward.Similarly, if both decision trees make negative pre-dictions, then we can assume that the unknown word175is neither a misspelling nor a name, but some othercategory of unknown word.However, it is also possible that both the spellingidentifier and the name identifier will make positivepredictions.
In these cases we need a mechanismto decide which assignment is upheld.
For the pur-poses of this paper, we make use of a simple heuris-tic where in the case of two positive predictions theone with the highest confidence measure is accepted.The decision trees return a confidence measure foreach leaf of the tree.
The confidence measure for aparticular leaf is calculated from the training dataand corresponds to the proportion of correct predic-tions over the total number of predictions at thisleaf.4 Eva luat ionIn this section we evaluate the unknown word cat-egorizer introduced above.
We begin by describingthe training and test data.
Following this, we eval-uate the individual components and finally, we eval-uate the decision making component.The training and test data for the decision treeconsists of 7000 cases of unknown words extractedfrom a 2.6 million word corpus of live business newscaptions.
Of the 7000 cases, 70.4% were manuallyidentified as names and 21.3% were identified as mis-spellings.The remaining cases were other types ofunknown words such as abbreviations, morphologi-cal variants, etc.
Seventy percent of the data wasrandomly selected to serve as the training corpus.The remaining thirty percent, or 2100 records, wasreserved as the test corpus.
The test data consists often samples of 2100 records selected randomly withreplacement from the test corpus.We now consider the results of training a decisiontree to identify misspellings using those features weintroduced in the section on the misspelling identi-fier.
The tree was trained on the training data de-scribed above.
The tree was evaluated using each ofthe ten test data sets.
The average precision andrecall data for the ten test sets are given in Ta-ble 3, together with the base-line case of assumingthat we categorize all unknown words as names (themost common category).
With the baseline case weachieve 70.4% precision but with 0% recall.
In con-trast, the decision tree approach obtains 77.1% pre-cision and 73.8% recall.We also trained a decision tree using not only thefeatures identified in our discussion on misspellingsbut also those features that we introduced in ourdiscussion of name identification.
The results forthis tree can be found in the second line of Table3.
The inclusion of the additional features has in-creased precision by approximately 5%.
However, ithas also decreased recall by about the same amount.The overall F-score is quite similar.
It appears thatthe name features are not predictive for identifyingmisspellings in this domain.
This is not surprisingconsidering that eight of the ten features specifiedfor name identification are concerned with featuresof the two preceding and two following words.
Suchword-external information is of little use in identify-ing a misspelling.An analysis of the cases where the misspelling de-cision tree failed to identify a misspelling revealedtwo major classes of omissions.
The first class con-tains a collection of words which have typical char-acteristics of English words, but differ from the in-tended word by the addition or deletion of a syllable.Words in this class include creditability for credi-bility, coordmatored for coordinated, and represen-tires for representatives.
The second class containsmisspellings that differ from known words by thedeletion of a blank.
Examples in this class includewebpage, crewmembers, and rainshower.
The secondclass of misspellings can be addressed by adding afeature that specifies whether the unknown word canbe split up into two component known words.
Sucha feature should provide strong predictability for thesecond class of words.
The first class of words aremore of a challenge.
These words have a close ho-mophonic relationship with the intended word ratherthan a close homographic relationship (as capturedby edit distance).
Perhaps this class of words wouldbenefit from a feature representing phonetic distancerather than edit distance.Among those words which were incorrectly iden-tified as misspellings, it is also possible to identifycommon causes for the misidentification.
Amongthese words are many foreign words which havecharacter sequences which are not common in En-glish.
Examples include khanehanalak, phytopla~2k-ton, brycee1~.The results for our name identifier are given inTable 4.
Again, the decision tree approach is a sig-nificant improvement over the baseline case.
If wetake the baseline approach and assume that all un-known words are names, then we would achieve aprecision of 70.4%.
However, using the decision treeapproach, we obtain 86.5% precision and 92.9% re-call.We also trained a tree using both the name andmisspelling features.
The results can be found inthe second line of Table 4.
Unlike the case when wetrained the misspelling identifier on all the features,the extended tree for the name identifier providesincreased recall as well as increased precision.
Un-like the case with the misspelling decision-tree, themisspelling-identification features do provide predic-tive information for name identification.
If we reviewthe features, this result seems quite reasonable: fea-tures such as corpus frequency and non-English char-acters can provide evidence for/against name iden-176Baseline Precision/RecallMisspelling features only 70.4%/0%All featuresPrecision Recall F-score73.8% 77 .1% 75.482.8% 68.9% 75.2Table 3: Precision and recall for misspelling identificationtification as well as for/against misspelling identifi-cation.
For example, an unknown word that occursquite frequently (such as clinton) is likely to be aname, whereas an unknown word that occurs infre-quently (such as wful) is likely to be a misspelling.A review of the errors made by the name iden-tifier again provides insight for future development.Among those unknown words that are names butwhich were not identified as such are predominantlynames that can (and did) appear with determiners.Examples of this class include steelers in the steelers,and pathfinder in the pathfinder.
Hence, the nameidentifier seems adept at finding the names of indi-vidual people and places, which typically cannot becombined with determiners.
But, the name identi-fier has more problems with names that have similardistributions to common nouns.The cases where the name identifier incorrectlyidentifies unknown words as names also have identifi-able characteristics.
These examples mostly includewords with unusual character sequences such as themisspellings xetion and fwlamg.
No doubt thesehave similar characteristics to foreign names.
Asthe misidentified words are also correctly identifiedas misspellings by the misspelling identifier, theseare less problematic.
It is the task of the decision-making component to resolve issues such as these.The final results we include are for the unknownword categorizer itself using the voting procedureoutlined in previous discussion.
As introduced pre-viously, confidence measure is used as a tie-breakerin cases where the two components make positivedecision.
We evaluate the categorizer using preci-sion and recall metrics.
The precision metric identi-fies the number of correct misspelling or name cat-egorizations over the total number of times a wordwas identified as a misspelling or a name.
The re-call metric identifies the number of times the systemcorrectly identifies a misspelling or name over thenumber of misspellings and names existing in thedata.
As illustrated in Table 5, the unknown wordcategorizer achieves 86% precision and 89.9% recallon the task of identifying names and misspellings.An examination of the confusion matrix of the tie-breaker decisions is also revealing.
We include theconfusion matrix for one test data set in Table 6.Firstly, in only about 5% of the cases was it nec-essary to revert to confidence measure to determinethe category of the unknown word.
In all other casesthe predictions were compatible.
Secondly, in themajority of cases the decision-maker rules in favourof the name prediction.
In hindsight his is not sur-prising since the name decision tree has higher re-suits and hence is likely to have higher confidencemeasures.A review of the largest error category in this con-fusion matrix is also insightful.
These are caseswhere the decision-maker classifies the unknownword as a name when it should be a misspelling (37cases).
The words in this category are typically ex-amples where the misspelled word has a phoneticrelationship with the intended word.
For example,temt for tempt, floyda for florida, and dimow part ofthe intended word democrat.
Not surprisingly, it wasthese types of words which were identified as prob-lematic for the current misspelling identifier.
Aug-menting the misspelling identifier with features toidentify these types of misspellings hould also leadto improvement in the decision-maker.We find these results encouraging: they indicatethat the approach we are taking is productive.
Ourfuture work will focus on three fronts.
Firstly, wewill improve our existing components by developingfurther features which are sensitive to the distinctionbetween names and misspellings.
The discussion inthis section has indicated several promising direc-tions.
Secondly, we will develop components o iden-tify the remaining types of unknown words, such asabbreviations, morphological variants, etc.
Thirdly,we will experiment with alternative decision-makingprocesses.5 Examining PortabilityIn this paper we have introduced a means for iden-tifying names and misspellings from among othertypes of unknown words and have illustrated the pro-cess using the domain of closed captions.
Althoughnot explicitly specified, one of the goals of the re-search has been to develop an approach that will beportable to new domains and languages.We are optimistic that the approach we have de-veloped is portable.
The system that we have de-veloped requires very little in terms of linguistic re-sources.
Apart from a corpus of the new domainand language, the only other requirements are somemeans of generating spelling suggestions (ispell isavailable for many languages) and a part-of-speechtagger.
For this reason, the unknown word cate-gorizer should be portable to new languages, evenwhere extensive language resources do not exist.
If177Baseline Precision Precision Recall F-scoreName features only 70.4% 86.5% 92.9% 89.6All Features 91.8% 94.5% 93.1Table 4: Precision and recall for name identificationPrecision Recall F-scorePredicting Names and Misspellings 86.6% 89.9% 88.2Table 5: Precision and recall for decision-making componentmore information sources are available, then thesecan be readily included in the information providedto the decision tree training algorithm.For many languages, the features used in theunknown word categorizer may well be sufficient.However, the features used do make some assump-tions about the nature of the writing system used.For example, the edit distance feature in the mis-spelling identifier assumes that words consist of al-phabetic haracters which have undergone substitu-tion/addition/deletion.
However, this feature will beless useful in a language such as Japanese or Chinesewhich use ideographic haracters.
However, whilethe exact features used in this paper may be inap-propriate for a given language, we believe the generMapproach is transferable.
In the case of a languagesuch as Japanese, one would consider the means bywhich misspellings differ from their intended wordand identify features to capture these differences.6 Re la ted  ResearchThere is little research that has focused on differen-tiating the different types of unknown words.
Forexample, research on spelling error detection andcorrection for the most part assumes that all un-known words are misspellings and makes no attemptto identify other types of unknown words, e.g.
(Elmiand Evens, 1998).
Naturally, these are not appropri-ate comparisons for the work reported here.
How-ever, as is evident from the discussion above, previ-ous spelling research does provide an important rolein suggesting productive features to include in thedecision tree.Research that is more similar in goal to that out-lined in this paper is Vosse (Vosse, 1992).
Vosse usesa simple algorithm to identify three classes of un-known words: misspellings, neologisms, and names.Capitalization is his sole means of identifying names.However, capitalization i formation is not availablein closed captions.
Hence, his system would be inef-fective on the closed caption domain with which weare working.
(Granger, 1983) uses expectations gen-erated by scripts to anMyze unknown words.
Thedrawback of his system is that it lacks portabilitysince it incorporates scripts that make use of worldknowledge of the situation being described; in thiscase, naval ship-to-shore messages.Research that is similar in technique to that re-ported here is (Baluja et al, 1999).
Baluja and hiscolleagues use a decision tree classifier to identifyproper names in text.
They incorporate three typesof features: word level (essentially utilizes case in-formation), dictionary-level (comparable to our is-pell feature), and POS information (comparable toour POS tagging).
Their highest F-score for nameidentification is 95.2, slightly higher than our nameidentifier.
However, it is difficult to compare thetwo sets of results since our tasks are slightly dif-ferent.
The goal of Baluja's research, and all otherproper name identification research, is to identify allthose words and phrases in the text which are propernames.
Our research, on the other hand, is not con-cerned with all text, but only those words which areunknown.
Also preventing comparison is the type ofdata that we deal with.
Baluja's data contains caseinformation whereas ours does not- the lack of caseinformation makes name identification significantlymore difficult.
Indeed, Baluja's results when theyexclude their word-level (case) features are signifi-cantly lower: a maximum F-score of 79.7.7 ConclusionIn this paper we have introduced an unknown wordeategorizer that can identify misspellings and names.The unknown word categorizer consists of individ-ual components, each of which specialize in iden-tifying a particular class of unknown word.
Thetwo existing components are implemented as deci-sion trees.
The system provides encouraging resultswhen evaluated against a particularly challengingdomain: transcripts from live closed captions.ReferencesE.
Agirre, K. Gojenola, K. Sarasola,  and A. Vouti-lainen.
1998.
Towards a single proposal in spellingcorrection.
In Proceedings of the 36th AmmalMeeting of the ACL and the 17th International1" /~ 178Predicted Spelling Predicted NameNeither name nor misspelling 0 6Misspelling 10 37Name 4 43Table 6: Confusion matrix for decision maker: includes only those examples where both components madea positive prediction.Conference o1~ Computational Linguistics, pages22-28.S.
Baluja, V. Mittal, and R.. Sukthankar.
1999.Applying machine learning for high performancenamed-entity extraction.
In Proceedings of theColzference of the Pacific Association for Com-putational Linguistics , pages 365-378.K.
Church 1988.
A stochastic parts program andnoun phrase parser for unrestricted text.
In Pro-ceedings of the Second Conference on Applied Nat-ural Language Processing, pages 136-143.F.
Damerau.
1964.
A technique for computer detec-tion and correction of spelling errors.
Communi-cations of the ACM, 7:171-176.M.
Elmi and M. Evens.
1998.
Spelling correctionusing context.
In Proceedings of the 36th AnnualMeeting of the A CL and the 17th hlternationalCollference on Computational Linguistics, pages360-364.D.
Elworthy.
1998.
Language identification withconfidence limits.
In Proceedings of the 6th Work-shop on Very large Corpora.R.
Granger.
1983.
The nomad system: expectation-based detection and correction of errors during un-derstanding of syntactically and semantically ill-formed text.
American Journal of ComputationalLinguistics, 9:188-198.J.
Hull and S. Srihari.
1982.
Experiments in textrecognition with binary n-gram and viterbi algo-rithms.
IEEE Trans.
Patt.
Anal.
Machine b~tell.PAMI-4, 5:520-530.K.. Kukich.
1992.
Techniques for automatically cor-recting words in text.
ACM Computing Surveys,24:377-439.I.
Mani, R. McMillan, S. Luperfoy, E. Lusher, andS.
Laskowski, 1996.
Corpus Processing for LexicalAcquisition, chapter Identifying unknown propernames in newswire text.
MIT Press, Cambridge.D.
McDonald, 1996.
Corpus Processing for Lexi-cal Acquisition, chapter Internal and external ev-idence in the identification and semantic atego-rization of proper names.
MIT Press, Cambridge.K.
Min and W. Wilson.
1998.
Integrated control ofchart items for error repair.
In Proceedings of the36th Annual Meeting of the Association for Com-putational Linguistics and the 17th hlternationalConferet~ce on Computational Linguistics.K.
Min.
1996.
Hierarchical Error Re.covery Basedon Bidirectional Chart Parsing Techniques.
Ph.D.thesis, University of NSW, Sydney, Australia.R.
Mitton.
1987.
Spelling checkers, spelling coffee-tots, and the misspellings of poor spellers.
Inf.Process.
Manage, 23:495-505.J.
Toole 1999 Categorizing Unknown Words: A de-cision tree-based misspelling identifier In Foo, N(ed.)
Advanced Topics in Artificial h2telligence,pages 122-133.H.
van Halteren, J. Zavrel, and W. Daelemans.
1998.Improving data driven word class tagging by sys-tem combination.
In Proceedings of the 36th An-nual Meeting of the ACL and the 17th Interna-tional Conference on Computational Linguistics,pages 491-497.T.
Vosse.
1992.
Detecting and correcting morpho-syntactic errors in real texts.
In Proceedin9s ofthe 3rd Conference o11 Applied Natural LanguageProcessing, pages 111-118.S.
Weiss and N. Indurkhya.
1998.
Predictive DataMining.
Morgan Kauffman Publishers.S.
Weiss, and C. Apte, and F. Damerau, andD.
Johnson, and F. Oles and T. Goetz, andT.
Hampp.
1999 Maximizing text-mining per-formance.
IEEE Intelligent Systems and theirApplications, 14(4):63-69E.
Zamora, J. Pollock, and A. Zamora.
1981.
Theuse of tri-gram analysis for spelling error detec-tion.
he Process.
Manage., 17:305-316.179
