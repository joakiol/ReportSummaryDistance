Proceedings of the 43rd Annual Meeting of the ACL, pages 26?33,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsSupersense Tagging of Unknown Nouns using Semantic SimilarityJames R. CurranSchool of Information TechnologiesUniversity of SydneyNSW 2006, Australiajames@it.usyd.edu.auAbstractThe limited coverage of lexical-semantic re-sources is a significant problem for NLP sys-tems which can be alleviated by automati-cally classifying the unknown words.
Su-persense tagging assigns unknown nouns oneof 26 broad semantic categories used by lex-icographers to organise their manual inser-tion into WORDNET.
Ciaramita and Johnson(2003) present a tagger which uses synonymset glosses as annotated training examples.
Wedescribe an unsupervised approach, based onvector-space similarity, which does not requireannotated examples but significantly outper-forms their tagger.
We also demonstrate the useof an extremely large shallow-parsed corpus forcalculating vector-space semantic similarity.1 IntroductionLexical-semantic resources have been applied successfulto a wide range of Natural Language Processing (NLP)problems ranging from collocation extraction (Pearce,2001) and class-based smoothing (Clark and Weir, 2002),to text classification (Baker and McCallum, 1998) andquestion answering (Pasca and Harabagiu, 2001).
In par-ticular, WORDNET (Fellbaum, 1998) has significantly in-fluenced research in NLP.Unfortunately, these resource are extremely time-consuming and labour-intensive to manually develop andmaintain, requiring considerable linguistic and domainexpertise.
Lexicographers cannot possibly keep pacewith language evolution: sense distinctions are contin-ually made and merged, words are coined or becomeobsolete, and technical terms migrate into the vernacu-lar.
Technical domains, such as medicine, require sepa-rate treatment since common words often take on specialmeanings, and a significant proportion of their vocabu-lary does not overlap with everyday vocabulary.
Bur-gun and Bodenreider (2001) compared an alignment ofWORDNET with the UMLS medical resource and foundonly a very small degree of overlap.
Also, lexical-semantic resources suffer from:bias towards concepts and senses from particular topics.Some specialist topics are better covered in WORD-NET than others, e.g.
dog has finer-grained distinc-tions than cat and worm although this does not re-flect finer distinctions in reality;limited coverage of infrequent words and senses.
Cia-ramita and Johnson (2003) found that commonnouns missing from WORDNET 1.6 occurred every8 sentences in the BLLIP corpus.
By WORDNET 2.0,coverage has improved but the problem of keepingup with language evolution remains difficult.consistency when classifying similar words into cate-gories.
For instance, the WORDNET lexicographerfile for ionosphere (location) is different to exo-sphere and stratosphere (object), two other layersof the earth?s atmosphere.These problems demonstrate the need for automatic orsemi-automatic methods for the creation and mainte-nance of lexical-semantic resources.
Broad semanticclassification is currently used by lexicographers to or-ganise the manual insertion of words into WORDNET,and is an experimental precursor to automatically insert-ing words directly into the WORDNET hierarchy.
Cia-ramita and Johnson (2003) call this supersense taggingand describe a multi-class perceptron tagger, which usesWORDNET?s hierarchical structure to create many anno-tated training instances from the synset glosses.This paper describes an unsupervised approach to su-persense tagging that does not require annotated sen-tences.
Instead, we use vector-space similarity to re-trieve a number of synonyms for each unknown commonnoun.
The supersenses of these synonyms are then com-bined to determine the supersense.
This approach sig-nificantly outperforms the multi-class perceptron on thesame dataset based on WORDNET 1.6 and 1.7.1.26LEX-FILE DESCRIPTIONact acts or actionsanimal animalsartifact man-made objectsattribute attributes of people and objectsbody body partscognition cognitive processes and contentscommunication communicative processes and contentsevent natural eventsfeeling feelings and emotionsfood foods and drinksgroup groupings of people or objectslocation spatial positionmotive goalsobject natural objects (not man-made)person peoplephenomenon natural phenomenaplant plantspossession possession and transfer of possessionprocess natural processesquantity quantities and units of measurerelation relations between people/things/ideasshape two and three dimensional shapesstate stable states of affairssubstance substancestime time and temporal relationsTable 1: 25 noun lexicographer files in WORDNET2 SupersensesThere are 26 broad semantic classes employed by lex-icographers in the initial phase of inserting words intothe WORDNET hierarchy, called lexicographer files (lex-files).
For the noun hierarchy, there are 25 lex-files and afile containing the top level nodes in the hierarchy calledTops.
Other syntactic classes are also organised usinglex-files: 15 for verbs, 3 for adjectives and 1 for adverbs.Lex-files form a set of coarse-grained sense distinc-tions within WORDNET.
For example, company appearsin the following lex-files in WORDNET 2.0: group, whichcovers company in the social, commercial and troupefine-grained senses; and state, which covers companion-ship.
The names and descriptions of the noun lex-filesare shown in Table 1.
Some lex-files map directly tothe top level nodes in the hierarchy, called unique begin-ners, while others are grouped together as hyponyms ofa unique beginner (Fellbaum, 1998, page 30).
For exam-ple, abstraction subsumes the lex-files attribute, quantity,relation, communication and time.Ciaramita and Johnson (2003) call the noun lex-fileclasses supersenses.
There are 11 unique beginners inthe WORDNET noun hierarchy which could also be usedas supersenses.
Ciaramita (2002) has produced a mini-WORDNET by manually reducing the WORDNET hier-archy to 106 broad categories.
Ciaramita et al (2003)describe how the lex-files can be used as root nodes in atwo level hierarchy with the WORDNET synsets appear-ing directly underneath.Other alternative sets of supersenses can be created byan arbitrary cut through the WORDNET hierarchy nearthe top, or by using topics from a thesaurus such asRoget?s (Yarowsky, 1992).
These topic distinctions arecoarser-grained than WORDNET senses, which have beencriticised for being too difficult to distinguish even forexperts.
Ciaramita and Johnson (2003) believe that thekey sense distinctions are still maintained by supersenses.They suggest that supersense tagging is similar to namedentity recognition, which also has a very small set of cat-egories with similar granularity (e.g.
location and person)for labelling predominantly unseen terms.Supersense tagging can provide automated or semi-automated assistance to lexicographers adding words tothe WORDNET hierarchy.
Once this task is solved suc-cessfully, it may be possible to insert words directlyinto the fine-grained distinctions of the hierarchy itself.Clearly, this is the ultimate goal, to be able to insertnew terms into lexical resources, extending the structurewhere necessary.
Supersense tagging is also interestingfor many applications that use shallow semantics, e.g.
in-formation extraction and question answering.3 Previous WorkA considerable amount of research addresses structurallyand statistically manipulating the hierarchy of WORD-NET and the construction of new wordnets using the con-cept structure from English.
For lexical FreeNet, Beefer-man (1998) adds over 350 000 collocation pairs (triggerpairs) extracted from a 160 million word corpus of broad-cast news using mutual information.
The co-occurrencewindow was 500 words which was designed to approxi-mate average document length.Caraballo and Charniak (1999) have explored deter-mining noun specificity from raw text.
They find thatsimple frequency counts are the most effective way ofdetermining the parent-child ordering, achieving 83% ac-curacy over types of vehicle, food and occupation.
Theother measure they found to be successful was the en-tropy of the conditional distribution of surrounding wordsgiven the noun.
Specificity ordering is a necessary stepfor building a noun hierarchy.
However, this approachclearly cannot build a hierarchy alone.
For instance, en-tity is less frequent than many concepts it subsumes.
Thissuggests it will only be possible to add words to an ex-isting abstract structure rather than create categories rightup to the unique beginners.Hearst and Schu?tze (1993) flatten WORDNET into 726categories using an algorithm which attempts to min-imise the variance in category size.
These categories areused to label paragraphs with topics, effectively repeat-ing Yarowsky?s (1992) experiments using the their cat-egories rather than Roget?s thesaurus.
Schu?tze?s (1992)27WordSpace system was used to add topical links, suchas between ball, racquet and game (the tennis problem).Further, they also use the same vector-space techniquesto label previously unseen words using the most commonclass assigned to the top 20 synonyms for that word.Widdows (2003) uses a similar technique to insertwords into the WORDNET hierarchy.
He first extractssynonyms for the unknown word using vector-space sim-ilarity measures based on Latent Semantic Analysis andthen searches for a location in the hierarchy nearest tothese synonyms.
This same technique as is used in ourapproach to supersense tagging.Ciaramita and Johnson (2003) implement a super-sense tagger based on the multi-class perceptron classi-fier (Crammer and Singer, 2001), which uses the standardcollocation, spelling and syntactic features common inWSD and named entity recognition systems.
Their insightwas to use the WORDNET glosses as annotated trainingdata and massively increase the number of training in-stances using the noun hierarchy.
They developed an effi-cient algorithm for estimating the model over hierarchicaltraining data.4 EvaluationCiaramita and Johnson (2003) propose a very naturalevaluation for supersense tagging: inserting the extracommon nouns that have been added to a new versionof WORDNET.
They use the common nouns that havebeen added to WORDNET 1.7.1 since WORDNET 1.6 andcompare this evaluation with a standard cross-validationapproach that uses a small percentage of the words fromtheir WORDNET 1.6 training set for evaluation.
Theirresults suggest that the WORDNET 1.7.1 test set is sig-nificantly harder because of the large number of abstractcategory nouns, e.g.
communication and cognition, thatappear in the 1.7.1 data, which are difficult to classify.Our evaluation will use exactly the same test sets asCiaramita and Johnson (2003).
The WORDNET 1.7.1 testset consists of 744 previously unseen nouns, the majorityof which (over 90%) have only one sense.
The WORD-NET 1.6 test set consists of several cross-validation setsof 755 nouns randomly selected from the BLLIP train-ing set used by Ciaramita and Johnson (2003).
Theyhave kindly supplied us with the WORDNET 1.7.1 test setand one cross-validation run of the WORDNET 1.6 testset.
Our development experiments are performed on theWORDNET 1.6 test set with one final run on the WORD-NET 1.7.1 test set.
Some examples from the test sets aregiven in Table 2 with their supersenses.5 CorpusWe have developed a 2 billion word corpus, shallow-parsed with a statistical NLP pipeline, which is by far theWORDNET 1.6 WORDNET 1.7.1NOUN SUPERSENSE NOUN SUPERSENSEstock index communication week timefast food food buyout actbottler group insurer groupsubcompact artifact partner personadvancer person health statecash flow possession income possessiondownside cognition contender persondiscounter artifact cartel grouptrade-off act lender personbillionaire person planner artifactTable 2: Example nouns and their supersenseslargest NLP processed corpus described in published re-search.
The corpus consists of the British National Cor-pus (BNC), the Reuters Corpus Volume 1 (RCV1), andmost of the Linguistic Data Consortium?s news text col-lected since 1987: Continuous Speech Recognition III(CSR-III); North American News Text Corpus (NANTC);the NANTC Supplement (NANTS); and the ACQUAINTCorpus.
The components and their sizes including punc-tuation are given in Table 3.
The LDC has recently re-leased the English Gigaword corpus which includes mostof the corpora listed above.CORPUS DOCS.
SENTS.
WORDSBNC 4 124 6.2M 114MRCV1 806 791 8.1M 207MCSR-III 491 349 9.3M 226MNANTC 930 367 23.2M 559MNANTS 942 167 25.2M 507MACQUAINT 1 033 461 21.3M 491MTable 3: 2 billion word corpus statisticsWe have tokenized the text using the Grok-OpenNLPtokenizer (Morton, 2002) and split the sentences usingMXTerminator (Reynar and Ratnaparkhi, 1997).
Anysentences less than 3 words or more than 100 words longwere rejected, along with sentences containing more than5 numbers or more than 4 brackets, to reduce noise.
Therest of the pipeline is described in the next section.6 Semantic SimilarityVector-space models of similarity are based on the distri-butional hypothesis that similar words appear in similarcontexts.
This hypothesis suggests that semantic simi-larity can be measured by comparing the contexts eachword appears in.
In vector-space models each headwordis represented by a vector of frequency counts record-ing the contexts that it appears in.
The key parametersare the context extraction method and the similarity mea-sure used to compare context vectors.
Our approach to28vector-space similarity is based on the SEXTANT systemdescribed in Grefenstette (1994).Curran and Moens (2002b) compared several contextextraction methods and found that the shallow pipelineand grammatical relation extraction used in SEXTANTwas both extremely fast and produced high-quality re-sults.
SEXTANT extracts relation tuples (w, r,w?)
foreach noun, where w is the headword, r is the relation typeand w?
is the other word.
The efficiency of the SEXTANTapproach makes the extraction of contextual informationfrom over 2 billion words of raw text feasible.
We de-scribe the shallow pipeline in detail below.Curran and Moens (2002a) compared several differ-ent similarity measures and found that Grefenstette?sweighted JACCARD measure performed the best:?min(wgt(w1, ?r, ?w?
),wgt(w2, ?r, ?w?
))?max(wgt(w1, ?r, ?w?
),wgt(w2, ?r, ?w?
))(1)where wgt(w, r, w?)
is the weight function for relation(w, r, w?).
Curran and Moens (2002a) introduced theTTEST weight function, which is used in collocation ex-traction.
Here, the t-test compares the joint and productprobability distributions of the headword and context:p(w, r, w?)
?
p(?, r, w?
)p(w, ?, ?
)?p(?, r, w?
)p(w, ?, ?
)(2)where ?
indicates a global sum over that element of therelation tuple.
JACCARD and TTEST produced betterquality synonyms than existing measures in the literature,so we use Curran and Moen?s configuration for our super-sense tagging experiments.6.1 Part of Speech Tagging and ChunkingOur implementation of SEXTANT uses a maximum en-tropy POS tagger designed to be very efficient, taggingat around 100 000 words per second (Curran and Clark,2003), trained on the entire Penn Treebank (Marcus et al,1994).
The only similar performing tool is the Trigrams?n?
Tags tagger (Brants, 2000) which uses a much simplerstatistical model.
Our implementation uses a maximumentropy chunker which has similar feature types to Koel-ing (2000) and is also trained on chunks extracted fromthe entire Penn Treebank using the CoNLL 2000 script.Since the Penn Treebank separates PPs and conjunctionsfrom NPs, they are concatenated to match Grefenstette?stable-based results, i.e.
the SEXTANT always prefers nounattachment.6.2 Morphological AnalysisOur implementation uses morpha, the Sussex morpho-logical analyser (Minnen et al, 2001), which is imple-mented using lex grammars for both affix splitting andgeneration.
morpha has wide coverage ?
nearly 100%RELATION DESCRIPTIONadj noun?adjectival modifier relationdobj verb?direct object relationiobj verb?indirect object relationnn noun?noun modifier relationnnprep noun?prepositional head relationsubj verb?subject relationTable 4: Grammatical relations from SEXTANTagainst the CELEX lexical database (Minnen et al, 2001)?
and is very efficient, analysing over 80 000 words persecond.
morpha often maintains sense distinctions be-tween singular and plural nouns; for instance: specta-cles is not reduced to spectacle, but fails to do so inother cases: glasses is converted to glass.
This inconsis-tency is problematic when using morphological analysisto smooth vector-space models.
However, morphologicalsmoothing still produces better results in practice.6.3 Grammatical Relation ExtractionAfter the raw text has been POS tagged and chunked,the grammatical relation extraction algorithm is run overthe chunks.
This consists of five passes over each sen-tence that first identify noun and verb phrase heads andthen collect grammatical relations between each commonnoun and its modifiers and verbs.
A global list of gram-matical relations generated by each pass is maintainedacross the passes.
The global list is used to determine if aword is already attached.
Once all five passes have beencompleted this association list contains all of the noun-modifier/verb pairs which have been extracted from thesentence.
The types of grammatical relation extracted bySEXTANT are shown in Table 4.
For relations betweennouns (nn and nnprep), we also create inverse relations(w?, r?, w) representing the fact that w?
can modify w.The 5 passes are described below.Pass 1: Noun Pre-modifiersThis pass scans NPs, left to right, creating adjectival(adj) and nominal (nn) pre-modifier grammatical rela-tions (GRs) with every noun to the pre-modifier?s right,up to a preposition or the phrase end.
This corresponds toassuming right-branching noun compounds.
Within eachNP only the NP and PP heads remain unattached.Pass 2: Noun Post-modifiersThis pass scans NPs, right to left, creating post-modifierGRs between the unattached heads of NPs and PPs.
Ifa preposition is encountered between the noun heads, aprepositional noun (nnprep) GR is created, otherwise anappositional noun (nn) GR is created.
This correspondsto assuming right-branching PP attachment.
After thisphrase only the NP head remains unattached.Tense DeterminationThe rightmost verb in each VP is considered the head.
A29VP is initially categorised as active.
If the head verb is aform of be then the VP becomes attributive.
Otherwise,the algorithm scans the VP from right to left: if an auxil-iary verb form of be is encountered the VP becomes pas-sive; if a progressive verb (except being) is encounteredthe VP becomes active.Only the noun heads on either side of VPs remainunattached.
The remaining three passes attach these tothe verb heads as either subjects or objects depending onthe voice of the VP.Pass 3: Verb Pre-AttachmentThis pass scans sentences, right to left, associating thefirst NP head to the left of the VP with its head.
If the VPis active, a subject (subj) relation is created; otherwise,a direct object (dobj) relation is created.
For example,antigen is the subject of represent.Pass 4: Verb Post-AttachmentThis pass scans sentences, left to right, associating thefirst NP or PP head to the right of the VP with its head.If the VP was classed as active and the phrase is an NPthen a direct object (dobj) relation is created.
If the VPwas classed as passive and the phrase is an NP then asubject (subj) relation is created.
If the following phraseis a PP then an indirect object (iobj) relation is created.The interaction between the head verb and the preposi-tion determine whether the noun is an indirect object ofa ditransitive verb or alternatively the head of a PP that ismodifying the verb.
However, SEXTANT always attachesthe PP to the previous phrase.Pass 5: Verb Progressive ParticiplesThe final step of the process is to attach progressive verbsto subjects and objects (without concern for whether theyare already attached).
Progressive verbs can function asnouns, verbs and adjectives and once again a na?
?ve ap-proximation to the correct attachment is made.
Any pro-gressive verb which appears after a determiner or quan-tifier is considered a noun.
Otherwise, it is a verb andpasses 3 and 4 are repeated to attach subjects and objects.Finally, SEXTANT collapses the nn, nnprep and adj re-lations together into a single broad noun-modifier gram-matical relation.
Grefenstette (1994) claims this extractorhas a grammatical relation accuracy of 75% after manu-ally checking 60 sentences.7 ApproachOur approach uses voting across the known supersensesof automatically extracted synonyms, to select a super-sense for the unknown nouns.
This technique is simi-lar to Hearst and Schu?tze (1993) and Widdows (2003).However, sometimes the unknown noun does not appearin our 2 billion word corpus, or at least does not appearfrequently enough to provide sufficient contextual infor-mation to extract reliable synonyms.
In these cases, ourSUFFIX EXAMPLE SUPERSENSE-ness remoteness attribute-tion, -ment annulment act-ist, -man statesman person-ing, -ion bowling act-ity viscosity attribute-ics, -ism electronics cognition-ene, -ane, -ine arsine substance-er, -or, -ic, -ee, -an mariner person-gy entomology cognitionTable 5: Hand-coded rules for supersense guessingfall-back method is a simple hand-coded classifier whichexamines the unknown noun and makes a guess based onsimple morphological analysis of the suffix.
These ruleswere created by inspecting the suffixes of rare nouns inWORDNET 1.6.
The supersense guessing rules are givenin Table 5.
If none of the rules match, then the defaultsupersense artifact is assigned.The problem now becomes how to convert the rankedlist of extracted synonyms for each unknown noun intoa single supersense selection.
Each extracted synonymvotes for its one or more supersenses that appear inWORDNET 1.6.
There are many parameters to consider:?
how many extracted synonyms to use;?
how to weight each synonym?s vote;?
whether unreliable synonyms should be filtered out;?
how to deal with polysemous synonyms.The experiments described below consider a range of op-tions for these parameters.
In fact, these experiments areso quick to run we have been able to exhaustively testmany combinations of these parameters.
We have exper-imented with up to 200 voting extracted synonyms.There are several ways to weight each synonym?s con-tribution.
The simplest approach would be to give eachsynonym the same weight.
Another approach is to usethe scores returned by the similarity system.
Alterna-tively, the weights can use the ranking of the extractedsynonyms.
Again these options have been consideredbelow.
A related question is whether to use all of theextracted synonyms, or perhaps filter out synonyms forwhich a small amount of contextual information has beenextracted, and so might be unreliable.The final issue is how to deal with polysemy.
Does ev-ery supersense of each extracted synonym get the wholeweight of that synonym or is it distributed evenly betweenthe supersenses like Resnik (1995)?
Another alternativeis to only consider unambiguous synonyms with a singlesupersense in WORDNET.A disadvantage of this similarity approach is that it re-quires full synonym extraction, which compares the un-known word against a large number of words when, in30SYSTEM WN 1.6 WN 1.7.1Ciaramita and Johnson baseline 21% 28%Ciaramita and Johnson perceptron 53% 53%Similarity based results 68% 63%Table 6: Summary of supersense tagging accuraciesfact, we want to calculate the similarity to a small numberof supersenses.
This inefficiency could be reduced sig-nificantly if we consider only very high frequency words,but even this is still expensive.8 ResultsWe have used the WORDNET 1.6 test set to experi-ment with different parameter settings and have kept theWORDNET 1.7.1 test set as a final comparison of bestresults with Ciaramita and Johnson (2003).
The experi-ments were performed by considering all possible config-urations of the parameters described above.The following voting options were considered for eachsupersense of each extracted synonym: the initial vot-ing weight for a supersense could either be a constant(IDENTITY) or the similarity score (SCORE) of the syn-onym.
The initial weight could then be divided by thenumber of supersenses to share out the weight (SHARED).The weight could also be divided by the rank (RANK) topenalise supersenses further down the list.
The best per-formance on the 1.6 test set was achieved with the SCOREvoting, without sharing or ranking penalties.The extracted synonyms are filtered before contribut-ing to the vote with their supersense(s).
This filtering in-volves checking that the synonym?s frequency and num-ber of contexts are large enough to ensure it is reliable.We have experimented with a wide range of cutoffs andthe best performance on the 1.6 test set was achieved us-ing a minimum cutoff of 5 for the synonym?s frequencyand the number of contexts it appears in.The next question is how many synonyms are consid-ered.
We considered using just the nearest unambiguoussynonym, and the top 5, 10, 20, 50, 100 and 200 syn-onyms.
All of the top performing configurations used 50synonyms.
We have also experimented with filtering outhighly polysemous nouns by eliminating words with two,three or more synonyms.
However, such a filter turnedout to make little difference.Finally, we need to decide when to use the similaritymeasure and when to fall-back to the guessing rules.
Thisis determined by looking at the frequency and number ofattributes for the unknown word.
Not surprisingly, thesimilarity system works better than the guessing rules ifit has any information at all.The results are summarised in Table 6.
The accuracyof the best-performing configurations was 68% on theWORDNET 1.6 WORDNET 1.7.1SUPERSENSE N P R F N P R FTops 2 0 0 0 1 50 100 67act 84 60 74 66 86 53 73 61animal 16 69 56 62 5 33 60 43artifact 134 61 86 72 129 57 76 65attribute 32 52 81 63 16 44 69 54body 8 88 88 88 5 50 40 44cognition 31 56 45 50 41 70 34 46communication 66 80 56 66 57 58 44 50event 14 83 36 50 10 80 40 53feeling 8 70 88 78 1 0 0 0food 29 91 69 78 12 67 67 67group 27 75 22 34 26 50 4 7location 43 81 30 44 13 40 15 22motive 0 0 0 0 1 0 0 0object 17 73 47 57 13 75 23 35person 155 76 89 82 207 81 86 84phenomenon 3 100 100 100 9 0 0 0plant 11 80 73 76 0 0 0 0possession 9 100 22 36 16 78 44 56process 2 0 0 0 9 50 11 18quantity 12 80 33 47 5 0 0 0relation 2 100 50 67 0 0 0 0shape 1 0 0 0 0 0 0 0state 21 48 48 48 28 50 39 44substance 24 58 58 58 44 63 73 67time 5 100 60 75 10 36 40 38Overall 756 68 68 68 744 63 63 63Table 7: Breakdown of results by supersenseWORDNET 1.6 test set with several other parameter com-binations described above performing nearly as well.
Onthe previously unused WORDNET 1.7.1 test set, our accu-racy is 63% using the best system on the WORDNET 1.6test set.
By optimising the parameters on the 1.7.1 testset we can increase that to 64%, indicating that we havenot excessively over-tuned on the 1.6 test set.
Our resultssignificantly outperform Ciaramita and Johnson (2003)on both test sets even though our system is unsupervised.The large difference between our 1.6 and 1.7.1 test setaccuracy demonstrates that the 1.7.1 set is much harder.Table 7 shows the breakdown in performance for eachsupersense.
The columns show the number of instancesof each supersense with the precision, recall and f-scoremeasures as percentages.
The most frequent supersensesin both test sets were person, attribute and act.
Of thefrequent categories, person is the easiest supersense toget correct in both the 1.6 and 1.7.1 test sets, followedby food, artifact and substance.
This is not surprisingsince these concrete words tend to have very fewer othersenses, well constrained contexts and a relatively highfrequency.
These factors are conducive for extracting re-liable synonyms.These results also support Ciaramita and Johnson?sview that abstract concepts like communication, cognitionand state are much harder.
We would expect the location31supersense to perform well since it is quite concrete, butunfortunately our synonym extraction system does notincorporate proper nouns, so many of these words wereclassified using the hand-built classifier.
Also, in the datafrom Ciaramita and Johnson all of the words are in lowercase, so no sensible guessing rules could help.9 Other Alternatives and Future WorkAn alternative approach worth exploring is to create con-text vectors for the supersense categories themselves andcompare these against the words.
This has the advantageof producing a much smaller number of vectors to com-pare against.
In the current system, we must compare aword against the entire vocabulary (over 500 000 head-words), which is much less efficient than a comparisonagainst only 26 supersense context vectors.The question now becomes how to construct vectorsof supersenses.
The most obvious solution is to sum thecontext vectors across the words which have each su-persense.
However, our early experiments suggest thatthis produces extremely large vectors which do not matchwell against the much smaller vectors of each unseenword.
Also, the same questions arise in the construc-tion of these vectors.
How are words with multiple su-persenses handled?
Our preliminary experiments suggestthat only combining the vectors for unambiguous wordsproduces the best results.One solution would be to take the intersection betweenvectors across words for each supersense (i.e.
to find thecommon contexts that these words appear in).
However,given the sparseness of the data this may not leave verylarge context vectors.
A final solution would be to con-sider a large set of the canonical attributes (Curran andMoens, 2002a) to represent each supersense.
Canonicalattributes summarise the key contexts for each headwordand are used to improve the efficiency of the similaritycomparisons.There are a number of problems our system does notcurrently handle.
Firstly, we do not include proper namesin our similarity system which means that location enti-ties can be very difficult to identify correctly (as the re-sults demonstrate).
Further, our similarity system doesnot currently incorporate multi-word terms.
We over-come this by using the synonyms of the last word inthe multi-word term.
However, there are 174 multi-wordterms (23%) in the WORDNET 1.7.1 test set which wecould probably tag more accurately with synonyms forthe whole multi-word term.
Finally, we plan to imple-ment a supervised machine learner to replace the fall-back method, which currently has an accuracy of 37%on the WORDNET 1.7.1 test set.We intend to extend our experiments beyond the Cia-ramita and Johnson (2003) set to include previous andmore recent versions of WORDNET to compare their dif-ficulty, and also perform experiments over a range of cor-pus sizes to determine the impact of corpus size on thequality of results.We would like to move onto the more difficult taskof insertion into the hierarchy itself and compare againstthe initial work by Widdows (2003) using latent seman-tic analysis.
Here the issue of how to combine vec-tors is even more interesting since there is the additionalstructure of the WORDNET inheritance hierarchy and thesmall synonym sets that can be used for more fine-grainedcombination of vectors.10 ConclusionOur application of semantic similarity to supersense tag-ging follows earlier work by Hearst and Schu?tze (1993)and Widdows (2003).
To classify a previously unseencommon noun our approach extracts synonyms whichvote using their supersenses in WORDNET 1.6.
We haveexperimented with several parameters finding that thebest configuration uses 50 extracted synonyms, filteredby frequency and number of contexts to increase their re-liability.
Each synonym votes for each of its supersensesfrom WORDNET 1.6 using the similarity score from oursynonym extractor.Using this approach we have significantly outper-formed the supervised multi-class perceptron Ciaramitaand Johnson (2003).
This paper also demonstrates theuse of a very efficient shallow NLP pipeline to processa massive corpus.
Such a corpus is needed to acquirereliable contextual information for the often very rarenouns we are attempting to supersense tag.
This appli-cation of semantic similarity demonstrates that an unsu-pervised methods can outperform supervised methods forsome NLP tasks if enough data is available.AcknowledgementsWe would like to thank Massi Ciaramita for supplyinghis original data for these experiments and answering ourqueries, and to Stephen Clark and the anonymous re-viewers for their helpful feedback and corrections.
Thiswork has been supported by a Commonwealth scholar-ship, Sydney University Travelling Scholarship and Aus-tralian Research Council Discovery Project DP0453131.ReferencesL.
Douglas Baker and Andrew McCallum.
1998.
Distributionalclustering of words for text classification.
In Proceedingsof the 21st annual international ACM SIGIR conference onResearch and Development in Information Retrieval, pages96?103, Melbourne, Australia.Doug Beeferman.
1998.
Lexical discovery with an enrichedsemantic network.
In Proceedings of the Workshop on Usage32of WordNet in Natural Language Processing Systems, pages358?364, Montre?al, Que?bec, Canada.Thorsten Brants.
2000.
TnT - a statistical part-of-speech tag-ger.
In Proceedings of the 6th Applied Natural LanguageProcessing Conference, pages 224?231, Seattle, WA USA.Anita Burgun and Olivier Bodenreider.
2001.
Comparingterms, concepts and semantic classes in WordNet and theUnified Medical Language System.
In Proceedings of theWorkshop on WordNet and Other Lexical Resources: Appli-cations, Extensions and Customizations, pages 77?82, Pitts-burgh, PA USA.Sharon A. Caraballo and Eugene Charniak.
1999.
Determiningthe specificity of nouns from text.
In Proceedings of the JointACL SIGDAT Conference on Empirical Methods in NaturalLanguage Processing and Very Large Corpora, pages 63?70,College Park, MD USA.Massimiliano Ciaramita and Mark Johnson.
2003.
Supersensetagging of unknown nouns in WordNet.
In Proceedings ofthe 2003 Conference on Empirical Methods in Natural Lan-guage Processing, pages 168?175, Sapporo, Japan.Massimiliano Ciaramita, Thomas Hofmann, and Mark John-son.
2003.
Hierarchical semantic classification: Word sensedisambiguation with world knowledge.
In Proceedings ofthe 18th International Joint Conference on Artificial Intelli-gence, Acapulco, Mexico.Massimiliano Ciaramita.
2002.
Boosting automatic lexical ac-quisition with morphological information.
In Proceedingsof the Workshop on Unsupervised Lexical Acquisition, pages17?25, Philadelphia, PA, USA.Stephen Clark and David Weir.
2002.
Class-based probabilityestimation using a semantic hierarchy.
Computational Lin-guistics, 28(2):187?206, June.Koby Crammer and Yoram Singer.
2001.
Ultraconservativeonline algorithms for multiclass problems.
In Proceedings ofthe 14th annual Conference on Computational Learning The-ory and 5th European Conference on Computational Learn-ing Theory, pages 99?115, Amsterdam, The Netherlands.James R. Curran and Stephen Clark.
2003.
Investigating GISand smoothing for maximum entropy taggers.
In Proceed-ings of the 10th Conference of the European Chapter of theAssociation for Computational Linguistics, pages 91?98, Bu-dapest, Hungary.James R. Curran and Marc Moens.
2002a.
Improvementsin automatic thesaurus extraction.
In Proceedings of theWorkshop on Unsupervised Lexical Acquisition, pages 59?66, Philadelphia, PA, USA.James R. Curran and Marc Moens.
2002b.
Scaling contextspace.
In Proceedings of the 40th annual meeting of theAssociation for Computational Linguistics, pages 231?238,Philadelphia, PA, USA.Christiane Fellbaum, editor.
1998.
WordNet: An ElectronicLexical Database.
MIT Press, Cambridge, MA USA.Gregory Grefenstette.
1994.
Explorations in Automatic The-saurus Discovery.
Kluwer Academic Publishers, Boston,MA USA.Marti A. Hearst and Hinrich Schu?tze.
1993.
Customizing alexicon to better suit a computational task.
In Proceedingsof the Workshop on Acquisition of Lexical Knowledge fromText, pages 55?69, Columbus, OH USA.Rob Koeling.
2000.
Chunking with maximum entropy models.In Proceedings of the 4th Conference on Computational Nat-ural Language Learning and of the 2nd Learning Languagein Logic Workshop, pages 139?141, Lisbon, Portugal.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1994.
Building a large annotated corpusof English: the Penn Treebank.
Computational Linguistics,19(2):313?330.Guido Minnen, John Carroll, and Darren Pearce.
2001.
Ap-plied morphological processing of English.
Natural Lan-guage Engineering, 7(3):207?223.Tom Morton.
2002.
Grok tokenizer.
Grok OpenNLP toolkit.Marius Pasca and Sanda M. Harabagiu.
2001.
The informa-tive role of WordNet in open-domain question answering.
InProceedings of the Workshop on WordNet and Other Lex-ical Resources: Applications, Extensions and Customiza-tions, pages 138?143, Pittsburgh, PA USA.Darren Pearce.
2001.
Synonymy in collocation extraction.
InProceedings of the Workshop on WordNet and Other Lex-ical Resources: Applications, Extensions and Customiza-tions, pages 41?46, Pittsburgh, PA USA.Philip Resnik.
1995.
Using information content to evaluatesemantic similarity.
In Proceedings of the 14th InternationalJoint Conference on Artificial Intelligence, pages 448?453,Montreal, Canada.Jeffrey C. Reynar and Adwait Ratnaparkhi.
1997.
A maxi-mum entropy approach to identifying sentence boundaries.In Proceedings of the Fifth Conference on Applied NaturalLanguage Processing, pages 16?19, Washington, D.C. USA.Hinrich Schu?tze.
1992.
Context space.
In Intelligent Proba-bilistic Approaches to Natural Language, number FS-92-04in Fall Symposium Series, pages 113?120, Stanford Univer-sity, CA USA.Dominic Widdows.
2003.
Unsupervised methods for develop-ing taxonomies by combining syntactic and statistical infor-mation.
In Proceedings of the Human Language TechnologyConference of the North American Chapter of the Associa-tion for Computational Linguistics, pages 276?283, Edmon-ton, Alberta Canada.David Yarowsky.
1992.
Word-sense disambiguation using sta-tistical models of Roget?s categories trained on large corpora.In Proceedings of the 14th international conference on Com-putational Linguistics, pages 454?460, Nantes, France.33
