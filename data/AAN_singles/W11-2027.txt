Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 248?258,Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational LinguisticsEmbedded WizardryRebecca J. Passonneau1, Susan L. Epstein2,3, Tiziana Ligorio3 and Joshua Gordon11Columbia UniversityNew York, NY, USA(becky|joshua)@cs.columbia.edu2,3Hunter College3The Graduate Center of the City University of New YorkNew York, NY, USA (susan.epstein@hunter|tligorio@gc).cuny.eduAbstractThis paper presents a progressively challeng-ing series of experiments that investigate clar-ification subdialogues to resolve the words innoisy transcriptions of user utterances.
We fo-cus on user utterances where the user?s spe-cific intent requires little additional inference,given sufficient understanding of the form.
Welearned decision-making strategies for a dia-logue manager from run-time features of ourspoken dialogue system and from observationof human wizards we had embedded within it.Results show that noisy ASR can be resolvedbased on predictions from context about whata user might say, and that dialogue manage-ment strategies for clarifications of linguisticform benefit from access to features from spo-ken language understanding.1 IntroductionUtterances have literal meaning derived from theirlinguistic form, and pragmatic intent, the actionsspeakers aim to achieve through words (Austin,1962).
Because the channel is usually not noisyenough to impede communication, misunderstand-ings that arise between adult human interlocutorsare more often due to confusions about intent, ratherthan about words.
Between humans and machines,however, verbal interaction has a much higher rateof linguistic misunderstandings because the channelis noisy, and machines are not as adept at using spo-ken language.
It is difficult to arrive at accurate ratesfor misunderstandings of form versus intent in hu-man conversation, because the two types cannot al-ways be distinguished (Schlangen and Fern?andez,2005).
However, one estimate of the rate of mis-understandings of literal meaning between humans,based on text transcripts of the British National Cor-pus, is in the low range of 4% (Purver et al, 2001),compared with a 30% estimate for human-computerdialogue (Rieser and Lemon, 2011).
The thesisof our work is that misunderstandings of linguis-tic form in human-machine dialogue are more ef-fectively resolved through greater reliance on con-text, and through closer integration of spoken lan-guage understanding (SLU) with dialogue manage-ment (DM).
We investigate these claims by focusingon noisy speech recognition for utterances where theuser?s specific intent requires little additional infer-ence, given sufficient understanding of the form.This paper presents three experiments that pro-gressively address SLU methods to compensate forpoor automated speech recognition (ASR), and com-plementary DM strategies.
In two of the experi-ments, human wizards are embedded in the spokendialogue system while run-time SLU features arecollected.
Many wizard-of-Oz investigations haveaddressed the noisy channel issue for SDS (Zollo,1999; Skantze, 2003; Williams and Young, 2004;Skantze, 2005; Rieser and Lemon, 2006; Schlangenand Fern?andez, 2005; Rieser and Lemon, 2011).Like them, we study how human wizards solve thejoint problem of interpreting users?
words and in-ferring users?
intents.
Our work differs in its ex-ploration of the role context can play in the literalinterpretation of noisy language.
We rely on knowl-edge in the backend database to propose candidatelinguistic forms for noisy ASR.Our principal results are that both wizards and our248SDS can achieve high accuracy interpretations, in-dicating that predictions about what the user mightbe saying can play a significant role in resolvingnoise.
We show it is possible to achieve low ratesof unresolved misunderstanding, even at word errorrates (WER) as poor as 50%-70%.
We achieve thisthrough machine learned models of DM actions thatcombine standard DM features with a rich numberand variety of SLU features.
The learned modelspredict DM actions to determine whether a reliablecandidate interpretation exists for a noisy utterance,and if not, what action to take.
The results supportan approach to DM design that integrates the twoproblems of understanding form and intent.The next sections present related work, our librarydomain and our baseline SDS architecture.
Subse-quent sections discuss the SLU settings across thethree experiments, and present the experimental de-signs and results, discussion and conclusion.2 Related WorkPrevious WOz studies of wizards?
ability to pro-cess noisy transcriptions of speaker utterances in-clude the use of real (Skantze, 2003; Zollo, 1999)or simulated ASR (Kruijff-Korbayova?
et al, 2005;Williams and Young, 2004).
WOz studies thatdirected their attention to the wizard include ef-forts to predict: the wizard?s response when theuser is not understood (Bohus 2004); the wizard?suse of multimodal clarification strategies (Rieserand Lemon, 2006; Rieser and Lemon, 2011); andthe wizard?s use of application-specific clarificationstrategies (Skantze, 2003; Skantze, 2005).
WOzstudies that address real or simulated ASR revealthat wizards can find ways to not respond to utter-ances they fail to understand (Zollo, 1999; Skantze,2003; Kruijff-Korbayova?
et al, 2005; Williams andYoung, 2004).
For example, they can prompt theuser for an alternative attribute of the same object.Our work differs in that we address clarificationsabout the words used, and rely on a rich set of SLUfeatures.
Further, we compare behavior across wiz-ards.
Our SDS benefits from models of the mostskilled wizards.To limit communication errors incurred by faultyASR, an SDS can rely on strategies to detect and re-spond to incorrect recognition output (Bohus, 2004).The SDS can repeatedly request user confirmationto avoid misunderstanding, or ask for confirmationusing language that elicits responses from the userthat the system can handle (Raux and Eskenazi,2004).
When the user adds unanticipated informa-tion in response to a system prompt, two-pass recog-nition can rely on a concept-specific language modelto improve the recognition of the domain conceptswithin the utterance containing unknown words, andthereby achieve better recognition (Stoyanchev andStent, 2009).
An SDS could take this approach onestep further and use context-specific language for in-cremental understanding of noisy input throughoutthe dialogue (Aist et al, 2007).Current work on error recovery and grounding forSDS assumes that the primary responsibility of adialogue management strategy is to understand theuser?s intent.
Errors of understanding are addressedby ignoring the utterances where understanding fail-ures occur, asking users to repeat, or pursuing clari-fications about intent.
These strategies typically relyon knowledge sources that follow the SLU stage.The RavenClaw dialogue manager, which representsdomain-dependent (task-based) DM strategy as atree of goals, triggers error handling by means of asingle confidence score associated with the conceptshypothesized to represent the user?s intent (Bohusand Rudnicky, 2002; Bohus and Rudnicky, 2009).Features for reinforcement learning of MDP-basedDM strategies include a few lexical features and ameasure of noise analogous to WER (Rieser andLemon, 2011).
The WOz studies reported here yieldlearned models of specific actions in response tonoisy input, such as whether to treat a candidate in-terpretation as correct, or to pursue one of many pos-sible clarification strategies, including clarificationsof form or intent.
These models rely on relativelylarge numbers of features from all phases of spokenlanguage understanding, as well as on typical dia-logue management features.3 CheckItOut3.1 DomainOur domain of investigation simulates book ordersfrom the Andrew Heiskell Braille and Talking BookLibrary, part of the New York Public Library and theLibrary of Congress.
Patrons order books by tele-249phone during conversation with a librarian, and re-ceive them by mail.
Patrons typically have identify-ing information for the books they seek, which theyget from monthly newsletters.
In a corpus of eightytwo calls recorded at the library, we found that mostbook requests by title were very faithful to the actualtitle.
Challenges to SLU in this domain include thesize of the database, the size of the vocabulary, andthe average sentence length.While large databases have been used for inves-tigations of phonological query expansion (Georgilaet al, 2003), much of the research on DM strategyrelies on relatively small databases.
A recent studyof reinforcement learning of DM strategy modeledas a Markov Decision Process reported in (Rieserand Lemon, 2011) relies on a database of 438 items.In (Gordon and Passonneau, 2011) we comparedthe SLU challenges faced by CheckItOut and theLet?s Go bus schedule information system, both ofwhich rely on the same architecture (Raux et al,2005).
The Let?s Go corpus contained 70 bus routesnames and 1300 place names, and a mean utterancelength of 4.4 words.
The work reported here uses thefull 2007 version of Heiskell?s database of 71,166books and 28,031 authors, and a sanitized versionof its 2007 patron database of 5,028 active patrons.Authors and titles contribute 45,636 distinct words,with a 10.43% overlap between the two.
Averagebook title length is 5.4 words; 26% of titles are 1-2words, 44% are 3-5 words, 20% are 6 to 10.
Con-sequently, our domain has relatively long utterances.The syntax of book titles is much richer than typicalSDS slot fillers, such as place or person names.To achieve high-confidence SLU, we integratevoice search into the SLU components of our twoSDS experiments (Wang et al, 2008).1 Our customvoice search query relies on Ratcliff/Obershershelp(R/O) pattern matching (Ratcliff and Metzener,1988), the ratio of the number of matching charac-ters to the total length of both strings.
This simplemetric captures gross similarities without overfittingto a specific application domain.
The criteria for se-lecting R/O derive from our first offline experiment,described in Section 4.2.For an experiment focused only on a single turn1In concurrent work on a new SDS architecture, we use en-sembles of SLU strategies (Gordon and Passonneau, 2011; Gor-don et al, 2011).
(a) Baseline CheckItOut(b) Embedded WizardFigure 1: CheckItOut information pipelineexchange beginning with a user book request, wequeried the backend directly with the ASR string.For a subsequent experiment on full dialogues, wequeried the backend with a modified ASR string, be-cause the SDS architecture we used permits backendqueries to occur only during the dialogue manage-ment phase, after natural language understanding.The next section describes this architecture.3.2 ArchitectureCheckItOut, our baseline SDS, employs the Olym-pus/RavenClaw architecture developed at CarnegieMellon University (CMU) (Raux et al, 2005; Bo-hus and Rudnicky, 2009).
SDS modules commu-nicate via message passing, controlled by a centralhub.
However, the information flow is largely apipeline, as depicted in Figure 1(a).
The Pocket-Sphinx recognizer (Huggins-Daines et al, 2006) re-ceives acoustic data segmented by the audio man-ager, and passes a single recognition hypothesis tothe Phoenix parser (Ward and Issar, 1994).
Phoenixsends one or more equivalently ranked semanticparses to the Helios confidence annotator (Bohusand Rudnicky, 2002), which selects a parse and as-signs a confidence score.
The Apollo interactionmanager (Raux and Eskenazi, 2007) monitors thethree SLU modules?the recognizer, the semanticparser, and the confidence annotator?to determinewhether the user or SDS has the current turn.
Toa limited degree, Apollo can override the early seg-mentation decisions based solely on pause length.250Confidence-annotated concepts from the semanticparse are passed to the RavenClaw DM, which de-cides when to prompt the user, present informationto her, or query the backend database.A wizard server communicates with other mod-ules via the hub, as shown in Figure 1(b).
For eachwizard experiment, we constructed a graphical userinterface (GUI).
Wizard GUIs display informationfor the wizard in a manageable form, and allow thewizard to query the backend or select communica-tive actions that result in utterances directed to theuser.
Figure 1(b) shows an arrow from the speechrecognizer directly to the wizard: the recognitionstring has been vetted by Apollo before it is dis-played to the wizard.4 Experiments and ResultsThe experiments reported here are an off-line pilotstudy to identify book titles under worst case recog-nition (Title Pilot), an embedded WOz study of asingle turn exchange involving book requests by ti-tle (Turn Exchange), and an embedded WOz studyof dialogues where users followed scenarios that in-cluded four books at a time (Full WOz).
To evaluatethe impact of learned models of wizard actions fromthe Full WOz wizard data, we evaluated CheckItOutbefore and after the dialogue manager was enhancedwith wizard models for specific actions.4.1 Experimental SettingsAll three experiments use the full database forsearch.
To control for WER, the knowledge sourcesfor speech recognition and semantic parsing varyacross experiments.
For each experiment, Table 1indicates the acoustic model (AM) used, the num-ber of hours of domain-specific spontaneous speechused for AM adaptation, the number of titles usedto construct the language model (LM), the type ofLM, the type of grammar rules in the Phoenix booktitle subgrammar, and average WER as measured byLevenstein word edit distance (Levenshtein, 1996).For the first two experiments, we used CMU?sOpen Source WSJ1 dictation AMs for wideband(16kHz) microphone (dictation) speech.
For FullWOz we adapted narrowband (8kHz) WSJ1 dicta-tion speech with about eight hours of data collectedfrom Turn Exchange and two hours of scripted spon-taneous speech typical of CheckItOut dialogues.Logios is a CMU toolkit for generating a pseudo-corpus from a Phoenix grammar.
It produces a setof strings generated by Phoenix production rules,which in turn are used to build an LM (CarnegieMellon University Speech Group, 2008).
Before weexplain the three rightmost columns in Table 1, wefirst briefly describe Phoenix, the Phoenix book titlesubgrammar, and how we combine title strings witha Logios pseudo-corpus.Phoenix is a context-free grammar (CFG) parserthat produces one or more semantic frames perparse.
A semantic frame has slots, where each slot isa concept with its own CFG productions (subgram-mar).
To accommodate noisy ASR, the parser canskip words between frames or slots.
Phoenix is well-suited for restricted domains, where a frame repre-sents a particular type of subdialogue (e.g., orderinga plane ticket), and slots represent constrained con-cepts (e.g., departure city, destination city).
Phoenixis not well-suited for book titles, which have a richvocabulary and syntax, and no obvious componentslots.
The CFG rules for the Turn Exchange book ti-tle subgrammar consisted of a verbatim rule for eachbook title.
Rules that consisted of a bag-of-words(BOW; i.e., unordered) for each title proved to betoo unconstrained.2 In Turn Exchange, interpreta-tion of ASR consisted primarily of voice search; thehighly constrained CFG rules (exact words in exactorder) had little impact on performance.
For base-line CheckItOut dialogues, and for Full WOz, werequired more constrained grammar rules that wouldpreserve Phoenix?s robustness to noise.To avoid the brittleness of exact string CFG rules,and the massive over-generation of BOW CFG rules,we wrote a transducer that mapped dependencyparses of book titles to CFG rules.
When ASRwords are skipped, book title parses can consist ofmultiple slots.
We used MICA, a broad-coveragedependency grammar (Bangalore et al, 2009) toparse the entire book title database.
When a setof titles is selected for an experiment, the corre-sponding MICA parses are transduced to the rele-vant CFG productions, and inserted into a Phoenixgrammar.
Productions for the author subgrammar2BOW Phoenix rules for book titles are used in a more re-cent Olympus/RavenClaw system inspired in part by Check-ItOut (Lee et al, 2010), with a database of 15,088 eBooks.251Exp.
AM Adapted # Titles for LM LM Grammar rules WERTitle Pilot WSJ1 16kHz NA 500 unigram NA 0.76Turn Exchange WSJ1 16kHz NA 7,500 trigram title strings 0.71Full WOz WSJ1 8kHz 10 hr.
3,000 Logios + book data Mica-based 0.50 (est)Table 1: SLU settings across experimentsconsist largely of a first name slot followed by a lastname slot.
The remaining portions of the PhoenixCheckItOut grammar consist of subgrammars forbook request prefixes and affixes (e.g., ?I would likethe book called?
), for confirmations and rejections,phone numbers, book catalogue numbers, and mis-cellaneous additional concepts.
The set of subgram-mars excluding the book title and author subgram-mars (book requests, confirmations, and so on; thegrammar shell) are the same for all experiments.The MICA-based book title grammar also providesseveral features (e.g., number of slots in a parse) formachine learning.The Title Pilot LM consisted of unigram frequen-cies of the 1400 word types from a random sample(without replacement) of 500 titles.
For Turn Ex-change, a trigram LM was constructed from 7,500titles randomly selected from the 19,708 titles thatremained after we eliminated one-word titles and ti-tles with below average circulation.
For Full WOz,3,000 books were randomly selected from the fullbook database (with no more than three titles bythe same author, and no one-word titles).
Logioswas used on the grammar shell to generate an initialpseudo-corpus, which was combined with the booktitle and author strings to generate a full pseudo-corpus for the trigram LM (denoted as ?Logios +book data?
in Table 1).4.2 Title PilotThe Title Pilot (Passonneau et al, 2009) was an of-fline investigation of how reliance on prior knowl-edge in the database might facilitate interpretationof noisy ASR.
It demonstrates that given the contextof things a user might say, ASR that is otherwise un-intelligible becomes intelligible.Three males each read 50 randomly selected ti-tles from the LM subset of 500 (see Table 1).
Theiraverage WER was 0.75, 0.83 and 0.69, respectively.Three undergraduates (A, B, C) were each given oneof the sets of 50 recognition strings from a differentspeaker.
Each also received a plain text file listing allthe titles in the database, and word frequency statis-tics for the book titles.
Their task was to try to findthe correct title, and to provide a brief description oftheir overall strategy.A was accurate on 66.7% of the titles he matched,B and C on 71.7%.
We identified similar strate-gies for A and B, including number of exact wordmatches, types of exact word matches (e.g., contentwords were favored over stop words), rarity of ex-act word matches, and phonetic similarity.
Analysisof C?s responses showed dependency on number andtypes of exact word matches, and on miscellaneousstrategies that could not be grouped.
Through in-spection, we determined that similarity in length andnumber of words were important factors.
From thisexperiment, we concluded that humans are adept atinterpreting noisy ASR when provided with context;that voice search (queries to the backend with ASR)would prove useful, given an appropriate similaritymetric; and that there would likely always be uncer-tain cases that might lead to false hits.
As we discussbelow, two of seven Turn Exchange wizards werefairly adept, and five of six Full WOz wizards werevery adept, at avoiding false hits from voice search.4.3 Turn ExchangeThe offline Title Pilot suggested that voice searchcould lead to far fewer non-understandings, givensome predictions as to the actual words a noisy ASRstring might represent.
The next experiment ad-dressed, in real time, the question of what level ofaccuracy might be achieved through an online im-plementation of voice search for book requests bytitle (Passonneau et al, 2010; Ligorio et al, 2010b).We embedded wizards into the CheckItOut SDS topresent them with live ASR, and to collect runtimerecognition features.
On the GUI, variations in thedisplay fonts for ASR and voice search returns cuedthe wizard to gross differences in word-level recog-nition confidence, and similarities between an ASRstring and each candidate returned by the search.Learned models of wizard actions indicated that252recognition features such as acoustic model fit andspeech rate, along with various measures of sim-ilarity between the ASR output string and candi-date titles, number of books ordered thus far (Re-centSuccess), and number of relatively close candi-date matches, were useful in modeling the most ac-curate wizards.
These results show that DM strat-egy for determing what actions to take, given an in-terpretation of a user request, can depend on subtlerecognition metrics.In Turn Exchange, users requested books by ti-tle from embedded wizards.
Speech input and out-put was by microphone and headset, with wizardsand users seated in separate rooms, each using a dif-ferent GUI.
Seven undergraduates (one female andsix males, including two non-native speakers of En-glish) participated as paid subjects.
Each of the 21possible pairs of students met for five trials.
A trialhad two sessions.
In the first, one student served aswizard and the other as user for a session in whichthe user requested 20 books by title.
In the secondsession, the students reversed roles.
We collected4,192 turn exchanges.The GUI displayed the ASR corresponding to theuser utterance, with confident words in bolder font.The wizard could query the backend with some orall of the ASR.
Voice search results displayed a sin-gle candidate above a high R/O threshold with allmatching words in boldface, or three candidates ofmoderate similarity with matching words in mediumbold, or five to ten candidates of lower similarity ingrayscale.
There were four available wizard actions:to offer a candidate title to the user in a confidentmanner (through Text-to-Speech), to offer a title ten-tatively, to select two or more candidates and ask afree-form question about them (here the user wouldhear the wizard?s speech), or to give up.
The user in-dicated whether an offered candidate was correct, orindicated the quality and appropriateness of a wiz-ard?s question.
A prize would go to the wizard whooffered the most correct titles.The top ranked search return was correct 65.24%of the time.
The two wizards who most often offeredthe top ranked return (81% and 86% of the time)both achieved 69.5% accuracy.
The two best wiz-ards (W4 and W5) could detect search returns thatdid not contain the correct title, thus avoiding falsehits.
On average, they offered the top return only73% of the time and both achieved the highest accu-racy (83.4%).Several classification methods were used to pre-dict the four wizard actions: firm offer, tentative of-fer, question, and give up.
Features (N=60) includedmany ASR metrics, such as word-level confidence,AM fit, and three measures of speech rate; variousmeasures of the average similarity or overlap be-tween the ASR string and the candidate titles fromthe R/O query; the dialogue history; the number ofcandidates titles returned; and so on.
The learnedclassifiers, including C4.5 decision trees (Quinlan,1993), all had similar performance.
Learned treesfor W4 and W5 both had F measures of 0.85.
De-cision trees give a transparent view of the relativeimportance of features; those nearer the root havegreater discriminatory power.
Common features atthe tops of trees for all wizards were the type andsize of the query return, how often the wizard hadchosen the correct title in the last three title cycles,the average of the maximum number of contiguousexact word matches between the ASR string and thecandidate titles, and the Helios confidence score.We trained an additional decision tree to learnhow W4 (the best wizard) chose between offeringa title versus asking a question (F=0.91 for makingan offer; F=0.68 for asking a question).
The treeis distinctive in that it splits at the root on a mea-sure of speech rate.
If the ASR is short (as mea-sured both by the number of recognition frames andthe words), W4 asks a question if the query returnis not a single title, and either RecentSuccess=1 orContiguousWord-Match=0, and the acoustic modelscore is low.
Note that shorter titles are more con-fusable.
If the ASR is long, W4 asks a questionwhen ContiguousWordMatch=1, RecentSuccess=2,and either CandidateDisplay = NoisyList, or HeliosConfidence is low, and there is a choice of titles.4.4 Full WOzThe third experiment was a full WOz study demon-strating that embedded wizards could achieve hightask success by relying on a large number of actionsthat included clarifications of utterance form or in-tent.
Here we briefly report results on task successand time on task in a comparision of baseline Check-ItOut with an enhanced version, CheckItOut+, thatincorporates learned models of wizard actions.
The253evaluation demonstrates improved performance withmore books ordered, more correct books ordered,and less elapsed time per book, or per correct book.For Full WOz (Ligorio et al, 2010a), CheckItOutrelied on VOIP (Voice over Internet Protocol) tele-phony.
Users interacted with the embedded wizardsby telephone, and wizards took over after Check-ItOut answered the phone.
After familiarizationwith the task and GUI, nine wizards auditioned andsix were selected.
There were ten users.
Both groupswere evenly balanced for gender.
Users were di-rected to a website that presented scenarios for eachcall.
The scenario page gave the user a patron iden-tity and phone number, and author, title and cata-logue number information for four books they wereto order.
Each user was to make at least fifteen callsto each wizard; we recorded 913 usable calls.A single trainer prepared the original nine wizardvolunteers one at a time.
First, each trainee practicedon data from the experiments described above.
Next,the trainer explained the wizard GUI and demon-strated it, serving as wizard on a sample call.
Fi-nally, the trainee served as wizard on five test callswith guidance from the trainer.
The trainer chose thesix most skilled and motivated trainees as wizards.The GUI had two screens, one for user loginand one for book requests.
Users identified them-selves by scenario phone number.
The book re-quest screen had a scrollable frame displaying theASR for each user utterance.
Separate frames onthe GUI displayed the query return, dialogue history,basic actions (e.g., querying the backend with a cus-tom R/O query, or prompting the user for a book),and auxiliary actions (e.g., removing a book fromthe order in progress).
Finally, wizards could selectamong four types of dialogue acts: signals of non-understanding, or clarifications about the ASR, thebook request or the query return.
A dialogue act se-lected by the wizard was passed to a template-basednatural language generator, and then to a Text-to-Speech component.
Due to their complexity, callscould be time consuming.
A clock on the GUI indi-cated call duration; wizards were instructed to finishthe current book request and then terminate the callafter six minutes.A wizard?s precision is the proportion of booksshe offer that correctly match the user?s request; fiveof the six wizards had precision over 90%.
A wiz-ard?s recall is the number of books in the scenariothat she correctly identified.
The two best wizards,WA and WB, had the highest recall, 63% and 67%respectively.The number of book requests per dialogue wastallied automatically.
Some dialogues were termi-nated before all scenario books could be requested.Also, a wizard who experienced problems with abook request could abandon the current request andprompt the user for a new book.
The user could re-sume the abandoned book request later in the dia-logue.
In such cases, the abandoned and resumed re-quests for the same book would count as two distinctbook requests.
Given these facts, the ratio of numberof correct books to number of book requests yieldsonly an approximate estimate of how many scenariobooks were correctly identified.
WA correctly iden-tified 2.69 books per call from 3.64 requests per call,yielding a total success rate of 73.9% per book re-quest, and 67.25% per 4-book scenario.
WB cor-rectly identified 2.54 books per call from 4.44 re-quests per call, yielding success rates of 57.21% perrequest and 63.50% per 4-book scenario.
WA andWB had quite distinct strategies.
WA persisted witheach book request and exploited a wide range ofthe available GUI actions, with the greatest num-ber of actions per book request among all wizards(N=8.24).
WB abandoned book requests early andmoved on to the next book request, exploited rela-tively fewer GUI actions, and had the fewest actionsper book request (N=5.10).From 163 features that characterize the ASR,search, current user utterance, current turn ex-change, current book request, and the entire dia-logue, we learned models for three types of wiz-ard actions: select a non-understanding prompt, per-form a search, or select a prompt to disambiguateamong search returns.
We used three machine learn-ing methods for classification: decision trees, logis-tic regression and support vector machines.
Table 2gives the accuracies and overall F measures for de-cision trees that model WA and WB.
(All learningmethods have similar performance.
)Of note here is the range of features that predictwhen the best wizards selected a non-understanding,shown in Table 3.
In addition, the two models de-pend partly on different features.
Trees for the otheractions in Table 2 have similarly diverse features.254Wizard Action Acc FA Non-Understanding 0.71 0.71B Non-Understanding 0.73 0.73A Disambiguate 0.80 0.81B Disambiguate 0.86 0.87A Search 0.94 0.95B Search 0.93 0.94Table 2: Performance of learned treesTo evaluate the benefit of learned models of wiz-ard actions for SDS, we conducted two data collec-tions where subjects placed calls following the sametypes of scenarios used in Full WOz.
For our base-line evaluation of CheckItOut, 10 subjects were re-cruited from Columbia University and Hunter Col-lege.
Each was to place a minimum of 50 calls overa period of three days; 562 calls were collected.
Foreach call, subjects visited a web page that presenteda new scenario.
Each scenario included mock patrondata for the caller to use (e.g., name, address andphone number), a list of four books, and instructionsto request one book by catalogue number, one bytitle, one by author, and one by any of those meth-ods.
At three points during their calls, subjects com-pleted a user satisfaction survey containing elevenquestions adapted from (Hone and Graham, 2006).CheckItOut+ is an enhanced version of our SDSin which the DM was modified to include learnedmodels for three decisions.
The first determineswhether the system should signal non-understandingin response to the caller?s last utterance, and exe-cutes before voice search would take place.
Thesecond determines whether to perform voice searchwith the ASR (i.e., before the parse, in contrast toCheckItOut).
The third executes after voice search,and determines whether to offer the candidate withthe highest R/O score to the user.
The evaluationsetup for CheckItOut+ also included 10 callers whowere to place 50 calls each; 505 calls were collected.Here we report results that compare the numberof books ordered per call, the number of correctbooks per call, the elapsed time per book ordered,and elapsed time per correct book.
T-tests show alldifferences to be highly significant.
(A full discus-sion of the evaluation results will appear in futurepublications.)
Callers to CheckItOut+ nearly alwaysordered four books (3.998), compared with 3.217 forthe baseline (p < 0.0001).
There was an increaseof correct books in the order from 2.40 in the base-Feature WA WB# books ordered so far Y Y% unparsed ASR words Y NAvg.
word confidence Y N# explicit confirms in call Y Y# MICA slots per concept Y N# searches in call Y NMost recent wizard action N YMost frequent concept in call N YSpeech rate N Y# user utts.
this request N Y# author searches in call Y YNormalized LM score this utt Y YTable 3: Features that predict wizards?
non-understandingline to 2.70 in CheckItOut+ (p < 0.0001).
The totalelapsed time per call increased by only 13 secondsfrom 210.93 to 223.96 (p < 0.0175).
Given thatCheckItOut+ callers ordered more books and morecorrect books, CheckItOut+ performed much faster.The elapsed time per ordered book decreased from65.57 to 56.01 seconds, and decreased from 87.89 to82.95 seconds per correct books.5 DiscussionSpoken language understanding has been relativelyunder-investigated in SDS design.
Our experimentssuggest that tighter integration of all phases of SLUwith dialogue management can lead to more robustsystem behavior.
We illustrate here with an exam-ple of WA?s strategic questioning in which a non-understanding is avoided, and WA builds on partialunderstanding to identify the user?s objective.In response to ASR MARY .JO.
EARTH, wherethe ?.
?s bracket an unconfident word, WA?s searchreturned three authors with first name Mary, andlast names that had moderate character overlap with.JO.
EARTH.
WA first asked whether the bookwas by Mary O?Hara.
When the user respondednegatively, WA confirmed the first name, promptedfor the last name, and got SURE as the ASR.
WA fi-nally confirmed that the book was by Mary Stewart.Although it took four turns, WA was able to identifythe correct book.In general, the Full WOz corpus contains a veryhigh proportion of wizard questions.
In the to-tal corpus of 20,415 caller utterances, there were11,562 wizard questions.
The types of questions255S1: What?s the next book?U1: .BARBARA.
THREE THREES2: Is the author barbara freethy?U2: YESS3: Would you like ?some kind of wonderful?
byBARBARA FREETHY?U3: YES(a) Example 1S1: Sorry, I misunderstood.
Can you repeat the author?U1: DIDN?T I AM THE .GOLDEN.
ARMS2: Is the title ?THE man with THE golden ARM ?
?U2: NOS3: Sorry, I misunderstood.
Can you repeat the title please?U3: .A.
.AMBLING.
.THE.
.GAME.
.EDELMAN.
STORYS4: Is the title ?up and running the jami goldman STORY ?
?U4: YES(b) Example 2Figure 2: Sample Clarification Subdialogueswizard?s ask not only often lead to successful con-cept identification, they also avoid prompting theuser to repeat what they said.
Previous work haspresented results showing that the hyperarticulationassociated with user repetitions often leads users toslow their speech, speak more loudly, and pronouncewords more carefully, which hurts recognition per-formance (Hirschberg et al, 2004).Figure 2 illustrates two clarification subdialoguesfrom CheckItOut+.
The first illustrates how priorknowledge about what a user might say providessufficient constraints to interpret ASR that wouldotherwise be unintelligible.
The first word in theASR for the caller?s first utterance is bracketed by?.
?, which again represents low word confidence.The high confidence words THREE THREE arephonologically and orthographically similar to theactual author name, Freethy.
Note that from thecaller?s point of view, the same question shownin S3 could be motivated by confusion over thewords alone, as in this case, or confusion over thewords and multiple candidate referents (e.g., Bar-bara Freethy versus Freeling).The second clarification subdialogue illustrateshow confusions about the linguistic input can beresolved through strategies that combine questionsabout words and intents.
The prompt at system turn3 indicates that the system believes that the callerprovided a title in user turn 1, which is incorrect.The caller responds with the title, however, whichprovides an alternative means to guess the intendedbook, Jami Goldman?s memoir Up and Running.6 ConclusionThe studies reported here are premised on two hy-potheses about the role spoken language understand-ing plays in SDS design.
First, prior knowledgederived from the context in which a dialogue takesplace can yield predictions about the words a usermight produce, and that these predictions can playa key role in interpreting noisy ASR.
Here we haveused context derived from knowledge in the appli-cation database.
Similar results could follow frompredictions from other sources, such as an explicitmodel of the alignment of linguistic representa-tions proposed in the work of Pickering and Gar-rod (e.g., (Pickering and Garrod, 2006).
Second,closer integration of spoken language understandingand dialogue management affords a wider range ofclarification subdialogues.Our results from the experiments reported heresupport both hypotheses.
Our first experimentdemonstrated that words obscured by very noisyASR (50% ?
WER ?
75%) can be inferred by re-liance on what might have been said, predictionsthat came from the database of entities in the do-main.
We assume that an SDS that interacts wellwhen ASR quality is poor will perform all the betterwhen ASR quality is good.
Our second experimentdemonstrated that two of five human wizards wereable to achieve high accuracy in on-line resolutionof noisy ASR, when presented with no more than tencandidate matches.
Run-time recognition featuresnot available to the wizards were nonetheless usefulin modeling the ability of the two best wizards toavoid false hits.
Our third experiment demonstratedthat wizards could achieve high task success on fulldialogues where callers requested four books, andan enhancement of our baseline SDS with learnedmodels of three wizard actions led to improved tasksuccess with less time per subtask.
The variety offeatures that contribute to learned models of wiz-ard actions demonstrates the advantages of embed-ded wizardry, as well as the benefit of DM clarifica-tion strategies that include features from all phasesof SLU.256AcknowledgmentsThe Loqui project is funded by the National ScienceFoundation under awards IIS-0745369, IIS-0744904and IIS-084966.
We thank those at Carnegie Mel-lon University who helped us construct Check-ItOut through tutorials and work sessions held atColumbia University and Carnegie Mellon Univer-sity, and who responded to numerous emails aboutthe Olympus/RavenClaw architecture and compo-nent modules: Alex Rudnicky, Brian Langner,David Huggins-Daines, and Antoine Raux.
We alsothank the many undergraduates from Columbia Col-lege, Barnard College, and Hunter College who as-sisted with tasks that supported the implementationof CheckItOut, including the telephony.ReferencesGregory Aist, James Allen, Ellen Campana, Car-los Gomez Gallo, Scott Stoness, Mary Swift, andMichael K. Tanenhaus.
2007.
Incremental dialoguesystem faster than and preferred to its nonincrementalcounterpart.
In COGSCI 2007, pages 779?74.John L. Austin.
1962.
How to Do Things with Words.Oxford University Press, New York.Srinivas Bangalore, Pierre B. Boullier, Alexis Nasr,Owen Rambow, and Beno?
?it Sagot.
2009.
Mica: aprobabilistic dependency parser based on tree insertiongrammars.
In NAACL/HLT, pages 185?188.Dan Bohus and Alex Rudnicky.
2002.
Integrating multi-ple knowledge sources for utterance-level confidenceanno-tation in the CMU Communicator spoken dia-logue system.
Technical Report CS-02-190, CarnegieMellon University, Department of Computer Science.Dan Bohus and Alex Rudnicky.
2009.
The RavenClawdialog management framework.
Computer Speech andLanguage, 23:332?361.Dan Bohus.
2004.
Error awareness and recovery in con-versational spoken language interfaces.
Ph.D. thesis,Carnegie Mellon University, Computer Science.Carnegie Mellon University Speech Group.
2008.The Logios tool.
https://cmusphinx.svn.sourceforge.net/svnroot/cmusphinx/trunk/logios.Kallirroi Georgila, Kyrakos Sgarbas, AnastasiosTsopanoglou, Nikos Fakotakis, and George Kokki-nakis.
2003.
A speech-based human-computerinteraction system for automating directory assistanceservices.
International Journal of Speech Technology,Special Issue on Speech and Human-ComputerInteraction, 6:145?59.Joshua Gordon and Rebecca J. Passonneau.
2011.An evaluation framework for natural language under-standing in spoken dialogue systems.
In 7th LREC.Joshua Gordon, Rebecca J. Passonneau, and Susan L. Ep-stein.
2011.
Helping agents help their users despiteimperfect speech recognition.
In Proceedings of theAAAI Spring Symposium 2011 (SS11): Help Me HelpYou: Bridging the Gaps in Human-Agent Collabora-tion.Julia Hirschberg, Diane Litman, and Marc Swerts.
2004.Prosodic and other cues to speech recognition failures.Speech Communication, 43(1-2):155?75.Kate S. Hone and Robert Graham.
2006.
Towards a toolfor the subjective assessment of speech system inter-faces (sassi).
Natural Language Engineering, SpecialISsue on Best Practice in Spoken Dialogue Systems,6(3-4):287?303.David Huggins-Daines, Mohit Kumar, Arthur Chan,Allen W. Black, Mosur Ravishankar, and Alex I. Rud-nicky.
2006.
PocketSphinx: A free, real-time contin-uous speech recognition system for hand-led devices.In Proceedings of ICASSP, volume I, pages 185?188.Ivana Kruijff-Korbayova?, Nate Blaylock, Ciprian Ger-stenberger, Verena Rieser, Tilman Becker, MichaelKaisser, Peter Poller, and Jan Schehl.
2005.
An ex-periment setup for collecting data for adaptive outputplanning in a multimodal dialogue system.
In 10thENLG, pages 191?196.Cheongjae Lee, Alexander Rudnicky, and Gary GeunbaeLee.
2010.
Let?s buy books: finding ebooks usingvoice search.
In IEEE-SLT 2010, pages 442?447.Vladimir I. Levenshtein.
1996.
Binary codes capable ofcorrecting deletions, insertions and reversals.
SovietPhysics Doklady, 10(8):707?710.Tiziana Ligorio, Susan L. Epstein, and Rebecca J. Pas-sonneau.
2010a.
Wizards?
dialogue strategies to han-dle noisy speech recognition.
In IEEE-SLT 2010.Tiziana Ligorio, Susan L. Epstein, Rebecca J. Passon-neau, and Joshua Gordon.
2010b.
What you didand didn?t mean: Noise, context and human skill.
InCOGSCI 10.Rebecca J. Passonneau, Susan L. Epstein, and JoshuaGordon.
2009.
Help me understand you: Address-ing the speech recognition bottleneck.
In Proceedingsof the AAAI Spring Symposium 2009 (SS09): Agentsthat Learn from Human Teachers, pages 23?25.Rebecca J. Passonneau, Susan L. Epstein, Tiziana Ligo-rio, Joshua Gordon, and Pravin Bhutada.
2010.
Learn-ing about voice search for spoken dialogue systems.
InNAACL-HLT 2010, pages 840?848.Martin J. Pickering and Simon Garrod.
2006.
Alignmentas the basis for successful communication.
Researchon Language and Communication, 4(2):203?228.Matthew Purver, Jonathan Ginzburg, and Patrick Healey.2001.
On the means for clarification in dialogue.In Proceedings of the 2nd SIGdial Workshop on Dis-course and Dialogue, pages 116?125.257J.
Ross Quinlan.
1993.
C4.5: Programs for MachineLearning.
Morgan Kaufmann, San Mateo, CA.John W. Ratcliff and David Metzener.
1988.
Patternmatching: the gestalt approach.Antoine Raux and Maxine Eskenazi.
2004.
Non-nativeusers in the Let?s Go!
spoken dialogue systems.
InHLT/NAACL, pages 217?224.Antoine Raux and Maxine A. Eskenazi.
2007.
A multi-layer architecture for semi-synchronous event-drivendialogue management.
In ASRU 2007, pages 514?519.Antoine Raux, Brian Langner, Allan W. Black, and Max-ine Eskenazi.
2005.
Let?s Go Public!
taking a spokendialogue system to the real world.
In Interspeech - Eu-rospeech 2005, pages 885?888.Verena Rieser and Oliver Lemon.
2006.
Using ma-chine learning to explore human multimodal clarifica-tion strategies.
In COLING/ACL, pages 659?666.Verena Rieser and Oliver Lemon.
2011.
Learning andevaluation of dialogue strategies for new applications:Empirical methods for optimization from small datasets.
Computational Linguistics, 37:153?96.David Schlangen and Raquel Fern?andez.
2005.
Speak-ing through a noisy channel ?
experiments on induc-ing clarification behaviour in human-human diaogue.In 8th Annual Converence of the International SpeechCommunication Association (INTERSPEECH 2007),pages 1266?1269.Gabriel Skantze.
2003.
Exploring human error handlingstrategies: Implications for spoken dialogue systems.In Proceedings of ISCA Tutorial and Research Work-shop on Error Handling in Spoken Dialogue Systems,pages 71?76.Gabriel Skantze.
2005.
Exploring human recoverystrategies: Implications for spoken dialogue systems.Speech Communication, 45:325?41.Svetlana Stoyanchev and Amanda Stent.
2009.
Predict-ing concept types in user corrections in dialog.
InEACL Workshop SRSL, pages 42?49.Ye-Yi Wang, Yu Dong, Yun-Cheng Ju, and Alex Acero.2008.
An introduction to voice search.
IEEE SignalProcessing Magazine: Special ISsue on Spoken Lan-guage Technology, 25(3):28?38.Wayne Ward and Sunil Issar.
1994.
Recent improve-ments in the CMU spoken language understandingsystem.
In Proceedings of the ARPA Human LanguageTechnology Workshop, pages 213?216.Jason D. Williams and Steve Young.
2004.
Characteriz-ing task-oriented dialog using a simulated ASR chan-nel.
In ICSLP/Interspeech, pages 185?188.Teresa Zollo.
1999.
A study of human dialogue strate-gies in the presence of speech recognition errors.
InProceedings of the AAAI Fall Symposium on Psycho-logical Models of Communication in CollaborativeSystems, pages 132?139.258
