Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 141?151,Vancouver, October 2005. c?2005 Association for Computational LinguisticsConstituent Parsing by ClassificationJoseph Turian and I. Dan Melamed{lastname}@cs.nyu.eduComputer Science DepartmentNew York UniversityNew York, New York 10003AbstractOrdinary classification techniques candrive a conceptually simple constituentparser that achieves near state-of-the-artaccuracy on standard test sets.
Here wepresent such a parser, which avoids someof the limitations of other discriminativeparsers.
In particular, it does not placeany restrictions upon which types of fea-tures are allowed.
We also present sev-eral innovations for faster training of dis-criminative parsers: we show how train-ing can be parallelized, how examplescan be generated prior to training with-out a working parser, and how indepen-dently trained sub-classifiers that havenever done any parsing can be effectivelycombined into a working parser.
Finally,we propose a new figure-of-merit for best-first parsing with confidence-rated infer-ences.
Our implementation is freely avail-able at: http://cs.nyu.edu/?turian/software/parser/1 IntroductionDiscriminative machine learning methods have im-proved accuracy on many NLP tasks, such as POS-tagging (Toutanova et al, 2003), machine translation(Och & Ney, 2002), and relation extraction (Zhao &Grishman, 2005).
There are strong reasons to believethe same would be true of parsing.
However, onlylimited advances have been made thus far, perhapsdue to various limitations of extant discriminativeparsers.
In this paper, we present some innovationsaimed at reducing or eliminating some of these lim-itations, specifically for the task of constituent pars-ing:?
We show how constituent parsing can be per-formed using standard classification techniques.?
Classifiers for different non-terminal labels can beinduced independently and hence training can beparallelized.?
The parser can use arbitrary information to evalu-ate candidate constituency inferences.?
Arbitrary confidence scores can be aggregated ina principled manner, which allows beam search.In Section 2 we describe our approach to parsing.
InSection 3 we present experimental results.The following terms will help to explain our work.A span is a range over contiguous words in the in-put sentence.
Spans cross if they overlap but nei-ther contains the other.
An item (or constituent) isa (span, label) pair.
A state is a set of parse items,none of which may cross.
A parse inference is a pair(S , i), given by the current state S and an item i to beadded to it.
A parse path (or history) is a sequenceof parse inferences over some input sentence (Klein& Manning, 2001).
An item ordering (ordering, forshort) constrains the order in which items may be in-ferred.
In particular, if we prescribe a complete itemordering, the parser is deterministic (Marcus, 1980)and each state corresponds to a unique parse path.For some input sentence and gold-standard parse, astate is correct if the parser can infer zero or moreadditional items to obtain the gold-standard parse.
Aparse path is correct if it leads to a correct state.
An141inference is correct if adding its item to its state iscorrect.2 Parsing by ClassificationRecall that with typical probabilistic parsers, ourgoal is to output the parse ?P with the highest like-lihood for the given input sentence x:?P = arg maxP?P(x)Pr(P) (1)= arg maxP?P(x)?I?PPr(I) (2)or, equivalently,= arg maxP?P(x)?I?Plog(Pr(I)) (3)where each I is a constituency inference in the parsepath P.In this work, we explore a generalization in whicheach inference I is assigned a real-valued confidencescore Q(I) and individual confidences are aggre-gated using some function A, which need not be asum or product:?P = arg maxP?P(x)AI?PQ(I) (4)In Section 2.1 we describe how we induce scoringfunction Q(I).
In Section 2.2 we discuss the aggre-gation function A.
In Section 2.3 we describe themethod used to restrict the size of the search spaceover P(x).2.1 Learning the Scoring Function Q(I)During training, our goal is to induce the scoringfunction Q, which assigns a real-valued confidencescore Q(I) to each candidate inference I (Equa-tion 4).
We treat this as a classification task: If infer-ence I is correct, we would like Q(I) to be a positivevalue, and if inference I is incorrect, we would likeQ(I) to be a negative value.Training discriminative parsers can be computa-tionally very expensive.
Instead of having a singleclassifier score every inference, we parallelize train-ing by inducing 26 sub-classifiers, one for each con-stituent label ?
in the Penn Treebank (Taylor, Mar-cus, & Santorini, 2003): Q(I?)
= Q?(I?
), whereQ?
is the ?-classifier and I?
is an inference that in-fers a constituent with label ?.
For example, the VP-classifier QVP would score the VP-inference in Fig-ure 1, preferably assigning it a positive confidence.Figure 1 A candidate VP-inference, with head-children annotated using the rules given in (Collins,1999).VP (was)NP (timing) VBD / was ADJP (perfect)DT / The NN / timing JJ / perfectEach ?-classifier is independently trained on trainingset E?, where each example e?
?
E?
is a tuple (I?, y),I?
is a candidate ?-inference, and y ?
{?1}.
y = +1 ifI?
is a correct inference and ?1 otherwise.
This ap-proach differs from that of Yamada and Matsumoto(2003) and Sagae and Lavie (2005), who parallelizeaccording to the POS tag of one of the child items.2.1.1 Generating Training ExamplesOur method of generating training examples doesnot require a working parser, and can be run prior toany training.
It is similar to the method used in theliterature by deterministic parsers (Yamada & Mat-sumoto, 2003; Sagae & Lavie, 2005) with one ex-ception: Depending upon the order constituents areinferred, there may be multiple bottom-up paths thatlead to the same final parse, so to generate trainingexamples we choose a single random path that leadsto the gold-standard parse tree.1 The training ex-amples correspond to all candidate inferences con-sidered in every state along this path, nearly all ofwhich are incorrect inferences (with y = ?1).
Forinstance, only 4.4% of candidate NP-inferences arecorrect.2.1.2 Training AlgorithmDuring training, for each label ?
we induce scor-ing function Q?
to minimize the loss over trainingexamples E?:Q?
= arg minQ???
(I?,y)?E?L(y ?
Q??(I?))
(5)1 The particular training tree paths used in our experiments areincluded in the aforementioned implementation so that ourresults can be replicated under the same experimental condi-tions.142where y ?
Q?(I?)
is the margin of example (I?, y).Hence, the learning task is to maximize the marginsof the training examples, i.e.
induce scoring functionQ?
such that it classifies correct inferences with pos-itive confidence and incorrect inferences with nega-tive confidence.
In our work, we minimized the lo-gistic loss:L(z) = log(1 + exp(?z)) (6)i.e.
the negative log-likelihood of the training sam-ple.Our classifiers are ensembles of decisions trees,which we boost (Schapire & Singer, 1999) to min-imize the above loss using the update equationsgiven in Collins, Schapire, and Singer (2002).
Morespecifically, classifier QT?
is an ensemble comprisingdecision trees q1?, .
.
.
, qT?
, where:QT?
(I?)
=T?t=1qt?(I?)
(7)At iteration t, decision tree qt?
is grown, its leavesare confidence-rated, and it is added to the ensemble.The classifier for each constituent label is trained in-dependently, so we henceforth omit ?
subscripts.An example (I, y) is assigned weight wt(I, y):2wt(I, y) = 11 + exp(y ?
Qt?1(I)) (8)The total weight of y-value examples that fall in leaff is W tf ,y:W tf ,y =?(I,y?
)?Ey?=y, I?
fwt(I, y) (9)and this leaf has loss Ztf :Ztf = 2 ?
?W tf ,+ ?Wtf ,?
(10)Growing the decision tree: The loss of the entiredecision tree qt isZ(qt) =?leaf f?qtZtf (11)2 If we were to replace this equation with wt(I, y) =exp(y?Qt?1(I))?1, but leave the remainder of the algorithm un-changed, this algorithm would be confidence-rated AdaBoost(Schapire & Singer, 1999), minimizing the exponential lossL(z) = exp(?z).
In preliminary experiments, however, wefound that the logistic loss provided superior generalizationaccuracy.We will use Zt as a shorthand for Z(qt).
When grow-ing the decision tree, we greedily choose node splitsto minimize this Z (Kearns & Mansour, 1999).
Inparticular, the loss reduction of splitting leaf f us-ing feature ?
into two children, f ?
?
and f ?
?
?, is?Ztf (?
):?Ztf (?)
= Ztf ?
(Ztf??
+ Ztf???)
(12)To split node f , we choose the ??
that reduces lossthe most:??
= arg max???
?Ztf (?)
(13)Confidence-rating the leaves: Each leaf f isconfidence-rated as ?tf :?tf =12?
logW tf ,+ + W tf ,?
+ (14)Equation 14 is smoothed by the  term (Schapire& Singer, 1999) to prevent numerical instability inthe case that either W tf ,+ or Wtf ,?
is 0.
In our ex-periments, we used  = 10?8.
Although our exam-ple weights are unnormalized, so far we?ve foundno benefit from scaling  as Collins and Koo (2005)suggest.
All inferences that fall in a particular leafnode are assigned the same confidence: if inferenceI falls in leaf node f in the tth decision tree, thenqt(I) = ?tf .2.1.3 Calibrating the Sub-ClassifiersAn important concern is when to stop growing thedecision tree.
We propose the minimum reductionin loss (MRL) stopping criterion: During training,there is a value ?t at iteration t which serves as athreshold on the minimum reduction in loss for leafsplits.
If there is no splitting feature for leaf f thatreduces loss by at least ?t then f is not split.
For-mally, leaf f will not be bisected during iteration t ifmax???
?Ztf (?)
< ?t.
The MRL stopping criterionis essentially `0 regularization:?t corresponds to the`0 penalty parameter and each feature with non-zeroconfidence incurs a penalty of ?t, so to outweigh thepenalty each split must reduce loss by at least ?t.
?t decreases monotonically during training atthe slowest rate possible that still allows train-ing to proceed.
We start by initializing ?1 to ?,and at the beginning of iteration t we decrease ?tonly if the root node ?
of the decision tree can-not be split.
Otherwise, ?t is set to ?t?1.
Formally,143?t = min(?t?1,max???
?Zt?(?)).
In this manner, thedecision trees are induced in order of decreasing ?t.During training, the constituent classifiers Q?never do any parsing per se, and they train at dif-ferent rates: If ?
, ?
?, then ?t?
isn?t necessarilyequal to ?t??
.
We calibrate the different classifiers bypicking some meta-parameter ??
and insisting thatthe sub-classifiers comprised by a particular parserhave all reached some fixed ?
in training.
Given ?
?,the constituent classifier for label ?
is Qt?, where?t?
?
??
> ?t+1?
.
To obtain the final parser, wecross-validate ?
?, picking the value whose set of con-stituent classifiers maximizes accuracy on a devel-opment set.2.1.4 Types of Features used by the ScoringFunctionOur parser operates bottom-up.
Let the frontier ofa state be the top-most items (i.e.
the items with noparents).
The children of a candidate inference arethose frontier items below the item to be inferred, theleft context items are those frontier items to the leftof the children, and the right context items are thosefrontier items to the right of the children.
For exam-ple, in the candidate VP-inference shown in Figure 1,the frontier comprises the NP, VBD, and ADJP items,the VBD and ADJP items are the children of the VP-inference (the VBD is its head child), the NP is the leftcontext item, and there are no right context items.The design of some parsers in the literature re-stricts the kinds of features that can be usefully andefficiently evaluated.
Our scoring function and pars-ing algorithm have no such limitations.
Q can, inprinciple, use arbitrary information from the historyto evaluate constituent inferences.
Although some ofour feature types are based on prior work (Collins,1999; Klein & Manning, 2003; Bikel, 2004), wenote that our scoring function uses more history in-formation than typical parsers.All features check whether an item has someproperty; specifically, whether the item?s la-bel/headtag/headword is a certain value.
These fea-tures perform binary tests on the state directly, un-like Henderson (2003) which works with an inter-mediate representation of the history.
In our baselinesetup, feature set ?
contained five different featuretypes, described in Table 1.Table 2 Feature item groups.?
all children?
all non-head children?
all non-leftmost children?
all non-rightmost children?
all children left of the head?
all children right of the head?
head-child and all children left of the head?
head-child and all children right of the head2.2 Aggregating ConfidencesTo get the cumulative score of a parse path P, we ap-ply aggregatorA over the confidences Q(I) in Equa-tion 4.
Initially, we definedA in the customary fash-ion as summing the loss of each inference?s confi-dence:?P = arg maxP?P(x)????????
?I?PL (Q(I))???????
(15)with the logistic loss L as defined in Equation 6.
(Wenegate the final sum because we want to minimizethe loss.)
This definition of A is motivated by view-ing L as a negative log-likelihood given by a logisticfunction (Collins et al, 2002), and then using Equa-tion 3.
It is also inspired by the multiclass loss-baseddecoding method of Schapire and Singer (1999).With this additive aggregator, loss monotonically in-creases as inferences are added, as in a PCFG-basedparser in which all productions decrease the cumu-lative probability of the parse tree.In preliminary experiments, this aggregator gavedisappointing results: precision increased slightly,but recall dropped sharply.
Exploratory data analy-sis revealed that, because each inference incurs somepositive loss, the aggregator very cautiously buildsthe smallest trees possible, thus harming recall.
Wehad more success by defining A to maximize theminimum confidence.
Essentially,?P = arg maxP?P(x)minI?PQ(I) (16)Ties are broken according to the second lowest con-fidence, then the third lowest, and so on.2.3 SearchGiven input sentence x, we choose the parse path Pin P(x) with the maximum aggregated score (Equa-tion 4).
Since it is computationally intractable to144Table 1 Types of features.?
Child item features test if a particular child item has some property.
E.g.
does the item one right of thehead have headword ?perfect??
(True in Figure 1)?
Context item features test if a particular context item has some property.
E.g.
does the first item of leftcontext have headtag NN?
(True)?
Grandchild item features test if a particular grandchild item has some property.
E.g.
does the leftmostchild of the rightmost child item have label JJ?
(True)?
Exists features test if a particular group of items contains an item with some property.
E.g.
does somenon-head child item have label ADJP?
(True) Exists features select one of the groups of items specified inTable 2.
Alternately, they can select the terminals dominated by that group.
E.g.
is there some terminalitem dominated by non-rightmost children items that has headword ?quux??
(False)consider every possible sequence of inferences, weuse beam search to restrict the size of P(x).
Asan additional guard against excessive computation,search stopped if more than a fixed maximum num-ber of states were popped from the agenda.
As usual,search also ended if the highest-priority state in theagenda could not have a better aggregated score thanthe best final parse found thus far.3 ExperimentsFollowing Taskar, Klein, Collins, Koller, and Man-ning (2004), we trained and tested on ?
15 word sen-tences in the English Penn Treebank (Taylor et al,2003), 10% of the entire treebank by word count.3We used sections 02?21 (9753 sentences) for train-ing, section 24 (321 sentences) for development,and section 23 (603 sentences) for testing, prepro-cessed as per Table 3.
We evaluated our parser us-ing the standard PARSEVAL measures (Black etal., 1991): labelled precision, recall, and F-measure(LPRC, LRCL, and LFMS, respectively), which arecomputed based on the number of constituents in theparser?s output that match those in the gold-standardparse.
We tested whether the observed differences inPARSEVAL measures are significant at p = 0.05 us-ing a stratified shuffling test (Cohen, 1995, Section5.3.2) with one million trials.4As mentioned in Section 1, the parser cannot in-fer any item that crosses an item already in the state.3 There was insufficient time before deadline to train on allsentences.4 The shuffling test we used was originally implementedby Dan Bikel (http://www.cis.upenn.edu/?dbikel/software.html) and subsequently modified to compute p-values for LFMS differences.We placed three additional candidacy restrictionson inferences: (a) Items must be inferred under thebottom-up item ordering; (b) To ensure the parserdoes not enter an infinite loop, no two items in a statecan have both the same span and the same label;(c) An item can have no more than K = 5 children.
(Only 0.24% of non-terminals in the preprocesseddevelopment set have more than five children.)
Thenumber of candidate inferences at each state, as wellas the number of training examples generated by thealgorithm in Section 2.1.1, is proportional to K. Inour experiment, there were roughly |E?| ?
1.7 mil-lion training examples for each classifier.3.1 BaselineIn the baseline setting, context item features (Sec-tion 2.1.4) could refer to the two nearest items ofcontext in each direction.
The parser used a beamwidth of 1000, and was terminated in the rare eventthat more than 10,000 states were popped from theagenda.
Figure 2 shows the accuracy of the base-line on the development set as training progresses.Cross-validating the choice of ??
against the LFMS(Section 2.1.3) suggested an optimum of ??
= 1.42.At this ?
?, there were a total of 9297 decision treesplits in the parser (summed over all constituentclassifiers), LFMS = 87.16, LRCL = 86.32, andLPRC = 88.02.3.2 Beam WidthTo determine the effect of the beam width on theaccuracy, we evaluated the baseline on the devel-opment set using a beam width of 1, i.e.
parsingentirely greedily (Wong & Wu, 1999; Kalt, 2004;Sagae & Lavie, 2005).
Table 4 compares the base-145Table 3 Steps for preprocessing the data.
Starred steps are performed only on input with tree structure.1.
* Strip functional tags and trace indices, and remove traces.2.
* Convert PRT to ADVP.
(This convention was established by Magerman (1995).)3.
Remove quotation marks (i.e.
terminal items tagged ??
or ??).
(Bikel, 2004)4.
* Raise punctuation.
(Bikel, 2004)5.
Remove outermost punctuation.a6.
* Remove unary projections to self (i.e.
duplicate items with the same span and label).7.
POS tag the text using Ratnaparkhi (1996).8.
Lowercase headwords.9.
Replace any word observed fewer than 5 times in the (lower-cased) training sentences with UNK.a As pointed out by an anonymous reviewer of Collins (2003), removing outermost punctuation may discard useful information.It?s also worth noting that Collins and Roark (2004) saw a LFMS improvement of 0.8% over their baseline discriminative parserafter adding punctuation features, one of which encoded the sentence-final punctuation.Figure 2 PARSEVAL scores of the baseline on the ?
15 words development set of the Penn Treebank.
Thetop x-axis shows accuracy as the minimum reduction in loss ??
decreases.
The bottom shows the correspond-ing number of decision tree splits in the parser, summed over all classifiers.74%76%78%80%82%84%86%88%90%20000 10000 5000 2500 1000 25074%76%78%80%82%84%86%88%90%0.341.02.75.0102540120PARSEVALscoreTotal # of splitsMinimum reduction in lossLabelled precisionLabelled F-measureLabelled recallline results on the development set with a beamwidth of 1 and a beam width of 1000.5 The widerbeam seems to improve the PARSEVAL scores ofthe parser, although we were unable to detect a sta-tistically significant improvement in LFMS on ourrelatively small development set.5 Using a beam width of 100,000 yielded output identical tousing a beam width of 1000.3.3 Context SizeTable 5 compares the baseline to parsers that couldnot examine as many context items.
A significantportion of the baseline?s accuracy is due to contex-tual clues, as evidenced by the poor accuracy of theno context run.
However, we did not detect a signif-icant difference between using one context item ortwo.146Table 4 PARSEVAL results on the ?
15 wordsdevelopment set of the baseline, varying the beamwidth.
Also, the MRL that achieved this LFMS andthe total number of decision tree splits at this MRL.Dev Dev Dev MRL #splitsLFMS LRCL LPRC ??
totalBeam=1 86.36 86.20 86.53 2.03 7068Baseline 87.16 86.32 88.02 1.42 9297Table 5 PARSEVAL results on the ?
15 words de-velopment set, given the amount of context avail-able.
is statistically significant.
The score differencesbetween ?context 0?
and ?context 1?
are significant,whereas the differences between ?context 1?
and thebaseline are not.Dev Dev Dev MRL #splitsLFMS LRCL LPRC ??
totalContext 0 75.15 75.28 75.03 3.38 3815Context 1 86.93 85.78 88.12 2.45 5588Baseline 87.16 86.32 88.02 1.42 9297Table 6 PARSEVAL results of decision stumps onthe ?
15 words development set, through 8200splits.
The differences between the stumps run andthe baseline are statistically significant.Dev Dev Dev MRL #splitsLFMS LRCL LPRC ??
totalStumps 85.72 84.65 86.82 2.39 5217Baseline 87.07 86.05 88.12 1.92 72833.4 Decision StumpsOur features are of relatively fine granularity.
To testif a less powerful machine could provide accuracycomparable to the baseline, we trained a parser inwhich we boosted decisions stumps, i.e.
decisiontrees of depth 1.
Stumps are equivalent to learninga linear discriminant over the atomic features.
Sincethe stumps run trained quite slowly, it only reached8200 splits total.
To ensure a fair comparison, in Ta-ble 6 we chose the best baseline parser with at most8200 splits.
The LFMS of the stumps run on the de-velopment set was 85.72%, significantly less accu-rate than the baseline.For example, Figure 3 shows a case where NPclassification better served by the informative con-junction ?1 ?
?2 found by the decision trees.
GivenFigure 3 An example of a decision (a) stump and(b) tree for scoring NP-inferences.
Each leaf?s valueis the confidence assigned to all inferences that fallin this leaf.
?1 asks ?does the first child have a de-terminer headtag??.
?2 asks ?does the last child havea noun label??.
NP classification is better served bythe informative conjunction ?1?
?2 found by the de-cision trees.
(a)?1true f alse+0.5 0(b)?1true f alse?2true f alse0+1.0 -0.2Table 7 PARSEVAL results of deterministic parserson the ?
15 words development set through 8700splits.
A shaded cell means that the difference be-tween this value and that of the baseline is statisti-cally significant.
All differences between l2r and r2lare significant.Dev Dev Dev MRL #splitsLFMS LRCL LPRC ??
totall2r 83.61 82.71 84.54 3.37 2157r2l 85.76 85.37 86.15 3.39 1881Baseline 87.07 86.05 88.12 1.92 7283the sentence ?The man left?, at the initial state thereare six candidate NP-inferences, one for each span,and ?
(NP The man)?
is the only candidate inferencethat is correct.
?1 is true for the correct inference andtwo of the incorrect inferences (?
(NP The)?
and ?
(NPThe man left)?).
?1 ?
?2, on the other hand, is trueonly for the correct inference, and so it is better atdiscriminating NPs over this sample.3.5 Deterministic ParsingOur baseline parser simulates a non-deterministicmachine, as at any state there may be several correctdecisions.
We trained deterministic variations of theparser, for which we imposed strict left-to-right (l2r)and right-to-left (r2l) item orderings.
For these vari-ations we generated training examples using the cor-responding unique path to each gold-standard train-ing tree.
The r2l run reached only 8700 splits to-tal, so in Table 7 we chose the best baseline and l2r147Table 8 PARSEVAL results of the full vocabularyparser on the ?
15 words development set.
The dif-ferences between the full vocabulary run and thebaseline are not statistically significant.Dev Dev Dev MRL #splitsLFMS LRCL LPRC ?
totalBaseline 87.16 86.32 88.02 1.42 9297Full vocab 87.50 86.85 88.15 1.27 10711parser with at most 8700 splits.r2l parsing is significantly more accurate than l2r.The reason is that the deterministic runs (l2r and r2l)must avoid prematurely inferring items that comelater in the item ordering.
This puts the l2r parserin a tough spot.
If it makes far-right decisions, it?smore likely to prevent correct subsequent decisionsthat are earlier in the l2r ordering, i.e.
to the left.But if it makes far-left decisions, then it goes againstthe right-branching tendency of English sentences.In contrast, the r2l parser is more likely to be correctwhen it infers far-right constituents.We also observed that the accuracy of the de-terministic parsers dropped sharply as training pro-gressed (See Figure 4).
This behavior was unex-pected, as the accuracy curve levelled off in everyother experiment.
In fact, the accuracy of the deter-ministic parsers fell even when parsing the trainingdata.
To explain this behavior, we examined the mar-gin distributions of the r2l NP-classifier (Figure 5).As training progressed, the NP-classifier was able toreduce loss by driving up the margins of the incor-rect training examples, at the expense of incorrectlyclassifying a slightly increased number of correcttraining examples.
However, this is detrimental toparsing accuracy.
The more correct inferences withnegative confidence, the less likely it is at some statethat the highest confidence inference is correct.
Thiseffect is particularly pronounced in the deterministicsetting, where there is only one correct inference perstate.3.6 Full VocabularyAs in traditional parsers, the baseline was smoothedby replacing any word that occurs fewer than fivetimes in the training data with the special token UNK(Table 3.9).
Table 8 compares the baseline to a fullvocabulary run, in which the vocabulary containedall words observed in the training data.
As evidencedby the results therein, controlling for lexical sparsitydid not significantly improve accuracy in our setting.In fact, the full vocabulary run is slightly more ac-curate than the baseline on the development set, al-though this difference was not statistically signifi-cant.
This was a late-breaking result, and we usedthe full vocabulary condition as our final parser forparsing the test set.3.7 Test Set ResultsTable 9 shows the results of our best parser on the?
15 words test set, as well as the accuracy reportedfor a recent discriminative parser (Taskar et al,2004) and scores we obtained by training and test-ing the parsers of Charniak (2000) and Bikel (2004)on the same data.
Bikel (2004) is a ?clean room?reimplementation of the Collins parser (Collins,1999) with comparable accuracy.
Both Charniak(2000) and Bikel (2004) were trained using the gold-standard tags, as this produced higher accuracy onthe development set than using Ratnaparkhi (1996)?stags.3.8 Exploratory Data AnalysisTo gain a better understanding of the weaknesses ofour parser, we examined a sample of 50 develop-ment sentences that the full vocabulary parser didnot get entirely correct.
Besides noise and cases ofgenuine ambiguity, the following list outlines all er-ror types that occurred in more than five sentences,in roughly decreasing order of frequency.
(Note thatthere is some overlap between these groups.)?
ADVPs and ADJPs A disproportionate amount ofthe parser?s error was due to ADJPs and ADVPs.Out of the 12.5% total error of the parser on thedevelopment set, an absolute 1.0% was due toADVPs, and 0.9% due to ADJPs.
The parser hadLFMS = 78.9%,LPRC = 82.5%,LRCL = 75.6%on ADVPs, and LFMS = 68.0%,LPRC =71.2%,LRCL = 65.0% on ADJPs.These constructions can sometimes involve trickyattachment decisions.
For example, in the frag-ment ?to get fat in times of crisis?, the parser?soutput was ?
(VP to (VP get (ADJP fat (PP in (NP(NP times) (PP of (NP crisis)))))))?
instead of thecorrect construction ?
(VP to (VP get (ADJP fat) (PPin (NP (NP times) (PP of (NP crisis))))))?.148Figure 4 LFMS of the baseline and the deterministic runs on the ?
15 words development set of the PennTreebank.
The x-axis shows the LFMS as training progresses and the number of decision tree splits in-creases.74767880828486888700 5000 2500 1000 2507476788082848688ParsevalFMSTotal # of splitsBaselineRight-to-leftLeft-to-rightFigure 5 The margin distributions of the r2l NP-classifier, early in training and late in training, (a) over theincorrect training examples and (b) over the correct training examples.
(a)-200204060801001201401600  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9  1MarginPercentileLate in trainingEarly in training(b)-40-30-20-10010200  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9  1MarginPercentileLate in trainingEarly in trainingThe amount of noise present in ADJP and ADVPannotations in the PTB is unusually high.
Annota-tion of ADJP and ADVP unary projections is partic-ularly inconsistent.
For example, the developmentset contains the sentence ?The dollar was tradingsharply lower in Tokyo .
?, with ?sharply lower?bracketed as ?
(ADVP (ADVP sharply) lower)?.
?sharply lower?
appears 16 times in the completetraining section, every time bracketed as ?
(ADVPsharply lower)?, and ?sharply higher?
10 times,always as ?
(ADVP sharply higher)?.
Because of thehigh number of negative examples, the classifiers?149Table 9 PARSEVAL results of on the ?
15 words test set of various parsers in the literature.
The differ-ences between the full vocabulary run and Bikel or Charniak are significant.
Taskar et al (2004)?s outputwas unavailable for significance testing, but presumably its differences from the full vocab parser are alsosignificant.Test Test Test Dev Dev DevLFMS LRCL LPRC LFMS LRCL LPRCFull vocab 87.13 86.47 87.80 87.50 86.85 88.15Bikel (2004) 88.85 88.31 89.39 86.82 86.43 87.22Taskar et al (2004) 89.12 89.10 89.14 89.98 90.22 89.74Charniak (2000) 90.09 90.01 90.17 89.50 89.69 89.32bias is to cope with the noise by favoring negativeconfidences predictions for ambiguous ADJP andADVP decisions, hence their abysmal labelled re-call.
One potential solution is the weight-sharingstrategy described in Section 3.5.?
Tagging Errors Many of the parser?s errorswere due to poor tagging.
Preprocessing sentence?Would service be voluntary or compulsory ?
?gives ?would/MD service/VB be/VB voluntary/JJor/CC UNK/JJ?
and, as a result, the parser brack-ets ?service .
.
.
compulsory?
as a VP instead ofcorrectly bracketing ?service?
as an NP.
We alsofound that the tagger we used has difficulties withcompletely capitalized words, and tends to tagthem NNP.
By giving the parser access to the samefeatures used by taggers, especially rich lexicalfeatures (Toutanova et al, 2003), the parser mightlearn to compensate for tagging errors.?
Attachment decisions The parser does not de-tect affinities between certain word pairs, so it hasdifficulties with bilexical dependency decisions.In principle, bilexical dependencies can be rep-resented as conjunctions of feature given in Sec-tion 2.1.4.
Given more training data, the parsermight learn these affinities.4 ConclusionsIn this work, we presented a near state-of-the-art approach to constituency parsing which over-comes some of the limitations of other discrimina-tive parsers.
Like Yamada and Matsumoto (2003)and Sagae and Lavie (2005), our parser is driven byclassifiers.
Even though these classifiers themselvesnever do any parsing during training, they can becombined into an effective parser.
We also presenteda beam search method under the objective functionof maximizing the minimum confidence.To ensure efficiency, some discriminative parsersplace stringent requirements on which types of fea-tures are permitted.
Our approach requires no suchrestrictions and our scoring function can, in prin-ciple, use arbitrary information from the history toevaluate constituent inferences.
Even though ourfeatures may be of too fine granularity to dis-criminate through linear combination, discrimina-tively trained decisions trees determine useful fea-ture combinations automatically, so adding new fea-tures requires minimal human effort.Training discriminative parsers is notoriouslyslow, especially if it requires generating examples byrepeatedly parsing the treebank (Collins & Roark,2004; Taskar et al, 2004).
Although training timeis still a concern in our setup, the situation is ame-liorated by generating training examples in advanceand inducing one-vs-all classifiers in parallel, a tech-nique similar in spirit to the POS-tag parallelizationin Yamada and Matsumoto (2003) and Sagae andLavie (2005).This parser serves as a proof-of-concept, in thatwe have not fully exploited the possibilities of en-gineering intricate features or trying more complexsearch methods.
Its flexibility offers many oppor-tunities for improvement, which we leave to futurework.AcknowledgmentsThe authors would like to thank Dan Bikel, MikeCollins, Ralph Grishman, Adam Meyers, MehryarMohri, Satoshi Sekine, and Wei Wang, as well as theanonymous reviewers, for their helpful comments150and constructive criticism.
This research was spon-sored by an NSF CAREER award, and by an equip-ment gift from Sun Microsystems.ReferencesBikel, D. M. (2004).
Intricacies of Collins?
pars-ing model.
Computational Linguistics, 30(4),479?511.Black, E., Abney, S., Flickenger, D., Gdaniec, C.,Grishman, R., Harrison, P., et al (1991).A procedure for quantitatively comparing thesyntactic coverage of English grammars.
InSpeech and Natural Language (pp.
306?311).Charniak, E. (2000).
A maximum-entropy-inspiredparser.
In NAACL (pp.
132?139).Cohen, P. R. (1995).
Empirical methods for artificialintelligence.
MIT Press.Collins, M. (1999).
Head-driven statistical modelsfor natural language parsing.
Unpublisheddoctoral dissertation, UPenn.Collins, M. (2003).
Head-driven statistical modelsfor natural language parsing.
ComputationalLinguistics, 29(4), 589?637.Collins, M., & Koo, T. (2005).
Discriminativereranking for natural language parsing.
Com-putational Linguistics, 31(1), 25?69.Collins, M., & Roark, B.
(2004).
Incremental pars-ing with the perceptron algorithm.
In ACL.Collins, M., Schapire, R. E., & Singer, Y.
(2002).Logistic regression, AdaBoost and Bregmandistances.
Machine Learning, 48(1-3), 253?285.Henderson, J.
(2003).
Inducing history representa-tions for broad coverage statistical parsing.
InHLT/NAACL.Kalt, T. (2004).
Induction of greedy controllersfor deterministic treebank parsers.
In EMNLP(pp.
17?24).Kearns, M. J., & Mansour, Y.
(1999).
On the boost-ing ability of top-down decision tree learningalgorithms.
Journal of Computer and SystemsSciences, 58(1), 109?128.Klein, D., & Manning, C. D. (2001).
Parsing andhypergraphs.
In IWPT (pp.
123?134).Klein, D., & Manning, C. D. (2003).
Accurate un-lexicalized parsing.
In ACL (pp.
423?430).Magerman, D. M. (1995).
Statistical decision-treemodels for parsing.
In ACL (pp.
276?283).Marcus, M. P. (1980).
Theory of syntactic recogni-tion for natural languages.
MIT Press.Och, F. J., & Ney, H. (2002).
Discriminative trainingand maximum entropy models for statisticalmachine translation.
In ACL.Ratnaparkhi, A.
(1996).
A maximum entropy part-of-speech tagger.
In EMNLP (pp.
133?142).Sagae, K., & Lavie, A.
(2005).
A classifier-basedparser with linear run-time complexity.
InIWPT.Schapire, R. E., & Singer, Y.
(1999).
Improvedboosting using confidence-rated predictions.Machine Learning, 37(3), 297?336.Taskar, B., Klein, D., Collins, M., Koller, D., &Manning, C. (2004).
Max-margin parsing.In EMNLP (pp.
1?8).Taylor, A., Marcus, M., & Santorini, B.
(2003).
ThePenn Treebank: an overview.
In A.
Abeille?(Ed.
), Treebanks: Building and using parsedcorpora (pp.
5?22).Toutanova, K., Klein, D., Manning, C. D., & Singer,Y.
(2003).
Feature-rich part-of-speech tag-ging with a cyclic dependency network.
InHLT/NAACL (pp.
252?259).Wong, A., & Wu, D. (1999).
Learning alightweight robust deterministic parser.
InEUROSPEECH.Yamada, H., & Matsumoto, Y.
(2003).
Statisticaldependency analysis with support vector ma-chines.
In IWPT.Zhao, S., & Grishman, R. (2005).
Extracting rela-tions with integrated information using kernelmethods.
In ACL.151
