Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 153?161,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsA Hybrid Approach to Emotional Sentence Polarity andIntensity ClassificationJorge Carrillo de Albornoz, Laura Plaza, Pablo Gerv?sUniversidad Complutense de MadridMadrid, Spain{jcalbornoz,lplazam}@fdi.ucm.es, pgervas@sip.ucm.esAbstractIn this paper, the authors present a new ap-proach to sentence level sentiment analysis.The aim is to determine whether a sentenceexpresses a positive, negative or neutral sen-timent, as well as its intensity.
The methodperforms WSD over the words in the sentencein order to work with concepts rather thanterms, and makes use of the knowledge in anaffective lexicon to label these concepts withemotional categories.
It also deals with the ef-fect of negations and quantifiers on polarityand intensity analysis.
An extensive evaluationin two different domains is performed in orderto determine how the method behaves in 2-classes (positive and negative), 3-classes (posi-tive, negative and neutral) and 5-classes(strongly negative, weakly negative, neutral,weakly positive and strongly positive) classifi-cation tasks.
The results obtained compare fa-vorably with those achieved by other systemsaddressing similar evaluations.1 IntroductionSentiment analysis has gained much attentionfrom the research community in recent years.
Itis concerned with the problem of discoveringemotional meanings in text, and most commontasks usually include emotion labeling (assigninga text its main emotion), polarity recognition(classifying a statement into positive or negative)and subjectivity identification (determiningwhether a text is subjective or objective).
Thegrowing research interest is mainly due to thepractical applications of sentiment analysis.Companies and organizations are interested infinding out costumer sentiments and opinions,while individuals are interested in others?
opi-nions when purchasing a product or decidingwhether or not watching a movie.Many approaches have dealt with sentimentanalysis as the problem of classifying product orservice reviews (Pang et al, 2002; Turney,2002), while others have attempted to classifynews items (Devitt and Ahmad, 2007).
The taskis usually addressed as a 2-classes classificationproblem (positive vs. negative).
Recent workshave included the neutral class, trying to detectnot only the polarity but also the absence of emo-tional meaning (Wilson et al, 2005; Esuli andSebastiani, 2006).
However, few approaches tryto face a more fine-grained prediction of the in-tensity (e.g.
classifying the polarity into stronglynegative, weakly negative, neutral, weakly posi-tive and strongly positive).Another important problem of most of theseapproximations is that they usually work withterms, and so disregard the contextual meaningof those terms in the sentence (Martineau andFinin, 2009; Moilanen and Pulman, 2007).
Theuse of word disambiguation is not usual in thistask, due to the fact that most approaches uselexical resources created to work with terms.However, it is essential to correctly capture themeaning of these terms within the text.In this paper, we present a hybrid approachbased on machine learning techniques and lexicalrules to classify sentences according to their po-larity and intensity.
Thus, given an input text, themethod is able to determine the polarity of eachsentence (i.e.
if it is negative or positive), as wellas its intensity.
The system tackles the effect ofnegations and quantifiers in sentiment analysis,and addresses the problem of word ambiguity,taken into account the contextual meaning of theterms in the text by using a word sense disam-biguation algorithm.The paper is organized as follows.
Section 2exposes some background and related work onsentiment analysis.
Section 3 presents the lexicalresources and corpora used by the system.
Sec-153tion 4 describes the method proposed for polarityand intensity classification.
Section 5 presentsthe evaluation framework and discusses the ex-perimental results.
Finally, section 6 providesconcluding remarks and future lines of work.2 Related workThe sentiment analysis discipline in computa-tional linguistic is mainly focused on identify-ing/classifying different emotional contents with-in a phrase, sentence or document.
This fieldusually encloses tasks such as emotion identifica-tion, subjectivity classification and polarity rec-ognition.
Sentiment analysis has obtained greatpopularity in the last years mostly due to its suc-cessful application to different business domains,such as the evaluation of products and services,where the goal is to discern whether the opinionexpressed by a user about a product or service isfavorable or unfavorable.Focusing on polarity recognition, the aim ofthis task is the classification of texts into positiveor negative according to their emotional mean-ing.
Most of the approaches rely on machinelearning techniques or rule based methods.
Sta-tistical approaches based on term frequencies andbags of words are frequently used in machinelearning approximations.
Pang et al (2002)present a comparison between three differentmachine learning algorithms trained with bags offeatures computed over term frequencies, andconclude that SVM classifiers can be efficientlyused in polarity identification.
Martineau andFinin (2009) use a similar approach where thewords are scored using a Delta TF-IDF functionbefore classifying the documents.
On the otherhand, Meena and Prabhakar (2007) study the ef-fect of conjuncts in polarity recognition usingrule based methods over the syntax tree of thesentence.
Whitelaw et al (2005) introduce theconcept of ?appraisal groups?
which are com-bined with bags of word features to automatical-ly classify movie reviews.
To this aim, they use asemi-automated method to generate a lexicon ofappraising adjectives and modifiers.During the past few years, the problem of po-larity recognition has been usually faced as a stepbeyond the identification of the subjectivity orobjectivity of texts (Wiebe et al, 1999).
Differ-ent approximations have been proposed to dealwith this problem.
Pang and Lee (2004) proposea graph-based method which finds minimum cutsin a document graph to classify the sentencesinto subjective or objective.
After that, they use abag of words approximation to classify the sub-jective sentences into positive or negative.
Kimand Hovy (2004) also introduce a previous stepto identify the subjectivity of sentences regardinga certain topic, and later classify these sentencesinto positives or negatives.Most recent approaches do not only deal withthe 2-classes classification problem, but also in-troduce a new class representing neutrality.
Thus,the aim of these works is to classify the text intopositive, negative or neutral.
Wilson et al (2005)present a double subjectivity classifier based onfeatures such as syntactic classes and sentenceposition, and more semantic features such as ad-jective graduation.
The first classifier determinesthe subjectivity or neutrality of the phrases in thetext, while the second determines its polarity (in-cluding neutrality).
Esuli and Sebastiani (2006)also address this problem testing three differentvariants of a semi-supervised method, and classi-fy the input into positive, negative or neutral.The method proposed yields good results in the2-classes polarity classification, while the resultsdecrease when dealing with 3-classes.
A moreambitious classification task is proposed byBrooke (2009), where the goal is to measure theintensity of polarity.
To this aim, the author clas-sifies the input into 3-classes (strongly-negative,ambivalent, and strongly-positive), 4 classes(strongly-negative, weakly-negative, weakly-positive and strongly-positive) and 5-classes(strongly-negative, weakly-negative, ambivalent,weakly-positive and strongly-positive).
The re-sults decrease considerably with the number ofclasses, from 62% of accuracy for 3-classes to38% of accuracy for 5-classes.3 Corpora and resourcesThe evaluation of the system has been carried outusing two corpora from two very distinct do-mains: the Sentence Polarity Movie Review Da-taset1 and the one used in the SemEval 2007 Af-fective Text task 21 http://www.cs.cornell.edu/People/pabo/movie-review-data/.
The first one consists of10.662 sentences selected from different moviereview websites.
These sentences are labeled aspositive or negative depending on whether theyexpress a positive or negative opinion within themovie review.
The second one consists of atraining set and a test set of 250 and 1000 newsheadlines respectively, extracted from differentnews sites.
Each sentence is labeled with a value2 http://www.cse.unt.edu/~rada/affectivetext/154between -100 and 100, where -100 means highlynegative emotional intensity, 100 means highlypositive and 0 means neutral.
To the purpose ofthis work, the test set from the SemEval corpusand 1000 sentences randomly extracted from theSentence Polarity Movie Review corpus (500positive and 500 negative) were used as evalua-tion datasets.In order to identify the emotional categoriesassociated to the concepts in the sentences, anaffective lexical database based on semanticsenses, instead of terms, is needed.
To this aim,the authors have tested different resources andfinally selected the WordNet Affect affectivedatabase (Strapparava and Valitutti, 2004).
Thisaffective lexicon has the particularity of assign-ing emotional categories to synsets of the Word-Net lexical database (Miller et al, 1990), allow-ing the system to correctly disambiguate theterms using one of the many WordNet-basedword sense disambiguation algorithms.
The emo-tional categories in WordNet Affect are orga-nized hierarchically, and its first level distin-guishes between positive-emotion, negative-emotion, neutral-emotion and ambiguous-emotion.
The second level encloses the emotion-al categories themselves, and consists of a set of32 categories.
For this work, a subset of 16 emo-tional categories from this level has been se-lected, since the hierarchy proposed in WordNetAffect is considerably broader than those com-monly used in sentiment analysis.
On the otherhand, the first level of emotional categories maybe useful to predict the polarity, but it is clearlynot enough to predict the intensity of this polari-ty.
To be precise, the subset of emotional catego-ries used in this work is: {joy, love, liking, calm-ness, positive-expectation, hope, fear, sadness,dislike, shame, compassion, despair, anxiety,surprise, ambiguous-agitation and ambiguous-expectation}.
The authors consider this subset tobe a good representation of the human feeling.Since the WordNet Affect hierarchy does notprovide an antonym relationship, the authors hascreated that relation for the previous set of emo-tional categories.
Only relationships betweenemotional categories with a strongly oppositemeaning are created, such as liking-disliking andjoy-sadness.
The purpose of this antonym rela-tionship is twofold: first, it contributes to handlenegation forms; and second, it can be used toautomatically expand the affective lexicon.
Bothissues are discussed in detail later in the docu-ment.On the other hand, since a good amount ofwords with a highly emotional meaning, such asdead, cancer and violent, are not labeled inWordNet Affect, these words have been manual-ly labeled by the authors and have been later ex-tended with their synonyms, antonyms and de-rived adjectives using the corresponding seman-tic and lexical relations in WordNet.
This processhas been done in two steps in order to measurethe effect of the number of synsets labeled on theclassification accuracy, as described in section 5.The WordNet Affect 1.1 lexicon consists of aset of 911 synsets.
However, the authors havedetected that a good number of these synsetshave been labeled more than once, and with dif-ferent emotional categories (e.g.
the synset?a#00117872 {angered, enraged, furious, infu-riated, maddened}?
is labeled with three differentcategories: anger, fury and infuriation).
Thus,after removing these synsets and those labeledwith an emotional category not included in the16-categories subset used in this work, the affec-tive lexicon presents 798 synsets.
After the firststep of semi-automatic labeling, the affectivelexicon increased the number of synsets in 372,of which 100 synsets were manually labeled, and272 were automatically derived throughout theWordNet relations.
The second and last step ofsemi-automatic labeling added 603 synsets to thelexicon, of which 200 synsets were manuallylabeled, and 403 were automatically derived.The final lexicon presents 1773 synsets and 4521words labeled with an emotional category.
Table1 shows the distribution of the affective lexiconin grammatical categories.GrammaticalCategoryWNAffectWNAffect +1st  stepWNAffect +2nd stepNouns 280 440 699Verbs 122 200 309Adjectives 273 394 600Adverbs 123 136 165Table 1: Distribution in grammatical categories of the syn-sets in the affective lexicon.4 The methodIn this section, the method for automaticallylabeling sentences with an emotional intensityand polarity is presented.
The problem is facedas a text classification task, which is accomplish-es throughout four steps.
Each step is explainedin detail in the following subsections.1554.1 Pre-processing: POS tagging and con-cept identificationIn order to determine the appropriate emotionalcategory for each word in its context, a pre-processing step is accomplished to translate eachterm in the sentence to its adequate sense inWordNet.
To this aim, the system analyzes thetext, splits it into sentences and tags the tokenswith their part of speech.
The Gate architecture3and the Stanford Parser4Once the sentences have been split and tagged,the method maps each word of each sentenceinto its sense in WordNet according to its con-text.
To this end, the lesk WSD algorithm im-plemented in the WordNet Sense-Relate perlpackage is used (Patwardhan et al, 2005).
Thedisambiguation is carried out only over thewords belonging to the grammatical categoriesnoun, verb, adjective and adverb, as only thesecategories can present an emotional meaning.
Asa result, we get the stem and sense in WordNetof each word, and this information is used to re-trieve its synset.were selected to carryout this process.
In particular the Annie EnglishTokeniser, Hash Gazetter, RegEx Sentence Split-ter and the Stanford Parser modules in Gate areused to analyze the input.
In this step also thesyntax tree and dependencies are retrieved fromthe Stanford Parser.
These features will be usedin the post-processing step in order to identifythe negations and the quantifiers, as well as theirscope.A good example of the importance of perform-ing word disambiguation can be shown in thesentence ?Test to predict breast cancer relapse isapproved?
from the SemEval news corpus.
Thenoun cancer has five possible entries in WordNetand only one refers to a ?malignant growth ortumor?, while the others are related with ?astrol-ogy?
and the ?cancer zodiacal constellation?.Obviously, without a WSD algorithm, the wrongsynset will be considered, and a wrong emotionwill be assigned to the concept.Besides, to enrich the emotion identificationstep, the hypernyms of each concept are also re-trieved from WordNet.4.2 Emotion identificationThe aim of the emotion identification step is tomap the WordNet concepts previously identifiedto those present in the affective lexicon, as well3 http://gate.ac.uk/4 http://nlp.stanford.edu/software/lex-parser.shtmlas to retrieve from this lexicon the correspondingemotional category of each concept.We hypothesize that the hypernyms of a con-cept entail the same emotions than the conceptitself, but the intensity of such emotions decreas-es as we move up the hierarchy (i.e.
the moregeneral the hypernym becomes, the less its emo-tional intensity is).
Following this hypothesis,when no entry is found in the affective lexiconfor a given concept, the emotional category asso-ciated to its nearest hypernym, if any, is used tolabel the concept.
However, only a certain levelof hypernymy is accepted, since an excessivegeneralization introduces some noise in the emo-tion identification.
This parameter has been em-pirically set to 3 (Carrillo de Albornoz et al,2010).
Previous experiments have shown that,upper this level, the working hypothesis becomesunreliable.The sentence ?Siesta cuts risk of heart diseasedeath study finds?
clearly illustrates the processdescribed above.
In this sentence, the conceptsrisk, death and disease are labeled with an emo-tional category: in particular, the categories as-signed to them are fear, fear and dislike respec-tively.
However, while the two firsts are re-trieved from the affective lexicon by their ownsynsets, the last one is labeled through its hyper-nym: since no matching is found for disease inthe lexicon, the analysis over its hypernyms de-tects the category dislike assigned to the synsetof its first hypernym, which contains words suchas illness and sickness, and the same emotion(dislike) is assigned to disease.It must be noted that, to perform this analysis,a previous mapping between 2.1 and 1.6 Word-Net versions was needed, since the method andthe affective lexicon work on different versionsof the database.4.3 Post-processing: Negation and quantifi-ers detectionOnce the concepts of the sentence have been la-beled with their emotional categories, the nextstep aims to detect and solve the effect of thenegations and the quantifiers on the emotionalcategories identified in the previous step.The effect of negation has been broadly stu-died in NLP (Morante and Daelemans, 2009) andsentiment analysis (Jia et al, 2009).
Two mainconsiderations must be taken into account whendealing with negation.
First, the negation scopemay affect only a word (no reason), a proposi-tion (Beckham does not want to play again forReal) or even a subject (No one would like to do156this).
Different approximations have been pro-posed to delimit the scope of negation.
Someassume the scope to be those words between thenegation token and the first punctuation mark(Pang et al, 2002), others consider a fixed num-ber of words after the negation token (Hu andLiu, 2004).
Second, the impact of negation isusually neutralized by reversing the polarity ofthe sentence (Polanyi and Zaenen, 2006) or usingcontextual valence shifters which increase ordismiss the final value of negativity or positivityof the sentence (Kennedy and Inkpen, 2006).In this work, the negation scope is detected us-ing the syntax tree and dependencies generatedby the Stanford Parser.
The dependency neg al-lows us to easily determine the presence of sev-eral simple types of negation, such as those pre-ceded by don?t, didn?t, not, never, etc.
Otherwords not identified with this dependency, butalso with a negation meaning, such as no, none?nor or nobody, are identified using a negationtoken list.
To determine the negation scope, wefind in the syntax tree the first common ancestorthat encloses the negation token and the wordimmediately after it, and assume all descendantleaf nodes to be affected by the negation.For each concept in the sentence that falls intothe scope of a negation, the system retrieves itsantonym emotional category, if any, and assignsthis category to the concept.
If no antonym emo-tion is obtained, the concept is labeled with noemotion, according to the premise that the nega-tion may change or neutralize the emotional po-larity.
An example of this process can be shownin the sentence ?Children and adults enamoredof all things pokemon won't be disappointed?.
Inthis sentence, the Stanford Parser discovers anegation and the system, through the syntax tree,determines that the scope of the negation enclos-es the words ?won?t be disappointed?.
As thesynset of ?disappointed?
has been labeled withthe emotional category despair, its antonym isretrieved, and the emotional category of the an-tonym, hope, is used to label the concept.On the other hand, the quantifiers are wordsconsidered in sentiment analysis as amplifiers ordowntoners (Quirk et al, 1985).
That is to say,the word very in the sentence ?That is a verygood idea?
amplifies the intensity of the emo-tional meaning and the positivity of the sentence,while the word less in the sentence ?It is lesshandsome than I was expecting?
dismisses itsintensity and polarity.
The most common ap-proach to identify quantifiers is the use of lists ofwords which play specific grammatical roles inthe sentence.
These lists normally contain a fixedvalue for all positive words and another value forall negative words (Polanyi and Zaenen, 2006).By contrast, Brooke (2009) proposes a novel ap-proach where each quantifier is assigned its ownpolarity and weight.The quantifiers are usually represented as sen-tence modifiers, assuming their scope to be thewhole sentence and modifying its overall polari-ty.
However, when dealing with sentences like?The house is really nice and the neighborhoodis not bad?, these approaches assume that thequantifier really amplifies the intensity of bothconjunctives, when it only should amplify theintensity of the first one.
By contrast, our ap-proach determines the scope of the quantifiers bythe syntax tree and the dependencies over them.Thus, when a quantifier is detected in a sentence,the dependencies are checked and only those thatplay certain roles, such as adverbial or adjectivalmodifiers, are considered.
All concepts affectedby a quantifier are marked with the weight cor-responding to that quantifier, which will serve toamplify/dismiss the emotions of these conceptsin the classification step.
The quantifier list usedhere is the one proposed in Brooke (2009).The sentence ?Stale first act, scrooge story,blatant product placement, some very good com-edic songs?
illustrates the analysis of the quan-tifiers.
The system detects two tokens which arein the quantifier list and play the appropriategrammatical roles.
The first quantifier some af-fects to the words ?very good comedic songs?,while the second quantifier very only affects to?good?.
So these concepts are marked with thespecific weight of each quantifier.
Note that theconcept ?good?
is marked twice.4.4 Sentence classificationUp to this point, the sentence has been labeledwith a set of emotional categories, negations andtheir scope have been detected and the quantifi-ers and the concepts affected by them have beenidentified.
In this step, this information is used totranslate the sentence into a Vector of EmotionalOccurrences (VEO), which will be the input tothe machine learning classification algorithm.Thus, each sentence is represented as a vector of16 values, each of one representing an emotionalcategory.
The VEO vector is generated as fol-lows:?
If the concept has been labeled with anemotional category, the position of thevector for this category is increased in 1.157?
If no emotional category has been foundfor the concept, then the category of itsfirst hypernym labeled is used.
As thehypernym generalizes the meaning of theconcept, the value assigned to the positionof the emotional category in the VEO isweighted as follows:[ ] [ ]1.1++=DepthHyperiVEOiVEO?
If a negation scope encloses the concept,then the antonym emotion is used, as de-scribed in the previous step.
The emotion-al category position of this antonym in theVEO is increased in 0.9.
Different testshave been carried out to set this parameter,and the 0.9 value got the best results.
Thereason for using a lower value for theemotional categories derived from nega-tions is that the authors consider that a ne-gation changes the emotional meaning of aconcept but usually in a lower percentage.For example, the sentence ?The neighbor-hood is not bad?
does not necessarilymean that it is a good neighborhood, but itis a quite acceptable one.?
If a concept is affected by a quantifier,then the weight of that quantifier is addedto the position in the VEO of the emotion-al category assigned to the concept.Thus, a sentence like ?This movie?.
isn?tworth the energy it takes to describe how reallybad it is?
will be represented by the VEO [1.0, 0,0.0, 0, 0, 0.0, 0, 0, 2.95, 0, 0, 0, 0, 0, 0, 0].
Inthis sentence, the concept movie is labeled withthe emotional category joy, the concept worth islabeled with positive-expectation, the conceptenergy is labeled with liking, and the conceptbad is labeled with dislike.
Since the conceptsworth and energy fall into the negation scope,they both change their emotional category to dis-like.
Besides, since the concept bad is amplifiedby the quantifier really, the weight of this con-cept in the VEO is increased in 0.15.5 Evaluation framework and resultsIn this work, two different corpora have beenused for evaluation (see Section 3): a movie re-view corpus containing 1000 sentences labeledwith either a positive or negative polarity; and anews headlines corpus composed of 1000 sen-tences labeled with an emotional intensity valuebetween -100 and 100.To determine the best machine learning algo-rithm for the task, 20 classifiers currently imple-mented in Weka55.1 Evaluating polarity classificationwere compared.
We only showthe results of the best performance classifiers: alogistic regression model (Logistic), a C4.5 deci-sion tree (J48Graph) and a support vector ma-chine (LibSVM).
The best outcomes for the threealgorithms were reported when using their de-fault parameters, except for J48Graph, where theconfidence factor was set to 0.2.
The evaluationis accomplishes using 10-fold cross validation.Therefore, 100 instances of each dataset are heldback for testing in each fold, and the additional900 instances are used for training.We first analyze the effect of expanding the cov-erage of the emotional lexicon by semi-automatically adding to WordNet Affect moresynsets labeled with emotional categories, as ex-plained in Section 3.
To this end, we compare theresults of the method using three different affec-tive lexical databases: WordNet Affect andWordNet Affect extended with 372 and 603 syn-sets, respectively.
For the sake of comparing theresults in both corpora, the news dataset has beenmapped to a -100/100 classification (-100 = [-100, 0), 100 = [0,100]).Table 2 shows the results as average precisionand accuracy of these experiments.
Note that, asthe weight of mislabeling for both classes is thesame and the classes are balanced, accuracy isequal to recall in all cases.
Labeling 975 newsynsets significantly improves the performanceof our system in both datasets and for all MLtechniques.
In particular, the best improvement isachieved by the Logistic classifier: from 52.7%to 72.4% of accuracy in the news dataset, andfrom 50.5% to 61.5% of accuracy in the moviesdataset.EmotionalLexiconMethodNews Corpus Movie ReviewsPr.
Ac.
Pr.
Ac.WNAffectLogistic 52.8 52.7 51.3 50.5J48Graph 27.7 52.6 50 50LibSVM 27.7 52.6 53.2 50.6WNAffect +372 synsetsLogistic 69.9 65.2 53.9 53.8J48Graph 70.1 64.8 55.3 55.1LibSVM 68.9 63.9 52 51.8WNAffect +603 synsetsLogistic 73.8 72.4 61.6 61.5J48Graph 73.6 70.9 60.9 60.9LibSVM 71.6 70.3 62.5 59.4Table 2: Precision and accuracy percentages achieved byour system using different affective databases.5 http://www.cs.waikato.ac.nz/ml/weka/158We have observed that, especially in the newsdataset, an important number of sentences thatare labeled with a low positive or negative emo-tional intensity could be perfectly considered asneutral.
The intensity of these sentences highlydepends on the previous knowledge and particu-lar interpretation of the reader.
For instance, thesentence ?Looking beyond the iPhone?
does notexpress any emotion itself, unless you are fan ordetractor of Apple.
However, this sentence islabeled in the corpus with a 15 intensity value.
Itis likely that these kinds of sentences introducenoise into the dataset.
In order to estimate theinfluence of such sentences in the experimentalresults, we conducted a test removing from thenews dataset those instances with an intensityvalue in the range [-25, 25].
As expected, theaccuracy of the method increases substantially,i.e.
from 72.4% to 76.3% for logistic regression.The second group of experiments is directed toevaluate if dealing with negations and quantifiersimproves the performance of the method.
To thisend, the approach described in Section 4.3 wasapplied to both datasets.
Table 3 shows thatprocessing negations consistently improves theaccuracy of all algorithms in both datasets; whilethe effect of the quantifiers is not straightforward.Even if we expected that using quantifiers wouldlead to better results, the performance in bothdatasets decreases in 2 out of the 3 ML algo-rithms.
However, combining both features im-proves the results in both datasets.
The reasonseems to be that, when no previous negation de-tection is performed, if the emotional categoryassigned to certain concepts are wrong (becausethese concepts are affected by negations), thequantifiers will be weighting the wrong emotions.Features MethodNews Corpus Movie ReviewsPr.
Ac.
Pr.
Ac.NegationsLogistic 74.2 72.5 61.7 61.6J48Graph 74.1 71.2 62.8 62.6LibSVM 72.7 71.1 62.4 60.1QuantifiersLogistic 73.7 72.2 61.9 61.9J48Graph 73.6 70.9 59.5 59.5LibSVM 72.1 70.6 61.1 59Negations +QuantifiersLogistic 74.4 72.7 62.4 62.4J48Graph 74.1 71.2 62.5 62.1LibSVM 72.8 71.2 62.6 60.5Table 3: Precision and accuracy of the system improvedwith negation and quantifier detection.The comparison with related work is difficultdue to the different datasets and methods used inthe evaluations.
For instance, Pang et al (2002)use the Movie Review Polarity Dataset, achiev-ing an accuracy of 82.9% training a SVM over abag of words.
However, their aim was to deter-mine the polarity of documents (i.e.
the wholemovie reviews) instead of sentences.
Whenworking at the sentence level, the informationfrom the context is missed, and the results areexpected to be considerably lower.
As a matterof fact, it happens that many sentences in theSentence Polarity Movie Review Dataset are la-beled as positive or negative, but do not expressany polarity when taken out of the context of theoverall movie review.
This conclusion is alsodrawn by Meena and Prabhakar (2007), whoachieve an accuracy of 39% over a movie reviewcorpus (not specified) working at the sentencelevel, using a rule based method to analyze theeffect of conjuncts.
This accuracy is well belowthat of our method (62.6%).Molianen and Pulman (2007) present a senti-ment composition model where the polarity of asentence is calculated as a complex function ofthe polarity of its parts.
They evaluate their sys-tem over the SemEval 2007 news corpus, andachieve an accuracy of 65.6%, under our sameexperimental conditions, which is also signifi-cantly lower than the accuracy obtained by ourmethod.5.2 Evaluating intensity classificationApart from identifying of polarity, we also wantto examine the ability of our system to determinethe emotional intensity in the sentences.
To thisaim, we define two intensity distributions: the 3-classes and the 5-classes distribution.
For thefirst distribution, we map the news dataset to 3-classes: negative [-100, -50), neutral [-50, 50)and positive [50, 100].
For the second distribu-tion, we map the dataset to 5-classes: stronglynegative [-100, -60), negative [-60, -20), neutral[-20, 20), positive [20, 60) and strongly positive[60, 100].
We can see in Table 4 that, as thenumber of intensity classes increases, the resultsare progressively worse, since the task is pro-gressively more difficult.IntensityclassesMethodNews CorpusPr.
Ac.2-classesLogistic 74.4 72.7J48Graph 74.1 71.2LibSVM 72.8 71.23-classesLogistic 60.2 63.8J48Graph 66 64.8LibSVM 54.8 64.65-classesLogistic 48.3 55.4J48Graph 47.3 54.8LibSVM 43.1 53.1Table 4: Precision and accuracy in three different intensityclassification tasks.159The 3-classes distribution coincides exactlywith that used in one of the SemEval 2007 Af-fective task, so that we can easily compare ourresults with those of the systems that participatedin the task.
The CLaC and CLaC-NB systems(Andreevskaia and Bergler, 2007) achieved, re-spectively, the best precision and recall.
CLaCreported a precision of 61.42 % and a recall of9.20%; while CLaC-NB reported a precision of31.18% and a recall of 66.38%.
Our methodclearly outperforms both systems in precision,while provides a recall (which is equal to the ac-curacy) near to that of the best system.
Besides,our results for both metrics are well-balanced,which does not occur in the other systems.Regarding the 5-classes distribution evalua-tion, to the authors?
knowledge no other workhas been evaluated under these conditions.
How-ever, our system reports promising results: using5 classes it achieves better results than other par-ticipant in the SemEval task using just 3 classes(Chaumartin, 2007; Katz et al, 2007).5.3 Evaluating the effect of  word ambiguityon sentiment analysisA further test has been conducted to examine theeffect of word ambiguity on the classificationresults.
To this aim, we repeated the experimentsabove without using WSD.
First, we simply as-signed to each word its first sense in WordNet.Second, we selected these senses randomly.
Theresults are presented in Table 5.
We only showthose of the best algorithm for each intensity dis-tribution.Intensity classes MethodNews CorpusPr.
Ac.2-classes (Logistic)WSD 74.4 72.61st Sense 71.6 69.3Random Sense 69.1 64.13-classes (J48Graph)WSD 66 64.81st Sense 59 62.9Random Sense 50.8 615-classes  (Logistic)WSD 48.3 55.41st Sense 43.7 53.8Random Sense 46.8 51.6Table 5: Precision and accuracy for three different worddisambiguation strategies.It can be observed that, even though the use ofword disambiguation improves the classificationprecision and accuracy, the improvement withrespect to the first sense heuristic is less than ex-pected.
This may be due to the fact that thesenses of the words in WordNet are ranked ac-cording to their frequency, and so the first senseof a word is also the most frequent one.
Besides,the Most Frequent Sense (MFS) heuristic inWSD is usually regarded as a difficult competi-tor.
On the contrary, the improvement with re-spect to the random sense heuristic is quite re-markable.6 Conclusions and future workIn this paper, a novel approach to sentence levelsentiment analysis has been described.
The sys-tem has resulted in a good method for sentencepolarity classification, as well as for intensityidentification.
The results obtained outperformthose achieved by other systems which aim tosolve the same task.Nonetheless, some considerations must benoted.
Even with the extended affective lexicon,around 1 in 4 sentences of each corpus has notbeen assigned any emotional category, some-times because their concepts are not labeled inthe lexicon, but mostly because their concepts donot have any emotional meaning per se.
A test onthe news corpus removing those sentences notlabeled with any emotional meaning has beenperformed for the 2-classes classification prob-lem, allowing the method to obtain an accuracyof 81.7%.
However, to correctly classify thesesentences, it would be necessary to have addi-tional information about their contexts (i.e.
thebody of the news item, its section in the newspa-per, etc.
).Finally, the authors plan to extend the methodto deal with modal and conditional operators,which will allow us to distinguish among situa-tions that have happened, situations that are hap-pening, situations that could, might or possiblyhappen or will happen, situations that are wishedto happen, etc.AcknowledgmentsThis research is funded by the Spanish Ministryof Science and Innovation (TIN2009-14659-C03-01), the Comunidad Autonoma de Madridand the European Social Fund through the IVPRICIT program, and the Spanish Ministry ofEducation through the FPU program.ReferencesJulian Brooke.
2009.
A Semantic Approach to Auto-mated Text Sentiment Analysis.
Simon FraserUniversity.
Ph.
D. Thesis.Jorge Carrillo de Albornoz, Laura Plaza and PabloGerv?s.
2010.
Improving Emotional Intensity Clas-160sification using Word Sense Disambiguation.
Re-search in Computing Science 46 :131-142.Fran?ois-R?gis Chaumartin.
2007.
UPAR7: A Know-ledge-based System for Headline Sentiment Tag-ging.
In Proceedings of the 4th Workshop on Se-mantic Evaluations (SemEval 2007), pages 422-425.Ann Devitt and Khurshid Ahmad.
2007.
SentimentPolarity Identification in Financial News: A Cohe-sion-based Approach.
In Proceedings of the 45thAnnual Meeting of the ACL, pages 984-991.Andrea Esuli and Fabrizio Sebastiani.
2006.
Deter-mining Term Subjectivity and Term Orientation forOpinion Mining.
In Proceedings of the 11th Confe-rence of the EACL, pages 193-200.Minging Hu and Bing Liu.
2004.
Mining and Summa-rizing Customer Reviews.
In Proceedings of the10th ACM SIGKDD Conference on KnowledgeDiscovery and Data Mining, pages 168-177.Lifeng Jia, Clement Yu and Weiji Meng.
2009.
TheEffect of Negation on Sentiment Analysis and Re-trieval Effectiveness.
In Proceeding of the 18thACM Conference on Information and KnowledgeManagement, pages 1827-1830.Phil Katz, Matthew Singleton and Richard Wicen-towski.
2007.
SWAT-MP: the SemEval-2007 Sys-tems for Task 5 and Task 14.
In Proceedings of the4th Workshop on Semantic Evaluations (SemEval2007), pages 308-313.Alistair Kennedy and Diana Inkpen.
2006.
SentimentClassification of Movie Reviews Using ContextualValence Shifters.
Computational Intelligence22(2): 110-125.Soo-Min Kim and Eduard Hovy.
2004.
Determiningthe Sentiment of Opinions.
In Proceedings of COL-ING 2004, pages 1367-1373.Justin Martineau and Tim Finin.
2009.
Delta TFIDF:An Improved Feature Space for Sentiment Analy-sis.
In Proceedings of the 3rd AAAI InternationalConference on Weblogs and Social Media.Arun Meena and T.V.
Prabhakar.
2007.
SentenceLevel Sentiment Analysis in the Presence of Con-juncts Using Linguistic Analysis.
In Proceedingsof ECIR 2007, pages 573-580.George A. Miller, Richard Beckwith, Christiane Fell-baum Derek Gross and Katherine Miller.
1990.
In-troduction to WordNet: An On-Line Lexical Data-base.
International Journal of Lexicography3(4):235-244.Karo Moilanen and Stephen Pulman.
2007.
SentimentComposition.
In Proceedings of RANLP 2007,pages 378-382.Roser Morante and Walter Daelemans.
2009.
A Meta-learning Approach to Processing the Scope of Ne-gation.
In Proceedings of the CONLL 2009, pages21-29.Bo Pang, Lillian Lee and Shivakumar Vaithyanathan.2002.
Thumbs up?
Sentiment Classification usingMachine Learning Techniques.
In Proceedings ofCoRR 2002.Bo Pang and Lillian Lee.
2004.
A Sentimental Educa-tion: Sentiment Analysis using Subjectivity Sum-marization based on Minimum Cuts.
In Proceed-ings of the 42nd  Annual Meeting of the ACL, pages271-278.Siddharth Patwardhan, Satanjeev Banerjee and TedPedersen.
2005.
SenseRelate::TargetWord - A Ge-neralized Framework for Word Sense Disambigua-tion.
In Proceedings of the ACL 2005 on Interac-tive Poster and Demonstration Sessions, pages 73-76.Livia Polanyi and Annie Zaenen.
2006.
ContextualValence Shifters.
Computing Attitude and Affect inText: Theory and Applications.
In The InformationRetrieval Series 20, pages 1-10.Randolph Quirk, Sidney Greenbaum, Geoffrey Leechand Jan Svartvik.
1985.
A Comprehensive Gram-mar of the English Language.
Longman.Carlo Strapparava and Alessandro Valitutti.
2004.Wordnet-Affect: an Affective Extension of Word-Net.
In Proceedings of the LREC 2004, pages1083-1086.Peter D. Turney.
2002.
Thumbs up or Thumbsdown?
: Semantic Orientation applied to Unsuper-vised Classification of Reviews.
In Proceedings ofthe 40th Annual Meeting of the ACL, pages 417-424.Casey Whitelaw, Navendu Garg and Shlomo Arga-mon.
2005.
Using Appraisal Groups for SentimentAnalysis.
In Proceedings of the 14th ACM Confe-rence on Information and Knowledge Manage-ment, pages 625-631.Janyce M. Wiebe, Rebecca F. Bruce and Thomas P.O?Hara.
1999.
Development and Use of a Gold-standard Data Set for Subjectivity Classification.
InProceedings of the 37th Annual Meeting of theACL, pages 246-253.Theresa Wilson, Janyce Wiebe and Paul Hoffman.2005.
Recognizing Contextual Polarity in Phrase-level Sentiment Analysis.
In Proceedings of theHLT-EMNLP 2005, pages 347-354.161
