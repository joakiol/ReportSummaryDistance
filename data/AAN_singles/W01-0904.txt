Translating Treebank Annotation for EvaluationStephen Watkinson and Suresh ManandharDepartment of Computer Science,University of York,York YO10 5DD,UK.stephen.watkinson@cs.york.ac.uksuresh.manandhar@cs.york.ac.ukAbstractIn this paper we discuss the need forcorpora with a variety of annotationsto provide suitable resources to evalu-ate different Natural Language Process-ing systems and to compare them.
Asupervised machine learning techniqueis presented for translating corpora be-tween syntactic formalisms and is ap-plied to the task of translating the PennTreebank annotation into a CategorialGrammar annotation.
It is comparedwith a current alternative approach andresults indicate annotation of broadercoverage using a more compact gram-mar.1 IntroductionAnnotated corpora have become a vital tool forNatural Language Processing (NLP) systems, asthey provide both a standard against which resultscan be evaluated and a resource from which to ex-tract linguistic information e.g.
lexicons.
This isespecially true in any NLP task that requires theannotation of examples, e.g.
part-of-speech tag-ging, parsing and semantic annotation, where itis vital to have a correct standard against whichto compare the results of systems attempting tosolve the task.
Similarly, it is crucial in a lan-guage learning context, where what is learned canbe used to annotate examples e.g.
syntax learning,lexical learning.
In this case the learned artefactis used to annotate the examples, which can thenbe compared against the correctly annotated ver-sion.
Hence, correctly annotated corpora are vitalfor the evaluation of a very large number of NLPtasks.Unfortunately, there are often no suitably an-notated corpora for a given task.
For example, thePenn Treebank (Marcus et al, 1993; Marcus etal., 1994; Bies et al, 1994) provides a large cor-pus of syntactically annotated examples mostlyfrom the Wall Street Journal.
It is an excellentresource for tasks dealing with the syntax of writ-ten English.
However, if the annotation formal-ism (a phrase-structure grammar with some sim-ple features) does not match that of one?s NLPsystem, it is of very little use.
For example, sup-pose a parser using Categorial Grammar (Wood,1993; Steedman, 1993) is developed and appliedto the examples in the corpus.
While the bracket-ing of the examples will bear a strong relationshipto the bracketing of the treebank, the labelling ofthe lexical items and the inner nodes of the treewill be entirely different and no labelling evalua-tion will be possible.However, intuitively, plenty of syntactic infor-mation is available.
In fact, for most evaluation,all the syntactic information required is available,but in the wrong form.
It seems obvious that asystem for translating the syntactic informationbetween formalisms would be a useful tool.Here, we present a system that translates theannotation of the Penn Treebank from the stan-dard phrase structure annotation to a CategorialGrammar (CG) annotation and in the process in-duces large scale CG lexicons.
It is a data-drivenmulti-pass system that uses both predefined rulesand machine learning techniques to translate thetrees and in the process induce a large scale CGlexicon.
The system was designed to produce thelexical annotations for the sentences without nullelements (i.e.
without movement) from the PennTreebank, so that these could be used to evalu-ate the results produced by an unsupervised CGlexicon learner (Watkinson and Manandhar, 2000;Watkinson and Manandhar, 2001).The system has four major features.
Firstly,there is significant control over how the treebankis annotated.
This is vital if the results are to beused for evaluation.
Secondly, the system pre-vents propagation of translation errors throughoutthe trees by being data-driven.
Thirdly, the systemdeals elegantly with erroneous annotation, evenproviding a degree of self-correction.
Finally, theapproach is general enough to apply to other sim-ilar problems.The system is compared with a top-down alter-native based on the algorithm of Hockenmaier etal (Hockenmaier et al, 2000), which is currentlythe system which has been applied to the mostsimilar task, although it is really for CG lexiconextraction.
The comparison suggests that the al-gorithm presented here gives more compact andlinguistically elegant solutions.
Investigation alsoindicates that the corpus produced is effectivelytranslated for its purpose.In Section 2 other work in the area is briefly re-viewed.
In Section 3 the precise translation taskis described.
This is followed in Section 4 witha detailed description of the algorithms used forthis task and some discussion as to their appropri-ateness.
The results from the experiments are inSection 5.
Finally, in Section 6 the results are dis-cussed along with the contributions of the workand some suggestions for future work.2 Previous WorkThe most appropriate work to consider within thiscontext is the grammar extraction literature.
Per-haps the earliest example is the approach of Char-niak (Charniak, 1996), who simply extracted acontext-free grammar by reading off the produc-tion rules implied by the trees in the Penn Tree-bank.
While not translating the formalism of thetreebank, this has led to work extracting gram-mars of different formalisms.The majority of work is based on the most obvi-ous extension of the Charniak approach, which isto extract subtree-based grammars e.g.
the Data-Oriented Parsing (DOP) approach (Bod, 1995),or extracting Lexicalised Tree Adjoining Gram-mars (LTAGs), or more generally LexicalisedTree Grammars (LTGs) (Neumann, 1998; Xia,1999; Chen and Vijay-Shanker, 2000).
Each ap-proach involves a process that splits up the anno-tated trees in the treebank into a set of subtreesthat define the grammar.
These approaches stillcontinue to work with the syntactic data in thesame form as it is found in the corpora.A slightly different approach has been followedby Krotov et al(Krotov et al, 1998), where theyextract the grammar from the Penn Treebank likeCharniak, but then compact it.
This provides asmaller grammar of similar quality to a grammarthat has not been compacted, when a linguisti-cally motivated compaction is used.
However, theformalism remains unchanged.
Similarly, John-son (Johnson, 1998) modifies the labelling of thePenn Treebank, but remains within a CFG frame-work.Hockenmaier et al(Hockenmaier et al, 2000),although to some extent following the approachof Xia (Xia, 1999) where LTAGs are extracted,have pursued an alternative by extracting Com-binatory Categorial Grammar (CCG) (Steedman,1993; Wood, 1993) lexicons from the Penn Tree-bank.
In this case the data in the treebank istruly translated into another formalism providingan entire CCG annotation for the corpus basedon a top-down algorithm.
The lexicon is built byreading off the lexical assignments made for eachtree.
This is the most closely related work to thisresearch, especially as it translates into a formal-ism very closely related to CG.The algorithm presented by Hockenmaier et al(Hockenmaier et al, 2000) has been used to builda top-down system against which to compare ourdata-driven system.
The algorithms are both de-scribed in detail in Section 4.3 The TaskGiven a subset of the examples from the PennTreebank annotated with syntactic and part-of-speech information (slightly modified), the sys-tem should return the examples annotated withthe correct CG categories attached to the wordsof the sentence and the lexicons these imply.The context of the task explains some parts ofits definition.
The translated corpus is to be usedas a standard against which to compare the lex-ical annotation (i.e.
the categories assigned tothe words) of the output of an unsupervised CGlearner that annotated the words of the exampleswith CG categories and then extracts a proba-bilistic lexicon (see Watkinson and Manandhar(Watkinson and Manandhar, 2001) for details).Hence, there is no need for specific tree annota-tion.
The learner currently uses a slightly mod-ified subset of the treebank, which is describedbelow.3.1 The CorpusThe systems are applied to examples from thePenn Treebank (Marcus et al, 1993; Marcus etal., 1994; Bies et al, 1994) a corpus of over4.5 million words of American English annotatedwith both part-of-speech and syntactic tree infor-mation.To be exact, we are using the Treebank II ver-sion (Bies et al, 1994; Marcus et al, 1994),which attempts to address the problem of com-plement/adjunct distinction, which previous ver-sions had ignored.
While the documentation isclear that the complement/adjunct structure is notexplicitly marked (Marcus et al, 1994), the anno-tation includes a set of labels that relate to the roleof a particular constituent in the sentence.
Theselabels are attached to the standard constituent la-bel and it is possible to use heuristics to determinethe probable complement/adjunct structure in thetrees (Collins, 1999; Xia, 1999), which is obvi-ously useful in translating the annotation.The full Penn Treebank is not being used.
Asmentioned already, the current research only usessentences without null elements (i.e.
withoutmovement) from the treebank and does not in-clude any of the sentence fragments.
However,as Categorial Grammar formalisms do not usuallychange the lexical entries of words to deal withmovement, but use further rules (Wood, 1993;Steedman, 1993; Hockenmaier et al, 2000), thelexicons learned here will be valid over corporawith movement.
The extracted corpus, C1, in factcontains 5000 of the declarative sentences of fif-teen words or less (although the sentence lengthmakes little difference to either of the translationprocedures described) from the Wall Street Jour-nal section of the treebank.
To give an indicationof the complexity of the corpus, the number oftokens, i.e.
the total number of words includingrepetitions of the same word, is 47,782.
The totalnumber of unique words, i.e.
not including repe-titions of the same word, is 12,277.
We also ex-tracted C2, a 1000 example corpus (also of declar-ative sentences from the Wall Street Journal sec-tion) with 9467 tokens and 3731 words, which isused in the evaluation process.The corpora also have some small modifica-tions, which mean that adjacent nominals in thesame subtree are combined to form a single nom-inal and the punctuation is removed.
These mod-ifications are made for use with the unsuper-vised learner (Watkinson and Manandhar, 2000;Watkinson and Manandhar, 2001) to simplify thelearning process.
They may also slightly simplifythe translation process, but it is necessary for thecorpus annotation that we want.3.2 Categorial GrammarCategorial Grammar (CG) (Wood, 1993; Steed-man, 1993) provides a functional approach to lex-icalised grammar, and so can be thought of asdefining a syntactic calculus.
Below we describethe basic (AB) CG.
The current work uses thissimple form of the grammar, which suffices forthe syntactic annotation of the corpora currentlybeing used.There is a set of atomic categories in CG, whichare usually nouns (n), noun phrases (np) sen-tences (s) and sometimes prepositional phrases(pp), although this can be consider shorthand forthe full category (Wood, 1993).
It is then possibleto build up complex categories using the two slashoperators ?/?
and ?n?.
If A and B are categoriesthen A/B and AnB are categories, where (follow-ing Steedman?s notation (Steedman, 1993)) A isthe resulting category when B, the argument cate-gory, is found.
The direction of the ?slash?
func-tors indicates the position of the argument in thesentence i.e.
a ?/?
indicates that a word or phrasewith the category of the argument should imme-diately follow in the sentence.
With the ?n?
theword or phrase with the argument category shouldimmediately precede the word or phrase with thiscategory.
This is most easily seen with examples.Suppose we consider an intransitive verb like?run?.
The category that is required to completethe sentence is a subject noun phrase.
Hence, thecategory of ?run?
is a sentence that is missing apreceding noun phrase i.e.
snnp.
Similarly, witha transitive verb like ?ate?, the verb requires asubject noun phrase.
However, it also requires anobject noun phrase, which is attached first.
Thecategory for ?ate?
is therefore (snnp)/np.With basic CG there are just two rules for com-bining categories: the forward (FA) and back-ward (BA) functional application rules.
Follow-ing Steedman?s notation (Steedman, 1993) theseare:X=Y Y ) X (FA)Y XnY ) X (BA)where X and Y are CG categories.
In Figure 1the parse derivation for ?John ate the apple?
ispresented, showing examples of how these rulesare applied to categories.ate the applenp (s\np)/np np/n nnps\npsFAFABAJohnFigure 1: An Example Parse in Basic CGThe CG formalism described above has beenshown to be weakly equivalent to context-freephrase structure grammars (Bar-Hillel et al,1964).
While such expressive power covers alarge amount of natural language structure, ithas been suggested that a more flexible and ex-pressive formalism may capture natural languagemore accurately (Wood, 1993; Steedman, 1993).In future we may consider applying the principledeveloped here to perform translations to thesemore complex formalisms, although many of thechanges will not actually change the lexical en-tries, just the way they can be combined.4 Alternative ApproachesThis section presents the two approaches to trans-lation that are being compared.
Firstly, there isalsoH(RB)A(ADVP)declinedH(VBD)H(VP)the dollarA(DT) H(NN)C(NP-SBJ)H(VP)H(S)Figure 2: A tree with constituents markedthe top-down method, which is a version of thealgorithm described by Hockenmaier et al(Hock-enmaier et al, 2000), but used for translating intosimple (AB) CG rather than the Steedman?s Com-binatory Categorial Grammar (CCG) (Steedman,1993).
The algorithm here does not need to dealwith movement, as the corpus does not containany.
The atomic pp category is included in the CGwith this approach, but not with our approach, asit is a convenient shorthand for the prepositionalphrase category.The second approach is a multiple-pass data-driven system.
Rules for translating the trees areapplied in order of complexity starting with sim-ple part-of-speech translation and finishing with acategory generation stage.4.1 Top-Down Category GenerationThe algorithm has two stages.Mark constituents All the nodes of all treesare marked with their roles i.e.
as heads, com-plements or adjuncts.
While Hockenmaier etal (Hockenmaier et al, 2000) are unclear, it isassumed that this is achieved using heuristics.Collins (Collins, 1999) describes such a set ofheuristics, which are used with some minor mod-ifications for CG and the changed Penn Treebankannotation.
Figure 2 shows an example of an an-notated tree.Assign categories This is a recursive top-downprocess, where the top category in the tree is an s.The category of the complements is determinedby a mapping between Treebank labels and cate-gories e.g.
NP in the treebank becomes np.
Hock-enmaier et al(Hockenmaier et al, 2000) do notprovide the mapping, so it was built specially forthis system.
This mapping led to the inclusionthesalso(s\np)/(s\np)declinednpnp/np npdollars\nps\np(s\np)/(s\np) s\npFigure 3: An example with categories assignedof the pp category as shorthand for prepositionalcomplements.
It should make no difference to theannotation process, but could lead to the genera-tion of a few more categories.
The head child of asubtree is given the category of the parent plus thecomplements required, which are found by look-ing first to the left of the head and then to theright, and adding them in the order they shouldprocessed in.
Finally, adjuncts are assigned thegeneric X=X or XnX where X is the head cate-gory with the complements removed which havebeen dealt with before the adjunct is processed.Figure 3 shows an example of a tree with the cat-egories assigned to it.This algorithm has several advantages.
It issimple and robust and has been shown by Hock-enmaier et al(Hockenmaier et al, 2000) to pro-vide good lexical annotation leading to usefulCCG lexicons.However, it has two main disadvantages.Firstly, there is no control over category gener-ation other than the rather weak constraints of theformalism and the heuristic syntactic roles.
Thisis likely to lead to some linguistically implausibleannotation.
Secondly, the top-down nature of thealgorithm is likely to lead to any translation errorsbeing propagated down the tree, which will leadto some unusual and large categories, as Hocken-maier et al(Hockenmaier et al, 2000) report.4.2 Bottom-Up SequentialOur system uses a four stage process, where thetype of translation changes at each stage.4.2.1 Stage 1: Parts-of-SpeechThis is the simplest level of translation.
Themapping between the Penn Treebank part-of-speech annotation and the CG category annota-tion is many-to-many, but some parts-of-speechthe dollar also declinedSVPNP-SBJNNnp/nADVPRBVPVBDFigure 4: Example of the output of Stage 1can be translated directly into categories usingsimple rules e.g.
the following rule states thatwords with the determiner part of speech (DT)can be translated into the CG category np/n.DT!
np=nThe system passes through the full set of ex-amples and translates the appropriate parts-of-speech.
See Figure 4 for an example of the outputof this stage.4.2.2 Stage 2: SubtreesThe next pass through the data allows morecomplex rules to be used.
Consider the part-of-speech label NNS, used in the Penn Treebank an-notation scheme to indicate a plural noun.
Its syn-tactic role can be that of a simple noun (n) or anoun phrase (np), so we need a mechanism forchoosing between these two possibilities.The most obvious mechanism is to use the sur-rounding subtree to provide the context to selectthe correct rule.
If the NNS tag is part of a nounphrase which begins with something fulfilling thedeterminer role, then the tag should be translatedto the CG category n, otherwise it should be trans-lated as an np.The algorithm for applying the set of context-based rules is a simple matching process through-out the treebank.
Figure 5 shows the output fromthis stage on an example.4.2.3 Stage 3: Structural HeuristicIn this stage, the system uses further knowl-edge to attempt to inform the translation process.Where words have not been translated, the systemannotates the subtree with the head, complementsand adjuncts using a modified version of Collins?heuristics (Collins, 1999).the dollar also declinedSVPNP-SBJnp/nADVPRBVPVBDnFigure 5: Example of the output of Stage 2the also declinedSVPNP-SBJnp/nADVPRBVPs\npndollarFigure 6: Example of the output of Stage 3Further categories can now be obtained.
Forexample, if the head of the subtree requires an npcategory to its right as its first complement andthere is a word marked as a complement in thisposition, then it can be translated as an np.
Alter-natively, if the head category is unknown, but it isverbal according to the Penn Treebank label thenlooking at the categories of the complements candetermine the type of verb it is e.g.
no comple-ments following a verb indicates a CG categorysnnp.
Figure 6 shows the effects of this stage onthe example.4.2.4 Stage 4: Category GenerationIn the final stage each lexical category that hasnot been annotated is given a variable for a cat-egory.
The tree is then traversed bottom-up in-stantiating these categories by using head, com-plement and adjunct annotation and the alreadyannotated categories.
The building of head andadjunct categories follows the same process de-scribed for the top-down algorithm.
Comple-ments either gain their categories through thisprocess or have already had them assigned.
Fig-ure 7 shows the final output.This approach has two main advantages.Firstly, the user has control over the type of CGto which the treebank is translated, due to thethe also declinedss\npnp/ns\nps\npndollarnp(s\np)/(s\np)(s\np)/(s\np)Figure 7: Example of the output of Stage 4use of predefined categories for predefined con-texts.
Secondly, the bottom-up approach ensuresthat translation errors are not propagated seriouslythrough the tree.A further advantage exists that has not, as yet,been fully investigated.
The system, due to itsmulti-pass nature, has the potential for transla-tions to clash.
Experience has shown that this oc-curs when there is an annotation error, so the sys-tem can be used to highlight these and can alsoprovide some level of self-correction.
This hasnot been investigated in detail, but the current ap-proach, which gives satisfactory results, is to as-sume the head category is correct and adjust com-plements and adjuncts accordingly.
In future, asimple correction scheme could easily be addedto produce a self-correcting translator.The main weakness of the system is the re-liance upon the head/complement/adjunct anno-tating heuristics, which were not designed to beused with a CG.The system also returns some categories withvariables.
This is due in part to the heuristics andin part to the small number of rules currently usedin the early stages of the translation process.
Mostof the problem categories could be dealt with bythe addition of a few more rules in stages 2 and 3.5 ResultsHere we provide similar evaluation of the systemsas others (Hockenmaier et al, 2000; Xia, 1999)for easy comparison.
Both systems were usedtranslate C1 and C2.
C2 is used for determin-ing the coverage of the grammar used by the twosystems.
Both systems, at times, failed to trans-late examples (frequently due to annotation errorin the original treebank).
The top-down systemfailed on 60 and 15 examples from C1 and C2Top-down Bottom-upNo.
of cats 167 106Lexicon size 15887 15136Ave.
cats/word 1.31 1.25Ave.
cat size 8.02 5.12Table 1: Table of category and lexicon informa-tion on the translated corporaFreq.
Range Number of CategoriesTop-down Bottom-up1f1 42 292f10 61 3411f20 14 921f100 24 11101f1000 17 131001f5000 7 75001f10000 1 210001f12000 0 112001f15000 1 0Table 2: Table of the category frequencies forboth approachesrespectively.
The bottom-up system failed on 66and 15 examples from C1 and C2 respectively.Table 1 describes the type of categories usedto translate C1 and the size of the lexicons gen-erated.
Categories with variables in were ig-nored, as they could usually be unified with analready existing category.
With this in mind, thebottom-up algorithm extracted a more compactlexicon.
The average category sizes (the numberof slash operators in categories) are interesting,as they indicate the profligacy of the top-down al-gorithm in creating unwieldy categories, whereasthe bottom-up approach uses smaller and, on in-spection, more plausible categories.
These resultsseem, in part, to vindicate the choice of a con-trolled bottom-up approach.Tables 2 and 3 present the results for both sys-tems for the frequency distribution of categories(i.e.
the number of categories that appeared witha particular frequency) and the frequency distri-bution of the number of categories for a word (i.e.the number of words that had a particular num-ber of categories).
The trends for both systemsare similar.
There are a large number of cate-gories that appear very infrequently, these tendFreq.
Range Word frequencyTop-down Bottom-upf=1 10486 10377f=2 1263 1264f=3 264 264f=4 86 865  f  9 100 10010  f  14 20 2015  f  24 10 1025  f  30 2 2Table 3: Frequencies of words appearing in a fre-quency range of number of categoriesto be the larger, generated categories and oftenfit unusual circumstances e.g.
misannotation ofthe treebank, or mistakes in the use of the heuris-tics.
The bottom-up approach has many fewer ofthese categories, indicating the problem of propa-gating of errors down the tree with the top-downapproach.
There are also a few exceptionally fre-quent categories, these are noun phrases, nouns,and some of the common verbs.The number of categories per word is simi-lar, suggesting the approaches are similar in theirability to produce the variety of categories re-quired for words.While these figures give some indication of thequality and compactness of the translation, it isuseful to determine the coverage of the lexiconextracted from C1 by comparing it with a lexiconextracted from C2 and so determine the qualityand generality of the lexicon that has been pro-duced in the translation.
Table 4 shows the com-parison.
Here entry means the C1 lexicon con-tains an entry the same as the C2 entry.
kwkcmeans that the entry from C2 is not in C1, butboth the word and the category are known.
kwucmeans the word is in the C1 lexicon, but the cat-egory is not.
Finally, uw indicates that the wordis in C1.
Despite a smaller lexicon and a smallernumber of categories, the bottom-up system givesbetter coverage.
Note especially that there are nounknown categories with with the bottom-up ap-proach and that the percentage of exact entries ismuch higher.Top-down Bottom-upCategories 98 65New categories 4 0entry % 37.29 48.31kwkc % 10.55 11.09kwuc % 11.46 0uw % 40.70 40.60Table 4: Table comparing the coverage of the twoapproaches6 ConclusionsThe system presented provides a useful and accu-rate method for translating the annotation of thePenn Treebank into a CG annotation.
Compar-isons with an alternative approach suggest that theincrease of control provided by the system lead toa more accurate and compact translation, which ismore linguistically plausible.
Most importantly,the system is flexible enough to allow the user toannotate corpora with the kind of CG they are in-terested in, which is vital when it is to be used forevaluation.It would be useful to expand the systems towork on the full treebank i.e.
including sentenceswith movement (see Hockenmaier et al(Hocken-maier et al, 2000) for discussion of a possiblemethod).
The correcting of the annotation of thetreebank during translation should also be inves-tigated further.ReferencesY.
Bar-Hillel, C. Gaifman, and E. Shamir.
1964.
Oncategorial and phrase structure grammars.
In Lan-guage and Information, pages 99 ?
115.
Addison-Wesley.
First appeared in The Bulletin of the Re-search Council of Israel, vol.
9F, pp.
1-16, 1960.Ann Bies, Mark Ferguson, Karen Katz, and RobertMacIntyre, 1994.
Bracketing Guidlines for Tree-bank II Style Penn Treebank Project.Rens Bod.
1995.
Enriching Linguistics with Statis-tics: Performance Models of Natural Language.Ph.D.
thesis, Department of Computational Lin-guistics, Universiteit van Amsterdam.Eugene Charniak.
1996.
Treebank grammars.
In Pro-ceedings of AAAI/IAAI.John Chen and K. Vijay-Shanker.
2000.
Automatedextraction of tags from the Penn Treebank.
In Pro-ceedings of the 6th International Workshop on Pars-ing Technologies.Michael Collins.
1999.
Head-Driven Statistical Mod-els for Natural Language Parsing.
Ph.D. thesis,University of Pennsylvania.Julia Hockenmaier, Gann Bierner, and JasonBaldridge.
2000.
Providing robustness for accg system.
In Proceedings of the Workshop onLinguistic Theory and Grammar Implementation,ESSLLI 2000.Mark Johnson.
1998.
PCFG models of linguistictree representations.
Computational Linguistics,24(4):613?632.Alexander Krotov, Mark Hepple, Robert Gaizauskas,and Yorick Wilks.
1998.
Compacting thePenn Treebank grammar.
In Proceedings ofCOLING?98?ACL?98.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of English: the Penn Treebank.
Computa-tional Linguistics, 19.Mitchell Marcus, Grace Kim, Mary AnnMarcinkiewicz, Robert MacIntyre, Ann Bies,Mark Ferguson, Karen Katz, and Britta Schas-berger.
1994.
The Penn Treebank: Annotatingpredicate argument structure.
In The ARPA HumanLanguage Technology Workshop.Gu?nter Neumann.
1998.
Automatic extraction ofstochastic lexicalized tree grammars from tree-banks.
In Proceedings of the 4th Workshop on tree-adjoining grammars and related frameworks.Mark Steedman.
1993.
Categorial grammar.
Lingua,90:221 ?
258.Stephen Watkinson and Suresh Manandhar.
2000.Unsupervised lexical learning with categorial gram-mars using the LLL corpus.
In James Cussensand Sas?o Dz?eroski, editors, Learning Language inLogic, volume 1925 of Lecture Notes in ArtificialIntelligence.
Springer.Stephen Watkinson and Suresh Manandhar.
2001.
Apsychologically plausible and computationally ef-fective approach to learning syntax.
To appear atCoNLL?01.Mary McGee Wood.
1993.
Categorial Grammars.Linguistic Theory Guides.
Routledge.
General Ed-itor Richard Hudson.F.
Xia.
1999.
Extracting tree adjoining grammarsfrom bracketed corpora.
In Proceedings of the 5thNatural Language Processing Pacific Rim Sympo-sium (NLPRS-99).
