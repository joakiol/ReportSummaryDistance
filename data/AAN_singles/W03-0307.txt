Phrase-based Evaluation of Word-to-Word AlignmentsMichael Carl and Sisay FissahaInstitut f?r Angewandte Informationsforschung66111 Saarbr?cken, Germany{carl;sisay}@iai.uni-sb.deAbstractWe evaluate the English?French word align-ment data of the shared tasks from a phrasealignment perspective.
We discuss pe-culiarities of the submitted data and the testdata.
We show that phrase-based evaluation isclosely related to word-based evaluation.
Weshow examples of phrases which are easy toalign and also phrases which are difficult toalign.12IntroductionWe describe a phrase-based evaluation of the 16 Eng-lish-French alignment submissions for the shared taskon Parallel Texts.
The task was to indicate which wordtoken in an English alignment sample corresponds towhich word token in the French alignment sample.
Twotypes of submission were permitted: for restricted sub-missions were allowed a ?sentence?
aligned segment ofthe Canadian Hansards to train the systems while unre-stricted submission would be allowed to use additionalresources.
The performance of the systems was com-pared for a set of 447 English?French hand-alignedtest samples which were also taken from the CanadianHansards.Five institutes participated in the English?Frenchalignment task, submitting a total of 16 sets of align-ment data.
To evaluate the submitted data, we extractedbilingual phrase dictionaries from the word-alignmentdata.
The extracted dictionaries of the submitted datawere compared with the extracted dictionary of the testdata.We first discuss word-to-word and phrase-to-phrasealignment format.
We present two different methods forextracting bilingual dictionaries from the word align-ment data: a minimal dictionary contains the least num-ber of unambiguous phrase-to-phrase translations whilean exhaustive dictionary contains all possible unambi-guous translations.
We examine the test data (i.e.
the?golden standard?)
and the submitted alignment data.We discuss their peculiarities and give examples ofphrases easy and difficult to align.Types of AlignmentThe test set consists of 447 alignment samples from theCanadian Hansards which were pre-tokenized.
A three-tuple containing the alignment number, an English wordoffset and a French word offset would indicate an exactword-to-word translation1.
The submitted data was sup-posed to comply with this word-to-word alignment for-mat.
In example 1 the English sentence has 15 tokenswhile the French sentence has 16 tokens.
Example 1shows the word-to-word alignment data of sample 91for submission 12 and a plot of the data.Example 1:   Alignment sample 91:English (vertical):i was not asking for a detailed explana-tion as to what he was doing .French (horizontal):je ne lui ai pas demand?
de me fournir detelles explications sur ces activit?s .Plot and word alignment data for submission 12:Sample En Fr15                 x 91 15 1614               x   91 14 1413         x         91 13  812         x         91 12  811             x     91 11 1210          x        91 10  909            x      91  9 1108             x     91  8 1207          x        91  7  906          x        91  6  905             x     91  5 1204    x              91  4  303   x               91  3  202         x         91  2  801  x                91  1  100     xxxx  x  x x012345678901234561 There was also an optional slot to indicate whether thisalignment would be [S]ure or [P]robable.
We ignore this in-formation in our evaluation.Figure1:   Number of word alignment points and size of extracted dictionaries050001000015000200002500030000350001 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17null-alignmentsubmittedtext-dic1text-dic2align-dic12.1 Word-to-word alignmentThere are two underlying assumptions in word-to-wordalignment:(i) each word token on the English side can haveany number of word correspondences -- includ-ing zero -- on the French side and vice versa.Word alignments may have crossing and am-biguous branches.
For instance in example 1,the French word ?me?
on position 8 has thetranslations ?was?, and ?he?, while ?ai?
has noconnection to the English side.
(ii) words (English or French) for which no align-ments are given in the submitted data are as-signed a null-alignment.Example 1 has 22 word alignment points, where theevaluators inserted 7 null-alignments.
In some cases (i.e.submission 11) this insertion accounts for almost 50%of the alignment data.
In figure 1, ?null-alignment?
plotsthe union of the submitted alignment data and the in-serted null-alignments.
Null-alignments were not addedto submission 16 as it provides alignment informationfor every word.
The last data point on the x-axis (i.e.
17)epresents the test data.s outlined in Melamed (1998), a sequence of words(ii) an English phrase may only be unambiguouslylinked to exactly one French phrase and viceversa.Phrase-to-phrase alignments can be nested.
For instance,the shorter English?French phrase translation 9-9 <-> 11-11 is included in the longer phrase translation5-11 <-> 9-12:5-11 9-12: for a detailed explanationas to what<-> fournir de telles explications9-9 11-11: as <-> tellesIn this way structural information can be stored.
On theother hand, we do not allow ambiguous phrase align-ments as e.g.
:8-8 12-12 explanation <-> explications11-11 12-12  what <-> explicationsWhen extracting phrase-to-phrase translations from theword-to-word alignment data we include a sufficientcontext which disambiguates the phrases.
Given theword alignment data in example 1, the minimum con-text required to disambiguate the French word ?explica-tions?
is the phrase 5-11 <-> 9-12.From the word alignment data we generate bilingualdictionaries in two different ways: a minimal dictionarycontains only the shortest unambiguous phrase-to-rA2.2which translates in a non-compositional fashion into atarget sequence is exhaustively linked (see example 2).Phrase-to-phrase alignmentPhrase-to-phrase alignment is represented by intervalsindicating the starting and ending words of the phrases.In phrase-to-phrase alignment:(i) a sequence of English word tokens (i.e.
aphrase) are mutually linked with sequences ofFrench word tokens (i.e.
a French phrase)2.2 We do not use the term ?phrase?
here in its linguistic sense: aphrase in this paper can be any sequence of words, even ifthey are not a linguistic constituent.phrase translations.
For instance, from the alignmentdata in example 1, the following 8 entries are generatedas a minimal dictionary:3En  Fr1-1   1-12-13  2-123-3   2-24-4   3-35-11   9-129-9 11-1114-14 14-1415-15 16-163 As shorthand notation we use here the offset numbers.
In thegenerated dictionary, we have extracted the sequences ofwords instead of the offset numbers.In an exhaustive dictionary all possible unambiguousphrase translations are extracted.
An exhaustive diction-ary is a superset of the minimal dictionary.
For example1, seven additional entries are generated:En  Fr1-13  1-121-14  1-141-15  1-162-14  2-142-15  2-163-4   2-314-15 14-16Note that these additional phrase translations can becompositionally generated with the minimal dictionary.To evaluate the word alignment data through phrasalalignments, we generated three types of dictionaries forall 16 submissions and the test data:(i) an alignment-based minimal dictionary,align-dic1; actually 447 small dictionar-ies for each sample alignment.
(ii) a text-based minimal dictionary (text-dic1)which is the union of the align-dic1.
(iii) an exhaustive text-based dictionary (text-dic2) which is the union of exhaustivealignment dictionaries.As can be seen from figure 1, the size of the ex-haustive dictionary (text-dic2) is in most casesmuch bigger than those of the minimal dictionar-ies align-dic1 and text-dic1.
The reason is due tothe way the data has been aligned.3 The word alignment dataIn this section we show that the test alignmentdata is structurally different from the submitteddata.
The hand aligned test data reflects thephrasal nature of the alignments, while the sub-missions are to a greater extent compositional.The test data (see set 17 in figure 1) has about twicethree times as many word-alignment points thansubmissions.
While this often leads to high precisand lower recall for word alignment, the reverse is tfor the extracted phrasal dictionaries (also figure 3).
Ttest alignment data of sample 91 contains 68 wordword alignment points shown in example 2; about ftimes the average number of word alignment pointsthis sample.
Comparing example 2 with the submitdata of submission 16 (example 3) brings to lightphrasal nature of the test set.Extracting a minimal phrase dictionary from testword alignment data in example 2 produces the folloing three entries1-14  1-155-8   7-1215-15 16-16The following additional entry is generated in thehaustive dictionary:1-15 <-> 1-16.
Note that mword alignments could have been possible here, for in-stance:i <-> jeasking <-> demand?explanation <-> explicationsnot <-> ne , pasDespite the existence of some fine-grained word-to-word correspondences in the test data, human alignerstend to mark phrasal translations.
In contrast to thephrasal nature of the test alignments, most submissionsshow a more compositional alignment structure.
Forinstance, alignment data of sample 91 for submission 16has 18 word-to-word alignment points (example 3).When the alignments are more compositional, morecoherent phrasal translations can be extracted.
Thus, theminimal phrase dictionary extracted from example 3hile t -efore ethe ltprec -Example 2: sample 91of test set:Example 3: sample 91of submission 16tothecontains 13 entries, wtains 54 entries.
Therphrase dictionaries onin high recall and low15                 x14              xxx13              xxx12    x         xxx11              xxx10              xxx09              xxx08        xxxxxx07        xxxxxx06        xxxxxx05        xxxxxx04  xxxxxx03  xxxxxx02  xxxxxx01  xxxxxx00012345678901234564ionruehe-to-ourfortedthesetw-ex-oremitted word alignment datin high precision and low rPhrase-based EvaFigure 2 shows the correlaextracted dictionaries and tsure and probable).
For eaccalculated as 2*precisioncurves for the alignment-bshow the mean f-score comRoughly all submissionsword-to-word alignment anWe wanted to see what farecall.
A correlation betwalignment and its average he exhaustive dictionary con, we expect that mapping thtest dictionaries would resuision while mapping the sub15                 x14                x13              xx12         x11           x10           x09            x08             x07          x06        x05       x04       x03      x02   xxx01  x0001234567890123456a on the test data would resultecall.luationtion of the f-score of the threehe word alignment data (bothh submission the f-score was*recall/precision+recall.
Theased dictionaries (align-dic1)puted over all 447 samples.show a similar pattern ford phrase dictionaries.ctors influence precision andeen the length of the samplerecall and precision is shownFigure 2:  f-score of word alignments and dictionaries01020304050607080901 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16sureprobalign-dic1text-dic1text dic2in figure 3.
As one would have expected, the graphshows a tendency that shorter samples are easier to align(higher precision and recall) than longer samples.
How-ever, there is higher variation among shorter alignmentsthan among longer sample alignments which indicatesunpredictability of shorter samples.
As an example con-sider alignment sample 7 (length 3):hear, hear !
<-> bravo !The extracted minimal test dictionary contains the twoentries:hear,hear <-> bravo!
<->!While most of the minimal dictionaries extracted fromthe submitted data contain the entries:hear <-> bravo!
<-> !This leads to a value of 50 for recall and precision forboth word and phrase alignments.
The average recalland precision of sample 7 (length 3) is 53,1 and 57,8.Figure 3 also shows that PD-recall (phrasal dictionary)is higher than PD-precision as the samples becomelonger.
For example, sample 91 (length 15,5) has PD-recall and PD-precision values of 65, and 50,2 respec-tively.
For word-to-word evaluation, however, WD-precision is higher than WD-recall.
For sample 91(length 15,5)  the WD-recall and WD-precision valuesare 18,03 and 53,86 respectively.Next we wanted to see which parts in the sample align-ments would be easy and which parts would be difficultto align.
We assume that correct translations which ap-pear in all submissions would be easy to find whiletranslations which occur only in the test set but in noneof the submissions would be difficult to find.
Finally,the same noisy translations produced by all submissionswould indicate mistakes in the test data.
The cardinalityof these sets is shown in the table below.Intersection of text-dic1 text-dic2correct 150 434missing 837 1949noise 11 22There were 150 one-word entries inthe intersection of the correct transla-tions contained in all 16 dictionariestext-dic1.
These translations includetransfer rules which are easy to dis-cover such as numbers, functionwords, pronouns, frequent contentwords and also domain specific trans-lations:Figure3:  length of alignments vs. Recall and Precision0102030405060708090100LengthLengthPD_RecallPD_PrecisionWD_RecallWD_Precision1) pronounshe <-> ilit <-> ilthere <-> il2) frequent content wordswomen <-> femmeswork <-> travaillentcompulsory <--> obligatoiresay <-> diresays <-> dit3) function words 4,5 79,5 1214,5 1719,5 2224,5 27such <-> telto <-> deto <-> pour5) Text typical translations:House <-> ChambreThe set of translation equivalences missing in all sub-missions was much larger.
There were only the follow-ing five one-word equivalences:1) on-word translations:and <-> puisquebalance <-> niveaudo <-> faitper <-> levery <-> fondamentalementMost of the missing entries were multi-word transla-tions, such as idioms, compound words etc.1) idiomatic expressionsA buck is a buck is a buck <-> unepiastre est toujours une piastrethank you very much <-> je vous re-mercie2) compound:Canadian Wheat Board <->Commission canadienne de le bl?3) complex prepositionsas for <-> en ce qui concerne4) complex verbs and negationdoes not like <-> ne aime paswill be <-> feront5) adverbs and adjective phrasesprevious <-> qui me a pr?c?d?a good thing <-> int?ressant6) unresolved pronounsthe government <-> ilThere were also 11 noisy entries which occurred in allgenerated submissions dictionaries but not in the testdata dictionary.
The obvious explanation for this is,again, the phrasal nature of the test data: single wordtranslations would be hidden in phrase translations andnot extracted as separated word translations:before <-> avantbelieve <-> croisdays <-> joursevery <-> chacunefacilities <-> installationsjobs <-> emploipositive <-> positifspublic <-> publicrepresentations <-> instanceswhy <-> commentwill <-> servira56SubmissionsThis section lists the origin of the submitted data.
Amore detailed description can be found in the systemdescription contained in these proceedings.1 BiBr.EF.7Limited Resources  7. intersection of 1 & 32 BiBr.EF.1Limited Resources 1.
Baseline of Bi-lingual Bracketing3 BiBr.EF.2Unlimited Resources 2.
Baseline of Bi-lingual Bracket-ing + POS (Brill's POS tagger for English only)4 BiBr.EF.8Unlimited Resources 8.  intersection of 3 & 65 BiBr.EF.3Unlimited Resources 3.
Baseline of Bi-lingual Bracket-ing + POS (Brill's POS tagger for English only) + Eng-lish_Chunker.6 BiBr.EF.4Limited Resources 4.  reverse direction of (1)7 BiBr.EF.5Unlimited Resources 4.  reverse direction of (2)8 BiBr.EF.6Unlimited Resources 4.  reverse direction of (3)9 data withdrawn10 UMD.EF.Limited Resources Trained on House and Senate Data11 ProAlign.EF.1Unlimited Resources ProAlign uses the cohesion be-tween the source and target languages to constrain thesearch for the most probable alignment (based on anovel probability model).
The extra resources include:An English parser A distributional similarity databasefor English words.12 data withdrawn13 XRCE.Base.EF.1Limited Resources GIZA++ with English and Frenchlemmatizer (no trinity lexicon)14 XRCE.Nolem.EF.2Limited Resources GIZA++ only (no lemmatizer, notrinity lexicon), Corpus used: Quarter15 XRCE.Nolem.EF.3Limited Resources GIZA++ only (no lemmatizer, notrinity lexicon), Corpus used: Half16 ralign.EF.1Limited Resources Recursive parallel segmentation oftexts; scoring based on IBM-217 test data (golden standard)ReferencesUlrich Germann, editor (2001).
Aligned Hansards of the36th Parliamentof Canada.
http://www.isi.edu/natural-language/download/hansard/index.htmlI.
Dan Melamed (1998).
Annotation Style Guide for theBlinker Project, IRCS Technical Report #98-06,http://www.cs.nyu.edu/~melamed/ftp/papers/styleguide.ps.gzFranz Josef Och, Hermann Ney (2000).
A Comparisonof Alignment Models for Statistical Machine Transla-tion.. COLING 2000..http://www-i6.informatik.rwth-aachen.de/Colleagues/och/COLING00.ps
