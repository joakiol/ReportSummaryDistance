Proceedings of NAACL HLT 2009: Short Papers, pages 29?32,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsLarge-scale Computation of Distributional Similarities for QueriesEnrique AlfonsecaGoogle ResearchZurich, Switzerlandealfonseca@google.comKeith HallGoogle ResearchZurich, Switzerlandkbhall@google.comSilvana HartmannUniversity of StuttgartStuttgart, Germanysilvana.hartmann@ims.uni-stuttgart.deAbstractWe present a large-scale, data-driven approachto computing distributional similarity scoresfor queries.
We contrast this to recent web-based techniques which either require the off-line computation of complete phrase vectors,or an expensive on-line interaction with asearch engine interface.
Independent of thecomputational advantages of our approach, weshow empirically that our technique is moreeffective at ranking query alternatives that thecomputationally more expensive technique ofusing the results from a web search engine.1 IntroductionMeasuring the semantic similarity between queriesor, more generally, between pairs of very short texts,is increasingly receiving attention due to its manyapplications.
An accurate metric of query simi-larities is useful for query expansion, to improverecall in Information Retrieval systems; for querysuggestion, to propose to the user related queriesthat might help reach the desired information morequickly; and for sponsored search, where advertisersbid for keywords that may be different but semanti-cally equivalent to user queries.In this paper, we study the problem of measuringsimilarity between queries using corpus-based unsu-pervised methods.
Given a query q, we would liketo rank all other queries according to their similarityto q.
The proposed approach compares favorably toa state-of-the-art unsupervised system.2 Related workDistributional similarity methods model the similar-ity or relatedness of words using a metric definedover the set of contexts in which the words appear(Firth, 1957).
One of the most common representa-tions for contexts is the vector space model (Saltonet al, 1975).
This is the basic idea of approachessuch as (Grefenstette, 1992; Bordag, 2008; Lin,1998; Riloff and Shepherd, 1997), with some varia-tions; e.g., whether syntactic information is used ex-plicitly, or which weight function is applied.
Most ofthe existing work has focused on similarity betweensingle words or syntactically-correct multiword ex-pressions.
In this work, we adapt these techniquesto calculate similarity metrics between pairs of com-plete queries, which may or may not be syntacticallycorrect.Other approaches for query similarity use sta-tistical translation models (Riezler et al, 2008),analysing search engine logs (Jones et al, 2006),looking for different anchor texts pointing to thesame pages (Kraft and Zien, 2004), or replacingquery words with other words that have the high-est pointwise mutual information (Terra and Clarke,2004).Sahami and Helman (Sahami and Heilman, 2006)define a web kernel function for semantic similaritybased on the snippets of the search results returnedby the queries.
The algorithm used is the following:(a) Issue a query x to a search engine and collectthe set of n snippets returned by the search engine;(b) Compute the tf?idf vector vi for each documentsnippet di; (c) Truncate each vector to include its m29highest weighted terms; (d) Construct the centroidof the L2-normalized vectors vi; (e) Calculate thesimilarity of two queries as the dot product of theirL2-normalized vectors, i.e.
as the cosine of bothvectors.This work was followed up by Yih and Meek (Yihand Meek, 2007), who combine the web kernel withother simple metrics of similarity between word vec-tors (Dice Coefficient, Jaccard Coefficient, Overlap,Cosine, KL Divergence) in a machine learning sys-tem to provide a ranking of similar queries.3 Proposed methodUsing a search engine to collect snippets (Sahamiand Heilman, 2006; Yih and Meek, 2007; Yih andMeek, 2008) takes advantage of all the optimizationsperformed by the retrieval engine (spelling correc-tion, relevance scores, etc.
), but it has several disad-vantages: first, it is not repeatable, as the code un-derlying search engines is in a constant state of flux;secondly, it is usually very expensive to issue a largenumber of search requests; sometimes the APIs pro-vided limit the number of requests.
In this section,we describe a method which overcomes these draw-backs.
The distributional methods we propose forcalculating similarities between words and multi-word expressions profit from the use of a large Web-based corpus.The contextual vectors for a query can be col-lected by identifying the contexts in which the queryappears.
Queries such as [buy a book] and [buysome books] are supposed to appear close to simi-lar context words in a bag-of-words model, and theyshould have a high similarity.
However, there aretwo reasons why this would yield poor results:First, as the length of the queries grows, the prob-ability of finding exact queries in the corpus shrinksquickly.
As an example, when issuing the queries[Lindsay Lohan pets] and [Britney Spears pets] toGoogle enclosed in double quotes, we obtain only6 and 760 results, respectively.
These are too fewoccurrences in order to collect meaningful statisticsabout the contexts of the queries.Secondly, many user queries are simply a concate-nation of keywords with weak or no underlying syn-tax.
Therefore, even if they are popular queries, theymay not appear as such in well-formed text foundin web documents.
For example, queries like [hol-lywood dvd cheap], enclosed in double quotes, re-trieve less than 10 results.
Longer queries, such as[hotel cheap new york fares], are still meaningful,but do not appear frequently in web documents.In order to use of distributional similarities in thequery setting, we propose the following method.Given a query of interest p = [w1, w2, ..., wn]:1.
For each word wi collect all words that appearclose to wi in the web corpus (i.e., a bag-fo-words models).
Empirically we have chosenall the words whose distance to wi is less orequal to 3.
This gives us a vector of contextwords and frequencies for each of the words inthe query, ~vi = (fi1, fi2, ..., fi|V |), where |V | isthe size of the corpus vocabulary.2.
Represent the query p with a vector of words,and the weight associated to each word is thegeometric mean of the frequencies for the wordin the original vectors:~qv =0B@0@|n|Yi=1fi11A1n,0@|n|Yi=1fi21A1n, ...,0@|n|Yi=1fi|V |1A1n1CA3.
Apply the ?2 test as a weighting function test tomeasure whether the query and the contextualfeature are conditionally independent.4.
Given two queries, use the cosine between theirvectors to calculate their similarity.The motivations for this approach are: the geo-metric mean is a way to approximate a boolean ANDoperation between the vectors, while at the sametime keeping track of the magnitude of the frequen-cies.
Therefore, if two queries only differ on a verygeneral word, e.g.
[books] and either [buy books]or [some books], the vector associated to the generalwords (buy or some in the example) will have non-zero values for most of the contextual features, be-cause they are not topically constrained; and the vec-tors for the queries will have similar sets of featureswith non-zero values.
Equally relevant, terms thatare closely related will appear in the proximity of asimilar set of words and will have similar vectors.For example, if the two queries are Sir Arthur Co-nan Doyle books and Sir Arthur Conan Doyle nov-els, given that the vectors for books and novels areexpected to have similar features, these two queries30Contextual word acid fast bacteria Queryacidogenicity 11 6 4 6.41506auramin 2 5 2 2.71441bacillae 3 10 4 4.93242carbolfuchsin 1 28 2 8.24257dehydrogena 5 3 3 3.55689diphtheroid 5 9 92 16.05709fuchsine 42 3 4 7.95811glycosilation 3 2 3 2.62074Table 1: Example of context words for the query [acid fast bacteria].will receive a high similarity score.On the other hand, this combination also helps inreducing word ambiguity.
Consider the query bankaccount; the bag-of-words vector for bank will con-tain words related to the various senses of the word,but when combining it to account only the terms thatbelong to the financial domain and are shared be-tween the two vectors will be included in the finalquery vector.Finally, we note that the geometric mean providesa clean way to encode the pair-wise similarities ofthe individual words of the phrase.
One can inter-pret the cosine similarity metric as the magnitude ofthe vector constructed by the scalar product of theindividual vectors.
Our approach scales this up bytaking the scalar product of the vectors for all wordsin the phrase and then scaling them by the number ofwords (i.e., the geometric mean).
Instead of comput-ing the magnitude of this vector, we use it to com-pute similarities for the entire phrase.As an example of the proposed procedure, Table 1shows a random sample of the contextual featurescollected for the words in the query [acid fast bac-teria], and how the query?s vector is generated byusing the geometric mean of the frequencies of thefeatures in the vectors for the query words.4 Experiments and results4.1 Experimental settingsTo collect the contextual features for words andphrases, we have used a corpus of hundreds of mil-lions of documents crawled from the Web in August2008.
An HTML parser is used to extract text andnon-English documents are discarded.
After pro-cess, the remaining corpus contains hundreds of bil-lions of words.As a source of keywords, we have used the top0 1 2 3 40 280 95 14 1 01 108 86 65 4 02 11 47 83 16 03 1 2 17 45 24 0 0 1 1 2Table 2: Confusion matrix for the pairs in the goldstandard.
Rowsrepresent first rater scores, and columns second rater scores.one and a half million English queries sent to theGoogle search engine after being fully anonymized.We have calculated the pairwise similarity betweenall queries, which would potentially return 2.25 tril-lion similarity scores, but in practice returns a muchsmaller number as many pairs have non-overlappingcontexts.As a baseline, we have used a new implementa-tion of the Web Kernel similarity (Sahami and Heil-man, 2006).
The parameters are set the same as re-ported in the paper with the exception of the snip-pet size; in their study, the size was limited to 1,000characters and in our system, the normal snippet re-turned by Google is used (around 160 characters).In order to evaluate our system, we prepared agoldstandard set of query similarities.
We have ran-domly sampled 65 queries from our full dataset, andobtained the top 20 suggestions from both the Sa-hami system and the distributional similarities sys-tem.
Two human raters have rated the original queryand the union of the sets of suggestions, using thesame 5-point Likert scale that Sahami used.
Table 2shows the confusion matrix of scores between thetwo raters.
Most of the disagreements are betweenthe scores 0 and 1, which means that probably it wasnot clear enough whether the queries were unrelatedor only slightly related.
It is also noteworthy thatin this case, very few rewritten queries were clas-sified as being better than the original, which alsosuggests to us that probably we could remove thetopmost score from the classifications scale.We have evaluated inter-judge agreement in thefollowing two ways: first, using the weighted Kappascore, which has a value of 0.7111.
Second, bygrouping the pairs judged as irrelevant or slightlyrelevant (scores 0 and 1) as a class containing nega-tive examples, and the pairs judged as very relevant,equal or better (scores 2 through 4) as a class con-taining positive examples.
Using this two-class clas-31Method Prec@1 Prec@3 Prec@5 mAP AUCWeb Kernel 0.39 0.35 0.32 0.49 0.22Unigrams 0.47 0.53 0.47 0.57 0.26N-grams 0.70 0.57 0.52 0.71 0.54Table 3: Results.
mAP is mean average precision, and AUC is thearea under the precision/recall curve.sification, Cohen?s Kappa score becomes 0.6171.Both scores indicates substantial agreement amongstthe raters.The data set thus collected is a ranked list of sug-gestions for each query1, and can be used to evaluateany other suggestion-ranking system.4.2 Experiments and resultsAs an evolution of the distributional similaritiesapproach, we also implemented a second versionwhere the queries are chunked into phrases.
Themotivation for the second version is that, in somequeries, like [new york cheap hotel], it makes senseto handle new york as a single phrase with a sin-gle associated context vector collected from the webcorpus.
The list of valid n-grams is collected bycombining several metrics, e.g.
whether Wikipediacontains an entry with that name, or whether theyappear quoted in query logs.
The queries are thenchunked greedily always preferring the longer n-gram from our list.Table 3 shows the results of trying both systemson the same set of queries.
The original system isthe one called Unigrams, and the one that chunksthe queries is the one called N-grams.
The distri-butional similarity approaches outperform the web-based kernel on all the metrics, and chunking queriesshows a good improvement over using unigrams.5 ConclusionsThis paper extends the vector-space model of dis-tributional similarities to query-to-query similaritiesby combining different vectors using the geometricmean.
We show that using n-grams to chunk thequeries improves the results significantly.
This out-performs the web-based kernel method, a state-of-the-art unsupervised query-to-query similarity tech-nique, which is particularly relevant as the corpus-based method does not benefit automatically from1We plan to make it available to the research community.search engine features.ReferencesS.
Bordag.
2008.
A Comparison of Co-occurrence andSimilarity Measures as Simulations of Context.
Lec-ture Notes in Computer Science, 4919:52.J.R.
Firth.
1957.
A synopsis of linguistic theory 1930-1955.
Studies in Linguistic Analysis, pages 1?32.G.
Grefenstette.
1992.
Use of syntactic context to pro-duce term association lists for text retrieval.
In Pro-ceedings of the 15th annual international ACM SI-GIR conference on Research and development in infor-mation retrieval, pages 89?97.
ACM New York, NY,USA.R.
Jones, B. Rey, O. Madani, andW.
Greiner.
2006.
Gen-erating query substitutions.
In Proceedings of the 15thinternational conference on World Wide Web, pages387?396.
ACM New York, NY, USA.Reiner Kraft and Jason Zien.
2004.
Mining anchor textfor query refinement.
In WWW ?04: Proceedings ofthe 13th international conference on World Wide Web,pages 666?674, New York, NY, USA.
ACM.D.
Lin.
1998.
Extracting Collocations from Text Cor-pora.
In First Workshop on Computational Terminol-ogy, pages 57?63.Stefan Riezler, Yi Liu, and Alexander Vasserman.2008.
Translating Queries into Snippets for ImprovedQuery Expansion.
In Proceedings of the 22nd In-ternational Conference on Computational Linguistics(COLING?08).E.
Riloff and J. Shepherd.
1997.
A corpus-based ap-proach for building semantic lexicons.
In Proceed-ings of the Second Conference on Empirical Methodsin Natural Language Processing, pages 117?124.
As-sociation for Computational Linguistics.M.
Sahami and T.D.
Heilman.
2006.
A web-based ker-nel function for measuring the similarity of short textsnippets.
In Proceedings of the 15th international con-ference on World Wide Web, pages 377?386.G.
Salton, A. Wong, and CS Yang.
1975.
A vector spacemodel for automatic indexing.
Communications of theACM, 18(11):613?620.Egidio Terra and Charles L.A. Clarke.
2004.
Scoringmissing terms in information retrieval tasks.
In CIKM?04: Proceedings of the thirteenth ACM internationalconference on Information and knowledge manage-ment, pages 50?58, New York, NY, USA.
ACM.W.
Yih and C. Meek.
2007.
Improving Similarity Mea-sures for Short Segments of Text.
In Proceedings ofthe Natural Conference on Artificial Intelligence, vol-ume 2, page 1489.
Menlo Park, CA; Cambridge, MA;London; AAAI Press; MIT Press; 1999.W.
Yih and C. Meek.
2008.
Consistent Phrase RelevanceMeasures.
Data Mining and Audience Intelligence forAdvertising (ADKDD 2008), page 37.32
