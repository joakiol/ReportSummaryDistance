Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 13?23,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsUnsupervised Cross-Lingual Lexical SubstitutionMarianna ApidianakiAlpage, INRIA & Univ Paris DiderotSorbonne Paris Cite?, UMRI-00175013 Paris, FranceMarianna.Apidianaki@inria.frAbstractCross-Lingual Lexical Substitution (CLLS) isthe task that aims at providing for a targetword in context, several alternative substitutewords in another language.
The proposedsets of translations may come from externalresources or be extracted from textual data.In this paper, we apply for the first time anunsupervised cross-lingual WSD method tothis task.
The method exploits the results ofa cross-lingual word sense induction methodthat identifies the senses of words by cluster-ing their translations according to their seman-tic similarity.
We evaluate the impact of usingclustering information for CLLS by applyingthe WSD method to the SemEval-2010 CLLSdata set.
Our system performs better on the?out-of-ten?
measure than the systems that par-ticipated in the SemEval task, and is rankedmedium on the other measures.
We analyzethe results of this evaluation and discuss av-enues for a better overall integration of unsu-pervised sense clustering in this setting.1 IntroductionLexical Substitution (LS) aims at providing alterna-tive substitute words (or phrases) for a target wordin context, a process useful for monolingual taskssuch as paraphrasing and textual entailment (Mc-Carthy and Navigli, 2009).
Its multilingual coun-terpart, Cross-Lingual Lexical Substitution (CLLS),aims at finding for a target word in context, alter-native substitute words in another language.
CLLSsystems may assist human translators and languagelearners, while their output may constitute the in-put to cross-language Information Retrieval and Ma-chine Translation (MT) systems (Sinha et al, 2009;Mihalcea et al, 2010).The multilingual context in which CLLS is per-formed permits to override some issues common tomonolingual semantic processing tasks, such as theselection of an adequate sense inventory and the def-inition of the granularity of the semantic descrip-tions.
In a multilingual context, word senses can beeasily identified using their translations in other lan-guages (Resnik and Yarowsky, 2000).
Although thisconception of senses presents some theoretical andpractical drawbacks, it provides a standard criterionfor sense delimitation which explains its wide adop-tion in recent works on multilingual Word SenseDisambiguation (WSD) and WSD in MT (Carpuatand Wu, 2007; Ng and Chan, 2007).In this paper, we explain how semantic clusteringmay provide answers to some of the issues posedby the traditional cross-lingual sense induction ap-proach, and how it can be efficiently exploited forCLLS.
Given that existing CLLS systems rely onpredefined semantic resources, we show, for the firsttime, that CLLS can be performed in a fully un-supervised manner.
The paper is organized as fol-lows: in the next section, we present some argu-ments towards unsupervised clustering for cross-lingual sense induction.
The clustering method usedis presented in section 3.
Section 4 describes theSemEval-2010 CLLS task, and section 5 presentsthe cross-lingual WSD method used for CLLS.
Insection 6, we proceed to a detailed analysis of theobtained results, before concluding with some av-enues for future work.132 Cross-lingual sense induction2.1 Related workWord sense induction (WSI) methods offer an alter-native to the use of predefined semantic resourcesfor NLP.
They automatically define the senses ofwords from textual data and may adapt the obtaineddescriptions to the WSD needs of specific applica-tions.
In a monolingual context, WSI is performedby exploiting more or less refined distributional in-formation (Navigli, 2009), while in a multilingualcontext WSI is mostly based on translation informa-tion.
In this setting, the senses of words in one lan-guage are identified by their translations in anotherlanguage, usually found in a parallel corpus (Resnikand Yarowsky, 2000).This empirical approach to sense induction of-fers a standard criterion for sense delimitation and,consequently, dissociates WSD from semantic theo-ries and predefined semantic inventories.
Moreover,by establishing semantic distinctions pertinent fortranslation between the implicated languages, it al-lows to tune sense induction to the needs of multilin-gual applications.
It has thus been widely adopted inworks on multilingual WSD and WSD in MT, wheresenses are derived from parallel data (Diab, 2003;Ide, 1999; Ide et al, 2002; Ng et al, 2003; Chan etal., 2007; Carpuat and Wu, 2007).
By linking WSDand its evaluation to translation, this hypothesis alsooffers a solution to the problem of non-conformityof monolingual WSD methods in this setting.Nevertheless, the assumption of biunivocal (?one-to-one?)
correspondences between senses and trans-lations is rather simplistic.
One word sense may betranslated by different synonymous words in anotherlanguage, whose relatedness should be consideredduring sense induction.
Furthermore, this approachdoes not permit to account for cases of parallel am-biguities (Resnik, 2007), and cases where the sensesof a word share some of their translations (Sinha etal., 2009).
Additional problems arise at the practicallevel as the induced senses are uniform and, so, theconstraints used during WSD for selecting betweenclose and distant senses are similar.
Furthermore,when WSD coincides with lexical selection in MT,the selection of a translation different from the refer-ence is considered as wrong even if it is semanticallycorrect.
So, this conception of senses does not per-mit to penalize WSD errors relatively to their impor-tance (Resnik and Yarowsky, 2000), unless semanticresources are used to identify semantic correspon-dences.2.2 Cross-lingual sense clusteringInstead of using translations as straightforwardsense indicators, it is possible to perform a morethorough semantic analysis during cross-lingualWSI by combining distributional and translation in-formation.
The sense clustering method proposed byApidianaki (2008) identifies complex semantic re-lations between word senses and their translations.The method is based on the contextual hypothe-ses of meaning and of semantic similarity (Harris,1954; Miller and Charles, 1991), which underliemonolingual WSI methods, and is combined to theassumption of a semantic correspondence betweenwords and their translations in real texts (Chester-man, 1998).
Following these hypotheses, informa-tion coming from the source contexts of a targetword when translated with a precise translation ina parallel corpus, is used to reveal the senses carriedby the translation.
Furthermore, the similarity of thesource contexts reveals the semantic relatedness ofthe translations.This cross-lingual WSI method groups the seman-tically similar translations of ambiguous words intoclusters that serve to describe their senses insteadof the individual translations.
For instance, the tra-ditional cross-lingual WSI approach would proposethree senses for the English noun coach, correspond-ing to each of its Spanish translations: entrenador,autocar and autobu?s.1 However, this solution is notsound given that the translations autocar and au-tobu?s are semantically related and do not lexical-ize distinct senses of the English word, as is thecase with entrenador.
Sense clustering permits toestimate the semantic similarity of the translationsand to not consider synonymous translations as in-dicators of distinct senses.
Consequently, the En-glish word coach has two senses after sense cluster-ing: one described by the cluster {autocar, autobu?s}(the ?bus?
sense) and one described by the cluster{entrenador} (the ?trainer?
sense).
In the automat-1This set of translations was extracted from the word alignedEuroparl corpus (Koehn, 2005) after applying a set of filters thatwill be described in section 3.14ically built bilingual inventories, the senses of thewords in one language are thus described by clus-ters of their translations in another language.2.3 ApplicationsThis type of sense clustering has proved to be use-ful in various application settings.
When exploitedin cross-lingual WSD, it permits to assign ?sense-tags?
containing several semantically correct trans-lations to new instances of words in context (Apid-ianaki, 2009).
Moreover, the use of clustering in-formation during evaluation allows for a differingpenalization of WSD errors.
In an MT evaluationsetting, sense clusters have been integrated into anMT evaluation metric (METEOR) (Lavie and Agar-wal, 2007) and brought about an increase of the met-ric?s correlation with human judgments of transla-tion quality in different languages (Apidianaki andHe, 2010).
The use of sense clusters in this set-ting permits to identify semantic correspondencesbetween translations and hypotheses, and to circum-vent the strict requirement for exact surface corre-spondences, one of the main critics addressed to MTevaluation metrics.
The same notion of sense clus-ters has been adopted in the most recent SemEvalCross-Lingual WSD task (Lefever and Hoste, 2010).Instead of considering translations as indicators ofdistinct senses, as was the case in previous tasks, thesenses of a small number of ambiguous words weredescribed by manually created clusters of transla-tions.We consider that the sense cluster inventories cre-ated by the unsupervised WSI method proposed byApidianaki (2008) would be useful in other applica-tive contexts as well and, especially, in CLLS.
Inunsupervised cross-lingual WSD, the clusters con-stitute the candidate senses from which one has tobe selected for each new instance of the words incontext.
So, when an instance of a word is dis-ambiguated, a cluster of semantically related trans-lations is selected on the basis of the source con-text describing its sense.
This is exactly the goalof CLLS, as described in the relevant task set upin SemEval-2010, where the systems had to providefor instances of words in context, several possibletranslations in another language (Sinha et al, 2009;Mihalcea et al, 2010).
It seems thus that CLLS con-stitutes a suitable field for exploiting this sense clus-tering method and, in what follows, we will try toevaluate this assumption.3 Unsupervised clustering for senseinduction3.1 Bilingual lexiconsThe SemEval-2010 CLLS task concerned the pairof languages English (EN) - Spanish (SP).
In or-der to apply our cross-lingual WSD method to thedata of the SemEval-2010 CLLS task, an EN-SPsense cluster inventory had first to be built wherethe senses of English words would be described byclusters of their Spanish translations.
The trainingcorpus used for building the sense cluster inventoryis the SP-EN part of Europarl (release v5), whichcontains 1,689,850 aligned sentence pairs (Koehn,2005).
Before clustering, some preprocessing stepsare performed.
First, the corpus is lemmatized andtagged by POS (Schmid, 1994).
Then sentence pairspresenting a great difference in length (i.e caseswhere one sentence is three times longer than theother) are eliminated and the corpus is aligned atthe level of word types using Giza++ (Och and Ney,2003).Two bilingual lexicons of content words are builtfrom the alignment results, one for each translationdirection (EN-SP/SP-EN).
In the entries of these lex-icons, source words are associated with the transla-tions to which they are aligned.
As these lexiconsare automatically created, they contain some noisemainly due to spurious word alignments.
In order toeliminate erroneous translation correspondences, wefirst apply a filter which discards translations witha probability below 0.001 (according to the scoresassigned during word alignment).
Then an intersec-tion filter is applied which discards correspondencesnot found in lexicons of both directions.
Finally, thetwo lexicons are filtered by POS, keeping for each wonly its translations that pertain to the same POS cat-egory.2 The translations of a word (w) used for clus-tering are the ones that translate w at least 20 times inthe training corpus.
This frequency threshold leavesout some translations of the source words but hasa double merit: it eliminates erroneous translations2For instance, for English nouns we retain their noun trans-lations in Spanish; for verbs, we keep verbs, etc.15and reduces data sparseness issues which pose prob-lems in distributional semantic analysis.3.2 Clustering based on semantic similarityThe semantic clustering is performed in the targetlanguage by using source language feature vectors.Each translation of a word w is characterized bya vector built from the content words that cooccurwith w whenever it is translated by this word inthe aligned sentences of the training corpus.3 Thevector similarity is calculated using a variation ofthe Weighted Jaccard measure (Grefenstette, 1994)which weighs each source context feature accordingto its relevance for the estimation of the translationssimilarity.The input of the similarity calculation consists ofthe frequency lists of w?s translations.
The score as-signed to a pair of translations indicates their degreeof similarity.
Each feature (j) gets a total weight (tw)relatively to a translation (i), which corresponds tothe product of its global (gw) and its local weight(lw) with this translation.
The gw is based on the dis-persion of j in the contexts of w, and on its frequencyof cooccurrence (cooc freq) with w when translatedby each i (cf.
formula 1).
So, it depends on the num-ber of translations with which j is related (nrels) andon its probability of cooccurrence with each one ofthem (cf.
formula 2).
The local weight (lw) betweenj and i depends on their frequency of cooccurrence(cf.
formula 3).gw(j) = 1 ?
?i pij log(pij)nrels(1)pij =cooc freq of j with i|js| for i (2)lw(j, i) = log(cooc freq of j with i) (3)The Weighted Jaccard (WJ) coefficient of two trans-lations m and n is given by formula 4.WJ(m,n) =?j min(tw(m, j)tw(n, j))?j max(tw(m, j)tw(n, j))(4)The pairwise similarity of the translations is thus es-timated by comparing the corresponding weighted3We use a stoplist of English function words (conjunctions,prepositions and articles) that may be erroneously tagged ascontent words.source feature vectors.
A similarity score is assignedto each pair of translations and stored in a table thatis being looked up by the clustering algorithm.
Thepertinence of the relation of each translation pair isestimated by comparing its score to a threshold de-fined locally for each w by the following iterativeprocedure.1.
The initial threshold (T) corresponds to the mean ofthe scores (above 0) of the translation pairs of w.2.
The set of translations is segmented into pairswhose score exceeds the threshold and pairs whosescore is inferior to the threshold, creating two sets(G1, G2).3.
The average of each set is computed (m1 = averagevalue of G1, m2 = average value of G2).4.
A new threshold is created that is the average of m1and m2 (T = (m1 + m2)/2).5.
Go back to step 2, now using the new thresholdcomputed in step 4, keep repeating until conver-gence has been reached.The clustering algorithm groups the translationsby exploiting the similarity calculation results.
Thecondition for a translation to be included in a clusteris to have pertinent relations with all the elementsalready in the cluster.
The clustering stops when allthe translations of w are included in some cluster andall their relations have been checked.
All the ele-ments of the final clusters are linked to each other bypertinent relations.
The translations not having anystrong relations to other translations are included inseparate one-element clusters.3.3 The EN-SP sense cluster inventoryIn the obtained semantic inventory, the senses ofeach English word are described by clusters of itssemantically similar translations in Spanish.4 Someentries from the EN-SP sense cluster inventory arepresented in Table 1.
We provide examples forwords of different POS (nouns, verbs, adjectives andadverbs) and with varying degrees of polysemy.
The4The inventory contains entries for all English content wordsin the corpus.
Here, we focus on the target words used in theCLLS task.16POS EN word # SP Ts # occ Sense clustersNouns coach 3 265 {entrenador}{autocar, autobu?s}test 11 3162{prueba, ensayo, examen} {experimento, ana?lisis, examen, ensayo}{evaluacio?n} {comprobacio?n} {experimentacio?n, ensayo, ana?lisis, ex-perimento} {inspeccio?n} {experimento, control, ana?lisis, examen}{experimentacio?n, control, ana?lisis, experimento} {criterio}Verbs drop 10 390{disminuir, reducir, bajar, caer, descender} {retirar} {dejar, abandonar}{lanzar}check 5 1343 {examinar} {revisar} {controlar, verificar, comprobar}Adjs heavy 7 448{elevado, fuerte, grave, grande}{elevado, enorme}{grave, duro, fuerte,grande} {grave, alto, elevado}open 6 6286 {pu?blico, libre, transparente} {pu?blico, franco, transparente} {abierto}{sincero, franco}Advs around 5 742 {alrededores}{casi, aproximadamente, cerca}{menos}now 9 33662{aqu?
?, actualmente, hoy, ahora bien} {actualmente, ahora, hoy}{entretanto, aqu?
?, ahora bien} {de momento}, {adelante}, {por ahora, en-tretanto}Table 1: Entries from the EN-SP sense cluster inventory.third column of the table gives the number of Span-ish words (SP Ts) translating more than 20 occur-rences of the English words in the corpus and re-tained for clustering.
This threshold ensures thatthe words being clustered are good translations ofthe English words.
The fourth column of the ta-ble shows the number of English word occurrencestranslated by the retained translations.As is shown in these examples, the translationsof the English words are not considered as straight-forward indicators of their senses but are groupedinto clusters describing senses.
For instance, theword drop, which is translated by ten different wordsinto Spanish (disminuir, reducir, bajar, caer, descen-der, retirar, dejar, abandonar, lanzar) is not con-sidered as having ten distinct senses but four, de-scribed by each cluster of translations: {disminuir,reducir, bajar, caer, descender}: ?decrease, reduce?,{retirar}: ?remove, withdraw?, {dejar, abandonar}:?leave, abandon?
and {lanzar}: ?launch?.
Theobtained clusters group semantically similar wordswhich would be erroneously considered as indica-tors of distinct senses by the traditional cross-lingualsense induction method.Another important point is that this algorithmperforms a soft clustering, highly adequate in thissetting.
Given that the generated clusters de-scribe senses, their overlaps describe the relationsbetween the corresponding senses.
For instance,the two senses of the word test described by theclusters {experimentacio?n, control, ana?lisis, exper-imento} and {experimento, control, ana?lisis, exa-men} share three elements and are closer than thosedescribed by {experimentacio?n, control, ana?lisis,experimento} and {evaluacio?n}, which have no ele-ment in common.
The first two senses could also beconsidered as nuances of a coarser sense (?examina-tion / analysis?)
that could be obtained by mergingthe overlapping clusters.
Capturing inter-sense re-lations is important in lexical semantics and numer-ous works have been criticized for just enumeratingword senses without describing their relations.
Dis-covering these links automatically, as is done withthis sense clustering method, permits to account fordifferences in the status of senses during WSD andits evaluation.
It also offers the possibility to au-tomatically modify the granularity of the obtainedsenses according to the WSD needs of the applica-tions.
Moreover, when the sense cluster inventoryis used for cross-lingual WSD, it allows to capturesubtle relations between word usages in cases wherethe senses of a word share some of their translationsbut not all of them, an issue highlighted in the Se-mEval CLLS task (Sinha et al, 2009) which will bepresented in the next section.174 The SemEval-2010 CLLS taskIn the SemEval-2010 Cross-Lingual Lexical Substi-tution task, annotators and systems had to provideseveral alternative correct translations in Spanish forEnglish target words in context.
Given a paragraphcontaining an instance of an English target word, theannotators had to find as many good substitute trans-lations as possible for that word in Spanish.
Unlikea full-blown MT task, CLLS targets one word at atime rather than an entire sentence.
So, annotatorswere asked to translate the target word and not en-tire sentences.
Moreover, they were asked to supply,for each instance, as many translations as they feltwere valid and not just one translation, which wouldbe the case in MT.The task of the participating systems was then topredict the translations provided by the annotatorsfor each target word instance.
By analyzing the con-text of the English target word instances, the sys-tems had to provide for each instance, several cor-rect Spanish translations which should fit the givensource language context.
The set of target wordsin the SemEval CLLS task is composed of Nouns,Verbs, Adjectives and Adverbs exhibiting a wide va-riety of substitutes.
The annotators were allowed touse any resources they wanted to in order to supplysubstitutes for instances of the English target words.So, instances of the target words in context weretagged by sets of Spanish translations.5 The inter-annotator agreement for this task was calculated aspairwise agreement between sets of substitutes fromannotators and corresponds to 0.2777.The sets of translations provided for different in-stances of a target word could overlap in differentdegrees, depending on the meaning of the instances.These overlaps reveal subtle relations between wordusages in cases where they share some of their trans-lations but not all of them (Sinha et al, 2009).
Thisalso shows the absence of clear divisions betweenusages and senses: usages overlap to different ex-tents without having identical translations.
Althoughno clustering of translations from a specific resourceinto senses was performed for this task, the interestof examining the possibility of clustering the transla-5The average numbers of substitutes provided by the anno-tators for words of different POS are: 4.47 for nouns, 5.2 forverbs, 4.99 for adjectives and 4.77 for adverbs.tions provided by the annotators is highlighted (Mi-halcea et al, 2010).5 Cross-lingual WSDThe source language features that revealed the sim-ilarity of the translations and served to their cluster-ing (cf.
section 3) can be exploited by an unsuper-vised WSD classifier (Apidianaki, 2009).
In orderto disambiguate a new instance of an English wordw, cooccurrence information coming from its con-text is compared to these feature sets and the clus-ter that has the highest similarity with the new con-text is selected.
We adopt this WSD method in or-der to exploit the sense clustering results and per-form CLLS in an unsupervised manner.
Instead ofcomparing the new contexts to the features that arecommon to all the translations in a cluster (i.e.
theintersection of their source language features), as isdone in the initial method, we compare them to thefeatures shared by each pair of translations.
This in-creases the coverage of the method, given that thesesource features sets are larger than the ones contain-ing the intersection of the features of all the clus-tered translations.
As the training corpus was lem-matized and POS-tagged prior to building the fea-ture vectors (only content word cooccurrences wereretained), the new contexts have to be lemmatizedand POS-tagged as well.If common features (CFs) are found between thenew context and a translation pair, a score is as-signed to this ?context-pair?
association which cor-responds to the mean of the weights of the CFs rel-atively to each translation of the pair.
The weightsused here are the total weights (tws) that were as-signed to the context features relatively to the trans-lations during the semantic similarity calculation (cf.section 3.2).
In formula 5, i is equal to 2 (i.e.
thenumber of translations in the pair) and j is the num-ber of CFs between the translation pair and the newcontext.If the highest-ranked translation pair is found injust one sense cluster, this cluster is selected as de-scribing the sense of the new instance.
Otherwise,if the translation pair is found in different clusters,it is checked whether the CFs characterize the othertranslations in these clusters (or some of them).
Ifthis is the case, a score is assigned to each cluster18Test instance WSD suggestion Gold annotationtest.n 1698 prueba;ensayo;examen; examen 4;prueba 4;test 1;board.n 1781 consejo;bordo;junta;comite?
;cuenta;administracio?n;junta directiva 2;consejo 2;mesa directiva 1;junta1;junta de ayuda 1;directiva 1;comite 1;comision 1;drop.v 1288 bajar;disminuir;reducir;caer;descender dejar caer 2;tirar 1;arrojar 1;lanzar 1;soltar 1;dejar1;bajar 1;check.v 851 comprobar;controlar;verificar; verificar 3;checar 2;confirmar 1;anotar 1;rectificar1;revisar 1;comprobar 1;yet.r 1766 todav?
?a;au?n;sin embargo; sin embargo 2;pero 2;no obstante 1;aun 1;todavia 1;now.r 1019 hoy;aqu??
;actualmente;ahora bien; hoy 2;ahora 2;este momento 2;a partir 1;el presente1;de aqui 1;Table 2: Clusters suggested by the WSD method.depending on the weights of the features with theother translations, and the cluster with the highestscore is selected as describing the sense of the newinstance.
The score is again calculated by formula 5but this time i is equal to the number of translationsin the cluster having CFs with the new context.score =?i?j tw(i, j)i ?
j (5)If no CFs are found using the translation pairs, theWSD algorithm considers each translation?s featureset separately (which is naturally larger than the fea-ture sets of the translation pairs).
If CFs exist, thetranslation with the highest score is selected as wellas the cluster containing it.
If the translation isfound in the intersection of different clusters, it ischecked whether the CFs characterize some of theother translations found in the clusters.
If this is thecase, a score is assigned to the clusters depending onthe weights of the features with the translations andthe cluster with the highest score is selected.
Thecluster containing the translation pair with the high-est similarity to the new context is retained as thesense of the new instance.
If no CFs are found inthis way neither, a most frequent sense heuristic isused which selects the most frequent cluster (i.e.
theone assigned to most of the new instances of w).For the 1000 test instances in the SemEval CLLStask, the WSD method proposes 625 clusters withmore than one element and 118 one element clus-ters.6 The most frequent translation is suggested in6262 clusters with two elements; 157 clusters with three; 73with four; 64 with five; 69 clusters with more than five and less210 cases while the most frequent cluster is chosenin 43 cases.
A cluster is chosen randomly only in3 cases.
In Table 2, we present some suggestionsmade by the WSD method for target words of dif-ferent POS (n: nouns, v: verbs, a: adjectives, r: ad-verbs) and the corresponding gold standard (GS) an-notations.
For instance, the following occurrence ofthe English noun test:Entries typically identify the age or school grade lev-els for which the test is appropriate, as well as anysubtests.is tagged by the Spanish cluster {prueba, examen,ensayo} during WSD, which is close to the GS an-notation {examen, prueba, test} and correctly de-scribes its sense.The first translation provided in the results is theword of the cluster that translates most of the En-glish target word instances in the corpus (and whichis duplicated in order to be reinforced during the?out-of-ten?
evaluation, as we will explain in the nextsection).
We observe that this most frequent word,although it is a correct translation (i.e.
found in theGS annotations), does not coincide with the annota-tors?
first choice.
This explains the evaluation resultsthat we present in the next section.It is also important to note that the system sug-gests not only translations that have been proposedby the annotators, but also other semantically perti-nent translations that were found in the training cor-pus and which do not exist in the GS annotations.This is the case, for instance, with the translationthan ten elements; 23 clusters with ten elements and 22 clusterswith more than ten elements.19?controlar?
of the verb check and the translation ?en-sayo?
proposed for the noun test.
This shows thatthe suggestions made by the WSD method greatlydepend on the corpus used for training.6 Evaluation6.1 The settingWe evaluate our method on the SemEval-2010CLLS task test set.
The metrics used for evalua-tion are the best and out-of-ten (oot) precision (P)and recall (R) scores.
In the SemEval task, the sys-tems were allowed to supply as many translations asthey felt fit the context.
These suggestions were thengiven credit depending on the number of annotatorsthat had picked each translation.
The credit was di-vided by the number of annotator responses for theitem.
For the best score, the credit for the system an-swers for an item was also divided by the number ofanswers provided by the system, which allows morecredit to be given to instances with less variation.The oot scorer allows up to ten system responsesand does not divide the credit attributed to eachanswer by the number of system responses.
Thisscorer allows duplicates which means that systemscan get inflated scores (i.e.
> 100), as the creditfor each item is not divided by the number of substi-tutes and the frequency of each annotator responseis used.
Allowing duplicates permits that the sys-tems boost their scores with duplicates on transla-tions with higher probability.7Two baselines are used for evaluation: adictionary-based one (DICT), which contains theSpanish translations of all target words provided byan SP-EN dictionary, and a dictionary and corpus-based one (DICTCORP), where the translations pro-vided by the dictionary for a given target word areranked according to their frequencies in the SpanishWikipedia.
In DICT, the best baseline is producedby taking the first translation provided by the dic-tionary while the oot baseline considers the first tentranslations.6.2 ResultsIn order to evaluate our WSD method, we proceed asfollows.
If the cluster selected by the WSD method7The metrics used for evaluation are defined in Mihalcea etal.
(2010).contains ten translations (or more), all the transla-tions are given in the oot results.
Otherwise, thetranslations found in the cluster are proposed and themost frequent translation is duplicated till reachingten elements.
For best, we always retain the mostfrequent translation of the selected cluster.Our intuition was that the WSD method, whichassigns sense clusters (i.e.
sets of semantically sim-ilar and, more or less, substitutable translations),would fit and perform well on the oot subtask of theSemEval CLLS task.
This is confirmed by the re-sults presented in Table 3.8 Our method (denotedby ?WSD?
in the table) outperforms the 14 systemsthat participated in the CLLS task as well as the re-call (R) and precision (P) baselines.
It is importantto note that, contrary to our method which is totallyunsupervised, all the systems that participated in theSemEval-2010 task used predefined resources.
Thesecond ranked system (SWAT-E), for instance, per-forms lexical substitution in English and then trans-lates each substitute into Spanish using two prede-fined bilingual dictionaries, while SWAT-S does theinverse, performing lexical substitution in the trans-lated text (Wicentowski et al, 2010).Systems R P Mode R Mode PWSD 180.10 186.25 56.52 58.44SWAT-E 174.59 174.59 66.94 66.94SWAT-S 97.98 97.98 79.01 79.01UvT-v 58.91 58.91 62.96 62.96UvT-g 55.29 55.29 73.94 73.94DICT 44.04 44.04 73.53 73.53DICTCORP 42.65 42.65 71.60 71.60Table 3: oot results (%)Another interesting point is that the sense clusterinventory used by the cross-lingual WSD method isderived from Europarl, which is the European Par-liament Proceedings parallel corpus (Koehn, 2005).Despite this fact, the WSD method that exploitsthis inventory performs particularly well on this taskwhich concerns the semantic analysis and transla-tion of words of general language.
We would thusexpect the results to be even better if the sense induc-8We report the results obtained by the highest-ranked sys-tems in the SemEval-2010 CLLS task.
The full table of resultscan be found in Mihalcea et al (2010).20tion and the WSD method were trained on a bigger,or more general, parallel corpus.The mode recall and precision (Mode R and ModeP) metrics evaluate the performance of the systemsin predicting the translation that was most frequentlyselected by the annotators, provided that such atranslation extists.
To identify the most frequent re-sponse, we order the system responses according totheir frequency as translations of the target words inthe training corpus.
The relatively low scores ob-tained for the Mode R and Mode P metrics (com-pared to R and P) are explained by the fact that themost frequent translation in the training corpus doesnot always correspond to the translation that wasmost frequently selected by the annotators, althoughit may be a good translation for the target word.The same reason explains the weaker perfor-mance of the method in the best evaluation subtask(cf.
Table 4), where our system is ranked eighthcompared to the 14 systems that participated in thetask.9 Here too, the best translation according to theannotators does not correspond to the most frequenttranslation in the corpus.
This highlights the impactthat the relevance of the training corpus to the do-mains of the processed texts has on unsupervisedCLLS.Systems R P Mode R Mode PUBA-T 27.15 27.15 57.20 57.20USPWLV 26.81 26.81 58.85 58.85WLVUSP 25.27 25.27 52.81 52.81WSD 19.73 19.93 41.29 41.75UBA-W 19.68 19.68 39.09 39.09SWAT-S 18.87 18.87 36.63 36.63IRST-1 15.38 22.16 33.47 45.95TYO 8.39 8.62 14.95 15.31DICT 24.34 24.34 50.34 50.34DICTCORP 15.09 15.09 29.22 29.22Table 4: best results (%)Another important factor that has to be taken intoaccount is that the WSD method that we use is ori-ented towards multilingual applications (more pre-cisely MT).
In these applications, it is possible tofilter the proposed sense clusters by reference to the9We report some indicative results from the best subtask.The full table of results can be found in Mihalcea et al (2010).target language context (for instance, by using a lan-guage model) in order to retain the most adequatetranslation.
It is interesting to note that the systemsthat perform better in the best subtask get relativelylow results in the oot subtask, and the inverse.
Thisis the case, for instance, for UBA-T (Basile and Se-meraro, 2010), while Aziz and Specia (2010) clearlyspecify that their main goal is to maximize the accu-racy of their system (USPwlv) in choosing the besttranslation.
A conclusion that can be drawn is thateach subtask has different requirements, which maybe satisfied by different types of methods.In order to investigate other possible reasons be-hind the different behavior of the WSD method inthe two evaluation subtasks, we performed the eval-uation separately for each POS.
The results are pre-sented in Tables 5 and 6.POS R P Mode R Mode PAdjs 287.94 296.41 72.44 74.43Nouns 127.01 141.65 37.78 42.29Verbs 115.94 121.43 53.17 55.90Advs 111.46 111.46 65.15 65.15Table 5: oot results for different POS (%)POS R P Mode R Mode PAdjs 30.77 31.00 63.56 64.13Nouns 14.61 16.29 25.78 28.86Verbs 14.98 14.98 29.76 29.76Advs 13.07 13.07 37.88 37.88Table 6: best results for different POS (%)In both the oot and best evaluation subtasks, thebest scores are obtained for adjectives.
Especially inthe best subtask, where the method seemed to per-form worse than the other systems, the recall andprecision scores obtained for adjectives (with andwithout mode) are higher than those obtained by thehighest-ranked system (cf.
Table 4) and much higherthan the baselines.
A more detailed look at the ob-tained results proved that the most frequent transla-tion of the English adjectives in our training corpus ?proposed in the best evaluation subtask and empha-sized in the oot subtask ?
is often the most frequenttranslation proposed by the annotators.
This is notthe case for the other POS, where the most frequent21translation in the corpus often does not correspondto the annotators?
first choice.
Furthermore, thetranslation proposed by the system is not the sameas the most frequent translation of the word in thegeneral dictionary and the Spanish Wikipedia whichwere used, respectively, for the DICT and DICT-CORP baselines.
Consequently, this issue couldprobably be resolved if a more balanced corpus wasused for training the WSI and WSD methods.7 Conclusions and future workWe have shown that Cross-Lingual Lexical Substi-tution can be performed in a totally unsupervisedmanner, if a parallel corpus is available.
We appliedan unsupervised cross-lingual WSD method basedon semantic clustering to the SemEval-2010 CLLStask.
The method performs well compared to thesystems that participated in the task, which exploitpredefined lexico-semantic resources.
It is rankedfirst on the out-of-ten measure and medium on mea-sures that concern the choice of the best translation.We wish to pursue this work and explore other waysfor selecting best translations than solely relying onfrequency information.
As unsupervised methodsheavily rely on the training data, it would also beinteresting to experiment with different corpora inorder to evaluate the impact of the type and the sizeof the corpus on CLLS.The sense clusters assigned to target word in-stances during CLLS contain semantically similartranslations of these words, more or less substi-tutable in the target language context.
We considerthat it would be interesting to integrate target lan-guage information in the CLLS decision process forselecting best translations.
Given that MT is one ofthe envisaged applications for this type of task, butthe use of a full-blown MT system would probablymask system capabilities at a lexical level, a possi-bility would be to exploit the CLLS system sugges-tions in a simplified MT task such as word transla-tion (Vickrey et al, 2005) or lexical selection (Apid-ianaki, 2009), or in an MT evaluation context.
Thiswould permit to estimate the usefulness of the sys-tem suggestions in a specific application setting.ReferencesMarianna Apidianaki and Yifan He.
2010.
An algorithmfor cross-lingual sense clustering tested in a MT eval-uation setting.
In Proceedings of the 7th InternationalWorkshop on Spoken Language Translation (IWSLT-10), pages 219?226, Paris, France.Marianna Apidianaki.
2008.
Translation-oriented senseinduction based on parallel corpora.
In Proceedingsof the 6th International Conference on Language Re-sources and Evaluation (LREC-08), pages 3269?3275,Marrakech, Morocco.Marianna Apidianaki.
2009.
Data-driven SemanticAnalysis for Multilingual WSD and Lexical Selectionin Translation.
In Proceedings of the 12th Confer-ence of the European Chapter of the Association forComputational Linguistics (EACL-09), pages 77?85,Athens, Greece.Wilker Aziz and Lucia Specia.
2010.
USPwlv andWLVusp: Combining Dictionaries and Contextual In-formation for Cross-Lingual Lexical Substitution.
InProceedings of the 5th International Workshop on Se-mantic Evaluations (SemEval-2), ACL 2010, pages117?122, Uppsala, Sweden.Pierpaolo Basile and Giovanni Semeraro.
2010.
UBA:Using Automatic Translation and Wikipedia for Cross-Lingual Lexical Substitution.
In Proceedings of the5th International Workshop on Semantic Evaluations(SemEval-2), ACL 2010, pages 242?247, Uppsala,Sweden.Marine Carpuat and Dekai Wu.
2007.
Improving Sta-tistical Machine Translation using Word Sense Disam-biguation.
In Proceedings of the Joint EMNLP-CoNLLConference, pages 61?72, Prague, Czech Republic.Yee Seng Chan, Hwee Tou Ng, and David Chiang.
2007.Word Sense Disambiguation Improves Statistical Ma-chine Translation.
In Proceedings of the 45th AnnualMeeting of the Association for Computational Linguis-tics (ACL-07), pages 33?40, Prague, Czech Republic.Andrew Chesterman.
1998.
Contrastive FunctionalAnalysis.
John Benjamins Publishing Company, Ams-terdam/Philadelphia.Mona Diab.
2003.
Word sense disambiguation withina multilingual framework.
Ph.D. dissertation, Univer-sity of Maryland.Gregory Grefenstette.
1994.
Explorations in AutomaticThesaurus Discovery.
Kluwer Academic Publishers,Norwell, MA.Zelig Harris.
1954.
Distributional structure.
Word,10:146?162.Nancy Ide, Tomaz Erjavec, and Dan Tufis.
2002.
Sensediscrimination with parallel corpora.
In Proceedingsof the ACL Workshop on Word Sense Disambiguation:22Recent Successes and Future Directions, pages 54?60,Philadelphia.Nancy Ide.
1999.
Cross-lingual sense determination:Can it work?
Computers and the Humanities, 34(1-2):223?234.Philip Koehn.
2005.
Europarl: A Parallel Corpus forStatistical Machine Translation.
In Proceedings of MTSummit X, pages 79?86, Phuket, Thailand.Alon Lavie and Abhaya Agarwal.
2007.
METEOR: AnAutomatic Metric for MT Evaluation with High Levelsof Correlation with Human Judgments.
In Proceed-ings of the ACL-2007 Workshop on Statistical MachineTranslation, pages 228?231, Prague, Czech Republic.Els Lefever and Veronique Hoste.
2010.
SemEval-2010Task 3: Cross-lingual Word Sense Disambiguation.In Proceedings of the 5th International Workshop onSemantic Evaluations (SemEval-2), ACL 2010, pages15?20, Uppsala, Sweden.Diana McCarthy and Roberto Navigli.
2009.
The En-glish lexical substitution task.
Language Resourcesand Evaluation Special Issue on Computational Se-mantic Analysis of Language: SemEval-2007 and Be-yond, 43(2):139?159.Rada Mihalcea, Ravi Sinha, and Diana McCarthy.
2010.SemEval-2010 Task 2: Cross-Lingual Lexical Sub-stitution.
In Proceedings of the 5th InternationalWorkshop on Semantic Evaluations (SemEval-2), ACL2010, pages 9?14, Uppsala, Sweden.George A. Miller and Walter G. Charles.
1991.
Contex-tual correlates of semantic similarity.
Language andCognitive Processes, 6(1):1?28.Roberto Navigli.
2009.
Word Sense Disambiguation: aSurvey.
ACM Computing Surveys, 41(2):1?69.Hwee Tou Ng and Yee Seng Chan.
2007.
SemEval-2007 Task 11: English lexical sample task viaEnglish-Chinese parallel text.
In Proceedings of the4th International Workshop on Semantic Evaluations(SemEval-2007), pages 54?58, Prague, Czech Repub-lic.Hwee Tou Ng, Bin Wang, and Yee Seng Chan.
2003.Exploiting Parallel Texts for Word Sense Disambigua-tion: An Empirical Study.
In Proceedings of the 41stAnnual Meeting of the Association for ComputationalLinguistics, pages 455?462, Sapporo, Japan.Franz Josef Och and Hermann Ney.
2003.
A SystematicComparison of Various Statistical Alignment Models.Computational Linguistics, 29(1):19?51.Philip Resnik and David Yarowsky.
2000.
Distinguish-ing Systems and Distinguishing Senses: New Evalua-tion Methods for Word Sense Disambiguation.
Natu-ral Language Engineering, 5(3):113?133.Philip Resnik.
2007.
WSD in NLP applications.
InEneko Agirre and Philip Edmonds, editors, WordSense Disambiguation: Algorithms and Applications,pages 299?337, Dordrecht.
Springer.Helmut Schmid.
1994.
Probabilistic Part-of-Speech Tag-ging Using Decision Trees.
In Proceedings of the In-ternational Conference on New Methods in LanguageProcessing, pages 44?49, Manchester, UK.Ravi Sinha, Diana McCarthy, and Rada Mihalcea.
2009.SemEval-2010 Task 2: Cross-Lingual Lexical Substi-tution.
In Proceedings of the NAACL-HLT WorkshopSEW-2009 - Semantic Evaluations: Recent Achieve-ments and Future Directions, pages 76?81, Boulder,Colorado.David Vickrey, Luke Biewald, Marc Teyssier, andDaphne Koller.
2005.
Word-Sense Disambiguationfor Machine Translation.
In Proceedings of the JointConference on Human Language Technology / Empir-ical Methods in Natural Language Processing (HLT-EMNLP), pages 771?778, Vancouver, Canada.Richard Wicentowski, Maria Kelly, and Rachel Lee.2010.
SWAT: Cross-Lingual Lexical Substitutionusing Local Context Matching, Bilingual Dictionar-ies and Machine Translation.
In Proceedings of the5th International Workshop on Semantic Evaluations(SemEval-2), ACL, pages 123?128, Uppsala, Sweden.23
