Proceedings of the BioNLP Workshop on Linking Natural Language Processing and Biology at HLT-NAACL 06, pages 57?64,New York City, June 2006. c?2006 Association for Computational LinguisticsBIOSMILE: Adapting Semantic Role Labeling for Biomedical Verbs:An Exponential Model Coupled withAutomatically Generated Template FeaturesRichard Tzong-Han Tsai1,2, Wen-Chi Chou1, Yu-Chun Lin1,2, Cheng-Lung Sung1,Wei Ku1,3, Ying-Shan Su1,4, Ting-Yi Sung1 and Wen-Lian Hsu11Institute of Information Science, Academia Sinica2Dept.
of Computer Science and Information Engineering, National Taiwan University3Institute of Molecular Medicine, National Taiwan University4Dept.
of Biochemical Science and Technology, National Taiwan University{thtsai,jacky957,sbb,clsung,wilmaku,qnn,tsung,hsu}@iis.sinica.edu.twAbstractIn this paper, we construct a biomedicalsemantic role labeling (SRL) system thatcan be used to facilitate relation extraction.First, we construct a proposition bank ontop of the popular biomedical GENIAtreebank following the PropBank annota-tion scheme.
We only annotate the predi-cate-argument structures (PAS?s) of thirtyfrequently used biomedical predicates andtheir corresponding arguments.
Second,we use our proposition bank to train abiomedical SRL system, which uses amaximum entropy (ME) model.
Thirdly,we automatically generate argument-typetemplates which can be used to improveclassification of biomedical argumenttypes.
Our experimental results show thata newswire SRL system that achieves anF-score of 86.29% in the newswire do-main can maintain an F-score of 64.64%when ported to the biomedical domain.By using our annotated biomedical corpus,we can increase that F-score by 22.9%.Adding automatically generated templatefeatures further increases overall F-scoreby 0.47% and adjunct arguments (AM) F-score by 1.57%, respectively.1 IntroductionThe volume of biomedical literature available hasexperienced unprecedented growth in recent years.The ability to automatically process this literaturewould be an invaluable tool for both the design andinterpretation of large-scale experiments.
To thisend, more and more information extraction (IE)systems using natural language processing (NLP)have been developed for use in the biomedicalfield.
A key IE task in the biomedical field is ex-traction of relations, such as protein-protein andgene-gene interactions.Currently, most biomedical relation-extractionsystems fall under one of the following three ap-proaches: cooccurence-based (Leroy et al, 2005),pattern-based (Huang et al, 2004), and machine-learning-based.
All three, however, share the samelimitation when extracting relations from complexnatural language.
They only extract the relationtargets (e.g., proteins, genes) and the verbs repre-senting those relations, overlooking the many ad-verbial and prepositional phrases and words thatdescribe location, manner, timing, condition, andextent.
The information in such phrases may beimportant for precise definition and clarification ofcomplex biological relations.The above problem can be tackled by using se-mantic role labeling (SRL) because it not only rec-ognizes main roles, such as agents and objects, butalso extracts adjunct roles such as location, manner,57timing, condition, and extent.
The goal of SRL isto group sequences of words together and classifythem with semantic labels.
In the newswire domain,Morarescu et al (2005) have demonstrated thatfull-parsing and SRL can improve the performanceof relation extraction, resulting in an F-score in-crease of 15% (from 67% to 82%).
This significantresult leads us to surmise that SRL may also havepotential for relation extraction in the biomedicaldomain.
Unfortunately, no SRL system for thebiomedical domain exists.In this paper, we aim to build such a biomedicalSRL system.
To achieve this goal we roughly im-plement the following three steps as proposed byWattarujeekrit et al, (2004): (1) create semanticroles for each biomedical verb; (2) construct abiomedical corpus annotated with verbs and theircorresponding semantic roles (following defini-tions created in (1) as a reference resource;) (3)build an automatic semantic interpretation modelusing the annotated text as a training corpus formachine learning.
In the first step, we adopt thedefinitions found in PropBank (Palmer et al, 2005),defining our own framesets for verbs not in Prop-Bank, such as ?phosphorylate?.
In the second step,we first use an SRL system (Tsai et al, 2005)trained on the Wall Street Journal (WSJ) to auto-matically tag our corpus.
We then have the resultsdouble-checked by human annotators.
Finally, weadd automatically-generated template features toour SRL system to identify adjunct (modifier) ar-guments, especially those highly relevant to thebiomedical domain.2 Biomedical Proposition BankAs proposition banks are semantically annotatedversions of a Penn-style treebank, they provideconsistent semantic role labels across different syn-tactic realizations of the same verb (Palmer et al,2005).
The annotation captures predicate-argumentstructures based on the sense tags of polysemousverbs (called framesets) and semantic role labelsfor each argument of the verb.
Figure 1 shows theannotation of semantic roles, exemplified by thefollowing sentence: ?IL4 and IL13 receptors acti-vate STAT6, STAT3 and STAT5 proteins in thehuman B cells.?
The chosen predicate is the word?activate?
; its arguments and their associated wordgroups are illustrated in the figure.Figure 1.
A Treebank Annotated with SemanticRole LabelsSince proposition banks are annotated on top ofa Penn-style treebank, we selected a biomedicalcorpus that has a Penn-style treebank as our corpus.We chose the GENIA corpus (Kim et al, 2003), acollection of MEDLINE abstracts selected fromthe search results with the following keywords:human, blood cells, and transcription factors.
In theGENIA corpus, the abstracts are encoded in XMLformat, where each abstract also contains aMEDLINE UID, and the title and content of theabstract.
The text of the title and content is seg-mented into sentences, in which biological termsare annotated with their semantic classes.
TheGENIA corpus is also annotated with part-of-speech (POS) tags (Tateisi et al, 2004), and co-references (Yang et al, 2004).The Penn-style treebank for GENIA, created byTateisi et al (2005), currently contains 500 ab-stracts.
The annotation scheme of the GENIATreebank (GTB), which basically follows the PennTreebank II (PTB) scheme (Bies et al, 1995), isencoded in XML.
However, in contrast to the WSJcorpus, GENIA lacks a proposition bank.
Wetherefore use its 500 abstracts with GTB as ourcorpus.
To develop our biomedical propositionbank, BioProp, we add the proposition bank anno-tation on top of the GTB annotation.2.1 Important Argument TypesIn the biomedical domain, relations are often de-pendent upon locative and temporal factors(Kholodenko, 2006).
Therefore, locative (AM-LOC) and temporal modifiers (AM-TMP) are par-ticularly important as they tell us where and whenbiomedical events take place.
Additionally, nega-58tive modifiers (AM-NEG) are also vital to cor-rectly extracting relations.
Without AM-NEG, wemay interpret a negative relation as a positive oneor vice versa.
In total, we use thirteen modifiers inour biomedical proposition bank.2.2 Verb SelectionWe select 30 frequently used verbs from the mo-lecular biology domain given in Table 1.express trigger encodeassociate repress enhanceinteract signal increasesuppress activate induceprevent alter Inhibitmodulate affect Mediatephosphorylate bind Mutatedtransactivate block Reducetransform decrease Regulatedifferentiated promote StimulateTable 1.
30 Frequently Biomedical VerbsLet us examine a representative verb, ?activate?.Its most frequent usage in molecular biology is thesame as that in newswire.
Generally speaking, ?ac-tivate?
means, ?to start a process?
or ?to turn on.
?Many instances of this verb express the action ofwaking genes, proteins, or cells up.
The followingsentence shows a typical usage of the verb ?acti-vate.?
[NF-kappaBArg1] is [notAM-NEG] [activatedpredicate] [upon tetra-cycline removalAM-TMP] [in the NIH3T3 cell lineAM-LOC].3 Semantic Role Labeling on BioPropIn this section, we introduce our BIOmedical Se-MantIc roLe labEler, BIOSMILE.
Like POS tag-ging, chunking, and named entity recognition, SRLcan be formulated as a sentence tagging problem.A sentence can be represented by a sequence ofwords, a sequence of phrases, or a parsing tree; thebasic units of a sentence are words, phrases, andconstituents arranged in the above representations,respectively.
Hacioglu et al (2004) showed thattagging phrase by phrase (P-by-P) is better thanword by word (W-by-W).
Punyakanok et al, (2004)further showed that constituent-by-constituent (C-by-C) tagging is better than P-by-P.
Therefore, wechoose C-by-C tagging for SRL.
The gold standardSRL corpus, PropBank, was designed as an addi-tional layer of annotation on top of the syntacticstructures of the Penn Treebank.SRL can be broken into two steps.
First, wemust identify all the predicates.
This can be easilyaccomplished by finding all instances of verbs ofinterest and checking their POS?s.Second, for each predicate, we need to label allarguments corresponding to the predicate.
It is acomplicated problem since the number of argu-ments and their positions vary depending on averb?s voice (active/passive) and sense, along withmany other factors.In this section, we first describe the maximumentropy model used for argument classification.Then, we illustrate basic features as well as spe-cialized features such as biomedical named entitiesand argument templates.3.1 Maximum Entropy ModelThe maximum entropy model (ME) is a flexiblestatistical model that assigns an outcome for eachinstance based on the instance?s history, which isall the conditioning data that enables one to assignprobabilities to the space of all outcomes.
In SRL,a history can be viewed as all the information re-lated to the current token that is derivable from thetraining corpus.
ME computes the probability,p(o|h), for any o from the space of all possible out-comes, O, and for every h from the space of allpossible histories, H.The computation of p(o|h) in ME depends on aset of binary features, which are helpful in makingpredictions about the outcome.
For instance, thenode in question ends in ?cell?, it is likely to beAM-LOC.
Formally, we can represent this featureas follows:????
?===otherwise :0LOC-AMo andtrue)(s_in_cellde_endcurrent_no if :1),(hohfHere, current_node_ends_in_cell(h) is a binaryfunction that returns a true value if the currentnode in the history, h, ends in ?cell?.
Given a set offeatures and a training corpus, the ME estimationprocess produces a model in which every feature f ihas a weight ?i.
Following Bies et al (1995), wecan compute the conditional probability as:?=iohfiihZhop ),()(1)|( ??
?=o iohfiihZ ),()( ?59The probability is calculated by multiplying theweights of the active features (i.e., those of f i (h,o)= 1).
?i is estimated by a procedure called Gener-alized Iterative Scaling (GIS) (Darroch et al,1972).
The ME estimation technique guaranteesthat, for every feature, f i, the expected value of ?iequals the empirical expectation of ?i in the train-ing corpus.
We use Zhang?s MaxEnt toolkit andthe L-BFGS (Nocedal et al, 1999) method of pa-rameter estimation for our ME model.BASIC FEATURESz Predicate ?
The predicate lemmaz Path ?
The syntactic path through the parsing tree fromthe parse constituent be-ing classified to the predicatez Constituent typez Position ?
Whether the phrase is located before or afterthe predicatez Voice ?
passive: if the predicate has a POS tag VBN,and its chunk is not a VP, or it is preceded by a form of?to be?
or ?to get?
within its chunk; otherwise, it is ac-tivez Head word ?
calculated using the head word table de-scribed by (Collins, 1999)z Head POS ?
The POS of the Head Wordz Sub-categorization ?
The phrase structure rule that ex-pands the predicate?s parent node in the parsing treez First and last Word and their POS tagsz Level ?
The level in the parsing treePREDICATE FEATURESz Predicate?s verb classz Predicate POS tagz Predicate frequencyz Predicate?s context POSz Number of predicatesFULL PARSING FEATURESz Parent?s, left sibling?s, and right sibling?s paths, con-stituent types, positions, head words and head POStagsz Head of PP parent ?
If the parent is a PP, then the headof this PP is also used as a featureCOMBINATION FEATURESz Predicate distance combinationz Predicate phrase type combinationz Head word and predicate combinationz Voice position combinationOTHERSz Syntactic frame of predicate/NPz Headword suffixes of lengths 2, 3, and 4z Number of words in the phrasez Context words & POS tagsTable 2.
The Features Used in the Baseline Argu-ment Classification Model3.2 Basic FeaturesTable 2 shows the features that are used in ourbaseline argument classification model.
Their ef-fectiveness has been previously shown by (Pradhanet al, 2004; Surdeanu et al, 2003; Xue et al,2004).
Detailed descriptions of these features canbe found in (Tsai et al, 2005).3.3 Named Entity FeaturesIn the newswire domain, Surdeanu et al (2003)used named entity (NE) features that indicatewhether a constituent contains NEs, such as per-sonal names, organization names, location names,time expressions, and quantities of money.
Usingthese NE features, they increased their system?s F-score by 2.12%.
However, because NEs in thebiomedical domain are quite different from news-wire NEs, we create bio-specific NE features usingthe five primary NE categories found in theGENIA ontology1: protein, nucleotide, other or-ganic compounds, source and others.
Table 3 illus-trates the definitions of these five categories.
Whena constituent exactly matches an NE, the corre-sponding NE feature is enabled.NE DefinitionProtein Proteins include protein groups, families, molecules, complexes, and substructures.Nucleotide A nucleic acid molecule or the compounds that consist of nucleic acids.Other organiccompoundsOrganic compounds exclude protein andnucleotide.SourceSources are biological locations wheresubstances are found and their reactionstake place.OthersThe terms that are not categorized assources or substances may be marked up,withTable 3.
Five GENIA Ontology NE Categories3.4 Biomedical Template FeaturesAlthough a few NEs tend to belong almost exclu-sively to certain argument types (such as ?
?cell?being mainly AM-LOC), this information alone isnot sufficient for argument-type classification.
Forone, most NEs appear in a variety of argumenttypes.
For another, many appear in more than oneconstituent (node in a parsing tree) in the samesentence.
Take the sentence ?IL4 and IL13 recep-tors activate STAT6, STAT3 and STAT5 proteinsin the human B cells,?
for example.
The NE ?thehuman B cells?
is found in two constituents (?the1 http://www-tsujii.is.s.u-tokyo.ac.jp/~genia/topics/Corpus/genia-ontology.html60human B cells?
and ?in the human B cells?)
asshown in figure 1.
Yet only ?in the human B cells?is an AM-LOC because here ?human B cells?
ispreceded by the preposition ?in?
and the deter-miner ?the?.
Another way to express this would beas a template?<prep> the <cell>.?
We believesuch templates composed of NEs, real words, andPOS tags may be helpful in identifying constitu-ents?
argument types.
In this section, we first de-scribe our template generation algorithm, and thenexplain how we use the generated templates to im-prove SRL performance.Template Generation (TG)Our template generation (TG) algorithm extractsgeneral patterns for all argument types using thelocal alignment algorithm.
We begin by pairing allarguments belonging to the same type according totheir similarity.
Closely matching pairs are thenaligned word by word and a template that fits bothis created.
Each slot in the template is given con-straint information in the form of either a word, NEtype, or POS.
The hierarchy of this constraint in-formation is word > NE type > POS.
If the argu-ments share nothing in common for a given slot,the TG algorithm will put a wildcard in that posi-tion.
Figure 2 shows an aligned pair arguments.For this pair, the TG algorithm generated the tem-plate ?AP-1 CC PTN?
(PTN: protein name) be-cause in the first position, both arguments have?AP-1;?
in the second position, they have the samePOS ?CC;?
and in the third position, they share acommon NE type, ?PTN.?
The complete TG algo-rithm is described in Algorithm 1.AP-1/PTN/NN and/O/CC NF-AT/PTN/NNAP-1/PTN/NN or/O/CC NFIL-2A/PTN/NNFigure 2.
Aligned Argument PairApplying Generated TemplatesThe generated templates may match exactly or par-tially with constituents.
According to our observa-tions, the former is more useful for argumentclassification.
For example, constituents that per-fectly match the template ?IN a * <cell>?
areoverwhelmingly AM-LOCs.
Therefore, we onlyaccept exact template matches.
That is, if a con-stituent exactly matches a template t, then the fea-ture corresponding to t will be enabled.Algorithm 1 Template GenerationInput: Sentences set S = {s1, .
.
.
, sn},Output: A set of template T = {t1, .
.
.
, tk}.1: T = {};2: for each sentence si from s1 to sn-1 do3:    for each sentence sj from si to sn do4:        perform alignment on si and sj, then5:          pair arguments according to similarity;6:        generate common template t from argument pairs;7:        T?T?t;8:    end;9: end;10: return T;4 Experiments4.1 DatasetsIn this paper, we extracted all our datasets fromtwo corpora, the Wall Street Journal (WSJ) corpusand the BioProp, which respectively represent thenewswire and biomedical domains.
The WallStreet Journal corpus has 39,892 sentences, and950,028 words.
It contains full-parsing information,first annotated by Marcus et al (1997), and is themost famous treebank (WSJ treebank).
In additionto these syntactic structures, it was also annotatedwith predicate-argument structures (WSJ proposi-tion bank) by Palmer et al (2005).In biomedical domain, there is one availabletreebank for GENIA, created by Yuka Tateshi et al(2005), who has so far added full-parsing informa-tion to 500 abstracts.
In contrast to WSJ, however,GENIA lacks any proposition bank.Since predicate-argument annotation is essentialfor training and evaluating statistical SRL systems,to make up for GENIA?s lack of a propositionbank, we constructed BioProp.
Two biologists withmasters degrees in our laboratory undertook theannotation task after receiving computational lin-guistic training for approximately three months.We adopted a semi-automatic strategy to anno-tate BioProp.
First, we used the PropBank to traina statistical SRL system which achieves an F-scoreof over 86% on section 24 of the PropBank.
Next,we used this SRL system to annotate the GENIAtreebank automatically.
Table 4 shows the amountsof all adjunct argument types (AMs) in BioProp.The detail description of can be found in (Babko-Malaya, 2005).61Type Description # Type Description #NEG negationmarker103 ADV generalpurpose307LOC location 389 PNC purpose 3TMP time 145 CAU cause 15MNR manner 489 DIR direction 22EXT extent 23 DIS discourseconnectives179MOD modal verb 121Table 4.
Subtypes of the AM Modifier Tag4.2 Experiment DesignExperiment 1: PortabilityIdeally, an SRL system should be adaptable to thetask of information extraction in various domainswith minimal effort.
That is, we should be able toport it from one domain to another.
In this experi-ment, we evaluate the cross-domain portability ofour SRL system.
We use Sections 2 to 21 of thePropBank to train our SRL system.
Then, we useour system to annotate Section 24 of the PropBank(denoted by Exp 1a) and all of BioProp (denotedby Exp 1b).Experiment 2: The Necessity of BioPropTo compare the effects of using biomedical train-ing data vs. using newswire data, we train our SRLsystem on 30 randomly selected training sets fromBioProp (g1,.., g30) and 30 from PropBank (w1,..,w30), each having 1200 training PAS?s.
We thentest our system on 30 400-PAS test sets from Bio-Prop, with g1 and w1 being tested on test set 1, g2and w2 on set 2, and so on.
Then we add up thescores for w1-w30 and g1-g30, and compare theiraverages.Experiment 3: The Effect of Using Biomedical-Specific FeaturesIn order to improve SRL performance, we add do-main specific features.
In Experiment 3, we inves-tigate the effects of adding biomedical NE featuresand argument template features composed ofwords, NEs, and POSs.
The dataset selection pro-cedure is the same as in Experiment 2.5 Results and DiscussionAll experimental results are summarized in Table 5.For argument classification, we report the preci-sion (P), recall (R) and F-scores (F).
The detailsare illustrated in the following paragraphs.Configuration Training Test P R FExp 1a PropBank PropBank 90.47 82.48 86.29Exp 1b PropBank BioProp 75.28 56.64 64.64Exp 2a PropBank BioProp 74.78 56.25 64.20Exp 2b BioProp BioProp 88.65 85.61 87.10Exp 3a BioProp BioProp 88.67 85.59 87.11Exp 3b BioProp BioProp 89.13 86.07 87.57Table 5.
Summary of All ExperimentsExp 1a Exp 1b RoleP R F P R F+/-(%)Overall 90.47 82.48 86.29 75.28 56.64 64.64 -21.65ArgX 91.46 86.39 88.85 78.92 67.82 72.95 -15.90Arg0 86.36 78.01 81.97 85.56 64.41 73.49   -8.48Arg1 95.52 92.11 93.78 82.56 75.75 79.01 -14.77Arg2 87.19 84.53 85.84 32.76 31.59 32.16 -53.68AM 86.76 70.02 77.50 62.70 32.98 43.22 -34.28-ADV 73.44 52.32 61.11 39.27 26.34 31.53 -29.58-DIS 81.71 48.18 60.62 67.12 48.18 56.09 -4.53-LOC 89.19 57.02 69.57 68.54 2.67 5.14 -64.43-MNR 67.93 57.86 62.49 46.55 22.97 30.76 -31.73-MOD 99.42 92.5 95.84 99.05 88.01 93.2 -2.64-NEG 100 91.21 95.40 99.61 80.13 88.81 -6.59-TMP 88.15 72.83 79.76 70.97 60.36 65.24 -14.52Table 6.
Performance of Exp 1a and Exp 1bExperiment 1Table 6 shows the results of Experiment 1.
TheSRL system trained on the WSJ corpus obtains anF-score of 64.64% when used in the biomedicaldomain.
Compared to traditional rule-based ortemplate-based approaches, our approach suffersacceptable decrease in overall performance whenrecognizing ArgX arguments.
However, Table 6also shows significant decreases in F-scores fromother argument types.
AM-LOC drops 64.43% andAM-MNR falls 31.73%.
This may be due to thefact that the head words in PropBank are quite dif-ferent from those in BioProp.
Therefore, to achievebetter performance, we believe it will be necessaryto annotate biomedical corpora for training bio-medical SRL systems.Experiment 2Table 7 shows the results of Experiment 2.
Whentested on BioProp, BIOSMILE (Exp 2b) outper-forms the newswire SRL system (Exp 2a) by22.9% since the two systems are trained on differ-ent domains.
This result is statistically significant.Furthermore, Table 7 shows that BIOSMILEoutperforms the newswire SRL system in most62argument types, especially Arg0, Arg2, AM-ADV,AM-LOC, AM-MNR.Exp 2a Exp 2b RoleP R F P R F+/-(%)Overall 74.78 56.25 64.20 88.65 85.61 87.10 22.90ArgX 78.40 67.32 72.44 91.96 89.73 90.83 18.39Arg0 85.55 64.40 73.48 92.24 90.59 91.41 17.93Arg1 81.41 75.11 78.13 92.54 90.49 91.50 13.37Arg2 34.42 31.56 32.93 86.89 81.35 84.03 51.10AM 61.96 32.38 42.53 81.27 76.72 78.93 36.40-ADV 36.00 23.26 28.26 64.02 52.12 57.46 29.20-DIS 69.55 51.29 59.04 82.71 75.60 79.00 19.96-LOC 75.51 3.23 6.20 80.05 85.00 82.45 76.25-MNR 44.67 21.66 29.17 83.44 82.23 82.83 53.66-MOD 99.38 88.89 93.84 98.00 95.28 96.62 2.78-NEG 99.80 79.55 88.53 97.82 94.81 96.29 7.76-TMP 67.95 60.40 63.95 80.96 61.82 70.11 6.16Table 7.
Performance of Exp 2a and Exp 2bThe performance of Arg0 and Arg2 in our sys-tem increases considerably because biomedicalverbs can be successfully identified by BIOSMILEbut not by the newswire SRL system.
For AM-LOC, the newswire SRL system scored as low as76.25% lower than BIOSMILE.
This is likely dueto the reason that in the biomedical domain, manybiomedical nouns, e.g., organisms and cells, func-tion as locations, while in the newswire domain,they do not.
In newswire, the word ?cell?
seldomappears.
However, in biomedical texts, cells repre-sent the location of many biological reactions, and,therefore, if a constituent node on a parsing treecontains ?cell?, this node is very likely an AM-LOC.
If we use only newswire texts, the SRL sys-tem will not learn to recognize this pattern.
In thebiomedical domain, arguments of manner (AM-MNR) usually describe how to conduct an experi-ment or how an interaction arises or occurs, whilein newswire they are extremely broad in scope.Without adequate biomedical domain training cor-pora, systems will easily confuse adverbs of man-ner (AM-MNR), which are differentiated fromgeneral adverbials in semantic role labeling, withgeneral adverbials (AM-ADV).
In addition, theperformance of the referential arguments of Arg0,Arg1, and Arg2 increases significantly.Experiment 3Table 8 shows the results of Experiment 3.
Theperformance does not significantly improve afteradding NE features.
We originally expected thatNE features would improve recognition of AMarguments such as AM-LOC.
However, they failedto ameliorate the results since in the biomedicaldomain most NEs are just matched parts of a con-stituent.
This results in fewer exact matches.
Fur-thermore, in matched cases, NE information aloneis insufficient to distinguish argument types.
Forexample, even if a constituent exactly matches aprotein name, we still cannot be sure whether itbelongs to the subject (Arg0) or object (Arg1).Therefore, NE features were not as effective as wehad expected.NE (Exp 3a) Template (Exp 3b) RoleP R F P R F+/-(%)Overall 88.67 85.59 87.11 89.13 86.07 87.57 0.46ArgX 91.99 89.70 90.83 91.89 89.73 90.80 -0.03Arg0 92.41 90.57 91.48 92.19 90.59 91.38 -0.1Arg1 92.47 90.45 91.45 92.42 90.44 91.42 -0.03Arg2 86.93 81.3 84.02 87.08 81.66 84.28 0.26AM 81.30 76.75 78.96 82.96 78.18 80.50 1.54-ADV 64.11 52.23 57.56 65.66 55.60 60.21 2.65-DIS 82.51 75.42 78.81 83.00 75.79 79.23 0.42-LOC 80.07 85.09 82.50 84.24 85.48 84.86 2.36-MNR 83.50 82.19 82.84 84.56 84.14 84.35 1.51-MOD 98.14 95.28 96.69 98.00 95.28 96.62 -0.07-NEG 97.66 94.81 96.21 97.82 94.81 96.29 0.08-TMP 81.14 62.06 70.33 83.10 63.95 72.28 1.95Table 8.
Performance of Exp 3a and Exp 3b6 Conclusions and Future WorkIn Experiment 3b, we used the argument templatesas features.
Since ArgX?s F-score is close to 90%,adding the template features does not improve itsscore.
However, AM?s F-score increases by 1.54%.For AM-ADV, AM-LOC, and AM-TMP, the in-crease is greater because the automatically gener-ated templates effectively extract these AMs.In Figure 3, we compare the performance of ar-gument classification models with and without ar-gument template features.
The overall F-scoreimproves only slightly.
However, the F-scores ofmain adjunct arguments increase significantly.The contribution of this paper is threefold.
First,we construct a biomedical proposition bank, Bio-Prop, on top of the popular biomedical GENIAtreebank following the PropBank annotationscheme.
We employ semi-automatic annotationusing an SRL system trained on PropBank, therebysignificantly reducing annotation effort.
Second,we create BIOSMILE, a biomedical SRL system,which uses BioProp as its training corpus.
Thirdly,we develop a method to automatically generatetemplates that can boost overall performance, es-63pecially on location, manner, adverb, and temporalarguments.
In the future, we will expand BioPropto include more verbs and will also integrate anautomatic parser into BIOSMILE.Figure 3.
Improvement of Template FeaturesOverall and on Several Adjunct TypesAcknowledgementWe would like to thank Dr. Nianwen Xue for hisinstruction of using the WordFreak annotation tool.This research was supported in part by the NationalScience Council under grant NSC94-2752-E-001-001 and the thematic program of Academia Sinicaunder grant AS94B003.
Editing services were pro-vided by Dorion Berg.ReferencesBabko-Malaya, O.
(2005).
Propbank AnnotationGuidelines.Bies, A., Ferguson, M., Katz, K., MacIntyre, R.,Tredinnick, V., Kim, G., et al (1995).
BracketingGuidelines for Treebank II Style Penn TreebankProjectCollins, M. J.
(1999).
Head-driven Statistical Modelsfor Natural Language Parsing.
Unpublished Ph.D.thesis, University of Pennsylvania.Darroch, J. N., & Ratcliff, D. (1972).
GeneralizedIterative Scaling for Log-Linear Models.
The Annalsof Mathematical Statistics.Hacioglu, K., Pradhan, S., Ward, W., Martin, J. H., &Jurafsky, D. (2004).
Semantic Role Labeling byTagging Syntactic Chunks.
Paper presented at theCONLL-04.Huang, M., Zhu, X., Hao, Y., Payan, D. G., Qu, K., &Li, M. (2004).
Discovering patterns to extractprotein-protein interactions from full texts.Bioinformatics, 20(18), 3604-3612.Kholodenko, B. N. (2006).
Cell-signalling dynamics intime and space.
Nat Rev Mol Cell Biol, 7(3), 165-176.Kim, J. D., Ohta, T., Tateisi, Y., & Tsujii, J.
(2003).GENIA corpus--semantically annotated corpus forbio-textmining.
Bioinformatics, 19 Suppl 1, i180-182.Leroy, G., Chen, H., & Genescene.
(2005).
Anontology-enhanced integration of linguistic and co-occurrence based relations in biomedical texts.Journal of the American Society for InformationScience and Technology, 56(5), 457-468.Marcus, M. P., Santorini, B., & Marcinkiewicz, M.
A.(1997).
Building a large annotated corpus of English:the Penn Treebank.
Computational Linguistics, 19.Morarescu, P., Bejan, C., & Harabagiu, S. (2005).Shallow Semantics for Relation Extraction.
Paperpresented at the IJCAI-05.Nocedal, J., & Wright, S. J.
(1999).
NumericalOptimization: Springer.Palmer, M., Gildea, D., & Kingsbury, P. (2005).
Theproposition bank: an annotated corpus of semanticroles.
Computational Linguistics, 31(1).Pradhan, S., Hacioglu, K., Kruglery, V., Ward, W.,Martin, J. H., & Jurafsky, D. (2004).
Support vectorlearning for semantic argument classification.Journal of Machine LearningPunyakanok, V., Roth, D., Yih, W., & Zimak, D. (2004).Semantic Role Labeling via Integer LinearProgramming Inference.
Paper presented at theCOLING-04.Surdeanu, M., Harabagiu, S. M., Williams, J., &Aarseth, P. (2003).
Using Predicate-ArgumentStructures for Information Extraction.
Paperpresented at the ACL-03.Tateisi, Y., & Tsujii, J.
(2004).
Part-of-SpeechAnnotation of Biology Research Abstracts.
Paperpresented at the LREC-04.Tateisi, Y., Yakushiji, A., Ohta, T., & Tsujii, J.
(2005).Syntax Annotation for the GENIA corpus.Tsai, T.-H., Wu, C.-W., Lin, Y.-C., & Hsu, W.-L.(2005).
Exploiting Full Parsing Information to LabelSemantic Roles Using an Ensemble of ME and SVMvia Integer Linear Programming.
.
Paper presented atthe CoNLL-05.Wattarujeekrit, T., Shah, P. K., & Collier, N. (2004).PASBio: predicate-argument structures for eventextraction in molecular biology.
BMC Bioinformatics,5, 155.Xue, N., & Palmer, M. (2004).
Calibrating Features forSemantic Role Labeling.
Paper presented at theEMNLP-04.Yang, X., Zhou, G., Su, J., & Tan., C. (2004).Improving Noun Phrase Coreference Resolution byMatching Strings.
Paper presented at the IJCNLP-04.64
