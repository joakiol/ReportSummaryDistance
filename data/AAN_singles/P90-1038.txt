BOTTOM-UP PARSING EXTENDING CONTEXT-FREENESSIN  A PROCESS GRAMMAR PROCESSORMass imo Mar inoDepartment  o f  L inguist ics - Univers i ty o f  PisaVia S. Mar ia 36 1-56100 Pisa - ITALYBitnet: massimom@icnucevm.cnuce.cnr.itABSTRACTA new approach to bottom-up arsing that extendsAugmented Context-Free Grammar to a Process Grammaris formally presented.
A Process Grammar (PG) defines aset of rules suited for bottom-up arsing and conceived asprocesses that are applied by a P G Processor.
The matchingphase is a crucial step for process application, and aparsing structure for efficient matching is also presented.The PG Processor is composed of a process cheduler thatallows immediate constituent analysis of structures, andbehaves in a non-deterministic fashion.
On the other side,the PG offers means for implementing spec~c parsingstrategies improving the lack of determinism innate in theprocessor.1.
INTRODUCTIONBottom-up parsing methods are usually preferredbecause of their property of being driven from both theinput's syntactic/semantic structures and reducedconstituents structures.
Different strategies have beenrealized for handling the structures construction, e.g.,parallel parsers, backtracking parsers, augmented context-free parsers (Aho et al, 1972; Grishman, 1976; Winograd,1983).
The aim of this paper is to introduce a new approachto bottom-up parsing starting from a well known and basedframework - parallel bottom-up arsing in immediateconstituent analysis, where all possible parses areconsidered - making use of an Augmented Phrase-S tructureGrammar (APSG).
In such environment we must performefficient searches inthe graph the parser builds, and limit asmuch as possible the building of structures that will not bein the final parse tree.
For the efficiency of the search weintroduce a Parse Graph Structure, based on the def'mition ofadjacency of the subtrees, that provides an easy method ofevaluation for deciding at any step whether a matchingprocess can be accomplished or not.
The control of theparsing process is in the hands of an APSG called ProcessGrammar fPG), where grammar rules are conceived asprocesses that are applied whenever proper conditions,detected by a process scheduler, exist.
This is why theparser, called PG Processor, works following a non-deterministic parallel strategy, and only the ProcessGrammar has the power of altering and constraining thisbehaviour by means of some Kernel Functions that canmodify the control structures of the PG Processor, thus299improving determinism ofthe parsing process, or avoidingconstruction of useless tructures.
Some of the conceptsintroduced inthis paper, such as some definitions in Section2,  are a development from Grishman (1976) that can be alsoan introductory reading regarding the description of aparallel bottom-up parser which is, even if under adifferentaspect, the core of the PG Processor.2.
PARSE GRAPH STRUCTUREThe Parse Graph Structure (PGS) is built by the parserwhile applying rammar rules.
If s = a a a2... a is an inputstring the initial PGS is composed by a set of terminal nodes<0,$>, <l,aa>, <2,a2> ..... <n,a >, <n+l,$>, where nodes0,n+ 1 represent border markers for the sentence.
All the nextnon-terminal nodes are numbered starting from n+2.Definition 2.1.
A PGS is a triple (Nr,Nr~,T) where N r is theset of the terminal nodes numbers {0, 1 .. .
.
.
n, n+l}; N N isthe set of the non-terminal nodes numbers {n+2 .... }, and Tis the set of the subtrees.The elements of N N and N T are numbers identifying nodesof the PGS whose structure is defined below, andthroughout the paper we refer to nodes of the PGS by meansof such nodes number.Definition 2.2.
If ke Nr~ the node ie N r labeling a i at thebeginning of the clause covered by k is said to be the leftcorner leaf of k lcl(k).
If ke N r then lcl(k)=k.Definition 2.3.
I fke N s the nodeje N T labeling aj at the endof the clause covered by k is said to be the right corner leafof k rcl(k).
If ke N T then rcl(k) = k.Definition 2.4.
Ifk~ N N the node he N r that follows the rightcorner leaf of k rel(k) is said to be the anchor leafofk al(k),and al(k) = h = rel(k)+L IfkeNT-{n+l }then al(k) = k+l.Definition 2.5.
If ke N T the set of the anchored nodes ofk an(k) is an(k) = {j~ NTUN s I alQ) = k}.From this definition it follows that for every ke NT-{0},an(k) contains at the initial time the node number (k-l).Definition 2.6. a.
If keN T the subtree rooted in k T(k) isrepresented byT(k) = <k,lcl(k),rcl(k),an(k),cat(k)>, wherekis theroot node; lcl(k)-- rel(k)= k; an(k) = {(k-l)} initially;cat(k) = a~, the terminal category of the node.b.
If ke Nr~ the subtree rooted in k T(k) is represented byT(k)=<k,lcl(k),rcl(k),sons(k),cat(k)>, where k is the rootnode; sons(k) = {s I..... sv}, sic NTuN s, i = 1 ..... p, is the setof the direct descendants of k; cat(k) = A, a non-terminalcategory assigned to the node.From the above definitions the initial PGS for asentence s=a~av..a n is: Nr={0,1 ..... n,n+l}, Ns={},T= { T(0),T(1 ) ..... T(n) ,T(n+ 1 ) }; and: T(0)=<0,0,0, { } $>,T(i)=<i,i,i, {i- 1 } ,ai> for i= 1 ..... n, and T(n+ 1)=<n+ 1,n+l,n+l,{n} ,$>.
With this PGS the parser starts its workreducing new nodes from the already existing ones.
If forsome k~Nr~, T(k)=<k,lcl(k),rcl(k),{s 1 ..... sp},A>, andT(s)=<si,lcl(sl),rcl(s~),{ s n ..... s~t},zi>e T, for i = 1 ..... p0 arethe direct descendants of k, then k has been reduced froms~ .... ,s t by some grammar rule whose reduction rule, as weshall see later, has the form (A~---z v ..zp), and the followingholds: lcl(k) = lcl(st), rcl(s~) = lcl(s2)-l, rcl(s2) = lcl(ss)-1 .....rcl(sr, l) = lcl(sr)- 1, rcl(sp) = rcl(k).
From that we can give thefollowing definition:<12,a12><14,a14> <13,a13><0,$> <l,al> <2,a2>{} {0} {I}of the match process the matcher must start from the lastscanned or built node z s, finding afterwards z 2 and z~,respectively, sailing in the PGS right-to-left and passingthrough adjacent subtrees.
Steps through adjacent subtreesare easily accomplished by using the sets of the anchorednodes in the terminal nodes.
It follows from the abovedef'mitions that if k~ N N then the subtrees adjacent to T(k)are given by an(lel(k)), whereas ff k~ N r then the adjacentsubtrees are given by an(k).
The lists of the anchored nodesprovide an efficient way to represent the relation ofadjacency between odes.
These sets stored only in theterminal nodes provide an efficient data structure useful forthe matcher to accomplish its purpose.
Figure 1 shows aparse tree at a certain time of a parse, where under eachIT(9) = <9,1,2,{1,2},a9>TOO) = <10,2,2,{2},a10>T(ll) = <11,2,3,{ 10,3},al 1>T(12) = <12,1,3,{9,3},a12>T(13) = <13,4,5, {4,5 },a13>T(14) -- <14,3,5,{3,4,5 } a14><3#3> <4,a4> <5,a5> <6,a6> <7,a7> <8,$>{2,9,10} {3,11,12} {4} {5,13,14} {6} {7}Figure 1.
A parse tree with the sets of the anchored nodes5,4148a7 I7a6 I6a5 a1413 143 11 122 9 10 1la l  a l \ [1 1Figure 2.Definition 2.7.
If { s t ..... s.} is a set of nodes in the PGS, thentheir subtrees T(s a) ..... T(~p) are said to be adjacent whenrcl(si) = lcl(si.~)-1 or, alternatively, al(si) = lcl(sm), for i =1 .... ,p-1.During a parsing process agreat effort is made in finding aset of adjacent subtrees that match a fight-hand side of areduction rule.
Let (A~z~ z 2 z 3) be a reduction rule, then theparser should start a match process to find all possible setsof adjacent subtrees such that heir categories match za z 2 z 3.The parser scans the input string left-to-right, so reductionsgrow on the left of the scanner pointer, and for the efficiency300Adjacency Treeterminal node there is the corresponding list of the anchorednodes.
A useful structure that can be derived from these setsis an adjacency tree, recursively defined as follows:Definition 2.8.
If (Nr,NwT) is a PGS for an input sentences, and Isl = n, then the adjacency tree for the PGS is so built:- n+1 is the root of the adjacency tree;- for every k~Nr-{0,1}uN ., the sons ofk are the nodes inan(Icl(k)) unless an(Icl(k))= {0}.Figure 2 shows the adjacency tree obtained from the partialparse tree in Figure 1.
Any passage from a node k to one ofits sons h in the adjacency tree represents a passage from a3 11 12 2 9 10t alt2 9 10 1 1 1I "11 1subtree T(k) to one of its adjacent subtrees T(h) in the PGS.Moreover, during a match process this means that aconstituent of the right-hand side has been consumed, andmatching the first symbol that-match process is f'mished.The adjacency lace also provides further useful informationfor optimizing the search during a match.
For every node k,if we consider the longest path from k to a leaf, its length isan upper bound for the length of the right hand side still toconsume, and since the sons ofk are the nodes in an(lcl(k)),the longest path is always given by the sequence of theterminal nodes from the node 1 to the node lcl(k)- 1.
Thus itslength is just lcl(k)-l.Property 2.1.
If (Nr,Ns,T) is a PGS, (A~zl.
.
.z v) is areduction rule whose right-hand side has to be matched, andT(k)~ T such that cat(k) = z ,  then:a. the string z t ... zp is matc'hable iffp < lcl(k);b. for i = p ..... 1, zt is partially matchable to a nodeDefinition 2.10.
If (Nr,Ns,T) is a PGS, an adjacencydigraph can be represented asfollows:a. for any ke N r, k has outgoing arcs directed to the nodes inan(k);b. for any k?
N N, k has one outgoing arc directed to lcl(k).In the classic literature the lists of the anchored nodes arecalled adjacency lists, and are used for representing graphs(Aho et at., 1974).
A graph G=(V,E) can be usuallyrepresented byIVI adjacency lists.
In our representation wecan obtain an optimization representing an adjacencydigraph by n adjacency lists, if n is the length of the sentence,and by INsl simple pointers for accessing the adjacency listsfrom the non-terminal nodes, with respect to n+lNsladjacency lists for a full representation f an adjacencydigraph composed of arcs as in Det'mition 2.10.a.Figure 3 shows how a new non-terminal node is connectedin an adjacency digraph, and Figure 4 shows the adjacencyk \[ lcl(k) ~I- - - - k access from k to lcl(k),ql-lcl(k-1) ~ lcl(k),~- ... al(k) =rc l (k)+l .~-  I r ~ k T(k) is adjacent to T(r)Figure 3.
Adding a non-terminal node k to an adjacency digraph04$_ 1 ~4r  "a~ " " " ;~  " "~ _ ~ a4 5.4t.....~..__~ ~7.~.~__8".. , / , " .
.
j ' - - .-Id" '~ l l P "  " " '14~ '13 rFigure 4.
Adjacency Digraphhe NNuN riff cat(h) = z i and i < Icl(h).Property 2.
I. along with the adjacency relation provides amethod for an efficient navigation within the PGS amongthe subtrees.
This navigation isperformed by the matcher inthe PGS as visiting the adjacency tree in a pre-order fashion.It is easy to see that a pre-order visit of the adjacency treescans all possible sequences ofthe adjacent subtrees in thePGS, but Property 2.1 provides a shortcut for avoidinguseless passages when matchable conditions do not hold.When a match ends the matcher returns one or more sets ofnodes atisfying the following conditions:Definition 2.9.
A set RSet = {n I.... ,np} is a match for a stringzl...zpiff cat(nl) ffi z i, for i = 1,...,p, and T(nl) is adjacent toT(ni, l), for i = 1 .... ,p-1.
The set RSet is called a reductionset.The adjacency tree shows the hypothetical search space forsearching the reduction sets in a PGS, thus it is not arepresentation f what memory is actually required to storethe useful data for such a search.
A more suitablerepresentation is an adjacency directed graph defined bymeans of the lists of the anchored nodes in the terminalnodes, and by the pointers to the left comer leaf in the non-terminal nodes.301digraph for the parse tree of Figure 1.3.
PROCESS GRAMMARThe Process Grammar isan extension of the AugmentedContext-Free Grammar such as APSG, oriented to bottom-up parsing.
Some relevant features make a ProcessGrammar quite different from classical APSG.1.
The parser is a PG processor that ries to apply the rulesin a bottom-up fashion.
It does not have any knowledgeabout he running rammar but for the necessary structuresto access its rules.
Furthermore, it sees only its internal state,the Parse Graph Structure, and works with a non-deterministic strategy.2.
The rules are conceived as processes that the PGprocessor schedules somehow.
Any rule defines areductionrule that does not represent a rewriting rule, but rather astatement for search and construction of new nodes in abottom-up way within the Parse Graph Structure.3.
The rules are augmented with some sequences ofoperations to be performed as in the classical APSG.
Ingeneral, augmentations such as tests and actions concernmanipulation flinguistic data at syntactic and/or semanticlevel.
In this paper we are not concerned with this aspect (aninformal description about his is in Marino (1989)), ratherwe examine some aspects concerning parsing strategies bymeans of the augmentations.In a Process Grammar the rules can have knowledge ofthe existence of other ules and the purpose for which theyare defined.
They can call some functions that act as filterson the control structures of the parser for the scheduling ofthe processes, thus altering the state of the processor andforcing alternative applications.
This means that any rulehas the power of changing the state of the processorrequiring different scheduling, and the processor is a blindoperator that works following a loose strategy such as thenon-deterministic one, whereas the grammar can drive theprocessor altering its state.
In such a way the lack ofdeterminism of the processor can be put in the ProcessGrammar, implementing parsing strategies which aretransparent tothe processor.Definition 3.1.
A Process Grammar PG is a 6-tuple(VT,Vs,S,R,Vs,F) where:.
V r is the set of terminal symbols;- V N is the set of non-terminal symbols;- S?
V N is the Root Symbol of PG;- R = {r 1 .... ,rt} is the set of the rules.
Any rule r i in R is ofthe form r i = <red(ri),st(ri),t(ri),a(Q>, where red(ri) is areduction rule (A~---a), A~ Vr~, ct~ (VruVN)+; st(r) is thestate of the rule that can be active or inactive; t(Q and a(Qare the tests and the actions, respectively;- V s is a set of special symbols that can occur in a reductionrule and have a special meaning.
A special symbol is e a, anull category that can occur only in the left-hand side of areduction rule.
Therefore, a reduction rule can also have theform (e~?---a), and in the following we refer to it as e-reduction;- F = {fl ..... f \ ]  is a set of functions the rules can call withintheir augmentations.Such a definition extends classical APSG in some specificways: first, a Process Grammar is suited for bottom-upparsing; second, rules have a state concerning theapplicability of a rule at a certain time; third, we extend theCF structure of the reduction rule allowing null left-handsides by means of e-reductions; fourth, the set F is thestrategic side that should provide the necessary functions toperform operations on the processor structures.
As a matterof fact, the set F can be further structured giving the PG awider complexity and power.
In this paper we cannot reata formal extended definition for F due to space restrictions,but a brief outline can be given.
The set F can be defined asF=Fr~uFt,.
In F~ are all those functions devoted tooperations on the processor structures (Kernel Functions),and, in the case of a feature-based system, in Ft, are all thefunctions devoted to the management of feature structures(Marino, 1989).
In what follows we are also concerned withthe combined use of e-reductions and the function RA,standing for Rule Activation, devoted to the immediatescheduling of a rule.
RAe Fx~ ' and a call to it means that the302specified role must be applied, involving the schedulingprocess we describe in Section 4.
Before we introduce thePG processor we must give a useful definition:Definition 32.
Let reR be a rule with t(r)=\[f,1;...;f.~\],a(r)=\[f l ; .
.
.
; f  \] be sequences of operations in itsaugmentations, f,~..... f~,ft ..... feF .
Let {n 1 ..... rip) be areduction set for red(r) = (A~z  r..zv), and he Nr~ be the newnode for A such that T(h) is the new subtree created in thePGS, then we define the Process Environment for t(r) anda(r), denoted briefly by ProcEnv(r), as:ProcEnv(r) = {h,n 1 .... ,n.}If red(r) is an e-reduction then ProcEnv(r) = {nl .... ,np}.This definition states the operative range for theaugmentations of any rule is limited to the nodes involvedby the match of the reduction rule.4.
PG PROCESSORProcess Scheduler.
The process scheduler makespossible the scheduling of the proper ules to run whenevera terminal node is consumed in input or a new non-terminalnode is added to the PGS by a process.
By proper ules wemean all the rules satisfying Property 2.1.a.
with respect tothe node being scanned or built.
These rules are given by thesets def'med in the following definition:Definia'on 4.1.
Vce VsuV r such that 3 r~ R where red(r) =(Ac---ac), AeVNu{e~}, being c the right comer of thereduction rule, and lacl _< L, being L the size of the longestright-hand side having c as the right comer, the sets P(c,i),P,(c,i) for i = 1 ..... L, can be built as follows:P(c,i) = {re R I red(r)=(At---cxc), 1 < Itxcl _< i, st(r)=aclive}Pe(c,i)= {re R I red(r)=(eac---ac ), 1< lacl < i, st(r)=active}Whenever a node he NruNr~ has been scanned or built andk=lcl(h), then the process cheduler has to schedule the rulesin P(cat(h),k)uP,,(cat(h),k).
In the following this union isalso denoted by Yl(cat0a),k).
Such a rule scheduling allowsan efficient realization of the immediate constituentanalysis approach within a bottom-up arser by means of apartitioning of the roles in a Process Grammar.The process cheduler sets up aprocess descriptor for eachrule in l-l(cat0a),k) where the necessary data for applying aprocess in the proper environment are supplied.
In a ProcessGrammar we can have three main kinds of rules: rules thatare activated by others by means of the function RA; e-reduction roles; and standard rules that do not fall in theprevious cases.
This categorization implies that processeshave assigned a priority depending on their kind.
Thusactivated rules have the highest priority, e-reduction ruleshave an intermediate priority and standard rules the lowestpriority.
Rules become scheduled processes whenever aprocess descriptor for them is created and inserted in apriority queue by the process cheduler.
The priority queueis divided into three stacks, one for each kind of rule, andthey form one of the structures of the processor state.Definition 4.2.
A process descriptor is a triple PD=\[r,h,C\]where: m R is the rule involved; he NruNsu  {NIL} is eitherthe right corner node from which the marcher starts or NIL;C is a set of adjacent nodes or the empty set.
A processdescriptor of the form \[r,NiL,\[nl .... ,nc\] is built for anactivated rule r and pushed in the stack s r A processdescriptor of the form \[r,h, \[ } \] is built for all the other ulesand is pushed either in the stack s 2 if r is an e-reduction ruleor in the stack s 3 if a standard rule.
Process descriptors ofthese latter forms are handled by the process cheduler,whereas process descriptors for activated rules are onlycreated and queued by the function RA.State of Computation.
The PG processor operates bymeans of an operation Op on some internal structures thatdefine the processor state ProcState, and on the parsingstructures accessible by the process environment ProcEnv.The whole state of computation is therefore given by:\[Op,ProcState,ProcEnv\] = \[Op,pt,\[s~,svs3\],PD,pn,RSet\]where pt?
N r is the input pointer to the last terminal nodescanned; pn~ N~ is the pointer to the last non-terminal nodeadded to the PGS.
For a sentence s=a r. .a.
the computationstarts from the initial state \[begin,0,\[NIL,NIL,NIL\],NIL,n+I,{}\], and terminates when the state becomes\[end,n,\[NIL,NIL,NIL\],NIL,pn,\[ }\].
The aim of this sectionis not to give a complete description of the processor cyclein a parsing process, but an analysis of the activationmechanism of the processes by means of two main cases ofrule scheduling and processing.Scheduling and Processing of Standard Rules.Whenever the state of computation becomes as \[scan, pt,\[NIL,NILMIL\]MIL,pn,{ }\] the processor scans the nextterminal node, performing the following operations:sc an: scl if pt = n then Op <--- endsc2 else pt*-- pt + 1;sc3 schedule 0"I(cat(pt),lcl(pt)));sc4 Op <--- activate.Step sc4 allows the processor to enter in the state where itdetermines the first non-empty higher priority stack wherethe process descriptor for the next process to be activatedmust be popped off.
Let suppose that cat(pt)=zp, andl'I(z,lcl(p0)={r } where r is a standard rule such thatred(~)=(A<--zr..z ~.
At this point the state is \[activate,pt,\[NILMIL,\[r,pt, \[ } \]\] MIL,pn,\[ } \] and the processor has totry reduction for the process in the stack s v thusOp<--reduce performing the following statements:reduce: rl PD<---pop (%);\[reduce,pt,\[NIL,NIL,NIL\],\[r,pt, { } \],pn,{ }\]r2 C0--match (red(r), pt);C = {nl,...,n vpt}r3 PD<-Lir, pt, C\];303\[reduce,pt,\[Nfl.,MiL,NIL\],\[r,pt,C\],pn,{ }\]r4 V rset~ C:r5 RSet ~rset;\[reduce,pt,\[NiL,NILMIL\],\[r,pt, { } \],pn~RSet\]r6 if t(r) then pn<--pn + 1;r7 add subtree(pn ,red (r) ,R S e0;r8 a(r);r9 schedule (H(cat(pn),lcl(pn));\[reduce,pt,\[NIL,sv%\],\[r,pt, { } \], n,RSet\]rl00p<--activate.Step r9, where the process scheduler produces processdescriptors for all the rules in H(AJcl(pn)), impliesimmediate analysis of the new constituent added to the PGS.Scheduling and Processing of Rules Activated by ~-Reduction Rules.
Let consider the case when an ~-reduction rule r activates an inactive rule r' such that:red(r)f-(eat--zr..zp), a(r)=\[RA (r')\], red(r')=(A~zr..Zh),l~,_<.h<p, and st(r')=inactive.
When the operation activatehas checked that an g-reduction rule has to be activated thenOlx--~-reduce, thus the state of computation becomes:\[e.reduce,pt,\[NIL,\[r,m,{}\],NIL\],NIL,pn,{}\], a d thefollowing statements are performed:e-reduce: 0-I PD<---pop (sz);\[e-red uce,pt,\[NIL,NIL.NIL\] ,\[r,m, { } \] ,pn, { } \]~2 C<---match (red(r), m);C = (n I .... ,n I, m}0-3 f~b.\[r,m,C\];\[e.red uce,pt,\[NIL,NIL.NIL\] ,\[r,m,C\] ,pn, { } \]0-4 V rsemC:0"5 RSet.--rset;\[e-red uce,pt.
\[NIL,NIL,NIL\],\[r,m, { } \],pn,RSet\]0-6 if t(r) then a(r)=\[RA (r')\];\[?.reduce,pt,\[\[r',NIL,{n k .... , h}\],NIL,NIL\],\[r,m,{}\],pn,RSet\]0"70lx--activate.In this case, unlike that which the process cheduler does,the function RA performs at step 0-6 the scheduling of aprocess descriptor in the stack s, where a subset ofProcEnv(r) is passed as the ProcEnv(r').
Therefore, when ane-reduction rule r activates another rule r' the step er2 doesthe work also for r', and RA just has to identify the ProcEnvof the activated rule inserting it in the process descriptor.Afterwards, the operation activate checks the highestpriority stack s, is not empty, therefore it pops the processdescriptor \[r',NIL,{ nk ..... n u} \] and OIx--h-reduce that skipsthe match process applying immediately the rule r':h-reduce: hrl RSet<--C;\[h-reduce,pt,\[NiL,NIL,NlL\],\[r',NIL,{ } \],pn,RSet\]hr2 through hr6 as r6 through rl0.From the above descriptions it turns out that theoperation activate plays a central role for deciding whatoperation must run next depending on the state of the threestacks.
The operation activate just has to check whethersome process descriptor is in the first non-empty higherpriority stack, and afterwards to set the proper operation.The following statements describe such a work and Figure5 depicts graphically the connections among the operationsdefined in this Section.activate: al if sI=NILa2 then if %=NILa3 then if s,=NILa4 then Op ~ scana5 else Op <-- reducea6 else Op <--- c-reducea7 else PD ~ pop (%);PD = \[r,NIL,C\]a8 Op <-- h-reduce.s l=s2=s3=NI~Figure 5.
Operations Transition Diagram5.
EXAMPLEIt is well known that bottom-up parsers have problemsin managing rules with common right-hand sides like X --->ABCD, X ---> BCD, X ---> CD, X ---> D, since some or all ofthese rules can be fired and build unwanted nodes.
A strategycalled top-down filtering in order to circumvent such aproblem has been stated, and it is adopted within bottom-upparsers (Kay, 1982; Pratt, 1975; Slocum, 1981; Wir6n,1987) where it simulates a top-down parser together with thebottom-up parser.
The PG Processor must face this problemas well, and the example we give is a Process Grammarsubset of rules that tries to resolve it.
The kind of solutionproposed can be put in the family of top-down filters as well,taking advantage firstly of using e-reduction rules.Unfortunately, the means described so far are stillinsufficient o solve our problem, thus the followingdefinitions introduce some functions that extend the ProcessGrammar and the control over the PGS and the PGProcessor.Definition 5.1.
Let r be a rule of R with red(r)=(~--z v..z),and RSet={n,...np} bea reduction set for red(r).
Taken twonodes %,nje RSet where n,e N N such that we have cat(n)--z,,cat(nj)=zj, and T(n~), T(n) are adjacent, i.e., either j=i+ 1 or304j=i- 1, then the function Add_Son_Rel of Fx= when called ina(r) as Add_Son_Rel (zi~z) has the effect of creating anewparent-son relation between %, the parent, and n, the son,altering the sets sons(n), and either 1cI(%) or rcl(n) asfollows:a) sons(n) ~- sons(n) u {nj}b) lcl(n) ~ lcl(nj) ifj=i-1c) rcl(n) 6-- rcl(n) ifj=i+lSuch a function has the power of making an alteration i thestructure of a subtree in the PGS extending its coverage toone of its adjacent subtrees.Definition 5.2.
The function RE of Fr~, standing for RuleEnable, when called in the augmentations of some rule r asRE (r'), where r, r' are in R, sets the state of r' as active,masking the original state set in the definition of r'.Without entering into greater detail, the function RE canhave the side effect of scheduling the just enabled rule r'whenever the call to RE follows the call Add Son Rel(X,Y) for some category Xe V,,,Ye V,wVr, and the rightcorner of red(r') is X.Definition 5.3.
The function RD of Fx, , standing for RuleDisable, when called in the augmentations of ome rule r asRD (r'), where r, r' are in R, sets the state ofr '  as inactive,masking the original state set in the definition of r'.We axe now ready to put the problem as follows: given,for instance, the following set P1 of productions:PI = {X --> ABCD, X ---> BCD, X --> CD, X ---> D}we want o define aset of PG rules having the same coverageof the productions in PI with the feature of building in anycase just one node X in the PGS.Such a set of rules is shown in Figure 6 and its aim is to createlinks among the node X and the other constituents just whenthe case occurs and is detected.
All the possible cases aredepicted in Figure 7 in chronological order of building.The only active rule is r0 that is fired whenever a D is insertedin the PGS, thus a new node X is created by r0 (case (a)).Since the next possible case is to have a node C adjacent tothe node X, the only action of r0 enables the rule rl whosework is to find such an adjacency inthe PGS by means of thee-reduction rule red(rl)=(e,~ C X').
If such a C exists rl isscheduled and applied, thus the actions of rl create a newlink between X and C (case Co)), and the rule r2 is enabled inpreparation of the third possible case where a node B isadjacent to the node X.
The actions of rl disable rl itselfbefore ending their work.
Because of the side effect of REcited above the rule r2 is always cheduled, and whenever anode B exists then it is applied.
At this point it is clear howthe mechanism works and cases (c) and (d) are handled in thesame way by the rules r2 and r3, respectively.As the example.shows, henever the rules rl ?2?3 arescheduled their task is realized in two phases.
The first phaseis the match process of the e-reduction rules.
At this stage itis like when a top-down parser searches lower-levelconstituents for expanding the higher level constituent.
Ifthis search succeeds the second phase is when thered(r0) = (X ~-- D)st(r0) = activea(r0) = iRE (rl)\]red(rl) = (el<--- C X)st(rl) = inactivea(rl) = \[Add Son_Rel (X,C); RE (r2); RD (rl)\]red(a) = B XOst(r2) = inactivea(r2) = \[Add_Son Rel (X,B); RE (r3); RD (r2)\]red(r3) -- (el ?--- A X)st(r3) = inactivea(r3) = \[Add Son_Rel (X,A); RD (r3)\]Figure 6.
The Process Grammar of the exampleXDA(a)Xrl/'NC DAACo)XB C D/x/ A(c)XA B C DAAAA(d)Figure 7.
All the possible cases of the exampleappropriate links are created by means of the actions, and theadvantage of this solution is that the search processterminates in a natural way without searching and proposinguseless relations between constituents.We terminate this Section pointing out that this sameapproach can be used in the dual case of this example, witha set P2 of productions like:P2= {X --~ A, X ---> AB, X ---> ABC, X ---> ABCD}The exercise of finding a corresponding setofPG rules is leftto the reader.6.
RELATED WORKSSome comparisons can be made with related works onthree main levels: the data structure PGS; the ProcessGrammar; the PG Processor.ThePGS can be compared with the chart (Kaplan, 1973;Kay, 1982).
The PGS embodies much of the information thechart has.
As a matter of fact, our PGS can be seen as adenotational variant of the chart, and it is managed in adifferent way by the PG Processor since in the PGS wemainly use classical relations between the nodes of theparse-trees: the dominance r lation between a parent and ason node, encoded in the non-terminal nodes; the left-adjacency relation between subtrees, encoded in theterminal nodes.
Note that if we add the fight-adjacencyrelation to the PGS we obtain astructure fully comparable tothe chart.The Process Grammar can embody many kinds ofinformation.
Its structure comes from the general structurestated for the APSG, being very close to the ATN Grammarsstructure.
On the other hand, our approach proposes thatgrammar rules contain directives relative to the control ofthe parsing process.
This is a feature not in line with thecurrent rend of keeping separate control and linguisticrestrictions expressed in a declarative way, and it can be305found in parsing systems making use of grammars based onsituation-action rules 0Vinograd, 1983); furthermore, ourway of managing rammar rules, i.e., operations on thestates, activation and scheduling mechanisms, is verysimilar to that realized in Marcus (1980).7.
D ISCUSSION AND CONCLUSIONSThe PG Processor is bottom-up based, and it has to tryto take advantage from all the available sources ofinformation which are just the input sentence and thegrammar structure.
A slrong improvement in the parsingprocess is determined by how the rules of a ProcessGrammar are organized.
Take, for instance, a grammarwhere the only active rules are e-reduction rules.
Within theactivation model they merely have to activate inactive rulesto be needed next, after having determined a proper contextfor them.
This can be extended to chains of activations atdifferent levels of context in a sentence, thus limiting bothcalls to the matcher and nodes proliferation i the PGS.
Thiscase can be represented writing (ea~offl3) ~ (A~--T),reading itas if the e-reduction in the lhs applies then activatethe rule with the reduction in the rhs, thus realizing amechanism that works as a context-sensitive reduction of theform (otA\[3~---?c#), easily extendable also to the general case=,This is not the only reason for the presence of the e-reductionrules in the Process Grammar.
Italso becomes apparent fromthe example that the e-reduction rules are a powerful toolthat, extending the context-freeness of the reduction rules,allow the realization of a wide alternative of techniques,especially when its use is combined together with KernelFunctions uch as RA getting a powerful mean for thecontrol of the parsing process.
From that, a parser driven bythe input - for the main scheduling - and both by the PGS andthe rules - for more complex phenomena - can be a validframework for solving, as much as possible, classicalproblems of efficiency such as minimal activation of rules,and minimal node generation.
Our description isimplementation-independent, it is responsive toimprovements and extensions, and a first advantage is that itcan be a valid approach for realizing efficientimplementations of the PG Processor.Extending the Process Grammar.
In this paper wehave described a Process Grammar where rules areaugmented with simple tests and actions.
An extension ofthis structure that we have not described here and that canoffer further performance to the parsing process is if weintroduce in the PG some recovery actions that are appliedwhenever the detection of one of the two possible cases ofprocess failure happens in either the match process or thetests.
Consider, for instance, the reduction rule.
Its f'mal aimis to find a process environment for the rule when scheduled.This leads to say that whenever some failure conditionshappen and a process environment cannot be provided, therecovery actions would have to manage just the control ofwhat to do next to undertake some recovery task.
It is easyto add such an extension to the PG, consequently modifyingproperly the reduction operations of the PG processor.Other extensions concern the set F~, by adding furthercontrol and process management functions.
Functions uchas RE and RD can be defined for changing the state of therules during a parsing process, thus a Process Grammar canbe partitioned in clusters of rules that can be enabled ordisabled under proper circumstances detected by qow-level'(e-reduction) rules.
Finally, there can be also somecutting functions that stop local partial parses, or even haltthe PG processor accepting or rejecting the input, e.g., whena fatal condition has been detected making the inputunparsable, the PG processor might be halted, thus avoidingthe complete parse of the sentence and even starting arecovery process.
The reader can refer to Marino (1988) andMarino (1989) for an informal description regarding theimplementation ofsuch extensions.Conclusions.
We have presented a completeframework for efficient bottom-up arsing.
Efficiency isgained by means of: a structured representation of theparsing structure, the Parse Graph Structure, that allowsefficient matching of the reduction rules; the ProcessGrammar that extends APSG by means of the process-basedconception of the grammar rules and by the presence ofKernel Functions; the PG Processor that implements a non-deterministic parser whose behaviour can be altered by theProcess Grammar increasing the determinism ofthe wholesystem.
The mechanism of rule activation that can berealized in a Process Grammar is context-sensitive-based,but this does not increase computational effort sinceprocesses involved in the activations receive their processenvironments - which are computed only once - from theactivating rules.
At present we cannot ell which degree ofdeterminism can be got, but we infer that the partition of aProcess Grammar in clusters of rules, and the driving role thee-reductions can have are two basic aspects whoseimportance should be highlighted in the future.ACKNOWLEDGMENTSThe author is thankful to Giorgio Satta who madehelpful comments and corrections on the preliminary draftof this paper.REFERENCESAho, Alfred, V. and Ullman, Jeffrey, D. (1972).
TheTheory of Parsing, Translation, and Compiling.
Volume 1:Parsing.
Prentice Hall, Englewood Cliffs, NJ.Aho, Alfred, V., Hopcroft, John, E. and Ullman, Jeffrey,D.
(1974).
The Design and Analysis of ComputerAlgorithms.
Addison-Wesley.Grishman, Ralph (1976).
A Survey of SyntacticAnalysis Procedures for Natural Language.
AmericanJournal of ComputationalLinguistics.
Microfiche 47, pp.
2-96.Kaplan, Ronald, M. (1973).
A General SyntacticProcessor.
In Randall Rustin, ed., Natural LanguageProcessing, Algodthmics Press, New York, pp.
193-241.Kay, Martin (1982).
Algorithm Schemata nd DataStructures in Syntactic Processing.
In Barbara J. Grosz,Karen Sparck Jones and Bonnie Lynn Webber, eds.,Readings in Natural Language Processing, MorganKaufmann, Los Altos, pp.
35-70.
Also CSL-80-12, XeroxPARC, Palo Alto, California.Marcus, Mitchell, P. (1980).
A Theory of SyntacticRecognition for NaturaI Language.
MIT Press, Cambridge,MA.Marino, Massimo (1988).
A Process-Activation BasedParsing Algorithm for the Development of NaturalLanguage Grammars.
Proceedings of 12th InternationalConference on Computational Linguistics.
Budapest,Hungary, pp.
390-395.Marino, Massimo (1989).
A Framework for theDevelopment of Natural Language Grammars.
Proceedingsof lnternational Workshop on Parsing Technologies.
CMU,Pittsburgh, PA, August 28-31 1989, pp.
350-360.Pratt, Vaughan, R. (1975).
LINGOL - A ProgressReport.
Proceedings of4th IJCAI, Tbilisi, Georgia, USSR,pp.
422-428.Slocum, Johnathan (1981).
A Practical Comparison ofParsing Strategies.
Proceedings of 19th ACL, Stanford,California, pp.
1-6.Winograd, Terry (1983).
Language as a CognitiveProcess.
Vol.
1: Syntax.
Addison-Wesley, Reading, MA.Wirtn, Mats (1987).
A Comparison of Rule-InvocationStrategies in Context-Free Chart Parsing.
Proceedings of3rd Conference of the European Chapter of the ACL,Copenhagen, Denmark, pp.
226-233.306
