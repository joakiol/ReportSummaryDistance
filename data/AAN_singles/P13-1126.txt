Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1285?1293,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsVector Space Model for Adaptation in Statistical Machine TranslationBoxing Chen, Roland Kuhn and George FosterNational Research Council Canadafirst.last@nrc-cnrc.gc.caAbstractThis paper proposes a new approach todomain adaptation in statistical machinetranslation (SMT) based on a vector spacemodel (VSM).
The general idea is first tocreate a vector profile for the in-domaindevelopment (?dev?)
set.
This profilemight, for instance, be a vector with a di-mensionality equal to the number of train-ing subcorpora; each entry in the vector re-flects the contribution of a particular sub-corpus to all the phrase pairs that can beextracted from the dev set.
Then, foreach phrase pair extracted from the train-ing data, we create a vector with featuresdefined in the same way, and calculate itssimilarity score with the vector represent-ing the dev set.
Thus, we obtain a de-coding feature whose value represents thephrase pair?s closeness to the dev.
This isa simple, computationally cheap form ofinstance weighting for phrase pairs.
Ex-periments on large scale NIST evaluationdata show improvements over strong base-lines: +1.8 BLEU on Arabic to Englishand +1.4 BLEU on Chinese to Englishover a non-adapted baseline, and signifi-cant improvements in most circumstancesover baselines with linear mixture modeladaptation.
An informal analysis suggeststhat VSM adaptation may help in makinga good choice among words with the samemeaning, on the basis of style and genre.1 IntroductionThe translation models of a statistical machinetranslation (SMT) system are trained on paralleldata.
Usage of language and therefore the besttranslation practice differs widely across genres,topics, and dialects, and even depends on a partic-ular author?s or publication?s style; the word ?do-main?
is often used to indicate a particular combi-nation of all these factors.
Unless there is a per-fect match between the training data domain andthe (test) domain in which the SMT system willbe used, one can often get better performance byadapting the system to the test domain.Domain adaptation is an active topic in the nat-ural language processing (NLP) research commu-nity.
Its application to SMT systems has recentlyreceived considerable attention.
Approaches thathave been tried for SMT model adaptation includemixture models, transductive learning, data selec-tion, instance weighting, and phrase sense disam-biguation, etc.Research on mixture models has consideredboth linear and log-linear mixtures.
Both werestudied in (Foster and Kuhn, 2007), which con-cluded that the best approach was to combine sub-models of the same type (for instance, severaldifferent TMs or several different LMs) linearly,while combining models of different types (for in-stance, a mixture TM with a mixture LM) log-linearly.
(Koehn and Schroeder, 2007), instead,opted for combining the sub-models directly in theSMT log-linear framework.In transductive learning, an MT system trainedon general domain data is used to translate in-domain monolingual data.
The resulting bilingualsentence pairs are then used as additional train-ing data (Ueffing et al, 2007; Chen et al, 2008;Schwenk, 2008; Bertoldi and Federico, 2009).Data selection approaches (Zhao et al, 2004;Hildebrand et al, 2005; Lu?
et al, 2007; Mooreand Lewis, 2010; Axelrod et al, 2011) search forbilingual sentence pairs that are similar to the in-domain ?dev?
data, then add them to the trainingdata.Instance weighting approaches (Matsoukas etal., 2009; Foster et al, 2010; Huang and Xiang,2010; Phillips and Brown, 2011; Sennrich, 2012)1285typically use a rich feature set to decide on weightsfor the training data, at the sentence or phrase pairlevel.
For example, a sentence from a subcorpuswhose domain is far from that of the dev set wouldtypically receive a low weight, but sentences inthis subcorpus that appear to be of a general na-ture might receive higher weights.The 2012 JHU workshop on Domain Adapta-tion for MT 1 proposed phrase sense disambigua-tion (PSD) for translation model adaptation.
Inthis approach, the context of a phrase helps thesystem to find the appropriate translation.In this paper, we propose a new instance weight-ing approach to domain adaptation based on a vec-tor space model (VSM).
As in (Foster et al, 2010),this approach works at the level of phrase pairs.However, the VSM approach is simpler and morestraightforward.
Instead of using word-based fea-tures and a computationally expensive trainingprocedure, we capture the distributional propertiesof each phrase pair directly, representing it as avector in a space which also contains a representa-tion of the dev set.
The similarity between a givenphrase pair?s vector and the dev set vector be-comes a feature for the decoder.
It rewards phrasepairs that are in some sense closer to those foundin the dev set, and punishes the rest.
In initial ex-periments, we tried three different similarity func-tions: Bhattacharyya coefficient, Jensen-Shannondivergency, and cosine measure.
They all enabledVSM adaptation to beat the non-adaptive baseline,but Bhattacharyya similarity worked best, so weadopted it for the remaining experiments.The vector space used by VSM adaptation canbe defined in various ways.
In the experimentsdescribed below, we chose a definition that mea-sures the contribution (to counts of a given phrasepair, or to counts of all phrase pairs in the devset) of each training subcorpus.
Thus, the vari-ant of VSM adaptation tested here bears a super-ficial resemblance to domain adaptation based onmixture models for TMs, as in (Foster and Kuhn,2007), in that both approaches rely on informationabout the subcorpora from which the data origi-nate.
However, a key difference is that in this pa-per we explicitly capture each phrase pair?s dis-tribution across subcorpora, and compare it to theaggregated distribution of phrase pairs in the devset.
In mixture models, a phrase pair?s distribu-1http://www.clsp.jhu.edu/workshops/archive/ws-12/groups/dasmttion across subcorpora is captured only implicitly,by probabilities that reflect the prevalence of thepair within each subcorpus.
Thus, VSM adapta-tion occurs at a much finer granularity than mix-ture model adaptation.
More fundamentally, thereis nothing about the VSM idea that obliges us todefine the vector space in terms of subcorpora.For instance, we could cluster the words in thesource language into S clusters, and the words inthe target language into T clusters.
Then, treat-ing the dev set and each phrase pair as a pair ofbags of words (a source bag and a target bag) onecould represent each as a vector of dimension S +T, with entries calculated from the counts associ-ated with the S + T clusters (in a way similar tothat described for phrase pairs below).
The (dev,phrase pair) similarity would then be independentof the subcorpora.
One can think of several otherways of defining the vector space that might yieldeven better results than those reported here.
Thus,VSM adaptation is not limited to the variant of itthat we tested in our experiments.2 Vector space model adaptationVector space models (VSMs) have been widelyapplied in many information retrieval and naturallanguage processing applications.
For instance, tocompute the sense similarity between terms, manyresearchers extract features for each term from itscontext in a corpus, define a VSM and then ap-ply similarity functions (Hindle, 1990; Lund andBurgess, 1996; Lin, 1998; Turney, 2001).In our experiments, we exploited the fact thatthe training data come from a set of subcorpora.For instance, the Chinese-English training data aremade up of 14 subcorpora (see section 3 below).Suppose we have C subcorpora.
The domain vec-tor for a phrase-pair (f, e) is defined asV (f, e) =< w1(f, e), ...wi(f, e), ..., wC(f, e) >,(1)where wi(f, e) is a standard tf ?
idf weight, i.e.wi(f, e) = tfi (f, e) ?
idf (f, e) .
(2)To avoid a bias towards longer corpora, we nor-malize the raw joint count ci(f, e) in the corpussi by dividing by the maximum raw count of anyphrase pair extracted in the corpus si.
Let1286tfi (f, e) =ci (f, e)max {ci (fj , ek) , (fj , ek) ?
si}.
(3)The idf (f, e) is the inverse document fre-quency: a measure of whether the phrase-pair(f, e) is common or rare across all subcorpora.
Weuse the standard formula:idf (f, e) = log( Cdf (f, e) + ?
), (4)where df(f, e) is the number of subcorpora that(f, e) appears in, and ?
is an empirically deter-mined smoothing term.For the in-domain dev set, we first run wordalignment and phrases extracting in the usual wayfor the dev set, then sum the distribution of eachphrase pair (fj , ek) extracted from the dev dataacross subcorpora to represent its domain informa-tion.
The dev vector is thusV (dev) =< w1(dev), .
.
.
, wC(dev) >, (5)wherewi(dev) =j=J?j=0k=K?k=0cdev (fj , ek)wi(fj , ek) (6)J,K are the total numbers of source/targetphrases extracted from the dev data respectively.cdev (fj , ek) is the joint count of phrase pair fj , ekfound in the dev set.The vector can also be built with other featuresof the phrase pair.
For instance, we could replacethe raw joint count ci(f, e) in Equation 3 with theraw marginal count of phrase pairs (f, e).
There-fore, even within the variant of VSM adaptationwe focus on in this paper, where the definition ofthe vector space is based on the existence of sub-corpora, one could utilize other definitions of thevectors of the similarity function than those we uti-lized in our experiments.2.1 Vector similarity functionsVSM uses the similarity score between the vec-tor representing the in-domain dev set and the vec-tor representing each phrase pair as a decoder fea-ture.
There are many similarity functions we couldhave employed for this purpose (Cha, 2007).
Wetested three commonly-used functions: the Bhat-tacharyya coefficient (BC) (Bhattacharyya, 1943;Kazama et al, 2010), the Jensen-Shannon diver-gence (JSD), and the cosine measure.
Accordingto (Cha, 2007), these belong to three different fam-ilies of similarity functions: the Fidelity family,the Shannon?s entropy family, and the inner Prod-uct family respectively.
It was BC similarity thatyielded the best performance, and that we endedup using in subsequent experiments.To map the BC score onto a range from 0 to1, we first normalize each weight in the vector bydividing it by the sum of the weights.
Thus, we getthe probability distribution of a phrase pair or thephrase pairs in the dev data across all subcorpora:pi(f, e) =wi(f, e)?j=Cj=1 wj(f, e)(7)pi(dev) =wi(dev)?j=Cj=1 wj(dev)(8)To further improve the similarity score, we ap-ply absolute discounting smoothing when calcu-lating the probability distributions pi(f, e).
Wesubtract a discounting value ?
from the non-zeropi(f, e), and equally allocate the remaining proba-bility mass to the zero probabilities.
We carry outthe same smoothing for the probability distribu-tions pi(dev).
The smoothing constant ?
is deter-mined empirically on held-out data.The Bhattacharyya coefficient (BC) is definedas follows:BC(dev; f, e) =i=C?i=0?pi(dev) ?
pi(f, e) (9)The other two similarity functions we alsotested are JSD and cosine (Cos).
They are definedas follows:JSD(dev; f, e) = (10)12[i=C?i=1pi(dev) log2pi(dev)pi(dev) + pi(f, e)+i=C?i=1pi(f, e) log2pi(f, e)pi(dev) + pi(f, e)]Cos(dev; f, e) =?i pi(dev) ?
pi (f, e)?
?i p2i (dev)?
?i p2i (f, e)(11)1287corpus # segs # en tok % genresfbis 250K 10.5M 3.7 nwfinancial 90K 2.5M 0.9 fingale bc 79K 1.3M 0.5 bcgale bn 75K 1.8M 0.6 bn nggale nw 25K 696K 0.2 nwgale wl 24K 596K 0.2 wlhkh 1.3M 39.5M 14.0 hanshkl 400K 9.3M 3.3 legalhkn 702K 16.6M 5.9 nwisi 558K 18.0M 6.4 nwlex&ne 1.3M 2.0M 0.7 lexother nw 146K 5.2M 1.8 nwsinorama 282K 10.0M 3.5 nwun 5.0M 164M 58.2 unTOTAL 10.1M 283M 100.0 (all)devtesttune 1,506 161K nw wlNIST06 1,664 189K nw bngNIST08 1,357 164K nw wlTable 1: NIST Chinese-English data.
In thegenres column: nw=newswire, bc=broadcastconversation, bn=broadcast news, wl=weblog,ng=newsgroup, un=UN proc., bng = bn & ng.3 Experiments3.1 Data settingWe carried out experiments in two different set-tings, both involving data from NIST Open MT2012.2 The first setting is based on data fromthe Chinese to English constrained track, compris-ing about 283 million English running words.
Wemanually grouped the training data into 14 corporaaccording to genre and origin.
Table 1 summa-rizes information about the training, developmentand test sets; we show the sizes of the training sub-corpora in number of words as a percentage of alltraining data.
Most training subcorpora consist ofparallel sentence pairs.
The isi and lex&ne cor-pora are exceptions: the former is extracted fromcomparable data, while the latter is a lexicon thatincludes many named entities.
The developmentset (tune) was taken from the NIST 2005 evalua-tion set, augmented with some web-genre materialreserved from other NIST corpora.The second setting uses NIST 2012 Arabic toEnglish data, but excludes the UN data.
There areabout 47.8 million English running words in these2http://www.nist.gov/itl/iad/mig/openmt12.cfmcorpus # segs # en toks % gengale bc 57K 1.6M 3.3 bcgale bn 45K 1.2M 2.5 bngale ng 21K 491K 1.0 nggale nw 17K 659K 1.4 nwgale wl 24K 590K 1.2 wlisi 1,124K 34.7M 72.6 nwother nw 224K 8.7M 18.2 nwTOTAL 1,512K 47.8M 100.0 (all)devtestNIST06 1,664 202K nwlNIST08 1,360 205K nwlNIST09 1,313 187K nwlTable 2: NIST Arabic-English data.
In the gen(genres) column: nw=newswire, bc=broadcastconversation, bn=broadcast news, ng=newsgroup,wl=weblog, nwl = nw & wl.training data.
We manually grouped the trainingdata into 7 groups according to genre and origin.Table 2 summarizes information about the train-ing, development and test sets.
Note that for thislanguage pair, the comparable isi data represent alarge proportion of the training data: 72% of theEnglish words.
We use the evaluation sets fromNIST 2006, 2008, and 2009 as our developmentset and two test sets, respectively.3.2 SystemExperiments were carried out with an in-housephrase-based system similar to Moses (Koehn etal., 2007).
Each corpus was word-aligned usingIBM2, HMM, and IBM4 models, and the phrasetable was the union of phrase pairs extracted fromthese separate alignments, with a length limit of7.
The translation model (TM) was smoothed inboth directions with KN smoothing (Chen et al,2011).
We use the hierarchical lexicalized reorder-ing model (RM) (Galley and Manning, 2008), witha distortion limit of 7.
Other features include lex-ical weighting in both directions, word count, adistance-based RM, a 4-gram LM trained on thetarget side of the parallel data, and a 6-gram En-glish Gigaword LM.
The system was tuned withbatch lattice MIRA (Cherry and Foster, 2012).3.3 ResultsFor the baseline, we simply concatenate all train-ing data.
We have also compared our approachto two widely used TM domain adaptation ap-1288proaches.
One is the log-linear combinationof TMs trained on each subcorpus (Koehn andSchroeder, 2007), with weights of each modeltuned under minimal error rate training usingMIRA.
The other is a linear combination of TMstrained on each subcorpus, with the weights ofeach model learned with an EM algorithm to max-imize the likelihood of joint empirical phrase paircounts for in-domain dev data.
For details, refer to(Foster and Kuhn, 2007).The value of ?
and ?
(see Eq 4 and Section 2.1)are determined by the performance on the devset of the Arabic-to-English system.
For bothArabic-to-English and Chinese-to-English exper-iment, these values obtained on Arabic dev wereused to obtain the results below: ?
was set to 8,and ?
was set to 0.01.
(Later, we ran an exper-iment on Chinese-to-English with ?
and ?
tunedspecifically for that language pair, but the perfor-mance for the Chinese-English system only im-proved by a tiny, insignificant amount).Our metric is case-insensitive IBM BLEU (Pa-pineni et al, 2002), which performs matching ofn-grams up to n = 4; we report BLEU scores av-eraged across both test sets NIST06 and NIST08for Chinese; NIST08 and NIST09 for Arabic.Following (Koehn, 2004), we use the bootstrap-resampling test to do significance testing.
In ta-bles 3 to 5, * and ** denote significant gains overthe baseline at p < 0.05 and p < 0.01 levels, re-spectively.We first compare the performance of differ-ent similarity functions: cosine (COS), Jensen-Shannon divergence (JSD) and Bhattacharyya co-efficient (BC).
The results are shown in Table 3.All three functions obtained improvements.
BothCOS and BC yield statistically significant im-provements over the baseline, with BC performingbetter than COS by a further statistically signifi-cant margin.
The Bhattacharyya coefficient is ex-plicitly designed to measure the overlap betweenthe probability distributions of two statistical sam-ples or populations, which is precisely what we aretrying to do here: we are trying to reward phrasepairs whose distribution is similar to that of thedev set.
Thus, its superior performance in theseexperiments is not unexpected.In the next set of experiments, we comparedVSM adaptation using the BC similarity functionwith the baseline which concatenates all trainingdata and with log-linear and linear TM mixturessystem Chinese Arabicbaseline 31.7 46.8COS 32.3* 47.8**JSD 32.1 47.1BC 33.0** 48.4**Table 3: Comparison of different similarity func-tions.
* and ** denote significant gains over thebaseline at p < 0.05 and p < 0.01 levels, respec-tively.system Chinese Arabicbaseline 31.7 46.8loglinear tm 28.4 44.5linear tm 32.7** 47.5**vsm, BC 33.0** 48.4**Table 4: Results for variants of adaptation.whose components are based on subcorpora.
Ta-ble 4 shows that log-linear combination performsworse than the baseline: the tuning algorithmfailed to optimize the log-linear combination evenon dev set.
For Chinese, the BLEU score of thedev set on the baseline system is 27.3, while onthe log-linear combination system, it is 24.0; forArabic, the BLEU score of the dev set on the base-line system is 46.8, while on the log-linear com-bination system, it is 45.4.
We also tried addingthe global model to the loglinear combination andit didn?t improve over the baseline for either lan-guage pair.
Linear mixture was significantly betterthan the baseline at the p < 0.01 level for both lan-guage pairs.
Since our approach, VSM, performedbetter than the linear mixture for both pairs, it is ofcourse also significantly better than the baseline atthe p < 0.01 level.This raises the question: is VSM performancesignificantly better than that of a linear mixture ofTMs?
The answer (not shown in the table) is thatfor Arabic to English, VSM performance is bet-ter than linear mixture at the p < 0.01 level.
ForChinese to English, the argument for the superi-ority of VSM over linear mixture is less convinc-ing: there is significance at the p < 0.05 for oneof the two test sets (NIST06) but not for the other(NIST08).
At any rate, these results establish thatVSM adaptation is clearly superior to linear mix-ture TM adaptation, for one of the two languagepairs.In Table 4, the VSM results are based on the1289system Chinese Arabicbaseline 31.7 46.8linear tm 32.7** 47.5**vsm, joint 33.0** 48.4**vsm, src-marginal 32.2* 47.3*vsm, tgt-marginal 32.6** 47.6**vsm, src+tgt (2 feat.)
32.7** 48.2**vsm, joint+src (2 feat.)
32.9** 48.4**vsm, joint+tgt (2 feat.)
32.9** 48.4**vsm, joint+src+tgt (3 feat.)
33.1** 48.6**Table 5: Results for adaptation based on joint ormaginal counts.vector of the joint counts of the phrase pair.
Inthe next experiment, we replace the joint countswith the source or target marginal counts.
In Ta-ble 5, we first show the results based on sourceand target marginal counts, then the results of us-ing feature sets drawn from three decoder VSMfeatures: a joint count feature, a source marginalcount feature, and a target marginal count fea-ture.
For instance, the last row shows the resultswhen all three features are used (with their weightstuned by MIRA).
It looks as though the source andtarget marginal counts contain useful information.The best performance is obtained by combining allthree sources of information.
The 3-feature ver-sion of VSM yields +1.8 BLEU over the baselinefor Arabic to English, and +1.4 BLEU for Chineseto English.When we compared two sets of results in Ta-ble 4, the joint count version of VSM and lin-ear mixture of TMs, we found that for Arabic toEnglish, VSM performance is better than linearmixture at the p < 0.01 level; the Chinese toEnglish significance test was inconclusive (VSMfound to be superior to linear mixture at p < 0.05for NIST06 but not for NIST08).
We now havesomewhat better results for the 3-feature versionof VSM shown in Table 5.
How do these new re-sults affect the VSM vs. linear mixture compari-son?
Naturally, the conclusions for Arabic don?tchange.
For Chinese, 3-feature VSM is now su-perior to linear mixture at p < 0.01 on NIST06test set, but 3-feature VSM still doesn?t have a sta-tistically significant edge over linear mixture onNIST08 test set.
A fair summary would be that 3-feature VSM adaptation is decisively superior tolinear mixture adaptation for Arabic to English,and highly competitive with linear mixture adap-tation for Chinese to English.Our last set of experiments examined the ques-tion: when added to a system that already hassome form of linear mixture model adaptation,does VSM improve performance?
In (Foster andKuhn, 2007), two kinds of linear mixture were de-scribed: linear mixture of language models (LMs),and linear mixture of translation models (TMs).Some of the results reported above involved lin-ear TM mixtures, but none of them involved lin-ear LM mixtures.
Table 6 shows the results ofdifferent combinations of VSM and mixture mod-els.
* and ** denote significant gains over the rowno vsm at p < 0.05 and p < 0.01 levels, re-spectively.
This means that in the table, the base-line within each box containing three results is thetopmost result in the box.
For instance, with aninitial Chinese system that employs linear mixtureLM adaptation (lin-lm) and has a BLEU of 32.1,adding 1-feature VSM adaptation (+vsm, joint)improves performance to 33.1 (improvement sig-nificant at p < 0.01), while adding 3-feature VSMinstead (+vsm, 3 feat.)
improves performance to33.2 (also significant at p < 0.01).
For Arabic, in-cluding either form of VSM adaptation always im-proves performance with significance at p < 0.01,even over a system including both linear TM andlinear LM adaptation.
For Chinese, adding VSMstill always yields an improvement, but the im-provement is not significant if linear TM adapta-tion is already in the system.
These results showthat combining VSM adaptation and either or bothkinds of linear mixture adaptation never hurts per-formance, and often improves it by a significantamount.3.4 Informal Data AnalysisTo get an intuition for how VSM adaptation im-proves BLEU scores, we compared outputs fromthe baseline and VSM-adapted system (?vsm,joint?
in Table 5) on the Chinese test data.
Wefocused on examples where the two systems hadtranslated the same source-language (Chinese)phrase s differently, and where the target-language(English) translation of s chosen by the VSM-adapted system, tV , had a higher Bhattacharyyascore for similarity with the dev set than did thephrase that was chosen by the baseline system, tB .Thus, we ignored differences in the two transla-tions that might have been due to the secondaryeffects of VSM adaptation (such as a different tar-1290no-lin-adap lin-lm lin-tm lin-lm+lin-tmno vsm 31.7 32.1 32.7 33.1Chinese +vsm, joint 33.0** 33.1** 33.0 33.3+vsm, 3 feat.
33.1** 33.2** 33.1 33.4no vsm 46.8 47.0 47.5 47.7Arabic +vsm, joint 48.4** 48.7** 48.6** 48.8**+vsm, 3 feat.
48.6** 48.8** 48.7** 48.9**Table 6: Results of combining VSM and linear mixture adaptation.
?lin-lm?
is linear language modeladaptation, ?lin-tm?
is linear translation model adaptation.
* and ** denote significant gains over the row?no vsm?
at p < 0.05 and p < 0.01 levels, respectively.get phrase being preferred by the language modelin the VSM-adapted system from the one preferredin the baseline system because of a Bhattacharyya-mediated change in the phrase preceding it).An interesting pattern soon emerged: the VSM-adapted system seems to be better than the base-line at choosing among synonyms in a way that isappropriate to the genre or style of a text.
For in-stance, where the text to be translated is from aninformal genre such as weblog, the VSM-adaptedsystem will often pick an informal word where thebaseline picks a formal word with the same or sim-ilar meaning, and vice versa where the text to betranslated is from a more formal genre.
To oursurprise, we saw few examples where the VSM-adapted system did a better job than the baseline ofchoosing between two words with different mean-ing, but we saw many examples where the VSM-adapted system did a better job than the baselineof choosing between two words that both have thesame meaning according to considerations of styleand genre.Two examples are shown in Table 7.
In thefirst example, the first two lines show that VSMfinds that the Chinese-English phrase pair (?
?,assaulted) has a Bhattacharyya (BC) similarity of0.556163 to the dev set, while the phrase pair (?
?, beat) has a BC similarity of 0.780787 to thedev.
In this situation, the VSM-adapted systemthus prefers ?beat?
to ?assaulted?
as a translationfor ??.
The next four lines show the sourcesentence (SRC), the reference (REF), the baselineoutput (BSL), and the output of the VSM-adaptedsystem.
Note that the result of VSM adaptation isthat the rather formal word ?assaulted?
is replacedby its informal near-synonym ?beat?
in the trans-lation of an informal weblog text.?apprehend?
might be preferable to ?arrest?
ina legal text.
However, it looks as though theVSM-adapted system has learned from the devthat among synonyms, those more characteristicof news stories than of legal texts should be cho-sen: it therefore picks ?arrest?
over its synonym?apprehend?.What follows is a partial list of pairs of phrases(all single words) from our system?s outputs,where the baseline chose the first member of a pairand the VSM-adapted system chose the secondmember of the pair to translate the same Chinesephrase into English (because the second wordyields a better BC score for the dev set we used).It will be seen that nearly all of the pairs involvesynonyms or near-synonyms rather than wordswith radically different senses (one exceptionbelow is ?center?
vs ?heart?).
Instead, the differ-ences between the two words tend to be related togenre or style: gunmen-mobsters, champion-star,updated-latest, caricatures-cartoons, spill-leakage,hiv-aids, inkling-clues, behaviour-actions, deceit-trick, brazen-shameless, aristocratic-noble,circumvent-avoid, attack-criticized, descent-born,hasten-quickly, precipice-cliff, center-heart,blessing-approval, imminent-approaching,stormed-rushed, etc.4 Conclusions and future workThis paper proposed a new approach to domainadaptation in statistical machine translation, basedon vector space models (VSMs).
This approachmeasures the similarity between a vector repre-senting a particular phrase pair in the phrase ta-ble and a vector representing the dev set, yield-ing a feature associated with that phrase pair thatwill be used by the decoder.
The approach issimple, easy to implement, and computationallycheap.
For the two language pairs we lookedat, it provided a large performance improvementover a non-adaptive baseline, and also compared12911 phrase ???
assaulted (0.556163)pairs ???
beat (0.780787)SRC ...??????????
?...REF ... those local ruffians and hooligans who beat up villagers ...BSL ... those who assaulted the villagers land hooligans ...VSM ... hooligans who beat the villagers ...2 phrase ???
apprehend (0.286533)pairs ???
arrest (0.603342)SRC ...
????????????
?REF ... catch the killers and bring them to justice .BSL ... apprehend the perpetrators and bring them to justice .VSM ... arrest the perpetrators and bring them to justice .Table 7: Examples show that VSM chooses translations according to considerations of style and genre.favourably with linear mixture adaptation tech-niques.Furthermore, VSM adaptation can be exploitedin a number of different ways, which we have onlybegun to explore.
In our experiments, we basedthe vector space on subcorpora defined by the na-ture of the training data.
This was done purelyout of convenience: there are many, many ways todefine a vector space in this situation.
An obvi-ous and appealing one, which we intend to try infuture, is a vector space based on a bag-of-wordstopic model.
A feature derived from this topic-related vector space might complement some fea-tures derived from the subcorpora which we ex-plored in the experiments above, and which seemto exploit information related to genre and style.ReferencesAmittai Axelrod, Xiaodong He, and Jianfeng Gao.2011.
Domain adaptation via pseudo in-domain dataselection.
In EMNLP 2011.Nicola Bertoldi and Marcello Federico.
2009.
Do-main adaptation for statistical machine translationwith monolingual resources.
In Proceedings of the4th Workshop on Statistical Machine Translation,Athens, March.
WMT.A.
Bhattacharyya.
1943.
On a measure of divergencebetween two statistical populations defined by theirprobability distributions.
Bulletin of the CalcuttaMathematical Society, 35:99?109.Sung-Hyuk Cha.
2007.
Comprehensive survey on dis-tance/similarity measures between probability den-sity functions.
International Journal of Mathe-matical Models ind Methods in Applied Sciences,1(4):300?307.Boxing Chen, Min Zhang, Aiti Aw, and Haizhou Li.2008.
Exploiting n-best hypotheses for smt self-enhancement.
In ACL 2008.Boxing Chen, Roland Kuhn, George Foster, andHoward Johnson.
2011.
Unpacking and transform-ing feature functions: New ways to smooth phrasetables.
In MT Summit 2011.Colin Cherry and George Foster.
2012.
Batch tun-ing strategies for statistical machine translation.
InNAACL 2012.George Foster and Roland Kuhn.
2007.
Mixture-model adaptation for SMT.
In Proceedings of theACL Workshop on Statistical Machine Translation,Prague, June.
WMT.George Foster, Cyril Goutte, and Roland Kuhn.
2010.Discriminative instance weighting for domain adap-tation in statistical machine translation.
In Proceed-ings of the 2010 Conference on Empirical Methodsin Natural Language Processing (EMNLP), Boston.Michel Galley and C. D. Manning.
2008.
A simpleand effective hierarchical phrase reordering model.In EMNLP 2008, pages 848?856, Hawaii, October.Almut Silja Hildebrand, Matthias Eck, Stephan Vogel,and Alex Waibel.
2005.
Adaptation of the transla-tion model for statistical machine translation basedon information retrieval.
In Proceedings of the 10thEAMT Conference, Budapest, May.Donald Hindle.
1990.
Noun classification from predi-cate.argument structures.
In Proceedings of the 28thAnnual Meeting of the Association for Computa-tional Linguistics (ACL), pages 268?275, Pittsburgh,PA, June.
ACL.Fei Huang and Bing Xiang.
2010.
Feature-rich dis-criminative phrase rescoring for SMT.
In COLING2010.Jun?ichi Kazama, Stijn De Saeger, Kow Kuroda,Masaki Murata, and Kentaro Torisawa.
2010.
A1292bayesian method for robust estimation of distribu-tional similarities.
In Proceedings of the 48th An-nual Meeting of the Association for ComputationalLinguistics (ACL), pages 247?256, Uppsala, Swe-den, July.
ACL.Philipp Koehn and Josh Schroeder.
2007.
Experi-ments in domain adaptation for statistical machinetranslation.
In Proceedings of the Second Workshopon Statistical Machine Translation, pages 224?227,Prague, Czech Republic, June.
Association for Com-putational Linguistics.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,and E. Herbst.
2007.
Moses: Open source toolkitfor statistical machine translation.
In ACL 2007,Demonstration Session.P.
Koehn.
2004.
Statistical significance tests for ma-chine translation evaluation.
In Proceedings of the2004 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), Barcelona, Spain.Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
In Proceedings of COLING/ACL-98, pages 768?774, Montreal, Quebec, Canada.Yajuan Lu?, Jin Huang, and Qun Liu.
2007.
Improv-ing Statistical Machine Translation Performance byTraining Data Selection and Optimization.
In Pro-ceedings of the 2007 Conference on Empirical Meth-ods in Natural Language Processing (EMNLP),Prague, Czech Republic.K.
Lund and C. Burgess.
1996.
Producinghigh-dimensional semantic spaces from lexical co-occurrence.
Behavior Research Methods Instru-ments and Computers, 28(2):203?208.Spyros Matsoukas, Antti-Veikko I. Rosti, and BingZhang.
2009.
Discriminative corpus weight estima-tion for machine translation.
In Proceedings of the2009 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), Singapore.Robert C. Moore and William Lewis.
2010.
Intelligentselection of language model training data.
In ACL2010.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A method for automaticevaluation of Machine Translation.
In Proceedingsof the 40th Annual Meeting of the Association forComputational Linguistics (ACL), pages 311?318,Philadelphia, July.
ACL.Aaron B. Phillips and Ralf D. Brown.
2011.
Train-ing machine translation with a second-order taylorapproximation of weighted translation instances.
InMT Summit 2011.Holger Schwenk.
2008.
Investigations on large-scale lightly-supervised training for statistical ma-chine translation.
In IWSLT 2008.Rico Sennrich.
2012.
Perplexity minimization fortranslation model domain adaptation in statisticalmachine translation.
In EACL 2012.Peter Turney.
2001.
Mining the web for synonyms:Pmi-ir versus lsa on toefl.
In Twelfth EuropeanConference on Machine Learning, page 491?502,Berlin, Germany.Nicola Ueffing, Gholamreza Haffari, and AnoopSarkar.
2007.
Transductive learning for statisticalmachine translation.
In Proceedings of the 45th An-nual Meeting of the Association for ComputationalLinguistics (ACL), Prague, Czech Republic, June.ACL.Bing Zhao, Matthias Eck, and Stephan Vogel.
2004.Language model adaptation for statistical machinetranslation with structured query models.
In Pro-ceedings of the International Conference on Compu-tational Linguistics (COLING) 2004, Geneva, Au-gust.1293
