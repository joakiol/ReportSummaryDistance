Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 640?649,Honolulu, October 2008. c?2008 Association for Computational LinguisticsUnsupervised Models for Coreference ResolutionVincent NgHuman Language Technology Research InstituteUniversity of Texas at DallasRichardson, TX 75083-0688vince@hlt.utdallas.eduAbstractWe present a generative model for unsuper-vised coreference resolution that views coref-erence as an EM clustering process.
Forcomparison purposes, we revisit Haghighiand Klein?s (2007) fully-generative Bayesianmodel for unsupervised coreference resolu-tion, discuss its potential weaknesses and con-sequently propose three modifications to theirmodel.
Experimental results on the ACE datasets show that our model outperforms theiroriginal model by a large margin and com-pares favorably to the modified model.1 IntroductionCoreference resolution is the problem of identifyingwhich mentions (i.e., noun phrases) refer to whichreal-world entities.
The availability of annotatedcoreference corpora produced as a result of the MUCconferences and the ACE evaluations has promptedthe development of a variety of supervised machinelearning approaches to coreference resolution in re-cent years.
The focus of learning-based coreferenceresearch has also shifted from the acquisition of apairwise model that determines whether two men-tions are co-referring (e.g., Soon et al (2001), Ngand Cardie (2002), Yang et al (2003)) to the de-velopment of rich linguistic features (e.g., Ji et al(2005), Ponzetto and Strube (2006)) and the ex-ploitation of advanced techniques that involve jointlearning (e.g., Daume?
III and Marcu (2005)) andjoint inference (e.g., Denis and Baldridge (2007))for coreference resolution and a related extractiontask.
The rich features, coupled with the increasedcomplexity of coreference models, have made thesesupervised approaches more dependent on labeleddata and less applicable to languages for which lit-tle or no annotated data exists.
Given the growingimportance of multi-lingual processing in the NLPcommunity, however, the development of unsuper-vised and weakly supervised approaches for the au-tomatic processing of resource-scarce languages hasbecome more important than ever.In fact, several popular weakly supervised learn-ing algorithms such as self-training, co-training(Blum and Mitchell, 1998), and EM (Dempster etal., 1977) have been applied to coreference resolu-tion (Ng and Cardie, 2003) and the related task ofpronoun resolution (Mu?ller et al, 2002; Kehler etal., 2004; Cherry and Bergsma, 2005).
Given a smallnumber of coreference-annotated documents and alarge number of unlabeled documents, these weaklysupervised learners aim to incrementally augmentthe labeled data by iteratively training a classifier1on the labeled data and using it to label mentionpairs randomly drawn from the unlabeled documentsas COREFERENT or NOT COREFERENT.
However,classifying mention pairs using such iterative ap-proaches is undesirable for coreference resolution:since the non-coreferent mention pairs significantlyoutnumber their coreferent counterparts, the result-ing classifiers generally have an increasing tendencyto (mis)label a pair as non-coreferent as bootstrap-ping progresses (see Ng and Cardie (2003)).Motivated in part by these results, we present agenerative, unsupervised model for probabilistically1For co-training, a pair of view classifiers are trained; andfor EM, a generative model is trained instead.640inducing coreference partitions on unlabeled doc-uments, rather than classifying mention pairs, viaEM clustering (Section 2).
In fact, our model com-bines the best of two worlds: it operates at thedocument level, while exploiting essential linguisticconstraints on coreferent mentions (e.g., gender andnumber agreement) provided by traditional pairwiseclassification models.For comparison purposes, we revisit a fully-generative Bayesian model for unsupervised coref-erence resolution recently introduced by Haghighiand Klein (2007), discuss its potential weaknessesand consequently propose three modifications totheir model (Section 3).
Experimental results on theACE data sets show that our model outperforms theiroriginal model by a large margin and compares fa-vorably to the modified model (Section 4).2 Coreference as EM ClusteringIn this section, we will explain how we recast un-supervised coreference resolution as EM clustering.We begin by introducing some of the definitions andnotations that we will use in this paper.2.1 Definitions and NotationsA mention can be a pronoun, a name (i.e., a propernoun), or a nominal (i.e., a common noun).
An en-tity is a set of coreferent mentions.
Given a docu-ment D consisting of n mentions, m1, .
.
.
,mn, weuse Pairs(D) to denote the set of(n2)mention pairs,{mij | 1 ?
i < j ?
n}, where mij is formedfrom mentions mi and mj .
The pairwise probabil-ity formed from mi and mj refers to the probabil-ity that the pair mij is coreferent and is denoted asPcoref (mij).
A clustering of n mentions is an n xn Boolean matrix C , where Cij (the (i,j)-th entry ofC) is 1 if and only if mentions mi and mj are coref-erent.
An entry in C is relevant if it correspondsto a mention pair in Pairs(D).
A valid clusteringis a clustering in which the relevant entries satisfythe transitivity constraint.
In other words, C is validif and only if (Cij = 1 ?
Cjk = 1) =?
Cik = 1?
1 ?
i < j < k ?
n. Hence, a valid clusteringcorresponds to a partition of a given set of mentions,and the goal of coreference resolution is to producea valid clustering in which each cluster correspondsto a distinct entity.2.2 The ModelAs mentioned previously, our generative model op-erates at the document level, inducing a valid clus-tering on a given document D. More specifically,our model consists of two steps.
It first chooses aclustering C based on some clustering distributionP (C), and then generates D given C:P (D,C) = P (C)P (D | C).To facilitate the incorporation of linguistic con-straints defined on a pair of mentions, we representD by its mention pairs, Pairs(D).
Now, assumingthat these mention pairs are generated conditionallyindependently of each other given Cij ,P (D | C) =?mij?Pairs(D)P (mij | Cij).Next, we represent mij as a set of seven featuresthat is potentially useful for determining whether miand mj are coreferent (see Table 1).2 Hence, we canrewrite P (D | C) as?mij?Pairs(D)P (m1ij , .
.
.
,m7ij | Cij),where mkij is the value of the kth feature of mij .To reduce data sparseness and improve the es-timation of the above probabilities, we make con-ditional independence assumptions about the gen-eration of these feature values.
Specifically, asshown in the first column of Table 1, we di-vide the seven features into three groups (namely,strong coreference indicators, linguistic constraints,and mention types), assuming that two featurevalues are conditionally independent if and onlyif the corresponding features belong to differ-ent groups.
With this assumption, we can de-compose P (m1ij , .
.
.m7ij | Cij) into a productof three probabilities: P (m1ij,m2ij ,m3ij | Cij),P (m4ij ,m5ij,m6ij | Cij), and P (m7ij | Cij).
Each ofthese distributions represents a pair of multinomialdistributions, one for the coreferent mention pairs(Cij = 1) and the other for the non-coreferent men-tion pairs (Cij = 0).
Hence, the set of parametersof our model, ?, consists of P (m1,m2,m3 | c),P (m4,m5,m6 | c), and P (m7 | c).2See Soon et al (2001) for details on feature value compu-tations.
Note that all feature values are computed automatically.641Feature Type Feature ID Feature DescriptionStrong 1 STR MATCH T if neither of the two mentions is a pronoun and after discarding determiners,Coreference the string denoting mention mi is identical to that of mention mj ; else F.Indicators 2 ALIAS T if one mention is an acronym, an abbreviation, or a name variant of theother; else F. For instance, Bill Clinton and President Clinton are aliases, soare MIT and Massachusetts Institute of Technology.3 APPOSITIVE T if the mentions are in an appositive relationship; else F.Linguistic 4 GENDER T if the mentions agree in gender; F if they disagree; NA if gender informationConstraints for one or both mentions cannot be determined.5 NUMBER T if the mentions agree in number; F if they disagree; NA if number informa-tion for one or both mentions cannot be determined.6 SEM CLASS T if the mentions have the same semantic class; F if they don?t; NA if thesemantic class information for one or both mentions cannot be determined.Mention Types 7 NPTYPE the feature value is the concatenation of the mention type of the two mentions,titj , where ti, tj ?
{ PRONOUN, NAME, NOMINAL }.Table 1: Feature set for representing a mention pair.
The first six features are relational features that test whether someproperty P holds for the mention pair under consideration and indicate whether the mention pair is TRUE or FALSEw.r.t.
P; a value of NOT APPLICABLE is used when property P does not apply.2.3 The Induction AlgorithmTo induce a clustering C on a document D, we runEM on our model, treating D as observed data andC as hidden data.
Specifically, we use EM to itera-tively estimate the model parameters, ?, from doc-uments that are probabilistically labeled (with clus-terings) and apply the resulting model to probabilis-tically re-label a document (with clusterings).
Moreformally, we employ the following EM algorithm:E-step: Compute the posterior probabilities of theclusterings, P (C|D,?
), based on the current ?.M-step: Using P (C|D,?)
computed in the E-step,find the ??
that maximizes the expected completelog likelihood,?C P (C|D,?)
log P (D,C|??
).We begin the induction process at the M-step.3 Tofind the ?
that maximizes the expected complete loglikelihood, we use maximum likelihood estimationwith add-one smoothing.
Since P (C|D,?)
is notavailable in the first EM iteration, we instead usean initial distribution over clusterings, P (C).
Thequestion, then, is: which P (C) should we use?
Onepossibility is the uniform distribution over all (possi-bly invalid) clusterings.
Another, presumably better,choice is a distribution that assigns non-zero prob-ability mass to only the valid clusterings.
Yet an-other possibility is to set P (C) based on a docu-ment labeled with coreference information.
In ourexperiments, we employ this last method, assigning3Another possibility, of course, is to begin at the E-step bymaking an initial guess at ?.a probability of one to the correct clustering of thelabeled document (see Section 4.1 for details).After (re-)estimating ?
in the M-step, we proceedto the E-step, where the goal is to find the condi-tional clustering probabilities.
Given a documentD, the number of coreference clusterings is expo-nential in the number of mentions in D, even ifwe limit our attention to those that are valid.
Tocope with this computational complexity, we ap-proximate the E-step by computing only the condi-tional probabilities that correspond to the N mostprobable coreference clusterings given the current?.
We identify the N most probable clusterings andcompute their probabilities as follows.
First, usingthe current ?, we reverse the generative model andcompute Pcoref (mij) for each mention pair mij inPairs(D).
Next, using these pairwise probabilities,we apply Luo et al?s (2004) Bell tree approach tocoreference resolution to compute the N -best clus-terings and their probabilities (see Section 2.4 fordetails).
Finally, to obtain the required conditionalclustering probabilities for the E-step, we normalizethe probabilities assigned to the N -best clusteringsso that they sum to one.2.4 Computing the N-Best PartitionsAs described above, given the pairwise probabilities,we use Luo et al?s (2004) algorithm to heuristicallycompute the N -best clusterings (or, more precisely,N -best partitions4) and their probabilities based on4Note that Luo et al?s search algorithm only produces validclusterings, implying that the resulting N -best clusterings are642Input: M = {m1, ..., mn}: mentions, N : no.
of best partitionsOutput: N -best partitions1: // initialize the data structures that store partial partitions2: H1 := {PP := {[m1]}}, S(PP ) = 13: H2, ..., Hn = ?4: for i = 2 to n5: // process each partial partition6: foreach PP ?
Hi?17: // process each cluster in PP8: foreach C ?
PP9: Extend PP to PP ?
by linking mi to C10: Compute S(PP ?
)11: Hi := Hi?
{PP ?
}12: Extend PP to PP ?
by putting mi into a new cluster13: Compute S(PP ?
)14: Hi := Hi?
{PP ?
}15: return N most probable partitions in HnFigure 1: Our implementation of Luo et al?s algorithmthe Bell tree.
Informally, each node in a Bell treecorresponds to an ith-order partial partition (i.e., apartition of the first i mentions of the given docu-ment), and the ith level of the tree contains all possi-ble ith-order partial partitions.
Hence, the set of leafnodes constitutes all possible partitions of all of thementions.
The search for the N most probable parti-tions starts at the root, and a partitioning of the men-tions is incrementally constructed as we move downthe tree.
Since an exhaustive search is computation-ally infeasible, Luo et al employ a beam search pro-cedure to explore only the most probable paths ateach step of the search process.
Figure 1 shows ourimplementation of this heuristic search algorithm.The algorithm takes as input a set of n mentions(and their pairwise probabilities), and returns the Nmost probable partitionings of the mentions.
It usesdata structures S and the Hi?s to store intermediateresults.
Specifically, S(PP ) stores the score of thepartial partition PP .
Hi is associated with the ithlevel of the Bell tree, and is used to store the mostprobable ith-order partial partitions.
Each Hi has amaximum size of 2N : if more than 2N partitionsare inserted into a given Hi, then only the 2N mostprobable ones will be stored.
This amounts to prun-ing the search space by employing a beam size of2N (i.e., expanding only the 2N most probable par-tial partitions) at each step of the search.The algorithm begins by initializing H1 with theonly partial partition of order one, {[m1]}, whichindeed partitions.
This is desirable, as there is no reason for usto put non-zero probability mass on invalid clusterings.has a score of one (line 2).
Then it processes thementions sequentially, starting with m2 (line 4).When processing mi, it takes each partial partitionPP in Hi?1 and creates a set of ith-order parti-tions by extending PP with mi in all possible ways.Specifically, for each cluster C (formed by a subsetof the first i?1 mentions) in PP , the algorithm gen-erates a new ith-order partition, PP ?, by linking mito C (line 9), and stores PP ?
in Hi (line 11).
Thescore of PP ?, S(PP ?
), is computed by using thepairwise coreference probabilities as follows:S(PP ?)
= S(PP ) ?
maxmk?CPcoref (mki).Of course, PP can also be extended by putting miinto a new cluster (line 12).
This yields PP ?, an-other partition to be inserted into Hi (line 14), andS(PP ?)
= ?
?S(PP )?(1?
maxk?
{1,...,i?1}Pcoref (mki)),where ?
(the start penalty) is a positive constant (<1) used to penalize partitions that start a new clus-ter.
After processing each of the n mentions usingthe above steps, the algorithm returns the N mostprobable partitions in Hn (line 15).Our implementation of Luo et al?s search algo-rithm differs from their original algorithm only interms of the number of pruning strategies adopted.Specifically, Luo et al introduce a number of heuris-tics to prune the search space in order to speed up thesearch.
We employ only the beam search heuristic,with a beam size that is five times larger than theirs.Our larger beam size, together with the fact that wedo not use other pruning strategies, implies that weare searching through a larger part of the space thanthem, thus potentially yielding better partitions.3 Haghighi and Klein?s Coreference ModelTo gauge the performance of our model, we com-pare it with a Bayesian model for unsupervisedcoreference resolution that was recently proposed byHaghighi and Klein (2007).
In this section, we willgive an overview of their model, discuss its weak-nesses and propose three modifications to the model.3.1 NotationsFor consistency, we follow Haghighi and Klein?s(H&K) notations.
Z is the set of random variables643that refer to (indices of) entities.
?z is the set ofparameters associated with entity z. ?
is the entireset of model parameters, which includes all the ?z?s.Finally, X is the set of observed variables (e.g., thehead of a mention).
Given a document, the goal isto find the most probable assignment of entity in-dices to its mentions given the observed values.
Inother words, we want to maximize P (Z|X).
In aBayesian approach, we compute this probability byintegrating out all the parameters.
Specifically,P (Z|X) =?P (Z|X, ?
)P (?|X)d?.3.2 The Original H&K ModelThe original H&K model is composed of a set ofmodels: the basic model and two other models(namely, the pronoun head model and the saliencemodel) that aim to improve the basic model.53.2.1 Basic ModelThe basic model generates a mention in a two-stepprocess.
First, an entity index is chosen according toan entity distribution, and then the head of the men-tion is generated given the entity index based on anentity-specific head distribution.
Here, we assumethat (1) all heads H are observed and (2) a mentionis represented solely by its head noun, so nothingother than the head is generated.
Furthermore, weassume that the head distribution is drawn from asymmetric Dirichlet with concentration ?H .
Hence,P (Hi,j = h|Z,H?i,j) ?
nh,z + ?Hwhere Hi,j is the head of mention j in documenti, and nh,z is the number of times head h is emit-ted by entity index z in (Z,H?i,j).6 On the otherhand, since the number of entities in a document isnot known a priori, we draw the entity distributionfrom a Dirichlet process with concentration ?, ef-fectively yielding a model with an infinite numberof mixture components.
Using the Chinese restau-rant process representation (see Teh et al (2006)),P (Zij = z|Z?i,j) ?{?
, if z = znewnz , otherwise5H&K also present a cross-document coreference model,but since it focuses primarily on cross-document coreferenceand improves within-document coreference performance byonly 1.5% in F-score, we will not consider this model here.6H?i,j is used as a shorthand for H ?
{Hi,j}.where nz is the number of mentions in Z?i,j labeledwith entity index z, and znew is a new entity indexnot already in Z?i,j.
To perform inference, we useGibbs sampling (Geman and Geman, 1984) to gen-erate samples from this conditional distribution:P (Zi,j |Z?i,j,H) ?
P (Zi,j|Z?i,j)P (Hi,j |Z,H?i,j)where the two distributions on the right are definedas above.
Starting with a random assignment of en-tity indices to mentions, the Gibbs sampler itera-tively re-samples an entity index according to thisposterior distribution given the current assignment.3.2.2 Pronoun Head ModelHead generation in the basic model is too simplis-tic: it has a strong tendency to assign the same en-tity index to mentions having the same head.
This isparticularly inappropriate for pronouns.
Hence, weneed a different model for generating pronouns.Before introducing this pronoun head model, weneed to augment the set of entity-specific param-eters, which currently contains only a distributionover heads (?hZ ).
Specifically, we add distributions?tZ , ?gZ , and ?nZ over entity properties: ?tZ is adistribution over semantic types (PER, ORG, LOC,MISC), ?gZ over gender (MALE, FEMALE, EITHER,NEUTER), and ?nZ over number (SG, PL).
We assumethat each of these distributions is drawn from a sym-metric Dirichlet.
A small concentration parameteris used, since each entity should have a dominatingvalue for each of these properties.Now, to estimate ?tZ , ?gZ , and ?nZ , we need toknow the gender, number, and semantic type of eachmention.
For some mentions (e.g., ?he?
), theseproperties are easy to compute; for others (e.g., ?it?
),they are not.
Whenever a mention has unobservedproperties, we need to fill in the missing values.
Wecould resort to sampling, but sampling these prop-erties is fairly inefficient.
So, following H&K, wekeep soft counts for each of these properties and usethem rather than perform hard sampling.When an entity z generates a pronoun h using thepronoun head model,7 it first generates a gender g, anumber n, and a semantic type t independently fromthe distributions ?gz , ?nz , and ?tz; and then generatesh using the distribution P (H = h|G = g,N =7While pronouns are generated by this pronoun head model,names and nominals continue to be handled by the basic model.644n, T = t, ?).
Note that this last distribution is aglobal distribution that is independent of the chosenentity index.
?
is a parameter drawn from a symmet-ric Dirichlet (with concentration ?P ) that encodesour prior knowledge of the relationship between asemantic type and a pronoun.
For instance, given thetype PERSON, there is a higher probability of gener-ating ?he?
than ?it?.
As a result, we maintain a listof compatible semantic types for each pronoun, andgive a pronoun a count of (1 + ?P ) if it is compatiblewith the drawn semantic type; otherwise, we give ita count of ?P .
In essence, we use this prior to preferthe generation of pronouns that are compatible withthe chosen semantic type.3.2.3 Salience ModelPronouns typically refer to salient entities, so thebasic model could be improved by incorporatingsalience.
We start by assuming that each entity hasan activity score that is initially set to zero.
Givena set of mentions and an assignment of entity in-dices to mentions, Z, we process the mentions in aleft-to-right manner.
When a mention, m, is encoun-tered, we multiply the activity score of each entity by0.5 and add one to the activity score of the entity towhich m belongs.
This captures the intuitive notionthat frequency and recency both play a role in deter-mining salience.
Next, we rank the entities based ontheir activity scores and discretize the ranks into five?salience?
buckets S: TOP (1), HIGH (2?3), MID (4?6), LOW (7+), and NONE.
Finally, this salience in-formation is used to modify the entity distribution:8P (Zij = z|Z?i,j) ?
nz ?
P (Mi,j |Si,j,Z)where Si,j is the salience value of the jth mentionin document i, and Mi,j is its mention type, whichcan take on one of three values: pronoun, name, andnominal.
P (Mi,j |Si,j,Z), the distribution of men-tion type given salience, was computed from H&K?sdevelopment corpus (see Table 2).
According tothe table, pronouns are preferred for salient entities,whereas names and nominals are preferred for enti-ties that are less active.8Rather than having just one probability term on the righthand side of the sampling equation, H&K actually have a prod-uct of probability terms, one for each mention that appears laterthan mention j in the given document.
However, they acknowl-edge that having the product makes sampling inefficient, anddecided to simplify the equation to this form in their evaluation.Salience Feature Pronoun Name NominalTOP 0.75 0.17 0.08HIGH 0.55 0.28 0.17MID 0.39 0.40 0.21LOW 0.20 0.45 0.35NONE 0.00 0.88 0.12Table 2: Posterior distribution of mention type givensalience (taken from Haghighi and Klein (2007))3.3 Modifications to the H&K ModelNext, we discuss the potential weaknesses of H&K?smodel and propose three modifications to it.Relaxed head generation.
The basic model fo-cuses on head matching, and is therefore likely to(incorrectly) posit the large airport and the smallairport as coreferent, for instance.
In fact, headmatching is a relatively inaccurate indicator of coref-erence, in comparison to the ?strong coreference in-dicators?
shown in the first three rows of Table 1.
Toimprove H&K?s model, we replace head matchingwith these three strong indicators as follows.
Givena document, we assign each of its mentions a headindex, such that two mentions have the same headindex if and only if at least one of the three strongindicators returns a value of True.
Now, instead ofgenerating a head, the head model generates a headindex, thus increasing the likelihood that aliases areassigned the same entity index, for instance.
Notethat this modification is applied only to the basicmodel.
In particular, pronoun generation continuesto be handled by the pronoun head model and willnot be affected.
We hypothesize that this modifica-tion would improve precision, as the strong indica-tors are presumably more precise than head match.Agreement constraints.
While the pronoun headmodel naturally prefers that a pronoun be generatedby an entity whose gender and number are compati-ble with those of the pronoun, the entity (index) thatis re-sampled for a pronoun according to the sam-pling equation for P (Zi,j |Z?i,j,H) may still not becompatible with the pronoun with respect to gen-der and number.
The reason is that an entity in-dex is assigned based not only on the head distri-bution but also on the entity distribution.
Since enti-ties with many mentions are preferable to those withfew mentions, it is possible for the model to favorthe assignment of a grammatically incompatible en-tity (index) to a pronoun if the entity is sufficiently645large.
To eliminate this possibility, we enforce theagreement constraints at the global level.
Specifi-cally, we sample an entity index for a given mentionwith a non-zero probability if and only if the corre-sponding entity and the head of the mention agree ingender and number.
We hypothesize that this modi-fication would improve precision.Pronoun-only salience.
In Section 3.2.3, we mo-tivate the need for salience using pronouns only,since proper names can to a large extent be resolvedusing string-matching facilities and are not particu-larly sensitive to salience.
Nominals (especially def-inite descriptions), though more sensitive to saliencethan names, can also be resolved by simple string-matching heuristics in many cases (Vieira and Poe-sio, 2000; Strube et al, 2002).
Hence, we hypothe-size that the use of salience for names and nominalswould adversely affect their resolution performance,as incorporating salience could diminish the role ofstring match in the resolution process, according tothe sampling equations.
Consequently, we modifyH&K?s model by limiting the application of salienceto the resolution of pronouns only.
We hypothesizethat this change would improve precision.4 Evaluation4.1 Experimental SetupTo evaluate our EM-based model and H&K?s model,we use the ACE 2003 coreference corpus, whichis composed of three sections: Broadcast News(BNEWS), Newswire (NWIRE), and Newspaper(NPAPER).
Each section is in turn composed of atraining set and a test set.
Due to space limitations,we will present evaluation results only for the testsets of BNEWS and NWIRE, but verified that thesame performance trends can be observed on NPA-PER as well.
Unlike H&K, who report results us-ing only true mentions (extracted from the answerkeys), we show results for true mentions as well assystem mentions that were extracted by an in-housenoun phrase chunker.
The relevant statistics of theBNEWS and NWIRE test sets are shown in Table 3.Scoring programs.
To score the output of thecoreference models, we employ the commonly-usedMUC scoring program (Vilain et al, 1995) and therecently-developed CEAF scoring program (Luo,2005).
In the MUC scorer, recall is computed asBNEWS NWIRENumber of documents 51 29Number of true mentions 2608 2630Number of system mentions 5424 5197Table 3: Statistics of the BNEWS and NWIRE test setsthe percentage of coreference links in the referencepartition that appear in the system partition; preci-sion is computed in the same fashion as recall, ex-cept that the roles of the reference partition and thesystem partition are reversed.
As a link-based scor-ing program, the MUC scorer (1) does not rewardsuccessful identification of singleton entities and (2)tends to under-penalize partitions that have too fewentities.
The entity-based CEAF scorer was pro-posed in response to these two weaknesses.
Specif-ically, it operates by computing the optimal align-ment between the set of reference entities and theset of system entities.
CEAF precision and recallare both positively correlated with the score of thisoptimal alignment, which is computed by summingover each aligned entity pair the number of mentionsthat appear in both entities of that pair.
As a conse-quence, a system that proposes too many entities ortoo few entities will have low precision and recall.Parameter initialization.
We use a small amountof labeled data for parameter initialization for thetwo models.
Specifically, for evaluations on theBNEWS test data, we use as labeled data onerandomly-chosen document from the BNEWS train-ing set, which has 58 true mentions and 102 systemmentions.
Similarly for NWIRE, where the chosendocument has 42 true mentions and 72 system men-tions.
For our model, we use the labeled documentto initialize the parameters.
Also, we set N (thenumber of most probable partitions) to 50 and ?
(thestart penalty used in the Bell tree) to 0.8, the latterbeing recommended by Luo et al (2004).For H&K?s model, we use the labeled data to tunethe concentration parameter ?.
While H&K set ?
to0.4 without much explanation, a moment?s thoughtreveals that the choice of ?
should reflect the frac-tion of mentions that appear in a singleton cluster.We therefore estimate this value from the labeleddocument, yielding 0.4 for true mentions (which isconsistent with H&K?s choice) and 0.7 for systemmentions.
The remaining parameters, the ?
?s, are all646set to e?4, following H&K.
In addition, as is com-monly done in Bayesian approaches, we do not sam-ple entities directly from the conditional distributionP (Z|X); rather, we sample from this distributionraised to the power exp cik?1 , where c=1.5, i is thecurrent iteration number that starts at 0, and k (thenumber of sampling iterations) is set to 20.
Finally,due to sampling and the fact that the initial assign-ment of entity indices to mentions is random, all thereported results for H&K?s model are averaged overfive runs.4.2 Results and DiscussionsThe Heuristic baseline.
As our first baseline, weemploy a simple rule-based system that posits twomentions as coreferent if and only if at least one ofthe three strong coreference indicators listed in Ta-ble 1 returns True.
Results of this baseline, reportedin terms of recall (R), precision (P), and F-score (F)using the MUC scorer and the CEAF scorer, areshown in row 1 of Tables 4 and 5, respectively.
Eachrow in these tables shows performance using truementions and system mentions for the BNEWS andNWIRE data sets.
As we can see, (1) recall is gen-erally low, since this simple heuristic can only iden-tify a small fraction of the coreference relations; (2)CEAF recall is consistently higher than MUC recall,since CEAF also rewards successful identification ofnon-coreference relations; and (3) precision for truementions is higher than that for system mentions,since the number of non-coreferent pairs that satisfythe heuristic is larger for system mentions.The Degenerate EM baseline.
Our second base-line is obtained by running only one iteration of ourEM-based coreference model.
Specifically, it startswith the M-step by initializing the model parame-ters using the labeled document, and ends with theE-step by applying the resulting model (in combi-nation with the Bell tree search algorithm) to ob-tain the most probable coreference partition for eachtest document.
Since there is no parameter re-estimation, this baseline is effectively a purely su-pervised system trained on one (labeled) document.Results are shown in row 2 of Tables 4 and 5.As we can see, recall is consistently much higherthan precision, suggesting that the model has pro-duced fewer entities than it should.
Perhaps moreinterestingly, in comparison to the Heuristic base-line, Degenerate EM performs consistently worseaccording to CEAF but generally better according toMUC.
This discrepancy stems from the aforemen-tioned properties that MUC under-penalizes parti-tions with too few entities, whereas CEAF lowersboth recall and precision when given such partitions.Our EM-based coreference model.
Our modeloperates in the same way as the Degenerate EMbaseline, except that EM is run until convergence,with the test set being used as unlabeled data for pa-rameter re-estimation.
Any performance differencebetween our model and Degenerate EM can thus beattributed to EM?s exploitation of the unlabeled data.Results of our model are shown in row 3 of Tables4 and 5.
In comparison to Degenerate EM, MUCF-score increases by 4-5% for BNEWS and 4-21%for NWIRE; CEAF F-score increases even more dra-matically, by 10-17% for BNEWS and 16-27% forNWIRE.
Improvements stem primarily from largegains in precision and comparatively smaller loss inrecall.
Such improvements suggest that our modelhas effectively exploited the unlabeled data.In comparison to the Heuristic baseline, we gener-ally see increases in both recall and precision whensystem mentions are used, and as a result, F-scoreimproves substantially by 7-15%.
When true men-tions are used, we still see gains in recall, but thesegains are accompanied by loss in precision.
F-scoregenerally increases (by 2-22%), except for the casewith NWIRE where we see a 0.5% drop in CEAFF-score as a result of a larger decrease in precision.The Original H&K model.
We use as our thirdbaseline the Original H&K model (see Section 3.2).Results of this model are shown in row 4 of Tables4 and 5.9 Overall, it underperforms our model by 6-16% in MUC F-score and 6-14% in CEAF F-score,due primarily to considerable drop in both recall andprecision in almost all cases.The Modified H&K model.
Next, we incorporateour three modifications into the Original H&K base-line one after the other.
Results are shown in rows5-7 of Tables 4 and 5.
Several points deserve men-tioning.
First, the addition of each modification im-proves the F-score for both true and system mentions9The H&K results shown here are not directly comparablewith those reported in Haghighi and Klein (2007), since H&Kevaluated their system on the ACE 2004 coreference corpus.647Broadcast News (BNEWS) Newswire (NWIRE)True Mentions System Mentions True Mentions System MentionsExperiments R P F R P F R P F R P F1 Heuristic Baseline 27.8 72.0 40.1 30.9 44.3 36.4 31.2 70.3 43.3 36.3 53.4 43.22 Degenerate EM Baseline 63.6 53.1 57.9 70.8 36.3 48.0 64.5 42.6 51.3 69.0 25.1 36.83 Our EM-based Model 56.1 71.4 62.8 42.4 66.0 51.6 47.0 68.3 55.7 55.2 60.6 57.84 Haghighi and Klein Baseline 49.4 60.2 54.3 50.8 40.7 45.2 44.7 55.5 49.5 43.0 40.9 41.95 + Relaxed Head Generation 53.0 65.4 58.6 48.3 45.7 47.0 45.1 62.5 52.4 40.9 50.0 45.06 + Agreement Constraints 53.6 68.7 60.2 50.4 47.5 48.9 44.6 63.7 52.5 41.7 51.2 46.07 + Pronoun-only Salience 56.8 68.3 62.0 52.2 53.0 52.6 46.8 66.2 54.8 44.3 57.3 50.08 Fully Supervised Model 53.7 70.8 61.1 53.0 70.3 60.4 52.0 69.6 59.6 53.1 70.5 60.6Table 4: Results obtained using the MUC scoring program for the Broadcast News and Newswire data setsBroadcast News (BNEWS) Newswire (NWIRE)True Mentions System Mentions True Mentions System MentionsExperiments R P F R P F R P F R P F1 Heuristic Baseline 42.1 75.8 54.1 44.2 48.7 46.3 43.9 73.4 54.9 47.5 53.4 50.32 Degenerate EM Baseline 51.2 43.1 46.8 53.7 26.8 35.8 51.0 30.5 38.2 45.1 18.6 26.33 Our EM-based Model 53.3 60.5 56.7 47.5 59.6 52.9 49.2 60.7 54.4 53.5 52.1 52.84 Haghighi and Klein Baseline 43.7 48.8 46.1 46.0 33.9 39.0 45.5 51.7 48.4 44.6 39.2 41.75 + Relaxed Head Generation 45.8 52.4 48.9 45.4 39.6 42.3 46.0 57.0 50.9 44.5 48.3 46.36 + Agreement Constraints 51.8 60.5 55.8 50.6 43.8 47.0 47.8 60.1 53.2 46.5 50.4 48.47 + Pronoun-only Salience 53.9 59.9 56.7 52.3 49.9 51.1 49.6 62.8 55.4 47.4 55.7 51.28 Fully Supervised Model 55.0 63.3 58.8 56.2 64.2 59.9 54.7 64.7 59.3 56.5 65.4 60.6Table 5: Results obtained using the CEAF scoring program for the Broadcast News and Newswire data setsin both data sets using both scorers.
These resultsprovide suggestive evidence that our modificationsare highly beneficial.
The three modifications, whenapplied in combination, improve Original H&K sub-stantially by 5-8% in MUC F-score and 7-12% inCEAF F-score, yielding results that compare favor-ably to those of our model in almost all cases.Second, the use of agreement constraints yieldslarger improvements with CEAF than with MUC.This discrepancy can be attributed to the fact thatCEAF rewards the correct identification of non-coreference relations, whereas MUC does not.
Sinceagreement constraints are intended primarily for dis-allowing coreference, they contribute to the success-ful identification of non-coreference relations and asa result yield gains in CEAF recall and precision.Third, the results are largely consistent with ourhypothesis that these modifications enhance preci-sion.
Together, they improve the precision of theOriginal H&K baseline by 8-16% (MUC) and 11-16% (CEAF), yielding a coreference model thatcompares favorably with our EM-based approach.Comparison with a supervised model.
Finally,we compare our EM-based model with a fully super-vised coreference resolver.
Inspired by state-of-the-art resolvers, we create our supervised classificationmodel by training a discriminative learner (the C4.5decision tree induction system (Quinlan, 1993)) witha diverse set of features (the 34 features described inNg (2007)) on a large training set (the entire ACE2003 coreference training corpus), and cluster usingthe Bell tree search algorithm.
The fully supervisedresults shown in row 8 of Tables 4 and 5 suggest thatour EM-based model has room for improvements,especially when system mentions are used.5 ConclusionsWe have presented a generative model for unsuper-vised coreference resolution that views coreferenceas an EM clustering process.
Experimental resultsindicate that our model outperforms Haghighi andKlein?s (2007) coreference model by a large marginon the ACE data sets and compares favorably to amodified version of their model.
Despite these im-provements, its performance is still not comparableto that of a fully supervised coreference resolver.A natural way to extend these unsupervised coref-erence models is to incorporate additional linguis-tic knowledge sources, such as those employed byour fully supervised resolver.
However, feature en-gineering is in general more difficult for generativemodels than for discriminative models, as the formertypically require non-overlapping features.
We planto explore this possibility in future work.648AcknowledgmentsWe thank the three anonymous reviewers for theircomments on an earlier draft of the paper.
This workwas supported in part by NSF Grant 0812261.ReferencesAvrim Blum and Tom Mitchell.
1998.
Combining la-beled and unlabeled data with co-training.
In Proceed-ings of COLT, pages 92?100.Colin Cherry and Shane Bergsma.
2005.
An expecta-tion maximization approach to pronoun resolution.
InProceedings of CoNLL, pages 88?95.Hal Daume?
III and Daniel Marcu.
2005.
A large-scaleexploration of effective global features for a joint en-tity detection and tracking model.
In Proceedings ofHLT/EMNLP, pages 97?104.Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-bin.
1977.
Maximum likelihood from incomplete datavia the EM algorithm.
Journal of the Royal StatisticalSociety, Series B, 39(1):1?38.Pascal Denis and Jason Baldridge.
2007.
Global, jointdetermination of anaphoricity and coreference resolu-tion using integer programming.
In Proceedings ofNAACL/HLT, pages 236?243.Stuart Geman and Donald Geman.
1984.
Stochastic re-laxation, Gibbs distributions and the Bayesian restora-tion of images.
IEEE Transactions on Pattern Analysisand Machine Intelligence, 6:721?741.Aria Haghighi and Dan Klein.
2007.
Unsupervisedcoreference resolution in a nonparametric Bayesianmodel.
In Proceedings of the ACL, pages 848?855.Heng Ji, David Westbrook, and Ralph Grishman.
2005.Using semantic relations to refine coreference deci-sions.
In Proceedings of HLT/EMNLP, pages 17?24.Andrew Kehler, Douglas Appelt, Lara Taylor, and Alek-sandr Simma.
2004.
Competitive self-trained pronouninterpretation In Proceedings of HLT-NAACL 2004:Short Papers, pages 33?36.Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, NandaKambhatla, and Salim Roukos.
2004.
A mention-synchronous coreference resolution algorithm basedon the Bell tree.
In Proceedings of the ACL, pages135?142.Xiaoqiang Luo.
2005.
On coreference resolution perfor-mance metrics.
In Proceedings of HLT/EMNLP, pages25?32.Christoph Mu?ller, Stefan Rapp, and Michael Strube.2002.
Applying co-training to reference resolution.
InProceedings of the ACL, pages 352?359.Vincent Ng.
2007.
Shallow semantics for coreferenceresolution.
In Proceedings of IJCAI, pages 1689?1694.Vincent Ng and Claire Cardie.
2002.
Improving machinelearning approaches to coreference resolution.
In Pro-ceedings of the ACL, pages 104?111.Vincent Ng and Claire Cardie.
2003.
Weakly supervisednatural language learning without redundant views.
InHLT-NAACL: Main Proceedings, pages 173?180.Simone Paolo Ponzetto and Michael Strube.
2006.Exploiting semantic role labeling, WordNet andWikipedia for coreference resolution.
In Proceedingsof HLT/NAACL, pages 192?199.J.
Ross Quinlan.
1993.
C4.5: Programs for MachineLearning.
Morgan Kaufmann.Wee Meng Soon, Hwee Tou Ng, and Daniel Chung YongLim.
2001.
A machine learning approach to corefer-ence resolution of noun phrases.
Computational Lin-guistics, 27(4):521?544.Michael Strube, Stefan Rapp, and Christoph Mu?ller.2002.
The influence of minimum edit distance on ref-erence resolution.
In Proceedings of EMNLP, pages312?319.Yee Whye Teh, Michael Jordan, Matthew Beal, andDavid Blei.
2006.
Hierarchical Dirichlet pro-cesses.
Journal of the American Statistical Associa-tion, 101(476):1527?1554.Renata Vieira and Massimo Poesio.
2000.
Anempirically-based system for processing definite de-scriptions.
Computational Linguistics, 26(4):539?593.Marc Vilain, John Burger, John Aberdeen, Dennis Con-nolly, and Lynette Hirschman.
1995.
A model-theoretic coreference scoring scheme.
In Proceedingsof MUC-6, pages 45?52.Xiaofeng Yang, GuoDong Zhou, Jian Su, and Chew LimTan.
2003.
Coreference resolution using competitivelearning approach.
In Proceedings of the ACL, pages176?183.649
