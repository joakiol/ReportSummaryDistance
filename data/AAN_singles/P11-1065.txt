Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 642?652,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsLearning Hierarchical Translation Structure with Linguistic AnnotationsMarkos MylonakisILLCUniversity of Amsterdamm.mylonakis@uva.nlKhalil Sima?anILLCUniversity of Amsterdamk.simaan@uva.nlAbstractWhile it is generally accepted that many trans-lation phenomena are correlated with linguis-tic structures, employing linguistic syntax fortranslation has proven a highly non-trivialtask.
The key assumption behind many ap-proaches is that translation is guided by thesource and/or target language parse, employ-ing rules extracted from the parse tree orperforming tree transformations.
These ap-proaches enforce strict constraints and mightoverlook important translation phenomenathat cross linguistic constituents.
We proposea novel flexible modelling approach to intro-duce linguistic information of varying gran-ularity from the source side.
Our methodinduces joint probability synchronous gram-mars and estimates their parameters, by select-ing and weighing together linguistically moti-vated rules according to an objective functiondirectly targeting generalisation over futuredata.
We obtain statistically significant im-provements across 4 different language pairswith English as source, mounting up to +1.92BLEU for Chinese as target.1 IntroductionRecent advances in Statistical Machine Translation(SMT) are widely centred around two concepts:(a) hierarchical translation processes, frequentlyemploying Synchronous Context Free Grammars(SCFGs) and (b) transduction or synchronousrewrite processes over a linguistic syntactic tree.SCFGs in the form of the Inversion-TransductionGrammar (ITG) were first introduced by (Wu, 1997)as a formalism to recursively describe the trans-lation process.
The Hiero system (Chiang, 2005)utilised an ITG-flavour which focused on hierarchi-cal phrase-pairs to capture context-driven translationand reordering patterns with ?gaps?, offering com-petitive performance particularly for language pairswith extensive reordering.
As Hiero uses a singlenon-terminal and concentrates on overcoming trans-lation lexicon sparsity, it barely explores the recur-sive nature of translation past the lexical level.
Nev-ertheless, the successful employment of SCFGs forphrase-based SMT brought translation models as-suming latent syntactic structure to the spotlight.Simultaneously, mounting efforts have been di-rected towards SMT models employing linguisticsyntax on the source side (Yamada and Knight,2001; Quirk et al, 2005; Liu et al, 2006), targetside (Galley et al, 2004; Galley et al, 2006) or both(Zhang et al, 2008; Liu et al, 2009; Chiang, 2010).Hierarchical translation was combined with targetside linguistic annotation in (Zollmann and Venu-gopal, 2006).
Interestingly, early on (Koehn et al,2003) exemplified the difficulties of integrating lin-guistic information in translation systems.
Syntax-based MT often suffers from inadequate constraintsin the translation rules extracted, or from striving tocombine these rules together towards a full deriva-tion.
Recent research tries to address these issues,by re-structuring training data parse trees to bet-ter suit syntax-based SMT training (Wang et al,2010), or by moving from linguistically motivatedsynchronous grammars to systems where linguisticplausibility of the translation is assessed through ad-ditional features in a phrase-based system (Venu-gopal et al, 2009; Chiang et al, 2009), obscuringthe impact of higher level syntactic processes.While it is assumed that linguistic structure doescorrelate with some translation phenomena, in this642work we do not employ it as the backbone of trans-lation.
In place of linguistically constrained trans-lation imposing syntactic parse structure, we opt forlinguistically motivated translation.
We learn latenthierarchical structure, taking advantage of linguisticannotations but shaped and trained for translation.We start by labelling each phrase-pair span in theword-aligned training data with multiple linguisti-cally motivated categories, offering multi-grainedabstractions from its lexical content.
These phrase-pair label charts are the input of our learning al-gorithm, which extracts the linguistically motivatedrules and estimates the probabilities for a stochasticSCFG, without arbitrary constraints such as phraseor span sizes.
Estimating such grammars undera Maximum Likelihood criterion is known to beplagued by strong overfitting leading to degener-ate estimates (DeNero et al, 2006).
In contrast,our learning objective not only avoids overfittingthe training data but, most importantly, learns jointstochastic synchronous grammars which directlyaim at generalisation towards yet unseen instances.By advancing from structures which mimic lin-guistic syntax, to learning linguistically aware latentrecursive structures targeting translation, we achievesignificant improvements in translation quality for 4different language pairs in comparison with a stronghierarchical translation baseline.Our key contributions are presented in the fol-lowing sections.
Section 2 discusses the weak in-dependence assumptions of SCFGs and introducesa joint translation model which addresses these is-sues and separates hierarchical translation structurefrom phrase-pair emission.
In section 3 we considera chart over phrase-pair spans filled with source-language linguistically motivated labels.
We showhow we can employ this crucial input to extract andtrain a hierarchical translation structure model withmillions of rules.
Section 4 demonstrates decodingwith the model by constraining derivations to lin-guistic hints of the source sentence and presents ourempirical results.
We close with a discussion of re-lated work and our conclusions.2 Joint Translation ModelOur model is based on a probabilistic SynchronousCFG (Wu, 1997; Chiang, 2005).
SCFGs define aSBAR ?
[WHNP SBAR\WHNP] (a)SBAR\WHNP ?
?VP/NPL NPR?
(b)NPR ?
[NP PP] (c)WHNP ?
WHNPP (d)WHNPP ?
which / der (e)VP/NPL ?
VP/NPLP (f)VP/NPLP ?
is / ist (g)NPR ?
NPRP (h)NPRP ?
the solution / die Lo?sung (i)NP ?
NPP (j)NPP ?
the solution / die Lo?sung (k)PP ?
PPP (l)PPP ?
to the problem / fu?r das Problem (m)Figure 1: English-German SCFG rules for the relativeclause(s) ?which is the solution (to the problem) / der dieLo?sung (fu?r das Problem) ist?, [ ] signify monotone trans-lation, ?
?
a swap reordering.language over string pairs, which are generated be-ginning from a start symbol S and recursively ex-panding pairs of linked non-terminals across the twostrings using the grammar?s rule set.
By crossing thelinks between the non-terminals of the two sides re-ordering phenomena are captured.
We employ bi-nary SCFGs, i.e.
grammars with a maximum of twonon-terminals on the right-hand side.
Also, for thiswork we only used grammars with either purely lexi-cal or purely abstract rules involving one or two non-terminal pairs.
An example can be seen in Figure 1,using an ITG-style notation and assuming the samenon-terminal labels for both sides.We utilise probabilistic SCFGs, where each ruleis assigned a conditional probability of expandingthe left-hand side symbol with the rule?s right-handside.
Phrase-pairs are emitted jointly and the over-all probabilistic SCFG is a joint model over parallelstrings.2.1 SCFG Reordering WeaknessesAn interesting feature of all probabilistic SCFGs(i.e.
not only binary ones), which has received sur-prisingly little attention, is that the reordering pat-643tern between the non-terminal pairs (or in the caseof ITGs the choice between monotone and swap ex-pansion) are not conditioned on any other part of aderivation.
The result is that, the reordering patternwith the highest probability will always be preferred(e.g.
in the Viterbi derivation) over the rest, irre-spective of lexical or abstract context.
As an ex-ample, a probabilistic SCFG will always assign ahigher probability to derivations swapping or mono-tonically translating nouns and adjectives betweenEnglish and French, only depending on which of thetwo rules NP ?
[NN JJ ], NP ?
?NN JJ?has a higher probability.
The rest of the (sometimesthousands of) rule-specific features usually added toSCFG translation models do not directly help either,leaving reordering decisions disconnected from therest of the derivation.While in a decoder this is somehow mitigated bythe use of a language model, we believe that theweakness of straightforward applications of SCFGsto model reordering structure at the sentence levelmisses a chance to learn this crucial part of thetranslation process during grammar induction.
As(Mylonakis and Sima?an, 2010) note, ?plain?
SCFGsseem to perform worse than the grammars describednext, mainly due to wrong long-range reordering de-cisions for which the language model can hardlyhelp.2.2 Hierarchical Reordering SCFGWe address the weaknesses mentioned above by re-lying on an SCFG grammar design that is similar tothe ?Lexicalised Reordering?
grammar of (Mylon-akis and Sima?an, 2010).
As in the rules of Fig-ure 1, we separate non-terminals according to thereordering patterns in which they participate.
Non-terminals such as BL, CR take part only in swap-ping right-hand sides ?BL CR?
(with BL swap-ping from the source side?s left to the target side?sright, CR swapping in the opposite direction), whilenon-terminals such as B, C take part solely in mono-tone right-hand side expansions [B C].
These non-terminal categories can appear also on the left-handside of a rule, as in rule (c) of Figure 1.In contrast with (Mylonakis and Sima?an, 2010),monotone and swapping non-terminals do not emitphrase-pairs themselves.
Rather, each non-terminalNT is expanded to a dedicated phrase-pair emit-A ?
[B C] A ?
?BL CR?AL ?
[B C] AL ?
?BL CR?AR ?
[B C] AR ?
?BL CR?A ?
AP AP ?
?
/ ?AL ?
ALP ALP ?
?
/ ?AR ?
ARP ARP ?
?
/ ?Figure 2: Recursive Reordering Grammar rule cate-gories; A, B, C non-terminals; ?, ?
source and targetstrings respectively.ting non-terminal NTP, which generates all phrase-pairs for it and nothing more.
In this way, the pref-erence of non-terminals to either expand towardsa (long) phrase-pair or be further analysed recur-sively is explicitly modelled.
Furthermore, this setof pre-terminals allows us to separate the higher or-der translation structure from the process that emitsphrase-pairs, a feature we employ next.In (Mylonakis and Sima?an, 2010) this grammardesign mainly contributed to model lexical reorder-ing preferences.
While we retain this function, forthe rich linguistically-motivated grammars used inthis work this design effectively propagates reorder-ing preferences above and below the current rule ap-plication (e.g.
Figure 1, rules (a)-(c)), allowing tolearn and apply complex reordering patterns.The different types of grammar rules are sum-marised in abstract form in Figure 2.
We will subse-quently refer to this grammar structure as Hierarchi-cal Reordering SCFG (HR-SCFG).2.3 Generative ModelWe arrive at a probabilistic SCFG model whichjointly generates source e and target f strings, byaugmenting each grammar rule with a probability,summing up to one for every left-hand side.
Theprobability of a derivation D of tuple ?e, f?
begin-ning from start symbol S is equal to the product ofthe probabilities of the rules used to recursively gen-erate it.We separate the structural part of the derivationD, down to the pre-terminals NTP, from the phrase-emission part.
The grammar rules pertaining to the644X, SBAR, WHNP+VP, WHNP+VBZ+NPX, VBZ+NP, VP, SBAR\WHNPX, SBAR/NN, WHNP+VBZ+DTX, VBZ+DT, VP/NNX, WHNP+VBZ, X, NP,SBAR/NP VP\VBZX, WHNP, X, VBZ, X, DT, X, NN,SBAR/VP VP/NP NP/NN NP\DTwhich is the problemFigure 3: The label chart for the source fragment ?whichis the problem?.
Only a sample of the entries is listed.structural part and their associated probabilities de-fine a model p(?)
over the latent variable ?
de-termining the recursive, reordering and phrase-pairsegmenting structure of translation, as in Figure 4.Given ?, the phrase-pair emission part merely gener-ates the phrase-pairs utilising distributions from ev-ery NTP to the phrase-pairs that it covers, therebydefining a model over all sentence-pairs generatedgiven each translation structure.
The probabilities ofa derivation and of a sentence-pair are then as fol-lows:p(D) =p(?
)p(e, f |?)
(1)p(e, f) =?D:D??
?e,f?p(D) (2)By splitting the joint model in a hierarchical struc-ture model and a lexical emission one we facilitateestimating the two models separately.
The followingsection discusses this.3 Learning Translation Structure3.1 Phrase-Pair Label ChartThe input to our learning algorithm is a word-aligned parallel corpus.
We consider as phrase-pair spans those that obey the word-alignment con-straints of (Koehn et al, 2003).
For every train-ing sentence-pair, we also input a chart containingone or more labels for every synchronous span, suchas that of Figure 3.
Each label describes differ-ent properties of the phrase pair (syntactic, semanticetc.
), possibly in relation to its context, or supply-ing varying levels of abstraction (phrase-pair, deter-miner with noun, noun-phrase, sentence etc.).
Weaim to induce a recursive translation structure ex-plaining the joint generation of the source and targetsentence taking advantage of these phrase-pair spanlabels.For this work we employ the linguistically mo-tivated labels of (Zollmann and Venugopal, 2006),albeit for the source language.
Given a parse of thesource sentence, each span is assigned the followingkind of labels:Phrase-Pair All phrase-pairs are assigned the XlabelConstituent Source phrase is a constituent AConcatenation of Constituents Source phrase la-belled A+B as a concatenation of constituents A andB, similarly for 3 constituents.Partial Constituents Categorial grammar (Bar-Hillel, 1953) inspired labels A/B, A\B, indicatinga partial constituent A missing constituent B right orleft respectively.An important point is that we assign all applica-ble labels to every span.
In this way, each label setcaptures the features of the source side?s parse-treewithout being bounded by the actual parse structure,as well as provides a coarse to fine-grained view ofthe source phrase.3.2 Grammar ExtractionFrom every word-aligned sentence-pair and its la-bel chart, we extract SCFG rules as those of Figure2.
Binary rules are extracted from adjoining syn-chronous spans up to the whole sentence-pair level,with the non-terminals of both left and right-handside derived from the label names plus their reorder-ing function (monotone, left/right swapping) in thespan examined.
A single unary rule per non-terminalNT generates the phrase-pair emitting NTP.
Unaryrules NTP ?
?
/ ?
generating the phrase-pair arecreated for all the labels covering it.While we label the phrase-pairs similarly to (Zoll-mann and Venugopal, 2006), the extracted grammaris rather different.
We do not employ rules that aregrounded to lexical context (?gap?
rules), relying in-stead on the reordering-aware non-terminal set andrelated unary and binary rules.
The result is a gram-mar which can both capture a rich array of trans-lation phenomena based on linguistic and lexicalgrounds and explicitly model the balance between645SBARWHNPWHNPPwhichder< SBAR\WHNP >VP/NPLVP/NPLPisistNPRNPNPPthe solutiondie Lo?sungPPPPPto the problemfu?r das ProblemFigure 4: A derivation of a sentence fragment with thegrammar of Figure 1.memorising long phrase-pairs and generalising overyet unseen ones, as shown in the next example.The derivation in Figure 4 illustrates some of theformalism?s features.
A preference to reorder basedon lexical content is applied for is / ist.
Noun phraseNPR is recursively constructed with a preference toconstitute the right branch of an order swapping non-terminal expansion.
This is matched with VP/NPLwhich reorders in the opposite direction.
The labelsVP/NP and SBAR\WHNP allow linguistic syntaxcontext to influence the lexical and reordering trans-lation choices.
Crucially, all these lexical, attach-ment and reordering preferences (as encoded in themodel?s rules and probabilities) must be matched to-gether to arrive at the analysis in Figure 4.3.3 Parameter EstimationWe estimate the parameters for the phrase-emissionmodel p(e, f |?)
using Relative Frequency Estima-tion (RFE) on the label charts induced for the train-ing sentence-pairs, after the labels have been aug-mented by the reordering indications.
In the RFEestimate, every rule NTP ?
?
/ ?
receives a prob-ability in proportion with the times that ?
/ ?
wascovered by the NT label.On the other hand, estimating the parameters un-der Maximum-Likelihood Estimation (MLE) for thelatent translation structure model p(?)
is bound tooverfit towards memorising whole sentence-pairs asdiscussed in (Mylonakis and Sima?an, 2010), withthe resulting grammar estimate not being able togeneralise past the training data.
However, apartfrom overfitting towards long phrase-pairs, a gram-mar with millions of structural rules is also liable tooverfit towards degenerate latent structures which,while fitting the training data well, have limited ap-plicability to unseen sentences.We avoid both pitfalls by estimating the grammarprobabilities with the Cross-Validating Expectation-Maximization algorithm (CV-EM) (Mylonakis andSima?an, 2008; Mylonakis and Sima?an, 2010).
CV-EM is a cross-validating instance of the well knownEM algorithm (Dempster et al, 1977).
It works it-eratively on a partition of the training data, climb-ing the likelihood of the training data while cross-validating the latent variable values, considering forevery training data point only those which can beproduced by models built from the rest of the dataexcluding the current part.
As a result, the estima-tion process simulates maximising future data likeli-hood, using the training data to directly aim towardsstrong generalisation of the estimate.For our probabilistic SCFG-based translationstructure variable ?, implementing CV-EM boilsdown to a synchronous version of the Inside-Outsidealgorithm, modified to enforce the CV criterion.
Inthis way we arrive at cross-validated ML estimate ofthe ?
parameters while keeping the phrase-emissionparameters of p(e, f |?)
fixed.
The CV-criterion,apart from avoiding overfitting, results in discardingthe structural rules which are only found in a singlepart of the training corpus, leading to a more com-pact grammar while still retaining millions of struc-tural rules that are more hopeful to generalise.Unravelling the joint generative process, by mod-elling latent hierarchical structure separately fromphrase-pair emission, allows us to concentrate ourinference efforts towards the hidden, higher-leveltranslation mechanism.4 Experiments4.1 Decoding ModelThe induced joint translation model can be usedto recover argmaxe p(e|f), as it is equal toargmaxe p(e, f).
We employ the induced proba-bilistic HR-SCFGG as the backbone of a log-linear,feature based translation model, with the derivationprobability p(D) under the grammar estimate being646one of the features.
This is augmented with a smallnumber n of additional smoothing features ?i forderivation rules r: (a) conditional phrase translationprobabilities, (b) lexical phrase translation probabil-ities, (c) word generation penalty, and (d) a countof swapping reordering operations.
Features (a), (b)and (c) are applicable to phrase-pair emission rulesand features for both translation directions are used,while (d) is only triggered by structural rules.These extra features assess translation quality pastthe synchronous grammar derivation and learninggeneral reordering or word emission preferencesfor the language pair.
As an example, while ourprobabilistic HR-SCFG maintains a separate jointphrase-pair emission distribution per non-terminal,the smoothing features (a) above assess the condi-tional translation of surface phrases irrespective ofany notion of recursive translation structure.The final feature is the language model scorefor the target sentence, mounting up to the follow-ing model used at decoding time, with the featureweights ?
trained by Minimum Error Rate Training(MERT) (Och, 2003) on a development corpus.p(D??
?e, f?)
?
p(e)?lmpG(D)?Gn?i=1?r?D?i(r)?i4.2 Decoding ModificationsWe use a customised version of the Joshua SCFGdecoder (Li et al, 2009) to translate, with the fol-lowing modifications:Source Labels Constraints As for this work thephrase-pair labels used to extract the grammar arebased on the linguistic analysis of the source side,we can construct the label chart for every input sen-tence from its parse.
We subsequently use it to con-sider only derivations with synchronous spans whichare covered by non-terminals matching one of thelabels for those spans.
This applies both for the non-terminals covering phrase-pairs as well as the higherlevel parts of the derivation.In this manner we not only constrain the trans-lation hypotheses resulting in faster decoding time,but, more importantly, we may ground the hypothe-ses more closely to the available linguistic informa-tion of the source sentence.
This is of particularinterest as we move up the derivation tree, wherean initial wrong choice below could propagate to-wards hypotheses wildly diverging from the inputsentence?s linguistic annotation.Per Non-Terminal Pruning The decoder uses acombination of beam and cube-pruning (Huang andChiang, 2007).
As our grammar uses non-terminalsin the hundreds of thousands, it is important notto prune away prematurely non-terminals coveringsmaller spans and to leave more options to be con-sidered as we move up the derivation tree.For this, for every cell in the decoder?s chart, wekeep a separate bin per non-terminal and prune to-gether hypotheses leading to the same non-terminalcovering a cell.
This allows full derivations to befound for all input sentences, as well as avoids ag-gressive pruning at an early stage.
Given the sourcelabel constraint discussed above, this does not in-crease running times or memory demands consid-erably as we allow only up to a few tens of non-terminals per span.Expected Counts Rule Pruning To compact thehierarchical structure part of the grammar prior todecoding, we prune rules that fail to accumulate10?8 expected counts during the last CV-EM iter-ation.
For English to German, this brings the struc-tural rules from 15M down to 1.2M.
Note that wedo not prune the phrase-pair emitting rules.
Over-all, we consider this a much more informed pruningcriterion than those based on probability values (thatare not comparable across left-hand sides) or right-hand side counts (frequent symbols need many moreexpansions than a highly specialised one).4.3 Experimental Setting & BaselineWe evaluate our method on four different lan-guage pairs with English as the source languageand French, German, Dutch and Chinese as tar-get.
The data for the first three language pairs arederived from parliament proceedings sourced fromthe Europarl corpus (Koehn, 2005), with WMT-07 development and test data for French and Ger-man.
The data for the English to Chinese task iscomposed of parliament proceedings and news arti-cles.
For all language pairs we employ 200K and400K sentence pairs for training, 2K for develop-ment and 2K for testing (single reference per sourcesentence).
Both the baseline and our method decode647Training English toFrench German Dutch Chineseset size BLEU NIST BLEU NIST BLEU NIST BLEU NIST200Kjosh-base 29.20 7.2123 18.65 5.8047 21.97 6.2469 22.34 6.5540lts 29.43 7.2611** 19.10** 5.8714** 22.31* 6.2903* 23.67** 6.6595**400Kjosh-base 29.58 7.3033 18.86 5.8818 22.25 6.2949 23.24 6.7402lts 29.83 7.4000** 19.49** 5.9374** 22.92** 6.3727** 25.16** 6.9005**Table 1: Experimental results for training sets of 200K and 400K sentence pairs.
Statistically significant score im-provements from the baseline at the 95% confidence level are labelled with a single star, at the 99% level with two.with a 3-gram language model smoothed with modi-fied Knesser-Ney discounting (Chen and Goodman,1998), trained on around 1M sentences per targetlanguage.
The parses of the source sentences em-ployed by our system during training and decod-ing are created with the Charniak parser (Charniak,2000).We compare against a state-of-the-art hierarchi-cal translation (Chiang, 2005) baseline, based on theJoshua translation system under the default trainingand decoding settings (josh-base).
Apart of eval-uating against a state-of-the-art system, especiallyon the English-Chinese language pair, the compar-ison has an added interesting aspect.
The heuristi-cally trained baseline takes advantage of ?gap rules?to reorder based on lexical context cues, but makesvery limited use of the hierarchical structure abovethe lexical surface.
In contrast, our method inducesa grammar with no such rules, relying on lexicalcontent and the strength of a higher level translationstructure instead.4.4 Training & Decoding DetailsTo train our Latent Translation Structure (LTS) sys-tem, we used the following settings.
CV-EM cross-validated on a 10-part partition of the training dataand performed 10 iterations.
The structural ruleprobabilities were initialised to uniform per left-hand side.The decoder does not employ any ?glue grammar?as is usual with hierarchical translation systems tolimit reordering up to a certain cut-off length.
In-stead, we rely on our LTS grammar to reorder andconstruct the translation output up to the full sen-tence length.In summary, our system?s experimental pipeline isas follows.
All input sentences are parsed and labelcharts are created from these parses.
The Hierarchi-cal Reordering SCFG is extracted and its parame-ters are estimated employing CV-EM.
The structuralrules of the estimate are pruned according to theirexpected counts and smoothing features are added toall rules.
We train the feature weights under MERTand decode with the resulting log-linear model.The overall training and decoding setup is appeal-ing also regarding computational demands.
On an8-core 2.3GHz system, training on 200K sentence-pairs demands 4.5 hours while decoding runs on 25sentences per minute.4.5 ResultsTable 1 presents the results for the baseline and ourmethod for the 4 language pairs, for training sets ofboth 200K and 400K sentence pairs.
Our system(lts) outperforms the baseline for all 4 languagepairs for both BLEU and NIST scores, by a marginwhich scales up to +1.92 BLEU points for English toChinese translation when training on the 400K set.In addition, increasing the size of the training datafrom 200K to 400K sentence pairs widens the per-formance margin between the baseline and our sys-tem, in some cases considerably.
All but one of theperformance improvements are found to be statis-tically significant (Koehn, 2004) at the 95% confi-dence level, most of them also at the 99% level.We selected an array of target languages ofincreasing reordering complexity with English assource.
Examining the results across the target lan-guages, LTS performance gains increase the morechallenging the sentence structure of the target lan-guage is in relation to the source?s, highlighted whentranslating to Chinese.
Even for Dutch and German,which pose additional challenges such as compoundwords and morphology which we do not explicitlytreat in the current system, LTS still delivers signif-icant improvements in performance.
Additionally,648System 200K 400K(a)lts-nolabels 22.50 24.24lts 23.67** 25.16**(b)josh-base-lm4 23.81 24.77lts-lm4 24.48** 26.35**Table 2: Additional experiments for English to Chi-nese translation examining (a) the impact of the linguis-tic annotations in the LTS system (lts), when com-pared with an instance not employing such annotations(lts-nolabels) and (b) decoding with a 4th-orderlanguage model (-lm4).
BLEU scores for 200K and400K training sentence pairs.the robustness of our system is exemplified by deliv-ering significant performance increases for all lan-guage pairs.For the English to Chinese translation task, weperformed further experiments along two axes.
Wefirst investigate the contribution of the linguisticannotations, by comparing our complete system(lts) with an otherwise identical implementation(lts-nolabels) which does not employ any lin-guistically motivated labels.
The latter system thenuses a labels chart as that of Figure 3, which howeverlabels all phrase-pair spans solely with the genericX label.
The results in Table 2(a) indicate that alarge part of the performance improvement can beattributed to the use of the linguistic annotations ex-tracted from the source parse trees, indicating thepotential of the LTS system to take advantage ofsuch additional annotations to deliver better trans-lations.The second additional experiment relates to theimpact of employing a stronger language model dur-ing decoding, which may increase performance butslows down decoding speed.
Notably, as can be seenin Table 2(b), switching to a 4-gram LM results inperformance gains for both the baseline and our sys-tem and while the margin between the two systemsdecreases, our system continues to deliver a con-siderable and significant improvement in translationBLEU scores.5 Related WorkIn this work, we focus on the combination oflearning latent structure with syntax and linguisticannotations, exploring the crossroads of machinelearning, linguistic syntax and machine translation.Training a joint probability model was first dis-cussed in (Marcu and Wong, 2002).
We show thata translation system based on such a joint modelcan perform competitively in comparison with con-ditional probability models, when it is augmentedwith a rich latent hierarchical structure trained ade-quately to avoid overfitting.Earlier approaches for linguistic syntax-basedtranslation such as (Yamada and Knight, 2001; Gal-ley et al, 2006; Huang et al, 2006; Liu et al, 2006)focus on memorising and reusing parts of the struc-ture of the source and/or target parse trees and con-straining decoding by the input parse tree.
In con-trast to this approach, we choose to employ lin-guistic annotations in the form of unambiguous syn-chronous span labels, while discovering ambiguoustranslation structure taking advantage of them.Later work (Marton and Resnik, 2008; Venugopalet al, 2009; Chiang et al, 2009) takes a more flex-ible approach, influencing translation output usinglinguistically motivated features, or features basedon source-side linguistically-guided latent syntacticcategories (Huang et al, 2010).
A feature-based ap-proach and ours are not mutually exclusive, as wealso employ a limited set of features next to ourtrained model during decoding.
We find augment-ing our system with a more extensive feature set aninteresting research direction for the future.An array of recent work (Chiang, 2010; Zhang etal., 2008; Liu et al, 2009) sets off to utilise sourceand target syntax for translation.
While for this workwe constrain ourselves to source language syntaxannotations, our method can be directly applied toemploy labels taking advantage of linguistic annota-tions from both sides of translation.
The decodingconstraints of section 4.2 can then still be applied onthe source part of hybrid source-target labels.For the experiments in this paper we employ a la-bel set similar to the non-terminals set of (Zollmannand Venugopal, 2006).
However, the synchronousgrammars we learn share few similarities with thosethat they heuristically extract.
The HR-SCFG weadopt allows capturing more complex reorderingphenomena and, in contrast to both (Chiang, 2005;Zollmann and Venugopal, 2006), is not exposed tothe issues highlighted in section 2.1.
Nevertheless,our results underline the capacity of linguistic anno-649tations similar to those of (Zollmann and Venugopal,2006) as part of latent translation variables.Most of the aforementioned work does concen-trate on learning hierarchical, linguistically moti-vated translation models.
Cohn and Blunsom (2009)sample rules of the form proposed in (Galley et al,2004) from a Bayesian model, employing Dirich-let Process priors favouring smaller rules to avoidoverfitting.
Their grammar is however also basedon the target parse-tree structure, with their systemsurpassing a weak baseline by a small margin.
Incontrast to the Bayesian approach which imposesexternal priors to lead estimation away from degen-erate solutions, we take a data-driven approach toarrive to estimates which generalise well.
The richlinguistically motivated latent variable learnt by ourmethod delivers translation performance that com-pares favourably to a state-of-the-art system.Mylonakis and Sima?an (2010) also employ theCV-EM algorithm to estimate the parameters of anSCFG, albeit a much simpler one based on a hand-ful of non-terminals.
In this work we employ someof their grammar design principles for an immenselymore complex grammar with millions of hierarchi-cal latent structure rules and show how such gram-mar can be learnt and applied taking advantage ofsource language linguistic annotations.6 ConclusionsIn this work we contribute a method to learn andapply latent hierarchical translation structure.
Tothis end, we take advantage of source-language lin-guistic annotations to motivate instead of constrainthe translation process.
An input chart over phrase-pair spans, with each cell filled with multiple lin-guistically motivated labels, is coupled with the HR-SCFG design to arrive at a rich synchronous gram-mar with millions of structural rules and the capacityto capture complex linguistically conditioned trans-lation phenomena.
We address overfitting issues bycross-validating climbing the likelihood of the train-ing data and propose solutions to increase the effi-ciency and accuracy of decoding.An interesting aspect of our work is deliveringcompetitive performance for difficult language pairssuch as English-Chinese with a joint probabilitygenerative model and an SCFG without ?gap rules?.Instead of employing hierarchical phrase-pairs, weinvest in learning the higher-order hierarchical syn-chronous structure behind translation, up to the fullsentence length.
While these choices and the relatedresults challenge current MT research trends, theyare not mutually exclusive with them.
Future workdirections include investigating the impact of hierar-chical phrases for our models as well as any gainsfrom additional features in the log-linear decodingmodel.Smoothing the HR-SCFG grammar estimatescould prove a possible source of further perfor-mance improvements.
Learning translation and re-ordering behaviour with respect to linguistic cuesis facilitated in our approach by keeping separatephrase-pair emission distributions per emitting non-terminal and reordering pattern, while the employ-ment of the generic X non-terminals already allowsbacking off to more coarse-grained rules.
Neverthe-less, we still believe that further smoothing of thesesparse distributions, e.g.
by interpolating them withless sparse ones, could in the future lead to an addi-tional increase in translation quality.Finally, we discuss in this work how our methodcan already utilise hundreds of thousands of phrase-pair labels and millions of structural rules.
A fur-ther promising direction is broadening this set withlabels taking advantage of both source and target-language linguistic annotation or categories explor-ing additional phrase-pair properties past the parsetrees such as semantic annotations.AcknowledgmentsBoth authors are supported by a VIDI grant (nr.639.022.604) from The Netherlands Organizationfor Scientific Research (NWO).
The authors wouldlike to thank Maxim Khalilov for helping withexperimental data and Andreas Zollmann and theanonymous reviewers for their valuable comments.ReferencesYehoshua Bar-Hillel.
1953.
A quasi-arithmetical nota-tion for syntactic description.
Language, 29(1):47?58.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proceedings of the North American Asso-ciation for Computational Linguistics (HLT/NAACL),Seattle, Washington, USA, April.650Stanley Chen and Joshua Goodman.
1998.
An empiricalstudy of smoothing techniques for language modeling.Technical Report TR-10-98, Harvard University, Au-gust.David Chiang, Kevin Knight, and Wei Wang.
2009.11,001 new features for statistical machine transla-tion.
In Proceedings of Human Language Technolo-gies: The 2009 Annual Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics, pages 218?226, Boulder, Colorado, June.
As-sociation for Computational Linguistics.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofACL 2005, pages 263?270.David Chiang.
2010.
Learning to translate with sourceand target syntax.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Linguis-tics, pages 1443?1452, Uppsala, Sweden, July.
Asso-ciation for Computational Linguistics.Trevor Cohn and Phil Blunsom.
2009.
A Bayesian modelof syntax-directed tree to string grammar induction.In Proceedings of the 2009 Conference on Empiri-cal Methods in Natural Language Processing, pages352?361, Singapore, August.
Association for Compu-tational Linguistics.A.P.
Dempster, N.M. Laird, and D.B.
Rubin.
1977.
Max-imum likelihood from incomplete data via the em al-gorithm.
Journal of the Royal Statistical Society, Se-ries B, 39(1):1?38.John DeNero, Dan Gillick, James Zhang, and Dan Klein.2006.
Why generative phrase models underperformsurface heuristics.
In Proceedings on the Workshopon Statistical Machine Translation, pages 31?38, NewYork City.
Association for Computational Linguistics.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule?
InDaniel Marcu Susan Dumais and Salim Roukos, ed-itors, HLT-NAACL 2004: Main Proceedings, pages273?280, Boston, Massachusetts, USA, May.
Associ-ation for Computational Linguistics.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Proceed-ings of the 21st International Conference on Computa-tional Linguistics and 44th Annual Meeting of the As-sociation for Computational Linguistics, pages 961?968, Sydney, Australia, July.
Association for Compu-tational Linguistics.Liang Huang and David Chiang.
2007.
Forest rescoring:Faster decoding with integrated language models.
InProceedings of the 45th Annual Meeting of the Asso-ciation of Computational Linguistics, pages 144?151,Prague, Czech Republic, June.
Association for Com-putational Linguistics.Liang Huang, Kevin Knight, and Aravind Joshi.
2006.Statistical syntax-directed translation with extendeddomain of locality.
In Proceedings of the 7th BiennialConference of the Association for Machine Translationin the Americas (AMTA), Boston, MA, USA.Zhongqiang Huang, Martin Cmejrek, and Bowen Zhou.2010.
Soft syntactic constraints for hierarchicalphrase-based translation using latent syntactic distri-butions.
In Proceedings of the 2010 Conference onEmpirical Methods in Natural Language Processing,pages 138?147, Cambridge, MA, October.
Associa-tion for Computational Linguistics.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In HLT-NAACL 2003.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Dekang Lin andDekai Wu, editors, Proceedings of EMNLP 2004,pages 388?395, Barcelona, Spain, July.
Associationfor Computational Linguistics.Philipp Koehn.
2005.
Europarl: A Parallel Corpus forStatistical Machine Translation.
In MT Summit 2005.Zhifei Li, Chris Callison-Burch, Chris Dyer, SanjeevKhudanpur, Lane Schwartz, Wren Thornton, JonathanWeese, and Omar Zaidan.
2009.
Joshua: An opensource toolkit for parsing-based machine translation.In Proceedings of the Fourth Workshop on StatisticalMachine Translation, pages 135?139, Athens, Greece,March.
Association for Computational Linguistics.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machine trans-lation.
In Proceedings of the 21st International Con-ference on Computational Linguistics and 44th AnnualMeeting of the Association for Computational Linguis-tics, pages 609?616, Sydney, Australia, July.
Associa-tion for Computational Linguistics.Yang Liu, Yajuan Lu?, and Qun Liu.
2009.
Improvingtree-to-tree translation with packed forests.
In Pro-ceedings of the Joint Conference of the 47th AnnualMeeting of the ACL and the 4th International JointConference on Natural Language Processing of theAFNLP, pages 558?566, Suntec, Singapore, August.Association for Computational Linguistics.Daniel Marcu andWilliamWong.
2002.
A phrase-based,joint probability model for statistical machine transla-tion.
In Proceedings of Empirical methods in naturallanguage processing, pages 133?139.
Association forComputational Linguistics.Yuval Marton and Philip Resnik.
2008.
Soft syntacticconstraints for hierarchical phrased-based translation.In Proceedings of ACL-08: HLT, pages 1003?1011,651Columbus, Ohio, June.
Association for ComputationalLinguistics.Markos Mylonakis and Khalil Sima?an.
2008.
Phrasetranslation probabilities with ITG priors and smooth-ing as learning objective.
In Proceedings of the 2008Conference on Empirical Methods in Natural Lan-guage Processing, pages 630?639, Honolulu, USA,October.Markos Mylonakis and Khalil Sima?an.
2010.
Learn-ing probabilistic synchronous CFGs for phrase-basedtranslation.
In Fourteenth Conference on Computa-tional Natural Language Learning, Uppsala, Sweden,July.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of the41st Annual Meeting of the Association for Compu-tational Linguistics, pages 160?167, Sapporo, Japan,July.
Association for Computational Linguistics.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.
De-pendency treelet translation: Syntactically informedphrasal smt.
In Proceedings of 43rd Annual Meetingof the Association for Computational Linguistics, AnnArbor, Michigan, USA, June.Ashish Venugopal, Andreas Zollmann, Noah A. Smith,and Stephan Vogel.
2009.
Preference grammars: Soft-ening syntactic constraints to improve statistical ma-chine translation.
In Proceedings of Human LanguageTechnologies: The 2009 Annual Conference of theNorth American Chapter of the Association for Com-putational Linguistics, pages 236?244, Boulder, Col-orado, June.
Association for Computational Linguis-tics.Wei Wang, Jonathan May, Kevin Knight, and DanielMarcu.
2010.
Re-structuring, re-labeling, and re-aligning for syntax-based machine translation.
Com-putational Linguistics, 36(2):247?277.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?403.Kenji Yamada and Kevin Knight.
2001.
A syntax-basedstatistical translation model.
In Proceedings of 39thAnnual Meeting of the Association for ComputationalLinguistics, pages 523?530, Toulouse, France, July.Association for Computational Linguistics.Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,Chew Lim Tan, and Sheng Li.
2008.
A tree sequencealignment-based tree-to-tree translation model.
InProceedings of ACL-08: HLT, pages 559?567, Colum-bus, Ohio, June.
Association for Computational Lin-guistics.Andreas Zollmann and Ashish Venugopal.
2006.
Syntaxaugmented machine translation via chart parsing.
InProceedings on the Workshop on Statistical MachineTranslation, pages 138?141, New York City, June.
As-sociation for Computational Linguistics.652
