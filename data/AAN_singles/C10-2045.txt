Coling 2010: Poster Volume, pages 391?399,Beijing, August 2010Learning Summary Content Units with Topic ModelingLeonhard Hennig Ernesto William De LucaDistributed Artificial Intelligence Laboratory (DAI-Lab)Technische Universita?t Berlin{leonhard.hennig,ernesto.deluca,sahin.albayrak}@dai-labor.deSahin AlbayrakAbstractIn the field of multi-document summa-rization, the Pyramid method has be-come an important approach for evaluat-ing machine-generated summaries.
Themethod is based on the manual annotationof text spans with the same meaning in aset of human model summaries.
In thispaper, we present an unsupervised, prob-abilistic topic modeling approach for au-tomatically identifying such semanticallysimilar text spans.
Our approach revealssome of the structure of model summariesand identifies topics that are good approx-imations of the Summary Content Units(SCU) used in the Pyramid method.
Ourresults show that the topic model identi-fies topic-sentence associations that corre-spond to the contributors of SCUs, sug-gesting that the topic modeling approachcan generate a viable set of candidateSCUs for facilitating the creation of Pyra-mids.1 IntroductionIn the field of multi-document summariza-tion (MDS), the Pyramid method has becomean important approach for evaluating machine-generated summaries (Nenkova and Passonneau,2004; Passonneau et al, 2005; Nenkova et al,2007).
The method rewards automatic summariesfor conveying content that has the same meaningas content represented in a set of human modelsummaries.
This approach allows for variation inthe way the content is expressed, which contraststhe Pyramid method with other evaluation meth-ods such as ROUGE that measure word n-gramoverlap (Lin and Hovy, 2003).The Pyramid method groups content withthe same meaning into Summary Content Units(SCU).
Shared content needs to be identified man-ually by human inspection of summaries, addingyet another level of human effort (on top ofcreating model summaries) to the task of sum-mary evaluation.
However, Nenkova and Passon-neau (2004) as well as Harnly et al (2005) ob-serve that semantically similar text spans writ-ten by different human summarizers are often ex-pressed with a similar choice of words, albeit withdifferences e.g.
in word variants, word order andparaphrasing (Section 2).In this paper, we present an approach for au-tomatically identifying semantically similar textspans in human model summaries on the basisof such re-occurring word patterns.
We utilizea method known as probabilistic topic model-ing (Steyvers and Griffiths, 2007).
Topic modelsare claimed to derive semantic information fromtext in an unsupervised fashion, using only the ob-served word distributions (Section 3).?
We train a probabilistic topic model basedon Latent Dirichlet Allocation (Blei et al,2003) on the term-sentence matrix of humanmodel summaries used in the Document Un-derstanding Conference (DUC) 2007 Pyra-mid evaluation1.
We analyze the resultingmodel to evaluate whether a topic model cap-tures useful structures of these summaries(Section 4.1).1http://duc.nist.gov391?
Given the model, we compare the automati-cally identified topics with SCUs on the ba-sis of their word distributions (Sections 4.2and 4.3).
We discover a clear correspondencebetween topics and SCUs, which suggeststhat many automatically identified topics aregood approximations of manually annotatedSCUs.?
We analyze the distribution of topics oversummary sentences in Section 4.4, and com-pare the topic-sentence associations com-puted by our model with the SCU-sentenceassociations given by the Pyramid annota-tion.
Our results suggest that the topic modelfinds many SCU-like topics, and associatesa given topic with the same summary sen-tences in which a human annotator identifiesthe corresponding SCU.Automatically identifying topics that approxi-mate SCUs has clear practical applications: Thetopics can be used as a candidate set of SCUs forhuman annotators to speed up the process of SCUcreation.
Topics can also be identified in machine-generated summaries using standard statistical in-ference techniques (Asuncion et al, 2009).2 Summary Content UnitsIn this section, we briefly introduce the Pyramidmethod and the properties of Summary ContentUnits that we intend to exploit in our approach.A Pyramid is a model predicting the distribu-tion of information content in summaries, as re-flected in the summaries humans write (Passon-neau et al, 2005; Nenkova et al, 2007).
Simi-lar information content is identified by inspectionof similar sentences, and parts of these, in differ-ent human model summaries.
Typically, the textspans which express the same semantic contentare not longer than a clause.
An SCU consists ofa collection of text spans with the same meaning(contributors) and a defining label specified by theannotator.Each SCU is weighted by the number of hu-man model summaries it occurs in (i.e.
the num-ber of contributors).
The Pyramid metric assumesthat an SCU with a high number of contributors ismore informative than an SCU with few contrib-utors.
An optimal summary, in terms of contentselection, is obtained by maximizing the sum ofSCU weights, given a maximum number of SCUsthat can be included for a predefined summarylength (Nenkova and Passonneau, 2004).Two example SCUs are given in Table 1.
SCU18 has a weight of 3, since three model sum-maries contribute to it, SCU 21 has a weight of 2.SCU 18 aggregates contributors which share somekey phrases such as ?Air National Guard?
and?search?, but otherwise exhibit a quite heteroge-neous word usage.
Contributor 3 gives details onthe aircraft type, and specifies a time when the firstsea vessel was launched to search for the missingplane.
Only contributor 1 gives information aboutthe location of the search.
In SCU 21, the firstcontributor contains additional information aboutcommunication with the Kennedy family, which isnot expressed in the SCU label and therefore notpart of the meaning of the SCU.
Both contributorscontain key terms such as ?officials?, ?search?
and?recovery?, but vary in word order and verb usage.Passonneau et al (2005) discuss this observation,and argue that SCUs emerge from the judgmentof annotators, and are thus independent of whatwords are used, or how many.However, an analysis of typical SCUs showsthat contributors written by different human sum-marizers are often expressed with a similar choiceof words or even phrases.
Contributors vary inusing different forms of the same words (inflec-tional or derivational variants), different word or-der, syntactic structure, and paraphrases (Harnlyet al, 2005; Nenkova et al, 2007).3 Probabilistic Topic ModelsOur approach for discovering semantically similartext spans makes use of a statistical method knownas topic modeling.
Probabilistic topic models canderive semantic information from text automat-ically, on the basis of the observed word pat-terns (Hofmann, 1999; Blei et al, 2003; Steyversand Griffiths, 2007).
The main assumption ofthese models is that a latent set of variables ?
thetopics ?
can be utilized to explain the observedpatterns in the data.
Documents are represented asmixtures of topics, and each topic is a distribution392SCU 18 The US Coast Guard with help from the Air National Guard then began a massivesearch-and-rescue mission, searching waters along the presumed flight pathContributor 1: The US Coast Guard with help from the Air National Guard then began a massivesearch-and-rescue mission, searching waters along the presumed flight pathContributor 2: A multi-agency search and rescue mission began at 3:28 a.m., with the CoastGuard and Air National Guard participatingContributor 3: The first search vessel was launched at about 4:30am.
An Air National GuardC-130 and many Civil Air Patrol aircraft joined the searchSCU 21 Federal officials shifted the mission to search and recoveryContributor 1: Federal officials shifted the mission to search and recovery and communicated theKennedy and Bessette familiesContributor 2: federal officials ended the search for survivors and began a search-and-recoverymissionTable 1: Example SCUs from topic D0742 of DUC 2007.MNwz???
?TFigure 1: Graphical model representation of LDAfor N words, T topics and a corpus of M docu-ments.over words.
For example, a news article describ-ing a meeting of the International Monetary Fundmay in equal parts discuss economic and politi-cal issues.
Topic models discover in a completelyunsupervised fashion meaningful topics as well asintra- and inter-document statistical structure us-ing no information except the distribution of thewords themselves (Griffiths and Steyvers, 2004).For our analysis, we use the Latent DirichletAllocation (LDA) model introduced by Blei etal.
(2003).
In this model, each document is gen-erated by first choosing a distribution over topics?
(d), parametrized by a conjugate Dirichlet prior?.
Subsequently, each word of this document isgenerated by drawing a topic zk from ?
(d), andthen drawing a word wi from topic zk?s distri-bution over words ?(k).
We follow Griffiths etal.
(2004) and place a conjugate Dirichlet prior ?over ?
(k) as well.
Figure 1 shows the graphicalmodel representation of LDA.For T topics, the matrix ?
specifies the proba-bility p(w|z) of words given topics, and ?
spec-ifies the probability p(z|d) of topics given docu-ments.
p(w|z) indicates which words are impor-tant in a topic, and p(z|d) tells us which topics aredominant in a document.
We employ Gibbs sam-pling (Griffiths and Steyvers, 2004) to estimatethe posterior distribution over z (the assignment ofword tokens to topics), given the observed wordsw of the document set.
From this estimate we canapproximate the distributions for the matrices ?and ?.4 ExperimentsCan a topic model reveal some of the structureof human model summaries and learn topics thatare approximations of manually annotated SCUs?To answer these questions, we train a topic modelon the human model summaries of each of the 23document clusters of the DUC 2007 dataset thatwere used in Pyramid evaluation2.
There are 4human model summaries available for each docu-ment cluster.
On average, the summary sets con-tain 52.4 sentences, with a vocabulary of 260.5terms, which occur a total of 549.7 times.
ThePyramids of these summary sets consist of 68.8SCUs on average.
The number of SCUs per SCU2http://www-nlpir.nist.gov/projects/duc/data.html393weight follows a Zipfian distribution, i.e.
there aretypically very few SCUs of weight 4, and verymany SCUs of weight 1 (see also Passonneau etal.
(2005)).4.1 Topic model trainingSince we are interested in modeling topics for sen-tences, we treat each sentence as a document3.We construct a matrix A of term-sentence co-occurrence observations for each set of humanmodel summaries S. Each entry Aij correspondsto the frequency of word i in sentence j, and jranges over the union of the sentences containedin S. We preprocess terms using stemming andremoving a standard list of stop words with theNLTK toolkit4.We run the Gibbs sampling algorithm on A,setting the parameter T , the number of latent top-ics to learn, equal to the number of SCUs con-tained in the Pyramid of S. We use this particularvalue for T since we want to learn a topic modelwith a structure that reflects the SCUs and the dis-tribution of SCUs of the corresponding Pyramid.For an unannotated set of summaries, determiningan optimal value for T is a Bayesian model selec-tion problem (Kass and Raftery, 1995).The topic distribution for each sentence shouldbe peaked toward a single or only very few top-ics.
To ensure that the topic-specific word dis-tributions p(w|z) as well as the sentence-specifictopic distributions p(z|d) behave as intended, weset the Dirichlet priors ?
= 0.01 and ?
= 0.01.This enforces a bias toward sparsity, resulting indistributions that are more peaked (Steyvers andGriffiths, 2007).
A low value of ?
also favorsmore fine-grained topics (Griffiths and Steyvers,2004).
We run the Gibbs sampler for 2000 itera-tions, and collect a single sample from the result-ing posterior distribution over topic assignmentsfor words.
From this sample we compute the con-ditional distributions p(w|z) and p(z|d).During our experiments, we observed that theGibbs Sampler did not always use all the topicsavailable.
Instead, some topics had a uniformdistribution over words, i.e.
no words were as-3We will use the words document and sentence inter-changeably from here on.4http://www.nltk.orgsigned to these topics during the sampling pro-cess.
We assume this is due to the relatively lowprior ?
= 0.01 we use in our experiments.
We ex-plore the consequences of varying the LDA priorsand T in Section 4.4.This observation indicates that the topic modelcannot learn as many distinct topics from a givenset of summaries as there are SCUs in the Pyra-mid of these summaries.
On average, 24.4% (?
=17.4) of the sampled topics had a uniform worddistribution, but the fraction of such topics varied.For some summary sets, it was very low (D0701,D0706 with 0%), whereas for others it was veryhigh (D0704, D0728 with 52%).
Both of the lat-ter summary sets contain many SCUs with verysimilar labels and often only a single contributor,e.g.
about ?Amnesty International?:?
AI criticism frequently involves genocide?
AI criticism frequently involves intimidation?
AI criticism frequently involves police vio-lenceThese SCUs are derived from summary sen-tences that contain enumerations: ?AI criticismfrequently involves political prisoners, torture, in-timidation, police violence, the death penalty,no alternative service for conscientious objectors,and interference with the judiciary.?
A topicmodel is based on word-document co-occurrencedata, and cannot distinguish between the differentgrammatical objects in this case.
Instead, it treatsthese phrases as semantically similar since theyoccur in the same sentence.4.2 SCU word distributions and SCU-sentence associationsIn order to evaluate the quality of the LDA top-ics, we compare their word distributions to theword distributions of SCUs.
This allows us to an-alyze if the LDA topics capture similar word pat-terns as SCUs.
We approximate the distributionover words p(w|sl) for each SCU sl as the rel-ative frequency of word wi in the bag-of-wordsconstructed from the texts of sl?s label and con-tributors.
We denote the resulting matrix of for aset of SCUs as ?
?.394(a) DUC Topic D0706 (b) DUC Topic D0742 (c) DUC Topic D0743Figure 2: Pairwise Jensen-Shannon divergence of word distributions of LDA topics and SummaryContent Units (SCUs), for 3 DUC 2007 Pyramids.
Topic-SCU matches are ordered by increasingdivergence along the diagonal, using a simple greedy algorithm.
The examples suggest that many ofthe automatically identified LDA topics correspond to manually annotated SCUs.Topic 17 SCU 31 Topic 5 SCU 32 Topic 9 SCU 25 Topic 8 SCU 36pilot pilot analysi analysi bodi bodi kennedi kennedikennedi condit control control diver diver edward edwardcondit conduc corkscrew corkscrew entomb entomb recoveri recovericonduc dark descent descent floor floor son sondark disorient fall fall found found wit witTable 2: Top terms of best matching LDA topics and SCUs for summary set D0742In addition, we can compare the topic-sentenceassociations computed by the model to the SCU-sentence associations given by the Pyramid anno-tation.
If the probability of a given topic is highin those sentences which contribute to a particularSCU, this would suggest that the topic model canautomatically learn topics which not only have aword distribution similar to a specific SCU, butalso a similar distribution over contributing sen-tences.SCU contributors are typically annotated as aset of contiguous sequences of words within a sin-gle sentence.
In the DUC 2007 data, there are onlya few cases where a contributor spans more thanone sentence.
The DUCView annotation tool5stores the start and end character position of thephrases marked as contributors of an SCU.
We canutilize this information to define which sentencesan SCU is associated with.
We store the associa-tions in a matrix ?
?, where ?
?ij = 1 if SCU i isassociated with sentence j. Sentences may con-tain multiple SCUs, and SCUs are associated with5http://www1.cs.columbia.edu/?becky/DUC2006/2006-pyramid-guidelines.htmlas many sentences as their number of contributors.4.3 Matching SCUs and LDA topicsBefore we can compare the topic-sentence asso-ciations computed by the LDA topic model withthe SCU-sentence associations, we need to matchSCUs to LDA topics.
We consider a topic to besimilar to an SCU if their word distributions aresimilar.
We discard all LDA topics with a uni-form word distribution (see Section 4.1) beforethe matching step.We then compute the pair-wise Jensen-Shannon(JS) divergence between columns j of ?
and k of??
:JS(?j , ?
?k) =[12DKL(?j ||M)+12DKL(?
?k||M)], (1)where M = 1/2(?j + ??k).
SCUs from ??
arematched to topics of ?
on the basis of this dis-similarity using a simple greedy approach, i.e.
byiteratively selecting the current most similar SCU-395(a) Precision, recall and fraction of Topic-SCU matches fordifferent settings of ?
(b) F1 and MAP for different values of T as a fraction ?
ofthe number of SCUsFigure 3: (a) Precision, Recall and the fraction of LDA topics matched to SCUs for different settingsof parameter ?, averaged over all summary sets with Pyramid annotations from DUC 2007.
Error barsshow the standard deviation.
Only topic-SCU matches with JS(?j , ?
?k) ?
?
are considered whencomputing precision and recall.
Both are very high, suggesting that the model identifies topics that arevery similar to SCUs.
(b) F1 measure and Mean Average Precision (MAP) for different settings of thenumber of latent topics T as a fraction of the number of SCUs in the corresponding Pyramid (?
= 0.5).topic pair.
We reorder the rows of ?
according tothe computed matching.Figure 2 shows some example SCU-topicmatches for three different DUC 2007 summarysets.
Each cell displays the JS divergence of theword distributions of an LDA topic (rows) com-pared to an SCU (columns).
On the diagonal, thebest matches of LDA topics and SCUs are orderedby increasing JS divergence.
Multiple points withlow JS divergence in a single column indicate thatmore than one LDA topic was very similar to thisSCU.
Overall, the graphs show a clear correspon-dence of LDA topics to the SCUs.
The plots sug-gest that a large percentage of topics have simi-lar distributions over words as the correspondingSCUs.
Table 2 shows the most likely terms forsome example topic-SCU matches.
For each ofthese matches, the top terms are almost identical.4.4 EvaluationTo compare the topic distributions ?
with theSCU-sentence assignments ?
?, we binarize ?
togive ??
by setting all entries ?
?ij = 1 if ?ij > ,and 0 otherwise.
We set  = 0.1 in our experi-ments6.
?
?ij is therefore equal to 1 if a topic i hasa high probability sentence j.
We can now eval-uate if a given topic occurs in the same sentencesas the corresponding SCU (recall), and if it occursin no other sentences (precision).We compute precision and recall for each topic-SCU match with JS(?j , ?
?k) ?
?.
Averagedover matches, these measures give us an indica-tion of how well the LDA model approximatesthe set of SCUs.
The parameter ?
allows us totune the performance of the model with respectto the quality and number of topic-SCU matches.Setting ?
to a low value will consider only topic-SCU matches with a low JS divergence, whichgenerally results in higher precision and recall.
In-creasing ?
will include more topic-SCU matches,namely those with a larger JS divergence, whichwill therefore introduce some noise.Figure 3(a) shows the precision and recall6Since the LDA algorithm learns very peaked distribu-tions, the actual value of this threshold does not have a largeimpact on the resulting binary matrix and subsequent eval-uation results.
We evaluated a range of settings for  in[0.001 ?
0.5], all with similar performance.
This observa-tion is confirmed by the threshold-less Mean Average Preci-sion results in Figure 3(b).396curves for different values of the parameter ?, av-eraged over all summary sets.
The plots showthat both the precision and recall of the discov-ered topic-sentence associations are quite high,suggesting that the model automatically identifiestopics which are very similar to manually anno-tated SCUs.
With increasing ?, precision and re-call scores decrease: The word distributions ofthe topic-SCU pairs are increasingly dissimilar,and hence the sentences associated with a topicdo not necessarily overlap anymore with the sen-tences of the paired SCU.
The figure also showsthe fraction of topic matches that are consideredin the evaluation of precision and recall.
Thereis a clear trade off between performance and thenumber of matches retrieved.
However, many ofthe topic-SCU matches (?
50%) have a JS diver-gence ?
0.4, suggesting that the word distribu-tions of many LDA topics are very similar to SCUword distributions.Since we observed that the Gibbs samplingdoes not always utilize the full set of topics, werepeat our experiments to evaluate how the per-formance of the model changes when varying theLDA priors and T .
Figure 3(b) shows F1 andMean Average Precision (MAP)7 results of thetopic model for different values of the parameter?, where T = ?
?
|SCU |.
For example, a value of0.6 means that for each summary set, T was set to60% of the number of SCUs in the correspondingPyramid.
We see that the MAP score increasesquickly, and reaches a plateau for ?
?
0.3.
TheF1 score increases more slowly, and levels out for?
?
0.6.
The model?s performance is relativelyrobust with respect to ?.
This observation can behelpful when training models for new summarysets without an existing Pyramid, and which there-fore consider T as a parameter to be optimized.When varying the LDA priors, we observe thatfor 0.01 ?
?
?
0.05, F1 and MAP scores areconsistently high, whereas for other settings, per-formance decreases significantly.
Similarly, ?
?0.05 results in lower F1 and MAP scores.
The7MAP is a rank-based measure, which avoids the needfor introducing a threshold to binarize ?
(Baeza-Yates andRibeiro-Neto, 1999).
For each topic, we create a ranked listof sentences according to the transposed matrix ?T .
Thisgives high ranks to sentences for which a particular topic hasa high probability.fraction of uniform topics decreases with higher?, e.g.
for ?
= 0.1 it is close to zero.
In con-trast, higher settings of ?
increase the fraction ofuniform topics.8Finally, Figure 4 shows separate precision andrecall curves for SCUs of different weights, andfor different settings of parameter ?.
Results areagain averaged over all summary sets.
In 4(a),we see that the recall of topic-sentence associa-tions is very similar for all SCUs, with SCUs ofhigher weight exhibiting a slightly better recall.However, as Figure 4(b) shows, the average pre-cision of SCUs with lower weight is much higher.Intuitively, this is expectable as SCUs of higherweight tend to have a larger vocabulary due to thehigher number of contributors.
This results in alarger word overlap with non-relevant sentences.The fraction of topic-SCU matches retrieved forSCUs of different weight is similar for all types ofSCUs (not shown here).5 Related WorkThe Pyramid approach was introduced byNenkova and Passonneau (2004) as a method forevaluating machine-generated summaries basedon a set of human model summaries.
The authorsaddress a number of shortcomings of manual andautomatic summary evaluation methods such asROUGE (Lin and Hovy, 2003), and argue that thePyramid method is reliable, diagnostic and predic-tive.Passonneau and et al (2005) give an account ofthe results of applying the Pyramid method duringthe DUC 2005 summarization evaluation, and dis-cuss the annotation process.
In subsequent work,Nenkova et al (2007) describe in more detail theincorporation of human variation in the Pyramidmethod, the reliability of content annotation, andthe correlation of Pyramid scores with other eval-uation measures.Harnly et al (2005) present an approach for au-tomatically scoring a machine summary given anexisting Pyramid.
Their method searches for anoptimal set of candidate contributors created auto-matically from the machine summary and matchescandidates to SCUs using a clustering approach.8Results are not shown due to space constraints.397(a) Recall of Topic-SCU matches for SCUs by weight (b) Precision of Topic-SCU matches for SCUs by weightFigure 4: (a) Recall of topic-SCU matches for SCUs of different weights, and settings of parameter ?,averaged over all summary sets.
Recall is similar for SCUs of all weights.
(b) Precision of the sametopic-SCU matches.
SCUs with a lower weight have a higher average precision.
(Error bars show thestandard deviation.
)The method assumes the existence of a Pyramid,whereas our approach aims to discover candidateSCUs from a set of human model summaries in anunsupervised fashion.Recently, Louis and Nenkova (2009) presentedan approach for fully automatic, model-free eval-uation of machine-generated summaries.
Themethod assumes that the distribution of words inthe input and an informative summary should besimilar.
We think that it could be an interestingidea to combine the proposed method with ourapproach, in an attempt to exploit both the model-free evaluation and the shallow semantics of latenttopics.Probabilistic topic models have been success-fully applied to a variety of tasks (Hofmann, 1999;Blei et al, 2003; Griffiths and Steyvers, 2004;Hall et al, 2008).
In text summarization, mosttopic modeling approaches utilize a term-sentenceco-occurrence matrix to discover topics in the setof input documents.
Each sentence is typically as-signed to a single topic, and a topic is a cluster ofmultiple sentences (Wang et al, 2009; Tang et al,2009; Hennig, 2009).6 Conclusions and future workWe presented a probabilistic topic modeling ap-proach that reveals some of the structure of humanmodel summaries.
The topic model is trained onthe term-sentence matrix of a set of human sum-maries, and discovers semantic topics in a com-pletely unsupervised fashion.
Many of the topicsidentified by our model for a given set of sum-maries show a similar distribution over words asthe manually annotated Summary Content Unitsof the summaries?
Pyramid.We utilized the word distributions of SCUsand topics to match topics to similar SCUs, andshowed that the topics identified by the model of-ten occur in the same sentences as the contribu-tors of the corresponding SCU.
Precision and re-call of these topic-sentence assignments are veryhigh when compared to the SCU-sentence associ-ations, indicating that many of the automaticallyacquired topics are good approximations of SCUs.Our results suggest that a topic model can be usedto learn a candidate set of SCUs to facilitate theprocess of Pyramid creation.We note that the topic model that we ap-plied is one of the simplest latent variable mod-els.
A more complex model could integrate syn-tax to relax the bag-of-words assumption (Wal-lach, 2006), or combine the statistical model withmore linguistically-grounded methods to handlelinguistic features such as enumerations or nega-tion.398ReferencesAsuncion, Arthur, Max Welling, Padhraic Smyth, andYee Whye Teh.
2009.
On smoothing and infer-ence for topic models.
In UAI ?09: Proceedings ofthe Twenty-Fifth Conference on Uncertainty in Arti-ficial Intelligence, pages 27?34.Baeza-Yates, Ricardo A. and Berthier Ribeiro-Neto.1999.
Modern Information Retrieval.
Addison-Wesley Longman Publishing Co., Inc., Boston, MA,USA.Blei, David M., Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alocation.
J. Mach.
Learn.Res., 3:993?1022.Griffiths, T. L. and M. Steyvers.
2004.
Finding scien-tific topics.
Proceedings of the National Academyof Sciences, 101(Suppl.
1):5228?5235.Hall, David, Daniel Jurafsky, and Christopher D. Man-ning.
2008.
Studying the history of ideas usingtopic models.
In EMNLP ?08: Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing, pages 363?371.Harnly, A., A. Nenkova, R. Passonneau, and O. Ram-bow.
2005.
Automation of summary evaluation bythe Pyramid method.
In Proceedings of the Con-ference on Recent Advances in Natural LanguageProcessing (RANLP).Hennig, Leonhard.
2009.
Topic-based multi-document summarization with probabilistic latentsemantic analysis.
In International Conference onRecent Advances in Natural Language Processing(RANLP).Hofmann, Thomas.
1999.
Probabilistic latent seman-tic indexing.
In SIGIR ?99: Proceedings of the 22ndannual international ACM SIGIR conference on Re-search and development in information retrieval,pages 50?57.Kass, R. E. and A. E. Raftery.
1995.
Bayes fac-tors.
Journal of the American Statistical Associa-tion, 90:773?795.Lin, Chin-Yew and Eduard Hovy.
2003.
Auto-matic evaluation of summaries using N-gram co-occurrence statistics.
In NAACL ?03: Proceed-ings of the 2003 Conference of the North Ameri-can Chapter of the Association for ComputationalLinguistics on Human Language Technology, pages71?78.Louis, Annie and Ani Nenkova.
2009.
Automaticallyevaluating content selection in summarization with-out human models.
In EMNLP ?09: Proceedings ofthe 2009 Conference on Empirical Methods in Nat-ural Language Processing, pages 306?314.Nenkova, Ani and Rebecca Passonneau.
2004.
Eval-uating Content Selection in Summarization: ThePyramid Method.
In Susan Dumais, Daniel Marcuand Salim Roukos, editors, HLT-NAACL 2004:Main Proceedings, pages 145?152.Nenkova, Ani, Rebecca Passonneau, and KathleenMcKeown.
2007.
The Pyramid Method: Incorpo-rating human content selection variation in summa-rization evaluation.
ACM Trans.
Speech Lang.
Pro-cess., 4(2):4.Passonneau, R. J, A. Nenkova, K. McKeown, andS.
Sigelman.
2005.
Applying the Pyramid methodin DUC 2005.
In Proceedings of the Document Un-derstanding Conference (DUC?05).Steyvers, Mark and Tom Griffiths.
2007.
Probabilis-tic topic models.
In Landauer, T., S. Dennis Mc-Namara, and W. Kintsch, editors, Latent SemanticAnalysis: A Road to Meaning.
Laurence Erlbaum.Tang, J., L. Yao, and D. Chen.
2009.
Multi-topic basedquery-oriented summarization.
In Proceedings ofthe Siam International Conference on Data Mining.Wallach, Hanna M. 2006.
Topic modeling: beyondbag-of-words.
In ICML ?06: Proceedings of the23rd international conference on Machine learning,pages 977?984.Wang, Dingding, Shenghuo Zhu, Tao Li, and YihongGong.
2009.
Multi-document summarization usingsentence-based topic models.
In ACL-IJCNLP ?09:Proceedings of the ACL-IJCNLP 2009 ConferenceShort Papers, pages 297?300.399
