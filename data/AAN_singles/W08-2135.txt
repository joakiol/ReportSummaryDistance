CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 243?247Manchester, August 2008The Integration of Dependency Relation Classification and Semantic RoleLabeling Using Bilayer Maximum Entropy Markov ModelsWeiwei Sun and Hongzhan Li and Zhifang SuiInstitute of Computational LinguisticsPeking University{weiwsun, lihongzhan.pku}@gmail.com, szf@pku.edu.cnAbstractThis paper describes a system to solvethe joint learning of syntactic and seman-tic dependencies.
An directed graphicalmodel is put forward to integrate depen-dency relation classification and semanticrole labeling.
We present a bilayer di-rected graph to express probabilistic re-lationships between syntactic and seman-tic relations.
Maximum Entropy MarkovModels are implemented to estimate con-ditional probability distribution and to doinference.
The submitted model yields76.28% macro-average F1 performance,for the joint task, 85.75% syntactic depen-dencies LAS and 66.61% semantic depen-dencies F1.1 IntroductionDependency parsing and semantic role labeling arebecoming important components in many kinds ofNLP applications.
Given a sentence, the task of de-pendency parsing is to identify the syntactic headof each word in the sentence and classify the rela-tion between the dependent and its head; the taskof semantic role labeling consists of analyzing thepropositions expressed by some target predicates.The integration of syntactic and semantic parsinginterests many researchers and some approacheshas been proposed (Yi and Palmer, 2005; Ge andMooney, 2005).
CoNLL 2008 shared task pro-poses the merging of both syntactic dependenciesand semantic dependencies under a unique unifiedrepresentation (Surdeanu et al, 2008).
We explorec?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.the integration problem and evaluate our approachusing data provided on CoNLL 2008.This paper explores the integration of depen-dency relation classification and semantic role la-beling, using a directed graphical model that is alsoknown as Bayesian Networks.
The directed graphof our system can be seen as one chain of obser-vations with two label layers: the observations areargument candidates; one layer?s label set is syn-tactic dependency relations; the other?s is semanticdependency relations.
To estimate the probabilitydistribution of each arc and do inference, we im-plement a Maximum Entropy Markov Model (Mc-Callum et al, 2000).
Specially, a logistic regres-sion model is used to get the conditional probabil-ity of each arc; dynamic programming algorithmis applied to solve the ?argmax?
problem.2 System DescriptionOur DP-SRL system consists of 5 stages:1. dependency parsing;2. predicate prediction;3. syntactic dependency relation classificationand semantic dependency relation identifica-tion;4. semantic dependency relation classification;5. semantic dependency relation inference.2.1 Dependency ParsingIn dependency parsing stage, MSTParser1(Mc-Donald et al, 2005), a dependency parser thatsearches for maximum spanning trees over di-rected graphs, is used.
we use MSTParser?s default1http://www.seas.upenn.edu/ strctlrn/MSTParser/MSTParser.html243Lemma and its POS tagNumber of childrenSequential POS tags of childrenLemma and POS of Neighboring wordsLemma and POS of parentIs the word in word list of NomBankIs the word in word list of PropBankIs POS of the word is VB* or NN*Table 1: Features used to predict target predicatesparameters to train a parsing model.
In the thirdstage of our system, dependency relations betweenargument candidates and target predicates are up-dated, if there are dependency between the candi-dates and the predicates.2.2 Predicate PredictionDifferent from CoNLL-2005 shared task, the tar-get predicates are not given as input.
Our systemformulates the predicate predication problem as atwo-class classification problem using maximumentropy classifier MaxEnt2(Berger et al, 1996).Table 1 lists features used.
We use a empiricalthreshold to filter words: if the ?being target?
prob-ability of a word is greater than 0.075, it is seen asa target predicate.
This strategy achieves a 79.96%precision and a 98.62% recall.2.3 Syntactic Dependency RelationClassification and Semantic DependencyRelation IdentificationWe integrate dependency parsing and semanticrole labeling to some extent in this stage.
Some de-pendency parsing systems prefer two-stage archi-tecture: unlabeled parsing and dependency clas-sification (Nivre et al, 2007).
Previous semanticrole labeling approaches also prefer two-stage ar-chitecture: argument identification and argumentclassification.
Our system does syntactic relationsclassification and semantic relations identificationat the same time.
Specially, using a pruning al-gorithm, we collect a set of argument candidates;then we classify dependency relations between ar-gument candidates and the predicates and predictwhether a candidate is an argument.
A directedgraphical model is used to represent the relationsbetween syntactic and semantic relations.2http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.htmlLemma, POS tag voice of predicatesPOS pattern of predicate?s childrenIs the predicate from NomBank or PropBankPredicate class.
This information is extractedform frame file of each predicate.Position: whether the candidate is before orafter the predicateLemma and POS tag of the candidateLemma and POS of Neighboring words of thecandidateLemma and POS of sibling words of thecandidateLength of the constituent headed by thecandidateLemma and POS of the left and right mostwords of the constituent of the candidatePunctuation before and after the candidatePOS path: the chain of POS from candidate topredicateSingle Character POS path: each POS in a pathis clustered to a category defined by itsfirst characterPOS Pattern (string of POS tags) of allcandidatesSingle Character POS Pattern of all candidatesTable 2: Features used for semantic role labeling2.4 Semantic Dependency RelationClassificationThis stage assigns the final argument labels to theargument candidates supplied from the previousstage.
A multi-class classifier is trained to classifythe types of the arguments supplied by the previousstage.
Table 2 lists the features used.
It is clear thatthe general type of features used here is stronglybased on previous work on the SRL task (Gildeaand Jurafsky, 2002; Pradhan et al, 2005; Xue andPalmer, 2004).
Different from CoNLL-2005, thesense of predicates should be labeled as a part ofthe task.
Our system assigns 01 to all predicates.This is a harsh tactic since it do not take the lin-guistic meaning of the argument-structure into ac-count.2.5 Semantic Dependency Relation InferenceThe purpose of inference stage is to incorporatesome prior linguistic and structural knowledge,such as ?each predicate takes at most one argumentof each type.?
We use the inference process intro-244duced by (Punyakanok et al, 2004; Koomen et al,2005).
The process is modeled as an integer Lin-ear Programming Problem (ILP).
It takes the pre-dicted probability over each type of the argumentsas inputs, and takes the optimal solution that max-imizes the linear sum of the probability subject tolinguistic constraints as outputs.
The constraintsare a subset of constraints raised by Koomen et al(2005) and encoded as following: 1) No overlap-ping or embedding arguments; 2) No duplicate ar-gument classes for A0-A5; 3) If there is an R-argargument, then there has to be an arg argument;4) If there is a C-arg argument, there must be anarg argument; moreover, the C-arg argument mustoccur after arg; 5) Given the predicate, some argu-ment types are illegal.
The list of illegal argumenttypes is extracted from framefile.The ILP process can improve SRL performanceon constituent-based parsing (Punyakanok et al,2004).
In our experiment, it also works ondependency-based parsing.3 Bilayer Maximum Entropy MarkovModels3.1 SequentializationThe sequentialization of a argument-structure is si-miliar to the pruning algorithm raised by (Xue andPalmer, 2004).
Given a constituent-based parsingtree, the recursive pruning process starts from a tar-get predicate.
It first collects the siblings of thepredicate; then it moves to the parent of the pred-icate, and collects the siblings of the parent.
Inaddition, if a constituent is a prepositional phrase,its children are also collected.Our system uses a similar pruning algorithm tofilter out very unlikely argument candidates in adependency-based parsing tree.
Given a depen-dency parsing tree, the pruning process also startsfrom a target predicate.
It first collects the depen-dents of the predicate; then it moves to the parentof the predicate, and collects all the dependentsagain.
Note that, the predicate is also taken intoaccount.
If the target predicate is a verb, the pro-cess goes on recursively until it reaches the root.The process of a noun target ends when it sees aPMOD, NMOD, SBJ or OBJ dependency relation.If a preposition is returned as a candidate, its childis also collected.
When the predicate is a verb, theset of constituents headed by survivors of our prun-ing algorithm is a superset of the set of survivors ofthe previous pruning algorithm on the correspond-Figure 1: Directed graphical Model of The systeming constituent-based parsing tree.
This pruningalgorithm will recall 99.08% arguments of verbs,and the candidates are 3.75 times of the real argu-ments.
If the stop relation such as PMOD of a nounis not taken into account, the recall is 97.67% andthe candidates is 6.28 times of arguments.
If theharsh stop condition is implemented, the recall isjust 80.29%.
Since the SRL performance of nounsis very low, the harsh pruning algorithm works bet-ter than the original one.After pruning, our system sequentializes all ar-gument candidates of the target predicate accord-ing to their linear order in the given sentence.3.2 Graphical ModelFigure 1 is the directed graph of our system.There is a chain of candidates x = (x0=BOS, x1, ..., xn) in the graph which are observa-tions.
There are two tag layers in the graph: the uplayer is information of semantic dependency rela-tions; the down layer is information of syntacticdependency relations.Given x, denote the corresponding syntactic de-pendency relations d = (d0= BOS, d1, ..., dn)and the corresponding semantic dependency rela-tions s = (s0= BOS, s1, ..., sn).
Our systemlabels the syntactic and semantic relations accord-ing to the conditional probability in argmax fla-vor.
Formally, labels the system assigned makethe score p(d, s|x) reaches its maximum.
We de-compose the probability p(d, s|x) according to thedirected graph modeled as following:p(d, s|x) = p(s1|s0, d1;x)p(d1|s0, d0;x) ?
?
?p(si+1|si, di+1;x)p(di+1|si, di;x) ?
?
?p(sn|sn?1, dn;x)p(dn|sn?1, dn?1;x)=n?i=1p(si|si?1, di;x)p(di|si?1, di?1;x)245Lemma, POS tag voice of predicatesPOS pattern of predicate?s childrenLemma and POS tag of the candidateLemma and POS of Neighboring words of thecandidateLemma and POS of sibling words of thecandidateLength of the constituent headed by thecandidateLemma and POS of the left and right mostwords of the constituent of the candidateConjunction of lemma of candidates andpredicates; Conjunction of POS of candidatesand predicatesPOS Pattern of all candidatesTable 3: Features used to predict syntactic depen-dency parsing3.3 Probability EstimationThe system defines the conditional probabilityp(si|si?1, di;x) and p(di|si?1, di?1;x) by usingthe maximum entropy (Berger et al, 1996) frame-work Denote the tag set of syntactic dependencyrelations D and the tag set of semantic dependencyrelations S. Formally, given a feature map ?sanda weight vector ws,pws(si|si?1, di;x) =exp{ws?
?s(x, si, si?1, di)}Zx,si?1,di;wswhere,Zx,si?1,di;ws=?s?Sexp{ws?
?s(x, s, si?1, di)}Similarly, given a feature map ?danda weight vector wd, (pwd(di) is short forpwd(di|si?1, di?1;x)pwd(di) =exp{wd?
?d(x, di, si?1, di?1)}Zx,si?1,di?1;wdwhere,Zx,si?1,di?1;wd=?d?Dexp{wd?
?d(x, d, si?1, di?1)}For different characteristic properties betweensyntactic parsing and semantic parsing, differentfeature maps are taken into account.
Table 2lists the features used to predict semantic depen-dency relations, whereas table 3 lists the featuresused to predict the syntactic dependency relations.The features used for syntactic dependency rela-tion classification are strongly based on previousworks (McDonald et al, 2006; Nakagawa, 2007).We just integrate syntactic dependency Rela-tion classification and semantic dependency rela-tion here.
If one combines identification and clas-sification of semantic roles as one multi-class clas-sification, the tag set of the second layer can besubstituted by the tag set of semantic roles plus aNULL (?not an argument?)
label.3.4 InferenceThe ?argmax problem?
in structured prediction isnot tractable in the general case.
However, the bi-layer graphical model presented in form sectionsadmits efficient search using dynamic program-ming solution.
Searching for the highest probabil-ity of a graph depends on the factorization chosen.According to the form of the global scorep(d, s|x) =n?i=1p(si|si?1, di;x)p(di|si?1, di?1;x), we define forward probabilities ?t(s, d) to be theprobability of semantic relation being s and syn-tactic relation being d at time t given observationsequence up to time t. The recursive dynamic pro-gramming step is?t+1(d, s) = arg maxd?D,s?S?d??D,s?
?S?t(d?, s?)
?p(si|si?1, di;x)p(di|si?1, di?1;x)Finally, to compute the globally most proba-ble assignment (?d,?s) = argmaxd,sp(d, s|x), aViterbi recursion works well.4 ResultsWe trained our system using positive examplesextracted from all training data of CoNLL 2008shared task.
Table 4 shows the overall syntacticparsing results obtained on the WSJ test set (Sec-tion 23) and the Brown test set (Section ck/01-03).Table 5 shows the overall semantic parsing resultsobtained on the WSJ test set (Section 23) and theBrown test set (Section ck/01-03).246Test Set UAS LAS Label AccuracyWSJ 89.25% 86.37% 91.25%Brown 86.12% 80.75% 87.14%Table 4: Overall syntactic parsing resultsTask Precision Recall F?=1WSJ ID 73.76% 85.24% 79.08ID&CL 63.07% 72.88% 67.62Brown ID 70.77% 80.50% 75.32ID&CL 54.74% 62.26% 58.26Table 5: Overall semantic parsing resultsTest WSJ Precision(%) Recall(%) F?=1SRL of VerbsAll 73.53 73.28 73.41Core-Arg 78.83 76.93 77.87AM-* 62.51 64.83 63.65SRL of NounsAll 62.06 45.49 52.50Core-Arg 61.47 46.56 52.98AM-* 66.19 39.93 49.81Table 6: Semantic role labeling results on verbsand nouns.
Core-Arg means numbered argument.Table 6 shows the detailed semantic parsing re-sults obtained on the WSJ test set (Section 23)of verbs and nouns respectively.
The comparisonsuggests that SRL on NomBank is much harderthan PropBank.AcknowlegementsThe work is supported by the National NaturalScience Foundation of China under Grants No.60503071, 863 the National High Technology Re-search and Development Program of China un-der Grants No.2006AA01Z144, and the Project ofToshiba (China) Co., Ltd. R&D Center.ReferencesBerger, Adam, Stephen Della Pietra, and Vincent DellaPietra.
1996.
A Maximum Entropy Approach toNatural Language Processing.
Computional Lin-guistics, 22(1):39?71.Ge, Ruifang and Raymond J. Mooney.
2005.
A Statis-tical Semantic Parser that Integrates Syntax and Se-mantics.
In Proceedings of the Conference of Com-putational Natural Language Learning.Gildea, Daniel and Daniel Jurafsky.
2002.
AutomaticLabeling of Semantic Roles.
Computional Linguis-tics, 28(3):245?288.Koomen, Peter, Vasina Punyakanok, Dan Roth, andWen-tau Yih.
2005.
Generalized Inference withMultiple Semantic Role Labeling Systems.
In Pro-ceedings of Conference on Natural Language Learn-ing.McCallum, Andrew, Dayne Freitag, and FernandoPereira.
2000.
Maximum Entropy Markov Mod-els for Information Extraction and Segmentation.In Proceedings of International Conference on Ma-chine Learning.McDonald, Ryan, Fernando Pereira, Kiril Ribarov, andJan Haji?c.
2005.
Non-projective dependency pars-ing using spanning tree algorithms.
In Proceedingsof the conference on Human Language Technologyand Empirical Methods in Natural Language Pro-cessing.McDonald, Ryan, Kevin Lerman, and FernandoPereira.
2006.
Multilingual Dependency Analysiswith a Two-Stage Discriminative Parser.
In Proceed-ings of Conference on Natural Language Learning.Nakawa, Tetsuji.
2007.
Multilingual DependencyParsing using Global Features.
In Proceedings ofConference on Natural Language Learning.Nivre, Joakim, Johan Hall, Sandra K?ubler, Ryan Mc-Donald, Jens Nilsson, Sebastian Riedel, and DenizYuret.
The CoNLL 2007 Shared Task on Depen-dency Parsing.
2007.
In Proceedings of the CoNLLShared Task Session of EMNLP-CoNLL 2007, 915?932,Pradhan, Sameer, Kadri Hacioglu, Valerie Krugler,Wayne Ward, James Martin, and Daniel Jurafsky.2005.
Support Vector Learning for Semantic Argu-ment Classification.
In Proceedings of Conferenceon Association for Computational Linguistics.Punyakanok, Vasin , Dan Roth, Wen-tau Yih, and DavZimak.
2004.
Semantic Role Labeling via IntegerLinear Programming Inference.
In Proceedings ofthe 20th International Conference on ComputationalLinguistics.Surdeanu, Mihai, Richard Johansson, Adam Meyers,Llu?
?s M`arquez, and Nivre, Joakim.
2008.
TheCoNLL-2008 Shared Task on Joint Parsing of Syn-tactic and Semantic Dependencies.
In Proceedingsof the 12th Conference on Computational NaturalLanguage Learning (CoNLL-2008).Xue, Nianwen and Martha Palmer.
2004.
Calibrat-ing Features for Semantic Role Labeling.
In Pro-ceedings of Empirical Methods in Natural LanguageProcessing.Yi, Szu-ting and Martha Palmer.
2005.
The Integra-tion of Syntactic Parsing and Semantic Role Label-ing.
In Proceedings of the Conference of Computa-tional Natural Language Learning.247
