Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 284?292,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsModeling Perspective using Adaptor GrammarsEric A. HardistyDepartment of Computer Scienceand UMIACSUniversity of MarylandCollege Park, MDhardisty@cs.umd.eduJordan Boyd-GraberUMD iSchooland UMIACSUniversity of MarylandCollege Park, MDjbg@umiacs.umd.eduPhilip ResnikDepartment of Linguisticsand UMIACSUniversity of MarylandCollege Park, MDresnik@umd.eduAbstractStrong indications of perspective can oftencome from collocations of arbitrary length; forexample, someone writing get the governmentout of my X is typically expressing a conserva-tive rather than progressive viewpoint.
How-ever, going beyond unigram or bigram featuresin perspective classification gives rise to prob-lems of data sparsity.
We address this prob-lem using nonparametric Bayesian modeling,specifically adaptor grammars (Johnson et al,2006).
We demonstrate that an adaptive na?
?veBayes model captures multiword lexical usagesassociated with perspective, and establishes anew state-of-the-art for perspective classifica-tion results using the Bitter Lemons corpus, acollection of essays about mid-east issues fromIsraeli and Palestinian points of view.1 IntroductionMost work on the computational analysis of senti-ment and perspective relies on lexical features.
Thismakes sense, since an author?s choice of words isoften used to express overt opinions (e.g.
describinghealthcare reform as idiotic or wonderful) or to framea discussion in order to convey a perspective moreimplicitly (e.g.
using the term death tax instead ofestate tax).
Moreover, it is easy and efficient to rep-resent texts as collections of the words they contain,in order to apply a well known arsenal of supervisedtechniques (Laver et al, 2003; Mullen and Malouf,2006; Yu et al, 2008).At the same time, standard lexical features havetheir limitations for this kind of analysis.
Such fea-tures are usually created by selecting some smalln-gram size in advance.
Indeed, it is not uncommonto see the feature space for sentiment analysis limitedto unigrams.
However, important indicators of per-spective can also be longer (get the government outof my).
Trying to capture these using standard ma-chine learning approaches creates a problem, sinceallowing n-grams as features for larger n gives riseto problems of data sparsity.In this paper, we employ nonparametric Bayesianmodels (Orbanz and Teh, 2010) in order to addressthis limitation.
In contrast to parametric models, forwhich a fixed number of parameters are specified inadvance, nonparametric models can ?grow?
to thesize best suited to the observed data.
In text analysis,models of this type have been employed primarilyfor unsupervised discovery of latent structure ?
forexample, in topic modeling, when the true number oftopics is not known (Teh et al, 2006); in grammaticalinference, when the appropriate number of nontermi-nal symbols is not known (Liang et al, 2007); andin coreference resolution, when the number of enti-ties in a given document is not specified in advance(Haghighi and Klein, 2007).
Here we use them forsupervised text classification.Specifically, we use adaptor grammars (Johnsonet al, 2006), a formalism for nonparametric Bayesianmodeling that has recently proven useful in unsuper-vised modeling of phonemes (Johnson, 2008), gram-mar induction (Cohen et al, 2010), and named entitystructure learning (Johnson, 2010), to make super-vised na?
?ve Bayes classification nonparametric inorder to improve perspective modeling.
Intuitively,na?
?ve Bayes associates each class or label with aprobability distribution over a fixed vocabulary.
Weintroduce adaptive na?
?ve Bayes (ANB), for which inprinciple the vocabulary can grow as needed to in-clude collocations of arbitrary length, as determined284by the properties of the dataset.
We show that usingadaptive na?
?ve Bayes improves on state of the artclassification using the Bitter Lemons corpus (Linet al, 2006), a document collection that has beenused by a variety of authors to evaluate perspectiveclassification.In Section 2, we review adaptor grammars, showhow na?
?ve Bayes can be expressed within the for-malism, and describe how ?
and how easily ?
anadaptive na?
?ve Bayes model can be created.
Section 3validates the approach via experimentation on the Bit-ter Lemons corpus.
In Section 4, we summarize thecontributions of the paper and discuss directions forfuture work.2 Adapting Na?
?ve Bayes to be Less Na?
?veIn this work we apply the adaptor grammar formal-ism introduced by Johnson, Griffiths, and Goldwa-ter (Johnson et al, 2006).
Adaptor grammars are ageneralization of probabilistic context free grammars(PCFGs) that make it particularly easy to express non-parametric Bayesian models of language simply andreadably using context free rules.
Moreover, John-son et al provide an inference procedure based onMarkov Chain Monte Carlo techniques that makesparameter estimation straightforward for all modelsthat can be expressed using adaptor grammars.1 Vari-ational inference for adaptor grammars has also beenrecently introduced (Cohen et al, 2010).Briefly, adaptor grammars allow nonterminals tobe rewritten to entire subtrees.
In contrast, a non-terminal in a PCFG rewrites only to a collectionof grammar symbols; their subsequent productionsare independent of each other.
For instance, a tradi-tional PCFG might learn probabilities for the rewriterule PP 7?
P NP.
In contrast, an adaptor gram-mar can learn (or ?cache?)
the production PP 7?
(P up)(NP(DET a)(N tree)).
It does this by posit-ing that the distribution over children for an adaptednon-terminal comes from a Pitman-Yor distribution.A Pitman-Yor distribution (Pitman and Yor, 1997)is a distribution over distributions.
It has three pa-rameters: the discount, a, such that 0 ?
a < 1,the strength, b, a real number such that ?a < b,1And, better still, they provide code thatimplements the inference algorithm; seehttp://www.cog.brown.edu/ mj/Software.htm.and a probability distribution G0 known as the basedistribution.
Adaptor grammars allow distributionsover subtrees to come from a Pitman-Yor distribu-tion with the PCFG?s original distribution over treesas the base distribution.
The generative process forobtaining draws from a distribution drawn from aPitman-Yor distribution can be described by the ?Chi-nese restaurant process?
(CRP).
We will use the CRPto describe how to obtain a distribution over obser-vations composed of sequences of n-grams, the keyto our model?s ability to capture perspective-bearingn-grams.Suppose that we have a base distribution ?
that issome distribution over all sequences of words (theexact structure of such a distribution is unimportant;such a distribution will be defined later in Table 1).Suppose further we have a distribution ?
drawn fromPY (a, b,?
), and we wish to draw a series of obser-vations w from ?.
The CRP gives us a generativeprocess for doing those draws from ?, marginaliz-ing out ?.
Following the restaurant metaphor, weimagine the ith customer in the series entering therestaurant to take a seat at a table.
The customer sitsby making a choice that determines the value of then-gram wi for that customer: she can either sit at anexisting table or start a new table of her own.2If she sits at a new table j, that table is assigneda draw yj from the base distribution, ?
; note that,since ?
is a distribution over n-grams, yj is an n-gram.
The value of wi is therefore assigned to be yj ,and yj becomes the sequence of words assigned tothat new table.
On the other hand, if she sits at anexisting table, then wi simply takes the sequence ofwords already associated with that table (assigned asabove when it was first occupied).The probability of joining an existing table j,with cj patrons already seated at table j, iscj?ac?+b,where c?
is the number of patrons seated at all tables:c?
=?j?
cj?
.
The probability of starting a new tableis b+t?ac?+b , where t is the number of tables presentlyoccupied.Notice that ?
is a distribution over the same spaceas ?, but it can drastically shift the mass of the dis-tribution, compared with ?, as more and more pa-2Note that we are abusing notation by allowing wi to cor-respond to a word sequence of length ?
1 rather than a singleword.285trons are seated at tables.
However, there is alwaysa chance of drawing from the base distribution, andtherefore every word sequence can also always bedrawn from ?.In the next section we will write a na?
?ve Bayes-likegenerative process using PCFGs.
We will then usethe PCFG distribution as the base distribution for aPitman-Yor distribution, adapting the na?
?ve Bayesprocess to give us a distribution over n-grams, thuslearning new language substructures that are usefulfor modeling the differences in perspective.2.1 Classification Models as PCFGsNa?
?ve Bayes is a venerable and popular mechanismfor text classification (Lewis, 1998).
It posits thatthere are K distinct categories of text ?
each with adistinct distribution over words ?
and that every doc-ument, represented as an exchangeable bag of words,is drawn from one (and only one) of these distribu-tions.
Learning the per-category word distributionsand global prevalence of the classes is a problem ofposterior inference which can be approached using avariety of inference techniques (Lowd and Domingos,2005).More formally, na?
?ve Bayes models can be ex-pressed via the following generative process:31.
Draw a global distribution over classes ?
?Dir (?)2.
For each class i ?
{1, .
.
.
,K}, draw a worddistribution ?i ?
Dir (?)3.
For each document d ?
{1, .
.
.
,M}:(a) Draw a class assignment zd ?
Mult (?
)(b) For each word position n ?
{1, .
.
.
, Nd,draw wd,n ?
Mult (?zd)A variant of the na?
?ve Bayes generative process canbe expressed using the adaptor grammar formalism(Table 1).
The left hand side of each rule representsa nonterminal which can be expanded, and the righthand side represents the rewrite rule.
The rightmostindices show replication; for instance, there are |V |rules that allow WORDi to rewrite to each word in the3Here ?
and ?
are hyperparameters used to specify priorsfor the class distribution and classes?
word distributions, respec-tively; ?
is a symmetric K-dimensional vector where each ele-ment is pi.
Nd is the length of document d. Resnik and Hardisty(2010) provide a tutorial introduction to the na?
?ve Bayes genera-tive process and underlying concepts.SENT 7?
DOCd d = 1, .
.
.
,mDOCd0.001 7?
IDd WORDSi d = 1, .
.
.
,m;i ?
{1,K}WORDSi 7?
WORDSi WORDi i ?
{1,K}WORDSi 7?
WORDi i ?
{1,K}WORDi 7?
v v ?
V ; i ?
{1,K}Table 1: A na?
?ve Bayes-inspired model expressed as aPCFG.vocabulary.
One can assume a symmetric Dirichletprior of Dir (1?)
over the production choices unlessotherwise specified ?
as with the DOCd productionrule above, where a sparse prior is used.Notice that the distribution over expansions forWORDi corresponds directly to ?i in Figure 1(a).There are, however, some differences between themodel that we have described above and the standardna?
?ve Bayes model depicted in Figure 1(a).
In par-ticular, there is no longer a single choice of class perdocument; each sentence is assigned a class.
If thedistribution over per-sentence labels is sparse (as itis above for DOCd), this will closely approximatena?
?ve Bayes, since it will be very unlikely for thesentences in a document to have different labels.
Anon-sparse prior leads to behavior more like modelsthat allow parts of a document to express sentimentor perspective differently.2.2 Moving Beyond the Bag of WordsThe na?
?ve Bayes generative distribution posits thatwhen writing a document, the author selects a distri-bution of categories zd for the document from ?.
Theauthor then generates words one at a time: each wordis selected independently from a flat multinomialdistribution ?zd over the vocabulary.However, this is a very limited picture of how textis related to underlying perspectives.
Clearly wordsare often connected with each other as collocations,and, just as clearly, extending a flat vocabulary toinclude bigram collocations does not suffice, sincesometimes relevant perspective-bearing phrases arelonger than two words.
Consider phrases like healthcare for all or government takeover of health care,connected with progressive and conservative posi-tions, respectively, during the national debate onhealthcare reform.
Simply applying na?
?ve Bayes,or any other model, to a bag of n-grams for high n is286KM?zdNdWd,n??
?i(a) Na?
?ve BayesKM?zdNdWd,na??ib??
(b) Adaptive Na?
?ve BayesFigure 1: A plate diagram for na?
?ve Bayes and adaptive na?
?ve Bayes.
Nodes represent random variables and parameters;shaded nodes represent observations; lines represent probabilistic dependencies; and the rectangular plates denotereplication.going to lead to unworkable levels of data sparsity;a model should be flexible enough to support bothunigrams and longer phrases as needed.Following Johnson (2010), however, we can useadaptor grammars to extend na?
?ve Bayes flexibly toinclude richer structure like collocations when theyimprove the model, and not including them whenthey do not.
This can be accomplished by introduc-ing adapted nonterminal rules: in a revised genera-tive process, the author can draw from Pitman-Yordistribution whose base distribution is over word se-quences of arbitrary length.4 Thus in a setting where,say, K = 2, and our two classes are PROGRESSIVEand CONSERVATIVE, the sequence health care for allmight be generated as a single unit for the progressiveperspective, but in the conservative perspective thesame sequence might be generated as three separatedraws: health care, for, all.
Such a model is pre-sented in Figure 1(b).
Note the following differencesbetween Figures 1(a) and 1(b):?
zd selects which Pitman-Yor distribution to drawfrom for document d.?
?i is the distribution over n-grams that comesfrom the Pitman-Yor distribution.?
Wd,n represents an n-gram draw from ?i?
a, b are the Pitman-Yor strength and discountparameters.?
?
is the Pitman-Yor base distribution with ?
asits uniform hyperparameter.4As defined above, the base distribution is that of the PCFGproduction rule WORDSi.
Although it has non-zero probabilityof producing any sequence of words, it is biased toward shorterword sequences.Returning to the CRP metaphor discussed when weintroduced the Pitman-Yor distribution, there are tworestaurants, one for the PROGRESSIVE distributionand one for the CONSERVATIVE distribution.
Healthcare for all has its own table in the PROGRESSIVErestaurant, and enough people are sitting at it to makeit popular.
There is no such table in the CONSERVA-TIVE restaurant, so in order to generate those words,the phrase health care for all would need to comefrom a new table; however, it is more easily explainedby three customers sitting at three existing, populartables: health care, for, and all.We follow the convention of Johnson (2010) bywriting adapted nonterminals as underlined.
Thegrammar for adaptive na?
?ve Bayes is shown in Ta-ble 2.
The adapted COLLOCi rule means that everytime we need to generate that nonterminal, we areactually drawing from a distribution drawn from aPitman-Yor distribution.
The distribution over thepossible yields of the WORDSi rule serves as thebase distribution.Given this generative process for documents, wecan now use statistical inference to uncover the pos-terior distribution over the latent variables, thus dis-covering the tables and seating assignments of ourmetaphorical restaurants that each cater to a specificperspective filled with tables populated by words andn-grams.The model presented in Table 2 is the most straight-forward way of extending na?
?ve Bayes to collocations.For completeness, we also consider the alternativeof using a shared base distribution rather than dis-tinguishing the base distributions of the two classes.287SENT 7?
DOCd d = 1, .
.
.
,mDOCd0.001 7?
IDd SPANi d = 1, .
.
.
,m;i ?
{1,K}SPANi 7?
SPANi COLLOCi i ?
{1,K}SPANi 7?
COLLOCi i ?
{1,K}COLLOCi 7?
WORDSi i ?
{1,K}WORDSi 7?
WORDSi WORDi i ?
{1,K}WORDSi 7?
WORDi i ?
{1,K}WORDi 7?
v v ?
V ; i ?
{1,K}Table 2: An adaptive na?
?ve Bayes grammar.
TheCOLLOCi nonterminal?s distribution over yields is drawnfrom a Pitman-Yor distribution rather than a Dirichlet overproduction rules.SENT 7?
DOCd d = 1, .
.
.
,mDOCd0.001 7?
IDd SPANi d = 1, .
.
.
,m;i ?
{1,K}SPANi 7?
SPANi COLLOCi i ?
{1,K}SPANi 7?
COLLOCi i ?
{1,K}COLLOCi 7?
WORDS i ?
{1,K}WORDS 7?
WORDS WORDWORDS 7?
WORDWORD 7?
v v ?
VTable 3: An adaptive na?
?ve Bayes grammar with a com-mon base distribution for collocations.
Note that, in con-trast to Table 2, there are no subscripts on WORDS orWORD.Briefly, using a shared base distribution posits thatthe two classes use similar word distributions, butgenerate collocations unique to each class, whereasusing separate base distributions assumes that thedistribution of words is unique to each class.3 Experiments3.1 Corpus DescriptionWe conducted our classification experiments on theBitter Lemons (BL) corpus, which is a collection of297 essays averaging 700-800 words in length, onvarious Middle East issues, written from both theIsraeli and Palestinian perspectives.
The BL corpuswas compiled by Lin et al (2006) and is derived froma website that invites weekly discussions on a topicand publishes essays from two sets of authors eachweek.5 Two of the authors are guests, one from eachperspective, and two essays are from the site?s regularcontributors, also one from each perspective, for a5http://www.bitterlemons.orgKM?zdNdWd,na??ib?
?Figure 2: An alternative adaptive na?
?ve Bayes with a com-mon base distribution for both classes.Training SetTest SetCorpus FilterGrammarGeneratorCorpus FilterVocabularyGeneratorAG ClassifierFigure 3: Corpus preparation and experimental setup.total of four essays on each topic per week.
We chosethis corpus to allow us to directly compare our resultswith Greene and Resnik?s (2009) Observable Proxiesfor Underlying Semantics (OPUS) features and Linet al?s Latent Sentence Perspective Model (LSPM).The classification goal for this corpus is to label eachdocument with the perspective of its author, eitherIsraeli or Palestinian.Consistent with prior work, we prepared the corpusby dividing it into two groups, one group containingall of the essays written by the regular site contrib-utors, which we call the Editor set, and one groupcomprised of all the essays written by the guest con-tributors, which we call the Guest set.
Similar to theabove mentioned prior work, we perform classifica-tion using one group as training data and the other astest data and perform two folds of classification.
Theoverall experimental setup and corpus preparationprocess is presented in Figure 3.2883.2 Experimental SetupThe vocabulary generator determines the vocabularyused by a given experiment by converting the trainingset to lower case, stemming with the Porter stemmer,and filtering punctuation.
We remove from the vocab-ulary any words that appeared in only one documentregardless of frequency within that document, wordswith frequencies lower than a threshold, and stopwords.6 The vocabulary is then passed to a grammargenerator and a corpus filter.The grammar generator uses the vocabulary to gen-erate the terminating rules of the grammar from theANB grammar presented in Tables 2 and 3.
The cor-pus filter takes in a set of documents and replaces allwords not in the vocabulary with ?out of vocabulary?markers.
This process ensures that in all experimentsthe vocabulary is composed entirely of words fromthe training set.
After the groups have been filtered,the group used as the test set has its labels removed.The test and training set are then sent, along with thegrammar, into the adaptor grammar inference engine.Each experiment ran for 3000 iterations.
For theruns where adaptation was used we set the initialPitman-Yor a and b parameters to 0.01 and 10 respec-tively, then slice sample (Johnson and Goldwater,2009).We use the resulting sentence parses for classifi-cation.
By design of the grammar, each sentence?swords will belong to one and only one distribution.We identify that distribution from each of the testset sentence parses and use it as the sentence levelclassification for that particular sentence.
We thenuse majority rule on the individual sentence classifi-cations in a document to obtain the document classifi-cation.
(In most cases the sentence-level assignmentsare overwhelmingly dominated by one class.
)3.3 Results and AnalysisTable 4 gives the results and compares to priorwork.
The support vector machine (SVM), NB-B and LSPM results are taken directly from Linet al (2006).
NB-B indicates na?
?ve Bayes withfull Bayesian inference.
LSPM is the LatentSentence Perspective Model, also from Lin etal.
(2006).
OPUS results are taken from Greene6In these experiments, a frequency threshold of 4 was se-lected prior to testing.Training Set Test Set Classifier AccuracyGuests Editors SVM 88.22Guests Editors NB-B 93.46Guests Editors LSPM 94.93Guests Editors OPUS 97.64Guests Editors ANB* 99.32Guests Editors ANB Com 99.93Guests Editors ANB Sep 99.87Editors Guests SVM 81.48Editors Guests NB-B 85.85Editors Guests LSPM 86.99Editors Guests OPUS 85.86Editors Guests ANB* 84.98Editors Guests ANB Com 82.76Editors Guests ANB Sep 88.28Table 4: Classification results.
ANB* indicates the samegrammar as Adapted Na?
?ve Bayes, but with adaptation dis-abled.
Com and Sep refer to whether the base distributionwas common to both classes or separate.and Resnik (2009).
Briefly, OPUS features are gener-ated from observable grammatical relations that comefrom dependency parses of the corpus.
Use of thesefeatures provided the best classification accuracy forthis task prior to this work.
ANB* refers to the gram-mar from Table 2, but with adaptation disabled.
Thereported accuracy values for ANB*, ANB with acommon base distribution (see Table 3), and ANBwith separate base distributions (see Table 2) arethe mean values from five separate sampling chains.Bold face indicates statistical signficance (p < 0.05)by unpaired t-test between the reported value andANB*.Consistent with all prior work on this corpus wefound that the classification accuracy for training oneditors and testing on guests was lower than the otherdirection since the larger number of editors in theguest set alows for greater generalization.
The dif-ference between ANB* and ANB with a commonbase distribution is not statistically significant.
Alsoof note is that the classification accuracy improvesfor testing on Guests when the ANB grammar is al-lowed to adapt and a separate base distribution is usedfor the two classes (88.28% versus 84.98% withoutadaptation).Table 5 presents some data on adapted rules289Unique Unique Percent of GroupClass Group Unigrams Cached n-grams Cached Vocabulary CachedIsraeli Editors 2,292 19,614 77.62Palestinian Editors 2,180 17,314 86.54Israeli Guests 2,262 19,398 79.91Palestinian Guests 2,005 16,946 74.94Table 5: Counts of cached unigrams and n-grams for the two classes compared to the vocabulary sizes.Israeli Palestinianzionist dream american jewzionist state achieve freedomzionist movement palestinian freedomamerican leadership support palestinianamerican victory palestinian sufferabandon violence palestinian territoryfreedom (of the) press palestinian statehoodpalestinian violence palestinian refugeeTable 6: Charged bigrams captured by the framework.learned once inference is complete.
The columnlabeled unique unigrams cached indicates the num-ber of unique unigrams that appear on the right handside of the adapted rules.
Similarly, unique n-gramscached indicates the number of unique n-grams thatappear on the right hand side of the adapted rules.The rightmost column indicates the percentage ofterms from the group vocabulary that appear on theright hand side of adapted rules as unigrams.
Valuesless than 100% indicate that the remaining vocabu-lary terms are cached in n-grams.
As the table shows,a significant number of the rules learned during infer-ence are n-grams of various sizes.Inspection of the captured bigrams showed thatit captured sequences that a human might associatewith one perspective over the other.
Table 6 lists justa few of the more charged bigrams that were capturedin the adapted rules.More specific caching information on the individ-ual groups and classes is provided in Table 7.
Thisdata clearly demonstrates that raw n-gram frequencyalone is not indicative of how many times an n-gramis used as a cached rule.
For example, consider thebigram people go, which is used as a cached bigramonly three times, yet appears in the corpus 407 times.Compare that with isra palestinian, which is cachedthe same number of times but appears only 18 timesin the corpus.
In other words, the sequence people gois more easily explained by two sequential unigrams,not a bigram.
The ratio of cache use counts to rawbigrams gives a measure of strength of collocationbetween the terms of the n-gram.
We conjecture thatthe rareness of caching for n > 2 is a function of thesmall corpus size.
Also of note is the improvement inperformance of ANB* over NB-B when training onguests, which we suspect is due to our use of sampledversus fixed hyperparameters.4 ConclusionsIn this paper, we have applied adaptor grammars ina supervised setting to model lexical properties oftext and improve document classification accordingto perspective, by allowing nonparametric discoveryof collocations that aid in perspective classification.The adaptive na?
?ve Bayes model improves on stateof the art supervised classification performance inhead-to-head comparisons with previous approaches.Although there have been many investigations onthe efficacy of using multiword collocations in textclassification (Bekkerman and Allan, 2004), usuallysuch approaches depend on a preprocessing step suchas computing tf-idf or other measures of frequencybased on either word bigrams (Tan et al, 2002) orcharacter n-grams (Raskutti et al, 2001).
In con-trast, our approach combines phrase discovery withthe probabilistic model of the text.
This allows forthe collocation selection and classification to be ex-pressed in a single model, which can then be extendedlater; it also is truly generative, as compared with fea-ture induction and selection algorithms that eitherunder- or over-generate data.There are a number of interesting directions inwhich to take this research.
As Johnson et al (2006)argue, and as we have confirmed here, the adaptor290Guest EditorIsraeli Palestinian Israeli Palestinianpalestinian OOV 11 299 palestinian isra 6 178 palestinian OOV 8 254 OOV israel 7 198OOV palestinian 7 405 OOV palestinian 6 405 OOV palestinian 7 319 OOV palestinian 6 319isra OOV 6 178 palestinian OOV 5 29 OOV israel 7 123 OOV work 5 254israel OOV 6 94 one OOV 4 25 OOV us 6 115 OOV agreement 5 75sharon OOV 4 74 side OOV 3 21 OOV part 5 56 palestinian reform 4 49polit OOV 4 143 polit OOV 3 299 israel OOV 5 81 palestinian OOV 4 81OOV us 4 29 peopl go 3 407 attempt OOV 5 91 OOV isra 4 15OOV state 4 37 palestinian govern 3 94 time OOV 4 63 one OOV 4 27israel palestinian 4 52 palestinian accept 3 220 remain OOV 4 85 isra palestinian 4 17even OOV 4 43 OOV state 3 150 OOV time 4 70 isra OOV 4 63arafat OOV 4 41 OOV israel 3 18 OOV area 4 49 howev OOV 4 149appear OOV 4 53 OOV end 3 20 OOV arafat 4 28 want OOV 3 36total OOV 3 150 OOV act 3 105 isra OOV 4 8 us OOV 3 35palestinian would 3 65 isra palestinian 3 18 would OOV 3 28 recent OOV 3 220palestinian isra 3 35 israel OOV 3 198 use OOV 3 198 palestinian isra 3 115Table 7: Most frequently used cached bigrams.
The first colum in each section is the number of times that bigram wasused as a cached rule.
The second column indicates the raw count of that bigram in the Guests or Editors group.grammar formalism makes it quite easy to work withlatent variable models, in order to automatically dis-cover structures in the data that have predictive value.For example, it is easy to imagine a model where inaddition to a word distribution for each class, thereis also an additional shared ?neutral?
distribution:for each sentence, the words in that sentence can ei-ther come from the class-specific content distributionor the shared neutral distribution.
This turns out tobe the Latent Sentence Perspective Model of Lin etal.
(2006), which is straightforward to encode usingthe adaptor grammar formalism simply by introduc-ing two new nonterminals to represent the neutraldistribution:SENT 7?
DOCd d = 1, .
.
.
,mDOCd 7?
IDd WORDSi d = 1, .
.
.
,m;i ?
{1,K}DOCd 7?
IDd NEUTS d = 1, .
.
.
,m;WORDSi 7?
WORDSi WORDi i ?
{1,K}WORDSi 7?
WORDi i ?
{1,K}WORDi 7?
v v ?
V ; i ?
{1,K}NEUT 7?
NEUTSi NEUTiNEUT 7?
NEUTNEUT 7?
v v ?
VRunning this grammar did not produce improvementsconsistent with those reported by Lin et al We plan toinvestigate this further, and a natural follow-on wouldbe to experiment with adaptation for this variety oflatent structure, to produce an adapted LSPM-likemodel analogous to adaptive na?
?ve Bayes.Viewed in a larger context, computational classi-fication of perspective is closely connected to socialscientists?
study of framing, which Entman (1993)characterizes as follows: ?To frame is to select someaspects of a perceived reality and make them moresalient in a communicating text, in such a way asto promote a particular problem definition, causalinterpretation, moral evaluation, and/or treatment rec-ommendation for the item described.?
Here and inother work (e.g.
(Laver et al, 2003; Mullen and Mal-ouf, 2006; Yu et al, 2008; Monroe et al, 2008)),it is clear that lexical evidence is one key to under-standing how language is used to frame discussionfrom one perspective or another; Resnik and Greene(2009) have shown that syntactic choices can pro-vide important evidence, as well.
Another promisingdirection for this work is the application of adaptorgrammar models as a way to capture both lexical andgrammatical aspects of framing in a unified model.AcknowledgmentsThis research was funded in part by the Army Re-search Laboratory through ARL Cooperative Agree-ment W911NF-09-2-0072 and by the Office of theDirector of National Intelligence (ODNI), Intelli-gence Advanced Research Projects Activity (IARPA),through the Army Research Laboratory.
All state-ments of fact, opinion or conclusions containedherein are those of the authors and should not be con-strued as representing the official views or policies291of ARL, IARPA, the ODNI, or the U.S. Government.The authors thank Mark Johnson and the anonymousreviewers for their helpful comments and discussions.We are particularly grateful to Mark Johnson for mak-ing his adaptor grammar code available.ReferencesR.
Bekkerman and J. Allan.
2004.
Using bigrams in textcategorization.
Technical Report IR-408, Center ofIntelligent Information Retrieval, UMass Amherst.S.
B. Cohen, D. M. Blei, and N. A. Smith.
2010.
Varia-tional inference for adaptor grammars.
In Conferenceof the North American Chapter of the Association forComputational Linguistics.R.M.
Entman.
1993.
Framing: Toward Clarification of aFractured Paradigm.
The Journal of Communication,43(4):51?58.Stephan Greene and Philip Resnik.
2009.
More thanwords: Syntactic packaging and implicit sentiment.
InConference of the North American Chapter of the Asso-ciation for Computational Linguistics, pages 503?511.Aria Haghighi and Dan Klein.
2007.
Unsupervised coref-erence resolution in a nonparametric bayesian model.In Proceedings of the 45th Annual Meeting of the Asso-ciation of Computational Linguistics, pages 848?855,Prague, Czech Republic, June.Mark Johnson and Sharon Goldwater.
2009.
Improvingnonparameteric bayesian inference: experiments onunsupervised word segmentation with adaptor gram-mars.
In Conference of the North American Chapter ofthe Association for Computational Linguistics, pages317?325, Boulder, Colorado, June.Mark Johnson, Thomas L. Griffiths, and Sharon Gold-water.
2006.
Adaptor grammars: A framework forspecifying compositional nonparametric Bayesian mod-els.
In Proceedings of Advances in Neural InformationProcessing Systems.Mark Johnson.
2008.
Using adaptor grammars to identifysynergies in the unsupervised acquisition of linguisticstructure.
In Proceedings of ACL-08: HLT, pages 398?406, Columbus, Ohio, June.Mark Johnson.
2010.
PCFGs, topic models, adaptor gram-mars and learning topical collocations and the structureof proper names.
In Proceedings of the Association forComputational Linguistics, pages 1148?1157, Uppsala,Sweden, July.Michael Laver, Kenneth Benoit, and John Garry.
2003.Extracting policy positions from political texts usingwords as data.
American Political Science Review,pages 311?331.David D. Lewis.
1998.
Naive (bayes) at forty: The inde-pendence assumption in information retrieval.
In ClaireNe?dellec and Ce?line Rouveirol, editors, Proceedingsof ECML-98, 10th European Conference on MachineLearning, number 1398, pages 4?15, Chemnitz, DE.Springer Verlag, Heidelberg, DE.Percy Liang, Slav Petrov, Michael Jordan, and Dan Klein.2007.
The infinite PCFG using hierarchical Dirichletprocesses.
In Proceedings of Emperical Methods inNatural Language Processing, pages 688?697.Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and Alexan-der Hauptmann.
2006.
Which side are you on?
Identi-fying perspectives at the document and sentence levels.In Proceedings of the Conference on Natural LanguageLearning (CoNLL).Daniel Lowd and Pedro Domingos.
2005.
Naive bayesmodels for probability estimation.
In ICML ?05: Pro-ceedings of the 22nd international conference on Ma-chine learning, pages 529?536, New York, NY, USA.ACM.Burt L. Monroe, Michael P. Colaresi, and Kevin M. Quinn.2008.
Fightin?
Words: Lexical Feature Selection andEvaluation for Identifying the Content of Political Con-flict.
Political Analysis, Vol.
16, Issue 4, pp.
372-403,2008.Tony Mullen and Robert Malouf.
2006.
A preliminary in-vestigation into sentiment analysis of informal politicaldiscourse.
In AAAI Symposium on Computational Ap-proaches to Analysing Weblogs (AAAI-CAAW), pages159?162.P.
Orbanz and Y. W. Teh.
2010.
Bayesian nonparamet-ric models.
In Encyclopedia of Machine Learning.Springer.J.
Pitman and M. Yor.
1997.
The two-parameter Poisson-Dirichlet distribution derived from a stable subordinator.Annals of Probability, 25(2):855?900.Bhavani Raskutti, Herman L.
Ferra?, and Adam Kowal-czyk.
2001.
Second order features for maximising textclassification performance.
In EMCL ?01: Proceedingsof the 12th European Conference on Machine Learning,pages 419?430, London, UK.
Springer-Verlag.Philip Resnik and Eric Hardisty.
2010.
Gibbssampling for the uninitiated.
Technical Re-port UMIACS-TR-2010-04, University of Maryland.http://www.lib.umd.edu/drum/handle/1903/10058.Chade-Meng Tan, Yuan-Fang Wang, and Chan-Do Lee.2002.
The use of bigrams to enhance text categoriza-tion.
Inf.
Process.
Manage., 38(4):529?546.Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, andDavid M. Blei.
2006.
Hierarchical Dirichlet pro-cesses.
Journal of the American Statistical Association,101(476):1566?1581.B.
Yu, S. Kaufmann, and D. Diermeier.
2008.
Classify-ing party affiliation from political speech.
Journal ofInformation Technology and Politics, 5(1):33?48.292
