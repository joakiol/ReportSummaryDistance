Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 285?288,Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLPIterative Scaling and Coordinate Descent Methods for Maximum EntropyFang-Lan Huang, Cho-Jui Hsieh, Kai-Wei Chang, and Chih-Jen LinDepartment of Computer ScienceNational Taiwan UniversityTaipei 106, Taiwan{d93011,b92085,b92084,cjlin}@csie.ntu.edu.twAbstractMaximum entropy (Maxent) is useful inmany areas.
Iterative scaling (IS) methodsare one of the most popular approaches tosolve Maxent.
With many variants of ISmethods, it is difficult to understand themand see the differences.
In this paper, wecreate a general and unified framework forIS methods.
This framework also connectsIS and coordinate descent (CD) methods.Besides, we develop a CD method forMaxent.
Results show that it is faster thanexisting iterative scaling methods1.1 IntroductionMaximum entropy (Maxent) is widely used inmany areas such as natural language processing(NLP) and document classification.
Maxent mod-els the conditional probability as:Pw(y|x)?Sw(x, y)/Tw(x), (1)Sw(x, y)?ePtwtft(x,y), Tw(x)?
?ySw(x, y),where x indicates a context, y is the label of thecontext, and w ?
Rn is the weight vector.
Afunction ft(x, y) denotes the t-th feature extractedfrom the context x and the label y.Given an empirical probability distribution?P (x, y) obtained from training samples, Maxentminimizes the following negative log-likelihood:minw?
?x,y?P (x, y) logPw(y|x)=?x?P (x) log Tw(x)?
?twt?P (ft),(2)where ?P (x) =?y?P (x, y) is the marginal prob-ability of x, and ?P (ft) =?x,y?P (x, y)ft(x, y) isthe expected value of ft(x, y).
To avoid overfit-ting the training samples, some add a regulariza-tion term and solve:minwL(w)?
?x?P (x)logTw(x)?
?twt?P(ft)+Ptw2t2?2,(3)1A complete version of this work is at http://www.csie.ntu.edu.tw/?cjlin/papers/maxent_journal.pdf.where ?
is a regularization parameter.
We focuson (3) instead of (2) because (3) is strictly convex.Iterative scaling (IS) methods are popular intraining Maxent models.
They all share the sameproperty of solving a one-variable sub-problemat a time.
Existing IS methods include general-ized iterative scaling (GIS) by Darroch and Rat-cliff (1972), improved iterative scaling (IIS) byDella Pietra et al (1997), and sequential condi-tional generalized iterative scaling (SCGIS) byGoodman (2002).
In optimization, coordinate de-scent (CD) is a popular method which also solvesa one-variable sub-problem at a time.
With thesemany IS and CD methods, it is uneasy to see theirdifferences.
In Section 2, we propose a unifiedframework to describe IS and CD methods froman optimization viewpoint.
Using this framework,we design a fast CD approach for Maxent in Sec-tion 3.
In Section 4, we compare the proposedCD method with IS and LBFGS methods.
Resultsshow that the CD method is more efficient.Notation n is the number of features.
The totalnumber of nonzeros in samples and the averagenumber of nonzeros per feature are respectively#nz ?
?x,y?t:ft(x,y) 6=01 and ?l ?
#nz/n.2 A Framework for IS Methods2.1 The FrameworkThe one-variable sub-problem of IS methods is re-lated to the function reduction L(w+zet)?L(w),where et= [0, .
.
.
, 0, 1, 0, .
.
.
, 0]T. IS methodsdiffer in how they approximate the function reduc-tion.
They can also be categorized according towhether w?s components are sequentially or par-allely updated.
In this section, we create a frame-work in Figure 1 for these methods.Sequential update For a sequential-updatealgorithm, once a one-variable sub-problem issolved, the corresponding element in w is up-dated.
The new w is then used to construct thenext sub-problem.
The procedure is sketched in285Iterative scalingSequential updateFind At(z) to approximateL(w + zet)?
L(w)SCGISLet At(z) =L(w+zet)?L(w)CDParallel updateFind a separable function A(z) toapproximate L(w + z)?
L(w)GIS, IISFigure 1: An illustration of various iterative scaling methods.Algorithm 1 A sequential-update IS methodWhile w is not optimalFor t = 1, .
.
.
, n1.
Find an approximate function At(z) sat-isfying (4).2.
Approximately minzAt(z) to get z?t.3.
wt?
wt+ z?t.Algorithm 1.
If the t-th component is selected forupdate, a sequential IS method solves the follow-ing one-variable sub-problem:minzAt(z),where At(z) bounds the function difference:At(z) ?
L(w + zet)?
L(w)=?x?P (x) logTw+zet(x)Tw(x)+ Qt(z)(4)and Qt(z)?2wtz+z22?2?
z?P (ft).
(5)An approximate function At(z) satisfying (4) doesnot ensure that the function value is strictly de-creasing.
That is, the new function value L(w +zet) may be only the same as L(w).
Therefore,we can impose an additional conditionAt(0) = 0 (6)on the approximate function At(z).
If A?t(0) 6= 0and assume z?t?
argminzAt(z) exists, with thecondition At(0)=0, we have At(z?t)<0.
This in-equality and (4) then imply L(w + z?tet)<L(w).If A?t(0) = ?tL(w) = 0, the convexity of L(w)implies that we cannot decrease the function valueby modifying wt.
Then we should move on tomodify other components of w.A CD method can be viewed as a sequential ISmethod.
It solves the following sub-problem:minzACDt(z) = L(w + zet)?
L(w)without any approximation.
Existing IS methodsconsider approximations as At(z) may be simplerfor minimization.Parallel update A parallel IS method simul-taneously constructs n independent one-variablesub-problems.
After (approximately) solving allof them, the whole vector w is updated.
Algo-rithm 2 gives the procedure.
The differentiablefunction A(z), z ?
Rn, is an approximation ofL(w + z)?
L(w) satisfyingA(z) ?
L(w + z)?
L(w), A(0) = 0, andA(z) =?tAt(zt).
(7)Similar to (4) and (6), the first two conditions en-Algorithm 2 A parallel-update IS methodWhile w is not optimal1.
Find approximate functions At(zt) ?t satis-fying (7).2.
For t = 1, .
.
.
, nApproximately minztAt(zt) to get z?t.3.
For t = 1, .
.
.
, nwt?
wt+ z?t.sure that the function value is strictly decreasing.The last condition shows thatA(z)is separable, sominzA(z) =?tminztAt(zt).That is,we can minimizeAt(zt),?t simultaneously,and then update wt?t together.
A parallel-updatemethod possesses nice implementation properties.However, since it less aggressively updates w, itusually converges slower.
If A(z) satisfies (7),taking z = ztetimplies that (4) and (6) hold forany At(zt).
A parallel method could thus be trans-formed to a sequential method using the same ap-proximate function, but not vice versa.2.2 Existing Iterative Scaling MethodsWe introduce GIS, IIS and SCGIS via the pro-posed framework.
GIS and IIS use a parallel up-date, but SCGIS is sequential.
Their approximatefunctions aim to bound the function reductionL(w+z)?L(w)=?x?P (x) logTw+z(x)Tw(x)+?tQt(zt),(8)where Tw(x) and Qt(zt) are defined in (1) and (5),respectively.
Then GIS, IIS and SCGIS use simi-lar inequalities to get approximate functions.
Theyapply log?
?
??
1 ??
> 0 to get(8)?
?x,y?P (x)Pw(y|x)(ePtztft(x,y)?1)+?tQt(zt).
(9)GIS definesf#?
maxx,yf#(x, y), f#(x, y) ?
?tft(x, y),and adds a feature fn+1(x, y)?f#?f#(x, y) withzn+1=0.
Assuming ft(x, y) ?
0, ?t, x, y, andusing Jensen?s inequalityePn+1t=1ft(x,y)f#(ztf#)??n+1t=1ft(x,y)f#eztf#andePtztft(x,y)?
?tft(x,y)f#eztf#+fn+1(x,y)f#, (10)we obtain n independent one-variable functions:AGISt(zt) =eztf#?1f#?x,y?P (x)Pw(y|x)ft(x, y)+ Qt(zt).286IIS applies Jensen?s inequalityePtft(x,y)f#(x,y)(ztf#(x,y))?
?tft(x,y)f#(x,y)eztf#(x,y)on (9) to get the approximate functionAIISt(zt) =?x,y?P (x)Pw(y|x)ft(x, y)eztf#(x,y)?1f#(x,y)+ Qt(zt).SCGIS is a sequential-update method.
It replacesf# in GIS with f#t?
maxx,yft(x, y).
Using ztetas z in (8), a derivation similar to (10) giveseztft(x,y)?ft(x,y)f#teztf#t+f#t?ft(x,y)f#t.The approximate function of SCGIS isASCGISt(zt) =eztf#t?1f#t?x,y?P (x)Pw(y|x)ft(x, y)+ Qt(zt).We prove the linear convergence of existing ISmethods (proof omitted):Theorem 1 Assume each sub-problem Ast(zt) isexactly minimized, where s is IIS, GIS, SCGIS, orCD.
The sequence {wk} generated by any of thesefour methods linearly converges.
That is, there isa constant ?
?
(0, 1) such thatL(wk+1)?L(w?)
?
(1??)(L(wk)?L(w?
)),?k,where w?
is the global optimum of (3).2.3 Solving one-variable sub-problemsWithout the regularization term, by A?t(zt) = 0,GIS and SCGIS both have a simple closed-formsolution of the sub-problem.
With the regular-ization term, the sub-problems no longer have aclosed-form solution.
We discuss the cost of solv-ing sub-problems by the Newton method, whichiteratively updates ztbyzt?
zt?Ast?(zt)/Ast??(zt).
(11)Here s indicates an IS or a CD method.Below we check the calculation of Ast?
(zt) asthe cost of Ast??
(zt) is similar.
We haveAst?
(zt)=?x,y?P (x)Pw(y|x)ft(x, y)eztfs(x,y)+ Q?t(zt)(12)wherefs(x, y) ?????
?f# if s is GIS,f#tif s is SCGIS,f#(x, y) if s is IIS.For CD,ACDt?
(zt)=Q?t(zt)+?x,y?P (x)Pw+ztet(y|x)ft(x, y).
(13)The main cost is on calculating Pw(y|x) ?x, y,whenever w is updated.
Parallel-update ap-proaches calculate Pw(y|x) once every n sub-problems, but sequential-update methods evalu-ates Pw(y|x) after every sub-problem.
Considerthe situation of updating w to w+ztet.
By (1),Table 1: Time for minimizing At(zt) by the New-ton method CD GIS SCGIS IIS1st Newton direction O(?l) O(?l) O(?l) O(?l)Each subsequentNewton direction O(?l) O(1) O(1) O(?l)obtaining Pw+ztet(y|x) ?x, y requires expensiveO(#nz) operations to evaluate Sw+ztet(x, y) andTw+ztet(x) ?x, y.
A trick to trade memory fortime is to store all Sw(x, y) and Tw(x),Sw+ztet(x, y)=Sw(x, y)eztft(x,y),Tw+ztet(x)=Tw(x)+?ySw(x, y)(eztft(x,y)?1).Since Sw+ztet(x, y) = Sw(x, y) if ft(x, y) =0, this procedure reduces the the O(#nz) opera-tions to O(#nz/n) = O(?l).
However, it needsextra spaces to store all Sw(x, y) and Tw(x).This trick for updating Pw(y|x) has been usedin SCGIS (Goodman, 2002).
Thus, the firstNewton iteration of all methods discussed heretakes O(?l) operations.
For each subsequentNewton iteration, CD needs O(?l) as it calcu-lates Pw+ztet(y|x) whenever ztis changed.
ForGIS and SCGIS, if ?x,y?P (x)Pw(y|x)ft(x, y)is stored at the first Newton iteration, then (12)can be done in O(1) time.
For IIS, becausef#(x, y) of (12) depends on x and y, we cannotstore?x,y?P (x)Pw(y|x)ft(x, y) as in GIS andSCGIS.
Hence each Newton direction needs O(?l).We summarize the cost for solving sub-problemsin Table 1.3 Comparison and a New CD Method3.1 Comparison of IS/CD methodsFrom the above discussion, an IS or a CD methodfalls into a place between two extreme designs:At(zt) a loose bound?At(zt) a tight boundEasy to minimize At(zt) Hard to minimizeAt(zt)There is a tradeoff between the tightness to boundthe function difference and the hardness to solvethe sub-problem.
To check how IS and CD meth-ods fit into this explanation, we obtain relation-ships of their approximate functions:ACDt(zt) ?
ASCGISt(zt) ?
AGISt(zt),ACDt(zt) ?
AIISt(zt) ?
AGISt(zt) ?
zt.
(14)The derivation is omitted.
From (14), CD con-siders more accurate sub-problems than SCGISand GIS.
However, to solve each sub-problem,from Table 1, CD?s each Newton step takes moretime.
The same situation occurs in comparingIIS and GIS.
Therefore, while a tight At(zt) can287give faster convergence by handling fewer sub-problems, the total time may not be less due tothe higher cost of each sub-problem.3.2 A Fast CD MethodWe develop a CD method which is cheaper insolving each sub-problem but still enjoys fast fi-nal convergence.
This method is modified fromChang et al (2008), a CD approach for linearSVM.
We approximately minimize ACDt(z) by ap-plying only one Newton iteration.
The Newton di-rection at z = 0 is nowd = ?ACDt?(0)/ACDt??(0).
(15)As taking the full Newton direction may not de-crease the function value, we need a line searchprocedure to find ?
?
0 such that z = ?d satisfiesthe following sufficient decrease condition:ACDt(z)?ACDt(0) = ACDt(z) ?
?zACDt?
(0), (16)where ?
is a constant in (0, 1/2).
A simpleway to find ?
is by sequentially checking ?
=1, ?, ?2, .
.
.
, where ?
?
(0, 1).
The line searchprocedure is guaranteed to stop (proof omitted).We can further prove that near the optimum tworesults hold: First, the Newton direction (15) sat-isfies the sufficient decrease condition (16) with?=1.
Then the cost for each sub-problem is O(?l),similar to that for exactly solving sub-problems ofGIS or SCGIS.
This result is important as other-wise each trial of z = ?d expensively costs O(?l)for calculating ACDt(z).
Second, taking one New-ton direction of the tighter ACDt(zt) reduces thefunction L(w) more rapidly than exactly minimiz-ing a loose At(zt) of GIS, IIS or SCGIS.
Thesetwo results show that the new CD method im-proves upon the traditional CD by approximatelysolving sub-problems, while still maintains fastconvergence.4 ExperimentsWe apply Maxent models to part ofspeech (POS) tagging for BROWN corpus(http://www.nltk.org) and chunk-ing tasks for CoNLL2000 (http://www.cnts.ua.ac.be/conll2000/chunking).We randomly split the BROWN corpusto 4/5 training and 1/5 testing.
Our im-plementation is built upon OpenNLP(http://maxent.sourceforge.net).We implement CD (the new one in Section 3.2),GIS, SCGIS, and LBFGS for comparisons.
Weinclude LBFGS as Malouf (2002) reported thatit is better than other approaches including GIS0 500 1000 1500 200010?210?1100101Training Time (s)RelativefunctionvaluedifferenceCDSCGISGISLBFGS(a) BROWN0 50 100 150 20010?210?1100101102Training Time (s)RelativefunctionvaluedifferenceCDSCGISGISLBFGS(b) CoNLL20000 500 1000 1500 20009494.59595.59696.597Training Time (s)TestingAccuracyCDSCGISGISLBFGS(c) BROWN0 50 100 150 2009090.59191.59292.59393.5Training Time (s)F1measureCDSCGISGISLBFGS(d) CoNLL2000Figure 2: First row: time versus the relative func-tion value difference (17).
Second row: time ver-sus testing accuracy/F1.
Time is in seconds.and IIS.
We use ?2 = 10, and set ?
= 0.5 and?
= 0.001 in (16).We begin at checking time versus the relativedifference of the function value to the optimum:L(w)?
L(w?)/L(w?).
(17)Results are in the first row of Figure 2.
We checkin the second row of Figure 2 about testing ac-curacy/F1 versus training time.
Among the threeIS/CD methods compared, the new CD approachis the fastest.
SCGIS comes the second, whileGIS is the last.
This result is consistent withthe tightness of their approximation functions; see(14).
LBFGS has fast final convergence, but itdoes not perform well in the beginning.5 ConclusionsIn summary, we create a general framework forexplaining IS methods.
Based on this framework,we develop a new CD method for Maxent.
It ismore efficient than existing IS methods.ReferencesK.-W. Chang, C.-J.
Hsieh, and C.-J.
Lin.
2008.
Coor-dinate descent method for large-scale L2-loss linearSVM.
JMLR, 9:1369?1398.John N. Darroch and Douglas Ratcliff.
1972.
Gener-alized iterative scaling for log-linear models.
Ann.Math.
Statist., 43(5):1470?1480.Stephen Della Pietra, Vincent Della Pietra, and JohnLafferty.
1997.
Inducing features of random fields.IEEE PAMI, 19(4):380?393.Joshua Goodman.
2002.
Sequential conditional gener-alized iterative scaling.
In ACL, pages 9?16.Robert Malouf.
2002.
A comparison of algorithmsfor maximum entropy parameter estimation.
InCONLL.288
