Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
590?599, Prague, June 2007. c?2007 Association for Computational LinguisticsExperimental Evaluation of LTAG-based Featuresfor Semantic Role LabelingYudong Liu and Anoop SarkarSimon Fraser UniversityBurnaby, BC, Canada{yudongl,anoop}@cs.sfu.caAbstractThis paper proposes the use of Lexical-ized Tree-Adjoining Grammar (LTAG) for-malism as an important additional sourceof features for the Semantic Role Labeling(SRL) task.
Using a set of one-vs-all Sup-port Vector Machines (SVMs), we evalu-ate these LTAG-based features.
Our exper-iments show that LTAG-based features canimprove SRL accuracy significantly.
Whencompared with the best known set of fea-tures that are used in state of the art SRL sys-tems we obtain an improvement in F-scorefrom 82.34% to 85.25%.1 IntroductionSemantic Role Labeling (SRL) aims to identify andlabel all the arguments for each predicate occurringin a sentence.
It involves identifying constituents inthe sentence that represent the predicate?s argumentsand assigning pre-specified semantic roles to them.
[A0seller Ports of Call Inc.] reached agreements to[Vverb sell] [A1thing its remaining seven aircraft][A2buyer to buyers that weren?t disclosed] .is an example of SRL annotation from the PropBankcorpus (Palmer et al, 2005), where the subscriptedinformation maps the semantic roles A0, A1, A2to arguments for the predicate sell as defined in thePropBank Frame Scheme.
For SRL, high accuracyhas been achieved by:(i) proposing new types of features (see Table 1 inSection 3 for previously proposed features),(ii) modeling the predicate frameset by capturing de-pendencies between arguments (Gildea and Juraf-sky, 2002; Pradhan et al, 2004; Toutanova et al,2005; Punyakanok et al, 2005a),(iii) dealing with incorrect parser output by usingmore than one parser (Pradhan et al, 2005b).Our work in this paper falls into category (i).
Wepropose several novel features based on LexicalizedTree Adjoining Grammar (LTAG) derivation treesin order to improve SRL performance.
To showthe usefulness of these features, we provide an ex-perimental study comparing LTAG-based featureswith the standard set of features and kernel meth-ods used in state-of-the-art SRL systems.
The LTAGformalism provides an extended domain of localityin which to specify predicate-argument relationshipsand also provides the notion of a derivation tree.These two properties of LTAG make it well suitedto address the SRL task.SRL feature extraction has relied on various syn-tactic representations of input sentences, such assyntactic chunks (Hacioglu et al, 2004) and fullsyntactic parses (Gildea and Jurafsky, 2002).
Incontrast with features from shallow parsing, previ-ous work (Gildea and Palmer, 2002; Punyakanok etal., 2005b) has shown the necessity of full syntacticparsing for SRL.
In order to generalize the path fea-ture (see Table 1 in Section 3) which is probably themost salient (while being the most data sparse) fea-ture for SRL, previous work has extracted featuresfrom other syntactic representations, such as CCGderivations (Gildea and Hockenmaier, 2003) and de-pendency trees (Hacioglu, 2004) or integrated fea-tures from different parsers (Pradhan et al, 2005b).To avoid explicit feature engineering on trees, (Mos-chitti, 2004) used convolution kernels on selectiveportions of syntactic trees.
In this paper, we alsocompare our work with tree kernel based methods.Most SRL systems exploit syntactic trees as themain source of features.
We would like to take thisone step further and show that using LTAG deriva-590SNP VPMD(will) VPV(join) PPSVPV(join)VPMD(will) VP?SVPMD(will)VPV(join)?1(join)NP ?1(will) PP?2(will)NP ?3(join)PP?1: ?1:?2: ?3:?1: ?2:Figure 1: A parse tree schematic, and two plausibleLTAG derivation trees for it: derivation tree ?1 useselementary trees ?1 and ?1 while ?2 uses ?2 and ?3.tion trees as an additional source of features can im-prove both argument identification and classificationaccuracy in SRL.2 Using LTAG-based Features in SRLWe assume some familiarity with Lexicalized Tree-Adjoining Grammar (LTAG); (Joshi and Schabes,1997) is a good introduction to this formalism.
ALTAG is defined to be a set of lexicalized elementarytrees (etree for short), of which there are two types,initial trees and auxiliary trees.
Typically etreescan be composed through two operations into parsetrees, substitution and adjunction.
We use sister ad-junction which is commonly used in LTAG statisti-cal parsers to deal with the relatively flat Penn Tree-bank trees (Chiang, 2000).
The tree produced bycomposing the etrees is the derived/parse tree andthe tree that records the history of composition is thederivation tree.A reasonable way to define SRL features is to pro-vide a strictly local dependency (i.e.
within a sin-gle etree) between predicate and argument.
Therehave been many different proposals on how to main-tain syntactic locality (Xia, 1999; Chen and Vijay-Shanker, 2000) and SRL locality (Chen and Ram-bow, 2003; Shen and Joshi, 2005) when extract-ing LTAG etrees from a Treebank.
These proposedmethods are exemplified by the derivation tree ?1 inFig.
1.
However, in most cases they can only providea local dependency between predicate and argumentfor 87% of the argument constituents (Chen andRambow, 2003), which is too low to provide highSRL accuracy.
In LTAG-based statistical parsers,high accuracy is obtained by using the Magerman-Collins head-percolation rules in order to providethe etrees (Chiang, 2000).
This method is exem-plified by the derivation tree ?2 in Fig.
1.
Compar-ing ?1 with ?2 in Fig.
1 and assuming that join isthe predicate and the NP is the potential argument,the path feature as defined over the LTAG deriva-tion tree ?2 is more useful for the SRL task as it dis-tinguishes between main clause and non-finite em-bedded clause predicates.
This alternative derivationtree also exploits the so-called extended domain oflocality (Joshi and Schabes, 1997) (the examples inSection 2.1 show this clearly).
In this paper, we cru-cially rely on features defined on LTAG derivationtrees of the latter kind.
We use polynomial kernelsto create combinations of features defined on LTAGderivation trees.2.1 LTAG-based Feature ExtractionIn order to create training data for the LTAG-basedfeatures, we convert the Penn Treebank phrase struc-ture trees into LTAG derivations.
First, we prune theTreebank parse tree using certain constraints.
Thenwe decompose the pruned parse trees into a set ofLTAG elementary trees and obtain a derivation tree.For each constituent in question, we extract featuresfrom the LTAG derivation tree.
We combine thesefeatures with the standard features used for SRLand train an SVM classifier on the combined LTAGderivation plus SRL annotations from the PropBankcorpus.For the test data, we report on results using thegold-standard Treebank data, and in addition we alsoreport results on automatically parsed data using theCharniak parser (Charniak, 2000) as provided by theCoNLL 2005 shared task.
We did this for three rea-sons: (i) our results are directly comparable to thosewho have used the Charniak parses distributed withthe CoNLL 2005 data-set; (ii) we avoid the possi-bility of a better parser identifying a larger numberof argument constituents and thus leading to bet-ter results, which is orthogonal to the discrimina-tive power of our proposed LTAG-based features;and (iii) the quality of LTAG derivation trees de-pends indirectly on the quality of head dependen-cies recovered by the parser and it is a well-knownfolklore result (see Table 3 in (McDonald et al,5912005)) that applying the head-percolation heuristicson parser output produces better dependencies whencompared to dependencies directly recovered by theparser (whether the parser is an LTAG parser or alexicalized PCFG parser).2.1.1 Pruning Parse TreesGiven a parse tree, the pruning component iden-tifies the predicate in the tree and then only admitsthose nodes that are sisters to the path from the pred-icate to the root.
It is commonly used in the SRLcommunity (cf.
(Xue and Palmer, 2004)) and our ex-periments show that 91% of the SRL targets can berecovered despite this aggressive pruning.
We maketwo enhancements to the pruned Propbank tree: weenrich the sister nodes with head information, a part-of-speech tag and word pair: ?t, w?
and PP nodes areexpanded to include the NP complement of the PP(including head information).
The target SRL nodeis still the PP.
Figure 2 is a pruned parse tree for asentence from the PropBank.2.1.2 Decompositions of Parse TreesAfter pruning, the pruned tree is decom-posed around the predicate using standard head-percolation based heuristic rules1 to convert a Tree-bank tree into an LTAG derivation tree.
Figure 3shows the resulting etrees after decomposition.
Fig-ure 4 is the derivation tree for the entire pruned tree.Each node in this derivation tree represents an etreein Figure 3.
In our model we make an independenceassumption that each SRL is assigned to each con-stituent independently, conditional only on the pathfrom the predicate etree to the argument etree in thederivation tree.
Different etree siblings in the LTAGderivation tree do not influence each other in our cur-rent models.2.1.3 LTAG-based FeaturesWe defined 5 LTAG feature categories: predicateetree-related features (P for short), argument etree-related features (A), subcategorization-related fea-tures (S), topological relation-related features (R),intermediate etree-related features (I).
Since weconsider up to 6 intermediate etrees between thepredicate and the argument etree, we use I-1 to I-6to represent these 6 intermediate trees respectively.1using http://www.isi.edu/?chiang/software/treep/treep.htmlSNPNNP-HInc.VP-HVBD-HreachedNPNNS-HagreementsSVP-HTO-HtoVPVB-HsellNPNN-HaircraftPPTO-HtoNPNNS-HbuyersFigure 2: The pruned tree for the sentence ?Ports ofCall Inc. reached agreements to sell its remainingseven aircraft to buyers that weren?t disclosed.
?VPVBsellNPNNaircraftPPTOtoSVPTOtoe0: e1: e2: e3:NPNNSagreementsSVPVBDreachedNPNNPInc.e4: e5: e6:Figure 3: Elementary trees after decomposition ofthe pruned tree.Category P: Predicate etree & its variants Pred-icate etree is an etree with predicate, such as e0 inFigure 3.
This new feature complements the pred-icate feature in the standard SRL feature set.
Onevariant is to remove the predicate lemma.
Anothervariant is a combination of predicate tree w/o predi-cate lemma&POS and voice.
In addition, this variantcombined with predicate lemma comprises anothernew feature.
In the example, these three variants are(VP(VB)) and (VP) active and (VP) active sell re-spectively.Category A: Argument etree & its variants Anal-ogous to the predicate etree, the argument etree is anetree with the target constituent and its head.
Similar592e5(reached)e6(Inc.) e4(agreements)e3(to)e0(sell)e1(aircraft) e2(to)Figure 4: LTAG derivation tree for Figure 2.to predicate etree related features, argument etree,argument etree with removal of head word, combi-nation of argument etree w/o head POS&head wordand head Named Entity (NE) label (if any) are con-sidered.
For example, in Figure 3, these 3 featuresfor e6 are e6, (NP(NNP)) and (NP) LOC with headword ?Inc.?
having NE label ?LOC?.Category S: Index of current argument etree insubcat frame of predicate etree Sub-categorizationis a standard feature that denotes the immediate ex-pansion of the predicate?s parent.
For example, itis V NP PP for predicate sell in the given sentence.For argument etree e1 in Figure 3, the index featurevalue is 1 since it is the very first element in the (or-dered) subcat sequence.Category R:Relation type between argument etree & predi-cate etree This feature is a combination of positionand modifying relation.
Position is a binary valuedstandard feature to describe if the argument is beforeor after the predicate in a parse tree.
For each argu-ment etree and intermediate etree, we consider threetypes of modifying relations they may have with thepredicate etree: modifying (value 1), modified (value2) and neither (value 3).
From Figure 4, we can seee1 modifies e0 (predicate tree).
So their modifyingrelation type value is 1; Combining this value withthe position value, this feature for e1 is ?1 after?.Attachment point of argument etree This fea-ture describes where the argument etree is sister-adjoined/adjoined to the predicate etree, or the otherway around.
For e1 in the example, VP in the predi-cate tree is the attachment point.Distance This feature is the number of intermediateetrees between argument etree and predicate etree inthe derivation tree.
In Figure 4, the distance from e4to the predicate etree is 1 since only one intermediateetree e3 is between them.Category I:Intermediate etree related features Intermediateetrees are those etrees that are located between thepredicate etree and argument etrees.
The set of fea-tures we propose for each intermediate etree is quitesimilar to those for argument etrees except we donot consider the named-entity label for head wordsin this case.Relation type of intermediate etree & predicateetree.Attachment point of intermediate etree.Distance between intermediate etree and predicateetree.Up to 6 intermediate etrees are considered and thecategory I features are extracted for each of them (ifthey exist).Each etree represents a linguistically meaningfulfragment.
The features of relation type, attachmentpoint as well as the distance characterize the topo-logical relations among the relevant etrees.
In par-ticular, the attachment point and distance featurescan explicitly capture important information hiddenin the standard path feature.
The intermediate treerelated features can give richer contextual informa-tion between predicate tree and argument trees.
Weadded the subcat index feature to be complemen-tary to the sub-cat and syntactic frame features inthe standard feature set.3 Standard Feature SetOur standard feature set is a combination of featuresproposed by (Gildea and Jurafsky, 2002), (Surdeanuet al, 2003; Pradhan et al, 2004; Pradhan et al,2005b) and (Xue and Palmer, 2004).
All featureslisted in Table 1 are used for argument classifica-tion in our baseline system; and features with aster-isk are not used for argument identification2.
Wecompare this baseline SRL system with a systemthat includes a combination of these features withthe LTAG-based features.
Our baseline uses all fea-tures that have been used in the state-of-the-art SRLsystems and as our experimental results show, thesestandard features do indeed obtain state-of-the-art2This is a standard idea in the SRL literature: removing fea-tures more useful for classification, e.g.
named entity features,makes the classifier for identification more accurate.593Table 1: Standard features adopted by a typical SRLsystem.
Features with asterisk ?
are not used for ar-gument identification.Basic features from (Gildea and Jurafsky, 2002)?
predicate lemma and voice?
phrase type and head word?
path from phrase to predicate 1?
position: phrase relative to predicate: before or after?
sub-cat records the immediate structure that expands frompredicate?s parent2Additional features proposed by (Surdeanu et al 2003;Pradhan et al, 2004, 2005)?
predicate POS?
head word POS?
first/last word/POS?
POS of word immediately before/after phrase?
path length 1?
LCA(Lowest Common Ancestor) path from phrase to itslowest common ancestor with predicate?
punctuation immediately before/after phrase??
path trigrams?
: up to 9 are considered?
head word named entity label such as ?PER, ORG,LOC???
content word named entity label for PP parent node?Additional features proposed by (Xue and Palmer, 2004)?
predicate phrase type?
predicate head word?
voice position?
syntactic frame?1 In Fig.
2 NNS?NP?S?VP?VB is the path from the con-stituent NNS(agreements) to the predicate VB(sell) and thepath length is 4.2 This feature is different from the frame feature which usu-ally refers to all the semantic participants for the particularpredicate.accuracy on the SRL task.
We will show that addingLTAG-based features can improve the accuracy overthis very strong baseline.4 Experiments4.1 Experimental SettingsTraining data (PropBank Sections 2-21) and testdata (PropBank Section 23) are taken from CoNLL-2005 shared task3 All the necessary annotation in-formation such as predicates, parse trees as well asNamed Entity labels is part of the data.
The ar-3http://www.lsi.upc.es/?srlconll/.gument set we consider is {A0, A1, A2, A3, A4,AM} where AM is a generalized annotation of alladjuncts such as AM-TMP, AM-LOC, etc., wherePropBank function tags like TMP or LOC in AM-TMP, AM-LOC are ignored (a common setting forSRL, see (Xue and Palmer, 2004; Moschitti, 2004)).We chose these labels for our experiments becausethey have sufficient training/test data for the per-formance comparison and provide sufficient countsfor accurate significance testing.
However, we alsoprovide the evaluation result on the test set for fullCoNLL-2005 task (all argument types).We use SVM-light4 (Joachims, 1999) with a poly-nomial kernel (degree=3) as our binary classifier forargument classification.
We applied a linear kernelto argument identification because the training costof this phase is extremely computationally expen-sive.
We use 30% of the training samples to fine tunethe regularization parameter c and the loss-functioncost parameter j for both stages of argument identifi-cation and classification.
With parameter validationexperiments, we set c = 0.258 and j = 1 for the ar-gument identification learner and c = 0.1 and j = 4for the argument classification learner.The classification performance is evaluated usingPrecision/Recall/F-score (p/r/f) measures.
We ex-tracted all the gold labels of A0-A4 and AM withthe argument constituent index from the original testdata as the ?gold output?.
When we evaluate, wecontrast the output of our system with the gold out-put and calculate the p/r/f for each argument type.Our evaluation criteria which is based on predict-ing the SRL for constituents in the parse tree is basedon the evaluation used in (Toutanova et al, 2005).However, we also predict and evaluate those Prop-Bank arguments which do not have a correspondingconstituent in the gold parse tree or the automaticparse tree: the missing constituent case.
We alsoevaluate discontinuous PropBank arguments usingthe notation used in the CoNLL-2005 data-set butwe do not predict them.
This is contrast with someprevious studies where the problematic cases havebeen usually discarded or the largest constituents inthe parse tree that almost capture the missing con-stituent cases are picked as being the correct answer.Note that, in addition to the constituent based evalu-4http://svmlight.joachims.org/594Gold Standard Charniak Parserstd std+ltag std std+ltagp(%) 95.66 96.79 87.71 89.11r(%) 94.36 94.59 84.86 85.51f(%) 95.00 95.68 86.26 87.27?Table 2: Argument identification results on test dataation, in Section 4.4 we also provide the evaluationof our model on the CoNLL-2005 data-set.Because the main focus of this work is to evaluatethe impact of the LTAG-based features, we did notconsider the frameset or a distribution over the en-tire argument set or apply any inference/constraintsas a post-processing stage as most current SRL sys-tems do.
We focus our experiments on showing thevalue added by introducing LTAG-based features tothe SRL task over and above what is currently usedin SRL research.4.2 Argument IdentificationTable 2 shows results on argument identification (abinary classification of constituents into argument ornon-argument).
To fully evaluate the influence of theLTAG-based features, we report the identification re-sults on both Gold Standard parses and on Charniakparser output (Charniak, 2000)5.As we can see, after combing the LTAG-basedfeatures with the standard features, F-score in-creased from 95.00% to 95.68% with Gold-standardparses; and from 86.26% to 87.27% with the Char-niak parses (a larger increase).
We can see LTAG-based features help in argument identification forboth cases.
This result is better than (Xue andPalmer, 2004), and better on gold parses com-pared to (Toutanova et al, 2005; Punyakanok et al,2005b).4.3 Argument ClassificationBased on the identification results, argument clas-sification will assign the semantic roles to the ar-gument candidates.
For each argument of A0-A4and AM, a ?one-vs-all?
SVM classifier is trained onboth the standard feature set (std) and the augmentedfeature set (std+ltag).
Table 3 shows the classifi-cation results on the Gold-standard parses with the5We use the parses supplied with the CoNLL-2005 sharedtask for reasons of comparison.gold argument identification; Table 4 and 5 show theclassification results on the Charniak parser with thegold argument identification and the automatic ar-gument identification respectively.
Scores for multi-class SRL are calculated based on the total numberof correctly predicted labels, total number of goldlabels and the number of labels in our prediction forthis argument set.class std(p/r/f)% std+ltag(p/r/f)%A096.69 96.71 96.71 96.7796.70 96.74A193.82 93.30 97.30 94.8793.56 96.07A287.05 79.98 92.43 81.4283.37 86.58A394.44 68.79 97.69 73.4179.60 83.33A496.55 82.35 94.11 78.4388.89 85.56AM98.41 96.61 98.67 97.8897.50 98.27multi- 95.35 93.62 97.15 94.70class 94.48 95.91Table 3: Argument classification results on Gold-standard parses with gold argument boundaries4.4 DiscussionFrom the results shown in the tables, we can see thatby adding the LTAG-based features, the overall per-formance of the systems is improved both for argu-ment identification and for argument classification.Table 3 and 4 show that with the gold argu-ment identification, the classification for each classin {A0, A1, A2, A3, AM} consistently benefit fromLTAG-based features.
Especially for A3, LTAG-based features lead to more than 3 percent improve-ment.
But for A4 arguments, the performance drops3 percent in both cases.
As we noticed in Table5, which presents the argument classification resultson Charniak parser output with the automatic ar-gument identification, the prediction accuracy forclasses A0, A1, A3, A4 and AM is improved, butdrops a little for A2.In addition, we also evaluated our feature seton the full CoNLL 2005 shared task.
The over-595class std(p/r/f)% std+ltag(p/r/f)%A096.04 92.92 96.07 92.9294.46 94.47A190.64 85.71 94.64 86.6788.11 90.48A284.46 75.72 89.26 75.2279.85 81.64A387.50 62.02 87.10 68.3572.59 76.60A490.00 79.12 90.54 73.6284.21 81.21AM95.14 85.54 96.60 86.5190.09 91.27multi- 93.25 86.45 94.71 87.15class 89.72 90.77Table 4: Argument classification results on Charniakparser output with gold argument boundariesall performance using LTAG features increased from74.41% to 75.31% in terms of F-score on the full ar-gument set.
Our accuracy is most closely compara-ble to the 78.63% accuracy achieved on the full taskby (Pradhan et al, 2005a).
However, (Pradhan etal., 2005a) uses some additional information since itdeals with incorrect parser output by using multipleparsers.
The 79.44% accuracy obtained by the topsystem in CoNLL 2005 (Punyakanok et al, 2005a)is not directly comparable since their system usedthe more accurate n-best parser output of (Charniakand Johnson, 2005).
In addition their system alsoused global inference.
Our focus in this paper wasto propose new LTAG features and to evaluate im-pact of these features on the SRL task.We also compared our proposed feature setagainst predicate/argument features (PAF) proposedby (Moschitti, 2004).
We conducted an experimentusing SVM-light-TK-1.2 toolkit6.
The PAF tree ker-nel is combined with the standard feature vectors bya linear operator.
With settings of Table 5, its multi-class performance (p/r/f)% is 83.09/80.18/81.61with linear kernel and 85.36/81.79/83.53 with poly-nomial kernel (degree=3) over std feature vectors.6http://ai-nlp.info.uniroma2.it/moschitti/TK1.2-software/Tree-Kernel.htmclass std(p/r/f)% std+ltag(p/r/f)%A086.50 86.18 88.17 87.7086.34 87.93?A178.73 83.82 88.78 85.2281.19 86.97?A285.40 73.93 83.11 75.4279.25 79.08A385.71 60.76 85.71 68.3571.11 76.06?A484.52 78.02 89.47 74.7281.15 81.43AM80.47 82.11 83.87 81.5481.29 82.69?multi- 81.79 82.90 86.04 84.47class 82.34 85.25?Table 5: Argument classification results on Charniakparser output with automatic argument boundaries4.5 Significance TestingTo assess the statistical significance of the im-provements in accuracy we did a two-tailed sig-nificance test on the results of both Table 2 and5 where Charniak?s parser outputs were used.We chose SIGF7, which is an implementationof a computer-intensive, stratified approximate-randomization test (Yeh, 2000).
The statistical dif-ference is assessed on SRL identification, classifica-tion for each class (A0-A4, AM) and the full SRLtask (overall performance).
In Table 2 and 5, we la-beled numbers under std+ltag that are statisticallysignificantly better from those under std with aster-isk.
The significance tests show that for identifica-tion and full SRL task, the improvements are statis-tically significant with p value of 0.013 and 0.0001at a confidence level of 95%.
The significance teston each class shows that the improvement by addingLTAG-based features is statistically significant forclass A0, A1, A3 and AM.
Even though in Table 5the performance of A2 appears to be worse it is notsignificantly so, and A4 is not significantly better.
Incomparison, the performance of PAF did not showsignificantly better than std with p value of 0.593 atthe same confidence level of 95%.7http://www.coli.uni-saarland.de/?pado/sigf/index.html596full full?P full?R full?S full?A full?I stdid 90.5 90.6 90.0 90.5 90.5 90.1 89.6A0 84.5 84.3 84.6 84.5 84.3 83.5 84.2A1 89.8 90.1 89.4 89.3 89.6 89.3 88.9A2 84.2 84.2 84.0 83.7 83.6 83.6 84.9A3 76.7 80.7 75.1 76.0 75.6 76.7 78.6A4 80.0 83.3 80.0 79.6 80.0 80.0 79.2AM 82.8 83.3 82.9 82.8 82.6 83.1 82.4Table 6: Impact of each LTAG feature category (P, R, S, A, I defined in Section 2.1.3) on argument classi-fication and identification on CoNLL-2005 development set (WSJ Section 24).
full denotes the full featureset, and we use ??
to denote removal of a feature category of type ?.
For example, full?P is the feature setobtained by removing all P category features.
std denotes the standard feature set.5 Analysis of the LTAG-based featuresWe analyzed the drop in performance when a partic-ular type of LTAG feature category is removed fromthe full set of LTAG features (we use the broad cat-egories P, R, S, A, I as defined in Section 2.1.3).Table 6 shows how much performance is lost (orgained) when a particular type of LTAG feature isdropped from the full set.These experiments were done on the developmentset from CoNLL-2005 shared task, using the pro-vided Charniak parses.
All the SVM models weretrained using a polynomial kernel with degree 3.
Itis clear that the S, A, I category features help in mostcases and P category features hurt in most cases,including argument identification.
It is also worthnoting that the R and I category features help mostfor identification.
This vindicates the use of LTAGderivations as a way to generalize long paths in theparse tree between the predicate and argument.
Al-though it seems LTAG features have negative impacton prediction of A3 arguments on this developmentset, dropping the P category features can actuallyimprove performance over the standard feature set.In contrast, for the prediction of A2 arguments, noneof the LTAG feature categories seem to help.Note that since we use a polynomial kernel in thefull set, we cannot rule out the possibility that a fea-ture that improves performance when dropped maystill be helpful when combined in a non-linear ker-nel with features from other categories.
However,this analysis on the development set does indicatethat overall performance may be improved by drop-ping the P feature category.
We plan to examine thiseffect in future work.6 Related WorkThere has been some previous work in SRL that usesLTAG-based decomposition of the parse tree.
(Chenand Rambow, 2003) use LTAG-based decomposi-tion of parse trees (as is typically done for statis-tical LTAG parsing) for SRL.
Instead of extractinga typical ?standard?
path feature from the derivedtree, (Chen and Rambow, 2003) uses the path withinthe elementary tree from the predicate to the con-stituent argument.
Under this frame, they only re-cover semantic roles for those constituents that arelocalized within a single etree for the predicate, ig-noring cases that occur outside the etree.
As statedin their paper, ?as a consequence, adjunct seman-tic roles (ARGM?s) are basically absent from ourtest corpus?
; and around 13% complement seman-tic roles cannot be found in etrees in the gold parses.In contrast, we recover all SRLs by exploiting moregeneral paths in the LTAG derivation tree.
A simi-lar drawback can be found in (Gildea and Hocken-maier, 2003) where a parse tree pathwas defined interms of Combinatory Categorial Grammar (CCG)types using grammatical relations between predicateand arguments.
The two relations they defined canonly capture 77% arguments in Propbank and theyhad to use a standard path feature as a replacementwhen the defined relations cannot be found in CCGderivation trees.
In our framework, we use interme-diate sub-structures from LTAG derivations to cap-ture these relations instead of bypassing this issue.597Compared to (Liu and Sarkar, 2006), we haveused a more sophisticated learning algorithm and aricher set of syntactic LTAG-based features in thistask.
In particular, in this paper we built a strongbaseline system using a standard set of features anddid a thorough comparison between this strong base-line and our proposed system with LTAG-based fea-tures.
The experiments in (Liu and Sarkar, 2006)were conducted on gold parses and it failed to showany improvements after adding LTAG-based fea-tures.
Our experimental results show that LTAG-based features can help improve the performance ofSRL systems.
While (Liu and Sarkar, 2006) proposesome new features for SRL based on LTAG deriva-tions, we propose several novel features and in ad-dition they do not show that their features are usefulfor SRL.Our approach shares similar motivations with theapproach in (Shen and Joshi, 2005) which uses Prop-Bank information to recover an LTAG treebank as ifit were hidden data underlying the Penn Treebank.However their goal was to extract an LTAG grammarusing PropBank information from the Treebank, andnot the SRL task.Features extracted from LTAG derivations are dif-ferent and provide distinct information when com-pared to predicate-argument features (PAF) or sub-categorization features (SCF) used in (Moschitti,2004) or even the later use of argument spanningtrees (AST) in the same framework.
The adjunc-tion operation of LTAG and the extended domain oflocality is not captured by those features as we haveexplained in detail in Section 2.7 Conclusion and Future WorkIn this paper we show that LTAG-based featuresimprove on the best known set of features used incurrent SRL prediction systems: the F-score forargument identification increased from 86.26% to87.27% and from 82.34% to 85.25% for the SRLtask.
The analysis of the impact of each LTAG fea-ture category shows that the intermediate etrees areimportant for the improvement.
In future work weplan to explore the impact that different types ofLTAG derivation trees have on this SRL task, and ex-plore the use of tree kernels defined over the LTAGderivation tree.
LTAG derivation tree kernels werepreviously used for parse re-ranking by (Shen et al,2003).
Our work also provides motivation to do SRLand LTAG parsing simultaneously.AcknowledgementsThis research was partially supported by NSERC,Canada (RGPIN: 264905).
We would like to thankAravind Joshi, Libin Shen, and the anonymous re-viewers for their comments.ReferencesE.
Charniak and M. Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminative reranking.
InACL-2005.E.
Charniak.
2000.
A maximum-entropy-inspired parser.In NAACL-2000.J.
Chen and O. Rambow.
2003.
Use of deep linguisticfeatures for the recognition and labeling of semanticarguments.
In EMNLP-2003.J.
Chen and K. Vijay-Shanker.
2000.
Automated Extrac-tion of TAGs from the Penn Treebank.
In Proc.
of the6th International Workshop on Parsing Technologies(IWPT-2000), Italy.D.
Chiang.
2000.
Statistical parsing with an automati-cally extracted tree adjoining grammars.
In ACL-2000.D.
Gildea and J. Hockenmaier.
2003.
Identifying se-mantic roles using combinatory categorial grammar.In EMNLP-2003.D.
Gildea and D. Jurafsky.
2002.
Automatic label-ing of semantic roles.
Computational Linguistics,58(3):245?288.D.
Gildea and M. Palmer.
2002.
The necessity of parsingfor predicate argument recognition.
In ACL-2002.K.
Hacioglu, S. Pradhan, W. Ward, J. Martin, and D. Ju-rafsky.
2004.
Semantic role labeling by tagging syn-tactic chunks.
In CoNLL-2004 Shared Task.K.
Hacioglu.
2004.
Semantic role labeling using depen-dency trees.
In COLING-2004.T.
Joachims.
1999.
Making large-scale svm learningpractical.
Advances in Kernel Methods - Support Vec-tor Machines.A.
Joshi and Y. Schabes.
1997.
Tree-adjoining gram-mars.
Handbook of Formal Languages, 3.Y.
Liu and A. Sarkar.
2006.
Using LTAG-Based Featuresfor Semantic Role Labeling.
In TAG+8-2006.598R.
McDonald, K. Crammer, and F. Pereira.
2005.
On-line Large-Margin Training of Dependency Parsers.
InACL-2005.A.
Moschitti.
2004.
A study on convolution kernels forshallow semantic parsing.
In ACL-2004.M.
Palmer, D. Gildea, and P. Kingsbury.
2005.
Theproposition bank: An annotated corpus of semanticroles.
Computational Linguistics, 31(1).S.
Pradhan, W. Ward, K. Hacioglu, , J. H. Martin, andD.
Jurafsky.
2004.
Shallow Semantic Parsing UsingSupport Vector Machines.
In HLT-NAACL-2004.S.
Pradhan, K. Hacioglu, W. Ward, J. Martin, and D. Ju-rafsky.
2005a.
Semantic role chunking combin-ing complementary syntactic views.
In CoNLL-2005Shared Task.S.
Pradhan, W. Ward, K. Hacioglu, , J. H. Martin, andD.
Jurafsky.
2005b.
Semantic role labeling using dif-ferent syntactic views.
In ACL-2005.V.
Punyakanok, D. Roth, and W. Yih.
2005a.
Gener-alized inference with multiple semantic role labelingsystems (shared task paper).
In CoNLL-2005.V.
Punyakanok, D. Roth, and W. Yih.
2005b.
The neces-sity of syntactic parsing for semantic role labeling.
InIJCAI-2005.L.
Shen and A. Joshi.
2005.
Building an LTAG Tree-bank.
Technical Report Technical Report MS-CIS-05-15,5, CIS Department, University of Pennsylvania.L.
Shen, A. Sarkar, and A. Joshi.
2003.
Using LTAGbased features in parse reranking.
In EMNLP-2003.M.
Surdeanu, S. Harabagiu, J. Williams, and P. Aarseth.2003.
Using predicate-argument structures for infor-mation extraction.
In ACL-2003.K.
Toutanova, A. Haghighi, and C. D. Manning.
2005.Joint learning improves semantic role labeling.
InACL-2005.F.
Xia.
1999.
Extracting tree adjoining grammars frombracketed corpora.
In Proceedings of 5th Natural Lan-guage Processing Pacific Rim Symposium (NLPRS-99), Beijing, China.N.
Xue and M. Palmer.
2004.
Calibrating features forsemantic role labeling.
In EMNLP-2004.A.
Yeh.
2000.
More accurate tests for the statistical sig-nificance of result differences.
In COLING-2000.599
