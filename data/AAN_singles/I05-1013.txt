Automatic Partial Parsing Rule AcquisitionUsing Decision Tree InductionMyung-Seok Choi, Chul Su Lim, and Key-Sun ChoiKorea Advanced Institute of Science and Technology, 373-1 Guseong-dong,Yuseong-gu, Daejeon 305-701, Republic of Korea{mschoi, cslim}@kaist.ac.kr kschoi@cs.kaist.ac.krAbstract.
Partial parsing techniques try to recover syntactic informa-tion efficiently and reliably by sacrificing completeness and depth of anal-ysis.
One of the difficulties of partial parsing is finding a means to extractthe grammar involved automatically.
In this paper, we present a methodfor automatically extracting partial parsing rules from a tree-annotatedcorpus using decision tree induction.
We define the partial parsing rulesas those that can decide the structure of a substring in an input sentencedeterministically.
This decision can be considered as a classification; assuch, for a substring in an input sentence, a proper structure is chosenamong the structures occurred in the corpus.
For the classification, weuse decision tree induction, and induce partial parsing rules from thedecision tree.
The acquired grammar is similar to a phrase structuregrammar, with contextual and lexical information, but it allows buildingstructures of depth one or more.
Our experiments showed that the pro-posed partial parser using the automatically extracted rules is not onlyaccurate and efficient, but also achieves reasonable coverage for Korean.1 IntroductionConventional parsers try to identify syntactic information completely.
Theseparsers encounter difficulties when processing unrestricted texts, because of un-grammatical sentences, the unavoidable incompleteness of lexicon and grammar,and other reasons like long sentences.
Partial parsing is an alternative techniquedeveloped in response to these problems.
This technique aims to recover syn-tactic information efficiently and reliably from unrestricted texts by sacrificingcompleteness and depth of analysis, and relying on local information to resolveambiguities [1].Partial parsing techniques can be roughly classified into two groups.
The firstgroup of techniques involves partial parsing via finite state machines [2,3,9,10].These approaches apply the sequential regular expression recognizer to an in-put sentence.
When multiple rules match an input string at a given position, This research was supported in part by the Ministry of Science and Technology, theMinistry of Culture and Tourism, and the Korea Science and Engineering Foundationin Korea.R.
Dale et al (Eds.
): IJCNLP 2005, LNAI 3651, pp.
143?154, 2005.c?
Springer-Verlag Berlin Heidelberg 2005144 M.-S. Choi, C.S.
Lim, and K.-S. Choithe longest-matching rule is selected.
Therefore, these parsers always producea single best analysis and operate very fast.
In general, these approaches usea hand-written regular grammar.
As would be expected, manually writing agrammar is both very time consuming and prone to have inconsistencies.The other group of partial parsing techniques is text chunking, that is, recog-nition of non-overlapping and non-recursive cores of major phrases (chunks), byusing machine learning techniques [4,7,8,13,15,17].
Since Ramshaw and Mar-cus [15] first proposed formulating the chunking task as a tagging task, mostchunking methods have followed this word-tagging approach.
In base noun phrasechunking, for instance, each word is marked with one of three chunk tags: I (fora word inside an NP), O (for outside of an NP), and B (for between the end ofone NP and the start of another) as follows1:In ( early trading ) in ( Hong Kong ) ( Monday ), ( gold ) was quotedat ( $ 366.50 ) ( an ounce ).InO earlyI tradingI inO HongI KongI MondayB ,O goldI wasO quotedOatO $I 366.50I anB ounceI .OWith respect to these approaches, there have been several studies on automat-ically extracting chunking rules from large-scale corpora using transformation-based learning [15], error-driven pruning [7], the ALLiS top-down inductive sys-tem [8].
However, it is not yet clear how these approaches could be extendedbeyond the chunking task.In this paper, we present a method of automatically extracting partial pars-ing rules from a tree-annotated corpus using the decision tree method.
Our goalis to extract rules with higher accuracy and broader coverage.
We define thepartial parsing rules as those that can establish the structure of a substring inan input sentence deterministically.
This decision can be considered as a classifi-cation; as such, for a substring in an input sentence, a proper structure is chosenamong the structures occurred in the corpus, as extended from the word-taggingapproach of text chunking.
For the classification, we use decision tree inductionwith features of contextual and lexical information.
In addition, we use negativeevidence, as well as positive evidence, to gain higher accuracy.
For general re-cursive phrases, all possible substrings in a parse tree are taken into account byextracting evidence recursively from a parse tree in a training corpus.
We inducepartial parsing rules from the decision tree, and, to retain only those rules thatare accurate, verify each rule through cross-validation.In many cases, several different structures are assigned to the same substringin a tree-annotated corpus.
Substrings for coordination and compound nouns aretypical examples of such ambiguous cases in Korean.
These ambiguities can pre-vent us from extracting partial parsing rules that cover the substrings with morethan one substructure and, consequently, can cause the result of partial parsingto be limited to a relatively shallow depth.
In this work, we address this problemby merging substructures with ambiguity using an underspecified representation.1 This example is excerpted from Tjong Kim Sang [17].Automatic Partial Parsing Rule Acquisition 145This underspecification leads to broader coverage without deteriorating eitherthe determinism or the precision of partial parsing.The acquired grammar is similar to a phrase structure grammar, with con-textual and lexical information, but it allows building structures of depth one ormore.
It is easy to understand; it can be easily modified; and it can be selectivelyadded to or deleted from the grammar.
Partial parsing with this grammar pro-cesses an input sentence deterministically using longest-match heuristics.
Theacquired rules are then recursively applied to construct higher structures.2 Automatic Rule AcquisitionTo start, we define the rule template, the basic format of a partial parsing rule,as follows:left context | substring | right context ??
substructureThis template shows how the substring of an input sentence, surrounded by theleft context and the right context, constructs the substructure.
The left contextand the right context are the remainder of an input sentence minus the substring.For automatic learning of the partial parsing rules, the lengths of the left contextand the right context are restricted to one respectively.
Note that applying apartial parsing rule results in a structure of depth one or more.
In other words,the rules extracted by this rule template reduce a substring into a subtree, asopposed to a single non-terminal; hence, the resultant rules can be applied morespecifically and strictly.c4.5tree-annotatedcorpuspartial parsingrulesrule candidateextractiontreeunderspecificationcontextualization& lexicalizationverificationrefinementFig.
1.
Procedure for extracting partial parsing rulesFigure 1 illustrates the procedure for the extraction of partial parsing rules.First, we extract all possible rule candidates from a tree-annotated corpus, com-pliant with the rule template.
The extracted candidates are grouped according146 M.-S. Choi, C.S.
Lim, and K.-S. Choito their respective substrings.
Next, using the decision tree method, these candi-dates are enriched with contextual and lexical information.
The contextualizedand lexicalized rules are verified through cross-validation to retain only thoserules that are accurate.
The successfully verified accurate rules become the finalpartial parsing rules.
Remaining rules that cannot be verified are forwarded tothe tree underspecification step, which merges tree structures with hard ambi-guities.
As seen in Fig.
1, the underspecified candidates return to the refinementstep.
The following subsections describe each step in detail.2.1 Extracting CandidatesFrom the tree-annotated corpus, we extract all the possible candidates for partialparsing rules in accordance with the rule template.
Scanning input sentences an-notated with its syntactic structure one by one, we can extract the substructurecorresponding to every possible substring at each level of the syntactic struc-ture.
We define level 0 as part-of-speech tags in an input sentence, and level nas the nodes whose maximum depth is n. If no structure precisely correspondsto a particular substring, then a null substructure is extracted, which representsnegative evidence.Figure 2 shows an example sentence2 with its syntactic structure3 and someof the candidates for the partial parsing rules extracted from the left side ofthe example.
In this figure, the first partial parsing rule candidate shows howthe substring ?npp?
can be constructed into the substructure ?NP?.
Snull denotesnegative evidence.The extracted rule candidates are gathered and grouped according to theirrespective substrings.
Figure 34 shows the candidate groups.
In this figure, G1and G2 are the group names, and the number in the last column refers to thefrequency that each candidate occurs in the training corpus.
Group G1 and G2have 2 and 3 candidates, respectively.
When a particular group has only onecandidate, the candidate can always be applied to a corresponding substring2 ?NOM?
refers to the nominative case and ?ACC?
refers to the accusative case.
Theterm ?npp?
denotes personal pronoun; ?jxt?
denotes topicalized auxiliary particle;?ncn?
denotes non-predicative common noun; ?jco?
denotes objective case particle;?pvg?
denotes general verb; ?ef?
denotes final ending; and ?sf?
denotes full stop symbol.For a detailed description of the KAIST corpus and its tagset, refer to Lee [11].
Thesymbol ?+?
is not a part-of-speech, but rather a delimiter between words within aword phrase.3 In Korean, a word phrase, similar to bunsetsu in Japanese, is defined as a spacing unitwith one or more content words followed by zero or more functional words.
A contentword indicates the meaning of the word phrase in a sentence, while a functionalword?a particle or a verbal-ending?indicates the grammatical role of the wordphrase.
In the KAIST corpus used in this paper, a functional word is not included inthe non-terminal that the preceding content word belongs to, following the restrictedrepresentation of phrase structure grammar for Korean [12].
For example, a wordphrase ?na/npp + neun/jxt?
is annotated as ?
(NP na/npp ) + neun/jxt?, as inFig.
2.Automatic Partial Parsing Rule Acquisition 147NP NP VPVPVPSna/npp+neun/jxtI-NOMsagwa/ncn+reul/jcoapple-ACCmeok/pvg + neunda/ef + ./sfeat|npp ||npp + jxt ||npp + jxt ncn |?|NP + jxt NP + jco VP   |?|NP + jco VP |?|VP + ef + sf |SnullSnullNP + jxt NP + jco VPVPVPNPVPSFig.
2.
An example sentence and the extracted candidates for partial parsing rules|etm nbn + jcs paa |SnullAUXPG1G2|VP + ecs VP + ecs VP|Snull66151123170487freq.VP + ecs VP + ecs VPVPVPVP + ecs VP + ecs VPVPVPFig.
3.
Groups of partial parsing rules candidatesdeterministically.
In contrast, if there is more than one candidate in a particulargroup, those candidates should be enriched with contextual and lexical informa-tion to make each candidate distinct for proper application to a correspondingsubstring.2.2 Refining CandidatesThis step refines ambiguous candidates with contextual and lexical informationto make them unambiguous.First, each candidate needs to be annotated with contextual and lexical in-formation occurring in the training corpus, as shown in Fig.
4.
In this figure, wecan see that a substring with lexical information such as ?su/nbn?
unambigu-ously constitutes the substructure ?AUXP?.
We use the decision tree method,C4.5 [14], to select the important contextual and lexical information that canfacilitate the establishment of unambiguous partial parsing rules.
The featuresused in the decision tree method are the lexical information of each terminal or4 The term ?etm?
denotes adnominalizing ending; ?nbn?
denotes non-unit bound noun;?jcs?
denotes subjective case particle; ?paa?
denotes attributive adjective; ?ecs?
denotessubordinate conjunctive ending; and ?AUXP?
denotes auxiliary phrase.148 M.-S. Choi, C.S.
Lim, and K.-S. Choisal/pvg + | r/etm su/nbn + ga/jcs iss/paa | + da/ef ?
AUXPi/jp + | r/etm su/nbn + ga/jcs eop/paa | + da/ef ?
AUXPnolla/pvg + | n/etm jeok/nbn + i/jcs iss/paa | + da/ef ?
Snullwanjeonha/paa + | n/etm geot/nbn + i/jcs eop/paa | + go/ecc?
Snullkkeutna/pvg + | n/etm geut/nbn + i/jcs ani/paa | + ra/ecs ?
Snullik/pvg + | neun/etm geut/nbn + i/jcs jot/paa | + da/ef ?
Snullha/xsv + | r/etm nawi/nbn + ga/jcs eop/paa | + da/ef ?
SnullFig.
4.
Annotated candidates for the G1 group rulesnbn = su(way):paa = iss(exist)paa = eop(not exist)paa = man(much) SnullAUXPAUXP  Fig.
5.
A section of the decision treenon-terminal for the substring, and the parts-of-speech and lexical informationfor the left context and the right context.
Lexical information of a non-terminalis defined as the part-of-speech and lexical information of its headword.Figure 5 shows a section of the decision tree learned from our example sub-string.
The deterministic partial parsing rules in Fig.
6 are extracted from thedecision tree.
As shown in Fig.
6, only the lexical entries for the second and thefourth morphemes in the substring are selected as additional lexical informa-tion, and none of the contexts is selected in this case.
We should note that therules induced from the decision tree are ordered.
Since these ordered rules donot interfere with those from other groups, they can be modified without muchdifficulty.| etm su/nbn + jcs iss/paa | ??
AUXP| etm su/nbn + jcs eop/paa | ??
AUXP| etm su/nbn + jcs man/paa| ??
SnullFig.
6.
Partial parsing rules extracted from a section of the decision tree in Fig.
5Automatic Partial Parsing Rule Acquisition 149After we enrich the partial parsing rules using the decision tree method, weverify them by estimating the accuracy of each rule to filter out less deterministicrules.
We estimate the error rates (%) of the rule candidates via a 10-fold crossvalidation on the training corpus.
The rule candidates of the group with an errorrate that is less than the predefined threshold, ?, can be extracted to the finalpartial parsing rules.
The candidates in the group G2 in Fig.
3 could not beextracted as the final partial parsing rules, because the estimated error rate ofthe group was higher than the threshold.
The candidates in G2 are set asidefor tree underspecification processing.
Using the threshold ?, we can control thenumber of the final partial parsing rules and the ratio of the precision/recalltrade-off for the parser that adopts the extracted partial parsing rules.2.3 Dealing with Hard Ambiguities: The UnderspecifiedRepresentationThe group G2 in Fig.
3 has one of the attachment ambiguities, namely, consecu-tive subordinate clauses.
Figure 7 shows sections of two different trees extractedfrom a tree-annotated corpus.
The two trees have identical substrings, but are an-alyzed differently.
This figure exemplifies how an ambiguity relates to the lexicalassociation between verb phrases, which is difficult to annotate in rules.
Thereare many other syntactic ambiguities, such as coordination and noun phrasebracketing, that are difficult to resolve with local information.
The resolutionusually requires lexical co-occurrence, global context, or semantics.
Such am-biguities can deteriorate the precision of partial parsing or limit the result ofpartial parsing to a relatively shallow depth.Rule candidates with these ambiguities mostly have several different struc-tures assigned to the same substrings under the same non-terminals.
In thispaper, we refer to them as internal syntactic ambiguities.
We manually exam-ined the patterns of the internal syntactic ambiguities, which were found in theKAIST corpus as they could not be refined automatically due to low estimatedaccuracies.
During the process, we observed that few internal syntactic ambigu-ities could be resolved with local information.In this paper, we handle internal syntactic ambiguities by merging the candi-dates using tree intersection and making them underspecified.
This underspeci-fied representation enables an analysis with broader coverage, without deterio-VPVP VPcheonsaui ttange ga + seo/ecsgo to the land of angels - asjal sarabo + ryeogo/ecslive well - in order toaesseudamake effortVPVP(a)jibe ga + seo/ecsgo home - asTVreul bo + ryeogo/ecswatch TV - in order togabangeul chaenggidapack one?s bagVPVP VPVPVP(b)Fig.
7.
Examples of internal syntactic ambiguities150 M.-S. Choi, C.S.
Lim, and K.-S. ChoiG2|VP + ecs VP + ecs VP|SnullVPFig.
8.
Underspecified candidatesrating the determinism or the precision of partial parsing.
Since only differentstructures under the same non-terminal are merged, the underspecification doesnot harm the structure of higher nodes.
Figure 8 shows the underspecified can-didates of group G2.
In this figure, the first two rules in G2 are reduced to themerged ?VP?.
Underspecified candidates are also enriched with contextual andlexical information using the decision tree method, and they are verified throughcross-validation, as described in Sect.
2.2.
The resolution of internal syntacticambiguities is forwarded to a module beyond the partial parser.
If necessary,by giving all possible structures of underspecified parts, we can prevent a laterprocessing from re-analyzing the parts.
Any remaining candidates that are notselected as the partial parsing rules after all three steps are discarded.3 Experimental ResultsWe have performed experiments to show the usefulness of automatically ex-tracted partial parsing rules.
For our evaluations, we implemented a naive par-tial parser, using TRIE indexing to search the partial parsing rules.
The inputof the partial parser is a part-of-speech tagged sentence and the result is usuallythe sequence of subtrees.
At each position in an input sentence, the parser triesto choose a rule group using longest-match heuristics.
Then, if any matches arefound, the parser applies the first-matching rule in the group to the correspond-ing substring, because the rules induced from the decision tree are ordered.In our experiments, we used the KAIST tree-annotated corpus [11].
Thetraining corpus contains 10,869 sentences (289,362 morphemes), with an averagelength of 26.6 morphemes.
The test corpus contains 1,208 sentences, with anaverage length of 26.0 morphemes.
The validation corpus, used for choosing thethreshold, ?, contains 1,112 sentences, with an average length of 20.1 morphemes,and is distinct from both the training corpus and the test corpus.The performance of the partial parser was evaluated using PARSEVAL mea-sures [5].
The F measure, a complement of the E measure [16], is used to combineprecision and recall into a single measure of overall performance, and is definedas follows:F?
=(?2 + 1) ?
LP ?
LR?2 ?
LP + LRIn the above equation, ?
is a factor that determines the weighting of precisionand recall.
Thus, ?
< 1 is used to weight precision heavier than recall, ?
> 1is used to weight recall heavier than precision, and ?
= 1 is used to weightprecision and recall equally.Automatic Partial Parsing Rule Acquisition 151Table 1.
Precision/Recall with respect to the threshold, ?, for the validation corpus?
# of rules precision recall F?=0.46 18,638 95.5 72.9 91.611 20,395 95.1 75.1 91.716 22,650 94.2 78.0 91.621 25,640 92.6 83.3 91.226 28,180 92.0 84.7 90.9Table 2.
Experimental results of the partial parser for KoreanGrammar precision recall F?=0.4 F?=1baseline 73.0 72.0 72.9 72.5depth 1 rule only 95.2 68.3 90.3 79.6not underspecified 95.7 71.6 91.4 81.9underspecified 95.7 73.6 91.9 83.2underspecified (in case ?=26) 92.2 83.5 90.9 87.6PCFG 80.0 81.5 80.2 80.7Lee [11] 87.5 87.5 87.5 87.5The parsing result can be affected by the predefined threshold, ?
(describedin Sect.
2.2), which can control both the accuracy of the partial parser andthe number of the extracted rules.
Table 1 shows the number of the extractedrules and how precision and recall trade off for the validation corpus as thethreshold, ?, is varied.
As can be seen, a lower threshold, ?, corresponds to ahigher precision and a lower recall.
A higher threshold corresponds to a lowerprecision and a higher recall.
For a partial parser, the precision is generallyfavored over the recall.
In this paper, we used a value of 11 for ?, where theprecision was over 95% and f?=0.4 was the highest.
The value of this thresholdshould be set according to the requirements of the relevant application.Table 2 presents the precision and the recall of the partial parser for the testcorpus when the threshold, ?, was given a value of 11.
In the baseline gram-mar, we selected the most probable structure for a given substring from eachgroup of candidates.
The ?depth 1 rule only?
grammar is the set of the rulesextracted along with the restriction, stating that only a substructure of depthone is permitted in the rule template.
The ?underspecified?
grammar is the finalversion of our partial parsing rules, and the ?not underspecified?
grammar isthe set of the rules extracted without the underspecification processing.
BothPCFG and Lee [11] are statistical full parsers of Korean, and Lee enriched thegrammar using contextual and lexical information to improve the accuracy of aparser.
Both of them were trained and tested on the same corpus as ours wasfor comparison.
The performance of both the ?not underspecified?
grammar andthe ?underspecified?
grammar was greatly improved compared to the baselinegrammar and PCFG, neither of which adopts contextual and lexical informa-tion in their rules.
The ?not underspecified?
grammar performed better than152 M.-S. Choi, C.S.
Lim, and K.-S. Choithe ?depth 1 rule only?
grammar.
This indicates that increasing the depth of arule is helpful in partial parsing, as in the case of a statistical full parsing, Data-Oriented Parsing [6].
Comparing the ?underspecified?
grammar with the ?notunderspecified?
grammar, we can see that underspecification leads to broadercoverage, that is, higher recall.
The precision of the ?underspecified?
grammarwas above 95%.
In other words, when a parser generates 20 structures, 19 outof 20 structures are correct.
However, its recall dropped far beyond that of thestatistical full parser [11].
When we set ?
to a value of 26, the underspecifiedgrammar slightly outperformed that of the full parser in terms of f?=1, althoughthe proposed partial parser does not always produce one complete parse tree5.It follows from what has been said thus far that the proposed parser has thepotential to be a high-precision partial parser and approach the performancelevel of a statistical full parser, depending on the threshold ?.The current implementation of our parser has a O(n2mr) worst case timecomplexity for a case involving a skewed binary tree, where n is the length ofthe input sentence and mr is the number of rules.
Because mr is the constant,much more than two elements are reduced to subtrees of depth one or more ineach level of parsing, and, differing from full parsing, the number of recursionsin the partial parsing seems to be limited6, we can parse in near-linear time.Figure 9 shows the time spent in parsing as a function of the sentence length7.05101520253035400  10  20  30  40  50  60  70p arsingtime (ms )sent.lengthFig.
9.
Time spent in parsingLastly, we manually examined the first 100 or so errors occurring in the testcorpus.
In spite of underspecification, the errors related to conjunctions and5 In the test corpus, the percentage that our partial parser (?=26) produced onecomplete parse tree was 70.9%.
When ?=11, the percentage was 35.9%.6 In our parser, the maximum number of recursion was 10 and the average number ofrecursion was 4.47.7 This result was obtained using a Linux machine with Pentium III 700MHz processor.Automatic Partial Parsing Rule Acquisition 153attachments were the most frequent.
The errors of conjunctions were mostlycaused by substrings not occurring in the training corpus, while the cases ofattachments lacked contextual or lexical information for a given substring.
Theseerrors can be partly resolved by increasing the size of the corpus, but it seemsthat they cannot be resolved completely with partial parsing.
In addition, therewere errors related to noun phrase bracketing, date/time/unit expression, andeither incorrectly tagged sentences or inherently ambiguous sentences.
For date,time, and unit expressions, manually encoded rules may be effective with partialparsing, since they appear to be used in a regular way.
We should note thatmany unrecognized phrases included expressions not occurring in the trainingcorpus.
This is obviously because our grammar cannot handle unseen substrings;hence, alleviating the sparseness in the sequences will be the goal of our futureresearch.4 ConclusionIn this paper, we have proposed a method of automatically extracting the par-tial parsing rules from a tree-annotated corpus using a decision tree method.
Weconsider partial parsing as a classification; as such, for a substring in an inputsentence, a proper structure is chosen among the structures occurred in the cor-pus.
Highly accurate partial parsing rules can be extracted by (1) allowing rulesto construct a subtree of depth one or more; (2) using decision tree induction,with features of contextual and lexical information for the classification; and (3)verifying induced rules through cross-validation.
By merging substructures withambiguity in non-deterministic rules using an underspecified representation, wecan handle syntactic ambiguities that are difficult to resolve with local infor-mation, such as coordination and noun phrase bracketing ambiguities.
Using athreshold, ?, we can control the number of the partial parsing rules and the ratioof the precision/recall trade-off of the partial parser.
The value of this thresh-old should be set according to the requirements of the relevant application.
Ourexperiments showed that the proposed partial parser using the automaticallyextracted rules is not only accurate and efficient, but also achieves reasonablecoverage for Korean.References1.
Abney, S.P.
: Part-of-speech tagging and partial parsing.
Corpus-Based Methods inLanguage and Speech.
Kluwer Academic Publishers (1996)2.
Abney, S.P.
: Partial parsing via finite-state cascades.
Proceedings of the ESSLLI?96 Robust Parsing Workshop (1996) 8?153.
A?
?t-Mokhtar, S., Chanod, J.P.: Incremental finite-state parsing.
Proceedings ofApplied Natural Language Processing (1997) 72?794.
Argamon-Engelson, S., Dagan, I., Krymolowski, Y.: A memory-based approach tolearning shallow natural language patterns.
Journal of Experimental and Theoret-ical AI 11(3) (1999) 369?390154 M.-S. Choi, C.S.
Lim, and K.-S. Choi5.
Black, E., Abney, S., Flickenger, D., Gdaniec, C., Grishman, R., Harrison, P.,Hindle, D., Ingria, R., Jelinek, F., Klavans, J., Liberman, M., Marcus, M., Roukos,S., Santorini, B., Strzalkowski, T.: A procedure for quantitatively comparing thesyntactic coverage of English grammars.
Proceedings of the DARPA Speech andNatural Language Workshop (1991) 306?3116.
Bod, R.: Enriching Linguistics with Statistics: Performance Models of NaturalLanguage.
Ph.D Thesis.
University of Amsterdam (1995)7.
Cardie, C., Pierce, D.: Error-driven pruning of treebank grammars for base nounphrase identification.
Proceedings of 36th Annual Meeting of the Association forComputational Linguistics and 17th International Conference on ComputationalLinguistics (1998) 218?2248.
De?jean, H.: Learning rules and their exceptions.
Journal of Machine Learning Re-search 2 (2002) 669?6939.
Hindle, D.: A parser for text corpora.
Computational Approaches to the Lexicon.Oxford University (1995) 103?15110.
Hobbs, J.R., Appelt, D., Bear, J., Israel, D., Kameyama, M., Stickel, M., Tyson, M.:Fastus: A cascaded finite-state transducer for extracting information from natural-language text.
Finite-State Language Processing.
The MIT Press (1997) 383?40611.
Lee, K.J.
: Probabilistic Parsing of Korean based on Language-Specific Properties.Ph.D.
Thesis.
KAIST, Korea (1998)12.
Lee, K.J., Kim, G.C., Kim, J.H., Han, Y.S.
: Restricted representation of phrasestructure grammar for building a tree annotated corpus of Korean.
Natural Lan-guage Engineering 3(2) (1997) 215?23013.
Mun?oz, M., Punyakanok, V., Roth, D., Zimak, D.: A learning approach to shallowparsing.
Proceedings of the 1999 Joint SIGDAT Conference on Empirical Methodsin Natural Language Processing and Very Large Copora (1999) 168?17814.
Quinlan, J.R.: C4.5: Programs for Machine Learning.
Morgan Kaufmann Publish-ers (1993)15.
Ramshaw, L.A., Marcus, M.P.
: Text chunking using transformation-based learning.Proceedings of Third Wordkshop on Very Large Corpora (1995) 82?9416.
van Rijsbergen, C.: Information Retrieval.
Buttersworth (1975)17.
Tjong Kim Sang, E.F.: Memory-based shallow parsing.
Journal of Machine Learn-ing Research 2 (2002) 559?594
