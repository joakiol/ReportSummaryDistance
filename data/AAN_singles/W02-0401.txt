Using Maximum Entropy for Sentence ExtractionMiles Osborneosborne@cogsci.ed.ac.ukDivision of InformaticsUniversity of Edinburgh2 Buccleuch PlaceEdinburgh EH8 9LWUnited Kingdom.AbstractA maximum entropy classier can be usedto extract sentences from documents.
Ex-periments using technical documents showthat such a classier tends to treat featuresin a categorical manner.
This results in per-formance that is worse than when extract-ing sentences using a naive Bayes classier.Addition of an optimised prior to the max-imum entropy classier improves perfor-mance over and above that of naive Bayes(even when naive Bayes is also extendedwith a similar prior).
Further experimentsshow that, should we have at our disposalextremely informative features, then max-imum entropy is able to yield excellent re-sults.
Naive Bayes, in contrast, cannot ex-ploit these features and so fundamentallylimits sentence extraction performance.1 IntroductionSentence extraction |the recovery of a given setof sentences from some document| is useful fortasks such as document summarisation (where theextracted sentences can form the basis of a summary)or question-answering (where the extracted sentencescan form the basis of an answer).
In this paper, weconcentrate upon extraction of sentences for inclu-sion into a summary.
From a machine learning per-spective, sentence extraction is interesting becausetypically, the number of sentences to be extractedis a very small fraction of the total number of sen-tences in the document.
Furthermore, those clueswhich determine whether a sentence should be ex-tracted or not tend to be either extremely specic,or very weak, and furthermore interact together innon-obvious ways.
From a linguistic perspective, thetask is challenging since success hinges upon the abil-ity to integrate together diverse levels of linguisticdescription.Frequently (see section 6 for examples), sentenceextraction systems are based around simple algo-rithms which assume independence between thosefeatures used to encode the task.
A consequence ofthis assumption is that such approaches are funda-mentally unable to exploit dependencies which pre-sumably exist in the features that would be presentin an ideal sentence extraction system.
This situ-ation may be acceptable when the features used tomodel sentence extraction are simple.
However, itwill rapidly become unacceptable when more sophis-ticated heuristics, with complicated interactions, arebrought to bear upon the problem.
For example,Boguraev and Ne (2000a) argue that the quality ofsummarisation can be increased if lexical cohesionfactors (rhetorical devices which help achieve cohe-sion between related document utterances) are mod-elled by a sentence extraction system.
Clearly suchdevices (for example, lexical repetition, ellipsis, co-reference and so on) all contribute towards the gen-eral discourse structure of some text and furthermoreare related to each other in non-obvious ways.Maximum entropy (log-linear) models, on theother hand, do not make unnecessary independenceassumptions.
Within the maximum entropy frame-work, we are able to optimally integrate togetherwhatever sources of knowledge we believe potentiallyto be useful for the task.
Should we use features thatare benecial, then the model will be able to exploitthis fact.
Should we use features that are irrelevant,then again, the model will be able to notice this, andeectively ignore them.
Models based on maximumentropy are therefore well suited to the sentence ex-traction task, and furthermore, yield competitive re-sults on a variety of language tasks (Ratnaparkhi,1996; Berger et al, 1996; Charniak, 1999; Nigam etal., 1999).In this paper, we outline a conditional maximumPhiladelphia, July 2002, pp.
1-8.
Association for Computational Linguistics.Proceedings of the Workshop on Automatic Summarization (including DUC 2002),entropy classication model for sentence extraction.Our model works incrementally, and does not alwaysneed to process the entire document before assign-ing classication.1It discriminates between thosesentences which should and should not be extracted.This contrasts with ranking approaches which needto process the entire document before extracting sen-tences.
Because we model whether a sentence shouldbe extracted or not in terms of features that are ex-tracted from the sentence (and its context in the doc-ument), we do not need to specify the size of the sum-mary.
Again, this contrasts with ranking approacheswhich need to specify a priori the summary size.Our maximum entropy approach for sentence ex-traction does not come without problems.
Usingreasonably standard features, and when extractingsentences from technical papers, we nd that pre-cision levels are high, but recall is very low.
Thisarises from the fact that those features which pre-dict whether a sentence should be extracted tend tobe very specic and occur infrequently.
Features forsentences that should not be extracted tend to bemuch more abundant, and so more likely to be seenin the future.
A simple prior probability is shownto help counter-act this tendency.
Using our prior,we nd that the maximum entropy approach is ableto yield results that are better than a naive Bayesclassier.Our nal set of experiments looks more closely atthe dierences between maximum entropy and naiveBayes.
We show that when we have access to an ora-cle that is able to tell us when to extract a sentence,then in the situation when that information is en-coded in dependent features, maximum entropy eas-ily outperforms naive Bayes.
Furthermore, we alsoshow that even when that information is encoded interms of independent features, naive Bayes can beincapable of fully utilising this information, and soproduces worse results than maximum entropy.21Incremental classication means that a document isprocessed from start-to-nish and decisions are made assoon as sentences are encountered.
Some of our features(in particular, those which encode sentence position ina document) do require processing the entire document.Using such features prevents true incremental process-ing.
However, it is trivial to remove such features and soensure true incrementality.2As a reviewer commented, under certain circum-stances, naive Bayes can do well even when there arestrong dependencies within features (Domingos and Paz-zani, 1997).
For example, when the sample size is small,naive Bayes can be competitive with more sophisticatedapproaches such as maximum entropy.
Given this, afuller comparison of naive Bayes and maximum entropyfor sentence extraction requires considering sample sizein addition to the choice of features.The rest of this paper is as follows.
Section 2outlines the general framework for sentence extrac-tion using maximum entropy modelling.
Section 3presents our naive Bayes classier (which is used as acomparison with maximum entropy).
We then showin section 4 how both our maximum entropy andnaive Bayes classiers can be extended with an (op-timised) prior.
The issue of summary size is touchedupon in section 5.
Section 6 discusses related work.We then present our main results (section 7).
Fi-nally, section 8 discusses our results and considersfuture work.2 Maximum Entropy for SentenceExtraction2.1 Conditional Maximum EntropyThe parametric form for a conditional maximum en-tropy model is as follows (Nigam et al, 1999):P (c j s) =1Z(s)exp(Xiifi(c; s)) (1)Z(s) =Xcexp(Xiifi(c; s)) (2)Here, c is a label (from the set of labels C) and s isthe item we are interested in labelling (from the setof items S).
In our domain, C simply consists of twolabels: one indicating that a sentence should be inthe summary (`keep'), and another label indicatingthat the sentence should not be in the summary (`re-ject').
S consists of a training set of sentences, linkedto their originating documents.
This means that wecan recover the position of any given sentence in anygiven document.Within maximum entropy models, the training setis viewed in terms of a set of features.
Each fea-ture expresses some characteristic of the domain.For example, a feature might capture the idea thatabstract-worthy sentences contain the words in thispaper.
In equation 1, fi(c; s) is a feature.
In this pa-per we restrict ourselves to integer-valued functions.An example feature might be as follows:fi(c; s) =8>><>>:1 if s contains the phrasein this paperand c is the label keep0 otherwise(3)Features are related to each other through weights(as can be seen in equation 1, where some featurefihas a weight i).
Weights are real-valued num-bers.
When a closed form solution cannot be found,they are determined by numerical optimisation tech-niques.
In this paper, we use conjugate gradient de-scent to nd the optimal set of weights.
ConjugateGradient descent converges faster than Improved It-erative Scaling (Laerty et al, 1997), and empiricallywe nd that it is numerically more stable.2.2 Maximum Entropy ClassicationWhen classifying sentences with maximum entropy,we use the equation:label(s) = argmaxc2CP (c j s) (4)In practice, we are not interested in the probabil-ity of a label given a sentence.
Instead we use theunnormalised score:label(s) = argmaxc2Cexp(Xiifi(c; s)) (5)Note that this maximum entropy classier assumesa uniform prior.
Section 4 shows how a non-uniformprior is used in place of this uniform prior.We now present our basic naive Bayes classier.Afterwards, we extend this classier with a non-uniform prior.3 Naive Bayes ClassicationAs an alternative to maximum entropy, we also inves-tigated a naive Bayes classier.
Unlike maximum en-tropy, naive Bayes assumes features are conditionallyindependent of each other.
So, comparing the two to-gether will give an indication of the level of statisticaldependencies which exist between features in the sen-tence extraction domain.
For our experiments, weused a variant of the multi-variate Bernoulli eventmodel (McCallum and Nigam, 1998).
In particular,we did not consider features that are absent in someexample.
This allows us to avoid summing over allfeatures in the model for each example.
Note thatour maximum entropy model also did not considerabsent features.Within our naive Bayes approach, the probabilityof a label given the sentence is as follows:P (c j s) =P (c)Qni=1P (gij c)P (s)(6)As before, s is some sentence, c the label, and giis some active feature describing sentence s. NaiveBayes models can be estimated in a closed formby simple counting.
For features which have zerocounts, we use add-k smoothing (where k is a smallnumber less than one).Since the probability of the data (P (s)) is con-stant:P (c j s) / P (c)nYi=1P (gij c) (7)If we assume a uniform prior (in which case P (c) is aconstant for all c), this can be further simplied to:P (c j s) /nYi=1P (gij c) (8)Our basic naive Bayes classier is as follows:label(s) = argmaxc2CnYi=1P (gij c) (9)As with the maximum entropy classier, we laterreplace the uniform prior with a non-uniform prior.4 Maximum a PosterioriClassicationIn this section, we show how our classiers can beextended with a non-uniform prior.
We also describehow such a prior can be optimised.4.1 Adding a non-uniform priorNow, the two classiers mentioned previously (equa-tions 9 and 5) are both based on maximum likeli-hood estimation.
However, as we describe later, forsentence extraction, the maximum entropy classiertends to over-select labels.
In particular, it tendsto reject too many sentences for inclusion into thesummary.
So, it it useful to extend the two previousclassiers with a non-uniform prior.
For the naiveBayes classier, we have:label(s) = argmaxc2CP (c)nYi=1P (gij c) (10)Here, P (c) is our prior.
The probability of the data(P (s)) is constant and so can be dropped.For the maximum entropy case, we are not inter-ested in the actual probability:label(s) = argmaxc2CF (c) exp(Xiifi(c; s)) (11)F (c) is a function equivalent to the prior whenusing the unnormalised classier.
When this priordistribution (or equivalent function) is uniform, clas-sication is as before (namely as outlined in sections2 and 3), and depends upon the maximum entropyor naive Bayes component.
When the prior is non-uniform, the classier behaviour will change.
Thisprior therefore allows us to aect the performanceof our system.
In particular, we can change theprecision-recall balance.4.2 Optimising the priorWe treat the problem of selecting a prior as an opti-misation task: select some P (c) (or F (c)) such thatperformance, as measured by some objective func-tion of the overall classier, is maximised.
Since thechoice of objective function is up to us, we can eas-ily optimise the classier in any way we decide.
Forexample, we could optimise for recall by using as ourobjective function an f-measure that weighted recallmore highly than precision.
In this paper, we opti-mise the prior using as an objective function the f2score of the classier (section 7 details this score).Our prior therefore does not reect relative frequen-cies of labels (as found in some corpus).We now need to optimise our prior.
Brent's onedimensional function minimisation method is wellsuited to this task (Press et al, 1993), since for arandom variable taking two values, the probabilityof one value can be dened in terms of the othervalue.
Section 7 describes the held-out optimisationstrategy used in our experiments.Should we decide to use a more elaborate prior (forexample, one which was also sensitive to propertiesof documents) then we would need to use a multi-dimensional function minimisation method.Note that we have not simultaneously optimisedthe likelihood and prior probabilities.
This meansthat we do not necessarily nd the optimal maxi-mum a posteriori (MAP) solution.
It is possible tointegrate into maximum entropy estimation (simple)conjugate priors that do allow MAP solutions to befound (Chen and Rosenfeld, 1999).
Although it isan open question whether more complex priors canbe directly integrated, future work ought to considerthe e?cacy of such approaches in the context of sum-marisation.5 Summary sizeDetermining the size of the summary is an impor-tant consideration for summarisation.
Frequently,this is carried out dynamically, and specied by theuser.
For example, when there is limited opportu-nity to display long summaries a user might want aterse summary.
Alternatively, when recall is impor-tant, a user might prefer a longer summary.
Usually,systems rank all sentences in terms of how abstract-worthy that are, and then take the top n most highlyranked sentences.
This always requires the size ofsummary to be specied.In our classication framework, sentences are pro-cessed (largely) independently of each other, and sothere is no direct way of controlling the size of thesummary.
Altering the prior will indirectly inuencethe summary size.
For more direct control over sum-mary size, we can rank sentences using our classiers(we not only label but can also assign label prob-abilities) and select the top n most highly rankedsentences.Within our classication approach, the optimisedprior plays a similar role to the user-dened numberof sentences that a ranking approach might return.Experiments (not reported here) showed thatranking sentences using our maximum entropy classi-er, and then selecting the top n most highly rankedsentences produced slightly worse results than whenselecting sentences in terms of classication.6 Related WorkThe summarisation literature is large.
Here we con-sider only a representative sample.Kupiec et al (1995) used Naive Bayes for sentenceextraction.
They did not consider the role of theprior, nor did they use Naive Bayes for classica-tion.
Instead, they used it to rank sentences andselected the top n sentences.
The TEXTRACTsystem included a sentence extraction componentthat is frequency-based (Boguraev and Ne, 2000b).Whilst the system uses a wide variety of linguis-tic cues when scoring sentences, it does not com-bine these scores in an optimal manner.
Also, itdoes not consider interactions between the linguisticcues.
Goldstein et al (1999) used a centroid similar-ity measure to score sentences.
They do not appearto have optimised their metric, nor do they deal withstatistical dependencies between their features.7 ExperimentsSummarisation evaluation is a hard task, principallybecause the notion of an objective summary is ill-dened.
That aside, in order to compare our varioussystems, we used an intrinsic evaluation approach.Our summaries were evaluated using the standardf2 score:r =jmp =jkf2 =2prp + rwhere:r = Recallp = Precisionj = Number of correct sentences in summaryk = Number of sentences in summarym = Number of correct sentences in the documentA sentence being `correct' means that it wasmarked as being somehow important (abstract-worthy) by a human and labelled `keep' by one ofour classiers.
Summaries produced by our systemswill therefore attempt to mimic the process of select-ing what it means for a sentence to be important ina document.Naturally this premise |that an annotator candecide a priori whether a sentence is abstract-worthyor not| is open to question.
That aside, in othersentence extraction scenarios, it may well be the casethat sentences can be reliably annotated.The f2 score treats recall and precision equally.This is a sensible metric to use as we have no a priorireason to believe in some other non-equal ratio of thetwo components.Our evaluation results are based on the followingapproach:1.
Split the set of documents into two disjoint sets(T1 and T2), with 70 documents in T1 and 10documents in T2.2.
Further split T1 into two disjoint sets T3 andT4.
T3 is used to train a model, and T4 isa held-out set.
The prior is estimated usingBrent's line minimisation method, when train-ing using T3 and evaluating on T4.
T3 consistedof 60 documents and T4 consisted of 10 docu-ments.3.
Results are then presented using a model trainedon T1, with the prior just found, and evaluatedusing T2.
T1 is therefore the training set andT2 is the testing set.
Results are also presentedusing aat prior.4.
The whole process is then repeated after ran-domising the documents.
The nal results arethen averaged over these n runs.
We set n to40.7.1 Document setFor data, we used the same documents thatTeufel (2001) used in her experiments.3In brief,these were 80 conference papers, taken from theComp-lang preprint archive, and semi-automaticallyconverted from LATEXto XML.
The XML annotateddocuments were then additionally manually marked-up with tags indicating the status of various sen-tences.
This document set is modest in size.
On theother hand, the actual documents are longer thannewswire messages typically used for summarisationtasks.
Also, the documents show variation in style.For example, some documents are written by non-native speakers, some by students, some by multipleauthors and so on.
Summarisation is therefore hard.3A superset of the documents is described in (Teufeland Moens, 1997).Here are some properties of the documents.
Onaverage, each document contained 8 sentences thatwere marked as being abstract-worthy (standard de-viation of 3.1).
The documents on average eachcontained in total 174 sentences (standard deviation50.7).
Here, a `sentence' is either any sequence ofwords that happened to be in a title, or else any se-quence of words in the rest of the document.
As canbe seen, the summaries are not uniformly long.
Also,the documents vary considerably in length.
Sum-mary size is therefore not constant.7.2 FeaturesWe used the following, fairly standard features whendescribing all sentences in the documents: Word pairs.
Word pairs are consecutive wordsas found in a sentence.
A word pair feature sim-ply indicates whether a particular word pair ispresent.
All words were reduced: truncated tobe at most 10 characters long.
Stemming (asfor example carried out by the Porter stemmer)produced worse results.
We extracted all wordpairs found in all sentences, and for any givensentence, found the set of (reduced) word pairs. Sentence length.
We encoded in three binaryfeatures whether a sentence was less than 6words in length, whether it was greater than 20words in length, or whether it was in betweenthese two ranges.
We also used a feature whichencoded whether a previous sentence was lessthan 5 words or longer.
This captured the ideathat summary sentences tend to follow headings(which are short). Sentence position.
Summary sentences tend tooccur either at the start, or the end of a docu-ment.
We used three features: whether a givensentence was within the rst 8 paragraphs of adocument, whether a sentence was in the last3 paragraphs, or whether the sentence was ina paragraph between these two ranges to en-code sentence position.
Note that this featurerequires the whole document to be processed be-fore classication can take place. (Limited) discourse features.
Our features de-scribed whether a sentence immediately followedtypical headings such as conclusion or introduc-tion, whether a sentence was at the start of aparagraph, or whether a sentence followed somegeneric heading.Our features are not exhaustive, and are not designedto maximise performance.
Instead, they are designedto be typical of those found in sentence extractionsystems.
Note that some of our features exploit thefact that the documents are annotated with struc-tural information (such as headers etc).Experiments with removing stop words from docu-ments resulted in decreased performance.
We conjec-ture that this is because our word pairs are extremelycrude syntax approximations.
Removing stop wordsfrom sentences and then creating word pairs makesthese pairs even worse syntax approximations.
How-ever, using stop words increased the number of fea-tures in our model, and so again reduced perfor-mance.
We therefore compromised between thesetwo positions, and mapped all stop words to the samesymbol prior to creation of word pair features.
Wealso found it useful to remove word pairs which con-sisted solely of stop words.
Finally, for maximumentropy, we deleted any feature that occurred lessthan 4 times.
Naive Bayes did not benet from afrequency-based cuto.7.3 Classier comparisonHere we report on our classiers.As a baseline model, we simply extracted the rstn sentences from a given document.
Figure 1 sum-marises our results as n varies.
In this table, as in allsubsequent tables, P and R are averaged precisionand recall values, whilst F2 is the f2 score of theseaveraged values.n F2 P R n F2 P R1 0 0 0 26 16 10 366 3 3 2 31 18 12 4511 19 15 26 36 18 11 5316 20 16 29 41 17 10 5821 23 16 38 46 16 9 58Figure 1: Results for the baseline modelFigure 2 shows our results for maximum entropy,both with and without the prior.
Prior optimisationwas with respect to the f2 score.
As in subsequenttables, we show system performance when addingmore and more features.Performance without the prior is heavily skewedtowards precision.
This is because our features arelargely acting categorically: the sheer presence ofsome feature is su?cient to inuence labelling choice.Further evidence for this analysis is supported by in-specting one of the models produced when using thefull set of all feature types.
We see that of the 85883Features Flat prior Optimised priorF2 P R F2 P RWord pairs 8 5 30 20 40 14and sent length 25 63 16 36 36 36and sent position 28 62 18 39 35 45and discourse 35 63 24 42 43 41Figure 2: Results for the maximum entropy modelfeature instances in the model, the vast majority aredeeded irrelevant by maximum entropy, and assigneda zero weight.
Only 7086 features (roughly 10% intotal) had non zero weights.Performance using the optimised prior shows morebalanced results, with an increase in F2 score.Clearly optimising the prior has helped counter thecategorical behaviour of features in our maximumentropy classier.Figure 3 shows the results we obtained when us-ing a naive Bayes classier.
As before, the resultsshow performance with and without the addition ofthe optimised prior.
Naive Bayes outperforms maxi-mum entropy when both classiers do not use a prior.Performance with and without the prior however, isworse than the performance of our maximum entropyclassier with the prior.
Evidently, even our rela-tively simple features interact with each other, andso approaches such as maximum entropy are requiredto fully exploit them.Features Flat prior Optimised priorF2 P R F2 P RWord pairs 26 29 23 29 26 32and sent length 31 33 28 32 29 35and sent position 33 34 33 36 31 43and discourse 38 39 37 39 38 40Figure 3: Results for the naive Bayes model7.4 Using informative featuresOur previous results showed that maximum entropycould outperform naive Bayes.
However, the dier-ences, though present, were not large.
Clearly, ourfeature set was imperfect.4It is therefore instruc-tive to see what happens if we had access to an or-acle who always told us the true status of some un-seen sentence.
To make things more interesting, we4Another possible reason for the closeness of the re-sults is the small sample size.
There may just not beenough evidence to reliably estimate dependencies withinthe data.Features Naive Bayes MaxentF2 P R F2 P RWord pairs 30 34 26 32 93 19and sent length 35 38 32 99 100 99and sent position 40 41 39 100 100 100and discourse 43 44 41 99 100 97Figure 4: Results for basic naive Bayes and max-imum entropy models using dependent informativefeaturesFeatures Naive Bayes MaxentF2 P R F2 P RWord pairs 84 74 97 25 15 91and sent length 85 75 97 100 100 100and sent position 84 73 97 100 100 100and discourse 84 74 97 100 100 100Figure 5: Results for basic naive Bayes and maxi-mum entropy models using independent informativefeaturesencoded this information in terms of dependent fea-tures.
We simulated this oracle by using two featureswhich were active whenever a sentence should notbe in the summary; for sentences that should be in-cluded in the summary, we let either one of those twofeatures be active, but on a random basis.
Our fea-tures therefore are only informative when the learneris capable of noting that there are dependencies.
Wethen repeated our previous maximum entropy andnaive Bayes experiments.
Figure 4 summarise ourresults.Unsurprisingly, we see that when features arehighly dependent upon each other, maximum en-tropy easily outperforms naive Bayes.Even when we have access to features that are in-dependent of each other, naive Bayes can still doworse than maximum entropy.
To demonstrate this,we used a feature that was active whenever a sen-tence should be in the summary.
This feature wasnot active on sentences that should not be in thesummary.
Figure 5 summarises our results.As can be seen (gure 5), even when naive Bayeshas access to a perfectly reliable informative feature,the fact that the other features are not suitably dis-counted means that performance is worse than thatof maximum entropy.
Maximum entropy can dis-count the other features, and so can take advantageof reliable features.8 Comments and Future WorkWe showed how maximum entropy could be used forsentence extraction, and in particular, that addinga prior could deal with the categorical nature ofthe features.
Maximum entropy, with an opti-mised prior, did yield marginally better results thannaive Bayes (with and without a similarly optimisedprior).
However, the dierences were not that great.Our further experiments with informative featuresshowed that this lack of dierence was probably due(at least in part) to the actual features used, and notdue to the technique itself.Our oracle results are an idealisation.
A fullercomparison should use more sophisticated features,along with more data.
As a result of this, we conjec-ture that should we use a much more sophisticatedfeature set, we would expect that the dierences be-tween maximum entropy and naive Bayes would be-come greater.Our approach treated sentences largely indepen-dently of each other.
However, abstract-worthy sen-tences tend to bunch together, particularly at thebeginning and end of a document.
We intend cap-turing this idea by making our approach sequence-based: future decisions should also be conditionedon previous choices.A problem with supervised approaches (such asours) is that we need annotated material (Marcu,1999).
This is costly to produce.
Future work willconsider weakly supervised approaches (for examplecotraining) as a way of bootstrapping labelled mate-rial from unlabelled documents (Blum and Mitchell,1998).
Note that there is a close connection betweenmulti-document summarisation (where many alter-native documents all consider similar issues) and theconcept of a view in cotraining.
We expect that thisredundancy could be exploited as a means of provid-ing more annotated training material, and so yieldbetter results.In summary, maximum entropy can be beneciallyused in sentence extraction.
However, one needs toguard against categorial features.
An optimised priorcan provide such help.AcknowledgementWe would like to thank Rob Malouf for supplying theexcellent log-linear estimation code, Simone Teufelfor providing the annotated data, Karen Spark Jonesfor a discussion about summarisation, Steve Clarkfor spotting textual bugs and the anonymous review-ers for useful comments.ReferencesAdam Berger, Stephen Della Pietra, and Vin-cent Della Pietra.
1996.
A maximum entropyapproach to natural language processing.
Com-putational Linguistics, 21{22.Avrim Blum and Tom Mitchell.
1998.
Combin-ing labeled and unlabeled data with co-training.In Proceedings of the Workshop on ComputationalLearning Theory.
Morgan Kaufmann Publishers.Branimir K. Boguraev and Mary S. Ne.
2000a.
Theeects of analysing cohesion on document sum-marisation.
In Proceedings of the 18thInterna-tional Conference on Computational Linguistics,volume 1, pages 76{82, Saarbrucken.Branmir K. Boguraev and Mary S. Ne.
2000b.
Dis-course Segmentation in Aid of Document Summa-rization.
In Proceedings of the 33rdHawaii Inter-national Conference on Systems Science.Eugene Charniak.
1999.
A maximum-entropy-inspired parser.
Technical Report CS99-12, De-partment of Computer Science, Brown University.Stanley F. Chen and Ronald Rosenfeld.
1999.A Gaussian prior for smoothing maximum en-tropy models.
Technical Report CMU-CS-99-108,Carnegie Mellon University.Pedro Domingos and Michael J. Pazzani.
1997.
Onthe optimality of the simple bayesian classier un-der zero-one loss.
Machine Learning, 29(2-3):103{130.Jade Goldstein, Mark Kantrowitz, Vibhu O. Mit-tal, and Jaime G. Carbonell.
1999.
Summarizingtext documents: Sentence selection and evaluationmetrics.
In Research and Development in Informa-tion Retrieval, pages 121{128.Julian Kupiec, Jan Pedersen, and Francine Chen.1995.
A Trainable Document Summarizer.
InProceedings of the 18thACM-SIGIR Conferenceon Research and Development in Information Re-trieval, pages 68{73.J.
Laerty, S. Della Pietra, and V. Della Pietra.1997.
Inducing features of random elds.
IEEETransactions on Pattern Analysis and Machine In-telligence, 19(4):380{393, April.Daniel Marcu.
1999.
The automatic constructionof large-scale corpora for summarization research.In Research and Development in Information Re-trieval, pages 137{144.A.
McCallum and K. Nigam.
1998.
A comparison ofevent models for naive bayes text classicatio.
InAAAI-98 Workshop on Learning for Text Catego-rization.Kamal Nigam, John Laerty, , and Andrew Mc-Callum.
1999.
Using maximum entropy for textclassication.
In IJCAI-99 Workshop on MachineLearning for Information Filtering,.William H. Press, Saul A. Teukolsky, William T. Vet-terling, and Brian P. Flannery.
1993.
Numeri-cal Recipes in C: the Art of Scientic Computing.Cambridge University Press, second edition.Adwait Ratnaparkhi.
1996.
A Maximum En-tropy Part-Of-Speech Tagger.
In Proceed-ings of Empirical Methods in Natural Lan-guage, University of Pennsylvania, May.
Tagger:ftp://ftp.cis.upenn.edu/pub/adwait/jmx.S.
Teufel and M. Moens.
1997.
Sentence extractionas a classication task.
In ACL/EACL-97 Work-shop on Intelligent and Scalable Text Summariza-tion, Madrid, Spain.Simone Teufel.
2001.
Task-Based Evaluation ofSummary Quality: Describing Relationships Be-tween Scientic Papers.
In NAACL Workshop onAutomatic Summarization, Pittsburgh, Pennsyl-vania, USA, June.
Carnegie Mellon University.
