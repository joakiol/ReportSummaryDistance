Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1650?1659,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsLinguistic Models for Analyzing and Detecting Biased LanguageMarta RecasensStanford Universityrecasens@google.comCristian Danescu-Niculescu-MizilStanford UniversityMax Planck Institute SWScristiand@cs.stanford.eduDan JurafskyStanford Universityjurafsky@stanford.eduAbstractUnbiased language is a requirement forreference sources like encyclopedias andscientific texts.
Bias is, nonetheless, ubiq-uitous, making it crucial to understand itsnature and linguistic realization and hencedetect bias automatically.
To this end weanalyze real instances of human edits de-signed to remove bias from Wikipedia ar-ticles.
The analysis uncovers two classesof bias: framing bias, such as praising orperspective-specific words, which we linkto the literature on subjectivity; and episte-mological bias, related to whether propo-sitions that are presupposed or entailed inthe text are uncontroversially accepted astrue.
We identify common linguistic cuesfor these classes, including factive verbs,implicatives, hedges, and subjective inten-sifiers.
These insights help us develop fea-tures for a model to solve a new predictiontask of practical importance: given a bi-ased sentence, identify the bias-inducingword.
Our linguistically-informed modelperforms almost as well as humans testedon the same task.1 IntroductionWriters and editors of reference works such asencyclopedias, textbooks, and scientific articlesstrive to keep their language unbiased.
For ex-ample, Wikipedia advocates a policy called neu-tral point of view (NPOV), according to whicharticles should represent ?fairly, proportionately,and as far as possible without bias, all signifi-cant views that have been published by reliablesources?
(Wikipedia, 2013b).
Wikipedia?s styleguide asks editors to use nonjudgmental language,to indicate the relative prominence of opposingpoints of view, to avoid presenting uncontroversialfacts as mere opinion, and, conversely, to avoidstating opinions or contested assertions as facts.Understanding the linguistic realization of biasis important for linguistic theory; automaticallydetecting these biases is equally significant forcomputational linguistics.
We propose to ad-dress both by using a powerful resource: edits inWikipedia that are specifically designed to removebias.
Since Wikipedia maintains a complete revi-sion history, the edits associated with NPOV tagsallow us to compare the text in its biased (before)and unbiased (after) form, helping us better under-stand the linguistic realization of bias.
Our workthus shares the intuition of prior NLP work apply-ing Wikipedia?s revision history (Nelken and Ya-mangil, 2008; Yatskar et al, 2010; Max and Wis-niewski, 2010; Zanzotto and Pennacchiotti, 2010).The analysis of Wikipedia?s edits provides valu-able linguistic insights into the nature of biasedlanguage.
We find two major classes of bias-drivenedits.
The first, framing bias, is realized by sub-jective words or phrases linked with a particularpoint of view.
In (1), the term McMansion, unlikehomes, appeals to a negative attitude toward largeand pretentious houses.
The second class, episte-mological bias, is related to linguistic features thatsubtly (often via presupposition) focus on the be-lievability of a proposition.
In (2), the assertivestated removes the bias introduced by claimed,which casts doubt on Kuypers?
statement.
(1) a.
Usually, smaller cottage-style houses have been de-molished to make way for these McMansions.b.
Usually, smaller cottage-style houses have been de-molished to make way for these homes.
(2) a. Kuypers claimed that the mainstream press in Amer-ica tends to favor liberal viewpoints.b.
Kuypers stated that the mainstream press in Americatends to favor liberal viewpoints.Bias is linked to the lexical and grammatical cuesidentified by the literature on subjectivity (Wiebeet al, 2004; Lin et al, 2011), sentiment (Liu etal., 2005; Turney, 2002), and especially stance1650or ?arguing subjectivity?
(Lin et al, 2006; So-masundaran and Wiebe, 2010; Yano et al, 2010;Park et al, 2011; Conrad et al, 2012).
For ex-ample, like stance, framing bias is realized whenthe writer of a text takes a particular position ona controversial topic and uses its metaphors andvocabulary.
But unlike the product reviews or de-bate articles that overtly use subjective language,editors in Wikipedia are actively trying to avoidbias, and hence biases may appear more subtly,in the form of covert framing language, or pre-suppositions and entailments that may not play asimportant a role in other genres.
Our linguisticanalysis identifies common classes of these subtlebias cues, including factive verbs, implicatives andother entailments, hedges, and subjective intensi-fiers.Using these cues could help automatically de-tect and correct instances of bias, by first findingbiased phrases, then identifying the word that in-troduces the bias, and finally rewording to elim-inate the bias.
In this paper we propose a so-lution for the second of these tasks, identifyingthe bias-inducing word in a biased phrase.
Since,as we show below, this task is quite challengingfor humans, our system has the potential to bevery useful in improving the neutrality of refer-ence works like Wikipedia.
Tested on a subset ofnon-neutral sentences from Wikipedia, our modelachieves 34% accuracy?and up to 59% if thetop three guesses are considered?on this difficulttask, outperforming four baselines and nearing hu-mans tested on the same data.2 Analyzing a Dataset of BiasedLanguageWe begin with an empirical analysis based onWikipedia?s bias-driven edits.
This section de-scribes the data, and summarizes our linguisticanalysis.12.1 The NPOV Corpus from WikipediaGiven Wikipedia?s strict enforcement of an NPOVpolicy, we decided to build the NPOV corpus,containing Wikipedia edits that are specifically de-signed to remove bias.
Editors are encouraged toidentify and rewrite biased passages to achieve amore neutral tone, and they can use several NPOV1The data and bias lexicon we developed are available athttp://www.mpi-sws.org/?cristian/Biased_language.htmlData Articles Revisions Words Edits SentsTrain 5997 2238K 11G 13807 1843Dev 653 210K 0.9G 1261 163Test 814 260K 1G 1751 230Total 7464 2708K 13G 16819 2235Table 1: Statistics of the NPOV corpus, extractedfrom Wikipedia.
(Edits refers to bias-driven ed-its, i.e., with an NPOV comment.
Sents refers tosentences with a one-word bias-driven edit.
)tags to mark biased content.2 Articles tagged thisway fall into Wikipedia?s category of NPOV dis-putes.We constructed the NPOV corpus by retrievingall articles that were or had been in the NPOV-dispute category3 together with their full revisionhistory.
We used Stanford?s CoreNLP tools4 to to-kenize and split the text into sentences.
Table 1shows the statistics of this corpus, which we splitinto training (train), development (dev), and test.Following Wikipedia?s terminology, we call eachversion of a Wikipedia article a revision, and so anarticle can be viewed as a set of (chronologicallyordered) revisions.2.2 Extracting Edits Meant to Remove BiasGiven all the revisions of a page, we extracted thechanges between pairs of revisions with the word-mode diff function from the Diff Match and Patchlibrary.5 We refer to these changes between revi-sions as edits, e.g., McMansion > large home.
Anedit consists of two strings: the old string that isbeing replaced (i.e., the before form), and the newmodified string (i.e., the after form).Our assumption was that among the edits hap-pening in NPOV disputes, we would have a highdensity of edits intended to remove bias, which wecall bias-driven edits, like (1) and (2) from Sec-tion 1.
But many other edits occur even in NPOVdisputes, including edits to fix spelling or gram-matical errors, simplify the language, make themeaning more precise, or even vandalism (Max2{{POV}}, {{POV-check}}, {{POV-section}}, etc.Adding these tags displays a template such as ?The neutralityof this article is disputed.
Relevant discussion may be foundon the talk page.
Please do not remove this message until thedispute is resolved.
?3http://en.wikipedia.org/wiki/Category:All_NPOV_disputes4http://nlp.stanford.edu/software/corenlp.shtml5http://code.google.com/p/google-diff-match-patch1651and Wisniewski, 2010).
Therefore, in order to ex-tract a high-precision set of bias-driven edits, wetook advantage of the comments that editors canassociate with a revision?typically short and briefsentences describing the reason behind the revi-sion.
We considered as bias-driven edits those thatappeared in a revision whose comment mentioned(N)POV, e.g., Attempts at presenting some claimsin more NPOV way; or merging in a passagefrom the researchers article after basic NPOV-ing.
We only kept edits whose before and af-ter forms contained five or fewer words, and dis-carded those that only added a hyperlink or thatinvolved a minimal change (character-based Lev-enshtein distance < 4).
The final number of bias-driven edits for each of the data sets is shown inthe ?Edits?
column of Table 1.2.3 Linguistic AnalysisStyle guides talk about biased language in a pre-scriptive manner, listing a few words that shouldbe avoided because they are flattering, vague, orendorse a particular point of view (Wikipedia,2013a).
Our focus is on analyzing actual bi-ased text and bias-driven edits extracted fromWikipedia.As we suggested above, this analysis uncoveredtwo major classes of bias: epistemological biasand framing bias.
Table 2 shows the distribution(from a sample of 100 edits) of the different typesand subtypes of bias presented in this section.
(A) Epistemological bias involves propositionsthat are either commonly agreed to be true or com-monly agreed to be false and that are subtly pre-supposed, entailed, asserted or hedged in the text.1.
Factive verbs (Kiparsky and Kiparsky, 1970)presuppose the truth of their complementclause.
In (3-a) and (4-a), realize and re-veal presuppose the truth of ?the oppressionof black people...?
and ?the Meditation tech-nique produces...?, whereas (3-b) and (4-b)present the two propositions as somebody?sstand or an experimental result.
(3) a.
He realized that the oppression of black peo-ple was more of a result of economic exploita-tion than anything innately racist.b.
His stand was that the oppression of blackpeople was more of a result of economic ex-ploitation than anything innately racist.
(4) a.
The first research revealed that the Meditationtechnique produces a unique state fact.b.
The first research indicated that the Medita-tion technique produces a unique state fact.Bias Subtype %A.
Epistemological bias 43- Factive verbs 3- Entailments 25- Assertives 11- Hedges 4B.
Framing bias 57- Intensifiers 19- One-sided terms 38Table 2: Proportion of the different bias types.2.
Entailments are directional relations thathold whenever the truth of one word orphrase follows from another, e.g., murder en-tails kill because there cannot be murderingwithout killing (5).
However, murder en-tails killing in an unlawful, premeditated way.This class includes implicative verbs (Kart-tunen, 1971), which imply the truth or un-truth of their complement, depending on thepolarity of the main predicate.
In (6-a), co-erced into accepting entails accepting in anunwilling way.
(5) a.
After he murdered three policemen, thecolony proclaimed Kelly a wanted outlaw.b.
After he killed three policemen, the colonyproclaimed Kelly a wanted outlaw.
(6) a.
A computer engineer who was coerced intoaccepting a plea bargain.b.
A computer engineer who accepted a plea bar-gain.3.
Assertive verbs (Hooper, 1975) are thosewhose complement clauses assert a proposi-tion.
The truth of the proposition is not pre-supposed, but its level of certainty dependson the asserting verb.
Whereas verbs of say-ing like say and state are usually neutral,point out and claim cast doubt on the cer-tainty of the proposition.
(7) a.
The ?no Boeing?
theory is a controversial is-sue, even among conspiracists, many of whomhave pointed out that it is disproved by ...b.
The ?no Boeing?
theory is a controversial is-sue, even among conspiracists, many of whomhave said that it is disproved by...(8) a. Cooper says that slavery was worse in SouthAmerica and the US than Canada, but clearlystates that it was a horrible and cruel practice.b.
Cooper says that slavery was worse in SouthAmerica and the US than Canada, but pointsout that it was a horrible and cruel practice.16524.
Hedges are used to reduce one?s commit-ment to the truth of a proposition, thusavoiding any bold predictions (9-b) or state-ments (10-a).6(9) a.
Eliminating the profit motive will decrease therate of medical innovation.b.
Eliminating the profit motive may have alower rate of medical innovation.
(10) a.
The lower cost of living in more rural areasmeans a possibly higher standard of living.b.
The lower cost of living in more rural areasmeans a higher standard of living.Epistemological bias is bidirectional, that is,bias can occur because doubt is cast on a propo-sition commonly assumed to be true, or becausea presupposition or implication is made about aproposition commonly assumed to be false.
Forexample, in (7) and (8) above, point out is replacedin the former case, but inserted in the second case.If the truth of the proposition is uncontroversiallyaccepted by the community (i.e., reliable sources,etc.
), then the use of a factive is unbiased.
In con-trast, if only a specific viewpoint agrees with itstruth, then using a factive is biased.
(B) Framing bias is usually more explicit thanepistemological bias because it occurs when sub-jective or one-sided words are used, revealing theauthor?s stance in a particular debate (Entman,2007).1.
Subjective intensifiers are adjectives or ad-verbs that add (subjective) force to the mean-ing of a phrase or proposition.
(11) a. Schnabel himself did the fantastic reproduc-tions of Basquiat?s work.b.
Schnabel himself did the accurate reproduc-tions of Basquiat?s work.
(12) a. Shwekey?s albums are arranged by many tal-ented arrangers.b.
Shwekey?s albums are arranged by many dif-ferent arrangers.2.
One-sided terms reflect only one of the sidesof a contentious issue.
They often belongto controversial subjects (e.g., religion, ter-rorism, etc.)
where the same event can beseen from two or more opposing perspec-tives, like the Israeli-Palestinian conflict (Linet al, 2006).6See Choi et al (2012) for an exploration of the interfacebetween hedging and framing.
(13) a. Israeli forces liberated the eastern half ofJerusalem.b.
Israeli forces captured the eastern half ofJerusalem.
(14) a.
Concerned Women for America?s major ar-eas of political activity have consisted of op-position to gay causes, pro-life law...b.
Concerned Women for America?s major ar-eas of political activity have consisted of op-position to gay causes, anti-abortion law...(15) a. Colombian terrorist groups.b.
Colombian paramilitary groups.Framing bias has been studied within the liter-ature on stance recognition and arguing subjectiv-ity.
Because this literature has focused on iden-tifying which side an article takes on a two-sideddebate such as the Israeli-Palestinian conflict (Linet al, 2006), most studies cast the problem as atwo-way classification of documents or sentencesinto for/positive vs. against/negative (Anand etal., 2011; Conrad et al, 2012; Somasundaran andWiebe, 2010), or into one of two opposing views(Yano et al, 2010; Park et al, 2011).
The fea-tures used by these models include subjectivityand sentiment lexicons, counts of unigrams andbigrams, distributional similarity, discourse rela-tionships, and so on.The datasets used by these studies come fromgenres that overtly take a specific stance (e.g.,debates, editorials, blog posts).
In contrast,Wikipedia editors are asked not to advocate a par-ticular point of view, but to provide a balanced ac-count of the different available perspectives.
Forthis reason, overtly biased opinion statements suchas ?I believe that...?
are not common in Wikipedia.The features used by the subjectivity literaturehelp us detect framing bias, but we also need fea-tures that capture epistemological bias expressedthrough presuppositions and entailments.3 Automatically Identifying BiasedLanguageWe now show how the bias cues identified in Sec-tion 2.3 can help solve a new task.
Given a biasedsentence (e.g., a sentence that a Wikipedia editorhas tagged as violating the NPOV policy), our goalin this new task is to identify the word that intro-duces bias.
This is part of a potential three-stepprocess for detecting and correcting biased lan-guage: (1) finding biased phrases, (2) identifyingthe word that introduces the bias, (3) rewording toeliminate the bias.
As we will see below, it can be1653hard even for humans to track down the sources ofbias, because biases in reference works are oftensubtle and implicit.
An automatic bias detectorthat can highlight the bias-inducing word(s) anddraw the editors?
attention to words that need tobe modified could thus be important for improvingreference works like Wikipedia or even in news re-porting.We selected the subset of sentences that had asingle NPOV edit involving one (original) word.
(Although the before form consists of only oneword, the after form can be either one or morewords or the null string (i.e., deletion edits); we donot use the after string in this identification task).The number of sentences in the train, dev and testsets is shown in the last column of Table 1.We trained a logistic regression model on afeature vector for every word that appears in theNPOV sentences from the training set, with thebias-inducing words as the positive class, and allthe other words as the negative class.
The featuresare described in the next section.At test time, the model is given a set of sen-tences and, for each of them, it ranks the words ac-cording to their probability to be biased, and out-puts the highest ranked word (TOP1 model), thetwo highest ranked words (TOP2 model), or thethree highest ranked words (TOP3 model).3.1 FeaturesThe types of features used in the logistic regres-sion model are listed in Table 3, together withtheir value space.
The total number of features is36,787.
The ones targeting framing bias draw onprevious work on sentiment and subjectivity de-tection (Wiebe et al, 2004; Liu et al, 2005).
Fea-tures to capture epistemological bias are based onthe bias cues identified in Section 2.3.A major split separates the features that de-scribe the word under analysis (e.g., lemma, POS,whether it is a hedge, etc.)
from those that de-scribe its surrounding context (e.g., the POS of theword to the left, whether there is a hedge in thecontext, etc.).
We define context as a 5-gram win-dow, i.e., two words to the left of the word un-der analysis, and two to the right.
Taking con-text into account is important given that biases canbe context-dependent, especially epistemologicalbias since it depends on the truth of a proposition.To define some of the features like POS and gram-matical relation, we used the Stanford?s CoreNLPtagger and dependency parser (de Marneffe et al,2006).Features 9?10 use the list of hedges from Hy-land (2005), features 11?14 use the factives andassertives from Hooper (1975), features 15?16use the implicatives from Karttunen (1971), fea-tures 19?20 use the entailments from Berant etal.
(2012), features 21?25 employ the subjectiv-ity lexicon from Riloff and Wiebe (2003), and fea-tures 26?29 use the sentiment lexicon?positiveand negative words?from Liu et al (2005).
If theword (or a word in the context) is in the lexicon,then the feature is true, otherwise it is false.We also included a ?bias lexicon?
(feature 31)that we built based on our NPOV corpus fromWikipedia.
We used the training set to extract thelemmas of words that were the before form of atleast two NPOV edits, and that occurred in at leasttwo different articles.
Of the 654 words includedin this lexicon, 433 were unique to this lexicon(i.e., recorded in neither Riloff and Wiebe?s (2003)subjectivity lexicon nor Liu et al?s (2005) senti-ment lexicon) and represented many one-sided orcontroversial terms, e.g., abortion, same-sex, exe-cute.Finally, we also included a ?collaborative fea-ture?
that, based on the previous revisions of theedit?s article, computes the ratio between the num-ber of times that the word was NPOV-edited andits frequency of occurrence.
This feature was de-signed to capture framing bias specific to an articleor topic.3.2 BaselinesPrevious work on subjectivity and stance recog-nition has been evaluated on the task of classify-ing documents as opinionated vs. factual, for vs.against, positive vs. negative.
Given that the taskof identifying the bias-inducing word of a sentenceis novel, there were no previous results to comparedirectly against.
We ran the following five base-lines.1.
Random guessing.
Naively returns a randomword from every sentence.2.
Role baseline.
Selects the word with thesyntactic role that has the highest probabil-ity to be biased, as computed on the train-ing set.
This is the parse tree root (proba-bility p = .126 to be biased), followed byverbal arguments (p = .085), and the subject(p = .084).1654ID Feature Value Description1* Word <string> Word w under analysis.2 Lemma <string> Lemma of w.3* POS {NNP, JJ, ...} POS of w.4* POS ?
1 {NNP, JJ, ...} POS of one word before w.5 POS ?
2 {NNP, JJ, ...} POS of two words before w.6* POS + 1 {NNP, JJ, ...} POS of one word after w.7 POS + 2 {NNP, JJ, ...} POS of two words after w.8 Position in sentence {start, mid, end} Position of w in the sentence (split into three parts).9 Hedge {true, false} w is in Hyland?s (2005) list of hedges (e.g., apparently).10* Hedge in context {true, false} One/two words) around w is a hedge (Hyland, 2005).11* Factive verb {true, false} w is in Hooper?s (1975) list of factives (e.g., realize).12* Factive verb in context {true, false} One/two word(s) around w is a factive (Hooper, 1975).13* Assertive verb {true, false} w is in Hooper?s (1975) list of assertives (e.g., claim).14* Assertive verb in context {true, false} One/two word(s) around w is an assertive (Hooper, 1975).15 Implicative verb {true, false} w is in Karttunen?s (1971) list of implicatives (e.g., manage).16* Implicative verb in context {true, false} One/two word(s) around w is an implicative (Karttunen, 1971).17* Report verb {true, false} w is a report verb (e.g., add).18 Report verb in context {true, false} One/two word(s) around w is a report verb.19* Entailment {true, false} w is in Berant et al?s (2012) list of entailments (e.g., kill).20* Entailment in context {true, false} One/two word(s) around w is an entailment (Berant et al, 2012).21* Strong subjective {true, false} w is in Riloff and Wiebe?s (2003) list of strong subjectives (e.g.,absolute).22 Strong subjective in context {true, false} One/two word(s) around w is a strong subjective (Riloff andWiebe, 2003).23* Weak subjective {true, false} w is in Riloff and Wiebe?s (2003) list of weak subjectives (e.g.,noisy).24* Weak subjective in context {true, false} One/two word(s) around w is a weak subjective (Riloff andWiebe, 2003).25 Polarity {+, ?, both, ...} The polarity of w according to Riloff and Wiebe (2003), e.g.,praising is positive.26* Positive word {true, false} w is in Liu et al?s (2005) list of positive words (e.g., excel).27* Positive word in context {true, false} One/two word(s) around w is positive (Liu et al, 2005).28* Negative word {true, false} w is in Liu et al?s (2005) list of negative words (e.g., terrible).29* Negative word in context {true, false} One/two word(s) around w is negative (Liu et al, 2005).30* Grammatical relation {root, subj, ...} Whether w is the subject, object, root, etc.
of its sentence.31 Bias lexicon {true, false} w has been observed in NPOV edits (e.g., nationalist).32* Collaborative feature <numeric> Number of times that w was NPOV-edited in the article?s priorhistory / frequency of w.Table 3: Features used by the bias detector.
The star (*) shows the most contributing features.3.
Sentiment baseline.
Logistic regressionmodel that only uses the features based onLiu et al?s (2005) lexicons of positive andnegative words (i.e., features 26?29).4.
Subjectivity baseline.
Logistic regressionmodel that only uses the features based onRiloff and Wiebe?s (2003) lexicon of subjec-tive words (i.e., features 21?25).5.
Wikipedia baseline.
Selects as biased thewords that appear in Wikipedia?s list of wordsto avoid (Wikipedia, 2013a).These baselines assessed the difficulty of thetask, as well as the extent to which traditionalsentiment-analysis and subjectivity features wouldsuffice to detect biased language.3.3 Results and DiscussionTo measure performance, we used accuracy de-fined as:#sentences with the correctly predicted biased word#sentencesThe results are shown in Table 4.
As explainedearlier, we evaluated all the models by outputtingas biased either the highest ranked word or thetwo or three highest ranked words.
These corre-spond to the TOP1, TOP2 and TOP3 columns, re-spectively.
The TOP3 score increases to 59%.
Atool that highlights up to three words to be revisedwould simplify the editors?
job and decrease sig-nificantly the time required to revise.Our model outperforms all five baselines by alarge margin, showing the importance of consid-ering a wide range of features.
Wikipedia?s listof words to avoid falls very short on recall.
Fea-1655System TOP1 TOP2 TOP3Baseline 1: Random 2.18 7.83 9.13Baseline 2: Role 15.65 20.43 25.65Baseline 3: Sentiment 14.78 22.61 27.83Baseline 4: Subjectivity 16.52 25.22 33.91Baseline 5: Wikipedia 10.00 10.00 10.00Our system 34.35 46.52 58.70Humans (AMT) 37.39 50.00 59.13Table 4: Accuracy (%) of the bias detector on thetest set.tures that contribute the most to the model?s per-formance (in a feature ablation study on the devset) are highlighted with a star (*) in Table 3.
Inaddition to showing the importance of linguisticcues for different classes of bias, the ablation studyhighlights the role of contextual features.
The biaslexicon does not seem to help much, suggestingthat it is overfit to the training data.An error analysis shows that our system makesacceptable errors in that words wrongly predictedas bias-inducing may well introduce bias in a dif-ferent context.
In (16), the system picked eschew,whereas orthodox would have been the correctchoice according to the gold edit.
Note that boththe sentiment and the subjectivity lexicons list es-chew as a negative word.
The bias type that posesthe greatest challenge to the system are terms thatare one-sided or loaded in a particular topic, suchas orthodox in this example.
(16) a.
Some Christians eschew orthodox theology; suchas the Unitarians, Socinian, [...]b.
Some Christians eschew mainstream trinitariantheology; such as the Unitarians, Socinian, [...]The last row in Table 4 lists the performanceof humans on the same task, presented in the nextsection.4 Human Perception of Biased LanguageIs it difficult for humans to find the word in asentence that induces bias, given the subtle, of-ten implicit biases in Wikipedia.
We used Ama-zon Mechanical Turk7 (AMT) to elicit annotationsfrom humans for the same 230 sentences from thetest set that we used to evaluate the bias detectorin Section 3.3.
The goal of this annotation wastwofold: to compare the performance of our biasdetector against a human baseline, and to assessthe difficulty of this task for humans.
While AMTlabelers are not trained Wikipedia editors, under-7http://www.mturk.comstanding how difficult these cases are for untrainedlabelers is an important baseline.4.1 TaskOur HIT (Human Intelligence Task) was called?Find the biased word!?.
We kept the task descrip-tion succinct.
Turkers were shown Wikipedia?sdefinition of a ?biased statement?
and two exam-ple sentences that illustrated the two types of bias,framing and epistemological.
In each HIT, annota-tors saw 10 sentences, one after another, and eachone followed by a text box entitled ?Word intro-ducing bias.?
For each sentence, they were askedto type in the text box the word that caused thestatement to be biased.
They were only allowed toenter a single word.Before the 10 sentences, turkers were asked tolist the languages they spoke as well as their pri-mary language in primary school.
This was En-glish in all the cases.
In addition, we included aprobe question in the form of a paraphrasing task:annotators were given a sentence and two para-phrases (a correct and a bad one) to choose from.The goal of this probe question was to discardannotators who were not paying attention or didnot have a sufficient command of English.
Thissimple test was shown to be effective in verifyingand eliciting linguistic attentiveness (Munro et al,2010).
This was especially important in our caseas we were interested in using the human annota-tions as an oracle.
At the end of the task, partici-pants were given the option to provide additionalfeedback.We split the 230 sentences into 23 sets of 10sentences, and asked for 10 annotations of eachset.
Each approved HIT was rewarded with $0.30.4.2 Results and DiscussionOn average, it took turkers about four minutes tocomplete each HIT.
The feedback that we got fromsome of them confirmed our hypothesis that find-ing the bias source is difficult: ?Some of the ?bi-ases?
seemed very slight if existent at all,?
?Thiswas a lot harder than I thought it would be... Inter-esting though!
?.We postprocessed the answers ignoring case,punctuation signs, and spelling errors.
To ensurean answer quality as high as possible, we onlykept those turkers who answered attentively by ap-plying two filters: we only accepted answers thatmatched a valid word from the sentence, and wediscarded answers from participants who did not16562 3 4 5 6 7 8 9 10Number of times the top word was selectedNumber of sentences01020304050Figure 1: Distribution of the number of turkerswho selected the top word (i.e., the word selectedby the majority of turkers).pass the paraphrasing task?there were six suchcases.
These filters provided us with confidence inthe turkers?
answers as a fair standard of compari-son.Overall, humans correctly identified the biasedword 30% of the time.
For each sentence, weranked the words according to the number of turk-ers (out of 10) who selected them and, like wedid for the automated system, we assessed per-formance when considering only the top word(TOP1), the top 2 words (TOP2), and the top 3words (TOP3).
The last row of Table 4 reports theresults.
Only 37.39% of the majority answers co-incided with the gold label, slightly higher thanour system?s accuracy.
The fact that the humananswers are very close to the results of our systemreflects the difficulty of the task.
Biases in refer-ence works can be very subtle and go unnoticedby humans; automated systems could thus be ex-tremely helpful.As a measure of inter-rater reliability, we com-puted pairwise agreement.
The turkers agreed40.73% of the time, compared to the 5.1% chanceagreement that would be achieved if raters hadrandomly selected a word for each sentence.
Fig-ure 1 plots the number of times the top word ofeach sentence was selected.
The bulk of the sen-tences only obtained between four and six answersfor the same word.There is a good amount of overlap (?34%) be-tween the correct answers predicted by our systemand those from humans.
Much like the automatedsystem, humans also have the hardest time identi-fying words that are one-sided or controversial toa specific topic.
They also picked eschew for (16)instead of orthodox.
Compared to the system, theydo better in detecting bias-inducing intensifiers,and about the same with epistemological bias.5 Related WorkThe work in this paper builds upon prior work onsubjectivity detection (Wiebe et al, 2004; Lin etal., 2011; Conrad et al, 2012) and stance recogni-tion (Yano et al, 2010; Somasundaran and Wiebe,2010; Park et al, 2011), but applied to the genreof reference works such as Wikipedia.
Unlike theblogs, online debates and opinion pieces whichhave been the major focus of previous work, biasin reference works is undesirable.
As a result,the expression of bias is more implicit, making itharder to detect by both computers and humans.Of the two classes of bias that we uncover, fram-ing bias is indeed strongly linked to subjectiv-ity, but epistemological bias is not.
In this re-spect, our research is comparable to Greene andResnik?s (2009) work on identifying implicit sen-timent or perspective in journalistic texts, based onsemantico-syntactic choices.Given that the data that we use is not supposedto be opinionated, our task consists in detecting(implicit) bias instead of classifying into side Aor B documents about a controversial topic likeObamaCare (Conrad et al, 2012) or the Israeli-Palestinian conflict (Lin et al, 2006; Greene andResnik, 2009).
Our model detects whether allthe relevant perspectives are fairly represented byidentifying statements that are one-sided.
To thisend, the features based on subjectivity and senti-ment lexicons turn out to be helpful, and incor-porating more features for stance detection is animportant direction for future work.Other aspects of Wikipedia structure have beenused for other NLP applications.
The Wikipediarevision history has been used for spelling correc-tion, text summarization (Nelken and Yamangil,2008), lexical simplification (Yatskar et al, 2010),paraphrasing (Max and Wisniewski, 2010), andtextual entailment (Zanzotto and Pennacchiotti,2010).
Ganter and Strube (2009) have usedWikipedia?s weasel-word tags to train a hedge de-tector.
Callahan and Herring (2011) have exam-ined cultural bias based on Wikipedia?s NPOVpolicy.16576 ConclusionsOur study of bias in Wikipedia has implicationsfor linguistic theory and computational linguis-tics.
We show that bias in reference works fallsbroadly into two classes, framing and epistemo-logical.
The cues to framing bias are more ex-plicit and are linked to the literature on subjec-tivity; cues to epistemological bias are subtle andimplicit, linked to presuppositions and entailmentsin the text.
Epistemological bias has not receivedmuch attention since it does not play a major rolein overtly opinionated texts, the focus of much re-search on stance recognition.
However, our logis-tic regression model reveals that epistemologicaland other features can usefully augment the tradi-tional sentiment and subjectivity features for ad-dressing the difficult task of identifying the bias-inducing word in a biased sentence.Identifying the bias-inducing word is a chal-lenging task even for humans.
Our linguistically-informed model performs nearly as well as hu-mans tested on the same task.
Given the sub-tlety of some of these biases, an automated sys-tem that highlights one or more potentially biasedwords would provide a helpful tool for editors ofreference works and news reports, not only mak-ing them aware of unnoticed biases but also sav-ing them hours of time.
Future work could in-vestigate the incorporation of syntactic features orfurther features from the stance detection litera-ture.
Features from the literature on veridicality(de Marneffe et al, 2012) could be informative ofthe writer?s commitment to the truth of the eventsdescribed, and document-level features could helpassess the extent to which the article provides abalanced account of all the facts and points ofview.Finally, the NPOV data and the bias lexicon thatwe release as part of this research could prove use-ful in other bias related tasks.AcknowledgmentsWe greatly appreciate the support of Jean Wu andChristopher Potts in running our task on Ama-zon Mechanical Turk, and all the Amazon Turkerswho participated.
We benefited from commentsby Valentin Spitkovsky on a previous draft andfrom the helpful suggestions of the anonymous re-viewers.
The first author was supported by a Beat-riu de Pino?s postdoctoral scholarship (2010 BP-A00149) from Generalitat de Catalunya.
The sec-ond author was supported by NSF IIS-1016909.The last author was supported by the Center forAdvanced Study in the Behavioral Sciences atStanford.ReferencesPranav Anand, Marilyn Walker, Rob Abbott, JeanE.
Fox Tree, Robeson Bowmani, and Michael Mi-nor.
2011.
Cats rule and dogs drool!
: Classifyingstance in online debate.
In Proceedings of ACL-HLT 2011 Workshop on Computational Approachesto Subjectivity and Sentiment Analysis, pages 1?9.Jonathan Berant, Ido Dagan, Meni Adler, and JacobGoldberger.
2012.
Efficient tree-based approxima-tion for entailment graph learning.
In Proceedingsof ACL 2012, pages 117?125.Ewa Callahan and Susan C. Herring.
2011.
Cul-tural bias in Wikipedia articles about famous per-sons.
Journal of the American Society for Informa-tion Science and Technology, 62(10):1899?1915.Eunsol Choi, Chenhao Tan, Lillian Lee, CristianDanescu-Niculescu-Mizil, and Jennifer Spindel.2012.
Hedge detection as a lens on framing in theGMO debates: a position paper.
In Proceedingsof the ACL-2012 Workshop on Extra-PropositionalAspects of Meaning in Computational Linguistics,pages 70?79.Alexander Conrad, Janyce Wiebe, and Rebecca Hwa.2012.
Recognizing arguing subjectivity and argu-ment tags.
In Proceedings of ACL-2012 Workshopon Extra-Propositional Aspects of Meaning in Com-putational Linguistics, pages 80?88.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InProceedings of LREC 2006.Marie-Catherine de Marneffe, Christopher D. Man-ning, and Christopher Potts.
2012.
Did it hap-pen?
The pragmatic complexity of veridicality as-sessment.
Computational Linguistics, 38(2):301?333.Robert M. Entman.
2007.
Framing bias: Media in thedistribution of power.
Journal of Communication,57(1):163?173.Viola Ganter and Michael Strube.
2009.
Findinghedges by chasing weasels: Hedge detection usingWikipedia tags and shallow linguistic features.
InProceedings of ACL-IJCNLP 2009, pages 173?176.Stephan Greene and Philip Resnik.
2009.
More thanwords: Syntactic packaging and implicit sentiment.In Proceedings of NAACL-HLT 2009, pages 503?511.1658Joan B. Hooper.
1975.
On assertive predicates.
InJ.
Kimball, editor, Syntax and Semantics, volume 4,pages 91?124.
Academic Press, New York.Ken Hyland.
2005.
Metadiscourse: Exploring Interac-tion in Writing.
Continuum, London and New York.Lauri Karttunen.
1971.
Implicative verbs.
Language,47(2):340?358.Paul Kiparsky and Carol Kiparsky.
1970.
Fact.
InM.
Bierwisch and K. E. Heidolph, editors, Progressin Linguistics, pages 143?173.
Mouton, The Hague.Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, andAlexander Hauptmann.
2006.
Which side are youon?
Identifying perspectives at the document andsentence levels.
In Proceedings of CoNLL 2006,pages 109?116.Chenghua Lin, Yulan He, and Richard Everson.2011.
Sentence subjectivity detection withweakly-supervised learning.
In Proceedings ofAFNLP 2011, pages 1153?1161.Bing Liu, Minqing Hu, and Junsheng Cheng.
2005.Opinion Observer: analyzing and comparing opin-ions on the Web.
In Proceedings of WWW 2005,pages 342?351.Aure?lien Max and Guillaume Wisniewski.
2010.
Min-ing naturally-occurring corrections and paraphrasesfrom Wikipedia?s revision history.
In Proceedingsof LREC 2010, pages 3143?3148.Robert Munro, Steven Bethard, Victor Kuperman,Vicky Tzuyin Lai, Robin Melnick, ChristopherPotts, Tyler Schnoebelen, and Harry Tily.
2010.Crowdsourcing and language studies: the new gen-eration of linguistic data.
In Proceedings of theNAACL-HLT 2010 Workshop on Creating Speechand Language Data With Amazons MechanicalTurk, pages 122?130.Rani Nelken and Elif Yamangil.
2008.
MiningWikipedias article revision history for training Com-putational Linguistics algorithms.
In Proceedings ofthe 1st AAAI Workshop on Wikipedia and ArtificialIntelligence.Souneil Park, KyungSoon Lee, and Junehwa Song.2011.
Contrasting opposing views of news articleson contentious issues.
In Proceedings of ACL 2011,pages 340?349.Ellen Riloff and Janyce Wiebe.
2003.
Learning extrac-tion patterns for subjective expressions.
In Proceed-ings of EMNLP 2003, pages 105?112.Swapna Somasundaran and Janyce Wiebe.
2010.
Rec-ognizing stances in ideological on-line debates.
InProceedings of the NAACL-HLT 2010 Workshop onComputational Approaches to Analysis and Genera-tion of Emotion in Text, pages 116?124.Peter D. Turney.
2002.
Thumbs up or thumbs down?Semantic orientation applied to unsupervised clas-sification of reviews.
In Proceedings of ACL 2002,pages 417?424.Janyce Wiebe, Theresa Wilson, Rebecca Bruce,Matthew Bell, and Melanie Martin.
2004.
Learn-ing subjective language.
Computational Linguistics,30(3):277?308.Wikipedia.
2013a.
Wikipedia: Manual of style / Wordsto watch.
http://en.wikipedia.org/wiki/Wikipedia:Words_to_avoid.
[Re-trieved February 5, 2013].Wikipedia.
2013b.
Wikipedia: Neutral point of view.http:http://en.wikipedia.org/wiki/Wikipedia:Neutral_point_of_view.
[Retrieved February 5, 2013].Tae Yano, Philip Resnik, and Noah A. Smith.
2010.Shedding (a thousand points of) light on biased lan-guage.
In Proceedings of the NAACL-HLT 2010Workshop on Creating Speech and Language DataWith Amazons Mechanical Turk, pages 152?158.Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-Mizil, and Lillian Lee.
2010.
For the sake of sim-plicity: Unsupervised extraction of lexical simplifi-cations from Wikipedia.
In Proceedings of NAACL-HLT 2010, pages 365?368.Fabio M. Zanzotto and Marco Pennacchiotti.
2010.Expanding textual entailment corpora fromWikipedia using co-training.
In Proceedings ofthe 2nd Coling Workshop on The People?s WebMeets NLP: Collaboratively Constructed SemanticResources, pages 28?36.1659
