Person Name Disambiguation based on Topic ModelJiashen Sun, Tianmin Wang and Li LiCenter of Intelligence Science and TechnologyBeijing University of Posts and Telecommunicationsb.bigart911@gmail.com,tianmin180@sina.com, wbg111@126.comXing WuSchool of ComputerBeijing University of Posts andTelecommunicationswuxing-6@163.comAbstractIn this paper we describe ourparticipation in the SIGHAN 2010 Task-3 (Person Name Disambiguation) anddetail our approaches.
Person NameDisambiguation is typically viewed as anunsupervised clustering problem wherethe aim is to partition a name?s contextsinto different clusters, each representinga real world people.
The key point ofClustering is the similarity measure ofcontext, which depends upon the featuresselection and representation.
Twoclustering algorithms, HAC andDBSCAN, are investigated in our system.The experiments show that the topicfeatures learned by LDA outperformstoken features and more robust.1 IntroductionMost current web searches relate to personnames.
A study of the query log of theAllTheWeb and Altavista search sites gives anidea of the relevance of the people search task:11-17% of the queries were composed of aperson name with additional terms and 4% wereidentified simply as person names (Spink et al,2004).However, there is a high level of ambiguitywhere multiple individuals share the same nameand thus the harvesting and the retrieval ofrelevant information becomes more difficult.This ambiguity has recently become an activeresearch topic and, simultaneously, a relevantapplication domain for Web search services.Zoominfo.com, Spock.com and 123people.comare examples of sites which perform web peoplesearch, although with limited disambiguationcapabilities (Artiles et al, 2009).This issue directed current researcherstowards the definition of a new task called WebPeople Search (WePS) or Personal NameDisambiguation (PND).
The key assumptionunderlying the task is that the contextsurrounding an ambiguous person name isindicative of its ascription.
The goal of theclustering task was to group web pagescontaining the target person's name, so thatpages referring to the same individual areassigned to the same cluster.
For this purpose alarge dataset was collected and manuallyannotated.Moreover, because of the ambiguity in wordsegmentation in Chinese, person name detectionis necessary, which is subtask of Named EntityRecognition (NER).
NER is one of difficultiesof the study of natural language processing, ofwhich the main task is to identify person names,place names, organization names, number, timewords, money and other entities.
The maindifficulties of Chinese person name entityrecognition are embodied in the following points:1) the diversity of names form; 2) the Chinesecharacter within names form words with each; 3)names and their context form words; 4)translation of foreign names require specialconsiderations.In this paper we describe our system andapproach in the SIGHAN 2010 task-3 (PersonName Disambiguation).
A novel Bayesianapproach is adopt in our system, whichformalizes the disambiguation problem in agenerative model.
For each ambiguous name wefirst draw a distribution over person, and thengenerate context words according to thisdistribution.
It is thus assumed that differentpersons will correspond to distinct lexicaldistributions.
In this framework, Person NameDisambiguation postulates that the observed data(contexts) are explicitly intended tocommunicate a latent topic distributioncorresponding to real world people.The remainder of this paper is structured asfollows.
We first present an overview of relatedwork (Section 2) and then describe our systemwhich consists of NER and clustering in moredetails (Sections 3 and 4).
Section 5 describesthe resources and evaluation results in ourexperiments.
We discuss our results andconclude our work in Section 6.2 Related WorkThe most commonly used feature is the bag ofwords in local or global context of theambiguous name (Ikeda et al, 2009; Romano etal., 2009).
Because the given corpus is often notlarge enough to learn the realistic probabilitiesor weights for those features, traditionalalgorithm such as vector-based techniques usedin large-scale text will lead to data sparseness.In recent years, more and more importantstudies have attempted to overcome the problemto get a better (semantic) similarity measures.
Alot of features such as syntactic chunks, namedentities, dependency parses, semantic role labels,etc., were employed.
However, these featuresneed many NLP preprocessing (Chen, 2009).Many studies show that they can achieve state-of-the-art performances only with lightweightfeatures.
Pedersen et al (2005) presentSenseClusters which represents the instances tobe clustered using second order co?occurrencevectors.
Kozareva (2008) focuses on theresolution of the web people search problemthrough the integration of domain information,which can represent relationship betweencontexts and is learned from WordNet.
PoBOCclustering (Cleuziou et al, 2004) is used whichbuilds a weighted graph with weights being thesimilarity among the objects.Another way is to utilize universal datarepositories as external knowledge sources (Raoet al, 2007; Kalmar and Blume, 2007; Pedersenand Kulkarni; 2007) in order to give morerealistic frequency for a proper name or measurewhether a bigram is a collocation.Phan et al (2008) presents a generalframework for building classifiers that deal withshort and sparse text and Web segments bymaking the most of hidden topics discoveredfrom large-scale data collections.
Samuel Brodyet al (2009) adopt a novel Bayesian approachand formalize the word sense induction problemin a generative model.Previous work using the WePS1 (Artiles et al,2007) or WePS2 data set (Artiles et al, 2009)shows that standard document clusteringmethods can deliver excellent performance ifsimilarity measure is enough good to representrelationship of context.The study in Chinese PND is still in itsinfancy.
Person Name detection is oftennecessary in Chinese.
At present, the maintechnology of person name recognition is usedstatistical models, and the hybrid approach.
Liuet al (2000) designed a Chinese person namerecognition system based on statistical methods,using samples of names from the text corpus andthe real amount of statistical data to improve thesystem performance, while the shortcoming isthat samples of name database are too small,resulting in low recall.
Li et al (2006) use thecombination of the boundary templates and localstatistics to recognize Chinese person name, therecognition process is to use the boundary withthe frequency of template to identify potentialnames, and to recognize the results spread to theentire article in order to recall missing namescaused by sparse data.3 Person Name RecognitionIn this section, we focus on Conditional RandomFields (CRFs) algorithm to establish theappropriate language model.
Given of the inputtext, we may detect the potential person namesin the text fragments, and then take variousfeatures into account to recognize of Chineseperson names.Conditional Random Fields as a sequencelearning method has been successfully applied inmany NLP tasks.
More details of the itsprinciple can be referred in (Lafferty, McCallum,and Pereira, 2001; Wallach, 2004).
We here willfocus on how to apply CRFs in our person namerecognition task.3.1 CRFs-based name recognitionCRFs is used to get potential names as the firststage name recognition outcome.
To avoid theinterference that caused by word segmentationerrors, we use single Chinese characterinformation rather than word as discriminativefeatures for CRFs learning model.We use BIEO label strategy to transfer the namerecognition as a sequence learning task.
Thelabel set includes: B-Nr (Begin, the initialcharacter of name), I-Nr (In, the middlecharacter of name), E-Nr(End, the end characterof name) and O (Other, other characters thataren?t name).3.2 Rule-based CorrectionAfter labeling the potential names by CRFsmodel, we apply a set of rules to boostrecognition result, which has been proved to bethe key to improve Chinese name recognition.The error of the potential names outcome byCRFs model is mainly divided into the followingcategories: the initial character of name is notrecognized, the middle character of name is notrecognized, the end character of name is notrecognized, and their combinations of thosethree errors.
The other two extreme errors,including non-name recognition for the anchorname, and the name is not recognized aspotential names.In the stage of rule-based correction, we firstconduct word segmentation for the text.
Thesegmentation process is also realized with themethod of CRFs, without using dictionaries andother external knowledge.
The detaileddescription is beyond this paper, which can beaccessible in the paper (Lafferty, McCallum, andPereira, 2001).
The only thing we should note isthat part of the error in such segmentation resultobtained in this way can be corrected throughthe introduction of an external dictionary.For each potential name, and we examine itfrom the following two aspects:1) It is reasonable to use the word in a personname, including checking the surname and thecharacter used in names;2) The left and right borders are correct.Check the left and right sides of the cutting unitcan be added to the names, including the wordsused before names, the words used behindnames and the surname and character used innames.4 Clustering4.1 FeaturesThe clustering features we used can be dividedinto two types, one is token features, includingword (after stop-word removal), uni-characterand bi-character, the other is topic features,which is topic-based distribution of global orwindow context learned by LDA (LatentDirichlet Allocation) model.4.1.1 Token-based FeaturesSimple token-based features are used in almostevery disambiguation system.
Here, we extractthree kinds of tokens: words, uni-char and bi-char occurring in a given document.Then, each token in each feature vector isweighed by using a tf-idf weighting and entropyweighting schemes defined as follows.tf-idf weighting:log( )ik ikiNa fn<entropy weighting:11log( 1.0) 1 log( )log( )Nij ijik ikj i if fa fN n n?
??
? ?
??
??
??
??
?
?<where is the frequency of term i indocument k, N is the number of document incorpus, is the frequency of term i in corpus.So,11log( )log( )Nij ijj i if fN n n?
??
??
?
?is the average uncertainty or entropy of term i.Entropy weighting is based on informationtheoretic ideas and is the most sophisticatedweighting scheme.4.1.2 Features SelectionIn this Section, we give a brief introduction ontwo effective unsupervised feature selectionmethods, DF and global tf-idf.DF (Document frequency) is the number ofdocuments in which a term occurs in a dataset.
Itis the simplest criterion for term selection andeasily scales to a large dataset with linearcomputation complexity.
It is a simple buteffective feature selection method for textcategorization (Yang & Pedersen, 1997).We introduce a new feature selection methodcalled ?global tf-idf?
that takes the term weightinto account.
Because DF assumes that eachterm is of same importance in differentdocuments, it is easily biased by those commonterms which have high document frequency butuniform distribution over different classes.Global tf-idf is proposed to deal with thisproblem:1Ni ikkg tfidf?4.1.3 Latent Dirichlet Allocation (LDA)Our work is related to Latent DirichletAllocation (LDA, Blei et al 2003), aprobabilistic generative model of text generation.LDA models each document using a mixtureover K topics, which are in turn characterized asdistributions over words.
The main motivation isthat the task, fail to achieve high accuracy due tothe data sparseness.LDA is a generative graphical model asshown in Figure 1.
It can be used to model anddiscover underlying topic structures of any kindof discrete data in which text is a typicalexample.
LDA was developed based on anassumption of document generation processdepicted in both Figure 1 and Table 1.Figure 1 Generation Process for LDA4.1.4 LDA Estimation with Gibbs SamplingEstimating parameters for LDA by directly andexactly maximizing the likelihood of the wholedata collection is intractable.
The solution to thisis to use approximate estimation methods likeGibbs Sampling (Griffiths and Steyvers, 2004).Here, we only show the most importantformula that is used for topic sampling for words.$IWHUILQLVKLQJ*LEEV6DPSOLQJWZRPDWULFHV?DQG?DUH computed as follows.where ?
is the latent topic distributioncorresponding to real world people.4.1.5 Topic-based FeaturesThrough the observation for the given corpus,many key information, like occupation,affiliation, mentor, location, and so on, in manycases, around the target name.
So, both local andglobal context are choose to doing topic analysis.Finally, the latent topic distributions are topic-based representation of context.4.2 ClusteringOur system trusts the result of Person Namedetection absolutely, so contexts need to doclustering only if they refer to persons with thesame name.
We experimented with two differentclassical clustering methods: HAC andDBSCAN.4.2.1 HACAt the heart of hierarchical clustering lies thedefinition of similarity between clusters, whichbased on similarity between individualdocuments.
In my system, a linear combinationof similarity based on both local and globalcontext is employed:(1 )global localsim sim simD D ?
 where, the general similarity between twofeatures-vector of documents di and dj isdefined as the cosine similarity:( , ) i ji ji jd dsim d dd d<We will now refine this algorithm for thedifferent similarity measures of single-link,complete-link, group-average and centroidclustering when clustering two smaller clusterstogether.
In our approach we used an overallsimilarity stopping threshold.4.2.2 DBSCANIn this section, we present the algorithmDBSCAN (Density Based Spatial Clustering ofApplications with Noise) (Ester et al, 1996)(Table 2) which is designed to discover theclusters and the noise in a spatial database.Table 2 Algorithm of DBSCANArbitrary select a point pRetrieve all points density-reachable from p wrtEps and MinPts.If p is a core point, a cluster is formed.If p is a border point, no points are density-reachable from p and DBSCAN visits thenext point of the database.Continue the process until all of the points5 Experiments and Results AnalysisWe run all experiments on SIGHAN 2010training and test corpus.5.1 Preprocessing and Person NameRecognitionFirstly, a word segmentation tool based on CRFis used in each document.
Then, person namerecognition is processing.
The training data forword segmentation and PNR is People's Daily inJanuary, 1998 and the whole 2000, respectively.5.2 Feature SpaceOur experiments used five types of feature (uni-char, bi-char, word and topic in local and global),two feature weighting methods (tf-idf andentropy) and two feature selection methods (DFand global tf-idf).5.3 Model Selection in LDAOur model is conditioned on the Dirichlethyperparameters D andE , the number of topicK and iterations.
The value for the D was set to0.2, which was optimized in tuning experimentused training datasets.
The E was set to 0.1,which is often considered optimal in LDA-related models (Griffiths and Steyvers, 2004).The K was set to 200.
The Gibbs sampler wasrun for 1,000 iterations.5.4 Clustering Results and AnalysisSince the parameter setting for the clusteringsystem is very important, we focus only on theB-cubed scoring (Artiles et al, 2009), andacquire an overall optimal fixed stop-thresholdfrom the training data, and then use it in test data.In this section, we report our results evaluatedby the clustering scoring provided by SIGNAN2010 evaluation, which includes both the B-cubed scoring and the purity-based scoring.Table 3 and 4 demonstrate the performance (Fscores) of our system in different featuresrepresentation and clustering for the trainingdata of the SIGNAN 2010.
In Table 3, thenumbers in parentheses are MinPts and Epsrespectively, and stop-threshold in Table 4.
Asshown in Table 3, DBSCAN isn?t suitable forthis task, and the results are very sensitive toparameters.
So we didn?t submit DBSCAN-based results.Table 4 shows that the best averaged F-scoresfor PND are based on topic model, which meetour initial assumptions, and result based onmerging local and global information is a bitbetter than both local and global informationindependently.
Also, the results based on topicmodel are the most robust because the F-score ofvariation is slightly with stop-threshold changing.Conversely, the results based on token are notlike this.
As the performance of segmentation isnot very satisfactory, results based on word areworst, even worse than uni-char-based.
Inaddition, it is found that global tf-idf is betterthan DF, which is the simplest unsupervisedfeature selection method.
Entropy weighting ismore effective than tf-idf weighting.Table 5 shows that the evaluation results intest data on SIGHAN 2010, and the last twolines are results in diagnosis test.
We are in fifthplace.
The evaluation results (F-score) of PersonName Recognition in training data is 0.965.Features FS Weighting B-Cubed P-IPprecision recall F P IP Fword (0.19)DFtf-idf79.05 79.68 76.49 83.25 85.84 82.72word (0.2) 80.99 75.72 75.54 84.67 83.08 82.2word (0.3) entropy 78.8 80.71 77.42 83.13 86.62 83.58word (0.25)global tf-idf tf-idf80.79 83.1 80.53 84.88 88.32 85.79word (0.23) 79.45 84.49 79.66 83.76 89.25 85.08uni-char (0.43)DF tf-idf76.47 85.46 78.77 81.7 90.05 84.45uni-char (0.5) 82.34 75.97 77 86.11 83.54 83.78uni-char (0.48) 80.42 79.44 78.01 84.53 86.17 84.26bi-char (0.35) 88.3 67.75 75.34 89.96 77.38 82.44bi-char (0.315) 81.84 81.58 80.54 85.72 87.17 85.8local topic (0.6) 78.76 86.8 80.63 83.27 91.16 85.88global topic (0.4) 77.92 88.72 81.04 82.67 92.64 86.26global topic (0.7) 80.54 88.43 83.55 84.76 92.55 88.02merged topic (0.63) 81.39 87.82 83.88 85.42 91.94 88.21Table 3    Performance of HACB-Cubed P-IPMinPtsand Epsprecision recall F P IP F2  0.9 64.15 95.84 74.19 71.95 97.36 80.972  0.4 71.34 62.25 63.95 76.56 71.94 72.593  0.9 64.15 95.88 74.2 71.95 97.37 80.976  0.95 64.12 96.55 74.44 71.92 97.79 81.12B-Cubed P-IPprecision recall F P IP F80.33 94.52 85.79 85.1 96.46 89.7780.56 92.56 85.29 85.34 95.19 89.580.43 95.41 86.18 85.07 97.06 89.9680.82 93.41 85.77 85.62 95.76 89.91Table 5   Evaluation Results in test dataTable 4    Performance of DBSCAN6 Discussion and Future WorkIn this paper, we present implementation of oursystems for SIGHAN-2010 PND bekeoff,.Theexperiments show that the topic features learnedby LDA outperform token features and exhibitgood robustness.However, in our system, only given data isexploited.
We are going to collect a very largeexternal data as universal dataset to train topicmodel, and then do clustering on both a small setof training data and a rich set of hidden topicsdiscovered from universal dataset.
The universaldataset can be snippets returned by searchengine or Wikipedia queried by target name andsome keywords, and so on.We built our PDN system on the result ofperson name recognition.
However, it is notappropriate to toally trust the result of PersonName detection.
So an algorithm that can correctNER mistakes should be investigated in futurework..Moreover, Cluster Ensemble system canensure the result to be more robust and accurateaccordingly, which is another direction of futurework..AcknowledgmentsThis research has been partially supported by theNational Science Foundation of China (NO.NSFC90920006).
We also thank Xiaojie Wang,Caixia Yuan and Huixing Jiang for usefuldiscussion of this work.ReferencesSpink, B. Jansen, and J. Pedersen.
2004.Searching for people on web search engines.Journal of Documentation, 60:266 -278.Javier Artiles, Julio Gonzalo, and Satoshi Sekine.2009.
Weps 2 evaluation campaign: overviewof the web people search clustering task.
InWePS 2 Evaluation Workshop.
WWWConference.Javier Artiles, Julio Gonzalo, and Satoshi Sekine.2007.
The semeval-2007 weps evaluation:Establishing a benchmark for the web peoplesearch task.
In Proceedings of the FourthInternational Workshop on SemanticEvaluations (SemEval-2007).ACL.M.
Ikeda, S. Ono, I. Sato, M. Yoshida, and H.Nakagawa.
2009.
Person namedisambiguation on the web by twostageclustering.
In 2nd Web People SearchEvaluation Workshop (WePS 2009), 18thWWW Conference.L.
Romano, K. Buza, C. Giuliano, and L.Schmidt-Thieme.
2009.
Person namedisambiguation on the web by twostageclustering.
In 2nd Web People SearchEvaluation Workshop (WePS 2009), 18thWWW Conference.Y.
Chen, S. Y. M. Lee, and C.-R. Huang.
2009.Polyuhk: A robust information extractionsystem for web personal names.
In 2nd WebPeople Search Evaluation Workshop (WePS2009), 18th WWW Conference.Z.
Kozareva, R. Moraliyski, and G. Dias.
2008.Web people search with domain ranking.
InTSD '08: Proceedings of the 11thinternational conference on Text, Speech andDialogue, 133-140, Berlin, Heidelberg.Pedersen, Ted, Amruta Purandare, and AnaghaKulkarni.
2005.
Name Discrimination byClustering Similar Contexts.
In Proceedingsof the Sixth International Conference onIntelligent Text Processing andComputational Linguistics, Mexico City,Mexico.G.
Cleuziou, L. Martin, and C. Vrain.
2004.Poboc: an overlapping clustering algorithm.application to rule-based classification andtextual data, 440-444.Kalmar, Paul and Matthias Blume.
2007.
FICO:Web Person Disambiguation Via WeightedSimilarity of Entity Contexts.
In Proceedingsof the Fourth International Workshop onSemantic Evaluations (SemEval-2007).ACL.Rao, Delip, Nikesh Garera and David Yarowsky.2007.
JHU1 : An Unsupervised Approach toPerson Name Disambiguation using WebSnippets.
In Proceedings of the FourthInternational Workshop on SemanticEvaluations (SemEval-2007).ACL.Pedersen, Ted and Anagha Kulkarni.
2007.Unsupervised Discrimination of PersonNames in Web Contexts.
In Proceedings ofthe Eighth International Conference onIntelligent Text Processing andComputational Linguistics, Mexico City,Mexico.Phan, X., Nguyen, L. and Horiguchi.
2008.Learning to Classify Short and Sparse Test &Web with Hidden Topics from large-scaleData collection.
In Proceedings of 17thInternational World Wide Web Conference.
(Beijing, China, April 21-25, 2008).
ACMPress, New York, NY, 91-100.Samuel Brody and Mirella Lapata.
2009.Bayesian word sense induction InProceedings of the 12th Conference of theEuropean Chapter of the Association forComputational Linguistics, 103-111.Sun et al 1995.
Identifying Chinese Names inUnrestricted Texts (in Chinese).
In Journal ofChinese Information Processing, 9(2):16-27.Liu et al 2000.
Statistical Chinese PersonNames Identification (in Chinese).
In Journalof Chinese Information Processing, 14(3):16-24.Huang et al 2001.
Identification of ChineseNames Based on Statistics (in Chinese).
InJournal of Chinese Information Processing,15(2):31-37.Li et al 2006.
Chinese Name Recognition Basedon Boundary Templates and Local Frequency(in Chinese).
In Journal of ChineseInformation Processing, 20(5):44-50.Mao et al 2007.
Recognizing Chinese PersonNames Based on Hybrid Models (in Chinese).In Journal of Chinese Information Processing,21(2):22-27.J.
Lafferty, A. McCallum, and F. Pereira.
2001.Conditional random fields: Probabilisticmodels for segmenting and labeling sequencedata.
In Proc.
ICML-01, 282-289, 2001.Wallach, Hanna.
2004.
Conditional randomfields: An introduction.
Technical report,University of Pennsylvania, Department ofComputer and Information Science.Yang, Y. and Pedersen, J. O.
1997.
Acomparative study on feature selection in textcategorization.
In Proceedings of ICML-97,14th International Conference on MachineLearning (Nashville, US, 1997), 412?420.Martin Ester, Hans-Peter Kriegel, J?rg Sander,Xiaowei Xu.
1996.
A density-based algorithmfor discovering clusters in large spatialdatabases with noise.
In Proceedings of theSecond International Conference onKnowledge Discovery and Data Mining(KDD-96).
AAAI Press.
226-231.Blei, David M., Andrew Y. Ng, and Michael I.Jordan.
2003.
Latent dirichlet alocation.
In J.Machine Learn.
Res.
3, 993-1022.T.
Griffiths and M. Steyvers.
2004.
Findingscientific topics.
In The National Academy ofSciences, 101:5228-5235.
