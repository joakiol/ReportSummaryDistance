COMPOSE-REWUCE PARSINGHenry  S. Thompson1Mike  Dixon2John  Lamping21: Human Communicat ion  Research  Cent reUn ivers i ty  of Ed inburgh2 Bucc leuch PlaceEdinburgh EH8 9LWSCOTLAND2: Xerox Palo Alto Research Center3333 Coyote Hill RoadPalo Alto, CA  94304ABSTRACTTwo new parsing algorithms forcontext-free phrase structure gram-mars are presented which perform abounded amount of processing perword per analysis path, independentlyof sentence length.
They are thus ca-pable of parsing in real-time in a par-allel implementation which forks pro-cessors in response to non-determinis-tic choice points.0.
INTRODUCTIONThe work reported here grew out ofour attempt o improve on the o (n 2)performance of the SIMD paral lelparser described in (Thompson 1991).Rather than start with a commitmentto a specific SIMD architecture, as thatwork had, we agreed that the bestplace to start was with a more abstractarchitecture- independent considera-tion of the CF-PSG parsing problem--given arbitrary resources, what algo-r i thms could one envisage whichcould recognise and/or parse atomiccategory phrase-structure grammarsin o (n) ?
In the end, two quite differ-ent approaches emerged.
One took asits start ing point non-deterministicshift-reduce parsing, and sought toachieve linear (indeed real-time) com-plexity by performing a constant-timestep per word of the input.
The othertook as its starting point tabular pars-ing (Earley, C KY), and sought toachieve linear complexity by perform-ing a constant-time step for the identi-fication/construction f constituents ofeach length from 0 to n. The latterroute has been widely canvassed,although to our knowledge has not yetbeen implemented--see (Nijholt 1989,90) for extensive references.
Theformer route, whereby real-time pars-ing is achieved by processor forking atnon-deterministic choice points in anextended shill-reduce parser, is to ourknowledge new.
In this paper we pre-sent outlines of two such parsers,which we call compose-reduceparsers.L COMPOSE-Rk~nUCE PARSINGWhy couldn't a simple breadth-first chart parser achieve linear per-formance on an appropriate parallelsystem?
If you provided enough pro-cessors to immediately process allagenda entries as they were created,would not this give the desired result?No, because the processing of a singleword might require many serialised87steps.
Consider processing the word"park" in the sentence "The peoplewho ran in the park got wet."
Given asimple tradit ional sort of grammar,that word completes an sP, which inturn completes a P P, which in turncompletes a vP, which in turn com-pletes an s, which in tu rn  completes aREL, which in turn completes an NP.The construction/recognition of theseconstituents i  necessari ly serialised,so regardless of the number of proces-sors available a constant-time step isimpossible.
(Note that this only pre-cludes a real-time parse by this route,but not necessarily a l inear one.)
Inthe shift-reduce approach to parsing,all this means is that for non-lineargrammars, a single shift step may befollowed by many reduce steps.
Thisin turn suggested the beginnings of away out, based on categorial gram-mar, namely that  mult iple reducescan be avoided i f  composition is al-lowed.
To return to our exampleabove, in a simple shift-reduce parserwe would have had all the words pre-ceding the word "park" in the stack.When it was shifted in, there wouldfollow six reduce steps.
If alterna-tively following a shift step one was al-lowed (non-deterministically) a com-pose step, this could be reduced (!)
to asingle reduce step.
Restricting our-selves to a simpler example, considerjust "run in the park" as a vv, givenrulesVP --) v PPNP  --) d nPP  --) p NP.With a composition step allowed,the parse would then proceed as fol-lows:Shift run as a vShift in as a pCompose v and p to give\[vP v \[PP p ?
NP\]\]where I use a combination of brack-eted strings and the 'dotted rule' nota-tion to indicate the result of composi-tion.
The categorial equivalent wouldhave been to notate v as vP/P P, P asPP/NP, and the result of the composi-tion as therefore vP/NP.Shift the as dCompose the dotted vp with dto give\[VP v \[PP p \[NP d ?
n \ ] \ ] \ ]Shift park as nReduce the dotted vp with n togive the complete result.Although a number of details re-mained to be worked out, this simplemove of allowing composition was theenabling step to achieving o(n) pars-ing.
Paral lel ism would arise by fork-ing processors at each non-determin-istic choice point, following the gen-eral model of Dixon's earlier work onparal le l is ing the ATMS (Dixon & deKleer 1988).Simply allowing composition is notin itself sufficient to achieve o (n) per-formance.
Some means of guarantee-ing that each step is constant timemust still be provided.
Here we foundtwo different ways forward.II.
TEn~.
FIRST COMPOSE-REDUCEPARSER---CR4In this parser there is no stack.We have simply a current structure,which corresponds to the top node ofthe stack in a normal shift-reduceparser.
This is achieved by extendingthe appeal to composition to include aform of left-embedded raising, whichwil l  be discussed fur ther  below.Special attention is also required tohandle left-recursive rules.88II.1 The Basic Parsing AlgorithmThe constant-time parsing step isgiven below (sl ightly simplified, inthat empty productions and some unitproductions are not handled).
In thisalgorithm schema, and in subsequentdiscussion, the annotation "ND" will beused in situations where a number ofalternatives are (or may be) described.The meaning is that these alternativesare to be pursued non-deterministi-cally.Algorithm CR-I1 Shift the next word;2 ND look it up in the lexicon;3 ND close the resulting cate-gory wrt the unit produc-t ions;4a ND reduce the resultingcategory with the currents t ructureor4b N D raise* the resulting cat-egory wrt the non-unaryrules in the grammar forwhich it is a left corner, andcompose the result with thecurrent structure.If reduction ever completes acategory which is marked asthe left corner of one ormore left-recursive rules orrule sequences, ND raise* inplace wrt those rules(sequences), and propagatethe marking.Some of these ND steps may at var-ious points produce complete struc-tures.
If .the input is exhausted, thenthose structures are parses, or not,depending on whether or not they havereached the distinguished symbol.
Ifthe input is not exhausted, it is ofcourse the incomplete structures, theresults of composition or raising,which are carried forward to the nextstep.The operation referred to above as"raise*" is more than simple raising,as was involved in the simple examplein section IV.
In order to allow for allpossible compositions to take place allpossible left-embedded raising must bepursued.
Consider the followinggrammar fragment:S ~NP VPVP -~ v NP CMPCMP --)that SNP -~ propnNP -+ dnand the utterance "Kim told Robin thatthe child likes Kim".If we ignore all the ND incorrectpaths, the current structure after the"that" has been processed is\[S \[NP \[propn Kim\]\]\[VP \[v told\]\[NP \[propn Robin\] \]\[CMP that  ?
S\] \] \]In order for the next word, "the", tobe correctly processed, it must  beraised all the way to s, namely wemust have\[S \[NP \[d the\] ?
n\] VP\]\]to compose with the current structure.What this means is that for every en-try in the normal bottom-up reachabil-ity table pairing a left corner with a topcategory, we need a set of dotted struc-tures, corresponding to all the waysthe grammar can get from that leftcorner to that top category.
It is thesestructures which are ND made avail-able in step 4b of the parsing step algo-r ithm CR-I above.89II.2 Handling Left RecursionNow this in itself is not sufficient ohandle left recursive structures, sinceby definition there could be an arbi-trary number of left-embeddings of aleft-recursive structure.
The finalnote in the description of algorithmCR-I above is designed to handle this.Glossing over some subtleties, left-re-cursion is handled by marking someof the structures introduced in step 3b,and ND raising in place if the markedstructure is ever completed by reduc-tion in the course of a parse.
Considerthe sentence ~Robin likes the chi ld'sdog."
We add the following two rulesto the grammar:D -9 a r tD -9 NP  'sthereby transforming D from a pre-terminal to a non-terminal.
When weshift "the", we will raise to inter alia\[NP \[D \ [art  the\ ] \ ]  ?
n\] rwith the NP marked for potential re-raising.
This structure will be com-posed with the then current structureto produceIS \[NP \[propn Robin\]\]\[VP Iv l ikes \ ]\[NP (as above) \]r\] \]After reduction with ~child", wewill have\[S \[NP \[propn Robin\]\]\[VP \[v l ikes \ ]\[NP \[D \ [art  the\ ] \ ]\[n chi ld\ ]  jr\] \]The last reduction will have com-pleted the marked N P introducedabove, so we ND left-recursively raisein place, giving\[S \[NP \[propn Robin\]\]\[VP Iv l ikes \ ]\[NP \[D \[NP the chi ld\]? '
S \ ]n\ ] r \ ] \ ]which will then take us through therest of the sentence.One final detail needs to be clearedup.
Although directly left-recursiverules, such as e.g.
NP -9 NP PP, arecorrectly dealt with by the abovemechanism, indirectly left-recursivesets of rules, such as the one exempli-fied above, require one additional sub-tlety.
Care must be taken not to intro-duce the potential for spurious ambi-guity.
We will introduce the full de-tails in the next section.II.3 Nature of the required tablesSteps 3 and 4b of CR-I require tablesof partial structures: Closures of unitproductions up from pre-terminals,for step 3; left-reachable raisings upfrom (unit production closures of) pre-terminals, for step 4b.
In this sectionwe discuss the creation of the neces-sary tables, in particular Ra ise* ,against the background of a simpleexemplary grammar, given below asTable 1.We have grouped the rules accord-ing to type--two kinds of unit produc-tions (from pre-terminals or non-ter-minals), two kinds of left recursiverules (direct and indirect) and the re-mainder.van i l l aS --) NP  VPVP -9 v NPCMP --) cmp SPP -9 prep  NPTable 1.un i t l  un i t2  i rd  i r iNP  -9 propn  NP  -9 CMP NP  -9 NP  PP  NP  -9 D nD -9 a r t  VP  -9 VP  PP D --) NP  'sExemplary grammar in groups by rule type90Cl*LRdirLRindir  2RS*I:2:\[NP pr?pn\]l'2 \[D art\]4\[NP NP PP\] 3: \[VP VP PP\]\[NP \[D NP 's\] n\]\[CMP cmp S\],\[pp prep NP\]\[VP v NP\] 3\[NP D n\]l, 2,\[D NpI 's\]4,\[NP CMP\] 1,24: \[D \[NP D n\] 1 's\]\[NP \[CMP cmp s\]\]l, 2,\[D \[NP \[CMP cmp S\]\] 1,2 's\],\[S \[NP \[CMP cmp S\]\]I, 2 VP\]\[S \[NP D n\]l, 2 VP\]\[S NpI'2 VP\]Table 2.
Partial structures for CR-IRas* \[NP -\[NP propn\] ?
pp\]l,2, \[NP \[D -\[NP propn\] ?
's\] n\] 1,2\[D \[NP i ~  ?
n\] 1 's\] 4\[CMP cmp ?
S\], \[NP \[CMP cmp ?
S\]\]I, 2,\[D \[NP \[CMP cmp ?
S\]\]I, 2 's\],\[S \[NP \[CMP cmp ?
S\]\]I, 2 VP\]\[pp prep ?
NP\]\[VP v ?
NP\] 3\[NP \ [ D ~  " r i l l '2  ?
\[S \[NF J-D art\] " n\]l'2 VP\]\[D \[Np pr?pn\]l " 's\]4, \[S \[NP P r?pn\]l'2 " VP\]Table 3.
Projecting non-terminal left daughtersAs a first step towards computingthe table which step 4b above woulduse, we can pre-compute the partialstructures given above in Table 2.c l*  contains all backbone frag-ments constructable from the unitproductions, and is already essentiallywhat we require for step 3 of the algo-rithm.
LRdir contains all directly left-recursive structures.
LR ind i r2  con-tains all indirectly left-recursive struc-tures involving exactly two rules, andthere might be LRind i r3 ,  4,... aswell.
R s* contains all non-recursivetree fragments constructable from left-embedding of binary or greater ulesand non-terminal unit productions.The superscripts denote loci whereleft-recursion may be appropriate, andidentify the relevant structures.In order to get the full Raise* tableneeded for step 4b, first we need to pro-ject the non-terminal left daughters ofrules such as \[ s NpI' 2 VP \] down toterminal left daughters.
We achievethis by substituting terminal entriesfrom Cl* wherever we can in LRdir,LRindir2 and Rs* to give us Table 3from Table 2 (new embeddings areunderlined).Left recursion has one remainingproblem for us.
Algorithm CR-I onlychecks for annotations and ND raisesin place after a reduction completes aconstituent.
But in the last line ofRas* above there are unit constituents91\[NP \[NP propn\] ?\[D \[NP \[D art\] ?\[CMP cmp ?
S\],pp\]l,2, \[NP \[D \[NP propn\] ?
's\]n\] 1 ,s\] 4\[NP \[CMP cmp ?
S\]\]1,2,\[D \[NP \[CMP cmp ?
S\]\]I, 2 's\],\[S \[NP \[CMP cmp ?
S\]\]I, 2 VP\]\[pp prep ?
NP\]\[VP v ?
NP\] 3\[NP \[D art\] ?
n\]l, 2, \[S \[NP \[D art\] ?
n\]l, 2 VP\]\[D \[NP propn\] ?
's\]4, \[D \[NP \[NP propn\] ?
pp\]l ,s\]4\[S \[NP propn\] ?
VP\], \[S \[NP \[NP propn\] ?
pp\]l,2 VP\],\[S \[NP \[D \[NP propn\] ?
's\] n\] 1,2 VP\]Table 4.
Final form of the structure table Ra i S e *n\]l, 2with annotations.
Being already com-plete, they will not ever be completed,and consequently the annotations willnever be checked.
So we pre-computethe desired result, augmenting theabove l ist with expansions of thoseunits via the indicated left recursions.This gives us the f inal version ofRa ise  * ,  now shown with dots in-cluded, in Table 4.This table is now suited to its rolein the algorithm.
Every entry has alexical left daughter, all annotatedconstituents are incomplete, and allunit productions are factored in.
It isinteresting to note that with these treefragments, taken together with theterminal entries in Cl*, as the initialtrees and LRd i r ,  LR ind i r2  , etc.
as theaux i l ia ry  trees we have a TreeAdjoining Grammar  ( Joshi 1985)which is strongly equivalent to the CF-PSG we started with.
We might call itthe left-lexical TAG for that CF-PSG,after Schabes et al (1988).
Note fur-ther that if a TAG parser respected theannotations as restricting adjunction,no spur ious ly  ambiguous parseswould be produced.Indeed it was via this relationshipwith TAGs that  the detai ls wereworked out of how the annotations aredistributed, not presented here to con-serve space.II.4 Implementation a d EfficiencyOnly a serial pseudo-parallel im-p lementat ion  has  been wr i t ten.Because of the high degree of pre-computation of structure, this versioneven though serialised runs quite effi-ciently.
There is very little computa-tion at each step, as it is straight-for-ward to double index the mai s e* tableso that only structures which willcompose with the current structureare retrieved.The price one pays for this effi-ciency, whether in serial or parallelversions, is that  only left-commonstructure is shared.
Right-commonstructure, as for instance in P P at-tachment ambiguity, is not shared be-tween analysis paths.
This causes nodifficulties for the parallel approach inone sense, in that it does not compro-mise the real-time performance of theparser.
Indeed, it is precisely becauseno recombination is attempted that thebasic parsing step is constant time.But it does mean that if  the CF-PSG be-ing parsed is the first half  of a two stepprocess, in which addit ional  con-92straints are solved in the second pass,then the duplication of structure willgive rise to duplication of effort.
Anyparal lel  parser  which adopts thestrategy of forking at non-determinis-tic choice points will suffer from thisweakness, including CR-II below.III.
THE SECOND COMPOSE-R~nUCEPARSER CR-IIOur second approach to compose-reduce parsing differs from the first inretaining a stack, having a more com-plex basic parsing step, while requir-ing far less pre-processing of thegrammar.
In particular, no specialtreatment is required for left-recursiverules.
Nevertheless, the basic step isstill constant time, and despite thestack there is no potential processing'balloon' at the end of the input.III.
1 The Basic Parsing AlgorithmAlgorithm CR-II1 Shift the next word;2 ND look it up in the lexicon;3 ND close the resulting cate-gory wrt the unit produc-t ions;4 N D reduce the resulting cat-egory with the top of thestack--if  results are com-plete and there is input re-maining, pop the stack;5a N D raise the results of (2),(3) and, where complete, (4)and5b N D either push the resultonto the stackor5c N D compose the result withthe top of the stack, replac-ing it.This is not an easy algorithm tounderstand.
In the next section wepresent a number of different ways ofmotivating it, together with an illus-trative example.III.2 CR-II ExplainedLet us first consider how CR-II willoperate on purely left-branching andpurely right-branching structures.
Ineach case we will consider the se-quence of algorithm steps along thenon-determinist ical ly  correct path,ignoring the others.
We will also re-strict ourselves to considering binarybranching rules, as pre-terminal unitproductions are handled entirely bystep 3 of the algorithm, and non-ter-minal unit productions must be fac-tored into the grammar.
On the otherhand, interior daughters of non-bi-nary nodes are all handled by step 4without changing the depth of thestack.III.2.1 Left-branching analysisFor a purely left-branching struc-ture, the first word will be processedby steps 1, 2, 5a and 5b, producing astack with one entry which we canschematise as in Figure 1, wherefilled circles are processed nodes andunfilled ones are waiting.Figure 1.All subsequent words except thelast will be processed by steps 4, 5a and5b (here and subsequently we will notmention steps 1 and 2, which occur forall words), effectively replacing theprevious sole entry in the stack withthe one given in Figure 2.93Figure 2.It should be evident hat the cycle ofsteps 4, 5a and 5b constructs a left-branching structure of increasingdepth as the sole stack entry, with oneright daughter, of the top node, wait-ing to be filled.
The last input word ofcourse is simply processed by step 4and, as there is no further input, lefton the stack as the final result.
Thecomplete sequence of steps for any left-branching analysis is thus ra ise J re-duce&raise*--reduce.
An ordinaryshift-reduce or left-corner parserwould go through the same sequenceof steps.III.2.2 Right-branching analysisThe first word of a purely right-branching structure is analysed ex-actly as for a left-branching one, thatis, with 5a and 5b, with results as inFigure 1 (repeated here as Figure 3):z%Figure 3.Subsequent words, except he last,are processed via steps 5a and 5c, withthe result remaining as the sole stackentry, as in Figure 4.Figure 4.Again it should be evident hat cy-cling steps 5a and 5c will construct aright-branching structure of increas-ing depth as the sole stack entry, withone right daughter, of the most em-bedded node, waiting to be filled.Again, the last input word will be pro-cessed by step 4.
The complete se-quence of steps for any right-branch-ing ana lys is  is thus ra i semraise&compose*--reduce.
A catego-rial grammar parser with a compose-first strategy would go through anisomorphic sequence of steps.III.2.3 Mixed Left- and Right-branch-ing AnalysisAll the steps in algorithm CR-IIhave now been illustrated, but we haveyet to see the stack grow beyond oneentry.
This will occur in where an in-dividual word, as opposed to a com-pleted complex constituent, is pro-cessed by steps 5a and 5b, that is,where steps 5a and 5b apply other thanto the results of step 4.Consider for instance the sentence"the child believes that the dog likesbiscuits.
~ With a grammar which Itrust will be obvious, we would arriveat the structure shown in Figure 5after processing "the child believesthat  ~, having done raise--reduce&raiseJra ise&compose--raise&compose, that is, a bit of left-branching analysis, followed by a bit ofright-branching analysis.94S SVPVPS'thai Flr~hle~ir~ili~\[::~: be dorieS the child believes t~ v~pwith "the" which will allow immediateintegration with this.
The ND correctpath applies steps 5a and 5b,raise&push, giving a stack as shownin Figure 6:SNPthe NVPSthe ch i ld  be l ieves  thatFigure 6.We can then apply steps 4, 5a and5c, reduce&raise&compose, to "dog",with the result shown in Figure 7.This puts uss back on the standardright-branching path for the rest of thesentence.the  dogFigure 7.III.3 An Alternative View of CR-IIReturning to a question raised ear-lier, we can now see how a chartparser could be modified in order torun in real-time given enough proces-sors to empty the agenda s fast as it isfilled.
We can reproduce the process-ing of CR-II within the active chartparsing framework by two modifica-tions to the fundamental rule (see e.g.Gazdar and Mellish 1989 or Thompsonand Ritchie 1984 for a tutorial intro-duction to active chart parsing).
Firstwe restrict its normal operation, inwhich an active and an inactive edgeare combined, to apply only in the caseof pre-terminal inactive edges.
Thiscorresponds to the fact that in CR-IIstep 4, the reduction step, applies onlyto pre-terminal categories (continuingto ignore unit productions).
Secondlywe allow the fundamental rule tocombine two active edges, provided thecategory to be produced by one is whatis required by the other.
This effectscomposition.
If we now run our chartparser left-to-right, left-corner andbreadth-first, it will duplicate CR-II.95The maximum number of edges alonga given analysis path which can be in-troduced by the processing of a singleword is now at most four, correspond-ing to steps 2, 4, 5a and 5c of CR-IIDthepre-terminal itself, a constituent com-pleted by it, an active edge containingthat constituent as left daughter, cre-ated by left-corner rule invocation, anda further active edge combining thatone with one to its left.
This in turnmeans that there is a fixed limit to theamount of processing required foreach word.III.4 Implementation and EfficiencyAlthough clearly not benefitingfrom as much pre-computation ofstructure as CR-I, CR-II is also quite ef-ficient.
Two modifications can beadded to improve efficiencyDa reach-ability filter on step 5b, and a shapertest (Kuno 1965), also on 5b.
For thelatter, we need simply keep a count ofthe number of open nodes on the stack(equal to the number of stack entries ifall rules are binary), and ensure thatthis number never exceeds the num-ber of words remaining in the input,as each entry will require a number ofwords equal to the number of its opennodes to pop it off the stack.
This testactually cuts down the number of non-deterministic paths quite dramati-cally, as the ND optionality of step 5bmeans that quite deep stacks wouldotherwise be pursued along somesearch paths.
Again this reduction insearch space is of limited significancein a true parallel implementation, butin the serial simulation it makes a bigdifference.Note also that no attention has beenpaid to unit productions, which wepre-compute as in CR-I.
Furthermore,neither CR-I nor CR-II address emptyproductions, whose effect would alsoneed to be pre-computed.IV.
CONCLUSIONSAside from the intrinsic interest inthe abstract of real-time parsablility, isthere any practical significance tothese results.
Two drawbacks, one al-ready referred to, certainly restricttheir significance.
One is that the re-striction to atomic category CF-PSGs iscrucial the fact that the comparisonbetween a rule element and a node la-bel is atomic and constant ime is fun-damental.
Any move to features orother annotations would put an end toreal-time processing.
This fact givesadded weight to the problem men-tioned above in section II,4, that onlyleft-common analysis results areshared between alternatives.
Thus ifone finesses the atomic category prob-lem by using a parser such as thosedescribed here only as the first pass ofa two pass system, one is only puttingoff the payment of the complexity priceto the second pass, in the absence todate of any linear time solution to theconstraint satisfaction problem.
Onthis basis, one would clearly prefer aparallel CKY/Earley algorithm, whichdoes share all common substructure,to the parsers presented here.Nevertheless, there is one class ofapplications where the left-to-rightreal-time behaviour of these algo-rithms may be of practical benefit,namely  in speech recognit ion.Present day systems require on-lineavailability of syntactic and domain-semantic constraint to l imit thesearch space at lower levels of the sys-tem.
Hitherto this has meant theseconstraints must be brought to bearduring recognition as some form ofregular grammar, either explicitly96constructed as such or compiled into.The parsers presented here offer thealternative of parallel application ofgenuinely context-free grammars di-rectly, with the potential added benefitthat, with sufficient processor width,quite high degrees of local ambiguitycan be tolerated, such as would arise if(a finite subset of) a feature-basedgrammar were expanded out intoatomic category form.ACKNOWLEDGEMENTSThe work reported here was car-ried out while the first author was avisitor to the Embedded Computationand Natural Language Theory andTechnology groups of the SystemsScience Laboratory at the Xerox PaloAlto Research Center.
These groupsprovided both the intellectual and ma-terial resources required to supportour work, for which our thanks.REFERENCESDixon, Mike and de Kleer, Johan.1988.
"Massively Paral le lAssumpt ion-based  TruthMaintenance".
In Proceedings ofthe AAAI-88 National Conferenceon Artificial Intelligence, alsoreprinted in Proceedings of theSecond International Workshop onNon-Monotonic Reasoning.Gazdar, Gerald and Mellish, Chris.1989.
Natural LanguageProcessing in LISP.
Addison-Wesley, Wokingham, England(sic).Joshi, Aravind K. 1985.
"How MuchContext-Sensitivity s Necessary forCharacterizing StructuralDescriptions--Tree AdjoiningGrammars".
In Dowty, D.,Karttunen, L., and Zwicky, A. eds,Natural Language Processing--Theoretical Computational ndPsychological Perspectives.Cambridge University Press, NewYork.Kuno, Susumo.
1965.
"The predictiveanalyzer and a path eliminationtechnique", Communications of theACM, 8, 687-698.Nijholt, Anton.
1989.
"Parallelparsing strategies in naturallanguage processing ~.
In Tomita,M.
ed, Proceedings of theInternational Workshop onParsing Technologies, 240-253,Carnegie-Mellon University,Pittsburgh.Nijholt, Anton.
1990.
The CYK-Approach to Serial and ParallelParsing.
Memoranda Informatica90-13, faculteit der informatica,Universiteit Twente, Netherlands.Shabes, Yves, Abeill6, Anne andJoshi, Aravind K. 1988.
"ParsingStrategies with 'Lexicalized'Grammars: Application to TreeAdjoining Grammars".
InProceedings of the 12thInternational Conference onComputational Linguistics, 82-93.Thompson, Henry S. 1991.
"ParallelParsers for Context-FreeGrammars--Two ActualImplementations Compared".
Toappear in Adriaens, G. and Hahn,U.
eds, Parallel Models of NaturalLanguage Computation, Ablex,Norword NJ.Thompson, Henry S. and Ritchie,Graeme D. 1984.
"Techniques forParsing Natural Language: TwoExamples".
In Eisenstadt, M., andO'Shea, T., editors, ArtificialIntelligence: Tools, Techniques,and Applications.
Harper andRow, London.
Also DAI ResearchPaper 183, Dept.
of ArtificialIntelligence, Univ.
of Edinburgh.97
