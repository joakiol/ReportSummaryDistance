Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1352?1362,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsTuning as RankingMark Hopkins and Jonathan MaySDL Language WeaverLos Angeles, CA 90045{mhopkins,jmay}@sdl.comAbstractWe offer a simple, effective, and scalablemethod for statistical machine translation pa-rameter tuning based on the pairwise approachto ranking (Herbrich et al, 1999).
Unlikethe popular MERT algorithm (Och, 2003), ourpairwise ranking optimization (PRO) methodis not limited to a handful of parameters andcan easily handle systems with thousands offeatures.
Moreover, unlike recent approachesbuilt upon the MIRA algorithm of Crammerand Singer (2003) (Watanabe et al, 2007; Chi-ang et al, 2008b), PRO is easy to imple-ment.
It uses off-the-shelf linear binary classi-fier software and can be built on top of an ex-isting MERT framework in a matter of hours.We establish PRO?s scalability and effective-ness by comparing it to MERT and MIRA anddemonstrate parity on both phrase-based andsyntax-based systems in a variety of languagepairs, using large scale data scenarios.1 IntroductionThe MERT algorithm (Och, 2003) is currently themost popular way to tune the parameters of a sta-tistical machine translation (MT) system.
MERTis well-understood, easy to implement, and runsquickly, but can behave erratically and does not scalebeyond a handful of features.
This lack of scalabilityis a significant weakness, as it inhibits systems fromusing more than a couple dozen features to discrimi-nate between candidate translations and stymies fea-ture development innovation.Several researchers have attempted to addressthis weakness.
Recently, Watanabe et al (2007)and Chiang et al (2008b) have developed tuningmethods using the MIRA algorithm (Crammer andSinger, 2003) as a nucleus.
The MIRA technique ofChiang et al has been shown to perform well onlarge-scale tasks with hundreds or thousands of fea-tures (2009).
However, the technique is complex andarchitecturally quite different from MERT.
Tellingly,in the entire proceedings of ACL 2010 (Hajic?
et al,2010), only one paper describing a statistical MTsystem cited the use of MIRA for tuning (Chiang,2010), while 15 used MERT.1Here we propose a simpler approach to tuning thatscales similarly to high-dimensional feature spaces.We cast tuning as a ranking problem (Chen et al,2009), where the explicit goal is to learn to correctlyrank candidate translations.
Specifically, we followthe pairwise approach to ranking (Herbrich et al,1999; Freund et al, 2003; Burges et al, 2005; Cao etal., 2007), in which the ranking problem is reducedto the binary classification task of deciding betweencandidate translation pairs.Of primary concern to us is the ease of adoption ofour proposed technique.
Because of this, we adhereas closely as possible to the established MERT ar-chitecture and use freely available machine learningsoftware.
The end result is a technique that scalesand performs just as well as MIRA-based tuning,but which can be implemented in a couple of hoursby anyone with an existing MERT implementation.Mindful that many would-be enhancements to the1The remainder either did not specify their tuning method(though a number of these used the Moses toolkit (Koehn et al,2007), which uses MERT for tuning) or, in one case, set weightsby hand.1352state-of-the-art are false positives that only show im-provement in a narrowly defined setting or with lim-ited data, we validate our claims on both syntax andphrase-based systems, using multiple language pairsand large data sets.We describe tuning in abstract and somewhat for-mal terms in Section 2, describe the MERT algo-rithm in the context of those terms and illustrate itsscalability issues via a synthetic experiment in Sec-tion 3, introduce our pairwise ranking optimizationmethod in Section 4, present numerous large-scaleMT experiments to validate our claims in Section 5,discuss some related work in Section 6, and con-clude in Section 7.2 TuningIn Figure 1, we show an example candidate space,defined as a tuple ?
?, I, J, f, e,x?
where:?
?
is a positive integer referred to as the dimen-sionality of the space?
I is a (possibly infinite) set of positive integers,referred to as sentence indices?
J maps each sentence index to a (possibly infi-nite) set of positive integers, referred to as can-didate indices?
f maps each sentence index to a sentence fromthe source language?
e maps each pair ?i, j?
?
I ?
J(i) to the jthtarget-language candidate translation of sourcesentence f(i)?
x maps each pair ?i, j?
?
I ?
J(i) to a?-dimension feature vector representation ofe(i, j)The example candidate space has two source sen-tences, three candidate translations for each sourcesentence, and feature vectors of dimension 2.
It isan example of a finite candidate space, defined asa candidate space for which I is finite and J mapseach index of I to a finite set.A policy of candidate space ?
?, I, J, f, e,x?
is afunction that maps each member i ?
I to a memberof J(i).
A policy corresponds to a choice of onecandidate translation for each source sentence.
Forthe example in Figure 1, policy p1 = {1 7?
2, 2 7?3} corresponds to the choice of ?he does not go?
forthe first source sentence and ?I do not go?
for thesecond source sentence.
Obviously some policiesare better than others.
Policy p2 = {1 7?
3, 2 7?
1}corresponds to the inferior translations ?she not go?and ?I go not.
?We assume the MT system distinguishes betweenpolicies using a scoring function for candidate trans-lations of the form hw(i, j) = w ?
x(i, j), where wis a weight vector of the same dimension as featurevector x(i, j).
This scoring function extends to apolicy p by summing the cost of each of the policy?scandidate translations: Hw(p) = ?i?I hw(i, p(i)).As can be seen in Figure 1, using w = [?2, 1],Hw(p1) = 9 and Hw(p2) = ?8.The goal of tuning is to learn a weight vector wsuch that Hw(p) assigns a high score to good poli-cies, and a low score to bad policies.2 To do so,we need information about which policies are goodand which are bad.
This information is provided bya ?gold?
scoring function G that maps each policyto a real-valued score.
Typically this gold functionis BLEU (Papineni et al, 2002), though there areseveral common alternatives (Lavie and Denkowski,2009; Melamed et al, 2003; Snover et al, 2006;Chiang et al, 2008a).We want to find a weight vector w such that Hwbehaves ?similarly?
to G on a candidate space s.We assume a loss function ls(Hw, G) which returnsthe real-valued loss of using scoring function Hwwhen the gold scoring function is G and the candi-date space is s. Thus, we may say the goal of tuningis to find the weight vector w that minimizes loss.3 MERTIn general, the candidate space may have infinitelymany source sentences, as well as infinitely manycandidate translations per source sentence.
In prac-tice, tuning optimizes over a finite subset of sourcesentences3 and a finite subset of candidate transla-tions as well.
The classic tuning architecture usedin the dominant MERT approach (Och, 2003) formsthe translation subset and learns weight vector w via2Without loss of generality, we assume that a higher scoreindicates a better translation.3See Section 5.2 for the tune sets used in this paper?s exper-iments.1353Source Sentence Candidate Translationsi f(i) j e(i, j) x(i, j) hw(i, j) g(i, j)1 ?il ne va pas?
1 ?he goes not?
[2 4] 0 0.282 ?he does not go?
[3 8] 2 0.423 ?she not go?
[6 1] -11 0.122 ?je ne vais pas?
1 ?I go not?
[-3 -3] 3 0.152 ?we do not go?
[1 -5] -7 0.183 ?I do not go?
[-5 -3] 7 0.34Figure 1: Example candidate space of dimensionality 2.
Note: I = {1, 2}, J(1) = J(2) = {1, 2, 3}.
We also show alocal scoring function hw(i, j) (where w = [?2, 1]) and a local gold scoring function g(i, j).Algorithm TUNE(s, G):1: initialize pool: let s?
= ?
?, I ?, J ?, f, e,x?,where I ?
?
I and J ?
= ?2: for the desired number of iterations do3: candidate generation: choose index pairs(i, j); for each, add j to J ?
(i)4: optimization: find vector w that minimizesls?
(Hw, G)5: return wFigure 2: Schema for iterative tuning of base candidatespace s = ?
?, I, J, f, e,x?
w.r.t.
gold function G.a feedback loop consisting of two phases.
Figure 2shows the pseudocode.
During candidate genera-tion, candidate translations are selected from a basecandidate space s and added to a finite candidatespace s?
called the candidate pool.
During optimiza-tion, the weight vector w is optimized to minimizeloss ls?
(Hw, G).For its candidate generation phase, MERT gener-ates the k-best candidate translations for each sourcesentence according to hw, where w is the weightvector from the previous optimization phase (or anarbitrary weight vector for the first iteration).For its optimization phase, MERT defines the lossfunction as follows:ls(Hw, G) = maxp G(p)?G(arg maxpHw(p))In other words, it prefers weight vectors w suchthat the gold function G scores Hw?s best policy ashighly as possible (if Hw?s best policy is the sameas G?s best policy, then there is zero loss).
Typicallythe optimization phase is implemented using Och?sline optimization algorithm (2003).MERT has proven itself effective at tuning candi-date spaces with low dimensionality.
However, it isoften claimed that MERT does not scale well withdimensionality.
To test this claim, we devised thefollowing synthetic data experiment:1.
We created a gold scoring function G that isalso a linear function of the same form as Hw,i.e.,G(p) = Hw?
(p) for some gold weight vec-tor w?.
Under this assumption, the role of theoptimization phase reduces to learning back thegold weight vector w?.2.
We generated a ?-dimensionality candidatepool with 500 source ?sentences?
and 100 can-didate ?translations?
per sentence.
We createdthe corresponding feature vectors by drawing?
random real numbers uniformly from the in-terval [0, 500].3.
We ran MERT?s line optimization on this syn-thetic candidate pool and compared the learnedweight vector w to the gold weight vector w?using cosine similarity.We used line optimization in the standard way,by generating 20 random starting weight vectors andhill-climbing on each independently until no furtherprogress is made, then choosing the final weight vec-tor that minimizes loss.
We tried various dimen-sionalities from 10 to 1000.
We repeated each set-ting three times, generating different random dataeach time.
The results in Figure 3 indicate that asthe dimensionality of the problem increases MERTrapidly loses the ability to learn w?.
Note that thissynthetic problem is considerably easier than a realMT scenario, where the data is noisy and interdepen-dent, and the gold scoring function is nonlinear.
If1354MERT cannot scale in this simple scenario, it has lit-tle hope of succeeding in a high-dimensionality de-ployment scenario.4 Optimization via Pairwise RankingWe would like to modify MERT so that it scales wellto high-dimensionality candidate spaces.
The mostprominent example of a tuning method that per-forms well on high-dimensionality candidate spacesis the MIRA-based approach used by Watanabe etal.
(2007) and Chiang et al (2008b; 2009).
Unfortu-nately, this approach requires a complex architecturethat diverges significantly from the MERT approach,and consequently has not been widely adopted.
Ourgoal is to achieve the same performance with mini-mal modification to MERT.With MERT as a starting point, we have a choice:modify candidate generation, optimization, or both.Although alternative candidate generation methodshave been proposed (Macherey et al, 2008; Chianget al, 2008b; Chatterjee and Cancedda, 2010), wewill restrict ourselves to MERT-style candidate gen-eration, in order to minimize divergence from theestablished MERT tuning architecture.
Instead, wefocus on the optimization phase.4.1 Basic ApproachWhile intuitive, the MERT optimization module fo-cuses attention on Hw?s best policy, and not on itsoverall prowess at ranking policies.
We will cre-ate an optimization module that directly addressesHw?s ability to rank policies in the hope that thismore holistic approach will generalize better to un-seen data.Assume that the gold scoring function G decom-poses in the following way:G(p) =?i?Ig(i, p(i)) (1)where g(i, j) is a local scoring function that scoresthe single candidate translation e(i, j).
We show anexample g in Figure 1.
For an arbitrary pair of can-didate translations e(i, j) and e(i, j?
), the local goldfunction g tells us which is the better translation.Note that this induces a ranking on the candidatetranslations for each source sentence.We follow the pairwise approach to ranking (Her-brich et al, 1999; Freund et al, 2003; Burges et al,2005; Cao et al, 2007).
In the pairwise approach,the learning task is framed as the classification ofcandidate pairs into two categories: correctly or-dered and incorrectly ordered.
Specifically, for can-didate translation pair e(i, j) and e(i, j?
), we want:g(i, j) > g(i, j?)
?
hw(i, j) > hw(i, j?).
We canre-express this condition:g(i, j) > g(i, j?)?
hw(i, j) > hw(i, j?)?
hw(i, j)?
hw(i, j?)
> 0?
w ?
x(i, j)?w ?
x(i, j?)
> 0?
w ?
(x(i, j)?
x(i, j?))
> 0Thus optimization reduces to a classic binary clas-sification problem.
We create a labeled training in-stance for this problem by computing difference vec-tor x(i, j) ?
x(i, j?
), and labeling it as a positiveor negative instance based on whether, respectively,the first or second vector is superior according togold function g. To ensure balance, we considerboth possible difference vectors from a pair.
For ex-ample, given the candidate space of Figure 1, sinceg(1, 1) > g(1, 3), we would add ([?4, 3],+) and([4,?3],?)
to our training set.
We can then feed thistraining data directly to any off-the-shelf classifica-tion tool that returns a linear classifier, in order to ob-tain a weight vector w that optimizes the above con-dition.
This weight vector can then be used directlyby the MT system in the subsequent candidate gen-eration phase.
The exact loss function ls?
(Hw, G)optimized depends on the choice of classifier.4Typical approaches to pairwise ranking enumer-ate all difference vectors as training data.
For tuninghowever, this means O(|I| ?
J2max) vectors, whereJmax is the cardinality of the largest J(i).
SinceI and Jmax commonly range in the thousands, afull enumeration would produce billions of featurevectors.
Out of tractability considerations, we sam-ple from the space of difference vectors, using thesampler template in Figure 4.
For each source sen-tence i, the sampler generates ?
candidate transla-tion pairs ?j, j?
?, and accepts each pair with proba-bility ?i(|g(i, j) ?
g(i, j?)|).
Among the acceptedpairs, it keeps the ?
with greatest g differential, andadds their difference vectors to the training data.54See (Chen et al, 2009) for a brief survey.5The intuition for biasing toward high score differential is135500.20.40.60.8 1  02004006008001000Cosine similarityof learned parameter weightsDimensionalitySynthetic parameter learningof MERT and PRO PRONoisyPRO MERTNoisyMERTFigure 3: Result of synthetic data learning experimentfor MERT and PRO, with and without added noise.
Asthe dimensionality increases MERT is unable to learn theoriginal weights but PRO still performs adequately.4.2 ScalabilityWe repeated the scalability study from Section 3,now using our pairwise ranking optimization (here-after, PRO) approach.
Throughout all experimentswith PRO we choose ?
= 5000, ?
= 50, and thefollowing step function ?
for each ?i: 6?
(n) ={0 if n < 0.051 otherwiseWe used MegaM (Daume?
III, 2004) as a binaryclassifier in our contrasting synthetic experiment andran it ?out of the box,?
i.e., with all default settingsfor binary classification.7 Figure 3 shows that PROis able to learn w?
nearly perfectly at all dimension-alities from 10 to 1000.As noted previously, though, this is a rather sim-ple task.
To encourage a disconnect between g andhw and make the synthetic scenario look more likeMT reality, we repeated the synthetic experimentsthat our primary goal is to ensure good translations are preferredto bad translations, and not to tease apart small differences.6We obtained these parameters by trial-and-error experi-mentation on a single MT system (Urdu-English SBMT), thenheld them fixed throughout our experiments.
We obtained sim-ilar results using ?
= ?
= 100, and for each ?i, a logistic sig-moid function centered at the mean g differential of candidatetranslation pairs for the ith source sentence.
This alternative ap-proach has the advantage of being agnostic about which goldscoring function is used.7With the sampling settings previously described andMegaM as our classifier we were able to optimize two to threetimes faster than with MERT?s line optimization.Algorithm SAMPLERs,g( ?, ?, i, ?i ):1: V = ?
?2: for ?
samplings do3: Choose ?j, j??
?
J(i)?J(i) uniformly at ran-dom.4: With probability ?i(|g(i, j)-g(i, j?
)|), add(x(i, j),x(i, j?
), |g(i, j)-g(i, j?
)|) to V .5: Sort V decreasingly by |g(i, j)-g(i, j?
)|.6: return (x(i, j) ?
x(i, j?
), sign(g(i, j)-g(i, j?
))and (x(i, j?
)-x(i, j), sign(g(i, j?
)-g(i, j))) foreach of the first ?
members of V .Figure 4: Pseudocode for our sampler.
Arguments: s =?
?, I, J, f, e,x?
is a finite candidate space; g is a scoringfunction; ?, ?, i are nonnegative integers; ?i is a func-tion from the nonnegative real numbers to the real interval[0, 1].but added noise to each feature vector, drawn froma zero-mean Gaussian with a standard deviation of500.
The results of the noisy synthetic experiments,also in Figure 3 (the lines labeled ?Noisy?
), showthat the pairwise ranking approach is less successfulthan before at learning w?
at high dimensionality,but still greatly outperforms MERT.4.3 DiscussionThe idea of learning from difference vectors also liesat the heart of the MIRA-based approaches (Watan-abe et al, 2007; Chiang et al, 2008b) and the ap-proach of Roth et al (2010), which, similar to ourmethod, uses sampling to select vectors.
Here, weisolate these aspects of those approaches to createa simpler tuning technique that closely mirrors theubiquitous MERT architecture.
Among other sim-plifications, we abstract away the choice of MIRAas the classification method (our approach can useany classification technique that learns a separatinghyperplane), and we eliminate the need for oracletranslations.An important observation is that BLEU does notsatisfy the decomposability assumption of Equa-tion (1).
An advantage of MERT is that it can di-rectly optimize for non-decomposable scoring func-tions like BLEU.
In our experiments, we usethe BLEU+1 approximation to BLEU (Liang et al,2006) to determine class labels.
We will neverthe-less use BLEU to evaluate the trained systems.1356PBMTLanguage Experiment BLEUfeats method tune testUrdu-EnglishbaseMERT 20.5 17.7MIRA 20.5 17.9PRO 20.4 18.2ext MIRA 21.8 17.8PRO 21.6 18.1Arabic-EnglishbaseMERT 46.8 41.2MIRA 47.0 41.1PRO 46.9 41.1ext MIRA 47.5 41.7PRO 48.5 41.9Chinese-EnglishbaseMERT 23.8 22.2MIRA 24.1 22.5PRO 23.8 22.5ext MIRA 24.8 22.6PRO 24.9 22.7SBMTLanguage Experiment BLEUfeats method tune testUrdu-EnglishbaseMERT 23.4 21.4MIRA 23.6 22.3PRO 23.4 22.2ext MIRA 25.2 22.8PRO 24.2 22.8Arabic-EnglishbaseMERT 44.7 39.0MIRA 44.6 39.0PRO 44.5 39.0ext MIRA 45.8 39.8PRO 45.9 40.3Chinese-EnglishbaseMERT 25.5 22.7MIRA 25.4 22.9PRO 25.5 22.9ext MIRA 26.0 23.3PRO 25.6 23.5Table 1: Machine translation performance for the experiments listed in this paper.
Scores are case-sensitive IBMBLEU.
For every choice of system, language pair, and feature set, PRO performs comparably with the other methods.5 ExperimentsWe now turn to real machine translation condi-tions to validate our thesis: We can cleanly replaceMERT?s line optimization with pairwise ranking op-timization and immediately realize the benefits ofhigh-dimension tuning.
We now detail the threelanguage pairs, two feature scenarios, and two MTmodels used for our experiments.
For each languagepair and each MT model we used MERT, MIRA, andPRO to tune with a standard set of baseline features,and used the latter two methods to tune with an ex-tended set of features.8 At the end of every experi-ment we used the final feature weights to decode aheld-out test set and evaluated it with case-sensitiveBLEU.
The results are in Table 1.5.1 SystemsWe used two systems, each based on a different MTmodel.
Our syntax-based system (hereafter, SBMT)follows the model of Galley et al (2004).
Our8MERT could not run to a satisfactory completion in anyextended feature scenario; as implied in the synthetic data ex-periment of Section 3, the algorithm makes poor choices forits weights and this leads to low-quality k-best lists and dismalperformance, near 0 BLEU in every iteration.phrase-based system (hereafter, PBMT) follows themodel of Och and Ney (2004).
In both systemswe learn alignments with GIZA++ (Och and Ney,2000) using IBM Model 4; for Urdu-English andChinese-English we merged alignments with the re-fined method, and for Arabic-English we mergedwith the union method.5.2 DataTable 2 notes the sizes of the datasets used in our ex-periments.
All tune and test data have four Englishreference sets for the purposes of scoring.Data U-E A-E C-ETrain lines 515K 6.5M 7.9Mwords 2.2M 175M 173MTune lines 923 1994 1615words 16K 65K 42KTest lines 938 1357 1357words 18K 47K 37KTable 2: Data sizes for the experiments reported in thispaper (English words shown).1357ClassUrdu-English Arabic-English Chinese-EnglishPBMT SBMT PBMT SBMT PBMT SBMTbase ext base ext base ext base ext base ext base extbaseline 15 15 19 19 15 15 19 19 15 15 19 19target word ?
51 ?
50 ?
51 ?
50 ?
51 ?
299discount ?
11 ?
11 ?
11 ?
10 ?
11 ?
10node count ?
?
?
99 ?
?
?
138 ?
?
?
96rule overlap ?
?
?
98 ?
?
?
136 ?
?
?
93word pair ?
2110 ?
?
?
6193 ?
?
?
1688 ?
?phrase length ?
63 ?
?
?
63 ?
?
?
63 ?
?total 15 2250 19 277 15 6333 18 352 15 1828 19 517Table 3: Summary of features used in experiments in this paper.5.2.1 Urdu-EnglishThe training data for Urdu-English is that madeavailable in the constrained track in the NIST 2009MT evaluation.
This includes many lexicon entriesand other single-word data, which accounts for thelarge number of lines relative to word count.
TheNIST 2008 evaluation set, which contains newswireand web data, is split into two parts; we used roughlyhalf each for tune and test.
We trained a 5-gramEnglish language model on the English side of thetraining data.5.2.2 Arabic-EnglishThe training data for Arabic English is that madeavailable in the constrained track in the NIST 2008MT evaluation.
The tune set, which contains onlynewswire data, is a mix from NIST MT evaluationsets from 2003?2006 and from GALE developmentdata.
The test set, which contains both web andnewswire data, is the evaluation set from the NIST2008 MT evaluation.
We trained a 4-gram Englishlanguage model on the English side of the trainingdata.5.2.3 Chinese-EnglishFor Chinese-English we used 173M words oftraining data from GALE 2008.
For SBMT we useda 32M word subset for extracting rules and buildinga language model, but used the entire training datafor alignments, and for all PBMT training.
The tuneand test sets both contain web and newswire data.The tune set is selected from NIST MT evaluationsets from 2003?2006.
The test set is the evaluationset from the NIST 2008 MT evaluation.
We trained a3-gram English language model on the English sideof the training data.5.3 FeaturesFor each of our systems we identify two feature sets:baseline, which correspond to the typical small fea-ture set reported in current MT literature, and ex-tended, a superset of baseline, which adds hundredsor thousands of features.
Specifically, we use 15baseline features for PBMT, similar to the baselinefeatures described by Watanabe et al (2007).
Weuse 19 baseline features for SBMT, similar to thebaseline features described by Chiang et al (2008b).We used the following feature classes in SBMTand PBMT extended scenarios:?
Discount features for rule frequency bins (cf.Chiang et al (2009), Section 4.1)?
Target word insertion features9We used the following feature classes in SBMT ex-tended scenarios only (cf.
Chiang et al (2009), Sec-tion 4.1):10?
Rule overlap features?
Node count features9For Chinese-English and Urdu-English SBMT these fea-tures only fired when the inserted target word was unaligned toany source word.10The parser used for Arabic-English had a different nonter-minal set than that used for the other two SBMT systems, ac-counting for the wide disparity in feature count for these featureclasses.135820212223242526  0510152025304-ref BLEUIterationUrdu-English SBMT baseline featuretuningTUNE TESTMERT MIRA PRO20212223242526  0510152025304-ref BLEUIterationUrdu-English SBMT extended featuretuningTUNE TESTMIRA PROFigure 5: Comparison of MERT, PRO, and MIRA on tuning Urdu-English SBMT systems, and test results at everyiteration.
PRO performs comparably to MERT and MIRA.We used the following feature classes in PBMTextended scenarios only:?
Unigram word pair features for the 80 most fre-quent words in both languages plus tokens forunaligned and all other words (cf.
Watanabe etal.
(2007), Section 3.2.1)11?
Source, target, and joint phrase length fea-tures from 1 to 7, e.g.
?tgt=4?, ?src=2?, and?src/tgt=2,4?The feature classes and number of features usedwithin those classes for each language pair are sum-marized in Table 3.5.4 Tuning settingsEach of the three approaches we compare in thisstudy has various details associated with it that mayprove useful to those wishing to reproduce our re-sults.
We list choices made for the various tuningmethods here, and note that all our decisions weremade in keeping with best practices for each algo-rithm.5.4.1 MERTWe used David Chiang?s CMERT implementationof MERT that is available with the Moses system(Koehn et al, 2007).
We ran MERT for up to 30 it-erations, using k = 1500, and stopping early when11This constitutes 6,723 features in principle (822 ?
1 since?unaligned-unaligned?
is not considered) but in practice farfewer co-occurrences were seen.
Table 3 shows the number ofactual unigram word pair features observed in data.the accumulated k-best list does not change in an it-eration.
In every tuning iteration we ran MERT oncewith weights initialized to the last iteration?s chosenweight set and 19 times with random weights, andchose the the best of the 20 ending points accordingto G on the development set.
The G we optimizeis tokenized, lower-cased 4-gram BLEU (Papineni etal., 2002).5.4.2 MIRAWe for the most part follow the MIRA algorithmfor machine translation as described by Chiang et al(2009)12 but instead of using the 10-best of each ofthe best hw, hw +g, and hw-g, we use the 30-bestaccording to hw.13 We use the same sentence-levelBLEU calculated in the context of previous 1-besttranslations as Chiang et al (2008b; 2009).
We ranMIRA for 30 iterations.5.4.3 PROWe used the MegaM classifier and sampled as de-scribed in Section 4.2.
As previously noted, we usedBLEU+1 (Liang et al, 2006) for g. MegaM was easyto set up and ran fairly quickly, however any linearbinary classifier that operates on real-valued featurescan be used, and in fact we obtained similar results12and acknowledge the use of David Chiang?s code13This is a more realistic scenario for would-be implementersof MIRA, as obtaining the so-called ?hope?
and ?fear?
transla-tions from the lattice or forest is significantly more complicatedthan simply obtaining a k-best list.
Other tests comparing thesemethods have shown between 0.1 to 0.3 BLEU drop using 30-best hw on Chinese-English (Wang, 2011).1359using the support vector machine module of WEKA(Hall et al, 2009) as well as the Stanford classifier(Manning and Klein, 2003).
We ran for up to 30 iter-ations and used the same k and stopping criterion aswas used for MERT, though variability of samplingprecluded list convergence.While MERT and MIRA use each iteration?s finalweights as a starting point for hill-climbing the nextiteration, the pairwise ranking approach has no ex-plicit tie to previous iterations.
To incorporate suchstability into our process we interpolated the weightsw?
learned by the classifier in iteration t with thosefrom iteration t ?
1 by a factor of ?, such thatwt = ?
?w?
+ (1??)
?wt?1.
We found ?
= 0.1gave good performance across the board.5.5 DiscussionWe implore the reader to avoid the natural tendencyto compare results using baseline vs. extended fea-tures or between PBMT and SBMT on the same lan-guage pair.
Such discussions are indeed interesting,and could lead to improvements in feature engineer-ing or sartorial choices due to the outcome of wagers(Goodale, 2008), but they distract from our thesis.As can be seen in Table 1, for each of the 12 choicesof system, language pair, and feature set, the PROmethod performed nearly the same as or better thanMIRA and MERT on test data.In Figure 5 we show the tune and test BLEU us-ing the weights learned at every iteration for eachUrdu-English SBMT experiment.
Typical of the restof the experiments, we can clearly see that PRO ap-pears to proceed more monotonically than the othermethods.
We quantified PRO?s stability as comparedto MERT by repeating the Urdu-English baselinePBMT experiment five times with each configura-tion.
The tune and test BLEU at each iteration isdepicted in Figure 6.
The standard deviation of thefinal test BLEU of MERT was 0.13 across the fiveexperiment instances, while PRO had a standard de-viation of just 0.05.6 Related WorkSeveral works (Shen et al, 2004; Cowan et al,2006; Watanabe et al, 2006) have used discrimina-tive techniques to re-rank k-best lists for MT.
Till-mann and Zhang (2005) used a customized form of1718192021  0510152025304-ref BLEUIterationUrdu-English PBMT tuning stabilityTUNE TESTMERT PROFigure 6: Tune and test curves of five repetitions of thesame Urdu-English PBMT baseline feature experiment.PRO is more stable than MERT.multi-class stochastic gradient descent to learn fea-ture weights for an MT model.
Och and Ney (2002)used maximum entropy to tune feature weights butdid not compare pairs of derivations.
Ittycheriah andRoukos (2005) used a maximum entropy classifier totrain an alignment model using hand-labeled data.Xiong et al (2006) also used a maximum entropyclassifier, in this case to train the reordering com-ponent of their MT model.
Lattice- and hypergraph-based variants of MERT (Macherey et al, 2008; Ku-mar et al, 2009) are more stable than traditionalMERT, but also require significant engineering ef-forts.7 ConclusionWe have described a simple technique for tuningan MT system that is on par with the leading tech-niques, exhibits reliable behavior, scales gracefullyto high-dimension feature spaces, and is remark-ably easy to implement.
We have demonstrated, viaa litany of experiments, that our claims are validand that this technique is widely applicable.
It isour hope that the adoption of PRO tuning leads tofewer headaches during tuning and motivates ad-vanced MT feature engineering research.AcknowledgmentsThanks to Markus Dreyer, Kevin Knight, SaiyamKohli, Greg Langmead, Daniel Marcu, DragosMunteanu, and Wei Wang for their assistance.Thanks also to the anonymous reviewers, especiallythe reviewer who implemented PRO during the re-view period and replicated our results.1360ReferencesChris Burges, Tal Shaked, Erin Renshaw, Ari Lazier,Matt Deeds, Nicole Hamilton, and Greg Hullender.2005.
Learning to rank using gradient descent.
In Pro-ceedings of the 22nd International Conference on Ma-chine Learning, ICML ?05, pages 89?96, Bonn, Ger-many.
ACM.Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, andHang Li.
2007.
Learning to rank: From pairwiseapproach to listwise approach.
In Proceedings of the24th International Conference on Machine Learning,pages 129?136, Corvalis, OR.Samidh Chatterjee and Nicola Cancedda.
2010.
Mini-mum error rate training by sampling the translation lat-tice.
In Proceedings of the 2010 Conference on Empir-ical Methods in Natural Language Processing, pages606?615, Cambridge, MA, October.
Association forComputational Linguistics.Wei Chen, Tie-Yan Liu, Yanyan Lan, Zhi-Ming Ma, andHang Li.
2009.
Ranking measures and loss functionsin learning to rank.
In Y. Bengio, D. Schuurmans,J.
Lafferty, C. K. I. Williams, and A. Culotta, editors,Advances in Neural Information Processing Systems22, pages 315?323.David Chiang, Steve DeNeefe, Yee Seng Chan, andHwee Tou Ng.
2008a.
Decomposability of transla-tion metrics for improved evaluation and efficient al-gorithms.
In Proceedings of the 2008 Conference onEmpirical Methods in Natural Language Processing,pages 610?619, Honolulu, HI, October.
Associationfor Computational Linguistics.David Chiang, Yuval Marton, and Philip Resnik.
2008b.Online large-margin training of syntactic and struc-tural translation features.
In Proceedings of the 2008Conference on Empirical Methods in Natural Lan-guage Processing, pages 224?233, Honolulu, HI, Oc-tober.
Association for Computational Linguistics.David Chiang, Kevin Knight, and Wei Wang.
2009.11,001 new features for statistical machine transla-tion.
In Proceedings of Human Language Technolo-gies: The 2009 Annual Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics, pages 218?226, Boulder, CO, June.
Associa-tion for Computational Linguistics.David Chiang.
2010.
Learning to translate with sourceand target syntax.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Linguis-tics, pages 1443?1452, Uppsala, Sweden, July.
Asso-ciation for Computational Linguistics.Brooke Cowan, Ivona Kuc?erova?, and Michael Collins.2006.
A discriminative model for tree-to-tree trans-lation.
In Proceedings of the 2006 Conference onEmpirical Methods in Natural Language Processing,pages 232?241, Sydney, Australia, July.
Associationfor Computational Linguistics.Koby Crammer and Yoram Singer.
2003.
Ultraconserva-tive online algorithms for multiclass problems.
Jour-nal of Machine Learning Research, 3:951?991.Hal Daume?
III.
2004.
Notes on CG and LM-BFGSoptimization of logistic regression.
Paper available athttp://pub.hal3.name#daume04cg-bfgs,implementation available at http://hal3.name/megam/, August.Yoav Freund, Raj Iyer, Robert E. Schapire, and YoramSinger.
2003.
An efficient boosting algorithm forcombining preferences.
Journal of Machine LearningResearch, 4:933?969.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule?
In HLT-NAACL 2004: Main Proceedings, pages 273?280,Boston, MA, May.
Association for Computational Lin-guistics.Gloria Goodale.
2008.
Language Weaver: fastin translation.
The Christian Science Monitor,October 1. http://www.csmonitor.com/Innovation/Tech-Culture/2008/1001/language-weaver-fast-in-translation.Jan Hajic?, Sandra Carberry, Stephen Clark, and JoakimNivre, editors.
2010.
Proceedings of the 48th AnnualMeeting of the Association for Computational Linguis-tics.
Association for Computational Linguistics, Upp-sala, Sweden, July.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The WEKA data mining software: An update.SIGKDD Explorations, 11(1).Ralf Herbrich, Thore Graepel, and Klaus Obermayer.1999.
Support vector learning for ordinal regression.In Proceedings of the 1999 International Conferenceon Artificial Neural Networks, pages 97?102.Abraham Ittycheriah and Salim Roukos.
2005.
A max-imum entropy word aligner for Arabic-English ma-chine translation.
In Proceedings of Human LanguageTechnology Conference and Conference on Empiri-cal Methods in Natural Language Processing, pages89?96, Vancouver, Canada, October.
Association forComputational Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Proceed-ings of the 45th Annual Meeting of the Association forComputational Linguistics Companion Volume Pro-ceedings of the Demo and Poster Sessions, pages 177?1361180, Prague, Czech Republic, June.
Association forComputational Linguistics.Shankar Kumar, Wolfgang Macherey, Chris Dyer, andFranz Och.
2009.
Efficient minimum error rate train-ing and minimum bayes-risk decoding for translationhypergraphs and lattices.
In Proceedings of the JointConference of the 47th Annual Meeting of the ACL andthe 4th International Joint Conference on Natural Lan-guage Processing of the AFNLP, pages 163?171, Sun-tec, Singapore, August.
Association for ComputationalLinguistics.Alon Lavie and Michael J. Denkowski.
2009.
TheMETEOR metric for automatic evaluation of machinetranslation.
Machine Translation, 23(2?3):105?115,September.Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, andBen Taskar.
2006.
An end-to-end discriminative ap-proach to machine translation.
In Proceedings of the21st International Conference on Computational Lin-guistics and 44th Annual Meeting of the Associationfor Computational Linguistics, pages 761?768, Syd-ney, Australia, July.
Association for ComputationalLinguistics.Wolfgang Macherey, Franz Josef Och, Ignacio Thayer,and Jakob Uszkoreit.
2008.
Lattice-based minimumerror rate training for statistical machine translation.In Proceedings of the 2008 Conference on EmpiricalMethods in Natural Language Processing, pages 725?734, Honolulu, HI, October.
Association for Compu-tational Linguistics.Christopher Manning and Dan Klein.
2003.
Optimiza-tion, maxent models, and conditional estimation with-out magic.
Tutorial at HLT-NAACL 2003 and ACL2003.I.
Dan Melamed, Ryan Green, and Joseph P. Turian.2003.
Precision and recall of machine translation.
InCompanion Volume of the Proceedings of HLT-NAACL2003 - Short Papers, pages 61?63, Edmonton, Canada,May?June.
Association for Computational Linguis-tics.Franz Och and Hermann Ney.
2000.
Improved statisticalalignment models.
In Proceedings of the 38th AnnualMeeting of the Association for Computational Linguis-tics, pages 440?447, Hong Kong, October.Franz Josef Och and Hermann Ney.
2002.
Discrimi-native training and maximum entropy models for sta-tistical machine translation.
In Proceedings of 40thAnnual Meeting of the Association for ComputationalLinguistics, pages 295?302, Philadelphia, PA, July.Association for Computational Linguistics.Franz Och and Hermann Ney.
2004.
The alignment tem-plate approach to statistical machine translation.
Com-putational Linguistics, 30(4):417?449.Franz Och.
2003.
Minimum error rate training in statis-tical machine translation.
In Proceedings of the 41stAnnual Meeting of the Association for ComputationalLinguistics, pages 160?167, Sapporo, Japan, July.
As-sociation for Computational Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedings of40th Annual Meeting of the Association for Computa-tional Linguistics, pages 311?318, Philadelphia, PA,July.
Association for Computational Linguistics.Benjamin Roth, Andrew McCallum, Marc Dymetman,and Nicola Cancedda.
2010.
Machine translation us-ing overlapping alignments and samplerank.
In Pro-ceedings of Association for Machine Translation in theAmericas, Denver, CO.Libin Shen, Anoop Sarkar, and Franz Josef Och.
2004.Discriminative reranking for machine translation.
InDaniel Marcu Susan Dumais and Salim Roukos, ed-itors, HLT-NAACL 2004: Main Proceedings, pages177?184, Boston, MA, May 2 - May 7.
Associationfor Computational Linguistics.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of Association for Machine Translationin the Americas, pages 223?231.Christoph Tillmann and Tong Zhang.
2005.
A localizedprediction model for statistical machine translation.
InProceedings of the 43rd Annual Meeting of the ACL,pages 557?564, Ann Arbor, MI, June.
Association forComputational Linguistics.Wei Wang.
2011.
Personal communication.Taro Watanabe, Jun Suzuki, Hajime Tsukada, and HidekiIsozaki.
2006.
NTT statistical machine translation forIWSLT 2006.
In Proceedings of IWSLT 2006, pages95?102.Taro Watanabe, Jun Suzuki, Hajime Tsukada, and HidekiIsozaki.
2007.
Online large-margin training for sta-tistical machine translation.
In Proceedings of the2007 Joint Conference on Empirical Methods in Nat-ural Language Processing and Computational Natu-ral Language Learning (EMNLP-CoNLL), pages 764?773, Prague, Czech Republic, June.
Association forComputational Linguistics.Deyi Xiong, Qun Liu, and Shouxun Lin.
2006.
Maxi-mum entropy based phrase reordering model for sta-tistical machine translation.
In Proceedings of the 21stInternational Conference on Computational Linguis-tics and 44th Annual Meeting of the Association forComputational Linguistics, pages 521?528, Sydney,Australia, July.
Association for Computational Lin-guistics.1362
