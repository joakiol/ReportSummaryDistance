Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 155?160,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsDependency-based Pre-ordering for Chinese-English Machine TranslationJingsheng Cai?
?Masao Utiyama?Eiichiro Sumita?Yujie Zhang?
?School of Computer and Information Technology, Beijing Jiaotong University?National Institute of Information and Communications Technologyjoycetsai99@gmail.com{mutiyama, eiichiro.sumita}@nict.go.jpyjzhang@bjtu.edu.cnAbstractIn statistical machine translation (SMT),syntax-based pre-ordering of the sourcelanguage is an effective method for deal-ing with language pairs where there aregreat differences in their respective wordorders.
This paper introduces a novelpre-ordering approach based on depen-dency parsing for Chinese-English SMT.We present a set of dependency-based pre-ordering rules which improved the BLEUscore by 1.61 on the NIST 2006 evalua-tion data.
We also investigate the accuracyof the rule set by conducting human eval-uations.1 IntroductionSMT systems have difficulties translating betweendistant language pairs such as Chinese and En-glish.
The reason for this is that there are greatdifferences in their word orders.
Reordering there-fore becomes a key issue in SMT systems betweendistant language pairs.Previous work has shown that the approachestackling the problem by introducing a pre-orderingprocedure into phrase-based SMT (PBSMT) wereeffective.
These pre-ordering approaches firstparse the source language sentences to create parsetrees.
Then, syntactic reordering rules are ap-plied to these parse trees with the goal of re-ordering the source language sentences into theword order of the target language.
Syntax-basedpre-ordering by employing constituent parsinghave demonstrated effectiveness in many languagepairs, such as English-French (Xia and McCord,2004), German-English (Collins et al, 2005),Chinese-English (Wang et al, 2007; Zhang et al,2008), and English-Japanese (Lee et al, 2010).
?This work was done when the first author was on aninternship in NICT.As a kind of constituent structure, HPSG (Pol-lard and Sag, 1994) parsing-based pre-orderingshowed improvements in SVO-SOV translations,such as English-Japanese (Isozaki et al, 2010; Wuet al, 2011) and Chinese-Japanese (Han et al,2012).
Since dependency parsing is more concisethan constituent parsing in describing sentences,some research has used dependency parsing inpre-ordering approaches for language pairs suchas Arabic-English (Habash, 2007), and English-SOV languages (Xu et al, 2009; Katz-Brown etal., 2011).
The pre-ordering rules can be mademanually (Collins et al, 2005; Wang et al, 2007;Han et al, 2012) or extracted automatically froma parallel corpus (Xia and McCord, 2004; Habash,2007; Zhang et al, 2007; Wu et al, 2011).The purpose of this paper is to introduce a noveldependency-based pre-ordering approach throughcreating a pre-ordering rule set and applying it tothe Chinese-English PBSMT system.
Experimentresults showed that our pre-ordering rule set im-proved the BLEU score on the NIST 2006 evalua-tion data by 1.61.
Moreover, this rule set substan-tially decreased the total times of rule applicationabout 60%, compared with a constituent-based ap-proach (Wang et al, 2007).
We also conducted hu-man evaluations in order to assess its accuracy.
Toour knowledge, our manually created pre-orderingrule set is the first Chinese-English dependency-based pre-ordering rule set.The most similar work to this paper is that ofWang et al (2007).
They created a set of pre-ordering rules for constituent parsers for Chinese-English PBSMT.
In contrast, we propose a set ofpre-ordering rules for dependency parsers.
Weargue that even though the rules by Wang et al(2007) exist, it is almost impossible to automati-cally convert their rules into rules that are appli-cable to dependency parsers.
In fact, we aban-doned our initial attempts to automatically converttheir rules into rules for dependency parsers, and155(a) A constituent parse tree(b) Stanford typed dependency parse treeFigure 1: A constituent parse tree and its cor-responding Stanford typed dependency parse treefor the same Chinese sentence.spent more than two months discovering the rulesintroduced in this paper.
By applying our rulesand Wang et al?s rules, one can use both depen-dency and constituency parsers for pre-ordering inChinese-English PBSMT.This is especially important on the point of thesystem combination of PBSMT systems, becausethe diversity of outputs from machine translationsystems is important for system combination (Ceret al, 2013).
By using both our rules and Wang etal.
?s rules, one can obtain diverse machine trans-lation results because the pre-ordering results ofthese two rule sets are generally different.Another similar work is that of (Xu et al, 2009).They created a pre-ordering rule set for depen-dency parsers from English to several SOV lan-guages.
In contrast, our rule set is for Chinese-English PBSMT.
That is, the direction of transla-tion is opposite.
Because there are a lot of lan-guage specific decisions that reflect specific as-pects of the source language and the language paircombination, our rule set provides a valuable re-source for pre-ordering in Chinese-English PB-SMT.2 Dependency-based Pre-ordering RuleSetFigure 1 shows a constituent parse tree and itsStanford typed dependency parse tree for the sameFigure 2: An example of a preposition phrase witha plmod structure.
The phrase translates into ?infront of the US embassy?.Chinese sentence.
As shown in the figure, thenumber of nodes in the dependency parse tree(i.e.
9) is much fewer than that in its correspond-ing constituent parse tree (i.e.
17).
Because de-pendency parse trees are generally more concisethan the constituent ones, they can conduct long-distance reorderings in a finer way.
Thus, we at-tempted to conduct pre-ordering based on depen-dency parsing.
There are two widely-used de-pendency systems ?
Stanford typed dependenciesand CoNLL typed dependencies.
For Chinese,there are 45 types of grammatical relations forStanford typed dependencies (Chang et al, 2009)and 25 for CoNLL typed dependencies.
As wethought that Stanford typed dependencies coulddescribe language phenomena more meticulouslyowing to more types of grammatical relations, wepreferred to use it for searching candidate pre-ordering rules.We designed two types of formats in ourdependency-based pre-ordering rules.
They are:Type-1: x : yType-2: x - yHere, both x and y are dependency relations(e.g., plmod or lobj in Figure 2).
We define thedependency structure of a dependency relation asthe structure containing the dependent word (e.g.,the word directly indicated by plmod, or ???
inFigure 2) and the whole subtree under the depen-dency relation (all of the words that directly orindirectly depend on the dependent word, or thewords under ???
in Figure 2).
Further, we defineX and Y as the corresponding dependency struc-tures of the dependency relations x and y, respec-tively.
We define X\Y as structure X except Y. Forexample, in Figure 2, let x and y denote plmod andlobj dependency relations, then X represents ??
?and all words under ??
?, Y represents ????
?and all words under ????
?, and X\Y represents156Figure 3: An example of rcmod structure withinan nsubj structure.
The phrase translates into ?asenior official close to Sharon said?.???.
For Type-1, Y is a sub-structure of X. Therule repositions X\Y to the position before Y. ForType-2, X and Y are ordered sibling structures un-der a same parent node.
The rule repositions X tothe position after Y.We obtained rules as the following steps:1 Search the Chinese dependency parse treesin the corpus and rank all of the structuresmatching the two types of rules respectivelyaccording to their frequencies.
Note thatwhile calculating the frequencies of Type-1 structures, we dismissed the structures inwhich X occurred before Y originally.2 Filtration.
1) Filter out the structures whichoccurred less than 5,000 times.
2) Filterout the structures from which it was almostimpossible to derive candidate pre-orderingrules because x or y was an ?irrespective?
de-pendency relation, for example, root, conj, ccand so on.3 Investigate the remaining structures.
For eachkind of structure, we selected some of thesample dependency parse trees that containedit, tried to restructure the parse trees accord-ing to the matched rule and judged the re-ordered Chinese phrases.
If the reorderingproduced a Chinese phrase that had a closerword order to that of the English one, thisstructure would be a candidate pre-orderingrule.4 Conduct primary experiments which used thesame training set and development set as theexperiments described in Section 3.
In theprimary experiments, we tested the effective-ness of the candidate rules and filtered theones that did not work based on the BLEUscores on the development set.Figure 4: An example of rcmod structure with apreposition modifier.
The phrase translates into ?apress conference held in Kabul?.As a result, we obtained eight pre-ordering rulesin total, which can be divided into three depen-dency relation categories.
They are: plmod (lo-calizer modifier of a preposition), rcmod (relativeclause modifier) and prep (preposition modifer).Each of these categories are discussed in detail be-low.plmod Figure 2 shows an example of a preposi-tional phrase with a plmod structure, which trans-lates literally into ?in the US embassy front?.
InChinese, the dependent word of a plmod relation(e.g., ???
in Figure 2) occurs in the last positionof the prepositional phrase.
However, in English,this kind of word (e.g., ?front?
in the caption ofFigure 2) always occur directly after prepositions,which is to say, in the second position in a preposi-tional phrase.
Therefore, we applied a rule plmod: lobj (localizer object) to reposition the depen-dent word of the plmod relation (e.g., ???
in Fig-ure 2) to the position before the lobj structure (e.g.,???
????
in Figure 2).
In this case, it alsocomes directly after the preposition.
Similarly, wecreated a rule plmod : lccomp (clausal comple-ment of a localizer).rcmod Figure 3 shows an example of an rcmodstructure under an nsubj (nominal subject) struc-ture.
Here ?mw?
means ?measure word?.
Asshown in the figure, relative clause modifiers inChinese (e.g., ???
??
??
in Figure 3) oc-curs before the noun being modified, which is incontrast to English (e.g., ?close to Sharon?
in thecaption of Figure 3), where they come after.
Thus,we introduced a series of rules NOUN : rcmodto restructure rcmod structures so that the nounis moved to the head.
In this example, with theapplication of an nsubj : rcmod rule, the phrasecan be translated into ?a senior official close toSharon say?, which has a word order very closeto English.
Since a noun can be nsubj, dobj (di-rect object), pobj (prepositional object) and lobj157Type System Parser BLEU Counts #Sent.- No pre-ordering - 29.96 - -Constituent WR07 Berkeley 31.45 2,561,937 852,052Dependency OUR DEP 1 Berkeley Const.
31.54 978,013 556,752OUR DEP 2 Mate 31.57 947,441 547,084Table 1: The comparison of four systems, including the performance (BLEU) on the test set, the totalcount of each rule set and the number of sentences they were applied to on the training set.Figure 5: An example of verb phrase with apreposition modifier.
The phrase translates into?Musharraf told reporters here?.in Stanford typed dependencies, we created fourrules from the NOUN pattern.
Note that for somepreposition modifiers, we needed a rule rcmod :prep to conduct the same work.
For instance, theChinese phrase in Figure 4 can be translated into?hold in Kabul press conference?
with the appli-cation of this rule.prep Within verb phrases, the positions of prepstructures are quite different between Chinese andEnglish.
Figure 5 shows an example of a verbphrase with a preposition modifier (prep), whichliterally translates into ?Musharraf at this place tellreporter?.
Recognizing that prep structures occurbefore the verb in Chinese (e.g., ?????
in Fig-ure 5) but after the verb in English (usually in thelast position of a verb phrase, e.g., ?here?
in thecaption of Figure 5), we applied a rule prep - dobjto reposition prep structures after their sibling dobjstructures.In summary, the dependency-based pre-ordering rule set has eight rules: plmod : lobj,plmod : lccomp, nsubj : rcmod, dobj : rcmod,pobj : rcmod, lobj : rcmod, rcmod : prep, andprep - dobj.3 ExperimentsWe used the MOSES PBSMT system (Koehn etal., 2007) in our experiments.
The training data,which included those data used in Wang et al(2007), contained 1 million pairs of sentences ex-tracted from the Linguistic Data Consortium?s par-allel news corpora.
Our development set wasthe official NIST MT evaluation data from 2002to 2005, consisting of 4476 Chinese-English sen-tences pairs.
Our test set was the NIST 2006 MTevaluation data, consisting of 1664 sentence pairs.We employed the Stanford Segmenter1to segmentall of the data sets.
For evaluation, we used BLEUscores (Papineni et al, 2002).We implemented the constituent-based pre-ordering rule set in Wang et al (2007) for compar-ison, which is called WR07 below.
The BerkeleyParser (Petrov et al, 2006) was employed for pars-ing the Chinese sentences.
For training the Berke-ley Parser, we used Chinese Treebank (CTB) 7.0.We conducted our dependency-based pre-ordering experiments on the Berkeley Parser andthe Mate Parser (Bohnet, 2010), which wereshown to be the two best parsers for Stanfordtyped dependencies (Che et al, 2012).
First, weconverted the constituent parse trees in the re-sults of the Berkeley Parser into dependency parsetrees by employing a tool in the Stanford Parser(Klein and Manning, 2003).
For the Mate Parser,POS tagged inputs are required both in trainingand in inference.
Thus, we then extracted thePOS information from the results of the Berke-ley Parser and used these as the pre-specified POStags for the Mate Parser.
Finally, we applied ourdependency-based pre-ordering rule set to the de-pendency parse trees created from the convertedBerkeley Parser and the Mate Parser, respectively.Table 1 presents a comparison of the systemwithout pre-ordering, the constituent system us-ing WR07 and two dependency systems employ-ing the converted Berkeley Parser and the MateParser, respectively.
It shows the BLEU scores onthe test set and the statistics of pre-ordering on thetraining set, which includes the total count of eachrule set and the number of sentences they were ap-1http://nlp.stanford.edu/software/segmenter.shtml158Category Count Correct Incorrect Accuracyplmod 42 26 16 61.9%rcmod 89 49 40 55.1%prep 54 36 18 66.7%All 185 111 74 60.0%Table 2: Accuracy of the dependency-based pre-ordering rules on a set of 200 sentences randomly se-lected from the development set.plied to.
Both of our dependency systems outper-formed WR07 slightly but were not significant atp = 0.05.
However, both of them substantially de-creased the total times about 60% (or 1,600,000)for pre-ordering rule applications on the trainingset, compared with WR07.
In our opinion, the rea-son for the great decrease was that the dependencyparse trees were more concise than the constituentparse trees in describing sentences and they couldalso describe the reordering at the sentence level ina finer way.
In contrast, the constituent parse treeswere more redundant and they needed more nodesto conduct long-distance reordering.
In this case,the affect of the performance of the constituentparsers on pre-ordering is larger than that of thedependency ones so that the constituent parsers arelikely to bring about more incorrect pre-orderings.Similar to Wang et al (2007), we carried outhuman evaluations to assess the accuracy of ourdependency-based pre-ordering rules by employ-ing the system ?OUR DEP 2?
in Table 1.
Theevaluation set contained 200 sentences randomlyselected from the development set.
Among them,107 sentences contained at least one rule and therules were applied 185 times totally.
Since theaccuracy check for dependency parse trees tookgreat deal of time, we did not try to select er-ror free (100% accurately parsed) sentences.
Abilingual speaker of Chinese and English lookedat an original Chinese phrase and the pre-orderedone with their corresponding English phrase andjudged whether the pre-ordering obtained a Chi-nese phrase that had a closer word order to the En-glish one.
Table 2 shows the accuracies of threecategories of our dependency-based pre-orderingrules.
The overall accuracy of this rule set is60.0%, which is almost at the same level as theWR07 rule set (62.1%), according to the similarevaluation (200 sentences and one annotator) con-ducted in Wang et al (2007).
Notice that someof the incorrect pre-orderings may be caused byerroneous parsing as also suggested by Wang etal.
(2007).
Through human evaluations, we foundthat 19 out of the total 74 incorrect pre-orderingsresulted from errors in parsing.
Among them, 13incorrect pre-orderings applied the rules of the rc-mod category.
The analysis suggests that we needto introduce constraints on the rule application ofthis category in the future.4 ConclusionIn this paper, we introduced a novel pre-orderingapproach based on dependency parsing for aChinese-English PBSMT system.
The resultsshowed that our approach achieved a BLEU scoregain of 1.61.
Moreover, our dependency-basedpre-ordering rule set substantially decreased thetime for applying pre-ordering rules about 60%compared with WR07, on the training set of 1Msentences pairs.
The overall accuracy of our ruleset is 60.0%, which is almost at the same level asthe WR07 rule set.
These results indicated thatdependency parsing is more effective for conduct-ing pre-ordering for Chinese-English PBSMT.
Al-though our work focused on Chinese, the ideas canalso be applied to other languages.In the future, we attempt to create more efficientpre-ordering rules by exploiting the rich informa-tion in dependency structures.AcknowledgmentsWe thank the anonymous reviewers for their valu-able comments and suggestions.
This work is sup-ported in part by the International Science & Tech-nology Cooperation Program of China (Grant No.2014DFA11350) and Key Lab of Intelligent In-formation Processing of Chinese Academy of Sci-ences (CAS), Institute of Computing Technology,CAS, Beijing 100190, China.ReferencesBernd Bohnet.
2010.
Very high accuracy and fast de-pendency parsing is not a contradiction.
In Proceed-159ings of the 23rd International Conference on Com-putational Linguistics (COLING 2010).Daniel Cer, Christopher D. Manning, and Dan Juraf-sky.
2013.
Positive Diversity Tuning for MachineTranslation System Combination.
In Proceedings ofthe Eighth Workshop on Statistical Machine Trans-lation (WMT 2013).Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, andChristopher D. Manning.
2009.
Discriminativereordering with Chinese grammatical relations fea-tures.
In Proceedings of the HLT-NAACL Workshopon Syntax and Structure in Statistical Translation,pages 51-59.Wanxiang Che, Valentin Spitkovsky, and Ting Liu.2012.
A comparison of Chinese parsers for Stan-ford dependencies.
In Proceedings of the 50th An-nual Meeting of the Association for ComputationalLinguistics, pages 11-16.Michael Collins, Philipp Koehn, and Ivona Kucerova.2005.
Clause restructuring for statistical machinetranslation.
In Proceedings of the 43rd AnnualMeeting of the Association for Computational Lin-guistics, pages 531-540.Dan Han, Katsuhito Sudoh, Xianchao Wu, Kevin Duh,Hajime Tsukada, and Masaaki Nagata.
2012.
HeadFinalization reordering for Chinese-to-Japanese ma-chine translation.
In Proceedings of SSST-6, SixthWorkshop on Syntax, Semantics and Structure inStatistical Translation, pages 57-66.Nizar Habash.
2007.
Syntactic preprocessing for sta-tistical machine translation.
In Proceedings of the11th Machine Translation Summit (MT-Summit).Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, andKevin Duh.
2010.
Head Finalization: A simple re-ordering rule for SOV languages.
In Proceedingsof the Joint Fifth Workshop on Statistical MachineTranslation and MetricsMATR, pages 250-257.Jason Katz-Brown, Slav Petrov, Ryan McDonald,Franz J. Och, David Talbot, Hiroshi Ichikawa,Masakazu Seno, and Hideto Kazawa.
2011.
Train-ing a parser for machine translation reordering.
InProceedings of the Conference on Empirical Meth-ods in Natural Language Processing, pages 183-192.Dan Klein and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting on Association for Computa-tional Linguistics, pages 423-430.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-dra Constantin, and Evan Herbst.
2007.
Moses:Open source toolkit for statistical machine transla-tion.
In Proceedings of the 45th Annual Meeting ofthe Association for Computational Linguistics Com-panion Volume Proceedings of the Demo and PosterSessions, pages 177-180.Young-Suk Lee, Bing Zhao, and Xiaoqian Luo.2010.
Constituent reordering and syntax models forEnglish-to-Japanese statistical machine translation.In Proceedings of the 23rd International of Confer-ence on Computational Linguistics, pages 626-634.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedingsof 40th Annual Meeting of the Association for Com-putational Linguistics, pages 311-318.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, andinterpretable tree annotation.
In Proceedings ofthe 21st International Conference on ComputationalLinguistics and 44th Annual Meeting of the Associa-tion for Computational Linguistics, pages 433-440.Carl Pollard and Ivan A.
Sag.
1994.
Head-DrivenPhrase Structure Grammar.
University of ChicagoPress.Chao Wang, Michael Collins, and Philipp Koehn.2007.
Chinese syntactic reordering for statisticalmachine translation.
In Proceedings of the 2007Joint Conference on Empirical Methods in NaturalLanguage Processing and Computational NaturalLanguage Learning, pages 737-745.Xianchao Wu, Katsuhito Sudoh, Kevin Duh, HajimeTsukada, and Masaaki Nagata.
2011.
Extractingpreordering rules from predicate-argument struc-tures.
In Proceedings of 5th International Joint Con-ference on Natural Language Processing, pages 29-37.Fei Xia and Michael McCord.
2004.
Improvinga statistical MT system with automatically learnedrewrite patterns.
In Proceedings of Coling 2004,pages 508-514.Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz J.Och.
2009.
Using a dependency parser to improveSMT for subject-object-verb languages.
In Proceed-ings of HLT-NAACL, pages 245-253.Jiajun Zhang, Chengqing Zong, and Shoushan Li.2008.
Sentence type based reordering model for sta-tistical machine translation.
In Proceedings of the22nd International Conference on ComputationalLinguistics, pages 1089-1096.Yuqi Zhang, Richard Zens, and Hermann Ney.
2011.Chunk-level reordering of source language sen-tences with automatically learned rules for statisti-cal machine translation.
In HLT-NAACL Workshopon Syntax and Structure in Statistical Translation,pages 1-8.160
