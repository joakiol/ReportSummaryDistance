Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 234?244,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsA Probabilistic Model of Syntactic and Semantic Acquisition fromChild-Directed Utterances and their MeaningsTom Kwiatkowski* ?tomk@cs.washington.eduSharon Goldwater?sgwater@inf.ed.ac.ukLuke Zettlemoyer?lsz@cs.washington.eduMark Steedman?steedman@inf.ed.ac.uk?
ILCC, School of InformaticsUniversity of EdinburghEdinburgh, EH8 9AB, UK?Computer Science & EngineeringUniversity of WashingtonSeattle, WA, 98195, USAAbstractThis paper presents an incremental prob-abilistic learner that models the acquis-tion of syntax and semantics from a cor-pus of child-directed utterances paired withpossible representations of their meanings.These meaning representations approxi-mate the contextual input available to thechild; they do not specify the meanings ofindividual words or syntactic derivations.The learner then has to infer the meaningsand syntactic properties of the words in theinput along with a parsing model.
We usethe CCG grammatical framework and traina non-parametric Bayesian model of parsestructure with online variational Bayesianexpectation maximization.
When tested onutterances from the CHILDES corpus, ourlearner outperforms a state-of-the-art se-mantic parser.
In addition, it models suchaspects of child acquisition as ?fast map-ping,?
while also countering previous crit-icisms of statistical syntactic learners.1 IntroductionChildren learn language by mapping the utter-ances they hear onto what they believe those ut-terances mean.
The precise nature of the child?sprelinguistic representation of meaning is notknown.
We assume for present purposes thatit can be approximated by compositional logicalrepresentations such as (1), where the meaning isa logical expression that describes a relationshiphave between the person you refers to and theobject another(x, cookie(x)):Utterance : you have another cookie (1)Meaning : have(you, another(x, cookie(x)))Most situations will support a number of plausi-ble meanings, so the child has to learn in the faceof propositional uncertainty1, from a set of con-textually afforded meaning candidates, as here:Utterance : you have another cookieCandidateMeanings??
?have(you, another(x, cookie(x)))eat(you, your(x, cake(x)))want(i, another(x, cookie(x)))The task is then to learn, from a sequence of such(utterance, meaning-candidates) pairs, the correctlexicon and parsing model.
Here we present aprobabilistic account of this task with an empha-sis on cognitive plausibility.Our criteria for plausibility are that the learnermust not require any language-specific informa-tion prior to learning and that the learning algo-rithm must be strictly incremental: it sees eachtraining instance sequentially and exactly once.We define a Bayesian model of parse structurewith Dirichlet process priors and train this on aset of (utterance, meaning-candidates) pairs de-rived from the CHILDES corpus (MacWhinney,2000) using online variational Bayesian EM.We evaluate the learnt grammar in three ways.First, we test the accuracy of the trained modelin parsing unseen utterances onto gold standardannotations of their meaning.
We show thatit outperforms a state-of-the-art semantic parser(Kwiatkowski et al 2010) when run with similartraining conditions (i.e., neither system is giventhe corpus based initialization originally used byKwiatkowski et al.
We then examine the learn-ing curves of some individual words, showing thatthe model can learn word meanings on the ba-sis of a single exposure, similar to the fast map-ping phenomenon observed in children (Careyand Bartlett, 1978).
Finally, we show that our1Similar to referential uncertainty but relating to propo-sitions rather than referents.234learner captures the step-like learning curves forword order regularities that Thornton and Tesan(2007) claim children show.
This result coun-ters Thornton and Tesan?s criticism of statisticalgrammar learners?that they tend to exhibit grad-ual learning curves rather than the abrupt changesin linguistic competence observed in children.1.1 Related WorkModels of syntactic acquisition, whether theyhave addressed the task of learning both syn-tax and semantics (Siskind, 1992; Villavicencio,2002; Buttery, 2006) or syntax alone (Gibsonand Wexler, 1994; Sakas and Fodor, 2001; Yang,2002) have aimed to learn a single, correct, deter-ministic grammar.
With the exception of Buttery(2006) they also adopt the Principles and Param-eters grammatical framework, which assumes de-tailed knowledge of linguistic regularities2.
Ourapproach contrasts with all previous models in as-suming a very general kind of linguistic knowl-edge and a probabilistic grammar.
Specifically,we use the probabilistic Combinatory CategorialGrammar (CCG) framework, and assume onlythat the learner has access to a small set of generalcombinatory schemata and a functional mappingfrom semantic type to syntactic category.
Further-more, this paper is the first to evaluate a modelof child syntactic-semantic acquisition by parsingunseen data.Models of child word learning have focusedon semantics only, learning word meanings fromutterances paired with either sets of concept sym-bols (Yu and Ballard, 2007; Frank et al 2008; Fa-zly et al 2010) or a compositional meaning rep-resentation of the type used here (Siskind, 1996).The models of Alishahi and Stevenson (2008)and Maurits et al(2009) learn, as well as word-meanings, orderings for verb-argument structuresbut not the full parsing model that we learn here.Semantic parser induction as addressed byZettlemoyer and Collins (2005, 2007, 2009), Kateand Mooney (2007), Wong and Mooney (2006,2007), Lu et al(2008), Chen et al(2010),Kwiatkowski et al(2010, 2011) and Bo?rschingeret al(2011) has the same task definition as theone addressed by this paper.
However, the learn-ing approaches presented in those previous pa-2This linguistic use of the term ?parameter?
is distinctfrom the statistical use found elsewhere in this paper.pers are not designed to be cognitively plausible,using batch training algorithms, multiple passesover the data, and language specific initialisations(lists of noun phrases and additional corpus statis-tics), all of which we dispense with here.
Inparticular, our approach is closely related that ofKwiatkowski et al(2010) but, whereas that workrequired careful initialisation and multiple passesover the training data to learn a discriminativeparsing model, here we learn a generative parsingmodel without either.1.2 Overview of the approachOur approach takes, as input, a corpus of (ut-terance, meaning-candidates) pairs {(si, {m}i) :i = 1, .
.
.
, N}, and learns a CCG lexicon ?
andthe probability of each production a ?
b thatcould be used in a parse.
Together, these definea probabilistic parser that can be used to find themost probable meaning for any new sentence.We learn both the lexicon and production prob-abilities from allowable parses of the trainingpairs.
The set of allowable parses {t} for a sin-gle (utterance, meaning-candidates) pair consistsof those parses that map the utterance onto one ofthe meanings.
This set is generated with the func-tional mapping T :{t} = T (s,m), (2)which is defined, following Kwiatkowski et al(2010), using only the CCG combinators and amapping from semantic type to syntactic category(presented in in Section 4).The CCG lexicon ?
is learnt by reading offthe lexical items used in all parses of all trainingpairs.
Production probabilities are learnt in con-junction with ?
through the use of an incremen-tal parameter estimation algorithm, online Varia-tional Bayesian EM, as described in Section 5.Before presenting the probabilistic model, themapping T , and the parameter training algorithm,we first provide some background on the meaningrepresentations we use and on CCG.2 Background2.1 Meaning RepresentationsWe represent the meanings of utterances in first-order predicate logic using the lambda-calculus.An example logical expression (henceforth alsoreferred to as a lambda expression) is:like(eve,mummy) (3)235which expresses a logical relationship like be-tween the entity eve and the entity mummy.
InSection 6.1 we will see how logical expressionslike this are created for a set of child-directed ut-terances (to use in training our model).The lambda-calculus uses ?
operators to definefunctions.
These may be used to represent func-tional meanings of utterances but they may also beused as a ?glue language?, to compose elements offirst order logical expressions.
For example, thefunction ?x?y.like(y, x) can be combined withthe object mummy to give the phrasal mean-ing ?y.like(y,mummy) through the lambda-calculus operation of function application.2.2 CCGCombinatory Categorial Grammar (CCG; Steed-man 2000) is a strongly lexicalised linguistic for-malism that tightly couples syntax and seman-tics.
Each CCG lexical item in the lexicon ?
isa triple, written as word ` syntactic category :logical expression .
Examples are:You ` NP : youread ` S\NP/NP : ?x?y.read(y, x)the ` NP/N : ?f.the(x, f(x))book ` N : ?x.book(x)A full CCG category X : h has syntactic cate-gory X and logical expression h. Syntactic cat-egories may be atomic (e.g., S or NP) or com-plex (e.g., (S\NP)/NP).
Slash operators in com-plex categories define functions from the range onthe right of the slash to the result on the left inmuch the same way as lambda operators do in thelambda-calculus.
The direction of the slash de-fines the linear order of function and argument.CCG uses a small set of combinatory rules toconcurrently build syntactic parses and semanticrepresentations.
Two example combinatory rulesare forward (>) and backward (<) application:X/Y : f Y : g ?
X : f(g) (>)Y : g X\Y : f ?
X : f(g) (<)Given the lexicon above, the phrase ?You read thebook?
can be parsed using these rules, as illus-trated in Figure 1 (with additional notation dis-cussed in the following section)..CCG also includes combinatory rules offorward (> B) and backward (< B) composition:X/Y : f Y/Z : g ?X/Z : ?x.f(g(x)) (> B)Y \Z : g X\Y : f ?X\Z : ?x.f(g(x)) (< B)3 Modelling DerivationsThe objective of our learning algorithm is tolearn the correct parameterisation of a probabilis-tic model P (s,m, t) over (utterance, meaning,derivation) triples.
This model assigns a proba-bility to each of the grammar productions a ?
bused to build the derivation tree t. The probabil-ity of any given CCG derivation t with sentences and semantics m is calculated as the product ofall of its production probabilities.P (s,m, t) =?a?b?tP (b|a) (4)For example, the derivation in Figure 1 contains13 productions, and its probability is the productof the 13 production probabilities.
Grammar pro-ductions may be either syntactic?used to build asyntactic derivation tree, or lexical?used to gen-erate logical expressions and words at the leavesof this tree.A syntactic production Ch ?
R expands ahead node Ch into a result R that is either anordered pair of syntactic parse nodes ?Cl,Cr?
(for a binary production) or a single parse node(for a unary production).
Only two unary syn-tactic productions are allowed in the grammar:START?
A to generate A as the top syntacticnode of a parse tree and A?
[A]lex to indicatethat A is a leaf node in the syntactic derivationand should be used to generate a logical expres-sion and word.
Syntactic derivations are built byrecursively applying syntactic productions to non-leaf nodes in the derivation tree.
Each syntacticproduction Ch ?
R has conditional probabilityP (R|Ch).
There are 3 binary and 5 unary syntac-tic productions in Figure 1.Lexical productions have two forms.
Logicalexpressions are produced from leaf nodes in thesyntactic derivation tree Alex ?
m with condi-tional probability P (m|Alex).
Words are then pro-duced from these logical expressions with condi-tional probability P (w|m).
An example logicalproduction from Figure 1 is [NP]lex ?
you.
Anexample word production is you?
You.Every production a ?
b used in a parse tree tis chosen from the set of productions that couldbe used to expand a head node a.
If there are afinite K productions that could expand a then aK-dimensional Multinomial distribution parame-terised by ?a can be used to model the categorical236STARTSdclNP[NP]lexyouYouSdcl\NP(Sdcl\NP)/NP[(Sdcl\NP)/NP]lex?x?y.read(y, x)readNPNP/N[NP/N]lex?f?x.the(x, f(x))theN[N]lex?x.book(x)bookFigure 1: Derivation of sentence You read thebook with meaning read(you, the(x, book(x))).choice of production:b ?
Multinomial(?a) (5)However, before training a model of language ac-quisition the dimensionality and contents of boththe syntactic grammar and lexicon are unknown.In order to maintain a probability model withcover over the countably infinite number of pos-sible productions, we define a Dirichlet Process(DP) prior for each possible production head a.For the production head a, DP (?a, Ha) assignssome probability mass to all possible productiontargets {b} covered by the base distribution Ha.It is possible to use the DP as an infinite priorfrom which the parameter set of a finite dimen-sional Multinomial may be drawn provided thatwe can choose a suitable partition of {b}.
Whencalculating the probability of an (s,m, t) triple,the choice of this partition is easy.
For any givenproduction head a there is a finite set of usableproduction targets {b1, .
.
.
, bk?1} in t. We createa partition that includes one entry for each of thesealong with a final entry {bk, .
.
. }
that includes allother ways in which a could be expanded in dif-ferent contexts.
Then, by applying the distributionGa drawn from the DP to this partition, we get aparameter vector ?a that is equivalent to a drawfrom a k dimensional Dirichlet distribution:Ga ?
DP (?a, Ha) (6)?a = (Ga(b1), .
.
.
, Ga(bk?1), Ga({bk, .
.
.
})?
Dir(?aH(b1), .
.
.
, ?aHa(bk?1), (7)?aHa({bk, .
.
.
}))Together, Equations 4-7 describe the joint distri-bution P (X,S, ?)
over the observed training dataX = {(si, {m}i) : i = 1, .
.
.
, N}, the latent vari-ables S (containing the productions used in eachparse t) and the parsing parameters ?.4 Generating ParsesThe previous section defined a parameterisationover parses assuming that the CCG lexicon ?
wasknown.
In practice ?
is empty prior to trainingand must be populated with the lexical items fromparses t consistent with training pairs (s, {m}).The set of allowed parses {t} is defined by thefunction T from Equation 2.
Here we review thesplitting procedure of Kwiatkowski et al(2010)that is used to generate CCG lexical items and de-scribe how it is used by T to create a packed chartrepresentation of all parses {t} that are consistentwith s and at least one of the meaning represen-tations in {m}.
In this section we assume that sis paired at each point with only a single meaningm.
Later we will show how T is used multipletimes to create the set of parses consistent with sand a set of candidate meanings {m}.The splitting procedure takes as input a CCGcategory X :h, such as NP : a(x, cookie(x)), andreturns a set of category splits.
Each category splitis a pair of CCG categories (Cl :ml,Cr :mr) thatcan be recombined to give X : h using one of theCCG combinators in Section 2.2.
The CCG cat-egory splitting procedure has two parts: logicalsplitting of the category semantics h; and syntac-tic splitting of the syntactic category X.
Each logi-cal split of h is a pair of lambda expressions (f, g)in the following set:{(f, g) | h = f(g) ?
h = ?x.f(g(x))}, (8)which means that f and g can be recombined us-ing either function application or function com-position to give the original lambda expressionh.
An example split of the lambda expressionh = a(x, cookie(x)) is the pair(?y.a(x, y(x)), ?x.cookie(x)), (9)where ?y.a(x, y(x)) applied to ?x.cookie(x) re-turns the original expression a(x, cookie(x)).Syntactic splitting assigns linear order and syn-tactic categories to the two lambda expressions fand g. The initial syntactic category X is split bya reversal of the CCG application combinators inSection 2.2 if f and g can be recombined to give237Syntactic Category Semantic Type Example PhraseSdcl ?ev, t?
I took it ` Sdcl :?e.took(i, it, e)St t I?m angry ` St :angry(i)Swh ?e, ?ev, t??
Who took it?
` Swh :?x?e.took(x, it, e)Sq ?ev, t?
Did you take it?
` Sq :?e.Q(take(you, it, e))N ?e, t?
cookie `N:?x.cookie(x)NP e John `NP:johnPP ?ev, t?
on John ` PP:?e.on(john, e)Figure 2: Atomic Syntactic Categories.h with function application:{(X/Y : f Y : g), (10)(Y : g : X\Y : f)|h = f(g)}or by a reversal of the CCG composition combi-nators if f and g can be recombined to give hwithfunction composition:{(X/Z : f Z/Y : g, (11)(Z\Y : g : X\Z : f)|h = ?x.f(g(x))}Unknown category names in the result of asplit (Y in (10) and Z in (11)) are labelled via afunctional mapping cat from semantic type T tosyntactic category:cat(T ) =??
?Atomic(T ) if T ?
Figure 2cat(T1)/cat(T2) if T = ?T1, T2?cat(T1)\cat(T2) if T = ?T1, T2???
?which uses the Atomic function illustratedin Figure 2 to map semantic-type to basic CCGsyntactic category.
As an example, the logicalsplit in (9) supports two CCG category splits, onefor each of the CCG application rules.
(NP/N :?y.a(x, y(x)), N :?x.cookie(x)) (12)(N :?x.cookie(x), NP\N :?y.a(x, y(x))) (13)The parse generation algorithm T uses the func-tion split to generate all CCG category pairs thatare an allowed split of an input category X :h:{(Cl :ml,Cr :mr)} = split(X :h),and then packs a chart representation of {t} in atop-down fashion starting with a single cell entryCm :m for the top node shared by all parses {t}.For the utterance and meaning in (1) the top parsenode, spanning the entire word-string, isS :have(you, another(x, cookie(x))).T cycles over all cell entries in increasingly smallspans and populates the chart with their splits.
Forany cell entry X :h spanning more than one wordT generates a set of pairs representing the splits ofX :h. For each split (Cl :ml,Cr :mr) and every bi-nary partition (wi:k, wk:j) of the word-span T cre-ates two new cell entries in the chart: (Cl :ml)i:kand (Cr :mr)k:j .Input : Sentence [w1, .
.
.
, wn], top node Cm :mOutput: Packed parse chart Ch containing {t}Ch = [ [{}1, .
.
.
, {}n]1, .
.
.
, [{}1, .
.
.
, {}n]n ]Ch[1][n?
1] = Cm :mfor i = n, .
.
.
, 2; j = 1 .
.
.
(n?
i) + 1 dofor X:h ?
Ch[j][i] dofor (Cl :ml,Cr :mr) ?
split(X:h) dofor k = 1, .
.
.
, i?
1 doCh[j][k]?
Cl :mlCh[j + k][i?
k]?
Cr :mrAlgorithm 1: Generating {t} with T .Algorithm 1 shows how the learner uses T togenerate a packed chart representation of {t} inthe chart Ch.
The function T massively overgen-erates parses for any given natural language.
Theprobabilistic parsing model introduced in Sec-tion 3 is used to choose the best parse from theovergenerated set.5 Training5.1 Parameter EstimationThe probabilistic model of the grammar describesa distribution over the observed training data X,latent variables S, and parameters ?.
The goal oftraining is to estimate the posterior distribution:p(S, ?|X) = p(S,X|?)p(?
)p(X)(14)which we do with online Variational Bayesian Ex-pectation Maximisation (oVBEM; Sato (2001),Hoffman et al(2010)).
oVBEM is an online238Bayesian extension of the EM algorithm thataccumulates observation pseudocounts na?b foreach of the productions a ?
b in the grammar.These pseudocounts define the posterior over pro-duction probabilities as follows:(?a?b1 , .
.
.
, ?a?b{k,... })) | X,S ?
(15)Dir(?H(b1) + na?b1 , .
.
.
,?
?j=k?H(bj) + na?bj )These pseudocounts are computed in two steps:oVBE-step For the training pair (si, {m}i)which supports the set of parses {t}, the expec-tation E{t}[a ?
b] of each production a ?
b iscalculated by creating a packed chart representa-tion of {t} and running the inside-outside algo-rithm.
This is similar to the E-step in standardEM apart from the fact that each production isscored with the current expectation of its parame-ter weight ?
?i?1a?b, where:?
?i?1a?b =e?(?aHa(a?b)+ni?1a?b)e?(?K{b?}
?aHa(a?b?)+ni?1a?b?)
(16)and ?
is the digamma function (Beal, 2003).oVBM-step The expectations from the oVBEstep are used to update the pseudocounts in Equa-tion 15 as follows,nia?b = ni?1a?b + ?i(N ?
E{t}[a?
b]?
ni?1a?b)(17)where ?i is the learning rate and N is the size ofthe dataset.5.2 The Training AlgorithmNow the training algorithm used to learn the lex-icon ?
and pseudocounts {na?b} can be defined.The algorithm, shown in Algorithm 2, passes overthe training data only once and one training in-stance at a time.
For each (si, {m}i) it uses thefunction T |{m}i| times to generate a set of con-sistent parses {t}?.
The lexicon is populated byusing the lex function to read all of the lexicalitems off from the derivations in each {t}?.
Inthe parameter update step, the training algorithmupdates the pseudocounts associated with each ofthe productions a ?
b that have ever been seenduring training according to Equation (17).Only non-zero pseudocounts are stored in ourmodel.
The count vector is expanded with a newentry every time a new production is used.
WhileInput : Corpus D = {(si, {m}i)|i = 1, .
.
.
, N},Function T , Semantics to syntactic cate-gory mapping cat, function lex to readlexical items off derivations.Output: Lexicon ?, Pseudocounts {na?b}.?
= {}, {t} = {}for i = 1, .
.
.
, N do{t}i = {}form?
?
{m}i doCm?
= cat(m?){t}?
= T (si,Cm?
:m?
){t}i = {t}i ?
{t}?, {t} = {t} ?
{t}??
= ?
?
lex ({t}?
)for a?
b ?
{t} donia?b = ni?1a?b + ?i(N ?
E{t}i [a?
b]?ni?1a?b)Algorithm 2: Learning ?
and {na?b}the parameter update step cycles over all produc-tions in {t} it is not neccessary to store {t}, justthe set of productions that it uses.6 Experimental Setup6.1 DataThe Eve corpus, collected by Brown (1973), con-tains 14, 124 English utterances spoken to a sin-gle child between the ages of 18 and 27 months.These have been hand annotated by Sagae et al(2004) with labelled syntactic dependency graphs.An example annotation is shown in Figure 3.While these annotations are designed to rep-resent syntactic information, the parent-child re-lationships in the parse can also be viewed as aproxy for the predicate-argument structure of thesemantics.
We developed a template based de-terministic procedure for mapping this predicate-argument structure onto logical expressions of thetype discussed in Section 2.1.
For example, thedependency graph in Figure 3 is automaticallytransformed into the logical expression?e.have(you,another(y, cookie(y)), e) (18)?
on(the(z, table(z)), e),where e is a Davidsonian event variable used todeal with adverbial and prepositional attachments.The deterministic mapping to logical expressionsuses 19 templates, three of which are used in thisexample: one for the verb and its arguments, onefor the prepositional attachment and one (usedtwice) for the quantifier-noun constructions.239SUBJ ROOT DET OBJ JCT DET POBJpro|you v|have qn|another n|cookie prep|on det|the n|tableYou have another cookie on the tableFigure 3: Syntactic dependency graph from Eve corpus.This mapping from graph to logical expressionmakes use of a predefined dictionary of allowed,typed, logical constants.
The mapping is success-ful for 31% of the child-directed utterances in theEve corpus3.
The remaining data is mostly ac-counted for by one-word utterances that have nostraightforward interpretation in our typed logi-cal language (e.g.
what; okay; alright; no; yeah;hmm; yes; uhhuh; mhm; thankyou), missing ver-bal arguments that cannot be properly guessedfrom the context (largely in imperative sentencessuch as drink the water), and complex noun con-structions that are hard to match with a small setof templates (e.g.
as top to a jar).
We also re-move the small number of utterances containingmore than 10 words for reasons of computationalefficiency (see discussion in Section 8).Following Alishahi and Stevenson (2010), wegenerate a context set {m}i for each utterance siby pairing that utterance with its correct logicalexpression along with the logical expressions ofthe preceding and following (|{m}i|?1)/2 utter-ances.6.2 Base Distributions and Learning RateEach of the production heads a in the grammarrequires a base distribution Ha and concentrationparameter ?a.
For word-productions the base dis-tribution is a geometric distribution over characterstrings and spaces.
For syntactic-productions thebase distribution is defined in terms of the newcategory to be named by cat and the probabilityof splitting the rule by reversing either the appli-cation or composition combinators.Semantic-productions?
base distributions aredefined by a probabilistic branching process con-ditioned on the type of the syntactic category.This distribution prefers less complex logical ex-pressions.
All concentration parameters are set to1.0.
The learning rate for parameter updates is?i = (0.8 + i)?0.5.3Data available at www.tomkwiat.com/resources.html0.0 0.2 0.4 0.6 0.8 1.0Proportion of Data Seen0.00.10.20.30.40.50.60.70.8AccuracyOur ApproachOur Approach + Guess UBL1UBL10Figure 4: Meaning Prediction: Train on files 1, .
.
.
, ntest on file n+ 1.7 Experiments7.1 Parsing Unseen SentencesWe test the parsing model that is learnt by trainingon the first i files of the longitudinally ordered Evecorpus and testing on file i + 1, for i = 1 .
.
.
19.For each utterance s?
in the test file we use theparsing model to predict a meaning m?
and com-pare this to the target meaning m?.
We report theproportion of utterances for which the predictionm?
is returned correctly both with and withoutword-meaning guessing.
When a word has neverbeen seen at training time our parser has the abil-ity to ?guess?
a typed logical meaning with place-holders for constant and predicate names.For comparison we use the UBL semanticparser of Kwiatkowski et al(2010) trained ina similar setting?i.e., with no language specificinitialisation4.
Figure 4 shows accuracy for ourapproach with and without guessing, for UBL4Kwiatkowski et al(2010) initialise lexical weights intheir learning algorithm using corpus-wide alignment statis-tics across words and meaning elements.
Instead we runUBL with small positive weight for all lexical items.
Whenrun with Giza++ parameter initialisations, UBL10 achieves48.1% across folds compared to 49.2% for our approach.240when run over the training data once (UBL1) andfor UBL when run over the training data 10 times(UBL10) as in Kwiatkowski et al(2010).
Eachof the points represents accuracy on one of the19 test files.
All of these results are from parserstrained on utterances paired with a single candi-date meaning.
The lines of best fit show the up-ward trend in parser performance over time.Despite only seeing each training instanceonce, our approach, due to its broader lexi-cal search strategy, outperforms both versions ofUBL which performs a greedy search in the spaceof lexicons and requires initialisation with co-occurence statistics between words and logicalconstants to guide this search.
These statistics arenot justified in a model of language acquisitionand so they are not used here.
The low perfor-mance of all systems is due largely to the sparsityof the data with 32.9% of all sentences containinga previously unseen word.7.2 Word LearningDue to the sparsity of the data, the training algo-rithm needs to be able to learn word-meanings onthe basis of very few exposures.
This is also a de-sirable feature from the perspective of modellinglanguage acquisition as Carey and Bartlett (1978)have shown that children have the ability to learnword meanings on the basis of one, or very few,exposures through the process of fast mapping.0 500 1000 1500 20000.00.20.40.60.81.0P(m|w) 1 Meaning0 500 1000 1500 20003 Meanings0 500 1000 1500 2000Number of Utterances0.00.20.40.60.81.0P(m|w) 5 Meanings0 500 1000 1500 2000Number of Utterances7 Meaningsf = 168 a?
?f.a(x, f (x))f = 10 another?
?f.another(x, f (x))f = 2 any?
?f.any(x, f (x))Figure 5: Learning quantifiers with frequency f.Figure 5 shows the posterior probability of thecorrect meanings for the quantifiers ?a?, ?another?and ?any?
over the course of training with 1, 3,5 and 7 candidate meanings for each utterance5.These three words are all of the same class buthave very different frequencies in the trainingsubset shown (168, 10 and 2 respectively).
In alltraining settings, the word ?a?
is learnt graduallyfrom many observations but the rarer words ?an-other?
and ?any?
are learnt (when they are learnt)through large updates to the posterior on the ba-sis of few observations.
These large updates re-sult from a syntactic bootstrapping effect (Gleit-man, 1990).
When the model has great confidenceabout the derivation in which an unseen lexicalitem occurs, the pseudocounts for that lexical itemget a large update under Equation 17.
This largeupdate has a greater effect on rare words whichare associated with small amounts of probabilitymass than it does on common ones that have al-ready accumulated large pseudocounts.
The fastlearning of rare words later in learning correlateswith observations of word learning in children.7.3 Word Order LearningFigure 6 shows the posterior probability of thecorrect SVO word order learnt from increasingamounts of training data.
This is calculated bysumming over all lexical items containing transi-tive verb semantics and sampling in the space ofparse trees that could have generated them.
Withno propositional uncertainty in the training datathe correct word order is learnt very quickly andstabilises.
As the amount of propositional uncer-tainty increases, the rate at which this rule is learntdecreases.
However, even in the face of ambigu-ous training data, the model can learn the cor-rect word-order rule.
The distribution over wordorders also exhibits initial uncertainty, followedby a sharp convergence to the correct analysis.This ability to learn syntactic regularities abruptlymeans that our system is not subject to the crit-icisms that Thornton and Tesan (2007) levelledat statistical models of language acquisition?thattheir learning rates are too gradual.5The term ?fast mapping?
is generally used to refer tonoun learning.
We chose to examine quantifier learning hereas there is a greater variation in quantifier frequencies.
Fastmapping of nouns is also achieved.2410 500 1000 1500 2000Number of Utterances7 Meanings0 500 1000 1500 2000Number of Utterances0.00.20.40.60.81.0P(wordorder)5 Meanings 0 500 1000 1500 20003 Meanings0 500 1000 1500 20000.00.20.40.60.81.0P(wordorder)1 Meaningvsosvo ovssov vososvFigure 6: Learning SVO word order.8 DiscussionWe have presented an incremental model of lan-guage acquisition that learns a probabilistic CCGgrammar from utterances paired with one ormore potential meanings.
The model assumesno language-specific knowledge, but does assumethat the learner has access to language-universalcorrespondences between syntactic and semantictypes, as well as a Bayesian prior encouraginggrammars with heavy reuse of existing rules andlexical items.
We have shown that this modelnot only outperforms a state-of-the-art semanticparser, but also exhibits learning curves similarto children?s: lexical items can be acquired on asingle exposure and word order is learnt suddenlyrather than gradually.Although we use a Bayesian model, our ap-proach is different from many of the Bayesianmodels proposed in cognitive science and lan-guage acquisition (Xu and Tenenbaum, 2007;Goldwater et al 2009; Frank et al 2009; Grif-fiths and Tenenbaum, 2006; Griffiths, 2005; Per-fors et al 2011).
These models are intendedas ideal observer analyses, demonstrating whatwould be learned by a probabilistically optimallearner.
Our learner uses a more cognitively plau-sible but approximate online learning algorithm.In this way, it is similar to other cognitively plau-sible approximate Bayesian learners (Pearl et al2010; Sanborn et al 2010; Shi et al 2010).Of course, despite the incremental nature of ourlearning algorithm, there are still many aspectsthat could be criticized as cognitively implausi-ble.
In particular, it generates all parses consistentwith each training instance, which can be bothmemory- and processor-intensive.
It is unlikelythat children do this once they have learnt at leastsome of the target language.
In future, we planto investigate more efficient parameter estimationmethods.
One possibility would be an approxi-mate oVBEM algorithm in which the expectationsin Equation 17 are calculated according to a highprobability subset of the parses {t}.
Another op-tion would be particle filtering, which has beeninvestigated as a cognitively plausible method forapproximate Bayesian inference (Shi et al 2010;Levy et al 2009; Sanborn et al 2010).As a crude approximation to the context inwhich an utterance is heard, the logical represen-tations of meaning that we present to the learnerare also open to criticism.
However, Steedman(2002) argues that children do have access tostructured meaning representations from a mucholder apparatus used for planning actions and wewish to eventually ground these in sensory input.Despite the limitations listed above, our ap-proach makes several important contributions tothe computational study of language acquisition.It is the first model to learn syntax and seman-tics concurrently; previous systems (Villavicen-cio, 2002; Buttery, 2006) learnt categorial gram-mars from sentences where all word meaningswere known.
Our model is also the first to beevaluated by parsing sentences onto their mean-ings, in contrast to the work mentioned above andthat of Gibson and Wexler (1994), Siskind (1992)Sakas and Fodor (2001), and Yang (2002).
Theseall evaluate their learners on the basis of a smallnumber of predefined syntactic parameters.Finally, our work addresses a misunderstand-ing about statistical learners?that their learn-ing curves must be gradual (Thornton and Tesan,2007).
By demonstrating sudden learning of wordorder and fast mapping, our model shows that sta-tistical learners can account for sudden changes inchildren?s grammars.
In future, we hope to extendthese results by examining other learning behav-iors and testing the model on other languages.9 AcknowledgementsWe thank Mark Johnson for suggesting an analy-sis of learning rates.
This work was funded by theERC Advanced Fellowship 24952 GramPlus andEU IP grant EC-FP7-270273 Xperience.242ReferencesAlishahi and Stevenson, S. (2008).
A computa-tional model for early argument structure ac-quisition.
Cognitive Science, 32:5:789?834.Alishahi, A. and Stevenson, S. (2010).
Learninggeneral properties of semantic roles from usagedata: a computational model.
Language andCognitive Processes, 25:1.Beal, M. J.
(2003).
Variational algorithms for ap-proximate Bayesian inference.
Technical re-port, Gatsby Institute, UCL.Bo?rschinger, B., Jones, B. K., and Johnson, M.(2011).
Reducing grounded learning tasksto grammatical inference.
In Proceedings ofthe 2011 Conference on Empirical Methodsin Natural Language Processing, pages 1416?1425, Edinburgh, Scotland, UK.
Associationfor Computational Linguistics.Brown, R. (1973).
A First Language: the EarlyStages.
Harvard University Press, CambridgeMA.Buttery, P. J.
(2006).
Computational models forfirst language acquisition.
Technical ReportUCAM-CL-TR-675, University of Cambridge,Computer Laboratory.Carey, S. and Bartlett, E. (1978).
Acquring a sin-gle new word.
Papers and Reports on ChildLanguage Development, 15.Chen, D. L., Kim, J., and Mooney, R. J.
(2010).Training a multilingual sportscaster: Using per-ceptual context to learn language.
J. Artif.
In-tell.
Res.
(JAIR), 37:397?435.Fazly, A., Alishahi, A., and Stevenson, S. (2010).A probabilistic computational model of cross-situational word learning.
Cognitive Science,34(6):1017?1063.Frank, M., Goodman, S., and Tenenbaum, J.(2009).
Using speakers referential intentionsto model early cross-situational word learning.Psychological Science, 20(5):578?585.Frank, M. C., Goodman, N. D., and Tenenbaum,J.
B.
(2008).
A bayesian framework for cross-situational word-learning.
Advances in NeuralInformation Processing Systems 20.Gibson, E. and Wexler, K. (1994).
Triggers.
Lin-guistic Inquiry, 25:355?407.Gleitman, L. (1990).
The structural sources ofverb meanings.
Language Acquisition, 1:1?55.Goldwater, S., Griffiths, T. L., and Johnson, M.(2009).
A Bayesian framework for word seg-mentation: Exploring the effects of context.Cognition, 112(1):21?54.Griffiths, T. L., .
T. J.
B.
(2005).
Structure andstrength in causal induction.
Cognitive Psy-chology, 51:354?384.Griffiths, T. L. and Tenenbaum, J.
B.
(2006).
Op-timal predictions in everyday cognition.
Psy-chological Science.Hoffman, M., Blei, D. M., and Bach, F. (2010).Online learning for latent dirichlet alcation.In NIPS.Kate, R. J. and Mooney, R. J.
(2007).
Learninglanguage semantics from ambiguous supervi-sion.
In Proceedings of the 22nd Conferenceon Artificial Intelligence (AAAI-07).Kwiatkowski, T., Zettlemoyer, L., Goldwater, S.,and Steedman, M. (2010).
Inducing proba-bilistic CCG grammars from logical form withhigher-order unification.
In Proceedings of theConference on Emperical Methods in NaturalLanguage Processing.Kwiatkowski, T., Zettlemoyer, L., Goldwater, S.,and Steedman, M. (2011).
Lexical general-ization in ccg grammar induction for semanticparsing.
In Proceedings of the Conference onEmperical Methods in Natural Language Pro-cessing.Levy, R., Reali, F., and Griffiths, T. (2009).
Mod-eling the effects of memory on human onlinesentence processing with particle filters.
In Ad-vances in Neural Information Processing Sys-tems 21.Lu, W., Ng, H. T., Lee, W. S., and Zettlemoyer,L.
S. (2008).
A generative model for parsingnatural language to meaning representations.
InProceedings of The Conference on EmpiricalMethods in Natural Language Processing.MacWhinney, B.
(2000).
The CHILDES project:tools for analyzing talk.
Lawrence Erlbaum,Mahwah, NJ u.a.
EN.Maurits, L., Perfors, A., and Navarro, D. (2009).Joint acquisition of word order and word refer-ence.
In Proceedings of the 31th Annual Con-ference of the Cognitive Science Society.Pearl, L., Goldwater, S., and Steyvers, M. (2010).How ideal are we?
Incorporating human limi-243tations into Bayesian models of word segmen-tation.
pages 315?326, Somerville, MA.
Cas-cadilla Press.Perfors, A., Tenenbaum, J.
B., and Regier, T.(2011).
The learnability of abstract syntacticprinciples.
Cognition, 118(3):306 ?
338.Sagae, K., MacWhinney, B., and Lavie, A.(2004).
Adding syntactic annotations to tran-scripts of parent-child dialogs.
In Proceed-ings of the 4th International Conference onLanguage Resources and Evaluation.
Lisbon,LREC.Sakas, W. and Fodor, J. D. (2001).
The struc-tural triggers learner.
In Bertolo, S., editor,Language Acquisition and Learnability, pages172?233.
Cambridge University Press, Cam-bridge.Sanborn, A. N., Griffiths, T. L., and Navarro,D.
J.
(2010).
Rational approximations to ratio-nal models: Alternative algorithms for categorylearning.
Psychological Review.Sato, M. (2001).
Online model selection basedon the variational bayes.
Neural Computation,13(7):1649?1681.Shi, L., Griffiths, T. L., Feldman, N. H., and San-born, A. N. (2010).
Exemplar models as amechanism for performing bayesian inference.Psychonomic Bulletin & Review, 17(4):443?464.Siskind, J. M. (1992).
Naive Physics, Event Per-ception, Lexical Semantics, and Language Ac-quisition.
PhD thesis, Massachusetts Instituteof Technology.Siskind, J. M. (1996).
A computational study ofcross-situational techniques for learning word-to-meaning mappings.
Cognition, 61(1-2):1?38.Steedman, M. (2000).
The Syntactic Process.MIT Press, Cambridge, MA.Steedman, M. (2002).
Plans, affordances, andcombinatory grammar.
Linguistics and Philos-ophy, 25.Thornton, R. and Tesan, G. (2007).
Categori-cal acquisition: Parameter setting in universalgrammar.
Biolinguistics, 1.Villavicencio, A.
(2002).
The acquisition of aunification-based generalised categorial gram-mar.
Technical Report UCAM-CL-TR-533,University of Cambridge, Computer Labora-tory.Wong, Y. W. and Mooney, R. (2006).
Learning forsemantic parsing with statistical machine trans-lation.
In Proceedings of the Human LanguageTechnology Conference of the NAACL.Wong, Y. W. and Mooney, R. (2007).
Learn-ing synchronous grammars for semantic pars-ing with lambda calculus.
In Proceedings ofthe Association for Computational Linguistics.Xu, F. and Tenenbaum, J.
B.
(2007).
Word learn-ing as Bayesian inference.
Psychological Re-view, 114:245?272.Yang, C. (2002).
Knowledge and Learning in Nat-ural Language.
Oxford University Press, Ox-ford.Yu, C. and Ballard, D. H. (2007).
A unified modelof early word learning: Integrating statisti-cal and social cues.
Neurocomputing, 70(13-15):2149 ?
2165.Zettlemoyer, L. S. and Collins, M. (2005).
Learn-ing to map sentences to logical form: Struc-tured classification with probabilistic categorialgrammars.
In Proceedings of the Conference onUncertainty in Artificial Intelligence.Zettlemoyer, L. S. and Collins, M. (2007).
Onlinelearning of relaxed CCG grammars for pars-ing to logical form.
In Proc.
of the Joint Con-ference on Empirical Methods in Natural Lan-guage Processing and Computational NaturalLanguage Learning.Zettlemoyer, L. S. and Collins, M. (2009).
Learn-ing context-dependent mappings from sen-tences to logical form.
In Proceedings of TheJoint Conference of the Association for Com-putational Linguistics and International JointConference on Natural Language Processing.244
