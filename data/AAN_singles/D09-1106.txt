Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1017?1026,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPWeighted Alignment Matrices for Statistical Machine TranslationYang Liu , Tian Xia , Xinyan Xiao and Qun LiuKey Laboratory of Intelligent Information ProcessingInstitute of Computing TechnologyChinese Academy of SciencesP.O.
Box 2704, Beijing 100190, China{yliu,xiatian,xiaoxinyan,liuqun}@ict.ac.cnAbstractCurrent statistical machine translation sys-tems usually extract rules from bilingualcorpora annotated with 1-best alignments.They are prone to learn noisy rules dueto alignment mistakes.
We propose a newstructure called weighted alignment matrixto encode all possible alignments for a par-allel text compactly.
The key idea is to as-sign a probability to each word pair to in-dicate how well they are aligned.
We de-sign new algorithms for extracting phrasepairs from weighted alignment matricesand estimating their probabilities.
Our ex-periments on multiple language pairs showthat using weighted matrices achieves con-sistent improvements over using n-bestlists in significant less extraction time.1 IntroductionStatistical machine translation (SMT) relies heav-ily on annotated bilingual corpora.
Word align-ment, which indicates the correspondence be-tween the words in a parallel text, is one of themost important annotations in SMT.
Word-alignedcorpora have been found to be an excellent sourcefor translation-related knowledge, not only forphrase-based models (Och and Ney, 2004; Koehnet al, 2003), but also for syntax-based models(e.g., (Chiang, 2007; Galley et al, 2006; Shenet al, 2008; Liu et al, 2006)).
Och and Ney(2003) indicate that the quality of machine transla-tion output depends directly on the quality of ini-tial word alignment.Modern alignment methods can be divided intotwo major categories: generative methods and dis-criminative methods.
Generative methods (Brownet al, 1993; Vogel and Ney, 1996) treat wordalignment as a hidden process and maximize thelikelihood of bilingual training corpus using theexpectation maximization (EM) algorithm.
Incontrast, discriminative methods (e.g., (Moore etal., 2006; Taskar et al, 2005; Liu et al, 2005;Blunsom and Cohn, 2006)) have the freedom todefine arbitrary feature functions that describe var-ious characteristics of an alignment.
They usu-ally optimize feature weights on manually-aligneddata.
While discriminative methods show supe-rior alignment accuracy in benchmarks, genera-tive methods are still widely used to produce wordalignments for large sentence-aligned corpora.However, neither generative nor discriminativealignment methods are reliable enough to yieldhigh quality alignments for SMT, especially fordistantly-related language pairs such as Chinese-English and Arabic-English.
The F-measures forChinese-English and Arabic-English are usuallyaround 80% (Liu et al, 2005) and 70% (Fraserand Marcu, 2007), respectively.
As most currentSMT systems only use 1-best alignments for ex-tracting rules, alignment errors might impair trans-lation quality.Recently, several studies have shown that offer-ing more alternatives of annotations to SMT sys-tems will result in significant improvements, suchas replacing 1-best trees with packed forests (Miet al, 2008) and replacing 1-best word segmenta-tions with word lattices (Dyer et al, 2008).
Sim-ilarly, Venugopal et al (2008) use n-best align-ments instead of 1-best alignments for translationrule extraction.
While they achieve significant im-provements on the IWSLT data, extracting rulesfrom n-best alignments might be computationallyexpensive.In this paper, we propose a new structure namedweighted alignment matrix to represent the align-ment distribution for a sentence pair compactly.
Ina weighted matrix, each element that correspondsto a word pair is assigned a probability to measurethe confidence of aligning the two words.
There-fore, a weighted matrix is capable of using a lin-1017thedevelopmentofChina?seconomyzhongguodejingjifazhanFigure 1: An example of word alignment betweena pair of Chinese and English sentences.ear space to encode the probabilities of exponen-tially many alignments.
We develop a new algo-rithm for extracting phrase pairs from weightedmatrices and show how to estimate their relativefrequencies and lexical weights.
Experimental re-sults show that using weighted matrices achievesconsistent improvements in translation quality andsignificant reduction in extraction time over usingn-best lists.2 BackgroundFigure 1 shows an example of word alignment be-tween a pair of Chinese and English sentences.The Chinese and English words are listed horizon-tally and vertically, respectively.
The dark pointsindicate the correspondence between the words intwo languages.
For example, the first Chineseword ?zhongguo?
is aligned to the fourth Englishword ?China?.Formally, given a source sentence f = fJ1=f1, .
.
.
, fj, .
.
.
, fJand a target sentence e = eI1=e1, .
.
.
, ei, .
.
.
, eI, we define a link l = (j, i) toexist if fjand eiare translation (or part of trans-lation) of one another.
Then, an alignment a is asubset of the Cartesian product of word positions:a ?
{(j, i) : j = 1, .
.
.
, J ; i = 1, .
.
.
, I} (1)Usually, SMT systems only use the 1-best align-ments for extracting translation rules.
For exam-ple, given a source phrase ?f and a target phrasee?, the phrase pair ( ?f , e?)
is said to be consistent(Och and Ney, 2004) with the alignment if andonly if: (1) there must be at least one word in-side one phrase aligned to a word inside the otherphrase and (2) no words inside one phrase can bealigned to a word outside the other phrase.After all phrase pairs are extracted from thetraining corpus, their translation probabilities canbe estimated as relative frequencies (Och and Ney,2004):?
(e?|?f) =count(?f, e?)?e?
?count(?f , e??
)(2)where count( ?f , e?)
indicates how often the phrasepair ( ?f, e?)
occurs in the training corpus.Besides relative frequencies, lexical weights(Koehn et al, 2003) are widely used to estimatehow well the words in ?f translate the words ine?.
To do this, one needs first to estimate a lexi-cal translation probability distribution w(e|f) byrelative frequency from the same word alignmentsin the training corpus:w(e|f) =count(f, e)?e?count(f, e?
)(3)Note that a special source NULL token is addedto each source sentence and aligned to each un-aligned target word.As the alignment a?
between a phrase pair ( ?f, e?
)is retained during extraction, the lexical weightcan be calculated aspw(e?|?f, a?)
=|e?|?i=11|{j|(j, i) ?
a?
}|?w(ei|fj) (4)If there are multiple alignments a?
for a phrasepair ( ?f , e?
), Koehn et al (2003) choose the onewith the highest lexical weight:pw(e?|?f) = maxa?
{pw(e?|?f, a?
)}(5)Simple and effective, relative frequencies andlexical weights have become the standard featuresin modern discriminative SMT systems.3 Weighted Alignment MatrixWe believe that offering more candidate align-ments to extracting translation rules might helpimprove translation quality.
Instead of using n-best lists (Venugopal et al, 2008), we propose anew structure called weighted alignment matrix.We use an example to illustrate our idea.
Fig-ure 2(a) and Figure 2(b) show two alignments ofa Chinese-English sentence pair.
We observe thatsome links (e.g., (1,4) corresponding to the word1018thedevelopmentofChina?seconomyzhongguodejingjifazhanthedevelopmentofChina?seconomyzhongguodejingjifazhanthedevelopmentofChina?seconomyzhongguodejingjifazhan1.00.60.40.41.01.00.400000000000000000(a) (b) (c)Figure 2: (a) One alignment of a sentence pair; (b) another alignment of the same sentence pair; (c)the resulting weighted alignment matrix that takes the two alignments as samples, of which the initialprobabilities are 0.6 and 0.4, respectively.pair (?zhongguo?, ?China?))
occur in both align-ments, some links (e.g., (2,3) corresponding to theword pair (?de?,?of?))
occur only in one align-ment, and some links (e.g., (1,1) correspondingto the word pair (?zhongguo?, ?the?))
do not oc-cur.
Intuitively, we can estimate how well twowords are aligned by calculating its relative fre-quency, which is the probability sum of align-ments in which the link occurs divided by theprobability sum of all possible alignments.
Sup-pose that the probabilities of the two alignments inFigures 2(a) and 2(b) are 0.6 and 0.4, respectively.We can estimate the relative frequencies for everyword pair and obtain a weighted matrix shown inFigure 2(c).
Therefore, each word pair is associ-ated with a probability to indicate how well theyare aligned.
For example, in Figure 2(c), we saythat the word pair (?zhongguo?, ?China?)
is def-initely aligned, (?zhongguo?, ?the?)
is definitelyunaligned, and (?de?, ?of?)
has a 60% chance toget algned.Formally, a weighted alignment matrix m is aJ ?
I matrix, in which each element stores a linkprobability pm(j, i) to indicate how well fjandeiare aligned.
Currently, we estimate link proba-bilities from an n-best list by calculating relativefrequencies:pm(j, i) =?a?Np(a)?
?
(a, j, i)?a?Np(a)(6)=?a?Np(a)?
?
(a, j, i) (7)where?
(a, j, i) ={1 (j, i) ?
a0 otherwise (8)Note that N is an n-best list, p(a) is the probabil-ity of an alignment a in the n-best list, ?
(a, j, i)indicates whether a link (j, i) occurs in the align-ment a or not.
We assign 0 to any unseenalignment.
As p(a) is usually normalized (i.e.,?a?Np(a) ?
1), we remove the denominator inEq.
(6).Accordingly, the probability that the two wordsfjand eiare not aligned isp?m(j, i) = 1.0?
pm(j, i) (9)For example, as shown in Figure 2(c), the prob-ability for the two words ?de?
and ?of?
beingaligned is 0.6 and the probability that they are notaligned is 0.4.Intuitively, the probability of an alignment a isthe product of link probabilities.
If a link (j, i)occurs in a, we use pm(j, i); otherwise we usep?m(j, i).
Formally, given a weighted alignmentmatrix m, the probability of an alignment a canbe calculated aspm(a) =J?j=1I?i=1(pm(j, i) ?
?
(a, j, i) +p?m(j, i) ?
(1?
?
(a, j, i))) (10)It proves that the sum of all alignment proba-bilities is always 1:?a?Apm(a) ?
1, where A10191: procedure PHRASEEXTRACT(fJ1, eI1, m, l)2: R ?
?3: for j1?
1 .
.
.
J do4: j2?
j15: while j2< J ?
j2?
j1< l do6: T ?
{i|?j : j1?
j ?
j2?
pm(j, i) > 0}7: il?
MIN(T )8: iu?
MAX(T )9: for n?
1 .
.
.
l do10: for i1?
il?
n + 1 .
.
.
iudo11: i2?
i1+ n?
112: R ?
R?
{(f j2j1, ei2i1)}13: end for14: end for15: j2?
j2+ 116: end while17: end for18: returnR19: end procedureFigure 3: Algorithm for extracting phrase pairsfrom a sentence pair ?fJ1, eI1?
annotated with aweighted alignment matrix m.is the set of all possible alignments.
Therefore, aweighted alignment matrix is capable of encodingthe probabilities of 2J?I alignments using only aJ ?
I space.Note that pm(a) is not necessarily equal to p(a)because the encoding of a weighted alignment ma-trix changes the alignment probability distribu-tion.
For example, while the initial probability ofthe alignment in Figure 2(a) (i.e., p(a)) is 0.6, theprobability of the same alignment encoded in thematrix shown in Figure 2(c) (i.e., pm(a)) becomes0.1296 according to Eq.
(10).
It should be em-phasized that a weighted matrix encodes all pos-sible alignments rather than the input n-best list,although the link probabilities are estimated fromthe n-best list.4 Phrase Pair ExtractionIn this section, we describe how to extract phrasepairs from the training corpus annotated withweighted alignment matrices (Section 4.1) andhow to estimate their relative frequencies (Section4.2) and lexical weights (Section 4.3).4.1 Extraction AlgorithmOch and Ney (2004) describe a ?phrase-extract?algorithm for extracting phrase pairs from a sen-tence pair annotated with a 1-best alignment.Given a source phrase, they first identify the targetphrase that is consistent with the alignment.
Then,they expand the boundaries of the target phrase ifthe boundary words are unaligned.Unfortunately, this algorithm cannot be directlyused to manipulate a weighted alignment matrix,which is a compact representation of all pos-sible alignments.
The major difference is thatthe ?tight?
phrase that has both boundary wordsaligned is not necessarily the smallest candidatein a weighted matrix.
For example, in Figure2(a), the ?tight?
target phrase corresponding tothe source phrase ?zhongguo de?
is ?of China?.According to Och?s algorithm, the target phrase?China?
breaks the alignment consistency andtherefore is not valid candidate.
However, this isnot true for using the weighted matrix shown inFigure 2(c).
The target phrase ?China?
is treatedas a ?potential?
candidate 1, although it might beassigned only a small fractional count (see Table1).Therefore, we enumerate all potential phrasepairs and calculate their fractional counts foreliminating less promising candidates.
Figure 3shows the algorithm for extracting phrases froma weighted matrix.
The input of the algorithmis a source sentence fJ1, a target sentence eI1, aweighted alignment matrix m, and a phrase lengthlimit l (line 1).
After initializing R that stores col-lected phrase pairs (line 2), we identify the cor-responding target phrases for all possible sourcephrases (lines 3-5).
Given a source phrase f j2j1, wefind the lower and upper bounds of target positions(i.e., iland iu) that have positive link probabili-ties (lines 6-8).
For example, the lower bound is3 and the upper bound is 5 for the source phrase?zhongguo de?
in Figure 2(c).
Finally, we enu-merate all target phrases that allow for unalignedboundary words with varying phrase lengths (lines9-14).
Note that we need to ensure that 1 ?
i1?
Iand 1 ?
i2?
I in lines 10-11, which are omittedfor simplicity.4.2 Calculating Relative FrequenciesTo estimate the relative frequency of a phrase pair,we need to estimate how often it occurs in thetraining corpus.
Given an n-best list, the fractionalcount of a phrase pair is the probability sum ofthe alignments with which the phrase pair is con-sistent.
Obviously, it is unrealistic for a weightedalignment matrix to enumerate all possible align-ments explicitly to calculate fractional counts.
In-stead, we resort to link probabilities to calculate1By potential, we mean that the fractional count of aphrase pair is positive.
Section 4.2 describes how to calcu-late fractional counts.1020thedevelopmentofChina?seconomyzhongguodejingjifazhan1.00.60.40.41.01.00.400000000000000000Figure 4: An example of calculating fractionalcount.
Given the phrase pair (?zhongguo de?, ?ofChina?
), we divide the matrix into three areas: in-side (heavy shading), outside (light shading), andirrelevant (no shading).counts efficiently.
Equivalent to explicit enumera-tion, we interpret the fractional count of a phrasepair as the probability that it satisfies the two align-ment consistency conditions (see Section 2).Given a phrase pair, we divide the elements ofa weighted alignment matrix into three categories:(1) inside elements that fall inside the phrase pair,(2) outside elements that fall outside the phrasepair while fall in the same row or the same col-umn, and (3) irrelevant elements that fall outsidethe phrase pair while fall in neither the same rownor the same column.
Figure 4 shows an exam-ple.
Given the phrase pair (?zhongguo de?, ?ofChina?
), we divide the matrix into three areas: in-side (heavy shading), outside (light shading), andirrelevant (no shading).To what extent a phrase pair satisfies the align-ment consistency is measured by calculating in-side and outside probabilities.
Although there arethe same terms in the parsing literature, they havedifferent meanings here.
The inside probability in-dicates the chance that there is at least one wordinside one phrase aligned to a word inside theother phrase.
The outside probability indicates thechance that no words inside one phrase are alignedto a word outside the other phrase.Given a phrase pair (f j2j1, ei2i1), we denote the in-side area as in(j1, j2, i1, i2) and the outside areaas out(j1, j2, i1, i2).
Therefore, the inside proba-bility of a phrase pair is calculated as?
(j1, j2, i1, i2) = 1??
(j,i)?in(j1,j2,i1,i2)p?m(j, i) (11)target phrase ?
?
countof China 1.0 0.36 0.36of China ?s 1.0 0.36 0.36China ?s 1.0 0.24 0.24China 1.0 0.24 0.24?s economy 0.4 0 0Table 1: Some candidate target phrases of thesource phrase ?zhongguo de?
in Figure 4, where ?is inside probability, ?
is outside probability, andcount is fractional count.For example, the inside probability for (?zhong-guo de?, ?of China?)
in Figure 4 is 1.0, whichmeans that there always exists at least one alignedword pair inside.Accordingly, the outside probability of a phrasepair is calculated as?
(j1, j2, i1, i2) =?
(j,i)?out(j1,j2,i1,i2)p?m(j, i) (12)For example, the outside probability for(?zhongguo de?, ?of China?)
in Figure 4 is 0.36,which means the probability that there are noaligned word pairs outside is 0.36.Finally, we use the product of inside and outsideprobabilities as the fractional count of a phrasepair:count(fj2j1, ei2i1) = ?
(j1, j2, i1, i2)??
(j1, j2, i1, i2) (13)Table 1 lists some candidate target phrases ofthe source phrase ?zhongguo de?
in Figure 4.
Wealso give their inside probabilities, outside proba-bilities, and fractional counts.After collecting the fractional counts from thetraining corpus, we then use Eq.
(2) to calculaterelative frequencies in two translation directions.Often, our approach extracts a large amount ofphrase pairs from training corpus as we softenthe alignment consistency constraint.
To main-tain a reasonable phrase table size, we discard anyphrase pair that has a fractional count lower thana threshold t. During extraction, we first obtaina list of candidate target phrases for each sourcephrase, as shown in Table 1.
Then, we prune thelist according to the threshold t. For example, weonly retain the top two candidates in Table 1 ift = 0.3.
Note that we perform the pruning locally.Although it is more reasonable to prune a phrasetable after accumulating all fractional counts from1021training corpus, such global pruning strategy usu-ally leads to very large disk and memory require-ments.4.3 Calculating Lexical WeightsRecall that we need to obtain two translation prob-ability tables w(e|f) and w(f |e) before calculat-ing lexical weights (see Section 2).
FollowingKoehn et al (2003), we estimate the two distribu-tions by relative frequencies from the training cor-pus annotated with weighted alignment matrices.In other words, we still use Eq.
(3) but the way ofcalculating fractional counts is different now.Given a source word fj, a target word ei, anda weighted alignment matrix, the fractional countcount(fj, ei) is pm(j, i).
For NULL words, thefractional counts can be calculated ascount(fj, e0) =I?i=1p?m(j, i) (14)count(f0, ei) =J?j=1p?m(j, i) (15)For example, in Figure 4, count(de, of) is 0.6,count(de,NULL) is 0.24, and count(NULL,of) is0.24.Then, we adapt Eq.
(4) to calculate lexicalweight:pw(e?|?f ,m) =|e?|?i=1((1{j|pm(j, i) > 0}???j:pm(j,i)>0p(ei|fj)?
pm(j, i))+p(ei|f0)?|?f |?j=1p?m(j, i))(16)For example, for the target word ?of?
in Figure4, the sum of aligned and unaligned probabilitiesis12?
(p(of|de)?
0.6 + p(of|fazhan)?
0.4) +p(of|NULL)?
0.24Note that we take link probabilities into accountand calculate the probability that a target wordtranslates a source NULL token explicitly.5 Experiments5.1 Data PreparationWe evaluated our approach on Chinese-to-Englishtranslation.
We used the FBIS corpus (6.9M+ 8.9M words) as the training data.
For lan-guage model, we used the SRI Language Mod-eling Toolkit (Stolcke, 2002) to train a 4-grammodel on the Xinhua portion of GIGAWORD cor-pus.
We used the NIST 2002 MT evaluation testset as our development set, and used the NIST2005 test set as our test set.
We evaluated the trans-lation quality using case-insensitive BLEU metric(Papineni et al, 2002).To obtain weighted alignment matrices, we fol-lowed Venugopal et al (2008) to produce n-best lists via GIZA++.
We first ran GIZA++to produce 50-best lists in two translation direc-tions.
Then, we used the refinement technique?grow-diag-final-and?
(Koehn et al, 2003) to all50 ?
50 bidirectional alignment pairs.
Supposethat ps2tand pt2sare the probabilities of an align-ment pair assigned by GIZA++, respectively.
Weused ps2t?
pt2sas the probability of the result-ing symmetric alignment.
As different alignmentpairs might produce the same symmetric align-ments, we followed Venugopal et al (2008) toremove duplicate alignments and retain only thealignment with the highest probability.
Therefore,there were 550 candidate alignments on averagefor each sentence pair in the training data.
Weobtained n-best lists by selecting the top n align-ments from the 550-best lists.
The probability ofeach alignment in the n-best list was re-estimatedby re-normalization (Venugopal et al, 2008).
Fi-nally, these n-best alignments served as samplesfor constructing weighted alignment matrices.After extracting phrase pairs from n-best listsand weighted alignment matrices, we ran Moses(Koehn et al, 2007) to translate the developmentand test sets.
We used the simple distance-basedreordering model to remove the dependency oflexicalization on word alignments for Moses.5.2 Effect of Pruning ThresholdOur first experiment investigated the effect ofpruning threshold on translation quality (BLEUscores on the test set) and the phrase table size (fil-tered for the test set), as shown in Figure 5.
Tosave time, we extracted phrase pairs just from thefirst 10K sentence pairs of the FBIS corpus.
Weused 12 different thresholds: 0.0001, 0.001, 0.01,0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, and 0.9.
Obvi-ously, the lower the threshold is, the more phrasepairs are extracted.
When t = 0.0001, the numberof phrase pairs used on the test set was 460,28410220.1950.1960.1970.1980.1990.2000.2010.2020.2030.2040.2050.2060.2070.208150  200  250  300  350  400  450  500BLEUscorephrase table size (103)t=10-4t=10-3t=10-2t=0.9...0.1Figure 5: Effect of pruning threshold on transla-tion quality and phrase table size.and the BLEU score was 20.55.
Generally, boththe number of phrase pairs and the BLEU scorewent down with the increase of t. However, thistrend did not hold within the range [0.1, 0.9].
Toachieve a good tradeoff between translation qual-ity and phrase table size, we set t = 0.01 for thefollowing experiments.5.3 N -best lists Vs.
Weighted MatricesFigure 6 shows the BLEU scores and aver-age extraction time using n-best alignments andweighted matrices, respectively.
We used the en-tire training data for phrase extraction.
When us-ing 1-best alignments, Moses achieved a BLEUscore of 0.2826 and the average extraction timewas 4.19 milliseconds per sentence pair (see pointn = 1).
The BLEU scores rose with the in-crease of n for using n-best alignments.
How-ever, the score went down slightly when n = 50.This suggests that including more noisy align-ments might be harmful.
These improvementsover 1-best alignments are not statistically signif-icant.
This finding failed to echo the promisingresults reported by Venogopal et al (2008).
Wethink that there are two possible reasons.
First,they evaluated their approach on the IWSLT datawhile we used the NIST data.
It might be easierto obtain significant improvements on the IWSLTdata in which the sentences are shorter.
Sec-ond, they used the hierarchical phrase-based sys-tem while we used the phrase-based system, whichmight be less sensitive to word alignments becausethe alignments inside the phrase pairs hardly havean effect.When using weighted alignment matrices, we0.2800.2810.2820.2830.2840.2850.2860.2870.2880.2890.2900.2910.2920.2930  10  20  30  40  50  60  70  80  90BLEUscoreaverage extracting time (milliseconds/sentence pair)n=1n=5n=10n=50n=5n=10n=50n-bestm(n)Figure 6: Comparison of n-best alignments andweighted alignment matrices.
We use m(n) to de-note the matrices that take n-best lists as samples.obtained higher BLEU scores than using n-bestlists with much less extraction time.
We achieveda BLEU score of 0.2901 when using the weightedmatrices estimated from 10-best lists.
The abso-lute improvement of 0.75 over using 1-best align-ments (from 0.2826 to 0.2901) is statistically sig-nificant at p < 0.05 by using sign-test (Collinset al, 2005).
Although the improvements over n-best lists are not always statistically significant,weighted alignment matrices maintain consistentsuperiority in both translation quality and extrac-tion speed.5.4 Comparison of Parameter EstimationIn theory, the set of phrase pairs extracted from n-best alignments is the subset of the set extractedfrom the corresponding weighted matrices.
Inpractice, however, this is not true because we usethe pruning threshold t to maintain a reasonabletable size.
Even so, the phrase tables produced byn-best lists and weighted matrices still share manyphrase pairs.Table 2 gives some statistics.
We use m(10)to represent the weighted matrices estimated from10-best lists.
?all?
denotes the full phrase table,?shared?
denotes the intersection of two tables,and ?non-shared?
denotes the complement.
Notethat the probabilities of ?shared?
phrase pairs aredifferent for the two approaches.
We obtained6.13M and 6.34M phrase pairs for the test set byusing 10-best lists and the corresponding matrices,respectively.
There were 4.58M phrase pairs in-cluded by both tables.
Note that the relative fre-quencies and lexical weights for the same phrase1023shared non-shared allmethod phrases BLEU phrases BLEU phrases BLEU10-best 4.58M 28.35 1.55M 12.32 6.13M 28.47m(10) 4.58M 28.90 1.76M 13.21 6.34M 29.01Table 2: Comparison of phrase tables learned from n-best lists and weighted matrices.
We use m(10)to represent the weighted matrices estimated from 10-best lists.
?all?
denotes the full phrase table,?shared?
denotes the intersection of two tables, and ?non-shared?
denotes the complement.
Note that theprobabilities of ?shared?
phrase pairs are different for the two approaches.0.2000.2100.2200.2300.2400.2500.2600.2700.2800.2900  50  100  150  200  250BLEUscoretraining corpus size (103)1-best10-bestm(10)Figure 7: Comparison of n-best alignments andweighted alignment matrices with varying trainingcorpus sizes.pairs might be different in two tables.
We foundthat using matrices outperformed using n-best listseven with the same phrase pairs.
This suggests thatour methods for parameter estimation make betteruse of noisy data.
Another interesting finding wasthat using the shared phrase pairs achieved almostthe same results with using full phrase tables.5.5 Effect of Training Corpus SizeTo investigate the effect of training corpus size onour approach, we extracted phrase pairs from n-best lists and weighted matrices trained on fivetraining corpora with varying sizes: 10K, 50K,100K, 150K, and 239K sentence pairs.
As shownin Figure 7, our approach outperformed both 1-best and n-best lists consistently.
More impor-tantly, the gains seem increase when more trainingdata are used.5.6 Results on Other Language PairsTo further examine the efficacy of the proposed ap-proach, we scaled our experiments to large datawith multiple language pairs.
We used the Eu-roparl training corpus from the WMT07 sharedS?E F?E G?ESentences 1.26M 1.29M 1.26MForeign words 33.16M 33.18M 29.58MEnglish words 31.81M 32.62M 31.93MTable 3: Statistics of the Europarl training data.?S?
denotes Spanish, ?E?
denotes English, ?F?
de-notes French, ?G?
denotes German.1-best 10-best m(10)S?E 30.90 30.97 31.03E?S 31.16 31.25 31.34F?E 30.69 30.76 30.82E?F 26.42 26.65 26.54G?E 24.46 24.58 24.66E?G 18.03 18.30 18.20Table 4: BLEU scores (case-insensitive) on theEuroparl data.
?S?
denotes Spanish, ?E?
denotesEnglish, ?F?
denotes French, ?G?
denotes Ger-man.task.
2 Table 3 shows the statistics of the train-ing data.
There are four languages (Spanish,French, German, and English) and six transla-tion directions (Foreign-to-English and English-to-Foreign).
We used the ?dev2006?
data in the?dev?
directory as the development set and the?test2006?
data in the ?devtest?
directory as thetest set.
Both the development and test sets contain2,000 sentences with single reference translations.We tokenized and lowercased all the training,development, and test data.
We trained a 4-gramlanguage model using SRI Language ModelingToolkit on the target side of the training corpus foreach task.
We ran GIZA++ on the entire train-ing data to obtain n-best alignments and weightedmatrices.
To save time, we just used the first 100Ksentences of each aligned training corpus to ex-tract phrase pairs.2http://www.statmt.org/wmt07/shared-task.html1024Table 4 lists the case-insensitive BLEU scoresof 1-best, 10-best, and m(10) on the Europarldata.
Using weighted packed matrices continuedto show advantage over using 1-best alignments onmultiple language pairs.
However, these improve-ments were very small and not significant.
We at-tribute this to the fact that GIZA++ usually pro-duces high quality 1-best alignments for closely-related European language pairs, especially whentrained on millions of sentences.6 Related WorkRecent studies has shown that SMT systemscan benefit from making the annotation pipelinewider: using packed forests instead of 1-best trees(Mi et al, 2008), word lattices instead of 1-bestsegmentations (Dyer et al, 2008), and n-bestalignments instead of 1-best alignments (Venu-gopal et al, 2008).
We propose a compact repre-sentation of multiple word alignments that enablesSMT systems to make a better use of noisy align-ments.Matusov et al (2004) propose ?cost matrices?for producing symmetric alignments.
Kumar et al(2007) describe how to use ?posterior probabil-ity matrices?
to improve alignment accuracy viaa bridge language.
Although not using the term?weighted matrices?
directly, they both assign aprobability to each word pair.We follow Och and Ney (2004) to developa new phrase extraction algorithm for weightedalignment matrices.
The methods for calculatingrelative frequencies (Och and Ney, 2004) and lex-ical weights (Koehn et al, 2003) are also adaptedfor the weighted matrix case.Many researchers (e.g., (Venugopal et al, 2003;Deng et al, 2008)) observe that softening thealignment consistency constraint help improvetranslation quality.
For example, Deng et al(2008) define a feature named ?within phrase pairconsistency ratio?
to measure the degree of consis-tency.
As each link is associated with a probabilityin a weighted matrix, we use these probabilities toevaluate the validity of a phrase pair.We estimate the link probabilities by calculatingrelative frequencies over n-best lists.
Niehues andVogel (2008) propose a discriminative approach tomodeling the alignment matrix directly.
The dif-ference is that they assign a boolean value insteadof a probability to each word pair.7 Conclusion and Future WorkWe have presented a new structure called weightedalignment matrix that encodes the alignment dis-tribution for a sentence pair.
Accordingly, we de-velop new methods for extracting phrase pairs andestimating their probabilities.
Our experimentsshow that the proposed approach achieves bettertranslation quality over using n-best lists in lessextraction time.
An interesting finding is that ourapproach performs better than the baseline eventhey use the same phrase pairs.Although our approach consistently outper-forms using 1-best alignments for varying lan-guage pairs, the improvements are comparativelysmall.
One possible reason is that taking n-bestlists as samples sometimes might change align-ment probability distributions inappropriately.
Amore principled solution is to directly model theweighted alignment matrices, either in a genera-tive or a discriminative way.
We believe that betterestimation of alignment distributions will result inmore significant improvements.Another interesting direction is applying our ap-proach to extracting translation rules with hierar-chical structures such as hierarchical phrases (Chi-ang, 2007) and tree-to-string rules (Galley et al,2006; Liu et al, 2006).
We expect that thesesyntax-based systems could benefit more from ourapproach.AcknowledgementThe authors were supported by Microsoft Re-search Asia Natural Language Processing ThemeProgram grant (2009-2010), High-TechnologyR&D Program (863) Project No.
2006AA010108,and National Natural Science Foundation of ChinaContract 60736014.
Part of this work was donewhile Yang Liu was visiting the SMT group led byStephan Vogel at CMU.
We thank the anonymousreviewers for their insightful comments.
We arealso grateful to Stephan Vogel, Alon Lavie, Fran-cisco Guzman, Nguyen Bach, Andreas Zollmann,Vamshi Ambati, and Kevin Gimpel for their help-ful feedback.ReferencesPhil Blunsom and Trevor Cohn.
2006.
Discrimina-tive word alignment with conditional random fields.In Proceedings of COLING/ACL 2006, pages 65?72,Sydney, Australia, July.1025Peter F. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra, and Robert L. Mercer.
1993.
Themathematics of statistical machine translation: Pa-rameter estimation.
Computational Linguistics,19(2):263?311.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2):201?228.Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.2005.
Clause restructuring for statistical machinetranslation.
In Proceedings of ACL 2005, pages531?540, Ann Arbor, USA, June.Yonggang Deng, Jia Xu, and Yuqing Gao.
2008.Phrase table training for precision and recall: Whatmakes a good phrase and a good phrase pair?In Proceedings of ACL/HLT 2008, pages 81?88,Columbus, Ohio, USA, June.Christopher Dyer, Smaranda Muresan, and PhilipResnik.
2008.
Generalizing word lattice trans-lation.
In Proceedings of ACL/HLT 2008, pages1012?1020, Columbus, Ohio, June.Alexander Fraser and Daniel Marcu.
2007.
Measur-ing word alignment quality for statistical machinetranslation.
Computational Linguistics, Squibs andDiscussions, 33(3):293?303.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Pro-ceedings of COLING/ACL 2006, pages 961?968,Sydney, Australia, July.Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In Proceedingsof HLT/NAACL 2003, pages 127?133, Edmonton,Canada, May.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-dra Constantin, and Evan Herbst.
2007.
Moses:Open source toolkit for statistical machine transla-tion.
In Proceedings of ACL 2007 (poster), pages77?80, Prague, Czech Republic, June.Shankar Kumar, Franz J. Och, and WolfgangMacherey.
2007.
Improving word alignment withbridge languages.
In Proceedings of EMNLP 2007,pages 42?50, Prague, Czech Republic, June.Yang Liu, Qun Liu, and Shouxun Lin.
2005.
Log-linear models for word alignment.
In Proceedingsof ACL 2005, pages 459?466, Ann Arbor, Michigan,June.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machinetranslation.
In Proceedings of COLING/ACL 2006,pages 609?616, Sydney, Australia, July.Evgeny Matusov, Richard Zens, and Hermann Ney.2004.
Symmetric word alignments for statisticalmachine translation.
In Proceedings of COLING2004, pages 219?225, Geneva, Switzerland, August.Haitao Mi, Liang Huang, and Qun Liu.
2008.
Forest-based translation.
In Proceedings of ACL/HLT 2008,pages 192?199, Columbus, Ohio, June.Robert C. Moore, Wen-tau Yih, and Andreas Bode.2006.
Improved discriminative bilingual wordalignment.
In Proceedings of COLING/ACL 2006,pages 513?520, Sydney, Australia, July.Jan Niehues and Stephan Vogel.
2008.
Discrimina-tive word alignment via alignment matrix modeling.In Proceedings of WMT-3, pages 18?25, Columbus,Ohio, USA, June.Franz J. Och and Hermann Ney.
2003.
A systematiccomparison of various statistical alignment models.Computational Linguistics, 29(1):19?51.Franz J. Och and Hermann Ney.
2004.
The alignmenttemplate approach to statistical machine translation.Computational Linguistics, 30(4):417?449.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automaticevaluation of machine translation.
In Proceedingsof ACL 2002, pages 311?318, Philadelphia, Penn-sylvania, USA, July.Libin Shen, Jinxi Xu, and Ralph Weischedel.
2008.A new string-to-dependency machine translation al-gorithm with a target dependency language model.In Proceedings of ACL/HLT 2008, pages 577?585,Columbus, Ohio, June.Andreas Stolcke.
2002.
Srilm - an extension languagemodel modeling toolkit.
In Proceedings of ICSLP2002, pages 901?904, Denver, Colorado, Septem-ber.Ben Taskar, Simon Lacoste-Julien, and Dan Klein.2005.
A discriminative matching approach to wordalignment.
In Proceedings of HLT/EMNLP 2005,pages 73?80, Vancouver, British Columbia, Canada,October.Ashish Venugopal, Stephan Vogel, and Alex Waibel.2003.
Effective phrase translation extraction fromalignment models.
In Proceedings of ACL 2003,pages 319?326, Sapporo, Japan, July.Ashish Venugopal, Andreas Zollmann, Noah A. Smith,and Stephan Vogel.
2008.
Wider pipelines: n-best alignments and parses in mt training.
In Pro-ceedings of AMTA 2008, pages 192?201, Waikiki,Hawaii, October.Stephan Vogel and Hermann Ney.
1996.
Hmm-basedword alignment in statistical translation.
In Pro-ceedings of COLING 1996, pages 836?841, Copen-hagen, Danmark, August.1026
