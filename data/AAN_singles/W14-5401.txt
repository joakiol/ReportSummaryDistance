Proceedings of the 25th International Conference on Computational Linguistics, pages 1?8,Dublin, Ireland, August 23-29 2014.The Effect of Sensor Errors in Situated Human-Computer DialogueNiels SchuetteDublin Institute of Technologyniels.schutte@student.dit.ieJohn KelleherDublin Institute of Technologyjohn.d.kelleher@dit.ieBrian Mac NameeDublin Institute of Technologybrian.macnamee@dit.ieAbstractErrors in perception are a problem for computer systems that use sensors to perceive the envi-ronment.
If a computer system is engaged in dialogue with a human user, these problems inperception lead to problems in the dialogue.
We present two experiments, one in which partici-pants interact through dialogue with a robot with perfect perception to fulfil a simple task, and asecond one in which the robot is affected by sensor errors and compare the resulting dialogues todetermine whether the sensor problems have an impact on dialogue success.1 IntroductionComputer systems that can engage in natural language dialogue with human users are known as dia-logue systems.
A special class of dialogue systems are situated dialogue systems, which are dialoguesystems that operate in a spatial context.
Situated dialogue systems are an active research topic (e.g.
(Kelleher, 2006)).
Recently opportunities for more practical applications of situated dialogue systemshave arisen due to advances in the robustness of speech recognition and the increasing proliferation ofmobile computer systems such as mobile phones or augmented reality glasses.When a dialogue system operates in a situated context, it needs the ability to perceive the environment.Perception, such as computer vision, always has the potential of producing errors, such as failing tonotice an object or misrecognizing an object.
We are interested in the effect of perception-based errorson human-computer dialogue.
If the human user and the system have shared view, false perception by thesystem will lead to a divergence between the user?s understanding of the environment and the system?sunderstanding.
Such misunderstandings are frequent in human-human dialogue and human speakers usedifferent strategies to establish a shared understanding or common ground (Clark and Schaefer, 1989).We investigated this problem in an earlier work based on a corpus of human dialogue (Schuette et al.,2012) and are currently moving toward the same problem in human-computer dialogue.The problem of misunderstandings in human-computer dialogue has previously mostly been addressedunder the aspect of problems arising from problems in speech recognition or language understanding (e.g.
(Aberdeen and Ferro, 2003; Shin et al., 2002; L?opez-C?ozar et al., 2010)).
The problem of producingreferring expressions when it is not certain that the other participant shares the same perception andunderstanding of the scene has been addressed by (Horacek, 2005).
More recently (Liu et al., 2012)performed a similar experiment in the context of human-human interaction.
Their work was chieflyconcerned with the generation of referring expressions.We report on a work in progress in which we investigate the effect of sensor problems on human-computer dialogue using a dialogue system for a simulated robot.
We describe two experiments weperformed so far.
Both experiments are based on a shared experimental platform.
In the first experimentparticipants interact with a simulated robot using a text based dialogue interface to complete a series oftasks.
In the second experiment the participants again interact with the robot, except this time errors areintroduced into the robots perception.
The goal of the second experiment is to investigate what effectThis work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedingsfooter are added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/1(a) The interaction window.
(b) The simulation view.Figure 1: The user interface.the presence of sensor errors has on the dialogue and the task performance and compare it to the resultsfrom the first experiment.
It should be emphasized that the goal of the experiments is not to evaluate theperformance of the dialogue system, but to investigate the effect of perception errors on the dialogues.2 Experiment MethodologyThe experiments were performed using an experiment system that was developed for this experiment.It consists of a simulated world and a dialogue system.
The world contains a number of objects suchas boxes and balls.
These object can be manipulated by an abstract simulated robot arm.
The dialoguesystem is a frame based dialogue system that uses the Stanford Parser (Klein and Manning, 2003) forparsing.
The simulation environment was implement using Microsoft Robotics Studio.
The system iscapable of understanding and performing a range of simple to complicated spatial action instructionssuch as ?Put the ball behind the red box?
or ?Pick up the red ball between the green box and the yellowbox?.The participants interact with the system through the user interface shown in Figure 1.
It consists oftwo elements.
The simulation window shows a rendering of the simulation world that is updated in realtime.
The interaction window provides access to a text based chat interface that the participants use tointeract with the simulated robot.
When the participant sends a request to the system, the system analysesthe input and attempts to perform it in the simulation world.
If it can not perform the request, it repliesthrough the user interface and explains its problem.The robot?s perception is provided by a simulated vision system.
In general its perception is correct,but sensor errors can be introduced.
For example, it can be specified that the robot perceives entireobjects or some of their properties incorrectly.Each run of the experiment consisted of a sequence of test scenes.
Each scene consisted of a startscene and a target scene.
The start scene determined how the objects in the simulation world werearranged at the beginning of the test scene.
The target scene was presented to the participants as animage in the interaction window.
The participants?
task was to interact with the robot to recreate thetarget scene in the simulation world.After a participant had successfully recreated the target scene, the system automatically advanced tothe next scene.
The participants were also offered the option to abandon a scene and go on to the nextone if they thought they would not be able to complete the current scene.All utterances by the participant and the system are transcribed and annotated with their semantic2(a) Scene 1 (b) Target scene 1 (c) Scene 4 (d) Target scene 4Figure 2: Two scenes from Experiment 1 and their target scenes.interpretation.
The system also logs metrics that are used in the evaluation of dialogue systems to describethe cost of a dialogue, such as the task completion rate, the number of utterances, the completion timeand the number of errors (Walker et al., 1997).In the following we describe two experiments we performed with this setup so far.
In the first exper-iment participants completed a series of tasks.
In the second experiment, participants also completed aseries of tasks.
In this iteration however, errors were introduced into the system?s perception.3 Experiment 1The first experiment uses the basic version of the experiment system.
The purpose of the experimentwas to establish how difficult the basic experiment task would be and to create a set of performancemeasurements that could be used to compare this version of the system to later ones.3.1 InstructionsThe participants were provided with an instruction manual that described the experiment, introduced theuser interface and provided example interactions.
Participants were encouraged to abandon a scene ifthey felt that they would not be able to complete it.
After reading the instructions, the participants wereshown a video recording of some example interactions with the system.
This was done to prime theparticipants towards using language and concepts that were covered by the system.
No time limit wasset for experiment.3.2 Test ScenesThe set of test scenes contained 10 scenes in total.
Figure 2 shows some of the start scenes together withtheir respective target scenes.
Scene 1 (Figure 2a) is an example of a simple scene.
Scene 4 (Figure 2c)is an example of a more complex scene.The scenes were presented in fixed order.
The two initial scenes contained simple tasks.
Their mainpurpose is to allow the participants to gain practical experience with interacting with the system beforeapproaching the actual test scenes.
The remaining scenes were designed to elicit specific referringexpressions.
To transform a scene into its target scene, the participants had to move a number objectsfrom their original location to their respective target location as specified in the target scene.
To get therobot to move a target to a location, the participants had to specify which target the robot should move(e.g.
?Take the red ball?
), and specify where to move it (e.g.
?Put it behind the green box on the left?
).The complexity of the this task depends on the objects contained in the scene and their placement inrelation to each other.
We were particularly interested in getting the participants to use specific objectsas landmarks in their referring expressions, and designed the scenes in such a way that participantswere influenced towards specific expressions.
This was done with the motive of using landmark objectsas targets for perception errors in the second experiment.
For each scene a set of target conditions wasspecified that determined when a scene was complete.3.3 ParticipantsIn total 11 participants participated in the experiment.
Most of them were native English speakers ornon-native speakers who had been speaking English for a number of years.
Two of the participants were3(a) The start scene.
(b) The target scene.
(c) The start scene as per-ceived by the robot.Figure 3: One of the scenes from Experiment 2.female, the rest were male.
The participants were between 20 and 50 years of age.
All were collegesciences graduates who worked with computers on a daily basis.3.4 ResultsIn total 11 participants completed the experiments.
This resulted in a total of 110 interactions, twoof which had to be discarded due to recording problems.
A summary of the recorded metrics for thisexperiment is given in Table 1.
It shows for each scene:?
How many instructions the participants used on average to complete it.?
How long the participants needed to complete each scene on average.?
How many of the instructions the participants produced contained a reference that was either am-biguous (it could not be resolved to a unique referent) or unresolved (no referent that matched thereferring expression was found).?
The final column show how often each scene was abandoned.For the current investigation the last two columns are of primary interest.
Participants had been in-structed to abandon a scene if they thought that they would not be able to complete it.
The fact that thisonly occurred three times in 108 interactions indicates that the task was not very difficult and that thedialogue system?s performance was adequate for the task.
The percentage of unresolved references inthe second to last column is also interesting because it indicates how often participants made referencesthat the system was not able to resolve.
Since there were no errors introduced at this stage, the figurescan be seen as a baseline for the system?s ability to understand referring expressions.4 Experiment 2The main purpose of the second experiment was to investigate how the introduction of sensor errorswould influence the interactions and the outcome.4.1 InstructionsThe participants were provided with an extended version of the instruction manual as well as the intro-duction video from the first experiment.
The manual was identical to the manual from Experiment 1except for a small section that was added to explain that errors could occur in some of the scenes.
Theparticipants were encouraged to either try to work around the errors or to abandon the scene if the thoughtthey would not be able to finish it.
Again, no time limit was set.4.2 Test ScenesThe set of test scenes was based on the set of test scenes for Experiment 1, except that this time sensorerrors were introduced.
We investigated three possible error conditions.
In the missing object condition,the perception system did not register an object at all.
In the colour misclassification, the system did4Scene name Averagenumber ofactions persceneAveragetime perscenePercentageof am-biguous orunresolvedreferencesNumberof timesaban-donedScene 1 2.9 00:00:56 0 0Scene 2 2.3 00:00:54 0 0Scene 3 8.7 00:01:45 2.1 0Scene 4 5.9 00:01:52 10.8 0Scene 5 2 00:00:28 0 0Scene 6 5.2 00:01:23 5.2 1 (?
9%)Scene 7 2.6 00:00:40 0 0Scene 8 5.8 00:01:06 3.1 1 (?
9%)Scene 9 5.3 00:01:12 8.4 0Scene 10 6.8 00:01:30 6.7 1 (?
9%)Average 5.1 00:01:14 6 0.3Table 1: Summary of the cost metrics for Phase 1.
Few scenes were abandoned.
The percentage ofunresolved references forms a baseline for the resolution performance of the system.Scene name Averagenumber ofactions persceneAveragetime perscenePercentageof am-biguous orunresolvedreferencesNumber oftimes aban-donedScene 1 2.29 00:00:59 2.6 0 (0%)Scene 2 3.29 00:00:56 3.6 2(?
11.8%)Scene 3 9.12 00:02:13 9.7 3 (?
17.6%)Scene 4 9.88 00:01:58 10.1 5 (?
29.4%)Scene 5 10.35 00:01:46 9.7 2 (?
11.8%)Scene 6 12.82 00:02:43 7.3 9 (?
52.9%)Scene 7 4.82 00:01:08 14.6 2 (?
11.8%)Scene 8 3.35 00:00:47 8.8 1 (?
5.9%)Scene 9 9.88 00:01:34 9.5 4 (?
23.5%)Scene 10 9.59 00:01:47 9.8 5 (?
29.4%)Scene 11 10.82 00:02:08 5.4 3 (?
17.6%)Scene 12 7 00:01:21 8.4 1 (?
5.9%)Scene 13 6.65 00:01:29 8 2 (?
11.8%)Scene 14 11.7 00:03:10 8.5 17 (100%)Scene 15 5.18 00:01:02 15.9 1 (?
5.9%)Scene 16 4.88 00:01:04 14.5 1 (?
5.9%)Scene 17 6.82 00:01:01 1.7 0 (0%)Scene 18 8.65 00:02:00 6.8 1 (?
5.9%)Scene 19 9.4 00:01:45 7.8 0 (0%)Scene 20 6 00:01:17 6.9 0 (0%)Average 7.6 00:01:36 8.5 2.95Average (scenesw/o errors)6.1 00:01:20 4.9 0.5Average (scenesw/ errors)8.3 00:01:44 10 4Table 2: Summary of the cost metrics for Phase 2.
Scenes that contained no errors are highlighted ingreen.
Compared to Table 1, scenes that contained errors were more often abandoned, and resolutionproblems were more frequent.5perceive the affected object but determined its colour incorrectly.
A green ball for example, might bemistaken for a red ball.
In the type misclassification condition, the system also perceives the object, butdetermines the object?s type incorrectly, for example, a green ball might be mistaken for a green box.
Werestricted the errors so that at most one object was affected per scene.
This was done to create scenes thatcontained errors, but would still be solvable in most cases without major communication breakdowns.The impact a sensor error has on the interaction greatly depends on which object it affects, the contextthe object appears in, and the role the object plays in the task.
For example, if an object is affected thatdoes not need to be moved and that is unlikely to be mentioned as a landmark, it is likely that the errorwill not be noticed by the participant, and have no influence on the dialogue at all.
On the other hand, ifan error affects an object that absolutely needs to be moved in order to complete the task in such a waythat it becomes impossible to interact with the object (e.g.
because the robot does not see the object atall), it becomes effectively impossible to complete the task.
In less severe cases, errors may introduceproblems that can be solved.
For example, if the first attempt at a reference fails because a landmarkis not available to the system, the participant may reformulate the expression with a different landmark.This highlights the fact that sensor errors can have different effects depending on the circumstances.We therefore decided to design each scene and the errors for the second phase manually in order tomake sure that examples for as many problem combinations as possible were presented to the partici-pants.
We based the design of the scenes on our experiences from Experiment 1.
We selected suitablescenes and introduced errors such that the preferred expressions used in Experiment 1 would be affected.Each new scene created this way together with the original scene formed a corresponding scene pairs.Members of a pair can be compared against each other to assess the impact of errors in Experiment 2.The final set of scenes contained 14 scenes with sensor errors.
We added four more scenes without errorsto the test set.
Their purpose was to complement the data from the first experiment, and to check if thepresence of errors in other scenes would influence the behaviour of the participants in non-error scenes.We also added the two introductory scenes from the first experiment.
They were always presented asthe first scenes.
The remaining scenes were presented in randomized order to prevent learning effects.Therefore each participant was presented with a set of 20 scenes.
In total there were 22 correspondingscene pairs.Figure 3 contains an example of a scene from the second experiment that contained a perception error.Figure 3a show the start scene as presented to the participant.
Figure 3b shows the target scene that waspresented to the participant.
Figure 3c shows the start scene as it was perceived by the robot (it mistakesthe green box for a ball).Each scene was annotated with a set of target conditions and a set of sensor error specifications.4.3 Participants17 participants were recruited for the experiment from roughly the same demographic as the first exper-iment.
About half of the participants had participated in the first experiment.
A space of about 60 dayswas left between the first experiment and the second experiment to minimize any influence between theexperiments.4.4 ResultsIn total 17 participants completed the experiment.
This results in a total of 340 interactions.
Two inter-actions were lost, resulting in a set of 338 interactions.
The results for this experiment are given in Table2.
The highlighted rows (Scene 1,2,17,18,19 and 20) refer to scenes in which no errors were introduced.As in the first experiment, the two last columns are the most interesting ones.
Overall it can beobserved that more scenes were abandoned than in the first experiment.
Every scene except for theones without errors was abandoned at least once (Scene 14 was abandoned by all participants.
This wasexpected because it was designed to be not completable due to the errors).
This indicates that the taskwith the errors was more difficult than the one in the first experiment.It also appears that unresolved or ambiguous references were more frequent than in the first experi-ment.
At the bottom of the table we present overall averages for the different metrics.
It appears thatscenes with sensor errors generally show higher values than scenes without.6Figure 4: A boxplot comparing the number of actions between scenes from Experiment 1 and 2.
Pairedplots with the same colour refer to corresponding scenes (continued in Figure 5).5 Discussion and AnalysisOverall the results indicate that the introduction of sensor errors increases the difficulty of the task.
Theresults show that the participants had to abandon scenes with errors more often than scenes withouterrors.
On average they used more actions to complete scenes with errors.
A possible explanation canbe found in the higher percentage of unresolved references.
Participants attempted to refer to an object,but the system was unable to interpret it as expected due to a sensor error.
This forced the participants totry a different expression to progress with the task.
It should be noted that the number of unresolved andambiguous references at the present does not account for references that were resolved to an object thatwas not the object intended by the speaker.
We may approach this problem at a later stage.Figure 4 and 5 visualize the distribution of the number of actions for all the corresponding scene pairs.They are numbered 1 to 22.
Plots labelled with a correspond to scenes without errors, plots labelled withb to their counterparts with errors.
For easier visual comprehension, we coloured pairs alternatingly inblue and white.In general it can be observed that the median number of actions is generally higher for scenes witherrors than for their non-error counterparts, and that the interquartile range also tends to be higher.
Thedistributions appear to be fairly spread out.
This suggests that there is considerable variation betweenparticipants.
We performed t-tests between corresponding scenes to determine whether the differencesbetween corresponding scenes were significant.
The test shows that 12 out of 22 pairs were significantlydifferent with a p-value below 0.05.
We will investigate at a later stage how much the strength of thecorrespondence depends on the type of the error that was introduced.A comparison of the distribution of the completion times was less conclusive.
For some correspon-dence pairs, the median completion time is higher for error scenes, for other pairs it is lower.
We con-jecture that there is some sort of self-selection mechanism at work where participants who were lessconfident with the task in the first place task abandoned scenes earlier than confident participants whenthey encountered problems, but this will require further investigation.To summarize: The presence of sensor errors appears to increase the difficulty of the task, althoughthe effect appears to be small in some cases.
This was in some way to be expected because the errorswere designed to pose solvable problems and not lead to major communication breakdowns.6 Future WorkThe results from this experiment are still very fresh, and this paper represents the first step in theiranalysis.
In the next step we are going to try to identify strategies the participants employed once theyencountered an error and see how well they match up with the strategies we described for the human-human domain (Schuette et al., 2012).
We are also interested in finding out how strategies evolved overthe course of the experiment, and in how much variation there is between individual participants.7Figure 5: A boxplot comparing the number of actions between scenes from Experiment 1 and 2.
Pairedplots with the same colour refer to corresponding scenes (continued from Figure 5).We are currently preparing a third experiment based on the experiment setup.
In this experiment, theparticipants will be offered different ways of accessing the robot?s understanding of what it sees to theparticipant.
For example, in one condition, the system will be able to generate descriptions of how itperceives the scene.
The results of this third experiment will be evaluated in the context of the first andsecond experiment.ReferencesJohn Aberdeen and Lisa Ferro.
2003.
Dialogue patterns and misunderstandings.
In ISCA Tutorial and ResearchWorkshop on Error Handling in Spoken Dialogue Systems.Herbert H. Clark and Edward F. Schaefer.
1989.
Contributing to discourse.
Cognitive Science, pages 259?294.Helmut Horacek.
2005.
Generating referential descriptions under conditions of uncertainty.
In Proceedings of the10th European Workshop on Natural Language Generation (ENLG), pages 58?67.
Citeseer.J.
D. Kelleher.
2006.
Attention driven reference resolution in multimodal contexts.
Artificial Intelligence Review,25(1):2135.Dan Klein and Christopher D. Manning.
2003.
Accurate unlexicalized parsing.
In Proceedings of the 41st AnnualMeeting on Association for Computational Linguistics-Volume 1, page 423430.
Association for ComputationalLinguistics.Changsong Liu, Rui Fang, and Joyce Y. Chai.
2012.
Towards mediating shared perceptual basis in situateddialogue.
In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue,page 140149.
Association for Computational Linguistics.Ram?on L?opez-C?ozar, Zoraida Callejas, and David Griol.
2010.
Using knowledge of misunderstandings to increasethe robustness of spoken dialogue systems.
Knowledge-Based Systems, 23(5):471?485, July.Niels Schuette, John Kelleher, and Brian Mac Namee.
2012.
A corpus based dialogue model for grounding insituated dialogue.
In Proceedings of the 1st Workshop on Machine Learning for Interactive Systems: Bridgingthe Gap Between Language, Motor Control and Vision (MLIS-2012)., Montpellier, France, August.Jongho Shin, Shrikanth S. Narayanan, Laurie Gerber, Abe Kazemzadeh, Dani Byrd, and others.
2002.
Analysisof user behavior under error conditions in spoken dialogs.
In INTERSPEECH.Marilyn A. Walker, Diane J. Litman, Candace A. Kamm, and Alicia Abella.
1997.
PARADISE: a frameworkfor evaluating spoken dialogue agents.
In Proceedings of the eighth conference on European chapter of theAssociation for Computational Linguistics, page 271280.
Association for Computational Linguistics.8
