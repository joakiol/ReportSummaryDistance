Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 43?50,Sofia, Bulgaria, August 9, 2013. c?2013 Association for Computational LinguisticsMachine Translation with Many Manually Labeled Discourse ConnectivesThomas MeyerIdiap Research Institute and EPFLMartigny and Lausanne, Switzerlandthomas.meyer@idiap.chLucie Pola?kova?Institute of Formal and Applied LinguisticsFaculty of Mathematics and PhysicsCharles University in PraguePrague, Czech Republicpolakova@ufal.mff.cuni.czAbstractThe paper presents machine translation ex-periments from English to Czech with alarge amount of manually annotated dis-course connectives.
The gold-standarddiscourse relation annotation leads to bet-ter translation performance in ranges of4?60% for some ambiguous English con-nectives and helps to find correct syntacti-cal constructs in Czech for less ambiguousconnectives.
Automatic scoring confirmsthe stability of the newly built discourse-aware translation systems.
Error analysisand human translation evaluation point tothe cases where the annotation was mostand where less helpful.1 IntroductionRecently, research in statistical machine transla-tion (SMT) has renewed interest in the fact thatfor a variety of linguistic phenomena one needsinformation from a longer-range context.
Cur-rent statistical translation models and decodingalgorithms operate at the sentence and/or phraselevel only, not considering already translated con-text from previous sentences.
This local dis-tance is in many cases too restrictive to correctlymodel lexical cohesion, referential expressions(noun phrases, pronouns), and discourse markers,all of which relate to the sentence(s) before the oneto be translated.Discourse relations between sentences are oftenconveyed by explicit discourse connectives (DC),such as although, because, but, since, while.
DCsplay a significant role in coherence and readabil-ity of a text.
Likewise, if a wrong connective isused in translation, the target text can be fully in-comprehensible or not conveying the same mean-ing as was established by the discourse relationsin the source text.
In English, about 100 typesof such explicit connectives have been annotatedin the Penn Discourse TreeBank (PDTB, see Sec-tion 4), signaling discourse relations such as tem-porality or contrast between two spans of text.
De-pending on the set of relations used, there can beup to 130 such relations and combinations thereof.Discourse relations can also be present implicitly(inferred from the context), without any explicitmarker being present.
Although annotation for im-plicit DCs exists as well, we only deal with explicitDCs in this paper.
DCs are difficult to translatemainly because a same English connective can sig-nal different discourse relations in different con-texts and when the target language has either dif-ferent connectives according to the source rela-tions signaled or uses different lexical or syntac-tical constructs in place of the English connective.In this paper, we present MT experiments fromEnglish (EN) to Czech (CZ) with a large amountof manually annotated DCs.
The corpus, the par-allel Prague Czech-English Dependency Treebank(PCEDT) (Section 4), is directly usable for MTexperiments: the entire discourse annotation inEN is paralleled with a human CZ translation.This means that we can build and evaluate, againstthe CZ reference, a translation system, that learnsfrom the EN gold standard discourse relations.These then have no distortion from wrongly la-beled connectives as it is given in related work(Section 3) where automatic classifiers have beenused to label the connectives with a certain er-ror rate.
Furthermore, we can use the sense la-bels for 100 types of EN connectives, whereas re-lated work only focused on a few highly ambigu-ous connectives that are especially problematic fortranslation.The paper starts by illustrating difficult trans-lations involving connectives (Section 2) and dis-cusses related work in Section 3.
The resourcesand data used are introduced in Section 4.
TheMT experiments are explained in Section 5 and43automatic evaluation is given in Section 6.
We fur-ther provide a detailed manual evaluation and erroranalysis for the CZ translations generated by ourSMT systems (Section 7).
Future work describedin Section 8 concludes the paper.2 MotivationThe following example shows a CZ translationof the English DC meanwhile.
The previoussentences to the example were about other com-puter producers expected to report disappointingfinancial results.
The interpretation of meanwhileand the discourse relation (or sense) signaled istherefore CONTRASTIVE and not TEMPORAL:SOURCE: Apple Computer Inc., mean-while<COMPARISONCONTRAST>, is expected toshow improved earnings for the period ended September.BASELINE: Spolec?nost Apple Computer Inc., mezit?
?m byme?la uka?zat leps???
pr??
?jmy za obdob??
konc???c??
v za?r??
?.SYSTEM2: Spolec?nost Apple Computer Inc., naopak byme?la uka?zat leps???
pr??
?jmy za obdob??
konc???c??
v za?r??
?.A baseline SMT system for EN/CZ generated theincorrect CZ connective mezit?
?m which signals atemporal relation only.
The translation markedSYSTEM2 in the example was output by one ofthe systems we trained on manual DC annotations(cf.
Section 5).
The system correctly generatedthe CZ connective naopak signaling a contrastivesense.
The example sentence is taken from theWall Street Journal corpus, section 2365.
Thesense tag for meanwhile was manually annotatedin the Penn Discourse TreeBank, see Section 4.3 Related WorkThe disambiguation of DCs can be seen as a spe-cial form of Word Sense Disambiguation (WSD),that has been applied to SMT for content wordswith slight improvements to translation qual-ity (Chan et al 2007; Carpuat and Wu, 2007).DCs however form a class of procedural functionwords that relate text spans from an arbitrarilylong context and their disambiguation needs fea-tures from that longer-range context.
Only fewstudies address function word disambiguation forSMT: Chang et al(2009) disambiguate a mul-tifunctional Chinese particle for Chinese/Englishtranslation and Ma et al(2011) use tagging ofEnglish collocational particles for translation intoChinese.
Lexical cohesion at the document levelhas recently also come into play, with studieson lexical consistency in SMT (Carpuat, 2009;Carpuat and Simard, 2012), topic modeling ap-plied to SMT (Eidelman et al 2012) or decod-ing with document-wide features (Hardmeier etal., 2012).
A recently published article summa-rizes most of the work on SMT with the broaderperspective of discourse, lexical cohesion and co-reference (Hardmeier, 2013).For discourse relations and DCs especially,more and more annotated resources have be-come available in several languages, such as En-glish (Prasad et al 2008), French (Pe?ry-Woodleyet al 2009; Danlos et al 2012), German (Stede,2004), Arabic (AlSaif, 2012), Chinese (Zhou andXue, 2012) and Czech (Mladova?
et al 2009).These resources however remain mostly monolin-gual, i.e.
translations or parallel texts in other lan-guages do normally not exist.
This makes theseresources not directly usable for MT experiments.Recent work has shown that more adequateand coherent translations can be generated forEnglish/French when ambiguous connectives inthe source language are annotated with the dis-course relation they signal (Popescu-Belis et al2012).
SMT systems for European languagepairs are most often trained on Europarl corpusdata (Koehn, 2005), where only a small amount ofdiscourse-annotated instances is available (8 con-nectives with about 300-500 manual annotationseach).
Meyer and Popescu-Belis (2012) there-fore used these few examples to train automaticclassifiers that introduce the sense labels for theconnectives in the entire English text of the Eu-roparl corpus.
Although these classifiers are state-of-the-art, they can have an error rate of up to30% when labeling unseen instances of connec-tives.
The discourse-aware SMT systems never-theless improved about 8-10% of the connectivetranslations.
When integrating into SMT directlythe small manually-labeled data, without train-ing classifiers, hardly any translation improvementwas measurable, cf.
(Meyer and Popescu-Belis,2012).4 The Parallel Prague Czech-EnglishDependency TreebankWith the English-Czech parallel text provided inthe Prague Czech-English Dependency Treebank2.0 (PCEDT) (Hajic?
et al 2011)1, comes a hu-man CZ translation of the entire Wall Street Jour-nal Corpus in EN (WSJ, sections 00-24, approxi-1http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?catalogId=LDC2012T0844mately 50k sentences).The syntactical annotation of WSJ, the PennTreeBank (Marcus et al 1993), has been followedby a discourse annotation project, the Penn Dis-course TreeBank (PDTB) (Prasad et al 2008),over the same sections of the corpus.
In thePDTB version 2.0, 18,459 instances of explicitDCs, among other discourse-related phenomena(implicit relations, alternative lexicalizations), arelabeled along with the text spans they connect (dis-course arguments) and the discourse relation theysignal (sense tags).The sense tags are organized in a three-levelsense hierarchy with four top semantic classes,16 sub-senses on the second and further 23 sub-senses on the third hierarchy level.
The annotatorswere not forced to make the finest distinction (onthe sub-sense level).
A token can also be annotatedwith two senses, forming a composite sense witha label combination from wherever in the hierar-chy, resulting in 129 theoretically possible distinctsense tags (see Section 5 for the sense levels weuse).
For the latter reason, some of the sense labelsare very scarcely used and although they make forimportant and fine-grained distinctions in English,this granularity level might not be useful for trans-lation, where only certain ambiguities have to beresolved to obtain a correct target language con-nective, see Section 7.The PCEDT is a 1:1 sentence-aligned paral-lel resource with a manual multilayer dependencyanalysis of both original Penn TreeBank-WSJtexts and their translations to Czech.
Despitethe manually annotated parallel dependency treeswhich are very valuable in other linguistic stud-ies, for translation we only used the plain CZ textsprovided with the treebank.5 Experimental SetupIn the following, we describe a series of SMT ex-periments that made direct use of the EN/CZ textas provided with the PCEDT.
The SMT modelswere all phrase-based and trained with the Mosesdecoder (Koehn et al 2007), either on plain textfor the BASELINE or on text where the EN con-nective word-forms have been concatenated withthe PDTB sense labels.
All texts have been tok-enized and lowercased with the Moses tools beforetraining SMT.
In future work, we will build fac-tored translation models (Koehn and Hoang, 2007)as well, as this would reduce the label scarcitythat was likely a problem when just concatenatingword-forms and labels (see Sections 7 and 8).For SYSTEM1 in the following, we inserted, intothe English side of the PCEDT data, the full senselabels from the PDTB, which can be, as alreadymentioned, as detailed as containing 3 sense levelsand allowing for composite tags (where annotatorschose that two senses hold at the same time).
SYS-TEM1 therefore operates on a total of 63 distinctand observed sense tags for all DCs.For SYSTEM2, we reduced the sense labels tocontain only senses from PDTB sense hierarchylevel 2 and 1, not allowing for composite senses,i.e.
for those instances that were annotated withtwo senses we discarded the secondary (but notless important) sense.
This reduced the set ofsenses for SYSTEM2 to 22.The procedure is exemplified in the examplebelow with an EN sentence 1 (WSJ section2300) containing a complex PDTB sense tagthat has been kept for SYSTEM1.
For SYS-TEM2 we have reduced the sense of when to:<CONTINGENCYCONDITIONGENERAL>.
Sen-tence 2 (WSJ section 2341) contains two alreadysimplified sense tags.
The original PDTB sensetags for meanwhile and as were respectively<COMPARISONCONTRASTJUXTAPOSITION>and <CONTINGENCYPRAGMATICCAUSE-JUSTIFICATION>, where JUXTAPOSITION andJUSTIFICATION were dropped because they stemfrom the third level of the PDTB sense hierarchy:1.
Selling snowballed because of waves of au-tomatic ?stop-loss?
orders, which are triggered bycomputer when<CONTINGENCYCONDITIONGENERAL-TEMPORALASYNCHRONOUSSUCCESSION> prices fall tocertain levels.2.
Meanwhile<COMPARISONCONTRAST>, analysts saidPfizer?s recent string of lackluster quarterly performancescontinued, as<CONTINGENCYPRAGMATICCAUSE> earn-ings in the quarter were expected to decline by about 5%.In order to build SMT systems of reasonablequality, we still need to combine the PCEDT texts(50k sentences) with other resources such as theEN/CZ parts of the Europarl corpus.
This resultsin a mixture of labeled and unlabeled DCs in thedata and estimates might be noisy.
We howeveralso checked system performance on the PDTBtest set (section 23) with labeled DCs only (seeSection 6) for which the unlabeled ones in themodel do not pose a problem, as they are not con-sidered as valid target phrases by the SMT de-coder.
The following list gives an overview of thedata used to build three SMT systems.
No modi-45fications have been done to the texts of the BASE-LINE system, that uses exactly the same amount ofsentences, but no sense labels.?
BASELINE: no tags for connectives?
SYSTEM1: complex PDTB sense tags?
SYSTEM2: simplified PDTB sense tags?
training: Europarlv7 (645,155 sentences)+ PDTB sections 02-21 (41,532 sentences;15,402 connectives)?
tuning: newstest2011 (3,003 sentences) +PDTB sections 00,01,22,24 (5,260 sentences;2,134 connectives)?
testing: newstest2012 (3,001 sentences) +PDTB section 23 (2,416 sentences; 923 con-nectives)2The language model, the same for BASE-LINE, SYSTEM1 and SYSTEM2, was built usingSRILM (Stolcke et al 2011) with 5-grams overEuroparl and the news data sets 2007-2011 in CZ,as distributed by the Workshop on Machine Trans-lation3.
All systems were tuned by MERT (Och,2003) as implemented in Moses.6 Automatic EvaluationMost automatic MT scoring relies on n-grammatching of a system?s candidate translationagainst (usually) only one human reference trans-lation.
For DCs therefore, automatic scores do notreveal much of a system?s performance, as oftenonly one or two words, i.e.
the DC is changed.When a candidate translation however contains amore accurate and correct connective, the trans-lation output is often more coherent and readablethan the baseline?s output, see Section 7.Automatic evaluation has been done using theMultEval tool, version 0.5.1 (Clark et al 2011).The BLEU scores are computed by jBLEU V0.1.1(an exact reimplementation of NIST?s mteval-v13.pl without tokenization).
Table 1 provides anoverview of the BLEU scores for the BASELINEand systems 1 and 2 on the full test set (new-stest2012 + PDTB section 23), and on PDTB sec-tion 23 only, the latter containing 2,416 sentencesand 923 labeled DCs.In order to gain reliable automatic evaluationscores, we executed 5 runs of MERT for each2Note that this PDTB section division for training, devel-opment and testing is the same as is used for automatic clas-sification experiments, as recommended in the PDTB anno-tation manual.3http://www.statmt.org/wmt12/translation model configuration.
MERT is imple-mented as a randomized, non-deterministic opti-mization process, so that each run leads to differ-ent feature weights and as a consequence, to dif-ferent BLEU scores when translating unseen text.The scores from the 5 runs were then averaged andwith a t-test we calculated the confidence p-valuesfor the score differences.
When these are below0.05, they confirm that it is statistically likely,that such scores would occur again in other tun-ing runs.
In terms of BLEU, neither SYSTEM1 norSYSTEM2 therefore performs significantly betteror worse than the BASELINE.In order to show how little the DC labeling ac-tually affects the BLEU score, we randomized allconnective sense tags in PDTB test section 23 andtranslated again 5 times (with the weights fromeach tuning run) with both, SYSTEM1 and SYS-TEM2.
With randomized labels, both systems per-form statistically significantly worse (p = 0.01,marked with a star in Table 1) than the BASELINE,but only with an average performance loss of ?0.6BLEU points.
Note that some sense tags mightstill have been correct due to randomization.Test set System BLEUnt2012 + PDTB 23BASELINE 17.6SYSTEM1 17.6SYSTEM2 17.6PDTB 23BASELINE 21.4SYSTEM1 21.4SYSTEM2 21.4PDTB 23 randomSYSTEM1 20.8*SYSTEM2 20.8*Table 1: BLEU scores when testing on the com-bined test set (newstest2012 + PDTB 23); onPDTB section 23 only (2416 sentences, 923 con-nectives); and when randomizing the sense tags(PDTB 23 random), for the BASELINE system andthe two systems using PDTB connective labels:SYSTEM1: complex labels, SYSTEM2: simplifiedlabels.
When testing on randomized sense labels(PDTB 23 random), the BLEU scores are statisti-cally significantly lower than the ones on the cor-rectly labeled test set (PDTB 23), which is indi-cated by starred values.Automatic MT scoring does therefore not revealactual changes in translation quality due to DCusage.
In the next section, we manually analyze46samples of the translation output by SYSTEM2 thatreached the highest scores observed in some of thesingle tuning runs before averaging.7 Manual Evaluation and Error AnalysisTwo human judges went both through two randomsamples of SYSTEM2 translations from WSJ sec-tion 23, namely sentences 1-300 and 1000-2416.In these sentences, there were 630 observed con-nectives.
The judges counted the translations thatwere better, equal and worse in terms of the DCs asoutput by SYSTEM2 versus the BASELINE system.We then summarized the counts over the two sam-ples and give the scores as ?
(%) in Table 2.
Tofurther test if we just had bad samples, the judgeswent through another set of translations (1024?1138), containing 50 DCs, for which the countsare summarized in Table 2 as well.
A translationwas counted as being correct when it generated avalid CZ connective for the corresponding context,without grading the rest of the sentences.Overall, it was found that the number of bettertranslations is only slightly higher for SYSTEM2than the ones from the BASELINE system.
Thevast majority of DCs was translated correctly byboth the BASELINE and SYSTEM2, and in very fewcases, both systems translated the DCs incorrectly.SYSTEM2 appeared to systematically repeat onemistake, namely translating the very frequent con-nective but preferably with jenz?e, which is correctbut rare in CZ (the primary and default equivalentfor but in CZ is ale).
This ?mis-learning?
likelyhappened to a frequent correspondence of but?jenz?e in the SMT training data, which then doesnot necessarily scale to and be of appropriate stylein the testing data.
If one disregards these occur-rences, SYSTEM2 translates between about 8 and20% of all connectives better than the BASELINE(discounted percentages for jenz?e in Table 2).
Theresults seem therefore to be dependent on the partsof the test set evaluated and the DCs occurring inthem.The only slight quantitative improvements andcases were SYSTEM2 performed worse are mostlikely due to the overall scarcity of the PDTBsense tags (cf.
Section 4).
Especially for SYS-TEM1 but to some extent also for SYSTEM2, raresense tags such as CONTINGENCYPRAGMATIC-CAUSE might not be seen often or even not at all inthe SMT training data and therefore not be learnedappropriately to provide good translations for thetest data.
In relation to that, simply concatenat-ing the sense tags onto the connective word-formsleads to scarcity of the latter, whereas other waysto include linguistic labels in SMT, such as fac-tored translation models, would account for the la-bels as additional translation features, which willbe investigated in future work (Section 8).In the following, we analyze cases where SYS-TEM2 translates the connectives better and moreappropriately than the BASELINE.
These casesinclude highly ambiguous connectives, temporalDCs with verbal ing-forms and conditionals.In general, for the very ambiguous EN connec-tives (e.g.
as, when, while), disambiguated forSYSTEM2 with the PDTB sense tags, we indeedobtained more accurate translations than thosegenerated by the BASELINE.
One of the humanjudges had a close look at 25 randomly sampledinstances of as, taken from the manually evalu-ated sets mentioned above.
In these test cases,68% of all occurrences of as were better translatedby SYSTEM2 and only 4% of the translations weredegraded when compared to the BASELINE.
Fordetails, see Table 34.In the following translation example (WSJsection 2365), and often elsewhere, the BASELINEsystem treats the connective as as a prepositionjako with the meaning She worked as a teacher.This frequent interpretation seems to be learnedquite reasonably from the SMT training data, it ishowever incorrect where as actually functions asa DC.
SYSTEM2, in agreement with the tagging,then correctly generates the causal connectiveprotoz?e:SOURCE: In the occupied lands, underground leaders ofthe Arab uprising rejected a U.S. plan to arrange Israeli-Palestinian talks as<CONTINGENCYCAUSE> Shamir op-posed holding such discussions in Cairo.BASELINE: *Na okupovany?ch u?zem?
?ch, podzemn??
vu?dcu?arabsky?ch povsta?n??
odm?
?tl americky?
pla?n uspor?a?datizraelsko-palestinske?
rozhovory jako S?amira proti por?a?da?n?
?takovy?ch diskus??
v Ka?hir?e.SYSTEM2: Na okupovany?ch u?zem?
?ch, podzemn??
vu?dcu?arabske?ho povsta?n??
odm?
?tl americky?
pla?n uspor?a?datizraelsko-palestinske?
rozhovory, protoz?e S?amira protipor?a?da?n??
takovy?ch diskus??
v Ka?hir?e.DCs can also be translated to other syntacticalconstructs available in the target language thatconvey the same discourse relation without any4We included simple occurrences only, i.e.
not compoundconnectives like as if, as soon as or translations were the con-nective was dropped.
In the PDTB, as can have up to 17distinct senses, ranging from temporal, causal to concessiverelations.47Configuration ?
(%) vs. BASELINE Total (%)Improved Equal Degradedsentences 1?300 / 1000?2416630 labeled DCsSYSTEM2 7.9 75.2 9.4 92.5not counting 25 x but?jenz?e 8.2 80.3 4.0 92.5both systems wrong 7.5100sentences 1024?113850 labeled DCsSYSTEM2 16 76 6 98not counting 2 x but?jenz?e 19 77 2 98both systems wrong 2100Table 2: Performance of SYSTEM2 (simplified PDTB tags) when manually counting for improved, equaland degraded translations compared to the BASELINE, in samples from the PDTB section 23 test set.explicit DC.
For EN/CZ this occurs for DCs suchas before/after/since + Verb in Present Continu-ous.
In CZ, these either should be rendered as averbal clause or a nominalization.
We accountedfor translations as being well-formed, if theSMT systems generated one of these possibilitiescorrectly, i.e.
not only the connective/prepositionbut also the verb/noun.
In CZ, it must be decidedbetween using a preposition (e.g.
pr?ed) or aconnective (e.g.
nez?).
A good translation wouldfor example be: before climbing = PREP+NP orDC+V, and a bad translation: before climbing= PREP+V/ADJ or DC+NP.
The followingexample (WSJ section 2381) is a SYSTEM2 outputwhere the sense tag in English helped to translatethe connective before more correctly by DC+V,whereas the BASELINE renders this wrongly byusing PREP+ADJ:SOURCE: Mr. Weisman predicts stocks willappear to stabilize in the next few days be-fore<TEMPORALASYNCHRONOUS> declining again,trapping more investors.BASELINE: *Pan Weisman pr?edpov?
?da?, z?e akcie budoustabilizovat v pr???s?t?
?ch ne?kolika dnech pr?ed/PREP kle-saj??c?
?m/ADJ ope?t odchytu v?
?ce investoru?.SYSTEM2: Pan Weisman pr?edpov?
?da?, z?e akcie budestabilizovat, jak se zda?, v pr???s?t?
?ch ne?kolika dn?
?, nez?/DCope?t klesat/V, zablokova?n??
v?
?ce investoru?.A further difficult case in CZ is the binding ofconditionals with personal pronouns, e.g.
if I =kdybych, if you = kdybys, if he/she = kdyby etc.In the following example (WSJ section 2386), theBASELINE system completely missed to renderthe personal pronoun (but still generated thecorrect conditional connective if?pokud), whereasSYSTEM2 outputs the much better if I?kdybych.However, apart from the better connective, SYS-TEM2?s translation is worse than the BASELINE?s,because the first verb form is misconjugated andthe second verb (will take) is missing:SOURCE: If<CONTINGENCYCONDITION> I sell now, I?lltake a big loss.BASELINE: *Pokud chte?l prodat, ted?
budu bra?t s velkouztra?tou.LIT.
: If he-wanted to-sell, now I-will take with big-Instrumental loss-Instrumental.SYSTEM2: Kdybych se nyn??
proda?vaj?
?, se z tohohle velkouztra?tu.LIT.
: If-I themselves-ReflexPron now they-are selling, Re-flexPron out-of this big-Accusative loss-Accusative.From the automatic and manual translationevaluation, we conclude that using the sense tagsfor all 100 connectives in EN is not the most ap-propriate method, and that only certain connec-tives such as as, when, while, yet and a few oth-ers are very problematic in translation due to themany discourse relations they can signal.
In fu-ture work, we will therefore analyze in more detailwhich connectives and which sense labels from thePDTB should actually be included in the data totrain SMT.48BASELINE SYSTEM2 occ.
PDTBjak kdyz?
1 SYjak kdyz?
1 SYjelikoz?
jelikoz?
1 CAnebot?
nebot?
1 CAprotoz?e protoz?e 2 SY/CO; CAa protoz?e 1 SY/COaby kdyz?
1 SYjak kdyz?
1 SYjak protoz?e 1 CAjako protoz?e 4 SY/CO; CAjako kdyz?
5 SY; ASY; CAjako kdy 2 SYprotoz?e kdyz?
1 SYz?e kdyz?
1 SYjako jak 1 SYjako pote?, co 1 SYTotal 25SYS2 + 68%SYS2 = 20%SYS2 ?
4%both ?
8%Table 3: Translation outputs for the EN con-nective as, which was translated more correctlyby SYSTEM2 thanks to the disambiguating sensetags compared to the BASELINE that often justproduces the prepositional as ?
jako.
The erro-neous translations are marked in bold.
The PDTBsense tags indicate the meaning of the CZ trans-lations and are encoded as follows: Synchrony(Sy), Asynchrony (Asy), Contingency (Co), Cause(Ca).8 ConclusionWe presented experiments for EN/CZ SMT witha large amount of hand-labeled discourse connec-tives that are disambiguated in the source languageand training material for MT systems by theirsense tags or discourse relations they signal.
Thisleads to improved translations in cases where thesource DC is highly ambiguous or where the tar-get language uses other syntactical constructs thana connective to convey the discourse relation.Using all 100 types of EN DCs in the corpusand/or all the detailed sense tags from the man-ual annotation most probably lead to the only veryslight improvements for the discourse-aware sys-tems when measured quantitatively over the wholetest sets.
In future work we plan to more thor-oughly analyze which connectives need to be dis-ambiguated at which sense granularity level beforeimplementing them into an SMT system.For label implementation there also are otherways worth examining, such as factored transla-tion models that handle the supplementary linguis-tic information as separate features and alternativedecoding paths.AcknowledgmentsWe are grateful for the funding of this work to theSwiss National Science Foundation (SNSF) underthe COMTIS Sinergia project, n. CRSI22 127510(see www.idiap.ch/comtis/), to the GrantAgency of the Czech Republic (project n.P406/12/0658) and to the SVV of the CharlesUniversity (project n. 267 314).
We would liketo thank Lenka Sa?ndor for her help with manualtranslation evaluation.ReferencesAmal AlSaif.
2012.
Human and Automatic Annota-tion of Discourse Relations for Arabic.
Ph.D. thesis,University of Leeds.Marine Carpuat and Michel Simard.
2012.
The Trou-ble with SMT Consistency.
In Proceedings of the7th Workshop on Statistical Machine Translation(WMT), pages 442?449, Montreal, Canada.Marine Carpuat and Dekai Wu.
2007.
Improving Sta-tistical Machine Translation using Word Sense Dis-ambiguation.
In Proceedings of Joint Conference onEmpirical Methods in Natural Language Process-ing (EMNLP) and Computational Natural LanguageLearning (CoNLL), pages 61?72, Prague, Czech Re-public.Marine Carpuat.
2009.
One Translation per Discourse.In Proceedings of the Workshop on Semantic Evalu-ations: Recent Achievements and Future Directions(SEW), pages 19?27, Singapore.Yee Seng Chan, Hwee Tou Ng, and David Chiang.2007.
Word Sense Disambiguation Improves Sta-tistical Machine Translation.
In Proceedings of the45th Annual Meeting of the Association of Com-putational Linguistics (ACL), pages 33?40, Prague,Czech Republic.Pi-Chuan Chang, Dan Jurafsky, and Christopher D.Manning.
2009.
Disambiguating ?DE?
for Chinese-English Machine Translation.
In Proceedings of theFourth Workshop on Statistical Machine Translationat the 12th Meeting of the European Chapter of theAssociation for Computational Linguistics (EACL),Athens, Greece.49Jonathan Clark, Chris Dyer, Alon Lavie, and NoahSmith.
2011.
Better Hypothesis Testing for Statisti-cal Machine Translation: Controlling for OptimizerInstability.
In Proceedings of ACL-HLT 2011 (46thAnnual Meeting of the ACL: Human Language Tech-nologies), Portland, OR.Laurence Danlos, Die?go Antolinos-Basso, Chloe?Braud, and Charlotte Roze.
2012.
Vers leFDTB : French Discourse Tree Bank.
In Actes dela confe?rence conjointe JEP-TALN-RECITAL 2012,volume 2: TALN, pages 471?478, Grenoble, France.Vladimir Eidelman, Jordan Boyd-Graber, and PhilipResnik.
2012.
Topic Models for Dynamic Transla-tion Model Adaptation.
In Proceedings of ACL 2012(50th Annual Meeting of the Association for Compu-tational Linguistics, pages 115?119, Jeju, Republicof Korea.Jan Hajic?, Eva Hajic?ova?, Jarmila Panevova?, Petr Sgall,Silvie Cinkova?, Eva Fuc??
?kova?, Marie Mikulova?, PetrPajas, Jan Popelka, Jir???
Semecky?, Jana S?indlerova?,Jan S?te?pa?nek, Josef Toman, Zden?ka Ures?ova?, andZdene?k Z?abokrtsky?.
2011.
Prague Czech-EnglishDependency Treebank 2.0.
Institute of Formaland Applied Linguistics, Charles University, Prague,Czech Republic.Christian Hardmeier, Joakim Nivre, and Jo?rg Tiede-mann.
2012.
Document-Wide Decoding for Phrase-Based Statistical Machine Translation.
In Proceed-ings of the Conference on Empirical Methods in Nat-ural Language Processing and Natural LanguageLearning (EMNLP-CoNLL), Jeju, Korea.Christian Hardmeier.
2013.
Discourse in StatisticalMachine Translation.
DISCOURS, 11:1?29.Philipp Koehn and Hieu Hoang.
2007.
FactoredTranslation Models.
In Proceedings of the JointConference on Empirical Methods in Natural Lan-guage Processing (EMNLP) and ComputationalNatural Language Learning (CONLL), pages 868?876, Prague, Czech Republic.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-dra Constantin, and Evan Herbs.
2007.
Moses:Open Source Toolkit for Statistical Machine Trans-lation.
In Proceedings of 45th Annual Meeting of theAssociation for Computational Linguistics (ACL),Demonstration Session, pages 177?180, Prague,Czech Republic.Philipp Koehn.
2005.
Europarl: A Parallel Corpus forStatistical Machine Translation.
In Proceedings ofMT Summit X, pages 79?86, Phuket, Thailand.Jianjun Ma, Degen Huang, Haixia Liu, and WenfengSheng.
2011.
POS Tagging of English Particles forMachine Translation.
In Proceedings of the Thir-teenth Machine Translation Summit, pages 57?63,Xiamen, China.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a Large AnnotatedCorpus of English: The Penn Treebank.
Computa-tional Linguistics, 19(2):313?330.Thomas Meyer and Andrei Popescu-Belis.
2012.
Us-ing Sense-labeled Discourse Connectives for Statis-tical Machine Translation.
In Proceedings of theEACL 2012 Joint Workshop on Exploiting Synergiesbetween IR and MT, and Hybrid Approaches to MT(ESIRMT-HyTra), pages 129?138, Avignon, FR.Lucie Mladova?, S?a?rka Zika?nova?, Zuzanna Bedr?ichova?,and Eva Hajic?ova?.
2009.
Towards a discourse cor-pus of Czech.
In Proceedings of the Corpus Linguis-tics Conference, Liverpool, UK.Franz Josef Och.
2003.
Minimum Error Rate Trainingin Statistical Machine Translation.
In Proceedingsof the 41st Annual Meeting of the Association forComputational Linguistics (ACL), pages 160?167,Sapporo, Japan.Marie-Paule Pe?ry-Woodley, Nicholas Asher, PatriceEnjalbert, Farah Benamara, Myriam Bras, Ce?cileFabre, Ste?phane Ferrari, Lydia-Mai Ho-Dac, AnneLe Draoulec, Yann Mathet, Philippe Muller, Lau-rent Pre?vot, Josette Rebeyrolle, Ludovic Tanguy,Marianne Vergez-Couret, Laure Vieu, and AntoineWidlo?cher.
2009.
ANNODIS: une approche out-ille?e de l?annotation de structures discursives.
InActes de la 16e`me Confe?rence sur le TraitementAutomatique des Langues Naturelles (TALN), Paris,France.Andrei Popescu-Belis, Thomas Meyer, JeevanthiLiyanapathirana, Bruno Cartoni, and Sandrine Zuf-ferey.
2012.
Discourse-level Annotation over Eu-roparl for Machine Translation: Connectives andPronouns.
In Proceedings of the eighth interna-tional conference on Language Resources and Eval-uation (LREC), Istanbul, Turkey.Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-sakaki, Livio Robaldo, Aravind Joshi, and BonnieWebber.
2008.
The Penn Discourse Treebank 2.0.In Proceedings of 6th International Conference onLanguage Resources and Evaluation (LREC), pages2961?2968, Marrakech, Morocco.Manfred Stede.
2004.
The Potsdam Commentary Cor-pus.
In Proceedings of the ACL Workshop on Dis-course Annotation, pages 96?102, Barcelona, Spain.Andreas Stolcke, Jing Zheng, Wen Wang, and VictorAbrash.
2011.
SRILM at Sixteen: Update andOutlook.
In Proceedings of the IEEE AutomaticSpeech Recognition and Understanding Workshop,Waikoloa, Hawaii.Yuping Zhou and Nianwen Xue.
2012.
PDTB-styleDiscourse Annotation of Chinese Text.
In Proceed-ings of the 50th Annual Meeting on Association forComputational Linguistics (ACL), Jeju Island, Ko-rea.50
