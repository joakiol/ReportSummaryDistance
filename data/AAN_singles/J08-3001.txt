SquibsReliability Measurement without LimitsDennis Reidsma?University of TwenteJean Carletta?
?University of EdinburghIn computational linguistics, a reliability measurement of 0.8 on some statistic such as ?
iswidely thought to guarantee that hand-coded data is fit for purpose, with 0.67 to 0.8 tolerable,and lower values suspect.
We demonstrate that the main use of such data, machine learning, cantolerate data with low reliability as long as any disagreement among human coders looks likerandom noise.
When the disagreement introduces patterns, however, the machine learner canpick these up just like it picks up the real patterns in the data, making the performance figureslook better than they really are.
For the range of reliability measures that the field currentlyaccepts, disagreement can appreciably inflate performance figures, and even a measure of 0.8 doesnot guarantee that what looks like good performance really is.
Although this is a commonsenseresult, it has implications for how we work.
At the very least, computational linguists shouldlook for any patterns in the disagreement among coders and assess what impact they will have.1.
IntroductionIn computational linguistics, 0.8 is often regarded as some kind of magical reliabilitycut-off guaranteeing the quality of hand-coded data (e.g., Reithinger and Kipp 1998;Shriberg et al 1998; Galley et al 2004), with 0.67 to 0.8 tolerable?although it is asoften honored in the breech as in the observance.
The argument for the meaning of0.8 arises originally from Krippendorff (1980, page 147), in a comment about practicein the field of content analysis.
He states that correlations found between two variablesusing their hand-coded values ?tend to be insignificant?
when the hand-codings havea reliability below 0.8.
He uses a specific reliability statistic, ?, for his measurements,but Carletta (1996) implicitly assumes kappa-like metrics are similar enough in practicefor the rule of thumb to apply to them as well.
A detailed discussion on the differencesand similarities of these, and other, measures is provided by Krippendorff (2004); in thisarticle we will use Cohen?s ?
(1960) to investigate the value of the 0.8 reliability cut-offfor computational linguistics.Modern computational linguists use data in a completely different way from 1970scontent analysts.
Rather than correlating two variables, we use hand-coded data as?
University of Twente, Human Media Interaction, Room ZI2067, PO Box 217, NL-7500 AE Enschede,The Netherlands, D.Reidsma@utwente.nl.??
University of Edinburgh, Human Communication Research Centre, J.Carletta@ed.ac.uk.Submission received: 4 September 2007; revised submission received: 20 December 2007; accepted forpublication: 6 April 2008.?
2008 Association for Computational LinguisticsComputational Linguistics Volume 34, Number 3training and test material for automatic classifiers.
The 0.8 rule of thumb is irrelevantfor this purpose, because classifiers will be affected by disagreement differently thancorrelations.
Furthermore, Krippendorff?s argument comes with a caveat: the disagree-ment must be due to random noise.
For his case of correlations, any patterns in thedisagreement could accidentally bolster the relationship perceived in the data, leadingto false results.
To be sure that data is fit for the intended purpose, Krippendorff advisesthe analyst to look for structure in the disagreement and consider how it might affectdata use.
Although computational linguists have rarely followed this advice, it is justas relevant to us.
Machine-learning algorithms are designed specifically to look for, andpredict, patterns in noisy data.
In theory, this makes random disagreement unimportant.More data will yield more signal and the learner will ignore the noise.
However, asCraggs and McGee Wood (2005) suggest, this also makes systematic disagreementdangerous, because it provides an unwanted pattern for the learner to detect.
Wedemonstrate that machine learning can tolerate data with a low reliability measurementas long as the disagreement looks like random noise, and that when it does not, datacan have a reliability measure commonly held to be acceptable but produce misleadingresults.2.
MethodTo explain what is wrong with using 0.8 as a cut-off, we need to think about how datais used for classification tasks.
Consider Figure 1, which shows a relation between somefeatures A and a class label B.
Learning labels from a set of features is a common taskin computational linguistics; for instance, in Shriberg et al (1998), which assumes apre-existing dialogue act segmentation, the labels are dialogue act types, and they arelearned from automatically derived prosodic features.
In this way of using data, onlyone of the variables?the output dialogue act label?is hand-coded.
In the figure, thereal relationship between prosody and dialogue act label is shown on the left; R relatesthe prosodic features A to the output act B.Figure 1Hand-coded target labels are used to train classifiers to automatically predict those labelsfrom features.320Reidsma and Carletta Reliability Measurement without LimitsIn theory, there is one correct label for any given act.
However, in practice hu-man coders disagree, choosing different labels for the same act (sometimes even withdivergences that make one question whether there is one correct answer).
The dataactually available for analysis is shown in the middle of the figure.
Here, the automaticfeatures, A, are the same as before, but there are multiple, possibly differing labelsfor the same act, Bobs, coming from different human annotators.
Finally, on the rightthe figure shows the classifier.
It takes the same prosodic features A and uses them topredict a dialogue act label Bpred on new data, using the relationship learned from theobserved data, RML.
Projects vary in how they choose data from which to build theclassifier when coders disagree, but whatever they do is colored by the observationsthey have available to them.
We often think of reliability assessment as telling us howmuch disagreement there is among the human coders, but the real issue is how theirindividual interpretations of the coding scheme make RML differ from R.There is a problem that arises for anyone using this methodology.
Without the?real?
data, it is impossible to judge how well the learned relationship reflects the realone.
Classification performance for Bpred can only be calculated with respect to the ?ob-served?
data Bobs.
In this article, we surmount this problem by simulating the real worldso that we can measure the differences between this ?observed?
performance and the?real?
performance.
Our simulation uses a Bayesian network (Pearl 1988) to create aninitial, ?real?
data set with 3,000 samples of features (A) and their corresponding targetlabels (B).
For simplicity, we use a single five-valued feature and five possible labels.The relative label frequencies vary between 17% and 25%.
This gives us a small amountof variation around what is essentially equally distributed data.
We corrupt the labels(B) to simulate the ?hand-coded?
observed data (Bobs) corresponding to the output ofa human coder, and then train a neural network constructed using the WEKA toolkit(Witten and Frank 2005) on 2,000 samples from Bobs.
Finally, we calculate the neuralnetwork?s performance twice, using as test data either the remaining 1,000 samples fromBobs or the initial, ?real?
versions of those same 1,000 samples.There are three ways in which we need to vary our simulation in order to be sys-tematic.
The first is in the strength of the relationship between the features the machinelearner takes as input and the target labels, which we achieve simply by changing theprobabilities in the Bayesian network that creates the data set.
In the simulation, wevary the strength of the relationship in eight graded steps.1 The second is in the amountof disagreement we introduce when we create the observed data (Bobs).
We create 200different versions of the hand-coded data that cover a range of values from ?
= 0 to1 We use Cramer?s phi to measure the strength of a relationship.
Cramer?s phi is defined as?c =?
?2(N) ?
dfsmallerwith N the number of samples and dfsmaller the smallest degree of freedom of the two involved variables,and is a measure of association for nominal variables with more than two values.
It can be ?consideredlike a correlation coefficient?
(Aron and Aron 2003) that takes data set size into account and can easilybe derived for a Bayesian network from the priors and the conditional probability tables.
We variedthe strength of the network between ?c = 0.06 and ?c = 0.45.
Following Cohen (1988), for a five-waydistinction Aron and Aron (page 527) would consider 0.06 to represent a small real relationship?thatis, one with not much effect?and 0.3, a large one.
Thus we describe 0.06 as ?weak,?
0.45 as ?verystrong,?
and intermediate points as ?moderate?
and ?strong.?
It is an open question what strengthsof relationships actually occur in computational linguistics data, although there may be no point inlearning a relationship that?s too strong.321Computational Linguistics Volume 34, Number 3Figure 2Machine-learning performance obtained on annotations with noise-like disagreements for (a)weak (?c = 0.06), (b) moderate (?c = 0.20), (c) strong (?c = 0.32), and (d) very strong (?c = 0.45)relationships between the features and labels.?
= 1, by introducing a varying amount of observation errors in the simulated codingprocess.2 The third is in the type of disagreement with which we degrade the real datato create the observed data (Bobs), representing the types of coding errors the humanannotators make.
Again for simplicity, we describe the effects of both random errorsand the overuse of a single coding label.3.
The Case of NoiseFigure 2 shows how a neural network performs when coders make random mistakesin their coding task, that is, for noise-like disagreement, for the cases of (a) weak, (b)moderate, (c) strong, and (d) very strong relationships between the features (A) andlabels (B).
Here, the y axis shows ?accuracy,?
or the percentage of samples in the test2 To calculate ?
for a specific simulated coding we generate two copies of additional ?real?
data that hasnot been used for training or testing, apply the same simulated human annotator to one copy, and asecond annotator who makes the same number of ?mistakes?
to the other copy.
This mimics thecommon practice of having one annotator code the data, with a second annotator coding enough totest the reliability.322Reidsma and Carletta Reliability Measurement without Limitsdata for which the network chooses the correct label.
The x axis varies the amount ofcoder errors in the data to correspond to different ?
values, with the two black linesmarking the values of ?
= 0.67 and ?
= 0.8.Look first at the series depicted as a line.
It shows accuracy measured by usingthe ?observed?
version of the test data, which is how testing is normally done.
For eachrelationship strength, as ?
increases, so does accuracy.
In all cases, at ?
= 0 (that is, whenthe coders fail to agree beyond what one would expect if they were all choosing theirlabels randomly) accuracy is at 20%, which is what one would expect if the classifierwere choosing randomly as well.
For any given ?
value, the stronger the underlyingrelationship, the more benefit the neural network can derive from the data.
Now lookat the other of the two series, depicted as small squares.
It shows accuracy measuredby using the ?real?
version of the data.
Interestingly, the ?real?
performance, that is, thepower of the learned model to predict reality, is higher than performance as measuredagainst the observed data.
This is because for some samples, the classifier?s predictionsare correct, but because the observations contain errors, the test data actually getsthem wrong.
The stronger the relationship in the real data, the more marked this effectbecomes.
The neural network is able to disregard noise-like coding errors at very low?
values simply because the errors contain no patterns for it to learn.4.
The Case of Overusing a LabelNow consider the case where instead of random coding errors, the coder over-usesthe least frequent one of the five labels for B.
Figure 3 shows the results for this kindof coding error.
Remember that in the graphs, the series depicted as a line shows theobserved performance of the classifier?that is, performance as it is usually measured.The two black lines again mark the ?
values of interest (?
= 0.67 and ?
= 0.8).The graphs show an entirely different effect from the one obtained for noise-likecoding errors: For lower values of ?, the observed performance is spuriously high.
Thismakes perfect sense??
is low when the pattern of label overuse is strong, and the neuralnetwork picks it up.
When the observed data is used to test performance, some of thesamples match not because the classifier gets the label right, but because it overusesthe same label as the human coder.
For data with a very strong correlation between theinput features A and the output labels B, the turning point below which performanceis spuriously high occurs at around ?
= 0.55 (Figure 3d), a value the community holdsto be pretty low but which is not unknown in published work.
However, when theunderlying relationship to be learned is moderate or strong (Figures 3b and 3c), thespuriously high results already occur for ?
values commonly held to be tolerable.
Witha weak relationship, the turning point can occur at ?
> 0.8 (Figure 3a).5.
DiscussionOur simulation highlights a danger for current practice in computational linguistics,among other fields.
Overuse of a label is a realistic type of error for human annotatorsto make.
For instance, imagine a coding scheme for dialogue acts that distinguishesbackchannel utterances from utterances which indicate agreement.
In data containingmany utterances where the speech consists of ?Yeah,?
individual coders can easily havea marked bias for either one of these two categories.
Clearly, in actual coding, not alldisagreement will be of one type, but will contain a mix of different systematic andnoise-like errors.
In addition, the underlying relationships that our systems attempt to323Computational Linguistics Volume 34, Number 3Figure 3Machine-learning performance obtained on annotations that suffered from over-coding for (a)weak (?c = 0.06), (b) moderate (?c = 0.20), (c) strong (?c = 0.32), and (d) very strong (?c = 0.45)relationships between the features and labels.learn vary in strength.
This makes discerning the degree of danger more difficult, butdoes not change the substance of our argument.Although the graphs we show are for a specific simulation, the general pattern wedescribe is robust.
In particular, using ?
in place of ?
does not markedly change theresults; neither does increasing or decreasing the data set size.
Our simulations andresults are presented for a machine-learning context.
However, that does not mean thatother types of data use are immune to the problems we describe here.
Other statisticaluses of data will be affected in their own ways by the difference between structural andnoise-like disagreement.6.
ImplicationsAt the moment, much of the effort we devote to reliability measurement as a communityis used to establish one or more overall reliability statistics for our data sets and to argueabout which reliability statistic is most appropriate.
Methodological discussions focuson questions such as how to force annotated data structures into the mathematical formnecessary to calculate ?, or what effects certain aspects of the annotation have on thevalues of some metric rather than on possible uses of the resulting data (Marcu, Amorrortu,324Reidsma and Carletta Reliability Measurement without Limitsand Romera 1999; Di Eugenio and Glass 2004; Artstein and Poesio 2005).
Computationallinguists are of course aware that no overall reliability measure can give a completestory, but often fail to spend time analyzing coder disagreements further.
Unfortunately,our results suggest that current practice is insufficient, at least where the data is destinedto be input for a machine-learning process and quite possibly for other data uses aswell.
This complements observations of Artstein and Poesio: Besides the fact that manydifferent ways of calculating reliability metrics lead to different values, which makescomparing them to a threshold difficult (Artstein and Poesio in press), the very ideaof having any such single threshold in the first place turns out to be impossible tohold.
Instead of worrying about exactly how much disagreement there is in a dataset and how to measure it, we should be looking at the form the disagreement takes.A headline measurement, no matter how it is expressed, will not show the differencebetween noise-like and systematic disagreement, but this difference can be critical forestablishing whether or not a data set is fit for the purpose for which it is intended.To tease out what sort of disagreement a data set contains, Krippendorff suggestscalculating odd-man-out and per-class reliability to find out which class distinctions areproblematic (1980, page 150).
Bayerl and Paul (2007) discuss methods for determiningwhich factors (schema changes, coding team changes, etc.)
were involved in causingpoor annotation quality.
Wiebe, Bruce, and O?Hara (1999) suggest looking at the mar-ginals and how they differ between coders to find indications of whether disagreementsare caused by systematic bias (as opposed to being random) and in which classesthey occur.
Although clearly useful techniques, none of these diagnostics is specificallydesigned to address the needs of machine learners which are designed to recognize pat-terns.
Overusing a label is just one simple example of a type of systematic disagreementthat adds unwanted patterns that a machine learner can find.
Any spurious patterncould be a problem.
For this reason, we should be looking specifically for patterns inthe disagreement itself.Our suggestion for one possible diagnostic technique is based on the followingobservation: If the disagreements between two coders contain no pattern, any test forassociation or correlation, when performed on only the disagreed items, should showno relation between the labels assigned by the two coders.
For certain patterns in thedisagreement, however, a correlation would show up.
(To see this, consider the casewhere one coder tends to label rhetorical questions as yes/no-questions and the othercoder assigns both labels correctly: If this happens often enough, tests for associationwould come up with a relation between the labels for the two coders for the disagreeditems.)
If the test shows a correlation, the disagreements add patterns for the machinelearner to find.
Unfortunately, the converse does not necessarily hold: It is possiblethat not all patterns that could be picked up by a machine learner will show up incorrelations between disagreed items, for example because the amount of multiply-annotated data is too small.
The computational linguistics community therefore needsto develop additional diagnostics for patterns in the coder disagreements.It should go without saying that analysts will benefit from keeping how theyintend to use the data firmly in mind at all times.
As Krippendorff (2004, page 429)recommends, one should test reliability for the ?distinctions that matter?
and perform?suitable experiments of the effects of unreliable data on the conclusions.?
Patternsfound for an overall coding scheme will not always affect every possible data use.
Forinstance, we often build classifiers not for complete coding schemes, but for some subsetof the labels or some ?class map?
that transforms the scheme into a smaller set of classes.In these cases, what is important is disagreement for the subset or transformation, notthe entire scheme.
Similarly, where classifier performance is reported per class, the325Computational Linguistics Volume 34, Number 3reliability for that particular label will be the most important.
Finally, different machine-learning algorithms may react differently to different kinds of patterns in the data andto combinations of patterns in different relative strengths.
In complicated cases, perhapsthe safest way to assess whether or not there is a problem with systematic disagreementis to run a simulation like the one we have reported but with the kind and scale ofdisagreement suspected of the data, and to use that to estimate the possible effects ofunreliable data on the performance of machine-learning algorithms.AcknowledgmentsWe thank Rieks op den Akker, Ron Artstein,and Bonnie Webber for discussions that havehelped us frame this article, as well as theanonymous reviewers for their thoughtfulcomments.
This work is supported by theEuropean IST Programme Project FP6-033812(AMIDA, publication 36).
This article onlyreflects the authors?
views and fundingagencies are not liable for any use that maybe made of the information contained herein.ReferencesAron, Arthur and Elaine N. Aron.
2003.Statistics for Psychology.
Prentice Hall,Upper Saddle River, NJ.Artstein, Ron and Massimo Poesio.
2005.Bias decreases in proportion to the numberof annotators.
In Proceedings of FG-MoL2005, pages 141?150, Edinburgh.Artstein, Ron and Massimo Poesio.
In press.Inter-coder agreement for computationallinguistics.
Computational Linguistics.Bayerl, Petra Saskia and Karsten IngmarPaul.
2007.
Identifying sources ofdisagreement: Generalizability theoryin manual annotation studies.Computational Linguistics, 33(1):3?8.Carletta, Jean C. 1996.
Assessing agreementon classification tasks: The kappa statistic.Computational Linguistics, 22(2):249?254.Cohen, Jacob.
1960.
A coefficient ofagreement for nominal scales.
Educationaland Psychological Measurement, 20(1):37?46.Cohen, Jacob.
1988.
Statistical power analysisfor the behavioral sciences, 2nd edition.Lawrence Erlbaum, Hillsdale, NJ.Craggs, Richard and Mary McGee Wood.2005.
Evaluating discourse and dialoguecoding schemes.
Computational Linguistics,31(3):289?296.Di Eugenio, Barbara and Michael Glass.2004.
The kappa statistic: A second look.Computational Linguistics, 30(1):95?101.Galley, Michel, Kathleen McKeown, JuliaHirschberg, and Elizabeth Shriberg.2004.
Identifying agreement anddisagreement in conversational speech:Use of Bayesian networks to modelpragmatic dependencies.
In Proceedingsof the 42nd Meeting of the Association forComputational Linguistics (ACL?04),pages 669?676, Barcelona.Krippendorff, Klaus.
1980.
Content Analysis:An Introduction to its Methodology,volume 5 of The Sage CommText Series.Sage Publications, London.Krippendorff, Klaus.
2004.
Reliabilityin content analysis.
Some commonmisconceptions and recommendations.Human Communication Research,30(3):411?433.Marcu, Daniel, Estibaliz Amorrortu, andMagdalena Romera.
1999.
Experiments inconstructing a corpus of discourse trees.In Marilyn Walker, editor, TowardsStandards and Tools for Discourse Tagging:Proceedings of the Workshop.
Associationfor Computational Linguistics, Somerset,NJ, pages 48?57.Pearl, Judea.
1988.
Probabilistic Reasoning inIntelligent Systems: Networks of PlausibleInference.
Morgan Kaufmann PublishersInc., San Francisco, CA.Reithinger, Norbert and Michael Kipp.1998.
Large scale dialogue annotation inVerbmobil.
In Workshop Proceedings ofESSLLI 98, pages 1?6, Saarbru?cken.Shriberg, Elizabeth, Rebecca Bates, PaulTaylor, Andreas Stolcke, Daniel Jurafsky,Klaus Ries, Noah Coccaro, RachelMartin, Marie Meteer, and Carol VanEss-Dykema.
1998.
Can prosody aid theautomatic classification of dialog acts inconversational speech?
Language andSpeech, 41(3-4):443?492.Wiebe, Janyce M., Rebecca F. Bruce, andThomas P. O?Hara.
1999.
Developmentand use of a gold-standard data setfor subjectivity classifications.
InProceedings of the 37th Annual Meetingof the Association for ComputationalLinguistics, pages 246?253, Morristown, NJ.Witten, Ian H. and Eibe Frank.
2005.
DataMining: Practical Machine Learning Tools andTechniques, 2nd edition.
Morgan Kaufmann,San Francisco, CA.326This article has been cited by:1.
Ron Artstein, Massimo Poesio.
2008.
Inter-Coder Agreement for Computational Linguistics.Computational Linguistics 34:4, 555-596.
[Abstract] [PDF] [PDF Plus]
