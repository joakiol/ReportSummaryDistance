Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 10?18,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsIntegrating Knowledge for Subjectivity Sense LabelingYaw Gyamfi and Janyce WiebeUniversity of Pittsburgh{anti,wiebe}@cs.pitt.eduRada MihalceaUniversity of North Texasrada@cs.unt.eduCem AkkayaUniversity of Pittsburghcem@cs.pitt.eduAbstractThis paper introduces an integrative approachto automatic word sense subjectivity annota-tion.
We use features that exploit the hier-archical structure and domain information inlexical resources such as WordNet, as well asother types of features that measure the sim-ilarity of glosses and the overlap among setsof semantically related words.
Integrated in amachine learning framework, the entire set offeatures is found to give better results than anyindividual type of feature.1 IntroductionAutomatic extraction of opinions, emotions, andsentiments in text (subjectivity analysis) to supportapplications such as product review mining, sum-marization, question answering, and information ex-traction is an active area of research in NLP.Many approaches to opinion, sentiment, and sub-jectivity analysis rely on lexicons of words that maybe used to express subjectivity.
However, words mayhave both subjective and objective senses, which isa source of ambiguity in subjectivity and sentimentanalysis.
We show that even words judged in pre-vious work to be reliable clues of subjectivity havesignificant degrees of subjectivity sense ambiguity.To address this ambiguity, we present a methodfor automatically assigning subjectivity labels toword senses in a taxonomy, which uses new featuresand integrates more diverse types of knowledge thanin previous work.
We focus on nouns, which arechallenging and have received less attention in auto-matic subjectivity and sentiment analysis.A common approach to building lexicons for sub-jectivity analysis is to begin with a small set ofseeds which are prototypically subjective (or posi-tive/negative, in sentiment analysis), and then fol-low semantic links in WordNet-like resources.
Byfar, the emphasis has been on horizontal relations,such as synonymy and antonymy.
Exploiting verticallinks opens the door to taking into account the infor-mation content of ancestor concepts of senses withknown and unknown subjectivity.
We develop novelfeatures that measure the similarity of a target wordsense with a seed set of senses known to be sub-jective, where the similarity between two conceptsis determined by the extent to which they share in-formation, measured by the information content as-sociated with their least common subsumer (LCS).Further, particularizing the LCS features to domaingreatly reduces calculation while still maintainingeffective features.We find that our new features do lead to signif-icant improvements over methods proposed in pre-vious work, and that the combination of all featuresgives significantly better performance than any sin-gle type of feature alone.We also ask, given that there are many approachesto finding subjective words, if it would make sensefor word- and sense-level approaches to work in tan-dem, or should we best view them as competing ap-proaches?
We give evidence suggesting that firstidentifying subjective words and then disambiguat-ing their senses would be an effective approach tosubjectivity sense labeling.10There are several motivations for assigning sub-jectivity labels to senses.
First, (Wiebe and Mi-halcea, 2006) provide evidence that word sense la-bels, together with contextual subjectivity analysis,can be exploited to improve performance in wordsense disambiguation.
Similarly, given subjectivitysense labels, word-sense disambiguation may poten-tially help contextual subjectivity analysis.
In addi-tion, as lexical resources such as WordNet are devel-oped further, subjectivity labels would provide prin-cipled criteria for refining word senses, as well as forclustering similar meanings to create more course-grained sense inventories.For many opinion mining applications, polarity(positive, negative) is also important.
The overallframework we envision is a layered approach: clas-sifying instances as objective or subjective, and fur-ther classifying the subjective instances by polar-ity.
Decomposing the problem into subproblems hasbeen found to be effective for opinion mining.
Thispaper addresses the first of these subproblems.2 BackgroundWe adopt the definitions of subjective and objectivefrom Wiebe and Mihalcea (2006) (hereafter WM).Subjective expressions are words and phrases beingused to express opinions, emotions, speculations,etc.
WM give the following examples:His alarm grew.He absorbed the information quickly.UCC/Disciples leaders roundly condemned theIranian President?s verbal assault on Israel.What?s the catch?Polarity (also called semantic orientation) is alsoimportant to NLP applications in sentiment analysisand opinion extraction.
In review mining, for exam-ple, we want to know whether an opinion about aproduct is positive or negative.
Even so, we believethere are strong motivations for a separate subjec-tive/objective (S/O) classification as well.First, expressions may be subjective but not haveany particular polarity.
An example given by (Wil-son et al, 2005) is Jerome says the hospital feelsno different than a hospital in the states.
An NLPapplication system may want to find a wide rangeof private states attributed to a person, such as theirmotivations, thoughts, and speculations, in additionto their positive and negative sentiments.Second, distinguishing S and O instances has of-ten proven more difficult than subsequent polarityclassification.
Researchers have found this at vari-ous levels of analysis, including the manual anno-tation of phrases (Takamura et al, 2006), sentimentclassification of phrases (Wilson et al, 2005), sen-timent tagging of words (Andreevskaia and Bergler,2006b), and sentiment tagging of word senses (Esuliand Sebastiani, 2006a).
Thus, effective methods forS/O classification promise to improve performancefor sentiment classification.
In fact, researchers insentiment analysis have realized benefits by decom-posing the problem into S/O and polarity classifica-tion (Yu and Hatzivassiloglou, 2003; Pang and Lee,2004; Wilson et al, 2005; Kim and Hovy, 2006).One reason is that different features may be relevantfor the two subproblems.
For example, negation fea-tures are more important for polarity classificationthan for subjectivity classification.Note that some of our features require verticallinks that are present in WordNet for nouns andverbs but not for other parts of speech.
Thus we ad-dress nouns (leaving verbs to future work).
Thereare other motivations for focusing on nouns.
Rela-tively little work in subjectivity and sentiment anal-ysis has focused on subjective nouns.
Also, a study(Bruce and Wiebe, 1999) showed that, of the majorparts of speech, nouns are the most ambiguous withrespect to the subjectivity of their instances.Turning to word senses, we adopt the definitionsfrom WM.
First, subjective: ?Classifying a sense asS means that, when the sense is used in a text or con-versation, we expect it to express subjectivity; wealso expect the phrase or sentence containing it tobe subjective [WM, pp.
2-3].
?In WM, it is noted that sentences containing ob-jective senses may not be objective, as in the sen-tence Will someone shut that darn alarm off?
Thus,objective senses are defined as follows: ?Classifyinga sense as O means that, when the sense is used in atext or conversation, we do not expect it to expresssubjectivity and, if the phrase or sentence containingit is subjective, the subjectivity is due to somethingelse [WM, p 3].
?The following subjective examples are given in11WM:His alarm grew.alarm, dismay, consternation ?
(fear resulting from the aware-ness of danger)=> fear, fearfulness, fright ?
(an emotion experienced in an-ticipation of some specific pain or danger (usually accompa-nied by a desire to flee or fight))What?s the catch?catch ?
(a hidden drawback; ?it sounds good but what?s thecatch??
)=> drawback ?
(the quality of being a hindrance; ?hepointed out all the drawbacks to my plan?
)The following objective examples are given in WM:The alarm went off.alarm, warning device, alarm system ?
(a device that signals theoccurrence of some undesirable event)=> device ?
(an instrumentality invented for a particular pur-pose; ?the device is small enough to wear on your wrist?
; ?adevice intended to conserve water?
)He sold his catch at the market.catch, haul ?
(the quantity that was caught; ?the catch was only10 fish?
)=> indefinite quantity ?
(an estimated quantity)WM performed an agreement study and reportthat good agreement (?=0.74) can be achieved be-tween human annotators labeling the subjectivity ofsenses.
For a similar task, (Su and Markert, 2008)also report good agreement.3 Related WorkMany methods have been developed for automati-cally identifying subjective (opinion, sentiment, at-titude, affect-bearing, etc.)
words, e.g., (Turney,2002; Riloff and Wiebe, 2003; Kim and Hovy, 2004;Taboada et al, 2006; Takamura et al, 2006).Five groups have worked on subjectivity sense la-beling.
WM and Su and Markert (2008) (hereafterSM) assign S/O labels to senses, while Esuli and Se-bastiani (hereafter ES) (2006a; 2007), Andreevskaiaand Bergler (hereafter AB) (2006b; 2006a), and(Valitutti et al, 2004) assign polarity labels.WM, SM, and ES have evaluated their systemsagainst manually annotated word-sense data.
WM?sannotations are described above; SM?s are similar.In the scheme ES use (Cerini et al, 2007), sensesare assigned three scores, for positivity, negativity,and neutrality.
There is no unambiguous mappingbetween the labels of WM/SM and ES, first becauseWM/SM use distinct classes and ES use numericalratings, and second because WM/SM distinguish be-tween objective senses on the one hand and neutralsubjective senses on the other, while those are bothneutral in the scheme used by ES.WM use an unsupervised corpus-based approach,in which subjectivity labels are assigned to wordsenses based on a set of distributionally similarwords in a corpus annotated with subjective expres-sions.
SM explore methods that use existing re-sources that do not require manually annotated data;they also implement a supervised system for com-parison, which we will call SMsup.
The other threegroups start with positive and negative seed sets andexpand them by adding synonyms and antonyms,and traversing horizontal links in WordNet.
AB, ES,and SMsup additionally use information containedin glosses; AB also use hyponyms; SMsup also usesrelation and POS features.
AB perform multipleruns of their system to assign fuzzy categories tosenses.
ES use a semi-supervised, multiple-classifierlearning approach.
In a later paper, (Esuli and Se-bastiani, 2007), ES again use information in glosses,applying a random walk ranking algorithm to agraph in which synsets are linked if a member ofthe first synset appears in the gloss of the second.Like ES and SMsup, we use machine learning, butwith more diverse sources of knowledge.
Further,several of our features are novel for the task.
TheLCS features (Section 6.1) detect subjectivity bymeasuring the similarity of a candidate word sensewith a seed set.
WM also use a similarity measure,but as a way to filter the output of a measure of distri-butional similarity (selecting words for a given wordsense), not as we do to cumulatively calculate thesubjectivity of a word sense.
Another novel aspectof our similarity features is that they are particular-ized to domain, which greatly reduces calculation.The domain subjectivity LCS features (Section 6.2)are also novel for our task.
So is augmenting seedsets with monosemous words, for greater coveragewithout requiring human intervention or sacrificingquality.
Note that none of our features as we specif-ically define them has been used in previous work;combining them together, our approach outperformsprevious approaches.124 Lexicon and AnnotationsWe use the subjectivity lexicon of (Wiebe and Riloff,2005)1 both to create a subjective seed set and tocreate the experimental data sets.
The lexicon is alist of words and phrases that have subjective uses,though only word entries are used in this paper (i.e.,we do not address phrases at this point).
Some en-tries are from manually developed resources, includ-ing the General Inquirer, while others were derivedfrom corpora using automatic methods.Through manual review and empirical testing ondata, (Wiebe and Riloff, 2005) divided the clues intostrong (strongsubj) and weak (weaksubj) subjectiv-ity clues.
Strongsubj clues have subjective meaningswith high probability, and weaksubj clues have sub-jective meanings with lower probability.To support our experiments, we annotated thesenses2 of polysemous nouns selected from the lex-icon, using WM?s annotation scheme described inSection 2.
Due to time constraints, only some of thedata was labeled through consensus labeling by twoannotators; the rest was labeled by one annotator.Overall, 2875 senses for 882 words were anno-tated.
Even though all are senses of words from thesubjectivity lexicon, only 1383 (48%) of the sensesare subjective.The words labeled strongsubj are in fact less am-biguous than those labeled weaksubj in our analysis,thus supporting the reliability classifications in thelexicon.
55% (1038/1924) of the senses of strong-subj words are subjective, while only 36% (345/951)of the senses of weaksubj words are subjective.For the analysis in Section 7.3, we form subsetsof the data annotated here to test performance of ourmethod on different data compositions.5 Seed SetsBoth subjective and objective seed sets are used todefine the features described below.
For seeds, alarge number is desirable for greater coverage, al-though high quality is also important.
We begin tobuild our subjective seed set by adding the monose-mous strongsubj nouns of the subjectivity lexicon(there are 397 of these).
Since they are monose-mous, they pose no problem of sense ambiguity.
We1Available at http://www.cs.pitt.edu/mpqa2In WordNet 2.0then expand the set with their hyponyms, as theywere found useful in previous work by AB (2006b;2006a).
This yields a subjective seed set of 645senses.
After removing the word senses that belongto the same synset, so that only one word sense persynset is left, we ended up with 603 senses.To create the objective seed set, two annotatorsmanually annotated 800 random senses from Word-Net, and selected for the objective seed set the onesthey both agreed are clearly objective.
This createsan objective seed set of 727.
Again we removedmultiple senses from the same synset leaving us with722.
The other 73 senses they annotated are addedto the mixed data set described below.
As this sam-pling shows, WordNet nouns are highly skewed to-ward objective senses, so finding an objective seedset is not difficult.6 Features6.1 Sense Subjectivity LCS FeatureThis feature measures the similarity of a target sensewith members of the subjective seed set.
Here, sim-ilarity between two senses is determined by the ex-tent to which they share information, measured byusing the information content associated with theirleast common subsumer.
For an intuition behind thisfeature, consider this example.
In WordNet, the hy-pernym of the ?strong criticism?
sense of attack iscriticism.
Several other negative subjective sensesare descendants of criticism, including the relevantsenses of fire, thrust, and rebuke.
Going up onemore level, the hypernym of criticism is the ?ex-pression of disapproval?
meaning of disapproval,which has several additional negative subjective de-scendants, such as the ?expression of opposition anddisapproval?
sense of discouragement.
Our hypoth-esis is that the cases where subjectivity is preservedin the hypernym structure, or where hypernyms dolead from subjective senses to others, are the onesthat have the highest least common subsumer scorewith the seed set of known subjective senses.We calculate similarity using the information-content based measure proposed in (Resnik, 1995),as implemented in the WordNet::Similarity pack-age (using the default option in which LCS valuesare computed over the SemCor corpus).3 Given a3http://search.cpan.org/dist/WordNet-Similarity/13taxonomy such as WordNet, the information con-tent associated with a concept is determined as thelikelihood of encountering that concept, defined as?log(p(C)), where p(C) is the probability of see-ing concept C in a corpus.
The similarity betweentwo concepts is then defined in terms of informationcontent as: LCSs(C1, C2) = max[?log(p(C))],where C is the concept that subsumes both C1 andC2 and has the highest information content (i.e., it isthe least common subsumer (LCS)).For this feature, a score is assigned to a targetsense based on its semantic similarity to the mem-bers of a seed set; in particular, the maximum suchsimilarity is used.For a target sense t and a seed set S, we couldhave used the following score:Score(t, S) = maxs?SLCSs(t, s)However, several researchers have noted that sub-jectivity may be domain specific.
A version ofWordNet exists, WordNet Domains (Gliozzo et al,2005), which associates each synset with one of thedomains in the Dewey Decimal library classifica-tion.
After sorting our subjective seed set into differ-ent domains, we observed that over 80% of the sub-jective seed senses are concentrated in six domains(the rest are distributed among 35 domains).Thus, we decided to particularize the semanticsimilarity feature to domain, such that only the sub-set of the seed set in the same domain as the tar-get sense is used to compute the feature.
This in-volves much less calculation, as LCS values are cal-culated only with respect to a subset of the seed set.We hypothesized that this would still be an effec-tive feature, while being more efficient to calculate.This will be important when this method is appliedto large resources such as the entire WordNet.Thus, for seed set S and target sense t which isin domain D, the feature is defined as the followingscore:SenseLCSscore(t,D, S) = maxd?D?SLCSs(t, d)The seed set is a parameter, so we could havedefined a feature reflecting similarity to the objec-tive seed set as well.
Since WordNet is alreadyhighly skewed toward objective noun senses, anynaive classifier need only guess the majority classfor high accuracy for the objective senses.
We in-cluded only a subjective feature to put more empha-sis on the subjective senses.
In the future, featurescould be defined with respect to objectivity, as wellas polarity and other properties of subjectivity.6.2 Domain Subjectivity LCS ScoreWe also include a feature reflecting the subjectivityof the domain of the target sense.
Domains areassigned scores as follows.
For domain D and seedset S:DomainLCSscore(D,S) =aved?D?SMemLCSscore(d,D, S)where:MemLCSscore(d,D, S) =maxdi?D?S,di 6=dLCSs(d, di)The value of this feature for a sense is the scoreassigned to that sense?s domain.6.3 Common Related SensesThis feature is based on the intersection between theset of senses related (via WordNet relations) to thetarget sense and the set of senses related to membersof a seed set.
First, for the target sense and eachmember of the seed set, a set of related senses isformed consisting of its synonyms, antonyms and di-rect hypernyms as defined by WordNet.
For a senses, R(s) is s together with its related senses.Then, given a target sense t and a seed set S wecompute an average percentage overlap as follows:RelOverlap(t, S) =?si?S|R(t)?R(si)|max (|R(t)|,|R(si)|)|S|The value of a feature is its score.
Two featuresare included in the experiments below, one for eachof the subjective and objective seed sets.6.4 Gloss-based featuresThese features are Lesk-style features (Lesk, 1986)that exploit overlaps between glosses of target andseed senses.
We include two types in our work.6.4.1 Average Percentage Gloss OverlapFeaturesFor a sense s, gloss(s) is the set of stems in thegloss of s (excluding stop words).
Then, given a tar-14get sense t and a seed set S, we compute an averagepercentage overlap as follows:GlOverlap(t, S) =?si?S|gloss(t)?
?r?R(si)gloss(r)|max (|gloss(t)|,|?r?R(si)gloss(r)|)|S|As above, R(s) is considered for each seed senses, but now only the target sense t is considered, notR(t).
We did this because we hypothesized that thegloss can provide sufficient context for a given targetsense, so that the addition of related words is notnecessary.We include two features, one for each of the sub-jective and objective seed sets.6.4.2 Vector Gloss Overlap FeaturesFor this feature we also consider overlaps ofstems in glosses (excluding stop words).
The over-laps considered are between the gloss of the tar-get sense t and the glosses of R(s) for all s in aseed set (for convenience, we will refer to these asseedRelationSets).A vector of stems is created, one for each stem(excluding stop words) that appears in a gloss ofa member of seedRelationSets.
If a stem in thegloss of the target sense appears in this vector, thenthe vector entry for that stem is the total count ofthat stem in the glosses of the target sense and allmembers of seedRelationSets.A feature is created for each vector entry whosevalue is the count at that position.
Thus, these fea-tures consider counts of individual stems, rather thanaverage proportions of overlaps, as for the previoustype of gloss feature.Two vectors of features are used, one where theseed set is the subjective seed set, and one where itis the objective seed set.6.5 SummaryIn summary, we use the following features (here, SSis the subjective seed set and OS is the objectiveone).1.
SenseLCSscore(t,D, SS)2.
DomainLCSscore(D,SS)3.
RelOverlap(t, SS)4.
RelOverlap(t, OS)5.
GlOverlap(t, SS)6.
GlOverlap(t, OS)Features Acc P R FAll 77.3 72.8 74.3 73.5Standalone Ablation ResultsAll 77.3 72.8 74.3 73.5LCS 68.2 69.3 44.2 54.0Gloss vector 74.3 71.2 68.5 69.8Overlaps 69.4 75.8 40.6 52.9Leave-One-Out Ablation ResultsAll 77.3 72.8 74.3 73.5LCS 75.2 70.9 70.6 70.7Gloss vector 75.0 74.4 61.8 67.5Overlaps 74.8 71.9 73.8 72.8Table 1: Results for the mixed corpus (2354 senses,57.82% O))7.
Vector of gloss words (SS)8.
Vector of gloss words (OS)7 ExperimentsWe perform 10-fold cross validation experimentson several data sets, using SVM light (Joachims,1999)4 under its default settings.Based on our random sampling of WordNet, itappears that WordNet nouns are highly skewed to-ward objective senses.
(Esuli and Sebastiani, 2007)argue that random sampling from WordNet wouldyield a corpus mostly consisting of objective (neu-tral) senses, which would be ?pretty useless as abenchmark for testing derived lexical resources foropinion mining [p.
428].?
So, they use a mixture ofsubjective and objective senses in their data set.To create a mixed corpus for our task, we anno-tated a second random sample from WordNet (whichis as skewed as the previously mentioned one).
Weadded together all of the senses of words in the lexi-con which we annotated, the leftover senses from theselection of objective seed senses, and this new sam-ple.
We removed duplicates, multiple senses fromthe same synset, and any senses belonging to thesame synset in either of the seed sets.
This resultedin a corpus of 2354 senses, 993 (42.18%) of whichare subjective and 1361 (57.82%) of which are ob-jective.The results with all of our features on this mixedcorpus are given in Row 1 of Table 1.
In Table 1, the4http://svmlight.joachims.org/15first column identifies the features, which in this caseis all of them.
The next three columns show overallaccuracy, and precision and recall for finding sub-jective senses.
The baseline accuracy for the mixeddata set (guessing the more frequent class, which isobjective) is 57.82%.
As the table shows, the accu-racy is substantially above baseline.57.1 Analysis and DiscussionIn this section, we seek to gain insights by perform-ing ablation studies, evaluating our method on dif-ferent data compositions, and comparing our resultsto previous results.7.2 Ablation StudiesSince there are several features, we divided theminto sets for the ablation studies.
The vector-of-gloss-words features are the most similar to onesused in previous work.
Thus, we opted to treatthem as one ablation group (Gloss vector).
TheOverlaps group includes the RelOverlap(t, SS),RelOverlap(t, OS), GlOverlap(t, SS), andGlOverlap(t, OS) features.
Finally, the LCSgroup includes the SenseLCSscore and theDomainLCSscore features.There are two types of ablation studies.
In thefirst, one group of features at a time is included.Those results are in the middle section of Table 1.Thus, for example, the row labeled LCS in this sec-tion is for an experiment using only the LCS fea-tures.
In comparison to performance when all fea-tures are used, F-measure for the Overlaps and LCSablations is significantly different at the p < .01level, and, for the Gloss Vector ablation, it is sig-nificantly different at the p = .052 level (one-tailedt-test).
Thus, all of the features together have betterperformance than any single type of feature alone.In the second type of ablation study, we use allthe features minus one group of features at a time.The results are in the bottom section of Table 1.Thus, for example, the row labeled LCS in this sec-tion is for an experiment using all but the LCS fea-tures.
F-measures for LCS and Gloss vector are sig-nificantly different at the p = .056 and p = .014 lev-els, respectively.
However, F-measure for the Over-laps ablation is not significantly different (p = .39).5Note that, because the majority class is O, baseline recall(and thus F-measure) is 0.Data (#senses) Acc P R Fmixed (2354 57.8% O) 77.3 72.8 74.3 73.5strong+weak (1132) 77.7 76.8 78.9 77.8weaksubj (566) 71.3 70.3 71.1 70.7strongsubj (566) 78.6 78.8 78.6 78.7Table 2: Results for different data sets (all are 50% S,unless otherwise notes)These results provide evidence that LCS and Glossvector are better together than either of them alone.7.3 Results on Different Data SetsSeveral methods have been developed for identify-ing subjective words.
Perhaps an effective strategywould be to begin with a word-level subjectivity lex-icon, and then perform subjectivity sense labelingto sort the subjective from objective senses of thosewords.
We also wondered about the relative effec-tiveness of our method on strongsubj versus weak-subj clues.To answer these questions, we apply the fullmodel (again in 10-fold cross validation experi-ments) to data sets composed of senses of polyse-mous words in the subjectivity lexicon.
To supportcomparison, all of the data sets in this section havea 50%-50% objective/subjective distribution.6 Theresults are presented in Table 2.For comparison, the first row repeats the resultsfor the mixed corpus from Table 1.
The secondrow shows results for a corpus of senses of a mix-ture of strongsubj and weaksubj words.
The corpuswas created by selecting a mixture of strongsubj andweaksubj words, extracting their senses and the S/Olabels applied to them in Section 4, and then ran-domly removing senses of the more frequent classuntil the distribution is uniform.
We see that theresults on this corpus are better than on the mixeddata set, even though the baseline accuracy is lowerand the corpus is smaller.
This supports the ideathat an effective strategy would be to first identifyopinion-bearing words, and then apply our methodto those words to sort out their subjective and objec-tive senses.The third row shows results for a weaksubj subset6As with the mixed data set, we removed from these datasets multiple senses from the same synset and any senses in thesame synset in either of the seed sets.16Method P R FOur method 56.8 66.0 61.1WM, 60% recall 44.0 66.0 52.8SentiWordNet mapping 60.0 17.3 26.8Table 3: Results for WM Corpus (212 senses, 76% O)Method A P R FOur Method 81.3% 60.3% 63.3% 61.8%SM CV* 82.4% 70.8% 41.1% 52.0%SM SL* 78.3% 53.0% 57.4% 54.9%Table 4: Results for SM Corpus (484 senses, 76.9% O)of the strong+weak corpus and the fourth shows re-sults for a strongsubj subset that is of the same size.As expected, the results for the weaksubj sensesare lower while those for the strongsubj senses arehigher, as weaksubj clues are more ambiguous.7.4 Comparisons with Previous WorkWM and SM address the same task as we do.
Tocompare our results to theirs, we apply our fullmodel (in 10-fold cross validation experiments) totheir data sets.7Table 3 has the WM data set results.
WM ranktheir senses and present their results in the form ofprecision recall curves.
The second row of Table 3shows their results at the recall level achieved by ourmethod (66%).
Their precision at that level is sub-stantially below ours.Turning to ES, to create S/O annotations, we ap-plied the following heuristic mapping (which is alsoused by SM for the purpose of comparison): anysense for which the sum of positive and negativescores is greater than or equal to 0.5 is S, otherwiseit is O.
We then evaluate the mapped tags against thegold standard of WM.
The results are in Row 3 ofTable 3.
Note that this mapping is not fair to Sen-tiWordNet, as the tasks are quite different, and wedo not believe any conclusions can be drawn.
Weinclude the results to eliminate the possibility thattheir method is as good ours on our task, despite thedifferences between the tasks.Table 4 has the results for the noun subset of SM?s7The WM data set is available athttp://www.cs.pitt.edu/www.cs.pitt.edu/?wiebe.
ES appliedtheir method in (2006b) to WordNet, and made the resultsavailable as SentiWordNet at http://sentiwordnet.isti.cnr.it/.data set, which is the data set used by ES, reanno-tated by SM.
CV* is their supervised system andSL* is their best non-supervised one.
Our methodhas higher F-measure than the others.8 Note that thefocus of SM?s work is not supervised machine learn-ing.8 ConclusionsIn this paper, we introduced an integrative approachto automatic subjectivity word sense labeling whichcombines features exploiting the hierarchical struc-ture and domain information of WordNet, as wellas similarity of glosses and overlap among setsof semantically related words.
There are severalcontributions.
First, we learn several things.
Wefound (in Section 4) that even reliable lists of sub-jective (opinion-bearing) words have many objec-tive senses.
We asked if word- and sense-level ap-proaches could be used effectively in tandem, andfound (in Section 7.3) that an effective strategy is tofirst identify opinion-bearing words, and then applyour method to sort out their subjective and objectivesenses.
We also found (in Section 7.2) that the entireset of features gives better results than any individ-ual type of feature alone.Second, several of the features are novel forour task, including those exploiting the hierarchicalstructure of a lexical resource, domain information,and relations to seed sets expanded with monose-mous senses.Finally, the combination of our particular featuresis effective.
For example, on senses of words froma subjectivity lexicon, accuracies range from 20 to29 percentage points above baseline.
Further, ourcombination of features outperforms previous ap-proaches.AcknowledgmentsThis work was supported in part by National Sci-ence Foundation awards #0840632 and #0840608.The authors are grateful to Fangzhong Su and KatjaMarkert for making their data set available, and tothe three paper reviewers for their helpful sugges-tions.8We performed the same type of evaluation as in SM?s paper.That is, we assign a subjectivity label to one word sense for eachsynset, which is the same as applying a subjectivity label to asynset as a whole as done by SM.17ReferencesAlina Andreevskaia and Sabine Bergler.
2006a.
Miningwordnet for a fuzzy sentiment: Sentiment tag extrac-tion from wordnet glosses.
In Proceedings of the 11rdConference of the European Chapter of the Associa-tion for Computational Linguistics.Alina Andreevskaia and Sabine Bergler.
2006b.
Sen-timent tag extraction from wordnet glosses.
In Pro-ceedings of 5th International Conference on LanguageResources and Evaluation.Rebecca Bruce and Janyce Wiebe.
1999.
Recognizingsubjectivity: A case study of manual tagging.
NaturalLanguage Engineering, 5(2):187?205.S.
Cerini, V. Campagnoni, A. Demontis, M. Formentelli,and C. Gandini.
2007.
Micro-wnop: A gold standardfor the evaluation of automatically compiled lexical re-sources for opinion mining.
In Language resourcesand linguistic theory: Typology, second language ac-quisition, English linguistics.
Milano.Andrea Esuli and Fabrizio Sebastiani.
2006a.
Determin-ing term subjectivity and term orientation for opinionmining.
In 11th Meeting of the European Chapter ofthe Association for Computational Linguistics.Andrea Esuli and Fabrizio Sebastiani.
2006b.
Senti-WordNet: A publicly available lexical resource foropinion mining.
In Proceedings of the 5th Conferenceon Language Resources and Evaluation, Genova, IT.Andrea Esuli and Fabrizio Sebastiani.
2007.
PageRank-ing wordnet synsets: An application to opinion min-ing.
In Proceedings of the 45th Annual Meeting of theAssociation of Computational Linguistics, pages 424?431, Prague, Czech Republic, June.A.
Gliozzo, C. Strapparava, E. d?Avanzo, andB.
Magnini.
2005.
Automatic acquisition ofdomain specific lexicons.
Tech.
report, IRST, Italy.T.
Joachims.
1999.
Making large-scale SVM learningpractical.
In B. Scholkopf, C. Burgess, and A. Smola,editors, Advances in Kernel Methods ?
Support VectorLearning, Cambridge, MA.
MIT-Press.Soo-Min Kim and Eduard Hovy.
2004.
Determining thesentiment of opinions.
In Proceedings of the TwentiethInternational Conference on Computational Linguis-tics, pages 1267?1373, Geneva, Switzerland.Soo-Min Kim and Eduard Hovy.
2006.
Identifyingand analyzing judgment opinions.
In Proceedings ofEmpirical Methods in Natural Language Processing,pages 200?207, New York.M.E.
Lesk.
1986.
Automatic sense disambiguation us-ing machine readable dictionaries: How to tell a pinecone from an ice cream cone.
In Proceedings of theSIGDOC Conference 1986, Toronto, June.Bo Pang and Lillian Lee.
2004.
A sentimental education:Sentiment analysis using subjectivity summarizationbased on minimum cuts.
In Proceedings of the AnnualMeeting of the Association for Computational Linguis-tics , pages 271?278, Barcelona, ES.
Association forComputational Linguistics.Philip Resnik.
1995.
Using information content to eval-uate semantic similarity in a taxonomy.
In Proc.
Inter-national Joint Conference on Artificial Intelligence.E.
Riloff and J. Wiebe.
2003.
Learning extraction pat-terns for subjective expressions.
In Conference onEmpirical Methods in Natural Language Processing,pages 105?112.Fangzhong Su and Katja Markert.
2008.
From wordto sense: a case study of subjectivity recognition.
InProceedings of the 22nd International Conference onComputational Linguistics, Manchester.M.
Taboada, C. Anthony, and K. Voll.
2006.
Methodsfor creating semantic orientation databases.
In Pro-ceedings of 5th International Conference on LanguageResources and Evaluation .Hiroya Takamura, Takashi Inui, and Manabu Okumura.2006.
Latent variable models for semantic orienta-tions of phrases.
In Proceedings of the 11th Meetingof the European Chapter of the Association for Com-putational Linguistics , Trento, Italy.P.
Turney.
2002.
Thumbs up or thumbs down?
semanticorientation applied to unsupervised classification of re-views.
In Proceedings of the 40th Annual Meeting ofthe Association for Computational Linguistics, pages417?424, Philadelphia.Alessandro Valitutti, Carlo Strapparava, and OlivieroStock.
2004.
Developing affective lexical resources.PsychNology Journal, 2(1):61?83.J.
Wiebe and R. Mihalcea.
2006.
Word sense and subjec-tivity.
In Proceedings of the Annual Meeting of the As-sociation for Computational Linguistics, Sydney, Aus-tralia.Janyce Wiebe and Ellen Riloff.
2005.
Creating sub-jective and objective sentence classifiers from unan-notated texts.
In Proceedings of the 6th InternationalConference on Intelligent Text Processing and Com-putational Linguistics , pages 486?497, Mexico City,Mexico.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-levelsentiment analysis.
In Proceedings of the Human Lan-guage Technologies Conference/Conference on Empir-ical Methods in Natural Language Processing , pages347?354, Vancouver, Canada.Hong Yu and Vasileios Hatzivassiloglou.
2003.
Towardsanswering opinion questions: Separating facts fromopinions and identifying the polarity of opinion sen-tences.
In Conference on Empirical Methods in Nat-ural Language Processing , pages 129?136, Sapporo,Japan.18
