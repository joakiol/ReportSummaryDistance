Proceedings of the ACL 2007 Demo and Poster Sessions, pages 113?116,Prague, June 2007. c?2007 Association for Computational LinguisticsReal-Time Correction of Closed-CaptionsP.
Cardinal, G. Boulianne, M. Comeau, M. BoisvertCentre de recherche Informatique de Montreal (CRIM)Montreal, Canadapatrick.cardinal@crim.caAbstractLive closed-captions for deaf and hard ofhearing audiences are currently producedby stenographers, or by voice writers us-ing speech recognition.
Both techniques canproduce captions with errors.
We are cur-rently developing a correction module thatallows a user to intercept the real-time cap-tion stream and correct it before it is broad-cast.
We report results of preliminary ex-periments on correction rate and actual userperformance using a prototype correctionmodule connected to the output of a speechrecognition captioning system.1 IntroductionCRIM?s automatic speech recognition system hasbeen applied to live closed-captioning of french-canadian television programs (Boulianne et al,2006).
The low error rate of our approach dependsnotably on the integration of the re-speak method(Imai et al, 2002) for a controlled acoustic environ-ment, automatic speaker adaptation and dynamic up-dates of language models and vocabularies, and wasdeemed acceptable by several Canadian broadcast-ers (RDS,CPAC,GTVA and TQS) who have adoptedit over the past few years for captioning sports, pub-lic affairs and newscasts.However, for sensitive applications where errorrates must practically be zero, or other situationswhere speech recognition error rates are too high,we are currently developing a real-time correctioninterface.
In essence, this interface allows a user tocorrect the word stream from speech recognition be-fore it arrives at the closed-caption encoder.2 BackgroundReal-time correction must be done within difficultconstraints : with typical captioning rates of 130words per minute, and 5 to 10% word error rate,the user must correct between 6 and 13 errors perminute.
In addition, the process should not introducemore than a few seconds of additional delay over the3 seconds already needed by speech recognition.In a previous work, (Wald et al, 2006) ex-plored how different input modalities, such asmouse/keyboard combination, keyboard only orfunction keys to select words for editing, could re-duce the amount of time required for correction.
In(Bateman et al, 2000), the correction interface con-sisted in a scrolling window which can be edited bythe user using a text editor style interface.
Theyintroduced the idea of a controllable delay duringwhich the text can be edited.Our approach combines characteristics of the twoprevious systems.
We use a delay parameter, whichcan be modified online, for controlling the outputrate.
We also use the standard mouse/keyboard com-bination for selecting and editing words.
Howeverwe added, for each word, a list of alternate wordsthat can be selected by a simple mouse click; thissimplifies the edition process and speeds up the cor-rection time.
However, manual word edition is stillavailable.Another distinctive feature of our approach isthe fixed word position.
When a word appears onscreen, it will remain in its position until it is sent113out.
This allows the user to focus on the wordsand not be distracted by word-scrolling or any otherword movement.3 Correction SoftwareThe correction software allows edition of the closed-captions by intercepting them while they are beingsent to the encoder.
Both assisted and manual cor-rections can be applied to the word stream.Assisted correction reduces the number of opera-tions by presenting a list of alternate words, so thata correction can be done with a simple mouse click.Manual correction requires editing the word to bechanged and is more expensive in terms of delay.As a consequence, the number of these operationsshould be reduced to a strict minimum.The user interface shown in figure 1 has been de-signed with this consideration in mind.
The princi-pal characteristic of the interface is that there is noscrolling.
Words never move; instead the matrix isfilled from left to right, top to bottom, with wordscoming from the speech recognition, in synchroni-sation with the audio.
When the bottom right ofthe matrix is reached, filling in starts from the upperleft corner again.
Words appear in blue while theyare editable, and in red once they have been sent tothe caption encoder.
Thus a blue ?window?, cor-responding to the interval during which words canbe edited, moves across the word matrix, while thewords themselves remain fixed.For assisted correction, the list of available alter-natives is presented in a list box under each word.These lists are always present, instead of being pre-sented only upon selection of a word.
In this waythe user has the opportunity of scanning the lists inadvance whenever his time budget alows.The selected word can also be deleted with a sin-gle click.
Different shortcut corrections, as sug-gested in (Wald et al, 2006) can also be applieddepending on the mouse button used to select theword: a left button click changes the gender (mas-culin or feminin) of the word while a right buttonclick changes the plurality (singular or plural) of theword.
These available choices are in principle ex-cluded from the list box choices.To apply a manual correction, the user simplyclicks the word with the middle button to make iteditable; modifications are done using the keyboard.Two users can run two correction interfaces inparallel, on alternating sentences.
This configurationavoids the accumulation of delays.
This functional-ity may prove useful if the word rate is so high that itbecomes too difficult to keep track of the word flow.In this mode, the second user can begin the correc-tion of a new sentence even if the first has not yetcompleted the correction of his/her sentence.
Onlyone out of two sentences is editable by each user.The synchronisation is on a sentence basis.3.1 Alternate word listsAs described in the previous section, the gen-der/plurality forms of the word are implicitly in-cluded and accessible through a simple left/rightmouse click.
Other available forms explicitly appearin a list box.
This approach has two major benefits.First, when a gender/plurality error is detected by theuser, no delay is incurred from scanning the choicesin the list box.
Second, since the gender/pluralityforms are not included in the list box, their place be-comes available for additional alternate words.The main problem is to establish word lists shortenough to reduce scanning time, but long enough tocontain the correct form.
For a given word output bythe speech recognition system, the alternate wordsshould be those that are most likely to be confusedby the recognizer.We experimented with two pre-computed sourcesof alternate word lists:1.
A list of frequently confused words was com-puted from all the available closed-captions ofour speech recognition system for which corre-sponding exact transcriptions exist.
The train-ing and development sets were made up of1.37M words and 0.17M words, respectively.2.
A phoneme based confusion matrix was usedfor scoring the alignment of each word of thevocabulary with every other word of the samevocabulary.
The alignment program was an im-plementation of the standard dynamic program-ming technique for string alignment (Cormenet al, 2001).Each of these techniques yields a list of alternatewords with probabilities based on substitution like-114Figure 1: Real-time corrector software.Source of alternates coverage (%)Word confusion matrix 52%Phoneme confusion matrix 37%Combined 60%Table 1: Coverage of substitutions (dev set).lihoods.
Table 1 shows how many times substitu-tions in the development set could be corrected witha word in the list, for each list and their combination.To combine both lists, we take this coverage intoconsideration and the fact that 48% of the wordswere common to both lists.
On this basis, we haveconstructed an alternate list of 10 words comprisedof the most likely 7 words of case 1; the remaining 3words are the most probable substitutions from theremaining words of both lists.3.2 Real-time List UpdateThe previous technique can only handle simple sub-stitutions: a word that is replaced by another one.Another frequent error in speech recognition is thereplacement of a single word by several smallerones.
In this case, the sequence of errors containsone substitution and one or more insertions.
Fromthe interface point of view, the user must delete somewords before editing the last word in the sequence.To assist the user in this case, we have imple-mented the following procedure.
When a word isdeleted by the user, the phonemes of this word areconcatenated with those of the following words.
Theresulting sequence of phonemes is used to search thedictionary for the most likely words according to thepronunciation.
These words are dynamically addedto the list appearing under the preceding word.
Thesearch technique used is the same alignment proce-dure implemented for computing the confusion ma-trix based on phoneme confusion.4 ResultsIn this section we present the results of two prelim-inary experiments.
In the first one, we simulateda perfect correction, as if the user had an infiniteamount of time, to determine the best possible re-sults that can be expected from the alternate wordlists.
In the second experiment, we submitted a pro-totype to users and collected performance measure-ments.4.1 Simulation ResultsThe simulation is applied to a test set consistingof a 30 minute hockey game description for whichclosed-captions and exact transcripts are available.We aligned the produced closed-captions with theircorrected transcripts and replaced any incorrectword by its correct counterpart if it appeared in thealternate list.
In addition, all insertion errors weredeleted.
Table 2 shows the word error rate (WER)115Source of alternates WEROriginal closed-captions 5.8%Phoneme confusion matrix 4.4%Word confusion matrix 3.1%Combined 2.9%Table 2: Error rate for perfect correction.Delay2 seconds 15 secondstest duration 30 minutes 8 minutes# of words 4631 1303# of editions 21 28WER before 6.8% 6.2%WER after 6.1% 2.5%Gain (relative %) 8.1% 58.7%Table 3: Error rate after user correction.obtained for different alternate word lists.The word confusion matrix captures most of thesubstitutions.
This behavior was expected since thematrix has been trained explicitely for that purpose.The performance should increase in the future as theamount of training data grows.
In comparison, thecontribution of words from the phoneme confusionmatrix is clearly limited.The corrected word was the first in the list 35%of the time, while it was in the first three 59% ofthe time.
We also simulated the effect of collaps-ing words in insertion-substitution sequences to al-low corrections of insertions : the increase in perfor-mance was less than 0.5%.4.2 User TestsExperiments were performed by 3 unacquaintedusers of the system on hockey game descriptions.In one case, we allowed a delay of 15 seconds; thesecond case allowed a 2 second delay to give a pre-liminary assessment of user behavior in the caseof minimum-delay real-time closed-captioning.
Ta-ble 3 shows the error rate before and after correction.The results show that a significant WER decreaseis achieved by correcting using a delay of 15 sec-onds.
The reduction with a 2 second delay is minor;with appropriate training, however, we can expectthe users to outperform these preliminary results.5 Conclusion and Future WorkWe are currently developing a user interface for cor-recting live closed-captions in real-time.
The inter-face presents a list of alternatives for each automati-cally generated word.
The theoretical results that as-sumes the user always chooses the correct suggestedword shows the potential for large error reductions,with a minimum of interaction.
When larger delaysare allowed, manual edition of words for which thereis no acceptable suggested alternative can yield fur-ther improvements.We tested the application for real-time text cor-rection produced in a real-world application.
Withusers having no prior experience and with only a 15second delay, the WER dropped from 6.1% to 2.5%.In the future, users will be trained on the systemand we expect an important improvement in bothaccuracy and required delay.
We will also experi-ment the effect of running 2 corrections in parallelfor more difficult tasks.
Future work also includesthe integration of an automatic correction tool forimproving or highlighting the alternate word list.ReferencesA.
Bateman, J. Hewitt, A. Ariyaeeinia, P. Sivakumaran,and A. Lambourne.
2000.
The Quest for The Last5%: Interfaces for Correcting Real-Time Speech-Generated Subtitles Proceedings of the 2000 Confer-ence on Human Factors in Computing Systems (CHI2000), April 1-6, The Hague, Netherlands.T.H.
Cormen, C.E.
Leiserson, R.L.
Rivest, and C. Stein2001.
Introduction to Algorithms second edition, MITPress, Cambridge, MA.G.
Boulianne, J.-F. Beaumont, M. Boisvert, J. Brousseau,P.
Cardinal, C. Chapdelaine, M.Comeau, P. Ouellet,and F. Osterrath.
2006.
Computer-assisted closed-captioning of live TV broadcasts in French Proceed-ings of the 2006 Interspeech - ICSLP, September 17-21, Pittsburg, US.T.
Imai, A. Matsui, S. Homma, T. Kobayakawa, O.Kazuo, S. Sato, and A. Ando 2002.
Speech Recogni-tion with a respeak method for subtiling live broadcastProceedings of the 2002 ICSLP, September 16-20, Or-lando, US.Wald, M. 2006 Creating Accessible Educational Multi-media through Editing Automatic Speech RecognitionCaptioning in Real Time.
International Journal of In-teractive Technology and Smart Education : SmarterUse of Technology in Education 3(2) pp.
131-142116
