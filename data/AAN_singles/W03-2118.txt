Domain Specific Speech Acts for Spoken Language TranslationLori Levin, Chad Langley, Alon Lavie,Donna Gates, Dorcas Wallace and Kay PetersonLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA, United States{lsl,clangley,alavie,dmg,dorcas,kay+}@cs.cmu.eduAbstractWe describe a coding scheme for ma-chine translation of spoken task-oriented dialogue.
The coding schemecovers two levels of speaker intention ?domain independent speech acts anddomain dependent domain actions.
Ourdatabase contains over 14,000 taggedsentences in English, Italian, and Ger-man.
We argue that domain actions, andnot speech acts, are the relevant dis-course unit for improving translationquality.
We also show that, althoughdomain actions are domain specific, theapproach scales up to large domainswithout an explosion of domain actionsand can be coded with high inter-coderreliability across research sites.
Fur-thermore, although the number of do-main actions is on the order of ten timesthe number of speech acts, sparseness isnot a problem for the training of classi-fiers for identifying the domain action.We describe our work on developinghigh accuracy speech act and domainaction classifiers, which is the core ofthe source language analysis module ofour NESPOLE machine translation sys-tem.1 IntroductionThe NESPOLE and C-STAR machine translationprojects use an interlingua representation basedon speaker intention rather than literal meaning.The speaker's intention is represented as adomain independent speech act followed by do-main dependent concepts.
We use the termdomain action to refer to the combination of aspeech act with domain specific concepts.
Exam-ples of domain actions and speech acts are shownin Figure 1.c:give-information+party?I will be traveling with my husband andour two children ages two and eleven?c:request-information+existence+facility?Do they have parking available?
"?Is there someplace to go ice skating?
"c:give-information+view+information-object?I see the bus icon?Figure 1: Examples of Speech Acts and DomainActions.Domain actions are constructed compositionallyfrom an inventory of speech acts and an inven-tory of concepts.
The allowable combinations ofspeech acts and concepts are formalized in a hu-man- and machine-readable specification docu-ment.
The specification document is supportedby a database of over 14,000 tagged sentences inEnglish, German, and Italian.The discourse community has long recog-nized the potential for improving NLP systemsby identifying speaker intention.
It has been hy-pothesized that predicting speaker intention ofthe next utterance would improve speech recog-nition (Reithinger et al, Stolcke et al), or reduceambiguity for machine translation (Qu et al,1996, Qu et al, 1997).
Identifying speaker inten-tion is also critical for sentence generation.We argue in this paper that the explicit repre-sentation of speaker intention using domain ac-tions can serve as the basis for an effectivelanguage-independent representation of meaningfor speech-to-speech translation and that therelevant units of speaker intention are the domainspecific domain action as well as the domain in-dependent speech act.
After a brief description ofour database, we present linguistic motivation fordomain actions.
We go on to show that althoughdomain actions are domain specific, there is notan explosion or exponential growth of domainactions when we scale up to a larger domain orport to a new domain.
Finally we will show that,although the number of domain actions is on theorder of ten times the number of speech acts,data sparseness is not a problem in training adomain action classifier.
We present extensivework on developing a high-accuracy classifierfor domain actions using a variety of classifica-tion approaches and conclusions on the adequacyof these approaches to the task of domain actionclassification.2 Data Collection Scenario and Data-baseOur study is based on data that was collected forthe NESPOLE and C-STAR speech-to-speechtranslation projects.
Three domains are included.The NESPOLE travel domain covers inquiriesabout vacation packages.
The C-STAR traveldomain consists largely of reservation and pay-ment dialogues and overlaps only about 50% invocabulary with the NESPOLE travel domain.The medical assistance domain includes dia-logues about chest pain and flu-like symptoms.There were two data collection protocols forthe NESPOLE travel domain ?
monolingual andbilingual.
In the monolingual protocol, an Eng-lish speaker in the United States had a conversa-tion with an Italian travel agent speaking (non-native) English in Italy.
Monolingual data wasalso collected for German, French and Italian.Bilingual data was collected during user studieswith, for example, an English speaker in theUnited States talking to an Italian-speaking travelagent in Italy, with the NESPOLE system pro-viding the translation between the two parties.The C-STAR data consists of only monolingualrole-playing dialogues with both speakers at thesame site.
The medical dialogues are monolin-gual with doctors playing the parts of both doctorand patient.The dialogues were transcribed and multi-sentence utterances were broken down into mul-tiple Semantic Dialogue Units (SDUs) that eachcorrespond to one domain action.
Some SDUshave been translated into other NESPOLE or C-STAR languages.
Over 14,000 SDUs have beentagged with interlingua representations includingdomain actions as well as argument-value pairs.Table 1 summarizes the number of tagged SDUsin complete dialogues in the interlingua database.There are some additional tagged dialogue frag-ments that are not counted.
Figure 2 shows anexcerpt from the database.English NESPOLE Travel 4691English C-STAR Travel 2025German NESPOLE Travel 1538Italian NESPOLE Travel 2248English Medical Assistance 2001German Medical Assistance 1152Italian Medical Assistance 935Table 1: Tagged SDUs in the Interlingua Data-base.e709wa.19.0  comments: DATA frome709_1_0018_ITAGOR_00e709wa.19.1  olang ITA  lang ITA Prv CMU?hai in mente una localita specifica?
"e709wa.19.1  olang ITA  lang GER  Prv CMU?haben Sie einen bestimmten Ort im Sinn?
"e709wa.19.1  olang ITA  lang FRE  PrvCLIPS ?
"e709wa.19.1  olang ITA  lang ENG  Prv CMU?do you have a specific place in mind"e709wa.19.1                   IF  Prv CMUa:request-information+disposition+object(object-spec=(place, modifier=specific,identifiability=no), disposi-tion=(intention, who=you))e709wa.19.1  comments: Tagged by dmgFigure 2: Excerpt from the Interlingua Database.3 Linguistic Argument for Domain Ac-tionsProponents of Construction Grammar (Fillmoreet.
al.
1988, Goldberg 1995) have argued thathuman languages consist of constructional unitsthat include a syntactic structure along with itsassociated semantics and pragmatics.
Some con-structions follow the typical syntactic rules of thelanguage but have a semantic or pragmatic focusthat is not compositionally predictable from theparts.
Other constructions do not even follow thetypical syntax of the language (e.g., Why not go?with no tensed verb).Our work with multilingual machine transla-tion of spoken language shows that fixed expres-sions cannot be translated literally.
For example,Why not go to the meeting?
can be translatedinto Japanese as Kaigi ni itte mitara doo?
(meet-ing to going see/try-if how), which differs fromthe English in several ways.
It does not have aword corresponding to not; it has a word thatmeans see/try that does not appear in the Englishsentence; and so on.
In order to produce an ac-ceptable translation, we must find a commonground between the English fixed expressionWhy not V-inf?
and the Japanese fixed expression-te mittara doo?.
The common ground is thespeaker's intention (in this case, to make a sug-gestion) rather than the syntax or literal meaning.Speaker intention is partially captured with adirect or indirect speech act.
However, whereasspeech acts are generally domain independent,task-oriented language abounds with fixed ex-pressions that have domain specific functions.For example, the phrases We have?
or Thereare?
in the hotel reservation domain expressavailability of rooms in addition to their moreliteral meanings of possession and existence.
Inthe past six years, we have been successful inusing domain specific domain actions as the ba-sis for translation of limited-domain task-oriented spoken language (Levin et al, 1998,Levin et al 2002; Langley and Lavie, 2003)4 Scalability and Portability of DomainActionsDomain actions, like speech acts, convey speakerintention.
However, domain actions also repre-sent components of meaning and are thereforemore numerous than domain independent speechacts.
1168 unique domain actions are used in ourNESPOLE database, in contrast to only 72speech acts.
We show in this section that domainactions yield good coverage of task-oriented do-mains, that domain actions can be coded effec-tively by humans, and that scaling up to largerdomains or porting to new domains is feasiblewithout an explosion of domain actions.Coverage of Task-Oriented Domains: OurNESPOLE domain action database contains dia-logues from two task-oriented domains: medicalassistance and travel.
Table 2 shows the numberof speech acts and concepts that are used in thetravel and medical domains.
The 1168 uniquedomain actions that appear in our database arecomposed of the 72 speech acts and 125 con-cepts.Travel Medical CombinedDAs 880 459 1168SAs 67 44 72Concepts 91 74 125Table 2: DA component counts in NESPOLEdata.Our domain action based interlingua has quitehigh coverage of the travel and medical dia-logues we have collected.
To measure how wellthe interlingua covers a domain, we define theno-tag rate as the percent of sentences that arenot covered by the interlingua, according to ahuman expert.
The no-tag rate for the EnglishNESPOLE travel dialogues is 4.3% for dialoguesthat have been used for system development.We have also estimated the domain action no-tag rate for unseen data using the NESPOLEtravel database (English, German, and Italiancombined).
We randomly selected 100 SDUs asseen data and extracted their domain actions.
Wethen randomly selected 100 additional SDUsfrom the remaining data and estimated the no-tagrate by counting the number of SDUs not cov-ered by the domain actions in the seen data.
Wethen added the unseen data to the seen data setand randomly selected 100 new SDUs.
We re-peated this process until the entire database hadbeen seen, and we repeated the entire samplingprocess 10 times.
Although the number of do-main actions increases steadily with the databasesize (Figure 4), the no-tag rate for unseen datastabilizes at less than 10%.We also randomly selected half of the SDUs(4200) from the database as seen data and ex-tracted the domain actions.
Holding the seen dataset fixed, we then estimated the no-tag rates inincreasing amounts of unseen data from the re-maining half of the database.
We repeated thisprocess 10 times.
With a fixed amount of seendata, the no-tag rate remains stable for increasingamounts of unseen data.
We observed similar no-tag rate results for the medical assistance domainand for the combination of travel and medicaldomains.It is also important to note that although thereis a large set of uncommon domain actions, thetop 105 domain actions cover 80% of the sen-tences in the travel domain database.
Thus do-main actions are practical for covering task-oriented domains.Intercoder Agreement: Intercoder agreement isanother indicator of manageability of the domainaction based interlingua.
We calculate intercoderagreement as percent agreement.
Three interlin-gua experts at one NESPOLE site achieved 94%agreement (average pairwise agreement) onspeech acts and 88% agreement on domain ac-tions.
Across sites, expert agreement on speechacts is still quite high (89%), although agreementon domain actions is lower (62%).
Since manydomain actions are similar in meaning, some dis-agreement can be tolerated without affectingtranslation quality.Figure 3: DAs to cover data (English).Figure 4: DAs to cover data (All languages).Scalability and Portability: The graphs in Figure3 and Figure 4 illustrate growth in the number ofdomain actions as the database size increases andas new domains are added.
The x-axis representsthe sample size randomly selected from the data-base.
The y-axis shows the number of uniquedomain actions (types) averaged over 10 samplesof each size.
Figure 3 shows the growth in do-main actions for three English databases(NESPOLE travel, C-STAR travel, and medicalassistance) as well as the growth in domain ac-tions for a database consisting of equal amountsof data from each domain.
Figure 4 shows thegrowth in domain actions for combined English,German, and Italian data in the NESPOLE traveland medical domains.Figure 3 and Figure 4 show that the numberof domain actions increases steadily as the data-base grows.
However, closer examination revealsthat scalability to larger domains and portabilityto new domains are in fact feasible.
The curvesrepresenting combined domains (travel plusmedical in Figure 4 and NESPOLE travel, C-STAR travel, and medical in Figure 3) show onlya small increase in the number of domain actionswhen two domains are combined.
In fact, there isa large overlap between domains.
In Table 3 theOverlap columns show the number of DA typesand tokens that are shared between the travel andmedical domains.
We can see around 70% of DAtokens are covered by DA types that occur inboth domains.DATypesTypeOverlapDATokensTokenOverlapNESPOLETravel 880 171 84776004(70.8%)NESPOLEMedical 459 171 40882743(67.1%)Table 3: DA Overlap (All languages).5 A Hybrid Analysis Approach for Pars-ing Domain ActionsLangley et al (2002; Langley and Lavie, 2003)describe the hybrid analysis approach that is usedin the NESPOLE!
system (Lavie et al, 2002).The hybrid analysis approach combines gram-mar-based phrasal parsing and machine learningtechniques to transform utterances into our inter-lingua representation.
Our analyzer operates inthree stages to identify the domain action andarguments.First, an input utterance is parsed into a se-quence of arguments using phrase-level semanticgrammars and the SOUP parser (Gavald?, 2000).Four grammars are defined for argument parsing:an argument grammar, a pseudo-argument gram-mar, a cross-domain grammar, and a sharedgrammar.
The argument grammar containsphrase-level rules for parsing arguments definedin the interlingua.
The pseudo-argument gram-mar contains rules for parsing common phrasesthat are not covered by interlingua arguments.For example, all booked up, full, and sold outmight be grouped into a class of phrases that in-dicate unavailability.
The cross-domain grammarcontains rules for parsing complete DAs that aredomain independent.
For example, this grammarcontains rules for greetings (Hello, Good bye,Nice to meet you, etc.).
Finally, the sharedgrammar contains low-level rules that can beused by all other subgrammars.After argument parsing, the utterance is seg-mented into SDUs using memory-based learning(k-nearest neighbor) techniques.
Spoken utter-ances often consist of several SDUs.
Since DAsare assigned at the SDU level, it is necessary tosegment utterances before assigning DAs.01002003004005006007008009000 1000 2000 3000 4000 5000 6000 7000SDUs per SampleMeanUniqueDAsover10RandomSamplesNespole Travel Nespole Medical C-STARNespole Travel+Medical C-STAR + Nespole Travel+Medical010020030040050060070080090010000 1000 2000 3000 4000 5000 6000 7000 8000 9000SDUs per SampleMeanUniqueDAsover10RandomSamplesNespole Travel Nespole Medical Nespole Travel+MedicalThe final stage in the hybrid analysis ap-proach is domain action classification.6 Domain Action ClassificationIdentifying the domain action is a critical step inthe analysis process for our interlingua-basedtranslation systems.
One possible approachwould be to manually develop grammars de-signed to parse input utterances all the way to thedomain action level.
However, while grammar-based parsing may provide very accurate analy-ses, it is generally not feasible to develop agrammar that completely covers a domain.
Thisproblem is exacerbated with spoken input, wheredisfluencies and deviations from the grammar arevery common.
Furthermore, a great deal of effortby human experts is generally required to de-velop a wide-coverage grammar.An alternative to writing full domain actiongrammars is to train classifiers to identify theDA.
Machine learning approaches allow the ana-lyzer to generalize beyond training data and tendto degrade gracefully in the face of noisy input.Machine learning methods may, however, be lessaccurate than grammars, especially on commonin-domain input, and may require a large amountof training data in order to achieve adequate lev-els of performance.
In the hybrid analyzer de-scribed above, classifiers are used to identify theDA for domain specific portions of utterancesthat are not covered by the cross-domain gram-mar.We tested classifiers trained to classify com-plete DAs.
We also split the DA classificationtask into two subtasks: speech act classificationand concept sequence classification.
This simpli-fies the task of each classifier, allows for the useof different approaches and/or feature sets foreach task, and reduces data sparseness.
Our hy-brid analyzer uses the output of each classifieralong with the interlingua specification to iden-tify the DA (Langley et al, 2002; Langley andLavie, 2003).7 Experimental SetupWe conducted experiments to assess the per-formance of several machine-learning ap-proaches on the DA classification tasks.
Weevaluated all of the classifiers on English andGerman input in the NESPOLE travel domain.7.1 CorpusThe corpus used in all of the experiments was theNESPOLE!
travel and tourism database.
Sinceour goal was to evaluate the SA and concept se-quence classifiers and not segmentation, we cre-ated training examples for each SDU in thedatabase rather than for each utterance.
Table 4contains statistics regarding the contents of thecorpus for our classification tasks.
Table 5 showsthe frequency of the most common domain ac-tion, speech act, and concept sequence in thecorpus.
These frequencies provide a baseline thatwould be achieved by a simple classifier that al-ways returned the most common class.English GermanSDUs 8289 8719Domain Actions 972 1001Speech Acts 70 70Concept Sequences 615 638Vocabulary Size 1946 2815Table 4: Corpus Statistics.English GermanDA (acknowledge) 19.2% 19.7%SA (give-information) 41.4% 40.7%Concept Sequence(No concepts)38.9% 40.3%Table 5: Most frequent DAs, SAs, and CSs.All of the results presented in this paper wereproduced using a 20-fold cross validation setup.The corpus was randomly divided into 20 sets ofequal size.
Each of the sets was held out as thetest set for one fold with the remaining 19 setsused as training data.
Within each language, thesame random split was used for all of the classi-fication experiments.
Because the same split ofthe data was used for different classifiers, theresults of two classifiers on the same test set aredirectly comparable.
Thus, we tested for signifi-cance using two-tailed matched pair t-tests.7.2 Machine Learning ApproachesWe evaluated the performance of four differentmachine-learning approaches on the DA classifi-cation tasks: memory-based learning (k-Nearest-Neighbor), decision trees, neural networks, andna?ve Bayes n-gram classifiers.
We selectedthese approaches because they vary substantiallyin the their representations of the training dataand their methods for selecting the best class.Our purpose was not to implement each ap-proach from scratch but to test the approach forour particular task.
Thus, we chose to use exist-ing software for each approach ?off the shelf.
?The ease of acquiring and setting up the softwareinfluenced our choice.
Furthermore, the ease ofincorporating the software into our online trans-lation system was also a factor.Our memory-based classifiers were imple-mented using TiMBL (Daelemans et al, 2002).We used C4.5 (Quinlan, 1993) for our decisiontree classifiers.
Our neural network classifierswere implemented using SNNS (Zell et al,1998).
We used Rainbow (McCallum, 1996) forour na?ve Bayes n-gram classifiers.8 ExperimentsIn our first experiment, we compared the per-formance of the four machine learning ap-proaches.
Each SDU was parsed using theargument and pseudo-argument grammars de-scribed above.
The feature set for the DA and SAclassifiers consisted of binary features indicatingthe presence or absence of labels from thegrammars in the parse forest for the SDU.
Thefeature set included 212 features for English and259 features for German.
The concept sequenceclassifiers used the same feature set with the ad-dition of the speech act.In the SA classification experiment, theTiMBL classifier used the IB1 (k-NN) algorithmwith 1 neighbor and gain ratio feature weighting.The C4.5 classifier required at least one instanceper branch and used node post-pruning.
Both theTiMBL and C4.5 classifiers used the binary fea-tures described above and produced the singlebest class as output.
The SNNS classifier used asimple feed-forward network with 1 input unitfor each binary feature, 1 hidden layer containing15 units, and 1 output unit for each speech act.The network was trained using backpropagation.The order of presentation of the training exam-ples was randomized in each epoch, and theweights were updated after each training exam-ple presentation.
In order to simulate the binaryfeatures used by the other classifiers as closely aspossible, the Rainbow classifier used a simpleunigram model whose vocabulary was the set oflabels included in the binary feature set.
Thesetup for the DA classification experiment wasidentical except that the neural network had 50hidden units.The setup of the classifiers for the concept se-quence classification experiment was very simi-lar.
The TiMBL and C4.5 classifiers were set upexactly as in the DA and SA experiments withone extra feature whose value was the speech act.The SNNS concept sequence classifier used asimilar network with 50 hidden units.
The SAfeature was represented as a set of binary inputunits.
The Rainbow classifier was set up exactlyas in the DA and SA experiments.
The SA fea-ture was not included.As mentioned above, both experiments used a20-fold cross-validation setup.
In each fold, theTiMBL, C4.5, and Rainbow classifiers were sim-ply trained on 19 subsets of the data and testedon the remaining set.
The SNNS classifiers re-quired a more complex setup to determine thenumber of epochs to train the neural network foreach test set.
Within each fold, a cross-validationsetup was used to determine the number of train-ing epochs.
Each of the 19 training subsets for afold was used as a validation set.
The networkwas trained on the remaining 18 subsets until theaccuracy on the validation set did not improvefor 50 consecutive epochs.
The network was thentrained on all 19 training subsets for the averagenumber of epochs from the validation sets.
Thisprocess was used for all 20-folds in the SA clas-sification experiment.
For the DA and conceptsequence experiments, this process ran for ap-proximately 1.5 days for each fold.
Thus, thisprocess was run for the first two folds, and theaverage number of epochs from those folds wasused for training.English GermanTiMBL 49.69% 46.51%C4.5 48.90% 46.58%SNNS 49.39% 46.21%Rainbow 39.74% 38.32%Table 6: Domain Action classifier accuracy.English GermanTiMBL 69.82% 67.57%C4.5 70.41% 67.90%SNNS 71.52% 67.61%Rainbow 51.39% 46.00%Table 7: Speech Act classifier accuracy.English GermanTiMBL 69.59% 67.08%C4.5 68.47% 66.45%SNNS 71.35% 68.67%Rainbow 51.64% 51.50%Table 8: Concept Sequence classifier accuracy.Table 6, Table 7, and Table 8 show the aver-age accuracy of each learning approach on the20-fold cross validation experiments for domainaction, speech act, and concept classification re-spectively.
For DA classification, there were nosignificant differences between the TiMBL,C4.5, and SNNS classifiers for English or Ger-man.
In the SA experiment, the difference be-tween the TiMBL and C4.5 classifiers forEnglish was not significant.
The SNNS classifierwas significantly better than both TiMBL andC4.5 (at least p=0.0001).
For German SA classi-fication, there were no significant differencesbetween the TiMBL, C4.5, and SNNS classifiers.For concept sequence classification, SNNS wassignificantly better than TiMBL and C4.5 (atleast p=0.0001) for both English and German.For English only, TiMBL was significantly betterthan C4.5 (p=0.005).For both languages, the Rainbow classifierperformed much worse than the other classifiers.However, the unigram model over arguments didnot exploit the strengths of the n-gram classifica-tion approach.
Thus, we ran another experimentin which the Rainbow classifier was trained onsimple word bigrams.
No stemming or stopwords were used in building the bigram models.English GermanDomain Action 48.59% 48.09%Speech Act 79.00% 77.46%Concept Sequence 56.87% 57.77%Table 9: Rainbow accuracy with word bigrams.Table 9 shows the average accuracy of theRainbow word bigram classifiers using the same20-fold cross-validation setup as in the previousexperiments.
As we expected, using word bi-grams rather than parse label unigrams improvedthe performance of the Rainbow classifiers.
ForGerman DA classification, the word bigram clas-sifier was significantly better than all of the pre-vious German DA classifiers (at least p=0.005).Furthermore, the Rainbow word bigram SA clas-sifiers for both languages outperformed all of theSA classifiers that used only the parse labels.Although the argument parse labels providean abstraction of the words present in an SDU,the words themselves also clearly provided use-ful information for classification, at least for theSA task.
Thus, we conducted additional experi-ments to examine whether combining parse andword information could further improve per-formance.We chose to incorporate word informationinto the TiMBL classifiers used in the first ex-periment.
Although the SNNS SA classifier per-formed significantly better than the TiMBL SAclassifier for English, there was no significantdifference for SA classification in German.
Fur-thermore, because of the complexity and timerequired for training with SNNS, we preferredworking with TiMBL.We tested two approaches to adding word in-formation to the TiMBL classifier.
In both ap-proaches, the word-based information for eachfold was computed only based on the data in thetraining set.
In our first approach, we added bi-nary features for the 250 words that had thehighest mutual information with the class.
Eachfeature indicated the presence or absence of theword in the SDU.
In this condition, we used theTiMBL classifier with gain ratio feature weight-ing, 3 neighbors, and unweighted voting.
Thesecond approach we tested combined the Rain-bow word bigram classifier with the TiMBLclassifier.
We added one input feature for eachpossible speech act to the TiMBL classifier.
Thevalue of each SA feature was the probability ofthe speech act computed by the Rainbow wordbigram classifier.
In this condition, we used theTiMBL classifier with gain ratio feature weight-ing, 11 neighbors, and inverse linear distanceweighted voting.English GermanTiMBL + words 78.59% 75.98%TiMBL + Rainbow 81.25% 78.93%Table 10: Word+Parse SA classifier accuracy.Table 10 shows the average accuracy of theSA classifiers that combined parse and word in-formation using the same 20-fold cross-validation setup as the previous experiments.Although adding binary features for individualwords improved performance over the classifierswith no word information, it did not allow thecombined classifiers to outperform the Rainbowword bigram classifiers.
However, for bothlanguages, adding the probabilities computed bythe Rainbow bigram model resulted in a SA clas-sifier that outperformed all previous classifiers.The improvement in accuracy was highly signifi-cant for both languages.We conducted a similar experiment for com-bining parse and word information in the conceptsequence classifiers.
The first condition wasanalogous to the first condition in the combinedSA classification experiment.
The second condi-tion was slightly different.
A concept sequencecan be broken down into a set of individual con-cepts.
The set of individual concepts is muchsmaller than the set of concept sequences (110for English and 111 for German).
Thus, we useda Rainbow word bigram classifier to compute theprobability of each individual concept rather thanthe complete concept sequence.
The probabilitiesfor the individual concepts were added to theparse label features for the combined classifier.In both conditions, the performance of the com-bined classifiers was roughly the same as theclassifiers that used only parse labels as features.English GermanTiMBL + words 56.48% 54.98%Table 11: Word+Parse DA classifier accuracy.Table 11 shows the average accuracy of DAclassifiers for English and German using a setupsimilar to the first approach in the combined SAexperiment.
In this experiment, we added binaryfeatures for the 250 words that the highest mu-tual information with the class.
We used aTiMBL classifier with gain ratio feature weight-ing and one neighbor.
The improvement in accu-racy for both languages was highly significant.English GermanTiMBL SA+ TiMBL CS 49.63% 46.50%TiMBL+Rainbow SA+ TiMBL CS 57.74% 53.93%Table 12: DA accuracy of SA+CS classifiers.Finally, Table 12 shows the results from twotests to compare the performance of combiningthe best output of the SA and concept sequenceclassifiers with the performance of the completeDA classifiers.
In the first test, we combined theoutput from the TiMBL SA and CS classifiersshown in Table 7 and Table 8.
The performanceof the combined SA+CS classifiers was almostidentical to that of the TiMBL DA classifiersshown in Table 6.
In the second test, we com-bined our best SA classifier (TiMBL+Rainbow,shown in Table 10) with the TiMBL CS classi-fier.
In this case, we had mixed results.
The per-formance of the combined classifiers was betterthan our best DA classifier for English and worsefor German.9 DiscussionOne of our main goals was to determine the fea-sibility of automatically classifying domain ac-tions.
As the data in Table 4 show, DAclassification is a challenging problem with ap-proximately 1000 classes.
Even when the task isdivided into subproblems of identifying the SAand concept sequence, the subtasks remain diffi-cult.
The difficulty is compounded by relativelysparse training data with unevenly distributedclasses.
Although the most common classes inour training corpus had over 1000 training exam-ples, many of the classes had only 1 or 2 exam-ples.Despite these difficulties, our results indicatethat domain action classification is feasible.
ForSA classification in particular we were able toachieve very strong performance.
Although per-formance on concept sequence and DA classifi-cation is not as high, it is still quite strong,especially given that there are an order of magni-tude more classes than in SA classification.Based on our experiments, it appears that all ofthe learning approaches we tested were able tocope with data sparseness at the level found inour data, with the possible exception of the na?veBayes n-gram approach (Rainbow) for the con-cept sequence task.One additional point worth noting is that thereis evidence that domain action classificationcould be performed reasonably well using onlyword-based information.
Although our best-performing classifiers combined word and argu-ment parse information, the na?ve Bayes wordbigram classifier (Rainbow) performed very wellon the SA classification task.
With additionaldata, the performance of the concept sequenceand DA word bigram classifiers could be ex-pected to improve.
Cattoni et al (2001) also ap-ply statistical language models to DAclassification.
A word bigram model is trainedfor each DA, and the DA with the highest likeli-hood is assigned to each SDU.
Arguments areidentified using recursive transition networks,and interlingua specification constraints are usedto find the most likely valid interlingua represen-tation.
Although it is clear that argument infor-mation is useful for the task, it appears thatwords alone can be used to achieve reasonableperformance.Another goal of our experiments was to helpin the selection of a machine learning approachto be used in our hybrid analyzer.
Certainly oneof the most important considerations is how wellthe learning approach performs the task.
For SAclassification, the combination of parse featuresand word bigram probabilities clearly gave thebest performance.
For concept sequence classifi-cation, no learning approach clearly outper-formed any other (with the exception that thena?ve Bayes n-gram approach performed worsethan other approaches).
However, the perform-ance of the classifiers is not the only considera-tion to be made in selecting the classifier for ourhybrid analyzer.Several additional factors are also importantin selecting the particular machine learning ap-proach to be used.
One important attribute of thelearning approach is the speed of both classifica-tion and training.
Since the classifiers are part ofa translation system designed for use betweentwo humans to facilitate (near) real-time com-munication, the DA classifiers must classify in-dividual utterances online very quickly.Furthermore, since humans must write and testthe argument grammars, training and batchclassification should be fast so that the grammarwriters can update the grammars, retrain the clas-sifiers, and test efficiently.The machine learning approach should alsobe able to easily accommodate both continuousand discrete features from a variety of sources.Possible sources for features include wordsand/or phrases in an utterance, the argumentparse, the interlingua representation of the argu-ments, and properties of the dialogue (e.g.speaker tag).
The classifier should be able to eas-ily combine features from any or all of thesesources.Another desirable attribute for the machinelearning approach is the ability to produce aranked list of possible classes.
Our interlinguaspecification defines how speech acts and con-cepts are allowed to combine as well as how ar-guments are licensed by the domain action.These constraints can be used to select an alter-native DA if the best DA violates the specifica-tion.Based on all of these considerations, theTiMBL+Rainbow classifier, which combinesparse label features with word bigram probabili-ties, seems like an excellent choice for speech actclassification.
It was the most accurate classifierthat we tested.
Furthermore, the main TiMBLclassifier meets all of the requirements discussedabove except the ability to produce a completeranked list of the classes for each instance.
How-ever, such a list could be produced as a backupfrom the Rainbow probability features.
Addingnew features to the combined classifier wouldalso be very easy because TiMBL was the pri-mary classifier in the combination.
Finally, sinceboth TiMBL and Rainbow provide an onlineserver mode for classifying single instances, in-corporating the combined classifier into anonline translation system would not be difficult.Since there were no significant differences in theperformance of most of the concept sequenceclassifiers, this combined approach is probablyalso a good option for that task.10 ConclusionWe have described a representation ofspeaker intention that includes domain independ-ent speech acts as well as domain dependent do-main actions.
We have shown that domainactions are a useful level of abstraction for ma-chine translation of task-oriented dialogue, andthat, in spite of their domain specificity, they arescalable to larger domains and portable to newdomains.We have also presented classifiers for domainactions that have been comparatively tested andused successfully in the NESPOLE speech-to-speech translation system.
We experimentallycompared the effectiveness of several machine-learning approaches for classification of domainactions, speech acts, and concept sequences ontwo input languages.
Despite the difficulty of theclassification tasks due to a large number ofclasses and relatively sparse data, the classifiersexhibited strong performance on all tasks.
Wealso demonstrated how the combination of twolearning approaches could be used to improveperformance and overcome the weaknesses ofthe individual approaches.Acknowledgements: NESPOLE was fundedby NSF (Grant number 9982227) and the EU.The NESPOLE partners are ITC-irst, UniversiteJoseph Fourrier, Universitat Karlsruhe, APTTrentino travel board, and AETHRA telecom-munications.
We would like to acknowledge thecontribution of the following people in particu-lar: Fabio Pianesi, Emanuele Pianta, NadiaMana, and Herve Blanchon.ReferencesCattoni, R., M. Federico, and A. Lavie.
2001.
RobustAnalysis of Spoken Input Combining Statisticaland Knowledge-Based Information Sources.
InProceedings of the IEEE ASRU Workshop, Trento,Italy.Daelemans, W., J. Zavrel, K. van der Sloot, and A.van den Bosch.
2002.
TiMBL: Tilburg MemoryBased Learner, version 4.3, Reference Guide.
ILKTechnical Report 02-10.
Available fromhttp://ilk.kub.nl/downloads/pub/papers/ilk0210.ps.gz.Fillmore, C.J., Kay, P. and O'Connor, M.C.
1988.Regularity and Idiomaticity in Grammatical Con-structions.
Language, 64(3), 501-538.Gavald?, M. 2000.
SOUP: A Parser for Real-WorldSpontaneous Speech.
In Proceedings of IWPT-2000, Trento, Italy.Goldberg, Adele E. 1995.
Constructions: A Construc-tion Grammar Approach to Argument Structure.Chicago University Press.Langley, C. and A. Lavie.
2003.
Parsing Domain Ac-tions with Phrase-Level Grammars and Memory-Based Learners.
To appear in Proceedings ofIWPT-2003.
Nancy, France.Langley, C., A. Lavie, L. Levin, D. Wallace, D.Gates, and K. Peterson.
2002.
Spoken LanguageParsing Using Phrase-Level Grammars and Train-able Classifiers.
In Workshop on Algorithms forSpeech-to-Speech Machine Translation at ACL-02.Philadelphia, PA.Lavie, A., F. Metze, F. Pianesi, et al 2002.
Enhancingthe Usability and Performance of NESPOLE!
?
aReal-World Speech-to-Speech Translation System.In Proceedings of HLT-2002.
San Diego, CA.Levin, L., D. Gates, A. Lavie, A. Waibel.
1998.
AnInterlingua Based on Domain Actions for MachineTranslation of Task-Oriented Dialogues.
In Pro-ceedings of ICSLP 98, Vol.
4, pages 1155-1158,Sydney, Australia.Levin, L., D. Gates, D. Wallace, K. Peterson, A. La-vie F. Pianesi, E. Pianta, R. Cattoni, N. Mana.2002.
Balancing Expressiveness and Simplicity inan Interlingua for Task Based Dialogue.
In Pro-ceedings of Workshop on Spoken Language Trans-lation.
ACL-02, Philadelphia.McCallum, A. K. 1996.
Bow: A toolkit for statisticallanguage modeling, text retrieval, classification andclustering.http://www.cs.cmu.edu/~mccallum/bow.Qu, Y., B. DiEugenio, A. Lavie, L. Levin and C.P.Rose.
1997.
Minimizing Cumulative Error in Dis-course Context.
In Dialogue Processing in SpokenLanguage Systems: Revised Papers from ECAI-96Workshop, E. Maier, M. Mast and S.
LuperFoy(eds.
), LNCS series, Springer Verlag.Qu, Y., C. P. Rose, and B. DiEugenio.
1996.
UsingDiscourse Predictions for Ambiguity Resolution.
InProceedings of COLING-1996.Quinlan, J. R. 1993.
C4.5: Programs for MachineLearning.
San Mateo: Morgan Kaufmann.Reithinger, N., R. Engel, M. Kipp, M. Klesen.
1996.Predicting Dialogue Acts for a Speech-To-SpeechTranslation System.
DFKI GmbH Saarbruecken.Verbmobil-Report 151.http://verbmobil.dfki.de/cgi-bin/verbmobil/htbin/doc-access.cgiStolcke, A., K. Ries, N. Coccaro, E. Shriberg, R.Bates, D. Jurafsky, P. Taylor, R. Martin, M.Meteer, and C. Van Ess-Dykema.
2000.
DialogueAct Modeling for Automatic Tagging and Recogni-tion of Conversational Speech.
Computational Lin-guistics 26:3, 339-371.Zell, A., G. Mamier, M. Vogt, et al 1998.
SNNS:Stuttgart Neural Network Simulator User Manual,Version 4.2.
