Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 42?47,Prague, June 2007. c?2007 Association for Computational LinguisticsSemEval-2007 Task 09: Multilevel Semantic Annotation ofCatalan and SpanishLlu?
?s Ma`rquez and Luis VillarejoTALP Research CenterTechnical University of Catalonia{lluism,luisv}@lsi.upc.eduM.
A.
Mart??
and Mariona Taule?Centre de Llenguatge i Computacio?, CLiCUniversitat de Barcelona{amarti,mtaule}@ub.eduAbstractIn this paper we describe SemEval-2007 tasknumber 9 (Multilevel Semantic Annotationof Catalan and Spanish).
In this task, weaim at evaluating and comparing automaticsystems for the annotation of several seman-tic linguistic levels for Catalan and Spanish.Three semantic levels are considered: nounsense disambiguation, named entity recogni-tion, and semantic role labeling.1 IntroductionThe Multilevel Semantic Annotation of Catalan andSpanish task is split into the following three sub-tasks:Noun Sense Disambiguation (NSD): Disambigua-tion of all frequent nouns (?all words?
style).Named Entity Recognition (NER): The annotationof (possibly embedding) named entities with basicentity types.Semantic Role Labeling (SRL): Including also twosubtasks, i.e., the annotation of verbal predicateswith semantic roles (SR), and verb tagging withsemantic?class labels (SC).All semantic annotation tasks are performed onexactly the same corpora for each language.
We pre-sented all the annotation levels together as a com-plex global task, since we were interested in ap-proaches which address these problems jointly, pos-sibly taking into account cross-dependencies amongthem.
However, we were also accepting systems ap-proaching the annotation in a pipeline style, or ad-dressing any of the particular subtasks in any of thelanguages.In Section 2 we describe the methodology fol-lowed to develop the linguistic corpora for the task.Sections 3 and 4 summarize the task setting and theparticipant systems, respectively.
Finally, Section 5presents a comparative analysis of the results.
Forany additional information on corpora, resources,formats, tagsets, annotation manuals, etc.
we referthe reader to the official website of the task1.2 Linguistic corporaThe corpora used in this SemEval task are a subset ofCESS-ECE, a multilingual Treebank, composed ofa Spanish (CESS-ESP) and a Catalan (CESS-CAT)corpus of 500K words each (Mart??
et al, 2007b).These corpora were enriched with different kinds ofsemantic information: argument structure, thematicroles, semantic class, named entities, and WordNetsynsets for the 150 most frequent nouns.
The an-notation process was carried out in a semiautomaticway, with a posterior manual revision of all auto-matic processes.A sequential approach was adopted for the anno-tation of the corpus, beginning with the basic lev-els of analysis, i.e., POS tagging and chunking (au-tomatically performed) and followed by the morecomplex levels: syntactic constituents and functions(manually tagged) and semantic annotation (man-ual and semiautomatic processes with manual com-pletion and posterior revision).
Furthermore, someexperiments concerning inter-annotator agreement1www.lsi.upc.edu/?nlp/semeval/msacs.html42were carried out at the syntactic (Civit et al, 2003)and semantic levels (Ma`rquez et al, 2004) in orderto evaluate the quality of the results.2.1 Syntactic AnnotationThe syntactic annotation consists of the labeling ofconstituents, including elliptical subjects, and syn-tactic functions.
The surface order was maintainedand only those constituents directly attached to anykind of ?Sentence?
root node were considered (?S?,?S.NF?, ?S.F?, ?S*?).
The syntactic functions are:subject (SUJ), direct object (OD), indirect object(OI), attribute (ATR), predicative (CPRED), agentcomplement (CAG), and adjunct (CC).
Other func-tions such as textual element (ET), sentence adjunct(AO), negation (NEG), vocative (VOC) and verbmodifiers (MOD) were tagged, but did not receiveany thematic role.2.2 Lexical Semantic Information: WordNetWe selected the 150 most frequent nouns in thewhole corpus and annotated their occurrences withWordNet synsets.
No other word categories weretreated (verbs, adjectives and adverbs).
We used asteady version of Catalan and Spanish WordNets,linked to WordNet 1.6.
Each noun either matcheda WordNet synset or a special label indicating a spe-cific circumstance (for instance, the tag C2S indi-cates that the word does not appear in the dictio-nary).
All this process was carried out manually.2.3 Named EntitiesThe corpora were annotated with both strong andweak Named Entities.
Strong NEs correspond to sin-gle lexical tokens (e.g., ?[U.S.]LOC?
), while weakNEs include, by definition, some strong entities(e.g., ?The [president of [US]LOC]PER?).
(Are?valoet al, 2004).
Thus, NEs may embed.
Six basic se-mantic categories were distinguished: Person, Orga-nization, Location, Date, Numerical expression, andOthers (Borrega et al, 2007).Two golden rules underlie the definition of NEs inSpanish and Catalan.
On the one hand, only a nounphrase can be a NE.
On the other hand, its referentmust be unique and unambiguous.
Finally, anotherhard rule (although not 100% reliable) is that only adefinite singular noun phrase might be a NE.2.4 Thematic Role Labeling / Semantic ClassBasic syntactic functions were tagged with both ar-guments and thematic roles, taking into account thesemantic class related to the verbal predicate (Taule?et al, 2006b).
We characterized predicates by meansof a limited number of Semantic Classes based onEvent Structure Patterns, according to four basicevent classes: states, activities, accomplishments,and achievements.
These general classes were splitinto 17 subclasses, depending on thematic roles anddiathesis alternations.Similar to PropBank, the set of arguments se-lected by the verb are incrementally numbered ex-pressing the degree of proximity of an argument inrelation to the verb (Arg0, Arg1, Arg2, Arg3, Arg4).In our proposal, each argument includes the the-matic role in its label (e.g., Arg1-PAT).
Thus, wehave two different levels of semantic description:the argument position and the specific thematic role.This information was previously stored in a verballexicon for each language.
In these lexicons, a se-mantic class was established for each verbal sense,and the mapping between their syntactic functionswith the corresponding argument structure and the-matic roles was declared.
These classes resultedfrom the analysis of 1,555 verbs from the Span-ish corpus and 1,077 from the Catalan.
The anno-tation process was performed in two steps: firstly,we annotated automatically the unambiguous cor-respondences between syntactic functions and the-matic roles (Mart??
et al, 2007a); secondly, we man-ually checked the outcome of the previous processand completed the rest of thematic role assignments.2.5 Subset for SemEval-2007The corpora extracted from CESS-ECE to conformSemEval-2007 datasets are: (a) SemEval-CESS-ESP (Spanish), made of 101,136 words (3,611 sen-tences), with 29% of the corpus coming from theSpanish EFE News Agency and 71% coming fromLexesp, a Spanish balanced corpus; (b) SemEval-CESS-CAT (Catalan), consisting of 108,207 words(3,202 sentences), with 71% of the corpus consistinfof Catalan news from EFE News Agency and 29%coming from the Catalan News Agency (ACN).These corpora were split into training and testsubsets following a a 90%?10% proportion.
Each43test set was also partitioned into two subsets: ?in-domain?
and ?out-of-domain?
test corpora.
The firstis intended to be homogeneous with respect to thetraining corpus and the second was extracted froma part of the CESS-ECE corpus annotated later andnot involved in the development of the resources(e.g., verbal dictionaries).23 Task settingData formats are similar to those of CoNLL-2004/2005 shared tasks on SRL (column style pre-sentation of levels of annotation), in order to beable to share evaluation tools and already developedscripts for format conversion.In Figure 1 you can find an example of a fully an-notated sentence in the column-based format.
Thereis one line for each token, and a blank line after thelast token of each sentence.
The columns, separatedby blank spaces, represent different annotations ofthe sentence with a tagging along words.
For struc-tured annotations (parse trees, named entities, andarguments), we use the Start-End format.
Columns1?6 correspond to the input information; columns 7and above contain the information to be predicted.We can group annotations in five main categories:BASIC INPUT INFO (columns 1?3).
The basic inputinformation, including: (a) WORD (column 1) wordsof the sentence; (b) TN (column 2) target nouns ofthe sentence, marked with ?*?
(those that are to beassigned WordNet synsets); (c) TV (column 3) targetverbs of the sentence, marked with ?*?
(those that areto be annotated with semantic roles).EXTRA INPUT INFO (columns 4?6).
The extra inputinformation, including: (a) LEMMA (column 4) lem-mas of the words; (b) POS (column 5) part-of-speechtags; (c) SYNTAX (column 6) Full syntactic tree.NE (column 7).
Named Entities.NS (column 8).
WordNet sense of target nouns.SR (columns 9 and above).
Information on semanticroles, including: (a) SC (column 9).
Semantic classof the verb; (b) PROPS (columns 10 and above).
Foreach target verb, a column representing the argu-ment structure.
Core numbered arguments include2For historical reasons we referred to these splits as ?3LB?and ?CESS-ECE?, respectively.
Participants in the task are ac-tually using these names, but we opted for using a more simplenotation in this paper (see Section 5).the thematic role labels.
ArgM?s are the adjuncts.Columns are ordered according to the textual orderof the predicates.All these annotations in column format are ex-tracted automatically from the syntactic-semantictrees from the CESS-ECE corpora, which were dis-tributed with the datasets.
Participants were alsoprovided with the whole Catalan and Spanish Word-Nets (v1.6), the verbal lexicons used in the role la-beling annotation, the annotation guidelines as wellas the annotated corpora.4 Participant systemsAbout a dozen teams expressed their interest in thetask.
From those, only 5 registered and downloadeddatasets, and finally, only two teams met the dead-line and submitted results.
ILK2 (Tilburg Univer-sity) presented a system addressing Semantic RoleLabeling, and UPC* (Technical University of Cat-alonia) presented a system addressing all subtasksindependently3 .
The ILK2 SRL system is basedon memory-based classification of syntactic con-stituents using a rich feature set.
UPC* used severalmachine learning algorithms for addressing the dif-ferent subtasks (AdaBoost, SVM, Perceptron).
ForSRL, the system implements a re-ranking strategyusing global features.
The candidates are generatedusing a state?of?the?art SRL base system.Although the task targeted at systems addressingall subtasks jointly none of the participants did it.4We believe that the high complexity of the wholetask together with the short period of time avail-able were the main reasons for this failure.
Fromthis point of view, the conclusions are somehow dis-appointing.
However, we think that we have con-tributed with a very valuable resource for the futureresearch and, although not complete, the current sys-tems provide also valuable insights about the taskand are very good baselines for the systems to come.5 EvaluationIn the following subsections we present an analysisof the results obtained by participant systems in the3Some members of this team are also task organizers.
Thisis why we mark the team name with an asterisk.4The UPC* team tried some inter-task features to improveSRL but initial results were not successful.44INPUT--------------------------------------------------------------> OUTPUT-----------------------------------BASIC_INPUT_INFO-----> EXTRA_INPUT_INFO---------------------------> NE NS-------> SR------------------------>WORD TN TV LEMMA POS SYNTAX NE NS SC PROPS----------->---------------------------------------------------------------------------------------------------------------Las - - el da0fp0 (S(sn-SUJ(espec.fp*) * - - * (Arg1-TEM*conclusiones * - conclusion ncfp000 (grup.nom.fp* * 05059980n - * *de - - de sps00 (sp(prep*) * - - * *la - - el da0fs0 (sn(espec.fs*) (ORG* - - * *comision * - comision ncfs000 (grup.nom.fs* * 06172564n - * *Zapatero - - Zapatero np00000 (grup.nom*) (PER*) - - * *, - - , Fc (S.F.R* * - - * *que - - que pr0cn00 (relatiu-SUJ*) * - - (Arg0-CAU*) *ampliara - * ampliar vmif3s0 (gv*) * - a1 (V*) *el - - el da0ms0 (sn-CD(espec.ms*) * - - (Arg1-PAT* *plazo * - plazo ncms000 (grup.nom.ms* * 10935385n - * *de - - de sps00 (sp(prep*) * - - * *trabajo * - trabajo ncms000 (sn(grup.nom.ms*))))) * 00377835n - *) *, - - , Fc *)))))) *) - - * *)quedan - * quedar vmip3p0 (gv*) * - b3 * (V*)para - - para sps00 (sp-CC(prep*) * - - * (ArgM-TMP*despues_del - - despues_del spcms (sp(prep*) * - - * *verano * - verano ncms000 (sn(grup.nom.ms*)))) * 10946199n - * *).
- - .
Fp *) * - - * *Figure 1: An example of an annotated sentence.three subtasks.
Results on the test set are presentedalong 2 dimensions: (a) language (?ca?=Catalan;?es?=Spanish); (b) corpus source (?in?=in?domaincorpus; ?out?=out?of?domain corpus).
We will usea language.source pair to denote a particular test set.Finally, ?*?
will denote the addition of the two sub-corpora, either in the language or source dimensions.5.1 NSDResults on the NSD subtask are presented in Table 1.BSL stands for a baseline system consisting of as-signing to each word occurrence the most frequentsense in the training set.
For new nouns the firstsense in the corresponding WordNet is selected.
TheUPC* team trained a SVM classifier for each word ina pre-selected subset and applied the baseline in therest of cases.
The selected words are frequent words(more than 15 occurrences in the training corpus)showing a not too skewed distribution of senses inthe training set (the most predominant sense coversless than 90% of the cases).
No other teams pre-sented results for this task.All words Selected wordsTest BSL UPC* BSL UPC*ca.
* 85.49% 86.47% 70.06% 72.75%es.
* 84.22% 85.10% 61.80% 65.17%*.in 84.84% 86.49% 67.30% 72.24%*.out 85.02% 85.33% 67.07% 67.87%*.
* 84.94% 85.87% 67.19% 70.12%Table 1: Overall accuracy on the NSD subtaskThe left part of the table (?all words?)
containsresults on the complete test sets, while the right part(?selected words?)
contains the results restricted tothe set of words with trained SVM classifiers.
Thisset covers 31.0% of the word occurrences in thetraining set and 28.2% in the complete test set.The main observation is that training/test corporacontain few sense variations.
Sense distributions arevery skewed and, thus, the simple baseline shows avery high accuracy (almost 85%).
The UPC* systemonly improves BSL accuracy by one point.
This canbe partly explained by the small size of the word-based training corpora.
Also, this improvement isdiminished because UPC* only treated a subset ofwords.
However, looking at the right?hand sideof the table, the improvement over the baseline isstill modest (?3 points) when focusing only on thetreated words.
As a final observation, no significantdifferences are observed across languages and cor-pora sources.5.2 NERResults on the NER subtask are presented in Table 2.This time, BSL stands for a baseline system consist-ing of collecting a gazetteer with the strong NEs ap-pearing in the training set and assigning the longestmatches of these NEs in the test set.
Weak entitiesare simply ignored by BSL.
UPC* presented a systemwhich treats strong and weak NEs in a pipeline oftwo processors.
Classifiers trained with multiclass45AdaBoost are used to predict the strong and weakNEs.
See authors?
paper for details.BSL UPC*Test Prec.
Recall F1 Prec.
Recall F1ca.
* 75.85 15.45 25.68 80.94 77.96 79.42es.
* 71.88 12.07 20.66 70.65 65.69 68.08*.in 83.06 17.43 28.82 78.21 74.04 76.09*.out 68.63 12.20 20.72 76.21 72.51 74.31*.
* 74.45 14.11 23.72 76.93 73.08 74.96Table 2: Overall results on the NER subtaskUPC* system largely overcomes the baseline,mainly due to the low recall of the latter.
By lan-guages, results on Catalan are significantly betterthan those on Spanish.
We think this is attributablemainly to corpora variations across languages.
Bycorpus source, ?in-domain?
results are slightly bet-ter, but the difference is small (1.78 points).
Overall,the results for the NER task are in the mid seventies,a remarkable result given the small training set andthe complexity of predicting embedded NEs.Detailed results on concrete entity types are pre-sented in Table 3 (sorted by decreasing F1).
As ex-pected, DAT and NUM are the easiest entities to rec-ognize since they can be easily detected by simplepatterns and POS tags.
On the contrary, entity typesrequiring more semantic information present fairlylower results.
ORG PER and LOC are in the sev-enties, while OTH is by far the most difficult class,showing a very low recall.
This is not surprisingsince OTH agglutinates a wide variety of entity caseswhich are difficult to characterize as a whole.Prec.
Recall F1DAT 97.38% 96.88% 97.13NUM 98.05% 89.68% 93.68ORG 75.72% 75.36% 75.54PER 70.48% 75.97% 73.13LOC 73.41% 68.29% 70.76OTH 56.90% 37.79% 45.41Table 3: Detailed results on the NER subtask: UPC*team; Test corpus *.
*Another interesting analysis is to study the differ-ences between strong and weak entities (see Table4) .
Contrary to our first expectations, results onweak entities are much better (up to 11 F1 pointshigher).
Weak NEs are simpler for two reasons: (a)there exist simple patters to characterize them, with-out the need of fully recognizing their internal strongNEs; (b) there is some redundancy in the corpuswhen tagging many equivalent weak NEs in embed-ded noun phrases.
It is worth noting that the low re-sults for strong NEs come from classification ratherthan recognition (recognition is almost 100% giventhe ?proper noun?
PoS tag), thus the recall for weakentities is not diminished by the errors in strong en-tity classification.Prec.
Recall F1Strong NEs 73.04% 63.36% 67.85Weak NEs 78.96% 78.91% 78.93Table 4: Results on strong vs. weak named entities:UPC* team; Test corpus *.
*5.3 SRLSRL is the most complex and interesting problem inthe task.
We had two participants ILK2 and UPC*,which participated in both subproblems, i.e., label-ing arguments of verbal predicates with thematicroles (SR), and assigning semantic class labels totarget verbs (SC).
Detailed results of the two sys-tems are presented in Tables 5 and 6.UPC* ILK2Test Prec.
Recall F1 Prec.
Recall F1ca.
* 84.49 77.97 81.10 84.72 82.12 83.40es.
* 83.88 78.49 81.10 84.30 83.98 84.14*.in 84.17 82.90 83.53 84.71 84.12 84.41*.out 84.19 72.77 78.06 84.26 81.84 83.03*.
* 84.18 78.24 81.10 84.50 83.07 83.78Table 5: Overall results on the SRL subtask: seman-tic role labeling (SR)The ILK2 system outperforms UPC* in both SRand SC.
For SR, both systems use a traditional ar-chitecture of labeling syntactic tree nodes with the-matic roles using supervised classifiers.
We wouldattribute the overall F1 difference (2.68 points) toa better feature engineering by ILK2, rather thanto differences in the Machine Learning techniquesused.
Overall results in the eighties are remarkablyhigh given the training set size and the granularityof the thematic roles (though we have to take intoaccount that systems work with gold parse trees).Again, the results are comparable across languagesand slightly better in the ?in-domain?
test set.46UPC* ILK2Test Prec.
Recall F1 Prec.
Recall F1ca.
* 86.57 86.57 86.57 90.25 88.50 89.37es.
* 81.05 81.05 81.05 84.30 83.63 83.83*.in 81.17 81.17 81.17 84.68 83.11 83.89*.out 86.72 86.72 86.72 90.04 89.08 89.56*.
* 83.86 83.86 83.86 87.12 85.81 86.46Table 6: Overall results on the SRL subtask: seman-tic class tagging (SC)In the SC subproblem, the differences are simi-lar (2.60 points).
In this case, ILK2 trained special-ized classifiers for the task, while UPC* used heuris-tics based on the SR outcomes.
As a reference,the baseline consisting of tagging each verb withits most frequent semantic class achieves F1 valuesof 64.01, 63.97, 41.00, and 57.42 on ca.in, ca.out,es.in, es.out, respectively.
Now, the results are sig-nificantly better in Catalan, and, surprisingly, the?out?
test corpora makes F1 to raise.
The latter is ananomalous situation provoked by the ?es.in?
tset.5Table 7 shows the global SR results by numberedarguments and adjuncts Interestingly, tagging ad-juncts is far more difficult than tagging core argu-ments (this result was also observed for English inprevious works).
Moreover, the global differencebetween ILK2 and UPC* systems is explained bytheir ability to tag adjuncts (70.22 vs. 58.37).
Inthe core arguments both systems are tied.
Also inthe same table we can see the overall results on asimplified SR setting, in which the thematic roles areeliminated from the SR labels keeping only the argu-ment number (like other evaluations on PropBank).The results are only ?2 points higher in this setting.UPC* ILK2Test Prec.
Recall F1 Prec.
Recall F1Arg 90.41 87.73 89.05 89.42 88.58 88.99Adj 64.72 53.16 58.37 72.54 68.04 70.22A-TR 92.91 90.15 91.51 91.31 90.45 90.88Table 7: Global results on numbered arguments(Arg), adjuncts (Adj), and numbered argumentswithout thematic role tag (A-TR).
Test corpus *.
*Finally, Table 8 compares overall SR results onknown vs. new predicates.
As expected, the re-5By chance, the genre of this part of corpus is mainly liter-ary.
We are currently studying how this is affecting performanceresults on all subtasks and, particularly, semantic class tagging.sults on the verbs not appearing in the training setare lower, but the performance decrease is not dra-matic (3?6 F1 points) indicating that generalizationto new predicates is fairly good.UPC* ILK2Test Prec.
Recall F1 Prec.
Recall F1Known 84.39 78.43 81.30 84.88 83.46 84.16New 81.31 75.56 78.33 79.34 77.81 78.57Table 8: Global results on semantic role labeling forknown versus new predicates.
Test corpus *.
*Acknowledgements The organizers would like tothank the following people for their hard work onthe corpora used in the task: Juan Aparicio, ManuBertran, Oriol Borrega, Nu?ria Buf?
?, Joan Castellv?
?,Maria Jesu?s D?
?az, Marina Lloberes, Difda Mon-terde, Aina Peris, Lourdes Puiggro?s, Marta Re-casens, Santi Reig, and Ba`rbara Soriano.
This re-search has been partially funded by the Spanishgovernment: Lang2World (TIN2006-15265-C06-06) and CESS-ECE (HUM-2004-21127-E) projects.ReferencesAre?valo, M., M. Civit and M. A.
Mart??.
2004.
MICE: a Modulefor Named-Entities Recognition and Classification.
Interna-tional Journal of Corpus Linguistics, 9(1).
John Benjamins,Amsterdam.Borrega, O., M.
Taule?, M. A.
Mart??.
2007.
What do we meanwhen we speak about Named Entities?
In Proceedings ofCorpus Linguistics (forthcoming).
Birmingham, UK.Civit, M., A. Ageno, B. Navarro, N.
Buf??
and M. A.
Mart??.2003.
Qualitative and Quantitative Analysis of Annotatotrs:Agreement in the Development of Cast3LB.
In Proceed-ings of 2nd Workshop on Treebanks and Linguistics Theories(TLT-2003), 33?45.
Vaxjo, Sweden.Ma`rquez, L., M.
Taule?, L.
Padro?, L. Villarejo and M. A.
Mart??.2004.
On the Quality of Lexical Resources for Word SenseDisambiguation.
In Proceedings of the 4th EsTAL Confer-ence, Advances in natural Language Processing, LNCS, vol.3230, 209?221.
Alicante, Spain.Mart?
?, M. A., M.
Taule?, L. Ma`rquez, and M. Bertran.
2007a.Anotacio?n semiautoma?tica con papeles tema?ticos de los cor-pus CESS-ECE.
In Revista de la SEPLN - Monograf?
?a TIMM(forthcoming).Mart?
?, M. A., M.
Taule?, L. Ma`rquez, and M. Bertran.
2007b.CESS-ECE: A multilingual and Multilevel Annotated Cor-pus.
E-pub., http://www.lsi.upc.edu/?mbertran/cess-eceTaule?, M., J.
Castellv??
and M. A.
Mart??.
2006.
SemanticClasses in CESS-LEX: Semantic Annotation of CESS-ECE.In Proceedings of the Fifth Workshop on Treebanks and Lin-guistic Theories (TLT-2006).
Prague, Czech Republic.47
