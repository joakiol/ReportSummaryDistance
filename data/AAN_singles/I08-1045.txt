A Hybrid Feature Set based Maximum Entropy Hindi Named EntityRecognitionSujan Kumar SahaIndian Institute of TechnologyKharagpur, West BengalIndia - 721302sujan.kr.saha@gmail.comSudeshna SarkarIndian Institute of TechnologyKharagpur, West BengalIndia - 721302shudeshna@gmail.comPabitra MitraIndian Institute of TechnologyKharagpur, West BengalIndia - 721302pabitra@gmail.comAbstractWe describe our effort in developing aNamed Entity Recognition (NER) systemfor Hindi using Maximum Entropy (Max-Ent) approach.
We developed a NER an-notated corpora for the purpose.
We havetried to identify the most relevant featuresfor Hindi NER task to enable us to developan efficient NER from the limited corporadeveloped.
Apart from the orthographic andcollocation features, we have experimentedon the efficiency of using gazetteer lists asfeatures.
We also worked on semi-automaticinduction of context patterns and experi-mented with using these as features of theMaxEnt method.
We have evaluated the per-formance of the system against a blind testset having 4 classes - Person, Organization,Location and Date.
Our system achieved af-value of 81.52%.1 IntroductionNamed Entity Recognition involves locating andclassifying the names in text.
NER is an importanttask, having applications in Information Extraction(IE), question answering, machine translation and inmost other NLP applications.NER systems have been developed for Englishand few other languages with high accuracies.
Thesesystems take advantage of large amount of NamedEntity (NE) annotated corpora and other NER re-sources.
However when we started working on aNER system for Hindi, we did not have any NERannotated corpora for Hindi, neither did we have ac-cess to any comprehensive gazetteer list.In this work we have identified suitable featuresfor the Hindi NER task.
Orthography features, thesuffix and prefix information, as well as informationabout the sorrounding words and their tags are usedto develop a Maximum Entropy (MaxEnt) basedHindi NER system.
Additionally, we have acquiredgazetteer lists for Hindi and used these gazetteers inthe Maximum Entropy (MaxEnt) based Hindi NERsystem.
We also worked on semi-automaticallylearning of context pattern for identifying names.These context pattern rules have been integrated intothe MaxEnt based NER system, leading to a high ac-curacy.The paper is organized as follows.
A brief surveyof different techniques used for the NER task in dif-ferent languages and domains are presented in Sec-tion 2.
The MaxEnt based NER system is describedin Section 3.
Various features used in NER are thendiscussed.
Next we present the experimental resultsand related discussions.
Finally Section 8 concludesthe paper.2 Previous WorkA variety of techniques has been used for NER.
Thetwo major approaches to NER are:1.
Linguistic approaches.2.
Machine Learning based approaches.The linguistic approaches typically use rules man-ually written by linguists.
There are several rule-based NER systems, containing mainly lexicalized343grammar, gazetteer lists, and list of trigger words,which are capable of providing 88%-92% f-measureaccuracy for English (Grishman, 1995; McDonald,1996; Wakao et al, 1996).The main disadvantages of these rule-based tech-niques are that these require huge experience andgrammatical knowledge of the particular languageor domain and these systems are not transferable toother languages or domains.Machine Learning (ML) based techniques forNER make use of a large amount of NE anno-tated training data to acquire high level languageknowledge.
Several ML techniques have been suc-cessfully used for the NER task of which HiddenMarkov Model (Bikel et al, 1997), Maximum En-tropy (Borthwick, 1999), Conditional Random Field(Li and Mccallum, 2004) are most common.
Com-binations of different ML approaches are also used.Srihari et al (2000) combines Maximum Entropy,Hidden Markov Model and handcrafted rules tobuild an NER system.NER systems use gazetteer lists for identifyingnames.
Both the linguistic approach (Grishman,1995; Wakao et al, 1996) and the ML based ap-proach (Borthwick, 1999; Srihari et al, 2000) usegazetteer lists.The linguistic approach uses hand-crafted ruleswhich needs skilled linguistics.
Some recent ap-proaches try to learn context patterns through MLwhich reduce amount of manual labour.
Talukder etal.
(2006) combined grammatical and statistical tech-niques to create high precision patterns specific forNE extraction.
An approach to lexical pattern learn-ing for Indian languages is described by Ekbal andBandopadhyay (2007).
They used seed data and an-notated corpus to find the patterns for NER.The NER task for Hindi has been explored byCucerzan and Yarowsky in their language indepen-dent NER work which used morphological and con-textual evidences (Cucerzan and Yarowsky, 1999).They ran their experiment with 5 languages - Roma-nian, English, Greek, Turkish and Hindi.
Amongthese the accuracy for Hindi was the worst.
ForHindi the system achieved 41.70% f-value with avery low recall of 27.84% and about 85% preci-sion.
A more successful Hindi NER system wasdeveloped by Wei Li and Andrew Mccallum (2004)using Conditional Random Fields (CRFs) with fea-ture induction.
They were able to achieve 71.50%f-value using a training set of size 340k words.
InHindi the maximum accuracy is achieved by (Kumarand Bhattacharyya, 2006).
Their Maximum EntropyMarkov Model (MEMM) based model gives 79.7%f-value.3 Maximum Entropy Based ModelWe have used a Maximum Entropy model to buildthe NER in Hindi.
MaxEnt is a flexible statisticalmodel which assigns an outcome for each tokenbased on its history and features.
MaxEnt computesthe probability p(o|h) for any o from the space ofall possible outcomes O, and for every h from thespace of all possible histories H .
A history is allthe conditioning data that enables one to assignprobabilities to the space of outcomes.
In NER,history can be viewed as all information derivablefrom the training corpus relative to the currenttoken.
The computation of p(o|h) in MaxEntdepends on a set of features, which are helpful inmaking predictions about the outcome.
The featuresmay be binary-valued or multi-valued.
For instance,one of our features is: the current token is a partof the surname list; how likely is it to be part ofa person name.
Formally, we can represent thisfeature as follows:f(h, o) ={1 if wi in surname list and o = person0 otherwise(1)Given a set of features and a training corpus,the MaxEnt estimation process produces a modelin which every feature fi has a weight ?i.
We cancompute the conditional probability as (Pietra et al,1997):p(o|h) = 1Z(h)?i?ifi(h,o) (2)Z(h) =?o?i?ifi(h,o) (3)So the conditional probability of the outcome isthe product of the weights of all active features, nor-malized over the products of all the features.
Forour development we have used a Java based open-nlp MaxEnt toolkit1 to get the probability values of1www.maxent.sourceforge.net.344a word belonging to each class.
That is, given a se-quence of words, the probability of each class is ob-tained for each word.
To find the most probable tagcorresponding to each word of a sequence, we canchoose the tag having the highest class conditionalprobability value.
But this method is not good as itmight result in an inadmissible output tag.Some tag sequences should never happen.
Toeliminate these inadmissible sequences we havemade some restrictions.
Then we used a beamsearch algorithm with a beam of length 3 with theserestrictions.The training data for this task is composed ofabout 243K words which is collected from thepopular daily Hindi newspaper ?Dainik Jagaran?.This corpus has been manually annotated and hasabout 16,482 NEs.
In this development we haveconsidered 4 types of NEs, these are Person(P),Location(L), Organization(O) and Date(D).
Torecognize entity boundaries each name class Nis subdivided into 4 sub-classes, i.e., N Begin,N Continue, N End, and N Unique.
Hence,there are a total of 17 classes including 1 class fornot-name.
The corpus contains 6, 298 Person, 4, 696Location, 3, 652 Organization and 1, 845 Date enti-ties.4 Features for Hindi NERMachine learning approaches like MaxEnt, CRF etc.make use of different features for identifying theNEs.
Orthographic features (like capitalization, dec-imal, digits), affixes, left and right context (like pre-vious and next words), NE specific trigger words,gazetteer features, POS and morphological featuresetc.
are generally used for NER.
In English andsome other languages, capitalization features playan important role as NEs are generally capitalizedfor these languages.
Unfortunately this feature is notapplicable for Hindi.
Also Indian person names aremore diverse, lots of common words having othermeanings are also used as person names.
Thesemake difficult to develop a NER system on Hindi.Li and Mccallum (2004) used the entire word text,character n-grams (n = 2, 3, 4), word prefix and suf-fix of lengths 2, 3 and 4, and 24 Hindi gazetteer listsas atomic features in their Hindi NER.
Kumar andBhattacharyya (2006) used word features (suffixes,digits, special characters), context features, dictio-nary features, NE list features etc.
in their MEMMbased Hindi NER system.
In the following we havediscussed about the features we have identified andused to develop the Hindi NER system.4.1 Feature DescriptionThe features which we have identified for HindiNamed Entity Recognition are:Static Word Feature: The previous and nextwords of a particular word are used as features.
Theprevious m words (wi?m...wi?1) to next n words(wi+1...wi+n) can be treated.
During our experi-ment different combinations of previous 4 to next4 words are used.Context Lists: Context words are defined as thefrequent words present in a word window for a par-ticular class.
We compiled a list of the most frequentwords that occur within a window of wi?3...wi+3of every NE class.
For example, location con-text list contains the words like ?jAkara2?
(go-ing to), ?desha?
(country), ?rAjadhAnI?
(capital)etc.
and person context list contains ?kahA?
(say),?prdhAnama.ntrI?
(prime minister) etc.
For agiven word, the value of this feature correspond-ing to a given NE type is set to 1 if the windowwi?3...wi+3 around the wi contains at last one wordfrom this list.Dynamic NE tag: Named Entity tags of the pre-vious words (ti?m...ti?1) are used as features.First Word: If the token is the first word of asentence, then this feature is set to 1.
Otherwise, itis set to 0.Contains Digit: If a token ?w?
contains digit(s)then the feature ContainsDigit is set to 1.
Thisfeature is helpful for identifying company productnames (e.g.
06WD1992), house number (e.g.
C226)etc.Numerical Word: For a token ?w?
if the wordis a numerical word i.e.
a word denoting a number(e.g.
eka (one), do (two), tina (three) etc.)
then thefeature NumWord is set to 1.Word Suffix: Word suffix information is helpfulto identify the named NEs.
Two types of suffix fea-tures have been used.
Firstly a fixed length wordsuffix of the current and surrounding words are used2All Hindi words are written in italics using the ?Itrans?transliteration.345as features.
Secondly we compiled lists of commonsuffixes of person and place names in Hindi.
For ex-ample, ?pura?, ?bAda?, ?nagara?
etc.
are locationsuffixes.
We used two binary features correspond-ing to the lists - whether a given word has a suffixfrom the list.Word Prefix: Prefix information of a word maybe also helpful in identifying whether it is a NE.
Afixed length word prefix of current and surroundingwords are treated as a features.Parts-of-Speech (POS) Information: The POSof the current word and the surrounding words maybe useful feature for NER.
We have access to a HindiPOS pagger developed at IIT Kharagpur which hasan accuracy about 90%.
The tagset of the taggercontains 28 tags.
We have used the POS values ofthe current and surrounding tokens as features.We realized that the detailed POS tagging is notvery relevant.
Since NEs are noun phrases, the nountag is very relevant.
Further the postposition follow-ing a name may give a clue to the NE type.
So we de-cided to use a coarse-grained tagset with only threetags - nominal (Nom), postposition (PSP) and other(O).The POS information is also used by defining sev-eral binary features.
An example is the NomPSPbinary feature.
The value of this feature is definedto be 1 if the current token is nominal and the nexttoken is a PSP.5 Enhancement using Gazetteer FeatureLists of names of various types are helpful in nameidentification.
We have compiled some specializedname lists from different web sources.
But thenames in these lists are in English, not in Hindi.So we have transliterated these English name liststo make them useful for our Hindi NER task.For the transliteration we have build a 2-phasetransliteration module.
We have defined an inter-mediate alphabet containing 34 characters.
Englishnames are transliterated to this intermediate form us-ing a map-table.
Hindi strings are also transliter-ated to the intermediate alphabet form using a dif-ferent map-table.
For a English-Hindi string pair,if transliterations of the both strings are same, thenwe conclude that one string is the transliteration ofthe other.
This transliteration module works with91.59% accuracy.Using the transliteration approach we have con-structed 8 lists.
Which are, month name and days ofthe week (40)3, organization end words list (92), per-son prefix words list (123), list of common locations(80), location names list (17,600), first names list(9722), middle names list (35), surnames list (1800).The lists can be used in name identification in var-ious ways.
One way is to check whether a token isin any list.
But this approach is not good as it hassome limitations.
Some words may present in two ormore gazetteer lists.
For example, ?bangAlora?
is insurnames list and also in location names list.
Confu-sions arise to make decisions for these words.
Somewords are in gazetteer lists but sometimes these areused in text as not-name entity.
For example, ?gayA?is in location list but sometimes the word is used asverb in text and makes confusion.
These limitationsmight be reduced if the contexts are considered.We have used these gazetteer lists as featuresof MaxEnt.
We have prepared several binary fea-tures which are defined as whether a given word isin a particular list.
For example, a binary featureFirstName is 1 for a particular token ?t?
if ?t?
is inthe first name list.6 Context Pattern based FeaturesContext patterns are helpful for identifying NEs.
Asmanual identification of context patterns takes muchmanual labour and linguistic knowledge, we havedeveloped a module for semi-automatically learningof context pattern.
The summary of the context pat-tern learning module is given follows:1.
Collect some seed entities (E) for each class.2.
For each seed entity e in E, from the corpusfind context string(C) comprised of n tokensbefore e, a placeholder for the class instanceand n tokens after e. [We have used n = 3]This set of tokens form initial pattern.3.
Search the pattern in the corpus and find thecoverage and precision.4.
Discard the patterns having low precision.3The italics integers in brackets indicate the size of the lists.3465.
Generalize the patterns by dropping one ormore tokens to increase coverage.6.
Find best patterns having good precision andcoverage.The quality of a pattern is measured by precisionand coverage.
Precision is the ratio of correct iden-tification and the total identification, when the par-ticular pattern is used to identify of NEs of a spe-cific type from a raw text.
Coverage is the amountof total identification.
We have given more impor-tance to precision and we have marked a pattern aseffective if the precision is more than 95%.
Themethod is applied on an un-annotated text having4887011 words collected from ?Dainik Jagaran?
andcontext patterns are learned.
These context patternsare used as features of MaxEnt in the Hindi NERsystem.
Some example patterns are:1. mukhyama.ntrI <PER> Aja2.
<PER> ne kahA ki3.
rAjadhAnI <LOC> me7 EvaluationWe have evaluated the system using a blind test cor-pus of 25K words, which is distinct from the trainingcorpus.
The accuracies are measured in terms of thef-measure, which is the weighted harmonic mean ofprecision and recall.
Here we can mention that wehave evaluated the performance of the system on ac-tual NEs.
That means the system annotates the testdata using 17 tags, similar to the training data.
Dur-ing evaluation we have merged the sub-tags of a par-ticular entity to get a complete NEs and calculatedthe accuracies.
At the end of section 7.1 we havealso mentioned the accuracies if evaluated on thetags.
A number of experiments are conducted con-sidering various combinations of features to identifythe best feature set for the Hindi NER task.7.1 BaselineThe baseline performance of the system without us-ing gazetteer and context patterns are presented inTable 1.
They are summarized below.While experimenting with static word features,we have observed that a window of previous twoFeature Class F-valuef1 = Word, NE TagPER 63.33LOC 69.56ORG 58.58DAT 91.76TOTAL 69.64f2 = Word, NE Tag,PER 69.75LOC 75.8ORG 59.31Suffix (?
2) DAT 89.09TOTAL 73.42f3 = Word, NE Tag,PER 70.61LOC 71ORG 59.31Suffix (?
2), Prefix DAT 89.09TOTAL 72.5f4 = Word, NE Tag,PER 70.61LOC 75.8ORG 60.54Digit, Suffix (?
2) DAT 93.8TOTAL 74.26f5 = Word, NE Tag, POSPER 64.25LOC 71ORG 60.54DAT 89.09TOTAL 70.39Suffix (?
2), Digit,PER 72.26f6 = Word, NE Tag, LOC 78.6ORG 51.36NomPSP DAT 92.82TOTAL 75.6Table 1: F-values for different featureswords to next two words (Wi?2...Wi+2) gives bestresults.
But when several other features are com-bined then single word window (Wi?1...Wi+1) per-forms better.
Similarly we have experimented withsuffixes of different lengths and observed that thesuffixes of length ?
2 gives the best result for theHindi NER task.
In using POS information, wehave observed that the coarse-grained POS taggerinformation is more effective than the finer-grainedPOS values.
A feature set, combining finer-grainedPOS values, surrounding words and previous NEtag, gives a f-value of 70.39%.
But when thecoarse-grained POS values are used instead of the347finer-grained POS values, the f-value is increasedto 74.16%.
The most interesting fact we have ob-served that more complex features do not guaran-tee to achieve better results.
For example, a featureset combined with current and surrounding words,previous NE tag and fixed length suffix information,gives a f-value 73.42%.
But when prefix informationare added the f-value decreased to 72.5%.
The high-est accuracy achieved by the system is 75.6% f-valuewithout using gazetteer information and context pat-terns.The results in Table 1 are obtained by evaluatingon the actual NEs.
But when the system is evaluatedon the tags the f-value increases.
For f6, the accu-racy achieved on actual NEs is 75.6%, but if eval-uated on tags, the value increased to 77.36%.
Sim-ilarly, for f2, the accuracy increased to 75.91% ifevaluated on tags.
The reason is the NEs contain-ing 3 or more words, are subdivided to N-begin, N-continue (1 or more) and N-end.
So if there is anerror in any of the subtags, the total NE becomesan error.
We observed many cases where NEs arepartially identified by the system, but these are con-sidered as error during evaluation.7.2 Using Gazetteer Lists and Context PatternsNext we add gazetteer and context patterns as fea-tures in our MaxEnt based NER system.
In Ta-ble 2 we have compared the results after additionof gazetteer information and context patterns withprevious results.
While experimenting we have ob-served that gazetteer lists and context patterns arecapable of increasing the performance of our base-line system.
That is tested on all the baseline featuresets.
In Table 2 the comparison is shown for onlytwo features - f2 and f6 which are defined in Table 1.It may be observed that the relative advantage of us-ing both gazetteer and context patterns together overusing them individually is not much.
For example,when gazetteer information are added with f2, the f-value is increased by 6.38%, when context patternsare added the f-value is increased by 6.64%., butwhen both are added the increment is 7.27%.
Thismay be due to the fact that both gazetteer and con-text patterns lead to the same identifications.
Usingthe comprehensive feature set (using gazetteer infor-mation and context patterns) the MaxEnt based NERsystem achieves the maximum f-value of 81.52%.F-valueFea-tureClass NoGazorPatWithGazWithPatWithGazandPatf2PER 69.75 74.2 75.61 76.03LOC 75.8 82.02 79.94 82.02ORG 59.31 72.61 73.4 74.63DAT 89.09 94.29 95.32 95.32TOTAL 73.42 79.8 80.06 80.69f6PER 72.26 76.03 75.61 78.41LOC 78.6 82.02 80.49 83.26ORG 51.36 72.61 74.1 75.43DAT 92.82 94.28 95.87 96.5TOTAL 75.6 80.24 80.37 81.52Table 2: F-values for different features withgazetteers and context patterns8 ConclusionWe have shown that our MaxEnt based NER sys-tem is able to achieve a f-value of 81.52%, using ahybrid set of features including traditional NER fea-tures augmented with gazetteer lists and extractedcontext patterns.
The system outperforms the exist-ing NER systems in Hindi.Feature selection and feature clustering mightlead to further improvement of performance and isunder investigation.9 AcknowledgementThe work is partially funded by Microsoft ResearchIndia.ReferencesBikel Daniel M., Miller Scott, Schwartz Richard andWeischedel Ralph.
1997.
Nymble: A High Perfor-mance Learning Name-finder.
In Proceedings of theFifth Conference on Applied Natural Language Pro-cessing, pages 194?201.Borthwick Andrew.
1999.
A Maximum Entropy Ap-proach to Named Entity Recognition.
Ph.D. thesis,Computer Science Department, New York University.Cucerzan Silviu and Yarowsky David.
1999.
LanguageIndependent Named Entity Recognition Combining348Morphological and Contextual Evidence.
In Proceed-ings of the Joint SIGDAT Conference on EMNLP andVLC 1999, pages 90?99.Ekbal A. and Bandyopadhyay S. 2007.
Lexical PatternLearning from Corpus Data for Named Entity Recog-nition.
In Proceedings of International Conference onNatural Language Processing (ICON), 2007.Grishman Ralph.
1995.
The New York University Sys-tem MUC-6 or Where?s the syntax?
In Proceedings ofthe Sixth Message Understanding Conference.Kumar N. and Bhattacharyya Pushpak.
2006.
NamedEntity Recognition in Hindi using MEMM.
In Techni-cal Report, IIT Bombay, India..Li Wei and McCallum Andrew.
2004.
Rapid Develop-ment of Hindi Named Entity Recognition using Con-ditional Random Fields and Feature Induction (ShortPaper).
ACM Transactions on Computational Logic.McDonald D. 1996.
Internal and external evidence in theidentification and semantic categorization of propernames.
In B. Boguraev and J. Pustejovsky, editors,Corpus Processing for Lexical Acquisition, pages 21?39.Pietra Stephen Della, Pietra Vincent Della and LaffertyJohn.
1997.
Inducing features of random fields.
IEEETransactions on Pattern Analysis and Machine Intelli-gence, 19(4):380?393.Srihari R., Niu C. and Li W. 2000.
A Hybrid Approachfor Named Entity and Sub-Type Tagging.
In Proceed-ings of the sixth conference on Applied natural lan-guage processing.Talukdar Pratim P., Brants T., Liberman M., and PereiraF.
2006.
A context pattern induction methodfor named entity extraction.
In Proceedings of theTenth Conference on Computational Natural Lan-guage Learning (CoNLL-X).Wakao T., Gaizauskas R. and Wilks Y.
1996.
Evaluationof an algorithm for the recognition and classificationof proper names.
In Proceedings of COLING-96.349
