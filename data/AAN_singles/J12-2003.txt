Did It Happen?
The Pragmatic Complexity ofVeridicality AssessmentMarie-Catherine de Marneffe?Stanford UniversityChristopher D.
Manning?
?Stanford UniversityChristopher Potts?Stanford UniversityNatural language understanding depends heavily on assessing veridicality?whether eventsmentioned in a text are viewed as happening or not?but little consideration is given to this prop-erty in current relation and event extraction systems.
Furthermore, the work that has been donehas generally assumed that veridicality can be captured by lexical semantic properties whereaswe show that context and world knowledge play a significant role in shaping veridicality.
Weextend the FactBank corpus, which contains semantically driven veridicality annotations, withpragmatically informed ones.
Our annotations are more complex than the lexical assumptionpredicts but systematic enough to be included in computational work on textual understanding.They also indicate that veridicality judgments are not always categorical, and should thereforebe modeled as distributions.
We build a classifier to automatically assign event veridicalitydistributions based on our new annotations.
The classifier relies not only on lexical featureslike hedges or negations, but also on structural features and approximations of world knowledge,thereby providing a nuanced picture of the diverse factors that shape veridicality.
?All I know is what I read in the papers?
?Will Rogers1.
IntroductionA reader?s or listener?s understanding of an utterance depends heavily on assessing theextent to which the speaker (author) intends to convey that the events described did(or did not) happen.
An unadorned declarative like The cancer has spread conveys firmspeaker commitment, whereas qualified variants such as There are strong indicators thatthe cancer has spread or The cancer might have spread imbue the claim with uncertainty.
We?
Linguistics Department, Margaret Jacks Hall Building 460, Stanford CA 94305, USA.E-mail: mcdm@stanford.edu.??
Linguistics Department & Computer Science Department, Gates Building 1A, 353 Serra Mall, StanfordCA 94305, USA.
E-mail: manning@stanford.edu.?
Linguistics Department, Margaret Jacks Hall Building 460, Stanford CA 94305, USA.E-mail: cgpotts@stanford.edu.Submission received: 4 April 2011; revised submission received: 1 October 2011; accepted for publication:30 November 2011.?
2012 Association for Computational LinguisticsComputational Linguistics Volume 38, Number 2call this event veridicality, building on logical, linguistic, and computational insightsabout the relationship between language and reader commitment (Montague 1969;Barwise 1981; Giannakidou 1994, 1995, 1999, 2001; Zwarts 1995; Asher and Lascarides2003; Karttunen and Zaenen 2005; Rubin, Liddy, and Kando 2005; Rubin 2007; Saur??2008).
The central goal of this article is to begin to identify the linguistic and contextualfactors that shape readers?
veridicality judgments.1There is a long tradition of tracing veridicality to fixed properties of lexical items(Kiparsky and Kiparsky 1970; Karttunen 1973).
On this view, a lexical item L is veridicalif the meaning of L applied to argument p entails the truth of p. For example, becauseboth true and false things can be believed, one should not infer directly from A believesthat S that S is true, making believe non-veridical.
Conversely, realize appears to be veridi-cal, because realizing S entails the truth of S. The prototypical anti-veridical operator isnegation, because not S entails the falsity of S, but anti-veridicality is a characteristicof a wide range of words and constructions (e.g., have yet to, fail, without).
These basicveridicality judgments can be further subdivided using modal or probabilistic notions.For example, althoughmay is non-veridical by the basic classifications, wemight classifymay S as possiblewith regard to S.2Lexical theories of this sort provide a basis for characterizing readers?
veridicalityjudgments, but they do not tell the whole story, because they neglect the pragmaticenrichment that is pervasive in human communication.
In the lexical view, say can onlybe classified as non-veridical (both true and false things can be said), and yet, if aNew YorkTimes article contained the sentence United Widget said that its chairman resigned, readerswould reliably infer that United Widget?s chairman resigned?the sentence is, in thiscontext, veridical (at least to some degree) with respect to the event described by theembedded clause, with United Widget said functioning to mark the source of evidence(Simons 2007).
Cognitive authority, as termed in information science (Rieh 2010), plays acrucial role in how people judge the veridicality of events.
Here, the provenance of thedocument (theNew York Times) and the source (United Widget) combine to reliably leada reader to infer that the author intended to convey that the event really happened.Conversely, allege is lexically non-veridical, and yet this only begins to address thecomplex interplay of world knowledge and lexical meaning that will shape people?sinferences about the sentence FBI agents alleged in court documents today that Zazi had ad-mitted receiving weapons and explosives training from al Qaeda operatives in Pakistan last year.We conclude from examples like this that veridicality judgments have an importantpragmatic component, and, in turn, that veridicality should be assessed using infor-mation from the entire sentence as well as from the context.
Lexical theories have asignificant role to play here, but we expect their classifications to be buffeted by othercommunicative pressures.
For example, the lexical theory can tell us that, as a narrowlysemantic fact, X alleges S is non-veridical with regard to S. Where X is a trustworthysource for S-type information, however, we might fairly confidently conclude that S istrue.
Where X is known to spread disinformation, we might tentatively conclude thatS is false.
These pragmatic enrichments move us from uncertainty to some degree of1 Our use of the term ?veridicality?
most closely matches that of Giannakidou (1999), where it is definedso as to be (i) relativized to particular agents or perspectives, (ii) gradable, and (iii) general enough tocover not only facts but also the commitments that arise from using certain referential expressions andaspectual morphology.
The more familiar term ?factuality?
seems at odds with all three of these criteria,so we avoid it.2 Lexical notions of veridicality must be relativized to specific argument positions, with the otherarguments existentially closed for the purposes of checking entailment.
For example, believe isnon-veridical on its inner (sentential) argument because ?
?x : x believes p?
does not entail p.302de Marneffe, Manning, and Potts The Pragmatic Complexity of Veridicality Assessmentcertainty.
Such enrichments can be central to understanding how a listener (or a reader)understands a speaker?s (or author?s) message.Embracing pragmatic enrichment means embracing uncertainty.
Although listenerscan feel reasonably confident about the core lexical semantics of the words of their lan-guage, there is no such firm foundation when it comes to this kind of pragmatic enrich-ment.
The newspaper says, United Widget said that its profits were up in the fourth quarter,but just how trustworthy is United Widget on such matters?
Speakers are likely to varyin what they intend in such cases, and listeners are thus forced to operate under uncer-tainty when making the requisite inferences.
Lexical theories allow us to abstract awayfrom these challenges, but a pragmatically informed approach must embrace them.The FactBank corpus is a leading resource for research on veridicality (Saur??
andPustejovsky 2009).
Its annotations are ?textual-based?
: They seek to capture the waysin which lexical meanings and local semantic interactions determine veridicality judg-ments.
In order to better understand the role of pragmatic enrichment, we had a largegroup of linguistically naive annotators annotate a portion of the FactBank corpus,given very loose guidelines.
Whereas the FactBank annotators were explicitly told toavoid bringing world knowledge to bear on the task, our annotators were encouragedto choose labels that reflected their own natural reading of the texts.
Each sentence wasannotated by 10 annotators, which increases our confidence in them and also highlightsthe sort of vagueness and ambiguity that can affect veridicality.
These new annotationshelp confirm our hypothesis that veridicality judgments are shaped by a variety of otherlinguistic and contextual factors beyond lexical meanings.The nature of such cues is central to linguistic pragmatics and fundamental toa range of natural language processing (NLP) tasks, including information extrac-tion, opinion detection, and textual entailment.
Veridicality is prominent in BioNLP,where identifying negations (Chapman et al 2001; Elkin et al 2005; Huang and Lowe2007; Pyysalo et al 2007; Morante and Daelemans 2009) and hedges or ?speculations?
(Szarvas et al 2008; Kim et al 2009) is crucial to proper textual understanding.
Recently,more attention has been devoted to veridicality within NLP, with the 2010 workshopon negation and speculation in natural language processing (Morante and Sporleder2010).
Veridicality was also at the heart of the 2010 CoNLL shared task (Farkas et al2010), where the goal was to distinguish uncertain events from the rest.
The centralityof veridicality assessment to tasks like event and relation extraction is arguably stillnot fully appreciated, however.
At present the vast majority of information extractionsystems work at roughly the clause level and regard any relation they find as true.
Butrelations in actual text may not be facts for all sorts of reasons, such as being embeddedunder an attitude verb like doubt, being the antecedent of a conditional, or being part ofthe report by an untrustworthy source.
To avoid wrong extractions in these cases, it isessential for NLP systems to assess the veridicality of extracted facts.In the present article, we argue for three main claims about veridicality.
First andforemost, we aim to show that pragmatically informed veridicality judgments are sys-tematic enough to be included in computational work on textual understanding.
Sec-ond, we seek to justify FactBank?s seven-point categorization over simpler alternatives(e.g., certain vs. uncertain, as in the CoNLL task).
Finally, the inherent uncertainty ofpragmatic inference suggests to us that veridicality judgments are not always categori-cal, and thus are better modeled as probability distributions over veridicality categories.To substantiate these claims, we analyze in detail the annotations we collected, andwe report on experiments that treat veridicality as a distribution-prediction task.
Ourfeature set includes not only lexical items like hedges, modals, and negations, butalso complex structural features and approximations of world knowledge.
Though the303Computational Linguistics Volume 38, Number 2resulting classifier has limited ability to assess veridicality in complex real worldcontexts, it still does a quite good job of capturing human pragmatic judgments ofveridicality.
We argue that the model yields insights into the complex pragmatic factorsthat shape readers?
veridicality judgments.2.
Corpus AnnotationFactBank?s annotations are intended to isolate semantic effects from pragmatic ones inthe area of veridicality assessment.
Our overarching goal is to examine how pragmaticenrichment affects this picture.
Thus, we use the FactBank sentences in our own inves-tigation, to facilitate comparisons between the two kinds of information and to create asupplement of FactBank itself.
This section introduces FactBank in more detail and thenthoroughly reviews our own annotation project and its results.2.1 FactBank CorpusFactBank provides veridicality annotations on events relative to each participant in-volved in the discourse.
It consists of 208 documents from newswire and broadcastnews reports in which 9,472 event descriptions (verbs, nouns, and adjectives) weremanually identified.
There is no fundamental difference in the way verbs, nouns, andadjectives are annotated.
Events are single words.
The data come from TimeBank 1.2and a fragment of AQUAINT TimeML (Pustejovsky et al 2006).
The documents in theAQUAINT TimeML subset come from two topics: ?NATO, Poland, Czech Republic,Hungary?
and ?the Slepian abortion murder.
?The tags annotate ?event, participant?
pairs in sentences.
The participant can beanyone mentioned in the sentence as well as its author.
In Example (1), the target eventidentified by the word means is assigned a value with respect to both the source someexperts and the author of the sentence.Example 1Some experts now predict Anheuser?s entry into the fray means near-term earningstrouble for all the industry players.Veridicality(means, experts) = PR+Veridicality(means, author) = UuThe tags are summarized in Table 1.
Each tag consists of a veridicality value (certain[CT], probable [PR], possible [PS], underspecified [U]) and a polarity value (positive[+], negative [?
], unknown [u]).
CT+ corresponds to the standard notion of veridicality,CT?
to anti-veridicality, and Uu to non-veridicality.
The PR and PS categories add amodal or probabilistic element to the scale, to capture finer-grained intuitions.Examples (2) and (3) illustrate the annotations for a noun and an adjective.Example 2But an all-out bidding war between the world?s top auto giants for Britain?s leadingluxury-car maker seems unlikely.Veridicality(war, author) = PR?304de Marneffe, Manning, and Potts The Pragmatic Complexity of Veridicality AssessmentTable 1FactBank annotation scheme.
CT = certain; PR = probable; PS = possible; U = underspecified;+ = positive; ?
= negative; u = unknown.Value Definition CountCT+ According to the source, it is certainly the case that X 7,749 (57.6%)PR+ According to the source, it is probably the case that X 363 (2.7%)PS+ According to the source, it is possibly the case that X 226 (1.7%)CT?
According to the source, it is certainly not the case that X 433 (3.2%)PR?
According to the source it is probably not the case that X 56 (0.4%)PS?
According to the source it is possibly not the case that X 14 (0.1%)CTu The source knows whether it is the case that X or that not X 12 (0.1%)Uu The source does not know what the factual status of the event is, 4,607 (34.2%)or does not commit to it13,460Example 3Recently, analysts have said Sun also is vulnerable to competition from InternationalBusiness Machines Corp., which plans to introduce a group of workstations early nextyear, and Next Inc.Veridicality(vulnerable, analysts) = CT+Veridicality(vulnerable, author) = UuThe last column of Table 1 reports the value counts in the corpus.
The data areheavily skewed, with 62% of the events falling to the positive side and 57.6% in the CT+category alone.
The inter-annotator agreement for assigning veridicality tags was high(?
= 0.81, a conservative figure given the partial ordering in the tags).
A training/testsplit is defined in FactBank: The documents from TimeBank 1.2 are used as the trainingdata and the ones from the subset of the AQUAINT TimeML corpus as the testbed.As we noted earlier, FactBank annotations are supposed to be as purely semanticas possible; the goal of the project was to ?focus on identifying what are the judgmentsthat the relevant participants make about the factuality nature of events, independentlyfrom their intentions and beliefs, and exclusively based on the linguistic expressionsemployed in the text to express such judgments,?
disregarding ?external factors such assource reliability or reader bias?
(Saur??
2008, page 5).
The annotation manual containsan extensive set of discriminatory tests (Saur??
2008, pages 230?235) that are informedby lexical theories of veridicality.
The resulting annotations are ?textual-based, thatis, reflecting only what is expressed in the text and avoiding any judgment based onindividual knowledge?
(Saur??
and Pustejovsky 2009, page 253).
In addition, discoursestructure is not taken into account: ?we decided to constrain our annotation to informa-tion only present at the sentence level?
(Saur??
and Pustejovsky 2009, page 253).2.2 Annotations from the Reader?s PerspectiveIn the terminology of Levinson (1995, 2000), FactBank seeks to capture aspects of sen-tence meaning, whereas we aim to capture aspects of utterance meaning, which brings us305Computational Linguistics Volume 38, Number 2closer to characterizing the amount and kind of information that a reader can reliablyextract from an utterance.
We thus extend the FactBank annotations by bringing worldknowledge into the picture.
Whereas the FactBank annotators were explicitly told toavoid any reader bias, to disregard the credibility of the source, and to focus only on thelinguistic terms used in the text to express veridicality, we are interested in capturinghow people judge the veridicality of events when reader bias, credibility of the source,and what we know about the world is allowed to play a role.To do this, we took a subset of the FactBank sentences annotated at the authorlevel and recruited annotators for them using Mechanical Turk.
We restricted the task toannotators located in the United States.
Our subset consists of 642 sentences (466 verbs,155 nouns, 21 adjectives); we use all the PR+, PS+, PR?, PS?
items from the FactBanktraining set plus some randomly chosen Uu, CT+, and CT?
items.
(We did not takeany CTu sentences into account, as there are not enough of them to support experi-mentation.)
The annotators were asked to decide whether they thought the boldfacedevent described in the sentence did (or will) happen.
Thus the judgments are from thereader?s perspective, and not from the author?s or participants?
perspective, as in the originalFactBank annotations.
We used Saur??
?s seven-point annotation scheme (removing CTu).To ensure that the workers understood the task, we first gave them four mandatorytraining items?simple non-corpus examples designed to help them conceptualize theannotation categories properly.
The sentences were presented in blocks of 26 items,three of which were ?tests?
very similar to the training items, included to ensure thatthe workers were careful.
We discarded data from two Turkers because they did notcorrectly tag the three test sentences.3Like Saur?
?, we did not take the discourse structure into account: Turkers sawonly disconnected sentences and judged the event sentence by sentence.
Subsequentmentions of the same event in a discourse can, however, lead a listener to revise averidicality judgment already posed for that event.
For instance in Example (4) fromSaur??
(2008, page 56), a reader?s veridicality judgment about the tipped off event willprobably change when reading the second sentence.Example 4Yesterday, the police denied that drug dealers were tipped off before the operation.However, it emerged last night that a reporter from London Weekend Televisionunwittingly tipped off residents about the raid when he phoned contacts on the estateto ask if there had been a raid?before it had actually happened.Here, though, we concentrate on the sentence level, and leave the issue of discoursestructure for future work.
In other words, we capture the reader?s judgment about theveridicality of an event after each sentence, independent on whether the judgment willbe revised when later information is read.
This is partly to facilitate comparisons withFactBank and partly because we are presently unsure how to computationally modelthe effects of context in this area.Figure 1 shows how the items were displayed to the Turkers.
We rephrased theevent under consideration (the bold sentence in Figure 1), because it is not alwaysstraightforward to identify the intended rephrasing.
Following Saur?
?, we refer to thisrephrasing process as normalization.
The normalization strips out any polarity and3 The data are available at http://christopherpotts.net/ling/data/factbank/.306de Marneffe, Manning, and Potts The Pragmatic Complexity of Veridicality AssessmentFigure 1Design of the Mechanical Turk experiment.modality markers to focus only on the core event talked about.
For example, in Policegave no details, we needed to make sure that workers evaluated the positive form(?Police gave details?
), rather than the negative one (?Police gave no details?).
Similarly,in Hudson?s Bay Co. announced terms of a previously proposed rights issue that is expected toraise about 396 million Canadian dollars (US$337 million) net of expenses, the normalizationwill remove the modality marker is expected (?the proposed rights issue will raiseabout 396 million Canadian dollars net of expenses?).
We followed Saur??
?s extensiveguidelines for this rephrasing process (Saur??
2008, pages 218?222).We collected 10 annotations for each of the 642 events.
A total of 177 Turkersparticipated in the annotations.
Most Turkers did just one batch of 23 non-test examples;themean number of annotations per Turker was 44, and they each annotated between 23and 552 sentences.
Table 2 reports Fleiss kappa scores (Fleiss 1971) using the full seven-category scheme.
These scores are conservative because they do not take into accountthe fact that the scale is partially ordered, with CT+, PR+, and PS+ forming a ?posi-tive?
category, CT?, PR?, and PS?
forming a ?negative?
category, and Uu remainingalone.
The overall Fleiss kappa for this three-category version is much higher (0.66),reflecting the fact that many of the disagreements were about degree of confidence (e.g.,CT+ vs. PR+) rather than the basic veridicality judgment of ?positive?, ?negative?, or?unknown?.
At least 6 out of 10 Turkers agreed on the same tag for 500 of theTable 2Fleiss kappa scores with associated p-values.?
p valueCT+ 0.63 < 0.001CT?
0.80 < 0.001PR+ 0.41 < 0.001PR?
0.34 < 0.001PS+ 0.40 < 0.001PS?
0.12 < 0.001Uu 0.25 < 0.001Overall 0.53 < 0.001307Computational Linguistics Volume 38, Number 2642 sentences (78%).
For 53% of the examples, at least 8 Turkers agreed with each other,and total agreement is obtained for 26% of the data (165 sentences).2.3 An Alternative ScaleOne of our goals is to assess whether FactBank?s seven-category scheme is the right onefor the task.
To this end, we also evaluated whether a five-tag version would increaseagreement and perhaps provide a better match with readers?
intuitions.
Logically, PR?is equivalent to PS+, and PS?
to PR+, so it seemed natural to try to collapse theminto a two-way division between ?probable?
and ?possible?.
We thus ran the MTurkexperiment again with the five-point scheme in Table 3.The five-point scheme led to lower agreement between Turkers.
Globally, the PR?items were generally mapped to ?no?, and PS?
to either ?no?
or ?unknown?.
SomeTurkers chose the expected mappings (PS?
to ?probable?
and PR?
to ?possible?
), butonly very rarely.
This is explicable in terms of the pragmatics of veridicality judgments.Though PR?
may be logically equivalent to PS+, and PS?
to PR+, there are importantpragmatic differences between giving a positive judgment and giving a negative one.For example, in Example (5), speaker B will not infer that he can possibly get a furtherdiscount, even if ?Probably not?
is consistent with ?Possibly?.
Conversely, had theanswer been ?Possibly?, A would have remained hopeful.Example 5A: Is it possible to get further discount on the rate?B: Probably not.In sum, there seems to be a very intuitive notion of veridicality along the partiallyordered scale proposed by Saur??.
In their work on assessing the degree of event certaintyto which an author commits, Rubin, Liddy, and Kando (2005) used the following five-point scale: absolute, high, moderate, low, and uncertain.
They did not obtain very highinter-annotator agreement (?
= 0.41).
Saur??
hypothesized that their low agreement isdue to a fuzzy approach and the lack of precise guidelines.
Rubin, Liddy, and Kandohad no clear identification of certainty markers, and no explicit test for distinguishingdifferent degrees of certainty (unlike Saur??).
In our experiment, however, the guidelineswere similarly loose: Turkers were instructed only to ?read 30 sentences and decidewhether the events described in these sentences did (or will) happen.?
They were notasked to limit their attention to just the information in the sentence, and they were notTable 3An alternative five-tag annotation scheme.Category Originalyes CT+probable PR+/PS?possible PS+/PR?no CT?unknown Uu308de Marneffe, Manning, and Potts The Pragmatic Complexity of Veridicality Assessmentgiven any mappings between linguistic markers and veridicality values.
Nonetheless,Turkers reached good agreement levels in assessing event veridicality on a seven-pointscale.
We conclude from this that Saur??
?s scale comes closer than its competitors tocapturing reader intuitions about veridicality.
The good agreement mirrors the generalhigh inter-annotator agreement levels that have been found for the Recognizing TextualEntailment task (Manning 2006), perhaps reflecting the fact that judging inference andveridicality in context is a natural, everyday human task.Diab et al (2009) annotated a 10,000-word corpus for what they call ?committedbeliefs?
: whether the author of the sentence indicates with linguistic means that hebelieves or disbelieves that the event described by the sentence is a fact.
Thus, in essence,the annotations assess the degree of event certainty to which an author commits, as inRubin?s work.
They use a three-point scale: committed belief, non-committed belief, andnot applicable.
An example of committed belief is GM has laid off workers.
Affirmativesentences in the future are also considered as committed belief (e.g., GM will lay offworkers).
Sentences with modals and events embedded under reported verbs are an-notated as non-committed belief.
The third category, not applicable, consists of sentencesexpressing desire (Some wish GM would lay of workers), questions (Many wonder if GM willlay off workers), and requirements (Lay off workers!).
The corpus covers different genres(newswire, e-mail, blog, dialogue).
The inter-annotator agreement was high (95%).Prabhakaran, Rambow, and Diab (2010) used the corpus to automatically tag committedbeliefs according to that three-point scale.
This, too, is an important resource, but it isdifficult to compare it with our own task, for two reasons.
First, the annotators sought toprevent world knowledge from influencing their annotations, which is concerned onlywith linguistic markers.
Second, the category non-committed belief conflates the possible,probable, and unknown categories of our corpus (Saur???s).
Though some work in thebiomedical domain (i.e., Hobby et al 2000) suggests that the distinction between possi-ble and probable is hard to make, we did not want to avoid it, because people routinelymake such fine-grained modal distinctions when assessing claims.
What?s more, theapproach we develop allows us to quantify the degree to which such judgments arein fact variable and uncertain.3.
Lessons from the New AnnotationsThis section presents two kinds of high-level analysis of our annotations.
We first com-pare them with FactBank annotations for veridicality according to the author, identify-ing places where the annotations point to sharp divergences between sentence meaningand utterance meaning.
We then study the full distribution of annotations we received(10 per sentence), using them to highlight the uncertainty of veridicality judgments.Both of these discussions deeply inform the modeling work of Section 4.3.1 The Impact of Pragmatic EnrichmentAlthough the MTurk annotations largely agree with those of FactBank, there are sys-tematic differences between the two that are indicative of the ways in which pragmaticenrichment plays a role in assessing veridicality.
The goal of this section is to uncoverthose differences.
To sharpen the picture, we limit attention to the sentences for whichthere is a majority-vote category, that is, at least 6 out of 10 Turkers agreed on theannotation.
This threshold was met for 500 of the 642 examples.309Computational Linguistics Volume 38, Number 2Table 4Inter-annotator agreement comparing FactBank annotations with MTurk annotations.
The dataare limited to the 500 examples in which at least 6 of the 10 Turkers agreed on the label, which isthen taken to be the true MTurk label.
The very poor value for PS?
derives from the fact, in thissubset, that label was chosen only once in FactBank and not at all by our annotators.?
p valueCT+ 0.37 < 0.001PR+ 0.79 < 0.001PS+ 0.86 < 0.001CT?
0.91 < 0.001PR?
0.77 < 0.001PS?
?0.001 = 0.982Uu 0.06 = 0.203Overall 0.60 < 0.001Table 4 uses kappa scores to measure the agreement between FactBank and ourannotations on this 500-sentence subset of the data.
We treat FactBank as one annotatorand our collective Turkers as a second annotator, with the majority label the correct onefor that annotator.
What we see is modest to very high agreement for all the categoriesexcept Uu.
The agreement level is also relatively low for CT+.
The correspondingconfusion matrix in Table 5 helps explicate these numbers.
The Uu category is usedmuch more often in FactBank than by Turkers, and the dominant alternative choice forthe Turkers was CT+.
Thus, the low score for Uu also effectively drops the score forCT+.
The question is why this contrast exists.
In other words, why do Turkers chooseCT+ where FactBank says Uu?The divergence can be traced to the way in which lexicalist theories handle eventsembedded under attitude predicates like say, report, and indicate: any such embeddedevent is tagged Uu in FactBank.
In our annotations, readers are not viewing the veridi-cality of reported events as unknown.
Instead they are sensitive to a wide range ofsyntactic and contextual features, including markers in the embedded clause, expec-tations about the subject as a source for the information conveyed by the embeddedclause, and lexical competition between the author?s choice of attitude predicate andits alternatives.
For example, even though the events in Example (6) are all embeddedTable 5Confusion matrix comparing the FactBank annotations (rows) with our annotations (columns).MTurkCT+ PR+ PS+ CT?
PR?
PS?
Uu TotalFactBankCT+ 54 2 0 0 0 0 0 56PR+ 4 63 2 0 0 0 0 69PS+ 1 1 55 0 0 0 2 59CT?
5 0 0 146 0 0 2 153PR?
0 0 0 0 5 0 1 6PS?
0 0 0 0 0 0 1 1Uu 94 18 9 12 2 0 21 156Total 158 84 66 158 7 0 27 500310de Marneffe, Manning, and Potts The Pragmatic Complexity of Veridicality Assessmentunder an attitude predicate (say), the events in Examples (6a) and (6b) are assessed ascertain (CT+), whereas the words highly confident in Example (6c) trigger PR+, and mayin Example (6d) leads to PS+.Example 6(a) Magna International Inc.?s chief financial officer, James McAlpine,resigned and its chairman, Frank Stronach, is stepping in to helpturn the automotive-parts manufacturer around, the company said.Normalization: James McAlpine resignedAnnotations: CT+: 10(b) In the air, U.S. Air Force fliers say they have engaged in ?a little cat andmouse?
with Iraqi warplanes.Normalization: U.S. Air Force fliers have engaged in ?a little cat andmouse?
with Iraqi warplanesAnnotations: CT+: 9, PS+: 1(c) Merieux officials said last week that they are ?highly confident?
the offerwill be approved.Normalization: the offer will be approvedAnnotations: PR+: 10(d) U.S. commanders said 5,500 Iraqi prisoners were taken in the first hours ofthe ground war, though some military officials later said the total mayhave climbed above 8,000.Normalization: the total Iraqi prisoners climbed above 8,000Annotations: PS+: 7, PR+: 3In Example (6a), the company said is a parenthetical modifying the main clause(Ross 1973).
Asher (2000), Rooryck (2001), and Simons (2007) argue that such construc-tions often mark the evidential source for the main-clause information, rather thanembedding it semantically.
In terms of our annotations, this predicts that CT+ or CT?judgments will be common for such constructions, because they become more like two-part meanings: the main-clause and the evidential commentary.To test this hypothesis, we took from the Turkers?
annotations the subset of sen-tences tagged Uu in FactBank where the event is directly embedded under an attitudeverb or introduced by a parenthetical.
We also removed examples where a modal aux-iliary modified the event, because those are a prominent source of non-CT annotationsindependently of attitude predication.
This yielded a total of 78 sentences.
Of these,33 are parenthetical, and 31 (94%) of those are tagged CT+ or CT?.
Of the remaining311Computational Linguistics Volume 38, Number 245 non-parenthetical examples, 42 (93%) of those are tagged CT+ or CT?.
Thus, bothparenthetical and non-parenthetical verbs are about equally likely to lead to a CT tag.4This finding is consistent with the evidential analysis of such parentheticals,but it suggests that standard embedding can function pragmatically as an evidentialas well.
This result is expected under the analysis of Simons (2007) (see also Frazierand Clifton 2005; Clifton and Frazier 2010).
It is also anticipated by Karttunen (1973),who focuses on the question of whether attitude verbs are plugs for presuppositions,that is, whether presuppositions introduced in their complements are interpreted assemantically embedded.
He reviews evidence suggesting that these verbs can be veridi-cal with respect to such content, but he tentatively concludes that these are purelypragmatic effects, writing, ?we do not seem to have any alternative except to classifyall propositional attitude verbs as plugs, although I am still not convinced that this isthe right approach?
(page 190).
The evidence of the present article leads us to agree withKarttunen about this basic lexicalist classification, with the major caveat that the utter-ance meanings involved are considerably more complex.
(For additional discussion ofthis point, see Zaenen, Karttunen, and Crouch [2005] and Manning [2006].
)There are also similarities between the FactBank annotations and our own in thecase of Uu.
As in FactBank, antecedents of conditionals (7), generic sentences (8), andclear cases of uncertainty with respect to the future (9) were tagged Uu by a majority ofTurkers.Example 7(a) If the heavy outflows continue, fund managers will face increasingpressure to sell off some of their junk to pay departing investors in theweeks ahead.Normalization: the heavy outflows will continueAnnotations: Uu: 7, PS+: 2, CT+: 1(b) A unit of DPC Acquisition Partners said it would seek to liquidate thecomputer-printer maker ?as soon as possible,?
even if a merger isn?tconsummated.Normalization: a merger will be consummatedAnnotations: Uu: 8, PS+: 2Example 8When prices are tumbling, they must bewilling to buy shares from sellers when no oneelse will.Normalization: they are willing to buy sharesAnnotations: Uu: 7, PR+: 2, PS+: 14 We do not regard this as evidence that there is no difference between parenthetical and non-parentheticaluses when it comes to veridicality, but rather only that the categorical examples do not reveal one.Indeed, if we consider the full distribution of annotations, then a linear model with Parenthetical andVerb predicting the number of CT tags reveals Parenthetical to be positively correlated with CT+(coefficient estimate = 1.36, p = 0.028).312de Marneffe, Manning, and Potts The Pragmatic Complexity of Veridicality AssessmentExample 9(a) The program also calls for coordination of economic reforms and jointimprovement of social programs in the two countries.Normalization: there will be coordination of economic reforms andjoint improvement of social programs in the two countriesAnnotations: Uu: 7, PR+: 2, PS+: 1(b) But weak car sales raise questions about future demand from theauto sector.Normalization: there will be demand from the auto sectorAnnotations: Uu: 6, PS+: 2, CT+: 1, PR?
: 1Another difference between FactBank and the Turkers is the more nuanced cate-gories for PS and PR events.
In FactBank, markers of possibility or probability, suchas could or likely, uniquely determine the corresponding tag (Saur??
2008, page 233).
Incontrast, the Turkers allow the bias created by these lexical items to be swayed by otherfactors.
For example, the auxiliary could can trigger a possible or an unknown event (10).In FactBank, all such sentences are marked PS+.Example 10(a) They aren?t being allowed to leave and could become hostages.Normalization: they will become hostagesAnnotations: PS+: 10(b) Iraq could start hostilitieswith Israel either through a direct attackor by attacking Jordan.Normalization: there will be hostilitiesAnnotations: Uu: 6, PS+: 3, PR+: 1Similarly, expected and appeared are often markers of PR events.
Whereas it isuniquely so in FactBank, however, our annotations showmuch shifting to PS.
Examples(11) and (12) highlight the contrast: It seems likely that the annotators simply havedifferent overall expectations about the forecasting described in each example, a high-level pragmatic influence that does not attach to any particular lexical item.Example 11(a) Big personal computer makers are developing 486-based machines, whichare expected to reach the market early next year.Normalization: 486-based machines will reach the market earlynext yearAnnotations: PR+: 10313Computational Linguistics Volume 38, Number 2(b) Beneath the tepid news-release jargon lies a powerful threat from thebrewing giant, which last year accounted for about 41% of all U.S. beersales and is expected to see that grow to 42.5% in the current year.Normalization: there will be growth to 42.5% in the current yearAnnotations: PS+: 6, PR+: 3, CT+: 1Example 12(a) Despite the lack of any obvious successors, the Iraqi leader?s internalpower base appeared to be narrowing even before the war began.Normalization: the Iraqi leader?s internal power base was narrowingeven before the war beganAnnotations: PR+: 7, CT+: 1, PS+: 1, PS?
: 1(b) Saddam appeared to accept a border demarcation treaty he had rejected inpeace talks following the August 1988 cease-fire of the eight-year war withIran.Normalization: Saddam accepted a border demarcation treatyAnnotations: PS+: 6, PR+: 2, CT+: 2Another difference is that nouns appearing in a negative context were tagged asCT+ by the Turkers but as CT?
or PR?
in FactBank.Example 13(a) However, its equity in the net income of National Steel declined to$6.3 million from $10.9 million as a result of softer demand and lostorders following prolonged labor talks and a threatened strike.Normalization: there were ordersAnnotations: CT+: 6, PR+: 1, PR?
: 1, PS+: 1, Uu: 1This seems to trace to uncertainty about what the annotation should be whenthe event involves a change of state (from orders existing to not existing).
Saur??
andPustejovsky (2009, page 260) note that noun events were a frequent source of disagree-ment between the two annotators because the annotation guidelines did not address atall how to deal with them.3.2 The Uncertainty of Pragmatic EnrichmentFor the purpose of comparing our annotations with those of FactBank, it is useful to sin-gle out the Turkers?
majority-choice category, as we did here.
We have 10 annotations foreach event, however, which invites exploration of the full distribution of annotations,314de Marneffe, Manning, and Potts The Pragmatic Complexity of Veridicality Assessmentto see if the areas of stability and variation can teach us something about the natureof speakers?
veridicality judgments.
In this section, we undertake such an exploration,arguing that the patterns reveal veridicality judgments to be importantly probabilistic,as one would expect from a truly pragmatic phenomenon.Figure 2 provides a high-level summary of the reaction distributions that our sen-tences received.
The labels on the y-axis characterize types of distribution.
For example,5/5 groups the sentences for which the annotators were evenly split between twocategories (e.g., a sentence for which 5 Turkers assigned PR+ and 5 assigned PS+, ora sentence for which 5 Turkers chose PR+ and 5 chose Uu).
The largest grouping, 10,pools the examples on which all the annotators were in agreement.We can safely assume that some of the variation seen in Figure 2 is due to thenoisiness of the crowd-sourced annotation process.
Some annotators might have beeninattentive or confused, or simply lacked the expertise to make these judgments (Snowet al 2008).
For example, the well-represented 1/9 and 1/1/8 groups probably rep-resent examples for which veridicality assessment is straightforward but one or twoof the annotators did not do a good job.
If all the distributions were this skewed,we might feel secure in treating veridicality as categorical.
There are many examplesfor which it seems implausible to say that the variation is due to noise, however.For example, 5/5 groups include sentences like Examples (14) and (15), for which thejudgments depend heavily on one?s prior assumptions about the entities and conceptsinvolved.Figure 2Reaction distributions by type.315Computational Linguistics Volume 38, Number 2Example 14In a statement, the White House said it would do ?whatever is necessary?
to ensurecompliance with the sanctions.Normalization: there will be compliance with the sanctionsAnnotations: Uu: 5, PR+: 5Example 15Diplomacy appears to be making headway in resolving the United Nations?
standoffwith Iraq.Normalization: diplomacy is resolving the United Nations?
standoff with IraqAnnotations: PR+: 5, PS+: 5The 4/6, 1/4/5, 4/4, and 3/3 groups contain many similarly difficult cases.
Com-bining all of the rows where two categories received at least 3 votes, we get 162 exam-ples, which is 25% of the total data set.
Thus, a non-negligible subset of our sentencesseem to involve examples where readers?
responses are divided, suggesting that thereis no unique correct label for them.Finally, it seems likely that the long tail of very high-entropy distributions at the topof the graph in Figure 2 is owed in large part to the fact that veridicality judgmentsare often not reachable with confidence, because the utterance is inherently under-specified or because additional contextual information is needed in order to be sure.This, too, suggests to us that it would be foolhardy to assign a unique veridicalitylabel to every example.
Of course, situating the sentences in context would reducesome of this uncertainty, but no amount of background information could eliminate itentirely.When we look more closely at these distributions, we find additional evidence forthe idea that veridicality is graded and variable.
One of the most striking patternsconcerns the question of whether the annotators enriched an example at all, in thefollowing sense.
Consider an event that is semantically non-veridical.
This could besimply because it is embedded under a non-factive attitude predicate (say, allege), oran evidential marker (according to sources, it seems).
The semantic strategy for suchcases is to pick Uu.
Depending on the amount and nature of the contextual infor-mation brought to bear on the assessment, however, one might enrich this into oneof the positive or negative categories.
A cautious positive enrichment would be PS+,for example.In light of this, it seems promising to look at the subset of 4/6 and 5/5 examplesin which one of the chosen categories is Uu, to see what the other choices are like.
Onthe enrichment hypothesis, the other choices should be uniformly positive or negative(up to some noise).
Figure 3 summarizes the sentences in our corpus that result in thiskind of split.
The y-axis represents the percentage of non-Uu tags, with the positivevalues (CT+, PR+, PS+) extending upwards and the negative ones extending down-wards.
For sentences 1?5 and 18?47 (74% of the total), all of the non-Uu tags wereuniform in their basic polarity.
What?s more, the distributions within the positive andnegative portions are highly systematic.
In the positive realm, the dominant choice isPS+, the most tentative positive enrichment, followed by PR+, and then CT+.
(In thenegative realm, CT?
is the most represented, but we are unsure whether this supportsany definitive conclusions, given the small number of examples.)
Our generalization316de Marneffe, Manning, and Potts The Pragmatic Complexity of Veridicality AssessmentFigure 3The subset of 4/6 and 5/5 distributions in which one of the dominant categories was Uu.
Thebars represent the distribution of non-Uu tags for each of these sentences.
The top portiondepicts the positive tags, and the bottom portion depicts the negative tags.about these patterns is that enrichment from a semantic Uu baseline is systematic andcommon, though with interesting variation both in whether it occurs and, if it does,how much.The full distributions are also informative when it comes to understanding therange of effects that specific lexical items can have on veridicality assessment.
To illus-trate, we focus on the modal auxiliary verbs can, could, may, might, must, will, would.5 Inkeepingwith lexicalist theories, when they are clausemate to an event, that event is oftentagged with one of the PR and PS tags.
The relationship is a loose one, however; themodal seems to steer people into these weaker categories but does not determine theirfinal judgment.
We illustrate in Example (16) with examples involving may in positivecontexts.Example 16(a) Last Friday?s announcement was the first official word that the project wasin trouble and that the company?s plans for a surge in market share mayhave been overly optimistic.Normalization: the company?s plans have been overly optimisticAnnotations: PS+: 5, PR+: 5(b) In a letter, prosecutors told Mr. Antar?s lawyers that because of the recentSupreme Court rulings, they could expect that any fees collected from Mr.Antar may be seized.Normalization: fees collected from Mr. Antar will be seizedAnnotations: PS+: 4, PR+: 65 Other modals, such as should, ought, and have to, are either not well represented in our data orsimply absent.317Computational Linguistics Volume 38, Number 2(c) The prospectus didn?t include many details about the studio and themepark, although conceptual drawings, released this month, show that itmay feature several ?themed?
areas similar to those found at parks builtby Walt Disney Co.Normalization: the park features several ?themed?
areas similar tothose found at parks built by Walt Disney Co.Annotations: PS+: 4, PR+: 4, CT+:1, Uu:1Figure 4 summarizes the data for the set of modals on which we could focus.
Here,we restrict attention to event descriptions that are clausemate to a modal, effectivelytaking each modal to be annotated with the distribution of annotations for its clause-mate event.
We also look only at the positive tags, because the negative ones were tooinfrequent to provide reliable estimates.Two types of modals have been recognized in the literature, weak and strongmodals(Wierzbicka 1987; S?bo 2001; von Fintel and Iatridou 2008; Finlay 2009).
Each type hasdifferent distribution profiles.
As expected, the weak possibility modals can, could, may,and might correlate strongly with PS.
The other categories are also well-representedfor these modals, however, indicating that the contribution of these markers is heavilyinfluenced by other factors.
The strong (or necessity) modals must, will, and would aremuch more evenly distributed across the categories.The mixed picture for modal auxiliaries seems to be typical of modal markers moregenerally.
We do not have enough data to present a quantitative picture for items likepotentially, apparently, and partly, but the following sentences suggest that they are everybit as nuanced in their contributions to veridicality.Figure 4The contribution of modal auxiliaries to veridicality judgments.318de Marneffe, Manning, and Potts The Pragmatic Complexity of Veridicality AssessmentExample 17(a) Anheuser-Busch Cos. said it plans to aggressively discount its majorbeer brands, setting the stage for a potentially bruising price war as thematuring industry?s growth continues to slow.Normalization: there will be a bruising price warAnnotations: PS+: 5, PR+: 5(b) The portfolio unit of the French bank group Credit Lyonnais told stockmarket regulators that it bought 43,000 shares of Cie. de NavigationMixte, apparently to help fend off an unwelcome takeover bid for thecompany.Normalization: the 43,000 shares of Cie. de Navigation Mixte willfend off an unwelcome takeover bid for the companyAnnotations: PS+: 4, PR+: 4, CT:1, Uu:1(c) Nonetheless, concern about the chip may have been responsiblefor a decline of 87.5 cents in Intel?s stock to $32 a share yesterday inover-the-counter trading, on volume of 3,609,800 shares, and partlyresponsible for a drop in Compaq?s stock in New York Stock Exchangecomposite trading on WednesdayNormalization: concern about the chip is responsible for a drop inCompaq?s stockAnnotations: PS+: 4, PR+: 4, CT+:1, PR?
:1The discussion in this section suggests to us that work on veridicality shouldembrace variation and uncertainty as part of the characterization of veridicality, ratherthan trying to approximate the problem as one of basic categorization.
We now turnto experiments with a system for veridicality assessment that acknowledges the multi-valued nature of veridicality.4.
A System for Veridicality AssessmentIn this section, we describe amaximum entropy classifier (Berger, Della Pietra, andDellaPietra 1996) that we built to automatically assign veridicality.
For classification tasks, thedominant tradition within computational linguistics has been to adjudicate differinghuman judgments and to assign a single class for each item in the training data.
InSection 3.2, however, we reviewed the evidence in our annotations that veridicality isnot necessarily categorical, by virtue of the uncertainty involved in making pragmaticjudgments of this sort.
In order to alignwith our theoretical conception of the problem asprobabilistic, we treat each annotator judgment as a training item.
Thus, each sentenceappears 10 times in our training data.319Computational Linguistics Volume 38, Number 2A maximum entropy model computes the probability of each class c given the datad as follows:p(c|d, ?)
=exp?i ?ifi(c, d)?c?
exp?i ?ifi(c?, d)where, for us, the features fi are indicator functions of a property ?
of the data dand a particular class c: fi(c, d) ?
?
(d) ?
c = ck.
The weights ?i of the features are theparameters of the model chosen to maximize the conditional likelihood of the trainingdata according to the model.
The maximum entropy model thus gives us a distribu-tion over the veridicality classes, which will be our output.
To assess how good theoutput of the model is, we will give the log-likelihood of some data according to themodel.
For comparison, we will also give the log-likelihood for the exact distributionfrom the Turkers (which thus gives an upper-bound) as well as a log-likelihood fora baseline model which uses only the overall distribution of classes in the trainingdata.A maximum entropy classifier is an instance of a generalized linear model with alogit link function.
It is almost exactly equivalent to the standard multi-class (also calledpolytomous or multinomial) logistic regression model from statistics, and readers morefamiliar with this presentation can think of it as such.
In all our experiments, we use theStanford Classifier (Manning and Klein 2003) with a Gaussian prior (also known as L2regularization) set to N (0, 1).64.1 FeaturesThe features were selected through 10-fold cross-validation on the training set.Predicate classes.
Saur??
(2008) defines classes of predicates (nouns and verbs) that projectthe same veridicality value onto the events they introduce.
The classes also definethe grammatical relations that need to hold between the predicate and the event itintroduces, because grammatical contexts matter for veridicality.
Different veridicalityvalues will indeed be assigned to X in He doesn?t know that X and in He doesn?t know if X.The classes have names like ANNOUNCE, CONFIRM, CONJECTURE, and SAY.
Like Saur?
?,we used dependency graphs produced by the Stanford parser (Klein andManning 2003;de Marneffe, MacCartney, and Manning 2006) to follow the path from the target eventto the root of the sentence.
If a predicate in the path was contained in one of the classesand the grammatical relation matched, we added both the lemma of the predicate as afeature and a feature marking the predicate class.6 The maximum entropy formulation differs from the standard multi-class logistic regression model byhaving a parameter value for each class giving logit terms for how a feature?s value affects the outcomeprobability relative to a zero feature, whereas in the standard multi-class logistic regression modelthere are no parameters for one distinguished reference class, and the parameters for other classes sayhow the value of a feature affects the outcome probability differentially from the reference class.
Withoutregularization, the maximum entropy formulation is overparameterized, and the parameters areunidentifiable; in a regularized setting, however, this is no longer a problem and the maximum entropyformulation then has the advantage that all classes are treated symmetrically, with a simpler symmetricform of model regularization.320de Marneffe, Manning, and Potts The Pragmatic Complexity of Veridicality AssessmentWorld knowledge.
For each verb found in the path and contained in the predicate classes,we also added the lemma of its subject, and whether or not the verb was negated.Our rationale for including the subject is that, as we saw in Section 3, readers?
inter-pretations differ for sentences such as The FBI said it received .
.
.
and Bush said he re-ceived .
.
.
, presumably because of world knowledge they bring to bear on the judgment.To approximate such world knowledge, we also obtained subject?verb bigram and sub-ject counts from theNew York Times portion of GigaWord and then included log(subject?verb-counts/subject-counts) as a feature.
The intuition here is that some embeddedclauses carry the main point of the sentence (Frazier and Clifton 2005; Simons 2007;Clifton and Frazier 2010), with the overall frequency of the elements introducing theembedded clause contributing to readers?
veridicality assessments.General features.We used the lemma of the event, the lemma of the root of the sentence,the incoming grammatical relation to the event, and a general class feature.Modality features.
We used Saur??
?s list of modal words as features.
We distinguishedbetween modality markers found as direct governors or children of the event underconsideration, andmodal words found elsewhere in the context of the sentence.
Figure 4provides some indication of how these will relate to our annotations.Negation.
A negation feature captures the presence of linguistic markers of negativecontexts.
Events are considered negated if they have a negation dependency in thegraph or an explicit linguistic marker of negation as dependent (e.g., simple negation(not), downward-monotone quantifiers (no, any), or restricting prepositions).
Events arealso considered negated if embedded in a negative context (e.g., fail, cancel).Conditional.
Antecedents of conditionals and words clearly marking uncertainty arereliable indicators of the Uu category.
We therefore checked for events in an if -clauseor embedded under markers such as call for.Quotation.
Another reliable indicator of the Uu category is quotation.
We generated aquotation feature if the sentence opened and ended with quotation marks, or if the rootsubject was we.In summary, our feature set alows combinations of evidence from various sourcesto determine veridicality.
To be sure, lexical features are important, but they must beallowed to interact with pragmatic ones.
In addition, the model does not presumethat individual lexical items will contribute in only one way to veridicality judgments.Rather, their contributions are affected by the rest of the feature set.4.2 Test DataAs test set, we used 130 sentences from the test items in FactBank.
We took all thesentences with events annotated PR+ and PS+ at the author level (there are very few),and we randomly chose sentences for the other values (CT+, CT?, and Uu, because theFactBank test set does not contain any PR?
and PS?
items).
Three colleagues providedthe normalizations of the sentences following Saur??
?s guidelines, and the data were then321Computational Linguistics Volume 38, Number 2annotated usingMechanical Turk, as described in Section 2.
For 112 of the 130 sentences,at least six Turkers agreed on the same value.5.
ResultsTable 6 gives log-likelihood values of the classifier for the training and test sets, alongwith the upper and lower bounds.
The upper bound is the log-likelihood of the modelthat uses the exact distribution from the Turkers.
The lower bound is the log-likelihoodof a model that uses only the overall rate of each class in our annotations for the trainingdata.Kullback-Leibler (KL) divergence provides a related way to assess the effectivenessof the classifier.
The KL divergence between two distributions is an asymmetric measureof the difference between them.
We use Example (6d) to illustrate.
For that sentence, theclassifier assigns a probability of 0.64 to PS+ and 0.28 to PR+, with very low probabilitiesfor the remaining categories.
It thus closely models the gold distribution (PS+: 7/10,PR+: 3/10).
The KL divergence is correspondingly low: 0.13.
The KL divergence for aclassifier that assigned 0.94 probability to the most frequent category (i.e., CT+) and 0.01to the remaining categories would be much higher: 5.76.The mean KL divergence of our model is 0.95 (SD 1.13) for the training data and0.81 (SD 0.91) for the test data.
The mean KL divergence for the baseline model is 1.58(SD 0.57) for the training data and 1.55 (SD 0.47) for the test data.
To assess whether ourclassifier is a statistically significant improvement over the baseline, we use a pairedtwo-sided t-test over the KL divergence values for the two models.
The t-test requiresthat both vectors of values in the comparison have normal distributions.
This is nottrue of the raw KL values, which have approximately gamma distributions, but it isbasically true of the log of the KL values: For the model?s KL divergences, the normalityassumption is very good, whereas for the baseline model there is some positive skew.Nonetheless, the t-test provides a fair way to contextualize and compare the KL valuesof the two models.
By this test, our model improves significantly over the lower bound(two-sided t = ?11.1983, df = 129, p-value < 2.2e?16).We can also compute precision and recall for the subsets of the data where there isa majority vote, that is, where six out of ten annotators agreed on the same label.
Thisallows us to give results per veridicality tag.
We take as the true veridicality value theone on which the annotators agreed.
The value assigned by the classifier is the one withthe highest probability.
Table 7 reports precision, recall, and F1 scores on the trainingand test sets, along with the number of instances in each category.
None of the items inour test data were tagged with PR?
or PS?
and these categories were very infrequentin the training data, so we leave them out.
The table also gives baseline results: WeTable 6Log-likelihood values for the training and test data.Train Testlower-bound ?10813.97 ?1987.86classifier ?8021.85 ?1324.41upper-bound ?3776.30 ?590.75322de Marneffe, Manning, and Potts The Pragmatic Complexity of Veridicality AssessmentTable 7Precision, recall, and F1 on the subsets of the training data (10-fold cross-validation) and testdata where there is majority vote, as well as F1 for the baseline.Train Test# P R F1 Baseline F1 # P R F1 Baseline F1CT+ 158 74.3 84.2 78.9 32.6 61 86.9 86.9 86.9 31.8CT?
158 89.4 91.1 90.2 34.1 31 96.6 90.3 93.3 29.4PR+ 84 74.4 69.1 71.6 19.8 7 50.0 57.1 53.3 6.9PS+ 66 75.4 69.7 72.4 16.7 7 62.5 71.4 66.7 0.0Uu 27 57.1 44.4 50.0 10.7 6 50.0 50.0 50.0 0.0Macro-avg 74.1 71.7 72.6 22.8 69.2 71.1 70.0 13.6Micro-avg 78.6 78.6 78.6 27.0 83.0 83.0 83.0 22.3used a weighted random guesser, as for the lower-bound given in Table 6.
Our resultssignificantly exceed the baseline (McNemar?s test, p < 0.001).7The classifier weights give insights about the interpretation of lexical markers.
Somemarkers behave as linguistic theories predict.
For example, believe is often a marker ofprobability whereas could and may are more likely to indicate possibility.
But as seen inExamples (10) and (16), world knowledge and other linguistic factors shape the veridi-cality of these items.
The greatest departure from theoretical predictions occurs withthe SAY category, which is logically non-veridical but correlates highly with certainty(CT+) in our corpus.8 Conversely, the class KNOW, which includes know, acknowledge,and learn, is traditionally analyzed as veridical (CT+), but in our data is sometimes amarker of possibility, as we discuss in the Conclusion.
Our model thus shows that toaccount for how readers interpret sentences, the space of veridicality should be cut updifferently than the lexicalist theories propose.6.
Error AnalysisWe focus on two kinds of errors.
First, where there is a majority label (a label six ormore of the annotators agreed on) in the annotations, we can compare that label withthe one assigned the highest probability according to our model.
Second, we can studycases where the the annotation distribution diverges considerably from our model?sdistribution (i.e., cases with a very high KL divergence).For the majority-label cases, errors of polarity are extremely rare; the classifierwrongly assesses the polarity of only four events, shown in Example (18).
Most ofthe errors are thus in the degree of confidence (e.g., CT+ vs. PR+).
The graphs next7 McNemar?s test assesses whether the proportions of right and wrong predictions for two systems aresignificantly different.
We calculated the test exactly via the binomial distribution.
We chose a randomguess baseline rather than a choose-most-frequent-class baseline hoping to illustrate a sensible baseline F1performance for each class, but actually the baseline F1 remains zero for two classes on the test set (thereare few items in each class and the random guesser never guessed correctly).
In sum, the performancedifferences are significant for both the training and test sets compared to either of these baselines.8 This might be due to the nature of our corpus, namely, newswire, where in the vast majority of the cases,reports are considered true.
The situation could be totally different in another genre (such as blogs, forinstance).323Computational Linguistics Volume 38, Number 2to the examples compare the gold annotation from the Turkers (the black bars) withthe distribution proposed by the classifier (the gray bars).
The KL divergence value isincluded to help convey how such values relate to these distributions.Example 18(a) Addressing a NATO flag-lowering ceremony at the Dutch embassy, Orbansaid the occasion indicated the end of the embassy?smission of liaisonbetween Hungary and NATO.Normalization: there is an embassy?smission of liaison between Hungary andNATOAnnotations: CT+:7, CT?
: 3(b) But never before has NATO reached out to its former Eastern-blocenemies.Normalization: NATO has reached out to itsformer Eastern-bloc enemies in the pastAnnotations: CT?
: 9, Uu: 1(c) Horsley was not a defendant in the suit, in which the Portland, Ore., juryruled that such sites constitute threats to abortion providers.Normalization: Horsley was a defendant inthe suitAnnotations: CT?
: 10(d) A total of $650,000, meanwhile, is being offered for information leading tothe arrest of Kopp, who is charged with gunning down Dr. Barnett Slepianlast fall in his home in Buffalo.Normalization: Kopp has been arrestedAnnotations: CT?
: 8, Uu: 2When the system missed CT?
events, it failed to find an explicit negative marker,as in Example (18b), where (due to a parse error) never is treated as a dependent of the324de Marneffe, Manning, and Potts The Pragmatic Complexity of Veridicality Assessmentverb have and not of the reaching out event.
Similarly, the system could not capture in-stances in which the negation was merely implicit, as in Example (18d), where the non-veridicality of the arresting event requires deeper interpretation that our feature-set canmanage.In Example (19), we give examples of CT+ events that are incorrectly tagged PR+,PS+, or Uu by the system because of the presence of a weak modal auxiliary or a verbthat lowers certainty, such as believe.
As we saw in Section 3.2, these markers correlatestrongly with the PS categories.Example 19(a) The NATO summit, she said, would produce an initiative that ?respondsto the grave threat posed by weapons of mass destruction and their meansof delivery.
?Normalization: there will be an initiativeAnnotations: CT+: 7, PR+: 3(b) Kopp, meanwhile, may have approached the border with Mexico, but it isunknown whether he crossed into that country, said Freeh.Normalization: it is unknown whetherKopp crossed into MexicoAnnotations: CT+: 10(c) They believe Kopp was driven to Mexico by a female friend after theshooting, and have a trail of her credit card receipts leading to Mexico, thefederal officials have said.Normalization: there was a shootingAnnotations: CT+: 10In the case of PR+ and PS+ events, all the erroneous values assigned by the systemare CT+.
Some explicit modality markers were not seen in the training data, such aspotential in Example (20a), and thus the classifier assigned them no weight.
In othercases, such as Example (20b), the system did not capture the modality implicit in theconditional.325Computational Linguistics Volume 38, Number 2Example 20(a) Albright also used her speech to articulate a forward-looking vision forNATO, and to defend NATO?s potential involvement in Kosovo.Normalization: NATO will be involved inKosovoAnnotations: PS+: 6, PR+: 2, CT+: 1, Uu: 1(b) ?And we must be resolute in spelling out the consequences ofintransigence,?
she added, referring to the threat of NATO air strikesagainst Milosevic if he does not agree to the deployment.Normalization: there will be NATO airstrikesAnnotations: PS+: 8, PR+: 1, Uu: 1(c) But the decision by District Attorney Frank C. Clark to begin presentingevidence to a state grand jury suggests that he has amassed enoughmaterial to support a criminal indictment for homicide.Normalization: District Attorney Frank C.Clark has amassed material to support acriminal indictment for homicideAnnotations: PR+: 6, CT+: 3, PS+: 1(d) The first round of DNA tests on the hair at the FBI Laboratory hereestablished a high probability it came from the same person as a hairfound in a New Jersey home where James C. Kopp, a 44-year-oldanti-abortion protester, lived last year, the official said.Normalization: the hair came from the samepersonAnnotations: PR+: 10326de Marneffe, Manning, and Potts The Pragmatic Complexity of Veridicality AssessmentThe only Uu events that the system correctly retrieved were antecedents of a con-ditional.
For the other Uu events in Example (21), the system assigned CT+ or PR+.The majority of Uu events proved to be very difficult to detect automatically sincecomplex pragmatic factors are at work, many of them only very indirectly reflected inthe texts.Example 21(a) Kopp?s stepmother, who married Kopp?s father when Kopp was in his30s, said Thursday from her home in Irving, Texas: ?I would like tosee him come forward and clear his name if he?s not guilty, and if he?sguilty, to contact a priest and make his amends with society, face whathe did.
?Normalization: Kopp did somethingAnnotations: Uu: 7, PS+: 2, CT+: 1(b) Indeed, one particularly virulent anti-abortion Web site lists the namesof doctors it says perform abortions, or ?crimes against humanity,?with a code indicating whether they are ?working,?
?wounded?
ora ?fatality.
?Normalization: doctors are workingAnnotations: Uu:7, CT+: 3It is also instructive to look at the examples for which there is a large KL divergencebetween our model?s predicted distribution and the annotation distribution.
Very often,this is simply the result of a divergence between the predicted and actual majoritylabel, as discussed earlier.
Instances like Example (22) aremore interesting in this regard,however: These are cases where there was no majority label, as in Example (22a), orwhere the model guessed the correct majority label but failed to capture other aspectsof the distribution, as in Examples (22b) and (22c).Example 22(a) On Tuesday, the National Abortion and Reproductive Rights ActionLeague plans to hold a news conference to screen a televisionadvertisement made last week, before Slepian died, featuring Emily327Computational Linguistics Volume 38, Number 2Lyons, a nurse who was badly wounded earlier this year in thebombing of an abortion clinic in Alabama.Normalization: there will be a newsconference to screen a televisionadvertisementAnnotations: CT+: 5, PR+: 5(b) Vacco?s campaign manager, Matt Behrmann, said in a statement thatSpitzer had ?sunk to a new and despicable low by attempting tocapitalize on the murder of a physician in order to garner votes.
?Normalization: Spitzer had attempted tocapitalize on the murder of a physician inorder to garner votesAnnotations: CT+: 5, PR+: 1, PS+: 3, Uu: 1(c) Since there is no federal homicide statute as such, the federal officials saidKopp could be charged under the recent Freedom of Access to ClinicEntrances Act, which provides for a sentence of up to life imprisonmentfor someone convicted of physical assaults or threats against abortionproviders.Normalization: Kopp will be charged underthe recent Freedom of Access to ClinicEntrances ActAnnotations: PS+: 8, Uu: 2In Example (22a), the classifier is confused by an ambiguity: it treats hold as akind of negation, which leads the system to assign a 0.78 probability to CT?.
In Ex-ample (22b), there are no features indicating possibility, but a number of SAY-relatedfeatures are present, which leads to a very strong bias for CT+ (0.86) and a corre-sponding failure to model the rest of the distribution properly.
In Example (23c), theclassifier correctly assigns most probability to PS+, but the rest of the probability massis distributed between CT+ and PR+.
This is another manifestation of the problem,noted earlier, that we have very few strong indicators of Uu.
The exception to that isconditional antecedents.
As a result, we do well with cases like Example (23a), wherethe event is in a conditional; the classifier assigns 70% of the probability to Uu and 0.15to PS+.328de Marneffe, Manning, and Potts The Pragmatic Complexity of Veridicality AssessmentExample 23(a) On Monday, Spitzer called for Vacco to revive that unit immediately,vowing that he would do so on his first day in office if elected.Normalization: Spitzer will be electedAnnotations: Uu: 7, PS+: 3Overall the system assigns incorrect veridicality distributions in part because itmisses explicit linguistic markers of veridicality, but also because contextual and prag-matic factors cannot be captured.
This is instructive, though, and serves to furthersupport our central thesis that veridicality judgments are not purely lexical, but ratherinvolve complex pragmatic reasoning.7.
ConclusionOur central goal for this article was to explore veridicality judgments at the level ofutterance meaning.
To do this, we extended FactBank (Saur??
and Pustejovsky 2009)with veridicality annotations that are informed by context and world knowledge (Sec-tion 2).
Although the two sets of annotations are similar in many ways, their differenceshighlight areas in which pragmatic factors play a leading role in shaping readers?judgments (Section 3.1).
In addition, because each one of our sentences was judged by 10annotators, we actually have annotation distributions for our sentences, which allows usto identify areas of uncertainty in veridicality assessment (Section 3.2).
This uncertaintyis so pervasive that the problem itself seems better modeled as one of predicting adistribution over veridicality categories, rather than trying to predict a single label.
Thepredictive model we developed (Section 4) is true to this intuition, because it trains onand predicts distributions.
All the features of the model, even the basic lexical ones,show the influence of interacting pragmatic factors (Section 5).
Although automaticallyassigning veridicality judgments that correspond to readers?
intuitions when pragmaticfactors are allowed to play a role is challenging, our classifier shows that it can be doneeffectively using a relatively simple feature set, and we expect performance to improveas we find ways to model richer contextual features.These findings resonate with the notion of entailment used in the RecognizingTextual Entailment challenges (Dagan, Glickman, and Magnini 2006), where the goalis to determine, for each pair of sentences ?T,H?, whether T (the text) justifies H (thehypothesis).
The original task definition draws on ?common-sense?
understanding oflanguage (Chapman 2005), and focuses on how people interpret utterances naturalisti-cally.
Thus, these entailments are not calculated over just the information contained inthe sentence pairs, as amore classical logical approachwould have it, but rather over thefull utterance meaning.
As a result, they are imbuedwith all the uncertainty of utterancemeanings (Zaenen, Karttunen, and Crouch 2005; Crouch, Karttunen, and Zaenen 2006;Manning 2006).
This is strongly reminiscent of our distinction between semantic andpragmatic veridicality.
For example, as a purely semantic fact, might(S) is non-veridical329Computational Linguistics Volume 38, Number 2with regard to S. Depending on the nature of S, however, the nature of the source, thecontext, and countless other factors, one might nonetheless infer S. This is one of thecentral lessons of our new annotations.In an important sense, we have been conservative in bringing semantics and prag-matics together, because we do not challenge the basic veridicality categorizations thatcome from linguistic and logical work on this topic.
Rather, we just showed that thosesemantic judgments are often enriched pragmatically?for example, from uncertainty toone of the positive or negative categories, or from PS to PR or even CT.
The interactionbetween lexical markers and pragmatic context is also crucial in the case of absolutism:Too many lexical markers conveying certainty might in some cases undermine thespeaker?s credibility (e.g., in a car salesman pitch) and actually incite mistrust inthe hearer/reader.
In such instances, lexical markers only are not good indicators of theveridicality of the event, but the pragmatic context of the utterance needs to be takeninto account to fully appreciate the interpretation people assign to it.
There is, however,evidence suggesting that we should be even more radically pragmatic (Searle 1978;Travis 1996), by dropping the notion that lexical items can be reliably classified onceand for all.
For example, lexical theories generally agree that know is veridical withrespect to its sentential complement, and the vast majority of its uses seem to supportthat claim.
There are exceptions, though, as in Example (24) (see also Beaver 2010;Hazlett 2010):Example 24(a) For the first time in history, the U.S. has gone to war with an Arab andMuslim nation, and we know a peaceful solution was in reach.
(b) Let me tell you something, when it comes to finishing the fight, Rockyand I have a lot in common.
I never quit, I never give up, and I knowthat we?re going tomake it together.?
Hillary Clinton, 1 September 2008.
(c) ?That woman who knew I had dyslexia ?
I never interviewed her.??
George W. Bush (New York Times, 16 September 2000.
Quoted byMiller [2001].
)All of these examples seem to use know to report emphatically held belief, a muchweaker sense than a factive lexical semantics would predict.
Example (24c) is themost striking of the group, because it seems to be pragmatically non-veridical: Thecontinuation is Bush?s evidence that the referent of that woman could not possibly bein a position to determine whether he is dyslexic.
Such examples further emphasizethe importance of a pragmatically informed perspective on veridicality in naturallanguage.One key component of veridicality judgments that we left out in this study is thetext provenance.
Our data did not allow us to examine its impact because we did nothave enough variation in the provenance.
All FactBank sentences are from newspaperand newswire text such as the Wall Street Journal, the Associated Press, and the NewYork Times.
The trustworthiness of the document provenance can affect veridicalityjudgments, however: People might have different reactions reading a sentence in theNew York Times versus in a random blog on theWeb.We plan to examine and incorporatethe role of text provenance in future work.330de Marneffe, Manning, and Potts The Pragmatic Complexity of Veridicality AssessmentAcknowledgmentsWe thank our three anonymous referees fortheir insights and advice.
Thanks also toAnastasia Giannakidou, Jesse Harris, EdHovy, Lauri Karttunen, James Pustejovsky,and Annie Zaenen for valuable commentsat various stages of the project.
A specialthanks to Eric Acton, Alex Djalali, and ScottGrimm for their help with the test data.
Thisresearch was supported in part by ONRgrant no.
N00014-10-1-0109, ARO grantno.
W911NF-07-1-0216, and by a StanfordInterdisciplinary Graduate Fellowship tothe first author.ReferencesAsher, Nicholas.
2000.
Truth conditionaldiscourse semantics for parentheticals.Journal of Semantics, 17(1):31?50.Asher, Nicholas and Alex Lascarides.2003.
Logics of Conversation.
CambridgeUniversity Press, Cambridge.Barwise, Jon.
1981.
Scenes and othersituations.
The Journal of Philosophy,78(7):369?397.Beaver, David.
2010.
Have you noticed thatyour belly button lint colour is related tothe colour of your clothing?
In RainerBa?uerle, Uwe Reyle, and Thomas EdeZimmermann, editors, Presuppositionsand Discourse: Essays Offered to HansKamp.
Elsevier, Philadelphia, PA,pages 65?99.Berger, Adam L., Stephen A. Della Pietra,and Vincent J. Della Pietra.
1996.
Amaximum entropy approach to naturallanguage processing.
ComputationalLinguistics, 22(1):39?71.Chapman, Siobhan.
2005.
Paul Grice:Philosopher and Linguist.
PalgraveMacmillan, Houndmills, Basingstoke,Hampshire.Chapman, Wendy W., Will Bridewell,Paul Hanbury, Gregory F. Cooper, andBruce G. Buchanan.
2001.
A simplealgorithm for identifying negatedfindings and diseases in dischargesummaries.
Journal of BiomedicalInformatics, 34(5):301?310.Clifton, Charles Jr. and Lyn Frazier.
2010.Imperfect ellipsis: Antecedents beyondsyntax?
Syntax, 13(4):279?297.Crouch, Richard, Lauri Karttunen, andAnnie Zaenen.
2006.
Circumscribingis not excluding: A reply to Manning.Ms., Palo Alto Research Center,Palo Alto, CA.Dagan, Ido, Oren Glickman, andBernardo Magnini.
2006.
The PASCALrecognising textual entailment challenge.In J. Quinonero-Candela, I. Dagan,B.
Magnini, and F. d?Alche?-Buc, editors,Machine Learning Challenges, LectureNotes in Computer Science, volume 3944.Springer-Verlag, New York, pages 177?190.de Marneffe, Marie-Catherine, BillMacCartney, and Christopher D. Manning.2006.
Generating typed dependencyparses from phrase structure parses.In Proceedings of the 5th InternationalConference on Language Resources andEvaluation, Genoa, Italy, pages 449?454.Diab, Mona, Lori Levin, Teruko Mitamura,Owen Rambow, VinodkumarPrabhakaran, and Weiwei Guo.
2009.Committed belief annotation and tagging.In Proceedings of the Third LinguisticAnnotation Workshop, Singapore,pages 68?73.Elkin, Peter L., Steven H. Brown, Brent A.Bauer, Casey S. Husser, William Carruth,Larry R. Bergstrom, and Dietlind L.Wahner-Roedler.
2005.
A controlled trialof automated classification of negationfrom clinical notes.
BMC MedicalInformatics and Decision Making, 5(13).Farkas, Richa?rd, Veronika Vincze, Gyo?rgyMo?ra, Ja?nos Csirik, and Gyo?rgy Szarvas.2010.
The CoNLL-2010 shared task:Learning to detect hedges and their scopein natural language text.
In Proceedings ofthe Fourteenth Conference on ComputationalNatural Language Learning: Shared Task,Uppsala, Sweden, pages 1?12.Finlay, Stephen.
2009.
Oughts and ends.Philosophical Studies, 143(3):315?340.Fleiss, Joseph I.
1971.
Measuring nominalscale agreement among many raters.Psychological Bulletin, 76(5):378?382.Frazier, Lyn and Charles Clifton, Jr. 2005.
Thesyntax-discourse divide: Processingellipsis.
Syntax, 8(1):121?174.Giannakidou, Anastasia.
1994.
The semanticlicensing of NPIs and the Modern Greeksubjunctive.
In Language and Cognition 4,Yearbook of the Research Group for Theoreticaland Experimental Linguistics.
University ofGroningen, The Netherlands, pages 55?68.Giannakidou, Anastasia.
1995.
Weak andstrong licensing: Evidence from Greek.In Artemis Alexiadou, Geoffrey Horrocks,andMelita Stavrou, editors, Studies in GreekSyntax.
Kluwer, Dordrecht, pages 113?133.Giannakidou, Anastasia.
1999.
Affectivedependencies.
Linguistics and Philosophy,22(4):367?421.331Computational Linguistics Volume 38, Number 2Giannakidou, Anastasia.
2001.
The meaningof free choice.
Linguistics and Philosophy,24(6):659?735.Hazlett, Allan.
2010.
The myth of factiveverbs.
Philosophy and PhenomenologicalResearch, 80(3):497?522.Hobby, Jonathan L., Brian D. M. Tom,C.
Todd, Philip W. P. Bearcroft, andAdrian K. Dixon.
2000.
Communicationof doubt and certainty in radiologicalreports.
The British Journal of Radiology,73(873):999?1001.Huang, Yang and Henry J. Lowe.
2007.A novel hybrid approach to automatednegation detection in clinical radiologyreports.
Journal of the American MedicalInformatics Association, 14(3):304?311.Karttunen, Lauri.
1973.
Presuppositions andcompound sentences.
Linguistic Inquiry,4(2):169?193.Karttunen, Lauri and Annie Zaenen.2005.
Veridicity.
In Graham Katz, JamesPustejovsky, and Frank Schilder, editors,Annotating, Extracting and Reasoning aboutTime and Events, number 05151 in DagstuhlSeminar Proceedings, Dagstuhl, Germany.Internationales Begegnungs- undForschungszentrum fu?r Informatik(IBFI), Schloss Dagstuhl, Germany.Kim, Jin-Dong, Tomoko Ohta, SampoPyysalo, Yoshinobu Kano, and Jun?ichiTsujii.
2009.
Overview of BioNLP?09shared task on event extraction.
InProceedings of the Workshop on BioNLP:Shared Task, Boulder, Colorado, pages 1?9.Kiparsky, Paul and Carol Kiparsky.
1970.Facts.
In M. Bierwisch and K. E. Heidolph,editors, Progress in Linguistics.
Mouton.The Hague, Paris, pages 143?173.Klein, Dan and Christopher D. Manning.2003.
Accurate unlexicalized parsing.In Proceedings of the 41st Meeting of theAssociation of Computational Linguistics,Sapporo, Japan, pages 423?430.Levinson, Stephen C. 1995.
Three levels ofmeaning: Essays in honor of Sir JohnLyons.
In Frank R. Palmer, editor, Grammarand Meaning.
Cambridge University Press,Cambridge, pages 90?115.Levinson, Stephen C. 2000.
PresumptiveMeanings: The Theory of GeneralizedConversational Implicature.
MIT Press,Cambridge, MA.Manning, Christopher D. 2006.
Local textualinference: it?s hard to circumscribe, butyou know it when you see it?and NLPneeds it.
Ms., Stanford University.Manning, Christopher D. and Dan Klein.2003.
Optimization, maxent models, andconditional estimation without magic.
InTutorial at HLT-NAACL 2003 and ACL 2003.Available at http://nlp.stanford.edu/software/classifier.shtml.Miller, Mark Crispin.
2001.
The BushDyslexicon.
W. W. Norton and Company,New York, NY.Montague, Richard.
1969.
On the nature ofcertain philosophical entities.
The Monist,volume 2, pages 159?194.Morante, Roser and Walter Daelemans.
2009.A metalearning approach to processingthe scope of negation.
In Proceedings of theThirteenth Conference on ComputationalNatural Language Learning, Boulder,Colorado, pages 21?29.Morante, Roser and Caroline Sporleder,editors.
2010.
Proceedings of the Workshopon Negation and Speculation in NaturalLanguage Processing, Uppsala, Sweden.Prabhakaran, Vinodkumar, Owen Rambow,and Mona Diab.
2010.
Automaticcommitted belief tagging.
In Proceedingsof COLING 2010: Poster Volume, Beijing,China, pages 1014?1022.Pustejovsky, James, Marc Verhagen, RoserSaur?
?, Jessica Littman, Robert Gaizauskas,Graham Katz, Inderjeet Mani, RobertKnippen, and Andrea Setzer.
2006.Timebank 1.2.
Linguistic DataConsortium, Philadelphia, PA.Pyysalo, Sampo, Filip Ginter, JuhoHeimonen, Jari Bjo?rne, Jorma Boberg,Jouni Ja?rvinen, and Tapio Salakoski.2007.
BioInfer: A corpus for informationextraction in the biomedical domain.BMC Bioinformatics, 8(50).
doi:10.1186/1471-2105-8-50.Rieh, Soo Young.
2010.
Credibility andcognitive authority of information.In M. Bates and M. N. Maack, editors,Encyclopedia of Library and InformationSciences, 3rd ed., Taylor and FrancisGroup, LLC, New York, pages 1337?1344.Rooryck, Johan.
2001.
Evidentiality, Part I.Glot International, 5(4):3?11.Ross, John Robert.
1973.
Slifting.
In MauriceGross, Morris Halle, and Marcel-PaulSchu?tzenberger, editors, The FormalAnalysis of Natural Languages.
Mouton deGruyter, The Hague, pages 133?169.Rubin, Victoria L. 2007.
Stating withcertainty or stating with doubt: Intercoderreliability results for manual annotationof epistemically modalized statements.In Proceedings of the NAACL-HLT 2007,Rochester, NY, pages 141?144.Rubin, Victoria L., Elizabeth D. Liddy,and Noriko Kando.
2005.
Certainty332de Marneffe, Manning, and Potts The Pragmatic Complexity of Veridicality Assessmentidentification in texts: Categorizationmodel and manual tagging results.In J. G. Shanahan, Y. Qu, and J. Wiebe,editors, Computing Attitude and Affectin Text: Theory and Applications (TheInformation Retrieval Series), Springer-Verlag, New York, pages 61?76.S?bo, Kjell Johan.
2001.
Necessaryconditions in a natural language.
InCaroline Fe?ry and Wolfgang Sternefeld,editors, Audiatur Vox Sapientia.
A Festschriftfor Arnim von Stechow.
Akademie Verlag,Berlin, pages 427?449.Saur?
?, Roser.
2008.
A Factuality Profilerfor Eventualities in Text.
Ph.D. thesis,Computer Science Department,Brandeis University.Saur?
?, Roser and James Pustejovsky.
2009.FactBank: A corpus annotated withevent factuality.
Language Resourcesand Evaluation, 43(3):227?268.Searle, John R. 1978.
Literal meaning.Erkenntnis, 13:207?224.Simons, Mandy.
2007.
Observations onembedding verbs, evidentiality, andpresupposition.
Lingua, 117(6):1034?1056.Snow, Rion, Brendan O?Connor, DanielJurafsky, and Andrew Y. Ng.
2008.
Cheapand fast?but is it good?
Evaluatingnon-expert annotations for naturallanguage tasks.
In Proceedings of the 2008Conference on Empirical Methods in NaturalLanguage Processing, Waikiki, Honolulu,Hawaii, pages 254?263.Szarvas, Gyo?rgy, Veronika Vincze, Richa?rdFarkas, and Ja?nos Csirik.
2008.
TheBioScope corpus: annotation for negation,uncertainty and their scope in biomedicaltexts.
In BioNLP 2008: Current Trends inBiomedical Natural Language Processing,Columbus, OH, pages 38?45.Travis, Charles.
1996.
Meaning?s role intruth.Mind, 105(419):451?466.von Fintel, Kai and Sabine Iatridou.2008.
How to say ought in foreign: Thecomposition of weak necessity modals.In Jacqueline Gue?ron and JacquelineLecarme, editors, Studies in NaturalLanguage and Linguistic Theory, volume 75.Springer, Berlin, pages 115?141.Wierzbicka, Anna.
1987.
The semantics ofmodality.
Folia Linguistica, 21(1):25?43.Zaenen, Annie, Lauri Karttunen, and RichardCrouch.
2005.
Local textual inference: Canit be defined or circumscribed?
In ACLWorkshop on Empirical Modelling of SemanticEquivalence and Entailment, pages 31?36,Ann Arbor, MI.Zwarts, Frans.
1995.
Nonveridical contexts.Linguistic Analysis, 25(3?4):286?312.333
