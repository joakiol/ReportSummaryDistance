Proceedings of the 43rd Annual Meeting of the ACL, pages 197?204,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsAutomatic Measurement of Syntactic Development in Child LanguageKenji Sagae and Alon LavieLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15232{sagae,alavie}@cs.cmu.eduBrian MacWhinneyDepartment of PsychologyCarnegie Mellon UniversityPittsburgh, PA 15232macw@cmu.eduAbstractTo facilitate the use of syntactic infor-mation in the study of child languageacquisition, a coding scheme for Gram-matical Relations (GRs) in transcripts ofparent-child dialogs has been proposed bySagae, MacWhinney and Lavie (2004).We discuss the use of current NLP tech-niques to produce the GRs in this an-notation scheme.
By using a statisti-cal parser (Charniak, 2000) and memory-based learning tools for classification(Daelemans et al, 2004), we obtain highprecision and recall of several GRs.
Wedemonstrate the usefulness of this ap-proach by performing automatic measure-ments of syntactic development with theIndex of Productive Syntax (Scarborough,1990) at similar levels to what child lan-guage researchers compute manually.1 IntroductionAutomatic syntactic analysis of natural language hasbenefited greatly from statistical and corpus-basedapproaches in the past decade.
The availability ofsyntactically annotated data has fueled the develop-ment of high quality statistical parsers, which havehad a large impact in several areas of human lan-guage technologies.
Similarly, in the study of childlanguage, the availability of large amounts of elec-tronically accessible empirical data in the form ofchild language transcripts has been shifting much ofthe research effort towards a corpus-based mental-ity.
However, child language researchers have onlyrecently begun to utilize modern NLP techniquesfor syntactic analysis.
Although it is now commonfor researchers to rely on automatic morphosyntacticanalyses of transcripts to obtain part-of-speech andmorphological analyses, their use of syntactic pars-ing is rare.Sagae, MacWhinney and Lavie (2004) haveproposed a syntactic annotation scheme for theCHILDES database (MacWhinney, 2000), whichcontains hundreds of megabytes of transcript dataand has been used in over 1,500 studies in child lan-guage acquisition and developmental language dis-orders.
This annotation scheme focuses on syntacticstructures of particular importance in the study ofchild language.
In this paper, we describe the useof existing NLP tools to parse child language tran-scripts and produce automatically annotated data inthe format of the scheme of Sagae et al We alsovalidate the usefulness of the annotation scheme andour analysis system by applying them towards thepractical task of measuring syntactic development inchildren according to the Index of Productive Syn-tax, or IPSyn (Scarborough, 1990), which requiressyntactic analysis of text and has traditionally beencomputed manually.
Results obtained with currentNLP technology are close to what is expected of hu-man performance in IPSyn computations, but thereis still room for improvement.2 The Index of Productive Syntax (IPSyn)The Index of Productive Syntax (Scarborough,1990) is a measure of development of child lan-guage that provides a numerical score for grammat-ical complexity.
IPSyn was designed for investigat-ing individual differences in child language acqui-197sition, and has been used in numerous studies.
Itaddresses weaknesses in the widely popular MeanLength of Utterance measure, or MLU, with respectto the assessment of development of syntax in chil-dren.
Because it addresses syntactic structures di-rectly, it has gained popularity in the study of gram-matical aspects of child language learning in bothresearch and clinical settings.After about age 3 (Klee and Fitzgerald, 1985),MLU starts to reach ceiling and fails to properly dis-tinguish between children at different levels of syn-tactic ability.
For these purposes, and because of itshigher content validity, IPSyn scores often tells usmore than MLU scores.
However, the MLU holdsthe advantage of being far easier to compute.
Rel-atively accurate automated methods for computingthe MLU for child language transcripts have beenavailable for several years (MacWhinney, 2000).Calculation of IPSyn scores requires a corpus of100 transcribed child utterances, and the identifica-tion of 56 specific language structures in each ut-terance.
These structures are counted and used tocompute numeric scores for the corpus in four cat-egories (noun phrases, verb phrases, questions andnegations, and sentence structures), according to afixed score sheet.
Each structure in the four cate-gories receives a score of zero (if the structure wasnot found in the corpus), one (if it was found oncein the corpus), or two (if it was found two or moretimes).
The scores in each category are added, andthe four category scores are added into a final IPSynscore, ranging from zero to 112.1Some of the language structures required in thecomputation of IPSyn scores (such as the presenceof auxiliaries or modals) can be recognized with theuse of existing child language analysis tools, suchas the morphological analyzer MOR (MacWhinney,2000) and the part-of-speech tagger POST (Parisseand Le Normand, 2000).
However, more complexstructures in IPSyn require syntactic analysis thatgoes beyond what POS taggers can provide.
Exam-ples of such structures include the presence of aninverted copula or auxiliary in a wh-question, con-joined clauses, bitransitive predicates, and frontedor center-embedded subordinate clauses.1See (Scarborough, 1990) for a complete listing of targetedstructures and the IPSyn score sheet used for calculation ofscores.Sentence (input):We eat the cheese sandwichGrammatical Relations (output):[Leftwall]     We     eat     the     cheese     sandwichSUBJROOT OBJDETMODFigure 1: Input sentence and output produced by oursystem.3 Automatic Syntactic Analysis of ChildLanguage TranscriptsA necessary step in the automatic computation ofIPSyn scores is to produce an automatic syntac-tic analysis of the transcripts being scored.
Wehave developed a system that parses transcribedchild utterances and identifies grammatical relations(GRs) according to the CHILDES syntactic annota-tion scheme (Sagae et al, 2004).
This annotationscheme was designed specifically for child-parentdialogs, and we have found it suitable for the iden-tification of the syntactic structures necessary in thecomputation of IPSyn.Our syntactic analysis system takes a sentenceand produces a labeled dependency structure repre-senting its grammatical relations.
An example of theinput and output associated with our system can beseen in figure 1.
The specific GRs identified by thesystem are listed in figure 2.The three main steps in our GR analysis are: textpreprocessing, unlabeled dependency identification,and dependency labeling.
In the following subsec-tions, we examine each of them in more detail.3.1 Text PreprocessingThe CHAT transcription system2 is the formatfollowed by all transcript data in the CHILDESdatabase, and it is the input format we use for syn-tactic analysis.
CHAT specifies ways of transcrib-ing extra-grammatical material such as disfluency,retracing, and repetition, common in spontaneousspoken language.
Transcripts of child language maycontain a large amount of extra-grammatical mate-2http://childes.psy.cmu.edu/manuals/CHAT.pdf198SUBJ, ESUBJ, CSUBJ, XSUBJCOMP, XCOMPJCT, CJCT, XJCTOBJ, OBJ2, IOBJPRED, CPRED, XPREDMOD, CMOD, XMODAUX NEG DET QUANT POBJ PTLCPZR COM INF VOC COORD ROOTSubject, expletive subject, clausal subject (finite and non?finite) Object, second object, indirect objectClausal complement (finite and non?finite) Predicative, clausal predicative (finite and non?finite)Adjunct, clausal adjunct (finite and non?finite) Nominal modifier, clausal nominal modifier (finite and non?finite)Auxiliary Negation Determiner Quantifier Prepositional object Verb particleCommunicatorComplementizer Infinitival "to" Vocative Coordinated item Top nodeFigure 2: Grammatical relations in the CHILDES syntactic annotation scheme.rial that falls outside of the scope of the syntactic an-notation system and our GR identifier, since it is al-ready clearly marked in CHAT transcripts.
By usingthe CLAN tools (MacWhinney, 2000), designed toprocess transcripts in CHAT format, we remove dis-fluencies, retracings and repetitions from each sen-tence.
Furthermore, we run each sentence throughthe MOR morphological analyzer (MacWhinney,2000) and the POST part-of-speech tagger (Parisseand Le Normand, 2000).
This results in fairly cleansentences, accompanied by full morphological andpart-of-speech analyses.3.2 Unlabeled Dependency IdentificationOnce we have isolated the text that should be ana-lyzed in each sentence, we parse it to obtain unla-beled dependencies.
Although we ultimately needlabeled dependencies, our choice to produce unla-beled structures first (and label them in a later step)is motivated by available resources.
Unlabeled de-pendencies can be readily obtained by processingconstituent trees, such as those in the Penn Tree-bank (Marcus et al, 1993), with a set of rules todetermine the lexical heads of constituents.
Thislexicalization procedure is commonly used in sta-tistical parsing (Collins, 1996) and produces a de-pendency tree.
This dependency extraction proce-dure from constituent trees gives us a straightfor-ward way to obtain unlabeled dependencies: use anexisting statistical parser (Charniak, 2000) trainedon the Penn Treebank to produce constituent trees,and extract unlabeled dependencies using the afore-mentioned head-finding rules.Our target data (transcribed child language) isfrom a very different domain than the one of the dataused to train the statistical parser (the Wall StreetJournal section of the Penn Treebank), but the degra-dation in the parser?s accuracy is acceptable.
Anevaluation using 2,018 words of in-domain manu-ally annotated dependencies shows that the depen-dency accuracy of the parser is 90.1% on child lan-guage transcripts (compared to over 92% on section23 of the Wall Street Journal portion of the PennTreebank).
Despite the many differences with re-spect to the domain of the training data, our domainfeatures sentences that are much shorter (and there-fore easier to parse) than those found in Wall StreetJournal articles.
The average sentence length variesfrom transcript to transcript, because of factors suchas the age and verbal ability of the child, but it isusually less than 15 words.3.3 Dependency LabelingAfter obtaining unlabeled dependencies as describedabove, we proceed to label those dependencies withthe GR labels listed in Figure 2.Determining the labels of dependencies is in gen-eral an easier task than finding unlabeled dependen-cies in text.3 Using a classifier, we can choose oneof the 30 possible GR labels for each dependency,given a set of features derived from the dependen-cies.
Although we need manually labeled data totrain the classifier for labeling dependencies, the sizeof this training set is far smaller than what would benecessary to train a parser to find labeled dependen-3Klein and Manning (2002) offer an informal argument thatconstituent labels are much more easily separable in multidi-mensional space than constituents/distituents.
The same argu-ment applies to dependencies and their labels.199cies in one pass.We use a corpus of about 5,000 words with man-ually labeled dependencies to train TiMBL (Daele-mans et al, 2003), a memory-based learner (set touse the k-nn algorithm with k=1, and gain ratioweighing), to classify each dependency with a GRlabel.
We extract the following features for each de-pendency:?
The head and dependent words;?
The head and dependent parts-of-speech;?
Whether the dependent comes before or afterthe head in the sentence;?
How many words apart the dependent is fromthe head;?
The label of the lowest node in the constituenttree that includes both the head and dependent.The accuracy of the classifier in labeling depen-dencies is 91.4% on the same 2,018 words used toevaluate unlabeled accuracy.
There is no intersec-tion between the 5,000 words used for training andthe 2,018-word test set.
Features were tuned on aseparate development set of 582 words.When we combine the unlabeled dependenciesobtained with the Charniak parser (and head-findingrules) and the labels obtained with the classifier,overall labeled dependency accuracy is 86.9%, sig-nificantly above the results reported (80%) by Sagaeet al (2004) on very similar data.Certain frequent and easily identifiable GRs, suchas DET, POBJ, INF, and NEG were identified withprecision and recall above 98%.
Among the mostdifficult GRs to identify were clausal complementsCOMP and XCOMP, which together amount to lessthan 4% of the GRs seen the training and test sets.Table 1 shows the precision and recall of GRs of par-ticular interest.Although not directly comparable, our resultsare in agreement with state-of-the-art results forother labeled dependency and GR parsers.
Nivre(2004) reports a labeled (GR) dependency accuracyof 84.4% on modified Penn Treebank data.
Briscoeand Carroll (2002) achieve a 76.5% F-score on avery rich set of GRs in the more heterogeneous andchallenging Susanne corpus.
Lin (1998) evaluateshis MINIPAR system at 83% F-score on identifica-tion of GRs, also in data from the Susanne corpus(but using simpler GR set than Briscoe and Carroll).GR Precision Recall F-scoreSUBJ 0.94 0.93 0.93OBJ 0.83 0.91 0.87COORD 0.68 0.85 0.75JCT 0.91 0.82 0.86MOD 0.79 0.92 0.85PRED 0.80 0.83 0.81ROOT 0.91 0.92 0.91COMP 0.60 0.50 0.54XCOMP 0.58 0.64 0.61Table 1: Precision, recall and F-score (harmonicmean) of selected Grammatical Relations.4 Automating IPSynCalculating IPSyn scores manually is a laboriousprocess that involves identifying 56 syntactic struc-tures (or their absence) in a transcript of 100 childutterances.
Currently, researchers work with a par-tially automated process by using transcripts in elec-tronic format and spreadsheets.
However, the ac-tual identification of syntactic structures, which ac-counts for most of the time spent on calculating IP-Syn scores, still has to be done manually.By using part-of-speech and morphological anal-ysis tools, it is possible to narrow down the num-ber of sentences where certain structures may befound.
The search for such sentences involves pat-terns of words and parts-of-speech (POS).
Somestructures, such as the presence of determiner-nounor determiner-adjective-noun sequences, can be eas-ily identified through the use of simple patterns.Other structures, such as front or center-embeddedclauses, pose a greater challenge.
Not only are pat-terns for such structures difficult to craft, they arealso usually inaccurate.
Patterns that are too gen-eral result in too many sentences to be manually ex-amined, but more restrictive patterns may miss sen-tences where the structures are present, making theiridentification highly unlikely.
Without more syntac-tic analysis, automatic searching for structures in IP-Syn is limited, and computation of IPSyn scores stillrequires a great deal of manual inspection.Long, Fey and Channell (2004) have developeda software package, Computerized Profiling (CP),for child language study, which includes a (mostly)200automated computation of IPSyn.4 CP is an exten-sively developed example of what can be achievedusing only POS and morphological analysis.
It doeswell on identifying items in IPSyn categories thatdo not require deeper syntactic analysis.
However,the accuracy of overall scores is not high enough tobe considered reliable in practical usage, in particu-lar for older children, whose utterances are longerand more sophisticated syntactically.
In practice,researchers usually employ CP as a first pass, andmanually correct the automatic output.
Section 5presents an evaluation of the CP version of IPSyn.Syntactic analysis of transcripts as described insection 3 allows us to go a step further, fully au-tomating IPSyn computations and obtaining a levelof reliability comparable to that of human scoring.The ability to search for both grammatical relationsand parts-of-speech makes searching both easier andmore reliable.
As an example, consider the follow-ing sentences (keeping in mind that there are no ex-plicit commas in spoken language):(a) Then [,] he said he ate.
(b) Before [,] he said he ate.
(c) Before he ate [,] he ran.Sentences (a) and (b) are similar, but (c) is dif-ferent.
If we were looking for a fronted subordinateclause, only (c) would be a match.
However, eachone of the sentences has an identical part-speech-sequence.
If this were an isolated situation, wemight attempt to fix it by having tags that explic-itly mark verbs that take clausal complements, or byadding lexical constraints to a search over part-of-speech patterns.
However, even by modifying thissimple example slightly, we find more problems:(d) Before [,] he told the man he was cold.
(e) Before he told the story [,] he was cold.Once again, sentences (d) and (e) have identicalpart-of-speech sequences, but only sentence (e) fea-tures a fronted subordinate clause.
These limited toyexamples only scratch the surface of the difficultiesin identifying syntactic structures without syntactic4Although CP requires that a few decisions be made man-ually, such as the disambiguation of the lexical item ??s?
ascopula vs. genitive case marker, and the definition of sentencebreaks for long utterances, the computation of IPSyn scores isautomated to a large extent.analysis beyond part-of-speech and morphologicaltagging.
In these sentences, searching with GRsis easy: we simply find a GR of clausal type (e.g.CJCT, COMP, CMOD, etc) where the dependent isto the left of its head.For illustration purposes of how searching forstructures in IPSyn is done with GRs, let us lookat how to find other IPSyn structures5:?
Wh-embedded clauses: search for wh-wordswhose head, or transitive head (its head?s head,or head?s head?s head...) is a dependent inGR of types [XC]SUBJ, [XC]PRED, [XC]JCT,[XC]MOD, COMP or XCOMP;?
Relative clauses: search for a CMOD where thedependent is to the right of the head;?
Bitransitive predicate: search for a word that isa head of both OBJ and OBJ2 relations.Although there is still room for under- and over-generalization with search patterns involving GRs,finding appropriate ways to search is often madetrivial, or at least much more simple and reliablethan searching without GRs.
An evaluation of ourautomated version of IPSyn, which searches for IP-Syn structures using POS, morphology and GR in-formation, and a comparison to the CP implemen-tation, which uses only POS and morphology infor-mation, is presented in section 5.5 EvaluationWe evaluate our implementation of IPSyn in twoways.
The first is Point Difference, which is cal-culated by taking the (unsigned) difference betweenscores obtained manually and automatically.
Thepoint difference is of great practical value, sinceit shows exactly how close automatically producedscores are to manually produced scores.
The secondis Point-to-Point Accuracy, which reflects the overallreliability over each individual scoring decision inthe computation of IPSyn scores.
It is calculated bycounting how many decisions (identification of pres-ence/absence of language structures in the transcriptbeing scored) were made correctly, and dividing that5More detailed descriptions and examples of each structureare found in (Scarborough, 1990), and are omitted here forspace considerations, since the short descriptions are fairly self-explanatory.201number by the total number of decisions.
The point-to-point measure is commonly used for assessing theinter-rater reliability of metrics such as the IPSyn.
Inour case, it allows us to establish the reliability of au-tomatically computed scores against human scoring.5.1 Test DataWe obtained two sets of transcripts with correspond-ing IPSyn scoring (total scores, and each individualdecision) from two different child language researchgroups.
The first set (A) contains 20 transcripts ofchildren of ages ranging between two and three.
Thesecond set (B) contains 25 transcripts of children ofages ranging between eight and nine.Each transcript in set A was scored fully manu-ally.
Researchers looked for each language structurein the IPSyn scoring guide, and recorded its pres-ence in a spreadsheet.
In set B, scoring was donein a two-stage process.
In the first stage, each tran-script was scored automatically by CP.
In the secondstage, researchers checked each automatic decisionmade by CP, and corrected any errors manually.Two transcripts in each set were held out for de-velopment and debugging.
The final test sets con-tained: (A) 18 transcripts with a total of 11,704words and a mean length of utterance of 2.9, and(B) 23 transcripts with a total of 40,819 words and amean length of utterance of 7.0.5.2 ResultsScores computed automatically from transcriptsparsed as described in section 3 were very closeto the scores computed manually.
Table 2 shows asummary of the results, according to our two eval-uation metrics.
Our system is labeled as GR, andmanually computed scores are labeled as HUMAN.For comparison purposes, we also show the resultsof running Long et al?s automated version of IPSyn,labeled as CP, on the same transcripts.Point DifferenceThe average (absolute) point difference between au-tomatically computed scores (GR) and manuallycomputed scores (HUMAN) was 3.3 (the range ofHUMAN scores on the data was 21-91).
There wasno clear trend on whether the difference was posi-tive or negative.
In some cases, the automated scoreswere higher, in other cases lower.
The minimum dif-System Avg.
Pt.
Difference Point-to-Pointto HUMAN ReliabilityGR (Total) 3.3 92.8%CP (Total) 8.3 85.4%GR (Set A) 3.7 92.5%CP (Set A) 6.2 86.2%GR (Set B) 2.9 93.0%CP (Set B) 10.2 84.8%Table 2: Summary of evaluation results.
GR is ourimplementation of IPSyn based on grammatical re-lations, CP is Long et al?s (2004) implementation ofIPSyn, and HUMAN is manual scoring.Histogram of Point Differences (3 point bins)01020304050603 6 9 12 15 18 21Point DifferenceFrequency (%) GRCPFigure 3: Histogram of point differences betweenHUMAN scores and GR (black), and CP (white).ference was zero, and the maximum difference was12.
Only two scores differed by 10 or more, and 17scores differed by two or less.
The average point dif-ference between HUMAN and the scores obtainedwith Long et al?s CP was 8.3.
The minimum waszero and the maximum was 21.
Sixteen scores dif-fered by 10 or more, and six scores differed by 2 orless.
Figure 3 shows the point differences betweenGR and HUMAN, and CP and HUMAN.It is interesting to note that the average point dif-ferences between GR and HUMAN were similar onsets A and B (3.7 and 2.9, respectively).
Despite thedifference in age ranges, the two averages were lessthan one point apart.
On the other hand, the averagedifference between CP and HUMAN was 6.2 on setA, and 10.2 on set B.
The larger difference reflectsCP?s difficulty in scoring transcripts of older chil-dren, whose sentences are more syntactically com-plex, using only POS analysis.202Point-to-Point AccuracyIn the original IPSyn reliability study (Scarborough,1990), point-to-point measurements using 75 tran-scripts showed the mean inter-rater agreement forIPSyn among human scorers at 94%, with a min-imum agreement of 90% of all decisions within atranscript.
The lowest agreement between HUMANand GR scoring for decisions within a transcript was88.5%, with a mean of 92.8% over the 41 transcriptsused in our evaluation.
Although comparisons ofagreement figures obtained with different sets oftranscripts are somewhat coarse-grained, given thevariations within children, human scorers and tran-script quality, our results are very satisfactory.
Fordirect comparison purposes using the same data, themean point-to-point accuracy of CP was 85.4% (arelative increase of about 100% in error).In their separate evaluation of CP, using 30 sam-ples of typically developing children, Long andChannell (2001) found a 90.7% point-to-point ac-curacy between fully automatic and manually cor-rected IPSyn scores.6 However, Long and Channellcompared only CP output with manually correctedCP output, while our set A was manually scoredfrom scratch.
Furthermore, our set B containedonly transcripts from significantly older children (asin our evaluation, Long and Channell observed de-creased accuracy of CP?s IPSyn with more com-plex language usage).
These differences, and theexpected variation from using different transcriptsfrom different sources, account for the difference inour results and Long and Channell?s.5.3 Error AnalysisAlthough the overall accuracy of our automaticallycomputed scores is in large part comparable to man-ual IPSyn scoring (and significantly better than theonly option currently available for automatic scor-ing), our system suffers from visible deficiencies inthe identification of certain structures within IPSyn.Four of the 56 structures in IPSyn account for al-most half of the number of errors made by our sys-tem.
Table 3 lists these IPSyn items, with their re-spective percentages of the total number of errors.6Long and Channell?s evaluation also included samplesfrom children with language disorders.
Their 30 samples oftypically developing children (with a mean age of 5) are moredirectly comparable to the data used in our evaluation.IPSyn item ErrorS11 (propositional complement) 16.9%V15 (copula, modal or aux for 12.3%emphasis or ellipsis)S16 (relative clause) 10.6%S14 (bitransitive predicate) 5.8%Table 3: IPSyn structures where errors occur mostfrequently, and their percentages of the total numberof errors over 41 transcripts.Errors in items S11 (propositional complements),S16 (relative clauses), and S14 (bitransitive predi-cates) are caused by erroneous syntactic analyses.For an example of how GR assignments affect IP-Syn scoring, let us consider item S11.
Searching forthe relation COMP is a crucial part in finding propo-sitional complements.
However, COMP is one ofthe GRs that can be identified the least reliably inour set (precision of 0.6 and recall of 0.5, see table1).
As described in section 2, IPSyn requires thatwe credit zero points to item S11 for no occurrencesof propositional complements, one point for a singleoccurrence, and two points for two or more occur-rences.
If there are several COMPs in the transcript,we should find about half of them (plus others, inerror), and correctly arrive at a credit of two points.However, if there are very few or none, our count islikely to be incorrect.Most errors in item V15 (emphasis or ellipsis)were caused not by incorrect GR assignments, butby imperfect search patterns.
The searching failed toaccount for a number of configurations of GRs, POStags and words that indicate that emphasis or ellip-sis exists.
This reveals another general source of er-ror in our IPSyn implementation: the search patternsthat use GR analyzed text to make the actual IP-Syn scoring decisions.
Although our patterns are farmore reliable than what we could expect from POStags and words alone, these are still hand-craftedrules that need to be debugged and perfected overtime.
This was the first evaluation of our system,and only a handful of transcripts were used duringdevelopment.
We expect that once child languageresearchers have had the opportunity to use the sys-tem in practical settings, their feedback will allow usto refine the search patterns at a more rapid pace.2036 Conclusion and Future WorkWe have presented an automatic way to annotatetranscripts of child language with the CHILDESsyntactic annotation scheme.
By using existing re-sources and a small amount of annotated data, weachieved state-of-the-art accuracy levels.GR identification was then used to automate thecomputation of IPSyn scores to measure grammati-cal development in children.
The reliability of ourautomatic IPSyn was very close to the inter-rater re-liability among human scorers, and far higher thanthat of the only other computational implementationof IPSyn.
This demonstrates the value of automaticGR assignment to child language research.From the analysis in section 5.3, it is clear that theidentification of certain GRs needs to be made moreaccurately.
We intend to annotate more in-domaintraining data for GR labeling, and we are currentlyinvestigating the use of other applicable GR parsingtechniques.Finally, IPSyn score calculation could be mademore accurate with the knowledge of the expectedlevels of precision and recall of automatic assign-ment of specific GRs.
It is our intuition that in anumber of cases it would be preferable to trade re-call for precision.
We are currently working on aframework for soft-labeling of GRs, which will al-low us to manipulate the precision/recall trade-offas discussed in (Carroll and Briscoe, 2002).AcknowledgmentsThis work was supported in part by the National Sci-ence Foundation under grant IIS-0414630.ReferencesEdward J. Briscoe and John A. Carroll.
2002.
Robust ac-curate statistical annotation of general text.
Proceed-ings of the 3rd International Conference on LanguageResources and Evaluation, (pp.
1499?1504).
Las Pal-mas, Gran Canaria.John A. Carroll and Edward J. Briscoe.
2002.
High pre-cision extraction of grammatical relations.
Proceed-ings of the 19th International Conference on Compu-tational Linguistics, (pp.
134-140).
Taipei, Taiwan.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proceedings of the First Annual Meetingof the North American Chapter of the Association forComputational Linguistics.
Seattle, WA.Michael Collins.
1996.
A new statistical parser based onbigram lexical dependencies.
Proceedings of the 34thMeeting of the Association for Computational Linguis-tics (pp.
184-191).
Santa Cruz, CA.Walter Daelemans, Jacub Zavrel, Ko van der Sloot, andAntal van den Bosch.
2004.
TiMBL: Tilburg MemoryBased Learner, version 5.1, Reference Guide.
ILK Re-search Group Technical Report Series no.
04-02, 2004.T.
Klee and M. D. Fitzgerald.
1985.
The relation be-tween grammatical development and mean length ofutterance in morphemes.
Journal of Child Language,12, 251-269.Dan Klein and Christopher D. Manning.
2002.
A genera-tive constituent-context model for improved grammarinduction.
Proceedings of the 40th Annual Meetingof the Association for Computational Linguistics (pp.128-135).Dekang Lin.
1998.
Dependency-based evaluation ofMINIPAR.
In Proceedings of the Workshop on theEvaluation of Parsing Systems.
Granada, Spain.Steve H. Long and Ron W. Channell.
2001.
Accuracy offour language analysis procedures performed automat-ically.
American Journal of Speech-Language Pathol-ogy, 10(2).Steven H. Long, Marc E. Fey, and Ron W. Channell.2004.
Computerized Profiling (Version 9.6.0).
Cleve-land, OH: Case Western Reserve University.Brian MacWhinney.
2000.
The CHILDES Project: Toolsfor Analyzing Talk.
Mahwah, NJ: Lawrence ErlbaumAssociates.Mitchel P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewics.
1993.
Building a large annotated cor-pus of English: The Penn Treebank.
ComputationalLinguistics, 19.Joakim Nivre and Mario Scholz.
2004.
Deterministic de-pendency parsing of English text.
Proceedings of In-ternational Conference on Computational Linguistics(pp.
64-70).
Geneva, Switzerland.Christophe Parisse and Marie-Thrse Le Normand.
2000.Automatic disambiguation of the morphosyntax inspoken language corpora.
Behavior Research Meth-ods, Instruments, and Computers, 32, 468-481.Kenji Sagae, Alon Lavie, and Brian MacWhinney.
2004.Adding Syntactic annotations to transcripts of parent-child dialogs.
Proceedings of the Fourth InternationalConference on Language Resources and Evaluation(LREC 2004).
Lisbon, Portugal.Hollis S. Scarborough.
1990.
Index of Productive Syn-tax.
In Applied Psycholinguistics, 11, 1-22.204
