Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 528?535,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsCoreference Resolution Using Semantic Relatedness Information fromAutomatically Discovered PatternsXiaofeng Yang Jian SuInstitute for Infocomm Research21 Heng Mui Keng Terrace, Singapore, 119613{xiaofengy,sujian}@i2r.a-star.edu.sgAbstractSemantic relatedness is a very important fac-tor for the coreference resolution task.
Toobtain this semantic information, corpus-based approaches commonly leverage pat-terns that can express a specific semanticrelation.
The patterns, however, are de-signed manually and thus are not necessar-ily the most effective ones in terms of ac-curacy and breadth.
To deal with this prob-lem, in this paper we propose an approachthat can automatically find the effective pat-terns for coreference resolution.
We explorehow to automatically discover and evaluatepatterns, and how to exploit the patterns toobtain the semantic relatedness information.The evaluation on ACE data set shows thatthe pattern based semantic information ishelpful for coreference resolution.1 IntroductionSemantic relatedness is a very important factor forcoreference resolution, as noun phrases used to re-fer to the same entity should have a certain semanticrelation.
To obtain this semantic information, previ-ous work on reference resolution usually leveragesa semantic lexicon like WordNet (Vieira and Poe-sio, 2000; Harabagiu et al, 2001; Soon et al, 2001;Ng and Cardie, 2002).
However, the drawback ofWordNet is that many expressions (especially forproper names), word senses and semantic relationsare not available from the database (Vieira and Poe-sio, 2000).
In recent years, increasing interest hasbeen seen in mining semantic relations from largetext corpora.
One common solution is to utilize apattern that can represent a specific semantic rela-tion (e.g., ?X such as Y?
for is-a relation, and ?Xand other Y?
for other-relation).
Instantiated withtwo given noun phrases, the pattern is searched in alarge corpus and the occurrence number is used asa measure of their semantic relatedness (Markert etal., 2003; Modjeska et al, 2003; Poesio et al, 2004).However, in the previous pattern based ap-proaches, the selection of the patterns to represent aspecific semantic relation is done in an ad hoc way,usually by linguistic intuition.
The manually se-lected patterns, nevertheless, are not necessarily themost effective ones for coreference resolution fromthe following two concerns:?
Accuracy.
Can the patterns (e.g., ?X such asY?)
find as many NP pairs of the specific se-mantic relation (e.g.
is-a) as possible, with ahigh precision??
Breadth.
Can the patterns cover a wide varietyof semantic relations, not just is-a, by whichcoreference relationship is realized?
For ex-ample, in some annotation schemes like ACE,?Beijing:China?
are coreferential as the capitaland the country could be used to represent thegovernment.
The pattern for the common ?is-a?
relation will fail to identify the NP pairs ofsuch a ?capital-country?
relation.To deal with this problem, in this paper we pro-pose an approach which can automatically discovereffective patterns to represent the semantic relations528for coreference resolution.
We explore two issues inour study:(1) How to automatically acquire and evaluatethe patterns?
We utilize a set of coreferential NPpairs as seeds.
For each seed pair, we search a largecorpus for the texts where the two noun phrases co-occur, and collect the surrounding words as the sur-face patterns.
We evaluate a pattern based on itscommonality or association with the positive seedpairs.
(2) How to mine the patterns to obtain the seman-tic relatedness information for coreference resolu-tion?
We present two strategies to exploit the pat-terns: choosing the top best patterns as a set of pat-tern features, or computing the reliability of seman-tic relatedness as a single feature.
In either strategy,the obtained features are applied to do coreferenceresolution in a supervised-learning way.To our knowledge, our work is the first effort thatsystematically explores these issues in the corefer-ence resolution task.
We evaluate our approach onACE data set.
The experimental results show thatthe pattern based semantic relatedness informationis helpful for the coreference resolution.The remainder of the paper is organized as fol-lows.
Section 2 gives some related work.
Section 3introduces the framework for coreference resolution.Section 4 presents the model to obtain the pattern-based semantic relatedness information.
Section 5discusses the experimental results.
Finally, Section6 summarizes the conclusions.2 Related WorkEarlier work on coreference resolution commonlyrelies on semantic lexicons for semantic relatednessknowledge.
In the system by Vieira and Poesio(2000), for example, WordNet is consulted to obtainthe synonymy, hypernymy and meronymy relationsfor resolving the definite anaphora.
In (Harabagiuet al, 2001), the path patterns in WordNet are uti-lized to compute the semantic consistency betweenNPs.
Recently, Ponzetto and Strube (2006) suggestto mine semantic relatedness from Wikipedia, whichcan deal with the data sparseness problem sufferedby using WordNet.Instead of leveraging existing lexicons, manyresearchers have investigated corpus-based ap-proaches to mine semantic relations.
Garera andYarowsky (2006) propose an unsupervised modelwhich extracts hypernym relation for resloving def-inite NPs.
Their model assumes that a definite NPand its hypernym words usually co-occur in texts.Thus, for a definite-NP anaphor, a preceding NP thathas a high co-occurrence statistics in a large corpusis preferred for the antecedent.Bean and Riloff (2004) present a system calledBABAR that uses contextual role knowledge to docoreference resolution.
They apply an IE componentto unannotated texts to generate a set of extractioncaseframes.
Each caseframe represents a linguis-tic expression and a syntactic position, e.g.
?mur-der of <NP>?, ?killed <patient>?.
From the case-frames, they derive different types of contextual roleknowledge for resolution, for example, whether ananaphor and an antecedent candidate can be filledinto co-occurring caseframes, or whether they aresubstitutable for each other in their caseframes.
Dif-ferent from their system, our approach aims to findsurface patterns that can directly indicate the coref-erence relation between two NPs.Hearst (1998) presents a method to automate thediscovery of WordNet relations, by searching for thecorresponding patterns in large text corpora.
She ex-plores several patterns for the hyponymy relation,including ?X such as Y?
?X and/or other Y?, ?Xincluding / especially Y?
and so on.
The use ofHearst?s style patterns can be seen for the referenceresolution task.
Modjeska et al (2003) explore theuse of the Web to do the other-anaphora resolution.In their approach, a pattern ?X and other Y?
is used.Given an anaphor and a candidate antecedent, thepattern is instantiated with the two NPs and forms aquery.
The query is submitted to the Google search-ing engine, and the returned hit number is utilized tocompute the semantic relatedness between the twoNPs.
In their work, the semantic information is usedas a feature for the learner.
Markert et al (2003) andPoesio et al (2004) adopt a similar strategy for thebridging anaphora resolution.In (Hearst, 1998), the author also proposes to dis-cover new patterns instead of using the manuallydesigned ones.
She employs a bootstrapping algo-rithm to learn new patterns from the word pairs witha known relation.
Based on Hearst?s work, Pan-tel and Pennacchiotti (2006) further give a method529which measures the reliability of the patterns basedon the strength of association between patterns andinstances, employing the pointwise mutual informa-tion (PMI).3 Framework of Coreference ResolutionOur coreference resolution system adopts thecommon learning-based framework as employedby Soon et al (2001) and Ng and Cardie (2002).In the learning framework, a training or testinginstance has the form of i{NPi, NPj}, in whichNPj is a possible anaphor and NPi is one of its an-tecedent candidates.
An instance is associated witha vector of features, which is used to describe theproperties of the two noun phrases as well as theirrelationships.
In our baseline system, we adopt thecommon features for coreference resolution such aslexical property, distance, string-matching, name-alias, apposition, grammatical role, number/genderagreement and so on.
The same feature set is de-scribed in (Ng and Cardie, 2002) for reference.During training, for each encountered anaphorNPj , one single positive training instance is createdfor its closest antecedent.
And a group of negativetraining instances is created for every interveningnoun phrases between NPj and the antecedent.Based on the training instances, a binary classifiercan be generated using any discriminative learningalgorithm, like C5 in our study.
For resolution, aninput document is processed from the first NP to thelast.
For each encountered NPj , a test instance isformed for each antecedent candidate, NPi1.
Thisinstance is presented to the classifier to determinethe coreference relationship.
NPj will be resolvedto the candidate that is classified as positive (if any)and has the highest confidence value.In our study, we augment the common frameworkby incorporating non-anaphors into training.
We fo-cus on the non-anaphors that the original classifierfails to identify.
Specifically, we apply the learnedclassifier to all the non-anaphors in the training doc-uments.
For each non-anaphor that is classified aspositive, a negative instance is created by pairing thenon-anaphor and its false antecedent.
These neg-1For resolution of pronouns, only the preceding NPs in cur-rent and previous two sentences are considered as antecedentcandidates.
For resolution of non-pronouns, all the precedingnon-pronouns are considered.ative instances are added into the original traininginstance set for learning, which will generate a clas-sifier with the capability of not only antecedent iden-tification, but also non-anaphorically identification.The new classier is applied to the testing documentto do coreference resolution as usual.4 Patterned Based Semantic Relatedness4.1 Acquiring the PatternsTo derive patterns to indicate a specific semantic re-lation, a set of seed NP pairs that have the relation ofinterest is needed.
As described in the previous sec-tion, we have a set of training instances formed byNP pairs with known coreference relationships.
Wecan just use this set of NP pairs as the seeds.
That is,an instance i{NPi, NPj} will become a seed pair(Ei:Ej) in which NPi corresponds to Ei and NPjcorresponds to Ej .
In creating the seed, for a com-mon noun, only the head word is retained while fora proper name, the whole string is kept.
For ex-ample, instance i{?Bill Clinton?, ?the former pres-ident?}
will be converted to a NP pair (?Bill Clin-ton?:?president?
).We create the seed pair for every training instancei{NPi, NPj}, except when (1) NPi or NPj is apronoun; or (2) NPi and NPj have the same headword.
We denote S+ and S- the set of seed pairsderived from the positive and the negative traininginstances, respectively.
Note that a seed pair maypossibly belong to S+ can S- at the same time.For each of the seed NP pairs (Ei:Ej), we searchin a large corpus for the strings that match the reg-ular expression ?Ei * * * Ej?
or ?Ej * * * Ei?,where * is a wildcard for any word or symbol.
Theregular expression is defined as such that all the co-occurrences of Ei and Ej with at most three words(or symbols) in between are retrieved.For each retrieved string, we extract a surface pat-tern by replacing expression Ei with a mark <#t1#>and Ej with <#t2#>.
If the string is followed by asymbol, the symbol will be also included in the pat-tern.
This is to create patterns like ?X * * * Y [, .
?
]?where Y, with a high possibility, is the head word,but not a modifier of another noun phrase.As an example, consider the pair (?Bill Clin-ton?:?president?).
Suppose that two sentences in acorpus can be matched by the regular expressions:530(S1) ?
Bill Clinton is elected President of theUnited States.?
(S2) ?The US President, Mr Bill Clinton, to-day advised India to move towards nuclear non-proliferation and begin a dialogue with Pakistan to... ?.The patterns to be extracted for (S1) and (S2), re-spectively, areP1: <#t1#> is elected <#t2#>P2: <#t2#> , Mr <#t1#> ,We record the number of strings matched by a pat-tern p instantiated with (Ei:Ej), noted |(Ei, p, Ej)|,for later use.For each seed pair, we generate a list of surfacepatterns in the above way.
We collect all the pat-terns derived from the positive seed pairs as a setof reference patterns, which will be scored and usedto evaluate the semantic relatedness for any new NPpair.4.2 Scoring the Patterns4.2.1 FrequencyOne possible scoring scheme is to evaluate a pat-tern based on its commonality to positive seed pairs.The intuition here is that the more often a pattern isseen for the positive seed pairs, the more indicativethe pattern is to find positive coreferential NP pairs.Based on this idea, we score a pattern by calculatingthe number of positive seed pairs whose pattern listcontains the pattern.
Formally, supposing the pat-tern list associated with a seed pair s is PList(s), thefrequency score of a pattern p is defined asFreqency(p) = |{s|s ?
S+, p ?
PList(s)}| (1)4.2.2 ReliabilityAnother possible way to evaluate a pattern isbased on its reliability, i.e., the degree that the pat-tern is associated with the positive coreferential NPs.In our study, we use pointwise mutual informa-tion (Cover and Thomas, 1991) to measure associ-ation strength, which has been proved effective inthe task of semantic relation identification (Panteland Pennacchiotti, 2006).
Under pointwise mutualinformation (PMI), the strength of association be-tween two events x and y is defined as follows:pmi(x, y) = log P (x, y)P (x)P (y) (2)Thus the association between a pattern p and apositive seed pair s:(Ei:Ej) is:pmi(p, (Ei : Ej)) = log|(Ei,p,Ej)||(?,?,?)||(Ei,?,Ej)||(?,?,?)||(?,p,?)||(?,?,?
)|(3)where |(Ei,p,Ej)| is the count of strings matchedby pattern p instantiated with Ei and Ej .
Asterisk *represents a wildcard, that is:|(Ei, ?, Ej)| =?p?PList(Ei:Ej)|(Ei, p, Ej)| (4)|(?, p, ?
)| =?
(Ei:Ej)?S+?S?|(Ei, p, Ej)| (5)|(?, ?, ?
)| =?(Ei:Ej)?S+?S?
;p?Plist(Ei:Ej)|(Ei, p, Ej)| (6)The reliability of pattern is the average strength ofassociation across each positive seed pair:r(p) =?s?S+pmi(p,s)max pmi|S + | (7)Here max pmi is used for the normalization pur-pose, which is the maximum PMI between all pat-terns and all positive seed pairs.4.3 Exploiting the Patterns4.3.1 Patterns FeaturesOne strategy is to directly use the reference pat-terns as a set of features for classifier learning andtesting.
To select the most effective patterns forthe learner, we rank the patterns according to theirscores and then choose the top patterns (first 100 inour study) as the features.As mentioned, the frequency score is based on thecommonality of a pattern to the positive seed pairs.However, if a pattern also occurs frequently for thenegative seed pairs, it should be not deemed a goodfeature as it may lead to many false positive pairsduring real resolution.
To take this factor into ac-count, we filter the patterns based on their accuracy,which is defined as follows:Accuracy(p) = |{s|s ?
S+, p ?
PList(s)}||{s|s ?
S + ?
S?, p ?
PList(s)}| (8)A pattern with an accuracy below threshold 0.5 iseliminated from the reference pattern set.
The re-maining patterns are sorted as normal, from whichthe top 100 patterns are selected as features.531NWire NPaper BNewsR P F R P F R P FNormal Features 54.5 80.3 64.9 56.6 76.0 64.9 52.7 75.3 62.0+ ?X such as Y?
proper names 55.1 79.0 64.9 56.8 76.1 65.0 52.6 75.1 61.9all types 55.1 78.3 64.7 56.8 74.7 64.4 53.0 74.4 61.9+ ?X and other Y?
proper names 54.7 79.9 64.9 56.4 75.9 64.7 52.6 74.9 61.8all types 54.8 79.8 65.0 56.4 75.9 64.7 52.8 73.3 61.4+ pattern features (frequency) proper names 58.7 75.8 66.2 57.5 73.9 64.7 54.0 71.1 61.4all types 59.7 67.3 63.3 57.4 62.4 59.8 55.9 57.7 56.8+ pattern features (filtered frequency) proper names 57.8 79.1 66.8 56.9 75.1 64.7 54.1 72.4 61.9all types 58.1 77.4 66.4 56.8 71.2 63.2 55.0 68.1 60.9+ pattern features (PMI reliability) proper names 58.8 76.9 66.6 58.1 73.8 65.0 54.3 72.0 61.9all types 59.6 70.4 64.6 58.7 61.6 60.1 56.0 58.8 57.4+ single reliability feature proper names 57.4 80.8 67.1 56.6 76.2 65.0 54.0 74.7 62.7all types 57.7 76.4 65.7 56.7 75.9 64.9 55.1 69.5 61.5Table 1: The results of different systems for coreference resolutionEach selected pattern p is used as a single fea-ture, PFp.
For an instance i{NPi, NPj}, a list ofpatterns is generated for (Ei:Ej) in the same way asdescribed in Section 4.1.
The value of PFp for theinstance is simply |(Ei, p, Ej)|.The set of pattern features is used together withthe other normal features to do the learning and test-ing.
Thus, the actual importance of a pattern incoreference resolution is automatically determinedin a supervised learning way.4.3.2 Semantic Relatedness FeatureAnother strategy is to use only one semantic fea-ture which is able to reflect the reliability that a NPpair is related in semantics.
Intuitively, a NP pairwith strong semantic relatedness should be highlyassociated with as many reliable patterns as possi-ble.
Based on this idea, we define the semantic re-latedness feature (SRel) as follows:SRel(i{NPi, NPj}) =1000 ?
?p?PList(Ei:Ej)pmi(p, (Ei : Ej)) ?
r(p) (9)where pmi(p, (Ei:Ej)) is the pointwise mutual in-formation between pattern p and a NP pair (Ei:Ej),as defined in Eq.
3. r(p) is the reliability score of p(Eq.
7).
As a relatedness value is always below 1,we multiple it by 1000 so that the feature value willbe of integer type with a range from 0 to 1000.
Notethat among PList(Ei:Ej), only the reference patternsare involved in the feature computing.5 Experiments and Discussion5.1 Experimental setupIn our study we did evaluation on the ACE-2 V1.0corpus (NIST, 2003), which contains two data set,training and devtest, used for training and testing re-spectively.
Each of these sets is further divided bythree domains: newswire (NWire), newspaper (NPa-per), and broadcast news (BNews).An input raw text was preprocessed automati-cally by a pipeline of NLP components, includ-ing sentence boundary detection, POS-tagging, TextChunking and Named-Entity Recognition.
Two dif-ferent classifiers were learned respectively for re-solving pronouns and non-pronouns.
As mentioned,the pattern based semantic information was only ap-plied to the non-pronoun resolution.
For evaluation,Vilain et al (1995)?s scoring algorithm was adoptedto compute the recall and precision of the wholecoreference resolution.For pattern extraction and feature computing, weused Wikipedia, a web-based free-content encyclo-pedia, as the text corpus.
We collected the EnglishWikipedia database dump in November 2006 (re-fer to http://download.wikimedia.org/).
After all thehyperlinks and other html tags were removed, thewhole pure text contains about 220 Million words.5.2 Results and DiscussionTable 1 lists the performance of different coref-erence resolution systems.
The first line of thetable shows the baseline system that uses onlythe common features proposed in (Ng and Cardie,2002).
From the table, our baseline system can532NO Frequency Frequency (Filtered) PMI Reliabilty1 <#t1> <#t2> <#t2> | | <#t1> | <#t1> : <#t2>2 <#t2> <#t1> <#t1> ) is a <#t2> <#t2> : <#t1>3 <#t1> , <#t2> <#t1> ) is an <#t2> <#t1> .
the <#t2>4 <#t2> , <#t1> <#t2> ) is an <#t1> <#t2> ( <#t1> )5 <#t1> .
<#t2> <#t2> ) is a <#t1> <#t1> ( <#t2>6 <#t1> and <#t2> <#t1> or the <#t2> <#t1> ( <#t2> )7 <#t2> .
<#t1> <#t1> ( the <#t2> <#t1> | | <#t2> |8 <#t1> .
the <#t2> <#t1> .
during the <#t2> <#t2> | | <#t1> |9 <#t2> and <#t1> <#t1> | <#t2> <#t2> , the <#t1>10 <#t1> , the <#t2> <#t1> , an <#t2> <#t1> , the <#t2>11 <#t2> .
the <#t1> <#t1> ) was a <#t2> <#t2> ( <#t1>12 <#t2> , the <#t1> <#t1> in the <#t2> - <#t1> , <#t2>13 <#t2> <#t1> , <#t1> - <#t2> <#t1> and the <#t2>14 <#t1> <#t2> , <#t1> ) was an <#t2> <#t1> .
<#t2>15 <#t1> : <#t2> <#t1> , many <#t2> <#t1> ) is a <#t2>16 <#t1> <#t2> .
<#t2> ) was a <#t1> <#t1> during the <#t2>17 <#t2> <#t1> .
<#t1> ( <#t2> .
<#t1> <#t2> .18 <#t1> ( <#t2> ) <#t2> | <#t1> <#t1> ) is an <#t2>19 <#t1> and the <#t2> <#t1> , not the <#t2> <#t2> in <#t1> .20 <#t2> ( <#t1> ) <#t2> , many <#t1> <#t2> , <#t1>.
.
.
.
.
.
.
.
.
.
.
.Table 2: Top patterns chosen under different scoring schemesachieve a good precision (above 75%-80%) with arecall around 50%-60%.
The overall F-measure forNWire, NPaper and BNews is 64.9%, 64.9% and62.0% respectively.
The results are comparable tothose reported in (Ng, 2005) which uses similar fea-tures and gets an F-measure of about 62% for thesame data set.The rest lines of Table 1 are for the systems us-ing the pattern based information.
In all the sys-tems, we examine the utility of the semantic infor-mation in resolving different types of NP Pairs: (1)NP Pairs containing proper names (i.e., Name:Nameor Name:Definites), and (2) NP Pairs of all types.In Table 1 (Line 2-5), we also list the results ofincorporating two commonly used patterns, ?X(s)such as Y?
and ?X and other Y(s)?.
We can find thatneither of the manually designed patterns has signif-icant impact on the resolution performance.
For allthe domains, the manual patterns just achieve slightimprovement in recall (below 0.6%), indicating thatcoverage of the patterns is not broad enough.5.2.1 Pattern FeaturesIn Section 4.3.1 we propose a strategy that di-rectly uses the patterns as features.
Table 2 lists thetop patterns that are sorted based on frequency, fil-tered frequency (by accuracy), and PMI reliability,on the NWire domain for illustration.From the table, evaluated only based on fre-quency, the top patterns are those that indicate theappositive structure like ?X, an/a/the Y?.
However,if filtered by accuracy, patterns of such a kind willbe removed.
Instead, the top patterns with both highfrequency and high accuracy are those for the copulastructure, like ?X is/was/are Y?.
Sorted by PMI reli-ability, patterns for the above two structures can beseen in the top of the list.
These results are consis-tent with the findings in (Cimiano and Staab, 2004)that the appositive and copula structures are indica-tive to find the is-a relation.
Also, the two commonlyused patterns ?X(s) such as Y?
and ?X and otherY(s)?
were found in the feature lists (not shown inthe table).
Their importance for coreference resolu-tion will be determined automatically by the learn-ing algorithm.An interesting pattern seen in the lists is ?X || Y |?,which represents the cases when Y and X appear inthe same of line of a table in Wikipedia.
For exam-ple, the following text?American || United States | Washington D.C. | .
.
.
?is found in the table ?list of empires?.
Thus the pair?American:United States?, which is deemed coref-erential in ACE, can be identified by the pattern.The sixth till the eleventh lines of Table 1 list theresults of the system with pattern features.
From thetable, adding the pattern features brings the improve-ment of the recall against the baseline.
Take the sys-tem based on filtered frequency as an example.
Wecan observe that the recall increases by up to 3.3%(for NWire).
However, we see the precision drops(up to 1.2% for NWire) at the same time.
Over-all the system achieves an F-measure better than thebaseline in NWire (1.9%), while equal (?0.2%) inNPaper and BNews.Among the three ranking schemes, simply usingfrequency leads to the lowest precision.
By contrast,using filtered frequency yields the highest precisionwith nevertheless the lowest recall.
It is reasonablesince the low accuracy features prone to false posi-533NameAlias = 1: ...NameAlias = 0::..Appositive = 1: ...Appositive = 0::..P014 > 0::...P003 <= 4: 0 (3): P003 > 4: 1 (25)P014 <= 0::..P004 > 0:...P004 <= 0::..P027 > 0: 1 (25/7)P027 <= 0::..P002 > 0: ...P002 <= 0::..P005 > 0: 1 (49/22)P005 <= 0::..String_Match = 1: .String_Match = 0: .// p002: <t1> ) is a <t2>// P003: <t1> ) is an <t2>// P004: <t2> ) is an <t1>// p005: <t2> ) is a <t1>// P014: <t1> ) was an <t2>// p027: <t1> , ( <t2> ,Figure 1: The decision tree (NWire domain) for thesystem using pattern features (filtered frequency)(feature String Match records whether the string of anaphorNP j matches that of a candidate antecedent NP i)tive NP pairs are eliminated, at the price of recall.Using PMI Reliability can achieve the highest re-call with a medium level of precision.
However, wedo not find significant difference in the overall F-measure for all these three schemes.
This should bedue to the fact that the pattern features need to befurther chosen by the learning algorithm, and onlythose patterns deemed effective by the learner willreally matter in the real resolution.From the table, the pattern features only workwell for NP pairs containing proper names.
Ap-plied on all types of NP pairs, the pattern featuresfurther boost the recall of the systems, but in themeanwhile degrade the precision significantly.
TheF-measure of the systems is even worse than thatof the baseline.
Our error analysis shows that anon-anaphor is often wrongly resolved to a false an-tecedent once the two NPs happen to satisfy a pat-tern feature, which affects precision largely (as anevidence, the decrease of precision is less significantwhen using filtered frequency than using frequency).Still, these results suggest that we just apply the pat-tern based semantic information in resolving propernames which, in fact, is more compelling as the se-mantic information of common nouns could be moreeasily retrieved from WordNet.We also notice that the patterned based semanticinformation seems more effective in the NWire do-main than the other two.
Especially for NPaper, theimprovement in F-measure is less than 0.1% for allthe systems tested.
The error analysis indicates itmay be because (1) there are less NP pairs in NPa-per than in NWire that require the external seman-tic knowledge for resolution; and (2) For many NPpairs that require the semantic knowledge, no co-occurrence can be found in the Wikipedia corpus.To address this problem, we could resort to the Webwhich contains a larger volume of texts and thuscould lead to more informative patterns.
We wouldlike to explore this issue in our future work.In Figure 1, we plot the decision tree learnedwith the pattern features for non-pronoun resolution(NWire domain, filtered frequency), which visuallyillustrates which features are useful in the referencedetermination.
We can find the pattern features oc-cur in the top of the decision tree, among the featuresfor name alias, apposition and string-matching thatare crucial for coreference resolution as reported inprevious work (Soon et al, 2001).
Most of the pat-tern features deemed important by the learner are forthe copula structure.5.2.2 Single Semantic Relatedness FeatureSection 4.3.2 presents another strategy to exploitthe patterns, which uses a single feature to reflect thesemantic relatedness between NP pairs.
The last twolines of Table 1 list the results of such a system.Observed from the table, the system with the sin-gle semantic relatedness feature beats those withother solutions.
Compared with the baseline, thesystem can get improvement in recall (up to 2.9%as in NWire), with a similar or even higher preci-sion.
The overall F-measure it produces is 67.1%,65.0% and 62.7%, better than the baseline in all thedomains.
Especially in the NWire domain, we cansee the significant (t-test, p ?
0.05) improvement of2.1% in F-measure.
When applied on All-Type NPpairs, the degrade of performance is less significantas using pattern features.
The resulting performanceis better than the baseline or equal.
Compared withthe systems using the pattern features, it can stillachieve a higher precision and F-measure (with a lit-tle loss in recall) .There are several reasons why the single seman-tic relatedness feature (SRel) can perform better thanthe set of pattern features.
Firstly, the feature valueof SRel takes into consideration the information ofall the patterns, instead of only the selected patterns.Secondly, since the SRel feature is computed basedon all the patterns, it reduces the risk of false posi-534NameAlias = 1: ...NameAlias = 0::..Appositive = 1: ...Appositive = 0::..SRel > 28::..SRel > 47: ...: SRel <= 47: ...SRel <= 28::..String_Match = 1: ...String_Match = 0: ...Figure 2: The decision tree (Nwire) for the systemusing the single semantic relatedness featuretive when a NP pair happens to satisfy one or severalpattern features.
Lastly, from the point of view ofmachine learning, using only one semantic feature,instead of hundreds of pattern features, can avoidoverfitting and thus benefit the classifier learning.In Figure 2, we also show the decision tree learnedwith the semantic relatedness feature.
We observethat the decision tree is simpler than that with pat-tern features as depicted in Figure 1.
After featurename-alias and apposite, the classifier checks dif-ferent ranges of the SRel value and make differentresolution decision accordingly.
This figure furtherillustrates the importance of the semantic feature.6 ConclusionsIn this paper we present a pattern based approach tocoreference resolution.
Different from the previouswork which utilizes manually designed patterns, ourapproach can automatically discover the patterns ef-fective for the coreference resolution task.
In ourstudy, we explore how to acquire and evaluate pat-terns, and investigate how to exploit the patterns tomine semantic relatedness information for corefer-ence resolution.
The evaluation on ACE data setshows that the patterned based features, when ap-plied on NP pairs containing proper names, can ef-fectively help the performance of coreference res-olution in the recall (up to 4.3%) and the overallF-measure (up to 2.1%).
The results also indicatethat using the single semantic relatedness feature hasmore advantages than using a set of pattern features.For future work, we intend to investigate ourapproach in more difficult tasks like the bridginganaphora resolution, in which the semantic relationsinvolved are more complicated.
Also, we would liketo explore the approach in technical (e.g., biomedi-cal) domains, where jargons are frequently seen andthe need for external knowledge is more compelling.Acknowledgements This research is supported by aSpecific Targeted Research Project (STREP) of the EuropeanUnion?s 6th Framework Programme within IST call 4, Boot-strapping Of Ontologies and Terminologies STrategic REsearchProject (BOOTStrep).ReferencesD.
Bean and E. Riloff.
2004.
Unsupervised learning of contex-tual role knowledge for coreference resolution.
In Proceed-ings of NAACL, pages 297?304.P.
Cimiano and S. Staab.
2004.
Learning by googling.SIGKDD Explorations Newsletter, 6(2):24?33.T.
Cover and J. Thomas.
1991.
Elements of Information The-ory.
Hohn Wiley & Sons.N.
Garera and D. Yarowsky.
2006.
Resolving and generatingdefinite anaphora by modeling hypernymy using unlabeledcorpora.
In Proceedings of CoNLL , pages 37?44.S.
Harabagiu, R. Bunescu, and S. Maiorano.
2001.
Text knowl-edge mining for coreference resolution.
In Proceedings ofNAACL, pages 55?62.M.
Hearst.
1998.
Automated discovery of wordnet relations.
InChristiane Fellbaum, editor, WordNet: An Electronic LexicalDatabase and Some of its Applications.
MIT Press, Cam-bridge, MA.K.
Markert, M. Nissim, and N. Modjeska.
2003.
Using theweb for nominal anaphora resolution.
In Proceedings of theEACL workshop on Computational Treatment of Anaphora,pages 39?46.N.
Modjeska, K. Markert, and M. Nissim.
2003.
Using theweb in machine learning for other-anaphora resolution.
InProceedings of EMNLP, pages 176?183.V.
Ng and C. Cardie.
2002.
Improving machine learning ap-proaches to coreference resolution.
In Proceedings of ACL,pages 104?111, Philadelphia.V.
Ng.
2005.
Machine learning for coreference resolution:From local classification to global ranking.
In Proceedingsof ACL, pages 157?164.P.
Pantel and M. Pennacchiotti.
2006.
Espresso: Leveraginggeneric patterns for automatically harvesting semantic rela-tions.
In Proceedings of ACL, pages 113?1200.M.
Poesio, R. Mehta, A. Maroudas, and J. Hitzeman.
2004.Learning to resolve bridging references.
In Proceedings ofACL, pages 143?150.S.
Ponzetto and M. Strube.
2006.
Exploiting semantic rolelabeling, wordnet and wikipedia for coreference resolution.In Proceedings of NAACL, pages 192?199.W.
Soon, H. Ng, and D. Lim.
2001.
A machine learning ap-proach to coreference resolution of noun phrases.
Computa-tional Linguistics, 27(4):521?544.R.
Vieira and M. Poesio.
2000.
An empirically based systemfor processing definite descriptions.
Computational Linguis-tics, 27(4):539?592.M.
Vilain, J. Burger, J. Aberdeen, D. Connolly, andL.
Hirschman.
1995.
A model-theoretic coreference scoringscheme.
In Proceedings of the Sixth Message understand-ing Conference (MUC-6), pages 45?52, San Francisco, CA.Morgan Kaufmann Publishers.535
