Handling Information Access Dialogue through QA Technologies?
A novel challenge for open-domain question answering ?Tsuneaki KatoThe University of Tokyokato@boz.c.u-tokyo.ac.jpFumito MasuiMie Universitymasui@ai.info.mie-u.ac.jpJun?ichi FukumotoRitsumeikan Universityfukumoto@media.ritsumei.ac.jpNoriko KandoNational Institute of Informaticskando@nii.ac.jpAbstractA novel challenge for evaluating open-domainquestion answering technologies is proposed.In this challenge, question answering systemsare supposed to be used interactively to an-swer a series of related questions, whereas inthe conventional setting, systems answer iso-lated questions one by one.
Such an interac-tion occurs in the case of gathering informa-tion for a report on a specific topic, or whenbrowsing information of interest to the user.
Inthis paper, first, we explain the design of thechallenge.
We then discuss its reality and showhow the capabilities measured by the challengeare useful and important in practical situations,and that the difficulty of the challenge is properfor evaluating the current state of open-domainquestion answering technologies.1 IntroductionOpen-domain question answering technologies allowusers to ask a question in natural language and obtainthe answer itself rather than a list of documents that con-tain the answer.
These technologies make it possibleto retrieve information itself rather than merely docu-ments, and will lead to new styles of information access(Voorhees, 2000).The recent research on open-domain question answer-ing concentrates on answering factoid questions one byone in isolation from each other.
Such systems that an-swer isolated factoid questions are the most basic level ofquestion answering technologies, and will lead to moresophisticated technologies that can be used by profes-sional reporters and information analysts.
On some stageof that sophistication, a cub reporter writing an article ona specific topic will be able to translate the main issue ad-dressed by his report into a set of simpler questions andthen pose those questions to the question answering sys-tem (Burger et al, 2001).In addition, there is a relation between multi-documentsummarization and question answering.
In his lecture,Eduard Hovy mentioned that multi-document summa-rization may be able to be reduced into a series of ques-tion answering (Hovy, 2001).
In SUMMAC, an intrinsicevaluation was conducted which measures the extent towhich a summary provides answers to a set of obligatoryquestions on a given topic (Mani et al, 1998).
Those sug-gest such question answering systems that can answer aseries of related questions would surely be a useful aid tosummarization work by human and by machine.Against this background, question answering systemsneed to be able to answer a series of questions, whichhave a common topic and/or share a local context.
Inthis paper, we propose a challenge to measure objectivelyand quantitatively such an ability of question answeringsystems.
We call this challenge QACIAD (Question An-swering Challenge for Information Access Dialogue).
Inthis challenge, question answering systems are used in-teractively to participate in dialogues for accessing infor-mation.
Such information access dialogue occurs suchas when gathering information for a report on a specifictopic, or when browsing information of interest to theuser.
Actually, in QACIAD, the interaction is only simu-lated and systems answer a series of questions in a batchmode.
Although such a simulation may neglect the in-herent dynamics of dialogue, it is a practical compromisefor objective evaluation and, as a result, the test sets ofthe challenge are reusable.Question answering systems need a wide range of abil-ities in order to participate in information access dia-logues (Burger et al, 2001).
First, the systems must re-spond in real time to make interaction possible.
Theymust also properly interpret a given question within thecontext of a specific dialogue, and also be cooperativeby adding appropriate information not mentioned explic-itly by the user.
Moreover, the systems should be ableto pose a question for clarification to resolve ambiguityconcerning the user?s goal and intentions, and to partici-pate in mixed initiative dialogue by making suggestionsand leading the user toward solving the problem.
Amongthese various capabilities, QACIAD focuses on the mostfundamental aspect of dialogue, that is, interpreting agiven question within the context of a specific dialogue.It measures context processing abilities of systems suchas anaphora resolution and ellipses handling.This paper is organized as follows.
The next chap-ter explains the design of QACIAD.
The following threechapters discuss the reality of the challenge.
First, we ex-plain the process of constructing the test set of the chal-lenge and introduce the results of a study conducted dur-ing this process which show the validity of QACIAD.That is, QACIAD measures valid abilities needed forparticipating in information access dialogues.
In otherwords, the ability measured by the challenge is crucialto the systems for realizing information access dialoguesfor writing reports and summaries.
Second, we show thestatistics of pragmatic phenomena in the constructed testset, and demonstrate that the challenge covers a wide va-riety of pragmatic phenomena observed in real dialogues.Third, based on a preliminary analysis of the QACIADrun, we show that the challenge has a proper difficultyfor evaluating the current state of open-domain questionanswering technologies.
In the last two chapters, we dis-cuss problems identified while constructing the test setand conducting the run, and draw some conclusions.2 Design of QACIAD2.1 HistoryThe origin of QACIAD comes from QAC1 (QuestionAnswering Challenge), one of the tasks of the NTCIR3workshop conducted from March 2001 through October2002 (NTCIR, 2001).
QACIAD was originally proposedin March 2001 as the third subtask of QAC1, its formalrun was conducted in May 2002 (Fukumoto et al, 2001;Fukumoto et al, 2002; Fukumoto et al, 2003), and the re-sults were reported at the NTCIR3 workshop meeting inOctober 2002.
The current design of QACIAD reportedin this paper is based on that challenge and is the resultof extensive elaboration.
The design of the challenge andconstruction of the test set were performed from January2003 through December 2003.
The formal run was con-ducted in December 2003, as a subtask of QAC2, whichin turn is a task of the NTCIR4 workshop (NTCIR, 2003).2.2 QAC as a common groundQAC is a challenge for evaluating question answeringtechnologies in Japanese.
It consists of three subtasksincluding QACIAD, and the common scope of those sub-tasks covers factoid questions that have names as an-swers.
Here, names mean not only names of proper items(named entities) including date expressions and monetaryvalues, but also common names such as names of speciesand names of body parts.
Although the syntactical rangeof the names approximately corresponds to compoundnouns, some of them, such as the titles of novels andmovies, deviate from that range.
The underlying docu-ment set consists of two years of articles of two newspa-pers in QAC2, and one newspaper in QAC1.
Using thosedocuments as the data source, the systems answer variousopen-domain questions.From the outset, QAC has focused on question answer-ing technologies that can be used as components of largerintelligent systems and technologies that can handle re-alistic problems.
It persists in requesting exact answersrather than the text snippets that contain them with thecost of avoiding handling definition questions and whyquestions, because such answers are crucial in order to beused as inputs to other intelligent systems such as multi-document summarization systems.
Moreover, as such asituation is considered to be more realistic, the systemsmust collect all the possible correct answers and detectthe absence of an answer.
Therefore two subtasks, one ofwhich is QACIAD, request systems to return one list ofanswers that contains all and only correct answers, whilethe other subtask requests systems to return a ranked listof possible answers as in TREC-8.
In both subtasks, thepresence of answers in the underlying documents is notguaranteed and the number of answers is not specified, sothese subtasks are similar to the list question task in theTREC-2003 style rather than the TREC-10 style (TREC,2003).2.3 Information access dialogueConsidering scenes in which those question answeringsystems participate in a dialogue, we classified informa-tion access dialogues into the following two categories.As discussed later, dialogues in a real situation may havedifferent features in their different portions; the classifi-cation just shows two extremes.Gathering Type The user has a concrete objective suchas writing a report and summary on a specific topic,and asks a system a series of questions all concern-ing that topic.
The dialogue has a common globaltopic, and, as a result, each consecutive questionshares a local context.Browsing Type The user does not have any fixed topicof interest; the topic of interest varies as the dialogueprogresses.
No global topic covers a whole dialoguebut each consecutive question shares a local context.This paper proposes the design of the challenge, whichcan measure the abilities of question answering systemsuseful in such dialogues.2.4 The settingQACIAD requests participant systems to return all pos-sible answers to a series of questions, each of which is afactoid question that has names as answer.
This series ofquestions and the answers to those questions comprise aninformation access dialogue.
Two examples of the seriesof questions are shown in Figure 1, which were pickedup from our test set discussed in the next chapter.
Se-ries 14 is a series of a typical gathering type, while series22 of a typical browsing type.
In QACIAD, a number ofseries (in the case of our test set, 36 series) are given tothe system at once and systems are requested to answerthose series in a batch mode.
One series consists of sevenquestions on average.
The systems must identify the typeto which a series belongs, as it is not given.
The systemsneed not identify the changes of series, as the boundaryof series is given.
Those, however, must not look aheadto the questions following the one currently being han-dled.
This restriction reflects the fact that QACIAD is asimulation of interactive use of question answering sys-tems in dialogues.
This restriction, accompanied with theexistence of two types of series, increases the complexityof the context processing that the systems must employ.For example, the systems need to identify that series 22is a browsing type and the focus of the second question isYankee stadium rather than New York Yankees withoutlooking ahead to the following questions.
Especially inJapanese, since anaphora are not realized often and thedefinite and indefinite are not clearly distinguished, thoseproblems are more serious.2.5 Evaluation measureIn QACIAD, as the systems are requested to return onelist consisting all and only correct answers and the num-ber of correct answers differs for each question1, mod-ified F measure is used for the evaluation, which takesaccount of both precision and recall.
Two modificationswere needed.
The first is for the case where an answerlist returned by a system contains the same answer morethan once or answers in different expressions denotingthe same item.
In that case, only one answer is regardedas the correct one, and so the precision of such answerlist decreases.
Cases regarded as different expressionsdenoting the same item include a person?s name with andwithout the position name, variations of foreign name no-tation, differences of monetary units used, differences oftime zone referred to, and so on.
The second modifica-tion is for questions with no answer.
For those questions,modified F measure is 1.0 if a system returns an emptylist as the answer, and is 0.0 otherwise.1It is a special case that the number of answers is just onefor all questions shown in Figure 1.Series 14When was Seiji Ozawa born?Where was he born?Which university did he graduate from?Who did he study under?Who recognized him?Which orchestra was he conducting in 1998?Which orchestra will he begin to conduct in 2002?Series 22Which stadium is home to the New York Yankees?When was it built?How many persons?
monuments have beendisplayed there?Whose monument was displayed in 1999?When did he come to Japan on honeymoon?Who was the bride at that time?Who often draws pop art using her as a motif?What company?s can did he often draw also?Figure 1: Examples of series of questionsThe judgment as to whether a given answer is corrector not takes into account not only an answer itself butalso the accompanying article from which the answer wasextracted.
When the article does not validly support theanswer, that is, assessors cannot understand that the an-swer is the correct one for a given question by readingthat article, it is regarded as incorrect even though theanswer itself is correct.
The correctness of an answeris determined according to the interpretation of a givenquestion done by human assessors within the given con-text.
The system?s answers to previous questions, and itsunderstanding of the context from which those answerswere derived, are irrelevant.
For example, the correct an-swer to the second question of series 22, namely when theYankee stadium was built, is 1923.
If the system wronglyanswers the Shea stadium to the first question, and then?correctly?
answers the second question 1964, the yearwhen the Shea stadium was built, that answer to the sec-ond question is not correct.
On the other hand, if thesystem answers 1923 to the second question with an ap-propriate article supporting it, that answer is correct nomatter how the system answered the first question.3 Constructing a Test Set and Usefulnessof the ChallengeWe collected and analyzed questions for two purposes.The first purpose was to establish a methodology for con-structing a test set based on the design of QACIAD dis-cussed in the previous chapter.
The second purpose wasto confirm the reality of the challenge, that is, to deter-mine whether it is useful for information access dialoguesto use question answering systems that can answer ques-tions that have names as answers.3.1 Collecting questionsQuestions were collected as follows.
Subjects were pre-sented various topics, which included persons, organiza-tions, and events selected from newspaper articles, andwere requested to make questions that ask for informa-tion to be used in the report on that topic.
The report issupposed to describe facts on a given topic, rather thancontain opinions or prospects on the topic.
The ques-tions are restricted to wh-type questions, and natural se-ries of questions containing anaphoric expressions and soon were constructed.
The topics were presented in threedifferent ways: only by a short description of the topic,which corresponds to the title part of the TREC topic def-inition; with a short article or the lead of a longer article,which is representative of that topic and corresponds tothe narrative part of the TREC topic definition; and withfive articles concerning that topic.
The number of top-ics was 60, selected from two years of newspaper arti-cles.
Thirty subjects participated in the experiment.
Eachsubject made questions for ten topics for each topic pre-sentation pattern, and was instructed to make around tenquestions for each topic.
It is worth noting that the ques-tions obtained were natural in both content and expres-sion since in this experiment the subjects did not considerwhether the answers to their questions would be found inthe newspapers, and some subjects did not read the arti-cles at all.This time, for the test set construction and preliminaryanalysis, 1,033 questions on 40 topics, made by three sub-jects for each topic with different topic presentation pat-terns, were used.
All of the questions collected are nowbeing analyzed extensively, especially on the differencesamong questions according to the topic presentation pat-tern.3.2 Analysis of the questionsOur main concern here is how many of the questionscollected fall into the category of questions that the cur-rent question answering systems could answer.
In otherwords, how many of the questions can be answered by alist of names?
In the case the majority of them fall intosuch a category, it is realistic to use question answeringsystems for information access dialogues and the chal-lenge on such abilities must be useful.Table 1 shows the classification of questions accordingto the subject asked.
In the case where users ask ques-tions to get information for a report, the number of whyquestions is relatively small.
Moreover, there were fewerquestions requesting an explanation or definition than ex-Table 1: Categorization of questions by subjectAsking about4W (Who, When, Where, What)70%incl.
several types of numerical valuesWhy 4%How, for a procedure or method 10%Definitions, descriptions or explanations 16%Table 2: Categorization of questions by answer typeAnswered inNumerical values or date expressions 28%Proper names 22%Common names (in compound nouns) 8%Names probably 14%Clauses, sentences, or texts 28%pected, probably because questions such as ?Who is SeijiOzawa?
were decomposed into relatively concrete ques-tions such as those asking for his birthday and birth place.However, not all questions that were categorized as4W questions could be answered by names.
For exam-ple, whereas questions asking where, such as ?Where wasShakespeare born?
?, could be answered by a place name,questions like ?Where do lobsters like to live??
need adescription and not a proper name as the answer.
Table 2shows the result of categorization according to this as-pect.
This categorization was conducted by inspectingquestions only, and some of the questions were hard todetermine decisively whether those could be answeredby names or not, and so were categorized as ?Namesprobably?.
For example, the question ?Where does thename ?AIBO?
come from??
could be answered by nameif AIBO is an acronym, but there may be a long story asto its origin.
Although such cases happened in other com-binations of categories, those questions were categorizedinto a more complex category as only the border of namesand descriptions are important in the current analysis.As Table 2 shows, 58% to 72% of questions could beanswered by names.
The amount of those questions is al-most same as the amount of 4W questions, since whilesome 4W questions could not be answered by names,some definition and explanation questions might be ableto be answered by names.
The fact that 58% to 72% ofquestions for writing reports could be answered by namesdemonstrates that question answering systems that an-swer these questions are useful in such situations.In addition, the answers to 84% of those 72% questionscould be found by humans from newspaper articles.
Thisindicates that the setting is realistic where users write re-ports through interacting with a question answering sys-tem that uses newspaper articles as its data source.3.3 Constructing a test setUsing the questions collected, we constructed a test setas follows.
We selected 26 from 40 topics, and choseappropriate questions and rearranged them for construct-ing gathering type series.
Some of the questions wereedited in order to resolve semantic or pragmatic ambigui-ties, though we tried to use the questions without modifi-cation where possible.
The topics of the gathering seriesconsisted of 5 persons, 2 organizations, 11 events, 5 ar-tifacts, and 3 animals and fishes, among which 4 topicsconcerned sets of organizations and events, such as thebig three companies in the beer industry, simultaneousterrorist attacks, and annual festival events.Browsing type series were constructed by using someof the remaining questions as seeds of a sequence and byadding new questions to create a flow to/from those ques-tions.
For example, series 22 shown in Figure 1 was com-posed by adding the last four newly created questions tothe first four questions which were collected for the Yan-kee stadium2.
For such seeds, we also used the collectionof questions for evaluating summarization constructed forTSC (Text Summarization Challenge), another challengein the NTCIR workshop (TSC, 2003).
Some topics usedfor the question collection were the same as the topicsused in TSC also.
We made 10 browsing series in thisway.Finally, the test set constructed this time contained 36series and 251 questions, with 26 series of the gather-ing type and 10 series of the browsing type.
The averagenumber of questions in one series was 6.92.4 Characteristics of the Test SetThis chapter describes the pragmatic characteristics ofthe constructed test set.
Japanese has four major typesof anaphoric devices: pronouns, zero pronouns, definitenoun phrases, and ellipses.
Zero pronouns are very com-mon in Japanese in which pronouns are not realized onthe surface.
As Japanese also has a completely differentdeterminer system from English, the difference betweendefinite and indefinite is not apparent on the surface,and definite noun phrases usually have the same formas generic noun phrases.
Table 3 shows the summaryof such pragmatic phenomena observed in 215 questionsobtained by removing the first one of each series fromthe 251 questions in the test set.
The total number ismore than 215 as 12 questions contain more than one phe-nomenon.
The sixth question in series 22, ?Who was thebride at that time??
is an example of such a question with2The question focus of the first one was changed.Table 3: Pragmatic phenomena observed in the test setType OccurencePronouns 76 (21)Zero pronouns 134 (33)Definite noun phrases 11 (4)Ellipses 7multiple anaphoric expressions.
The numbers in paren-theses show the number of cases in which the referenceditem is an event.
As the table indicates, a wide range ofpragmatic phenomena is observed in the test set.Precisely speaking, the series in the test set can becharacterized through the pragmatic phenomena that theycontain.
Gathering type series consist of questions thathave a common referent in a broad sense, which is aglobal topic mentioned in the first question of the series.Strictly gathering type series can be distinguished as aspecial case of gathering type series.
In those series, allquestions refer exactly to the same item mentioned in thefirst question and do not have any other anaphoric ex-pression.
In other words, questions about the commontopic introduced by the first question comprise a wholesequence.
Series 14 in Figure 1 is an example of thestrictly gathering type and all questions can be interpretedby supplying Seiji Ozawa, who is introduced in the firstquestion.
The test set has 5 series of the strictly gatheringtype.
Other gathering type series have other two types ofquestions.
The first type of questions not only has a ref-erence to the global topic but also refers to other items orhas an ellipsis.
The second type of questions has a refer-ence to a complex item, such as an event that contains theglobal topic as its component.
Series 20 shown in Fig-ure 2 is such a series.
The third question refers not onlyto the global topic, George Mallory, in this case, but alsoto his famous phrase.
The sixth one refers to an eventGeorge Mallory was concerned in.On the other hand, the questions of a browsing typeseries do not have such a global topic.
Sometimes thereferent is the answer of the immediately preceding ques-tion, such as the fifth, seventh and eighth questions inseries 22 in Figure 1.
No series, however, consists solelyof questions that have only a reference to the answer tothe immediately previous questions.
All series containreferences to the answers to non-immediately previousquestions or items mentioned in the previous questions,or more than one pragmatic phenomenon.
In series 22,the third, fourth and sixth questions belong to such a case.In both types, therefore, the shifting pattern of the fo-cus is not simple, and so a sophisticated way is needed totrack it.
Such focus tracking is indispensable to get cor-rect answers.
Systems cannot even retrieve articles con-Series 20In which country was George Mallory born?What was his famous phrase?When did he say it?How old was he when he started climbingmountains?On which expedition did he go missing near thetop of Everest?When did it happen?At what altitude on Everest was he seen last?Who found his body?Figure 2: Another example of series of questionstaining the answer just by accumulating keywords.
Thisis clear for the browsing type, as an article is unlikely tomention both the New York Yankees and Campbell soup.In the gathering type, since the topics mentioned in rela-tively many articles were chosen, it is not easy to locatethe answer to a question from those articles retrieved us-ing that topic as the keyword.
For example, there are 155articles mentioning Seiji Ozawa in our document sets, ofwhich 22 articles mention his move to the Vienna Phil-harmonic Orchestra, and only two articles also mentionhis birthday.
An extensive, quantitative analysis is nowin progress.5 Difficulty of the Challenge and theCurrent State of TechnologiesSeven teams and fourteen systems participated in the runusing the test set mentioned in the previous chapter con-ducted in December 2003.
In this chapter, based on apreliminary analysis of the run, the difficulty of the chal-lenge and the current state of technologies for addressingthe challenge are discussed.
The techniques employed inthe participant systems have not yet been published, butwill be published by the NTCIR workshop 4 meeting atthe latest.Figure 3 shows the mean modified F measures of thetop 10 participant systems.
The chart shows the meanmodified F measure of three categories: all of the test setquestions, the questions of the first of each series, andquestions of the second and after.
As anticipated, it ismore difficult to answer correctly the questions other thanthe first question of each series.
This indicates that moresophisticated context processing is needed.The mean modified F measure is not high even for thetop systems.
This is probably because of not only thedifficulties of context processing but also the difficultiesof returning the list of all and only correct answers.
Itis difficult to achieve high recall since some of the ques-                ffflfi ffi "!$#&% ' ffi(fi '' "!)
*#+% ,' fi ffi 'Figure 3: Evaluation by mean F measure                    ffflfi fiffi  !#"%$'&)( * !+ * ",* "%$-".&)( *  !
* "Figure 4: Another evaluationtions have many correct answers, such as asking for all ofthe countries and regions which participated in the 1998football world cup held in France.
The modified F mea-sure is only 0.33 if a system returns a list of five itemsincluding the only correct answer, as the precision is 0.2in that case.
In order to remove the effects of such dif-ficulties on answering lists approximately, the number ofquestions to which the system gives at least one of thecorrect answers was calculated.
The result is shown inFigure 4.
The rank of the systems somewhat changes bythis approximation, as some systems benefit from this ap-proximation and others do not.
Based on this criterion,the best system answered correctly 45% of the questions,which is inadequate for practical use.
However, the resultshows that this challenge is not too hard and desperate,though it is challenging for existing question answeringtechnologies.The mean modified F measures for the strictly gath-ering type, other gathering type, and browsing type areshown in Figure 5.
For the majority, the questions in thebrowsing type series are more difficult to answer, as an-ticipated.          	        fiffffifl "!$#&%(' ) *ffffifl "!$#&%+' ),*- %+.
/10,' ) *Figure 5: Differences on series types6 DiscussionWith existing technologies, which still have room forstudy for answering ordinary questions without prag-matic processing and particularly remain inadequate foranswering list questions correctly, QACIAD cannot eas-ily independently evaluate the context processing abilitiesfrom other general abilities concerning question answer-ing.
The ability that QACIAD measures is a combina-tion of several kinds of abilities concerning question an-swering for handling information access dialogues.
Al-though this may be desirable and an objective of QA-CIAD, sometime we need an isolated evaluation of con-text processing.
In order to fulfill this need, we devisedtwo types of accompanying test sets for reference.
Thefirst reference test set consists of isolated questions, thatis, not in series, obtained from questions of the originaltest set by manually resolving all anaphoric expressionsincluding zero anaphora.
The second reference test setconsists of isolated questions obtained from questions ofthe original test set by mechanically removing anaphoricexpressions.
Though most of the questions in the secondtest set are semantically under-specified, such as asking abirthday without specifying whose one, all the questionsare syntactically well formed in the case of Japanese.
Thefirst reference test set measures the ceiling of the contextprocessing in a given original test set, while the secondmeasures the floor.
These are only for reference, sincethere are several ways of resolving anaphora and con-text processing sometimes makes thing worse.
Neverthe-less, the reference test sets should be useful for analyzingthe characteristics of technologies used by the participantsystems.
We are now analyzing the results of the run onthose reference test sets for our current test set, and willpresent the results in due course.As described above, we believe that the task setting inQACIAD is real, even though this is not clear from theevaluation method.
There are two major problems.
Thefirst concerns the F measure.
First, the F measure can-not be calculated until the number of correct answers isfixed, which means the value of the F measure changeswhen a new correct answer is found.
This makes theevaluation cumbersome.
Especially in question answer-ing, as the number of correct answers is usually relativelysmall, the recall rate sometimes falls to half if a minoralternative answer is found to a question that had beenassumed to have only one correct answer.
Even worse,some questions have more than one way of enumeratingcorrect answers.
For example, to a question asking for thesites of a ski jump competition, a system may answer sixcity names, and another system may answer three coun-try names.
Neither are wrong.
A system could even an-swer four city names and one country name.
We needa principle for handling such cases.
In TREC-2003 thisproblem were cleverly avoided by carefully checking thequestion3.The second and more serious problem comes fromhandling dialogues.
As mentioned above, whether an an-swer is correct or not is determined by human interpre-tation of a given question within the given context andis not affected by a system?s interpretation and the an-swers it returned to the previous questions.
Many feel thatthis evaluation criterion is somewhat peculiar.
As men-tioned in the example in chapter 2.5, in series 22, the an-swer to the second question, 1923, is considered correcteven if the system wrongly answered the Shea stadiumto the first question.
This is not completely absurd be-cause that system may manage the context intensionally,in which case the system may interpret the second ques-tion as ?When was the home to the New York Yankeesbuilt??
It is doubtful, however, whether such a ?correct?answer has any value in practice.
This problem shows theimportance of cooperative response.
It may be effectiveto change the style of answering from a current list ofanswers to answers with additional information.
In thisexample, it would be better to answer ?The Yankee sta-dium was built in 1923?, and the correctness of answersshould be judged by including this additional informa-tion.
The difficult and remaining problem is to formalizethis type of cooperative response to a sufficient level foruse in objective evaluations like QACIAD.7 ConclusionA novel challenge, QACIAD (Question Answering Chal-lenge for Information Access Dialogues), was proposedfor evaluating the abilities for handling information ac-cess dialogues through open-domain question answer-3Personal communication with Dr. Ellen Voorhees.ing technologies.
Question answering systems with suchabilities measured by this challenge are expected to beuseful for making reports and summaries.
The proposedchallenge has reasonable difficulties with existing tech-nologies.
Our proposal also has several important ideas,including the distinction of series of questions into gath-ering type and browsing type series, and the introduc-tion of reference test sets for extracting and evaluatingthe context processing abilities of the systems.AcknowledgmentsThe authors would like to thank all participants in NT-CIR4 Workshop QAC2 task for their valuable commentson the task design and intensive works for addressing thetask.
This research was supported in part by the joint re-search grant of National Institute of Informatics.ReferencesEduard Hovy.
2001.http://www-nlpir.nist.gov/projects/duc/pubs/2001papers/isi hovy duc.pdf.John Burger, Claire Cardie, and et al 2001.
Issues,Tasks and Program Structures to Roadmap Researchin Question & Answering (Q&A) http://www-nlpir.nist.gov/projrcts/duc/roadmpping.html.NTCIR3 Workshop publication Home Page.
2001.http://research.nii.ac.jp/ntcir/workshop/OnlineProceedings3/index.html.NTCIR4 Workshop Home Page.
2003.http://research.nii.ac.jp/ntcir/workshop/work-en.html.Jun?ichi Fukumoto and Tsuneaki Kato.
2001.
AnOverview of Question and Answering Challenge(QAC) of the next NTCIR Workshop.
The Second NT-CIR Workshop Meeting.Jun?ichi Fukumoto, Tsuneaki Kato and Fumito Masui.2002.
Question Answering Challenge(QAC-1) Ques-tion answering evaluation at NTCIR workshop 3 NT-CIR workshop 3 Meeting Overview, pp.
77 - 86.Jun?ichi Fukumoto, Tsuneaki Kato and Fumito Masui.2003.
Question Answering Challenge(QAC-1) AnEvaluation of question answering tasks at the NTCIRworkshop 3 AAAI 2003 Spring Symposium New Di-rections in Question Answering, pp.
122-133.Inderjeet Mani, David House, and et al 1998.
TheTIPSER SUMMAC text summarization evaluation fi-nal report.
Technical Report MTR98W0000138, TheMITRE Corporation.Ellen M. Voorhees and Dawn M. Tice.
2000.
Building aQuestion Answering Test Collection the Proceedingsof the 23rd Annual International ACM SIGIR Confer-ence on Research and Development in Information Re-trieval, pp.
200 - 207.TREC Home Page.
2003.http://trec.nist.gov/.Text Summarization Challenge Home Page.
2003.http://lr-www.pi.titech.ac.jp/tsc/index-en.html.
