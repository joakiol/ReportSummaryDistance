Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 497?506,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsImproving pairwise coreference models throughfeature space hierarchy learningEmmanuel LassalleAlpage Project-teamINRIA & Univ.
Paris DiderotSorbonne Paris Cite?, F-75205 Parisemmanuel.lassalle@ens-lyon.orgPascal DenisMagnet ProjectINRIA Lille - Nord EuropeAvenue Helo?
?se, 59650 Villeneuve d?Ascqpascal.denis@inria.frAbstractThis paper proposes a new method forsignificantly improving the performanceof pairwise coreference models.
Given aset of indicators, our method learns howto best separate types of mention pairsinto equivalence classes for which we con-struct distinct classification models.
In ef-fect, our approach finds an optimal fea-ture space (derived from a base feature setand indicator set) for discriminating coref-erential mention pairs.
Although our ap-proach explores a very large space of pos-sible feature spaces, it remains tractableby exploiting the structure of the hierar-chies built from the indicators.
Our exper-iments on the CoNLL-2012 Shared TaskEnglish datasets (gold mentions) indicatethat our method is robust relative to dif-ferent clustering strategies and evaluationmetrics, showing large and consistent im-provements over a single pairwise modelusing the same base features.
Our bestsystem obtains a competitive 67.2 of aver-age F1 over MUC, B3, and CEAF which,despite its simplicity, places it above themean score of other systems on thesedatasets.1 IntroductionCoreference resolution is the problem of partition-ing a sequence of noun phrases (or mentions), asthey occur in a natural language text, into a set ofreferential entities.
A common approach to thisproblem is to separate it into two modules: onthe one hand, one defines a model for evaluatingcoreference links, in general a discriminative clas-sifier that detects coreferential mention pairs.
Onthe other hand, one designs a method for group-ing the detected links into a coherent global out-put (i.e.
a partition over the set of entity men-tions).
This second step is typically achievedusing greedy heuristics (McCarthy and Lehnert,1995; Soon et al, 2001; Ng and Cardie, 2002;Bengston and Roth, 2008), although more so-phisticated clustering approaches have been used,too, such as cutting graph methods (Nicolae andNicolae, 2006; Cai and Strube, 2010) and IntegerLinear Programming (ILP) formulations (Klenner,2007; Denis and Baldridge, 2009).
Despite itssimplicity, this two-step strategy remains competi-tive even when compared to more complex modelsutilizing a global loss (Bengston and Roth, 2008).In this kind of architecture, the performance ofthe entire coreference system strongly depends onthe quality of the local pairwise classifier.1 Con-sequently, a lot of research effort on coreferenceresolution has focused on trying to boost the per-formance of the pairwise classifier.
Numerousstudies are concerned with feature extraction, typ-ically trying to enrich the classifier with morelinguistic knowledge and/or more world knowl-edge (Ng and Cardie, 2002; Kehler et al, 2004;Ponzetto and Strube, 2006; Bengston and Roth,2008; Versley et al, 2008; Uryupina et al, 2011).A second line of work explores the use of dis-tinct local models for different types of mentions,specifically for different types of anaphoric men-tions based on their grammatical categories (suchas pronouns, proper names, definite descriptions)(Morton, 2000; Ng, 2005; Denis and Baldridge,2008).2 An important justification for such spe-1There are however no theoretical guarantees that improv-ing pair classification will always result in overall improve-ments if the two modules are optimized independently.2Sometimes, distinct sample selections are also adopted497cialized models is (psycho-)linguistic and comesfrom theoretical findings based on salience or ac-cessibility (Ariel, 1988).
It is worth noting that,from a machine learning point of view, this is re-lated to feature extraction in that both approachesin effect recast the pairwise classification problemin higher dimensional feature spaces.In this paper, we claim that mention pairsshould not be processed by a single classifier, andinstead should be handled through specific mod-els.
But we are furthermore interested in learninghow to construct and select such differential mod-els.
Our argument is therefore based on statisti-cal considerations, rather than on purely linguis-tic ones3.
The main question we raise is, givena set of indicators (such as grammatical types,distance between two mentions, or named entitytypes), how to best partition the pool of mentionpair examples in order to best discriminate coref-erential pairs from non coreferential ones.
In ef-fect, we want to learn the ?best?
subspaces for ourdifferent models: that is, subspaces that are neithertoo coarse (i.e., unlikely to separate the data well)nor too specific (i.e., prone to data sparseness andnoise).
We will see that this is also equivalent toselecting a single large adequate feature space byusing the data.Our approach generalizes earlier approaches inimportant ways.
For one thing, the definitionof the different models is no longer restricted togrammatical typing (our model allows for variousother types of indicators) or to the sole typing ofthe anaphoric mention (our models can also bespecific to a particular type antecedent or to thetwo types of the mention pair).
More importantly,we propose an original method for learning thebest set of models that can be built from a givenset of indicators and a training set.
These modelsare organized in a hierarchy, wherein each leaf cor-responds to a mutually disjoint subset of mentionpair examples and the classifier that can be trainedfrom it.
Our models are trained using the OnlinePassive-Aggressive algorithm or PA (Crammer etal., 2006), a large margin version of the percep-tron.
Our method is exact in that it explores the fullspace of hierarchies (of size at least 22n) definableon an indicator sequence, while remaining scal-able by exploiting the particular structure of theseduring the training of the distinct local models (Ng andCardie, 2002; Uryupina, 2004).3However it should be underlined that the statistical view-point is complementary to the linguistic work.hierarchies with dynamic programming.
This ap-proach also performs well, and it largely outper-forms the single model.
As will be shown basedon a variety of experiments on the CoNLL-2012Shared Task English datasets, these improvementsare consistent across different evaluation metricsand for the most part independent of the clusteringdecoder that was used.The rest of this paper is organized as follows.Section 2 discusses the underlying statistical hy-potheses of the standard pairwise model and de-fines a simple alternative framework that uses asimple separation of mention pairs based on gram-matical types.
Next, in section 3, we generalize themethod by introducing indicator hierarchies andexplain how to learn the best models associatedwith them.
Section 4 provides a brief system de-scription and Section 5 evaluates the various mod-els on CoNLL-2012 English datasets.2 Modeling pairsPairwise models basically employ one local clas-sifier to decide whether two mentions are corefer-ential or not.
When using machine learning tech-niques, this involves certain assumptions about thestatistical behavior of mention pairs.2.1 Statistical assumptionsLet us adopt a probabilistic point of view to de-scribe the prototype of pairwise models.
Givena document, the number of mentions is fixed andeach pair of mentions follows a certain distribution(that we partly observe in a feature space).
The ba-sic idea of pairwise models is to consider mentionpairs independently from each other (that is why adecoder is necessary to enforce transitivity).If we use a single classifier to process allpairs, then they are supposed to be identically dis-tributed.
We claim that pairs should not be pro-cessed by a single classifier because they are notidentically distributed (or a least the distribution istoo complex for the classifier); rather, we shouldseparate different ?types?
on pairs and create aspecific model for each of them.Separating different kinds of pairs and handlingthem with different specific models can lead tomore accurate global models.
For instance, somecoreference resolution systems process differentkinds of anaphors separately, which suggests forexample that pairs containing an anaphoric pro-noun behave differently from pairs with non-498pronominal anaphors.
One could rely on a rich setof features to capture complex distributions, buthere we actually have a rather limited set of ele-mentary features (see section 4) and, for instance,using products of features must be done carefullyto avoid introducing noise in the model.
Insteadof imposing heuristic product of features, we willshow that a clever separation of instances leads tosignificant improvements of the pairwise model.2.2 Feature spaces2.2.1 DefinitionsWe first introduce the problem more formally.
Ev-ery pair of mentions mi and mj is modeled by arandom variable:Pij : ?
?
X ?Y?
7?
(xij(?
), yij(?
))where ?
classically represents randomness, X isthe space of objects (?mention pairs?)
that is notdirectly observable and yij(?)
?
Y = {+1,?1}are the labels indicating whether mi and mj arecoreferential or not.
To lighten the notations, wewill not always write the index ij.
Now we definea mapping:?F : X ?
Fx 7?
xthat casts pairs into a feature space F throughwhich we observe them.
For us, F is simply avector space over R (in our case many features areBoolean; they are cast into R as 0 and 1).For technical coherence, we assume that?F1(x(?))
and ?F2(x(?))
have the same valueswhen projected on the feature space F1 ?
F2:it means that common features from two featurespaces have the same values.From this formal point of view, the task ofcoreference resolution consists in fixing ?F , ob-serving labeled samples {(?F (x), y)t}t?TrainSetand, given partially observed new variables{(?F (x))t}t?TestSet, recovering the correspond-ing values of y.2.2.2 Formalizing the statistical assumptionsWe claimed before that all mention pairs seemednot to be identically distributed since, for exam-ple, pronouns do not behave like nominals.
Wecan formulate this more rigorously: since the ob-ject space X is not directly observable, we do notknow its complexity.
In particular, when using amapping to a too small feature space, the classifiercannot capture the distribution very well: the datais too noisy.Now if we say that pronominal anaphora do notbehave like other anaphora, we distinguish twokinds of pair i.e.
we state that the distribution ofpairs in X is a mixture of two distributions, andwe deterministically separate pairs to their specificdistribution part.
In this way, we may separatepositive and negative pairs more easily if we casteach kind of pair into a specific feature space.
Letus call these feature spaces F1 and F2.
We can ei-ther create two independent classifiers on F1 andF2 to process each kind of pair or define a singlemodel on a larger feature space F = F1 ?
F2.
Ifthe model is linear (which is our case), these ap-proaches happen to be equivalent.So we can actually assume that the random vari-ables Pij are identically distributed, but drawnfrom a complex mixture.
A new issue arises: weneed to find a mapping ?F that renders the bestview on the distribution of the data.From a theoretical viewpoint, the higher the di-mension of the feature space (imagine taking thedirect sum of all feature spaces), the more we getdetails on the distribution of mention pairs and themore we can expect to separate positives and neg-atives accurately.
In practice, we have to copewith data sparsity: there will not be enough datato properly train a linear model on such a space.Finally, we seek a feature space situated betweenthe two extremes of a space that is too big (sparse-ness) or too small (noisy data).
The core of thiswork is to define a general method for choosingthe most adequate space F among a huge num-ber of possibilities when we do not know a prioriwhich is the best.2.2.3 Linear modelsIn this work, we try to linearly separate pos-itive and negative instances in the large spaceF with the Online Passive-Aggressive (PA) algo-rithm (Crammer et al, 2006): the model learns aparameter vector w that defines a hyperplane thatcuts the space into two parts.
The predicted classof a pair x with feature vector ?F (x) is given by:CF (x) := sign(wT ?
?F (x))Linearity implies an equivalence between: (i)separating instances of two types, t1 and t2, in two499independent models with respective feature spacesF1 and F2 and parameters w1 and w2, and (ii) asingle model on F1?F2.
To see why, let us definethe map:?F1?F2(x) :=?????
(?F1(x)T 0)T if x typed t1(0 ?F2(x)T)T if x typed t2and the parameter vector w =(w1w2)?
F1 ?F2.
Then we have:CF1?F2(x) ={CF1(x) if x typed t1CF2(x) if x typed t2Now we check that the same property applieswhen the PA fits its parameter w. For each newinstance of the training set, the weight is updatedaccording to the following rule4:wt+1 = arg minw?F12 ?w ?wt?2 s.t.
l(w; (xt, yt)) = 0where l(w; (xt, yt)) = min(0, 1?yt(w?
?F (xt))),so that when F = F1 ?
F2, the minimum if x istyped t1 is wt+1 =(w1t+1w2t)and if x is typedt2 is wt+1 =(w1tw2t+1)where the wit+1 corre-spond to the updates in space Fi independentlyfrom the rest.
This result can be extended easilyto the case of n feature spaces.
Thus, with a deter-ministic separation of the data, a large model canbe learned using smaller independent models.2.3 An example: separation by gramtypeTo motivate our approach, we first introduce asimple separation of mention pairs which cre-ates 9 models obtained by considering all possi-ble pairs of grammatical types {nominal, name,pronoun} for both mentions in the pair (a simi-lar fine-grained separation can be found in (Chenet al, 2011)).
This is equivalent to using 9 differ-ent feature spacesF1, .
.
.
,F9 to capture the globaldistribution of pairs.
With the PA, this is also a sin-gle model with feature space F = F1 ?
?
?
?
?
F9.We will call it the GRAMTYPE model.As we will see in Section 5, these separatedmodels significantly outperform a single model4The parameter is updated to obtain a margin of a least 1.It does not change if the instance is already correctly classi-fied with such margin.that uses the same base feature set.
But we wouldlike to define a method that adapts a feature spaceto the data by choosing the most adequate separa-tion of pairs.3 Hierarchizing feature spacesIn this section, we have to keep in mind that sep-arating the pairs in different models is the sameas building a large feature space in which the pa-rameter w can be learned by parts in independentsubspaces.3.1 Indicators on pairsFor establishing a structure on feature spaces, weuse indicators which are deterministic functionson mention pairs with a small number of outputs.Indicators classify pairs in predefined categories inone-to-one correspondence with independent fea-ture spaces.
We can reuse some features of the sys-tem as indicators, e.g.
the grammatical or namedentity types.
We can also employ functions thatare not used as features, e.g.
the approximate po-sition of one of the mentions in the text.The small number of outputs of an indica-tor is required for practical reasons: if a cate-gory of pairs is too refined, the associated fea-ture space will suffer from data sparsity.
Accord-ingly, distance-based indicators must be approxi-mated by coarse histograms.
In our experimentsthe outputs never exceeded a dozen values.
Oneway to reduce the output span of an indicator isto binarize it like binarizing a tree (many possiblebinarizations).
This operation produces a hierar-chy of indicators which is exactly the structure weexploit in what follows.3.2 Hierarchies for separating pairsWe define hierarchies as combinations of indi-cators creating finer categories of mention pairs:given a finite sequence of indicators, a mentionpair is classified by applying the indicators suc-cessively, each time refining a category into sub-categories, just like in a decision tree (each nodehaving the same number of children as the numberof outputs of its indicator).
We allow the classifi-cation to stop before applying the last indicator,but the behavior must be the same for all the in-stances.
So a hierarchy is basically a sub-tree ofthe complete decision tree that contains copies ofthe same indicator at each level.If all the leaves of the decision tree have the500same depth, this corresponds to taking the Carte-sian product of outputs of all indicators for in-dexing the categories.
In that case, we refer toproduct-hierarchies.
The GRAMTYPE model canbe seen as a two level product-hierarchy (figure 1).Figure 1: GRAMTYPE seen as a product-hierarchyProduct-hierarchies will be the starting point ofour method to find a feature space that fits the data.Now choosing a relevant sequence of indicatorsshould be achieved through linguistic intuitionsand theoretical work (gramtype separation is oneof them).
The system will find by itself the bestusage of the indicators when optimizing the hier-archy.
The sequence is a parameter of the model.3.3 Relation with feature spacesLike we did for the GRAMTYPE model, we asso-ciate a feature space Fi to each leaf of a hierarchy.Likewise, the sum F = ?iFi defines a large fea-ture space.
The corresponding parameter w of themodel can be obtained by learning the wi in Fi.Given a sequence of indicators, the number ofdifferent hierarchies we can define is equal to thenumber of sub-trees of the complete decision tree(each non-leaf node having all its children).
Theminimal case is when all indicators are Boolean.The number of full binary trees of height at mostn can be computed by the following recursion:T (1) = 1 and T (n + 1) = 1 + T (n)2.
SoT (n) ?
22n : even with small values of n, thenumber of different hierarchies (or large featurespaces) definable with a sequence of indicators isgigantic (e.g.
T (10) ?
3.8.1090).Among all the possibilities for a large featurespace, many are irrelevant because for them thedata is too sparse or too noisy in some subspaces.We need a general method for finding an ade-quate space without enumerating and testing eachof them.3.4 Optimizing hierarchiesLet us assume now that the sequence of indicatorsis fixed, and let n be its length.
To find the bestfeature space among a very high number of pos-sibilities, we need a criterion we can apply with-out too much additional computation.
For that weonly evaluate the feature space locally on pairs,i.e.
without applying a decoder on the output.
Weemploy 3 measures on pairwise classification re-sults: precision, recall and F1-score.
Now select-ing the best space for one of these measures canbe achieved by using dynamic programming tech-niques.
In the rest of the paper, we will optimizethe F1-score.Training the hierarchy Starting from theproduct-hierarchy, we associate a classifier and itsproper feature space to each node of the tree5.
Theclassifiers are then trained as follows: for each in-stance there is a unique path from the root to a leafof the complete tree.
Each classifier situated onthe path is updated with this instance.
The numberof iterations of the Passive-Aggressive is fixed.Computing scores After training, we test all theclassifiers on another set of pairs6.
Again, a classi-fier is tested on an instance only if it is situated onthe path from the root to the leaf associated withthe instance.
We obtain TP/FP/FN numbers7 onpair classifications that are sufficient to computethe F1-score.
As for training, the data on which aclassifier at a given node is evaluated is the sameas the union of all data used to evaluate the clas-sifiers corresponding to the children of this node.Thus we are able to compare the scores obtainedat a node to the ?union of the scores?
obtained atits children.Cutting down the hierarchy For the momentwe have a complete tree with a classifier at eachnode.
We use a dynamic programming techniqueto compute the best hierarchy by cutting this treeand only keeping classifiers situated at the leaf.The algorithm assembles the best local models (orfeature spaces) together to create larger models.
Itgoes from the leaves to the root and cuts the sub-tree starting at a node whenever it does not pro-5In the experiments, the classifiers use a copy of a samefeature space, but not the same data, which corresponds tocrossing the features with the categories of the decision tree.6The training set is cut into two parts, for training andtesting the hierarchy.
We used 10-fold cross-validation in ourexperiments.7True positives, false positives and false negatives.501vide a better score than the node itself, or on thecontrary propagates the score of the sub-tree whenthere is an improvement.
The details are given inalgorithm 1.list?
list of nodes given by a breadth-first1search for node in reversed list doif node.children 6= ?
then2if sum-score(node.children) >3node.score thennode.TP/FP/FN?4sum-num(node.children)else5node.children?
?6end7end8end9Algorithm 1: Cutting down a hierarchyLet us briefly discuss the correctness and com-plexity of the algorithm.
Each node is seen twotimes so the time complexity is linear in the num-ber of nodes which is at least O(2n).
However,only nodes that have encountered at least onetraining instance are useful and there are O(n ?k) such nodes (where k the size of the trainingset).
So we can optimize the algorithm to runin time O(n ?
k)8.
If we scan the list obtainedby breadth-first search backwards, we are ensuredthat every node will be processed after its chil-dren.
(node.children) is the set of children ofnode, and (node.score) its score.
sum-num pro-vides TP/FP/FN by simply adding those of thechildren and sum-score computes the score basedon these new TP/FP/FN numbers.
(line 6) cuts thechildren of a node when they are not used in thebest score.
The algorithm thus propagates the bestscores from the leaves to the root which finallygives a single score corresponding to the best hi-erarchy.
Only the leaves used to compute the bestscore are kept, and they define the best hierarchy.Relation between cutting and the global featurespace We can see the operation of cutting as re-placing a group of subspaces by a single subspacein the sum (see figure 2).
So cutting down theproduct-hierarchy amounts to reducing the globalinitial feature space in an optimal way.8In our experiments, cutting down the hierarchy wasachieved very quickly, and the total training time was aboutfive times longer than with a single model.Figure 2: Cutting down the hierarchy reduces thefeature spaceTo sum up, the whole procedure is equivalent totraining more than O(2n) perceptrons simultane-ously and selecting the best performing.4 System descriptionOur system consists in the pairwise model ob-tained by cutting a hierarchy (the PA with selectedfeature space) and using a greedy decoder to cre-ate clusters from the output.
It is parametrized bythe choice of the initial sequence of indicators.4.1 The base featuresWe used classical features that can be found indetails in (Bengston and Roth, 2008) and (Rah-man and Ng, 2011): grammatical type and sub-type of mentions, string match and substring, ap-position and copula, distance (number of sepa-rating mentions/sentences/words), gender/numbermatch, synonymy/hypernym and animacy (usingWordNet), family name (based on lists), namedentity types, syntactic features (gold parse) andanaphoricity detection.4.2 IndicatorsAs indicators we used: left and right grammati-cal types and subtypes, entity types, a boolean in-dicating if the mentions are in the same sentence,and a very coarse histogram of distance in terms ofsentences.
We systematically included right gram-type and left gramtype in the sequences and addedother indicators, producing sequences of differentlengths.
The parameter was optimized by docu-ment categories using a development set after de-coding the output of the pairwise model.4.3 DecodersWe tested 3 classical greedy link selection strate-gies that form clusters from the classifier decision:Closest-First (merge mentions with their closestcoreferent mention on the left) (Soon et al, 2001),502Best-first (merge mentions with the mention onthe left having the highest positive score) (Ngand Cardie, 2002; Bengston and Roth, 2008), andAggressive-Merge (transitive closure on positivepairs) (McCarthy and Lehnert, 1995).
Each ofthese decoders is typically (although not always)used in tandem with a specific sampling selec-tion at training.
Thus, Closest-First for instance isused in combination with a sample selection thatgenerates training instances only for the mentionsthat occur between the closest antecedent and theanaphor (Soon et al, 2001).P R F1SINGLE MODEL 22.28 63.50 32.99RIGHT-TYPE 29.31 45.23 35.58GRAMTYPE 39.12 45.83 42.21BEST HIERARCHY 45.27 51.98 48.40Table 1: Pairwise scores on CoNLL-2012 test.5 Experiments5.1 DataWe evaluated the system on the English part of thecorpus provided in the CoNLL-2012 Shared Task(Pradhan et al, 2012), referred to as CoNLL-2012here.
The corpus contains 7 categories of doc-uments (over 2K documents, 1.3M words).
Weused the official train/dev/test data sets.
We evalu-ated our system in the closed mode which requiresthat only provided data is used.5.2 SettingsOur baselines are a SINGLE MODEL, the GRAM-TYPE model (section 2) and a RIGHT-TYPEmodel, defined as the first level of the gramtypeproduct hierarchy (i.e.
grammatical type of theanaphora (Morton, 2000)), with each greedy de-coder and also the original sampling with a singlemodel associated with those decoders.The hierarchies were trained with 10-fold cross-validation on the training set (the hierarchies arecut after cumulating the scores obtained by cross-validation) and their parameters are optimized bydocument category on the development set: thesequence of indicators obtaining the best averagescore after decoding was selected as parameter forthe category.
The obtained hierarchy is referred toas the BEST HIERARCHY in the results.
We fixedthe number of iterations for the PA for all models.In our experiments, we consider only the goldmentions.
This is a rather idealized setting but ourfocus is on comparing various pairwise local mod-els rather than on building a full coreference reso-lution system.
Also, we wanted to avoid having toconsider too many parameters in our experiments.5.3 Evaluation metricsWe use the three metrics that are most commonlyused9, namely:MUC (Vilain et al, 1995) computes for eachtrue entity cluster the number of system clustersthat are needed to cover it.
Precision is this quan-tity divided by the true cluster size minus one.
Re-call is obtained by reversing true and predicatedclusters.
F1 is the harmonic mean.B3 (Bagga and Baldwin, 1998) computes recalland precision scores for each mention, based onthe intersection between the system/true clustersfor that mention.
Precision is the ratio of the in-tersection and the true cluster sizes, while recall isthe ratio of the intersection to the system clustersizes.
Global recall, precision, and F1 scores areobtained by averaging over the mention scores.CEAF (Luo, 2005) scores are obtained by com-puting the best one-to-one mapping between thesystem/true partitions, which is equivalent to find-ing the best optimal alignment in the bipartitegraph formed out of these partitions.
We use the?4 similarity function from (Luo, 2005).These metrics were recently used in the CoNLL-2011 and -2012 Shared Tasks.
In addition, thesecampaigns use an unweighted average over the F1scores given by the three metrics.
Following com-mon practice, we use micro-averaging when re-porting our scores for entire datasets.5.4 ResultsThe results obtained by the system are reported intable 2.
The original sampling for the single modelassociated to Closest-First and Best-First decoderare referred to as SOON and NGCARDIE.The P/R/F1 pairwise scores before decoding aregiven in table 1.
BEST HIERARCHY obtains astrong improvement in F1 (+15), a better precisionand a less significant diminution of recall com-pared to GRAMTYPE and RIGHT-TYPE.9BLANC metric (Recasens and Hovy, 2011) results arenot reported since they are not used to compute the CoNLL-2012 global score.
However we can mention that in our ex-periments, using hierarchies had a positive effect similar towhat was observed on B3 and CEAF.503MUC B3 CEAFClosest-First P R F1 P R F1 P R F1 MeanSOON 79.49 93.72 86.02 26.23 89.43 40.56 49.74 19.92 28.44 51.67SINGLE MODEL 78.95 75.15 77.0 51.88 68.42 59.01 37.79 43.89 40.61 58.87RIGHT-TYPE 79.36 67.57 72.99 69.43 56.78 62.47 41.17 61.66 49.37 61.61GRAMTYPE 80.5 71.12 75.52 66.39 61.04 63.6 43.11 59.93 50.15 63.09BEST HIERARCHY 83.23 73.72 78.19 73.5 67.09 70.15 47.3 60.89 53.24 67.19MUC B3 CEAFBest-First P R F1 P R F1 P R F1 MeanNGCARDIE 81.02 93.82 86.95 23.33 93.92 37.37 40.31 18.97 25.8 50.04SINGLE MODEL 79.22 73.75 76.39 40.93 75.48 53.08 30.52 37.59 33.69 54.39RIGHT-TYPE 77.13 65.09 70.60 48.11 66.21 55.73 31.07 47.30 37.50 54.61GRAMTYPE 77.21 65.89 71.1 49.77 67.19 57.18 32.08 47.83 38.41 55.56BEST HIERARCHY 78.11 69.82 73.73 53.62 70.86 61.05 35.04 46.67 40.03 58.27MUC B3 CEAFAggressive-Merge P R F1 P R F1 P R F1 MeanSINGLE MODEL 83.15 88.65 85.81 35.67 88.18 50.79 36.3 28.27 31.78 56.13RIGHT-TYPE 83.48 89.79 86.52 36.82 88.08 51.93 45.30 33.84 38.74 59.07GRAMTYPE 83.12 84.27 83.69 44.73 81.58 57.78 45.02 42.94 43.95 61.81BEST HIERARCHY 83.26 85.2 84.22 45.65 82.48 58.77 46.28 43.13 44.65 62.55Table 2: CoNLL-2012 test (gold mentions): Closest-First, Best-First and Aggressive-Merge decoders.Despite the use of greedy decoders, we observea large positive effect of pair separation in thepairwise models on the outputs.
On the meanscore, the use of distinct models versus a sin-gle model yields F1 increases from 6.4 up to 8.3depending on the decoder.
Irrespective of thedecoder being used, GRAMTYPE always outper-forms RIGHT-TYPE and single model and is al-ways outperformed by BEST HIERARCHY model.Interestingly, we see that the increment in pair-wise and global score are not proportional: forinstance, the strong improvement of F1 betweenRIGHT-TYPE and GRAMTYPE results in a smallamelioration of the global score.Depending on the document category, we foundsome variations as to which hierarchy was learnedin each setting, but we noticed that parametersstarting with right and left gramtypes often pro-duced quite good hierarchies: for instance rightgramtype ?
left gramtype ?
same sentence ?right named entity type.We observed that product-hierarchies did notperformed well without cutting (especially whenusing longer sequences of indicators, because ofdata sparsity) and could obtain scores lower thanthe single model.
Hopefully, after cutting them theresults always became better as the resulting hier-archy was more balanced.Looking at the different metrics, we notice thatoverall, pair separation improves B3 and CEAF(but not always MUC) after decoding the output:GRAMTYPE provides a better mean score than thesingle model, and BEST HIERARCHY gives thehighest B3, CEAF and mean score.The best classifier-decoder combination reachesa score of 67.19, which would place it above themean score (66.41) of the systems that took partin the CoNLL-2012 Shared Task (gold mentionstrack).
Except for the first at 77.22, the bestperforming systems have a score around 68-69.Considering the simple decoding strategy we em-ployed, our current system sets up a strong base-line.6 Conclusion and perspectivesIn this paper, we described a method for select-ing a feature space among a very large number ofchoices by using linearity and by combining indi-cators to separate the instances.
We employed dy-namic programming on hierarchies of indicatorsto compute the feature space providing the bestpairwise classifications efficiently.
We applied this504method to optimize the pairwise model of a coref-erence resolution system.
Using different kindsof greedy decoders, we showed a significant im-provement of the system.Our approach is flexible in that we can use a va-riety of indicators.
In the future we will apply thehierarchies on finer feature spaces to make moreaccurate optimizations.
Observing that the gen-eral method of cutting down hierarchies is not re-stricted to modeling mention pairs, but can be ap-plied to problems having Boolean aspects, we aimat employing hierarchies to address other tasks incomputational linguistics (e.g.
anaphoricity detec-tion or discourse and temporal relation classifica-tion wherein position information may help sepa-rating the data).In this work, we have only considered standard,heuristic linking strategies like Closest-First.
So,a natural extension of this work is to combine ourmethod for learning pairwise models with moresophisticated decoding strategies (like Bestcut orusing ILP).
Then we can test the impact of hierar-chies with more realistic settings.Finally, the method for cutting hierarchiesshould be compared to more general but similarmethods, for instance polynomial kernels for SVMand tree-based methods (Hastie et al, 2001).
Wealso plan to extend our method by breaking thesymmetry of our hierarchies.
Instead of cuttingproduct-hierarchies, we will employ usual tech-niques to build decision trees10 and apply our cut-ting method on their structure.
The objective istwofold: first, we will get rid of the sequence ofindicators as parameter.
Second, we will avoidfragmentation or overfitting (which can arise withclassification trees) by deriving an optimal largemargin linear model from the tree structure.AcknowledgmentsWe thank the ACL 2013 anonymous reviewers fortheir valuable comments.ReferencesM.
Ariel.
1988.
Referring and accessibility.
Journalof Linguistics, pages 65?87.A.
Bagga and B. Baldwin.
1998.
Algorithms forscoring coreference chains.
In Proceedings ofLREC 1998, pages 563?566.10(Bansal and Klein, 2012) show good performances of de-cision trees on coreference resolution.Mohit Bansal and Dan Klein.
2012.
Coreference se-mantics from web features.
In Proceedings of the50th Annual Meeting of the Association for Compu-tational Linguistics: Long Papers-Volume 1, pages389?398.
Association for Computational Linguis-tics.Eric Bengston and Dan Roth.
2008.
Understandingthe value of features for coreference resolution.
InProceedings of EMNLP 2008, pages 294?303, Hon-olulu, Hawaii.Jie Cai and Michael Strube.
2010.
End-to-end coref-erence resolution via hypergraph partitioning.
InCOLING, pages 143?151.Bin Chen, Jian Su, Sinno Jialin Pan, and Chew LimTan.
2011.
A unified event coreference resolu-tion by integrating multiple resolvers.
In Proceed-ings of 5th International Joint Conference on Nat-ural Language Processing, pages 102?110, ChiangMai, Thailand, November.
Asian Federation of Nat-ural Language Processing.Koby Crammer, Ofer Dekel, Joseph Keshet, ShaiShalev-Shwartz, and Yoram Singer.
2006.
Onlinepassive-aggressive algorithms.
Journal of MachineLearning Research, 7:551?585.Pascal Denis and Jason Baldridge.
2008.
Specializedmodels and ranking for coreference resolution.
InProceedings of EMNLP 2008, pages 660?669, Hon-olulu, Hawaii.Pascal Denis and Jason Baldridge.
2009.
Global jointmodels for coreference resolution and named entityclassification.
Procesamiento del Lenguaje Natural,43.Trevor Hastie, Robert Tibshirani, and J. H. Friedman.2001.
The elements of statistical learning: datamining, inference, and prediction: with 200 full-color illustrations.
New York: Springer-Verlag.A.
Kehler, D. Appelt, L. Taylor, and A. Simma.
2004.The (non)utility of predicate-argument frequenciesfor pronoun interpretation.
In Proceedings of HLT-NAACL 2004.M.
Klenner.
2007.
Enforcing coherence on corefer-ence sets.
In Proceedings of RANLP 2007.X.
Luo.
2005.
On coreference resolution performancemetrics.
In Proceedings of HLT-NAACL 2005, pages25?32.J.
F. McCarthy and W. G. Lehnert.
1995.
Using de-cision trees for coreference resolution.
In IJCAI,pages 1050?1055.T.
Morton.
2000.
Coreference for NLP applications.In Proceedings of ACL 2000, Hong Kong.V.
Ng and C. Cardie.
2002.
Improving machine learn-ing approaches to coreference resolution.
In Pro-ceedings of ACL 2002, pages 104?111.505V.
Ng.
2005.
Supervised ranking for pronoun resolu-tion: Some recent improvements.
In Proceedings ofAAAI 2005.Cristina Nicolae and Gabriel Nicolae.
2006.
Best-cut: A graph algorithm for coreference resolution.In EMNLP, pages 275?283.Simone Paolo Ponzetto and Michael Strube.
2006.Exploiting semantic role labeling, WordNet andWikipedia for coreference resolution.
In Proceed-ings of the HLT 2006, pages 192?199, New YorkCity, N.Y.Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,Olga Uryupina, and Yuchen Zhang.
2012.
Conll-2012 shared task: Modeling multilingual unre-stricted coreference in ontonotes.
In Joint Confer-ence on EMNLP and CoNLL - Shared Task, pages1?40, Jeju Island, Korea, July.
Association for Com-putational Linguistics.Altaf Rahman and Vincent Ng.
2011.
Narrowing themodeling gap: a cluster-ranking approach to coref-erence resolution.
J. Artif.
Int.
Res., 40(1):469?521.Recasens and Hovy.
2011.
Blanc: Implementing therand index for coreference evaluation.
Natural Lan-guage Engineering, 17:485?510, 9.W.
M. Soon, H. T. Ng, and D. Lim.
2001.
Amachine learning approach to coreference resolu-tion of noun phrases.
Computational Linguistics,27(4):521?544.Olga Uryupina, Massimo Poesio, Claudio Giuliano,and Kateryna Tymoshenko.
2011.
Disambiguationand filtering methods in using web knowledge forcoreference resolution.
In FLAIRS Conference.O.
Uryupina.
2004.
Linguistically motivated sampleselection for coreference resolution.
In Proceedingsof DAARC 2004, Furnas.Yannick Versley, Alessandro Moschitti, Massimo Poe-sio, and Xiaofeng Yang.
2008.
Coreference systemsbased on kernels methods.
In COLING, pages 961?968.M.
Vilain, J. Burger, J. Aberdeen, D. Connolly, andL.
Hirschman.
1995.
A model-theoretic coreferencescoring scheme.
In Proceedings fo the 6th MessageUnderstanding Conference (MUC-6), pages 45?52,San Mateo, CA.
Morgan Kaufmann.506
