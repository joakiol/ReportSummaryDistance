Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics ?
Student Research Workshop, pages 65?71,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsSingleton Detection using Word Embeddings and Neural NetworksHessel HaagsmaUniversity of GroningenThe Netherlandshessel.haagsma@rug.nlAbstractSingleton (or non-coreferential) mentionsare a problem for coreference resolutionsystems, and identifying singletons be-fore mentions are linked improves res-olution performance.
Here, a singletondetection system based on word embed-dings and neural networks is presented,which achieves state-of-the-art perfor-mance (79.6% accuracy) on the CoNLL-2012 shared task development set.
Ex-trinsic evaluation with the Stanford andBerkeley coreference resolution systemsshows significant improvement for thefirst, but not for the latter.
The results showthe potential of using neural networks andword embeddings for improving both sin-gleton detection and coreference resolu-tion.1 BackgroundCoreference resolution is the task of identifyingand linking all expressions in language which re-fer to the same entity.
It is an essential part of bothhuman language understanding and natural lan-guage processing.
In NLP, coreference resolutionis often approached as a two-part problem: findingall referential expressions (a.k.a.
?mentions?)
in atext, and clustering those mentions that refer to thesame entity.So, in Example (1), the first part consists offinding My internet, It, and it.
The second partthen consists of clustering My internet and it to-gether (as indicated by the indices), and not clus-tering It with anything (as indicated by thex).
(1) [My internet]1wasn?t working properly.
[It]xseems that [it]1is fixed now, however.This example also serves to showcase the diffi-culty of the clustering step, since it is challeng-ing to decide between clustering My internet withit, clustering My internet with It, or clustering allthree mentions together.
However, note that in thissentence It is non-referential, i.e.
it does not referto any real world entity.
This means that this men-tion could already be filtered out after the first step,making the clustering a lot easier.In this paper, we improve mention filtering forcoreference resolution by building a system basedon word embeddings and neural networks, andevaluate performance both as a stand-alone taskand extrinsically with coreference resolution sys-tems.1.1 Previous WorkMention filtering is not a new task, and there existsa large body of previous work, ranging from therule-based non-referential it filtering of Paice andHusk (1987) to the machine learning approach tosingleton detection by de Marneffe et al (2015).Different mention filtering tasks have beentried: filtering out non-referential it (Boyd etal., 2005; Bergsma and Yarowsky, 2011), non-anaphoric NPs (Uryupina, 2003), non-antecedentNPs (Uryupina, 2009), discourse-new mentions(Ng and Cardie, 2002), and singletons, i.e.
non-coreferential mentions (de Marneffe et al, 2015).All these tasks can be done quite accurately, butsince they are only useful as part of an end-to-endcoreference resolution system, it is more interest-ing to look at what is most effective for improvingcoreference resolution performance.There is much to gain with improved mentionfiltering.
For example, the authors of one state-of-the-art coreference resolution system estimatethat non-referential mentions are the direct causeof 14.8% of their system?s error (Lee et al, 2013).The importance of mention detection and filtering65is further exemplified by the fact that several re-cent systems focus on integrating the processingof mentions and the clustering of mentions into asingle model or system (Ma et al, 2014; Peng etal., 2015; Wiseman et al, 2015).Other lessons regarding mention filtering andcoreference resolution performance come from Ngand Cardie (2002) and Byron and Gegg-Harrison(2004).
They find that the mentions filtered outby their systems are also the mentions which areleast problematic in the clustering phase.
As a re-sult, the gain in clustering precision is smaller thanexpected, and does not compensate for the recallloss.
They also find that high precision in mentionfiltering is more important than high recall.The state-of-the-art in mention filtering is thesystem described by de Marneffe et al (2015),who work on singleton detection.
De Marneffeet al used a logistic regression classifier withboth discourse-theoretically inspired semantic fea-tures and more superficial features (animacy, NE-type, POS, etc.)
to perform singleton detection.They achieve 56% recall and 90% precision on theCoNLL-2012 shared task data, which translates toa coreference resolution performance increase of0.5-2.0 percentage point in CoNLL F1-score.1.2 The Current ApproachIn this paper, a novel singleton detection systemwhich makes use of word embeddings and neuralnetworks is presented.
There are three main mo-tivations for choosing this approach, partly basedon lessons drawn from previous work.The first is that the coreference resolution sys-tems we evaluate with here do not make use ofembeddings.
Thus, using embeddings as an ad-ditional data source can aid in filtering out thosesingletons which are problematic for the cluster-ing system.
Word embeddings are chosen becausewe expect that the syntactic and semantic infor-mation contained in them should help the single-ton detection system to generalize over the train-ing data better.
For example, knowing that ?snow-ing?
is similar to ?raining?
makes it easier to clas-sify ?It?
in ?It is snowing?
as singleton, when only?It is raining?
occurs in the training data.Second, previous work indicated that precisionin filtering is more important than recall.
There-fore, a singleton detection system should not onlybe able to filter out singletons with high accuracy,but should also be able to vary the precision/recalltrade-off.
Here, the output is a class probability,which fulfils this requirement.Third, both Bergsma and Yarowsky (2011) andde Marneffe et al (2015) find that the contextwords around the mention are an important featurefor mention filtering.
Context tokens can easily beincluded in the current set-up, and by using wordembeddings generalization on these context wordsshould be improved.2 MethodsThe singleton detection system presented hereconsists of two main parts: a recursive autoen-coder and a multilayer perceptron.
The recur-sive autoencoder is used to create fixed-length rep-resentations for multi-word mentions, based onword embeddings.
The multi-layer perceptron isused to perform the actual singleton detection.2.1 DataWe use the OntoNotes corpus (Weischedel et al,2011; Weischedel et al, 2013), since it was alsoused in the CoNLL-2011 and 2012 shared tasks oncoreference resolution (Pradhan et al, 2011; Prad-han et al, 2012), and is used by de Marneffe et al(2015).
A downside of the OntoNotes corpus isthat singletons are not annotated.
As such, an ex-tra mention selection step is necessary to recoverthe singleton mentions from the data.We solve this problem by simply taking thementions as they are selected by the Stanfordcoreference resolution system (Lee et al, 2013),and use this as the full set of mentions.
Theseare similar to the Berkeley coreference resolu-tion system?s mentions (Durrett and Klein, 2013),since they mention they base their mention de-tection rules on those of Lee et al This makesthem suitable here.
In addition, de Marneffe et al(2015) use the Stanford system?s mentions as basisfor their singleton detection experiments, so usingthese mentions aids comparability as well.2.2 Recursive AutoencoderA recursive autoencoder (RAE) is applied to thevector representations of mentions, reducing themto a single word-embedding-length sized vector.This is done to compress the variable length men-tions to a fixed-size representation, which is re-quired by the multi-layer perceptron.The RAE used here is similar to the one usedby Socher et al (2011), with the following de-66sign choices: a sigmoid activation function is used,training is done using stochastic gradient descent,and the weights are untied.
A left-branching bi-nary tree structure is used, since only the finalmention representation is of interest.
Euclideandistance is used as an error measure, with eachvector?s error weighted by the number of wordsit represents.2.3 Multi-layer PerceptronThe multi-layer perceptron consists of an inputlayer, one hidden layer, and a binary classificationlayer.
As input, three types of features are used:the mention itself, context words around the men-tion, and other mentions in the context.The implementation of the MLP is straightfor-ward.
The input order is randomized, to preventspurious order effects.
Stochastic gradient descentis used for training.
Experiments with various set-tings for the parameters governing learning rate,number of training epochs, stopping criteria, hid-den layer size, context size and weight regulariza-tion are conducted, and their values and optimiza-tion are discussed in Section 3.1.2.4 Integrating Singleton Detection intoCoreference Resolution SystemsTwo coreference resolution systems are used forthe evaluation of singleton detection performance:the Berkeley system (Durrett and Klein, 2013) andthe Stanford system (Lee et al, 2013).The Stanford system is a deterministic rule-based system, in which different rules are appliedsequentially.
It was the highest scoring corefer-ence resolution system of the CoNLL-2011 sharedtask.
The most natural way of integrating a single-ton detection model in this system is by filteringout mentions directly after the mention detectionphase.The Berkeley system, on the other hand, is alearning-based model, which relies on template-based surface-level features.
It is currently oneof the best-performing coreference resolution sys-tems for English.
Because the system is a retrain-able learner, the most obvious way to use single-ton detection probabilities is as a feature, ratherthan a filter.
For both systems, varying ways ofintegrating the singleton detection information arepresented in Section 3.3.3 Evaluation & Results3.1 Preprocessing and OptimizationThe recursive autoencoder was trained on theCoNLL-2011 and 2012 training sets, with a learn-ing rate of 0.005.
Training was stopped when thelowest validation error was not achieved in the last25% of epochs.
The trained model was then usedto generate representations for all mentions in thedevelopment and test sets.Using these mention representations, the pa-rameters of the MLP were optimized on theCoNLL-2012 development set.
The stopping cri-terion was the same as for the RAE, and the learn-ing rate was fixed at 0.001, in order to isolatethe influence of other parameters.
During opti-mization, the following default parameter valueswere used: 50-dimensional embeddings, 150 hid-den nodes, 5 context words (both sides), 2 con-text mentions (both sides), and a 0.5 threshold forclassifying a mention as singleton.
A competitivebaseline was established by tagging each pronounas coreferential and all other mentions as single-ton.
We test for significant improvement over thedefault values using pair-wise approximate ran-domization tests (Yeh, 2000).For the hidden layer size, no value from theset {50, 100, 300, 600, 800} was significantly bet-ter than the default of 150.
In order to keep theinput/hidden layer proportion fixed, a 5:1 propor-tion was used during the rest of the optimizationand evaluation process.For the number of context words, the values{0, 1, 2, 3, 4, 10, 15, 20} were tested, yielding onlysmall differences.
However, the best-performingmodel, using only 1 context word on either side ofthe mention, was significantly better than the de-fault of 5 context words.For the number of context mentions, the defaultvalue of 2 turned out to be optimal, as it workedsignificantly better than most values from the set{0, 1, 3, 4, 5, 6}.Of all parameters, the choice for a set of wordembeddings was the most influential.
Differentsets of GloVe embeddings were tested, varying indimensionality and number of tokens trained on.The default set was 50D/6B, i.e.
50-dimensionalembeddings trained on 6 billion tokens of trainingdata.
The sets {100D/6B, 200D/6B, 300D/6B,300D/42B, 300D/840B} were evaluated.
Allout-performed the default set, and the 300D/42Bset performed the best.67Test set Acc.Singleton DetectionP R F111-dev 76.37 75.72 90.32 82.3811-test 77.47 77.26 88.42 82.4612-dev 79.57 79.77 85.83 82.6912-test 80.08 81.57 83.78 82.6612-dev-dM15 79.0 81.1 80.8 80.912-dev-BL 68.19 66.90 87.21 75.72Table 1: Singleton detection performance usingthe best-scoring model.
The CoNLL-2012 train-ing set was used as training data.
?dM15?
marksthe results by de Marneffe et al (2015) ?BL?
marksthe baseline performance.3.2 Singleton Detection ResultsThe final singleton detection model was evaluatedon the CoNLL-2011 development and test set, andthe CoNLL-2012 test set, in order to evaluate gen-eralization.
Results are reported in Table 1.
Gen-erally, performance holds up well across data sets,although the results on the 2011 sets are slightlylower than on the 2012 datasets.At 76-80% accuracy, the multi-layer perceptronis clearly better than the baseline.
Performance isalso compared to that of the state-of-the-art, by deMarneffe et al (2015), who only report scores onthe CoNLL-2012 development set.
The accuracyof my system is 0.6 percentage points higher.Because word embeddings are the only sourceof information used by the system, its performancemay be vulnerable to the presence of ?unknownwords?, i.e.
words for which there is no embed-ding.
Looking at the 2012 development set, wesee that classification accuracy for mentions con-taining one or more unknown words is 76.55%,as compared to 79.63% for mentions without un-known words.
The difference is smaller whenlooking at the context: accuracy for mentions withone or more unknown words in their context is78.73%, whereas it is 79.79% for mentions withfully known contexts.3.3 Coreference Resolution ResultsTable 2 shows the performance of the Stanfordsystem.
Multiple variables governing singletonfiltering were explored.
?NE?
indicates whethernamed entities were excluded from filtering ornot.
?Pairs?
indicates whether individual mentionsare filtered out, or only links between pairs ofmentions.
?Threshold?
indicates the threshold un-der which mentions are classified singleton.
Thethreshold value of 0.15 is chosen so that the single-ton classification has a precision of approximately90%We cannot compare directly to the system of deMarneffe et al (2015), because they used an older,faulty version of the CoNLL-scorer.
For the Stan-ford system, we therefore compare to a precursorof their system, by Recasens et al (2013), whosesingleton detection system is integrated with theStanford system.
For the Berkeley system, thisis not possible.
In both cases, we also com-pare to the system without any singleton detection.Differences were tested for significance using apaired-bootstrap re-sampling test (Koehn, 2004)over documents, 10000 times.The performance of the different filtering meth-ods is as expected.
For more widely applica-ble filters, precision goes up more, but recall alsodrops more.
For more selective filters, the dropin recall is smaller, but so is the gain in preci-sion.
The best balance here is yielded by the?Incl./Yes/0.15?-model, the most restrictive model,except for that it includes named entities in fil-tering.
This model yields a small improvementof 0.7 percentage points over the baseline.
Thisis slightly more than the Recasens et al (2013)system, and also slightly larger than the 0.5 per-centage point gain reported by de Marneffe et al(2015)NE Pairs Threshold CoNLL-F1Incl.
No 0.5 50.41*Incl.
No 0.15 56.73Incl.
Yes 0.5 55.46*Incl.
Yes 0.15 57.17*Excl.
No 0.5 53.64*Excl.
No 0.15 56.71Excl.
Yes 0.5 55.96*Excl.
Yes 0.15 56.92*Recasens et al (2013) 56.90*No Singleton Detection 56.44Table 2: Performance of the Stanford system onthe 2012 development set.
Significant differences(p < 0.05) from the baseline are marked *.Table 3 shows the performance of the Berke-ley system.
Here, singleton detection probabili-ties are incorporated as a feature.
Again, there are68multiple variations: ?Prob?
indicates each mentionwas assigned its predicted probability as a fea-ture.
?Mentions?
indicates each mention was as-signed a boolean feature indicating whether it waslikely singleton (P < 0.15), and a feature indicat-ing whether it was likely coreferential (P > 0.8).?Pairs?
indicates the same as ?Mentions?, but forpairs of mentions, where both have P < 0.15 orP > 0.8.
?Both?
indicates that both ?Mentions?-and ?Pairs?-features are added.Here, the performance differences are muchsmaller, yielding only a non-significant 0.3 per-centage point increase over the baseline.
All mod-els show an increase in precision, and a drop inrecall.
In contrast, de Marneffe et al (2015) reporta larger performance increase of almost 2 percent-age points for the Berkeley system.Model CoNLL-F1Prob 61.83Prob + Mentions 61.81Prob + Pairs 62.02Prob + Both 62.02No Singleton Detection 61.71Table 3: Performance of the Berkeley system onthe 2012 development set.
As a baseline system,the system using the ?FINAL?
feature set was used.Significant differences (p < 0.05) from the base-line are marked *.4 DiscussionThe singleton detection model was optimized withregard to four variables: hidden layer size, num-ber of context tokens, number of context mentions,and set of word embeddings.For hidden layer size, no clear effect was found.Regarding the set of word embeddings, we foundthat higher-dimensional embeddings provide bet-ter performance, which is in accordance with whatPennington et al (2014) found.
They, and Col-lobert et al (2011), also found that embeddingstrained on more text performed better on a rangeof tasks, but we do not see that clearly, here.As far as the number of context mentions isconcerned, the effect is small, and 2 mentions oneither side seems an optimal number.
Since theclosest mentions are likely the most relevant, thismakes sense.
Also, since the dataset contains bothshort pronoun mentions and longer NP mentions,the optimal number is likely a compromise; forpronouns like it, one would expect mentions in theleft-context to be most important, while this is notthe case for NP mentions.The most counter-intuitive result of parameteroptimization is the fact that just 1 context tokenon either side of the mention proved to be optimal.This contrasts with previous work: de Marneffeet al (2015) use 2 words around the mention, andsemantic information from a larger window, andBergsma and Yarowsky (2011) use up to 5 wordsbefore and 20 words after it.
Looking at the men-tion detection literature in general, we see that thispattern holds up: in non-referential it detection,larger context windows are used than in works thatdeal with complete NPs.Clearly, since large NP mentions already con-tain more information internally, they requiresmaller context windows.
Likely, the same dy-namic is at play here.
The OntoNotes dataset con-tains a majority of NP mentions, and has relativelylong mentions, since it only annotates the largestNP of a set of nested head-sharing NPs.The other main observation to be made on theresults is the discrepancy in the effect of single-ton information on the Berkeley coreference reso-lution system in this work and that by de Marneffeet al (2015).
Although singleton detection per-formance and the performance with the Stanfordsystem are similar, there is almost no performancegain with the Berkeley system here.Using the Berkeley coreference analyser (Kum-merfeld and Klein, 2013), the types of errors madeby the resolution systems can be analysed.
For theStanford system, we find the same error type pat-terns as de Marneffe et al (2015), which matcheswell with the similar performance gain.
For theBerkeley system, the increases in missing entityand missing mention errors are higher, and we donot find the large decrease in divided entity errorsthat de Marneffe et al (2015) found.
It is difficultto point out the underlying cause for this, due tothe learning-based nature of the Berkeley system.Somehow, there is a qualitative difference betweenthe probabilities produced by the two singleton de-tection systems.Regarding the question of how to integrate sin-gleton information in coreference resolution sys-tems, the picture is clear.
Both here and in deMarneffe et al (2015), the best way of using theinformation is with a high-precision filter, and for69pairs of mentions, rather than individual mentions.The only difference is that excluding named enti-ties from filtering was not beneficial here, whichmight be due to the fact that word embeddings alsocover names, which improves handling of them bythe singleton detection model.For future work, several avenues of explorationare available.
The first is to split singleton detec-tion according to mention type (similar to Hosteand Daelemans (2005) for coreference resolution).Since the current model covers all types of men-tions, it cannot exploit specific properties of thesemention types.
Training separate systems, for ex-ample for pronouns and NPs, might boost perfor-mance.Another improvement lies with the way men-tions are represented.
Here, a recursive autoen-coder was used to generate fixed size representa-tions for variable-length mentions.
However, a lotof information is lost in this compression step, andperhaps it is not the best compression method.
Al-ternative neural network architectures, such as re-current neural networks, convolutional neural net-works, and long short-term memories might yieldbetter results.In addition, an improved treatment of unknownwords could boost performance, since their pres-ence hurts classification accuracy.
Currently, anaverage of all embeddings is used to represent un-known words, but more advanced approaches arepossible, e.g.
by using part-of-speech information.To further investigate the interaction betweensingleton detection and coreference resolution, itwould be insightful to look into combining the cur-rent system with more recent coreference resolu-tion systems (e.g.
Wiseman et al, 2016; Clarkand Manning, 2015) which perform better thanthe Stanford and Berkeley systems.
On the onehand, singleton detection information could yieldlarger gains with these systems, as they might beable to exploit the information better.
For exam-ple, improved clustering algorithms might bene-fit more from a reduced number of mentions inthe search space.
On the other hand, improve-ments in these systems could overlap with the gainfrom singleton detection information, lowering theadded value of a separate singleton detection sys-tem.All in all, it is shown that a word embedding andneural network based singleton detection systemcan perform as well as a learner based on hand-crafted, linguistic-intuition-based features.
Witha straightforward neural network architecture, andoff-the-shelf word embeddings, neither of whichis specifically geared towards this task, state-of-the-art performance can be achieved.
As an addedbenefit, this approach can easily be extended toany other language, if word embeddings are avail-able.AcknowledgementsI am grateful to Jennifer Spenader for providinginspiration and feedback during all stages of thisproject, and to the anonymous reviewers for theirkind and useful comments.ReferencesShane Bergsma and David Yarowsky.
2011.
NADA:a robust system for non-referential pronoun detec-tion.
In Iris Hendrickx, Sobha Lalitha Devi, Ant?onioBranco, and Ruslan Mitkov, editors, Anaphora Pro-cessing and Applications, pages 12?23.
Springer,Berlin.Adriane Boyd, Whitney Gegg-Harrison, and Donna K.Byron.
2005.
Identifying non-referential it: amachine learning approach incorporating linguisti-cally motivated patterns.
In Proceedings of theACL Workshop on Feature Engineering for MachineLearning in NLP, pages 40?47.Donna K. Byron and Whitney Gegg-Harrison.
2004.Eliminating non-referring noun phrases from coref-erence resolution.
In Proceedings of DAARC 2004,pages 21?26.Kevin Clark and Christopher D. Manning.
2015.Entity-centric coreference resolution with modelstacking.
In Proceedings of ACL 2015, pages 1405?1415.Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
Journal of Machine Learning Research,12:2493?2537.Marie-Catherine de Marneffe, Marta Recasens, andChristopher Potts.
2015.
Modeling the lifespanof discourse entities with application to coreferenceresolution.
Journal of Artificial Intelligence Re-search, 52:445?475.Greg Durrett and Dan Klein.
2013.
Easy victories anduphill battles in coreference resolution.
In Proceed-ings of EMNLP 2013, pages 1971?1982.Veronique Hoste and Walter Daelemans.
2005.
Learn-ing Dutch coreference resolution.
In Proceedings ofCLIN 2004, pages 133?148.70Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proceedings ofEMNLP 2004, pages 388?395.Jonathan K. Kummerfeld and Dan Klein.
2013.
Error-driven analysis of challenges in coreference resolu-tion.
In Proceedings of EMNLP 2013, pages 265?277.Heeyoung Lee, Angel Chang, Yves Peirsman,Nathanael Chambers, Mihai Surdeanu, and Dan Ju-rafsky.
2013.
Deterministic coreference resolu-tion based on entity-centric, precision-ranked rules.Computational Linguistics, 39(4):885?916.Chao Ma, Janardhan Rao Doppa, J. Walker Orr,Prashanth Mannem, Xiaoli Fern, Tom Dietterich,and Prasad Tadepalli.
2014.
Prune-and-Score:Learning for greedy coreference resolution.
In Pro-ceedings of EMNLP 2014, pages 2115?2126.Vincent Ng and Claire Cardie.
2002.
Identifyinganaphoric and non-anaphoric noun phrases to im-prove coreference resolution.
In Proceedings ofCOLING 2002, pages 1?7.C.
D. Paice and G. D. Husk.
1987.
Towards the au-tomatic recognition of anaphoric features in Englishtext: the impersonal pronoun ?it?.
Computer Speechand Language, 2:109?132.Haoruo Peng, Kai-Wei Chang, and Dan Roth.
2015.A joint framework for corefference resolution andmention head detection.
In Proceedings of CoNLL2015, pages 12?21.Jeffrey Pennington, Richard Socher, and Christo-pher D. Manning.
2014.
GloVe: Global vectorsfor word representation.
In Proceedings of EMNLP2014, pages 1532?1543.Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,Martha Palmer, Ralph Weischedel, and NianwenXue.
2011.
CoNLL-2011 shared task: modelingunrestricted coreference in OntoNotes.
In Proceed-ings of the Fifteenth Conference on CoNLL, pages1?27.Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,Olga Uryupina, and Yuchen Zhang.
2012.
CoNLL-2012 shared task: Modeling multilingual unre-stricted coreference in OntoNotes.
In Proceedingsof the Joint Conference on EMNLP and CoNLL,pages 1?40.Marta Recasens, Marie-Catherine de Marneffe, andChristopher Potts.
2013.
The life and death of dis-course entities: Identifying singleton mentions.
InProceedings of NAACL-HLT 2013, pages 627?633.Richard Socher, Eric H. Huang, Jeffrey Pennington,Andrew Y. Ng, and Christopher D. Manning.
2011.Dynamic pooling and unfolding recursive autoen-coders for paraphrase detection.
In Proceedings ofNIPS 2011, pages 801?809.Olga Uryupina.
2003.
High-precision identification ofdiscourse new and unique noun phrases.
In Proceed-ings of the ACL 2003 Student Research Workshop,pages 80?86.Olga Uryupina.
2009.
Detecting anaphoricity andantecedenthood for coreference resolution.
Proce-samiento del Lenguaje Natural, 42:113?120.Ralph Weischedel, Martha Palmer, Mitchell Marcus,Eduard Hovy, Sameer Pradhan, Lance Ramshaw,Nianwen Xue, Ann Taylor, Jeff Kaufman, MichelleFranchini, Mohammed El-Bachouti, Robert Belvin,and Ann Houston.
2011.
OntoNotes Release 4.0.DVD.Ralph Weischedel, Martha Palmer, Mitchell Marcus,Eduard Hovy, Sameer Pradhan, Lance Ramshaw,Nianwen Xue, Ann Taylor, Jeff Kaufman, MichelleFranchini, Mohammed El-Bachouti, Robert Belvin,and Ann Houston.
2013.
OntoNotes Release 5.0.Web Download.Sam Wiseman, Alexander M. Rush, Stuart M. Shieber,and Jason Weston.
2015.
Learning anaphoricityand antecedent ranking features for coreference res-olution.
In Proceedings of ACL 2015, pages 1416?1426.Sam Wiseman, Alexander M. Rush, and Stuart M.Shieber.
2016.
Learning gloabl features for corefer-ence resolution.
arXiv preprint arXiv:1604.03035.Alexander Yeh.
2000.
More accurate tests for thestatistical significance of result differences.
In Pro-ceedings of COLING 2000, pages 947?953.71
