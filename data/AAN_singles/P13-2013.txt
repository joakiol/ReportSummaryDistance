Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 69?73,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsAggregated Word Pair Features for Implicit Discourse RelationDisambiguationOr BiranColumbia UniversityDepartment of Computer Scienceorb@cs.columbia.eduKathleen McKeownColumbia UniversityDepartment of Computer Sciencekathy@cs.columbia.eduAbstractWe present a reformulation of the wordpair features typically used for the taskof disambiguating implicit relations in thePenn Discourse Treebank.
Our word pairfeatures achieve significantly higher per-formance than the previous formulationwhen evaluated without additional fea-tures.
In addition, we present resultsfor a full system using additional featureswhich achieves close to state of the art per-formance without resorting to gold syntac-tic parses or to context outside the relation.1 IntroductionDiscourse relations such as contrast and causal-ity are part of what makes a text coherent.
Be-ing able to automatically identify these relationsis important for many NLP tasks such as gener-ation, question answering and textual entailment.In some cases, discourse relations contain an ex-plicit marker such as but or because which makesit easy to identify the relation.
Prior work (Pitlerand Nenkova, 2009) showed that where explicitmarkers exist, the class of the relation can be dis-ambiguated with f-scores higher than 90%.Predicting the class of implicit discourse rela-tions, however, is much more difficult.
Without anexplicit marker to rely on, work on this task ini-tially focused on using lexical cues in the formof word pairs mined from large corpora wherethey appear around an explicit marker (Marcu andEchihabi, 2002).
The intuition is that these pairswill tend to represent semantic relationships whichare related to the discourse marker (for example,word pairs often appearing around but may tendto be antonyms).
While this approach showedsome success and has been used extensively inlater work, it has been pointed out by multipleauthors that many of the most useful word pairsare pairs of very common functional words, whichcontradicts the original intuition, and it is hard toexplain why these are useful.In this work we focus on the task of identi-fying and disambiguating implicit discourse rela-tions which have no explicit marker.
In particular,we present a reformulation of the word pair fea-tures that have most often been used for this taskin the past, replacing the sparse lexical featureswith dense aggregated score features.
This is themain contribution of our paper.
We show that ourformulation outperforms the original one while re-quiring less features, and that using a stop list offunctional words does not significantly affect per-formance, suggesting that these features indeedrepresent semantically related content word pairs.In addition, we present a system which com-bines these word pairs with additional features toachieve near state of the art performance withoutthe use of syntactic parse features and of contextoutside the arguments of the relation.
Previouswork has attributed much of the achieved perfor-mance to these features, which are easy to get inthe experimental setting but would be less reliableor unavailable in other applications.12 Related WorkThis line of research began with (Marcu and Echi-habi, 2002), who used a small number of unam-biguous explicit markers and patterns involvingthem, such as [Arg1, but Arg2] to collect sets ofword pairs from a large corpus using the cross-product of the words in Arg1 and Arg2.
The au-thors created a feature out of each pair and built anaive bayes model directly from the unannotatedcorpus, updating the priors and posteriors usingmaximum likelihood.
While they demonstrated1Reliable syntactic parses are not always available in do-mains other than newswire, and context (preceding relations,especially explicit relations) is not always available in someapplications such as generation and question answering.69some success, their experiments were run on datathat is unnatural in two ways.
First, it is balanced.Second, it is constructed with the same unsuper-vised method they use to extract the word pairs -by assuming that the patterns correspond to a par-ticular relation and collecting the arguments froman unannotated corpus.
Even if the assumption iscorrect, these arguments are really taken from ex-plicit relations with their markers removed, whichas others have pointed out (Blair-Goldensohn etal., 2007; Pitler et al, 2009) may not look like trueimplicit relations.More recently, implicit relation prediction hasbeen evaluated on annotated implicit relationsfrom the Penn Discourse Treebank (Prasad et al,2008).
PDTB uses hierarchical relation typeswhich abstract over other theories of discoursesuch as RST (Mann and Thompson, 1987) andSDRT (Asher and Lascarides, 2003).
It contains40, 600 annotated relations from the WSJ corpus.Each relation has two arguments, Arg1 and Arg2,and the annotators decide whether it is explicit orimplicit.The first to evaluate directly on PDTB in a re-alistic setting were Pitler et al (2009).
They usedword pairs as well as additional features to trainfour binary classifiers, each corresponding to oneof the high-level PDTB relation classes.
Althoughother features proved to be useful, word pairs werestill the major contributor to most of these clas-sifiers.
In fact, their best system for comparisonincluded only the word pair features, and for allother classes other than expansion the word pairfeatures alone achieved an f-score within 2 pointsof the best system.
Interestingly, they found thattraining the word pair features on PDTB itself wasmore useful than training them on an external cor-pus like Marcu and Echihabi (2002), although insome cases they resort to information gain in theexternal corpus for filtering the word pairs.Zhou et al (2010) used a similar method andadded features that explicitly try to predict theimplicit marker in the relation, increasing perfor-mance.
Most recently to the best of our knowl-edge, Park and Cardie (2012) achieved the highestperformance by optimizing the feature set.
An-other work evaluating on PDTB is (Lin et al,2009), who are unique in evaluating on the morefine-grained second-level relation classes.3 Word Pairs3.1 The Problem: SparsityWhile Marcu and Echihabi (2002)?s approach oftraining a classifier from an unannotated corpusprovides a relatively large amount of training data,this data does not consist of true implicit relations.However, the approach taken by Pitler et al (2009)and repeated in more recent work (training directlyon PDTB) is problematic as well: when training amodel with so many sparse features on a datasetthe size of PDTB (there are 22, 141 non-explicitrelations overall), it is likely that many importantword pairs will not be seen in training.In fact, even the larger corpus of Marcu andEchihabi (2002) may not be quite large enoughto solve the sparsity issue, given that the num-ber of word pairs is quadratic in the vocabulary.Blair-Goldensohn et al (2007) report that usingeven a very small stop list (25 words) significantlyreduces performance, which is counter-intuitive.They attribute this finding to the sparsity of thefeature space.
An analysis in (Pitler et al, 2009)also shows that the top word pairs (ranked byinformation gain) all contain common functionalwords, and are not at all the semantically-relatedcontent words that were imagined.
In the caseof some reportedly useful word pairs (the-and; in-the; the-of...) it is hard to explain how they mightaffect performance except through overfitting.3.2 The Solution: AggregationRepresenting each word pair as a single feature hasthe advantage of allowing the weights for each pairto be learned directly from the data.
While pow-erful, this approach requires large amounts of datato be effective.Another possible approach is to aggregate someof the pairs together and learn weights from thedata only for the aggregated sets of words.
For thisapproach to be effective, the pairs we choose togroup together should have similar meaning withregard to predicting the relation.Biran and Rambow (2011) is to our knowledgethe only other work utilizing a similar approach.They used aggregated word pair set features topredict whether or not a sentence is argumentative.Their method is to group together word pairs thathave been collected around the same explicit dis-course marker: for every discourse marker suchas therefore or however, they have a single fea-ture whose value depends only on the word pairs70collected around that marker.
This is reasonablegiven the intuition that the marker pattern is unam-biguous and points at a particular relation.
Usingone feature per marker can be seen as analogous(yet complementary) to Zhou et al (2010)?s ap-proach of trying to predict the implicit connectiveby giving a score to each marker using a languagemodel.This work uses binary features which only in-dicate the appearance of one or more of the pairs.The original frequencies of the word pairs are notused anywhere.
A more powerful approach is touse an informed function to weight the word pairsused inside each feature.3.3 Our ApproachOur approach is similar in that we choose to ag-gregate word pairs that were collected around thesame explicit marker.
We first assembled a list ofall 102 discourse markers used in PDTB, in bothexplicit and implicit relations.2Next, we extract word pairs for each markerfrom the Gigaword corpus by taking the crossproduct of words that appear in a sentence aroundthat marker.
This is a simpler approach than us-ing patterns - for example, the marker because canappear in two patterns: [Arg1 because Arg2] and[because Arg1, Arg2], and we only use the first.We leave the task of listing the possible patternsfor each of the 102 markers to future work becauseof the significant manual effort required.
Mean-while, we rely on the fact that we use a very largecorpus and hope that the simple pattern [Arg1marker Arg2] is enough to make our features use-ful.
There are, of course, markers for which thispattern does not normally apply, such as by com-parison or on one hand.
We expect these featuresto be down-weighted by the final classifier, as ex-plained at the end of this section.
When collect-ing the pairs, we stem the words and discard pairswhich appear only once around the marker.We can think of each discourse marker as hav-ing a corresponding unordered ?document?, whereeach word pair is a term with an associated fre-quency.
We want to create a feature for eachmarker such that for each data instance (that is,for each potential relation in the PDTB data) thevalue for the feature is the relevance of the markerdocument to the data instance.2in implicit relations, there is no marker in the text but theimplicit marker is provided by the human annotatorsEach data instance in PDTB consists of two ar-guments, and can therefore also be representedas a set of word pairs extracted from the cross-product of the two arguments.
To represent the rel-evance of the instance to each marker, we set thevalue of the marker feature to the cosine similarityof the data instance and the marker?s ?document?,where each word pair is a dimension.While the terms (i.e.
word pairs) of thedata instance are weighted by simple occurencecount, we weight the terms in each marker?sdocument with tf-idf, where tf is defined inone of two ways: normalized term frequency( count(t)max{count(s,d):s?d}) and pointwise mutual infor-mation (log count(t)count(w1)?count(w2)), where w1 and w2are the member words of the pair.
Idf is calculatednormally given that the set of all documents is de-fined as the 102 marker documents.We then train a binary classifier (logistic regres-sion) using these 102 features for each of the fourhigh-level relations in PDTB: comparison, con-tingency, expansion and temporal.
To make sureour results are comparable to previous work, wetreat EntRel relations as instances of expansionand use sections 2-20 for training and sections 21-22 for testing.
We use a ten fold stratified cross-validation of the training set for development.
Ex-plicit relations are excluded from all data sets.As mentioned earlier, there are markers that donot fit the simple pattern we use.
In particular,some markers always or often appear as the firstterm of a sentence.
For these, we expect the list ofword pairs to be empty or almost empty, since inmost sentences there are no words on the left (andrecall that we discard pairs that appear only once).Since the features created for these markers willbe uninformative, we expect them to be weighteddown by the classifier and have no significant ef-fect on prediction.4 Evaluation of Word PairsFor our main evaluation, we evaluate the perfor-mance of word pair features when used with noadditional features.
Results are shown in Table 1.Our word pair features outperform the previousformulation (represented by the results reported by(Pitler et al, 2009), but used by virtually all previ-ous work on this task).
For most relation classes,tf is significantly better than pmi.
33Significance was verified for our own results in all exper-iments shown in this paper with a standard t-test71Comparison Contingency Expansion TemporalPitler et al, 2009 21.96 (56.59) 45.6 (67.1) 63.84 (60.28) 16.21 (61.98)tf-idf, no stop list 23 (61.72) 44.03 (66.78) 66.48 (60.93) 19.54 (68.09)pmi-idf, no stop list 24.38 (61.72) 38.96 (61.52) 62.22 (57.26) 16 (65.53)tf-idf, with stop list 23.77 44.33 65.33 16.98Table 1: Main evaluation.
F-measure (accuracy) for various implementations of the word pairs featuresComparison Contingency Expansion TemporalBest System 25.4 (63.36) 46.94 (68.09) 75.87 (62.84) 20.23 (68.35)features used pmi+1,2,3,6 tf+ALL tf+8 tf+3,9Pitler et al, 2009 21.96 (56.59) 47.13 (67.3) 76.42 (63.62) 16.76 (63.49)Zhou et al, 2010 31.79 (58.22) 47.16 (48.96) 70.11 (54.54) 20.3 (55.48)Park and Cardie, 2012 31.32 (74.66) 49.82 (72.09) 79.22 (69.14) 26.57 (79.32)Table 2: Secondary evaluation.
F-measure (accuracy) for the best systems.
tf and pmi refer to the wordpair features used (by tf implementation), and the numbers refer to the indeces of Table 3Comp.
Cont.
Exp.
Temp.1 WordNet 20.07 34.07 52.96 11.582 Verb Class 14.24 24.84 49.6 10.043 MPN 23.84 38.58 49.97 13.164 Modality 17.49 28.92 13.84 10.725 Polarity 16.46 26.36 65.15 11.586 Affect 18.62 31.59 59.8 13.377 Similarity 20.68 34.5 43.16 12.18 Negation 8.28 22.47 75.87 11.19 Length 20.75 31.28 65.72 10.19Table 3: F-measure for each feature categoryWe also show results using a stop list of 50 com-mon functional words.
The stop list has only asmall effect on performance except in the tempo-ral class.
This may be because of functional wordslike was and will which have a temporal effect.5 Other FeaturesFor our secondary evaluation, we include addi-tional features to complement the word pairs.
Pre-vious work has relied on features based on the goldparse trees of the Penn Treebank (which overlapswith PDTB) and on contextual information fromrelations preceding the one being disambiguated.We intentionally limit ourselves to features that donot require either so that our system can be readilyused on arbitrary argument pairs.WordNet Features: We define four featuresbased on WordNet (Fellbaum, 1998) - Synonyms,Antonyms, Hypernyms and Hyponyms.
The valuesare the counts of word pairs in the cross-product ofthe words in the arguments that have the particularrelation (synonymy, antonymy etc) between them.Verb Class: This is the count of pairs of verbsfrom Arg1 and Arg2 that share the same class, de-fined as the highest level Levin verb class (Levin,1993) from the LCS database (Dorr, 2001).Money, Percentages and Numbers (MPN): Thecounts of currency symbols/abbreviations, per-centage signs or cues (?percent?, ?BPS?...)
andnumbers in each argument.Modality: Presence or absence of each Englishmodal in each argument.Polarity: Based on MPQA (Wilson et al, 2005).We include the counts of positive and negativewords according to the MPQA subjectivity lexiconfor both arguments.
Unlike Pitler et al (2009), wedo not use neutral polarity features.
We also do notexplicitly group negation with polarity (althoughwe do have separate negation features).Affect: Based on the Dictionary of Affect in Lan-guage (Whissell, 1989).
Each word in the DALgets a score for three dimensions - pleasantness(pleasant - unpleasant), activation (passive - ac-tive) and imagery (hard to imagine - easy to imag-ine).
We use the average score for each dimensionin each argument as a feature.Content Similarity: We use the cosine similarityand word overlap of the arguments as features.Negation: Presence or absence of negation termsin each of the arguments.Length: The ratio between the lengths (counts ofwords) of the arguments.6 Evaluation of Additional FeaturesFor our secondary evaluation, we present resultsfor each feature category on its own in Table 3 andfor our best system for each of the relation classesin Table 2.
We show results for the best systemsfrom (Pitler et al, 2009), (Zhou et al, 2010) and72(Park and Cardie, 2012) for comparison.7 ConclusionWe presented an aggregated approach to word pairfeatures and showed that it outperforms the previ-ous formulation for all relation types but contin-gency.
This is our main contribution.
With thisapproach, using a stop list does not have a majoreffect on results for most relation classes, whichsuggests most of the word pairs affecting perfor-mance are content word pairs which may truly besemantically related to the discourse structure.In addition, we introduced the new and usefulWordNet, Affect, Length and Negation feature cat-egories.
Our final system outperformed the bestsystem from Pitler et al (2009), who used mostlysimilar features, for comparison and temporal andis competitive with the most recent state of theart systems for contingency and expansion with-out using any syntactic or context features.AcknowledgmentsThis research is supported by the Intelligence Ad-vanced Research Projects Activity (IARPA) viaDepartment of Interior National Business Cen-ter (DoI/NBC) contract number D11PC20153.The U.S. Government is authorized to reproduceand distribute reprints for Governmental purposesnotwithstanding any copyright annotation thereon.Disclaimer: The views and conclusions containedherein are those of the authors and should not beinterpreted as necessarily representing the officialpolicies or endorsements, either expressed or im-plied, of IARPA, DoI/NBC, or the U.S. Govern-ment.ReferencesNicholas Asher and Alex Lascarides.
2003.
Logics ofConversation.
Studies in Natural Language Process-ing Series.
Cambridge University Press.Or Biran and Owen Rambow.
2011.
Identifying justifi-cations in written dialog by classifying text as argu-mentative.
International Journal of Semantic Com-puting, 5(4):363?381, December.Sasha Blair-Goldensohn, Kathleen McKeown, andOwen Rambow.
2007.
Building and refin-ing rhetorical-semantic relation models.
In HLT-NAACL, pages 428?435.
The Association for Com-putational Linguistics.Bonnie J. Dorr.
2001.
LCS Verb Database, OnlineSoftware Database of Lexical Conceptual Structuresand Documentation.
University Of Maryland Col-lege Park.Christiane Fellbaum, editor.
1998.
WordNet An Elec-tronic Lexical Database.
The MIT Press.Beth Levin.
1993.
English Verb Classes and Alterna-tions: A Preliminary Investigation.
University OfChicago Press.Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng.
2009.Recognizing implicit discourse relations in the penndiscourse treebank.
In Proceedings of the 2009 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 343?351.William C. Mann and Sandra A. Thompson.
1987.Rhetorical Structure Theory: A theory of text orga-nization.
Technical Report ISI/RS-87-190, ISI.Daniel Marcu and Abdessamad Echihabi.
2002.
Anunsupervised approach to recognizing discourse re-lations.
In ACL, pages 368?375.
ACL.Joonsuk Park and Claire Cardie.
2012.
Improving im-plicit discourse relation recognition through featureset optimization.
In Proceedings of the 13th AnnualMeeting of the Special Interest Group on Discourseand Dialogue, pages 108?112.Emily Pitler and Ani Nenkova.
2009.
Using syntax todisambiguate explicit discourse connectives in text.In ACL/IJCNLP (Short Papers), pages 13?16.
TheAssociation for Computer Linguistics.Emily Pitler, Annie Louis, and Ani Nenkova.
2009.Automatic sense prediction for implicit discourse re-lations in text.
In ACL/IJCNLP, pages 683?691.
TheAssociation for Computer Linguistics.Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-sakaki, Livio Robaldo, Aravind Joshi, and BonnieWebber.
2008.
The penn discourse treebank 2.0.
InIn Proceedings of LREC.Cynthia M. Whissell.
1989.
The dictionary of affect inlanguage.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-level sentiment analysis.
In Proceedings of the con-ference on Human Language Technology and Em-pirical Methods in Natural Language Processing,pages 347?354.Zhi-Min Zhou, Yu Xu, Zheng-Yu Niu, Man Lan, JianSu, and Chew Lim Tan.
2010.
Predicting discourseconnectives for implicit discourse relation recogni-tion.
In Proceedings of the 23rd International Con-ference on Computational Linguistics.73
