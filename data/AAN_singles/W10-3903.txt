Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 12?20,Beijing, August 2010Summarizing Search Results using PLSIJun Harashima?
and Sadao KurohashiGraduate School of InformaticsKyoto UniversityYoshida-honmachi, Sakyo-ku,Kyoto, 606-8501, Japan{harashima,kuro}@nlp.kuee.kyoto-u.ac.jpAbstractIn this paper, we investigate generatinga set of query-focused summaries fromsearch results.
Since there may be manytopics related to a given query in thesearch results, in order to summarizethese results, they should first be clas-sified into topics, and then each topicshould be summarized individually.
Inthis summarization process, two types ofredundancies need to be reduced.
First,each topic summary should not containany redundancy (we refer to this prob-lem as redundancy within a summary).Second, a topic summary should not besimilar to any other topic summary (werefer to this problem as redundancy be-tween summaries).
In this paper, wefocus on the document clustering pro-cess and the reduction of redundancy be-tween summaries in the summarizationprocess.
We also propose a method usingPLSI to summarize search results.
Eval-uation results confirm that our methodperforms well in classifying search re-sults and reducing the redundancy be-tween summaries.1 IntroductionCurrently, the World Wide Web contains vastamounts of information.
To make efficient use ofthis information, search engines are indispens-able.
However, search engines generally return*Research Fellow of the Japan Society for the Promotionof Science (JSPS)only a long list containing the title and a snip-pet of each of the retrieved documents.
Whilesuch lists are effective for navigational queries,they are not helpful to users with informationalqueries.
Some systems (e.g., Clusty1) presentkeywords related to a given query together withthe search results.
It is, however, difficult forusers to understand the relation between the key-words and the query, as the keywords are merelywords or phrases out of context.
To solve thisproblem, we address the task of generating a setof query-focused summaries from search resultsto present information about a given query usingnatural sentences.Since there are generally many topics re-lated to a query in the search results, the taskof summarizing these results is one of, so tospeak, multi-topic multi-document summariza-tion.
Studies on multi-document summariza-tion typically address summarizing documentsrelated to a single topic (e.g., TAC2).
Howeverwe need to address summarizing documents re-lated to multiple topics when considering thesummarization of search results.To summarize documents containing multipletopics, we first need to classify them into top-ics.
For example, if a set of documents related toswine flu contains topics such as the outbreaks ofswine flu, the measures to treat swine flu, and soon, the documents should be divided into thesetopics and summarized individually.
Note that amethod for soft clustering should be employedin this process, as one document may belong toseveral topics.1http://clusty.com/2http://www.nist.gov/tac/12In the summarization process, two types ofredundancies need to be addressed.
First, eachtopic summary should not contain any redun-dancy.
We refer to this problem as redun-dancy within a summary.
This problem is wellknown in the field of multi-document summa-rization (Mani, 2001) and several methods havebeen proposed to solve it, such as MaximumMarginal Relevance (MMR) (Goldstein et al,2000) (Mori et al, 2004), using Integer Lin-ear Programming (ILP) (Filatova and Hatzivas-siloglou, 2004) (McDonald, 2007) (Takamuraand Okumura, 2009), and so on.Second, no topic summary should be similarto any of the other topic summaries.
We re-fer to this problem as redundancy between sum-maries.
For example, to summarize the above-mentioned documents related to swine flu, thesummary for outbreaks should contain specificinformation about outbreaks, whereas the sum-mary for measures should contain specific infor-mation about measures.
This problem is char-acteristic of multi-topic multi-document summa-rization.
Some methods have been proposedto generate topic summaries from documents(Radev and Fan, 2000) (Haghighi and Vander-wende, 2009), but to the best of our knowledge,the redundancy between summaries has not yetbeen addressed in any study.In this paper, we focus on the document clus-tering process and the reduction of redundancybetween summaries in the summarization pro-cess.
Furthermore, we propose a method usingPLSI (Hofmann, 1999) to summarize search re-sults.
In the proposed method, we employ PLSIto estimate the membership degree of each doc-ument to each topic, and then classify the searchresults into topics using this information.
In thesame way, we employ PLSI to estimate the mem-bership degree of each keyword to each topic,and then extract the important sentences spe-cific to each topic using this information in orderto reduce the redundancy between summaries.The evaluation results show that our method per-forms well in classifying search results and suc-cessfully reduces the redundancy between sum-maries.	D Dz  Sz	  Sz?			 W Dz?
	Figure 1: Overview of the proposed method.2 Proposed Method2.1 OverviewFigure 1 gives an overview of the proposedmethod, which comprises the following foursteps.Step 1.
Acquisition of Search Results Using asearch engine, obtain the search results fora given query.Step 2.
Keyword Extraction Extract the key-words related to the query from the searchresults using the method proposed by Shi-bata et al (2009).Step 3.
Document Clustering Estimate themembership degree of each document toeach topic using PLSI, and classify thesearch results into topics.Step 4.
Summarization For each topic, gener-ate a summary by extracting the importantsentences specific to each topic from eachdocument cluster.In the following subsections, we describe eachstep in detail.2.2 Step 1.
Acquisition of Search ResultsFirst, we obtain the search results for a givenquery using a search engine.
To be more precise,we obtain the topN ?
documents of the search en-gine results.
Next, we remove those documentsthat should not be included in the summarization,such as link collections, using a simple filteringmethod.
For example, we regard any documentthat has too many links as a link collection, andremove it.In this paper, we write D to denote the searchresults after the filtering, and let N = |D|.132.3 Step 2.
Keyword ExtractionWe extract the keywords related to a query fromD using the method proposed by Shibata etal.
(2009), which comprises the following foursteps.Step 2-1.
Relevant Sentence Extraction Foreach document in D, extract the sentencescontaining the query and the sentencesaround the query as relevant sentences.Step 2-2.
Keyword Candidate Extraction Foreach relevant sentence, extract compoundnouns and parenthetic strings as keywordcandidates.Step 2-3.
Synonymous Candidate UnificationFind the paraphrase pairs and the ortho-graphic variant pairs in the keywordcandidates, and merge them.Step 2-4.
Keyword Selection Score each key-word candidate, rank them, and select thebestM as the keywords related to the query.In this paper, we write W to denote the ex-tracted keywords.2.4 Step 3.
Document ClusteringWe classify D into topics using PLSI.
In PLSI,a document d and a word w are assumed to beconditionally independent given a topic z, andthe joint probability p(d,w) is calculated as fol-lows.p(d,w) =?zp(z) p(d|z) p(w|z) (1)p(z), p(d|z), and p(w|z) are estimated by max-imizing the log-likelihood function L, which iscalculated asL =?d?wfreq(d,w) log p(d,w), (2)where freq(d,w) represents the frequency ofword w in document d. L is maximized usingthe EM algorithm, in which the E-step and M-step are given below.E-stepp(z|d,w) = p(z) p(d|z) p(w|z)?z?
p(z?)
p(d|z?)
p(w|z?
)(3)M-stepp(z) =?d?w freq(d,w) p(z|d,w)?d?w freq(d,w)(4)p(d|z) =?w freq(d,w) p(z|d,w)?d?
?w freq(d?, w) p(z|d?, w)(5)p(w|z) =?d freq(d,w) p(z|d,w)?d?w?
freq(d,w?)
p(z|d,w?
)(6)The EM algorithm iterates through these stepsuntil convergence.First, we give PLSI the number of topics K,the search results D, and the keywords W asinput, and estimate p(z), p(d|z), and p(w|z),where z is a topic related to the query, d is a doc-ument in D, and w is a keyword in W .
There is,however, no way of knowing the value ofK; thatis, we do not know in advance how many topicsrelated to the query there are in the search results.Hence, we perform PLSI for several values ofK,and select the K that has the minimum AkaikeInformation Criterion (AIC) (Akaike, 1974), cal-culated as follows.AIC = ?2L + 2K(N + M) (7)Furthermore, we select p(z), p(d|z), and p(w|z)estimated using the selected K as the result ofPLSI.Next, we calculate the membership degree ofeach document to each topic.
The membershipdegree of document d to topic z, denoted p(z|d),is calculated asp(z|d) = p(d|z) p(z)?z?
p(d|z?).
(8)Finally, for each topic, we collect those docu-ments whose membership degree to the topic islarger than the threshold ?.
If there is a docu-ment whose membership degree to multiple top-ics is larger than the threshold, we classify thedocument into each topic.In this paper, Dz denotes the documents clas-sified into topic z.2.5 Step 4.
SummarizationFor each topic, we extract the important sen-tences specific to that topic from each document14Figure 2: Algorithm for summarization.Input: A set of K document clusters {Dz}(z ?
Z)Output: A set of K summaries {Sz}(z ?
Z)Procedure:1: for all z ?
Z2: while |Sz| < num(z)3: for all s ?
Dz4: calculate s score(z, s, Sz)5: smax = argmaxs?Dz\Sz s score(z, s, Sz)6: Sz = Sz ?
{smax}7: return Szcluster.
Figure 2 gives the algorithm for sum-marization.
When we generate the summary Szfor topic z, we calculate the importance of sen-tence s to topic z, denoted as s score(z, s, Sz),for each sentence in Dz (lines 3-4).
Then we ex-tract the sentence smax with the maximum im-portance as an important sentence, and includesmax in Sz (lines 5-6).
When we extract thenext important sentence, we recalculate the im-portance s score(z, s, Sz) for each sentence inDz except the sentence in Sz (lines 3-4).
Thenwe extract the sentence smax with the maximumimportance as an important sentence, and addsmax to Sz (lines 5-6).
We continue this processuntil the number of important sentences compos-ing the summary, denoted |Sz|, reaches the num-ber of important sentences extracted for topic z,denoted num(z) (line 2).s score(z, s, Sz) is calculated as follows:s score(z, s, Sz)= ?w?Ws(w score(z, w) ?
c score(w, Sz, s))(9)whereWs represents the keywords in sentence s.w score(z, w) is a function to reduce the re-dundancy between summaries, and representsthe importance of keyword w to topic z.
We canuse the probability ofw given z, denoted p(w|z),as the w score(z, w).
This approach fails, how-ever, because if there are keywords with a highprobability in both topic z and another topic z?,the sentences containing such keywords are ex-tracted as the important sentences in both top-ics, and it follows that the generated summarieswill contain redundancy.
To solve this problem,we use the membership degree of keyword wTable 1: Values of c score(w, Sz, s).w is w is notcontained in Sz contained in Szw is the subject of s 2 -2otherwise 0 1to topic z, denoted p(z|w), as w score(z, w).We use p(z) and p(w|z) estimated using PLSIin Section 2.4 to calculate p(z|w).p(z|w) = p(w|z) p(z)?z?
p(w|z?
)(10)Keywords with high probability in several topicsshould have a low membership degree to eachtopic.
Thus, using p(z|w) as the w score(z, w)prevents extracting sentences containing suchkeywords as important sentences, and it followsthat the similarity between the summaries is re-duced.
Furthermore, the keywords which arespecific to a topic are supposed to have a highmembership degree to that topic.
Thus, usingp(z|w) as w score(z, w) makes it easier to ex-tract sentences containing such keywords as im-portant sentences, and with the result that eachsummary is specific to the particular topic.c score(w, Sz, s) is a function to reduce theredundancy within a summary, and representsthe importance of a keyword w in a sentences under the condition that there is a set ofextracted important sentences Sz .
The valueof c score(w,Sz, s) is determined mainly bywhether or not w is contained in Sz .
Ta-ble 1 gives the values of c score(w,Sz, s).For example, if w is contained in Sz , weset c score(w,Sz, s) = 0, else we setc score(w, Sz, s) = 1.
In this way, we can ex-tract the sentences containing the keywords thatare not contained in Sz as important sentences,and reduce the redundancy within the summary.Note that we make some exceptions to generatea coherent summary.
For example, even if w iscontained in Sz , we set c score(w, Sz, s) = 2as long as w is the subject of s. In the sameway, even if w is not contained in Sz , we setc score(w, Sz, s) = ?2 as long as w is the sub-ject of s. These values for c score(w, Sz, s) areempirically determined.15Finally, using p(z) we determine the numberof important sentences extracted for topic z, de-noted as num(z).num(z) ={b I ?
p(z) c ( p(z) ?
?
)Imin ( p(z) < ?
)(11)where I represents the parameter that controlsthe total number of important sentences ex-tracted for each topic.
The higher the probabilitya topic has, the more important sentences we ex-tract.
Note that no matter how low p(z) is, weextract at least Imin important sentences.3 Experiments3.1 OverviewTo evaluate the proposed method, we recruited48 subjects, mainly IT workers, and asked themto fill in a questionnaire.
We prepared a sys-tem implemented according to our method, andasked the subjects to use our system to evaluatethe following four aspects of our method.?
Validity of the number of topics?
Precision of document clustering?
Degree of reduction in redundancy betweensummaries?
Effectiveness of the method for presentinginformation through summariesWe allowed the subjects to create arbitraryqueries for our system.3.2 SystemFigure 3 shows the system results for the queryswine flu.
Our system presents a separate sum-mary for each topic related to a given query.
InFig.3, the colored words in the summaries arekeywords specific to each topic.
If a user clickson a keyword, the system presents a list of doc-uments containing that keyword at the bottom ofthe browser.The configuration of our system was as fol-lows.
In the acquisition process, the system ob-tained the search results for a given query us-ing the search engine TSUBAKI (Shinzato et al,2008b).
Setting N ?
= 1, 000, we obtained thetop 1, 000 documents in the search results forthe query.
In the keyword extraction process,we set M = 100, and extracted 100 keywordsrelated to the query from the search results.
Inthe document clustering process, we performedPLSI for K = 3, 4, 5, and selected the K withthe minimum AIC.
We set the initial value ofp(z) = 1/K, and the initial values of p(d|z)and p(w|z) to random values.
The EM algorithmcontinued until the increase in L reached just be-low 1 to achieve convergence.
We set ?
= 1/K.In the summarization process, we set I = 10,since the number of important sentences able tobe presented in a browser is about 10.
We setImin = 2 and ?
= 0.2, and extracted at least twoimportant sentences, even if p(z) was very low.3.3 Validity of the Number of TopicsFirst, we investigated how well the proposedmethod determined the number of topics.
In ourmethod, the number is determined using AIC.Ideally, we should have manually counted thetopics in the search results, and compared thiswith the number determined using AIC.
It was,however, difficult to count the topics, because thesearch results contained 1, 000 documents.
Fur-thermore, it was impossible to count the numberof topics for each query given by each subject.Thus, in this investigation, we simply asked thesubjects whether they felt the number of topicsummaries presented to them was appropriate ornot, and investigated our method in terms of us-ability.Table 2 gives the results.
According to Table2, 60.4% of the subjects agreed that the numberof topic summaries presented by our system wasacceptable.
The average of the number of topicsdetermined by our method was 3.18 per 1 query.On the other hand, 33.3% of the subjects saidthe number of topic summaries was too low orsomewhat too low.
According to these results,it seems that users are satisfied with the systempresenting about 3 or 4 topic summaries, and ourmethod determined the desirable number of top-ics in terms of usability.16	  	 			  	 	    		 	  	    	 		   	 	 		  				     !
""# $"" "  %&'(	%	   	)		 * 		  +  	 		 ,	-	 		 		.&Figure 3: System results for the query swine flu.Table 2: Validity of the number of topics.options # subjects ( % )(a) definitely too many 0 ( 0.0)(b) somewhat too many 3 ( 6.3)(c) acceptable 29 (60.4)(d) somewhat too few 11 (22.9)(e) definitely too few 5 (10.4)3.4 Precision of Document ClusteringSecond, we investigated how precisely the pro-posed method classified the search results intotopics.
To be more precise, we evaluated the re-liability of the membership degree p(z|d) usedin the document clustering process.
It is gen-erally difficult to evaluate clustering methods.In our case, we did not have any correct dataand could not even create these since, as men-tioned previously, the number of topics is notknown.
Furthermore, it is not possible to classifyby hand search results containing 1, 000 docu-ments.
Consequently, we did not evaluate ourmethod directly by comparing correct data withthe clustering result from our method, but insteadevaluated it indirectly by investigating the relia-bility of the membership degree p(z|d) used inthe document clustering process.The evaluation process was as follows.
First,we presented the subjects with a document d,which was estimated by our system to have ahigh membership degree to a topic z. Strictlyspeaking, we selected as d, a document with amembership degree of about 0.9.
Next, we pre-sented two documents to the subjects.
One wasa document d?
whose membership degree to zwas also about 0.9, and another was a documentd??
whose membership degree to z was about0.1.
Finally, we asked themwhich document wasmore similar to d3.Table 3 gives the results.
According to this ta-ble, 60.5% of the subjects said d?
was more simi-lar or somewhat more similar.
On the other hand,only 12.6% of the subjects said d??
was moresimilar or somewhat more similar.
We see fromthese results that the ability to recognize topics inour system is in agreement to some extent with3Naturally, we did not tell them that d?
had a similarmembership degree to d, whereas d??
did not.17Table 3: Precision of the estimation p(z|d).options # subjects ( % )(a) d?
is definitely more similar 14 (29.2)(b) d?
is somewhat more similar 15 (31.3)(c) undecided 13 (27.1)(d) d??
is somewhat more similar 3 ( 6.3)(e) d??
is definitely more similar 3 ( 6.3)the subjects?
ability for recognizing topics; thatis, our method was able to estimate a reliablemembership degree p(z|d).
Thus, it seems thatour method using p(z|d) is able to classify searchresults into topics to some extent.3.5 Degree of Reduction in Redundancybetween SummariesThird, we investigated how well the proposedmethod reduced the redundancy between sum-maries.
To be more precise, we used three mea-sures as w score(z, w) to generate summariesand investigated which measure generated theleast redundant summaries.
Generally, meth-ods for reducing redundancy are evaluated usingROUGE (Lin, 2004), BE (Hovy et al, 2005),or Pyramid (Nenkova and Passonneau, 2004).However, the use of these methods require thatideal summaries are created by humans, and thiswas not possible for the same reason as men-tioned previously.
Thus, we did not performa direct evaluation using the methods such asROUGE, but instead evaluated how well ourmethod performed in reducing redundancy be-tween summaries using the membership degreep(z|w) as w score(z, w).The evaluation process was as follows.
Weused three measures as w score(z, w), and gen-erated three sets of summaries.Summaries A This set of summaries was gen-erated using dfidf(w) as w score(z, w),with dfidf(w) calculated as ldf(w) ?log(100million/gdf(w)), ldf(w) repre-senting the document frequency of keywordw in the search results, and gdf(w) rep-resenting the document frequency of key-word w in the TSUBAKI document col-lection (Shinzato et al, 2008a) comprisingabout 100 million documents.Table 4: Comparison of dfidf(w), p(w|z) andp(z|w).options # subjects ( % )(a) B is definitely less redundant 5 (10.4)(b) B is somewhat less redundant 16 (33.3)(c) undecided 15 (31.3)(d) A is somewhat less redundant 6 (12.5)(e) A is definitely less redundant 6 (12.5)options # subjects ( % )(a) C is definitely less redundant 16 (33.3)(b) C is somewhat less redundant 14 (29.2)(c) undecided 6 (12.5)(d) A is somewhat less redundant 8 (16.7)(e) A is definitely less redundant 4 ( 8.3)options # subjects ( % )(a) C is definitely less redundant 15 (31.3)(b) C is somewhat less redundant 8 (16.7)(c) undecided 10 (20.8)(d) B is somewhat less redundant 6 (12.5)(e) B is definitely less redundant 9 (18.8)Summaries B This set of summaries was gen-erated using p(w|z) as w score(z, w).Summaries C This set of summaries was gen-erated using p(z|w) as w score(z, w).We then presented the subjects with three pairsof summaries, namely a pair from A and B, apair from A and C, and a pair from B and C, andasked them which summaries in each pair wasless redundant4.The results are given in Tables 4.
Firstly, ac-cording to the comparison of A and B and thecomparison of A and C, A was more redundantthan B and C. The value of dfidf(w) to key-word w was the same for all topics.
Thus, us-ing dfidf(w) asw score(z, w) made summariesredundant, as each summary tended to containthe same keywords with high dfidf(w).
On theother hand, as the value of p(w|z) and p(z|w)were dependent on the topic, the summaries gen-erated using these measures were less redundant.Second, the comparison of B and C shows that48.0% of the subjects considered C to be lessredundant or somewhat less redundant.
p(w|z)was a better measure than dfidf(w), but even us-ing p(w|z) generated redundancy between sum-4Naturally, we did not tell them which summaries weregenerated using which measures18Table 5: Comparison of summaries and keywords.options # subjects ( % )(a) X is definitely more helpful 25 (52.1)(b) X is somewhat more helpful 10 (20.8)(c) undecided 3 ( 6.3)(d) Y is somewhat more helpful 8 (16.7)(e) Y is definitely more helpful 2 ( 4.2)maries.
Because common keywords to a queryhave high p(w|z) for several topics, sentencescontaining these keywords were extracted as theimportant sentences for those topics, and thusthe summaries were similar to one another.
Onthe other hand, the keywords?
value for p(z|w)was low, allowing us to extract the importantsentences specific to each topic using p(z|w) asw score(z, w), thereby reducing the redundancybetween summaries.3.6 Effectiveness of the Method forPresenting Information UsingSummariesWe also investigated the effectiveness of themethod for presenting information through sum-maries.
We asked the subjects to compare twodifferent ways of presenting information and tojudge which way was more effective in termsof usefulness for collecting information abouta query.
One of the methods presented thesearch results with topic summaries generated byour system (method X), and while the anothermethod presented the search results with the key-words included in each topic summary (methodY).Table 5 gives the results.
72.9% of the sub-jects considered the method using summaries tobe more effective or somewhat more effective.From these results, it appears that the method ofpresenting information through summaries is ef-fective in terms of usefulness for collecting in-formation about a query.4 ConclusionIn this paper, we focused on the task of gen-erating a set of query-focused summaries fromsearch results.
To summarize the search resultsfor a given query, a process of classifying theminto topics related to the query was needed.
Inthe proposed method, we employed PLSI to es-timate the membership degree of each documentto each topic, and then classified search resultsinto topics using this metric.
The evaluation re-sults showed that our method estimated reliabledegrees of membership.
Thus, it seems that ourmethod is able to some extent to classify searchresults into topics.In the summarization process, redundancywithin a summary and redundancy between sum-maries needs to be reduced.
In this paper, we fo-cused on the reduction of the latter redundancy.Our method made use of PLSI to estimate themembership degree of each keyword to eachtopic, and then extracted the important sentencesspecific to each topic using this metric.
The eval-uation results showed that our method was ableto reduce the redundancy between summaries us-ing the membership degree.In future, we will investigate the use ofmore sophisticated topic models.
Although ourmethod detected the topics related to a query us-ing a simple topic model (i.e., PLSI), we be-lieve that more sophisticated topic models suchas LDA (Blei et al, 2003) allow us to improveour method.ReferencesAkaike, Hirotugu.
1974.
A new look at the statis-tical model identification.
IEEE Transactions onAutomation Control, 19(6):716?723.Blei, David M., Andrew Y. Ng, and Michael I. Jor-dan.
2003.
Latent Dirichlet Allocation.
Journalof Machine Learning Research, 3:993?1022.Filatova, Elena and Vasileios Hatzivassiloglou.
2004.A formal model for information selection in multi-sentence text extraction.
In Proceedings of COL-ING 2004, pages 397?403.Goldstein, Jade, Vibhu Mittal, Jaime Carbonell, andMark Kantrowitz.
2000.
Multi-document summa-rization by sentence extraction.
In Proceedings ofANLP/NAACL 2000 Workshop on Automatic Sum-marization, pages 40?48.Haghighi, Aria and Lucy Vanderwende.
2009.
Ex-ploring content models for multi-document sum-marization.
In Proceedings of HLT-NAACL 2009,pages 362?370.19Hofmann, Thomas.
1999.
Probabilistic latent se-mantic indexing.
In Proceedings of SIGIR 1999,pages 50?57.Hovy, Eduard, Chin-Yew Lin, and Liang Zhou.
2005.Evaluating duc 2005 using basic elements.
In Pro-ceedings of DUC 2005.Lin, Chin-Yew.
2004.
Rouge: A package for au-tomatic evaluation of summaries.
In Proceedingsof ACL 2004 Workshop on Text SummarizationBranches Out, pages 74?81.Mani, Inderjeet.
2001.
Automatic Summarization.John Benjamins Publishing Company.McDonald, Ryan.
2007.
A study of global inferencealgorithms in multi-document summarization.
InProceedings of ECIR 2007, pages 557?564.Mori, Tatsunori, Masanori Nozawa, and Yoshi-aki Asada.
2004.
Multi-answer-focusedmulti-document summarization using a question-answering engine.
In Proceedings of COLING2004, pages 439?445.Nenkova, Ani and Rebecca Passonneau.
2004.
Eval-uating content selection in summarization: Thepyramid method.
In Proceedings of NAACL-HLT2004.Radev, Dragomir R. and Weiguo Fan.
2000.
Auto-matic summarization of search engine hit lists.
InProceedings of ACL 2000 Workshop on Recent ad-vances in NLP and IR, pages 1361?1374.Shibata, Tomohide, Yasuo Bamba, Keiji Shinzato,and Sadao Kurohashi.
2009.
Web information or-ganization using keyword distillation based clus-tering.
In Proceedings of WI 2009, pages 325?330.Shinzato, Keiji, Daisuke Kawahara, ChikaraHashimoto, and Sadao Kurohashi.
2008a.
Alarge-scale web data collection as a natural lan-guage processing infrastructure.
In Proceedingsof LREC 2008, pages 2236?2241.Shinzato, Keiji, Tomohide Shibata, Daisuke Kawa-hara, Chikara Hashimoto, and Sadao Kurohashi.2008b.
TSUBAKI: An Open Search Engine In-frastructure for Developing New Information Ac-cess Methodology.
In Proceedings of IJCNLP2008, pages 189?196.Takamura, Hiroya and Manabu Okumura.
2009.Text summarization model based on maximumcoverage problem and its variant.
In Proceedingsof EACL 2009, pages 781?789.20
