Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 716?725,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsValidation of sub-sentential paraphrases acquiredfrom parallel monolingual corporaHouda Bouamor Aure?lien MaxLIMSI-CNRS & Univ.
Paris SudOrsay, Francefirstname.lastname@limsi.frAnne VilnatAbstractThe task of paraphrase acquisition from re-lated sentences can be tackled by a varietyof techniques making use of various typesof knowledge.
In this work, we make thehypothesis that their performance can beincreased if candidate paraphrases can bevalidated using information that character-izes paraphrases independently of the set oftechniques that proposed them.
We imple-ment this as a bi-class classification prob-lem (i.e.
paraphrase vs. not paraphrase),allowing any paraphrase acquisition tech-nique to be easily integrated into the com-bination system.
We report experiments ontwo languages, English and French, with5 individual techniques on parallel mono-lingual parallel corpora obtained via multi-ple translation, and a large set of classifi-cation features including surface to contex-tual similarity measures.
Relative improve-ments in F-measure close to 18% are ob-tained on both languages over the best per-forming techniques.1 IntroductionThe fact that natural language allows messagesto be conveyed in a great variety of ways consti-tutes an important difficulty for NLP, with appli-cations in both text analysis and generation.
Theterm paraphrase is now commonly used in theNLP litterature to refer to textual units of equiva-lent meaning at the phrasal level (including singlewords).
For instance, the phrases six months andhalf a year form a paraphrase pair applicable inmany different contexts, as they would appropri-ately denote the same concept.
Although one canenvisage to manually build high-coverage lists ofsynonyms, enumerating meaning equivalences atthe level of phrases is too daunting a task for hu-mans.
Because this type of knowledge can how-ever greatly benefit many NLP applications, au-tomatic acquisition of such paraphrases has at-tracted a lot of attention (Androutsopoulos andMalakasiotis, 2010; Madnani and Dorr, 2010),and significant research efforts have been devotedto this objective (Callison-Burch, 2007; Bhagat,2009; Madnani, 2010).Central to acquiring paraphrases is the need ofassessing the quality of the candidate paraphrasesproduced by a given technique.
Most works todate have resorted to human evaluation of para-phrases on the levels of grammaticality and mean-ing equivalence.
Human evaluation is howeveroften criticized as being both costly and non re-producible, and the situation is even more compli-cated by the inherent complexity of the task thatcan produce low inter-judge agreement.
Task-based evaluation involving the use of paraphras-ing into some application thus seem an acceptablesolution, provided the evaluation methodologiesfor the given task are deemed acceptable.
This,in turn, puts the emphasis on observing the im-pact of paraphrasing on the targeted applicationand is rarely accompanied by a study of the intrin-sic limitations of the paraphrase acquisition tech-nique used.The present work is concerned with the task ofsub-sentential paraphrase acquisition from pairsof related sentences.
A large variety of tech-niques have been proposed that can be appliedto this task.
They typically make use of differ-ent kinds of automatically or manually acquiredknowledge.
We make the hypothesis that theirperformance can be increased if candidate para-716phrases can be validated using information thatcharacterize paraphrases in complement to the setof techniques that proposed them.
We propose toimplement this as a bi-class classification problem(i.e.
paraphrase vs. not paraphrase), allowingany paraphrase acquisition technique to be easilyintegrated into the combination system.
In thisarticle, we report experiments on two languages,English and French, with 5 individual techniquesbased on a) statistical word alignment models,b) translational equivalence, c) handcoded rules ofterm variation, d) syntactic similarity, and e) editdistance on word sequences.
We used parallelmonolingual parallel corpora obtained via mul-tiple translation from a single language as oursources of related sentences, and a large set offeatures including surface to contextual similaritymeasures.
Relative improvements in F-measureclose to 18% are obtained on both languages overthe best performing techniques.The remainder of this article is organized asfollows.
We first briefly review previous workon sub-sentential paraphrase acquisition in sec-tion 2.
We then describe our experimental settingin section 3 and the individual techniques that wehave studied in section 4.
Section 5 is devoted toour approach for validating paraphrases proposedby individual techniques.
Finally, section 6 con-cludes the article and presents some of our futurework in the area of paraphrase acquisition.2 Related workThe hypothesis that if two words or, by exten-sion, two phrases, occur in similar contexts thenthey may be interchangeable has been extensivelytested.
The distributional hypothesis, attributed toZellig Harris, was for example applied to syntac-tic dependency paths in the work of Lin and Pan-tel (2001).
Their results take the form of equiva-lence patterns with two arguments such as {X asksfor Y, X requests Y, X?s request for Y, X wants Y,Y is requested by X, .
.
.
}.Using comparable corpora, where the same in-formation probably exists under various linguis-tic forms, increases the likelihood of finding veryclose contexts for sub-sentential units.
Barzilayand Lee (2003) proposed a multi-sequence align-ment algorithm that takes structurally similar sen-tences and builds a compact lattice representationthat encodes local variations.
The work by Bhagatand Ravichandran (2008) describes an applicationof a similar technique on a very large scale.The hypothesis that two words or phrases areinterchangeable if they share a common trans-lation into one or more other languages hasalso been extensively studied in works on sub-sentential paraphrase acquisition.
Bannard andCallison-Burch (2005) described a pivoting ap-proach that can exploit bilingual parallel corporain several languages.
The same technique hasbeen applied to the acquisition of local paraphras-ing patterns in Zhao et al(2008).
The work ofCallison-Burch (2008) has shown how the mono-lingual context of a sentence to paraphrase can beused to improve the quality of the acquired para-phrases.Another approach consists in modelling localparaphrasing identification rules.
The work ofJacquemin (1999) on the identification of termvariants, which exploits rewriting morphosyntac-tic rules and descriptions of morphological andsemantic lexical families, can be extended to ex-tract the various forms corresponding to input pat-terns from large monolingual corpora.When parallel monolingual corpora aligned atthe sentence level are available (e.g.
multipletranslations into the same language), the task ofsub-sentential paraphrase acquisition can be castas one of word alignment between two alignedsentences (Cohn et al 2008).
Barzilay andMcKeown (2001) applied the distributionality hy-pothesis on such parallel sentences, and Pang etal.
(2003) proposed an algorithm to align sen-tences by recursive fusion of their common syn-tactic constituants.Finally, they has been a recent interest in auto-matic evaluation of paraphrases (Callison-Burchet al 2008; Liu et al 2010; Chen and Dolan,2011; Metzler et al 2011).3 Experimental settingWe used the main aspects of the methodologydescribed by Cohn et al(2008) for constructingevaluation corpora and assessing the performanceof techniques on the task of sub-sentential para-phrase acquisition.
Pairs of related sentences arehand-aligned to define a set of reference atomicparaphrase pairs at the level of words or phrases,denoted asRatom1.1Note that in this study we do not distinguish between?Sure?
and ?Possible?
alignments, and when reusing anno-717single language multiple language video descriptions multiply-translated news headlinestranslation translation subtitles# tokens 4,476 4,630 1,452 2,721 1,908# unique tokens 656 795 357 830 716% aligned tokens (excluding identities) 60.58 48.80 23.82 29.76 14.46lexical overlap (tokens) 77.21 61.03 59.50 32.51 39.63lexical overlap (lemmas content words) 83.77 71.04 64.83 39.54 45.31translation edit rate (TER) 0.32 0.55 0.76 0.68 0.62penalized n-gram prec.
(BLEU) 0.33 0.15 0.13 0.14 0.39Table 1: Various indicators of sentence pair comparability for different corpus types.
Statistics are reported forFrench on sets of 100 sentence pairs.We conducted a small-scale study to assess dif-ferent types of corpora of related sentences:1. single language translation Corpora ob-tained by several independent human trans-lation of the same sentences (e.g.
(Barzilayand McKeown, 2001)).2. multiple language translation Same asabove, but where a sentence is translatedfrom 4 different languages into the same lan-guage (Bouamor et al 2010).3. video descriptions Descriptions of shortYouTube videos obtained via MechanicalTurk (Chen and Dolan, 2011).4. multiply-translated subtitles Aligned mul-tiple translations of contributed movie subti-tles (Tiedemann, 2007).5. comparable news headlines News head-lines collected from Google News clusters(e.g.
(Dolan et al 2004)).We collected 100 sentence pairs of each typein French, for which various comparability mea-sures are reported on Table 1.
In particular, the?% aligned tokens?
row indicates the propor-tion of tokens from the sentence pairs that couldbe manually aligned by a native-speaker annota-tor.2 Obviously, the more common tokens twosentences from a pair contain, the fewer sub-sentential paraphrases may be extracted from thatpair.
However, high lexical overlap increases theprobability that two sentences be indeed para-phrases, and in turn the probability that some oftheir phrases be paraphrases.
Furthermore, thetated corpora using them we considered all alignments as be-ing correct.2The same annotator hand-aligned the 5*100=500 para-phrase pairs using the YAWAT (Germann, 2008) manualalignment tool.presence of common token may serve as usefulclues to guide paraphrase extraction.For our experiments, we chose to use parallelmonolingual corpora obtained by single languagetranslation, the most direct resource type for ac-quiring sub-sentential paraphrase pairs.
This al-lows us to define acceptable references for thetask and resort to the most consensual evaluationtechnique for paraphrase acquisition to date.
Us-ing such corpora, we expect to be able to extractprecise paraphrases (see Table 1), which will benatural candidates for further validation, whichwill be addressed in section 5.3.Figure 1 illustrates a reference alignment ob-tained on a pair of English sentential paraphrasesand the list of atomic paraphrase pairs that can beextracted from it, against which acquisition tech-niques will be evaluated.
Note that we do not con-sider pairs of identical units during evaluation, sowe filter them out from the list of reference para-phrase pairs.The example in Figure 1 shows different casesthat point to the inherent complexity of this task,even for human annotators: it could be argued,for instance, that a correct atomic paraphrasepair should be reached ?
amounted to ratherthan reached ?
amounted.
Also, aligning in-dependently 260 ?
0.26 and million ?
billionis assuredly an error, while the pair 260 mil-lion?
0.26 billion would have been appropriate.A case of alignment that seems non trivial can beobserved in the provided example (during the en-tire year ?
annual).
The abovementioned rea-sons will explain in part the difficulties in reach-ing high performance values using such gold stan-dards.Reference composite paraphrase pairs (denotedas R), obtained by joining adjacent atomic para-phrase pairs from Ratom up to 6 tokens3, will3We used standard biphrase extraction heuristics (Koehn718theamountofforeigncapitalactuallyutilizedduringtheentireyearreached260millionusdollars.theannualforeigninvestmentactuallyusedamountedto us$0.26billioncapital ?
investmentutilized ?
usedduring the entire year ?
annualreached ?
amounted260 ?
0.26million ?
billionus dollars ?
us$Figure 1: Reference alignments for a pair of Englishsentential paraphrases from the annotation corpus ofCohn et al(2008) (note that possible and sure align-ments are not distinguished here) and the list of atomicparaphrase pairs extracted from these alignments.also be considered when measuring performance.Evaluated techniques have to output atomic can-didate paraphrase pairs (denoted as Hatom) fromwhich composite paraphrase pairs (denoted asH) are computed.
The usual measures of pre-cision (P ), recall (R) and F-measure (F1) canthen be defined in the following way (Cohn et al2008):P =|Hatom ?R||Hatom|R =|H ?
Ratom||Ratom|F1 =2prp+ rWe conducted experiments using two differentcorpora in English and French.
In each case,a held-out development corpus of 150 sententialparaphrase pairs was used for development andtuning, and all techniques were evaluated on thesame test set consisting of 375 sentential para-phrase pairs.
For English, we used the MTCet al 2007) : all words from a phrase must be aligned to atleast one word from the other and not to words outside, butunaligned words at phrase boundaries are not used.corpus described in (Cohn et al 2008), consist-ing of multiply-translated Chinese sentences intoEnglish, and used as our gold standard both thealignments marked as ?Sure?
and ?Possible?.
ForFrench, we used the CESTA corpus of news ar-ticles4 obtained by translating into French fromEnglish.We used the YAWAT (Germann, 2008) manualalignment tool.
Inter-annotator agreement val-ues (averaging with each annotation set as thegold standard) are 66.1 for English and 64.6 forFrench, which we interpret as acceptable val-ues.
Manual inspection of the two corpora revealsthat the French corpus tends to contain more lit-eral translations, possibly due to the original lan-guages of the sentences, which are closer to thetarget language than Chinese is to English.4 Individual techniques for paraphraseacquisitionAs discussed in section 2, the acquisition of sub-sentential paraphrases is a challenging task thathas previously attracted a lot of work.
In thiswork, we consider the scenario where sententialparaphrases are available and words and phrasesfrom one sentence can be aligned to words andphrases from the other sentence to form atomicparaphrase pairs.
We now describe several tech-niques that perform the task of sub-sentential unitalignment.
We have selected and implementedfive techniques which we believe are representa-tive of the type of knowledge that these techniquesuse, and have reused existing tools, initially devel-oped for other tasks, when possible.4.1 Statistical learning of word alignments(Giza)The GIZA++ tool (Och and Ney, 2004) computesstatistical word alignment models of increasingcomplexity from parallel corpora.
While origi-nally developed in the bilingual context of Statis-tical Machine Translation, nothing prevents build-ing such models on monolingual corpora.
How-ever, in order to build reliable models, it is nec-essary to use enough training material includ-ing minimal redundancy of words.
To this end,we provided GIZA++ with all possible sentencepairs from our mutiply-translated corpus to im-prove the quality of its word alignments (note that4http://www.elda.org/article125.html719we used symmetrized alignments from the align-ments in both directions).
This constitutes a sig-nificant advantage for this technique that tech-niques working on each sentence pair indepen-dently do not have.4.2 Translational equivalence (Pivot)Translational equivalence can be exploited to de-termine that two phrases may be paraphrases.Bannard and Callison-Burch (2005) defined aparaphrasing probability between two phrasesbased on their translation probability through allpossible pivot phrases as:Ppara(p1, p2) =?pivPt(piv|p1)Pt(p2|piv)where Pt denotes translation probabilies.
We usedthe Europarl corpus5 of parliamentary debates inEnglish and French, consisting of approximately1.7 million parallel sentences : this allowed usto use the same resource to build paraphrases forEnglish, using French as the pivot language, andfor French, using English as the pivot language.The GIZA++ tool was used for word alignmentand the MOSES Statistical Machine Translationtoolkit (Koehn et al 2007) was used to com-pute phrase translation probabilities from theseword alignments.
For each sentential paraphrasepair, we applied the following algorithm: for eachphrase, we build the entire set of paraphrases us-ing the previous definition.
We then extract itsbest paraphrase as the one exactly appearing in theother sentence with maximum paraphrase proba-bility, using a minimal threshold value of 10?4.4.3 Linguistic knowledge on term variation(Fastr)The FASTR tool (Jacquemin, 1999) was designedto spot term/phrase variants in large corpora.Variants are described through metarules express-ing how the morphosyntactic structure of a termvariant can be derived from a given term by meansof regular expressions on word morphosyntacticcategories.
Paradigmatic variation can also be ex-pressed by expressing constraints between words,imposing that they be of the same morphologi-cal or semantic family.
Both constraints rely onpreexisting repertoires available for English andFrench.
To compute candidate paraphrase pairsusing FASTR, we first consider all phrases from5http://statmt.org/europarlthe first sentence and search for variants in theother sentence, then do the reverse process andfinally take the intersection of the two sets.4.4 Syntactic similarity (Synt)The algorithm introduced by Pang et al(2003)takes two sentences as input and merges them bytop-down syntactic fusion guided by compatiblesyntactic substructure.
A lexical blocking mecha-nism prevents constituents from fusionning whenthere is evidence of the presence of a word in an-other constituent of one of the sentence.
We usethe Berkeley Probabilistic parser (Klein and Man-ning, 2003) to obtain syntactic trees for Englishand its adapted version for French (Candito et al2010).
Because this process is highly sensitive tosyntactic parse errors, we use in our implemen-tation k-best parses and retain the most compactfusion from any pair of candidate parses.4.5 Edit rate on word sequences (TERp)TERp (Translation Edit Rate Plus) (Snover et al2010) is a score designed for the evaluation ofMachine Translation output.
Its typical use takesa system hypothesis to compute an optimal set ofword edits that can transform it into some exist-ing reference translation.
Edit types include ex-act word matching, word insertion and deletion,block movement of contiguous words (computedas an approximation), as well as optionally vari-ants substitution through stemming, synonym orparaphrase matching.6 Each edit type is parame-terized by at least one weight which can be opti-mized using e.g.
hill climbing.
TERp being a tun-able metric, our experiments will include tuningTERp systems towards either precision (?
P ),recall (?
R), or F-measure (?
F1).74.6 Evaluation of individual techniquesResults for the 5 individual techniques are givenon the left part of Table 2.
It is first apparentthat all techniques but TERp fared better on theFrench corpus than on the English corpus.
Thiscan certainly be explained by the fact that the for-mer results from more literal translations (from6Note that for these experiments we did not use the stem-ming module, the interface to WordNet for synonym match-ing and the provided paraphrase table for English, due to thefact that these resources were available for English only.7Hill climbing was used for all tunings as done by Snoveret al(2010), and we used one iteration starting with uniformweights and 100 random restarts.720Individual techniques CombinationsGIZA PIVOT FASTR SYNT TERp union validation?
P ?
R ?
F1EnglishP 31.01 31.78 37.38 52.17 50.00 29.15 33.37 21.44 50.51R 38.30 18.50 6.71 2.53 5.83 45.19 45.37 60.87 41.19F1 34.27 23.39 11.38 4.83 10.44 35.44 38.46 31.71 45.37FrenchP 28.99 29.53 52.48 62.50 31.35 30.26 31.43 17.58 40.77R 45.98 26.66 8.59 8.65 44.22 44.60 44.10 63.36 45.85F1 35.56 28.02 14.77 15.20 36.69 36.05 36.70 27.53 43.16Table 2: Results on the test set on English and French for the 5 individual paraphrase acquisition techniques (leftpart) and for the 2 combination techniques (right part).English to French, compared with from Chineseto English), which should be consequently eas-ier to word-align.
This is for example clearlyshown by the results of the statistical alignerGIZA, which obtains a 7.68 advantage on recallfor French over English.The two linguistically-aware techniques,FASTR and SYNT, have a very strong precisionon the more parallel French corpus, but fail toachieve an acceptable recall on their own.
Thisis not surprising : FASTR metarules are focussedon term variant extraction, and SYNT requirestwo syntactic trees to be highly comparableto extract sub-sentential paraphrases.
Whenthese constrained conditions are met, these twotechniques appear to perform quite well in termsof precision.GIZA and TERp perform roughly in the samerange on French, with acceptable precision andrecall, TERp performing overall better, with e.g.a 1.14 advantage on F-measure on French and4.19 on English.
The fact that TERp performscomparatively better on English than on French8,with a 1.76 advantage on F-measure, is not con-tradictory: the implemented edit distance makesit possible to align reasonably distant words andphrases independently from syntax, and to findalignments for close remaining words, so the dif-ferences of performance between the two lan-guages are not necessarily expected to be com-parable with the results of a statistical alignmenttechnique.
English being a poorly-inflected lan-guage, alignment clues between two sententialparaphrases are expected to be more numerous8Recall that all specific linguistic modules for Englishonly from TERp had been disabled, so the better perfor-mance on English cannot be explained by a difference interms of resources used.than for highly-inflected French.PIVOT is on par with GIZA as regards preci-sion, but obtains a comparatively much lower re-call (differences of 19.32 and 19.80 on recall onFrench and English respectively).
This may firstbe due in part to the paraphrasing score thresholdused for PIVOT, but most certainly to the use ofa bilingual corpus from the domain of parliamen-tary debates to extract paraphrases when our testsets are from the news domain: we may be ob-serving differences inherent to the domain, andpossibly facing the issue of numerous ?out-of-vocabulary?
phrases, in particular for named en-tities which frequently occur in the news domain.Importantly, we can note that we obtain at besta recall of 45.98 on French (GIZA) and of 45.37on English (TERp).
This may come as a disap-pointment but, given the broad set of techniquesevaluated, this should rather underline the inher-ent complexity of the task.
Also, recall that themetrics used do not consider identity paraphrases(e.g.
at the same time ?
at the same time), aswell as the fact that gold standard alignment isa very difficult process as shown by interjudgeagreement values and our example from section 3.This, again, confirms that the task that is ad-dressed is indeed a difficult one, and provides fur-ther justification for initially focussing on parallelmonolingual corpora, albeit scarce, for conduct-ing fine-grained studies on sub-sentential para-phrasing.Lastly, we can also note that precision is notvery high, with (at best, using TERp?P ) averagevalues for all techniques of 40.97 and 40.46 onFrench and English, respectively.
Several factsmay provide explanations for this observation.First, it should be noted that none of those tech-niques, except SYNT, was originally developed721for the task of sub-sentential paraphrase acqui-sition from monolingual parallel corpora.
Thisresults in definitions that are at best closely re-lated to this task.9 Designing new techniqueswas not one of the objectives of our study, so wehave reused existing techniques, originally devel-oped with different aims (bilingual parallel cor-pora word alignment (GIZA), term variant recog-nition (FASTR), Machine Translation evaluation(TERp)).
Also, techniques such as GIZA andTERp attempt to align as many words as possi-ble in a sentence pair, when gold standard align-ments sometimes contain gaps.10 Finally, the met-rics used will count as false small variations ofgold standard paraphrases (e.g.
missing functionword): the acceptability or not of such candi-dates could be either evaluated in a scenario wheresuch ?acceptable?
variants would be taken intoaccount, and could be considered in the contextof some actual use of the acquired paraphrasesin some application.
Nonetheless, on average thetechniques in our study produce more candidatesthat are not in the gold standard: this will be animportant fact to keep in mind when tackling thetask of combining their outputs.
In particular, wewill investigate the use of features indicating thecombination of techniques that predicted a givenparaphrase pair, aiming to capture consensus in-formation.5 Paraphrase validation5.1 Technique complementarityBefore considering combining and validating theoutputs of individual techniques, it is informativeto look at some notion of ?complementarity?
be-tween techniques, in terms of how many correctparaphrases a technique would add to a combinedset.
The following formula was used to accountfor the complementarity between the set of can-didates from some technique i, ti, and the set forsome technique j, tj :C(ti, tj) = recall(ti?tj)?max(recall(ti), recall(tj))9Recall, however, that our best performing technique onF-measure, TERp, was optimized to our task using a heldout development set.10It is arguable whether such cases should happen in sen-tence pairs obtained by translating the same original sentenceinto the same language, but this clearly depends on the inter-pretation of the expected level of annotation by the annota-tors.Results on the test set for the two languagesare given in Table 3.
A number of pairs of tech-niques have strong complementarity values, thestrongest one being for GIZA and TERp for bothlanguages.
According to these figures, PIVOTidentify paraphrases which are slightly more sim-ilar to those of TERp than those of GIZA.
Inter-estingly, FASTR and SYNT exhibit a strong com-plementarity, where in French, for instance, theyonly have a very small proportion of paraphrasesin common.
Considering the set of all other tech-niques, GIZA provides the more new paraphraseson French and TERp on English.GIZA PIVOT FASTR SYNT TERp?R all othersEnglishGIZA - 4.65 2.83 0.59 10.31 8.31PIVOT 4.65 - 2.30 1.88 3.12 3.72FASTR 2.83 2.30 - 2.42 1.71 0.53SYNT 0.59 1.88 2.42 - 0.59 0.00TERp?R 10.31 3.12 1.71 0.59 - 12.20FrenchGIZA - 9.79 3.64 2.20 10.73 8.91PIVOT 9.79 - 2.26 5.22 7.84 3.39FASTR 3.64 2.26 - 7.28 3.01 0.19SYNT 2.20 5.22 7.28 - 1.76 0.44TERp?R 10.73 7.84 3.01 1.76 - 5.65Table 3: Values of complementarity on the test set forboth languages, where the following formula was usedfor the set of technique outputs T = {t1, t2, ..., tn} :C(ti, tj) = recall(ti?tj)?max(recall(ti), recall(tj)).Complementarity values are computed between allpairs of individual techniques, and each individualtechnique and the set of all other techniques.
Values inbold indicate highest values for the technique of eachrow.5.2 Naive combination by unionWe first implemented a naive combination ob-tained by taking the union of all techniques.
Re-sults are given in the first column of the right partof Table 2.
The first result is quite encouraging:in both languages, more than 6 paraphrases fromthe gold standard out of 10 are found by at leastone of the techniques, which, given our previousdiscussion, constitutes a good result and providea clear justification for combining different tech-niques for improving performance on this task.Precision is mechanically lowered to account forroughly 1 correct paraphrase over 5 candidatesfor both languages.
F-measure values are muchlower than those of TERp and GIZA, showingthat the union of all techniques is only interest-ing for recall-oriented paraphrase acquisition.
In722the next section, we will show how the results ofthe union can be validated using machine learningto improve these figures.5.3 Paraphrase validation via automaticclassificationA natural improvement to the naive combinationof paraphrase candidates from all techniques canconsist in validating candidate paraphrases by us-ing several models that may be good indicators oftheir paraphrasing status.
We can therefore castour problem as one of biclass classification (i.e.?paraphrase?
vs. ?not paraphrase?
).We have used a maximum entropy classifier11with the following features, aiming at capturinginformation on the paraphrase status of a candi-date pair:Morphosyntactic equivalence (POS) It maybe the case that some sequences of part-of-speechcan be rewritten as different sequences, e.g.
asa result of verb nominalization.
We thereforeuse features to indicate the sequences of part-of-speech for a pair of candidate paraphrases.
Weused the preterminal symbols of the syntactictrees of the parser used for SYNT.Character-based distance (CAR) Morpholog-ical variants often have close word forms, andmore generally close word forms in sententialparaphase pairs may indicate related words.
Weused features for discretized values of the editdistance between the two phrases of a candidateparaphrase pair as measured by the Levenshteindistance.Stem similarity (STEM) Inflectional morphol-ogy, which is quite productive in languages suchas French, can increase vocabulary size signifi-cantly, while in sentential paraphrases commonstems may indicate related words.
We used abinary feature indicating whether the stemmedphrases of a candidate paraphrase pair match.12Token set identity (BOW) Syntactic rearrange-ments may involve the same sets of words in var-ious orders.
We used discretized features indicat-ing the proportion of common tokens in the set11We used the implementation available at:http://homepages.inf.ed.ac.uk/lzhang10/maxent_toolkit.html12We use the implementations of the Snowball stem-mer from English and French available from: http://snowball.tartarus.orgof tokens for the two phrases of a candidate para-phrase pair.Context similarity (CTXT) It can be derivedfrom the distributionality hypothesis that the moretwo phrases will be seen in similar contexts, themore they are likely to be paraphrases.
We useddiscretized features indicating how similar thecontexts of occurrences of two paraphrases are.For this, we used the full set of bilingual English-French data available for the translation task ofthe Workshop on Statistical Machine Transla-tion13, totalling roughly 30 million parallel sen-tences: this again ensures that the same resourcesare used for experiments in the two languages.
Wecollect all occurrences for the phrases in a pair,and build a vector of content words cooccurringwithin a distance of 10 words from each phrase.We finally compute the cosine between the vec-tors of the two phrases of a candidate paraphrasepair.Relative position in a sentence (REL) De-pending on the language in which parallel sen-tences are analyzed, it may be the case that sub-sentential paraphrases occur at close locations intheir respective sentence.
We used a discretizedfeature indicating the relative position of the twophrases in their original sentence.Identity check (COOC) We used a binary fea-ture indicating whether one of the two phrasesfrom a candidate pair, or the two, occurred atsome other location in the other sentence.Phrase length ratio (LEN) We used a dis-cretized feature indicating phrase length ratio.Source techniques (SRC) Finally, as our set-ting validates paraphrase candidates produced bya set of techniques, we used features indicat-ing which combination of techniques predicted aparaphrase candidate.
This can allow learning thatparaphrases in the intersection of the predictedsets for some techniques may produce good re-sults.We used a held out training set consisting of150 sentential paraphrase pairs from the same cor-pora as our previous developement and test setsfor both languages.
Positive examples were takenfrom the candidate paraphrase pairs from any of13http://www.statmt.org/wmt11/translation-task.html723the 5 techniques in our study which belong tothe gold standard, and we used a correspondingnumber of negative examples (randomly selected)from candidate pairs not in the gold standard.
Theright part of Table 2 provides the results for ourvalidation experiments of the union set for all pre-vious techniques.We obtain our best results for this study usingthe output of our validation classifier over the setof all candidate paraphrase pairs.
On French, ityields an improvement in F-measure (43.16) of+6.46 over the best individual technique (TERp)and of +15.63 over the naive union from all indi-vidual techniques.
On English, the improvementin F-measure (45.37) is for the same conditions ofrespectively +6.91 (over TERp) and +13.66.
Weunfortunately observe an important decrease in re-call over the naive union, of respectively -17.54and -19.68 for French and English.
Increasing ouramount of training data to better represent the fullrange of paraphrase types may certainly overcomethis in part.
This would indeed be sensible, as bet-ter covering the variety of paraphrase types as aone-time effort would help all subsequent valida-tions.
Figure 2 shows how performance varies onFrench with number of training examples for var-ious feature configurations.
However, some para-phrase types will require integration of more com-plex knowledge, as is the case, for instance, forparaphrase pairs involving some anaphora and itsantecedent (e.g.
China?
it).While these results, which are very comparablefor the two languages studied, are already satisfy-ing given the complexity of our task, further in-spection of false positives and negatives may helpus to develop additional models that will help usobtain a better classification performance.6 Conclusions and future workIn this article, we have addressed the task of com-bining the results of sub-sentential paraphrase ac-quition from parallel monolingual corpora using alarge variety of techniques.
We have provided jus-tifications for using highly parallel corpora con-sisting of multiply translated sentences from asingle language.
All our experiments were con-ducted on both English and French using com-parable resources, so although the results cannotbe directly compared they give some acceptablecomparison points.
The best recall of any indi-vidual technique is around 45 for both language,10 20 30 40 50 60 70 80 90 10031333537394143All\POS\SRC\CTXT\STEM\LEN\COOCF-measure% of examples from training corpusFigure 2: Learning curves obtained on French by re-moving features individually.and F-measure in the range 36-38, indicating thatthe task under study is a very challenging one.Our validation strategy based on bi-class classi-fication using a broad set of features applicable toall candidate paraphrase pairs allowed us to obtaina 18% relative improvement in F-measure overthe best individual technique for both languages.Our future work include performing a deepererror analysis of our current results, to better com-prehend what characteristics of paraphrase stilldefy current validation.
Also, we want to inves-tigate adding new individual techniques to pro-vide so far unseen candidates.
Another possibleapproach would be to submit all pairs of sub-sentential paraphrase pairs from a sentence pairto our validation process, which would obviouslyrequire some optimization and devising sensibleheuristics to limit time complexity.
We also in-tend to collect larger corpora for all other corpustypes appearing in Table 1 and conducting anewour acquisition and validation tasks.AcknowledgementsThe authors would like to thank the reviewers fortheir comments and suggestions, as well as Guil-laume Wisniewski for helpful discussions.
Thiswork was partly funded by ANR project Edylex(ANR-09-CORD-008).ReferencesIon Androutsopoulos and Prodromos Malakasiotis.2010.
A Survey of Paraphrasing and Textual En-724tailment Methods.
Journal of Artificial IntelligenceResearch, 38:135?187.Colin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with Bilingual Parallel Corpora.
In Pro-ceedings of ACL, Ann Arbor, USA.Regina Barzilay and Lillian Lee.
2003.
Learn-ing to paraphrase: an unsupervised approach us-ing multiple-sequence alignment.
In Proceedingsof NAACL-HLT, Edmonton, Canada.Regina Barzilay and Kathleen R. McKeown.
2001.Extracting paraphrases from a parallel corpus.
InProceedings of ACL, Toulouse, France.Rahul Bhagat and Deepak Ravichandran.
2008.
Largescale acquisition of paraphrases for learning surfacepatterns.
In Proceedings of ACL-HLT, Columbus,USA.Rahul Bhagat.
2009.
Learning Paraphrases from Text.Ph.D.
thesis, University of Southern California.Houda Bouamor, Aure?lien Max, and Anne Vilnat.2010.
Comparison of Paraphrase Acquisition Tech-niques on Sentential Paraphrases.
In Proceedings ofIceTAL, Rejkavik, Iceland.Chris Callison-Burch, Trevor Cohn, and Mirella La-pata.
2008.
Parametric: An automatic evaluationmetric for paraphrasing.
In Proceedings of COL-ING, Manchester, UK.Chris Callison-Burch.
2007.
Paraphrasing and Trans-lation.
Ph.D. thesis, University of Edinburgh.Chris Callison-Burch.
2008.
Syntactic Constraintson Paraphrases Extracted from Parallel Corpora.
InProceedings of EMNLP, Hawai, USA.Marie Candito, Beno?
?t Crabbe?, and Pascal Denis.2010.
Statistical French dependency parsing: tree-bank conversion and first results.
In Proceedings ofLREC, Valletta, Malta.David Chen and William Dolan.
2011.
Collectinghighly parallel data for paraphrase evaluation.
InProceedings of ACL, Portland, USA.Trevor Cohn, Chris Callison-Burch, and Mirella Lap-ata.
2008.
Constructing corpora for the develop-ment and evaluation of paraphrase systems.
Com-putational Linguistics, 34(4).Bill Dolan, Chris Quirk, and Chris Brockett.
2004.Unsupervised construction of large paraphrase cor-pora: Exploiting massively parallel news sources.In Proceedings of COLING, Geneva, Switzerland.Ulrich Germann.
2008.
Yawat : Yet Another WordAlignment Tool.
In Proceedings of the ACL-HLT,demo session, Columbus, USA.Christian Jacquemin.
1999.
Syntagmatic and paradig-matic representations of term variation.
In Proceed-ings of ACL, College Park, USA.Dan Klein and Christopher D. Manning.
2003.
Accu-rate unlexicalized parsing.
In Proceedings of ACL,Sapporo, Japan.Philipp Koehn, Hieu Hoang, Alexandra Birch,Chris Callison-Burch, Marcello Federico, NicolaBertoldi, Brooke Cowan, Wade Shen, ChristineMoran, Richard Zens, Chris Dyer, Ondrej Bojar,Alexandra Constantin, and Evan Herbst.
2007.Moses: Open Source Toolkit for Statistical MachineTranslation.
In Proceedings of ACL, demo session,Prague, Czech Republic.Dekang Lin and Patrick Pantel.
2001.
Discovery of in-ference rules for question answering.
Natural Lan-guage Engineering, 7(4):343?360.Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng.2010.
PEM: A paraphrase evaluation metric ex-ploiting parallel texts.
In Proceedings of EMNLP,Cambridge, USA.Nitin Madnani and Bonnie J. Dorr.
2010.
Generat-ing Phrasal and Sentential Paraphrases: A Surveyof Data-Driven Methods .
Computational Linguis-tics, 36(3).Nitin Madnani.
2010.
The Circle of Meaning: FromTranslation to Paraphrasing and Back.
Ph.D. the-sis, University of Maryland College Park.Donald Metzler, Eduard Hovy, and Chunliang Zhang.2011.
An empirical evaluation of data-driven para-phrase generation techniques.
In Proceedings ofACL-HLT, Portland, USA.Franz Josef Och and Herman Ney.
2004.
The align-ment template approach to statistical machine trans-lation.
Computational Linguistics, 30(4).Bo Pang, Kevin Knight, and Daniel Marcu.
2003.Syntax-based alignement of multiple translations:Extracting paraphrases and generating new sen-tences.
In Proceedings of NAACL-HLT, Edmonton,Canada.Matthew Snover, Nitin Madnani, Bonnie J. Dorr, andRichard Schwartz.
2010.
TER-Plus: paraphrase,semantic, and alignment enhancements to Transla-tion Edit Rate.
Machine Translation, 23(2-3).Jo?rg Tiedemann.
2007.
Building a Multilingual Paral-lel Subtitle Corpus.
In Proceedings of the Confer-ence on Computational Linguistics in the Nether-lands, Leuven, Belgium.Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.2008.
Pivot Approach for Extracting ParaphrasePatterns from Bilingual Corpora.
In Proceedingsof ACL-HLT, Columbus, USA.725
