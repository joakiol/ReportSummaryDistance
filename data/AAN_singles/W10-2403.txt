Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 21?28,Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational LinguisticsReport of NEWS 2010 Transliteration Mining Shared TaskA Kumaran Mitesh M. Khapra Haizhou LiMicrosoft Research IndiaBangalore, IndiaIndian Institute of Technology BombayMumbai, IndiaInstitute for InfocommResearch, SingaporeAbstractThis report documents the details of the Trans-literation Mining Shared Task that was run asa part of the Named Entities Workshop(NEWS 2010), an ACL 2010 workshop.
Theshared task featured mining of name translite-rations from the paired Wikipedia titles in 5different language pairs, specifically, betweenEnglish and one of Arabic, Chinese, HindiRussian and Tamil.
Totally 5 groups took partin this shared task, participating in multiplemining tasks in different languages pairs.
Themethodology and the data sets used in thisshared task are published in the Shared TaskWhite Paper [Kumaran et al 2010].
We meas-ure and report 3 metrics on the submitted re-sults to calibrate the performance of individualsystems on a commonly available Wikipediadataset.
We believe that the significant contri-bution of this shared task is in (i) assembling adiverse set of participants working in the areaof transliteration mining, (ii) creating a base-line performance of transliteration mining sys-tems in a set of diverse languages using com-monly available Wikipedia data, and (iii) pro-viding a basis for meaningful comparison andanalysis of trade-offs between various algo-rithmic approaches used in mining.
We be-lieve that this shared task would complementthe NEWS 2010 transliteration generationshared task, in enabling development of prac-tical systems with a small amount of seed datain a given pair of languages.1 IntroductionProper names play a significant role in MachineTranslation (MT) and Information Retrieval (IR)systems.
When the systems involve multiplelanguages, The MT and IR system rely on Ma-chine Transliteration systems, as the propernames are not usually available in standard trans-lation lexicons.
The quality of the MachineTransliteration systems plays a significant part indetermining the overall quality of the system,and hence, they are critical for most multilingualapplication systems.
The importance of MachineTransliteration systems has been well understoodby the community, as evidenced by significantpublication in this important area.While research over the last two decades hasshown that reasonably good quality MachineTransliteration systems may be developed easily,they critically rely on parallel names corpora fortheir development.
The Machine TransliterationShared Task of the NEWS 2009 workshop(NEWS 2009) has shown that many interestingapproaches exist for Machine Transliteration,and about 10-25K parallel names is sufficient formost state of the art systems to provide a practic-al solution for the critical need.
The traditionalsource for crosslingual parallel data ?
the bilin-gual dictionaries ?
offer only limited support asthey do not include proper names (other thanones of historical importance).
The statisticaldictionaries, though they contain parallel names,do not have sufficient coverage, as they dependon some threshold statistical evidence 1 .
Newnames and many variations of them are intro-duced to the vocabulary of a language every daythat need to be captured for any good qualityend-to-end system such as MT or CLIR.
Sothere is a perennial need for harvesting parallelnames data, to support end-user applications andsystems well and accurately.This is the specific focus of the TransliterationMining Shared Task in NEWS 2010 workshop(an ACL 2010 Workshop): To mine accuratelyparallel names from a popular, ubiquitous source,the Wikipedia.
Wikipedia exists in more than250 languages, and every Wikipedia article has alink to an equivalent article in other languages2.We focused on this specific resource ?
the Wiki-pedia titles in multiple languages and the inter-linking between them ?
as the source of parallelnames.
Any successful mining of parallel namesfrom title would signal copious availability ofparallel names data, enabling transliteration gen-eration systems in many languages of the world.1 In our experiments with Indian Express news corpo-ra over 2 years shows that 80% of the names occurless than 5 times in the entire corpora.2 Note that the titles contain concepts, events, dates,etc., in addition to names.
Even when the titles arenames, parts of them may not be transliterations.212 Transliteration Mining Shared TaskIn this section, we provide details of the sharedtask, and the datasets used for the task and resultsevaluation.2.1 Shared Task: Task DetailsThe task featured in this shared task was to de-velop a mining system for identifying singleword transliteration pairs from the standard inter-linked Wikipedia topics (aka, Wikipedia Inter-Language Links, or WIL3) in one or more of thespecified language pairs.
The WIL?s link articleson the same topic in multiple languages, and aretraditionally used as a parallel language resourcefor many natural language processing applica-tions, such as Machine Translation, CrosslingualSearch, etc.
Specific WIL?s of interest for ourtask were those that contained proper names ?either wholly or partly ?
which can yield richtransliteration data.The task involved transliteration mining in thelanguage pairs summarized in Table 1.SourceLanguageTarget Lan-guageTrack IDEnglish  Chinese  WM-EnCnEnglish  Hindi  WM-EnHiEnglish  Tamil WM-EnTaEnglish  Russian  WM-EnRuEnglish Arabic WM-EnArTable 1: Language Pairs in the shared taskEach WIL consisted of a topic in the sourceand target language pair, and the task was toidentify parts of the topic (in the respective lan-guage titles) that are transliterations of each oth-er.
A seed data set (of about 1K transliterationpairs) was provided for each language pair, andwas the only resource to be used for developing amining system.
The participants were expectedto produce a paired list of source-target singleword named entities, for every WIL provided.
Atthe evaluation time, a random subset of WIL?s(about 1K WIL?s) in each language pair werehand labeled, and used to test the results pro-duced by the participants.Participants were allowed to use only the 1Kseed data provided by the organizers to produce?standard?
results; this restriction is imposed toprovide a meaningful way of comparing the ef-3 Wikipedia?s Interlanguage Links:http://en.wikipedia.org/wiki/Help:Interlanguage_links.fective methods and approaches.
However,?non-standard?
runs were permitted where par-ticipants were allowed to use more seed data orany language-specific resource available to them.2.2 Data Sets for the TaskThe following datasets were used for each lan-guage pair, for this task.Training Data  Size RemarksSeed Data(Parallelnames)~1K Paired names be-tween source andtarget languages.To-be-minedWikipedia In-ter-Wiki-LinkData (Noisy)Vari-ablePaired named entitiesbetween source andtarget languages ob-tained directly fromWikipediaTest Data~1K This was a subset ofWikipedia Inter-Wiki-Link data,which was hand la-beled for evaluation.Table 2: Datasets created for the shared taskThe first two sets were provided by the orga-nizers to the participants, and the third was usedfor evaluation.Seed transliteration data:  In addition we pro-vided approximately 1K parallel names in eachlanguage pair as seed data to develop any metho-dology to identify transliterations.
For standardrun results, only this seed data was to be used,though for non-standard runs, more data or otherlinguistics resources were allowed.English Names Hindi Namesvillage ????
?linden ?????
?market ??????
?mysore ????
?Table 3: Sample English-Hindi seed dataEnglish Names Russian Namesgregory ???????
?hudson ?????
?victor ?????
?baranowski ??????????
?Table 4: Sample English-Russian seed dataTo-Mine-Data WIL data:  All WIL?s were ex-tracted from the Wikipedia around January 2010,22and provided to the participants.
The extractednames were provided as-is, with no hand verifi-cation about their correctness, completeness orconsistency.
As sample of the WIL data for Eng-lish-Hindi and English-Russian is shown inTables 5 and 6 respectively.
Note that there are0, 1 or more single-word transliterations fromeach WIL.# English WikipediaTitleHindi WikipediaTitle1 Indian National Congress ??????
?????????
????????
?2 University of Oxford ?????????
????????????
?3 Indian Institute of Science??????
??????????????
?4 Jawaharlal Nehru University????????
?????????????????
?Table 5: English-Hindi Wikipedia title pairs# English WikipediaTitleRussian WikipediaTitle1 Mikhail Gorbachev???????
?, ??????????????
?2 George Washington ????????
?, ?????
?3 Treaty of Versailles ???????????
??????
?4 French Republic ??????
?Table 6: English-Russian Wikipedia title pairsTest set:  We randomly selected ~1000 wikipe-dia links (from the large noisy Inter-wiki-links)as test-set, and manually extracted the singleword transliteration pairs associated with each ofthese WILs.
Please note that a given WIL canprovide 0, 1 or more single-word transliterationpairs.
To keep the task simple, it was specifiedthat only those transliterations would be consi-dered correct that were clear transliterationsword-per-word (morphological variations one orboth sides are not considered transliterations)These 1K test set was be a subset of Wikipediadata provided to the user.
The gold dataset is asshown in Tables 7 and 8.WIL# English Names Hindi Names1 Congress ????????
?2 Oxford ????????
?3 <Null> <Null>4 Jawaharlal ???????
?4 Nehru ????
?Table 7: Sample English-Hindi transliterationpairs mined from Wikipedia title pairsWIL# English Names Russian Names1 Mikhail ?????
?1 Gorbachev ???????
?2 George ?????
?2 Washington ????????
?3 Versailles ??????????
?4 <Null> <Null>Table 8: Sample English-Russian translitera-tion pairs mined from Wikipedia title pairs2.3 Evaluation:The participants were expected to mine such sin-gle-word transliteration data for every specificWIL, though the evaluation was done onlyagainst the randomly selected, hand-labeled testset.
A participant may submit a maximum of 10runs for a given language pair (including a min-imum of one mandatory ?standard?
run).
Therecould be more standard runs, without exceeding10 (including the non-standard runs).At evaluation time, the task organizerschecked every WIL in test set from among theuser-provided results, to evaluate the quality ofthe submission on the 3 metrics described later.3 Evaluation MetricsWe measured the quality of the mining task us-ing the following measures:1.
PrecisionCorrectTransliterations(PTrans)2.
RecallCorrectTransliteration  (RTrans)3.
F-ScoreCorrectTransliteration (FTrans).Please refer to the following figures for the ex-planations:A = True Positives (TP) = Pairs that were identi-fied as "Correct Transliterations" by the partici-pant and were indeed "Correct Transliterations"as per the gold standardB = False Positives (FP) = Pairs that were identi-fied as "Correct Transliterations" by the partici-pant but they were "Incorrect Transliterations" asper the gold standard.C = False Negatives (FN) = Pairs that were iden-tified as "Incorrect Transliterations" by the par-ticipant but were actually "Correct Translitera-tions" as per the gold standard.D = True Negatives (TN) = Pairs that were iden-tified as "Incorrect Transliterations" by the par-ticipant and were indeed "Incorrect Translitera-tions" as per the gold standard.23Figure 1: Overview of the mining task and evaluation1.
RecallCorrectTransliteration  (RTrans)The recall was computed using the sample asfollows:??????
=????
+ ??=??
+ ?=??2.
PrecisionCorrectTransliteration  (PTrans)The precision was computed using the sample asfollows:??????
=????
+ ??=??
+ ?3.
F-Score (F)?
=2 ?
??????
?
????????????
+ ?????
?4 Participants & ApproachesThe following 5 teams participated in the Trans-literation Mining Task?
:# Team Organization1   Alberta University of Alberta, Canada2   CMIC Cairo Microsoft InnovationCentre, Egypt3   Groningen University of Groningen,Netherlands4   IBM Egypt IBM Egypt, Cairo, Egypt5   MINT?
Microsoft Research India, India?
Non-participating system, included for reference.Table 9: Participants in the Shared TaskThe approaches used by the 4 participatinggroups can be broadly classified as discrimina-tive and generation based approaches.
Discri-minative approaches treat the mining task as abinary classification problem where the goal is tobuild a classifier that identifies whether a givenpair is a valid transliteration pair or not.
Genera-tion based approaches on the other hand generatetransliterations for each word in the source titleand measure their similarity with the candidatewords in the target title.
Below, we give a sum-mary of the various participating systems.The CMIC team (Darwish et.
al., 2010) used agenerative transliteration model (HMM) to trans-literate each word in the source title and com-pared the transliterations with the words appear-ing in the target title.
For example, for a givenword Ei in the source title if the model generatesa transliteration Fj which appears in the targettitle then (Ei, Fj) are considered as transliterationpairs.
The results are further improved by usingphonetic conflation (PC) and iteratively training(IterT) the generative model using the minedtransliteration pairs.
For phonetic conflation amodified SOUNDEX scheme is used whereinvowels are discarded and phonetically similarcharacters are conflated.
Both, phonetic confla-tion and iterative training, led to an increase in24recall which was better than the correspondingdecline in precision.The Alberta team (Jiampojamarn et.
al., 2010)fielded 5 different systems in the shared task.The first system uses a simple edit distance basedmethod where a pair of strings is classified as atransliteration pair if the Normalized Edit Dis-tance (NED) between them is above a certainthreshold.
To calculate the NED, the target lan-guage string is first Romanized by replacing eachtarget grapheme by the source grapheme havingthe highest conditional probability.
These condi-tional probabilities are obtained by aligning theseed set of transliteration pairs using an M2M-aligner approach (Jiampojamarn et.
al., 2007).The second system uses a SVM based discrimin-ative classifier trained using an improved featurerepresentation (BK 2007) (Bergsma and Kon-drak, 2007).
These features include all substringpairs up to a maximum length of three as ex-tracted from the aligned word pairs.
The transli-teration pairs in the seed data provided for theshared task were used as positive examples.
Thenegative examples were obtained by generatingall possible source-target pairs in the seed dataand taking those pairs which are not translitera-tions but have a longest common subsequenceratio above a certain threshold.
One drawback ofthis system is that longer substrings cannot beused due to the combinatorial explosion in thenumber of unique features as the substring lengthincreases.
To overcome this problem they pro-pose a third system which uses a standard n-gramstring kernel (StringKernel) that implicitly em-beds a string in a feature space that has one co-ordinate for each unique n-gram (Shawe-Taylorand Cristianini, 2004).
The above 3 systems areessentially discriminative systems.
In addition,they propose a generation based approach (DI-RECTL+) which determines whether the gener-ated transliteration pairs of a source word andtarget word are similar to a given candidate pair.They use a state-of-the-art online discriminativesequence prediction model based on many-to-many alignments, further augmented by the in-corporation of joint n-gram features (Jiampoja-marn et.
al., 2010).
Apart from the four systemsdescribed above, they propose an additional sys-tem for English Chinese, wherein they formulatethe mining task as a matching problem (Match-ing) and greedily extract the pairs with highestsimilarity.
The similarity is calculated using thealignments obtained by training a generationmodel (Jiampojamarn et.
al., 2007) using theseed data.The IBM Cairo team (Noemans et.
al., 2010)proposed a generation based approach whichtakes inspiration from Phrase Based StatisticalMachine Translation (PBSMT) and learns a cha-racter-to-character alignment model between thesource and target language using GIZA++.
Thisalignment table is then represented using a finitestate automaton (FSA) where the input is thesource character and the output is the target cha-racter.
For a given word in the source title, can-didate transliterations are generated using thisFST and are compared with the words in the tar-get title.
In addition they also submitted a base-line run which used phonetic edit distance.The Groningen (Nabende et.
al., 2010) teamused a generation based approach that uses pairHMMs (P-HMM) to find the similarity betweena given pair of source and target strings.
Theproposed variant of pair HMM uses transitionparameters that are distinct between each of theedit states and emission parameters that are alsodistinct.
The three edits states are substitutionstate, deletion state and insertion state.
The pa-rameters of the pair HMM are estimated usingthe Baum-Welch Expectation Maximization al-gorithm (Baum et.
al.
1970).Finally, as a reference, results of a previouslypublished system ?
MINT (Udupa et.
al., 2009) ?were also included in this report as a reference.MINT is a large scalable mining system for min-ing transliterations from comparable corpora,essentially multilingual news articles in the sametimeline.
While MINT takes a two step approach?
first aligning documents based on content simi-larity, and subsequently mining transliterationsbased on a name similarity model ?
for this task,only the transliteration mining step is employed.For mining transliterations a logistic functionbased similarity model (LFS) trained discrimina-tively with the seed parallel names data was em-ployed.
It should be noted here that the MINTalgorithm was used as-is for mining translitera-tions from Wikipedia paired titles, with no fine-tuning.
While the standard runs used only thedata provided by the organizers, the non-standardruns used about 15K (Seed+) parallel names be-tween the languages.5 Results & AnalysisThe results for EnAr, EnCh, EnHi, EnRu andEnTa are summarized in Tables 10, 11, 12, 13and 14 respectively.
The results clearly indicatethat there is no single approach which performswell across all languages.
In fact, there is even25no single genre (discriminative v/s generationbased) which performs well across all languages.We, therefore, do a case by case analysis of theresults and highlight some important observa-tions.?
The discriminative classifier using stringkernels proposed by Jiampojamarn et.
al.
(2010) consistently performed well in all the4 languages that it was tested on.
Specifical-ly, it gave the best performance for EnHi andEnTa.?
The simple discriminative approach based onNormalized Edit Distance (NED) gave thebest result for EnRu.
Further, the authors re-port that the results of StringKernel and BK-2007 were not significantly better than NED.?
The use of phonetic conflation consistentlyperformed better than the case when phonet-ic conflation was not used.?
The results for EnCh are significantly lowerwhen compared to the results for other lana-guge pairs.
This shows that mining translite-ration pairs between alphabetic languages(EnRu, EnAr, EnHi, EnTa) is relatively easi-er as compared to the case when one of thelanguages is non-alphabetic (EnCh)6 Plans for the Future EditionsThis shared task was designed as a comple-mentary shared task to the popular NEWSShared Tasks on Transliteration Generation; suc-cessful mining of transliteration pairs demon-strated in this shared task would be a viablesource for generating data for developing a stateof the art transliteration generation system.We intend to extend the scope of the mining in3 different ways: (i) extend mining to more lan-guage pairs, (ii) allow identification of neartransliterations where there may be changes do tothe morphology of the target (or the source) lan-guages, and, (iii) demonstrate an end-to-endtransliteration system that may be developedstarting with a small seed corpora of, say, 1000paired names.ReferencesBaum, L., Petrie, T., Soules, G. and Weiss, N. 1970.
AMaximization Technique Occurring in the Statis-tical Analysis of Probabilistic Functions of MarkovChains.
In The Annals of Mathematical Statistics,41 (1): 164-171.Bergsma, S. and Kondrak, G. 2007.
Alignment BasedDiscriminative String Similarity.
In Proceedings ofthe 45th Annual Meeting of the ACL, 2007.Darwish, K. 2010.
Transliteration Mining with Pho-netic Conflation and Iterative Training.
Proceed-ings of the 2010 Named Entities Workshop: SharedTask on Transliteration Mining, 2010.Jiampojamarn, S., Dwyer, K., Bergsma, S., Bhargava,A., Dou, Q., Kim, M. Y. and Kondrak, G. 2010.Transliteration generation and mining with limitedtraining resources.
Proceedings of the 2010Named Entities Workshop: Shared Task on Trans-literation Mining, 2010.Shawe-Taylor, J and Cristianini, N. 2004.
Kernel Me-thods for Pattern Analysis.
Cambridge UniversityPress.Klementiev, A. and Roth, D. 2006.
Weakly supervisednamed entity transliteration and discovery frommultilingual comparable corpora.
Proceedings ofthe 44th Annual Meeting of the ACL, 2006.Knight, K. and Graehl, J.
1998.
Machine Translitera-tion.
Computational Linguistics.Kumaran, A., Khapra, M. and Li, Haizhou.
2010.Whitepaper on NEWS 2010 Shared Task on Trans-literation Mining.
Proceedings of the 2010 NamedEntities Workshop: Shared Task on TransliterationMining, 2010.Nabende, P. 2010.
Mining Transliterations from Wi-kipedia using Pair HMMs.
Proceedings of the 2010Named Entities Workshop: Shared Task on Trans-literation Mining, 2010.Noeman, S. and Madkour, A.
2010.
Language inde-pendent Transliteration mining system using FiniteState Automata framework.
Proceedings of the2010 Named Entities Workshop: Shared Task onTransliteration Mining, 2010.Udupa, R., Saravanan, K., Kumaran, A. and Jagarla-mudi, J.
2009.
MINT: A Method for Effective andScalable Mining of Named Entity Transliterationsfrom Large Comparable Corpora.
Proceedings ofthe 12th Conference of the European Chapter ofAssociation for Computational Linguistics, 2009.26Participant Run Type Description Precision Recall F-ScoreIBM EgyptStandardFST, edit distance 2 with nor-malized characters 0.887 0.945 0.915IBM EgyptStandardFST, edit distance 1 with nor-malized characters 0.859 0.952 0.903IBM EgyptStandardPhonetic distance, with norma-lized characters 0.923 0.830 0.874CMIC Standard HMM + IterT 0.886 0.817 0.850CMIC Standard HMM + PC 0.900 0.796 0.845CMIC Standard (HMM + ItertT) + PC 0.818 0.827 0.822Alberta Non- Standard  0.850 0.780 0.820Alberta Standard BK-2007 0.834 0.798 0.816Alberta Standard NED+ 0.818 0.783 0.800CMIC Standard (HMM + PC + ItertT) + PC 0.895 0.678 0.771Alberta Standard DirecTL+ 0.861 0.652 0.742CMIC Standard HMM 0.966 0.587 0.730CMIC Standard HMM + PC + IterT 0.952 0.588 0.727IBM EgyptStandardFST, edit distance 2 withoutnormalized characters 0.701 0.747 0.723IBM EgyptStandardFST, edit distance 1 withoutnormalized characters 0.681 0.755 0.716IBM EgyptStandardPhonetic distance, withoutnormalized characters 0.741 0.666 0.702Table 10: Results of the English Arabic taskParticipant Run Type Description Precision Recall F-ScoreAlberta Standard Matching 0.698 0.427 0.530Alberta Non-Standard  0.700 0.430 0.530CMIC Standard (HMM + IterT) + PC 1 0.030 0.059CMIC Standard HMM + IterT 1 0.026 0.05CMIC Standard HMM + PC 1 0.024 0.047CMIC Standard (HMM + PC + IterT) + PC 1 0.022 0.044CMIC Standard HMM 1 0.016 0.032CMIC Standard HMM + PC + IterT 1 0.016 0.032Alberta Standard DirecTL+ 0.045 0.005 0.009Table 11: Results of the English Chinese taskParticipant Run Type Description Precision Recall F-ScoreMINT?
Non-Standard LFS + Seed+ 0.967 0.923 0.944Alberta  Standard StringKernel 0.954 0.895 0.924Alberta Standard NED+ 0.875 0.941 0.907Alberta Standard DirecTL+ 0.945 0.866 0.904CMIC Standard (HMM + PC + IterT) + PC 0.953 0.855 0.902Alberta Standard BK-2007 0.883 0.880 0.882CMIC Standard (HMM + IterT) + PC  0.951 0.812 0.876CMIC Standard HMM + PC 0.959 0.786 0.864Alberta Non-Standard  0.890 0.820 0.860MINT?
Standard LFS 0.943 0.780 0.854MINT?
Standard LFS 0.946 0.773 0.851?
Non-participating system27CMIC Standard HMM + PC + IterT 0.981 0.687 0.808CMIC Standard HMM + IterT 0.984 0.569 0.721CMIC Standard HMM 0.987 0.559 0.714Table 10: Results of the English Hindi taskParticipant Run Type Description Precision Recall F-ScoreAlberta Standard NED+ 0.880 0.869 0.875CMIC Standard HMM + PC 0.813 0.839 0.826MINT?
Non-Standard LFS + Seed+ 0.797 0.853 0.824Groningen?
Standard P-HMM 0.780 0.834 0.806Alberta Standard StringKernel 0.746 0.889 0.811CMIC Standard HMM 0.868 0.748 0.804CMIC Standard HMM + PC + IterT 0.843 0.747 0.792Alberta Non-Standard  0.730 0.870 0.790Alberta Standard DirecTL+ 0.778 0.795 0.786CMIC Standard HMM + IterT 0.716 0.868 0.785MINT?
Standard LFS 0.822 0.752 0.785CMIC Standard (HMM + PC + IterT) + PC 0.771 0.794 0.782Alberta Standard BK-2007 0.684 0.902 0.778CMIC Standard (HMM + IterT) + PC 0.673 0.881 0.763Groningen Standard P-HMM 0.658 0.334 0.444Table 11: Results of the English Russian taskParticipant Run Type Description Precision Recall F-ScoreAlberta Standard StringKernel 0.923 0.906 0.914MINT?
Non-Standard LFS + Seed+ 0.910 0.897 0.904MINT?
Standard LFS 0.899 0.814 0.855MINT?
Standard LFS 0.913 0.790 0.847Alberta Standard BK-2007 0.808 0.852 0.829CMIC Standard (HMM + IterT) + PC 0.939 0.741 0.828Alberta Non-Standard  0.820 0.820 0.820Alberta Standard DirectL+ 0.919 0.710 0.801Alberta Standard NED+ 0.916 0.696 0.791CMIC Standard HMM + IterT 0.952 0.668 0.785CMIC Standard HMM + PC 0.963 0.604 0.743CMIC Standard (HMM + PC + IterT) + PC 0.968 0.567 0.715CMIC Standard HMM + PC + IterT 0.975 0.446 0.612CMIC Standard HMM 0.976 0.407 0.575Table 12: Results of the English Tamil task?
Non-participating system?
Post-deadline submission of the participating system28
