Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 326?333,Prague, June 2007. c?2007 Association for Computational LinguisticsTOR, TORMD: Distributional Profiles of Conceptsfor Unsupervised Word Sense DisambiguationSaif MohammadDept.
of Computer ScienceUniversity of TorontoToronto, ON M5S 3G4Canadasmm@cs.toronto.eduGraeme HirstDept.
of Computer ScienceUniversity of TorontoToronto, ON M5S 3G4Canadagh@cs.toronto.eduPhilip ResnikDept.
of Linguistics and UMIACSUniversity of MarylandCollege Park, MD 20742USAresnik@umiacs.umd.eduAbstractWords in the context of a target wordhave long been used as features by su-pervised word-sense classifiers.
Moham-mad and Hirst (2006a) proposed a way todetermine the strength of association be-tween a sense or concept and co-occurringwords?the distributional profile of a con-cept (DPC)?without the use of manuallyannotated data.
We implemented an unsu-pervised na?
?ve Bayes word sense classifierusing these DPCs that was best or withinone percentage point of the best unsuper-vised systems in the Multilingual Chinese?English Lexical Sample Task (task #5) andthe English Lexical Sample Task (task #17).We also created a simple PMI-based classi-fier to attempt the English Lexical Substi-tution Task (task #10); however, its perfor-mance was poor.1 IntroductionDetermining the intended sense of a word is poten-tially useful in many natural language tasks includ-ing machine translation and information retrieval.The best approaches for word sense disambiguationare supervised and they use words that co-occur withthe target as features.
These systems rely on sense-annotated data to identify words that are indicativeof the use of the target in each of its senses.However, only limited amounts of sense-annotated data exist and it is expensive to create.
Inour previous work (Mohammad and Hirst, 2006a),we proposed an unsupervised approach to determinethe strength of association between a sense or con-cept and its co-occurring words?the distributionalprofile of a concept (DPC)?relying simply on rawtext and a published thesaurus.
The categories in apublished thesaurus were used as coarse senses orconcepts (Yarowsky, 1992).
We now show how dis-tributional profiles of concepts can be used to cre-ate an unsupervised na?
?ve Bayes word-sense classi-fier.
We also implemented a simple classifier thatrelies on the pointwise mutual information (PMI)between the senses of the target and co-occurringwords.
These DPC-based classifiers participated inthree SemEval 2007 tasks: the English Lexical Sam-ple Task (task #17), the English Lexical Substitu-tion Task (task #10), and the Multilingual Chinese?English Lexical Sample Task (task #5).The English Lexical Sample Task (Pradhan et al,2007) is a traditional word sense disambiguationtask wherein the intended (WordNet) sense of a tar-get word is to be determined from its context.
Wemanually mapped the WordNet senses to the cate-gories in a thesaurus and the DPC-based na?
?ve Bayesclassifier was used to identify the intended sense(category) of the target words.The object of the Lexical Substitution Task (Mc-Carthy and Navigli, 2007) is to replace a target wordin a sentence with a suitable substitute that preservesthe meaning of the utterance.
The list of possiblesubstitutes for a given target word is usually contin-gent on its intended sense.
Therefore, word sensedisambiguation is expected to be useful in lexicalsubstitution.
We used the PMI-based classier to de-termine the intended sense.326The objective of the Multilingual Chinese?English Lexical Sample Task (Jin et al, 2007) is toselect from a given list a suitable English translationof a Chinese target word in context.
Mohammad etal.
(2007) proposed a way to create cross-lingualdistributional profiles of a concepts (CL-DPCs)?the strengths of association between the concepts ofone language and words of another.
For this task, wemapped the list of English translations to appropri-ate thesaurus categories and used an implementationof a CL-DPC?based unsupervised na?
?ve Bayes clas-sifier to identify the intended senses (and thereby theEnglish translations) of target Chinese words.2 Distributional profiles of conceptsIn order to determine the strength of association be-tween a sense of the target word and its co-occurringwords, we need to determine their individual andjoint occurrence counts in a corpus.
Mohammad andHirst (2006a) and Mohammad et al (2007) proposedways to determine these counts in a monolingual andcross-lingual framework without the use of sense-annotated data.
We summarize the ideas in this sec-tion; the original papers give more details.2.1 Word?category co-occurrence matrixWe create a word?category co-occurrence matrix(WCCM) having English word types wen as one di-mension and English thesaurus categories cen as an-other.
We used the Macquarie Thesaurus (Bernard,1986) both as a very coarse-grained sense inventoryand a source of words that together represent eachcategory (concept).
The WCCM is populated withco-occurrence counts from a large English corpus(we used the British National Corpus (BNC)).
A par-ticular cell mi j, corresponding to word weni and con-cept cenj , is populated with the number of times wenico-occurs with any word that has cenj as one of itssenses (i.e., weni co-occurs with any word listed un-der concept cenj in the thesaurus).cen1 cen2 .
.
.
cenj .
.
.wen1 m11 m12 .
.
.
m1 j .
.
.wen2 m21 m22 .
.
.
m2 j .
.
...................weni mi1 mi2 .
.
.
mi j .
.
.......... .
.
.......A particular cell mi j, corresponding to word weniand concept cenj , is populated with the number oftimes weni co-occurs with any word that has cenjas one of its senses (i.e., weni co-occurs with anyword listed under concept cenj in the thesaurus).This matrix, created after a first pass of the corpus,is the base word?category co-occurrence matrix(base WCCM) and it captures strong associationsbetween a sense and co-occurring words (see dis-cussion of the general principle in Resnik (1998)).From the base WCCM we can determine the num-ber of times a word w and concept c co-occur, thenumber of times w co-occurs with any concept, andthe number of times c co-occurs with any word.
Astatistic such as PMI can then give the strength ofassociation between w and c. This is similar to howYarowsky (1992) identifies words that are indicativeof a particular sense of the target word.Words that occur close to a target word tend tobe good indicators of its intended sense.
Therefore,we make a second pass of the corpus, using the baseWCCM to roughly disambiguate the words in it.
Foreach word, the strength of association of each ofthe words in its context (?5 words) with each of itssenses is summed.
The sense that has the highest cu-mulative association is chosen as the intended sense.A new bootstrapped WCCM is created such thateach cell mi j, corresponding to word weni and con-cept cenj , is populated with the number of times wenico-occurs with any word used in sense cenj .Mohammad and Hirst (2006a) used the DPCscreated from the bootstrapped WCCM to attainnear-upper-bound results in the task of determin-ing word sense dominance.
Unlike the McCarthyet al (2004) dominance system, this approach canbe applied to much smaller target texts (a fewhundred sentences) without the need for a largesimilarly-sense-distributed text1.
Mohammad andHirst (2006b) used the DPC-based monolingual dis-tributional measures of concept-distance to rankword pairs by their semantic similarity and to correctreal-word spelling errors, attaining markedly betterresults than monolingual distributional measures ofword-distance.
In the spelling correction task, the1The McCarthy et al (2004) system needs to first gener-ate a distributional thesaurus from the target text (if it is largeenough?a few million words) or from another large text with adistribution of senses similar to the target text.327Figure 1: The cross-lingual candidate senses of Chi-nese words and .distributional concept-distance measures performedbetter than all WordNet-based measures as well, ex-cept for the Jiang and Conrath (1997) measure.2.2 Cross-lingual word?categoryco-occurrence matrixGiven a Chinese word wch in context, we use aChinese?English bilingual lexicon to determine itsdifferent possible English translations.
Each En-glish translation wen may have one or more possi-ble coarse senses, as listed in an English thesaurus.These English thesaurus concepts (cen) will be re-ferred to as cross-lingual candidate senses of theChinese word wch.2 Figure 1 depicts examples.We create a cross-lingual word?category co-occurrence matrix (CL-WCCM) with Chinese wordtypes wch as one dimension and English thesaurusconcepts cen as another.cen1 cen2 .
.
.
cenj .
.
.wch1 m11 m12 .
.
.
m1 j .
.
.wch2 m21 m22 .
.
.
m2 j .
.
...................wchi mi1 mi2 .
.
.
mi j .
.
.......... .
.
.......The matrix is populated with co-occurrence countsfrom a large Chinese corpus; we used a collection ofLDC-distributed corpora3?Chinese Treebank En-glish Parallel Corpus, FBIS data, Xinhua Chinese?English Parallel News Text Version 1.0 beta 2, Chi-nese English News Magazine Parallel Text, Chinese2Some of the cross-lingual candidate senses of wch might notreally be senses of wch (e.g., ?celebrity?, ?practical lesson?, and?state of the atmosphere?
in Figure 1).
However, as substanti-ated by experiments by Mohammad et al (2007), our algorithmis able to handle the added ambiguity.3http://www.ldc.upenn.eduFigure 2: Chinese words having ?celestial body?
asone of their cross-lingual candidate senses.News Translation Text Part 1, and Hong Kong Paral-lel Text.
A particular cell mi j, corresponding to wordwchi and concept cenj , is populated with the numberof times the Chinese word wchi co-occurs with anyChinese word having cenj as one of its cross-lingualcandidate senses.
For example, the cell for(?space?)
and ?celestial body?
will have the sum ofthe number of times co-occurs with , ,, , , and so on (see Figure 2).
We usedthe Macquarie Thesaurus (Bernard, 1986) (about98,000 words).
The possible Chinese translationsof an English word were taken from the Chinese?English Translation Lexicon version 3.0 (Huang andGraff, 2002) (about 54,000 entries).This base word?category co-occurrence matrix(base WCCM), created after a first pass of the cor-pus, captures strong associations between a cate-gory (concept) and co-occurring words.
For ex-ample, even though we increment counts for both?
?celestial body?
and ??celebrity?
for a par-ticular instance where co-occurs with ,will co-occur with a number of words such as, , and that each have the sense of ce-lestial body in common (see Figure 2), whereas alltheir other senses are likely different and distributedacross the set of concepts.
Therefore, the co-occurrence count of and ?celestial body?
willbe relatively higher than that of and ?celebrity?.As in the monolingual case, a second pass ofthe corpus is made to disambiguate the (Chinese)words in it.
For each word, the strength of associ-ation of each of the words in its context (?5 words)with each of its cross-lingual candidate senses issummed.
The sense that has the highest cumula-tive association with co-occurring words is chosenas the intended sense.
A new bootstrapped WCCMis created by populating each cell mi j, correspond-ing to word wchi and concept cenj , with the number oftimes the Chinese word wchi co-occurs with any Chi-328nese word used in cross-lingual sense cenj .
A statisticsuch as PMI is then applied to these counts to deter-mine the strengths of association between a targetconcept and co-occurring words, giving the distri-butional profile of the concept.Mohammad et al (2007) combined German textwith an English thesaurus using a German?Englishbilingual lexicon to create German?English DPCs.These DPCs were used to determine semantic dis-tance between German words, showing that state-of-the-art accuracies for one language can be achievedusing a knowledge source (thesaurus) from another.Given that a published thesaurus has about 1000categories and the size of the vocabulary N is atleast 100,000, the CL-WCCM and the WCCM aremuch smaller matrices (about 1000?N) than the tra-ditional word?word co-occurrence matrix (N ?N).Therefore the WCCMs are relatively inexpensiveboth in terms of memory and computation.3 ClassificationWe implemented two unsupervised classifiers.
Thewords in context were used as features.3.1 Unsupervised Na?
?ve Bayes ClassifierThe na?
?ve Bayes classifier has the following formulato determine the intended sense cnb:cnb = argmaxc j?CP(c j) ?wi?WP(wi|c j) (1)where C is the set of possible senses (as listed inthe Macquarie Thesaurus) and W is the set of wordsthat co-occur with the target (we used a window of?5 words).Traditionally, prior probabilities of the senses(P(c j)) and the conditional probabilities in the like-lihood (?wi?W P(wi|c j)) are determined by sim-ple counts in sense-annotated data.
We approx-imate these probabilities using counts from theword?category co-occurrence matrix (monolingualor cross-lingual), thereby obviating the need formanually-annotated data.P(c j) =?i mi j?i, j mi j(2)P(wi|c j) =mi j?i mi j(3)For the English Lexical Task, mi j is the number oftimes the English word wi co-occurs with the En-glish category c j?as listed in the word?categoryco-occurrence matrix (WCCM).
For the Multilin-gual Chinese?English Lexical Task, mi j is the num-ber of times the Chinese word wi co-occurs with theEnglish category c j?as listed in the cross-lingualword?category co-occurrence matrix (CL-WCCM).3.2 PMI-based classifierWe calculate the pointwise mutual information be-tween a sense of the target word and a co-occurringword using the following formula:PMI(wi,c j) = logP(wi,c j)P(wi)?P(c j)(4)where P(wi,c j) =mi j?i, j mi j(5)and P(wi) =?
j mi j?i, j mi j(6)mi j is the count in the WCCM or CL-WCCM (as de-scribed in the previous subsection).
For each senseof the target, the sum of the strength of association(PMI) between it and each of the co-occurring words(in a window of ?5 words) is calculated.
The sensewith the highest sum is chosen as the intended sense.cpmi = argmaxc j?C?wi?WPMI(wi,c j) (7)Note that this PMI-based classifier does not capital-ize on prior probabilities of the different senses.4 Data4.1 English Lexical Sample TaskThe English Lexical Sample Task training and testdata (Pradhan et al, 2007) have 22281 and 4851instances respectively for 100 target words (50nouns and 50 verbs).
WordNet 2.1 is used asthe sense inventory for most of the target words,but certain words have one or more senses fromOntoNotes (Hovy et al, 2006).
Many of the fine-grained senses are grouped into coarser senses.Our approach relies on representing a sense witha number of near-synonymous words, for which athesaurus is a natural source.
Even though the ap-proach can be ported to WordNet4, there was no easy4The synonyms within a synset, along with its one-hopneighbors and all its hyponyms, can represent that sense.329TRAINING DATA TEST DATAWORDS BASELINE PMI-BASED NAI?VE BAYES PRIOR LIKELIHOOD NAI?VE BAYESall 27.8 41.4 50.8 37.4 49.4 52.1nouns only 25.6 43.4 53.6 18.1 49.6 49.7verbs only 29.2 38.4 44.5 58.9 49.1 54.7Table 1: English Lexical Sample Task: Results obtained using the PMI-based classifier on the training dataand the na?
?ve Bayes classifier on both training and test dataway of representing OntoNotes senses with near-synonymous words.
Therefore, we asked four na-tive speakers of English to map the WordNet andOntoNotes senses of the 100 target words to theMacquarie Thesaurus and use it as our sense inven-tory.
We also wanted to examine the effect of usinga very coarse sense inventory such as the categoriesin a published thesaurus (811 in all).The annotators were presented with a target word,its WordNet/OntoNotes senses, and the Macquariesenses.
WordNet senses were represented by syn-onyms, gloss, and example usages.
The OntoNotessenses were described through syntactic patterns andexample usages (provided by the task organizers).The Macquarie senses (categories) were describedby the category head (a representative word forthe category) and five other words in the category.Specifically, words in the same semicolon group5 asthe target were chosen.
Annotators 1 and 2 labeledeach WordNet/OntoNotes sense of the first 50 targetwords with one or more appropriate Macquarie cat-egories.
Annotators 3 and 4 labeled the senses of theother 50 words.
We combined all four annotationsinto a WordNet?Macquarie mapping file by taking,for each target word, the union of categories chosenby the two annotators.4.2 English Lexical Substitution TaskThe English Lexical Substitution Task has 1710 testinstances for 171 target words (nouns, verbs, adjec-tives, and adverbs) (McCarthy and Navigli, 2007).Some instances were randomly extracted from anInternet corpus, whereas others were selected man-ually from it.
The target word might or might not bepart of a multiword expression.
The task is not tiedto any particular sense inventory.5Words within a semicolon group of a thesaurus tend to bemore closely related than words across groups.4.3 Multilingual Chinese?English LexicalSample TaskThe Multilingual Chinese?English Lexical SampleTask training and test data (Jin et al, 2007) have2686 and 935 instances respectively for 40 targetwords (19 nouns and 21 verbs).
The instances aretaken from a corpus of People?s Daily News.
Theorganizers used the Chinese Semantic Dictionary(CSD), developed by the Institute of ComputationalLinguistics, Peking University, both as a sense in-ventory and bilingual lexicon (to extract a suitableEnglish translation of the target word once the in-tended Chinese sense is determined).In order to determine the English translations ofChinese words in context, our system relies on Chi-nese text and an English thesaurus.
As the thesaurusis used as our sense inventory, the first author and anative speaker of Chinese mapped the English trans-lations of the target to appropriate Macquarie cate-gories.
We used three examples (from the trainingdata) per English translation for this purpose.5 Evaluation5.1 English Lexical Sample TaskBoth the na?
?ve Bayes classifier and the PMI-basedone were applied to the training data.
For each in-stance, the Macquarie category c that best capturesthe intended sense of the target was determined.
Theinstance was labeled with all the WordNet sensesthat are mapped to c in the WordNet?Macquariemapping file (described earlier in Section 4.1).5.1.1 ResultsTable 1 shows the performances of the two clas-sifiers.
The system attempted to label all instancesand so we report accuracy values instead of pre-cision and recall.
The na?
?ve Bayes classifier per-formed markedly better in training than the PMI-330based one and so was applied to the test data.
Thetable also lists baseline results obtained when a sys-tem randomly guesses one of the possible senses foreach target word.
Note that since this is a com-pletely unsupervised system, it is not privy to thedominant sense of the target words.
We do not relyon the ranking of senses in WordNet as that wouldbe an implicit use of the sense-tagged SemCor cor-pus.
Therefore, the most-frequent-sense baselinedoes not apply.
Table 1 also shows results obtainedusing just the prior probability and likelihood com-ponents of the na?
?ve Bayes formula.
Note that thecombined accuracy is higher than individual com-ponents for nouns but not for verbs.5.1.2 DiscussionThe na?
?ve Bayes classifier?s accuracy is onlyabout one percentage point lower than that of thebest unsupervised system taking part in the task(Pradhan et al, 2007).
One reason that it does bet-ter than the PMI-based one is that it takes into ac-count prior probabilities of the categories.
However,using just the likelihood also outperforms the PMIclassifier.
This may be because of known problemsof using PMI with low frequencies (Manning andSchu?tze, 1999).
In case of verbs, lower combinedaccuracies compared to when using just prior proba-bilities suggests that the bag-of-words type featuresare not very useful.
It is expected that more syntac-tically oriented features will give better results.
Us-ing window sizes (?1,?2, and ?10) on the trainingdata resulted in lower accuracies than that obtainedusing a window of ?5 words.
A smaller windowsize is probably missing useful co-occurring words,whereas a larger window size is adding words thatare not indicative of the target?s intended sense.The use of a sense inventory (Macquarie The-saurus) different from that used to label the data(WordNet) clearly will have a negative impact onthe results.
The mapping from WordNet/OntoNotesto Macquarie is likely to have some errors.
Further,for 19 WordNet/OntoNotes senses, none of the an-notators found a thesaurus category close enough inmeaning.
This meant that our system had no wayof correctly disambiguating instances with thesesenses.
Also impacting accuracy is the significantlyfine-grained nature of WordNet compared to the the-saurus.
For example, following are the three coarseBEST OOTAcc Mode Acc Acc Mode Accall 2.98 4.72 11.19 14.63Further AnalysisNMWT 3.22 5.04 11.77 15.03NMWS 3.32 4.90 12.22 15.26RAND 3.10 5.20 9.98 13.00MAN 2.84 4.17 12.61 16.49Table 2: English Lexical Substitution Task: Resultsobtained using the PMI-based classifiersenses for the noun president in WordNet: (1) exec-utive officer of a firm or college, (2) the chief exec-utive of a republic, and (3) President of the UnitedStates.
The last two senses will fall into just one cat-egory for most, if not all, thesauri.5.2 English Lexical Substitution TaskWe used the PMI-based classifier6 for the EnglishLexical Substitution Task.
Once it identifies a suit-able thesaurus category as the intended sense for atarget, ten candidate substitutes are chosen from thatcategory.
Specifically, the category head word andup to nine words in the same semicolon group as thetarget are selected (words within a semicolon groupare closer in meaning).
Of the ten candidates, thesingle-word expression that is most frequent in theBNC is chosen as the best substitute; the motivationis that the annotators, who created the gold standard,were instructed to give preference to single wordsover multiword expressions as substitutes.5.2.1 ResultsThe system was evaluated not only on the bestsubstitute (BEST) but also on how good the top tencandidate substitutes are (OOT).
Table 2 presents theresults.7 The system attempted all instances.
Thetable also lists performances of the system on in-stances where the target is not part of a multiwordexpression (NMWT), on instances where the substi-tute is not a multiword expression (NMWS), on in-stances randomly extracted from the corpus (RAND),and on instances manually selected (MAN).6Due to time constraints, we were able to upload results onlywith the PMI-based classifier by the task deadline.7The formulae for accuracy and mode accuracy are as de-scribed by Pradhan et al (2007).331TRAINING DATA TEST DATABASELINE PMI-BASED NAI?VE BAYES PRIOR LIKELIHOOD NAI?VE BAYESWORDS micro macro micro macro micro macro micro macro micro macro micro macroall 33.1 38.3 33.9 40.0 38.5 44.7 35.4 41.7 38.8 44.6 37.5 43.1nouns only 41.9 43.5 43.6 45.0 49.4 50.5 45.3 47.1 48.1 50.8 50.0 51.6verbs only 28.0 34.1 28.0 35.6 31.9 39.6 29.1 36.8 32.9 39.0 29.6 35.5Table 3: Multilingual Chinese?English Lexical Sample Task: Results obtained using the PMI-based classi-fier on the training data and the na?
?ve Bayes classifier on both training and test data5.2.2 DiscussionCompetitive performance of our DPC-based sys-tem on the English Lexical Sample Task and theChinese?English Lexical Sample Task (see nextsubsection) suggests that DPCs are useful for sensedisambiguation.
Poor results on the substitution taskcan be ascribed to several factors.
First, we usedthe PMI-based classifier that we found later to bemarkedly less accurate than the na?
?ve Bayes clas-sifier in the other two tasks.
Second, the words inthe thesaurus categories may not always be near-synonyms; they might just be strongly related.
Suchwords will be poor substitutes for the target.
Also,we chose as the best substitute simply the most fre-quent of the ten candidates.
This simple techniqueis probably not accurate enough.
On the other hand,because we chose the candidates without any regardto frequency in a corpus, the system chose certaininfrequent words such as wellnigh and ecchymosed,which were not good candidate substitutes.5.3 Multilingual Chinese?English LexicalSample TaskIn the Multilingual Chinese?English Lexical SampleTask, both the na?
?ve Bayes classifier and the PMI-based classifier were applied to the training data.For each instance, the Macquarie category, say c,that best captures the intended sense of the targetword is determined.
Then the instance is labeledwith all the English translations that are mapped to cin the English translations?Macquarie mapping file(described earlier in Section 4.3).5.3.1 ResultsTable 3 shows accuracies of the two classifiers.Macro average is the ratio of number of instancescorrectly disambiguated to the total, whereas microaverage is the average of the accuracies achievedon each target word.
As in the English LexicalSample Task, both classifiers, especially the na?
?veBayes classifier, perform well above the randombaseline.
Since the na?
?ve Bayes classifier also per-formed markedly better than the PMI-based one intraining, it was applied to the test data.
Table 3also shows results obtained using just the likelihoodand prior probability components of the na?
?ve Bayesclassifier on the test data.5.3.2 DiscussionOur na?
?ve Bayes classifier scored highest of allunsupervised systems taking part in the task (Jin etal., 2007).
As in the English Lexical Sample Task,using just the likelihood again outperforms the PMIclassifier on the training data.
The use of a senseinventory different from that used to label the dataagain will have a negative impact on the results asthe mapping may have a few errors.
The anno-tator believed none of the given Macquarie cate-gories could be mapped to two Chinese SemanticDictionary senses.
This meant that our system hadno way of correctly disambiguating instances withthese senses.There were also a number of cases where morethan one CSD sense of a word was mapped to thesame Macquarie category.
This occurred for tworeasons: First, the categories of the Macquarie The-saurus act as very coarse senses.
Second, for cer-tain target words, the two CSD senses may be differ-ent in terms of their syntactic behavior, yet semanti-cally very close (for example, the ?be shocked?
and?shocked?
senses of ).
This many-to-one map-ping meant that for a number of instances more thanone English translation was chosen.
Since the taskrequired us to provide exactly one answer (and therewas no partial credit in case of multiple answers), acategory was chosen at random.3326 ConclusionWe implemented a system that uses distributionalprofiles of concepts (DPCs) for unsupervised wordsense disambiguation.
We used words in the con-text as features.
Specifically, we used the DPCsto create a na?
?ve Bayes word-sense classifier and asimple PMI-based classifier.
Our system attemptedthree SemEval-2007 tasks.
On the training dataof the English Lexical Sample Task (task #17) andthe Multilingual Chinese?English Lexical SampleTask (task #5), the na?
?ve Bayes classifier achievedmarkedly better results than the PMI-based classi-fier and so was applied to the respective test data.On both test and training data of both tasks, thesystem achieved accuracies well above the randombaseline.
Further, our system placed best or close toone percentage point from the best among the unsu-pervised systems.
In the English Lexical Substitu-tion Task (task #10), for which there was no train-ing data, we used the PMI-based classifier.
Thesystem performed poorly, which is probably a re-sult of using the weaker classifier and a simple bruteforce method for identifying the substitute amongthe words in a thesaurus category.
Markedly higher-than-baseline performance of the na?
?ve Bayes clas-sifier on task #17 and task #5 suggests that the DPCsare useful for word sense disambiguation.AcknowledgmentsWe gratefully acknowledge Xiaodan Zhu, Michael Demko,Christopher Parisien, Frank Rudicz, and Timothy Fowler formapping training-data labels to categories in the MacquarieThesaurus.
We thank Michael Demko, Siddharth Patwardhan,Xinglong Wang, Vivian Tsang, and Afra Alishahi for helpfuldiscussions.
This research is financially supported by the Natu-ral Sciences and Engineering Research Council of Canada, theUniversity of Toronto, ONR MURI Contract FCPO.810548265and Department of Defense contract RD-02-5700.ReferencesJ.R.L.
Bernard, editor.
1986.
The Macquarie Thesaurus.Macquarie Library, Sydney, Australia.Eduard Hovy, Mitchell Marcus, Martha Palmer, LanceRamshaw, and Ralph Weischedel.
2006.
OntoNotes:The 90% Solution.
In Proceedings of the Human Lan-guage Technology Conference of the North AmericanChapter of the ACL, pages 57?60, New York, NY.Shudong Huang and David Graff.
2002.
Chinese?english translation lexicon version 3.0.
LinguisticData Consortium.Jay J. Jiang and David W. Conrath.
1997.
Semanticsimilarity based on corpus statistics and lexical taxon-omy.
In Proceedings of International Conference onResearch on Computational Linguistics, Taiwan.Peng Jin, Yunfang Wu, and Shiwen Yu.
2007.
SemEval-2007 task 05: Multilingual Chinese-English lexicalsample task.
In Proceedings of the Fourth Interna-tional Workshop on the Evaluation of Systems for theSemantic Analysis of Text, Prague, Czech Republic.Christopher D. Manning and Hinrich Schu?tze.
1999.Foundations of Statistical Natural Language Process-ing.
MIT Press, Cambridge, Massachusetts.Diana McCarthy and Roberto Navigli.
2007.
SemEval-2007 task 10: English lexical substitution task.
InProceedings of the Fourth International Workshop onthe Evaluation of Systems for the Semantic Analysis ofText (SemEval-2007), Prague, Czech Republic.Diana McCarthy, Rob Koeling, Julie Weeds, and JohnCarroll.
2004.
Finding predominant senses in un-tagged text.
In Proceedings of the 42nd Annual Meet-ing of the Association for Computational Linguistics(ACL-04), pages 280?267, Barcelona, Spain.Saif Mohammad and Graeme Hirst.
2006a.
Determiningword sense dominance using a thesaurus.
In Proceed-ings of the 11th Conference of the European Chap-ter of the Association for Computational Linguistics(EACL), Trento, Italy.Saif Mohammad and Graeme Hirst.
2006b.
Distribu-tional measures of concept-distance: A task-orientedevaluation.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing(EMNLP-2006), Sydney, Australia.Saif Mohammad, Iryna Gurevych, Graeme Hirst, andTorsten Zesch.
2007.
Cross-lingual distributionalprofiles of concepts for measuring semantic dis-tance.
In Proceedings of the Joint Conference onEmpirical Methods in Natural Language Process-ing and Computational Natural Language Learning(EMNLP/CoNLL-2007), Prague, Czech Republic.Sameer Pradhan, Martha Palmer, and Edward Loper.2007.
SemEval-2007 task 17: English lexical sample,English SRL and English all-words tasks.
In Proceed-ings of the Fourth International Workshop on the Eval-uation of Systems for the Semantic Analysis of Text(SemEval-2007), Prague, Czech Republic.Philip Resnik.
1998.
Wordnet and class-based prob-abilities.
In Christiane Fellbaum, editor, WordNet:An Electronic Lexical Database, pages 239?263.
TheMIT Press, Cambridge, Massachusetts.David Yarowsky.
1992.
Word-sense disambiguation us-ing statistical models of Roget?s categories trained onlarge corpora.
In Proceedings of the 14th InternationalConference on Computational Linguistics (COLING-92), pages 454?460, Nantes, France.333
