Proceedings of NAACL HLT 2009: Short Papers, pages 165?168,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsSearch Result Re-ranking by Feedback Control Adjustment forTime-sensitive QueryRuiqiang Zhang?
and Yi Chang?
and Zhaohui Zheng?Donald Metzler?
and Jian-yun Nie??Yahoo!
Labs, 701 First Avenue, Sunnyvale, CA94089?University of Montreal, Montreal, Quebec,H3C 3J7, Canada?
{ruiqiang,yichang,zhaohui,metzler}@yahoo-inc.com?nie@iro.umontreal.caAbstractWe propose a new method to rank a specialcategory of time-sensitive queries that are yearqualified.
The method adjusts the retrievalscores of a base ranking function accordingto time-stamps of web documents so that thefreshest documents are ranked higher.
Ourmethod, which is based on feedback controltheory, uses ranking errors to adjust the searchengine behavior.
For this purpose, we usea simple but effective method to extract yearqualified queries by mining query logs and atime-stamp recognition method that considerstitles and urls of web documents.
Our methodwas tested on a commercial search engine.
Theexperiments show that our approach can sig-nificantly improve relevance ranking for yearqualified queries even if all the existing meth-ods for comparison failed.1 IntroductionRelevance ranking plays a crucial role in searchengines.
There are many proposed machine learn-ing based ranking algorithms such as languagemodeling-based methods (Zhai and Lafferty, 2004),RankSVM (Joachims, 2002), RankBoost (Freund et al,1998) and GBrank (Zheng et al, 2007).
The input tothese algorithms is a set of feature vectors extracted fromqueries and documents.
The goal is to find the parametersetting that optimizes some relevance metric giventraining data.
While these machine learning algorithmscan improve average relevance, they may be ineffctivefor certain special cases.
Time-sensitive queries are onesuch special case that machine-learned ranking functionsmay have a hard time learning, due to the small numberof such queries.Consider the query ?sigir?
(the name of a conference),which is time sensitive.
Table 1 shows two examplesearch result pages for the query, SERP1 and SERP2.
Thequery: sigirSERP1 url1: http://www.sigir.orgurl2: http://www.sigir2008.orgurl3: http://www.sigir2004.orgurl4: http://www.sigir2009.orgurl5: http://www.sigir2009.org/scheduleSERP2 url1: http://www.sigir.orgurl2: http://www.sigir2009.orgurl3: http://www.sigir2009.org/scheduleurl4: http://www.sigir2008.orgurl5: http://www.sigir2004.orgTable 1: Two contrived search engine result pagesranking of SERP2 is clearly better than that of SERP1 be-cause the most recent event, ?sigir2009?, is ranked higherthan other years.Time is an important dimension of relevance in websearch, since users tend to prefer recent documents to olddocuments.
At the time of this writing (February 2009),none of the major commercial search engines ranked thehomepage for SIGIR 2009 higher than previous SIGIRhomepages for the query ?sigir?.
One possible reason forthis is that ranking algorithms are typically based on an-chor text features, hyperlink induced features, and click-through rate features.
However, these features tend to fa-vor old pages more than recent ones.
For example, ?si-gir2008?
has more links and clicks than ?sigir2009?
be-cause ?sigir2008?
has existed longer time and thereforehas been visited more.
It is less likely that newer webpages from ?sigir2009?
can be ranked higher using fea-tures that implicitly favor old pages.However, the fundamental problem is that current ap-proaches have focused on improving general ranking al-gorithms.
Methods for improving ranking of specifictypes of query like temporal queries are often overlooked.Aiming to improve ranking results, some methods ofre-ranking search results are proposed, such as the workby (Agichtein et al, 2006) and (Teevan et al, 2005).165Search EngineDetectorControllererror R(q, yn)R(q, yo)_+Figure 1: Feedback control for search engineThese work uses user search behavior information or per-sonalization information as features that are integratedinto an enhanced ranking model.
We propose a novelmethod of re-ranking search results.
This new methodis based on feedback control theory, as illustrated in 1.We make a Detector to monitor search engine (SE) out-put and compare it with the input, which is the desiredsearch engine ranking.
If an error is found, we designthe controller that uses the error to adjust the search en-gine output, such that the search engine output tracks theinput.
We will detail the algorithm in Section 4.1.Our method was applied to a special class of time-sensitive query, year qualified queries (YQQs).
For thiscategory, we found users either attached a year with thequery explicitly, like ?sigir 2009?, or used the query onlywithout a year attached,like ?sigir?.
We call the formerexplicit YQQs, and the latter implicit YQQs.
Using querylog analysis, we found these types of queries made upabout 10% of the total query volume.
We focus exclu-sively on implicit YQQs by translating the user?s im-plicit intention as the most recent year.
Explicit YQQsare less interesting, because the user?s temporal inten-tion is clearly specified in the query.
Therefore, rank-ing for these types of queries is relatively straightfor-ward.
Throughout the remainder of this paper, we usethe ?YQQ?
to refer to implicit YQQs, unless otherwisestated.2 Adaptive score adjustmentOur proposed re-ranking model is shown in Eq.
1, as be-low.F(q, d) ={R(q, d) if q < YQQR(q, d) + Q(q, d) otherwiseQ(q, d) ={ (e(do, dn) + k)e??
(q) if y(d) = yn0 otherwisee(do, dn) = R(q, do) ?
R(q, dn)(1)This work assumes that a base ranking function is usedto rank documents with respect to an incoming query.
Wedenote this base ranking function as R(q, d).
This rankingfunction is conditioned on a query q and a document d. Itis assumed to model the relevance between q and d.Our proposed method is flexible for all YQQ queries.Suppose the current base ranking function gives the re-sults as SERP1 of Table 1.
To correct the ranking, wepropose making an adjustment to R(q, d).In Eq.
1, F(q, d) is the final ranking function.
If thequery is not an YQQ, the base ranking function is used.Otherwise, we propose an adjustment function, Q(q, d) ,to adjust the base ranking function.
Q(q, d) is controlledby the ranking error, e(do, dn), signifying the base func-tion ranking error if the newest web page dn is rankedlower than the oldest web page do.
y(d) is the year thatthe event described by d has occurred or will occur.
Ifyo and yn indicate the oldest year and the newest year,then y(do) = yo, y(dn) = yn.
R(q, do) and R(q, dn) are thebase ranking function scores for the oldest and the newestdocuments.k is a small shift value for direction control.
Whenk < 0, the newest document is adjusted slightly under theold one.
Otherwise, it is adjusted slightly over the oldone.
Experiments show k > 0 gave better results.
Thevalue of k is determined in training.?
(q) is the confidence score of a YQQ query, mean-ing the likelihood of a query to be YQQ.
The confidencescore is bigger if a query is more likely to be YQQ.
Moredetails are given in next section.
?
is a weighting param-eter for adjusting ?
(q).The exp function e??
(q) is a weighting to control boost-ing value.
A higher value, confidence ?, a larger boostingvalue, Q(q, d).Our method can be understood by feedback controltheory, as illustrated in Fig.
1.
The ideal input is R(q, yo)representing the desired ranking score for the newestWeb page, R(q, yn).
But the search engine real outputis R(q, yn).
Because search engine is a dynamic system,its ranking is changing over time.
This results in rankingerrors, e(do, dn) = R(q, do) ?
R(q, dn).
The function of?Controller?
is to design a function to adjust the searchengine ranking so that the error approximates to zero,e(do, dn) = 0.
For this work, ?Controller?
is Q(q, d).?Detector?
is a document year-stamp recognizer, whichwill be described more in the next section.
?Detector?is used to detect the newest Web pages and their rankingscores.
Fig.
1 is an ideal implementation of our methods.We cannot carry out real-time experiments in this work.Therefore, the calculation of ranking errors was made inoffline training.3 YQQ detection and year-stamprecognitionTo implement Eq.
1, we need to find YQQ queries and toidentify the year-stamp of web documents.Our YQQ detection method is simple, efficient, andrelies only on having access to a query log with frequencyinformation.
First, we extracted all explicit YQQs from166query log.
Then, we removed all the years from explicitYQQs.
Thus, implicit YQQs are obtained from explicitYQQs.
The implicit YQQs are saved in a dictionary.
Inonline test, we match input queries with each of implicitYQQs in the dictionary.
If an exact match is found, weregard the input query as YQQ, and apply Eq.
1 to re-ranksearch results.After analyzing samples of the extracted YQQs, wegroup them into three classes.
One is recurring-eventquery, like ?sigir?, ?us open tennis?
; the second is news-worthy query, like ?steve ballmer?, ?china foreign re-serves?
; And the class not belong to any of the abovetwo, like ?christmas?, ?youtube?.
We found our proposedmethods were the most effective for the first category.
InEq.
1, we can use confidence ?
(q) to distinguish the threecategories and their change of ranking as shown in Eq.1,that is defined as below.?
(q) =?y w(q, y)#(q) +?y w(q, y)(2)where w(q, y) = #(q.y)+#(y.q).
#(q.y) denotes the num-ber of times that the base query q is post-qualified withthe year y in the query log.
Similarly, #(y.q) is the num-ber of times that q is pre-qualified with the year y. Thisweight measures how likely q is to be qualified with y,which forms the basis of our mining and analysis.
#(q) isthe counts of independent query, without associating withany other terms.We also need to know the year-stamp y(d) for eachweb document so that the ranking score of a documentis updated if y(d) = yn is satisfied.
We can do thisfrom a few sources such as title, url, anchar text, andextract date from documents that is possible for manynews pages.
For example, from url of the web page,?www.sigir2009.org?, we detect its year-stamp is 2009.We have also tried to use some machine generateddates.
However, in the end we found such dates are in-accurate and cannot be trusted.
For example, discoverytime is the time when the document was found by thecrawler.
But a web document may exist several years be-fore a crawler found it.
We show the worse effect of usingdiscovery time in the experiments.4 ExperimentsWe will describe the implementation methods and experi-mental results in this section.
Our methods include offlinedictionary building and online test.
In offline training, ourfirst step is to mine YQQs.
A commercial search enginecompany provided us with six months of query logs.
Weextracted a list of YQQs using Section 3?s method.
Foreach of the YQQs, we run the search engine and outputthe top N results.
For each document, we used the methoddescribed in Section 3 to recognize the year-stamp andfind the oldest and the newest page.
If there are multipleurls with the same yearstamp, we choose the first oldestand the first most recent.
Next,we calculated the boost-ing value according to Eq.
1.
Each query has a boostingvalue.
For online test, a user?s query is matched with eachof the YQQs in the dictionary.
If an exact match is found,the boosting value will be added to the base ranking scoreiff the document has the newest yearstamp.For evaluating our methods, we randomly extracted600 YQQs from the dictionary.
We extracted the top-5search results for each of queries using the base rankingfunction and the proposed ranking function.
We askedhuman editors to judge all the scraped results.
We usedfive judgment grades: Perfect, Excellent, Good, Fair,and Bad.
Editors were instructed to consider temporalissues when judging.
For example, sigir2004 is givena worse grade than sigir2009.
To avoid bias, we ad-vised editors to retain relevance as their primary judg-ment criteria.
Our evaluation metric is relative changein DCG, %?dcg = DCGproposed?DCGbaselineDCGbaseline , where DCG isthe traditional Discounted Cumulative Gain (Jarvelin andKekalainen, 2002).4.1 Effect of the proposed boosting methodOur experimental results are shown in Table 2, where wecompared our work with the existing methods.
While wecannot apply (Li and Croft, 2003)?s approach directly be-cause first, our search engine is not based on languagemodeling; second, it is impossible to obtain exact times-tamp for web pages as (Li and Croft, 2003) did in thetrack evaluation.
However, we tried to simulate (Li andCroft, 2003)?s approach in web search by using the linearintegration method exactly as the same as(Li and Croft,2003) by adding a time-based function with our baseranking function.
For the timestamp, we used discoverytime in the time-based function.
The parameters (?, ?
)have the exact same meaning as in (Li and Croft, 2003)but were tuned according to our base ranking function.With regards to the approach by (Diaz and Jones, 2004),we ranked the web pages in decreasing order of discov-ery time.
Our own approaches were tested under optionswith and without using adaptation.
For no adaption, welet the e of Eq.1 equal to 0, meaning no score differencebetween the oldest document and the newest documentwas captured, but a constant value was used.
It is equiv-alent to an open loop in Fig.1.
For adaption, we used theranking errors to adjust the base ranking.
In the Table weused multiple ks to show the effect of changing k. Usingdifferent k can have a big impact on the performance.
Thebest value we found was k = 0.3.
In this experiment, welet ?
(q) = 0 so that the result responds to k only.Our approach is significantly better than the existingmethods.
Both of the two existing methods producedworse results than the baseline, which shows the ap-167Li & Croft (?, ?
)=(0.2,2.0) -0.5(?, ?
)=(0.2,4.0) -1.2Diaz & Jones -4.5?No adaptation (e = 0, k=0.3 1.2open loop) k=0.4 0.8Adaptation (closed loop) k=0.3 6.6?k=0.4 6.2?Table 2: %?dcg of proposed method comparing withexisting methods.A sign ???
indicates statistical signifi-cance (p-value<0.05)?
0 0.2 0.4 0.6 0.8 1.0%?dcg 6.6?
7.8?
8.4?
4.5 2.1 -0.2?Table 3: Effect of confidence as changing ?.proaches may be inappropriate for Web search.
Not sur-prisingly, using adaption achieved much better resultsthan without using adaption.
Thus, these experimentsprove the effectiveness of our proposed methods.Another important parameter in the Eq.1 is the confi-dence score ?
(q), which indicates the confidence of queryto be YQQ.
In Eq.
1, ?
is used to adjusting ?(q).
Weobserved dcg gain for each different ?.
The results areshown in Table 3.
The value of ?
needs to be tuned fordifferent base ranking functions.
A higher ?
can hurt per-formance.
In our experiments, the best value of 0.4 gave a8.4% statistically significant gain in DCG.
The ?
= 0 set-ting means we turn off confidence, which results in lowerperformance.
Thus, using YQQ confidence is effective.5 Discussions and conclusionsIn this paper, we proposed a novel approach to solveYQQ ranking problem, which is a problem that seemsto plague most major commercial search engines.
Ourapproach for handling YQQs does not involve any queryexpansion that adds a year to the query.
Instead, keepingthe user?s query intact, we re-rank search results by ad-justing the base ranking function.
Our work assumes theintent of YQQs is to find documents about the most recentyear.
For this reason, we use YQQ confidence to measurethe probability of this intent.
As our results showed, ourproposed method is highly effective.
A real example isgiven in Fig.
2 to show the significant improvement byour method.Our adaptive methods are not limited to YQQs only.We believe this framework can be applied to any categoryof queries once a query classification and a score detectorhave been implemented.Figure 2: Ranking improvement for query ICML by ourmethod: before re-rank(left) and after(right)ReferencesEugene Agichtein, Eric Brill, and Susan Dumais.
2006.Improving web search ranking by incorporating userbehavior information.
In SIGIR ?06, pages 19?26.Fernando Diaz and Rosie Jones.
2004.
Using temporalprofiles of queries for precision prediction.
In Proc.27th Ann.
Intl.
ACM SIGIR Conf.
on Research and De-velopment in Information Retrieval, pages 18?24, NewYork, NY, USA.
ACM.Yoav Freund, Raj D. Iyer, Robert E. Schapire, and YoramSinger.
1998.
An efficient boosting algorithm forcombining preferences.
In ICML ?98: Proceedingsof the Fifteenth International Conference on MachineLearning, pages 170?178.Kalervo Jarvelin and Jaana Kekalainen.
2002.
Cumu-lated gain-based evaluation of IR techniques.
ACMTransactions on Information Systems, 20:2002.Thorsten Joachims.
2002.
Optimizing search enginesusing clickthrough data.
In KDD ?02: Proceedingsof the eighth ACM SIGKDD international conferenceon Knowledge discovery and data mining, pages 133?142.Xiaoyan Li and W. Bruce Croft.
2003.
Time-basedlanguage models.
In Proc.
12th Intl.
Conf.
on Infor-mation and Knowledge Management, pages 469?475,New York, NY, USA.
ACM.Jaime Teevan, Susan T. Dumais, and Eric Horvitz.
2005.Personalizing search via automated analysis of inter-ests and activities.
In SIGIR ?05, pages 449?456.Chengxiang Zhai and John Lafferty.
2004.
A study ofsmoothing methods for language models applied to in-formation retrieval.
ACM Trans.
Inf.
Syst., 22(2):179?214.Zhaohui Zheng, Keke Chen, Gordon Sun, and HongyuanZha.
2007.
A regression framework for learning rank-ing functions using relative relevance judgments.
InSIGIR ?07, pages 287?294.168
