Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 186?195,Honolulu, October 2008. c?2008 Association for Computational LinguisticsRevisiting Readability: A Unified Framework for Predicting Text QualityEmily PitlerComputer and Information ScienceUniversity of PennsylvaniaPhiladelphia, PA 19104, USAepitler@seas.upenn.eduAni NenkovaComputer and Information ScienceUniversity of PennsylvaniaPhiladelphia, PA 19104, USAnenkova@seas.upenn.eduAbstractWe combine lexical, syntactic, and discoursefeatures to produce a highly predictive modelof human readers?
judgments of text readabil-ity.
This is the first study to take into ac-count such a variety of linguistic factors andthe first to empirically demonstrate that dis-course relations are strongly associated withthe perceived quality of text.
We show thatvarious surface metrics generally expected tobe related to readability are not very good pre-dictors of readability judgments in our WallStreet Journal corpus.
We also establish thatreadability predictors behave differently de-pending on the task: predicting text readabil-ity or ranking the readability.
Our experi-ments indicate that discourse relations are theone class of features that exhibits robustnessacross these two tasks.1 IntroductionThe quest for a precise definition of text quality?pinpointing the factors that make text flow and easyto read?has a long history and tradition.
Way backin 1944 Robert Gunning Associates was set up, of-fering newspapers, magazines and business firmsconsultations on clear writing (Gunning, 1952).In education, teaching good writing technique andgrading student writing has always been of keyimportance (Spandel, 2004; Attali and Burstein,2006).
Linguists have also studied various aspects oftext flow, with cohesion-building devices in English(Halliday and Hasan, 1976), rhetorical structure the-ory (Mann and Thompson, 1988) and centering the-ory (Grosz et al, 1995) among the most influentialcontributions.Still, we do not have unified computational mod-els that capture the interplay between various as-pects of readability.
Most studies focus on a sin-gle factor contributing to readability for a given in-tended audience.
The use of rare words or technicalterminology for example can make text difficult toread for certain audience types (Collins-Thompsonand Callan, 2004; Schwarm and Ostendorf, 2005;Elhadad and Sutaria, 2007).
Syntactic complexityis associated with delayed processing time in un-derstanding (Gibson, 1998) and is another factorthat can decrease readability.
Text organization (dis-course structure), topic development (entity coher-ence) and the form of referring expressions also de-termine readability.
But we know little about the rel-ative importance of each factor and how they com-bine in determining perceived text quality.In our work we use texts from the Wall StreetJournal intended for an educated adult audienceto analyze readability factors including vocabulary,syntax, cohesion, entity coherence and discourse.We study the association between these features andreader assigned readability ratings, showing that dis-course and vocabulary are the factors most stronglylinked to text quality.
In the easier task of text qual-ity ranking, entity coherence and syntax featuresalso become significant and the combination of fea-tures allows for ranking prediction accuracy of 88%.Our study is novel in the use of gold-standard dis-course features for predicting readability and the si-multaneous analysis of various readability factors.1862 Related work2.1 Readability with respect to intendedreadersThe definition of what one might consider to bea well-written and readable text heavily dependson the intended audience (Schriver, 1989).
Obvi-ously, even a superbly written scientific paper willnot be perceived as very readable by a lay personand a great novel might not be appreciated by athird grader.
As a result, the vast majority of priorwork on readability deals with labeling texts withthe appropriate school grade level.
A key observa-tion in even the oldest work in this area is that thevocabulary used in a text largely determines its read-ability.
More common words are easier, so somemetrics measured text readability by the percent-age of words that were not among the N most fre-quent in the language.
It was also observed that fre-quently occurring words are often short, so wordlength was used to approximate readability morerobustly than using a predefined word frequencylist.
Standard indices were developed based on thelink between word frequency/length and readabil-ity, such as Flesch-Kincaid (Kincaid, 1975), Auto-mated Readability Index (Kincaid, 1975), GunningFog (Gunning, 1952), SMOG (McLaughlin, 1969),and Coleman-Liau (Coleman and Liau, 1975).
Theyuse only a few simple factors that are designed tobe easy to calculate and are rough approximationsto the linguistic factors that determine readability.For example, Flesch-Kincaid uses the average num-ber of syllables per word to approximate vocabularydifficulty and the average number of words per sen-tence to approximate syntactic difficulty.In recent work, the idea of linking word frequencyand text readability has been explored for makingmedical information more accessible to the generalpublic.
(Elhadad and Sutaria, 2007) classified wordsin medical texts as familiar or unfamiliar to a gen-eral audience based on their frequencies in corpora.When a description of the unfamiliar terms was pro-vided, the perceived readability of the texts almostdoubled.A more general and principled approach to usingvocabulary information for readability decisions hasbeen the use of language models.
For any given text,it is easy to compute its likelihood under a given lan-guage model, i.e.
one for text meant for children,or for text meant for adults, or for a given gradelevel.
(Si and Callan, 2001), (Collins-Thompson andCallan, 2004), (Schwarm and Ostendorf, 2005), and(Heilman et al, 2007) used language models to pre-dict the suitability of texts for a given school gradelevel.
But even for this type of task other factorsbesides vocabulary use are at play in determiningreadability.
Syntactic complexity is an obvious fac-tor: indeed (Heilman et al, 2007) and (Schwarm andOstendorf, 2005) also used syntactic features, suchas parse tree height or the number of passive sen-tences, to predict reading grade levels.
For the taskof deciding whether a text is written for an adult orchild reader, (Barzilay and Lapata, 2008) found thatadding entity coherence to (Schwarm and Ostendorf,2005)?s list of features improves classification accu-racy by 10%.2.2 Readability as coherence for competentlanguage usersIn linguistics and natural language processing, thetext properties rather than those of the reader are em-phasized.
Text coherence is defined as the ease withwhich a person (tacitly assumed to be a competentlanguage user) understands a text.
Coherent text ischaracterized by various types of cohesive links thatfacilitate text comprehension (Halliday and Hasan,1976).In recent work, considerable attention has beendevoted to entity coherence in text quality, espe-cially in relation to information ordering.
In manyapplications such as text generation and summariza-tion, systems need to decide the order in which se-lected sentences or generated clauses should be pre-sented to the user.
Most models attempting to cap-ture local coherence between sentences were basedon or inspired by centering theory (Grosz et al,1995), which postulated strong links between thecenter of attention in comprehension of adjacentsentences and syntactic position and form of refer-ence.
In a detailed study of information orderingin three very different corpora, (Karamanis et al, toappear) assessed the performance of various formu-lations of centering.
Their results were somewhatunexpected, showing that while centering transitionpreferences were useful, the most successful strat-egy for information ordering was based on avoid-187ing rough shifts, that is, sequences of sentences thatshare no entities in common.
This supports previousfindings that such types of transitions are associatedwith poorly written text and can be used to improvethe accuracy of automatic grading of essays basedon various non-discourse features (Miltsakaki andKukich, 2000).
In a more powerful generalizationof centering, Barzilay and Lapata (2008) developeda novel approach which doesn?t postulate a prefer-ence for any type of transition but rather computesa set of features that capture transitions of all kindsin the text and their relative proportion.
Their en-tity coherence features prove to be very suitable forvarious tasks, notably for information ordering andreading difficulty level.Form of reference is also important in well-written text and appropriate choices lead to im-proved readability.
Use of pronouns for referenceto highly salient entities is perceived as more de-sirable than the use of definite noun phrases (Gor-don et al, 1993; Krahmer and Theune, 2002).
Thesyntactic forms of first mention?when an entity isfirst introduced in a text?differ from those of subse-quent mentions (Poesio and Vieira, 1998; Nenkovaand McKeown, 2003) and can be exploited for im-proving and predicting text coherence (Siddharthan,2003; Nenkova and McKeown, 2003; Elsner andCharniak, 2008).3 DataThe objective of our study is to analyze variousreadability factors, including discourse relations, be-cause few empirical studies exist that directly linkdiscourse structure with text quality.
In the past,subsections of the Penn Treebank (Marcus et al,1994) have been annotated for discourse relations(Carlson et al, 2001; Wolf and Gibson, 2005).
Forour study we chose to work with the newly releasedPenn Discourse Treebank which is the largest anno-tated resource which focuses exclusively on implicitlocal relations between adjacent sentences and ex-plicit discourse connectives.3.1 Discourse annotationThe Penn Discourse Treebank (Prasad et al, 2008)is a new resource with annotations of discourse con-nectives and their senses in the Wall Street Journalportion of the Penn Treebank (Marcus et al, 1994).All explicit relations (those marked with a discourseconnective) are annotated.
In addition, each adjacentpair of sentences within a paragraph is annotated.
Ifthere is a discourse relation, then it is marked im-plicit and annotated with one or more connectives.
Ifthere is a relation between the sentences but adding aconnective would be inappropriate, it is marked Al-tLex.
If the consecutive sentences are only relatedby entity-based coherence (Knott et al, 2001) theyare annotated with EntRel.
Otherwise, they are an-notated with NoRel.Besides labeling the connective, the PDTB alsoannotates the sense of each relation.
The relationsare organized into a hierarchy.
The top level rela-tions are Expansion, Comparison, Contingency, andTemporal.
Briefly, an expansion relation means thatthe second clause continues the theme of the firstclause, a comparison relation indicates that some-thing in the two clauses is being compared, contin-gency means that there is a causal relation betweenthe clauses, and temporal means they occur either atthe same time or sequentially.3.2 Readability ratingsWe randomly selected thirty articles from the WallStreet Journal corpus that was used in both the PennTreebank and the Penn Discourse Treebank.1 Eacharticle was read by at least three college students,each of whom was given unlimited time to read thetexts and perform the ratings.2 Subjects were askedthe following questions:?
How well-written is this article??
How well does the text fit together??
How easy was it to understand??
How interesting is this article?For each question, they provided a rating between 1and 5, with 5 being the best and 1 being the worst.1One of the selected articles was missing from the PennTreebank.
Thus, results that do not require syntactic informa-tion (Tables 1, 2, 4, and 6) are over all thirty articles, whileTables 3, 5, and 7 report results for the twenty-nine articles withTreebank parse trees.2(Lapata, 2006) found that human ratings are significantlycorrelated with self-paced reading times, a more direct measureof processing effort which we plan to explore in future work.188After collecting the data, it turned out that most ofthe time subjects gave the same rating to all ques-tions.
For competent language users, we view textreadability and text coherence as equivalent prop-erties, measuring the extent to which a text is wellwritten.
Thus for all subsequent analysis, we willuse only the first question (?On a scale of 1 to 5,how well written is this text??).
The score of an arti-cle was then the average of all the ratings it received.The article scores ranged from 1.5 to 4.33, with amean of 3.2008 and a standard deviation of .7242.The median score was 3.286.We define our task as predicting this average rat-ing for each article.
Note that this task may bemore difficult than predicting reading level, as eachof these articles appeared in the Wall Street Journaland thus is aimed at the same target audience.
Wesuspected that in classifying adult text, more subtlefeatures might be necessary.4 Identifying correlates of text quality4.1 Baseline measuresWe first computed the Pearson correlation coeffi-cients between the simple metrics that most tradi-tional readability formulas use and the average hu-man ratings.
These results are shown in Table 1.
Wetested the average number of characters per word,average number of words per sentence, maximumnumber of words per sentence, and article length(F7).3 Article length (F7) was the only significantbaseline factor, with correlation of -0.37.
Longer ar-ticles are perceived as less well-written and harderto read than shorter ones.
None of the other baselinemetrics were close to being significant predictors ofreadability.Average Characters/Word r = -.0859, p = .6519Average Words/Sentence r = .1637, p = .3874Max Words/Sentence r = .0866, p = .6489F7 text length r = -.3713, p = .0434Table 1: Baseline readability features3For ease of reference, we number each non-baseline featurein the text and tables.4.2 VocabularyWe use a unigram language model, where the prob-ability of an article is:?wP (w|M)C(w) (1)P (w|M) is the probability of word-type w accord-ing to a background corpus M , and C(w) is thenumber of times w appears in the article.The log likelihood of an article is then:?wC(w) log(P (w|M)) (2)Note that this model will be biased in favor ofshorter articles.
Since each word has probability lessthan 1, the log probability of each word is less than0, and hence including additional words decreasesthe log likelihood.
We compensate for this by per-forming linear regressions with the unigram log like-lihood and with the number of words in the article asan additional variable.The question then arises as to what to use as abackground corpus.
We chose to experiment withtwo corpora: the entire Wall Street Journal corpusand a collection of general AP news, which is gen-erally more diverse than the financial news found inthe WSJ.
We predicted that the NEWS vocabularywould be more representative of the types of wordsour readers would be familiar with.
In both cases weused Laplace smoothing over the word frequenciesand a stoplist.The vocabulary features we used are article like-lihood estimated from a language model from WSJ(F5), and article likelihood according to a unigramlanguage model from NEWS (F6).
We also combinethe two likelihood features with article length, in or-der to get a better estimate of the language model?sinfluence on readability independent of the length ofthe article.F5 Log likelihood, WSJ r = .3723, p = .0428F6 Log likelihood, NEWS r= .4497, p = .0127LL with length, WSJ r = .3732, p = .0422LL with length, NEWS r = .6359, p = .0002Table 2: Vocabulary featuresBoth vocabulary-based features (F5 and F6) aresignificantly correlated with the readability judg-ments, with p-values smaller than 0.05 (see Table 2).189The correlations are positive: the more probable anarticle was based on its vocabulary, the higher it wasgenerally rated.
As expected, the NEWS model thatincluded more general news stories had a higher cor-relation with people?s judgments.
When combinedwith the length of the article, the unigram languagemodel from the NEWS corpus becomes very predic-tive of readability, with the correlation between thetwo as high as 0.63.4.3 Syntactic featuresSyntactic constructions affect processing difficultyand so might also affect readability judgments.We examined the four syntactic features used in(Schwarm and Ostendorf, 2005): average parse treeheight (F1), average number of noun phrases persentence (F2), average number of verb phrases persentence (F3), and average number of subordinateclauses per sentence(SBARs in the Penn Treebanktagset) (F4).
The sentence ?We?re talking aboutyears ago [SBAR before anyone heard of asbestoshaving any questionable properties].?
contains anexample of an SBAR clause.Having multiple noun phrases (entities) in eachsentence requires the reader to remember moreitems, but may make the article more interesting.
(Barzilay and Lapata, 2008) found that articles writ-ten for adults tended to contain many more entitiesthan articles written for children.
While includingmore verb phrases in each sentence increases thesentence complexity, adults might prefer to have re-lated clauses explicitly grouped together.F1 Average Parse Tree Height r = -.0634, p = .7439F2 Average Noun Phrases r = .2189, p = .2539F3 Average Verb Phrases r = .4213, p = .0228F4 Average SBARs r = .3405, p = .0707Table 3: Syntax-related featuresThe correlations between readability and syntac-tic features is shown in Table 3.
The strongest corre-lation is that between readability and number of verbphrases (0.42).
This finding is in line with prescrip-tive clear writing advice (Gunning, 1952; Spandel,2004), but is to our knowledge novel in the compu-tational linguistics literature.
As (Bailin and Graf-stein, 2001) point out, the sentences in (1) are eas-ier to comprehend than the sentences in (2), eventhough they are longer.
(1) It was late at night, but it was clear.
The starswere out and the moon was bright.
(2) It was late at night.
It was clear.
The stars wereout.
The moon was bright.Multiple verb phrases in one sentence may be in-dicative of explicit discourse relations, which wewill discuss further in section 4.6.Surprisingly, the use of clauses introducedby a (possibly empty) subordinating conjunction(SBAR), are actually positively correlated (and al-most approaching significance) with readability.
Sowhile for children or less educated adults these con-structions might pose difficulties, they were favoredby our assessors.
On the other hand, the averageparse tree height negatively correlated with readabil-ity as expected, but surprisingly the correlation isvery weak (-0.06).4.4 Elements of lexical cohesionIn their classic study of cohesion in English, (Hal-liday and Hasan, 1976) discuss the various aspectsof well written discourse, including the use of cohe-sive devices such as pronouns, definite descriptionsand topic continuity from sentence to sentence.4 Tomeasure the association between these features andreadability rankings, we compute the number of pro-nouns per sentence (F11) and the number of defi-nite articles per sentence (F12).
In order to qual-ify topic continuity from sentence to sentence inthe articles, we compute average cosine similarity(F8), word overlap (F9) and word overlap over justnouns and pronouns (F10) between pairs of adjacentsentences5.
Each sentence is turned into a vectorof word-types, where each type?s value is its tf-idf(where document frequency is computed over all thearticles in the WSJ corpus).
The cosine similaritymetric is then:cos (s, t) =s ?
t|s| |t|(3)4Other cohesion building devises discussed by Hallidayand Hansan include lexical reiteration and discourse relations,which we address next.5Similar features have been used for automatic essay grad-ing as well (Higgins et al, 2004).190F8 Avr.
Cosine Overlap r = -.1012, p = .5947F9 Avr.
Word Overlap r = -.0531, p = .7806F10 Avr.
Noun+Pronoun Overlap r = .0905, p = .6345F11 Avr.
# Pronouns/Sent r = .2381, p = .2051F12 Avr # Definite Articles r = .2309, p = .2196Table 4: Superficial measures of topic continuity and pro-noun and definite description useNone of these features correlate significantly withreadability as can be seen from the results in Ta-ble 4.
The overlap features are particularly badpredictors of readability, with average word/cosineoverlap in fact being negatively correlated with read-ability.
The form of reference?use of pronounsand definite descriptions?exhibit a higher correla-tion with readability (0.23), but these values are notsignificant for the size of our corpus.4.5 Entity coherenceWe use the Brown Coherence Toolkit6 to computeentity grids (Barzilay and Lapata, 2008) for each ar-ticle.
In each sentence, an entity is identified as thesubject (S), object (O), other (X) (for example, partof a prepositional phrase), or not present (N).
Theprobability of each transition type is computed.
Forexample, an S-O transition occurs when an entityis the subject in one sentence then an object in thenext; X-N transition occurs when an entity appearsin non-subject or object position in one sentence andnot present in the next, etc.7 The entity coherencefeatures are the probability of each of these pairs oftransitions, for a total of 16 features (F17?32; seecomplete results in Table 5).None of the entity grid features are significantlycorrelated with the readability ratings.
One very in-teresting result is that the proportion of S-S transi-tions in which the same entity was mentioned in sub-ject position in two adjacent sentences, is negativelycorrelated with readability.
In centering theory, thisis considered the most coherent type of transition,keeping the same center of attention.
Moreover, thefeature most strongly correlated with readability isthe S-N transition (0.31) in which the subject of onesentence does not appear at all in the following sen-6http://www.cs.brown.edu/ melsner/manual.html7The Brown Coherence Toolkit identifies NPs as the sameentity if they have identical head nouns.F17 Prob.
of S-S transition r = -.1287, p = .5059F18 Prob.
of S-O transition r = -.0427, p = .8261F19 Prob.
of S-X transition r = -.1450, p = .4529F20 Prob.
of S-N transition r = .3116, p = .0999F21 Prob.
of O-S transition r = .1131, p = .5591F22 Prob.
of O-O transition r = .0825, p = .6706F23 Prob.
of O-X transition r = .0744, p = .7014F24 Prob.
of O-N transition r = .2590, p = .1749F25 Prob.
of X-S transition r = .1732, p = .3688F26 Prob.
of X-O transition r = .0098, p = .9598F27 Prob.
of X-X transition r = -.0655, p = .7357F28 Prob.
of X-N transition r = .1319, p = .4953F29 Prob.
of N-S transition r = .1898, p = .3242F30 Prob.
of N-O transition r = .2577, p = .1772F31 Prob.
of N-X transition r = .1854, p = .3355F32 Prob.
of N-N transition r = -.2349, p = .2200Table 5: Linear correlation between human readabilityratings and entity coherence.tence.
Of course, it is difficult to interpret the en-tity grid features one by one, since they are inter-dependent and probably it is the interaction of fea-tures (relative proportions of transitions) that captureoverall readability patterns.4.6 Discourse relationsDiscourse relations are believed to be a major factorin text coherence.
We computed another languagemodel which is over discourse relations instead ofwords.
We treat each text as a bag of relations ratherthan a bag of words.
Each relation is annotatedfor both its sense and how it is realized (implicitor explicit).
For example, one text might contain{Implicit Comparison, Explicit Temporal, NoRel}.We computed the probability of each of our articlesaccording to a multinomial model, where the proba-bility of a text with n relation tokens and k relationtypes is:P (n)n!x1!...xk!px11 ...pxkk (4)P (n) is the probability of an article having lengthn, xi is the number of times relation i appeared, andpi is the probability of relation i based on the PennDiscourse Treebank.
P (n) is the maximum likeli-hood estimation of an article having n discourse re-lations based on the entire Penn Discourse Treebank(the number of articles with exactly n discourse re-lations, divided by the total number of articles).191The log likelihood of an article based on its dis-course relations (F13) feature is defined as:log(P (n)) + log(n!)
+k?i=1(xi log(pi)?
log(xi!
))(5)The multinomial distribution is particularly suit-able, because it directly incorporates length, whichsignificantly affects readability as we discussed ear-lier.
It also captures patterns of relative frequency ofrelations, unlike the simpler unigram model.
Notealso that this equation has an advantage over the un-igram model that was not present for vocabulary.While every article contains at least one word, somearticles do not contain any discourse relations.
Sincethe PDTB annotated all explicit relations and re-lations between adjacent sentences in a paragraph,an article with no discourse connectives and onlysingle sentence paragraphs would not contain anyannotated discourse relations.
Under the unigrammodel, these articles?
probabilities cannot be com-puted.
Under the multinomial model, the probabil-ity of an article with zero relations is estimated asPr(N = 0), which can be calculated from the cor-pus.As in the case of vocabulary features, the presenceof more relations will lead to overall lower probabil-ities so we also consider the number of discourserelations (F14) and the log likelihood combined withthe number of relations as features.
In order to iso-late the effect of the type of discourse relation (ex-plicitly expressed by a discourse connective such as?because?
or ?however?
versus implicitly expressedby adjacency), we also compute multinomial modelfeatures for the explicit discourse relations (F15) andover just the implicit discourse relations (F16).F13 LogL of discourse rels r = .4835, p = .0068F14 # of discourse relations r = -.2729, p = .1445LogL of rels with # of rels r = .5409, p = .0020# of relations with # of words r = .3819, p = .0373F15 Explicit relations only r = .1528, p = .4203F16 Implicit relations only r = .2403, p = .2009Table 6: Discourse featuresThe likelihood of discourse relations in the textunder a multinomial model is very highly and sig-nificantly correlated with readability ratings, espe-cially after text length is taken into account.
Cor-relations are 0.48 and 0.54 respectively.
The prob-ability of the explicit relations alone is not a suffi-ciently strong indicator of readability.
This fact isdisappointing as the explicit relations can be iden-tified much more easily in unannotated text (Pitleret al, 2008).
Note that the sequence of just the im-plicit relations is also not sufficient.
This observa-tion implies that the proportion of explicit and im-plicit relations may be meaningful but we leave theexploration of this issue for later work.4.7 Summary of findingsSo far, we introduced six classes of factors that havebeen discussed in the literature as readability cor-relates.
Through statistical tests of associations weidentified the individual factors significantly corre-lated with readability ratings.
These are, in decreas-ing order of association strength:LogL of Discourse Relations (r = .4835)LogL, NEWS (r= .4497)Average Verb Phrases (.4213)LogL, WSJ (r = .3723)Number of words (r = -.3713)Vocabulary and discourse relations are thestrongest predictors of readability, followed by aver-age number of verb phrases and length of the text.This empirical confirmation of the significance ofdiscourse relations as a readability factor is novel forthe computational linguistics literature.
Note thoughthat for our work we use oracle discourse annota-tions directly from the PDTB and no robust systemsfor automatic discourse annotation exist today.The significance of the average number of verbphrases as a readability predictor is somewhat sur-prising but intriguing.
It would lead to reexamina-tion of the role of verbs/predicates in written text,which we also plan to address in future work.
Noneof the other factors showed significant associationwith readability ratings, even though some correla-tions had relatively large positive values.5 Combining readability factorsIn this section, we turn to the question of how thecombination of various factors improves the predic-tion of readability.
We use the leaps package in Rto find the best subset of features for linear regres-sion, for subsets of size one to eight.
We use the192squared multiple correlation coefficient (R2) to as-sess the effectiveness of predictions.
R2 is the pro-portion of variance in readability ratings explainedby the model.
If the model predicts readability per-fectly, R2 = 1, and if the model has no predictivecapability, R2 = 0.F13, R2 = 0.2662F6 + F7, R2 = 0.4351F6 + F7 + F13, R2 = 0.5029F6 + F7 + F13 + F14, R2 = 0.6308F1 + F6 + F7 + F10 + F13, R2 = 0.6939F1 + F6 + F7 + F10 + F13 + F23, R2 = 0.7316F1 + F6 + F7 + F10 + F13 + F22 + F23, R2 = 0.7557F1+F6+F7+F10+F11+F13+F19+F30, R2 = 0.776.The linear regression results confirm the expec-tation that the combination of different factors is arather complex issue.
As expected, discourse, vo-cabulary and length which were the significant in-dividual factors appear in the best model for eachfeature set size.
Their combination gives the bestresult for regression with three predictors, and theyexplain half of the variance in readability ratings,R2 = 0.5029.But the other individually significant feature, av-erage number of verb phrases per sentence (F3)never appears in the best models.
Instead, F1?thedepth of the parse tree?appears in the best modelwith more than four features.Also unexpectedly, two of the superficial cohe-sion features appear in the larger models: F10 isthe average word overlap over nouns and pronounsand F11 is the average number of pronouns per sen-tence.
Entity grid features also make their way intothe best models when more features are used for pre-diction: S-X, O-O, O-X, N-O transitions (F19, F22,F23, F30).6 Readability as rankingIn this section we consider the problem of pairwiseranking of text readability.
That is, rather than try-ing to predict the readability of a single document,we consider pairs of documents and predict whichone is better.
This task may in fact be the more natu-ral one, since in most applications the main concernis with the relative quality of articles rather than theirabsolute scores.
This setting is also beneficial interms of data use, because each pair of articles withdifferent average readability scores now becomes adata point for the classification task.We thus create a classification problem: given twoarticles, is article 1 more readable than article 2?For each pair of texts whose readability ratings onthe 1 to 5 scale differed by at least 0.5, we formone data point for the ranking problem, resulting in243 examples.
The predictors are the differences be-tween the two articles?
features.
For classification,we used WEKA?s linear support vector implemen-tation (SMO) and performance was evaluated using10-fold cross-validation.Features AccuracyNone (Majority Class) 50.21%ALL 88.88%log l discourse rels 77.77%number discourse rels 74.07%N-O transition 70.78%O-N transition 69.95%Avg VPs sen 69.54%log l NEWS 66.25%number of words 65.84%Grid only 79.42%Discourse only 77.36%Syntax only 74.07%Vocab only 66.66%Length only 65.84%Cohesion only 64.60%no cohesion 89.30%no vocab 88.88%no length 88.47%no discourse 88.06%no grid 84.36%no syntax 82.71%Table 7: SVM prediction accuracy, linear kernelThe classification results are shown in Table 7.When all features are used for prediction, the ac-curacy is high, 88.88%.
The length of the articlecan serve as a baseline feature?longer articles areranked lower by the assessors, so this feature canbe taken as baseline indicator of readability.
Onlysix features used by themselves lead to accuracieshigher than the length baseline.
These results indi-cate that the most important individual factors in thereadability ranking task, in decreasing order of im-portance, are log likelihood of discourse relations,number of discourse relations, N-O transitions, O-N193transitions, average number of VPs per sentence andtext probability under a general language model.In terms of classes of features, the 16 entitygrid features perform the best, leading to an accu-racy of 79.41%, followed by the combination ofthe four discourse features (77.36%), and syntaxfeatures (74.07%).
This is evidence for the factthat there is a complex interplay between readabil-ity factors: the entity grid factors which individ-ually have very weak correlation with readabilitycombine well, while adding the three additional dis-course features to the likelihood of discourses rela-tions actually worsens performance slightly.
Simi-lar indication for interplay between features is pro-vided by the class ablation classification results, inwhich classes of features are removed.
Surprisingly,removing syntactic features causes the biggest dete-rioration in performance, a drop in accuracy from88.88% to 82.71%.
The removal of vocabulary,length, or discourse features has a minimal negativeimpact on performance, while removing the cohe-sion features actually boosts performance.7 ConclusionWe have investigated which linguistic features cor-relate best with readability judgments.
While sur-face measures such as the average number of wordsper sentence or the average number of charactersper word are not good predictors, there exist syn-tactic, semantic, and discourse features that do cor-relate highly.
The average number of verb phrasesin each sentence, the number of words in the article,the likelihood of the vocabulary, and the likelihoodof the discourse relations all are highly correlatedwith humans?
judgments of how well an article iswritten.While using any one out of syntactic, lexical, co-herence, or discourse features is substantally betterthan the baseline surface features on the discrim-ination task, using a combination of entity coher-ence and discourse relations produces the best per-formance.8 AcknowledgmentsThis work was partially supported by an Inte-grative Graduate Education and Research Trainee-ship grant from National Science Foundation (NS-FIGERT 0504487) and by NSF Grant IIS-07-05671.We thank Aravind Joshi, Bonnie Webber, and theanonymous reviewers for their many helpful com-ments.ReferencesY.
Attali and J. Burstein.
2006.
Automated essay scoringwith e-rater v.2.
The Journal of Technology, Learningand Assessment, 4(3).A.
Bailin and A. Grafstein.
2001.
The linguistic assump-tions underlying readability formulae a critique.
Lan-guage and Communication, 21(3):285?301.R.
Barzilay and M. Lapata.
2008.
Modeling local coher-ence: An entity-based approach.
Computational Lin-guistics, 34(1):1?34.L.
Carlson, D. Marcu, and M. E. Okurowski.
2001.Building a discourse-tagged corpus in the frameworkof rhetorical structure theory.
In Proceedings of theSecond SIGdial Workshop, pages 1?10.M.
Coleman and TL Liau.
1975.
A computer readabil-ity formula designed for machine scoring.
Journal ofApplied Psychology, 60(2):283?284.K.
Collins-Thompson and J. Callan.
2004.
A languagemodeling approach to predicting reading difficulty.
InProceedings of HLT/NAACL?04.Noemie Elhadad and Komal Sutaria.
2007.
Mining a lex-icon of technical terms and lay equivalents.
In Biolog-ical, translational, and clinical language processing,pages 49?56, Prague, Czech Republic.
Association forComputational Linguistics.M.
Elsner and E. Charniak.
2008.
Coreference-inspiredcoherence modeling.
In Proceedings of ACL-HLT?08,(short paper).E.
Gibson.
1998.
Linguistic complexity: locality of syn-tactic dependencies.
Cognition, 68:1?76.P.
Gordon, B. Grosz, and L. Gilliom.
1993.
Pronouns,names, and the centering of attention in discourse.Cognitive Science, 17:311?347.B.
Grosz, A. Joshi, and S. Weinstein.
1995.
Centering:a framework for modelling the local coherence of dis-course.
Computational Linguistics, 21(2):203?226.Robert Gunning.
1952.
The technique of clear writing.McGraw-Hill; Fouth Printing edition.Michael A.K.
Halliday and Ruqaiya Hasan.
1976.
Cohe-sion in English.
Longman Group Ltd, London, U.K.M.
Heilman, K. Collins-Thompson, J. Callan, and M. Es-kenazi.
2007.
Combining Lexical and GrammaticalFeatures to Improve Readability Measures for Firstand Second Language Texts.
Proceedings of NAACLHLT, pages 460?467.D.
Higgins, J. Burstein, D. Marcu, and C. Gentile.
2004.Evaluating multiple aspects of coherence in student es-says.
In Proceedings of HLT/NAACL?04.194N.
Karamanis, M. Poesio, C. Mellish, and J.
Oberlander.
(to appear).
Evaluating centering for information or-dering using corpora.
Computational Linguistics.JP Kincaid.
1975.
Derivation of New Readability For-mulas (Automated Readability Index, Fog Count andFlesch Reading Ease Formula) for Navy Enlisted Per-sonnel.A.
Knott, J. Oberlander, M. ODonnell, and C. Mellish.2001.
Beyond elaboration: The interaction of relationsand focus in coherent text.
Text representation: lin-guistic and psycholinguistic aspects, pages 181?196.E.
Krahmer and M. Theune.
2002.
Efficient context-sensitive generation of referring expressions.
In K. vanDeemter and R. Kibble, editors, Information Sharing:Reference and Presupposition in Language Genera-tion and Interpretation, pages 223?264.
CSLI Publi-cations.M.
Lapata.
2006.
Automatic evaluation of informationordering: Kendalls tau.
Computational Linguistics,32(4):471?484.W.
Mann and S. Thompson.
1988.
Rhetorical structuretheory: Towards a functional theory of text organiza-tion.
Text, 8.M.P.
Marcus, B. Santorini, and M.A.
Marcinkiewicz.1994.
Building a Large Annotated Corpus of En-glish: The Penn Treebank.
Computational Linguistics,19(2):313?330.G.H.
McLaughlin.
1969.
SMOG grading: A new read-ability formula.
Journal of Reading, 12(8):639?646.E.
Miltsakaki and K. Kukich.
2000.
The role of centeringtheory?s rough-shift in the teaching and evaluation ofwriting skills.
In Proceedings of ACL?00, pages 408?415.A.
Nenkova and K. McKeown.
2003.
References tonamed entities: a corpus study.
In Proceedings ofHLT/NAACL 2003 (short paper).E.
Pitler, M. Raghupathy, H. Mehta, A. Nenkova, A. Lee,and A. Joshi.
2008.
Easily identifiable discourse re-lations.
In Coling 2008: Companion volume: Postersand Demonstrations, pages 85?88, Manchester, UK,August.M.
Poesio and R. Vieira.
1998.
A corpus-based investi-gation of definite description use.
Computational Lin-guistics, 24(2):183?216.R.
Prasad, N. Dinesh, A. Lee, E. Miltsakaki, L. Robaldo,A.
Joshi, and B. Webber.
2008.
The penn discoursetreebank 2.0.
In Proceedings of LREC?08.KA Schriver.
1989.
Evaluating text quality: the con-tinuum from text-focused toreader-focused methods.Professional Communication, IEEE Transactions on,32(4):238?255.S.
Schwarm and M. Ostendorf.
2005.
Reading level as-sessment using support vector machines and statisticallanguage models.
In Proceedings of ACL?05, pages523?530.L.
Si and J. Callan.
2001.
A statistical model for sci-entific readability.
Proceedings of the tenth interna-tional conference on Information and knowledge man-agement, pages 574?576.A.
Siddharthan.
2003.
Syntactic simplification and TextCohesion.
Ph.D. thesis, University of Cambridge, UK.V.
Spandel.
2004.
Creating writers through 6-trait writ-ing assessment and instruction.
Allyn & Bacon.F.
Wolf and E. Gibson.
2005.
Representing discoursecoherence: A corpus-based study.
Computational Lin-guistics, 31(2):249?288.195
