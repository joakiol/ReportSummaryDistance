A NEW APPROACH TO TEXT UNDERSTANDINGRalph Weischedel, Damaris Ayuso, Sean Boisen, Heidi Fox, Robert IngriaBBN Systems and Technologies10 Moulton St.Cambridge, MA 02138ABSTRACTThis paper first briefly describes the architecture of PLUM, BBN'stext processing system, and then reports on some experimentsevaluating the effectiveness of the design at the component level.Three features are unusual in PLUM's architecture: a domain-independent deterrninistie parser, processing of (the resulting)fragments at the semantic and discourse l vel, and probabilistiemodels.1.
INTRODUCTIONThe PLUM (Probabilistic Language Understanding Model)natural anguage understanding system for extracting datafrom text is based on three unusual features: probabilisticlanguage models, a domain-independent deterministicparser, and processing of (the resulting) fragments at thesemantic and discourse level.
Earlier papers have focusedon the probabilistic aspects of the system \[Weischedel etal., 1991; de Marcken, 1990\]; here we focus on the othertwo design features.While several deterministic parsers have been constructedbased on Marcus's Determinism Hypothesis, PLUM seemsto be the first application system that employs adeterministic parser.
Many systems have been built basedon semantic and discourse-level processing of fragments,most notably systems based on conceptual dependency andscripts \[Schank and Riesbeck, 1981\].
However, PLUMmay be the first system that uses a hybnd of such semantictechniques with the purely syntactic processing of Marcus'sDeterminism Hypothesis, two approaches that seemedtotally antithetical when first proposed.The impact of marrying those two techniques i a robustsystem that produces answers in the application domain inspite of syntactic omplexity, syntactic ill-formedness,extra-grammaticality and a high percentage of unknownwords.
A second impact is that the system can produceanswers at a very early stage of porting it to a new domain.Both of these claims are substantiated in this paper byevaluating the performance of the system as the lexicongrows, without changing the syntactic, semantic, anddiscourse rules of the system.2.
BR IEF  SYNOPSIS  OF  SYSTEMCOMPONENTSMajor system components are shown in the diagram inFigure 1.
We expect he particular implementations tochange and improve substantially during the next two yearsof research and development.
A preprocessor d iven byfinite state rules divides the message into header material(if any), paragraphs, entences, and trailer material (if any).A well-known problem in using deterministic parsing is thefact that most words in English are ambiguous evenregarding part of speech.
In the Foreign BroadcastInformation Service texts of MUC-3, we estimate that thevocabulary had an average ambiguity of over two parts ofspeech in the TREEBANK tag system.ageMorphological AnalyzerFast Partial Parsersem II'Semantic InterpreterI I I?
m oFragment Combinerse\[nn,,II\[ IDiscourse Modulesm'n,,,IIeventl .
~ventm~remplate FillerI Itemplate 1. .
.
template m,Figure 1.
PLUM System ArchitectureIn PLUM, determining the part of speech of highlyambiguous words is performed by well-known Markovmodelling techniques.
Though part of speech ambiguitywas high, the only ambiguity that negatively impactedperformance in extracting the desired information from textwas recognizing proper nouns, since the text is upper caseonly, and the set of names is open-ended, as is the generalvocabulary.
To improve the recognition of Latin Americannames, we employed a statistically derived five-gram (fiveletter) model of words of Spanish origin and a similar five-gram model of English words, under the assumption that316words of Spanish origin in these English texts about LatinAmerica were probably proper names.The parser and grammar are designed to find analyses for anon-overlapping sequence of fragments, as represented bythe multiple fragments in Figure 1.
When cases ofpermanent, predictable ambiguity arise, such as aprepositional phrase that can be attached in multiple ways,most conjoined phrases, commas, and parentheses, theparser closes the analysis of the current fragment (includingall open constituents), and begins the analysis of a newfragment.This is a departure from Marcus's D-Theory proposal,where an ancestor relationship for each constituent must bestated even if a parent relationship cannot be.
Thus, in theexample below, but and no injuries are analyzed, but notplaced under any node.
The departure does not seem tohurt semantic processing, since the critical entities in thetext and some relations between them are found in everysentence, whether syntactically ill-formed, complex, novel,or straightforward.
Rather, the impact was beneficial, forwe were able to produce output of the whole system muchearlier than if the grammar ules, semantic rules, andlexicon had to be more complete.Unlike the previous systems based on conceptualdependency and script application \[Schank and Riesbeck,1981\], this parsing is done using domain-independentsyntactic rules.The deterministic parser employed was developed by deMarcken at MIT \[de Marcken 1990\].
Though we have not(yet) made substantial changes to the parsing code nor tothe grammar, we are replacing his "disambiguator", whichdeals with part-of-speech ambiguity with our stochasticpart-of-speech tagger (POST) \[Meteer, et al 1991\].
Theresulting syntactic omponent is named the Fast PartialParser (FPP).
"Here are the parse fragments generated forthe sentence, "THE BOMBS CAUSED DAMAGE BUTNO INJURIES":("THE BOMBS CAUSED DAMAGE"(S (NP (DET "THE") (N "BOMBS"))(VP (AUX) (VP (V "CAUSED")(NP (N "DAMAGE")))))("BUT" (CONJ "BUT"))("NO INJURIES"(NP (DET "NO")(N "INJURIES")))("."
(PUNCT ".
"))Each fragment is processed by the semantic interpreter,producing a partial semantic representation i  a framelanguage, like KL-ONE.
(See Figure 2.)
Semantic analysisis shallow; for example, in Figure 2 the pp-modifier slot ofthe entity corresponding tothe embassies of the PRC is notsemantically analyzed further by the semantic interpreter,e.g., to determine whether the PRC owns the embassybuildings, whether the PRC uses the embassy buildings,etc.
Shallow analysis is necessary since most of the wordsin an article are semantically unknown, and since it ishighly desirable that some analysis be produced for eachfragment to avoid totally missing information.
(Jacobs etal.
\[1991\] estimate that 75% of the words in these MUCtexts are not relevant.
)The semantic interpreter uses structural rules; nearly all ofthese carry over to all new domains.
Domain-dependent,lexical semantic rules contain traditional case frameinformation, e.g., the logical object of a murder is a livingthing.
The novel aspect in PLUM is that the case framesfor verbs were hypothesized by a statistical inductionalgorithm \[Weischedel, t al., 1991a\].
Each hypothesizedcase frame was manually reviewed over a two day period,rather than the weeks or even months of effort that mightnormally be involved in writing case frames for verbs.
Theframe-based semantic representation for an unusually shortand simple sentence appears in Figure 2.Based on local syntactic and semantic information, afragment combining algorithm combines phrases to providemore complete analyses of the input \[Weischedel, tal.,1991a\].
The current set of fragment combining rules focuson finding conjoined phrases, 1 prepositional phraseattachment 2, appositive recognition, and on correctingsome errors made by the parser (e.g., combining adjacentfragments into a single noun phrase).
Though there was notime to integrate and test this component for use MUC-3 inMay, 1991, an experiment on the improvement in syntacticanalyses produced based on this component is included inthis paper.Our fragment combining code is rule-based, and can takeinto account syntactic ategories, imple properties of thetree configuration (for example, whether a node is the onlychild of its parent), and semantic type.
The simplestattachment strategy is to process the fragments of asentence from left to right, considering each pair ofsuccessive fragments.
For each pair of fragments, allpossible attachment points on the right edge of the leftfragment are considered, starting from the lowest (closest)node.
Some rules consider more than one fragment to theright, for example, combining an NP with commas on eachside into a single appositive NP.
Therefore, as in thedeterministic parser, decisions are made locally, rather thanassuming lobal context.I The parser usually produces fragments where a conjoinedphrase appears because local syntactic nformation is typically notsufficient to reliably predict the correct parse.2 The parser usually does not attach prepositional phrasesbecause of the inherent ambiguity.317"POL ICE  HAVE REPORTED THAT TERRORISTS  TONIGHT BOMBED"\]\['HE EMBASSIES  OF  THE PRC"--C% t.oTICATIONI EVENT -- BOMBING \[ ti-perp-of-object-of:I ENTITY -- PERSON \[social-role-of:  LAW ENFORCEMENTnumber -o f :  PLURAL"~ description-of: "POLICE"ENTITY -- BUILDINGsolcal-role-of:  DIPLOMATICnumber -o f :  PLURALdescr ipt ion-of :  "THE EMBASSIES"det: "THE"pp-modirer :  "OF"ENTITY -- PERSON I ?
"~ social-role-of: TERRORISMnumber -o f :  PLURALdescr ipt ion-of :  "TERRORISTS"ENTITY -- COUNTRY I name-of: "PRC" I description-or: "THE PRC"I det: "THE"\[ canonical-name-of: "PEOPLES REPUBLIC OF CHINA"Figure 2: Example Semantic RepresentationThe discourse component performs three tasks:hypothesizing relevant events from diverse descriptions,recognizing co-reference, and hypothesizing values forcomponents of an event.
Discourse processing wouldnormally look for entities to fill roles in stated predicates,as Hobbs \[1989, 1988\] has argued.
Since completesyntactic accounts of a sentence are not usually found byour system, 3 semantic representations of events and statesof affairs have more unfilled slots (roles) than if completesyntactic analyses were found.
In our case, there are simplymore such unfilled roles, and less syntactic relationshelping out.
A second challenge faced by the discoursecomponent is that reference resolution must be performedwith limited semantic understanding.
Given thesechallenges, it is clear from the test results in MUC-3 thatthe discourse component does reconstruct event structurewell, in spite of missing syntactic and semantic relations.An example frame for an event produced by discourseprocessing appears in Figure 3.
A score of 0 indicates thefiller was found directly by the semantics; a score of 1indicates it was in the same fragment; 2 indicates it was inthe same sentence; 4 indicates it was found in the sameparagraph; and 6 that it was found in an adjacent paragraph.Note that El Salvador, though not in the text, wasintroduced by the definition of San Isidro in the lexicon,which had only been seen previously as a town of  E1Salvador.complete template structure, deciding whether to default hevalue of template slots not found in the event structure (e.g,using date and location information in the header), andcreating the required template forms.3.
EVALUATIONThe system as a whole was formally evaluated in theGovernment-sponsored Third Message UnderstandingConference (MUC-3), and scored among the top systems inextracting data from text \[Proceedings of MUC-3, 1991\].In this paper we report on two additional experiments runsince then to assess component conlributions to the system.3.1 LexiconIf the grammar rules and semantic rules are bothcompositional nd domain-independent, o ewould expectthe recall of the system (the percent of informationcorrectly found by the system out of all desired informationin the text) to grow linearly at first as the lexicon growsfollowed by tapering off to an asyptote.
4To test this, we ran the system after randomly removinglexical entries (though not removing a word's part ofspeech).
The results with various percentages of thelexicon and with linear curve fitting appear in Figure 4.The template generator has three tasks: finding and/ormerging events hypothesized bydiscourse processing into a3 Apparently none of the fifteen systems entered in the ThirdMessage Understanding Conference (MUC-3) usually foundcomplete syntactic analyses of the long, complex sentences in theMUC-3 corpus.4 Of course, when the lexicon is so small that very littleinformation is found at all, recall might not increase linearly as thelexicon grows.
Presumably, at some point asymptotic growthmust limit as recall approaches 100%.318t_00.6y = - 3.0000e-3 + 0.53833x R'2 = 0 .975  /0.50.40.3'0.2'0.
I0.00.2 0;4 0;6 0;8 I;0 ,2?
Recall OverallPresentFigure 4: Growth in Ability to extract Data from Text as the Lexicon Grows"POLICE HAVE REPORTED THAT TERRORISTSTONIGHT BOMBED THE EMBASSIES OF THE PRCAND THE SOVIET UNION.
THE BOMBS CAUSEDDAMAGE BUT NO INJURIES.
""A CAR-BOMB EXPLODED IN FRONT OF THE PRCEMBASSY, WHICH IS THE LIMA RESIDENTIALDISTRICT OF SAN ISIDRO.
MEANWHILE, TWOBOMBS WERE THROWN AT A USSR EMBASSYVEHICLE THAT WAS PARKED IN FRONT OF THEEMBASSY LOCATED IN ORRANTIA DISTRICT, NEARSAN ISIDRO.
"Event: BOMBINGTrigger: "BOMBED" (?29)Slots:TI-PERP-OF: "TERRORISTS" (?9, score=0)EVENT-TIME-OF:EVENT-LOCATION-OF:"EL SALVADOR" (?100, score=6)" SAN ISIDRO" (?104, score=6)" RESIDENTIAL DISTRICT" (7105, score=6)"ORRANTIA DISTRICT" (7169, score=6)TI-INSTR-OF : "THE BOMBS" (741, score=4)TI-RESULT-OF:"DAMAGE" (?46, score=4)"NO INJURIES" {?54, score=4)OBJECT-OF: "THE EMBASSIES" (?22, score=0)Figure 3: An Example Event ProducedPrecision, the percent of data correctly extracted out of allthe information extracted, should be relatively unaffectedin a compositional, domain-independent system.
That is, ifthe lexicon is declarative rather than itself containing rules,the quality of answers produced should be unaffected.Precision in tests corresponding to the recall data plotted in319Figure 4 varied only 5% throughout the range; thedifference between having only 20% of the lexicon tohaving the full lexicon was only 2% in precision.3.2 Deterministic parser and grammar ofEnglish versus Fragment combining.In the experiment reported here, only a small set offragment combining rules were tested, those deemed to bemost useful in the ability to extract information fro MUC-3; no attempt to provide coverage for the full variety ofEnglish syntax has been made.
The fragment combiningrules were as follows ranked by frequency of occurrence inthe experiment are as follows:- PP attachment to an NP (55%)- PP attachment to a VP (14%)merging of several N's into a single NP(13%)combing appositive NPs (7%)attaching a conjoined NP (6%)PP attachment to an ADJP (3%)attaching time NP to VP (1%)repairing dates (< 1%)To evaluate the relative contribution of the deterministicparser and the fragment combining component, we usedrecently developed grammar evaluation software \[Black, etal., 1991\].
This software uses TREEBANK parse trees asa reference answer.
To factor out most grammaticalidiosyncracies where legitimate theoretical differences mayexist, a TREEBANK tree is reduced by a homomorphismto essential phrase bracketings, uch as that in Figure 5.The user of the evaluation software then writes ahomomorphism component that reduces his/her parser'soutput o a similar bracketed form.
Then a comparator inthe evaluation software counts three things:?
Recall, the number of bracketed phrases inboth answers divided by the number ofbracketed phrases in the reference answer?
Precision, the number of bracketed phrasesin both answers divided by the number ofbracketed phrases in the system's output?
Cross ings,  the number of times a systemphrase crosses a bracketed boundary in thereference answer.TREEBANK Tree (without parts of speech(S (NP the Catholic hurch)(Xhas(VP expressed(NP satisfaction(PP with(NP the investigations(PP in(NP the c a s e(PP of(NP the murdered Jesuits))))))))and(Xis(VP encouraging(NP the government)(s (NP *)to(VP continue(s (NP *)to(VP search(PP for(NP the perpetrators(PP of(NP this crime)))))))))))))Reduced Form Given TREEBANK Parse Tree\[\[THE CATHOLIC CHURCH\]\[HAS\[EXPRESSED\[SATISFACTION\[WITH\[THE INVESTIGATIONStIN\[THE CASE\[OF \[THE MURDERED JESUITS\]\]\]\]\]\]\]\]AND\[IS\[ENCOURAGING\[THE GOVERNMENT\]\[CONTmrtm\[SEARCH\[FOR\[THE PERPETRATORS\[OF \[THIS CRIMEI\]\]\]\]\]\]\]\]\]Reduced Form Given PLUM Parse Tree\[\[\[THE CATHOLIC CHURCH\]\[HAS \[EXPRESSED SATISFACTION\]\]\]\[WITH \[THE INVESTIGATIONS\]\]\[IN\[\[THE CASE\]\[OF \[THE MURDERED JESUITS\]\]\]\]AND\[IS\[ENCOURAGING\[THE GOVERNMENT\]\[CONTINUE SEARCH\]\]\]\[FOR\[\[THE PERPETRATORS\]\[OF \[THIS CRIME\]\]\]\]\]320Figure 5: Parse Trees Reduced to Minimal Bracketing forParser EvaluationIn using the evaluation software, it became readilyapparent hat the absolute numbers output for ourdeterministic parser were not particularly informative,though the relative performance hange from one parserrun to another was instructive.
To see this, consider theexample in Figure 5.
The input sentence contains aprepositional phrase whose attachment is ambiguous.Therefore, the system, by design, closes the constituents upuntil the prepositional phrase; however, the evaluatorcounts this as three crossings (three errors) for the onedesign feature.
Since permanent predictable ambiguityoccurs frequently in the long, textual sentences of theMUC corpus, this multiplicative penalty is applied veryoften.However, relative comparison of one parser to measuresystem improvement (or retrenchment) over time isvaluable.
For instance, on a test set of 900 sentences, ourfragment combining component successfully found 1,000more phrases than running the deterministic parser alone,eliminated 250 incorrect structures, and reduced the totalnumber of crossings by 300.4.
CONCLUSIONSPLUM has the following key features:1.
Deterministic parsing and semanticinterpretation fall fragments produced...Event-based and template-based knowledgeto find relations among entities whensyntax/semantics cannot fmd them.Statistical language models at multiplelevels.These were key to PLUM performing among the topsystems evaluated in MUC-3.
Because of the focus onproducing syntactic and semantic analyses of fragmentswhen no complete analysis was possible, and because ofthe assumption that discourse processing can fit thefragments ogether based on required roles in defined eventstructures, the system can produce answers end-to-end veryearly on when porting to a new domain, long beforedomain-specific lexical items and any domain-specificsemantic rules are complete.That conclusion is supported quantitatively by ourexperiments.
Recall, the percent of information i the textcorrectly extracted, grew nearly linearly as the lexicongrew in the experiment.
Precision, the percent correctinformation extracted of information output by PLUM,remained flat as a function of lexicon size, also supportingthe intuition that the lexicon was declarative and separatefrom rule-based processing.A second conclusion is that deterministic parsing can besupplemented by locally applied, fragment combining rulesthat use both the syntactic and semantic properties offragments produced to resolve ambiguity that syntax alonecan not resolve in a deterministic parser.
The experimentreported here demonstrates that.The degree of success obtained by marrying domain-independent, deterministic parsing with partialunderstanding and statistical techniques has been quitegratifying.
The techniques which seemed so incompatibleand antithetical inthe seventies have proven synergistic.ACKNOWLEDGMENTSThe work reported here was supported in part by theDefense Advanced Research Projects Agency and wasmonitored by the Rome Air Development Center underContract No.
F30602-91-C-0051.
The views andconclusions contained in this document are those of theauthors and should not be interpreted as necessarilyrepresenting the official policies, either expressed orimplied, of the Defense Advanced Research ProjectsAgency or the United States Government.REFERENCESi.2.3.4..6.7.Black, E., et al, A Procedure for QuantitativelyComparing the Syntactic Coverage ofEnglish Grammars,Proceedings ofthe Fourth DARPA Workshop on Speechand Natural Language, 1991.de Marcken, C.G.
Parsing the LOB Corpus.
Proceedingsof the 28th Annual Meeting of the Association forComputational Linguistics 1990, pp.
243-251.Hobbs, J.R.
Coherence and Coreference, CognitiveScience.
Vol.
3, No.
1, 1979, pp.
67-90.Hobbs, J.R. et.
al., Interpretation as Abduction,Proceedings of the 26th Annual Meeting of theAssociation for Computational Linguistics, 1988, pp.
95-103.Jacobs, P., Krupka, G.P., and Ran, L.F. Lexicon-Semantic Pattern Matching as a Companion toParsing inText Understanding, Proceedings of the Fourth DARPAWorkshop on Speech and Natural Language, MorganKaufmann Publishers, San Marco, CA, February 1991,pp.
337-341.Meteer, M., Schwartz, R., and Weischedel, R. EmpiricalStudies in Part of Speech Labelling., Proceedings of theFourth DARPA Workshop on Speech and NaturalLanguage, Morgan Kaufinann Publishers, San Mateo,CA.
February 1991, pp.
331-336.Proceedings of the Third Message UnderstandingConference.
Morgan Kauffmann Publishers, San Mateo,CA, 1991.3218.9.10.Schank, R.C.
and Riesbeck C.K., Inside ComputerUnderstanding, Lawrence Erlbaum Associates, Inc.,1981.Weischedel, R., Ayuso, D.M., Bobrow, R., Boisen, S.,Ingria, R., and Palmucci, J., Partial Parsing, A Report onWork in Progress, Proceedings of the Fourth DARPAWorkshop on Speech and Natural Language, MorganKaufmann Mateo, CA, 1991a, pp.
204-210.WeischedeL R., Meteer, M., and Schwartz, Applicationsof Statistical Language Modelling to Natural LanguageProcessing, unpubfished manuscript, 1991b.322
