Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 122?130,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsCrowdsourcing and language studies: the new generation of linguistic dataRobert Munroa Steven Bethardb Victor Kupermana Vicky Tzuyin LaicRobin Melnicka Christopher Pottsa Tyler Schnoebelena Harry TilyaaDepartment of Linguistics, Stanford UniversitybDepartment of Computer Science, Stanford UniversitycDepartment of Linguistics, University of Colorado{rmunro,bethard,vickup,rmelnick,cgpotts,tylers,hjt}@stanford.eduvicky.lai@colorado.eduAbstractWe present a compendium of recent and cur-rent projects that utilize crowdsourcing tech-nologies for language studies, finding that thequality is comparable to controlled labora-tory experiments, and in some cases superior.While crowdsourcing has primarily been usedfor annotation in recent language studies, theresults here demonstrate that far richer datamay be generated in a range of linguistic dis-ciplines from semantics to psycholinguistics.For these, we report a number of successfulmethods for evaluating data quality in the ab-sence of a ?correct?
response for any givendata point.1 IntroductionCrowdsourcing?s greatest contribution to languagestudies might be the ability to generate new kindsof data, especially within experimental paradigms.The speed and cost benefits for annotation are cer-tainly impressive (Snow et al, 2008; Callison-Burch, 2009; Hsueh et al, 2009) but we hope toshow that some of the greatest gains are in the verynature of the phenomena that we can now study.For psycholinguistic experiments in particular, weare not so much utilizing ?artificial artificial?
intelli-gence as the plain intelligence and linguistic intu-itions of each crowdsourced worker ?
the ?voicesin the crowd?, so to speak.
In many experimentswe are studying gradient phenomena where thereare no right answers.
Even when there is binaryresponse we are often interested in the distributionof responses over many speakers rather than spe-cific data points.
This differentiates experimentationfrom more common means of determining the qual-ity of crowdsourced results as there is no gold stan-dard against which to evaluate the quality or ?cor-rectness?
of each individual response.The purpose of this paper is therefore two-fold.We summarize seven current projects that are utiliz-ing crowdsourcing technologies, all of them some-what novel to the NLP community but with potentialfor future research in computational linguistics.
Foreach, we also discuss methods for evaluating quality,finding the crowdsourced results to often be indistin-guishable from controlled laboratory experiments.In Section 2 we present the results from seman-tic transparency experiments showing near-perfectinterworker reliability and a strong correlation be-tween crowdsourced data and lab results.
Ex-tending to audio data, we show in Section 3that crowdsourced subjects were statistically in-distinguishable from a lab control group in seg-mentation tasks.
Section 4 shows that labora-tory results from simple Cloze tasks can be repro-duced with crowdsourcing.
In Section 5 we offerstrong evidence that crowdsourcing can also repli-cate limited-population, controlled-condition lab re-sults for grammaticality judgments.
In Section 6 weuse crowdsourcing to support corpus studies with aprecision not possible with even very large corpora.Moving to the brain itself, Section 7 demonstratesthat ERP brainwave analysis can be enhanced bycrowdsourced analysis of experimental stimuli.
Fi-nally, in Section 8 we outline simple heuristics forensuring that microtasking workers are applying thelinguistic attentiveness required to undertake morecomplex tasks.1222 Transparency of phrasal verbsPhrasal verbs are those verbs that spread their mean-ing out across both a verb and a particle, as in ?liftup?.
Semantic transparency is a measure of howstrongly the phrasal verb entails the component verb.For example, to what extent does ?lifting up?
entail?lifting??
We can see the variation between phrasalverbs when we compare the transparency of ?lift up?to the opacity of ?give up?.We conducted five experiments around seman-tic transparency, with results showing that crowd-sourced results correlate well with each other andagainst lab data (?
up to 0.9).
Interrater reliability isalso very high: ?
= 0.823, which Landis and Koch(1977) would call ?almost perfect agreement.
?The crowdsourced results reported here representjudgments by 215 people.
Two experiments wereperformed using Stanford University undergradu-ates.
The first involved a questionnaire asking par-ticipants to rate the semantic transparency of 96phrasal verbs.
The second experiment consisted ofa paper questionnaire with the phrasal verbs in con-text.
That is, the first group of ?StudentLong?
par-ticipants rated the similarity of ?cool?
to ?cool down?on a scale 1-7:cool cool downThe ?StudentContext?
participants performed thesame basic task but saw each verb/phrasal verb pairwith an example of the phrasal verb in context.With Mechanical Turk, we had three conditions:TurkLong: A replication of the first questionnaireand its 96 questions.TurkShort: The 96-questions were randomized intobatches of 6.
Thus, some participants ended up giv-ing responses to all phrasal verbs, while others onlygave 6, 12, 18, etc responses.TurkContext: A variation of the ?StudentContext?task ?
participants were given examples of thephrasal verbs, though as with ?TurkShort?, they wereonly asked to rate 6 phrasal verbs at a time.What we find is a split into relatively high and lowcorrelations, as Figure 1 shows.
All MechanicalTurk tests correlate very well with one another (all?
> 0.7), although the tasks and raters are differ-ent.
The correlation between the student participantswho were given sentence contexts and the workersTurkLong2 3 4 5 6 2 3 4 5 62.03.55.023456r = 0.92p = 0rs = 0.92p = 0TurkShortr = 0.74p = 0rs = 0.73p = 0r = 0.77p = 0rs = 0.75p = 0TurkContext345623456 r = 0.68p = 0rs = 0.67p = 0r = 0.7p = 0rs = 0.67p = 0r = 0.9p = 0rs = 0.9p = 0StudentContext2.0 3.5 5.0r = 0.46p = 0rs = 0.46p = 0r = 0.48p = 0rs = 0.48p = 03 4 5 6r = 0.46p = 0rs = 0.45p = 0r = 0.41p = 0rs = 0.44p = 02.5 3.5 4.5 5.52.53.54.55.5StudentLongFigure 1: Panels at the diagonal report histograms of dis-tributions of ratings across populations of participants;panels above the diagonal plot the locally weighted scat-terplot smoothing Lowess functions for a pair of corre-lated variables; panels below the diagonal report correla-tion coefficients (the r value is Pearson?s r, the rs valueis Spearman?s ?)
and respective ?
values.who saw context is especially high (0.9).
All corre-lations with StudentLong are relatively low, but thisis actually true for StudentLong vs. StudentContext,too (?
= 0.44), even though both groups are Stan-ford undergraduates.Intra-class correlation coefficients (ICC) measurethe agreement among participants, and these arehigh for all groups except StudentLong.
Just amongStudentLong participants, the ICC consistency isonly 0.0934 and their ICC agreement is 0.0854.Once we drop StudentLong, we see that all of theremaining tests have high consistency (average of0.78 for ICC consistency, 0.74 for ICC agreement).For example, if we combine TurkContext and Stu-dentContext, ICC consistency is 0.899 and ICCagreement of 0.900.
Cohen?s kappa measurementalso measures how well raters agree, weeding outchance agreements.
Again, StudentLong is an out-lier.
Together, TurkContext / StudentContext gets aweighted kappa score of 0.823 ?
the overall average(excepting StudentLong) is ?
= 0.700.More details about the results in this section canbe found in Schnoebelen and Kuperman (submit-ted).1233 Segmentation of an audio speech streamThe ability of browsers to present multimedia re-sources makes it feasible to use crowdsourcing tech-niques to generate data using spoken as well as writ-ten stimuli.
In this section we report an MTurk repli-cation of a classic psycholinguistic result that relieson audio presentation of speech.
We developed aweb-based interface that allows us to collect datain a statistical word segmentation paradigm.
Thecore is a Flash applet developed using Adobe Flexwhich presents audio stimuli and collects participantresponses (Frank et al, submitted).Human children possess a remarkable ability tolearn the words and structures of languages they areexposed to without explicit instruction.
One partic-ularly remarkable aspect is that unlike many writtenlanguages, spoken language lacks spaces betweenwords: from spoken input, children learn not onlythe mapping between meanings and words but alsowhat the words themselves are, with no direct infor-mation about where one ends and the next begins.Research in statistical word segmentation has shownthat both infants and adults use statistical propertiesof speech in an unknown language to infer a proba-ble vocabulary.
In one classic study, Saffran, New-port & Aslin (1996) showed that after a few minutesof exposure to a language made by randomly con-catenating copies of invented words, adult partici-pants could discriminate those words from syllablesequences that also occurred in the input but crosseda word boundary.
We replicated this study showingthat cheap and readily accessible data from crowd-sourced workers compares well to data from partic-ipants recorded in person in the lab.Participants heard 75 sentences from one of 16 ar-tificially constructed languages.
Each language con-tained 2 two-syllable, 2 three-syllable, and 2 foursyllable words, with syllables drawn from a possi-ble set of 18.
Each sentence consisted of four wordssampled without replacement from this set and con-catenated.
Sentences were rendered as audio bythe MBROLA synthesizer (Dutoit et al, 1996) at aconstant pitch of 100Hz with 25ms consonants and225ms vowels.
Between each sentence, participantswere required to click a ?next?
button to continue,preventing workers from leaving their computer dur-ing this training phase.
To ensure workers could ac-Figure 2: Per-subject correct responses for lab and MTurkparticipants.
Bars show group means, and the dashed lineindicates the chance baseline.tually hear the stimuli, they were first asked to enteran English word presented auditorily.Workers then completed ten test trials in whichthey heard one word from the language and one non-word made by concatenating all but the first syllableof one word with the first syllable of another.
If thewords ?bapu?
and ?gudi?
had been presented adja-cently, the string ?pugu?
would have been heard, de-spite not being a word of the language.
Both werealso displayed orthographically, and the worker wasinstructed to click on the one which had appeared inthe previously heard language.The language materials described above weretaken from a Saffran et al (1996) replication re-ported as Experiment 2 in Frank, Goldwater, Grif-fiths & Tenenbaum (under review).
We comparedthe results from lab participants reported in that ar-ticle to data from MTurk workers using the appletdescribed above.
Each response was marked ?cor-rect?
if the participant chose the word rather than thenonword.
12 lab subjects achieved 71% correct re-sponses, while 24 MTurk workers were only slightlylower at 66%.
The MTurk results proved signif-icantly different from a ?random clicking?
base-line of 50% (t(23) = 5.92, p = 4.95 ?
10?06)but not significantly different from the lab subjects(Welch two-sample t-test for unequal sample sizes,t(21.21) = ?.92, p = .37).
Per-subject means forthe lab and MTurk data are plotted in Figure 2.1244 Contextual predictabilityAs psycholinguists build models of sentence pro-cessing (e.g., from eye tracking studies), they needto understand the effect of the available sentencecontext.
One way to gauge this is the Cloze task pro-posed in Taylor (1953): participants are presentedwith a sentence fragment and asked to provide theupcoming word.
Researchers do this for every wordin every stimulus and use the percentage of ?correct?guesses as input into their statistical and computa-tional models.Rather than running such norming studies on un-dergraduates in lab settings (as is typical), our resultssuggest that psycholinguists will be able to crowd-source these tasks, saving time and money withoutsacrificing reliability (Schnoebelen and Kuperman,submitted).Our results are taken from 488 Americans, rang-ing from age 16-80 (mean: 34.49, median: 32,mode: 27) with about 25% each from the East andMidwest, 31% from the South, the rest from theWest and Alaska.
They represent a range of educa-tion levels, though the majority had been to college:about 33.8% had bachelor?s degrees, another 28.1%had some college but without a degree.By contrast, the lab data was gathered from 20participants, all undergraduates at the University ofMassachusetts at Amherst in the mid-1990?s (Re-ichle et al, 1998).
Both populations provided judg-ments on 488 words in 48 sentences.
In general,crowdsourcing gave more diverse responses, as wewould expect from a more diverse population.The correlation between lab and crowdsourceddata by Spearman?s rank correlation is 0.823 (?
<0.0001), but we can be even more conservative byeliminating the 124 words that had predictabilityscores of 0 across both groups.
By and large, thelab participants and the workers are consistent inwhich words they fail to predict.
Even when weeliminate these shared zeros, the correlation is stillhigh between the two data sets: weighted ?
= 0.759(?
< 0.0001).5 Judgment studies of fine-grainedprobabilistic grammatical knowledgeMoving to syntax, we demonstrate here that gram-maticality judgments from lab studies can also beFigure 3: Mean ?that?-inclusion ratings plotted againstcorresponding corpus-model predictions.
The solid linewould represent perfect alignment between judgmentsand corpus model.
Non-parametric Lowess smoothers il-lustrate the significant correlation between lab and crowdpopulation results.reproduced through crowdsourcing.Corpus studies of spontaneous speech suggestthat grammaticality is gradient (Wasow, 2008), andmodels of English complement clause (CC) and rel-ative clause (RC) ?that?-optionality have as theirmost significant factor the predictability of embed-ding, given verb (CC) and head noun (RC) lemma(Jaeger, 2006; Jaeger, in press).
Establishing thatthese highly gradient factors are similarly involvedin judgments could provide evidence that such fine-grained probabilistic knowledge is part of linguisticcompetence.We undertook six such judgment experiments:two baseline studies with lab populations then fouradditional crowdsourced trials via MTurk.Experiment 1, a lab trial (26 participants, 30items), began with the models of RC-reduction de-veloped in Jaeger (2006).
Corpus tokens werebinned by relative model-predicted probability of?that?-omission.
Six tokens were extracted at ran-dom from each of five bins (0?
?<20% likelihood of?that?-inclusion; 20?
?<40%; and so on).
In a gra-dient scoring paradigm with 100 points distributedbetween available options (Bresnan, 2007) partici-125pants rated how likely each choice ?
with or without?that?
?
was as the continuation of a segment of dis-course.
As hypothesized, mean participant ratingssignificantly correlate with corpus model predictions(r = 0.614, ?
= 0.0003).Experiment 2 (29 participants) replicated Exper-iment 1 to address concerns that subjects might be?over-thinking?
the process.
We used a timed forced-choice paradigm where participants had from 5 to 24seconds (varied as a linear function of token length)to choose between the reduced/unreduced RC stim-uli.
These results correlate even more closely withpredictions (r = 0.838, ?
< 0.0001).Experiments 3 and 4 replicated 1 and 2 on MTurk(1200 tasks each).
Results were filtered by volun-teered demographics to select the same subject pro-file as the lab experiments.
Response-time outlierswere also excluded to avoid fast-click-through anddistracted-worker data.
Combined, these steps elim-inated 384 (32.0%) and 378 (31.5%) tasks, respec-tively, with 89 and 66 unique participants remaining.While crowdsourced measures might be expected toyield lower correlations due to such unbalanced datasets, the results remain significant in both trials (r =0.562, ?
= 0.0009; r = 0.364, ?
= 0.0285), offer-ing strong evidence that crowdsourcing can replicatelimited-population, controlled-condition lab results,and of the robustness of the alignment between pro-duction and judgment models.
Figure 3 compareslab and crowd population results in the 100-pointtask (Experiments 1 and 3).Experiments 5 and 6 (1600 hits each) employedthe same paradigms via MTurk to investigate ?that?-mentioning in CCs, where predictability of embed-ding is an even stronger factor in the corpus model.Filtering reduced the data by 590 (36.9%) and 863(53.9%) hits.
As with the first four experiments,each of these trials produced significant correlations(r = 0.433, ?
= 0.0107; r = 0.500, ?
= 0.0034; re-spectively).
Finally, mixed-effect binary logistic re-gression models ?
with verb lemma and test subjectID as random effects ?
were fitted to these judgmentdata.
As in the corpus-derived models, predictabilityof embedding remains the most significant factor inall experimental models.The results across both lab and crowdsourcedstudies suggest that speakers consider the same fac-tors in judgment as in production, offering evidenceFigure 4: Odds ratio of a Nominal Agent being embed-ded within a Sentential Agent or non-Agent, relative torandom chance.
(?
< 0.001 for all)that competence grammar includes access to prob-ability distributions.
Meanwhile, the strong cor-relations across populations offer encouraging evi-dence in support of using the latter in psycholinguis-tic judgment research.6 Confirming corpus trendsCrowdsourcing can also be used to establish the va-lidity of corpus trends found in otherwise skeweddata.
The experiments in this section were mo-tivated by the NomBank corpus of nominal pred-icate/arguments (Meyers et al, 2004) where wefound that an Agent semantic role was much morelikely to be embedded within a sentential Agent.
Forexample, (1) is more likely than (2) to receive theAgent interpretation for the ?the police?, but bothhave same potential range of meanings:(1) ?The investigation of the police took 3 weeks tocomplete?
(2) ?It took 3 weeks to complete the investigation ofthe police?While the trend is significant (?
< 0.001), thecorpus is not representative speech.First, there are no minimal pairs of sentences inNomBank like (1) and (2) that have the same poten-tial range of meanings.
Second, the s-genitive (?thepolice?s investigation?)
is inherently more Agen-tive than the of-genitive (?the investigation of thepolice?)
and it is also more compact.
Sententialsubjects tend to be lighter than objects, and morelikely to realize Agents, so the resulting correlationcould be indirect.
Finally, if we sampled only thepredicates/arguments in NomBank that are frequentin different sentential positions, we are limited to:126?earning, product, profit, trading, loss, share, rate,sale, price?.
This purely financial terminology is notrepresentative of a typical acquisition environment ?no child should be exposed to only such language ?so it is difficult to draw broad conclusions about thecognitive viability of this correlation, even withinEnglish.
It is because of factors like these that cor-pus linguistics has been somewhat of a ?poor cousin?to theoretical linguistics.Therefore, two sets of experiments were under-taken to confirm that the trend is not epiphenomenal,one testing comprehension and one testing produc-tion.The first tested thousands of workers?
interpre-tations of sentences like those in (1) and (2), overa number of predicate/argument pairs (?shooting ofthe hunters?, ?destruction of the army?
etc).
Work-ers were asked their interpretation of the most likelymeaning.
For example, does (1) mean: ?a: the po-lice were doing the investigation?
or ?b: the po-lice are being investigated?.
To control for errorsor click-throughs, two plainly incorrect options wereincluded.
We estimate the erroneous response rate atabout 0.4% ?
less than many lab studies.For the second set of experiments, workers wereasked to reword an unambiguous sentence using agiven phrase.
For example, rewording the followingusing ?the investigation of the police?
:(3) ?Following the shooting of a commuter in Oak-land last week, a reporter has uncovered new evi-dence while investigating the police involved.
?We then (manually) recorded whether the requiredphrase was in a sentential Agent or non-Agent posi-tion.Figure 4 gives the results from the corpus analy-sis and both experiments.
The results clearly showa significant trend for all, and that the NomBanktrend falls between the comprehension and produc-tion tasks, which would be expected for this highlyedited register.
It therefore supports the validity ofthe corpus results.The phenomena likely exists to aid comprehen-sion, as the cognitive realization of just one roleneeds to be activated at a given moment.
Despitethe near-ubiquity of ?Agent?
in studies of semanticroles, we do not yet have a clear theory of this lin-guistic entity, or even firm evidence of its existenceFigure 5: Distribution of metaphorical frequencies.
(Parikh, 2010).
This study therefore goes some waytowards illuminating this.
More broadly, the experi-ments in this section support the wider use of crowd-sourcing as a tool for language cognition research inconjunction with more traditional corpus studies.7 Post-hoc metaphorical frequencyanalysis of electrophysiological responsesBeyond reproducing laboratory and corpus studies,crowdsourcing also offers the opportunity to newlyanalyze data drawn from many other experimentalstimuli.
In this section, we demonstrate that crowd-sourced workers can help us better understand ERPbrainwave data by looking at how frequently wordsare used metaphorically.Recent work in event related potentials (ERP) hassuggested that even conventional metaphors, such as?All my ideas were attacked?
require additional pro-cessing effort in the brain as compared to literal sen-tences like ?All the soldiers were attacked?
(Lai etal., 2009).
This study in particular observed an N400effect where negative waves 400 milliseconds afterthe presentation of the target words (e.g.
attacked)were larger when the word was used metaphoricallythan when used literally.The proposed explanation for this effect is thatmetaphors really do demand more from the brainthan literal sentences.
However, N400 effects arealso observed when subjects encounter somethingthat is semantically inappropriate or unexpected.While the Lai experiment controlled for overallword frequency, it might be possible to explain awaythese N400 effects if it turned out that in the real127world the target words were almost always usedliterally, so that seeing them used metaphoricallywould be semantically incongruous.To test this alternative hypothesis, we gatheredsense frequency distributions for each of the targetwords ?
the hypothesis predicts that these shouldbe skewed towards literal senses.
For each of the104 target words, we selected 50 random sentencesfrom the American National Corpus (ANC), fill-ing in with British National Corpus sentences whenthere were too few in the ANC.
We gave the sen-tences to crowdsourced workers and asked them tolabel each target word as being used literally ormetaphorically.
Each task contained one sentencefor each of the 104 target words, with the order ofwords and the literal/metaphorical buttons random-ized.
Each sentence was annotated 5 times.To encourage native speakers of English, we hadthe MTurk service require that our workers be withinthe United States, and posted the text ?Please ac-cept this HIT only if you are a native speaker of En-glish?
in bold at the top of each HIT.
We also usedJavascript to force workers to spend at least 2 sec-onds on each sentence and we rejected results fromworkers that had chance level (50%) agreement withthe other workers.Though our tasks produced words annotated withliteral and metaphorical tags, we were less inter-ested in the individual annotations (though agree-ment was decent at 73%) and more interested in theoverall pattern for each target word.
Some words,like fruit, were almost always used literally (92%),while other words, like hurdle were almost alwaysused metaphorically (91%) .Overall, the target words had a mean metaphor-ical frequency of 53%, indicating that their literaland metaphorical senses were used in nearly equalproportions.
Figure 5 shows that the metaphoricalfrequencies follow roughly a bell-curved distribu-tion1, which is especially interesting given that thetarget words were hand-selected for the Lai experi-ment and not drawn randomly from a corpus.
We didnot observe any skew towards literal senses as thealternative hypothesis would have predicted.
Thissuggests that the findings of Lai, Curran, and Menn1A Shapiro-Wilk test fails to reject the null hypothesis of anormal distribution (p=0.09).Item type correct incorrect?easy?
60 2?promise?
59 3stacked genitive 55 7Table 1: Response data for three control items, with thegoal of identifying workers who lack the requisite atten-tiveness.
All show high attentiveness.
The difference be-tween the ?easy?
and ?stacked genitive?
is trending but notsignificant (?
= 0.0835), indicating that any of these maybe used.
(2009) cannot be dismissed based on a sense fre-quency argument.We also took advantage of the collected sense fre-quency distributions to re-analyze data from the Laiexperiment.
We split the target words into a high bin(average 72% metaphorical) and a low bin (average33% metaphorical), matching the number of itemsand average word log-frequency per bin.
Looking atthe average ERPs (brain waves) over time for eachbin revealed that when subjects were reading novelmetaphors, there was a significant difference (p =.01) at about 200ms (P200) between the ERPs forthe highly literal words and the ERPs for the highlymetaphorical words.
Thus, not only does metaphori-cal frequency influence figurative language process-ing, but it does so much earlier than semantic effectsare usually observed (e.g.
N400 effects at 400ms)2.8 Screening for linguistic attentivenessFor annotation tasks, crowdsourcing is most suc-cessful when the tasks are designed to be as simpleas possible, but in experimental work we don?t al-ways want to target the shallowest knowledge of theworkers, so here we seek to discover just how atten-tive the workers really are.When running psycholinguistics experiments inthe lab, the experimenters generally have the chanceto interact with participants.
It is not uncommonfor prospective subjects to be visibly exhausted, dis-tracted, or inebriated, or not fluent in the given lan-guage to a requisite level of competence.
Whenthese participants turn up as outliers in the experi-mental data, it is easy enough to see why ?
theyfell asleep, couldn?t understand the instructions, etc.2These results are consistent with recent findings that ironyfrequency may also produce P200 effects (Regel et al, 2010).128With crowdsourcing we lose the chance to havethese brief but valuable encounters, and so anoma-lous response data are harder to interpret.We present two simple experiments for measuringlinguistic attentiveness, which can be used as onecomponent of a language study or to broadly evalu-ate the linguistic competency of the workers.
Takingwell-known constructions from the literature, we se-lected constructions that: (a) exist in most (perhapsall) dialects of English; (b) involve high frequencylexical items; and (c) tend to be acquired relativelylate by first-language learners.We have found two constructions from CarolChomsky?s (1969) work on first-language acquisi-tion to be particularly useful:(4) John is easy to see.
(5) John is eager to see.Example (4) is accurately paraphrased as ?It is easyto see John?, where John is the object of ?see?,whereas (5) is accurately paraphrased as ?John is ea-ger for John to see?, where John is the subject of?see?.
A similar shift happens with ?promise?
:(6) Bozo told Donald to sing.
(7) Bozo promised Donald to sing.We presented workers with a multiple-choice ques-tion that contained both subject and object para-phrases as options.In similar experiments, we adapted examplesfrom Roeper (2007), who looked at stacked prenom-inal possessive constructions:(8) John?s sister?s friend?s car.These are cross-linguistically rare and challengingeven for native speakers.
As above, the workerswere asked to choose between paraphrases.Workers who provide accurate judgments arelikely to have a level of English competence and de-votion to the task that suffices for many languageexperiments.
The results from one short audio studyare given in Table 1.
They indicate a high degree ofattentiveness; as a group, our subjects performed atthe near-perfect levels we expect for fluent adults.We predict that adding tasks like these to experi-ments will not only screen for attentiveness, but alsoprompt for greater attention from an otherwise dis-tracted worker, improving results at both ends.9 ConclusionsWhile crowdsourcing was first used by linguists forannotation, we hope that the results here demon-strate the potential for far richer studies.
In arange of linguistic disciplines from semantics topsycholinguistics it enables systematic, large-scalejudgment studies that are more affordable and con-venient than expensive, time-consuming lab-basedstudies.
With crowdsourcing technologies, linguistshave a reliable new tool for experimentally investi-gating language processing and linguistic theory.Here, we have reproduced many ?classic?
large-scale lab studies with a relative ease.
We can en-vision many more ways that crowdsourcing mightcome to shape new methodologies for languagestudies.
The affordability and agility brings experi-mental linguistics closer to corpus linguistics, allow-ing the quick generation of targeted corpora.
Multi-ple iterations that were previously possible only overmany years and several grants (and therefore neverattempted) are now possible in a matter of days.
Thiscould launch whole new multi-tiered experimentaldesigns, or at the very least allow ?rapid prototyp-ing?
of experiments for later lab-based verification.Crowdsourcing also brings psycholinguisticsmuch closer to computational linguistics.
Thetwo fields have always shared empirical data-drivenmethodologies and computer-aided methods.
Wenow share a work-space too.
Historically, NLP hasnecessarily drawn corpora from the parts of linguis-tic theory that have stayed still long enough to sup-port time-consuming annotation projects.
The re-sults here have implications for such tasks, includ-ing parsing, word-sense disambiguation and seman-tic role labeling, but the most static parts of a fieldare rarely the most exciting.
We therefore predictthat crowdsourcing will also lead to an expanded,more dynamic NLP repertoire.Finally, for the past half-century theoretical lin-guistics has relied heavily on ?introspective?
corpusgeneration, as the rare edge cases often tell us themost about the boundaries of a given language.
Nowthat we can quickly and confidently generate empir-ical results to evaluate hypotheses drawn from intu-itions about the most infrequent linguistic phenom-ena, the need for this particular fallback has dimin-ished ?
the stimuli are abundant.129AcknowledgementsWe owe thanks to many people, especially withinthe Department of Linguistics at Stanford, which hasquickly become a hive of activitiy for crowdsourcedlinguistic research.
In particular, we thank Tom Wa-sow for his guidance in Section 5, Chris Manning forhis guidance in Section 6, and Florian T. Jaeger forproviding the corpus-derived base models in Section5 (Jaeger, 2006).
We also thank Michael C. Frankfor providing the design, materials, and lab data usedto evaluate the methods in Section 3.
Several of theprojects reported here were supported by StanfordGraduate Fellowships.ReferencesJoan Bresnan.
2007.
Is syntactic knowledge probabilis-tic?
Experiments with the English dative alternation.In Sam Featherston and Wolfgang Sternefeld, editors,Roots: Linguistics in search of its evidential base,pages 75?96.
Mouton de Gruyter, Berlin.Chris Callison-Burch.
2009.
Fast, cheap, and cre-ative: evaluating translation quality using Amazon?sMechanical Turk.
In EMNLP ?09: Proceedings ofthe 2009 Conference on Empirical Methods in Natu-ral Language Processing, pages 286?295.Carol Chomsky.
1969.
The Acquisition of Syntax in Chil-dren from 5 to 10.
MIT Press, Cambridge, MA.Thierry Dutoit, Vincent Pagel, Nicolas Pierret, FranoisBataille, and Olivier van der Vrecken.
1996.
TheMBROLA project: Towards a set of high qualityspeech synthesizers free of use for non commercialpurposes.
In Fourth International Conference on Spo-ken Language Processing, pages 75?96.Michael Frank, Harry Tily, Inbal Arnon, and SharonGoldwater.
submitted.
Beyond transitional probabili-ties: Human learners impose a parsimony bias in sta-tistical word segmentation.Michael Frank, Sharon Goldwater, Thomas Griffiths, andJoshua Tenenbaum.
under review.
Modeling humanperformance in statistical word segmentation.Pei-Yun Hsueh, Prem Melville, and Vikas Sindhwani.2009.
Data quality from crowdsourcing: a study ofannotation selection criteria.
In Proceedings of theNAACL HLT 2009 Workshop on Active Learning forNatural Language Processing, pages 27?35.Florian Jaeger.
2006.
Redundancy and syntactic reduc-tion in spontaneous speech.
Ph.D. thesis, StanfordUniversity, Stanford, CA.Florian Jaeger.
in press.
Redundancy and reduction:Speakers manage syntactic information density.
Cog-nitive Psychology.Vicky Tzuyin Lai, Tim Curran, and Lise Menn.
2009.Comprehending conventional and novel metaphors:An ERP study.
Brain Research, 1284:145?155, Au-gust.Richard Landis and Gary Koch.
1977.
The measurementof observer agreement for categorical data.
Biomet-rics, 33(1).Adam Meyers, Ruth Reeves, Catherine Macleod, RachelSzekely, Veronika Zielinska, Brian Young, , and RalphGrishman.
2004.
Annotating noun argument structurefor NomBank.
In Proceedings of LREC-2004.Prashant Parikh.
2010.
Language and Equilibrium.
MITPress, Cambridge, MA.Stefanie Regel, Seana Coulson, and Thomas C. Gunter.2010.
The communicative style of a speaker can af-fect language comprehension?
ERP evidence from thecomprehension of irony.
Brain Research, 1311:121?135.Erik D. Reichle, Alexander Pollatsek, Donald L. Fisher,and Keith Rayner.
1998.
Toward a model of eyemovement control in reading.
Psychological Review,105:125?157.Tom Roeper.
2007.
The Prism of Grammar: How ChildLanguage Illuminates Humanism.
MIT Press, Cam-bridge, MA.Jenny R. Saffran, Richard N. Aslin, and Elissa L. New-port.
1996.
Word segmentation: The role of distribu-tional cues.
Journal of memory and language, 35:606?621.Tyler Schnoebelen and Victor Kuperman.
submitted.
Us-ing Amazon Mechanical Turk for linguistic research:Fast, cheap, easy, and reliable.Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-drew T. Ng.
2008.
Cheap and fast?but is it good?
:evaluating non-expert annotations for natural languagetasks.
In EMNLP ?08: Proceedings of the Conferenceon Empirical Methods in Natural Language Process-ing, pages 254?263.Wilson Taylor.
1953.
Cloze procedure: A new tool formeasuring readability.
Journalism Quarterly, 30:415?433.Tom Wasow.
2008.
Gradient data and gradient gram-mars.
In Proceedings of the 43rd Annual Meeting ofthe Chicago Linguistics Society, pages 255?271.130
