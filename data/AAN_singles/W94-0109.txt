Integrating Symbolic and Statistical Approachesin Speech and Natural Language ApplicationsMarie Meteer & Herbert GishBBN Systems & TechnologiesCambridge, Massachusettsmmeteer@bbn.comhgish@bbn.comABSTRACTSymbolic and statistical pproaches have traditionally beenkept separate and applied to very different problems.Symbolic techniques apply best where we have a prioriknowledge of the language or the domain and where theapplication of a theory or study of selected examples canhelp leverage and extend our knowledge.
Statisticalapproaches apply best where the results of decisions can berepresented in a model and where we have sufficienttraining data to accurately estimate the parameters of themodel.
Another factor in selecting which approach to usein a particular situation is whether there is sufficientuncertainty o warrant he need to make educated guesses(statistical approach) rather than assertions (symbolicapproach).In our work in gisting, word spotting, and topicclassification, we have successfully integrated symbolicand statistical approaches in a range of tasks, includinglanguage modeling for speech recognition, informationextraction from speech, and topic and event spotting.
Inthis paper we outline the contributions and drawbacks ofeach approach and illustrate our points with the variouscomponents of our systems.1.
INTRODUCTIONSymbolic and statistical approaches have both madesignificant contributions in speech and natural anguageprocessing.
However, they have traditionally been keptseparate and applied to very different kinds of problems.Most speech recognition systems use statistical techniquesexclusively, whereas natural anguage (NL) systems aremostly symbolic.
We are seeing more integration ofstatistical methods in NL, but usually in some well definedcomponent, such as a statistically based part of speechtagger as a preprocessor toparsing.In this paper, we have two goals: first, to characterize thekinds of problems that are most amenable to each of theseapproaches and, second, to show how we have integratedthe approaches in our work in information extraction fromspeech, topic classification, and word and phrase spotting.We begin with a brief overview characterizing the twoapproaches, then discuss in more detail how we haveintegrated these two approaches in our work.1.1 Characterizing symbolic andstatistical approachesSymbolic approaches have dominated work in NL.
Bywriting rules, we can take advantage of what we alreadyknow about a language or domain and we can apply atheoretical framework or study of selected examples toleverage and extend our knowledge.
Most symbolicapproaches also have meaningful intermediate structuresthat indicate what steps a system goes through inprocessing.
Furthermore, since in a rule based approachthe system either works or fails (as opposed to being moreor less likely as is the case in a statistical approach), wegenerally have a clearer understanding of what a system iscapable of and where its weaknesses lie.
However, thisfeature is also the greatest flaw of this kind of approach, asit makes a system brittle.Statistical approaches begin with a model and estimate theparameters of the model based on data.
Since decisions aremore or less likely (rather than right or wrong), systemsusing these approaches are more robust in the face ofunseen data.
In particular, statistical modeling approachesprovide the conditional probability of an event, whichcombines both prior knowledge of the distribution ofevents and the distribution learned from a training set,which can take into account both how often an event isseen and the context in which it occurs.
There are twoimportant considerations in choosing to use a statisticalapproach: (1) the output must be representable in amodel--that is, we need to understand the problem wellenough to represent output and specify its relationship tothe input.
This can presently be done for part of speechtags, for example, but not for discourse; (2) there must besufficient data (paired I/O) and/or prior statisticalknowledge to estimate the parameters.69While these approaches have been kept separate, they haveinfluenced each other.
Statistical techniques have broughtto NL a clearer notion of evaluation: that here are separatetraining and testing corpuses and a "fair" test is on data youhaven't seen before.
Symbolic techniques have brought thenotion of understanding a problem by looking closely atthe places where it performs poorly.
For example, we'reseeing a renewed emphasis on tools in speech processingwork.1.2 Integrating symbolic and statisticaltechniquesIn determining how to most effectively combine theseapproaches, it is useful to view them not as a dichotomy,but rather as a continuum of approaches.
Kanal andChandrasekan (1972) take this view in their analysis ofpattern recognition techniques, which they characterize as,at one end, purely "linguistic", with generative grammarsrepresenting syntactic structure, and at the other"geometric" approaches, which are purely statistical--patterns are represented as points in a multidimensionalfeature space, where the "features" are left undefined in themodel.
In the middle are "structural" approaches, wherepatterns are defined as relations among a set of primitiveswhich may or may not be associated with probabilities.Kanal and Chandrasekan argue that rather than select alinguistic or geometric solution for a particular problem,one should divide the problem into subproblemshierarchically, deciding at each level whether to apply asolution from the range between linguistic and geometricor to "further subdivide.
In this view the various methodsare complementary, rather than rivals.
Importantconsiderations in making the choice of what approach touse is how much and what kind of a priori information isavailable and where information is noisy or uncertain.In fact, nearly all "statistical" approaches used in NL andspeech fall somewhere in this continuum, rather than at theextreme.
Purely statistical topic classification techniquesuse words as the primitives, which are features that havesome meaning and relationships to one another, eventhough these relationships may be exploited only throughstatistical correlations.
The states in a hidden Markovmodel for speech form phonemes, which is conceptualrather than acoustic phenomenon and specific to aparticular language, and the expansion of phoneme statesinto networks are based on a dictionary.
Therefore, even ina null grammar there is a great deal of a priori knowledgebeing brought to bear.In the work described here, we have attempted a closeintegration of statistical and symbolic methods thatleverages the a priori knowledge that can be represented inphrase grammars with the knowledge that can be acquiredusing statistical methods.
For example, a classificationalgorithm can select which key words can be used todiscriminate a topic.
By adding semantic features to a textusing a parser and semantic grammar, we can increase theamount of domain specific information available for theclassification algorithm to operate over.
Another exampleis in language modeling for recognition: a statistical N-gram language model provides information on thefikefihood of one word to follow another; by adding phrasegrammars, we can also learn the likelihood of particulardomain specific phrases, and then we can use that samegrammar to actually interpret those phrases and extract heinformation being communicated.
The body of this paperdescribes in detail where we have chosen to integratelinguistic and structural knowledge into our statisticalalgorithms.2.
APPLICATION OF TECHNIQUESThe bulk of our work in integrating symbolic andstatistical approaches has been in the development of the"Gister" system (Rohlicek, et.
al 1992), which is designedto extract information from voice communications.
Wedeveloped and tested the algorithms using off-the-airAudioSignal Transmissions Pilot/controller separationAcoustic similarityRecognlzerand parseroutputDialoguesandscensdcoFigure 1: The high level "Gister" boxology70commercial air traffic control recordings, where the goalwas to identify the flights present and determine thescenario (e.g.
takeoff, landing).
We have also extended thesystem to extract more specific information from the ATCcommands, uch as direction orders and tower clearances.Figure One shows the overall boxology of the Gistingsystem.There are several characteristics of the domain that make itamenable to the unique combination of techniques weemployed.
First, the language is stereotypical, that isthere are few variations in the way the information can beexpressed, and we have available xpertise on how theinformation is expressed: it is regulated by the FAA anddescribed in FAA manuals.
Second, the signal is verynoisy, so that traditional techniques don't work very well(recognition results how a 25-30% word error rate).
Ourgoal was to leverage our a priori knowledge of the domainto reduce the uncertainty inherent in the problem.
Themost obvious place to start was to improve speechrecognition by introducing domain specific information ithe language model.2.1 Language Model ingThe role of the language model in the speech recognitioncomponent is to constrain the possibilities of what wordcan come next and to mark each possibility with itsprobability: the likelihood that it will occur in a particularcontext.
A common approach to language modeling is touse statistically based Markov-chain language models (n-gram models).
While this approach as been shown to beeffective for speech recognition, there is, in general, morestructure present in natural language than n-gram modelscan capture.
In particular n-grams do not explicitly capturelong distance dependencies.
For example, a private planeidentifier consists of the name of a plane type, some digits,and one or two letter words (e.g.
"Sessna six one two onekilo").
Because of the frequency of digits in this domain,an n-gram will find that the most likely thing to follow adigit is another digit; the relationship between the firstelements of the phrase (the plane type) and the last (a letterword) is lost.In our approach we integrated phrase grammars (whichwere already being used to extract information from theresults of recognition) with n-grams, thereby introducingas much linguistic structure and prior statisticalinformation as is available while maintaining a robust full-coverage statistical language model for recognition.As shown in Figure Two, there are two main inputs to themodel construction portion of the system: a transcribedspeech training set and a phrase-structure grammar.
Thephrase-structure grammar is used to partially parse thetraining text.
The output of this is: (1) a top-level versionof the original text with subsequences of words replaced bythe non-terminals that accept hose subsequences; and (2) aset of parse trees for the instances of those nonterminals.We first describe the parser and grammar and then discusshow we use them for language modeling.For both the language modeling and information extraction(the shaded boxes in Figure 2), we are using the partialparser Sparser (McDonald 1992).
Sparser is a bottom-upchart parser which uses a semantic phrase structuregrammar (i.e.
the nonterminals are semantic ategories,such as HEADING or FLIGHT-ID, rather than traditionalsyntactic ategories, uch as CLAUSE or NOUN-PHRASE).Sparser makes no assumption that the chart will becomplete, i.e.
that a top level category will cover all of theinput, or even that all terminals will be covered bycategories, effectively allowing unknown words to beignored.
Rather it simply builds constituent s ructure forthose phrases that are in its grammar.Our approach to creating the rules was typical of symbolicapproaches: we wrote rules using our knowledge of theATC domain gained from experts and manuals, ran themon a portion of our data, inspected the results, rewrote therules, and iterated.
In the case of flight IDs, we couldapply more extensive valuation techniques since eachutterance in our corpus was already annotated with thisFigure 2: Language Modeling and Information Extraction in the Glsting system71information.
However, for other kinds of statements, suchas controller orders or pilot replies, there was no master"answer" list against which to evaluate.
We had only twomeasures to use to evaluate our grammar:l the overallcoverage (what percentage of the words was covered bysome category in the grammar), and the specific coverage,which can only be determined by inspecting the results byhand and noticing when some command occurred that wasnot picked up by the parser.
Note that since in thisdomain we know that there is relatively little variation, sothat sampling the data can be assumed to be sufficient todetermine coverage, which is not the case in lessconstrained domains.
Figure 3 shows a small set ofexamples of the rules:R I (def-ruleR2 (def-ruleR3 (dcf-ruleR4 (def-ruleR5 (def-ruleR6 (def-ruleR7 (def-ruleR8 (def-ruleFigureland-action > ("land"))takeoff-action > ("takeoff"))takeoff-action > ("go"))clrd/land > ("cleared" to" land-action)clrd/takeoff >("cleared" to" takeoff-action))clrd/takeoff >("cleared" for" takeoff-action )))tower-clearance > (runway clrd/land)tower-clearance > (runway clrd/takeoff ))3: Phrase structure rules for towerclearanceThe n-gram model was trained not with the originaltranscripts, but rather with transcripts where the targetedphrases defined in our grammar were replaced by theirnonterminal categories.
Note that in this case, where goalis to model aircraft identifiers and a small set of air trafficcontrol commands, other phrases like the identification ofthe controller, traffic information, etc., are left as words tobe modeled by the n-gram.
Examples of the originaltranscripts and the n-gram training are shown below:>Nera twenty one zero nine runway two two right clearedfor takeoff>COMMERCIAL.AIRPLANE TOWER.CLEARANCE>Nera thirty seven twelve Boston tower runway two tworight cleared for takeoff>COMMERCIAL-AIRPLANE Boston tower TOWER-CLEARANCE>Jet Link thirty eight sixteen Boston tower runway twotwo right cleared for takeoff traffic on a five mile finallanding two two right>COMMERCIAL-AIRPLANE Boston tower TOWER-CLEARANCE traffic on a five mile final landing RUNWAY>Jet Link thirty eight zero five runway two two rightcleared for takeoff sorry for the delay>COMMERCIAL-AIRPLANE TOWER-CLEARANCE sorry forthe delayFigure 4: Training text modified by parserFor the specific phrases we are interested in, we use theparse trees are used to obtain statistics for the estimation ofproduction probabilities for the rules in the grammar.Since we assume that the production probabilities dependon their context, a simple count is insufficient.
Smoothedmaximum likelihood production probabilities are estimatedbased on context dependent counts.
The context is definedas the sequence of rules and positions on the right-handsides of the rules leading from the root of the parse tree tothe non-terminal at the leaf.
The probability of a parsetherefore takes into account that the expansion of acategory may depend on its parents.For example, in the above grammar (Figure 3), theexpansion of TAKEOFF-ACTION may be different dependingon whether it is part of rule 5 or rule 6.
Therefore, the"context" of a production is a sequence of rules andpositions that have been used up to that point, where the"position" is where in the RHS of the rule the nonterminalis.
For example, in the parse shown below (Figure 4), thecontext of R2 (TAKEOFF-ACTION > "takeoff") is rule6/position 3, rule 8/position 2.
(See Meteer & Rohlicek1993 for a more detailed discussion of the probabilitiesrequired evaluate the probabifity of a parse.
)TOWER-CLEARANCE (RS)CLRD/TAKEOFF (R6)"runway" ~ .c le~~:OFF  -oNFs o Es Ln-O A on n2"two" "six" "ddht = "taki~off"Figure 5: Parse tree with path highlightedIn order to use a phrase-structure grammar directly in atime-synchronous recognition algorithm, it is necessary toconstruct a finite-state network representation.
2 If there isno recursion in the grammar, then this procedure isstraightforward: for each rule, each possible contextcorresponds toa separate subnetwork.
The subnetworks fordifferent rules are nested.
Figure 6 shows the expansion ofthe rules in Figure 3.I Note that given the narrowness of the domain, the issue inprocessing transcripts is rarely correctness, but rathercoverage: do the rules capture all of the alternative ways theinformation carl be expressed.2 The phrase grammar formalism is context free; however, inpractice, we limited the grammar to finite state so that it canbe more easily integrated into the recognizer.
We areconsidering various means of finite state approximations inorder to use a more powerful grammar, but haven't foundsufficient need in this domain to press the issue.72There have been several attempts to use probabilityestimates with context free grammars.
The most commontechnique is using the Inside-Outside algorithm (e.g.Pereira & Schabes 1992, Mark, et al 1992) to infer agrammar over bracketed texts or to obtain Maximum-Likelihood estimates for a highly ambiguous grammar.However, most require a full coverage grammar, whereaswe assume that only a selective portion of the text will becovered by the grammar.
A second ifference is that theyuse a syntactic grammar, which results in the parse beinghighly ambiguous (thus requiring the use of the Inside-Outside algorithm).
We use a semantic grammar, withwhich there are rarely multiple interpretations for a singleutterance in this domain.2.2 In format ion  extract ionThe information extraction component of the systememploys purely symbolic techniques, using same grammardefined for language modeling (as in Figure 3) withassociated routines for creating referents as a side affect offiring a rule.
Since the uncertainty of the problem lies inthe fact that he recognition iserrorful, once a grammar hasbeen developed on one set of transcripts one can achievenearly perfect extraction of flight IDs and commands, incethey are the most regular (and regulated) portions of theutterances.
In fact, because of this, we were able to use theresults of the parser on the transcripts o provide an answerkey for evaluation.
While this is not a completely accuratetest, since there may be cases where a command isexpressed in a way that is outside the competence of thegrammar, it does make evaluation tractable, since the timeit would take to mark the transcripts by hand would beprohibitive.
(See Meteer & Rohlicek 1994 for a moredetailed escription of the information extraction portion ofthe system and the precision and recall results.
)2.3 Scenar io  c lass i f icat ionAnother component of the Gisting system is scenarioclassification: given a dialog between pilot and controller,determine the overall scenario being followed.
Animportant aspect of the problem is that classification isperformed on the output of the speech recognizer.
We useda standard statistical technique for classification, a decisiontree constructed using the CART methodology (Breiman,et.al).
Decision trees have the advantage that theysimultaneously select what the most discriminatingfeatures are (from some given feature set, which in the caseof text classification is generally the words), and build themodel.Decision trees are interesting predictors, in that they oftenfind features that are telling, but that an expert would notnecessarily have thought of.
For example if one scenariois more likely to include a radio frequency, then the word"point" may turn out to be very discriminating.
Whenapplying classification to the output of recognition, onemust choose not only features that are distinguishing, butalso ones that are easily recognized, so that they will bereliably in the output.
One must be also careful to crossvalidate results on a test set to avoid overtraining: findingfeatures that are peculiar to the training.
For example if inthe collected ata, one airline had many more takeoffs thanlandings, then that airline may be picked as adiscriminator, even though it is not a good discriminator ingeneral (all the planes that ake off eventually land.We used integrated symbolic methods into classification byusing the parser and grammar to augment the input to theclassifier with semantic features, as shown in the examplebelow.
3 This is the same process as that which created the3 Note that for clarity this example is from the transcriptions.not from the output of re, cognition.
In the Gisting system, theclassifier is trained on both the annotated ten best outputsFigure 6: Finite state network73N-gram training, only in this case, nonterminals areinserted rather than replacing phrases.
Note that some ofthe categories merely emphasize something alreadyavailable from a lexical item, such as "takeoff' and"takeoff-action", whereas others capture information that isonly implicit, such as the fact that "two two right" is arunway.COMMERCIAL-AIRPLANE nera thirty four ninety eightTURN-ORDER turn right heading two seven zeroCLRD/TAKEOFF RUNWAY two two fight CLRDITAKEOFFcleared for TAKEOFF-ACTION takeoffIn a sense, the parser provided equivalence classes forphrases, since, for example, the nonterminal RUNWAYwas added when any one of the several runways werementioned.As in the information extraction component, we used theparser to determine the correct scenario based on thetranscripts, which in this case provided training for themodel.
Remember, the uncertainty in this problem isintroduced by the poor recognition results; the domain issufficiently narrow that scenarios can be classifieddeterministically.
For each dialog (which we determineusing the speaker and hearer fields in the transcript), thesystem parsed transmissions until an unambiguouscommand is found (for example "cleared to land" and"contact ground" are only given when a plane is landing),then marked all the transmissions in that dialog as to thescenario.
There will be some cases that are uncertain, forexample, if only part of the transcription is available, andthese cases are marked "unknown" and presented tothe userwho may be able to find some more subtle clue to thescenario.2.4 Event SpottingWe are also applying these techniques in otherapplications.
In particular, we have recently performedexperiments in Event Spotting, which is an extension ofword spotting where the goal is to determine the location'of phrases, rather than single keywords.
We used theparser/extraction portion of the system to find examples ofphrase types in the corpus and to evaluate the results, aswell as in the language model of the recognizer.
In anexperiment detecting time and date phrases in theSwitchboard corpus (which is conversational telephonequality data), we saw an increase in detection rate overstrictly bi-gram or phoneme loop language models(Jeanrenaud, etal.
1994).from the recognizer and the annotated transcription; testing isjust on the Ist best recognition output.3.
CONCLUSIONCombining symbolic and statistical techniques inour workso far has increased both the competence and performanceof our systems.
We are also beginning to combine thesetechniques into tools to help us better understand theproblems, ranging from corpus based techniques, whichbegin with rules and apply them to large bodies of data tofind examples of specific kinds of phenomena, to statisticaltechniques, such as mutual information, to help usunderstand what features contribute the most in aprobabilistic model.
In the full paper, we will expand bothon this aspect of our work and project forward from ourexperience to help assess where to best apply thesemethodologies.AcknowledgmentsThis work was supported by ARPA and the Air ForceRome Laboratory under contract F30602-89-C-0170 andthe Department ofDefense.REFERENCESBreiman, L., Friedman, J.H., Osben, R.A. & Stone, C.J.
(1984) Classification and Regression Trees.
WadsworthInternational Group.
Belmont, CA.Jeanrenaud, P., Siu, M., Rohlicek, R., M., Meteer, Gish,H.
(1994) "Spotting Events in Continuous Speech" inProceedings of International Conference of Acoustics,Speech, and Signal Processing (ICASSP), April 1994,Adelaide, Australia.Kanal and Chandrasekan (1972) "On Linguistic, Statisticaland Mixed Models for Pattern Recognition" inProceedings of the International Conference on Frontiersof Pattern Recognition, S. Watanabc (ed) AcademicPress.
p.161-185.Mark, K., Miller, M., Grenander, U., & Abney, S. (1992)"Parameter Estimation for Constrained Context-freeLanguage Models" in Proceedings of the Speech andNatural Language Workshop, February, 1992, MorganKaufmann, San Mateo, CA, p. 146-149.McDonald, David D. (1992) "An Efficient Chart-basedAlgorithm for Partial Parsing of Unrestricted Texts" inProceedings of the 3rd Conference on Applied NaturalLanguage Processing, April 1-3, 1992, Trento, Italy,pp.
193-200.Meteer, M. & Rohlicek, R. (1993) "Statistical LanguageModeling Combining N-gram and Context-freeGrammars" inProceedings of International Conference ofAcoustics, Speech, and Signal Processing (ICASSP).Meteer, M: & Rohlicek, R. (1994) "Integrated Techniquesfor Phrase Extraction from Speech" in Proceedings for74the ARPA Workshop on Human Language Technology,March 8- I I, Princeton, New Jersey.Pereira, F. & Schabes, E. (1992) "Inside-OutsideReestimation from Partially Bracketed Corpora" inProceedings of the Speech and Natural LanguageWorkshop, February, 1992, Morgan Kaufmann, SanMateo, CA, p. 122-127.Rohlicek, R., Ayuso, D., Bates, M., Bobrow, R.,Boulanger, A., Gish, H., Jeanrenaud, M., Meteer, M.,Siu, M. (1992) "Gisting Conversational Speech" inProc~dings of International Conference of Acoustics,Speech, and Signal Processing (ICASSP), Mar.
23-26,1992, Voi.2, pp.
113-116.Weischedel, R., Meteer, M., and Schwartz, R. (1993)"Coping with Ambiguity and Unknown Words ThroughProbabilistic Models" Computational Linguistics ,19(2), pp.359-382.75
