Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 72?82,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsExact Decoding of Syntactic Translation Modelsthrough Lagrangian RelaxationAlexander M. RushMIT CSAIL,Cambridge, MA 02139, USAsrush@csail.mit.eduMichael CollinsDepartment of Computer Science,Columbia University,New York, NY 10027, USAmcollins@cs.columbia.eduAbstractWe describe an exact decoding algorithm forsyntax-based statistical translation.
The ap-proach uses Lagrangian relaxation to decom-pose the decoding problem into tractable sub-problems, thereby avoiding exhaustive dy-namic programming.
The method recovers ex-act solutions, with certificates of optimality,on over 97% of test examples; it has compa-rable speed to state-of-the-art decoders.1 IntroductionRecent work has seen widespread use of syn-chronous probabilistic grammars in statistical ma-chine translation (SMT).
The decoding problem fora broad range of these systems (e.g., (Chiang, 2005;Marcu et al, 2006; Shen et al, 2008)) correspondsto the intersection of a (weighted) hypergraph withan n-gram language model.1 The hypergraph rep-resents a large set of possible translations, and iscreated by applying a synchronous grammar to thesource language string.
The language model is thenused to rescore the translations in the hypergraph.Decoding with these models is challenging,largely because of the cost of integrating an n-gramlanguage model into the search process.
Exact dy-namic programming algorithms for the problem arewell known (Bar-Hillel et al, 1964), but are too ex-pensive to be used in practice.2 Previous work ondecoding for syntax-based SMT has therefore beenfocused primarily on approximate search methods.This paper describes an efficient algorithm for ex-act decoding of synchronous grammar models fortranslation.
We avoid the construction of (Bar-Hillel1This problem is also relevant to other areas of statisticalNLP, for example NL generation (Langkilde, 2000).2E.g., with a trigram language model they run in O(|E|w6)time, where |E| is the number of edges in the hypergraph, andw is the number of distinct lexical items in the hypergraph.et al, 1964) by using Lagrangian relaxation to de-compose the decoding problem into the followingsub-problems:1.
Dynamic programming over the weighted hy-pergraph.
This step does not require languagemodel integration, and hence is highly efficient.2.
Application of an all-pairs shortest path al-gorithm to a directed graph derived from theweighted hypergraph.
The size of the deriveddirected graph is linear in the size of the hyper-graph, hence this step is again efficient.Informally, the first decoding algorithm incorporatesthe weights and hard constraints on translations fromthe synchronous grammar, while the second decod-ing algorithm is used to integrate language modelscores.
Lagrange multipliers are used to enforceagreement between the structures produced by thetwo decoding algorithms.In this paper we first give background on hyper-graphs and the decoding problem.
We then describeour decoding algorithm.
The algorithm uses a sub-gradient method to minimize a dual function.
Thedual corresponds to a particular linear programming(LP) relaxation of the original decoding problem.The method will recover an exact solution, with acertificate of optimality, if the underlying LP relax-ation has an integral solution.
In some cases, how-ever, the underlying LP will have a fractional solu-tion, in which case the method will not be exact.
Thesecond technical contribution of this paper is to de-scribe a method that iteratively tightens the underly-ing LP relaxation until an exact solution is produced.We do this by gradually introducing constraints tostep 1 (dynamic programming over the hypergraph),while still maintaining efficiency.72We report experiments using the tree-to-stringmodel of (Huang and Mi, 2010).
Our method givesexact solutions on over 97% of test examples.
Themethod is comparable in speed to state-of-the-art de-coding algorithms; for example, over 70% of the testexamples are decoded in 2 seconds or less.
We com-pare our method to cube pruning (Chiang, 2007),and find that our method gives improved modelscores on a significant number of examples.
Oneconsequence of our work is that we give accurateestimates of the number of search errors for cubepruning.2 Related WorkA variety of approximate decoding algorithms havebeen explored for syntax-based translation systems,including cube-pruning (Chiang, 2007; Huang andChiang, 2007), left-to-right decoding with beamsearch (Watanabe et al, 2006; Huang and Mi, 2010),and coarse-to-fine methods (Petrov et al, 2008).Recent work has developed decoding algorithmsbased on finite state transducers (FSTs).
Iglesias etal.
(2009) show that exact FST decoding is feasiblefor a phrase-based system with limited reordering(the MJ1 model (Kumar and Byrne, 2005)), and deGispert et al (2010) show that exact FST decodingis feasible for a specific class of hierarchical gram-mars (shallow-1 grammars).
Approximate searchmethods are used for more complex reordering mod-els or grammars.
The FST algorithms are shown toproduce higher scoring solutions than cube-pruningon a large proportion of examples.Lagrangian relaxation is a classical techniquein combinatorial optimization (Korte and Vygen,2008).
Lagrange multipliers are used to add lin-ear constraints to an existing problem that can besolved using a combinatorial algorithm; the result-ing dual function is then minimized, for exampleusing subgradient methods.
In recent work, dualdecomposition?a special case of Lagrangian relax-ation, where the linear constraints enforce agree-ment between two or more models?has been ap-plied to inference in Markov random fields (Wain-wright et al, 2005; Komodakis et al, 2007; Sontaget al, 2008), and also to inference problems in NLP(Rush et al, 2010; Koo et al, 2010).
There are closeconnections between dual decomposition and workon belief propagation (Smith and Eisner, 2008).3 Background: HypergraphsTranslation with many syntax-based systems (e.g.,(Chiang, 2005; Marcu et al, 2006; Shen et al, 2008;Huang and Mi, 2010)) can be implemented as atwo-step process.
The first step is to take an in-put sentence in the source language, and from thisto create a hypergraph (sometimes called a transla-tion forest) that represents the set of possible trans-lations (strings in the target language) and deriva-tions under the grammar.
The second step is tointegrate an n-gram language model with this hy-pergraph.
For example, in the system of (Chiang,2005), the hypergraph is created as follows: first, thesource side of the synchronous grammar is used tocreate a parse forest over the source language string.Second, transduction operations derived from syn-chronous rules in the grammar are used to create thetarget-language hypergraph.
Chiang?s method usesa synchronous context-free grammar, but the hyper-graph formalism is applicable to a broad range ofother grammatical formalisms, for example depen-dency grammars (e.g., (Shen et al, 2008)).A hypergraph is a pair (V,E) where V ={1, 2, .
.
.
, |V |} is a set of vertices, and E is a set ofhyperedges.
A single distinguished vertex is takenas the root of the hypergraph; without loss of gener-ality we take this vertex to be v = 1.
Each hyper-edge e ?
E is a tuple ?
?v1, v2, .
.
.
, vk?, v0?
wherev0 ?
V , and vi ?
{2 .
.
.
|V |} for i = 1 .
.
.
k. Thevertex v0 is referred to as the head of the edge.
Theordered sequence ?v1, v2, .
.
.
, vk?
is referred to asthe tail of the edge; in addition, we sometimes referto v1, v2, .
.
.
vk as the children in the edge.
The num-ber of children k may vary across different edges,but k ?
1 for all edges (i.e., each edge has at leastone child).
We will use h(e) to refer to the head ofan edge e, and t(e) to refer to the tail.We will assume that the hypergraph is acyclic: in-tuitively this will mean that no derivation (as definedbelow) contains the same vertex more than once (see(Martin et al, 1990) for a formal definition).Each vertex v ?
V is either a non-terminal in thehypergraph, or a leaf.
The set of non-terminals isVN = {v ?
V : ?e ?
E such that h(e) = v}Conversely, the set of leaves is defined asVL = {v ?
V : 6 ?e ?
E such that h(e) = v}73Finally, we assume that each v ?
V has a labell(v).
The labels for leaves will be words, and willbe important in defining strings and language modelscores for those strings.
The labels for non-terminalnodes will not be important for results in this paper.3We now turn to derivations.
Define an index setI = V ?
E. A derivation is represented by a vectory = {yr : r ?
I} where yv = 1 if vertex v is used inthe derivation, yv = 0 otherwise (similarly ye = 1 ifedge e is used in the derivation, ye = 0 otherwise).Thus y is a vector in {0, 1}|I|.
A valid derivationsatisfies the following constraints:?
y1 = 1 (the root must be in the derivation).?
For all v ?
VN , yv =?e:h(e)=v ye.?
For all v ?
2 .
.
.
|V |, yv =?e:v?t(e) ye.We use Y to refer to the set of valid derivations.The set Y is a subset of {0, 1}|I| (not all members of{0, 1}|I| will correspond to valid derivations).Each derivation y in the hypergraph will imply anordered sequence of leaves v1 .
.
.
vn.
We use s(y) torefer to this sequence.
The sentence associated withthe derivation is then l(v1) .
.
.
l(vn).In a weighted hypergraph problem, we assume aparameter vector ?
= {?r : r ?
I}.
The score forany derivation is f(y) = ?
?
y = ?r?I ?ryr.
Sim-ple bottom-up dynamic programming?essentiallythe CKY algorithm?can be used to find y?
=argmaxy?Y f(y) under these definitions.The focus of this paper will be to solve problemsinvolving the integration of a k?th order languagemodel with a hypergraph.
In these problems, thescore for a derivation is modified to bef(y) =?r?I?ryr +n?i=k?
(vi?k+1, vi?k+2, .
.
.
, vi) (1)where v1 .
.
.
vn = s(y).
The ?
(vi?k+1, .
.
.
, vi)parameters score n-grams of length k. Theseparameters are typically defined by a languagemodel, for example with k = 3 we would have?
(vi?2, vi?1, vi) = log p(l(vi)|l(vi?2), l(vi?1)).The problem is then to find y?
= argmaxy?Y f(y)under this definition.Throughout this paper we make the following as-sumption when using a bigram language model:3They might for example be non-terminal symbols from thegrammar used to generate the hypergraph.Assumption 3.1 (Bigram start/end assump-tion.)
For any derivation y, with leavess(y) = v1, v2, .
.
.
, vn, it is the case that: (1)v1 = 2 and vn = 3; (2) the leaves 2 and 3 cannotappear at any other position in the strings s(y) fory ?
Y; (3) l(2) = <s> where <s> is the startsymbol in the language model; (4) l(3) = </s>where </s> is the end symbol.This assumption allows us to incorporate lan-guage model terms that depend on the start and endsymbols.
It also allows a clean solution for boundaryconditions (the start/end of strings).44 A Simple Lagrangian RelaxationAlgorithmWe now give a Lagrangian relaxation algorithm forintegration of a hypergraph with a bigram languagemodel, in cases where the hypergraph satisfies thefollowing simplifying assumption:Assumption 4.1 (The strict ordering assumption.
)For any two leaves v and w, it is either the casethat: 1) for all derivations y such that v and w areboth in the sequence l(y), v precedes w; or 2) for allderivations y such that v and w are both in l(y), wprecedes v.Thus under this assumption, the relative orderingof any two leaves is fixed.
This assumption is overlyrestrictive:5 the next section describes an algorithmthat does not require this assumption.
However de-riving the simple algorithm will be useful in devel-oping intuition, and will lead directly to the algo-rithm for the unrestricted case.4.1 A Sketch of the AlgorithmAt a high level, the algorithm is as follows.
We in-troduce Lagrange multipliers u(v) for all v ?
VL,with initial values set to zero.
The algorithm theninvolves the following steps: (1) For each leaf v,find the previous leaf w that maximizes the score?
(w, v) ?
u(w) (call this leaf ??
(v), and define?v = ?(??
(v), v) ?
u(??(v))).
(2) find the high-est scoring derivation using dynamic programming4The assumption generalizes in the obvious way to k?th or-der language models: e.g., for trigram models we assume thatv1 = 2, v2 = 3, vn = 4, l(2) = l(3) = <s>, l(4) = </s>.5It is easy to come up with examples that violate this as-sumption: for example a hypergraph with edges ?
?4, 5?, 1?
and?
?5, 4?, 1?
violates the assumption.
The hypergraphs found intranslation frequently contain alternative orderings such as this.74over the original (non-intersected) hypergraph, withleaf nodes having weights ?v + ?v + u(v).
(3) Ifthe output derivation from step 2 has the same set ofbigrams as those from step 1, then we have an exactsolution to the problem.
Otherwise, the Lagrangemultipliers u(v) are modified in a way that encour-ages agreement of the two steps, and we return tostep 1.Steps 1 and 2 can be performed efficiently; in par-ticular, we avoid the classical dynamic programmingintersection, instead relying on dynamic program-ming over the original, simple hypergraph.4.2 A Formal DescriptionWe now give a formal description of the algorithm.Define B ?
VL?VL to be the set of all ordered pairs?v, w?
such that there is at least one derivation y withv directly preceding w in s(y).
Extend the bit-vectory to include variables y(v, w) for ?v, w?
?
B wherey(v, w) = 1 if leaf v is followed by w in s(y), 0otherwise.
We redefine the index set to be I = V ?E ?
B, and define Y ?
{0, 1}|I| to be the set of allpossible derivations.
Under assumptions 3.1 and 4.1above, Y = {y : y satisfies constraints C0, C1, C2}where the constraint definitions are:?
(C0) The yv and ye variables form a derivationin the hypergraph, as defined in section 3.?
(C1) For all v ?
VL such that v 6= 2, yv =?w:?w,v?
?B y(w, v).?
(C2) For all v ?
VL such that v 6= 3, yv =?w:?v,w?
?B y(v, w).C1 states that each leaf in a derivation has exactlyone in-coming bigram, and that each leaf not in thederivation has 0 incoming bigrams; C2 states thateach leaf in a derivation has exactly one out-goingbigram, and that each leaf not in the derivation has 0outgoing bigrams.6The score of a derivation is now f(y) = ?
?
y, i.e.,f(y) =?v?vyv+?e?eye+??v,w??B?
(v, w)y(v, w)where ?
(v, w) are scores from the language model.Our goal is to compute y?
= argmaxy?Y f(y).6Recall that according to the bigram start/end assumptionthe leaves 2/3 are reserved for the start/end of the sequences(y), and hence do not have an incoming/outgoing bigram.Initialization: Set u0(v) = 0 for all v ?
VLAlgorithm: For t = 1 .
.
.
T :?
yt = argmaxy?Y?
L(ut?1, y)?
If yt satisfies constraints C2, return yt,Else ?v ?
VL, ut(v) =ut?1(v)?
?t(yt(v)??w:?v,w?
?B yt(v, w)).Figure 1: A simple Lagrangian relaxation algorithm.
?t > 0 is the step size at iteration t.Next, define Y ?
asY ?
= {y : y satisfies constraints C0 and C1}In this definition we have dropped the C2 con-straints.
To incorporate these constraints, we useLagrangian relaxation, with one Lagrange multiplieru(v) for each constraint in C2.
The Lagrangian isL(u, y) = f(y) +?vu(v)(y(v)??w:?v,w?
?By(v, w))= ?
?
ywhere ?v = ?v + u(v), ?e = ?e, and ?
(v, w) =?
(v, w)?
u(v).The dual problem is to find minu L(u) whereL(u) = maxy?Y ?L(u, y)Figure 1 shows a subgradient method for solvingthis problem.
At each point the algorithm findsyt = argmaxy?Y ?
L(ut?1, y), where ut?1 are theLagrange multipliers from the previous iteration.
Ifyt satisfies the C2 constraints in addition to C0 andC1, then it is returned as the output from the algo-rithm.
Otherwise, the multipliers u(v) are updated.Intuitively, these updates encourage the values of yvand?w:?v,w?
?B y(v, w) to be equal; formally, theseupdates correspond to subgradient steps.The main computational step at each iteration is tocompute argmaxy?Y ?
L(ut?1, y) This step is easilysolved, as follows (we again use ?v, ?e and ?
(v1, v2)to refer to the parameter values that incorporate La-grange multipliers):?
For all v ?
VL, define ??
(v) =argmaxw:?w,v?
?B ?
(w, v) and ?v =?(??
(v), v).
For all v ?
VN define ?v = 0.75?
Using dynamic programming, find values forthe yv and ye variables that form a valid deriva-tion, and that maximizef ?
(y) =?v(?v + ?v)yv +?e ?eye.?
Set y(v, w) = 1 iff y(w) = 1 and ??
(w) = v.The critical point here is that through our definitionof Y ?, which ignores the C2 constraints, we are ableto do efficient search as just described.
In the firststep we compute the highest scoring incoming bi-gram for each leaf v. In the second step we useconventional dynamic programming over the hyper-graph to find an optimal derivation that incorporatesweights from the first step.
Finally, we fill in they(v, w) values.
Each iteration of the algorithm runsin O(|E|+ |B|) time.There are close connections between Lagrangianrelaxation and linear programming relaxations.
Themost important formal results are: 1) for any valueof u, L(u) ?
f(y?)
(hence the dual value providesan upper bound on the optimal primal value); 2) un-der an appropriate choice of the step sizes ?t, thesubgradient algorithm is guaranteed to converge tothe minimum of L(u) (i.e., we will minimize theupper bound, making it as tight as possible); 3) ifat any point the algorithm in figure 1 finds a yt thatsatisfies the C2 constraints, then this is guaranteedto be the optimal primal solution.Unfortunately, this algorithm may fail to producea good solution for hypergraphs where the strict or-dering constraint does not hold.
In this case it ispossible to find derivations y that satisfy constraintsC0, C1, C2, but which are invalid.
As one exam-ple, consider a derivation with s(y) = 2, 4, 5, 3 andy(2, 3) = y(4, 5) = y(5, 4) = 1.
The constraintsare all satisfied in this case, but the bigram variablesare invalid (e.g., they contain a cycle).5 The Full AlgorithmWe now describe our full algorithm, which does notrequire the strict ordering constraint.
In addition, thefull algorithm allows a trigram language model.
Wefirst give a sketch, and then give a formal definition.5.1 A Sketch of the AlgorithmA crucial idea in the new algorithm is that ofpaths between leaves in hypergraph derivations.Previously, for each derivation y, we had de-fined s(y) = v1, v2, .
.
.
, vn to be the sequenceof leaves in y.
In addition, we will defineg(y) = p0, v1, p1, v2, p2, v3, p3, .
.
.
, pn?1, vn, pnwhere each pi is a path in the derivation betweenleaves vi and vi+1.
The path traces through the non-terminals that are between the two leaves in the tree.As an example, consider the following derivation(with hyperedges ?
?2, 5?, 1?
and ?
?3, 4?, 2?
):123 45For this example g(y) is ?1 ?, 2 ??
?2 ?, 3 ??
?3 ?
?, 3, ?3 ??
?3 ?, 4 ??
?4 ?
?, 4, ?4 ??
?4 ?, 2 ??
?2 ?, 5 ??
?5 ?
?, 5, ?5 ??
?5 ?, 1 ??.
States of theform ?a ??
and ?a ??
where a is a leaf appear inthe paths respectively before/after the leaf a. Statesof the form ?a, b?
correspond to the steps taken in atop-down, left-to-right, traversal of the tree, wheredown and up arrows indicate whether a node is be-ing visited for the first or second time (the traversalin this case would be 1, 2, 3, 4, 2, 5, 1).The mapping from a derivation y to a path g(y)can be performed using the algorithm in figure 2.For a given derivation y, define E(y) = {y : ye =1}, and use E(y) as the set of input edges to thisalgorithm.
The output from the algorithm will be aset of states S, and a set of directed edges T , whichtogether fully define the path g(y).In the simple algorithm, the first step was topredict the previous leaf for each leaf v, undera score that combined a language model scorewith a Lagrange multiplier score (i.e., computeargmaxw ?
(w, v) where ?
(w, v) = ?
(w, v) +u(w)).
In this section we describe an algorithm thatfor each leaf v again predicts the previous leaf, but inaddition predicts the full path back to that leaf.
Forexample, rather than making a prediction for leaf 5that it should be preceded by leaf 4, we would alsopredict the path ?4 ??
?4 ?, 2 ??
?2 ?, 5 ??
?5 ??
be-tween these two leaves.
Lagrange multipliers willbe used to enforce consistency between these pre-dictions (both paths and previous words) and a validderivation.76Input: A set E of hyperedges.
Output: A directed graphS, T where S is a set of vertices, and T is a set of edges.Step 1: Creating S: Define S = ?e?ES(e) where S(e)is defined as follows.
Assume e = ?
?v1, v2, .
.
.
, vk?, v0?.Include the following states in S(e): (1) ?v0 ?, v1 ??
and?vk?, v0??.
(2) ?vj ?, vj+1??
for j = 1 .
.
.
k ?
1 (if k = 1then there are no such states).
(3) In addition, for any vjfor j = 1 .
.
.
k such that vj ?
VL, add the states ?vj ?
?and ?vj ?
?.Step 2: Creating T : T is formed by including the fol-lowing directed arcs: (1) Add an arc from ?a, b?
?
Sto ?c, d?
?
S whenever b = c. (2) Add an arc from?a, b ??
?
S to ?c ??
?
S whenever b = c. (3) Addan arc from ?a ??
?
S to ?b ?, c?
?
S whenever a = b.Figure 2: Algorithm for constructing a directed graph(S, T ) from a set of hyperedges E.5.2 A Formal DescriptionWe first use the algorithm in figure 2 with the en-tire set of hyperedges, E, as its input.
The resultis a directed graph (S, T ) that contains all possiblepaths for valid derivations in V,E (it also containsadditional, ill-formed paths).
We then introduce thefollowing definition:Definition 5.1 A trigram path p is p =?v1, p1, v2, p2, v3?
where: a) v1, v2, v3 ?
VL;b) p1 is a path (sequence of states) between nodes?v1 ??
and ?v2 ??
in the graph (S, T ); c) p2 is apath between nodes ?v2 ??
and ?v3 ??
in the graph(S, T ).
We define P to be the set of all trigram pathsin (S, T ).The set P of trigram paths plays an analogous roleto the set B of bigrams in our previous algorithm.We use v1(p), p1(p), v2(p), p2(p), v3(p) to referto the individual components of a path p. In addi-tion, define SN to be the set of states in S of theform ?a, b?
(as opposed to the form ?c ??
or ?c ?
?where c ?
VL).We now define a new index set, I = V ?
E ?SN ?
P , adding variables ys for s ?
SN , and yp forp ?
P .
If we take Y ?
{0, 1}|I| to be the set ofvalid derivations, the optimization problem is to findy?
= argmaxy?Y f(y), where f(y) = ?
?
y, that is,f(y) =?v?vyv +?e?eye +?s?sys +?p?pypIn particular, we might define ?s = 0 for all s,and ?p = log p(l(v3(p))|l(v1(p)), l(v2(p))) where?
D0.
The yv and ye variables form a valid derivationin the original hypergraph.?
D1.
For all s ?
SN , ys =?e:s?S(e) ye (see figure 2for the definition of S(e)).?
D2.
For all v ?
VL, yv =?p:v3(p)=v yp?
D3.
For all v ?
VL, yv =?p:v2(p)=v yp?
D4.
For all v ?
VL, yv =?p:v1(p)=v yp?
D5.
For all s ?
SN , ys =?p:s?p1(p) yp?
D6.
For all s ?
SN , ys =?p:s?p2(p) yp?
Lagrangian with Lagrange multipliers for D3?D6:L(y, ?, ?, u, v) = ?
?
y+?v ?v(yv ?
?p:v2(p)=v yp)+?v ?v(yv ?
?p:v1(p)=v yp)+?s us(ys ?
?p:s?p1(p) yp)+?s vs(ys ?
?p:s?p2(p) yp).Figure 3: Constraints D0?D6, and the Lagrangian.p(w3|w1, w2) is a trigram probability.The set P is large (typically exponential in size):however, we will see that we do not need to representthe yp variables explicitly.
Instead we will be ableto leverage the underlying structure of a path as asequence of states.The set of valid derivations is Y = {y :y satisfies constraints D0?D6} where the constraintsare shown in figure 3.
D1 simply states that ys = 1iff there is exactly one edge e in the derivation suchthat s ?
S(e).
Constraints D2?D4 enforce consis-tency between leaves in the trigram paths, and the yvvalues.
Constraints D5 and D6 enforce consistencybetween states seen in the paths, and the ys values.The Lagrangian relaxation algorithm is then de-rived in a similar way to before.
DefineY ?
= {y : y satisfies constraints D0?D2}We have dropped the D3?D6 constraints, but thesewill be introduced using Lagrange multipliers.
Theresulting Lagrangian is shown in figure 3, and canbe written as L(y, ?, ?, u, v) = ?
?
y where ?v =?v+?v+?v, ?s = ?s+us+vs, ?p = ?p??(v2(p))??(v1(p))?
?s?p1(p) u(s)?
?s?p2(p) v(s).The dual is L(?, ?, u, v) =maxy?Y ?
L(y, ?, ?, u, v); figure 4 shows a sub-gradient method that minimizes this dual.
The keystep in the algorithm at each iteration is to compute77Initialization: Set ?0 = 0, ?0 = 0, u0 = 0, v0 = 0Algorithm: For t = 1 .
.
.
T :?
yt = argmaxy?Y?
L(y, ?t?1, ?t?1, ut?1, vt?1)?
If yt satisfies the constraints D3?D6, return yt, else:- ?v ?
VL, ?tv = ?t?1v ?
?t(ytv ?
?p:v2(p)=v ytp)- ?v ?
VL, ?tv = ?t?1v ?
?t(ytv ?
?p:v1(p)=v ytp)- ?s ?
SN , uts = ut?1s ?
?t(yts ?
?p:s?p1(p) ytp)- ?s ?
SN , vts = vt?1s ?
?t(yts ?
?p:s?p2(p) ytp)Figure 4: The full Lagrangian relaxation algortihm.
?t >0 is the step size at iteration t.argmaxy?Y ?
L(y, ?, ?, u, v) = argmaxy?Y ?
?
?
ywhere ?
is defined above.
Again, our definitionof Y ?
allows this maximization to be performedefficiently, as follows:1.
For each v ?
VL, define ?
?v =argmaxp:v3(p)=v ?
(p), and ?v = ?(??v).
(i.e., for each v, compute the highest scoringtrigram path ending in v.)2.
Find values for the yv, ye and ys variables thatform a valid derivation, and that maximizef ?
(y) =?v(?v+?v)yv+?e ?eye+?s ?sys3.
Set yp = 1 iff yv3(p) = 1 and p = ?
?v3(p).The first step involves finding the highest scoring in-coming trigram path for each leaf v. This step can beperformed efficiently using the Floyd-Warshall all-pairs shortest path algorithm (Floyd, 1962) over thegraph (S, T ); the details are given in the appendix.The second step involves simple dynamic program-ming over the hypergraph (V,E) (it is simple to in-tegrate the ?s terms into this algorithm).
In the thirdstep, the path variables yp are filled in.5.3 PropertiesWe now describe some important properties of thealgorithm:Efficiency.
The main steps of the algorithm are:1) construction of the graph (S, T ); 2) at each it-eration, dynamic programming over the hypergraph(V,E); 3) at each iteration, all-pairs shortest path al-gorithms over the graph (S, T ).
Each of these stepsis vastly more efficient than computing an exact in-tersection of the hypergraph with a language model.Exact solutions.
By usual guarantees for La-grangian relaxation, if at any point the algorithm re-turns a solution yt that satisfies constraints D3?D6,then yt exactly solves the problem in Eq.
1.Upper bounds.
At each point in the algorithm,L(?t, ?t, ut, vt) is an upper bound on the score ofthe optimal primal solution, f(y?).
Upper boundscan be useful in evaluating the quality of primal so-lutions from either our algorithm or other methodssuch as cube pruning.Simplicity of implementation.
Construction ofthe (S, T ) graph is straightforward.
The othersteps?hypergraph dynamic programming, and all-pairs shortest path?are widely known algorithmsthat are simple to implement.6 Tightening the RelaxationThe algorithm that we have described minimizesthe dual function L(?, ?, u, v).
By usual results forLagrangian relaxation (e.g., see (Korte and Vygen,2008)), L is the dual function for a particular LP re-laxation arising from the definition of Y ?
and the ad-ditional constaints D3?D6.
In some cases the LPrelaxation has an integral solution, in which casethe algorithm will return an optimal solution yt.7In other cases, when the LP relaxation has a frac-tional solution, the subgradient algorithm will stillconverge to the minimum of L, but the primal solu-tions yt will move between a number of solutions.We now describe a method that incrementallyadds hard constraints to the set Y ?, until the methodreturns an exact solution.
For a given y ?
Y ?,for any v with yv = 1, we can recover the previ-ous two leaves (the trigram ending in v) from ei-ther the path variables yp, or the hypergraph vari-ables ye.
Specifically, define v?1(v, y) to be the leafpreceding v in the trigram path p with yp = 1 andv3(p) = v, and v?2(v, y) to be the leaf two posi-tions before v in the trigram path p with yp = 1 andv3(p) = v. Similarly, define v?
?1(v, y) and v?
?2(v, y)to be the preceding two leaves under the ye vari-ables.
If the method has not converged, these twotrigram definitions may not be consistent.
For a con-7Provided that the algorithm is run for enough iterations forconvergence.78sistent solution, we require v?1(v, y) = v?
?1(v, y)and v?2(v, y) = v?
?2(v, y) for all v with yv = 1.Unfortunately, explicitly enforcing all of these con-straints would require exhaustive dynamic program-ming over the hypergraph using the (Bar-Hillel etal., 1964) method, something we wish to avoid.Instead, we enforce a weaker set of constraints,which require far less computation.
Assume somefunction pi : VL ?
{1, 2, .
.
.
q} that partitions theset of leaves into q different partitions.
Then we willadd the following constraints to Y ?
:pi(v?1(v, y)) = pi(v?
?1(v, y))pi(v?2(v, y)) = pi(v?
?2(v, y))for all v such that yv = 1.
Finding argmaxy?Y ?
?
?y under this new definition of Y ?
can be performedusing the construction of (Bar-Hillel et al, 1964),with q different lexical items (for brevity we omitthe details).
This is efficient if q is small.8The remaining question concerns how to choosea partition pi that is effective in tightening the relax-ation.
To do this we implement the following steps:1) run the subgradient algorithm until L is close toconvergence; 2) then run the subgradient algorithmfor m further iterations, keeping track of all pairsof leaf nodes that violate the constraints (i.e., pairsa = v?1(v, y)/b = v?
?1(v, y) or a = v?2(v, y)/b =v?
?2(v, y) such that a 6= b); 3) use a graph color-ing algorithm to find a small partition that places allpairs ?a, b?
into separate partitions; 4) continue run-ning Lagrangian relaxation, with the new constraintsadded.
We expand pi at each iteration to take into ac-count new pairs ?a, b?
that violate the constraints.In related work, Sontag et al (2008) describea method for inference in Markov random fieldswhere additional constraints are chosen to tightenan underlying relaxation.
Other relevant work inNLP includes (Tromble and Eisner, 2006; Riedeland Clarke, 2006).
Our use of partitions pi is relatedto previous work on coarse-to-fine inference for ma-chine translation (Petrov et al, 2008).7 ExperimentsWe report experiments on translation from Chineseto English, using the tree-to-string model described8In fact in our experiments we use the original hypergraphto compute admissible outside scores for an exact A* searchalgorithm for this problem.
We have found the resulting searchalgorithm to be very efficient.Time %age %age %age %age(LR) (DP) (ILP) (LP)0.5s 37.5 10.2 8.8 21.01.0s 57.0 11.6 13.9 31.12.0s 72.2 15.1 21.1 45.94.0s 82.5 20.7 30.7 63.78.0s 88.9 25.2 41.8 78.316.0s 94.4 33.3 54.6 88.932.0s 97.8 42.8 68.5 95.2Median time 0.79s 77.5s 12.1s 2.4sFigure 5: Results showing percentage of examples that are de-coded in less than t seconds, for t = 0.5, 1.0, 2.0, .
.
.
, 32.0.
LR= Lagrangian relaxation; DP = exhaustive dynamic program-ming; ILP = integer linear programming; LP = linear program-ming (LP does not recover an exact solution).
The (I)LP ex-periments were carried out using Gurobi, a high-performancecommercial-grade solver.in (Huang and Mi, 2010).
We use an identicalmodel, and identical development and test data, tothat used by Huang and Mi.9 The translation modelis trained on 1.5M sentence pairs of Chinese-Englishdata; a trigram language model is used.
The de-velopment data is the newswire portion of the 2006NIST MT evaluation test set (616 sentences).
Thetest set is the newswire portion of the 2008 NISTMT evaluation test set (691 sentences).We ran the full algorithm with the tighteningmethod described in section 6.
We ran the methodfor a limit of 200 iterations, hence some exam-ples may not terminate with an exact solution.
Ourmethod gives exact solutions on 598/616 develop-ment set sentences (97.1%), and 675/691 test setsentences (97.7%).In cases where the method does not convergewithin 200 iterations, we can return the best primalsolution yt found by the algorithm during those it-erations.
We can also get an upper bound on thedifference f(y?
)?f(yt) using mint L(ut) as an up-per bound on f(y?).
Of the examples that did notconverge, the worst example had a bound that was1.4% of f(yt) (more specifically, f(yt) was -24.74,and the upper bound on f(y?)?
f(yt) was 0.34).Figure 5 gives information on decoding time forour method and two other exact decoding methods:integer linear programming (using constraints D0?D6), and exhaustive dynamic programming usingthe construction of (Bar-Hillel et al, 1964).
Our9We thank Liang Huang and Haitao Mi for providing us withtheir model and data.79method is clearly the most efficient, and is compara-ble in speed to state-of-the-art decoding algorithms.We also compare our method to cube pruning(Chiang, 2007; Huang and Chiang, 2007).
We reim-plemented cube pruning in C++, to give a fair com-parison to our method.
Cube pruning has a parame-ter, b, dictating the maximum number of items storedat each chart entry.
With b = 50, our decoderfinds higher scoring solutions on 50.5% of all exam-ples (349 examples), the cube-pruning method gets astrictly higher score on only 1 example (this was oneof the examples that did not converge within 200 it-erations).
With b = 500, our decoder finds better so-lutions on 18.5% of the examples (128 cases), cube-pruning finds a better solution on 3 examples.
Themedian decoding time for our method is 0.79 sec-onds; the median times for cube pruning with b = 50and b = 500 are 0.06 and 1.2 seconds respectively.Our results give a very good estimate of the per-centage of search errors for cube pruning.
A naturalquestion is how large b must be before exact solu-tions are returned on almost all examples.
Even atb = 1000, we find that our method gives a bettersolution on 95 test examples (13.7%).Figure 5 also gives a speed comparison of ourmethod to a linear programming (LP) solver thatsolves the LP relaxation defined by constraints D0?D6.
We still see speed-ups, in spite of the factthat our method is solving a harder problem (it pro-vides integral solutions).
The Lagrangian relaxationmethod, when run without the tightening methodof section 6, is solving a dual of the problem be-ing solved by the LP solver.
Hence we can mea-sure how often the tightening procedure is abso-lutely necessary, by seeing how often the LP solverprovides a fractional solution.
We find that this isthe case on 54.0% of the test examples: the tighten-ing procedure is clearly important.
Inspection of thetightening procedure shows that the number of par-titions required (the parameter q) is generally quitesmall: 59% of examples that require tightening re-quire q ?
6; 97.2% require q ?
10.8 ConclusionWe have described a Lagrangian relaxation algo-rithm for exact decoding of syntactic translationmodels, and shown that it is significantly more effi-cient than other exact algorithms for decoding tree-to-string models.
There are a number of possibleways to extend this work.
Our experiments havefocused on tree-to-string models, but the methodshould also apply to Hiero-style syntactic transla-tion models (Chiang, 2007).
Additionally, our ex-periments used a trigram language model, howeverthe constraints in figure 3 generalize to higher-orderlanguage models.
Finally, our algorithm recoversthe 1-best translation for a given input sentence; itshould be possible to extend the method to find k-best solutions.A Computing the Optimal Trigram PathsFor each v ?
VL, define ?v = maxp:v3(p)=v ?
(p), where?
(p) = h(v1(p), v2(p), v3(p))??1(v1(p))??2(v2(p))?
?s?p1(p) u(s)?
?s?p2(p) v(s).
Here h is a function thatcomputes language model scores, and the other terms in-volve Lagrange mulipliers.
Our task is to compute ?
?v forall v ?
VL.It is straightforward to show that the S, T graph isacyclic.
This will allow us to apply shortest path algo-rithms to the graph, even though the weights u(s) andv(s) can be positive or negative.For any pair v1, v2 ?
VL, define P(v1, v2) to be theset of paths between ?v1 ??
and ?v2 ??
in the graph S, T .Each path p gets a score scoreu(p) = ?
?s?p u(s).Next, define p?u(v1, v2) = argmaxp?P(v1,v2) scoreu(p),and score?u(v1, v2) = scoreu(p?).
We assume similardefinitions for p?v(v1, v2) and score?v(v1, v2).
The p?u andscore?u values can be calculated using an all-pairs short-est path algorithm, with weights u(s) on nodes in thegraph.
Similarly, p?v and score?v can be computed usingall-pairs shortest path with weights v(s) on the nodes.Having calculated these values, define T (v) for anyleaf v to be the set of trigrams (x, y, v) such that: 1)x, y ?
VL; 2) there is a path from ?x ??
to ?y ??
and from?y ??
to ?v ??
in the graph S, T .
Then we can calculate?v = max(x,y,v)?T (v)(h(x, y, v)?
?1(x)?
?2(y)+p?u(x, y) + p?v(y, v))in O(|T (v)|) time, by brute force search through the setT (v).Acknowledgments Alexander Rush and MichaelCollins were supported under the GALE program of theDefense Advanced Research Projects Agency, ContractNo.
HR0011-06-C-0022.
Michael Collins was also sup-ported by NSF grant IIS-0915176.
We also thank theanonymous reviewers for very helpful comments; wehope to fully address these in an extended version of thepaper.80ReferencesY.
Bar-Hillel, M. Perles, and E. Shamir.
1964.
On formalproperties of simple phrase structure grammars.
InLanguage and Information: Selected Essays on theirTheory and Application, pages 116?150.D.
Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofthe 43rd Annual Meeting on Association for Compu-tational Linguistics, pages 263?270.
Association forComputational Linguistics.D.
Chiang.
2007.
Hierarchical phrase-based translation.computational linguistics, 33(2):201?228.Adria de Gispert, Gonzalo Iglesias, Graeme Blackwood,Eduardo R. Banga, and William Byrne.
2010.
Hierar-chical Phrase-Based Translation with Weighted Finite-State Transducers and Shallow-n Grammars.
In Com-putational linguistics, volume 36, pages 505?533.Robert W. Floyd.
1962.
Algorithm 97: Shortest path.Commun.
ACM, 5:345.Liang Huang and David Chiang.
2007.
Forest rescoring:Faster decoding with integrated language models.
InProceedings of the 45th Annual Meeting of the Asso-ciation of Computational Linguistics, pages 144?151,Prague, Czech Republic, June.
Association for Com-putational Linguistics.Liang Huang and Haitao Mi.
2010.
Efficient incrementaldecoding for tree-to-string translation.
In Proceedingsof the 2010 Conference on Empirical Methods in Natu-ral Language Processing, pages 273?283, Cambridge,MA, October.
Association for Computational Linguis-tics.Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,and William Byrne.
2009.
Rule filtering by patternfor efficient hierarchical translation.
In Proceedings ofthe 12th Conference of the European Chapter of theACL (EACL 2009), pages 380?388, Athens, Greece,March.
Association for Computational Linguistics.N.
Komodakis, N. Paragios, and G. Tziritas.
2007.MRF optimization via dual decomposition: Message-passing revisited.
In International Conference onComputer Vision.Terry Koo, Alexander M. Rush, Michael Collins, TommiJaakkola, and David Sontag.
2010.
Dual decompo-sition for parsing with non-projective head automata.In Proceedings of the 2010 Conference on Empiri-cal Methods in Natural Language Processing, pages1288?1298, Cambridge, MA, October.
Association forComputational Linguistics.B.H.
Korte and J. Vygen.
2008.
Combinatorial optimiza-tion: theory and algorithms.
Springer Verlag.Shankar Kumar and William Byrne.
2005.
Local phrasereordering models for statistical machine translation.In Proceedings of Human Language Technology Con-ference and Conference on Empirical Methods in Nat-ural Language Processing, pages 161?168, Vancou-ver, British Columbia, Canada, October.
Associationfor Computational Linguistics.I.
Langkilde.
2000.
Forest-based statistical sentence gen-eration.
In Proceedings of the 1st North Americanchapter of the Association for Computational Linguis-tics conference, pages 170?177.
Morgan KaufmannPublishers Inc.Daniel Marcu, Wei Wang, Abdessamad Echihabi, andKevin Knight.
2006.
Spmt: Statistical machinetranslation with syntactified target language phrases.In Proceedings of the 2006 Conference on EmpiricalMethods in Natural Language Processing, pages 44?52, Sydney, Australia, July.
Association for Computa-tional Linguistics.R.K.
Martin, R.L.
Rardin, and B.A.
Campbell.
1990.Polyhedral characterization of discrete dynamic pro-gramming.
Operations research, 38(1):127?138.Slav Petrov, Aria Haghighi, and Dan Klein.
2008.Coarse-to-fine syntactic machine translation using lan-guage projections.
In Proceedings of the 2008 Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 108?116, Honolulu, Hawaii, October.Association for Computational Linguistics.Sebastian Riedel and James Clarke.
2006.
Incrementalinteger linear programming for non-projective depen-dency parsing.
In Proceedings of the 2006 Conferenceon Empirical Methods in Natural Language Process-ing, EMNLP ?06, pages 129?137, Stroudsburg, PA,USA.
Association for Computational Linguistics.Alexander M Rush, David Sontag, Michael Collins, andTommi Jaakkola.
2010.
On dual decomposition andlinear programming relaxations for natural languageprocessing.
In Proceedings of the 2010 Conference onEmpirical Methods in Natural Language Processing,pages 1?11, Cambridge, MA, October.
Association forComputational Linguistics.Libin Shen, Jinxi Xu, and Ralph Weischedel.
2008.
Anew string-to-dependency machine translation algo-rithm with a target dependency language model.
InProceedings of ACL-08: HLT, pages 577?585, Colum-bus, Ohio, June.
Association for Computational Lin-guistics.D.A.
Smith and J. Eisner.
2008.
Dependency parsing bybelief propagation.
In Proc.
EMNLP, pages 145?156.D.
Sontag, T. Meltzer, A. Globerson, T. Jaakkola, andY.
Weiss.
2008.
Tightening LP relaxations for MAPusing message passing.
In Proc.
UAI.Roy W. Tromble and Jason Eisner.
2006.
A fastfinite-state relaxation method for enforcing global con-straints on sequence decoding.
In Proceedings of81the main conference on Human Language TechnologyConference of the North American Chapter of the As-sociation of Computational Linguistics, HLT-NAACL?06, pages 423?430, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.M.
Wainwright, T. Jaakkola, and A. Willsky.
2005.
MAPestimation via agreement on trees: message-passingand linear programming.
In IEEE Transactions on In-formation Theory, volume 51, pages 3697?3717.Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.2006.
Left-to-right target generation for hierarchicalphrase-based translation.
In Proceedings of the 21stInternational Conference on Computational Linguis-tics and the 44th annual meeting of the Association forComputational Linguistics, ACL-44, pages 777?784,Morristown, NJ, USA.
Association for ComputationalLinguistics.82
