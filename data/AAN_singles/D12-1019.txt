Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 205?213, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsSpectral Dependency Parsing with Latent VariablesParamveer S. Dhillon1, Jordan Rodu2, Michael Collins3, Dean P. Foster2and Lyle H. Ungar11Computer & Information Science/ 2Statistics, University of Pennsylvania, Philadelphia, PA, U.S.A3 Computer Science, Columbia University, New York, NY, U.S.A{dhillon|ungar@cis.upenn.edu}, {jrodu|foster@wharton.upenn.edu}mcollins@cs.columbia.eduAbstractRecently there has been substantial interest inusing spectral methods to learn generative se-quence models like HMMs.
Spectral meth-ods are attractive as they provide globally con-sistent estimates of the model parameters andare very fast and scalable, unlike EM meth-ods, which can get stuck in local minima.
Inthis paper, we present a novel extension ofthis class of spectral methods to learn depen-dency tree structures.
We propose a simpleyet powerful latent variable generative modelfor dependency parsing, and a spectral learn-ing method to efficiently estimate it.
As a pi-lot experimental evaluation, we use the spec-tral tree probabilities estimated by our modelto re-rank the outputs of a near state-of-the-art parser.
Our approach gives us a moderatereduction in error of up to 4.6% over the base-line re-ranker.1 IntroductionMarkov models have been for two decades aworkhorse of statistical pattern recognition with ap-plications ranging from speech to vision to lan-guage.
Adding latent variables to these models givesus additional modeling power and have shown suc-cess in applications like POS tagging (Merialdo,1994), speech recognition (Rabiner, 1989) and ob-ject recognition (Quattoni et al2004).
However,this comes at the cost that the resulting parameterestimation problem becomes non-convex and tech-niques like EM (Dempster et al1977) which areused to estimate the parameters can only lead to lo-cally optimal solutions.Recent work by Hsu et al2008) has shown thatglobally consistent estimates of the parameters ofHMMs can be found by using spectral methods, par-ticularly by singular value decomposition (SVD) ofappropriately defined linear systems.
They avoid theNP Hard problem of the global optimization prob-lem of the HMM parameters (Terwijn, 2002), byputting restrictions on the smallest singular valueof the HMM parameters.
The main intuition be-hind the model is that, although the observed data(i.e.
words) seem to live in a very high dimensionalspace, but in reality they live in a very low dimen-sional space (size k ?
30 ?
50) and an appropriateeigen decomposition of the observed data will re-veal the underlying low dimensional dynamics andthereby revealing the parameters of the model.
Be-sides ducking the NP hard problem, the spectralmethods are very fast and scalable to train comparedto EM methods.In this paper we generalize the approach of Hsuet al2008) to learn dependency tree structures withlatent variables.1 Petrov et al2006) and Musilloand Merlo (2008) have shown that learning PCFGsand dependency grammars respectively with latentvariables can produce parsers with very good gen-eralization performance.
However, both these ap-proaches rely on EM for parameter estimation andcan benefit from using spectral methods.We propose a simple yet powerful latent vari-able generative model for use with dependency pars-1Actually, instead of using the model by Hsu et al2008)we work with a related model proposed by Foster et al2012)which addresses some of the shortcomings of the earlier modelwhich we detail below.205ing which has one hidden node for each word inthe sentence, like the one shown in Figure 1 andwork out the details for the parameter estimationof the corresponding spectral learning model.
Ata very high level, the parameter estimation of ourmodel involves collecting unigram, bigram and tri-gram counts sensitive to the underlying dependencystructure of the given sentence.Recently, Luque et al2012) have also proposeda spectral method for dependency parsing, howeverthey deal with horizontal markovization and use hid-den states to model sequential dependencies within aword?s sequence of children.
In contrast with that, inthis paper, we propose a spectral learning algorithmwhere latent states are not restricted to HMM-likedistributions of modifier sequences for a particularhead, but instead allow information to be propagatedthrough the entire tree.More recently, Cohen et al2012) have proposeda spectral method for learning PCFGs.Its worth noting that recent work by Parikh et al(2011) also extends Hsu et al2008) to latent vari-able dependency trees like us but under the restric-tive conditions that model parameters are trained fora specified, albeit arbitrary, tree topology.2 In otherwords, all training sentences and test sentences musthave identical tree topologies.
By doing this they al-low for node-specific model parameters, but must re-train the model entirely when a different tree topol-ogy is encountered.
Our model on the other hand al-lows the flexibility and efficiency of processing sen-tences with a variety of tree topologies from a singletraining run.Most of the current state-of-the-art dependencyparsers are discriminative parsers (Koo et al2008;McDonald, 2006) due to the flexibility of represen-tations which can be used as features leading to bet-ter accuracies and the ease of reproducibility of re-sults.
However, unlike discriminative models, gen-erative models can exploit unlabeled data.
Also, asis common in statistical parsing, re-ranking the out-puts of a parser leads to significant reductions in er-ror (Collins and Koo, 2005).Since our spectral learning algorithm uses a gen-2This can be useful in modeling phylogeny trees for in-stance, but precludes most NLP applications, since there is aneed to model the full set of different tree topologies possiblein parsing.h0h1 h2wasKilroy hereFigure 1: Sample dependency parsing tree for ?Kilroywas here?erative model of words given a tree structure, it canscore a tree structure i.e.
its probability of genera-tion.
Thus, it can be used to re-rank the n-best out-puts of a given parser.The remainder of the paper is organized as fol-lows.
In the next section we introduce the notationand give a brief overview of the spectral algorithmfor learning HMMs (Hsu et al2008; Foster et al2012).
In Section 3 we describe our proposed modelfor dependency parsing in detail and work out thetheory behind it.
Section 4 provides experimentalevaluation of our model on Penn Treebank data.
Weconclude with a brief summary and future avenuesfor research.2 Spectral Algorithm For Learning HMMsIn this section we describe the spectral algorithm forlearning HMMs.32.1 NotationThe HMM that we consider in this section is a se-quence of hidden states h ?
{1, .
.
.
, k} that followthe Markov property:p(ht|h1, .
.
.
, ht?1) = p(ht|ht?1)and a sequence of observations x ?
{1, .
.
.
, n} suchthatp(xt|x1, .
.
.
, xt?1, h1, .
.
.
, ht) = p(xt|ht)3As mentioned earlier, we use the model by Foster et al(2012) which is conceptually similar to the one by Hsu et al(2008), but does further dimensionality reduction and thus haslower sample complexity.
Also, critically, the fully reduced di-mension model that we use generalizes much more cleanly totrees.206The parameters of this HMM are:?
A vector pi of length k where pii = p(h1 = i):The probability of the start state in the sequencebeing i.?
A matrix T of size k ?
k whereTi,j = p(ht+1 = i|ht = j): The probability oftransitioning to state i, given that the previousstate was j.?
A matrix O of size n?
k whereOi,j = p(x = i|h = j): The probability ofstate h emitting observation x.Define ?j to be the vector of length n with a 1 inthe jth entry and 0 everywhere else, and diag(v) tobe the matrix with the entries of v on the diagonaland 0 everywhere else.The joint distribution of a sequence of observa-tions x1, .
.
.
, xm and a sequence of hidden statesh1, .
.
.
, hm is:p(x1, .
.
.
,xm, h1, .
.
.
, hm)= pih1m?1?j=2Thj ,hj?1m?j=1Oxj ,hjNow, we can write the marginal probability of asequence of observations asp(x1, .
.
.
xm)=?h1,...,hmp(x1, .
.
.
, xm, h1, .
.
.
, hm)which can be expressed in matrix form4 as:p(x1, .
.
.
, xm) = 1>AxmAxm?1 ?
?
?Am1piwhere Axm ?
Tdiag(O>?xm), and 1 is a k-dimensional vector with every entry equal to 1.A is called an ?observation operator?, and is ef-fectively a third order tensor, and Axm which is amatrix, gives the distribution vector over states attimem+1 as a function of the state distribution vec-tor at the current time m and the current observation?xm .
SinceAxm depends on the hidden state, it is notobservable, and hence cannot be directly estimated.4This is essentially the matrix form of the standard dynamicprogram (forward algorithm) used to estimate HMMs.However, Hsu et al2008) and Foster et al2012)showed that under certain conditions there exists afully observable representation of the observable op-erator model.2.2 Fully observable representationBefore presenting the model, we need to address afew more points.
First, let U be a ?representationmatrix?
(eigenfeature dictionary) which maps eachobservation to a reduced dimension space (n ?
k)that satisfies the conditions:?
U>O is invertible?
|Uij | < 1.Hsu et al2008); Foster et al2012) discuss Uin more detail, but U can, for example, be obtainedby the SVD of the bigram probability matrix (wherePij = p(xt+1 = i|xt = j)) or by doing CCA onneighboring n-grams (Dhillon et al2011).Letting yi = U>?xi , we havep(x1, .
.
.
, xm)= c>?C(ym)C(ym?1) .
.
.
C(y1)c1 (1)wherec1 = ?c?
= ?>?
?1C(y) = K(y)?
?1and ?, ?
and K, described in more detail below, arequantities estimated by frequencies of unigrams, bi-grams, and trigrams in the observed (training) data.Under the assumption that data is generated byan HMM, the distribution p?
obtained by substitutingthe estimated values c?1, c?
?, and C?
(y) into equation(1) converges to p sufficiently fast as the amount oftraining data increases, giving us consistent param-eter estimates.
For details of the convergence proof,please see Hsu et al2008) and Foster et al2012).3 Spectral Algorithm For LearningDependency TreesIn this section, we first describe a simple latent vari-able generative model for dependency parsing.
Wethen define some extra notation and finally present207the details of the corresponding spectral learning al-gorithm for dependency parsing, and prove that ourlearning algorithm provides a consistent estimationof the marginal probabilities.It is worth mentioning that an alternate way of ap-proaching the spectral estimation of latent states fordependency parsing is by converting the dependencytrees into linear sequences from root-to-leaf and do-ing a spectral estimation of latent states using Hsuet al2008).
However, this approach would notgive us the correct probability distribution over treesas the probability calculations for different pathsthrough the trees are not independent.
Thus, al-though one could calculate the probability of a pathfrom the root to a leaf, one cannot generalize fromthis probability to say anything about the neighbor-ing nodes or words.
Put another way, when a par-ent has more than the one descendant, one has to becareful to take into account that the hidden variablesat each child node are all conditioned on the hiddenvariable of the parent.3.1 A latent variable generative model fordependency parsingIn the standard setting, we are given training exam-ples where each training example consists of a se-quence of words x1, .
.
.
, xm together with a depen-dency structure over those words, and we want toestimate the probability of the observed structure.This marginal probability estimates can then be usedto build an actual generative dependency parser or,since the marginal probability is conditioned on thetree structure, it can be used re-rank the outputs of aparser.As in the conventional HMM described in the pre-vious section, we can define a simple latent variablefirst order dependency parsing model by introduc-ing a hidden variable hi for each word xi.
Thejoint probability of a sequence of observed nodesx1, .
.
.
, xm together with hidden nodes h1, .
.
.
, hmcan be written asp(x1, .
.
.
,xm, h1, .
.
.
, hm)= pih1m?j=2td(j)(hj |hpa(j))m?j=1o(xj |hj)(2)h1h2 h3y1y2 y3Figure 2: Dependency parsing tree with observed vari-ables y1, y2, and y3.where pa(j) is the parent of node j and d(j) ?
{L,R} indicates whether hj is a left or a right nodeof hpa(j).
For simplicity, the number of hidden andobserved nodes in our tree are the same, howeverthey are not required to be so.As is the case with the conventional HMM, theparameters used to calculate this joint probabilityare unobservable, but it turns out that under suitableconditions a fully observable model is also possiblefor the dependency tree case with the parameteriza-tion as described below.3.2 Model parametersWe will define both the theoretical representationsof our observable parameters, and the sampling ver-sions of these parameters.
Note that in all the cases,the estimated versions are unbiased estimates of thetheoretical quantities.Define Td and T ud where d ?
{L,R} to be thehidden state transition matrices from parent to leftor right child, and from left or right child to parent(hence the u for ?up?
), respectively.
In other words(referring to Figure 2)TR = t(h3|h1)TL = t(h2|h1)T uR = t(h1|h3)T uL = t(h1|h2)Let Ux(i) be the ith entry of vector U>?x andG =U>O.
Further, recall the notation diag(v), which isa matrix with elements of v on its diagonal, then:?
Define the k-dimensional vector ?
(unigram208counts):?
= Gpi[??
]i =n?u=1c?
(u)Uu(i)where c?
(u) = c(u)N1 , c(u) is the count of ob-servation u in the training sample, and N1 =?u?n c(u).?
Define the k?k matrices ?L and ?R (left child-parent and right child-parent bigram counts):[?
?L]i,j =n?u=1n?v=1c?L(u, v)Uu(j)Uv(i)?L = GTuLdiag(pi)G>[?
?R]i,j =n?u=1n?v=1c?R(u, v)Uu(j)Uv(i)?R = GTuRdiag(pi)G>where c?L(u, v) =cL(u,v)N2L, cL(u, v) is the countof bigram (u, v) where u is the left child andv is the parent in the training sample, andN2L =?
(u,v)?n?n cL(u, v).
Define c?R(u, v)similarly.?
Define k ?
k ?
k tensor K (left child-parent-right child trigram counts):K?i,j,l =n?u=1n?v=1n?w=1c?
(u, v, w)Uw(i)Uu(j)Uv(l)K(y) = GTLdiag(G>y)T uRdiag(pi)G>where c?
(w, u, v) = c(w,u,v)N3 , c(w, u, v) isthe count of bigram (w, u, v) where w isthe left child, u is the parent and v is theright child in the training sample, and N3 =?
(w,u,v)?n?n?n c(w, u, v).?
Define k?k matrices ?L and ?R (skip-bigramcounts (left child-right child) and (right child-left child)) 5:[?
?L]i,j =n?u=1n?v=1n?w=1c?
(u, v, w)Uw(i)Uu(j)?L = GTLTuRdiag(pi)G>[?
?R]i,j =n?u=1n?v=1n?w=1c?
(u, v, w)Uw(j)Uu(i)?R = GTRTuLdiag(pi)G>3.3 Parameter estimationUsing the above definitions, we can estimate the pa-rameters of the model, namely ?,?L,?R,?L,?RandK, from the training data and define observablesuseful for the dependency model as6c1 = ?cT?
= ?T?
?1REL = ?L?
?1RER = ?R?
?1LD(y) = E?1L K(y)?
?1RAs we will see, these quantities allow us to recur-sively compute the marginal probability of the de-pendency tree, p?
(x1, .
.
.
, xm), in a bottom up man-ner by using belief propagation.To see this, let hch(i) be the set of hidden chil-dren of hidden node i (in Figure 2 for instance,hch(1) = {2, 3}) and let och(i) be the set of ob-served children of hidden node i (in the same figureoch(i) = {1}).
Then compute the marginal proba-bility p(x1, .
.
.
, xm) from Equation 2 asri(h) =?j?hch(i)?j(h)?j?och(i)o(xj |h) (3)where ?i(h) is defined by summing over allthe hidden random variables i.e., ?i(h) =?h?
p(h?|h)ri(h?
).This can be written in a compact matrix form as?
?ri> = 1>?j?hch(i)diag(T>dj?
?rj )?
?j?och(i)diag(O>?xj ) (4)5Note than ?R = ?TL , which is not immediately obviousfrom the matrix representations.6The details of the derivation follow directly from the matrixversions of the variables.209where ?
?ri is a vector of size k (the dimensionality ofthe hidden space) of values ri(h).
Note that since inEquation 2 we condition on whether xj is the left orright child of its parent, we have separate transitionmatrices for left and right transitions from a givenhidden node dj ?
{L,R}.The recursive computation can be written in termsof observables as:?
?ri> = c>??j?hch(i)D(E>dj?
?rj )?
?j?och(i)D((U>U)?1U>?xj )The final calculation for the marginal probabilityof a given sequence isp?
(x1, .
.
.
, xm) =?
?r1>c1 (5)The spectral estimation procedure is described be-low in Algorithm 1.Algorithm 1 Spectral dependency parsing (Comput-ing marginal probability of a tree.
)1: Input: Training examples- x(i) for i ?
{1, .
.
.
,M}along with dependency structures where each se-quence x(i) = x(i)1 , .
.
.
, x(i)mi .2: Compute the spectral parameters ?
?, ?
?R, ?
?L, ??R,?
?L, and K?#Now, for a given sentence, we can recursively com-pute the following:3: for x(i)j for j ?
{mi, .
.
.
, 1} do4: Compute:?
?ri> = c>??j?hch(i)D(E>dj?
?rj )?
?j?och(i)D((U>U)?1U>?xj )5: end for6: Finally computep?
(x1, .
.
.
, xmi) =?
?r1>c1#The marginal probability of an entire tree.3.4 Sample complexityOur main theoretical result states that the abovescheme for spectral estimation of marginal proba-bilities provides a guaranteed consistent estimationscheme for the marginal probabilities:Theorem 3.1.
Let the sequence {x1, .
.
.
, xm} begenerated by an k ?
2 state HMM.
Suppose we aregiven a U which has the property that U>O is in-vertible, and |Uij | ?
1.
Suppose we use equation(5) to estimate the probability based on N indepen-dent triples.
ThenN ?
Cmk22log(k?
)(6)where Cm is specified in the appendix, implies that1?
 ?????p?
(x1, .
.
.
, xm)p(x1, .
.
.
, xm)????
?
1 + holds with probability at least 1?
?.Proof.
A sketch of the proof, in the case without di-rectional transition parameters, can be found in theappendix.
The proof with directional transition pa-rameters is almost identical.4 Experimental EvaluationSince our algorithm can score any given tree struc-ture by computing its marginal probability, a natu-ral way to benchmark our parser is to generate n-best dependency trees using some standard parserand then use our algorithm to re-rank the candidatedependency trees, e.g.
using the log spectral prob-ability as described in Algorithm 1 as a feature in adiscriminative re-ranker.4.1 Experimental SetupOur base parser was the discriminatively trainedMSTParser (McDonald, 2006), which implementsboth first and second order parsers and is trainedusing MIRA (Crammer et al2006) and used thestandard baseline features as described in McDon-ald (2006).We tested our methods on the English Penn Tree-bank (Marcus et al1993).
We use the standardsplits of Penn Treebank; i.e., we used sections 2-21for training, section 22 for development and section23 for testing.
We used the PennConverter7 tool toconvert Penn Treebank from constituent to depen-dency format.
Following (McDonald, 2006; Koo7http://nlp.cs.lth.se/software/treebank_converter/210et al2008), we used the POS tagger by Ratnaparkhi(1996) trained on the full training data to providePOS tags for development and test sets and used 10-way jackknifing to generate tags for the training set.As is common practice we stripped our sentences ofall the punctuation.
We evaluated our approach onsentences of all lengths.4.2 Details of spectral learningFor the spectral learning phase, we need to just col-lect word counts from the training data as describedabove, so there are no tunable parameters as such.However, we need to have access to an attribute dic-tionary U which contains a k dimensional represen-tation for each word in the corpus.
A possible wayof generating U as suggested by Hsu et al2008) isby performing SVD on bigrams P21 and using theleft eigenvectors as U .
We instead used the eigen-feature dictionary proposed by Dhillon et al2011)(LR-MVL) which is obtained by performing CCAon neighboring words and has provably better sam-ple complexity for rare words compared to the SVDalternative.We induced the LR-MVL embeddings for wordsusing the Reuters RCV1 corpus which containsabout 63 million tokens in 3.3 million sentences andused their context oblivious embeddings as our esti-mate of U .
We experimented with different choicesof k (the size of the low dimensional projection)on the development set and found k = 10 to workreasonably well and fast.
Using k = 10 we wereable to estimate our spectral learning parameters?,?L,R,?L,R,K from the entire training data in un-der 2 minutes on a 64 bit Intel 2.4 Ghz processor.4.3 Re-ranking the outputs of MST parserWe could not find any previous work which de-scribes features for discriminative re-ranking for de-pendency parsing, which is due to the fact that un-like constituency parsing, the base parsers for depen-dency parsing are discriminative (e.g.
MST parser)which obviates the need for re-ranking as one couldadd a variety of features to the baseline parser itself.However, parse re-ranking is a good testbed for ourspectral dependency parser which can score a giventree.
So, we came up with a baseline set of featuresto use in an averaged perceptron re-ranker (Collins,2002).
Our baseline features comprised of two mainMethod Accuracy CompleteI OrderMST Parser (No RR) 90.8 37.2RR w. Base.
Features 91.3 37.5RR w. Base.
Features +log p?
91.7 37.8II OrderMST Parser (No RR) 91.8 40.6RR w. Base.
Features 92.4 41.0RR w. Base.
Features +log p?
92.7 41.3Table 1: (Unlabeled) Dependency Parse re-ranking re-sults for English test set (Section 23).
Note: 1).
RR =Re-ranking 2).
Accuracy is the number of words whichcorrectly identified their parent and Complete is the num-ber of sentences for which the entire dependency tree wascorrect.
3).
Base.
Features are the two re-ranking fea-tures described in Section 4.3.
4).
log p?
is the spectral logprobability feature.features which capture information that varies acrossthe different n-best parses and moreover were notused as features by the baseline MST parser, ?POS-left-modifier ?
POS-head ?
POS-right-modifier?and ?POS-left/right-modifier ?
POS-head ?
POS-grandparent?8.
In addition to that we used the log ofspectral probability (p?
(x1, .
.
.
, xm) - as calculatedusing Algorithm 1) as a feature.We used the MST parser trained on entire trainingdata to generate a list of n-best parses for the devel-opment and test sets.
The n-best parses for the train-ing set were generated by 3-fold cross validation,where we train on 2 folds to get the parses for thethird fold.
In all our experiments we used n = 50.The results are shown in Table 1.
As can be seen,the best results give up to 4.6% reduction in errorover the re-ranker which uses just the baseline set offeatures.5 Discussion and Future WorkSpectral learning of structured latent variable mod-els in general is a promising direction as has beenshown by the recent interest in this area.
It al-lows us to circumvent the ubiquitous problem of get-ting stuck in local minima when estimating the la-tent variable models via EM.
In this paper we ex-8One might be able to come up with better features for de-pendency parse re-ranking.
Our goal in this paper was just toget a reasonable baseline.211tended the spectral learning ideas to learn a simpleyet powerful dependency parser.
As future work, weare working on building an end-to-end parser whichwould involve coming up with a spectral version ofthe inside-outside algorithm for our setting.
We arealso working on extending it to learn more power-ful grammars e.g.
split head-automata grammars(SHAG) (Eisner and Satta, 1999).6 ConclusionIn this paper we proposed a novel spectral methodfor dependency parsing.
Unlike EM trained gen-erative latent variable models, our method does notget stuck in local optima, it gives consistent param-eter estimates, and it is extremely fast to train.
Weworked out the theory of a simple yet powerful gen-erative model and showed how it can be learned us-ing a spectral method.
As a pilot experimental evalu-ation we showed the efficacy of our approach by us-ing the spectral probabilities output by our model forre-ranking the outputs of MST parser.
Our methodreduced the error of the baseline re-ranker by up toa moderate 4.6%.7 AppendixThis appendix offers a sketch of the proof of The-orem 1.
The proof uses the following definitions,which are slightly modified from those of Fosteret al2012).Definition 1.
Define ?
as the smallest element of ?,?
?1, ?
?1, and K().
In other words,?
?min{mini|?i|,mini,j|?
?1ij |,mini,j|?
?1ij |,mini,j,k|Kijk|,mini,j|?ij |,mini,j|?ij |, }where Kijk = K(?j)ik are the elements of the ten-sor K().Definition 2.
Define ?k as the smallest singularvalue of ?
and ?.The proof relies on the fact that a row vector mul-tiplied by a series of matrices, and finally multipliedby a column vector amounts to a sum over all possi-ble products of individual entries in the vectors andmatrices.
With this in mind, if we bound the largestrelative error of any particular entry in the matrix by,say, ?, and there are, say, s parameters (vectors andmatrices) being multiplied together, then by simplealgebra the total relative error of the sum over theproducts is bounded by ?s.The proof then follows from two basic steps.First, one must bound the maximal relative error, ?for any particular entry in the parameters, which canbe done using central limit-type theorems and thequantity ?
described above.
Then, to calculate theexponent s one simply counts the number of param-eters multiplied together when calculating the prob-ability of a particular sequence of observations.Since each hidden node is associated with exactlyone observed node, it follows that s = 12m + 2L,where L is the number of levels (for instance in ourexample ?Kilroy was here?
there are two levels).
scan be easily computed for arbitrary tree topologies.It follows from Foster et al2012) that we achievea sample complexityN ?128k2s22 ?2?4klog(2k?)??1?
??
?2/s2( s?1 + ?
1)2(7)leading to the theorem stated above.Lastly, note that in reality one does not see ?
and?k but instead estimates of these quantities; Fosteret al2012) shows how to incorporate the accuracyof the estimates into the sample complexity.Acknowledgement: We would like to thankEmily Pitler for valuable feedback on the paper.ReferencesShay Cohen, Karl Stratos, Michael Collins, DeanFoster, and Lyle Ungar.
Spectral learning oflatent-variable pcfgs.
In Association of Compu-tational Linguistics (ACL), volume 50, 2012.Michael Collins.
Ranking algorithms for named-entity extraction: boosting and the voted percep-tron.
In Proceedings of the 40th Annual Meet-ing on Association for Computational Linguis-tics, ACL ?02, pages 489?496, Stroudsburg, PA,USA, 2002.
Association for Computational Lin-guistics.
URL http://dx.doi.org/10.3115/1073083.1073165.Michael Collins and Terry Koo.
Discriminativereranking for natural language parsing.
Comput.212Linguist., 31(1):25?70, March 2005.
ISSN 0891-2017.Koby Crammer, Ofer Dekel, Joseph Keshet, ShaiShalev-Shwartz, and Yoram Singer.
Onlinepassive-aggressive algorithms.
Journal of Ma-chine Learning Research, 7:551?585, 2006.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
Max-imum likelihood from incomplete data via the emalgorithm.
JRSS, SERIES B, 39(1):1?38, 1977.Paramveer S. Dhillon, Dean Foster, and Lyle Un-gar.
Multi-view learning of word embeddings viacca.
In Advances in Neural Information Process-ing Systems (NIPS), volume 24, 2011.Jason Eisner and Giorgio Satta.
Efficient pars-ing for bilexical context-free grammars and head-automaton grammars.
In Proceedings of the 37thAnnual Meeting of the Association for Computa-tional Linguistics (ACL), pages 457?464, Univer-sity of Maryland, June 1999.
URL http://cs.jhu.edu/?jason/papers/#acl99.Dean Foster, Jordan Rodu, and Lyle Ungar.
Spec-tral dimensionality reduction for HMMs.
ArXiVhttp://arxiv.org/abs/1203.6130, 2012.D Hsu, S M. Kakade, and Tong Zhang.
A spec-tral algorithm for learning hidden markov models.arXiv:0811.4413v2, 2008.Terry Koo, Xavier Carreras, and Michael Collins.Simple semi-supervised dependency parsing.
InIn Proc.
ACL/HLT, 2008.F.
Luque, A. Quattoni, B. Balle, and X. Carreras.Spectral learning for non-deterministic depen-dency parsing.
In EACL, 2012.Mitchell P. Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
Building a large annotatedcorpus of english: the penn treebank.
Comput.Linguist., 19:313?330, June 1993.
ISSN 0891-2017.Ryan McDonald.
Discriminative learning and span-ning tree algorithms for dependency parsing.
PhDthesis, University of Pennsylvania, Philadelphia,PA, USA, 2006.
AAI3225503.Bernard Merialdo.
Tagging english text with a prob-abilistic model.
Comput.
Linguist., 20:155?171,June 1994.
ISSN 0891-2017.Gabriele Antonio Musillo and Paola Merlo.
Un-lexicalised hidden variable models of split de-pendency grammars.
In Proceedings of the 46thAnnual Meeting of the Association for Computa-tional Linguistics on Human Language Technolo-gies: Short Papers, HLT-Short ?08, pages 213?216, Stroudsburg, PA, USA, 2008.
Associationfor Computational Linguistics.Ankur P. Parikh, Le Song, and Eric P. Xing.
A spec-tral algorithm for latent tree graphical models.
InICML, pages 1065?1072, 2011.Slav Petrov, Leon Barrett, Romain Thibaux, andDan Klein.
Learning accurate, compact, and in-terpretable tree annotation.
In Proceedings of the21st International Conference on ComputationalLinguistics and the 44th annual meeting of the As-sociation for Computational Linguistics, ACL-44,pages 433?440, Stroudsburg, PA, USA, 2006.
As-sociation for Computational Linguistics.Ariadna Quattoni, Michael Collins, and Trevor Dar-rell.
Conditional random fields for object recog-nition.
In In NIPS, pages 1097?1104.
MIT Press,2004.Lawrence R. Rabiner.
A tutorial on hidden markovmodels and selected applications in speech recog-nition.
In Proceedings of the IEEE, pages 257?286, 1989.Adwait Ratnaparkhi.
A Maximum Entropy Modelfor Part-Of-Speech Tagging.
In Eric Brill andKenneth Church, editors, Proceedings of the Em-pirical Methods in Natural Language Processing,pages 133?142, 1996.Sebastiaan Terwijn.
On the learnability of hiddenmarkov models.
In Proceedings of the 6th Inter-national Colloquium on Grammatical Inference:Algorithms and Applications, ICGI ?02, pages261?268, London, UK, UK, 2002.
Springer-Verlag.
ISBN 3-540-44239-1.213
