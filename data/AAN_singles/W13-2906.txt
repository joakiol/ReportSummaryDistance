Proceedings of the 2nd Workshop on Predicting and Improving Text Readability for Target Reader Populations, pages 49?58,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsModeling Comma Placement in Chinese Text for Better Readability usingLinguistic Features and Gaze InformationTadayoshi Hara1 Chen Chen2?
Yoshinobu Kano3,1 Akiko Aizawa11National Institute of Informatics, Japan 2The University of Tokyo, Japan3PRESTO, Japan Science and Technology Agency{harasan, kano, aizawa}@nii.ac.jpAbstractComma placements in Chinese text arerelatively arbitrary although there aresome syntactic guidelines for them.
In thisresearch, we attempt to improve the read-ability of text by optimizing comma place-ments through integration of linguistic fea-tures of text and gaze features of readers.We design a comma predictor for gen-eral Chinese text based on conditional ran-dom field models with linguistic features.After that, we build a rule-based filter forcategorizing commas in text according totheir contribution to readability based onthe analysis of gazes of people reading textwith and without commas.The experimental results show that ourpredictor reproduces the comma distribu-tion in the Penn Chinese Treebank with78.41 in F1-score and commas chosen byour filter smoothen certain gaze behaviors.1 IntroductionChinese is an ideographic language, with no natu-ral apparent word boundaries, little morphology,and no case markers.
Moreover, most Chinesesentences are quite long.
These features make itespecially difficult for Chinese learners to identifycomposition of a word or a clause in a sentence.Punctuation marks, especially commas, are al-lowed to be placed relatively arbitrarily to serve asimportant segmentation cues (Yue, 2006) for pro-viding syntactic and prosodic boundaries in text;commas indicate not only phrase or clause bound-aries but also sentence segmentations, and theycapture some of the major aspects of a writer?sprosodic intent (Chafe, 1988).
The combinationof both aspects promotes cognition when readingtext (Ren and Yang, 2010; Walker et al 2001).
?The Japan Research Institute, Ltd. (from April, 2013)Linguistic FeaturesCRF modelCRF model-based Comma PredictorGaze FeaturesHuman AnnotationRule-based Comma Filter Text with/without CommasParse TreeTreebankComma Distribution for ReadabilityComma Distribution in General TextInput (Comma-less) Text++Figure 1: Our approachHowever, although there are guidelines and re-search on the syntactic aspects of comma place-ment, prosodic aspects have not been explored,since they are more related with cognition.
It isas yet unclear how comma placement should beoptimized for reading, and it has thus far been upto the writer (Huang and Chen, 2011).In this research, we attempt to optimize commaplacements by integrating the linguistic features oftext and the gaze features of readers.
Figure 1 il-lustrates our approach.
First, we design a commapredictor for general Chinese text based on con-ditional random field (CRF) models with variouslinguistic features.
Second, we build a rule-basedfilter for classifying commas in text into ones fa-cilitating or obstructing readability, by comparingthe gaze features of persons reading text with andwithout commas.
These two steps are connectedby applying our rule-based filter to commas pre-dicted by our comma predictor.
The experimentalresults for each step validate our approach.Related work is described in Section 2.
Thefunctions of Chinese commas are described inSection 3.
Our CRF model-based comma predic-tor is examined in Section 4, and our rule-basedcomma filter is constructed and examined in Sec-tion 5 and 6.
Section 7 contains a summary andoutlines future directions of this research.49[Case 1] When a pause between a subject and a predicate is needed.
(?
(,) means the original or comparative position of the comma in Chinese text.)e.g.
????????????????????????
(The stars we can see (,)?
are mostly fixed stars that are far away from the earth.
)[Case 2] When a pause between an inner predicate and an object of a sentence is needed.e.g.
?????????????????????
(We should see that (,) science needs a person to devote all his/her life to it.
)[Case 3] When a pause after an inner (adverbial, prepositional, etc.)
modifier of a sentence is needed.e.g.
?????????????
(He is no stranger (,) to this city.)
(The order of the modifier and the main clause is opposite in the English translation.
)[Case 4] When a pause between clauses in a complex sentence is needed, besides the use of semicolon (?).e.g.
??????????????????????
(It is said that there are more than 100 Suzhou traditional gardens, (,) no more than 10 of which Ihave been to.
)[Case 5] When a pause between phrases of the same syntactic type is needed.e.g.
???????????????
(The students prefer young (,) and energetic teachers.
)Table 1: Five main usages of commas in Chinese text(a) Screenshot of a materialDisplay PC Monitor SubjectEye TrackerHost PC Monitor(b) Scene of the experiment (c) Window around a gaze pointFigure 3: Settings for eye-tracking experimentsWS Word surfacePOS POS tagDIP Depth of a word in the parse treeSTAG Syntactic tagOIC Order of the clause in a sentence that a word belongs toWL Word lengthLOD Length of fragment with specific depth in a parsing treeTable 2: Features used in our CRF model2 Related WorkPrevious work on Chinese punctuation predictionmostly focuses on sentence segmentation in au-tomatic speech recognition (Shriberg et al 2000;Huang and Zweig, 2002; Peitz et al 2011).Jin et al(2002) classified commas for sentencesegmentation and succeeded in improving pars-ing performance.
Lu and Ng (2010) proposedan approach built on a dynamic CRF for predict-ing punctuations, sentence boundaries, and sen-tence types of speech utterances without prosodiccues.
Zhang et al(2006) suggested that a cascadeCRF-based approach can deal with ancient Chi-nese prose punctuation better than a single CRF.Guo et al(2010) implemented a three-tier max-imum entropy model incorporating linguisticallymotivated features for generating commonly usedChinese punctuation marks in unpunctuated sen-tences output by a surface realizer.
(a)WS|POS|STAG|DIP|OIC|WL|LOD|IOB-tag(b)Figure 2: Example of a parse tree (a) and its cor-responding training data (b) with the features3 Functions of Chinese CommasThere are five main uses of commas in Chinesetext, as shown in Table 1.
Cases 1 to 4 are fromZDIC.NET (2005), and Case 5 obviously exists inChinese text.
The first three serve the function ofemphasis, while the latter two indicate coordinat-ing or subordinating clauses or phrases.In Cases 1 and 2, a comma is inserted as akind of pause between a short subject and a longpredicate, or between a short remainder predicate,such as??
(see/know),??/??
(indicate),?50Feature F1 (P/R) AWS 59.32 (72.67/50.12) 95.45POS 32.51 (69.06/21.26) 94.08DIP 34.14 (68.65/22.72) 94.13STAG 22.44 (64.00/13.60) 93.67OIC 9.27 (66.56/ 4.98) 93.42WL 10.70 (75.24/ 5.76) 93.52LOD 35.32 (59.20/25.17) 93.81WS+POS 63.75 (79.93/53.01) 96.03WS +DIP 70.06 (83.27/60.47) 96.61WS +STAG 57.42 (81.94/44.19) 95.67WS +OIC 60.35 (77.98/49.22) 95.73WS +WL 60.90 (76.39/50.63) 95.71WS +LOD 70.85 (78.87/64.31) 96.53WS+POS+DIP 73.41 (84.62/64.82) 96.93WS+POS+DIP+STAG 74.58 (83.66/67.27) 97.01WS+POS+DIP +OIC 76.87 (84.29/70.65) 97.23WS+POS+DIP +WL 70.18 (83.33/60.62) 96.63WS+POS+DIP +LOD 76.61 (82.61/71.43) 97.16WS+POS+DIP+STAG+OIC 76.62 (84.48/70.09) 97.21WS+POS+DIP+STAG +WL 74.12 (84.00/66.33) 96.98WS+POS+DIP+STAG +LOD 77.64 (85.11/71.38) 97.33WS+POS+DIP +OIC+WL 75.43 (84.76/67.95) 97.11WS+POS+DIP +OIC +LOD 78.23 (84.23/73.03) 97.36WS+POS+DIP +WL+LOD 74.01 (85.80/65.06) 97.02WS+POS+DIP+STAG+OIC+WL 77.25 (83.97/71.53) 97.26WS+POS+DIP+STAG+OIC +LOD 77.31 (86.36/69.97) 97.33WS+POS+DIP+STAG +WL+LOD 76.55 (85.24/69.46) 97.23WS+POS+DIP +OIC+WL+LOD 77.60 (84.30/71.89) 97.30WS+POS+DIP+STAG+OIC+WL+LOD 78.41 (83.97/73.54) 97.36F1: F1-Score, P: precision (%), R: recall (%), A: accuracy (%)Table 3: Performance of the comma predictor(A) #Characters,Article (B) #Punctuations, (C) / (A) (C) / (B) SubjectsID (C) #Commas6 692 49 28 4.04% 57.14% L, T, C7 335 30 15 4.48% 50.00% L, T, C10 346 18 7 2.02% 38.89% L, T, C, Z12 221 18 7 3.17% 38.89% L, T, C14 572 33 14 2.45% 42.42% L, T, C18 471 36 13 2.76% 36.11% C, Z79 655 53 28 4.27% 52.83% Z82 471 30 13 2.76% 43.33% Z121 629 41 19 3.02% 46.34% Z294 608 50 24 3.95% 48.00% Z401 567 43 21 3.70% 48.84% L, T, C406 558 39 18 3.23% 46.15% Z413 552 52 22 3.99% 42.31% T, C, Z423 580 49 26 4.48% 53.06% L, C, Z438 674 46 28 4.15% 60.87% ZAverage 528.73 39.13 18.87 3.57% 48.22% -Table 4: Materials assigned to each subject?
(find) etc., and following long clause-style ob-jects.
English commas, on the other hand, sel-dom have such usages (Zeng, 2006).
In Cases 3and 4, commas instead of conjunctions sometimesconnect two clauses in a relation of either coordi-nation or subordination.
English commas, on theother hand, are only required between independentclauses connected by conjunctions (Zeng, 2006).Liu et al(2010) proved that Chinese commascan change the syntactic structures of sentencesby playing lexical or syntactic roles.
Ren andYang (2010) claimed that inserting commas asclause boundaries shortens the fixation time inpost-comma regions.
Meanwhile, in computa-tional linguistics, Xue and Yang (2011) showedFigure 4: Obtained eye-movement trace map0100,000200,000L1 L2 L3 L4 L5 L6 L7 T1 T2 T3 T4 T5 T6 T8 C1 C2 C3 C4 C5 C6 C7 C8 Z1 Z2 Z3 Z4 Z5 Z6 Z7 Z8 Z9 Z10Trials (?Subject?
+ ?Trial No.?
)With Comma No CommaL1 ?
L7 T1 ?
T7 1 ?
8 Z1 ?
Z10ith commas Without commasTotal viewingtime(sec.
)01 02 0Figure 5: Total viewing timethat Chinese sentence segmentation can be viewedas detecting loosely coordinated clauses separatedby commas.4 CRF Model-based Comma PredictorWe first predict comma placements in existingtext.
The prediction is formalized as a task to an-notate each word in a word sequence with an IOB-style tag such as I-Comma (following a comma),B-Comma (preceding a comma) or O (neither I-Comma nor B-Comma).
We utilize a CRF modelfor this sequential labeling (Lafferty et al 2001).4.1 CRF Model for Comma PredictionA conditional probability assigned to a label se-quence Y for a particular sequence of words X ina first-order linear-chain CRF is given by:P?
(Y |X) =exp(?nw?ki ?ifi(Yw?1, Yw, X,w))Z0(X)where w is a word position in X , fi is a binaryfunction describing a feature for Yw?1, Yw, X , andw, ?i is a weight for that feature, and Z0 is a nor-malization factor over all possible label sequences.The weight ?i for each fi is learned on trainingdata.
For fi, the linguistic features shown in Ta-ble 2 are derived from a syntactic parse of a sen-tence1.
The first three were used initially; the restwere added after we got feedback from construc-tion of our rule-based filters (see Section 5).
Fig-ure 2 shows an example of a parsing tree and itscorresponding training data.1Some other features or tag formats which worked well inthe previous research, such as bi-/tri-gram, a preceding word(L-1) or its POS (POS-1), and IO-style tag (Leaman and Gon-zalez, 2008) were also examined, but they did not work thatwell, probably because of the difference in task settings.5101,0002,000L1 L2 L3 L4 L5 L6 L7 T1 T2 T3 T4 T5 T6 T8 C1 C2 C3 C4 C5 C6 C7 C8 Z1 Z2 Z3 Z4 Z5 Z6 Z7 Z8 Z9 Z10Trials (?Subject?
+ ?Trial No.?
)With Comma No CommaL1 ?
L7 T1 ?
T7 1 ?
8 Z1 ?
Z10Fixationtime/comma(sec.
)0.01.02.0 ith commas Without commasFigure 6: Fixation time per comma0.01.02.03.0L1 L2 L3 L4 L5 L6 L7 T1 T2 T3 T4 T5 T6 T8 C1 C2 C3 C4 C5 C6 C7 C8 Z1 Z2 Z3 Z4 Z5 Z6 Z7 Z8 Z9 Z10Trials (?Subject?
+ ?Trial No.?
)With Comma No CommaL1 ?
L7 T1 ?
T7 1 ?
8 Z1 ?
Z10#regressions /comma 0ith co mas Without commas123Figure 7: Number of regressions per comma4.2 Experimental SettingsThe Penn Chinese Treebank (CTB) 7.0 (Nai-wen Xue and Palmer, 2005) consists of 2,448articles in five genres.
It contains 1,196,329words, and all sentences are annotated with parsetrees.
We selected four genres for written Chi-nese (newswire, news magazine, broadcast newsand newsgroups/weblogs) from this corpus as ourdataset.
These were randomly divided into train-ing (90%) and test data (10%).
We also correctederrors in tagging and inconsistencies in the dataset,mainly by solving problems around strange char-acters tagged as PU (punctuation).
The commasand characters after this preprocessing numbered63,571 and 1,533,928 in the training data and4,116 and 111,172 in the test data.MALLET (McCallum, 2002) and its applica-tion ABNER (Settles, 2005) were used to train theCRF model.
We evaluated the results in termsof precision (P = tp/(tp + fp)), recall (R =tp/(tp+fn)), F1-score (F1 = 2PR/(P+R)), andaccuracy (A = (tp + tn)/(tp + tn + fp + fn)),where tp, tn, fp and fn are respectively the num-ber of true positives, true negatives, false positivesand false negatives, based on whether the modeland the corpus provided commas at each location.4.3 Performance of the CRF ModelTable 3 shows the performance of our CRFmodel2.
We can see that WS contributed muchmore to the performance than other features, prob-ably because a word surface itself has a lot ofinformation on both prosodic and syntactic func-tions.
Combining WS with other features greatlyimproved performance, and as a result, with all2Precision, recall, F1-score, and accuracy with WS + POS+ DIP + L-1 + POS-1 were 82.96%, 65.04%, 72.91 and96.84%, respectively (lower than those with WS+POS+DIP).4080120160L1 L2 L3 L4 L5 L6 L7 T1 T2 T3 T4 T5 T6 T8 C1 C2 C3 C4 C5 C6 C7 C8 Z1 Z2 Z3 Z4 Z5 Z6 Z7 Z8 Z9 Z10Trials (?Subject?
+ ?Trial No.?
)With Comma No CommaL1 ?
L7 T1 ?
T7 1 ?
8 Z1 ?
Z10Saccade length(1) /commaith co mas Without commas4080120160(pixel)Figure 8: Saccade length (1) per comma306090L1 L2 L3 L4 L5 L6 L7 T1 T2 T3 T4 T5 T6 T8 C1 C2 C3 C4 C5 C6 C7 C8 Z1 Z2 Z3 Z4 Z5 Z6 Z7 Z8 Z9 Z10Trials (?Subject?
+ ?Trial No.?
)With Comma No Commaith co mas Without commasL1 ?
L7 T1 ?
T7 1 ?
8 Z1 ?
Z1030Saccade length(2) /comma 6090(pixel)Figure 9: Saccade length (2) per commafeatures (WS + POS + STAG + DIP + OIC + LOD+ WL), precision, recall, F1-score and accuracywere 83.97%, 73.54%, 78.41 and 97.36%.We also found that a large number of false pos-itives seemed helpful according to native speakers(see the description of the subjects in Section 5 and6).
Although these commas do not appear in theCTB text, they might smoothen the reading expe-rience.
We constructed a rule-based filter in orderto pick out such commas.5 Rule-based Comma FilterWe constructed a rule-based comma filter for clas-sifying commas in text into ones facilitating (pos-itive) or obstructing (negative) the reading processas follows:[Step 1]: Collect gaze data from persons readingtext with or without commas (Section 5.1).
[Step 2]: Compare gaze features around commasto find those features that reflect the effect ofcomma placement.
(Section 5.2).
[Step 3]: Annotate commas with categories basedon the obtained features (Section 5.3), and deviserules to explain the annotation (Section 5.4).5.1 Collecting Human Eye-movement DataEye-movements during reading contain rich infor-mation on how the document is being read, whatthe reader is interested in, where difficulties hap-pen, etc.
The movements are characterized by fix-ations (short periods of steadiness), saccades (fastmovements), and regressions (backward saccades)(Rayner, 1998).
In order to analyze the effect ofcommas on reading through the features, we col-lected gaze data from subjects reading text in thefollowing settings.
[Subjects and Materials] Four native Man-52Categories Effect on readability Outward manifestationPositive (?)
Can improve readability.
Presence would cause GF+.Semi-positive (?)
Might be necessary for readability, but the importance is not as obvious as a positive comma.
Absence might cause GF-.Semi-negative (2) Might be negative, but its severity is not as obvious as a negative comma.
Absence might cause GF+.Negative (?)
Thought to reduce a document?s readability.
Presence would cause GF-.GF+/GF-: values of eye-tracking features that represent good/poor readabilityTable 5: Comma categoriesSubject Positive (?)
Semi-positive (?)
Semi-negative (2) Negative (?)
Adjustment formulaL ?FT?>800 500<?FT?
?800 -100<?FT?
?500 ?FT?<-100 ?FT?
= ?FT ?
?RT ?
200C ?FT?>900 600<?FT?
?900 -200<?FT?
?600 ?FT?<-200 ?FT?
= ?FT ?
?RT ?
275T ?FT?>600 300<?FT?
?600 -300<?FT?
?300 ?FT?<-300 ?FT?
= ?FT ?
?RT ?
250Z ?FT?>650 350<?FT?
?650 -250<?FT?
?350 ?FT?<-250 ?FT?
= ?FT ?
?RT ?
250?FT = [ fixation time (without commas) [ms]]?
[ fixation time (with commas) [ms]]?RT = [ #regressions (without commas) ]?
[ #regressions (with commas) ]Table 6: Estimation formula for judging the contribution of commas to readabilityID ?
?
2 ?6 13 6 4 57 8 6 1 010 5 0 1 112 1 4 2 014 4 4 5 118 5 1 4 379 11 4 9 482 5 6 2 0ID ?
?
2 ?121 11 2 6 0294 9 9 4 1401 10 7 2 2406 5 6 5 2413 8 5 6 3423 11 4 7 4438 6 16 6 0Total 112 80 64 26Table 7: Categories of annotated commasdarin Chinese speakers (graduate students and re-searchers) read 15 newswire articles selected fromCTB 7.0 (included in the test data in Section 4.2).Table 4 and Figure 3(a) show the materials as-signed to each subject and a screenshot of one ma-terial.
Each article was presented in 12-15 pointsof bold-faced Fang-Song font occupying 13?13,14?15, 15?16 or 16?16 pixels along with a linespacing of 5-10 pixels3.
[Apparatus] Figure 3(b) shows a scene of theexperiment.
An EyeLink 1000 eye tracker (SRResearch Ltd., Toronto, Canada) with a desktopmount monitored the movements of a right eye at1,000 Hz.
The subject?s head was supported at thechin and forehead.
The distance between the eyesand the monitor was around 55 cm, and each Chi-nese character subtended a visual angle 1?.
Textwas presented on a 19?
monitor at a resolutionof 800?600 pixels, with the brightness adjustedto a comfortable level.
The displayed article wasmasked except for the area around a gaze point(see Figure 3(c)) in order to confirm that the gazepoint was correctly detected and make the subjectconcentrate on the area (adjusted for him/her).
[Procedure] Each article was presented twice(once with/once without commas) to each subject.3These values, as well as the screen position of the article,were adjusted for each subject.The one without commas was presented first4 (notnecessarily in a row).
We did not give any compre-hension test after reading; we just asked the sub-jects to read carefully and silently at their normalor lower speed, in order to minimize the effect ofthe first reading on the second.
The subjects wereinformed of the presence or absence of commasbeforehand.
The apparatus was calibrated beforethe experiment and between trials.
The experi-ment lasted around two hours for each subject.
[Alignment of eye-tracking data to text] Figure 4shows an example of the obtained eye-movementtrace map, where circles and lines respectivelymean fixation points and saccades, and color depthshows their duration.
The alignment of the data tothe text is a critical task, and although automaticapproaches have been proposed (Mart?
?nez-Go?mezet al 2012a; Mart?
?nez-Go?mez et al 2012b), theydo not seem robust enough for our purpose.
Ac-cordingly, we here just compared the entire layoutof the gaze point distribution and that of the actualtext, and adjusted them to have relatively coherentpositions on the x-axis; i.e., the beginning and endof the gaze point sequence in a line were made asclose as possible to those of the line in the text.5.2 Analysis of Eye-movement DataThe gaze data were analyzed by focusing on re-gions around each comma or where each oneshould be (three characters left and right to thecomma5).4If we had used the reversed order, the subject would haveknowledge about original comma distribution, and this wouldcause abnormally quick reading of the text without commas.With the order we set, conflicts between false segmentations(made in first reading) and correct ones might bother the sub-ject, which is trade-off (though minor) in the second reading.5When a comma appeared at the beginning of a line, twocharacters to the left and right of the comma and one charac-531.
If L Seg and R Seg are both very long, a comma must be put between them.2.
If two ?
appear serially, one is necessary whereas the other might be optional or judged negative, but it still depends on the lengths of the siblings.3.
If two neighboring commas appear very close to each other, one of them is judged as negative whereas judgment on the other one is reserved.4.
If several (more than 2) ?s appear continually, one or more ?s might be reserved in consideration of the global condition.5.
A comma is always needed after a long sentence or clause without any syntactically significant punctuation with the function of segmentation.6.
If a ?
appears near a ?, it might be judged as negative with a high probability.
However, the judgment process is always from the bottom up, whichmeans ?
?
2?
???.
For example, if a 2 appears near a ?, we judge 2 first (to be positive or negative), then judge the ?
in the conditionwith or without the comma of 2.Table 8: General rules for referenceFigure 5, 6 and 7 respectively show the totalviewing time, fixation time (duration for all fix-ations and saccades in a target region) per comma,and number of regressions per comma6 for eachtrial.
We can see a general trend wherein the for-mer two were shorter and the latter was smaller forthe articles with commas than without.
The diver-sity of the subjects was also observed in Figure 6.Figure 8 and 9 show the saccade length percomma for different measures.
The former (lat-ter) figure considers a saccade in which at leastone edge (both edges) was in the region.
We can-not see any global trend, probably because of thedifference in global layout of materials brought bythe presence or absence of commas.5.3 Categorization of CommasUsing the features shown to be effective to repre-sent the effect of comma placement, we analyzedthe statistics for each comma in order to manu-ally construct an estimation formula for judgingthe contribution of each comma to readability.
Thecontribution was classified into four categories(Table 5), and the formula is described in Table 67.The adjustment formula was based on our obser-vation that the number of regressions could onlybe regarded as an aid.
For example, for subjectC, if ?FT=200ms and ?RT =?2, ?FT?=?350,and therefore, the comma is annotated as negative.All parameters were decided empirically and man-ually checked twice (self-judgment and feedbackfrom the subjects).On the basis of this estimation formula, all arti-cles in Table 4 were manually annotated.
Table 7shows the distribution of the assigned categories8.ter to the left and right of the final character of the last linewere analyzed.6Calculated by counting the instances where the x-position of [a fixation / end point of a saccade ] was aheadof [the former fixation / its start point].
Although the countsof these two types were almost the same, by counting both ofthem, we expected to cover any possible regression.7One or two features are used to judge the category of acomma.
We will explore more features in the future.8In the case of severe contradictions, the annotators dis-cussed them and resolved them by voting.5.4 Implementation of Rule-based FilterThe annotated commas were classified into Cases1 to 5 in Table 1, based on the types of left andright segment conjuncts (L Seg and R Seg, whichwere obtained from the parse trees in CTB).
Foreach of the five cases, the reason for the assign-ment of a category (?, ?, 2 or ?)
to eachcomma was explained by a manually constructedrule which utilized information about L Seg andR Seg.
The rules were constructed so that theywould cover as many instances as possible.
Ta-ble 8 shows the general rules utilized as a refer-ence, and Table 9 shows the finally obtained rules.The rightmost column in this table shows the num-ber of commas matching each rule.
These ruleswere then implemented as a filter for classifyingcommas in a given text.For several rules (?10, 28, 210, 211 and212), there were only single instances.
In addi-tion, although our rules were built carefully, a fewexceptions to the detailed threshold were found.Collecting and investigating more gaze data wouldhelp to make our rules more sophisticated.6 Performance of the Rule-based FilterWe assumed that our comma predictor provides aCTB text with the same distribution as the origi-nal one in CTB (see Figure 1).
Accordingly, weexamined the quality of the comma categorizationby our rule-based filter through gaze experiments.6.1 Experimental SettingsAnother five native Mandarin Chinese speakerswere invited as test subjects.
The CTB articles as-signed to the subjects are listed in Table 10.
Thesearticles were selected from the test data in Sec-tion 4.2 in such a way that 520<#characters<700,#commas>17, #commas/#punctuations>38%,and #commas/#characters>3.1%, since weneeded articles of appropriate length with a fairnumber of commas.
After that, we manuallychose articles that seemed to attract the subjects?interest from those that satisfied the conditions.54Case 1: L Subject + R Predicate #commas?6 L IP-SBJ + R VP (length both<14 (In Seg Len)) 2?7 L IP-SBJ/NP-SBJ (Org Len>13, Ttl Len>15) 7?6 L NP-SBJ/IP-SBJ (<14) + R VP (?25) 2Case 2: L Predicate + R Object #commas?9 Long frontings (Modifier/Subject, >7) + short L predicate (VV/VRD/VSB?
?
?
, ?3) + Longer R object (IP-OBJ, >28) 6?8 Short frontings (<5) + short L predicate (<3) + moderate-length R object (IP-SBJ, <20) 426 Short frontings (<6) + short L predicate (?3) + long R object (IP-SBJ, >23) 9Case 3: L Modifier #commas?3 Short frequently used L modifier (2-3,??,?
?, etc.)
+ moderate-length/long R SPO (?w18p10) 13?7 Short L (PP/LCP)-TMP (5, 6) + long R NP (?10) 4?10 Long L CP-CND (e.g.,?
?, >18) + moderate-length R Seg (SPO, IP, etc.
<18) 1?1 Long L modifier (PP(-XXX, P+Long NP/IP), IP-ADV, ?17) 6?4 Moderate-length/short L modifier (PP(-XXX, P+IP, There is IP inside, >6<15, cf.
26 (NP)) 9?9 Long L (PP/LCP)-TMP (Ttl Len?10), short R Seg (NP/ADVP, <3) 4?10 Short L (LCP/PP)-LOC (<8) 222 Long L LOC (or there is LCP inside PP, >10) 523 Very short frequently used L ADVP/ADV (2) 825 Short L (PP/LCP/NP)-TMP (4;5-6, when R Seg is short (<10)) 1224 Moderate-length PP(-XXX, P+NP, >8 ?13) + R Seg (SPO, IP, VO, MSPO, etc.)
628 Short L IP-CND (<8) 1211 Long L PP-DIR (>20) + short R VO (?10) 1?2 Very short L (QP/NP/LCP)-TMP (?3) 8?5 Short frequently used L modifier (as in ?3, ?3) + short/moderate-length R Seg (SPO etc., <c20w9) 1Case 4: L c + R c #commas?2 L c & R c are both long (In Seg Len?15; or one>13, the other near 20) 39?8 L c is the summary of R c 2?2 Moderate-length L c + R c (both ?10?15; or one?17, the other?12) 25?3 Moderate-length clause (>10), but connected with familiar CC or ADVP 6?5 Three or more consecutive moderate-length clauses (all<15, and at least one ?10) 12?7 Very short L c + R c (both <5), something like slogan) 1Case 5: L p + R p #commas?1 Short coordinate modifiers (Both side <5) 4?4 Short L p+R p (both<c15w5, and at least one <10), but pre-L p (e.g., SBJ) is too long (>18) 2?5 Between two moderate-length/long phrases (both ?15; or L p?17, R p=10-14; Or L p=10-14, R p>20) 39?11 Long pre-L p (SBJ /ADV, etc.
>16) + short L p (?5) + long R p (?18) 2(?3 Moderate-length phrase (>10), but connected with familiar CC or ADVP) (6)?6 Three or more consecutive short/moderate-length phrases (both<15, at least one<8) 521 Between short phrases (both ?c13w5), and pre-L p (SBJ/ADV, etc.)
is short/moderate-length (<11) 1327 Coordinate VPs, and L VP is a moderate-length VP (PP-MNR VP) 429 Phrasal coordination between a long (?18) and a short (<10) phrase 3210 Moderate-length coordinate VPs (>10<15), and R VP has the structure like VP (MSP VP) 1212 Between two short/moderate-length NP phrases (both ?15, e.g., L NP-TPC+R NP-SBJ) 1?1 Moderate-length/short phrase ((i) c:one>10<18, The other >5?10, w:one?5, the other>5?10; (ii) c:both?10<15, 13w:both>5?7), and pre-L p (SBJ/ADV, etc.)
is short (?5)?
L x/R x: the left/right segment of a target comma which is x.
(x can be ?p?
(phrase) / ?c?
(clause), syntactic tags (with function tags) such as ?VP?
and ?IP-SBJ?, or general functions such as ?Subject?
and ?Predicate?.)?
Org Len: the number of characters in a segment (including other commas or punctuation inside).?
In Seg Len/Ttl Len: the number of characters between the comma and nearest punctuation (inside a long/outside a short target segment).?
SPO: subject + predicate + object, belonging to the outermost sentence.
The length is defined in the similar way as In Seg Len.?
MSPO: modifier + subject + predicate + object.
The length is defined in the similar way as In Seg Len.?
-XX or -XXX: arbitrary type of possible functional tag (or without any functional tag) connected with the former syntactic tag.?
?ciwj: #characters?i and #words?j.?
In some cases (in Case 3, 4 and 5), the length is calculated after negative (or judged negative) commas are eliminated.?
The rules related with TMP are applied faster than ones related with LCP (in Case 3).?
?3 appears in both Case 4 (clause) and Case 5 (phrase).
The number of commas is given by the sum of those in both cases.Table 9: Entire classification of rules based on traditional comma categories(A) #Characters,Article (B) #Punctuations, (C) / (A) (C) / (B) SubjectsID (C) #Commas6 692 49 28 4.04% 57.14% L, S, H11 672 48 21 3.13% 43.75% L, S, F15 674 67 26 3.86% 38.81% L, S, H16 547 43 22 4.02% 51.16% L, S, F56 524 43 18 3.44% 41.86% L, H, M73 595 46 28 4.71% 60.87% S, H, F, M79 655 53 28 4.27% 52.83% H, F, M99 671 55 24 3.58% 43.64% F, MAverage 628.75 50.50 24.38 3.88% 48.27% -Table 10: Materials assigned to each subjectOur rule-based filter was applied to the commasof each article9, and the commas were classified9Instances of incoherence among the applied rules were040,00080,000120,000F79 F11 F16 F73 F99 H73 H06 H15 H79 H56 L06 L11 L15 L16 L56 M99 M79 M73 M56 S06 S11 S15 S16 S73Trials (?Subject?
+ ?Article ID?
)Distribution(G) Distribution(B)Positive distribution Negative distribution0408012Total viewingtime (sec.
)Figure 10: Total viewing time for two distributionsinto two distributions: a positive one (positive +semi-positive commas) and a negative one (nega-tive + semi-negative commas).
Two types of ma-terials were thus generated by leaving the commasin one distribution and removing the others.manually checked and corrected.5520406080F79 F11 F16 F73 F99 H73 H06 H15 H79 H56 L06 L11 L15 L16 L56 M99 M79 M73 M56 S06 S11 S15 S16 S73Trials (?Subject?
+ ?Article ID?
)Distribution(G) Distribution(B)EMFFT(100) Positive distribution Negative distributionFigure 11: EMFFT for two distributions46810F79 F11 F16 F73 F99 H73 H06 H15 H79 H56 L06 L11 L15 L16 L56 M99 M79 M73 M56 S06 S11 S15 S16 S73Trials (?Subject?
+ ?Article ID?
)Distribution(G) Distribution(B)EMFT(800) Positive distribution Negative distributionFigure 12: EMFT for two distributionsThe apparatus and procedure were almost thesame as those in Section 5.1, whereas, on the ba-sis of the feedback from the previous experiments,the font size, number of characters in a line, andline spacing were fixed to single optimized values,respectively, 14-point Fang-Song font occupying15?16 pixels, 33 characters and 7 pixels.6.2 Evaluation MetricsWe examined whether our positive/negative distri-butions really facilitated/obstructed the subjects?reading process by using the following metrics:TT, EMFFT = FFTFT10, EMFT = FTCN?TT11,EMRT = RT2?CN12, EMSLO = SLO2?TT ,where TT, FT, RT and CN are total viewing time,fixation time, number of regressions, and num-ber of commas respectively, as described in Sec-tion 5.2.
FFT and SLO are additionally introducedmetrics respectively for the ?total duration for allfirst-pass fixations in a target region that excludeany regressions?
and for the ?length of saccadesfrom inside a target region to the outside?13.
All ofthe areas around commas appearing in the originalarticle were considered target areas for the metrics.The other settings were the same as in Section 5.6.3 Contribution of Categorized CommasFigure 10, 11, 12, 13 and 14 respectively show TT,EMFFT , EMFT , EMRT and EMSLO for two typesof comma distributions in each trial.10Ratio to the total fixation time in the target areas (FT).11Normalized by the total viewing time (TT).12Two types of RT count (see Section 5.2) were averaged.13Respectively to reflect ?the early-stage processing of theregion?
and ?the information processed for a fixation and adecision of the next fixation point?
(Hirotani et al 2006).0510F79 F11 F16 F73 F99 H73 H06 H15 H79 H56 L06 L11 L15 L16 L56 M99 M79 M73 M56 S06 S11 S15 S16 S73Trials (?Subject?
+ ?Article ID?
)Distribution(G) Distribution(B)EMRT(10) Positive distribution Nega ive distributionFigure 13: EMRT for two distributions0510F79 F11 F16 F73 F99 H73 H06 H15 H79 H56 L06 L11 L15 L16 L56 M99 M79 M73 M56 S06 S11 S15 S16 S73Trials (?Subject?
+ ?Article ID?
)Distribution(G) Distribution(B)Positive distribution Negative distributionEMSLO(100)Figure 14: EMSLO for two distributionsFor TT, we cannot see any general trend, mainlybecause this time, the reading order of the textwas random, which spread out the second readingeffect evenly between the two distributions.
ForEMFFT , we cannot reach a conclusion either.
Incontrast, in more than half of the trials, EMFFTwas larger for positive distributions, which wouldimply that the positive commas helped to preventthe reader?s gaze from revisiting the target regions.For most trials, except for subject S whose cal-ibration was poor and reading process was poorin M56, EMFT and EMRT decreased and EMSLOincreased for positive distributions, which impliesthat the positive commas smoothed the readingprocess around the target regions.7 ConclusionWe proposed an approach for modeling commaplacement in Chinese text for smoothing reading.In our approach, commas are added to the text onthe basis of a CRF model-based comma predic-tor trained on the treebank, and a rule-based filterthen classifies the commas into ones facilitating orobstructing reading.
The experimental results oneach part of this approach were encouraging.In our future work, we would like see how com-mas affect reading by using much more material,and thereby refine our framework in order to bringa better reading experience to readers.AcknowledgmentsThis research was partially supported by Kakenhi,MEXT Japan [23650076] and JST PRESTO.56ReferencesWallace Chafe.
1988.
Punctuation and the prosody ofwritten language.
Written Communication, 5:396?426.Yuqing Guo, Haifeng Wang, and Josef van Genabith.2010.
A linguistically inspired statistical modelfor Chinese punctuation generation.
ACM Trans-actions on Asian Language Information Processing,9(2):6:1?6:27, June.Masako Hirotani, Lyn Frazier, and Keith Rayner.
2006.Punctuation and intonation effects on clause andsentence wrap-up: Evidence from eye movements.Journal of Memory and Language, 54(3):425?443.Hen-Hsen Huang and Hsin-Hsi Chen.
2011.
Pause andstop labeling for Chinese sentence boundary detec-tion.
In Proceedings of Recent Advances in NaturalLanguage Processing, pages 146?153.Jing Huang and Geoffrey Zweig.
2002.
Maximum en-tropy model for punctuation annotation from speech.In Proceedings of the International Conference onSpoken Language Processing, pages 917?920.Mei xun Jin, Mi-Young Kim, Dongil Kim, and Jong-Hyeok Lee.
2002.
Segmentation of Chineselong sentences using commas.
In Proceedings ofthe Third SIGHAN Workshop on Chinese LanguageProcessing, pages 1?8.John D. Lafferty, Andrew McCallum, and FernandoC.
N. Pereira.
2001.
Conditional random fields:Probabilistic models for segmenting and labeling se-quence data.
In Proceedings of the Eighteenth Inter-national Conference on Machine Learning, ICML?01, pages 282?289, San Francisco, CA, USA.
Mor-gan Kaufmann Publishers Inc.Robert Leaman and Graciela Gonzalez.
2008.
BAN-NER: An executable survery of advances in biomed-ical named entity recognition.
In Pacific Symposiumon Biocomputing (PSB?08), pages 652?663.Baolin Liu, Zhongning Wang, and Zhixing Jin.
2010.The effects of punctuations in Chinese sentencecomprehension: An erp study.
Journal of Neurolin-guistics, 23(1):66?68.Wei Lu and Hwee Tou Ng.
2010.
Better punctuationprediction with dynamic conditional random fields.In Proceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing (EMNLP?10), pages 177?186.Pascual Mart?
?nez-Go?mez, Chen Chen, Tadayoshi Hara,Yoshinobu Kano, and Akiko Aizawa.
2012a.
Imageregistration for text-gaze alignment.
In Proceedingsof the 2012 ACM international conference on Intel-ligent User Interfaces (IUI ?12), pages 257?260.Pascual Mart?
?nez-Go?mez, Tadayoshi Hara, ChenChen, Kyohei Tomita, Yoshinobu Kano, and AkikoAizawa.
2012b.
Synthesizing image representa-tions of linguistic and topological features for pre-dicting areas of attention.
In Patricia Anthony, Mit-suru Ishizuka, and Dickson Lukose, editors, PRICAI2012: Trends in Artificial Intelligence, pages 312?323.
Springer.Andrew Kachites McCallum.
2002.
MALLET: A ma-chine learning for language toolkit.Fu-dong Chiou Naiwen Xue, Fei Xia and MartaPalmer.
2005.
The Penn Chinese TreeBank: Phrasestructure annotation of a large corpus.
Natural Lan-guage Engineering, 11(2):207?238.Stephan Peitz, Markus Freitag, Arne Mauser, and Her-mann Ney.
2011.
Modeling punctuation predictionas machine translation.
In Proceedings of Interna-tional Workshop on Spoken Language Translation,pages 238?245.Keith Rayner.
1998.
Eye movements in reading andinformation processing: 20 years of research.
Psy-chological Bulletin, 124(3):372?422.Gui-Qin Ren and Yufang Yang.
2010.
Syntac-tic boundaries and comma placement during silentreading of Chinese text: evidence from eye move-ments.
Journal of Research in Reading, 33(2):168?177.Burr Settles.
2005.
ABNER: an open source toolfor automatically tagging genes, proteins, and otherentity names in text.
Bioinformatics, 21(14):3191?3192.Elizabeth Shriberg, Andreas Stolcke, Dilek Hakkani-Tu?r, and Go?khan Tu?r.
2000.
Prosody-based au-tomatic segmentation of speech into sentences andtopics.
Speech Communication, 32(1-2):127?154.Judy Perkins Walker, Kirk Fongemie, and TracyDaigle.
2001.
Prosodic facilitation in the resolu-tion of syntactic ambiguities in subjects with leftand right hemisphere damage.
Brain and Language,78(2):169?196.Nianwen Xue and Yaqin Yang.
2011.
Chinese sen-tence segmentation as comma classification.
In Pro-ceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics:shortpapers,pages 631?635.Ming Yue.
2006.
Discursive usage of six Chinesepunctuation marks.
In Proceedings of the COL-ING/ACL 2006 Student Research Workshop, pages43?48.ZDIC.NET.
2005.
Commonly used Chinese punctua-tion usage short list.
Long Wiki, Retrieved Dec 10,2012, from http://www.zdic.net/appendix/f3.htm.
(in Chinese).X.
Y. Zeng.
2006.
The comparison and the useof English and Chinese comma.
College English,3(2):62?65.
(in Chinese).57Kaixu Zhang, Yunqing Xia, and Hang Yu.
2006.CRF-based approach to sentence segmentation andpunctuation for ancient Chinese prose.
Jour-nal of Tsinghua Univ (Science and Technology),49(10):1733?1736.
(in Chinese).58
