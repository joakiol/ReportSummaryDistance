Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 1?18,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsThe CoNLL-2009 Shared Task:Syntactic and Semantic Dependencies in Multiple LanguagesJan Hajic??
Massimiliano Ciaramita?
Richard Johansson?
Daisuke Kawahara?Maria Anto`nia Mart????
Llu?
?s Ma`rquez??
Adam Meyers??
Joakim Nivre??
Sebastian Pado??
?Jan ?Ste?pa?nek?
Pavel Stran?a?k?
Mihai Surdeanu??
Nianwen Xue??
Yi Zhang???
: Charles University in Prague, {hajic,stepanek,stranak}@ufal.mff.cuni.cz?
: Google Inc., massi@google.com?
: University of Trento, johansson@disi.unitn.it?
: National Institute of Information and Communications Technology, dk@nict.go.jp??
: University of Barcelona, amarti@ub.edu??
: Technical University of Catalonia, Barcelona, lluism@lsi.upc.edu??
: New York University, meyers@cs.nyu.edu??
: Uppsala University and Va?xjo?
University, joakim.nivre@lingfil.uu.se??
: Stuttgart University, pado@ims.uni-stuttgart.de??
: Stanford University, mihais@stanford.edu??
: Brandeis University, xuen@brandeis.edu??
: Saarland University, yzhang@coli.uni-sb.deAbstractFor the 11th straight year, the Conferenceon Computational Natural Language Learn-ing has been accompanied by a shared taskwhose purpose is to promote natural languageprocessing applications and evaluate them ina standard setting.
In 2009, the shared taskwas dedicated to the joint parsing of syntac-tic and semantic dependencies in multiple lan-guages.
This shared task combines the sharedtasks of the previous five years under a uniquedependency-based formalism similar to the2008 task.
In this paper, we define the sharedtask, describe how the data sets were createdand show their quantitative properties, reportthe results and summarize the approaches ofthe participating systems.1 IntroductionEvery year since 1999, the Conference on Com-putational Natural Language Learning (CoNLL)launches a competitive, open ?Shared Task?.
Acommon (?shared?)
task is defined and datasets areprovided for its participants.
In 2004 and 2005, theshared tasks were dedicated to semantic role label-ing (SRL) in a monolingual setting (English).
In2006 and 2007 the shared tasks were devoted tothe parsing of syntactic dependencies, using corporafrom up to 13 languages.
In 2008, the shared task(Surdeanu et al, 2008) used a unified dependency-based formalism, which modeled both syntactic de-pendencies and semantic roles for English.
TheCoNLL-2009 Shared Task has built on the 2008 re-sults by providing data for six more languages (Cata-lan, Chinese, Czech, German, Japanese and Span-ish) in addition to the original English1.
It has thusnaturally extended the path taken by the five mostrecent CoNLL shared tasks.As in 2008, the CoNLL-2009 shared task com-bined dependency parsing and the task of identify-ing and labeling semantic arguments of verbs (andother parts of speech whenever available).
Partici-pants had to choose from two tasks:?
Joint task (syntactic dependency parsing andsemantic role labeling), or?
SRL-only task (syntactic dependency parseshave been provided by the organizers, usingstate-of-the art parsers for the individual lan-guages).1There are some format changes and deviations from the2008 task data specification; see Sect.
2.31In contrast to the previous year, the evaluation dataindicated which words were to be dealt with (for theSRL task).
In other words, (predicate) disambigua-tion was still part of the task, whereas the identi-fication of argument-bearing words was not.
Thisdecision was made to compensate for the significantdifferences between languages and between the an-notation schemes used.The ?closed?
and ?open?
challenges have beenkept from last year as well; participants could havechosen one or both.
In the closed challenge, systemshad to be trained strictly with information containedin the given training corpus; in the open challenge,systems could have been developed making use ofany kind of external tools and resources.This paper is organized as follows.
Section 2 de-fines the task, including the format of the data, theevaluation metrics, and the two challenges.
A sub-stantial portion of the paper (Section 3) is devotedto the description of the conversion and develop-ment of the data sets in the additional languages.Section 4 shows the main results of the submittedsystems in the Joint and SRL-only tasks.
Section 5summarizes the approaches implemented by partic-ipants.
Section 6 concludes the paper.
In all sec-tions, we will mention some of the differences be-tween last year?s and this year?s tasks while keepingthe text self-contained whenever possible; for detailsand observations on the English data, please refer tothe overview paper of the CoNLL-2008 Shared Task(Surdeanu et al, 2008) and to the references men-tioned in the sections describing the other languages.2 Task DefinitionIn this section we provide the definition of the sharedtask; after introducing the two challenges and thetwo tasks the participants were to choose, we con-tinue with the format of the shared task data, fol-lowed by a description of the evaluation metricsused.For three of the languages (Czech, English andGerman), out-of-domain data (OOD) have also beenprepared for the final evaluation, following the sameguidelines and formats.2.1 Closed and Open ChallengesSimilarly to the CoNLL-2005 and CoNLL-2008shared tasks, this shared task evaluation is separatedinto two challenges:Closed Challenge The aim of this challenge was tocompare performance of the participating systems ina fair environment.
Systems had to be built strictlywith information contained in the given training cor-pus, and tuned with the development section.
Inaddition, the lexical frame files (such as the Prop-Bank and NomBank for English, the valency dictio-nary PDT-Vallex for Czech etc.)
were provided andmay have been used.
These restrictions mean thatoutside parsers (not trained by the participants?
sys-tems) could not be used.
However, we did providethe output of a single, state-of-the-art dependencyparser for each language so that participants couldbuild a SRL-only system (using the provided parsesas inputs) within the closed challenge (as opposed tothe 2008 shared task).Open Challenge Systems could have been devel-oped making use of any kind of external tools andresources.
The only condition was that such tools orresources must not have been developed with the an-notations of the test set, both for the input and outputannotations of the data.
In this challenge, we wereinterested in learning methods which make use ofany tools or resources that might improve the per-formance.
The comparison of different systems inthis setting may not be fair, and thus ranking of sys-tems is not necessarily important.2.2 Joint and SRL-only tasksIn 2008, systems participating in the open challengecould have used state-of-the-art parsers for the syn-tactic dependency part of the task.
This year, wehave provided the output of these parsers for all thelanguages in an uniform way, thus allowing an or-thogonal combination of the two tasks and the twochallenges.
For the SRL-only task, participants inthe closed challenge simply had to use the providedparses only.Despite the provisions for the SRL-only task, weare more interested in the approaches and results ofthe Joint task.
Therefore, primary system ranking isprovided for the Joint task while additional measures2are computed for various combinations of parsersand SRL methods across the tasks and challenges.2.3 Data FormatThe data format used in this shared task has beenbased on the CoNLL-2008 shared task, with somedifferences.
The data follows these general rules:?
The files contain sentences separated by a blankline.?
A sentence consists of one or more tokens andthe information for each token is represented ona separate line.?
A token consists of at least 14 fields.
The fieldsare separated by one or more whitespace char-acters (spaces or tabs).
Whitespace charactersare not allowed within fields.The data is thus a large table with whitespace-separated fields (columns).
The fields provided inthe data are described in Table 1.
They are identicalfor all languages, but they may differ in contents;for example, some fields might not be filled for allthe languages provided (such as the FEAT or PFEATfields).For the SRL-only task, participants have beenprovided will all the data but the PRED andAPREDs, which they were supposed to fill in withtheir correct values.
However, they did not haveto determine which tokens are predicates (or moreprecisely, which are the argument-bearing tokens),since they were marked by ?Y?
in the FILLPREDfield.For the Joint task, participants could not (in ad-dition to the PRED and APREDs) see the gold-standard nor the predicted syntactic dependencies(HEAD, PHEAD) and their labels (DEPREL, PDE-PREL).
These syntactic dependencies were also tobe filled by participants?
systems.In both tasks, participants have been free touse any other data (columns) provided, except theLEMMA, POS and FEAT columns (to get more ?re-alistic?
results using only their automatically pre-dicted variants PLEMMA, PPOS and PFEAT).Besides the corpus proper, predicate dictionarieshave been provided to participants in order to be ableto properly match the predicates to the tokens in thecorpus; their contents could have been used e.g.
asfeatures for the PRED/APREDs predictions (or evenfor the syntactic dependencies, i.e., for filling in thePHEAD and PDEPREL fields).The system of filling-in the APREDs followsthe 2008 pattern; for each argument-bearing token(predicate), a new APREDn column is created in theorder in which the predicate token is encounteredwithin the sentence (i.e., based on its ID seen as anumerical value).
Then, for each token in the sen-tence, the value in the intersection of the APREDncolumn and the token row is either left unfilled(if the token is not an argument), or a predicate-argument label(s) is(are) filled in.The differences between the English-only 2008task and this year?s multilingual task can be brieflysummarized as follows:?
only ?split?2 lemmas and forms have been pro-vided in the English datasets (for the other lan-guages, original tokenization from the respec-tive treebanks has been used);?
rich morphological features have been addedwherever available;?
syntactic dependencies by state-of-the-artparsers have been provided (for the SRL-onlytask);?
multiple semantic labels for a single token havebeen allowed (and properly evaluated) in theAPREDs columns;?
predicates have been pre-identified and markedin both the training and test data;?
some of the fields (e.g.
the APREDx) and val-ues (ARG0?
A0 etc.)
have been renamed.2.4 Evaluation MeasuresIt was required that participants submit results in allseven languages in the chosen task and in any of (orboth) the challenges.
Submission of out-of-domaindata files has been optional.The main evaluation measure, according to whichsystems are primarily compared, is the Joint task,2Splitting of forms and lemmas in English has been intro-duced in the 2008 shared task to match the tokenization con-vention for the arguments in NomBank.3Field # Name Description1 ID Token counter, starting at 1 for each new sentence2 FORM Form or punctuation symbol (the token; ?split?
for English)3 LEMMA Gold-standard lemma of FORM4 PLEMMA Automatically predicted lemma of FORM5 POS Gold-standard POS (major POS only)6 PPOS Automatically predicted major POS by a language-specific tagger7 FEAT Gold-standard morphological features (if applicable)8 PFEAT Automatically predicted morphological features (if applicable)9 HEAD Gold-standard syntactic head of the current token (ID or 0 if root)10 PHEAD Automatically predicted syntactic head11 DEPREL Gold-standard syntactic dependency relation (to HEAD)12 PDEPREL Automatically predicted dependency relation to PHEAD13 FILLPRED Contains ?Y?
for argument-bearing tokens14 PRED (sense) identifier of a semantic ?predicate?
coming from a current token15... APREDn Columns with argument labels for each semantic predicate (in the ID order)Table 1: Description of the fields (columns) in the data provided.
The values of columns 9, 11 and 14 and above arenot provided in the evaluation data; for the Joint task, columns 9?12 are also empty in the evaluation data.closed challenge, Macro F1 score.
However, scorescan also be computed for a number of other condi-tions:?
Task: Joint or SRL-only?
Challenge: open or closed?
Domain: in-domain data (IDD, separated fromtraining corpus) or out-of-domain data (OOD)Joint task participants are also evaluated separatelyon the syntactic dependency task (labeled attach-ment score, LAS).
Finally, systems competing inboth tasks are compared on semantic role labelingalone, to assess the impact of the the joint pars-ing/SRL task compared to an SRL-only task on pre-parsed data.Finally, as an explanatory measure, precision andrecall of the semantic labeling task have been com-puted and tabulated.We have decided to omit several evaluation fig-ures that were reported in previous years, such as thepercentage of completely correct sentences (?ExactMatch?
), unlabeled scores, etc.
With seven lan-guages, two tasks (plus two challenges, and theIDD/OOD distinction), there are enough results toget lost even as it is.2.4.1 Syntactic Dependency MeasuresThe LAS score is defined similarly as in the pre-vious shared tasks, as the percentage of tokens forwhich a system has predicted the correct HEAD andDEPREL columns.
The unlabeled attachment score(UAS), i.e., the percentage of tokens with correctHEAD regardless if the DEPREL is correct, has notbeen officially computed this year.
No precision andrecall measures are applicable, since all systems aresupposed to output a single dependency with a singlelabel (see also below the footnote to the descriptionof the combined score).2.4.2 Semantic Labeling MeasuresThe semantic propositions are evaluated by con-verting them to semantic dependencies, i.e., we cre-ate n semantic dependencies from every predicateto its n arguments.
These dependencies are labeledwith the labels of the corresponding arguments.
Ad-ditionally, we create a semantic dependency fromeach predicate to a virtual ROOT node.
The latterdependencies are labeled with the predicate senses.This approach guarantees that the semantic depen-dency structure conceptually forms a single-rooted,connected (but not necessarily acyclic) graph.
Moreimportantly, this scoring strategy implies that if asystem assigns the incorrect predicate sense, it stillreceives some points for the arguments correctly as-signed.
For example, for the correct proposition:verb.01: A0, A1, AM-TMPthe system that generates the following output forthe same argument tokens:4verb.02: A0, A1, AM-LOCreceives a labeled precision score of 2/4 because twoout of four semantic dependencies are incorrect: thedependency to ROOT is labeled 02 instead of 01and the dependency to the AM-TMP is incorrectly la-beled AM-LOC.
Using this strategy we compute pre-cision, recall, and F1 scores for semantic dependen-cies (labeled only).For some languages (Czech, Japanese) there maybe more than one label in a given argument position;for example, this happens in Czech in special casesof reciprocity when the same token serves as two ormore arguments to the same predicate.
The scorertakes this into account and considers such cases tobe (as if) multiple predicate-argument relations forthe computation of the evaluation measures.For example, for the correct proposition:v1f1: ACT|EFF, ADDRthe system that generates the following output forthe same argument tokens:v1f1: ACT, ADDR|PATreceives a labeled precision score of 3/4 becausethe PAT is incorrect and labeled recall 3/4 be-cause the EFF is missing (should the ACT|EFF andADDR|PAT be taken as atomic values, the scoreswould then be zero).2.4.3 Combined Syntactic and Semantic ScoreWe combine the syntactic and semantic measuresinto one global measure using macro averaging.
Wecompute macro precision and recall scores by aver-aging the labeled precision and recall for semanticdependencies with the LAS for syntactic dependen-cies:3LMP = Wsem ?
LPsem + (1?Wsem) ?
LAS (1)LMR = Wsem ?
LRsem + (1 ?Wsem) ?
LAS (2)where LMP is the labeled macro precision andLPsem is the labeled precision for semantic depen-dencies.
Similarly, LMR is the labeled macro re-call and LRsem is the labeled recall for semanticdependencies.
Wsem is the weight assigned to the3We can do this because the LAS for syntactic dependen-cies is a special case of precision and recall, where the predictednumber of dependencies is equal to the number of gold depen-dencies.semantic task.4 The macro labeled F1 score, whichwas used for the ranking of the participating sys-tems, is computed as the harmonic mean of LMPand LMR.3 DataThe unification of the data formats for the variouslanguages appeared to be a challenge in itself.
Wewill briefly describe the processes of the conversionof the existing treebanks in the seven languages ofthe CoNLL-2009 shared task.
In many instances,the original treebanks had to be not only convertedformat-wise, but also merged with other resources inorder to generate useful training and testing data thatfit the task description.3.1 The Input CorporaThe data used as the input for the transformationsaimed at arriving at the data contents and format de-scribed in Sect.
2.3 are described in (Taule?
et al,2008), (Xue and Palmer, 2009), (Hajic?
et al, 2006),(Surdeanu et al, 2008), (Burchardt et al, 2006) and(Kawahara et al, 2002).In the subsequent sections, the procedures for thedata conversion for the individual languages are de-scribed.
The data has been collected by the mainorganization site and checked for format errors, andrepackaged for distribution.There were three packages of the data distributedto the participants: Trial, Training plus Develop-ment, and Evaluation.
The Trial data were rathersmall, just to give the feeling of the format andlanguages involved.
A visual representation of theTrial data was also created to make understandingof the data easier.
Any data in the same formatcan be transformed and displayed in the Tree EditorTrEd5 (Pajas and ?Ste?pa?nek, 2008) with the CoNLL2009 Shared Task extension that can be installedfrom within the editor.
A sample visualization of anEnglish sentence after its conversion to the sharedtask format (Sect.
2.3) is in Fig.
1.Due to licensing requirements, every package ofthe data had to be split into two portions.
Oneportion (Catalan, German, Japanese, and Spanishdata) was published on the task?s webpage for down-4We assign equal weight to the two tasks, i.e., Wsem = 0.5.5http://ufal.mff.cuni.cz/?pajas/tred5$QG'(3 &&VRPHWLPHV703 5%D102' '7UHSXWDEOH102' --FKDULW\6%- 11ZLWK102' ,1D102' '7KRXVHKROG102' 11QDPH QDPH302' 11JHWV JHW5227 9%=XVHG XVH9& 9%1DQG&225' &&GRHV&21- 9%=QW$'9 5%HYHQ$'9 5%NQRZ NQRZ9& 9%LW2%- 3533  $0703$0703$0703 $$$$ $$$01(*$0$'9$Figure 1: Visualisation of the English sentence ?And sometimes a reputable charity with a houshold name gets usedand doesn?t even know it.?
(Penn Treebank, wsj 0559) showing jointly the labeled syntactic and semantic depen-dencies.
The basic tree shape comes from the syntactic dependencies; syntactic labels and POS tags are on the 2ndline at each node.
Semantic dependencies which do not follow the syntactic ones use dotted lines.
Predicate sensesin parentheses (use:01, ...) follow the word label.
SRLs (A0, AM-TMP, ...) are on the last line.
Please note thatmultiple semantic dependencies (e.g., there are four for charity: A0?
know, A1?
gets, A1?
used, A1?
name)and self-dependencies (name) appear in this sentence.load, the other portion (Czech, English, and Chinesedata) was invoiced and distributed by the LinguisticData Consortium under a special agreement free ofcharge.Distribution of the Evaluation package was a bitmore complicated, because there were two types ofthe packages - one for the Joint task and one for theSRL-only task.
Every participant had to subscribeto one of the two tasks; subsequently, they obtainedthe appropriate data (again, from the webpage andLDC).Prior to release, each data file was checked toeliminate errors.
The following test were carriedout:?
For every sentence, number of PREDs rowsmatches the number of APREDs columns.?
The first line of each file is never empty, whilethe last line always is.?
The first character on a non-empty line is al-ways a digit, the last one is never a whitespace.?
The number of empty lines (i.e.
the numberof sentences) equals the number of lines begin-ning with ?1?.?
The data contain no spaces nor double tabs.Some statistics on the data can be seen in Ta-bles 2, 3 and 4.
Whereas the training sizes of thedata have not been that different as they were e.g.for the 2007 shared task on multilingual dependencyparsing (Nivre et al, 2007)6, substantial differencesexisted in the distribution of the predicates and ar-guments, the input features, the out-of-vocabularyrates, and other statistical characteristics of the data.Data sizes have been relatively uniform in all thedatasets, with Japanese having the smallest dataset6http://nextens.uvt.nl/depparse-wiki/DataOverview6containing data for SRL annotation training.
Tocompensate at least for the dependency parsing part,an additional, large Japanese corpus with syntacticdependency annotation has been provided.The average sentence length, the vocabulary sizesfor FORM and LEMMA fields and the OOV ratescharacterize quite naturally the properties of the re-spective languages (in the domain of the training andevaluation data).
It is no surprise that the FORMOOV rate is the highest for Czech, a highly inflec-tional language, and that the LEMMA OOV rate isthe highest for German (as a consequence of keepingcompounds as a single lemma).
The other statisticsalso reflect (to a large extent) the annotation speci-fication and conventions used for the original tree-banks and/or the result of the conversion process tothe unified CoNLL-2009 Shared Task format.Starting with the POS and FEAT fields, it can beseen that Catalan, Czech and Spanish use only the12 major part-of-speech categories as values of thePOS field (with richly populated FEAT field); En-glish and Chinese are the opposite extreme, disre-garding the use of the FEAT field completely andcoding everything as a POS value.
While for Chi-nese this is quite understandable, English follows thePTB tradition in this respect.
German and Japaneseuse relatively rich set of values in both the POS andFEAT fields.For the dependency relations (DEPREL), allthe languages use a similarly-sized set except forJapanese, which only encodes the distinction be-tween a root and a dependent node (and some in-frequent special ones).Evaluation data are over 10% of the size of thetraining data for Catalan, Chinese, Czech, Japaneseand Spanish and roughly 5% for English and Ger-man.Table 3 shows the distribution of the five most fre-quent dependency relations (determined as part ofthe subtask of syntactic parsing).
With the exceptionof Japanese, which essentially does not label depen-dency relations at this level, all the other languagesshow little difference in this distribution.
For exam-ple, the unconditioned probability of ?subjects?
isalmost the same for all the six other languages (be-tween 6 and 8 percent).
The probability mass cov-ered by the first five most frequent DEPRELs is alsoalmost the same (again, except for Japanese), sug-gesting that the labeling task might have similar dif-ficulty7.
The most skewed one is for Czech (afterJapanese).Table 4 shows similar statistics for the argumentlabels (PRED/APREDs); it also adds the averagenumber of arguments per ?predicate?
token, sincethis is part of the SRL task8.
It is apparent from thecomparison of the ?Total?
rows in this table and Ta-ble 3 that the first five argument labels cover morethat their syntactic counterparts.
For example, thearguments A0-A4 account for all but 3% of all ar-guments labels, whereas Spanish and Catalan havemuch more rich set of argument labels, with a highentropy of the most-frequent-label distribution.3.2 Catalan and SpanishThe Catalan and Spanish datasets (Taule?
et al, 2008)were generated from the AnCora corpora9 throughan automatic conversion process from a constituent-based formalism to dependencies (Civit et al, 2006).AnCora corpora contain about half million wordsfor Catalan and Spanish annotated with syntacticand semantic information.
Text sources for the Cata-lan corpus are EFE news agency (?75Kw), ACNCatalan news agency (?225Kw), and ?El Perio?dico?newspaper (?200Kw).
The Spanish corpus comesfrom the Lexesp Spanish balanced corpus (?75Kw),the EFE Spanish news agency (?225Kw), and theSpanish version of ?El Perio?dico?
(?200Kw).
Thesubset from ?El Perio?dico?
corresponds to the samenews in Catalan and Spanish, spanning from Januaryto December 2000.Linguistic annotation is the same in both lan-guages and includes: PoS tags with morphologi-cal features (gender, number, person, etc.
), lemma-tization, syntactic dependencies (syntactic func-tions), semantic dependencies (arguments and the-matic roles), named entities and predicate semanticclasses (Lexical Semantic Structure, LSS).
Tag setsare shared by the two languages.If we take into account the complete PoS tags,7Yes, this is overgeneralization since this distribution doesnot condition on the features, dependencies etc.
But as a roughmeasure, it often correlates well with the results.8A number below 1 means there are some argument-bearingwords (often nouns) which have no arguments in the particularsentence in which they appear.9http://clic.ub.edu/ancora7Characteristic Catalan Chinese Czech English German Japanese SpanishTraining data size (sentences) 13200 22277 38727 39279 36020 4393a 14329Training data size (tokens) 390302 609060 652544 958167 648677 112555a 427442Avg.
sentence length (tokens) 29.6 27.3 16.8 24.4 18.0 25.6 29.8Tokens with argumentsb (%) 9.6 16.9 63.5 18.7 2.7 22.8 10.3DEPREL types 50 41 49 69 46 5 49POS types 12 41 12 48 56 40 12FEAT types 237 1 1811 1 267 302 264FORM vocabulary size 33890 40878 86332 39782 72084 36043 40964LEMMA vocabulary size 24143 40878 37580 28376 51993 30402 26926Evaluation data size (sent.)
1862 2556 4213 2399 2000 500 1725Evaluation data size (tokens) 53355 73153 70348 57676 31622 13615 50630Evaluation FORM OOVc 5.40 3.92 7.98/8.62d 1.58/3.76d 7.93/7.57d 6.07 5.63Evaluation LEMMA OOVc 4.14 3.92 3.03/4.29d 1.08/2.30d 5.83/7.36d 5.21 3.69Table 2: Elementary data statistics for the CoNLL-2009 Shared Task languages.
The data themselves, the originaltreebanks they were derived from and the conversion process are described in more detail in sections 3.2-3.7.
Allevaluation data statistics are derived from the in-domain evaluation data.aThere were additional 33257 sentences (839947 tokens) available for syntactic dependency parsing of Japanese; the type andvocabulary statistics are computed using this larger dataset.bPercentage of tokens with FILLPRED=?Y?.cPercentage of FORM/LEMMA tokens not found in the respective vocabularies derived solely from the training data.dOOV percentage for in-domain/out-of-domain data.DEPREL Catalan Chinese Czech English German Japanese Spanishsn 0.16 COMP 0.21 Atr 0.26 NMOD 0.27 NK 0.31 D 0.93 sn 0.16spec 0.15 NMOD 0.14 AuxP 0.10 P 0.11 PUNC 0.14 ROOT 0.04 spec 0.15Labels f 0.11 ADV 0.10 Adv 0.10 PMOD 0.10 MO 0.12 P 0.03 f 0.12sp 0.09 UNK 0.09 Obj 0.07 SBJ 0.07 SB 0.07 A 0.00 sp 0.08suj 0.07 SBJ 0.08 Sb 0.06 OBJ 0.06 ROOT 0.06 I 0.00 suj 0.08Total 0.58 0.62 0.59 0.61 0.70 1.00 0.59Table 3: Unigram probability for the five most frequent DEPREL labels in the training data of the CoNLL-2009Shared Task is shown.
Total is the probability mass covered by the five dependency labels shown.APRED Catalan Chinese Czech English German Japanese Spanisharg1-pat 0.22 A1 0.30 RSTR 0.30 A1 0.37 A0 0.40 GA 0.33 arg1-pat 0.20arg0-agt 0.18 A0 0.27 PAT 0.18 A0 0.25 A1 0.39 WO 0.15 arg0-agt 0.19Labels arg1-tem 0.15 ADV 0.20 ACT 0.17 A2 0.12 A2 0.12 NO 0.15 arg1-tem 0.15argM-tmp 0.08 TMP 0.07 APP 0.06 AM-TMP 0.06 A3 0.06 NI 0.09 arg2-atr 0.08arg2-atr 0.08 DIS 0.04 LOC 0.04 AM-MNR 0.03 A4 0.01 DE 0.06 argM-tmp 0.08Total 0.71 0.91 0.75 0.83 0.97 0.78 0.70Avg.
2.25 2.26 0.88 2.20 1.97 1.71 2.26Table 4: Unigram probability for the five most frequent APRED labels in the training data of the CoNLL-2009Shared Task is shown.
Total is the probability mass covered by the five argument labels shown.
The ?Avg.?
lineshows the average number of arguments per predicate or other argument-bearing token (i.e.
for those marked byFILLPRED=?Y?
).8AnCora has 280 different labels.
Considering onlythe main syntactic categories, the tag set is reducedto 47 tags.
The syntactic tag set consists of 50 dif-ferent syntactic functions.
Regarding semantic ar-guments, we distinguish Arg0, Arg1, Arg2, Arg3,Arg4, ArgM, and ArgL.
The first five tags are num-bered from less to more obliqueness with respectto the verb, ArgM corresponds to adjuncts.
Thelist of thematic roles consists of 20 different labels:AGT (Agent), AGI (Induced Agent), CAU (Cause),EXP (Experiencer), SCR (Source), PAT (Patient),TEM (Theme), ATR (Attribute), BEN (Beneficiary),EXT (Extension), INS (Instrument), LOC (Loca-tive), TMP (Time), MNR (Manner), ORI (Origin),DES (Goal), FIN (Purpose), EIN (Initial State), EFI(Final State), and ADV (Adverbial).
Each argumentposition can map onto specific thematic roles.
Byway of example, Arg1 can be PAT, TEM or EXT.
ForNamed Entities, we distinguish six types: Organiza-tion, Person, Location, Date, Number, and Others.An incremental process guided the annotation ofAnCora, since semantics depends on morphosyntax,and syntax relies on morphology.
This proceduremade it possible to check, correct, and completethe previous annotations, thus guaranteeing the finalquality of the corpora and minimizing the error rate.The annotation process was carried out sequentiallyfrom lower to upper layers of linguistic description.All resulting layers are independent of each other,thus making easier the data management.
The ini-tial annotation was performed manually for syntax,semiautomatically in the case of arguments and the-matic roles, and fully automatically for PoS (Mart?
?et al, 2007; Ma`rquez et al, 2007).The Catalan and Spanish AnCora corpora werestraightforwardly translated into the CoNLL-2009shared task formatting (information about namedentities was skipped in this process).
The resultingCatalan corpus (including training, development andtest partitions) contains 16,786 sentences with an av-erage length of 29.59 lexical tokens per sentence.Long sentences abound in this corpus.
For instance,10.73% of the sentences are longer than 50 tokens,and 4.42% are longer than 60.
The corpus con-tains 47,537 annotated predicates (2.83 predicatesper sentence, on average) with 107,171 arguments(2.25 arguments per predicate, on average).
Fromthe latter, 73.89% correspond to core arguments and26.11% to adjuncts.
Numbers for the Spanish cor-pus are comparable in all aspects: 17,709 sentenceswith 29.84 lexical tokens on average (11.58% of thesentences longer than 50 tokens, 4.07% longer than60); 54,075 predicates (3.05 per sentence, on aver-age) and 122,478 arguments (2.26 per predicate, onaverage); 73.34% core arguments and 26.66% ad-juncts.The following are important features of the Cata-lan and Spanish corpora in the CoNLL-2009 sharedtask setting: (1) all dependency trees are projective;(2) no word can be the argument of more than onepredicate in a sentence; (3) semantic dependenciescompletely match syntactic dependency structures(i.e., no new edges are introduced by the semanticstructure); (4) only verbal predicates are annotated(with exceptional cases referring to words that canbe adjectives and past participles); (5) the corpus issegmented so multi-words, named entities, temporalexpressions, compounds, etc.
are grouped together;and (6) segmentation also accounts for elliptical pro-nouns (there are marked as empty lexical tokens ?_?with a pronoun POS tag).Finally, the predicted columns (PLEMMA,PPOS, and PFEAT) have been generated with theFreeLing Open source suite of Language Analyz-ers10.
Accuracy in PLEMMA and PPOS columnsis above 95% for the two languages.
PHEADand PDEPREL columns have been generated usingMaltParser11.
Parsing accuracy (LAS) is above 86%for the the two languages.3.3 ChineseThe Chinese Corpus for the 2009 CoNLL SharedTask was generated by merging the Chinese Tree-bank (Xue et al, 2005) and the Chinese PropositionBank (Xue and Palmer, 2009) and then convertingthe constituent structure to a dependency formalismas specified in the CoNLL Shared Task.
The Chi-nese data used in the shared task is based on ChineseTreebank 6.0 and the Chinese Proposition Bank 2.0,both of which are publicly available via the Linguis-tic Data Consortium.The Chinese Treebank Project originated at Pennand was later moved to University of Colorado at10http://www.lsi.upc.es/?nlp/freeling11http://w3.msi.vxu.se/?jha/maltparser9Boulder.
Now it is the process of being to movedto Brandeis University.
The data sources of the Chi-nese Treebank range from Xinhua newswire (main-land China), Hong Kong news, and Sinorama Maga-zine (Taiwan).
More recently under DARPA GALEfunding it has been expanded to include broadcastnews, broadcast conversation, news groups and weblog data.
It currently has over one million wordsand is fully segmented, POS-tagged and annotatedwith phrase structure.
The version of the ChineseTreebank used in this shared task, CTB 6.0, includesnewswire, magazine articles, and transcribed broad-cast news 12.
The training set has 609,060 tokens,the development set has 49,620 tokens, and the testset has 73,153 tokens.The Chinese Proposition Bank adds a layer of se-mantic annotation to the syntactic parses in the Chi-nese Treebank.
This layer of semantic annotationmainly deals with the predicate-argument structureof Chinese verbs and their nominalizations.
Eachmajor sense (called frameset) of a predicate takes anumber of core arguments annotated with numeri-cal labels Arg0 through Arg5 which are defined ina predicate-specific manner.
The Chinese Proposi-tion Bank also annotates adjunctive arguments suchas locative, temporal and manner modifiers of thepredicate.
The version of the Chinese Propbank usedin this CoNLL Shared Task is CPB 2.0, but nominalpredicates are excluded because the annotation is in-complete.Since the Chinese Treebank is annotated withconstituent structures, the conversion and mergingprocedure converts the constituent structures to de-pendencies by identifying the head for each con-stituent in a parse tree and making its sisters its de-pendents.
The Chinese Propbank pointers are thenshifted from the entire constituent to the head of thatconstituent.
The conversion procedure identifies thehead by first exploiting the structural informationin the syntactic parse and detecting six broad cate-gories of syntactic relations that hold between thehead and its dependents (predication, modification,complementation, coordination, auxiliary, and flat)and then designating the head based on these rela-tions.
In particular, the first conjunct of a coordina-12A small number of files were taken out of the CoNLLshared task data due to conversion problems and time con-straints to fix them.tion structure is designated as the head and the headsof the other conjuncts are the conjunctions preced-ing them.
The conjunctions all ?modify?
the firstconjunct.3.4 CzechFor the training, development and evaluation data,Prague Dependency Treebank 2.0 was used (Hajic?et al, 2006).
For the out-of-domain evaluation data,part of the Czech side of the Prague Czech-EnglishDependency Treebank (version 2, under construc-tion) was used13, see also ( ?Cmejrek et al, 2004).
Forthe OOD data, no manual annotation of LEMMA,POS, and FEAT existed, so the predicted valueswere used.
The same conversion procedure has beenapplied to both sources.The FORM column was created from the formelement of the morphological layer, not from the?token?
from the word-form layer.
Therefore, mosttypos, errors in word segmentation and tokenizationare corrected and numerals are normalized.The LEMMA column was created from thelemma element of the morphological layer.
Onlythe initial string of the element was used, so there isno distinction between homonyms.
However, somecomponents of the detailed lemma explanation wereincorporated into the FEAT column (see below).The POS column was created form the morpho-logical tag element, its first character more pre-cisely.The FEAT column was created from the remain-ing characters of the tag element.
In addition, thespecial feature ?Sem?
corresponds to a semantic fea-ture of the lemma.For the HEAD and DEPREL columns, the PDTanalytical layer was used.
The DEPREL was takenfrom the analytic function (the afun node at-tribtue).
There are 27 possible values for afun el-ement: Pred, Pnom, AuxV, Sb, Obj, Atr, Adv,Atv, AtvV, Coord, Apos, ExD, and a numberof auxiliary and ?double-function?
labels.
The firstnine of these are the ?most interesting?
from thepoint of view of the shared task, since they relate tosemantics more closely than the rest (at least fromthe linguistic point of view).
The HEAD is a pointerto its parent, which means the PDT?s ord attribute13http://ufal.mff.cuni.cz/pedt10(within-sentence ID / word position number) of theparent.
If a node is a member of a coordinationor apposition (is_member element), its DEPRELobtains the _M suffix.
The parenthesis annotation(is_parenthesis_root element) was ignored.The PRED and APREDs columns were createdfrom the tectogrammatical layer of PDT 2.0 and thevalency lexicon PDT-Vallex according to the follow-ing rules:?
Every line corresponding to an analytical nodereferenced by a lexical reference (a/lex.rf)from the tectogrammatical layer has a PREDvalue filled.
If the referring non-generatedtectogrammatical node (is_generated notequal to 1) has a valency frame assigned(val_frame.rf), the value of PRED is theidentifier of the frame.
Otherwise, it is set tothe same value as the LEMMA column.?
For every tectogrammatical node, a corre-sponding analytical node is searched for:1.
If the tectogrammatical node is notgenerated and has a lexical reference(a/lex.rf), the referenced node istaken.2.
Otherwise, if the tectogrammatical nodehas a coreference (coref_text.rf orcoref_gram.rf) or complement refer-ence (compl.rf) to a node that has ananalytical node assigned (by 1. or 2.
), theassigned node is taken.APRED columns are filled with respect to thefollowing correspondence: for a tectogrammaticalnode P and its effective child C with functor F, thecolumn for P?s corresponding analytical node at therow for C?s corresponding analytical node is filledwith F. Some nodes can thus have several functorsin one APRED column, separated by a vertical bar(see Sect.
2.4.2).PLEMMA, PPOS and PFEAT were gener-ated by the (cross-trained) morphological taggerMORCE (Spoustova?
et al, 2009), which gives fullcombined accuracy (PLEMMA+PPOS+PFEAT)slightly under 96%.PHEAD and PDEPREL were generated bythe (cross-trained) MST parser for Czech (Chu?Liu/Edmonds algorithm, (McDonald et al, 2005)),which has typical dependency accuracy around85%.The valency lexicon, converted from (Hajic?
et al,2003), has four columns:1. lemma (can occur several times in the lexicon,with different frames)2. frame identifier (as found in the PRED column)3. list of space-separated actants and obligatorymembers of the frame4.
example(s)The source of the out-of-domain data uses anextended valency lexicon (because of out-of-vocabulary entries).
For simplicity, the extendedlexicon was not provided; instead, such words werenot marked as predicates in the OOD data (theirFILLPRED was set to ?_?)
and thus not evaluated.3.5 EnglishThe English corpus is almost identical to the cor-pus used in the closed challenge in the CoNLL-2008shared task evaluation (Surdeanu et al, 2008).
Thiscorpus was generated through a process that mergesseveral input corpora and converts them from theconstituent-based formalism to dependencies.
Thefollowing corpora were used as input to the mergingprocedure:?
Penn Treebank 3 ?
The Penn Treebank 3 cor-pus (Marcus et al, 1994) consists of hand-coded parses of the Wall Street Journal (test,development and training) and a small subsetof the Brown corpus (W. N. Francis and H.Kucera, 1964) (test only).?
BBN Pronoun Coreference and Entity TypeCorpus ?
BBN?s NE annotation of the WallStreet Journal corpus (Weischedel and Brun-stein, 2005) takes the form of SGML inlinemarkup of text, tokenized to be completelycompatible with the Penn Treebank annotation.For the CoNLL-2008 shared task evaluation,this corpus was extended by the task organizersto cover the subset of the Brown corpus used asa secondary testing dataset.
From this corpuswe only used NE boundaries to derive NAME11dependencies between NE tokens, e.g., we cre-ate a NAME dependency from Mary to Smithgiven the NE mention Mary Smith.?
Proposition Bank I (PropBank) ?
The Prop-Bank annotation (Palmer et al, 2005) classifiesthe arguments of all the main verbs in the PennTreebank corpus, other than be.
Arguments arenumbered (Arg0, Arg1, .
.
.)
based on lexicalentries or frame files.
Different sets of argu-ments are assumed for different rolesets.
De-pendent constituents that fall into categories in-dependent of the lexical entries are classified asvarious types of adjuncts (ArgM-TMP, -ADV,etc.).?
NomBank ?
NomBank annotation (Meyers etal., 2004) uses essentially the same frameworkas PropBank to annotate arguments of nouns.Differences between PropBank and NomBankstem from differences between noun and verbargument structure; differences in treatment ofnouns and verbs in the Penn Treebank; and dif-ferences in the sophistication of previous re-search about noun and verb argument structure.Only the subset of nouns that take argumentsare annotated in NomBank and only a subset ofthe non-argument siblings of nouns are markedas ArgM.The complete merging process and the conversionfrom the constituent representation to dependenciesis detailed in (Surdeanu et al, 2008).The main difference between the 2008 and 2009version of the corpora is the generation of word lem-mas.
In the 2008 version the only lemmas pro-vided were predicted using the built-in lemmatizerin WordNet (Fellbaum, 1998) based on the most fre-quent sense for the form and the predicted part-of-speech tag.
These lemmas are listed in the 2009corpus under the PLEMMA column.
The LEMMAcolumn in the 2009 version of the corpus containslemmas generated using the same algorithm but us-ing the correct Treebank part-of-speech tags.
Addi-tionally, the PHEAD and PDEPREL columns weregenerated using MaltParser14, similarly to the openchallenge corpus in the CoNLL 2008 shared task.14http://w3.msi.vxu.se/?nivre/research/MaltParser.html3.6 GermanThe German in-domain dataset is based on the an-notated verb instances of the SALSA corpus (Bur-chardt et al, 2006), a total of around 40k sen-tences15.
SALSA provides manual semantic roleannotation on top of the syntactically annotatedTIGER newspaper corpus, one of the standard Ger-man treebanks.
The original SALSA corpus uses se-mantic roles in the FrameNet paradigm.
We con-structed mappings between FrameNet frame ele-ments and PropBank argument positions at the levelof frame-predicate pairs semi-automatically.
For theframe elements of each frame-predicate pair, we firstidentified the semantically defined PropBank Arg-0 and Arg-1 positions.
To do so, we annotated asmall number of very abstract frame elements withthese labels (Agent, Actor, Communicator as Arg-0, and Theme, Effect, Message as Arg-1) and per-colated these labels through the FrameNet hierar-chy, adding further manual labels where necessary.Then, we used frequency and grammatical realiza-tion information to map the remaining roles ontohigher-numbered Arg roles.
We considerably sim-plified the annotations provided by SALSA, whichuse a rather complex annotation scheme.
In partic-ular, we removed annotation for multi-word expres-sions (which may be non-contiguous), annotationsinvolving multiple frames for the same predicate(metaphors, underspecification), and inter-sentenceroles.The out-of-domain dataset was taken from a studyon the multi-lingual projection of FrameNet annota-tion (Pado and Lapata, 2005).
It is sampled fromthe EUROPARL corpus and was chosen to maxi-mize the lexical coverage, i.e., it contains of a largenumber of infrequent predicates.
Both syntactic andsemantic structure were annotated manually, in theTIGER and SALSA format, respectively.
Since ituses a simplified annotation schemes, we did nothave to discard any annotation.For both datasets, we converted the syntacticTIGER (Brants et al, 2002) representations into de-pendencies with a similar set of head-finding rulesused for the preparation of the CoNLL-X shared taskGerman dataset.
Minor modifications (for the con-15Note, however, that typically not all predicates in each sen-tence are annotated (cf.
Table 2).12version of person names and coordinations) weremade to achieve better consistency with datasetsof other languages.
Since the TIGER annotationallows non-contiguous constituents, the resultingdependencies can be non-projective.
Secondaryedges were discarded in the conversion.
As for theautomatically constructed features, we used Tree-Tagger (Schmid, 1994) to produce the PLEMMAand PPOS columns, and the Morphisto morphol-ogy (Zielinski and Simon, 2008) for PFEAT.3.7 JapaneseFor Japanese, we used the Kyoto University TextCorpus (Kawahara et al, 2002), which consists ofapproximately 40k sentences taken from MainichiNewspapers.
Out of them, approximately 5k sen-tences are annotated with syntactic and semantic de-pendencies, and are used the training, developmentand test data of this year?s shared task.
The remain-ing sentences, which are annotated with only syntac-tic dependencies, are provided for the training cor-pus of syntactic dependency parsers.This corpus adopts a dependency structure repre-sentation, and thus the conversion to the CoNLL-2009 format was relatively straightforward.
How-ever, since the original dependencies are annotatedon the basis of phrases (Japanese bunsetsu), weneeded to automatically convert the original annota-tions to word-based ones using several criteria.
Weused the following basic criteria: the words exceptthe last word in a phrase depend on the next (right)word, and the last word in a phrase basically dependson the head word of the governing phrase.Semantic dependencies are annotated for bothverbal predicates and nominal predicates.
The se-mantic roles (APRED columns) consist of 41 sur-face cases, many of which are case-marking post-positions such as ga (nominative), wo (accusative)and ni (dative).
Semantic frame discrimination is notannotated, and so the PRED column is the same asthe LEMMA column.
The original corpus containscoreference annotations and inter-sentential seman-tic dependencies, such as inter-sentential zero pro-nouns and bridging references, but we did not usethese annotations, which are not the target of thisyear?s shared task.To produce the PLEMMA, PPOS and PFEATcolumns, we used the morphological analyzer JU-MAN 16 and the dependency and case structure an-alyzer KNP 17.
To produce the PHEAD and PDE-PREL columns, we used the MSTParser 18.4 Submissions and ResultsParticipants uploaded the results through the sharedtask website, and the official evaluation was per-formed centrally.
Feedback was provided if any for-mal problems were encountered (for a list of checks,see the previous section).
One submission had tobe rejected because only English results were pro-vided.
After the evaluation period had passed, theresults were anonymized and published on the web.A total of 20 systems participated in the closedchallenge; 13 of them in the Joint task and seven inthe SRL-only task.
Two systems participated in theopen challenge (Joint task).
Moreover, 17 systemsprovided output in the out-of-domain part of the task(11 in the OOD Joint task and six in the OOD SRL-only task).The main results for the core task - the Joint task(dependency syntax and semantic relations) in thecontext of the closed challenge - are summarized andranked in Table 5.The largest number of systems can be comparedin the SRL results table (Table 6), where all the sys-tems have been evaluated solely on the SRL perfor-mance regardless whether they participated in theJoint or SRL-only task.
However, since the resultsmight have been influenced by the supplied parser,separate ranking is provided for both types of thesystems.Additional breakdown of the results (open chal-lenge, precision and recall tables for the semanticlabeling task, etc.)
are available from the CoNLL-2009 Shared Task website19.5 ApproachesTable 7 summarizes the properties of the systemsthat participated in the closed the open challenges.16http://nlp.kuee.kyoto-u.ac.jp/nl-resource/juman-e.html17http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp-e.html18http://sourceforge.net/projects/mstparser19http://ufal.mff.cuni.cz/conll2009-st13Rank System Average Catalan Chinese Czech English German Japanese Spanish1 Che 82.64 81.84 76.38 83.27 87.00 82.44 85.65 81.902 Chen 82.52 83.01 76.23 80.87 87.69 81.22 85.28 83.313 Merlo 82.14 82.66 76.15 83.21 86.03 79.59 84.91 82.434 Bohnet 80.85 80.44 75.91 79.57 85.14 81.60 82.51 80.755 Asahara 78.43 75.91 73.43 81.43 86.40 69.84 84.86 77.126 Brown 77.27 77.40 72.12 75.66 83.98 77.86 76.65 77.217 Zhang 76.49 75.00 73.42 76.93 82.88 73.76 78.17 75.258 Dai 73.98 72.09 72.72 67.14 81.89 75.00 80.89 68.149 Lu Li 73.97 71.32 65.53 75.85 81.92 70.93 80.49 71.7210 Llu?
?s 71.49 56.64 66.18 75.95 81.69 72.31 81.76 65.9111 Vallejo 70.81 73.75 67.16 60.50 78.19 67.51 77.75 70.7812 Ren 67.81 59.42 75.90 60.18 77.83 65.77 77.63 57.9613 Zeman 51.07 49.61 43.50 57.95 50.27 49.57 57.69 48.90Table 5: Official results of the Joint task, closed challenge.
Teams are denoted by the last name (first name addedonly where needed) of the author who registered for the evaluation data.
Results are sorted in descending order of thelanguage-averaged macro F1 score on the closed challenge Joint task.
Bold numbers denote the best result for a givenlanguage.Rank Rank in task System Average Catalan Chinese Czech English German Japanese Spanish1 1 (SRLonly) Zhao 80.47 80.32 77.72 85.19 85.44 75.99 78.15 80.462 2 (SRLonly) Nugues 80.31 80.01 78.60 85.41 85.63 79.71 76.30 76.523 1 (Joint) Chen 79.96 80.10 76.77 82.04 86.15 76.19 78.17 80.294 2 (Joint) Che 79.94 77.10 77.15 86.51 85.51 78.61 78.26 76.475 3 (Joint) Merlo 78.42 77.44 76.05 86.02 83.24 71.78 77.23 77.196 3 (SRLonly) Meza-Ruiz 77.46 78.00 77.73 75.75 83.34 73.52 76.00 77.917 4 (Joint) Bohnet 76.00 74.53 75.29 79.02 80.39 75.72 72.76 74.318 5 (Joint) Asahara 75.65 72.35 74.17 84.69 84.26 63.66 77.93 72.509 6 (Joint) Brown 72.85 72.18 72.43 78.02 80.43 73.40 61.57 71.9510 7 (Joint) Dai 70.78 66.34 71.57 75.50 78.93 67.43 71.02 64.6411 8 (Joint) Zhang 70.31 67.34 73.20 78.28 77.85 62.95 64.71 67.8112 9 (Joint) Lu Li 69.72 66.95 67.06 79.08 77.17 61.98 69.58 66.2313 4 (SRLonly) Baoli Li 69.26 74.06 70.37 57.46 69.63 67.76 72.03 73.5414 10 (Joint) Vallejo 68.95 70.14 66.71 71.49 75.97 61.01 68.82 68.4815 5 (SRLonly) Moreau 66.49 65.60 67.37 71.74 72.14 66.50 57.75 64.3316 11 (Joint) Llu?
?s 63.06 46.79 59.72 76.90 75.86 62.66 71.60 47.8817 6 (SRLonly) Ta?ckstro?m 61.27 57.11 63.41 71.05 67.64 53.42 54.74 61.5118 7 (SRLonly) Lin 57.18 61.70 70.33 60.43 65.66 59.51 23.78 58.8719 12 (Joint) Ren 56.69 41.00 72.58 62.82 67.56 54.31 58.73 39.8020 13 (Joint) Zeman 32.14 24.19 34.71 58.13 36.05 16.44 30.13 25.36Table 6: Official results of the semantic labeling, closed challenge, all systems.
Teams are denoted by the last name(first name added only where needed) of the author who registered for the evaluation data.
Results are sorted indescending order of the semantic labeled F1 score (closed challenge).
Bold numbers denote the best result for a givenlanguage.
Separate ranking is provided for SRL-only systems.The second column of the table highlights the over-all architectures.
We used + to indicate that thecomponents are sequentially connected.
The lack ofa + sign indicates that the corresponding tasks areperformed jointly.It is perhaps not surprising that most of the obser-vations from the 2008 shared task still hold; namely,the best systems overall do not use joint learning oroptimization (the best such system was placed thirdin the Joint task, and there were only four systemswhere the learning methodology can be considered?joint?
).Therefore, most of the observations and conclu-sions from 2008 shared task hold as well for thecurrent results.
For details, we will leave it to thereader to interpret the architectures and methods14OverallDDDPAPAPAJointMLSystemaArch.bArch.Comb.InferencecArch.Comb.InferenceLearning/Opt.MethodsZhaoPAIC(SRL-only)(SRL-only)(SRL-only)classnogreedy/globalsearch(SRL-only)MENugues(PC+AI+AC)+AIC(SRL-only)(SRL-only)(SRL-only)classnobeamsearch+reranking(SRL-only)L2-regularizedlin.regressionChenP+PC+AI+ACgraphpartiallyMSTCL/Eclassnogreedy(?)noMECheD+PC+AICgraphnoMSTHOEclassnoILPnoSVM,MEMerloDPAIC+Dgenerative,transnobeamsearchtransnobeamsearchsynchronizedderivationISBNMeza-RuizPAIC(SRL-only)(SRL-only)(SRL-only)MarkovLNnoCuttingPlane(SRL-only)MIRABohnetD+AI+AC+PCgraphnoMSTC+rearrangeclassnogreedynoSVM(MIRA)AsaharaD+PIC+AICgraphnoMSTCclassnon-bestrelax.noperceptronDaiD+PC+ACgraphnoMSTCclassnoprobiterativeMEZhangD+AI+AC+PCgraphnoMSTEclassnoclassificationnoMIRA,MELuLiD+(PC||AIC)graphforeachlang.MSTCL/E,MSTEclassnogreedynoMEBaoliLiPC+AIC(SRL-only)(SRL-only)(SRL-only)classnogreedy(SRL-only)SVM,kNN,MEVallejod[D+P+A]C+DIclassnorerankingclassnorerankingunifiedlabelsMBLMoreauD+PI+Clustering+AI+AC(SRL-only)(SRL-only)(SRL-only)classnoCRF(SRL-only)CRFLlu?
?sD+DAIC+PCgraphnoMSTEgraphnoMSTEyes,MSTEAvg.PerceptronTa?ckstro?mD+PI+AI+AC+ConstraintSatisfaction(SRL-only)(SRL-only)(SRL-only)classnogreedy(SRL-only)SVMRenD+PC+AICtransnogreedyclassnogreedynoSVM(Malt),MEZemanDI+DC+PC+AI+ACtransnogreedywithheuristicsclassnogreedynocooccurrenceTable7:SummaryofsystemarchitecturesfortheCoNLL-2009sharedtask;allsystemsareincluded.SRL-onlysystemsdonothavetheDcolumnsandtheJointLearing/Opt.columnsfilledin.ThesystemsaresortedbythesemanticlabeledF 1scoreaveragedoverallthelanguages(sameasinTable6).Onlythesystemsthathaveacorrespondingpaperintheproceedingsareincluded.Acronymsused:D-syntacticdependencies,P-predicate,A-argument,I-identification,C-classification.Overallarch.standsforthecompletesystemarchitecture;DArch.standsforthearchitectureofthesyntacticparser;DComb.indicatesifthefinalparseroutputwasgeneratedusingparsercombination;DInferencestandsforthetypeofinferenceusedforsyntacticparsing;PAArch.standsthetypeofarchitectureusedforPAIC;PAComb.indicatesifthePAoutputwasgeneratedthroughsystemcombination;PAInferencestandsforthethetypeofinferenceusedforPAIC;JointLearning/Opt.indicatesifsomeformofjointlearningoroptimizationwasimplementedforthesyntactic+semanticglobaltask;MLMethodsliststheMLmethodsusedthroughoutthecompletesystem.aAuthorsoftwosystems:?Brown?and?Lin?didn?tsubmitapaper,sotheirsystems?architecturesareunknown.b Thesymbol+indicatessequentialprocessing(otherwise,parallel/joint).The||meansthatseveraldifferentarchitecturesspanningmultiplesubtasksraninparallel.cMSTCL/EasusedbyMcDonald(2005),MSTCbyCarreras(2007),MSTEbyEisner(2000),MSTHOE=MSTEwithhigher-orderfeatures(siblings+allgrandchildren).d Thesystemunifiesthesyntacticandsemanticlabelsintoonelabel,andtrainsclassifiersoverthem.Itisthusdifficulttosplitthesystemcharacteristicintoa?D?/?PA?part.15when comparing Table 7 with the Tables 5 and 6).6 ConclusionThis year?s task has been demanding in several re-spects, but certainly the most difficulty came fromthe fact that participants had to tackle all seven lan-guages.
It is encouraging that despite this added af-fort the number of participating systems has beenalmost the same as last year (20 vs. 22 in 2008).There are several positive outcomes from thisyear?s enterprise:?
we have prepared a unified format and data forseveral very different lanaguages, as a basisfor possible extensions towards other languagesand unified treatment of syntactic depenndeciesand semantic role labeling across natural lan-guages;?
20 participants have produced SRL results forall seven languages, using several differentmethods, giving hope for a combined systemwith even substantially better performance;?
initial results have been provided for three lan-guages on out-of-domain data (being in factquite close to the in-domain results).Only four systems tried to apply what can be de-scribed as joint learning for the syntactic and seman-tic parts of the task.
(Morante et al, 2009) use a truejoint learning formulation that phrases syntactico-semantic parsing as a series of classification wherethe class labels are concatenations of syntactic andsemantic edge labels.
They predict (a), the set ofsyntactico-semantic edge labels for each pair of to-kens; (b), the set of incoming syntactico-semanticedge labels for each individual token; and (c), theexistence of an edge between each pair of tokens.Subsequently, they combine the (possibly conflict-ing) output of the three classifiers by a ranking ap-proach to determine the most likely structure thatmeets all well-formedness constraints.
(Llu?
?s et al,2009) present a joint approach based on an exten-sion of Eisner?s parser to accommodate also seman-tic dependency labels.
This architecture is similarto the one presented by the same authors in the pastedition, with the extension to a second-order syn-tactic parsing and a particular setting for Catalanand Spanish.
(Gesmundo et al, 2009) use an in-cremental parsing model with synchronous syntac-tic and semantic derivations and a joint probabilitymodel for syntactic and semantic dependency struc-tures.
The system uses a single input queue but twoseparate stacks and synchronizes syntactic and se-mantic derivations at every word.
The synchronousderivations are modeled with an Incremental Sig-moid Belief Network that has latent variables forboth syntactic and semantic states and connectionsfrom syntax to semantics and vice versa.
(Dai etal., 2009) designed an iterative system to exploitthe inter-connections between the different subtasksof the CoNLL shared task.
The idea is to decom-pose the joint learning problem into four subtasks?
syntactic dependency identification, syntactic de-pendency labeling, semantic dependency identifica-tion and semantic dependency labeling.
The initialstep is to use a pipeline approach to use the input ofone subtask as input to the next, in the order speci-fied.
The iterative steps then use additional featuresthat are not available in the initial step to improve theaccuracy of the overall system.
For example, in theiterative steps, semantic information becomes avail-able as features to syntactic parsing, so on and soforth.Despite these results, it is still not clear whetherjoint learning has a significant advantage over otherapproaches (and if yes, then for what languages).
Itis thus necessary to carefully plan the next sharedtasks; it might be advantageous to bring up a sim-ilar task in the future once again, and/or couple itwith selected application(s).
There, (we hope) thebenefits of the dependency representation combinedwith semantic roles the way we have formulated itin 2008 and 2009 will really show up.AcknowledgmentsWe would like to thank the Linguistic Data Consor-tium, mainly to Denise DiPersio, Tony Castelettoand Christopher Cieri for their help and handlingof invoicing and distribution of the data for whichLDC has a license.
For all of the trial, training andevaluation data they had to act a very short notice.All the data has been at the participants?
disposal(again) free of charge.
We are grateful to all of themfor LDC?s continuing support of the CoNLL Shared16Tasks.We would also like to thank organizers of the pre-vious four shared tasks: Sabine Buchholz, XavierCarreras, Ryan McDonald, Amit Dubey, Johan Hall,Yuval Krymolowski, Sandra Ku?bler, Erwin Marsi,Jens Nilsson, Sebastian Riedel and Deniz Yuret.This shared task would not have been possible with-out their previous effort.We also acknowledge the support of the M?SMTof the Czech Republic, projects MSM0021620838and LC536; the Grant Agency of the Academy ofsciences of the Czech Republic 1ET201120505 (forJan Hajic?, Jan ?Ste?pa?nek and Pavel Stran?a?k).Llu?
?s Ma`rquez and M. Anto`nia Mart??
partici-pation was supported by the Spanish Ministry ofEducation and Science, through the OpenMT andTextMess research projects (TIN2006-15307-C03-02, TIN2006-15265-C06-06).The following individuals directly contributed tothe Chinese Treebank (in alphabetic order): MeiyuChang, Fu-Dong Chiou, Shizhe Huang, Zixin Jiang,Tony Kroch, Martha Palmer, Mitch Marcus, FeiXia, Nianwen Xue.
The contributors to the Chi-nese Proposition Bank include (in alphabetic order):Meiyu Chang, Gang Chen, Helen Chen, Zixin Jiang,Martha Palmer, Zhiyi Song, Nianwen Xue, Ping Yu,Hua Zhong.
The Chinese Treebank and the ChineseProposition Bank were funded by DOD, NSF andDARPA.Adam Meyers?
work on the shared task has beensupported by the NSF Grant IIS-0534700 ?StructureAlignment-based MT.
?We thank the Mainichi Newspapers for the per-mission of distributing the sentences of the KyotoUniversity Text Corpus for this shared task.ReferencesSabine Brants, Stefanie Dipper, Silvia Hansen, WolfgangLezius, and George Smith.
2002.
The TIGER tree-bank.
In Proceedings of the Workshop on Treebanksand Linguistic Theories, Sozopol.Aljoscha Burchardt, Katrin Erk, Anette Frank, AndreaKowalski, Sebastian Pado?, and Manfred Pinkal.
2006.The SALSA corpus: a German corpus resource forlexical semantics.
In Proceedings of the 5th Interna-tional Conference on Language Resources and Evalu-ation (LREC-2006), Genoa, Italy.Xavier Carreras.
2007.
Experiments with a higher-order projective dependency parser.
In Proceedings ofEMNLP-CoNLL 2007, pages 957?961, June.
Prague,Czech Republic.Montserrat Civit, M. Anto`nia Mart?
?, and Nu?ria Buf??.2006.
Cat3LB and Cast3LB: from constituents todependencies.
In Proceedings of the 5th Interna-tional Conference on Natural Language Processing,FinTAL, pages 141?153, Turku, Finland.
Springer Ver-lag, LNAI 4139.Qifeng Dai, Enhong Chen, and Liu Shi.
2009.
An it-erative approach for joint dependency parsing and se-mantic role labeling.
In Proceedings of the 13th Con-ference on Computational Natural Language Learning(CoNLL-2009), June 4-5, Boulder, Colorado, USA.June 4-5.Jason Eisner.
2000.
Bilexical grammars and their cubic-time parsing algorithms.
In Harry Bunt and AntonNijholt, editors, Advances in Probabilistic and OtherParsing Tehcnologies, pages 29?62.
Kluwer AcademicPublishers.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
The MIT Press, Cambridge.Andrea Gesmundo, James Henderson, Paola Merlo, andIvan Titov.
2009.
A latent variable model of syn-chronous syntactic-semantic parsing for multiple lan-guages.
In Proceedings of the 13th Conference onComputational Natural Language Learning (CoNLL-2009), June 4-5, Boulder, Colorado, USA.
June 4-5.Jan Hajic?, Jarmila Panevova?, Zden?ka Ures?ova?, AlevtinaBe?mova?, Veronika Kola?r?ova?- ?Rezn??c?kova?
?, and PetrPajas.
2003.
PDT-VALLEX: Creating a Large-coverage Valency Lexicon for Treebank Annotation.In J. Nivre and E. Hinrichs, editors, Proceedings of TheSecond Workshop on Treebanks and Linguistic Theo-ries, pages 57?68, Vaxjo, Sweden.
Vaxjo UniversityPress.Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, PetrSgall, Petr Pajas, Jan ?Ste?pa?nek, Jir???
Havelka, MarieMikulova?, and Zdene?k ?Zabokrtsky?.
2006.
Prague De-pendency Treebank 2.0.Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida.2002.
Construction of a Japanese relevance-taggedcorpus.
In Proceedings of the 3rd InternationalConference on Language Resources and Evaluation(LREC-2002), pages 2008?2013, Las Palmas, CanaryIslands.Xavier Llu?
?s, Stefan Bott, and Llu?
?s Ma`rquez.
2009.A second-order joint eisner model for syntactic andsemantic dependency parsing.
In Proceedings ofthe 13th Conference on Computational Natural Lan-guage Learning (CoNLL-2009), June 4-5, Boulder,Colorado, USA.
June 4-5.17M.
P. Marcus, B. Santorini, and M. A. Marcinkiewicz.1994.
Building a large annotated corpus of en-glish: The penn treebank.
Computational Linguistics,19(2):313?330.Llu?
?s Ma`rquez, Luis Villarejo, M. Anto`nia Mart?
?, andMariona Taule?.
2007.
SemEval-2007 Task 09: Mul-tilevel semantic annotation of catalan and spanish.In Proceedings of the 4th International Workshop onSemantic Evaluations (SemEval-2007), pages 42?47,Prague, Czech Republic.M.
Anto`nia Mart?
?, Mariona Taule?, Llu?
?s Ma`rquez, andManu Bertran.
2007.
Anotacio?n semiautoma?ticacon papeles tema?ticos de los corpus CESS-ECE.Procesamiento del Lenguaje Natural, SEPLN Journal,38:67?76.Ryan McDonald, Fernando Pereira, Jan Hajic?, and KirilRibarov.
2005.
Non-projective dependency parsingusing spanning tree algortihms.
In Proceedings ofNAACL-HLT?05, Vancouver, Canada, pages 523?530.A.
Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-ska, B.
Young, and R. Grishman.
2004.
The Nom-Bank Project: An Interim Report.
In NAACL/HLT2004 Workshop Frontiers in Corpus Annotation,Boston.Roser Morante, Vincent Van Asch, and Antal van denBosch.
2009.
A simple generative pipeline approachto dependency parsing and semantic role labeling.
InProceedings of the 13th Conference on ComputationalNatural Language Learning (CoNLL-2009), Boulder,Colorado, USA.
June 4-5.Joakim Nivre, Johann Hall, Sandra Ku?bler, Ryan Mc-Donald, Jens Nilsson, Sebastian Riedel, and DenizYuret.
2007.
The conll 2007 shared task on depen-dency parsing.
In Proceedings of the EMNLP-CoNLL2007 Conference, pages 915?932, Prague, Czech Re-public.Sebastian Pado and Mirella Lapata.
2005.
Cross-lingualprojection of role-semantic information.
In Proceed-ings of the Human Language Technology Conferenceand Conference on Empirical Methods in Natural Lan-guage Processing (HLT/EMNLP-2005), pages 859?866, Vancouver, BC.Petr Pajas and Jan ?Ste?pa?nek.
2008.
Recent advances ina feature-rich framework for treebank annotation.
InThe 22nd International Conference on ComputationalLinguistics - Proceedings of the Conference (COL-ING?08), pages 673?680, Manchester.M.
Palmer, D. Gildea, and P. Kingsbury.
2005.
TheProposition Bank: An annotated corpus of semanticroles.
Computational Linguistics, 31(1):71?106.Helmut Schmid.
1994.
Probabilistic part-of-speech tag-ging using decision trees.
In Proceedings of Interna-tional Conference on New Methods in Language Pro-cessing.Drahom?
?ra ?Johanka?
Spoustova?, Jan Hajic?, Jan Raab,and Miroslav Spousta.
2009.
Semi-supervised train-ing for the averaged perceptron POS tagger.
In Pro-ceedings of the European ACL Cenference EACL?09,Athens, Greece.Mihai Surdeanu, Richard Johansson, Adam Meyers,Llu?
?s Ma`rquez, and Joakim Nivre.
2008.
The CoNLL-2008 shared task on joint parsing of syntactic and se-mantic dependencies.
In Proceedings of the 12th Con-ference on Computational Natural Language Learning(CoNLL-2008), pages 159?177.Mariona Taule?, Maria Anto`nia Mart?
?, and Marta Re-casens.
2008.
AnCora: Multilevel Annotated Corporafor Catalan and Spanish.
In Proceedings of the 6thInternational Conference on Language Resources andEvaluation (LREC-2008), Marrakesh, Morroco.Martin ?Cmejrek, Jan Cur??
?n, Jan Hajic?, Jir???
Havelka,and Vladislav Kubon?.
2004.
Prague Czech-EnglishDependency Treebank: Syntactically Anntoated Re-sources for Machine Translation.
In Proceedings ofthe 4th International Conference on Language Re-sources and Evaluation (LREC-2004), pages 1597?1600, Lisbon, Portugal.W.
N. Francis and H. Kucera.
1964.
Brown Corpus Man-ual of Information to accompany A Standard Corpusof Present-Day Edited American English, for use withDigital Computers.
Revised 1971, Revised and Am-plified 1979, available at www.clarinet/brown.R.
Weischedel and A. Brunstein.
2005.
BBN pronouncoreference and entity type corpus.
Technical report,Lin- guistic Data Consortium.Nianwen Xue and Martha Palmer.
2009.
Adding seman-tic roles to the Chinese Treebank.
Natural LanguageEngineering, 15(1):143?172.Nianwen Xue, Fei Xia, Fu Dong Chiou, and MarthaPalmer.
2005.
The Penn Chinese TreeBank: PhraseStructure Annotation of a Large Corpus.
Natural Lan-guage Engineering, 11(2):207?238.Andrea Zielinski and Christian Simon.
2008.
Morphisto:An open-source morphological analyzer for german.In Proceedings of the Conference on Finite State Meth-ods in Natural Language Processing.18
