Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 69?73,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsPassive-Aggressive Sequence Labeling with Discriminative Post-Editingfor Recognising Person Entities in TweetsLeon DerczynskiUniversity of Sheffieldleon@dcs.shef.ac.ukKalina BontchevaUniversity of Sheffieldkalina@dcs.shef.ac.ukAbstractRecognising entities in social media text isdifficult.
NER on newswire text is conven-tionally cast as a sequence labeling prob-lem.
This makes implicit assumptions re-garding its textual structure.
Social me-dia text is rich in disfluency and oftenhas poor or noisy structure, and intuitivelydoes not always satisfy these assumptions.We explore noise-tolerant methods for se-quence labeling and apply discriminativepost-editing to exceed state-of-the-art per-formance for person recognition in tweets,reaching an F1 of 84%.1 IntroductionThe language of social media text is unusualand irregular (Baldwin et al., 2013), with mis-spellings, non-standard capitalisation and jargon,disfluency and fragmentation.
Twitter is one of thesources of social media text most challenging forNLP (Eisenstein, 2013; Derczynski et al., 2013).In particular, traditional approaches to NamedEntity Recognition (NER) perform poorly ontweets, especially on person mentions ?
for exam-ple, the default model of a leading system reachesan F1 of less than 0.5 on person entities in a ma-jor tweet corpus.
This indicates a need for ap-proaches that can cope with the linguistic phe-nomena apparently common among social mediaauthors, and operate outside of newswire with itscomparatively low linguistic diversity.So, how can we adapt?
This paper contributestwo techniques.
Firstly, it demonstrates that en-tity recognition using noise-resistant sequence la-beling outperforms state-of-the-art Twitter NER,although we find that recall is consistently lowerthan precision.
Secondly, to remedy this, we intro-duce a method for automatically post-editing theresulting entity annotations by using a discrimina-tive classifier.
This improves recall and precision.2 BackgroundNamed entity recognition is a well-studied prob-lem, especially on newswire and other long-document genres (Nadeau and Sekine, 2007; Rati-nov and Roth, 2009).
However, experiments showthat state-of-the-art NER systems from these gen-res do not transfer well to social media text.For example, one of the best performinggeneral-purpose named entity recognisers (hereonreferred to as Stanford NER) is based on linear-chain conditional random fields (CRF) (Finkel etal., 2005).
The model is trained on newswiredata and has a number of optimisations, includ-ing distributional similarity measures and sam-pling for remote dependencies.
While excellenton newswire (overall F1 90%), it performs poorlyon tweets (overall F1 44%) (Ritter et al., 2011).Rule-based named entity recognition has per-formed a little better on tweets.
Another general-purpose NER system, ANNIE (Cunningham et al.,2002), reached F1 of 60% over the same data (Der-czynski et al., 2013); still a large difference.These difficulties spurred Twitter-specific NERresearch, much of which has fallen into two broadclasses: semi-supervised CRF, and LDA-based.Semi-supervised CRF: Liu et al.
(2011) com-pare the performance of a person name dictio-nary (F1 of 33%) to a CRF-based semi-supervisedapproach (F1 of 76% on person names), using adataset of 12 245 tweets.
This, however, is basedon a proprietary corpus, and cannot be comparedto, since the system is also not available.Another similar approach is TwiNER (Li et al.,2012), which is focused on a single topic streamas opposed to general-purpose NER.
This leadsto high performance for a topic-sensitive classi-fier trained to a particular stream.
In contrast wepresent a general-purpose approach.
Further, weextract a specific entity class, where TwiNER per-forms entity chunking and no classification.69LDA and vocabularies: Ritter et al.
(2011)?sT-NER system uses 2,400 labelled tweets, unla-belled data and Linked Data vocabularies (Free-base), as well as co-training.
These techniqueshelped but did not bring person recognition accu-racy above the supervised MaxEnt baseline in theirexperiments.
We use this system as our baseline.3 Experimental Setup3.1 CorpusThe experiments combine person annotationsfrom three openly-available datasets: Ritter etal.
(2011), UMBC (Finin et al., 2010) andMSM2013 (Basave et al., 2013).
In line with pre-vious research (Ritter et al., 2011), annotations on@mentions are filtered out.
The placeholder to-kens in MSM data (i.e.
MENTION , HASHTAG ,URL ) are replaced with @Mention, #hashtag,and http://url/, respectively, to give case and char-acter n-grams more similar to the original values.The total corpus has 4 285 tweets, around a thirdthe size of that in Liu et al.
(2011).
This datasetcontains 86 352 tokens with 1 741 entity mentions.Person entity recognition was chosen as it is achallenging entity type.
Names of persons popularon Twitter change more frequently than e.g.
loca-tions.
Person names also tend to have a long tail,not being confined to just public figures.
Lastly,although all three corpora cover different entitytypes, they all have Person annotations.3.2 Labeling SchemeFollowing Li et al.
(2009) we used two-class IO la-beling, where each token is either in-entity or out-of-entity.
In their NER work, this performed betterthan the alternative BIO format, since data sparsityis reduced.
The IO scheme has the disadvantageof being unable to distinguish cases where multi-ple different entities of the same type follow eachother without intervening tokens.
This situation isuncommon and does not arise in our dataset.3.3 FeaturesThe Stanford NER tool was used for feature gen-eration.
When required, nominal values were con-verted to sparse one-hot vectors.
Features formodelling context are included (e.g.
ngrams, ad-joining labels).
Our feature sets were:base: default Stanford NER features, plus theprevious and next token and its word shape.11Default plus useClassFeature=true, noMidNGrams=true,Figure 1: Training curve for lem.
Diagonal cross(blue) is CRF/PA, vertical cross (red) SVM/UM.lem: with added lemmas, lower-case versionsof tokens, word shape, and neighbouring lemmas(in attempt to reduce feature sparsity & cope betterwith lexical and orthographic noise).
Word shapedescribes the capitalisation and the type of char-acters (e.g.
letters, numbers, symbols) of a word,without specifying actual character choices.
Forexample, Capital may become Ww.These representations are chosen to comparethose that work well for newswire to those withscope for tolerance of noise, prevalent in Twitter.3.4 ClassifiersFor structured sequence labeling, we experimentwith conditional random fields ?
CRF (Laffertyet al., 2001) ?
using the CRFsuite implementa-tion (Okazaki, 2007) and LBFGS.
We also usean implementation of the passive-aggressive CRFfrom CRFsuite, choosing max iterations = 500.Passive-aggressive learning (Crammer et al.,2006) demonstrates tolerance to noise in trainingdata, and can be readily adapted to provide struc-tured output, e.g.
when used in combination withCRF.
Briefly, it skips updates (is passive) whenthe hinge loss of a new weight vector during up-date is zero, but when it is positive, it aggres-sively adjusts the weight vector regardless of therequired step size.
This is integrated into CRF us-ing a damped loss function and passive-aggressive(PA) decisions to choose when to update.
We ex-plore the PA-I variant, where the objective func-tion scales linearly with the slack variable.maxNGramLeng=6, usePrev=true, useNext=true, usePre-vSequences=true, maxLeft=1, useTypeSeqs=true, useType-Seqs2=true, useTypeSeqs3=true, useTypeySequences=true,wordShape=chris2useLC, useDisjunctive=true, lowercaseN-Grams=true, useShapeConjunctions=true70Approach Precision Recall F1Stanford 85.88 50.00 63.20Ritter 77.23 80.18 78.68MaxEnt 86.92 59.09 70.35SVM 77.55 59.16 67.11SVM/UM 73.26 69.63 71.41CRF 82.94 62.39 71.21CRF/PA 80.37 65.57 72.22Table 1: With base features (base)Approach Precision Recall F1Stanford 90.60 60.00 72.19Ritter 77.23 80.18 78.68MaxEnt 91.10 66.33 76.76SVM 88.22 66.58 75.89SVM/UM 81.16 74.97 77.94CRF 89.52 70.52 78.89CRF/PA 86.85 74.71 80.32Table 2: With shape and lemma features (lem)For independent discriminative classification,we use SVM, SVM/UM and a maximum entropyclassifier (MegaM (Daum?e III, 2004)).
SVM isprovided by the SVMlight (Joachims, 1999) im-plementation.
SVM/UM is an uneven marginsSVM model, designed to deal better with imbal-anced training data (Li et al., 2009).3.5 BaselinesThe first baseline is the Stanford NER CRF al-gorithm, the second Ritter?s NER algorithm.
Weadapted the latter to use space tokenisation, topreserve alignment when comparing algorithms.Baselines are trained and evaluated on our dataset.3.6 EvaluationCandidate entity labelings are compared using theCoNLL NER evaluation tool (Sang and Meulder,2003), using precision, recall and F1.
FollowingRitter, we use 25%/75% splits made at tweet, andnot token, level.4 ResultsThe base feature set performs relatively poorlyon all classifiers, with only MaxEnt beating abaseline on any score (Table 1).
However, allachieve a higher F1 score than the default Stan-ford NER.
Of these classifiers, SVM/UM achievedthe best precision and CRF/PA ?
the best F1.
Thisdemonstrates that the noise-tolerance adaptationsto SVM and CRF (uneven margins and passive-aggressive updates, respectively) did provide im-provements over the original algorithms.Results using the extended features (lem) areshown in Table 2.
All classifiers improved, in-Entity length (tokens) Count1 6102 10653 514 15Table 3: Distribution of person entity lengths.cluding the baseline Stanford NER system.
TheSVM/UM and CRF/PA adaptations continued tooutperform the vanilla models.
With these fea-tures, MaxEnt achieved highest precision and CRFvariants beat both baselines, with a top F1 of80.32%.
We continue using the lem feature set.5 Discriminative Post-EditingPrecision is higher than recall for most systems,especially the best CRF/PA (Table 2).
To improverecall, potential entities are re-examined in post-editing (Gadde et al., 2011).
Manual post-editingimproves machine translation output (Green et al.,2013); we train an automatic editor.We adopt a gazetteer-based approach to trig-gering a discriminative editor, which makes deci-sions about labels after primary classification.
Thegazetteer consists of the top 200 most commonnames in English speaking countries.
The firstnames of popular figures over the past two years(e.g.
Helle, Barack, Scarlett) are also included.This gives 470 case-sensitive trigger terms.Often the trigger term is just the first in a se-quence of tokens that make up the person name.As can be seen from the entity length statisticsshown in Table 3, examining up to two tokens cov-ers most (96%) person names in our corpus.
Basedon this observation, we look ahead just one extratoken beyond the trigger term.
This gives a to-ken sub-sequence that was marked as out-of-entityby the original NER classifier.
Its constituents be-come candidate person name tokens.Candidates are then labeled using a high-recallclassifier.
The classifier should be instance-based,since we are not labeling whole sequences.
Wechose SVM with variable cost (Morik et al., 1999),which can be adjusted to prefer high recall.To train this classifier, we extract a subset of in-stances from the current training split as follows.Each trigger term is included.
Also, if the trig-ger term is labeled as an entity, each subsequentin-entity token is also included.
Finally, the nextout-of-entity token is also included, to give exam-ples of when to stop.
For example, these tokensare either in or out of the training set:71OverallMethod Missed entity F1 P R F1No editing - plain CRF/PA 0.00 86.85 74.71 80.32Na?
?ve: trigger token only 5.82 86.61 78.91 82.58Na?
?ve: trigger plus one 6.05 81.26 82.08 81.67SVM editor, Cost = 0.1 78.26 87.38 79.16 83.07SVM editor, Cost = 0.5 89.72 87.17 80.30 83.60SVM editor, Cost = 1.0 90.74 87.19 80.43 83.67SVM editor, Cost = 1.5 92.73 87.23 80.69 83.83SVM editor, Cost = 2.0 92.73 87.23 80.69 83.83Table 4: Post-editing performance.
Higher Cost sacrifices precision for recall.Miley O inHeights O outMiley PERSON inCyrus PERSON inis O infamous O outWhen post-editing, the window is any triggerterm and the following token, regardless of initiallabel.
The features used were exactly the same aswith the earlier experiment, using the lem set.
Thisis compared with two na?
?ve baselines: always an-notating trigger terms as Person, and always anno-tating trigger terms and the next token as Person.Results are shown in Table 4.
Na?
?ve editingbaselines had F1 on missed entities of around 6%,showing that post-editing needs to be intelligent.At Cost = 1.5, recall increased to 80.69, ex-ceeding the Ritter recall of 80.18 (raising Cost be-yond 1.5 had no effect).
This setup gave good ac-curacy on previously-missed entities (second col-umn) and improved overall F1 to 83.83.
It alsogave better precision and recall than the best na?
?vebaseline (trigger-only), and 6% absolute higherprecision than trigger plus one.
This is a 24.2% re-duction in error over the Ritter baseline (F1 78.68),and a 17.84% error reduction compared to the bestnon-edited system (CRF/PA+lem).6 Error AnalysisWe examine two types of classification error: falsepositives (spurious) and false negatives (missed).False positives occur most often where non-person entities are mentioned.
This occurred withmentions of organisations (Huff Post), locations(Galveston) and products (Exodus Porter).
De-scriptive titles were also sometimes mis-includedin person names (Millionaire Rob Ford).
Names ofpersons used in other forms also presented as falsepositives (e.g.
Marie Claire ?
a magazine).
Pol-ysemous names (i.e.
words that could have otherfunctions, such as a verb) were also mis-resolved(Mark).
Finally, proper nouns referring to groupswere sometimes mis-included (Haitians).Despite these errors, precision almost alwaysremained higher than recall over tweets.
We usein-domain training data, and so it is unlikely thatthis is due to the wrong kinds of person being cov-ered in the training data ?
as can sometimes be thecase when applying tools trained on newswire.False negatives often occurred around incorrectcapitalisation and spelling, with unusual names,with ambiguous tokens and in low-context set-tings.
Both omitted and added capitalisation gavefalse negatives (charlie gibson, or KANYE WEST).Spelling errors also led to missed names (Rus-sel Crowe).
Ambiguous names caused false neg-atives and false positives; our approach missedmark used as a name, and the surname of JackStraw.
Unusual names with words typically usedfor other purposes were also not always correctlyrecognised (e.g.
the Duck Lady, or the last twotokens of Spicy Pickle Jr.).
Finally, names withfew or no context words were often missed (Video:Adele 21., and 17-9-2010 Tal al-Mallohi, a 19-).7 ConclusionFinding named entities in social media text, par-ticularly tweets, is harder than in newswire.
Thispaper demonstrated that adapted to handle noisyinput is useful in this scenario.
We achieved thegood results using CRF with passive-aggressiveupdates.
We used representations rich in wordshape and contextual features and achieved highprecision with moderate recall (65.57?74.71).To improve recall, we added a post-editing stagewhich finds candidate person names based on trig-ger terms and re-labels them using a cost-adjustedSVM.
This flexible and re-usable approach lead toa final reduction in error rate of 24.2%, giving per-formance well above that of comparable systems.Acknowledgment This work received fundingfrom EU FP7 under grant agreement No.
611233,Pheme.
We thank Chris Manning and John Bauerof Stanford University for help with the NER tool.72ReferencesT.
Baldwin, P. Cook, M. Lui, A. MacKinlay, andL.
Wang.
2013.
How noisy social media text,how diffrnt social media sources.
In Proceedings ofthe Sixth International Joint Conference on NaturalLanguage Processing, pages 356?364.
ACL.A.
E. C. Basave, A. Varga, M. Rowe, M. Stankovic,and A.-S. Dadzie.
2013.
Making Sense of Micro-posts (# MSM2013) Concept Extraction Challenge.In Proceedings of the Concept Extraction Challengeat the Workshop on ?Making Sense of Microposts?,volume 1019.
CEUR-WS.K.
Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,and Y.
Singer.
2006.
Online passive-aggressive al-gorithms.
Journal of Machine Learning Research,7:551?585.H.
Cunningham, D. Maynard, K. Bontcheva, andV.
Tablan.
2002.
GATE: an Architecture for Devel-opment of Robust HLT Applications.
In Proceed-ings of the 40th Annual Meeting on Association forComputational Linguistics, pages 168?175.H.
Daum?e III.
2004.
Notes on CG and LM-BFGS optimization of logistic regression.
Pa-per available at http://pub.hal3.name#daume04cg-bfgs, implementation available athttp://hal3.name/megam/, August.L.
Derczynski, D. Maynard, N. Aswani, andK.
Bontcheva.
2013.
Microblog-Genre Noise andImpact on Semantic Annotation Accuracy.
In Pro-ceedings of the 24th ACM Conference on Hypertextand Social Media.
ACM.J.
Eisenstein.
2013.
What to do about bad languageon the internet.
In Proceedings of the 2013 Con-ference of the North American Chapter of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, pages 359?369.
Association forComputational Linguistics.T.
Finin, W. Murnane, A. Karandikar, N. Keller, J. Mar-tineau, and M. Dredze.
2010.
Annotating namedentities in Twitter data with crowdsourcing.
In Pro-ceedings of the NAACL HLT 2010 Workshop on Cre-ating Speech and Language Data with Amazon?sMechanical Turk, pages 80?88.J.
Finkel, T. Grenager, and C. Manning.
2005.
In-corporating non-local information into informationextraction systems by Gibbs sampling.
In Proceed-ings of the 43rd Annual Meeting of the Associationfor Computational Linguistics, pages 363?370.
As-sociation for Computational Linguistics.P.
Gadde, L. Subramaniam, and T. A. Faruquie.
2011.Adapting a WSJ trained part-of-speech tagger tonoisy text: preliminary results.
In Proceedings ofthe 2011 Joint Workshop on Multilingual OCR andAnalytics for Noisy Unstructured Text Data.
ACM.S.
Green, J. Heer, and C. D. Manning.
2013.
The effi-cacy of human post-editing for language translation.In Proceedings of the SIGCHI Conference on Hu-man Factors in Computing Systems, pages 439?448.ACM.T.
Joachims.
1999.
Svmlight: Support vector machine.SVM-Light Support Vector Machine http://svmlight.joachims.
org/, University of Dortmund, 19(4).J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Condi-tional Random Fields: Probabilistic Models for Seg-menting and Labeling Sequence Data.
In Proceed-ings of the Eighteenth International Conference onMachine Learning, pages 282?289, San Francisco:Morgan Kaufmann.Y.
Li, K. Bontcheva, and H. Cunningham.
2009.Adapting SVM for Data Sparseness and Imbalance:A Case Study on Information Extraction.
NaturalLanguage Engineering, 15(2):241?271.C.
Li, J. Weng, Q.
He, Y. Yao, A. Datta, A.
Sun, andB.-S. Lee.
2012.
Twiner: named entity recogni-tion in targeted twitter stream.
In Proceedings ofthe 35th international ACM SIGIR conference onResearch and development in information retrieval,pages 721?730.
ACM.X.
Liu, S. Zhang, F. Wei, and M. Zhou.
2011.
Rec-ognizing named entities in tweets.
In Proceedingsof the 49th Annual Meeting of the Association forComputational Linguistics: Human Language Tech-nologies, pages 359?367.K.
Morik, P. Brockhausen, and T. Joachims.
1999.Combining statistical learning with a knowledge-based approach-a case study in intensive care moni-toring.
In ICML, volume 99, pages 268?277.D.
Nadeau and S. Sekine.
2007.
A survey of namedentity recognition and classification.
LingvisticaeInvestigationes, 30(1):3?26.N.
Okazaki.
2007.
CRFsuite: a fast implementation ofConditional Random Fields (CRFs).L.
Ratinov and D. Roth.
2009.
Design challenges andmisconceptions in named entity recognition.
In Pro-ceedings of the Thirteenth Conference on Computa-tional Natural Language Learning, pages 147?155.Association for Computational Linguistics.A.
Ritter, S. Clark, Mausam, and O. Etzioni.
2011.Named entity recognition in tweets: An experimen-tal study.
In Proc.
of Empirical Methods for NaturalLanguage Processing (EMNLP), Edinburgh, UK.E.
F. T. K. Sang and F. D. Meulder.
2003.
Introduc-tion to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition.
In Pro-ceedings of CoNLL-2003, pages 142?147.
Edmon-ton, Canada.73
