Proceedings of the 7th Workshop on Statistical Machine Translation, pages 460?467,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsLeave-One-Out Phrase Model Training for Large-Scale DeploymentJoern WuebkerHuman Language Technologyand Pattern Recognition GroupRWTH Aachen University, Germanywuebker@cs.rwth-aachen.deMei-Yuh Hwang, Chris QuirkMicrosoft CorporationRedmond, WA, USA{mehwang,chrisq}@microsoft.comAbstractTraining the phrase table by force-aligning(FA) the training data with the reference trans-lation has been shown to improve the phrasaltranslation quality while significantly reduc-ing the phrase table size on medium sizedtasks.
We apply this procedure to severallarge-scale tasks, with the primary goal of re-ducing model sizes without sacrificing transla-tion quality.
To deal with the noise in the auto-matically crawled parallel training data, we in-troduce on-demand word deletions, insertions,and backoffs to achieve over 99% successfulalignment rate.
We also add heuristics to avoidany increase in OOV rates.
We are able to re-duce already heavily pruned baseline phrasetables by more than 50% with little to nodegradation in quality and occasionally slightimprovement, without any increase in OOVs.We further introduce two global scaling fac-tors for re-estimation of the phrase table viaposterior phrase alignment probabilities anda modified absolute discounting method thatcan be applied to fractional counts.Index Terms: phrasal machine translation, phrasetraining, phrase table pruning1 IntroductionExtracting phrases from large amounts of noisyword-aligned training data for statistical machinetranslation (SMT) generally has the disadvantage ofproducing many unnecessary phrases (Johnson etal., 2007).
These can include poor quality phrases,composite phrases that are concatenations of shorterones, or phrases that are assigned very low proba-bilities, so that they have no realistic chance whencompeting against higher scoring phrase pairs.
Thegoal of this work is two-fold: (i) investigating forcedalignment training as a phrase table pruning methodfor large-scale commercial SMT systems and (ii)proposing several extensions to the training proce-dure to deal with practical issues and stimulate fur-ther research.Generative phrase translation models have the in-herent problem of over-fitting to the training data(Koehn et al, 2003; DeNero et al, 2006).
(Wue-bker et al, 2010) introduce a leave-one-out proce-dure which is shown to counteract over-fitting ef-fects.
The authors report significant improvementson the German-English Europarl data with the ad-ditional benefit of a severely reduced phrase tablesize.
This paper investigates its impact on a num-ber of commercial large-scale systems and presentsseveral extensions.The first extension is to deal with the highly noisytraining data, which is automatically crawled andsentence aligned.
The noise and the baseline prun-ing of the phrase table lead to low success rateswhen aligning the source sentence with the targetsentence.
We introduce on-demand word deletions,insertions, and backoff phrases to increase the suc-cess rate so that we can cover essentially the en-tire training data.
Secondly, phrase table pruningmakes out-of-vocabulary (OOV) issues even morepronounced.
To avoid an increased OOV rate, weretrieve single-word translations from the baselinephrase table.
Lastly, we propose two global scaling460factors to allow fine-tuning of the phrase counts inan attempt to re-estimate the translation probabili-ties and a modification of absolute discounting thatcan be applied to fractional counts.Our main contribution is applying forced-alignment on the training data to prune the phrasetable.
The rationale behind this is that by decodingthe training data, we can identify the phrases that areactually used by the decoder.
Further, we presentpreliminary experiments on re-estimating the chan-nel models in the phrase table based on counts ex-tracted from the force-aligned data.This work is organized as follows.
We discuss re-lated work in Section 2, describe our decoder andtraining procedure in Section 3 and the experimentsin Section 4.
A conclusion and discussion of futurework is given in Section 5.2 Related WorkForce-aligning bilingual data has been explored asa means of model training in previous work.
Lianget al (2006) use it for their bold updating strategyto update discriminative feature weights.
Utilizingforce-aligned data to train a unigram phrase segmen-tation model is proposed by Shen et al (2008).
Wue-bker et al (2010) apply forced alignment to train thephrase table in an EM-like fashion.
They report asignificant reduction in phrase table size.In this work we apply forced alignment trainingas a pure phrase table pruning technique.
Johnsonet al (2007) successfully investigate a number ofpruning methods for the phrase inventory based onsignificance testing.
While their approach is morestraightforward and less elaborate, we argue that ourmethod is directly tailored to the decoding processand works on top of an already heavily pruned base-line phrase table.We further experiment with applying the (scaled)phrase alignment posteriors to train the phrase ta-ble.
A similar idea has been addressed in previouswork, e.g.
(Venugopal et al, 2003; de Gispert et al,2010), where word alignment posterior probabilitiesare leveraged for grammar extraction.Finally, a number of papers describe extendingreal phrase training to the hierarchical machinetranslation paradigm (Blunsom et al, 2008; Cme-jrek et al, 2009; Mylonakis and Sima?an, 2010).3 Phrase Training3.1 DecoderOur translation decoder is similar to the open-sourcetoolkit Moses (Koehn et al, 2007).
It models trans-lation as a log-linear combination of two phrasaland two lexical channel models, an n-gram languagemodel (LM), phrase, word and distortion penaltiesand a lexicalized reordering model.
The decodingcan be summarized as finding the best scoring targetsentence T ?
given a source sentence S:T ?
= argmaxT?i?i log gi(S,T ) (1)where each gi represents one feature (the channelmodels, n-gram, phrase count, etc.).
The modelweights ?i are usually discriminatively learned on adevelopment data set via minimum error rate train-ing (MERT) (Och, 2003).Constraining the decoder to a fixed target sentenceis straightforward.
Each partial hypothesis is com-pared to the reference and discarded if it does notmatch.
The language model feature can be droppedsince all hypotheses lead to the same target sentence.The training data is divided into subsets for parallelalignment.
A bilingual phrase matching is applied tothe phrase table to extract only the subset of entriesthat are pertinent to each subset of training data, formemory efficiency.
For forced alignment training,we set the distortion limit ?
to be larger than in reg-ular translation decoding.
As unlimited distortionleads to very long training times, we compromise onthe following heuristic.
The distortion limit is setto be the maximum of 10, twice that of the baselinesetting, and 1.5 times the maximum phrase length:?
= max{10,2?
(baseline distortion),1.5?
(max phrase length)} (2)To avoid over-fitting, we employ the same leave-one-out procedure as (Wuebker et al, 2010) fortraining.
Here, it is applied on top of the Good-Turing (GT) smoothed phrase table (Foster et al,4612006).
Our phrase table stores the channel proba-bilites and marginal counts for each phrase pair, butnot the discounts applied.
Therefore, for each sen-tence, if the phrase pair (s, t) has a joint count c(s, t)computed from the entire training data, and occursc1(s, t) times in the current sentence, the leave-one-out probability p?
(t|s) for the current sentence willbe:p?
(t|s) =c?
(s, t)?dc?
(s)=c(s, t)?
c1(s, t)?dc(s)?
c1(s)=p(t|s)c(s)?
c1(s, t)c(s)?
c1(s)(3)since p(t|s)c(s) = c(s, t)?d, where d is the GT dis-count value.
In the case where c(s, t) = c1(s, t) (i.e.
(s, t) occurs exclusively in one sentence pair), weuse a very low probability as the floor value.
Weapply leave-one-out discounting to the forward andbackward translation models only, not to the lexicalchannel models.Our baseline phrase extraction applies someheuristic-based pruning strategies.
For example,it prunes offensive translations and many-words tomany-words singletons (i.e.
a joint count of 1 andboth source phrase and target phrase contain mul-tiple words)?.
Finally the forward and backwardtranslation probabilities are smoothed with Good-Turing discounting.3.2 Weak Lambda Training with HighDistortionOur leave-one-out training flowchart can be illus-trated in Figure 1.
To force-align the training datawith good quality, we need a set of trained lambdaweights, as shown in Equation 1.
We can use thelambda weights learned from the baseline system forthat purpose.
However, ideally we want the lambdavalues to be learned under a similar configuration asthe forced alignment.
Therefore, for this purpose werun MERT with the larger distortion limit given inEquation 2.?The pruned entries are nevertheless used in computing jointcounts and marginal counts.Parallel training datawith word-level alignmentsPhrase extraction withheuristic pruningWeak lambda trainingPhrase tableLeave-one-outforced alignment?
1 = {?
}Normal lambda trainingIntersection +OOV RecoverySelected phrasesSelected phrases+Large ?2-grams5-gramsSmall ??
2 = {?
}{uniform ?
}{baseline ?
}Figure 1: Flowchart of forced-alignment phrase training.Additionally, since forced alignment does not usethe language model, we propose to use a weaker lan-guage model for training the lambdas (?1) to be usedin the forced alignment decoding.Using a weaker language model also speeds up thelambda training process, especially when we are us-ing a distortion limit ?
at least twice as high as inthe baseline system.
In our experiments, the base-line system uses an English 5-gram language modeltrained on a large amount of monolingual data.
Thelambda values used for forced alignment are learnedusing the bigram LM trained on the target side of the462parallel corpus for each system.We compared a number of systems using differ-ent degrees of weak models and found out the im-pact on the final system was minimal.
However, us-ing a small bigram LM with large distortion yieldeda stable performance in terms of BLEU, and was25% faster than using a large 5-gram with the base-line distortion.
Because of the speed improvementand its stability, this paper adopts the weak bigramlambda training.3.3 On-demand Word Insertions and DeletionsFor many training sentences the translation decoderis not able to find a phrasal alignment.
We identifiedthe following main reasons for failed alignments:?
Incorrect sentence alignment or sentence seg-mentation by the data crawler,?
OOVs due to initial pruning in the phrase ex-traction phase,?
Faulty word alignments,?
Strongly reordered sentence structure.
That is,the distortion limit during forced alignment istoo restrictive.For some of these cases, discarding the sentencepairs can be seen as implicit data cleaning.
Forothers, there do exist valid sub-sentences that arealigned properly.
We would like to be able to lever-age those sub-sentences, effectively allowing us todo partial sentence removal.
Therefore, we in-troduce on-demand word insertions and deletions.Whenever a partial hypothesis can not be expandedto the next target word t j, with the given phrase ta-ble, we allow the decoder to artificially introduce aphrase pair (null, t j) to insert the target word intothe hypothesis without consuming any source word.These artificial phrase pairs are introduced with ahigh penalty and are ignored when creating the out-put phrase table.
We can also introduce backoffphrase pairs (si, t j) for all source words si that arenot covered so far, also with a fixed penalty.After we reach the end of the target sentence, ifthere are any uncovered source words si, we arti-ficially add the deletion phrase pairs (si,null) witha high penalty.
Introducing on-demand word inser-tions and deletions increases the data coverage toat least 99% of the training sentences on all taskswe have worked on.
Due to the success of inser-tion/deletion phrases, we have not conducted exper-iments using backoff phrases within the scope of thiswork, but leave this to future work.3.4 Phrase Training as PruningThis work concentrates on practical issues with largeand noisy training data.
Our main goal is to ap-ply phrase training to reduce phrase table size with-out sacrificing quality.
We do this by dumping n-best alignments of the training data, where n rangesfrom 100-200.
We prune the baseline phrase table toonly contain phrases that appear in any of the n-bestphrase alignments, leaving the channel probabilitiesunchanged.
That is, the model scores are still esti-mated from the original counts.
We can control thesize of the final phrase table by adjusting the sizeof the n-best list.
Based on the amount of memorywe can afford, we can thus keep the most importantentries in the phrase table.3.5 OOV retrievalWhen performing phrase table pruning as de-scribed in Section 3.4, OOV rates tend to increase.This effect is even more pronounced when dele-tion/insertion phrases are not used, due to the lowalignment success rate.
For commercial applica-tions, untranslated words are a major concern forend users, although it rarely has any impact on BLEUscores.
Therefore, for the final phrase table afterforced alignment training, we check the translationsfor single words in the baseline phrase table.
If anysingle word has no translation in the new table, werecover the top x translations from the baseline table.In practice, we set x = 3.3.6 Fractional Counts and ModelRe-estimationAs mentioned in Section 3.4, for each training sen-tence pair we produce the n-best phrasal alignments.If we interpret the model score of an alignment asits log likelihood, we can weight the count for eachphrase by its posterior probability.
However, as the463log-linear model weights are trained in a discrim-inative fashion, they do not directly correspond toprobabilities.
In order to leverage the model scores,we introduce two scaling factors ?
and ?
that al-low us to shape the count distribution according toour needs.
For one sentence pair, the count for thephrase pair (s, t) is defined asc(s, t)=????
?n?i=1c(s, t|hi) ?exp(?
??(hi))n?j=1exp(?
??
(h j))?????
?, (4)where hi is the i-th hypothesis of the n-best list,?
(hi) the log-linear model score of the alignmenthypothesis hi and c(s, t|hi) the count of (s, t) withinhi.
If ?
= 0, all alignments within the n-best listare weighted equally.
Setting ?
= 0 means that allphrases that are used anywhere in the n-best list re-ceive a count of 1.Absolute discounting is a popular smoothingmethod for relative frequencies (Foster et al, 2006).Its application, however, is somewhat difficult, ifcounts are not required to be integer numbers andcan in fact reach arbitrarily small values.
We pro-pose a minor modification, where the discount pa-rameter d is added to the denominator, rather thansubtracting it from the numerator.
The discountedrelative frequency for a phrase pair (s, t) is computedasp(s|t) =c(s, t)d+?s?c(s?, t)(5)3.7 Round-Two Lambda TrainingAfter the phrase table is pruned with forced align-ment (either re-estimating the channel probabilitiesor not), we recommend a few more iterations oflambda training to ensure our lambda values are ro-bust with respect to the new phrase table.
In ourexperiments, we start from the baseline lambdas andtrain at most 5 more iterations using the baseline dis-tortion and the 5-gram English language model.
Thesettings have to be consistent with the final decod-ing; therefore we are not using weak lambda traininghere.system parallel corpus Dev Test1 WMT(sent.
pairs)it-en 13.0M 2000 5000 3027pt-en 16.9M 2448 5000 1000nl-en 15.0M 499 4996 1000et-en 3.5M 1317 1500 995Table 1: Data sizes of the four systems Italian, Por-tuguese, Dutch and Estonian to English.
All numbersrefer to sentence pairs.Empirically we found the final lambdas (?2) madea very small improvement over the baseline lamb-das.
However, we decided to keep this second roundof lambda training to guarantee its stability acrossall language pairs.4 ExperimentsIn this section, we describe our experiments onlarge-scale training data.
First, we prune the orig-inal phrase table without re-estimation of the mod-els.
We conducted experiments on many languagepairs.
But due to the limited space here, we chose topresent two high traffic systems and the two worstsystems so that readers can set the correct expecta-tion with the worst-case scenario.
The four systemsare: Italian (it), Portuguese (pt), Dutch (nl) and Es-tonian (et), all translating to English (en).4.1 CorporaThe amount of data for the four systems is shown inTable 1.
There are two test sets: Test1 and WMT.Test1 is our internal data set, containing web pagetranslations among others.
WMT is sampled fromthe English side of the benchmark test sets of theWorkshop on Statistical Machine Translation?.
Thesampled English sentences are then manually trans-lated into other languages, as the input to test X-to-English translation.
WMT tends to contain news-like and longer sentences.
The development set (forlearning lambdas) is from our internal data set.
Wemake sure that there is no overlap among the devel-opment set, test sets, and the training set.
?www.statmt.org/wmt09464baseline FA w/ del.
FA w/o del.it-ensuc.rate ?
99.5% 61.2%Test1 42.27 42.05 42.31WMT 30.16 30.19 30.19pt-ensuc.rate ?
99.5% 66.9%Test1 47.55 47.47 47.24WMT 40.74 41.36 41.01nl-ensuc.rate ?
99.6% 79.9%Test1 32.39 31.87 31.18WMT 43.37 43.06 43.38et-ensuc.rate ?
99.1% 73.1%Test1 46.14 46.35 45.77WMT 20.08 19.60 19.83Table 2: BLEU scores of forced-alignment-based phrase-table pruning using weak lambda training.
n-best size is100 except for nl-en, where it is 160.
We contrast forcedalignment with and without on-demand insertion/deletionphrases.
With the on-demand artificial phrases, FA suc-cess rate is over 99%.4.2 Insertion/Deletion PhrasesUnless explicitly stated, all experiments here usedthe weak bigram LMs to obtain the lambdas used forforced alignment, and on-demand insertion/deletionphrases are applied.
For the size of n-best, we usen = 100.
The only exception is the nl-en languagepair, for which we set n = 160 because its phrasedistortion setting is higher than the others and for itshigher number of morphological variations.
Table 2shows the BLEU performance of the four systems, inthe baseline setting and in the forced-alignment set-ting with insertion/deletion phrases and without in-sertion/deletion phrases.
Whether partial sentencesshould be kept or not (via insertion/deletion phrases)depends on the quality of the training data.
Onewould have to run both settings to decide which isbetter for each system.
In all cases, there is littleor no degradation in quality after the table is suffi-ciently pruned.Table 3 shows that our main goal of reducing thephrase table size is achieved.
On all four languagepairs, we are able to prune over 50% of the phrasePT size reductionw/o del.
w/ del.it-en 65.4% 54.0%pt-en 68.5% 61.3%nl-en 64.1% 56.9%et-en 63.6% 58.5%Table 3: % Phrase table size reduction compared with thebaseline phrase tabletable.
Without on-demand insertions/deletions, thesize reduction is even stronger.
Notice the size re-duction here is relative to the already heavily prunedbaseline phrase table.With such a successful size cut, we expected asignificant increase in decoding speed in the finalsystem.
In practice we experienced 3% to 12% ofspeedup across all the systems we tested.
Both ourbaseline and the reduced systems use a tight beamwidth of 20 hypotheses per stack.
We assume thatwith a wider beam, the speed improvement wouldbe more pronounced.We also did human evaluation on all 8 system out-puts (four language pairs, with two test sets per lan-guage pair) and all came back positive (more im-provements than regressions), even on those that hadminor BLEU degradation.
We conclude that the sizecut in the phrase table is indeed harmless, and there-fore we declare our initial goal of phrase table prun-ing without sacrificing quality is achieved.In (Wuebker et al, 2010) it was observed, thatphrase training reduces the average phrase length.The longer phrases, which are unlikely to gener-alize, are dropped.
We can confirm this obersva-tion for the it-en and pt-en language pairs in Ta-ble 4.
However, for nl-en and et-en the aver-age source phrase length is not significantly af-fected by phrase training, especially with the inser-tion/deletion phrases.
When these artificial phrasesare added during forced alignment, they tend to en-courage long target phrases as uncovered single tar-get words can be consumed by the insertion phrases.However, these insertion phrases are not dumpedinto the final phrase table and hence cannot helpin reducing the average phrase length of the finalphrase table.465avg.
src phrase lengthbaseline w/o del.
w/ del.it-en 3.1 2.4 2.4pt-en 3.7 3.0 3.0nl-en 3.1 3.0 3.0et-en 2.9 2.8 3.0Table 4: Comparison of average source phrase length inthe phrase table.nl-en Test1 WMT PT size reductionbaseline 32.29 43.37 ?n=100 31.45 42.90 66.0%n=160 31.87 43.06 64.1%et-en Test1 WMT PT size reductionbaseline 46.14 20.08 ?n=100 46.35 19.60 63.6%n=200 46.34 19.88 58.4%Table 5: BLEU scores of different n-best sizes for thehighly inflected Dutch system and the noisy Estonian sys-tem.Table 5 illustrates how the n-best size affectsBLEU scores and model sizes for the nl-en and et-en systems.4.3 Phrase Model Re-estimationThis section conducts a preliminary evaluation ofthe techniques introduced in Section 3.6.
For fastturnaround, these experiments were conducted onapproximately 1/3 of the Italian-English trainingdata.
Training is performed with and without inser-tion/deletion phrases and both with (FaTrain) andwithout (FaPrune) re-training of the forward andbackward phrase translation probabilities.
Table 6shows the BLEU scores with different settings of theglobal scaling factor ?
and the inverse discount d.The second global scaling factor is fixed to ?
= 0.The preliminary results seem to be invariant of thesettings.
We conclude that using forced alignmentposteriors as a feature training method seems to beless effective than using competing hypotheses fromfree decoding as in (He and Deng, 2012).BLEUins/del ?
d Test1 WMTbaseline - - - 40.6 28.9FaPrune no - - 40.7 29.1FaTrain no 0 0 40.4 28.90.5 0 40.2 28.9FaPrune yes - - 40.6 28.9FaTrain yes 0 0 40.1 28.60.5 0 40.5 29.10.5 0.2 40.5 29.00.5 0.4 40.5 29.0Table 6: Phrase pruning (FaPrune) vs. further modelre-estimation after pruning (FaTrain) on 1/3 it-en train-ing data, both with and without on-demand inser-tions/deletions.5 Conclusion and OutlookWe applied forced alignment on parallel trainingdata with leave-one-out on four large-scale commer-cial systems.
In this way, we were able to reduce thesize of our already heavily pruned phrase tables byat least 54%, with almost no loss in translation qual-ity, and with a small improvement in speed perfor-mance.
We show that for language pairs with strongreordering, the n-best list size needs to be increasedto account for the larger search space.We introduced several extensions to the trainingprocedure.
On-demand word insertions and dele-tions can increase the data coverage to nearly 100%.We plan to extend our work to use backoff transla-tions (the target word that can not be extended giventhe input phrase table will be aligned to any uncov-ered single source word) to provide more alignmentvarieties, and hence hopefully to be able to keepmore good phrase pairs.
To avoid higher OOV ratesafter pruning, we retrieved single-word translationsfrom the baseline phrase table.We would like to emphasize that this leave-one-out pruning technique is not restricted to phrasaltranslators, even though all experiments presentedin this paper are on phrasal translators.
It is possibleto extend the principle of forced alignment guidedpruning to hierarchical decoders, treelet decoders, orsyntax-based decoders, to prune redundant or use-less phrase mappings or translation rules.466Re-estimating phrase translation probabilities us-ing forced alignment posterior scores did not yieldany noticable BLEU improvement so far.
Instead, wepropose to apply discriminative training similar to(He and Deng, 2012) after forced-alignment-basedpruning as future work.References[Blunsom et al2008] Phil Blunsom, Trevor Cohn, andMiles Osborne.
2008.
A discriminative latent vari-able model for statistical machine translation.
In Pro-ceedings of the 46th Annual Conference of the Associa-tion for Computational Linguistics: Human LanguageTechnologies (ACL-08:HLT), pages 200?208, Colum-bus, Ohio, June.
Association for Computational Lin-guistics.
[Cmejrek et al2009] Martin Cmejrek, Bowen Zhou, andBing Xiang.
2009.
Enriching SCFG Rules DirectlyFrom Efficient Bilingual Chart Parsing.
In Proc.
of theInternational Workshop on Spoken Language Transla-tion, pages 136?143, Tokyo, Japan.
[de Gispert et al2010] Adria?
de Gispert, Juan Pino, andWilliam Byrne.
2010.
Hierarchical Phrase-basedTranslation Grammars Extracted from Alignment Pos-terior Probabilities.
In Proceedings of the 2010 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 545?554, MIT, Massachusetts,U.S.A., October.
[DeNero et al2006] John DeNero, Dan Gillick, JamesZhang, and Dan Klein.
2006.
Why Generative PhraseModels Underperform Surface Heuristics.
In Proceed-ings of the Workshop on Statistical Machine Transla-tion, pages 31?38, New York City, June.
[Foster et al2006] George Foster, Roland Kuhn, andHoward Johnson.
2006.
Phrasetable Smoothing forStatistical Machine Translation.
In Proc.
of the Conf.on Empirical Methods for Natural Language Process-ing (EMNLP), pages 53?61, Sydney, Australia, July.
[He and Deng2012] Xiaodong He and Li Deng.
2012.Maximum Expected BLEU Training of Phrase andLexicon Translation Models.
In Proceedings of the50th Annual Meeting of the Association for Computa-tional Linguistics (ACL), page to appear, Jeju, Republicof Korea, Jul.
[Johnson et al2007] J Howard Johnson, Joel Martin,George Foster, and Roland Kuhn.
2007.
Improv-ing Translation Quality by Discarding Most of thePhrasetable.
In Proceedings of 2007 Joint Conferenceon Empirical Methods in Natural Language Process-ing and Computational Natural Language Learning,pages 967?975, Prague, June.
[Koehn et al2003] P. Koehn, F. J. Och, and D. Marcu.2003.
Statistical Phrase-Based Translation.
In Pro-ceedings of the 2003 Meeting of the North Americanchapter of the Association for Computational Linguis-tics (NAACL-03), pages 127?133, Edmonton, Alberta.
[Koehn et al2007] Philipp Koehn, Hieu Hoang, Alexan-dra Birch, Chris Callison-Burch, Marcello Federico,Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-tine Moran, Richard Zens, Chris Dyer, Ondr?ej Bo-jar, Alexandra Constantine, and Evan Herbst.
2007.Moses: Open Source Toolkit for Statistical MachineTranslation.
In Annual Meeting of the Association forComputational Linguistics (ACL), demonstration ses-sion, pages 177?180, Prague, Czech Republic, June.
[Liang et al2006] Percy Liang, Alexandre Buchard-Co?te?,Dan Klein, and Ben Taskar.
2006.
An End-to-EndDiscriminative Approach to Machine Translation.
InProceedings of the 21st International Conference onComputational Linguistics and the 44th annual meet-ing of the Association for Computational Linguistics,pages 761?768, Sydney, Australia.
[Mylonakis and Sima?an2010] Markos Mylonakis andKhalil Sima?an.
2010.
Learning Probabilistic Syn-chronous CFGs for Phrase-based Translation.
In Pro-ceedings of the Fourteenth Conference on Computa-tional Natural Language Learning, pages 117?, Up-psala,Sweden, July.
[Och2003] Franz Josef Och.
2003.
Minimum Error RateTraining in Statistical Machine Translation.
In Proc.
ofthe 41th Annual Meeting of the Association for Com-putational Linguistics (ACL), pages 160?167, Sapporo,Japan, July.
[Shen et al2008] Wade Shen, Brian Delaney, Tim Ander-son, and Ray Slyh.
2008.
The MIT-LL/AFRL IWSLT-2008 MT System.
In Proceedings of IWSLT 2008,pages 69?76, Hawaii, U.S.A., October.
[Venugopal et al2003] Ashish Venugopal, Stephan Vo-gel, and Alex Waibel.
2003.
Effective Phrase Transla-tion Extraction from Alignment Models.
In Proceed-ings of the 41st Annual Meeting on Association forComputational Linguistics, pages 319?326, Sapporo,Japan, July.
[Wuebker et al2010] Joern Wuebker, Arne Mauser, andHermann Ney.
2010.
Training phrase translation mod-els with leaving-one-out.
In Proceedings of the 48thAnnual Meeting of the Assoc.
for Computational Lin-guistics, pages 475?484, Uppsala, Sweden, July.467
