Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 436?439,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsTwitter Based System: Using Twitter for DisambiguatingSentiment Ambiguous AdjectivesAlexander Pak, Patrick ParoubekUniversite?
de Paris-Sud,Laboratoire LIMSI-CNRS, Ba?timent 508,F-91405 Orsay Cedex, Francealexpak@limsi.fr, pap@limsi.frAbstractIn this paper, we describe our systemwhich participated in the SemEval 2010task of disambiguating sentiment ambigu-ous adjectives for Chinese.
Our systemuses text messages from Twitter, a popu-lar microblogging platform, for building adataset of emotional texts.
Using the builtdataset, the system classifies the meaningof adjectives into positive or negative sen-timent polarity according to the given con-text.
Our approach is fully automatic.
Itdoes not require any additional hand-builtlanguage resources and it is language in-dependent.1 IntroductionThe dataset of the SemEval task (Wu and Jin,2010) consists of short texts in Chinese contain-ing target adjectives whose sentiments need to bedisambiguated in the given contexts.
Those adjec-tives are: ?
big, ?
small, ?
many, ?
few, ?high, ?
low,?
thick, ?
thin, ?
deep, shallow,?
heavy, light,??
huge,??
grave.Disambiguating sentiment ambiguous adjec-tives is a challenging task for NLP.
Previous stud-ies were mostly focused on word sense disam-biguation rather than sentiment disambiguation.Although both problems look similar, the latter ismore challenging in our opinion because impreg-nated with more subjectivity.
In order to solve thetask, one has to deal not only with the semanticsof the context, but also with the psychological as-pects of human perception of emotions from thewritten text.In our approach, we use Twitter1 microbloggingplatform to retrieve emotional messages and formtwo sets of texts: messages with positive emotionsand those with negative ones (Pak and Paroubek,1http://twitter.com2010).
We use emoticons2 as indicators of an emo-tion (Read, 2005) to automatically classify textsinto positive or negative sets.
The reason we useTwitter is because it allows us to collect the datawith minimal supervision efforts.
It provides anAPI3 which makes the data retrieval process muchmore easier then Web based search or other re-sources.After the dataset of emotional texts has beenobtained, we build a classifier based on n-gramsNa?
?ve Bayes approach.
We tested two approachesto build a sentiment classifier:1.
In the first one, we collected Chinese textsfrom Twitter and used them to train a classi-fier to annotate the test dataset.2.
In the second one, we used machine trans-lator to translate the dataset from Chinese toEnglish and annotated it using collected En-glish texts from Twitter as the training data.We have made the second approach because wewere able to collect much more of English textsfrom Twitter than Chinese ones and we wantedto test the impact of machine translation on theperformance of our classifier.
We have exper-imented with Google Translate and Yahoo Ba-belfish4.
Google Translate yielded better results.2 Related workIn (Yang et al, 2007), the authors use web-blogsto construct a corpora for sentiment analysis anduse emotion icons assigned to blog posts as indica-tors of users?
mood.
The authors applied SVM andCRF learners to classify sentiments at the sentencelevel and then investigated several strategies to de-termine the overall sentiment of the document.
As2An emoticon is a textual representation of an author?semotion often used in Internet blogs and textual chats3http://dev.twitter.com/doc/get/search4http://babelfish.yahoo.com/436the result, the winning strategy is defined by con-sidering the sentiment of the last sentence of thedocument as the sentiment at the document level.J.
Read in (Read, 2005) used emoticons such as?:-)?
and ?:-(?
to form a training set for the sen-timent classification.
For this purpose, the authorcollected texts containing emoticons from Usenetnewsgroups.
The dataset was divided into ?pos-itive?
(texts with happy emoticons) and ?nega-tive?
(texts with sad or angry emoticons) samples.Emoticons-trained classifiers: SVM and Na?
?veBayes, were able to obtain up to 70% accuracy onthe test set.In (Go et al, 2009), authors used Twitter tocollect training data and then to perform a senti-ment search.
The approach is similar to the onein (Read, 2005).
The authors construct corporaby using emoticons to obtain ?positive?
and ?neg-ative?
samples, and then use various classifiers.The best result was obtained by the Na?
?ve Bayesclassifier with a mutual information measure forfeature selection.
The authors were able to obtainup to 84% of accuracy on their test set.
However,the method showed a bad performance with threeclasses (?negative?, ?positive?
and ?neutral?
).In our system, we use a similar idea as in (Goet al, 2009), however, we improve it by using acombination of unigrams, bigrams and trigrams ((Go et al, 2009) used only unigrams).
We alsohandle negations by attaching a negation particleto adjacent words when forming ngrams.3 Our method3.1 Corpus collectionUsing Twitter API we collected a corpus of textposts and formed a dataset of two classes: positivesentiments and negative sentiments.
We queriedTwitter for two types of emoticons consideringeastern and western types of emoticons5:?
Happy emoticons: :-), :), ?
?, ?o?, etc.?
Sad emoticons: :-(, :(, T T, ; ;, etc.We were able to obtain 10,000 Twitter posts inChinese, and 300,000 posts in English evenly splitbetween negative and positive classes.The collected texts were processed as follows toobtain a set of n-grams:1.
Filtering ?
we remove URL links (e.g.http://example.com), Twitter user names (e.g.5http://en.wikipedia.org/wiki/Emoticon#Asian style@alex ?
with symbol @ indicating auser name), Twitter special words (such as?RT?6), and emoticons.2.
Tokenization ?
we segment text by split-ting it by spaces and punctuation marks, andform a bag of words.
For English, we keptshort forms as a single word: ?don?t?, ?I?ll?,?she?d?.3.
Stopwords removal ?
in English, texts we re-moved articles (?a?, ?an?, ?the?)
from the bagof words.4.
N-grams construction ?
we make a set of n-grams out of consecutive words.A negation particle is attached to a word whichprecedes it and follows it.
For example, a sen-tence ?I do not like fish?
will form three bigrams:?I do+not?, ?do+not like?, ?not+like fish?.
Sucha procedure improves the accuracy of the classi-fication since the negation plays a special role inopinion and sentiment expression (Wilson et al,2005).
In English, we used negative particles ?no?and ?not?.
In Chinese, we used the following par-ticles:1. ?
?
is not + noun2.
?
?
does not + verb, will not + verb3.
?
(?)
?
do not (imperative)4. ?
(??)
?
does not have3.2 ClassifierWe build a sentiment classifier using the multi-nomial Na?
?ve Bayes classifier which is based onBayes?
theorem.P (s|M) =P (s) ?
P (M |s)P (M)(1)where s is a sentiment, M is a text.
We assumethat a target adjective has the same sentiment po-larity as the whole text, because in general thelengths of the given texts are small.Since we have sets of equal number of positiveand negative messages, we simplify the equation:P (s|M) =P (M |s)P (M)(2)6An abbreviation for retweet, which means citation or re-posting of a message437P (s|M) ?
P (M |s) (3)We train Bayes classifiers which use a presenceof an n-grams as a binary feature.
We have ex-perimented with unigrams, bigrams, and trigrams.Pang et al (Pang et al, 2002) reported that uni-grams outperform bigrams when doing sentimentclassification of movie reviews, but Dave et al(Dave et al, 2003) have obtained contrary re-sults: bigrams and trigrams worked better for theproduct-review polarity classification.
We tried todetermine the best settings for our microbloggingdata.
On the one hand high-order n-grams, suchas trigrams, should capture patterns of sentimentsexpressions better.
On the other hand, unigramsshould provide a good coverage of the data.
There-fore we combine three classifiers that are basedon different n-gram orders (unigrams, bigrams andtrigrams).
We make an assumption of conditionalindependence of n-gram for the calculation sim-plicity:P (s|M) ?
P (G1|s) ?
P (G2|s) ?
P (G3|s) (4)where G1 is a set of unigrams representing themessage, G2 is a set of bigrams, and G3 is a set oftrigrams.
We assume that n-grams are condition-ally independent:P (Gn|s) =?g?GnP (g|s) (5)Where Gn is a set of n-grams of order n.P (s|M) ?
?g?G1P (g|s)?
?g?G2P (g|s)?
?g?G3P (g|s)(6)Finally, we calculate a log-likelihood of each sen-timent:L(s|M) =?g?G1log(P (g|s)) +?g?G2log(P (g|s))+?g?G3log(P (g|s))(7)In order to improve the accuracy, we changedthe size of the context window, i.e.
the number ofwords before and after the target adjective used forclassification.4 Experiments and ResultsIn our experiments, we used two datasets: a trialdataset containing 100 sentences in Chinese and0 5 10 15 20 25 30 35 40 450.450.470.490.510.530.550.570.590.610.630.65google yahoowindow sizemicroaccuracyFigure 1: Micro accuracy when using GoogleTranslate and Yahoo Babelfish0 5 10 15 20 25 30 35 40 450.40.450.50.550.60.65google yahoowindow sizemacroaccuracyFigure 2: Macro accuracy when using GoogleTranslate and Yahoo Babelfisha test dataset with 2917 sentences.
Both datasetswere provided by the task organizers.
Micro andmacro accuracy were chosen as the evaluationmetrics.First, we compared the performance of ourmethod when using Google Translate and YahooBabelfish for translating the trial dataset.
The re-sults for micro and macro accuracy are shown inGraphs 1 and 2 respectively.
The x-axis repre-sents a context window-size, equal to a number ofwords on both sides of the target adjective.
The y-axis shows accuracy values.
From the graphs wesee that Google Translate provides better results,therefore it was chosen when annotating the testdataset.Next, we studied the impact of the context win-dow size on micro and macro accuracy.
Theimpact of the size of the context window onthe accuracy of the classifier trained on Chinesetexts is depicted in Graph 3 and for the classifiertrained on English texts with translated test dataset4380 5 10 15 20 25 30 35 40 450.450.470.490.510.530.550.570.590.610.630.65Micro Macrowindow sizeaccuracyFigure 3: Micro and macro accuracy for the firstapproach (training on Chinese texts)0 5 10 15 20 25 30 35 40 450.450.470.490.510.530.550.570.590.610.630.65Micro Macrowindow sizeaccuracyFigure 4: Micro and macro accuracy for the sec-ond approach (training on English texts whichhave been machine translated)in Graph 4.The second approach achieves better results.We were able to obtain 64% of macro and 61% ofmicro accuracy when using the second approachbut only 63% of macro and 61% of micro accu-racy when using the first approach.Another observation from the graphs is thatChinese requires a smaller size of a context win-dow to obtain the best performance.
For the firstapproach, a window size of 8 words gave the bestmacro accuracy.
For the second approach, we ob-tained the highest accuracy with a window size of22 words.5 ConclusionIn this paper, we have described our system fordisambiguating sentiments of adjectives in Chi-nese texts.
Our Na?
?ve Bayes approach uses infor-mation automatically extracted from Twitter mi-croblogs using emoticons.
The techniques used inour approach can be applied to any other language.Our system is fully automate and does not utilizeany hand-built lexicon.
We were able to achieveup to 64% of macro and 61% of micro accuracy atthe SemEval 2010 taskFor the future work, we would like to collectmore Chinese texts from Twitter or similar mi-croblogging platforms.
We think that increasingthe training dataset will improve much the accu-racy of the sentiment disambiguation.ReferencesKushal Dave, Steve Lawrence, and David M. Pen-nock.
2003.
Mining the peanut gallery: opinionextraction and semantic classification of product re-views.
In WWW ?03: Proceedings of the 12th in-ternational conference on World Wide Web, pages519?528, New York, NY, USA.
ACM.Alec Go, Lei Huang, and Richa Bhayani.
2009.
Twit-ter sentiment analysis.
Final Projects from CS224Nfor Spring 2008/2009 at The Stanford Natural Lan-guage Processing Group.Alexander Pak and Patrick Paroubek.
2010.
Twitter asa corpus for sentiment analysis and opinion mining.In Proceedings of LREC 2010.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
sentiment classification usingmachine learning techniques.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing (EMNLP), pages 79?86.Jonathon Read.
2005.
Using emoticons to reduce de-pendency in machine learning techniques for senti-ment classification.
In Proceedings of the ACL Stu-dent Research Workshop, pages 43?48.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-level sentiment analysis.
In HLT ?05: Proceedingsof the conference on Human Language Technologyand Empirical Methods in Natural Language Pro-cessing, pages 347?354, Morristown, NJ, USA.
As-sociation for Computational Linguistics.Yunfang Wu and Peng Jin.
2010.
Semeval-2010task 18: Disambiguating sentiment ambiguous ad-jectives.
In SemEval 2010: Proceedings of Interna-tional Workshop of Semantic Evaluations.Changhua Yang, Kevin Hsin-Yih Lin, and Hsin-Hsi Chen.
2007.
Emotion classification usingweb blog corpora.
In WI ?07: Proceedings ofthe IEEE/WIC/ACM International Conference onWeb Intelligence, pages 275?278, Washington, DC,USA.
IEEE Computer Society.439
