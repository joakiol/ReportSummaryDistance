Proceedings of the 2nd Workshop on Predicting and Improving Text Readability for Target Reader Populations, pages 20?29,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsThe C-Score ?
Proposing a Reading Comprehension Metrics as aCommon Evaluation Measure for Text SimplificationIrina TemnikovaLinguistic Modelling Department,Institute of Informationand Communication Technologies,Bulgarian Academy of Sciencesirina.temnikova@gmail.comGalina ManevaLab.
of Particle and Astroparticle Physics,Institute of Nuclear Researchand Nuclear Energy,Bulgarian Academy of Sciencesgalina.maneva@gmail.comAbstractThis article addresses the lack of commonapproaches for text simplification evalu-ation, by presenting the first attempt fora common evaluation metrics.
The arti-cle proposes reading comprehension eval-uation as a method for evaluating the re-sults of Text Simplification (TS).
An ex-periment, as an example application of theevaluation method, as well as three for-mulae to quantify reading comprehension,are presented.
The formulae produce anunique score, the C-score, which gives anestimation of user?s reading comprehen-sion of a certain text.
The score can beused to evaluate the performance of a textsimplification engine on pairs of complexand simplified texts, or to compare theperformances of different TS methods us-ing the same texts.
The approach can beparticularly useful for the modern crowd-sourcing approaches, such as those em-ploying the Amazon?s Mechanical Turk1or CrowdFlower2.
The aim of this paperis thus to propose an evaluation approachand to motivate the TS community to starta relevant discussion, in order to come upwith a common evaluation metrics for thistask.1 Context and MotivationCurrently, the area of Text Simplification (TS)is getting more and more attention.
Starting asearly as in the 1996, Chandrasekar et alpro-posed an approach for TS as a pre-processing stepbefore feeding the text to a parser.
Next, the1http://aws.amazon.com/mturk/.
Last accessed on May3rd, 2013.2http://crowdflower.com/.
Last accessed on June 14th,2013.PSET project (Devlin, 1999; Canning, 2002), pro-posed two modules for simplifying text for apha-sic readers.
The text simplification approachescontinued in 2003 with Siddharthan (2003) andInui et al(2003), and through the 2005-2006until the recent explosion of TS approaches in2010-2012.
Recently, several TS-related work-shops took place: PITR 2012 (Williams et al2012), SLPAT 2012 (Alexandersson et al 2012),and NLP4ITA 20123 and 2013.
As in confirma-tion with the text simplification definition as the?process for reducing text complexity at differ-ent levels?
(Temnikova, 2012), the TS approachestackle a variety of text complexity aspects, rang-ing from lexical (Devlin, 1999; Inui et al 2003;Elhadad, 2006; Gasperin et al 2009; Yatskaret al 2010; Coster and Kauchak, 2011; Bott etal., 2012; Specia et al 2012; Rello et al 2013;Drndarevic?
et al 2013), syntactic (Chandrasekaret al 1996; Canning, 2002; Siddharthan, 2003;Inui et al 2003; Gasperin et al 2009; Zhu etal., 2010; Woodsend and Lapata, 2011; Costerand Kauchak, 2011; Drndarevic?
et al 2013), todiscourse/cohesion (Siddharthan, 2003).
The va-riety of problems tackled by the TS approachesdiffer, according to their final aim: (1) being apre-processing step of an input to text process-ing applications, or (2) addressing the reading dif-ficulties of specific groups of readers.
The firsttype of final application ranges between parserinput (Chandrasekar et al 1996), small screensdisplays (Daelemans et al 2004; Grefenstette,1998), text summarization (Vanderwende et al2007), text extraction (Klebanov et al 2004), se-mantic role labeling (Vickrey and Koller, 2008)and Machine Translation (MT) (Ruffino, 1982;Streiff, 1985).The TS approaches addressing spe-cific human reading needs, instead, address read-ers with low levels of literacy (Siddharthan, 2003;3http://www.taln.upf.edu/nlp4ita/.
Last accessed on May3rd, 2013.20Gasperin et al 2009; Elhadad, 2006; Williamsand Reiter, 2008), language learners (Petersen andOstendorf, 2007), and readers with specific cogni-tive and language disabilities.
The TS approaches,addressing this last type of readers target thosesuffering from aphasia (Devlin, 1999; Canning,2002), deaf readers (Inui et al 2003), dyslexics(Rello et al 2013) and the readers with generaldisabilities (Max, 2006; Drndarevic?
et al 2013).Despite the large number of current work inTS, there has been almost no attention to defin-ing common text simplification evaluation ap-proaches, which would allow the comparison ofdifferent TS systems.
Until the present moment,usually, each approach has applied his/her ownmethods and materials, often taken from otherNatural Language Processing (NLP) fields, mak-ing the comparison difficult or impossible.The aim of this paper is thus to propose an eval-uation method and to foster the discussion of thistopic in the text simplification community, as wellas to motivate the TS community to come up withcommon evaluation metrics for this task.Next, Section 2 will describe the existing ap-proaches to evaluating TS, as well as the fewattempts towards offering a common evaluationstrategy.
After that, the next sections will presentour evaluation approach, starting with Section 3describing its context, Section 4 presenting the for-mulae, Section 5 offering the results, and finallySection 6, providing a Discussion and the Conclu-sions.2 Evaluation Methods in TextSimplificationAs mentioned in the previous section, until now,the different authors adopted different combina-tions of metrics, without reaching to a commonapproach, which would allow the comparison ofdifferent systems.
As the different TS evalua-tion methods are applied on a variety of differenttext units (words, sentences, texts), this makes thecomparison between approaches even harder.
Asthe aim of this article is to propose a text simpli-fication evaluation metrics which would take intoaccount text comprehensibility and reading com-prehension, in this discussion we will focus mostlyon the approaches, whose aim is to simplify textsfor target readers and their evaluation strategies.The existing TS evaluation approaches focus ei-ther on the quality of the generated text/sentences,or on the effectiveness of text simplification onreading comprehension.
The first group of ap-proaches include human judges ratings of simpli-fication, content preservation, and grammatical-ity, standard MT evaluation scores (BLEU andNIST), a variety of other automatic metrics (per-plexity, precision/recall/F-measure, and edit dis-tance).
The methods, aiming to evaluate the textsimplification impact on reading comprehension,use, instead, reading speed, reading errors, speecherrors, comprehension questions, answer correct-ness, and users?
feedback.
Several approachesuse a variety of readability formulae (the Flesch,Flesch-Kincaid, Coleman-Liau, and Lorge formu-lae for English, as well as readability formulae forother languages, such as for Spanish).
Due to thecriticisms of readability formulae (DuBay, 2004),which often restrict themselves to a very super-ficial text level, they can be considered to standon the borderline between the two previously de-scribed groups of TS evaluation approaches.
Ascan be seen from the discussion below, differentTS systems employ a combination of the listedevaluation approaches.As one of the first text simplification systemsfor target reader populations, PSET, seems to haveapplied different evaluation strategies for differentof its components, without running an evaluationof the system as a whole.
The lexical simplifi-cation component (Devlin, 1999), which replacedtechnical terms with more frequent synonyms, wasevaluated via user feedback, comprehension ques-tions and the use of the Lorge readability formula(Lorge, 1948).
The syntactic simplification systemevaluated its single components and the system asa whole from different points of view, to a dif-ferent extent, and used different evaluation strate-gies.
Namely, the text comprehensibility was eval-uated via reading time and answers?
correctnessgiven by sixteen aphasic readers; the componentsreplacing passive with active voice and splittingsentences were evaluated for content preservationand grammaticality via four human judges?
rat-ings; and finally, the anaphora resolution compo-nent was evaluated using precision and recall.
Sid-dharthan (2003) did not carry out evaluation withtarget readers, while three human judges rated thegrammaticality and the meaning preservation ofninety-five sentences.
Gasperin et al(2009) usedprecision, recall and f-measure.
Other approaches,using human judges are those of Elhadad (2006),21who also used precision and recall and Yatskar etal.
(2010), who employed three annotators com-paring pairs of words and indicating them same,simpler, or more complex.
Williams and Reiter(2008) run two experiments, the larger one in-volving 230 subjects and measured oral readingrate, oral reading errors, response correctness tocomprehension questions and finally, speech er-rors.
Drndarevic et al(2013) used 7 readabil-ity measures for Spanish to evaluate the degreeof simplification, and twenty-five human annota-tors to evaluate on a Likert scale the grammat-icality of the output and the preservation of theoriginal meaning.
The recent approaches consid-ering TS as an MT task, such as Specia (2010),Zhu et al(2010), Coster and Kauchak (2011)and Woodsend and Lapata (2011), apply standardMT evaluation techniques, such as BLEU (Pap-ineni et al 2002), NIST (Doddington, 2002), andTERp (Snover et al 2009).
In addition, Wood-send and Lapata (2011) apply two readability mea-sures (Flesch-Kincaid, Coleman-Liau) to evalu-ate the actual reduction in complexity and humanjudges ratings for simplification, meaning preser-vation, and grammaticality.
Zhu et al(2010) ap-ply the Flesch readability score (Flesch, 1948) andn-gram language model perplexity, and Coster andKauchak (2011) ?
two additional automatic tech-niques (the word-level-F1 and simple string accu-racy), taken from sentence compression evaluation(Clarke and Lapata, 2006).As we consider that the aim of text simplifica-tion for human readers is to improve text com-prehensibility, we argue that reading comprehen-sion must be evaluated, and that evaluating justthe quality of produced sentences is not enough.Differently from the approaches that employ hu-man judges, we consider that it is better to test realhuman comprehension with target readers popula-tions, rather than to make conclusions about theextent of population?s understanding on the basisof the opinion of a small number of human judges.In addition, we consider that measuring readingspeed, rate, as well as reading and speed errors,requires much more complicated and expensivetools, than having an online system to measuretime to reply and recognize correct answers.
Fi-nally, we consider that cloze tests are an evalu-ation method that cannot really reflect the com-plexity of reading comprehension (for example formeasuring manipulations of the syntactic struc-ture of sentences), and for this reason, we selectmultiple-choice questions as the testing method,which we consider the most reflecting the speci-ficities of the complexity of a text, more accessi-ble than eye-tracking technologies, and more ob-jective than users?
feedback.
The approach doesnot explicitly evaluate the fluency, grammaticalityand content preservation of the simplified text, butcan be coupled with such additional evaluation.The closest to ours approach is that of Relloet al(2013) who evaluated reading comprehen-sion with over ninety readers with and withoutdyslexia.
Besides using eye-tracking (reading timeand fixations duration), different reading devices,and users rating the text according to how easy it isit read, to understand and to remember, they obtainalso a comprehension score based on multiple-choice questions (MCQ) with 3 answers (1 cor-rect, 1 partially correct and 1 wrong).
The dif-ference with our approach is that we consider thathaving only one correct answer (as suggested byGronlund (1982)), is a more objective evaluation,rather than having one partially correct answer,which would introduce subjectivity in evaluation.To support our motivation, some state-of-the-artapproaches state the scarcity of evaluation withtarget readers (Williams and Reiter, 2008), notethat there are no commonly accepted evaluationmeasures (Coster and Kauchak, 2011), attemptto address the need of developing reading com-prehension evaluation methods (Siddharthan andKatsos, 2012), and propose common evaluationframeworks (Specia et al 2012; De Belder andMoens, 2012).
More concretely, Siddhathan andKatsos (2012) propose the magnitude estimationof readability judgements and the delayed sen-tence recall as reading comprehension evaluationmethods.
Specia et al(2012) provide a lexicalsimplification evaluation framework in the contextof Semeval-2012.
The evaluation is performed us-ing a measure of inter-annotator agreement, basedon Cohen (1960).
Similarly, De Belder and Moens(2012) propose a dataset for evaluating lexicalsimplification.
No common evaluation frameworkhas been yet developed for syntactic simplifica-tion.As seen in the overview, besides the multitudeof existing approaches, and the few approaches at-tempting to propose a common evaluation frame-work, there are no widely accepted evaluationmetrics or methods, which would allow the com-22parison of existing approaches.
The next sectionpresents our evaluation approach, which we offeras a candidate for common evaluation metrics.3 Proposed Evaluation Metrics3.1 The Evaluation ExperimentThe metrics proposed in this article, was devel-oped in the context of a previously conductedlarge-scale text simplification evaluation experi-ment (Temnikova, 2012).
The experiment aimedto determine whether a manual, rule-based textsimplification approach (namely a controlled lan-guage), can re-write existing texts into more un-derstandable versions.
Impact on reading com-prehension was necessary to evaluate, as the pur-pose of text simplification was to enhance in firstplace the reading comprehension of emergency in-structions.
The controlled language used for sim-plification was the Controlled Language for Cri-sis Management (CLCM, more details in (Tem-nikova, 2012)), which was developed on the ba-sis of existing psychological and psycholinguis-tic literature discussing human comprehension un-der stress, which ensures its psychological valid-ity.
The text units evaluated in this experimentswere whole texts, and more concretely pairs oforiginal texts and their simplified versions.
We ar-gue that using whole texts for measuring readingcomprehension is better than single sentences, asthe texts provide more context for understanding.The experiment took place in the format of an on-line experiment, conducted via a specially devel-oped web interface, and required users to read sev-eral texts and answer Multiple-Choice Questions(MCQ), testing the readers?
understanding of eachof the texts.
Due to the purpose of the text simpli-fication (emergency situations simulation), userswere required to read the texts in a limited time,as to imitate a stressful situation with no time tothink and re-read the text.
This aspect will not betaken into account in the evaluation, as the pur-pose is to propose a general formula, applicableto a variety of different text simplification experi-ments.
After reading the text in a limited time, thetext was hidden from the readers, and they werepresented with a screen, asking if they were readyto proceed with the questions.
Next, each questionwas displayed one by one, along with its answers,with the readers not having the option to go backto the text.
In order to ensure the constant atten-tion of the readers and to reduce readers?
tirednessfact or, the texts were kept short (about 150-170words each), and the number of texts to be readby the reader was kept to four.
In addition, to en-sure comparability, all the texts were selected in away to be more or less of the same length.
The ex-periment employed a collection of a total of eighttexts, four of which original, non simplified (?com-plex?)
versions, and the other four ?
their manu-ally simplified versions.
Each user had to read twocomplex and two simplified texts, none of whichwas a variant of the other.
The interface automati-cally randomized the order of displaying the texts,to ensure that different users would get differentcombinations of texts in one of the following twodifferent sequences:?
Complex-Simplified-Complex-Simplified?
Simplified-Complex-Simplified-ComplexThis was done in order to minimize the im-pact of the order of displaying the texts on thetext comprehension results.
After reading eachtext, the readers were prompted to answer betweenfour and five questions about each text.
The MCQmethod was selected as it is considered being themost objective and easily measurable way of as-sessing comprehension (Gronlund, 1982).
Thenumber of questions and answers was selected ina way to not tire the reader (four to five questionsper text and four to five answers for each ques-tion), and the questions and answers themselveswere designed following the the best MCQ prac-tices (Gronlund, 1982).
Some of the practices fol-lowed involved ensuring that there is only one cor-rect answer per question, making all wrong an-swers (or ?distractors?)
grammatically, and as textlength consistent with the correct answer, in orderto avoid giving hints to the reader, and making alldistractors plausible and equally attractive.
Simi-larly to the texts, the questions and answers werealso displayed in different order to different read-ers, to avoid that the order influences the compre-hension results.
The correct answer was displayedin different positions to avoid learning its positionand internally marked in a way to distinguish itduring evaluation from all the distractors in what-ever position it was displayed.
The questions re-quired understanding of key aspects of the texts, toavoid relying on pure texts?
memorization (such asunder which conditions what was supposed to bedone, explanations, and the order in which actionsneeded to be taken).
The information, evaluating23the users?
comprehension, collected during the ex-periment, was, on one hand the time for answeringeach question, and on the other hand, the numberof correct answers given by all participants whilereplying to the same question.
Besides the fact thatwe used a specially developed interface, this eval-uation approach can be applied to any experimentemploying an interface capable of calculating thetime for answering and to distinguish the correctanswers from the incorrect ones.The efficiency of the experiment design wasthoroughly tested by running it through severalrounds of pilot experiments and requiring partic-ipants?
feedback.We claim that the evaluation approach proposedin this paper can be applied to more simply orga-nized experiments, as the randomization aspectsare not reflected in the evaluation formulae.The final experiment involved 103 participants,collected via a request sent to several mailing lists.The participants were 55 percent women and 44percent male, and ranged from undergraduate stu-dents to retired academicians (i.e.
correspondedto nineteen to fifty-nine years old).
As the ex-periment allowed entering lots of personal data,it was also known that participants had a vari-ety of professions (including NLP people, teach-ers, and lawyers), knew English from the beginnerthrough intermediate, to native level, and spokea large variety of native languages, allowing tohave native speakers from many of the World?slanguage families (Non Indo-European and Indo-European included).
Figure 1 shows the coarse-grained classification made at the time of the ex-periment, and the distribution of participants pernative languages.
A subset of specific native lan-guage participants will be selected to give an ex-ample of applying the evaluation metrics to a realevaluation experiment.In order to obtain results, we have asked theparticipants to enter a rich selection of informa-tion, and recorded the chosen answer (be it cor-rect or not), and the time which each participantemployed to give each answer (correct or wrong).Table 1 shows the data we recorded for each singleanswer of every participant.The data in Table 1 is: Entry id is each given an-swer, the Domain background (answer y ?
yes andn ?
no) indicates whether the participant has anyprevious knowledge of the experiment (crisis man-agement) domain.
As each text, question and com-Type ExampleEntry id 1Age of the participant 24Gender of the participant fProfession of the participant StudentDomain background (y/n) nNative lang.
EnglishLevel of English NativeText number 4Exper.
completed (0/1) 1User number 1Question number 30Answer number 0Time to reply 18695Texts pair number 1Table 1: Participant?s information recorded foreach answer.plex/simplified texts pair are given reference num-bers, respectively Text number, Question number,and Texts pair number record that.
As requiredby the evaluation method, each entry records alsothe Time to reply each question (measured in ?mil-liseconds?
), and the Answer number.
As said be-fore, the correct answers are marked in a specialway, allowing to distinguish them at a later stage,when counting the number of correct answers.3.2 Definitions and Evaluation HypothesesIn order to correctly evaluate the performance ofthe text simplification method on the basis of theabove described experiment, the data obtained wasthoughtfully analyzed.
The two criteria selected tobest describe the users?
performance were time toreply and number of correct answers.
The eval-uation was done offline, after collecting the datafrom the participants.
The evaluation analysisaimed to test the following two hypotheses:If the text simplification approach has a positiveimpact on the reading comprehension:1.
The percentage of correct answers given forthe simplified text will be higher than the per-centage of correct answers given for the com-plex text.2.
The time to recognize the correct answer andreply correctly to the questions about the sim-plified text will be significantly lower thanthe time to recognize the correct answer and24Figure 1: Coarse-grained distribution of participants per native languages.reply correctly to the questions about thecomplex text.The two hypotheses were tested previously byemploying only the key variables (time to replyand number of correct answers).
It has beenproven that comprehension increases with the per-centage of correct answers and decreases with theincrease of the time to reply.
On the basis ofthese facts, we define the C-Score (a text Compre-hension Score) ?
an objective evaluation metrics,which allows to give a reading comprehension es-timate to a text, or to compare two texts or two ormore text simplification approaches.
The C-Scoreis calculated text per text.
In order to address a va-riety of situations, we propose three versions of theC-Score, which cover, gradually, all possible vari-ables which can affect comprehension in such anexperiment.
In the following sections we presenttheir formulae, the variables involved, and discusstheir results, advantages and shortcomings.3.3 The C-Score Version One.
The C-ScoreSimple.Given a text comprehension experiment featuringn texts with m questions with r answers each, anability to measure time to reply to questions andto recognize the correct answers, we define the C-Score Simple as given below:Csimple =Prtmean(1)Where: Pr is the percentage of correct answers,from all answers given to all the questions aboutthis text, and t is the average time to reply to allquestions about this text (both with a correct anda wrong answer).
The time is expressed in arbi-trary seconds-based units, depending on the ex-periment.
The logic behind this formula is simple:we consider that comprehension increases with thepercentage of correctly answered questions, anddiminishes if the mean time to answer questionsincreases.3.4 The C-Score Version Two.
C-ScoreComplete.The C-Score complete takes into consideration arich selection of variables reflecting the questionsand answers complexity.
In this C-Score version,we consider that the experiment designers will se-lect short texts (e.g.
150 words) of a similar length,with the aim to reduce participants?
tiredness fac-tor, as we did in our experimental settings.Ccomplete =PrNqNq?q=1Qs(q)tmean(q)(2)In this formula, Pr is the percentage of correctanswers by all participants for this text, Nq is the25number of questions of this text (4-5 in our experi-ment), and t is the average time to reply to all ques-tions about this text (4-5 in our experiment).
Weintroduce the concept Question Size, (Qs), whichis calculated for each question and takes into ac-count the number of answers of the question (Na),the question length in words (Lq), and the totallength in words of its answers (La):Qs = Na(Lq + La) (3)We consider that the number of questions nega-tively influences the comprehension results, as thereader gets cognitively tired to process more andmore questions about different key aspects of thetext.
In addition, Gronlund (1982) suggests to re-strict the number of questions per text to four-fiveto achieve better learning.
For this reason, we con-sider that comprehension decreases, if the num-ber of questions is higher.
We also consider thatanswering correctly/faster to a difficult questionshows better text comprehension than giving fasta correct answer to a simply-worded question.
Forthis reason we award question difficulty, and weplace it above the fraction.3.5 The C-Score Version Three.
C-ScoreTextsize.Finally, the last version of C-Score takes into ac-count the case when the texts used for compari-son can be of a different length, and in this way,the texts?
complexity (for example, when compar-ing the results of two different TS engines, withouthaving access to the same texts).
For this reason,the C-Score 3 considers the text length (called textsize, Ts) of the texts used in the experiment.
Asa longer text will be more difficult to understandthan a shorter text, the text length is placed nearthe percentage of correct answers.Ctextsize =PrTsNqNq?q=1Qs(q)tmean(q)(4)4 C-Score ResultsWe have implemented and applied the above de-scribed formulae to the experimental data, pre-sented in Section 3.1.
As we have only one textsimplification approach, two user scenarios arepresented:1.
Original (?Complex?)
vs. Simplified (?Sim-ple?)
pairs of texts comparison.
The subset ofparticipants are the speakers of Basque, Turk-ish, Hungarian, Lithuanian, Vietnamese, Chi-nese, and Indian languages.
All three formu-lae have been applied.2.
Comparison of the comprehension of thesame text of readers from different sub-groups.
The readers have been divided byage.
This scenario can be used to inferpsycho-linguistic findings about the readingabilities of different participants.Please note that the texts pairs are: Text 1 and2; Text 3 and 4; Text 5 and 6; and Text 7 and 8.In each couple, the first text is complex and thesecond is its simplified version.
The results forthe first evaluation scenario are respectively dis-played in Table 2 for C-Score Simple, Table 3 forC-Score Complete and Table 4 for C-Score Text-size.
The results of C-Score Complete have beenmultiplied per 100 for better readability.
As a re-minder, we consider that higher the score is, betteris text comprehension.
From this point of view,if the text simplification approach was successful,Text 2 (Simplified) should have a higher C-Scorethan its original, complex Text 1, Text 4 (Simpli-fied) should have a higher C-Score than its orig-inal Text 3, Text 6 (Simplified) ?
a higher scorethan the complex Text 5, and Text 8 (Simplified) ?a higher score than its original Text 7.In the second scenario, the participants datahas been divided into data relevant to participantsunder 45 years old (ninety-two participants) andinto participants over 45 years old (eleven partic-ipants).
In this case only the C-Score Simple hasbeen applied.
The results of this evaluation areshown in Table 5.
As our aim is to compare thereading abilities of different ages of people, andnot the results of text simplification, only the com-plex texts are taken into account.
The results showthat the comprehension score of participants under45 years old is higher for all texts (despite the un-even participants?
distribution), except in the caseof complex Text 5.A similar phenomenon can be observed in Ta-bles 2, 3 and 4, where in all text pairs, except forpair 3, i.e.
Texts 5 and 6 (where can be observedthe opposite), the simplified text has a higher com-prehension score than its complex original.
Thehypothesis about the different behavior of Text 5and 6 is that it is text-specific.
This is confirmedby Table 5, which shows that besides the big dif-26Text number C-Score SimpleText 1 (Complex) 21.3Text 2 (Simplified) 35.3Text 3 (Complex) 24.8Text 4 (Simplified) 34.9Text 5 (Complex) 36.8Text 6 (Simplified) 23.6Text 7 (Complex) 40.5Text 8 (Simplified) 51.5Table 2: Experiment results for C-Score Simple.ferences in reading comprehension between par-ticipants under 45 years old and participants over45 years old, Text 5 has more or less the samecomprehension score for both groups of readers.From this fact we can assume that this text is prob-ably fairly easy, so this type of combination of textsimplification rules does not simplify it, and in-stead, when applied makes it less comprehensibleor more awkward for the human readers.Text number C-Score CompleteText 1 (Complex) 66.3Text 2 (Simplified) 114.3Text 3 (Complex) 65.3Text 4 (Simplified) 89.9Text 5 (Complex) 104.0Text 6 (Simplified) 66.9Text 7 (Complex) 106.7Text 8 (Simplified) 153.0Table 3: Experiment results for C-Score Com-plete.Text number C-Score TextsizeText 1 (Complex) 109.5Text 2 (Simplified) 192.0Text 3 (Complex) 107.7Text 4 (Simplified) 131.3Text 5 (Complex) 171.6Text 6 (Simplified) 102.4Text 7 (Complex) 176.1Text 8 (Simplified) 263.3Table 4: Experiment results for C-ScoreTextsize.5 Discussion and ConclusionsThis article has presented an extended discussionof the methods employed for evaluation in the textText number Under 45 Over 45Text 1 (Complex) 39.7 22.5Text 3 (Complex) 37.2 18.4Text 5 (Complex) 38.4 38.9Text 7 (Complex) 54.3 35.9Table 5: C-Score Simple for one text.simplification domain.
In order to address the lackof common or standard evaluation approaches,this article proposed three evaluation formulae,which measure the reading comprehension of pro-duced texts.
The formulae have been developed onthe basis of an extensive reading comprehensionexperiment, aiming to evaluate the impact of a textsimplification approach (a controlled language) onemergency instructions.
Two evaluation scenarioshave been presented, the first of which calculatedwith all three formulae, while the second used onlythe simplest one.
In this way, the article aimsto address both the lack of common TS evalua-tion metrics as suggested in Section 2 (Coster andKauchak, 2011) and the scarcity of reading com-prehension (Siddharthan and Katsos, 2012) evalu-ation with real users (Williams and Reiter, 2008),by proposing a tailored approach for this type oftext simplification evaluation.
With this article weaim at inciting the Text Simplification Commu-nity to open a discussion forum about commonmethods for evaluating text simplification, in or-der to provide objective evaluation metrics allow-ing the comparison of different approaches, and toensure that simplification really achieves its aims.We also argue that taking in consideration the end-users and text units used for evaluation is impor-tant.
In our approach, we address only the eval-uation of text simplification approaches aiming toimprove reading comprehension and experimentsin which time to reply to questions and percent-age of correct answers can be measured.
A plausi-ble scenario for applying our evaluation approachwould be to use the Amazon Mechanical Turkfor crowd-sourcing and then to evaluate the per-formance of a text simplification system on com-plex and simplified texts, to compare the perfor-mance of two or more approaches, or of two ver-sions of the same system on the same pairs oftexts.
These formulae can be also employed inpsycholinguistically-oriented experiments, whichaim to reach cognitive findings regarding specifictarget reader groups, such as dyslexics or autis-27tic readers.
Future work will involve the com-parison of the above proposed evaluation metricswith any of the metrics already employed in therelated work, such as the recent and classic read-ability formulae, eye-tracking, reading rate, hu-man judges ratings, and others.
We consider thatcontent preservation and grammaticality are notnecessary to be evaluated for this approach, as thesimplified texts have been produced manually, bylinguists, who were native speakers of English.AcknowledgmentsThe authors would like to thank Prof. Dr. PetarTemnikov for the ideas and advices about the re-search methodology, Dr. Anke Buttner for the psy-cholinguistic counseling about the experiment de-sign, including questions, answers and texts se-lection and the simplification method psycholog-ical validity, and Dr. Constantin Orasan and Dr.Le An Ha for the testing interface implementa-tion.
The research of Irina Temnikova reported inthis paper was partially supported by the projectAComIn ?Advanced Computing for Innovation?,grant 316087, funded by the FP7 Capacity Pro-gramme (Research Potential of Convergence Re-gions).Finally, the authors would also like to thank thePITR 2013 reviewers for their useful feedback.ReferencesJan Alexandersson, Peter Ljunglf, Kathleen F. Mc-Coy, Brian Roark, and Annalu Waller, editors.2012.
Proceedings of the Third Workshop on Speechand Language Processing for Assistive Technolo-gies.
Association for Computational Linguistics,Montre?al, Canada, June.Stefan Bott, Luz Rello, Biljana Drndarevic?, and Hora-cio Saggion.
2012.
Can spanish be simpler?
lexsis:Lexical simplification for spanish.
In Proceedings ofthe 24th International Conference on ComputationalLinguistics (Coling 2012), Mumbai, India (Decem-ber 2012).Yvonne Canning.
2002.
Syntactic Simplification ofText.
Ph.D. thesis, University of Sunderland, UK.Raman Chandrasekar, Christine Doran, and BangaloreSrinivas.
1996.
Motivations and methods for textsimplification.
In Proceedings of the 16th confer-ence on Computational linguistics-Volume 2, pages1041?1044.
Association for Computational Linguis-tics.James Clarke and Mirella Lapata.
2006.
Modelsfor sentence compression: A comparison across do-mains, training requirements and evaluation mea-sures.
In Proceedings of the 21st International Con-ference on Computational Linguistics and the 44thannual meeting of the Association for Computa-tional Linguistics, pages 377?384.
Association forComputational Linguistics.Jacob Cohen et al1960.
A coefficient of agreementfor nominal scales.
Educational and psychologicalmeasurement, 20(1):37?46.William Coster and David Kauchak.
2011.
Learning tosimplify sentences using wikipedia.
In Proceedingsof the Workshop on Monolingual Text-To-Text Gen-eration, pages 1?9.
Association for ComputationalLinguistics.Walter Daelemans, Anja Ho?thker, and Erik Tjong KimSang.
2004.
Automatic sentence simplification forsubtitling in dutch and english.
In Proceedings ofthe 4th International Conference on Language Re-sources and Evaluation, pages 1045?1048.Jan De Belder and Marie-Francine Moens.
2012.
Adataset for the evaluation of lexical simplification.In Computational Linguistics and Intelligent TextProcessing, pages 426?437.
Springer.Siobhan Devlin.
1999.
Automatic Language Simplifi-cation for Aphasic Readers.
Ph.D. thesis, Universityof Sunderland, UK.George Doddington.
2002.
Automatic evaluationof machine translation quality using n-gram co-occurrence statistics.
In Proceedings of the secondinternational conference on Human Language Tech-nology Research, pages 138?145.
Morgan Kauf-mann Publishers Inc.Biljana Drndarevic?, Sanja S?tajner, Stefan Bott, SusanaBautista, and Horacio Saggion.
2013.
Automatictext simplification in spanish: A comparative evalu-ation of complementing modules.
In ComputationalLinguistics and Intelligent Text Processing, pages488?500.
Springer.William H. DuBay.
2004.
The principles of readabil-ity.
Impact Information, pages 1?76.Noe?mie Elhadad.
2006.
User-sensitive text summa-rization: Application to the medical domain.
Ph.D.thesis, Columbia University.Rudolf Flesch.
1948.
A new readability yardstick.
TheJournal of applied psychology, 32(3).Caroline Gasperin, Erick Maziero, Lucia Specia, TASPardo, and Sandra M Aluisio.
2009.
Natural lan-guage processing for social inclusion: a text sim-plification architecture for different literacy levels.the Proceedings of SEMISH-XXXVI Semina?rio Inte-grado de Software e Hardware, pages 387?401.Gregory Grefenstette.
1998.
Producing intelligenttelegraphic text reduction to provide an audio scan-ning service for the blind.
In Working notes of the28AAAI Spring Symposium on Intelligent Text summa-rization, pages 111?118.Norman Edward Gronlund.
1982.
Constructingachievement tests.
Prentice Hall.Kentaro Inui, Atsushi Fujita, Tetsuro Takahashi, RyuIida, and Tomoya Iwakura.
2003.
Text simplifica-tion for reading assistance: a project note.
In Pro-ceedings of the second international workshop onParaphrasing-Volume 16, pages 9?16.
Associationfor Computational Linguistics.Beata Beigman Klebanov, Kevin Knight, and DanielMarcu.
2004.
Text simplification for information-seeking applications.
In On the Move to Mean-ingful Internet Systems 2004: CoopIS, DOA, andODBASE, pages 735?747.
Springer.Irving Lorge.
1948.
The lorge and flesch readabil-ity formulae: A correction.
School and Society,67:141?142.Aure?lien Max.
2006.
Writing for language-impairedreaders.
In Computational Linguistics and Intelli-gent Text Processing, pages 567?570.
Springer.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automaticevaluation of machine translation.
In Proceedings ofthe 40th annual meeting on association for compu-tational linguistics, pages 311?318.
Association forComputational Linguistics.Sarah E. Petersen and Mari Ostendorf.
2007.
Text sim-plification for language learners: a corpus analysis.In In Proc.
of Workshop on Speech and LanguageTechnology for Education.Luz Rello, Ricardo Baeza-Yates, Stefan Bott, and Ho-racio Saggion.
2013.
Simplify or help?
text sim-plification strategies for people with dyslexia.
Proc.W4A, 13.J.
Richard Ruffino.
1982.
Coping with machine trans-lation.
Practical Experience of Machine Transla-tion.Advaith Siddharthan and Napoleon Katsos.
2012.
Of-fline sentence processing measures for testing read-ability with users.
In Proceedings of the First Work-shop on Predicting and Improving Text Readabilityfor target reader populations, pages 17?24.
Associ-ation for Computational Linguistics.Advaith Siddharthan.
2003.
Syntactic simplificationand text cohesion.
Ph.D. thesis, University of Cam-bridge, UK.Matthew Snover, Nitin Madnani, Bonnie J Dorr, andRichard Schwartz.
2009.
Fluency, adequacy, orhter?
: exploring different human judgments with atunable mt metric.
In Proceedings of the FourthWorkshop on Statistical Machine Translation, pages259?268.
Association for Computational Linguis-tics.Lucia Specia, Sujay Kumar Jauhar, and Rada Mihal-cea.
2012.
Semeval-2012 task 1: English lexi-cal simplification.
In Proceedings of the First JointConference on Lexical and Computational Seman-tics, pages 347?355.
Association for ComputationalLinguistics.Lucia Specia.
2010.
Translating from complex to sim-plified sentences.
In Computational Processing ofthe Portuguese Language, pages 30?39.
Springer.A.
A. Streiff.
1985.
New developments in titus 4.Lawson (1985), 185:192.Irina Temnikova.
2012.
Text Complexity and Text Sim-plification in the Crisis Management domain.
Ph.D.thesis, Wolverhampton, UK.Lucy Vanderwende, Hisami Suzuki, Chris Brockett,and Ani Nenkova.
2007.
Beyond sumbasic: Task-focused summarization with sentence simplificationand lexical expansion.
Information Processing &Management, 43(6):1606?1618.David Vickrey and Daphne Koller.
2008.
Sentencesimplification for semantic role labeling.
In Pro-ceedings of the 46th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies (ACL-2008: HLT), pages 344?352.Sandra Williams and Ehud Reiter.
2008.
Generatingbasic skills reports for low-skilled readers.
NaturalLanguage Engineering, 14(4):495?525.Sandra Williams, Advaith Siddharthan, and AniNenkova, editors.
2012.
Proceedings of the FirstWorkshop on Predicting and Improving Text Read-ability for target reader populations.
Associationfor Computational Linguistics, Montre?al, Canada,June.Kristian Woodsend and Mirella Lapata.
2011.
Learn-ing to simplify sentences with quasi-synchronousgrammar and integer programming.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing, pages 409?420.
Associationfor Computational Linguistics.Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-Mizil, and Lillian Lee.
2010.
For the sake of sim-plicity: Unsupervised extraction of lexical simplifi-cations from wikipedia.
In Human Language Tech-nologies: The 2010 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, pages 365?368.
Association forComputational Linguistics.Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.2010.
A monolingual tree-based translation modelfor sentence simplification.
In Proceedings of the23rd international conference on computational lin-guistics, pages 1353?1361.
Association for Compu-tational Linguistics.29
