Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 474?483,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsTables as Semi-structured Knowledge for Question AnsweringSujay Kumar JauharCarnegie Mellon UniversityPittsburgh, PA, USAsjauhar@cs.cmu.eduPeter D. TurneyAllen Institute for Artificial IntelligenceSeattle, WA, USApetert@allenai.orgEduard HovyCarnegie Mellon UniversityPittsburgh, PA, USAhovy@cs.cmu.eduAbstractQuestion answering requires access to aknowledge base to check facts and rea-son about information.
Knowledge in theform of natural language text is easy to ac-quire, but difficult for automated reason-ing.
Highly-structured knowledge basescan facilitate reasoning, but are difficult toacquire.
In this paper we explore tablesas a semi-structured formalism that pro-vides a balanced compromise to this trade-off.
We first use the structure of tablesto guide the construction of a dataset ofover 9000 multiple-choice questions withrich alignment annotations, easily and ef-ficiently via crowd-sourcing.
We thenuse this annotated data to train a semi-structured feature-driven model for ques-tion answering that uses tables as a knowl-edge base.
In benchmark evaluations, wesignificantly outperform both a strong un-structured retrieval baseline and a highly-structured Markov Logic Network model.1 IntroductionQuestion answering (QA) has emerged as a prac-tical research problem for pushing the boundariesof artificial intelligence (AI).
Dedicated projectsand open challenges to the research community in-clude examples such as Facebook AI Research?schallenge problems for AI-complete QA (Westonet al, 2015) and the Allen Institute for AI?s (AI2)Aristo project (Clark, 2015) along with its recentlycompleted Kaggle competition1.
The reason forthis emergence is the diversity of core languageand reasoning problems that a complex, integrated1https://www.kaggle.com/c/the-allen-ai-science-challengetask like QA exposes: information extraction (Sri-hari and Li, 1999), semantic modelling (Shen andLapata, 2007; Narayanan and Harabagiu, 2004),logic and reasoning (Moldovan et al, 2003), andinference (Lin and Pantel, 2001).Complex tasks such as QA require some formof knowledge base to store facts about the worldand reason over them.
By knowledge base, wemean any form of knowledge: structured (e.g., ta-bles, ontologies, rules) or unstructured (e.g., nat-ural language text).
For QA, knowledge has beenharvested and used in a number of different modesand formalisms: large-scale extracted and curatedknowledge bases (Fader et al, 2014), structuredmodels such as Markov Logic Networks (Khot etal., 2015), and simple text corpora in informationretrieval approaches (Tellex et al, 2003).There is, however, a fundamental trade-off inthe structure and regularity of a formalism and itsability to be curated, modelled or reasoned witheasily.
For example, simple text corpora containno structure, and are therefore hard to reason within a principled manner.
Nevertheless, they are eas-ily and abundantly available.
In contrast, MarkovLogic Networks come with a wealth of theoreticalknowledge connected with their usage in princi-pled inference.
However, they are difficult to in-duce automatically from text or to build manually.In this paper we explore tables as semi-structured knowledge for multiple-choice question(MCQ) answering.
Specifically, we focus on ta-bles that represent general knowledge facts, withcells that contain free-form text (Secton 3 detailsthe nature and semantics of these tables).
Thestructural properties of tables, along with theirfree-form text content represents a semi-structuredbalanced compromise in the trade-off between de-gree of structure and ubiquity.
We present twomain contributions, with tables and their structuralproperties playing a crucial role in both.
First,474we crowd-source a collection of over 9000 MCQswith alignment annotations to table elements, us-ing tables as guidelines in efficient data harvest-ing.
Second, we develop a feature-driven modelthat uses these MCQs to perform QA, while fact-checking and reasoning over tables.Others have used tables in the context of QA.Question bank creation for tables has been inves-tigated (Pasupat and Liang, 2015), but withoutstructural guidelines or the alignment informationthat we propose.
Similarly, tables have been usedin QA reasoning (Yin et al, 2015b; Neelakantanet al, 2015; Sun et al, 2016) but have not explic-itly attempted to encode all the semantics of ta-ble structure (see Section 3.1).
To the best of ourknowledge, no previous work uses tables for bothcreation and reasoning in a connected framework.We evaluate our model on MCQ answering forthree benchmark datasets.
Our results consis-tently and significantly outperform a strong re-trieval baseline as well as a Markov Logic networkmodel (Khot et al, 2015).
We thus show the ben-efits of semi-structured data and models over un-structured or highly-structured counterparts.
Wealso validate our curated MCQ dataset and its an-notations as an effective tool for training QA mod-els.
Finally, we find that our model learns general-izations that permit inference when exact answersmay not even be contained in the knowledge base.2 Related WorkOur work with tables, semi-structured knowledgebases and QA relates to several parallel lines ofresearch.
In terms of dataset creation via crowd-sourcing, Aydin et al (2014) harvest MCQs via agamified app, although their work does not involvetables.
Pasupat and Liang (2015) use tables fromWikipedia to construct a set of QA pairs.
Howevertheir annotation setup does not impose structuralconstraints from tables, and does not collect fine-grained alignment to table elements.On the inference side Pasupat and Liang (2015)also reason over tables to answer questions.
Un-like our approach, they do not require alignmentsto table cells.
However, they assume knowledge ofthe table that contains the answer, a priori ?
whichwe do not.
Yin et al (2015b) and Neelakantan etal.
(2015) also use tables in the context of QA, butdeal with synthetically generated query data.
Sunet al (2016) perform cell search over web tablesvia relational chains, but are more generally inter-ested in web queries.
Clark et al (2016) combinedifferent levels of knowledge for QA, includingan integer-linear program for searching over tablecells.
None of these other efforts leverage tablesfor generation of data.Our research more generally pertains to natu-ral language interfaces for databases.
Answer-ing questions in this context refers to executingqueries over relational databases (Cafarella et al,2008; Pimplikar and Sarawagi, 2012).
Yin etal.
(2015a) consider databases where informationis stored in n-tuples, which are essentially ta-bles.
Also, investigation of the relational structureof tables is connected with research on databaseschema analysis and induction (Venetis et al,2011; Syed et al, 2010).
Finally, unstructured textand structured formats links to work on open infor-mation extraction (Etzioni et al, 2008) and knowl-edge base population (Ji and Grishman, 2011).3 Tables as Semi-structured KnowledgeRepresentationTables can be found on the web containing a widerange of heterogenous data.
To focus and fa-cilitate our work on QA we select a collectionof tables that were specifically designed for thetask.
Specifically we use AI2?s Aristo Tablestore2.However, it should be noted that the contributionsof this paper are not tied to specific tables, as weprovide a general methodology that could equallybe applied to a different set of tables.
The struc-tural properties of this class of tables is further de-scribed in Section 3.1.The Aristo Tablestore consists of 65 hand-crafted tables organized by topic.
Some of thetopics are bounded, containing only a fixed num-ber of facts, such as the possible phase changes ofmatter (see Table 1).
Other topics are unbounded,containing a very large or even infinite number offacts, such as the kind of energy used in perform-ing an action (the corresponding tables can onlycontain a sample subset of these facts).
A totalof 3851 facts (one fact per row) are present in themanually constructed tables.
An individual tablehas between 2 and 5 content columns.The target domain for these tables is two 4thgrade science exam datasets.
The majority of thetables were constructed to contain topics and facts2http://allenai.org/content/data/AristoTablestore-Nov2015Snapshot.zip475Phase Change Initial State Final State Form of Energy TransferMelting causes a solid to change into a liquid by adding heatVaporization causes a liquid to change into a gas by adding heatCondensation causes a gas to change into a liquid by removing heatSublimation causes a solid to change into a gas by adding heatTable 1: Part of a table concerning phase changes in matter.
Rows are facts.
Columns without header textprovide filler text, so that each row forms a sentence.
In columns with header text, the header describesthe type of entry in the column; the header is a hypernym of the text in the body below.from the publicly available Regents dataset3.
Therest were targeted at an unreleased dataset calledMonarch.
In both cases only the training partitionof each dataset was used to formulate and hand-craft tables.
However, for unbounded topics, addi-tional facts were added to each table, using scienceeducation text books and websites.3.1 Table Semantics and RelationsPart of a table from the Aristo Tablestore is givenas an example in Table 1.
The format is semi-structured: the rows of the table (with the excep-tion of the header) are a list of sentences, but withwell-defined recurring filler patterns.
Togetherwith the header, these patterns divide the rows intomeaningful columns.
This semi-structured dataformat is flexible.
Since facts are presented assentences, the tables can act as a text corpus forinformation retrieval.
At the same time the struc-ture can be used ?
as we do ?
to focus on specificnuggets of information.
The flexibility of these ta-bles allows us to compare our table-based systemto an information retrieval baseline.Such tables have some interesting structural se-mantics, which we will leverage throughout thepaper.
A row in a table corresponds to a fact4.The cells in a row correspond to concepts, enti-ties, or processes that participate in this fact.
Acontent column5corresponds to a group of con-cepts, entities, or processes that are the same type.The header cell of the column is an abstract de-scription of the type.
We may view the head asa hypernym and the cells in the column below asco-hyponyms of the head.
The header row definesa generalization of which the rows in the table arespecific instances.This structure is directly relevant to multiple-choice QA.
Facts (rows) form the basis for creat-3http://allenai.org/content/data/Regents.zip4Also predicates, or more generally frames with typed ar-guments.5Different from filler columns, which only contain a re-curring pattern, and no information in their header cells.ing or answering questions, while instances of atype (columns) act as the choices of an MCQ.
Weuse these observations both for crowd-sourcingMCQ creation as well as for designing features toanswer MCQs with tables.4 Crowd-sourcing Multiple-choiceQuestions from TablesWe use Amazon?s Mechanical Turk (MTurk) ser-vice to generate MCQs by imposing constraintsderived from the structure of the tables.
Theseconstraints help annotators create questions withscaffolding information, and lead to consistentquality in the generated output.
An additional ben-efit of this format is the alignment information,linking cells in the tables to the MCQs generatedby the Turkers.
The alignment information is gen-erated as a by-product of making the MCQs.We present Turkers with a table such as the onein Figure 1.
Given this table, we choose a targetcell to be the correct answer for a new MCQ; forexample, the red cell in Figure 1.
First, Turkerscreate a question by using information from therest of the row containing the target (i.e., the bluecells in Figure 1), such that the target is its cor-rect answer.
Then they select the cells in the rowthat they used to construct the question.
Follow-ing this, they construct four succinct choices forthe question, one of which is the correct answerand the other three are distractors.
Distractors areformed from other cells in the column containingthe target (i.e.
yellow cells in Figure 1).
If thereare insufficient unique cells in the column Turk-ers create their own.
Annotators can rephrase andshuffle the contents of cells as required.In addition to an MCQ, we obtain alignmentinformation with no extra effort from annotators.We know which table, row, and column containsthe answer, and thus we know which header cellsmight be relevant to the question.
We also knowthe cells of a row that were used to construct aquestion.476Figure 1: Example table from MTurk annotation task illustrating constraints.
We ask Turkers to constructquestions from blue cells, such that the red cell is the correct answer, and yellow cells form distractors.Task Avg.
Time (s) $/hour % RejectRewrite 345 2.61 48Paraphrase 662 1.36 49Add choice 291 2.47 24Write new 187 5.78 38TabMCQ 72 5.00 2Table 2: Comparison of different ways of generat-ing MCQs with MTurk.What is the orbital event withthe longest day and the shortest night?A) Summer solsticeB) Winter solsticeC) Spring equinoxD) Fall equinoxSteel is a/an of electricityA) SeparatorB) IsolatorC) InsulatorD) ConductorTable 3: Examples of MCQs generated by MTurk.Correct answer choices are in bold.4.1 The TabMCQ DatasetWe created a HIT (the MTurk acronym for Hu-man Intelligence Task) for every non-filler cell(see Section 3) from each one of the 65 manuallyconstructed tables of the Aristo Tablestore.
Wepaid annotators 10 cents per MCQ, and asked for1 annotation per HIT for most tables.
For an initialset of four tables which we used in a pilot study,we asked for three annotations per HIT6.
We re-quired Turkers to have a HIT approval rating of95% or higher, with a minimum of at least 500HITs approved.
We restricted the demographicsof our workers to the US.Table 2 compares our method with other studiesconducted at AI2 to generate MCQs.
These meth-ods attempt to generate new MCQs from existing6The goal was to obtain diversity in the MCQs created fora target cell.
The results were not sufficiently conclusive towarrant a threefold increase in the cost of creation.ones, or write them from scratch, but do not in-volve tables in any way.
Our annotation procedureleads to faster data creation, with consistent out-put quality that resulted in the lowest percentageof rejected HITs.
Manual inspection of the gener-ated output also revealed that questions are of con-sistently good quality.
They are good enough fortraining machine learning models and many aregood enough as evaluation data for QA.
A sampleof generated MCQs is presented in Table 3.We implemented some simple checks to eval-uate the data before approving HITs.
These in-cluded things like checking whether an MCQ hasat least three choices and whether choices are re-peated.
We had to further prune our data to dis-card some MCQs due to corrupted data or badlyconstructed MCQs.
A total of 159 MCQs werelost through the cleanup.
In the end our com-plete data consists of 9092 MCQs, which is ?
tothe best of our knowledge ?
orders of magnitudelarger than any existing collection of science examstyle MCQs available for research.
These MCQsalso come with alignment information to tables,rows, columns and cells.
The dataset, bundled to-gether with the Aristo Tablestore, can be freelydownloaded7.5 Solving MCQs with Table Cell SearchConsider the MCQ ?What is the process by whichwater is changed from a liquid to a gas??
withchoices ?melting, sublimation, vaporization, con-densation?, and the table given in Figure 1.
Find-ing the correct answer amounts to finding a cell inthe table that is most relevant to a candidate QApair.
In other words, a relevant cell should confirmthe assertion made by a particular QA pair.By applying the reasoning used to create MCQs7http://ai2-website.s3.amazonaws.com/data/TabMCQ_v_1.0.zip477(see Section 4) in the inverse direction, findingthese relevant cells becomes the task of finding anintersection between rows and columns of interest.Consider the table in Figure 1: assuming we havesome way of aligning a question to a row (bluecells) and choices to a column (yellow cells), thenthe relevant cell is at the intersection of the two(the red cell).
This alignment is precisely what weget as a by-product of the annotation task we setupin Section 4 to harvest MCQs.We can thus featurize connections betweenMCQs and elements of tables and use the align-ment data to train a model over the features.
Thisis outlined in the next section, describing our Fea-ture Rich Table Embedding Solver (FRETS).5.1 Model and Training ObjectiveLet Q = {q1, ..., qN} denote a set of MCQs, andAn= {a1n, ..., akn} be the set of candidate answerchoices for a given question qn.
Let the set of ta-bles be defined as T = {T1, ..., TM}.
Given a ta-ble Tm, let tijmbe the cell in that table correspond-ing to the ith row and jth column.We define a log-linear model that scores everycell tijmof every table in our collection accordingto a set of discrete weighted features, for a givenQA pair.
We have the following:log p(tijm|qn, akn;An, T ) =?d?dfd(qn, akn, tijm;An, T )?
logZ (1)Here ?dare weights and fd(qn, akn, tijm;An, T ) arefeatures.
These features should ideally leverageboth structure and content of tables to assign highscores to relevant cells, while assigning low scoresto irrelevant cells.
Z is the partition function, de-fined as follows:Z =?m,i,jexp(?d?dfd(qn, akn, tijm;An, T ))(2)Z normalizes the scores associated with every cellover all the cells in all the tables to yield a prob-ability distribution.
During inference the partitionterm logZ can be ignored, making scoring cells ofevery table for a given QA pair efficient.These scores translate to a solution for an MCQ.Every QA pair produces a hypothetical fact, andas noted in Section 3.1, the row of a table is inessence a fact.
Relevant cells (if they exist) shouldconfirm the hypothetical fact asserted by a givenQA pair.
During inference, we assign the score ofthe highest scoring row (or the most likely fact)to a hypothetical QA pair.
Then the correct solu-tion to the MCQ is simply the answer choice as-sociated with the QA pair that was assigned thehighest score.
Mathematically, this is expressed asfollows:a?n= argmaxaknmaxm,i?j?d?dfd(qn, akn, tijm;An, T ) (3)5.1.1 TrainingSince FRETS is a log-linear model, training in-volves optimizing a set of weights ?d.
As train-ing data, we use alignment information betweenMCQs and table elements (see Section 4.1).
Thepredictor value that we try to maximize with ourmodel is an alignment score that is closest to thetrue alignments in the training data.
True align-ments to table cells for a given QA pair are es-sentially indicator values but we convert them tonumerical scores as follows8.
For a correct QAhypothesis we assign a score of 1.0 to cells whoserow and column and both aligned to the MCQ(i.e.
cells that exactly answer the question), 0.5to cells whose row but not column is aligned insome way to the question (i.e.
cells that were usedto construct the question), and 0.0 otherwise.
Foran incorrect QA hypothesis we assign a score of0.1 to random cells from tables that contain noalignments to the QA (so all except one), with aprobability of 1%, while all other cells are scored0.0.
The intuition behind this scoring scheme isto guide the model to pick relevant cells for cor-rect answers, while encouraging it to pick faultyevidence with low scores for incorrect answers.Given these scores assigned to all cells of all ta-bles for all QA pairs in the training set, suitablynormalized to a probability distribution over ta-bles for a given QA pair, we can then proceed totrain our model.
We use cross-entropy, which min-imizes the following loss:8On training data, we experimented with a few differentscoring heuristics and found that these ones worked well.478Level Feature Description Intuition S-Var CmpctTableTable score Ratio of words in t to q+a Topical consistency ?
?TF-IDF table score Same but TF-IDF weights Topical consistency ?Row-question score Ratio of words in r to q Question align ?Row Row-question w/o focus score Ratio of words in r to q-(af+qf) Question align ?Header-question score Ratio of words in h to q Prototype align ?Column overlap Ratio of elements in c and A Choices align ?Column Header answer-type match Ratio of words in chto afChoices hypernym align ?Header question-type match Ratio of words in chto qfQuestion hypernym align ?
?Cell salience Salience of s to q+a QA hypothesis assert ?Cell ?Cell answer-type entailment Entailment score between s and afHypernym-hyponym alignCell answer-type similarity Avg.
vector sim between s and afHypernym-hyponym sim.Table 4: Summary of features.
For a question (q) and answer (a) we compute scores for elementsof tables: whole tables (t), rows (r), header rows (h), columns (c), column headers (ch) and cells (s).Answer-focus (af) and question-focus (qf) terms added where appropriate.
Features marked ?
denotesoft-matching variants, marked with while those marked with a ?
are described in further detail in Sec-tion 5.2.
Finally, features denote those that received high weights during training with all features, andwere subsequently selected to form a compact FRETS model.L(~?)
=?qnakn?An?m,i,jp(t?ijm|qn, akn; T )?log p(tijm|qn, akn;An, T ) (4)Here p(t?ijm|qn, akn; T ) is the normalized probabil-ity of the true alignment scores.While this is an indirect way to train our modelto pick the best answer, in our pilot experimentsit worked better than direct maximum likelihoodor ranking with hinge loss, achieving a trainingaccuracy of almost 85%.
Our experimental re-sults on the test suite, presented in the next section,also support the empirical effectiveness of this ap-proach.5.2 FeaturesThe features we use are summarized in Ta-ble 4.
These features compute statistics be-tween question-answer pairs and different struc-tural components of tables.
While the features areweighted and summed for each cell individually,they can capture more global properties such asscores associated with tables, rows or columns inwhich the specific cell is contained.
Features aredivided into four broad categories based on thelevel of granularity at which they operate.
In whatfollows we give some details of Table 4 that re-quire further elaboration.5.2.1 Soft matchingMany of the features that we implement are basedon string overlap between bags of words.
How-ever, since the tables are defined statically in termsof a fixed vocabulary (which may not necessarilymatch words contained in an MCQ), these over-lap features will often fail.
We therefore softenthe constraint imposed by hard word overlap bya more forgiving soft variant.
More specificallywe introduce a word-embedding based soft match-ing overlap variant for every feature in the tablemarked with ?.
The soft variant targets high recallwhile the hard variant aims at providing high pre-cision.
We thus effectively have almost twice thenumber of features listed.Mathematically, let a hard overlap feature de-fine a score |S1?
S2| / |S1| between two bags ofwords S1and S2.
We can define the denominatorS1here, without loss of generality.
Then, a corre-sponding word-embedding soft overlap feature isgiven by this formula:1|S1|?wi?S1maxwj?S2sim( ~wi, ~wj) (5)Intuitively, rather than matching a word to its exactstring match in another set, we instead match it toits most similar word, discounted by the score ofthat similarity.5.2.2 Question parsingWe parse questions to find the desired answer-type and, in rarer cases, question-type words.
Forexample, in the question ?What form of energyis required to convert water from a liquid to agas?
?, the type of the answer we are expecting isa ?form of energy?.
Generally, this answer-typecorresponds to a hypernym of the answer choices,and can help find relevant information in the table,specifically related to columns.479By carefully studying the kinds of question pat-terns in our data, we implemented a rule-basedparser that finds answer-types from queries.
Thisparser uses a set of hand-coded regular expres-sions over phrasal chunks.
The parser is designedto have high accuracy, so that we only produce anoutput for answer-types in high confidence situa-tions.
In addition to producing answer-types, insome rarer cases we also detect hypernyms forparts of the questions.
We call this set of wordsquestion-type words.
Together, the question-typeand answer-type words are denoted as focus wordsin the question.5.2.3 TF-IDF weightingTF-IDF scores for weighting terms are pre-computed for all words in all the tables.
We dothis by treating every table as a unique document.At run-time we discount scores by table length aswell as length of the QA pair under considerationto avoid disproportionately assigning high scoresto large tables or long MCQs.5.2.4 SalienceThe salience of a string for a particular QA pairis an estimate of how relevant it is to the hypoth-esis formed from that QA pair.
It is computed bytaking words in the question, pairing them withwords in an answer choice and then computingPMI statistics between these pairs from a largecorpus.
A high salience score indicates words thatare particularly relevant for a given QA pair hy-pothesis.5.2.5 EntailmentTo calculate the entailment score between twostrings, we use several features, such as overlap,paraphrase probability, lexical entailment likeli-hood, and ontological relatedness, computed withn-grams of varying lengths.5.2.6 NormalizationAll the features in Table 4 produce numericalscores, but the range of these scores vary to someextent.
To make our final model more robust, wenormalize all feature scores to have a range be-tween 0.0 and 1.0.
We do this by finding the maxi-mum and minimum values for any given feature ona training set.
Subsequently, instead of using theraw feature value of a feature fd, we instead re-place it with (fd?min fd) / (max fd?min fd).6 Experimental ResultsWe train FRETS (Section 5) on the TabMCQdataset (Section 4) using adaptive gradient descentwith an L2 penalty of 1.0 and a mini-batch sizeof 500 training instances.
We train two variants:one consisting of all the features from Table 4,the other ?
a compact model ?
consisting of themost important features (above a threshold) fromthe first model by feature-weight.
These featuresare noted by  in the final column of Table 4.We run experiments on three 4th grade scienceexam MCQ datasets: the publicly available Re-gents dataset, the larger but unreleased datasetcalled Monarch, and a third even larger publicdataset of Elementary School Science Questions(ESSQ)9.
For the first two datasets we use the testsplits only, since the training sets were directlystudied to construct the Aristo Tablestore, whichwas in turn used to generate our TabMCQ trainingdata.
On ESSQ we use all the questions since theyare independent of the tables.
The Regents test setconsists of 129 MCQs, the Monarch test set of 250MCQs, and ESSQ of 855 MCQs.Since we are investigating semi-structured mod-els, we compare against two baselines.
The firstis an unstructured information retrieval method,which uses the Lucene search engine.
To ap-ply Lucene to the tables, we ignore their struc-ture and simply use rows as plain-text sentences.The score for top retrieved hits are used to rankthe different choices of MCQs.
The second base-line is the highly-structured Markov-logic Net-work (MLN) model from Khot et al (2015) as re-ported in Clark et al (2016), who use the model asa baseline10.
Note that Clark et al (2016) achievea score of 71.3 on Regents Test, which is higherthan FRETS?
scores (see Table 5), but their resultsare not comparable to ours because they use anensemble of algorithms.
In contrast, we use a sin-gle algorithm with a much smaller collection ofknowledge.
FRETS rivals the best individual al-gorithm from their work.We primarily use the tables from the Aristo Ta-blestore as knowledge base data in three differentsettings: with only tables constructed for Regents(40 tables), with only supplementary tables con-structed for Monarch (25 tables), and with all ta-9http://aristo-public-data.s3.amazonaws.com/AI2-Elementary-NDMC-Feb2016.zip10We do not re-implement the MLN, and therefore onlycite results from previous work on part of our test suite.480Model Data Regents Test Monarch Test ESSQLuceneRegents Tables 37.5 32.6 36.9Monarch Tables 28.4 27.3 27.7Regents+Monarch Tables 34.8 35.3 37.3Waterloo Corpus 55.4 51.8 54.4MLN47.5(Khot et al, 2015)Regents Tables 60.7 47.2 51.0FRETS Monarch Tables 56.0 45.6 48.4(Compact) Regents+Monarch Tables 59.9 47.6 50.7Regents Tables 59.1 52.8 54.4FRETS Monarch Tables 52.9 49.8 49.5Regents+Monarch Tables 59.1 52.4 54.9Table 5: Evaluation results on three benchmark datasets using different sets of tables as knowledge bases.Best results on a dataset are highlighted in bold.bles together (all 65 tables; see Section 3).
For theLucene baseline we also experiment with severalorders of magnitude more data by indexing overthe 5 ?
1010words Waterloo corpus compiled byCharles Clarke at the University of Waterloo.
Datais not a variable for MLN, since we directly citeresults from Clark et al (2016).The word vectors we used in soft matching fea-ture variants (i.e., ?
features from Table 4) forall our experiments were trained on 300 millionwords of Newswire English from the monolingualsection of the WMT-2011 shared task data.
Thesevectors were improved post-training by retrofitting(Faruqui et al, 2014) them to PPDB (Ganitkevitchet al, 2013).The results of these experiments is presented inTable 5.
All numbers are reported in percentageaccuracy.
We perform statistical significance test-ing on these results using Fisher?s exact test with ap-value of 0.05 and report them in our discussions.First, FRETS ?
in both full and compact form?
consistently outperforms the baselines, often bylarge margins.
For Lucene, the improvements overall but the Waterloo corpus baseline are statisti-cally significant.
Thus FRETS is able to capital-ize on data more effectively and rival an unstruc-tured model with access to orders of magnitudemore data.
For MLN, the improvements are sta-tistically significant in the case of Regents and Re-gents+Monarch tables.
FRETS is thus performingbetter than a highly structured model while mak-ing use of a much simpler data formalism.Our models are able to effectively generalize.With Monarch tables, the Lucene baseline is lit-tle better than random (25%).
But with the sameknowledge base data, FRETS is competitive andsometimes scores higher than the best Lucene orMLN models (although this difference is statisti-Model REG MON ESSQFRETS 59.1 52.4 54.9w/o tab features 59.1 47.6 52.8w/o row features 49.0 40.4 44.3w/o col features 59.9 47.2 53.1w/o cell features 25.7 25.0 24.9w/o ?
features 62.2 47.5 53.3Table 6: Ablation study on FRETS, removinggroups of features based on level of granularity.
?refers to the soft matching features from Table 4.Best results on a dataset are highlighted in bold.cally insignificant).
These results indicate that ourmodels are able to effectively capture both con-tent and structure, reasoning approximately (andeffectively) when the knowledge base may noteven contain the relevant information to answera question.
The Monarch tables themselves seemto add little value, since results for Regents tablesby themselves are just as good or better than Re-gents+Monarch tables.
This is not a problem withFRETS, since the same phenomenon is witnessedwith the Lucene baseline.
It is noteworthy, how-ever, that our models do not suffer from the addi-tion of more tables, showing that our search pro-cedure over table cells is robust.Finally, dropping some features in the compactmodel doesn?t always hurt performance, in com-parison with the full model.
This indicates thatpotentially higher scores are possible by a prin-cipled and detailed feature selection process.
Inthese experiments the difference between the twoFRETS models on equivalent data is statisticallyinsignificant.6.1 Ablation StudyTo evaluate the contribution of different featureswe perform an ablation study, by individually re-moving groups of features from the full FRETS481model, and re-training.
Evaluation of these partialmodels is given in Table 6.
In this experiment weuse all tables as knowledge base data.Judging by relative score differential, cell fea-tures are by far the most important group, fol-lowed by row features.
In both cases the dropsin score are statistically significant.
Intuitively,these results make sense, since row features arecrucial in alignment to questions, while cell fea-tures capture the most fine-grained properties.
Itis less clear which among the other three featuregroups is dominant, since the differences are notstatistically significant.
It is possible that cell fea-tures replicate information of other feature groups.For example, the cell answer-type entailment fea-ture indirectly captures the same information asthe header answer-type match feature (a columnfeature).
Similarly, salience captures weightedstatistics that are roughly equivalent to the coarse-grained table features.
Interestingly, the success ofthese fine-grained features would explain our im-provements over the Lucene baseline in Table 5,which is incapable of such fine-grained search.7 ConclusionsWe have presented tables as knowledge bases forquestion answering.
We explored a connectedframework in which tables are first used to guidethe creation of MCQ data with alignment infor-mation to table elements, then jointly with thisdata are used in a feature-driven model to answerunseen MCQs.
A central research question ofthis paper was the trade-off between the degreeof structure in a knowledge base and its ability tobe harvested or reasoned with.
On three bench-mark evaluation sets our consistently and signif-icantly better scores over an unstructured and ahighly-structured baseline strongly suggest that ta-bles can be considered a balanced compromise inthis trade-off.
We also showed that our model isable to generalize from content to structure, thusreasoning about questions whose answer may noteven be contained in the knowledge base.We are releasing our dataset of more than 9000MCQs and their alignment information, to the re-search community.
We believe it offers interestingchallenges that go beyond the scope of this paper?
such as question parsing, or textual entailment ?and are exciting avenues for future research.AcknowledgementWe?d like to thank AI2 for funding this researchand the creation of our MCQ dataset.
The firstand third authors of this paper were also supportedin part by the following grants: NSF grant IIS-1143703, NSF award IIS-1147810, DARPA grantFA87501220342.
Thanks also go to the anony-mous reviewers, whose valuable comments helpedto improve the quality of the paper.ReferencesBahadir Ismail Aydin, Yavuz Selim Yilmaz, YaliangLi, Qi Li, Jing Gao, and Murat Demirbas.
2014.Crowdsourcing for multiple-choice question an-swering.
In Twenty-Sixth IAAI Conference.Michael J Cafarella, Alon Halevy, Daisy Zhe Wang,Eugene Wu, and Yang Zhang.
2008.
Webtables: ex-ploring the power of tables on the web.
Proceedingsof the VLDB Endowment, 1(1):538?549.Peter Clark, Oren Etzioni, Tushar Khot, Ashish Sab-harwal, Oyvind Tafjord, Peter Turney, and DanielKhashabi.
2016.
Combining retrieval, statistics, andinference to answer elementary science questions.Proceedings of the 30th AAAI Conference on Arti-ficial Intelligence, AAAI-2016.Peter Clark.
2015.
Elementary school science andmath tests as a driver for ai: Take the aristo chal-lenge.
Proceedings of IAAI, 2015.Oren Etzioni, Michele Banko, Stephen Soderland, andDaniel S Weld.
2008.
Open information extrac-tion from the web.
Communications of the ACM,51(12):68?74.Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.2014.
Open question answering over curated andextracted knowledge bases.
In Proceedings of the20th ACM SIGKDD international conference onKnowledge discovery and data mining, pages 1156?1165.
ACM.Manaal Faruqui, Jesse Dodge, Sujay K Jauhar, ChrisDyer, Eduard Hovy, and Noah A Smith.
2014.Retrofitting word vectors to semantic lexicons.arXiv preprint arXiv:1411.4166.Juri Ganitkevitch, Benjamin Van Durme, and ChrisCallison-Burch.
2013.
PPDB: The paraphrasedatabase.
In Proceedings of NAACL-HLT, pages758?764, Atlanta, Georgia, June.
Association forComputational Linguistics.Heng Ji and Ralph Grishman.
2011.
Knowledge basepopulation: Successful approaches and challenges.In Proceedings of the 49th Annual Meeting of theAssociation for Computational Linguistics: Hu-man Language Technologies-Volume 1, pages 1148?1158.
Association for Computational Linguistics.482Tushar Khot, Niranjan Balasubramanian, EricGribkoff, Ashish Sabharwal, Peter Clark, and OrenEtzioni.
2015.
Exploring markov logic networksfor question answering.
Proceedings of EMNLP,2015.Dekang Lin and Patrick Pantel.
2001.
Discovery of in-ference rules for question-answering.
Natural Lan-guage Engineering, 7(04):343?360.Dan Moldovan, Christine Clark, Sanda Harabagiu, andSteve Maiorano.
2003.
Cogex: A logic prover forquestion answering.
In Proceedings of the 2003Conference of the North American Chapter of theAssociation for Computational Linguistics on Hu-man Language Technology-Volume 1, pages 87?93.Association for Computational Linguistics.Srini Narayanan and Sanda Harabagiu.
2004.
Ques-tion answering based on semantic structures.
InProceedings of the 20th international conference onComputational Linguistics, page 693.
Associationfor Computational Linguistics.Arvind Neelakantan, Quoc V Le, and Ilya Sutskever.2015.
Neural programmer: Inducing latent pro-grams with gradient descent.
arXiv preprintarXiv:1511.04834.Panupong Pasupat and Percy Liang.
2015.
Compo-sitional semantic parsing on semi-structured tables.arXiv preprint arXiv:1508.00305.Rakesh Pimplikar and Sunita Sarawagi.
2012.
An-swering table queries on the web using columnkeywords.
Proceedings of the VLDB Endowment,5(10):908?919.Dan Shen and Mirella Lapata.
2007.
Using semanticroles to improve question answering.
In EMNLP-CoNLL, pages 12?21.Rohini Srihari and Wei Li.
1999.
Information extrac-tion supported question answering.
Technical re-port, DTIC Document.Huan Sun, Xiaodong He, Wen-tau Yih, Yu Su, andXifeng Yan.
2016.
Table cell search for questionanswering.
In Proceedings of the 25th InternationalConference on World Wide Web (to appear).Zareen Syed, Tim Finin, Varish Mulwad, and AnupamJoshi.
2010.
Exploiting a web of semantic data forinterpreting tables.
In Proceedings of the SecondWeb Science Conference.Stefanie Tellex, Boris Katz, Jimmy Lin, Aaron Fernan-des, and Gregory Marton.
2003.
Quantitative eval-uation of passage retrieval algorithms for questionanswering.
In Proceedings of the 26th annual inter-national ACM SIGIR conference on Research anddevelopment in informaion retrieval, pages 41?47.ACM.Petros Venetis, Alon Halevy, Jayant Madhavan, Mar-ius Pas?ca, Warren Shen, Fei Wu, Gengxin Miao, andChung Wu.
2011.
Recovering semantics of tableson the web.
Proc.
VLDB Endow., 4(9):528?538,June.Jason Weston, Antoine Bordes, Sumit Chopra, andTomas Mikolov.
2015.
Towards ai-complete ques-tion answering: a set of prerequisite toy tasks.
arXivpreprint arXiv:1502.05698.Pengcheng Yin, Nan Duan, Ben Kao, Junwei Bao, andMing Zhou.
2015a.
Answering questions with com-plex semantic constraints on open knowledge bases.In Proceedings of the 24th ACM International onConference on Information and Knowledge Man-agement, pages 1301?1310.
ACM.Pengcheng Yin, Zhengdong Lu, Hang Li, and Ben Kao.2015b.
Neural enquirer: Learning to query tables.arXiv preprint arXiv:1512.00965.483
