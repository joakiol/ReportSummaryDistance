Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 517?521,Dublin, Ireland, August 23-24, 2014.SAP-RI: A Constrained and Supervised Approach for Aspect-BasedSentiment AnalysisNishtha Malhotra1,2?, Akriti Vij1,2,?, Naveen Nandan1and Daniel Dahlmeier11Research & Innovation, SAP Asia, Singapore2Nanyang Technological University, Singapore{nishtha.malhotra,akriti.vij,naveen.nandan,d.dahlmeier}@sap.comAbstractWe describe the submission of the SAPResearch & Innovation team to the Se-mEval 2014 Task 4: Aspect-Based Senti-ment Analysis (ABSA).
Our system fol-lows a constrained and supervised ap-proach for aspect term extraction, catego-rization and sentiment classification of on-line reviews and the details are included inthis paper.1 IntroductionThe increasing popularity of the internet as asource of information, and e-commerce as a wayof life, has led to a major surge in the number ofreviews that can be found online, for a wide rangeof products and services.
Consequently, more andmore consumers have taken to consulting these on-line reviews as part of their pre-purchase researchbefore deciding on availing services from a localbusiness or investing in a product from a particu-lar brand.
This calls for innovative techniques forthe sentiment analysis of online reviews so as togenerate accurate and relevant recommendations.Sentiment analysis has been extensively studiedand applied in different domains.
Predicting thesentiment polarity (positive, negative, neutral) ofuser opinions by mining user reviews (Hu and Liu,2004; Liu, 2012; Pang and Lee, 2008; Liu, 2010)has been of high commercial and research interest.In these studies, sentiment analysis is often con-ducted at one of the three levels: document level,sentence level or attribute level.Through the SemEval 2014 Task 4 on AspectBased Sentiment Analysis (Pontiki et al., 2014),we explore sentiment analysis at the aspect level.
?The work was done during an internship at SAP.This work is licenced under a Creative Commons Attribution4.0 International License.
Page numbers and proceedingsfooter are added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/The task consists of four subtasks: in subtask 1 as-pect term extraction, participants need to identifythe aspect terms present in a sentence and returna list containing all distinct aspect terms, in sub-task 2 aspect term polarity, participants were todetermine the polarity of each aspect term in a sen-tence, in subtask 3 aspect category detection, par-ticipants had to identify the aspect categories dis-cussed in a given sentence, and in subtask 4 aspectcategory polarity, participants were to determinethe polarity of each aspect category.
The polarityclassification subtasks consider sentiment analysisto be a three-way classification problem betweenpositive, negative and neutral sentiment.
On theother hand, the aspect category detection subtaskis a multi-label classification problem where onesentence can be labelled with more than one as-pect category.In this paper, we describe the submission of theSAP-RI team to the SemEval 2014 Task 4.
Wemake use of supervised techniques to extract theaspects of interest (Jakob and Gurevych, 2010),categorize them (Lu et al., 2011) and predict thesentiment of customer online reviews on Laptopsand Restaurants.
We developed a constrained sys-tem for aspect-based sentiment analysis of theseonline reviews.
The system is constrained in thesense that we only use the training data that wasprovided by the challenge organizers and no otherexternal data sources.
Our system performed rea-sonably well, especially with a F1score of 75.61%for the aspect category polarity subtask, 79.04%F1score on the aspect category detection task and66.61% F1score on the aspect term extractiontask.2 Subtask 1: Aspect Term ExtractionGiven a review with annotated entities in the train-ing set, the task was to extract the aspect terms forreviews in the test set.
For this subtask, training,development and testing were conducted for both517the laptop and the restaurant domain.2.1 FeaturesEach review was represented as a feature vectormade up of the following features:?
Word N-grams: all unigrams, bigrams andtrigrams from the review text?
Casing: presence or absence of capital case/title case words?
POS tags: POS tags of a word and its neigh-bours?
Parse dependencies and relations: parsedependency relations of the aspects, i.e.,presence/absence of adjectives and adverbs inthe dependency parse tree?
Punctuation Marks: presence/absence ofpunctuation marks, such as ?, !2.2 MethodWe approach the task by casting it as a sequencetagging task where each token in a candidate sen-tence is labelled as either Beginning, Inside orOutside (BIO).
We then employ conditional ran-dom fields (CRF), which is a discriminative, prob-abilistic model for sequence data with state-of-the-art performance (Lafferty et al., 2001).
A linear-chain CRF tries to estimate the conditional prob-ability of a label sequence y given the observedfeatures x, where each label ytis conditioned onthe previous label yt?1.
In our case, we use BIOCoNLL-style tags (Sang and De Meulder, 2003).During development, we split the training datain the ratio of 60:20:20 as training, development(dev) and testing (dev-test).
We train the CRFmodel on the training set of the data, performfeature selection based on the dev set, and testthe resulting model on the dev-test.
In all ex-periments, we use the CRF++1implementationof conditional random fields with the parameterc=4.0.
This value was chosen based on manualobservation.
We perform a feature ablation studyand the results are reported in Table 1.
Featureslisted in section 2.1 were those that were retainedfor the final run.1code.google.com/p/crfpp/3 Subtask 2: Aspect Term PolarityEstimationFor this subtask, the training, development andtesting was done using reviews on laptops andrestaurants.
Given the aspect terms in a sentence,the task was to predict their sentiment polarities.3.1 FeaturesFor each review, we used the following features:?
Word N-grams: all lowercased unigrams,bigrams and trigrams from the review text?
Polarity of neighbouring adjectives: ex-tracted word sentiment from SentiWordNetlexicon (Baccianella et al., 2010)?
Neighbouring POS tags: the POS tags of upto neighbouring 3 words?
Parse dependencies and relations: parsedependency relations of the aspects, i.e.,presence/absence of adjectives and adverbs inthe dependency parse tree3.2 MethodFor each aspect term of a sentence, the afore-mentioned features were extracted.
For exam-ple, for the term Sushi in the sentence Sushiwas delicious., the following feature vector isconstructed, {aspect: ?sushi?, advmod:?null?,amod:?delicious?, uni sushi: 1, uni was: 1,uni delicious, uni the: 0, .. }.We then treat the aspect sentiment polarity es-timation as a multi-class classification task whereeach instance would be labelled as either positive,negative or neutral.
For the classification task, weexperimented with Naive Bayes and Support Vec-tor Machines (SVM) ?
both linear and RBF ker-nels ?
and it was observed that linear SVM per-formed best.
Hence, we use linear SVM for theclassification task.
Table 2 summarizes the resultsobtained from our experiments for various featurecombinations.
The classifiers used are implemen-tations from scikit-learn2, which is also used forthe remaining tasks.4 Subtask3: Aspect Category DetectionGiven a review with annotated entities or aspectterms, the task was to predict the aspect categories.2scikit-learn.org/stable/518Features Precision Recall F1-ScoreN-grams, POS tags 0.7655 0.4283 0.5496N-grams, Parse relations, POS tags 0.8192 0.6641 0.7336N-Grams, Parse relations, POS tags, casing 0.8101 0.6641 0.7299N-grams, Parse relations, POS tags, !
0.8116 0.6641 0.7305N-grams, Parse relations, POS tags,!, ?
0.8123 0.6672 0.7326Table 1: Training-phase experimental results for Subtask 1 on Restaurant reviews.Features Laptops RestaurantsNeighbouring words, 2,3 POS grams, bigrams, trigrams, Sentiment,1,2 ngram lower 0.4196 0.5997Parse Relations, 2,3 POS grams, bigrams, trigrams, Sentiment, 1,2 ngram lower 0.5869 0.6375Parse Relations, Neighbouring words, bigram, trigrams, Sentiment, 1,2 ngram lower 0.5848 0.6380Parse Relations, 2,3 POS grams, Neighbouring words, Sentiment, 1,2 ngram lower 0.5890 0.6240Parse Relations, 2,3 POS grams , Neighbouring words, bigram, trigrams, 1,2 ngram lower 0.5626 0.6239Parse Relations, 2,3 POS grams , Neighbouring words, bigram, trigrams, Sentiment 0.5922 0.6409Table 2: Training-phase experimental results (Accuracy) for Subtask 2.As one sentence in a review could belong to mul-tiple aspect categories, we model the task as amulti-label classification problem, i.e., given aninstance, predict all labels that the instance fits to.4.1 FeaturesWe experimented with different features, for ex-ample unigrams, dependency tree relations, bi-grams, POS tags and sentiment of the words (Sen-tiWordNet), but using just the unigrams alone hap-pened to yield the best result.
The feature vectorwas merely a bag-of-words vector indicating thepresence or absence of a word in an instance.4.2 MethodThe training instances were divided into 5 setsbased on the aspect categories and thereby, wetreated the multi-label classification task as 5 dif-ferent binary classification tasks.
Hence, we usedan ensemble of binary classifiers for the multi-label classification.
An SVM model was trainedusing one classifier per class to distinguish it fromall other classes.
For the binary classificationtasks, directly estimating a linear separating func-tion (such as linear SVM) gave better results, asshown in Table 3.
Finally, the results of the 5 bi-nary classifiers were combined to label the test in-stance.The category Miscellaneous was observed tohave the lowest accuracy, probably due to the factthat miscellaneous captures all those aspects termsthat do not have a clearly defined category.5 Subtask4 Aspect Category PolarityDetectionFor each review with pre-labelled aspect cate-gories, the task was to produce a model whichpredicts the sentiment polarity of each aspect cat-egory.5.1 FeaturesThe training data contains reviews with the po-larity for the corresponding aspect category.
Themodels performed best on using just unigram andbigram features.5.2 MethodThe training instances were split into 5 sets basedon the aspect categories.
We make use of the sen-timent polarity classifier, as described in section3.2, thereby, training one sentiment polarity classi-fier for each aspect category.
Table 4 indicates theperformance of different classifiers for this task,using features as discussed in section 5.1.6 ResultsTable 5 gives an overview of the performance ofour system in this year?s task based on the offi-cial scores from the organizers.
We see that oursystem performs relatively well for subtasks 1, 3and 4, while for subtask 2 the F1scores are be-hind the best system by about 12%.
As observed,a sentence could have more than one aspect andeach of these aspects could have different polar-ities expressed.
Including features that preservethe context of the aspect could probably improvethe performance in the subtask 2.
In most cases,a simple set of features was enough to result in a519Restaurants Category Naive Bayes AdaBoost LinearSVCFood 0.7130 0.8000 0.8470Service 0.6064 0.9137 0.8997Miscellaneous 0.6710 0.7490 0.7890Ambience 0.6770 0.9063 0.8940Price 0.7608 0.8548 0.9590Table 3: Training-phase experimental results (F1score) for Subtask 3.Restaurants Category Naive Bayes AdaBoost LinearSVCFood 0.7136 0.6711 0.7417Service 0.6733 0.5244 0.6688Miscellaneous 0.4756 0.3170 0.4756Ambience 0.6574 0.7232 0.6885Price 0.7477 0.7752 0.6651Table 4: Training-phase experimental results (F1score) for Subtask 4.high F1score, for example, in subtask 3 a bag-of-words feature set proved to yield a relatively highF1score.
In general, for the classification tasks,we observe that the linear SVM performs best.Subtask Dataset Best score Our score Rank1 Laptops 74.55 66.61 8/271 Restaurants 84.01 77.88 12/292 Laptops 70.48 58.56 18/322 Restaurants 80.95 69.92 22/363 Restaurants 88.57 79.04 7/214 Restaurants 82.92 75.61 5/25Table 5: Results (F1score and ranking) for theSemeval-2014 test set.7 ConclusionIn this paper, we have described the submission ofthe SAP-RI team to the SemEval 2014 Task 4.
Wemodel the classification tasks using linear SVMand the term extraction task using CRF in orderto develop an aspect-based sentiment analysis sys-tem that performs reasonably well.AcknowledgementThe research is partially funded by the EconomicDevelopment Board and the National ResearchFoundation of Singapore.ReferencesStefano Baccianella, Andrea Esuli, and Fabrizio Sebas-tiani.
2010.
SentiWordNet 3.0: An enhanced lexi-cal resource for sentiment analysis and opinion min-ing.
In Proceedings of the Seventh Conference onInternational Language Resources and Evaluation(LREC?10), volume 10, pages 2200?2204.Minqing Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In Proceedings of the tenthACM SIGKDD International Conference on Knowl-edge Discovery and Data Mining, pages 168?177.Niklas Jakob and Iryna Gurevych.
2010.
Extractingopinion targets in a single-and cross-domain settingwith conditional random fields.
In Proceedings ofthe 2010 Conference on Empirical Methods in Nat-ural Language Processing, pages 1035?1045.John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.
In Proceedings of the Eighteenth In-ternational Conference on Machine Learning, pages282?289.Bing Liu.
2010.
Sentiment analysis and subjectiv-ity.
In Handbook of Natural Language Processing,pages 627?666.
Chapman & Hall, 2 edition.Bing Liu.
2012.
Sentiment analysis and opinion min-ing.
Synthesis Lectures on Human Language Tech-nologies, 5(1):1?167.Bin Lu, Myle Ott, Claire Cardie, and Benjamin Tsou.2011.
Multi-aspect sentiment analysis with topicmodels.
In Proceedings of Sentiment Elicitationfrom Natural Text for Information Retrieval and Ex-traction, pages 81?88.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and trends in infor-mation retrieval, 2(1-2):1?135.Maria Pontiki, Dimitrios Galanis, John Pavlopou-los, Haris Papageorgiou, Ion Androutsopoulos, andSuresh Manandhar.
2014.
SemEval-2014 Task 4:Aspect based sentiment analysis.
In Proceedings ofthe 8th International Workshop on Semantic Evalu-ation (SemEval 2014).Erik Tjong Kim Sang and Fien De Meulder.
2003.Introduction to the CoNLL-2003 shared task:Language-independent named entity recognition.
In520Proceedings of the Seventh Conference on NaturalLanguage Learning at HLT-NAACL, pages 142?147.521
