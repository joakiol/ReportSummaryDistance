Applying Natural Language Processing Techniques toAugmentative Communication SystemsKathleen McCoy, Patrick Demaseo, Mark Jones, Christopher Pennington & Charles RoweApplied Science and Engineering Laboratories, University of Delaware, A.I.
duPont InstituteEO.
Box 269, Wilmington, DE, USA19899IntroductionA large population of non-speaking, motor-im-paired individuals must rely on computer-basedcommunication aids.
These aids, in essence, presentthe user 'with a list of letters, words, or phrases thatmay be selected to compose the desired utterance.The resulting utterance may then be passed to aspeech synthesizer o document preparation system.While m~my of these individuals desire to communi-cate in complete well-lormed sentences, the expen-diture in effort and time is often prohibitive.
Thegoal of this project is to increase the communicationrate of physically disabled individuals by cuttingdown the number of items that must be selected inorder to compose awell-formed utterance.
We wishto do this while placing as little a burden on the useras possible.Input to our system are the uninflected contentwords of the desired utterance, consider', "APPLEEAT JOHN".
The system first employs a semanticparser to I'orm a semantic representation f the in-put.
In this example, the parser must recognize thatEAT can be a verb which accepts an animate AC-'FOR and an inanimate/food OBJECT.
The resultingsemantic representation (along with a specificationof the original word order) is then passed to thetranslator which is responsible for replacing the se-mantic terms with their language-specific instantia-tions.
Thc final phase of processing is a sentencegenerator which attempts to form a syntacticallycorrect sentence that retains the general order of theoriginal input producing, for example, "THE AP-PLE IS EATEN BY JOttN"hi this paper we discuss the three processing phasesdescribed above as well as examples illustrating thecurrent capabilities of the system.Semant ic  ParserThis sub-system is responsible for generating a setof semantic structures (based on Fillmore's caseframes\[Filhnore77\]) representing possible interpre-tations of the input sentence.Due to the severe syntactic ill-fonnedness ofour in-put and the relatively unconstrained domain of dis-course, our system may not rely on syntactic ordomain specific cues.
Instead, our parser elies (in abottom-up fashion) on semantic information associ-ated with individual words in order to determinehow the words are contributing to the sentence as awhole \[Small & Rieger 82\].
In addition, we employa top-down component which ensures that the indi-vidual words axe fit together to form a well-formedsemantic structure.
Both processing components aredriven by knowledge sources associated with thesystem's lexical items.The first problem faced by the system is determin-ing the general function of each word in the sen-tence.
Each individual word can have differentsemantic lassifications and thus its function in thesentence may be ambiguous.
For example, the word"study" has two meanings: an action, as in "Johnstudies", or a location, as in "John is in his study".In order to recognize "all possible meanings of aword (and to constrain further processing) we em-ploy five hierarchies of word meaning.
Each hierar-chy represents a different semantic function that aword can play: Actions (verbs), Objects, Descrip-tive Lexicons (adjectives), Modifiers to the Descrip-tive Lexicon (adverbs), and Prepositions.Distinctions within each hierarchy provide a finergranularity of knowledge representation.
For exam-ple, distinguishing which objects are animate.For each word of the input, a subframe is generatedwhich indicates the word's semantic type for each ofits occurrences in the hierarchies.Each individualword is likely to have a number of interpretations(i.e., subframes).
However, if the input words aretaken together, then many of the interpretations canbe eliminated.
In the case frame semantic represen-tation we have chosen, the main verb of an utteranceis instrumental in eliminating interpretations of theother input words.
We employ additional knowl-edge associated with the items in the VERBS hier-archy to capture this predictive information.Themain verb predicts which semantic roles are manda-tory and which roles should never appear, as well astype information concerning possible fillers of eachrole.
For example, the verb "go" cannot have aTHEME case in the semantic structure.
Further-more, it cannot have a FROM-LOC case withouthaving a TO-LOC at the same time.
But "go" cantake a TO-LOC without a FROM-LOC.
Since spe--cific types of verbs have their own predictions on413the final structure, we attach predictive frameswhich encode a possible sentence interpretation toeach verb type in the hieramhy of VERBS.
Theframes contain typed w~riables where words fromthe input can be fit, and act as a top-down influenceon the final sentence structure.
They can be used toreduce the number of interpretations of ambiguouswords because they dictate types of words whichmust and types that cannot occur in an utterance.A final level of ambiguity remains, however.
A par-ticular input word may not be modifying the verb(and thus fit into the verb frame), but rather may bemodifying another non-verb input woN.To reducethis ambiguity the system employs a knowledgestructure that specifies the kind of modifiers that canbe attached to various types of words.
Thus for ex-ample "green" may be restricted from modifying"idea".Given these knowledge sources the system worksboth top-down and bottom-up.
With the initialframes for the individual input words, the system at-tempts to find a legal interpretation based on eachpossible verb found in the input.
In a top-down way,the fr~unes resulting from a particular choice of verbattempt to find words of specific types to fill theirvariables.
Bottom-up processing considers themeaning of each individual word mad the modifica-tion relationships which may hold between words.
Itattempts to put individual words together to formsub-frames which take on the semantic type of theword being modified.
These sub-frames are eventu-ally used to fill the frame structure obtained fromtop-down processing.The result of this processing is a set of (partiallyfilled) semantic structures.
All well-formed struc-tures (i.e., all structures whose mandatory roles havebeen filled and which have been able to accommo-date each word of the input) are potential interpreta-tions of the user's input and are passed one at a timeto the next component of the system)TranslatorThe next phase, the translator, acts as the interfacebetween the semantic parser and the generator.
Ittakes the semantic representation f the sentence asinput and associates language specific informationto be passed to the generator component.
Following\[McDonald 80, McKeown 82\] it replaces each cle-ment of the semantic structure with a specificationof how that element could be realized in English.Each component type in the semantic message has1.
Our system does not handle metaphorsan entry in the translator's "dictionary" that holds itspossible structural translations.
The actual transla-tion chosen may be dependent on other semantic el-ements.
When the "dictionary" is accessed for aparticular semantic element, we give it both the ele-ment and the rest of the semantic structure.
The"dictionary" returns a transformed structure con-taining the translation of the particular elementalong with annotations that may affect he eventualsyntactic realization.GeneratorThe final phase, the generator, uses a functional uni-fication grammar \[Kay 79\] in order to generate asyntactically well-formed English sentence.
We em-ploy a functional unification grammar generatorprovided by Columbia University \[Elhadad 88\].
Thefundamental unit of a functional unification gram-mar is the attribute-value (A-V) pair.
Attributesspecify syntactic, semantic, or functional categoriesand the values are the legal fillers for the attributes.The values may be complex (e.g., a set of A-Vpairs).
This type of grammar is particularly attrac-tive for sentence generation because it allows the in-put to the generator to be in functional terms.
Thusthe language specific knowledge needed in thetranslation component can be minimized.In the functional unification model, both the inputand the grammar are represented in the same for-realism, as sets of A-V pairs each containing "vari-ables".
The grammar specifies legal syntacticrealizations of various functional units.
It containsvariables where the actual exical items that specifythose functional units must be fit.
The input, on theother hand, contains a specification of a particularfunctional unit, but contains variables where thesyntactic specification of this unit must occur.The generator works by the process of unificationwhere variables in the grammar are filled by the in-put and input variables are filled by the grammar.The resulting specification precisely identifies anEnglish utterance realizing the generator input.Current StatusAn implementation f the system has been complet-ed and is currently being evaluated.The system is aback-end system which takes as input the uninflect-ed content words of the target sentence.
Output fromthe system is a set of semantically and syntacticallywell-formed sentences which arc possible interpre-tations of the input.
Before the system can actuallybe deployed to the disabled community, it must beprovided with a front-end system which will pro-vide the potential words to the user for ,selection.
In414addition, the front-end must allow the user to selectthe intended utterance from the ones provided whenthe system finds more than one interpretation for theinput.
Care has been made in the design of the back-end system so that it will be compatible with manykinds of front-end systems being developed today.System capabilities are illustrated below.Input: John  call MaryOutput: John  calls MaryOutput: John  is called by MaryNotice that it is unclear which of John or Mary isplaying the AGENT and THEME roles since theyboth have the same type and that type is appropriatefor both roles.
In such instances of ambiguity allpossible structures are output.
In this particular ex-ample, the passive form was chosen in an attempt topreserve the user's original word order.
Note that ifthe verb of the sentence could not undergo the pas-sive transformation, only one option would be giv-en.Input: John  study weather universityOutput John  studies weather at theuniversityInput: John  read book studyOutput: John  reads the book/n  the studyThe above set illustrates multiple meanings of somewords.
Even though study can be both a verb and aplace, in the first instance it is taken as the verb sinceneither weather, university, nor John are appropri-ate.
Notice in the second example study is taken asa place since the system cannot find a reasonable in-terpretation with study as the verb.The first example of this set alo illustrates the top-down processing power.
Here, the system correctlyinfers weather to be the THEME and university tobe the LOCATION.
While technically universitycould De the THEME of study, weather is appropri-ate for no other role.
Note the appropriate preposi-tions are used to introduce the roles.In some cases, our system is capable of inferring theverb intended by the user even though it is unstated.Since our analysis indicates that the verbs HAVEand BE are often intended but left unstated, the sys-tem chooses between these verbs when the verb isleft implicit by the user.
The chosen verb is depen-dent on the suitability of the other input elements toplay the mandatory roles of the verb.The system may also infer the actor (subject) of theintended sentence.
In particular, if no word thatcan play the role of agent is given, the system willinfer the user to be the agent, and thus generate afirst person pronoun for that slot.
2Input: hungryOutput: I am hungryInput: John  paperOutput: John  has the paperConclusionWe have successfully applied natural language pro-cessing techniques toaugmentative communication.Our problem is one of expanding compressed inputin order to generate well-formed sentences.
It is areaPwodd problem, we have not relied on limiteddomain or micro-world assumptions.
The solutionhas required innovations in both understanding andgeneration.
Our system must first understand a se-verely ill-formed utterance.
The resulting semanticrcpresentation, is then translated into a well-formedEnglish sentence.
This process may require infer-ring such elements as function words verb morphol-ogy and, some content words.ReferencesC.J.
Fillmore.The case for case reopened.
In P. Cole andJ.M.
Sadock, editors, Syntax and Semantics VIIIGrammatical Relations, pages 59-81, AcademicPress, New York, 1977.Elhadad, M.The FUF Functional Unifier: U~r's manual.Tezhnical Report # CUCS-408-88 Columbia Uni-versity, 1988.M.
Kay.
Functional grammar.
In Proceedings of the 5thAnnual Meeting, Berkeley Linguistics Society,1979.D.D.
McDonald.
Natural Language Production as a Pro-cess of Decision Making Under Constraint.
Ph.D.thesis, MIT, 1980.K.R.
McKeown.
Discourse strategies for generating nat-ural-language t xt.
Artificial Intelligence, 27(1): 1-41, 1985.S.
Small and C. Rieger.
Parsing and comprehending withword experts (a theory and its realization).
InWendy G. Lehnert and Martin H. Ringle, Editors,Strategies for Natural Language Processing, 1982.AcknowledgmentsThis work is supported by Grant NumberH133E80015 from the National Institute on Disabil-ity and Rehabilitation Research.
Additional supporthas been provided by the Nemours Foundation.2.
We plan to employ an analysis of previous ut-ter~mces to infer agents in a more general case.415
