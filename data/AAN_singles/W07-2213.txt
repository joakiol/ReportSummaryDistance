Proceedings of the 10th Conference on Parsing Technologies, pages 94?105,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsAre Very Large Context-Free Grammars Tractable?Pierre Boullier & Beno?
?t SagotINRIA-RocquencourtDomaine de Voluceau, Rocquencourt BP 10578153 Le Chesnay Cedex, France{Pierre.Boullier,Benoit.Sagot}@inria.frAbstractIn this paper, we present a method which, inpractice, allows to use parsers for languagesdefined by very large context-free grammars(over a million symbol occurrences).
Theidea is to split the parsing process in twopasses.
A first pass computes a sub-grammarwhich is a specialized part of the large gram-mar selected by the input text and variousfiltering strategies.
The second pass is a tra-ditional parser which works with the sub-grammar and the input text.
This approachis validated by practical experiments per-formed on a Earley-like parser running ona test set with two large context-free gram-mars.1 IntroductionMore and more often, in real-word natural lan-guage processing (NLP) applications based upongrammars, these grammars are no more written byhand but are automatically generated, this has sev-eral consequences.
This paper will consider one ofthese consequences: the generated grammars maybe very large.
Indeed, we aim to deal with grammarsthat have, say, over a million symbol occurrencesand several hundred thousands rules.
Traditionalparsers are not usually prepared to handle them,either because these grammars are simply too big(the parser?s internal structures blow up) or the timespent to analyze a sentence becomes prohibitive.This paper will concentrate on context-free gram-mars (CFG) and their associated parsers.
However,virtually all Tree Adjoining Grammars (TAG, seee.g., (Schabes et al, 1988)) used in NLP applica-tions can (almost) be seen as lexicalized Tree In-sertion Grammars (TIG), which can be convertedinto strongly equivalent CFGs (Schabes and Waters,1995).
Hence, the parsing techniques and tools de-scribed here can be applied to most TAGs used forNLP, with, in the worst case, a light over-generationwhich can be easily and efficiently eliminated in acomplementary pass.
This is indeed what we haveachieved with a TAG automatically extracted from(Villemonte de La Clergerie, 2005)?s large-coveragefactorized French TAG, as we will see in Section 4.Even (some kinds of) non CFGs may benefit fromthe ideas described in this paper.The reason why the run-time of context-free (CF)parsers for large CFGs is damaged relies on a theo-retical result.
A well-known result is that CF parsersmay reach a worst-case running time ofO(|G|?n3)where |G| is the size of the CFG and n is the lengthof the source text.1 In typical NLP applicationswhich mainly work at the sentence level, the lengthof a sentence does not often go beyond a value ofsay 100, while its average length is around 20-30words.2 In these conditions, the size of the grammar,despite its linear impact on the complexity, may bethe prevailing factor: in (Joshi, 1997), the author re-marks that ?the real limiting factor in practice is thesize of the grammar?.The idea developed in this paper is to split theparsing process in two passes.
A first pass calledfiltering pass computes a sub-grammar which is the1These two notions will be defined precisely later on.2At least for French, English and similar languages.94sub-part of the large input grammar selected by theinput sentence and various filtering strategies.
Thesecond pass is a traditional parser which works withthe sub-grammar and the input sentence.
The pur-pose is to find a filtering strategy which, in typicalpractical situations, minimizes on the average thetotal run-time of the filtering pass followed by theparser pass.A filtering pass may be seen as a (filtering) func-tion that uses the input sentence to select a sub-grammar out of a large input CFG.
Our hope, us-ing such a filter, is that the time saved by the parserpass which uses a (smaller) sub-grammar will nottotally be used by the filter pass to generate this sub-grammar.It must be clear that this method cannot improvethe worst-case parse-time because there exists gram-mars for which the sub-grammar selected by the fil-tering pass is the input grammar itself.
In such acase, the filtering pass is simply a waste of time.
Ourpurpose in this paper is to argue that this techniquemay profit from typical grammars used in NLP.
Todo that we put aside the theoretical view point andwe will consider instead the average behaviour ofour processors.More precisely we will study on two large NLCFGs the behaviour of our filtering strategies on aset of test sentences.
The purpose being to choosethe best filtering strategy, if any.
By best, we meanthe one which, on the average, minimizes the totalrun-time of both the filtering pass followed by theparsing pass.Useful formal notions and notations are recalledin Section 2.
The filtering strategies are presentedin Section 3 while the associated experiments arereported in Section 4.
This paper ends with someconcluding remarks in Section 5.2 Preliminaries2.1 Context-free grammarsA CFG G is a quadruple (N,T, P, S) where N isa non-empty finite set of nonterminal symbols, T isa finite set of terminal symbols, P is a finite set of(context-free rewriting) rules (or productions) andS is a distinguished nonterminal symbol called theaxiom.
The sets N and T are disjoint and V = N?Tis the vocabulary.
The rules in P have the form A?
?, with A ?
N and ?
?
V ?.For a given string ?
?
V ?, its size (length)is noted |?|.
As an example, for the input stringw = a1 ?
?
?
an, ai ?
T , we have |w| = n. The emptystring is denoted ?
and we have |?| = 0.
The size |G|of a CFG G is defined by |G| = ?A??
?P |A?|.For G, on strings of V ?, we define the binary re-lation derive, noted ?, by ?1A?2 A??
?G ?1?
?2 ifA ?
?
?
P and ?1, ?2 ?
V ?.
The subscript Gor even the superscript A ?
?
may be omitted.
Asusual, its transitive (resp.
reflexive transitive) clo-sure is noted +?G(resp.
??G).
We call derivation anysequence of the form ?1 ?G ?
?
?
?G ?2.
A completederivation is a derivation which starts with the ax-iom and ends with a terminal string w. In that casewe have S ??G?
?
?Gw, and ?
is a sentential form.The string language defined (generated, recog-nized) by G is the set of all the terminal strings thatare derived from the axiom: L(G) = {w | S +?Gw,w ?
T ?}.
We say that a CFG is empty iff itslanguage is empty.A nonterminal symbol A is nullable iff it can de-rive the empty string (i.e., A +?G?).
A CFG is ?-freeiff its nonterminal symbols are non-nullable.A CFG is reduced iff every symbol of every pro-duction is a symbol of at least one complete deriva-tion.
A reduced grammar is empty iff its productionset is empty (P = ?).
We say that a non-emptyreduced grammar is in canonical form iff its vocab-ulary only contains symbols that appear in the pro-ductions of P .3,4Two CFGs G and G?
are weakly equivalent iffthey generate the same string language.
They arestrongly equivalent iff they generate the same set ofstructural descriptions (i.e., parse trees).
It is a wellknown result (See Section 3.2) that every CFG Gcan be transformed in time linear w.r.t.
|G| into astrongly equivalent (canonical) reduced CFG G?.For a given input string w ?
T ?, we define its3We may say that the canonical form of the empty reducedgrammar is ({S}, ?, ?, S) though the axiom S does not appearin any production.4Note that the pair (P, S) completely defines a reduced CFGG = (N,T, P, S) in canonical form since we have N = {X0 |X0 ?
?
?
P} ?
{S}, T = {Xi | X0 ?
X1 ?
?
?Xp ?P ?1 ?
i ?
p}?N .
Thus, in the sequel, we often note simplyG = (P, S) grammars in canonical form.95ranges as the set Rw = {[i..j] | 1 ?
i ?
j ?|w| + 1}.
If w = w1tw3 ?
T ?
is a terminal string,and if t ?
T ?
{?}
is a (terminal or empty) sym-bol, the instantiation of t in w is the triple notedt[i..j] where [i..j] is a range with i = |w1| + 1 andj = i + |t|.
More generally, the instantiation of theterminal string w2 in w1w2w3 is noted w2[i..j] withi = |w1| + 1 and j = i + |w2|.
Obviously, the in-stantiation of w itself is then w[1..1 + |w|].Let us consider an input string w = w1w2w3and a CFG G. If we have a complete derivationd = S ?
?Gw1Aw3 A??
?G w1?w3?
?Gw1w2w3, wesee that A derives w2 (we have A +?G w2).
More-over, in this complete derivation, we also know arange in Rw, namely [i..j], which covers the sub-string w2 which is derived by A (i = |w1| + 1and j = i + |w2|).
This is represented by the in-stantiated nonterminal symbol A[i..j].
In fact, eachsymbol which appears in a complete derivation maybe transformed into its instantiated counterpart.
Wethus talk of instantiated productions or (complete)instantiated derivations.
For a given input text w,and a CFG G, let PwG be the set of instantiated pro-ductions that appears in all complete instantiatedderivations.5 The pair (PwG , S[1..|w|+1]) is the (re-duced) shared parse forest in canonical form.62.2 Finite-state automataA finite-state automaton (FSA) is the 5-tuple A =(Q,?, ?, q0, F ) where Q is a non empty finite setof states, ?
is a finite set of terminal symbols, ?
isthe transition relation ?
= {(qi, t, qj)|qi, qj ?
Q ?t ?
T ?
{?
}}, q0 is a distinguished element of Qcalled the initial state and F is a subset of Q whoseelements are called final states.
The size of A isdefined by |A| = |?|.As usual, we define both a configuration as an ele-ment of Q?T ?
and derive a binary relation between5For example, in the previous complete derivationd, let the right-hand side ?
be the (vocabulary) stringX1 ?
?
?Xk ?
?
?Xp in which each symbol Xk derives the ter-minal string xk ?
T ?
(we have Xk ?
?Gxk and w2 =x1 ?
?
?xk ?
?
?xp), then the instantiated production A[i0..ip] ?X1[i0..i1] ?
?
?Xk[ik?1..ik] ?
?
?Xp[ip?1..ip] with i0 = |w1| +1, i1 = i0 + |x1|, .
.
.
, ik = ik?1 + |xk| .
.
.
and ip = i0 + |w2|is an element of PwG .6The popular notion of shared forests mainly comes from(Billot and Lang, 1989).configurations, noted ?Aby (q, tx) ?A(q?, x), iff(q, t, q?)
?
?.
If w?w??
?
T ?, we call derivation anysequence of the form (q?, w?w??)
?A?
?
?
?A(q?
?, w??
).If w ?
T ?, the initial configuration is noted c0 andis the pair (q0, w).
A final configuration is noted cfand has the form (qf , ?)
with qf ?
F .
A completederivation is a derivation which starts with c0 andends in a final configuration cf .
In that case we havec0?
?Acf .The language L(A) defined (generated, recog-nized) by the FSA A is the set of all terminal stringsw for which there exists a complete derivation.
Wesay that an FSA is empty iff its language is empty.Two FSAs A and A?
are equivalent iff they definedthe same language.An FSA is ?-free iff its transition relation has theform ?
= {(qi, t, qj)|qi, qj ?
Q, t ?
?
}, except per-haps for a distinguished transition, the ?-transitionwhich has the form (q0, ?, qf ), qf ?
F and allowsthe empty string ?
to be in L(A).
Every FSA can betransformed into an equivalent ?-free FSA.An FSA A = (Q,?, ?, q0, F ) is reduced iff everyelement of ?
appears in a complete derivation.
Areduced FSA is empty iff we have ?
= ?.
We saythat a non-empty reduced FSA is in canonical formiff its set of states Q and its set of terminal symbols?
only contain elements that appear in the transitionrelation ?.7 It is a well known result that every FSAA can be transformed in time linear with |A| into anequivalent (canonical) reduced FSA A?.2.3 Input strings and input DAGsIn many NLP applications8 the source text cannotbe considered as a single string of terminal symbolsbut rather as a finite set of terminal strings.
Thesesets are finite languages which can be defined byparticular FSAs.
These particular type of FSAs arecalled directed-acyclic graphs (DAGs).
In a DAGw = (Q,?, ?, q0, F ), the initial state q0 is 1 and weassume that there is a single final state f (F = {f}),Q is a finite subset of the positive integers less thanor equal to f : Q = {i|1 ?
i ?
f}, ?
is the set ofterminal symbols.
For the transition relation ?, we7We may say that the canonical form of the empty reducedFSA is ({q0}, ?, ?, q0, ?)
though the initial state q0 does notappear in any transition.8Speech processing, lexical ambiguity representation, .
.
.96require that its elements (i, t, j) are such that i < j(there are no loops in a DAG).
Without loss of gen-erality, we will assume that DAGs are ?-free reducedFSAs in canonical form and that any DAG w is notedby a triple (?, ?, f) since its initial state is always 1and its set of states is {i | 1 ?
i ?
f}.For a given CFG G, the recognition of an inputDAG w is equivalent to the emptiness of its inter-section with G. This problem can be solved in timelinear in |G| and cubic in |Q| the number of states ofw.If the input text is a DAG, the previous notions ofrange, instantiations and parse forest easily general-ize: the indices i and j which in the string case locatethe positions of substrings are changed in the DAGcase into DAG states.
For example if A[i0..ip] ?X1[i0..i1] ?
?
?Xp[ip?1..ip] is an instantiated produc-tion of the parse forest for G = (N,T, P, S) andw = (?, ?, f), we have A ?
X1 ?
?
?Xp ?
P andthere is a path in the input DAG from state i0 to stateip via states i1, .
.
.
, ip?1.Of course, any nonempty terminal string w ?
T+,may be viewed as a DAG (?, ?, f) where ?
= {t |w = w1tw2 ?
t ?
T}, ?
= {(i, t, i + 1) | w =w1tw2?t ?
T?i = 1+|w1|} and f = 1+|w|.
If theinput string w is the empty string ?, the associatedDAG is (?, ?, f) where ?
= ?, ?
= {(1, ?, 2)} andf = 2.
Thus, in the sequel, we will assume that theinputs of our parsers are not strings but DAGs.
As aconsequence the size (or length) of a sentence is thesize of its DAG (i.e., its number of transitions).3 Filtering Strategies3.1 Gold StrategyLet G = (N,T, P, S) be a CFG, w = (?, ?, f)be an input DAG of size n = |?| and ?Fw?
=(?Pw?, S[1..f ]) be the reduced output parse for-est in canonical form.
From ?Pw?, it is pos-sible to extract a set of (reduced) uninstanti-ated productions P gw = {A ?
X1 ?
?
?Xp |A[i0..ip] ?
X1[i0..i1]X2[i1..i2] ?
?
?Xp[ip?1..ip] ??Pw?
}, which, together with the axiom S, defines anew reduced CFG Ggw = (P gw, S) in canonical form.This grammar is called the gold grammar of G forw, hence the superscript g. Now, if we use Ggw toreparse the same input DAG w, we will get the sameoutput forest ?Fw?.
But in that case, we are sure thatevery production in P gw is used in at least one com-plete derivation.
Now, if this process is viewed asa filtering strategy that computes a filtering functionas introduced in Section 1, it is clear that this strat-egy is size-optimal in the sense that P gw is of minimalsize, we call it the gold strategy and the associatedgold filtering function is noted g. Since we do notwant that a filtering strategy looses parses, the resultGfw = (P fw , S) of any filtering function f must besuch that, for every sentence w, P fw is a superset ofP gw.
In other words the recall score of any filteringfunction f must be of 100%.
We can note that theparsing pass which generates Ggw may be led by anyfiltering strategy f .As usual, the precision score (precision for short)of a filtering strategy f (w.r.t.
the gold case) is, fora given w, defined by the quotient |Pgw||P fw|which ex-presses the number of useful productions selected byf on w (for some G).However, it is clear that we are interested in strate-gies that are time-optimal and size-optimal strategiesare not necessarily also time-optimal: the time takenat filtering-time to get a smaller grammar will notnecessarily be won back at parse-time.For a given CFG G, an input DAG w and a filter-ing strategy c, we only have to plot the times takenby the filtering pass and by the parsing pass to makesome estimations on their average (median, decile)parse times and then to decide which is the winner.However, it may well happens that a strategy whichhas not received the award (with the sample of CFGsand the test sets tried) would be the winner in an-other context!All the following filtering strategies exhibit nec-essary conditions that any production must hold inorder to be in a parse.3.2 The make-a-reduced-grammar AlgorithmAn algorithm which takes as input any CFGG = (N,T, P, S) and generates as output astrongly equivalent reduced CFG G?
and whichruns in O(|G|) can be found in many text books(See (Hopcroft and Ullman, 1979) for example).So as to eliminate from all our intermediate sub-grammars all useless productions, each filteringstrategy will end by a call to such an algorithmnamed make-a-reduced-grammar.97The make-a-reduced-grammar algorithm worksas follows.
It first finds all productive9 symbols.
Af-terwards it finds all reachable10 symbols.
A symbolis useful (otherwise useless) if it is both productiveand reachable.
A production A?
X1 ?
?
?Xp is use-ful (otherwise useless) iff all its symbols are useful.A last scan over the grammar erases all useless pro-duction and leaves the reduced form.
The canonicalform is reached in only retaining in the nonterminaland terminal sets of the sub-grammar the symbolswhich occur in the (useful) production set.3.3 Basic Filtering Strategy: b-filterThe basic filtering strategy (b-filter for short) whichis described in this section will always be tried thefirst.
Thus, its input is the couple (G,w) whereG = (N,T, P, S) is the large initial CFG and the in-put sentence w is a reduced DAG in canonical formw = (?, ?, f) of size n. It generates a reduced CFGin canonical form noted Gb = (P b, S) in which thereferences to both G and w are assumed.
Besidesthis b-filter, we will examine in Sections 3.4 and 3.5two others filtering strategies named a and d. Thesefilters will always have as input a couple (Gc, w)where Gc = (P c, S) is a reduced CFG in canonicalform which has already been filtered by a previoussequence of strategies noted c. They generate a re-duced CFG in canonical form noted Gcf = (P cf , S)with f = a or f = d respectively.
Of course it mayhappens that Gcf is identical to Gc if the f -filter isnot effective.
A filtering strategy or a combination offiltering strategies may be applied several times andlead to a filtered grammar of the form say Gba2dain which the sequence ba2da explicits the order inwhich the filtering strategies have been performed.We may even repeatedly apply a until a fixed pointis reached before applying d, and thus get somethingof the form Gba?d.The idea behind the b-filter is very simple and haslargely been used in lexicalized formalisms parsing,in particular in LTAG (Schabes et al, 1988) parsing.The filter rejects productions of P which contain ter-minal symbols that do not occur in ?
(i.e., that arenot terminal symbols of the DAG w) and thus takes9X ?
V is productive iff we have X ?
?Gw,w ?
T ?.10X ?
V is reachable iff we have S ?
?Gw1Xw2, w1w2 ?T ?.S ?
AB (1)S ?
BA (2)A ?
a (3)A ?
ab (4)B ?
b (5)B ?
bc (6)Table 1: A simple grammarO(|G|) time if we assume that the access to the ele-ments of the terminal set ?
is performed in constanttime.
Unlexicalized productions whose right-handsides are in N?
are kept.
It also rejects productionsin which several terminal symbol occurs, in an orderwhich is not compatible with the linear order of theinput.Consider for example the set of productionsshown in Table 1 and assume that the source textis the terminal string ab.
It is clear that the b-filterwill erase production 6 since c is not in the sourcetext.The execution of the b-filter produces a (non-reduced) CFG G?
such that |G?| ?
|G|.
However, itmay be the case that some productions of G?
are use-less, it will thus be the task of the make-a-reduced-grammar algorithm to transform G?
into its reducedcanonical form Gb in time O(|G?|).
The worst-casetotal running time of the whole b-filter pass is thusO(|G| ?
n).We can remark that, after the execution of the b-filter, the set of terminal symbols of Gb is a subsetof T ?
?.3.4 Adjacent Filtering Strategy: a-filterAs explained before, we assume that the input tothe adjacent filtering strategy (a-filter for short) de-scribed in this section is a couple (Gc, w) whereGc = (N c, T c, P c, S) is a reduced CFG in canon-ical form.
However, the a-filter would also workfor a non-reduced CFG.
As usual, we define thesymbols of Gc as the elements of the vocabularyV c = N c ?
T c.The idea is to erase productions that cannot bepart of any parses for w in using an adjacency crite-ria: if two symbols are adjacent in a rule, they must98derive terminal symbols that are also adjacent in w.To give a (very) simple practical idea of what wemean by adjacency criteria, let us consider again thesource string ab and the grammar defined in Table 1in which the last production has already been erasedby the b-filter.The fact that the B-production ends with a b andthat the A-productions all start with an a, impliesthat production 2 is in a complete parse only if thesource text is such that b is immediately followedby a.
Since it is not the case, production 2 can beerased.More generally, consider a production of the formA ?
?
?
?XY ?
?
?
.
If for each couple (a, b) ?
T 2 inwhich a is a terminal symbol that can terminate (theterminal strings generated by) X and b is a terminalsymbol that can lead (the terminal strings generatedby) Y , there is no transition on b that can follow atransition on a in the DAG w, it is clear that the pro-duction A?
?
?
?XY ?
?
?
can be safely erased.Now assume that we have the following (left)derivation Y ??
Y1?1 ??
Yi?i ?
?
?
?1 ???
?
?
Yp?1??pYp?p?
?pYp?p ?
?
?
?1 ??
Yp?p ?
?
?
?1,with ?p ??
?.
If for each couple (a, b?)
in whicha has the previous definition and b?
is a terminalsymbol that can lead (the terminal strings gener-ated by) Yp, there is no transition on b?
that can fol-low a transition on a in the DAG w, the productionYp?1 ?
?pYp?p can be erased if it is not valid inanother context.Moreover, consider a (right) derivation of theform X ??
?1X1 ??
?1 ?
?
?
?iXi ???
?
?
Xp?1??pXp?p?
?1 ?
?
?
?pXp?p ??
?1 ?
?
?
?pXp,with ?p ??
?.
If for each couple (a?, b) in which bhas the previous definition and a?
is a terminal sym-bol that can terminate (the terminal strings gener-ated by) Xp, there is no transition on b that can fol-low a transition on a?
in the DAG w, the productionXp?1 ?
?pXp?p can be erased if it is not valid inanother context.In order to formalize these notions we define sev-eral binary relations together with their (reflexive)transitive closure.Within a CFG G = (N,T, P, S), we first defineleft-corner noted x. Left-corner (Nederhof, 1993;Moore, 2000), hereafter LC, is a well-known rela-tion since many parsing strategies are based upon it.We say that X is in the LC of A and we write A x Xiff (A,X) ?
{(B,Y ) | B ?
?Y ?
?
P ?
?
??G?
}.We can write A xA?
?X?X to enforce how the cou-ple (A,X) may be produced.For its dual relation, right-corner, noted y, we saythat X is in the right corner of A and we write X y Aiff (X,A) ?
{(Y,B) | B ?
?Y ?
?
P ?
?
??G?}.
We can write X yA?
?X?A to enforce how thecouple (X,A) may be produced.We also define the first (resp.
last) relation noted?
?t (resp.
??
t) by ?
?t= {(X, t) | X ?
V ?
t ?T ?X ?
?Gtx ?
x ?
T ?}
(resp.
??
t= {(X, t) | X ?V ?
t ?
T ?X ?
?Gxt ?
x ?
T ?
}).We define the adjacent ternary relation on V ?N?
?
V noted ?
and we write X ??
Y iff(X,?, Y ) ?
{(U, ?, V ) | A?
?U?V ?
?
P ??
??G?}.
This means that X and Y occur in that order inthe right-hand side of some production and are sep-arated by a nullable string ?.
Note that X or Y mayor may not be nullable.On the input DAG w = (?, ?, f), we define theimmediately precede relation noted < and we writea < b for a, b ?
?
iff w1abw3 ?
L(w), w1, w3 ??
?.We also define the precede relation noted ?
andwe write a ?
b for a, b ?
?
iff w1aw2bw3 ?L(w), w1, w2, w3 ?
?
?.We can note that ?
is notthe transitive closure of <.11For each production A ?
?X0X1 ?
?
?Xp?1Xp?in P c and for each symbol pairs (X0,Xp) of non-nullable symbols s.t.
X1 ?
?
?Xp?1 ?
?Gc ?, we com-pute two sets A1 and A2 of couples (a, b), a, b ?
T cdefined by A1 = ?0<i?p = {(a, b) | a ??
tX0X1???Xi?1?
Xi ?
?t b} and A2 = ?0?i<p ={(a, b) | a ??
t XiXi+1???Xp?1?
Xp ?
?t b}.
Any11Consider the source string bcab for which we have a+< c,but not a ?
c.99pair (a, b) of A1 is such that the terminal symbola may terminate a phrase of X0 while the terminalsymbol b may lead a phrase of X1 ?
?
?Xp.
SinceX0 and Xp are not nullable, A1 is not empty.
Ifnone of its elements (a, b) is such that a < b, theproduction A ?
?X0X1 ?
?
?Xp?1Xp?
is uselessand can be erased.
Analogously, any pair (a, b) ofA2 is such that the terminal symbol a may termi-nate a phrase of X0X1 ?
?
?Xp?1 while the terminalsymbol b may lead a phrase of Xp.
Since X0 andXp are not nullable, A2 is not empty.
If none ofits elements (a, b) is such that a < b, the produc-tion A ?
?X0X1 ?
?
?Xp?1Xp?
is useless and canbe erased.
Of course if X1 ?
?
?Xp?1 = ?, we haveA1 = A2.12The previous method has checked some adjacentproperties inside the right-hand sides of productions.The following will perform some analogous checksbut at the beginning and at the end of the right-handsides of productions.Let us go back to Table 1 to illustrate our pur-pose.
Recall that, with source text ab, productions 6and 2 have already been erased.
Consider produc-tion 4 whose left-hand side is an A, the terminalstring ab that it generates ends by b.
If we look forthe occurrences of A in the right-hand sides of the(remaining) productions, we only find production 1which indicates that A is followed by B.
Since thephrases of B all start with b (See production 5) andsince in the source text b does not immediately fol-low another b, production 4 can be erased.In order to check that the input sentence w startsand ends by valid terminal symbols, we augmentthe adjacent relation with two elements ($, ?, S) and(S, ?, $) where $ is a new terminal symbol which issupposed to start and to end every sentence.13Let Z ?
?U?
be a production in P c in which Uis non-nullable and ?
??Gc?.
If X is a non-nullablesymbol, we compute the set L = {(a, b) | a ??
tX ??
Y ?x Z xZ?
?U?U ?
?t b}.
Since Gc is reducedand since $ < S, we are sure that the set X ??
Y ?x12It can be shown that the previous check can be performedon (Gc, w) in worst-case timeO(|Gc|?|?|3) (recall that |?| ?n).
This time reduces to O(|Gc| ?
|?|2) if the input sentenceis not a DAG but a string.13This is equivalent to assume the existence in the grammarof a super-production whose right-hand side has the form $S$.Z is non-empty, thus L is also non-empty.14We can associate with each couple (a, b) ?L at least one (left) derivation of the formX?Y ?
?Gcw0aw1?Y ?
?Gc w0aw1w2Y??Gcw0aw1w2w3Z?2Z??U??Gcw0aw1w2w3?U?
?2 ??Gcw0aw1w2w3w4U?
?2 ?
?Gc w0aw1w2w3w4w5b?1?
?2in which w1w2w3w4w5 ?
T c?.
These derivationscontains all possible usages of the production Z ??U?
in a parse.
If for every couple (a, b) ?
L, thestatement a?
b does not hold, we can conclude thatthe production Z ?
?U?
is not used in any parseand can thus be deleted.Analogously, we can check that the order of ter-minal symbols is compatible with both a productionand its right grammatical context.Let Z ?
?U?
be a production in P c in which Uis non-nullable and ?
??Gc?.
If Y is a non-nullablesymbol, we compute the set R = {(a, b) | a ??
tU yZ?
?U?Z ?y X ??
Y ?
?t b}.
Since Gc is reducedand since S < $, we are sure that the set Z ?y X ?
?Y is non-empty, thus R is also non-empty.14To each couple (a, b) ?
R we can asso-ciate at least one (right) derivation of the formX?Y ?
?GcX?w1bw0 ?
?Gc Xw2w1bw0??Gc?1Zw3w2w1bw0Z??U?
?Gc?1?U?w3w2w1bw0 ?
?Gc?1?Uw4w3w2w1bw0 ?
?Gc ?1?
?2aw5w4w3w2w1bw0in which w5w4w3w2w1 ?
T c?.
These deriva-tions contains all possible usages of the productionZ ?
?U?
in a partial parse.
If for every couple(a, b) ?
L, the statement a ?
b does not hold, wecan conclude that the production Z ?
?U?
is notused in any parse and can thus be deleted.Now, a call to the make-a-reduced-grammar al-gorithm produces a reduced CFG in canonical formnamed Gca = (N ca, T ca, P ca, S).14This statement does not hold any more if we exclude fromP c the productions that have been previously erased during thecurrent a-filter.
In that case, an empty set indicates that theproduction Z ?
?U?
can be erased.1003.5 Dynamic Set Automaton FilteringStrategy: d-filterIn (Boullier, 2003) the author has presented amethod that takes a CFG G and computes a FSAthat defines a regular superset of L(G).
However hismethod would produce intractable gigantic FSAs.Thus he uses his method to dynamically computethe FSA at parse time on a given source text.
Basedon experimental results, he shows that his methodcalled dynamic set automaton (DSA) is tractable.He uses it to guide an Earley parser (See (Ear-ley, 1970)) and shows improvements over the nonguided version.
The DSA method can directly beused as a filtering strategy since the states of the un-derlying FSA are in fact sets of items.
For a CFGG = (N,T, P, S), an item (or dotted production)is an element of {[A ?
?.?]
| A ?
??
?
P}.A complete item has the form [A ?
?.
], it indi-cates that the production A ?
?
has been, in somesense, recognized.
Thus, the complete items of theDSA states gives the set of productions selected bythe DSA.
This selection can be further refined if wealso use the mirror DSA which processes the sourcetext from right to left and if we only select completeitems that both belong to the DSA and to its mirror.Thus, if we assume that the input to the DSA fil-tering strategy (d-filter) is a couple (Gc, w) whereGc = (P c, S) is a reduced CFG in canonical form,we will eventually get a set of productions which isa subset of P c. If it is a strict subset, we then ap-ply the make-a-reduced-grammar algorithm whichproduces a reduced CFG in canonical form namedGcd = (P cd, S).The Section 4 will give measures that may help tocompare the practical merits of the a and d-filteringstrategies.4 ExperimentsThe measures presented in this section have beentaken on a 1.7GHz AMD Athlon PC with 1.5 Gbof RAM running Linux.
All parsers are written in Cand have been compiled with gcc 2.96 with the O2optimization flag.4.1 Grammars and corpusWe have performed experiments with two largegrammars described below.
The first one is an auto-matically generated CFG, the other one is the CFGequivalent of a TIG automatically extracted from afactorized TAG.The first grammar, named GT>N , is a variant ofthe CFG backbone of a large-coverage LFG gram-mar for French used in the French LFG parser de-scribed in (Boullier and Sagot, 2005).
In this vari-ant, the set T of terminal symbols is the whole set ofFrench inflected forms present in the Lefff , a large-coverage syntactic lexicon for French (Sagot et al,2006).
This leads to as many as 407,863 differentterminal symbols and 520,711 lexicalized produc-tions (hence, the average number of categories ?which are here non-terminal symbols ?
for an in-flected form is 1.27).
Moreover, this CFG entailsa non-neglectible amount of syntactic constraints(including over-generating sub-categorization framechecking), which implies as many as |Pu| = 19, 028non-lexicalized productions.
All in all, GT>N has539,739 productions.The second grammar, named GTIG, is a CFGwhich represents a TIG.
To achieve this, we applied(Boullier, 2000)?s algorithm on the unfolded versionof (Villemonte de La Clergerie, 2005)?s factorizedTAG.
The number of productions in GTIG is com-parable to that of GT>N .
However, these two gram-mars are completely different.
First, GTIG has muchless terminal and non-terminal symbols than GT>N .This means that the basic filter may be less efficienton GTIG than on GT>N .
Second, the size of GTIGis enormous (more than 10 times that of GT>N ),which shows that right-hand sides of GTIG?s pro-ductions are huge (the average number of right-handside symbols is more than 24).
This may increasethe usefulness of a- and d-filtering strategies.Global quantitative data about these grammars isshown in Table 2.Both grammars, as evoked in the introduction,have not been written by hand.
On the contrary, theyare automatically generated from a more abstractand more compact level (a meta-level over LFG forGT>N , and a metagrammar for GTIG).
These gram-mars are not artificial grammars set up only for thisexperiment.
On the contrary, they are automaticallygenerated huge real-life CFGs that are variants ofgrammars used in real NLP applications.Our test suite is a set of 3093 French journalisticsentences.
These sentences are the general lemonde101G |N | |T | |P | |Pu| |G|GT>N 7,862 407,863 539,739 19,028 1,123,062GTIG 448 173 493,408 4,338 12,455,767Table 2: Sizes of the grammars GT>N and GTIGused in our experimentspart of the EASy parsing evaluation campaign cor-pus.
Raw sentences have been turned into DAGsof inflected forms known by both grammar/lexiconcouples.15 This step has been achieved by the pre-syntactic processing chain SXPipe (Sagot and Boul-lier, 2005).
They are all recognized by both gram-mars.16 The resulting DAGs have a median size of28 and an average size of 31.7.Before entering into details, let us give here thefirst important result of these experiments: it wasactually possible to build parsers out of GT>N andGTIG and to parse efficiently with the resultingparsers (we shall detail later on efficiency results).Given the fact that we are dealing with grammarswhose sizes are respectively over 1,000,000 and over12,000,000, this is in itself a very satisfying result.4.2 Precision resultsLet us recall informally that the precision of a filter-ing strategy is the proportion of productions in theresulting sub-grammar that are in the gold grammar,i.e., that have effectively instantiated counterparts inthe final parse forest.We have applied different strategies so as to com-pare their precisions.
The results on GT>N andGTIG are summed up in Table 3.
These results giveseveral valuable results.
First, as we expected, thebasic b-filter drastically reduces the size of the gram-mar.
The result is even better on GT>N thanks to itslarge number of terminal symbols.
Second, both theadjacency a-filter and the DSA d-filter efficiently re-duce the size of the grammar: on GT>N , the a-filtereliminates 20% of the productions they receive asinput, a bit less for the d-filter.
Indeed, the a-filterperforms better than the d-filter introduced in (Boul-15As seen above, inflected forms are directly terminal sym-bols of GT>N , while GTIG uses a lexicon to map these in-flected forms into its own terminal symbols, thereby possiblyintroducing lexical ambiguity.16Approx.
15% of the original set of sentences were not rec-ognized, and required error recovery techniques; we decided todiscard them for this experiment.Strategy Average precisionGT>N GTIGno filter 0.04% 0.03%b 62.87% 39.43%bd 74.53% 66.56%ba 77.31% 66.94%ba?
77.48% 67.48%bad 80.27% 77.16%ba?d 80.30% 77.41%gold 100% 100%Table 3: Average precision of six different filteringstrategies on our test corpus with GT>N and GTIG.lier, 2003), at least as precision is concerned.
Weshall see later that this is still the case on globalparsing times.
However, applying the d-filter afterthe a-filter still removes a non-neglectible amountof productions:17 each technique is able to eliminateproductions that are kept by the other one.
The resultof these filters is suprisingly good: in average, afterall filters, only approx.
20% of the productions thathave been kept will not be successfully instantiatedin the final parse forest.
Third, the adjacency filtercan be used in its one-pass mode, since almost allthe benefit from the full (fix-point) mode is alreadyreached after the first application.
This is practicallya very valuable result, since the one-pass mode isobviously faster than the full mode.However, all these filters do require computingtime, and it is necessary to evaluate not only the pre-cision of these filters, but also their execution timeas well as the influence they have on the global (in-cluding filtering) parsing time .4.3 Parsing time and best filterFilter execution times for the six filtering strategiesintroduced in Table 3 are illustrated for GT>N inFigure 1.
These graphics show three extremely valu-able pieces of information.
First, filtering times areextremely low: the average filtering time for theslowest filter (ba?d, i.e., basic plus full adjacencyplus DSA) on 40-word sentences is around 20 ms.Second, on small sentences, filtering times are virtu-ally zero.
This is important, since it means that there17Although not reported here, applying the a before d leadsto the same conclusion.10200.020.040.060.080.10  20  40  60  80  100Filter executiontime(seconds)Sentence lengthMedian filtering timeFiltering time at percentile rank 90Filtering time at percentile rank 1000.020.040.060.080.10  20  40  60  80  100Filter executiontime(seconds)Sentence lengthMedian filtering timeFiltering time at percentile rank 90Filtering time at percentile rank 1000.020.040.060.080.10  20  40  60  80  100Filter executiontime(seconds)Sentence lengthMedian filtering timeFiltering time at percentile rank 90Filtering time at percentile rank 10b-filter bd-filter ba-filter00.020.040.060.080.10  20  40  60  80  100Filter executiontime(seconds)Sentence lengthMedian filtering timeFiltering time at percentile rank 90Filtering time at percentile rank 1000.020.040.060.080.10  20  40  60  80  100Filter executiontime(seconds)Sentence lengthMedian filtering timeFiltering time at percentile rank 90Filtering time at percentile rank 1000.020.040.060.080.10  20  40  60  80  100Filter executiontime(seconds)Sentence lengthMedian filtering timeFiltering time at percentile rank 90Filtering time at percentile rank 10ba?-filter bad-filter ba?d-filterFigure 1: Filtering times for six different strategies with GT>Nis almost no fixed cost to pay when we use thesefilters (let us recall that without any filter, buildingefficient parsers for such a huge grammar is highlyproblematic).
Third, all these filters, at least whenused with GT>N , are executed in a time which islinear w.r.t.
the size of the input sentence (i.e., thesize of the input DAG).The results on GTIG lead to the same conclusions,with one exception: with this extremely huge gram-mar with so long right-hand sides, the basic filteris not as fast as on GT>N (and not as precise, aswe will see below, which slows down the make-a-reduced-grammar algorithm since it is applied ona larger filtered grammars).
For example, the me-dian execution time for the basic filter on sentenceswhose size is approximately 40 is 0.25 seconds,to be compared with the 0.00 seconds reached onGT>N (this zero value means a median time strictlylower than 0.01 seconds, which is the granularity ofour time measurments).Figure 2 and 3 show the global (filtering+parsing)execution time for the 6 different filters.
We onlyshow median times computed on classes of sen-tences of length 10i to 10(i + 1) ?
1 and plottedwith a centered x-coordinate (10(i + 1/2)), but re-sults with other percentiles or average times on thesame classes draw the same overall picture.00.050.10.150.20  20  40  60  80  100Averageglobalexecutiontime(seconds)Sentence lengthBasic filter onlyDSA filterOne-pass adjacency filterFull adjacency filterOne-pass adjacency filter and DSA filterFull adjacency filter and DSA filterFigure 2: Global (filtering+parsing) times for sixdifferent strategies with GT>NOne can see that the results are completely differ-ent, showing a strong dependency on the character-istics of the grammar.
In the case of GT>N , the hugenumber of terminal symbols and the reasonable av-erage size of right-hand sides of productions, the ba-sic filtering strategy is the best strategy: although itis fast because relatively simple, it reduces the gram-mar extremely efficiently (it has a 60.56% precision,to be compared with the precision of the void filterwhich is 0.04%).
Hence, for GT>N , our only result10300.511.520  20  40  60  80  100Averageglobalexecutiontime(seconds)Sentence lengthBasic filter onlyDSA filterOne-pass adjacency filterFull adjacency filterOne-pass adjacency filter and DSA filterFull adjacency filter and DSA filterFigure 3: Global (filtering+parsing) times for sixdifferent strategies with GTIGis that this basic filter does allow us to build an effi-cient parser (the most efficient one), but that refinedadditionnal filtering strategies are not useful.The picture is completely different with GTIG.Contrary to GT>N , this grammar has comparativelyvery few terminal and non-terminal symbols, andvery long right-hand sides.
These two facts leadto a lower precision of the basic filter (39.43%),which keeps many more productions when appliedon GTIG than when applied on GT>N , and leads,when applied alone, to the less efficient parser.
Thisgives to the adjacency filter much more opportunityto improve the global execution time.
However, thecomplexity of the grammar makes the constructionof the DSA filter relatively costly despite its preci-sion, leading to the following conclusion: on GTIG(and probably on any grammar with similar charac-teristics), the best filtering strategy is the one-passadjacency strategy.
In particular, this leads to an im-provement over the work of (Boullier, 2003) whichonly introduced the DSA filter.
Incidentally, theextreme size of GTIG leads to much higher pars-ing times, approximately 10 times higher than withGT>N , which is consistent with the ratio betweenthe sizes of both involved grammars.5 ConclusionIt is a well known result in optimization techniquesthat the key to practically improve these processes isto reduce their search space.
This is also the case inparsing and in particular in CF parsing.Many parsers process their inputs from left toright but we can find in the literature other parsingstrategies.
In particular, in NLP, (van Noord, 1997)and (Satta and Stock, 1994) propose bidirectional al-gorithms.
These parsers have the reputation to havea better efficiency than their left-to-right counterpart.This reputation is not only based upon experimentalresults (van Noord, 1997) but also upon mathemat-ical arguments in (Nederhof and Satta, 2000).
Thisis specially true when the productions of the CFGstrongly depend on lexical information.
In that casethe parsing search space is reduced because the con-straints associated to lexical elements are evaluatedas early as possible.
We can note that our filteringstrategies try to reach the same purpose by a totallydifferent mean: we reduce the parsing search spaceby eliminating as many productions as possible, in-cluding possibly non-lexicalized productions whoseirrelevance to parse the current input can not be di-rectly deduced from that input.We can also remark that our results are not in con-tradiction with the claims of (Nederhof and Satta,2000) in which they argue that ?Earley algorithmand related standard parsing techniques [.
.
. ]
can-not be directly extended to allow left-to-right andcorrect-prefix-property parsing in acceptable timebound?.
First, as already noted in Section 1, ourmethod does not work for any large CFG.
In orderto work well, the first step of our basic strategy mustfilter out a great amount of (lexicalized) productions.To do that, it is clear that the set of terminals in theinput text must select a small ratio of lexicalized pro-ductions.
To give a more concrete idea we advo-cate that the selected productions produce roughly agrammar of normal size out of the large grammar.Second, our method as a whole clearly does not pro-cess the input text from left-to-right and thus doesnot enter in the categories studied in (Nederhof andSatta, 2000).
Moreover, the authors bring strong evi-dences that in case of polynomial-time off-line com-pilation of the grammar, left-to-right parsing cannotbe performed in polynomial time, independently ofthe size of the lexicon.
Once again, if our filter passis viewed as an off-line processing of the large inputgrammar, our output is not a compilation of the largegrammar, but a (compilation of a) smaller grammar,specialized in (some abstractions of) the source textonly.
In other words their negative results do not104necessarily apply to our specific case.The experiment campaign as been conducted inusing an Earley-like parser.18 We have also success-fuly tried the coupling of our filtering strategies witha CYK parser (Kasami, 1967; Younger, 1967) aspost-processor.
However the coupling with a GLRparser (See (Satta, 1992) for example) is perhapsmore problematic since the time taken to build upthe underlying nondeterministic LR automaton fromthe sub-grammar can be prohibitive.Though no definitive answer can be made to thequestion asked in the title, we have shown that, insome cases, the answer is certainly yes.ReferencesSylvie Billot and Bernard Lang.
1989.
The structure ofshared forests in ambiguous parsing.
In Meeting ofthe Association for Computational Linguistics, pages143?151.Pierre Boullier and Beno?
?t Sagot.
2005.
Efficient and ro-bust LFG parsing: SxLfg.
In Proceedings of IWPT?05,pages 1?10, Vancouver, Canada.Pierre Boullier.
2000.
On TAG parsing.
Traitement Au-tomatique des Langues (T.A.L.
), 41(3):759?793.Pierre Boullier.
2003.
Guided Earley parsing.
In Pro-ceedings of IWPT 03, pages 43?54, Nancy, France.Jay Earley.
1970.
An efficient context-free parsing algo-rithm.
Communication of the ACM, 13(2):94?102.Jeffrey D. Hopcroft and John E. Ullman.
1979.
Intro-duction to Automata Theory, Languages, and Compu-tation.
Addison-Wesley, Reading, Mass.Aravind Joshi.
1997.
Parsing techniques.
In Sur-vey of the state of the art in human language tech-nology, pages 351?356.
Cambridge University Press,New York, NY, USA.Tadao Kasami.
1967.
An efficient recognition and syntaxalgorithm for context-free languages.
Scientific Re-port AFCRL-65?758, Air Force Cambridge ResearchLaboratory, Bedford, Massachusetts, USA.Robert C. Moore.
2000.
Improved left-cornerchart parsing for large context-free gram-mars.
In Proceedings of IWPT 2000, pages18Contrarily to classical Earley parsers, its predictor phaseuses a pre-computed structure which is roughly an LC relation.Note that this feature forces our filters to compute an LC rela-tion on the generated sub-grammar.
This also shows that LCparsers may also benefit from our filtering techniques.171?182, Trento, Italy.
Revised version athttp://www.cogs.susx.ac.uk/lab/nlp/carroll/cfg-resources/iwpt2000-rev2.ps.Mark-Jan Nederhof and Giorgio Satta.
2000.
Left-to-right parsing and bilexical context-free grammars.
InProceedings of the first conference on North Americanchapter of the ACL, pages 272?279, San Francisco,CA, USA.
Morgan Kaufmann Publishers Inc.Mark-Jan Nederhof.
1993.
Generalized left-corner pars-ing.
In Proceedings of the sixth conference on Euro-pean chapter of the ACL, pages 305?314, Morristown,NJ, USA.
ACL.Beno?
?t Sagot and Pierre Boullier.
2005.
From raw cor-pus to word lattices: robust pre-parsing processing.
InProceedings of L&TC 2005, pages 348?351, Poznan?,Poland.Beno?
?t Sagot, Lionel Cle?ment, ?Eric Villemonte de LaClergerie, and Pierre Boullier.
2006.
The Lefff 2 syn-tactic lexicon for french: architecture, acquisition, use.In Proc.
of LREC?06.Giorgio Satta and Oliviero Stock.
1994.
Bidirectionalcontext-free grammar parsing for natural languageprocessing.
Artif.
Intell., 69(1-2):123?164.Giorgio Satta.
1992. Review of ?generalized lr parsing?by masaru tomita.
kluwer academic publishers 1991.Comput.
Linguist., 18(3):377?381.Yves Schabes and Richard C. Waters.
1995.
Tree in-sertion grammar: Cubic-time, parsable formalism thatlexicalizes context-free grammar without changing thetrees produced.
Comput.
Linguist., 21(4):479?513.Yves Schabes, Anne Abeille?, and Aravind K. Joshi.1988.
Parsing strategies with ?lexicalized?
grammars:Application to tree adjoining grammars.
In Proceed-ings of the 12th International Conference on Comput.Linguist.
(COLING?88), Budapest, Hungary.Gertjan van Noord.
1997.
An efficient implementation ofthe head-corner parser.
Comput.
Linguist., 23(3):425?456.
?Eric Villemonte de La Clergerie.
2005.
From metagram-mars to factorized TAG/TIG parsers.
In Proceedingsof IWPT?05, pages 190?191, Vancouver, Canada.Daniel H. Younger.
1967.
Recognition and parsing ofcontext-free languages in time n3.
Information andControl, 10(2):189?208.105
