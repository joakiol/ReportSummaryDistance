Multimodal Database Access on Handheld DevicesElsa Pecourt and Norbert ReithingerDFKI GmbHStuhlsatzenhausenweg3D-66123 Saarbru?cken, Germany{pecourt,reithinger}@dfki.deAbstractWe present the final MIAMM system, a multimodaldialogue system that employs speech, haptic inter-action and novel techniques of information visual-ization to allow a natural and fast access to largemultimedia databases on small handheld devices.1 IntroductionNavigation in large, complex and multidimensionalinformation spaces is still a challenging task.
Thesearch is even more difficult in small devices such asMP3 players, which only have a reduced screen andlack of a proper keyboard.
In the MIAMM project1we have developed a multimodal dialogue systemthat uses speech, haptic interaction and advancedtechniques for information visualization to allow anatural and fast access to music databases on smallscale devices.
The user can pose queries in naturallanguage, using different dimensions, e.g.
releaseyear, genre, artist, or mood.
The retrieved data arepresented along this dimensions using various vi-sualization metaphors.
Haptic feedback allows theuser to feel the size, density and structure of the vi-sualized data to facilitate the navigation.
All modal-ities are available for the user to access and navi-gate through the database, and to select titles to beplayed.The envisioned end-user device is a handheldPersonal Digital Assistant (PDA, see figure 1) thatprovides an interface to a music database.
Thedevice includes a screen where data and systemmessages are visualized, three force-feedback but-tons on the left side and one combined scrollwheel/button on the upper right side, that can beused to navigate on the visualized data, as well as toperform actions on the data items (e.g.
play or se-lect a song), a microphone to capture spoken input,and speakers to give audio output.
Since we do notdevelop the hardware, we simulate the PDA usinga 3D model on a computer screen, and the buttons1http://www.miamm.orgFigure 1: The PDA simulator with the terrain visu-alization of the databaseby means of Phantom devices2 that allow the user totouch and manipulate virtual objects.In the rest of this paper, we will first give anoverview of the visualization metaphors, the MI-AMM architecture, and a short description of itsinterface language.
Then we will demonstrate itsfunctionality using an example dialogue.
For moredetails on the MIAMM system and its componentssee (Reithinger et al, 2004).2 Visualization metaphorsThe information from the database is presented onthe device using metaphors of real world objects(cf.
conceptual spaces (Ga?rdenfors, 2000)) so as toprovide an intuitive handling of abstract concepts.The lexicon metaphor, shown in figure 2 to the left,presents the items alphabetically ordered in a rotarycard file.
Each card represents one album and con-tains detailed background information.
The time-2http://www.sensable.comFigure 2: Visualizationsline visualization shows the items in chronologi-cal order, on a ?rubber?
band that can be stretchedto get a more detailed view.
The wheel metaphorpresents the items as a list on a conveyor belt, whichcan be easily and quickly rotated.
Finally, the ter-rain metaphor (see figure 1) visualizes the entiredatabase.
The rendering is based on a three layertype hierarchy, with genre, sub-genre and title lay-ers.
Each node of the hierarchy is represented asa circle containing its daughter nodes.
Similaritiesbetween the items are computed from the genre andmood information in the database and mapped tointeraction forces in a physical model that groupssimilar items together on the terrain.
Since usuallyalbums are assigned more than one genre, they canbe contained in different circles and therefore be re-dundantly represented on the terrain.
This redun-dancy is made clear by lines connecting the differentinstances of the same item.3 The MIAMM prototypeThe MIAMM system uses the standard architec-ture for dialogue systems with analysis and gener-ation layers, interaction management and applica-tion interface (see figure 3).
To minimize the reac-tion delay of haptic feedback, the visual-haptic in-teraction component is decoupled from other moretime-consuming reasoning processes.
The Germanexperimental prototype3 incorporates the following3There are also French and English versions of the system.The modular architecture facilitates the replacement of the lan-guage dependent modules.components, some of which were reused from otherprojects (semantic parser and action planning): aspeaker independent, continuous speech recognizerconverts the spoken input in a word lattice; it usesa 500 word vocabulary, and was trained on a auto-matically generated corpus.
A template based se-mantic parser for German, see (Engel, 2004), inter-prets this word lattice semantically.
The multimodalfusion module maintains the dialogue history andhandles anaphoric expressions and quantification.The action planner, an adapted and enhanced ver-sion of (Lo?ckelt, 2004), uses non-linear regressionplanning and the notion of communicative gamesto trigger and control system actions.
The visual-haptic interaction manager selects the appropriatevisualization metaphor based on data characteris-tics, and maintains the visualization history.
Finally,the domain model provides access to the MYSQLdatabase, which contains 7257 records with 85722songs by 667 artists.
Speech output is done byspeech prompts, both for spoken and for written out-put.
The prototype also includes a MP3 Player toplay the music and speech output files.
The demon-stration system requires a Linux based PC for themajor parts of the modules written in Java and C++,and a Windows NT computer for visualization andhaptics.
The integration environment is based on thestandard Simple Object Access Protocol SOAP4 forinformation exchange in a distributed environment.The communication between the modules uses adeclarative, XML-schema based representation lan-4http://www.w3.org/TR/SOAP/Continuous SpeechVisualizationDisplayHaptic ProcessorHaptic DeviceSemantic RepresentationDatabaseMicrophoneSpeakerVisual?Haptic GenerationVisual?Haptic InterpretationDIALOGUE MANAGERMultimodal FusionAction PlannerVisualization StatusVisualization RequestResponseRepresentationGoalDomain Model Query ResponseDomain Model DatabaseQueryRecognizerMP3 PlayerSpeech promptsMusic filesSpeech GenerationRequestVisual?Haptic InteractionPlayer RequestAudio OutputSemantic InterpretationAudio InputFigure 3: MIAMM architectureguage called MMIL (Romary and Bunt, 2002).
Thisinterface specification accounts for the incrementalintegration of multimodal data to achieve a full un-derstanding of the multimodal acts within the sys-tem.
Therefore, it is flexible enough to handle thevarious types of information processed and gener-ated by the different modules.
It is also independentfrom any theoretical framework, and extensible sothat further developments can be incorporated.
Fur-thermore it is compatible with existing standardiza-tion initiatives so that it can be the source of fu-ture standardizing activities in the field5.
Figure 4shows a sample of MMIL representing the output ofthe speech interpretation module for the user?s ut-terance ?Give me rock?.4 An exampleTo sketch the functionality of the running prototypewe will use a sample interaction, showing the user?sactions, the system?s textual feedback on the screenand finally the displayed information.
Some of thedialogue capabilities of the MIAMM system in thisexample are, e.g.
search history (S2), relaxationof queries (S3b), and anaphora resolution (S5).
Atany moment of the interaction the user is allowed tonavigate on the visualized items, zoom in and outfor details, or change the visualization metaphor.U1: Give me rockS1a: I am looking for rockS1b: displays a terrain with rock albumsU2: I want something calmS2a: I am looking for calm rockS2b: displays list of calm rock albumsU3: I want something from the 30?sS3a: I am looking for calm rock5The data categories are expressed in a RDF format com-patible with ISO 11179-31930-1939S3b: I could only find albums ofthe adjacent yearsdisplays list of calm rock albums of the 40?sU4: What about the 50?sS4a: I am looking for calm rock1950-1959S4b: displays a map with rock albumsU5: selects ALBUM with the haptic buttonsPlay this oneS5a: Playing ALBUMS5b: MP3 player startsWe will show the processing details on the basisof the first utterance in the sample interaction Giveme rock.
The speech recognizer converts the spo-ken input in a word graph in MPEG7.
The semanticparser analyzes this graph and interprets it semanti-cally.
The semantic representation consists, in thisexample, of a speak and a display event, withtwo participants, the user and music with con-straints on its genre (see figure 4).The multimodal fusion module receives thisrepresentation, updates the dialogue context, andpasses it on to the action planner, which defines thenext goal on the basis of the propositional contentof the top event (in the example event id1) andits object (in the example participant id3).
In thiscase the user?s goal cannot be directly achieved be-cause the object to display is still unresolved.
Theaction planner has to initiate a database query to ac-quire the required information.
It uses the constrainton the genre of the requested object to produce adatabase query for the domain model and a feed-back request for the visual-haptic interaction mod-ule.
This feedback message (S1a in the example)is sent to the user while the database query is beingdone, providing thus implicit grounding.
The do-<component><event id="id0"><evtType>speak</evtType><speaker>user</speaker><addressee>system</addressee><dialogueAct>request</dialogueAct></event><event id="id1"><evtType>display</evtType></event><participant id="id2"><objType>user</objType><refType>1PPDeixis</refType><refStatus>pending</refStatus></participant><participant id="id3"><objType>music</objType><genre>rock</genre><refType>indefinite</refType><refStatus>pending</refStatus></participant><relationsource="id3"target="id1"type="object"/><relationsource="id1"target="id0"type="propContent"/></component>Figure 4: MMIL samplemain model sends the result back to the action plan-ner who inserts the data in a visualization request.The visual-haptic interaction module computesthe most suitable visualization for this data set, andsends the request to the visualization module to ren-der it.
This component also reports the actual vi-sualization status to the multimodal fusion module.This report is used to update the dialogue context,that is needed for reference resolution.
The user cannow use the haptic buttons to navigate on the searchresults, select a title to be played or continue search-ing.5 ConclusionsThe MIAMM final prototype combines speech withnew techniques for haptic interaction and data visu-alization to facilitate access to multimedia databaseson small handheld devices.
The final evaluationof the system supports our initial hypothesis thatusers prefer language to select information and hap-tics to navigate in the search space.
The visualiza-tions proved to be intuitive (van Esch and Cremers,2004).AcknowledgmentsThis work was sponsored by the European Union(IST-2000-29487).
Thanks are due to our projectpartners: Loria (F), Sony Europe (D), Canon (UK),and TNO (NL).ReferencesRalf Engel.
2004.
Natural language understanding.In Wolfgang Wahlster, editor, SmartKom - Foun-dations of Multi-modal Dialogue Systems, Cog-nitive Technologies.
Springer Verlag (in Press).Peter Ga?rdenfors.
2000.
Conceptual Spaces.
MITPress.Markus Lo?ckelt.
2004.
Action planning.
In Wolf-gang Wahlster, editor, SmartKom - Founda-tions of Multi-modal Dialogue Systems, Cogni-tive Technologies.
Springer Verlag (in Press).Norbert Reithinger, Dirk Fedeler, Ashwani Kumar,Christoph Lauer, Elsa Pecourt, and Laurent Ro-mary.
2004.
Miamm - a multimodal dialoguesystem using haptics.
In Jan van Kuppevelt, LailaDybkjaer, and Niels Ole Bersen, editors, Natu-ral, Intelligent and Effective Interaction in Multi-modal Dialogue Systems.
Kluwer Academic Pub-lications.Laurent Romary and Harry Bunt.
2002.
Towardsmultimodal content representation.
In Proceed-ings of LREC 2002, Workshop on InternationalStandards of Terminology and Linguistic Re-sources Management, Las Palmas.Myra P. van Esch and Anita H. M. Cremers.
2004.User evaluation.
MIAMM Deliverable D1.6.
