In: Proceedings of CoNLL-2000 and LLL-2000, pages 73-78, Lisbon, Portugal, 2000.Using Induced Rules as Complex Featuresin Memory-Based Language LearningAnta l  van den  BoschILK / Computational LinguisticsTilburg University, The NetherlandsAnt al.
vdnBo s ch@kub, nlAbst ractAn extension to memory-based learning is de-scribed in which automatically induced rulesare used as binary features.
These featureshave an "active" value when the left-hand sideof the underlying rule applies to the instance.The RIPPER rule induction algorithm is adoptedfor the selection of the underlying rules.
Thesimilarity of a memory instance to a new in-stance is measured by taking the sum of theweights of the matching rules both instancesshare.
We report on experiments hat indicatethat (i) the method works equally well or bet-ter than RIPPER on various language learningand other benchmark datasets; (ii) the methoddoes not necessarily perform better than defaultmemory-based learning, but (iii) when multi-valued features are combined with the rule-based features, some slight to significant im-provements are observed.1 Ru les  as featuresA common machine-learning solution to classi-fication problems is rule induction (Clark andNiblett, 1989; Quinlan, 1993; Cohen, 1995).The goal of rule induction is generally to inducea set of rules from data, that captures all gener-alisable knowledge within that data, and that isas small as possible at the same time.
Classifica-tion in rule-induction classifiers is based on thefiring of rules on a new instance, triggered bymatching feature values to the left-hand side ofthe rule.
Rules can be of various normal forms,and can furthermore be ordered.
The appropri-ate content and ordering of rules can be hardto find, and at the heart of most rule inductionsystems are strong search algorithms that at-tempt to minimise search through the space ofpossible rule sets and orderings.Although rules appear quite different from in-stances as used in memory-based or instance-based learning (Aha et al, 1991; Daelemans andVan den Bosch, 1992; Daelemans et al, 1997b)there is a continuum between them.
Rules canbe seen as generalised instances; they representthe set of training instances with the same classthat match on the conditions on the left-handside of the rule.
Therefore, classification strate-gies from memory-based learning can naturallybe applied to rules.
For example, (Domingos,1996) describes the RISE system, in which rulesare (carefully) generalised from instances, andin which the k-NN classification rule searchesfor nearest neighbours within these rules whenclassifying new instances.Often, the sets of instances covered by rulesoverlap.
In other words, seen from the instanceperspective, a single instance can match morethan one rule.
Consider the schematic exam-ple displayed in Figure 1.
Three instances withthree multi-valued features match individuallywith one or two of the four rules; for example,the first instance matches with rule 1 (if f l  = Athen c = Z) and with rule 3 (if f2 = C thenc= Z).Pursuing this reasoning, it is possible to in-dex instances by the rules that apply to them.For example, in Figure 1, the first instance canbe indexed by the "active" rule identificationnumbers 1 and 3.
When the left-hand sides ofrules are seen as complex features (in which thepresence of some combination of feature valuesis queried) that are strong predictors of a singleclass, indexing instances by the rules that applyto them is essentially the same as representinginstances by a set of complex features.Note that when a rule matches an instance,this does not guarantee that the class of theinstance is identical to the rule's predicted class- many rules will classify with some amount of73123" ' "  4: f l  f2 f3.-.-..Ig Ig I0 Iif f l=A then c=Zif f l=B and f2=B then c=Yif f2=C then c=Zif f3=C then c=ZcFigure 1: Schematic visualization of the encod-ing of multi-valued instances via matching rulesto rule-indexed instances, characterlsed by thenumbers of the rules that match them.
f l ,  f2,and f3 represent the three features, c representsthe class label.error.
In Figure 1, the third memory instancematches rules 3 and 4 which both predict a Z,while the instance itself has class X.Now when instances are represented this way,they can be used in k-NN classification.
Eachcomplex feature then becomes a binary feature,that can also be assigned some weight (e.g.,gain-ratio feature weights, chi-square, or equalweights (Daelemans et al, 2000)); when a mem-ory instance and a new test instance share com-plex features, their similarity becomes the sumof the weights of the matching features.
In Fig-ure 1, a new instance (bottom) matches rules 2and 4, thereby (partially) matching the secondand third memory instances.
If, for example,rule 4 would have a higher overall weight thanrule 2, the third memory instance would becomethe nearest neighbor.
The k-NN rule then saysthat the class of the nearest neighbour transfersto the new instance, which would mean thatclass X would be copied - which is a differ-ent class than those predicted either by rule 2or 4.
This is a marked difference with classi-fication in RIPPER, where the class is assigneddirectly to the new instance by the rule thatfires first.
It can be expected that many classi-fications in this approach would be identical tothose made by RIPPER, but it is possible thatthe k-NN approach as some consistent advan-tage in the cases where classification diverges.In this paper we investigate some effects ofrecoding instances by complex features inducedby an external rule-induction algorithm, andshow that the approach is promising for lan-guage learning tasks.
We find that the methodworks equally well or better than RIPPER onvarious language learning and other benchmarkdatasets.
However, the method does not nec-essarily perform better than default memory-based learning.
Only when the rule-indexingfeatures are added to the original multi-valuedfeatures, improvements are observed.2 Ru le -Based  Memory :  a lgor i thmA new memory-based learning variant RBM,which stands for Rule-Based Memory, imple-ments the ideas described in the previous sec-tion using the following procedure: given atraining set and a test set of a certain classifi-cation task, (1) apply RIPPER (Cohen, 1995) tothe training set, and collect the set of inducedrules; (2) recode the instances in the trainingand test set according to these rules; (3) ap-ply the basic memory-based learning algorithmIBi-IG to the recoded training set, and k-NN-classify the recoded test set.
We describe achof these steps briefly here.RIPPER (Cohen, 1995) is a fast rule inductionalgorithm that splits the training set in two.On the basis of one part it induces rules in astraightforward way, with potential overfitting.When the induced rules classify instances in theother part of the split training set below someclassification accuracy threshold, they are notstored.
Rules are induced per class, in a certainclass ordering.
By default, the ordering is fromlow-frequency classes to high frequency classes,leaving the most frequent class as the defaultrule, which is generally beneficial for the totaldescription length of the rule set.
In our experi-ments, we let RIPPER order the rules from high-frequent o low-frequent, the idea being thatthis method would yield more complex features.Then, the rule set was taken as the basisfor recoding both the training and test set, asschematically visualised in Figure 1.
As withthe training material, each test set was recodedin batch, but this could have been done on-74line during classification without much compu-tational overhead.
For each language task weexperimented on, we performed 10-fold crossvalidation tests, so ten different rain-test par-titions were produced (Weiss and Kulikowski,1991) that were recoded, and then tested on.Tests were performed with the TiMBL softwarepackage (Daelemans et al, 2000), using the soft-ware's dedicated routines for handling binaryfeatures.
The default IBi-IG algorithm was used(for details, consult (Aha et al, 1991; Daele-mans and Van den Bosch, 1992; Daelemans etal., 1997b), with gain ratio selected as featureweighting metric.3 Resu l tsWe performed experiments on the following fivelanguage data sets - More details on numbers offeatures, values per features, number of classesand number of instances are displayed in Ta-ble 1:D iminut ive  fo rmat ion  (henceforth DIM):choosing the correct diminutive inflectionto Dutch nouns out of five possible: je, tje,pie, kje, and etje, on the basis of phonemicword transcriptions, segmented at thelevel of syllable onset, nucei and codaof the final three syllables of the word.The data stems from a study described in(Daelemans et al, 1997a).Grapheme-phoneme convers ion (GPSM):the conversion of a window of nine lettersto the phonemic transcription of themiddle letter.
From the original data setdescribed in (Van den Bosch, 1997) a 10%subset was drawn.Base-NP  chunk ing  (NPSM): the segmenta-tion of sentences into non-recursive NPs.
(Veenstra, 1998) used the Base-NP tag setas presented in (Ramshaw and Marcus,1995): I for inside a Base-NP, O for out-side a Base-NP, and B for the first wordin a Base-NP following another Base-NP.See (Veenstra, 1998) for more details, and(Daelemans et al, 1999) for a series of ex-periments on the original data set fromwhich we have used a randomly-extracted10%.Par t -o f - speech tagg ing  (POSSM): the disam-biguation of syntactic classes of words inPPparticular contexts.
We assume a taggerarchitecture that processes a sentence froma disambiguated left to an ambiguous rightcontext, as described in (Daelemans et al,1996).
The original data set for the part-of-speech tagging task, extracted from theLOB corpus, contains 1,046,151 instances;we have used a randomly-extracted 10% ofthis data.a t tachment  (PP): the attachment ofa PPin the sequence VP hip PP (VP = verbphrase, 51P = noun phrase, PP = prepo-sitional phrase).
The data consists of four-tuples of words, extracted from the WallStreet Journal Treebank.
From the origi-nal data set, used by (Ratnaparkhi et al,1994), (Collins and Brooks, 1995), and (Za-vrel et al, 1997), (Daelemans et al, 1999)took the train and test set together to formthe particular data also used here.Table 2 lists the average (10-fold cross-validation) accuracies, measured in percentagesof correctly classified test instances, of IBI-IG,RIPPER, and RBM on these five tasks.
The clear-est overall pattern in this table is the high accu-racy of IBi-IG, surpassed only twice by RBM onthe DIM and NPSM tasks (significantly, accord-ing to one-tailed t-tests, with p < 0.05).
Onthe other three tasks, IBI-IG outperforms RBM.RIPPER performs ignificantly more accuratelythan IBi-IG only on the DIM task.
Once again,evidence is collected for the global finding thatforgetting parts of the training material, as ob-viously happens in rule induction, tends to beharmful to generalisation accuracy in languagelearning (Daelemans et al, 1999).A surprising result apparent in Table 2 is thatRBM never performs worse than RIPPER; in fact,it performs significantly more accurately thanRIPPER with the GPSM, NPSM, and POSSM tasks.There appears to be an advantage in the k-NN approach to rule matching and voting, overthe RIPPER strategy of ordered rule firing, withthese tasks.Another advantage, now of RBM as opposedto IBi-IG, is the reduced memory requirementsand resulting speed enhancements.
As listed inTable 3, the average number of rules in the rulesets induced by RIPPER range between 29 and971.
Averaged over all tasks, the rules have on75Data setDIMGPSMPOSNPPP#lFeat.11195114# Values of ~ature1 2 3 4 5 6 7 8 9 10 11 123 51 19 40 3 61 20 79 2 64 1842 42 42 42 41 42 42 42 42155 157 414 395 3845961 5911 5895 5908 51 50 55 49 3 3 33474 4612 68 578043# # Data setCla~ instances 39506i I 67,575 15 104,61725 11423,898Table 1: Specifications of the five investigated language learning tasks: numbers of features, valuesper feature, classes, and instances.
The rightmost column gives the total number of values timesthe number of classes.TaskDIMGPSMNPSMPOSSMPP% Correct est instancesIBi-IG RIPPER RBM96.2?0.6 96 .9?0.7 .
96 .9?0.7 .88.9?0.6 80.4?0.5 83 .3?0.5+ x/97.2?0.3 96.9?0.4 97 .5?0.4 .96.6?0.2 94.3?0.2 95 .0?0.2+x/82.0 :t= 0.5 77.0 ?
0.7 77.0 ?
0.6 +Table 2: Average generalisation accuracies ofIB i - IG,  RIPPER, and RBM on five language learn-ing tasks.
'*' denotes ignificantly better accu-racy of RBM or RIPPER over IBi-IG with p0.05.
'+'  denotes ignificance in the reverse di-rection, x/denotes ignificantly better accuracyof RBM over RIPPER with p < 0.05.average about two to four conditions (feature-value tests).
More importantly, as the thirdcolumn of Table 3 shows, the average numberof active rules in instances is below two for alltasks.
This means that in most instances of anyof the five tasks, only one complex feature (bit)is active.Especially with the smaller rule sets (DIM,NPSM, and PP - which all have few classes, cf.Table 1), RBM's classification is very speedy.
Itreduces, for example, classification of the NPSMtest set from 19 seconds to 1 second 1.
Largerule sets (GPSM), however, can have adverse f-fects - from 8 seconds in ml-IG to 17 secondsin RBM.In sum, we observe two cases (DIM and NPSM)in which RBM attains a significant general?sa-t?on accuracy improvement over IBi-IG as wellas some interesting classification speedup, butfor the other tasks, for now unpredictably, geE-1 Timings are measured on one partition, using a dual-Pentium II 200 Mhz machine running Linux 2.2.TaskDIMGPSMNPSMPOSSMPPRIPPER / RBM#rules c/r f/i61 2.5 1.3971 3.9 1.572 2.8 1.8628 2.7 1.029 3.0 0.3Classif.
time (s)IBi-IG RBM1 18 1719 132 1319 1Table 3: Average number of RIPPER rules, con-ditions per rule (c/r), and coded features perinstance (f/i); and one-partition timings (s) ofclassification of test material in IBI-IG and RBM,for five language tasks.eralisation accuracy losses and even a slowdownare observed.
The latter occurs with GPSM,which has been analysed earlier as being ex-tremely disjunct in class space, and thereforehighly sensitive to the "forgetting exceptionsis harmful" syndrome (Daelemans et al, 1999;Van den Bosch, 1999a).The complex features used in RBM are takenas the only information available; the originalinformation (the feature values) are discarded.This need not be the case; it is possible that therecoded instances are merged with their orig-inal feature-value vectors.
We performed ex-periments in which we made this fusion; theresults are listed in Table 4.
Comparing thecolumn labeled "IBi-IG+RBM, denoting the fu-sion variant, with the IBi-IG column, it can beseen that it reaches ome modest error reduc-tion percentages (rightmost column in Table 4).In fact, with NPSM and POSSM, it performs ig-nificantly better (again, according to one-tailedt-tests, with p < 0.05) than IBI-IG.
On theother hand, adding the (average) 971 complexfeatures to the nine multi-valued features in the76TaskDIMGPSMNPSMPOSSMPP% Correct test instancesIBi-IG IBI-IG-bRBM96.2 ?
0.6 96.2 ?
0.788.9 ?
0.6 88.6 ?
0.497.2?0.3 97.6?0.4.96.6?0.2 96.8?0.2.82.0 ?
0.5 82.1 ?
0.5% Errorreduct.0.0-2.36.04.61.0Table 4: Average general?sat?on accuracies ofIBI - IG and  IB i - IG + RBM, and the percentage oferror reduction, on five language learning tasks.'. '
denotes ignificantly better accuracy of IB1-IG--~-RBM over IBi-IG with p < 0.05.GPSM causes a slight drop in performance - anda slowdown.4 Discuss ionRepresenting instances by complex featuresthat have been induced by a rule induction al-gorithm appears, in view of the measured ac-curacies, a viable alternative approach to us-ing rules, as compared to standard rule induc-tion.
This result is in line with results reportedby Domingos on the RISE algorithm (Domingos,1995; Domingos, 1996).
A marked difference isthat in RISE, the rules are the instances in k-NN classification (and due to the careful gen-eral?sat?on strategy of RISE, they can be veryinstance-specific), while in RBM, the rules arethe features by which the original instances areindexed.
When a nearest neighbor is found to aquery instance in RBM, it is because the two in-stances hare one or more matching rules.
Theactual classification that is transferred from thememory instance to the new instance is just theclassification that this memory item is storedwith - it may well be another class than any ofits matching rules predict.Second, the method is a potential ly helpfulextension to memory-based learning of languageprocessing tasks.
When nothing is known aboutthe characteristics of a language processing dataset, it is advisable to add the induced complexfeatures to the original features, and do k-NNclassification on the combination; it is not ad-visable to base classification only on the inducedcomplex features.
On its own, the method basi-cally inherits a part of the detrimental "forget-ting exceptions i harmful" effect from its rule-induction source (this effect is stronger when% Correct est instancesTask IBI-IG RBM IBi-IG-bRBMCAR 93.9 ?
2.1 98.9 ?
0.8 97.2 ?
1.3NURSERY 94.6 ?
0.6 98.6 ?
0.5 98.7 ?
0.2SPLICE 91.7 ?
1.1 89.0 ?
2.1 92.7 ?
1.7Table 5: Average generalisation accuracies ofIB i - IG,  RIPPER, and RBM on  three machine-learning benchmark tasks.a data set is more disjunct (Daelemans et al,1999)).
Although RBM performs equal to or bet-ter than RIPPER, it often does not regain thelevel of IBi-IG.High disjunctivity appears to be a typical fea-ture of language tasks (Van den Bosch, 1999b);other non-language tasks generally display lessdisjunctivity, which opens the possibility thatthe RBM approach may work well for someof these tasks.
We performed pilot tests onthree machine learning benchmark classificationtasks (taken from the UCI benchmark repos-?tory (Blake and Merz, 1998)) with symbolic,multi-valued features.
Table 5 displays the re-sults of these experiments.
Although the dataset selection is small, the results of RBM and es-pecially of IBi-IG--~-RBM are promising; the lat-ter algorithm is consistently better than IBi-IG.More research and comparisons are needed toarrive at a broader picture.An immediate point of further research liesin the external rule induction algorithm.
First,RIPPER has options that have not been usedhere, but that may be relevant for the currentissue, e.g.
RIPPER's ability to represent setsof values at left-hand side conditions, and itsflexibility in producing larger or smaller num-bers of rules.
Second, other rule induction algo-rithms exist that may play RIPPER'S role, suchas C4.5RULES (Quinlan, 1993).More generally, further research should fo-cus on the scaling properties of the approach(including the scaling of the external rule-induction algorithm), should investigate moreand larger language data sets, and should seekcomparisons with other existing methods thatclaim to handle complex features efficiently(Brill, 1993; Ratnaparkhi, 1997; Roth, 1998;Brants, 2000).77AcknowledgementsThe author thanks the members of the TilburgILK group and the Antwerp CNTS group forfruitful discussions.
This research has beenmade possible by a fellowship of the RoyalNetherlands Academy of Arts and Sciences(KNAW).Re ferencesD.W.
Aha, D. Kibler, and M. Albert.
1991.Instance-based learning algorithms.
MachineLearning, 6:37-66.C.L.
Blake and C.J.
Merz.
1998.
UCI repository ofmachine learning databases.Thorsten Brants.
2000.
TnT - a statistical part-of-speech tagger.
In Proceedings of the Sixth AppliedNatural Language Processing (ANLP-2000), Seat-tle, WA.E.
Brill.
1993.
A Corpus-Based Approach to Lan-guage Learning.
Dissertation, Department ofComputer and Information Science, University ofPennsylvania.P.
Clark and T. Niblett.
1989.
The CN2 rule induc-tion algorithm.
Machine Learning, 3:261-284.W.
W. Cohen.
1995.
Fast effective rule induction.In Proceedings of the Twelfth International Con-ference on Machine Learning, Lake Tahoe, Cali-fornia.M.J Collins and J. Brooks.
1995.
Prepositionalphrase attachment through a backed-off model.In Proc.
of Third Workshop on Very Large Cor-pora, Cambridge.W.
Daelemans and A.
Van den Bosch.
1992.
Gener-alisation performance ofbackpropagation learningon a syllabification task.
In M. F. J. Drossaers andA.
Nijholt, editors, Proc.
of TWLT3: Connection-ism and Natural Language Processing, pages 27-37, Enschede.
Twente University.W.
Daelemans, J. Zavrel, P. Berck, and S. Gillis.1996.
MBT: A memory-based part of speech tag-ger generator.
In E. Ejerhed and I.Dagan, editors,Proc.
of Fourth Workshop on Very Large Corpora,pages 14-27.
ACL SIGDAT.W.
Daelemans, P. Berck, and S. Gillis.
1997a.
Datamining as a method for linguistic analysis: Dutchdiminutives.
Folia Linguistica, XXXI(1-2).W.
Daelemans, A.
Van den Bosch, and A. Weijters.1997b.
IGTree: using trees for compression andclassification in lazy learning algorithms.
Artifi-cial Intelligence Review, 11:407-423.W.
Daelemans, A.
Van den Bosch, and J. Zavrel.1999.
Forgetting exceptions is harmful in lan-guage learning.
Machine Learning, 34(1-3):11-43.W.
Daelemans, J. Zavrel, K. Van der Sloot, andA.
Van den Bosch.
2000.
TiMBL: Tilburg Mem-ory Based Learner, version 3.0, reference manual.Technical Report ILK-0001, ILK, Tilburg Univer-sity.P.
Domingos.
1995.
The rise 2.0 system: A casestudy in multistrategy learning.
Technical Re-port 95-2, University of California at Irvine, De-partment of Information and Computer Science,Irvine, CA.P.
Domingos.
1996.
Unifying instance-based andrule-based induction.
Machine Learning, 24:141-168.J.R.
Quinlan.
1993. c4.5: Programs for MachineLearning.
Morgan Kaufmann, San Mateo, CA.L.A.
Ramshaw and M.P.
Marcus.
1995.
Text chunk-ing using transformation-based learning.
In Proc.of Third Workshop on Very Large Corpora, pages82-94, June.A.
Ratnaparkhi, J. Reynar, and S. Roukos.
1994.
Amaximum entropy model for prepositional phraseattachment.
In Workshop on Human LanguageTechnology, Plainsboro, N J, March.
ARPA.A.
Ratnaparkhi.
1997.
A linear observed time sta-tistical parser based on maximum entropy models.Technical Report cmp-lg/9706014, Computationand Language, http://xxx.lanl.gov/list/cmp-lg/,June.D.
Roth.
1998.
Learning to resolve natural anguageambiguities: A unified approach.
In Proceedingsof the National Conference on Artificial Intelli-gence, pages 898-904.A.
Van den Bosch.
1997.
Learning to pronouncewritten words: A study in inductive languagelearning.
Ph.D. thesis, Universiteit Maastricht.A.
Van den Bosch.
1999a.
Careful abstraction frominstance families in memory-based language learn-ing.
Journal for Experimental nd Theoretical Ar-tificial Intelligence, 11(3):339-368.A.
Van den Bosch.
1999b.
Instance-family ab-straction in memory-based language learning.
InI.
Bratko and S. Dzeroski, editors, MachineLearning: Proceedings of the Sixteenth Interna-tional Conference, pages 39-48, Bled, Slovenia.J.
Veenstra.
1998.
Fast NP chunking using memory-based learning techniques.
In Proceedings ofBENELEARN'98, Wageningen, The Netherlands.S.
Weiss and C. Kulikowski.
1991.
Computer sys-tems that learn.
San Mateo, CA: Morgan Kauf-mann.J.
Zavrel, W. Daelemans, and J. Veenstra.
1997.
Re-solving PP attachment ambiguities with memory-based learning.
In M. Ellison, editor, Proc.
of theWorkshop on Computational Language Learning(CoNLL'97), ACL, Madrid.78
