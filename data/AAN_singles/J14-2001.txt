Constrained Arc-Eager Dependency ParsingJoakim Nivre?Uppsala UniversityYoav Goldberg?
?Bar-Ilan UniversityRyan McDonald?GoogleArc-eager dependency parsers process sentences in a single left-to-right pass over the inputand have linear time complexity with greedy decoding or beam search.
We show how suchparsers can be constrained to respect two different types of conditions on the output dependencygraph: span constraints, which require certain spans to correspond to subtrees of the graph,and arc constraints, which require certain arcs to be present in the graph.
The constraints areincorporated into the arc-eager transition system as a set of preconditions for each transition andpreserve the linear time complexity of the parser.1.
IntroductionData-driven dependency parsers in general achieve high parsing accuracy without re-lying on hard constraints to rule out (or prescribe) certain syntactic structures (Yamadaand Matsumoto 2003; Nivre, Hall, and Nilsson 2004; McDonald, Crammer, and Pereira2005; Zhang and Clark 2008; Koo and Collins 2010).
Nevertheless, there are situationswhere additional information sources, not available at the time of training the parser,may be used to derive hard constraints at parsing time.
For example, Figure 1 showsthe parse of a greedy arc-eager dependency parser trained on the Wall Street Journalsection of the Penn Treebank before (left) and after (right) being constrained to build asingle subtree over the span corresponding to the named entity ?Cat on a Hot Tin Roof,?which does not occur in the training set but can easily be found in on-line databases.
Inthis case, adding the span constraint fixes both prepositional phrase attachment errors.Similar constraints can also be derived from dates, times, or other measurements thatcan often be identified with high precision using regular expressions (Karttunen et al.1996), but are under-represented in treebanks.?
Uppsala University, Department of Linguistics and Philology, Box 635, SE-75126, Uppsala, Sweden.E-mail: joakim.nivre@lingfil.uu.se.??
Bar-Ilan University, Department of Computer Science, Ramat-Gan, 5290002, Israel.E-mail: yoav.goldberg@gmail.com.?
Google, 76 Buckingham Palace Road, London SW1W9TQ, United Kingdom.E-mail: ryanmcd@google.com.Submission received: 26 June 2013; accepted for publication: 10 October 2013.doi:10.1162/COLI a 00184?
2014 Association for Computational LinguisticsComputational Linguistics Volume 40, Number 2Figure 1Span constraint derived from a title assisting parsing.
Left: unconstrained.
Right: constrained.In this article, we examine the problem of constraining transition-based dependencyparsers based on the arc-eager transition system (Nivre 2003, 2008), which perform asingle left-to-right pass over the input, eagerly adding dependency arcs at the earliestpossible opportunity, resulting in linear time parsing.
We consider two types of con-straints: span constraints, exemplified earlier, require the output graph to have a singlesubtree over one or more (non-overlapping) spans of the input; arc constraints insteadrequire specific arcs to be present in the output dependency graph.
The main contri-bution of the article is to show that both span and arc constraints can be implementedas efficiently computed preconditions on parser transitions, thus maintaining the linearruntime complexity of the parser.1Demonstrating accuracy improvements due to hard constraints is challenging, be-cause the phenomena we wish to integrate as hard constraints are by definition notavailable in the parser?s training and test data.
Moreover, adding hard constraints maybe desirable even if it does not improve parsing accuracy.
For example, many organi-zations have domain-specific gazetteers and want the parser output to be consistentwith these even if the output disagrees with gold treebank annotations, sometimesbecause of expectations of downstream modules in a pipeline.
In this article, we con-centrate on the theoretical side of constrained parsing, but we nevertheless providesome experimental evidence illustrating how hard constraints can improve parsingaccuracy.2.
Preliminaries and NotationDependency Graphs.
Given a set L of dependency labels, we define a dependency graphfor a sentence x = w1, .
.
.
, wn as a labeled directed graph G = (Vx, A), consisting of aset of nodes Vx = {1, .
.
.
, n}, where each node i corresponds to the linear position of aword wi in the sentence, and a set of labeled arcs A ?
Vx ?
L ?
Vx, where each arc (i, l, j)represents a dependency with head wi, dependent wj, and label l. We assume that thefinal word wn is always a dummy word ROOT and that the corresponding node n is adesignated root node.Given a dependency graph G for sentence x, we say that a subgraph G[i,j] =(V[i,j], A[i,j]) of G is a projective spanning tree over the interval [i, j] (1 ?
i ?
j ?
n) iff(i) G[i,j] contains all nodes corresponding to words between wi and wj inclusive, (ii) G[i,j]is a directed tree, and (iii) it holds for every arc (i, l, j) ?
G[i,j] that there is a directed path1 Although span and arc constraints can easily be added to other dependency parsing frameworks, thisoften affects parsing complexity.
For example, in graph-based parsing (McDonald, Crammer, and Pereira2005) arc constraints can be enforced within the O(n3) Eisner algorithm (Eisner 1996) by pruning outinconsistent chart cells, but span constraints require the parser to keep track of full subtree end points,which would necessitate the use of O(n4) algorithms (Eisner and Satta 1999).250Nivre, Goldberg, and McDonald Constrained Arc-Eager Dependency Parsingfrom i to every node k such that min(i, j) < k < max(i, j) (projectivity).
We now definetwo constraints on a dependency graph G for a sentence x:r G is a projective dependency tree (PDT) if and only if it is a projectivespanning tree over the interval [1, n] rooted at node n.r G is a projective dependency graph (PDG) if and only if it can be extendedto a projective dependency tree simply by adding arcs.It is clear from the definitions that every PDT is also a PDG, but not the other way around.Every PDG can be created by starting with a PDT and removing some arcs.Arc-Eager Transition-Based Parsing.
In the arc-eager transition system of Nivre (2003), aparser configuration is a triple c = (?|i, j|?, A) such that ?
and B are disjoint sublists ofthe nodes Vx of some sentence x, and A is a set of dependency arcs over Vx (and somelabel set L).
Following Ballesteros and Nivre (2013), we take the initial configurationfor a sentence x = w1, .
.
.
, wn to be cs(x) = ([ ], [1, .
.
.
, n], { }), where n is the designatedroot node, and we take a terminal configuration to be any configuration of the formc = ([ ], [n], A) (for any arc set A).
We will refer to the list ?
as the stack and the list Bas the buffer, and we will use the variables ?
and ?
for arbitrary sublists of ?
and B,respectively.
For reasons of perspicuity, we will write ?
with its head (top) to the rightand B with its head to the left.
Thus, c = (?|i, j|?, A) is a configuration with the node i ontop of the stack ?
and the node j as the first node in the buffer B.There are four types of transitions for going from one configuration to the next,defined formally in Figure 2 (disregarding for now the Added Preconditions column):r LEFT-ARCl adds the arc (j, l, i) to A, where i is the node on top of the stackand j is the first node in the buffer, and pops the stack.
It has as aprecondition that the token i does not already have a head.r RIGHT-ARCl adds the arc (i, l, j) to A, where i is the node on top of the stackand j is the first node in the buffer, and pushes j onto the stack.
It has as aprecondition that j 6= n.r REDUCE pops the stack and requires that the top token has a head.r SHIFT removes the first node in the buffer and pushes it onto the stack,with the precondition that j 6= n.A transition sequence for a sentence x is a sequence C0,m = (c0, c1, .
.
.
, cm) of configu-rations, such that c0 is the initial configuration cs(x), cm is a terminal configuration, andthere is a legal transition t such that ci = t(ci?1) for every i, 1 ?
i ?
m. The dependencygraph derived by C0,m is Gcm = (Vx, Acm ), where Acm is the set of arcs in cm.Complexity and Correctness.
For a sentence of length n, the number of transitions inthe arc-eager system is bounded by 2n (Nivre 2008).
This means that a parser usinggreedy inference (or constant width beam search) will run in O(n) time provided thattransitions plus required precondition checks can be performed in O(1) time.
This holdsfor the arc-eager system and, as we will demonstrate, its constrained variants as well.The arc-eager transition system as presented here is sound and complete for the setof PDTs (Nivre 2008).
For a specific sentence x = w1, .
.
.
, wn, this means that any transi-tion sequence for x produces a PDT (soundness), and that any PDT for x is generated by251Computational Linguistics Volume 40, Number 2Transition Added PreconditionsLEFT-ARCl (?|i, j|?, A) ?
(?, j|?, A?
{(j, l, i)}) ARC CONSTRAINTS?
?a ?
AC : da = i ?
[ha ?
?
?
la 6= l]?
?a ?
A : da = i ?
?a ?
AC : ha = i ?
da ?
j|?SPAN CONSTRAINTS?
[IN-SPAN(i) ?
s(i) = s(j) ?
i = r(s(i))]?
[IN-SPAN(i) ?
s(i) 6= s(j) ?
i 6= r(s(i))]?[NONE?
IN-SPAN(j) ?
s(i) 6= s(j)]?
[ROOT ?
IN-SPAN(j) ?
s(i) 6= s(j) ?
j 6= r(s(j))]RIGHT-ARCl (?|i, j|?, A) ?
(?|i|j,?, A?
{(i, l, j)}) ARC CONSTRAINTS?
?a ?
AC : da = j ?
[ha ?
?
?
la 6= l]j 6= n ?
?a ?
AC : ha = j ?
da ?
i|?SPAN CONSTRAINTS?
[ENDS-SPAN(j) ?
#CC > 1]?
[IN-SPAN(j) ?
s(i) = s(j) ?
j = r(s(j))]?
[IN-SPAN(j) ?
s(i) 6= s(j) ?
j 6= r(s(j))]?[NONE?
IN-SPAN(i) ?
s(i) 6= s(j)]?
[ROOT ?
IN-SPAN(i) ?
s(i) 6= s(j) ?
i 6= r(s(i))]REDUCE (?|i, j|?, A) ?
(?, j|?, A) ARC CONSTRAINTS?
?a ?
AC : ha = i ?
da ?
j|?
?a ?
A : da = i SPAN CONSTRAINTS?
[IN-SPAN(i) ?
s(i) = s(j) ?
i = r(s(i))]SHIFT (?, i|?, A) ?
(?|i,?, A) ARC CONSTRAINTS?
?a ?
AC : da = j ?
ha ?
i|?i 6= n ?
?a ?
AC : ha = j ?
da ?
i|?SPAN CONSTRAINTS?
[ENDS-SPAN(j) ?
#CC > 0]Figure 2Transitions for the arc-eager transition system with preconditions for different constraints.
Thesymbols ha, la, and da are used to denote the head node, label, and dependent node, respectively,of an arc a (that is, a = (ha, la, da )); IN-SPAN(i) is true if i is contained in a span in SC; END-SPAN(i)is true if i is the last word in a span in SC; s(i) denotes the span containing i (with a dummy spanfor all words that are not contained in any span); r(s) denotes the designated root of span s(if any); #CC records the number of connected components in the current span up to andincluding the last word that was pushed onto the stack; NONE and ROOT are true if we allow nooutgoing arcs from spans and if we allow outgoing arcs only from the span root, respectively.some transition sequence (completeness).2 In constrained parsing, we want to restrictthe system so that, when applied to a sentence x, it is sound and complete for the subsetof PDTs that satisfy all constraints.3.
Parsing with Arc ConstraintsArc Constraints.
Given a sentence x = w1, .
.
.
, wn and a label set L, an arc constraintset is a set AC of dependency arcs (i, l, j) (1 ?
i, j ?
n, i 6= j 6= n, l ?
L), where each arcis required to be included in the parser output.
Because the arc-eager system can onlyderive PDTs, the arc constraint set has to be such that the constraint graph GC = (Vx, AC)can be extended to a PDT, which is equivalent to requiring that GC is a PDG.
Thus, thetask of arc-constrained parsing can be defined as the task of deriving a PDT G such2 Although the transition system in Nivre (2008) is complete but not sound, it is trivial to show that thesystem as presented here (with the root node at the end of the buffer) is both sound and complete.252Nivre, Goldberg, and McDonald Constrained Arc-Eager Dependency Parsingthat GC ?
G. An arc-constrained transition system is sound if it only derives properextensions of the constraint graph and complete if it derives all such extensions.Added Preconditions.
We know that the unconstrained arc-eager system can derive anyPDT for the input sentence x, which means that any arc in Vx ?
L ?
Vx is reachablefrom the initial configuration, including any arc in the arc constraint set AC.
Hence, inorder to make the parser respect the arc constraints, we only need to add preconditionsthat block transitions that would make an arc in AC unreachable.3 We achieve thisthrough the following preconditions, defined formally in Figure 2 under the headingARC CONSTRAINTS for each transition:r LEFT-ARCl in a configuration (?|i, j|?, A) adds the arc (j, l, i) and makesunreachable any arc that involves i and a node in the buffer (other than(j, l, i)).
Hence, we permit LEFT-ARCl only if no such arc is in AC.r RIGHT-ARCl in a configuration (?|i, j|?, A) adds the arc (i, l, j) and makesunreachable any arc that involves j and a node on the stack (other than(i, l, j)).
Hence, we permit RIGHT-ARCl only if no such arc is in AC.r REDUCE in a configuration (?|i, j|?, A) pops i from the stack and makesunreachable any arc that involves i and a node in the buffer.
Hence, wepermit REDUCE only if no such arc is in AC.r SHIFT in a configuration (?, i|?, A) moves i to the stack and makesunreachable any arc that involves j and a node on the stack.
Hence,we permit SHIFTl only if no such arc is in AC.Complexity and Correctness.
Because the transitions remain the same, the arc-constrainedparser will terminate after at most 2n transitions, just like the unconstrained system.However, in order to guarantee termination, we must also show that at least onetransition is applicable in every non-terminal configuration.
This is trivial in the un-constrained system, where the SHIFT transition can apply to any configuration thathas a non-empty buffer.
In the arc-constrained system, SHIFT will be blocked if thereis an arc a ?
AC involving the node i to be shifted and some node on the stack, andwe need to show that one of the three remaining transitions is then permissible.
If ainvolves i and the node on top of the stack, then either LEFT-ARCl and RIGHT-ARClis permissible (in fact, required).
Otherwise, either LEFT-ARCl or REDUCE must bepermissible, because their preconditions are implied by the fact that AC is a PDG.In order to obtain linear parsing complexity, we must also be able to check all pre-conditions in constant time.
This can be achieved by preprocessing the sentence x andarc constraint set AC and recording for each node i ?
Vx its constrained head (if any),its leftmost constrained dependent (if any), and its rightmost constrained dependent (ifany), so that we can evaluate the preconditions in each configuration without havingto scan the stack and buffer linearly.
Because there are at most O(n) arcs in the arcconstraint set, the preprocessing will not take more than O(n) time but guarantees thatall permissibility checks can be performed in O(1) time.Finally, we note that the arc-constrained system is sound and complete in the sensethat it derives all and only PDTs compatible with a given arc constraint set AC for a sen-tence x.
Soundness follows from the fact that, for every arc (i, l, j) ?
AC, the preconditions3 For further discussion of reachability in the arc-eager system, see Goldberg and Nivre (2012, 2013).253Computational Linguistics Volume 40, Number 2force the system to reach a configuration of the form (?|min(i, j),max(i, j)|?, A) in whicheither LEFT-ARCl (i > j) or RIGHT-ARCl (i < j) will be the only permissible transition.Completeness follows from the observation that every PDT G compatible with AC is alsoa PDG and can therefore be viewed as a larger constraint set for which every transitionsequence (given soundness) derives G exactly.Empirical Case Study: Imperatives.
Consider the problem of parsing commands topersonal assistants such as Siri or Google Now.
In this setting, the distribution ofutterances is highly skewed towards imperatives making them easy to identify.Unfortunately, parsers trained on treebanks like the Penn Treebank (PTB) typicallydo a poor job of parsing such utterances (Hara et al.
2011).
However, we know thatif the first word of a command is a verb, it is likely the root of the sentence.
If wetake an arc-eager beam search parser (Zhang and Nivre 2011) trained on the PTB, itgets 82.14 labeled attachment score on a set of commands.4 However, if we constrainthe same parser so that the first word of the sentence must be the root, accuracyjumps dramatically to 85.56.
This is independent of simply knowing that the firstword of the sentence is a verb, as both parsers in this experiment had access to goldpart-of-speech tags.4.
Parsing with Span ConstraintsSpan Constraints.
Given a sentence x = w1, .
.
.
, wn, we take a span constraint set to bea set SC of non-overlapping spans [i, j] (1 ?
i < j ?
n).
The task of span-constrainedparsing can then be defined as the task of deriving a PDT G such that, for every span[i, j] ?
SC, G[i,j] is a (projective) spanning tree over the interval [i, j].
A span-constrainedtransition system is sound if it only derives dependency graphs compatible with thespan constraint set and complete if it derives all such graphs.
In addition, we may addthe requirement that no word inside a span may have dependents outside the span(NONE), or that only the root of the span may have such dependents (ROOT).Added Preconditions.
Unlike the case of arc constraints, parsing with span constraintscannot be reduced to simply enforcing (and blocking) specific dependency arcs.
Inthis sense, span constraints are more global than arc constraints as they require en-tire subgraphs of the dependency graph to have a certain property.
Nevertheless,we can use the same basic technique as before and enforce span constraints byadding new preconditions to transitions, but these preconditions need to refer to vari-ables that are updated dynamically during parsing.
We need to keep track of twothings:r Which word is the designated root of a span?
A word becomes thedesignated root r(s) of its span s if it acquires a head outside the span orif it acquires a dependent outside the span under the ROOT condition.r How many connected components are in the subgraph over the currentspan up to and including the last word pushed onto the stack?
A variable#CC is set to 1 when the first span word enters the stack, incremented by1 for every SHIFT and decremented by 1 for every LEFT-ARCl.4 Data and splits from the Web Treebank of Petrov and McDonald (2012).
Commands used for evaluationwere sentences from the test set that had a sentence initial verb root.254Nivre, Goldberg, and McDonald Constrained Arc-Eager Dependency ParsingGiven this information, we need to add preconditions that guarantee the following:r The designated root must not acquire a head inside the span.r No word except the designated root may acquire a head outside the span.r The designated root must not be popped from the stack before the lastword of the span has been pushed onto the stack.r The last word of a span must not be pushed onto the stack in aRIGHT-ARCl transition if #CC > 1.r The last word of a span must not be pushed onto the stack in a SHIFTtransition if #CC > 0.In addition, we must block outside dependents of all words in a span under the NONEcondition, and of all words in a span other than the designated root under the ROOTcondition.
All the necessary preconditions are given in Figure 2 under the headingSPAN CONSTRAINTS.Complexity and Correctness.
To show that the span-constrained parser always terminatesafter at most 2n transitions, it is again sufficient to show that there is at least onepermissible transition for every non-terminal configuration.
Here, SHIFT is blocked ifthe word i to be shifted is the last word of a span and #CC > 0.
But in this case, one of theother three transitions must be permissible.
If #CC = 1, then RIGHT-ARCl is permissible;if #CC > 1 and the word on top of the stack does not have a head, then LEFT-ARCl ispermissible; and if #CC > 1 and the word on top of the stack already has a head, thenREDUCE is permissible (as #CC > 1 rules out the possibility that the word on top of thestack has its head outside the span).
In order to obtain linear parsing complexity, allpreconditions should be verifiable in constant time.
This can be achieved during initialsentence construction by recording the span s(i) for every word i (with a dummy spanfor words that are not inside a span) and by updating r(s) (for every span s) and #CC asdescribed herein.Finally, we note that the span-constrained system is sound and complete in thesense that it derives all and only PDTs compatible with a given span constraint set SC fora sentence x.
Soundness follows from the observation that failure to have a connectedsubgraph G[i,j] for some span [i, j] ?
SC can only arise from pushing j onto the stack ina SHIFT with #CC > 0 or a RIGHT-ARCl with #CC > 1, which is explicitly ruled out bythe added preconditions.
Completeness can be established by showing that a transitionsequence that derives a PDT G compatible with SC in the unconstrained system cannotviolate any of the added preconditions, which is straightforward but tedious.Empirical Case Study: Korean Parsing.
In Korean, white-space-separated tokens corre-spond to phrasal units (similar to Japanese bunsetsus) and not to basic syntactic cat-egories like nouns, adjectives, or verbs.
For this reason, a further segmentation step isneeded in order to transform the space-delimited tokens to units that are a suitable inputfor a parser and that will appear as the leaves of a syntactic tree.
Here, the white-spaceboundaries are good candidates for posing hard constraints on the allowed sentencestructure, as only a single dependency link is allowed between different phrasal units,and all the other links are phrase-internal.
An illustration of the process is given inFigure 3.
Experiments on the Korean Treebank from McDonald et al.
(2013) show thatadding span constraints based on white space indeed improves parsing accuracy foran arc-eager beam search parser (Zhang and Nivre 2011).
Unlabeled attachment score255Computational Linguistics Volume 40, Number 2Figure 3Parsing a Korean sentence (the man writes the policy decisions) using span constraints derived fromoriginal white space cues indicating phrasal chunks.increases from an already high 94.10 without constraints to 94.92, and labeled attach-ment score increases from 89.91 to 90.75.Combining Constraints.
What happens if we want to add arc constraints on top ofthe span constraints?
In principle, we can simply take the conjunction of the addedpreconditions from the arc constraint case and the span constraint case, but somecare is required to enforce correctness.
First of all, we have to check that the arcconstraints are consistent with the span constraints and do not require, for example,that there are two words with outside heads inside the the same span.
In addition, weneed to update the variables r(s) already in the preprocessing phase in case the arcconstraints by themselves fix the designated root because they require a word insidethe span to have an outside head or (under the ROOT condition) to have an outsidedependent.5.
ConclusionWe have shown how the arc-eager transition system for dependency parsing canbe modified to take into account both arc constraints and span constraints, withoutaffecting the linear runtime and while preserving natural notions of soundness andcompleteness.
Besides the practical applications discussed in the introduction and casestudies, constraints can also be used as partial oracles for parser training.ReferencesBallesteros, Miguel and Joakim Nivre.2013.
Getting to the roots of dependencyparsing.
Computational Linguistics,39:5?13.Eisner, Jason and Giorgio Satta.
1999.Efficient parsing for bilexical context-freegrammars and head automaton grammars.In Proceedings of the 37th Annual Meeting ofthe Association for Computational Linguistics,pages 457?464, Santa Cruz, CA.Eisner, Jason M. 1996.
Three newprobabilistic models for dependencyparsing: An exploration.
In Proceedingsof the 16th International Conference onComputational Linguistics (COLING),pages 340?345, Copenhagen.Goldberg, Yoav and Joakim Nivre.2012.
A dynamic oracle for arc-eagerdependency parsing.
In Proceedingsof the 24th International Conference onComputational Linguistics, pages 959?976,Shanghai.Goldberg, Yoav and Joakim Nivre.
2013.Training deterministic parsers withnon-deterministic oracles.
Transactionsof the Association for ComputationalLinguistics, 1:403?414.Hara, Tadayoshi, Takuya Matsuzaki, YusukeMiyao, and Jun?ichi Tsujii.
2011.
Exploringdifficulties in parsing imperatives andquestions.
In Proceedings of the 5thInternational Joint Conference on NaturalLanguage Processing, pages 749?757,Chiang Mai.Karttunen, Lauri, Jean-Pierre Chanod,Gregory Grefenstette, and Anne Schiller.1996.
Regular expressions for languageengineering.
Natural Language Engineering,2(4):305?328.256Nivre, Goldberg, and McDonald Constrained Arc-Eager Dependency ParsingKoo, Terry and Michael Collins.
2010.Efficient third-order dependency parsers.In Proceedings of the 48th Annual Meeting ofthe Association for Computational Linguistics,pages 1?11, Uppsala.McDonald, Ryan, Koby Crammer, andFernando Pereira.
2005.
Onlinelarge-margin training of dependencyparsers.
In Proceedings of the 43rd AnnualMeeting of the Association for ComputationalLinguistics, pages 91?98, Ann Arbor, MI.McDonald, Ryan, Joakim Nivre, YvonneQuirmbach-Brundage, Yoav Goldberg,Dipanjan Das, Kuzman Ganchev, KeithHall, Slav Petrov, Hao Zhang, OscarTa?ckstro?m, Claudia Bedini, Nu?riaBertomeu Castello?, and Jungmee Lee.2013.
Universal dependency annotationfor multilingual parsing.
In Proceedings ofthe 51st Annual Meeting of the Associationfor Computational Linguistics (Volume 2:Short Papers), pages 92?97, Sofia.Nivre, Joakim.
2003.
An efficient algorithmfor projective dependency parsing.
InProceedings of the 8th International Workshopon Parsing Technologies, pages 149?160,Nancy.Nivre, Joakim.
2008.
Algorithms fordeterministic incremental dependencyparsing.
Computational Linguistics,34:513?553.Nivre, Joakim, Johan Hall, and Jens Nilsson.2004.
Memory-based dependency parsing.In Proceedings of the 8th Conference onComputational Natural Language Learning,pages 49?56, Boston, MA.Petrov, Slav and Ryan McDonald.
2012.Overview of the 2012 shared task onparsing the web.
In Notes of the FirstWorkshop on Syntactic Analysis ofNon-Canonical Language (SANCL),Montreal.Yamada, Hiroyasu and Yuji Matsumoto.2003.
Statistical dependency analysiswith support vector machines.In Proceedings of the 8th InternationalWorkshop on Parsing Technologies,pages 195?206, Nancy.Zhang, Yue and Stephen Clark.
2008.A tale of two parsers: Investigatingand combining graph-based andtransition-based dependency parsing.In Proceedings of the Conference onEmpirical Methods in Natural LanguageProcessing (EMNLP), pages 562?571,Honolulu, HI.Zhang, Yue and Joakim Nivre.
2011.Transition-based parsing with richnon-local features.
In Proceedings of the49th Annual Meeting of the Association forComputational Linguistics, pages 188?193,Portland, OR.257
