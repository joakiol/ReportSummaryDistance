A Best-Match Algorithm for Broad-CoverageExample-Based DisambiguationNaohiko URAM()TOIBM Research, Tokyo Research Laboratory1623-1.4 S imotsuru ln~L Ya lnato -sh i ,  Kanagawa-ken  242 Japanur tmloto( (~tr l .vnc l ; .
ibm.comAbst ractTo improve tit(.'
coverage of examl)le-bases , twonlethods are introduced into the 1)est-match algo-rithm.
The first is for acquiring conjunctive rela-tionships fl'om corpora, as measures of word simi-larity that  can be used in addition to thesauruses.The Second, used when a word does not appear inan examltled)asc or a thesaurus, is for inferring linksto words in the examph>base by ('mnparing the us-age of the word in the text ~md that of words in theexample- base.1 I n t roduct ionImprovement of cow, rage in practical domains is oneof the most important issues in the area of example-based systems.
The examl)le-based apI)roach \[6\] hasbecome a (:amman technique for m~turM languageprocessing apI)lications uch as machine translation*rod disambiguatkm (e.g.
\[5, 10\]).
However, fewexisting systems can cover a practical domain orhandle a l)road range of phenomena.The most serious obstacle to robust example-based systems is the coverage of examt)le-bases.
It isan oi)en question how many e~xaml)les are requiredfor disambiguating sentences in a specific domain.The Sentence AnMyzer (SENA) wax developedin order to resolve attachment, word-sense, andconjunctive anlbiguitics t)y using constraints andexample-based preferences \[11\].
It lists at)out57,000 disambiguated head-modifier relationshipsand al)out 300,000 synonyms and is-a 1)inary~relationships.
Even so, lack of examl)les (no rele-vant examlfles ) accounted for 46.1% of failures in aexperiment with SENA \[12\].Previously, it was believed to be easier to collectexamples than to develop rules for resolving ambi-guities.
However, the coverage of each examltie isnmch nlore local than a rule, and therefore a hugemunber of examt)les is required in order to resolverealistic 1)rot)lems.
There has been some carl)uS-based research (m how to acquire large-scah~ knowl-edge automati(-ally in order to cover the domain tobe disambiguatcd, lint there are still major 1)rot)-l cn ls  to  \])e overeon le .First, smmmtic kvowledge such as word-sensecannot be extracted by automatic cort)u~-base(lknowledge, acquisition.
The example-base in SENAis deveh)l)ed by using a bootstr~q)ping method.However, the results of word-sense disambiguationnmst be (:he(:ked by a hutnan, a,nd word-senses aretagged to only about ;t half of all the examt)les , incethe task is very time-consmning.A second ditliculty in the exalnple-t)ased att-proach ix the algorithm itself, namely, the be.st-match algorithm, which was used in earlier systemsbuilt around a thesaurus that  consisted of a hierttr-chy of is-a or synonym relationships between words(word-senses).This paper proposes two methods for ilnprov-ing the coverage of exantple-bases.
The selecteddomain is th~tt of sentences in comt)uter manmds.First, knowledge thtd; represents a type of similar-ity other than synonym or is-a relationships i a(>quired.
As one measurement of the similarity, inter-changeability between words (:~m be used.
In thispaper, two types of the relationship reflect such in-terchangeability.
First, the elements of coordinatedstructures are good clues to the interchangeat)ilityof words.
Words can be extracted easily from adolnain-specitic carl)us , and therefore the example-base can I)e adapted to the sl)ecific domain by usingthe domain-specific relationships.If there are no examples and relations in the the-saurus, the example-base gives no information fordisambiguation.
However, the text to be disam-1)iguate.d provides useful knowledge for this pur-pose \[7, 3\].
'\['he relationshit)s between words in theexample-base and ;ut unknown word can be guessedby comi)aring that word's usage in extracted cxant-ples and in the text.2 A Best -Match  Algor i thmIn this section, conventional algorithms forexami)le-b~tsed disalnl)iguation~ art(1 their associat-e(i prol)lems, a.re briefly introduced.
The algorithmsof lnost examph>l)ased systems consist of the fol-lowing three steps~:till some systenls, the exact-mah:h ttl|(I Lhe best-match~tr(!
ll/orge({.717"store+V" *storel "in" "disk" *disk 1)"store+V" *store1 "in" "storage-device" *device 2)"store+V" *storel "in" "cell" *cell 1)"store+V" *store1 "in" "computer" *computer1 4)"store+V" *storel "in" "storage" *storage2 3)"store+V" *storel "in" "format" *formatl 1)"store+V" *storel "in" "data-network" *network3 t)Fig.
1: Examples for R1("progrmn+N" *progl "in" "profile+N" *profile 5)("program+N" *progl "in" "data-storage+N" *stor-age3 1)("program+N" *progl "in" "publieation+N" *publica-tion1 2)("program+N" *progl "in" "form+N" *form1 2)("program+N" *prog2 "in" "group+N" *group1 1)Fig.
2: Examples for R21.
Searching for examples2.
Exact matching3.
Best matching with a thesaurusSuppose the prepositional phase attachment ambi-guity in $1 is resolved by using these steps.
(S1) A managed AS/400 system can s tore  anew program in the repos i to ry .There are two candidates for the attachnmnt ofthe prepositional phrase "in the repository."
Theyare represented by the following head-modifier rela-tionships:(R1) ("store+V" (PP "in") "repository-FN")(R2) ("program+N" (PP "in") "repository+N")In R1 the m)un "repository" modifies the verb"store" with "in," while in R2, it modifies the noun"program.
"First,, SENA searches for examples whose headsmatch the candidate.
Figures 1 and 2 show therelevant examples for R1 and I/.2.
They representthe head-modifier elationships, including word-senses, a relation label between the word-senses,(e.g.
'in"), and a frequency.If a relationship identical to either of the can-didates R1 and R2 is found, a high similarity isattached to the candidate and the example (exactmatching).Word-sense ambiguities are resolved by using thesame framework \[12\].
In this case, each candi-date represent each word sense.
For example, theword-sense *store1 is preferred among the examplesshown in Fig.
I.If no examples are obtained by the exact-matching process, the system executes the best-matching process, which is the most importantmechanism in the example-based approach.
For thecomparison, synonym or is-a relationships describedin a thesaurus are used.
For example, if synonymrelations are h)und between "repository" and "disk"in the first example for the R1, a similarity whosevalue is smaller than that for exact matching is giv-en to the examples.
The most preferable candi-date is selected by comparing all examples in Fig.
1and computing the total similarity value for eachcandidate.
If multiple candidates have tile samesimilarity values, the frequency of the example andsome heuristics (for example, innermost attachmentis preferred) are used to weight the similarities.Experience with SENA reveals two problems thatprevent an improvement in the performance of thebest-matching algorithm.
First, the approach isstrongly dependent on the thesaurus.
Many sys-tems calculate the similarity or preference mainlyor entirely by using the hierarchy of the thesaurus.However, these relationships indicate only a cer-tain kind of similarity between words.
To improvethe coverage of the example-base, other additionaltypes of knowledge are required, as will be discussedin the following sections.Another problem is the existence of unknownwords; that is, words that are described in the sys-tem dictionary but do not appear in the example-base or the thesaurus.
In SENA, the New CollinsThesaurus \[1\] is used to disambiguate sentences incomputer manuals.
Many unknown words appear,especially nouns, since the thesaurus is for the gen-eral domain.
Therefore, a inechanism for handlingthe unknown words is required.
This is covered inChapter 4.3 Knowledge Acquis i t ion  forRobust Best-MatchingAs described in the previous section, the best-matching algorithm is a basic element of example-based disambiguation, but is strongly dependent onthe thesaurus.
Nirenburg \[8\] discusses the type ofknowledge needed for the matching; in his method,morphological information and antonyms are usedin addition to synonym and is-a relationships.
Thissection discusses the acquisition of knowledge frontother aspects for a broad-coverage b st-match algo-rithm.3.1 Acquisition of Conjunctive Rela-tionships from CorporaThe New Collins Thesaurus, which is used in SENAas a source of synonym or is-a relationships, givesthe following synonyms of "store":store:accumulate, deposit, garner, hoard, keep, etc.In our example-base, there are few examples forany of the words except "keep," since the example-base was developed nminly to resolve sentences intechnical documents uch as computer manuals.When the domain is changed, the vocabulary and718the usage of words also (:hange.
Even a general-dommn thesaurus ome, tinms does not suit a. spe-(:ific domain.
Moreover, develolmmnk of a domain-spccitie thesaurus is it time-consuming task.The use of synonym or is-a relationships suggeststhe hypothesis that from the viewpoint of theexalni)le-l)~tsed itI)pl'oadl ~ a, word in iL sentell(;e citnbe replaced by its synonyms or t~xonyms.
Thatis, it supports the existe, nce of the (virtual) exam-pie $1' when "store" and "keep" h~tve a synonynlrelationshil).
(SI') A managed AS/400 systenl can keep a newprogram in tile repository.l}~terchangeability is :m important condition forcM('ulating similarity or preferences t)etween words.Our claim is that if words are inter(:hangeat)h~ insenten(:es, they should have strong similarity.In this l)al)er, (:onjmtetive relationships, whMtare COllllDon ill te(:hnictd (lOClllDetlts~ 3,re l)roposedas relationships that  satisfy the conditiml of interehlmgeability.
Seutenee, s in which the word "store"ix used as an element of coordinated structure canbe extracted from computer manuMs, as followingexamples how:(1) The service retrieves, fornlats, all(/ stores a messagefor the user,(2) Delete the identifier being stored or rood|tied frointhe tM)le.
(3) This EXEC verifies mM StOlIt!S the language defaultsin your tile.
(4) You use the fltnetion to add, store, retrieve, ~tll(lupdate inforlna, tion Mmut doculnents.From tile sentences, the R)tlowing words that areinter(:hangeable with "store" are acquired:store,: retrieve, fo'r'm, at, modiJy, "oeTiiflj, add, "ltpda, teOften the words share easeq)atterns, which is ;tuseNl characteristic fi)r determining interchanl,/e--ability.
Another reason we use (:onjunctive re-lationships is that they can 1)e extracted scmi-automatieMly from untagged or tagged corpora 1)yusing a simph', patkeri>matehing mtho(l. We ex-tract, ed about 700 conjunctive relationships fromnntagged computer mamlMs by i)attern matching.The relationships include various types of knowl-edge, such as 10t ) antonyms (e.g.
"private" itnd"publiC'), (t>)sequences of ~ctions (e.g.
"toad"itnd "edit"), (c) (weak) synonyms (e.g.
"program"and "service"), and ((l) part-of relationships (e.g.
"tape" ~tn(l "device").
Another merit of conjunctiverelationships i  that  they reflect dommn-specili(: re-lations.3.2 Acquisition from Text to Be Dis-ambiguatedIf there are no exami)les of i~ word to I)e dismn.-biguated, and the word does not appear in the the-saurus, no relationships ~Lre acquired.The existence of words theft m'e mlknown to kheexaml)le-base antl the thesaurus ix inevitat)le wtmnone is deMing with tile disambiguation <>f senten<:esin f>ri~(:ti(:al dmmdns.
Computer manuals, for e?-~nni)le , coIiLain lnally special llOUns such as llantesof colDlllands and products, but, there are no the-sauruses for such highly domMn-speeilic words.One w~ty of resolving the prol)h'nt ix to use thetext to be processed as the most domainospecilicexample-base.
This idea ix supported by the factthat most word-It;O-word ependencies il,<:luding theUllklloWll words aq)pear lltalty kimes il~ the sAIuetext.
Nasukawa \[7\] deveh)pe(l the Dis(:ourse An-alyzer (DIANA), which resolves ambiguities in atext by dynamically referring to contextual infor-mation.
Kinoshita et ;-I.1.
\[3\] Mso prolmsed *t nletho<lfor machine I;ra.nslatiml by lm.rsing ;t eoml)lete textin advance aud using it as an ex~mlple-1)ase, tlowev-er, neither system works for llllkllown wt)rds~ sinceboth use only dependencies that al)l)eltr explicitlyin the texl.4 An  A lgor i thm to Search  tbrUnknown WordsWe first give ~ut enilaneed best-matci~ algorithm fordisamlfiguation.
'\['he steps given ill Chapter 2 axemoditied as follows:\[.
Searching for examph!s2.
\]~xlt(q, matching3.
Best matching with a thesmtrus and conjunc-tive relationshil)s4.
Unknowll-word-makx:hil~g using a. context-base'\]'he outline of the the algorithm is as follows: Sen-tences in the text; to he processed are parsed ill ad-VILl lC(!
1 aud 1;11(!
parse trees axe stored as a, context-base.
'\['tie com;ext-h~tse caAI inchlde alIll)igllOllSword-to-word dependencies, ince no disambigua-kion l)rot:ess is executed.
Using tm exanq)le-baseslid the contextd)ase, the sentences ill the text aredisantbiguated sequentially.
If an ambiguous worddoes not ~q~pear in an exanlple-base or in the the-saltrus, 3.11 IlIIklIOWII word search is executed (other-wise, the COltve(lliOllil,\[ best~lllaA;ch process is eX( !Ct l l ; -ed.)
The mlknow:u-word-matching )l'oeess includesthe following ske, ps:1.
'\['he dependencies that include the unknownword are extracted froIil the context-base.2.
A candidate set of words that is interchange-abh; with tile unknown word ix searched for inkite (!xamph>base by using the context depen-dency.3.
The e~mdidate set ~(:quired ill step 2 is com-p~tred with the examples extracted for eachcandidate of interpretation.
A preference wd-ue is ea.leulated by using the sets, and the mostpreferred interpretation is selected.719Let us see how the algorithm resolves the attach-ment ambiguity in sentence S1 from Chapter 2,which is taken from a text (manual) for the AS/400system.
(Sl) A managed AS/400 system can store anew program in the repository.The text that contains S1 is parsed in advance,and stored in the context-base.
The results of theexample search arc shown in Fig.
1.
There are twocandidate relationships for the attachment of theprepositional phrase "in the repository".
(R1) ("store+V" (PP "in") "repository+N")(R2) ( "program+N" (PP "in") "repository+N")Tile noun "repository" does not appear in theexample-base or thesaurus, and therefore no infor-mation for the attachment is acquired.Consequently, the word-to-word dependenciesthat contain "repository" are searched for in thecontext-base.
The following sentences appear be-fore or after S1 in the text:(CBI) The repository can hold objects that areready to be sent or that have been receivedfrom another user library.
(CB2) A distribution catalog entry exists ~oreach object in the distribution repository.
(CB3) A data object can be loaded into thedistribution repository from an AS/400 library.
(CB4) The object type of the object specifiedmust match the information in the distributionrepository.From the sentences, the head-nn)difier relation-ships that contain the unknown word "repository"are listed.
These relationships are called the contextdependency for the word.
The context dependencyof "repository" is us follows:(D1) ("hold+V" (sub j) "repository+N"): 1(D2) ("exist+V" (PP "in") "repository+N') : 0.5(D3) ("object+N" (PP "in") "repository+N'): 0,5(D4) ("load+V" (PP "into") "relmsitory+N"): 1(D5) ("information+N" (PP "in") "repository+N') :0.5(D6) ("match+V" (PP "into") "repository+N"): 0.5The last number in each relation is the certaintyfactor (CF) of the relationship.
The value is 1/(thenumber of candidates for the resolving ambiguity).For example, the attachment of "repository" in CB2has two candidates, D2 and D3.
Therefore, the cer-tainty factors for D2 and D3 are 1/2.For each dependency, candidate words (CB) inthe context-base are searched for in the example-base.
The words in the set can be considered assubstitutable synonyms of the unknown word.
Forexample, the WORDs that satisfy the relationship("hold+V" (subj) WORD+N) in  the case of D1 aresearched for.
The Mlowing are candidate words inthe context-base for the word"repository.
"CB1 = {I, user, cradle, rock} (for D1)CB2 = {storage, transient data} (for D2)CB3 = {condition, format, path, 1916, technique,control area} (for DO)CB4 = {systema8, facility} (for D4)CB5 ={reeord} (for DS)CB6 = {} (for D6)The total set of candidate words (CB) of the"repository" is an union of CB1 through CB6.
Theset is compared with the extracted examples foreach attachntent candidate (Fig.
1).
The words inthe examples are candidate words in the example-base.
By intersecting the candidate words in thecontext-base and the example-base, word that areinterchangeable with the unknown word can be ex-tracted.
The intersections of ea(:h set are as follows:For 111, CBr3C1 -- {storage, format}For R2, CBNC2 = {}This result means that "storage" and "format"have the same usage (or are interchangeal)le) in thetext.
The preference value P(R) for the candidateR with the interchangeable word w is calculated bythe formula:P(R) = E~,(CF) ?
(frequency)In this (:use, P(R1) = 0.5 x 1+0.5  x 1 = 1.0,and P(R2) = 0 (sui)posing that the frequency ofthe words is 1).
As a result, R1 is preferred to R2.if both sets of candidates are empty, the num-bers of extracted examples are coml)ared (this iscalled Heuristic-I).
If there are no related words inthis ease, R1 is preferred to i"12 (see Fig.
1).
Thisheuristic indicates that "in" is preferred after "s-tore," irrespective of the head word of the preposi-tional phrase.5 Exper imenta l  Results5.1 Example -Base  and  ThesaurusAll example-base for disambigu~tion f sentences incomputer manuMs is now being developed.
Table 1shows its currem; size.
The sentences are extractedfrom examples in the L(mgman Dictionary of Con-temporary English \[9\] and definitions in the IBMDictionary of Computing \[2\].
Synonym and is-a re-lati(mships arc extracted from the New Collins The-saurus \[1\] and Webster's Seventh New CollegiateDictionary \[4\].Our exainple-base is a set of head-modifier binarydependencies with relations between word, such as(subject), (object), and (PP "in").
It was developedby a bootstrapping method with human correction.In SENA, the example-base is used to resolve threetypes of ambiguity: attachment, wor(l-scnse~ andcoordination.
The h,vel of knowledge depends onthe type of ambiguity.720Table 1: Size of the Example-Base and 'rlmsaurusExample-BaseExamples 57,170 binary relationshit)s(in 9,500 sentences)Distinct words 8,602ThesaurusSynonylns 283,21 i binary relationshil)s(11,1)06 entries)Is-a relations 6,353 binary relationslfips52.4 (%) I Success with unknown word matctfingSuccess with Heuristic-1L'tilure20.o (%)27.6 (%)Fig.
3: Result of disambiguationTo resolve semantic ambiguities, the examl)lesshould be disambiguated semantically.
On the oth-er band, structural def)endencies can be extractedfrom raw or tagged corpora t)y using simple rules orpatterns, in our approach, multile, vel descriptionsof examples are allowed: one example may provideboth structural and word-sense information, whileanother may provide only structural dependem:ies.Word-senses are added to a half of the sentences inexample-base.5 .2  Exper imentWe did a small experiment on disambiguation ofprepositional I)hrase attachment.
First, we pre-pared 105 ambiguous test dater andomly from 3,000sentences in a (:olni)ute.r manual.
The format of thedata was as follows:verb noun prep unknown-nounNone of these data (:an be disambiguated by us-ing the conventional best-mateldng algorithm, s-ince noun2 does not appear in the example-base orthesaurus.
Conjunctive, relationslfips, described inChapter 3, are used with the exmnple-base and thethesaurus.The results of the disambiguation are shown inFig.
3.
We were able to disambiguate 52.4% of the,test data by using mlknown-word-matching.
By us-ing Heuristic-1 in addition, we obt~ine(l a 72.4%success rate for unknown words.ODe cause of failure is imbalai,ce among exam-pies.
The number of exanq)les for frequent verbsis larger than the number of exanq)les tk)r frequentnouns.
As a result, verb attactunent tends to bepreferred.
2 Another cause of failure is the mmfl)erof context dependen(:ies.
In tim experim(mt, at mostthe nearest eight sentences were used; the optinmmnumber is still an open question.2We did not use other heuristics uch as prefl?r(mce lopinner attachment.6 Conc lus ionMethods h)r improving the coverage of example-bases were 1)reposed in order to allow the realizationof broad-coverage examph>l)ased systems.
~vV(, areevMuating our approacl) with larger amounts of da-ta.
For future progress, the following issues mustbe discussed:I.
In this paper, conjunctive relationships wereused as knowledge with the best-match algo-rithm, in addition to a thesaurus.
However,various types of knowledge will be required ona large scale for a more robust system.
Au-tomatic or semi-mttomatic acquisition, usingcorpus-based methods, is also needed.2.
If there are many unknown words ill an all\]-biguity, unknown-word matching will not workwell.
In additio,t to scaling up the example-base and the tlwsaurus, we should deve, top anmre robust algorithm.References\[1\] Collins.
The New Collin,~ The,~aurus.
Collins Pub-lishers, Glasgow, 1984.\[2\] IBM Corpor~ttion.
1BM Dictionary of Comp'~din.q,volume SC20q699o(17.
IBM Corporation, 1988.\[3\] S. Kinoslfita, M. Shimazu, and H. Hirakawa.
"Bet-ter Translation with Knowledge Extracted fromSouree Text".
In Proceedings of TM1-93, pages240 252, 1993.\[4\] Merriam.
Webster'a Seventh Nc'wCollegiate Dictionary.
G.& C. Merriam, Spring-tield,Massttehuset t , 1963.\[5\] K. Nagao.
"Delmmleney Analyzer: A Knowledge-Based A1)proaeh to Structural Disambiguation".
InProcceding,~ of COLING-90, pages 282 287, 1990.\[6\] M. Nagao.
"A Frmnework of a Mechanical Trans-lation between a~pmtese ~md English by AnalogyPrinciple".
In A. Elithorn and 12.
Banerji, editors,Artificial and Human Intelligence.
NAT(), 1984.\[7\] T. Nasukawa.
"Discourse Constraint ill ComputerMammls".
In Proceedinga of TMI-93, pages 183194, 1993.\[8\] S. Niren|mrg, (2.
Domashnev, and D. I.
Grannes.
"Two Approaclms to Matching iu Exmnple-BasedMachine ~lYmLslation".
In PTvcec, ding.~ of TMI-93,pages 4"\[ 57, 1993.\[9\] P. Proeter.
Longman Dictionary of ContemporaryEngli,~h.
Longman Group Limited, Harlow toldLondon, England, 1978.\[10\] S. Sate and M. Nagao.
"Towards Memory-BasedTrmtslation".
in Proeeeding,~ of CO LING-90, pages1.46 152, 1990.\[11\] N. Urmnoto.
"LexieM and Structural Dismnbigua-lion Using an Exmnl)le-Base".
In The 2rid Japan-Australia ,loint 5'ympo,~i,um on Natural LanguageProcc,~,sing, 1);tges 150 160, 1991.\[12\] N. Urmnoto.
"lgxmnple-Based Word-Sense I)isam-biguation".
IEICE Transactio.n.~ on Informationand Sy.~tcma, E77-D(2), 1.994.721
