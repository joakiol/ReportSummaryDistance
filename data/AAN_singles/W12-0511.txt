Proceedings of the Workshop on Innovative Hybrid Approaches to the Processing of Textual Data (Hybrid2012), EACL 2012, pages 78?86,Avignon, France, April 23 2012. c?2012 Association for Computational LinguisticsA random forest system combination approach for error detection indigital dictionariesMichael Bloodgood and Peng Ye and Paul Rodriguesand David Zajic and David DoermannUniversity of MarylandCollege Park, MDmeb@umd.edu, pengye@umiacs.umd.edu, prr@umd.edu,dzajic@casl.umd.edu, doermann@umiacs.umd.eduAbstractWhen digitizing a print bilingual dictionary,whether via optical character recognition ormanual entry, it is inevitable that errors areintroduced into the electronic version that iscreated.
We investigate automating the pro-cess of detecting errors in an XML repre-sentation of a digitized print dictionary us-ing a hybrid approach that combines rule-based, feature-based, and language model-based methods.
We investigate combin-ing methods and show that using randomforests is a promising approach.
We findthat in isolation, unsupervised methods ri-val the performance of supervised methods.Random forests typically require trainingdata so we investigate how we can applyrandom forests to combine individual basemethods that are themselves unsupervisedwithout requiring large amounts of trainingdata.
Experiments reveal empirically thata relatively small amount of data is suffi-cient and can potentially be further reducedthrough specific selection criteria.1 IntroductionDigital versions of bilingual dictionaries oftenhave errors that need to be fixed.
For example,Figures 1 through 5 show an example of an er-ror that occurred in one of our development dic-tionaries and how the error should be corrected.Figure 1 shows the entry for the word ?turfah?
asit appeared in the original print copy of (Qureshiand Haq, 1991).
We see this word has three senseswith slightly different meanings.
The third senseis ?rare?.
In the original digitized XML versionof (Qureshi and Haq, 1991) depicted in Figure 2,this was misrepresented as not being the meaningFigure 1: Example dictionary entryFigure 2: Example of error in XMLof ?turfah?
but instead being a usage note that fre-quency of use of the third sense was rare.
Figure 3shows the tree corresponding to this XML repre-sentation.
The corrected digital XML representa-tion is depicted in Figure 4 and the correspondingcorrected tree is shown in Figure 5.Zajic et al (2011) presented a method for re-pairing a digital dictionary in an XML format us-ing a dictionary markup language called DML.
Itremains time-consuming and error-prone howeverto have a human read through and manually cor-rect a digital version of a dictionary, even withlanguages such as DML available.
We thereforeinvestigate automating the detection of errors.We investigate the use of three individual meth-ods.
The first is a supervised feature-basedmethod trained using SVMs (Support Vector Ma-chines).
The second is a language-modeling78.ENTRY.
.?
?
?..SENSE.USG.rare.?
?
?FORM.
.PRON.t?r?fahORTH.???
?Figure 3: Tree structure of errorFigure 4: Example of error in XML, fixed.ENTRY.
.?
?
?..SENSE.TRANS.TR.rare..?
?
?FORM.
.PRON.t?r?fahORTH.???
?Figure 5: Tree structure of error, fixedmethod that replicates the method presented in(Rodrigues et al, 2011).
The third is a simplerule inference method.
The three individual meth-ods have different performances.
So we investi-gate how we can combine the methods most effec-tively.
We experiment with majority vote, scorecombination, and random forest methods and findthat random forest combinations work the best.For many dictionaries, training data will not beavailable in large quantities a priori and thereforemethods that require only small amounts of train-ing data are desirable.
Interestingly, for automati-cally detecting errors in dictionaries, we find thatthe unsupervised methods have performance thatrivals that of the supervised feature-based methodtrained using SVMs.
Moreover, when we com-bine methods using the random forest method, thecombination of unsupervised methods works bet-ter than the supervised method in isolation and al-most as well as the combination of all availablemethods.
A potential drawback of using the ran-dom forest combination method however is that itrequires training data.
We investigated how muchtraining data is needed and find that the amountof training data required is modest.
Furthermore,by selecting the training data to be labeled withthe use of specific selection methods reminiscentof active learning, it may be possible to train therandom forest system combination method witheven less data without sacrificing performance.In section 2 we discuss previous related workand in section 3 we explain the three individualmethods we use for our application.
In section 4we explain the three methods we explored forcombining methods; in section 5 we present anddiscuss experimental results and in section 6 weconclude and discuss future work.2 Related WorkClassifier combination techniques can be broadlyclassified into two categories: mathematical andbehavioral (Tulyakov et al, 2008).
In the firstcategory, functions or rules combine normalizedclassifier scores from individual classifiers.
Ex-amples of techniques in this category include Ma-jority Voting (Lam and Suen, 1997), as well assimple score combination rules such as: sum rule,min rule, max rule and product rule (Kittler et al,1998; Ross and Jain, 2003; Jain et al, 2005).
Inthe second category, the output of individual clas-sifiers are combined to form a feature vector as79the input to a generic classifier such as classifi-cation trees (P. and Chollet, 1999; Ross and Jain,2003) or the k-nearest neighbors classifier (P. andChollet, 1999).
Our method falls into the secondcategory, where we use a random forest for sys-tem combination.The random forest method is described in(Breiman, 2001).
It is an ensemble classifier con-sisting of a collection of decision trees (called arandom forest) and the output of the random for-est is the mode of the classes output by the indi-vidual trees.
Each single tree is trained as follows:1) a random set of samples from the initial train-ing set is selected as a training set and 2) at eachnode of the tree, a random subset of the features isselected, and the locally optimal split is based ononly this feature subset.
The tree is fully grownwithout pruning.
Ma et al (2005) used randomforests for combining scores of several biometricdevices for identity verification and have shownencouraging results.
They use all fully supervisedmethods.
In contrast, we explore minimizing theamount of training data needed to train a randomforest of unsupervised methods.The use of active learning in order to re-duce training data requirements without sacri-ficing model performance has been reported onextensively in the literature (e.g., (Seung et al,1992; Cohn et al, 1994; Lewis and Gale, 1994;Cohn et al, 1996; Freund et al, 1997)).
Whentraining our random forest combination of indi-vidual methods that are themselves unsupervised,we explore how to select the data so that onlysmall amounts of training data are needed becausefor many dictionaries, gathering training data maybe expensive and labor-intensive.3 Three Single Method Approaches forError DetectionBefore we discuss our approaches for combiningsystems, we briefly explain the three individualsystems that form the foundation of our combinedsystem.First, we use a supervised approach where wetrain a model using SVMlight (Joachims, 1999)with a linear kernel and default regularization pa-rameters.
We use a depth first traversal of theXML tree and use unigrams and bigrams of thetags that occur as features for each subtree tomake a classification decision.We also explore two unsupervised approaches.The first unsupervised approach learns rules forwhen to classify nodes as errors or not.
The rule-based method computes an anomaly score basedon the probability of subtree structures.
Givena structure A and its probability P(A), the eventthat A occurs has anomaly score 1-P(A) and theevent that A does not occur has anomaly scoreP(A).
The basic idea is if a certain structure hap-pens rarely, i.e.
P(A) is very small, then the oc-currence of A should have a high anomaly score.On the other hand, if A occurs frequently, thenthe absence of A indicates anomaly.
To obtainthe anomaly score of a tree, we simply take themaximal scores of all events induced by subtreeswithin this tree.The second unsupervised approach uses a reim-plementation of the language modeling methoddescribed in (Rodrigues et al, 2011).
Briefly,this methods works by calculating the probabil-ity a flattened XML branch can occur, given aprobability model trained on the XML branchesfrom the original dictionary.
We used (Stolcke,2002) to generate bigram models using Good Tur-ing smoothing and Katz back off, and evaluatedthe log probability of the XML branches, rankingthe likelihood.
The first 1000 branches were sub-mitted to the hybrid system marked as an error,and the remaining were submitted as a non-error.Results for the individual classifiers are presentedin section 5.4 Three Methods for CombiningSystemsWe investigate three methods for combining thethree individual methods.
As a baseline, we in-vestigate simple majority vote.
This method takesthe classification decisions of the three methodsand assigns the final classification as the classifi-cation that the majority of the methods predicted.A drawback of majority vote is that it does notweight the votes at all.
However, it might makesense to weight the votes according to factors suchas the strength of the classification score.
For ex-ample, all of our classifiers make binary decisionsbut output scores that are indicative of the confi-dence of their classifications.
Therefore we alsoexplore a score combination method that consid-ers these scores.
Since measures from the differ-ent systems are in different ranges, we normal-ize these measurements before combining them(Jain et al, 2005).
We use z-score which com-80putes the arithmetic mean and standard deviationof the given data for score normalization.
We thentake the summation of normalized measures asthe final measure.
Classification is performed bythresholding this final measure.1Another approach would be to weight them bythe performance level of the various constituentclassifiers in the ensemble.
Weighting based onperformance level of the individual classifiers isdifficult because it would require extra labeleddata to estimate the various performance lev-els.
It is not clear how to translate the differ-ent performance estimates into weights, or howto have those weights interact with weights basedon strengths of classification.
Therefore, we didnot weigh based on performance level explicitly.We believe that our third combination method,the use of random forests, implicitly cap-tures weighting based on performance level andstrengths of classifications.
Our random forest ap-proach uses three features, one for each of the in-dividual systems we use.
With random forests,strengths of classification are taken into accountbecause they form the values of the three fea-tures we use.
In addition, the performance levelis taken into account because the training dataused to train the decision trees that form the for-est help to guide binning of the feature values intoappropriate ranges where classification decisionsare made correctly.
This will be discussed furtherin section 5.5 ExperimentsThis section explains the details of the experi-ments we conducted testing the performance ofthe various individual and combined systems.Subsection 5.1 explains the details of the data weexperiment on; subsection 5.2 provides a sum-mary of the main results of our experiments; andsubsection 5.3 discusses the results.5.1 Experimental SetupWe obtained the data for our experiments usinga digitized version of (Qureshi and Haq, 1991),the same Urdu-English dictionary that Zajic etal.
(2011) had used.
Zajic et al (2011) pre-sented DML, a programming language used tofix errors in XML documents that contain lexico-graphic data.
A team of language experts used1In our experiments we used 0 as the threshold.Recall Precision F1-Measure AccuracyLM 11.97 89.90 21.13 57.53RULE 99.79 70.83 82.85 80.37FV 35.34 93.68 51.32 68.14Table 1: Performance of individual systems atENTRY tier.DML to correct errors in a digital, XML repre-sentation of the Kitabistan Urdu dictionary.
Thecurrent research compared the source XML doc-ument and the DML commands to identify the el-ements that the language experts decided to mod-ify.
We consider those elements to be errors.
Thisis the ground truth used for training and evalua-tion.
We evaluate at two tiers, corresponding totwo node types in the XML representation of thedictionary: ENTRY and SENSE.
The example de-picted in Figures 1 through 5 shows an example ofSENSE.
The intuition of the tier is that errors aredetectable (or learnable) from observing the ele-ments within a tier, and do not cross tier bound-aries.
These tiers are specific to the KitabistanUrdu dictionary, and we selected them by observ-ing the data.
A limitation of our work is that we donot know at this time whether they are generallyuseful across dictionaries.
Future work will beto automatically discover the meaningful evalua-tion tiers for a new dictionary.
After this process,we have a dataset with 15,808 Entries, of which47.53% are marked as errors and 78,919 Senses,of which 10.79% are marked as errors.
We per-form tenfold cross-validation in all experiments.In our random forest experiments, we use 12 de-cision trees, each with only 1 feature.5.2 ResultsThis section presents experimental results, firstfor individual systems and then for combined sys-tems.5.2.1 Performance of individual systemsTables 1 and 2 show the performance of lan-guage modeling-based method (LM), rule-basedmethod (RULE) and the supervised feature-basedmethod (FV) at different tiers.
As can be seen,at the ENTRY tier, RULE obtains the highest F1-Measure and accuracy, while at the SENSE tier,FV performs the best.81Recall Precision F1-Measure AccuracyLM 9.85 94.00 17.83 90.20RULE 84.59 58.86 69.42 91.96FV 72.44 98.66 83.54 96.92Table 2: Performance of individual systems atSENSE tier.5.2.2 Improving individual systems usingrandom forestsIn this section, we show that by applying ran-dom forests on top of the output of individual sys-tems, we can have gains (absolute gains, not rel-ative) in accuracy of 4.34% to 6.39% and gains(again absolute, not relative) in F1-measure of3.64% to 11.39%.
Tables 3 and 4 show our ex-perimental results at ENTRY and SENSE tierswhen applying random forests with the rule-basedmethod.2 These results are all obtained from 100iterations of the experiments with different parti-tions of the training data chosen at each iteration.Mean values of different evaluation measures andtheir standard deviations are shown in these ta-bles.
We change the percentage of training dataand repeat the experiments to see how the amountof training data affects performance.It might be surprising to see the gains in per-formance that can be achieved by using a ran-dom forest of decision trees created using onlythe rule-based scores as features.
To shed lighton why this is so, we show the distribution ofRULE-based output scores for anomaly nodes andclean nodes in Figure 6.
They are well separatedand this explains why RULE alone can have goodperformance.
Recall RULE classifies nodes withanomaly scores larger than 0.9 as errors.
How-ever, in Figure 6, we can see that there are manyclean nodes with anomaly scores larger than 0.9.Thus, the simple thresholding strategy will bringin errors.
Applying random forest will help usidentify these errorful regions to improve the per-formance.
Another method for helping to identifythese errorful regions and classify them correctlyis to apply random forest of RULE combined withthe other methods, which we will see will evenfurther boost the performance.2We also applied random forests to our language mod-eling and feature-based methods, and saw similar gains inperformance.0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1050010001500output score of rule-based systemoccurrencesanomalycleanFigure 6: Output anomalies score from RULE(ENTRY tier).5.2.3 System combinationIn this section, we explore different methodsfor combining measures from the three systems.Table 5 shows the results of majority voting andscore combination at the ENTRY tier.
As canbe seen, majority voting performs poorly.
Thismay be due to the fact that the performances ofthe three systems are very different.
RULE sig-nificantly outperforms the other two systems, andas discussed in Section 4 neither majority votingnor score combination weights this higher perfor-mance appropriately.Tables 6 and 7 show the results of combiningRULE and LM.
This is of particular interest sincethese two systems are unsupervised.
Combin-ing these two unsupervised systems works betterthan the individual methods, including supervisedmethods.
Tables 8 and 9 show the results for com-binations of all available systems.
This yields thehighest performance, but only slightly higher thanthe combination of only unsupervised base meth-ods.The random forest combination technique doesrequire labeled data even if the underlying basemethods are unsupervised.
Based on the ob-servation in Figure 6, we further study whetherchoosing more training data from the most error-ful regions will help to improve the performance.Experimental results in Table 10 show how thechoice of training data affects performance.
Itappears that there may be a weak trend towardhigher performance when we force the selectionof the majority of the training data to be fromENTRY nodes whose RULE anomaly scores are82Training % Recall Precision F1-Measure Accuracy0.1 78.17( 14.83) 75.87( 3.96) 76.18( 7.99) 77.68( 5.11)1 82.46( 4.81) 81.34( 2.14) 81.79( 2.20) 82.61( 1.69)10 87.30( 1.96) 84.11( 1.29) 85.64( 0.46) 86.10( 0.35)50 89.19( 1.75) 83.99( 1.20) 86.49( 0.34) 86.76( 0.28)Table 3: Mean and std of evaluation measures from 100 iterations of experiments using RULE+RF.
(ENTRY tier)Training % Recall Precision F1-Measure Accuracy0.1 60.22( 12.95) 69.66( 9.54) 63.29( 7.92) 92.61( 1.57)1 70.28( 3.48) 86.26( 3.69) 77.31( 1.39) 95.55( 0.25)10 71.52( 1.23) 91.26( 1.39) 80.18( 0.41) 96.18( 0.07)50 72.11( 0.75) 91.90( 0.64) 80.81( 0.39) 96.30( 0.06)Table 4: Mean and std of evaluation measures from 100 iterations of experiments using RULE+RF.
(SENSE tier)larger than 0.9.
However, the magnitudes of theobserved differences in performance are within asingle standard deviation so it remains for futurework to determine if there are ways to select thetraining data for our random forest combinationin ways that substantially improve upon randomselection.5.3 DiscussionMajority voting (at the entry level) performspoorly, since the performance of the three individ-ual systems are very different and majority votingdoes not weight votes at all.
Score combinationis a type of weighted voting.
It takes into accountthe confidence level of output from different sys-tems, which enables it to perform better than ma-jority voting.
However, score combination doesnot take into account the performance levels ofthe different systems, and we believe this limits itsperformance compared with random forest com-binations.Random forest combinations perform the best,but the cost is that it is a supervised combinationmethod.
We investigated how the amount of train-ing data affects the performance, and found that asmall amount of labeled data is all that the randomforest needs in order to be successful.
Moreover,although this requires further exploration, there isweak evidence that the size of the labeled data canpotentially be reduced by choosing it carefullyfrom the region that is expected to be most error-ful.
For our application with a rule-based system,this is the high-anomaly scoring region becausealthough it is true that anomalies are often errors,it is also the case that some structures occur rarelybut are not errorful.RULE+LM with random forest is a little bet-ter than RULE with random forest, with gain ofabout 0.7% on F1-measure when evaluated at theENTRY level using 10% data for training.An examination of examples that are marked asbeing errors in our ground truth but that were notdetected to be errors by any of our systems sug-gests that some examples are decided on the ba-sis of features not yet considered by any system.For example, in Figure 7 the second FORM iswell-formed structurally, but the Urdu text in thefirst FORM is the beginning of the phrase translit-erated in the second FORM.
Automatic systemsdetected that the first FORM was an error, how-ever did not mark the second FORM as an errorwhereas our ground truth marked both as errors.Examination of false negatives also revealedcases where the systems were correct that therewas no error but our ground truth wrongly indi-cated that there was an error.
These were due toour semi-automated method for producing groundtruth that considers elements mentioned in DMLcommands to be errors.
We discovered instancesin which merely mentioning an element in a DMLcommand does not imply that the element is an er-ror.
These cases are useful for making refinementsto how ground truth is generated from DML com-mands.Examination of false positives revealed twocategories.
One was where the element is indeedan error but was not marked as an element in ourground truth because it was part of a larger error83Method Recall Precision F1-Measure AccuracyMajority voting 36.71 90.90 52.30 68.18Score combination 76.48 75.82 76.15 77.23Table 5: LM+RULE+FV (ENTRY tier)Training % Recall Precision F1-Measure Accuracy0.1 77.43( 15.14) 72.77( 6.03) 74.26( 8.68) 75.32( 6.71)1 86.50( 3.59) 80.41( 1.95) 83.27( 1.33) 83.51( 1.11)10 88.12( 1.12) 84.65( 0.57) 86.34( 0.46) 86.76( 0.39)50 89.12( 0.62) 87.39( 0.56) 88.25( 0.30) 88.72( 0.29)Table 6: System combination based on random forest (LM+RULE).
(ENTRY tier, mean (std))Training % Recall Precision F1-Measure Accuracy0.1 65.85( 12.70) 71.96( 7.63) 67.68( 7.06) 93.38( 1.03)1 80.29( 3.58) 84.97( 3.13) 82.45( 1.36) 96.31( 0.28)10 82.68( 2.49) 90.91( 2.37) 86.53( 0.41) 97.22( 0.07)50 83.22( 2.43) 92.21( 2.29) 87.42( 0.35) 97.42( 0.04)Table 7: System combination based on random forest (LM+RULE).
(SENSE tier, mean (std))Training % Recall Precision F1-Measure Accuracy20 91.57( 0.55) 87.77( 0.43) 89.63( 0.23) 89.93( 0.22)50 92.04( 0.54) 88.85( 0.48) 90.41( 0.29) 90.72( 0.28)Table 8: System combination based on random forest (LM+RULE+FV).
(ENTRY tier, mean (std))Training % Recall Precision F1-Measure Accuracy20 86.47( 1.01) 90.67( 1.02) 88.51( 0.26) 97.58( 0.06)50 86.50( 0.81) 92.04( 0.85) 89.18( 0.30) 97.73( 0.06)Table 9: System combination based on random forest (LM+RULE+FV).
(SENSE tier, mean (std))Recall Precision F1-Measure Accuracy50% 85.40( 4.65) 80.71( 3.49) 82.82( 1.57) 82.63( 1.54)70% 86.13( 3.94) 80.97( 2.64) 83.36( 1.33) 83.30( 1.21)90% 85.77( 3.61) 81.82( 2.72) 83.65( 1.45) 83.69( 1.35)95% 85.93( 3.46) 82.14( 2.98) 83.89( 1.32) 83.94( 1.18)random 86.50( 3.59) 80.41( 1.95) 83.27( 1.33) 83.51( 1.11)Table 10: Effect of choice of training data based on rule based method (Mean evaluation measuresfrom 100 iterations of experiments using RULE+LM at ENTRY tier).
We choose 1% of the data fortraining and the first column in the table specifies the percentage of training data chosen from Entrieswith anomalous score larger than 0.9.84Figure 7: Example of error in XMLthat got deleted and therefore no DML commandever mentioned the smaller element but lexicog-raphers upon inspection agree that the smaller el-ement is indeed errorful.
The other category waswhere there were actual errors that the dictionaryeditors didn?t repair with DML but that shouldhave been repaired.A major limitation of our work is testing howwell it generalizes to detecting errors in other dic-tionaries besides the Urdu-English one (Qureshiand Haq, 1991) that we conducted our experi-ments on.6 ConclusionsWe explored hybrid approaches for the applica-tion of automatically detecting errors in digitizedcopies of dictionaries.
The base methods weexplored consisted of a variety of unsupervisedand supervised methods.
The combination meth-ods we explored also consisted of some methodswhich required labeled data and some which didnot.We found that our base methods had differ-ent levels of performance and with this scenariomajority voting and score combination methods,though appealing since they require no labeleddata, did not perform well since they do notweight votes well.We found that random forests of decision treeswas the best combination method.
We hypothe-size that this is due to the nature of our task andbase systems.
Random forests were able to helptease apart the high-error region (where anoma-lies take place).
A drawback of random forestsas a combination method is that they require la-beled data.
However, experiments reveal empiri-cally that a relatively small amount of data is suf-ficient and the amount might be able to be furtherreduced through specific selection criteria.AcknowledgmentsThis material is based upon work supported, inwhole or in part, with funding from the UnitedStates Government.
Any opinions, findings andconclusions, or recommendations expressed inthis material are those of the author(s) and do notnecessarily reflect the views of the University ofMaryland, College Park and/or any agency or en-tity of the United States Government.
Nothingin this report is intended to be and shall not betreated or construed as an endorsement or recom-mendation by the University of Maryland, UnitedStates Government, or the authors of the product,process, or service that is the subject of this re-port.
No one may use any information containedor based on this report in advertisements or pro-motional materials related to any company prod-uct, process, or service or in support of other com-mercial purposes.ReferencesLeo Breiman.
2001.
Random forests.
MachineLearning, 45:5?32.
10.1023/A:1010933404324.David A. Cohn, Les Atlas, and Richard Ladner.
1994.Improving generalization with active learning.
Ma-chine Learning, 15:201?221.David A. Cohn, Zoubin Ghahramani, and Michael I.Jordan.
1996.
Active learning with statistical mod-els.
Journal of Artificial Intelligence Research,4:129?145.Yoav Freund, H. Sebastian Seung, Eli Shamir, andNaftali Tishby.
1997.
Selective sampling using thequery by committee algorithm.
Machine Learning,28:133?168.Anil K. Jain, Karthik Nandakumar, and Arun Ross.2005.
Score normalization in multimodal biometricsystems.
Pattern Recognition, pages 2270?2285.Thorsten Joachims.
1999.
Making large-scale SVMlearning practical.
In Bernhard Scho?lkopf, Christo-pher J. Burges, and Alexander J. Smola, editors, Ad-vances in Kernel Methods ?
Support Vector Learn-ing, chapter 11, pages 169?184.
The MIT Press,Cambridge, US.J.
Kittler, M. Hatef, R.P.W.
Duin, and J. Matas.1998.
On combining classifiers.
Pattern Analysisand Machine Intelligence, IEEE Transactions on,20(3):226 ?239, mar.L.
Lam and S.Y.
Suen.
1997.
Application of majorityvoting to pattern recognition: an analysis of its be-havior and performance.
Systems, Man and Cyber-netics, Part A: Systems and Humans, IEEE Trans-actions on, 27(5):553 ?568, sep.David D. Lewis and William A. Gale.
1994.
A se-quential algorithm for training text classifiers.
InSIGIR ?94: Proceedings of the 17th annual inter-national ACM SIGIR conference on Research anddevelopment in information retrieval, pages 3?12,85New York, NY, USA.
Springer-Verlag New York,Inc.Yan Ma, Bojan Cukic, and Harshinder Singh.
2005.A classification approach to multi-biometric scorefusion.
In AVBPA?05, pages 484?493.Verlinde P. and G. Chollet.
1999.
Comparing deci-sion fusion paradigms using k-nn based classifiers,decision trees and logistic regression in a multi-modal identity verification application.
In Proceed-ings of the 2nd International Conference on Audioand Video-Based Biometric Person Authentication(AVBPA), pages 189?193.Bashir Ahmad Qureshi and Abdul Haq.
1991.
Stan-dard Twenty First Century Urdu-English Dictio-nary.
Educational Publishing House, Delhi.Paul Rodrigues, David Zajic, David Doermann,Michael Bloodgood, and Peng Ye.
2011.
Detect-ing structural irregularity in electronic dictionariesusing language modeling.
In Proceedings of theConference on Electronic Lexicography in the 21stCentury, pages 227?232, Bled, Slovenia, Novem-ber.
Trojina, Institute for Applied Slovene Studies.Arun Ross and Anil Jain.
2003.
Information fusion inbiometrics.
Pattern Recognition Letters, 24:2115?2125.H.
S. Seung, M. Opper, and H. Sompolinsky.
1992.Query by committee.
In COLT ?92: Proceedings ofthe fifth annual workshop on Computational learn-ing theory, pages 287?294, New York, NY, USA.ACM.Andreas Stolcke.
2002.
Srilm - an extensible languagemodeling toolkit.
In Proceedings of the Interna-tional Conference on Spoken Language Processing.Sergey Tulyakov, Stefan Jaeger, Venu Govindaraju,and David Doermann.
2008. Review of classi-fier combination methods.
In Machine Learning inDocument Analysis and Recognition, volume 90 ofStudies in Computational Intelligence, pages 361?386.
Springer Berlin / Heidelberg.David Zajic, Michael Maxwell, David Doermann, PaulRodrigues, and Michael Bloodgood.
2011.
Cor-recting errors in digital lexicographic resources us-ing a dictionary manipulation language.
In Pro-ceedings of the Conference on Electronic Lexicog-raphy in the 21st Century, pages 297?301, Bled,Slovenia, November.
Trojina, Institute for AppliedSlovene Studies.86
