Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 500?509,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsCoherent Citation-Based Summarization of Scientific PapersAmjad Abu-JbaraEECS DepartmentUniversity of MichiganAnn Arbor, MI, USAamjbara@umich.eduDragomir RadevEECS Department andSchool of InformationUniversity of MichiganAnn Arbor, MI, USAradev@umich.eduAbstractIn citation-based summarization, text writtenby several researchers is leveraged to identifythe important aspects of a target paper.
Previ-ous work on this problem focused almost ex-clusively on its extraction aspect (i.e.
selectinga representative set of citation sentences thathighlight the contribution of the target paper).Meanwhile, the fluency of the produced sum-maries has been mostly ignored.
For exam-ple, diversity, readability, cohesion, and order-ing of the sentences included in the summaryhave not been thoroughly considered.
This re-sulted in noisy and confusing summaries.
Inthis work, we present an approach for produc-ing readable and cohesive citation-based sum-maries.
Our experiments show that the pro-posed approach outperforms several baselinesin terms of both extraction quality and fluency.1 IntroductionScientific research is a cumulative activity.
Thework of downstream researchers depends on accessto upstream discoveries.
The footnotes, end notes,or reference lists within research articles make thisaccumulation possible.
When a reference appears ina scientific paper, it is often accompanied by a spanof text describing the work being cited.We name the sentence that contains an explicitreference to another paper citation sentence.
Cita-tion sentences usually highlight the most importantaspects of the cited paper such as the research prob-lem it addresses, the method it proposes, the goodresults it reports, and even its drawbacks and limita-tions.By aggregating all the citation sentences that citea paper, we have a rich source of information aboutit.
This information is valuable because human ex-perts have put their efforts to read the paper and sum-marize its important contributions.One way to make use of these sentences is creat-ing a summary of the target paper.
This summaryis different from the abstract or a summary gener-ated from the paper itself.
While the abstract rep-resents the author?s point of view, the citation sum-mary is the summation of multiple scholars?
view-points.
The task of summarizing a scientific paperusing its set of citation sentences is called citation-based summarization.There has been previous work done on citation-based summarization (Nanba et al, 2000; Elkiss etal., 2008; Qazvinian and Radev, 2008; Mei and Zhai,2008; Mohammad et al, 2009).
Previous work fo-cused on the extraction aspect; i.e.
analyzing thecollection of citation sentences and selecting a rep-resentative subset that covers the main aspects of thepaper.
The cohesion and the readability of the pro-duced summaries have been mostly ignored.
Thisresulted in noisy and confusing summaries.In this work, we focus on the coherence and read-ability aspects of the problem.
Our approach pro-duces citation-based summaries in three stages: pre-processing, extraction, and postprocessing.
Our ex-periments show that our approach produces bettersummaries than several baseline summarization sys-tems.The rest of this paper is organized as follows.
Af-ter we examine previous work in Section 2, we out-line the motivation of our approach in Section 3.Section 4 describes the three stages of our summa-rization system.
The evaluation and the results arepresented in Section 5.
Section 6 concludes the pa-per.5002 Related WorkThe idea of analyzing and utilizing citation informa-tion is far from new.
The motivation for using in-formation latent in citations has been explored tensof years back (Garfield et al, 1984; Hodges, 1972).Since then, there has been a large body of researchdone on citations.Nanba and Okumura (2000) analyzed citationsentences and automatically categorized citationsinto three groups using 160 pre-defined phrase-based rules.
They also used citation categoriza-tion to support a system for writing surveys (Nanbaand Okumura, 1999).
Newman (2001) analyzedthe structure of the citation networks.
Teufel etal.
(2006) addressed the problem of classifying ci-tations based on their function.Siddharthan and Teufel (2007) proposed a methodfor determining the scientific attribution of an arti-cle by analyzing citation sentences.
Teufel (2007)described a rhetorical classification task, in whichsentences are labeled as one of Own, Other, Back-ground, Textual, Aim, Basis, or Contrast accordingto their role in the authors argument.
In parts of ourapproach, we were inspired by this work.Elkiss et al (2008) performed a study on citationsummaries and their importance.
They concludedthat citation summaries are more focused and con-tain more information than abstracts.
Mohammadet al (2009) suggested using citation information togenerate surveys of scientific paradigms.Qazvinian and Radev (2008) proposed a methodfor summarizing scientific articles by building a sim-ilarity network of the citation sentences that citethe target paper, and then applying network analy-sis techniques to find a set of sentences that coversas much of the summarized paper facts as possible.We use this method as one of the baselines when weevaluate our approach.
Qazvinian et al (2010) pro-posed a citation-based summarization method thatfirst extracts a number of important keyphrases fromthe set of citation sentences, and then finds the bestsubset of sentences that covers as many keyphrasesas possible.
Qazvinian and Radev (2010) addressedthe problem of identifying the non-explicit citingsentences to aid citation-based summarization.3 MotivationThe coherence and readability of citation-basedsummaries are impeded by several factors.
First,many citation sentences cite multiple papers besidesthe target.
For example, the following is a citationsentence that appeared in the NLP literature andtalked about Resnik?s (1999) work.
(1) Grefenstette and Nioche (2000) and Jonesand Ghani (2000) use the web to generate cor-pora for languages where electronic resources arescarce, while Resnik (1999) describes a methodfor mining the web for bilingual texts.The first fragment of this sentence describes dif-ferent work other than Resnik?s.
The contributionof Resnik is mentioned in the underlined fragment.Including the irrelevant fragments in the summarycauses several problems.
First, the aim of the sum-marization task is to summarize the contribution ofthe target paper using minimal text.
These frag-ments take space in the summary while being irrel-evant and less important.
Second, including thesefragments in the summary breaks the context and,hence, degrades the readability and confuses thereader.
Third, the existence of irrelevant fragmentsin a sentence makes the ranking algorithm assign alow weight to it although the relevant fragment maycover an aspect of the paper that no other sentencecovers.A second factor has to do with the ordering of thesentences included in the summary.
For example,the following are two other citation sentences forResnik (1999).
(2) Mining the Web for bilingual text (Resnik, 1999) isnot likely to provide sufficient quantities of high qualitydata.
(3) Resnik (1999) addressed the issue of languageidentification for finding Web pages in the languages ofinterest.If these two sentences are to be included in thesummary, the reasonable ordering would be to putthe second sentence first.Thirdly, in some instances of citation sentences,the reference is not a syntactic constituent in the sen-501tence.
It is added just to indicate the existence ofcitation.
For example, in sentence (2) above, the ref-erence could be safely removed from the sentencewithout hurting its grammaticality.In other instances (e.g.
sentence (3) above), thereference is a syntactic constituent of the sentenceand removing it makes the sentence ungrammatical.However, in certain cases, the reference could be re-placed with a suitable pronoun (i.e.
he, she or they).This helps avoid the redundancy that results from re-peating the author name(s) in every sentence.Finally, a significant number of citation sentencesare not suitable for summarization (Teufel et al,2006) and should be filtered out.
The followingsentences are two examples.
(4) The two algorithms we employed in our depen-dency parsing model are the Eisner parsing (Eisner,1996) and Chu-Lius algorithm (Chu and Liu, 1965).
(5) This type of model has been used by, among others,Eisner (1996).Sentence (4) appeared in a paper by Nguyen et al(2007).
It does not describe any aspect of Eisner?swork, rather it informs the reader that Nguyen et alused Eisner?s algorithm in their model.
There is novalue in adding this sentence to the summary of Eis-ner?s paper.
Teufel (2007) reported that a significantnumber of citation sentences (67% of the sentencesin her dataset) were of this type.Likewise, the comprehension of sentence (5) de-pends on knowing its context (i.e.
its surroundingsentences).
This sentence alone does not provideany valuable information about Eisner?s paper andshould not be added to the summary unless its con-text is extracted and included in the summary aswell.In our approach, we address these issues toachieve the goal of improving the coherence and thereadability of citation-based summaries.4 ApproachIn this section we describe a system that takes a sci-entific paper and a set of citation sentences that citeit as input, and outputs a citation summary of thepaper.
Our system produces the summaries in threestages.
In the first stage, the citation sentences arepreprocessed to rule out the unsuitable sentences andthe irrelevant fragments of sentences.
In the sec-ond stage, a number of citation sentences that coverthe various aspects of the paper are selected.
In thelast stage, the selected sentences are post-processedto enhance the readability of the summary.
We de-scribe the stages in the following three subsections.4.1 PreprocessingThe aim of this stage is to determine which pieces oftext (sentences or fragments of sentences) should beconsidered for selection in the next stage and whichones should be excluded.
This stage involves threetasks: reference tagging, reference scope identifica-tion, and sentence filtering.4.1.1 Reference TaggingA citation sentence contains one or more references.At least one of these references corresponds to thetarget paper.
When writing scientific articles, au-thors usually use standard patterns to include point-ers to their references within the text.
We use patternmatching to tag such references.
The reference tothe target is given a different tag than the referencesto other papers.The following example shows a citation sentencewith all the references tagged and the target refer-ence given a different tag.In <TREF>Resnik (1999)</TREF>, <REF>Nie,Simard, and Foster (2001)</REF>, <REF>Ma andLiberman (1999)</REF>, and <REF>Resnik andSmith (2002)</REF>, the Web is harvested in search ofpages that are available in two languages.4.1.2 Identifying the Reference ScopeIn the previous section, we showed the importanceof identifying the scope of the target reference; i.e.the fragment of the citation sentence that corre-sponds to the target paper.
We define the scope ofa reference as the shortest fragment of the citationsentence that contains the reference and could forma grammatical sentence if the rest of the sentencewas removed.To find such a fragment, we use a simple yet ade-quate heuristic.
We start by parsing the sentence us-ing the link grammar parser (Sleator and Temperley,5021991).
Since the parser is not trained on citation sen-tences, we replace the references with placeholdersbefore passing the sentence to the parser.
Figure 1shows a portion of the parse tree for Sentence (1)(from Section 1).Figure 1: An example showing the scope of a target ref-erenceWe extract the scope of the reference from theparse tree as follows.
We find the smallest subtreerooted at an S node (sentence clause node) and con-tains the target reference node.
We extract the textthat corresponds to this subtree if it is grammati-cal.
Otherwise, we find the second smallest subtreerooted at an S node and so on.
For example, theparse tree shown in Figure 1 suggests that the scopeof the reference is:Resnik (1999) describes a method for mining the web forbilingual texts.4.1.3 Sentence FilteringThe task in this step is to detect and filter out unsuit-able sentences; i.e., sentences that depend on theircontext (e.g.
Sentence (5) above) or describe theown work of their authors, not the contribution ofthe target paper (e.g Sentence (4) above).
Formally,we classify the citation sentences into two classes:suitable and unsuitable sentences.
We use a ma-chine learning technique for this purpose.
We ex-tract a number of features from each sentence andtrain a classification model using these features.
Thetrained model is then used to classify the sentences.We use Support Vector Machines (SVM) with linearkernel as our classifier.
The features that we use inthis step and their descriptions are shown in Table 1.4.2 ExtractionIn the first stage, the sentences and sentence frag-ments that are not useful for our summarization taskare ruled out.
The input to this stage is a set of cita-tion sentences that are believed to be suitable for thesummary.
From these sentences, we need to selecta representative subset.
The sentences are selectedbased on these three main properties:First, they should cover diverse aspects of the pa-per.
Second, the sentences that cover the same as-pect should not contain redundant information.
Forexample, if two sentences talk about the drawbacksof the target paper, one sentence can mention thecomputation inefficiency, while the other criticizethe assumptions the paper makes.
Third, the sen-tences should cover as many important facts aboutthe target paper as possible using minimal text.In this stage, the summary sentences are selectedin three steps.
In the first step, the sentences are clas-sified into five functional categories: Background,Problem Statement, Method, Results, and Limita-tions.
In the second step, we cluster the sen-tences within each category into clusters of simi-lar sentences.
In the third step, we compute theLexRank (Erkan and Radev, 2004) values for thesentences within each cluster.
The summary sen-tences are selected based on the classification, theclustering, and the LexRank values.4.2.1 Functional Category ClassificationWe classify the citation sentences into the five cat-egories mentioned above using a machine learningtechnique.
A classification model is trained on anumber of features (Table 2) extracted from a la-beled set of citation sentences.
We use SVM withlinear kernel as our classifier.4.2.2 Sentence ClusteringIn the previous step we determined the categoryof each citation sentence.
It is very likely thatsentences from the same category contain similar oroverlapping information.
For example, Sentences(6), (7), and (8) below appear in the set of citation503Feature DescriptionSimilarity to the target paper The value of the cosine similarity (using TF-IDF vectors) between the citation sentence and the target paper.Headlines The section in which the citation sentence appeared in the citing paper.
We recognize 10 section types suchas Introduction, Related Work, Approach, etc.Relative position The relative position of the sentence in the section and the paragraph in which it appearsFirst person pronouns This feature takes a value of 1 if the sentence contains a first person pronoun (I, we, our, us, etc.
), and 0otherwise.Tense of the first verb A sentence that contains a past tense verb near its beginning is more likely to be describing previous work.Determiners Demonstrative Determiners (this, that, these, those, and which) and Alternative Determiners (another, other).The value of this feature is the relative position of the first determiner (if one exists) in the sentence.Table 1: The features used for sentence filteringFeature DescriptionSimilarity to the sections of the target paper The sections of the target paper are categorized into five categories: 1) Introduction, Moti-vation, Problem Statement.
2) Background, Prior Work, Previous Work, and Related Work.3) Experiments, Results, and Evaluation.
4) Discussion, Conclusion, and Future work.
5)All other headlines.
The value of this feature is the cosine similarity (using TF-IDF vectors)between the sentence and the text of the sections of each of the five section categories.Headlines This is the same feature that we used for sentence filtering in Section 4.1.3.Number of references in the sentence Sentences that contain multiple references are more likely to be Background sentences.Verbs We use all the verbs that their lemmatized form appears in at least three sentences that belongto the same category in the training set.
Auxiliary verbs are excluded.
In our annotated dataset,for example, the verb propose appeared in 67 sentences from the Methodology category, whilethe verbs outperform and achieve appeared in 33 Result sentences.Table 2: The features used for sentence classificationsentences that cite Goldwater and Griffiths?
(2007).These sentences belong to the same category (i.eMethod).
Both Sentences (6) and (7) convey thesame information about Goldwater and Griffiths(2007) contribution.
Sentence (8), however, de-scribes a different aspect of the paper methodology.
(6) Goldwater and Griffiths (2007) proposed aninformation-theoretic measure known as the Variation ofInformation (VI)(7) Goldwater and Griffiths (2007) propose using theVariation of Information (VI) metric(8) A fully-Bayesian approach to unsupervised POStagging has been developed by Goldwater and Griffiths(2007) as a viable alternative to the traditional maximumlikelihood-based HMM approach.Clustering divides the sentences of each cate-gory into groups of similar sentences.
FollowingQazvinian and Radev (2008), we build a cosine sim-ilarity graph out of the sentences of each category.This is an undirected graph in which nodes are sen-tences and edges represent similarity relations.
Eachedge is weighted by the value of the cosine similarity(using TF-IDF vectors) between the two sentencesthe edge connects.
Once we have the similarity net-work constructed, we partition it into clusters usinga community finding technique.
We use the Clausetalgorithm (Clauset et al, 2004), a hierarchical ag-glomerative community finding algorithm that runsin linear time.4.2.3 RankingAlthough the sentences that belong to the same clus-ter are similar, they are not necessarily equally im-portant.
We rank the sentences within each clus-ter by computing their LexRank (Erkan and Radev,2004).
Sentences with higher rank are more impor-tant.4.2.4 Sentence SelectionAt this point we have determined (Figure 2), for eachsentence, its category, its cluster, and its relative im-portance.
Sentences are added to the summary inorder based on their category, the size of their clus-ters, then their LexRank values.
The categories are504Figure 2: Example illustrating sentence selectionordered as Background, Problem, Method, Results,then Limitations.
Clusters within each category areordered by the number of sentences in them whereasthe sentences of each cluster are ordered by theirLexRank values.In the example shown in Figure 2, we have threecategories.
Each category contains several clusters.Each cluster contains several sentences with differ-ent LexRank values (illustrated by the sizes of thedots in the figure.)
If the desired length of the sum-mary is 3 sentences, the selected sentences will bein order S1, S12, then S18.
If the desired length is 5,the selected sentences will be S1, S5, S12, S15, thenS18.4.3 PostprocessingIn this stage, we refine the sentences that we ex-tracted in the previous stage.
Each citation sentencewill have the target reference (the author?s namesand the publication year) mentioned at least once.The reference could be either syntactically and se-mantically part of the sentence (e.g.
Sentence (3)above) or not (e.g.
Sentence (2)).
The aim of thisrefinement step is to avoid repeating the author?snames and the publication year in every sentence.We keep the author?s names and the publication yearonly in the first sentence of the summary.
In thefollowing sentences, we either replace the referencewith a suitable personal pronoun or remove it.
Thereference is replaced with a pronoun if it is part ofthe sentence and this replacement does not make thesentence ungrammatical.
The reference is removedif it is not part of the sentence.
If the sentence con-tains references for other papers, they are removed ifthis doesn?t hurt the grammaticality of the sentence.To determine whether a reference is part of thesentence or not, we again use a machine learningapproach.
We train a model on a set of labeled sen-tences.
The features used in this step are listed inTable 3.
The trained model is then used to classifythe references that appear in a sentence into threeclasses: keep, remove, replace.
If a reference is tobe replaced, and the paper has one author, we use?he/she?
(we do not know if the author is male orfemale).
If the paper has two or more authors, weuse ?they?.5 EvaluationWe provide three levels of evaluation.
First, we eval-uate each of the components in our system sepa-rately.
Then we evaluate the summaries that oursystem generate in terms of extraction quality.
Fi-nally, we evaluate the coherence and readability ofthe summaries.5.1 DataWe use the ACL Anthology Network (AAN) (Radevet al, 2009) in our evaluation.
AAN is a collectionof more than 16000 papers from the ComputationalLinguistics journal, and the proceedings of the ACLconferences and workshops.
AAN provides all cita-tion information from within the network includingthe citation network, the citation sentences, and thecitation context for each paper.We used 55 papers from AAN as our data.
Thepapers have a variable number of citation sentences,ranging from 15 to 348.
The total number of cita-tion sentences in the dataset is 4,335.
We split thedata randomly into two different sets; one for evalu-ating the components of the system, and the other forevaluating the extraction quality and the readabilityof the generated summaries.
The first set (dataset1,henceforth) contained 2,284 sentences coming from25 papers.
We asked humans with good backgroundin NLP (the area of the annotated papers) to providetwo annotations for each sentence in this set: 1) labelthe sentence as Background, Problem, Method, Re-sult, Limitation, or Unsuitable, 2) for each referencein the sentence, determine whether it could be re-placed with a pronoun, removed, or should be kept.505Feature DescriptionPart-of-speech (POS) tag We consider the POS tags of the reference, the word before, and the word after.
Before passing thesentence to the POS tagger, all the references in the sentence are replaced by placeholders.Style of the reference It is common practice in writing scientific papers to put the whole citation between parenthesiswhen the authors are not a constitutive part of the enclosing sentence, and to enclose just the yearbetween parenthesis when the author?s name is a syntactic constituent in the sentence.Relative position of the reference This feature takes one of three values: first, last, and inside.Grammaticality Grammaticality of the sentence if the reference is removed/replaced.
Again, we use the LinkGrammar parser (Sleator and Temperley, 1991) to check the grammaticalityTable 3: The features used for author name replacementEach sentence was given to 3 different annotators.We used the majority vote labels.We use Kappa coefficient (Krippendorff, 2003) tomeasure the inter-annotator agreement.
Kappa coef-ficient is defined as:Kappa =P (A)?
P (E)1?
P (E)(1)where P (A) is the relative observed agreementamong raters and P (E) is the hypothetical proba-bility of chance agreement.The agreement among the three annotators on dis-tinguishing the unsuitable sentences from the otherfive categories is 0.85.
On Landis and Kochs(1977)scale, this value indicates an almost perfect agree-ment.
The agreement on classifying the sentencesinto the five functional categories is 0.68.
On thesame scale this value indicates substantial agree-ment.The second set (dataset2, henceforth) contained30 papers (2051 sentences).
We asked humans witha good background in NLP (the papers topic) to gen-erate a readable, coherent summary for each paper inthe set using its citation sentences as the source text.We asked them to fix the length of the summariesto 5 sentences.
Each paper was assigned to two hu-mans to summarize.5.2 Component EvaluationReference Tagging and Reference Scope Iden-tification Evaluation: We ran our reference tag-ging and scope identification components on the2,284 sentences in dataset1.
Then, we went throughthe tagged sentences and the extracted scopes, andcounted the number of correctly/incorrectly tagged(extracted)/missed references (scopes).
Our tagging- Bkgrnd Prob Method Results Limit.Precision 64.62% 60.01% 88.66% 76.05% 33.53%Recall 72.47% 59.30% 75.03% 82.29% 59.36%F1 68.32% 59.65% 81.27% 79.04% 42.85%Table 4: Precision and recall results achieved by our cita-tion sentence classifiercomponent achieved 98.2% precision and 94.4% re-call.
The reference to the target paper was taggedcorrectly in all the sentences.Our scope identification component extracted thescope of target references with good precision(86.4%) but low recall (35.2%).
In fact, extractinga useful scope for a reference requires more thanjust finding a grammatical substring.
In future work,we plan to employ text regeneration techniques toimprove the recall by generating grammatical sen-tences from ungrammatical fragments.Sentence Filtering Evaluation: We used Sup-port Vector Machines (SVM) with linear kernel asour classifier.
We performed 10-fold cross validationon the labeled sentences (unsuitable vs all other cat-egories) in dataset1.
Our classifier achieved 80.3%accuracy.Sentence Classification Evaluation: We usedSVM in this step as well.
We also performed 10-fold cross validation on the labeled sentences (thefive functional categories).
This classifier achieved70.1% accuracy.
The precision and recall for eachcategory are given in Table 4Author Name Replacement Evaluation: Theclassifier used in this task is also SVM.
We per-formed 10-fold cross validation on the labeled sen-tences of dataset1.
Our classifier achieved 77.41%accuracy.506Produced using our systemThere has been a large number of studies in tagging and morphological disambiguation using various techniques such as statistical tech-niques, e.g.
constraint-based techniques and transformation-based techniques.
A thorough removal of ambiguity requires a syntacticprocess.
A rule-based tagger described in Voutilainen (1995) was equipped with a set of guessing rules that had been hand-crafted usingknowledge of English morphology and intuitions.
The precision of rule-based taggers may exceed that of the probabilistic ones.
Theconstruction of a linguistic rule-based tagger, however, has been considered a difficult and time-consuming task.Produced using Qazvinian and Radev (2008) systemAnother approach is the rule-based or constraint-based approach, recently most prominently exemplified by the Constraint Grammar work(Karlsson et al , 1995; Voutilainen, 1995b; Voutilainen et al , 1992; Voutilainen and Tapanainen, 1993), where a large number ofhand-crafted linguistic constraints are used to eliminate impossible tags or morphological parses for a given word in a given context.Some systems even perform the POS tagging as part of a syntactic analysis process (Voutilainen, 1995).
A rule-based tagger describedin (Voutilainen, 1995) is equipped with a set of guessing rules which has been hand-crafted using knowledge of English morphologyand intuition.
Older versions of EngCG (using about 1,150 constraints) are reported ( butilainen et al 1992; Voutilainen and HeikkiUi1994; Tapanainen and Voutilainen 1994; Voutilainen 1995) to assign a correct analysis to about 99.7% of all words while each word inthe output retains 1.04-1.09 alternative analyses on an average, i.e.
some of the ambiguities remait unresolved.
We evaluate the resultingdisambiguated text by a number of metrics defined as follows (Voutilainen, 1995a).Table 5: Sample Output5.3 Extraction EvaluationTo evaluate the extraction quality, we use dataset2(that has never been used for training or tuning anyof the system components).
We use our system togenerate summaries for each of the 30 papers indataset2.
We also generate summaries for the pa-pers using a number of baseline systems (describedin Section 5.3.1).
All the generated summaries were5 sentences long.
We use the Recall-Oriented Un-derstudy for Gisting Evaluation (ROUGE) based onthe longest common substrings (ROUGE-L) as ourevaluation metric.5.3.1 BaselinesWe evaluate the extraction quality of our system(FL) against 7 different baselines.
In the first base-line, the sentences are selected randomly from theset of citation sentences and added to the sum-mary.
The second baseline is the MEAD summa-rizer (Radev et al, 2004) with all its settings setto default.
The third baseline is LexRank (Erkanand Radev, 2004) run on the entire set of citationsentences of the target paper.
The forth baseline isQazvinian and Radev (2008) citation-based summa-rizer (QR08) in which the citation sentences are firstclustered then the sentences within each cluster areranked using LexRank.
The remaining baselines arevariations of our system produced by removing onecomponent from the pipeline at a time.
In one vari-ation (FL-1), we remove the sentence filtering com-ponent.
In another variation (FL-2), we remove thesentence classification component; so, all the sen-tences are assumed to come from one category in thesubsequent components.
In a third variation (FL-3),the clustering component is removed.
To make thecomparison of the extraction quality to those base-lines fair, we remove the author name replacementcomponent from our system and all its variations.5.3.2 ResultsTable 6 shows the average ROUGE-L scores (with95% confidence interval) for the summaries of the30 papers in dataset2 generated using our systemand the different baselines.
The two human sum-maries were used as models for comparison.
TheHuman score reported in the table is the result ofcomparing the two human summaries to each others.Statistical significance was tested using a 2-tailedpaired t-test.
The results are statistically significantat the 0.05 level.The results show that our approach outperformsall the baseline techniques.
It achieves higherROUGE-L score for most of the papers in our test-ing set.
Comparing the score of FL-1 to the scoreof FL shows that sentence filtering has a significantimpact on the results.
It also shows that the classifi-cation and clustering components both improve theextraction quality.5.4 Coherence and Readability EvaluationWe asked human judges (not including the authors)to rate the coherence and readability of a numberof summaries for each of dataset2 papers.
Foreach paper we evaluated 3 summaries.
The sum-507- Human Random MEAD LexRank QR08ROUGE-L 0.733 0.398 0.410 0.408 0.435- FL-1 FL-2 FL-3 FL -ROUGE-L 0.475 0.511 0.525 0.539 -Table 6: Extraction EvaluationAverage Coherence RatingNumber of summariesHuman FL QV081?
coherence <2 0 9 172?
coherence <3 3 11 123?
coherence <4 16 9 14?
coherence ?5 11 1 0Table 7: Coherence Evaluationmary that our system produced, the human sum-mary, and a summary produced by Qazvinian andRadev (2008) summarizer (the best baseline - afterour system and its variations - in terms of extrac-tion quality as shown in the previous subsection.
)The summaries were randomized and given to thejudges without telling them how each summary wasproduced.
The judges were not given access to thesource text.
They were asked to use a five point-scale to rate how coherent and readable the sum-maries are, where 1 means that the summary is to-tally incoherent and needs significant modificationsto improve its readability, and 5 means that the sum-mary is coherent and no modifications are needed toimprove its readability.
We gave each summary to 5different judges and took the average of their ratingsfor each summary.
We used Weighted Kappa withlinear weights (Cohen, 1968) to measure the inter-rater agreement.
The Weighted Kappa measure be-tween the five groups of ratings was 0.72.Table 7 shows the number of summaries in eachrating range.
The results show that our approach sig-nificantly improves the coherence of citation-basedsummarization.
Table 5 shows two sample sum-maries (each 5 sentences long) for the Voutilainen(1995) paper.
One summary was produced using oursystem and the other was produced using Qazvinianand Radev (2008) system.6 ConclusionsIn this paper, we presented a new approach forcitation-based summarization of scientific papersthat produces readable summaries.
Our approach in-volves three stages.
The first stage preprocesses theset of citation sentences to filter out the irrelevantsentences or fragments of sentences.
In the secondstage, a representative set of sentences are extractedand added to the summary in a reasonable order.
Inthe last stage, the summary sentences are refined toimprove their readability.
The results of our exper-iments confirmed that our system outperforms sev-eral baseline systems.AcknowledgmentsThis work is in part supported by the NationalScience Foundation grant ?iOPENER: A FlexibleFramework to Support Rapid Learning in Unfamil-iar Research Domains?, jointly awarded to Univer-sity of Michigan and University of Maryland asIIS 0705832, and in part by the NIH Grant U54DA021519 to the National Center for IntegrativeBiomedical Informatics.Any opinions, findings, and conclusions or rec-ommendations expressed in this paper are those ofthe authors and do not necessarily reflect the viewsof the supporters.ReferencesAaron Clauset, M. E. J. Newman, and Cristopher Moore.2004.
Finding community structure in very large net-works.
Phys.
Rev.
E, 70(6):066111, Dec.Jacob Cohen.
1968.
Weighted kappa: Nominal scaleagreement provision for scaled disagreement or partialcredit.
Psychological Bulletin, 70(4):213 ?
220.Aaron Elkiss, Siwei Shen, Anthony Fader, Gu?nes?
Erkan,David States, and Dragomir Radev.
2008.
Blind menand elephants: What do citation summaries tell usabout a research article?
J.
Am.
Soc.
Inf.
Sci.
Tech-nol., 59(1):51?62.Gunes Erkan and Dragomir R. Radev.
2004.
Lexrank:graph-based lexical centrality as salience in text sum-marization.
J. Artif.
Int.
Res., 22(1):457?479.E.
Garfield, Irving H. Sher, and R. J. Torpie.
1984.
TheUse of Citation Data in Writing the History of Science.Institute for Scientific Information Inc., Philadelphia,Pennsylvania, USA.T.
L. Hodges.
1972.
Citation indexing-its theoryand application in science, technology, and humani-ties.
Ph.D. thesis, University of California at Berke-ley.Ph.D.
thesis, University of California at Berkeley.508Klaus H. Krippendorff.
2003.
Content Analysis: An In-troduction to Its Methodology.
Sage Publications, Inc,2nd edition, December.J.
Richard Landis and Gary G. Koch.
1977.
The Mea-surement of Observer Agreement for Categorical Data.Biometrics, 33(1):159?174, March.Qiaozhu Mei and ChengXiang Zhai.
2008.
Generatingimpact-based summaries for scientific literature.
InProceedings of ACL-08: HLT, pages 816?824, Colum-bus, Ohio, June.
Association for Computational Lin-guistics.Saif Mohammad, Bonnie Dorr, Melissa Egan, AhmedHassan, Pradeep Muthukrishan, Vahed Qazvinian,Dragomir Radev, and David Zajic.
2009.
Using ci-tations to generate surveys of scientific paradigms.
InProceedings of Human Language Technologies: The2009 Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 584?592, Boulder, Colorado, June.
Associationfor Computational Linguistics.Hidetsugu Nanba and Manabu Okumura.
1999.
To-wards multi-paper summarization using reference in-formation.
In IJCAI ?99: Proceedings of the Six-teenth International Joint Conference on Artificial In-telligence, pages 926?931, San Francisco, CA, USA.Morgan Kaufmann Publishers Inc.Hidetsugu Nanba, Noriko Kando, Manabu Okumura, andOf Information Science.
2000.
Classification of re-search papers using citation links and citation types:Towards automatic review article generation.M.
E. J. Newman.
2001.
The structure of scientificcollaboration networks.
Proceedings of the NationalAcademy of Sciences of the United States of America,98(2):404?409, January.Vahed Qazvinian and Dragomir R. Radev.
2008.
Scien-tific paper summarization using citation summary net-works.
In Proceedings of the 22nd International Con-ference on Computational Linguistics (Coling 2008),pages 689?696, Manchester, UK, August.Vahed Qazvinian and Dragomir R. Radev.
2010.
Identi-fying non-explicit citing sentences for citation-basedsummarization.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Linguis-tics, pages 555?564, Uppsala, Sweden, July.
Associa-tion for Computational Linguistics.Vahed Qazvinian, Dragomir R. Radev, and ArzucanOzgur.
2010.
Citation summarization throughkeyphrase extraction.
In Proceedings of the 23rd In-ternational Conference on Computational Linguistics(Coling 2010), pages 895?903, Beijing, China, Au-gust.
Coling 2010 Organizing Committee.Dragomir Radev, Timothy Allison, Sasha Blair-Goldensohn, John Blitzer, Arda C?elebi, StankoDimitrov, Elliott Drabek, Ali Hakim, Wai Lam,Danyu Liu, Jahna Otterbacher, Hong Qi, HoracioSaggion, Simone Teufel, Michael Topper, AdamWinkel, and Zhu Zhang.
2004.
MEAD - a platformfor multidocument multilingual text summarization.In LREC 2004, Lisbon, Portugal, May.Dragomir R. Radev, Pradeep Muthukrishnan, and VahedQazvinian.
2009.
The acl anthology network corpus.In NLPIR4DL ?09: Proceedings of the 2009 Workshopon Text and Citation Analysis for Scholarly Digital Li-braries, pages 54?61, Morristown, NJ, USA.
Associa-tion for Computational Linguistics.Advaith Siddharthan and Simone Teufel.
2007.
Whoseidea was this, and why does it matter?
attributingscientific work to citations.
In In Proceedings ofNAACL/HLT-07.Daniel D. K. Sleator and Davy Temperley.
1991.
Parsingenglish with a link grammar.
In In Third InternationalWorkshop on Parsing Technologies.Simone Teufel, Advaith Siddharthan, and Dan Tidhar.2006.
Automatic classification of citation function.
InIn Proc.
of EMNLP-06.Simone Teufel.
2007.
Argumentative zoning for im-proved citation indexing.
computing attitude and affectin text.
In Theory and Applications, pages 159170.509
