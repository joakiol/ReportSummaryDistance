Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 351?358,New York, June 2006. c?2006 Association for Computational LinguisticsA Better -Best List: Practical Determinization of Weighted Finite TreeAutomataJonathan MayInformation Sciences InstituteUniversity of Southern CaliforniaMarina del Rey, CA 90292jonmay@isi.eduKevin KnightInformation Sciences InstituteUniversity of Southern CaliforniaMarina del Rey, CA 90292knight@isi.eduAbstractRanked lists of output trees from syn-tactic statistical NLP applications fre-quently contain multiple repeated entries.This redundancy leads to misrepresenta-tion of tree weight and reduced informa-tion for debugging and tuning purposes.It is chiefly due to nondeterminism in theweighted automata that produce the re-sults.
We introduce an algorithm that de-terminizes such automata while preserv-ing proper weights, returning the sum ofthe weight of all multiply derived trees.We also demonstrate our algorithm?s ef-fectiveness on two large-scale tasks.1 IntroductionA useful tool in natural language processing taskssuch as translation, speech recognition, parsing, etc.,is the ranked list of results.
Modern systems typ-ically produce competing partial results internallyand return only the top-scoring complete result tothe user.
They are, however, also capable of pro-ducing lists of runners-up, and such lists have manypractical uses:The lists may be inspected to determinethe quality of runners-up and motivatemodel changes.The lists may be re-ranked with extraknowledge sources that are difficult to ap-ply during the main search.The lists may be used with annotation anda tuning process, such as in (Collins andRoark, 2004), to iteratively alter featureweights and improve results.Figure 1 shows the best 10 English translationparse trees obtained from a syntax-based translationsystem based on (Galley, et.
al., 2004).
Noticethat the same tree occurs multiple times in this list.This repetition is quite characteristic of the output ofranked lists.
It occurs because many systems, suchas the ones proposed by (Bod, 1992), (Galley, et.
al.,2004), and (Langkilde and Knight, 1998) representtheir result space in terms of weighted partial resultsof various sizes that may be assembled in multipleways.
There is in general more than one way to as-semble the partial results to derive the same com-plete result.
Thus, the -best list of results is reallyan -best list of derivations.When list-based tasks, such as the ones mentionedabove, take as input the top results for some con-stant , the effect of repetition on these tasks is dele-terious.
A list with many repetitions suffers from alack of useful information, hampering diagnostics.Repeated results prevent alternatives that would behighly ranked in a secondary reranking system fromeven being considered.
And a list of fewer uniquetrees than expected can cause overfitting when thislist is used to tune.
Furthermore, the actual weight ofobtaining any particular tree is split among its repeti-tions, distorting the actual relative weights betweentrees.
(Mohri, 1997) encountered this problem in speechrecognition, and presented a solution to the prob-lem of repetition in -best lists of strings that arederived from finite-state automata.
That work de-scribed a way to use a powerset construction along35134.73: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBD(had) VP-C(VBN(caused) NP-C(NPB(DT(the) JJ(american) NNS(protests))))) .(.
))34.74: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBD(had) VP-C(VBN(aroused) NP-C(NPB(DT(the) JJ(american) NNS(protests))))) .(.
))34.83: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBD(had) VP-C(VBN(caused) NP-C(NPB(DT(the) JJ(american) NNS(protests))))) .(.
))34.83: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBD(had) VP-C(VBN(aroused) NP-C(NPB(DT(the) JJ(american) NNS(protests))))) .(.
))34.84: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBD(had) VP-C(VBN(caused) NP-C(NPB(DT(the) JJ(american) NNS(protests))))) .(.
))34.85: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBD(had) VP-C(VBN(caused) NP-C(NPB(DT(the) JJ(american) NNS(protests))))) .(.
))34.85: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBD(had) VP-C(VBN(aroused) NP-C(NPB(DT(the) JJ(american) NNS(protests))))) .(.
))34.85: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBD(had) VP-C(VBN(aroused) NP-C(NPB(DT(the) JJ(american) NNS(protests))))) .(.
))34.87: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBD(had) VP-C(VB(arouse) NP-C(NPB(DT(the) JJ(american) NNS(protests))))) .(.
))34.92: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBD(had) VP-C(VBN(aroused) NP-C(NPB(DT(the) JJ(american) NNS(protests))))) .(.
))Figure 1: Ranked list of machine translation results with repeated trees.
Scores shown are negative logs ofcalculated weights, thus a lower score indicates a higher weight.
The bulleted sentences indicate identicaltrees.with an innovative bookkeeping system to deter-minize the automaton, resulting in an automaton thatpreserves the language but provides a single, prop-erly weighted derivation for each string in it.
Put an-other way, if the input automaton has the ability togenerate the same string with different weights, theoutput automaton generates that string with weightequal to the sum of all of the generations of thatstring in the input automaton.
In (Mohri and Riley,2002) this technique was combined with a procedurefor efficiently obtaining -best ranked lists, yieldinga list of string results with no repetition.In this paper we extend that work to deal withgrammars that produce trees.
Regular tree gram-mars (Brainerd, 1969), which subsume the tree sub-stitution grammars developed in the NLP commu-nity (Schabes, 1990), are of particular interest tothose wishing to work with additional levels ofstructure that string grammars cannot provide.
Theapplication to parsing is natural, and in machinetranslation tree grammars can be used to modelsyntactic transfer, control of function words, re-ordering, and target-language well-formedness.
Inthe world of automata these grammars have as a nat-ural dual the finite tree recognizer (Doner, 1970).Like tree grammars and packed forests, they arecompact ways of representing very large sets oftrees.
We will present an algorithm for determiniz-ing weighted finite tree recognizers, and use a vari-ant of the procedure found in (Huang and Chiang,2005) to obtain -best lists of trees that are weightedcorrectly and contain no repetition.Section 2 describes related work.
In Section 3, weintroduce the formalisms of tree automata, specifi-cally the tree-to-weight transducer.
In Section 4, wepresent the algorithm.
Finally, in Section 5 we showthe results of applying weighted determinization torecognizers obtained from the packed forest outputof two natural language tasks.2 Previous WorkThe formalisms of tree automata are summarizedwell in (Gecseg and Steinby, 1984).
Bottom-uptree recognizers are due to (Thatcher and Wright,1968), (Doner, 1970), and (Magidor and Moran,1969).
Top-down tree recognizers are due to (Rabin,1969) and (Magidor and Moran, 1969).
(Comon, et.al., 1997) show the determinization of unweightedfinite-state tree automata, and prove its correctness.
(Borchardt and Vogler, 2003) present determiniza-tion of weighted finite-state tree automata with a dif-ferent method than the one we present here.
Whileour method is applicable to finite tree sets, the previ-ous method claims the ability to determinize someclasses of infinite tree sets.
However, for the fi-nite case the previous method produces an automa-ton with size on the order of the number of deriva-tions, so the technique is limited when applied toreal world data.3 Grammars, Recognizers, andTransducersAs described in (Gecseg and Steinby, 1984), tree au-tomata may be broken into two classes, recognizersand transducers.
Recognizers read tree input and de-cide whether the input is in the language representedby the recognizer.
Formally, a bottom-up tree recog-nizer is defined by :1is a finite set of states,1Readers familiar with (Gecseg and Steinby, 1984) will no-tice that we have introduced a start state, modified the notion ofinitial assignment, and changed the arity of nullary symbols tounary symbols.
This is to make tree automata more palatable tothose accustomed to string automata and to allow for a usefulgraphical interpretation.352Figure 2: Visualization of a bottom-up tree recog-nizeris a ranked alphabet,is the initial state,is a set of final states, andis a finite setof transitions from a vector of states toone state that reads a -ary symbol.Consider the following tree recognizer:2As with string automata, it is helpful to have a vi-sualization to understand what the recognizer is rec-ognizing.
Figure 2 provides a visualization of therecognizer above.
Notice that some members ofare drawn as arcs with multiple (and ordered) tails.This is the key difference in visualization betweenstring and tree automata ?
to capture the arity of thesymbol being read we must visualize the automataas an ordered hypergraph.The function of the members of in the hyper-graph visualization leads us to refer to the vector ofstates as an input vector of states, and the singlestate as an output state.
We will refer to as thetransition set of the recognizer.In string automata, a path through a recognizerconsists of a sequence of edges that can be followedfrom a start to an end state.
The concatenation of la-bels of the edges of a path, typically in a left-to-rightorder, forms a string in the recognizer?s language.In tree automata, however, a hyperpath through arecognizer consists of a sequence of hyperedges thatcan be followed, sometimes in parallel, from a start2The number denotes the arity of the symbol.Figure 3: Bottom-up tree-to-weight transducerto an end state.
We arrange the labels of the hy-peredges to form a tree in the recognizer?s languagebut must now consider proper order in two dimen-sions.
The proper vertical order is specified by theorder of application of transitions, i.e., the labels oftransitions followed earlier are placed lower in thetree than the labels of transitions followed later.
Theproper horizontal order within one level of the tree isspecified by the order of states in a transition?s inputvector.
In the example recognizer, the treesand are valid.
Notice that may berecognized in two different hyperpaths.Like tree recognizers, tree transducers read treeinput and decide whether the input is in the lan-guage, but they simultaneously produce some out-put as well.
Since we wish to associate a weightwith every acceptable tree in a language, we willconsider transducers that produce weights as theiroutput.
Note that in transitioning from recognizersto transducers we are following the convention es-tablished in (Mohri, 1997) where a transducer withweight outputs is used to represent a weighted rec-ognizer.
One may consider the determinization oftree-to-weight transducers as equivalent to the de-terminization of weighted tree recognizers.Formally, a bottom-up tree-to-weight transduceris defined by where ,, , and are defined as for recognizers, and:is afinite set of transitions from a vector ofstates to one state, reading a -ary symboland outputting some weightis the initial weight function mappingtois the final weight function mapping353to .We must also specify a convention for propagat-ing the weight calculated in every transition.
Thiscan be explicitly defined for each transition but wewill simplify matters by defining the propagation ofthe weight to a destination state as the multiplicationof the weight at each source state with the weight ofthe production.We modify the previous example by addingweights as follows: As an example, consider the fol-lowing tree-to-weight transducer ( , , , and areas before):Figure 3 shows the addition of weights onto theautomata, forming the above transducer.
Notice thetree yields the weight 0.036 (), and yields the weight 0.012 () or 0.054 ( ), depending onthe hyperpath followed.This transducer is an example of a nonsubsequen-tial transducer.
A tree transducer is subsequential iffor each vector v of states and each thereis at most one transition in with input vector v andlabel .
These restrictions ensure a subsequentialtransducer yields a single output for each possibleinput, that is, it is deterministic in its output.Because we will reason about the destination stateof a transducer transition and the weight of a trans-ducer transition separately, we make the followingdefinition.
For a given v wherev is a vector of states, , , and, let v and v .
Equiva-lent shorthand forms are and .4 DeterminizationThe determinization algorithm is presented as Algo-rithm 1.
It takes as input a bottom-up tree-to-weighttransducer and returns as output a subsequentialbottom-up tree-to-weight transducer such that thetree language recognized by is equivalent to thatof and the output weight given input tree on isequal to the sum of all possible output weights givenon .
Like the algorithm of (Mohri, 1997), thisFigure 4: a) Portion of a transducer before deter-minization; b) The same portion after determiniza-tionalgorithm will terminate for automata that recognizefinite tree languages.
It may terminate on some au-tomata that recognize infinite tree languages, but wedo not consider any of these cases in this work.Determinizing a tree-to-weight transducer can bethought of as a two-stage process.
First, the structureof the automata must be determined such that a sin-gle hyperpath exists for each recognized input tree.This is achieved by a classic powerset construction,i.e., a state must be constructed in the output trans-ducer that represents all the possible reachable desti-nation states given an input and a label.
Because weare working with tree automata, our input is a vectorof states, not a single state.
A comparable power-set construction on unweighted tree automata and aproof of correctness can be found in (Comon, et.
al.,1997).The second consideration to weighted deter-minization is proper propagation of weights.
For thiswe will use (Mohri, 1997)?s concept of the residualweight.
We represent in the construction of statesin the output transducer not only a subset of statesof the input transducer, but also a number associatedwith each of these states, called the residual.
Sincewe want ?s hyperpath of a particular input tree tohave as its associated weight the sum of the weightsof the all of ?s hyperpaths of the input tree, we re-place a set of hyperedges in that have the sameinput state vector and label with a single hyperedgein bearing the label and the sum of ?s hyper-edge weights.
The destination state of the hyper-edge represents the states reachable by ?s applica-ble hyperedges and for each state, the proportion ofthe weight from the relevant transition.Figure 4 shows the determinization of a portionof the example transducer.
Note that the hyperedge354Figure 5: Determinized bottom-up tree-to-weighttransducerleading to state in the input transducer contributesof the weight on the output transducer hyperedgeand the hyperedge leading to state in the inputtransducer contributes the remaining .
This is re-flected in the state construction in the output trans-ducer.
The complete determinization of the exampletransducer is shown in Figure 5.To encapsulate the representation of states fromthe input transducer and associated residual weights,we define a state in the output transducer as a set oftuples, where and .
Sincethe algorithm builds new states progressively, wewill need to represent a vector of states from theoutput transducer, typically depicted as v. We mayconstruct the vector pair q w from v, where q isa vector of states of the input transducer and w isa vector of residual weights, by choosing a (state,weight) pair from each output state in v. For ex-ample, let .
Then two possible out-put transducer states could be and.
If we choose v then avalid vector pair q w is q , w .The sets v , v , and v are definedas follows:v q w from vq .v q w from vq .v q w from vq ..v is the set of vector pairs q w con-structed from v where each q is an input vector ina transition with label .
v is the set ofunique transitions paired with the appropriate pairfor each q w in v .
v is the set of statesreachable from the transitions in v .The consideration of vectors of states on the in-cident edge of transitions effects two noticeablechanges on the algorithm as it is presented in(Mohri, 1997).
The first, relatively trivial, changeis the inclusion of the residual of multiple states inthe calculation of weights and residuals on lines 16and 17.
The second change is the production ofvectors for consideration.
Whereas the string-basedalgorithm considered newly-created states in turn,we must consider newly-available vectors.
For eachnewly created state, newly available vectors can beformed by using that state with the other states ofthe output transducer.
This operation is performedon lines 7 and 22 of the algorithm.5 Empirical StudiesWe now turn to some empirical studies.
We examinethe practical impact of the presented work by show-ing:That the multiple derivation problem ispervasive in practice and determinizationis effective at removing duplicate trees.That duplication causes misleadingweighting of individual trees and thesumming achieved from weighted deter-minization corrects this error, leading tore-ordering of the -best list.That weighted determinization positivelyaffects end-to-end system performance.We also compare our results to a commonly usedtechnique for estimation of -best lists, i.e., sum-ming over the top derivations to get weightestimates of the top unique elements.5.1 Machine translationWe obtain packed-forest English outputs from 116short Chinese sentences computed by a string-to-tree machine translation system based on (Galley,et.
al., 2004).
The system is trained on all Chinese-English parallel data available from the LinguisticData Consortium.
The decoder for this system is aCKY algorithm that negotiates the space describedin (DeNeefe, et.
al., 2005).
No language model wasused in this experiment.The forests contain a median of En-glish parse trees each.
We remove cycles from each355Algorithm 1: Weighted Determinization of Tree AutomataInput: BOTTOM-UP TREE-TO-WEIGHT TRANSDUCER .Output: SUBSEQUENTIAL BOTTOM-UP TREE-TO-WEIGHT TRANSDUCER .begin123PRIORITY QUEUE456ENQUEUE7while do8v head9v10for each v such that do11if such that then12s.t.1314for each such that v do15vv16vvvv s.t.17v v v18/* RANK returns the largest hyperedge size that can leave state .COMBINATIONS returns all possible vectors of lengthcontaining members of and at least one member of .
*/if v is a new state then19for each u COMBINATIONS vvRANK do20if u is a new vector then21ENQUEUE u22v23DEQUEUE24end25forest,3 apply our determinization algorithm, and ex-tract the -best trees using a variant of (Huang andChiang, 2005).
The effects of weighted determiniza-tion on an -best list are obvious to casual inspec-tion.
Figure 7 shows the improvement in quality ofthe top 10 trees from our example translation afterthe application of the determinization algorithm.The improvement observed circumstantiallyholds up to quantitative analysis as well.
Theforests obtained by the determinized grammars havebetween 1.39% and 50% of the number of trees oftheir undeterminized counterparts.
On average, thedeterminized forests contain 13.7% of the original3As in (Mohri, 1997), determinization may be applicable tosome automata that recognize infinite languages.
In practice,cycles in tree automata of MT results are almost never desired,since these represent recursive insertion of words.number of trees.
Since a determinized forest con-tains no repeated trees but contains exactly the sameunique trees as its undeterminized counterpart, thisindicates that an average of 86.3% of the trees in anundeterminized MT output forest are duplicates.Weighted determinization also causes a surpris-ingly large amount of -best reordering.
In 77.6%of the translations, the tree regarded as ?best?
isdifferent after determinization.
This means that ina large majority of cases, the tree with the high-est weight is not recognized as such in the undeter-minized list because its weight is divided among itsmultiple derivations.
Determinization allows theseinstances and their associated weights to combineand puts the highest weighted tree, not the highestweighted derivation, at the top of the list.356method Bleuundeterminized 21.87top-500 ?crunching?
23.33determinized 24.17Figure 6: Bleu results from string-to-tree machinetranslation of 116 short Chinese sentences with nolanguage model.
The use of best derivation (unde-terminized), estimate of best tree (top-500), and truebest tree (determinized) for selection of translationis shown.We can compare our method with the more com-monly used methods of ?crunching?
-best lists,where .
The duplicate sentences in thetrees are combined, hopefully resulting in at leastunique members with an estimation of the truetree weight for each unique tree.
Our results indi-cate this is a rather crude estimation.
When the top500 derivations of the translations of our test cor-pus are summed, only 50.6% of them yield an esti-mated highest-weighted tree that is the same as thetrue highest-weighted tree.As a measure of the effect weighted determiniza-tion and its consequential re-ordering has on an ac-tual end-to-end evaluation, we obtain Bleu scoresfor our 1-best translations from determinization, andcompare them with the 1-best translations from theundeterminized forest and the 1-best translationsfrom the top-500 ?crunching?
method.
The resultsare tabulated in Figure 6.
Note that in 26.7% ofcases determinization did not terminate in a reason-able amount of time.
For these sentences we usedthe best parse from top-500 estimation instead.
It isnot surprising that determinization may occasionallytake a long time; even for a language of monadictrees (i.e.
strings) the determinization algorithm isNP-complete, as implied by (Casacuberta and de laHiguera, 2000) and, e.g.
(Dijkstra, 1959).5.2 Data-Oriented ParsingWeighted determinization of tree automata is alsouseful for parsing.
Data-Oriented Parsing (DOP)?smethodology is to calculate weighted derivations,but as noted in (Bod, 2003), it is the highest rankingparse, not derivation, that is desired.
Since (Sima?an,1996) showed that finding the highest ranking parseis an NP-complete problem, it has been common toestimate the highest ranking parse by the previouslymethod Recall Precision F-measureundeterminized 80.23 80.18 80.20top-500 ?crunching?
80.48 80.29 80.39determinized 81.09 79.72 80.40Figure 8: Recall, precision, and F-measure resultson DOP-style parsing of section 23 of the Penn Tree-bank.
The use of best derivation (undeterminized),estimate of best tree (top-500), and true best tree (de-terminized) for selection of parse output is shown.described ?crunching?
method.We create a DOP-like parsing model4 by extract-ing and weighting a subset of subtrees from sec-tions 2-21 of the Penn Treebank and use a DOP-style parser to generate packed forest representa-tions of parses of the 2416 sentences of section 23.The forests contain a median of parsetrees.
We then remove cycles and apply weighteddeterminization to the forests.
The number of treesin each determinized parse forest is reduced by afactor of between 2.1 and .
On aver-age, the number of trees is reduced by a factor of900,000, demonstrating a much larger number of du-plicate parses prior to determinization than in themachine translation experiment.
The top-scoringparse after determinization is different from the top-scoring parse before determinization for 49.1% ofthe forests, and when the determinization methodis ?approximated?
by crunching the top-500 parsesfrom the undeterminized list only 55.9% of the top-scoring parses are the same, indicating the crunch-ing method is not a very good approximation ofdeterminization.
We use the standard F-measurecombination of recall and precision to score thetop-scoring parse in each method against referenceparses.
The results are tabulated in Figure 8.
Notethat in 16.9% of cases determinization did not ter-minate.
For those sentences we used the best parsefrom top-500 estimation instead.6 ConclusionWe have shown that weighted determinization isuseful for recovering -best unique trees from aweighted forest.
As summarized in Figure 9, the4This parser acquires a small subset of subtrees, in contrastwith DOP, and the beam search for this problem has not beenoptimized.35731.87: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBD(had) VP-C(VBN(aroused) NP-C(NPB(DT(the) JJ(american) NNS(protests))))) .(.
))32.11: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBD(had) VP-C(VBN(caused) NP-C(NPB(DT(the) JJ(american) NNS(protests))))) .(.
))32.15: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBD(had) VP-C(VB(arouse) NP-C(NPB(DT(the) JJ(american) NNS(protests))))) .(.
))32.55: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBD(had) VP-C(VB(cause) NP-C(NPB(DT(the) JJ(american) NNS(protests))))) .(.
))32.60: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBD(had) VP-C(VBN(attracted) NP-C(NPB(DT(the) JJ(american) NNS(protests))))) .(.
))33.16: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBD(had) VP-C(VB(provoke) NP-C(NPB(DT(the) JJ(american) NNS(protests))))) .(.
))33.27: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBG(causing) NP-C(NPB(DT(the) JJ(american) NNS(protests)))) .(.
))33.29: S(NP-C(NPB(DT(this) NN(case))) VP(VBD(had) VP-C(VBN(aroused) NP-C(NPB(DT(the) JJ(american) NNS(protests))))) .(.
))33.31: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBD(had) VP-C(VBN(aroused) NP-C(NPB(DT(the) NN(protest)) PP(IN(of) NP-C(NPB(DT(the)NNS(united states))))))) .(.
))33.33: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBD(had) VP-C(VBN(incurred) NP-C(NPB(DT(the) JJ(american) NNS(protests))))) .(.
))Figure 7: Ranked list of machine translation results with no repeated trees.experiment undeterminized determinizedmachine translationparsingFigure 9: Median trees per sentence forest in ma-chine translation and parsing experiments before andafter determinization is applied to the forests, re-moving duplicate trees.number of repeated trees prior to determinizationwas typically very large, and thus determinization iscritical to recovering true tree weight.
We have im-proved evaluation scores by incorporating the pre-sented algorithm into our MT work and we believethat other NLP researchers working with trees cansimilarly benefit from this algorithm.Further advances in determinization will provideadditional benefit to the community.
The transla-tion system detailed here is a string-to-tree system,and the determinization algorithm returns the -bestunique trees from a packed forest.
Users of MT sys-tems are generally interested in the string yield ofthose trees, and not the trees per se.
Thus, an algo-rithm that can return the -best unique strings froma packed forest would be a useful extension.We plan for our weighted determinization algo-rithm to be one component in a generally availabletree automata package for intersection, composition,training, recognition, and generation of weightedand unweighted tree automata for research taskssuch as the ones described above.AcknowledgmentsWe thank Liang Huang for fruitful discussionswhich aided in this work and David Chiang, DanielMarcu, and Steve DeNeefe for reading an early draftand providing useful comments.
This work was sup-ported by NSF grant IIS-0428020.ReferencesRens Bod.
1992.
A Computational model of language perfor-mance: data oriented parsing.
In Proc.
COLINGRens Bod.
2003.
An efficient implementation of a new DOPmodel.
In Proc.
EACL,Bjo?rn Borchardt and Heiko Vogler.
2003.
Determinization offinite state weighted tree automata.
Journal of Automata,Languages and Combinatorics, 8(3).W.
S. Brainerd.
1969.
Tree generating regular systems.
Infor-mation and Control, 14.F.
Casacuberta and C. de la Higuera.
2000.
Computa-tional complexity of problems on probabilistic grammarsand transducers.
In Proc.
ICGI.Michael Collins and Brian Roark.
2004.
Incremental parsingwith the perceptron algorithm.
In Proc.
ACL.H.
Comon and M. Dauchet and R. Gilleron and F. Jacquemardand D. Lugiez and S. Tison and M. Tommasi.
1997 TreeAutomata Techniques and Applications.S.
DeNeefe and K. Knight and H. Chan.
2005.
Interactivelyexploring a machine translation model.
Poster in Proc.
ACL.Edsger W. Dijkstra 1959.
A note on two problems in connexionwith graphs Numerische Mathematik, 1.J.
E. Doner 1970.
Tree acceptors and some of their applicationsJ.
Comput.
System Sci., 4.M.
Galley and M. Hopkins and K. Knight and D. Marcu.
2004.What?s in a translation rule?
In Proc.
HLT-NAACL.Ferenc Ge?cseg and Magnus Steinby 1984.
Tree Automata.Akade?miai Kiado?, Budapest.Liang Huang and David Chiang 2005.
Better k-best parsing InProc.
IWPT.Irene Langkilde and Kevin Knight 1998 The Practical Value ofN-Grams in Generation In Proc.
INLG.M.
Magidor and G. Moran.
1969.
Finite automata over finitetrees Technical Report 30.
Hebrew University, Jerusalem.Mehryar Mohri.
1997.
Finite-state transducers in language andspeech processing.
Computational Linguistics, 23(2).Mehryar Mohri and Michael Riley.
2002.
An efficient algo-rithm for the -best strings problem.
In Proc.
ICSLP.M.
O. Rabin.
1969.
Decidability of second-order theories andautomata on infinite trees.
Trans.
Amer.
Math.
Soc., 141.Yves Schabes.
1990.
Mathematical and computational aspectsof lexicalized grammars.
Ph.D. thesis.
University of Penn-sylvania, Philadelphia, PA.Khalil Sima?an.
1996.
Computational complexity of proba-bilistic disambiguation by means of tree-grammars.
In Proc.COLING.J.
W. Thatcher and J.
B. Wright.
1968.
Generalized finite au-tomata theory with an application to a decision problem ofsecond order logic.
Mathematical Systems Theory, 2.358
