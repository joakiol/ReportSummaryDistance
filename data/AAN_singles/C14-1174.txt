Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 1844?1853, Dublin, Ireland, August 23-29 2014.Uncertainty Detection in Hungarian TextsVeronika Vincze1,21University of SzegedDepartment of Informatics2MTA-SZTE Research Group on Artificial Intelligencevinczev@inf.u-szeged.huAbstractUncertainty detection is essential for many NLP applications.
For instance, in information re-trieval, it is of primary importance to distinguish among factual, negated and uncertain informa-tion.
Current research on uncertainty detection has mostly focused on the English language, incontrast, here we present the first machine learning algorithm that aims at identifying linguisticmarkers of uncertainty in Hungarian texts from two domains: Wikipedia and news media.
Thesystem is based on sequence labeling and makes use of a rich feature set including orthographic,lexical, morphological, syntactic and semantic features as well.
Having access to annotated datafrom two domains, we also focus on the domain specificities of uncertainty detection by compar-ing results obtained in indomain and cross-domain settings.
Our results show that the domain ofthe text has significant influence on uncertainty detection.1 IntroductionUncertainty detection has become one of the most intensively studied problems of natural language pro-cessing (NLP) in these days (Morante and Sporleder, 2012).
For several NLP applications, it is essentialto distinguish between factual and nonfactual, i.e.
negated or uncertain information: for instance, inmedical information retrieval, it must be known whether the patient definitely suffers, probably suffersor does not suffer from an illness.
This type of information can only be revealed from the texts of thedocuments if reliable uncertainty detectors are available, which are able to identify linguistic markersof uncertainty, i.e.
cues within the text.
To the best of our knowledge, uncertainty detectors have beenmostly developed for the English language (Morante and Sporleder, 2012; Farkas et al., 2010).
Here,we present our machine learning based uncertainty detector developed for Hungarian, a morphologicallyrich language, and report our results on a manually annotated uncertainty corpus, which contains textsfrom two domains: first, Hungarian Wikipedia texts and second, pieces of news from a Hungarian newsportal.The main contributions of this paper are the following:?
it presents the first uncertainty corpus for Hungarian;?
it reports the first results on uncertainty detection in Hungarian texts;?
it introduces new features in the machine learning setting like semantic and pragmatic features;?
we show that there are domain specificities in the distribution of uncertainty cues in Hungarian texts;?
we show that domain specificities have a considerable effect on the efficiency of machine learning.The structure of the paper is the following.
First, related work on uncertainty detection is presented.Then our corpus is described in detail, which is followed by the elaboration of machine learning methodsand results on uncertainty detection.
The paper concludes with a discussion of results and possible waysfor future work are also outlined.This work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/18442 Related WorkIn these days, identifying uncertainty cues is one of the popular topics in NLP.
This is supported by theCoNLL-2010 Shared Task, which aimed at detecting uncertainty cues in biological papers and Wikipediaarticles written in English (Farkas et al., 2010).
Moreover, a special issue of the journal ComputationalLinguistics (Vol.
38, No.
2) was recently dedicated to detecting modality and negation in natural lan-guage texts (Morante and Sporleder, 2012).
As indicated above, most earlier research on uncertaintydetection focused on the English language.
As for the domains of the texts, newspapers (Saur??
andPustejovsky, 2009), biological or medical texts (Szarvas et al., 2012; Morante et al., 2009; Farkas et al.,2010; Kim et al., 2008), Wikipedia articles (Ganter and Strube, 2009; Farkas et al., 2010; Szarvas et al.,2012) and most recently social media texts (Wei et al., 2013) have been selected for the experiments.Systems for uncertainty detection were originally rule-based (Light et al., 2004; Chapman et al.,2007) but recently, they exploit machine learning methods, usually applying a supervised approach (seee.g.
Medlock and Briscoe (2007), Morante et al.
(2009),?Ozg?ur and Radev (2009), Szarvas et al.
(2012)and the systems of the CoNLL-2010 Shared Task (Farkas et al., 2010)).
In harmony with the latesttendencies, our system here is also based on supervised machine learning techniques, which employs arich feature set of lexical, morphological, syntactic and semantic features and also exploits contextualfeatures.Supervised machine learning methods require annotated corpora.
There have been several corporaannotated for uncertainty in different domains such as biology (Medlock and Briscoe, 2007; Kim et al.,2008; Settles et al., 2008; Shatkay et al., 2008; Vincze et al., 2008; Nawaz et al., 2010), medicine (Uzuneret al., 2009), news media (Saur??
and Pustejovsky, 2009; Wilson, 2008; Rubin et al., 2005; Rubin, 2010),encyclopedia (Farkas et al., 2010), reviews (Konstantinova et al., 2012; Cruz D?
?az, 2013) and socialmedia (Wei et al., 2013).
For our experiments, however, we make use of the first Hungarian uncertaintycorpus created for the purpose of this study.3 ExperimentsIn this section, we present our methodology to detect uncertainty cues in Hungarian.
We first describe theuncertainty categories applied and report some statistics on the corpus.
Then we describe our machinelearning approach based on a rich feature set.3.1 The hUnCertainty CorpusFor the purpose of this study, we manually annotated texts from two domains.
First, we randomlyselected 1,081 paragraphs from the Hungarian Wikipedia dump.
This selection contains 9,722 sentencesand 180,000 tokens.
Second, we downloaded 300 pieces of criminal news from a Hungarian news portal(http://www.hvg.hu), which altogether consist of 5,481 sentences and 94,000 tokens.
In total, thehUnCertainty corpus consists of 15,203 sentences and 274,000 tokens.During annotation, we followed the categorization of uncertainty phenomena as described in Szarvaset al.
(2012) and Vincze (2013) with some slight modifications, due to the morphologically rich natureof Hungarian (for instance, modal auxiliaries like may correspond to a derivational suffix in Hungarian,which required that in the case of j?ohet ?may come?
the whole word was annotated as uncertain, notjust the suffix -het).
Here we just briefly summarize uncertainty categories that were annotated ?
for adetailed discussion, please refer to Szarvas et al.
(2012) and Vincze (2013).Linguistic uncertainty is traditionally connected to modality and the semantics of the sentence.
Forinstance, the sentence It may be raining does not contain enough information to determine whether itis really raining (semantic uncertainty).
There are several phenomena that are categorized as semanticuncertainty.
A proposition is epistemically uncertain if its truth value cannot be determined on thebasis of world knowledge.
Conditionals and investigations also belong to this group ?
the latter isespecially frequent in research papers, where authors usually formulate the research question with thehelp of linguistic devices expressing this type of uncertainty.
Non-epistemic types of modality may alsobe listed here such as doxastic uncertainty, which is related to beliefs.1845However, there are other uncertainty phenomena that only become uncertain within the context ofcommunication.
For instance, the sentence Many researchers think that COLING will be the best con-ference of the year does not reveal how many (and which) researchers think that, hence the source ofthe proposition about COLING remains uncertain.
This is a type of discourse-level uncertainty, morespecifically, it is called weasel (Ganter and Strube, 2009).
On the other hand, hedges make the meaningof words fuzzy: they blur the exact meaning of some quality/quantity.
Finally, peacock cues expressunprovable (or unproven) evaluations, qualifications, understatements and exaggerations.Some examples of uncertainty cues are offered here (in English, for the sake of simplicity):EPISTEMIC: It may be raining.DYNAMIC: I have to go.DOXASTIC: He believes that the Earth is flat.INVESTIGATION: We examined the role of NF-kappa B in protein activation.CONDITION: If it rains, we?ll stay in.WEASEL: Some note that the number of deaths during confrontations with police is relativelyproportional for a city the size of Cincinnati.HEDGE: Magdalene Asylums were a generally accepted social institution until well into thesecond half of the 20th century.PEACOCK: The main source of their inspiration was native Georgia, with its rich and complexhistory and culture, its breathtaking landscapes and its courageous and hardworking people.Table 1 reports some statistics on the frequency of uncertainty cues in Hungarian and it is also vi-sualized in Figure 1.
It is revealed that the domain of the texts has a strong effect on the distributionof uncertainty cues: the distribution of semantic uncertainty cues and discourse-level uncertainty cuesis balanced in the news subcorpus but in the Wikipedia corpus, about 85% of the cues belong to thediscourse-level uncertainty type.Regarding different classes of uncertainty, we should mention that while weasels constitute the mostfrequent cue category in Wikipedia texts, they occur less frequently in the news corpus.
On the otherhand, doxastic cues are frequent in the news corpus but in Wikipedia texts, their number is considerablysmaller.Uncertainty cue Wikipedia News Total# % # % # %Weasel 2150 35.95 258 10.93 2408 28.87Hedge 2100 35.12 800 33.88 2900 34.77Peacock 788 13.18 94 3.98 882 10.57Discourse-level total 5038 84.25 1152 48.79 6190 74.21Epistemic 441 7.37 358 15.16 799 9.58Doxastic 316 5.28 710 30.07 1026 13.30Conditional 154 2.58 128 5.42 282 3.38Investigation 31 0.52 13 0.55 44 0.53Semantic total 942 15.75 1209 51.21 2151 25.79Total 5980 100 2361 100 8341 100Table 1: Uncertainty cues.3.2 Machine Learning MethodsIn order to automatically identify uncertainty cues, we developed a machine learning method to be dis-cussed below.
In our experiments, we used the above-described corpus and morphologically and syntac-tically parsed it with the help of the toolkit magyarlanc (Zsibrita et al., 2013).18460%10%20%30%40%50%60%70%80%90%100%News Wikipedia hUnCertaintyinvestigationconditiondoxasticepistemicpeacockhedgeweaselFigure 1: Distribution of cues across domains.On the basis of results reported in earlier literature, sequence labeling proved to be one of the mostsuccessful methods on English uncertainty detection (see e.g.
Szarvas et al.
(2012)), hence we also ap-plied a method based on conditional random fields (CRF) (Lafferty et al., 2001) in our experiments.
Weused the MALLET implementation (McCallum, 2002) of CRF with the following rich feature set:?
Orthographic features: we investigated whether the word contains punctuation marks, digits, up-percase or lowercase letters, the length of the word, consonant bi- and trigrams...?
Lexical features: we automatically collected uncertainty cues from the English corpora annotatedon the basis of similar linguistic principles and manually translated these lists into Hungarian.
Listswere used as binary features: if the lemma of the given word occurred in one of the lists, the featurewas assigned the value true, else it was false.?
Morphological features: for each word, its part of speech and lemma were noted.
As mentionedbefore, modality and mood are morphologically expressed in Hungarian (e.g.
in csin?alhatn?ank do-MOD-COND-1PL ?we could do?, the suffix -hat refers to modality and the suffix -n?a refers toconditional) hence for each verb, it was investigated whether it had a modal suffix, whether it wasin the conditional mood and whether its form was first person plural or third person plural as thesetwo latter verbal forms are typical instances of expressing generic phrases or generalizations inHungarian, which are related to weasels.
For each noun, its number (i.e.
singular/plural) was markedas feature.
For each pronoun, we checked whether it was an indefinite one since indefinite pronounslike valaki ?someone?
or valamilyen ?some?
are often used as weasel cues.
For each adjective, wemarked whether it was comparative or superlative as they can occur as peacock cues.?
Syntactic features: for each word, its dependency label was marked.
For each noun, it was checkedwhether it had a determiner as determinerless nouns may be used as weasels in Hungarian.
For eachverb, it was checked whether it had a subject1.?
Semantic/pragmatic features: we manually compiled a list of speech act verbs in Hungarian andchecked whether the given verb was one of them.
Besides, we translated lists of English words with1Hungarian is a pro-drop language, hence the subject is not obligatorily present in the clause.
Moreover, applying a thirdperson plural verb without a subject is a common way to express generalization in Hungarian, which is one typical strategy ofweasels.1847positive and negative content developed for sentiment analysis (Liu, 2012) and checked whether thelemma of the given word occurred in these lists.As contextual features for each word, we applied as features the POS tags and dependency labels ofwords within a window of size two.
Although earlier research on English uncertainty detection mostlymade use of orthographical, morphological and syntactic information (see e.g.
Szarvas et al.
(2012)),here we included some new feature types in our feature set, namely, pragmatic and semantic features.Based on this feature set, we carried out our experiments.
Since only 3% of the tokens in the corpusfunction as uncertainty cues, it seemed necessary to filter the training database: half of the cuelesssentences were randomly selected and deleted from the training dataset.
Moreover, as there were only44 investigation cues in the data, we omitted this class from training and evaluation as well, due tosparseness problems.First, we applied ten-fold cross validation on the corpus.
Since we had two domains of texts at hand, itenabled us to experiment with the two domains separately as well: ten-fold cross validation was carriedout for both domains individually and we also made use of cross-domain settings, where one of thedomains was used as the training database but the evaluation was performed on the other domain.
Forevaluation, we used the metrics precision, recall and F-score.
The results of our experiments will bepresented in Section 4.3.3 Baseline MethodsAs a baseline, we applied a simple dictionary lookup method.
Lists mentioned among the lexical featureswere utilized here: whenever the lemma of the given word matched one of the words in the list, we taggedit as an uncertainty cue of the type determined by the given list.4 ResultsTable 2 shows the results of the baseline and machine learning experiments on the hUnCertainty corpus,obtained by ten-fold cross validation.Dictionary lookup Machine learning DifferenceType Precision Recall F-score Precision Recall F-score Precision Recall F-scoreWeasel 18.12 35.92 24.09 52.48 30.73 38.76 +34.37 -5.19 +14.68Hedge 55.10 32.42 40.82 61.26 48.94 54.41 +6.17 +16.52 +13.59Peacock 21.66 30.77 25.42 32.61 11.88 17.41 +10.95 -18.89 -8.01Epistemic 42.46 30.02 35.18 63.18 34.07 44.27 +20.72 +4.04 +9.09Doxastic 29.30 46.16 35.85 52.42 46.26 49.15 +23.12 +0.10 +13.30Condition 31.73 62.90 42.18 51.41 25.80 34.35 +19.68 -37.10 -7.83Micro P/R/F 29.09 35.74 32.07 55.95 37.46 44.87 +26.86 +1.72 +12.80Table 2: Results on the hUnCertainty corpus.The results of the machine learning approach have outperformed those achieved by the baseline dic-tionary lookup method, except for two classes.
This is primarily due to better precision, which has grownfor each uncertainty category in the case of sequence labeling.
However, recall values are more diverse:for hedges and epistemic cues, it has grown, for doxastic cues it has not changed significantly, but forpeacocks and conditional cues we can see a serious decrease.
The low recall values might be the reasonwhy the F-score obtained by the dictionary lookup method is higher than the one obtained by machinelearning in the case of peacocks and conditionals.We also experimented separately on the two domains.
Table 3 shows those on the news subcorpus,whereas Table 4 shows the results achieved on the Wikipedia subcorpus.In both domains, we can observe that machine learning methods outperform the baseline dictionarylookup method, except for the peacock and conditional cue classes.
However, there are domain differ-ences in the results.
First, weasels seem to be much hard to detect in the news subcorpus than in the1848Dictionary lookup Machine learning DifferenceType Precision Recall F-score Precision Recall F-score Precision Recall F-scoreWeasel 3.24 17.83 5.48 37.50 15.12 21.55 +34.26 -2.71 +16.06Hedge 53.61 39.05 45.18 61.55 49.69 54.99 +7.94 +10.64 +9.80Peacock 13.82 31.91 19.29 47.06 8.51 14.41 +33.23 -23.40 -4.88Epistemic 31.90 20.67 25.08 56.63 39.39 46.46 +24.73 +18.72 +21.37Doxastic 33.50 37.61 35.43 57.05 51.83 54.32 +23.55 +14.23 +18.88Condition 35.27 57.03 43.58 54.39 24.22 33.51 +19.12 -32.81 -10.07Micro P/R/F 23.21 34.17 27.65 57.31 41.93 48.43 +34.10 +7.76 +20.78Table 3: Results on the news subcorpus.Dictionary lookup Machine learning DifferenceType Precision Recall F-score Precision Recall F-score Precision Recall F-scoreWeasel 26.03 38.50 31.06 59.26 34.74 43.80 +33.23 -3.76 +12.74Hedge 55.86 29.92 38.97 64.59 50.02 56.38 +8.73 +20.10 +17.41Peacock 23.29 30.63 26.46 37.85 13.8 20.22 +14.56 -16.83 -6.24Epistemic 49.57 37.34 42.59 63.95 36.03 46.09 +14.38 -1.31 +3.50Doxastic 25.24 65.20 36.40 54.31 33.54 41.47 +29.07 -31.66 +5.07Condition 29.66 67.74 41.26 47.12 31.61 37.84 +17.46 -36.13 -3.42Micro P/R/F 32.28 36.40 34.21 59.70 37.5 46.06 +27.42 +1.10 +11.85Table 4: Results on the Wikipedia subcorpus.Wikipedia subcorpus (21.55 vs. 43.8 in terms of F-score).
Second, peacocks are also harder to detect inthe news subcorpus (F-scores of 14.41 vs. 20.22).
Third, there is a considerable gap between the recallscores in the case of doxastic cues: in the Wikipedia subcorpus, the dictionary lookup method outper-forms CRF (the difference is 36.13 percentage points) but in the news subcorpus, CRF achieves higherrecall with 14.23 percentage points.To further explore domain differences, we carried out some cross validation experiments.
First, wetrained our CRF model on the Wikipedia domain and then evaluated it on the news domain.
Later, themodel was trained on the news domain and evaluated on the Wikipedia domain.
Tables 5 and 6 presentthe results, respectively, contrasted to the results achieved in the indomain settings.
It is also strikingthat although the gain in micro F-score is almost the same in the two settings, the biggest differencecan be observed for semantic uncertainty classes in the case of the Wikipedia?
news setting, while thedifference is much bigger for discourse-level uncertainty types in the news?Wikipedia setting.Cross validation Indomain ten fold DifferenceType Precision Recall F-score Precision Recall F-score Precision Recall F-scoreWeasel 17.53 19.77 18.58 37.50 15.12 21.55 +19.97 -4.65 +2.97Hedge 57.40 39.30 46.66 61.55 49.69 54.99 +4.15 +10.39 +8.33Peacock 22.81 13.83 17.22 47.06 8.51 14.41 +24.25 -5.32 -2.80Epistemic 50.00 16.76 25.10 56.63 39.39 46.46 +6.63 +22.63 +21.35Doxastic 46.63 10.70 17.41 57.05 51.83 54.32 +10.43 +41.13 +36.91Condition 62.96 26.56 37.36 54.39 24.22 33.51 -8.58 -2.34 -3.85Micro P/R/F 44.48 23.35 30.62 57.31 41.93 48.43 +12.83 +18.58 +17.81Table 5: Cross-domain results: Wikipedia?
news.As some uncertainty detectors aim at identifying uncertain sentences only, that is, they handle the taskat the sentence level and do not pay attention to the detection of individual cues (Medlock and Briscoe,2007), we also applied a more relaxed evaluation metric.
If at least one of the tokens within the sentencewas labeled as an uncertainty cue ?
regardless of its type ?, the sentence was considered as uncertain.1849Cross validation Indomain ten fold DifferenceType Precision Recall F-score Precision Recall F-score Precision Recall F-scoreWeasel 71.26 6.87 12.53 59.26 34.74 43.8 -12.00 +27.87 +31.27Hedge 63.48 26.33 37.22 64.59 50.02 56.38 +1.11 +23.69 +19.16Peacock 43.14 5.57 9.87 37.85 13.80 20.22 -5.29 +8.23 +10.35Epistemic 78.65 30.57 44.03 63.95 36.03 46.09 -14.70 +5.46 +2.06Doxastic 39.55 33.23 36.12 54.31 33.54 41.47 +14.76 +0.31 +5.35Condition 47.31 28.39 35.48 47.12 31.61 37.84 -0.19 +3.22 +2.36Micro P/R/F 59.98 18.00 27.68 59.7 37.5 46.06 -0.28 +19.50 +18.38Table 6: Cross-domain results: news?Wikipedia.Results on the identification of uncertain sentences are summarized in Table 7, in terms of precision,recall and F-score.
It is revealed that here there are no sharp differences in performance as far as theindomain settings are concerned since the system can achieve an F-score of about 70 in both domainsand on the whole corpus as well.
However, in the cross-domain settings lower precision values andF-scores can be observed, while recall values basically remain the same with regard to the indomainsettings.Evaluation setting Precision Recall F-scorehUnCertainty 10 fold 62.20 78.06 69.23News 10 fold 67.38 78.01 72.30Wikipedia 10 fold 60.32 80.05 68.80Wikipedia?
news 45.88 74.21 56.70News?Wikipedia 35.73 84.61 50.24Table 7: Machine learning results at the sentence level.5 DiscussionOur results prove that a sequence labeling approach can be efficiently used for the automatic identificationof uncertainty cues in Hungarian texts.
With our baseline dictionary lookup method, the best resultswere achieved on the epistemic, conditional and hedge cues while the sequence labeling approach wasthe most successful on the hedge, epistemic and doxastic cues.
All of this indicates that hedge andepistemic cues are the easiest to detect.
On the other hand, uncertainty types where there was a smalldifference between the results achieved by the two approaches (for instance, semantic uncertainty cues inthe Wikipedia subcorpus) are mostly expressed by lexical means and these cues are less ambiguous.
Inthis setting, the detection of discourse-level uncertainty categories, however, profits more from machinelearning, which is most probably due to the fact that here context (discourse) plays a more important rulehence a sequence labeling algorithm is more appropriate for the task, which takes into account contextualinformation as well.In the case of peacocks and conditional cues the sequence labeling approach obtained worse resultsthan dictionary lookup: in each case, precision got higher but recall seriously decreased.
This suggeststhat these classes highly rely on lexical features and our machine learning system needs further improve-ment, with special regard to specific (lexical) features defined for these uncertainty categories.As for domain differences, we found that the distribution of uncertainty cues differs in the two sub-corpora, weasels being more frequent in Wikipedia whereas doxastic cues are more probable to occurin the news subcorpus.
Domain differences concerning weasels and doxastic cues are highlighted in thecross domain experiments as well.
When the training dataset contains fewer cues of the given uncertaintytype, the performance falls back on the target domain: when trained on the news subcorpus, an F-scoreof 12.53 can be obtained for weasels in the Wikipedia subcorpus, which is 31.27 points less than theindomain results.
Similarly, an F-score of 17.41 can be obtained for doxastic cues in the news domain1850when Wikipedia is used as the training set but the indomain setting yields an F-score of 54.32.All of the above facts may be related to the characteristics of the texts.
Weasels are sourceless propo-sitions and in the news media, it is indispensable to know who the source of the news is, thus, piecesare usually reported with their source provided and so, propositions with no explicit source (i.e.
weasels)occur rarely in the news subcorpus.
On the other hand, doxastic cues are related to beliefs and the newssubcorpus consists of criminal news (mostly related to murders).
When describing the possible reasonsbehind each criminal act, phrases that refer to beliefs and mental states are often used and thus this typeof uncertainty is likely to be present in such pieces of news but not in Wikipedia articles.In the cross domain experiments, indomain results outperform those obtained by the cross domainmodels.
The difference in performance is significant (t-test, p = 0.042 for the news subcorpus and p= 0.0103 for the Wikipedia subcorpus).
That is, the choice of the training dataset significantly affectsthe results, which indicates that there really are domain differences in uncertainty detection.
There areonly two exceptions that do not correspond to these tendencies: the peacock and conditional cues in theWikipedia ?
news setting.
The reason why a model trained on a different domain can perform bettermight lie in the size of the subcorpora.
The Wikipedia domain contains much more peacock cues thanthe news domain and although the domains are different, training on a dataset with more cue instancesseems to be beneficial for the results.If we evaluate the models?
performance at the sentence level rather than at the cue level, it can beobserved that better results can be achieved, especially with regard to recall values.
One reason forthat may be that a single uncertain sentence may include more than one cues and should one of thembe missed, it does not seriously harm performance (in case at least one cue per sentence is correctlydetected).If our results are compared to those achieved on semantic uncertainty cues found in English Wikipediaarticles (Szarvas et al., 2012), it can be seen that the task seems to be somewhat easier in English thanin Hungarian: that paper reports F-scores from 0.6 to 0.8.
One possible reason for this is that there aretypological differences between English and Hungarian and so, uncertainty marking is rather lexicallydetermined in English but in Hungarian, morphology also plays an essential role.
For instance, themodal suffixes -hat/-het correspond to the auxiliaries may and might and while in English they functionas separate lexical items, in Hungarian they are always attached to the verbal stem and never occur ontheir own.
This is reflected in the number of different cues as well: in the English dataset, there are 166different semantic cues while in Hungarian, there are 319 (and note that the Hungarian corpus is abouthalf of the size of the English one).
As such, applying the word form or the lemma as features mayresult in relatively high F-scores in English, where the word form itself denotes uncertainty, but thesefeatures are less effective in Hungarian without any morphological features included.
Another language-specific feature is that Hungarian is a pro-drop language, so in some cases, the pronominal subject may beomitted from the sentence.
Subjectless sentences are a typical strategy in Hungarian to express sourcelessstatements (weasels), but the subject can be deleted due to syntactic ellipsis as well, thus distinguishingbetween subjectless sentences that denote uncertainty and those that do not is a special task in Hungarianuncertainty detection.The outputs of the machine learning system were further investigated, in order to find the most typi-cal errors our system made.
It was revealed that the most problematic issue was the disambiguation ofambiguous cues.
For instance, the words sz?amos ?several?
or sok ?many?
may function as hedges orweasels, or nagy ?big?
may be a hedge or a peacock, depending on the context.
Such cues were oftenmisclassified by the system.
Another common source of errors was that some cues have non-cue mean-ings as well, like the verb tart, which can be a doxastic cue with the meaning ?think?
but when it means?keep?, it is not uncertain at all.
The identification of epistemic cues that include negation words wasalso not straightforward: multiword cues such as nem z?arhat?o ki ?it cannot be excluded?
or nem tudni ?itis not known?
were not marked as cues by the system.18516 ConclusionsIn this paper, we presented the first results on Hungarian uncertainty detection.
For this, we made useof a manually annotated corpus, which contains texts from two domains: Wikipedia articles and piecesof news from a news portal.
We contrasted the cue distribution in the two domains and we also experi-mented with uncertainty detection.
For this purpose, we applied a supervised machine learning approach,which was based on sequence labeling and exploited a rich feature set.
We reported the first results onuncertainty detection for Hungarian, which also prove that the performance on uncertainty detection isinfluenced by the domain of the texts.
We hope that this study will enhance research on uncertaintydetection for languages other than English.In the future, we would like to improve our methods, especially in order to achieve better recall at thecue level.
Furthermore, we would like to investigate domain specificities in more detail and we wouldalso like to carry out some domain adaptation experiments as well.AcknowledgmentsThis research was supported by the European Union and the State of Hungary, co-financed by the Eu-ropean Social Fund in the framework of T?AMOP-4.2.4.A/2-11/1-2012-0001 ?National Excellence Pro-gram?.ReferencesWendy W. Chapman, David Chu, and John N. Dowling.
2007.
Context: An algorithm for identifying contextualfeatures from clinical text.
In Proceedings of the ACL Workshop on BioNLP 2007, pages 81?88.Noa P. Cruz D??az.
2013.
Detecting negated and uncertain information in biomedical and review texts.
In Proceed-ings of the Student Research Workshop associated with RANLP 2013, pages 45?50, Hissar, Bulgaria, September.RANLP 2013 Organising Committee.Rich?ard Farkas, Veronika Vincze, Gy?orgy M?ora, J?anos Csirik, and Gy?orgy Szarvas.
2010.
The CoNLL-2010Shared Task: Learning to Detect Hedges and their Scope in Natural Language Text.
In Proceedings of theFourteenth Conference on Computational Natural Language Learning (CoNLL-2010): Shared Task, pages 1?12, Uppsala, Sweden, July.
Association for Computational Linguistics.Viola Ganter and Michael Strube.
2009.
Finding Hedges by Chasing Weasels: Hedge Detection Using WikipediaTags and Shallow Linguistic Features.
In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers,pages 173?176, Suntec, Singapore, August.
Association for Computational Linguistics.Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii.
2008.
Corpus annotation for mining biomedical events fromliterature.
BMC Bioinformatics, 9(Suppl 10).Natalia Konstantinova, Sheila C.M.
de Sousa, Noa P. Cruz, Manuel J. Mana, Maite Taboada, and Ruslan Mitkov.2012.
A review corpus annotated for negation, speculation and their scope.
In Nicoletta Calzolari (ConferenceChair), Khalid Choukri, Thierry Declerck, Mehmet Ugur Dogan, Bente Maegaard, Joseph Mariani, Jan Odijk,and Stelios Piperidis, editors, Proceedings of the Eight International Conference on Language Resources andEvaluation (LREC?12), Istanbul, Turkey.
European Language Resources Association (ELRA).John Lafferty, Andrew McCallum, and Fernando Pereira.
2001.
Conditional random fields: Probabilistic modelsfor segmenting and labeling sequence data.
In Proceedings of ICML-01, 18th Int.
Conf.
on Machine Learning,pages 282?289.
Morgan Kaufmann.Marc Light, Xin Ying Qiu, and Padmini Srinivasan.
2004.
The language of bioscience: Facts, speculations,and statements in between.
In Proc.
of the HLT-NAACL 2004 Workshop: Biolink 2004, Linking BiologicalLiterature, Ontologies and Databases, pages 17?24.Bing Liu.
2012.
Sentiment Analysis and Opinion Mining.
Morgan & Claypool Publishers.Andrew Kachites McCallum.
2002.
MALLET: A Machine Learning for Language Toolkit.http://mallet.cs.umass.edu.Ben Medlock and Ted Briscoe.
2007.
Weakly Supervised Learning for Hedge Classification in Scientific Litera-ture.
In Proceedings of the ACL, pages 992?999, Prague, Czech Republic, June.1852Roser Morante and Caroline Sporleder.
2012.
Modality and negation: An introduction to the special issue.Computational Linguistics, 38:223?260, June.Roser Morante, Vincent van Asch, and Antal van den Bosch.
2009.
Joint memory-based learning of syntactic andsemantic dependencies in multiple languages.
In Proceedings of CoNLL, pages 25?30.Raheel Nawaz, Paul Thompson, and Sophia Ananiadou.
2010.
Evaluating a meta-knowledge annotation schemefor bio-events.
In Proceedings of the Workshop on Negation and Speculation in Natural Language Processing,pages 69?77, Uppsala, Sweden, July.
University of Antwerp.Arzucan?Ozg?ur and Dragomir R. Radev.
2009.
Detecting speculations and their scopes in scientific text.
InProceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1398?1407,Singapore, August.
Association for Computational Linguistics.Victoria L. Rubin, Elizabeth D. Liddy, and Noriko Kando.
2005.
Certainty identification in texts: Categorizationmodel and manual tagging results.
In J.G.
Shanahan, J. Qu, and J. Wiebe, editors, Computing attitude and affectin text: Theory and applications (the information retrieval series), New York.
Springer Verlag.Victoria L. Rubin.
2010.
Epistemic modality: From uncertainty to certainty in the context of information seekingas interactions with texts.
Information Processing & Management, 46(5):533?540.Roser Saur??
and James Pustejovsky.
2009.
FactBank: a corpus annotated with event factuality.
Language Re-sources and Evaluation, 43:227?268.Burr Settles, Mark Craven, and Lewis Friedland.
2008.
Active learning with real annotation costs.
In Proceedingsof the NIPS Workshop on Cost-Sensitive Learning, pages 1?10.Hagit Shatkay, Fengxia Pan, Andrey Rzhetsky, and W. John Wilbur.
2008.
Multi-dimensional classification ofbiomedical text: Toward automated, practical provision of high-utility text to diverse users.
Bioinformatics,24(18):2086?2093.Gy?orgy Szarvas, Veronika Vincze, Rich?ard Farkas, Gy?orgy M?ora, and Iryna Gurevych.
2012.
Cross-genre andcross-domain detection of semantic uncertainty.
Computational Linguistics, 38:335?367, June.
?Ozlem Uzuner, Xiaoran Zhang, and Tawanda Sibanda.
2009.
Machine Learning and Rule-based Approaches toAssertion Classification.
Journal of the American Medical Informatics Association, 16(1):109?115, January.Veronika Vincze, Gy?orgy Szarvas, Rich?ard Farkas, Gy?orgy M?ora, and J?anos Csirik.
2008.
The BioScope Corpus:Biomedical Texts Annotated for Uncertainty, Negation and their Scopes.
BMC Bioinformatics, 9(Suppl 11):S9.Veronika Vincze.
2013.
Weasels, Hedges and Peacocks: Discourse-level Uncertainty in Wikipedia Articles.In Proceedings of the Sixth International Joint Conference on Natural Language Processing, pages 383?391,Nagoya, Japan, October.
Asian Federation of Natural Language Processing.Zhongyu Wei, Junwen Chen, Wei Gao, Binyang Li, Lanjun Zhou, Yulan He, and Kam-Fai Wong.
2013.
An em-pirical study on uncertainty identification in social media context.
In Proceedings of the 51st Annual Meeting ofthe Association for Computational Linguistics (Volume 2: Short Papers), pages 58?62, Sofia, Bulgaria, August.Association for Computational Linguistics.Theresa Ann Wilson.
2008.
Fine-grained Subjectivity and Sentiment Analysis: Recognizing the Intensity, Polarity,and Attitudes of Private States.
Ph.D. thesis, University of Pittsburgh, Pittsburgh.J?anos Zsibrita, Veronika Vincze, and Rich?ard Farkas.
2013. magyarlanc: A Toolkit for Morphological and Depen-dency Parsing of Hungarian.
In Proceedings of RANLP-2013, pages 763?771, Hissar, Bulgaria.1853
