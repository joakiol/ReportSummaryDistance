Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 654?659,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsHierarchical Reinforcement Learning and Hidden Markov Models forTask-Oriented Natural Language GenerationNina DethlefsDepartment of Linguistics,University of Bremendethlefs@uni-bremen.deHeriberto Cuaya?huitlGerman Research Centre for Artificial Intelligence(DFKI), Saarbru?ckenheriberto.cuayahuitl@dfki.deAbstractSurface realisation decisions in language gen-eration can be sensitive to a language model,but also to decisions of content selection.
Wetherefore propose the joint optimisation ofcontent selection and surface realisation usingHierarchical Reinforcement Learning (HRL).To this end, we suggest a novel reward func-tion that is induced from human data and isespecially suited for surface realisation.
It isbased on a generation space in the form ofa Hidden Markov Model (HMM).
Results interms of task success and human-likeness sug-gest that our unified approach performs betterthan greedy or random baselines.1 IntroductionSurface realisation decisions in a Natural LanguageGeneration (NLG) system are often made accord-ing to a language model of the domain (Langkildeand Knight, 1998; Bangalore and Rambow, 2000;Oh and Rudnicky, 2000; White, 2004; Belz, 2008).However, there are other linguistic phenomena, suchas alignment (Pickering and Garrod, 2004), consis-tency (Halliday and Hasan, 1976), and variation,which influence people?s assessment of discourse(Levelt and Kelter, 1982) and generated output (Belzand Reiter, 2006; Foster and Oberlander, 2006).Also, in dialogue the most likely surface form maynot always be appropriate, because it does not cor-respond to the user?s information need, the user isconfused, or the most likely sequence is infelicitouswith respect to the dialogue history.
In such cases, itis important to optimise surface realisation in a uni-fied fashion with content selection.
We suggest touse Hierarchical Reinforcement Learning (HRL) toachieve this.
Reinforcement Learning (RL) is an at-tractive framework for optimising a sequence of de-cisions given incomplete knowledge of the environ-ment or best strategy to follow (Rieser et al, 2010;Janarthanam and Lemon, 2010).
HRL has the ad-ditional advantage of scaling to large and complexproblems (Dethlefs and Cuaya?huitl, 2010).
Sincean HRL agent will ultimately learn the behaviourit is rewarded for, the reward function is arguablythe agent?s most crucial component.
Previous workhas therefore suggested to learn a reward functionfrom human data as in the PARADISE framework(Walker et al, 1997).
Since PARADISE-based re-ward functions typically rely on objective metrics,they are not ideally suited for surface realisation,which is more dependend on linguistic phenomena,e.g.
frequency, consistency, and variation.
However,linguistic and psychological studies (cited above)show that such phenomena are indeed worth mod-elling in an NLG system.
The contribution of thispaper is therefore to induce a reward function fromhuman data, specifically suited for surface genera-tion.
To this end, we train HMMs (Rabiner, 1989)on a corpus of grammatical word sequences and usethem to inform the agent?s learning process.
In addi-tion, we suggest to optimise surface realisation andcontent selection decisions in a joint, rather than iso-lated, fashion.
Results show that our combined ap-proach generates more successful and human-likeutterances than a greedy or random baseline.
This isrelated to Angeli et al (2010), who also address in-terdependent decision making, but do not use an opt-misation framework.
Since language models in ourapproach can be obtained for any domain for whichcorpus data is available, it generalises to new do-mains with limited effort and reduced development654Utterancestring=?turn around and go out?, time=?20:54:55?Utterance typecontent=?orientation,destination?
[straight, path, direction]navigation level=?low?
[high]Useruser reaction=?perform desired action?
[perform undesired action, wait, request help]user position=?on track?
[off track]Figure 1: Example annotation: alternative values for at-tributes are given in square brackets.time.
For related work on using graphical modelsfor language generation, see e.g., Barzilay and Lee(2002), who use lattices, or Mairesse et al (2010),who use dynamic Bayesian networks.2 Generation SpacesWe are concerned with the generation of navigationinstructions in a virtual 3D world as in the GIVEscenario (Koller et al, 2010).
In this task, two peo-ple engage in a ?treasure hunt?, where one partici-pant navigates the other through the world, pressinga sequence of buttons and completing the task byobtaining a trophy.
The GIVE-2 corpus (Gargett etal., 2010) provides transcripts of such dialogues inEnglish and German.
For this paper, we comple-mented the English dialogues of the corpus with aset of semantic annotations,1 an example of whichis given in Figure 1.
This example also exempli-fies the type of utterances we generate.
The input tothe system consists of semantic variables compara-ble to the annotated values, the output correspondsto strings of words.
We use HRL to optimise deci-sions of content selection (?what to say?)
and HMMsfor decisions of surface realisation (?how to say it?
).Content selection involves whether to use a low-, orhigh-level navigation strategy.
The former consistsof a sequence of primitive instructions (?go straight?,?turn left?
), the latter represents contractions of se-quences of low-level instructions (?head to the nextroom?).
Content selection also involves choosing alevel of detail for the instruction corresponding tothe user?s information need.
We evaluate the learntcontent selection decisions in terms of task success.For surface realisation, we use HMMs to informthe HRL agent?s learning process.
Here we address1The annotations are available on request.the one-to-many relationship arising between a se-mantic form (from the content selection stage) andits possible realisations.
Semantic forms of instruc-tions have an average of 650 surface realisations,including syntactic and lexical variation, and deci-sions of granularity.
We refer to the set of alterna-tive realisations of a semantic form as its ?generationspace?.
In surface realisation, we aim to optimise thetradeoff between alignment and consistency (Picker-ing and Garrod, 2004; Halliday and Hasan, 1976) onthe one hand, and variation (to improve text qualityand readability) on the other hand (Belz and Reiter,2006; Foster and Oberlander, 2006) in a 50/50 dis-tribution.
We evaluate the learnt surface realisationdecisions in terms of similarity with human data.Note that while we treat content selection andsurface realisation as separate NLG tasks, their op-timisation is achieved jointly.
This is due to atradeoff arising between the two tasks.
For exam-ple, while surface realisation decisions that are opti-mised solely with respect to a language model tendto favour frequent and short sequences, these canbe inappropriate according to the user?s informationneed (because they are unfamiliar with the naviga-tion task, or are confused or lost).
In such situa-tions, it is important to treat content selection andsurface realisation as a unified whole.
Decisions ofboth tasks are inextricably linked and we will showin Section 5.2 that their joint optimisation leads tobetter results than an isolated optimisation as in, forexample, a two-stage model.3 NLG Using HRL and HMMs3.1 Hierarchical Reinforcement LearningThe idea of language generation as an optimisa-tion problem is as follows: given a set of genera-tion states, a set of actions, and an objective rewardfunction, an optimal generation strategy maximisesthe objective function by choosing the actions lead-ing to the highest reward for every reached state.Such states describe the system?s knowledge aboutthe generation task (e.g.
content selection, naviga-tion strategy, surface realisation).
The action setdescribes the system?s capabilities (e.g.
?use highlevel navigation strategy?, ?use imperative mood?,etc.).
The reward function assigns a numeric valuefor each action taken.
In this way, language gen-655Figure 2: Hierarchy of learning agents (left), where shaded agents use an HMM-based reward function.
The top threelayers are responsible for content selection (CS) decisions and use HRL.
The shaded agents in the bottom use HRLwith an HMM-based reward function and joint optimisation of content selection and surface realisation (SR).
Theyprovide the observation sequence to the HMMs.
The HMMs represent generation spaces for surface realisation.
Anexample HMM, representing the generation space of ?destination?
instructions, is shown on the right.eration can be seen as a finite sequence of states,actions and rewards {s0, a0, r1, s1, a1, ..., rt?1, st},where the goal is to find an optimal strategy auto-matically.
To do this we use RL with a divide-and-conquer approach to optimise a hierarchy of genera-tion policies rather than a single policy.
The hierar-chy of RL agents consists of L levels and N modelsper level, denoted as M ij , where j ?
{0, ..., N ?
1}and i ?
{0, ..., L ?
1}.
Each agent of the hierar-chy is defined as a Semi-Markov Decision Process(SMDP) consisting of a 4-tuple < Sij, Aij , T ij , Rij >.Sij is a set of states, Aij is a set of actions, T ij isa transition function that determines the next states?
from the current state s and the performed ac-tion a, and Rij is a reward function that specifiesthe reward that an agent receives for taking an ac-tion a in state s lasting ?
time steps.
The randomvariable ?
represents the number of time steps theagent takes to complete a subtask.
Actions can beeither primitive or composite.
The former yield sin-gle rewards, the latter correspond to SMDPs andyield cumulative discounted rewards.
The goal ofeach SMDP is to find an optimal policy that max-imises the reward for each visited state, according topi?ij(s) = arg maxa?Aij Q?ij(s, a), where Q?ij (s, a)specifies the expected cumulative reward for execut-ing action a in state s and then following policy pi?ij .We use HSMQ-Learning (Dietterich, 1999) to learna hierarchy of generation policies.3.2 Hidden Markov Models for NLGThe idea of representing the generation space ofa surface realiser as an HMM can be roughly de-fined as the converse of POS tagging, where an in-put string of words is mapped onto a hidden se-quence of POS tags.
Our scenario is as follows:given a set of (specialised) semantic symbols (e.g.,?actor?, ?process?, ?destination?
),2 what is the mostlikely sequence of words corresponding to the sym-bols?
Figure 2 provides a graphic illustration of thisidea.
We treat states as representing words, and se-quences of states i0...in as representing phrases orsentences.
An observation sequence o0...on consistsof a finite set of semantic symbols specific to the in-struction type (i.e., ?destination?, ?direction?, ?orien-tation?, ?path?, ?straight?).
Each symbol has an ob-servation likelihood bi(ot), which gives the proba-bility of observing o in state i at time t. The tran-sition and emission probabilities are learnt duringtraining using the Baum-Welch algorithm.
To de-sign an HMM from the corpus data, we used theABL algorithm (van Zaanen, 2000), which alignsstrings based on Minimum Edit Distance, and in-duces a context-free grammar from the aligned ex-amples.
Subsequently, we constructed the HMMsfrom the CFGs, one for each instruction type, andtrained them on the annotated data.2Utterances typically contain five to ten semantic categories.6563.3 An HMM-based Reward Function Inducedfrom Human DataDue to its unique function in an RL framework, wesuggest to induce a reward function for surface re-alisation from human data.
To this end, we createand train HMMs to represent the generation spaceof a particular surface realisation task.
We then usethe forward probability, derived from the Forwardalgorithm, of an observation sequence to inform theagent?s learning process.r =????????????????????
?0 for reaching the goal state+1 for a desired semantic choice ormaintaining an equal distributionof alignment and variation-2 for executing action a and remain-ing in the same state s = s?P (w0...wn) for for reaching a goal state corres-ponding to word sequence w0...wn-1 otherwise.Whenever the agent has generated a word sequencew0...wn, the HMM assigns a reward correspondingto the likelihood of observing the sequence in thedata.
In addition, the agent is rewarded for shortinteractions at maximal task success3 and optimalcontent selection (cf.
Section 2).
Note that while re-ward P (w0...wn) applies only to surface realisationagents M30...4, the other rewards apply to all agentsof the hierarchy.4 Experimental SettingWe test our approach using the (hand-crafted) hierar-chy of generation subtasks in Figure 2.
It consists ofa root agent (M00 ), and subtasks for low-level (M20 )and high-level (M21 ) navigation strategies (M11 ), andfor instruction types ?orientation?
(M30 ), ?straight?
(M31 ), ?direction?
(M32 ), ?path?
(M33 ) and destina-tion?
(M34 ).
Models M30...4 are responsible for sur-face generation.
They will be trained using HRLwith an HMM-based reward function induced fromhuman data.
All other agents use hand-crafted re-wards.
Finally, subtask M10 can repair a previoussystem utterance.
The states of the agent containall situational and linguistic information relevant toits decision making, e.g., the spatial environment,3Task success is addressed by that each utterance needs tobe ?accepted?
by the user (cf.
Section 5.1).discourse history, and status of grounding.4 Due tospace constraints, please see Dethlefs et al (2011)for the full state-action space.
We distinguish prim-itive actions (corresponding to single generation de-cisions) and composite actions (corresponding togeneration subtasks (Fig.
2)).5 Experiments and Results5.1 The Simulated EnvironmentThe simulated environment contains two kinds ofuncertainties: (1) uncertainty regarding the state ofthe environment, and (2) uncertainty concerning theuser?s reaction to a system utterance.
The first as-pect is represented by a set of contextual variablesdescribing the environment, 5 and user behaviour.6Altogether, this leads to 115 thousand different con-textual configurations, which are estimated fromdata (cf.
Section 2).
The uncertainty regardingthe user?s reaction to an utterance is represented bya Naive Bayes classifier, which is passed a set ofcontextual features describing the situation, mappedwith a set of semantic features describing the utter-ance.7 From these data, the classifier specifies themost likely user reaction (after each system act) ofperform desired action, perform undesired action, waitand request help.8 The classifier was trained on theannotated data and reached an accuracy of 82% in across-corpus validation using a 60%-40% split.5.2 Comparison of Generation PoliciesWe trained three different generation policies.
Thelearnt policy optimises content selection and sur-face realisation decisions in a unified fashion, andis informed by an HMM-based generation spacereward function.
The greedy policy is informedonly by the HMM and always chooses the most4An example for the state variables of model M11 are theannotation values in Fig.
1 which are used as the agent?sknowledge base.
Actions are ?choose easy route?, ?choose shortroute?, ?choose low level strategy?, ?choose high level strategy?.5previous system act, route length, route status(known/unknown), objects within vision, objects withindialogue history, number of instructions, alignment(proportion)6previous user reaction, user position, user wait-ing(true/false), user type(explorative/hesitant/medium)7navigation level(high / low), abstractness(implicit / ex-plicit), repair(yes / no), instruction type(destination / direction /orientation / path / straight)8User reactions measure the system?s task success.657likely sequence independent of content selection.The valid sequence policy generates any grammat-ical sequence.
All policies were trained for 20000episodes.9 Figure 3, which plots the average re-wards of all three policies (averaged over ten runs),shows that the ?learnt?
policy performs best in termsof task success by reaching the highest overall re-wards over time.
An absolute comparison of the av-erage rewards (rescaled from 0 to 1) of the last 1000training episodes of each policy shows that greedyimproves ?any valid sequence?
by 71%, and learntimproves greedy by 29% (these differences are sig-nificant at p < 0.01).
This is due to the learnt policyshowing more adaptation to contextual features thanthe greedy or ?valid sequence?
policies.
To evaluatehuman-likeness, we compare instructions (i.e.
wordsequences) using Precision-Recall based on the F-Measure score, and dialogue similarity based on theKulback-Leibler (KL) divergence (Cuaya?huitl et al,2005).
The former shows how the texts generated byeach of our generation policies compare to human-authored texts in terms of precision and recall.
Thelatter shows how similar they are to human-authoredtexts.
Table 1 shows results of the comparison oftwo human data sets ?Real1?
vs ?Real2?
and both ofthem together against our different policies.
Whilethe greedy policy receives higher F-Measure scores,the learnt policy is most similar to the human data.This is due to variation: in contrast to greedy be-haviour, which always exploits the most likely vari-ant, the learnt policy varies surface forms.
This leadsto lower F-Measure scores, but achieves higher sim-ilarity with human authors.
This ultimately is a de-sirable property, since it enhances the quality andnaturalness of our instructions.6 ConclusionWe have presented a novel approach to optimisingsurface realisation using HRL.
We suggested toinform an HRL agent?s learning process by anHMM-based reward function, which was induced9For training, the step-size parameter ?
(one for eachSMDP) , which indicates the learning rate, was initiated with1 and then reduced over time by ?
= 11+t , where t is the timestep.
The discount rate ?, which indicates the relevance of fu-ture rewards in relation to immediate rewards, was set to 0.99,and the probability of a random action ?
was 0.01.
See Suttonand Barto (1998) for details on these parameters.0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2x 104?250?200?150?100?50050AverageRewardsEpisodesValid SequenceGreedyLearntFigure 3: Performance of ?learnt?, ?greedy?, and ?anyvalid sequence?
generation behaviours (average rewards).Compared Policies F-Measure KL-DivergenceReal1 - Real2 0.58 1.77Real - ?Learnt?
0.40 2.80Real - ?Greedy?
0.49 4.34Real - ?Valid Seq.?
0.0 10.06Table 1: Evaluation of generation behaviours withPrecision-Recall and KL-divergence.from data and in which the HMM represents thegeneration space of a surface realiser.
We alsoproposed to jointly optimise surface realisationand content selection to balance the tradeoffs of(a) frequency in terms of a language model, (b)alignment/consistency vs variation, (c) propertiesof the user and environment.
Results showed thatour hybrid approach outperforms two baselines interms of task success and human-likeness: a greedybaseline acting independent of content selection,and a random ?valid sequence?
baseline.
Futurework can transfer our approach to different domainsto confirm its benefits.
Also, a detailed humanevaluation study is needed to assess the effectsof different surface form variants.
Finally, othergraphical models besides HMMs, such as BayesianNetworks, can be explored for informing the surfacerealisation process of a generation system.AcknowledgmentsThanks to the German Research Foundation DFGand the Transregional Collaborative Research Cen-tre SFB/TR8 ?Spatial Cognition?
and the EU-FP7project ALIZ-E (ICT-248116) for partial support ofthis work.658ReferencesGabor Angeli, Percy Liang, and Dan Klein.
2010.
Asimple domain-independent probabilistic approach togeneration.
In Proceedings of the 2010 Conference onEmpirical Methods in Natural Language Processing,EMNLP ?10, pages 502?512.Srinivas Bangalore and Owen Rambow.
2000.
Exploit-ing a probabilistic hierarchical model for generation.In Proceedings of the 18th Conference on Computa-tional Linguistics (ACL) - Volume 1, pages 42?48.Regina Barzilay and Lillian Lee.
2002.
Bootstrap-ping lexical choice via multiple-sequence alignment.In Proceedings of the 2002 Conference on EmpiricalMethods in Natural Language Processing (EMNLP),pages 164?171.Anja Belz and Ehud Reiter.
2006.
Comparing Automaticand Human Evaluation of NLG Systems.
In Proc.
ofthe European Chapter of the Association for Compu-tational Linguistics (EACL), pages 313?320.Anja Belz.
2008.
Automatic generation ofweather forecast texts using comprehensive probabilis-tic generation-space models.
Natural Language Engi-neering, 1:1?26.Heriberto Cuaya?huitl, Steve Renals, Oliver Lemon, andHiroshi Shimodaira.
2005.
Human-Computer Dia-logue Simulation Using Hidden Markov Models.
InProc.
of ASRU, pages 290?295.Nina Dethlefs and Heriberto Cuaya?huitl.
2010.
Hi-erarchical Reinforcement Learning for Adaptive TextGeneration.
Proceeding of the 6th International Con-ference on Natural Language Generation (INLG).Nina Dethlefs, Heriberto Cuaya?huitl, and Jette Viethen.2011.
Optimising Natural Language Generation De-cision Making for Situated Dialogue.
In Proc.
of the12th Annual SIGdial Meeting on Discourse and Dia-logue.Thomas G. Dietterich.
1999.
Hierarchical Reinforce-ment Learning with the MAXQ Value Function De-composition.
Journal of Artificial Intelligence Re-search, 13:227?303.Mary Ellen Foster and Jon Oberlander.
2006.
Data-driven generation of emphatic facial displays.
In Proc.of the European Chapter of the Association for Com-putational Linguistic (EACL), pages 353?360.Andrew Gargett, Konstantina Garoufi, Alexander Koller,and Kristina Striegnitz.
2010.
The GIVE-2 corpus ofgiving instructions in virtual environments.
In LREC.Michael A. K. Halliday and Ruqaiya Hasan.
1976.
Co-hesion in English.
Longman, London.Srinivasan Janarthanam and Oliver Lemon.
2010.
Learn-ing to adapt to unknown users: referring expressiongeneration in spoken dialogue systems.
In Proc.
of theAnnual Meeting of the Association for ComputationalLinguistics (ACL), pages 69?78.Alexander Koller, Kristina Striegnitz, Donna Byron, Jus-tine Cassell, Robert Dale, Johanna Moore, and JonOberlander.
2010.
The first challenge on generat-ing instructions in virtual environments.
In M. The-une and E. Krahmer, editors, Empirical Methodson Natural Language Generation, pages 337?361,Berlin/Heidelberg, Germany.
Springer.Irene Langkilde and Kevin Knight.
1998.
Generationthat exploits corpus-based statistical knowledge.
InProceedings of the 36th Annual Meeting of the As-sociation for Computational Linguistics (ACL), pages704?710.W J M Levelt and S Kelter.
1982.
Surface form andmemory in question answering.
Cognitive Psychol-ogy, 14.Franc?ois Mairesse, Milica Gas?ic?, Filip Jurc??
?c?ek, SimonKeizer, Blaise Thomson, Kai Yu, and Steve Young.2010.
Phrase-based statistical language generation us-ing graphical models and active learning.
In Proc.
ofthe Annual Meeting of the Association for Computa-tional Linguistics (ACL), pages 1552?1561.Alice H. Oh and Alexander I. Rudnicky.
2000.
Stochas-tic language generation for spoken dialogue systems.In Proceedings of the 2000 ANLP/NAACL Workshopon Conversational systems - Volume 3, pages 27?32.Martin J. Pickering and Simon Garrod.
2004.
Towarda mechanistc psychology of dialog.
Behavioral andBrain Sciences, 27.L R Rabiner.
1989.
A Tutorial on Hidden Markov Mod-els and Selected Applications in Speech Recognition.In Proceedings of IEEE, pages 257?286.Verena Rieser, Oliver Lemon, and Xingkun Liu.
2010.Optimising information presentation for spoken dia-logue systems.
In Proc.
of the Annual Meeting ofthe Association for Computational Lingustics (ACL),pages 1009?1018.Richard S Sutton and Andrew G Barto.
1998.
Reinforce-ment Learning: An Introduction.
MIT Press, Cam-bridge, MA, USA.Menno van Zaanen.
2000.
Bootstrapping syntax andrecursion using alginment-based learning.
In Pro-ceedings of the Seventeenth International Conferenceon Machine Learning (ICML), pages 1063?1070, SanFrancisco, CA, USA.Marilyn A. Walker, Diane J. Litman, Candace A. Kamm,and Alicia Abella.
1997.
PARADISE: A frameworkfor evaluating spoken dialogue agents.
In Proc.
of theAnnual Meeting of the Association for ComputationalLinguistics (ACL), pages 271?280.Michael White.
2004.
Reining in CCG chart realization.In Proc.
of the International Conference on NaturalLanguage Generation (INLG), pages 182?191.659
