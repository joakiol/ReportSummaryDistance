Proceedings of the IJCNLP-08 Workshop on NER for South and South East Asian Languages, pages 75?82,Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language ProcessingAn Experiment on Automatic Detection of Named Entities in BanglaBidyut Baran ChaudhuriHead- CVPR UnitIndian Statistical Institute203,B.T.
Road, Kolkata-108bbc@isical.ac.inSuvankar BhattacharyaSystems ExecutiveABP Pvt.
Ltd.6, P.S.
Street, Kolkata-1suvankar.bhattacharya@abp.inAbstractSeveral preprocessing steps are necessaryin various problems of automatic NaturalLanguage Processing.
One major step isnamed-entity detection, which is relativelysimple in English, because such entitiesstart with an uppercase character.
For In-dian scripts like Bangla, no such indicatorexists and the problem of identification ismore complex, especially for humannames, which may be common nouns andadjectives as well.
In this paper we haveproposed a three-stage approach of named-entity detection.
The stages are based onthe use of Named-Entity (NE) dictionary,rules for named-entity and left-right co-occurrence statistics.
Experimental resultsobtained on Anandabazar Patrika (Mostpopular Bangla newspaper) corpus arequite encouraging.1 IntroductionThe discipline of Natural Language Processing(NLP) is concerned with the design and implemen-tation of computational approaches that communi-cate with human using natural language.
Namesearching, matching and recognition have beenactive areas of research in the field of NLP andInformation retrieval for a long period.
This is animportant problem since search queries are oftenproper nouns while all proper nouns cannot be ex-haustively maintained in the dictionary for auto-matic identification.
Moreover, human names maybe picked from common nouns and adjectivewords (e.g.
Surya, Anindya) and hence dictionary-based syntactic information can confuse the Natu-ral Language Processor in such a situation.
Pet andother animal names, organization and place names,can also come from common nouns and adjectivese.g.
Shyamali (cow name), Bardhaman (Placename), Bhalobasha (Building name), Nandan(Auditorium name) etc.
So, it becomes a non-trivial problem to automatically detect the namedentity from a sentence.This paper aims at attacking this problem forBangla language, especially on the NE detectionfrom newspaper text.
Name recognition in Englishis somewhat easier since quite often the propernoun starts with an uppercase character.
Banglanames cannot be identified by such case informa-tion because Bangla has single-case alphabet.Some studies on Named Entity (NE) identifica-tion are reported in the literatures (from Zhou and2007 to Narayanswamy et al, Narayanswamy aslisted in the reference section of this paper).
Theapproaches mainly employ dictionary based,rulebased and statistical tools such as HMM, Maxi-mum entropy, Support vector machine and condi-tional random field for this purpose.
Name search-ing in the context of information retrieval andquery answering are also reported in the literature(Thompson and Dozier, 2007).
However, thesestudies are done on non-Indian languages.
AmongIndian languages typical efforts based on HMMand CRF are presented by EKbal et al (2007) andLi and McCallum (2003) respectively.The NE identification approach presented hereemploys a three tier combination of dictionary-based, rule-based and statistical information.
Theapproach employed here is explained in Section 2where use of the hybrid approach is also justified.In Section 3, the data collection and experimental75setup is described.
Tests have been made on amoderate size Anandabazar (most popular Banglanewspaper) news corpus.
The results are presentedin Section 4.2 Proposed Named Entity (NE) detectionapproachAs mentioned before, our method of NE detectionis a combination of dictionary-based, rule-basedand statistical (n-gram based) approaches.
In thedictionary based approach, we need a word-levelmorphological parser as well.
The approaches aresequentially described here and demonstrated inFig.1.
However, at first, we describe some proper-ties of named entity.2.1 Properties of named entityIf we look at a corpus of reasonable size from theperspective of NEs, we note that the words maybelong to three categories: (a) words that almostnever act as NE, (b) the words that almost alwaysact as NE, (c) the words that sometimes act asnames and sometimes as common nouns or adjec-tives.
Words like I, the, to, from, go belong tocategory (a) while words like India, Ganges, Paris,Himalayas belong to category (b).
Words likeNirmal, Swapan, Rabi belong to category (c).
TheEnglish meanings of these third category words areclean, dream and sun, respectively, but they areused as names of persons in Bangla and thus cancreate problems for the NLP of  Bangla language.In English, the names begin with uppercase, andare less problematic in nature.Another point to note is that the named entitymay be a single word or a multi word expression.The multi-word names pose additional difficultyfor automatic identification of NE.
A multi-wordmay have a component that alone is also a name,like England in New England or it may consist ofadjective and common noun, like White House.Such multi-words generate additional problems forNE detection.Fig 1.
Flow chart for NE detection2.2 Justification of hybrid approachIn the NE detection tasks, the entries that are con-sidered are person, organization, location, date,time, money, percentage.
In case of English, thereare indicators like uppercase character, dot mark,Dollar and Pound symbol etc.
to identify them.
Inaddition, rule-base or machine learning approachesare employed and hence an impressive result isobtained.In Bangla, date, time, money, percentage alsouse special symbols in some occasions, but for per-son, organization or location name this is not true.Moreover, nouns and adjectives are very fre-quently used for single-word or multi-word namesof above types.
Now, a dictionary or some specialkind of word data-base is used in most NLP prob-lems.
If we equip the same dictionary or data-basewhich have information about NE, then every wordof a text need not pass through more sophisticatedNE detection software.
We have noted that evenfor NE-rich text like news, the percentage of suchwords does not exceed 7% (See Table-1).
The dic-tionary helps us to detect 65% of Nes and discardmore than 90% of the non-NE words.
For that pur-pose, we have to tag the dictionary in the mannerdescribed in Section 2.3.
We can be left with about1.4% words, which may be ambiguous (can be orcannot be NE) and about 1.15% words, which arenot there in the dictionary (hence nothing can besaid using dictionary).Newspaper TotalWordTotal NE PersonNamePlaceNameOtherNamesAnandabazar 421046.53%[2753]2.50%[1023]2.30%[972]1.80%[758]Ajkaal 39452 6.93%[2734]2.89%[1143]1.92%[755]2.11%[836]Bartamaan 40323 6.50%[2621]3.43%[1383]1.60%[645]1.47%[593]Table 1.
NEs from different Bangla NewspapersSo, we can use the rule base at the second stage.Compared to statistical learning methods, rule-based system has both power and limitation.
Con-sider a robust simulation where each person nameand place name of West Bengal, Tripura and Bang-ladesh (the Bangla-language-using places) can ap-pear.
Note that there are about 240 million Bengalinames and a few tens of thousands of place names.Of course, not all are distinct names, but the dis-tinct names are also huge in number.
To explain itbetter, let there be 1000 distinct first names, 5076distinct middle names and 500 distinct last names(title) of persons.
Then the total number of distincthuman names that can be created is 1000 X 50 X500 = 25 million.
If the full names appear in thetest, then they could be very easily tackled by arule and a database of middle names and titles.
Onthe other hand, any statistical technique is based onprobability, and estimation of probability needs areasonable corpus size that is costly and may notbe available for design.
Even if the corpus is avail-able, the statistical approach will perhaps discoverthe same rule along with the same database in adifferent manner.
Moreover, extension for a fewmore names can be quickly accommodated in thedatabase or another rule, but the statistical ap-proach will need re-training, resulting in a new setof absolute and conditional probabilities.On the other hand, rule-based system cannottackle ambiguous situations very well.
So, when itis the question of a noun or adjective word beingused as NE or not NE, good rules cannot be formu-lated for every situation.
Rule-based system is alsouseless for a word not falling under any of the rulesgenerated so far.
In such a situation the statisticallearning technique may be very useful.In this way, we believe that the combination ofthree approaches will help us in detecting NE in arobust way.
Moreover, we believe that it will beeasily adapted to changed environment of the testset.2.3 Dictionary based NE detectionIf a dictionary is maintained where one of theabove three category tags are attached to eachword and if a word morphological analyzer is de-veloped, then the combination of these two can actas a NE detector for a text file.
The dictionaryshould be generated from a corpus of reasonablesize, say 5-10 million words, as well as from con-ventional dictionary book of say 50,000 rootwords.
Normally, 10 million word corpus ofBangla contains between 100,000 and 200,000 sur-face words.
A small fraction of these words belongto the set of NEs not found in the conventional dic-tionary.
These surface words should be properlyNE tagged as per three types described above andentered in the NE dictionary.
The corpus providesimportant information about the inflectional natureof root words, which, in turn, helps in building themorphological analyzer.
On the other hand, if wewant to avoid building sophisticated morph ana-lyzer, the most common inflected surface words ofthe corpus may also be included in the dictionarywith the three tags described above.
We have fol-lowed this procedure for our NE detection ap-proach.The detection algorithm will proceed as follows.Given a test word W, at first, a match is searchedin the NE tagged dictionary.
If no match is found,W is rejected and the next word is considered forexamination.
But if a match occurs, we look at thetag of the matched word.
If the tag is ?almost al-ways NE?
then we declare this W as NE withweight 1.
If the tag is ?almost never NE?
then W isdeclared as not NE (ie with weight 0).
But if thetag is ?may or may not be NE?
then again W has tobe rejected (say with weight 0.5), which makes thisapproach uncertain for such word.
To remedy thisdrawback, we next employ some rule-based ap-proach described in the next Section.However, before sending to the rule-based mod-ule, each of the words with weight 0.5 is subject tomorphological analysis.
Here for each word, thesuffix is stripped using a previously stored suffixdatabase.
If no database suffix matches, then thewhole word is sent to rule based method.
Else, thesuffix-stripped word is again matched in the NEdictionary.
If a match is found, then it is checked ifthe suffix can be morphologically accepted by thedictionary root word category.
Then W is properlytagged with weight 1 or 0.
Else, it is sent to themodule for rule-based approach described belowwith the hope for better decision.2.4 Rule-based NE detectionRule-based approaches rely on some rules, one ormore of which is to be satisfied by the test wordW.
There may be positive and negative rules.
Thepositive rules make the inference system biasedtowards NE while the negative rules tend to be bi-ased against NE.
Some small databases may beneeded to execute the rules.
For Bangla text NEs,some typical rules are given below.
Here, 1-8 arepositive and 9-12 are negative rules.Rule1.
If there are two or more words in a se-quence that represent the characters or spell likethe characters of Bangla or English, then they be-long to the named entity (with high weight).
Forexample, (B A), (C M D A),are all NEs.
Note that the rule will not distinguishbetween a proper name and common name.77Rule 2.
If the previous word of W is a pre-nameword like ,,  then W belongs to thenamed entity (with high weight).
To detect them,all words of this type can be maintained in a data-base.Rule 3.
If after W there are title words and mid-name words to human names likeetc.
andetc., respectively, then Walong with such words are likely to constitute amulti-word NE (with high weight).. For example,, are all NEs.
A set of titleand mid-name words should be collected andmaintained in a database.Rule 4.
If a substring like ?
- - -- - - - - - occurs at the endof the word W, then W is likely to be a NE (withhigh weight).
These strings can be collected in adatabase for future use.Rule 5.
If at the end of a word W there arestrings like ?
- - - - - - - -- then W is likely to be a name (with highweight).Rule 6.
If a word likeis found after W of type unknown in diction-ary then W along with such word may belong toNE (with high weight).
For example,are all NEs.Rule 7.
We note that only a few names or wordsin Bangla consist of characters (Chandrabindu)or (Khanda Ta).
So, if W does not belong tothose words and has the occurrence of any of thesetwo characters, then W may be a named entity(with high weight).
For example, ??
??
is a Frenchname.Rule 8.
If in the sentence containing unknownword W or a word W with may or may not be NEtag, the following words arewhich imply action thatcan be done by human being, then W is likely to bea name (with high weight).
A database of actionverbs of various types is needed to check this rule.Rule 9.
If W of the type given in rule 8 is fol-lowed by verb not in the set of verbs describedabove, then W is not likely to be a NE.
So, theweight should be reduced from 0.5 to a smallervalue.Rule 10.
If there is re-duplication of W in a sen-tence then W is not likely to be a named entity.This is so because rarely name words are redupli-cated.
In fact, reduplicated name word may signifysomething else.
For example is used to greeta person.
So, the NE weight should be reduced in asuch case to near zero.Rule 11.
If at the end of W there are suffixes like?
- - - - - - - -- - etc., then W is usually not a named en-tity.Rule 12.
If there is an echo-word after W e.g., then none of these two words is a named entity.The exact value of the weight for a rule is decidedfrom training dataset.
We increase or decrease theweight of the test word if a rule fires.
To be consis-tent, we have included the dictionary-based ap-proach under the same framework.Thus, in our scheme, if the weight is more thancertain value (say 0.75) then the word is finallyaccepted to be NE.
On the other hand, if the weightis less than certain value (say 0.25) then the wordis rejected to be NE.
For intermediate cases, theword may be subject to the n-gram based techniquedescribed below.2.5 n-gram based NE detectionThe n-gram based approach relies on the co-occurrence of other words before and after a NE.To generate the n-gram we need a corpus wherethe NE words are tagged manually.
From thesetagged words the left neighbor and right neighborwords are checked (for a 2-gram model).
The fre-quencies of each pair of left-right neighbor arecounted from the corpus.
The probability of eachleft-right pair with respect to W may be estimatedasPlr(W) = No of this left-right word pair aroundW/ total no of all left-right words around W in thetraining corpus.If a particular left-right neighbors occur about aword W, then W has a positive likelihood of beingNE, or a negative likelihood that W is not a NE.For example, in the sentence ?Mark the answerscript properly?
the word ?Mark?
is a negative in-stance for NE.
But in the sentence ?Mark is a goodboy?, ?Mark?
is a positive instance.
Here the left-right pair is ?blank?
and ?is?.
We have to countfrom the test corpus how many times the particularleft-right neighbor give positive instances of  Wbeing a NE, while how many are the instances of78non-NE.
From these positive and negative instancecounts, a NE weight value is found for a particularpair of left-right word pair around W aswlr(W) = Plr(W) Rlr(W)where Rlr(W) =  No of positive instances /(No ofpositive instances + No of negative  instances).However, a large number of words will be nega-tive instances at all times, so their wlr(W) valuewill come out as zero.
Examples are the so-calledstop words.
They can be dealt in the dictionary it-self, as discussed in Sec 2.2, reducing a lot ofcomputational effort for this n-gram based ap-proach.
Some words which will also be positiveinstance, irrespective of the left right words.
TheNE dictionary described in Section 2 can deal themas well.
This fact partly justifies the scheme ofhaving three approaches combined in our NE de-tection algorithm.Thus, the generation of training phase is com-pleted.
Now, in the test phase, if a word W has left-right neighbors whose weight is wlr(W) based onthe training phase, then W may be assigned thisweight of being named entity.
This is the modifiedweight over and above what was given in the pre-vious phases.
For the test phase, a threshold t is seton the weight.
If the weight for the test word W isw > t then we declare W as a NE.
Otherwise, wecall it not-NE.There may be left-right pair for a test word thatis absent in our probability list.
If none of the pairexist then the word is rejected since no decisioncan be made.
If only left or right word is presentthen we take a pessimistic estimate based on it.
Inother words, we take the minimum of probabilitiesindividually this W and the said left word.3 Data collectionTo obtain the corpus for our experiment, webrowsed the net and found the site of AnandabazarPatrika, the largest Bangla daily newspaper.
Wedownloaded the e-newspapers for the years 2001-2004.
Of this huge data, a portion for the years2001-2003 were used for training the system(about 20 million words) and a portion from 2004(about 100, 000 words) was used for testing.
Thedata could not be utilized in a straightforward way,since the newspaper authority used a proprietaryglyph code.
So, we had to discover which glyphcode denotes which character of Bangla script andthen convert the text into ISCII coding format.
Af-ter that, all the developed softwares were run onthese ISCII files.
At first a program was writtenwas used to collect all distinct surface words fromthis corpus of 20 million words.
These distinctwords were ranked in descending order of fre-quency and the top 20,000 ranked words were cho-sen for manual tagging of named entity by givingweight 0, 0.5 or 1.0.The manual tagging was done by the linguistsbased on their global knowledge.
However, if theperson is in doubt, (s)he would consult a few ex-amples in the original corpus involving the word inquestion.
Using the contextual information, mostproblematic cases could be disambiguated.
Thosewhich still appeared unclear were given ?may ormay not be?
status.
A morphological analyzer waspreviously developed in connection with the designof a spell checker in Bangla (Chaudhuri, 2001).That analyzer has been employed for stemming ofthe type-words in the current NE detection prob-lem also.
Moreover, a rule-based system as de-scribed in Section 2.3 is also developed.
The data-base needed for each rule is being continuouslyupdated to give better experimental results.Experimental results:The software was trained with the AnandabazarPatrika web corpus of the year 2001-2003.
Somegeographical names were further added to enrichthe database.
Then several files of the corpus of thesame newspaper of the year 2004 were used fortesting.
The results are presented in the form ofrecall(R), precision (P) and F-measure percentage.Here the recall is the ratio of number of NE wordsretrieved and the number of NE words actuallypresent in the file, expressed in percent.
In otherwords,%100% XtexttheinwordsNEofNumberTotalretrivedwordsNEofNumberRPrecision is the number of correctly retrievedNE words to the total number of words retrieved,expressed in percent.
So, we can write%100% XretrievedwordsNEofNumberTotalretrievedwordsNEcorrectofNumberPThe F-measure is often used in the InformationRetrieval and Natural Language Processing prob-lems.
This class of measures was introduced by C.79J.
van Rijsbergen.
F1- measure is the ratio of thetwice of the multiplication of precision (P) and re-call (R) and the sum of these two.
In other words,%100%%%%2%F1 XRPRPF1 measure combines recall (R) and precision(P) with an equal weight and hence is the harmonicmean of the two quantities.
Note that F1 cannotexceed 100%.
Experimental results on 10 sets oftest documents are shown in Table 2.NO.
OFWORDSNO.OFNECOR-RECTLYDETECTEDNO.OFER-RORRE-CALL%PRECISION%F1-MEAS-URE %2592 165 138 7 79.39 95.00 86.002938 186 157 6 81.10 96.20 88.002477 247 176 6 76.25 97.60 85.003816 336 268 7 79.76 97.40 87.002944 192 144 5 75.00 96.52 84.414843 255 210 13 82.35 93.50 87.852899 202 192 7 95.04 96.35 95.443420 232 201 9 86.63 95.52 90.854428 243 209 11 86.00 94.73 90.154228 210 177 16 84.28 90.96 87.424528 292 261 11 89.38 95.78 92.462991 193 168 5 87.04 97.02 91.75AVERAGE 85.50 94.24 89.51Table 2.
Results of the experimentIt is noted from Table 1 that the precision is rea-sonably high but the recall is somewhat moderate.The reason of moderate occurrence of recall is thatthe training has been done with only 20,000 corpuswords, while actual number of corpus words wasabout 200,000.
Also, we have to improve the data-base for rules, as well as search for other potentialrules that we have not included here.
The frontback 2-grams are also at present aggregated overall NE words tagged manually.
Such global occur-rence statistics can mask the local phenomenon.We are working towards improving our NE detec-tion approach.Every detection system is to be judged by someautomatic evaluation techniques, e.g.
BLEU (Bi-lingual Evaluation Understudy) (Papineni, 2002)and several others.
So, in case of ours we intro-duced an Automatic Evaluation approach for themain detection algorithm.
The evaluation system isactually based upon a manually annotated datasetof almost 70,000 words.
These datasets are taggedin a ?non-NE <NE Name NE> non-NE?
formatand are available at Chaudhuri (2007).
After thesystem detects and tags the names, the detectionsystem treats the NE-detected file location as the?Target Location?.
In our annotated dataset theannotated corpus is available for the same docu-ments.
That location is treated as the ?AnnotatedLocation?.
As the evaluation system starts evaluat-ing, a word by word comparison is done betweenthe target and annotated locations.
At the end ofevaluation number of correctly detected words, thenumber of wrong detection and the number of realNE is found and so the Precision, Recall and F1-Measure is calculated easily.
We have also ob-served that our evaluation system gives almost thesame result as found by manual evaluation.ReferencesG.
Zhou and J. Su 2002.
Named Entity recognitionusingHMM Based chunk tagger, Proc.
40-th Annual Meet-ing of ACL,Philadelphia,pp.
473-480.A.
Borthwick,1999.
A maximum Entropy approach toNamed Entity recognition, Computer Sc.
Dept.
NewYork University.J.
R. Finkel, T. Grnagar & C. Maning, 2005.
Incorpo-rating non-local information into information extrac-tion systems by Gibbs sampling, Proc.
43-rd Annualmeeting of ACL, pp.
363-370.U.
Pfeiffer; T. Poersch, and N. Fuhr.
1996.
Retrievaleffectiveness of proper name search methods, Infor-mation Processing & Management, Vol.
32, pp.
667-679.K.
Takeuchi and N. Collier, 2002, Use of support vectormachines in extended Named Entity recognition,Proc.
6th Conference Natural Language Learning,Taipei, pp.
119-125.D.
Marynard, V.Tablan, K. Cunningham and Y. Wilks.2003.
Muse : a multisource entity recognition system.Computers and the Humanities.
Website Reference:http://gate.ac.uk/sale/muse/muse.pdfD.
Maynard, K. Bonteheva, H. Cunningham, 2003.
To-wards a semantic extraction of named entity,.
Web-site reference: http://eprints.akfors.org/267/01/maynard.pdf.P.
Thompson and C. C. Dozier, on  Name Searchingand Information Retrieval.
Website reference:http://arxiv.org/html/cmplg/9706017.H.
Cunningham.
2002.
Gate, a general architecture fortext engineering.
Computers and the Humanities,Vol.
36, pp.
223- 254.M.
Narayanswamy, K.E.
Ravikumar, and V.K.
Shanker.2003.
A biological named entity recognizer.
Proceed-80ings of the Pacific Symposium on Biocomputing,Hawaii.A.
EKbal ,S. Naskar and S. Bandopadhyay, 2007,Named Recognition and Transliteration in Bengali,Special Issue of Lingvisticae Investigationes Jour-nal,30:1 (2007), pp.
95-114, John Benjamins Publish-ing Company.S.
Cucerzon and D. Yarowsky.
1999.
Language inde-pendent named entity recognition combining mor-phological and contextual evidence.
Proceedings ofthe 1999 Joint SIGDAT conference on EMNLP andVLC.B.B.
Chaudhuri.
2001.
A Novel Spell-checker forBangla Text Based on Reversed-Word Dictionary.Vivek Vol.
14 No.
4,pp.
3-12.W.
Li, A. McCallum, 2003.
Rapid development of HindiNamed Entity recognition using Conditional RandomFields and Feature Extraction.
ACM Trans on AsianLanguage Information Processing, Vol 2, No.
3, pp.290-293.D.
Okanohara, Y. Miyao, Y. Tsuruoka and J. Tsujii.2006.
Improving the Scalability of Semi-MarkovConditional Random Fields for Named Entity Recog-nition.
Proceedings of the COLING-ACL, Sydney,Australia, 17-21 July, pp.
465-472.Papineni, K., Roukos, S., Ward, T., and Zhu, W. J.2002.
BLEU: a method for automatic evaluation ofmachine translation  in ACL-2002: 40th Annualmeeting of the Association for Computational Lin-guistics pp.
311--318Annotated Bengali Corpus by Prof. B.
B. Chaudhuri,ISI, Calcutta and Suvankar Bhattacharya.
WebsiteReference: http://cybersuv.googlepages.com/Annotated.zipRapid Development of Hindi Named Entity Recognitionusing Conditional Random Fields and Feature Ex-traction  by Wei Li, University of MassachusettsAmherst and Andrew McCallum, University ofMassachusetts Amherst.8182
