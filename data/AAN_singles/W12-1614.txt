Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 108?112,Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational LinguisticsImproving Implicit Discourse Relation RecognitionThrough Feature Set OptimizationJoonsuk ParkDepartment of Computer ScienceCornell UniversityIthaca, NY, USAjpark@cs.cornell.eduClaire CardieDepartment of Computer ScienceCornell UniversityIthaca, NY, USAcardie@cs.cornell.eduAbstractWe provide a systematic study of previouslyproposed features for implicit discourse re-lation identification, identifying new featurecombinations that optimize F1-score.
The re-sulting classifiers achieve the best F1-scoresto date for the four top-level discourse rela-tion classes of the Penn Discourse Tree Bank:COMPARISON, CONTINGENCY, EXPAN-SION, and TEMPORAL.
We further identifyfactors for feature extraction that can have amajor impact on performance and determinethat some features originally proposed for thetask no longer provide performance gains inlight of more powerful, recently discoveredfeatures.
Our results constitute a new set ofbaselines for future studies of implicit dis-course relation identification.1 IntroductionThe ability to recognize the discourse relations thatexist between arbitrary text spans is crucial for un-derstanding a given text.
Indeed, a number of natu-ral language processing (NLP) applications rely on it?
e.g., question answering, text summarization, andtextual entailment.
Fortunately, explicit discourserelations ?
discourse relations marked by explicitconnectives ?
have been shown to be easily identi-fied by automatic means (Pitler et al, 2008): eachsuch connective is generally strongly coupled witha particular relation.
The connective ?because?, forexample, serves as a prominent cue for the CONTIN-GENCY relation.The identification of implicit discourse relations?
where such connectives are absent ?
is muchharder.
It has been the subject of much recent re-search since the release of the Penn Discourse Tree-bank 2.0 (PDTB) (Prasad et al, 2008), which anno-tates relations between adjacent text spans in WallStreet Journal (WSJ) articles, while clearly distin-guishing implicit from explicit discourse relations.1Recent studies, for example, explored the utility ofvarious classes of features for the task, includinglinguistically informed features, context, constituentand dependency parse features, and features that en-code entity information or rely on language mod-els (Pitler et al, 2009; Lin et al, 2009; Louis et al,2010; Zhou et al, 2010).To date, however, there has not been a systematicstudy of combinations of these features for implicitdiscourse relation identification.
In addition, the re-sults of existing studies are often difficult to comparebecause of differences in data set creation, featureset choice, or experimental methodology.This paper provides a systematic study of previ-ously proposed features for implicit discourse re-lation identification and identifies feature combina-tions that optimize F1-score using forward selection(John et al, 1994).
We report the performance of ourbinary (one vs. rest) classifiers on the PDTB dataset for its four top-level discourse relation classes:COMPARISON, CONTINGENCY, EXPANSION, andTEMPORAL.
In each case, the resulting classifiersachieve the best F1-scores for the PDTB to date.
We1Research on implicit discourse relation recognition prior tothe release of the PDTB instead relied on synthetic data cre-ated by removing explicit connectives from explicit discourserelation instances (Marcu and Echihabi, 2002), but the trainedclassifiers do not perform as well on real-world data (Blair-Goldensohn et al, 2007).108further identify factors for feature extraction that canhave a major impact performance, including stem-ming and lexicon look-up.
Finally, by document-ing an easily replicable experimental methodologyand making public the code for feature extraction2,we hope to provide a new set of baselines for futurestudies of implicit discourse relation identification.2 DataThe experiments are conducted on the PDTB(Prasad et al, 2008), which provides discourse rela-tion annotations between adjacent text spans in WSJarticles.
Each training and test instance representsone such pair of text spans and is classified in thePDTB w.r.t.
its relation type and relation sense.In the work reported here, we use the relationtype to distinguish examples of explicit vs. implicitdiscourse relations.
In particular, we consider all in-stances with a relation type other than explicit asimplicit relations since they lack an explicit con-nective between the text spans.
The relation sensedetermines the relation that exists between its textspan arguments as one of: COMPARISON, CONTIN-GENCY, EXPANSION, and TEMPORAL.
For exam-ple, the following shows an explicit CONTINGENCYrelation between argument1 (arg1) and argument2(arg2), denoted via the connective ?because?
:The federal government suspended sales ofU.S.
savings bonds because Congress hasn?tlisted the ceiling on government debt.The four relation senses comprise the target classesfor our classifiers.A notable feature of the PDTB is that the anno-tation is done on the same corpus as Penn Tree-bank (Marcus et al, 1993), which provides parsetrees and part-of-speech (POS) tags.
This enablesthe use of gold standard parse information for somefeatures, e.g., the production rules feature, one ofthe most effective features proposed to date.3 FeaturesBelow are brief descriptions of features whose ef-ficacy have been empirically determined in priorworks3, along with the rationales behind them:2These are available from http://www.joonsuk.org.3Word Pairs (Marcu and Echihabi, 2002).
First-Last-First3(Wellner et al, 2006).
Polarity, Verbs, Inquirer Tags, Modality,Context (Pitler et al, 2009).
Production Rules (Lin et al, 2009).Word Pairs (cross product of unigrams: arg1 ?arg2) ?
A few of these word pairs may capture in-formation revealing the discourse relation of the tar-get spans.
For instance, rain-wet can hint at CON-TINGENCY.First-Last-First3 (the first, last, and first threewords of each argument) ?
The words in this rangemay be expressions that function as connectives forcertain relations.Polarity (the count of words in arg1 and arg2, re-spectively, that hold negated vs. non-negated posi-tive, negative, and neutral sentiment) according tothe MPQA corpus (Wilson et al, 2005)) ?
Thechange in sentiment from arg1 to arg2 could be agood indication of COMPARISON.Inquirer Tags (negated and non-negated fine-grained semantic classification tags for the verbs ineach argument and their cross product) ?
The tagsare drawn from the General Inquirer Lexicon (Stoneet al, 1966)4, which provides word level relationsthat might be propagated to the target spans?
dis-course relation, e.g., rise:fall.Verbs (count of pairs of verbs from arg1 and arg2belonging to the same Levin English Verb Class(Levin and Somers, 1993)5, the average lengths ofverb phrases as well as their cross product, and thePOS of the main verb from each argument) ?
LevinVerb classes provide a means of clustering verbsaccording to their meanings and behaviors.
Also,longer verb phrases might correlate with CONTIN-GENCY, indicating a justification.Modality (three features denoting the presence ofmodal verbs in arg1, arg2, or both) ?
Modal verbsoften appear in CONTINGENCY relations.Context (the connective and the sense of the im-mediately preceding and following relations (if ex-plicit), and a feature denoting if arg1 starts a para-graph) ?
Certain relations co-occur.Production Rules (three features denoting the pres-ence of syntactic productions in arg1, arg2 or both,based on all pairs of parent-children nodes in the ar-gument parse trees) ?
The syntactic structure of anargument can influence that of the other argument as4http://www.wjh.harvard.edu/ inquirer/inqdict.txt5http://www-personal.umich.edu/ jlawler/levin.html109well as its relation type.4 ExperimentsWe aim to identify the optimal subsets of the afore-mentioned features for each of the four top-levelPDTB discourse relation senses: COMPARISON,CONTINGENCY, EXPANSION, and TEMPORAL.
Inorder to provide a meaningful comparison with ex-isting work, we carefully follow the experimentsetup of Pitler et al (2009), the origin of the ma-jority of the features under consideration:First, sections 0-2 and 21-22 of the PDTB areused as the validation and test set, respectively.Then, we randomly down-sample sections 2-20 toconstruct training sets for each of the classifiers,where each set has the same number of positive andnegative instances with respect to the target rela-tion.
Since the composition of the correspondingtraining set has a noticeable impact on the classifierperformance we select a down-sampled training setfor each classifier through cross validation.
All in-stances of non-explicit relation senses are used; theENTREL type is considered as having the EXPAN-SION sense.6Second, Naive Bayes is used not only to duplicatethe Pitler et al (2009) setting, but also because itequaled or outperformed other learning algorithms,such as SVM and MaxEnt, in preliminary experi-ments, while requiring a significantly shorter train-ing time.7Prior to the feature selection experiments, the bestpreprocessing methods for feature extraction are de-termined through cross validation.
We consider sim-ple lowercasing, Porter Stemming, PTB-style tok-enization8, and hand-crafted rules for matching to-kens to entries in the polarity and General Inquirerlexicons.Then, feature selection is performed via forwardselection, in which we start with the single best-performing feature and, in each iteration, add thefeature that improves the F1-score the most, untilno significant improvement can be made.
Once the6Some prior work uses a different experimental setting.
Forinstance, Zhou et al (2010) only considers two of the non-explicit relations, namely Implicit and NoRel.7We use classifiers from the nltk package (Bird, 2006).8Stanford Parser (Klein and Manning, 2003).optimal feature set for each relation sense is deter-mined by testing on the validation set, we retraineach classifier using the entire training set and re-port final performance on the test set.5 Results and AnalysisTable 5 indicates the performance achieved by em-ploying the feature set found to be optimal for eachrelation sense via forward selection, along with theperformance of the individual features that consti-tute the ideal subset.
The two bottom rows show theresults reported in two previous papers with the mostsimilar experiment methodology as ours.
The no-table efficacy of the production rules feature, yield-ing the best or the second best result across all re-lation senses w.r.t.
both F1-score and accuracy, con-firms the finding of Zhou et al (2010).
In contrastto their work, however, combining existing featuresenhances the performance.
Below, we discuss theprimary observations gleaned from the experiments.Word pairs as features.
Starting with earlier worksthat proposed them as features (Marcu and Echihabi,2002), some form of word pairs has generally beenpart of feature sets for implicit discourse relationrecognition.
According to our research, however,these features provide little or no additional gain,once other features are employed.
This seems sensi-ble, since we now have a clearer idea of the types ofinformation important for the task and have devel-oped a variety of feature types, each of which aimsto represent these specific aspects of the discourserelation arguments.
Thus, general features like wordpairs may no longer have a role to play for implicitdiscourse relation identification.Preprocessing.
Preprocessing turned out to impactthe classifier performance immensely, especially forfeatures like polarity and inquirer tags that rely oninformation retrieved from a lexicon.
For these fea-tures, if a match for a given word is not found in thelexicon, no information is passed on to the classifier.As an example, consider the General Inquirer lex-icon.
Most of its verb entries are present tense singu-lar in form; thus, without stemming, dictionary lookup fails for a large portion of the verbs.
In our case,the F1-score increases by roughly 10% after stem-ming.Further tuning is possible by a few hand-written110Feature Type COMP.
vs Rest CONT.
vs Rest EXP.
vs Rest TEMP.
vs RestF1 Acc.
F1 Acc.
F1 Acc.
F1 Acc.1.
Polarity 16.49 46.82 28.47 61.39 64.20 56.80 13.58 50.692.
First-Last-First3 22.54 53.05 37.64 66.71 62.27 56.40 15.24 51.813.
Inquirer Tags 18.07 82.14 34.88 69.60 77.76 66.38 21.65 80.044.
Verbs 18.05 55.29 23.61 78.33 68.33 58.37 18.11 58.445.
Production Rules 30.04 75.84 47.80 71.90 77.64 69.60 20.96 63.36Best Combination 2 & 4 & 5 2 & 4 & 5 1 & 3 & 4 & 5 1 & 3 & 531.32 74.66 49.82 72.09 79.22 69.14 26.57 79.32Pitler ?09 (Best) 21.96 56.59 47.13 67.30 76.42 63.62 16.76 63.49Zhou ?10 (Best)* 31.79 58.22 47.16 48.96 70.11 54.54 20.30 55.48* The experiments are conducted under a slightly different setting, as described in Section 4.Table 1: Summary of Classifier Performance.
4-way classifiers have been tested as well, but their performance is notas good as that of the binary classifiers shown here.
One major difference is that it is harder to balance the number ofinstances across all the classes when training 4-way classifiers.rules to guide lexicon lookup.
The word supplied,for instance, becomes suppli after stemming, whichstill fails to match the lexicon entry supply, unlessadjusted accordingly.Binning.
An additional finding regards featuresthat capture numeric, rather than binary, informa-tion, such as polarity.
Since this feature encodes thecounts of each type of sentiment word (with respectto each argument and their cross product), and NaiveBayes can only interpret binary features, we first em-ployed a binning mechanism with each bin coveringa single value.
For instance, if arg1 consists of threepositive words, we included arg1pos1, arg1pos2 andarg1pos3 as features instead of just arg1pos3.The rationale behind binning is that it capturesthe proximity of related instances.
Imagine havingthree instances each with one, two, and three pos-itive words in arg1, respectively.
Without binning,the features added are simply arg1pos1, arg1pos2,arg1pos3, respectively.
From the perspective of theclassifier, the third instance is no more similar to thesecond instance than it is to the first instance, eventhough having three positive words is clearly closerto having two positive words than having one posi-tive word.
With binning, this proximity is capturedby the fact that the first instance has just one fea-ture in common with the third instance, whereas thesecond instance has two.Binning, however, significantly degrades perfor-mance on most of the classification tasks.
One pos-sible explanation is that these features function as anabstraction of certain lexical patterns, rather than di-rectly capturing similarities among instances of thesame class.6 ConclusionWe employ a simple greedy feature selection ap-proach to identify subsets of known features forimplicit discourse relation identification that yieldthe best performance to date w.r.t.
F1-score on thePDTB data set.
We also identify aspects of featureset extraction and representation that are crucial forobtaining state-of-the-art performance.
Possible fu-ture work includes evaluating the performance with-out using the gold standard parses.
This will give abetter idea of how the features that rely on parseroutput will perform on real-world data where nogold standard parsing information is available.
Inthis way, we can ensure that findings in this area ofresearch bring practical gains to the community.AcknowledgmentsWe would like to thank Annie Louis and Yu Xu for help-ing us reimplement the systems from Louis et al (2010)and Zhou et al (2010), respectively.
We also thank theanonymous reviewers for their helpful feedback.
Thiswork was supported in part by National Science Founda-tion Grants IIS-1111176 and IIS-0968450, and by a giftfrom Google.111ReferencesSteven Bird.
2006.
Nltk: the natural language toolkit.
InProceedings of the COLING/ACL on Interactive pre-sentation sessions, COLING-ACL ?06, pages 69?72,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Sasha Blair-Goldensohn, Kathleen McKeown, and OwenRambow.
2007.
Building and refining rhetorical-semantic relation models.
In HLT-NAACL, pages 428?435.G.
John, R. Kohavi, and K. Pfleger.
1994.
IrrelevantFeatures and the Subset Selection Problem.
In W. Co-hen and H. Hirsh, editors, Proceedings of the EleventhInternational Conference on Machine Learning, pages121?129.
Morgan Kaufmann.Dan Klein and Christopher D. Manning.
2003.
Accu-rate unlexicalized parsing.
In IN PROCEEDINGS OFTHE 41ST ANNUAL MEETING OF THE ASSOCIA-TION FOR COMPUTATIONAL LINGUISTICS, pages423?430.Beth Levin and Harold Somers.
1993.
English VerbClasses and Alternations: A Preliminary Investiga-tion.
University Of Chicago Press.Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng.
2009.Recognizing implicit discourse relations in the penndiscourse treebank.
In EMNLP, pages 343?351.Annie Louis, Aravind K. Joshi, Rashmi Prasad, and AniNenkova.
2010.
Using entity features to classify im-plicit discourse relations.
In SIGDIAL Conference,pages 59?62.Daniel Marcu and Abdessamad Echihabi.
2002.
Anunsupervised approach to recognizing discourse rela-tions.
In ACL, pages 368?375.M.
Marcus, M. Marcinkiewicz, and B. Santorini.
1993.Building a Large Annotated Corpus of English:The Penn Treebank.
Computational Linguistics,19(2):313?330.Emily Pitler, Mridhula Raghupathy, Hena Mehta, AniNenkovak, Alan Lee, and Aravind K. Joshi.
2008.Easily identifiable discourse relations.
In COLING(Posters), pages 87?90.Emily Pitler, Annie Louis, and Ani Nenkova.
2009.
Au-tomatic sense prediction for implicit discourse rela-tions in text.
In ACL/AFNLP, pages 683?691.Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-sakaki, Livio Robaldo, Aravind Joshi, and Bon-nie Webber.
2008.
The penn discourse tree-bank 2.0.
In Bente Maegaard Joseph Mariani JanOdjik Stelios Piperidis Daniel Tapias Nicoletta Cal-zolari (Conference Chair), Khalid Choukri, editor,Proceedings of the Sixth International Conferenceon Language Resources and Evaluation (LREC?08),Marrakech, Morocco, may.
European LanguageResources Association (ELRA).
http://www.lrec-conf.org/proceedings/lrec2008/.P J Stone, D C Dunphy, M S Smith, and D M Ogilvie.1966.
The General Inquirer: A Computer Approachto Content Analysis, volume 08.
MIT Press.Ben Wellner, Lisa Ferro, Warren R. Greiff, and LynetteHirschman.
2006.
Reading comprehension tests forcomputer-based understanding evaluation.
NaturalLanguage Engineering, 12(4):305?334.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-levelsentiment analysis.
In HLT/EMNLP.Zhi-Min Zhou, Yu Xu, Zheng-Yu Niu, Man Lan, Jian Su,and Chew Lim Tan.
2010.
Predicting discourse con-nectives for implicit discourse relation recognition.
InCOLING (Posters), pages 1507?1514.112
