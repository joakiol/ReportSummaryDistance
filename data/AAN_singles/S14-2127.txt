Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 711?715,Dublin, Ireland, August 23-24, 2014.ULisboa: Identification and Classification of Medical ConceptsAndr?e Leal+, Diogo Gonc?alves+, Bruno Martins?, and Francisco M. Couto++LASIGE, Faculdade de Ci?encias, Universidade de Lisboa, 1749-016 Lisboa, Portugal.
?INESC-ID, Instituto Superior T?ecnico, Universidade de Lisboa, Portugal{aleal,dgoncalves}@lasige.di.fc.ul.pt , bruno.g.martins@ist.ul.pt , fcouto@di.fc.ul.ptAbstractThis paper describes our participation onTask 7 of SemEval 2014, which fo-cused on the recognition and disambigua-tion of medical concepts.
We used anadapted version of the Stanford NER sys-tem to train CRF models to recognize tex-tual spans denoting diseases and disor-ders, within clinical notes.
We consid-ered an encoding that accounts with non-continuous entities, together with a richset of features (i) based on domain spe-cific lexicons like SNOMED CT, or (ii)leveraging Brown clusters inferred from alarge collection of clinical texts.
Togetherwith this recognition mechanism, we useda heuristic similarity search method, toassign an unambiguous identifier to eachconcept recognized in the text.Our best run on Task A (i.e., in the recog-nition of medical concepts in the text)achieved an F-measure of 0.705 in thestrict evaluation mode, and a promisingF-measure of 0.862 in the relaxed mode,with a precision of 0.914.
For Task B (i.e.,the disambiguation of the recognized con-cepts), we achieved less promising results,with an accuracy of 0.405 in the strictmode, and of 0.615 in the relaxed mode.1 IntroductionCurrently, many off-the-shelf named entity recog-nition solutions are available, and these can beused to recognize mentions in clinical notes de-noting diseases and disorders.
We decided to usethe Stanford NER tool (Finkel et al., 2005) to trainCRF models based on annotated biomedical text.This work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/The use of unsupervised methods for inferringword representations is nowadays also known toincrease the accuracy of entity recognition mod-els (Turian et al., 2010).
Thus, we also used Brownclusters (Brown et al., 1992; Turian et al., 2009)inferred from a large collection of non-annotatedclinical texts, together with domain specific lexi-cons, to build features for our CRF models.An important challenge in entity recognition re-lates to the recognition of overlapping and non-continuous entities (Alex et al., 2007).
In thispaper, we describe how we modified the Stan-ford NER system to be able to recognize non-continuous entities, through an adapted version ofthe SBIEO scheme (Ratinov and Roth, 2009).Besides the recognition of medical concepts, wealso present the strategy used to map each of therecognized concepts into a SNOMED CT identi-fier (Cornet and de Keizer, 2008).
This task isparticularly challenging, since there are many am-biguous cases.
We describe our general approachto address the aforementioned CUI mapping prob-lem, based on similarity search and on the infor-mation content of SNOMED CT concept names.2 Task and DatasetsTask 7 of SemEval 2014 actually consisted of twosmaller tasks: recognition of mentions of medi-cal concepts (Task A) and mapping each medicalconcept, recognized in clinical notes, to a uniqueUMLS CUI (Task B).
In the first task, recogni-tion of medical concepts, systems have to detectcontinuous and discontinuous medical conceptsthat belong to the UMLS semantic group disor-ders.
The second task, concerning with normal-ization and mapping, is limited to UMLS CUIsof SNOMED CT codes (i.e., although the UMLSmeta-thesaurus integrates several resources, weare only interested in SNOMED CT).
Each con-cept that was previously recognized can have aunique CUI associated to it, or none at all (CUI-711LESS).
The goal here is to disambiguate the con-cepts and choose the right CUI for each case.
Forsupporting the recognition and CUI mapping ofmedical concepts, we retrieved the disorders sub-set of SNOMED CT directly from UMLS1.The evaluation can be done in a strict or a ina relaxed way.
For the case of strict evaluation,an exact match must be achieved in the recogni-tion, by having correct start and end offsets, withinthe text, for the continuous concepts, and a cor-rect set of start and end offsets for the discontin-uous concepts.
In the relaxed evaluation, there issome space for errors in the offset values from therecognition task.
If there is some overlap betweenthe concepts, then the result is considered a partialmatch, otherwise it is a recognition error.A set of annotated biomedical texts was givento the participants, separated in three categories:trial, development and training.
We also received afinal test set, and a large set of non-annotated texts.All the provided texts were initially converted intoa common tokenized format, to be used as inputto the tools that we considered for developing ourapproach.
After processing, we converted the re-sults back into the format used by SemEval 2014,this way generating the official runs.3 Entity RecognitionOur entity recognition approach was based on theusage of the Stanford NER software, which em-ploys a linear chain Conditional Random Field(CRF) approach for building probabilistic mod-els based on training data (Finkel et al., 2005).In Stanford NER, model training is based on theL-BFGS algorithm, and model decoding is madethrough the Viterbi algorithm.This tool requires all input texts to be tokenizedand encoded according to a named entity recog-nition scheme such as SBIEO (Ratinov and Roth,2009), characterized by only being able to recog-nize continuous entities.
As we also need to rec-ognize non-continuous entities, we modified theStanford NER software to use a SBIEON encod-ing scheme.
This new scheme has the followingspecific token-tag associations:S: Single, that indicates if the token individuallyconstitutes an entity to be recognized.B: Begin, identifying the beginning of the entity.This tag is only given to the first word of the1http://www.nlm.nih.gov/research/umls/entity, being followed is most cases by tokenslabeled as being inside the entity.I: Inside, representing the continuation of a nonsingle word entity (i.e., the middle tokens).E: Ending, representing the last word in the caseof entities composed by more than one word.N: Non-Continuous, which identifies all thewords that are between the beginning and theend of an entity, but that do not belong to it.This label specifically allows us to model thein-between tokens of non-continuous entities.O: Other, which is associated to all other wordsthat are not part of entities.We developed a Java parser that converts thebiomedical text, provided to the participants, intoa tokenized format.
This tokenized format, in thecase of the annotated texts, associates individualtokens to their SBIEON or SBIEO tags, so that thedatasets can be used as input to train CRF models.3.1 Concept Recognition ModelsAs we said, SBIEON tokenization differs fromSBIEO by the fact that the first one gives supportto non-continuous entities.
Based on these two in-put schemes, we generated two different models:Only continuous entities: A 2nd-order CRFmodel was trained based on the SBIOE entity en-coding scheme, which only recognizes continuousentities.
Non-continuous and overlapping entitieswill thus, in this case, only be partially modeled(i.e., we only considered the initial span of text as-sociated to the non-continuous entities).Non-continuous entities: A 2nd-order CRFmodel was trained based on the SBIOEN entityencoding scheme, accepting continuous and non-continuous entities, although still not supportingthe case of overlapping entities.
In these last cases,only the first entity in each of the overlappinggroups will be modeled correctly, while the oth-ers will only be partially modeled (i.e., by onlyconsidering the non-overlapping spans).Our CRF models relied on a standard set of fea-ture templates that includes (i) word tokens withina window of size 2, (ii) the token shape (e.g., if itis uppercased, capitalized, numeric, etc.
), (iii) to-ken prefixes and suffixes, (iv) token position (e.g.,at the beginning or ending of a sentence), and (v)conjunctions of the current token with the previ-ous 2 tags.
Besides these standard features, wealso considered (a) domain-specific lexicons, and(b) word representations based on Brown clusters.7123.2 Word ClustersIn addition to the annotated training dataset, par-ticipants were also provided with 403876 non-annotated texts, containing a total of 828509 to-kens.
We used this information to induce general-ized cluster-based word representations.Brown et al.
proposed a greedy agglomera-tive hierarchical clustering procedure that groupswords to maximize the mutual information of bi-grams (Brown et al., 1992).
According to Brown?sclustering procedure, clusters are initialized asconsisting of a single word each, and are thengreedily merged according to a mutual informa-tion criterion, based on bi-grams, to form a lower-dimensional representation of a vocabulary thatcan mitigate the feature sparseness problem.
Inthe context of named entity recognition studies,several authors have previously noted that usingthese types of cluster-based word representationscan indeed result in improvements (Turian et al.,2009).
The hierarchical nature of the clusteringallows words to be represented at different levelsin the hierarchy and, in our case, we considered1000 different clusters of similar words.We specifically used the set of training docu-ments, together with the non-annotated documentsthat were provided by the organizers, to induceword representations based on Brown?s clusteringprocedure, using an open-source implementationthat follows the description given by (Turian et al.,2010).
The word clusters are latter used as featureswithin the Stanford NER package, by consideringthat each word can be replaced by the correspond-ing cluster, this way adding some other back-offfeatures to the models (i.e., features that are lesssparse, in the sense that they will appear more fre-quently associated to some of the instances).4 Disambiguating ConceptsFor mapping entities to concept IDs (Task B), weused a heuristic method based on similarity search,supported on Lucene indexes (MacCandless et al.,2010).
We look for SNOMED CT concepts thathave a high n-gram overlap with the entity nameoccurring in the text, together with the informationcontent of each SNOMED CT concept.In our implementation, we used Lucene to re-trieve candidate SNOMED CT concepts accordingto different string distance algorithms: the NGramdistance (Kondrak, 2005) first, then according tothe Jaro-Winkler distance (Winkler, 1990), and fi-nally according to the Levenshtein distance.
Themost similar candidate is chosen as the disam-biguation.
The specific order for the similar-ity metrics was based on the intuition that met-rics based on individual character-level matchesare probably not as informative as metrics basedon longer sequences of characters, although theycan be useful for dealing with spelling variations.However, for future work, we plan to explore moresystematic approaches (e.g., based on learning torank) for combining multiple similarity metrics.Additionally to the aforementioned similaritymetrics, a measure of the Information Content (IC)of each SNOMED CT concept was also employed,to further disambiguate the mappings (i.e., to se-lect the SNOMED CT identifier that is more gen-eral, and thus more likely to be associated to a par-ticular concept descriptor).
Notice that the IC ofa concept corresponds to a measure of its speci-ficity, where higher values correspond to morespecific concepts, and lower values to more gen-eral ones.
Given the frequency freq(c) for eachconcept c in a corpus (i.e., the same corpus thatwas used to infer the word clusters), the informa-tion content of this concept can be computed fromthe ratio between its frequency (including its de-scendants) and the maximum frequency of all con-cepts (Resnik, 1995):IC(c) = ?
log(freq(c)maxFreq)In the formula, maxFreq represents the maximumfrequency of a concept, i.e.
the frequency of theroot concept, when it exists.
The frequency of aconcept can be computed using an extrinsic ap-proach that counts the exact matches of the con-cept names on a large text corpus.5 Evaluation ExperimentsWe submitted three distinct runs to the SemEvalcompetition.
These runs were as follows:Run 1: A SBIOEN model was used to recog-nize non-continuous entities.
This model wastrained using only the annotated texts fromthe provided training set.
We also used somedomain specific lexicons like SNOMED CT,or lists with names for drugs and diseasesretrieved from DBPedia.
Finally, the recog-nition model also used Brown clusters gen-erated from the non-annotated datasets pro-vided in the competition.713For assigning a SNOMED CT identifier toeach entity, we used the disambiguation tech-nique supported by Lucene indexes.
Inthis specific run we used all the consideredheuristics for similarity search.Run 2: A simpler model based on the SBIOEscheme was used in this case, which can onlyrecognize continuous entities.
The same fea-tures from Run 1 were used for training therecognition model.For assigning the SNOMED CT identifier toeach entity, we also used the same strategythat was presented for Run 1.Run 3: A similar SBIOE model to that from Run2 was used for the recognition.For assigning the corresponding SNOMEDCT identifier to each entity, we in this caselimited the heuristic rules that were used.Instead of using the string similarity algo-rithms, we used only exact matches, togetherwith the information content measure and theneighboring terms for disambiguation.6 Results and DiscussionWe present our official results in Table 1, whichhighlights our best results for each task.Specifically in Task A, we achieved encourag-ing results.
Run 1 achieved an F-measure of 0.705in the strict evaluation, and of 0.862 in the relaxedevaluation.
Since Runs 2 and 3 used the samerecognition strategy (i.e., models that attempted tocapture only the continuous entities), we obtainedthe same results for Task A in both these runs.
Ta-ble 1 also shows that our performance in Task Bwas significantly lower than in Task A.As we can see in the table, our first run was theone with the best results for Task A.
The modelused on this run recognizes non-continuous enti-ties, and this is perhaps the main reason for thehigher results (i.e., the other two runs used thesame features for the recognition models).On what concerns the results of Task B, it is im-portant to notice the distinct results from the firstand second runs, which used exactly the same dis-ambiguation strategy.
The differences in the re-sults are a consequence from the use of a differentrecognition model in Task A.
We can see that theability to recognize non-continuous entities leadsto the generation of worse mappings, when con-sidering our specific disambiguation strategy.
Ourlast run is the best in terms of the performance overTask B, but the difference is subtle.7 Conclusions and Future WorkThis paper described our participation in Task 7 ofthe SemEval 2014 competition, which was dividedinto two subtasks, namely (i) the recognition ofcontinuous and non-continuous medical concepts,and (ii) the mapping of each recognized concept toa SNOMED CT identifier.For the first task, we used the Stanford NERsoftware (Finkel et al., 2005), modified by usto recognize not only continuous, but also non-continuous entities.
This was possible by intro-ducing the SBIEON scheme, derived from the tra-ditional SBIEO encoding.
To increase the accu-racy and precision of the recognition we have alsoused domain specific lexicons and Brown clustersinferred from non-annotated documents.For the second task, we used a heuristic methodbased on similarity search, for matching conceptsin the text against concepts from SNOMED CT,together with a measure of information contentto disambiguate the cases of term polysemy inSNOMED CT. We implemented our disambigua-tion approach through the Lucene software frame-work (MacCandless et al., 2010).In the first task (Task A) we achieved some par-ticularly encouraging results, showing that an off-the-shelf NER system can be easily adapted tothe recognition of medical concepts in biomedi-cal text.
Our specific modifications to the StanfordNER system, in order to support the recognition ofnon-continuous entity names, indeed increased theprecision and recall on Task A.
However, our ap-proach for the disambiguation of the recognizedconcepts (Task B) performed much worse, achiev-ing an accuracy of 0.615 in the case of the relaxedevaluation.
Future developments will therefore fo-cus on improving the component that addressedthe entity disambiguation subtask.Specifically on what regards future work, weplan to experiment with the usage of machinelearning methods for the disambiguation subtask,instead of relying on a purely heuristic approach.We are interested in experimenting with the us-age of Learning to Rank (L2R) methods, similar tothose employed on the DNorm system (Leaman etal., 2013), to optimally combine different heuris-tics such as the ones that were used in our currentapproach.
A L2R model can be used to rank candi-714Task A Task BStrict Evaluation Relaxed Evaluation Strict RelaxedRun Precision Recall F-measure Precision Recall F-measure Accuracy Accuracy1 0.753 0.663 0.705 0.914 0.815 0.862 0.402 0.6062 0.752 0.660 0.703 0.909 0.806 0.855 0.404 0.6123 0.752 0.660 0.703 0.909 0.806 0.855 0.405 0.615Table 1: Our official results for Tasks A and B of the SemEval challenge focusing on clinical text.date disambiguations (e.g., retrieved through sim-ilarity search with basis on Lucene) according to acombination of multiple criteria, and we can thenchoose the top candidate as the disambiguation.Additionally, we plan to use ontology-basedsimilarity measures to validate and improve themappings (Couto and Pinto, 2013).
For example,by assuming that all entities in a given span of textare semantically related with each other, we canuse ontology relations to filter likely misannota-tions (Grego and Couto, 2013; Grego et al., 2013).AcknowledgmentsThe authors would like to thank Fundac?
?ao para aCi?encia e Tecnologia (FCT) for the financial sup-port of SOMER (PTDC/EIA-EIA/119119/2010),LASIGE (PEst-OE/EEI/UI0408/2014) andINESC-ID (Pest-OE/EEI/LA0021/2013).We also would like to thank our colleaguesBerta Alves, for her support in evaluating devel-opment errors, and Lu?
?s Atalaya, for the develop-ment of some of the data pre-processing scripts.ReferencesBeatrice Alex, Barry Haddow, and Claire Grover.2007.
Recognising nested named entities in biomed-ical text.
In Proceedings of the ACL-07 Workshopon Biological, Translational, and Clinical LanguageProcessing, pages 65?72.Peter F. Brown, Peter V. deSouza, Robert L. Mer-cer, Vincent J. Della Pietra, and Jenifer C. Lai.1992.
Class-based n-gram models of natural lan-guage.
Computational linguistics, 18(4):467?479.Ronald Cornet and Nicolette de Keizer.
2008.
Fortyyears of SNOMED: A literature review.
BMCMedical Informatics and Decision Making, 8(Suppl1:S2):1?6.Francisco M. Couto and Helena Sofia Pinto.
2013.
Thenext generation of similarity measures that fully ex-plore the semantics in biomedical ontologies.
Jour-nal of Bioinformatics and Computational Biology,11(05):1?11.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by Gibbssampling.
In Proceedings of the 43rd Annual Meet-ing on Association for Computational Linguistics,pages 363?370.Tiago Grego and Francisco M Couto.
2013.
Enhance-ment of chemical entity identification in text usingsemantic similarity validation.
PloS ONE, 8(5):1?9.Tiago Grego, Francisco Pinto, and Francisco Couto.2013.
LASIGE: using conditional random fields andchebi ontology.
In Proceedings of the 7th Inter-national Workshop on Semantic Evaluation, pages660?666.Grzegorz Kondrak.
2005.
N-gram similarity and dis-tance.
In Proceedings of the 12th InternationalConference String Processing and Information Re-trieval, pages 115?126.Robert Leaman, Rezarta Islamaj Do?gan, and Zhiy-ong Lu.
2013.
DNorm: disease name normaliza-tion with pairwise learning to rank.
Bioinformatics,29(22):2909?2917.Michael MacCandless, Erik Hatcher, and Otis Gospod-neti?c.
2010.
Lucene in Action.
Manning Publica-tions Company.Lev Ratinov and Dan Roth.
2009.
Design challengesand misconceptions in named entity recognition.
InProceedings of the 13th Conference on Computa-tional Natural Language Learning, pages 147?155.Philip Resnik.
1995.
Using information content toevaluate semantic similarity in a taxonomy.
In Pro-ceedings of the 14th International Joint Conferenceon Artificial Intelligence, pages 448?453.Joseph Turian, Lev Ratinov, Yoshua Bengio, and DanRoth.
2009.
A preliminary evaluation of word rep-resentations for named-entity recognition.
In Pro-ceedings of the NIPS-09 Workshop on Grammar In-duction, Representation of Language and LanguageLearning, pages 1?8.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: A simple and general methodfor semi-supervised learning.
In Proceedings of the48th Annual Meeting of the Association for Compu-tational Linguistics, pages 384?394.William E Winkler.
1990.
String comparator metricsand enhanced decision rules in the Fellegi-Suntermodel of record linkage.
In Proceedings of the Sec-tion on Survey Research of the American StatisticalAssociation, pages 354?359.715
