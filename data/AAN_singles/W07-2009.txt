Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 48?53,Prague, June 2007. c?2007 Association for Computational LinguisticsSemEval-2007 Task 10: English Lexical Substitution TaskDiana McCarthyUniversity of SussexFalmer, East SussexBN1 9QH, UKdianam@sussex.ac.ukRoberto NavigliUniversity of Rome ?La Sapienza?Via Salaria, 11300198 Roma, Italynavigli@di.uniroma1.itAbstractIn this paper we describe the English LexicalSubstitution task for SemEval.
In the task,annotators and systems find an alternativesubstitute word or phrase for a target word incontext.
The task involves both finding thesynonyms and disambiguating the context.Participating systems are free to use any lex-ical resource.
There is a subtask which re-quires identifying cases where the word isfunctioning as part of a multiword in the sen-tence and detecting what that multiword is.1 IntroductionWord sense disambiguation (WSD) has been de-scribed as a task in need of an application.
Whilstresearchers believe that it will ultimately prove use-ful for applications which need some degree of se-mantic interpretation, the jury is still out on thispoint.
One problem is that WSD systems have beentested on fine-grained inventories, rendering the taskharder than it need be for many applications (Ideand Wilks, 2006).
Another significant problem isthat there is no clear choice of inventory for anygiven task (other than the use of a parallel corpusfor a specific language pair for a machine translationapplication).The lexical substitution task follows on fromsome previous ideas (McCarthy, 2002) to exam-ine the capabilities of WSD systems built by re-searchers on a task which has potential for NLPapplications.
Finding alternative words that canoccur in given contexts would potentially be use-ful to many applications such as question answer-ing, summarisation, paraphrase acquisition (Daganet al, 2006), text simplification and lexical acquisi-tion (McCarthy, 2002).
Crucially this task does notspecify the inventory for use beforehand to avoidbias to one predefined inventory and makes it eas-ier for those using automatically acquired resourcesto enter the arena.
Indeed, since the systems inSemEval did not know the candidate substitutes fora word before hand, the lexical resource is evaluatedas much as the context based disambiguation com-ponent.2 Task set upThe task involves a lexical sample of nouns, verbs,adjectives and adverbs.
Both annotators and sys-tems select one or more substitutes for the targetword in the context of a sentence.
The data wasselected from the English Internet Corpus of En-glish produced by Sharoff (2006) from the Inter-net (http://corpus.leeds.ac.uk/internet.html).
This isa balanced corpus similar in flavour to the BNC,though with less bias to British English, obtainedby sampling data from the web.
Annotators are notprovided with the PoS (noun, verb, adjective or ad-verb) but the systems are.
Annotators can provideup to three substitutes but all should be equally asgood.
They are instructed that they can provide aphrase if they can?t think of a good single word sub-stitute.
They can also use a slightly more generalword if that is close in meaning.
There is a ?NAME?response if the target is part of a proper name and?NIL?
response if annotators cannot think of a goodsubstitute.
The subjects are also asked to identify48if they feel the target word is an integral part ofa phrase, and what that phrase was.
This optionwas envisaged for evaluation of multiword detec-tion.
Annotators did sometimes use it for paraphras-ing a phrase with another phrase.
However, for anitem to be considered a constituent of a multiword,a majority of at least 2 annotators had to identify thesame multiword.1The annotators were 5 native English speakersfrom the UK.
They each annotated the entire dataset.All annotations were semi-automatically lemma-tised (substitutes and identified multiwords) unlessthe lemmatised version would change the meaningof the substitute or if it was not obvious what thecanonical version of the multiword should be.2.1 Data SelectionThe data set comprises 2010 sentences, 201 targetwords each with 10 sentences.
We released 300 forthe trial data and kept the remaining 1710 for thetest release.
298 of the trial, and 1696 of the testrelease remained after filtering items with less than2 non NIL and non NAME responses and a few witherroneous PoS tags.
The words included were se-lected either manually (70 words) from examinationof a variety of lexical resources and corpora or au-tomatically (131) using information in these lexicalresources.
Words were selected from those having anumber of different meanings, each with at least onesynonym.
Since typically the distribution of mean-ings of a word is strongly skewed (Kilgarriff, 2004),for the test set we randomly selected 20 words ineach PoS for which we manually selected the sen-tences 2 (we refer to these words as MAN) whilst forthe remaining words (RAND) the sentences were se-lected randomly.2.2 Inter Annotator AgreementSince we have sets of substitutes for each item andannotator, pairwise agreement was calculated be-tween each pair of sets (p1, p2 ?
P ) from each pos-sible pairing (P ) as?p1,p2?Pp1?p2p1?p2|P |1Full instructions given to the annotators are posted athttp://www.informatics.susx.ac.uk/research/nlp/mccarthy/files/instructions.pdf.2There were only 19 verbs due to an error in automatic se-lection of one of the verbs picked for manual selection of sen-tences.Pairwise inter-annotator agreement was 27.75%.73.93% had modes, and pairwise agreement with themode was 50.67%.
Agreement is increased if we re-move one annotator who typically gave 2 or 3 sub-stitutes for each item, which increased coverage butreduced agreement.
Without this annotator, inter-annotator agreement was 31.13% and 64.7% withmode.Multiword detection pairwise agreement was92.30% and agreement on the identification of theexact form of the actual multiword was 44.13%.3 ScoringWe have 3 separate subtasks 1) best 2) oot and 3)mw which we describe below.
3 In the equationsand results tables that follow we use P for precision,R for recall, and Mode P and Mode R where wecalculate precision and recall against the substitutechosen by the majority of annotators, provided thatthere is a majority.Let H be the set of annotators, T be the set of testitems with 2 or more responses (non NIL or NAME)and hi be the set of responses for an item i ?
T forannotator h ?
H .For each i ?
T we calculate the mode (mi) i.e.the most frequent response provided that there is aresponse more frequent than the others.
The set ofitems where there is such a mode is referred to asTM .
Let A (and AM ) be the set of items from T(or TM ) where the system provides at least one sub-stitute.
Let ai : i ?
A (or ai : i ?
AM ) be the setof guesses from the system for item i.
For each iwe calculate the multiset union (Hi) for all hi for allh ?
H and for each unique type (res) in Hi willhave an associated frequency (freqres) for the num-ber of times it appears in Hi.For example: Given an item (id 9999) for happy;asupposing the annotators had supplied answers asfollows:annotator responses1 glad merry2 glad3 cheerful glad4 merry5 jovial3The scoring measures are as described in the doc-ument at http://nlp.cs.swarthmore.edu/semeval/tasks/task10/task10documentation.pdf released with our trial data.49then Hi would be glad glad glad merry merrycheerful jovial.
The res with associated frequencieswould be glad 3 merry 2 cheerful 1 and jovial 1.best measures This requires the best file producedby the system which gives as many guesses as thesystem believes are fitting, but where the creditfor each correct guess is divided by the number ofguesses.
The first guess in the list is taken as thebest guess (bg).P =?ai:i?A?res?aifreqres|ai||Hi||A| (1)R =?ai:i?T?res?aifreqres|ai||Hi||T | (2)Mode P =?bgi?AM 1 if bg = mi|AM | (3)Mode R =?bgi?TM 1 if bg = mi|TM | (4)A system is permitted to provide more than oneresponse, just as the annotators were.
They cando this if they are not sure which response is bet-ter, however systems will maximise the score if theyguess the most frequent response from the annota-tors.
For P and R the credit is divided by the num-ber of guesses that a system makes to prevent a sys-tem simply hedging its bets by providing many re-sponses.
The credit is also divided by the number ofresponses from annotators.
This gives higher scoresto items with less variation.
We want to emphasisetest items with better agreement.Using the example for happy;a id 9999 above, ifthe system?s responses for this item was glad; cheer-ful the credit for a9999 in the numerator of P and Rwould be3+127 = .286For Mode P and Mode R we use the system?sfirst guess and compare this to the mode of the anno-tators responses on items where there was a responsemore frequent than the others.oot measures This allows a system to make up to10 guesses.
The credit for each correct guess is notdivided by the number of guesses.
This allows forthe fact that there is a lot of variation for the task andwe only have 5 annotators.
With 10 guesses there isa better chance that the systems find the responsesof these 5 annotators.
There is no ordering of theguesses and the Mode scores give credit where themode was found in one of the system?s 10 guesses.P =?ai:i?A?res?aifreqres|Hi||A| (5)R =?ai:i?T?res?aifreqres|Hi||T | (6)Mode P =?ai:i?AM 1 if any guess ?
ai = mi|AM |(7)Mode R =?ai:i?TM 1 if any guess ?
ai = mi|TM |(8)mw measures For this measure, a system mustidentify items where the target is part of a multiwordand what the multiword is.
The annotators do not allhave linguistics background, they are simply askedif the target is an integral part of a phrase, and if sowhat the phrase is.
Sometimes this option is usedby the subjects for paraphrasing a phrase of the sen-tence, but typically it is used when there is a mul-tiword.
For scoring, a multiword item is one witha majority vote for the same multiword with morethan 1 annotator identifying the multiword.Let MW be the subset of T for which thereis such a multiword response from a majority ofat least 2 annotators.
Let mwi ?
MW be themultiword identified by majority vote for item i.Let MWsys be the subset of T for which there is amultiword response from the system and mwsysibe a multiword specified by the system for item i.detection P =?mwsysi?MWsys 1 if mwi exists at i|MWsys| (9)detection R =?mwsysi?MW 1 if mwi exists at i|MW | (10)identification P =?mwsysi?MWsys 1 if mwsysi = mwi|MWsys| (11)50identification R =?mwsysi?MW 1 if mwsysi = mwi|MW | (12)3.1 BaselinesWe produced baselines using WordNet 2.1 (Miller etal., 1993a) and a number of distributional similaritymeasures.
For the WordNet best baseline we foundthe best ranked synonym using the criteria 1 to 4below in order.
For WordNet oot we found up to 10synonyms using criteria 1 to 4 in order until 10 werefound:1.
Synonyms from the first synset of the targetword, and ranked with frequency data obtainedfrom the BNC (Leech, 1992).2. synonyms from the hypernyms (verbs andnouns) or closely related classes (adjectives) ofthat first synset, ranked with the frequency data.3.
Synonyms from all synsets of the target word,and ranked using the BNC frequency data.4.
synonyms from the hypernyms (verbs andnouns) or closely related classes (adjectives) ofall synsets of the target, ranked with the BNCfrequency data.We also produced best and oot baselines usingthe distributional similarity measures l1, jaccard, co-sine, lin (Lin, 1998) and ?SD (Lee, 1999) 4.
We tookthe word with the largest similarity (or smallest dis-tance for ?SD and l1) for best and the top 10 for oot.For mw detection and identification we usedWordNet to detect if a multiword in WordNet whichincludes the target word occurs within a window of2 words before and 2 words after the target word.4 Systems9 teams registered and 8 participated, and two ofthese teams (SWAG and IRST) each entered two sys-tems, we distinguish the first and second systemswith a 1 and 2 suffix respectively.The systems all used 1 or more predefined inven-tories.
Most used web queries (HIT, MELB, UNT)or web data (Brants and Franz, 2006) (IRST2, KU,4We used 0.99 as the parameter for ?
for this measure.SWAG1, SWAG2, USYD, UNT) to obtain counts fordisambiguation, with some using algorithms to de-rive domain (IRST1) or co-occurrence (TOR) infor-mation from the BNC.
Most systems did not usesense tagged data for disambiguation though MELBdid use SemCor (Miller et al, 1993b) for filtering in-frequent synonyms and UNT used a semi-supervisedword sense disambiguation combined with a host ofother techniques, including machine translation en-gines.5 ResultsIn tables 1 to 3 we have ordered systems accord-ing to R on the best task, and in tables 4 to 6 ac-cording to R on oot.
We show all scores as per-centages i.e.
we multiply the scores in section 3by 100.
In tables 3 and 6 we show results usingthe subset of items which were i) NOT identified asmultiwords (NMWT) ii) scored only on non multi-word substitutes from both annotators and systems(i.e.
no spaces) (NMWS).
Unfortunately we do nothave space to show the analysis for the MAN andRAND subsets here.
Please refer to the task websitefor these results.
5 We retain the same ordering forthe further analysis tables when we look at subsetsof the data.
Although there are further differencesin the systems which would warrant reranking on anindividual analysis, since we combined the subanal-yses in one table we keep the order as for 1 and 4respectively for ease of comparison.There is some variation in rank order of the sys-tems depending on which measures are used.
6 KUis highest ranking on R for best.
UNT is best at find-ing the mode, particularly on oot, though it is themost complicated system exploiting a great manyknowledge sources and components.
IRST2 doeswell at finding the mode in best.
The IRST2 bestR score is lower because it supplied many answersfor each item however it achieves the best R scoreon the oot task.
The baselines are outperformed bymost systems.
The WordNet baseline outperformsthose derived from distributional methods.
The dis-tributional methods, especially lin, show promisingresults given that these methods are automatic and5The task website is at http://www.informatics.sussex.ac.uk/research/nlp/mccarthy/task10index.html.6There is not a big difference between P and R becausesystems typically supplied answers for most items.51Systems P R Mode P Mode RKU 12.90 12.90 20.65 20.65UNT 12.77 12.77 20.73 20.73MELB 12.68 12.68 20.41 20.41HIT 11.35 11.35 18.86 18.86USYD 11.23 10.88 18.22 17.64IRST1 8.06 8.06 13.09 13.09IRST2 6.95 6.94 20.33 20.33TOR 2.98 2.98 4.72 4.72Table 1: best resultsSystems P R Mode P Mode RWordNet 9.95 9.95 15.28 15.28lin 8.84 8.53 14.69 14.23l1 8.11 7.82 13.35 12.93lee 6.99 6.74 11.34 10.98jaccard 6.84 6.60 11.17 10.81cos 5.07 4.89 7.64 7.40Table 2: best baseline resultsdon?t require hand-crafted inventories.
As yet wehaven?t combined the baselines with disambiguationmethods.Only HIT attempted the mw task.
It outperformsall baselines from WordNet.5.1 Post Hoc AnalysisChoosing a lexical substitute for a given word isnot clear cut and there is inherently variation in thetask.
Since it is quite likely that there will be syn-onyms that the five annotators do not think of weconducted a post hoc analysis to see if the synonymsselected by the original annotators were better, onthe whole, than those in the systems responses.
Werandomly selected 100 sentences from the subset ofitems which had more than 2 single word substitutes,no NAME responses, and where the target word wasNMWT NMWSSystems P R P RKU 13.39 13.39 14.33 13.98UNT 13.46 13.46 13.79 13.79MELB 13.35 13.35 14.19 13.82HIT 11.97 11.97 12.55 12.38USYD 11.68 11.34 12.48 12.10IRST1 8.44 8.44 8.98 8.92IRST2 7.25 7.24 7.67 7.66TOR 3.22 3.22 3.32 3.32Table 3: Further analysis for bestSystems P R Mode P Mode RIRST2 69.03 68.90 58.54 58.54UNT 49.19 49.19 66.26 66.26KU 46.15 46.15 61.30 61.30IRST1 41.23 41.20 55.28 55.28USYD 36.07 34.96 43.66 42.28SWAG2 37.80 34.66 50.18 46.02HIT 33.88 33.88 46.91 46.91SWAG1 35.53 32.83 47.41 43.82TOR 11.19 11.19 14.63 14.63Table 4: oot resultsSystems P R Mode P Mode RWordNet 29.70 29.35 40.57 40.57lin 27.70 26.72 40.47 39.19l1 24.09 23.23 36.10 34.96lee 20.09 19.38 29.81 28.86jaccard 18.23 17.58 26.87 26.02cos 14.07 13.58 20.82 20.16Table 5: oot baseline resultsNMWT NMWSSystems P R P RIRST2 72.04 71.90 76.19 76.06UNT 51.13 51.13 54.01 54.01KU 48.43 48.43 49.72 49.72IRST1 43.11 43.08 45.13 45.11USYD 37.26 36.17 40.13 38.89SWAG2 39.95 36.51 40.97 37.75HIT 35.60 35.60 36.63 36.63SWAG1 37.49 34.64 38.36 35.67TOR 11.77 11.77 12.22 12.22Table 6: Further analysis for ootHIT WordNet BLP R P Rdetection 45.34 56.15 43.64 36.92identification 41.61 51.54 40.00 33.85Table 7: MW results52good reasonable badsys 9.07 19.08 71.85origA 37.36 41.01 21.63Table 8: post hoc resultsnot one of those identified as a multiword (i.e.
a ma-jority vote by 2 or more annotators for the same mul-tiword as described in section 2).
We then mixed thesubstitutes from the human annotators with those ofthe systems.
Three fresh annotators7 were given thetest sentence and asked to categorise the randomlyordered substitutes as good, reasonable or bad.
Wetake the majority verdict for each substitute, but ifthere is one reasonable and one good verdict, thenwe categorise the substitute as reasonable.
The per-centage of substitutes for systems (sys) and originalannotators (origA) categorised as good, reasonableand bad by the post hoc annotators are shown in ta-ble 8.
We see the substitutes from the humans havea higher proportion of good or reasonable responsesby the post hoc annotators compared to the substi-tutes from the systems.6 Conclusions and Future DirectionsWe think this task is an interesting one in which toevaluate automatic approaches of capturing lexicalmeaning.
There is an inherent variation in the taskbecause several substitutes may be possible for agiven context.
This makes the task hard and scoringis less straightforward than a task which has fixedchoices.
On the other hand, we believe the task tapsinto human understanding of word meaning and wehope that computers that perform well on this taskwill have potential in NLP applications.
Since apre-defined inventory is not used, the task allows usto compare lexical resources as well as disambigua-tion techniques without a bias to any predefined in-ventory.
It is possible for those interested in disam-biguation to focus on this, rather than the choice ofsubstitutes, by using the union of responses from theannotators in future experiments.7 AcknowledgementsWe acknowledge support from the Royal Society UK for fund-ing the annotation for the project, and for a Dorothy Hodgkin7Again, these were native English speakers from the UK.Fellowship to the first author.
We also acknowledge supportto the second author from INTEROP NoE (508011, 6th EUFP).
We thank the annotators for their hard work.
We thankSerge Sharoff for the use of his Internet corpus, Julie Weeds forthe software we used for producing the distributional similaritybaselines and Suzanne Stevenson for suggesting the oot task .ReferencesThorsten Brants and Alex Franz.
2006.
Web 1T 5-gramcorpus version 1.1.
Technical Report.Ido Dagan, Oren Glickman, Alfio Gliozzo, Efrat Mar-morshtein, and Carlo Strapparava.
2006.
Direct wordsense matching for lexical substitution.
In Proceed-ings of the 21st International Conference on Computa-tional Linguistics and 44th Annual Meeting of the As-sociation for Computational Linguistics, Sydney, Aus-tralia, July.
Association for Computational Linguistics.Nancy Ide and Yorick Wilks.
2006.
Making sense aboutsense.
In Eneko Agirre and Phil Edmonds, editors,Word Sense Disambiguation, Algorithms and Applica-tions, pages 47?73.
Springer.Adam Kilgarriff.
2004.
How dominant is the common-est sense of a word?
In Proceedings of Text, Speech,Dialogue, Brno, Czech Republic.Lillian Lee.
1999.
Measures of distributional similarity.In Proceedings of the 37th Annual Meeting of the As-sociation for Computational Linguistics, pages 25?32.Geoffrey Leech.
1992.
100 million words of English:the British National Corpus.
Language Research,28(1):1?13.Dekang Lin.
1998.
An information-theoretic definitionof similarity.
In Proceedings of the 15th InternationalConference on Machine Learning, Madison, WI.Diana McCarthy.
2002.
Lexical substitution as a task forwsd evaluation.
In Proceedings of the ACL Workshopon Word Sense Disambiguation: Recent Successes andFuture Directions, pages 109?115, Philadelphia, USA.George Miller, Richard Beckwith, Christine Fellbaum,David Gross, and Katherine Miller, 1993a.
Intro-duction to WordNet: an On-Line Lexical Database.ftp://clarity.princeton.edu/pub/WordNet/5papers.ps.George A. Miller, Claudia Leacock, Randee Tengi, andRoss T Bunker.
1993b.
A semantic concordance.
InProceedings of the ARPA Workshop on Human Lan-guage Technology, pages 303?308.
Morgan Kaufman.Serge Sharoff.
2006.
Open-source corpora: Using thenet to fish for linguistic data.
International Journal ofCorpus Linguistics, 11(4):435?462.53
