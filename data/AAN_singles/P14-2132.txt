Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 816?821,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsObservational Initialization of Type-Supervised TaggersHui Zhang?Department of Computer ScienceUniversity of Southern Californiahzhang@isi.eduJohn DeNeroGoogle, Inc.denero@google.comAbstractRecent work has sparked new interestin type-supervised part-of-speech tagging,a data setting in which no labeled sen-tences are available, but the set of allowedtags is known for each word type.
Thispaper describes observational initializa-tion, a novel technique for initializing EMwhen training a type-supervised HMMtagger.
Our initializer allocates probabil-ity mass to unambiguous transitions in anunlabeled corpus, generating token-levelobservations from type-level supervision.Experimentally, observational initializa-tion gives state-of-the-art type-supervisedtagging accuracy, providing an error re-duction of 56% over uniform initializationon the Penn English Treebank.1 IntroductionFor many languages, there exist comprehensivedictionaries that list the possible parts-of-speechfor each word type, but there are no corpora la-beled with the part-of-speech of each token in con-text.
Type-supervised tagging (Merialdo, 1994)explores this scenario; a model is provided withtype-level information, such as the fact that ?only?can be an adjective, adverb, or conjunction, butnot any token-level information about which in-stances of ?only?
in a corpus are adjectives.
Re-cent research has focused on using type-level su-pervision to infer token-level tags.
For instance,Li et al (2012) derive type-level supervision fromWiktionary, Das and Petrov (2011) and T?ackstr?omet al (2013) project type-level tag sets across lan-guages, and Garrette and Baldridge (2013) solicittype-level annotations directly from speakers.
Inall of these efforts, a probabilistic sequence modelis trained to disambiguate token-level tags that are?Research conducted during an internship at Google.constrained to match type-level tag restrictions.This paper describes observational initialization,a simple but effective learning technique for train-ing type-supervised taggers.A hidden Markov model (HMM) can be usedto disambiguate tags of individual tokens by max-imizing corpus likelihood using the expectationmaximization (EM) algorithm.
Our approach ismotivated by a suite of oracle experiments thatdemonstrate the effect of initialization on the fi-nal tagging accuracy of an EM-trained HMM tag-ger.
We show that initializing EM with accuratetransition model parameters is sufficient to guidelearning toward a high-accuracy final model.Inspired by this finding, we introduce obser-vational initialization, which is a simple methodto heuristically estimate transition parameters fora corpus using type-level supervision.
Transi-tion probabilities are estimated from unambiguousconsecutive tag pairs that arise when two consec-utive words each have only a single allowed tag.These unambiguous word pairs can be tagged cor-rectly without any statistical inference.
InitializingEM with the relative frequency of these unambigu-ous pairs improves tagging accuracy dramaticallyover uniform initialization, reducing errors by56% in English and 29% in German.
This efficientand data-driven approach gives the best reportedtagging accuracy for type-supervised sequencemodels, outperforming the minimized model ofRavi and Knight (2009), the Bayesian LDA-basedmodel of Toutanova and Johnson (2008), and anHMM trained with language-specific initializationdescribed by Goldberg et al (2008).2 Type-Supervised TaggingA first-order Markov model for part-of-speechtagging defines a distribution over sentences forwhich a single tag is given to each word token.Let wi?W refer to the ith word in a sentence w,drawn from language vocabulary W .
Likewise,816ti?
T is the tag in tag sequence t of the ith word,drawn from tag inventory T .
The joint probabil-ity of a sentence can be expressed in terms of twosets of parameters for conditional multinomial dis-tributions: ?
defines the probability of a tag givenits previous tag and ?
defines the probability of aword given its tag.P?,?
(w, t) =|w|?i=1P?
(ti|ti?1) ?
P?
(wi|ti)Above, t0is a fixed start-of-sentence tag.For a set of sentences S, the EM algorithm canbe used to iteratively find a local maximum of thecorpus log-likelihood:`(?, ?
;S) =?w?Sln[?tP?,?
(w, t)]The parameters ?
and ?
can then be used to predictthe most likely sequence of tags for each sentenceunder the model:?t(w) = arg maxtP?,?
(w, t)Tagging accuracy is the fraction of these tags in?t(w) that match hand-labeled oracle tags t?
(w).Type Supervision.
In addition to an unlabeledcorpus of sentences, type-supervised models alsohave access to a tag dictionary D ?
W ?
T thatcontains all allowed word-tag pairs.
For an EM-trained HMM, initially setting P?
(w|t) = 0 for all(w, t) /?
D ensures that all words will be labeledwith allowed tags.Tag dictionaries can be derived from varioussources, such as lexicographic resources (Li etal., 2012) and cross-lingual projections (Das andPetrov, 2011).
In this paper, we will follow pre-vious work in deriving the tag dictionary froma labeled corpus (Smith and Eisner, 2005); thissynthetic setting maximizes experiment repeata-bility and allows for direct comparison of type-supervised learning techniques.Transductive Applications.
We consider atransductive data setting in which the test set isavailable during training.
In this case, the modelis not required to generalize to unseen examples orunknown words, as in the typical inductive setting.Transductive learning arises in document clus-tering and corpus analysis applications.
For ex-ample, before running a document clustering al-gorithm on a fixed corpus of documents, it may beuseful to tag each word with its most likely part-of-speech in context, disambiguating the lexicalfeatures in a bag-of-words representation.
In cor-pus analysis or genre detection, it may be usefulto determine for a fixed corpus the most commonpart-of-speech for each word type, which could beinferred by tagging each word with its most likelypart-of-speech.
In both cases, the set of sentencesto tag is known in advance of learning.3 Initializing HMM TaggersThe EM algorithm is sensitive to initialization.
Ina latent variable model, different parameter valuesmay yield similar data likelihoods but very differ-ent predictions.
We explore this issue via exper-iments on the Wall Street Journal section of theEnglish Penn Treebank (Marcus et al, 1993).
Weadopt the transductive data setting introduced bySmith and Eisner (2005) and used by Goldwa-ter and Griffiths (2007), Toutanova and Johnson(2008) and Ravi and Knight (2009); models aretrained on all sections 00-24, the tag dictionary Dis constructed by allowing all word-tag pairs ap-pearing in the entire labeled corpus, and the tag-ging accuracy is evaluated on a 1005 sentence sub-set sampled from the corpus.The degree of variation in tagging accuracy dueto initialization can be observed most clearly bytwo contrasting initializations.
UNIFORM initial-izes the model with uniform distributions over al-lowed outcomes:P?(t|t?)
=1|T |P?
(w|t) =1|{w : (w, t) ?
D}|SUPERVISED is an oracle setting that initializesthe model with the relative frequency of observedpairs in a labeled corpus:P?(t|t?)
??(w,t?)|w|?i=1?
((t?i, t?i?1), (t, t?))P?
(w|t) ??(w,t?)|w|?i=1?
((wi, t?i), (w, t))where the Kronecker ?
(x, y) function is 1 if x andy are equal and 0 otherwise.Figure 1 shows that while UNIFORM andSUPERVISED achieve nearly identical data log-likelihoods, their final tagging accuracy differs by8177080901000 5 10 15 20 25 30-13-11-9-7SUPERVISEDUNIFORM7080901000 5 10 15 20 25 30Data Log-Likelihood (106)Tagging Accuracy (%)94.1%82.1%96.7%72.0%93.7%91.0%92.8%93.5%Number of Iterations of Expectation Maximization-13-11-9-7SUPERVISED TRANSITIONSSUPERVISED EMISSIONSData Log-Likelihood (106)Tagging Accuracy (%)Number of Iterations of Expectation Maximization7080901000 5 10 15 20 25 3093.7%92.1%89.2%93.5%-13-11-9-7SUPERVISED TRANSITIONSOBSERVATIONALData Log-Likelihood (106)Tagging Accuracy (%)Number of Iterations of Expectation MaximizationFigure 1: The data log-likelihood (top) and tag-ging accuracy (bottom) of two contrasting initial-izers, UNIFORM and SUPERVISED, compared onthe Penn Treebank.7080901000 5 10 15 20 25 30-13-11-9-7SUPERVISEDUNIFORM7080901000 5 10 15 20 25 30Data Log-Likelihood (106)Tagging Accuracy (%)94.1%82.1%96.7%72.0%93.7%91.0%92.8%93.5%Number of Iterations of Expectation Maximization-13-11-9-7SUPERVISED TRANSITIONSSUPERVISED EMISSIONSData Log-Likelihood (106)Tagging Accuracy (%)Number of Iterations of Expectation Maximization7080901000 5 10 15 20 25 3093.7%92.1%89.2%93.5%-13-11-9-7SUPERVISED TRANSITIONSOBSERVATIONALData Log-Likelihood (106)Tagging Accuracy (%)Number of Iterations of Expectation MaximizationFigure 2: The data log-likelihood (top) and tag-ging accuracy (bottom) of two partially supervisedinitializers, one with SUPERVISED TRANSITIONSand one with SUPERVISED EMISSIONS, comparedon the Penn Treebank.12%.
Accuracy degrades somewhat from the SU-PERVISED initialization, since the data likelihoodobjective differs from the objective of maximizingtagging accuracy.
However, the final SUPERVISEDperformance of 94.1% shows that there is substan-tial room for improvement over the UNIFORM ini-tializer.Figure 2 compares two partially supervised ini-tializations.
SUPERVISED TRANSITIONS initial-izes the transition model with oracle counts, butthe emission model uniformly.
Conversely, SU-PERVISED EMISSIONS initializes the emission pa-rameters from oracle counts, but initializes thetransition model uniformly.
There are many moreemission parameters (57,390) than transition pa-rameters (1,858).
Thus, it is not surprising thatSUPERVISED EMISSIONS gives a higher initiallikelihood.
Again, both initializers lead to solu-tions with nearly the same likelihood as SUPER-VISED and UNIFORM.Figure 2 shows that SUPERVISED TRANSI-TIONS outperforms SUPERVISED EMISSIONS intagging accuracy, despite the fact that fewer pa-rameters are set with supervision.
With fixed D,an accurate initialization of the transition distribu-tions leads to accurate tagging after EM training.We therefore concentrate on developing an effec-tive initialization for the transition distribution.4 Observational InitializationThe SUPERVISED TRANSITIONS initialization isestimated from observations of consecutive tags ina labeled corpus.
Our OBSERVATIONAL initializeris likewise estimated from the relative frequencyof consecutive tags, taking advantage of the struc-ture of the tag dictionary D. However, it does notrequire a labeled corpus.Let D(w, ?)
= {t : (w, t) ?
D} denote theallowed tags for word w. The setU = {w : |D(w, ?
)| = 1}contains all words that have only one allowed tag.When a token of some w ?
U is observed in acorpus, its tag is unambiguous.
Therefore, its tagis observed as well, and a portion of the tag se-quence is known.
When consecutive pairs of to-kens are both in U , we can observe a transition inthe latent tag sequence.
The OBSERVATIONAL ini-tializer simply estimates a transition distributionfrom the relative frequency of these unambiguousobservations that occur whenever two consecutivetokens both have a unique tag.We now formally define the observational ini-tializer.
Let g(w, t) = ?
(D(w, ?
), {t}) be an indi-cator function that is 1 whenever w ?
U and itssingle allowed tag is t, and 0 otherwise.
Then, weinitialize ?
such that:P?(t|t?)
?
?w?S|w|?i=1g(wi, t) ?
g(wi?1, t?
)The emission parameters ?
are set to be uniformover allowed words for each tag, as in UNIFORMinitialization.Figure 3 compares the OBSERVATIONAL ini-tializer to the SUPERVISED TRANSITIONS initial-izer, and the top of Table 1 summarizes the perfor-mance of all initializers discussed so far for the8187080901000 5 10 15 20 25 30-13-11-9-7SUPERVISEDUNIFORM7080901000 5 10 15 20 25 30Data Log-Likelihood (106)Tagging Accuracy (%)94.1%82.1%96.7%72.0%93.7%91.0%92.8%93.5%Number of Iterations of Expectation Maximization-13-11-9-7SUPERVISED TRANSITIONSSUPERVISED EMISSIONSData Log-Likelihood (106)Tagging Accuracy (%)Number of Iterations of Expectation Maximization7080901000 5 10 15 20 25 3093.7%92.1%89.2%93.5%-13-11-9-7SUPERVISED TRANSITIONSOBSERVATIONALData Log-Likelihood (106)Tagging Accuracy (%)Number of Iterations of Expectation MaximizationFigure 3: The data log-likelihood (top) and tag-ging accuracy (bottom) of initializing with SU-PERVISED TRANSITIONS compared to the unsu-pervised OBSERVATIONAL initialization that re-quires only a tag dictionary and an unlabeled train-ing corpus.English Penn Treebank.
The OBSERVATIONALinitializer provides an error reduction over UNI-FORM of 56%, surpassing the performance of aninitially supervised emission model and nearingthe performance of a supervised transition model.The bottom of Table 1 shows a similar compar-ison on the T?ubingen treebank of spoken German(Telljohann et al, 2006).
Both training and test-ing were performed on the entire treebank.
Theobservational initializer provides an error reduc-tion over UNIFORM of 29%, and again outper-forms SUPERVISED EMISSIONS.
On this datasetOBSERVATIONAL initialization matches the finalperformance of SUPERVISED TRANSITIONS.5 DiscussionThe fact that observations and prior knowledge areuseful for part-of-speech tagging is well under-stood (Brill, 1995), but the approach of estimatingan initial transition model only from unambiguousword pairs is novel.Our experiments show that for EM-trainedHMM taggers in a type-supervised transductivedata setting, observational initialization is an ef-fective technique for guiding training toward high-accuracy solutions, approaching the oracle accu-racy of SUPERVISED TRANSITIONS initialization.The fact that models with similar data likeli-hood can vary dramatically in accuracy has beenobserved in other learning problems.
For instance,Toutanova and Galley (2011) show that optimalEnglish Initial EM-trainedUNIFORM 72.0 82.1OBSERVATIONAL 89.2 92.1SUP.
EMISSIONS 92.8 91.0SUP.
TRANSITIONS 93.5 93.7FULLY SUPERVISED 96.7 94.1German Initial EM-trainedUNIFORM 77.2 88.8OBSERVATIONAL 92.7 92.1SUP.
EMISSIONS 90.7 89.0SUP.
TRANSITIONS 94.8 92.0FULLY SUPERVISED 97.0 92.9Table 1: Accuracy of English (top) and German(bottom) tagging models at initialization (left) andafter 30 iterations of EM training (right) using var-ious initializers.parameters for IBM Model 1 are not unique, andalignments predicted from different optimal pa-rameters vary significantly in accuracy.However, the effectiveness of observational ini-tialization is somewhat surprising because EMtraining includes these unambiguous tag pairs inits expected counts, even with uniform initializa-tion.
Our experiments indicate that this signal isnot used effectively unless explicitly encoded inthe initialization.In our English data, 48% of tokens and 74% ofword types have only one allowed tag.
28% ofpairs of adjacent tokens have only one allowed tagpair and contribute to observational initialization.In German, 49% of tokens and 87% of word typesare unambiguous, and 26% of adjacent token pairsare unambiguous.6 Related WorkWe now compare with several previous publishedresults on type-supervised part-of-speech taggingtrained using the same data setting on the EnglishWSJ Penn Treebank, introduced by Smith and Eis-ner (2005).Contrastive estimation (Smith and Eisner, 2005)is a learning technique that approximates the par-tition function of the EM objective in a log-linearmodel by considering a neighborhood around ob-served training examples.
The Bayesian HMMof Goldwater and Griffiths (2007) is a second-order HMM (i.e., likelihood factors over triplesof tags) that is estimated using a prior distribu-tion that promotes sparsity.
Sparse priors have81945 tag set 17 tag setAll train 973k train All train 973k trainObservational initialization (this work) 92.1 92.8 93.9 94.8Contrastive Estimation (Smith and Eisner, 2005) ?
?
88.7 ?Bayesian HMM (Goldwater and Griffiths, 2007) 86.8 ?
87.3 ?Bayesian LDA-HMM (Toutanova and Johnson, 2008) ?
?
93.4 ?Linguistic initialization (Goldberg et al, 2008) 91.4 ?
93.8 ?Minimal models (Ravi and Knight, 2009) ?
92.3 ?
96.8Table 2: Tagging accuracy of different approaches on English Penn Treebank.
Columns labeled 973ktrain describe models trained on the subset of 973k tokens used by Ravi and Knight (2009).been motivated empirically for this task (Johnson,2007).
The Bayesian HMM model predicts tag se-quences via Gibbs sampling, integrating out modelparameters.
The Bayesian LDA-based model ofToutanova and Johnson (2008) models ambiguityclasses of words, which allows information shar-ing among words in the tag dictionary.
In addition,it incorporates morphology features and a sparseprior of tags for a word.
Inference approximationsare required to predict tags, integrating out modelparameters.Ravi and Knight (2009) employs integer linearprogramming to select a minimal set of parame-ters that can generate the test sentences, followedby EM to set parameter values.
This techniquerequires the additional information of which sen-tences will be used for evaluation, and its scalabil-ity is limited.
In addition, this work used a sub-set of the WSJ Penn Treebank for training and se-lecting a tag dictionary.
This restriction actuallytends to improve performance, because a smallertag dictionary further constrains model optimiza-tion.
We compare directly to their training set,kindly provided to us by the authors.The linguistic initialization of Goldberg et al(2008) is most similar to the current work, inthat it estimates maximum likelihood parametersof an HMM using EM, but starting with a well-chosen initialization with language specific lin-guistic knowledge.
That work estimates emissiondistributions using a combination of suffix mor-phology rules and corpus context counts.Table 2 compares our results to these relatedtechniques.
Each column represents a variant ofthe experimental setting used in prior work.
Smithand Eisner (2005) introduced a mapping from thefull 45 tag set of the Penn Treebank to 17 coarsetags.
We report results on this coarse set by pro-jecting from the full set after learning and infer-ence.1Using the full tag set or the full trainingdata, our method offers the best published perfor-mance without language-specific assumptions orapproximate inference.7 Future WorkThis paper has demonstrated a simple and effec-tive learning method for type-supervised, trans-ductive part-of-speech tagging.
However, it is anopen question whether the technique is as effec-tive for tag dictionaries derived from more naturalsources than the labels of an existing treebank.All of the methods to which we compare ex-cept Goldberg et al (2008) focus on learning andmodeling techniques, while our method only ad-dresses initialization.
We look forward to inves-tigating whether our technique can be used as aninitialization or prior for these other methods.ReferencesEric Brill.
1995.
Unsupervised learning of disam-biguation rules for part of speech tagging.
In In Nat-ural Language Processing Using Very Large Cor-pora, pages 1?13.
Kluwer Academic Press.Dipanjan Das and Slav Petrov.
2011.
Unsupervisedpart-of-speech tagging with bilingual graph-basedprojections.
In Proceedings of the Assocation forComputational Linguistics.Dan Garrette and Jason Baldridge.
2013.
Learning apart-of-speech tagger from two hours of annotation.In Proceedings of the North American Chapter ofthe Assocation for Computational Linguistics.Yoav Goldberg, Meni Adler, and Michael Elhadad.2008.
EM can find pretty good HMM POS-taggers1Training with the reduced tag set led to lower perfor-mance of 91.0% accuracy, likely because the coarse projec-tion drops critical information about allowable English tran-sitions, such as what verb forms can follow to be (Goldberget al, 2008).820(when given a good start).
In Proceedings of the As-sociation for Computational Linguistics.Sharon Goldwater and Tom Griffiths.
2007.
A fullyBayesian approach to unsupervised part-of-speechtagging.
In Proceedings of the Association for Com-putational Linguistics.Mark Johnson.
2007.
Why doesnt EM nd good HMMPOS-taggers?
In Proceedings of the Conference onEmpirical Methods in Natural Language Process-ing.Shen Li, Jo?ao V. Grac?a, and Ben Taskar.
2012.
Wiki-lysupervised part-of-speech tagging.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing.Mitchell Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of english: The penn treebank.
Computa-tional Linguistics.Bernard Merialdo.
1994.
Tagging English text with aprobabilistic model.
Computational Linguistics.Sujith Ravi and Kevin Knight.
2009.
Minimized mod-els for unsupervised part-of-speech tagging.
In Pro-ceedings of the Association for Computational Lin-guistics.Noah A. Smith and Jason Eisner.
2005.
Contrastiveestimation: Training log-linear models on unlabeleddata.
In Proceedings of the Association for Compu-tational Linguistics.Oscar T?ackstr?om, Dipanjan Das, Slav Petrov, RyanMcDonald, and Joakim Nivre.
2013.
Token andtype constraints for cross-lingual part-of-speech tag-ging.
Transactions of the Association for Computa-tional Linguistics.Heike Telljohann, Erhard Hinrichs, Sandra K?ubler, andHeike Zinsmeister.
2006.
Stylebook for the tbingentreebank of written german.Kristina Toutanova and Michel Galley.
2011.
Whyinitialization matters for ibm model 1: Multiple op-tima and non-strict convexity.
In Proceedings of the49th Annual Meeting of the Association for Com-putational Linguistics: Human Language Technolo-gies, pages 461?466, Portland, Oregon, USA, June.Association for Computational Linguistics.Kristina Toutanova and Mark Johnson.
2008.
ABayesian LDA-based model for semi-supervisedpart-of-speech tagging.
In Proceedings of Neuraland Information Processing Systems.821
