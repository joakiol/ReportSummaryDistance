Coling 2010: Poster Volume, pages 647?655,Beijing, August 2010Text Mining for Automatic Image TaggingChee Wee Leong and Rada Mihalcea and Samer HassanDepartment of Computer Science and EngineeringUniversity of North Texascheeweeleong@my.unt.edu, rada@cs.unt.edu, samer@unt.eduAbstractThis paper introduces several extractiveapproaches for automatic image tagging,relying exclusively on information minedfrom texts.
Through evaluations on twodatasets, we show that our methods ex-ceed competitive baselines by a large mar-gin, and compare favorably with the state-of-the-art that uses both textual and imagefeatures.1 IntroductionWith continuously increasing amounts of imagesavailable on the Web and elsewhere, it is impor-tant to find methods to annotate and organize im-age databases in meaningful ways.
Tagging im-ages with words describing their content can con-tribute to faster and more effective image searchand classification.
In fact, a large number of ap-plications, including the image search feature ofcurrent search engines (e.g., Yahoo!, Google) orthe various sites providing picture storage services(e.g., Flickr, Picasa) rely exclusively on the tagsassociated with an image in order to search for rel-evant images for a given query.However, the task of developing accurate androbust automatic image annotation models entailsdaunting challenges.
First, the availability of largeand correctly annotated image databases is cru-cial for the training and testing of new annotationmodels.
Although a number of image databaseshave emerged to serve as evaluation benchmarksfor different applications, including image anno-tation (Duygulu et al, 2002), content-based im-age retrieval (Li and Wang, 2008) and crosslanguage information retrieval (Grubinger et al,2006), such databases are almost exclusively cre-ated by manual labeling of keywords, requiringsignificant human effort and time.
The content ofthese image databases is often restricted only to afew domains, such as medical and natural photoscenes (Grubinger et al, 2006), and specific ob-jects like cars, airplanes, or buildings (Fergus etal., 2003).
For obvious practical reasons, it is im-portant to develop models trained and evaluatedon more realistic and diverse image collections.The second challenge concerns the extractionof useful image and text features for the construc-tion of reliable annotation models.
Most tradi-tional approaches relied on the extraction of imagecolors and textures (Li and Wang, 2008), or theidentification of similar image regions clustered asblobs (Duygulu et al, 2002) to derive correlationsbetween image features and annotation keywords.In comparison, there are only a few efforts thatleverage on the multitude of resources availablefor natural language processing to derive robustlinguistic-based image annotation models.
Oneof the earliest efforts involved the use of captionsfor face recognition in photographs through theconstruction of a specific lexicon that integrateslinguistic and photographic information (Srihariand Burhans, 1994).
More recently, several ap-proaches have proposed the use of WordNet asa knowledge-base to improve content-based im-age annotation models, either by removing noisykeywords through semantic clustering (Jin et al,2005) or by inducing a hierarchical classificationof candidate labels (Srikanth et al, 2005).In this paper, we explore the use of several natu-ral language resources to construct image annota-tion models that are capable of automatically tag-ging images from unrestricted domains with goodaccuracy.
Unlike traditional image annotationmethodologies that generate tags using image-based features, we propose to extract them in amanner analogous to keyword extraction.
Given atarget image and its surrounding text, we extractthose words and phrases that are most likely torepresent meaningful tags.
More importantly, we647are interested to investigate the potential of suchlinguistic-based models on image annotation ac-curacy and reliability.
Our work is motivated bythe need for annotation models that can be effi-ciently applied on a very large scale (e.g.
har-vesting images from the web), which are requiredin applications that cannot afford the complexityand time associated with current image process-ing techniques.The paper makes the following contributions.We first propose a new evaluation framework forimage tagging, which is based on an analogydrawn between the tasks of image labeling andlexical substitution.
Next, we present three extrac-tive approaches for the task of image annotation.The methods proposed are based only on the textsurrounding an image, without the use of imagefeatures.
Finally, by combining several orthogo-nal methods through machine learning, we showthat it is possible to achieve a performance that iscompetitive to a state-of-the-art image annotationsystem that relies on visual and textual features,thus demonstrating the effectiveness of text-basedextractive annotation models.2 Related WorkSeveral online systems have sprung into exis-tence to achieve annotation of real world imagesthrough human collaborative efforts (Flickr) andstimulating competition (von Ahn and Dabbish,2004).
Although a large number of image tags canbe generated in short time, these approaches de-pend on the availability of human annotators andare far from being automatic.
Similarly, researchin the other direction via text-to-image synthesis(Li and Fei-Fei, 2008; Collins et al, 2008; Mi-halcea and Leong, 2009) has also helped to har-vest images, mostly for concrete words, by refin-ing image search engines.Most approaches to automatic image annota-tion have focused on the generation of image la-bels using annotation models trained with imagefeatures and human annotated keywords (Barnardand Forsyth, 2001; Jeon et al, 2003; Makadia etal., 2008; Wang et al, 2009).
Instead of predict-ing specific words, these methods generally targetthe generation of semantic classes (e.g.
vegeta-tion, animal, building, places etc), which they canachieve with a reasonable amount of success.
Re-cent work has also considered the generation oflabels for real-world images (Li and Wang, 2008;Feng and Lapata, 2008).
To our knowledge, weare unaware of any other work that performs ex-tractive annotation for images from unrestricteddomains through the exclusive use of textual fea-tures.3 DatasetAs the methods we propose are extractive, stan-dard image databases with no surrounding textsuch as Corel (Duygulu et al, 2002) are not suit-able, nor are they representative for the challengesassociated with raw data from unrestricted do-mains.
We thus create our own dataset using im-ages randomly extracted from the Web.To avoid sparse searches, we use a list of themost frequent words in the British National Cor-pus as seed words, and query the web using theGoogle Image API.
A webpage is randomly se-lected from the query results if it contains a singleimage in the specified size range (width and heightof 275 to 1000 pixels1) and its text contains morethan 10 words.
Next, we use a Document ObjectModel (DOM) HTML parser2 to extract the con-tent of the webpage.
Note that we do not performmanual filtering of our images except where theycontain undesirable qualities (e.g.
porn, corruptedor blank images).In total, we collected 300 image-text pairs fromthe web.
The average image size is 496 pixelswidth and 461 pixels height.
The average textlength is 278 tokens and the average document ti-tle length is 6 tokens.
In total, there are 83,522words and the total vocabulary is 8,409 words.For each image, we also create a gold stan-dard of manually assigned tags, by using the la-bels assigned by five human annotators.
The im-age annotation is conducted via Amazon Mechan-ical Turk, which was shown in the past to producereliable annotations (Snow et al, 2008).
For in-creased annotation reliability, we only accept an-notators with an approval rating of 98%.Given an image, an annotator extracts fromthe associated text a minimum of five words orcollocations.Annotators can choose words freelyfrom the text, while collocation candidates are re-stricted to a fixed set obtained from the n-grams (n?
7) in the text that also appear as article names orsurface forms in Wikipedia.
Moreover, when in-terpreting the image, the annotators are instructedto focus on both the denotational and conotationalattributes present in the image3.1Empirically determined to filter advertisements, bannersand undersized images.2http://search.cpan.org/dist/HTML-ContentExtractor/3Annotation instructions, dataset and gold standard can648Normal Image Mode ImageGold standard czech (5), festival (5), oklahoma (4), yukon (4),october (4), web page (2), the first (2), event (2),success (1), every (1), year (1)train (5), station (4), steam (4), trans siberian (4),steam train (4), travel (3), park (3), siberian (3),old (3), photo (1), trans (2), yekaterinburg (2),the web (2), photo host (1)Table 1: Two sample images.
The number besides each label indicates the number of human annotatorsagreeing on that label.
Note that the mode image has a tag (i.e.?train?)
in the gold standard set mostfrequently selected by the annotators4 A New Evaluation Framework : ImageTagging as Lexical SubstitutionWhile evaluations of previous work in image an-notation were often based on labels provided withthe images, such as tags or image captions, in ourdataset such annotations are either missing or un-reliable.
We rely instead on human-produced ex-tractive annotations (as described in the previoussection), and formulate a new evaluation frame-work based on the intuition that an image can besubstituted with one or more tags that convey thesame meaning as the image itself.
Ideally, there isa single tag that ?best?
describes the image over-all (i.e.
the gold standard tag agreed by the major-ity of human annotators), but there are also mul-tiple tags that describe the fine-grained conceptspresent in the image.
Our evaluation frameworkis inspired by the lexical substitution task (Mc-Carthy and Navigli, 2007), where a system at-tempts to generate a word (or a set of words) toreplace a target word, such that the meaning ofthe sentence is preserved.Given this analogy, the evaluation metrics usedfor lexical substitution can be adapted to the eval-uation of image tagging.
Specifically, we measurethe precision and the recall of a tagging methodusing four subtasks: best normal: provides preci-sion and recall for the top-ranked tag returned by amethod; best mode: provides precision and recallonly if the top-ranked tag by a method matches thetag in the gold standard that was most frequentlyselected by the annotators; out of ten (oot) nor-be downloaded athttp://lit.csci.unt.edu/index.php/Downloadsmal: provides precision and recall for the top tentags by the system; and out of ten (oot) mode:similar to best mode, but it considers the top tentags returned by the system instead of one.
Table1 show examples of a normal and a mode image.Formally, let us assume that H is the setof annotators, namely {h1, h2, h3, ...}, and I,{i1, i2, i3, ...} is the set of images for which eachhuman annotator provide at least five tags.
Foreach ij, we calculate mj, which is the most fre-quent tag for that image, if available.
We also col-lect all rkj, which is the set of tags for the imageij from the annotator hk.Let the set of those images where there is a tagagreed upon by the most annotators (i.e.
the im-ages with a mode) be denoted by IM, such thatIM ?
I.
Also, let A ?
I be the set of images forwhich the system provides more than one tag.
Letthe corresponding set for the images with modesbe denoted by AM, such that AM ?
IM.
Let aj ?
Abe the set of system?s extracted tags for the imageij.Thus, for each image ij, we have the set of tagsextracted by the system, and the set of tags fromthe human annotators.
As the next step, the multi-set union of the human tags is calculated, and thefrequencies of the unique tags is noted.
Therefore,for image ij, we calculate Rj, which is?rkj, andthe individual unique tag in Rj, say res, will havea frequency associated with it, namely freqres.Given this setting, the precision (P ) and recall(R) metrics we use are defined below.649Best measures:P =?aj :ij?A?res?ajfreqres|aj ||Rj ||A|R =?aj :ij?I?res?ajfreqres|aj ||Rj ||I|modeP =?bestguessj?AM (1if best guess = mj)|AM |modeR =?bestguessj?IM (1if best guess = mj)|IM |Out of ten (oot) measures:P =?aj :ij?A?res?ajfreqres|Rj ||A|R =?aj :ij?I?res?ajfreqres|Rj ||I|modeP =?aj :ij?AM (1if any guess ?
aj = mj)|AM |modeR =?aj :ij?IM (1if any guess ?
aj = mj)|IM |As a simplified example (with less tags), con-sider ij showing a picture of a Chihuahua beinglabeled by five annotators with the following tags :Annotator Tags1 dog,pet2 chihuahua3 animal,dog4 dog,chihuahua5 dogIn this case, r1j = {dog,pet}, r2j = {chihuahua},r3j = {animal,dog} and so on.
The tag ?dog?
ap-pears the most frequent among the five annotators,hence mj = {dog}.
Rj={dog, dog, dog, dog, chi-huahua, chihuahua, animal, pet}.
The res withassociated frequencies would be dog 4, chihuahua2, animal 1, pet 1.
If the system?s proposed tag forij is {dog, animal}, then the numerator of P andR for best subtask would be4+128 = 0.313.
Simi-larly, the numerator of P and R for oot subtask is4+18 = 0.625.5 Extractive Image AnnotationThe main idea underlying our work is that we canperform effective image annotation using infor-mation drawn from the associated text.
Follow-ing (Feng and Lapata, 2008), we propose that animage can be annotated with keywords capturingthe denotative (entities or objects depicted) andconnotative (semantics or ideologies interpreted)attributes in the image.
For instance, a pictureshowing a group of athletes and a ball may also betagged with words like ?soccer,?
or ?sports activ-ity.?
Specifically, we use a combination of knowl-edge sources to model the denotative quality of aword as its picturability, and the connotative at-tribute as its saliency.
The idea of visualness andsalience as textual features for discovering namedentities in an image was first pursued by (De-schacht and Moens, 2007), using data from thenews domain.
In contrast, we are able to per-form annotation of images from unrestricted do-mains using content words (nouns, verbs and ad-jectives).
In the following, we first describe threeunsupervised extractive approaches for image an-notation, followed by a supervised method using are-ranking hypothesis that combines all the meth-ods.5.1 Flickr PicturabilityFeaturing a repository of four billion images,Flickr (http://www.flickr.com) is one of the mostcomprehensive image resources on the web.
As aphoto management and sharing application, it pro-vides users with the ability to tag, organize, andshare their photos online.
Interestingly, an inspec-tion of Flickr tags for randomly selected imagesreveal that users tend to describe the denotationalattributes of images, using concrete and picturablewords such as cat, bug, car etc.
This observationlends evidence to Flickr?s suitability as a resourceto model the picturability of words.Given the text (T ) of an image, we can usethe getRelatedTags API to retrieve the most fre-quent Flickr tags associated with a given word,and use them as corpus evidence to filter or pro-mote words in the text.
In the filtering phasewe ignore any words that return an empty list ofFlickr?s related tags, based on the assumption thatthese words are not used in the Flickr tags repos-itory.
We also discard words with a length that isless than three characters (?=3).
In the promotionphase, we reward any retrieved tags that appear assurface forms in the text.
This reward is propor-tional to the term frequency of these tags in the650Algorithm 1 Flickr Picturability AlgorithmStart : L[]=?
, TF[]=tf of each word in Tfor each word in T doif length(word) ?
?
thenRelatedTags=getRelatedTags(word);if size(RelatedTags) > 0 thenL[word]+=?
*TF[word]for each tag in RelatedTags doif exists TF [tag] thenL[tag]+=TF[tag]end ifend forend ifend ifend fortext.
Additionally, we also include in the final la-bel set any word that returns a non-empty relatedtags set with a discounted weight (?=0.5) of itsterm frequency, to the end of enriching our labelsset while assuring more credit are given to the pic-turable words.To extract multiword labels, we locate all n-grams formed exclusively from our extracted setof possible labels.
The subsequent score for eachof these n-grams is:L[wi..wi+k] = (j=i+k?j=iL[wj])/kBy reverse sorting the associative array in L, wecan retrieve the top K words to label the image.For illustration, let us consider the following textsnippet.On the Origin of Species, published byCharles Darwin in 1859, is consideredto be the foundation of evolutionary bi-ology.After removing stopwords, we consider the re-maining words as candidate labels.
For eachof these candidates wi (i.e.
origin, species,published, charles, darwin, foundation,evolutionary, and biology), we query Flickr andobtain their related tag set Ri.
origin, published,and foundation return an empty set of relatedtags and hence are removed from our set of can-didate labels, leaving species, charles, darwin,evolutionary, and biology as possible annotationkeywords with the initial score of 0.5.
In the pro-motion phase, we score each wi based on the num-ber of votes it receives from the remaining wj(Figure 1).
Each vote represents an occurrenceof the candidate tag wi in the related tag set Rjof the candidate tag wj .
For example, darwinappeared in the Flickr related tags for charles,evolutionary, and biology, hence it has a weightof 3.5.
The final list of candidate labels are shownin Table 2....
Species, published by Charles Darwin ?
founda!on of evolu!onary biologyFigure 1: Flickr Picturability LabelsLabel S(wi)darwin 3.5charles darwin 2.5charles 1.5biology 1.5evolutionary biology 1.0evolutionary 0.5species 0.5Table 2: Candidate labels obtained for a sampletext using the Flickr model5.2 Wikipedia SalienceWe hypothesize that an image often describes themost important concepts in the associated text.Thus, the keywords selected from a text could beused as candidate labels for the image.
We usea graph-based keyword extraction method similarto (Mihalcea and Tarau, 2004), enhanced with asemantic similarity measure.
Starting with a text,we extract all the candidate labels and add them asvertices in the graph.
A measure of word similar-ity is then used to draw weighted edges betweenthe nodes.
Using the PageRank algorithm, thewords are assigned with a score indicating theirsalience within the given text.To determine the similarity between words, weuse a directed measure of similarity.
Most wordsimilarity metrics provide a single-valued scorebetween a pair of words w1 and w2 to indicatetheir semantic similarity.
Intuitively, this is not al-ways the case, as w1 may be represented by con-cepts that are entirely embedded in other concepts,represented by w2.
In psycholinguistics terms, ut-tering w1 may bring to mind w2, while the appear-ance of w2 without any contextual clues may notassociate with w1.
For example, Obama bringsto mind the concept of president, but president651may trigger other concepts such as Washington,Lincoln, Ford etc., depending on the existingcontextual clues.
Thus, the degree of similarityof w1 with respect to w2 should be separated fromthat of w2 with respect to w1.
Specifically, we usethe following measure of similarity, based on theExplicit Semantic Analysis (ESA) vectors derivedfrom Wikipedia (Gabrilovich and Markovitch,2007):DSim(wi, wj) =CijCi?
Sim(wi, wj)where Cij is the count of articles in Wikipediacontaining words wi and wj , Ci is the count of ar-ticles containing words wi, and Sim(wi, wj) is thecosine similarity of the ESA vectors representingthe input words.The directional weight (Cij /Ci)amounts to the degree of association of wi with re-spect to wj .
Using the directional inferential sim-ilarity scores as directed edges and distinct wordsas vertices, we obtain a graph for each text.
Thedirected edges denotes the idea of ?recommenda-tion?
where we say w1 recommends w2 if andonly if there is a directed edge from w1 to w2, withthe weight of the recommendation being the direc-tional similarity score.
Starting with this graph,we use the graph iteration algorithm from (Mi-halcea and Tarau, 2004) to calculate a score foreach vertex in the graph.
The output is a sortedlist of words in decreasing order of their ranks,which are used as candidate labels to annotate theimage.
This is achieved by using Cj instead of Cifor the denominator in the directional weight.
Asan example, consider the text snippet :Microsoft Corporation is a multina-tional computer technology corporationthat develops, manufactures, licenses,and supports a wide range of softwareproducts for computing devicesafter stopword removal, the list of nouns ex-tracted is Microsoft, computer, corporation, de-vices, products, technology, software.
Note thatthe top-ranked word must infer some or all of thewords in the text.
In this case, the word Microsoftinfers the terms computer, technology and soft-ware.To calculate the semantic relatedness betweentwo collocations, we use a simplified version ofthe text-to-text relatedness technique proposed byand (Mihalcea et al, 2006) that incorporate thedirectional inferential similarity as an underlyingsemantic metric.5.3 Topical ModelingIntuitively, every text is written with a topic inmind, and the associated image serves as an illus-tration of the text meaning.
In this paper, we in-vestigate the effect of topical modeling on imageannotation accuracy directly.
We use the PachinkoAllocation Model (PAM) (Li and McCallum,2006) to model the topics in a text, where key-words forming the dominant topic are assumed asour set of annotation keywords.
Compared withprevious topic modeling approaches, such as La-tent Dirichlet alocation (LDA) or its improvedvariant Correlated Topic Model (CTM) (Blei andLafferty, 2007), PAM captures correlations be-tween all the topic pairs using a directed acyclicgraph (DAG).
It also supports finer-grained topicmodeling, and has state-of-the-art performance onthe tasks of document classification and topicalkeyword coherence.
Given a text, we use the PAMmodel to infer a list of super-topics and sub-topicstogether with words weighted according to thelikelihood that they belong to each of these topics.For each text, we retrieve the top words belong-ing to the dominant super-topic and sub-topic.
Weuse 50 super-topics and 100 sub-topics as operat-ing parameters for PAM, since these values werefound to provide good results in previous work ontopic modeling.
Default values are used for otherparameters in the model.5.4 Supervised LearningThe three tagging methods target different aspectsof what constitutes a good label for an image.
Weuse them as features in a machine learning frame-work, and introduce a final rank attribute S(tj),which is a linear combination of the reciprocals ofthe rank of each tag as given by each method,S(tj) =?m?methods?m1rmtjwhere rmtj is the rank for tag tj given by methodm.
The weight of each method ?m is estimatedfrom the training set using information gain val-ues.
Since our predicted variable (mode precisionor recall) is continuous, we use the Support Vec-tor Algorithm (nu-SVR) implementation of SVM(Chang and Lin, 2001) to perform regression anal-ysis on the weights for each method via a radialbasis function kernel.
A ten-fold cross-validationis applied on the entire dataset of 300 images.652Best out-of-ten (oot)Normal Mode Normal ModeModels P R P R P R P RFlickr picturability 6.32 6.32 78.57 78.57 35.61 35.61 92.86 92.86Wikipedia Salience 6.40 6.40 7.14 7.14 35.19 35.19 92.86 92.86Topic modeling 5.99 5.99 42.86 42.86 37.13 37.13 85.71 85.71Combined (SVM) 6.87 6.87 67.49 67.49 37.85 37.85 100.00 100.00Doc Title 6.40 6.40 75.00 75.00 18.97 18.97 82.14 82.14tf * idf 5.94 5.94 14.29 14.29 38.40 38.40 78.57 78.57Random 3.76 3.76 3.57 3.57 30.20 30.20 50.00 50.00Upper bound (human) 12.23 12.07 81.48 81.48 82.44 81.55 100.00 100.00Table 3: Results obtained on the Web dataset6 Experiments and EvaluationsWe evaluate the performance of each of the threetagging methods separately, followed by an eval-uation of the combined method.
Each system pro-duces a ranked list of K words or collocationsas tags assigned to a given image.
A system candiscretionary generate less (but not more) than Ktags, depending on its confidence level.For comparison, we implement three baselines:tf*idf, Doc Title and Random.
For tf*idf, we usethe British National Corpus to calculate the idfscores, while the frequency of a term is calcu-lated from the entire text associated with an im-age.
The Doc Title baseline is similar, except thatthe term frequency is calculated based on the titleof the document.
The Random baseline randomlyselects words from a co-occurrence window ofsize K before and after an image as its annota-tion.
Following other tagging methods, we apply apre-processing stage, where we part-of-speech tagthe text (to retain only nouns), followed by stem-ming.
We also determine an upper bound, whichis calculated as follows.
For each image, the la-bels assigned by each of the five annotators arein turn evaluated against a gold standard consist-ing of the annotations of the other four annotators.The best performing annotator is then recorded.This process is repeated for each of the 300 im-ages, and the average precision and recall are cal-culated.
This represents an upper bound, as it isthe best performance that a human can achieve onthis dataset.
Table 3 shows our experimental re-sults.Among the individual methods, the method im-plementing Flickr picturability has the highest in-dividual score for best and oot modes, yieldinga precision and recall of 78.57% and 92.86% re-spectively.
The Wikipedia Saliency method alsoscores the highest (jointly with Flickr) in the ootmode, but for the best mode achieves a score onlymarginally better than the random baseline.
Aplausible explanation is that it tends to favor ?all-inferring?
over-specific labels, while the most fre-quently selected tags in mode pictures are typi-cally more ?picturable?
than being specific (e.g.?train?
for the mode picture in Table 1).
The topicmodeling method has mixed results: its scoresfor oot normal and mode are somewhat compet-itive with tf*idf, but it scores consistently lowerthan the DocTitle in the best subtask, possiblydue to the absence of a more sophisticated re-ranking algorithm tailored for the image annota-tion task other than the intrinsic ranking mecha-nism in PAM.
It is worth noting that the combinedsupervised system provides the overall best results(6.87%) on the best normal, and achieves a perfectprecision and recall (100%) for oot mode, whichmeans perfect agreement with the human tagging.7 Comparison with Related WorkWe also compare our work against (Feng and Lap-ata, 2008) as it allows for a direct comparison withmodels using both image and textual features un-der a standard evaluation framework.
We obtainedthe BBC dataset used in their experiments, whichconsists of 3121 training and 240 testing images.In this dataset, images are implicitly tagged withcaptions by the author of the corresponding BBCarticle.
The evaluations are run against these cap-tions.In their experiments, Feng and Lapata createdfour annotation models.
The first two (tf*idf andDocument Title) are the same as used in our base-line experiments.
The third model (Lavrenko03)is an application of the continuous relevancemodel in (Jeon et al, 2003), trained with the BBCimage features and captions.
Finally, the forth(ExtModel) is an extension of the relevance modelusing additional information in auxiliary texts.Briefly, the model assumes a multiple Bernoullidistribution for words in a caption, and generatestags for a test image using a weighted combina-tion of the accompanying document, caption andimage features learned during training.653Top 10 Top 15 Top 20Models P R F1 P R F1 P R F1tf*idf 4.37 7.09 5.41 3.57 8.12 4.86 2.65 8.89 4.00DocTitle 9.22 7.03 7.20 9.22 7.03 7.20 9.22 7.03 7.20Lavrenko03 9.05 16.01 11.81 7.73 17.87 10.71 6.55 19.38 9.79ExtModel 14.72 27.95 19.82 11.62 32.99 17.18 9.72 36.77 15.39Flickr picturability 12.13 22.82 15.84 9.52 26.82 14.05 8.23 29.80 12.90Wikipedia Salience 11.63 21.89 15.18 9.28 26.20 13.70 7.81 29.41 12.35Topic Modeling 11.42 21.49 14.91 9.28 26.20 13.70 7.86 29.57 12.42Combined (SVM) 13.38 25.17 17.47 11.08 31.29 16.37 9.50 35.76 15.01Table 4: Results obtained on the BBC dataset used in (Feng and Lapata, 2008)The experimental setup is similar to the earliersection, but a few modifications are made for a fairand direct comparison.
First, we extend our mod-els coverage to include content words (i.e.
nouns,verbs, adjectives) determined using the Tree Tag-ger (Schmid, 1994).
Second, no collocations areused.
Third, we adopt the evaluation frameworkused by Feng and Lapata to extract the top 10, 15and 20 tags.
Note that in our methods, the extrac-tion of tags for a test image is only done on thedocument surrounding the image, after excludingthe caption.
As the number of negative examples(words not present in the caption) greatly outnum-ber the positive instances, we employ an under-sampling method (Kubat and Matwin, 1997) tobalance the dataset for training.The results are shown in Table 4.
Interest-ingly, all our unsupervised extraction-based mod-els perform consistently above the supervisedLavrenko03 model, indicating that textual fea-tures are more informative than captions and im-age features taken together.
Comparing with mod-els using significantly less document informa-tion (tf*idf and Doc title), our models gain evengreater advantage.
Note that the title of any BBCarticle does not exceed 10 words, hence compar-ison is only meaningful given the top 10 tags re-trieved.Feng and Lapata used LDA to perform rerank-ing of final candidates in their ExtModel.
How-ever, when used as a model alone, the PAM topicmodel achieved promising scores in all the cate-gories, performing best for top 10 keywords (F1of 14.91%).
Flickr picturability stands out asthe best performing unsupervised method, scor-ing the highest precision (12.13%, top 10), recall(29.80%, top 20) and F1 (15.84%, top 10).Overall, this comparative evaluation yieldssome important insights.
First, our combinedmodel using SVM is statistically better (p<0.1 fortop 10, 15, 20) than the Laverenko03 model, butnot statistically different from the ExtModel.
Thisdemonstrates the effectiveness of textual-basedmodels over traditional models trained with im-age features and captions.
While it is intuitivelyclear that image features help in improving tag-ging performance, we show that mining only thetext surrounding an image, where it exists, canyield a performance that is comparable to a state-of-the-art system that uses both textual and vi-sual features.
Moreover, an increase in complex-ity of a model by using more features may hinderits applicability to large datasets, but not neces-sarily improving annotation performance (Maka-dia et al, 2008).
On this, text-based annotationmodels can provide a desirable compromise.
Forinstance, our unsupervised models implementingFlickr picturability and Wikipedia Salience areable to extract annotations from a BBC article (av-erage 133.85 tokens) in approximately 1 secondand 20 seconds respectively.8 Conclusions and Future WorkIn this paper, we introduced several text-based ex-tractive approaches for automatic image annota-tion and showed that they compare favorably withthe state-of-the-art in image annotation using bothtext and image features.
We believe our workhas practical applications in mining and annotat-ing images over the Web, where texts are nat-urally associated with images, and scalability isimportant.
Our next direction seeks to derive ro-bust annotation models using additional ontolog-ical knowledge-bases.
We would also like to ad-vance the the state-of-the-art by augmenting cur-rent textual models with image features.AcknowledgmentsThis material is based in part upon work sup-ported by the National Science Foundation CA-REER award #0747340.
Any opinions, findings,and conclusions or recommendations expressed inthis material are those of the authors and do notnecessarily reflect the views of the National Sci-ence Foundation.654ReferencesKobus Barnard and David Forsyth.
2001.
Learning thesemantics of words and pictures.
In Proceedings ofInternational Conference on Computer Vision.David Blei and John Lafferty.
2007.
A correlated topicmodel of science.
In Annals of Applied Statistics,volume 1, pages 17?35.Chih-Chung Chang and Chih-Jen Lin, 2001.
LIBSVM:a library for support vector machines.Brendan Collins, Jia Deng, Kai Li, and Li Fei-Fei.2008.
Towards scalable dataset construction: Anactive learning approach.
In Proceedings of Euro-pean Conference on Computer Vision.Koen Deschacht and Marie-Francine Moens.
2007.Text analysis for automatic image annotation.
InProceedings of the Association for ComputationalLinguistics.Pinar Duygulu, Kobus Barnard, Nando de Freitas, andDavid Forsyth.
2002.
Object recognition as ma-chine translation:learning a lexicon for a fixed im-age vocabulary.
In Proceedings of the 7th EuropeanConference on Computer Vision.Yansong Feng and Mirella Lapata.
2008.
Automaticimage annotation using auxiliary text information.In Proceedings of the Association for Computa-tional Linguistics.Rob Fergus, Pietro Perona, and Andrew Zisserman.2003.
Object class recognition by unsupervisedscale-invariant learning.
In Proceedings of the In-ternational Conference on Computer Vision andPattern Recognition.Evgeniy Gabrilovich and Shaul Markovitch.
2007.Computing semantic relatedness using wikipedia-based explicit semantic analysis.
In InternationalJoint Conferences on Artificial Intelligence.Michael Grubinger, Clough Paul, Mller Henning, andDeselaers Thomas.
2006.
The iapr benchmark: Anew evaluation resource for visual information sys-tems.
In International Conference on Language Re-sources and Evaluation.Jiwoon Jeon, Victor Lavrenko, and R Manmatha.2003.
Automatic image annotation and retrieval us-ing cross-media relevance models.
In Proceedingsof the ACM SIGIR Conference on Research and De-velopment in Information Retrieval.Yohan Jin, Latifur Khan, Lei Wang, and MamounAwad.
2005.
Image annotations by combining mul-tiple evidence & wordnet.
In Proceedings of AnnualACM Multimedia.Miroslav Kubat and Stan Matwin.
1997.
Addressingthe curse of imbalanced training sets: one-sided se-lection.
In Proceedings of International Conferenceon Machine Learning.Li-Jia Li and Li Fei-Fei.
2008.
Optimol: au-tomatic online picture collection via incrementalmodel learning.
In International Journal of Com-puter Vision.Wei Li and Andrew McCallum.
2006.
Pachinko allo-cation: Dag-structured mixture models of topic cor-relations.
In Proceedings of the International Con-ference on Machine learning.Jia Li and James Wang.
2008.
Real-time computer-ized annotation of pictures.
In Proceedings of Inter-national Conference on Computer Vision.Ameesh Makadia, Vladimir Pavlovic, and Sanjiv Ku-mar.
2008.
A new baseline for image annotation.
InProceedings of European Conference on ComputerVision.Diana McCarthy and Roberto Navigli.
2007.
The se-meval English lexical substitution task.
In Proceed-ings of the ACL Semeval workshop.Rada Mihalcea and Chee Wee Leong.
2009.
To-wards communicating simple sentences using pic-torial representations.
In Machine Translation, vol-ume 22, pages 153?173.Rada Mihalcea and Paul Tarau.
2004.
Textrank:Bringing order into texts.
In Proceedings of Em-pirical Methods in Natural Language Processing.Rada Mihalcea, Courtney Corley, and Carlo Strappa-rava.
2006.
Corpus-based and knowledge-basedmeasures of text semantic similarity.
In Proceed-ings of Association for the Advancement of ArtificialIntelligence, pages 775?780.Helmut Schmid.
1994.
Probabilistic part-of-speechtagging using decision trees.
In Proceedings of theInternational Conference on New Methods in Lan-guage Processing.Rion Snow, Brendan O?Connor, Daniel Jurafsky, andAndrew Ng.
2008.
Cheap and fast - but is it good?evaluating non-expert annotations for natural lan-guage tasks.
In Proceedings of Empirical Methodsin Natural Language Processing.Srihari and Burhans.
1994.
Visual semantics: Extract-ing visual information from text accompanying pic-tures.
In Proceedings of the American Associationfor Artificial Intelligence.Munirathnam Srikanth, Joshua Varner, Mitchell Bow-den, and Dan Moldovan.
2005.
Exploiting ontolo-gies for automatic image annotation.
In Proceed-ings of the ACM Special Interest Group on Researchand Development in Information Retrieval.Luis von Ahn and Laura Dabbish.
2004.
Labeling im-ages with a computer game.
In Proceedings of theACM Special Interest Group on Computer HumanInteraction.Chong Wang, David Blei, and Li Fei-Fei.
2009.
Si-multaneous image classification and annotation.
InProceedings of IEEE Conference on Computer Vi-sion and Pattern Recognition.655
