Encounters with LanguageCharles J. Fillmore?University of California, Berkeley, andInternational Computer Science InstituteFirst of all, I am overwhelmed and humbled by the honor the ACL Executive Committeehas shown me, an honor that should be shared by the colleagues and students I?ve beenlucky enough to have around me this past decade-and-a-half while I?ve been engagedin the FrameNet Project at the International Computer Science Institute in Berkeley.I?ve been asked to say something about the evolution of the ideas behind the workwith which I?ve been associated, so my remarks will be a bit more autobiographical thanI might like.
I?d like to comment on my changing views of what language is like, andhow the facts of language can be represented.
As I am sure the ACL Executive Com-mittee knows, I have never been a direct participant in efforts in language engineering,but I have been a witness to, a neighbor of, and an indirect participant in some parts ofit, and I have been pleased to learn that some of the resources my colleagues and I arebuilding have been found by some researchers to be useful.I offer a record of my encounters with language and my changing views of whatone ought to believe about language and how one might represent its properties.
Inthe course of the narrative I will take note of changes I have observed over the pastseven decades or so in both technical and conceptual tools in linguistics and languageengineering.
One theme in this essay is how these tools, and the representations theysupport, obscure or reveal the properties of language and therefore affect what onemight believe about language.
The time frame my life occupies has presented manyopportunities to ponder this complex relationship.1.
Earliest EncountersThis story begins in the 1930s and 1940?s, in St. Paul, Minnesota.
There was nothinglinguistically exotic about growing up there, except perhaps the Norwegian-accentedEnglish of some of my mother?s older relatives.
But during much of my childhood Iwas convinced that I personally had difficulties with language: The symptomwas that Icould never think of anything to say.
I was tongue-tied.
I now suspect that it was mainlya problem of the shyness and awkwardness that goes along with growing up confused,and not an actual matter of language pathology.
Nevertheless, it led me into my earliestattempt to work with language data.At around age 14, I presentedmy problem to a librarian in the St. Paul Public library,and she foundme a book called 5000 Useful Phrases forWriters and Speakers.
Amemorable?
International Computer Science Institute, 1947 Center St. Ste.
600, Berkeley, CA 94611, USA.
E-mail:fillmore@icsi.berkeley.edu.
I am especially indebted to the three directors of the International ComputerScience Institute during the life of the FrameNet Project (Jerome Feldman, Nelson Morgan, and RobertoPieraccini) and to Collin Baker, FrameNet Project Manager, for keeping the project alive during the recentyears of my relative inactivity; to Mary Catherine O?Connor and Russell Lee Goldman for importantassistance in the preparation of the present document; and to Lily Wong Fillmore, videographer, editor,and censor for the broadcast version of the acceptance speech.?
2012 Association for Computational LinguisticsComputational Linguistics Volume 38, Number 4example was ?With a haggard lift of the upper lip .
.
.
?
I took the book home, cut sheetsof typewriter paper into eight pieces to make file slips, chose phrases I thought I shouldmemorize, and copied them onto these slips.
I held them together with rubber bands,and I kept them in a secret place in my room.
Thus supported with the early 1940stechnologies of paper, scissors, pencil, and rubber bands, my earliest theory of languagebegan to develop: Linguistic competence is having access to a large repertory of ready-madethings to say.I added to the collection over the years, as I came upon clever or wise expressions,and consulted a selection of them every night, scheming to create situations in which Icould use them, in speaking or writing.
In later years I held on to the suspicion thatmuch of ordinary conversation in real life involves calling on remembered phrasesrather than creating novel expressions from rules.
Much later I learned that in manyEastern European countries influenced by the Moscow School, the divisions of the fieldof Linguistics were Phonology, Morphology, Lexicology, Phraseology, and Syntax.
Thestudy of phraseological units?phraseologisms?was seen as central, not peripheral, tolinguistic inquiry.My first exposure to the actual field of Linguistics came a year later, around age15, when a missionary lady on leave, living on my block in St. Paul, gave me a copy ofEugene Nida?s little book, Linguistic Interludes (Nida 1947).
The text of this book takesthe form of conversations in a college campus co-op between a clever and wise linguistand a caricatured collection of innocent and unsuspecting students and colleagues,among them a classicist who strongly defended the logical perfection of the classicallanguages Greek and Latin.This book succeeded in conveying simply many of the things that linguists believe: Relevant linguistic generalizations are based on speech, not writing. Almost all concepts of ?correct grammar?
are inventions, with no basis inthe history of the language. There may be primitive communities, but there are no primitive languages.The minor protagonists in the conversation contested each of these principles, and thelinguist hero, from his vast knowledge of the most exotic of the world?s languages,kept showing them how wrong they were.
I liked the idea of knowing things that mostpeople, including college professors, had wrong opinions about.
I also liked the idea ofbeing able to help them change their wrong opinions, so I decided to study Linguistics.2.
Formal Studies BeginBefore long I was enrolled in a fairly small linguistics program at the University ofMinnesota.
I could live at home, take a streetcar to Minneapolis for classes, and takeanother streetcar to Montgomery Wards in St. Paul, where I wrapped venetian blinds tosupport my studies.In those days there were no linguistics textbooks in the modern sense; we studiedtwo books titled Language?one by Edward Sapir (1921) and the other by LeonardBloomfield (1933)?and we read grammars and treatises.
I took two years of Arabic.
Isupplemented my training in linguistic methods through Summer Linguistic Institutesput on by the Linguistic Society of America, one inMichigan and one in Berkeley, whereI learned about Thai, Sanskrit, and Navajo with Mary Haas, Franklin Edgerton, andHarry Hoijer.702Fillmore Computational Linguistics2.1 First Research Experience: Concordance-BuildingOne of my professors at the University Minnesota was building concordances of someof theminor Late Latin texts, and he permitted the students in his class to workwith himon these projects.
For the advanced students this was a chance to get valuable hands-onresearch experience; for the less advanced students it was an opportunity to get ?extracredit.
?This was in a sense my first exposure to corpus-based linguistics.
For any givendocument, the professor would pass on the text to that year?s students.
This ?firstgeneration?
of students copied word tokens onto separate index cards, together witheach word?s ?parse?
in the classical sense, and its location in the document.Generation 2?the students in the next year?s class?alphabetized these cards andtyped up the concordances.
Generation 3, in which I participated, took this same stackof cards and reverse-alphabetized them, so they could be used for research on suffixes.
(Personal note: alphabetizing words from right to left is stressful at first, but you getused to it.)
So with the tools of pre-cut index cards, a pencil, and a typewriter, westudents constructed a concordance?we physically experienced that concordance.So you can imagine my surprise when, thirty-some years later, I came upon UNIXcommands like sort, sort -r, and grep.
I don?t remember if I actually wept.
Andthese were nothing compared to the marvels I experienced later still, with key-word-in-context extraction, lemmatizers, morphological parsers, part-of-speech tagging, sortingby right and left context, and the full toolkit of corpus processing tools that exist today.In those days it took a lot of patience and physical effort to build a concordance.But it also took a lot of patience and physical effort to use a concordance.
A printedconcordance to the Shakespeare corpus was a vast index in which, for each word,you could find every line it occurred in, and you learned where that line appeared inShakespeare?s writings.
You would then go to the actual physical source text, look itup, and see it in its context.
For example, if, when studying the phrasal verb take upon Iwant to find the full context of This way will I take upon me to wash your liver I only needto open up As You Like It to Act 3, Scene 2, and hunt for it there.
Compare that to thefully-searchable Shakespeare app you can use while sitting on a bus holding your iPad.3.
Encounters Beyond CollegePresident Truman?s Displaced Persons Act of 1948?1950 brought thousands of EasternEuropean immigrants to Minnesota, enabling me to find work more satisfying thanvenetian-blind-wrapping.
I began to teach English to Russians, Poles, Ukrainians, andLatvians.
Depending onwhich of the daughters of the families inmy classes I was tryingto impress, I was motivated to learn something about Slavic and Baltic languages.Soonmy student deferment would run out, and I had to decide between waiting forthe draft (two years) or enlisting (three years).
A persuasive recruiting officer promisedme one year at the Army Language School inMonterey, CA, (now the Defense LanguageInstitute) for my first year.
Shortly after that, my head got shaved and I was suddenlya buck private.
No one had any record of an offer to spend a year in sunny Californialearning Polish.
I was not allowed to examine my file.So I took the U.S. Army Russian Language Proficiency Test instead.
The questionswere in spoken Russian, played on a record player, and the answers were multiplechoice in English.
In those days the art of designing guessproof multiple choice testshad not yet been perfected.
There was kind of a student sport to see howwell you coulddo in choosing answers without looking at the questions (you could usually at least get703Computational Linguistics Volume 38, Number 4a passing score); then you?d go back and read the questions to correct the choices thatweren?t obvious.Although I didn?t fully understand any of the questions, my score came out as ?highfluent?
based in large part on acquired test-taking skills.
After basic training, I was sentto Arlington, VA, for a few months in radio training, after which I was assigned toKyoto, Japan, to a small field station of the Army Security Agency.
My duty: ?listeningto Ivan.?
The Ivans I listened to on short wave radio never had anything interesting tosay: They were Soviet Air Force men reading numbers, which I was supposed to writedown.
Three days of the day shift, three days evening shift, three days night shift, threedays off.
I quickly acquired an uncanny ability to detect Russian numbers against noiseand static.
They were, of course, coded messages.My job was to write the numbers down on the most modern typewriter of the day,a model that had separate keys for zero and one!
(The ordinary office typewriter at thattime had separate keys for only the numbers 2 through 9, since lower-case L could beused for 1 and upper-case O could be used for zero.)
For this work I needed a veryrestricted vocabulary: the Russian long and short versions of the numbers 1?9,1 plus asingle version of zero, and the word for ?mistake.?
If I had been permitted to say whatI was doing I would have said I was in cryptanalysis, but of course actually I was onlycopying down the numbers I heard.
Somebody smart, thousands of miles away, wasfiguring out what they meant.The limited demands on my time and intellect allowed me to wander around inKyoto, with notebooks and dictionaries, trying to learn something about Japanese.
Thelinguistic methods I had learned back home stopped at morphology, the structure ofwords.
I hadn?t had any training in ways of representing the structure of a sentence,but I worked out a do-it-yourself style of sentence diagrams, for both Japanese andEnglish, and I was fascinated when I found the occasional sentence in Japanese whichcould be translated into English word by word backwards, going from the end to thebeginning.When it was time to be discharged, I believed?wrongly?that I was close to mas-tering the language, and I wanted to stay another year or two, because I knew I couldn?tafford to come back to Japan on my own.
I managed, with the help of Senator HubertHumphrey, to be the first Army soldier to get a local discharge in Japan.
As a civilianthere, I supported myself by teaching English.
With two other visiting Americans I waspermitted to work at Kyoto University with the endlessly kind and patient ProfessorEndo Yoshimoto ( ).Professor Endo was the author of the main school grammar of Japanese and oneof the founders of an organization favoring Romanized spelling for Japanese.
With hishelp, my fellow students and I stumbled through old texts and became acquainted withthe categories and terminology of the Japanese grammatical tradition.One of the themes weaving through this essay is the reality that it is not possible torepresent?in a writing system, in a parse, or in a grammar?every aspect of a languageworth noticing.
My study of Japanese confronted me with the realization that for anygiven representation system, it?s important to understand what it represents, and whatis missing.
The Japanese kana syllabary presented me with an early experience of this.The pronunciation of Japanese words is represented by the symbols of a syllabary, butunfortunately the components of complex words in this language, in particular theinflected verbs, are not segmented at syllable boundaries.1 The long form numbers were presumably more distinct in a noisy background.704Fillmore Computational LinguisticsSome verbs have consonant-final stems followed by vowel-initial suffixes, but thisfact is not apparent in the written language.
In the examples in Table 1, the verb stemmeans ?move?
and it ends in a consonant, /k/.
The suffixes all begin with vowels, butthe red kana characters do not reveal the boundary between verb and suffix.It struck me that the written form of a language should not prevent one fromdiscovering its boundaries.
I later learned that in 1946 the American linguist BernardBloch had published a ground-breaking description of Japanese verbmorphology basedon a phonemic transcription (collected and republished as Bloch [1970]) , allowing theregularities in the system to become apparent.Everyone knows that English spelling is a poor representation for English pronun-ciation, but it?s also true that it is a fairly good representation for recognizing derivation-ally related words.
Consider the second syllable in the three words compete, competitive,competition.
If we had to write these words with different letters for the different vowels,we?d be missing something.Yet of course some important generalizations about English can?t be captured inthe analysis of written English alone.
Numerous phonological generalizations require areduction to phonetic features of various kinds, but there are also grammatical general-izations that are hiding from us because of things like (1)whose (not who?s), (2) another(not an other), and the problems that text-to-speech researchers have to face related to thepronunciation of large numbers and indications of currency, like the dollar sign.
In post-war Japan, the fact that the kana writing system obscured morphological boundariesmerely meant that linguists would use phonemic transcriptions.
But as technologyhas advanced beyond cards and typewriters, supporting efforts such as text-to-speechand automatic speech recognition, we can see that written language obscurations (andaffordances) are ubiquitous.4.
Graduate Studies: Phonetics and PhonologyWhile living in Japan I had been keeping track of linguistics goings-on back home, andhad heard that one of the best graduate programs for linguistics was at the University ofMichigan in Ann Arbor.
So when I finally came back to the States, that?s where I went.There was a movement in linguistics in those days toward making linguistics more?scientific?
by designing so-called discovery procedures for linguistic analysis and Iwanted to participate in that work.
The basic textbooks in beginning linguistics classesat Michigan typically provided step-by-step procedures for going from data to units,so this movement was well-supported there.
Kenneth Pike?s Phonemics book had thesub-title: A technique for reducing language to writing (Pike 1947).I had noticed that there were alternative phonemic analyses for both English andJapanese, analyses that resulted in different actual numbers of consonants and vowels.If there?s no consistent way to do phonemic analysis, how can we compare differentlanguages with each other, or be confident in answering a simple question like, ?howTable 1Japanese kana and the obscuration of morpheme boundaries.ugok-u move (plain form)ugok-imasu move (polite form)ugok-anai does not moveugok-eru can move705Computational Linguistics Volume 38, Number 4many vowels does this language have??
I resolved to help design the correct discoveryprocedure for phonemic analysis, founded on the distribution of phonetic primes.
Forthat purpose I studied phonetics in the linguistics department and in the communica-tion sciences program: practical phonetics for field linguistics, acoustic phonetics, andphysiological phonetics in the laboratory.During those years I worked part-time on a Russian?English Machine Translation(MT) project with Andreas Koutsoudas and met many MT researchers.
I participatedin a memorable interview with Yehoshua Bar-Hillel (some of you will remember theoutcome of the nationwide tour that included this visit).
I also worked with speechresearcher Gordon Peterson and mathematician Frank Harary on automatic discoveryprocedures for phonemic analysis, a project that was eventually abandoned.The speech lab was visited once by a group of engineers who proposed devisingautomatic speech recognition by detecting the acoustic properties of individual phonesand mapping these to phonemes, and pairing phoneme sequences with English words.Ilse Lehiste put a damper on their enthusiasm by asking them to try to consistentlydistinguish acoustic traces of the two phonemically different English words, ?you?
and?ill.?
They couldn?t do this (Figure 1).
The properties of the representational systemfor individual phones would not allow them to get to the second step in their plan.This was obviously before anybody thought of large-vocabulary recognizers based onHidden Markov Models or statistics-based guesses derived from language models.Figure 1Unrevealing spectrograms: you and ill, spoken by Keith Johnson.706Fillmore Computational Linguistics5.
On to SyntaxEventually it became necessary to take on syntax.
At Michigan, sentences were spokenof as having a horizontal (syntagmatic) and a vertical (paradigmatic) dimension.
In itshorizontal aspect, a sentence could be seen as a sequence of positions.
In its verticalaspect, each position could be associated with a set of potential occupants of thatposition.In the English Department at Michigan, Charles Fries was constructing a grammarof English that was liberated from traditional notions of nouns and verbs and adjectives,counting on purely distributional facts to discover the relevant word classes.
In theLinguistics Department, Kenneth Pike was elaborating an extremely ambitious viewof language in which, at every level of structure, one could speak of linear sequences ofpositions, labeled roles naming the functions served by the occupants of these positions,and defined sets of the potential occupants (Pike?s preliminary manuscripts appearedin the 1950s and were eventually published as Pike [1967]).
Slots, roles, and fillers?itwas all very procedural.In the midst of all this, something big happened, and suddenly everything changed.I was among the first in Ann Arbor to read Syntactic Structures (Chomsky 1957).
Ibecame an instant convert, and I gave up all ideas of procedural linguistics.
The newview was something like this: The grammar of sentences is more than a set of linear structures separatelylearned. Sentences are generated by hierarchically organized phrase-defining rules. Regularities in the grammar are evidence for rules in the minds of thespeakers. The existence of a variety of sentence types is accounted for in terms of theapplication of rules that move things within, add them to, or delete themfrom, initial representations. There is no procedural way to learn how language is structured; thelinguist?s job is to figure out what rules reside in the minds of speakers. Therefore, linguistics is theory construction.The Chomskyan view flourished; universities that didn?t have linguistics programswanted one.
After I finished my degree I joined William S.-Y.
Wang in the brand newprogram at The Ohio State University in Columbus.
During my decade at Ohio State Iwas completely committed to the new paradigm.
Robert Lees, Chomsky?s first student,visited Ohio State for a time, and I spent lots of time talking to him, working onquestions of rule ordering and conjunction.
While discussing things with him, I wrote apaper on ?embedding rules in a transformational grammar?
that was the first statementof the transformational cycle (Fillmore 1963).The view represented in Chomsky?s Aspects of the Theory of Syntax (Chomsky1965), with its sharp separation of deep structure and surface structure, became themainstream, and I worked within it faithfully, participating eagerly in efforts to com-bine all the rules the young syntacticians had been writing into a single coherentgrammar of English, an effort heavily supported, for some reason, by the U.S. AirForce.
During this period I felt I knew what to do, and I believed that I understood707Computational Linguistics Volume 38, Number 4everything that everybody else in the framework was doing.
That feeling didn?t lastvery long.At one point I did a seminar in which a small group of students and I worked ourway through Lucien Tesnie`re?s E?le?ments de Syntaxe Structurale (Tesnie`re 1959), withoutnecessarily understanding everything in it, and I became aware of a different wayof organizing and representing linguistic facts.
Anyone who looks closely at syntaxknows that it becomes clear very quickly that you can never represent everythingabout a sentence in a single diagram.
Tesnie`re, my first exposure to what evolvedlater on into dependency grammar, made me aware of the impossibility of displayingsimultaneously the functional relations connecting the words in the sentence, the left-to-right sequence of words as the sentence is spoken, and the grouping of words intophonologically integrated phrases.As an extreme example of the kinds of information a Tesnie`re-style dependency treecould contain, I offer you his analysis of a complex sentence from the Latin of Cicero.I?m certain many of you will remember this from your high school studies.
Est enim inmanibus laudatio quam cum legimus quem philosophum non contemnimus?
(?There is in ourhands an oration, which when we read (it), which philosopher do we not despise??)
Ithas roughly the same structure as Here?s a sentence, while reading which, who wouldn?t getconfused?
Figure 2 presents the diagram, but I?ll only point out the connections assignedto one word in it, the relative pronoun quam.Instead of having lines pointing to a single token of the word, Tesnie`re breaks theword quam into two pieces connected by the broken line at the bottom.
The word agreeswith laudatio in gender and number and that connection is indicated by the upperbroken line; it is the marker of the relative clause headed by contemnimus, as shown inthe horizontal structure it is hanging from, and it is the direct object of legimus, bottomFigure 2A Tesnie`re-style dependency tree.708Fillmore Computational Linguisticsright.
This diagram shows more than simple dependency relations, and uses variousingenious tricks and decorations to smuggle in other kinds of facts.
The word-to-wordconnections are shown, but it?s really clear that a system for projecting from such adiagram to a linear string of words spread into phonologically separable phrases has tobe incredibly complex.The fact that dependency diagrams do not show the linear organization of the con-stituent words was presented by me as a representational problem, but in fact Tesnie`reuses precisely this separation to propose a typology of languages according to whetherthey tend to order dependents before heads or heads before dependents, and whetherwithin each language these tendencies vary within different kinds of constructions.
Ina centripetal language the dependents precede the head, in a centrifugal language thehead precedes the dependents.
There are extreme and moderated varieties of each ofthese in his scheme.Tesnie`re also described a number of conjoined structures in French for which heused the terminology of embryological mistakes, one kind being monsters that haveone head and more than one tail.
In general these correspond to Verb Gapping in ourterms (John likes apples and Mary oranges).
Another kind of embryological mistake hasmore than one head and a single tail, like Right Node Raising (John likes and Mary detestsanchovies), and the most monstrous of all are capital H-shaped monsters with two headsand two tails, like the kinds of sentences Paul Kay and Mary Catherine O?Connor andI played with in a paper (Fillmore, Kay, and O?Connor 1988) on ?let alne?
(I wouldn?ttouch, let alne eat, shrimp, let alne squid).
I think these phenomena have more to dowith sequencing patterns than with dependency relations, but I found it interesting thatTesnie`re delighted in exploring these kinds of structural complexities.
(My sensitivityto tone in French prose isn?t good enough to know whether in these descriptions ofsyntactic monsters Tesnie`re was having fun.
I?m not helped in that uncertainty byphotographs I?ve seen of the man.
)I ended up favoring phrase structure representations, partly because dependencyrepresentations have no easyway to identify a predicate or verb phrase (VP) constituent,and I?d like to believe that the VP can in general be treated as naming a familiarcategory (eating meat, parking a car, being breakable, etc.).
But I mainly preferredphrase-structural representations because they offer more material upon which to as-sign intonational contours.6.
What About Meaning?When linguists turned to the predicate calculus as a representation for sentence mean-ing, many were interested mainly in quantification and negation, where it?s possible toshow how complex logical structures can be formulated in ways that pay no attentionto the actual meanings of the words that name either the predicates or the arguments.
I,however, was specifically interested in the inner structure of the predicates themselves.So I encountered a representational problem when working with the notation that wascommon at the time.When working on meaning, linguists often used prefix notation, allowing theordered list of symbols following the name of the predicate to stand for the ?-arity?
?the number of arguments?of the particular predicate.
Thus P(a) could represent anadjective like hungry or a verb like vanish; P(a,b), relating two things to each other,could stand for an adjective like different or a verb like love; and P(a,b,c) with threearguments could stand for an adjective like intermediate or a verb like give, show, or709Computational Linguistics Volume 38, Number 4tell.
This notation also allowed one to represent cases in which the arguments couldthemselves be predications, permitting recursion.While working with the prefix notation I was struck by the fact that althoughthis representation afforded one the chance to make claims across diverse classes ofpredicates, it simultaneously obscured certain information about the arguments of thosepredicates?important semantic commonalities about classes of arguments.There are centuries-old traditions by which schoolteachers explain that the subjectnames the agent in an event and the object tells us what is affected by the agent?s actions,but it?s trivially easy to find examples that show that such generalizations don?t hold.Similarly, in a predicate?argument formula, there is nothingmeaningful about being thefirst or second or third item in a list.
Does it make sense to let the position in an orderedlist represent the semantic role of an argument in a predication?
Consider the followingexamples in which arguments are interchanged:(1) He blamed the accident on me.??
He blamedme for the accident.
(2) He strikesme as a fool.??
I regard him as a fool.
(3) Chuck bought a car from Jerry.??
Jerry sold a car to Chuck.In Example (1) the second and third arguments of blame are interchanged in theirgrammatical realization.
In Example (2), with the pair strike and regard, the first andsecond arguments are interchanged.
And in Example (3), with buy and sell, the first andthe third are interchanged.I felt that there ought to be some way of recognizing the sameness of the semanticfunctions of these arguments independently of where they happen to be sitting in anordered list.
An alternative was spelled out in a rambling paper called ?The Case forCase?
published in 1968 (Fillmore 1968).
It proposed a universal list of semantic roletypes (?cases?).
Configurations of these cases could then characterize the semanticstructures of verb and adjective meanings.
In this way, lexical predicates could beshown as differing according to the collection of cases that they required (obligatory)or welcomed (optional).The theory embedded in this view is that semantic relations (?deep cases?)
aredirectly linked to argument meanings.
(So in the sentence John gave Mary a rose, Johnis the Agent, Mary is the Recipient, and a rose is the transmitted Object.)
Grammaticalroles (subject, object) and markings (choice of preposition, etc.)
are predicted from caseconfigurations.
(So the Agent could be the subject, the Object could be the direct object,and the Recipient could be introduced with the preposition to.)
Generalizations areformulated in terms of specific named cases, for which a hierarchy is defined, and thelist of cases is finite and universal.The variable ?valences?
(a term from Tesnie`re) of a single verb can be explained interms of the cases available to it.
The starting examples in this discussion were with theverb open.
Its valences correlate with the cases available to it:(4) Agent>Instrument>Object hierarchy illustrated with V openO = The door openedAO = I opened the doorIO = The key opened the doorAIO = I opened the door with the key710Fillmore Computational LinguisticsThe occupants of nuclear syntactic slots (subject and object) are determined by thehierarchy, the rest are marked by prepositions (or in the case of arguments whose shapeis a VP or a clause, various other markers or complementizers).There was a time when Case Grammar, so-called, was very popular, and partlybecause of that I ended up in Berkeley, California, and eventually participated in thevibrant Cognitive Science Program there.
When I first arrived, I continued to work onCase Grammar and Transformational Grammar, disappointed that the former was notaccepted as a contribution to the latter.Gradually, the theory and representation of Case Grammar revealed a way to defineentities at a different level: Given lists of cases, it was possible to define situation typesas assemblies of these.
I referred to these assemblies as case frames.
With a large numberof case or semantic role names, it should be possible to define a very large number ofsituation types.
For example, Agent-Instrument-Object is some kind of caused change.Object-Path-Goal is some kind of motion event, and so on:(5) Case Frame Situation Types exemplified Agent, Instrument, Object: I fixed it with a screwdriver. Object, Path, Goal: The water flowed through the crack in the floor intothe storage room. Experiencer, Content: I remember the accident. Stimulus, Experiencer: The noise scared me. Stimulus, Experiencer, Content: The noise reminded me of the accident.Various proposals emerged (by John Sowa among others) that greatly increasedthe number of cases, enabling descriptions of more and more kinds of situations andevents.
Researchers working with semantic roles tend to think of them as identifyingthe roles of participants in the event, in the case of verbs that describe events.
But thisconceptualization shed light on some problematic (and eventually revealing) cases.
Oneof the first to hit me involved some uses of the verb replace.
Consider this sentence: TodayI finally replaced that bicycle that got stolen a year ago.Notice that the bicycle that got stolen a year ago was not a participant in the Re-placement event that happened today, at least not in the usual sense that is intended inwork on semantic roles or cases.
The bicycle can be mentioned in the sentence, given thegrammatical requirements of the verb replace, because the bicycle was a participant inthe narrative that defines a replacement event.This led to a conceptualization that will be familiar to readers of this journal.
Insteadof defining frames in terms of assemblies of roles, what aboutmaking frames primary, anddefining roles in terms of the frames?
I then started thinking that the job of lexical semanticsis to characterize frames on their own, and work out the participant structures frameby frame.7.
Beyond Syntax and SemanticsAt some point I was invited to give some lectures at Roger Schank?s Artificial Intel-ligence lab at Yale, where I witnessed work on information retrieval in the form ofa system that automatically collected information from newspaper accounts of trafficaccidents.
My impression was that the system was given texts that were known to be711Computational Linguistics Volume 38, Number 4about traffic accidents, and it was already provided with a checklist of informationto look for, based ultimately on the style sheets used by reporters working on trafficaccident assignments, or, really, ultimately, on the reporting traditions of the local policedepartments.The checklist included names, ages, and addresses of drivers, passengers, andvictims; the make, model, and year of the involved vehicles; location of the accident;directions of moving vehicles; presence of injuries or fatalities; reports from policeauthorities, and so forth.
The system needed to recognize capital letters, punctuation,numbers, and a set of words like driver, passenger, victim, ambulance, street, avenue,highway, sheriff, officer, vehicle, and so on, so that when it came upon something likethe following it would know what to do:Walter O. Magnusson, 23, of 79 W. Walnut St., Hartland, was westbound on 28th Streetnear Blossom Road in a 1998 Chevrolet pickup when he and passenger, Wilma J. Alter,27, same address, argued.
According to Sheriff Deputy Carl Voegelin, Magnussongrabbed the steering wheel, causing the vehicle to strike a tree on the south side of theroad.
Magnusson was taken by private vehicle to Hartland Community Hospital withpossible injuries.
The pickup was registered to Clarence Barker of 66 Larkin Rd.,Jarviston.I wondered if a kind of general purpose information extraction process could bedesigned in which the system didn?t know in advance what the text was about, but inwhich particular words in the text would evoke their own checklist?a list of things to lookfor that come with the entry for the word.
The presence in a text of a word like revenge, forexample, could initiate a search for the identity of the offender, the name of the injuredparty and the avenger, the punishment inflicted or intended, and so forth, a checklistthat would also be evoked by a dozen other words in the same frame.
In a case likethe given text, the heading of a newspaper article such as ?Fatal Accident on Highway17?
would get things started.
That is, a word could evoke a frame, and the semanticparser?s job would be to find the elements of that frame in the text, sometimes in thesame sentence, in positions determined by the grammar of the word, and sometimes inneighboring sentences.The idea behind frame semantics is that speakers are aware of possibly quite com-plex situation types, packages of connected expectations, that go by various names?frames, schemas, scenarios, scripts, cultural narratives, memes?and the words in ourlanguage are understood with such frames as their presupposed background.
Of coursethese terms are used to designate concepts developed with slightly different meanings,and for different purposes, in Artificial Intelligence, Cognitive Psychology, and Sociol-ogy.
I use the word ?frame?
promiscuously to cover all of them.
In ?frame semantics,?however, I?m particularly concerned with those that are clearly linked to items oflinguistic form: words or constructions.8.
RISK: The FrameIn 1988, at a summer school in Pisa run by the late Antonio Zampolli, I met Sue Atkins,the lexicographer.
I was teaching a course on frame semantics, and she was teaching acourse on corpus-based lexicography that included an examination of concordance linesfor the verb risk.
Sue and I decided to join forces and come up with a complete framedescription of risk, based on corpus evidence, that would show how the words thatbelong to this framework.
The title of the first paper that resulted from this research was?Toward a frame-based lexicon: The semantics of RISK and its neighbors.
?We presented712Fillmore Computational LinguisticsFigure 3HyperCard: Representational breakthrough.the main arguments, jointly, at the 1991 meeting of the ACL in Berkeley.
The paper waspublished as Fillmore and Atkins (1992).9.
The Gradual Birth of FrameNetAlong with some colleagues, I decided to seek funding to build a resource that wouldfeature a large number of frames, along with the words that belong to those frames.In any such funding request, the authors are challenged to represent the project incompelling detail so as to allow reviewers to envision the possibilities.
Our first attempt,created by John B. Lowe, made use of a demo created using the new tool HyperCard(Figure 3).
Sadly, the funders were not impressed.Of course, all of this work was carried out against the backdrop of George Miller?sground-breaking project, WordNet2 (Miller 1995; Fellbaum 1998).
By 1992 the creators ofWordNet had demonstrated the power and utility of a searchable and open database ofEnglish words, organized around core semantic relations such as synonymy, meronymyand holonymy, hypernymy and hyponymy, and so on.
Although WordNet was aninspiration to us, its purposes and structure are somewhat different from those ofFrameNet.The goal of the FrameNet project3 (Fillmore, Johnson, and Petruck 2003) was tocreate a database, to be used by humans and computers, that would include a list of all2 http://wordnet.princeton.edu.3 http://framenet.icsi.berkeley.edu.713Computational Linguistics Volume 38, Number 4COMPLIANCEDefinition: This frame concerns Acts and States of Affairs for which Protagonistsare responsible and which either follow or violate some set of rules or Norms .Figure 4Definition of the Compliance frame.of the Frames that we could possibly have time to describe.
Frames are the cognitiveschemata that underlie the meanings of the words associated with that Frame.
Theexample of the frame Compliance is given in Figure 4.
It begins with a definition ofthe frame in terms of Frame Elements (FEs), which are the things worth talking aboutwhen a given frame is relevant.
(There are generally three to eight FEs per frame.
)We currently have about 1,200 Frames defined and described.
A fragment of thelist of Frames alphabetically surrounding Compliance runs as follows: Compatibility,Competition, Complaining, Completeness, Compliance, Concessive....Next, we attempt to catalogue the Lexical Units (LUs) associated with the frame.These are words which, when encountered in a written or spoken text, may ?evoke?the frame.
Currently, our total number of Lexical Units across all 1,200 Frames is about13,000.
Example (6) lists a sample of the LUs tied to the Compliance frame.
(6) (in/out) line.n, abide.v, adhere.v, adherence.n, breach.n, breach.v, break.v,by-pass.v, circumvent.v, compliance.n, compliant.a, comply.v, conform.v,conformity.n, contrary.a, contravene.v, contravention.n, disobey.v, flout.v,follow.v, honor.v, in accordance.a, keep.v, lawless.a, noncompliance.n,obedient.a, obey.v, observance.n, observant.a, observe.v, play by therules.v, submit.v, transgress.v, transgression.n, violate.v, violation.nNot all LUs are simple words.
Many are phrasal words, such as take off, talk down,work out, pick up.
Some are idiomatic phrases: of course, all of a sudden.
Finally, some areproducts of constructions: best friends, make one?s way.Beyond the specification of cognitive and cultural frames, and their linguistictriggers or anchors, FrameNet analyses endeavor to catalogue the ways that FrameElements of a Frame are linguistically expressed, specifically in terms of syntacticstructures.
For example, in the Compliance frame, what are the possible forms in whichthe FEs can be expressed?To begin to answer this question, we compile for each Frame a set of Annotations.Each includes sentences that exemplify the Frame and its FEs, and demonstrate theuse of the relevant Lexical Units.
Examples (7)?
(9) illustrate how the subject of a sen-tence can instantiate three of the FEs in the Compliance frame, given the lexical itemsused.
(7) The wiring in this room is in violation of the building code.State of Affairs(8) You have broken the rules.Protagonist(9) My action was in compliance with the school?s traditions.Act714Fillmore Computational LinguisticsFigure 5Compliance frame in a network of frames.Finally, lexical entries summarize the mappings of individual FEs, Lexical Unit byLexical Unit.
For example, for the FE Norm within the Compliance frame, we find thefollowing LUs, where ?X?
is the variable whose value will be the Norm for each LU:(10) Lexical Units linked to the Frame Element Norm within Compliancecomplies [with X] conforms [to X]is in breach [of X] abides [by X]violates [X] adheres [to X]Note that these Lexical Units (all of which would be linked to the Frame ElementNORM in the Compliance frame) include antonyms, and thus these sets of LUs differfrom synsets.
Polysemous LUs can be linked to different frames.
For example, adherebelongs not only to the Compliance frame but also to the Attaching frame.The frames themselves are organized in a network, linked by various kinds ofrelations, including inheritance, part-of, presupposes,4 and so on.
Figure 5 is a glimpseof the place held by the Compliance frame in the network.Compliance inherits from both the Social-Behavior-Evaluation frame and theSatisfying frame.
The Satisfying frame includes satisfying desires, fulfilling ambi-tions, meeting one?s goals, and so forth.
Compliance elaborates on that by specifyingthat the Norm FE is some kind of institutionalized rule or law or principle or practice,and that the words in this frame evaluate people and their acts with reference to suchnorms.Frame-to-Frame relations also include FE-to-FE relations: For example, the BuyerFE of Commerce-buy is the Agent FE of Transfer.
Linking generalizations familiar4 Strictly speaking, the notion of frame presupposition is captured by several relations, including ?Using?and ?Perspective on.
?715Computational Linguistics Volume 38, Number 4from ?standard?
thematic roles can be captured by relating smaller frames to the moreschematic ones they inherit.The FrameNet annotation sets include not only the ?lexicographic?
annotations,but also a number of ?full-text?
annotations, where all words are annotated, that is,annotation layers are provided for each frame-relevant word.
In such examples, wefrequently encounter data that force us to expand and refine FrameNet.In most examples we can see core FEs, those that are required by the frame, aswell as peripheral FEs, those that fill out the roles traditionally described as adjunctsof time, place, manner, and so on.
But as we expand our catch to include sentencesbeyond those that simply provide good examples of the Frames, we encounter FEsthat we label as extrathematic.
This name is given to expressions that are syntacti-cally governed by a frame-bearing element, but convey information that is outsideof the Frame.
As Example (11) indicates, extrathematic elements frequently introducea new Frame, and thus are crucial for the enterprise of automatic understanding ofconnected text.
(11) Types of FEs in a sentence.The army destroyed the village yesterday in retaliation(CORE) [target] (CORE) (PERIPHERAL) (EXTRATHEMATIC)In our annotation work it has become necessary to notice contexts in which thesemantic head of a phrase and the syntactic head of a phrase are not identical.
Becausewe are interested in positioning frame-relevant words in their contexts, we have recog-nized support verbs, support prepositions, and transparent nouns.
What we find withsupport verbs and prepositions is that the governed noun is the LU that evokes theframe.
In expressions like take a turn, make a decision, wreak havoc, lodge a complaint, say aprayer, and give advice, the frame is evoked by the noun.
The same can be observed within trouble, at risk, under arrest, under consideration, and at rest.
The verb or prepositiondetermines the grammatical functioning, but also (in the case of the verb) features ofaspect, tone, and voice.Transparent nouns are nouns that intervene, in a [N1 of N2] structure, between theframe context and the frame-relevant noun.
That is, in examples like wreak this KIND ofhavoc, drink a DROP of vodka, divorce that JERK of a husband, it is the second (underlined)noun that matters in our understanding of the semantic nature of the Frame Element.These grammatical types may also be helpful in the enterprise of automatic understand-ing of connected text.We have noticed regularities that may be useful to expand upon for FEs: Theycan have ?semantic types?
associated with them, intended to say something about thetypes of entities, and thus phrases, that can serve in those roles.
For example, Agents,Experiences, and Recipients are of the semantic type ?sentient.?
This dimension is notwell-developed, currently, consisting mostly of categories such as artifact, container,factive (for verbs), and so on.The FrameNet wordlist is mostly from the ?general vocabulary?
and for the mostpart ignores the tens of thousands of words that either lack frames of their own orthat have specialist frames for which ordinary lexicographic inquiry cannot help.
Theseinclude artifact names, natural kinds, terrain features, and so on.
For these we wouldlike to make progress with what we call ?Gov-X annotation?
: annotating words withrespect to the frames they belong comfortably in.
For example, gunwould be annotatedin sentences where it is governed by brandish, fire, shoot, load, and so forth.716Fillmore Computational Linguistics10.
ConstructionsIn recent years we have added to the FrameNet database something we call the Con-structicon, which is a list of grammatical constructions, descriptions of their compo-nents, and descriptions of the properties and functions of the phrases or constituentsthat they license (Fillmore, Lee-Goldman, and Rhodes 2012).Some members of the team are participants in a movement called ConstructionGrammar, supporting a view of grammar as a collection of constructions, where eachconstruction constitutes a way of assembling the meaning of the components into asemantic whole, not obviously predictable, by familiar principles, from the meanings ofthe parts.This collection includes special constructions like the ones that license the biggerthey come the harder they fall, or rate expressions like twenty gallons an hour, or unusualsymmetric-relation expressions like I am friends with the President.
The collection isnot limited to special-purpose or idiosyncratic constructions, but also includes majorconstructions with broad semantic import and cross-linguistic relevance, such asconditional sentences, exclamations, a large variety of coordinating constructions, andcomparative constructions.The constructions bring frames of their own, and the analysis task is to integratethe information from the LUs embedded within their Frames with those contributed bythe constructions.
The Construction is linked to a set of sentences annotated accordingto the properties of the construction being analyzed.
Professor Hiroaki Sato of SenshuUniversity in Japan has designed a temporary tool for viewing the constructionalinformation.11.
ConclusionThe ultimate goal is to be able to understand everything that can be known about aword, or a sentence, or a language, or speakers?
knowledge of their language.
This goalcan never be achieved, but one keeps trying, piece by piece.
I recently came upon, in mynotes, a program from the 1988 Pisa Institute that showed I was on a panel one eveningaddressing the question ?What would a linguist like to find in the Dictionary of 2001?
?I don?t remember what I said, but I think that if everything could work the way weplanned it, and if the project ever gets the funds to complete the job, the ICSI FrameNetdatabase of 2020 will stand a chance of being close to that ideal dictionary of 2001.
I wantto thank the ACL Executive Committee again for the recognition, and the conferenceparticipants for listening.ReferencesBloch, Bernard.
1970.
Bernard Blochon Japanese.
Yale University Press,New Haven, CT.Bloomfield, Leonard.
1933.
Language.Henry Holt, New York.Chomsky, Noam.
1957.
Syntactic Structures.Mouton, The Hague.Chomsky, Noam.
1965.
Aspects of the Theory ofSyntax.
The MIT Press, Cambridge, MA.Fellbaum, Christiane.
1998.WordNet:An Electronic Lexical Database.The MIT Press, Cambridge, MA.Fillmore, Charles J.
1963.
The position ofembedding transformations in a grammar.Word, 18:208?231.Fillmore, Charles J.
1968.
The case for case.
InEmmond Werner Bach and Robert ThomasHarms, editors, Universals in LinguisticTheory, Chapter 1.
Holt, Rinehart, andWinston, New York, pages 1?88.Fillmore, Charles J. and Beryl T. S. Atkins.1992.
Towards a frame-based organizationof the lexicon: The semantics of RISK andits neighbors.
In A. Lehrer and E. Kitay,editors, Frames, Fields, and Contrast: New717Computational Linguistics Volume 38, Number 4Essays in Semantics and Lexical Organization.Lawrence Erlbaum Associates, Hillsdale,NJ, pages 75?102.Fillmore, Charles J., Christopher R.Johnson, and Miriam R. L. Petruck.2003.
Background to Framenet.International Journal of Lexicography,16(3):297?333.Fillmore, Charles J., Paul Kay, andMary Catherine O?Connor.
1988.Regularity and idiomaticity ingrammatical constructions:The case of ?let alne?.
Language,64:501?538.Fillmore, Charles J., Russell Lee-Goldman,and Russell Rhodes.
2012.
The FrameNetconstructicon.
In Ivan A.
Sag and HansC.
Boas, editors, Sign-based ConstructionGrammar.
CSLI, Stanford, CA.Miller, George A.
1995.
WordNet: A lexicaldatabase for English.
Communicationsof the ACM, 38(11):39?41.Nida, Eugene.
1947.
Linguistic Interludes.Summer Institute of Linguistics,Glendale, CA.Pike, Kenneth L. 1947.
Phonemics: ATechnique for Recuding Languages to Writing.University of Michigan PublicationsLinguistics 3, Ann Arbor, MI.Pike, Kenneth L. 1967.
Language in Relation toa Unified Theory of the Structure of HumanBehavior.
Janua Linguarum, series maior,24.
Mouton, The Hague.Sapir, Edward.
1921.
Language: AnIntroduction to the Study of Speech.Harcourt, Brace and Company, New York.Tesnie`re, L. 1959.
E?le?ments de SyntaxeStructurale.
Klincksieck, Paris.718
