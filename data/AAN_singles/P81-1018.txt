A Ru le -based  Conversat ion  Par t i c ipantRobert E. FrederkingComputer Science Department, Carnegie-Mellon UniversityPittsburgh, Pennsylvania 15213Abst rac tThe problem of modeling human understanding andgeneration of a coherent dialog is investigated by simulating aconversation participant.
The rule-based system currentlyunder development attempts to capture the intuitive conceptof "topic" using data structures consisting of declarativerepresentations of the subjects under discussion linked to theutterances and rules that generated them.
Scripts, goal trees,and a semantic network are brought to bear by general,domain-independent conversational rules to understand andgenerate coherent topic transitions and specific outpututterances.1.
Ru les ,  top ics ,  and  ut te rancesNumerous systems have been proposed to model human useof language in conversation (speech acts\[l\],  MICS\[3\],Grosz \[5\]).
They have attacked the problem from severaldifferent directions.
Often an attempt has been made todevelop some intersentential analog of syntax, despite thesevere problems that grammar-oriented parsers haveexperienced.
The program described in this paper avoids theuse of such a grammar, using instead a model of theconversation's topics to provide the necessary connectionsbetween utterances.
It is similar to the ELI parsing system,developed by Riesbeck and Schank \[7\], in that it usesrelatively small, independent segments of code (or "rules") todecide how to respond to each utterance, given the contextof the utterances that have already occurred.
The programcurrently operates in the role of a graduate studentdiscussing qualifier exams, although the rules and controlstructures are independent of the domain, and do not assumeany a priori topic of discussion.The main goals of this project are:?
To develop a small number of general rules thatmanipulate internal models of topics in order toproduce a coherent conversation.?
To develop a 'representation for these models oftopics which will enable the rules to generateresponses, control the flow of conversation, andmaintain a history of the system's actions duringthe current conversation.This research was sponsored in part by the DefenseAdvanced Research Projects Agency (DOO), ARPA Order No.3597, monitored by the Air Force Avionics Laboratory UnderContract F33615-78-C- 1551.The views and conclusions contained in this document arethose of the author and should not be interpreted asrepresenting the official policies, either expressed or implied,of the Defense Advanced Research Projects Agency or theUS Government.?
To integrate information from a semanticnetwork, scripts, dynamic goal trees, and thecurrent conversation in order to allow intelligentaction by the rules.The rule-based approach was chosen because it appears towork in a better and more natural way than syntactic patternmatching in the domain of single utterances, even though agrammatical structure can be clearly demonstrated there.
If itis awkward to use a grammar for single-sentence analysis,why expect it to work in the larger domain of humandiscourse,, where there is no obviously demonstrable"syntactic" structure?
in place of grammar productions,rules are used which can initiate and close topics, and formutterances based on the input, current topics, and long-termknowledge.
This set of rules does not include any domain-specific inferences; instead, these are placed into thesemantic network when the situations in which they apply arediscussed.It is important o realize that a "topic" in the sense used inthis paper is not the same thing as the concept of "focus"used in the anaphora and coreference disambiguationliterature.
There, the idea is to decide which part of asentence is being focused on (the "topic" of the sentence),so that the system can determine which phrase will bereferred to by any future anaphoric references (such aspronouns).
In this paper, a topic is a concept, possiblyencompassing more than the sentence itself, which is"brought to mind" when a person hears an utterance (the"topic" of a conversation).
It is used to decide whichutterances can be generated in response to the inpututterance, something that the focus of a sentence (by itself)can not in general do.
The topics need to be stored (asopposed to possibly generating them when needed) simplybecause a topic raised by an input utterance might not beaddressed until a more interesting topic has been discussed.The data structure used to represent a topic is simply anobject whose value is a Conceptual Dependency (or CD) \[8\]description of the topic, with pointers to rules, utterances,and other topics which are causally or temporally related to it,plus an indication of what conversational goal of the programthis topic is intended to fulfill.
The types of relationsrepresented include: the rule (and any utterances involved)that resulted in the generation of the topic, any utterancesgenerated from the topic, the topics generated before andafter this one (if any), and the rule (and utterances) thatresulted in the closing of this topic (if it has been closed).Utterances have a similar representation: a CD expressionwith pointers to the rules, topics, and other utterances towhich they are related.
This interconnected set of CDexpressions is referred to as the topic-utterance graph, asmall example of which (without CDs) is illustrated in Figure1.1.
The various pointers allow the program to rememberwhat it has or has not done, and why.
Some are used by rulesthat have already been implemented, while others areprovided for rules not yet built (the current rules aredescribed in sections 2.2 and 3).83UTTS t .
U1 t .
U2 / .
U3TOPICS e .
1"1 t .
T2 t i .
T3 t .
T4R3 - -Figu re 1 -1 : A topic-utterance graph2.
The  computat iona l  mode lThe system under implementation is, as the title says, a rule-based conversation participant.
Since language wasoriginally only spoken, and used primarily as an immediatecommunication device, it is not unreasonable to assume thatthe mental machinery we wish to model is designed primarilyfor use in an interactive fashion, such as in dialogue.
Thus, itis more natural to model one interacting participant han to tryto model an external observer's understanding of the wholeinteraction.2.1.
ControlOne of the nice properties of rule-based systems is that theytend to have simple control structures.
In the conversationparticipant," the rule application routine is simply aninitialization followed by a loop in which a CD expression isinput, rules are tried until one produces a reply-wait signal,and the output CD is printed.
A special token is output tOindicate that the conversation is over, causing an exit fromthe loop.
One can view this part of the model as aninput/output interface, connecting the data structures thatthe rules access with the outside world.Control decisions outside of the rules themselves are handledby the agenda structure and the interest-rating routine.
Anagenda is essentially a list of lists, with each of the sublistsreferred to as a "bucket".
Each bucket holds the names ofone or more rules.
The actual firing of rules is not as simpleas indicated in the above paragraph, in that all of the rules ina bucket are tested, and allowed to fire if their test clauses aretrue.
After all the rules in a bucket have been tested, if any ofthem have produced a reply-wait signal, the "best" utteranceis chosen for output by the interest-rating routine, and themain loop described above continues.
If none have indicateda need to wait, the next bucket is then tried.
Thus, the rules inthe first bucket are always tried and have highest priority.Priority decreases on a bucket.by.bucket basis down to thelast bucket.
In a normal agenda, the act of firing is the sameas what I am calling the reply-wait signal, but in this systemthere is an additional twist.
It is necessary to have a way toproduce two sentences in a row, not necessarily tightlyrelated to each other (such as an interjection followed by aQuestion).
Rather than trying to guarantee that all such setsof rules are in single buckets, the rules have been given theability to fire, produce an utterance, cause it to be outputimmediately, and not have the agenda stopped, simply byindicating that a reply-wait is not needed.
It is also possiblefor a rule to fire without producing either an utterance or areply-wait, as is the case for rules that simply create topics, orto produce a list of utterances, which the interest-rater mustthen look through.The interest-rating routine determines which of theutterances produced by the rules in a bucket (and notimmediately output) is the best, and so should be output.
Thisis done by comparing the proposed utterance to our model ofthe goals of the speaker, the listener, and the person beingdiscussed.
Currently only the goals of the person beingdiscussed are examined, but this will be extended to includethe goals of the other two.
The comparison involves lookingthrough our model of his goal tree, giving an utterance ahigher ranking for matching a more important goal.
This isadjusted by a small amount to favor utterances which implyreaching a goal and to disfavor those which imply failing toreach it.
Goal trees are stored in long-term memory (see nextsection).2.2.
MemoriesThere are three main kinds of memory in this model: workingmemory, long.term memory, and rule memory.
The datastructures representing working memory include severalglobal variables plus the topic-utterance graph.
The topic-utterance graph has the general form of two doubly-linkedlists, one consisting of all utterances input and output (inchronological order) and the other containing the topics (inthe order they were generated), with various pointersindicating the relationships between individual topics andutterances.
These were detailed in section 1.Long-term memory is represented as a semantic network \[2\].Input utterances which are accepted as true, as well as theirimmediate inferences, are stored here.
The typical semanticnetwork concept has been extended somewhat o include twotypes of information not usually found there: goal trees andscripts.Goal trees \[6, 3\] are stored under individual tokens or classes(on the property GOALS) by name.
They consist of severalCD concepts linked together by SUBGOAL/SUPERGOALlinks, with the top SUPERGOAL being the most importantgoal, and with importance decreasing with distance below thetop of the goal tree.
Goal trees represent the program'smodel of a person or organization's goals.
Unlike an earlierconversation program \[3\], in this system they can be changedduring the course of a conversation as the program gathersnew information about the entities it already knows somethingabout.
For example, if the program knows that graduatestudents want to pass a particular test, and that Frank is agraduate student, and it hears that Frank passed the test, itwill create an individual goal tree for Frank, and remove the -goal of passing that test.
This is clone by the routine whichstores CDs in the semantic network, whenever a goal ismentioned as the second clause of an inference rule that isbeing stored.
If the rule is stored as true, the first clause ofthe implication is made a subgoal of the mentioned goal in theactor's goal tree.
If the rule is negated, any subgoal matchingthe first clause is removed from the goal tree.84/ rAs for scripts \[9\], these are the model's episodic memory andare stored as tokens in the semantic network, under the classSCRIPT.
Each one represents a detailed knowledge of somesequence of events (and states), and can contain instances ofother scripts as events.
The individual events are representedin CD, and are generally descriptions of steps in a commonlyoccuring routine, such as going to a restaurant or taking atrain trip.
In the current context, the main script deals withthe various aspects of a graduate student taking a qualifier.There are parameters to a script, called "roles" ?
in this case,the student, the writers of the exam, the graders, etc.
Eachrole has some required preconditions.
For example, anywriter must be a professor at this university.
There are alsopostconditions, such as the fact that if the student passes thequal he/she has fulfilled that requirement for the Ph.D. andwill be pleased.
This post-condition is an example of adomain-dependent inference rule, which is stored in thesemantic network when a situation from the domain isdiscussed.Finally, we have the rule memory.
This is just the group ofdata objects whose names appear in the agenda.
Unlike theother data objects, however, rules contain Lisp code, storedin two parts: the TEST and the ACTION.
The TEST code isexecuted whenever the rule is being tried, and determineswhether it fires or not.
It is thus an indication of when this ruleis applicable.
(The conditions under which a rule is tried weregiven in the section on Control, section 2.1).
The ACTIONcode is executed when the rule fires, and returns either a listof utterances (with an implied reply-wait), an utterance withan indication that no reply wait is necessary, or NIL, thestandard Lisp symbol for "nothing".
The rules can have sideeffects, such as creating a possible topic and then returningNIL.
Although rules are connected into the topic-utterancegraph, they are not really considered part of it, since they area permanent part of the system, and contain Lisp code ratherthan CO expressions.3 .
An example  exp la inedA sample of what the present version of the system can do willnow be examined.
It is written in MacLisp, with utterancesinput and output in CO.
This assumes the existence ofprograms to map English to CO and CD to English, both ofwhich have been previously done to a degree.
The agendacurrently contains six rules.
The two in the highest prioritybucket stop the conversation if the other person says"goodbye" or leaves (Rule3-3 and Rule3-4).
They are thereto test the control of the system, and will have to be mademore sophisticated (i.e., they should try to keep up theconversation if important active topics remain).The three rules in the next bucket are the heart of the systemat its current level of development.
The first two raise topicsto request missing information.
The first (Rule1) asks aboutmissing pre-conditions for a script instance, such as whensomeone who is not known to be a student takes a qualifier.The second (Rule2) asks about incompletely specified post-conditions, such as.the actual project that someone must doif they get a remedial.
At this university, a remedial is aconditional pass, where the student must complete a projectin the same area as the qual in order to complete this degreerecluirement; there are four quals in the curriculum.
The thirdrule in this bucket (Rule4) generates questions from topicsthat are open requests for information, and is illustrated inFigure 3-1.RULE4TEST: (FOR-EACH TOPICS(AND (EQUAL 'REQINFO (GET X'CPURPOSE))(NULL (GET X 'CLOSEDBY))))ACTION: (MAPCAN '(LAMBDA (X)(PROG (TMP)(RETURN (COND ((SETQ TMP(QUESTIONIZE (GET-HYPO (EVAL X))))(MAPCAN '(LAMBDA (Y)(COND (Y(LIST (UTTER Y (LIST X))))))TMP))))))TEST-RESULT).Test: Are there any topics which are requests for informationwhich have not been answered?Action: Retrieve the hypothetical part, form all "necessary"questions, and offer them as utterances.Figure 3-1 : Rule4The last bucket in the agenda simply has a rule which says "1don't understand" in response to things that none of theprevious rules generated a response to (RuleK).
This servesas a safety net for the control structure, so it does not have toworry about what to do if no response is generated.Now let us look at how the program handles an actualconversation fragment.
The program always begins by asking"What's new?
", to which (this time) it gets the reply, "Frankgot a remedial on his hardware qual."
The CO form for this isshown in Figure 3-2 (the program currently assumes that theperson it is talking to is a student it knows named John).
TheCD version is an instance of the qual script, with Frank,hardware, and a remedial being the taker, area, and result,respectively.U0002((< = > ($QUAL &AREA (=HARDWARE*) &TAKER('FRANK') &RESULT ('REMEDIAL'))))(ISA ('UTTERANCE*) PERSON "JOHN" PREDUTrS)Figure 3-2."
First input utteranceWhen the rules examine this, five topics are raised, one due tothe pre-condition that he has not passed the qual before (byRule1), and four due to various partially specified post-conditions (by Rule2):?
If Frank was confident, he will be unhappy.?
If he was not confident, he will be content.?
He has to do a project.
We don't know what.?
If he has completed his project, he might be ableto graduate.The system only asks about things it does not know.
In thiscase, it knows that Frank is a student, so it does not ask aJoout85that.
As an example, the topic that asks whether he is contentis illustrated in Figure 3-3.T0005((CON ((< = > ($QUAL &AREA('HARDWARE')&TAKER('FRANK')&RESULT('REMEDIAL'))))LEADTO((CON ((ACTOR ('FRANK') IS('CONFIDENCE" VAL (> 0)))MOP('NEG" "HYPO'))LEADTO((ACTOR ('FRANK') IS ('HAPPINESS"VAL (0)))))MOP('HYPO'))))(INITIATED (U0013) SUCC T0009 CPURPOSEREQINFOINITIATEDBY (RULE2 U0002) ISA ( 'TOPIC')PRED T0004)Figure 3-3: A sample topic in detailAlong with raising these topics, the rules store the utteranceand script post-inferences in the semantic network, under allthe nodes mentioned in them.
The following have beenstored under Frank by this point:?
Frank got a remedial on his hardware qual.?
If he was confident, he'll be unhappy.?
If he was not confident, he'll be content.?
Passing the hardware clual will not contribute tohis graduating.?
He has a hardware project to do.?
Finishing his hardware project will contribute tohis graduating.While these were being stored, Frank's goal tree was altered.This occurred because two of the post-inferences arethemselves inference rules that affect whether he willgraduate, and graduating is already assumed to be a goal ofany student.
Thus when the first is stored, a new goal tree iscreated for Frank (since his interests were represented beforeby the Student goal tree), and the goal of passing thehardware clual is removed.
When 'the second is stored, thegoal of finishing the project is added below that of graduatingon Frank's tree.
These goal trees are illustrated in Figures 3-4and 3-5.
((ACTOR ('STUDENT*) IS (*HAPPINESS" VAL(5)))) ~ Subgoal((< = > ($GRAD &ACTOR ('STUDENT') &SCHOOL("CMU?))))
~ Subgoal((< = > ($QUAL &TAKER ('STUDENT') &AREA('HARDWARE') &RESULT ('PASSED=))))Figure 3.4: A student's goal tree((ACTOR ('FRANK') IS ('HAPPINESS" VAL (5))))~ Subgoal((< = > ($GRAD &ACTOR (~'FRANK') &SCHOOL('CMU')))) ~ Subgoal((< = > ($PROJECT &STUDENT ('FRANK') &AREA('HARDWARE') &RESULT ('COMPLETED')))MOP ('HYPO') TIME (> "NOW'))Figure 3-5: Frank's new goal treeAt this point, six utterances are generated by Rule4.
They aregiven in Figure 3-6.
Three are generated from the first topic,one iS generated from each of the next three topics, and noneis generated from the last topic.
The interest rating routinenow compares these utterances to Frank's goals, and picksthe most interesting one.
Because of the new goal tree, thelast three utterances match none of Frank's goals, andreceive zero ratings.
The first one matches his third goal in aneutral way, and receives a rating of 56 (an utterancereceives 64 points for the top goal, minus 4 for each levelbelow top, plus or minus one for positive/negativeimplications.
These numbers are, of course, arbitrary, as longas ratings from different goals do not overlap).
The secondone matches his top goal in a neutral way, and receives 64.Finally, the third one matches his top goal in a negative way,and receives 63.
Therefore, the second cluestion getsuttered, and ends uP with the links shown in Figure 3-7.
Theother generated utterances are discarded, possibly to beregenerated later, if their topics are still open.
((< = > ($PROJECT &STUDENT ('FRANK ?)
&AREA('HARDWARE') &BODY ('??
))))What project does he have to do?
((ACTOR ('FRANK') IS ('HAPPINESS" VAL (0)))MOO ('?
'))Is he content?.
((ACTOR ('FRANK') IS ('HAPPINESS ?
VAL (-3)))MOD ('?
'))IS he unhappy?.
((< = > ($QUAL &TAKER ('FRANK') &AREA('HARDWARE'))) MOD ('?"
"NEG'))Hadn't he taken it before?
((< = > ($QUAL &TAKER ('FRANK') &AREA(" HARDWARE ") &RESULT ( ?
CANCELLED')))MOO ('?
'))Had it been cancelled on him before?
((< = > ($QUAL &TAKER ('FRANK') &AREA('HARDWARE') &RESULT ('FAILED'))) MOD('??
))Had he failed it before?Figu re 3.6: The six possible utterances generated4.
Other  work ,  fu ture  workTwo other approaches used in modelling conversation aretask-oriented and speech acts based systems.
Both of thesemethodologies have their merits, but neither attacks all thesame aspects of the problem that this system does.
Task-86U0013((ACTOR ('FRANK') IS (*HAPPINESS* VAL (0)))MOP (*??
))(PRED UO002 ISA (*UTTERANCE*) PERSON"ME*INTEREST.REASON (GO006) INTEREST 64INITIATEDBY (RULE4 TO005))Figu re 3-7: System's response to first utteranceoriented systems \[5\] operate in the context of some fixed taskwhich both speakers are trying to accomplish.
Because ofthis, they can infer the topics that are likely to be discussedfrom the semantic structure of the task.
For example, a task.oriented system talking about qualifiers would use theknowledge of how to be a student in order tO talk about thosethings relevant to passing qualifiers (simulating a verystudious student).
It would not usually ask a question like "IsFrank content?.
", because that does not matter from apractical point of view.Speech acts based systems (such as \[1\]) try to reason aboutthe plans that the actors in the conversation are trying toexecute, viewing each utterance as an operator on theenvironment.
Consequently, they are concerned mostlyabout what people mean when they use indirect speech acts(such as using "It's cold in here" to say "Close the window")and are not as concerned about trying to say interestingthings as this system is.
Another way to took at the two kindsof systems is that speech acts systems reason about theactors' plans and assume fixed goals, whereas this systemreasons primarily about their goals.As for related work, ELI (the language analyzer mentioned insection 1) and this system (when fully developed) couldtheoretically be merged into a single conversation system,with some rules working on mapping English into CD, andothers using the CD to decide what responses to generate.
Infact, there are situations in which one needs to make use ofboth kinds of information (such as when a phrase signals atopic shift: "On the other hand...").
One of the possibledirections for future work is the incorporation and integrationof a rule-based parser into the system, along with some formof rule-based English generation.
Another related system,MICS \[3\], had research goals and a set of knowledge sourcessomewhat .similar to this system's, but it differed primarily inthat it could not alter its goal trees during a conversation, nordid it have explicit data structures for representing topics (theselection of topics was built into the interpreter).The main results of this research so far have been the topic-utterance graph and dynamic goal trees.
Although some wayof holding the intersentential information was obviouslyneeded, no precise form was postulated initially.
The currentstructure was invented after working with an earlier set ofrules to discover the most useful form the topics could take.Similarly, the idea that a changing view of someone else'sgoals should be used to control the course of theconversation arose during work on producing the interest-rating routine.
The current system is, of course, by no meansa complete model of human discourse.
More rules need to bedeveloped, and the current ones need to be refined.In addition to implementing more rules and incorporating aparser, possible areas for future work include replacing theinterest-rater with a second agenda (containing interest-determining rules), changing scripts and testing whether the8"7rules are truly independent of the subject matter, trying tomake the system work with several scripts at once (asSAM \[4\] does), and improving the semantic network to handlethe well-known problems which may arise.\[1\]\[2\]\[3\]\[4\]\[5\]\[6\]\[7\]\[8\]\[9\]ReferencesAllen, J. F. and Perrault, C. R.Analyzing Intention in Utterances.Artificial/nteJ/igence 15(3\]:143-178, December, 1980.Brachman, R. J.On the Epistemological Status of Semantic Networks.In Findler, N. V. (editor), Associative Networks:Representation and Use of Knowledge byComputers, chapter I in particular.
AcademicPress, New York, 1979.Carbonell, J. G.Subjective Understanding: Computer Mode/a of BeliefSystems.PhD thesis, Yale University, January, 1979.Computer Science Research Report # 150.Cullingford, R. E.Script Application: Computer Understanding ofNewspaper Stories.PhD thesis, Yale University, January, 1978.Computer Science Research Report # 116.Grosz, B.J.The Representation and use of Focus in DialogueUnderstanding.Technical Report 151, Stanford Research Institute,July, 1977.Newell, A. and Simon, H. A.Human Problem Solving.Prentice Hall, Englewood Cliffs, N. J., 1972, chapter 8.Riesbeck, C. and Schank, R. C.Comprehension by Computer: Expectation BasedAnalysis of Sentences in Context.Technical Report 78, Department of ComputerScience, Yale University, 1976.Schank, R. C.Conceptual Information Processing.North-Holland, 1975, chapter 3.Schank, R. C. and Abelson, R.Scripts.
Plans, Goals and Understanding.Erlbaum, 1977, chapter 3.
