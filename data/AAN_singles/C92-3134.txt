PLANNING TO FAIL, NOT FAIL ING TO PLAN:R ISK-TAK ING AND RECOVERY IN TASK-ORIENTED DIALOGUEJ EAN CARLETTA*Univers i ty of Ed inburghDepar tment  of Artif icial Intel l igencejcc@aisb.ed.ac.ukAbst ract  duplicate the route.
The HCRC Dialogue Database \[3\]We hypothesise that agents who engage in task- contains 128 such dialogues; in this work we examinedoriented ialogue usually try to complete the task with eight plus a set of dialogues from the pilot study usedthe least effort which will produce a satisfactory so- in Shadbolt 's work \[17\].
Agents who wish to avoid planlution.
Our analysis of a corpus of map navigationtask dialogues hows that there are a number of dif-ferent aspects of dialogue for which agents can chooseeither to expend extra effort when they produce theirinitial utterances, or to take the risk that they willhave to recover from a failure in the dialogue.
Someof these decisions and the strategies which agents useto recover from failures due to high risk choices aresimulated in the JAM system.
The human agents ofthe corpus purposely risk failure because this is gen-erally the most efficient behaviour.
Incorporating thesame behaviour in the JAM system produces dialoguewith more "natural" structure than that of traditionaldialogue systems.In t roduct ionThere are a great number of different dialogue styleswhich people use even in very restricted task.orienteddomains.
Agents can choose different levels of speci-ficity for referring expressions, ways of organising de-scriptions, amounts of feedback, complexities of expla-nation, and so on.
This work first identifies a numberof aspects of task-oriented dialogue along which agentscan make choices and identifies these choices in termsof how much effort the agent must expend in order togenerate utterances in line with them.
In general, ex-pending more effort in building an explanation meansthat the explalnee is more likely to understand it asis; thus we can classify some choices as being "higherrisk" than those which take more effort to generate butwhich are more likely to succeed on the first attempt.Then it identifies a number of recovery strategies whichagents use when risky hehaviour has led to a failure inthe dialogue.
The choices which agents make show ntrade-off of when effort is expended in the dialogue;agents can either expend effort early in order to headoff later difficulty, or take the risk of having to expendmore effort in an attempt at recovery.
For instance,consider the domain, first described in \[5\], in whichtwo participants who are separated by a partition haveslightly different versions of a simple map with approx-imately fifteen gross features on it.
The maps may havedifferent features or have some of the features in dif-ferent locations.
In addition, one agent has a routedrawn on the map.
The task is for the second agent o*This research was supported by a postgraduate stu-dentship from the M~trshMl Aid Commemoration Commis-sion and supervised by Chris Mellish.
The author's currentaddress is HCRC, 2 Buccleuch Place, University of Edin-burgh, Edinburgh Ett8 9LW, Scotland.failure may structure their explanations carefully andelicit feedback often, hehaving similarly to agent A inShadbolt's example 6.16:A: have you got wee palm trees aye?B: uhuA: right go just + a wee bit along to them haveyou got a swamp?B: erA: right well just go + have yon got a waterfall?On the other hand, agents who are willing to relyon interruptions from the partner and recovery fromfailure might behave more like agent A in Shadbolt'sexample 6.11:A: and then + go up about and over the bridgeB: I've not got a bridge I've got a lion's den and awoodA: have you got a river?Either of these approaches is likely to bring theagents to successful completion of the task.
However,it is also possible to include too little information in thedialogue, as in the following case, Shadholt's example6.21:A: right + you're going to have to cro~ the riverB: how?A: dinnae ken + any way you want...It is equally possible to give too much information,as in Shadbolt's example 6.27:B: ah right + erm + oh yes + er + I have a crashedplane marked here + can I + check this + mycrashed plane is ABOVE + it's in the BASEof the quadrant + top right hand imaginaryquadrant of the + erm + picture + yes er +that SOUNDS too high for me +A: erIn this case, B provides o much information that Ais unable to process it, and they eventually abandonthis section of the dialogue.
This work looks at the dif-ferences between the approaches which human agentsuse to complete the map task and simulates them usingthe JAM system.
Understanding and comparing thedifferent human approaches to task-oriented ialoguecan help us to create more robust computer dialogueagents.Communicat ive  PostureOur work extends Shadbolt's analysis of the map taskdata \[17\].
He identifies a number of "communica-tive posture parameters" or aspects of the dialogue forAcrEs DE COLING-92, NANTES, 23-28 AO~ 1992 8 9 6 PROC.
OP COLING-92, NANTES, AUG. 23-28.
1992which an agent may make tit(-" choice of how to proceed,and classifies the possible settings in ternm of risk: forthe most part, high risk settings leave the partner toinfer information aml risk the possibility of plan fail-ure, while low risk settings are more likely to work asplanned, lie then argues that hmnan agents decideupon their communicative postures according to thePrinciple of Parsimony, which is "a behavioural princiopie which instructs processors to do no more processingthan is necessary to achieve a goal."
(pg.
342) Agentschoose the settings for each individual parameter whichthey believe will prove most etlicient.
Shadbolt identi-fies seven different communicative posture parameters.Our own analysis extends tds byclearly separating out aspects of being a hearer fromthose of being a listener mt~lhy making the behaviour ofthe parameters more in~dcl)endcnt of each other mid subsequently dividingthem into sets dcpcnding on which part of an agent'splanning they affect.
Wc divide utterance planninginto diffcrcnt stages similar to Appelt's \[4\] for thispart of the analysis.The following revised set i)rovides a more solid foun..dation on which to build the implementation found inthe JAM system:Task P lann ing  Parameters'.l'hese parameters affect which task plan an agentchooses.
In the map domain, task plans determine thechoice of descriptions for sections of the route and forthe location of objects.Onto logy:  Thc choice of concepts to use when build-ins an explanation, lligh risk agents construct sim-ple and short descriptions, providing ms littlc infoflnation as they think the partner will allow, willielow risk agents provide precise, detailed explauatioaseven if that involves using fairly COml)lex backgroundconcepts and introducing new concepts into the dia-logue.Onto log ica l  l~esolution: The choice of concepts toask about when hearing an explanation, l\[igh riskagents asccpt the level of detail which is off~red tothem, wbile low risk ones ask how concepts are re-lated if they think that the relationship may be animportant piece of background for tim explanation.Par tner  Mode l l ing :  Wltether or not to heed a modelof the partner while building an explanation.
Highrisk agents do not, while low risk agents do, tailoringtile explanation f(u" the partner.
It takes more effortin the first instance to buihl an explanation which istailored to the partner, but the explanation is morelikely to succeed without revisions.Ontology and partner modelling are implemented intile J AM system by means of an evaluation selmmefor possible task plans which rates descriptions dif-ferently (let)ca(ling on whether these parmneters areset to low or high risk.
Low risk ontology prefers de-scriptions which rcfer to many map objects over sim-pler ones; if there arc sevcral descriptions of equalcomplexity, low risk l)artocr modelling prefers de-scril)tions which (Io not refer to map objects thatmay I)c unknown to the partner.
Ontological r(~)lu-Lion is not irnp\]enlente(l in tile JAM system becauseJAM agents are not capable of the spatial reasoningrequired to determine what other map objects arerelevaat o a given description.D iscourse  P lann ing  ParametersThese parameters affect the structure of the dimcourse, given the information from the task plan whichnumt be conveyed.Di f ference:  Whether or not agents assume that theirmodeln of the domain are the stone unless proven oth-erwise.
High risk agents make this assumption, whilelow risk agents do not, making them precede newconcepts in the dialogam with subdialogues which es-tablish certain knowledge of the partner's knowledgesuch as direct questions about the status of the con-cepts.
A low risk difference setting makes the (lia~logue longer and hence requires more effort, but alesprovides a greater strength of evidence about thepartner's beliefs \[7\] than does relying on the part-her's feedback to the explanation itself.
This param-eter is implemented in the JAM system by means ofoptional prerequisites on discourse plans which in-troduce new concepts; low risk agents expand theprerequisites, while high risk agents do not.Coherence:  Whether or not the agents orgauisc theirdiseourse coherently, lligh risk agents produce utter-ances in whatever ordcr they think of thcm, whereaslow risk agents try to order them in some way whichwill make the discourse ~icr  for the partner.
Thisparameter is not implenmnted in the JAM systembecause, map task participants do not often organisethe discourse xcept as if they were physically fol-lowing the route.
In less well structured omains, itcould be implemented using, for instance, RST \[11\]or focus trees \[12\].Ut terance  Rea l l sa t ion  ParametersThrum parameters affect the way in which each utterance in the given discourse structure is realised orunderstood.Context  Ar t ieu la t lon :  Whether or not the agentssignal awkward context shifts, llere context isloosely dcfincd as tile goM which is supported bythe current part of the dialogue; in the map task,contexts carl either be goals of sharing knowledgeabout a section of tile route or tim location of anobject, tligh risk agents do not signal awkward con-text shifts, while low risk agents use mcta~comments,changes in diction, or sot\[m other means to marktile new context.
A limited version of the low risksetting is imphmmnted in JAM which introduces areels-comment into the dialogue whenevcr a contextshift occurs.Context  ILesolufion: Whether or not agents ask forclarification of awkward context shifts.
Low riskagents ask tile partner what the current context isor make their assumptions clear when they are un-sure, whereas high risk agents simply choose themost likely context.
This parameter is not imple-mented in the JAM system because JAM agents usea language which (tt~s not allow for ambiguity ofcontext.~'ocus Ar t i cu la t ion :  Wllether or not agents signalawkward focus shifts, liere, focus is defined specifi-cally for tile map task in terms of distance on theACYES DE COl,INGo92, NANrEs.
23 28 AO~"H 1992 8 9 7 Ptto(:, OF COI,IN(I-92, NANrES.
AUU.
23-28, 1992map and semantic relationships among map iea~tures.
Low risk agents use meta-comments or modi-tiers on referring expressions to signal awkward focuschanges, and high risk agents do not.
Focus articula~tion is not implemented in the \]\[AM system becauseJAM agents are not capable of the spatial or seman-tic reasoning required to calculate focus; given theseabilities, low risk agents could use some theory ofhow focus usually moves (such as that of Grosz andSidner \[9\]) to determine whether or not signaling aparticular shift is necessary.Focus Reso lut ion :  Whether or not agents ask forclarification of awkward focus shifts.
Low risk agentsask the partner what the current focus is or marktheir assumptions in some other way, whereas highrisk agents imply choose the most likely focus.
Lowrisk focus resolution could be implemented by hay-ing low risk agents ask for clarification whenever afocus shift does not conform to sonm theory of focus,with high risk agents "guessing" the current focus.Speci f icat ion:  Whether or not agents construct refer-ring expressions carcfidty.
Low risk agents generatereferring expressions which are roughly minimallyunique, whereas ifigh risk agents generate whateverexpression comes to mind, even if that expressionis under- or over-specific.
This parameter could beimplemented in the JAM system nsing, for instance,work by Dale \[8\] and Reiter \[16\].Descr ip t ion  Reso lut ion :  Whether or not agents de-code referring expressions carefully.
Low risk agentsask for clarification of ambiguous referring expres-sions, while high risk agents simply choose themostly likely referent.
This parameter could havean implementation similar to that of the specifica-tion parameter, but from the point of view of theaddressee.Meta -P lann ing  ParameterThis parameter affects an agent's choice of how to con-tinue from the current situation in a dialogue.P lan  Commitment :  Whether or not agents decideto replan easily.
Low risk agents tend to stick tothe current plan unless there is sufficient proof thattile new plan is better, whereas high risk agents of_ten replan when they encounter failures even with-out carefully checking the viability of the new plan.Frequent changes in plans are likely to confuse thepartner and lead to difficulty in the dialogue, espe-cially if the agent's context ~ticulation setting isalso high risk.
This parameter is implemented in theJAM system by means of a "replanning threshold"which is added to tile estimated cost of a replan andwhich makes replanning seem less efficient o low riskagents that to high risk ones.Of course, the choice is not between extremes, butamong points on a spectrum which generally reflectsthe amount of effort to be expended.
Shadbolt adaptsthe Principle of Parsimony to state that agents makethe choices which they believe will lead to the lowest ef-fort solution for the entire task.
In each case, high riskagents may lose the efficiency advantage which theygained by using less effort initially, if their plans failand they have to expend more effort to recover fromthe failures.
Recovery strategies are more often neededby high risk agents than by low risk ones.Recovery  S t ra teg iesOur analysis has uncovered the following recoverystrategies.
Some strategies are only first steps towardsfinding a solution for the failure, mid one, goal adop-tion, is also useful in other circumstances.
We use thesame basic definitions for repair and replanuing as inMoore's work \[13\].Goa l  Adopt ion :  The agent may infer the partner'sgoals from sonm part of the dialogue he or she hasinitiated and adopt them as his or her own.Ced ing  the  Turn :  The agent may simply not takeany action and hope that his or her inaction willforce the partner into initiating the recovery.E laborat ion :  If an explanation has not been given inenough detail, the explainer may fill in the gaps.Omiss ion:  If an explanation has been given in toonmch detail, the agents may agree to discard someof the information.
This is especially useful in themap task if some description of the route or of thelocation of an object OIL the map turns out to holdfor one version of the map but not the other.Repet i t ion :  Under any circumstances, an agent maysimply repeat whatever action has already failed illthe hopes that it will work the subsequent tinve.Ignor ing  the  Problean:  An agent may ignore aproblem and hope that it will disappear.Repa i r :  If a plan has failed, then checking each of theprerequisites of the plan ill turn to see if they aresatisfied may lead to a diagnosis.
In the map task,plan prerequisites have to do with knowledge aboutobjects on the map.
A plan will fail if an agent pre-supposes that the partner has knowledge which he orstle does not have.
Since the knowledge transferredin the map domain is so simple, it is sufficient in arepair to re-execute any failed prerequisites, even ifthe plan has already been completely executed.Rep lannh ig :  If a plan has failed, then an agent mayattempt an entirely different plan with the same ef-fect.
In the map task, this involves using a differentdescription for the information under considerationor trying a different approach altogether.There are many past systems which have incorpo-rated some form of recovery from plan failure (e.g., \[2\],\[19\], \[14\]).
However, very little work has been done onincorporating more than one recovery strategy into tilesame system.
Moore's \[13\] work allows the use of re-pair, reinstantiation, aud replanning, but uses a strictordering on these strategies to determine which one totry next.
Moore's system first attempts any poesiblerepairs, then any reinstantiations, and then, only as alast resort, replanning.
Neither Moore's ordering norany other can account for the variety of behaviourswhich is present in the human map task corpus.
In ad-dition, Moore's system only considers replanning whenthere has been a plan failure, whereas human agentssometimes switch plans when they flesh out enough ofthe details and discover that the plans which they haveadopted are leas efficient han they had expected.
Thesolution to these shortcomings i  to invoke the Princi-ple of Parsimony and to allow agents at every choicepoint to decide what to do next based on an estimatesACIT~ DE COLING-92, NANaa~s, 23-28 hOt'It 1992 8 9 8 PRoc.
OV COLING-92, NANTES.
AUG. 23-28, 1992Figure 1: The Structure of a JAM Agent's PlannerInterpreterModesDomain  Operatorsof how much effort the remainder of the task will re-quire given each of the possible next actions.
This ap-proach is adopted in the JAM system.The  JAM SystemThe JAM system allows agents to converse about themap task by alternating control between them, follow.~ing Power \[15\].
Agents converse in an artificial an-guagc which is based on Houghton's \[10\] interactionframes; these frames pecify the forms of two and threemove dialogue games for informing, asking wh- and yes-no- questions, opening a subdialogue with a particulartopic, and closing a recovery, and also gives plausiblebelief updates associated with each.
An English-likegloss is provided for the benefit of the human observeronly by means of very simple template.
Unlike the hu-mans, JAM agents have their conununicative posturesset before the beginning of a dialogue and can not varythem during its course.
Each agent uses five commu-nicative posture parameters (ontology, partner mod-elling, context articulation, difference, and plan com-mitment) and three recovery strategies (goal adoption,repair, and replanning).
Space will not permit a de-scription of how the parameters arc implemented; formore details ee \[6\].
The recovery strategies are imple-mented within a layered message-passing architectureshown in figure I and adapted from MOLGEN \[18\].At each layer of the system, operators use the nextlayer down in order to decide whether or not they arcapplicable and how to apply themselves.
The bottomlayer of the system contains plan operators which ax-iomatise the domain knowledge and the meanings ofthe dialogue gaines.
The strategy layer contains oper-ators expressing all of the different actions which anagent cart take next in the current situation: agentscan decide to communicate, infer and adopt one ofthe partner's goals, plan, replan, or repair.
The modelayer contains operators which control which strategyis chosen.
The mode operator of most concern to us isthe comprehensive mode, which always communicateswhenever it has something to say, goal adopts wheneverit can recognise one of the partner's intentions (usinga siraplified version of Allen's plan recogniser \[1\]), andchooses whether to plan, replan, or repair based onan estimate of the effort needed to complete the entiretask if each of the options is taken ncz~ (for details,see \[6\]).
There are also other modes which eliminatesome of the recovery strategies in order to make exper-imentation with the strategies easier, and one modewhich reconstructs as far as possible the choices whichMoore's system \[13\] makes.
Finally, the interpreterchooses one of the mode level operators for the dura-tion of the dialogue.
If a theory of how agents includeand exclude consideration of different recovery strate-gies were available, it could be implemented in the in-terpreter and more layers could be added to the systemas needed.ExamplesThe JAM system generates the following dialogue x-tract between two agents who both have totally lowrisk communicative posture settings:mary: i want to talk about the first section.john: ok.mary: do you have the palm beach?john: yes.mary: do you have the swamp?john: no.mary: i want to talk about the swamp.john: ok.mary: do you have the waterfall?john: yes.mary: the swamp is between the waterfall and thepalm beach.john: ok.mary: the first section of the route goes betweenthe palm beach and the swamp.john: ok.In this extract, a low risk context articulation settingleads mary to open the dialogue about each distincttopic (the first section of the route and the swamp).Low risk ontology causes her to chooee her to choo~fairly complex descriptions of the first section of theroute and the location of the swamp (at least for JAMagents), and low risk difference makes her ask johnwhether or not he has all of the prerequisite knowledgebefore she gives any descriptions.
Finally, low risk plancommitment causes her not to to abandon her descrip-tion of the first section of the route when she discoversthat john's map does not have the swamp.
If we rnnthe same dialogue, but replace mary with an agent whohas high risk plan commitment and context articula-tion settings, then the structure of the dialogue changescompletely:janet: do you have the palm beach?john: yes.janet: do you have the swamp?john: no.janet: the first section of the route goes to the leftof the palm beach.john: ok.In this case, although janet begins using the sameplan as mary had, as she explores this plan she dis-covers that it will not work and decides that it will bele~ costly simply to abandon it for a description whichdoes not mention the swamp.
In another example, twoACRES DE COLING-92, NAhT~S.
23-28 Ao~rr 1992 8 9 9 Paoc.
ol: COLING-92, NArCr~s.
Auo.
23-28, 1992agents with high risk settings for context articulation,difference, and plan commitment also successfully cormplete this section of the route, but only by having a planfailure occur and invoking the repair recovery strategy:carol: the first section of the route goes betweenthe palm beach and the swamp.tom: i do not understand.tom: where is the swamp?carol: the swamp is between the waterfall and thepalm beach.tom: ok.tom: i am done talking about the first section.carol: ok.In this dialogue, tom infers that carol has the goal fortom to know how to get around the first section of theroute from her initial statement, adopts it, and thendiagnoses the failure of prerequisite knowledge aboutthe swamp in order to repair her plan.
He chooses thiscourse of action because he believes it will take lesseffort than the other possibilities (in this case, sim-ply telling carol he does not understand her statementwithout diagnosing the problem and allowing her toprovide a different description of the first section).
Thisfailure came about as a result of carol's high risk differ-ence setting, since a low risk difference setting wouldhave made carol ask tom ahead about his knowledge,a.s mary did for john.
Using agents with different com-municative postures and interpreters allows the JAMsystem to simulate many different behaviours whichcan be recognised in the human corpus.Conc lus ionsWe demonstrate a number of aspects of dialogue forwhich agents must choose between expending effortwhen they create their initial utterances and takingthe risk of plan failure, and go on to describe a num-ber of strategies which high risk agents use to recoverfrom failure.
A surprising outcome of the human ex-amples is that it is often most parsimonious to risk fail-ure.
Agents quickly reach the limits of their resourcebounds when they try to avoid possible confusions inthe dialogue, and dialogue is such a flexible mediumthat recovery is relatively inexpensive.
In other words,although their behaviour may make them seem to failto plan, human agents really plan to fail because itis more eMcient to do so in the long run.
Computeragents who are to interact with human ones shouldtake this into account when they react to their part-ners' contributions, and it might even be desirable forthem to adopt this approach themselves.
In additionto the analysis, we simulate some of the choices whichhuman agents make using conversations between twocomputer agents in the JAM system.
These agents,given particular communicative posture choices, try tominimise the total effort that will be expended in thedialogue by always considering all possible actions andtaking whichever one they believe will lead to tim leastcost completion of the dialogue.
We leave to furtherwork extensions which would allow the agents to de-cide not to deliberate about what to do completely,just taking the first action which they "think" of, andwhich would allow the agents to vary their communica-tive postures during the course of a dialogue.References\[1\] J. Allen.
Recognising intentions from natural lan-guage utterances.
In M. Brady and R. C. Berwiek,editors, Computational Models of Discourse, pages107-166.
MIT Press, 1983.\[2\] J.
A. Ambros-Ingerson.
Relationships betweenplanning and execution.
AISB Quarterly, (57),1980.\[3\] A. H. Anderson, M. Bader, E. G. Bard, E. Boyle,G.
Doherty, S. Garrod, S. Isard, J. Kowtko,J.
McAllister, J. Miller, C. Sotillo, H. Thompson,and R. Weinert.
The here map task corpus.
Lan-guage and Speech, 1992 (forthcoming).\[4\] D. Appelt.
Planning English Sentences.
Cain-bridge U.
Press, 1985.\[5\] G. Brown, A. Anderson, R. C. Shillcock, andG.
Yule.
Teaching Talk.
Cambridge UniversityPress, 1984.\[6\] J. C. Carletta.
Risk-taking and Recovery in Task-Oriented Dialogue.
PhD thesis, Edinburgh Uni-versity Department of Artificial Intelligence, 1992(forthcoming).\[7\] H. Clark and E. Schaefer.
Contributing to dis-course.
Cognitive Science, 13, 1989.\[8\] R. Dale.
Generating referring expressions in a do-main of objects and processes.
PhD thesis, Edin-burgh University, 1988.\[9\] B. Grosz and C. Sidner.
The structures of dis-course structure.
Technical Report 6097, BBN,1985.\[10\] G. Houghton.
The Production of Language in Di-alogue: A Computational Model.
PhD thesis, Uni-versity of Sussex, April 1986.\[11\] W. C. Mann and S. A. Thompson.
Rhetoricalstructure theory: A theory of text organization.Reprint Series 190, ISI, 1987.\[12\] K. McCoy and J. Cheng.
Focus of attention: con-straining what can be said next.
In Proceedings ofthe 4th International Workshop on Natural Lan-guage Generation, 1988.\[13\] J. D. Moore.
A reactive approach to explana-tion in expert and advice-giving systems.
Spe-cial Report 251, University of Southern Califor-nia/Information Sciences Institute, 1990.\[14\] D. Peachey and G. McCalla.
Using planning tech-niques in intelligent tutoring systenm.
Interna-tional Journal of Man-Machine Studies, 24, 1986.\[15\] R. J. D. Power.
A Computer Model o.f Conversa-tion.
PhD thesis, University of Edinburgh, 1974.\[16\] E. Reiter.
Generating descriptions that exploit auser's domain knowledge.
In K. Dale, C. Mellish,and M. Zock, editors, Current Research in NaturalLanguage Generation.
Academic Press, 1990.\[17\] N. R. Shadbolt.
Constituting reference in naturallanguage: the problem of referential opacity.
PhDthesis, Edinburgh University, 1984.\[18\] M. J. Stefik.
Planning and meta.planning.
Artifi-cial Intelligence, 16, 1981.\[19\] D. E. Wilkins.
Practical Planning.
Morgan Kanf-mann, 1988.ACRES DE COLING-92, NAN'rES, 23-28 ^ ofrr 1992 9 0 0 PROC.
OF COLING-92, NANTES, AUG. 23-28, 1992
