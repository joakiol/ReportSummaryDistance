Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 273?276,Suntec, Singapore, 4 August 2009.c?2009 ACL and AFNLPA Stochastic Finite-State Morphological Parser for TurkishHas?im Sak & Tunga G?ung?orDept.
of Computer EngineeringBo?gazic?i UniversityTR-34342, Bebek,?Istanbul, Turkeyhasim.sak@boun.edu.trgungort@boun.edu.trMurat Sarac?larDept.
of Electrical & Electronics EngineeringBo?gazic?i UniversityTR-34342, Bebek,?Istanbul, Turkeymurat.saraclar@boun.edu.trAbstractThis paper presents the first stochasticfinite-state morphological parser for Turk-ish.
The non-probabilistic parser is astandard finite-state transducer implemen-tation of two-level morphology formal-ism.
A disambiguated text corpus of200 million words is used to stochas-tize the morphotactics transducer, then itis composed with the morphophonemicstransducer to get a stochastic morpho-logical parser.
We present two applica-tions to evaluate the effectiveness of thestochastic parser; spelling correction andmorphology-based language modeling forspeech recognition.1 IntroductionTurkish is an agglutinative language with a highlyproductive inflectional and derivational morphol-ogy.
The computational aspects of Turkish mor-phology have been well studied and several mor-phological parsers have been built (Oflazer, 1994),(G?ung?or, 1995).In language processing applications, we mayneed to estimate a probability distribution over allword forms.
For example, we need probability es-timates for unigrams to rank misspelling sugges-tions for spelling correction.
None of the previ-ous studies for Turkish have addressed this prob-lem.
For morphologically complex languages, es-timating a probability distribution over a static vo-cabulary is not very desirable due to high out-of-vocabulary rates.
It would be very convenient for amorphological parser as a word generator/analyzerto also output a probability estimate for a wordgenerated/analyzed.
In this work, we build such astochastic morphological parser for Turkish1andgive two example applications for evaluation.1The stochastic morphological parser is available for re-search purposes at http://www.cmpe.boun.edu.tr/?hasim2 Language ResourcesWe built a morphological parser using the two-level morphology formalism of Koskenniemi(1984).
The two-level phonological rules and themorphotactics were adapted from the PC-KIMMOimplementation of Oflazer (1994).
The rules werecompiled using the twolc rule compiler (Karttunenand Beesley, 1992).
A new root lexicon of 55,278words based on the Turkish Language Institutiondictionary2was compiled.
For finite-state opera-tions and for running the parser, we used the Open-FST weighted finite-state transducer library (Al-lauzen et al, 2007).
The parser can analyze about8700 words per second on a 2.33 GHz Intel Xeonprocessor.We need a text corpus for estimating the param-eters of a statistical model of morphology.
For thispurpose, we compiled a text corpus of 200 million-words by collecting texts from online newspa-pers.
The morphological parser can analyze about96.7% of the tokens.The morphological parser may output morethan one possible analysis for a word due to am-biguity.
For example, the parser returns fouranalyses for the word kedileri as shown below.The morphological representation is similar tothe one used by Oflazer and Inkelas (2006).kedi[Noun]+lAr[A3pl]+SH[P3sg]+[Nom] (his/her cats)kedi[Noun]+lAr[A3pl]+[Pnon]+YH[Acc] (the cats)kedi[Noun]+lAr[A3pl]+SH[P3pl]+[Nom] (their cats)kedi[Noun]+[A3sg]+lArH[P3pl]+[Nom] (their cat)We need to resolve this ambiguity to train a prob-abilistic morphology model.
For this purpose, weused our averaged perceptron-based morphologi-cal disambiguator (Sak et al, 2008).
The disam-biguation system achieves about 97.05% disam-biguation accuracy on the test set.2http://www.tdk.gov.tr2730 1k:?/2.34 2e:?/1.76 3d:?/5.68 4i:kedi[Noun] 6l:+lAr[A3pl]/1.19 5?
:+[A3sg]8e:?7l:+lArH[P3pl]/5.73 9e:?10r:?11r:?14i:+SH[P3pl]/2.8913i:+SH[P3sg]/0.6212?
:+[Pnon] 15/3.83i:+[Nom]/1.06?:+[Nom]?
:+[Nom]i:+YH[Acc]/1.66Figure 1: Finite-state transducer for the word kedileri.3 Stochastic Morphological ParserThe finite-state transducer of the morphologicalparser is obtained as the composition of the mor-phophonemics transducer mp and the morphotac-tics transducer mt; mp ?
mt.
The morphotac-tics transducer encodes the morphosyntax of thelanguage.
If we can estimate a statistical mor-phosyntactic model, we can convert the morpho-logical parser to a probabilistic one by composingthe probabilistic morphotactics transducer with themorphophonemics transducer.
Eisner (2002) givesa general EM algorithm for parameter estimationin probabilistic finite-state transducers.
The algo-rithm uses a bookkeeping trick (expectation semir-ing) to compute the expected number of traversalsof each arc in the E step.
The M step reestimatesthe probabilities of the arcs from each state to beproportional to the expected number of traversalsof each arc - the arc probabilities are normalizedat each state to make the finite-state transducerMarkovian.
However, we do not need this generalmethod of training.
Since we can disambiguatethe possible morphosyntactic tag sequences of aword, there is a single path in the morphotacticstransducer that matches the chosen morphosyntac-tic tag sequence.
Then the maximum-likelihoodestimates of the weights of the arcs in the morpho-tactics transducer are found by setting the weightsproportional to the number of traversals of eacharc.
We can use a specialized semiring to cleanlyand efficiently count the number of traversals ofeach arc.Weights in finite-state transducers are elementsof a semiring, which defines two binary operations?
and ?, where ?
is used to combine the weightsof arcs on a path into a path weight and ?
is usedto combine the weights of alternative paths (Bers-tel and Reutenauer, 1988).
We define a countingsemiring to keep track of the number of traver-sals of each arc.
The weights in the mt trans-ducer are converted to the counting semiring.
Inthis semiring, the weigths are vectors of integershaving dimension as the total number of arcs inthe mt transducer.
We number the arcs in the mttransducer and set the weight of the ntharc as thenthbasis vector.
The binary plus ?
and the times?
operations of the counting semiring are definedas the sum of the weight vectors.
Thus, the nthvalue of the vector in the counting semiring justcounts the appearances of the ntharc of mt in apath.To estimate the weights of the stochastic modelof the mt transducer, we use the text corpus col-lected from the web.
First we parse the wordsin the corpus to get al the possible analyses ofthe words.
Then we disambiguate the morpho-logical analyses of the words to select one of themorphosyntactic tag sequences xifor each word.We build a finite-state transducer ?
xithat maps symbol to xiin the counting semiring.
Theweights of this transducer are zero vectors havingthe same dimension as themt transducer.
Then thefinite-state transducer (?xi)?
(mt?) having all :  arcs can be minimized to get a one-state FSTwhich has the weight vector that keeps the numberof traversals of each arc in mt.
The weight vec-tor is accumulated for all the ximorphosyntactictag sequences in the corpus.
The final accumu-lated weight vector is used to assign probabilitiesto each arc in the mt transducer proportional tothe traversal count of the arc, hence resulting inthe stochastic morphotactics transducer?mt.
Weuse add-one smoothing to prevent the arcs havingzero probability.
The?mt transducer is composedwith the morphophonemics transducer mp to get astochastic morphological parser.The stochastic parser now returns probabilitieswith the possible analyses of a word.
Figure 1shows the weighted paths for the four possibleanalyses of the word kedileri as represented in thestochastic parser.
The weights are negative logprobabilities.4 Spelling CorrectionThe productive morphology of Turkish allowsone to generate very long words such as274?ol?ums?uzles?tirdi?gimizden.
Therefore, the detectionand the correction of spelling errors by present-ing the user with a ranked list of spelling sugges-tions are highly desired.
There have been someprevious studies for spelling checking (Solak andOflazer, 1993) and spelling correction (Oflazer,1996).
However there has been no study to ad-dress the problem of ranking spelling suggestions.One can use a stochastic morphological parser todo spelling checking and correction, and presentspelling suggestions ranked with the parser outputprobabilities.
We assume that a word is misspelledif the parser fails to return an analysis of the word.Our method for spelling correction is to enumerateall the valid and invalid candidates that resemblethe incorrect input word and filter the invalid oneswith the morphological parser.To enumerate the alternative spellings for a mis-spelled word, we generate all the words in one-character edit distance with the input word, wherewe consider one symbol insertion, deletion or sub-stitution, or transposition of adjacent symbols.The Turkish alphabet includes six special letters(c?, ?g, ?, ?o, s?, ?u) that do not exist in English.These characters may not be supported in somekeyboards and message transfer protocols; thuspeople frequently use their nearest ASCII equiv-alents (c, g, i, o, s, u, respectively) instead of thecorrect forms, e.g., spelling nas?ls?n as nasilsin.Therefore, in addition to enumerating words inone edit distance, we also enumerate all the wordsfrom which the misspelled word can be obtainedby replacing these special Turkish characters withtheir ASCII counterparts.
For instance, for theword nasilsin, the alternative spellings nas?lsin,nasils?n, and nas?ls?n will also be generated.Note that although the context is important forspelling correction, we use only unigrams.
Onecan build a morpheme based language model toincorporate the context information.
We also lim-ited the edit distance to 1, but it is straightfor-ward to allow longer edit distances.
We can builda finite-state transducer to enumerate and repre-sent efficiently all the valid and invalid word formsthat can be obtained by these edit operations ona word.
For example, the deletion of a charac-ter can be represented by the regular expression??(?
: )?
?which can be compiled as a finite-state transducer, where ?
is the alphabet.
Theunion of the transducers encoding one-edit dis-tance operations and the restoration of the specialTurkish characters is precompiled and optimizedwith determinization and minimization algorithmsfor efficiency.
A misspelled input word transducercan be composed with the resulting transducer andin turn with the morphological parser to filter outthe invalid word forms.
The words with their es-timated probabilities can be read from the outputtransducer and constitute the list of spelling sug-gestions for the word.
The probabilities are usedto rank the list to show to the user.
We also handlethe spelling errors where omission of a space char-acter causes joining of two correct words by split-ting the word into all combinations of two stringsand checking if the string pieces are valid wordforms.
An example list of suggestions with the as-signed negative log probabilities and their Englishglosses for the misspelled word nasilsin is givenbelow.nas?ls?n (14.2) (How are you), nakilsin (15.3) (You area transfer), nesilsin (21.0) (You are a generation), nasipsin(21.2) (You are a share), basilsin (23.9) (You are a bacillus)On a manually chosen test set containing 225 cor-rect words which have relatively more complexmorphology and 43 commonly misspelled words,the Precision and the Recall scores for the detec-tion of spelling errors are 0.81 and 0.93, respec-tively.5 Morphology-based LanguageModelingThe closure of the transducer for the stochasticparser can be considered as a morphology-basedunigram language model.
Different than standardunigram word language models, this morphology-based model can assign probabilities to words notseen in the training corpus.
It can also achievelower out-of-vocabulary (OOV) rates than modelsthat use a static vocabulary by employing a rela-tively smaller number of root words in the lexicon.We compared the performances of themorphology-based unigram language modeland the unigram word language model on a broad-cast news transcription task.
The acoustic modeluses Hidden Markov Models (HMMs) trained on183.8 hours of broadcast news speech data.
Thetest set contains 3.1 hours of speech data (2,410utterances).
A text corpus of 1.2 million wordsfrom the transcriptions of the news recordings wasused to train the stochastic parser as explained inSection 3 and unigram word language models.We experimented with four different language2750.5 1.0 1.5 2.0 2.5434445464748Real?time factor (cpu time/audio time)WER(%)Morphology?basedWord?50KWord+MorphologyWord?100KFigure 2: Word error rate versus real-time factorobtained by changing the pruning beam width.models.
Figure 2 shows the word error rate ver-sus run-time factor for these models.
In this fig-ure the Word-50K and Word-100K are unigramword models with the specified vocabulary sizeand have the OOV rates 7% and 4.7% on the testset, respectively.
The morphology-based model isbased on the stochastic parser and has the OOVrate 2.8% .
The ?word+morphology?
model is theunion of the morphology-based model and the un-igram word model.Even though the morphology-based model hasa better OOV rate than the word models, the worderror rate (WER) is higher.
One of the reasons isthat the transducer for the morphological parser isambiguous and cannot be optimized for recogni-tion in contrast to the word models.
Another rea-son is that the probability estimates of this modelare not as good as the word models since proba-bility mass is distributed among ambiguous parsesof a word and over the paths in the transducer.The ?word+morphology?
model seems to allevi-ate most of the shortcomings of the morphologymodel.
It performs better than 50K word modeland is very close to the 100K word model.
Themain advantage of morphology-based models isthat we have at hand the morphological analysesof the words during recognition.
We plan to traina language model over the morphological featuresand use this model to rescore the hypothesis gener-ated by the morphology-based models on-the-fly.6 ConclusionWe described the first stochastic morphologicalparser for Turkish and gave two applications.
Thefirst application is a very efficient spelling correc-tion system where probability estimates are usedfor ranking misspelling suggestions.
We also gavethe preliminary results for incorporating the mor-phology as a knowledge source in speech recogni-tion and the results look promising.AcknowledgmentsThis work was supported by the Bo?gazic?i Uni-versity Research Fund under the grant numbers06A102 and 08M103, the Scientific and Techno-logical Research Council of Turkey (T?UB?ITAK)under the grant number 107E261, the Turk-ish State Planning Organization (DPT) underthe TAM Project, number 2007K120610 andT?UB?ITAK B?IDEB 2211.ReferencesCyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-jciech Skut, and Mehryar Mohri.
2007.
OpenFst: Ageneral and efficient weighted finite-state transducerlibrary.
In CIAA 2007, volume 4783 of LNCS, pages11?23.
Springer.
http://www.openfst.org.Jean Berstel and Christophe Reutenauer.
1988.
Ratio-nal Series and their Languages.
Springer-Verlag.Jason Eisner.
2002.
Parameter estimation for proba-bilistic finite-state transducers.
In ACL, pages 1?8.Tunga G?ung?or.
1995.
Computer Processing ofTurkish: Morphological and Lexical Investigation.Ph.D.
thesis, Bo?gazic?i University.Lauri Karttunen and Kenneth R. Beesley.
1992.
Two-level rule compiler.
Technical report, Xerox PaloAlto Research Center, Palo Alto, CA.Kimmo Koskenniemi.
1984.
A general computationalmodel for word-form recognition and production.
InACL, pages 178?181.Kemal Oflazer and Sharon Inkelas.
2006.
The archi-tecture and the implementation of a finite state pro-nunciation lexicon for Turkish.
Computer Speechand Language, 20(1):80?106.Kemal Oflazer.
1994.
Two-level description of Turk-ish morphology.
Literary and Linguistic Comput-ing, 9(2):137?148.Kemal Oflazer.
1996.
Error-tolerant finite-state recog-nition with applications to morphological analysisand spelling correction.
Computational Linguistics,22(1):73?89.Has?im Sak, Tunga G?ung?or, and Murat Sarac?lar.
2008.Turkish language resources: Morphological parser,morphological disambiguator and web corpus.
InGoTAL 2008, volume 5221 of LNCS, pages 417?427.
Springer.Ays?in Solak and Kemal Oflazer.
1993.
Design and im-plementation of a spelling checker for turkish.
Lit-erary and Linguistic Computing, 8(3):113?130.276
