Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1294?1303,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsFrom Natural Language Specifications to Program Input ParsersTao Lei, Fan Long, Regina Barzilay, and Martin RinardComputer Science and Artificial Intelligence LaboratoryMassachusetts Institute of Technology{taolei, fanl, regina, rinard}@csail.mit.eduAbstractWe present a method for automaticallygenerating input parsers from Englishspecifications of input file formats.
Weuse a Bayesian generative model to cap-ture relevant natural language phenomenaand translate the English specification intoa specification tree, which is then trans-lated into a C++ input parser.
We modelthe problem as a joint dependency pars-ing and semantic role labeling task.
Ourmethod is based on two sources of infor-mation: (1) the correlation between thetext and the specification tree and (2) noisysupervision as determined by the successof the generated C++ parser in reading in-put examples.
Our results show that ourapproach achieves 80.0% F-Score accu-racy compared to an F-Score of 66.7%produced by a state-of-the-art semanticparser on a dataset of input format speci-fications from the ACM International Col-legiate Programming Contest (which werewritten in English for humans with no in-tention of providing support for automatedprocessing).11 IntroductionThe general problem of translating natural lan-guage specifications into executable code has beenaround since the field of computer science wasfounded.
Early attempts to solve this problemproduced what were essentially verbose, clumsy,and ultimately unsuccessful versions of standardformal programming languages.
In recent years1The code, data, and experimental setup for this researchare available at http://groups.csail.mit.edu/rbg/code/nl2pthe inputa single integer T test casesan integer N the next N linesN charactersThe input contains a single integer T that indicates the number of test cases.
Then follow the T cases.
Each test case begins with a line contains an integer N, representing the size of wall.
The next N lines represent the original wall.
Each line contains N characters.
The j-th character of the i-th line figures out the color ...(a) Text Specification:(b) Specification Tree:(c) Two Program Input Examples:110YYWYYWWWWWYWWWYWWWWWYYWYYWWWWW...WWWWWWWWWW21Y5YWYWW...WWYYYFigure 1: An example of (a) one natural languagespecification describing program input data; (b)the corresponding specification tree representingthe program input structure; and (c) two input ex-ampleshowever, researchers have had success address-ing specific aspects of this problem.
Recent ad-vances in this area include the successful transla-tion of natural language commands into databasequeries (Wong and Mooney, 2007; Zettlemoyerand Collins, 2009; Poon and Domingos, 2009;Liang et al, 2011) and the successful mapping ofnatural language instructions into Windows com-mand sequences (Branavan et al, 2009; Branavanet al, 2010).In this paper we explore a different aspect ofthis general problem: the translation of naturallanguage input specifications into executable codethat correctly parses the input data and generates1294data structures for holding the data.
The needto automate this task arises because input formatspecifications are almost always described in natu-ral languages, with these specifications then man-ually translated by a programmer into the codefor reading the program inputs.
Our methodhighlights potential to automate this translation,thereby eliminating the manual software develop-ment overhead.Consider the text specification in Figure 1a.If the desired parser is implemented in C++, itshould create a C++ class whose instance objectshold the different fields of the input.
For exam-ple, one of the fields of this class is an integer, i.e.,?a single integer T?
identified in the text specifi-cation in Figure 1a.
Instead of directly generatingcode from the text specification, we first translatethe specification into a specification tree (see Fig-ure 1b), then map this tree into parser code (seeFigure 2).
We focus on the translation from thetext specification to the specification tree.2We assume that each text specification is ac-companied by a set of input examples that the de-sired input parser is required to successfully read.In standard software development contexts, suchinput examples are usually available and are usedto test the correctness of the input parser.
Note thatthis source of supervision is noisy ?
the generatedparser may still be incorrect even when it success-fully reads all of the input examples.
Specifically,the parser may interpret the input examples differ-ently from the text specification.
For example, theprogram input in Figure 1c can be interpreted sim-ply as a list of strings.
The parser may also failto parse some correctly formatted input files not inthe set of input examples.
Therefore, our goal is todesign a technique that can effectively learn fromthis weak supervision.We model our problem as a joint depen-dency parsing and role labeling task, assuminga Bayesian generative process.
The distributionover the space of specification trees is informedby two sources of information: (1) the correla-tion between the text and the corresponding spec-ification tree and (2) the success of the generatedparser in reading input examples.
Our method usesa joint probability distribution to take both of thesesources of information into account, and uses asampling framework for the inference of specifi-2During the second step of the process, the specificationtree is deterministically translated into code.1 struct TestCaseType {2 int N;3 vector<NLinesType*> lstLines;4 InputType* pParentLink;5 }67 struct InputType {8 int T;9 vector<TestCaseType*> lstTestCase;10 }1112 TestCaseType* ReadTestCase(FILE * pStream,13 InputType* pParentLink) {14 TestCaseType* pTestCase15 = new TestCaseType;16 pTestCase?pParentLink = pParentLink;1718 ...1920 return pTestCase;21 }2223 InputType* ReadInput(FILE * pStream) {24 InputType* pInput = new InputType;2526 pInput?T = ReadInteger(pStream);27 for (int i = 0; i < pInput?T; ++i) {28 TestCaseType* pTestCase29 = new TestCaseType;30 pTestCase = ReadTestCase (pStream,31 pInput);32 pInput?lstTestCase.push back (pTestCase);33 }3435 return pInput;36 }Figure 2: Input parser code for reading input filesspecified in Figure 1.cation trees given text specifications.
A specifica-tion tree is rejected in the sampling framework ifthe corresponding code fails to successfully readall of the input examples.
The sampling frame-work also rejects the tree if the text/specificationtree pair has low probability.We evaluate our method on a dataset of in-put specifications from ACM International Colle-giate Programming Contests, along with the cor-responding input examples.
These specificationswere written for human programmers with no in-tention of providing support for automated pro-cessing.
However, when trained using the noisysupervision, our method achieves substantiallymore accurate translations than a state-of-the-artsemantic parser (Clarke et al, 2010) (specifically,80.0% in F-Score compared to an F-Score of66.7%).
The strength of our model in the face ofsuch weak supervision is also highlighted by thefact that it retains an F-Score of 77% even whenonly one input example is provided for each input1295Your program is supposed to read the input from the standard input and write its output to the standard output.The first line of the input contains one integer N. N lines follow, the i-th of them contains two real numbers Xi, Yi separated by a single space - the coordinates of the i-th house.
Each of the following lines contains four real numbers separated by a single space.
These numbers are the coordinates of two different points (X1, Y1) and (X2, Y2), lying on the highway.
(a)Text Specification: the inputone integer N N lines the following linesSpecification Tree:(b)two real numbers Xi, Yi four real numbers(c)Input  :=  N           Lines [size = N]           FollowingLines [size = *]N      :=  intLines  :=  Xi YiXi     :=  floatYi     :=  floatFormal Input Grammar Definition:FollowingLines  :=  F1 F2 F3 F4F1     :=  floatFigure 3: An example of generating input parser code from text: (a) a natural language input specifica-tion; (b) a specification tree representing the input format structure (we omit the background phrases inthis tree in order to give a clear view of the input format structure); and (c) formal definition of the inputformat constructed from the specification tree, represented as a context-free grammar in Backus-NaurForm with additional size constraints.specification.2 Related WorkLearning Meaning Representation from TextMapping sentences into structural meaning rep-resentations is an active and extensively studiedtask in NLP.
Examples of meaning representationsconsidered in prior research include logical formsbased on database query (Tang and Mooney, 2000;Zettlemoyer and Collins, 2005; Kate and Mooney,2007; Wong and Mooney, 2007; Poon and Domin-gos, 2009; Liang et al, 2011; Goldwasser et al,2011), semantic frames (Das et al, 2010; Dasand Smith, 2011) and database records (Chen andMooney, 2008; Liang et al, 2009).Learning Semantics from Feedback Our ap-proach is related to recent research on learn-ing from indirect supervision.
Examples includeleveraging feedback available via responses froma virtual world (Branavan et al, 2009) or from ex-ecuting predicted database queries (Chang et al,2010; Clarke et al, 2010).
While Branavan etal.
(2009) formalize the task as a sequence of de-cisions and learns from local rewards in a Rein-forcement Learning framework, our model learnsto predict the whole structure at a time.
Anotherdifference is the way our model incorporates thenoisy feedback.
While previous approaches relyon the feedback to train a discriminative predictionmodel, our approach models a generative processto guide structure predictions when the feedbackis noisy or unavailable.NLP in Software Engineering Researchershave recently developed a number of approachesthat apply natural language processing techniquesto software engineering problems.
Examples in-clude analyzing API documents to infer API li-brary specifications (Zhong et al, 2009; Panditaet al, 2012) and analyzing code comments to de-tect concurrency bugs (Tan et al, 2007; Tan et al,2011).
This research analyzes natural language indocumentation or comments to better understandexisting application programs.
Our mechanism, incontrast, automatically generates parser programsfrom natural language input format descriptions.3 Problem FormulationThe task of translating text specifications to inputparsers consists of two steps, as shown in Figure 3.First, given a text specification describing an inputformat, we wish to infer a parse tree (which wecall a specification tree) implied by the text.
Sec-ond, we convert each specification tree into for-mal grammar of the input format (represented inBackus-Naur Form) and then generate code thatreads the input into data structures.
In this paper,we focus on the NLP techniques used in the firststep, i.e., learning to infer the specification treesfrom text.
The second step is achieved using a de-terministic rule-based tool.
3As input, we are given a set of text specifica-tions w = {w1, ?
?
?
, wN}, where each wi is a textspecification represented as a sequence of nounphrases {wik}.
We use UIUC shallow parser topreprocess each text specificaton into a sequenceof the noun phrases.4 In addition, we are given aset of input examples for each wi.
We use theseexamples to test the generated input parsers to re-3Specifically, the specification tree is first translated intothe grammar using a set of rules and seed words that identi-fies basic data types such as int.
Our implementation thengenerates a top-down parser since the generated grammar issimple.
In general, standard techniques such as Bison andYacc (Johnson, 1979) can generate bottom-up parsers givensuch grammar.4http://cogcomp.cs.illinois.edu/demo/shallowparse/?id=71296ject incorrect predictions made by our probabilis-tic model.We formalize the learning problem as a de-pendency parsing and role labeling problem.Our model predicts specification trees t ={t1, ?
?
?
, tN} for the text specifications, whereeach specification tree ti is a dependency tree overnoun phrases {wik}.
In general many program in-put formats are nested tree structures, in which thetree root denotes the entire chunk of program in-put data and each chunk (tree node) can be furtherdivided into sub-chunks or primitive fields that ap-pear in the program input (see Figure 3).
There-fore our objective is to predict a dependency treethat correctly represents the structure of the pro-gram input.In addition, the role labeling problem is to as-sign a tag zik to each noun phrase wik in a specifi-cation tree, indicating whether the phrase is a keyphrase or a background phrase.
Key phrases arenamed entities that identify input fields or inputchunks appear in the program input data, such as?the input?
or ?the following lines?
in Figure 3b.In contrast, background phrases do not define in-put fields or chunks.
These phrases are used to or-ganize the document (e.g., ?your program?)
or torefer to key phrases described before (e.g., ?eachline?
).4 ModelWe use two kinds of information to bias ourmodel: (1) the quality of the generated code asmeasured by its ability to read the given input ex-amples and (2) the features over the observed textwi and the hidden specification tree ti (this is stan-dard in traditional parsing problems).
We combinethese two kinds of information into a Bayesiangenerative model in which the code quality of thespecification tree is captured by the prior probabil-ity P (t) and the feature observations are encodedin the likelihood probability P (w|t).
The infer-ence jointly optimizes these two factors:P (t|w) ?
P (t) ?
P (w|t).Modeling the Generative Process.
We assumethe generative model operates by first generatingthe model parameters from a set of Dirichlet dis-tributions.
The model then generates text spec-ification trees.
Finally, it generates natural lan-guage feature observations conditioned on the hid-den specification trees.The generative process is described formally asfollows:?
Generating Model Parameters: For everypair of feature type f and phrase tag z, drawa multinomial distribution parameter ?zf froma Dirichlet prior P (?zf ).
The multinomial pa-rameters provide the probabilities of observ-ing different feature values in the text.?
Generating Specification Tree: For eachtext specification, draw a specification tree tfrom all possible trees over the sequence ofnoun phrases in this specification.
We denotethe probability of choosing a particular spec-ification tree t as P (t).Intuitively, this distribution should assignhigh probability to good specification treesthat can produce C++ code that reads all inputexamples without errors, we therefore defineP (t) as follows:5P (t) = 1Z ???????
?1 the input parser of tree treads all input exampleswithout error otherwisewhereZ is a normalization factor and  is em-pirically set to 10?6.
In other words, P (?
)treats all specification trees that pass the inputexample test as equally probable candidatesand inhibits the model from generating treeswhich fail the test.
Note that we do not knowthis distribution a priori until the specificationtrees are evaluated by testing the correspond-ing C++ code.
Because it is intractable to testall possible trees and all possible generatedcode for a text specification, we never explic-itly compute the normalization factor 1/Z ofthis distribution.
We therefore use samplingmethods to tackle this problem during infer-ence.?
Generating Features: The final step gener-ates lexical and contextual features for eachtree.
For each phrase wk associated with tagzk, let wp be its parent phrase in the tree andws be the non-background sibling phrase toits left in the tree.
The model generates thecorresponding set of features ?
(wp, ws, wk)for each text phrase tuple (wp, ws, wk), with5When input examples are not available, P (t) is just uni-form distribution.1297probability P (?
(wp, ws, wk)).
We assumethat each feature fj is generated indepen-dently:P (w|t) = P (?
(wp, ws, wk))=?fj??
(wp,ws,wk)?zkfjwhere ?zkfj is the j-th component in the multi-nomial distribution ?zkf denoting the proba-bility of observing a feature fj associatedwith noun phrase wk labeled with tag zk.
Wedefine a range of features that capture the cor-respondence between the input format and itsdescription in natural language.
For example,at the unigram level we aim to capture thatnoun phrases containing specific words suchas ?cases?
and ?lines?
may be key phrases(correspond to data chunks appear in the in-put), and that verbs such as ?contain?
mayindicate that the next noun phrase is a keyphrase.The full joint probability of a set w of N spec-ifications and hidden text specification trees t isdefined as:P (?, t,w) = P (?
)N?i=1P (ti)P (wi|ti, ?
)= P (?
)N?i=1P (ti)?kP (?
(wip, wis, wik)).Learning the Model During inference, we wantto estimate the hidden specification trees t giventhe observed natural language specifications w, af-ter integrating the model parameters out, i.e.t ?
P (t|w) =?
?P (t, ?|w)d?.We use Gibbs sampling to sample variables t fromthis distribution.
In general, the Gibbs samplingalgorithm randomly initializes the variables andthen iteratively solves one subproblem at a time.The subproblem is to sample only one variableconditioned on the current values of all other vari-ables.
In our case, we sample one hidden spec-ification tree ti while holding all other trees t?ifixed:ti ?
P (ti|w, t?i) (1)where t?i = (t1, ?
?
?
, ti?1, ti+1, ?
?
?
, tN ).However directly solving the subproblem (1)in our case is still hard, we therefore use aMetropolis-Hastings sampler that is similarly ap-plied in traditional sentence parsing problems.Specifically, the Hastings sampler approximates(1) by first drawing a new ti?
from a tractable pro-posal distribution Q instead of P (ti|w, t?i).
Wechoose Q to be:Q(ti?|?
?, wi) ?
P (wi|ti?, ??).
(2)Then the probability of accepting the new sampleis determined using the typical Metropolis Hast-ings process.
Specifically, ti?
will be accepted toreplace the last ti with probability:R(ti, ti?)
= min{1, P (ti?|w, t?i) Q(ti|?
?, wi)P (ti|w, t?i) Q(ti?|?
?, wi)}= min{1, P (ti?, t?i,w)P (wi|ti, ??
)P (ti, t?i,w)P (wi|ti?, ??
)},in which the normalization factors 1/Z are can-celled out.
We choose ??
to be the parameter ex-pectation based on the current observations, i.e.??
= E[?|w, t?i], so that the proposal distribu-tion is close to the true distribution.
This samplingalgorithm with a changing proposal distributionhas been shown to work well in practice (John-son and Griffiths, 2007; Cohn et al, 2010; Naseemand Barzilay, 2011).
The algorithm pseudo code isshown in Algorithm 1.To sample from the proposal distribution (2) ef-ficiently, we implement a dynamic programmingalgorithm which calculates marginal probabilitiesof all subtrees.
The algorithm works similarly tothe inside algorithm (Baker, 1979), except that wedo not assume the tree is binary.
We therefore per-form one additional dynamic programming stepthat sums over all possible segmentations of eachspan.
Once the algorithm obtains the marginalprobabilities of all subtrees, a specification treecan be drawn recursively in a top-down manner.Calculating P (t,w) in R(t, t?)
requires inte-grating the parameters ?
out.
This has a closedform due to the Dirichlet-multinomial conjugacy:P (t,w) = P (t) ??
?P (w|t, ?
)P (?)d??
P (t) ?
?Beta (count(f) + ?)
.Here ?
are the Dirichlet hyper parameters andcount(f) are the feature counts observed in data(t,w).
The closed form is a product of the Betafunctions of each feature type.1298Feature Type Description Feature ValueWord each word in noun phrase wk lines, VARVerb verbs in noun phrase wk and the verb phrase before wk containsDistance sentence distance between wk and its parent phrase wp 1Coreference wk share duplicate nouns or variable names with wp or ws TrueTable 1: Example of feature types and values.
To deal with sparsity, we map variable names such as ?N?and ?X?
into a category word ?VAR?
in word features.Input: Set of text specification documentsw = {w1, ?
?
?
, wN},Number of iterations TRandomly initialize specification trees1t = {t1, ?
?
?
, tN}for iter = 1 ?
?
?T do2Sample tree ti for i-th document:3for i = 1 ?
?
?N do4Estimate model parameters:5??
= E[?
?|w, t?i]6Sample a new specification tree from distribution7Q:t?
?
Q(t?|?
?, wi)8Generate and test code, and return feedback:9f ?
= CodeGenerator(wi, t?
)10Calculate accept probability r:11r = R(ti, t?
)12Accept the new tree with probability r:13With probability r : ti = t?14end15end16Produce final structures:17return { ti if ti gets positive feedback }18Algorithm 1: The sampling framework for learn-ing the model.Model Implementation: We define severaltypes of features to capture the correlation be-tween the hidden structure and its expression innatural language.
For example, verb features areintroduced because certain preceding verbs suchas ?contains?
and ?consists?
are good indicators ofkey phrases.
There are 991 unique features in totalin our experiments.
Examples of features appearin Table 1.We use a small set of 8 seed words to bias thesearch space.
Specifically, we require each leafkey phrase to contain at least one seed word thatidentifies the C++ primitive data type (such as ?in-teger?, ?float?, ?byte?
and ?string?
).We also encourage a phrase containing the word?input?
to be the root of the tree (for example, ?theinput file?)
and each coreference phrase to be aTotal # of words 7330Total # of noun phrases 1829Vocabulary size 781Avg.
# of words per sentence 17.29Avg.
# of noun phrase per document 17.26Avg.
# of possible trees per document 52KMedian # of possible trees per document 79Min # of possible trees per document 1Max # of possible trees per document 2MTable 2: Statistics for 106 ICPC specifications.background phrase (for example, ?each test case?after mentioning ?test cases?
), by initially addingpseudo counts to Dirichlet priors.5 Experimental SetupDatasets: Our dataset consists of problem de-scriptions from ACM International Collegiate Pro-gramming Contests.6 We collected 106 problemsfrom ACM-ICPC training websites.7 From eachproblem description, we extracted the portion thatprovides input specifications.
Because the testinput examples are not publicly available on theACM-ICPC training websites, for each specifica-tion, we wrote simple programs to generate 100random input examples.Table 2 presents statistics for the text specifica-tion set.
The data set consists of 424 sentences,where an average sentence contains 17.3 words.The data set contains 781 unique words.
Thelength of each text specification varies from a sin-gle sentence to eight sentences.
The difference be-tween the average and median number of trees islarge.
This is because half of the specifications arerelatively simple and have a small number of pos-sible trees, while a few difficult specifications haveover thousands of possible trees (as the number oftrees grows exponentially when the text length in-creases).Evaluation Metrics: We evaluate the model6Official Website: http://cm.baylor.edu/welcome.icpc7PKU Online Judge: http://poj.org/; UVA Online Judge:http://uva.onlinejudge.org/1299performance in terms of its success in generating aformal grammar that correctly represents the inputformat (see Figure 3c).
As a gold annotation, weconstruct formal grammars for all text specifica-tions.
Our results are generated by automaticallycomparing the machine-generated grammars withtheir golden counterparts.
If the formal grammaris correct, then the generated C++ parser will cor-rectly read the input file into corresponding C++data structures.We use Recall and Precision as evaluation mea-sures:Recall = # correct structures# text specificationsPrecision = # correct structures# produced structureswhere the produced structures are the positivestructures returned by our framework whose corre-sponding code successfully reads all input exam-ples (see Algorithm 1 line 18).
Note the number ofproduced structures may be less than the numberof text specifications, because structures that failthe input test are not returned.Baselines: To evaluate the performance of ourmodel, we compare against four baselines.The No Learning baseline is a variant of ourmodel that selects a specification tree withoutlearning feature correspondence.
It continuessampling a specification tree for each text speci-fication until it finds one which successfully readsall of the input examples.The second baseline Aggressive is a state-of-the-art semantic parsing framework (Clarke et al,2010).8 The framework repeatedly predicts hiddenstructures (specification trees in our case) using astructure learner, and trains the structure learnerbased on the execution feedback of its predictions.Specifically, at each iteration the structure learnerpredicts the most plausible specification tree foreach text document:ti = argmaxt f(wi, t).Depending on whether the corresponding codereads all input examples successfully or not, the(wi, ti) pairs are added as an positive or negativesample to populate a training set.
After each it-eration the structure learner is re-trained with thetraining samples to improve the prediction accu-racy.
In our experiment, we follow (Clarke et al,8We take the name Aggressive from this paper.Model Recall Precision F-ScoreNo Learning 52.0 57.2 54.5Aggressive 63.2 70.5 66.7Full Model 72.5 89.3 80.0Full Model (Oracle) 72.5 100.0 84.1Aggressive (Oracle) 80.2 100.0 89.0Table 3: Average % Recall and % Precision of ourmodel and all baselines over 20 independent runs.2010) and choose a structural Support Vector Ma-chine SVMstruct 9 as the structure learner.The remaining baselines provide an upperbound on the performance of our model.
The base-line Full Model (Oracle) is the same as our fullmodel except that the feedback comes from an or-acle which tells whether the specification tree iscorrect or not.
We use this oracle information inthe prior P (t) same as we use the noisy feedback.Similarly the baseline Aggressive (Oracle) is theAggressive baseline with access to the oracle.Experimental Details: Because no human an-notation is required for learning, we train ourmodel and all baselines on all 106 ICPC text spec-ifications (similar to unsupervised learning).
Wereport results averaged over 20 independent runs.For each of these runs, the model and all baselinesrun 100 iterations.
For baseline Aggressive, ineach iteration the SVM structure learner predictsone tree with the highest score for each text spec-ification.
If two different specification trees of thesame text specification get positive feedback, wetake the one generated in later iteration for evalu-ation.6 Experimental ResultsComparison with Baselines Table 3 presentsthe performance of various models in predictingcorrect specification trees.
As can be seen, ourmodel achieves an F-Score of 80%.
Our modeltherefore significantly outperforms the No Learn-ing baseline (by more than 25%).
Note that theNo Learning baseline achieves a low Precisionof 57.2%.
This low precision reflects the noisi-ness of the weak supervision - nearly one half ofthe parsers produced by No Learning are actuallyincorrect even though they read all of the inputexamples without error.
This comparison showsthe importance of capturing correlations betweenthe specification trees and their text descriptions.9www.cs.cornell.edu/people/tj/svm light/svm struct.html1300(a)The next N lines of the input file contain the Cartesian coordinates of watchtowers, one pair of coordinates per line.
(b)The input contains several testcases.Each is specified by two strings S, T of alphanumeric ASCII charactersFigure 4: Examples of dependencies and key phrases predicted by our model.
Green marks correct keyphrases and dependencies and red marks incorrect ones.
The missing key phrases are marked in gray.%supervisionFigure 5: Precision and Recall of our model byvarying the percentage of weak supervision.
Thegreen lines are the performance of Aggressivebaseline trained with full weak supervision.Because our model learns correlations via featurerepresentations, it produces substantially more ac-curate translations.While both the Full Model and Aggressive base-line use the same source of feedback, they capi-talize on it in a different way.
The baseline usesthe noisy feedback to train features capturing thecorrelation between trees and text.
Our model, incontrast, combines these two sources of informa-tion in a complementary fashion.
This combina-tion allows our model to filter false positive feed-back and produce 13% more correct translationsthan the Aggressive baseline.Clean versus Noisy Supervision To assess theimpact of noise on model accuracy, we comparethe Full Model against the Full Model (Oracle).The two versions achieve very close performance(80% v.s 84% in F-Score), even though Full Modelis trained with noisy feedback.
This demonstratesthe strength of our model in learning from suchweak supervision.
Interestingly, Aggressive (Ora-cle) outperforms our oracle model by a 5% mar-gin.
This result shows that when the supervisionis reliable, the generative assumption limits ourmodel?s ability to gain the same performance im-provement as discriminative models.#input examplesFigure 6: Precision and Recall of our model byvarying the number of available input examplesper text specification.Impact of Input Examples Our model can alsobe trained in a fully unsupervised or a semi-supervised fashion.
In real cases, it may not bepossible to obtain input examples for all text spec-ifications.
We evaluate such cases by varying theamount of supervision, i.e.
how many text specifi-cations are paired with input examples.
In eachrun, we randomly select text specifications andonly these selected specifications have access toinput examples.
Figure 5 gives the performance ofour model with 0% supervision (totally unsuper-vised) to 100% supervision (our full model).
Withmuch less supervision, our model is still able toachieve performance comparable with the Aggres-sive baseline.We also evaluate how the number of providedinput examples influences the performance of themodel.
Figure 6 indicates that the performance islargely insensitive to the number of input exam-ples ?
once the model is given even one inputexample, its performance is close to the best per-formance it obtains with 100 input examples.
Weattribute this phenomenon to the fact that if thegenerated code is incorrect, it is unlikely to suc-cessfully parse any input.Case Study Finally, we consider some text spec-ifications that our model does not correctly trans-1301late.
In Figure 4a, the program input is interpretedas a list of character strings, while the correct in-terpretation is that the input is a list of string pairs.Note that both interpretations produce C++ inputparsers that successfully read all of the input ex-amples.
One possible way to resolve this problemis to add other features such as syntactic depen-dencies between words to capture more languagephenomena.
In Figure 4b, the missing key phraseis not identified because our model is not able toground the meaning of ?pair of coordinates?
to twointegers.
Possible future extensions to our modelinclude using lexicon learning methods for map-ping words to C++ primitive types for example?coordinates?
to ?int, int?.7 ConclusionIt is standard practice to write English languagespecifications for input formats.
Programmersread the specifications, then develop source codethat parses inputs in the format.
Known disadvan-tages of this approach include development cost,parsers that contain errors, specification misunder-standings, and specifications that become out ofdate as the implementation evolves.Our results show that taking both the correlationbetween the text and the specification tree and thesuccess of the generated C++ parser in reading in-put examples into account enables our method tocorrectly generate C++ parsers for 72.5% of ournatural language specifications.8 AcknowledgementsThe authors acknowledge the support of BattelleMemorial Institute (PO #300662) and the NSF(Grant IIS-0835652).
Thanks to Mirella Lapata,members of the MIT NLP group and the ACL re-viewers for their suggestions and comments.
Anyopinions, findings, conclusions, or recommenda-tions expressed in this paper are those of the au-thors, and do not necessarily reflect the views ofthe funding organizations.ReferencesJames K. Baker.
1979.
Trainable grammars for speechrecognition.
In DH Klatt and JJ Wolf, editors,Speech Communication Papers for the 97th Meet-ing of the Acoustical Society of America, pages 547?550.S.
R. K. Branavan, Harr Chen, Luke S. Zettlemoyer,and Regina Barzilay.
2009.
Reinforcement learningfor mapping instructions to actions.
In Proceedingsof the Annual Meeting of the Association for Com-putational Linguistics.S.R.K Branavan, Luke Zettlemoyer, and Regina Barzi-lay.
2010.
Reading between the lines: Learning tomap high-level instructions to commands.
In Pro-ceedings of ACL, pages 1268?1277.Mingwei Chang, Vivek Srikumar, Dan Goldwasser,and Dan Roth.
2010.
Structured output learningwith indirect supervision.
In Proceedings of the 27thInternational Conference on Machine Learning.David L. Chen and Raymond J. Mooney.
2008.
Learn-ing to sportscast: A test of grounded language acqui-sition.
In Proceedings of 25th International Confer-ence on Machine Learning (ICML-2008).James Clarke, Dan Goldwasser, Ming-Wei Chang, andDan Roth.
2010.
Driving semantic parsing fromthe world?s response.
In Proceedings of the Four-teenth Conference on Computational Natural Lan-guage Learning.Trevor Cohn, Phil Blunsom, and Sharon Goldwater.2010.
Inducing tree-substitution grammars.
Jour-nal of Machine Learning Research, 11.Dipanjan Das and Noah A. Smith.
2011.
Semi-supervised frame-semantic parsing for unknownpredicates.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguis-tics: Human Language Technologies, pages 1435?1444.Dipanjan Das, Nathan Schneider, Desai Chen, andNoah A. Smith.
2010.
Probabilistic frame-semanticparsing.
In Human Language Technologies: The2010 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, pages 948?956.Dan Goldwasser, Roi Reichart, James Clarke, and DanRoth.
2011.
Confidence driven unsupervised se-mantic parsing.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Lin-guistics: Human Language Technologies - Volume1, HLT ?11.Mark Johnson and Thomas L. Griffiths.
2007.Bayesian inference for pcfgs via markov chainmonte carlo.
In Proceedings of the North AmericanConference on Computational Linguistics (NAACL?07).Stephen C. Johnson.
1979.
Yacc: Yet anothercompiler-compiler.
Unix Programmer?s Manual,vol 2b.Rohit J. Kate and Raymond J. Mooney.
2007.
Learn-ing language semantics from ambiguous supervi-sion.
In Proceedings of the 22nd national confer-ence on Artificial intelligence - Volume 1, AAAI?07.1302P.
Liang, M. I. Jordan, and D. Klein.
2009.
Learningsemantic correspondences with less supervision.
InAssociation for Computational Linguistics and In-ternational Joint Conference on Natural LanguageProcessing (ACL-IJCNLP).P.
Liang, M. I. Jordan, and D. Klein.
2011.
Learn-ing dependency-based compositional semantics.
InProceedings of the Annual Meeting of the Associa-tion for Computational Linguistics.Tahira Naseem and Regina Barzilay.
2011.
Using se-mantic cues to learn syntax.
In Proceedings of the25th National Conference on Artificial Intelligence(AAAI).Rahul Pandita, Xusheng Xiao, Hao Zhong, Tao Xie,Stephen Oney, and Amit Paradkar.
2012.
Inferringmethod specifications from natural language api de-scriptions.
In Proceedings of the 2012 InternationalConference on Software Engineering, ICSE 2012,pages 815?825, Piscataway, NJ, USA.
IEEE Press.Hoifung Poon and Pedro Domingos.
2009.
Unsuper-vised semantic parsing.
In Proceedings of the 2009Conference on Empirical Methods in Natural Lan-guage Processing: Volume 1 - Volume 1, EMNLP?09.Lin Tan, Ding Yuan, Gopal Krishna, and YuanyuanZhou.
2007.
/* iComment: Bugs or bad comments?*/.
In Proceedings of the 21st ACM Symposium onOperating Systems Principles (SOSP07), October.Lin Tan, Yuanyuan Zhou, and Yoann Padioleau.
2011.aComment: Mining annotations from comments andcode to detect interrupt-related concurrency bugs.
InProceedings of the 33rd International Conference onSoftware Engineering (ICSE11), May.Lappoon R. Tang and Raymond J. Mooney.
2000.
Au-tomated construction of database interfaces: inte-grating statistical and relational learning for seman-tic parsing.
In Proceedings of the conference onEmpirical Methods in Natural Language Process-ing, EMNLP ?00.Yuk Wah Wong and Raymond J. Mooney.
2007.Learning synchronous grammars for semantic pars-ing with lambda calculus.
In ACL.Luke S. Zettlemoyer and Michael Collins.
2005.Learning to map sentences to logical form: Struc-tured classification with probabilistic categorialgrammars.
In Proceedings of UAI, pages 658?666.Luke S. Zettlemoyer and Michael Collins.
2009.Learning context-dependent mappings from sen-tences to logical form.
In Proceedings of the An-nual Meeting of the Association for ComputationalLinguistics.Hao Zhong, Lu Zhang, Tao Xie, and Hong Mei.
2009.Inferring resource specifications from natural lan-guage api documentation.
In Proceedings of the2009 IEEE/ACM International Conference on Auto-mated Software Engineering, ASE ?09, pages 307?318, Washington, DC, USA.
IEEE Computer Soci-ety.1303
