Language Technology for Closely Related Languages and Language Variants (LT4CloseLang), pages 25?35,October 29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsLanguage variety identification in Spanish tweetsWolfgang MaierInstitute for Language and InformationUniversity of D?usseldorfD?usseldorf, Germanymaierw@hhu.deCarlos G?omez-Rodr??guezDepto.
de Computaci?onUniversidade da Coru?naA Coru?na, Spaincgomezr@udc.esAbstractWe study the problem of language vari-ant identification, approximated by theproblem of labeling tweets from Spanishspeaking countries by the country fromwhich they were posted.
While this taskis closely related to ?pure?
language iden-tification, it comes with additional com-plications.
We build a balanced collec-tion of tweets and apply techniques fromlanguage modeling.
A simplified versionof the task is also solved by human testsubjects, who are outperformed by theautomatic classification.
Our best auto-matic system achieves an overall F-scoreof 67.7% on 5-class classification.1 IntroductionSpanish (or castellano), a descendant of Latin,is currently the language with the second largestnumber of native speakers after Mandarin Chi-nese, namely around 414 million people (Lewiset al., 2014).
Spanish has a large number of re-gional varieties across Spain and the Americas(Lipski, 1994).1They diverge in spoken languageand vocabulary and also, albeit to a lesser extent,in syntax.
Between different American varietiesof Spanish, there are important differences; how-ever, the largest differences can be found betweenAmerican and European (?Peninsular?)
Spanish.Language identification, the task of automati-cally identifying the natural language used in agiven text segment, is a relatively well understoodproblem (see Section 2).
To our knowledge, how-ever, there is little previous work on the identifica-tion of the varieties of a single language, such asthe regional varieties of Spanish.
This task is espe-cially challenging because the differences between1We are aware that there are natively Spanish-speakingcommunities elsewhere, such as on the Philippines, but wedo not consider them in this study.variants are subtle, making it difficult to discernbetween them.
This is evidenced by the fact thathumans that are native speakers of the varietiesare often unable to solve the problem, particularlywhen given short, noisy text segments (which arethe focus of this work) where the amount of avail-able information is limited.In this paper, we approximate the problem oflanguage variety identification by the problemof classifying status messages from the micro-blogging service Twitter (?tweets?)
from Span-ish speaking countries by the country from whichthey were sent.
With the tweet, the location ofthe device from which the tweet was sent can berecorded (depending on the Twitter users?
permis-sion) and can then be retrieved from the metadataof the tweet.
The tweet location information doesnot always correlate with the actual language va-riety used in the tweet: it is conceivable, e.g., thatmigrants do not use the prevalent language vari-ety of the country in which they live, but rathertheir native variety.
Nevertheless, Twitter can givea realistic picture of actual language use in a cer-tain region, which, additionally, is closer to spokenthan to standard written language.
Eventually andmore importantly, Twitter data is available fromalmost all Spanish speaking countries.We proceed as follows.
We build a balancedcollection of tweets sent by Twitter users fromfive countries, namely Argentina, Chile, Colom-bia, Mexico, and Spain.
Applying different meth-ods, we perform an automatic classification be-tween all countries.
In order to obtain a more de-tailed view of the difficulty of our task, we alsoinvestigate human performance.
For this purpose,we build a smaller sample of tweets from Ar-gentina, Chile and Spain and have them classifiedby both our system and three native human evalua-tors.
The results show that automatic classificationoutperforms human annotators.
The best variantof our system, using a meta-classifier with voting,25reaches an overall F-score of 67.72 on the five-class problem.
On the two-class problem, humanclassification is outperformed by a large margin.The remainder of this paper is structured as fol-lows.
In the following section, we present relatedwork.
Section 3 presents our data collection.
Sec-tions 4 and 5 present our classification methodol-ogy and the experiments.
Section 7 discusses theresults, and Section 8 concludes the article.2 Related WorkResearch on language identification has seen a va-riety of methods.
A well established technique isthe use of character n-gram models.
Cavnar andTrenkle (1994) build n-gram frequency ?profiles?for several languages and classify text by match-ing it to the profiles.
Dunning (1994) uses lan-guage modeling.
This technique is general andnot limited to language identification; it has alsobeen successfully employed in other areas, e.g., inauthorship attribution (Ke?selj et al., 2003) and au-thor native language identification (Gyawali et al.,2013).
Other language identification systems usenon-textual methods, exploiting optical propertiesof text such as stroke geometry (Muir and Thomas,2000), or using compression methods which relyon the assumption that natural languages differby their entropy, and consequently by the rateto which they can be compressed (Teahan, 2000;Benedetto et al., 2002).
Two newer approachesare Brown (2013), who uses character n-grams,and?Reh?u?rek and Kolkus (2009), who treat ?noisy?web text and therefore consider the particular in-fluence of single words in discriminating betweenlanguages.Language identification is harder the shorter thetext segments whose language is to be identified(Baldwin and Lui, 2010).
Especially due to therise of Twitter, this particular problem has recentlyreceived attention.
Several solutions have beenproposed.
Vatanen et al.
(2010) compare charactern-gram language models with elaborate smooth-ing techniques to the approach of Cavnar andTrenkle and the Google Language ID API, on thebasis of different versions of the Universal Decla-ration of Human Rights.
Other researchers workon Twitter.
Bergsma et al.
(2012) use languageidentification to create language specific tweet col-lections, thereby facilitating more high-quality re-sults with supervised techniques.
Lui and Baldwin(2014) review a wide range of off-the-shelf toolsfor Twitter language identification, and achievetheir best results with a voting over three individ-ual systems, one of them being langid.py (Luiand Baldwin, 2012).
Carter et al.
(2013) exploitparticular characteristics of Twitter (such as userprofile data and relations between Twitter users)to improve language identification on this genre.Bush (2014) successfully uses LZW compressionfor Twitter language identification.Within the field of natural language processing,the problem of language variant identification hasonly begun to be studied very recently.
Zampieriet al.
(2013) have addressed the task for Spanishnewspaper texts, using character and word n-grammodels as well as POS and morphological infor-mation.
Very recently, the Discriminating betweenSimilar Languages (DSL) Shared Task (Zampieriet al., 2014) proposed the problem of identify-ing between pairs of similar languages and lan-guage variants on sentences from newspaper cor-pora, one of the pairs being Peninsular vs. Argen-tine Spanish.
However, all these approaches aretailored to the standard language found in newssources, very different from the colloquial, noisylanguage of tweets, which presents distinct chal-lenges for NLP (Derczynski et al., 2013; Vilares etal., 2013).
Lui and Cook (2013) evaluate variousapproaches to classify documents into Australian,British and Canadian English, including a corpusof tweets, but we are not aware of any previouswork on variant identification in Spanish tweets.A review of research on Spanish varieties froma linguistics point of view is beyond the scope ofthis article.
Recommended further literature in thisarea is Lipski (1994), Quesada Pacheco (2002)and Alvar (1996b; 1996a).3 Data CollectionWe first built a collection of tweets using theTwitter streaming API,2requesting all tweets sentwithin the geographic areas given by the coordi-nates -120?, -55?and -29?, 30?
(roughly delimit-ing Latin America), as well as -10?, 35?and 3?,46?
(roughly delimiting Spain).
The download ranfrom July 2 to July 4, 2014.
In a second step, wesorted the tweets according to the respective coun-tries.Twitter is not used to the same extent in allcountries where Spanish is spoken.
In the time2https://dev.twitter.com/docs/api/streaming26it took to collect 2,400 tweets from Bolivia,we could collect over 700,000 tweets from Ar-gentina.3To ensure homogeneous conditions forour experiments, our final tweet collection com-prises exactly 100,000 tweets from each of the fivecountries from which most tweets were collected,that is, Argentina, Chile, Colombia, Mexico, andSpain.At this stage, we do not perform any cleanupor normalization operations such as, e.g., deletingforwarded tweets (?re-tweets?
), deleting tweetswhich are sent by robots, or tweets not written inSpanish (some tweets use code switching, or areentirely written in a different language, mostly inEnglish or in regional and minority languages thatcoexist with Spanish in the focus countries).
Ourreasoning behind this is that the tweet productionin a certain country captures the variant of Spanishthat is spoken.We mark the start and end of single tweets by<s> and </s>, respectively.
We use 80% of thetweets of each language for training, and 10% fordevelopment and testing, respectively.
The datais split in a round-robin fashion, i.e., every ninthtweet is put into the development set and everytenth tweet is put in the test set, all other tweetsare put in the training set.In order to help with the interpretation of clas-sification results, we investigate the distribution oftweet lengths on the development set, as shown inFigure 1.
We see that in all countries, tweets tendto be either short, or take advantage of all availablecharacters.
Lengths around 100 to 110 charactersare the rarest.
The clearest further trend is that thetweets from Colombia and, especially, Argentinatend to be shorter than the tweets from the othercountries.4 Automatic Tweet ClassificationThe classification task we envisage is similar tothe task of language identification in short textsegments.
We explore three methods that havebeen used before for that task, namely charactern-gram frequency profiles (Cavnar and Trenkle,1994; Vatanen et al., 2010), character n-gram lan-guage models (Vatanen et al., 2010), as well asLZW compression (Bush, 2014).
Furthermore, weexplore the usability of syllable-based language3We are aware that the Twitter API does not make all senttweets available.
However, we still assume that this huge dif-ference reflects a variance in the number of Twitter users.0501001502000  20  40  60  80  100  120  140#Tweet lengthESCLCOARMXFigure 1: Tweet length distribution50 100 500 1k 10kAR 31.68 29.72 43.93 31.77 18.42CO 24.29 21.36 26.14 19.68 19.03MX 31.86 28.97 32.58 30.28 22.27ES 20.19 25.22 22.08 21.25 16.15CL 22.95 29.74 35.67 26.01 16.69Table 1: Results (F1): n-gram frequency profiles(classes/profile sizes)models.
For all four approaches, we train mod-els for binary classification for each class, i.e., fivemodels that decide for each tweet if it belongs to asingle class.
As final label, we take the output ofthe one of the five classifiers that has the highestscore.We finally use a meta-classifier on the basis ofvoting.
All methods are tested on the developmentset.
For evaluation, we compute precision, recalland F1overall as well as for single classes.Note that we decided to rely on the tweet textonly.
An exploration of the benefit of, e.g., directlyexploiting Twitter-specific information (such asuser mentions or hash tags) is out of the scope ofthis paper.4.1 Character n-gram frequency profilesWe first investigate the n-gram frequency ap-proach of Cavnar and Trenkle (1994).
We use thewell-known implementation TextCat.4The re-sults for all classes with different profile sizes areshown in Table 1.
Table 2 shows precision and re-call for the best setting, a profile with a maximalsize of 500 entries.The results obtained with a profile size of 5004As available from http://odur.let.rug.nl/?vannoord/TextCat/.27class precision recall F1AR 32.60 67.33 43.93CO 31.66 22.26 26.14MX 51.52 23.82 32.58ES 32.83 16.63 22.08CL 31.96 40.36 35.67overall 34.08 34.08 34.08Table 2: Results: n-gram frequency profile with500 n-gramsAR CO MX ES CLAR 6,733 949 384 610 1,324CO 4,207 2,226 720 803 2,044MX 2,547 1,342 2,382 1,051 2,678ES 3,781 1,361 649 1,663 2,546CL 3,384 1,153 488 939 4,036Table 3: Confusion matrix (n-gram freq.
profiles,500 n-grams)entries for Colombia align with the results forSpain and Mexico in that the precision is higherthan the recall.
The results for Chile align withthose for Argentina with the recall being higherthan the precision.
For Mexico and Argentina thedifferences between recall and precision are par-ticularly large (28 and 35 points, respectively).The confusion matrix in Table 3 reveals that tweetsfrom all classes are likely to be mislabeled ascoming from Argentina, while, on the other hand,Mexican tweets are mislabeled most frequently ascoming from other countries.Overall, the n-gram frequency profiles are notvery good at our task, achieving an maximal over-all F-score of only 34.08 with a profile size of 500entries.
However, this performance is still wellabove the 20.00 F-score we would obtain witha random baseline.
Larger profile sizes deterio-rate results: with 10,000 entries, we only havean overall F-score of 18.23.
As observed before(Vatanen et al., 2010), the weak performance canmost likely be attributed to the shortness of thetweets and the resulting lack of frequent n-gramsthat hinders a successful profile matching.
WhileVatanen et al.
alleviate this problem to some ex-tent, they have more success with character-leveln-gram language models, the approach which weexplore next.404550556065702  3  4  5  6F-scoren-gram orderno pruning0.01 pruning0.1 pruning1 pruningFigure 2: Character n-gram lm: Pruning vs. n-gram order4.2 Character n-gram language modelsWe recur to n-gram language models as avail-able in variKN (Siivola et al., 2007).5We runvariKN with absolute discounting and the cross-product of four different pruning settings (no prun-ing, and thresholds 0.01, 0.1 and 1) and five differ-ent n-gram lengths (2 to 6).Figure 2 contrasts the effect of different pruningsettings with different n-gram lengths.
While ex-cessive pruning is detrimental to the result, slightpruning has barely any effect on the results, whilereducing look-up time immensely.
The order ofthe n-grams, however, does have an important in-fluence.
We confirm that also for this problem, wedo not benefit from increasing it beyond n = 6,like Vatanen et al.
(2010).We now check if some countries are more dif-ficult to identify than others and how they bene-fit from different n-gram orders.
Figure 3 visual-izes the corresponding results.
Not all countriesprofit equally from longer n-grams.
When com-paring the 3- and 6-gram models without pruning,we see that the F1for Argentina is just 8 pointshigher, while the difference is more than 14 pointsfor Mexico.Table 4 shows all results including precision andrecall for all classes, in the setting with 6-gramsand no pruning.
We can see that this approachworks noticeably better than the frequency pro-files, achieving an overall F-score of 66.96.
Thebehavior of the classes is not uniform: Argentinashows the largest difference between precision andrecall, and is furthermore the only class in whichprecision is higher than recall.
Note also that in5https://github.com/vsiivola/variKN2835404550556065702  3  4  5  6F-scoren-gram orderESCLCOARMXFigure 3: Character n-gram lm: Classes vs. n-gram order (no pruning)class precision recall F1AR 70.67 66.22 68.37CO 62.56 62.77 62.66MX 65.23 65.74 65.48ES 68.75 69.36 69.06CL 67.81 70.73 69.24overall 66.96 66.96 66.96Table 4: Results: 6-grams without pruninggeneral, the differences between precision and re-call are lower than for the n-gram frequency pro-file approach.
The confusion matrix shown in Ta-ble 5 reveals that the Colombia class is the onewith the highest confusion, particularly in com-bination with the Mexican class.
This could in-dicate that those classes are more heterogeneousthan the others, possibly showing more Twitter-specific noise, such as tweets consisting only ofURLs, etc.We finally investigate how tweet length influ-ences classification performance in the 6-grammodel.
Figure 4 shows the F-scores for intervalsof length 20 for all classes.
The graph confirmsthat longer tweets are easier to classify.
This cor-relates with findings from previous work.
Over82 points F1are achieved for tweets from ChileAR CO MX ES CLAR 6,622 1,036 702 740 900CO 800 6,277 1,151 875 897MX 509 1,237 6,574 847 833ES 630 850 857 6,936 727CL 809 634 794 690 7,073Table 5: Confusion matrix (6-grams, no pruning)40506070809020  40  60  80  100  120  140F-scorelengthESCLCOARMXFigure 4: Character n-grams: Results (F1) fortweet length intervals40506070809020  40  60  80  100  120  140precision/recalllengthCO precisionCO recallCL precisionCL recallFigure 5: Character n-grams: Precision/recall forAR and CLlonger than 120 characters, while for those con-taining up to 20 characters, F1is almost 30 pointslower.
We investigate precision and recall sepa-rately.
Figure 5 shows the corresponding curvesfor the best and worst performing classes, namely,CL and CO. For Chile, both precision and recalldevelop in parallel to the F-score (i.e., the longerthe tweets, the higher the scores).
For Colombia,the curves confirm that the low F1is rather due toa low precision than a low recall, particularly fortweets longer than 40 characters.
This correlateswith the counts in the confusion table (Tab.
5).4.3 Syllable n-gram language modelsSince varieties of Spanish exhibit differences invocabulary, we may think that models based onword n-grams can be more useful than charactern-grams to discriminate between varieties.
How-ever, the larger diversity of word n-grams meansthat such models run into sparsity problems.
Anintermediate family of models can be built by us-2945505560652  3  4F-scoren-gram orderESCLCOARMXFigure 6: Syllable n-gram lm: pruning vs. n-gramordering syllable n-grams, taking advantage of the factthat Spanish variants do not differ in the criteriafor syllabification of written words.
Note that thisproperty does not hold in general for the languageidentification problem, as different languages typ-ically have different syllabification rules, which isa likely reason why syllable n-gram models havenot been used for this problem.To perform the splitting of Spanish words intosyllables, we use the TIP syllabifier (Hern?andez-Figeroa et al., 2012), which applies an algorithmimplementing the general syllabification rules de-scribed by the Royal Spanish Academy of Lan-guage and outlined in standard Spanish dictionar-ies and grammars.
These rules are enough to cor-rectly split the vast majority of Spanish words, ex-cluding only a few corner cases related with wordprefixes (Hern?andez-Figueroa et al., 2013).
Whileaccurate syllabification requires texts to be writtencorrectly with accented characters, and this is of-ten not the case in informal online environments(Vilares et al., 2014); we assume that this need notcause problems because the errors originated byunaccented words will follow a uniform pattern,producing a viable model for the purposes of clas-sification.We train n-gram language models withvariKN as described in the last section, usingabsolute discounting.
Due to the larger vocabularysize, we limit ourselves to 0.01 pruning, and ton-gram orders 2 to 4.
Figure 6 shows the results(F1) of all classes for the different n-gram orders,and Table 6 shows the results for all classes forthe 4-gram language model.As expected, shorter n-grams are more effectivefor syllable than for character language models.class precision recall F1AR 55.94 61.11 58.41CO 53.23 53.03 53.13MX 59.10 56.17 57.60ES 62.35 56.96 59.53CL 59.31 62.12 60.68overall 57.88 57.88 57.88Table 6: Results (F1): Syllable 4-gram lmFor the Chilean tweets, e.g., the F-score for the 2-gram language model is around 11 points higherthan for the character 2-gram language model.Furthermore, the performance seems to convergeearlier, given that the results change only slightlywhen raising the n-gram order from 3 to 4.
Theoverall F-score for the 4-gram language model isaround 6 points lower than for character 4-grams.However, the behavior of the classes is similar:again, Mexico and Colombia have slightly lowerresults than the other classes.4.4 CompressionWe eventually test the applicability ofcompression-based classification using theapproach of Bush (2014).
As mentioned ear-lier, the assumption behind compression-basedstrategies for text categorization is that differenttext categories have a different entropy.
Clas-sification is possible because the effectivity ofcompression algorithms depends on the entropyof the data to be compressed (less entropy ?
morecompression).A simple classification algorithm is Lempel-Ziv-Welch (LZW) (Welch, 1984).
It is based ona dictionary which maps sequences of symbols tounique indices.
Compression is achieved by re-placing sequences of input symbols with the re-spective dictionary indices.
More precisely, com-pression works as follows.
First, the dictionaryis initialized with the inventory of symbols (i.e.,with all possible 1-grams).
Then, until the input isfully consumed, we repeat the following steps.
Wesearch the dictionary for the longest sequence ofsymbols s that matches the current input, we out-put the dictionary entry for s, remove s from theinput and add s followed by the next input symbolto the dictionary.For our experiments, we use our own imple-mentation of LZW.
We first build LZW dictionar-ies by compressing our training sets as described301k 8k 25k 50kAR 28.42 38.78 46.92 51.89CO 19.81 28.27 32.81 36.05MX 22.07 33.90 43.10 45.06ES 22.08 29.48 35.15 38.61CL 27.08 28.22 33.59 36.68Table 7: Results (F1): LZW without tiesabove, using different limits on dictionary lengths.As symbol inventory, we use bytes, not unicodesymbols.
Then we use these dictionaries to com-press all tweets from all test sets, skipping the ini-tialization stage.
The country assigned to eachtweet is the one whose dictionary yields the high-est compression.
We run LZW with different max-imal dictionary sizes.The problem with the evaluation of the resultsis that the compression produced many ties, i.e.,the compression of a single tweet with dictionariesfrom different languages resulted in identical com-pression rates.
On the concatenated dev sets (50ktweets, i.e., 10k per country) with a maximal dic-tionary size of 1k, 8k, 25k and 50k entries, we got14.867, 20,166, 22,031, and 23,652 ties, respec-tively.
In 3,515 (7%), 4,839 (10%), 5,455 (11%)and 6,102 (12%) cases, respectively, the correct re-sult was hidden in a tie.
If we replace the labelsof all tied instances with a new label TIE, we ob-tain the F-scores shown in Table 7.
While they arehigher than the scores for n-gram frequency pro-files, they still lie well below the results for bothsyllable and character language models.While previous literature mentions an ideal sizelimit on the dictionary of 8k entries (Bush, 2014),we obtain better results the larger the dictionaries.Note that already with a dictionary of size 1000,even without including the ties, we are above the20.00 F-score of a random baseline.
The highrate of ties constitutes a major problem of this ap-proach, and remains even if we would find im-provements to the approach (one possibility couldbe to use unicode characters instead of bytes fordictionary initialization).
It cannot easily be alle-viated, because if the compression rate is taken asthe score, particularly the scores for short tweetsare likely to coincide.4.5 VotingVoting is a simple meta-classifying techniquewhich takes the output of different classifiers andclass precision recall F1AR 70.96 68.36 69.64CO 62.44 64.22 63.32MX 66.37 65.67 66.02ES 70.10 69.64 69.87CL 68.97 70.72 69.83overall 67.72 67.72 67.72Table 8: Results: Votingdecides based on a predefined method on one ofthem, thereby combining their strengths and level-ing out their weaknesses.
It has been successfullyused to improve language identification on Twitterdata by Lui and Baldwin (2014).We utilize the character 5-gram and 6-gram lan-guage models without pruning, as well as the syl-lable 3-gram and 4-gram models.
We decide asfollows.
All instances for which the output of the5-gram model coincides with the output of at leastone of the syllable models are labeled with the out-put of the 5-gram model.
For all other instances,the output of the 6-gram model is used.
The corre-sponding results for all classes are shown in Table8.We obtain a slightly higher F-score than forthe 6-gram character language model (0.8 points).In other words, even though the 6-gram languagemodel leads to the highest overall results amongindividual models, in some instances it is out-performed by the lower-order character languagemodel and by the syllable language models, whichhave a lower overall score.5 Human Tweet ClassificationIn order to get a better idea of the difficulty of thetask of classifying tweets by the country of theirauthors, we have tweets classified by humans.Generally, speakers of Spanish have limitedcontact with speakers of other varieties, simplydue to geographical separation of varieties.
Wetherefore recur to a simplified version of our task,in which the test subjects only have to distinguishtheir own variety from one other variety, i.e., per-form a binary classification.
We randomly drawtwo times 150 tweets from the Argentinian test and150 tweets from the Chilean and Spanish test sets,respectively.
We then build shuffled concatena-tions of the first 150 Argentinian and the Chileantweets, as well as of the remaining 150 Argen-tinian and the Spanish tweets.
Then we let three31data subject class prec.
rec.
F1AR-ES AR AR 68.5 76.7 72.3ES 73.5 64.7 68.8ES AR 71.5 62.0 66.4ES 66.5 75.3 70.6n-gram AR 92.3 87.3 89.7ES 88.0 92.7 90.3AR-CL AR AR 61.0 77.3 68.2CL 69.1 50.7 58.5CL AR 70.0 70.0 70.0CL 70.0 70.0 70.0n-gram AR 93.4 84.7 88.8CL 86.0 94.0 89.8Table 9: Results: Human vs. automatic classifica-tionnatives classify them.
The test subjects are notgiven any other training data samples or similar re-sources before the task, and they are instructed notto look up on the Internet any information withinthe tweet that might reveal the country of its author(such as hyperlinks, user mentions or hash tags).Table 9 shows the results, together with the re-sults on the same task of the character 6-grammodel without pruning.
Note that with 300 testinstances out of 20,000, there is a sampling er-ror of ?
4.7% (confidence interval 95%).
The re-sults confirm our intuition in the light of the goodperformance achieved by the n-gram approach inthe 5-class case: when reducing the classificationproblem from five classes to two, human classi-fication performance is much below the perfor-mance of automatic classification, by between 17and 31 F-score points.
In terms of error rate, thehuman annotators made between 3 and 4 timesmore classification errors than the automatic sys-tem.
One can observe a tendency among the hu-man test subjects that more errors come from la-beling too many tweets as coming from their na-tive country than vice versa (cf.
the recall values).In order to better understand the large result dif-ference, we ask the test subjects for the strategiesthey used to label tweets.
They stated that the eas-iest tweets where those specifying a location (?Es-toy en Madrid?
), or referencing local named en-tities (TV programs, public figures, etc.).
In caseof absence of such information, other clues wereused that tend to occur in only one variety.
Theyinclude the use of different words (such as en-fadado (Spain) vs. enojado (America) (?angry?
)),data subject class prec.
rec.
F1AR-ES AR AR 71.8 80.0 75.7ES 74.8 65.4 69.7ES AR 74.6 62.9 68.2ES 65.1 76.3 70.2n-gram AR 93.2 88.6 90.8ES 88.1 92.9 90.4AR-CL AR AR 61.1 78.6 68.8CL 68.8 48.5 56.9CL AR 73.0 71.4 72.2CL 71.2 72.8 72.0n-gram AR 95.3 87.1 91.0CL 87.8 95.6 91.5Table 10: Results: Human vs. automatic classifi-cation (filtered)a different distribution of the same word (such asthe filler pues), and different inflection, such as thesecond person plural verb forms, which in Amer-ican Spanish, albeit sometimes not in Chile, is re-placed by the identical third person plural forms(for the verb hacer (?do?
), the peninsular formwould be hac?eis instead of hacen), and the per-sonal pronoun vos (?you?
), which is rarely usedin Chile, and not used in Spain.
To sum up, thetest subjects generally relied on lexical cues onthe surface, and were therefore bound to miss non-obvious information captured by the character n-gram model.Since the test subjects also stated that sometweets were impossible to assign to a country be-cause they contained only URLs, emoticons, orsimilar, in Table 10 we show a reevaluation of asecond version of the two shuffled concatenatedsamples in which we remove all tweets which con-tain only emoticons, URLs, or numbers; tweetswhich are entirely written in a language other thanSpanish; and tweets which are only two or onewords long (i.e., tweets with zero or one spaces).For the AR-ES data, we remove 23 Spanish and10 Argentinian tweets, while for the AR-CL data,we remove 10 Argentinian and 14 Chilean tweets.As for the human classification on the AR/ESdata, the results for Spain do not change much.
ForArgentina, there is an increase in performance (2to 3 points).
On the AR/CL data, there is a slightimprovement on all sets except for the Chileandata classified.As for the automatic classification, the filter-ing gives better result on all data sets.
However,32training dev testAR 57,546 (71.9%) 7,174 7,196CO 58,068 (72.6%) 7,249 7,289MX 48,527 (60.7%) 6,117 6,061ES 53,199 (66.5%) 6,699 6,657CL 56,865 (71.1%) 6,998 7,071Table 11: Data sizes (filtered by langid.py)the difference between the F1of the filtered andunfiltered data is larger on the AR/CL data set.This can be explained with the fact that amongthe tweets removed from the AR/ES data set, therewere more longer tweets (not written in Spanish)than among the tweets removed from the CL/ARdata set, the longer tweets being easier to iden-tify.
Note that the filtering of tweets does not causemuch change in the difference between human andautomatic classification.6 Language FilteringAs mentioned before, our data has not beencleaned up or normalized.
In particular, the dataset contains tweets written in languages other thanSpanish.
We have reasoned that those can be seenas belonging to the ?natural?
language productionof a country.
However, in order to see what im-pact they have on our classification results, weperform an additional experiment on a version ofthe data were we only include the tweets that thestate-of-the-art language identifier langid.pylabels Spanish (Lui and Baldwin, 2012).6Table11 shows the sizes of all data sets after filtering.Note that many of the excluded tweets are in factwritten in Spanish, but are very noisy, due to or-thography, Twitter hash tags, etc.
The next mostfrequent labels across all tweets is English (9%).Note that in the data from Spain, 2% of the tweetsare labeled as Catalan, 1.2% as Galician, and only0.3% as Basque.Table 12 finally shows the classification re-sults for character 6-gram language models with-out pruning.The changes in F1are minor, i.e., below onepoint, except for the Mexican tweets, which losearound 4 points.
The previous experiments havealready indicated that the Mexican data set is themost heterogeneous one which also resulted in thelargest number of tweets being filtered out.
Ingeneral, we see that the character n-gram method6https://github.com/saffsd/langid.py.class precision recall F1AR 70.32 66.09 68.14CO 63.76 62.22 62.98MX 61.52 61.11 61.31ES 69.13 69.20 69.17CL 67.12 73.29 70.07overall 66.45 66.45 66.45Table 12: Results: Filtered by langid.pyseems to be relatively stable with respect to a dif-ferent number of non-Spanish tweets in the data.More insight could be obtained by performing ex-periments with advanced methods of tweet nor-malization, such as those of Han and Baldwin(2011).
We leave this for future work.7 DiscussionHuman classification of language varieties wasjudged by our test subjects to be considerablymore difficult that differentiating between lan-guages.
Additionally, the test subjects were onlyable to differentiate between two classes.
Whilethe automatic classification results lie below theresults which one would expect for language iden-tification, n-gram classification still achieves goodperformance.Our experiments touch on the more generalquestion of how a language variety is defined.
Inorder to take advantage of the metadata providedby Twitter, we had to restrict the classificationproblem to identifying varieties associated withcountries were tweets were sent.
In reality, theboundaries between variants are often blurred, andthere can also be variance within the same country(e.g., the Spanish spoken in the southern Spanishregion of Andalusia is different from that of As-turias, even if they both share features commonto Peninsular Spanish and larger differences withAmerican Spanish).
However, it would be diffi-cult to obtain a reliable corpus with this kind offine-grained distinctions.It is also worth noting that not all the classifica-tion criteria used by the human test subjects werepurely linguistic ?
for example, a subject couldguess a tweet as being from Chile by recogniz-ing a mention to a Chilean city, public figure orTV show.
Note that this factor intuitively seems tobenefit humans ?
who have a wealth of knowledgeabout entities, events and trending topics fromtheir country ?
over the automatic system.
In spite33of this, automatic classification still vastly outper-formed human classification, suggesting that thelanguage models are capturing linguistic patternsthat are not obvious to humans.8 ConclusionWe have studied different approaches to the taskof classifying tweets from Spanish-speaking coun-tries according to the country from which theywere sent.
To the best of our knowledge, these arethe first results for this problem.
On the problemof assigning one of five classes (Argentina, Mex-ico, Chile, Colombia, Spain) to 10,000 tweets, thebest performance, an overall F-score of 67.72, wasobtained with a voting meta-classifier approachthat recombines the results for four single clas-sifiers, the 6-gram (66.96 F1) and 5-gram (66.75F1) character-based language models, and the 4-gram (57.87 F1) and 3-gram (57.24 F1) syllable-based language models.
For a simplified versionof the problem that only required a decision be-tween two classes (Argentina vs. Chile and Spainvs.
Argentina), given a sample of 150 tweets fromeach class, human classification was outperformedby automatic classification by up to 31 points.In future work, we want to investigate the ef-fect of tweet normalization on our problem, andfurthermore, how the techniques we have used canbe applied to classify text from other social mediasources, such as Facebook.AcknowledgementsThe first author has been funded by DeutscheForschungsgemeinschaft (DFG).
The second au-thor has been partially funded by Ministeriode Econom?
?a y Competitividad/FEDER (GrantTIN2010-18552-C03-02) and by Xunta de Galicia(Grant CN2012/008).ReferencesManuel Alvar, editor.
1996a.
Manual de dialectolog??ahisp?anica.
El espa?nol de Am?erica.
Ariel, Barcelona.Manuel Alvar, editor.
1996b.
Manual de dialectolog??ahisp?anica.
El espa?nol de Espa?na.
Ariel, Barcelona.Timothy Baldwin and Marco Lui.
2010.
Languageidentification: The long and the short of the mat-ter.
In Human Language Technologies: The 2010Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 229?237, Los Angeles, CA.Dario Benedetto, Emanuele Caglioti, and VittorioLoreto.
2002.
Language trees and zipping.
Phys-ical Review Letters, 88(4).Shane Bergsma, Paul McNamee, Mossaab Bagdouri,Clayton Fink, and Theresa Wilson.
2012.
Languageidentification for creating language-specific twittercollections.
In Proceedings of the Second Workshopon Language in Social Media, LSM ?12, pages 65?74, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Ralph D. Brown.
2013.
Selecting and weighting n-grams to identify 1100 languages.
In Springer, ed-itor, Proceedings of the 16th International Confer-ence on Text, Speech, and Dialogue, volume 8082 ofLNCS, pages 475?483, Pilsen, Czech Republic.Brian O. Bush.
2014.
Language identication of tweetsusing LZW compression.
In 3rd Pacific NorthwestRegional NLP Workshop: NW-NLP 2014, Redmond,WA.Simon Carter, Wouter Weerkamp, and ManosTsagkias.
2013.
Microblog language identification:overcoming the limitations of short, unedited and id-iomatic text.
Language Resources and Evaluation,47(1):195?215.William B. Cavnar and John M. Trenkle.
1994.
N-gram-based text categorization.
In In Proceedingsof SDAIR-94, 3rd Annual Symposium on DocumentAnalysis and Information Retrieval, pages 161?175,Las Vegas, NV.Leon Derczynski, Alan Ritter, Sam Clark, and KalinaBontcheva.
2013.
Twitter part-of-speech taggingfor all: Overcoming sparse and noisy data.
In Pro-ceedings of the International Conference on RecentAdvances in Natural Language Processing, pages198?206.
Association for Computational Linguis-tics.Ted Dunning.
1994.
Statistical identification of lan-guage.
Technical Report MCCS-94-273, Comput-ing Research Lab, New Mexico State University.Binod Gyawali, Gabriela Ramirez, and ThamarSolorio.
2013.
Native language identification: asimple n-gram based approach.
In Proceedings ofthe Eighth Workshop on Innovative Use of NLP forBuilding Educational Applications, pages 224?231,Atlanta, Georgia, June.
Association for Computa-tional Linguistics.Bo Han and Timothy Baldwin.
2011.
Lexical normali-sation of short text messages: Makn sens a #twitter.In Proceedings of the 49th Annual Meeting of theAssociation for Computational Linguistics: HumanLanguage Technologies, pages 368?378, Portland,Oregon, USA, June.
Association for ComputationalLinguistics.Zen?on Hern?andez-Figeroa, Gustavo Rodr??guez-Rodr?
?guez, and Francisco J. Carreras-Riudavets.
2012.
Separador de s?
?labas34del espa?nol - silabeador TIP.
Available athttp://tip.dis.ulpgc.es.Zen?on Hern?andez-Figueroa, Francisco J. Carreras-Riudavets, and Gustavo Rodr??guez-Rodr??guez.2013.
Automatic syllabification for Spanishusing lemmatization and derivation to solve theprefix?s prominence issue.
Expert Syst.
Appl.,40(17):7122?7131.Vlado Ke?selj, Fuchun Peng, Nick Cercone, and CalvinThomas.
2003.
N-gram-based author profiles forauthorship attribution.
In Proceedings of PACLING,pages 255?264.M.
Paul Lewis, Gary F. Simons, and Charles D. Fen-nig, editors.
2014.
Ethnologue: Languages ofthe World.
SIL International, Dallas, Texas, sev-enteenth edition edition.
Online version: http://www.ethnologue.com.John M. Lipski.
1994.
Latin American Spanish.
Long-man, London.Marco Lui and Timothy Baldwin.
2012. langid.py: Anoff-the-shelf language identification tool.
In Pro-ceedings of the ACL 2012 System Demonstrations,pages 25?30, Jeju Island, Korea, July.
Associationfor Computational Linguistics.Marco Lui and Timothy Baldwin.
2014.
Accuratelanguage identification of twitter messages.
In Pro-ceedings of the 5th Workshop on Language Analysisfor Social Media (LASM), pages 17?25, Gothenburg,Sweden.Marco Lui and Paul Cook.
2013.
Classifying englishdocuments by national dialect.
In Proceedings ofthe Australasian Language Technology AssociationWorkshop 2013 (ALTA 2013), pages 5?15, Brisbane,Australia, December.Douglas W. Muir and Timothy R. Thomas.
2000.
Au-tomatic language identification by stroke geometryanalysis, May 16.
US Patent 6,064,767.Miguel?Angel Quesada Pacheco.
2002.
El Espa?nolde Am?erica.
Editorial Tecnol?ogica de Costa Rica,Cartago, 2a edition.Vesa Siivola, Teemu Hirsim?aki, and Sami Virpi-oja.
2007.
On growing and pruning kneser-ney smoothed n-gram models.
IEEE Transac-tions on Speech, Audio and Language Processing,15(5):1617?1624.William J. Teahan.
2000.
Text classification and seg-mentation using minimum cross-entropy.
In Pro-ceedings of RIAO?00, pages 943?961.Tommi Vatanen, Jaakko J. Vyrynen, and Sami Virpi-oja.
2010.
Language identification of short textsegments with n-gram models.
In Proceedingsof the Seventh International Conference on Lan-guage Resources and Evaluation (LREC?10), Val-letta, Malta.
European Language Resources Associ-ation (ELRA).David Vilares, Miguel A. Alonso, and Carlos G?omez-Rodr??guez.
2013.
Supervised polarity classificationof spanish tweets based on linguistic knowledge.
InProceedings of 13th ACM Symposium on DocumentEngineering (DocEng 2013), pages 169?172, Flo-rence, Italy.David Vilares, Miguel A. Alonso, and Carlos G?omez-Rodr??guez.
2014.
A syntactic approach for opinionmining on Spanish reviews.
Natural Language En-gineering, FirstView:1?25, 6.Radim?Reh?u?rek and Milan Kolkus.
2009.
Languageidentification on the web: Extending the dictionarymethod.
In Proceedings of CICLing, pages 357?368.Terry A. Welch.
1984.
A technique for high-performance data compression.
Computer, 17(6):8?19, June.Marcos Zampieri, Binyam Gebrekidan Gebre, andSascha Diwersy.
2013.
N-gram language modelsand pos distribution for the identification of spanishvarieties.
In Proceedings of TALN2013, pages 580?587.Marcos Zampieri, Liling Tan, Nikola Ljube?si?c, andJ?org Tiedemann.
2014.
A report on the dsl sharedtask 2014.
In Proceedings of the First Workshopon Applying NLP Tools to Similar Languages, Va-rieties and Dialects, pages 58?67, Dublin, Ireland,August.
Association for Computational Linguisticsand Dublin City University.35
