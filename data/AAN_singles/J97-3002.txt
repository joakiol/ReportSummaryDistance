Stochastic Inversion TransductionGrammars and Bilingual Parsing ofParallel CorporaDeka i  Wu"Hong Kong University of Science andTechnologyWe introduce (1) a novel stochastic nversion transduction grammar formalism for bilinguallanguage modeling of sentence-pairs, and (2) the concept of bilingual parsing with a variety ofparallel corpus analysis applications.
Aside from the bilingual orientation, three major featuresdistinguish the formalism from the finite-state transducers more traditionally found in compu-tational inguistics: it skips directly to a context-free rather than finite-state base, it permits aminimal extra degree of ordering flexibility, and its probabilistic formulation admits an efficientmaximum-likelihood bilingual parsing algorithm.
A convenient normal form is shown to exist.Analysis of the formalism's expressiveness suggests that it is particularly well suited to modelingordering shifts between languages, balancing needed flexibility against complexity constraints.We discuss anumber of examples ofhow stochastic nversion transduction grammars bring bilin-gual constraints to bear upon problematic corpus analysis tasks uch as segmentation, bracketing,phrasal alignment, and parsing.1.
IntroductionWe introduce a general formalism for modeling of bilingual sentence pairs, known asan inversion transduction grammar, with potential application in a variety of corpusanalysis areas.
Transduction grammar models, especially of the finite-state family, havelong been known.
However, the imposition of identical ordering constraints upon bothstreams everely restricts their applicability, and thus transduction grammars have re-ceived relatively little attention in language-modeling research.
The inversion trans-duction grammar formalism skips directly to a context-free, rather than finite-state,base and permits one extra degree of ordering flexibility, while retaining propertiesnecessary for efficient computation, thereby sidestepping the limitations of traditionaltransduction grammars.In tandem with the concept of bilingual anguage-modeling, we propose the con-cept of bilingual parsing, where the input is a sentence-pair rather than a sentence.Though inversion transduction grammars remain inadequate as full-fledged transla-tion models, bilingual parsing with simple inversion transduction grammars turnsout to be very useful for parallel corpus analysis when the true grammar is not fullyknown.
Parallel bilingual corpora have been shown to provide a rich source of con-straints for statistical nalysis (Brown et al 1990; Gale and Church 1991; Gale, Church,and Yarowsky 1992; Church 1993; Brown et al 1993; Dagan, Church, and Gale 1993;Department ofComputer Science, University of Science and Technology, Clear Water Bay, Hong Kong.E-mail: dekai@cs.ust.hk?
1997 Association for Computational LinguisticsComputational Linguistics Volume 23, Number 3Fung and Church 1994; Wu and Xia 1994; Fung and McKeown 1994).
The primarypurpose of bilingual parsing with inversion transduction grammars is not to flag un-grammatical inputs; rather, the aim is to extract structure from the input data, whichis assumed to be grammatical, in keeping with the spirit of robust parsing.
The for-malism's uniform integration of various types of bracketing and alignment constraintsis one of its chief strengths.The paper is divided into two main parts.
We begin in the first part below bylaying out the basic formalism, then show that reduction to a normal form is possible.We then raise several desiderata for the expressiveness of any bilingual language-modeling formalism in terms of its constituent-matching flexibility and discuss howthe characteristics of the inversion transduction formalism are particularly suited toaddress these criteria.
Afterwards we introduce a stochastic version and give an al-gorithm for finding the optimal bilingual parse of a sentence-pair.
The formalism isindependent of the languages; we give examples and applications using Chinese andEnglish because languages from different families provide a more rigorous testingground.
In the second part, we survey a number of sample applications and exten-sions of bilingual parsing for segmentation, bracketing, phrasal alignment, and otherparsing tasks.2.
Inversion Transduction GrammarsA transduction grammar describes a structurally correlated pair of languages.
Forour purposes, the generative view is most convenient: the grammar generates trans-ductions, so that two output streams are simultaneously generated, one for each lan-guage.
This contrasts with the common input-output view popularized by both syntax-directed transduction grammars and finite-state transducers.
The generative view ismore appropriate for our applications because the roles of the two languages are sym-metrical, in contrast o the usual applications of syntax-directed transduction gram-mars.
Moreover, the input-output view works better when a machine for acceptingone of the languages (the input language) has a high degree of determinism, which isnot the case here.Our transduction model is context-free, rather than finite-state.
Finite-state trans-ducers, or FSTs, are well known to be useful for specific tasks such as analysis ofinflectional morphology (Koskenniemi 1983), text-to-speech onversion (Kaplan andKay 1994), and nominal, number, and temporal phrase normalization (Gazdar andMellish 1989).
FSTs may also be used to parse restricted classes of context-free gram-mars (Pereira 1991; Roche 1994; Laporte 1996).
However, the bilingual corpus analysistasks we consider in this paper are quite different from the tasks for which FSTs areapparently well suited.
Our domain is broader, and the model possesses very little apriori specific structural knowledge of the language.As a stepping stone to inversion transduction grammars, we first consider whata context-free model known as a simple transduction grammar (Lewis and Stearns1968) would look like.
Simple transduction grammars (as well as inversion transduc-tion grammars) are restricted cases of the general class of context-free syntax-directedtransduction grammars (Aho and Ullman 1969a, 1969b, 1972); however, we will avoidthe term syntax-directed here, so as to de-emphasize the input-output connotation asdiscussed above.A simple transduction grammar can be written by marking every terminal symbolfor a particular output stream.
Thus, each rewrite rule emits not one but two streams.For example, a rewrite rule of the form A ~ Bxly2Czl means that the terminal symbolsx and z are symbols of.
the language L1 emitted on stream 1, while y is a symbol of378Wu Bilingual Parsing(a) SSPPPNPNNVPVVDetPrepProNAConjAuxCopStop\[SP Stop\]\[NP VP\] I \[NP VV\] I \[NP V\]--* \[Prep NP\]--* \[Det NN\] I \[Det N\]\[ \[Pro\] I \[NP Conj NP\]\[A N\] I INN PP\]\[Aux VP\] I \[Aux VV\] I \[VV PP\]---.
\[V NP\] I \[Cop A\]the/~--* to /~-* I /~  l you/~author i ty /~ I secretary/~accountab le /~ I f inanc ia l /~- ,  and/~l\]-* w i l l /~be/c--~ */O(b) VP ~ (VV PP)Figure 1A simple transduction grammar (a) and an inverted-orientation production (b).the language L2 emitted on stream 2.
It follows that every nonterminal stands for aclass of derivable substring pairs.We can use a simple transduction grammar to model the generation of bilingualsentence pairs.
As a mnemonic onvention, we usually use the alternative notationA --.
B x/y C z/c to associate matching output okens.
Though this additional informa-tion has no formal generative effect, it reminds us that x/y must be a valid entry in thetranslation lexicon.
We call a matched terminal symbol pair such as x/y a couple.
Thenull symbol ?
means that no output token is generated.
We call x/?
an Ll-singleton,and ?/y an L2-singleton.Consider the simple transduction grammar fragment shown in Figure l(a).
(It willbecome apparent below why we explicitly include brackets around right-hand sidescontaining nonterminals, which are usually omitted with standard CFGs.)
The simpletransduction grammar can generate, for instance, the following pair of English andChinese sentences in translation:(1) a.
\[\[\[\[The \[Financial Secretary\]NN \]NP and \[I\]Np \]NP \[will \[beaccountable\]w \]vP \]sP .\]sb.
\ [ \ [ \ [ \ [ \ [~  ~----\]\]NN \] P ~ \[~'~\]NP \]NP \ [~ \ [~\ ]VV lVP lSP o \]SNotice that each nonterminal derives two substrings, one in each language.
The twosubstrings are counterparts of each other.
In fact, it is natural to write the parse treestogether:(2) \[\[\[\[The/c \[Financial/l~qC~J( Secretary/~----JlNN \] P and/~l\] [I/~:J~\]Np \]NP\[wil l /~@ \[be/c accountable/~t~\]vv P IsP ./o \]sOf course, in general, simple transduction grammars are not very useful, precisely379Computational Linguistics Volume 23, Number 3because they require the two languages to share exactly the same grammatical structure(modulo those distinctions that can be handled with lexical singletons).
For example,the following sentence pair from our corpus cannot be generated:(3) a.
The Authority will be accountable to the Financial Secretary.b.
~t:~ ~--a ~ ~ ~,q~  ~ ~.
~ o(Authority will to Financial Secretary accountable.
)To make transduction grammars truly useful for bilingual tasks, we must escapethe rigid parallel ordering constraint of simple transduction grammars.
At the sametime, any relaxation of constraints must be traded off against increases in the com-putational complexity of parsing, which may easily become exponential.
The key isto make the relaxation relatively modest but still handle a wide range of orderingvariations.The inversion transduction grammar (ITG) formalism only minimally extends thegenerative power of a simple transduction grammar, yet turns out to be surprisinglyeffective.
1 Like simple transduction grammars, ITGs remain a subset of context-free(syntax-directed) transduction grammars (Lewis and Steams 1968) but this view is toogeneral to be of much help.
2 The productions of an inversion transduction grammarare interpreted just as in a simple transduction grammar, except that two possibleorientations are allowed.
Pure simple transduction grammars have the implicit char-acteristic that for both output streams, the symbols generated by the right-hand-sideconstituents of a production are concatenated in the same left-to-right order.
Inversiontransduction grammars also allow such productions, which are said to have straightorientation.
In addition, however, inversion transduction grammars allow productionswith inverted orientation, which generate output for stream 2 by emitting the con-stituents on a production's right-hand side in right-to-left order.
We indicate a produc-tion's orientation with explicit notation for the two varieties of concatenation peratorson string-pairs.
The operator \[\] performs the "usual" pairwise concatenation so thatlAB\] yields the string-pair (C1, C2) where C1 = AtB1 and C2 = A2B2.
But the operator 0concatenates constituents on output stream I while reversing them on stream 2, so thatCt = A1B1 but C2 = B2A2.
Since inversion is permitted at any level of rule expansion,a derivation may intermix productions of either orientation within the parse tree.
Forexample, if the inverted-orientation production of Figure l(b) is added to the earliersimple transduction grammar, sentence-pair (3) can then be generated as follows:(4) a.
\[\[\[The Authority\]Np \[will \[\[be accountable\]vv \[to \[the \[\[FinancialSecretarylNN \]NNN \]NP \]PP \]VP \]VP \]SP -\]Sb.
\ [ \ [ \ [~\ ]NP  \ [~ \[\[\[~'\] \ [ [ \ [~  ~---J\]NN \]NNN \]NP \]PP \ [~\ ]VV \]VP \]VP\]sp o lsWe can show the common structure of the two sentences more clearly and com-pactly with the aid of the (/notation:1 The expressiveness of simple transduction grammars i equivalent to nondeterministic pushdowntransducers (Savitch 1982).2 Also keep in mind that ITGs turn out to be especially suited for bilingual parsing applications, whereaspushdown transducers and syntax-directed transduction grammars are designed for monolingualparsing (in tandem with generation).380Wu Bilingual ParsingS.
/owi l l /~The/?/ " p/ Author i ty /~}~Pbe/e accountable/NN the/cFinancial/l~l~ Secretary/~Figure 2Inversion transduction grammar parse tree.
(5) \[\[\[The/~ Author i ty /~ \]NP \[wi l l /~@ (\[be/c accountab le /~\ ]vv\[to/Fh-J \[the/?
\ [ \ [F inanc ia l /~  Secretary/~lNN \]NNN \]NP \]PP )VP \]vP lsp?
/o  IsAlternatively, a graphical parse tree notation is shown in Figure 2, where the (/ levelof bracketing is indicated by a horizontal line.
The English is read in the usual depth-first left-to-right order, but for the Chinese, a horizontal line means the right subtreeis traversed before the left.Parsing, in the case of an ITG, means building matched constituents for inputsentence-pairs rather than sentences.
This means that the adjacency constraints givenby the nested levels must be obeyed in the bracketings of both languages.
The result ofthe parse yields labeled bracketings for both sentences, as well as a bracket algnmentindicating the parallel constituents between the sentences.
The constituent alignmentincludes a word alignment as a by-product.The nonterminals may not always look like those of an ordinary CFG.
Clearly, thenonterminals of an ITG must be chosen in a somewhat different manner than for amonolingual grammar, since they must simultaneously account for syntactic patternsof both languages.
One might even decide to choose nonterminals for an ITG thatdo not match linguistic categories, sacrificing this to the goal of ensuring that allcorresponding substrings can be aligned.An ITG can accommodate a wider range of ordering variation between the lan-381Computational Linguistics Volume 23, Number 3Where is the Secretary of Finance when needed ?II~?~ ~ ~~ ~ ~ J \ ]~ ?Figure 3An extremely distorted alignment that can be accommodated by an ITG.guages than might appear at first blush, through appropriate decomposition of pro-ductions (and thus constituents), in conjuction with introduction of new auxiliary non-terminals where needed.
For instance, even messy alignments uch as that in Figure 3can be handled by interleaving orientations:(6) \[((Where/JJ\]~ is /T)  \[\[the/E (Secretary/~ \[of/( Finance/llq~\])\](when/l~ needed/~'~) \ ] )  ?/?\]This bracketing is of course linguistically implausible, so whether such parses are ac-ceptable depends on one's objective.
Moreover, it may even remain possible to alignconstituents for phenomena whose underlying structure is not context-free--say, ellip-sis or coordination--as long as the surface structures of the two languages fortuitouslyparallel each other (though again the bracketing would be linguistically implausible).We will return to the subject of ITGs' ordering flexibility in Section 4.We stress again that the primary purpose of ITGs is to maximize robustness forparallel corpus analysis rather than to verify grammaticality, and therefore writinggrammars is made much easier since the grammars can be minimal and very leaky.We consider elsewhere an extreme special case of leaky ITGs, inversion-invarianttransduction grammars, in which all productions occur with both orientations (Wu1995).
As the applications below demonstrate, the bilingual lexical constraints carrygreater importance than the tightness of the grammar.Formally, an inversion transduction grammar, or ITG, is denoted by G =(N, W1,W2,T?,S), where dV is a finite set of nonterminals, W1 is a finite set of words(terminals) of language 1, }4;2 is a finite set of words (terminals) of language 2, T?
isa finite set of rewrite rules (productions), and S E A/" is the start symbol.
The spaceof word-pairs (terminal-pairs) X = (W1 U {c}) x (W2 U {c}) contains lexical transla-tions denoted x/y and singletons denoted x/?
or ?/y, where x E W1 and y E W2.
Eachproduction is either of straight orientation written A --~ \[ala2 ... ar\], or of inverted ori-entation written A ~ (ala2.. ?
a r ) ,  where ai E A/" U X and r is the rank of the production.The set of transductions generated by G is denoted T(G).
The sets of (monolingual)strings generated by G for the first and second output languages are denoted LffG)and L2(G), respectively.3.
A Normal Form for Inversion Transduction GrammarsWe now show that every ITG can be expressed as an equivalent ITG in a 2-normal formthat simplifies algorithms and analyses on ITGs.
In particular, the parsing algorithmof the next section operates on ITGs in normal form.
The availability of a 2-normal382Wu Bilingual Parsingform is a noteworthy characteristic of ITGs; no such normal form is available forunrestricted context-free (syntax-directed) transduction grammars  (Aho and Ullman1969b).
The proof closely follows that for standard CFGs, and the proofs of the lemmasare omitted.LemmaFor anyduction1inversion transduction grammar  G, there exists an equivalent inversion trans-grammar  G' where T(G) = T(G'), such that:..If ?
E LI(G) and ?
C L2(G), then G' contains a single production of theform S ~ ~ c/c, where S ~ is the start symbol  of G ~ and does not appear onthe right-hand side of any production of G';otherwise G' contains no productions of the form A ~ c/c.nemmaFor anyductionduction2inversion transduction grammar  G, there exists an equivalent inversion trans-grammar  G' where T(G) = T(G'), such that the right-hand side of any pro-of G t contains either a single terminal-pair or a list of nonterminals.LemmaFor anyductiontions of3inversion transduction grammar  G, there exists an equivalent inversion trans-grammar  G t where T(G) -- T(G'), such that G' does not contain any produc-the form A ~ B.Theorem 1For any inversion transduction grammar  G, there exists an equivalent inversion trans-duction grammar  G t in which every production takes one of the following forms:s c/c A x/c A IBC\]A x/y A ?/y A (BC)ProofBy Lemmas 1, 2, and 3, we may assume G contains only productions of the formS ~ c/c, A --~ x/y,  A ~ x/G A ---* ~/y, A ~ \[BIB2\], A --~ (BIB2), A ~ \[B1... Bn\], andA ---* (B1 .. .
B,) where n _> 3 and A ~ S. Include in G ~ all productions of the first sixtypes.
The remaining two types are transformed as follows:For each production of the form A --~ \[B1... Bn\] we introduce new nonterminalsX1.. .
X,_2 in order to replace the production with the set of rules A --* \[B1X1\],X1 ---+\ [B2X2\ ]  .
.
.
.
.
Xn-3  --+ \ [Bn-2Xn-a \ ] ,Xn-2  ---+ \[Bn-IB,\].
Let (e,c) be any string-pair deriv-able from A ~ \ [B1 . "
Bn\], where e is output on stream 1 and c on stream 2.
Definee i as  the substring of e derived from Bi, and similarly define c i.
Then Xi generates(e  i+1 .
.
.en ,  c i+1 .
.
.C  n) for all 1 ~ i < n - 1, so the new production A --+ \ [B IX1 \ ]  alsogenerates (e, c).
No additional string-pairs are generated ue to the new productions(since each Xi is only reachable from Xi-1 and X1 is only reachable from A).For each production of the form A -~ (B1 .. .
Bn) we replace the production withthe set of rules A ~ ( B1Y1) , Y1 --~ ( B2 Y2)  , .
.
.
, Yn -  3 ---+ ( Bn-  R Yn-  2), Yn -  2 --~ ( Bn-  I Bn) .
Let(e, c) be any string-pair derivable from A ~ (B1 ' ' .
Bn) ,  where e is output on stream1 and c on stream 2.
Again define e i and c i as the substrings derived from Bi, butin this case (e, c) = (e 1 ?
?
?
e ", c" ?
?
?
c 1 ).
Then Yi  generates (e  i+1 ?
?
?
e n, c n ?
?
?
c i+1 ) for all383Computational Linguistics Volume 23, Number 31 _~ i < n - 1, so the new production A --* (B1Y1) also generates (e,c).
Again, noadditional string-pairs are generated ue to the new productions.
\[\]Henceforth all transduction grammars will be assumed to be in normal form.4.
Expressiveness CharacteristicsWe now turn to the expressiveness desiderata for a matching formalism.
It is of coursedifficult to make precise claims as to what characteristics are necessary and/or suffi-cient for such a model, since no cognitive studies that are directly pertinent to bilingualconstituent alignment are available.
Nonetheless, most related previous parallel cor-pus analysis models share certain conceptual approaches with ours, loosely based oncross-linguistic theories related to constituency, case frames, or thematic roles, as wellas computational feasibility needs.
Below we survey the most common constraintsand discuss their relation to ITGs.Crossing Constraints.
Arrangements where the matchings between subtrees crosseach another are prohibited by crossing constraints, unless the subtrees' immediateparent constituents are also matched to each other.
For example, given the constituentmatchings depicted as solid lines in Figure 4, the dotted-line matchings correspondingto potential lexical translations would be ruled illegal.
Crossing constraints are im-plicit in many phrasal matching approaches, both constituency-oriented (Kaji, Kida,and Morimoto 1992; Cranias, Papageorgiou, and Peperidis 1994; Grishman 1994) anddependency-oriented (Sadler and Vendelmans 1990; Matsumoto, Ishimoto, and Ut-suro 1993).
The theoretical cross-linguistic hypothesis here is that the core argumentsof frames tend to stay together over different languages.
The constraint is also usefulfor computational reasons, since it helps avoid exponential bilingual matching times.ITGs inherently implement a crossing constraint; in fact, the version enforced byITGs is even stronger.
This is because ven within a single constituent, immediatesubtrees are only permitted to cross in exact inverted order.
As we shall argue below,this restriction reduces matching flexibility in a desirable fashion.Rank Constraints.
The second expressiveness desideratum for a matching formal-ism is to somehow limit the rank of constituents (the number of children or right-hand-side symbols), which dictates the span over which matchings may cross.
As thenumber of subtrees of an Ll-constituent grows, the number of possible matchings tosubtrees of the corresponding L2-constituent grows combinatorially, with correspond-ing time complexity growth on the matching process.
Moreover, if constituents canimmediately dominate too many tokens of the sentences, the crossing constraint loseseffectiveness--in the extreme, if a single constituent immediately dominates the en-tire sentence-pair, then any permutation is permissible without violating the crossingconstraint.
Thus, we would like to constrain the rank as much as possible, while stillpermitting some reasonable degree of permutation flexibility.Recasting this issue in terms of the general class of context-free (syntax-directed)transduction grammars, the number of possible subtree matchings for a single con-stituent grows combinatorially with the number of symbols on a production's right-hand side.
However, it turns out that the ITG restriction of allowing only matchingswith straight or inverted orientation effectively cuts the combinatorial growth, whilestill maintaining flexibility where needed.To see how ITGs maintain eeded flexibility, consider Figure 5, which shows all 24possible complete matchings between two constituents of length four each.
Nearly allof these--22 out of 24--can be generated by an ITG, as shown by the parse trees (whose384Wu Bilingual ParsingThe Security Bureau grante / authority t o _ _ t h e  polic~ stationFigure 4The crossing constraint.nonterminal labels are omitted).
3 The 22 permitted matchings are representative of realtranspositions in word order between the English-Chinese sentences in our data.
Theonly two matchings that cannot be generated are very distorted transpositions that wemight call "inside-out" matchings.
We have been unable to find real examples in ourdata of constituent arguments undergoing "inside-out" transposition.Note that this hypothesis is for fixed-word-order languages that are lightly in-flected, such as English and Chinese.
It would not be expected to hold for so-calledscrambling or free-word-order languages, or heavily inflected languages.
However,inflections provide alternative surface cues for determining constituent roles (and3 As discussed later, in many cases more than one parse tree can generate the same subconstituentmatching.
The trees hown are the canonical parses, as generated by the grammar of Figure 10.385Wu Bilingual Parsingr ITG all matchings ratio0 1 1 1.0001 1 1 1.0002 2 2 1.0003 6 6 1.0004 22 24 0.9175 90 120 0.7506 394 720 0.5477 1,806 5,040 0.3588 8,558 40,320 0.2129 41,586 362,880 0.11510 206,098 3,628,800 0.05711 1,037,718 39,916,800 0.02612 5,293,446 479,001,600 0.01113 27,297,738 6,227,020,800 0.00414 142,078,746 87,178,291,200 0.00215 745,387,038 1,307,674,368,000 .00116 3,937,603,038 20,922,789,888,000 .000Figure 6Growth in number of legal complete subconstituent matchings for context-free (syntax-directed)transduction grammars with rank r, versus ITGs on a pair of subconstituent sequences oflength r each.5.
Stochastic Inversion Transduction GrammarsIn a stochastic ITG (SITG), a probability is associated with each rewrite rule.
Followingthe standard convention, we use a and b to denote probabilities for syntactic andlexical rules, respectively.
For example, the probability of the rule NN 0~ \[A N\] isaNN-,\[A N\] = 0.4.
The probability of a lexical rule A 0.0001 x/y is bA(X,y) ~- 0.001.
LetW1, W2 be the vocabulary sizes of the two languages, and X = {A1 .
.
.
.
.
AN} be theset of nonterminals with indices 1 , .
.
.
,N .
(For conciseness, we sometimes abuse thenotation by writing an index when we mean the corresponding nonterminal symbol,as long as this introduces no confusion.)
Then for every 1 < i < N, the productionprobabilities are subject o the constraint thatY~ (ai--qjk\] +ai-~(jk)) + y~ bi(x,y) = 11Kj,kK_N l<_x<wlI~y~W2We now introduce an algorithm for parsing with stochastic ITGs that computesan optimal parse given a sentence-pair using dynamic programming.
In bilingualparsing, just as with ordinary monolinguat parsing, probabilizing the grammar permitsambiguities to be resolved by choosing the maximum-likelihood parse.
Our algorithmis similar in spirit to the recognition algorithm for HMMs (Viterbi 1967) and to CYKparsing (Kasami 1965; Younger 1967).Let the input English sentence be el .
.
.
.
.
eT and the corresponding input Chinesesentence be cl .
.
.
.
.
cv.
As an abbreviation we write es.t for the sequence of words%+1, es+2 .
.
.
.
.
et, and similarly for cu v; also, es s = c is the empty string.
It is convenientto use a 4-tuple of the form q = (s, t, u, v) to identify each node of the parse tree, where387Computational Linguistics Volume 23 Number 3r ITG all matchings ratio0 1 1 1.0001 2 2 1.0002 7 7 1.0003 34 34 1.0004 207 209 0.9905 1,466 1,546 0.9486 11,471 13,327 0.8617 96,034 130,922 0.7348 843,527 1,441,729 0.5859 7,678,546 17,572,114 0.43710 71,852,559 234,662,231 0.30611 687,310,394 3,405,357,682 0.20212 6,693,544,171 53,334,454,417 0.12613 66,167,433,658 896,324,308,634 0.07414 662,393,189,919 16,083,557,845,279 0.04115 6,703,261,197,506 306,827,170,866,106 0.02216 68,474,445,473,303 6,199,668,952,527,617 0.011Figure 7Growth in number of all legal subconstituent matchings (complete or partial, meaning thatsome subconstituents are permitted to remain unmatched as singletons) for context-flee(syntax-directed) transduction grammars with rank r, versus ITGs on a pair of subconstituentsequences of length r each.the substrings es..t and ?u..v both derive from the node q. Denote the nonterminal labelon q by f(q).
Then for any node q = (s, t, u, v), define6q(i) = 6stuv(i) = max P\[subtree of q,e(q) = i , i  ~ es..t/Cu..v\]subtrees of qas the maximum probability of any derivation from i that successfully parses both es .tand cu..v. Then the best parse of the sentence pair has probability 60,T,0,v(S).The algorithm computes 60,T,0,v(S) using the following recurrences.
Note thatwe generalize argmax to the case where maximization ranges over multiple indices,by making it vector-valued.
Also note that \[\] and 0 are simply constants, writtenmnemonically.
The condition (S - s)(t  -S )  + (U  - u ) (v  - U) ~ 0 is a way to specifythat the substring in one, but not both, languages may be split into an empty string cand the substring itself; this ensures that the recursion terminates, but permits wordsthat have no match in the other language to map to an ~ instead.1.
Init ial izationl<t<T~t-l,t,v-l ,v(i) = bi(et/Cv), 1 < v < V (1)1<t<T6t-u,v,v(i) = bi(et/~), 0 < v < V (2)0<t<T~t,t,v-l,v(i) =- bi(?/Cv), 1 < V < V (3)388Wu Bil ingual Parsing..Recurs ionl ( i~NFor all i,s,t,u,v such that o_<s<,_<r o<_u<v<_vt--s+v--u)2Gt.v(i)Gtuv(i)ma \[\] ' 0 ?
= xG.d0, s, v(0\]\[\] if 6~),v(i ) > 6~( i )\[ 0 otherwise(4)(5)where6~,v(i)slur\ \]n\[:uv(i)o'~}~(i)V~)uv(i)6J~uv(i)~l,v(i)cr (> :D stuvk \]v 0 ri~ stuv \ \]max I~j~N1SkSN~Ssstu<U<v(s-s)(t-s)+(u-.
)(~-u)~oargmax1SjSN1SkSN~<s<tu~UKv(S--s)(t--S)+(U--u)(v--U)~Omaxl~j~Nl~k~NsSSStu<U<v(S--s)(t--S)+(U--u)(v--U)?OargmaxI<j<_Nl<k_<Ns_<s_<t .<u<v(S-~)(t-s)+(U-u)(v-u)~aoai.\[jk\] 6sSuU(j) 5Stuv(k) (6)ai--\[/k\] 6~S,U(j) 6stuv(k) (7)ai--(jk) 6sSUv(j) 6Stuu(k) (8)ai~(/k) 6sSUv(j) 5Stuu(k) (9)Reconst ruct ionInitialize by setting the root of the parse tree to ql = (0, T, 0, V) and itsnonterminal label to t(ql) = S. The remaining descendants in the optimalparse tree are then given recursively for any q = (s, t, u, v) by:NIL if t - s+v-u~2LEFT(q) = (S,O'~\](f(q)),U,v~\](~.
(q))) if Oq(f(q)) = \[\] and t-s+v-u>3 (10)(s, cr~ ) (f(q)), v~ )(f(q)),v) if Oq(t.(q)) = 0 and t-s+v-u>2NIL if t - s+v-u~2RmHT(q) = (o'~\](t.(q)),t,v~\](f(q)),V) if ~q(f(q)) = \[\] and t-s+v-u>2 (11)(cr~) (f(q)), t, u, v~ )(f(q))) if Oq(f(q)) = 0 and ,-s+v-u>2~?
(LEFT(q)) = ~*(ffq))(~(q)) (12)~(RIGHT(q)) = ~q(~(q))(f(q)) (13)The time complexity of this algorithm in the general case is O(N3T3V3), whereN is the number of distinct nonterminals and T and V are the lengths of the twosentences.
This is a factor of V 3 more than monolingual chart parsing, but has turnedout to remain quite practical for corpus analysis, where parsing need not be real-time.389Computational Linguistics Volume 23, Number 36.
Translation-driven SegmentationSegmentation f the input sentences i an important step in preparing bilingual cor-pora for various learning procedures.
Different languages realize the same conceptusing varying numbers of words; for example, a single English word may surface asa compound in French.
This complicates the problem of matching the words betweena sentence-pair, since it means that compounds or collocations must sometimes betreated as lexical units.
The translation lexicon is assumed to contain collocation trans-lations to facilitate such multiword matchings.
However, the input sentences do notcome broken into appropriately matching chunks, so it is up to the parser to decidewhen to break up potential collocations into individual words.The problem is particularly acute for English and Chinese because word bound-aries are not orthographically marked in Chinese text, so not even a default chunkingexists upon which word matchings could be postulated.
(Sentences (2) and (5) demon-strate why the obvious trick of taking single characters as words is not a workablestrategy.)
The usual Chinese NLP architecture first preprocesses input text through aword segmentation module (Chiang et al 1992; Lin, Chiang, and Su 1992, 1993; Changand Chen 1993; Wu and Tseng 1993; Sproat et al 1994; Wu and Fung 1994), but, clearly,bilingual parsing will be hampered by any errors arising from segmentation ambigui-ties that could not be resolved in the isolated monolingual context because ven if theChinese segmentation is acceptable monolingually, it may not agree with the wordspresent in the English sentence.
Matters are made still worse by unpredictable omis-sions in the translation lexicon, even for valid compounds.We therefore xtend the algorithm to optimize the Chinese sentence segmentationin conjunction with the bracketing process.
Note that the notion of a Chinese "word"is a longstanding linguistic question, that our present notion of segmentation doesnot address.
We adhere here to a purely task-driven definition of what a correct "seg-mentation" is, namely that longer segments are desirable only when no compositionaltranslation is possible.
The algorithm is modified to include the following computa-tions, and remains the same otherwise:1.
Initialization0 * (~stuv(l) ~-- bi(e~ t/cu..v),O<s<t<T0<u<v<V (14)2.
Recursion(Sstuv(i) max\[(~)uv(i),( ~0 " ? '
= stuvO), 6stuv(O\] (15)\[\] if 6~)uv(i ) > 6}~uv(i ) and \[\] ' 0 ?
~stuv(z) > 6stuv(Z)Gtuv(i) = (} if 6}~,~v(i ) > 6~,v(i ) and vstuv (i~, > 6st,v(Z ?
(16)0 otherwise3.
ReconstructionLEFT(q) z{ NIL (s, ~\]( e(q) ), u,,~,~ \](e(q) ) ) (s,,,~) (e(q)), ~,~1 (e(q)),v)NILif t-s+v-u<_2if Oq(f(q)) = \[\] and t -s+v-u>2if Oq(e(q)) = 0 and t -s+v-u>2otherwise(17)390Wu Bilingual ParsingNIL if t-s+v-u~2(rr~\](f(q)),t,v~\](f(q)),v) if Oq(f(q)) = \[\] and t-s+v-u>2 (18)RIGHT(q) = (?~)(f(q)),t,u,v~) (f(q))) if Oq(f(q)) = 0 and t-s+v-u>2NIL otherwiseIn our experience, this method has proven extremely effective for avoiding misseg-mentation pitfalls, essentially erring only in pathological cases involving coordinationconstructions or lexicon coverage inadequacies.
The method is also straightforward toemploy in tandem with other applications, uch as those below.7.
BracketingBracketing is another intermediate corpus annotation, useful especially when a full-coverage grammar with which to parse a corpus is unavailable (for Chinese, an evenmore common situation than with English).
Aside from purely linguistic interest,bracket structure has been empirically shown to be highly effective at constraining sub-sequent training of, for example, stochastic context-free grammars (Pereira and Schabes1992; Black, Garside, and Leech 1993).
Previous algorithms for automatic bracketingoperate on monolingual texts and hence require more grammatical constraints; for ex-ample, tactics employing mutual information have been applied to tagged text (Mager-man and Marcus 1990).Our method based on SITGs operates on the novel principle that lexical correspon-dences between parallel sentences yields information from which partial bracketingsfor both sentences can be extracted.
The assumption that no grammar is availablemeans that constituent categories are not differentiated.
Instead, a generic bracket-ing transduction grammar is employed, containing only one nonterminal symbol, A,which rewrites either recursively as a pair of A's or as a single terminal-pair:A a \[A A\]A a (A A)A "~ Ui/V jA ~ ui/?b~j A --, ( /v jfor all i,j English-Chinese l xical translationsfor all i English vocabularyfor all j Chinese vocabularyLonger productions with rank > 2 are not needed; we show in the subsections belowthat this minimal transduction grammar in normal form is generatively equivalentto any reasonable bracketing transduction grammar.
Moreover, we also show howpostprocessing using rotation and flattening operations restores the rank flexibility sothat an output bracketing can hold more than two immediate constituents, as shownin Figure 11.The bq distribution actually encodes the English-Chinese translation lexicon withdegrees of probability on each potential word translation.
We have been using a lexiconthat was automatically learned from the HKUST English-Chinese Parallel BilingualCorpus via statistical sentence alignment (Wu 1994) and statistical Chinese word andcollocation extraction (Fung and Wu 1994; Wu and Fung 1994), followed by an EMword-translation-learning procedure (Wu and Xia 1994).
The latter stage gives us thebij probabilities directly.
For the two singleton productions, which permit any word ineither sentence to be unmatched, a small c-constant can be chosen for the probabilitiesbit and bq, so that the optimal bracketing resorts to these productions only when it is391Computational Linguistics Volume 23, Number 3otherwise impossible to match the singletons.
The parameter a here is of no practicaleffect, and is chosen to be very small relative to the bq probabilities of lexical translationpairs.
The result is that the maximum-likelihood parser selects the parse tree that bestmeets the combined lexical translation preferences, as expressed by the bij probabilities.Pre-/postpositional biases.
Many bracketing errors are caused by singletons.
Withsingletons, there is no cross-lingual discrimination to increase the certainty betweenalternative bracketings.
A heuristic to deal with this is to specify for each of the twolanguages whether prepositions or postpositions are more common, where "preposi-tion" here is meant not in the usual part-of-speech sense, but rather in a broad senseof the tendency of function words to attach left or right.
This simple strategem iseffective because the majority of unmatched singletons are function words that lackcounterparts in the other language.
This observation holds assuming that the transla-tion lexicon's coverage is reasonably good.
For both English and Chinese, we specifya prepositional bias, which means that singletons are attached to the right wheneverpossible.A Singleton-Rebalancing Algorithm.
We give here an algorithm for further improv-ing the bracketing accuracy in cases of singletons.
Consider the following bracketingproduced by the algorithm of the previous ection:(7) \[\[The/c \ [Author i ty/~Y~j \[wi l l /~@ (\[be/?
accountable/~t~ \] \[to the/c\[?/f6-J \ [F inanc ia l /~  Secretary/~ \]\]\])\]\]\] ./o \]The prepositional bias has already correctly restricted the singleton The/?
to attach tothe right, but of course The does not belong outside the rest of the sentence, but ratherwith Authority.
The problem is that singletons have no discriminative power betweenalternative bracket matchings--they only contribute to the ambiguity.
We can minimizethe impact by moving singletons as deep as possible, closer to the individual wordthey precede or succeed; or in other words, we can widen the scope of the bracketsimmediately following the singleton.
In general this improves precision since wide-scope brackets are less constraining.The algorithm employs a rebalancing strategy reminiscent of balanced tree struc-tures using left and right rotations.
A left rotation changes a (A(BC)) structure to a((AB)C) structure, and vice versa for a right rotation.
The task is complicated by thepresence of both \[\] and 0 brackets with both L1- and L2-singletons, ince each com-bination presents different interactions.
To be legal, a rotation must preserve symbolorder on both output streams.
However, the following lemma shows that any subtreecan always be rebalanced at its root if either of its children is a singleton of eitherlanguage.Lemma 4Let x be an Ll-singleton, y be an L2-singleton, and A, B, C be arbitrary terminal ornonterminal symbols.
Then the following properties hold for the \[\] and () operators,where the ~ relation means that the same two output strings are generated, and thematching of the symbols is preserved:(Associativity)\[A\[BC\]\] = \[lAB\]C\](A(BC)) = ((AB)C)392Wu Bilingual ParsingSINK-SINGLETON(node)1 if node is not a leaf2 if a rotation property applies at node3 apply the rotation to node4 child *-- the child into which the singleton was rotated5 SINK-SINGLETON(child)REBALANCE- TREE(node)1 if node is not a leaf2 REBALANCE-TREE(left-child\[node\])3 REBALANCE-TREE(right-child\[node\])4 SINK-SINGLETON(node)Figure 8The singleton rebalancing schema.
(Ll-singleton bidirectionality)lax\] = (Ax)\[xA\] ~ (xA)(L2-singleton flipping commutativity)lAy\] : (yA)\[yA\] = (Ay)(Ll-singleton rotation properties)\[x(AB)\] = (x(AB)) ~- ((xA)B) = (\[xA\]B)(x\[AB\]) = \[x\[AB\]\] ~ \[\[xA\]B\] ~- \[(xA)B\]\[(aB)x\] ~ ((aB)x) ~ (A(Bx)) = (A\[Bx\])(\[AB\]x) = \[lAB\]x\] = \[A\[Bx\]\] = \[A(Bx)\](L2-singleton rotation properties)\[y(AB)\] ~ ((AB)y) = (A(By)) = (A\[yB\])(y\[AB\]) = \[lAB\]y\] ~- \[A\[By\]\] ~ \[A(yB)\]\[(AB)y\] = (y(AB)) = ((yA)B) = (\[Ay\]B)(\[ABly) = \[y\[AB\]\] = \[\[yA\]B\] = \[(Ay)B\]The method of Figure 8 modifies the input tree to attach singletons as closelyas possible to couples, but remaining consistent with the input tree in the followingsense: singletons cannot "escape" their immediately surrounding brackets.
The key isthat for any given subtree, if the outermost bracket involves a singleton that shouldbe rotated into a subtree, then exactly one of the singleton rotation properties willapply.
The method proceeds depth-first, sinking each singleton as deeply as possible.393Computational Linguistics Volume 23, Number 31 2 3 4 1 2 3 41 2 3 4 1 2 3 4(a) (b)Figure 9Alternative ITG parse trees for the same matching.1 2 3 41 2 3 4(c)For example, after rebalancing, sentence (7) is bracketed as follows:(8) \[\[\[\[The/?
Author i ty /~\ ]  \[wil l/~J~ (\[be/?
accountab le /~\ ]  [tothe/?
\[?/\[6-J \ [F inanc ia l /~  Secretary/~ \]\]\])\]\]\] ./o \]Flattening the Bracketing.
In the worst case, both sentences might have perfectlyaligned words, lending no discriminative leverage whatsoever to the bracketer.
Thisleaves a very large number of choices: if both sentences are of length l, then there(2t~ i are ~ l \] ~ possible bracketings with rank 2, none of which is better justified than anyother.
Thus to improve accuracy, we  should reduce the specificity of the bracketing'scommitment in such cases.An inconvenient problem with ambiguity arises in the simple bracketing rammarabove, illustrated by Figure 9; there is no justification for preferring either (a) or (b) overthe other.
In general the problem is that both the straight and inverted concatenationoperations are associative.
That is, \[A\[AA\]\] and \[\[AA\]A\] generate the same two outputstrings, which are also generated by \[AAA\]; and similarly with (A(AA)) and ((AA)A),which can also be generated by (AAA).
Thus the parse shown in (c) is preferable toeither (a) or (b) since it does not make an unjustifiable commitment either way.Productions in the form of (c), however, are not permitted by the normal form weuse, in which each bracket can only hold two constituents.
Parsing must overcommit,since the algorithm is always forced to choose between (A(BC)) and ((AB)C) structureseven when no choice is clearly better.
We could relax the normal form constraint,but longer productions clutter the grammar unnecessarily and, in the case of genericbracketing rammars, reduce parsing efficiency considerably.Instead, we employ a more complicated but better-constrained grammar as shownin Figure 10, designed to produce only canonical tail-recursive parses.
We differenti-ate type A and B constituents, representing subtrees whose roots have straight andinverted orientation, respectively.
Under this grammar, a series of nested constituentswith the same orientation will always have a left-heavy derivation.
The guaranteethat parsing will produce a tail-recursive tree facilitates easily identification of thosenesting levels that are associative (and therefore arbitrary), so that those levels canbe "flattened" by a postprocessing stage after parsing into non-normal form trees likethe one in Figure 9(c).
The algorithm proceeds bottom-up, eliminating as many brack-ets as possible, by making use of the associativity equivalences \[lAB\]C\] = \[ABC\] and((ABIC) ~ (ABC).
The singleton bidirectionality and flipping commutativity equiv-alences (see Lemma 4) can also be applied whenever they render the associativityequivalences applicable.394Wu Bilingual ParsingA a \[A B\]A -~ \[B B\]A a \[C B\]A a \[A C\]A a \[B C\]B Z (A A)B a (B A)B ~ {C A)B a (AC)B ~ (B C)C ~ ui/vjC ~ ui/?C ~b'i ?/vjfor all i,j English-Chinese l xical translationsfor all i English vocabularyfor all j Chinese vocabularyFigure 10A stochastic onstituent-matching ITG.The final result after flattening sentence (8) is as follows:(9) \[ The/e Authority/wi l l /  {\[ be/?
accountable/\] \[ to the/~ ~/Financial/Secretary/ \]) ./ \]Experiment.
Approximately 2,000 sentence-pairs with both English and Chineselengths of 30 words or less were extracted from our corpus and bracketed usingthe algorithm described.
Several additional criteria were used to filter out unsuitablesentence-pairs.
If the lengths of the pair of sentences differed by more than a 2:1 ratio,the pair was rejected; such a difference usually arises as the result of an earlier errorin automatic sentence alignment.
Sentences containing more than one word absentfrom the translation lexicon were also rejected; the bracketing method is not intendedto be robust against lexicon inadequacies.
We also rejected sentence-pairs with fewerthan two matching words, since this gives the bracketing algorithm no discriminativeleverage; such pairs accounted for less than 2% of the input data.
A random sample ofthe bracketed sentence-pairs was then drawn, and the bracket precision was computedunder each criterion for correctness.
Examples are shown in Figure 11.The bracket precision was 80% for the English sentences, and 78% for the Chinesesentences, as judged against manual bracketings.
Inspection showed the errors to bedue largely to imperfections of our translation lexicon, which contains approximately6,500 English words and 5,500 Chinese words with about 86% translation accuracy (Wuand Xia 1994), so a better lexicon should yield substantial performance improvement.Moreover, if the resources for a good monolingual part-of-speech or grammar-basedbracketer such as that of Magerman and Marcus (1990) are available, its output canreadily be incorporated in complementary fashion as discussed in Section 9.395Computational Linguistics Volume 23, Number 3\[These/~_- a~ arrangements/~J~ will/c c/~-J enhance/\]JIl~ our /~ (\[?/~ ability/~ll~~\] \[to/e /Et~ mainta in / , .~  monetary /~ s tab i l i ty / l~  in the years to come/el)./o \]\[The/?
Author i ty /~ w i l l /~  (\[be/e accountab le /~\ ]  \[to the/?
?
/~F inanc ia l /~  Secretary/~\]/ ./o \]\[They/~d~ ( are/e right/iE~tf e/nL~ to/e do /~ e/~_~!J~ so/e ) ./o \]\[(\[ Even/e more/~l~ important/~l~ \] \[,/?
however/{EI \]) \[,/c e/B-~, i s /~  to make thevery best of our/e e/~'~=JE~ own/T\];~ e/IY'J talent/,k:q- 1-/o \]\ [ I /~ hope/c e/<>~ employers /~E w i l l /~  make full/e e /~:~ use /~ \[of/ethose/~\]l~-aZ a\] ((\[~/f~J-r who/X\ ]  \[have acquired/e e/~-~\] new/~J~ skills/~\]l~ \])\ [through/~i~ th i s /L~ programme/~illl\]\]/ ./o 1\[I/~J~ have/~, <> at/e length/~-~,~l\]t ( on/e how/,a~,~ we/~ e/-~l~,,~) \[can/~--JJ)~boost/e e /~ j~ our/2~i'~ e /~ prosperity/~l~i~\] ./o IFigure 11Bracketing output examples.
(<> = unrecognized input token.)8.
Alignment8.1 Phrasal AlignmentPhrasal translation examples at the subsentential level are an essential resource formany MT and MAT architectures.
This requirement is becoming increasingly directfor the example-based machine translation paradigm (Nagao 1984), whose translationflexibility is strongly restricted if the examples are only at the sentential level.
It cannow be assumed that a parallel bilingual corpus may be aligned to the sentence levelwith reasonable accuracy (Kay and Ri3cheisen 1988; Catizone, Russel, and Warwick1989; Gale and Church 1991; Brown, Lai, and Mercer 1991; Chen 1993), even for lan-guages as disparate as Chinese and English (Wu 1994).
Algorithms for subsententialalignment have been developed as well as granularities of the character (Church 1993),word (Dagan, Church, and Gale 1993; Fung and Church 1994; Fung and McKeown1994), collocation (Smadja 1992), and specially segmented (Kupiec 1993) levels.
How-ever, the identification of subsentential, nested, phrasal translations within the paralleltexts remains a nontrivial problem, due to the added complexity of dealing with con-stituent structure.
Manual phrasal matching is feasible only for small corpora, eitherfor toy-prototype t sting or for narrowly restricted applications.Automatic approaches to identification of subsentential translation units havelargely followed what we might call a "parse-parse-match" procedure.
Each half ofthe parallel corpus is first parsed individually using a monolingual grammar.
Subse-quently, the constituents of each sentence-pair a e matched according to some heuristicprocedure.
A number of recent proposals can be cast in this framework (Sadler andVendelmans 1990; Kaji, Kida, and Morimoto 1992; Matsumoto, Ishimoto, and Utsuro1993; Cranias, Papageorgiou, and Peperidis 1994; Grishman 1994).The parse-parse-match procedure is susceptible to three weaknesses:Appropriate, robust, monolingual grammars may not be available.
Thiscondition is particularly relevant for many non-Western Europeanlanguages uch as Chinese.
A grammar for this purpose must be robustsince it must still identify constituents for the subsequent matchingprocess even for unanticipated or ill-formed input sentences.396Wu Bilingual ParsingThe grammars may be incompatible across languages.
The best-matchingconstituent types between the two languages may not include the samecore arguments.
While grammatical differences can make this problemunavoidable, there is often a degree of arbitrariness in a grammar'schosen set of syntactic ategories, particularly if the grammar is designedto be robust.
The mismatch can be exacerbated when the monolingualgrammars are designed independently, or under different heoreticalconsiderations.Selection between multiple possible arrangements may be arbitrary.
By an"arrangement" between any given pair of sentences from the parallelcorpus, we mean a set of matchings between the constituents of thesentences.
The problem is that in some cases, a constituent in onesentence may have several potential matches in the other, and thematching heuristic may be unable to discriminate between the options.In the sentence pair of Figure 4, for example, both Security Bureau andpolice station are potential lexical matches to ~ j .
To choose the bestset of matchings, an optimization over some measure of overlap betweenthe structural analysis of the two sentences i needed.
Previousapproaches to phrasal matching employ arbitrary heuristic functions on,say, the number of matched subconstituents.Our method attacks the weaknesses of the parse-parse-match procedure by us-ing (1) only a translation lexicon with no language-specific grammar, (2) a bilingualrather than monolingual formalism, and (3) a probabilistic formulation for resolvingthe choice between candidate arrangements.
The approach differs in its single-stageoperation that simultaneously chooses the constituents of each sentence and the match-ings between them.The raw phrasal translations suggested by the parse output were then filtered toremove those pairs containing more than 50% singletons, since such pairs are likely tobe poor translation examples.
Examples that occurred more than once in the corpuswere also filtered out, since repetitive sequences in our corpus tend to be nongram-matical markup.
This yielded approximately 2,800 filtered phrasal translations, omeexamples of which are shown in Figure 12.
A random sample of the phrasal translationpairs was then drawn, giving a precision estimate of 81.5%.Although this already represents a useful evel of accuracy, it does not in our opin-ion reflect he full potential of the formalism.
Inspection revealed that performance wasgreatly hampered by our noisy translation lexicon, which was automatically earned;it could be manually post-edited to reduce errors.
Commercial on-line translation lex-icons could also be employed if available.
Higher precision could be also achievedwithout great effort by engineering a small number of broad nonterminal categories.This would reduce errors for known idiosyncratic patterns, at the cost of manual rulebuilding.The automatically extracted phrasal translation examples are especially usefulwhere the phrases in the two languages are not compositionally derivable solely fromobvious word translations.
An example is \[have acquired/?
/ -~\ ]  new/~J~ ski l ls/~~j~\] in Figure 11.
The same principle applies to nested structures also, such as (\[ ~ /~I who/ ,~ \] \[ have acquired/?
/~\ ]  new/~J~ sk i l l s /~  \]), on up to the sentencelevel.397Computational Linguistics Volume 23, Number 31% in real 1%~\]~Would you ~_~;an acceptable starting point for this new policy ~ ~ I J ~ ~are about 3 .5  million pk~-350~born in Hong ~ ~ ~for Hong ~have the right to decide our ~ J~m~J~in what way the Government would increase ~( J~{~t~}J l l~~@;~their job opportunities ; andlast month _L~ J~never to say " never " ~-~"~"reserves and surpluses ~\ ]~\ [ I~ ,starting point for this new policy ~ _ ~ ~there will be many practical difficulties in terms \ ]~@~-~I~,~tof implementationyear ended 3 1 March 1 9 9 1 ~_~Ph~J~ --n u -Figure 12Examples of extracted phrasal translations.8.2 Word AlignmentUnder the ITG model, word alignment becomes imply the special case of phrasalalignment at the parse tree leaves.
This gives us an interesting alternative perspective,from the standpoint ofalgorithms that match the words between parallel sentences.
Bythemselves, word alignments are of little use, but they provide potential anchor pointsfor other applications, or for subsequent learning stages to acquire more interestingstructures.Word alignment is difficult because correct matchings are not usually linearlyordered, i.e., there are crossings.
Without some additional constraints, any word po-sition in the source sentence can be matched to any position in the target sentence,an assumption that leads to high error rates.
More sophisticated word alignment al-gorithms therefore attempt to model the intuition that proximate constituents in closerelationships in one language remain proximate in the other.
The later IBM models areformulated to prefer collocations (Brown et al 1993).
In the case of word_align (Dagan,Church, and Gale 1993; Dagan and Church 1994), a penalty is imposed according tothe deviation from an ideal matching, as constructed by linear interpolation?From this point of view, the proposed technique is a word alignment method thatimposes a more realistic distortion penalty.
The tree structure reflects the assumptionthat crossings hould not be penalized as long as they are consistent with constituentstructure.
Figure 7 gives theoretical upper bounds on the matching flexibility as thelengths of the sequences increase, where the constituent s ructure constraints are re-flected by high flexibility up to length-4 sequences and a rapid drop-off thereafter.
Inother words, ITGs appeal to a language universals hypothesis, that the core argumentsof frames, which exhibit great ordering variation between languages, are relativelyfew and surface in syntactic proximity.
Of course, this assumption over-simplistically4 Direct comparison with word_align should be avoided, however, since it is intended towork on corporawhose sentences are not aligned.398Wu Bilingual Parsingblends syntactic and semantic notions.
That semantic frames for different languagesshare common core arguments i more plausible than that syntactic frames do.
In ef-fect we are relying on the tendency of syntactic arguments to correlate closely withsemantics.
If in particular cases this assumption does not hold, however, the damageis not too great--the model will simply drop the offending word matchings (droppingas few as possible).In experiments with the minimal bracketing transduction grammar, the large ma-jority of errors in word alignment were caused by two outside factors.
First, wordmatchings can be overlooked simply due to deficiencies in our translation lexicon.
Thisaccounted for approximately 42% of the errors.
Second, sentences containing nonliteraltranslations obviously cannot be aligned down to the word level.
This accounted foranother approximate 50% of the errors.
Excluding these two types of errors, accuracyon word alignment was 96.3%.
In other words, the tree structure constraint is strongenough to prevent most false matches, but almost never inhibits correct word matcheswhen they exist.9.
Bi l ingual Constraint Transfer9.1 Mono l ingua l  Parse TreesA parse may be available for one of the languages, especially for well-studied lan-guages such as English.
Since this eliminates all degrees of freedom in the Englishsentence structure, the parse of the Chinese sentence must conform with that givenfor the English.
Knowledge of English bracketing is thus used to help parse the Chi-nese sentence; this method facilitates a kind of transfer of grammatical expertise inone language toward bootstrapping grammar acquisition in another.A parsing algorithm for this case can be implemented very efficiently.
Note thatthe English parse tree already determines the split point S for breaking e0.
T into twoconstituent subtrees deriving e0..s and eS..T respectively, as well as the nonterminallabels j and k for each subtree.
The same then applies recursively to each subtree.We indicate this by turning S, j, and k into deterministic functions on the Englishconstituents, writing Sst, jst, and kst to denote the split point and the subtree labels forany constituent es..t. The following simplifications can then be made to the parsingalgorithm:.
RecursionFor all English constituents es, t and all i, u, v such that ~ Ki<N 0<~<~<V /6~)uv(i ) --  max  ai_r, k., 6s st,, u(jst) 6s~,,cu,v(kst) (19) u<U<v ust stj , .
, ,V~uv(i ) = argmax6s,&t,u,u(jst) 6s,,t,U,v(kst) (20)u<UKv6~uv(i ) = max ai_(j~tka } 6s,S,,,U,v(jst ) 6s,t,t,u,U(kst) (21)u<U<vv (} (i~ s tuv ,  , = argmax 6s, S , t ,U ,v ( j s t  ) 6S , t , t ,u ,U(  kst  ) (22)u<UKv3.
Reconstruct ion{(s,S~t,u,v~\](f(q))) i  Oq(?
(q)) = \[\]L~FT(q) = (s, Gt, v~)(e(q)),v) if Oq(e(q)) = (> (23)399Computational Linguistics Volume 23, Number 3(Sst, t,v~\](g(q)),v) if Oq(g(q)) = \[\] RIGHT(q) (24) (Sst, t,u,v~ )(e(q))) if Oq(f(q)) = 0g(LEFT(q)) = jst (25)~(RIGHT(q)) = kst (26)The time complexity for this constrained version of the algorithm drops fromO(NBT3V 3) to O(TV3).9.2 Partial Parse TreesA more realistic in-between scenario ccurs when partial parse information is availablefor one or both of the languages.
Special cases of particular interest include applicationswhere bracketing or word alignment constraints may be derived from external sourcesbeforehand.
For example, abroad-coverage English bracketer may be available.
If suchconstraints are reliable, it would be wasteful to ignore them.A straightforward extension to the original algorithm inhibits hypotheses thatare inconsistent with given constraints.
Any entries in the dynamic programming ta-ble corresponding to illegal subhypotheses--i.e., those that would violate the givenbracket-nesting or word alignment conditions--are preassigned negative infinity val-ues during initialization indicating impossibility.
During the recursion phase, computa-tion of these entries is skipped.
Since their probabilities remain impossible throughout,the illegal subhypotheses will never participate in any ML bibracketing.
The runningtime reduction in this case depends heavily on the domain constraints.We have found this strategy to be useful for incorporating punctuation constraints.Certain punctuation characters give constituency indications with high reliability; "per-fect separators" include colons and Chinese full stops, while "perfect delimiters" in-clude parentheses and quotation marks.10.
Unrestricted-Form GrammarsIt is possible to construct a parser that accepts unrestricted-form, rather than normal-form, grammars.
In this case an Earley-style scheme (Earley 1970), employing an activechart, can be used.
The time complexity remains the same as the normal-form case.We have found this to be useful in practice.
For bracketing rammars of the typeconsidered in this paper, there is no advantage.
However, for more complex, linguisti-cally structured grammars, the more flexible parser does not require the unreasonablenumbers of productions that can easily arise from normal-form requirements.
For mostgrammars, we have found performance to be comparable or faster than the normal-form parser.11.
Conc lus ionThe twin concepts of bilingual language modeling and bilingual parsing have beenproposed.
We have introduced a new formalism, the inversion transduction grammar,and surveyed a variety of its applications to extracting linguistic information fromparallel corpora.
Its amenability to stochastic formulation, useful flexibility with leakyand minimal grammars, and tractability for practical applications are desirable proper-ties.
Various tasks such as segmentation, word alignment, and bracket annotation arenaturally incorporated as subproblems, and a high degree of compatibility with con-ventional monolingual methods is retained.
In conjunction with automatic proceduresfor learning word translation lexicons, SITGs bring relatively underexploited bilingual400Wu Bilingual Parsingcorrelations to bear on the task of extracting linguistic information for languages lessstudied than English.We are currently pursuing several directions.
We are developing an iterative train-ing method based on expectation-maximization for estimating the probabilities fromparallel training corpora.
Also, in contrast o the applications discussed here, whichdeal with analysis and annotation of parallel corpora, we are working on incorporatingthe SITG model directly into our run-time translation architecture.
The initial resultsindicate excellent performance gains.AcknowledgmentsI would like to thank Xuanyin Xia, EvaWai-Man Fong, Pascale Fung, and DerickWood, as well as an anonymous reviewerwhose comments were of great value.ReferencesAho, Alfred V. and Jeffrey D. Ullman.
1969a.Properties of syntax directed translations.Journal of Computer and System Sciences,3(3):319-334.Aho, Alfred V. and Jeffrey D. Ullman.1969b.
Syntax directed translations andthe pushdown assembler.
Journal ofComputer and System Sciences, 3(1):37-56.Aho, Alfred V. and Jeffrey D. Ullman.
1972.The Theory of Parsing, Translation, andCompiling.
Prentice Hall, EnglewoodCliffs, NJ.Black, Ezra, Roger Garside, and GeoffreyLeech, editors.
1993.
Statistically-DrivenComputer Grammars of English: TheIBM~Lancaster Approach.
Editions Rodopi,Amsterdam.Brown, Peter F., John Cocke, StephenA.
DellaPietra, Vincent J. DellaPietra,Frederick Jelinek, John D. Lafferty, RobertL.
Mercer, and Paul S. Roossin.
1990.
Astatistical approach to machinetranslation.
Computational Linguistics,16(2):29-85.Brown, Peter F., Stephen A. DellaPietra,Vincent J. DellaPietra, and RobertL.
Mercer.
1993.
The mathematics ofstatistical machine translation: Parameterestimation.
Computational Linguistics,19(2):263-311.Brown, Peter F., Jennifer C. Lai, and RobertL.
Mercer.
1991.
Aligning sentences inparallel corpora.
In Proceedings ofthe 29thAnnual Meeting, pages 169-176, Berkeley,CA.
Association for ComputationalLinguistics.Catizone, Roberta, Graham Russell, andSusan Warwick.
1989.
Derivingtranslation data from bilingual texts.
InProceedings ofthe First Lexical AcquisitionWorkshop, Detroit, MI.Chang, Chao-Huang and Cheng-Der Chen.1993.
HMM-based part-or-speech taggingfor Chinese corpora.
In Proceedings oftheWorkshop on Very Large Corpora, pages40-47, Columbus, OH, June.Chen, Stanley F. 1993.
Aligning sentences inbilingual corpora using lexicalinformation.
In Proceedings ofthe 31stAnnual Meeting, pages 9-16, Columbus,OH.
Association for ComputationalLinguistics.Chiang, Tung-Hui, Jing-Shin Chang,Ming-Yu Lin, and Keh-Yih Su.
1992.Statistical models for word segmentationand unknown resolution.
In Proceedings ofROCLING-92, pages 121-146.Church, Kenneth W. 1993.
Char-align: Aprogram for aligning parallel texts at thecharacter level.
In Proceedings ofthe 31stAnnual Meeting, pages 1-8, Columbus,OH.
Association for ComputationalLinguistics.Cranias, Lambros, Harris Papageorgiou, andStelios Peperidis.
1994.
A matchingtechnique in example-based machinetranslation.
In Proceedings ofthe Fifteen thInternational Conference on ComputationalLinguistics, pages 100-104, Kyoto.Dagan, Ido and Kenneth W. Church.
1994.Termight: Identifying and translatingtechnical terminology.
In Proceedings oftheFourth Conference on Applied NaturalLanguage Processing, pages 34-40,Stuttgart, October.Dagan, Ido, Kenneth W. Church, andWilliam A. Gale.
1993.
Robust bilingualword alignment for machine aidedtranslation.
In Proceedings ofthe Workshopon Very Large Corpora, pages 1-8,Columbus, OH, June.Earley, Jay.
1970.
An efficient context-freeparsing algorithm.
Communications of theAssociation for Computing Machinery,13(2):94-102.Fung, Pascale and Kenneth W. Church.1994.
K-vec: A new approach for aligningparallel texts.
In Proceedings ofthe FifteenthInternational conference on ComputationalLinguistics, pages 1096-1102, Kyoto.Fung, Pascale and Kathleen McKeown.1994.
Aligning noisy parallel corpora401Computational Linguistics Volume 23, Number 3across language groups: Word pairfeature matching by dynamic timewarping.
In AMTA-94, Association forMachine Translation i  the Americas, pages81-88, Columbia, MD, October.Fung, Pascale and Dekai Wu.
1994.Statistical augmentation f a Chinesemachine-readable dictionary.
InProceedings ofthe Second Annual Workshopon Very Large Corpora, pages 69-85, Kyoto,August.Gale, William A. and Kenneth W. Church.1991.
A program for aligning sentences inbilingual corpora.
In Proceedings ofthe 29thAnnual Meeting, pages 177-184, Berkeley,CA.
Association for ComputationalLinguistics.Gale, William A., Kenneth W. Church, andDavid Yarowsky.
1992.
Using bilingualmaterials to develop word sensedisambiguation methods.
In TMI-92,Proceedings ofthe Fourth InternationalConference on Theoretical nd MethodologicalIssues in Machine Translation, pages101-112, Montreal.Gazdar, Gerald and Christopher S. Mellish.1989.
Natural Language Processing in LISP:An Introduction to Computational Linguistics.Addison-Wesley, Reading, MA.Grishman, Ralph.
1994.
Iterative alignmentof syntactic structures for a bilingualcorpus.
In Proceedings ofthe Second AnnualWorkshop on Very Large Corpora, pages57--68, Kyoto, August.Kaji, Hiroyuki, Yuuko Kida, and YasutsuguMorimoto.
1992.
Learning translationtemplates from bilingual text.
InProceedings ofthe Fourteenth InternationalConference on Computational Linguistics,pages 672-678, Nantes.Kaplan, Ronald M. and Martin Kay.
1994.Regular models of phonological rulesystems.
Computational Linguistics,20(3):331-378.Kasami, T. 1965.
An efficient recognitionand syntax analysis algorithm forcontext-free languages.
Technical ReportAFCRL-65-758, Air Force CambridgeResearch Laboratory, Bedford, MA.Kay, Martin and M. ROscheisen.
1988.Text-translation alignment.
TechnicalReport P90-00143, Xerox Palo AltoResearch Center.Koskenniemi, Kimmo.
1983.
Two-levelmorphology: A general computationalmodel for word-form recognition andproduction.
Technical Report 11,Department of General Linguistics,University of Helsinki.Kupiec, Julian.
1993.
An algorithm forfinding noun phrase correspondences inbilingual corpora.
In Proceedings ofthe 31stAnnual Meeting, pages 17-22, Columbus,OH.
Association for ComputationalLinguistics.Laporte, Eric.
1996.
Context-free parsingwith finite-state transducers.
In StringProcessing Colloquium, Recife, Brazil.Lewis, P. M. and R. E. Stearns.
1968.Syntax-directed transduction.
Journal of theAssociation for Computing Machinery,15:465-488.Lin, Yi-Chung, Tung-Hui Chiang, andKeh-Yih Su.
1992. discrimination orientedprobabilistic tagging.
In Proceedings ofROCLING-92, pages 85-96.Lin, Ming-Yu, Tung-Hui Chiang, andKeh-Yih Su.
1993.
A preliminary study onunknown word problem in chinese wordsegmentation.
I  Proceedings ofROCLING-93, pages 119-141.Magerman, David M. and MitchellP.
Marcus.
1990.
Parsing a naturallanguage using mutual informationstatistics.
In Proceedings ofAAAI-90, EighthNational Conference on Artificial Intelligence,pages 984-989.Matsumoto, Yuji, Hiroyuki Ishimoto, andTakehito Utsuro.
1993.
Structuralmatching of parallel texts.
In Proceedings ofthe 31st Annual Meeting, pages 23-30,Columbus, OH.
Association forComputational Linguistics.Nagao, Makoto.
1984.
A framework of amechanical translation between Japaneseand English by analogy principle.
InAlick Elithorn and Ranan Banerji, editors,Artifiical and Human Intelligence: EditedReview Papers Presented at the InternationalNATO Symposium on Artificial and HumanIntelligence.
North-Holland, Amsterdam,pages 173-180.Pereira, Fernando.
1991.
Finite-stateapproximation of phrase structuregrammars.
In Proceedings ofthe 29th AnnualMeeting, Berkeley, CA.
Association forComputational Linguistics.Pereira, Fernando and Yves Schabes.
1992.Inside-outside r estimation from partiallybracketed corpora.
In Proceedings ofthe30th Annual Meeting, pages 128-135,Newark, DE.
Association forComputational Linguistics.Roche, Emmanuel.
1994.
Two parsingalgorithms by means of finite-statetransducers.
In Proceedings ofthe FifteenthInternational Conference on ComputationalLinguistics, Kyoto.Sadler, Victor and Ronald Vendelmans.1990.
Pilot implementation f a bilingualknowledge bank.
In Proceedings oftheThirteenth International Conference on402Wu Bilingual ParsingComputational Linguistics, pages 449-451,Helsinki.Savitch, Walter J.
1982.
Abstract Machines andGrammars.
Little, Brown, Boston, MA.Smadja, Frank A.
1992.
How to compile abilingual collocational lexiconautomatically.
In AAAI-92 Workshop onStatistically-Based NLP Techniques, pages65-71, San Jose, CA, July.Sproat, Richard, Chilin Shih, William Gale,and Nancy Chang.
1994.
A stochasticword segmentation algorithm for aMandarin text-to-speech system.
InProceedings ofthe 32nd Annual Meeting,pages 66-72, Las Cruces, NM, June.Association for ComputationalLinguistics.Viterbi, Andrew J.
1967.
Error bounds forconvolutional codes and anasymptotically optimal decodingalgorithm.
IEEE Transactions on InformationTheory, 13:260-269.Wu, Dekai.
1994.
Aligning a parallelEnglish-Chinese corpus statistically withlexical criteria.
In Proceedings ofthe 32ndAnnual Meeting, pages 80-87, Las Cruces,NM, June.
Association for ComputationalLinguistics.Wu, Dekai.
1995.
An algorithm forsimultaneously bracketing parallel textsby aligning words.
In Proceedings ofthe33rd Annual Meeting, pages 244-251,Cambridge, MA, June.
Association forComputational Linguistics.Wu, Dekai and Pascale Fung.
1994.Improving Chinese tokenization withlinguistic filters on statistical lexicalacquisition.
In Proceedings ofthe FourthConference on Applied Natural LanguageProcessing, pages 180-181, Stuttgart,October.Wu, Dekai and Xuanyin Xia.
1994.
Learningan English-Chinese l xicon from aparallel corpus.
In AMTA-94, Associationfor Machine Translation i the Americas,pages 206-213, Columbia, MD, October.Wu, Zimin and Gwyneth Tseng.
1993.Chinese text segmentation for textretrieval: Achievements and problems.Journal of The American Society forInformation Sciences, 44(9):532-542.Younger, David H. 1967.
Recognition andparsing of context-free languages in timen 3.
Information and Control, 10(2):189-208.403
