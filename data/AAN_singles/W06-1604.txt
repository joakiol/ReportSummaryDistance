Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 27?34,Sydney, July 2006. c?2006 Association for Computational LinguisticsDetecting Parser Errors Using Web-based Semantic FiltersAlexander Yates Stefan SchoenmackersUniversity of WashingtonComputer Science and EngineeringBox 352350Seattle, WA 98195-2350{ayates, stef, etzioni} @cs.washington.eduOren EtzioniAbstractNLP systems for tasks such as questionanswering and information extraction typ-ically rely on statistical parsers.
But the ef-ficacy of such parsers can be surprisinglylow, particularly for sentences drawn fromheterogeneous corpora such as the Web.We have observed that incorrect parses of-ten result in wildly implausible semanticinterpretations of sentences, which can bedetected automatically using semantic in-formation obtained from the Web.Based on this observation, we introduceWeb-based semantic filtering?a novel,domain-independent method for automat-ically detecting and discarding incorrectparses.
We measure the effectiveness ofour filtering system, called WOODWARD,on two test collections.
On a set of TRECquestions, it reduces error by 67%.
Ona set of more complex Penn Treebanksentences, the reduction in error rate was20%.1 IntroductionSemantic processing of text in applications suchas question answering or information extractionfrequently relies on statistical parsers.
Unfortu-nately, the efficacy of state-of-the-art parsers canbe disappointingly low.
For example, we foundthat the Collins parser correctly parsed just 42%of the list and factoid questions from TREC 2004(that is, 42% of the parses had 100% precision and100% recall on labeled constituents).
Similarly,this parser produced 45% correct parses on a sub-set of 100 sentences from section 23 of the PennTreebank.Although statistical parsers continue to improvetheir efficacy over time, progress is slow, par-ticularly for Web applications where training theparsers on a ?representative?
corpus of hand-tagged sentences is not an option.
Because of theheterogeneous nature of text on the Web, such acorpus would be exceedingly difficult to generate.In response, this paper investigates the possibil-ity of detecting parser errors by using semantic in-formation obtained from the Web.
Our fundamen-tal hypothesis is that incorrect parses often resultin wildly implausible semantic interpretations ofsentences, which can be detected automatically incertain circumstances.
Consider, for example, thefollowing sentence from the Wall Street Journal:?That compares with per-share earnings from con-tinuing operations of 69 cents.?
The Collins parseryields a parse that attaches ?of 69 cents?
to ?op-erations,?
rather than ?earnings.?
By computingthe mutual information between ?operations?
and?cents?
on the Web, we can detect that this attach-ment is unlikely to be correct.Our WOODWARD system detects parser errorsas follows.
First, it maps the tree produced by aparser to a relational conjunction (RC), a logic-based representation language that we describe inSection 2.1.
Second, WOODWARD employs fourdistinct methods for analyzing whether a conjunctin the RC is likely to be ?reasonable?
as describedin Section 2.Our approach makes several assumptions.
First,if the sentence is absurd to begin with, then a cor-rect parse could be deemed incorrect.
Second, werequire a corpus whose content overlaps at least inpart with the content of the sentences to be parsed.Otherwise, much of our semantic analysis is im-possible.In applications such as Web-based question an-swering, these assumptions are quite natural.
The27questions are about topics that are covered exten-sively on the Web, and we can assume that mostquestions link verbs to nouns in reasonable com-binations.
Likewise, when using parsing for infor-mation extraction, we would expect our assump-tions to hold as well.Our contributions are as follows:1.
We introduce Web-based semantic filtering?a novel, domain-independent method for de-tecting and discarding incorrect parses.2.
We describe four techniques for analyzingrelational conjuncts using semantic informa-tion obtained from the Web, and assess theirefficacy both separately and in combination.3.
We find that WOODWARD can filter goodparses from bad on TREC 2004 questions fora reduction of 67% in error rate.
On a harderset of sentences from the Penn Treebank, thereduction in error rate is 20%.The remainder of this paper is organized as fol-lows.
We give an overview of related work in Sec-tion 1.1.
Section 2 describes semantic filtering, in-cluding our RC representation and the four Web-based filters that constitute the WOODWARD sys-tem.
Section 3 presents our experiments and re-sults, and section 4 concludes and gives ideas forfuture work.1.1 Related WorkThe problem of detecting parse errors is most sim-ilar to the idea of parse reranking.
Collins (2000)describes statistical techniques for reranking alter-native parses for a sentence.
Implicitly, a rerank-ing method detects parser errors, in that if thereranking method picks a new parse over the orig-inal one, it is classifying the original one as lesslikely to be correct.
Collins uses syntactic and lex-ical features and trains on the Penn Treebank; incontrast, WOODWARD uses semantic features de-rived from the web.
See section 3 for a comparisonof our results with Collins?.Several systems produce a semantic interpreta-tion of a sentence on top of a parser.
For example,Bos et al (2004) build semantic representationsfrom the parse derivations of a CCG parser, andthe English Resource Grammar (ERG) (Toutanovaet al, 2005) provides a semantic representation us-ing minimal recursion semantics.
Toutanova et alalso include semantic features in their parse se-lection mechanism, although it is mostly syntax-driven.
The ERG is a hand-built grammar and thusdoes not have the same coverage as the grammarwe use.
We also use the semantic interpretationsin a novel way, checking them against semanticinformation on the Web to decide if they are plau-sible.NLP literature is replete with examples of sys-tems that produce semantic interpretations anduse semantics to improve understanding.
Sev-eral systems in the 1970s and 1980s used hand-built augmented transition networks or semanticnetworks to prune bad semantic interpretations.More recently, people have tried incorporatinglarge lexical and semantic resources like WordNet,FrameNet, and PropBank into the disambiguationprocess.
Allen (1995) provides an overview ofsome of this work and contains many references.Our work focuses on using statistical techniquesover large corpora, reducing the need for hand-built resources and making the system more robustto changes in domain.Numerous systems, including Question-Answering systems like MULDER (Kwok etal., 2001), PiQASso (Attardi et al, 2001), andMoldovan et al?s QA system (2003), use parsingtechnology as a key component in their analysisof sentences.
In part to overcome incorrect parses,Moldovan et al?s QA system requires a complexset of relaxation techniques.
These systemswould greatly benefit from knowing when parsesare correct or incorrect.
Our system is the firstto suggest using the output of a QA system toclassify the input parse as good or bad.Several researchers have used pointwise mu-tual information (PMI) over the Web to help makesyntactic and semantic judgments in NLP tasks.Volk (2001) uses PMI to resolve preposition at-tachments in German.
Lapata and Keller (2005)use web counts to resolve preposition attachments,compound noun interpretation, and noun count-ability detection, among other things.
And Mark-ert et al (2003) use PMI to resolve certain types ofanaphora.
We use PMI as just one of several tech-niques for acquiring information from the Web.2 Semantic FilteringThis section describes semantic filtering as imple-mented in the WOODWARD system.
WOODWARDconsists of two components: a semantic interpreterthat takes a parse tree and converts it to a conjunc-tion of first-order predicates, and a sequence offour increasingly sophisticated methods that checksemantic plausibility of conjuncts on the Web.
Be-low, we describe each component in turn.281.
What(NP1) ?
are(VP1, NP1, NP2) ?
states(NP2) ?
producing(VP2, NP2, NP3) ?
oil(NP3) ?
in(PP1, NP2, U.S.)2.
What(NP1) ?
states(NP2) ?
producing(VP1, NP3, NP2, NP1) ?
oil(NP3) ?
in(PP1, NP2, U.S.)Figure 2: Example relational conjunctions.
The first RC is the correct one for the sentence ?What are oil producingstates in the U.S.??
The second is the RC derived from the Collins parse in Figure 1.
Differences between the two RCsappear in bold.	 		   Figure 1: An incorrect Collins Parse of a TREC ques-tion.
The parser treats ?producing?
as the main verb inthe clause, rather than ?are?.2.1 Semantic InterpreterThe semantic interpreter aims to make explicit therelations that a sentence introduces, and the argu-ments to each of those relations.
More specifically,the interpreter identifies the main verb relations,preposition relations, and semantic type relationsin a sentence; identifies the number of argumentsto each relation; and ensures that for every ar-gument that two relations share in the sentence,they share a variable in the logical representation.Given a sentence and a Penn-Treebank-style parseof that sentence, the interpreter outputs a conjunc-tion of First-Order Logic predicates.
We call thisrepresentation a relational conjunction (RC).
Eachrelation in an RC consists of a relation name anda tuple of variables and string constants represent-ing the arguments of the relation.
As an example,Figure 1 contains a sentence taken from the TREC2003 corpus, parsed by the Collins parser.
Fig-ure 2 shows the correct RC for this sentence andthe RC derived automatically from the incorrectparse.Due to space constraints, we omit details aboutthe algorithm for converting a parse into an RC,but Moldovan et al (2003) describe a method sim-ilar to ours.2.2 Semantic FiltersGiven the RC representation of a parsed sentenceas supplied by the Semantic Interpreter, we test theparse using four web-based methods.
Fundamen-tally, the methods all share the underlying princi-ple that some form of co-occurrence of terms inthe vast Web corpus can help decide whether aproposed relationship is semantically plausible.Traditional statistical parsers also use co-occurrence of lexical heads as features for makingparse decisions.
We expand on this idea in twoways: first, we use a corpus several orders of mag-nitude larger than the tagged corpora traditionallyused to train statistical parses, so that the funda-mental problem of data sparseness is ameliorated.Second, we search for targeted patterns of wordsto help judge specific properties, like the numberof complements to a verb.
We now describe eachof our techniques in more detail.2.3 A PMI-Based FilterA number of authors have demonstrated importantways in which search engines can be used to un-cover semantic relationships, especially Turney?snotion of pointwise mutual information (PMI)based on search-engine hits counts (Turney, 2001).WOODWARD?s PMI-Based Filter (PBF) uses PMIscores as features in a learned filter for predicates.Following Turney, we use the formula below forthe PMI between two terms t1 and t2:PMI(t1, t2) = log( P (t1 ?
t2)P (t1)P (t2))(1)We use PMI scores to judge the semantic plau-sibility of an RC conjunct as follows.
We con-struct a number of different phrases, which we calldiscriminator phrases, from the name of the rela-tion and the head words of each argument.
Forexample, the prepositional attachment ?operationsof 65 cents?
would yield phrases like ?operationsof?
and ?operations of * cents?.
(The ?*?
char-acter is a wildcard in the Google interface; it canmatch any single word.)
We then collect hitcountsfor each discriminator phrase, as well as for therelation name and each argument head word, andcompute a PMI score for each phrase, using thephrase?s hitcount as the numerator in Equation 1.29Given a set of such PMI scores for a single rela-tion, we apply a learned classifier to decide if thePMI scores justify calling the relation implausible.This classifier (as well as all of our other ones)is trained on a set of sentences from TREC andthe Penn Treebank; our training and test sets aredescribed in more detail in section 3.
We parsedeach sentence automatically using Daniel Bikel?simplementation of the Collins parsing model,1trained on sections 2?21 of the Penn Treebank,and then applied our semantic interpreter algo-rithm to come up with a set of relations.
We la-beled each relation by hand for correctness.
Cor-rect relations are positive examples for our clas-sifier, incorrect relations are negative examples(and likewise for all of our other classifiers).
Weused the LIBSVM software package2 to learn aGaussian-kernel support vector machine modelfrom the PMI scores collected for these relations.We can then use the classifier to predict if a rela-tion is correct or not depending on the various PMIscores we have collected.Because we require different discriminatorphrases for preposition relations and verb rela-tions, we actually learn two different models.After extensive experimentation, optimizing fortraining set accuracy using leave-one-out cross-validation, we ended up using only two patternsfor verbs: ?noun verb?
(?verb noun?
for non-subjects) and ?noun * verb?
(?verb * noun?
fornon-subjects).
We use the PMI scores from theargument whose PMI values add up to the lowestvalue as the features for a verb relation, with theintuition being that the relation is correct only ifevery argument to it is valid.For prepositions, we use a larger set of patterns.Letting arg1 and arg2 denote the head words ofthe two arguments to a preposition, and lettingprep denote the preposition itself, we used the pat-terns ?arg1 prep?, ?arg1 prep * arg2?, ?arg1prep the arg2?, ?arg1 * arg2?, and, for verb at-tachments, ?arg1 it prep arg2?
and ?arg1 themprep arg2?.
These last two patterns are helpful forpreposition attachments to strictly transitive verbs.2.4 The Verb Arity Sampling TestIn our training set from the Penn Treebank, 13%of the time the Collins parser chooses too many ortoo few arguments to a verb.
In this case, checkingthe PMI between the verb and each argument in-dependently is insufficient, and there is not enough1http://www.cis.upenn.edu/?dbikel/software.html2http://www.csie.ntu.edu.tw/?cjlin/libsvm/data to find hitcounts for the verb and all of its ar-guments at once.
We therefore use a different typeof filter in order to detect these errors, which wecall the Verb Arity Sampling Test (VAST).Instead of testing a verb to see if it can take aparticular argument, we test if it can take a certainnumber of arguments.
The verb predicate produc-ing(VP1, NP3, NP2, NP1) in interpretation 2 ofFigure 2, for example, has too many arguments.To check if this predicate can actually take threenoun phrase arguments, we can construct a com-mon phrase containing the verb, with the propertythat if the verb can take three NP arguments, thephrase will often be followed by a NP in text, andvice versa.
An example of such a phrase is ?whichit is producing.?
Since ?which?
and ?it?
are socommon, this phrase will appear many times onthe Web.
Furthermore, for verbs like ?produc-ing,?
there will be very few sentences in whichthis phrase is followed by a NP (mostly temporalnoun phrases like ?next week?).
But for verbs like?give?
or ?name,?
which can accept three nounphrase arguments, there will be significantly moresentences where the phrase is followed by a NP.The VAST algorithm is built upon this obser-vation.
For a given verb phrase, VAST first countsthe number of noun phrase arguments.
The Collinsparser also marks clause arguments as being es-sential by annotating them differently.
VASTcounts these as well, and considers the sum of thenoun and clause arguments as the number of es-sential arguments.
If the verb is passive and thenumber of essential arguments is one, or if the verbis active and the number of essential argumentsis two, VAST performs no check.
We call thesestrictly transitive verb relations.
If the verb is pas-sive and there are two essential arguments, or if theverb is active and there are three, it performs theditransitive check below.
If the verb is active andthere is one essential argument, it does the intran-sitive check described below.
We call these twocases collectively nontransitive verb relations.
Inboth cases, the checks produce a single real-valuedscore, and we use a linear kernel SVM to iden-tify an appropriate threshold such that predicatesabove the threshold have the correct arity.The ditransitive check begins by queryingGoogle for two hundred documents containing thephrase ?which it verb?
or ?which they verb?.
Itdownloads each document and identifies the sen-tences containing the phrase.
It then POS-tags andNP-chunks the sentences using a maximum en-tropy tagger and chunker.
It filters out any sen-30tences for which the word ?which?
is preceded bya preposition.
Finally, if there are enough sen-tences remaining (more than ten), it counts thenumber of sentences in which the verb is directlyfollowed by a noun phrase chunk, which we call anextraction.
It then calculates the ditransitive scorefor verb v as the ratio of the number of extractionsE to the number of filtered sentences F :ditransitiveScore(v) = EF (2)The intransitive check performs a very similarset of operations.
It fetches up to two hundredsentences matching the phrases ?but it verb?
or?but they verb?, tags and chunks them, and ex-tracts noun phrases that directly follow the verb.It calculates the intransitive score for verb v usingthe number of extractions E and sentences S as:intransitiveScore(v) = 1 ?
ES (3)2.5 TextRunner FilterTextRunner is a new kind of web search engine.Its design is described in detail elsewhere (Ca-farella et al, 2006), but we utilize its capabil-ities in WOODWARD.
TextRunner provides asearch interface to a set of over a billion triplesof the form (object string, predicate string, ob-ject string) that have been extracted automaticallyfrom approximately 90 million documents to date.The search interface takes queries of the form(string1, string2, string3), and returns all tu-ples for which each of the three tuple strings con-tains the corresponding query string as a substring.TextRunner?s object strings are very similar tothe standard notion of a noun phrase chunk.
Thenotion of a predicate string, on the other hand, isloose in TextRunner; a variety of POS sequenceswill match the patterns for an extracted relation.For example, a search for tuples with a predicatecontaining the word ?with?
will yield the tuple(risks, associated with dealing with, waste wood),among thousands of others.TextRunner embodies a trade-off with the PMImethod for checking the validity of a relation.
Itsstructure provides a much more natural search forthe purpose of verifying a semantic relationship,since it has already arranged Web text into pred-icates and arguments.
It is also much faster thanquerying a search engine like Google, both be-cause we have local access to it and because com-mercial search engines tightly limit the numberof queries an application may issue per day.
Onthe other hand, the TextRunner index is at presentstill about two orders of magnitude smaller thanGoogle?s search index, due to limited hardware.The TextRunner semantic filter checks the va-lidity of an RC conjunct in a natural way: it asksTextRunner for the number of tuples that matchthe argument heads and relation name of the con-junct being checked.
Since TextRunner predicatesonly have two arguments, we break the conjunctinto trigrams and bigrams of head words, and av-erage over the hitcounts for each.
For predicateP (A1, .
.
.
, An) with n ?
2, the score becomesTextRunnerScore =1n ?
1n?i=2hits(A1, P,Ai)+ 1n(hits(A1, P, ) +n?i=2hits(, P,Ai))As with PBF, we learn a threshold for good predi-cates using the LIBSVM package.2.6 Question Answering FilterWhen parsing questions, an additional method ofdetecting incorrect parses becomes available: usea question answering (QA) system to find answers.If a QA system using the parse can find an answerto the question, then the question was probablyparsed correctly.To test this theory, we implemented alightweight, simple, and fast QA system that di-rectly mirrors the semantic interpretation.
It re-lies on TextRunner and KnowItNow (Cafarella etal., 2005) to quickly find possible answers, giventhe relational conjunction (RC) of the question.KnowItNow is a state of the art Information Ex-traction system that uses a set of domain inde-pendent patterns to efficiently find hyponyms ofa class.We formalize the process as follows: define aquestion as a set of variables Xi corresponding tonoun phrases, a set of noun type predicates Ti(Xi),and a set of relational predicates Pi(Xi1, ..., Xik)which relate one or more variables and constants.The conjunction of type and relational predicatesis precisely the RC.We define an answer as a set of values for eachvariable that satisfies all types and predicatesans(x1, ..., xn) =?iTi(xi) ?
?jPj(xj1, ..., xjk)The algorithm is as follows:1.
Compute the RC of the question sentence.312.
?i find instances of the class Ti for possiblevalues for Xi, using KnowItNow.3.
?j find instances of the relation predicatePj(xj1, ..., xjk).
We use TextRunner to ef-ficiently find objects that are related by thepredicate Pj .4.
Return all tuples that satisfy ans(x1, ..., xn)The QA semantic filter runs the Question An-swering algorithm described above.
If the numberof returned answers is above a threshold (1 in ourcase), it indicates the question has been parsed cor-rectly.
Otherwise, it indicates an incorrect parse.This differs from the TextRunner semantic filter inthat it tries to find subclasses and instances, ratherthan just argument heads.2.7 The WOODWARD FilterEach of the above semantic filters has its strengthsand weaknesses.
On our training data, TextRunnerhad the most success of any of the methods onclassifying verb relations that did not have arity er-rors.
Because of sparse data problems, however, itwas less successful than PMI on preposition rela-tions.
The QA system had the interesting propertythat when it predicted an interpretation was cor-rect, it was always right; however, when it made anegative prediction, its results were mixed.WOODWARD combines the four semantic filtersin a way that draws on each of their strengths.First, it checks if the sentence is a question thatdoes not contain prepositions.
If so, it runs theQA module, and returns true if that module does.After trying the QA module, WOODWARDchecks each predicate in turn.
If the predicateis a preposition relation, it uses PBF to classifyit.
For nontransitive verb relations, it uses VAST.For strictly transitive verb relations, it uses Text-Runner.
WOODWARD accepts the RC if every re-lation is predicted to be correct; otherwise, it re-jects it.3 ExperimentsIn our experiments we tested the ability of WOOD-WARD to detect bad parses.
Our experiments pro-ceeded as follows: we parsed a set of sentences,ran the semantic interpreter on them, and labeledeach parse and each relation in the resulting RCsfor correctness.
We then extracted all of the nec-essary information from the Web and TextRunner.We divided the sentences into a training and testset, and trained the filters on the labeled RCs fromthe training sentences.
Finally, we ran each of thefilters and WOODWARD on the test set to predictwhich parses were correct.
We report the resultsbelow, but first we describe our datasets and toolsin more detail.3.1 Datasets and ToolsBecause question-answering is a key application,we began with data from the TREC question-answering track.
We split the data into a train-ing set of 61 questions (all of the TREC 2002 andTREC 2003 questions), and a test set of 55 ques-tions (all list and factoid questions from TREC2004).
We preprocessed the questions to removeparentheticals (this affected 3 training questionsand 1 test question).
We removed 12 test questionsbecause the Collins parser did not parse them asquestions,3 and that error was too easy to detect.25 training questions had the same error, but weleft them in to provide more training data.We used the Penn Treebank as our second dataset.
Training sentences were taken from section22, and test sentences from section 23.
BecausePBF is time-consuming, we took a subset of 100sentences from each section to expedite our exper-iments.
We extracted from each section the first100 sentences that did not contain conjunctions,and for which all of the errors, if any, were con-tained in preposition and verb relations.For our parser, we used Bikel?s implementationof the Collins parsing model, trained on sections2-21 of the Penn Treebank.
We only use the top-ranked parse for each sentence.
For the TRECdata only, we first POS-tagged each question usingRatnaparkhi?s MXPOST tagger.
We judged eachof the TREC parses manually for correctness, butscored the Treebank parses automatically.3.2 Results and DiscussionOur semantic interpreter was able to produce theappropriate RC for every parsed sentence in ourdata sets, except for a few minor cases.
Two id-iomatic expressions in the WSJ caused the seman-tic interpreter to find noun phrases outside of aclause to fill gaps that were not actually there.
Andin several sentences with infinitive phrases, the se-mantic interpreter did not find the extracted sub-ject of the infinitive expression.
It turned out thatnone of these mistakes caused the filters to rejectcorrect parses, so we were satisfied that our resultsmainly reflect the performance of the filters, ratherthan the interpreter.3That is, the root node was neither SBARQ nor SQ.32Relation Type num.
correct num.
incorrect PBF acc.
VAST acc.
TextRunner acc.Nontrans.
Verb 41 35 0.54 0.66 0.52Other Verb 126 68 0.72 N/A 0.73Preposition 183 58 0.73 N/A 0.76Table 1: Accuracy of the filters on three relation types in the TREC 2004 questions and WSJ data.Baseline WOODWARDsents.
parser eff.
filter prec.
filter rec.
F1 filter prec.
filter rec.
F1 red.
err.trec 43 54% 0.54 1.0 0.70 0.82 1.0 0.90 67%wsj 100 45% 0.45 1.0 0.62 0.58 0.88 0.70 20%Table 2: Performance of WOODWARD on different data sets.
Parser efficacy reports the percentage of sentences thatthe Collins parser parsed correctly.
See the text for a discussion of our baseline and the precision and recall metrics.
Weweight precision and recall equally in calculating F1.
Reduction in error rate (red.
err.)
reports the relative decrease inerror (error calculated as 1 ?
F1) over baseline.In Table 1 we report the accuracy of our firstthree filters on the task of predicting whether a re-lation in an RC is correct.
We break these resultsdown into three categories for the three types ofrelations we built filters for: strictly transitive verbrelations, nontransitive verb relations, and prepo-sition relations.
Since the QA filter works at thelevel of an entire RC, rather than a single relation,it does not apply here.
These results show that thetrends on the training data mostly held true: VASTwas quite effective at verb arity errors, and Text-Runner narrowly beat PBF on the remaining verberrors.
However, on our training data PBF nar-rowly beat TextRunner on preposition errors, andthe reverse was true on our test data.Our QA filter predicts whether a full parse iscorrect with an accuracy of 0.76 on the 17 TREC2004 questions that had no prepositions.
TheCollins parser achieves the same level of accuracyon these sentences, so the main benefit of the QAfilter for WOODWARD is that it never misclassi-fies an incorrect parse as a correct one, as was ob-served on the training set.
This property allowsWOODWARD to correctly predict a parse is correctwhenever it passes the QA filter.Classification accuracy is important for goodperformance, and we report it to show how effec-tive each of WOODWARD?s components is.
How-ever, it fails to capture the whole story of a filter?sperformance.
Consider a filter that simply predictsthat every sentence is incorrectly parsed: it wouldhave an overall accuracy of 55% on our WSJ cor-pus, not too much worse than WOODWARD?s clas-sification accuracy of 66% on this data.
However,such a filter would be useless because it filters outevery correctly parsed sentence.Let the filtered set be the set of sentences that afilter predicts to be correctly parsed.
The perfor-mance of a filter is better captured by two quanti-ties related to the filtered set: first, how ?pure?
thefiltered set is, or how many good parses it containscompared to bad parses; and second, how waste-ful the filter is in terms of losing good parses fromthe original set.
We measure these two quantitiesusing metrics we call filter precision and filter re-call.
Filter precision is defined as the ratio of cor-rectly parsed sentences in the filtered set to totalsentences in the filtered set.
Filter recall is definedas the ratio of correctly parsed sentences in the fil-tered set to correctly parsed sentences in the un-filtered set.
Note that these metrics are quite dif-ferent from the labeled constituent precision/recallmetrics that are typically used to measure statisti-cal parser performance.Table 2 shows our overall results for filteringparses using WOODWARD.
We compare againsta baseline model that predicts every sentence isparsed correctly.
WOODWARD outperforms thisbaseline in precision and F1 measure on both ofour data sets.Collins (2000) reports a decrease in error rateof 13% over his original parsing model (the samemodel as used in our experiments) by performinga discriminative reranking of parses.
Our WSJtest set is a subset of the set of sentences usedin Collins?
experiments, so our results are not di-rectly comparable, but we do achieve a roughlysimilar decrease in error rate (20%) when we useour filtered precision/recall metrics.
We also mea-sured the labeled constituent precision and recallof both the original test set and the filtered set, andfound a decrease in error rate of 37% according tothis metric (corresponding to a jump in F1 from90.1 to 93.8).
Note that in our case, the error is re-33duced by throwing out bad parses, rather than try-ing to fix them.
The 17% difference between thetwo decreases in error rate is probably due to thefact that WOODWARD is more likely to detect theworse parses in the original set, which contribute aproportionally larger share of error in labeled con-stituent precision/recall in the original test set.WOODWARD performs significantly better onthe TREC questions than on the Penn Treebankdata.
One major reason is that there are far moreclause adjuncts in the Treebank data, and adjuncterrors are intrinsically harder to detect.
Con-sider the Treebank sentence: ?The S&P pit stayedlocked at its 30-point trading limit as the Dow av-erage ground to its final 190.58 point loss Friday.
?The parser incorrectly attaches the clause begin-ning ?as the Dow .
.
.
?
to ?locked?, rather thanto ?stayed.?
Our current methods aim to use keywords in the clause to determine if the attachmentis correct.
However, with such clauses there is nosingle key word that can allow us to make that de-termination.
We anticipate that as the paradigmmatures we and others will design filters that canuse more of the information in the clause to helpmake these decisions.4 Conclusions and Future WorkGiven a parse of a sentence, WOODWARD con-structs a representation that identifies the key se-mantic relationships implicit in the parse.
It thenuses a set of Web-based sampling techniques tocheck whether these relationships are plausible.If any of the relationships is highly implausible,WOODWARD concludes that the parse is incorrect.WOODWARD successfully detects common errorsin the output of the Collins parser including verbarity errors as well as preposition and verb attach-ment errors.
While more extensive experimentsare clearly necessary, our results suggest that theparadigm of Web-based semantic filtering couldsubstantially improve the performance of statisti-cal parsers.In future work, we hope to further validate thisparadigm by constructing additional semantic fil-ters that detect other types of errors.
We also planto use semantic filters such as WOODWARD tobuild a large-scale corpus of automatically-parsedsentences that has higher accuracy than can beachieved today.
Such a corpus could be used tore-train a statistical parser to improve its perfor-mance.
Beyond that, we plan to embed semanticfiltering into the parser itself.
If semantic filtersbecome sufficiently accurate, they could rule outenough erroneous parses that the parser is left withjust the correct one.AcknowledgementsThis research was supported in part by NSF grantIIS-0312988, DARPA contract NBCHD030010,ONR grant N00014-02-1-0324 as well as giftsfrom Google, and carried out at the University ofWashington?s Turing Center.ReferencesJ.
Allen.
1995.
Natural Language Understand-ing.
Benjamin/Cummings Publishing, RedwoodCity, CA, 2nd edition.G.
Attardi, A. Cisternino, F. Formica, M. Simi, andA.
Tommasi.
2001.
PiQASso: Pisa Question An-swering System.
In TREC.J.
Bos, S. Clark, M. Steedman, J. R. Curran, andJ.
Hockenmaier.
2004.
Wide-coverage semanticrepresentations from a CCG parser.
In COLING.Michael J. Cafarella, Doug Downey, Stephen Soder-land, and Oren Etzioni.
2005.
KnowItNow: Fast,scalable information extraction from the web.
InHLT-EMNLP.M.
J. Cafarella, M. Banko, and O. Etzioni.
2006.
Re-lational web search.
UW Tech Report 06-04-02.M.
Collins.
2000.
Discriminative reranking for naturallanguage parsing.
In ICML, pages 175?182.C.
C. T. Kwok, O. Etzioni, and D. S. Weld.
2001.
Scal-ing question answering to the web.
In WWW.M.
Lapata and F. Keller.
2005.
Web-based models fornatural language processing.
ACM Transactions onSpeech and Language Processing, 2:1?31.K.
Markert, N. Modjeska, and M. Nissim.
2003.
Us-ing the web for nominal anaphora resolution.
InEACL Workshop on the Computational Treatment ofAnaphora.D.
Moldovan, C. Clark, S. Harabagiu, and S. Maiorano.2003.
Cogex: A logic prover for question answer-ing.
In HLT.K.
Toutanova, C. D. Manning, D. Flickinger, andS.
Oepen.
2005.
Stochastic HPSG parse disam-biguation using the Redwoods Corpus.
Journal ofLogic and Computation.P.D.
Turney.
2001.
Mining the Web for Synonyms:PMI?IR versus LSA on TOEFL.
Lecture Notes inComputer Science, 2167:491?502.M.
Volk.
2001.
Exploiting the WWW as a corpus toresolve PP attachment ambiguities.
In Corpus Lin-guistics.34
