Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 599?603,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsAutomatic Detection of Sentence FragmentsChak Yan Yeung and John LeeHalliday Centre for Intelligent Applications of Language StudiesDepartment of Linguistics and TranslationCity University of Hong Kongchak.yeung@my.cityu.edu.hkjsylee@cityu.edu.hkAbstractWe present and evaluate a method for au-tomatically detecting sentence fragmentsin English texts written by non-nativespeakers.
Our method combines syntacticparse tree patterns and parts-of-speech in-formation produced by a tagger to detectthis phenomenon.
When evaluated on acorpus of authentic learner texts, our bestmodel achieved a precision of 0.84 and arecall of 0.62, a statistically significant im-provement over baselines using non-parsefeatures, as well as a popular grammarchecker.1 IntroductionIt is challenging to detect and correct sentence-level grammatical errors because it involves au-tomatic syntactic analysis on noisy, learner sen-tences.
Indeed, none of the teams achieved any re-call for comma splices in the most recent CoNLLshared task (Ng et al., 2014).
Sentence fragmentsfared hardly better: of the thirteen teams, twoscored a recall of 0.25 for correction and anotherscored 0.2; the rest did not achieve any recall.Although parser performance degrades onlearner text (Foster, 2007), parsers can still be use-ful for identifying grammatical errors if they pro-duce consistent patterns that indicate these errors.We show that parse tree patterns, automatically de-rived from training data, significantly improve sys-tem performance on detecting sentence fragments.The rest of the paper is organized as follows.The next section defines the types of sentence frag-ments treated in this paper.
Section 3 reviews re-lated work.
Section 4 describes the features usedin our model.
Section 5 discusses the datasets andsection 6 analyzes the experiment results.
Our bestmodel significantly outperforms baselines that donot consider syntactic information and a widelyused grammar checker.2 Sentence FragmentEvery English sentence must have a main or in-dependent clause.
Most linguists require a clauseto contain a subject and a finite verb (Hunt, 1965;Polio, 1997); otherwise, it is considered a sentencefragment.
Following Bram (1995), we classifysentence fragments into the following four cate-gories:No Subject.
Fragments that lack a subject,1such as ?According to the board, is $100.
?No finite verb.
Fragments that lack a finiteverb.
These may be a nonfinite verb phrase, ora noun phrase, such as ?Mrs.
Kern in a show.
?No subject and finite verb.
Fragments lackingboth a subject and a finite verb; a typical exampleis a prepositional phrase, such as ?Up through theranks.
?Subordinate clause.
These fragments consistof a stand-alone subordinate clause; the clausetypically begins with a relative pronoun or a sub-ordinating conjunction, such as ?While they takepains to hide their assets.
?3 Related WorkUsing parse tree patterns to judge the grammati-cality of a sentence is not new.
Wong and Dras(2011) exploited probabilistic context-free gram-mar (PCFG) rules as features for native languageidentification.
In addition to production rules, Post(2011) incorporated parse fragment features com-puted from derivations of tree substitution gram-mars.
Heilman et al.
(2014) used the parse scoresand syntactic features to classify the comprehensi-bility of learner text, though they made no attemptto correct the errors.In current grammatical error correction sys-tems, parser output is used mainly to locate1Our evaluation data distinguishes between imperativesand fragments.
Our automatic classifier, however, makes nosuch attempt because it would require analysis of the contextand significant real-world knowledge.599relevant information involved in long-distancegrammatical constructions (Tetreault et al., 2010;Yoshimoto et al., 2013; Zhang and Wang, 2014).To the best of our knowledge, the only previouswork that used distinctive parse patterns to detectspecific grammatical errors was concerned withcomma splices.
Lee et al.
(2014) manually identi-fied distinctive production rules which, when usedas features in a CRF, significantly improved theprecision and recall in locating comma splices inlearner text.
Our method will similarly leverageparse tree patterns, but with the goal of detectingsentence fragment errors.
More importantly, ourapproach is fully automatic, and can thus poten-tially be broadly applied on other syntax-relatedlearner errors.Many commercial systems, such as the Cri-terion Online Writing Service (Burstein et al.,2004), Grammarly2, and WhiteSmoke3, give feed-back about sentence fragments.
To the best of ourknowledge, these systems do not explicitly con-sider parse tree patterns.
The grammar checkerembedded in Microsoft Word also gives feedbackabout sentence fragments, and will serve as one ofour baselines.Aside from the CoNLL-2014 shared task (seeSection 1), the only other reported evaluation ondetecting or correcting sentence fragments hasbeen performed on Microsoft ESL Assistant andthe NTNU Grammar Checker (Chen, 2009).
Nei-ther tool detected any of the sentence fragments inthe test set.4 Fragment DetectionWe cast the problem of sentence fragment detec-tion as a multiclass classification task.
Given asentence, the system would mark it either as false,if it is not a fragment, or as one of the four frag-ment categories described in Section 2.
Ratherthan a binary decision on whether a sentence is afragment, this categorisation provides more usefulfeedback to the learner, since each of the four frag-ment categories requires its own correction strat-egy.4.1 ModelsBaseline Models.
We trained three baseline mod-els with features that incorporate an increasingamount of information about sentence structure.2www.grammarly.com3www.whitesmoke.comThe first baseline model was trained on the wordtrigrams of the sentences, the second model onpart-of-speech unigrams, and the third on part-of-speech trigrams.
All of these features can be ob-tained without syntactic parsing.
To reduce thenumber of features, we filtered out the word tri-grams that occur less than twenty times and thePOS trigrams that occur less than a hundred timesin the training data.Parse Models.
Our approach uses parse treepatterns as features.
Although any arbitrary sub-tree structure can potentially serve as a feature, thechildren of the root of the tree tend to be mostsalient.
These nodes usually denote the syntac-tic constituents of the sentence, and so often re-veal differences between well-formed sentencesand fragments.
Consider the sentence ?While Pe-ter was a good boy.
?, shown in the parse tree inFigure 1.
The child of the root of the tree is SBAR.When the subordinating conjunction ?while?
is re-moved to yield a well-formed sentence, the chil-dren nodes change accordingly into the expectedNP and VP.
In contrast, the POS tags, used in thebaseline models, tend to remain the same.We use the label of the root and the trigrams ofits children nodes as features, similar to Sj?obergh(2005) and Lin et al.
(2011).
We also extend ourpatterns to grandchildren in some cases.
Whenanalyzing an ill-formed sentence, the parser cansometimes group words into constituents to whichthey do not belong, such as forming a VP that doesnot contain a verb.
For example, the phrase ?up thehill?
was analyzed as a VP in the fragment ?A newchallenger up the hill?
when in fact the sentence ismissing a verb.
To take into account such misanal-yses, we also include the POS tag of the first childof all NP, VP, PP, ADVP, and ADJP as features.The first child is chosen because it often exposesthe parsing error, as is the case with the preposi-tion ?up?
in the purported VP ?up the hill?
in theabove example.We trained two models for experiments: the?Parse?
model used the parser?s POS tags and the?Parse + Tag?
model used the tags produced bythe POS tagger, which was trained on local fea-tures and tends to be less affected by ill-formedsentence structures.
For example, in the sentence?Certainly was not true.
?, the word ?certainly?
wastagged as a plural noun by the parser while the tag-ger correctly identified it as an adverb.
The NPconstruction in the fragment was encoded as ?NP-600NNP?
in the ?Parse?
model and ?NP-RB?
in the?Parse + Tag?
model.
To reduce the number offeatures, we filtered out the node trigrams that oc-cur less than ten times in the training data.While/IN Peter/NNP was/VBD a/DT good/JJboy/NNFRAGSBARWhile Peter was a good boyPeter/NNP was/VBD a/DT good/JJ boy/NNSNPPeterVPwas a good boyFigure 1: The POS-tagged words and parse treesof the fragment ?While Peter was a good boy.?
andthe well-formed sentence ?Peter was a good boy.
?.5 Data5.1 Training DataWe automatically produced training data from theNew York Times portion of the AQUAINT Cor-pus of English News Text (Graff, 2002).
Similarto Foster and Andersen (2009), we artificially gen-erate fragments that correspond to the four cate-gories (Section 2) by removing different compo-nents from well-formed English sentences.
Forthe ?no subject?
category, the NP immediately un-der the topmost S was removed.
For the ?no finiteverb?
category, we removed the finite verb in theVP immediately under the topmost S. For the ?nosubject and finite verb?
category, we removed boththe NP and the finite verb in the VP immediatelyunder the topmost S. For the ?subordinate clause?category, we looked for any SBAR in the sentencethat is preceded by a comma and consists of anIN child followed by an S. The words under theSBAR are extracted as the fragment.
Using thismethod, we created a total of 60,000 fragments,with 15,000 sentences in each category.
Togetherwith the original sentences, our training data con-sists of 120,000 sentences, half of which are frag-ments.5.2 Evaluation DataFragment was among the 28 error types introducedin the CoNLL-2014 shared task (Ng et al., 2014),but the test set used in the task only contained 16such errors and is too small for our purpose.
In-stead, we evaluated our system on the NUCLEcorpus (Dahlmeier et al., 2013), which was usedas the training data in the shared task.
The errorlabel ?SFrag?
in the NUCLE corpus was used forsentence fragments in a wider sense than the fourcategories defined by Bram (1995) (see Section2).
For example, ?SFrag?
also labels sentenceswith stylistic issues, such as those beginning with?therefore?
or ?hence?, and sentences that, thoughwell-formed, should be merged with its neighbor,such as ?In Singapore, we can see that this prob-lem is occurring.
This is so as there is a huge dis-crepancy in the education levels.
?.We asked two human annotators to classify thefragments into the different categories describedin Section 2.
The kappa was 0.84.
Most ofthe disagreements involved sentences that con-tain a semi-colon which, when replaced with acomma, would become well-formed.
One anno-tator flagged these cases as fragments while theother did not, considering them to be punctua-tion errors.
Another source of disagreements waswhether a sentence should be considered an im-perative.Among the 249 sentences marked as fragments,86 were classified as one of the Bram (1995) cat-egories by at least one of the annotators.
Most ofthe fragments belong to categories ?no finite verb?and ?subordinate clause?, accounting for 43.0%and 31.4% of the cases respectively.
The cate-gories ?no subject and finite verb?
and ?no sub-ject?
both account for 12.8% of the cases.
Weleft all errors in the sentences in place so as to re-flect our models?
performance on authentic learnerdata.6 ResultsWe obtained the POS tags and parse trees of thesentences in our datasets with the Stanford POStagger (Toutanova et al., 2003) and the Stanfordparser (Manning et al., 2014).
We used the logis-tic regression implementation in scikit-learn (Pe-dregosa et al., 2011) for the maximum entropymodels in our experiments.
In addition to thethree baseline models described in Section 4.1,we computed a fourth baseline using the grammar601checker in Microsoft Word 2013 by configuringthe checker to capture ?Fragments and Run-ons?and ?Fragment - stylistic suggestions?.6.1 Fragment detectionWe first evaluated the systems?
ability to detectfragments.
The fragment categories are disre-garded in this evaluation and the system?s resultis considered correct even if its output categorydoes not match the one marked by the annota-tors.
We adopted the metric used in the CoNLL-2014 shared task, F0.5, which emphasizes preci-sion twice as much as recall because it is importantto minimize false alarms for language learners4.The results are shown in Table 1.
The ?Parse?model achieved a precision of 0.82, a recall of0.57 and an F0.5of 0.75.
Using the POS tagsproduced by the POS tagger instead of the onesproduced by the parser, the ?Parse + Tag?
modelachieved a precision of 0.84, a recall of 0.62 andan F0.5of 0.78, improving upon the results of the?Parse?
model and significantly outperforming allfour baselines5.Most of the false negatives are in the ?no fi-nite verb?
category and many of them involvefragments with subordinate clauses, such as ?Theincreased of longevity as the elderly are leadinglonger lives.?.
In order to create parse trees that fitthose of complete sentences, the parser tended tointerpret the verbs in the subordinate clauses (e.g.,?are?
in the above example) as the fragments?main verbs, causing the errors.
For false positives,the errors were caused mostly by the presence ofintroductory phrases.
The parse trees of these sen-tences usually contain a PP or an ADVP immedi-ately under the root, which is a pattern shared byfragments.
The system also flagged some impera-tive sentences as fragments.6.2 Fragment classificationFor the fragments that the system has correctlyidentified, we evaluated their classification accu-racy6.
Table 2 shows the confusion matrix of thesystem?s results.The largest source of error is the systemwrongly classifying ?no finite verb?
and ?subor-4F0.5is calculated by F0.5= (1 + 0.52) x R x P / (R + 0.52x P) for recall R and precision P.5At p ?
0.002 by McNemar?s test.6The grammar checker in Microsoft Word is excludedfrom this evaluation because it does not provide any correc-tion suggestions for fragments.System P/R/F0.5Word Trigrams 0.20/0.03/0.09POS Tags 0.56/0.33/0.47POS Trigrams 0.55/0.42/0.52MS Word 0.80/0.15/0.43Parse 0.82/0.57/0.75Parse + Tag 0.84/0.62/0.78Table 1: System precision, recall and F-measurefor fragment detection.dinate clause?
fragments as ?no subject and finiteverb?.
Most of these involve fragments that beginwith a prepositional phrase, such as ?for example?,followed by a comma.
The annotators treated theprepositional phrase as introductory phrase and fo-cused on the segment after the comma.
In con-trast, based on the parser output, the system oftentreated the entire fragment as a PP, which shouldthen belong to ?no subject and finite verb?.
It canbe argued that both interpretations are valid.
Forinstance, the fragment ?For example, apples andoranges?
can be corrected as ?For example, applesand oranges are fruits?
or, alternatively, ?I lovefruits, for example, apples and oranges?.?
ExpectedS V SV C?
SystemS [6] 4 1 1V 0 [12] 2 0SV 0 5 [2] 11C 0 0 0 [9]Table 2: The confusion matrix of the system forclassifying the detected sentence fragments intothe categories no subject (S), no finite verb (V), nosubject and finite verb (SV) and subordinate clause(C).7 ConclusionWe have presented a data-driven method for auto-matically detecting sentence fragments.
We haveshown that our method, which uses syntactic parsetree patterns and POS tagger output, significantlyimproves accuracy in detecting fragments in En-glish learner texts.AcknowledgmentsThis work was supported in part by a Strategic Re-search Grant (#7008166) from City University of602Hong Kong.ReferencesBarli Bram.
1995.
Write Well, Improving WritingSkills.
Kanisius.Jill Burstein, Martin Chodorow, and Claudia Leacock.2004.
Automated essay evaluation: The Criteriononline writing service.
AI Magazine, 25(3):27.Hao-Jan Howard Chen.
2009.
Evaluating two web-based grammar checkers-Microsoft ESL Assistantand NTNU statistical grammar checker.
Computa-tional Linguistics and Chinese Language Process-ing, 14(2):161?180.Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.2013.
Building a large annotated corpus of learnerenglish: The NUS Corpus of Learner English.
InProceedings of the Eighth Workshop on InnovativeUse of NLP for Building Educational Applications,pages 22?31.Jennifer Foster and ?istein E Andersen.
2009.
Gen-ERRate: generating errors for use in grammatical er-ror detection.
In Proceedings of the fourth workshopon innovative use of NLP for building educationalapplications, pages 82?90.
Association for Compu-tational Linguistics.Jennifer Foster.
2007.
Treebanks gone bad.
Interna-tional Journal of Document Analysis and Recogni-tion (IJDAR), 10(3-4):129?145.David Graff.
2002.
The AQUAINT corpus of Englishnews text.
Linguistic Data Consortium, Philadel-phia.Michael Heilman, Joel Tetreault, Aoife Cahill, NitinMadnani, Melissa Lopez, and Matthew Mulholland.2014.
Predicting grammaticality on an ordinal scale.In Proceedings of ACL-2014.Kellogg W Hunt.
1965.
Grammatical structures writ-ten at three grade levels.
NCTE research report no.3.John Lee, Chak Yan Yeung, and Martin Chodorow.2014.
Automatic detection of comma splices.
InProceedings of PACLIC-2014.Nay Yee Lin, Khin Mar Soe, and Ni Lar Thein.2011.
Developing a chunk-based grammar checkerfor translated English sentences.
In Proceedings ofPACLIC-2011, pages 245?254.Christopher D Manning, Mihai Surdeanu, John Bauer,Jenny Finkel, Steven J Bethard, and David Mc-Closky.
2014.
The Stanford CoreNLP natural lan-guage processing toolkit.
In Proceedings of 52ndAnnual Meeting of the Association for Computa-tional Linguistics: System Demonstrations, pages55?60.Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, ChristianHadiwinoto, Raymond Hendy Susanto, and Christo-pher Bryant.
2014.
The CoNLL-2014 shared taskon grammatical error correction.
In Proceedings ofthe Eighteenth Conference on Computational Natu-ral Language Learning: Shared Task.Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gram-fort, Vincent Michel, Bertrand Thirion, OlivierGrisel, Mathieu Blondel, Peter Prettenhofer, RonWeiss, Vincent Dubourg, et al.
2011.
Scikit-learn:Machine learning in python.
The Journal of Ma-chine Learning Research, 12:2825?2830.Charlene G Polio.
1997.
Measures of linguistic accu-racy in second language writing research.
Languagelearning, 47(1):101?143.Matt Post.
2011.
Judging grammaticality with treesubstitution grammar derivations.
In Proceedings ofthe 49th Annual Meeting of the Association for Com-putational Linguistics: Human Language Technolo-gies: short papers-Volume 2, pages 217?222.
Asso-ciation for Computational Linguistics.Jonas Sj?obergh.
2005.
Chunking: an unsupervisedmethod to find errors in text.
In Proceedings of the15th NODALIDA conference, pages 180?185.Joel Tetreault, Jennifer Foster, and Martin Chodorow.2010.
Using parse features for preposition selectionand error detection.
In Proceedings of ACL-2010,pages 353?358.
Association for Computational Lin-guistics.Kristina Toutanova, Dan Klein, Christopher D Man-ning, and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In Proceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology-Volume 1, pages 173?180.
Association for Compu-tational Linguistics.Sze-Meng Jojo Wong and Mark Dras.
2011.
Exploit-ing parse structures for native language identifica-tion.
In Proceedings of the Conference on Empiri-cal Methods in Natural Language Processing, pages1600?1610.
Association for Computational Linguis-tics.Ippei Yoshimoto, Tomoya Kose, Kensuke Mitsuzawa,Keisuke Sakaguchi, Tomoya Mizumoto, YutaHayashibe, Mamoru Komachi, and Yuji Matsumoto.2013.
NAIST at 2013 CoNLL grammatical errorcorrection shared task.
In Proceedings of the Seven-teenth Conference on Computational Natural Lan-guage Learning: Shared Task, volume 26.Longkai Zhang and Houfeng Wang.
2014.
Go climb adependency tree and correct the grammatical errors.In Proceedings of EMNLP-2014.603
