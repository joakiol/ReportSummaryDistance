Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 356?365,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsUsing Smaller Constituents Rather Than Sentencesin Active Learning for Japanese Dependency ParsingManabu SassanoYahoo Japan CorporationMidtown Tower,9-7-1 Akasaka, Minato-ku,Tokyo 107-6211, Japanmsassano@yahoo-corp.jpSadao KurohashiGraduate School of Informatics,Kyoto UniversityYoshida-honmachi, Sakyo-ku,Kyoto 606-8501, Japankuro@i.kyoto-u.ac.jpAbstractWe investigate active learning methods forJapanese dependency parsing.
We proposeactive learning methods of using partialdependency relations in a given sentencefor parsing and evaluate their effective-ness empirically.
Furthermore, we utilizesyntactic constraints of Japanese to ob-tain more labeled examples from preciouslabeled ones that annotators give.
Ex-perimental results show that our proposedmethods improve considerably the learn-ing curve of Japanese dependency parsing.In order to achieve an accuracy of over88.3%, one of our methods requires only34.4% of labeled examples as compared topassive learning.1 IntroductionReducing annotation cost is very important be-cause supervised learning approaches, which havebeen successful in natural language processing, re-quire typically a large number of labeled exam-ples.
Preparing many labeled examples is timeconsuming and labor intensive.One of most promising approaches to this is-sue is active learning.
Recently much attention hasbeen paid to it in the field of natural language pro-cessing.
Various tasks have been targeted in theresearch on active learning.
They include wordsense disambiguation, e.g., (Zhu and Hovy, 2007),POS tagging (Ringger et al, 2007), named entityrecognition (Laws and Schu?tze, 2008), word seg-mentation, e.g., (Sassano, 2002), and parsing, e.g.,(Tang et al, 2002; Hwa, 2004).It is the main purpose of this study to proposemethods of improving active learning for parsingby using a smaller constituent than a sentence asa unit that is selected at each iteration of activelearning.
Typically in active learning for parsing asentence has been considered to be a basic unit forselection.
Small constituents such as chunks havenot been used in sample selection for parsing.
Weuse Japanese dependency parsing as a target taskin this study since a simple and efficient algorithmof parsing is proposed and, to our knowledge, ac-tive learning for Japanese dependency parsing hasnever been studied.The remainder of the paper is organized as fol-lows.
Section 2 describes the basic framework ofactive learning which is employed in this research.Section 3 describes the syntactic characteristics ofJapanese and the parsing algorithm that we use.Section 4 briefly reviews previous work on activelearning for parsing and discusses several researchchallenges.
In Section 5 we describe our proposedmethods and others of active learning for Japanesedependency parsing.
Section 6 describes experi-mental evaluation and discussion.
Finally, in Sec-tion 7 we conclude this paper and point out somefuture directions.2 Active Learning2.1 Pool-based Active LearningOur base framework of active learning is based onthe algorithm of (Lewis and Gale, 1994), which iscalled pool-based active learning.
Following theirsequential sampling algorithm, we show in Fig-ure 1 the basic flow of pool-based active learning.Various methods for selecting informative exam-ples can be combined with this framework.2.2 Selection Algorithm for Large MarginClassifiersOne of the most accurate approaches to classifica-tion tasks is an approach with large margin classi-fiers.
Suppose that we are given data points {xi}such that the associated label yi will be either ?1or 1, and we have a hyperplane of some large mar-gin classifier defined by {x : f(x) = 0} where the3561.
Build an initial classifier from an initial la-beled training set.2.
While resources for labeling examples areavailable(a) Apply the current classifier to each un-labeled example(b) Find the m examples which are most in-formative for the classifier(c) Have annotators label the m examples(d) Train a new classifier on all labeled ex-amplesFigure 1: Flow of the pool-based active learningLisa-ga kare-ni ano pen-wo age-ta.Lisa-subj to him that pen-acc give-past.ID 0 1 2 3 4Head 4 4 3 4 -Figure 2: Sample sentence.
An English translationis ?Lisa gave that pen to him.
?classification function is G(x) = sign{f(x)}.
Inpool-based active learning with large margin clas-sifiers, selection of examples can be done as fol-lows:1.
Compute f(xi) over all unlabeled examplesxi in the pool.2.
Sort xi with |f(xi)| in ascending order.3.
Select top m examples.This type of selection methods with SVMs is dis-cussed in (Tong and Koller, 2000; Schohn andCohn, 2000).
They obtain excellent results on textclassification.
These selection methods are simplebut very effective.3 Japanese Parsing3.1 Syntactic UnitsA basic syntactic unit used in Japanese parsing isa bunsetsu, the concept of which was initially in-troduced by Hashimoto (1934).
We assume thatin Japanese we have a sequence of bunsetsus be-fore parsing a sentence.
A bunsetsu contains oneor more content words and zero or more functionwords.A sample sentence in Japanese is shown in Fig-ure 2.
This sentence consists of five bunsetsus:Lisa-ga, kare-ni, ano, pen-wo, and age-ta wherega, ni, and wo are postpositions and ta is a verbending for past tense.3.2 Constraints of Japanese DependencyAnalysisJapanese is a head final language and in writtenJapanese we usually hypothesize the following:?
Each bunsetsu has only one head except therightmost one.?
Dependency links between bunsetsus gofrom left to right.?
Dependencies do not cross one another.We can see that these constraints are satisfied inthe sample sentence in Figure 2.
In this paper wealso assume that the above constraints hold truewhen we discuss algorithms of Japanese parsingand active learning for it.3.3 Algorithm of Japanese DependencyParsingWe use Sassano?s algorithm (Sassano, 2004) forJapanese dependency parsing.
The reason for thisis that it is very accurate and efficient1.
Further-more, it is easy to implement.
His algorithm isone of the simplest form of shift-reduce parsersand runs in linear-time.2 Since Japanese is a headfinal language and its dependencies are projectiveas described in Section 3.2, that simplification canbe made.The basic flow of Sassano?s algorithm is shownin Figure 3, which is slightly simplified from theoriginal by Sassano (2004).
When we use this al-gorithm with a machine learning-based classifier,function Dep() in Figure 3 uses the classifier todecide whether two bunsetsus have a dependencyrelation.
In order to prepare training examples forthe trainable classifier used with his algorithm, wefirst have to convert a treebank to suitable labeledinstances by using the algorithm in Figure 4.
Note1Iwatate et al (2008) compare their proposed algorithmwith various ones that include Sassano?s, cascaded chunk-ing (Kudo and Matsumoto, 2002), and one in (McDonald etal., 2005).
Kudo and Matsumoto (2002) compare cascadedchunking with the CYK method (Kudo and Matsumoto,2000).
After considering these results, we have concludedso far that Sassano?s is a reasonable choice for our purpose.2Roughly speaking, Sassano?s is considered to be a sim-plified version, which is modified for head final languages, ofNivre?s (Nivre, 2003).
Classifiers with Nivre?s are requiredto handle multiclass prediction, while binary classifiers canwork with Sassano?s for Japanese.357Input: wi: bunsetsus in a given sentence.N : the number of bunsetsus.Output: hj : the head IDs of bunsetsus wj .Functions: Push(i, s): pushes i on the stack s.Pop(s): pops a value off the stack s.Dep(j, i, w): returns true when wj shouldmodify wi.
Otherwise returns false.procedure Analyze(w, N , h)var s: a stack for IDs of modifier bunsetsusbegin{?1 indicates no modifier candidate}Push(?1, s);Push(0, s);for i ?
1 to N ?
1 do beginj ?
Pop(s);while (j 6= ?1and ((i = N ?
1) or Dep(j, i, w)) ) dobeginhj ?
i;j ?
Pop(s)endPush(j, s);Push(i, s)endendFigure 3: Algorithm of Japanese dependency pars-ingthat the algorithm in Figure 4 does not generateevery pair of bunsetsus.34 Active Learning for ParsingMost of the methods of active learning for parsingin previous work use selection of sentences thatseem to contribute to the improvement of accuracy(Tang et al, 2002; Hwa, 2004; Baldridge and Os-borne, 2004).
Although Hwa suggests that sampleselection for parsing would be improved by select-ing finer grained constituents rather than sentences(Hwa, 2004), such methods have not been investi-gated so far.Typical methods of selecting sentences are3We show a sample set of generated examples for trainingthe classifier of the parser in Figure 3.
By using the algorithmin Figure 4, we can obtain labeled examples from the samplesentences in Figure 2: {0, 1, ?O?
}, {1, 2, ?O?
}, {2, 3, ?D?
},and {1, 3, ?O?}.
Please see Section 5.2 for the notationused here.
For example, an actual labeled instance generatedfrom {2, 3, ?D?}
will be like ?label=D, features={modifier-content-word=ano, ..., head-content-word=pen, ...}.
?Input: hi: the head IDs of bunsetsus wi.Function: Dep(j, i, w, h): returns true if hj = i.Otherwise returns false.
Also prints afeature vector with a label according to hj .procedure Generate(w, N , h)beginPush(?1, s);Push(0, s);for i ?
1 to N ?
1 do beginj ?
Pop(s);while (j 6= ?1and ((i = N ?
1) or Dep(j, i, w, h)) ) dobeginj ?
Pop(s)endPush(j, s);Push(i, s)endendFigure 4: Algorithm of generating training exam-plesbased on some entropy-based measure of a givensentence (e.g., (Tang et al, 2002)).
We cannotuse this kind of measures when we want to selectother smaller constituents than sentences.
Otherbigger problem is an algorithm of parsing itself.If we sample smaller units rather than sentences,we have partially annotated sentences and have touse a parsing algorithm that can be trained fromincompletely annotated sentences.
Therefore, it isdifficult to use some of probabilistic models forparsing.
45 Active Learning for JapaneseDependency ParsingIn this section we describe sample selection meth-ods which we investigated.5.1 Sentence-wise Sample SelectionPassive Selection (Passive) This method is toselect sequentially sentences that appear in thetraining corpus.
Since it gets harder for the read-ers to reproduce the same experimental setting, we4We did not employ query-by-committee (QBC) (Seunget al, 1992), which is another important general frameworkof active learning, since the selection strategy with large mar-gin classifiers (Section 2.2) is much simpler and seems morepractical for active learning in Japanese dependency parsingwith smaller constituents.358avoid to use random sampling in this paper.Minimum Margin Selection (Min) Thismethod is to select sentences that contain bun-setsu pairs which have smaller margin valuesof outputs of the classifier used in parsing.
Theprocedure of selection of MIN are summarized asfollows.
Assume that we have sentences si in thepool of unlabeled sentences.1.
Parse si in the pool with the current model.2.
Sort si with min |f(xk)| where xk are bun-setsu pairs in the sentence si.
Note that xkare not all possible bunsetsu pairs in si andthey are limited to bunsetsu pairs checked inthe process of parsing si.3.
Select top m sentences.Averaged Margin Selection (Avg) This methodis to select sentences that have smaller values ofaveraged margin values of outputs of the classi-fier in a give sentences over the number of deci-sions which are carried out in parsing.
The differ-ence between AVG and MIN is that for AVG weuse?|f(xk)|/l where l is the number of callingDep() in Figure 3 for the sentence si instead ofmin |f(xk)| for MIN.5.2 Chunk-wise Sample SelectionIn chunk-wise sample selection, we select bun-setsu pairs rather than sentences.
Bunsetsu pairsare selected from different sentences in a pool.This means that structures of sentences in the poolare partially annotated.Note that we do not use every bunsetsu pair ina sentence.
When we use Sassano?s algorithm, wehave to generate training examples for the classi-fier by using the algorithm in Figure 4.
In otherwords, we should not sample bunsetsu pairs inde-pendently from a given sentence.Therefore, we select bunsetsu pairs that havesmaller margin values of outputs given by the clas-sifier during the parsing process.
All the sentencesin the pool are processed by the current parser.
Wecannot simply split the sentences in the pool intolabeled and unlabeled ones because we do not se-lect every bunsetsu pair in a given sentence.Naive Selection (Naive) This method is to selectbunsetsu pairs that have smaller margin values ofoutputs of the classifier.
Then it is assumed thatannotators would label either ?D?
for the two bun-setsu having a dependency relation or ?O?, whichrepresents the two does not.Modified Simple Selection (ModSimple) Al-though NAIVE seems to work well, it did not (dis-cussed later).
MODSIMPLE is to select bunsetsupairs that have smaller margin values of outputsof the classifier, which is the same as in NAIVE.The difference between MODSIMPLE and NAIVEis the way annotators label examples.
Assume thatwe have an annotator and the learner selects somebunsetsu pair of the j-th bunsetsu and the i-th bun-setsu such that j < i.
The annotator is then askedwhat the head of the j-th bunsetsu is.
We definehere the head bunsetsu is the k-th one.We differently generate labeled examples fromthe information annotators give according to therelation among bunsetsus j, i, and k.Below we use the notation {s, t, ?D?}
to de-note that the s-th bunsetsu modifies the t-th one.The use of ?O?
instead of ?D?
indicates that thes-th does not modify the t-th.
That is generating{s, t, ?D?}
means outputting an example with thelabel ?D?.Case 1 if j < i < k, then generate {j, i, ?O?}
and{j, k, ?D?
}.Case 2 if j < i = k, then generate {j, k, ?D?
}.Case 3 if j < k < i, then generate {j, k, ?D?
}.Note that we do not generate {j, i, ?O?}
inthis case because in Sassano?s algorithm wedo not need such labeled examples if j de-pends on k such that k < i.Syntactically Extended Selection (Syn) Thisselection method is one based on MODSIMPLEand extended to generate more labeled examplesfor the classifier.
You may notice that more labeledexamples for the classifier can be generated froma single label which the annotator gives.
Syntac-tic constraints of the Japanese language allow usto extend labeled examples.For example, suppose that we have four bunset-sus A, B, C, and D in this order.
If A dependson C, i.e., the head of A is C, then it is automati-cally derived that B also should depend on C be-cause the Japanese language has the no-crossingconstraint for dependencies (Section 3.2).
By uti-lizing this property we can obtain more labeled ex-amples from a single labeled one annotators give.In the example above, we obtain {A,B, ?O?}
and{B,C, ?D?}
from {A,C, ?D?
}.359Although we can employ various extensions toMODSIMPLE, we use a rather simple extension inthis research.Case 1 if (j < i < k), then generate?
{j, i, ?O?},?
{k ?
1, k, ?D?}
if k ?
1 > j,?
and {j, k, ?D?
}.Case 2 if (j < i = k), then generate?
{k ?
1, k, ?D?}
if k ?
1 > j,?
and {j, k, ?D?
}.Case 3 if (j < k < i), then generate?
{k ?
1, k, ?D?}
if k ?
1 > j,?
and {j, k, ?D?
}.In SYN as well as MODSIMPLE, we generateexamples with ?O?
only for bunsetsu pairs that oc-cur to the left of the correct head (i.e., case 1).6 Experimental Evaluation andDiscussion6.1 CorpusIn our experiments we used the Kyoto UniversityCorpus Version 2 (Kurohashi and Nagao, 1998).Initial seed sentences and a pool of unlabeled sen-tences for training are taken from the articles onJanuary 1st through 8th (7,958 sentences) and thetest data is a set of sentences in the articles on Jan-uary 9th (1,246 sentences).
The articles on Jan-uary 10th were used for development.
The split ofthese articles for training/test/development is thesame as in (Uchimoto et al, 1999).6.2 Averaged PerceptronWe used the averaged perceptron (AP) (Freundand Schapire, 1999) with polynomial kernels.
Weset the degree of the kernels to 3 since cubic ker-nels with SVM have proved effective for Japanesedependency parsing (Kudo and Matsumoto, 2000;Kudo and Matsumoto, 2002).
We found the bestvalue of the epoch T of the averaged perceptronby using the development set.
We fixed T = 12through all experiments for simplicity.6.3 FeaturesThere are features that have been commonly usedfor Japanese dependency parsing among relatedpapers, e.g., (Kudo and Matsumoto, 2002; Sas-sano, 2004; Iwatate et al, 2008).
We also usedthe same features here.
They are divided into threegroups: modifier bunsetsu features, head bunsetsufeatures, and gap features.
A summary of the fea-tures is described in Table 1.6.4 ImplementationWe implemented a parser and a tool for the av-eraged perceptron in C++ and used them for ex-periments.
We wrote the main program of activelearning and some additional scripts in Perl and sh.6.5 Settings of Active LearningFor initial seed sentences, first 500 sentences aretaken from the articles on January 1st.
In ex-periments about sentence wise selection, 500 sen-tences are selected at each iteration of active learn-ing and labeled5 and added into the training data.In experiments about chunk wise selection 4000pairs of bunsetsus, which are roughly equal to theaveraged number of bunsetsus in 500 sentences,are selected at each iteration of active learning.6.6 Dependency AccuracyWe use dependency accuracy as a performancemeasure of a parser.
The dependency accuracy isthe percentage of correct dependencies.
This mea-sure is commonly used for the Kyoto UniversityCorpus.6.7 Results and DiscussionLearning Curves First we compare methods forsentence wise selection.
Figure 5 shows that MINis the best among them, while AVG is not goodand similar to PASSIVE.
It is observed that activelearning with large margin classifiers also workswell for Sassano?s algorithm of Japanese depen-dency parsing.Next we compare chunk-wise selection withsentence-wise one.
The comparison is shown inFigure 6.
Note that we must carefully considerhow to count labeled examples.
In sentence wiseselection we obviously count the number of sen-tences.
However, it is impossible to count suchnumber when we label bunsetsus pairs.Therefore, we use the number of bunsetsus thathave an annotated head.
Although we know thismay not be a completely fair comparison, we be-lieve our choice in this experiment is reasonable5In our experiments human annotators do not give labels.Instead, labels are given virtually from correct ones that theKyoto University Corpus has.360Bunsetsu features for modifiers rightmost content word, rightmost function word, punctuation,and heads parentheses, location (BOS or EOS)Gap features distance (1, 2?5, or 6 ?
), particles, parentheses, punctuationTable 1: Features for deciding a dependency relation between two bunsetsus.
Morphological featuresfor each word (morpheme) are major part-of-speech (POS), minor POS, conjugation type, conjugationform, and surface form.for assessing the effect of reduction by chunk-wiseselection.In Figure 6 NAIVE has a better learning curvecompared to MIN at the early stage of learning.However, the curve of NAIVE declines at the laterstage and gets worse than PASSIVE and MIN.Why does this phenomenon occur?
It is becauseeach bunsetsu pair is not independent and pairs inthe same sentence are related to each other.
Theysatisfy the constraints discussed in Section 3.2.Furthermore, the algorithm we use, i.e., Sassano?s,assumes these constraints and has the specific or-der for processing bunsetsu pairs as we see in Fig-ure 3.
Let us consider the meaning of {j, i, ?O?}
ifthe head of the j-th bunsetsu is the k-th one suchthat j < k < i.
In the context of the algorithm inFigure 3, {j, i, ?O?}
actually means that the j-thbunsetsu modifies th l-th one such that i < l. Thatis ?O?
does not simply mean that two bunsetsusdoes not have a dependency relation.
Therefore,we should not generate {j, i, ?O?}
in the case ofj < k < i.
Such labeled instances are not neededand the algorithm in Figure 4 does not generatethem even if a fully annotated sentence is given.Based on the analysis above, we modified NAIVEand defined MODSIMPLE, where unnecessary la-beled examples are not generated.Now let us compare NAIVE with MODSIMPLE(Figure 7).
MODSIMPLE is almost always betterthan PASSIVE and does not cause a significant de-terioration of accuracy unlike NAIVE.6Comparison of MODSIMPLE and SYN is shownin Figure 8.
Both exhibit a similar curve.
Figure 9shows the same comparison in terms of requiredqueries to human annotators.
It shows that SYN isbetter than MODSIMPLE especially at the earlierstage of active learning.Reduction of Annotations Next we examinedthe number of labeled bunsetsus to be required in6We have to carefully see the curves of NAIVE and MOD-SIMPLE.
In Figure 7 at the early stage NAIVE is slightlybetter than MODSIMPLE, while in Figure 9 NAIVE does notoutperform MODSIMPLE.
This is due to the difference of theway of accessing annotation efforts.0.8550.860.8650.870.8750.880.8850.890  1000 2000 3000 4000 5000 6000 7000 8000AccuracyNumber of Labeled SentencesPassiveMinAverageFigure 5: Learning curves of methods for sentencewise selection0.8550.860.8650.870.8750.880.8850.890  10000  20000  30000  40000  50000AccuracyNumber of bunsetsus which have a headPassiveMinNaiveFigure 6: Learning curves of MIN (sentence-wise)and NAIVE (chunk-wise).3610.8550.860.8650.870.8750.880.8850.890  10000  20000  30000  40000  50000AccuracyNumber of bunsetsus which have a headPassiveModSimpleNaiveFigure 7: Learning curves of NAIVE, MODSIM-PLE and PASSIVE in terms of the number of bun-setsus that have a head.0.8550.860.8650.870.8750.880.8850.890  10000  20000  30000  40000  50000AccuracyNumber of bunsetsus which have a headPassiveModSimpleSyntaxFigure 8: Learning curves of MODSIMPLE andSYN in terms of the number of bunsetsus whichhave a head.0.8550.860.8650.870.8750.880.8850.890  10000  20000  30000  40000  50000  60000AccuracyNumber of queris to human annotatorsModSimpleSyntaxNaiveFigure 9: Comparison of MODSIMPLE and SYNin terms of the number of queries to human anno-tators0500010000150002000025000300003500040000Passive Min Avg Naive ModSimpleSyn#ofbunsetsusthathaveaheadSelection strategyFigure 10: Number of labeled bunsetsus to be re-quired to achieve an accuracy of over 88.3%.05000100001500020000250000  1000 2000 3000 4000 5000 6000 7000 8000Number of SupportVectorsNumber of Labeled SentencesPassiveMinFigure 11: Changes of number of support vectorsin sentence-wise active learning05000100001500020000250000  10000  20000  30000  40000  50000  60000Number of SupportVectorsNumber of QueriesModSimpleFigure 12: Changes of number of support vectorsin chunk-wise active learning (MODSIMPLE)362order to achieve a certain level of accuracy.
Fig-ure 10 shows that the number of labeled bunsetsusto achieve an accuracy of over 88.3% dependingon the active learning methods discussed in thisresearch.PASSIVE needs 37766 labeled bunsetsus whichhave a head to achieve an accuracy of 88.48%,while SYN needs 13021 labeled bunsetsus toachieve an accuracy of 88.56%.
SYN requires only34.4% of the labeled bunsetsu pairs that PASSIVErequires.Stopping Criteria It is known that incrementrate of the number of support vectors in SVM in-dicates saturation of accuracy improvement dur-ing iterations of active learning (Schohn and Cohn,2000).
It is interesting to examine whether theobservation for SVM is also useful for supportvectors7 of the averaged perceptron.
We plottedchanges of the number of support vectors in thecases of both PASSIVE and MIN in Figure 11 andchanges of the number of support vectors in thecase of MODSIMPLE in Figure 12.
We observedthat the increment rate of support vectors mildlygets smaller.
However, it is not so clear as in thecase of text classification in (Schohn and Cohn,2000).Issues on Accessing the Total Cost of Annota-tion In this paper, we assume that each annota-tion cost for dependency relations is constant.
Itis however not true in an actual annotation work.8In addition, we have to note that it may be easierto annotate a whole sentence than some bunsetsupairs in a sentence9.
In a real annotation task, itwill be better to show a whole sentence to anno-tators even when annotating some part of the sen-tence.Nevertheless, it is noteworthy that our researchshows the minimum number of annotations inpreparing training examples for Japanese depen-dency parsing.
The methods we have proposedmust be helpful when checking repeatedly anno-tations that are important and might be wrong ordifficult to label while building an annotated cor-7Following (Freund and Schapire, 1999), we use the term?support vectors?
for AP as well as SVM.
?Support vectors?of AP means vectors which are selected in the training phaseand contribute to the prediction.8Thus it is very important to construct models for estimat-ing the actual annotation cost as Haertel et al (2008) do.9Hwa (2004) discusses similar aspects of researches onactive learning.pus.
They also will be useful for domain adapta-tion of a dependency parser.10Applicability to Other Languages and OtherParsing Algorithms We discuss here whetheror not the proposed methods and the experimentsare useful for other languages and other parsingalgorithms.
First we take languages similar toJapanese in terms of syntax, i.e., Korean and Mon-golian.
These two languages are basically head-final languages and have similar constraints inSection 3.2.
Although no one has reported appli-cation of (Sassano, 2004) to the languages so far,we believe that similar parsing algorithms will beapplicable to them and the discussion in this studywould be useful.On the other hand, the algorithm of (Sassano,2004) cannot be applied to head-initial languagessuch as English.
If target languages are assumedto be projective, the algorithm of (Nivre, 2003)can be used.
It is highly likely that we will inventthe effective use of finer-grained constituents, e.g.,head-modifier pairs, rather than sentences in activelearning for Nivre?s algorithm with large marginclassifiers since Sassano?s seems to be a simplifiedversion of Nivre?s and they have several propertiesin common.
However, syntactic constraints in Eu-ropean languages like English may be less helpfulthan those in Japanese because their dependencylinks do not have a single direction.Even though the use of syntactic constraints islimited, smaller constituents will still be useful forother parsing algorithms that use some determin-istic methods with machine learning-based classi-fiers.
There are many algorithms that have sucha framework, which include (Yamada and Mat-sumoto, 2003) for English and (Kudo and Mat-sumoto, 2002; Iwatate et al, 2008) for Japanese.Therefore, effective use of smaller constituents inactive learning would not be limited to the specificalgorithm.7 ConclusionWe have investigated that active learning methodsfor Japanese dependency parsing.
It is observedthat active learning of parsing with the averagedperceptron, which is one of the large margin clas-sifiers, works also well for Japanese dependencyanalysis.10Ohtake (2006) examines heuristic methods of selectingsentences.363In addition, as far as we know, we are the firstto propose the active learning methods of usingpartial dependency relations in a given sentencefor parsing and we have evaluated the effective-ness of our methods.
Furthermore, we have triedto obtain more labeled examples from precious la-beled ones that annotators give by utilizing syntac-tic constraints of the Japanese language.
It is note-worthy that linguistic constraints have been shownuseful for reducing annotations in active learningfor NLP.Experimental results show that our proposedmethods have improved considerably the learningcurve of Japanese dependency parsing.We are currently building a new annotated cor-pus with an annotation tool.
We have a plan to in-corporate our proposed methods to the annotationtool.
We will use it to accelerate building of thelarge annotated corpus to improved our Japaneseparser.It would be interesting to explore the use of par-tially labeled constituents in a sentence in anotherlanguage, e.g., English, for active learning.AcknowledgementsWe would like to thank the anonymous review-ers and Tomohide Shibata for their valuable com-ments.ReferencesJason Baldridge and Miles Osborne.
2004.
Activelearning and the total cost of annotation.
In Proc.of EMNLP 2004, pages 9?16.Yoav Freund and Robert E. Schapire.
1999.
Largemargin classification using the perceptron algorithm.Machine Learning, 37(3):277?296.Robbie Haertel, Eric Ringger, Kevin Seppi, James Car-roll, and Peter McClanahan.
2008.
Assessing thecosts of sampling methods in active learning for an-notation.
In Proc.
of ACL-08: HLT, short papers(Companion Volume), pages 65?68.Shinkichi Hashimoto.
1934.
Essentials of JapaneseGrammar (Kokugoho Yousetsu) (in Japanese).Rebecca Hwa.
2004.
Sample selection for statisticalparsing.
Computational Linguistics, 30(3):253?276.Masakazu Iwatate, Masayuki Asahara, and Yuji Mat-sumoto.
2008.
Japanese dependency parsing us-ing a tournament model.
In Proc.
of COLING 2008,pages 361?368.Taku Kudo and Yuji Matsumoto.
2000.
Japanese de-pendency structure analysis based on support vectormachines.
In Proc.
of EMNLP/VLC 2000, pages 18?25.Taku Kudo and Yuji Matsumoto.
2002.
Japanesedependency analysis using cascaded chunking.
InProc.
of CoNLL-2002, pages 63?69.Sadao Kurohashi and Makoto Nagao.
1998.
Building aJapanese parsed corpus while improving the parsingsystem.
In Proc.
of LREC-1998, pages 719?724.Florian Laws and Hinrich Schu?tze.
2008.
Stopping cri-teria for active learning of named entity recognition.In Proc.
of COLING 2008, pages 465?472.David D. Lewis and William A. Gale.
1994.
A se-quential algorithm for training text classifiers.
InProc.
of the Seventeenth Annual International ACM-SIGIR Conference on Research and Development inInformation Retrieval, pages 3?12.Ryan McDonald, Koby Crammer, and FernandoPereira.
2005.
Online large-margin training of de-pendency parsers.
In Proc.
of ACL-2005, pages523?530.Joakim Nivre.
2003.
An efficient algorithm for pro-jective dependency parsing.
In Proc.
of IWPT-03,pages 149?160.Kiyonori Ohtake.
2006.
Analysis of selective strate-gies to build a dependency-analyzed corpus.
InProc.
of COLING/ACL 2006 Main Conf.
Poster Ses-sions, pages 635?642.Eric Ringger, Peter McClanahan, Robbie Haertel,George Busby, Marc Carmen, James Carroll, KevinSeppi, and Deryle Lonsdale.
2007.
Active learn-ing for part-of-speech tagging: Accelerating corpusannotation.
In Proc.
of the Linguistic AnnotationWorkshop, pages 101?108.Manabu Sassano.
2002.
An empirical study of activelearning with support vector machines for Japaneseword segmentation.
In Proc.
of ACL-2002, pages505?512.Manabu Sassano.
2004.
Linear-time dependency anal-ysis for Japanese.
In Proc.
of COLING 2004, pages8?14.Greg Schohn and David Cohn.
2000.
Less is more:Active learning with support vector machines.
InProc.
of ICML-2000, pages 839?846.H.
S. Seung, M. Opper, and H. Sompolinsky.
1992.Query by committee.
In Proc.
of COLT ?92, pages287?294.Min Tang, Xaoqiang Luo, and Salim Roukos.
2002.Active learning for statistical natural language pars-ing.
In Proc.
of ACL-2002, pages 120?127.364Simon Tong and Daphne Koller.
2000.
Support vec-tor machine active learning with applications to textclassification.
In Proc.
of ICML-2000, pages 999?1006.Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isa-hara.
1999.
Japanese dependency structure analy-sis based on maximum entropy models.
In Proc.
ofEACL-99, pages 196?203.Hiroyasu Yamada and Yuji Matsumoto.
2003.
Statis-tical dependency analysis with support vector ma-chines.
In Proc.
of IWPT 2003, pages 195?206.Jingbo Zhu and Eduard Hovy.
2007.
Active learningfor word sense disambiguation with methods for ad-dressing the class imbalance problem.
In Proc.
ofEMNLP-CoNLL 2007, pages 783?790.365
