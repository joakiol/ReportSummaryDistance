Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 174?180,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsPredicting Grammaticality on an Ordinal ScaleMichael Heilman Aoife Cahill Nitin Madnani Melissa Lopez Matthew MulhollandEducational Testing ServicePrinceton, NJ, USA{mheilman,acahill,nmadnani,mlopez002,mmulholland}@ets.orgJoel TetreaultYahoo!
ResearchNew York, NY, USAtetreaul@yahoo-inc.comAbstractAutomated methods for identifyingwhether sentences are grammaticalhave various potential applications (e.g.,machine translation, automated essayscoring, computer-assisted languagelearning).
In this work, we construct astatistical model of grammaticality usingvarious linguistic features (e.g., mis-spelling counts, parser outputs, n-gramlanguage model scores).
We also presenta new publicly available dataset of learnersentences judged for grammaticality onan ordinal scale.
In evaluations, wecompare our system to the one from Post(2011) and find that our approach yieldsstate-of-the-art performance.1 IntroductionIn this paper, we develop a system for the taskof predicting the grammaticality of sentences, andpresent a dataset of learner sentences rated forgrammaticality.
Such a system could be used, forexample, to check or to rank outputs from systemsfor text summarization, natural language genera-tion, or machine translation.
It could also be usedin educational applications such as essay scoring.Much of the previous research on predictinggrammaticality has focused on identifying (andpossibly correcting) specific types of grammati-cal errors that are typically made by English lan-guage learners, such as prepositions (Tetreault andChodorow, 2008), articles (Han et al, 2006), andcollocations (Dahlmeier and Ng, 2011).
Whilesome applications (e.g., grammar checking) relyon such fine-grained predictions, others might bebetter addressed by sentence-level grammaticalityjudgments (e.g., machine translation evaluation).Regarding sentence-level grammaticality, therehas been much work on rating the grammatical-ity of machine translation outputs (Gamon et al,2005; Parton et al, 2011), such as the MT QualityEstimation Shared Tasks (Bojar et al, 2013, ?6),but relatively little on evaluating the grammatical-ity of naturally occurring text.
Also, most other re-search on evaluating grammaticality involves arti-ficial tasks or datasets (Sun et al, 2007; Lee et al,2007; Wong and Dras, 2010; Post, 2011).Here, we make the following contributions.?
We develop a state-of-the-art approach forpredicting the grammaticality of sentences onan ordinal scale, adapting various techniquesfrom the previous work described above.?
We create a dataset of grammatical and un-grammatical sentences written by Englishlanguage learners, labeled on an ordinal scalefor grammaticality.
With this unique data set,which we will release to the research com-munity, it is now possible to conduct realis-tic evaluations for predicting sentence-levelgrammaticality.2 Dataset DescriptionWe created a dataset consisting of 3,129 sentencesrandomly selected from essays written by non-native speakers of English as part of a test ofEnglish language proficiency.
We oversampledlower-scoring essays to increase the chances offinding ungrammatical sentences.
Two of the au-thors of this paper, both native speakers of Englishwith linguistic training, annotated the data.
Werefer to these annotators as expert judges.
Whenmaking judgments of the sentences, they saw theprevious sentence from the same essay as context.These two authors were not directly involved indevelopment of the system in ?3.Each sentence was annotated on a scale from1 to 4 as described below, with 4 being the most174grammatical.
We use an ordinal rather than bi-nary scale, following previous work such as that ofClark et al (2013) and Crocker and Keller (2005)who argue that the distinction between grammati-cal and ungrammatical is not simply binary.
Also,for practical applications, we believe that it is use-ful to distinguish sentences with minor errors fromthose with major errors that may disrupt communi-cation.
Our annotation scheme was influenced bya translation rating scheme by Coughlin (2003).Every sentence judged on the 1?4 scale must bea clause.
There is an extra category (?Other?)
forsentences that do not fit this criterion.
We excludeinstances of ?Other?
in our experiments (see ?4).4.
Perfect The sentence is native-sounding.
It hasno grammatical errors, but may contain very mi-nor typographical and/or collocation errors, as inExample (1).
(1) For instance, i stayed in a dorm when iwent to collge.3.
Comprehensible The sentence may containone or more minor grammatical errors, includ-ing subject-verb agreement, determiner, and mi-nor preposition errors that do not make the mean-ing unclear, as in Example (2).
(2) We know during Spring Festival, Chinesefamily will have a abundand family banquetwith family memebers.
?Chinese family?, which could be corrected to?Chinese families?, ?each Chinese family?, etc.,would be an example of a minor grammatical er-ror involving determiners.2.
Somewhat Comprehensible The sentencemay contain one or more serious grammaticalerrors, including missing subject, verb, object,etc., verb tense errors, and serious prepositionerrors.
Due to these errors, the sentence mayhave multiple plausible interpretations, as inExample (3).
(3) I can gain the transportations such as busesand trains.1.
Incomprehensible The sentence contains somany errors that it would be difficult to correct,as in Example (4).
(4) Or you want to say he is only a little boy donot everything clearly?The phrase ?do not everything?
makes the sen-tence practically incomprehensible since the sub-ject of ?do?
is not clear.O.
Other/Incomplete This sentence is incom-plete.
These sentences, such as Example (5), ap-pear in our corpus due to the nature of timed tests.
(5) The police officer handed theThis sentence is cut off and does not at least in-clude one clause.We measured interannotator agreement on asubset of 442 sentences that were independentlyannotated by both expert annotators.
Exact agree-ment was 71.3%, unweighted ?
= 0.574, andPearson?s r = 0.759.1For our experiments, oneexpert annotator was arbitrarily selected, and forthe doubly-annotated sentences, only the judg-ments from that annotator were retained.The labels from the expert annotators are dis-tributed as follows: 72 sentences are labeled 1;538 are 2; 1,431 are 3; 978 are 4; and 110 are ?O?.We also gathered 5 additional judgments usingCrowdflower.2For this, we excluded the ?Other?category and any sentences that had been markedas such by the expert annotators.
We used 100(3.2%) of the judged sentences as ?gold?
data inCrowdflower to block contributors who were notfollowing the annotation guidelines.
For thosesentences, only disagreements within 1 point ofthe expert annotator judgment were accepted.
Inpreliminary experiments, averaging the six judg-ments (1 expert, 5 crowdsourced) for each itemled to higher human-machine agreement.
For allexperiments reported later, we used this averageof six judgments as our gold standard.For our experiments (?4), we randomly split thedata into training (50%), development (25%), andtesting (25%) sets.
We also excluded all instanceslabeled ?Other?.
These are relatively uncommonand less interesting to this study.
Also, we believethat simpler, heuristic approaches could be used toidentify such sentences.We use ?GUG?
(?Grammatical?
versus ?Un-Grammatical?)
to refer to this dataset.
The datasetis available for research at https://github.com/EducationalTestingService/gug-data.1The reported agreement values assume that ?Other?maps to 0.
For the sentences where both labels were in the1?4 range (n = 424), Pearson?s r = 0.767.2http://www.crowdflower.com1753 System DescriptionThis section describes the statistical model (?3.1)and features (?3.2) used by our system.3.1 Statistical ModelWe use `2-regularized linear regression (i.e., ridgeregression) to learn a model of sentence grammat-icality from a variety of linguistic features.34To tune the `2-regularization hyperparameter ?,the system performs 5-fold cross-validation on thedata used for training.
The system evaluates ?
?10{?4,...,4}and selects the one that achieves thehighest cross-validation correlation r.3.2 FeaturesNext, we describe the four types of features.3.2.1 Spelling FeaturesGiven a sentence with with n word tokens, themodel filters out tokens containing nonalpha-betic characters and then computes the num-ber of misspelled words nmiss(later referred toas num misspelled), the proportion of mis-spelled wordsnmissn, and log(nmiss+ 1) as fea-tures.
To identify misspellings, we use a freelyavailable spelling dictionary for U.S. English.53.2.2 n-gram Count and Language ModelFeaturesGiven each sentence, the model obtains the countsof n-grams (n = 1 .
.
.
3) from English Gigawordand computes the following features:6?
?s?Snlog(count(s) + 1)?Sn?3We use ridge regression from the scikit-learntoolkit (Pedregosa et al, 2011) v0.23.1 and theSciKit-Learn Laboratory (http://github.com/EducationalTestingService/skll).4Regression models typically produce conservative pre-dictions with lower variance than the original training data.So that predictions better match the distribution of labels inthe training data, the system rescales its predictions.
It savesthe mean and standard deviation of the training data goldstandard (Mgoldand SDgold, respectively) and of its ownpredictions on the training data (Mpredand SDpred, respec-tively).
During cross-validation, this is done for each fold.From an initial prediction y?, it produces the final prediction:y??=y??MpredSDpred?
SDgold+Mgold.
This transformation doesnot affect Pearson?s r correlations or rankings, but it wouldaffect binarized predictions.5http://pythonhosted.org/pyenchant/6We use the New York Times (nyt), the Los Ange-les Times-Washington Post (ltw), and the Washington Post-Bloomberg News (wpb) sections from the fifth edition of En-glish Gigaword (LDC2011T07).?
maxs?Snlog(count(s) + 1)?
mins?Snlog(count(s) + 1)where Snrepresents the n-grams of order n fromthe given sentence.
The model computes the fol-lowing features from a 5-gram language modeltrained on the same three sections of English Gi-gaword using the SRILM toolkit (Stolcke, 2002):?
the average log-probability of thegiven sentence (referred to asgigaword avglogprob later)?
the number of out-of-vocabulary words in thesentenceFinally, the system computes the averagelog-probability and number of out-of-vocabularywords from a language model trained on a col-lection of essays written by non-native Englishspeakers7(?non-native LM?
).3.2.3 Precision Grammar FeaturesFollowing Wagner et al (2007) and Wagner etal.
(2009), we use features extracted from preci-sion grammar parsers.
These grammars have beenhand-crafted and designed to only provide com-plete syntactic analyses for grammatically cor-rect sentences.
This is in contrast to treebank-trained grammars, which will generally providesome analysis regardless of grammaticality.
Here,we use (1) the Link Grammar Parser8and (2)the HPSG English Resource Grammar (Copestakeand Flickinger, 2000) and PET parser.9We use a binary feature, complete link,from the Link grammar that indicates whether atleast one complete linkage can be found for a sen-tence.
We also extract several features from theHPSG analyses.10They mostly reflect informationabout unification success or failure and the associ-ated costs.
In each instance, we use the logarithmof one plus the frequency.7This did not overlap with the data described in ?2 andwas a subset of the data released by Blanchard et al (2013).8http://www.link.cs.cmu.edu/link/9http://moin.delph-in.net/PetTop10The complete list of relevant statistics used as featuresis: trees, unify cost succ, unify cost fail,unifications succ, unifications fail,subsumptions succ, subsumptions fail,words, words pruned, aedges, pedges,upedges, raedges, rpedges, medges.
Duringdevelopment, we observed that some of these features varyfor some inputs, probably due to parsing search timeouts.
On10 preliminary runs with the development set, this variancehad minimal effects on correlations with human judgments(less than 0.00001 in terms of r).176rour system 0.668?
non-native LM (?3.2.2) 0.665?
HPSG parse (?3.2.3) 0.664?
PCFG parse (?3.2.4) 0.662?
spelling (?3.2.1) 0.643?
gigaword LM (?3.2.2) 0.638?
link parse (?3.2.3) 0.632?
gigaword count (?3.2.2) 0.630Table 1: Pearson?s r on the development set, forour full system and variations excluding each fea-ture type.
??
X?
indicates the full model withoutthe ?X?
features.3.2.4 PCFG Parsing FeaturesWe find phrase structure trees and basic depen-dencies with the Stanford Parser?s English PCFGmodel (Klein and Manning, 2003; de Marneffe etal., 2006).11We then compute the following:?
the parse score as provided by the Stan-ford PCFG Parser, normalized for sentencelength, later referred to as parse prob?
a binary feature that captures whether the topnode of the tree is sentential or not (i.e.
theassumption is that if the top node is non-sentential, then the sentence is a fragment)?
features binning the number of dep rela-tions returned by the dependency conversion.These dep relations are underspecified forfunction and indicate that the parser was un-able to find a standard relation such as subj,possibly indicating a grammatical error.4 ExperimentsNext, we present evaluations on the GUG dataset.4.1 Feature AblationWe conducted a feature ablation study to iden-tify the contributions of the different types of fea-tures described in ?3.2.
We compared the perfor-mance of the full model with all of the featuresto models with all but one type of feature.
Forthis experiment, all models were estimated fromthe training set and evaluated on the developmentset.
We report performance in terms of Pearson?sr between the averaged 1?4 human labels and un-rounded system predictions.The results are shown in Table 1.
From theseresults, the most useful features appear to be then-gram frequencies from Gigaword and whetherthe link parser can fully parse the sentence.4.2 Test Set ResultsIn this section, we present results on the held-outtest set for the full model and various baselines,summarized in Table 2.
For test set evaluations,we trained on the combination of the training anddevelopment sets (?2), to maximize the amount oftraining data for the final experiments.We also trained and evaluated on binarized ver-sions of the ordinal GUG labels: a sentence waslabeled 1 if the average judgment was at least 3.5(i.e., would round to 4), and 0 otherwise.
Evaluat-ing on a binary scale allows us to measure howwell the system distinguishes grammatical sen-tences from ungrammatical ones.
For some ap-plications, this two-way distinction may be morerelevant than the more fine-grained 1?4 scale.
Totrain our system on binarized data, we replaced the`2-regularized linear regression model with an `2-regularized logistic regression and used Kendall?s?
rank correlation between the predicted probabil-ities of the positive class and the binary gold stan-dard labels as the grid search metric (?3.1) insteadof Pearson?s r.For the ordinal task, we report Pearson?s r be-tween the averaged human judgments and eachsystem.
For the binary task, we report percentageaccuracy.
Since the predictions from the binaryand ordinal systems are on different scales, we in-clude the nonparametric statistic Kendall?s ?
as asecondary evaluation metric for both tasks.We also evaluated the binary system for the or-dinal task by computing correlations between itsestimated probabilities and the averaged humanscores, and we evaluated the ordinal system for thebinary task by binarizing its predictions.12We compare our work to a modified version ofthe publicly available13system from Post (2011),which performed very well on an artificial dataset.To our knowledge, it is the only publicly availablesystem for grammaticality prediction.
It is very11We use the Nov. 12, 2013 version of the Stanford Parser.12We selected a threshold for binarization from a grid of1001 points from 1 to 4 that maximized the accuracy of bina-rized predictions from a model trained on the training set andevaluated on the binarized development set.
For evaluatingthe three single-feature baselines discussed below, we usedthe same approach except with grid ranging from the min-imum development set feature value to the maximum plus0.1% of the range.13The Post (2011) system is available at https://github.com/mjpost/post2011judging.177Ordinal Task Binary Taskr Sig.r?
% Acc.
Sig.%Acc.
?our system 0.644 0.479 79.3 0.419our systemlogistic0.616 * 0.484 80.7 0.428Post 0.321 * 0.225 75.5 * 0.195Postlogistic0.259 * 0.181 74.4 * 0.181complete link 0.386 * 0.335 74.8 * 0.302gigaword avglogprob 0.414 * 0.290 76.7 * 0.280num misspelled -0.462 * -0.370 74.8 * -0.335Table 2: Human-machine agreement statistics for our system, the system from Post (2011), and simplebaselines, computed from the averages of human ratings in the testing set (?2).
?*?
in a Sig.
columnindicates a statistically significant difference from ?our system?
(p < .05, see text for details).
A majoritybaseline for the binary task achieves 74.8% accuracy.
The best results for each metric are in bold.different from our system since it relies on par-tial tree-substitution grammar derivations as fea-tures.
We use the feature computation componentsof that system but replace its statistical model.
Thesystem was designed for use with a dataset consist-ing of 50% grammatical and 50% ungrammaticalsentences, rather than data with ordinal or continu-ous labels.
Additionally, its classifier implementa-tion does not output scores or probabilities.
There-fore, we used the same learning algorithms as forour system (i.e., ridge regression for the ordinaltask and logistic regression for the binary task).14To create further baselines for comparison,we selected the following features that representways one might approximate grammaticality if acomprehensive model was unavailable: whetherthe link parser can fully parse the sentence(complete link), the Gigaword languagemodel score (gigaword avglogprob),and the number of misspelled tokens(num misspelled).
Note that we expectthe number of misspelled tokens to be negativelycorrelated with grammaticality.
We flipped thesign of the misspelling feature when computingaccuracy for the binary task.To identify whether the differences in perfor-mance for the ordinal task between our system andeach of the baselines are statistically significant,we used the BCaBootstrap (Efron and Tibshirani,1993) with 10,000 replications to compute 95%confidence intervals for the absolute value of r forour system minus the absolute value of r for eachof the alternative methods.
For the binary task, we14In preliminary experiments, we observed little differencein performance between logistic regression and the originalsupport vector classifier used by the system from Post (2011).used the sign test to test for significant differencesin accuracy.
The results are in Table 2.5 Discussion and ConclusionsIn this paper, we developed a system for predict-ing grammaticality on an ordinal scale and cre-ated a labeled dataset that we have released pub-licly (?2) to enable more realistic evaluations infuture research.
Our system outperformed an ex-isting state-of-the-art system (Post, 2011) in eval-uations on binary and ordinal scales.
This is themost realistic evaluation of methods for predictingsentence-level grammaticality to date.Surprisingly, the system from Post (2011) per-formed quite poorly on the GUG dataset.
We spec-ulate that this is due to the fact that the Post sys-tem relies heavily on features extracted from au-tomatic syntactic parses.
While Post found thatsuch a system can effectively distinguish gram-matical news text sentences from sentences gen-erated by a language model, measuring the gram-maticality of real sentences from language learn-ers seems to require a wider variety of features,including n-gram counts, language model scores,etc.
Of course, our findings do not indicate thatsyntactic features such as those from Post (2011)are without value.
In future work, it may be pos-sible to improve grammaticality measurement byintegrating such features into a larger system.AcknowledgementsWe thank Beata Beigman Klebanov, Yoko Futagi,Su-Youn Yoon, and the anonymous reviewers fortheir helpful comments.
We also thank JenniferFoster for discussions about this work and MattPost for making his system publicly available.178ReferencesDaniel Blanchard, Joel Tetreault, Derrick Higgins,Aoife Cahill, and Martin Chodorow.
2013.TOEFL11: A Corpus of Non-Native English.
Tech-nical report, Educational Testing Service.Ond?rej Bojar, Christian Buck, Chris Callison-Burch,Christian Federmann, Barry Haddow, PhilippKoehn, Christof Monz, Matt Post, Radu Soricut, andLucia Specia.
2013.
Findings of the 2013 Work-shop on Statistical Machine Translation.
In Pro-ceedings of the Eighth Workshop on Statistical Ma-chine Translation, pages 1?44, Sofia, Bulgaria, Au-gust.
Association for Computational Linguistics.Alexander Clark, Gianluca Giorgolo, and Shalom Lap-pin.
2013.
Towards a statistical model of grammat-icality.
In Proceedings of the 35th Annual Confer-ence of the Cognitive Science Society, pages 2064?2069.Ann Copestake and Dan Flickinger.
2000.
Anopen-source grammar development environmentand broad-coverage English grammar using HPSG.In Proceedings of the 2nd International Confer-ence on Language Resources and Evaluation (LREC2000), Athens, Greece.Deborah Coughlin.
2003.
Correlating automated andhuman assessments of machine translation quality.In Proceedings of MT Summit IX, pages 63?70.Matthew W. Crocker and Frank Keller.
2005.
Prob-abilistic grammars as models of gradience in lan-guage processing.
In Gradience in Grammar: Gen-erative Perspectives.
University Press.Daniel Dahlmeier and Hwee Tou Ng.
2011.
CorrectingSemantic Collocation Errors with L1-induced Para-phrases.
In Proceedings of the 2011 Conference onEmpirical Methods in Natural Language Process-ing, pages 107?117, Edinburgh, Scotland, UK., July.Association for Computational Linguistics.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating TypedDependency Parses from Phrase Structure Parses.
InLREC 2006, pages 449?454.B.
Efron and R. Tibshirani.
1993.
An Introduction tothe Bootstrap.
Chapman and Hall/CRC, Boca Ra-ton, FL.Michael Gamon, Anthony Aue, and Martine Smets.2005.
Sentence-level MT evaluation without refer-ence translations: Beyond language modeling.
InProceedings of EAMT, pages 103?111.
Springer-Verlag.Na-Rae Han, Martin Chodorow, and Claudia Leacock.2006.
Detecting errors in English article usage bynon-native speakers.
Natural Language Engineer-ing, 12(2):115?129.Dan Klein and Christopher D. Manning.
2003.
Accu-rate Unlexicalized Parsing.
In Proceedings of the41st Annual Meeting of the Association for Com-putational Linguistics, pages 423?430, Sapporo,Japan, July.
Association for Computational Linguis-tics.John Lee, Ming Zhou, and Xiaohua Liu.
2007.
De-tection of Non-Native Sentences Using Machine-Translated Training Data.
In Human LanguageTechnologies 2007: The Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics; Companion Volume, Short Pa-pers, pages 93?96, Rochester, New York, April.
As-sociation for Computational Linguistics.Kristen Parton, Joel Tetreault, Nitin Madnani, and Mar-tin Chodorow.
2011.
E-rating machine translation.In Proceedings of the Sixth Workshop on StatisticalMachine Translation, pages 108?115, Edinburgh,Scotland, July.
Association for Computational Lin-guistics.F.
Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,B.
Thirion, O. Grisel, M. Blondel, P. Pretten-hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-sos, D. Cournapeau, M. Brucher, M. Perrot, andE.
Duchesnay.
2011.
Scikit-learn: Machine Learn-ing in Python.
Journal of Machine Learning Re-search, 12:2825?2830.Matt Post.
2011.
Judging Grammaticality with TreeSubstitution Grammar Derivations.
In Proceedingsof the 49th Annual Meeting of the Association forComputational Linguistics: Human Language Tech-nologies, pages 217?222, Portland, Oregon, USA,June.
Association for Computational Linguistics.Andreas Stolcke.
2002.
SRILM - An Extensible Lan-guage Modeling Toolkit.
In 7th International Con-ference on Spoken Language Processing.Guihua Sun, Xiaohua Liu, Gao Cong, Ming Zhou,Zhongyang Xiong, John Lee, and Chin-Yew Lin.2007.
Detecting Erroneous Sentences using Auto-matically Mined Sequential Patterns.
In Proceed-ings of the 45th Annual Meeting of the Associationof Computational Linguistics, pages 81?88, Prague,Czech Republic, June.
Association for Computa-tional Linguistics.Joel R. Tetreault and Martin Chodorow.
2008.
TheUps and Downs of Preposition Error Detection inESL Writing.
In Proceedings of the 22nd Inter-national Conference on Computational Linguistics(Coling 2008), pages 865?872, Manchester, UK,August.
Coling 2008 Organizing Committee.Joachim Wagner, Jennifer Foster, and Josef van Gen-abith.
2007.
A Comparative Evaluation of Deepand Shallow Approaches to the Automatic Detec-tion of Common Grammatical Errors.
In Proceed-ings of the 2007 Joint Conference on Empirical179Methods in Natural Language Processing and Com-putational Natural Language Learning (EMNLP-CoNLL), pages 112?121, Prague, Czech Republic,June.
Association for Computational Linguistics.Joachim Wagner, Jennifer Foster, and Josef van Gen-abith.
2009.
Judging grammaticality: Experi-ments in sentence classification.
CALICO Journal,26(3):474?490.Sze-Meng Jojo Wong and Mark Dras.
2010.
ParserFeatures for Sentence Grammaticality Classifica-tion.
In Proceedings of the Australasian LanguageTechnology Association Workshop 2010, pages 67?75, Melbourne, Australia, December.180
