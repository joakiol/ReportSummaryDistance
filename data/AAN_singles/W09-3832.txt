Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 210?213,Paris, October 2009. c?2009 Association for Computational LinguisticsHPSG Supertagging: A Sequence Labeling ViewYao-zhong Zhang ?
Takuya Matsuzaki ??
Department of Computer Science, University of Tokyo?
School of Computer Science, University of Manchester?National Centre for Text Mining, UK{yaozhong.zhang, matuzaki, tsujii}@is.s.u-tokyo.ac.jpJun?ichi Tsujii??
?AbstractSupertagging is a widely used speed-uptechnique for deep parsing.
In anotheraspect, supertagging has been exploitedin other NLP tasks than parsing forutilizing the rich syntactic informationgiven by the supertags.
However, theperformance of supertagger is still abottleneck for such applications.
In thispaper, we investigated the relationshipbetween supertagging and parsing, notjust to speed up the deep parser; Westarted from a sequence labeling viewof HPSG supertagging, examining howwell a supertagger can do when separatedfrom parsing.
Comparison of two typesof supertagging model, point-wise modeland sequential model, showed that theformer model works competitively welldespite its simplicity, which indicatesthe true dependency among supertagassignments is far more complex than thecrude first-order approximation made inthe sequential model.
We then analyzedthe limitation of separated supertaggingby using a CFG-filter.
The results showedthat big gains could be acquired by resort-ing to a light-weight parser.1 IntroductionSupertagging is an important part of lexicalizedgrammar parsing.
A high performance supertag-ger greatly reduces the load of a parser and ac-celerates its speed.
A supertag represents a lin-guistic word category, which encodes syntactic be-havior of the word.
The concept of supertaggingwas first proposed for lexicalized tree adjoininggrammar (LTAG) (Bangalore and Joshi, 1999) andthen extended to other lexicalized grammars, suchas combinatory categorial grammar (CCG) (Clark,2002) and Head-driven phrase structure grammar(HPSG) (Ninomiya et al, 2006).
Recently, syn-tactic information in supertags has been exploitedfor NLP tasks besides parsing, such as NP chunk-ing (Shen and Joshi, 2003), semantic role label-ing (Chen and Rambow, 2003) and machine trans-lation (Hassan et al, 2007).
Supertagging servesthere as an implicit and convenient way to incor-porate rich syntactic information in those tasks.Improving the performance of supertagging canthus benefit these two aspects: as a preproces-sor for deep parsing and as an independent, al-ternative technique for ?almost?
parsing.
How-ever, supertags are derived from a grammar andthus have a strong connection to parsing.
To fur-ther improve the supertagging accuracy, the rela-tion between supertagging and parsing is crucial.With this motivation, we investigate how well a se-quence labeling model can do when it is separatedfrom a parser, and to what extent the ignorance oflong distance dependencies in the sequence label-ing formulation affects the supertagging results.Specifically, we evaluated two different typesof supertagging model, point-wise model and se-quential model, for HPSG supertagging.
CFG-filter was then used to empirically evaluate theeffect of long distance dependencies in supertag-ging.
The point-wise model achieved competitiveresult of 92.53% accuracy on WSJ-HPSG tree-bank with fast training speed, while the sequen-tial model augmented with supertag edge featuresdid not give much further improvement over thepoint-wise model.
Big gains acquired by usingCFG-filter indicates that further improvement maybe achieved by resorting to a light-weight parser.2 HPSG SupertagsHPSG (Pollard and Sag, 1994) is a kind of lexi-calized grammar.
In HPSG, many lexical entriesare used to express word-specific characteristics,210while only small amount of rule schemas are usedto describe general constructions.
A supertag inHPSG corresponds to a template of lexical entry.For example, one possible supertag for ?big?
is?
[<ADJP>]N lxm?, which indicates that the syn-tactic category of ?big?
is adjective and it modi-fies a noun to its right.
The number of supertagsis generally much larger than the number of labelsused in other sequence labeling tasks; Comparingto 45 POS tags used in PennTreebank, the HPSGgrammar used in our experiments includes 2,308supertags.
Because of this, it is often very hard oreven impossible to apply computationary demand-ing methods to HPSG supertagging.3 Perceptron and Bayes Point MachinePerceptron is an efficient online discriminativetraining method.
We used perceptron with weight-averaging (Collins, 2002) as the basis of our su-pertagging model.
We also use perceptron-basedBayes point machine (BPM) (Herbrich et al,2001) in some of the experiments.
In short, a BPMis an average of a number of averaged perceptrons?weights.
We use average of 10 averaged percep-trons, each of which is trained on a different ran-dom permutation of the training data.3.1 FormulationHere we follow the definition of Collins?
per-ceptron to learn a mapping from the input space(w, p) ?
W ?
P to the supertag space s ?
S. Weuse function GEN(w,p) to indicate all candidatesgiven input (w, p).
Feature function f maps a train-ing sample (w, p, s) ?W ?P ?S to a point in thefeature space Rd.
To get feature weights ?
?
Rdof feature function, we used the averaged percep-tron training method described in (Collins, 2002),and the average of its 10 different runs (i.e., BPM).For decoding, given an input (w, p) and a vectorof feature weights ?, we want to find an output swhich satisfies:F (w, p) = argmaxs?GEN(w, p)?
?
f(w, p, s)For the input (w, p), we treat it in two fash-ions: one is (w, p) representing a single wordand a POS tag.
Another is (w, p) representingwhole word and POS tags sequence.
We call thempoint-wise model and sequential model respec-tively.
Viterbi algorithm is used for decoding insequential model.template type templateWord wi,wi?1,wi+1,wi?1&wi, wi&wi+1POS pi, pi?1, pi?2, pi+1,pi+2, pi?1&pi, pi?2&pi?1,pi?1&pi+1, pi&pi+1, pi+1&pi+2Word-POS pi?1&wi, pi&wi, pi+1&wiSupertag?
si?1 , si?2&si?1Substructure {ssi,1, ..., ssi,N}?Word{ssi,1, ..., ssi,N}?
POS{ssi,1, ..., ssi,N}?Word-POS{ssi?1,1, ..., ssi?1,N}?
{ssi,1, ..., ssi,N}?Table 1: Feature templates for point-wise modeland sequential model.
Templates with ?
are onlyused by sequential model.
ssi,j represents j-thsubstructure of supertag at i.
For briefness, si isomitted for each template.
???
means set-product.e.g., {a,b}?
{A,B}={a&A,a&B,b&A,b&B}3.2 FeaturesFeature templates are listed in Table 1.
To makethe results comparable with previous work, weadopt the same feature templates as Matsuzaki et.al.
(2007).
For sequential model, supertag con-texts are added to the features.
Because of thelarge number of supertags, those supertag edgefeatures could be very sparse.
To alleviate thissparseness, we extracted sub-structures from thelexical template of each supertag, and use them formaking generalized node/edge features as shownin Table 1.
The sub-structures we used includesubcategorization frames (e.g., subject=NP, ob-ject=NP PP), direction and category of modifieephrase (e.g., mod left=VP), voice and tense of averb (e.g., passive past).3.3 CFG-filterLong distance dependencies are also encoded insupertags.
For example, when a transitive verbgets assigned a supertag that specifies it has a PP-object, in most cases a preposition to its right mustbe assigned an argument (not adjunct) supertag,and vice versa.
Such kind of long distance contextinformation might be important for supertag dis-ambiguation, but is not easy to incorporate into asequence labeling model separated from a parser.To examine the limitation of supertagging sep-arated from a parser, we used CFG-filter as an ap-211Model Name Acc%PW-AP 92.29SEQ-AP 92.53PW-AP+CFG 93.57SEQ-AP+CFG 93.68Table 2: Averaged 10-cross validation of averagedperceptron on Section 02-21.proximation of an HPSG parser.
We firstly cre-ated a CFG that approximates the original HPSGgrammar, using the iterative method by Kieferand Krieger (2000).
Given the supertags as pre-terminals, the approximating CFG was then usedfor finding a maximally scored sequence of su-pertags which satisfies most of the grammaticalconstraints in the original HPSG grammar (Mat-suzaki et al, 2007).
By comparing the supertag-ging results before and after CFG-filtering, we canquantify how many errors are caused by ignoranceof the long-range dependencies in supertagger.4 Experiments and AnalysisWe conducted experiments on WSJ-HPSG tree-bank corpus (Miyao, 2006), which was semi-automatically converted from the WSJ portion ofPennTreebank.
The number of training iterationswas set to 5 for all models.
Gold-standard POStags are used as input.
The performance is evalu-ated by accuracy1 and speed of supertagging on anAMD Opteron 2.4GHz server.Table 2 shows the averaged results of 10-fold cross-validation of averaged perceptron (AP)models2 on section 02-21.
We can see the dif-ference between point-wise AP model and se-quential AP model is small (0.24%).
It becomeseven smaller after CFG-filtering (0.11%).
Table3 shows the supertagging accuracy on section 22based on BPM.
Although not statistically signif-icantly different from previous ME model (Mat-suzaki et al, 2007), point-wise model (PW-BPM)achieved competitive result 92.53% with fastertraining.
In addition, 0.27% and 0.29% gains werebrought by using BPM from PW-AP (92.26%) andPW-SEQ (92.54%) with P-values less than 0.05.The improvement by using sequential mod-els (PW-AP?SEQ-AP: 0.24%, PW-BPM?SEQ-BPM: 0.3%, statistically significantly different),1?UNK?
supertags are ignored in evaluation as previous.2For time limitation, cross validation for BPM was notconducted.Model Name Acc% Training/Testing Time ?ME (Matsuzaki 07?)
92.45 ?
3h / 12sPW-BPM 92.53 285s / 10sSEQ-BPM 92.83 1721s / 13sPW-BPM+SUB 92.68 1275s / 25sSEQ-BPM+SUB 92.99 9468s / 107sPW-BPM+CFG 93.60 285s / 78sSEQ-BPM+CFG 93.70 1721s / 195sPW-BPM+SUB+CFG 93.72 1275s / 170sSEQ-BPM+SUB+CFG 93.88 9468s / 1011sTable 3: Supertagging accuracy and training&testing speed on section 22.
(?)
Test time was cal-culated on totally 1648 sentences.compared to point-wise models, were not so large,but the training time was around 6 times longer.We think the reason is twofold.
First, as previousresearch showed, POS sequence is very informa-tive in supertagging (Clark, 2004).
A large amountof local syntactic information can be captured inPOS tags of surrounding words, although a fewlong-range dependencies are of course not.
Sec-ond, the number of supertags is large and the su-pertag edge features used in sequential model areinevitably suffered from data sparseness.
To alle-viate this, we extracted sub-structure from lexicaltemplates (i.e., lexical items corresponding to su-pertags) to augment the supertag edge features, butonly got 0.16% improvement (SEQ-BPM+SUB).Furthermore, we also got 0.15% gains with P-value less than 0.05 by incorporating the sub-structure features into point-wise model (PW-BPM+SUB).
We hence conclude that the contri-bution of the first-order edge features is not largein sequence modeling for HPSG supertagging.As we explained in Section 3.3, sequence label-ing models have inherent limitation in the abilityto capture long distance dependencies between su-pertags.
This kind of ambiguity could be easier tosolve in a parser.
To examine this, we added CFG-filter which works as an approximation of a fullHPSG parser, after the sequence labeling model.As expected, there came big gains of 1.26% (fromPW-AP to PW-AP+CFG) and 1.15% (from PW-BPM to PW-BPM+CFG).
Even for the sequen-tial model we also got 1.15% (from SEQ-AP toSEQ-AP+CFG) and 0.87% (from SEQ-BPM toSEQ-BPM+CFG) respectively.
All these modelswere statistically significantly different from orig-212inal ones.We also gave error analysis on test results.Comparing SEQ-AP with SEQ-AP+CFG, one ofthe most frequent types of ?correct supertag?
bythe CFG-filter was for word ?and?, wherein a su-pertag for NP-coordination (?NP and NP?)
wascorrected to one for VP-coordination (?VP andVP?
or ?S and S?).
It means the disambiguationbetween the two coordination type is difficult forsupertaggers, presumably because they looks verysimilar with a limited length of context since thesequence of the NP-object of left conjunct, ?and?,the NP subject of right conjunct looks very similarto a NP coordination.
The different assignmentsby SEQ-AP+CFG from SEQ-AP include 725 rightcorrections, while it changes 298 correct predic-tions by SEQ-AP to wrong assignments.
One pos-sible reason for some of ?wrong correction?
is re-lated to the approximation of grammar.
But thisgives clue that for supertagging task: just usingsequence labeling models is limited, and we canresort to use some light-weight parser to handlelong distance dependencies.Although some of the ambiguous supertagscould be left for deep parsing, like multi-taggingtechnique (Clark, 2004), we also consider thetasks where supertags can be used while conduct-ing deep parsing is too computationally costly.
Al-ternatively, focusing on supertagging, we couldtreat it as a sequence labeling task, while a conse-quent light-weight parser is a disambiguator withlong distance constraint.5 ConclusionsIn this paper, through treating HPSG supertag-ging in a sequence labeling way, we examinedthe relationship between supertagging and parsingfrom an angle.
In experiment, even for sequentialmodels, CFG-filter gave much larger improvementthan one gained by switching from a point-wisemodel to a sequential model.
The accuracy im-provement given by the CFG-filter suggests thatwe could gain further improvement by combininga supertagger with a light-weight parser.AcknowledgmentsThanks to the anonymous reviewers for valuablecomments.
The first author was partially sup-ported by University of Tokyo Fellowship (UT-Fellowship).
This work was partially supportedby Grant-in-Aid for Specially Promoted Researchand Special Coordination Funds for PromotingScience and Technology (MEXT, Japan).ReferencesSrinivas Bangalore and Aravind K. Joshi.
1999.
Su-pertagging: An approach to almost parsing.
Com-putational Linguistics, 25:237?265.John Chen and Owen Rambow.
2003.
Use of deeplinguistic features for the recognition and labelingof semantic arguments.
In Proceedings of EMNLP-2003, pages 41?48.Stephen Clark.
2002.
Supertagging for combinatorycategorial grammar.
In Proceedings of the 6th In-ternational Workshop on Tree Adjoining Grammarsand Related Frameworks (TAG+ 6), pages 19?24.Stephen Clark.
2004.
The importance of supertaggingfor wide-coverage ccg parsing.
In Proceedings ofCOLING-04, pages 282?288.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and experi-ments with perceptron algorithms.
pages 1?8.Hany Hassan, Mary Hearne, and Andy Way.
2007.
Su-pertagged phrase-based statistical machine transla-tion.
In Proceedings of ACL 2007, pages 288?295.Ralf Herbrich, Thore Graepel, and Colin Campbell.2001.
Bayes point machines.
Journal of MachineLearning Research, 1:245?279.Bernd Kiefer and Hans-Ulrich Krieger.
2000.
Acontext-free approximation of head-driven phrasestructure grammar.
In Proceedings of IWPT-2000,pages 135?146.Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsu-jii.
2007.
Efficient hpsg parsing with supertaggingand cfg-filtering.
In Proceedings of IJCAI-07, pages1671?1676.Yusuke Miyao.
2006.
From Linguistic Theory to Syn-tactic Analysis: Corpus-Oriented Grammar Devel-opment and Feature Forest Model.
Ph.D. Disserta-tion, The University of Tokyo.Takashi Ninomiya, Yoshimasa Tsuruoka, Takuya Mat-suzaki, and Yusuke Miyao.
2006.
Extremely lex-icalized models for accurate and fast hpsg parsing.In Proceedings of EMNLP-2006, pages 155?163.Carl Pollard and Ivan A.
Sag.
1994.
Head-drivenPhrase Structure Grammar.
University of Chicago /CSLI.Libin Shen and Aravind K. Joshi.
2003.
A snow basedsupertagger with application to np chunking.
In Pro-ceedings of ACL 2003, pages 505?512.213
