Coling 2008: Proceedings of the workshop on Multi-source Multilingual Information Extraction and Summarization, pages 41?48Manchester, August 2008Evaluating automatically generated user-focused multi-documentsummaries for geo-referenced imagesAhmet AkerDepartment of Computer ScienceUniversity of SheffieldSheffield, S1 4DP, UKA.Aker@dcs.shef.ac.ukRobert GaizauskasDepartment of Computer ScienceUniversity of SheffieldSheffield, S1 4DP, UKR.Gaizauskas@dcs.shef.ac.ukAbstractThis paper reports an initial study that aimsto assess the viability of a state-of-the-artmulti-document summarizer for automaticcaptioning of geo-referenced images.
Theautomatic captioning procedure requiressummarizing multiple web documents thatcontain information related to images?
lo-cation.
We use SUMMA (Saggion andGaizauskas, 2005) to generate generic andquery-based multi-document summariesand evaluate them using ROUGE evalua-tion metrics (Lin, 2004) relative to humangenerated summaries.
Results show that,even though query-based summaries per-form better than generic ones, they are stillnot selecting the information that humanparticipants do.
In particular, the areasof interest that human summaries display(history, travel information, etc.)
are notcontained in the query-based summaries.For our future work in automatic imagecaptioning this result suggests that devel-oping the query-based summarizer furtherand biasing it to account for user-specificrequirements will prove worthwhile.1 IntroductionRetrieving textual information related to a loca-tion shown in an image has many potential appli-cations.
It could help users gain quick access tothe information they seek about a place of inter-est just by taking its picture.
Such textual informa-tion could also, for instance, be used by a journalistc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.who is planning to write an article about a building,or by a tourist who seeks further interesting placesto visit nearby.
In this paper we aim to generatesuch textual information automatically by utilizingmulti-document summarization techniques, wheredocuments to be summarized are web documentsthat contain information related to the image con-tent.
We focus on geo-referenced images, i.e.
im-ages tagged with coordinates (latitude and longi-tude) and compass information, that show thingswith fixed locations (e.g.
buildings, mountains,etc.
).Attempts towards automatic generation ofimage-related textual information or captions havebeen previously reported.
Deschacht and Moens(2007) and Mori et al (2000) generate imagecaptions automatically by analyzing image-relatedtext from the immediate context of the image,i.e.
existing image captions, surrounding text inHTML documents, text contained in the image,etc.
The authors identify named entities and othernoun phrases in the image-related text and assignthese to the image as captions.
Other approachescreate image captions by taking into considera-tion image features as well as image-related text(Westerveld, 2000; Barnard et al, 2003; Pan etal., 2004).
These approaches can address all kindsof images, but focus mostly on images of people.They analyze only the immediate textual context ofthe image on the web and are concerned with de-scribing what is in the image only.
Consequently,background information about the objects in theimage is not provided.
Our aim, however, is tohave captions that inform users?
specific interestsabout a location, which clearly includes more thanjust image content description.
Multi-documentsummarization techniques offer the possibility toinclude image-related information from multiple41documents, however, the challenge lies in beingable to summarize unrestricted web documents.Various multi-document summarization toolshave been developed: SUMMA (Saggion andGaizauskas, 2005), MEAD (Radev et al, 2004),CLASSY (Conroy et al, 2005), CATS (Farzin-der et al, 2005) and the system of Boros et al(2001), to name just a few.
These systems generateeither generic or query-based summaries or both.Generic summaries address a broad readershipwhereas query-based summaries are preferred byspecific groups of people aiming for quick knowl-edge gain about specific topics (Mani, 2001).SUMMA and MEAD generate both generic andquery-based multi-document summaries.
Boroset al (2001) create only generic summaries,while CLASSY and CATS create only query-basedsummaries from multiple documents.
The perfor-mance of these tools has been reported for DUCtasks1.
As Sekine and Nobata (2003) note, al-though DUC tasks provide a common evaluationstandard, they are restricted in topic and are some-what idealized.
For our purposes the summarizerneeds to create summaries from unrestricted webinput, for which there are no previous performancereports.For this reason we evaluate the performance ofboth a generic and a query-based summarizer anduse SUMMA which provides both summarizationmodes.
We hypothesize that a query-based sum-marizer will better address the problem of creatingsummaries tailored to users?
needs.
This is becausethe query itself may contain important hints as towhat the user is interested in.
A generic summa-rizer generates summaries based on the topics itobserves from the documents and cannot take userspecific input into consideration.
Using SUMMA,we generate both generic and query-based multi-document summaries of image-related documentsobtained from the web.
In an online data collectionprocedure we presented a set of images with re-lated web documents to human subjects and askedthem to select from these documents the infor-mation that best describes the image.
Based onthis user information we created model summariesagainst which we evaluated the automatically gen-erated ones.Section 2 in this paper describes how image-related documents were collected from the web.In section 3 SUMMA is described in detail.
In1http://www-nlpir.nist.gov/projects/duc/index.htmlsection 4 we explain how the human image de-scriptions were collected.
Section 5 discusses theresults, and section 6 concludes the paper and out-lines directions for future work and improvements.2 Web Document CollectionFor web document collection we used geo-referenced images of locations in London such asWestminster Abbey, London Eye, etc.
The imageswere taken with a digital SLR camera with a Geo-tagger plugged-in to its flash slot.
The Geotaggerhelped us to identify the location by means of co-ordinates of the position where the photographerstands, as well as the direction the camera is point-ing (compass information).
Based on the coordi-nates and compass information for each image, wecarried out the following steps to collect relateddocuments from the web:?
identify a set of toponyms (terms that denotelocations or associate names with locations,e.g.
Westminster Abbey) that can be passed toa search engine as query terms for documentsearch;?
use a search engine to retrieve HTML docu-ments to be summarized;?
extract the pure text out of the HTML docu-ments.2.1 Toponym CollectionIn order to create the web queries a set of to-ponyms were collected semi-automatically.
Weimplemented an application (cf.
Figure 1) thatsuggests a list of toponyms close to the photogra-pher?s location.
The application uses Microsoft?sMapPoint2service which allows users to querylocation-related information.
For example, a usercan query for tourist attractions (interesting build-ings, museums, art galleries etc.)
close to a loca-tion that is identified by its address or its coordi-nates.Based on the coordinates (latitude and longi-tude), important toponyms for a particular imagecan be queried from the MapPoint database.
Inorder to facilitate this, MapPoint returns a met-ric that measures the importance of each toponym.A value close to zero means that the returned to-ponym is closer to the specified coordinates thana toponym with a higher value.
For instance for2http://www.microsoft.com/mappoint/42Figure 1: Image Toponym Collector: WestminsterAbbey, Lat: 51.50024 Lon: -0.128138333: Direc-tion: 137.1the image of Westminster Abbey shown in the Im-age box of Figure 1 the following toponyms arecollected:Queen Elizabeth II Conf.
Centre: 0.059Parliament Square: 0.067Westminster Abbey: 0.067The photographer?s location is shown with a blackdot on the first map in the Maps box of Figure 1.The application suggests the toponyms shown inthe Suggested Terms list.Knowing the direction the photographer wasfacing helps us to select the correct toponyms fromthe list of suggested toponyms.
The current Map-Point implementation does not allow an arrow tobe drawn on the map which would be the best in-dication of the direction the photographer is facing.To overcome this problem we create a second map(cf.
Maps box of Figure 1) that shows another dotmoved 50 meters in the compass direction.
By fol-lowing the dot from the first map to the second mapwe can determine the direction the photographer isfacing.
When the direction is known, it is certainthat the image shows Westminster Abbey and notthe Queen Elizabeth II Conf.
Centre or ParliamentSquare.
The Queen Elizabeth II Conf.
Centre isbehind the photographer and Parliament Square ison the left hand side.Consequently in this example the toponymWestminster Abbey is selected manually for theweb search.
In order to avoid ambiguities, thecity name and the country name (also generatedby MapPoint) are added manually to the selectedtoponyms.
Hence, for Westminster Abbey, Lon-don and United Kingdom are added to the toponymlist.
Finally the terms in the toponym list are sim-ply separated by a boolean AND operator to formthe web query.
Then, the query is passed to thesearch engine as described in the next section.2.2 Document Query and Text ExtractionThe web queries were passed to the Google Searchengine and the 20 best search results were re-trieved, from which only 11 were taken for thesummarization process.
We ensure that these 20search results are healthy hyperlinks, i.e.
that thecontent of the hyperlink is accessible.
In additionto this, multiple hyperlinks belonging to the samedomain are ignored as it is assumed that the con-tent obtained from the same domain would be sim-ilar.
Each remaining search result is crawled to ob-tain its content.The web-crawler downloads only the content ofthe document residing under the hyperlink, whichwas previously found as a search result, and doesnot follow any other hyperlinks within the docu-ment.
The content obtained by the web-crawlerencapsulates an HTML structured document.
Wefurther process this using an HTML parser3to se-lect the pure text, i.e.
text consisting of sentences.The HTML parser removes advertisements,menu items, tables, java scripts etc.
from theHTML documents and keeps sentences which con-tain at least 4 words.
This number was chosen afterseveral experiments.
The resulting data is passedon to the multi-document summarizer which is de-scribed in the next section.3 SUMMASUMMA4is a set of language and processing re-sources to create and evaluate summarization sys-tems (single document, multi-document, multi-lingual).
The components can be used withinGATE5to produce ready summarization applica-tions.
SUMMA has been used in this work tocreate an extractive multi-document summarizer:both generic and query-based.In the case of generic summarization SUMMAuses a single cluster approach to summarize n re-lated documents which are given as input.
UsingGATE, SUMMA first applies sentence detectionand sentence tokenisation to the given documents.Then each sentence in the documents is repre-sented as a vector in a vector space model (Salton,1988), where each vector position contains a term3http://htmlparser.sourceforge.net/4http://www.dcs.shef.ac.uk/ saggion/summa/default.htm5http://gate.ac.uk43(word) and a value which is a product of the termfrequency in the document and the inverse docu-ment frequency (IDF), a measurement of the term?sdistribution over the set of documents (Salton andBuckley, 1988).
Furthermore, SUMMA enhancesthe sentence vector representation with further fea-tures such as the sentence position in its documentand the sentence similarity to the lead-part in itsdocument.
In addition to computing the vector rep-resentation for all sentences in the document col-lection the centroid of this sentence representationis also computed.In the sentence selection process, each sentencein the collection is ranked individually, and the topsentences are chosen to build up the final summary.The ranking of a sentence depends on its distanceto the centroid, its absolute position in its docu-ment and its similarity to the lead-part of its doc-ument.
For calculating vector similarities, the co-sine similarity measure is used (Salton and Lesk,1968).In the case of the query-based approach,SUMMA adds an additional feature to the sentencevector representation as computed for genericsummarization.
For each sentence, cosine simi-larity to the given query is computed and addedto the sentence vector representation.
Finally, thesentences are scored by summing all features in thevector space model according to the following for-mula:Sentencescore=n?i=1featurei?
weightiAfter the scoring process, SUMMA starts selectingsentences for summary generation.
In both genericand query-based summarization, the summary isconstructed by first selecting the sentence that hasthe highest score, followed by the next sentencewith the second highest score until the compres-sion rate is reached.
However, before a sentenceis selected a similarity metric for redundancy de-tection is applied to each sentence which decideswhether a sentence is distinct enough from alreadyselected sentences to be included in the summaryor not.
SUMMA uses the following formula tocompute the similarity between two sentences:NGramSim(S1, S2, n) =n?j=1wj?grams(S1, j)?grams(S2, j)grams(S1, j)?grams(S2, j)where n specifies maximum size of the n-grams tobe considered, grams(SX, j) is the set of j-grams insentence X and wjis the weight associated withj-gram similarity.
Two sentences are similar ifNGramSim(S1, S2, n) > ?.
In this work n is setto 4 and ?
to 0.1.
For j-gram similarity weightsw1= 0.1, w2= 0.2, w3= 0.3 and w4= 0.4 areselected.
These values are coded in SUMMA asdefaults.Using SUMMA, generic and query-based sum-maries are generated for the image-related docu-ments obtained from the web.
Each summary con-tains a maximum of 200 words.
The queries usedin the query-based mode are toponyms collected asdescribed in section 2.1.4 Creating Model SummariesFor evaluating automatically generated summariesas image captions, information that people asso-ciate with images is collected.
For this purpose, anonline data collection procedure was set up.
Par-ticipants were provided with a set of 24 images.Each image had a detailed map showing the loca-tion where it was taken, along with URLs to 11related documents which were used for the auto-mated summarization.
Figure 2 shows an exampleof an image and Table 2 contains the correspond-ing related information.Each participant was asked to familiarize him-or herself with the location of the image by an-alyzing the map and going through all 11 URLs.Then each participant decided on up to 5 differentpieces of information he/she would like to know ifhe/she sees the image or information about some-thing he/she relates with the image.
The informa-tion we collected in this way is similar to ?infor-mation nuggets?
(Voorhees, 2003).
Informationnuggets are facts which help us assess automaticsummaries by checking whether the summary con-tains the fact or not.
In addition to this, each par-ticipant was asked to collect the information onlyfrom the given documents, ignoring any other linksin these documents.Eleven students participated in this survey, sim-ulating the scenario in which tourists look for in-formation about an image of a popular sight.
Thenumber of images annotated by each participant isshown in Table 1.The participants selected the information fromoriginal HTML documents on the web and notfrom the documents which were preprocessed forthe multi-document summarization task.
We found44Table 1: Number of images annotated by each particantUser1 User2 User3 User4 User5 User6 User7 User8 User9 User10 User1124 7 24 24 18 24 8 4 16 12 24Figure 2: Example imageTable 2: Information related to Figure 21.
Westminster Abbey is the place of the coronation, mar-riage and burial of British monarchs, except Edward V andEdward VIII since 10662. the parish church of the Royal Family3.
the centrepiece to the City of Westminster4.
first church on the site is believed to have been con-structed around the year 7005.
The history and the monuments, crypts and memorialsare not to be missed.out that in some cases the participants selected in-formation that did not occur in the preprocesseddocuments.
To ensure that the information selectedby the participants also occurs in the preprocesseddocuments, we retained only the information se-lected by the participants that could also be foundin these documents, i.e.
that was available to thesummarizer.
Out of 807 nuggets selected by partic-ipants 21 (2.6%) were not found in the documentsavailable to the summarizer and were removed.Furthermore, as the example above shows (cf.Table 2), not all the items of information se-lected by the participants were in form of full sen-tences.
They vary from phrases to whole sen-tences.
The participants were free to select anytext unit from the documents that they related tothe image content.
However, SUMMA worksextractively and its summaries contain only sen-tences selected from the given input documents.The user selected information was normalized tosentences in order to have comparable summariesfor evaluation.
This was achieved by selectingthe sentence(s) from the documents in which theparticipant-selected information was found and re-placing the participant-selected phrases or clauseswith the full sentence(s).
In this way model sum-maries were obtained.5 ResultsThe model summaries were compared against24 summaries generated automatically usingSUMMA by calculating ROUGE-1 to ROUGE-4, ROUGE-L and ROUGE-W-1.2 recall metrics(Lin, 2004).
For all these metrics ROUGE com-pares each automatically generated summary spairwise to every model summary mifrom the setof M model summaries and takes the maximumROUGEScorevalue among all pairwise compar-isons as the best ROUGEScorescore:ROUGEScore= argmaxiROUGEScore(mi, s)ROUGE repeats this comparisonM times.
In eachiteration it applies the Jackknife method and takesone model summary from theM model summariesaway and compares the automatically generatedsummary s against the M ?
1 model summaries.In each iteration one best ROUGEScoreis calcu-lated.
The final ROUGEScoreis then the averageof all best scores calculated in M iterations.In this way each generic and query-based sum-mary was compared with the corresponding modelsummaries.
The results are given in the first twocolumns of Table 3.
We also collected the com-mon information all participants selected for a par-ticular image and compared this to the correspond-ing query-based summary.
The common informa-tion is the intersection set of the sets of informationeach of the participants selected for a particular im-age.
The results for this comparison are shown incolumn QueryToCPOfModel of Table 3.The model summaries were also comparedagainst each other in order to assess the agreementbetween the participants.
To achieve this, the im-age information selected by each participant wascompared against the rest.
The corresponding re-sults are shown in column UserToUser of Table4.
We applied the same pairwise comparison weused for our model summaries to the model sum-maries of task 5 in DUC 2004 in order to mea-45Table 3: Comparison: Automatically generated summaries against model summaries.
The column GenericToModel forexample shows ROUGE results for generic summaries relative to model summaries.
CP stands for common part, i.e.
commoninformation selected by all participants.Recall GenericToModel QueryToModel QueryToCPOfModel QueryToModelInDUCR-1 0.38293 0.39655 0.22084 0.3341R-2 0.14760 0.17266 0.09894 0.0723R-3 0.09286 0.11196 0.06222 0.0279R-4 0.07450 0.09219 0.04971 0.0131R-L 0.34437 0.35837 0.20913 0.3320R-W-1.2 0.11821 0.12606 0.06350 0.1130Table 4: Comparison: Model summaries against each otherRecall UserToUser UserToUserInDUCR-1 0.42765 0.45407R-2 0.30091 0.13820R-3 0.26338 0.05870R-4 0.24964 0.02950R-L 0.40403 0.41594R-W-1.2 0.15846 0.13973sure the agreements between the participants onthis standard task.
This gives us a benchmark rel-ative to which we can assess how well users agreeon what information should be related to images.The results for this comparison are shown in col-umn UserToUserInDUC of Table 4.All ROUGE metrics except R-1 and R-L in-dicate higher agreement in human image-relatedsummaries than in DUC document summaries.The ROUGE metrics most indicative of agreementbetween human summaries are those that best cap-ture words occurring in longer sequences of wordsimmediately following each other (R-2, R-3, R-4and R-W).
If long word sequences are identicalin two summaries it is more likely that they be-long to the same sentence than if only single wordsare common, as captured by R-1, or sequences ofwords that do not immediately follow each other,as captured by R-L.
In R-L gaps in word sequencesare ignored so that for instance A B C D G andA E B F C K D have the common sequence A BC D according to R-L. R-W considers the gaps inwords sequences so that this sequence would notbe recognized as common.
Therefore the agree-ment on our image-related human summaries issubstantially higher than agreement on DUC doc-ument human summaries.The results in Table 3 support our hypothesisthat query-based summaries will perform betterthan generic ones on image-related summaries.
AllROUGE results of the query-based summaries aregreater than the generic summary scores.
Thisreinforces our decision to focus on query-basedsummaries in order to create image-related sum-maries which also satisfy the users?
needs.
How-ever, even though the query-based summaries aremore appropriate for our purposes, they are notcompletely satisfactory.
The query-based sum-maries cover only 39% of the unigrams (ROUGE1) in the model summaries and only 17% of thebigrams (ROUGE 2), while the model summarieshave 42% agreement in unigrams and 30% agree-ment in bigrams (cf.
column UserToUser in Table4).
The agreement between the query-based andmodel summaries gets lower for ROUGE-3 andROUGE-4 indicating that the query-based sum-maries contain very little information in commonwith the participants?
results.
This indication issupported by the ROUGE-L (35%) and the lowROUGE-W (12%) agreement which are substan-tially lower compared to the UserToUser ROUGE-L (40%) and ROUGE-W (15%) and the lowROUGE scores in column QueryToCPOfModel.For comparison with automated summaries in adifferent domain, we include ROUGE scores ofquery based SUMMA used in DUC 2004 (Sag-gion and Gaizauskas, 2005) as shown in the lastcolumn of Table 3.
All scores are lower than ourQueryToModel results which might be due to lowagreement between human generated summariesfor the DUC task (cf.
UserToUserInDUC columnin Table 4) or maybe because image captioning isan easier task.
The possibility that our summariza-tion task is easier than DUC due to the summa-rizer having fewer documents to summarize or dueto the documents being shorter than those in theDUC task can be excluded.
In the DUC task themulti-document clusters contain 10 documents onaverage while our summarizer works with 11 doc-uments.
The mean length in documents in DUC46Table 5: Query-based summary for Westminster Abbey and information selected by participantsQuery-based summary Information selected by participantsThe City of London has St Pauls, but Westminster Abbeyis the centrepiece to the City of Westminster.
Westmin-ster Abbey should be at the top of any London traveler?slist.
Westminster Abbey, however, lacks the clear lines ofa Rayonnant church,...
I loved Westminster Abbey on mytrip to London.
Westminster Abbey was rebuilt after1245 by Henry III?s order, and in 1258 the remodelingof the east end of St. Paul?s Cathedral began.
He was in-terred in Westminster Abbey.
From 1674 to 1678 he tunedthe organ at Westminster Abbey and was employed therein 1675-76 to copy organ parts of anthems.
The architec-tural carving found at Westminster Abbey (mainly of the1250s) has much of the daintiness of contemporary Frenchwork, although the drapery is still more like that of the earlyChartres or Wells sculpture than that of the Joseph Master.Nevertheless, Westminster Abbey is something to see if youhave not seen it before.
I happened upon the WestminsterAbbey on an outing to Parliament and Big Ben.1.
(3) Westminster Abbey is the place of the coronation,marriage and burial of British monarchs, except EdwardV and Edward VIII since 1066.
2.
(1) What is unknown,however is just how old it is.
The first church on thesite is believed to have been constructed around the year700.
3.
(2) Standing as it does between Westminster Abbeyand the Houses of Parliament, and commonly called ?theparish church of the House of Commons?, St Margaret?s haswitnessed many important events in the life of this coun-try.
4.
(1) In addition, the Abbey is the parish church ofthe Royal Family, when in residence at Buckingham Palace.5.
(1) The history and the monuments, crypts and memorialsare not to be missed.
6.
(1) For almost one thousand years,Westminister Abbey has been the setting for much of Lon-don?s ceremonies such as Royal Weddings, Coronations,and Funeral Services.
7.
(1) It is also where many visitorspay pilgrimage to The Tomb of the Unknown Soldier.
8.
(1)The City of London has St Pauls, but Westminster Abbey isthe centrepiece to the City of Westminster.is 23 sentences while our documents have 44 sen-tences on average.Table 5 shows an example query-based sum-mary for the image of Westminster Abbey and theinformation participants selected for this particu-lar image.
Jointly the participants have selected 8different pieces of information as indicated by thebold numbers in the table.
The numbers in paren-theses show the number of times that a particularinformation unit was selected.
By comparing thetwo sides it can be seen that the query-based sum-mary does not cover most of the information fromthe list with the exception of item 2.
The item 2 issemantically related to the sentence in bold on thesummary side as it addresses the year the abbeywas built, but the information contained in the twodescriptions is different.Our results have confirmed our hypothesis thatquery-based summaries will better address the aimof this research, which is to get summaries tai-lored to users?
needs.
A generic summary does nottake the user query into consideration and gener-ates summaries based on the topics it observes.
Fora set of documents containing mainly historicaland little location-related information, a genericsummary will probably contain a higher numberof history-related than location-related sentences.This might satisfy a group of people seeking his-torical information, however, it might not be inter-esting for a group who want to look for location-related information.
Therefore using a query-based multi-document summarizer is more appro-priate for image-related summaries than a genericone.
However, the results of the query-based sum-maries show that even so they only cover a smallpart of the information the users select.
One reasonfor this is that the query-based summarizer takesrelevant sentences according to the query given toit and does not take into more general consider-ation the information likely to be relevant to theuser.
However, we can assume that users will haveshared interests in some of the information theywould like to get about a particular type of objectin an image (e.g.
a bridge, church etc.).
This as-sumption is supported by the high agreement be-tween participants?
performances in our online sur-vey (cf.
column UserToUser of Table 4).Therefore, one way to improve the performanceof the query-based summarizer is to give the sum-marizer the information that users typically asso-ciate with a particular object type as input and biasthe multi-document summarizer towards this in-formation.
To do this we plan to build models ofuser preferences for different object types from thelarge number of existing image captions from webresources, which we believe will improve the qual-ity of automatically generated captions.6 ConclusionIn this work we showed that query-based summa-rizers perform slightly better than generic sum-marizers on an image captioning task.
However,their output is not completely satisfactory whencompared to what human participants indicated asimportant in our data collection study.
Our fu-ture work will concentrate on extending the query-47based summarizer to improve its performance ingenerating captions that match user expectationsregarding specific image types.
This will includecollecting a large number of existing captions fromweb sources and applying machine learning tech-niques for building models of the kinds of informa-tion that people use for captioning.
Further workalso needs to be carried out on improving the read-ability of the extractive caption summaries.7 AcknowledgementThis work is supported by the EU-funded TRIPODproject6.
We would like to thank Horacio Saggionfor his support with SUMMA.
We are also gratefulto Emina Kurtic, Mark Sanderson, Mesude Bicakand Dilan Paranavithana for comments on the pre-vious versions of this paper.ReferencesBarnard, Kobus and Duygulu, Pinar and Forsyth, Davidand de Freitas, Nando and Blei M, David and Jor-dan I, Michael.
2003.
Matching words and pic-tures.
The Journal of Machine Learning Research,MIT Press Cambridge, MA, USA, 3: 1107?1135.Boros, Endre and Kantor B, Paul and Neu j, David.2001.
A Clustering Based Approach to CreatingMulti-Document Summaries.
Proc.
of the 24th An-nual International ACM SIGIR Conference on Re-search and Development in Information Retrieval.Conroy M, John and Schlesinger D, Judith and Stew-art G, Jade 2005.
CLASSY query-based multi-document summarization.
Proc.
of the 2005 Doc-ument Understanding Workshop, Boston.Deschacht, Koen andMoens F, Marie.
2007.
Text Anal-ysis for Automatic Image Annotation.
Proc.
of the45th Annual Meeting of the Association for Compu-tational Linguistics, Prague.Farzindar, Atefeh and Rozon, Frederik and Lapalme,Guy.
2005.
CATS a topic-oriented multi-document summarization system at DUC 2005.Proc.
of the 2005 Document Understanding Work-shop (DUC2005).Lin, Chin-Yew 2004.
ROUGE: A Package for Auto-matic Evaluation of Summaries.
Proc.
of the Work-shop on Text Summarization Branches Out (WAS2004).Mani, Inderjeet.
2001.
Automatic Summarization.John Benjamins Publishing Company.Mori, Yasuhide and Takahashi, Hironobu and Oka,Ryuichi.
2000.
Automatic word assignment to im-ages based on image division and vector quantiza-tion.
Proc.
of RIAO 2000: Content-Based Multime-dia Information Access.6http://tripod.shef.ac.uk/Pan, Jia-Yu.
and Yang, Hyung-Jeong and Duygulu,Pinar and Faloutsos, Christos.
2004.
Automaticimage captioning.
Multimedia and Expo, 2004.ICME?04.
2004 IEEE International Conference on.Radev R, Dragomir.
and Jing, Hongyan and Sty?s, Mal-gorzata and Tam, Daniel.
2004.
Centroid-basedsummarization of multiple documents.
InformationProcessing and Management,40(6): 919?938.Saggion, Horacio and Gaizauskas, Robert 2004.
Multi-document summarization by cluster/profile relevanceand redundancy removal.
Document UnderstandingConference (DUC04).Salton, Gerhard 1988.
Automatic text process-ing.
Addison-Wesley Longman Publishing Co., Inc.Boston, MA, USA.Salton, Gerhard and Buckley, Chris 1988.
Term-weighting approaches in automatic text retrieval.Pergamon Press, Inc. Tarrytown, NY, USA.Salton, Gerhard and Lesk E., Michael 1968.
ComputerEvaluation of Indexing and Text Processing.
Journalof the ACM,15(1):8?36.Sekine, Satoshi and Nobata, Chikashi.
2003.
A Sur-vey for Multi-Document Summarization.
Associa-tion for Computational Linguistics Morristown, NJ,USA, Proc.
of the HLT-NAACL 03 on Text summa-rization workshop-Volume 5.Voorhees M, Ellen.
2003.
Overview of the TREC 2003Question Answering Track.
Proc.
of the Twelfth TextREtrieval Conference (TREC 2003).Westerveld, Thijs.
2000.
Image retrieval: Content ver-sus context.
Content-Based Multimedia InformationAccess, RIAO 2000 Conference.48
