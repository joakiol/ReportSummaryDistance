Combining Linguistic and Machine Learning Techniques for EmailSummarizationSmaranda MuresanDept.
of Computer ScienceColumbia University500 West 120 StreetNew York, NY, 10027smara@cs.columbia.eduEvelyne TzoukermannBell LaboratoriesLucent Technologies700 Mountain AvenueMurray Hill, NJ, 07974evelyne@lucent.comJudith L. KlavansColumbia UniversityCenter for Research onInformation Access535 West 114th StreetNew York, NY 10027klavans@cs.columbia.eduAbstractThis paper shows that linguistic tech-niques along with machine learningcan extract high quality noun phrasesfor the purpose of providing the gistor summary of email messages.
Wedescribe a set of comparative experi-ments using several machine learningalgorithms for the task of salient nounphrase extraction.
Three main conclu-sions can be drawn from this study: (i)the modifiers of a noun phrase can besemantically as important as the head,for the task of gisting, (ii) linguistic fil-tering improves the performance of ma-chine learning algorithms, (iii) a combi-nation of classifiers improves accuracy.1 IntroductionIn this paper we present a comparative study ofsymbolic machine learning models applied to nat-ural language task of summarizing email mes-sages through topic phrase extraction.Email messages are domain-general text, theyare unstructured and not always syntactically wellformed.
These characteristics raise challenges forautomatic text processing, especially for the sum-marization task.
Our approach to email summa-rization, implemented in the GIST-IT system, isto identify topic phrases, by first extracting nounphrases as candidate units for representing doc-ument meaning and then using machine learningalgorithms to select the most salient ones.The comparative evaluation of several machinelearning models in the settings of our experimentsindicates that : (i) for the task of gisting the mod-ifiers of the noun phrase are equally as importantas the head, (ii) noun phrases are better than n-grams for the phrase-level representation of thedocument, (iii) linguistic filtering enhances ma-chine learning techniques, (iv) a combination ofclassifiers improves accuracy.Section 2 of the paper outlines the machinelearning aspect of extracting salient noun phrases,emphasizing the features used for classifica-tion and the symbolic machine learning modelsused in the comparative experiments.
Section3 presents the linguistic filtering steps that im-prove the accuracy of the machine learning algo-rithms.
Section 4 discusses in detail our conclu-sions stated above.2 Machine Learning for ContentExtractionSymbolic machine learning has been applied suc-cessfully in conjunction with many NLP applica-tions (syntactic and semantic parsing, POS tag-ging, text categorization, word sense disambigua-tion) as reviewed by Mooney and Cardie (1999).We used machine learning techniques for findingsalient noun phrases that can represent the sum-mary of an email message.
This section describesthe three steps involved in this classification task:1) what representation is appropriate for the infor-mation to be classified as relevant or non-relevant(candidate phrases), 2) which features should beassociated with each candidate, 3) which classifi-cation models should be used.Case 1CNP: scientific/JJ and/CC technical/JJ articles/NNSSNP1: scientific/JJ articles/NNSSNP2: technical/JJ articles/NNSCase 2CNP: scientific/JJ thesauri/NNS and databases/NNSSNP1: scientific/JJ thesauri/NNSSNP2: scientific/JJ databases/NNSCase 3CNP: physics/NN and/CC biology/NN skilled/JJ researchers/NNSSNP1: physics/NN skilled/JJ researchers/NNSSNP2: biology/NN skilled/JJ researchers/NNSTable 1: Resolving Coordination of NPs2.1 Candidate PhrasesOf the major syntactic constituents of a sentence,e.g.
noun phrases, verb phrases, and prepositionalphrases, we assume that noun phrases (NPs) carrythe most contentful information about the doc-ument, a well-supported hypothesis (Smeaton,1999; Wacholder, 1998).As considered by Wacholder (1998), the sim-ple NPs are the maximal NPs that contain pre-modifiers but not post-nominal constituents suchas prepositions or clauses.
We chose simple NPsfor content representation because they are se-mantically and syntactically coherent and they areless ambiguous than complex NPs.
For extractingsimple noun phrases we first used Ramshaw andMarcus?s base NP chunker (Ramshaw and Mar-cus, 1995).
The base NP is either a simple NP ora coordination of simple NPs.
We used heuristicsbased on POS tags to automatically split the co-ordinate NPs into simple ones, properly assigningthe premodifiers.
Table 1 presents some coordi-nate NPs (CNP) encountered in our data collec-tion and the results of our algorithm which splitthem into simple NPs (SNP1 and SNP2).2.2 Features used for ClassificationThe choice of features used to represent the can-didate phrases has a strong impact on the accu-racy of the classifiers (e.g.
the number of exam-ples needed to obtain a given accuracy on the testdata, the cost of classification).
For our classifica-tion task of determining if a noun phrase is salientor not to the document meaning, we chose a set ofnine features.Several studies rely on the linguistic intuitionthat the head of the noun phrase makes a greatercontribution to the semantics of the nominalgroup than the modifiers.
However, for somespecific tasks in NLP , the head is not necessar-ily the most semantically important part of thenoun phrase.
In analyzing email messages fromthe perspective of finding salient NPs, we claimthat the modifier(s) of the noun phrase - usuallynominal modifiers(s), often have as much seman-tic content as the head.
This opinion is also sup-ported in the work of Strzalkowski et al (1999),where syntactic NPs are captured for the goalof extracting their semantic content but are pro-cessed as an ?ordered?
string of words rather thana syntactic unit.
Thus we introduce as a sepa-rate feature in the feature vector, a new TF*IDFmeasure which consider the NP as a sequence ofequally weighted elements, counting individuallythe modifier(s) and the head.Consider the following list of simple NPs se-lected as candidates:1. conference workshop announcement2.
international conference3.
workshop description4.
conference deadlineIn the case of the first noun phrase, for exam-ple, its importance is found in the two noun mod-ifiers: conference and workshop as much as inthe head announcement, due to their presence asheads or modifiers in the candidate NPs 2-4.
Ournew feature will be: TF  IDFconference+TF IDFworkshop+ TF  IDFannouncement.
Givingthese linguistic observations we divided the set offeatures into three groups, as we mentioned alsoin (Tzoukermann et al, 2001): 1) one associatedwith the head of the noun phrase; 2) one associ-ated with the whole NP and 3) one that representsthe new TF*IDF measure discussed above.2.2.1 Features associated with the HeadWe choose two features to characterize thehead of the noun phrases: head tfidf: the TF*IDF measure of thehead of the candidate NP.
For the NP inexample (1) this feature will be TF IDFannouncement. head focc: The position of the first occur-rence of the head in text (the number ofwords that precede the first occurrence of thehead divided by the total number of words inthe document).2.2.2 Features associated with the whole NPWe select six features that we consider relevantin determining the relative importance of the nounphrase: np tfidf: the TF*IDF measure ofthe whole NP.
For the NP in theexample (1) this feature will beTF  IDFconference workshop announcement. np focc: The position of the first occurrenceof the noun phrase in the document. np length words: Noun phrase length mea-sured in number of words, normalized by di-viding it with the total number of words inthe candidate NP list. np length chars: Noun phrase length mea-sured in number of characters, normalizedby dividing it with the total number of char-acters in the candidate NPs list. sent pos: Position of the noun phrase in thesentence: the number of words that precedethe noun phrase, divided by sentence length.For noun phrases in the subject line (whichare usually short and will be affected by thismeasure), we consider the maximum lengthof sentence in document as the normalizationfactor. par pos: Position of noun phrase in para-graph, same as sent pos, but at the paragraphlevel.2.2.3 Feature that considers all constituentsof the NP equally weightedOne of the important hypotheses we tested inthis work is that both the modifiers and the headof NP contribute equally to its salience.
Thus weconsider mh tfidf as an additional feature in thefeature vector. mh tfidf: the new TF*IDF measure thattakes also into consideration the importanceof the modifiers.
In our example the value ofthis feature will be : TF  IDFconference+TF IDFworkshop+TF IDFannouncementIn computing the TF*IDF measures (head tfidf,np tfidf, mh tfidf), specific weights, wi, were as-signed to account for the presence in the emailsubject line and/or headlines in the email body. wi1: presence in the subject line and head-line wi2: presence in the subject line wi3: presence in headlines where wi1> wi2> wi3.These weights were manually chosen after a setof experiments, but we plan to use a regressionmethod to automatically learn them.2.3 Symbolic Machine Learning ModelsWe compared three symbolic machine learningparadigms (decision trees, rule induction and de-cision forests) applied to the task of salient NPextraction, evaluating five classifiers.2.3.1 Decision Tree ClassifiersDecision trees classify instances represented asfeature vectors, where internal nodes of the treetest one or several attributes of the instance andwhere the leaves represent categories.
Dependingon how the test is performed at each node, thereexists two types of decision tree classifiers: axisparallel and oblique.
The axis-parallel decisiontrees check at each node the value of a single at-tribute.
If the attributes are numeric, the test hasthe form xi> t, where xiis one of the attributeof an instance and t is the threshold.
Oblique de-cision trees test a linear combination of attributesat each internal node:nXi=1aixi+ an+1> 0where ai; :::; an+1are real-valued coefficients.We compared the performance of C4.5, an axis-parallel decision tree classifier (Quinlan, 1993)and OC1, an oblique decision tree classifier(Murthy et al, 1993).2.3.2 Rule Induction ClassifiersIn rule induction, the goal is to learn the small-est set of rules that capture all the generalisableknowledge within the data.
Rule induction clas-sification is based on firing rules on a new in-stance, triggered by the matching feature valuesto the left-hand side of the rules.
Rules can be ofvarious normal forms and can be ordered.
How-ever, the appropriate ordering can be hard to findand the key point of many rule induction algo-rithms is to minimize the search strategy throughthe space of possible rule sets and orderings.
Forour task, we test the effectiveness of two ruleinduction algorithms : C4.5rules that form pro-duction rules from unpruned decision tree, anda fast top-down propositional rule learning sys-tem, RIPPER (Cohen, 1995).
Both algorithmsfirst construct an initial model and then iterativelyimprove it.
C4.5rules improvement strategy is agreedy search, thus potentially missing the bestrule set.
Furthermore, as discussed in (Cohen,1995), for large noisy datasets RIPPER starts withan initial model of small size, while C4.5rulesstarts with an over-large initial model.
This meansthat RIPPER?s search is more efficient for noisydatasets and thus is more appropriate for our datacollection.
It also allows the user to specify theloss ratio, which indicates the ratio of the cost offalse positives to the cost of false negatives, thusallowing a trade off between precision and recall.This is crucial for our analysis since we deal withsparse data due to the fact that in a document thenumber of salient NPs is much smaller than thenumber of irrelevant NPs.2.3.3 Decision Forest ClassifierDecision forests are a collection of decisiontrees together with a combination function.
Wetest the performance of DFC (Ho, 1998), a deci-sion forest classifier that systematically constructsdecision trees by pseudo-randomly selecting sub-sets of components of feature vectors.
The advan-tage of this classifier is that it combines a set ofdifferent classifiers in order to improve accuracy.It implements different splitting functions.
In thesetting of our evaluation we tested the informa-tion gain ratio (similar to the one used by Quinlanin C4.5).
An augmented feature vector (pairwisesums, differences, and products of features) wasused for this classifier.3 Linguistic Knowledge EnhancesMachine LearningNot all simple noun phrases are equallyimportant to reflect document meaning.Boguraev and Kennedy (1999) discuss theissue that for the task of document gisting, topicalnoun phrases are usually noun-noun compounds.In our work, we rely on ML techniques to decidewhich are the salient NPs, but we claim that ashallow linguistic filtering applied before thelearning process improves the accuracy of theclassifiers.
We performed four filtering steps:1.
Inflectional morphological processing:Grouping inflectional variants together canhelp especially in case of short documents(which is sometimes the case for emailmessages).
English nouns have only twokinds of regular inflection: a suffix forthe plural mark and another suffix for thepossessive one.2.
Removing unimportant modifiers: In thissecond step we remove the determiners thataccompany the nouns and also the auxil-iary words most and more that form the pe-riphrastic forms of comparative and superla-tive adjectives modifying the nouns (e.g.
?the most complex morphology?
will be fil-tered to ?complex morphology?).3.
Removing common words: We used a listof 571 common words used in IR systemsin order to further filter the list of candi-date NPs.
Thus, words like even, following,every, are eliminated from the noun phrasestructure.4.
Removing empty nouns: Words like lot,group, set, bunch are considered emptyheads.
For example the primary concept ofthe noun phrases like ?group of students?,?lots of students?
or ?bunch of students?is given by the noun ?students?.
We ex-tracted all the nouns that appear in front ofthe preposition ?of?
and then sorted them byfrequency of appearance.
A threshold wasthen used to select the final list (Klavans etal., 1990).
Three different data collectionswere used: the Brown corpus, the Wall StreetJournal, and a set of 4000 email messages(most of them related to a conference orga-nization).
We generated a set of 141 emptynouns that we used in this forth step of thefiltering process.4 Results and DiscussionOne important step in summarization is the dis-covery of the relevant information from the sourcetext.
Our approach was to extract the salient NPsusing linguistic knowledge and machine learningtechniques.
Our evaluation corpus consists of acollection of email messages which is heteroge-neous in genre, length, and topic.
We used 2,500NPs extracted from 51 email messages as a train-ing set and 324 NPs from 8 messages for testing.Each NP was manually tagged for saliency by onehuman judge.
We are planning to add more judgesin the future and measure the interuser agreement.This section outlines a comparative evaluationof five classifiers using two feature settings on thetask of extracting salient NPs from email mes-sages.
The evaluation shows the following im-portant results:Result 1.
In the context of gisting, the head-modifier relationship is an ordered relation be-tween semantically equal elements.We evaluate the impact of adding mh tfidf (seesection 2.2), as an additional feature in the featurevector.
This is shown in Table 2 in the differentfeature vectors fv1 and fv2.
The first feature vec-tor, fv1, contains the features in sections 2.2.1 and2.2.2, while fv2 includes as an additional featuremh tfidf.As can be seen from Table 3, the results of eval-uating these two feature settings using five differ-ent classifiers, show that fv2 performed better thanfv1.
For example, the DFC classifier shows an in-crease both in precision and recall.
This allows usto claim that in the context of gisting, the syntactichead of the noun phrase is not always the seman-tic head, and modifiers can have also an importantrole.One advantage of the rule-induction algorithmsis that their output is easily interpretable by hu-mans.
Analyzing C4.5rules output, we gain aninsight on the features that contribute most in theclassification process.
In case of fv1, the most im-portant features are: the first appearance of theNP and its head (np focc, head focc), the lengthof NP in number of words (np length words) andthe tf*idf measure of the whole NP and its head(np tfidf, head tfidf ).
For example: IF head focc <= 0.0262172 AND np tfidf> 0.0435465 THEN Relevant IF np focc <= 0.912409 ANDnp length words > 0.0242424 THENRelevant IF head tfidf <= 0.0243452 AND np tfidf<= 0.0435465 AND np length words <=0.0242424 then Not relevantIn case of fv2, the new feature m tfidf impactsthe rules for both Relevant and Not relevant cat-egories.
It supercedes the need for np tfidf andhead tfidf, as can be seen also from the rules be-low: IF mh tfidf > 0.0502262 AND np focc <=0.892585 THEN Relevant IF mh tfidf > 0.0180134 ANDnp length words > 0.0260708 THENRelevant IF mh tfidf <= 0.0223546 ANDnp length words <= 0.0260708 THENNot relevant IF mh tfidf <= 0.191205 AND np focc >0.892585 THEN Not relevantFeature vector 1 (fv1)head focc head tfidf np focc np tfidf np length chars np length words par pos sent posFeature vector 2 (fv2)head focc head tfidf mh tfidf np focc np tfidf np length chars np length words par pos sent posTable 2: Two feature settings to evaluate the impact of mh tfidfC4.5 OC1 C4.5 rules Ripper DFCp r p r p r p r p rfv1 73.3% 78.6% 73.7% 93% 73.7% 88.5% 83.6% 71.4% 80.3% 83.5%fv2 70% 88.9% 82.3% 88% 73.7% 95% 85.7% 78.8% 85.7% 87.9%Table 3: Evaluation of two feature vectors using five classifiersResult 2.
Classifiers?
performance dependson the characteristics of the corpus, and com-bining classifiers improves accuracyThis result was postulated by evaluating theperformance of five different classifiers in the taskof extracting salient noun phrases.
As measuresof performance we use precision and recall .
Theevaluation was performed according to what de-gree the output of the classifiers corresponds tothe user judgments and the results are presentedin Table 3.We first compare two decision tree classifiers:one which uses as the splitting function only a sin-gle feature (C4.5) and the other, the oblique treeclassifier (OC1) which at each internal node testsa linear combination of features.
Table 3 showsthat OC1 outperforms C4.5.Columns 4 and 5 from Table 3 show the rela-tive performance of RIPPER and C4.5rules.
Asdiscussed in (Cohen, 1995), RIPPER is more ap-propriate for noisy and sparse data collection thanC4.5rules.
Table 3 shows that RIPPER performsbetter than C4.5rules in terms of precision.Finally, we investigate whether a combinationof classifiers will improve performance.
Thus wechoose the Decision Forest Classifier, DFC, toperform our test.
DFC obtains the best results,as can be seen from column 6 of Table 3.Result 3.
Linguistic filtering is an importantstep in extracting salient NPsAs seen from Result 2, the DFC performed bestin our task, so we chose only this classifier topresent the impact of linguistic filtering.
Table4 shows that linguistic filtering improves preci-sion and recall, having an important role espe-cially on fv2, where the new feature, mh tfidf wasused (from 69.2% precision and 56.25% recall to85.7% precision and 87.9% recall).without filtering with filteringprecision recall precision recallfv1 75% 75% 80.3% 83.5%fv2 69.2% 56.25% 85.7% 87.9%Table 4: Evaluation of linguistic filteringThis is explained by the fact that the filter-ing presented in section 3 removed the noise in-troduced by unimportant modifiers, common andempty nouns.Result 4.
Noun phrases are better candi-dates than n-gramsPresenting the gist of an email message byphrase extraction addresses one obvious question:are noun-phrases better than n-grams for repre-senting the document content?
To answer thisquestion we compared the results of our system,GIST-IT, that extracts linguistically well moti-vated phrasal units, with KEA output, that ex-tracts bigrams and trigrams as key phrases usinga Na?ive Bayes model (Witten et al, 1999).
Table5 shows the results on one email message.
Then-gram approach of KEA system extracts phraseslike sort of batch, extracting lots, wn, and evenURLs that are unlikely to represent the gist of adocument.
This is an indication that the linguis-tically motivated GIST-IT phrases are more use-ful for document gisting.
In future work we willperform also a task-based evaluation of these twoGIST-IT KEAperl module wordnet interface module?wn?
command line program sort of batchsimple easy perl interface WordNet datawordnet.pm module accesses the WordNetwordnet system lots of WordNetquery perl module WordNet perlwordnet QueryDatawordnet package wnwordnet relation perl modulecommand line extractingwordnet data use this moduleincluded man page extracting lotsfree software WordNet systemquerydata www.cogsci.princeton.eduTable 5: Salient phrase extraction with GIST-IT vs. KEA on one email messageapproaches, to test usability.5 Related WorkMachine learning has been successfully appliedto different natural language tasks, including textsummarization.
A document summary is seenas a succinct and coherent prose that capturesthe meaning of the text.
Prior work in docu-ment summarization has been mostly based onsentence extraction.
Kupiec et al (1995) use ma-chine learning for extracting the most impor-tant sentences of the document.
But extrac-tive summarization relies on the properties ofsource text that emails typically do not have:coherence, grammaticality, well defined struc-ture.
Berger and Mittal (2000) present a summa-rization system, named OCELOT that providesthe gist of the web documents based on proba-bilistic models.
Their approach is closed relatedwith statistical machine translation.As discussed in (Boguraev and Kennedy,1999), the meaning of ?summary?
should be ad-justed depending on the information managementtask for which it is used.
Key phrases, for ex-ample, can be seen as semantic metadata thatsummarize and characterize documents (Wittenet al, 1999; Turney, 2000).
These approachesselect a set of candidate phrases (bigrams or tri-grams) and then apply Na?ive Bayes learning toclassify them as key phrases or not.
But deal-ing only with n-grams does not always providegood output in terms of a summary.
In (Bogu-raev and Kennedy, 1999) the ?gist?
of a documentis seen as a sequence of salient objects, usuallytopical noun phrases, presented in a highlightedcontext.
Their approach is similar to extractingtechnical terms (Justeson and Katz, 1995).
Nounphrases are used also in IR task (Strzalkowski etal., 1999; Smeaton, 1999; Sparck Jones, 1999).The work of Strzalkowski et al (1999) supportsour hypothesis that for some NLP tasks (gisting,IR) the head+modifier relation of a noun phrase isin fact an ordered relation between semanticallyequally important elements.6 Conclusions and Future WorkIn this paper we presented a novel technique fordocument gisting suitable for domain and genreindependent collections such as email messages.The method extracts simple noun phrases usinglinguistic techniques and then uses machine learn-ing to classify them as salient for the documentcontent.
The contributions of this work are:1.
From a linguistic standpoint, we demon-strated that the modifiers of a noun phrasecan be as semantically important as the headfor the task of gisting.2.
From a machine learning standpoint, weevaluated the power and limitation of sev-eral classifiers: decision trees, rule induc-tion, and decision forests classifiers.3.
We proved that linguistic knowledge can en-hance machine learning by evaluating theimpact of linguistic filtering before applyingthe learning scheme.The study, the evaluation, and the results pro-vide experimental grounds for research not onlyin summarization, but also in information extrac-tion and topic detection.ReferencesA.L.
Berger and V.O.
Mittal.
2000.
OCELOT:A sys-tem for summarizing web pages.
In Proceedings ofthe 23rd Anual International ACM SIGIR Confer-ence on Research and Development in InformationRetrieval, pages 144?151, Athens, Greece.B.
Boguraev and C. Kennedy.
1999.
Salience-basedcontent characterisation of text documents.
In In-terjit Mani and T. Maybury, Mark, editors, Ad-vances in Automatic Text Summarization, pages 99?111.
The MIT Press.W.
Cohen.
1995.
Fast effective rule induction.
InMachine-Learning: Proceedings of the Twelfth In-ternational Conference.T.K.
Ho.
1998.
The random subspace methodfor constructing decision forests.
IEEE Transac-tions on Pattern Analysis and Machine Intelligence,20(8):832?844.J.
Justeson and S. Katz.
1995.
Technical terminol-ogy: Some linguistic properties and an algorithmfor identification in text.
Natural Language Engi-neering, (1):9?27.J.L.
Klavans, M.S.
Chodorow, and N. Wacholder.1990.
From dictionary to knowledge base viataxonomy.
In Proceedings of the Sixth Confer-ence of the University of Waterloo Centre for theNew Oxford English Dictionary and Text Research:Electronic Text Research, University of Waterloo,Canada.J.
Kupiec, J. Pedersen, and F. Chen.
1995.
A train-able document summarizer.
In Proceedings on the18th Annual International ACM SIGIR Conferenceon Research and Development in Information Re-trieval, pages 68?73, Seattle,WA.R.J Mooney and C. Cardie.
1999.
Symbolic ma-chine learning for natural language processing.
InACL?99 Tutorial.S.K.
Murthy, S. Kasif, S. Salzberg, and R. Beigel.1993.
OC1: Randomized induction of oblique de-cision trees.
In Proceedings of the Eleventh Na-tional Conference on Artificial Intelligence, pages322?327, Washington, D.C.J.R Quinlan.
1993.
C4.5: Program for MachineLearning.
Morgan Kaufmann Publisher, San Ma-teo, California.L.A.
Ramshaw and M.P.
Marcus.
1995.
Text chunk-ing using transformation-based learning.
In Pro-ceedings of Third ACL Workshop on Very LargeCorpora.A.
Smeaton.
1999.
Using NLP or NLP resourcesfor information retrieval tasks.
In Tomek Strza-lkowski, editor, Natural Language Information Re-trieval.
Kluwer, Boston, MA.K.
Sparck Jones.
1999.
What is the role for NLP intext retrieval.
In Tomek Strzalkowski, editor, Nat-ural Language Information Retrieval, pages 1?12.Kluwer, Boston, MA.T.
Strzalkowski, F. Lin, J. Wang, and J. Perez-Carballo.
1999.
Evaluating natural language pro-cessing techniques in information retrieval.
InTomek Strzalkowski, editor, Natural Language In-formation Retrieval.
Kluwer, Boston, MA.P.D.
Turney.
2000.
Learning algorithms for keyphraseextraction.
Information Retrieval, 2(4):303?336,May.E Tzoukermann, S Muresan, and J.L.
Klavans.2001.
GIST-IT: Summarizing email using linguis-tic knowledge and machine learning.
In Proceedingof the HLT and KM Workshop, EACL/ACL 2001.N.
Wacholder.
1998.
Simplex NPS sorted by head:A method for identifying significant topics withina document.
In Proceedings of the COLING-ACLWorkshop on the Computational Treatment of Nom-inals, Montreal, Canada.I.H.
Witten, G.W.
Paynter, E. Frank, C. Gutwin, andC.G.
Nevill-Manning.
1999.
KEA: Practical au-tomatic keyphrase extraction.
In Proceedings ofDL?99, pages 254?256.
