Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 46?54,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsLearning Better Monolingual Models with Unannotated Bilingual TextDavid Burkett?
Slav Petrov?
John Blitzer?
Dan Klein?
?University of California, Berkeley ?Google Research{dburkett,blitzer,klein}@cs.berkeley.edu slav@google.comAbstractThis work shows how to improve state-of-the-artmonolingual natural language processing modelsusing unannotated bilingual text.
We build a mul-tiview learning objective that enforces agreementbetween monolingual and bilingual models.
Inour method the first, monolingual view consists ofsupervised predictors learned separately for eachlanguage.
The second, bilingual view consists oflog-linear predictors learned over both languageson bilingual text.
Our training procedure estimatesthe parameters of the bilingual model using theoutput of the monolingual model, and we show howto combine the two models to account for depen-dence between views.
For the task of named entityrecognition, using bilingual predictors increases F1by 16.1% absolute over a supervised monolingualmodel, and retraining on bilingual predictionsincreases monolingual model F1 by 14.6%.
Forsyntactic parsing, our bilingual predictor increasesF1 by 2.1% absolute, and retraining a monolingualmodel on its output gives an improvement of 2.0%.1 IntroductionNatural language analysis in one language can beimproved by exploiting translations in another lan-guage.
This observation has formed the basis forimportant work on syntax projection across lan-guages (Yarowsky et al, 2001; Hwa et al, 2005;Ganchev et al, 2009) and unsupervised syntaxinduction in multiple languages (Snyder et al,2009), as well as other tasks, such as cross-lingualnamed entity recognition (Huang and Vogel, 2002;Moore, 2003) and information retrieval (Si andCallan, 2005).
In all of these cases, multilingualmodels yield increased accuracy because differ-ent languages present different ambiguities andtherefore offer complementary constraints on theshared underlying labels.In the present work, we consider a setting wherewe already possess supervised monolingual mod-els, and wish to improve these models using unan-notated bilingual parallel text (bitext).
We cast thisproblem in the multiple-view (multiview) learningframework (Blum and Mitchell, 1998; Collins andSinger, 1999; Balcan and Blum, 2005; Ganchev etal., 2008).
Our two views are a monolingual view,which uses the supervised monolingual models butnot bilingual information, and a bilingual view,which exploits features that measure agreementacross languages.
The parameters of the bilin-gual view are trained to reproduce the output ofthe monolingual view.
We show that by introduc-ing weakened monolingual models into the bilin-gual view, we can optimize the parameters of thebilingual model to improve monolingual models.At prediction time, we automatically account forthe between-view dependence introduced by theweakened monolingual models with a simple buteffective view-combination heuristic.We demonstrate the performance of this methodon two problems.
The first is named en-tity recognition (NER).
For this problem, ourmethod automatically learns (a variation on) ear-lier hand-designed rule-based bilingual NER pre-dictors (Huang and Vogel, 2002; Moore, 2003),resulting in absolute performance gains of up to16.1% F1.
The second task we consider is statis-tical parsing.
For this task, we follow the setupof Burkett and Klein (2008), who improved Chi-nese and English monolingual parsers using par-allel, hand-parsed text.
We achieve nearly iden-tical improvements using a purely unlabeled bi-text.
These results carry over to machine transla-tion, where we can achieve slightly better BLEUimprovements than the supervised model of Bur-kett and Klein (2008) since we are able to trainour model directly on the parallel data where weperform rule extraction.Finally, for both of our tasks, we use our bilin-gual model to generate additional automaticallylabeled monolingual training data.
We compare46this approach to monolingual self-training andshow an improvement of up to 14.4% F1 for entityrecognition.
Even for parsing, where the bilingualportion of the treebank is much smaller than themonolingual, our technique still can improve overpurely monolingual self-training by 0.7% F1.2 Prior Work on Learning fromBilingual TextPrior work in learning monolingual models frombitexts falls roughly into three categories: Unsu-pervised induction, cross-lingual projection, andbilingual constraints for supervised monolingualmodels.
Two recent, successful unsupervisedinduction methods are those of Blunsom et al(2009) and Snyder et al (2009).
Both of them es-timate hierarchical Bayesian models and employbilingual data to constrain the types of models thatcan be derived.
Projection methods, on the otherhand, were among the first applications of paralleltext (after machine translation) (Yarowsky et al,2001; Yarowsky and Ngai, 2001; Hwa et al, 2005;Ganchev et al, 2009).
They assume the existenceof a good, monolingual model for one languagebut little or no information about the second lan-guage.
Given a parallel sentence pair, they use theannotations for one language to heavily constrainthe set of possible annotations for the other.Our work falls into the final category: We wishto use bilingual data to improve monolingual mod-els which are already trained on large amounts ofdata and effective on their own (Huang and Vo-gel, 2002; Smith and Smith, 2004; Snyder andBarzilay, 2008; Burkett and Klein, 2008).
Proce-durally, our work is most closely related to thatof Burkett and Klein (2008).
They used an an-notated bitext to learn parse reranking models forEnglish and Chinese, exploiting features that ex-amine pieces of parse trees in both languages.
Ourmethod can be thought of as the semi-supervisedcounterpart to their supervised model.
Indeed, weachieve nearly the same results, but without anno-tated bitexts.
Smith and Smith (2004) considera similar setting for parsing both English and Ko-rean, but instead of learning a joint model, theyconsider a fixed combination of two parsers anda word aligner.
Our model learns parameters forcombining two monolingual models and poten-tially thousands of bilingual features.
The resultis that our model significantly improves state-of-the-art results, for both parsing and NER.3 A Multiview Bilingual ModelGiven two input sentences x = (x1, x2) thatare word-aligned translations of each other, weconsider the problem of predicting (structured)labels y = (y1, y2) by estimating conditionalmodels on pairs of labels from both languages,p(y1, y2|x1, x2).
Our model consists of two views,which we will refer to as monolingual and bilin-gual.
The monolingual view estimates the jointprobability as the product of independent marginaldistributions over each language, pM (y|x) =p1(y1|x1)p2(y2|x2).
In our applications, thesemarginal distributions will be computed by state-of-the-art statistical taggers and parsers trained onlarge monolingual corpora.This work focuses on learning parameters forthe bilingual view of the data.
We parameterizethe bilingual view using at most one-to-one match-ings between nodes of structured labels in eachlanguage (Burkett and Klein, 2008).
In this work,we use the term node to indicate a particular com-ponent of a label, such as a single (multi-word)named entity or a node in a parse tree.
In Fig-ure 2(a), for example, the nodes labeled NP1 inboth the Chinese and English trees are matched.Since we don?t know a priori how the componentsrelate to one another, we treat these matchings ashidden.
For each matching a and pair of labelsy, we define a feature vector ?
(y1, a, y2) whichfactors on edges in the matching.
Our model isa conditional exponential family distribution overmatchings and labels:p?
(y, a|x) = exp[?>?
(y1, a, y2)?A(?
;x)],where ?
is a parameter vector, and A(?
;x) is thelog partition function for a sentence pair x. Wemust approximate A(?
;x) because summing overall at most one-to-one matchings a is #P-hard.
Weapproximate this sum using the maximum-scoringmatching (Burkett and Klein, 2008):A?(?
;x) = log?ymaxa(exp[?>?
(y1, a, y2)]).In order to compute the distribution on labels y, wemust marginalize over hidden alignments betweennodes, which we also approximate by using themaximum-scoring matching:q?
(y|x)def= maxaexp[?>?
(y1, a, y2)?A?(?
;x)].47the reports of European CourtORG1of Auditorsdie Berichte des Europ?ischen RechnungshofesORG1theFigure 1: An example where English NER can beused to disambiguate German NER.We further simplify inference in our model byworking in a reranking setting (Collins, 2000;Charniak and Johnson, 2005), where we only con-sider the top k outputs from monolingual modelsin both languages, for a total of k2 labels y. Inpractice, k2 ?
10, 000 for our largest problem.3.1 Including Weakened ModelsNow that we have defined our bilingual model, wecould train it to agree with the output of the mono-lingual model (Collins and Singer, 1999; Ganchevet al, 2008).
As we will see in Section 4, however,the feature functions ?
(y1, a, y2) make no refer-ence to the input sentences x, other than through afixed word alignment.
With such limited monolin-gual information, it is impossible for the bilingualmodel to adequately capture all of the informationnecessary for NER or parsing.
As a simple ex-ample, a bilingual NER model will be perfectlyhappy to label two aligned person names as ORGinstead of PER: both labelings agree equally well.We briefly illustrate how poorly such a basic bilin-gual model performs in Section 10.One way to solve this problem is to include theoutput of the full monolingual models as featuresin the bilingual view.
However, we are training thebilingual view to match the output of these samemodels, which can be trivially achieved by puttingweight on only the monolingual model scores andnever recruiting any bilingual features.
There-fore, we use an intermediate approach: we intro-duce the output of deliberately weakened mono-lingual models as features in the bilingual view.A weakened model is from the same class as thefull monolingual models, but is intentionally crip-pled in some way (by removing feature templates,for example).
Crucially, the weakened models willmake predictions that are roughly similar to thefull models, but systematically worse.
Therefore,model scores from the weakened models provideenough power for the bilingual view to make accu-Feat.
types ExamplesAlgn Densty INSIDEBOTH=3 INENONLY=0Indicators LBLMATCH=true BIAS=trueTable 1: Sample features used for named entityrecognition for the ORG entity in Figure 1.rate predictions, but ensure that bilingual featureswill be required to optimize the training objective.Let `W1 = log pW1 (y1|x1), `W2 = log pW2 (y2|x2)be the log-probability scores from the weakenedmodels.
Our final approximation to the marginaldistribution over labels y is:q?1,?2,?
(y|x)def= maxaexph?1`W1 + ?2`W2 +?>?
(y1, a, y2)?
A?
(?1, ?2,?;x)i.(1)WhereA?
(?1, ?2,?
;x) =logXymaxaexph?1`W1 + ?2`W2 + ?>?
(y1, a, y2)iis the updated approximate log partition function.4 NER and Parsing ExamplesBefore formally describing our algorithm for find-ing the parameters [?1, ?2,?
], we first give exam-ples of our problems of named entity recognitionand syntactic parsing, together with node align-ments and features for each.
Figure 1 depicts acorrectly-labeled sentence fragment in both En-glish and German.
In English, the capitalization ofthe phrase European Court of Auditors helps iden-tify the span as a named entity.
However, in Ger-man, all nouns are capitalized, and capitalizationis therefore a less useful cue.
While a monolin-gual German tagger is likely to miss the entity inthe German text, by exploiting the parallel Englishtext and word alignment information, we can hopeto improve the German performance, and correctlytag Europa?ischen Rechnungshofes.The monolingual features are standard featuresfor discriminative, state-of-the-art entity recogniz-ers, and we can produce weakened monolingualmodels by simply limiting the feature set.
Thebilingual features, ?
(y1, a, y2), are over pairs ofaligned nodes, where nodes of the labels y1 andy2 are simply the individual named entities.
Weuse a small bilingual feature set consisting of twotypes of features.
First, we use the word alignmentdensity features from Burkett and Klein (2008),which measure how well the aligned entity pairmatches up with alignments from an independent48Input: full and weakened monolingual models:p1(y1|x1), p2(y2|x2), pw1 (y1|x1), pw2 (y2|x2)unannotated bilingual data: UOutput: bilingual parameters: ?
?, ?
?1, ??21.
Label U with full monolingual models:?x ?
U, y?M = argmaxy p1(y1|x1)p2(y2|x2).2.
Return argmax?1,?2,?Qx?U q?,?1,?2 (y?M |x),where q?,?1,?2 has the form in Equation 1.Figure 3: Bilingual training with multiple views.word aligner.
We also include two indicator fea-tures: a bias feature that allows the model to learna general preference for matched entities, and afeature that is active whenever the pair of nodeshas the same label.
Figure 1 contains sample val-ues for each of these features.Another natural setting where bilingual con-straints can be exploited is syntactic parsing.
Fig-ure 2 shows an example English prepositionalphrase attachment ambiguity that can be resolvedbilingually by exploiting Chinese.
The Englishmonolingual parse mistakenly attaches to to theverb increased.
In Chinese, however, this ambi-guity does not exist.
Instead, the word ?, whichaligns to to, has strong selectional preference forattaching to a noun on the left.In our parsing experiments, we use the Berke-ley parser (Petrov et al, 2006; Petrov and Klein,2007), a split-merge latent variable parser, for ourmonolingual models.
Our full model is the re-sult of training the parser with five split-mergephases.
Our weakened model uses only two.
Forthe bilingual model, we use the same bilingual fea-ture set as Burkett and Klein (2008).
Table 2 givessome examples, but does not exhaustively enumer-ate those features.5 Training Bilingual ModelsPrevious work in multiview learning has focusedon the case of agreement regularization (Collinsand Singer, 1999; Ganchev et al, 2008).
If we hadbilingual labeled data, together with our unlabeleddata and monolingual labeled data, we could ex-ploit these techniques.
Because we do not possessbilingual labeled data, we must train the bilingualmodel in another way.
Here we advocate train-ing the bilingual model (consisting of the bilin-gual features and weakened monolingual models)to imitate the full monolingual models.
In termsof agreement regularization, our procedure may bethought of as ?regularizing?
the bilingual model tobe similar to the full monolingual models.Input: full and weakened monolingual models:p1(y1|x1), p2(y2|x2), pw1 (y1|x1), pw2 (y2|x2)bilingual parameters: ?
?, ?
?1, ?
?2bilingual input: x = (x1, x2)Output: bilingual label: y?Bilingual w/ Weak Bilingual w/ Full1a.
l1 = log`pw1 (y1|x1)?1b.
l1 = log`p1(y1|x1)?2a.
l2 = log`pw2 (y2|x2)?2b.
l2 = log`p2(y2|x2)?3.
Return argmaxy maxa ?
?1l1 + ??2l2+??>?
(y1, a, y2)Figure 4: Prediction by combining monolingualand bilingual models.Our training algorithm is summarized in Fig-ure 3.
For each unlabeled point x = (x1, x2), lety?M be the joint label which has the highest scorefrom the independent monolingual models (line1).
We then find bilingual parameters ?
?, ?
?1, ?
?2that maximize q??,??1,?
?2(y?x|x) (line 2).
This max-likelihood optimization can be solved by an EM-like procedure (Burkett and Klein, 2008).
Thisprocedure iteratively updates the parameter esti-mates by (a) finding the optimum alignments foreach candidate label pair under the current pa-rameters and then (b) updating the parameters tomaximize a modified version of Equation 1, re-stricted to the optimal alignments.
Because we re-strict alignments to the set of at most one-to-onematchings, the (a) step is tractable using the Hun-garian algorithm.
With the alignments fixed, the(b) step just involves maximizing likelihood undera log-linear model with no latent variables ?
thisproblem is convex and can be solved efficientlyusing gradient-based methods.
The procedure hasno guarantees, but is observed in practice to con-verge to a local optimum.6 Predicting with Monolingual andBilingual ModelsOnce we have learned the parameters of the bilin-gual model, the standard method of bilingual pre-diction would be to just choose the y that is mostlikely under q??,??1,?
?2 :y?
= argmaxyq??,??1,?
?2(y|x) .
(2)We refer to prediction under this model as ?Bilin-gual w/ Weak,?
to evoke the fact that the model ismaking use of weakened monolingual models inits feature set.Given that we have two views of the data,though, we should be able to leverage additionalinformation in order to make better predictions.
In49VBNP1NPVPSThese measures increased the attractiveness of Tianjin to Taiwanese merchants(a)NP PP PPThese measures increased the attractiveness of Tianjin to Taiwanese merchantsVBNPNPVP1SNP PP PP??
?
??
?
?
?
??
?
??
??
?SNPVB NNPPPDE NNNP1VP??
?
??
?
?
?
??
?
??
??
?SNPVB NNPPPDE NNNP1VP(b)Figure 2: An example of PP attachment that is ambiguous in English, but simple in Chinese.
In (a) thecorrect parses agree (low PP attachment), whereas in (b) the incorrect parses disagree.Feature Types Feature TemplatesExamplesCorrect IncorrectAlignment Density INSIDEBOTH, INSIDEENONLY INSIDEENONLY=0 INSIDEENONLY=1Span Difference ABSDIFFERENCE ABSDIFFERENCE=3 ABSDIFFERENCE=4Syntactic Indicators LABEL?E,C?, NUMCHILDREN?E,C?
LABEL?NP,NP?=true LABEL?VP,NP?=trueTable 2: Sample bilingual features used for parsing.
The examples are features that would be extractedby aligning the parents of the PP nodes in Figure 2(a) (Correct) and Figure 2(b) (Incorrect).particular, the monolingual view uses monolingualmodels that are known to be superior to the mono-lingual information available in the bilingual view.Thus, we would like to find some way to incorpo-rate the full monolingual models into our predic-tion method.
One obvious choice is to choose thelabeling that maximizes the ?agreement distribu-tion?
(Collins and Singer, 1999; Ganchev et al,2008).
In our setting, this amounts to choosing:y?
= argmaxypM (y|x) q??,??1?
?2(y|x) .
(3)This is the correct decision rule if the views areindependent and the labels y are uniformly dis-tributed a priori,1 but we have deliberately in-troduced between-view dependence in the formof the weakened monolingual models.
Equa-tion 3 implicitly double-counts monolingual infor-mation.One way to avoid this double-counting is tosimply discard the weakened monolingual modelswhen making a joint prediction:y?
= argmaxymaxapM (y|x)exp[??>?
(y1, a, y2)].
(4)1See, e.g.
Ando & Zhang(Ando and Zhang, 2007) for aderivation of the decision rule from Equation 3 under theseassumptions.This decision rule uniformly combines the twomonolingual models and the bilingual model.Note, however, that we have already learned non-uniform weights for the weakened monolingualmodels.
Our final decision rule uses these weightsas weights for the full monolingual models:y?
= argmaxymaxaexp[?
?1 log(p1(y1|x1))+?
?2 log(p2(y2|x2))+??>?
(y1, a, y2)].
(5)As we will show in Section 10, this rule for com-bining the monolingual and bilingual views per-forms significantly better than the alternatives, andcomes close to the optimal weighting for the bilin-gual and monolingual models.We will refer to predictions made with Equa-tion 5 as ?Bilingual w/ Full?, to evoke the use ofthe full monolingual models alongside our bilin-gual features.
Prediction using ?Bilingual w/Weak?
and ?Bilingual w/ Full?
is summarized inFigure 4.7 Retraining Monolingual ModelsAlthough bilingual models have many direct ap-plications (e.g.
in machine translation), we alsowish to be able to apply our models on purelymonolingual data.
In this case, we can still take50Input: annotated monolingual data: L1, L2unannotated bilingual data: Umonolingual models: p1(y1|x1), p2(y2|x2)bilingual parameters: ?
?, ?
?1, ?
?2Output: retrained monolingual models:pr1(y1|x1), pr2(y2|x2)?x = (x1, x2) ?
U:Self-Retrained Bilingual-Retrained1a.
y?x1 = argmaxy1 p1(y1|x1) 1b.
Pick y?x, Fig.
4y?x2 = argmaxy2 p2(y2|x2) (Bilingual w/ Full)2.
Add (x1, y?x1 ) to L1 and add (x2, y?x2 ) to L2.3.
Return full monolingual models pr1(y1|x1),pr2(y2|x2) trained on newly enlarged L1, L2.Figure 5: Retraining monolingual models.advantage of parallel corpora by using our bilin-gual models to generate new training data for themonolingual models.
This can be especially use-ful when we wish to use our monolingual modelsin a domain for which we lack annotated data, butfor which bitexts are plentiful.2Our retraining procedure is summarized in Fig-ure 5.
Once we have trained our bilingual param-eters and have a ?Bilingual w/ Full?
predictor (us-ing Equation 5), we can use that predictor to an-notate a large corpus of parallel data (line 1b).
Wethen retrain the full monolingual models on a con-catenation of their original training data and thenewly annotated data (line 3).
We refer to the newmonolingual models retrained on the output of thebilingual models as ?Bilingual-Retrained,?
and wetested such models for both NER and parsing.
Forcomparison, we also retrained monolingual mod-els directly on the output of the original full mono-lingual models, using the same unannotated bilin-gual corpora for self-training (line 1a).
We refer tothese models as ?Self-Retrained?.We evaluated our retrained monolingual mod-els on the same test sets as our bilingual mod-els, but using only monolingual data at test time.The texts used for retraining overlapped with thebitexts used for training the bilingual model, butboth sets were disjoint from the test sets.8 NER ExperimentsWe demonstrate the utility of multiview learn-ing for named entity recognition (NER) on En-glish/German sentence pairs.
We built both ourfull and weakened monolingual English and Ger-man models from the CoNLL 2003 shared task2Of course, unannotated monolingual data is even moreplentiful, but as we will show, with the same amount of data,our method is more effective than simple monolingual self-training.training data.
The bilingual model parameterswere trained on 5,000 parallel sentences extractedfrom the Europarl corpus.
For the retrainingexperiments, we added an additional 5,000 sen-tences, for 10,000 in all.
For testing, we usedthe Europarl 2006 development set and the 2007newswire test set.
Neither of these data sets wereannotated with named entities, so we manually an-notated 200 sentences from each of them.We used the Stanford NER tagger (Finkel etal., 2005) with its default configuration as our fullmonolingual model for each language.
We weak-ened both the English and German models by re-moving several non-lexical and word-shape fea-tures.
We made one more crucial change to ourmonolingual German model.
The German entityrecognizer has extremely low recall (44 %) whenout of domain, so we chose y?x from Figure 3 tobe the label in the top five which had the largestnumber of named entities.Table 3 gives results for named entity recogni-tion.
The first two rows are the full and weak-ened monolingual models alone.
The second twoare the multiview trained bilingual models.
Wefirst note that for English, using the full bilin-gual model yields only slight improvements overthe baseline full monolingual model, and in prac-tice the predictions were almost identical.
For thisproblem, the monolingual German model is muchworse than the monolingual English model, and sothe bilingual model doesn?t offer significant im-provements in English.
The bilingual model doesshow significant German improvements, however,including a 16.1% absolute gain in F1 over thebaseline for parliamentary proceedings.The last two rows of Table 3 give results formonolingual models which are trained on data thatwas automatically labeled using the our models.English results were again mixed, due to the rel-atively weak English performance of the bilin-gual model.
For German, though, the ?Bilingual-Retrained?
model improves 14.4% F1 over the?Self-Retrained?
baseline.9 Parsing ExperimentsOur next set of experiments are on syntactic pars-ing of English and Chinese.
We trained both ourfull and weakened monolingual English modelson the Penn Wall Street Journal corpus (Marcuset al, 1993), as described in Section 4.
Our fulland weakened Chinese models were trained on51Eng Parliament Eng Newswire Ger Parliament Ger NewswirePrec Rec F1 Prec Rec F1 Prec Rec F1 Prec Rec F1Monolingual Models (Baseline)Weak Monolingual 52.6 65.9 58.5 67.7 83.0 74.6 71.3 36.4 48.2 80.0 51.5 62.7Full Monolingual 65.7 71.4 68.4 80.1 88.7 84.2 69.8 44.0 54.0 73.0 56.4 63.7Multiview Trained Bilingual ModelsBilingual w/ Weak 56.2 70.8 62.7 71.4 86.2 78.1 70.1 66.3 68.2 76.5 76.1 76.3Bilingual w/ Full 65.4 72.4 68.7 80.6 88.7 84.4 70.1 70.1 70.1 74.6 77.3 75.9Retrained Monolingual ModelsSelf-Retrained 71.7 74.0 72.9 79.9 87.4 83.5 70.4 44.0 54.2 79.3 58.9 67.6Bilingual-Retrained 68.6 70.8 69.7 80.7 89.3 84.8 74.5 63.6 68.6 77.9 69.3 73.4Table 3: NER Results.
Rows are grouped by data condition.
We bold all entries that are best in theirgroup and beat the strongest monolingual baseline.Chinese EnglishMonolingual Models (Baseline)Weak Monolingual 78.3 67.6Full Monolingual 84.2 75.4Multiview Trained Bilingual ModelsBilingual w/ Weak 80.4 70.8Bilingual w/ Full 85.9 77.5Supervised Trained Bilingual ModelsBurkett and Klein (2008) 86.1 78.2Retrained Monolingual ModelsSelf-Retrained 83.6 76.7Bilingual-Retrained 83.9 77.4Table 4: Parsing results.
Rows are grouped by datacondition.
We bold entries that are best in theirgroup and beat the the Full Monolingual baseline.the Penn Chinese treebank (Xue et al, 2002) (ar-ticles 400-1151), excluding the bilingual portion.The bilingual data consists of the parallel part ofthe Chinese treebank (articles 1-270), which alsoincludes manually parsed English translations ofeach Chinese sentence (Bies et al, 2007).
Onlythe Chinese sentences and their English transla-tions were used to train the bilingual models ?
thegold trees were ignored.
For retraining, we usedthe same data, but weighted it to match the sizesof the original monolingual treebanks.
We testedon the standard Chinese treebank development set,which also includes English translations.Table 4 gives results for syntactic parsing.
Forcomparison, we also show results for the super-vised bilingual model of Burkett and Klein (2008).This model uses the same features at predictiontime as the multiview trained ?Bilingual w/ Full?model, but it is trained on hand-annotated parses.We first examine the first four rows of Table 4.
The?Bilingual w/ Full?
model significantly improvesperformance in both English and Chinese relativeto the monolingual baseline.
Indeed, it performsPhrase-Based SystemMoses (No Parser) 18.8Syntactic SystemsMonolingual Parser 18.7Supervised Bilingual (Treebank Bi-trees) 21.1Multiview Bilingual (Treebank Bitext) 20.9Multiview Bilingual (Domain Bitext) 21.2Table 5: Machine translation results.only slightly worse than the supervised model.The last two rows of Table 4 are the results ofmonolingual parsers trained on automatically la-beled data.
In general, gains in English, whichis out of domain relative to the Penn Treebank,are larger than those in Chinese, which is in do-main.
We also emphasize that, unlike our NERdata, this bitext was fairly small relative to the an-notated monolingual data.
Therefore, while westill learn good bilingual model parameters whichgive a sizable agreement-based boost when doingbilingual prediction, we don?t expect retraining toresult in a coverage-based boost in monolingualperformance.9.1 Machine Translation ExperimentsAlthough we don?t have hand-labeled data for ourlargest Chinese-English parallel corpora, we canstill evaluate our parsing results via our perfor-mance on a downstream machine translation (MT)task.
Our experimental setup is as follows: first,we used the first 100,000 sentences of the English-Chinese bitext from Wang et al (2007) to trainMoses (Koehn et al, 2007), a phrase-based MTsystem that we use as a baseline.
We then used thesame sentences to extract tree-to-string transducerrules from target-side (English) trees (Galley et al,2004).
We compare the single-reference BLEUscores of syntactic MT systems that result fromusing different parsers to generate these trees.520.0 0.20.4 0.60.8 1.01.2 1.40.0 0.4 0.8 1.2 1.6 2.0 2.4 2.868-71 65-68 62-65 59-62 56-59English WeightGermanWeightGerman F170.3 70.1 59.1* + * +(a)0.0 0.20.4 0.60.8 1.01.2 1.40.0 0.2 0.4 0.6 0.8 1.0 1.2 1.481.8-82.1 81.5-81.8 81.2-81.5 80.9-81.2 80.6-80.9English WeightChinese WeightCombined F182.1 82.0 81.4* + ?
* +?
(b)Figure 6: (a) NER and (b) parsing results for different values of ?1 and ?2 (see Equation 6).
?*?
showsoptimal weights, ?+?
shows our learned weights, and ?-?
shows uniform combination weights.For our syntactic baseline, we used the mono-lingual English parser.
For our remaining experi-ments, we parsed both English and Chinese simul-taneously.
The supervised model and the first mul-tiview trained model are the same Chinese tree-bank trained models for which we reported pars-ing results.
We also used our multiview method totrain an additional bilingual model on part of thebitext we used to extract translation rules.The results are shown in Table 5.
Once again,our multiview trained model yields comparable re-sults to the supervised model.
Furthermore, whilethe differences are small, our best performancecomes from the model trained on in-domain data,for which no gold trees exist.10 Analyzing Combined PredictionIn this section, we explore combinations of the fullmonolingual models, p1(y1|x1) and p2(y2|x2),and the bilingual model, maxa??>?
(y1, a, y2).
Forparsing, the results in this section are for combinedF1.
This simply computes F1 over all of the sen-tences in both the English and Chinese test sets.For NER, we just use German F1, since English isrelatively constant across runs.We begin by examining how poorly our modelperforms if we do not consider monolingual in-formation in the bilingual view.
For parsing, thecombined Chinese and English F1 for this modelis 78.7%.
When we combine this model uniformlywith the full monolingual model, as in Equation 4,combined F1 improves to 81.2%, but is still wellbelow our best combined score of 82.1%.
NERresults for a model trained without monolingualinformation show an even larger decline.Now let us consider decision rules of the form:y?
= argmaxymaxaexp[?1 log`p1(y1|x1)?+?2 log`p2(y2|x2)?+??>?
(y1, a, y2)] .Note that when ?1 = ?2 = 1, this is exactlythe uniform decision rule (Equation 4).
When?1 = ?
?1 and ?2 = ?
?2, this is the ?Bilingual w/Full?
decision rule (Equation 5).
Figure 6 is acontour plot of F1 with respect to the parameters?1 and ?2.
Our decision rule ?Bilingual w/ Full?
(Equation 5, marked with a ?+?)
is near the opti-mum (?*?
), while the uniform decision rule (?-?
)performs quite poorly.
This is true for both NER(Figure 6a) and parsing (Figure 6b).There is one more decision rule which we haveyet to consider: the ?conditional independence?decision rule from Equation 3.
While this rule can-not be shown on the plots in Figure 6 (becauseit uses both the full and weakened monolingualmodels), we note that it also performs poorly inboth cases (80.7% F1 for parsing, for example).11 ConclusionsWe show for the first time that state-of-the-art,discriminative monolingual models can be signifi-cantly improved using unannotated bilingual text.We do this by first building bilingual models thatare trained to agree with pairs of independently-trained monolingual models.
Then we combinethe bilingual and monolingual models to accountfor dependence across views.
By automaticallyannotating unlabeled bitexts with these bilingualmodels, we can train new monolingual models thatdo not rely on bilingual data at test time, but stillperform substantially better than models trainedusing only monolingual resources.AcknowledgementsThis project is funded in part by NSF grants0915265 and 0643742, an NSF graduate researchfellowship, the DNI under grant HM1582-09-1-0021, and BBN under DARPA contract HR0011-06-C-0022.53ReferencesRie Kubota Ando and Tong Zhang.
2007.
Two-viewfeature generation model for semi-supervised learn-ing.
In ICML.Maria-Florina Balcan and Avrim Blum.
2005.
A pac-style model for learning from labeled and unlabeleddata.
In COLT.Ann Bies, Martha Palmer, Justin Mott, and ColinWarner.
2007.
English chinese translation treebankv 1.0.
Web download.
LDC2007T02.Avrim Blum and Tom Mitchell.
1998.
Combining la-beled and unlabeled data with co-training.
In COLT.Phil Blunsom, Trevor Cohn, and Miles Osborne.
2009.Bayesian synchronous grammar induction.
In NIPS.David Burkett and Dan Klein.
2008.
Two lan-guages are better than one (for syntactic parsing).
InEMNLP.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminativereranking.
In ACL.Michael Collins and Yoram Singer.
1999.
Unsuper-vised models for named entity classification.
InEMNLP.Michael Collins.
2000.
Discriminative reranking fornatural language parsing.
In ICML.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by Gibbssampling.
In ACL.Michel Galley, Mark Hopkins, Kevin Knight, andDaniel Marcu.
2004.
What?s in a translation rule?In HLT-NAACL.Kuzman Ganchev, Joao Graca, John Blitzer, and BenTaskar.
2008.
Multi-view learning over structuredand non-identical outputs.
In UAI.Kuzman Ganchev, Jennifer Gillenwater, and BenTaskar.
2009.
Dependency grammar induction viabitext projection constraints.
In ACL.Fei Huang and Stephan Vogel.
2002.
Improved namedentity translation and bilingual named entity extrac-tion.
In ICMI.Rebecca Hwa, Philip Resnik, Amy Weinberg, ClaraCabezas, and Okan Kolak.
2005.
Bootstrappingparsers via syntactic projection across parallel texts.Special Issue of the Journal of Natural LanguageEngineering on Parallel Texts, 11(3):311?325.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InACL.Mitchell P. Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a large anno-tated corpus of english: The penn treebank.
Compu-tational Linguistics, 19(2):313?330.Robert Moore.
2003.
Learning translations of named-entity phrases from parallel corpora.
In EACL.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In HLT-NAACL.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In COLING-ACL.Luo Si and Jamie Callan.
2005.
Clef 2005: Multi-lingual retrieval by combining multiple multilingualranked lists.
In CLEF.David A. Smith and Noah A. Smith.
2004.
Bilingualparsing with factored estimation: using english toparse korean.
In EMNLP.Benjamin Snyder and Regina Barzilay.
2008.
Cross-lingual propagation for morphological analysis.
InAAAI.Benjamin Snyder, Tahira Naseem, and Regina Barzi-lay.
2009.
Unsupervised multilingual grammar in-duction.
In ACL.Wen Wang, Andreas Stolcke, and Jing Zheng.
2007.Reranking machine translation hypotheses withstructured and web-based language models.
In IEEEASRU Workshop.Nianwen Xue, Fu-Dong Chiou, and Martha Palmer.2002.
Building a large-scale annotated chinese cor-pus.
In COLING.David Yarowsky and Grace Ngai.
2001.
Inducing mul-tilingual pos taggers and np bracketers via robustprojection across aligned corpora.
In NAACL.David Yarowsky, Grace Ngai, and Richard Wicen-towski.
2001.
Inducing multilingual text analysistools via robust projection across aligned corpora.In Human Language Technologies.54
