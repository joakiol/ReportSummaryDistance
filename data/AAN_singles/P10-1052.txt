Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 504?513,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsPractical very large scale CRFsThomas LavergneLIMSI ?
CNRSlavergne@limsi.frOlivier Cappe?Te?le?com ParisTechLTCI ?
CNRScappe@enst.frFranc?ois YvonUniversite?
Paris-Sud 11LIMSI ?
CNRSyvon@limsi.frAbstractConditional Random Fields (CRFs) area widely-used approach for supervisedsequence labelling, notably due to theirability to handle large description spacesand to integrate structural dependency be-tween labels.
Even for the simple linear-chain model, taking structure into accountimplies a number of parameters and acomputational effort that grows quadrati-cally with the cardinality of the label set.In this paper, we address the issue of train-ing very large CRFs, containing up to hun-dreds output labels and several billion fea-tures.
Efficiency stems here from the spar-sity induced by the use of a `1 penaltyterm.
Based on our own implementa-tion, we compare three recent proposalsfor implementing this regularization strat-egy.
Our experiments demonstrate thatvery large CRFs can be trained efficientlyand that very large models are able to im-prove the accuracy, while delivering com-pact parameter sets.1 IntroductionConditional Random Fields (CRFs) (Lafferty etal., 2001; Sutton and McCallum, 2006) constitutea widely-used and effective approach for super-vised structure learning tasks involving the map-ping between complex objects such as strings andtrees.
An important property of CRFs is their abil-ity to handle large and redundant feature sets andto integrate structural dependency between out-put labels.
However, even for simple linear chainCRFs, the complexity of learning and inferenceThis work was partly supported by ANR projects CroTaL(ANR-07-MDCO-003) and MGA (ANR-07-BLAN-0311-02).grows quadratically with respect to the number ofoutput labels and so does the number of structuralfeatures, ie.
features testing adjacent pairs of la-bels.
Most empirical studies on CRFs thus ei-ther consider tasks with a restricted output space(typically in the order of few dozens of output la-bels), heuristically reduce the use of features, es-pecially of features that test pairs of adjacent la-bels1, and/or propose heuristics to simulate con-textual dependencies, via extended tests on the ob-servations (see discussions in, eg., (Punyakanoket al, 2005; Liang et al, 2008)).
Limitating thefeature set or the number of output labels is how-ever frustrating for many NLP tasks, where thetype and number of potentially relevant featuresare very large.
A number of studies have tried toalleviate this problem.
Pal et al (2006) proposeto use a ?sparse?
version of the forward-backwardalgorithm during training, where sparsity is en-forced through beam pruning.
Related ideas arediscussed by Dietterich et al (2004); by Cohn(2006), who considers ?generalized?
feature func-tions; and by Jeong et al (2009), who use approx-imations to simplify the forward-backward recur-sions.
In this paper, we show that the sparsity thatis induced by `1-penalized estimation of CRFs canbe used to reduce the total training time, whileyielding extremely compact models.
The benefitsof sparsity are even greater during inference: lessfeatures need to be extracted and included in thepotential functions, speeding up decoding with alesser memory footprint.
We study and comparethree different ways to implement `1 penalty forCRFs that have been introduced recently: orthant-wise Quasi Newton (Andrew and Gao, 2007),stochastic gradient descent (Tsuruoka et al, 2009)and coordinate descent (Sokolovska et al, 2010),concluding that these methods have complemen-1In CRFsuite (Okazaki, 2007), it is even impossible tojointly test a pair of labels and a test on the observation, bi-grams feature are only of the form f(yt?1, yt).504tary strengths and weaknesses.
Based on an effi-cient implementation of these algorithms, we wereable to train very large CRFs containing more thana hundred of output labels and up to several billionfeatures, yielding results that are as good or betterthan the best reported results for two NLP bench-marks, text phonetization and part-of-speech tag-ging.Our contribution is therefore twofold: firstly adetailed analysis of these three algorithms, dis-cussing implementation, convergence and com-paring the effect of various speed-ups.
Thiscomparison is made fair and reliable thanks tothe reimplementation of these techniques in thesame software package.
Second, the experimen-tal demonstration that using large output label setsis doable and that very large feature sets actuallyhelp improve prediction accuracy.
In addition, weshow how sparsity in structured feature sets canbe used in incremental training regimes, wherelong-range features are progressively incorporatedin the model insofar as the shorter range featureshave proven useful.The rest of the paper is organized as follows: wefirst recall the basics of CRFs in Section 2, and dis-cuss three ways to train CRFs with a `1 penalty inSection 3.
We then detail several implementationissues that need to be addressed when dealing withmassive feature sets in Section 4.
Our experimentsare reported in Section 5.
The main conclusions ofthis study are drawn in Section 6.2 Conditional Random FieldsIn this section, we recall the basics of ConditionalRandom Fields (CRFs) (Lafferty et al, 2001; Sut-ton and McCallum, 2006) and introduce the nota-tions that will be used throughout.2.1 BasicsCRFs are based on the following modelp?
(y|x) =1Z?
(x)exp{K?k=1?kFk(x,y)}(1)where x = (x1, .
.
.
, xT ) and y = (y1, .
.
.
, yT )are, respectively, the input and output sequences2,and Fk(x,y) is equal to?Tt=1 fk(yt?1, yt, xt),where {fk}1?k?K is an arbitrary set of feature2Our implementation also includes a special label y0, thatis always observed and marks the beginning of a sequence.functions and {?k}1?k?K are the associated pa-rameter values.
We denote by Y and X , respec-tively, the sets in which yt and xt take their values.The normalization factor in (1) is defined byZ?
(x) =?y?Y Texp{K?k=1?kFk(x,y)}.
(2)The most common choice of feature functions is touse binary tests.
In the sequel, we distinguish be-tween two types of feature functions: unigram fea-tures fy,x, associated with parameters ?y,x, and bi-gram features fy?,y,x, associated with parameters?y?,y,x.
These are defined asfy,x(yt?1, yt, xt) = 1(yt = y, xt = x)fy?,y,x(yt?1, yt, xt) = 1(yt?1 = y?, yt = y, xt = x)where 1(cond.)
is equal to 1 when the conditionis verified and to 0 otherwise.
In this setting, thenumber of parametersK is equal to |Y |2?|X|train,where | ?
| denotes the cardinal and |X|train refers tothe number of configurations of xt observed dur-ing training.
Thus, even in moderate size applica-tions, the number of parameters can be very large,mostly due to the introduction of sequential de-pendencies in the model.
This also explains why itis hard to train CRFs with dependencies spanningmore than two adjacent labels.
Using only uni-gram features {fy,x}(y,x)?Y?X results in a modelequivalent to a simple bag-of-tokens position-by-position logistic regression model.
On theother hand, bigram features {fy?,y,x}(y,x)?Y 2?Xare helpful in modelling dependencies betweensuccessive labels.
The motivations for using si-multaneously both types of feature functions areevaluated experimentally in Section 5.2.2 Parameter EstimationGiven N independent sequences {x(i),y(i)}Ni=1,where x(i) and y(i) contain T (i) symbols, condi-tional maximum likelihood estimation is based onthe minimization, with respect to ?, of the negatedconditional log-likelihood of the observationsl(?)
= ?N?i=1log p?
(y(i)|x(i)) (3)=N?i=1{logZ?
(x(i))?K?k=1?kFk(x(i),y(i))}This term is usually complemented with an addi-tional regularization term so as to avoid overfitting505(see Section 3.1 below).
The gradient of l(?)
is?l(?)?
?k=N?i=1T (i)?t=1Ep?
(y|x(i)) fk(yt?1, yt, x(i)t )?N?i=1T (i)?t=1fk(y(i)t?1, y(i)t , x(i)t ) (4)where Ep?
(y|x) denotes the conditional expecta-tion given the observation sequence, i.e.Ep?
(y|x) fk(yt?1, yt, x(i)t ) =?
(y?,y)?Y 2fk(y, y?, xt) P?
(yt?1 = y?, yt = y|x) (5)Although l(?)
is a smooth convex function, its op-timum cannot be computed in closed form, andl(?)
has to be optimized numerically.
The com-putation of its gradient implies to repeatedly com-pute the conditional expectation in (5) for all in-put sequences x(i) and all positions t. The stan-dard approach for computing these expectationsis inspired by the forward-backward algorithm forhidden Markov models: using the notations intro-duced above, the algorithm implies the computa-tion of the forward{?1(y) = exp(?y,x1 + ?y0,y,x1)?t+1(y) =?y?
?t(y?)
exp(?y,xt+1 + ?y?,y,xt+1)and backward recursions{?Ti(y) = 1?t(y?)
=?y ?t+1(y) exp(?y,xt+1 + ?y?,y,xt+1),for all indices 1 ?
t ?
T and all labels y ?
Y .Then, Z?
(x) =?y ?T (y) and the pairwise prob-abilities P?
(yt = y?, yt+1 = y|x) are given by?t(y?)
exp(?y,xt+1 + ?y?,y,xt+1)?t+1(y)/Z?
(x)These recursions require a number of operationsthat grows quadratically with |Y |.3 `1 Regularization in CRFs3.1 RegularizationThe standard approach for parameter estimation inCRFs consists in minimizing the logarithmic lossl(?)
defined by (3) with an additional `2 penaltyterm ?22 ??
?22, where ?2 is a regularization parame-ter.
The objective function is then a smooth convexfunction to be minimized over an unconstrainedparameter space.
Hence, any numerical optimiza-tion strategy may be used and practical solutionsinclude limited memory BFGS (L-BFGS) (Liuand Nocedal, 1989), which is used in the popu-lar CRF++ (Kudo, 2005) and CRFsuite (Okazaki,2007) packages; conjugate gradient (Nocedal andWright, 2006) and Stochastic Gradient Descent(SGD) (Bottou, 2004; Vishwanathan et al, 2006),used in CRFsgd (Bottou, 2007).
The only caveatis to avoid numerical optimizers that require thefull Hessian matrix (e.g., Newton?s algorithm) dueto the size of the parameter vector in usual appli-cations of CRFs.The most significant alternative to `2 regulariza-tion is to use a `1 penalty term ?1??
?1: such regu-larizers are able to yield sparse parameter vectorsin which many component have been zeroed (Tib-shirani, 1996).
Using a `1 penalty term thus im-plicitly performs feature selection, where ?1 con-trols the amount of regularization and the numberof extracted features.
In the following, we willjointly use both penalty terms, yielding the so-called elastic net penalty (Zhou and Hastie, 2005)which corresponds to the objective functionl(?)
+ ?1??
?1 +?22??
?22 (6)The use of both penalty terms makes it possibleto control the number of non zero coefficients andto avoid the numerical problems that might occurin large dimensional parameter settings (see also(Chen, 2009)).
However, the introduction of a `1penalty term makes the optimization of (6) moreproblematic, as the objective function is no longerdifferentiable in 0.
Various strategies have beenproposed to handle this difficulty.
We will onlyconsider here exact approaches and will not dis-cuss heuristic strategies such as grafting (Perkinset al, 2003; Riezler and Vasserman, 2004).3.2 Quasi Newton MethodsTo deal with `1 penalties, a simple idea is that of(Kazama and Tsujii, 2003), originally introducedfor maxent models.
It amounts to reparameteriz-ing ?k as ?k = ?+k ??
?k , where ?+k and ?
?k are pos-itive.
The `1 penalty thus becomes ?1(?+ ?
??
).In this formulation, the objective function recoversits smoothness and can be optimized with conven-tional algorithms, subject to domain constraints.Optimization is straightforward, but the numberof parameters is doubled and convergence is slow506(Andrew and Gao, 2007): the procedure lacks amechanism for zeroing out useless parameters.A more efficient strategy is the orthant-wisequasi-Newton (OWL-QN) algorithm introduced in(Andrew and Gao, 2007).
The method is based onthe observation that the `1 norm is differentiablewhen restricted to a set of points in which eachcoordinate never changes its sign (an ?orthant?
),and that its second derivative is then zero, mean-ing that the `1 penalty does not change the Hessianof the objective on each orthant.
An OWL-QNupdate then simply consists in (i) computing theNewton update in a well-chosen orthant; (ii) per-forming the update, which might cause some com-ponent of the parameter vector to change sign; and(iii) projecting back the parameter value onto theinitial orthant, thereby zeroing out those compo-nents.
In (Gao et al, 2007), the authors show thatOWL-QN is faster than the algorithm proposed byKazama and Tsujii (2003) and can perform modelselection even in very high-dimensional problems,with no loss of performance compared to the useof `2 penalty terms.3.3 Stochastic Gradient DescentStochastic gradient (SGD) approaches update theparameter vector based on an crude approximationof the gradient (4), where the computation of ex-pectations only includes a small batch of observa-tions.
SGD updates have the following form?k ?
?k + ??l(?)?
?k, (7)where ?
is the learning rate.
In (Tsuruoka et al,2009), various ways of adapting this update to `1-penalized likelihood functions are discussed.
Twoeffective ideas are proposed: (i) only update pa-rameters that correspond to active features in thecurrent observation, (ii) keep track of the cumu-lated penalty zk that ?k should have received, hadthe gradient been computed exactly, and use thisvalue to ?clip?
the parameter value.
This is imple-mented by patching the update (7) as follows{if (?k > 0) ?k ?
max(0, ?k ?
zk)else if (?k < 0) ?k ?
min(0, ?k ?
zk)(8)Based on a study of three NLP benchmarks, theauthors of (Tsuruoka et al, 2009) claim this ap-proach to be much faster than the orthant-wise ap-proach and yet to yield very comparable perfor-mance, while selecting slightly larger feature sets.3.4 Block Coordinate DescentThe coordinate descent approach of Dud?
?k etal.
(2004) and Friedman et al (2008) uses thefact that optimizing a mono-dimensional quadraticfunction augmented with a `1 penalty can be per-formed analytically.
For arbitrary functions, thisidea can be adapted by considering quadratic ap-proximations of the objective around the currentvalue ??lk,??
(?k) =?l(??)?
?k(?k ?
?
?k) +12?2l(??)?
?2k(?k ?
?
?k)2+ ?1|?k|+?22?2k + Cst (9)The minimizer of the approximation (9) is simply?k =s{?2l(??)??2k?
?k ??l(??)?
?k, ?1}?2l(??)?
?2k+ ?2(10)where s is the soft-thresholding functions(z, ?)
=????
?z ?
?
if z > ?z + ?
if z < ?
?0 otherwise(11)Coordinate descent is ported to CRFs in(Sokolovska et al, 2010).
Making this schemepractical requires a number of adaptations,including (i) approximating the second orderterm in (10), (ii) performing updates in block,where a block contains the |Y | ?
|Y + 1| fea-tures ?y?,y,x and ?y,x for a fixed test x on theobservation sequence and (iii) approximating theHessian for a block by its diagonal terms.
(ii)is specially critical, as repeatedly cycling overindividual features to perform the update (10)is only possible with restricted sets of features.The block update schemes uses the fact thatall features within a block appear in the sameset of sequences, which means that most of thecomputations needed to perform theses updatescan be shared within the block.
One advantageof the resulting algorithm, termed BCD in thefollowing, is that the update of ?k only involvescarrying out the forward-backward recursions forthe set of sequences that contain symbols x suchthat at least one {fk(y?, y, x)}(y,y?
)?Y 2 is nonnull, which can be much smaller than the wholetraining set.5074 Implementation IssuesEfficiently processing very-large feature and ob-servation sets requires to pay attention to manyimplementation details.
In this section, we presentseveral optimizations devised to speed up training.4.1 Sparse Forward-Backward RecursionsFor all algorithms, the computation time is domi-nated by the evaluations of the gradient: our im-plementation takes advantage of the sparsity to ac-celerate these computations.
Assume the set of bi-gram features {?y?,y,xt+1}(y?,y)?Y 2 is sparse withonly r(xt+1)  |Y |2 non null values and definethe |Y | ?
|Y | sparse matrixMt(y?, y) = exp(?y?,y,xt)?
1.Using M , the forward-backward recursions are?t(y) =?y?ut?1(y?)
+?y?ut?1(y?
)Mt(y?, y)?t(y?)
=?yvt+1(y) +?yMt+1(y?, y)vt+1(y)with ut?1(y) = exp(?y,xt)?t?1(y) andvt+1(y) = exp(?y,xt+1)?t+1(y).
(Sokolovska etal., 2010) explains how computational savings canbe obtained using the fact that the vector/matrixproducts in the recursions above only involvethe sparse matrix Mt+1(y?, y).
They can thus becomputed with exactly r(xt+1) multiplicationsinstead of |Y |2.
The same idea can be usedwhen the set {?y,xt+1}y?Y of unigram features issparse.
Using this implementation, the complexityof the forward-backward procedure for x(i) can bemade proportional to the average number of activefeatures per position, which can be much smallerthan the number of potentially active features.For BCD, forward-backward can even be madeslightly faster.
When computing the gradient wrt.features ?y,x and ?y?,y,x (for all the values of yand y?)
for sequence x(i), assuming that x onlyoccurs once in x(i) at position t, all that is neededis ?
?t(y), ?t?
?
t and ??t(y),?t?
?
t.
Z?
(x) is thenrecovered as?y ?t(y)?t(y).
Forward-backwardrecursions can thus be truncated: in our experi-ments, this divided the computational cost by 1,8on average.Note finally that forward-backward is per-formed on a per-observation basis and is easilyparallelized (see also (Mann et al, 2009) for morepowerful ways to distribute the computation whendealing with very large datasets).
In our imple-mentation, it is distributed on all available cores,resulting in significant speed-ups for OWL-QNand L-BFGS; for BCD the gain is less acute, asparallelization only helps when updating the pa-rameters for a block of features that are occur inmany sequences; for SGD, with batches of sizeone, this parallelization policy is useless.4.2 ScalingMost existing implementations of CRFs, eg.CRF++ and CRFsgd perform the forward-backward recursions in the log-domain, whichguarantees that numerical over/underflows areavoided no matter the length T (i) of the sequence.It is however very inefficient from an implementa-tion point of view, due to the repeated calls to theexp() and log() functions.
As an alternative wayof avoiding numerical problems, our implementa-tion, like crfSuite?s, resorts to ?scaling?, a solutioncommonly used for HMMs.
Scaling amounts tonormalizing the values of ?t and ?t to one, makingsure to keep track of the cumulated normalizationfactors so as to compute Z?
(x) and the conditionalexpectations Ep?(y|x).
Also note that in our imple-mentation, all the computations of exp(x) are vec-torized, which provides an additional speed up ofabout 20%.4.3 Optimization in Large Parameter SpacesProcessing very large feature vectors, up to bil-lions of components, is problematic in many ways.Sparsity has been used here to speed up forward-backward, but we have made no attempt to accel-erate the computation of the OWL-QN updates,which are linear in the size of the parameter vector.Of the three algorithms, BCD is the most affectedby increases in the number of features, or moreprecisely, in the number of features blocks, whereone block correspond to a specific test of the ob-servation.
In the worst case scenario, each blockmay require to visit all the training instances,yielding terrible computational wastes.
In prac-tice though, most blocks only require to processa small fraction of the training set, and the ac-tual complexity depends on the average number ofblocks per observations.
Various strategies havebeen tried to further accelerate BCD, such as pro-cessing blocks that only visit one observation inparallel and updating simultaneously all the blocksthat visit all the training instances, leading to asmall speed-up on the POS-tagging task.508Working with billions of features finally re-quires to worry also about memory usage.
In thisrespect, BCD is the most efficient, as it only re-quires to store one K-dimensional vector for theparameter itself.
SGD requires two such vectors,one for the parameter and one for storing the zk(see Eq.
(8)).
In comparison, OWL-QN requiresmuch more memory, due to the internals of theupdate routines, which require several histories ofthe parameter vector and of its gradient.
Typi-cally, our implementation necessitates in the orderof a dozen K-dimensional vectors.
Parallelizationonly makes things worse, as each core will alsoneed to maintain its own copy of the gradient.5 ExperimentsOur experiments use two standard NLP tasks,phonetization and part-of-speech tagging, chosenhere to illustrate two very different situations, andto allow for comparison with results reported else-where in the literature.
Unless otherwise men-tioned, the experiments use the same protocol: 10fold cross validation, where eight folds are usedfor training, one for development, and one for test-ing.
Results are reported in terms of phoneme er-ror rates or tag error rates on the test set.Comparing run-times can be a tricky matter, es-pecially when different software packages are in-volved.
As discussed above, the observed run-times depend on many small implementation de-tails.
As the three algorithms share as much codeas possible, we believe the comparison reportedhereafter to be fair and reliable.
All experimentswere performed on a server with 64G of memoryand two Xeon processors with 4 cores at 2.27 Ghz.For comparison, all measures of run-times includethe cumulated activity of all cores and give verypessimistic estimates of the wall time, which canbe up to 7 times smaller.
For OWL-QN, we use 5past values of the gradient to approximate the in-verse of the Hessian matrix: increasing this valuehad no effect on accuracy or convergence and wasdetrimental to speed; for SGD, the learning rateparameter was tuned manually.Note that we have not spent much time optimiz-ing the values of ?1 and ?2.
Based on a pilot studyon Nettalk, we found that taking ?1 = .5 and ?2 inthe order of 10?5 to yield nearly optimal perfor-mance, and have used these values throughout.5.1 Tasks and Settings5.1.1 NettalkOur first benchmark is the word phonetizationtask, using the Nettalk dictionary (Sejnowski andRosenberg, 1987).
This dataset contains approxi-mately 20,000 English word forms, their pronun-ciation, plus some prosodic information (stressmarkers for vowels, syllabic parsing for con-sonants).
Grapheme and phoneme strings arealigned at the character level, thanks to the use ofa ?null sound?
in the latter string when it is shorterthan the former; likewise, each prosodic mark isaligned with the corresponding letter.
We have de-rived two test conditions from this database.
Thefirst one is standard and aims at predicting the pro-nunciation information only.
In this setting, the setof observations (X) contains 26 graphemes, andthe output label set contains |Y | = 51 phonemes.The second condition aims at jointly predict-ing phonemic and prosodic information3.
The rea-sons for designing this new condition are twofold:firstly, it yields a large set of composite labels(|Y | = 114) and makes the problem computation-ally challenging.
Second, it allows to quantify howmuch the information provided by the prosodicmarks help predict the phonemic labels.
Both in-formation are quite correlated, as the stress markand the syllable openness, for instance, greatly in-fluence the realization of some archi-phonemes.The features used in Nettalk experiments takethe form fy,w (unigram) and fy?,y,w (bigram),where w is a n-gram of letters.
The n-grm featuresets (n = {1, 3, 5, 7}) includes all features testingembedded windows of k letters, for all 0 ?
k ?
n;the n-grm- setting is similar, but only includesthe window of length n; in the n-grm+ setting,we add features for odd-size windows; in the n-grm++ setting, we add all sequences of letters upto size n occurring in current window.
For in-stance, the active bigram features at position t = 2in the sequence x=?lemma?
are as follows: the 3-grm feature set contains fy,y?
, fy,y?,e and fy?,y,lem;only the latter appears in the 3-grm- setting.
Inthe 3-grm+ feature set, we also have fy?,y,le andfy?,y,em.
The 3-grm++ feature set additionally in-cludes fy?,y,l and fy?,y,m.
The number of featuresranges from 360 thousands (1-grm setting) to 1.6billion (7-grm).3Given the design of the Nettalk dictionary, this experi-ment required to modify the original database so as to reas-sign prosodic marks to phonemes, rather than to letters.509Features With WithoutNettalk3-grm 10.74% 14.3M 14.59% 0.3M5-grm 8.48% 132.5M 11.54% 2.5MPOS taggingbase 2.91% 436.7M 3.47% 70.2MTable 1: Features jointly testing label pairs andthe observation are useful (error rates and featurescounts.
)`2 `1-sparse `1 % zero1-grm 84min 41min 57min 44.6%3-grm- 65min 16min 44min 99.6%3-grm 72min 48min 58min 19.9%Table 2: Sparse vs standard forward-backward(training times and percentages of sparsity of M )5.1.2 Part-of-Speech TaggingOur second benchmark is a part-of-speech (POS)tagging task using the PennTreeBank corpus(Marcus et al, 1993), which provides us with aquite different condition.
For this task, the numberof labels is smaller (|Y | = 45) than for Nettalk,and the set of observations is much larger (|X| =43207).
This benchmark, which has been used inmany studies, allows for direct comparisons withother published work.
We thus use a standard ex-perimental set-up, where sections 0-18 of the WallStreet Journal are used for training, sections 19-21for development, and sections 22-24 for testing.Features are also standard and follow the designof (Suzuki and Isozaki, 2008) and test the currentwords (as written and lowercased), prefixes andsuffixes up to length 4, and typographical charac-teristics (case, etc.)
of the words.
Our baselinefeature set alo contains tests on individual andpairs of words in a window of 5 words.5.2 Using Large Feature SetsThe first important issue is to assess the benefitsof using large feature sets, notably including fea-tures testing both a bigram of labels and an obser-vation.
Table 1 compares the results obtained withand without these features for various setting (us-ing OWL-QN to perform the optimization), sug-gesting that for the tasks at hand, these featuresare actually helping.`2 `1 Elastic-net1-grm 17.81% 17.86% 17.79%3-grm 10.62% 10.74% 10.70%5-grm 8.50% 8.45% 8.48%Table 3: Error rates of the three regularizers on theNettalk task.5.3 Speed, Sparsity, ConvergenceThe training speed depends of two main factors:the number of iterations needed to achieve conver-gence and the computational cost of one iteration.In this section, we analyze and compare the run-time efficiency of the three optimizers.5.3.1 ConvergenceAs far as convergence is concerned, the two formsof regularization (`2 and `1) yield the same per-formance (see Table 3), and the three algorithmsexhibit more or less the same behavior.
Theyquickly reach an acceptable set of active param-eters, which is often several orders of magnitudesmaller than the whole parameter set (see resultsbelow in Table 4 and 5).
Full convergence, re-flected by a stabilization of the objective function,is however not so easily achieved.
We have of-ten observed a slow, yet steady, decrease of thelog-loss, accompanied with a diminution of thenumber of active features as the number of iter-ations increases.
Based on this observation, wehave chosen to stop all algorithms based on theirperformance on an independent development set,allowing a fair comparison of the overall trainingtime; for OWL-QN, it allowed to divide the totaltraining time by almost 2.It has finally often been found useful to finetune the non-zero parameters by running a finalhandful of L-BFGS iterations using only a small`2 penalty; at this stage, all the other features areremoved from the model.
This had a small impactBCD and SGD?s performance and allowed them tocatch up with OWL-QN?s performance.5.3.2 Sparsity and the Forward-BackwardAs explained in section 4.1, the forward-backwardalgorithm can be written so as to use the sparsityof the matrix My,y?,x.
To evaluate the resultingspeed-up, we ran a series of experiments usingNettalk (see Table 2).
In this table, the 3-grm- set-ting corresponds to maximum sparsity for M , andtraining with the sparse algorithm is three timesfaster than with the non-sparse version.
Throwing510Method Iter.
# Feat.
Error TimeOWL-QN 1-grm 63.4 4684 17.79% 11min7-grm 140.2 38214 8.12% 1h02min5-grm+ 141.0 43429 7.89% 1h37minSGD 1-grm 21.4 3540 18.21% 9min5-grm+ 28.5 34319 8.01% 45minBCD1-grm 28.2 5017 18.27% 27min7-grm 9.2 3692 8.21% 1h22min5-grm+ 8.7 47675 7.91% 2h18minTable 4: Performance on Nettalkin more features has the effect of making M muchmore dense, mitigating the benefits of the sparserecursions.
Nevertheless, even for very large fea-ture sets, the percentage of zeros in M averages20% to 30%, and the sparse version remains 10 to20% faster than the non-sparse one.
Note that thenon-sparse version is faster with a `1 penalty termthan with only the `2 term: this is because exp(0)is faster to evaluate than exp(x) when x 6= 0.5.3.3 Training Speed and Test AccuracyTable 4 displays the results achieved on the Nettalktask.
The three algorithms yield very compara-ble accuracy results, and deliver compact models:for the 5-gram+ setting, only 50,000 out of 250million features are selected.
SGD is the fastestof the three, up to twice as fast as OWL-QN andBCD depending on the feature set.
The perfor-mance it achieves are consistently slightly worstthan the other optimizers, and only catch up whenthe parameters are fine-tuned (see above).
Thereare not so many comparisons for Nettalk withCRFs, due to the size of the label set.
Our resultscompare favorably with those reported in (Pal etal., 2006), where the accuracy attains 91.7% us-ing 19075 examples for training and 934 for test-ing, and with those in (Jeong et al, 2009) (88.4%accuracy with 18,000 (2,000) training (test) in-stances).
Table 5 gives the results obtained forthe larger Nettalk+prosody task.
Here, we onlyreport the results obtained with SGD and BCD.For OWL-QN, the largest model we could han-dle was the 3-grm model, which contained 69 mil-lion features, and took 48min to train.
Here again,performance steadily increase with the number offeatures, showing the benefits of large-scale mod-els.
We lack comparisons for this task, whichseems considerably harder than the sole phone-tization task, and all systems seem to plateauaround 13.5% accuracy.
Interestingly, simulta-Method Error TimeSGD 5-grm 14.71% / 8.11% 55min5-grm+ 13.91% / 7.51% 2h45minBCD5-grm 14.57% / 8.06% 2h46min7-grm 14.12% / 7.86% 3h02min5-grm+ 13.85% / 7.47% 7h14min5-grm++ 13.69% / 7.36% 16h03minTable 5: Performance on Nettalk+prosody.
Erroris given for both joint labels and phonemic labels.neously predicting the phoneme and its prosodicmarkers allows to improve the accuracy on the pre-diction of phonemes, which improves of almost ahalf point as compared to the best Nettalk system.For the POS tagging task, BCD appears to beunpractically slower to train than the others ap-proaches (SGD takes about 40min to train, OWL-QN about 1 hour) due the simultaneous increasein the sequence length and in the number of ob-servations.
As a result, one iteration of BCD typi-cally requires to repeatedly process over and overthe same sequences: on average, each sequence isvisited 380 times when we use the baseline fea-ture set.
This technique should reserved for taskswhere the number of blocks is small, or, as below,when memory usage is an issue.5.4 Structured Feature SetsIn many tasks, the ambiguity of tokens can be re-duced by looking up increasingly large windowsof local context.
This strategy however quicklyruns into a combinatorial increase of the numberof features.
A side note of the Nettalk experimentsis that when using embedded features, the activefeature set tends to reflect this hierarchical organi-zation.
This means that when a feature testing an-gram is active, in most cases, the features for allembedded k-grams are also selected.Based on this observation, we have designedan incremental training strategy for the POS tag-ging task, where more specific features are pro-gressively incorporated into the model if the cor-responding less specific feature is active.
This ex-periment used BCD, which is the most memory ef-ficient algorithm.
The first iteration only includestests on the current word.
During the second it-eration, we add tests on bigram of words, on suf-fixes and prefixes up to length 4.
After four itera-tions, we throw in features testing word trigrams,subject to the corresponding unigram block beingactive.
After 6 iterations, we finally augment the511model with windows of length 5, subject to thecorresponding trigram being active.
After 10 iter-ations, the model contains about 4 billion features,out of which 400,000 are active.
It achieves anerror rate of 2.63% (resp.
2.78%) on the develop-ment (resp.
test) data, which compares favorablywith some of the best results for this task (for in-stance (Toutanova et al, 2003; Shen et al, 2007;Suzuki and Isozaki, 2008)).6 Conclusion and PerspectivesIn this paper, we have discussed various ways totrain extremely large CRFs with a `1 penalty termand compared experimentally the results obtained,both in terms of training speed and of accuracy.The algorithms studied in this paper have com-plementary strength and weaknesses: OWL-QN isprobably the method of choice in small or moder-ate size applications while BCD is most efficientwhen using very large feature sets combined withlimited-size observation alphabets; SGD comple-mented with fine tuning appears to be the preferredchoice in most large-scale applications.
Our anal-ysis demonstrate that training large-scale sparsemodels can be done efficiently and allows to im-prove over the performance of smaller models.The CRF package developed in the course of thisstudy implements many algorithmic optimizationsand allows to design innovative training strategies,such as the one presented in section 5.4.
Thispackage is released as open-source software andis available at http://wapiti.limsi.fr.In the future, we intend to study how spar-sity can be used to speed-up training in the faceof more complex dependency patterns (such ashigher-order CRFs or hierarchical dependencystructures (Rozenknop, 2002; Finkel et al, 2008).From a performance point of view, it might alsobe interesting to combine the use of large-scalefeature sets with other recent improvements suchas the use of semi-supervised learning techniques(Suzuki and Isozaki, 2008) or variable-length de-pendencies (Qian et al, 2009).ReferencesGalen Andrew and Jianfeng Gao.
2007.
Scalable train-ing of l1-regularized log-linear models.
In Proceed-ings of the International Conference on MachineLearning, pages 33?40, Corvalis, Oregon.Le?on Bottou.
2004.
Stochastic learning.
In OlivierBousquet and Ulrike von Luxburg, editors, Ad-vanced Lectures on Machine Learning, LectureNotes in Artificial Intelligence, LNAI 3176, pages146?168.
Springer Verlag, Berlin.Le?on Bottou.
2007.
Stochastic gradient descent (sgd)implementation.
http://leon.bottou.org/projects/sgd.Stanley Chen.
2009.
Performance prediction for ex-ponential language models.
In Proceedings of theAnnual Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 450?458, Boulder, Colorado, June.Trevor Cohn.
2006.
Efficient inference in large con-ditional random fields.
In Proceedings of the 17thEuropean Conference on Machine Learning, pages606?613, Berlin, September.Thomas G. Dietterich, Adam Ashenfelter, and YaroslavBulatov.
2004.
Training conditional random fieldsvia gradient tree boosting.
In Proceedings ofthe International Conference on Machine Learning,Banff, Canada.Miroslav Dud?
?k, Steven J. Phillips, and Robert E.Schapire.
2004.
Performance guarantees for reg-ularized maximum entropy density estimation.
InJohn Shawe-Taylor and Yoram Singer, editors, Pro-ceedings of the 17th annual Conference on LearningTheory, volume 3120 of Lecture Notes in ComputerScience, pages 472?486.
Springer.Jenny Rose Finkel, Alex Kleeman, and Christopher D.Manning.
2008.
Efficient, feature-based, condi-tional random field parsing.
In Proceedings of theAnnual Meeting of the Association for Computa-tional Linguistics, pages 959?967, Columbus, Ohio.Jerome Friedman, Trevor Hastie, and Rob Tibshirani.2008.
Regularization paths for generalized linearmodels via coordinate descent.
Technical report,Department of Statistics, Stanford University.Jianfeng Gao, Galen Andrew, Mark Johnson, andKristina Toutanova.
2007.
A comparative study ofparameter estimation methods for statistical naturallanguage processing.
In Proceedings of the 45th An-nual Meeting of the Association of ComputationalLinguistics, pages 824?831, Prague, Czech republic.Minwoo Jeong, Chin-Yew Lin, and Gary Geunbae Lee.2009.
Efficient inference of crfs for large-scale nat-ural language data.
In Proceedings of the Joint Con-ference of the Annual Meeting of the Associationfor Computational Linguistics and the InternationalJoint Conference on Natural Language Processing,pages 281?284, Suntec, Singapore.Jun?ichi Kazama and Jun?ichi Tsujii.
2003.
Evalua-tion and extension of maximum entropy models withinequality constraints.
In Proceedings of the 2003Conference on Empirical Methods in Natural Lan-guage Processing, pages 137?144.Taku Kudo.
2005.
CRF++: Yet another CRF toolkit.http://crfpp.sourceforge.net/.512John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional random fields: prob-abilistic models for segmenting and labeling se-quence data.
In Proceedings of the InternationalConference on Machine Learning, pages 282?289.Morgan Kaufmann, San Francisco, CA.Percy Liang, Hal Daume?, III, and Dan Klein.
2008.Structure compilation: trading structure for features.In Proceedings of the 25th international conferenceon Machine learning, pages 592?599.Dong C. Liu and Jorge Nocedal.
1989.
On the limitedmemory BFGS method for large scale optimization.Mathematical Programming, 45:503?528.Gideon Mann, Ryan McDonald, Mehryar Mohri,Nathan Silberman, and Dan Walker.
2009.
Efficientlarge-scale distributed training of conditional maxi-mum entropy models.
In Y. Bengio, D. Schuurmans,J.
Lafferty, C. K. I. Williams, and A.Culotta, editors,Advances in Neural Information Processing Systems22, pages 1231?1239.Mitchell P. Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a large anno-tated corpus of English: The Penn treebank.
Com-putational Linguistics, 19(2):313?330.Jorge Nocedal and Stephen Wright.
2006.
NumericalOptimization.
Springer.Naoaki Okazaki.
2007.
CRFsuite: A fast im-plementation of conditional random fields (CRFs).http://www.chokkan.org/software/crfsuite/.Chris Pal, Charles Sutton, and Andrew McCallum.2006.
Sparse forward-backward using minimum di-vergence beams for fast training of conditional ran-dom fields.
In Proceedings of the International Con-ference on Acoustics, Speech, and Signal Process-ing, Toulouse, France.Simon Perkins, Kevin Lacker, and James Theiler.2003.
Grafting: Fast, incremental feature selectionby gradient descent in function space.
Journal ofMachine Learning Research, 3:1333?1356.Vasin Punyakanok, Dan Roth, Wen tau Yih, and DavZimak.
2005.
Learning and inference over con-strained output.
In Proceedings of the InternationalJoint Conference on Artificial Intelligence, pages1124?1129.Xian Qian, Xiaoqian Jiang, Qi Zhang, XuanjingHuang, and Lide Wu.
2009.
Sparse higher orderconditional random fields for improved sequence la-beling.
In Proceedings of the Annual InternationalConference on Machine Learning, pages 849?856.Stefan Riezler and Alexander Vasserman.
2004.
Incre-mental feature selection and l1 regularization for re-laxed maximum-entropy modeling.
In Dekang Linand Dekai Wu, editors, Proceedings of the confer-ence on Empirical Methods in Natural LanguageProcessing, pages 174?181, Barcelona, Spain, July.Antoine Rozenknop.
2002.
Mode`les syntaxiquesprobabilistes non-ge?ne?ratifs.
Ph.D. thesis, Dpt.d?informatique, E?cole Polytechnique Fe?de?rale deLausanne.Terrence J. Sejnowski and Charles R. Rosenberg.1987.
Parallel networks that learn to pronounce en-glish text.
Complex Systems, 1.Libin Shen, Giorgio Satta, and Aravind Joshi.
2007.Guided learning for bidirectional sequence classi-fication.
In Proceedings of the 45th Annual Meet-ing of the Association of Computational Linguistics,pages 760?767, Prague, Czech Republic.Nataliya Sokolovska, Thomas Lavergne, OlivierCappe?, and Franc?ois Yvon.
2010.
Efficient learningof sparse conditional random fields for supervisedsequence labelling.
IEEE Selected Topics in SignalProcessing.Charles Sutton and Andrew McCallum.
2006.
An in-troduction to conditional random fields for relationallearning.
In Lise Getoor and Ben Taskar, editors, In-troduction to Statistical Relational Learning, Cam-bridge, MA.
The MIT Press.Jun Suzuki and Hideki Isozaki.
2008.
Semi-supervisedsequential labeling and segmentation using giga-word scale unlabeled data.
In Proceedings of theConference of the Association for ComputationalLinguistics on Human Language Technology, pages665?673, Columbus, Ohio.Robert Tibshirani.
1996.
Regression shrinkage andselection via the lasso.
J.R.Statist.Soc.B, 58(1):267?288.Kristina Toutanova, Dan Klein, Christopher D. Man-ning, and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In Proceedings of the Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics on Human Language Technology, pages173?180.Yoshimasa Tsuruoka, Jun?ichi Tsujii, and Sophia Ana-niadou.
2009.
Stochastic gradient descent trainingfor l1-regularized log-linear models with cumula-tive penalty.
In Proceedings of the Joint Conferenceof the Annual Meeting of the Association for Com-putational Linguistics and the International JointConference on Natural Language Processing, pages477?485, Suntec, Singapore.S.
V. N. Vishwanathan, Nicol N. Schraudolph, MarkSchmidt, and Kevin Murphy.
2006.
Acceleratedtraining of conditional random fields with stochas-tic gradient methods.
In Proceedings of the 23th In-ternational Conference on Machine Learning, pages969?976.
ACM Press, New York, NY, USA.Hui Zhou and Trevor Hastie.
2005.
Regularization andvariable selection via the elastic net.
J. Royal.
Stat.Soc.
B., 67(2):301?320.513
