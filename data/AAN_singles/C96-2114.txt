Linguistic Indeterminacy as a Source of Errors in TaggingGunnel KiiilgrenDepartment of LinguisticsStockholm UniversityS- 106 91 StockholmSwedengunnel@ling.su.seAbstractMost evaluations of part-of-speech taggingcompare the utput of an automatic tagger tosome established standard, define thedifferences as tagging errors and try toremedy them by, e.g., more training of thetagger.
The present article is based on amanual analysis of a large number of taggingerrors.
Some clear patterns among the errorscan be discerned, and the sources of theerrors as well as possible alternative methodsof remedy are presented and discussed.
Inparticular are the problems with undecidablecases treated.1 BackgroundWhen the performance of automatic part-of-speechtaggers is discussed, it is normally measured relativeto some standard material, such as the Brown Corpus,or to a manual tagging or a manual proof-reading of(some smaller part of) the tagged material.
Theperformance of the automatic tagger is calculated asthe difference between the standard material and theoutput of the tagger to be evaluated, with alldifferences regarded as errors by the tagger.In a study carried out on material from a largeSwedish corpus, K~illgren (1996) made a carefulinspection of all instances where a manual and anautomatic tagging differed in a material of 50,000words of balanced text.
The differences wereclassified as 'man errors', 'machine rrors' or errorscommon to both.
The errors were furthermoreclassified according to type, and some clear patternscould be seen.
The present article picks up some ofthe findings and looks closer at a kind of error whichK~ltgren calls 'mirror image errors', where tworeadings of a word are constantly mixed up with eachother in both directions.
Errors of this kind have beennoted by others as well, and solutions to the problemsthey cause have been suggested.
Some suchsuggestions and the possible outcome of theirapplication to the Swedish material will be discussedin the following.2 The Linguistic Material Used inthe StudyThe error analysis on which this study is based wascarried out on material from the Stockholm-Ume~Corpus of modem written Swedish.
(See KNlgren1990.)
It is a carefully composed, balanced corpus.
Itscomposition follows the principles established by theBrown and LOB corpora, with adjustments for thefact that it should cover the most common genres ofthe Swedish of the 1990's.
It contains newspapertexts, fact, and fiction on several stylistic levels.
Thetexts all consist of written prose published sometimebetween 1990 and 1994.
No spoken language materialis included in the corpus.All words in the SUC are tagged for part-of-speechand for inflectional features.
For a description of theSUC annotation system, see Ejerhed et al (1992).
Thetagged texts of the SUC are converted into SGMLformat and additional tags are added in accordancewith the TEI Guidelines (Sperberg-McQueen andBumard 1993, K~illgren 1995) to give the format inwhich the corpus will finally be distributed.
There arelegal permissions allowing the corpus to be used anddistributed for non-commercial research purposes.3 Manual and Automatic MarkupThe SUC has been annotated by a process thatcombines automatic and manual steps.
The raw textsget their first analysis from the SWETWOLcomputerized ictionary (Karlsson 1992) and thenpass a step of postprocessing to reach the analysisdescribed in the SUC tagging manual (Ejerhed et al1992).
The coverage of the dictionary is high, but thedegree of ambiguity in Swedish is also high, actuallyhigher than in English, so the texts return fromdictionary lookup with 51% of the word tokenscarrying more than one analysis.In the next step, a human annotator is to mark foreach ambiguous word which of the suggestedreadings is the correct one and for each unambiguousword whether the suggested reading is correct.
Theoutput of this step is used as the 'man version' in the676man-machine comparison (or rather the 'womanversion' as the majority of the annotators were femalestudents).The entire corpus of 1 million words has passedthrough this stage of manual disambiguation andannotation, which makes it an important standard thatcan be used as a tool, e.g., when training probabilistictaggers.
The goal of the experiment reported inKallgren (1996) was, however, to compare 'sheer'machine tagging to the performance of humanannotators.
The tagger used is thus one that does notneed tagged and disambiguated material to be trainedon, namely the XPOST originally constructed atXerox Parc (Cutting et al 1992, Cutting and Pedersen1993).The XPOST algorithm has been transferred to otherlanguages than English.
Douglass Cutting himselfmade the first Swedish version of it (Cutting 1993)and a later version has been implemented by GunnarEriksson (Eriksson 1995) and refined by TomasSvensson (Svensson 1996).
It is this latter version thathas been used in the experiment.Starting from a set of texts and a lexicon, theXPOST looks up all words in the texts and assigns tothem a set of one or more readings.
The words arethen classified into so-called ambiguity classesaccording to which set of readings they have beenassigned.
The training is performed on ambiguityclasses and not on individual word tokens.
Kallgren(1996) gives a more covering description of howXPOST is used on the Swedish material and alsosketches the major differences between this algorithmand some others used for tagging, such as PARTS(Church 1988) and VOLSUNGA (DeRose 1988).A characteristic tbature of the SUC is its highnumber of different tags.
The number of part-of-speech tags used in the SUC is 21.
With the additionof a category for foreign words the number of majorcategories used is 22 (plus three tags for punctuation),which is in no way a remarkable amount, but the SUCtags are composite.
This means that all words haveone tag for part-of-speech, but for many parts-ofspeech this tag is followed by other tags for variousmorphological features, Where, e.g., English nounshave a variation between two possible values, singularand plural, the Swedish pattern allows for 1 x 2 x 2 x2 x 3 = 24 different ags, specifying not only part-of-speech but also gender, number, definiteness, andcase.
The number of different ags actually occurringin texts is mostly around 180.A remarkable fact is that the high number ofdifferent ags does not seem to influence the trainingand performance of probabilistic taggers negatively inthe way that might have been expected.
Themorphological errors in the material are notdisturbingly many, considering the fact that allSwedish content words have such features.Morphological agreement provides enoughinformation to make it possible fbr an atttomatictagger to pick the right form in most cases.
Thissensitivity to close context probably explains why thehigh number of tags does not influence performancewhen it comes to picking an alternative, but it doesnot explain why training is so little affected by thehigh number of different observed situations.Results from a Comparisonbetween 'Man' and 'Machine'The automatic tagger was run on 50,000 words of textnot used in the training of the tagger.
The output wascompared to the same texts with manualdisambiguation.
All instances where the two differhave been manually inspected.
The evaluation of theresults is far from trivial.
The 'correctness' of thetagging must be judged relative to some norm.
Onesuch norm is the SUC tagging manual (Ejerhed et al1992).
Although it is very comprehensive andexplicit, no manual can ever foresee and cover all thetricky instances that will occur in unrestrictedlanguage.
Another norm is the intuition of theworking linguist, with the possibility of consultingother people to get their intuitions.
This also has cleardrawbacks.
There will always remain a set of doubtfulcases which do not necessarily depend on deficits inthe linguistic description.
Be it here sufficient o saythat in general \[ prefer the term 'consistent (with acertain norm)' instead of the term 'correct';nevertheless, in the following discussion I will call thedeviances from the applied noun 'errors'.Table I gives the errors found in a material of50,498 words sorted according to whether theyoccurred in automatically or manually tagged text orboth.
Where both have an error, the errors cansometimes be of the same type, sometimes ofdifferent ypes.Table 1.
Tagging Errors According to SourceN %Errors only in automatic tagging 359l 7.1Errors only in manual tagging 503 1.0Errors in both 110 0.2Total 4204 8.3The automatic tagger is truly automatic in that it hasnot at all been adjusted to the specific task at hand.With fairly little trimming it could well reach a levelof at least 95-96% consistence with the humanannotator but now the basic idea was to test it 'raw'.Humans are not infallible, if anyone thought so, 1.2%of the errors are man-made.
It is still a consolation tosee that human annotators are seven times as good ascomputers when it comes to disambiguation.5 Types of Errors677The errors occurring in the material can be classifiedaccording to type.
By 'error type' is here meant aclassification of tag pairs with an erroneous tagfollowed by the correct tag, e.g., an error can be ofthe type 'preposition suggested where it should havebeen an adverb'.
This classification shows both whichparts-of-speech are most often involved in errors andwhich readings of a particular word are most oftenmixed up with each other, and in which direction theerrors mostly go.
The classification can also givehints about what could possibly be done about theerrors.5.1 Errors among Content WordsIt is clear that content words (here: nouns, verbs,adjectives, participles, proper nouns) are seldominvolved in errors.
Considering the large proportionof the number of running words that these majorcategories cover, this is even more remarkable.
Ifwords from these categories are ever mixed up, theyare mixed up in very specific patterns, namely withthemselves (as when different inflected forms of thesame stem coincide) or they are mixed up with wordsthey are related to (e.g., by derivation).
Among theten most common error types for either automatic ormanual disambiguation, there are actually only twothat involve content words.One of these error types is almost exclusively in therealm of automatic disambiguation.
Swedish nounsare inflected according to five different declensions,one of which has zero plural.
The automatic taggersometimes mistakes ingular nouns of that declensionwithout modifiers for plurals, but never the other wayround.
This is just as could be expected; 'naked'plurals are far more common than 'naked' singularsin all declinations and will thus be favoured by thestatistics.
To remedy this situation, it would probablybe necessary to have a phrasal lexicon, as mostinstances of naked singular nouns appear inlexicalized phrases.As has been pointed out for English material (cf.below) different inflections of the same verb can getmixed up.
This phenomenon can be found in Swedishtoo, but not very frequently.The other common error type involving contentwords concerns adverbs derived from adjectives.
Themost frequent derivational pattern for Swedishadverbs makes them identical to neutral singularindefinite adjectives.
Here both manual and automaticdisambiguation leads to errors but in differentdirections.
The automatic tagger suggests adverbwhere there should have been an adjective, whilehuman annotators sometimes call an adverb anadjective.
Both types mainly occur post-verbally andoften at the very end of a graphic sentence, where itmay be difficult to decide whether the concernedword is a predicative adjective or an adverb.
It maywell be that a subcategorization of verbs mighteliminate the problem, but this is a large task toimplement both in the lexicon and in the tagger.However, these errors are neither the most frequentnor the most disturbing ones.
Instead, it is thefunction words that get mixed up in all their differentuses.
Actually, almost all errors concern functionwords and a scrutiny of them makes it clear howdoubtful the whole concept of correctness i in thisconnection.5.2 Errors among Function WordsThe degree of homography - or is it polysemy?
- isgenerally higher among function words than amongcontent words which, of course, leads to moresituations where errors can occur.
Furthermore, thenumber of readings connected with each word tokenis highly dependent on the linguistic description usedas a basis for the tagging system, its theoreticalassumptions and the granularity of the system, amongother things.The ten words most frequently involved in errors inthe studied material are (with approximatetranslations and number of errors in parenthesis) thefollowing: 'det' (it~the in neuter gender, 330 errors),'ett' (a/one in neuter, 254), 'sore' (rel.pron and adv.,180), 'den' (it~the in common gender, 153), 'om' (if,about, 122), 'en' (a/one in common, 109), 'att' (that,inf.marker, 83), 'sS.'
(so, 79), 'ut' (out, 73), 'fOr' (for,70).
They are all high frequency function words thatplay many different syntactic roles depending on theircontext.One interesting fact that the classification into errortypes makes clear is that all the different readings ofthese words do not get mixed tip at random but inrather strong, often mirror-like patterns.
Let us takethe word 'om' as an example.
It can be used asadverb, preposition, or subordinating conjunction andall the six possible mistagged combinations do occur,but with quite varying frequency.
Three of them arealmost neglectable and one has a strong unidirectionalpattern where the reading as an adverb (moreprecisely a verbal particle) is often taken for apreposition.
This is an instance of the by far mostcommon error type in the entire material, and is ofcourse directly dependent on the way verbal particlesare treated in the underlying linguistic description.The remaining two error types are the mostinteresting ones.
They form a bidirectional patternwhere the reading as a preposition is confused withthe reading as a subordinating conjunction.Preposition instead of subjunction appears 40 times,subjunction instead of preposition 33 times, altogether77 of the 122 errors connected with the word 'om'.All errors on this word were machine-induced, except8 cases where human annotators took a subjunction to678be a preposition.
Some of the error situations may beregarded as truly undecidable.6 Tagging Undecidable SituationsHow are bidirectional error patterns like the oneabove to be treated?
Looking at their close context, itis often impossible to handle the situation with somesmart tagging restriction or other device.
They arealso so equal in number and so frequent that onecannot simply decide to let one reading overrule theother and live with the errors that such a happy-go-lucky solution would give rise to.
(As a practicingcorpus tagger, 1 know that this unorthodox methodcan sometimes be the best way out of problematicsituations.
)Another possibility would be to amalgamate tiletwo readings into one, bivalued or underspecified,depending on how one chooses to see it.
As ah'eadymentioned, these more or iess undecidablebidirectional patterns have been observed anddiscussed by others working with tile tagging of largecorpora, and they have, seemingly independently ofeach other, come up with similar suggestions.
Beloware three quotations dealing with this matter.The Penn Treebank: 'ltowever, even given explicitcriteria for assigning POS tags to potentiallyambiguous words, it is not always possible to assign aunique tag to a word with confidence.
Since a majorconcern of the Treebank is to avoid requiringannotators to make arbitrary decisions, we allowwords to be associated with more than one POS tag.Such multiple tagging indicates either that the word'spart of speech simply cannot be decided or that theannotator is unsure which of the alternative tags is thecorrect one.'
(Marcus et al 1993, 316.
)The British National Corpus: 'In order to providemore useful results in a substantial proportion of theresidual words which cannot be successfully tagged,we have introduced portmanteau tags.
A portmanteautag is used ill a situation where there is insufficientevidence for Claws to make a clear distinctionbetween two tags.
Thus, in the notoriously difficultchoice between a past participle and the past tense ofa verb, if there is insufficient probabilistic evidence tochoose between the two Claws marks the word asVVN-VVD.
A set of fifteen such portmanteau tagshave been declared, covering the major pairs ofconfusable tags.'
(Garside 1995.
)Constraint Grammar: 'In the rare cases where twoanalyses were regarded as equally legitimate, bothcould be marked.'
(Voutilainen and Jfirvinen 1995,212.
)It is, however, important hat the s/tuations whereunderspecified tags can be used are restricted to well-defined cases and that the reasons for using them arequite clear.
They should have what I call a 'mirror'character, in that the interchange goes in bothdirections, and they should concern clearly distinctpairs of tags even when a word has several other tagsas well.
Such situations are more common inautomatic tagging but they occur in manual tagging aswell.The reasons for a situation being undecidable can,however, vary.
Voutilainen and J~irvinen, in theirstudy of inter-annotator agreement, mention threesituations where an nnderdetermined analysis wasaccepted:'When the judges disagree about the correctanalysis even after negotiations.
In this case,comments were added to distinguish it from the othertwo types.
Neutralisation: both analyses wereregarded as equivalent.
(This often indicates aredundancy in the lexicon.)
Global ambiguity: thesentence was agreed to be globally ambiguous.
'(Voutilainen and J~trvinen 1995, 212.
)Marcus et at.
(1993) allow underspecified taggingboth for annotators' uncertainty or disagreement andfor cases that correspond to Voutilainen andJ~irvinen's neutralisation and global ambiguity.
Thismay be infelicitous.
It is important o keep a clearborderline between situations that could be solved inprinciple and such that are truly undecidable.
Thelatter ones may lead us to questions about the natureof language and to what extent natural anguage reallyis exact and welldefined.Introducing underspecified tags would influence thetraining and performance of a probabilistic tagger inat least the tbllowing ways: a) The concerned wordswould mostly get more alternative tags, one for eachof the unambigous readings plus one for theunderspecified one.
According to common taggingprinciples, this would be a disadvantage, b) Therewould be fewer obserw, tions of each of the alternativetags, as the competing unambiguous tags both wouldlose some of their instances to their commonunderspecified alternative.
This would also be adisadvantage, c) The observations of each tag wouldhopefully be more correct, as the instances 'lost' tothe underspecified tag would be the tricky andatypical cases that otherwise might obscure thecontextual patterns of the unambiguous tags.
d) Theunderspecified instances can later be automaticallyretrieved for either manual inspection or some moreelaborate disambiguation device.It is still an open question whether the more clear-cut distinctions introduced by the underspecified tagscompensate 1or the accompanying disadvantages, butat least they have the intellectually pleasing propertyof showing where there are truly ambiguous ituationsin language.
By systematic modifications of the tagsetalong these lines it is possible to decide to what extentthe introduction of underspecified tags will improvetile overall performance of a tagger and/or facilitatethe task of human annotators.679ReferencesChurch, K. W. (1988), 'A Stochastic Parts Programand Noun Phrase Parser for Unrestricted Text', inProceedings of the Second Conference on AppliedNatural Language Processing (ACL), 136-43(Austin).Cutting, D. (1993), 'Porting a Stochastic Part-of-Speech Tagger to Swedish', in Eklund, R.
(ed.
)Nordiska Datalingvistikdagarna, 65-70 (Stockholm).Cutting, D., Kupiec, J., Pedersen, J., and Sibun, P.(1992), 'A Practical Part-of-Speech Tagger', inProceedings of the Third Conference on AppliedNatural Language Processing (ACL) (Trento).Cutting, D. and Pedersen, J.
(1993), The XeroxPart-of-Speech Tagger, Xerox PARC technical report.DeRose, S. J.
(1988), 'Grammatical CategoryDisambiguation by Statistical Optimization',Computational Linguistics, Volume 14:1, 31-9.Ejerhed, E., Kfillgren, G., Wennstedt, O. andA.str6m, M. (1992), The Linguistic AnnotationSystem of the Stockholm-Umeh Corpus Project,version 4.31.
Publications from the Department ofGeneral Linguistics, University of Ume~, no.
33.Eriksson, G. (1995), 'Beskrivning av arbetet reedatt utveckla en XPOSTtagger for svenska', Technicalreport, Telia Research Infovox (Stockholm).Garside, R. (1995), 'Using CLAWS to Annotate theBritish National Corpus', URL:http ://info.ox.ac.uk/bnc/garside_allc.html.K~llgren, G. (1990), '"The First Million is Hardestto Get": Building a Large Tagged Corpus asAutomatically as Possible', in Proceedings fromColing '90 (Helsinki).K~llgren, G. (1995), 'Manual for TEI conformantmark-up of the SUC', draft version, Department ofLinguistics, Stockholm University.K~llgren, G. (1996), 'Man vs. Machine - Which isthe Most Reliable Annotator?
', to appear inPerissinotto, Giorgio (ed.
), Research in HumanitiesComputing 6, Oxford University Press.Karlsson, F. (1992), 'SWETWOL: AComprehensive Morphological Analyzer forSwedish', in Nordic Journal of Linguistics Vol.
15:1-45.Marcus, M. P., Marcinkiewicz, M. and Santorini, B.
(1993), 'Building a Large Annotated Corpus ofEnglish: The Penn Treebank', in ComputationalLinguistics Volume 19:2, 313-30.Sperberg-McQueen, C. M. and Burnard, L.
(1993)(eds.
), Guidelines for Electronic Encoding andInterchange (Chicago, Oxford).Svensson, T. (1995), 'Ore tagguppsfittningar i enf6rsta ordningens g6md Markovmodell', Technicalreport, Yelia Research Infovox (Stockholm).Voutilainen, A. and J~irvinen, T. (1995),'Specifying a shallow grammatical representation forparsing purposes', in Proceedings of the SeventhConference of the European Chapter of theAssociation for Computational Linguistics, 2 l 0-14.680
