Broad-Coverage Parsing Using Human-LikeMemory ConstraintsWilliam Schuler?University of MinnesotaSamir AbdelRahman?
?Cairo UniversityTim Miller?University of MinnesotaLane Schwartz?University of MinnesotaHuman syntactic processing shows many signs of taking place within a general-purposeshort-term memory.
But this kind of memory is known to have a severely constrained storagecapacity ?
possibly constrained to as few as three or four distinct elements.
This article describesa model of syntactic processing that operates successfully within these severe constraints, byrecognizing constituents in a right-corner transformed representation (a variant of left-cornerparsing) and mapping this representation to random variables in a Hierarchical Hidden MarkovModel, a factored time-series model which probabilistically models the contents of a boundedmemory store over time.
Evaluations of the coverage of this model on a large syntacticallyannotated corpus of English sentences, and the accuracy of a bounded-memory parsing strategybased on this model, suggest this model may be cognitively plausible.1.
IntroductionIt is an interesting possibility that human syntactic processingmay occur entirely withina general-purpose short-term memory.
Like other short-term memory processes, syn-tactic processing is susceptible to degradation if short-term memory capacity is loaded,for example, when readers are asked to retain lists of words while reading (Justand Carpenter 1992); and memory of words and syntax degrades over time withinand across sentences (Sachs 1967; Jarvella 1971), unlike semantics and discourseinformation about referents from other sentences (Ericsson and Kintsch 1995).
Butshort-term memory is known to have severe capacity limitations of perhaps no morethan three to four distinct elements (Miller 1956; Cowan 2001).
These limits may seem?
Department of Computer Science and Engineering, 200 Union St.
SE, Minneapolis, MN 55455.E-mail: schuler@cs.umn.edu; tmill@cs.umn.edu; lane@cs.umn.edu.??
Computer Science Department, Faculty of Computers and Information, 5 Dr. Ahmed Zewail Street,Postal Code: 12613, Orman, Giza, Egypt.
E-mail: s.abdelrahman@fci-cu.edu.eg.Submission received: 14 February 2008; revised submission received: 31 October 2008; accepted forpublication: 27 May 2009.?
2010 Association for Computational LinguisticsComputational Linguistics Volume 36, Number 1too austere to process the rich tree-like phrase structure commonly invoked to explainword-order regularities in natural language.This article aims to show that they are not.
The article describes a comprehensionmodel, based on a right-corner transform?a reversible tree transform related to theleft-corner transform of Johnson (1998a)?that associates familiar phrase structure treeswith the contents of a memory store of three to four partially completed constituentsover time.
Coverage results on the large syntactically annotated Penn Treebank corpusshow a vast majority of naturally occurring sentences can be recognized using a mem-ory store containing a maximum of only three incomplete constituents, and nearly allsentences can be recognized using four, consistent with estimates of human short-termmemory capacity.This transform reduces memory usage in incremental (left to right) processingby transforming right-branching constituent structures into left-branching structures,allowing child constituents to be composed with parent constituents before either havebeen completely recognized.
But because this composition identifies an incompletechild as the awaited portion of an incomplete parent, it implicitly predicts that thischild constituent will be the rightmost (i.e., last) child of the parent, before this childhas been completely recognized.
Parsing accuracy results on the Penn Treebankusing a Hierarchical Hidden Markov Model (Murphy and Paskin 2001)?essentially aprobabilistic pushdown automaton with a bounded pushdown store?show that thisprediction can be reliably learned from training data.The remainder of this article is organized as follows: Section 2 describes somerelated models of human syntactic processing using a bounded memory store; Section 3describes a Hierarchical Hidden Markov Model (HHMM) framework for statisticalparsing using this bounded store of incomplete constituents; Section 4 describes theright-corner transform and how it relates conventional phrase structure to incompleteconstituents in a bounded memory store; Section 5 describes an experiment to estimatethe level of coverage of the Penn Treebank corpus that can be achieved using thistransform with various memory limits, given a linguistically motivated binarization ofthis corpus; and Section 6 gives accuracy results of this bounded-memorymodel trainedon this corpus, given that some amount of incremental prediction (as described earlier)must be involved.2.
Bounded-Memory ParsingOne of the earliest bounded-memory parsing models is that of Marcus (1980).
Thismodel maintains a bounded store of complete but unattached constituents as a buffer,and operates on them using a variety of specialized memory manipulation operations,deferring certain attachment decisions until the contents of this buffer indicate it is safeto do so.
(In contrast, the model described in this article maintains a store of incom-plete constituents using ordinary stack-like push and pop operations, defined to allowconstituents to be composed before being completely recognized.)
The Marcus parserprovides a bounded-memory explanation for human difficulties in processing gardenpath sentences: for example, the horse raced past the barn fell, with intended interpretation[NP the horse [RC (which was) raced past the barn]] fell (Bever 1970), in which raced seems likethe main verb of the sentence until the word fell is encountered.
But this explanation dueto memory exhaustion is not compatible with observations of unproblematic parsingof sentences such as these when contextual information is provided in advance: forexample, two men on horseback had a race; one went by the meadow, and the other went by thebarn (Crain and Steedman 1985).2Schuler et al Parsing Using Human-Like Memory ConstraintsAdes and Steedman (1982) introduce the idea of composing incomplete constituentsto reduce storage demands in incremental processing using Combinatorial Catego-rial Grammar (CCG), avoiding the need to maintain large buffers of complete butunattached constituents.
The right-corner transform described in this article composesincomplete constituents in very much the same way, but CCG is essentially a compe-tence model, in that it seeks to unify lexical category representations used in processingwith learned generalizations about argument structure, whereas the model describedherein is exclusively a performance model, allowing generalizations about lexical ar-gument structures to be learned in some other representation, then combined withprobabilistic information about parsing strategies to yield a set of derived incompleteconstituents.
As a result, the model described in this article has a freer hand to satisfystrict working memory bounds, which may not permit some of the alternative compo-sition operations proposed in the CCG account, thought to be associated with availableprosody and quantifier scope analyses.1Johnson-Laird (1983) and Abney and Johnson (1991) propose a pure processingaccount of memory capacity limits in parsing ordinary phrase structure trees.
TheJohnson-Laird and Abney and Johnson models adopt a left-corner parsing strategy, ofwhich the right-corner transform introduced in this article is a variant, in order to bringmemory usage for most parsable sentences to within seven or so active or awaitedphrase structure constituents.
This account may be used to explain human processingdifficulties in processing triply center-embedded sentences like the rat that the cat that thedog chased killed ate the malt, with intended interpretation [NP the rat that [NP the cat that [NPthe dog] chased] killed] ate the malt (Chomsky and Miller 1963).
But this explanation doesnot account for examples of triply center-embedded sentences that typically do notcause processing problems: [NP that [NP the food that [NP John] ordered] tasted good] pleasedhim (Gibson 1991).
Moreover, the apparent competition between comprehension ofcenter-embedded object relatives and retention of unrelated words in general-purposememory (Just and Carpenter 1992) suggests that general-purpose memory is (or atleast, can be) used to store incomplete constituents during comprehension.
This wouldpredict three or four elements of reliable storage, rather than seven (Cowan 2001).The transform-based model described in this article exploits a conception of chunking(Miller 1956) to combine pairs of active and awaited constituents from the Abneyand Johnson analysis, connected by recognized structure, in order to operate withinestimates of human short-term memory bounds.Because of these counterexamples to the memory-exhaustion explanation of gardenpath and center-embedding difficulties, recent work has turned to explanations otherthan memory exhaustion for these phenomena.
Lewis and Vasishth (2005) attributeprocessing errors to activation interference among stored constituents that have sim-ilar syntactic and semantic roles.
Hale?s surprisal (2001) and entropic model (2006)link human processing difficulties to significant changes in the relative probability ofcompeting hypotheses in incremental parsing, such that if activation is taken to be amechanism for probability estimation, processing difficulties may be ascribed to therelatively slow speed of activation change within the brain (or to collapsing activationwhen probabilities grow too small, as in the case of garden path sentences).
Thesemodels explain many processing difficulties without invoking memory limits, and are1 The lack of support for some of these available scope analyses may not necessarily be problematic for thepresent model.
The complexity of interpreting nested raised quantifiers may place them beyond thecapability of human interactive incremental interpretation, but not beyond the capability of post hocinterpretation (understood after the listener has had time to think about it).3Computational Linguistics Volume 36, Number 1compatible with brain imaging evidence of increased cortical activity and recruitmentof auxiliary brain areas during periods of increased uncertainty in sentence processing(Just and Varma 2007).
But if interference or changing activation is posited as the sourceof processing difficulty, and delays are not linked to memory exhaustion per se, thenthese theories do not explain how (or whether) syntactic processing operates withingeneral-purpose short-term memory.Toward this end, this article will specifically evaluate the claim that syntacticprocessing can be performed entirely within general-purpose short-term memory byusing this memory to store unassimilated incomplete syntactic constituents, derivedthrough a right-corner transform from basic properties of phrase structure trees.
Asa probabilistic incremental parser, the model described in this article is compatiblewith surprisal-based explanations of processing difficulties; it is, however, in somesense orthogonal, because it models a different dimension of resource allocation.
Thesurprisal framework models allocation of processing resources (in this case, activation)among disjunctions of competing hypotheses, which are maintained for some amountof time in parallel, whereas the framework described here can be taken to model theallocation of processing resources (in this case, memory elements) among conjunctionsof incompletely recognized constituents within each competing hypothesis.2 Thus, inthis view, there are twoways to simultaneously activatemultiple concepts: disjunctively(sharing activation among competing hypotheses) and conjunctively (sharing activationamong unassimilated constituents within a hypothesis).
But only the inner conjunctiveallocation corresponds to the familiar discretely bounded store of short-term memoryas described by Miller (1956); the outer disjunctive allocation treats activation as acontinuous resource in which like-valued pockets expand and contract as they arereinforced or contradicted by incoming observations.
Indeed, it would be surprisingif these two dimensions of resource allocation did not exist: the former, because itwould contradict years of observations about the behavior of short-term memory; andthe latter, because it would require neural activation spreading to be instantaneousand uniform, contradicting most neuropsychological evidence.
Levy (2008) comparesthe allocation of activation in this kind of framework to the distributed allocationof resources in a particle filter (Gordon, Salmond, and Smith 1993), an approximateinference technique for probabilistic time-series models in which particles in a (typicallyfixed) reservoir are assigned randomly sampled hypotheses from learned transitionprobabilities, essentially functioning as units of activation.
The model described in thispaper qualifies this analogy by positing that each individual particle in this reservoirendorses a coherent hypothesis about the contents of a three- to four-element memorystore at any given time, rather than about an entire unbounded phrase structure tree.32 Probability distributions in entropy-based models like Hale?s are typically assumed to be defined oversets of hypotheses pursued in parallel, but other interpretations (for example, lookahead-baseddeterministic models) are possible.
The model described in this article is also compatible withdeterministic parsing frameworks, in which case it models allocation of processing resources amongincompletely-recognized constituents within a single non-competing hypothesis.3 Pure connectionist models of syntactic processing (Elman 1991; Berg 1992; Rohde 2002) attempt to unifystorage of constituent structure with that of ambiguous alternative analyses, but the memory demands ofsystems based on this approach typically do not scale well to broad-coverage parsing.
Recent results forusing self-organizing maps as a unified memory resource are encouraging (Mayberry and Miikkulainen2003), but are still limited to parsing relatively short travel planning queries with limited syntacticcomplexity.
Hybrid systems that generate explicit alternative hypotheses with explicit stacked-upconstituents, and use connectionist models for probability estimation over these hypotheses (Henderson2004) typically achieve better performance in practice.4Schuler et al Parsing Using Human-Like Memory ConstraintsPrevious memory-based explanations of problematic sentences (explaining gardenpath effects as exceeding a bound of four complete but unattached constituents, or ex-plaining center embedding difficulties as exceeding a bound of seven active or awaitedconstituents) have been shown to underestimate human sentence processing capacitywhen equally complex but unproblematic sentences were examined.
The hypothesisadvanced in this article, that human sentence processing uses general-purpose short-term memory to store incomplete constituents as defined by a right-corner transform,leaves the explanation of several negative examples of unparsable garden path and cen-ter embedding sentences to orthogonal models of surprisal or interference.
But in orderto determine whether this right-corner memory hypothesis still underestimates humansentence processing capacity, a corpus study was performed on two complementarycorpora of transcribed spontaneous speech and newspaper text, manually annotatedwith phrase structure trees (Marcus, Santorini, and Marcinkiewicz 1993).
These spon-taneous speech and newspaper text corpora contain only attested positive examplesof parsable sentences, but they may be considered complementary for this purposebecause the complexity of spontaneous speech may somewhat understate human recog-nition capacity (potentially limiting it to the cost of spontaneously generating sentencesin an unusual social context), and the complexity of newspaper text may somewhatoverstate human recognition capacity (though it is composed and edited to be readable,it is still composed and edited off-line), so results from these corpora may be takentogether to suggest generous and conservative upper bounds on human processingcapacity.3.
Bounded-Memory Parsing with a Time Series ModelThe framework adopted in this article is a factored HMM-like time series model, whichmaintains a probability distribution over the contents of a bounded set of randomvariables over time, corresponding to hypothesized stores of memory elements.
Therandom variables in this store may be understood as simultaneous activations in a cog-nitive model (similar to the superimposed roles described by Smolensky and Legendre[2006]), and the probability distribution over these stores may be thought of as compet-ing pockets of activation, as described in the previous section.
Some of these variablespersist as elements of the short-term memory store, and some are transient as results ofhypothesized compositions, which are estimated and immediately discarded or foldedinto the persistent store according to the dependencies in the model.
The variableshave values or contents (or fillers)?in this case incomplete constituent categories?thatchange over time, and although these values may be uncertain, the set of hypothesizedcontents of this memory store at any given point in time are collectively constrained toform a coherent (but possibly incomplete) syntactic analysis of a sentence.The particular model used here is an HHMM (Murphy and Paskin 2001), whichmimics a bounded-memory pushdown automaton (PDA), supporting simple push andpop operations on a bounded stack-like memory store.
A time-series model is usedhere instead of an explicit stack machine, first because the probability model is welldefined on a bounded memory store, and second because the plasticity of the randomvariables that mimic stack behavior in this model makes the model cross-linguisticallyattractive.
By evoking additional random variables and dependencies, the model canbe defined (or presumably, trained) to mimic other types of automata, such as extendedpushdown automata (EPDAs) recognizing tree-adjoining languages with crossed andnested dependencies, as have been hypothesized for languages like Dutch (Shieber5Computational Linguistics Volume 36, Number 11985).
However, the remainder of this article will only discuss random variables anddependencies necessary to mimic a bounded stack pushdown automaton.3.1 Hierarchical HMMsHierarchical Hidden Markov Models (Murphy and Paskin 2001) are essentially HiddenMarkov Models factored into some fixed number of stack-like elements at each timestep.HMMs characterize speech or text as sequences of hidden states qt (which mayconsist of phones, words, or other hypothesized syntactic or semantic information), andobserved states ot at corresponding time steps t (typically short, overlapping framesof an audio signal, or words or characters in a text processing application).
A mostlikely sequence of hidden states q?1..T can then be hypothesized given any sequence ofobserved states o1..T:q?1..T = argmaxq1..TP(q1..T | o1..T ) (1)= argmaxq1..TP(q1..T ) ?
P(o1..T | q1..T ) (2)def= argmaxq1..TT?t=1P?A (qt | qt?1) ?
P?B (ot | qt) (3)using Bayes?
Law (Equation 2) and Markov independence assumptions (Equation 3)to define a full P(q1..T | o1..T ) probability as the product of a Language Model (?A)prior probability P(q1..T )def=?t P?A (qt | qt?1) and anObservation Model (?B) likelihoodprobability P(o1..T | q1..T )def=?t P?B (ot | qt) (Baker 1975; Jelinek, Bahl, and Mercer 1975).Language model transitions P?A (qt | qt?1) over complex hidden states qt can bemodeled using synchronized levels of stacked-up component HMMs in an HHMM,analogous to a shift-reduce parser or pushdown automaton with a bounded stack.HHMM transition probabilities are calculated in two phases: a ?reduce?
phase (result-ing in an intermediate, transient final-state variable ft), modeling whether componentHMMs terminate; and a ?shift?
phase (resulting in a persistent modeled state qt), inwhich unterminated HMMs transition and terminated HMMs are re-initialized fromtheir parent HMMs.
Variables over intermediate ft and modeled qt states are factoredinto sequences of depth-specific variables?one for each of D levels in the HHMMhierarchy:ft = ?
f 1t .
.
.
fDt ?
(4)qt = ?q1t .
.
.
qDt ?
(5)Transition probabilities are then calculated as a product of transition probabilities ateach level, using level-specific ?reduce?
?F,d and ?shift?
?Q,d models:P?A (qt | qt?1) =?ftP( ft | qt?1) ?
P(qt | ft qt?1) (6)6Schuler et al Parsing Using Human-Like Memory ConstraintsFigure 1Graphical representation of a Hierarchical Hidden Markov Model.
Circles denote randomvariables, and edges denote conditional dependencies.
Shaded circles denote variables withobserved values.def=?f 1t..fDtD?d=1P?F,d ( fdt | fd+1t qdt?1qd?1t?1 ) ?D?d=1P?Q,d (qdt | fd+1t fdt qdt?1qd?1t ) (7)with f D+1t and q0t defined as constants.
In these equations, probabilities are marginalizedor summed over all combinations of intermediate variables f 1t ... fDt , so only the memorystore contents q1t ...qDt persist across time steps.4 A graphical representation of anHHMMwith three depth levels is shown in Figure 1.The independence assumptions in this model can be psycholinguistically moti-vated.
Independence across time points t (Equation 3) arise naturally from causality:Any change to a memory store configuration to generate a configuration at time stept+ 1 should depend only on the current memory store configuration at time step t;memory operations should not be able to peek backward or forward in time to consultpast or future memory stores.
Independence across depth levels d (Equation 7) arisenaturally from uncertainty about the structure between incomplete constituent chunks(this property of right-corner transform categories is elaborated in Section 4).5Shift and reduce probabilities can now be defined in terms of finitely recursiveHMMs with probability distributions over recursive expansion, transition, and reduc-tion of states at each depth level.
In the version of HHMMs used in this paper, eachmodeled variable is a syntactic state qdt ?
G?G (describing an incomplete constituentconsisting of an active grammatical category from domain G and an awaited grammat-ical category from domain G?for example, an incomplete constituent S/NP consistingof an active sentence S awaiting a noun phrase constituent NP); and each intermediate4 In Viterbi decoding, probabilities over intermediate variables may be maximized rather thanmarginalized, but in any case the intermediate variables do not persist.5 Also, the fact that this is a generative model, in which observations are conditioned on hypotheses, thenflipped using Bayes?
Law (Equation 2)?as opposed to a discriminative or conditional model, in whichhypotheses are conditioned directly on observations?is also appealing as a human model, in that itallows the same architecture to be used for both recognition and generation.
This is a desirable propertyfor modeling split utterances, in which interlocutors complete one another?s sentences (Lerner 1991;Helasvuo 2004).7Computational Linguistics Volume 36, Number 1variable is a reduction or non-reduction state f dt ?
G?
{1, 0} (indicating, respectively, areduction of incomplete constituent qdt?1 to a complete right child constituent of somegrammatical category from domainG, or a non-reduction of qdt?1 as a unary or left child,as defined in Section 4).
An intermediate variable f dt at depth d may indicate reductionor non-reduction according to ?F-Reduction,d if there is a reduction at the depth levelimmediately below d, but must indicate non-reduction ( f dt = 0) with probability 1 ifthere is no reduction below:6P?F,d ( fdt | fd+1t qdt?1qd?1t?1 )def={if f d+1t ?G : [ fdt = 0]if f d+1t ?G : P?F-Reduction,d ( fdt | qdt?1, qd?1t?1 )(8)where f D+1t = 1 and q0t = ROOT.Shift probabilities over the modeled variable qdt at each level are defined using level-specific transition ?Q-Transition,d and expansion ?Q-Expansion,d models:P?Q,d (qdt | fd+1t fdt qdt?1qd?1t )def=????
?if f d+1t ?G, fdt ?G : [qdt = qdt?1]if f d+1t ?G, fdt ?G : P?Q-Transition,d (qdt | fd+1t fdt qdt?1qd?1t )if f d+1t ?G, fdt ?G : P?Q-Expansion,d (qdt | qd?1t )(9)where f D+1t = 1 and q0t = ROOT.
This model is conditioned on final-state intermediatevariables f dt and fd+1t at and immediately below each HHMM level.
If there is no re-duction immediately below a given level (the first case provided), it deterministicallycopies the current HHMM state forward to the next time step.
If there is a reductionimmediately below the current level but no reduction at the current level (the secondcase provided), it transitions the HHMM state at the current level, according to thedistribution ?Q-Transition,d.
And if there is a reduction at the current level (the third caseabove), it re-initializes this state given the state at the level above, according to thedistribution ?Q-Expansion,d.Models ?F-Reduction,d, ?Q-Transition,d, and ?Q-Expansion,d are defined directly from train-ing examples, for example (in the experiments described in this article), using relativefrequency estimation.
The overall effect is that higher-level HMMs are allowed totransition only when lower-level HMMs terminate.
An HHMM therefore behaves likea probabilistic implementation of a shift?reduce parser or pushdown automaton with abounded stack, where the maximum stack depth is equal to the number of depth levelsin the HHMM hierarchy.4.
Right-Corner Transform and Incomplete ConstituentsThe model described in this article recognizes trees in a right-corner transformedrepresentation to minimize usage of a bounded short-term memory store.
This right-corner transform is a variant of the left-corner transform described by Johnson (1998a),but whereas the left-corner transform changes left-branching structure into right-branching structure, the right-corner transform changes right-branching structure into6 Here [?]
is an indicator function: [?]
= 1 if ?
is true, 0 otherwise.8Schuler et al Parsing Using Human-Like Memory Constraintsleft-branching structure.
Recognition using this transformed grammar, extracted froma transformed corpus, is similar to recognition using a left-corner parsing strategy (Ahoand Ullman 1972).
This kind of strategy was shown to reduce memory requirementsfor parsing sentences with mainly left- or right-recursive phrase structure to fewer thanseven active or awaited constituent categories (Abney and Johnson 1991).
This is withinMiller?s (1956) estimate of human short-term memory capacity (if memory elementsstore individual categories), whereas parsing heavily center-embedded sentences(known to be difficult for human readers) would require seven or more elements at thefrontier of this capacity.But recent research suggests that human memory capacity may be limited to as fewas three or four distinct items (Cowan 2001), with longer estimates of seven or morepossibly due to the human capacity to chunk remembered items into associated groups(Miller 1956).
The right-corner strategy described in this paper therefore assumesconstituent categories can similarly be chunked into incomplete constituents A/B formedby pairing an active category Awith an awaited category B somewhere along the activecategory?s right progeny (so, for example, a transitive verb may become an incompleteconstituent VP/NP consisting of an active verb phrase lacking an awaited noun phraseyet to come).7 These chunked incomplete constituent categories A and B are naturallyrelated through fixed contiguous phrase structure between them, established duringthe course of parsing prior to the beginning of B, and these incomplete constituents canbe composed with other incomplete constituents B/C to form similarly related categorypairs A/C.These chunks are not only contiguous sections of phrase structure trees, they alsohave contiguous string yields, so they correspond to the familiar notion of text chunksused in shallow parsing approaches (Hobbs et al 1996).
For example, a hypothesizedmemory store may contain incomplete constituents S/NP (a sentence without a nounphrase), followed by NP/NN (a noun phrase lacking a common noun), with cor-responding string yields demand for bonds propped up and the municipal, respectively,forming a complete contiguous segmentation of a sentence at any point in processing.Although these two chunks could be composed into an incomplete constituent S/NN,doing so at this point would close off the possibility of introducing another constituentbetween these two, containing the recognized noun phrase as a left child (e.g., demandfor bonds propped up [NP [NP the municipal bonds]?s prices]).This conception of chunking applied to right-branching progeny in phrase structuretrees does not have the power to eliminate the bounds of a memory store, however.
In alarger cognitive model, syntactic processing is assumed to occur as part of an interactivesemantic interpretation process, in which referents of constituents are calculated asthese constituents are recognized, and are used to constrain subsequent processingdecisions (Tanenhaus et al 1995; Brown-Schmidt, Campana, and Tanenhaus 2002).8The chunked category pairs A and B in these incomplete constituents A/B result fromsuccessful compositions of other such constituents earlier in the recognition process,which means that the relationship between the referents of A and B is known and fixed7 Incomplete constituents may also be defined through a left-corner transform, but left-corner transformedcategories are incomplete in the other direction?a goal category yet to come lacking analready-recognized constituent?so stored incomplete constituent categories resulting from a left-cornertransform would have the character of future goal events, rather than remembered past events.
This isdiscussed in greater detail in Section 4.4.8 This can be implemented in a time-series model by factoring the model to include additional randomvariables over referents, as described in Schuler, Wu, and Schwartz (2009).9Computational Linguistics Volume 36, Number 1in any hypothesized incomplete constituent.
But syntactic and semantic relationshipsbetween chunks in a hypothesized memory store are unspecified.
Chunking beyondthe level of incomplete constituents would therefore involve grouping referents whoseinterrelations have not necessarily been established by the parser.
Because the setof referents is presumably much larger than the set of syntactic categories, one mayassume there are real barriers to reliably chunking them in the absence of these fixedrelationships.There certainly may be cases where syntactically unconnected referents (belongingto different incomplete constituents) could be grouped together as chunks.
But forsimplicity, this article will assume a very strict condition that only a single incompleteconstituent can be stored in each short-term memory element.
Experimental resultsdescribed in Section 5 suggest that a vast majority of English sentences can be recog-nized within these human-like memory bounds, even with this strict condition onchunking.
If parsing can be performed in bounded memory under such strict condi-tions, it can reasonably be assumed to operate at least as well under more permissivecircumstances, where some amount of syntactically-unrelated referential chunking isallowed.Several existing incremental systems are organized around a left-corner parsingstrategy (Roark 2001; Henderson 2004).
But these systems generally keep large numbersof constituents open for modifier attachment in each hypothesis.
This allows modifiersto be attached as right children of any such open constituent.
But if any number ofopen constituents are allowed, then either the assumption that stored elements havefixed syntactic (and semantic) internal structure will be violated, or the assumption thatsyntax operates within a boundedmemory store will be violated, both of which are psy-cholinguistically attractive as simplifying assumptions.
The HHMM model describedin this article upholds both the fixed-element and bounded-memory assumptions byhypothesizing fixed reductions of right child constituents into incomplete parents in thesame memory element, to make room for new constituents that may be introduced at alater time.
These in-element reductions are defined naturally on phrase structure treesas the result of aligning right-corner transformed constituent structures to sequences ofrandom variables in a factored time-series model.
The success of this predictive strategyin corpus-based coverage and accuracy results described in Sections 5 and 6 suggeststhat it may be plausible as a cognitive model.Other accounts may model reductions in bounded memory as occurring as soonas possible, by maintaining the option of undoing them when necessary (Stevenson1998).
This seems unattractive in the context of an interactive semantic model, however,where syntactic constituents and semantic referents are composed in tandem, becausepotentially very rich referential constraints introduced by composing a child constituentinto a parent would have to be systematically undone.
An interesting possibility mightbe that the appearance of syntactic restructuring may arise from a collection of hypoth-esized stores of syntactically fixed incomplete constituents, pursued in parallel.
Theresults presented in this article suggest that this mechanism is possible, but these twopossibilities might be very difficult to distinguish empirically.There is also a tradition of defining incomplete constituents as head-driven?introduced in parsing only at the point in incremental recognition at which they canbe associated with a head word (Gibson 1991: Pritcher 1991: Gorrell 1995).
In typicallyhead-initial languages such as English, incomplete constituents derived from thesehead-driven models resemble those derived from a right-corner transform.
But head-driven incomplete constituents do not appear to obey general-purpose memory boundsin head-final languages such as Japanese, and do not appear to obey attachment prefer-10Schuler et al Parsing Using Human-Like Memory Constraintsences predicted by a head-driven account (Kamide and Mitchell 1999), favoring a pre-head attachment account, as a right-corner transform would predict.4.1 Tree Transforms Using Rewrite RulesThe incomplete constituents used in the present model are defined in terms of treetransforms, which consist of recursive operations that change tree structures into othertree structures.
These transforms are not cognitive processes?syntax in this model islearned and used entirely as time-series probabilities over random variable values inthe memory store.
The role of these transforms is as a means to associate sequences ofconfigurations of incomplete constituents in a memory store with linguistically familiarphrase structure representations, such as those studied in competence models or foundin annotated corpora.The transforms presented in this article will be defined in terms of destructive rewriterules applied iteratively to each constituent of a source tree, from leaves to root, and fromleft to right among siblings, to derive a target tree.
These rewrites are ordered; whenmultiple rewrite rules apply to the same constituent, the later rewrites are applied tothe results of the earlier ones.9 For example, the rewriteA0.
.
.
A1?2 ?3.
.
.
?A0.
.
.
?2 ?3 .
.
.could be used to iteratively eliminate all binary-branching nonterminal nodes in a tree,except the root.In the notation used in this article, Roman uppercase letters (Ai) are variables matching constituent labels, Roman lowercase letters (ai) are variables matching terminal symbols, Greek lowercase letters (?i) are variables matching entire subtree structure, Roman letters followed by colons, followed by Greek letters (Ai:?i) arevariables matching the label and structure, respectively, of the samesubtree, and ellipses (.
.
. )
are taken to match zero or more subtree structures,preserving the order of ellipses in cases where there are more than one (asin the rewrite shown herein).Many of the transforms used in this article are reversible, meaning that the resultof applying a transform to a tree, then applying the reverse of that transform to theresulting tree, will be the original tree itself.
In general, a transform can be reversedif the direction of its rewrite rules is reversed, and if each constituent in a target tree9 The appropriate analogy here is to a Unix sed script, made sensitive to the beginning and end brackets ofa constituent and those of its children.11Computational Linguistics Volume 36, Number 1matches a unique rewrite rule in the reversed transform.
The fact that not all rewritescan be unambiguously matched to HHMM output means that parse accuracy must beevaluated on partially-binarized gold-standard trees, in order to remove the effect ofthis ambiguous matching from the evaluation.
This will be discussed in greater detailin Section 6.4.2 Right-Corner Transform Using Rewrite RulesRewrite rules for the right-corner transform are shown here, first to flatten out right-recursive structure,A1?1 A2?2 A3a3?A1A1/A2?1A2/A3?2A3a3,A1?1 A2A2/A3?2.
.
.?A1A1/A2?1A2/A3?2.
.
.then to replace it with left-recursive structure,A1A1/A2:?1 A2/A3?2?3 .
.
.
?A1A1/A3A1/A2:?1 ?2?3 .
.
.Here, the first two rewrite rules are applied iteratively (bottom-up on the tree) to flattenall right recursion, using incomplete constituents to record the original nonterminalordering.
The second rule is then applied to generate left-recursive structure, preservingthis ordering.
Note that the last rewrite leaves a unary branch at the leftmost child ofeach flattened node.
This preserves the simple category labels of nodes that corresponddirectly to nodes in the original tree, so the original tree can be reconstructed whenthe right-corner transform concatenates multiple right-recursive sequences into a singleleft-recursive sequence.An example of a right-corner transformed tree is shown in Figure 2(c).
An importantproperty of this transform is that it is reversible.
Rewrite rules for reversing a right-corner transform are simply the converse of those shown here.
The correctness of thiscan be demonstrated by dividing a tree into maximal sequences of right-recursivebranches (that is, maximal sequences of adjacent right children).
The first two ?flatten-ing?
rewrites of the right-corner transform, applied to any such sequence, will replacethe right-branching nonterminal nodes with a flat sequence of nodes labeled withslash categories, which preserves the order of the nonterminal category symbols in theoriginal nodes.
Reversing this rewrite will therefore generate the original sequence ofnonterminal nodes.
The final rewrite similarly preserves the order of these nonterminalsymbols while grouping them from the left to the right, so reversing this rewrite willreproduce the flattened tree.12Schuler et al Parsing Using Human-Like Memory ConstraintsFigure 2A sample phrase structure tree (a) as it appears in the Penn Treebank, (b) after it has beenbinarized, and (c) after it has been right-corner transformed.4.3 Mapping Trees to HHMMDerivationsAny tree can be mapped to an HHMM derivation by aligning the nonterminals with qdtcategories.
First, it is necessary to define rightward depth d, right index position t, andfinal (rightmost) child status f dt+1, for every nonterminal node A in a tree, where d is defined to be the number of right branches between node A and theroot,13Computational Linguistics Volume 36, Number 1 t is defined to be the number of words beneath or to the left of node A, and f dt+1 is defined to be 0 if A is a left child, 1 if A is a unary child, and A if A isright.Any right-corner transformed tree can then be annotated with these values and rewrit-ten to define labels and final-state values for every combination of d and t covered bythe tree.
This is done using the rewrite ruled, t,A0, 0d, t,A1, 1?
d, t,A1, 1to replace unary branches with f dt+1 flags, andd, t,A0, fdt+1d, t?,A1, fdt?+1 d+1, t,A2,A2?d, t,A0, fdt+1d, t?1,A1, 0d, t?+1,A1, 0d, t?,A1, fdt?+1d+1, t,A2,A2to copy stacked-up left child constituents over multiple time steps, while lower-level(right child) constituents are being recognized.
The dashed line on the right side of therewrite rule represents the variable number of time steps for a stacked-up higher-levelconstituent (as seen, for example, in time steps 4?7 at depth 1 in Figure 3).
Coordinatesd, t ?
D, and T that have f dt+1=1 are assigned label ??
?, and coordinates not covered bythe tree are assigned label ???
and f dt+1=1.The resulting label and final-state values at each node now define a value of qdtand f dt for each depth d and time step t of the HHMM (see Figure 3).
Probabilities forHHMMmodels ?Q-Expansion,d, ?Q-Transition,d, and ?F-Reduction,d can then be estimated fromthese values directly.
Like the right-corner transform, this mapping is reversible, so qdtand f dt values can be taken from a hypothesized most likely sequence and mapped backFigure 3Sample tree from Figure 2 mapped to qdt variable positions of an HHMM at each stack depth d(vertical) and time step t (horizontal).
This tree uses only two levels of stack memory.
Values forfinal-state variables f dt are not shown.
Note that the mapping transform omits some nonterminallabels; labels for these nodes can be reconstructed from their children.14Schuler et al Parsing Using Human-Like Memory Constraintsto trees (which can then undergo the reverse of the right-corner transform to becomeordinary phrase structure trees).
Inspection of this rewrite rule will reveal the reverse ofthis transform simply involves deleting unary-branching sequences that differ only inthe value of t and restoring unary branches when f dt+1=1.This alignment of right-corner transformed trees also has the interesting propertythat the categories on the stack at any given time step represent a segmentation of theinput up to that time step.
For example, in Figure 3 at t = 12 the stack contains a sentencelacking a verb phrase: S/VP (strong demand for .
.
.
bonds), followed by a verb projectionlacking a particle: VBN/PRT (propped).4.4 Comparison with Left-Corner TransformA right-corner transform is used in this study, rather than a left-corner transform,mainly because the right-corner version coincides with an intuition about how incom-plete constituents might be stored in human memory.
Stacked-up constituents in theright-corner form correspond to chunks of words that have been encountered, ratherthan hypothesized goal constituents.
Intuitively, in the right-corner view, after a sen-tence has been recognized, the stack memory contains a complete sentential constituent(and some associated referent).
In the left corner view, on the other hand, the stackmem-ory after a sentence has been recognized contains only the lower-rightmost constituentin the corresponding phrase structure tree (see Figure 4).
This is because a time-orderFigure 4A left-corner transformed version of the tree (a) and memory store (b) from Figures 2 and 3.15Computational Linguistics Volume 36, Number 1alignment of a left-corner tree to elements in a bounded memory store corresponds toa top-down traversal of the tree, whereas a time-order alignment of a right-corner treeto elements in a bounded memory store corresponds to a bottom-up traversal of thetree.
If referential semantics are assumed to be calculated in tandem (as suggested bythe Tanenhaus et al [1995] results), a top-down traversal through time requires someeffort to reconcile with the traditional compositional semantic notion that the meaningsof constituents are composed from the meanings of their parts (Frege 1892).4.5 Comparison with CCGThe incomplete constituent categories generated in the right-corner transform have thesame form and much of the same meaning as non-constituent categories in a CCG(Steedman 2000).10 Both CCG operations of forward function application:A1  A1/A2 A2and forward function composition:A1/A3  A1/A2 A2/A3appear in the branching structure of right-corner transformed trees.
Nested operationscan also occur in CCG derivations:A1/A2  A1/A2/A3 A3as well as in right-corner transformed trees (using underscore delimiters to denotesequences of constituent categories, described in Section 5.1):A1/A2  A1/A3 A2 A3There are also correlates of type-raising (unary branches introduced by the right-cornertransform operations described in Section 4):A1/A2  A3But, importantly, the right-corner transform generates no correlates to the CCGoperations of backward function application or composition:A1  A2 A1\A2A1\A3  A2\A3 A1\A2This has two consequences.
First, right-corner transform models do not introduce am-biguity between type-raised forward and backward operations, as CCG derivations do.Second, because leftward dependencies (as between a verb and its subject in English)cannot be incorporated into lexical categories, right-corner transform models cannot betaken to explicitly encode argument structure, as CCGs are.
The right-corner transformmodel described in this article is therefore perhaps better regarded as a performancemodel of processing, given subcategorizations specified in some other grammar (suchas in this case the Treebank grammar), rather than a constraint on grammar itself.10 In fact, one of the original motivations for CCG as a model of language was to minimize stack usage inincremental processing (Ades and Steedman 1982).16Schuler et al Parsing Using Human-Like Memory Constraints4.6 Comparison with Cascaded FSAs in Information ExtractionHierarchies of weighted finite-state automata (FSA)?equivalent HMMs may also beviewed as probabilistic implementations of cascaded FSAs, used for modeling syntaxin information extraction systems such as FASTUS (Hobbs et al 1996).
Indeed, the left-branching sequences of transformed constituents recognized by this model (as shownin Figure 3) bear a strong resemblance to the flattened phrase structure representationsrecognized by cascaded FSA systems, in that most phrases are consolidated to flatsequences at one hierarchy level.
This flat structure is desirable in cascaded FSA systemsbecause it allows information to be extracted from noun or verb phrases using straight-forward pattern matching rules, implemented as FSA-equivalent regular expressions.Like FASTUS, this system produces layers of flat phrases that can be searchedusing regular expression pattern-matching rules.
It also has a fixed number of levelsand linear-time recognition complexity.
But unlike FASTUS, the model described herecan produce?and can be trained on?complete phrase structure trees (accessible byreversing the transforms described previously).5.
CoverageThe coverage of this model was evaluated on the large Penn Treebank corpus ofsyntactically annotated sentences from the Switchboard corpus of transcribed speech(Godfrey, Holliman, and McDaniel 1992) and the Wall Street Journal (Marcus, Santorini,and Marcinkiewicz 1993).
These sentences were right-corner transformed and mappedto a time-aligned bounded memory store as described in Section 4 to determine theamount of memory each sentence would require.5.1 Binary Branching StructureIn order to obtain a linguistically plausible right-corner transform representation ofincomplete constituents, the corpus is subjected to another pre-process transform tointroduce binary-branching nonterminal projections, and fold empty categories intononterminal symbols in amanner similar to that proposed by Johnson (1998b) and Kleinand Manning (2003).
This binarization is done in such a way as to preserve linguisticintuitions of head projection, so that the depth requirements of right-corner transformedtrees will be reasonable approximations to the working memory requirements of ahuman reader or listener.First, ordinary phrases and clauses are decomposed into head projections, eachconsisting of one subordinate head projection and one argument or modifier, forexample:A0.
.
.
VB:?1 NP:?2 .
.
.?A0.
.
.
VBVB:?1 NP:?2.
.
.The selection of head constituents is done using rewrite rules similar to the Magerman-Black head rules (Magerman 1995).
Any new constituent created by this process is17Computational Linguistics Volume 36, Number 1assigned the label of the subordinate head projection.
The subordinate projection maybe the left or complete list of head-projection rewrite rules is provided in Appendix A.11Conjunctions are decomposed into purely right-branching structures using non-terminals appended with a ?-LIST?
suffix:A0.
.
.
A1:?1 CC A1:?2?A0.
.
.
A1-LISTA1:?1 CC A1:?2A0.
.
.
A1:?1 A1-LIST:?2?A0.
.
.
A1-LISTA1:?1 A1-LIST:?2This right-branching decomposition of conjoined lists is motivated by the generalpreference in English toward right branching structure, and the distinction of rightchildren as ?-LIST?
categories is motivated by the asymmetry of conjunctions such asand and or generally occurring only between constituents at the end of a list, not at thebeginning.
(Thus, in decomposing coffee, tea or milk, the words tea or milk form an NP-LIST constituent, whereas the words coffee, tea do not.
)Empty constituents are removed outright, along with any unary projections thatmay arise from this removal.
In the case of empty constituents representing traces, theextracted category label is annotated onto the lowest nonterminal dominating the traceusing the suffix ?-extrX,?
where ?X?
is the category of the extracted constituent.
Topreserve grammaticality, this annotation is then passed up the tree and eliminated whenawh-, topicalized, or othermoved constituent is encountered, in amanner similar to thatused in Head-driven Phrase Structure Grammar (Pollard and Sag 1994), but this doesnot affect branching structure.Together these rewrites remove about 65% of super-binary branches from the un-processed Treebank.
All remaining super-binary branches are ?nominally?
decomposedinto right-branching structures by introducing intermediate nodes, each with a labelconcatenated from the labels of its children, delimited by underscores:A0.
.
.
A1:?1 A2:?2?A0.
.
.
A1 A2A1:?1 A2:?2This decomposition is ?nominal?
in that the concatenated labels leave the resulting bi-nary branches just as complex as the original n-ary branches prior to this decomposition.It is equivalent to leaving super-binary branches intact and using dot rules in parsing11 Although it is possible that in some cases these rules may generate counterintuitive branching patterns,inspection of transformed trees during this experiment showed no unusual branching structure, except inthe case of noun sequences in base noun phrases (e.g.
[general obligation] bonds or general [obligationbonds]), which were left flat in the Treebank.
Correct binarization of these structures would requireextensive annotator effort.
However, because base noun phrases are often very small, and seldom containany sub-structure, it seems safe to assume that structural errors in these base noun phrases would notdrastically alter coverage results reported in this section.18Schuler et al Parsing Using Human-Like Memory Constraints(Earley 1970).
This decomposition therefore does nothing to reduce sparse data effectsin statistical parsing.5.2 Coverage ResultsSections 2 and 3 (the standard training set) of the Penn Treebank Switchboard corpuswere binarized as described in Section 5.1, then right-corner transformed and mappedto elements in a boundedmemory store as described in Section 4.
Punctuation added bytranscribers was removed.
Coverage of this corpus, in sentences, for a recognizer usingright-corner transform chunking with one to five levels of stack memory, is shown inTable 1.
These results show that a simple syntax-based chunking into incomplete con-stituents, using the right-corner transform defined in Section 4 of this article, allows avast majority of Switchboard sentences (over 99%) to be recognized using three or fewerelements of memory, with no sentences requiring more than five elements, essentiallyas predicted by studies of human short-term memory.Although spontaneous speech is arguably more natural test data than preparedspeech or edited text, it is possible that coverage results on these data may under-estimate processing requirements, due to the preponderance of very short sentencesand sentence fragments in spontaneous speech (for example, nearly 30% of sentences inthe Switchboard corpus are only one word long).
It may also be argued that coverageresults on this corpus more accurately reflect the complexity of speech planning undersomewhat awkward social circumstances (being asked to start a conversation witha stranger), which may be more cognitively demanding than recognition.
For thesereasons, the right-corner transform chunking was also evaluated on Sections 2?21 (thestandard training set) of the Penn Treebank Wall Street Journal (WSJ) text corpus (seeTable 2, column 1).The WSJ text corpus results appear to show substantially higher memoryrequirements than Switchboard, with only 93% of sentences recognizable using three orfewer memory elements.
But much of this increase is due to arguably arbitrary treebankconventions in annotating punctuation (for example, commas between phrases areattached to the leftmost phrase: ([Pierre Vinken .
.
.
[61 years old] ,] joined .
.
. )
whichcan lead to psycholinguistically implausible analyses in which phrases (in this case61 years old) are center-embedded by lone punctuation marks on one side or the other.In general, branching structure for punctuation can be difficult to motivate on linguisticgrounds, because punctuation marks do not have lexical projections or argumentstructure in most linguistic theories.
In spoken language, punctuation corresponds toTable 1Percent coverage of right-corner transformed Switchboard Treebank Sections 2?3.memory capacity (right-corner, no punct) sentences coverageno stack memory 26,201 28.38%1 stack element 53,740 58.21%2 stack elements 85,068 92.14%3 stack elements 91,890 99.53%4 stack elements 92,315 99.99%5 stack elements 92,328 100.00%TOTAL 92,328 100.00%19Computational Linguistics Volume 36, Number 1Table 2Percent coverage of left- and right-corner transformed WSJ Treebank Sections 2?21.memory capacity right-corner, with punct right-corner, no punct left-corner, no punctsentences coverage sentences coverage sentences coverageno stack elements 35 0.09% 127 0.32% 127 0.32%1 stack elements 3,021 7.57% 3,550 8.90% 4,284 10.74%2 stack elements 21,916 54.95% 25,948 65.06% 26,750 67.07%3 stack elements 37,203 93.28% 38,948 97.66% 38,853 97.42%4 stack elements 39,703 99.54% 39,866 99.96% 39,854 99.93%5 stack elements 39,873 99.97% 39,883 100.00% 39,883 100.00%6 stack elements 39,883 100.00% - - - -TOTAL 39,883 100.00% 39,883 100.00% 39,883 100.00%pauses or patterns of inflection, distributed throughout an utterance.
It therefore seemsquestionable to account for punctuation marks in a psycholinguistic model as explicitcomposable concepts in a memory store.
In order to counter possible undesirableeffects of an arbitrary branching analysis of punctuation, a second evaluation of themodel was performed on a version of the WSJ corpus with punctuation removed.An analysis (Table 2, column 2) of the Penn Treebank WSJ corpus Sections 2?21without punctuation, using the right-corner transformed trees just described, showsthat 97.66% of trees can be recognized using three hidden levels, and 99.96% can berecognized using four, and again (similar to the Switchboard results), no sentencesrequire more than five remembered incomplete constituents.
Table 2, column 3, showssimilar results for a left-corner transformed corpus, using left-right reflections of therewrite rules presented in Section 4.Cowan (2001) documents empirically observed short-term memory limits of aboutfour elements across awide variety of tasks.
It is therefore not surprising to find a similarlimit in the memory required to parse the Treebank, assuming elements correspondingto right-corner-transformed incomplete constituents.As the table shows, some quintuply center-embedded constituents were found inboth corpora, suggesting that a three- to four-element limit may be soft, and can berelaxed for short durations.
Indeed, all quintuply embedded constituents were only afew words long.
Interestingly, many of the most heavily embedded words seemed tostrongly co-occur, which may suggest that these words arise from fixed expressions andare not compositional.
For example, Figure 5 shows one of the 13 phrase structure treesin the Switchboard corpus which require five stack elements in right-corner parsing.The complete sentence is:So if there?s no one else around and I have a chance to listen to something I?ll turn that on.If the construction there ?s NP AP in this sentence is parsed non-compositionally as asingle expression (and thus is rendered left-branching by the right-corner transform asdefined in Section 4), the sentence could be parsed using only four memory elements.Even constrained to only four center embeddings, the existence of such sentencesconfounds explanations of the center-embedding difficulties as directly arising fromstack limits in a left-corner (or right-corner) parser (Abney and Johnson 1991).
It isalso interesting to note that three of the incomplete constituents in this example arerecursively nested or self-embedded instances of sentential projections, essentially with20Schuler et al Parsing Using Human-Like Memory ConstraintsFigure 5A phrase structure tree requiring five stack elements.
Categories in bold will be incomplete at apoint after recognizing so if there?s no .
.
.the same category, similar to the center-embedded constructions which human readersfound difficult to process.
This suggests that restrictions on self-embedding of identicalconstituent categories would also fail to predict readability.Instead, these data seem to argue in favor of an explanation due to probability:Although the five-element sentences found in the Treebank use mostly common phrasestructure rules, problematic center-embedded sentences like the salmon the man the dogchased smoked fell may cause difficulty simply because they are examples of an unusualconstruction: a nested object relative clause.
The fact that this is an unusual constructionmay in turn be a result of the fact that speakers tend to avoid nesting object relativeclauses because they can lead to memory exhaustion, though such constructions maybecome readable with practice.6.
In-Element Composition Ambiguity and Parsing AccuracyThe right-corner transform described in Section 4 saves memory because it transformsany right-branching sequence with left-child subtrees into a left-branching sequence ofincomplete constituents, with the same sequence of subtrees as right children.
The left-branching sequences of siblings resulting from this transform can then be composedbottom-up through time by replacing each left child category with the category of theresulting parent, within the same memory element (or depth level).
For example, inFigure 6(a) a left-child category NP/NP at time t = 4 is composed with a noun new ofcategory NP/NNP (a noun phrase lacking a proper noun yet to come), resulting in anew parent category NP/NNP at time t = 5 replacing the left child category NP/NP inthe topmost d = 1 memory element.This in-element composition preserves elements of the bounded memory store foruse in processing descendants of this composed constituent, yielding the human-likememory demands reported in Section 5.
But whenever an in-element composition likethis is hypothesized, it isolates an intermediate constituent (in this example, the nounphrase new york city) from subsequent composition.
Allowing access to this intermediateconstituent?for example, to allow new york city to become a modifier of bonds, whichitself becomes an argument of for?requires an analysis in which the intermediate21Computational Linguistics Volume 36, Number 1Figure 6Alternative analyses of strong demand for new york city ...: (a) using in-element composition,compatible with strong demand for new york city is ... (in which the demand is for the city); and (b)using cross-element (or delayed) composition, compatible with either strong demand for new yorkcity is ... (in which the demand is for the city) or strong demand for new york city bonds is ... (inwhich a forthcoming referent?in this case, bonds?is associated with the city, and is indemand).
In-element composition (a) saves memory but closes off access to the noun phraseheaded by city, and so is not incompatible with the ...bonds completion.
Cross-elementcomposition (b) requires more memory, but allows access to the noun phrase headed by city, sois compatible with either completion.
This ambiguity is introduced at t = 4 and propagated untilat least t = 7.
An ordinary, non-right-corner stack machine would exclusively use analysis (b),avoiding ambiguity.constituent is stored in a separate memory element, shown in Figure 6(b).
This createsa local ambiguity in the parser (in this case, from time step t = 4) that may have to bepropagated across several words before it can be resolved (in this case, at time stept = 7).
This is essentially an ambiguity between arc-eager (in-element) and arc-standard(cross-element) composition strategies, as described by Abney and Johnson (1991).
Incontrast, an ordinary (purely arc-standard) parser with an unbounded stack would onlyhypothesize analysis (b), avoiding this ambiguity.12The right-corner HHMM approach described in this article relies on a learnedstatistical model to predict when in-element (arc-eager) compositions will occur, inaddition to hypothesizing parse trees.
The model encodes a mixed strategy: with someprobability arc-eager or arc-standard for each possible expansion.
Accuracy results ona right-corner HHMM model trained on the Penn Wall Street Journal Treebank suggestthat this kind of optionally arc-eager strategy can be reliably statistically learned.6.1 EvaluationIn order to determinewhether amemory-preserving parsing strategy, like the optionallyarc-eager strategy, can be reliably learned, a baseline Cocke-Kasami-Younger (CKY)parser and bounded-memory right-corner HHMM parser were evaluated on the stan-dard Penn Treebank WSJ Section 23 parsing task, using the binarized tree set describedin Section 5.2 (WSJ Sections 2?21) as training data.
Training examples requiring more12 It is important to note that neither the right-corner nor left-corner parsing strategy by itself creates thisambiguity.
The ambiguity arises from the decision to use this optionally arc-eager strategy to reducememory store allocation in a bounded memory parser.
Implementations of left-corner parsers such asthat of Henderson (2004) adopt an arc-standard strategy, essentially always choosing analysis (b), andthus do not introduce this kind of local ambiguity.
But in adopting this strategy, such parsers mustmaintain a stack memory of unbounded size, and thus are not attractive as models of human parsing inshort-term memory (Resnik 1992).22Schuler et al Parsing Using Human-Like Memory Constraintsthan four stack elements were excluded from training, in order to avoid generatinginconsistent model probabilities (e.g., from expansions that could not be re-composedwithin the bounded memory store).Most likely sequences of HHMM stack configurations are evaluated by reversingthe binarization, right-corner, and time-series mapping transforms described in Sec-tions 4 and 5.
But some of the binarization rewrites cannot be completely reversed,because they cannot be unambiguously matched to output trees.
Automatically derivedlexical projections below the annotated phrase level (e.g., binarizations of base nounphrases) can be completely reversed, because the derived categories are character-istically labeled with terminal symbols.
So, too, can the conjunction and ?nominal?binarizations described in Section 5.1, because they can be identified by characteristic?-LIST?
and underscore delimiters.
But automatically derived projections above theannotated phrase level cannot be reliably identified in parser output (for example, anintermediate projection ?S PP S?may or may not be annotated in the corpus).
In orderto isolate the evaluation from the effects of these ambiguous matchings, the evaluationwas performed using trees in a partially binarized format, obtained by reversing onlythose rewrites that result in unambiguous matches.
Evaluating on this partially bina-rized data does not seem to unfairly increase parsing performance compared to otherpublished results?quite the contrary: an evaluation using the state-of-the-art Charniak(2000) parser scores about half a point worse on labeled F-score (89.3% vs. 89.9%) whenits hypotheses and gold standard trees are converted into this format.13Both CKY baseline and HHMM test systems were run with a simple part of speech(POS) model using relative frequency estimates from the training set, backed off to adiscriminative (decision tree) model conditioned on the last five letters of each word,normalized over unigram POS probabilities.
The CKY baseline andHHMMresults wereobtained by training and evaluating on binarized trees, which is a necessary conditionfor the right-corner transform.
The CKY baseline results appear to be better than thosefor a baseline probabilistic context-free grammar (PCFG) system reported by Klein andManning (2003) using no modifications to the corpus, and no parent or sibling condi-tioning (see Table 3, top) because the binarization process allows the parser to avoidsome sparse data effects due to large flat branching structures in the Treebank, resultingin improved parsing accuracy.
Klein and Manning note that applying linguisticallymotivated binarization transforms can yield substantial improvements in accuracy?asmuch as nine points, in their study (in comparison, binarization only seems to improveaccuracy by about seven points above an unmodified baseline in the present study).
Butthe Klein and Manning results for binarization are provided only for models alreadyaugmented with Markov dependencies (that is, conditioning on parent and siblingcategories, analogous to HHMM dependencies), so it was not possible to compare toa binarized and un-Markovized benchmark.The results for HHMM parsing, training, and evaluating on these same binarizedtrees (modulo right-corner and variable-mapping transforms) were substantially bet-ter than binarized CKY, most likely due to the expanded HHMM dependencies onprevious (qdt?1) and parent (qd?1t ) variables at each qdt .
For example, binarized PCFGprobabilities may be defined in terms of three category symbols A, B, and C: P(A B C |A); whereas some of the HHMM probabilities are defined in terms of five category13 This is presumably because the probability that a human annotator will annotate phrase structurebrackets at a particular projection or not is something existing parsers learn and exploit to improve theiraccuracy.
But it is not clear that this distinction is linguistically motivated.23Computational Linguistics Volume 36, Number 1Table 3Labeled recall (LR), labeled precision (LP), weighted average (F-score), and parse failure(% of sentences yielding no tree output) results for basic CKY parser and HHMM parser onunmodified and binarized WSJ Sections 22 (sentences 1?393: ?devset?)
and 23?24 (all sentences).Results are shown with and without punctuation, compared to Klein and Manning 2003(KM?03) using baseline and parent+sibling (par+sib) conditioning, and Roark 2001 (R?01) usingparent+sibling conditioning.
Baseline CKY and test (parent+sibling) cases for the HHMMsystem start out at a higher accuracy than for the Klein-Manning system because the HHMMsystem requires binarization of trees, which removes some data sparsity in the raw Treebankannotation, whereas the Klein-Manning results are computed prior to binarization.
Because it isincremental, the parser occasionally eliminates all continuable analyses from the beam, andtherefore fails to find a parse.
HHMM parse failures are accounted as zeros in the recall statistics,but are also listed separately, because in principle it might be possible to recover useful syntacticstructure from partial sequences.with punctuation: (?40 wds) LR LP F-score sentence errorfailure reductionKM?03: unmodified, devset ?
?
72.6 0KM?03: par+sib, devset ?
?
77.4 0 17.5%CKY: binarized, devset 80.3 79.9 80.1 0.8HHMM: par+sib, devset 84.1 83.5 83.8 0.5 18.6%CKY: binarized, sect 23 78.8 79.4 79.1 0.1HHMM: par+sib, sect 23 83.4 83.7 83.5 0.1 21.1%no punctuation: (?120 wds) LR LP F failR?01: par+sib, sect 23?24 75.2 77.4 ?
0.1HHMM: par+sib, sect 23?24 77.2 78.3 77.7 0.0labels: P(A/B |C/D, E) (transitioning from incomplete constituent C/D to incompleteconstituent A/B in the context of an expanding category E).
This increases the numberof free parameters (estimated conditional probabilities) in the model,14 but apparentlynot to the point of sparsity; this is similar to the effect of horizontal Markovization (con-ditioning on the sibling category immediately previous to an expanded category) andvertical Markovization (conditioning on the parent of an expanded category) commonlyused in PCFG parsing models (Collins 1999).The improvement due to HHMM parsing over the PCFG baseline (18.6% reductionin error) is comparable to that reported by Klein and Manning for parent and siblingdependencies (first-order vertical and horizontal Markovization) over a baseline PCFGwithout binarization (17.5% reduction in error).
However, because it is not possibleto run the HHMM parser without binarization, and because Klein and Manning donot report results for binarization transforms in the absence of parent and siblingMarkovization, it is potentially misleading to compare the results directly.
For example,it is possible that the binarization transforms described here may have performance-optimizing effects that are latent in the binarized PCFG, but are brought out in HHMMparsing.Results on Section 23 of this corpus show close to 84% recall and precision, compa-rable to that reported for state-of-the-art cubic-time parsers (with no constant bounds14 Without punctuation, the HHMMmodel has 50,429 free parameters (including both Q and F models),whereas the binarized PCFG has 12,373.24Schuler et al Parsing Using Human-Like Memory Constraintson processing storage) using similar configurations of conditioning information, that is,without lexicalization or smoothing.Roark (2001) describes a similar incremental parser based on left-corner trans-formed grammars, and also reports results for parsing with and without parent andsibling Markovization.
Again the performance is comparable under similar conditions(Table 3, bottom).This system was run with a beam width of 2,000 hypotheses.
This beam widthwas selected in order to compare the performance of the bounded-memory model,which predicts in-element or cross-element composition, with that of conventionalbroad-coverage parsers, which also maintain large beams.
With better modeling andvastly more data from which to learn, it is possible that the human processor mayneed to maintain far fewer alternative analyses, or perhaps only one, conditioned ona lookahead window of observations (Henderson 2004).15These experiments used a maximum stack depth of four, and conditioned expan-sion and transition probabilities for each qdt on only the portion of the parent categoryfollowing the slash (that is, only A2 of A1/A2), in order to avoid sparse data effects.Examples requiring more than four stack elements were excluded from training.
Thisis because in the basic relative frequency estimation used here, training examples aredepth-specific.
Because the (unpunctuated) training set contains only about a dozensentences requiring more than four depth levels, each occupying that level for only afew words, the data on which the fifth level of this model would be trained are verysparse.
Models at greater stack depths, and models depending on complete parent cate-gories (or grandparent categories, etc., as in state-of-the-art parsers) could be developedusing smoothing and backoff techniques or feature-based log-linear models, but this isleft for later work (see Section 7).7.
ConclusionThis article has described a model of human syntactic processing that recognizes com-plete phrase structure trees using only a small store of memory elements of limitedcomplexity.
Sequences of hypothesized contents of this memory store can be mapped toand from conventional phrase structure trees using a reversible right-corner transform.If this syntactic processing model is combined with a bounded-memory interpreter(Schuler, Wu, and Schwartz 2009), however, allowing the contents of this store to beincrementally interpreted within the same bounded memory, it stands to reason thatcomplete, explicit phrase structure trees would not need to be constructed at any timein processing, in keeping with experimental results showing similar lack of retention ofwords and syntactic structure during human processing (Sachs 1967; Jarvella 1971).Initial results show the use of a memory store consisting of only three to four mem-ory elements within this framework provides nearly complete coverage of the PennTreebank Switchboard and WSJ corpora, consistent with recent estimates of general-purpose short-term memory capacity.
This suggests that, unlike some earlier mod-els, the hypothesis that human sentence processing uses general-purpose short-term15 Although, if most competing analyses are unconscious, they would be difficult to detect.
Formally, thecompeting pockets of activation hypothesized in a parallel-processing version of this model could bearbitrarily small and numerous, but it seems unlikely that very small pockets of activation would persistfor very long (just as low probability analyses would be unlikely to remain on the HHMM beam).
Thispossibility is discussed in the particle filter account of Levy (2008).25Computational Linguistics Volume 36, Number 1memory to store incomplete constituents, as defined by a right-corner transform, doesnot seem to substantially underestimate human processing capacity.
Moreover, despiteadditional predictions that must take place within this model to manage parsing in suchclose quarters, preliminary accuracy results for an unlexicalized, un-smoothed versionof this model, using only a four-element memory store, show close to 84% recall andprecision on the standard parsing evaluation.
This result is comparable to that reportedfor state-of-the-art cubic-time parsers (with no constant bounds on processing storage)using similar configurations of conditioning information, namely, without lexicalizationor smoothing.This model does not attempt to derive processing difficulties frommemory bounds,following evidence that garden path and center-embedding processing difficulties arecaused by interference or local probability estimates rather than encounters with mem-ory capacity limits.
But this does not mean that memory store capacity and probabilisticexplanations of processing difficulty are completely independent.
Probability estima-tion seems likely to be dependent on structural information from the memory store (forexample, incomplete object relative clauses seem to be very improbable in the contextof other incomplete object relative clauses).
As hypotheses use more elements in thememory store, the distribution over these hypotheses will tend to become broader,taxing the reservoir of activation capacity, and making it more likely for low proba-bility hypotheses to disappear, increasing the incidence of garden path errors.
Furtherinvestigations into how the memory store elements are allocated in various syntacticcontexts may allow these apparently disparate dimensions of processing capacity to beunified.The model described here may be promising as an engineering tool as well.
Butto achieve competitive performance with unconstrained state-of-the-art parsers willrequire the development of additional approximation algorithms beyond the scope ofthis article.
This is because most modern parsers are lexicalized, incorporating head-word dependencies into parsing decisions, and employing finely tuned smoothing andbackoff techniques to integrate these potentially sparse head-word dependencies withdenser unlexicalized models.
The bounded-memory right-corner HHMM describedin this article can also be lexicalized in this way, but because head word dependenciesare most straightforwardly defined in terms of top-down PCFG-like dependencystructures, this lexicalization requires the introduction of additional formal machineryto transform PCFG probabilities into right-corner form (Schuler 2009).
In otherwords, rather than transforming a training set of trees and mapping them to a timeseries model, it is necessary to transform a consistent probabilistically weightedgrammar (in some sense, an infinite set of trees) into appropriately weighted andconsistent right-corner PCFG and HHMM models.
This requires the introduction ofan approximate inference algorithm, similar to that used in value iteration (Bellman1957), which estimates probabilities of infinite left-recursive or right-recursive chainsby exploiting the fact that increasingly longer chains of events contribute exponentiallydecreasing probability mass.
On top of this, preserving head-word dependencies inincremental processing also requires the introduction of a framework for storing headwords of modifier constituents that precede the head word of a parent constituent;including some mechanism to ensure that probability assignments are fairly distributedamong competing hypotheses (e.g., by marginalizing over possible head words) incases where the calculation of accurate dependency probabilities must be deferreduntil the head word of the parent constituent is encountered.
For these reasons, acomplete lexicalized model is considered beyond the scope of this article, and is left forfuture work.26Schuler et al Parsing Using Human-Like Memory ConstraintsAppendix A: Head Transform RulesThe experiments described in this article used a binarization process that included thefollowing rewrite rules, designed to binarize flat Treebank constituents into linguisti-cally motivated head projections:1.
NP: right-binarize basal NPs as much as possible; then left-binarize NPsafter left context reduced to nil:A0=NP|WHNP.
.
.
A1=[A-Z]*:?1 A2=NN[A-Z]*:?2 .
.
.?A0.
.
.
A2A1:?1 A2:?2.
.
.A0=NPA1=NN[A-Z]*|NP:?1 A2=PP|S|VP|WHSBAR:?2 .
.
.
?A0A1A1:?1 A2:?2.
.
.2.
VP: left-binarize basal VPs as much as possible; then right-binarize VPsafter right context reduced to nil:A0=VP|SQ.
.
.
A1=VB[A-Z]*|BES:?1 A2=[A-Z]*:?2 .
.
.?A0.
.
.
A1A1:?1 A2:?2.
.
.A0=VP.
.
.
A1=ADVP|RB[A-Z]*|PP:?1 A2=VB[A-Z]*|VP:?2?A0.
.
.
A2A1:?1 A2:?23.
ADJP: right-binarize basal ADJPs as much as possible; then left-binarizeADJPs after left context reduced to nil:A0=ADJP[A-Z]*.
.
.
A1=RB[A-Z]*:?1 A2=JJ[A-Z]*:?2 .
.
.?A0.
.
.
A2A1:?1 A2:?2.
.
.A0=ADJPA1=JJ[A-Z]*|ADJP:?1 A2=PP|S:?2 .
.
.
?A0A1A1:?1 A2:?2.
.
.4.
ADVP: right-binarize basal ADVPs as much as possible; then left-binarizeADVPs after left context reduced to nil:A0=ADVP.
.
.
A1=RB[A-Z]*:?1 A2=RB[A-Z]*:?2 .
.
.?A0.
.
.
A2A1:?1 A2:?2.
.
.A0=ADVPA1=RB[A-Z]*|ADVP:?1 A2=PP|S:?2 .
.
.
?A0A1A1:?1 A2:?2.
.
.27Computational Linguistics Volume 36, Number 15.
PP: left-binarize PPs as much as possible; then right-binarize PPs afterright context reduced to nil:A0=PP|SBAR.
.
.
A1=IN|TO:?1 A2=[A-Z]*:?2 .
.
.?A0.
.
.
A1A1:?1 A2:?2.
.
.A0=PP.
.
.
A1=ADVP|RB|PP:?1 A2=PP:?2?A0.
.
.
A2A1:?1 A2:?26.
S: group subject NP and predicate VP of a sentence; then group modifiersto right and left:A0=S[A-Z]*.
.
.
A1=NP:?1 A2=VP:?2 .
.
.?A0.
.
.
SA1:?1 A2:?2.
.
.A0=S[A-Z]*.
.
.
A1=ADVP|RB[A-Z]*|PP:?1 A2=VB[A-Z]*|VP:?2 .
.
.?A0.
.
.
A2A1:?1 A2:?2.
.
.A0=S[A-Z]*.
.
.
A1=ADVP|RB[A-Z]*|PP:?1 A2=A0:?2 .
.
.?A0.
.
.
A2A1:?1 A2:?2.
.
.A0=S[A-Z]*.
.
.
A1=A0:?1 A2=ADVP|RB[A-Z]*|PP:?2 .
.
.?A0.
.
.
A1A1:?1 A2:?2.
.
.AcknowledgmentsThe authors would like to thankthe anonymous reviewers for their input.This research was supported by NationalScience Foundation CAREER/PECASEaward 0447685 and by NASA awardNNX08AC36A.
The views expressed are notnecessarily endorsed by the sponsors.ReferencesAbney, Steven P. and Mark Johnson.
1991.Memory requirements and localambiguities of parsing strategies.J.
Psycholinguistic Research, 20(3):233?250.Ades, Anthony E. and Mark Steedman.
1982.On the order of words.
Linguistics andPhilosophy, 4:517?558.Aho, Alfred V. and Jeffery D. Ullman.
1972.The Theory of Parsing, Translation andCompiling; Volume.
I: Parsing.
Prentice-Hall,Englewood Cliffs, NJ.Baker, James.
1975.
The Dragon system: anoverview.
IEEE Transactions on Acoustics,Speech and Signal Processing, 23(1):24?29.Bellman, Richard.
1957.
DynamicProgramming.
Princeton University Press,Princeton, NJ.Berg, George.
1992.
A connectionist parserwith recursive sentence structure andlexical disambiguation.
In Proceedings of theTenth National Conference on ArtificialIntelligence, pages 32?37, San Jose, CA.Bever, Thomas G.?1970.
The cognitive basisfor linguistic structure.
In J. R?.
Hayes,editor, Cognition and the Development ofLanguage.
Wiley, New York, pages 279?362.Brown-Schmidt, Sarah, Ellen Campana, andMichael K. Tanenhaus.
2002.
Referenceresolution in the wild: Onlinecircumscription of referential domains in a28Schuler et al Parsing Using Human-Like Memory Constraintsnatural interactive problem-solving task.In Proceedings of the 24th Annual Meeting ofthe Cognitive Science Society, pages 148?153,Fairfax, VA.Charniak, Eugene.
2000.
A maximum-entropy inspired parser.
In Proceedingsof the First Meeting of the North AmericanChapter of the Association for ComputationalLinguistics (ANLP-NAACL?00),pages 132?139, Seattle, WA.Chomsky, Noam and George A. Miller.1963.
Introduction to the formalanalysis of natural languages.In Handbook of Mathematical Psychology.Wiley, New York, pages 269?321.Collins, Michael.
1999.
Head-drivenstatistical models for natural language parsing.Ph.D.
thesis, University of Pennsylvania.Cowan, Nelson.
2001.
The magicalnumber 4 in short-term memory:A reconsideration of mental storagecapacity.
Behavioral and Brain Sciences,24:87?185.Crain, Stephen and Mark Steedman.1985.
On not being led up the garden path:The use of context by the psychologicalsyntax processor.
In D. R. Dowty,L.
Karttunen, and A. M. Zwicky, editors,Natural Language Parsing: Psychological,Computational, and Theoretical Perspectives,number 1 in Studies in Natural LanguageProcessing.
Cambridge UniversityPress, Cambridge, pages 320?358.Earley, Jay.
1970.
An efficient context-freeparsing algorithm.
CACM, 13(2):94?102.Elman, Jeffrey L. 1991.
Distributedrepresentations, simple recurrentnetworks, and grammatical structure.Machine Learning, 7:195?225.Ericsson, K. Anders and Walter Kintsch.1995.
Long-term working memory.Psychological Review, 102:211?245.Frege, Gottlob.
1892.
Uber sinnund bedeutung.
Zeitschrift fur Philosophieund Philosophischekritik, 100:25?50.Gibson, Edward.
1991.
A computational theoryof human linguistic processing: Memorylimitations and processing breakdown.Ph.D.
thesis, Carnegie Mellon University.Godfrey, John J., Edward C. Holliman,and Jane McDaniel.
1992.
Switchboard:Telephone speech corpus for researchand development.
In Proceedings ofICASSP, pages 517?520, San Francisco, CA.Gordon, N. J., D. J. Salmond, and A. F. M.Smith.
1993.
Novel approach to nonlinear/non-gaussian bayesian state estimation.IEE Proceedings F (Radar and SignalProcessing), 140(2):107?113.Gorrell, Paul.
1995.
Syntax and Parsing.Cambridge University Press, Cambridge.Hale, John.
2001.
A probabilistic earley parseras a psycholinguistic model.
In Proceedingsof the Second Meeting of the North AmericanChapter of the Association for ComputationalLinguistics, pages 159?166, Pittsburgh, PA.Hale, John.
2006.
Uncertainty about therest of the sentence.
Cognitive Science,30(4):609?642.Helasvuo, Marja-Liisa.
2004.
Sharedsyntax: the grammar of co-constructions.Journal of Pragmatics, 36:1315?1336.Henderson, James.
2004.
Lookaheadin deterministic left-corner parsing.In Proceedings Workshop on IncrementalParsing: Bringing Engineering andCognition Together, pages 26?33, Barcelona.Hobbs, Jerry R., Douglas E. Appelt, John Bear,David Israel, Megumi Kameyama, MarkStickel, and Mabry Tyson.
1996.
Fastus:A cascaded finite-state transducer forextracting information from natural-language text.
In Yves Schabes, editor,Finite State Devices for Natural LanguageProcessing.
MIT Press, Cambridge, MA,pages 383?406.Jarvella, Robert J.
1971.
Syntacticprocessing of connected speech.
Journalof Verbal Learning and Verbal Behavior,10:409?416.Jelinek, Frederick, Lalit R. Bahl, and Robert L.Mercer.
1975.
Design of a linguisticstatistical decoder for the recognitionof continuous speech.
IEEE Transactionson Information Theory, 21:250?256.Johnson, Mark.
1998a.
Finite stateapproximation of constraint-basedgrammars using left-corner grammartransforms.
In Proceedings of COLING/ACL,pages 619?623, Montreal.Johnson, Mark.
1998b.
PCFG modelsof linguistic tree representation.Computational Linguistics, 24:613?632.Johnson-Laird, P. N. 1983.
Mental Models:Towards a Cognitive Science of Language,Inference and Consciousness.
HarvardUniversity Press, Cambridge, MA.Just, Marcel Adam and Patricia A.Carpenter.
1992.
A capacity theoryof comprehension: Individual differencesin working memory.
Psychological Review,99:122?149.Just, Marcel Adam and Sashank Varma.2007.
The organization of thinking:What functional brain imagingreveals about the neuroarchitecture ofcomplex cognition.
Cognitive, Affective,& Behavioral Neuroscience, 7:153?191.29Computational Linguistics Volume 36, Number 1Kamide, Yuki and Don C. Mitchell.
1999.Incremental pre-head attachment inJapanese parsing.
Language andCognitive Processes, 14:631?662.Klein, Dan and Christopher D. Manning.2003.
Accurate unlexicalized parsing.
InProceedings of the 41st Annual Meeting of theAssociation for Computational Linguistics,pages 423?430, Sapporo.Lerner, Gene H. 1991.
On the syntax ofsentences in progress.
Language inSociety, 20:441?458.Levy, Roger.
2008.
Modeling the effects ofmemory on human online sentenceprocessing with particle filters.
InProceedings of NIPS, pages 937?944,Vancouver.Lewis, Richard L. and Shravan Vasishth.2005.
An activation-based model ofsentence processing as skilledmemory retrieval.
Cognitive Science,29(3):375?419.Magerman, David.
1995.
Statistical decision-tree models for parsing.
In Proceedings of the33rd Annual Meeting of the Associationfor Computational Linguistics (ACL?95),pages 276?283, Cambridge, MA.Marcus, Mitch.
1980.
Theory of SyntacticRecognition for Natural Language.
MIT Press,Cambridge, MA.Marcus, Mitchell P., Beatrice Santorini, andMary Ann Marcinkiewicz.
1993.
Buildinga large annotated corpus of English: thePenn Treebank.
Computational Linguistics,19(2):313?330.Mayberry, III, Marshall R. and RistoMiikkulainen.
2003.
Incrementalnonmonotonic parsing through semanticself-organization.
In Proceedings of the25th Annual Conference of the CognitiveScience Society, pages 798?803,Boston, MA.Miller, George A.
1956.
The magical numberseven, plus or minus two: Some limitson our capacity for processing information.Psychological Review, 63:81?97.Murphy, Kevin P. and Mark A. Paskin.
2001.Linear time inference in hierarchicalHMMs.
In Proceedings of NIPS,pages 833?840, Vancouver.Pollard, Carl and Ivan A.
Sag.
1994.Head-Driven Phrase StructureGrammar.
Chicago: University of ChicagoPress and Stanford: CSLI Publications.Pritchett, Bradley L. 1991.
Head positionand parsing ambiguity.
Journal ofPsycholinguistic Research, 20:251?270.Resnik, Philip.
1992.
Left-corner parsing andpsychological plausibility.
In Proceedingsof COLING, pages 191?197, Nantes.Roark, Brian.
2001.
Probabilistic top-downparsing and language modeling.Computational Linguistics, 27(2):249?276.Rohde, Douglas L. T. 2002.
A connectionistmodel of sentence comprehension andproduction.
Ph.D. thesis, Computer ScienceDepartment, Carnegie Mellon University.Sachs, Jacqueline.
1967.
Recognition memoryfor syntactic and semantic aspects ofconnected discourse.
Perception andPsychophysics, 2:437?442.Schuler, William.
2009.
Parsing with abounded stack using a model-basedright-corner transform.
In Proceedingsof the North American Association forComputational Linguistics (NAACL ?09),pages 344?352, Boulder, CO.Schuler, William, Stephen Wu, andLane Schwartz.
2009.
A framework forfast incremental interpretation duringspeech decoding.
ComputationalLinguistics, 35(3):313?343.Shieber, Stuart.
1985.
Evidenceagainst the context-freeness of naturallanguage.
Linguistics and Philosophy,8:333?343.Smolensky, P. and G. Legendre.
2006.
TheHarmonic Mind: From Neural Computationto Optimality-Theoretic Grammar.
MIT Press,Cambridge, MA.Steedman, Mark.
2000.
The SyntacticProcess.
MIT Press/Bradford Books,Cambridge, MA.Stevenson, Suzanne.
1998.
Parsing asincremental restructuring.
In J. D. Fodorand F. Ferreira, editors, Reanalysis inSentence Processing.
Kluwer Academic,Boston, MA, pages 327?363.Tanenhaus, Michael K., Michael J.Spivey-Knowlton, Kathy M. Eberhard,and Julie E. Sedivy.
1995.
Integration ofvisual and linguistic information inspoken language comprehension.
Science,268:1632?1634.30
