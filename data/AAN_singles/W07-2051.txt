Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 241?244,Prague, June 2007. c?2007 Association for Computational LinguisticsMELB-YB: Preposition Sense Disambiguation Using Rich SemanticFeaturesPatrick Ye and Timothy BaldwinComputer Science and Software EngineeringUniversity of Melbourne, Australia{jingy,tim}@csse.unimelb.edu.auAbstractThis paper describes a maxent-based prepo-sition sense disambiguation system entry tothe preposition sense disambiguation taskof the SemEval 2007.
This system uses awide variety of semantic and syntactic fea-tures to perform the disambiguation task andachieves a precision of 69.3% over the testdata.1 IntroductionPrepositional phrases (PPs) are both common andsemantically varied in open English text.
While theconventional view on prepositions from the com-putational linguistics community has been that theyare semantically transient at best, and semantically-vacuous at worst, a robust account of the semanticsof prepositions and disambiguation method can behelpful in a range of NLP tasks including machinetranslation, parsing (prepositional phrase attach-ment) and semantic role labelling (Durand, 1993;O?Hara and Wiebe, 2003; Ye and Baldwin, 2006a).The SemEval 2007 preposition sense disambigua-tion task provides a common test bed for the evalua-tion of preposition sense disambiguation systems.Our proposed method is maximum entropy based,and combines features developed in the context ofpreposition sense disambiguation for semantic rolelabelling (Ye and Baldwin, 2006a), and verb sensedisambiguation (Ye and Baldwin, 2006b).The remainder of this paper is structured as fol-lows.
We first discuss the pre-processing stepsused in our system (Section 2), and outline the fea-tures our preposition disambiguation method uses(Section 3) and our parameter tuning method (Sec-tion 4).
We then discuss and analyse the results ofour method (Section 5) and conclude the paper (Sec-tion 6).2 Pre-processingThe following list shows the pre-processing stepsthat our system goes through and the tools used:Part of speech tagging SVMTool version 1.2(Gime?nez and Ma`rquez, 2004).Chunking An in-house chunker implementedwith fnTBL, a transformation based learner (Ngaiand Florian, 2001), and trained on the British Na-tional Corpus (BNC).1Parsing Charniak?s re-ranking parser, version Au-gust, 2006 (Charniak and Johnson, 2005).Named entity extraction A statistical NER sys-tem described in Cohn et al (2005).Supersense tagging A WordNet-based super-sense tagger (Ciaramita and Altun, 2006).Semantic role labeling ASSERT version 1.4(Pradhan et al, 2004).3 FeaturesThe disambiguation features used by our system canbe divided into three categories: collocation fea-tures, syntactic features and semantic-role based fea-tures.
We discuss each in turn below.3.1 Collocation FeaturesThe collocation features were inspired by theone-sense-per-collocation heuristic proposed byYarowsky (1995).
These features were designed tocapture open class words that exhibit strong colloca-tion properties with respect to the different senses ofthe target preposition.
Details of the features in thiscategory are listed below.1This chunker is not exactly the same as Ngai and Florian?ssystem, however it does use the default transformation tem-plates supplied by fnTBL.241Bag of open class words The part-of-speech(POS) tags and lemmas of all the open class wordsthat occur in the same sentence as the target prepo-sition.Bag of WordNet synsets The WordNet (Miller,1993) synonym sets and their hypernyms of all theopen class words that occur in the same sentence asthe target preposition.Bag of named entities Each named entity in thesame sentence as the target preposition is treated asa separate feature.Surrounding words These features are the com-binations of the lemma, POS tag and relative posi-tion of the words surrounding the target prepositionwithin a window of 7 words.Surrounding super senses These features are thecombinations of super-sense tag, POS tag and rel-ative position of the words surrounding the targetpreposition within a window of 7 words.3.2 Syntactic FeaturesThe syntactic features were designed to capture boththe flat and recursive syntactic properties of the tar-get preposition.
The flat syntactic features were de-rived from the surrounding POS tags and chunk tagsof the target preposition; the recursive syntactic fea-tures were derived from the parse trees.
The detailsof these feature are given below.Surrounding POS tags These features are thecombination of POS tag and relative position of thewords surrounding the target preposition within awindow of 7 words.Surrounding chunk tags These features are thecombination of IOB style chunk tag and relative po-sition of the words surrounding the target preposi-tion within a window of 5 words.Surrounding chunk types Instead of using onlythe chunk tags themselves, we also extracted the ac-tual chunk types (NP, VP, ADJP, etc) of the wordssurrounding the target preposition within a windowof 5 words.
Each chunk type is also combined withits relative position to the target preposition as a sep-arate feature.SINP VPliveinPPMelbourneS_NP S_VPlive VP_PPin PP_NPMelbourneISNPFigure 1: Parse tree examplesParse tree features Given the position of the tar-get preposition p in the parse tree, the basic form ofthe corresponding parse tree feature is just the list ofnodes of p?s siblings in the tree (the POS tags aretreated as part of the terminal).
For example, sup-pose the original parse tree for the sentence I live inMelbourne is the left tree in Figure 1, for the targetpreposition in, the basic form of the parse tree fea-ture would be (1, NP).
In order to gain more syn-tactic information, we further annotated each non-terminal of the parse tree with its parent node, andused the new non-terminals as our features.
Theright tree in Figure 1 shows the result of applyingthis annotation once to the original parse tree.
Twolevels of additional annotation were performed onthe original parse trees in our feature extraction.3.3 Semantic-Role Based FeaturesFinally, since prepositional phrases can often func-tion as the temporal, location, and manner modifiersfor verbs, we designed semantic-role-based featuresto specifically capture this type of verb-prepositionsemantic information.
The details of these featuresare as follows:Surrounding semantic role tags The semanticrole tags of the words surrounding the target preposi-tion within a window of 5 words are combined withtheir relative positions to the target preposition andtreated as separate features.
For example, considerthe preposition on in the sentence The man whostole my car on Sunday has apologised to me, thesemantic roles for the two verbs (stole and apolo-gised ) are shown in Table 1.
The semantic roles forstole would generate the following features: (-5, I-A0), (-4, R-A0), (-3, TARGET), (-2, B-A1), (-1,I-A1), (0, B-AM-TMP), (1, I-AM-TMP), (2, O), (3,O), (4, O and (5, O).Attached verbs This feature was designed tocapture the verb-particle and verb-preposition-242The man who stole my car on Sunday has apologised to mestole B-A0 I-A0 R-A0 TARGET B-A1 I-A1 B-AM-TMP I-AM-TMP O O O Oapologised B-A0 I-A0 I-A0 I-A0 I-A0 I-A0 I-A0 I-A0 O TARGET B-A2 I-A2Table 1: Example semantic-role-labelled sentenceattachment relationships between verbs and prepo-sitions.
There are two situations in which a preposi-tion p is deemed to be attached to a verb v: (1) p hasa semantic role tag relative to v and this tag is a ?B?tag, (2) p has no semantic role tag relative to v, butthe first token to the right of p has a ?B?
tag relativeto v. In the sentence shown in Table 1, stole wouldbe considered as the governor of on.Verb?s relative position The lemma of each verbin the same sentence as the target preposition is com-bined with its relative position to the target preposi-tion and treated as a separate feature.
For example,the sentence shown in Table 1 would generate thetwo features: (-1, steal) and (1, apologize).More detailed descriptions and examples for thesefeatures may be found in Ye and Baldwin (2006b).4 Parameter TuningWe used the ranking-based feature selection methodfrom Ye and Baldwin (2006b) to select the most rele-vant feature based on our training data.
This methodworks in two steps.
Firstly, we calculated the infor-mation gain, gain ratio and Chi-squared statistics foreach feature, and used these values to generate 3 setsof rankings for the features.
We then summed up theindividual ranks, and used the sums to create a set offinal rankings for the features.The feature selection process is based on 10-foldcross validation: we divided our training data into10 pairs of training-test datasets; then for each fold,we extracted the top N% ranked features using ourfeature selection heuristic from the cv-training set(where N was set to values 5, 10, .., 100), and usedthese features to test the held-out test set.
The bestN as determined by the cross validation was thenapplied to the entire training data set.Additionally, since we used a maximum entropy-based machine learning package,2 it was importantto determine the best Gaussian smoothing parameterg for the probability distribution.
The tuning of g2http://homepages.inf.ed.ac.uk/s0450736/maxent_toolkit.htmlwas incorporated into the cross validation process offeature selection.Given the possible combinations of parametertuning, we trained the following three classifiers forthe preposition sense disambiguation task:Non-tuned Using all the original features and10.0 for the Gaussian smoothing parameter.Smoothing-tuned Using all the original featuresbut automatically tuned Gaussian smoothing param-eter.Fully-tuned Using both automatically tuned fea-tures and Gaussian smoothing parameter.5 Results and AnalysisThe overall precision (%) obtained by the three clas-sifiers for the fine-grained senses are as follows:Non-tuned Smoothing-tuned Fully tuned67.9 68.0 69.3The best overall results were achieved when boththe features and the Gaussian smoothing parameterswere automatically tuned, achieving a 1.4% absoluteprecision gain over the non-tuned system.
However,such parameter tuning may not always be useful: thesame tuning process was found to be detrimental in aSenseval-2 verb sense disambiguation task (Ye andBaldwin, 2006b).
Consistent with the findings ofYe and Baldwin (2006b), the improvement causedby the tuning of the Gaussian smoothing parame-ter is only marginal compared with the improvementcaused by the tuning of the features.We also evaluated our features based on their cate-gories and types.
Collocation features performed thebest among the three feature categories.
Without anyparameter tuning, the collocation-feature-only clas-sifier achieved an overall precision of 67.4% on thetest set; the semantic-role-feature-only classifier andthe syntactic-feature-only classifier achieved preci-sion of 46.9% and 50.5% respectively.The best-performing individual features are thebag-of-words features and bag-of-synsets features.243Feature type % in OverallFeature type top N% features % of the10 20 30 feature typeBag of Words 13.46 13.43 12.94 13.37Bag of Synsets 57.83 58.38 59.53 58.29Verb?s rel.
positions 3.97 3.95 3.76 4.02Surrounding POS tags 1.36 1.33 1.43 1.27Table 2: Percentages of top-performing featuretypes in the top N% ranked featuresOn the test set, the bag-of-words-only classifier andthe bag-of-synsets-only classifier achieved overallprecision of 63.2% and 61.9% respectively.We also analysed the top ranking features as cal-culated by our feature selection algorithm, as pre-sented in Table 2.
The results show the percentagesof the top-performing feature types of each featurecategory in the top N% ranked features.
It can beobserved that none of the top-performing featuresseem to have a significantly disproportional repre-sentation in the top-ranked features.
This indicatesthat the disambiguation power of a particular typeof features is determined mostly by the number offeatures of that type.On the other hand, the bag-of-words features ap-pear to be the most effective, considering that theyaccount for only 13.4% of the total features, butout-performed the bag-of-synsets features which ac-count for nearly 60% of the total features.It is also disappointing to see that the syntacticand semantic-role based features had little positiveinfluence in the disambiguation process.
However,this is perhaps caused by the sparseness of these fea-tures since they together only account for less than10% of all the extracted features.The overall finding from all this is that, similarto nouns and verbs, preposition sense is determinedprimarily by word context, and that syntactic and se-mantic role-based features play only a minor role.6 ConclusionsIn this paper, we have described a maximum entropybased preposition sense disambiguation system thatuses a rich set of features.
We have shown thatthis system performed well above the majority classbaseline of 39.6% precision.
Our analysis showedthat the most important disambiguation features arecollocation-based features.
This indicates that thesemantics of prepositions can be learnt mostly fromtheir surrounding context, and not syntactic proper-ties or verb-preposition semantics.AcknowledgementsThe research in this paper has been supported by the Aus-tralian Research Council through Discovery Project grant num-ber DP0663879.ReferencesEugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminative reranking.
In Pro-ceedings of the 43rd Annual Meeting of the Association forComputational Linguistics (ACL?05), pages 173?180, AnnArbor, USA.Massimiliano Ciaramita and Yasemin Altun.
2006.
Broad-coverage sense disambiguation and information extractionwith a supersense sequence tagger.
In Proceedings of the2006 Conference on Empirical Methods in Natural Lan-guage Processing, pages 594?602, Sydney, Australia.Trevor Cohn, Andrew Smith, and Miles Osborne.
2005.
Scal-ing conditional random fields using error-correcting codes.In Proceedings of the 43rd Annual Meeting of the Associa-tion for Computational Linguistics (ACL?05), pages 10?17,Ann Arbor, USA.Jacques Durand.
1993.
On the translation of prepositions inmultilingual MT.
In Frank Van Eynde, editor, Linguistic Is-sues in Machine Translation, pages 138?159.
Pinter Publish-ers, London, UK.Jesu?s Gime?nez and Llu?
?s Ma`rquez.
2004.
Svmtool: A gen-eral pos tagger generator based on support vector machines.In Proceedings of the 4th International Conference on Lan-guage Resources and Evaluation, pages 43?46, Lisbon, Por-tugal.George A. Miller.
1993.
Wordnet: a lexical database for en-glish.
In HLT ?93: Proceedings of the workshop on HumanLanguage Technology, pages 409?409, Princeton, USA.Grace Ngai and Radu Florian.
2001.
Transformation-basedlearning in the fast lane.
In Proc.
of the 2nd Annual Meetingof the North American Chapter of Association for Compu-tational Linguistics (NAACL2001), pages 40?7, Pittsburgh,USA.Tom O?Hara and Janyce Wiebe.
2003.
Preposition semanticclassification via Treebank and FrameNet.
In Proc.
of the 7thConference on Natural Language Learning (CoNLL-2003),pages 79?86, Edmonton, Canada.Sameer Pradhan, Kadri Hacioglu, Valerie Krugler, WayneWard, James H. Martin, and Daniel Jurafsky.
2004.
Supportvector learning for semantic argument classification.
Ma-chine Learning, 60(1?3):11?39.David Yarowsky.
1995.
Unsupervised word sense disambigua-tion rivaling supervised methods.
In Meeting of the Associ-ation for Computational Linguistics, pages 189?196, Cam-bridge, USA.Patrick Ye and Timothy Baldwin.
2006a.
Semantic role label-ing of prepositional phrases.
ACM Transactions on AsianLanguage Information Processing (TALIP), 5(3):228?244.Patrick Ye and Timothy Baldwin.
2006b.
Verb sense dis-ambiguation using selectional preferences extracted with astate-of-the-art semantic role labeler.
In Proceedings of theAustralasian Language Technology Workshop, pages 141?148, Sydney, Australia.244
