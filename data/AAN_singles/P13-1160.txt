Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1630?1639,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsA Bayesian Model for Joint Unsupervised Inductionof Sentiment, Aspect and Discourse RepresentationsAngeliki LazaridouUniversity of Trentoangeliki.lazaridou@unitn.itIvan TitovSaarland Universitytitov@mmci.uni-saarland.deCaroline SporlederTrier Universitycsporled@coli.uni-sb.deAbstractWe propose a joint model for unsuper-vised induction of sentiment, aspect anddiscourse information and show that by in-corporating a notion of latent discourse re-lations in the model, we improve the pre-diction accuracy for aspect and sentimentpolarity on the sub-sentential level.
Wedeviate from the traditional view of dis-course, as we induce types of discourse re-lations and associated discourse cues rel-evant to the considered opinion analysistask; consequently, the induced discourserelations play the role of opinion and as-pect shifters.
The quantitative analysis thatwe conducted indicated that the integra-tion of a discourse model increased theprediction accuracy results with respect tothe discourse-agnostic approach and thequalitative analysis suggests that the in-duced representations encode a meaning-ful discourse structure.1 IntroductionWith the rapid growth of the Web, it is becomingincreasingly difficult to discern useful from irrel-evant information, particularly in user-generatedcontent, such as product reviews.
To make it easierfor the reader to separate the wheat from the chaff,it is necessary to structure the available informa-tion.
In the review domain, this is done in aspect-based sentiment analysis which aims at identify-ing text fragments in which opinions are expressedabout ratable aspects of products, such as ?roomquality?
or ?service quality?.
Such fine-grainedanalysis can serve as the first step in aspect-basedsentiment summarization (Hu and Liu, 2004), atask with many practical applications.Aspect-based summarization is an active re-search area for which various techniques havebeen developed, both statistical (Mei et al, 2007;Titov and McDonald, 2008b) and not (Hu and Liu,2004), and relying on different types of supervi-sion sources, such as sentiment-annotated texts orpolarity lexica (Turney and Littman, 2002).
Mostmethods rely on local information (bag-of-words,short ngrams or elementary syntactic fragments)and do not attempt to account for more complexinteractions.
However, these local lexical repre-sentations by themselves are often not sufficient toinfer a sentiment or aspect for a fragment of text.For instance, in the following example taken froma TripAdvisor1 review:Example 1.
The room was nice but let?s not talkabout the view.it is difficult to deduce on the basis of local lexicalfeatures alone that the opinion about the view isnegative.
The clause let?s not talk about the viewcould by itself be neutral or even positive given theright context (e.g., I?ve never seen such a fancy ho-tel room, my living room doesn?t look that cool...and let?s not talk about the view).
However, thecontrast relation signaled by the connective butmakes it clear that the second clause has a nega-tive polarity.
The same observations can be madeabout transitions between aspects: changes in as-pect are often clearly marked by discourse connec-tives.
Importantly, some of these cues are not dis-course connectives in the strict linguistic sense andare specific to the review domain (e.g., the phraseI would also in a review indicates that the topicis likely to be changed).
In order to accuratelypredict sentiment and topic,2 a model needs to ac-1http://www.tripadvisor.com/2In what follows, we use the terms aspect and topic, inter-1630count for these discourse phenomena and cannotrely solely on local lexical information.These issues have not gone unnoticed to the re-search community.
Consequently, there has re-cently been an increased interest in models thatleverage content and discourse structure in senti-ment analysis tasks.
However, discourse-level in-formation is typically incorporated in a pipelinearchitecture, either in the form of sentiment po-larity shifters (Polanyi and Zaenen, 2006; Naka-gawa et al, 2010) that operate on the lexical levelor by using discourse relations (Taboada et al,2008; Zhou et al, 2011) that comply with dis-course theories like Rhetorical Structure Theory(RST) (Mann and Thompson, 1988).
Such ap-proaches have a number of disadvantages.
First,they require additional resources, such as lists ofpolarity shifters or discourse connectives whichsignal specific relations.
These resources are avail-able only for a handful of languages.
Second, re-lying on a generic discourse analysis step that iscarried out before sentiment analysis may intro-duce additional noise and lead to error propaga-tion.
Furthermore, these techniques will not nec-essarily be able to induce discourse relations in-formative for the sentiment analysis domain (Volland Taboada, 2007).An alternative approach is to define a task-specific scheme of discourse relations (Somasun-daran et al, 2009).
This previous work showedthat task-specific discourse relations are helpful inpredicting sentiment, however, in doing so they re-lied on gold-standard discourse annotation at testtime rather than predicting it automatically or in-ducing it jointly with sentiment polarity.We take a different approach and induce dis-course and sentiment information jointly in an un-supervised (or weakly supervised) manner.
Thishas the advantage of not having to pre-specify amapping from discourse cues to discourse rela-tions; our model induces this automatically, whichmakes it portable to new domains and languages.Joint induction of discourse and sentiment struc-ture also has the added benefit that the model isable to learn exactly those aspects of discoursestructure that are relevant for sentiment analysis.We start with a relatively standard joint modelof sentiment and topic, which can be regarded as across-breed between the JST model (Lin and He,2009) and the ASUM model (Jo and Oh, 2011),changeably as well as sentiment levels and opinion polarity.both state-of-the-art techniques.
This model isweakly supervised, as it relies solely on document-level (i.e.
not aspect-specific) opinion polarity la-bels to induce topics and sentiment on the sub-sentential level.
In order to test our hypothesisthat discourse information is beneficial, we adda discourse modeling component.
Note that inmodeling discourse we do not exploit any kindof supervision.
We demonstrate that the resultingmodel outperforms the baseline on a product re-view dataset (see Section 5).To the best of our knowledge, unsupervisedjoint induction of discourse structure, sentimentand topic information has not been consideredbefore, particularly not in the context of theaspect-based sentiment analysis task.
Importantly,our method for discourse modeling is a generalmethod which can be integrated in virtually anyLDA-style model of aspect and sentiment.2 Modeling Discourse StructureDiscourse cues typically do not directly indicatesentiment polarity (or aspect).
However, they canindicate how polarity (or aspect) changes as thetext unfolds.
As we have seen in the examplesabove, changes in polarity can happen on a sub-sentential level, i.e., between adjacent clauses or,from a discourse-theoretic point of view, betweenadjacent elementary discourse units (EDUs).
Tomodel these changes we need a strong linguisticsignal, for example, in the form of discourse con-nectives or other discourse cues.
We hypothesizethat these are more likely to occur at the beginningof an EDU than in the middle or at the end.
This iscertainly true for most of the traditional discourserelation cues (particularly connectives).Changes in polarity or aspect are often cor-related with specific discourse relations, such as?contrast?.
However, not all relations are rele-vant and there is no one-to-one correspondencebetween relations and sentiment changes.3 Fur-thermore, if a discourse relation signals a change,it is typically ambiguous whether this change oc-curs with the polarity (example 1) or the aspect(the room was nice but the breakfast was even bet-ter) or both (the room was nice but the breakfastwas awful).
Therefore, we do not explicitly model3The ?explanation?
relation, for example, can occur witha polarity change (We were upgraded to a really nice roombecause the hotel made a terrible blunder with our booking)but does not have to (The room was really nice because thehotel was newly renovated).1631Name DescriptionAltSame different polarity, same aspectSameAlt same polarity, different aspectAltAlt different polarity and aspectTable 1: Discourse relationsgeneric discourse relations; instead, inspired bythe work of Somasundaran et al (2008), we definethree very general relations which encode how po-larity and aspect change (Table 1).
Note that wedo not have a discourse relation SameSame sincewe do not expect to have strong linguistic evidencewhich states that an EDU contains the same senti-ment information as the previous one.4 However,we assume that the sentiment and topic flow isfairly smooth in general.
In other words, for twoadjacent EDUs not connected by any of the abovethree relations, the prior probability of staying atthe same topic and sentiment level is higher thanpicking a new topic and sentiment level (i.e.
weuse ?sticky states?
(Fox et al, 2008)).3 ModelIn this section we describe our Bayesian model,first the discourse-agnostic model and then an ex-tension needed to encode discourse information.The formal generative story is presented in Fig-ure 1: the red fragments correspond to the dis-course modeling component.
In order to obtain thegenerative story for the discourse-agnostic model,they simply need to be ignored.3.1 Discourse-agnostic modelIn our approach we make an assumption that allthe words in an EDU correspond to the same topicand sentiment level.
We also assume that an over-all sentiment of the document is defined, this isthe only supervision we use in inducing the model.Unlike some of the previous work (e.g., (Titov andMcDonald, 2008a)), we do not constrain aspect-specific sentiment to be the same across the docu-ment.
We describe our discourse-agnostic modelby first describing the set of corpus-level anddocument-level parameters, and then explain howthe content of each document is generated.Drawing model parameters On the corpuslevel, for every topic z ?
{1, .
.
.
,K} and ev-ery sentiment polarity level y ?
{?1, 0,+1},we start by drawing a unigram language model4The typical connective in this situation would be andwhich is highly ambiguous and can signal several traditionaldiscourse relations.from a Dirichlet prior.
For example, the languagemodel of the aspect service may indicate that theword friendly is used to express a positive opinion,whereas the word rude expresses a negative one.Similarly, for every topic z and every over-all sentiment polarity y?, we draw a distribution?y?,z over opinion polarity in this topic z. Intu-itively, one would expect the sentiment of an as-pect to more often agree with the overall sentimenty?
than not.
This intuition is encoded in an asym-metric Dirichlet prior Dir(?
y?)
for ?y?,z : ?
y?
=(?y?,1, .
.
.
, ?y?,M ), ?y?,y = ?
+ ?
?y,y?, where ?y,y?
isthe Kronecker symbol, ?
and ?
are nonnegativescalar parameters.
Using these ?heavy-diagonal?priors is crucial, as this is the way to ensure thatthe overall sentiment level is tied to the aspect-specific sentiment level.
Otherwise, sentiment lev-els will be specific to individual aspects (e.g., the?+1?
sentiment for one topic may correspond toa ?-1?
sentiment for another one).
Without thisproperty we would not be able to encode soft con-straints imposed by the discourse relations.Drawing documents On the document level, asin the standard LDA model, we choose the distri-bution over topics for the document from a sym-metric Dirichlet prior parametrized by ?, which isused to control sparsity of topic assignments.
Fur-thermore, we draw the global sentiment y?d from auniform distribution.The generation of a document is done on theEDU-by-EDU basis.
In this work, we assumethat EDU segmentation is provided by the prepro-cessing step.
First, we generate the aspect zd,sfor EDU s according to the distribution of top-ics ?d.
Then, we choose a sentiment level yd,sfor the considered EDU from the categorical dis-tribution ?y?d,zd,s , conditioned on the aspect zd,s,as well as on the global sentiment of the documenty?d.
Finally, we generate the bag of words for theEDU by drawing the words from the aspect- andsentiment-specific language model.This model can be seen as a variant of a state-of-the-art model for jointly inducing sentiment andaspect at the sentence level (Jo and Oh, 2011), or,more precisely, as its combination with the JSTmodel (Lin and He, 2009), adapted to the specificsof our setting.
Both these models have been shownto perform well on sentiment and topic predictiontasks, outperforming earlier models, such as theTSM model (Mei et al, 2007).
Consequently, itcan be considered as a strong baseline.16323.2 Discourse-informed modelIn order to integrate discourse information into thediscourse-agnostic model, we need to define a setof extra parameters and random variables.Drawing model parameters First, at the corpuslevel, we draw a distribution ??
over four discourserelations: three relations as defined in Table 1 andan additional dummy relation 4 to indicate thatthere is no relation between two adjacent EDUs(NoRelation).
This distribution is drawn from anasymmetric Dirichlet prior parametrized by a vec-tor of hyperparameters ?.
These parameters en-code the intuition that most pairs of EDUs do notexhibit a discourse relation relevant for the task(i.e.
favor NoRelation), that is ?4 has a distinctand larger value than other parameters ?4?.Every discourse relation c (includingNoRelation which is treated here as Same-Same) is associated with two groups of transitiondistributions, one governing transitions of sen-timent (?
?c) and another one controlling topictransitions (??c).
The parameter ?
?c,ys , defines adistribution over sentiment polarity for the EDUs+ 1 given the sentiment for the sth EDU ys andthe discourse relation c. This distribution encodesour beliefs about sentiment transitions betweenEDUs s and s+ 1 related through c. For example,the distribution ?
?SameAlt,+1 would assign higherprobability mass to the positive sentiment polarity(+1) than to the other 2 sentiment levels (0,-1).
Similarly, the parameter ?
?c,zs , defines adistribution over K aspects.These two families of transition distributionsare each defined in the following way.
For the dis-tribution ?
?, for relations that favor changing theaspect (SameAlt and AltAlt), the probability of thepreferred (K-1) transitions is proportional to ?
?and for the remaining transitions it is proportionalto 1.
On the other hand, for the relations that fa-vor keeping the same aspect (NoRelation and Alt-Same), the probability of the preferred transition isproportional to ??
?, whereas the probability of the(K-1) remaining transitions is again proportionalto 1.
For the sentiment transitions, the distribution?
?c,ys is defined in the analogous way (but dependson ??
and ???).
These scalars are hand-coded anddefine soft constraints that discourse relations im-pose on the local flow of sentiment and aspects.The parameter ?
?c is a language model over dis-course cues w?, which are not restricted to uni-grams but can generate phrases of arbitrary (andvariable) size.
For this reason, we draw themfrom a Dirichlet process (DP) (i.e.
one for eachdiscourse relation, except for NoRelation).
Thebase measure G0 provides the probability of an n-word sequence calculated with the bigram prob-ability model estimated from the corpus.5 Thismodel component bears strong similarities to theBayesian model of word segmentation (Goldwa-ter et al, 2009), though we use the DP processto generate only the prefix of the EDU, whereasthe rest of the EDU is generated from the bag-of-words model.Drawing documents As pointed out above, thecontent generation is broken into two steps, wherefirst we draw the discourse cue w?d,s from ?
?c andthen we generate the remaining words.The second difference at the data generationstep (Figure 1) is in the way the aspect and sen-timent labels are drawn.
As the discourse rela-tion between the EDUs has already been chosen,we have some expectations about the values of thesentiment and aspect of the following EDU, whichare encoded by the distributions ??
and ??.
Theseare only soft constraints that have to be taken intoconsideration along with the information providedby the aspect-sentiment model.
This coupling ofinformation naturally translates into the product-of-experts (PoE) (Hinton, 1999) approach, wheretwo sources of information jointly contribute tothe final result.
The PoE model seems to be moreappropriate here than a mixture model, as we donot want the discourse transition to overpower thesentiment-topic model.
In the PoE model, in or-der for an outcome to be chosen, it needs to havea non-negligible probability under both models.4 InferenceSince exact inference of our model is intractable,we use collapsed Gibbs sampling.
The variablesthat need to be inferred are the topic assignmentsz, the sentiment assignments y, the discourse re-lations c and the discourse cue w?
(or, more pre-cisely, its length) and are all sampled jointly (foreach EDU) since we expect them to be highly de-pendent.
All other variables (i.e.
unknown dis-tributions) could be marginalized out to obtain acollapsed Gibbs sampler (Griffiths and Steyvers,2004).5This measure is improper but it serves the purpose offavoring long cues, the behavior arguably desirable for ourapplication.1633Global parameters:??
?
Dir(?)
[distrib of disc rel]for each discourse relation c = 1, .., 4:?
?c ?
DP(?,Go) [distrib of disc rel specific disc cues]?
?c,k - fixed [distrib of rel specific aspect transitions]?
?c,y - fixed [distrib of rel specific sent transitions]for each aspect k = 1, 2...K:for each sentiment y = ?1, 0,+1:?k,y ?
Dir(?k) [unigram language models]for each global sentiment y?
= ?1, 0,+1:?y?,k ?
Dir(?)
[sent distrib given overall sentiment]Data Generation:for each document d:y?d ?
Unif(?1, 0,+1) [global sentiment]?d ?
Dir(?)
[distr over aspects]for every EDU s:cd,s ?
??
[draw disc relation]if cd,s 6= NoRelationw?d,s ?
?
?cd,s [draw disc cue]zd,s ?
?d ?
?
?cd,s, zd,s?1 [draw aspect]yd,s ?
?y?d,zd,s?
?
?cd,s,yd,s?1 [draw sentiment level]for each word after disc cue:wd,s ?
?zd,s,yd,s [draw words]Figure 1: The generative story for the joint model.The components responsible for modeling dis-course information are emphasized in red: whendropped, one is left with the discourse-agnosticmodel.Unfortunately, the use of the PoE model pre-vents us from marginalizing the parameters ex-actly.
Instead, as in Naseem et al (2009), we re-sort to an approximation.
We assume that zd,s andyd,s are drawn twice; once from the document spe-cific distribution and once from the discourse tran-sition distributions.
Under this simplification, wecan easily derive the conditional probabilities forthe collapsed Gibbs sampling.5 ExperimentsTo the best of our knowledge, this is the first workthat aims at evaluating directly the joint informa-tion of the sentiment and aspect assignment at thesub-sentential level of full reviews; most existingstudies either focus on indirect evaluation of theproduced models (e.g., classifying the overall sen-timent of sentences (Titov and McDonald, 2008a;Brody and Elhadad, 2010) or even reviews (Naka-gawa et al, 2010; Jo and Oh, 2011)) or evaluatedsolely at the sentential or even document level.Consequently, in order to evaluate our methods,we created a new dataset which will be publiclyreleased.Aspects Frequencyservice 246value 55location 121rooms 316sleep quality 56cleanliness 59amenities 180food 81recommendation 121rest 306Total 1541Table 2: Distribution of aspects in the data.Dataset and Annotation The dataset we createdconsists of 13559 hotel reviews from TripAdvi-sor.com.6 Since our modeling is performed on theEDU level, all sentences where segmented usingthe SLSEG software package.7 As a result, ourdataset consists of 322,935 EDUs.For creating the gold standard, 9 annotators an-notated a random subset of our dataset (65 re-views, 1541 EDUs).
The annotators were pre-sented with the whole review partitioned in EDUsand were asked to annotate every EDU with theaspect and sentiment (i.e.
+1, 0 or ?1) it ex-presses.
Table 2 presents the distribution of as-pects in the dataset.
The distribution of the sen-timents is uniform.
The label rest captures caseswhere EDUs do not refer to any aspect or to a veryrare aspect.
The inter-annotator agreement (IAA),as measured in terms of Cohen?s kappa score, was66% for the aspect labeling, 70% for the sentimentannotation and 61% for the joint task of sentimentand aspect annotation.
Though these scores maynot seem very high, they are similar to the ones re-ported in related sentiment annotation efforts (seee.g., Ganu et al (2009)).Experimental setup In order to quantitativelyevaluate the model predictions, we run two sets ofexperiments.
In the first, we treat the task as an un-supervised classification problem and evaluate theoutput of the models directly against the gold stan-dard annotation.
This is a very challenging set-up,as the model has no prior information about theaspects defined (Table 2).
In the second set ofexperiments, we show that aspects and sentimentsinduced by our model can be used to construct in-formative features for supervised classification.
In6Downloadable from http://clic.cimec.unitn.it/?angeliki.lazaridou/datasets/ACL2013Sentiment.tar.gz7www.sfu.ca/?mtaboada/research/SLSeg.html1634Model Precision Recall F1Random 3.9 3.8 3.8SentAsp 15.0 10.2 9.2Discourse 16.5 13.8 10.8Table 3: Results in terms of macro-averaged pre-cision, recall and F1.Model Unmarked MarkedSentAsp 9.2 5.4Discourse 9.3 11.5Table 4: Separate evaluation (F1) of the ?marked?and the ?unmarked?
EDUs.all the cases, we compare the discourse-agnosticand the discourse-informed models.In order to induce the model, we let the samplerrun for 2000 iterations.
We use the last sample todefine the labeling.
The number of topics K wasset to 10 in order to match the number of aspectsdefined in our annotation scheme (see Table 2).The hyperpriors were chosen in a qualitative ex-periment over a subset of our dataset by manuallyinspecting the produced languages models.
Theresulting values are: ?
= 10?3, ?
= 5 ?
10?4,?
= 5 ?
10?4, ?
= 10?3, ?4 = 103, ?4?
= 10?4,??
= 85 and ???
= ??
= ???
= 5.5.1 Direct clustering evaluationOur labels encoding aspect and sentiment level canbe regarded as clusters.
Consequently we can ap-ply techniques developed in the context of cluster-ing evaluation.
We use a version of the standardmetrics considered for the word sense inductiontask (Agirre and Soroa, 2007) where a clusteringis converted to a classification problem.
This isachieved by splitting the gold standard into twosubsets; the training portion is used to choose one-to-one correspondence from the gold classes to theinduced clusters and then the chosen mapping isapplied to the testing portion.
We perform 10-foldcross validation and report precision, recall and F1score.
Our dataset is very skewed and the majorityclass (rest) is arguably the least important, so weuse macro-averaging over labels and then averagethose across folds to arrive to the reported num-bers.
We compare the discourse-informed model(Discourse) against two baselines; the discourse-agnostic SentAsp model and Random which as-signs a random label to an EDU while respectingthe distribution of labels in the training set.Table 3 presents the first analysis conducted onthe full set of EDUs.
We observe that by incor-porating latent discourse relation we improve per-Content Aspect Polarity1 but certainly off its greatness value neg2 and while small they are nice rooms pos3 but it is not free for all guests amenities neg4 and the water was brown clean neg5 and no tea making facilities rooms neg6 when i checked out service pos7 and if you do not service neg8 when we got home clean neuTable 5: Examples of EDUs where local informa-tion is not sufficiently informative.formance over the discourse-agnostic model Sen-tAsp (statistically significant according to paired t-test with p < 0.01).
Note that fairly low scores inthis evaluation setting are expected for any unsu-pervised model of sentiment and topics, as modelsare unsupervised both in the aspect-specific senti-ment and in topic labels and the total number oflabels is 28 (all aspects can be associated with the3 sentiment levels except for rest which can onlybe used with neutral (0) sentiment).
Consequently,induced topics, though informative (as we confirmin Section 5.3), may not correspond to the topicsdefined in the gold standard.
For example, onewell-known property of LDA-style topic modelsis their tendency to induce topics which accountfor similar fraction of words in the dataset (Jagar-lamudi et al, 2012), thus, over-splitting ?heavy?topics (e.g.
rooms in our case).
The same, thoughto lesser degree, is true for sentiment levels wherethe border between neutral and positive (or nega-tive) is also vaguely defined.To gain insight into our model, we conductedan experiment similar to the one presented in So-masundaran et al (2009).
We divide the dataset intwo subsets; one containing all EDUs starting witha discourse cue (?marked?)
and one containing theremaining EDUs (?unmarked?).
We hypothesizethat the effect of the discourse-aware model shouldbe stronger on the first subset, since the presenceof the connective indicates the possibility of a dis-course relation with the previous EDU.
The set ofdiscourse connectives is taken from the Penn Dis-course Treebank (Prasad et al, 2008), thus creat-ing a list of 240 potential connectives.Table 5 presents a subset of ?marked?
EDUs forwhich trying to assign the sentiment and aspectout of context (i.e.
without the previous EDU) isa difficult task.
In examples 1-3 there is no ex-plicit mention of the aspect.
However, there isan anaphoric expression (marked in bold) which1635refers to a mention of the aspect in some previousEDU.
On the other hand, in examples 4 and 5 thereis an ambiguity in the choice of aspect; in example5, tea making facilities can refer to a breakfast atthe hotel (label food) or to facilities in the room(label rooms).
Finally, examples 6-8 are too shortand not informative at all which indicates that thesegmentation tool does not always predict a de-sirable segmentation.
Consequently, automatic in-duction of segmentation may be a better option.Table 4 presents quantitative results of this anal-ysis.
Although the performance over the ?un-marked?
example is the same for the two mod-els, this is not the case for the ?marked?
instanceswhere the discourse-informed model leverages thediscourse signal and achieves better performance.This behavior agrees with our initial hypothesis,and suggests that our discourse representation,though application-specific, relies in part on theinformation encoded in linguistically-defined dis-course cues.
We will confirm this intuition in thequalitative evaluation section.
The increase for the?marked?
EDUs does not translate into greater dif-ferences for the overall scores (Table 3) as markedrelations are considerably less frequent than un-marked ones in our gold standard (i.e.
35% of theEDUs are ?marked?).
Nevertheless, this clearlysuggests that the discourse-informed model is infact capable of exploiting discourse signal.5.2 Qualitative analysisTo investigate the quality of the induced discoursestructure, we present the most frequent discoursecues extracted for every discourse relation.
Ta-ble 6 presents a selection of cues that best explainthe discourse relation they have been associatedwith.
A general observation is that among the cuesthere are not only ?traditional?
discourse connec-tives like even though, although, and, but also cuesthat are discriminative for the specific application.In relation SameAlt we can mostly observephrases that tend to introduce a new aspect, sincean explicit mention of it is provided (e.g the loca-tion is, the room was) and more specific phraseslike in addition are used to introduce a new aspectwith the same sentiment.
However, these cues re-veal important information about the aspect of theEDU, and since they are associated with the lan-guage model ?
?, they are not visible anymore tothe language model of aspects ?.Cues for the relation AltSame also includeDiscourse Discourse CuesrelationSameAlt the location is , the room was, the hotelhas, and the room, and the bed, breakfastwas, the staff were, in addition, good luckAltSame but, and, it was, and it was, and they, al-though, and it, but it, but it was, however,which was, this is, this was, they were,the only thing, even though, unfortunately,needless to say, fortunatelyAltAlt the room was, the staff were, the only, thehotel is, but the, however, also, or, overalli, unfortunately, we will definitely, on theplus, the only downside , even though, andeven though, i would definatelyTable 6: Induced cues from the discourse relationsphrases that contain some anaphoric expressions,which might refer to previous mentions of an as-pect in the discourse (i.e.
previous EDU).
We ex-pect that since there is an anaphoric expression,explicit lexical features for the aspect will be miss-ing, making thus the decision concerning aspectassignment ambiguous for any discourse-agnosticmodel.
Interestingly, we found the expressions un-fortunately, fortunately, the only thing in the samerelation, since all indicate a change in sentiment.Finally, AltAlt can be viewed as a mixture of theother two relations.
Furthermore, for this relationwe can find expressions that tend to be used at theend of a review, since at this point we normallychange the aspect and often even sentiment.
Someexamples of these cases are overall, we will defi-nitely and even the misspelled version of the latteri would definately.5.3 Features in supervised learningAs an additional experiment to demonstrate infor-mative of the output of the two models, we de-sign a supervised learning task of predicting sen-timent and topic of EDUs.
In this setting, thefeature vector of every EDU consists of its bag-of-word-representation to which we add two extrafeatures; the models?
predictions of topic and sen-timent.
We train a support vector machine with apolynomial kernel using the default parameters ofWeka8 and perform 10-fold cross-validation.Table 7 presents results of this analysis in termsof accuracy for four classification tasks, i.e.
pre-dicting both sentiment and topic, only sentimentand only topic for all EDUs, as well as predict-ing sentiment and topic for the ?marked?
dataset.First, we observe that incorporation of the topic-8http://www.cs.waikato.ac.nz/ml/weka/1636Features aspect+sentiment aspect sentiment Marked only(28 classes) (10 classes) (3 classes) sentiment+aspect (28 classes)only unigrams 36.3 49.8 57.1 26.2unigrams + SentAsp 38.0 50.4 59.3 27.8unigrams + Discourse 39.1 52.4 59.4 29.1Table 7: Supervised learning at the EDU level (accuracy)model features on a unigram-only model resultsin an improvement in classification performanceacross all tasks (predicting sentiment, predictingaspects, or both); as a matter of fact, our accu-racy results for predicting sentiment are compa-rable to the sentence-level results presented byTa?ckstro?m and McDonald (2011).
We have tostress that accuracies for the joint task (i.e.
pre-dicting both sentiment and topic) are expected tobe lower since it can also be seen as the productof the two other tasks (i.e.
predicting only senti-ment and only topic).
We also observe that the fea-tures induced from the Discourse model result inhigher accuracy than the ones from the discourse-agnostic model SentAsp both in the complete setof EDUs and the ?marked?
subset, results that arein line with the ones presented in Table 4.
Fi-nally, the fact that the results for the complete setof EDUs are higher than the ones for the ?marked?dataset clearly suggests that the latter constitute ahard case for sentiment analysis, in which exploit-ing discourse signal proves to be beneficial.6 Related WorkRecently, there has been significant interest inleveraging content structure for a number of NLPtasks (Webber et al, 2011).
Sentiment analysishas not been an exception to this and discourse hasbeen used in order to enforce constraints on theassignment of polarity labels at several granular-ity levels, ranging from the lexical level (Polanyiand Zaenen, 2006) to the review level (Taboadaet al, 2011).
One way to deal with this prob-lem is to model the interactions by using a pre-compiled set of polarity shifters (Nakagawa et al,2010; Polanyi and Zaenen, 2006; Sadamitsu et al,2008).
Socher et al (2011) defined a recurrentneural network model, which, in essence, learnsthose polarity shifters relying on sentence-levelsentiment labels.
Though successful, this model isunlikely to capture intra-sentence non-local phe-nomena such as effect of discourse connectives,unless it is provided with syntactic informationas an input.
This may be problematic for thenoisy sentiment-analysis domain and especiallyfor poor-resource languages.
Similar to our work,others have focused on modeling interactions be-tween phrases and sentences.
However, this hasbeen achieved by either using a subset of relationsthat can be found in discourse theories (Zhou etal., 2011; Asher et al, 2008; Snyder and Barzi-lay, 2007) or by using directly (Taboada et al,2008) the output of discourse parsers (Soricut andMarcu, 2003).
Discourse cues as predictive fea-tures of topic boundaries have also been consid-ered in Eisenstein and Barzilay (2008).
This workwas extended by Trivedi and Eisenstein (2013),where discourse connectors are used as featuresfor modeling subjectivity transitions.Another related line of research was presentedin Somasundaran et al (2009) where a domain-specific discourse scheme is considered.
Simi-larly to our set-up, discourse relations enforce con-straints on sentiment polarity of associated sen-timent expressions.
Somasundaran et al (2009)show that gold-standard discourse information en-coded in this way provides a useful signal for pre-diction of sentiment, but they leave automatic dis-course relation prediction for future work.
Theyuse an integer linear programming framework toenforce agreement between classifiers and softconstraints provided by discourse annotations.This contrasts with our work; we do not rely onexpert discourse annotation, but rather induce bothdiscourse relations and cues jointly with aspectand sentiment.7 Conclusions and Future WorkIn this work, we showed that by jointly induc-ing discourse information in the form of discoursecues, we can achieve better predictions for aspect-specific sentiment polarity.
Our contribution con-sists in proposing a general way of how discourseinformation can be integrated in any LDA-stylediscourse-agnostic model of aspect and sentiment.In the future, we aim at modeling more flexiblesets of discourse relations and automatically in-ducing discourse segmentation relevant to the task.1637ReferencesEneko Agirre and Aitor Soroa.
2007.
Semeval-2007task 02: Evaluating word sense induction and dis-crimination systems.
In Proceedings of the Se-mEval, pages 7?12.Nicholas Asher, Farah Benamara, and Yvette YannickMathieu.
2008.
Distilling opinion in discourse: Apreliminary study.
Proceedings of Coling, pages 5?8.Samuel Brody and Noemie Elhadad.
2010.
An unsu-pervised aspect-sentiment model for online reviews.In Proceedings of NAACL, pages 804?812.Jacob Eisenstein and Regina Barzilay.
2008.
Bayesianunsupervised topic segmentation.
In Proceedings ofEMNLP, pages 334?343.Emily B Fox, Erik B Sudderth, Michael I Jordan, andAlan S Willsky.
2008.
An HDP-HMM for systemswith state persistence.
In Proceedings of ICML.Gayatree Ganu, Noemie Elhadad, and Amelie Marian.2009.
Beyond the stars: Improving rating predic-tions using review text content.
In Proceedings ofWebDB.Sharon Goldwater, Thomas L Griffiths, and Mark John-son.
2009.
A bayesian framework for word segmen-tation: Exploring the effects of context.
Cognition,112(1):21?54.Thomas L Griffiths and Mark Steyvers.
2004.
Find-ing scientific topics.
Proceedings of the NationalAcademy of Sciences of the United States of Amer-ica, 101(Suppl 1):5228?5235.Geoffrey E Hinton.
1999.
Products of experts.
In Pro-ceedings of ICANN, volume 1, pages 1?6.Minqing Hu and Bing Liu.
2004.
Mining and sum-marizing customer reviews.
In Proceedings of ACMSIGKDD, pages 168?177.Jagadeesh Jagarlamudi, Hal Daume?
III, and Raghaven-dra Udupa.
2012.
Incorporating lexical priors intotopic models.
Proceedings of EACL, pages 204?213.Yohan Jo and Alice H Oh.
2011.
Aspect and senti-ment unification model for online review analysis.In Proceedings of WSDM, pages 815?824.Chenghua Lin and Yulan He.
2009.
Joint senti-ment/topic model for sentiment analysis.
In Pro-ceeding of CIKM, pages 375?384.William C Mann and Sandra A Thompson.
1988.Rhetorical structure theory: Toward a functional the-ory of text organization.
Text, 8(3):243?281.Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su,and ChengXiang Zhai.
2007.
Topic sentiment mix-ture: modeling facets and opinions in weblogs.
InProceedings of WWW, pages 171?180.Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.2010.
Dependency tree-based sentiment classifica-tion using crfs with hidden variables.
In Proceed-ings of NAACL, pages 786?794.Tahira Naseem, Benjamin Snyder, Jacob Eisen-stein, and Regina Barzilay.
2009.
Multilin-gual part-of-speech tagging: Two unsupervised ap-proaches.
Journal of Artificial Intelligence Re-search, 36(1):341?385.Livia Polanyi and Annie Zaenen.
2006.
Contextualvalence shifters.
Computing attitude and affect intext: Theory and applications, pages 1?10.Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-sakaki, Livio Robaldo, Aravind Joshi, and BonnieWebber.
2008.
The penn discourse treebank 2.0.
InProceedings of LREC.Kugatsu Sadamitsu, Satoshi Sekine, and Mikio Ya-mamoto.
2008.
Sentiment analysis based on proba-bilistic models using inter-sentence information.
InProceedings of ACL, pages 2892?2896.Benjamin Snyder and Regina Barzilay.
2007.
Multipleaspect ranking using the good grief algorithm.
InProceedings of HLT-NAACL, pages 300?307.Richard Socher, Jeffrey Pennington, Eric H Huang,Andrew Y Ng, and Christopher D Manning.
2011.Semi-supervised recursive autoencoders for predict-ing sentiment distributions.
In Proceedings ofEMNLP, pages 151?161.Swapna Somasundaran, Janyce Wiebe, and Josef Rup-penhofer.
2008.
Discourse level opinion interpreta-tion.
In Proceedings of Coling, pages 801?808.Swapna Somasundaran, Galileo Namata, JanyceWiebe, and Lise Getoor.
2009.
Supervised andunsupervised methods in employing discourse rela-tions for improving opinion polarity classification.In Proceedings of EMNLP, pages 170?179.Radu Soricut and Daniel Marcu.
2003.
Sentence leveldiscourse parsing using syntactic and lexical infor-mation.
In Proceedings of NAACL, pages 149?156.Maite Taboada, Kimberly Voll, and Julian Brooke.2008.
Extracting sentiment as a function of dis-course structure and topicality.
Simon Fraser Uni-versity, Tech.
Rep, 20.Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-berly Voll, and Manfred Stede.
2011.
Lexicon-based methods for sentiment analysis.
Computa-tional Linguistics, 37(2):267?307.Oscar Ta?ckstro?m and Ryan McDonald.
2011.
Semi-supervised latent variable models for sentence-levelsentiment analysis.
In Proceedings of ACL, pages569?574.Ivan Titov and Ryan McDonald.
2008a.
A joint modelof text and aspect ratings for sentiment summariza-tion.
In Proceedings of ACL, pages 308?316.1638Ivan Titov and Ryan McDonald.
2008b.
Modeling on-line reviews with multi-grain topic models.
In Pro-ceedings of WWW, pages 112?120.Rakshit Trivedi and Jacob Eisenstein.
2013.
Discourseconnectors for latent subjectivity in sentiment anal-ysis.
In In Proceedings of NAACL.Peter D Turney and Michael L Littman.
2002.
Un-supervised learning of semantic orientation from ahundred-billion-word corpus.Kimberly Voll and Maite Taboada.
2007.
Not allwords are created equal: Extracting semantic orien-tation as a function of adjective relevance.
In Pro-ceedings of Australian Conf.
on AI.Bonnie Webber, Markus Egg, and Valia Kordoni.2011.
Discourse structure and language technology.Natural Language Engineering, 1(1):1?54.Lanjun Zhou, Binyang Li, Wei Gao, Zhongyu Wei,and Kam-Fai Wong.
2011.
Unsupervised discoveryof discourse relations for eliminating intra-sentencepolarity ambiguities.
In Proceedings EMNLP, pages162?171.1639
