Non-Contiguous Word Sequences for Information RetrievalAntoine Doucet and Helena Ahonen-MykaDepartment of Computer ScienceP.O.
Box 26 (Teollisuuskatu 23)FIN-00014 University of Helsinki,Finland,Antoine.Doucet@cs.helsinki.fi, Helena.Ahonen-Myka@cs.helsinki.fiAbstractThe growing amount of textual informationavailable electronically has increased the needfor high performance retrieval.
The use ofphrases was long seen as a natural way to im-prove retrieval performance over the commondocument models that ignore the sequential as-pect of word occurrences in documents, consid-ering them as ?bags of words?.
However, bothstatistical and syntactical phrases showed disap-pointing results for large document collections.In this paper we present a recent type ofmulti-word expressions in the form of Maxi-mal Frequent Sequences (Ahonen-Myka, 1999).Mined phrases rather than statistical or syntac-tical phrases, their main strengths are to forma very compact index and to account for thesequentiality and adjacency of meaningful wordco-occurrences, by allowing for a gap betweenwords.We introduce a method for using thesephrases in information retrieval and present ourexperiments.
They show a clear improvementover the well-known technique of extracting fre-quent word pairs.1 IntroductionThe constantly growing number of electronicdocuments increases the need for high perfor-mance retrieval, the precision of a system beingthe percentage of relevant documents among thetotal number of hits returned to a query.Most information retrieval systems do not ac-count for word order in a document.
However,we can assume that there must exist a way to ac-count for word order, which permits to improveretrieval performance.
Zhai et al (1997) men-tion many problems due the use of single wordterms only.
They observe that some word asso-ciations have a totally different meaning of the?sum?
of the meanings of the words that com-pose them (e.g., ?hot dog?
is usually not usedto refer to a warm dog !).
Other lexical unitspose similar problems (e.g., ?kick the bucket?
).Work on the use of phrases in IR has been car-ried out for more than 25 years.
Early resultswere very promising.
However, unexpectedly,the constant growth of test collections causeda drastic fall in the quality of the results.
In1975, Salton et al (1975) show an improve-ment in average precision over 10 recall pointsbetween 17% and 39%.
In 1989, Fagan (1989)reiterated the exact same experiments with a 10Mb collection and obtained improvements from11% to 20%.
This negative impact of the col-lection size was lately confirmed by Mitra et al(1987) over a 655 Mb collection, improving theaverage precison by only one percent !
Turpinand Moffat (1999) revisited and extended thiswork to obtain improvements between 4% and6%.A conclusion of this related work is thatphrases improve results in low levels of recall,but are globally inefficient for the n first rankeddocuments.
According to Mitra et al (1987),this low benefit from phrases to the best an-swers is explained by the fact that phrases pro-mote documents that deal with only one aspectof possibly multi-faceted queries.
For example,a topic of TREC-4 is about ?problems associ-ated with pension plans, such as fraud, skim-ming, tapping or raiding?.
Several top-rankeddocuments discuss pension plans, but no relatedproblem.
Mitra et al (1987) term this problemas one of inadequate query coverage.In our opinion, this does not contradict theidea that adding document descriptors account-ing for word order must permit to improve theperformance of IR systems.
But related workshows the need for another way to combinephrase and word term descriptors (Smeaton andKelledy, 1998) and even more the fact that thephrases currently used to model documents arenot well suited for that.In the next section, we will briefly describeSecond ACL Workshop on Multiword Expressions: Integrating Processing, July 2004, pp.
88-95the vector space model (sometimes quoted as?bag of words?, for it simply ignores words?
po-sitions).
We will then describe the differenttypes of phrases used in related work (section3).
In section 4, we define our own phrases(maximal frequent sequences) and explain howthey will be better document descriptors thanthose found in the state of the art.
In section 5,we present a technique to incorporate maximalfrequent sequences into document indexing andquery processing, so as to properly take advan-tage of this extra information in an informationretrieval framework.
In section 6, we presentour experiments and results, before we concludethe paper in section 7.2 Vector Space Model2.1 PreprocessingThe first step of the process is to clean the data.A way to do this consists in skipping a set ofwords that are considered least informative, thestopwords.
We also discarded all words of smallsize (less than three characters).We then reduced each word to its stem us-ing the Porter algorithm (Porter, 1980).
Forexample, the words ?models?, ?modelling?
and?modeled?
are all stemmed to ?model?.
Thistechnique for reducing words to their root per-mits to further reduce the number of wordterms.This feature selection phase brings more com-putational comfort for the next steps since itgreatly reduces the size of the document collec-tion representation in the vector space model(the dimension of the vector space).2.2 Vector Space ModelThe set of the distinct remaining word stemsW is used to represent the document collec-tion within the vector space model.
Each docu-ment is represented by a ?W?-dimensional vec-tor filled in with a weight standing for the im-portance of each word token with respect to thatdocument.
To calculate this weight, we use a tf-normalized version of the ?tfc?
term-weightedcomponents as described by Salton and Buck-ley (1988), i.e.
:tfidfw =tfw ?
log Nnwmax(tf) ??
?wi?W(tfwi ?
logNnwi)2,where tfw is the term frequency of the wordw.
N is the total number of documents in thecollection and nw the number of documents inwhich w occurs.3 The use of phrases in IRThere are various ways to incorporate phrasesin the document modeling.
The usual techniqueis to consider phrases as supplementary termsof the vector space, with the same techniqueas for word terms.
In other words, phrases arethrown into the bag of words.
However, Strza-lkowski and Carballo (1996) argue that usinga standard weighting scheme is inappropriatefor mixed feature sets (such as single words andphrases).
The weight given to least frequentphrases is considered too low.
Their specificityis nevertheless often crucial in order to deter-mine the relevance of a document (Lahtinen,2000).
In weighting the phrases, the interde-pendency between a phrase and the words thatcompose it is another difficult issue to accountfor Strzalkowski et al (1998).There are two main types of phrases: statisti-cal phrases, formed by straight word occurrencecounts, and syntactical phrases.Statistical Phrases.
Mitra et al (1987)form a statistical phrase for each pair of 2stemmed adjacent words that occur in at least25 documents of the TREC-1 collection.
Theselected pairs are then sorted in lexicograph-ical order.
In this technique, we see 2 prob-lems.
First, this lexicographical sorting meansto ignore crucial information about word pairs:their order of occurrence !
This is equivalentto saying that AB is identical to BA.
Further-more, no gap is allowed, although it is frequentto represent the same concept by adding at leastone word between two others.
For example,this definition of a phrase does not permit tonote any similarity between the two text frag-ments ?XML document retrieval?
and ?XMLretrieval?.
This model is thus quite far fromnatural language.Syntactical Phrases.
The technique pre-sented by Mitra et al (1987) for extractingsyntactical phrases is based on a parts-of-speechanalysis (POS) of the document collection.
Aset of tag sequence patterns are predefined tobe recognized as useful phrases.
All maximalsequences of words accepted by this grammarform the set of syntactical phrases.
For exam-ple, a sequence of words tagged as ?verb, car-dinal number, adjective, adjective, noun?
willconstitute a syntactical phrase of size 5.
Everysub-phrase occurring in this same order is alsogenerated, with an unlimited gap (e.g., the pair?verb, noun?
is also generated).
This techniqueoffers a sensible representation of natural lan-guage.
Unfortunately, to obtain the POS of awhole document collection is very costful.
Theindex size is another issue, given that all phrasesare stored, regardless of their frequency.
In theexperiments, the authors indeed admit to cre-ating no index a priori, but instead that thephrases were generated according to each query.This makes the process tractable, but impliesvery slow answers from the retrieval system, andquite a long wait for the end user.On top of computational problems, we see afew further issues.
First, the lack of a mini-mal frequency threshold to reduce the numberof phrases in the index.
This means that unfre-quent phrases are taking up most of the space,and have a big influence on the results, whereastheir low frequency may simply illustrate an in-adequate use or a typographical error.
To al-low an illimited gap so as to generate subpairsis dangerous as well: the phrase ?I like to eathot dogs?
will generate the subpair ?hot dogs?,but it will also generate the subpair ?like dogs?,whose semantical meaning is very far from thatof the original sentence.Other types of phrases.
Many efficienttechniques exist to extract multiword ex-pressions, collocations, lexical units and id-ioms (Church and Hanks, 1989; Smadja, 1993;Dias et al, 2000; Dias, 2003).
Unfortunately,very few have been applied to information re-trieval with a deep evaluation of the results.Maximal Frequent Sequences.
We pro-pose Maximal Frequent Sequences (MFS) as anew alternative to account for word ordering inthe modeling of textual documents.
One of theirstrength is the fact that they are extracted ifand only if they occur more often than a givenfrequency threshold, which hopefully permitsto avoid storing the numerous least significantphrases.
A gap between words is allowed withinthe extraction process itself, permitting to dealwith a larger variety of language.4 Maximal Frequent SequencesIn our approach, we represent documents byword features within the vector space model,and by Maximal Frequent Sequences, account-ing for the sequential aspect of text.
For eachof those two representations, a Retrieval Sta-tus Value (RSV) is computed.
Those values arelater combined to form a single RSV per docu-ment.4.1 Definition and ExtractionTechniqueMFS are sequences of words that are frequentin the document collection and, moreover, thatare not contained in any other longer frequentsequence.
Given a frequency threshold ?, a se-quence is considered to be frequent if it appearsin at least ?
documents.Ahonen-Myka (1999) presents an algorithmcombining bottom-up and greedy methods,which permits to extract maximal sequenceswithout considering all their frequent subse-quences.
This is a necessity, since maximal fre-quent sequences in documents may be ratherlong.Nevertheless, when we tried to extract themaximal frequent sequences from the collectionof documents, their number and the total num-ber of word features in the collection did pose aclear computational problem and did not actu-ally permit to obtain any result.To bypass this complexity problem, we de-composed the collection of documents into sev-eral disjoint subcollections, small enough sothat we could efficiently extract the set of max-imal frequent sequences of each subcollection.Joining all the sets of MFS?, we obtained an ap-proximate of the maximal frequent sequence setfor the full collection.We conjecture that more consistent subcol-lections permit to obtain a better approxima-tion.
This is due to the fact that maximal fre-quent sequences are formed from similar textfragments.
Accordingly, we formed the subcol-lection by clustering similar documents togetherusing the well-known k-means algorithm (see forexample Willett (1988) or Doucet and Ahonen-Myka (2002)).4.2 Main Strengths of the MaximalFrequent SequencesThe method efficiently extracts all the maxi-mal frequent word sequences from the collec-tion.
From the definitions above, a sequence issaid to be maximal if and only if no other fre-quent sequence contains that sequence.Furthermore, a gap between words is allowed:in a sentence, the words do not have to appearcontinuously.
A parameter g tells how manyother words two words in a sequence can havebetween them.
The parameter g usually getsvalues between 1 and 3.For instance, if g = 2, a phrase ?PresidentBush?
will be found in both of the followingtext fragments:..President of the United States Bush....President George W.
Bush..Note: Articles, prepositions and small wordswere pruned away during the preprocessing.This allowance of gaps between words of asequence is probably the strongest specificity ofthe method, compared to most existing meth-ods for extracting text descriptors.
This greatlyincreases the quality of the phrase, since pro-cessing takes the variety of natural languageinto account.The other powerful specificity of the tech-nique is the ability to extract maximal frequentsequences of any length.
This permits to ob-tain a very compact description of documents.For example, by restricting the length of phrasesto 8, the presence, in the document collection,of a frequent phrase of 25 words would resultin thousands of phrases representing the sameknowledge as this one maximal sequence.The result of this extraction is that each doc-ument of the collection is described by a (pos-sibly empty) set of MFS.5 Evaluating DocumentsOnce documents and queries are representedwithin our two models, a way to estimate therelevance of a document with respect to a queryremains to be found.
As mentioned earlier,we compute two separate RSV values for theword features vector space model and the MFSmodel.
In the second step, we aggregate thesetwo RSVs into one single relevance score foreach document with respect to a query.5.1 Word features RSVThe vector space model offers a very conve-nient framework for computing similarities be-tween documents and queries.
Indeed, thereexist a number of techniques to compare twovectors, Euclidean distance, Jaccard and cosinesimilarity being the most frequently used in IR.We have used cosine similarity because of itscomputational efficiency.
By normalizing thevectors, which we did in the indexing phase,cosine(??d1,?
?d2) indeed simplifies to the vectorproduct (d1 ?
d2).5.2 MFS RSVThe first step is to create an MFS index forthe document collection.
Once a set of maxi-mal frequent sequences has been extracted andeach document is attached to the correspond-ing phrases, as detailed in the previous section,it remains to define the procedure to match aphrase describing a document and a keyphrase(from a query).Note that from here onwards, keyphrase de-notes a phrase found in a query, and maximalsequence denotes a phrase extracted from a doc-ument.Our approach consists in decomposingkeyphrases of the query into pairs.
Each ofthese pairs is bound to a score representing itsquantity of relevance.
Informally speaking, thequantity of relevance of a word pair tells howmuch it makes a document relevant to includean occurrence of this pair.
This value dependson the specificity of the pair (expressed in termsof inverted document frequency) and modifiers,among which is an adjacency coefficient, reduc-ing the quantity of relevance given to a pairformed by two words that are not adjacent.5.2.1 Definitions:Let D be a collection of N documents andA1 .
.
.
Am a keyphrase of size m. Let Ai andAj be 2 words of A1 .
.
.
Am occurring in this or-der, and n be the number of documents of thecollection in which AiAj was found.
We definethe quantity of relevance of the pair AiAj to be:Qrel(AiAj) = idf(AiAj , D) ?
adj(AiAj),where idf(AiAj , D) represents the specificityof AiAj in collection D:idf(AiAj , D) = log(Nn),and when decomposing the keyphraseA1 .
.
.
Am into pairs, adj(AiAj) is a score mod-ifier to penalize word pairs AiAj formed fromnon-adjacent words, and d(Ai,Aj) indicates thenumber of words appearing between the twowords Ai and Aj (d(Ai,Aj) = 0 signifies thatAi and Aj are adjacent):adj(AiAj) =????
?1, if d(Ai,Aj) = 0?1, 0 ?
?1 ?
1, if d(Ai,Aj) = 1?2, 0 ?
?2 ?
?1 if d(Ai,Aj) = 2. .
.
?m?2, 0 ?
?m?2 ?
?m?3, if d(Ai,Aj) = m?2Accordingly, the larger the distance betweenthe two words, the lower a quantity of relevanceis attributed to the corresponding pair.
In ourruns, we will actually ignore distances higherthan 1 (i.e., (k > 1) ?
(?k = 0)).5.2.2 Example:For example, ignoring distances above 1, akeyphrase ABCD is decomposed into 5 tuples(pair, adjacency coefficient):(AB, 1), (BC, 1), (CD, 1), (AC, ?1), (BD, ?1)Let us compare this keyphrase to the doc-uments d1, d2, d3, d4 and d5, described respec-tively by the frequent sequences AB, AC, AFB,ABC and ACB.
The corresponding quantities ofrelevance brought by the keyphrase ABCD areshown in table 1.
Note that in practice, we lostthe maximality property during the partition-join step presented in subsection 4.1.
Hence,there can be a frequent sequence AB togetherwith a frequent sequence ABC, if they were ex-tracted from two different document clusters.Assuming equal idf values, we observe thatthe quantities of relevance form a coherentorder.
The longest matches rank first, andmatches of equal size are untied by adja-cency.
Moreover, non-adjacent matches (ACand ABC) are not ignored as in many otherphrase representations (Mitra et al, 1987).5.3 Aggregated RSVIn practice, some queries do not contain anykeyphrase, and some documents do not containany MFS.
However, there can of course be cor-rect answers to these queries, and those docu-ments must be relevant to some queries.
Also,all documents containing the same matchingphrases get the same MFS RSV.
Therefore, it isnecessary to find a way to separate them.
Theword-based cosine similarity measure is very ap-propriate for that.Another natural response would have been tore-decompose the pairs into single words andform document vectors accordingly.
However,this would not be satisfying, because the leastfrequent words are all missed by the algorithmfor MFS extraction.
An even more impor-tant category of missed words is that of fre-quent words that do not frequently co-occurwith other words.
The loss would be consid-erable.This is the reason to compute another RSVusing a basic word-features vector space model.<Keywords>"concurrency control""semantic transaction management""application" "performance benefit""prototype" "simulation" "analysis"</Keywords>Figure 1: Topic 47To combine both RSVs to one single score, wemust first make them comparable by mappingthem to a common interval.
To do so, weused Max Norm, as presented by Vogt and Cot-trell (1998), which permits to bring all positivescores within the range [0,1]:New Score =Old ScoreMax ScoreFollowing this normalization step, we aggre-gate both RSVs using a linear interpolation fac-tor ?
representing the relative weight of scoresobtained with each technique (similarly as inMarx et al (2002)).Aggregated Score = ?
?RSVWord Features+(1??
)?RSVMFSThe evidence of experiments with the INEX2002 collection showed good results whenweighting the single word RSV with the num-ber of distinct word terms in the query (let a bethat number), and the MFS RSV with the num-ber of distinct word terms found in keyphrasesof the query (let b be that number).
Thus:?
=aa+ bFor example, in Figure 1 showing topic 47,there are 11 distinct word terms and 7 distinctword terms occurring in keyphrases.
Thus, forthis topic, we have ?
= 1111+7 .6 Experiments and ResultsWe based our experiments on the 494Mb INEXdocument collection (Initiative for the Evalu-ation of XML retrieval1).
INEX was createdin 2002 to compensate the lack of an evalua-tion forum for the XML information retrieval.This collection consists of 12,107 scientific ar-ticles written in English from IEEE journals,combined to a set of queries and correspond-ing manual assessments.
The specificity of this1available at http://inex.is.informatik.uni-duisburg.de:2003/Document MFS Corresponding pairs Matches Quantity of relevanced1 AB AB AB idf(AB)d2 ACD AC CD AD AC CD idf(CD) + ?1.idf(AC)d3 AFB AF FB AB AB idf(AB)d4 ABC AB BC AC AB BC AC idf(AB) + idf(BC) + ?1.idf(AC)d5 ACB AC CB AB AC AB idf(AB) + ?1.idf(AC)Table 1: Quantity of relevance stemming from various indexing phrases w.r.t.
a keyphrase queryABCDdocument collection is its rich logical structureinto sections, subsections, paragraphs, lists, etc.However, in the present experiments, we ignorethis structure and only exploit plain text to re-turn full articles as our candidate retrieval an-swers.The manual assessments indeed tell us whichcandidate answers are relevant and which onesare not.
We use these relevance values to com-pute precision and recall measures, which per-mit scoring each set of candidate answers, andequivalently the means by which each set wasobtained.
In our experiments, we used averageprecision over the n first hits as our main refer-ence.
This evaluation measure was first intro-duced by Raghavan et al (1989) and was usedas the official evaluation measure in the INEX2002 campaign (Go?vert et al, 2003).Protocol of the Experiments.
As a base-line, we computed and evaluated a run usingonly single word terms, as detailed in section2.
Our goal was to compare our new tech-nique to the state of the art.
Thus we com-puted one run using our technique (aggregat-ing the MFS RSVs and the single word termRSVs topic-wise, with the weighting schemementioned hereabove), and one run by calcu-lating all statistical phrases following the defi-nition of Mitra et al (1987).
The only differ-ence is that we did not set a minimal documentfrequency threshold.
We made this choice fromthe standpoint that our aim was not to mea-sure efficiency, but the quality of the results.The corresponding number of features is givenin table 2.
We extracted 328,289 MFS of dif-ferent sizes.
Their splitting forms no more than674,257 pairs (this number is probably lower be-cause the same pair can be extracted from nu-merous MFS).MFS vs. Statistical Phrases.
For thoserepresentations, the average precision for the nfirst retrieved documents are presented in ta-ble 3.
We learn two things from those results.Number of FeaturesWord terms (Baseline) 156,723Statitiscal Phrases 4,941,051MFS 674,257Table 2: Number of feature termsWeight of the word RSV Words & Stat.
PairsTopicwise (subsection 5.3.)
0.0582520% 0.0590240% 0.0595760% 0.0584380% 0.05527100% 0.05302Table 4: Average Precision@100 for various lin-ear combinationsFirst, the fact that phrases improve results inlower levels of recall is confirmed, as greaterimprovement is obtained when we check fur-ther down the ranked list.
Second, our tech-nique outperforms that of statistical phrases.However, as we use different phrases indeed,but also a different technique to match themagainst queries, it remains to find out whetherthe improvement stems from the MFS them-selves, from the way they are used, or from both.Thus we experimented with various linearcombinations to aggregate the word term RSVand the statistical phrase RSV.
The results arepresented in table 4.
The technique of gath-ering word and pairs features within the samevector space clearly performs better in this case.Therefore, the better performance of MFS is notonly due to the aggregation weigthing schemepresented in subsection 5.3.
This underlinestheir intrinsic quality as document descriptors.Word Terms Words and Stat.
Phrases Words and MFSAverage Precision@100 0.05302 0.06199 (+16.9%) 0.06713 (+26.6%)Average Precision@50 0.64419 0.62456 (-3.0%) 0.64411 (-0.0%)Average Precision@10 0.67101 0.65021 (-3.1%) 0.66293 (-1.2%)Table 3: Average Precision@n7 ConclusionsWe have introduced a new type of phrases tothe problem of information retrieval.
We havedeveloped and presented a method to use maxi-mal frequent sequences in information retrieval.Using the INEX document collection, we com-pared it to a well-known technique of the stateof the art.
Our technique outperformed thatof statistical phrases, known to be perform-ing comparably to syntactical and linguisticalphrases from the literature.These results are due to the allowance of agap between words forming a sequence, offer-ing a more realistic model of natural language.Furthermore, the number of phrases to index israther small.
A weak spot is the greedy algo-rithm to extract MFS.
But many improvementsare under way on this side, and the partition-join technique mentioned in subsection 4.1 al-ready permits to extract good approximationsefficiently.Our results confirm that the best improve-ments are obtained at the highest levels of re-call.
Therefore, MFS would be most useful inthe case of exhaustive information needs.
Caseswhere no relevant information should be missed,and 100% recall should be reached in a mini-mal number of hits (their inner ordering beinga less serious matter).
Typically, examples ofsuch information lie in the judicial domain andin patent searching.More experiments remain to be done, to findout whether similar improvements can be ob-tained from other document collections.
TheINEX collection is of scientific articles and con-sistently uses a terminology of its own.
Whethersimilar performance would be observed from amore general document collection such as news-paper articles has to be verified.The use of phrases is factual in many lan-guages, which makes us optimistic regardingan application of this work to multilingualdocument corporas.
Thinking of the othertechniques, the gap should give us robustnessagainst the challenges of multilingualism.8 AcknowledgementsThis work was funded by the Academy of Fin-land under project 50959: DoReMi - DocumentManagement, Information Retrieval and TextMining.ReferencesHelena Ahonen-Myka.
1999.
Finding All Fre-quent Maximal Sequences in Text.
In Pro-ceedings of the 16th International Confer-ence on Machine Learning ICML-99 Work-shop on Machine Learning in Text Data Anal-ysis, Ljubljana, Slovenia, pages 11?17.
J. Ste-fan Institute, eds.
D. Mladenic and M. Gro-belnik.Kenneth W. Church and Patrick Hanks.
1989.Word association norms, mutual information,and lexicography.
In Proceedings of the 27thmeeting of the Association for ComputationalLinguistics (ACL), pages 76?83.Gae?l Dias, Sylvie Guillore?, Jean-Claude Bas-sano, and Jose?
Gabriel Pereira Lopes.
2000.Combining linguistics with statistics for mul-tiword term extraction: A fruitful as-sociation?
In Proceedings of Recherched?Informations Assistee par Ordinateur 2000(RIAO 2000).G.
Dias.
2003.
Multiword unit hybrid extrac-tion.
In Workshop on Multiword Expressionsof the 41st ACL meeting.
Sapporo.
Japan.A.
Doucet and H. Ahonen-Myka.
2002.
Naiveclustering of a large xml document collec-tion.
In Proceedings of the First Workshopof the Initiative for the Evaluation of XMLRetrieval (INEX), pages 81?87, Schloss Dag-suhl, Germany.J.
L. Fagan.
1989.
The effectiveness of a non-syntactic approach to automatic phrase in-dexing for document retrieval.
Journal of theAmerican Society for Information Science,40:115?132.Norbert Go?vert, Gabriella Kazai, Norbert Fuhr,and Mounia Lalmas.
2003.
Evaluating the ef-fectiveness of content-oriented XML retrieval.Technical report, University of Dortmund,Computer Science 6.Timo Lahtinen.
2000.
Automatic Indexing: anapproach using an index term corpus andcombining linguistic and statistical methods.Ph.D.
thesis, University of Helsinki.M.
Marx, J. Kamps, and M. de Rijke.
2002.The university of amsterdam at inex.M.
Mitra, C. Buckley, A. Singhal, andC.
Cardie.
1987.
An analysis of statisti-cal and syntactic phrases.
In Proceedingsof RIAO97, Computer-Assisted InformationSearching on the Internet, pages 200?214.M.
F. Porter.
1980.
An algorithm for suffixstripping.
Program, 14(3):130?137.V.
V. Raghavan, P. Bollmann, and Jung G. S.1989.
A critical investigation of recall andprecision as measures of retrieval system per-formance.
ACM Transactions on Informa-tion Systems, 7(3):205?229.G.
Salton and C. Buckley.
1988.
Term-weighting approaches in automatic text re-trieval.
Information Processing and Manage-ment: an International Journal, 24(5):513?523.G.
Salton, C.S.
Yang, and C.T.
Yu.
1975.
Atheory of term importance in automatic textanalysis.
Journal of the American Society forInformation Science, 26:33?44.F.
Smadja.
1993.
Retrieving collocations fromtext: Xtract.
Journal of Computational Lin-guistics, 19:143?177.A.
F. Smeaton and F. Kelledy.
1998.
User-chosen phrases in interactive query formula-tion for information retrieval.
In Proceedingsof the 20th BCS-IRSG Colloquium.Tomek Strzalkowski and Jose Perez Carballo.1996.
Natural language information retrieval:TREC-4 report.
In Text REtrieval Confer-ence, pages 245?258.Tomek Strzalkowski, Gees C. Stein, G. BowdenWise, Jose Perez Carballo, Pasi Tapanainen,Timo Jarvinen, Atro Voutilainen, and JussiKarlgren.
1998.
Natural language informa-tion retrieval: TREC-6 report.
In Text RE-trieval Conference, pages 164?173.A.
Turpin and A. Moffat.
1999.
Statisticalphrases for vector-space information retrieval.In Proceedings of the 22nd ACM SIGIR Con-ference on Research and Development in In-formation Retrieval, pages 309?310.Christopher C. Vogt and Garrison W. Cottrell.1998.
Predicting the performance of linearlycombined IR systems.
In Research and Devel-opment in Information Retrieval, pages 190?196.P.
Willett.
1988.
Recent trends in hierar-chic document clustering: a critical re-view.
Information Processing and Manage-ment, 24(5):577?597.Zhai, Chengxiang, Xiang Tong,N.
Milic Frayling, and Evans D.A.
1997.Evaluation of syntactic phrase indexing.In Proceedings of the 5th Text RetrievalConference, TREC-5, pages 347?358.
