Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 407?414,New York, June 2006. c?2006 Association for Computational LinguisticsLanguage Model Information Retrieval with Document ExpansionTao Tao, Xuanhui Wang, Qiaozhu Mei, ChengXiang ZhaiDepartment of Computer ScienceUniversity of Illinois at Urbana ChampaignAbstractLanguage model information retrieval de-pends on accurate estimation of documentmodels.
In this paper, we propose a docu-ment expansion technique to deal with theproblem of insufficient sampling of docu-ments.
We construct a probabilistic neigh-borhood for each document, and expandthe document with its neighborhood infor-mation.
The expanded document providesa more accurate estimation of the docu-ment model, thus improves retrieval ac-curacy.
Moreover, since document expan-sion and pseudo feedback exploit differentcorpus structures, they can be combined tofurther improve performance.
The experi-ment results on several different data setsdemonstrate the effectiveness of the pro-posed document expansion method.1 IntroductionInformation retrieval with statistical language mod-els (Lafferty and Zhai, 2003) has recently attractedmuch more attention because of its solid theoreti-cal background as well as its good empirical per-formance.
In this approach, queries and documentsare assumed to be sampled from hidden generativemodels, and the similarity between a document anda query is then calculated through the similarity be-tween their underlying models.Clearly, good retrieval performance relies on theaccurate estimation of the query and document mod-els.
Indeed, smoothing of document models hasbeen proved to be very critical (Chen and Good-man, 1998; Kneser and Ney, 1995; Zhai and Laf-ferty, 2001b).
The need for smoothing originatedfrom the zero count problem: when a term does notoccur in a document, the maximum likelihood esti-mator would give it a zero probability.
This is un-reasonable because the zero count is often due to in-sufficient sampling, and a larger sample of the datawould likely contain the term.
Smoothing is pro-posed to address the problem.While most smoothing methods utilize the globalcollection information with a simple interpolation(Ponte and Croft, 1998; Miller et al, 1999; Hiemstraand Kraaij, 1998; Zhai and Lafferty, 2001b), sev-eral recent studies (Liu and Croft, 2004; Kurland andLee, 2004) have shown that local corpus structurescan be exploited to improve retrieval performance.In this paper, we further study the use of local cor-pus structures for document model estimation andpropose to use document expansion to better exploitlocal corpus structures for estimating document lan-guage models.According to statistical principles, the accuracy ofa statistical estimator is largely determined by thesampling size of the observed data; a small dataset generally would result in large variances, thuscan not be trusted completely.
Unfortunately, in re-trieval, we often have to estimate a model based on asingle document.
Since a document is a small sam-ple, our estimate is unlikely to be very accurate.A natural improvement is to enlarge the data sam-ple, ideally in a document-specific way.
Ideally, theenlarged data sample should come from the sameoriginal generative model.
In reality, however, since407the underlying model is unknown to us, we wouldnot really be able to obtain such extra data.
Theessence of this paper is to use document expansionto obtain high quality extra data to enlarge the sam-ple of a document so as to improve the accuracyof the estimated document language model.
Docu-ment expansion was previously explored in (Sing-hal and Pereira, 1999) in the context of the vec-tor space retrieval model, mainly involving selectingmore terms from similar documents.
Our work dif-fers from this previous work in that we study doc-ument expansion in the language modeling frame-work and implement the idea quite differently.Our main idea is to augment a document prob-abilistically with potentially all other documents inthe collection that are similar to the document.
Theprobability associated with each neighbor documentreflects how likely the neighbor document is fromthe underlying distribution of the original document,thus we have a ?probabilistic neighborhood?, whichcan serve as ?extra data?
for the document for es-timating the underlying language model.
From theviewpoint of smoothing, our method extends the ex-isting work on using clusters for smoothing (Liu andCroft, 2004) to allow each document to have its owncluster for smoothing.We evaluated our method using six representativeretrieval test sets.
The experiment results show thatdocument expansion smoothing consistently outper-forms the baseline smoothing methods in all the datasets.
It also outperforms a state-of-the-art cluster-ing smoothing method.
Analysis shows that theimprovement tends to be more significant for shortdocuments, indicating that the improvement indeedcomes from the improved estimation of the docu-ment language model, since a short document pre-sumably would benefit more from the neighborhoodsmoothing.
Moreover, since document expansionand pseudo feedback exploit different corpus struc-tures, they can be combined to further improve per-formance.
As document expansion can be done inthe indexing stage, it is scalable to large collections.2 Document Expansion Retrieval Model2.1 The KL-divergence retrieval modelWe first briefly review the KL-divergence retrievalmodel, on which we will develop the documentexpansion technique.
The KL-divergence modelis a representative state-of-the-art language model-ing approach for retrieval.
It covers the basic lan-guage modeling approach (i.e., the query likelihoodmethod) as a special case and can support feedbackmore naturally.In this approach, a query and a document are as-sumed to be generated from a unigram query lan-guage model ?Q and a unigram document languagemodel ?D, respectively.
Given a query and a docu-ment, we would first compute an estimate of the cor-responding query model (?
?Q) and document model(?
?D), and then score the document w.r.t.
the querybased on the KL-divergence of the two models (Laf-ferty and Zhai, 2001):D(?
?Q || ?
?d) =?w?Vp(w|?
?Q) ?
logp(w|??Q)p(w|?
?d).where V is the set of all the words in our vocabulary.The documents can then be ranked according to theascending order of the KL-divergence values.Clearly, the two fundamental problems in such amodel are to estimate the query model and the doc-ument model, and the accuracy of our estimation ofthese models would affect the retrieval performancesignificantly.
The estimation of the query modelcan often be improved by exploiting the local cor-pus structure in a way similar to pseudo-relevancefeedback (Lafferty and Zhai, 2001; Lavrenko andCroft, 2001; Zhai and Lafferty, 2001a).
The esti-mation of the document model is most often donethrough smoothing with the global collection lan-guage model (Zhai and Lafferty, 2001b), though re-cently there has been some work on using clustersfor smoothing (Liu and Croft, 2004).
Our work ismainly to extend the previous work on documentsmoothing and improve the accuracy of estimationby better exploiting the local corpus structure.
Wenow discuss all these in detail.2.2 Smoothing of document modelsGiven a document d, the simplest way to estimatethe document language model is to treat the docu-ment as a sample from the underlying multinomialword distribution and use the maximum likelihoodestimator: P (w|?
?d) = c(w,d)|d| , where c(w, d) isthe count of word w in document d, and |d| is the408length of d. However, as discussed in virtually allthe existing work on using language models for re-trieval, such an estimate is problematic and inaccu-rate; indeed, it would assign zero probability to anyword not present in document d, causing problemsin scoring a document with query likelihood or KL-divergence (Zhai and Lafferty, 2001b).
Intuitively,such an estimate is inaccurate because the documentis a small sample.To solve this problem, many different smoothingtechniques have been proposed and studied, usuallyinvolving some kind of interpolation of the maxi-mum likelihood estimate and a global collection lan-guage model (Hiemstra and Kraaij, 1998; Miller etal., 1999; Zhai and Lafferty, 2001b).
For exam-ple, Jelinek-Mercer(JM) and Dirichlet are two com-monly used smoothing methods (Zhai and Lafferty,2001b).
JM smoothing uses a fixed parameter ?
tocontrol the interpolation:P (w|?
?d) = ?c(w, d)|d| + (1 ?
?
)P (w|?C),while the Dirichlet smoothing uses a document-dependent coefficient (parameterized with ?)
to con-trol the interpolation:P (w|?
?d) =c(w, d) + ?P (w|?C)|d| + ?
.Here P (w|?C) is the probability of word w given bythe collection language model ?C , which is usuallyestimated using the whole collection of documentsC , e.g., P (w|?C) =Pd?C c(d,w)Pd?C |d|.2.3 Cluster-based document model (CBDM)Recently, the cluster structure of the corpus has beenexploited to improve language models for retrieval(Kurland and Lee, 2004; Liu and Croft, 2004).
Inparticular, the cluster-based language model pro-posed in (Liu and Croft, 2004) uses clustering infor-mation to further smooth a document model.
It di-vides all documents into K different clusters (K =1000 in their experiments).
Both cluster informa-tion and collection information are used to improvethe estimate of the document model:P (w|?
?d) = ?c(w, d)|d| + (1 ?
?)?
[?P (w|?Ld) + (1 ?
?
)P (w|?C )],where ?Ld stands for document d?s cluster modeland ?
and ?
are smoothing parameters.
In thisclustering-based smoothing method, we first smootha cluster model with the collection model usingDirichlet smoothing, and then use smoothed clustermodel as a new reference model to further smooththe document model using JM smoothing; empiricalresults show that the added cluster information in-deed enhances retrieval performance (Liu and Croft,2004).2.4 Document expansionFrom the viewpoint of data augmentation, theclustering-based language model can be regarded as?expanding?
a document with more data from thecluster that contains the document.
This is intu-itively better than simply expanding every documentwith the same collection language model as in thecase of JM or Dirichlet smoothing.
Looking at itfrom this perspective, we see that, as the ?extra data?for smoothing a document model, the cluster con-taining the document is often not optimal.
Indeed,the purpose of clustering is to group similar doc-uments together, hence a cluster model representswell the overall property of all the documents in thecluster.
However, such an average model is often notaccurate for smoothing each individual document.We illustrate this problem in Figure 1(a), where weshow two documents d and a in cluster D. Clearlythe generative model of cluster D is more suitablefor smoothing document a than document d. In gen-eral, the cluster model is more suitable for smooth-ing documents close to the centroid, such as a, but isinaccurate for smoothing a document at the bound-ary, such as d.To achieve optimal smoothing, each documentshould ideally have its own cluster centered on thedocument, as shown in Figure 1(b).
This is pre-cisely what we propose ?
expanding each documentwith a probabilistic neighborhood around the doc-ument and estimate the document model based onsuch a virtual, expanded document.
We can then ap-ply any simple interpolation-based method (e.g., JMor Dirichlet) to such a ?virtual document?
and treatthe word counts given by this ?virtual document?
asif they were the original word counts.The use of neighborhood information is worthmore discussion.
First of all, neighborhood is not a409cluster Dd dd?s neighbors(a) (b)aFigure 1: Clusters, neighborhood, and document ex-pansionclearly defined concept.
In the narrow sense, onlya few documents close to the original one shouldbe included in the neighborhood, while in the widesense, the whole collection can be potentially in-cluded.
It is thus a challenge to define the neighbor-hood concept reasonably.
Secondly, the assumptionthat neighbor documents are sampled from the samegenerative model as the original document is notcompletely valid.
We probably do not want to trustthem so much as the original one.
We solve thesetwo problems by associating a confidence value withevery document in the collection, which reflects ourbelief that the document is sampled from the sameunderlying model as the original document.
When adocument is close to the original one, we have highconfidence, but when it is farther apart, our confi-dence would fade away.
In this way, we constructa probabilistic neighborhood which can potentiallyinclude all the documents with different confidencevalues.
We call a language model based on such aneighborhood document expansion language model(DELM).Technically, we are looking for a new enlargeddocument d?
for each document d in a text collec-tion, such that the new document d?
can be usedto estimate the hidden generative model of d moreaccurately.
Since a good d?
should presumably bebased on both the original document d and its neigh-borhood N(d), we define a function ?:d?
= ?(d,N(d)).
(1)The precise definition of the neighborhood con-cept N(d) relies on the distance or similarity be-tween each pair of documents.
Here, we simplychoose the commonly used cosine similarity, thoughother choices may also be possible.
Given any twodocument models X and Y , the cosine similarity isdFigure 2: Normal distribution of confidence values.defined as:sim(X,Y ) =?i xi ?
yi?
?i(xi)2 ?
?i(yi)2.To model the uncertainty of neighborhood, we as-sign a confidence value ?d(b) to every document b inthe collection to indicate how strongly we believe bis sampled from d?s hidden model.
In general, ?d(b)can be set based on the similarity of b and d ?
themore similar b and d are, the larger ?d(b) wouldbe.
With these confidence values, we construct aprobabilistic neighborhood with every document init, each with a different weight.
The whole problemis thus reduced to how to define ?d(b) exactly.Intuitively, an exponential decay curve can helpregularize the influence from remote documents.
Wetherefore want ?d(b) to satisfy a normal distributioncentered around d. Figure 2 illustrates the shapeof this distribution.
The black dots are neighbor-hood documents centered around d. Their proba-bility values are determined by their distances to thecenter.
We fortunately observe that the cosine sim-ilarities, which we use to decide the neighborhood,are roughly of this decay shape.
We thus use themdirectly without further transformation because thatwould introduce unnecessary parameters.
We set?d(b) by normalizing the cosine similarity scores :?d(b) =sim(d, b)?b??C?
{d} sim(d, b?
).Function ?
serves to balance the confidence be-tween d and its neighborhood N(d) in the model es-timation step.
Intuitively, a shorter document is lesssufficient, hence needs more help from its neighbor-hood.
Conversely, a longer one can rely more onitself.
We use a parameter ?
to control this balance.Thus finally, we obtain a pseudo document d?
with410the following pseudo term count:c(w, d?)
= ?c(w, d) + (1 ?
?)??b?C?
{d}(?d(b) ?
c(w, b)),We hypothesize that, in general, ?d can be estimatedmore accurately from d?
rather than d itself becaused?
contains more complete information about ?d.This hypothesis can be tested by by comparing theretrieval results of applying any smoothing methodto d with those of applying the same method to d?.In our experiments, we will test this hypothesis withboth JM smoothing and Dirichlet smoothing.Note that the proposed document expansion tech-nique is quite general.
Indeed, since it transformsthe original document to a potentially better ?ex-panded document?, it can presumably be used to-gether with any retrieval method, including the vec-tor space model.
In this paper, we focus on evalu-ating this technique with the language modeling ap-proach.Because of the decay shape of the neighborhoodand for the sake of efficiency, we do not have to ac-tually use all documents in C?{d}.
Instead, we cansafely cut off the documents on the tail, and only usethe top M closest neighbors for each document.
Weshow in the experiment section that the performanceis not sensitive to the choice of M when M is suf-ficiently large (for example 100).
Also, since doc-ument expansion can be done completely offline, itcan scale up to large collections.3 ExperimentsWe evaluate the proposed method over six repre-sentative TREC data sets (Voorhees and Harman,2001): AP (Associated Press news 1988-90), LA(LA Times), WSJ (Wall Street Journal 1987-92),SJMN (San Jose Mercury News 1991), DOE (De-partment of Energy), and TREC8 (the ad hoc dataused in TREC8).
Table 1 shows the statistics of thesedata.We choose the first four TREC data sets for per-formance comparison with (Liu and Croft, 2004).To ensure that the comparison is meaningful, we useidentical sources (after all preprocessing).
In addi-tion, we use the large data set TREC8 to show thatour algorithm can scale up, and use DOE because its#document queries #total qrelAP 242918 51-150 21819LA 131896 301-400 2350WSJ 173252 51-100 and 151-200 10141SJMN 90257 51-150 4881TREC8 528155 401-450 4728DOE 226087 DOE queries 2047Table 1: Experiment data setsdocuments are usually short, and our previous expe-rience shows that it is a relatively difficult data set.3.1 Neighborhood document expansionOur model boils down to a standard query likelihoodmodel when no neighborhood document is used.
Wetherefore use two most commonly used smoothingmethods, JM and Dirichlet , as our baselines.
The re-sults are shown in Table 2, where we report both themean average precision (MAP) and precision at 10documents.
JM and Dirichlet indicate the standardlanguage models with JM smoothing and Dirichletsmoothing respectively, and the other two are theones combined with our document expansion.
Forboth baselines, we tune the parameters (?
for JM,and ?
for Dirichlet) to be optimal.
We then use thesame values of ?
or ?
without further tuning for thedocument expansion runs, which means that the pa-rameters may not necessarily optimal for the docu-ment expansion runs.
Despite this disadvantage, wesee that the document expansion runs significantlyoutperform their corresponding baselines, with morethan 15% relative improvement on AP.
The parame-ters M and ?
were set to 100 and 0.5, respectively.To understand the improvement in more detail, weshow the precision values at different levels of recallfor the AP data in Table 3.
Here we see that ourmethod significantly outperforms the baseline at ev-ery precision point.In our model, we introduce two additional param-eters: M and ?.
We first examine M here, and thenstudy ?
in Section 3.3.
Figure 3 shows the perfor-mance trend with respect to the values of M .
Thex-axis is the values of M , and the y-axis is the non-interpolated precision averaging over all 50 queries.We draw two conclusions from this plot: (1) Neigh-borhood information improves retrieval accuracy;adding more documents leads to better retrieval re-sults.
(2) The performance becomes insensitive to411Data JM DELM+JM (impr.
%) Dirichlet DELM + Diri.(impr.
%)AP AvgPrec 0.2058 0.2405 (16.8%***) 0.2168 0.2505 (15.5%***)P@10 0.3990 0.4444 (11.4%***) 0.4323 0.4515 (4.4%**)DOE AvgPrec 0.1759 0.1904 (8.3%***) 0.1804 0.1898 (5.2%**)P@10 0.2629 0.2943 (11.9%*) 0.2600 0.2800 (7.7%*)TREC8 AvgPrec 0.2392 0.2539 (6.01%**) 0.2567 0.2671 (4.05%*)P@10 0.4300 0.4460 (3.7%) 0.4500 0.4740 (5.3%*)Table 2: Comparisons with baselines.
*,**,*** indicate that we accept the improvement hypothesis byWilcoxon test at significance level 0.1, 0.05, 0.01 respectively.AP, TREC queries 51-150Dirichlet DELM+Diri Improvement(%)Rel.
21819 21819Rel.Retr.
10126 10917 7.81% ***Prec.0.0 0.6404 0.6605 3.14% *0.1 0.4333 0.4785 10.4% ***0.2 0.3461 0.3983 15.1% ***0.3 0.2960 0.3496 18.1% ***0.4 0.2436 0.2962 21.6% ***0.5 0.2060 0.2418 17.4% ***0.6 0.1681 0.1975 17.5% ***0.7 0.1290 0.1580 22.5% ***0.8 0.0862 0.1095 27.0% **0.9 0.0475 0.0695 46.3% **1.0 0.0220 0.0257 16.8%ave.
0.2168 0.2505 15.5% ***Table 3: PR curve on AP data.
*,**,*** indicate thatwe accept the improvement hypothesis by Wilcoxontest at significant level 0.1, 0.05, 0.01 respectively.M when M is sufficiently large, namely 100.
Thereason is twofold: First, since the neighborhood iscentered around the original document, when M islarge, the expansion may be evenly magnified on allterm dimensions.
Second, the exponentially decay-ing confidence values reduce the influence of remotedocuments.3.2 Comparison with CBDMIn this section, we compare the CBDM method us-ing the model performing the best in (Liu and Croft,2004)1.
Furthermore, we also set Dirichlet prior pa-rameter ?
= 1000, as mentioned in (Liu and Croft,2004), to rule out any potential influence of Dirichletsmoothing.Table 4 shows that our model outperforms CBDMin MAP values on four data sets; the improvement1We use the exact same data, queries, stemming and allother preprocessing techniques.
The baseline results in (Liu andCroft, 2004) are confirmed.0.170.180.190.20.210.220.230.240.250.260.270 100 200 300 400 500 600 700 800averageprecesionM : the number of  neighborhood documentsAPDOETREC8Figure 3: Performance change with respect to MCBDM DELM+Diri.
improvement(%)AP 0.2326 0.2505 7.7%LA 0.2590 0.2655 2.5%WSJ 0.3006 0.3113 3.6%SJMN 0.2171 0.2266 4.3%Table 4: Comparisons with CBDM.presumably comes from a more principled way ofexploiting corpus structures.
Given that clusteringcan at least capture the local structure to some ex-tent, it should not be very surprising that the im-provement of document expansion over CBDM ismuch less than that over the baselines.Note that we cannot fulfill Wilcoxon test becauseof the lack of the individual query results of CBDM.3.3 Impact on short documentsDocument expansion is to solve the insufficient sam-pling problem.
Intuitively, a short document is lesssufficient than a longer one, hence would need more?help?
from its neighborhood.
We design experi-ments to test this hypothesis.Specifically, we randomly shrink each documentin AP88-89 to a certain percentage of its originallength.
For example, a shrinkage factor of 30%means each term has 30% chance to stay, or 70%chance to be filtered out.
In this way, we reduce theoriginal data set to a new one with the same number412average doc length 30% 50% 70% 100%baseline 0.1273 0.1672 0.1916 0.2168document expansion 0.1794 0.2137 0.2307 0.2505optimal ?
0.2 0.3 0.3 0.4improvement(%) 41% 28% 20% 16%Table 5: Impact on short documents (in MAP)0.140.160.180.20.220.240.260.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9averageprecisionalpha30%50%70%100%Figure 4: Performance change with respect to ?of documents but a shorter average document length.Table 5 shows the experiment results over docu-ment sets with different average document lengths.The results indeed support our hypothesis that doc-ument expansion does help short documents morethan longer ones.
While we can manage to improve41% on a 30%-length corpus, the same model onlygets 16% improvement on the full length corpus.To understand how ?
affects the performance weplot the sensitivity curves in Figure 4.
The curves alllook similar, but the optimal points slightly migratewhen the average document length becomes shorter.A 100% corpus gets optimal at ?
= 0.4, but 30%corpus has to use ?
= 0.2 to obtain its optimum.
(All optimal ?
values are presented in the fourth rowof Table 5.
)3.4 Further improvement with pseudofeedbackQuery expansion has been proved to be an effec-tive way of utilizing corpus information to improvethe query representation (Rocchio, 1971; Zhai andLafferty, 2001a).
It is thus interesting to examinewhether our model can be combined with query ex-pansion to further improve the retrieval accuracy.We use the model-based feedback proposed in (Zhaiand Lafferty, 2001a) and take top 5 returned docu-ments for feedback.
There are two parameters in themodel-based pseudo feedback process: the noisy pa-DELM pseudo DELM+pseudo Impr.
(%)AP 0.2505 0.2643 0.2726 3.14%*LA 0.2655 0.2769 0.2901 4.77%TREC8 0.2671 0.2716 0.2809 3.42%**DOE 0.1898 0.1918 0.2046 6.67%***Table 6: Combination with pseudo feed-back.
*,**,*** indicate that we accept the improve-ment hypothesis by Wilcoxon test at significantlevel 0.1, 0.05, 0.01 respectively.pseu.
inter.
combined (%) z-scoreAP 0.2643 0.2450 0.2660 (0.64%) -0.2888LA 0.2769 0.2662 0.2636 (-0.48%) -1.0570TREC8 0.2716 0.2702 0.2739 (0.84%) -1.6938Table 7: Performance of the interpolation algorithmcombined with the pseudo feedback.rameter ?
and the interpolation parameter ?2.
We fix?
= 0.9 and tune ?
to optimal, and use them directlyin the feedback process combined with our models.
(It again means that ?
is probably not optimal in ourresults.)
The combination is conducted in the fol-lowing way: (1) Retrieve documents by our DELMmethod; (2) Choose top 5 document to do the model-based feedback; (3) Use the expanded query modelto retrieve documents again with DELM method.Table 6 shows the experiment results (MAP); in-deed, by combining DELM with pseudo feedback,we can obtain significant further improvement ofperformance.As another baseline, we also tested the algorithmproposed in (Kurland and Lee, 2004).
Since the al-gorithm overlaps with pseudo feedback process, it isnot easy to further combine them.
We implement itsbest-performing algorithm, ?interpolation?
(labeledas inter.
), and show the results in Table 7.
Here,we use the same three data sets as used in (Kurlandand Lee, 2004).
We tune the feedback parameters tooptimal in each experiment.
The second last columnin Table 7 shows the performance of combination ofthe ?interpolation?
model with the pseudo feedbackand its improvement percentage.
The last column isthe z-scores of Wilcoxon test.
The negative z-scoresindicate that none of the improvement is significant.2 (Zhai and Lafferty, 2001a) uses different notations.
Wechange them because ?
has already been used in our ownmodel.4134 ConclusionsIn this paper, we proposed a novel document expan-sion method to enrich the document sample throughexploiting the local corpus structure.
Unlike pre-vious cluster-based models, we smooth each doc-ument using a probabilistic neighborhood centeredaround the document itself.Experiment results show that (1) The proposeddocument expansion method outperforms both the?no expansion?
baselines and the cluster-based mod-els.
(2) Our model is relatively insensitive to the set-ting of parameter M as long as it is sufficiently large,while the parameter ?
should be set according to thedocument length; short documents need a smaller?
to obtain more help from its neighborhood.
(3)Document expansion can be combined with pseudofeedback to further improve performance.
Since anyretrieval model can be presumably applied on top ofthe expanded documents, we believe that the pro-posed technique can be potentially useful for any re-trieval model.5 AcknowledgmentsThis work is in part supported by the National Sci-ence Foundation under award number IIS-0347933.We thank Xiaoyong Liu for kindly providing us sev-eral processed data sets for our performance com-parison.
We thank Jing Jiang and Azadeh Shakeryfor helping improve the paper writing, and thank theanonymous reviewers for their useful comments.ReferencesS.
F. Chen and J. Goodman.
1998.
An empirical study ofsmoothing techniques for language modeling.
Techni-cal Report TR-10-98, Harvard University.D.
Hiemstra and W. Kraaij.
1998.
Twenty-one at trec-7:Ad-hoc and cross-language track.
In Proc.
of SeventhText REtrieval Conference (TREC-7).R.
Kneser and H. Ney.
1995.
Improved smoothing for m-gram languagemodeling.
In Proceedings of the Inter-national Conference on Acoustics, Speech and SignalProcessing.Oren Kurland and Lillian Lee.
2004.
Corpus structure,language models, and ad hoc information retrieval.
InSIGIR ?04: Proceedings of the 27th annual interna-tional conference on Research and development in in-formation retrieval, pages 194?201.
ACM Press.John Lafferty and Chengxiang Zhai.
2001.
Documentlanguage models, query models, and risk minimiza-tion for information retrieval.
In Proceedings of SI-GIR?2001, pages 111?119, Sept.John Lafferty and ChengXiang Zhai.
2003.
Probabilisticrelevance models based on document and query gen-eration.Victor Lavrenko and Bruce Croft.
2001.
Relevance-based language models.
In Proceedings of SI-GIR?2001, Sept.Xiaoyong Liu and W. Bruce Croft.
2004.
Cluster-basedretrieval using language models.
In SIGIR ?04: Pro-ceedings of the 27th annual international conferenceon Research and development in information retrieval,pages 186?193.
ACM Press.D.
H. Miller, T. Leek, and R. Schwartz.
1999.
A hid-den markov model information retrieval system.
InProceedings of the 1999 ACM SIGIR Conference onResearch and Development in Information Retrieval,pages 214?221.J.
Ponte and W. B. Croft.
1998.
A language modelingapproach to information retrieval.
In Proceedings ofthe ACM SIGIR, pages 275?281.J.
Rocchio.
1971.
Relevance feedback in information re-trieval.
In The SMART Retrieval System: Experimentsin Automatic Document Processing, pages 313?323.Prentice-Hall Inc.Amit Singhal and Fernando Pereira.
1999.
Documentexpansion for speech retrieval.
In SIGIR ?99: Pro-ceedings of the 22nd annual international ACM SIGIRconference on Research and development in informa-tion retrieval, pages 34?41.
ACM Press.E.
Voorhees and D. Harman, editors.
2001.
Proceedingsof Text REtrieval Conference (TREC1-9).
NIST Spe-cial Publications.
http://trec.nist.gov/pubs.html.Chengxiang Zhai and John Lafferty.
2001a.
Model-based feedback in the KL-divergence retrieval model.In Tenth International Conference on Information andKnowledge Management (CIKM 2001), pages 403?410.Chengxiang Zhai and John Lafferty.
2001b.
A studyof smoothing methods for language models applied toad hoc information retrieval.
In Proceedings of SI-GIR?2001, pages 334?342, Sept.414
