Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 893?903, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsNo Noun Phrase Left Behind: Detecting and Typing Unlinkable EntitiesThomas Lin, Mausam, Oren EtzioniComputer Science & EngineeringUniversity of WashingtonSeattle, WA 98195, USA{tlin, mausam, etzioni}@cs.washington.eduAbstractEntity linking systems link noun-phrase men-tions in text to their corresponding Wikipediaarticles.
However, NLP applications wouldgain from the ability to detect and type allentities mentioned in text, including the longtail of entities not prominent enough to havetheir own Wikipedia articles.
In this paper weshow that once the Wikipedia entities men-tioned in a corpus of textual assertions arelinked, this can further enable the detectionand fine-grained typing of the unlinkable en-tities.
Our proposed method for detecting un-linkable entities achieves 24% greater accu-racy than a Named Entity Recognition base-line, and our method for fine-grained typing isable to propagate over 1,000 types from linkedWikipedia entities to unlinkable entities.
De-tection and typing of unlinkable entities canincrease yield for NLP applications such astyped question answering.1 IntroductionA key challenge in machine reading (Etzioni et al2006) is to identify the entities mentioned in text,and associate them with appropriate background in-formation such as their type.
Consider the sentence?Some people think that pineapple juice is good forvitamin C.?
To analyze this sentence, a machineshould know that ?pineapple juice?
refers to a bev-erage, while ?vitamin C?
refers to a nutrient.Entity linking (Bunescu and Pas?ca, 2006;Cucerzan, 2007) addresses this problem by link-ing noun phrases within the sentence to entriesin a large, fixed entity catalog (almost alwaysexample noun phrases status?apple juice?
?orange juice?
present?prune juice?
?wheatgrass juice?
absent?radiation exposure?
?workplace stress?
present?asbestos exposure?
?financial stress?
absent?IJCAI?
?OOPSLA?
present?EMNLP?
?ICAPS?
absentTable 1: Wikipedia has entries for prominent entities,while missing tail and new entities of the same types.Wikipedia).
Thus, entity linking has a limited andsomewhat arbitrary range.
In our example, systemsby (Ferragina and Scaiella, 2010) and (Ratinov etal., 2011) both link ?vitamin C?
correctly, but link?pineapple juice?
to ?pineapple.?
?Pineapple juice?is not entity linked as a beverage because it is notprominent enough to have its own Wikipedia entry.As Table 1 shows, Wikipedia often has prominententities, while missing tail and new entities of thesame types.1 (Wang et al2012) notes that thereare more than 900 different active shoe brands, butonly 82 exist in Wikipedia.
In scenarios such as in-telligence analysis and local search, non-Wikipediaentities are often the most important.Hence, we introduce the unlinkable noun phraseproblem: Given a noun phrase that does not linkinto Wikipedia, return whether it is an entity, as wellits fine-grained semantic types.
Deciding if a non-Wikipedia noun phrase is an entity is challengingbecause many of them are not entities (e.g., ?Somepeople,?
?an addition?
and ?nearly half?).
Predict-1The same problem occurs with Freebase, which is alsomissing the same Table 1 entities.893ing semantic types is a challenge because of the di-versity of entity types in general text.
In our experi-ments, we utilized the Freebase type system, whichcontains over 1,000 semantic types.The first part of this paper proposes a novelmethod for detecting entities by observing that enti-ties often have different usage-over-time character-istics than non-entities.
Evaluation shows that ourmethod achieves 24% relative accuracy gain overa NER baseline.
The second part of this papershows how instance-to-instance class propagation(Kozareva et al2011) can be adapted and scaled tosemantically type general noun-phrase entities usingtypes from linked entities, by leveraging over onemillion different possible textual relations.Contributions of our research include:?
We motivate and introduce the unlinkable nounphrase problem, which extends previous workin entity linking.?
We propose a novel method for discriminatingentities from arbitrary noun phrases, utilizingfeatures derived from Google Books ngrams.?
We adapt and scale instance-to-instance classpropagation in order to associate types withnon-Wikipedia entities.?
We implement and evaluate our methods, em-pirically verifying improvement over appropri-ate baselines.2 BackgroundIn this section we provide an overview of entity link-ing, how we entity link our data set, and describehow our problem and approach differ from relatedareas such as NER and Web extraction.2.1 Entity LinkingGiven text, the task of entity linking (Bunescuand Pas?ca, 2006; Cucerzan, 2007; Milne and Wit-ten, 2008; Kulkarni et al2009) is to identify theWikipedia entities within the text, and mark themwith which Wikipedia entity they correspond to.
En-tity linking elevates us from plain text into mean-ingful entities that have properties, semantic types,and relationships with each other.
Other entity cata-logs can be used in place of Wikipedia, especially indomain-specific contexts, but general purpose link-ing systems all use Wikipedia because of its broadgeneral coverage, and to leverage its article texts andlink structure during the linking process.A problem we observed when using entity link-ing systems is that despite containing over 3 millionentities, Wikipedia does not cover a significant num-ber of entities.
This occurs with entities that are notprominent enough to have their own dedicated arti-cle and with entities that are very new.
For exam-ple, Facebook has over 600 million users, and eachof them could be considered an entity.
The REVERBextractor (Fader et al2011) on the ClueWeb09 Webcorpus found over 1.4 billion noun phrases partic-ipating in textual relationships, and a sizable por-tion of these noun phrases are entities.
While re-cent research has used NIL features to determinewhether they are being asked to link an entity not inWikipedia (Dredze et al2010; Ploch, 2011), therehas been no research on whether given noun phrasesthat are unlinkable (for not being in Wikipedia) areentities, and how to make them usable if they are.Our goal is to address this problem of learningwhether non-Wikipedia noun phrases are entities,and assigning semantic types to them to make themuseful.
We begin with a corpus of 15 million ?
(nounphrase subject, textual relation, noun phrase object)?assertions from the Web that were extracted by RE-VERB (Fader et al2011).2 REVERB already filtersout relative pronouns, WHO-adverbs, and existential?there?
noun phrases that do not make meaningfularguments.
We then employ standard entity linkingtechniques including string matching, prominencepriors (Fader et al2009), and context matching(Bunescu and Pas?ca, 2006) to link the noun phrasesubjects into Wikipedia.In this manner, we were able to entity link thenoun phrase subject of 9,699,967 extractions, whilethe remaining 5,028,301 extractions had no matches(mostly due to no close string matches).
There were1,401,713 distinct noun phrase subjects in the 5 mil-lion extractions that had no matches.
These are theunlinkable noun phrases we will study here.2.2 Named Entity RecognitionNamed Entity Recognition (NER) is the task ofidentifying named entities in text.
A key differencebetween our final goals and NER is that in the con-2available at http://reverb.cs.washington.edu894text of entity linking and Wikipedia, there are manymore entities than just the named entities.
For ex-ample, ?apple juice?
and ?television?
are Wikipediaentities (with Wikipedia articles), but are not tradi-tional named entities.
Still, as named entities docomprise a sizable portion of our unlinkable nounphrases, we compare against a NER baseline in ourentity detection step.Fine-grained NER (Sekine and Nobata, 2004; Leeet al2007) has studied scaling NER to up to 200 se-mantic types.
This differs from our semantic typingof unlinked entities because our approach assumesaccess to corpora-level relationships between a largeset of linked entities (with semantic types) and theunlinked entities.
As a result we are able to propa-gate 1,339 Freebase semantic types from the linkedentities to the unlinked entities, which is substan-tially more types than fine-grained NER.2.3 Extracting Entity SetsThere is a line of research in using Web extraction(Etzioni et al2005) and entity set expansion (Pantelet al2009) to extract lists of typed entities from theWeb (e.g., a list of every city).
Our problem insteadfocuses on determining whether any individual nounphrase is an entity, and what semantic types it holds.Given a noun phrase representing a person name, wereturn that this is a person entity even if it is not in alist of people names harvested from the Web.3 ArchitectureOur goal is: given (1) a large set of linked assertionsL and (2) a large set of unlinked assertions U , foreach unlinkable noun phrase subject n ?
U , predict:(1) whether n is an entity, and if so, then (2) the setof Freebase semantic types for n. For L we use the9.7 million assertions whose subject argument wewere able to link in Section 2.1, and for U we usethe 5 million assertions that we could not link.We divide the system into two components.
Thefirst component (described in Section 4) takes anyunlinkable noun phrase and outputs whether it is anentity.
All n ?
U classified as entities are placed ina set E. The second component (described in Sec-tion 5) uses L and U to predict the semantic typesfor each entity e ?
E.Figure 1: Usage over time in Google books for the nounphrase ?Prices quoted?
(e.g., from ?Prices quoted are for2 adults?)
which is not an entity.Figure 2: Usage over time for the unlinkable noun phrase?Soluble fibre,?
which is an entity.
The best fit line hassteeper slope compared to Figure 1.4 Detecting Unlinkable EntitiesThis first task takes in any unlinkable noun phraseand outputs whether it is an entity.
There is a longhistory of discussion in analytic philosophy litera-ture on the question of what exists (e.g., (Quine,1948)).
We adopt a more pragmatic view, defin-ing an ?entity?
as a noun phrase that could have aWikipedia-style article if there were no notability ornewness considerations, and which would have se-mantic types.
We are interested in entities that couldhelp populate an entity store.
?EMNLP 2012?
is anexample of an entity, while ?The method?
and ?12cats?
are examples of noun phrases that are not en-tities.
This is challenging because at a surface level,many entities and non-entities look similar: ?Sexand the City?
is an entity, while ?John and David?is not.
?Eminem?
is an entity, while ?Youd?
(a typofrom ?You?d?)
is not.We address this task by training a classifier withfeatures primarily derived from a timestamped cor-pus.
An intuition here is that when consideringonly unlinkable noun phrases, usage patterns across895Figure 3: Plot of R2 vs Slope for the usage over time of a collection of noun phrases selected for illustrative purposes.Many of the non-entities occur at lower Slope and higher R2, while the entities often have higher slope and/or lowerR2.
?Bluetooth technology?
actually has even higher slope, but was adjusted left to fit in this figure.time often differ for entities and non-entities.
Nounphrase entities that are observed in text going backhundreds of years (e.g., ?Europe?)
almost all havetheir own Wikipedia entries, so in unlinkable nounphrase space, the remaining noun phrases that areobserved in text going back hundreds of years tendto be all the textual references and expressions thatare not entities.
We plan to use this signal to helpseparate the entities from the non-entities.4.1 Classifier FeaturesWe use the Google Books ngram corpus (Michelet al2010), which contains timestamped usageof 1-grams through 5-grams in millions of digi-tized books for each year from 1500 to 2007.3 Weuse ngram match count values from case-insensitivematching.
To avoid sparsity anomalies we observedin years before 1740, we use the data from 1740 on-ward.
While it has not been used for our task before,this corpus is a rich resource that enables reason-ing about knowledge (Evans and Foster, 2011) and3available at http://books.google.com/ngrams/datasetsunderstanding semantic changes of words over time(Wijaya and Yeniterzi, 2011).
Talukdar et al2012)recently used it to effectively temporally scope rela-tional facts.Our first feature is the slope of the least-squaresbest fit line for usage over time.
For example, if aterm appears 25 times in books in 1950, 30 times in1951, ..., 100 times in 2007, then we compute thestraight line that best fits {(1950, 25), (1951, 31), ...,(2007, 100)}, and examine the slope.
We have ob-served cases of non-entity noun phrases (e.g., Fig-ure 1) having lower slopes than entity noun phrases(e.g., Figure 2).
Note that we do not normalizematch counts by yearly total frequency, but we donormalize counts for each term to range from 0 to 1(setting the max count for each term to 1) to avoidbias from entity prominence.
To capture the currentusage, in cases where there exists a ?
5 year gap inusage of a term we only use the data after the gap.Another feature is the R2 fit of the best fit line.Higher R2 indicates that the data is closer to astraight line.
Figure 3 plots R2 vs Slope values896Figure 4: UsageSinceYear of example unlinked terms.for some sample noun phrases.
We observed thatalong with their lower Slope, the non-entities oftenalso had higher R2, indicating that their usage doesnot vary wildly from year to year.
This contrastswith certain entities (e.g., ?FY 99?
for ?Fiscal Year1999?)
whose usage sometimes varied sharply fromyear to year based on their prominence in those spe-cific years.A third feature is UsageSinceYear, which finds theyear from when a term last started continually beingused.
For example, a UsageSinceYear value of 1920would indicate that the term was used in books ev-ery year from 1920 through 2007.
Figure 4 showsexamples of where various terms fall along this di-mension.From the books ngram corpus, we also calcu-late features for: PercentProperCaps - the percent-age of case-insensitive matches for the term whereall words began with a capital letter, PercentExact-Match - the percentage of case-insensitive matchesfor the term that matched the capitalization in theassertion exactly, and Frequency - the total numberof case-insensitive occurrences of the term in thebook ngrams data, summed across all years, whichreflects prominence.
Last, we also include a sim-ple numeric feature to detect the presence of leadingnumeric words (e.g., ?5?
in ?5 days?
or ?Three?
in?Three choices?
).4.2 EvaluationFrom the corpus of 15 million REVERB assertions,there were 1.4 million unlinked noun phrases includ-ing 17% unigrams, 51% bigrams, 21% trigrams, and11% 4-grams or longer.
Bigrams comprise over halfthe noun phrases and the books bigram data is a self-contained download that is easier to obtain and storesystem correctly classifiedMajority class baseline 50.4%Named Entity Recognition 63.3%Slope feature only 61.1%PUF feature combination 69.1%ALL features 78.4%Table 2: Our classifier using all features (ALL) outper-forms majority class and NER baselines.than the full books ngram corpus, so we focus onbigrams in our evaluation.
In a random sample ofunlinked bigrams, we found that 73% were presentin the books ngram data (65% exact match, 8% case-insensitive match only), while 27% were not (thesewere mostly entities or errors with non-alphabeticcharacters).
Coverage is a greater issue with longerngrams (e.g., there are many more possible 5-gramsthan bigrams, so any individual 5-gram is less likelyto reach the minimum threshold to be included in thebooks data), but as mentioned earlier, only 11% ofunlinkable noun phrases were 4-grams or longer.We randomly sampled 250 unlinked bigrams thathad books ngram data, and asked 2 annotators to la-bel each as ?entity,?
?non-entity,?
or ?unclear.?
Ourgoal is to separate noun phrases that are clearly en-tities (e.g., ?prune juice?)
from those that are clearlynot entities (e.g., ?prices quoted?
), rather than to de-bate phrases that may be in some entity store defi-nitions but not others, so we asked the annotators tochoose ?unclear?
when there was any doubt.
Therewere 151 bigrams that both annotators believed tobe very clear labels, including 69 that both annota-tors labeled as entities, 70 that both annotators la-beled as non-entities, and 12 with label disagree-ment.
Cohen?s kappa was 0.84, indicating excellentagreement.
Our experiment is now to separate the69 clear entities from the 70 clear non-entities.For experiment baselines we use the majorityclass baseline MAJ, as well as a Named EntityRecognition baseline NER.
For NER we used theIllinois Named Entity Tagger (Ratinov and Roth,2009) on the highest setting (that achieved 90.5 F1score on the CoNLL03 shared task).
NER expects asentence, so we use the longest assertion in the cor-pus that the noun phrase was observed in.
We eval-uate several combinations of our features to test dif-897ferent aspects of our system: Slope uses only Slope,PUF uses PercentProperCaps + UsageSinceYear +Frequency, and ALL uses all features.
We evaluateusing the WEKA J48 Decision Tree on default set-tings, with leave-one-out cross validation.Table 2 shows the results.
MAJ correctly classifies50.4% of instances, NER correctly classifies 63.3%and ALL correctly classifies 78.4%.4.3 AnalysisOverall, 78.4% correctly classified instances is fairlystrong performance on this task.
By using the de-scribed features, our classifier was able to detect andfilter many of the non-entity noun phrases in thisscenario.
Compared to the 63.3% of NER, it is anabsolute gain of 15.1%, a relative gain of 24%, and areduction in error of 41.1% (from 36.7% to 21.6%).Student?s t-test at 95% confidence verified that thedifference was significant.We found that while low Slope (especially withhigher R2) often indicated non-entity, there were nu-merous cases where higher Slope did not necessarilyindicate entity.
For example, the noun phrase ?sev-eral websites?
has fairly sharp slope, but still doesnot denote a clear entity.
In these cases, the addi-tion of other features can serve as additional usefulsignal.
One error from ALL is the term ?Analyst esti-mates,?
which the annotators labeled as a non-entity,but which occasionally appears in text (especially ti-tles) as ?Analyst Estimates,?
and is a relatively re-cent phrase.
NER misses entities such as ?syntheticcubism?
and ?hunter orange?
that occur in our databut are not traditional named entities.
We observedthat while none of our features achieves over 70%accuracy by themselves, they perform well in con-junction with each other.5 Propagating Semantic TypesThis second task uses a set of linked assertions L andset of unlinked assertions U to predict the semantictypes for each entity e ?
E. If the previous stepoutput that ?Sun Microsystems?
is likely to be anentity, then the goal of this step is to further predictthat it has the Freebase types such as organizationand software developer.From L we use the set of linked entities and thetextual relations they occur with.
For example, Lmight contain that the entity Microsoft links to a par-ticular Wikipedia article, and also that it occurs withtextual relations such as ?has already announced?and ?has released updates for.?
For each Wikipedia-linked entity in L, we further look up its exact setof Freebase types.4 From U we obtain the set oftextual relations that each e ?
E is in the domainof.
We now have a large set of class-labeled in-stances (all entities in L), a large set of unlabeledinstances (E), and a method to connect the unla-beled instances with the class-labeled instances (viaany shared textual relations), so we cast this taskas an instance-to-instance class propagation problem(Kozareva et al2011) for propagating class labelsfrom labeled to unlabeled instances.We build on the recent work of Kozareva et al(2011), and adapt their approach to leverage thescale and resources of our scenario.
While they useonly one type of edge between instances, namelyshared presence in the high precision DAP pattern(Hovy et al2009), our final system uses 1.3 mil-lion textual relations from |L ?
U |, correspondingto 1.3 million potential edge types.
Their evaluationinvolved only 20 semantic classes, while we use all1,339 Freebase types covered by our entities in L.There is a rich history of other approaches forpredicting semantic types.
(Talukdar et al2008)and (Talukdar and Pereira, 2010) model relation-ships between instances and classes, but our un-linked entities do not come with any class informa-tion.
Pattern-based approaches (Pas?ca, 2004; Panteland Ravichandran, 2004) are popular, but (Kozarevaet al2011) notes that they ?are constraint to the in-formation matched by the pattern and often sufferfrom recall,?
meaning that they do not cover manyinstances.
Classifiers have also been trained for fine-grained semantic typing, but for noticeably fewertypes than we work with.
(Rahman and Ng, 2010)studied hierarchical and collective classification us-ing 92 types, and FIGER (Ling and Weld, 2012) re-cently used an adapted perceptron for multi-classmulti-label classification into 112 types.5.1 AlgorithmGiven an entity e, our algorithm involves: (1) find-ing the textual relations that e is in the domain of, (2)4data available at http://download.freebase.com/wex898Figure 5: This example illustrates the set of Freebase type predictions for the noun phrase ?Sun Microsystems.?
Wepredict the semantic type of a noun phrase by: (1) finding the textual relations it is in the domain of, (2) finding linkedentities that are also in the domain of those textual relations, and (3) observing their semantic types.finding linked entities that are also in the domain ofthose textual relations, and then (3) predicting typesby observing the types of those linked entities.
Fig-ure 5 illustrates how we would predict the semantictypes of the noun phrase ?Sun Microsystems.
?Find Relations: Obtain the set R of all textual re-lations in U that e is in the domain of.
For example,if U contains the assertion ?
(Sun Microsystems, hasreleased a minor update to, Java 1.4.2),?
then the tex-tual relation ?has released a minor update to?
shouldbe added to R when typing ?Sun Microsystems.
?Find Similar Entities: Find the linked entities inL that are in the domain of the most relations inR.
In our example, entities such as ?Microsoft?
and?Apple Inc.?
have the greatest overlap in textual re-lations because they are most often in the domainof the same textual relations, e.g., (?Microsoft, hasreleased a minor update to, Windows Live Essen-tials?).
Create a set S of the entities that share themost textual relations.
We found keeping 10 similarentities (|S| = 10) is generally enough to predict theoriginal entity?s types in the final step.Predict Types: Return the most frequent Freebasetypes of the entities in S as the prediction.
Toavoid penalizing very small types, if there are n in-stances of semantic class C in S, then we rank C us-ing a type score T (n,C, S) = max(n/|S|, n/|C|),which we found to perform better than T (n,C, S) =avg(n/|S|, n/|C|).
For ?Sun Microsystems,?
busi-ness operation was the top predicted type becauseall entities in S were observed to include businessoperation type.5.2 Edge ValidityThis algorithm will only be effective if entities thatshare textual relation strings are more likely to beof the same semantic types.
To verify this, we sam-pled 30,000 linked entities from L that had at least30 textual relations each, and associated each withtheir 30 most frequent relations.
From the 900 mil-lion possible entity pairs, we then randomly sample500 entity pairs that shared exactly k out of 30 rela-tions, for each k from 0 to 15.
At each k we then useour sampled pairs to estimate the probability that anytwo entities sharing exactly k relations (out of their30 possible) will share at least one type.The results are shown in Figure 6.
We found thatentities sharing more textual relations were in factmore likely to have semantic types in common.
Twoentities that shared exactly 0 of 30 textual relationswere only 11% likely to share a semantic type, whiletwo entities that shared exactly 10 of 30 relationswere 80% likely to share a semantic type.
This vali-dates our use of textual relations as a signal-bearingedge in instance-to-instance class propagation.5.3 Weighting Textual RelationsThe algorithm as currently described treats all tex-tual relations equally, when in reality some arestronger signal to entity type than others.
For exam-ple, two entities in the domain of the ?came with?relation often will not share semantic types, but twoentities in the domain of the ?autographed?
relationwill almost always share a type.
To capture this intu-ition, we define relation weight w(r) as the observedprobability (among the linked entities) that two en-899Figure 6: Entities that share more textual relations aremore likely to have semantic types in common.tities will share a Freebase type if they both occurin the domain of r. If D(r) = all entities observedin the domain of relation r and T (e) = all Freebasetypes listed for entity e, then weight w(r) of a tex-tual relation string r is:w(r) =?e1,e2?D(r), e1 6=e2I(e1, e2)|D(r)| ?
(|D(r)| ?
1)I(e1, e2) ={1, if |T (e1) ?
T (e2)| > 00, otherwise.Table 3 shows examples of high weight relations,and Table 4 shows low weight relations.
We nowmodify the Find Similar Entities step such that ifa linked entity shares a set of relations Q with theentity being typed, then it receives a score whichconsiders all shared relations q ?
Q but uses thehigh weight relations more.
On a development setwe found that a score of?q?Q 104?w(q) was effec-tive, as higher weight signifies much stronger signal.This score then determines which entities to place inS.5.4 EvaluationThe goal of the evaluation is to judge how well ourmethod can predict the Freebase semantic types ofentities in our scenario.
Our linked entities cov-ered 1,339 Freebase types, including many interest-ing types such as computer operating system, reli-gious text, airline and baseball team.
Human judgeswould have trouble manually annotating new enti-ties with all these types because there are too manyto keep in mind and understand the characteristics?is a highway in?
?is a university located in?
?became the president of?
?turned down the role of?
?has an embassy in?Table 3: Example relations found to have high weight.
?comes with?
?is a generic term for?
?works best on?
?can be made from?
?is almost identical to?Table 4: Example relations found to have low weight.of.
Instead, we automatically generate testing databy sampling entities from L, and then test on abil-ity to recover the actual Freebase types (which weknow).We sample a HEAD set of distinct 500 Freebaseentities (drawn randomly from our set of linked ex-tractions), and a TAIL set of 500 entities (drawn ran-domly from our set of linked entities).
An entitythat occurs in many extractions is more likely to bein HEAD than TAIL.
Our sampling also picks onlyentities that occur with at least 10 relations, whichis appropriate for the Web scenario where more in-stances can always be queried for.For baselines we use random baseline BRandomand a frequency baseline BFrequency which alwaysreturns types in order of their frequency amongall linked entities (e.g., always person, then loca-tion, etc).
We evaluate our system without rela-tion weighting (SNoWeight) and also with relationweighting (SWeighted).
For SWeighted we leave allthe test set entities out when calculating global re-lation weights.
Our metrics are Precision at 1 andF1 score.
Precision at 1 measures how often the topreturned type is a correct type, and is useful for ap-plications that want one type per entity.
F1 mea-sures how well the method recovers the full set ofFreebase types (for each test case we graph preci-sion/recall and take the max F1), and is useful forapplications such as typed question answering.Table 5 shows the results.
BRandom performspoorly because there are so many semantic types,and very few of them are correct for each testcase.
BFrequency performs slightly better on TAILthan HEAD because TAIL contains more entities ofthe most frequent types.
SNoWeight performance900HEAD TAILPrec@1 F1 Prec@1 F1BRandom 0.008 0.028 0.004 0.023BFrequency 0.244 0.302 0.298 0.322SNoWeight 0.542?
0.465?
0.510?
0.456?SWeighted 0.610?
0.521?
0.598?
0.522?Table 5: Evaluation on HEAD and TAIL, 500 elements each.
?
indicates statistical significance over BFrequency, and?
over both BFrequency and SNoWeight.
Significance is measured using the Student?s t-test at 95% confidence.
Thetop type predicted by our SWeighted method is correct about 60% of the time, while the top type predicted by theBFrequency baseline is correct under 30% of the time.is statistically significant above all baselines, andSWeighted is statistically significant over SNoWeighton both test sets and metrics.5.5 AnalysisSWeighted was successful at recovering the correctFreebase types of many entities.
For example, itfinds that ?Atherosclerosis?
is a medical risk fac-tor by connecting it to ?obesity?
and ?diabetes,?
that?Supernatural?
is a TV program and a Netflix titleby connecting it to ?House?
and ?30 Rock,?
and that?America West?
is an aircraft owner and an airlineby connecting it to ?Etihad Airways?
and ?ChinaEastern Airlines.?
While precision at 1 around 60%may not be high enough yet for certain applications,it is significantly better than competing approaches,which are under 30%, and we hope that our valuescan serve as a non-trivial baseline on this task forfuture systems.One example where SWeighted made some mis-takes is fictional characters.
Many fictional charac-ters participate in a textual relations that make themlook like people (e.g., ?was born on?
), but predictingthat they belong to people class is incorrect.
Someperformance hit was also due to entity linking errors.From an assertion like ?The Four Seasons is locatedin Hamamatsu,?
our entity linker (and other entitylinkers we tried) prefer linking ?The Four Seasons?to Vivaldi?s music composition rather than the hotelchain.
We are then unable to recover music compo-sition type from relations like ?is located in.?
Ouralgorithm relies on accurate entity linking in L, butthere is a precision/recall tradeoff to consider herebecause it also benefits from higher coverage of en-tities and relations in L.As a general reference for performance ofstate-of-the-art fine-grained entity classification, theFIGER system (Ling and Weld, 2012) for classify-ing into 112 types reported F1 scores ranging from0.471 to 0.693 in their experiments.
It is important tonote that these numbers are not directly comparableto us because they used different data, different (andfewer) types, and different evaluation methodology.6 DiscussionThis paper presented an approach for working withnon-Wikipedia entities in text.
Consider the follow-ing possibilities for a noun phrase in a text corpus:Wikipedia Entity: (e.g., ?Computer Science,?
?South America,?
?apple juice?)
- Entity linkingtechniques can identify and type these.Non-Wikipedia, Non-Entity: (e.g., ?strangethings,?
?Early studies,?
?A link?)
- Our classifierfrom Section 4 is able to filter these.Non-Wikipedia, Entity: (e.g., ?Safflower oil,?
?prune juice,?
?Amazon UK?)
- We identify theseas entities, then propagate semantic types to them.Our technique finds that ?Safflower oil?
occurs withhigh weight relations such as ?is sometimes used totreat?
and ?can be substituted for,?
making it similarto linked entities such as ?Phentermine?
and ?Dan-delion,?
and then correctly predicts semantic typesincluding food ingredient and medical treatment.6.1 Typed Question AnsweringFrom our set of 15 million assertions, we found andtyped many non-Wikipedia entities.
In food whileWikipedia has ?crab meat,?
we find it is missing oth-ers such as ?rabbit meat?
and ?goat milk.?
In job ti-tles it has ?scientist?
and ?lawyer,?
but we find it ismissing ?PhD student,?
?fashion designer,?
and oth-ers.
We find many of the people and employers not901prominent enough for Wikipedia.One application of this research is to increase theyield of applications such as Typed Question An-swering (Buscaldi and Rosso, 2006).
For example,consider the query ?What computer game is a lot offun??
A search for assertions matching ?
* is a lotof fun?
in the data yields around 1,000 results suchas ?camping,?
?David Sedaris?
and ?Hawaii.?
En-tity linking allows us to identify just the computergames in Wikipedia that match the query, such as?Civilization.?
However, around 400 query matchescould not be entity linked.
Our noun-phrase clas-sifier filters out non-entities such as ?actual play,?
?Just this?
and ?Two kids.?
After predicting typesfor the matches that did not get filtered, we find ad-ditional non-Wikipedia computer games that matchthe query, including ?Cooking Dash,?
?DeliciousDeluxe?
and ?Slingo Supreme.
?7 Future WorkAn area we are continuing to improve the systemon is textual ambiguity.
For example, an unlinkablenoun phrase might simultaneously be the name of afilm, a car, and a person.
Instead of outputting thatthe noun phrase holds all of those types, a strongeroutput would be to realize that the noun phrase isambiguous, determine how many senses it has, anddetermine which sense is being referred to in eachinstance.
We have ideas for how to detect ambiguousentities using mutual exclusion (Carlson, 2010) andfunctional relations.
For example, if we predict thata noun phrase has film and car types but we alsoobserve in our linked instances that these types aremutually exclusive, then this is good evidence thatthe noun phrase refers to multiple terms.We also plan to continue improving our tech-niques, as there is still plenty of room for improve-ment on both subtasks.
For detecting new entities,we are interested in seeing if timestamped Twitterdata could be analyzed to increase both recall andprecision.
For predicting semantic types, (Kozarevaet al2011) proposed additional techniques whichwe have not fully explored.
Also, we can incor-porate additional signals such as shared term headswhen they are available, in order to help find termsthat are likely to share types.
Last, we would liketo feed back our system output to improve systemperformance.
For example, non-entity noun phrasesthat make it to the typing step might lead to particu-lar predicted type distributions that indicate an erroroccurred earlier in the process.8 ConclusionIn this paper we showed that while entity linkingcannot link to entities outside of Wikipedia, once alarge text corpus has been entity linked, the presenceand content of the existing links can be leveraged tohelp detect and semantically type the non-Wikipediaentities as well.
We designed techniques for de-tecting whether unlinkable noun phrases are entities,and if they are, then propagating semantic types tothem from the linked entities.
In our evaluations, weshowed that our techniques achieve statistically sig-nificant improvement over baseline methods.Our research here takes initial steps toward a fu-ture where the vast universe of entities that are notprominent enough to include in manually-authoredknowledge bases is analyzed automatically insteadof being left behind.AcknowledgementsWe thank Stephen Soderland, Xiao Ling, and thethree anonymous reviewers for their helpful feed-back on earlier drafts.
This research was sup-ported in part by NSF grant IIS-0803481, ONR grantN00014-08-1-0431, and DARPA contract FA8750-09-C-0179, and carried out at the University ofWashington?s Turing Center.ReferencesRazvan Bunescu and Marius Pas?ca.
2006.
Using ency-clopedic knowledge for named entity disambiguation.In Proceedings of the 11th Conference of the EuropeanChapter of the Association of Computational Linguis-tics (EACL).Davide Buscaldi and Paolo Rosso.
2006.
Mining knowl-edge from wikipedia for the question answering task.In Proceedings of the 5th International Conference onLanguage Resources and Evaluation (LREC).Andrew Carlson.
2010.
Coupled Semi-SupervisedLearning.
Ph.D. thesis, Carnegie Mellon University.Silviu Cucerzan.
2007.
Large-scale named entity disam-biguation based on wikipedia data.
In Proceedings ofEMNLP.902Mark Dredze, Paul McNamee, Delip Rao, Adam Ger-ber, and Tim Finin.
2010.
Entity disambiguation forknowledge base population.
In Proceedings of COL-ING.Oren Etzioni, Michael Cafarella, Doug Downey, Ana-Maria Popescu, Tal Shaked, Stephen Soderland,Daniel S. Weld, and Alexander Yates.
2005.
Unsu-pervised named-entity extraction from the Web: Anexperimental study.
In Artificial Intelligence.Oren Etzioni, Michele Banko, and Michael J. Cafarella.2006.
Machine Reading.
In Proceedings of the 21stNational Conference on Artificial Intelligence (AAAI).James A. Evans and Jacob G. Foster.
2011.
Metaknowl-edge.
In Science.Anthony Fader, Stephen Soderland, and Oren Etzioni.2009.
Scaling Wikipedia-based named entity disam-biguation to arbitrary Web text.
In IJCAI-09 Workshopon User-contributed Knowledge and Artificial Intelli-gence (WikiAI09).Anthony Fader, Stephen Soderland, and Oren Etzioni.2011.
Identifying relations for open information ex-traction.
In Proceedings of EMNLP.Paolo Ferragina and Ugo Scaiella.
2010.
Tagme:On-the-fly annotation of short text fragments (bywikipedia entities).
In Proceedings of CIKM.Eduard Hovy, Zornitsa Kozareva, and Ellen Riloff.
2009.Toward completeness in concept extraction and classi-fication.
In Proceedings of EMNLP.Zornitsa Kozareva, Konstantin Voevodski, and Shang-Hua Teng.
2011.
Class label enhancement via relatedinstances.
In Proceedings of EMNLP.Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan, andSoumen Chakrabarti.
2009.
Collective annotation ofwikipedia entities in text.
In Proceedings of KDD.Changki Lee, Yi-Gyu Hwang, and Myung-Gil Jang.2007.
Fine-grained named entity recognition and rela-tion extraction for question answering.
In Proceedingsof SIGIR.Xiao Ling and Daniel S. Weld.
2012.
Fine-grained entityrecognition.
In Proceedings of the 26th Conference onArtificial Intelligence (AAAI).Jean-Baptiste Michel, Yuan Kui Shen, Aviva P. Aiden,Adrian Veres, Matthew K. Gray, The Google BooksTeam, Joseph P. Pickett, Dale Hoiberg, Dan Clancy,Peter Norvig, Jon Orwant, and Steven Pinker.
2010.Quantitative analysis of culture using millions of digi-tized books.
In Science.David Milne and Ian H. Witten.
2008.
Learning to linkwith wikipedia.
In Proceedings of the 17h ACM Inter-national Conference on Information and KnowledgeManagement (CIKM).Marius Pas?ca.
2004.
Acquisition of categorized namedentities for web search.
In Proceedings of the ACMInternational Conference on Information and Knowl-edge Management (CIKM).Patrick Pantel and Deepak Ravichandran.
2004.
Auto-matically labeling semantic classes.
In Proceedings ofHLT-NAACL.Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-Maria Popescu, and Vishnu Vyas.
2009.
Web-scaledistributional similarity and entity set expansion.
InProceedings of EMNLP.Danuta Ploch.
2011.
Exploring entity relations fornamed entity disambiguation.
In Proceedings of theAnnual Meeting of the Association of ComputationalLinguistics (ACL).Willard Van Orman Quine.
1948.
On what there is.
InReview of Metaphysics.Altaf Rahman and Vincent Ng.
2010.
Inducing fine-grained semantic classes via hierarchical and collec-tive classification.
In Proceedings of COLING.Lev Ratinov and Dan Roth.
2009.
Design challengesand misconceptions in named entity recognition.
InProceedings of the Thirteenth Conference on Compu-tational Natural Language Learning (CoNLL).Lev Ratinov, Dan Roth, Doug Downey, and Mike Ander-son.
2011.
Local and global algorithms for disam-biguation to wikipedia.
In Proceedings of the AnnualMeeting of the Association of Computational Linguis-tics (ACL).Satoshi Sekine and Chikashi Nobata.
2004.
Definition,dictionaries and tagger for extended named entity hier-archy.
In Proceedings of the 4th International Confer-ence on Language Resources and Evaluation (LREC).Partha Pratim Talukdar and Fernando Pereira.
2010.Experiments in graph-based semi-supervised learningmethods for class-instance acquisition.
In Proceedingsof the Annual Meeting of the Association of Computa-tional Linguistics (ACL).Partha Pratim Talukdar, Joseph Reisinger, Marius Pas?ca,Deepak Ravichandran, Rahul Bhagat, and FernandoPereira.
2008.
Weakly supervised acquisition of la-beled class instances using graph random walks.
InProceedings of EMNLP.Partha Pratim Talukdar, Derry Tanti Wijaya, and TomMitchell.
2012.
Coupled temporal scoping of rela-tional facts.
In Proceedings of WSDM.Chi Wang, Kaushik Chakrabarti, Tao Cheng, and SurajitChaudhuri.
2012.
Targeted disambiguation of ad-hoc,homogeneous sets of named entities.
In Proceedingsof the 21st International World Wide Web Conference(WWW).Derry Tanti Wijaya and Reyyan Yeniterzi.
2011.
Un-derstanding semantic changes of words over centuries.In Workshop on Detecting and Exploiting Cultural Di-versity on the Social Web.903
