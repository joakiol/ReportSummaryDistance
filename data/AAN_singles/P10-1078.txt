Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 760?769,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsMetadata-Aware Measures for Answer Summarizationin Community Question AnsweringMattia Tomasoni ?Dept.
of Information TechnologyUppsala University, Uppsala, Swedenmattia.tomasoni.8371@student.uu.seMinlie HuangDept.
Computer Science and TechnologyTsinghua University, Beijing 100084, Chinaaihuang@tsinghua.edu.cnAbstractThis paper presents a framework for au-tomatically processing information com-ing from community Question Answering(cQA) portals with the purpose of gen-erating a trustful, complete, relevant andsuccinct summary in response to a ques-tion.
We exploit the metadata intrinsicallypresent in User Generated Content (UGC)to bias automatic multi-document summa-rization techniques toward high quality in-formation.
We adopt a representation ofconcepts alternative to n-grams and pro-pose two concept-scoring functions basedon semantic overlap.
Experimental re-sults on data drawn from Yahoo!
An-swers demonstrate the effectiveness of ourmethod in terms of ROUGE scores.
Weshow that the information contained in thebest answers voted by users of cQA por-tals can be successfully complemented byour method.1 IntroductionCommunity Question Answering (cQA) portalsare an example of Social Media where the infor-mation need of a user is expressed in the form of aquestion for which a best answer is picked amongthe ones generated by other users.
cQA websitesare becoming an increasingly popular complementto search engines: overnight, a user can expect ahuman-crafted, natural language answer tailoredto her specific needs.
We have to be aware, though,that User Generated Content (UGC) is often re-dundant, noisy and untrustworthy (Jeon et al,?The research was conducted while the first author wasvisiting Tsinghua University.2006; Wang et al, 2009b; Suryanto et al, 2009).Interestingly, a great amount of information is em-bedded in the metadata generated as a byprod-uct of users?
action and interaction on Social Me-dia.
Much valuable information is contained in an-swers other than the chosen best one (Liu et al,2008).
Our work aims to show that such informa-tion can be successfully extracted and made avail-able by exploiting metadata to distill cQA content.To this end, we casted the problem to an instanceof the query-biased multi-document summariza-tion task, where the question was seen as a queryand the available answers as documents to be sum-marized.
We mapped each characteristic that anideal answer should present to a measurable prop-erty that we wished the final summary could ex-hibit:?
Quality to assess trustfulness in the source,?
Coverage to ensure completeness of the in-formation presented,?
Relevance to keep focused on the user?s in-formation need and?
Novelty to avoid redundancy.Quality of the information was assessed via Ma-chine Learning (ML) techniques under best an-swer supervision in a vector space consisting oflinguistic and statistical features about the answersand their authors.
Coverage was estimated by se-mantic comparison with the knowledge space of acorpus of answers to similar questions which hadbeen retrieved through the Yahoo!
Answers API 1.Relevance was computed as information overlapbetween an answer and its question, while Noveltywas calculated as inverse overlap with all otheranswers to the same question.
A score was as-signed to each concept in an answer according to1http://developer.yahoo.com/answers760the above properties.
A score-maximizing sum-mary under a maximum coverage model was thencomputed by solving an associated Integer LinearProgramming problem (Gillick and Favre, 2009;McDonald, 2007).
We chose to express conceptsin the form of Basic Elements (BE), a semanticunit developed at ISI2 and modeled semantic over-lap as intersection in the equivalence classes oftwo concepts (formal definitions will be given insection 2.3).The objective of our work was to present whatwe believe is a valuable conceptual framework;more advance machine learning and summariza-tion techniques would most likely improve the per-formances.The remaining of this paper is organized as fol-lows.
In the next section Quality, Coverage, Rel-evance and Novelty measures are presented; weexplain how they were calculated and combinedto generate a final summary of all answers to aquestion.
Experiments are illustrated in Section3, where we give evidence of the effectiveness ofour method.
We list related work in Section 5, dis-cuss possible alternative approaches in Section 4and provide our conclusions in Section 6.2 The summarization framework2.1 Quality as a ranking problemQuality assessing of information available on So-cial Media had been studied before mainly as abinary classification problem with the objective ofdetecting low quality content.
We, on the otherhand, treated it as a ranking problem and madeuse of quality estimates with the novel intent ofsuccessfully combining information from sourceswith different levels of trustfulness and writingability.
This is crucial when manipulating UGC,which is known to be subject to particularly greatvariance in credibility (Jeon et al, 2006; Wanget al, 2009b; Suryanto et al, 2009) and may bepoorly written.An answer a was given along with informationabout the user u that authored it, the set TAq (To-tal Answers) of all answers to the same question qand the set TAu of all answers by the same user.Making use of results available in the literature(Agichtein et al, 2008) 3, we designed a Quality2Information Sciences Institute, University of SouthernCalifornia, http://www.isi.edu3A long list of features is proposed; training a classifieron all of them would no doubt increase the performances.feature space to capture the following syntactic,behavioral and statistical properties:?
?, length of answer a?
?
, number of non-stopwords in a with a cor-pus frequency larger than n (set to 5 in ourexperiments)?
$, points awarded to user u according to theYahoo!
Answers?
points system?
%, ratio of best answers posted by user uThe features mentioned above determined a space?
; An answer a, in such feature space, assumedthe vectorial form:?a = ( ?, ?, $, % )Following the intuition that chosen best answers(a?)
carry high quality information, we used su-pervised ML techniques to predict the probabilityof a to have been selected as a best answer a?.
Wetrained a Linear Regression classifier to learn theweight vector W = (w1, w2, w3, w4) that wouldcombine the above feature.
Supervision was givenin the form of a training set TrQ of labeled pairsdefined as:TrQ = {?
?a, isbesta ?
}isbesta was a boolean label indicating whether awas an a?
answer; the training set size was de-termined experimentally and will be discussed inSection 3.2.
Although the value of isbesta wasknown for all answers, the output of the classifieroffered us a real-valued prediction that could beinterpreted as a quality score Q(?a):Q(?a) ?
P ( isbesta = 1 | a, u, TAu, )?
P ( isbesta = 1 | ?a )= W T ?
?a (1)The Quality measure for an answer a was approx-imated by the probability of such answer to be abest answer (isbesta = 1) with respect to its au-thor u and the sets TAu and TAq.
It was calcu-lated as dot product between the learned weightvector W and the feature vector for answer ?a.Our decision to proceed in an unsupervised di-rection came from the consideration that any useof external human annotation would have made itimpracticable to build an actual system on largerscale.
An alternative, completely unsupervised ap-proach to quality detection that has not undergoneexperimental analysis is discussed in Section 4.7612.2 Bag-of-BEs and semantic overlapThe properties that remain to be discussed, namelyCoverage, Relevance and Novelty, are measuresof semantic overlap between concepts; a conceptis the smallest unit of meaning in a portion ofwritten text.
To represent sentences and answerswe adopted an alternative approach to classical n-grams that could be defined bag-of-BEs.
a BEis ?a head|modifier|relation triple representationof a document developed at ISI?
(Zhou et al,2006).
BEs are a strong theoretical instrument totackle the ambiguity inherent in natural languagethat find successful practical applications in real-world query-based summarization systems.
Dif-ferent from n-grams, they are variant in length anddepend on parsing techniques, named entity de-tection, part-of-speech tagging and resolution ofsyntactic forms such as hyponyms, pronouns, per-tainyms, abbreviation and synonyms.
To each BEis associated a class of semantically equivalentBEs as result of what is called a transformationof the original BE; the mentioned class uniquelydefines the concept.
What seemed to us most re-markable is that this makes the concept context-dependent.
A sentence is defined as a set of con-cepts and an answer is defined as the union be-tween the sets that represent its sentences.The rest of this section gives formal definitionof our model of concept representation and seman-tic overlap.
From a set-theoretical point of view,each concepts c was uniquely associated with a setEc = {c1, c2 .
.
.
cm} such that:?i, j (ci ?L c) ?
(ci 6?
c) ?
(ci 6?
cj)In our model, the ???
relation indicated syntac-tic equivalence (exact pattern matching), while the??L?
relation represented semantic equivalenceunder the convention of some language L (twoconcepts having the same meaning).
Ec was de-fined as the set of semantically equivalent conceptsto c, called its equivalence class; each concept ciin Ec carried the same meaning (?L) of concept cwithout being syntactically identical (?
); further-more, no two concepts i and j in the same equiva-lence class were identical.
?Climbing a tree to escape a black bear is pointless be-cause they can climb very well.
?BE = they|climbEc = {climb|bears, bear|go up, climbing|animals,climber|instincts, trees|go up, claws|climb...}Given two concepts c and k:c ./ k{c ?
k orEc ?
Ek 6= ?We defined semantic overlap as occurring betweenc and k if they were syntactically identical or iftheir equivalence classes Ec and Ek had at leastone element in common.
In fact, given the abovedefinition of equivalence class and the transitivityof ???
relation, we have that if the equivalenceclasses of two concepts are not disjoint, then theymust bare the same meaning under the conventionof some language L; in that case we said that csemantically overlapped k. It is worth noting thatrelation ?./?
is symmetric, transitive and reflexive;as a consequence all concepts with the same mean-ing are part of a same equivalence class.
BE andequivalence class extraction were performed bymodifying the behavior of the BEwT-E-0.3 frame-work 4.
The framework itself is responsible forthe operative definition of the ??L?
relation andthe creation of the equivalence classes.2.3 Coverage via concept importanceIn the scenario we proposed, the user?s informa-tion need is addressed in the form of a unique,summarized answer; information that is left out ofthe final summary will simply be unavailable.
Thisraises the concern of completeness: besides ensur-ing that the information provided could be trusted,we wanted to guarantee that the posed questionwas being answered thoroughly.
We adopted thegeneral definition of Coverage as the portion ofrelevant information about a certain subject thatis contained in a document (Swaminathan et al,2009).
We proceeded by treating each answerto a question q as a separate document and weretrieved through the Yahoo!
Answers API a setTKq (Total Knowledge) of 50 answers 5 to ques-tions similar to q: the knowledge space of TKqwas chosen to approximate the entire knowledgespace related to the queried question q.
We cal-culated Coverage as a function of the portion ofanswers in TKq that presented semantic overlapwith a.4The authors can be contacted regarding the possibil-ity of sharing the code of the modified version.
Orig-inal version available from http://www.isi.edu/publications/licensed-sw/BE/index.html.5such limit was imposed by the current version of the API.Experiments with a greater corpus should be carried out in thefuture.762C(a, q) =?ci?a?
(ci) ?
tf(ci, a) (2)The Coverage measure for an answer a was cal-culated as the sum of term frequency tf(ci, a) forconcepts in the answer itself, weighted by a con-cept importance function, ?
(ci), for concepts inthe total knowledge space TKq.
?
(c) was definedas follows:?
(c) =|TKq,c||TKq|?
log2|TKq||TKq,c|(3)where TKq,c = {d ?
TKq : ?k ?
d, k ./ c}The function ?
(c) of concept c was calculated asa function of the cardinality of set TKq and setTKq,c, which was the subset of all those answersd that contained at least one concept k which pre-sented semantical overlap with c itself.
A similaridea of knowledge space coverage is addressed bySwaminathan et al (2009), from which formulas(2) and (3) were derived.A sensible alternative would be to estimate Cov-erage at the sentence level.2.4 Relevance and Novelty via ./ relationTo this point, we have addressed matters of trust-fulness and completeness.
Another widely sharedconcern for Information Retrieval systems is Rel-evance to the query.
We calculated relevance bycomputing the semantic overlap between conceptsin the answers and the question.
Intuitively, we re-ward concepts that express meaning that could befound in the question to be answered.R(c, q) =|qc||q|(4)where qc = {k ?
q : k ./ c}The Relevance measure R(c, q) of a concept cwith respect to a question q was calculated as theratio of the cardinality of set qc (containing allconcepts in q that semantically overlapped with c)normalized by the total number of concepts in q.Another property we found desirable, was tominimize redundancy of information in the finalsummary.
Since all elements in TAq (the set ofall answers to q) would be used for the final sum-mary, we positively rewarded concepts that wereexpressing novel meanings.N(c, q) = 1?|TAq,c||TAq|(5)where TAq,c = {d ?
TAq : ?k ?
d, k ./ c}The Novelty measure N(c, q) of a concept c withrespect to a question q was calculated as the ratioof the cardinality of set TAq,c over the cardinalityof set TAq; TAq,c was the subset of all those an-swers d in TAq that contained at least one conceptk which presented semantical overlap with c.2.5 The concept scoring functionsWe have now determined how to calculate thescores for each property in formulas (1), (2), (4)and (5); under the assumption that the Quality andCoverage of a concept are the same of its answer,every concept c part of an answer a to some ques-tion q, could be assigned a score vector as follows:?c = (Q(?a), C(a, q), R(c, q), N(c, q) )What we needed at this point was a function Sof the above vector which would assign a higherscore to concepts most worthy of being includedin the final summary.
Our intuition was that sinceQuality, Coverage, Novelty and Relevance wereall virtues properties, S needed to be monoton-ically increasing with respect to all its dimen-sions.
We designed two such functions.
Func-tion (6), which multiplied the scores, was basedon the probabilistic interpretation of each score asan independent event.
Further empirical consid-erations, brought us to later introduce a logarith-mic component that would discourage inclusion ofsentences shorter then a threshold t (a reasonablechoice for this parameter is a value around 20).The score for concept c appearing in sentence scwas calculated as:S?
(c) =4?i=1(?ci ) ?
logt(length(sc)) (6)A second approach that made use of humanannotation to learn a vector of weights V =(v1, v2, v3, v4) that linearly combined the scoreswas investigated.
Analogously to what had beendone with scoring function (6), the ?
space wasaugmented with a dimension representing thelength of the answer.S?
(c) =4?i=1(?ci ?
vi) + length(sc) ?
v5 (7)In order to learn the weight vector V that wouldcombine the above scores, we asked three humanannotators to generate question-biased extractivesummaries based on all answers available for acertain question.
We trained a Linear Regression763classifier with a set TrS of labeled pairs definedas:TrS = {?
(?c, length(sc)), includec ?
}includec was a boolean label that indicatedwhether sc, the sentence containing c, hadbeen included in the human-generated summary;length(sc) indicated the length of sentence sc.Questions and relative answers for the generationof human summaries were taken from the ?filtereddataset?
described in Section 3.1.The concept score for the same BE in two sep-arate answers is very likely to be different be-cause it belongs to answers with their own Qualityand Coverage values: this only makes the scoringfunction context-dependent and does not interferewith the calculation the Coverage, Relevance andNovelty measures, which are based on informationoverlap and will regard two BEs with overlappingequivalence classes as being the same, regardlessof their score being different.2.6 Quality constrained summarizationThe previous sections showed how we quantita-tively determined which concepts were more wor-thy of becoming part of the final machine sum-mary M .
The final step was to generate the sum-mary itself by automatically selecting sentencesunder a length constraint.
Choosing this constraintcarefully demonstrated to be of crucial importanceduring the experimental phase.
We again optedfor a metadata-driven approach and designed thelength constraint as a function of the lengths ofall answers to q (TAq) weighted by the respectiveQuality measures:lengthM =?a?TAqlength(a) ?Q(?a) (8)The intuition was that the longer and the moretrustworthy answers to a question were, the morespace was reasonable to allocate for informationin the final, machine summarized answer M .M was generated so as to maximize the scoresof the concepts it included.
This was done under amaximum coverage model by solving the follow-ing Integer Linear Programming problem:maximize:?iS(ci) ?
xi (9)subject to:?jlength(j) ?
sj ?
lengthM?jyj ?
occij ?
xi ?i (10)occij , xi, yj ?
{0, 1} ?i, joccij = 1 if ci ?
sj , ?i, jxi = 1 if ci ?
M, ?iyj = 1 if sj ?
M, ?jIn the above program,M is the set of selected sen-tences: M = {sj : yj = 1, ?j}.
The integervariables xi and yj were equals to one if the corre-sponding concept ci and sentence sj were includedin M .
Similarly occij was equal to one if conceptci was contained in sentence sj .
We maximizedthe sum of scores S(ci) (for S equals to S?
or S?
)for each concept ci in the final summary M .
Wedid so under the constraint that the total length ofall sentences sj included in M must be less thanthe total expected length of the summary itself.
Inaddition, we imposed a consistency constraint: ifa concept ci was included in M , then at least onesentence sj that contained the concept must alsobe selected (constraint (10)).
The described opti-mization problem was solved using lp solve 6.We conclude with an empirical side note: sincesolving the above can be computationally very de-manding for large number of concepts, we foundperformance-wise very fruitful to skim about onefourth of the concepts with lowest scores.3 Experiments3.1 Datasets and filtersThe initial dataset was composed of 216,563 ques-tions and 1,982,006 answers written by 171,676user in 100 categories from the Yahoo!
Answersportal7.
We will refer to this dataset as the ?un-filtered version?.
The metadata described in sec-tion 2.1 was extracted and normalized; qualityexperiments (Section 3.2) were then conducted.The unfiltered version was later reduced to 89,814question-answer pairs that showed statistical andlinguistic properties which made them particularlyadequate for our purpose.
In particular, trivial, fac-toid and encyclopedia-answerable questions were6the version used was lp solve 5.5, available at http://lpsolve.sourceforge.net/5.57The reader is encouraged to contact the authors regardingthe availability of data and filters described in this Section.764removed by applying a series of patterns for theidentification of complex questions.
The work byLiu et al (2008) indicates some categories of ques-tions that are particularly suitable for summariza-tion, but due to the lack of high-performing ques-tion classifiers we resorted to human-crafted ques-tion patterns.
Some pattern examples are the fol-lowing:?
{Why,What is the reason} [...]?
How {to,do,does,did} [...]?
How {is,are,were,was,will} [...]?
How {could,can,would,should} [...]We also removed questions that showed statisticalvalues outside of convenient ranges: the number ofanswers, length of the longest answer and lengthof the sum of all answers (both absolute and nor-malized) were taken in consideration.
In particularwe discarded questions with the following charac-teristics:?
there were less than three answers 8?
the longest answer was over 400 words(likely a copy-and-paste)?
the sum of the length of all answers outsideof the (100, 1000) words interval?
the average length of answers was outside ofthe (50, 300) words intervalAt this point a second version of the datasetwas created to evaluate the summarization perfor-mance under scoring function (6) and (7); it wasgenerated by manually selecting questions thatarouse subjective, human interest from the pre-vious 89,814 question-answer pairs.
The datasetsize was thus reduced to 358 answers to 100 ques-tions that were manually summarized (refer toSection 3.3).
From now on we will refer to thissecond version of the dataset as the ?filtered ver-sion?.3.2 Quality assessingIn Section 2.1 we claimed to be able to identifyhigh quality content.
To demonstrate it, we con-ducted a set of experiments on the original unfil-tered dataset to establish whether the feature space?
was powerful enough to capture the quality ofanswers; our specific objective was to estimate the8Being too easy to summarize or not requiring any sum-marization at all, those questions wouldn?t constitute an valu-able test of the system?s ability to extract information.Figure 1: Precision values (Y-axis) in detecting best an-swers a?
with increasing training set size (X-axis) for a Lin-ear Regression classifier on the unfiltered dataset.amount of training examples needed to success-fully train a classifier for the quality assessing task.The Linear Regression9 method was chosen to de-termine the probabilityQ(?a) of a to be a best an-swer to q; as explained in Section 2.1, those prob-abilities were interpreted as quality estimates.
Theevaluation of the classifier?s output was based onthe observation that given the set of all answersTAq relative to q and the best answer a?, a suc-cessfully trained classifier should be able to ranka?
ahead of all other answers to the same question.More precisely, we defined Precision as follows:|{q ?
TrQ : ?a ?
TAq, Q(?a?)
> Q(?a)}||TrQ|where the numerator was the number of questionsfor which the classifier was able to correctly ranka?
by giving it the highest quality estimate in TAqand the denominator was the total number of ex-amples in the training set TrQ.
Figure 1 shows theprecision values (Y-axis) in identifying best an-swers as the size of TrQ increases (X-axis).
Theexperiment started from a training set of size 100and was repeated adding 300 examples at a timeuntil precision started decreasing.
With each in-crease in training set size, the experiment was re-peated ten times and average precision values werecalculated.
In all runs, training examples werepicked randomly from the unfiltered dataset de-scribed in Section 3.1; for details on TrQ see Sec-tion 2.1.
A training set of 12,000 examples waschosen for the summarization experiments.9Performed with Weka 3.7.0 available at http://www.cs.waikato.ac.nz/?ml/weka765System a?
(baseline) S?
S?ROUGE-1 R 51.7% 67.3% 67.4%ROUGE-1 P 62.2% 54.0% 71.2%ROUGE-1 F 52.9% 59.3% 66.1%ROUGE-2 R 40.5% 52.2% 58.8%ROUGE-2 P 49.0% 41.4% 63.1%ROUGE-2 F 41.6% 45.9% 57.9%ROUGE-L R 50.3% 65.1% 66.3%ROUGE-L P 60.5% 52.3% 70.7%ROUGE-L F 51.5% 57.3% 65.1%Table 1: Summarization Evaluation on filtered dataset (re-fer to Section 3.1 for details).
ROUGE-L, ROUGE-1 andROUGE-2 are presented; for each, Recall (R), Precision (P)and F-1 score (F) are given.3.3 Evaluating answer summariesThe objective of our work was to summarize an-swers from cQA portals.
Two systems were de-signed: Table 1 shows the performances usingfunction S?
(see equation (7)), and function S?
(see equation (6)).
The chosen best answer a?was used as a baseline.
We calculated ROUGE-1and ROUGE-2 scores10 against human annotationon the filtered version of the dataset presented inSection 3.1.
The filtered dataset consisted of 358answers to 100 questions.
For each questions q,three annotators were asked to produce an extrac-tive summary of the information contained in TAqby selecting sentences subject to a fixed lengthlimit of 250 words.
The annotation resulted in 300summaries (larger-scale annotation is still ongo-ing).
For the S?
system, 200 of the 300 generatedsummaries were used for training and the remain-ing were used for testing (see the definition of TrSSection 2.5).
Cross-validation was conducted.
Forthe S?
system, which required no training, all ofthe 300 summaries were used as the test set.S?
outperformed the baseline in Recall (R) butnot in Precision (P); nevertheless, the combined F-1 score (F) was sensibly higher (around 5 pointspercentile).
On the other hand, our S?
systemshowed very consistent improvements of an orderof 10 to 15 points percentile over the baseline onall measures; we would like to draw attention onthe fact that even if Precision scores are higher,it is on Recall scores that greater improvementswere achieved.
This, together with the results ob-tained by S?, suggest performances could benefit10Available at http://berouge.com/default.aspxFigure 2: Increase in ROUGE-L, ROUGE-1 and ROUGE-2 performances of the S?
system as more measures are takenin consideration in the scoring function, starting from Rele-vance alone (R) to the complete system (RQNC).
F-1 scoresare given.from the enforcement of a more stringent lengthconstraint than the one proposed in (8).
Furtherpotential improvements on S?
could be obtainedby choosing a classifier able to learn a more ex-pressive underlying function.In order to determine what influence the singlemeasures had on the overall performance, we con-ducted a final experiment on the filtered dataset toevaluate (the S?
scoring function was used).
Theevaluation was conducted in terms of F-1 scores ofROUGE-L, ROUGE-1 and ROUGE-2.
First onlyRelevance was tested (R) and subsequently Qual-ity was added (RQ); then, in turn, Coverage (RQC)and Novelty (RQN); Finally the complete systemtaking all measures in consideration (RQNC).
Re-sults are shown in Figure 2.
In general perfor-mances increase smoothly with the exception ofROUGE-2 score, which seems to be particularlysensitive to Novelty: no matter what combinationof measures is used (R alone, RQ, RQC), changesin ROUGE-2 score remain under one point per-centile.
Once Novelty is added, performances riseabruptly to the system?s highest.
A summary ex-ample, along with the question and the best an-swer, is presented in Table 2.4 Discussion and Future DirectionsWe conclude by discussing a few alternatives tothe approaches we presented.
The lengthM con-straint for the final summary (Section 2.6), couldhave been determined by making use of externalknowledge such as TKq: since TKq represents766HOW TO PROTECT YOURSELF FROM A BEAR?http://answers.yahoo.com/question/index?qid=20060818062414AA7VldB***BEST ANSWER***Great question.
I have done alot of trekking through California, Montanaand Wyoming and have met Black bears (which are quite dinky and placidbut can go nuts if they have babies), and have been half an hour away from(allegedly) the mother of all grizzley s whilst on a trail through GlacierNational park - so some other trekkerers told me... What the park wardenssay is SING, SHOUT, MAKE NOISE...do it loudly, let them know youare there..they will get out of the way, it is a surprised bear wot will gomental and rip your little legs off..No fun permission: anything that willconfuse them and stop them in their tracks...I have been told be an nativeamerican buddy that to keep a bottle of perfume in your pocket...throw it atthe ground near your feet and make the place stink: they have good noses,them bears, and a mega concentrated dose of Britney Spears ObsessiveCompulsive is gonna give em something to think about...Have you got arape alarm?
Def take that...you only need to distract them for a secondthen they will lose interest..Stick to the trails is the most important thing,and talk to everyone you see when trekking: make sure others know whereyou are.
***SUMMARIZED ANSWER***[...] In addition if the bear actually approaches you or charges you.. stillstand your ground.
Many times they will not actually come in contactwith you, they will charge, almost touch you than run away.
[...] Theactions you should take are different based on the type of bear.
for ex-ample adult Grizzlies can t climb trees, but Black bears can even whenadults.
They can not climb in general as thier claws are longer and notsemi-retractable like a Black bears claws.
[...] I truly disagree with thewhole play dead approach because both Grizzlies and Black bears areoppurtunistic animals and will feed on carrion as well as kill and eat an-imals.
Although Black bears are much more scavenger like and tend notto kill to eat as much as they just look around for scraps.
Grizzlies on theother hand are very accomplished hunters and will take down large preyanimals when they want.
[...] I have lived in the wilderness of NorthernCanada for many years and I can honestly say that Black bears are not atall likely to attack you in most cases they run away as soon as they see orsmell a human, the only places where Black bears are agressive is in parkswith visitors that feed them, everywhere else the bears know that usuallyhumans shoot them and so fear us.
[...]Table 2: A summarized answer composed of five differentportions of text generated with the S?
scoring function; thechosen best answer is presented for comparison.
The rich-ness of the content and the good level of readability makeit a successful instance of metadata-aware summarization ofinformation in cQA systems.
Less satisfying examples in-clude summaries to questions that require a specific order ofsentences or a compromise between strongly discordant opin-ions; in those cases, the summarized answer might lack logi-cal consistency.the total knowledge available about q, a coverageestimate of the final answers against it would havebeen ideal.
Unfortunately the lack of metadataabout those answers prevented us from proceedingin that direction.
This consideration suggests theidea of building TKq using similar answers in thedataset itself, for which metadata is indeed avail-able.
Furthermore, similar questions in the datasetcould have been used to augment the set of an-swers used to generate the final summary with an-swers coming from similar questions.
Wang et al(2009a) presents a method to retrieve similar ques-tions that could be worth taking in considerationfor the task.
We suggest that the retrieval methodcould be made Quality-aware.
A Quality featurespace for questions is presented by Agichtein etal.
(2008) and could be used to rank the quality ofquestions in a way similar to how we ranked thequality of answers.The Quality assessing component itself couldbe built as a module that can be adjusted to thekind of Social Media in use; the creation of cus-tomized Quality feature spaces would make itpossible to handle different sources of UGC (fo-rums, collaborative authoring websites such asWikipedia, blogs etc.).
A great obstacle is the lackof systematically available high quality trainingexamples: a tentative solution could be to makeuse of clustering algorithms in the feature space;high and low quality clusters could then be labeledby comparison with examples of virtuous behav-ior (such as Wikipedia?s Featured Articles).
Thequality of a document could then be estimated as afunction of distance from the centroid of the clus-ter it belongs to.
More careful estimates could takethe position of other clusters and the concentrationof nearby documents in consideration.Finally, in addition to the chosen best answer, aDUC-styled query-focused multi-document sum-mary could be used as a baseline against whichthe performances of the system can be checked.5 Related WorkA work with a similar objective to our own isthat of Liu et al (2008), where standard multi-document summarization techniques are em-ployed along with taxonomic information aboutquestions.
Our approach differs in two fundamen-tal aspects: it took in consideration the peculiari-ties of the data in input by exploiting the nature ofUGC and available metadata; additionally, alongwith relevance, we addressed challenges that arespecific to Question Answering, such as Cover-age and Novelty.
For an investigation of Coveragein the context of Search Engines, refer to Swami-nathan et al (2009).At the core of our work laid information trust-fulness, summarization techniques and alternativeconcept representation.
A general approach tothe broad problem of evaluating information cred-ibility on the Internet is presented by Akamineet al (2009) with a system that makes use ofsemantic-aware Natural Language Preprocessingtechniques.
With analogous goals, but a focuson UGC, are the papers of Stvilia et al (2005),Mcguinness et al (2006), Hu et al (2007) and767Zeng et al (2006), which present a thorough inves-tigation of Quality and trust in Wikipedia.
In thecQA domain, Jeon et al (2006) presents a frame-work to use Maximum Entropy for answer qualityestimation through non-textual features; with thesame purpose, more recent methods based on theexpertise of answerers are proposed by Suryantoet al (2009), while Wang et al (2009b) introducethe idea of ranking answers taking their relation toquestions in consideration.
The paper that we re-gard as most authoritative on the matter is the workby Agichtein et al (2008) which inspired us in thedesign of the Quality feature space presented inSection 2.1.Our approach merged trustfulness estimationand summarization techniques: we adapted the au-tomatic concept-level model presented by Gillickand Favre (2009) to our needs; related work inmulti-document summarization has been carriedout by Wang et al (2008) and McDonald (2007).A relevant selection of approaches that insteadmake use of ML techniques for query-biased sum-marization is the following: Wang et al (2007),Metzler and Kanungo (2008) and Li et al (2009).An aspect worth investigating is the use of par-tially labeled or totally unlabeled data for sum-marization in the work of Wong et al (2008) andAmini and Gallinari (2002).Our final contribution was to explore the use ofBasic Elements document representation insteadof the widely used n-gram paradigm: in this re-gard, we suggest the paper by Zhou et al (2006).6 ConclusionsWe presented a framework to generate trust-ful, complete, relevant and succinct answers toquestions posted by users in cQA portals.
Wemade use of intrinsically available metadata alongwith concept-level multi-document summariza-tion techniques.
Furthermore, we proposed anoriginal use for the BE representation of conceptsand tested two concept-scoring functions to com-bine Quality, Coverage, Relevance and Noveltymeasures.
Evaluation results on human annotateddata showed that our summarized answers consti-tute a solid complement to best answers voted bythe cQA users.We are in the process of building a system thatperforms on-line summarization of large sets ofquestions and answers from Yahoo!
Answers.Larger-scale evaluation of results against otherstate-of-the-art summarization systems is ongoing.AcknowledgmentsThis work was partly supported by the Chi-nese Natural Science Foundation under grant No.60803075, and was carried out with the aid ofa grant from the International Development Re-search Center, Ottawa, Canada.
We would like tothank Prof. Xiaoyan Zhu, Mr. Yang Tang and Mr.Guillermo Rodriguez for the valuable discussionsand comments and for their support.
We wouldalso like to thank Dr. Chin-yew Lin and Dr. Eu-gene Agichtein from Emory University for sharingtheir data.ReferencesEugene Agichtein, Carlos Castillo, Debora Donato,Aristides Gionis, and Gilad Mishne.
2008.
Find-ing high-quality content in social media.
In MarcNajork, Andrei Z. Broder, and Soumen Chakrabarti,editors, Proceedings of the International Conferenceon Web Search and Web Data Mining, WSDM 2008,Palo Alto, California, USA, February 11-12, 2008,pages 183?194.
ACM.Susumu Akamine, Daisuke Kawahara, YoshikiyoKato, Tetsuji Nakagawa, Kentaro Inui, Sadao Kuro-hashi, and Yutaka Kidawara.
2009.
Wisdom: a webinformation credibility analysis system.
In ACL-IJCNLP ?09: Proceedings of the ACL-IJCNLP 2009Software Demonstrations, pages 1?4, Morristown,NJ, USA.
Association for Computational Linguis-tics.Massih-Reza Amini and Patrick Gallinari.
2002.
Theuse of unlabeled data to improve supervised learningfor text summarization.
In SIGIR ?02: Proceedingsof the 25th annual international ACM SIGIR con-ference on Research and development in informa-tion retrieval, pages 105?112, New York, NY, USA.ACM.Dan Gillick and Benoit Favre.
2009.
A scalable globalmodel for summarization.
In ILP ?09: Proceedingsof the Workshop on Integer Linear Programming forNatural Langauge Processing, pages 10?18, Morris-town, NJ, USA.
Association for Computational Lin-guistics.Meiqun Hu, Ee-Peng Lim, Aixin Sun, Hady WirawanLauw, and Ba-Quy Vuong.
2007.
Measuring arti-cle quality in wikipedia: models and evaluation.
InCIKM ?07: Proceedings of the sixteenth ACM con-ference on Conference on information and knowl-edge management, pages 243?252, New York, NY,USA.
ACM.Jiwoon Jeon, W. Bruce Croft, Joon Ho Lee, and SoyeonPark.
2006.
A framework to predict the quality of768answers with non-textual features.
In SIGIR ?06:Proceedings of the 29th annual international ACMSIGIR conference on Research and development ininformation retrieval, pages 228?235, New York,NY, USA.
ACM.Liangda Li, Ke Zhou, Gui-Rong Xue, Hongyuan Zha,and Yong Yu.
2009.
Enhancing diversity, cover-age and balance for summarization through struc-ture learning.
In WWW ?09: Proceedings of the 18thinternational conference on World wide web, pages71?80, New York, NY, USA.
ACM.Yuanjie Liu, Shasha Li, Yunbo Cao, Chin-Yew Lin,Dingyi Han, and Yong Yu.
2008.
Understand-ing and summarizing answers in community-basedquestion answering services.
In Proceedings of the22nd International Conference on ComputationalLinguistics (Coling 2008), pages 497?504, Manch-ester, UK, August.
Coling 2008 Organizing Com-mittee.Ryan T. McDonald.
2007.
A study of global infer-ence algorithms in multi-document summarization.In Giambattista Amati, Claudio Carpineto, and Gio-vanni Romano, editors, ECIR, volume 4425 of Lec-ture Notes in Computer Science, pages 557?564.Springer.Deborah L. Mcguinness, Honglei Zeng, Paulo Pin-heiro Da Silva, Li Ding, Dhyanesh Narayanan, andMayukh Bhaowal.
2006.
Investigation into trust forcollaborative information repositories: A wikipediacase study.
In In Proceedings of the Workshop onModels of Trust for the Web, pages 3?131.Donald Metzler and Tapas Kanungo.
2008.
Ma-chine learned sentence selection strategies for query-biased summarization.
In Proceedings of SIGIRLearning to Rank Workshop.Besiki Stvilia, Michael B. Twidale, Linda C. Smith,and Les Gasser.
2005.
Assessing information qual-ity of a community-based encyclopedia.
In Proceed-ings of the International Conference on InformationQuality.Maggy Anastasia Suryanto, Ee Peng Lim, Aixin Sun,and Roger H. L. Chiang.
2009.
Quality-aware col-laborative question answering: methods and evalu-ation.
In WSDM ?09: Proceedings of the SecondACM International Conference on Web Search andData Mining, pages 142?151, New York, NY, USA.ACM.Ashwin Swaminathan, Cherian V. Mathew, and DarkoKirovski.
2009.
Essential pages.
In WI-IAT ?09:Proceedings of the 2009 IEEE/WIC/ACM Interna-tional Joint Conference on Web Intelligence and In-telligent Agent Technology, pages 173?182, Wash-ington, DC, USA.
IEEE Computer Society.Changhu Wang, Feng Jing, Lei Zhang, and Hong-Jiang Zhang.
2007.
Learning query-biased webpage summarization.
In CIKM ?07: Proceedings ofthe sixteenth ACM conference on Conference on in-formation and knowledge management, pages 555?562, New York, NY, USA.
ACM.Dingding Wang, Tao Li, Shenghuo Zhu, and ChrisDing.
2008.
Multi-document summarization viasentence-level semantic analysis and symmetric ma-trix factorization.
In SIGIR ?08: Proceedings of the31st annual international ACM SIGIR conference onResearch and development in information retrieval,pages 307?314, New York, NY, USA.
ACM.Kai Wang, Zhaoyan Ming, and Tat-Seng Chua.
2009a.A syntactic tree matching approach to finding sim-ilar questions in community-based qa services.
InSIGIR ?09: Proceedings of the 32nd internationalACM SIGIR conference on Research and develop-ment in information retrieval, pages 187?194, NewYork, NY, USA.
ACM.Xin-Jing Wang, Xudong Tu, Dan Feng, and Lei Zhang.2009b.
Ranking community answers by modelingquestion-answer relationships via analogical reason-ing.
In SIGIR ?09: Proceedings of the 32nd interna-tional ACM SIGIR conference on Research and de-velopment in information retrieval, pages 179?186,New York, NY, USA.
ACM.Kam-Fai Wong, Mingli Wu, and Wenjie Li.
2008.
Ex-tractive summarization using supervised and semi-supervised learning.
In COLING ?08: Proceedingsof the 22nd International Conference on Computa-tional Linguistics, pages 985?992, Morristown, NJ,USA.
Association for Computational Linguistics.Honglei Zeng, Maher A. Alhossaini, Li Ding, RichardFikes, and Deborah L. McGuinness.
2006.
Com-puting trust from revision history.
In PST ?06: Pro-ceedings of the 2006 International Conference onPrivacy, Security and Trust, pages 1?1, New York,NY, USA.
ACM.Liang Zhou, Chin Y. Lin, and Eduard Hovy.
2006.Summarizing answers for complicated questions.
InProceedings of the Fifth International Conferenceon Language Resources and Evaluation (LREC),Genoa, Italy.769
