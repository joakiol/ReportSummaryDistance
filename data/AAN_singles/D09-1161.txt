Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1552?1560,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPK-Best Combination of Syntactic ParsersHui Zhang1, 2   Min Zhang1   Chew Lim Tan2   Haizhou Li11Institute for Infocomm Research                 2National University of Singaporezhangh1982@gmail.com   {mzhang, hli}@i2r.a-star.edu.sg   tancl@comp.nus.edu.sgAbstractIn this paper, we propose a linear model-basedgeneral framework to combine k-best parseoutputs from multiple parsers.
The proposedframework leverages on the strengths of pre-vious system combination and re-rankingtechniques in parsing by integrating them intoa linear model.
As a result, it is able to fullyutilize both the logarithm of the probability ofeach k-best parse tree from each individualparser and any additional useful features.
Forfeature weight tuning, we compare the simu-lated-annealing algorithm and the perceptronalgorithm.
Our experiments are carried out onboth the Chinese and English Penn Treebanksyntactic parsing task by combining two state-of-the-art parsing models, a head-driven lexi-calized model and a latent-annotation-basedun-lexicalized model.
Experimental resultsshow that our F-Scores of 85.45 on Chineseand 92.62 on English outperform the previ-ously best-reported systems by 1.21 and 0.52,respectively.1 IntroductionStatistical models have achieved great success inlanguage parsing and obtained the state-of-the-art results in a variety of languages.
In general,they can be divided into two major categories,namely lexicalized models (Collins 1997, 1999;Charniak 1997, 2000) and un-lexicalized models(Klein and Manning 2003; Matsuzaki et al 2005;Petrov et al 2006; Petrov and Klein 2007).
Inlexicalized models, word information play a keyrole in modeling grammar rule generation, whileun-lexicalized models usually utilize latent in-formation derived from the parse structure diver-sity.
Although the two models are different fromeach other in essence, both have achieved state-of-the-art results in a variety of languages andare complementary to each other (this will beempirically verified later in this paper).
There-fore, it is natural to combine the two models forbetter parsing performance.Besides individual parsing models, many sys-tem combination methods for parsing have beenproposed (Henderson and Brill 1999; Zeman and?abokrtsk?
2005; Sagae and Lavie 2006) andpromising performance improvements have beenreported.
In addition, parsing re-ranking (Collins2000; Riezler et al 2002; Charniak and Johnson2005; Huang 2008) has also been shown to beanother effective technique to improve parsingperformance.
This technique utilizes a bunch oflinguistic features to re-rank the k-best (Huangand Chiang 2005) output on the forest level ortree level.
In prior work, system combinationwas applied on multiple parsers while re-rankingwas applied on the k-best outputs of individualparsers.In this paper, we propose a linear model-basedgeneral framework for multiple parsers combina-tion.
The proposed framework leverages on thestrengths of previous system combination and re-ranking methods and is open to any type of fea-tures.
In particular, it is capable of utilizing thelogarithm of the parse tree probability from eachindividual parser while previous combinationmethods are unable to use this feature since theprobabilities from different parsers are not com-parable.
In addition, we experiment on k-bestcombination while previous methods are onlyverified on 1-best combination.
Finally, we applyour method in combining outputs from both thelexicalized and un-lexicalized parsers while pre-vious methods only carry out experiments onmultiple lexicalized parsers.
We also comparetwo learning algorithms in tuning the featureweights for the linear model.We perform extensive experiments on theChinese and English Penn Treebank corpus.
Ex-perimental results show that our final results, anF-Score of 92.62 on English and 85.45 on Chi-nese, outperform the previously best-reportedsystems by 0.52 point and 1.21 point, respec-tively.
This convincingly demonstrates the effec-tiveness of our proposed framework.
Our studyalso shows that the simulated-annealing algo-rithm (Kirkpatrick et al 1983) is more effective1552than the perceptron algorithm (Collins 2002) forfeature weight tuning.The rest of this paper is organized as follows.Section 2 briefly reviews related work.
Section 3discusses our method while section 4 presentsthe feature weight tuning algorithm.
In Section 5,we report our experimental results and then con-clude in Section 6.2 Related WorkAs discussed in the previous section, systemcombination and re-ranking are two techniquesto improve parsing performance by post-processing parsers?
k-best outputs.Regarding the system combination study,Henderson and Brill (1999) propose two parsercombination schemes, one that selects an entiretree from one of the parsers, and one that builds anew tree by selecting constituents suggested bythe initial trees.
According to the second scheme,it breaks each parse tree into constituents, calcu-lates the count of each constituent, then appliesthe majority voting to decide which constituentwould appear in the final tree.
Sagae and Lavie(2006) improve this second scheme by introduc-ing a threshold for the constituent count, andsearch for the tree with the largest number ofcount from all the possible constituent combina-tion.
Zeman and ?abokrtsk?
(2005) study fourcombination techniques, including voting, stack-ing, unbalanced combining and switching, forconstituent selection on Czech dependency pars-ing.
Promising results have been reported in allthe above three prior work.
Henderson and Brill(1999) combine three parsers and obtained an F1score of 90.6, which is better than the score of88.6 obtained by the best individual parser asreported in their paper.
Sagae and Lavie (2006)combine 5 parsers to obtain a score of 92.1,while they report a score of 91.0 for the best sin-gle parser in their paper.
Finally, Zeman and?abokrtsk?
(2005) reports great improvementsover each individual parsers and show that aparser with very low accuracy can also help toimprove the performance of a highly accurateparser.
However, there are two major limitationsin these prior works.
First, only one-best outputfrom each individual parsers are utilized.
Second,none of these works uses the parse probability ofeach parse tree output from the individual parser.Regarding the parser re-ranking, Collins (2000)proposes a dozen of feature types to re-rank k-best outputs of a single head-driven parser.
Heuses these feature types to extract around half amillion different features on the training set, andthen examine two loss functions, MRF andBoosting, to do feature selection.
Charniak andJohnson (2005) generate a more accurate k-bestoutput and adopt MaxEnt method to estimate thefeature weights for more than one million fea-tures extracted from the training set.
Huang(2008) further improves the re-ranking work ofCharniak and Johnson (2005) by re-ranking onpacked forest, which could potentially incorpo-rate exponential number of k-best list.
The re-ranking techniques also achieve great improve-ment over the original individual parser.
Collins(2002) improves the F1 score from 88.2% to89.7%, while Charniak and Johnson (2005) im-prove from 90.3% to 91.4%.
This latter workwas then further improved by Huang (2008) to91.7%, by utilizing the benefit of forest structure.However, one of the limitations of these tech-niques is the huge number of features whichmakes the training very expensive and inefficientin space and memory usage.3 K-best Combination of Lexicalizedand Un-Lexicalized Parsers withModel ProbabilitiesIn this section, we first introduce our proposed k-best combination framework.
Then we apply thisframework to the combination of two state-of-the-art lexicalized and un-lexicalized parserswith an additional feature inspired by traditionalcombination techniques.3.1 K-best Combination FrameworkOur proposed framework consists of the follow-ing steps:1) Given an input sentence and N differentparsers, each parser generates K-best parsetrees.2) We combine the N*K output trees andremove any duplicates to obtain M uniquetress.3) For each of the M unique trees, we re-evaluate it with all the N models which areused by the N parsers.
It is worth notingthat this is the key point (i.e.
one of themajor advantages) of our method sincesome parse trees are only generated fromone or I (I<N) parsers.
For example, if atree is only generated from head-drivenlexicalized model, then it only has thehead-driven model score.
Now we re-evaluate it with the latent-annotation un-lexicalized model to reflect the latent-1553annotation model?s confidence for thistree.
This enables our method to effec-tively utilize the confidence measure of allthe individual models without any bias.Without this re-evaluation step, the previ-ous combination methods are unable toutilize the various model scores.4) Besides model scores, we also computesome additional feature scores for eachtree, such as the widely-used ?constituentcount?
feature.5) Then we adopt the linear model to balanceand combine these feature scores and gen-erate an overall score for each parse tree.6) Finally we re-rank the M best trees andoutput the one with the highest score.?
?
?
?
?
??
?The above is the linear function used in ourmethod, where t is the tree to be evaluated,  toare the model confidence scores (in this paper,we use logarithm of the parse tree probability)from the N models,  to  are their weights,?
to ?
are the L additional features, ?
to ?are their weights.In this paper, we employ two individual pars-ing model scores and only one additional feature.Let  be the head-driven model score,  be thelatent-annotation model score, ?
be the consti-tuent count feature and ?
is the weight of fea-ture ?
.3.2 Confidences of Lexicalized and Un-lexicalized ModelThe term ?confidence?
was used in prior parsercombination studies to refer to the accuracy ofeach individual parser.
This reflects how muchwe can trust the parse output of each parser.
Inthis paper, we use the term ?confidence?
to referto the logarithm of the tree probability computedby each model, which is a direct measurement ofthe model?s confidence on the target tree beingthe best or correct parse output.
In fact, the fea-ture weight ?
in our linear model functions simi-larly as the traditional ?confidence?.
However,we do not directly use parser?s accuracy as itsvalue.
Instead we tune it automatically on devel-opment set to optimize it against the parsing per-formance directly.
In the following, we introducethe state-of-the-art head-driven lexicalized andlatent-annotation un-lexicalized models (whichare used as two individual models in this paper),and describe how they compute the tree probabil-ity briefly.Head-driven model is one of the most repre-sentative lexicalized models.
It attaches the headword to each non-terminal and views the genera-tion of each rule as a Markov process first fromfather to head child, and then to the head child?sleft and right siblings.Take following rule r as example,is the rule?s left hand side (i.e.
father label),is the head child,  is M?s left sibling andis M?s right sibling.
Let h be M?s head word, theprobability of this rule isThe probability of a tree is just the product of theprobabilities of all the rules in it.
The above isthe general framework of head-driven model.
Fora specific model, there may be some additionalfeatures and modification.
For example, themodel2 in Collins (1999) introduces sub-categorization and model3 introduces gap as ad-ditional features.
Charniak (2000)?s model intro-duces pre-terminal as additional features.The latent-annotation model (Matsuzaki et al2005; Petrov et al 2006) is one of the most ef-fective un-lexicalized models.
Briefly speaking,latent-annotation model views each non-terminalin the Treebank as a non-terminal followed by aset of latent variables, and uses EM algorithms toautomatically learn the latent variables?
probabil-ity functions to maximize the probability of thegiven training data.
Take the following binarizedrule as example,could be viewed as the set of rulesThe process of computing the probability of anormal tree is to first binarized all the rules in it,and then replace each rule to the correspondingset of rules with latent variables.
Now the pre-vious tree becomes a packed forest (Klein andManning 2001; Petrov et al 2007) in the latent-annotation model, and its probability is the insideprobability of the root node.
This model is quitedifferent from the head-driven model in which1554the probability of a tree is just the product all therules?
probability.3.3 Constituent CountsBesides the two model scores, we also adoptconstituent count as an additional feature in-spired by (Henderson and Brill 1999) and (Sagaeand Lavie 2006).
A constituent is a non-terminalnode covering a special span.
For example,?NP[2,4]?
means a constituent labelled as ?NP?which covers the span from the second word tothe fourth word.
If we have 100 trees and NP[2,4]appears in 60 of them, then its constituent countis 60.
For each tree, its constituent count is thesum of all the counts of its constituent.
However,as suggested in (Sagae and Lavie 2006), this fea-ture favours precision over recall.
To solve thisissue, Sagae and Lavie (2006) use a threshold tobalance them.
For any constituent, we calculateits count if and only if it appears more than Xtimes in the k-best trees; otherwise we set it as 0.In this paper, we normalize this feature by divid-ing the constituent count by the number of k-best.Note that the threshold value and the additionalfeature value are not independent.
Once thethreshold changes, the feature value has to be re-calculated.In conclusion, we have four parameters to es-timate: two model score weights, one additionalfeature weight and a threshold for the additionalfeature.4 Parameter EstimationWe adopt the minimum error rate principle totune the feature weights by minimizing the errorrate (i.e.
maximizing the F1 score) on the devel-opment set.
In our study, we implement andcompare two algorithms, the simulated-annealingstyle algorithm and the average perceptron algo-rithm.4.1 Simulated AnnealingSimulated-annealing algorithm has been provedto be a powerful and efficient algorithm in solv-ing NP problem (?ern?
1985).
Fig 1 is the pseu-do code of the simulated-annealing algorithmthat we apply.In a single iteration (line 4-11), the simulatedalgorithm selects some random points (the Mar-kov link) for hill climbing.
However, it acceptssome bad points with a threshold probabilitycontrolled by the annealing temperature (line 7-10).
The hill climbing nature gives this algorithmthe ability of converging at local maximal pointand the random nature offers it the chance tojump from some local maximal points to globalmaximal point.
We do a slight modification tosave the best parameter so far across all the fi-nished iterations and let it be the initial point forupcoming iterations (line 12-17).RandomNeighbour(p) is the function to gener-ate a random neighbor for the p (the four-tupleparameter to be estimated).
F1(p) is the functionto calculate the F1 score over the entire test set.Given a fixed parameter p, it selects the candi-date tree with best score for each sentence andcomputes the F1 score with the PARSEVAL me-trics.Pseudo code 1.
Simulated-annealing algorithmInput: k-best trees combined from two model outputNotation:p: the current parameter valueF1(p): the F1 score with the parameter value pTMF: the max F1 score of each iterationTMp: the optimal parameter value during iterationMaxF1: the max F1 score on dev setRp: the parameter value which maximizes the F1 scoreof the dev setT: annealing temperatureL: length of Markov linkOutput: Rp1.
MaxF1:= 0, Rp:= (0,0,0,0), T:=1, L=100 // initialize2.
Repeat                                                       // iteration3.
TMp :=Rp4.
for  i := 1 to L  do5.
p := RandomNeighbour(TMp)6.            d= F1(p)- TMF7.
if d>0 or exp(d/T) > random[0,1) then8.
TMF:=F1(p)9.
TMp:=p10.
end if11.
end for12.
if TMF > MaxF1 then13.
MaxF:=TMF14.
Rp:=TMp15.
else16.
TMp:=Rp17.
end if18.
T=T*0.919.
Until convergenceFig 1.
Simulated Annealing Algorithm4.2 Averaged PerceptronAnother algorithm we apply is the averaged per-ceptron algorithm.
Fig 2 is the pseudo code ofthis algorithm.
Averaged perceptron is an onlinealgorithm.
It iterates through each instance.
Ineach instance, it selects the candidate answerwith the maximum function score.
Then it up-dates the weight by the margin of feature valuebetween the select answer and the oracle answer(line 5-9).
After each iteration, it does average togenerate a new weight (line 10).
The averaged1555perceptron has a solid theoretical fundamentaland was proved to be effective across a variety ofNLP tasks (Collins 2002).However, it needs a slightly modification toadapt to our problem.
Since the threshold and theconstituent count are not independent, they arenot linear separable.
In this case, the perceptronalgorithm cannot be guaranteed to converge.
Tosolve this issue, we introduce an outer loop (line2) to iterate through the value range of thresholdwith a fixed step length and in the inner loop weuse perceptron to estimate the other three para-meters.
Finally we select the final parameterwhich has maximum F1 score across all the itera-tion (line 14-17).Pseudo code 2.
Averaged perceptron algorithmInput: k-best trees combined from two model outputNotation:MaxF1, Rp: already defined in pseudo code 1T: the max number of iterationsI: the number of instancesThreshold: the threshold for constituent countw: the three feature weights other than threshold?
: the candidate tree with max function score given afixed weight w?
: the candidate tree with the max F1 score (since theoracle tree may not appeared in our candidate set,we choose this one as the pseudo orcale tree): the set of candidate tree for ith sentenceOutput: Rp1.
MaxF1:=0, T=302.
for  Threshold :=0 to 1 with step 0.01 do3.
Initialize w4.
for iter : 1 to T do5.
for  i := 1 to I  do6.
?
?????????7.
?
?8.
?
:= w9.
end for10.
?
??I???I11.
if converged  then break12.
end for13.
p := (Threshold, w)14.     if F1(p) > MaxF1 then15.
MaxF1 := F1(p)16.
Rp:=p17.
end if18.
end forFig 2.
Averaged Perceptron Algorithm5 ExperimentsWe evaluate our method on both Chinese andEnglish syntactic parsing task with the standarddivision on Chinese Penn Treebank Version 5.0and WSJ English Treebank 3.0 (Marcus et al1993) as shown in Table 1.We use Satoshi Sekine and Michael Collins?EVALB script modified by David Ellis for accu-racy evaluation.
We use Charniak?s parser(Charniak 2000) and Berkeley?s parser (Petrovand Klein 2007) as the two individual parsers,where Charniak?s parser represents the best per-formance of the lexicalized model and the Berke-ley?s parser represents the best performance ofthe un-lexicalized model.
We retrain both ofthem according to the division in Table.
1.
Thenumber of EM iteration process for Berkeley?sparser is set to 5 on English and 6 on Chinese.Both the Charniak?s parser and Berkeley?s parserprovide function to evaluate an input parse tree?sprobability and output the logarithm of the prob-ability.Div.Lang.
Train Dev TestEnglish Sec.02-21 Sec.
22 Sec.
23ChineseArt.001-270,400-1151Art.301-325Art.271-300Table 1.
Data division5.1 Effectiveness of our Combination Me-thodThis sub-section examines the effectiveness ofour proposed methods.
The experiment is set upas follows: 1) for each sentence in the dev andtest sets, we generate 50-best from Charniak?sparser (Charniak 2000) and Berkeley?s parser(Petrov and Klein 2007), respectively; 2) the two50-best trees are merged together and duplicationwas removed; 3) we tune the parameters on thedev set and test on the test set.
(Without specificstatement, we use simulated-annealing as defaultweight tuning algorithm.
)The results are shown in Table 2 and Table 3.?P?
means precision, ?R?
means recall and ?F?
isthe F1-measure (all is in % percentage metrics);?Charniak?
represents the parser of (Charniak2000), ?Berkeley?
represents the parser of (Pe-trov and Klein 2007), ?Comb.?
represents thecombination of the two parsers.parseraccuracy Charniak Berkeley Comb.<=40wordsP 85.20 86.65 90.44R 83.70 84.18 85.96F 84.44 85.40 88.15AllP 82.07 84.63 87.76R 79.66 81.69 83.27F 80.85 83.13 85.45Table 2.
Results on Chinese1556parseraccuracy Charniak Berkeley Comb.<=40wordsP 90.45 90.27 92.36R 90.14 89.76 91.42F 90.30 90.02 91.89AllP 89.86 89.77 91.89R 89.53 89.26 90.97F 89.70 89.51 91.43Table 3.
Results on EnglishFrom Table 2 and Table 3, we can see our me-thod outperforms the single systems in all testcases with all the three evaluation metrics.
Usingthe entire Chinese test set, our method improvesthe performance by 2.3 (85.45-83.13) point inF1-Score, representing 13.8% error rate reduc-tion.
Using the entire English test set, our methodimproves the performance by 1.7 (91.43-89.70)point in F1-Score, representing 16.5% error ratereduction.
These improvements convincinglydemonstrate the effectiveness of our method.5.2 Effectiveness of KFig 3 and Fig.
4 show the relationship betweenF1 score and the number of K-best used whendoing combination on Chinese and English re-spectively.From Fig 3 and Fig.
4, we could see that theF1 score first increases with the increasing of K(there are some vibration points, this may due tostatistical noise) and reach the peak when K isaround 30-50, then it starts to drop.
It shows thatk-best list did provide more information thanone-best and thus can help improve the accuracy;however more k-best list may also contain morenoises and these noises may hurt the final com-bination quality.Fig 3.
F1-measure vs. K on ChineseFig 4.
F1-measure vs. K on English5.3 Diversity on the K-best Output of theHead-driven and Latent-annotation-driven ModelIn this subsection, we examine how different ofthe 50-best trees generated from Charnriak?sparser (head-driven model) (Charnriak, 2000)and Berkeley?s parser (latent-annotation model)(Petrov and Klein, 2007).Table 4 reports the statistics on the 50-bestoutput for Chinese and English test set.
Since forsome short sentences the parser cannot generateup to 50 best trees, the average number of trees isless than 50 for each sentence.
Each cell reportsthe total number of trees generated over the en-tire test set followed by the average count foreach sentence in bracket.
?Total?
means simplycombine the number of trees from the two pars-ers while ?Unique?
means the number after re-moving the duplicated trees for each sentence.
Inthe last row, we report the averaged redundantrate for each sentence, which is derived by divid-ing the figures in the row ?Duplicated?
by thosein the row ?Total?.Chinese EnglishCharniak 14577 (41.9) 120438 (49.9)Berkeley 14524 (41.7) 114299 (47.3)Total 29101 (83.6) 234737 (97.2)Unique 27747 (79.7) 221633 (91.7)Duplicated 1354 (3.9) 13104 (5.4)Redundant rate 4.65% 5.58%Table 4.
The statistics on the 50-best out-put for Chinese and English test set.The small redundant rate clearly suggests thatthe two parsing models are quite different andare complementary to each other.1557parserOracle Charniak Berkeley Comb.ChineseP 88.95 90.07 92.45R 86.51 87.12 89.67F 87.71 88.57 91.03EnglishP 97.06 95.86 98.10R 96.57 95.53 97.68F 96.82 95.70 97.89Table 5.
The oracle over 50-best output for in-dividual parser and our methodThe k-best oracle score is the upper bound ofthe quality of the k-best trees.
Table 5 reports theoracle score for the 50-best of the two individualparsers and our method.
Similar to Table 4, Ta-ble 5 shows again that the two models are com-plementary to each other and our method is ableto take the strength of the two models.5.4 Effectiveness of Model ConfidenceOne of the advantages of our method that weclaim is that we can utilize the feature of themodel confidence score (logarithm of the parsetree probability).Table 6 shows that all the three features con-tribute to the final accuracy improvement.
Evenif we only use the ?B+C?
confidence scores, italso outperforms the baseline individual parser(as reported in Table 2 and Table 3) greatly.
Allthese together clearly verify the effective of themodel confidence feature and our method caneffectively utilize this feature.Feat.Lang    I B+C B+C+IChinese 82.34 84.67 85.45English 90.20 91.02 91.43Table 6.
F1 score on 50-best combination withdifferent feature configuration.
?I?
means theconstituent count, ?B?
means Berkeley parserconfidence score and ?C?
means Charniak parserconfidence score.5.5 Comparison of the Weight Tuning Al-gorithmsIn this sub-section, we compare the two weighttuning algorithms on 50-best combination taskson both Chinese and English.
Dan Bikel?s ran-domized parsing evaluation comparator (Bikel2004) was used to do significant test on precisionand recall metrics.
The results are shown in Ta-ble 7.We can see, simulated annealing outperformsthe averaged perceptron significantly in bothprecision (p<0.005) and recall (p<0.05) metricsof Chinese task and precision (p<0.005) metricof English task.
Though averaged perceptron gotslightly better recall score on English task, it isnot significant according to the p-value (p>0.2).From table 8, we could see the simulated an-nealing algorithm is around 2-4 times slowerthan averaged perceptron algorithm.Algo.Lang SA.
AP.
P-valueChineseP 87.76 86.85 0.003R 83.27 82.90 0.030EnglishP 91.89 91.72 0.004R 90.97 91.02 0.205Table 7.
Precision and Recall score on 50-bestcombination by the two parameter estimationalgorithms with significant test; ?SA.?
is simu-lated annealing, ?AP.?
is averaged perceptron,?P-value?
is the significant test p-value.Algo.LangSimulatedAnnealingAveragedPerceptronChinese 2.3 0.6English 12 6Table 8.
Time taken (in minutes) on 50-bestcombination of the two parameter estimationalgorithms5.6 Performance-Enhanced IndividualParsers on EnglishFor Charniak?s lexicalized parser, there are twotechniques to improve its performance.
One is re-ranking as explained in section 2.
The other isthe self-training (McClosky et al 2006) whichfirst parses and reranks the NANC corpus, andthen use them as additional training data to re-train the model.
In this sub-section, we apply ourmethod to combine the Berkeley parser and theenhanced Charniak parser by using the newmodel confidence score output from the en-hanced Charniak parser.Table 9 and Table 10 show that the Charniakparser enhanced by re-ranking and self-trainingis able to help to further improve the perfor-mance of our method.
This is because that theenhanced Charniak parser provides more accu-rate model confidence score.1558parseraccuracy reranking Comb.
baseline<=40wordsP 92.34 93.41 92.36R 91.61 92.15 91.42F 91.97 92.77 91.89AllP 91.78 92.92 91.89R 91.03 91.70 90.97F 91.40 92.30 91.43Table 9.
Performance with Charniak parserenhanced by re-ranking; ?baseline?
is the per-formance of the combination of Table 3.parseraccuracyself-train+reranking Comb.
baseline<=40wordsP 92.87 93.69 92.36R 92.12 92.44 91.42F 92.49 93.06 91.89AllP 92.41 93.25 91.89R 91.64 92.00 90.97F 92.02 92.62 91.43Table 10.
Performance with Charniak parserenhanced by re-ranking plus self-training5.7 Comparison with Other State-of-the-artResultsTable 11 and table 12 compare our method withthe other state-of-the-art methods; we use I, B, R,S and C to denote individual model (Charniak2000; Collins 2000; Bod 2003; Petrov and Klein2007), bilingual-constrained model (Burkett andKlein 2008)1, re-ranking model (Charniak andJohnson 2005, Huang 2008), self-training model(David McClosky 2006) and combination model(Sagae and Lavie 2006) respectively.
The twotables clearly show that our method advance thestate-of-the-art results on both Chinese and Eng-lish syntax parsing.System  F1-MeasureICharniak (2000) 80.85Petrov and Klein (2007) 83.13B Burkett and Klein (2008)1 84.24C Our method 85.45Table 11.
Accuracy comparison on Chinese1 Burkett and Klein (2008) use the additional know-ledge from Chinese-English parallel Treebank to im-prove Chinese parsing accuracy.System  F1-MeasureIPetrov and Klein (2007) 89.5Charniak (2000) 89.7Bod (2003) 90.7RCollins (2000) 89.7Charniak and Johnson (2005) 91.4Huang (2008) 91.7S David McClosky (2006) 92.1CSagae and Lavie (2006) 92.1Our method 92.6Table 12.
Accuracy comparison on English.6 ConclusionsIn this paper2, we propose a linear model-basedgeneral framework for multiple parser combina-tion.
Compared with previous methods, our me-thod is able to use diverse features, includinglogarithm of the parse tree probability calculatedby the individual systems.
We verify our methodby combining the two representative parsingmodels, lexicalized model and un-lexicalizedmodel, on both Chinese and English.
Experimen-tal results show our method is very effective andadvance the state-of-the-art results on both Chi-nese and English syntax parsing.
In the future,we will explore more features and study the for-est-based combination methods for syntacticparsing.AcknowledgementWe would like to thank Prof. Hwee Tou Ng forhis help and support; Prof. Charniak for his sug-gestion on doing the experiments with the self-trained parser and David McCloksy for his helpon the self-trained model; Yee Seng Chan andthe anonymous reviewers for their valuablecomments.ReferencesDan  Bikel.
2004.
On the Parameter Space of Genera-tive Lexicalized Statistical Parsing Models.
Ph.D.Thesis, University of Pennsylvania 2004.Rens Bod.
2003.
An efficient implementation of a newDOP model.
EACL-04.David Burkett and Dan Klein.
2008.
Two Languagesare Better than One (for Syntactic Parsing).EMNLP-08.The corresponding authors of this paper are HuiZhang (zhangh1982@gmail.com) and Min Zhang(mzhang@i2r.a-star.edu.sg)1559V ?ern?
1985.
Thermodynamical approach to thetravelling salesman problem: an efficient simula-tion algorithm.
Journal of Optimization Theory andApplications, 45:41-51.1985.Eugene Charniak.
1997.
Statistical parsing with acontext-free grammar and word statistics.
AAAI-97, pages 598-603.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
NAACL-2000.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine-grained n-best parsing and discriminativereranking.
ACL-05, Pages 173-180.Michael Collins.
1997.
Three generative, lexicalisedmodels for statistical parsing.
ACL-97, pages 16-23.Michael Collins.1999.
Head-drivenstatistical modelsfor natural language parsing.
Doctoral Disserta-tion, Dept.
of Computer and Information Science,University of Pennsylvania, Philadelphia 1999.Michael Collins.
2000.
Discriminative reranking fornatural language parsing.
ICML-00, pages 175-182.Michael Collins.
2002.
Discriminative training me-thods for hidden markov models: Theory and expe-riments with perceptron algorithms.
EMNLP-02.Liang Huang.
2008.
Forest Reranking: DiscriminativeParsing with Non-Local Features.
ACL-HLT-08,pages 586-594.Liang Huang and David Chiang.
2005.
Better k-bestParsing.
IWPT-05.S.
Kirkpatrick, C. D. Gelatt, Jr. and M. P. Vecchi.1983.
Optimization by Simulated Annealing.Science.
New Series 220 (4598): 671-680.Dan Klein and Christopher D. Manning.
2001.
Pars-ing and Hypergraphs.
IWPT-01.Dan Klein and Christopher D. Manning.
2003.
Accu-rate unlexicalized parsing.
ACL-03, pages 423-430.John Henderson and Eric Brill.
1999.
Exploiting di-versity in natural language processing: combiningparsers.
EMNLP-99.Mitchell P. Marcus, Beatrice Santorini and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of English: the Penn Treebank.
Computa-tional Linguistics, 19:313-330.Takuya Matsuzaki.
Yusuke Miyao and Jun'ichi Tsujii.2005.
Probabilistic CFG with latent annotations.ACL-05, pages 75-82.David McClosky, Eugene Charniak and Mark John-son.
2006.
Effective self-training for parsing.NAACL-06, pages 152-159.Slav Petrov, Leon Barrett, Romain Thibaux and DanKlein.
2006.
Learning accurate, compact, and in-terpretable tree annotation.
COLING-ACL-06,pages 443-440.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
HLT-NAACL-07, pages401-411.Stefan Riezler, Tracy H. King, Ronald M. Kaplan,Richard Crouch, John T. III Maxwell and MarkJohnson.
2002.
Parsing the wall street journal us-ing a lexical-functional grammar and discrimina-tive estimation techniques.
ACL-02, pages 271?278.Kenji Sagae and Alon Lavie.
2006.
Parser combina-tion by reparsing.
HLT-NAACL-06, pages 129-132.Daniel Zeman and Zden?k ?abokrtsk?.
ImprovingParsing Accuracy by Combining Diverse Depen-dency Parsers.
IWPT-05.1560
