Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 222?230,Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational LinguisticsTerminology Extraction Approaches forProduct Aspect Detection in Customer ReviewsJu?rgen Bro?Institute of Computer ScienceFreie Universita?t Berlin14195 Berlin, Germanyjuergen.bross@fu-berlin.deHeiko EhrigNeofonie GmbHRobert-Koch-Platz 410115 Berlin, Germanyheiko.ehrig@neofonie.deAbstractIn this paper, we address the problem ofidentifying relevant product aspects in acollection of online customer reviews.
Be-ing able to detect such aspects representsan important subtask of aspect-based re-view mining systems, which aim at auto-matically generating structured summariesof customer opinions.
We cast the task asa terminology extraction problem and ex-amine the utility of varying term acquisi-tion heuristics, filtering techniques, vari-ant aggregation methods, and relevancemeasures.
We evaluate the different ap-proaches on two distinct datasets (hoteland camera reviews).
For the best config-uration, we find significant improvementsover a state-of-the-art baseline method.1 IntroductionIdentifying significant terms in a text corpus con-stitutes a core task in natural language process-ing.
Fields of application are for example glos-sary extraction (Kozakov et al 2004) or ontologylearning (Navigli and Velardi, 2004).
In this work,we particularly focus on the application scenarioof aspect-based customer review mining (Hu andLiu, 2004; Dave et al 2003).
It is best describedas a sentiment analysis task, where the goal isto summarize the opinions expressed in customerreviews.
Typically, the problem is decomposedinto three subtasks: 1) identify mentions of rele-vant product aspects, 2) identify sentiment expres-sions and determine their polarity, and 3) aggre-gate the sentiments for each aspect.
In this paper,we only consider the first subtask, i.e., finding rel-evant product aspects in reviews.More precisely, we define the problem settingas follows: Input is a homogeneous collection ofcustomer reviews, i.e., all reviews refer to a sin-gle product type (e.g., digital cameras or hotels).The goal is to automatically derive a lexicon of themost relevant aspects related to the product type.For example, given a set of hotel reviews, we wantto determine aspects such as ?room size?, ?frontdesk staff?
?sleep quality?, and so on.
In gen-eral, product aspects may occur as nominal (e.g.,?image stabilization?
), named (e.g., ?SteadyShotfeature?
), pronominal (e.g., ?it?
), or implicit men-tions (e.g., ?reduction of blurring from camerashake?).
We explicitly restrict the task to findingnominal aspect mentions1.The contribution of this paper is to explicitlycast the problem setting as a terminology extrac-tion (TE) task and to examine the utility of meth-ods that have been proven beneficial in this con-text.
Most related work does not consider thisclose relationship and rather presents ad-hoc ap-proaches.
Our main contributions are as follows:?
We experiment with varying term acquisitionmethods, propose a set of new term filtering ap-proaches, and consider variant aggregation tech-niques typically applied in TE systems.?
We compare the utility of different term rel-evance measures and experiment with combina-tions of these measures.?
We propose and assess a new method that fil-ters erroneous modifiers (adjectives) in term can-didates.
Our method exploits information obtainedfrom pros/cons summaries of customer reviews.?
Our best configuration improves over a state-of-the-art baseline by up to 7 percentage points.The remainder of the paper is organized as fol-lows: In Section 2, we cover related work, settingfocus on unsupervised approaches.
Section 3 de-scribes the TE methods we examine in this study.Section 4 introduces our evaluation datasets andSection 5 presents experiments and results.
Wesummarize and conclude in Section 6.1Nominal mentions account for over 80% of all mentionsin our datasets.
Also in other corpora, the ratio is quite simi-lar, e.g., (Kessler et al 2010).222product aspectdetection in reviewsmulti-class classification(supervised) topic modeling(unsupervised) sequence labeling(supervised) lexicon-based(unsupervised)sentence level mention leveltext categorization information extractioncan implementfocus ofthis paperFigure 1: Conceptual overview of related work inproduct aspect detection.2 Related WorkFigure 1 provides a conceptual overview of differ-ent tasks and approaches in the research area.
Ba-sically, we differentiate related work by the granu-larity of analysis, distinguishing between sentencelevel and mention level analysis.
While at the sen-tence level, the goal is to decide whether a givensentence refers to one or more predefined aspects,fine-grained mention level analysis aims at discov-ering each individual mention of a relevant prod-uct aspect (e.g., ?The image stabilization workswell, but I didn?t like the poor battery life.?
).We address aspect detection at the mentionlevel and our methods fall into the category of (un-supervised) lexicon-based approaches.
In con-trast to supervised methods, lexicon-based ap-proaches do not rely on labeled training data andthus scale better across domains2.
The commonapproach is to crawl a corpus of reviews and toapply frequency-based methods to extract a lex-icon of product aspects from the dataset.
Ap-proaches differ in the way corpus statistics arecomputed and to which extent linguistic featuresare exploited.
Section 2.1 briefly describes themost relevant previous works and Section 2.2 pro-vides an assessment of the different approaches.2.1 Creating Product Aspect LexiconsHu and Liu (2004) cast the problem as a frequentitemset mining task and apply the well-knownApriori algorithm (Agrawal and Srikant, 1994).Inherent drawbacks of this approach3 are heuris-tically treated in a post-processing step.Whereas Hu and Liu?s method exclusively ex-amines documents of the input collection, Popescuand Etzioni (2005) propose to incorporate the Web2For instance, (Jakob and Gurevych, 2010) report that F-scores for their sequence labeling method decrease by up to25 percentage points in cross domain settings.3The word order is not recognized and sub-terms of termsare not necessarily valid terms in natural language.as a corpus.
They assess a term candidate?s do-main relevance by computing the pointwise mu-tual information (PMI) (Zernik, 1991) betweenthe candidate term and some predefined phrasesthat are associated with the product type.
The PMIscore is used to prune term candidates.A further approach is to utilize a contrastivebackground corpus to determine the domain rel-evance of terms.
For instance, Yi et al(2003) usethe likelihood ratio test (LRT) to compute a confi-dence value that a term candidate originates fromthe relevant review corpus.
The computed score isused to rank term candidates.
Also Scaffidi et al(2007) follow the basic idea of using a contrastivecorpus, but simply compare relative frequencyratios instead of computing a confidence value.Other exemplary works consider the utility of sta-tistical language models (Wu et al 2009), pro-pose latent semantic analysis (Guo et al 2009),or examine a double propagation approach thatleverages the correlation between product aspectsand sentiment bearing words (Zhang et al 2010).Product aspect lexicons may also be created man-ually, e.g., Carenini et al(2005) or Bloom et al(2007) follow this approach.
Naturally, a manualapproach does not scale well across domains.2.2 Assessment of Lexicon-Based ApproachesOur goal in this section is to select a state-of-theart method that we can use as a baseline in ourexperiments.
Unfortunately, it is quite difficult toassess the relative performance of the different ap-proaches as the evaluation datasets and method-ologies often vary.
Popescu and Etzioni (2005)compare their results to the method by Hu andLiu (2004) and report significantly improved re-sults.
However, their method relies on the private?Know-it-all?
information extraction system andis therefore not suited as a baseline.
Scaffidi et al(2007) only assess the precision of the extractedaspect lexicon.
Their methodology does not al-low to measure recall, which renders their compar-ison to Hu?s method rather useless4.
Furthermore,the results are quite questionable as the number ofextracted aspects is extremely small (8-12 aspectscompared to around thousand with our approach).Also Yi et al(2003) only report results of an in-trinsic evaluation for their LRT-approach.
A sys-tematic comparison of Hu?s frequent itemset min-4Without considering recall, the precision can easily betweaked by adjusting threshold values.223candidateacquisitionand countingcandidatefilteringvariantaggregationcandidate rankingand selection termdictionarydocumentcollection linguisticpre-processingmanual revisionFigure 2: Pipeline architecture of a TE system.ing and Yi?s LRT-approach is conducted by Jakob(2011).
His results show that ?the Likelihood Ra-tio Test based approach generally yielded betterresults?.
In the absence of other valid compara-tive studies, we therefore select the LRT-approachas a baseline method for our experiments.3 Terminology Extraction for ProductAspect DetectionA typical TE system follows the pipeline archi-tecture depicted in Figure 2.
Depending on thespecific application domain, the implementationof the individual pipeline steps may differ widely.For example, we will see in the next section thatthe examined acquisition and filtering methods arehighly tailored to the domain of customer reviews.In contrast, the underlying concepts for the defi-nition of term relevance are applicable across do-mains.
From the multitude of statistical measuresproposed in the literature5, we can distill mainlythree underlying concepts: (1) contrastive domainrelevance, (2) intra domain relevance, and (3)term cohesion.
We will experiment with measuresfor all of the three concepts.
The following subsec-tions describe how we implement the individualsteps of the extraction pipeline (for the majority ofsteps, we propose several alternative approaches,which will be subject to experimentation).3.1 Linguistic PreprocessingWe preprocess all text documents by means of apart-of-speech tagger6 (which also performs tok-enization, sentence splitting, and lemmatization).All tokens are further normalized by case folding.3.2 Candidate AcquisitionThe candidate acquisition component initially de-cides which phrases are further considered and5For example, consult (Kageura and Umino, 1996) for athorough literature survey on terminology extraction.6http://nlp.stanford.edu/software/corenlp.shtmlwhich are directly discarded.
Defining too restric-tive filters may lower the recall, whereas too un-constrained filters may decrease the precision.Part-of-Speech Tag Filter We experiment withtwo POS-tag filters: BNP1 and BNP2.
As a base-line (BNP1), we use the ?base noun phrase pat-tern?
proposed in (Yi et al 2003):BNP1 := NN |NN NN |JJ NN |NN NN NN |JJ NN NN |JJ JJ NNIt restricts candidates to a maximum length ofthree words (adjectives or nouns), where adjec-tives must only occur as pre-modifiers to nouns.As an alternative, we examine the utility of a morerelaxed pattern (BNP2).
This pattern matchesterms of arbitrary length, also allows for pluralforms, and matches proper nouns (identified by thetags NNP or NNPS):BNP2 := (JJ )*(NN\w{0,2} )+Domain Specific Heuristics Acquisitionheuristics put further constraints on the validity ofterm candidates.
As a baseline, we consider twoheuristics proposed in (Yi et al 2003):?
The definite base noun phrase (DBNP) heuristicrestricts the BNPs to phrases that are preceded bythe definite article ?the?.?
The beginning definite base noun phrase(BBNP) heuristic restricts valid candidates toDBNPs that occur at the beginning of a sentence,followed by a verb phrase (e.g., ?The picturequality is great.?
).As an alternative, we propose two other heuris-tics.
Both are based on the hypothesis that the oc-currence of sentiment expressions in the context ofa candidate is a good indicator for the candidate?svalidity.
Sentiment expressions are detected witha small hand-crafted sentiment lexicon composedof 520 strongly positive/negative adjectives.
Weexperiment with two different strategies:?
The sentiment bearing sentence (SBS) heuris-tic only considers candidates that occur in sen-tences where at least one sentiment expression isdetected.?
The sentiment bearing pattern (SBP) heuristicdefines a set of four simple syntactic patterns thatrelate candidate terms to sentiment expressions.Only candidates that match one of the patterns arefurther considered.3.3 Candidate FilteringAlthough the candidate acquisition heuristics fo-cus on high precision, they generate a consider-224able number of irrelevant candidates.
These canbe pruned by further domain specific filters:Review Stop Word Filter We compile a listof review specific stop words and discard eachcandidate term that contains at least one of thewords.
The list (176 entries) has been con-structed based on observations on a developmentdataset and by (intelligent) extrapolation of thesefindings.
Roughly categorized, it includes sen-timent bearing nouns (e.g., ?complaint?
), reviewrelated terms (e.g., ?bottom line?
), purchase re-lated phrases (e.g., ?delivery?
), mentions of per-sons (e.g., ?wife?
), and phrases of reasoning (e.g.,?decision?
).Pre-Modifier Filter Both presented part-of-speech filters (BNP1/2) allow nouns to be mod-ified by multiple adjectives.
Unfortunately, thisleads to the extraction of many invalid terms (e.g.,?great/JJ design/NN?
or ?new/JJ design/NN?
).Quite frequently, sentiment bearing adjectivessuch as ?great?, ?fantastic?, or ?bad?
are erro-neously extracted.
We utilize our hand-craftedsentiment lexicon to prune these modifiers.
An-other type is related to adjectives that act as uni-versal modifiers in terms (e.g., ?new?, ?long?, or?red?).
For such adjectives we cannot compile astop word list.
We experiment with two differ-ent methods for filtering universal modifiers.
Asa baseline, we examine a filter proposed by Koza-kov et al(2004) as part of their GlossEx glossaryextraction system.
As a second approach, we pro-pose a method that uses signals from pros/conssummaries of reviews (Section 3.6).Product Name Filter As we are only interestedin finding nominal aspect mentions, we need todiscard all candidate terms that refer to product orbrand names.
For this purpose, we automaticallygenerate a stop word list by exploiting meta data(on products and brands) that is associated withthe crawled customer reviews.
Whenever a termcandidate contains a token that is present in the ap-propriate stop word list, the candidate is discarded.3.4 Variant AggregationThe goal of this step is to find all variants of a termand to identify a canonical representation.
Forexample, the variants ?auto-focus?, ?auto focus?,?autofocus?, or ?auto focuss?
should be mappedto the canonical form ?auto focus?.
The purposeof this step is twofold: (1) higher lexicon cov-erage and (2) preventing potential problems withdata sparseness during candidate ranking.
Follow-ing Kozakov et al(2004), we implement heuris-tics for finding symbolic, compounding, and mis-spelling variants.
In addition, we implement amethod that considers compositional variants ofthe form ?room size?
vs. ?size of the room?.3.5 Candidate Ranking and SelectionCandidate ranking is at the core of each termi-nology extraction system.
As it is unclear whichrelevance measure performs best in our context,we experiment with different approaches and alsoconsider reasonable combinations of individualscores.
Despite the newly proposed diversity valuescore, the selected measures are all taken fromprevious research in terminology extraction.
Wetherefore only briefly discuss the other measuresand refer to the original literature for more details.Raw Frequency (Intra Domain) The rankingis simply determined by the raw occurrence fre-quency of a term.Relative Frequency Ratio (Contrastive) Thisranking (MRFR) is based on the comparison ofrelative frequency ratios in two corpora.
While theoriginal measure (Damerau, 1993) is only definedfor single word terms, Kozakov et al(2004) showhow to extend the definition to multi-word terms.Likelihood Ratio Test (Contrastive) This rank-ing can be considered as a more robust version ofthe MRFR approach.
Put simply, it additionallycomputes confidence scores for the relative fre-quency ratios, which allows to prevent problemswith low frequency terms.
The score is based onthe likelihood ratio test (LRT).
Yi et al(2003) de-scribe how the score is computed in our context.Generalized Dice Coefficient (Term Cohesion)To measure the association between words of acomplex term, Park et al(2002) introduce a mea-sure that generalizes the Dice coefficient (Dice,1945).
The measure gives higher scores to termswith high co-occurrence frequencies.Diversity Value (Intra Domain) Based on theobservation that nested word sequences that ap-pear frequently in longer terms are likely to rep-resent the key parts or features of a product, wepropose a measure that gives higher scores to such?key terms?
(e.g., ?lens?
occurs in terms suchas ?autofocus lens?, ?zoom lens?, ?macro lens?,225?lens cap?, or ?lens cover?).
Inspired by the C-Value score (Frantzi and Ananiadou, 1996), we de-fine the measure as: diversity-score(ws) =log2(|ws|t + 1) ?
?wi?ws(f(wi) ?
log2(|T ?wi |+ 1))|ws|t ,where |ws|t denotes the number of tokens of aword sequence ws, wi refers to the i-th token inws, and T ?wi describes the set of other candidateterms that contain the token wi.
The functionf(wi) returns the frequency of the token wi in theconsidered text corpus.Combining Ranking MeasuresAs the presented ranking measures are based ondifferent definitions of term significance, it isreasonable to compute a combined score (e.g.,combining a term?s contrastive relevance with itsstrength of cohesion).
Since the different mea-sures are not directly comparable, we computea combined score by considering the individualrankings: Let T be the set of extracted candidateterms and let Ri(t) be a function that ranks candi-dates t ?
T .
Using a weight ?i for each of the nselected measures, we compute the final rank of acandidate t as: weighted-rank(t) =?ni=1 ?i ?Ri(t) , where?ni=1 ?i = 1.For our experiments, we chose equal weights foreach ranking measure, i.e., ?i = 1/n.3.6 Pros/Cons Pre-Modifier FilterSome sentiment bearing pre-modifiers are domainor aspect-specific (e.g., ?long battery life?)7.
TheGlossEx filter (see Section 3.3) cannot cope withthis type of modification.
To identify such pre-modifiers, we propose to exploit signals fromstructured pros/cons summaries that typically ac-company a customer review.
We hypothesize thatvalid pre-modifiers (e.g., ?digital?
in ?digital cam-era?)
occur similarly distributed with their headnoun in both, lists of pros and lists of cons.
In-valid pre-modifiers, i.e., aspect-specific sentimentwords, are likely to occur either more often in listsof pros or lists of cons.
We design a simple likeli-hood ratio test to operationalize this assumption.In particular, we consider the probabili-ties p1 = Pr(pm|head; pros) and p2 =Pr(pm|head; cons), where p1 (p2) denotes theprobability in a corpus of pros (cons) lists that pmoccurs as pre-modifier with the head noun head.7see also (Fahrni and Klenner, 2008)statistic hotel cameradocuments 150 150sentences 1,682 1,416tokens 29,249 24,765nominal aspect mentions(incl.
sentiment targets)2,066 1,918avg.
tokens per mention 1.28 1.4distinct mentions 490 477Table 1: Basic corpus statistics.To design a hypothesis test, we assume as null hy-pothesis H0 that p1 = p = p2 (equal distributionin pros and cons) and as alternative hypothesis thatp1 6= p2 (unequal distribution).
We calculate thelikelihood ratio ?
and utilize the value ?2 ?
log?to reject H0 at a desired confidence level (in thatcase, we prune the pre-modifier pm).4 DatasetsWe evaluate our approaches on datasets of hoteland digital camera reviews.
We crawled around500,000 hotel reviews from Tripadvisor.com andapproximately 200,000 digital camera reviewsfrom Amazon.com, Buzzillions.com, and Epin-ions.com.
From each of the two crawls, we ran-domly sample 20,000 reviews, which we use asforeground corpora for the terminology extrac-tion task8.
As a background corpus, we utilize a100,000 document subset (randomly sampled) ofthe ?ukWaC corpus?
(Baroni et al 2009).4.1 Evaluation CorporaTo evaluate our approaches, we manually anno-tate a subset of the crawled reviews.
In partic-ular, we randomly sample subsets of 150 hoteland 150 camera reviews that do not overlap withthe foreground corpora.
Following prior work onsentiment analysis (Wiebe et al 2005; Polanyiand Zaenen, 2006), we decompose an opinion intotwo functional constituents: sentiment expressionsand sentiment targets.
In addition, we considernominal mentions of product aspects that are nottargeted by a sentiment expression.
We anno-tate a document by marking relevant spans of textwith the appropriate annotation type, setting thetype?s properties (e.g., the polarity of a sentimentexpression), and relating the annotations to eachother.
Table 1 summarizes the statistics of the cre-ated evaluation corpora (regarding sentiment tar-gets and nominal aspect mentions).8Larger corpora did not improve our results.2265 Experiments and Results5.1 Evaluation MethodsWe conduct intrinsic and extrinsic evaluation ofthe approaches.
Intrinsic evaluation refers to as-sessing the quality of the generated product as-pect lexicons.
For this purpose, we manually in-spect the extracted lexicons and report results interms of precision (share of correct entries) or pre-cision@n (the precision of the n highest rankedlexicon entries).
For extrinsic evaluation (evalu-ation in use), we apply the extracted lexicons forthe task of aspect detection in customer reviewdocuments.
To match lexicon entries in reviewtexts, we apply the Aho-Corasick algorithm (Ahoand Corasick, 1975).
If multiple matches overlap,we select the left-most, longest-matching, highest-scoring lexicon entry (thus guaranteeing a set ofnon-overlapping matches).
Only exact matchesare counted as true positives.
We further differ-entiate between two evaluation scenarios:?
Scenario A: In this scenario, the task is to extractall product aspects, irrespective of being target ofa sentiment expression or not.
We thus define theunion of sentiment target and aspect mention an-notations as reference (gold standard).
Any ex-traction that matches either a sentiment target oran aspect mention is considered a true positive.?
Scenario B: This scenario considers the task ofdetecting sentiment targets.
As it is not our goalto assess the accuracy of sentiment expression de-tection, we provide the extraction algorithm withperfect (gold standard) knowledge on the presenceof sentiment expressions and their relations to sen-timent targets (in effect, the algorithm only consid-ers matches that overlap a sentiment target).5.2 Baseline Results (Yi et alMethod)To make our results comparable to other exist-ing methods, we first set a baseline by applying astate-of-the-art approach on our datasets.
As moti-vated in Section 2.2, the LRT-approach by Yi et al(2003) represents our baseline.
We can easily im-plement Yi?s method with our terminology extrac-tion framework by using the BNP1 POS-tag filter,the bBNP acquisition heuristic, and the LRT-scorefor ranking.
We select all terms with a minimumLRT-score of 3.849 and do not apply any candidatefiltering or variant aggregation.93.84 is the critical value of the ?2-distribution for onedegree of freedom at a confidence level of 95%.scenario precision recall f-measurehotel A 55.1% 73.0% 62.8%hotel B 81.3% 71.2% 75.9%camera A 65.0% 72.5% 68.6%camera B 76.8% 69.9% 73.2%Table 2: Extrinsic evaluation results for the base-line approach.scenario precision recall f-measurehotel A 56.9% (+1.8*) 75.2% (+2.2*) 64.8% (+2.0*)hotel B 85.7% (+4.4*) 75.1% (+3.9*) 80.0% (+4.1*)camera A 69.2% (+4.2*) 74.3% (+1.8*) 71.7% (+3.1*)camera B 79.3% (+2.5*) 72.2% (+2.3*) 75.6% (+2.4*)Table 3: Results with activated candidate filters.The baseline method produces lexicons with1,182 (hotel) and 953 (digital camera) entries.
Dueto our significantly larger foreground corpora, thedictionaries?
sizes are by far larger than reportedby (Yi et al 2003) or by (Ferreira et al 2008).Intrinsic evaluation of the lexicons reveals preci-sion values of 61.2% (hotel) and 67.6% (camera).For precision@40, we find values of 62.5 (hotel)80.0 (camera).Table 2 reports the extrinsic evaluation resultsfor the baseline configuration.
Naturally, the pre-cision values obtained for scenario A are lowerthan for the ?synthetic?
scenario B (where partialmatches are the only possible source for false pos-itives).
Recall values in both scenarios are moder-ately high with around 70%.If not otherwise stated, the configurations inthe following sections apply the BNP1 acquisitionpattern, the BBNP heuristic, and the LRT-rankingwith a minimum score of 3.84.5.3 Effectiveness of Candidate FilteringIn this section, we analyze the influence of candi-date filtering (baseline: Yi?s method).
When ap-plying all filters jointly (except for the pros/consfilter), the resulting lexicons consist of 975 (hotel)and 767 (camera) entries.
Compared to the base-line, the (intrinsic) precision of the lexicons im-proves by around 10 percentage points (hotel) and14 percentage points (camera).
Each individualfilter has a positive effect on the precision, wherethe GlossEx filter has the greatest influence (+5percentage points in both corpora).
Table 3 showsthat the improved lexicon precision also leads tobetter results for the product aspect extraction task.The observed f-measure values increase by up to4.1 percentage points compared to the baseline227scenario precision recall f-measurehotel A 56.7% (-0.2) 75.1% (-0.1) 64.6% (-0.2)hotel B 85.5% (-0.2) 75.1% (0.0) 79.9% (-0.1)camera A 69.8% (+0.6) 74.8% (+0.5) 72.2% (+0.5)camera B 80.7% (+1.4) 73.0% (+0.8) 76.7% (+1.1)Table 4: Results with variant aggregation.method.
All improvements are statistically signif-icant10.
The increase in recall is mainly due tosuccessful pruning of false modifiers.5.4 Effectiveness of Variant AggregationIn this section, we examine the influence of thedifferent variant aggregation techniques (baseline:Yi?s method + filter).
To assess the effectivenessof variant aggregation, we only evaluate extrinsi-cally (since we primarily expect a higher coverageof the lexicons).
Table 4 compares the results withvariant aggregation to the results of the previoussection (all filters activated).
The results show thatvariant aggregation has only marginal effects.
Al-though we can measure improved results for thecamera corpus, the differences are rather small andnot statistically significant.
For the hotel corpus,the influence is even lower.
To understand the rea-sons for the insignificant effect, we perform a mis-take analysis of the false negatives in scenario B.In particular, we compare the false negatives withand without variant aggregation.
For the hotel cor-pus, we only find 18 out of 251 false negatives(7.2%) that are candidates for variant aggregation.In the ideal case (variant aggregation successfullyrecognizes all the candidates), this translates to amaximum gain of 1.8 percentage points in recall.For the camera dataset, we calculate a maximumgain of 2.4 percentage points.
Our results deviatefrom the ideal case for mainly two reasons: (1)Most variants occur rarely and the ones that oc-cur in the evaluation corpora do not occur in theforeground corpora.
(2) Some variants (e.g., mis-spellings) are so frequent in the foreground corpusthat the LRT-ranking already selects them as inde-pendent terms.5.5 Influence of Acquisition MethodsThis section examines the influence of the differ-ent acquisition patterns and heuristics.
We onlyreport results for the hotel dataset as the resultsfor the camera corpus are similar.
Table 5 shows10We use the * notation to indicate statistically significantdifferences.
If not otherwise stated, significance is reportedat the 99% confidence level.precision recall f-measureheuristic BNP1 BNP2 BNP1 BNP2 BNP1 BNP2?
80.7% 79.5% 70.7% 71.7% 75.4% 75.4%SBS 81.1% 80.0% 72.2% 72.9% 76.4% 76.3%DBNP 83.2% 82.4% 73.6% 75.2% 78.1% 78.6%SBP 87.0% 84.5% 74.6% 75.8% 80.3% 79.9%BBNP 85.5% 85.5% 75.1% 77.7% 79.9% 81.5%Table 5: Extrinsic evaluation results with varyingacquisition patterns and heuristics (hotel dataset).hotel camerameasure precision p@40 precision p@40frequency 41.6% 55.0% 44.8% 70.0%dice 39.0% 55.0% 43.5% 87.5%diversity 66.4% 77.5% 76.7% 70.0%lrt 69.6% 72.5% 81.1% 87.5%mrfr 72.0% 87.5% 81.4% 92.5%Table 6: Intrinsic evaluation results with the fivedifferent ranking measures.results for scenario B (all filters and aggregationmethods activated).
As could be expected, themore relaxed acquisition pattern BNP2 trades pre-cision for an increased recall (+1-2 percentagepoints).
The results further show that the use ofappropriate acquisition heuristics is quite impor-tant.
We can improve the f-measure by up to 6.1percentage points.
We find that the SBP and BBNPheuristics perform best on our datasets.
The dif-ferences in f-measure, compared to the other twoheuristics, are statistically significant (not shownin the table).
As the BBNP heuristic is easier toimplement and shows comparable results, we con-clude that it is preferable over the SBP method.5.6 Influence of Ranking FunctionsWe now examine the influence of the differentranking measures (all filters and variant aggrega-tion are activated).
To rule out the influence ofvarying lexicon sizes, we choose a fixed size foreach dataset (determined by the number of termsthat exhibit an LRT-score greater than 3.84).
Forlarger lexicons, we prune the entries with the low-est scores.
For each configuration, we apply allfilter and variant aggregation approaches.
Table6 shows the intrinsic evaluation results.
We canclearly observe that the contrastive relevance mea-sures (LRT and MRFR) outperform the intra do-main and term cohesion measures.
The MRFR-ranking shows better results than the LRT-rankingin both corpora, especially w.r.t.
precision@40.The improved results with contrastive measuresare also reflected by our extrinsic evaluation.
Ta-228hotel camerameasure prec.
rec.
F prec.
rec.
Ffrequency 45.3% 79.1% 57.6% 50.7% 77.8% 61.4%dice 44.7% 78.3% 56.9% 50.4% 77.5% 61.1%diversity 51.4% 72.3% 60.1% 64.5% 73.8% 68.8%lrt 56.7% 75.1% 64.6% 69.8% 74.8% 72.2%mrfr 60.2% 67.3% 63.5% 73.1% 72.8% 73.0%all 46.6% 79.3% 58.7% 52.6% 78.7% 63.0%mrfr-dice 47.7% 78.2% 59.2% 55.3% 78.2% 64.8%lrt-div.
47.8% 73.5% 57.9% 57.0% 75.1% 64.8%mrfr-lrt 56.9% 73.5% 64.2% 68.2% 73.7% 70.8%mrfr-freq.
51.8% 77.8% 62.2% 61.2% 76.1% 67.9%mrfr-lrt-div.
53.3% 74.5% 62.2% 66.8% 75.4% 70.8%mrfr-div.
57.9% 73.1% 64.6% 71.8% 72.5% 72.1%Table 7: Extrinsic evaluation results for varyingranking methods (scenario A).scenario precision recall f-measurehotel A 58.0% (+1.1*) 76.3% (+1.1) 65.9% (+1.1*)hotel B 88.9% (+3.2*) 77.4% (+2.4*) 82.8% (+2.8*)camera A 71.7% (+2.5*) 76.2% (+1.9) 73.9% (+2.2*)camera B 83.4% (+4.0*) 75.1% (+2.9*) 79.0% (+3.4*)Table 8: Results with active pros/cons filter.ble 7 presents the results for scenario A, consid-ering the measures in isolation and in selectedcombinations (using equal weights).
Comparedto raw frequency, the contrastive measures exhibitf-measure values that are between 7 (hotel) and11.6 (camera) percentage points higher.
We hy-pothesized that the combination of different rele-vance concepts (e.g., contrastive + term cohesion)could improve the system?s performance, but theobtained results do not confirm this hypothesis.5.7 Effectiveness of Pros/Cons FilterIn this section we examine the the pros/cons pre-modifier filter.
The reported results are basedon pros/cons corpora composed of 100,000 (ho-tel) and 50,000 (camera) documents.
We set thethreshold for the hypothesis test to 10.83, corre-sponding to a 99.9% confidence level.
Table 8presents the results of additionally applying thisfilter (baseline: all other filters and variant aggre-gation activated).
We can observe statistically sig-nificant improvements with gains in f-measure ofup to 3.4 percentage points.
Examining the result-ing lexicons, we find that the filter successfullypruned around 40 false pre-modifiers, which in-creases the (intrinsic) precision by around 3 per-centage points for both datasets.
Despite the rela-tively few lexicon entries that are altered by meansof the filter, we observe the mentioned (signifi-cant) gains in f-measure.
For both datasets thisis mainly because the affected lexicon entries ex-hibit a high occurrence frequency in the evaluationdatasets (e.g., ?large room?
or ?low price?
).6 ConclusionsIdentifying the most relevant aspects of a givenproduct or product type constitutes an importantsubtask of an aspect-based review mining system.In this work, we explicitly cast the task as a ter-minology extraction problem.
We were interestedwhether methods that have been proven beneficialin TE systems also help in our application sce-nario.
Additionally, we proposed and evaluatedsome new term acquisition heuristics, candidatefiltering techniques, and a ranking measure.
Theresults show that our terminology extraction ap-proach allows to generate quite accurate productaspect lexicons (precision up to 85%), which inturn allow for f-measures of up to 74% for an as-pect detection task and up to 83% for a (synthetic)sentiment target detection task.
Compared to arelevant baseline approach (Yi et al 2003), weobserve increases in f-measure by 3-7 percentagepoints for different evaluation scenarios.With regard to the different configurations ofour system, we made the following observations:?
Improved results are mainly due to the proposedcandidate filtering techniques.
Each individual fil-ter has been found to be beneficial.
The proposedpros/cons filter raised the f-measure by up to 3.4percentage points.?
The choice of the acquisition heuristic is impor-tant.
We measured differences of up to 6.1 per-centage points in f-measure.
The SBP and BBNPheuristics performed best.
The relaxed BNP2 pat-tern increases the recall and is a reasonable choiceif extracted lexicons are manually post-processed.?
The variant aggregation techniques had only amarginal effect.?
The contrastive relevance measures LRT andMRFR performed best.
Neither the proposed di-versity value score, nor combinations of differentrelevance measures proved to be beneficial.?
In summary, we suggest to use the BNP2 ac-quisition pattern and the BBNP or SBP acquisi-tion heuristic, to activate all mentioned filters, andto use a contrastive relevance measure for rank-ing.
Whereas variant aggregation was not bene-ficial within the TE pipeline, it is nonetheless im-portant and should be considered downstream, i.e.,during application of the extracted lexicons.229ReferencesRakesh Agrawal and Ramakrishnan Srikant.
1994.Fast algorithms for mining association rules in largedatabases.
In Proceedings of the 20th VLDB, pages487?499, San Francisco, CA, USA.Alfred V. Aho and Margaret J. Corasick.
1975.
Ef-ficient string matching: An aid to bibliographicsearch.
Comm.
of the ACM, 18(6):333?340.M.
Baroni, S. Bernardini, A. Ferraresi, andE.
Zanchetta.
2009.
The WaCky Wide Web:A collection of very large linguistically processedweb-crawled corpora.
LREC, 43:209?226.K.
Bloom, N. Garg, and S. Argamon.
2007.
Extractingappraisal expressions.
In Proceedings of the NAACLHLT 2007, pages 308?315.
ACL.G.
Carenini, R. T. Ng, and E. Zwart.
2005.
Extractingknowledge from evaluative text.
In Proceedings ofthe K-CAP ?05, pages 11?18.
ACM.F.
J. Damerau.
1993.
Generating and evaluatingdomain-oriented multi-word terms from texts.
In-formation Proc.
and Management, 29(4):433?447.Kushal Dave, Steve Lawrence, and David M. Pennock.2003.
Mining the peanut gallery: Opinion extractionand semantic classification of product reviews.
InProceedings of the WWW ?03, pages 519?528, NewYork, NY, USA.
ACM.Lee R. Dice.
1945.
Measures of the amount of eco-logic association between species.
Ecology, 26(3).A.
Fahrni and M. Klenner.
2008.
Old wine or warmbeer: Target-specific sentiment analysis of adjec-tives.
In Symposion on Affective Language in Hu-man and Machine, AISB Convention, pages 60?63.L.
Ferreira, N. Jakob, and I. Gurevych.
2008.
A com-parative study of feature extraction algorithms incustomer reviews.
In Proceedings of the 2008 Inter-national Conference on Semantic Computing, pages144?151.
IEEE Computer Society.K.
T. Frantzi and S. Ananiadou.
1996.
Extractingnested collocations.
In Proceedings of the 16thCOLING, pages 41?46.
ACL.H.
Guo, H. Zhu, Z. Guo, X. Zhang, and Z. Su.
2009.Product feature categorization with multilevel latentsemantic association.
In Proceedings of the 18thCIKM, pages 1087?1096.
ACM.Minqing Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In Proceedings of the 10thACM SIGKDD, pages 168?177.
ACM.N.
Jakob and I. Gurevych.
2010.
Extracting opiniontargets in a single- and cross-domain setting withconditional random fields.
In Proceedings of theEMNLP ?10, pages 1035?1045.
ACL.Niklas Jakob.
2011.
Extracting Opinion Targets fromUser-Generated Discourse with an Application toRecommendation Systems.
Ph.D. thesis, TechnischeUniverstita?t Darmstadt.K.
Kageura and B. Umino.
1996.
Methods of au-tomatic term recognition: A review.
Terminology,3(2):259?289.J.
S. Kessler, M. Eckert, L. Clark, and N. Nicolov.2010.
The 2010 ICWSM JDPA sentiment corpusfor the automotive domain.
In Proceedings of the4th AAAI Conference on Weblogs and Social MediaData Workshop Challenge.L.
Kozakov, Y.
Park, T. Fin, Y. Drissi, Y. Doganata, andT.
Cofino.
2004.
Glossary extraction and utiliza-tion in the information search and delivery systemfor IBM technical support.
IBM Systems Journal,43(3):546?563.R.
Navigli and P. Velardi.
2004.
Learning domain on-tologies from document warehouses and dedicatedweb sites.
Comp.
Linguistics, 30(2):151?179.Youngja Park, Roy J. Byrd, and Branimir K. Boguraev.2002.
Automatic glossary extraction: Beyond ter-minology identification.
In Proceedings of the 19thCOLING, pages 1?7.
ACL.Livia Polanyi and Annie Zaenen.
2006.
Contextualvalence shifters.
In Computing Attitude and Affectin Text: Theory and Applications, volume 20 of TheInformation Retrieval Series, chapter 1, pages 1?10.Springer Netherlands, Berlin/Heidelberg.A.-M. Popescu and O. Etzioni.
2005.
Extracting prod-uct features and opinions from reviews.
In Proceed-ings of HLT EMNLP ?05, pages 339?346.
ACL.C.
Scaffidi, K. Bierhoff, E. Chang, M. Felker, H. Ng,and C. Jin.
2007.
Red opal: product-feature scoringfrom reviews.
Proceedings of the 8th ACM Confer-ence on Electronic Commerce, pages 182?191.J.
Wiebe, T. Wilson, and C. Cardie.
2005.
Annotatingexpressions of opinions and emotions in language.LREC, 39(2):165?210, May.Y.
Wu, Q. Zhang, X. Huang, and L. Wu.
2009.
Phrasedependency parsing for opinion mining.
In Proceed-ings of the EMNLP ?09, pages 1533?1541.
ACL.J.
Yi, T. Nasukawa, R. Bunescu, and W. Niblack.
2003.Sentiment Analyzer: Extracting sentiments about agiven topic using natural language processing tech-niques.
In Proceedings of the 3rd ICDM, pages 427?434.
IEEE Comput.
Soc.U.
Zernik.
1991.
Lexical Acquisition: Exploiting On-line Resources to Build a Lexicon.
Lawrence Erl-baum.L.
Zhang, B. Liu, S. H. Lim, and E. O?Brien-Strain.2010.
Extracting and ranking product features inopinion documents.
In Proceedings of the 23rdCOLING: Posters, pages 1462?1470.
ACL.230
