R. Dale et al (Eds.
): IJCNLP 2005, LNAI 3651, pp.
1004 ?
1016, 2005.?
Springer-Verlag Berlin Heidelberg 2005Web-Based Terminology Translation MiningGaolin Fang, Hao Yu, and Fumihito NishinoFujitsu Research and Development Center, Co., LTD. Beijing 100016, China{glfang, yu, nishino}@frdc.fujitsu.comAbstract.
Mining terminology translation from a large amount of Web data canbe applied in many fields such as reading/writing assistant, machine translationand cross-language information retrieval.
How to find more comprehensive re-sults from the Web and obtain the boundary of candidate translations, and howto remove irrelevant noises and rank the remained candidates are the challeng-ing issues.
In this paper, after reviewing and analyzing all possible methods ofacquiring translations, a feasible statistics-based method is proposed to mineterminology translation from the Web.
In the proposed method, on the basis ofan analysis of different forms of term translation distributions, character-basedstring frequency estimation is presented to construct term translation candidatesfor exploring more translations and their boundaries, and then sort-based subsetdeletion and mutual information methods are respectively proposed to deal withsubset redundancy information and prefix/suffix redundancy informationformed in the process of estimation.
Extensive experiments on two test sets of401 and 3511 English terms validate that our system has better performance.1   IntroductionThe goal of Web-based terminology translation mining is to mine the translations ofterminologies or proper nouns which cannot be looked up in the dictionary from theWeb using a statistical method, and then construct an application system for read-ing/writing assistant (e.g.
Mont Blanc???
?, ???).
Translators and technicalresearchers cannot yet obtain an accurate translation after many lookup efforts whenthey encounter terminology or proper noun during translating or writing foreign lan-guage.
According to Web statistics by Google, 76.59% of Web pages are English.
InChina, statistical results by China Internet Network Information Center in July 2004show that the number of Internet users has reached 94 million, and nearly 87.4% ofusers have educational backgrounds beyond high school.
These users can smoothly readgeneral English pages, but some terminologies in the Web hamper them to exactly un-derstand the whole content.
Some skilled users perhaps resort to a Web search engine,but they cannot obtain effective information from a large amount of retrieved irrelevantpages and redundancy information.
Thus, it is necessary to provide a system to auto-matically mine translation knowledge of terms or proper nouns using abundant Webinformation so as to help users accurately read or write foreign language.The system of Web-based terminology translation mining has many applications.1) Reading/writing assistant, as one part of computer-assisted language learning(CALL) used in the E-learning.
During reading or writing, users often meet termsWeb-Based Terminology Translation Mining 1005whose translations cannot be found in the dictionary, but this system can help themmine native and accurate translations from the Web.
2) The tool for constructing bi-lingual dictionary.
The system can not only provide translation candidates for compil-ing bilingual lexicon, but also evaluate or rescore the candidate list of the dictionary.The constructed dictionary can be further applied in cross-language information re-trieval (CLIR) and machine translation.
3) As one of the typical application paradigmsof the combination of CLIR and Web mining.There are some issues that need to be solved using Web information to mine termi-nology translation: 1) How to find more comprehensive results, i.e.
mining all possi-ble forms of annotation pairs in the Web.
2) How to obtain the boundary of candidatetranslations, especially for the language without the boundary mark such as Chineseand Japanese.
Because we don?t know the translation is at left or right, and what isbetween the pair, and where is the candidate endpoint?
3) How to remove the noisesformed in the statistics and rank the remained candidates.On the basis of reviewing all possible methods of acquiring translations, a feasiblestatistics-based method is proposed to mine terminology translation from the Web.
Inthe proposed method, after analyzing different forms of term translation distributions,character-based string frequency estimation is employed to construct term candidatetranslations for exploring more translations and their boundaries, and then the candi-date noises formed in the process of statistics are defined as two categories: subsetredundancy information and prefix/suffix redundancy information.
Sort-based subsetdeletion and mutual information methods are respectively proposed to deal with tworedundancy information.
Experiments on two test sets of 401 and 3511 English termsshow that our system has better performance.
In all reported literatures, our experi-ment is the first time for the extensive research on Web-based terminology translationmining on the largest scale.2   Related WorkAutomatic acquisition of bilingual word pairs or translations has been extensivelyresearched in the literature.
The methods of acquiring translations are usually summa-rized as four categories: 1) acquiring translation from parallel corpora, 2) acquiringtranslation from a combination of translations of constituent words, 3) acquiringtranslation from bilingual annotation in the Web, and 4) acquiring translation fromnon-parallel corpora.1)  Acquiring translation from parallel corpora.Acquiring bilingual lexicon or translations from parallel corpora (including sentencealignment and paragraph alignment) is to utilize statistics information such as co-occurrence, position, and length between source word and translation equivalence inparallel texts as an evaluation criterion to obtain one-to-one map word pairs.
Manyprevious researches focused on extracting bilingual lexicon from parallel corpora, andreaders can refer to the reviews [1], [2] for the details.
However, due to the restrictionof current available parallel corpora of different languages, together with the fact thatcorpus annotation requires a lot of manpower and resources, researchers have at-tempted to extract translations from non-parallel corpus or Web data.
As opposed toextracting from parallel corpora, there are no corresponding units in non-parallel1006 G. Fang, H. Yu, and F. Nishinocorpora so that statistics information such as co-occurrence, position and length be-come unreliable.
New statistical clues have to be proposed to build the relationshipfor acquiring translation pairs from non-parallel corpora, which is more difficult tohandle than in parallel corpora.2) Acquiring translation from a combination of translations of constituent words.Grefenstette [3] employed an example-based approach to obtain compound wordtranslations.
His method first combined possible translations of each constituent, andthen searched them in WWW, where the retrieved number was viewed as an evalua-tion criterion.
Experiments on a set of 724 German words and a set of 1140 Spanishterms showed that the accuracies of English translations were about 87% and 86%,respectively.Cao and Li [4] proposed a dictionary-based translation combination method to col-lect translation candidates of English base noun phrases, and then employed a naiveBayesian classifier and TF-IDF vector constructed with EM algorithm as evaluationcriterions for translation selection.
In an experiment with 1000 English base nounphrases, the coverage of acquiring translations was 91.4%, and the accuracy of top 3choices was 79.8%.
The system was further improved in the literature [5].Navigli et al [6] proposed an ontology learning method for acquiring terminologytranslations from English to Italian.
His method was based on bilingual lexicon andsemantic relation between the constituents of source language derived from ontologylearning, where disambiguated terms dramatically reduced the number of alternativetranslations and their combinations.
This system can automatically extract the transla-tions of 405 complex terms in the tourism domain.Using the translation combination of each constituent to acquire the translation of amultiword term is very suitable for translation acquisitions of base noun phrases.However, terminologies and technical terms often consist of unknown words, andtheir translations are seldom the combination of each constituent.
Thus, the result ofdirect combination is not very desirable for terminology translation acquisition.3)  Acquiring translation from bilingual annotation in the Web.Nagata et al [7] proposed an empirical function of the byte distance between Japa-nese and English terms as an evaluation criterion to extract the translation of Japaneseword, and their results could be used as a Japanese-English dictionary.
Preliminaryexperiments on the 50 word pairs showed that an accuracy of top 50 candidatesreached 56%.
The reasons for such experimental results have two aspects: first, thesystem didn?t further deal with candidate noises for mining useful knowledge; second,this system only handled top 100 Web pages retrieved from search engine.
In fact,previous 100 Web pages seldom contain effective bilingual annotation informationonly directly using keyword search rather than imposing other restrictions.
Thus, thisproblem should be further researched for practical applications.
Since his researchfocused on finding English translation given a Japanese term, the segmentation ofJapanese could be avoided.
However, our problem is to find Chinese equivalent usingEnglish term, so we have to cope with how to obtain the correct boundary of Chinesetranslations.
Therefore, the issue and the proposed method in this paper are distinctlydifferent with Nagata?s.Web-Based Terminology Translation Mining 10074)  Acquiring translation from non-parallel corpora.Acquiring translation from non-parallel corpora is based on the clue that the contextof the source term is very similar to that of the target translation in a large amount ofcorpora.
In 1995, Rapp [8] assumed that there is a correlation between the patterns ofword co-occurrence in non-parallel texts of different languages, and then proposed amatrix permutation method to match these patterns.
However, computational limita-tion hampered further extension of this method.
In 1996, Tannaka and Iwasaki [9]demonstrated how to extract lexical translation candidates from non-aligned corporausing the similar idea.
In 1999, this method was developed and improved by Rapp[10].
Rather than computing the co-occurrence relation matrix between one word andall words, the matrix between one word and a small base lexicon are estimated.
Ex-periments on 100 German words indicated that an accuracy of top 1 English transla-tion was 72%, and top 10 was 89%.
This system was only suitable for the situation ofone word to one word, and didn?t further research on the translation acquisition frommultiword to multiword.In 1995, Fung [11] proposed a ?context heterogeneity?
method to compute themeasure similarity between word and its translation for finding translation candidates.In the experiment with 58 English words, an accuracy of 50% is obtained in the top10 Chinese word candidates.
Based on this work, Fung presented the word relationmatrix to find the translation pair in 1997 [12].
This method respectively computedthe correlation vectors between source word and seed word, target word and seedword.
In 19 Japanese term test set, the accuracy of English translations reached 30%.In 1998, the method was improved to extend to non-parallel, comparable texts fortranslation acquisition [13].
This system use TF/IDF as the feature, and differentmeasure functions as the similarity computation between the candidate pair.
However,the system was restricted to the assumption that there are no missing translations andall translations are included in the candidate word list.Shahzad et al [14] first extracted the sentence corpora that are likely to contain thetarget translation using bilingual dictionary and transformation table.
And then, theheuristics method was employed to obtain the correct candidate by analyzing therelations of source compound nouns and using partial context information.
Experi-ments on the 10 compound nouns showed that the average accuracy and recall wererespectively 34% and 60%.As shown from the current situation of translation acquisition from non-parallelcorpora, all experiments above are basically performed on small-scaled word set, andtheir results are very inspiring but difficult to put into practical use.
Furthermore, mostexperimental methods are only suitable for one word translation, i.e.
the word numberratio of translation pair is on a basis of 1:1.
Thus, there are many issues to be furtherresearched before it is used to explore new translation in the application area.From the review above, we know that Method 1 requires a large number of parallelcorpora, and Method 2 and Method 4 have some limitations when they are applied toacquire the terminology translation, and Method 3 makes the best of mass Web re-sources and is a feasible approach.
When people use Asia language such as Chinese,Japanese, and Korean to write, especially scientific article or technical paper, theyoften annotate the associated English meaning after the terminology.
With the devel-opment of Web and the open of accessible electronic documents, digital library, and1008 G. Fang, H. Yu, and F. Nishinoscientific articles, these resources will become more and more abundant.
Thus,Method 3 is a feasible way to solve the terminology translation acquisition, which isalso validated by the following experiments.3   The Framework of the Terminology Translation Mining SystemThe Web-based terminology  translation  mining  system  is  depicted  in Fig.
1 asfollows:Query?Mont Blanc?WWW(bilingual annotation)Rank & sortcandidatesString frequencyestimationResult????
?Web pagedownloadmoduleCandidate noises& solutionsHTMLanalysisWeb page collectionTerminology translation miningFig.
1.
The Web-based terminology translation mining systemThe system consists of two parts: Web page collection and terminology translationmining.
Web page collection includes download module and HTML analysis module.The function of download module is to collect these Web pages with terms?
associ-ated bilingual annotations, and then the pages are inputted into HTML analysis mod-ule.
In HTML analysis, Web pages are built as a tree structure from which possiblefeatures for the bilingual pair and text information in the HTML page are simultane-ously extracted.Terminology translation mining includes string frequency estimation, candidatenoises and their solutions, and rank & sort candidates.
Translation candidates areconstructed through string frequency estimation module, and then we analyze theirnoises and propose the corresponding methods to handle them.
At last, the approachcombining the possible features such as frequency, distribution, length proportion,distance, keywords and key symbols is employed to rank these candidates.In Web pages, there are a variety of bilingual annotation forms.
Correctly exploringall kinds of forms can make the mining system extract the comprehensive translationresults.
After analyzing a large amount of Web page examples, we summarize transla-tion distribution forms as the following six categories: 1) Direct annotation 2) Sepa-rate annotation 3) Subset form 4) Table form 5) List form 6) Explanation form.
Directannotation is the most widely used form in the Web, where English meaning oftenWeb-Based Terminology Translation Mining 1009follows after Chinese terminology, and some have symbol marks such as bracketparentheses and bracket, and some have nothing, e.g.
???
?Mont Blanc?.
Separateannotation is referred to as the case that there are some Chinese words or Englishletters between the translation pair, e.g.
?????,??
?universal life insurance?.Subset form is that the extracted translation pair is a subset of existing bilingual pair,for example, during searching the term ?Mont Blanc?, the term pair ????????
(Chamonix Mont Blanc)?
also provides the valid information.
Table or list form is theWeb page in the form of table or list.
Explanation form is the explanation and illustra-tion for technical terms.Fig.
2.
The examples of translation distribution forms, (a) Direct annotation, some has no mark(a1), and some have some symbol marks (a2, a3) (b) Separate annotation, there are Englishletters (b1) or some Chinese words (b2, b3) between the translation pair  (c) Subset form  (d)Table form  (e) List form  (f) Explanation form4   Statistics Based Translation Finding4.1   Character-Based String Frequency EstimationAll kinds of possible translation forms of terminologies in the Web can be effectivelyand comprehensively mined through character-based string frequency estimation.
Theproposed method with Chinese character as the basic unit of statistics can not onlyobtain the correct boundary of the translation candidate, but also conveniently explorethese Chinese candidate terminologies that usually consist of unknown words or un-known compound words.String frequency information is one of the important clues during extracting candi-date translations.
Its estimation method has a direct influence on the system perform-ance efficiency.
The method combing hash index and binary search is employed to(a1) (a2) (a3)(b1) (b2) (b3)(c) (d) (e) (f)1010 G. Fang, H. Yu, and F. Nishinoconstruct the index for all translation candidates.
The definition of hash function iscalculated according to 6763 Chinese characters in GB2312 system with a one-to-onemap.
Hash function is formulized as:otherwiseccccccY 21517621567635)161()176(94)161()176(94001010>?????????+?
?+?=  ,                    (1)where 10 ,cc  are respectively the unsigned encoding values of the first, second bytesof first Chinese character of candidate items.
All strings are partitioned into differentblocks in terms of the first Chinese character with the hash function above, where thestrings with the same first character are sorted by lexicographic order, and the stringswith non-Chinese character as the first position are indexed to the value of 6763.Here, GB2312 is employed as our statistics standard.
Other encoding system is con-verted to the corresponding characters in GB2312, and the characters will be omittedif there is no counterpart.
The reasons for this strategy are as follows: 1) terminologyseldom consists of rare words out of GB2312, 2) the index space is dramatically re-duced using GB2312 rather than the Unicode encoding so as to quicken the estimationspeed.The terminology to be looked up is inputted into search engine, and the relevantWeb pages with this term?s associated bilingual annotation are collected.
Web pagesare transformed into text through HTML analysis module.
The term position is lo-cated as the center point through keyword search, and then string frequency and dis-tribution estimation is performed in a window of 100 bytes.
In Web pages, terminol-ogies are often written as different forms because of the effect of noise.
For example,the term ?Mont Blanc?
may be written as ?MONT BLANC?, ?Mont-Blanc?, ?Mont?
?Blanc?, and ?MontBlanc?.
For finding different forms of keywords in the Web, thefuzzy string matching approach is proposed.
This method takes 26 English letters inthe keyword as effective matching symbols, while ignoring the blank space and othersymbols.
In the matched text, only these English letters are viewed as effective itemsfor comparison.
Using this method can effectively locate different forms of terms andtherefore obtain comprehensive translation candidates.The process of string frequency estimation is described as follows.
In the windowswith keyword as the center, each character is built as a beginning index, and then thestring candidates are constructed with the increase of the string in the form of oneChinese character unit.
Since terminology translation usually consists of unknownwords or compound words, character is employed as the basic unit of statistics ratherthan word so as to explore these unknown term translations as more as possible.String candidates are indexed in the database with hash and binary search method, ifthere exists the same item as the inputted candidate, its frequency is increased by 1,otherwise, this candidate is added to the database at this position.
After handling oneWeb page, the distribution information is also estimated at the same time.
In the pro-gramming implementation, the table of stop words and some heuristic rules of thebeginning and end with respect to the keyword position are constructed to acceleratethe statistics process.Web-Based Terminology Translation Mining 10114.2   Translation Noises and Their SolutionsAll possible forms of terminology translations can be comprehensively mined aftercharacter-based string frequency estimation.
However, there are many irrelevant itemsand redundancy noises formed in the process of mining.
These noises are defined asthe following two categories.1) Subset redundancy information.
The characteristic of this kind information isthat this item is a subset of one item, but its frequency is lower than that item.
Forexample: ?Mont Blanc???
(38) ??
(27) ??
(11)?, where ???
?, ????
belongto subset redundancy information.
They should be removed.2) Prefix/suffix redundancy information.
The characteristic of this kind informationis that this item is the prefix or suffix of one item, but its frequency is greater than thatitem.
For example: 1.
?Mont Blanc ??
(16) ???
(9) ???
(8)?, 2.
?Credit Rating??
(12) ????
(10)?, 3.
?Knowledge Portal ????
(33) ??????
(30)?.In Example 1, the item ????
is suffix redundancy information and should be re-moved.
In Example 2, the item ????
is prefix redundancy information and shouldalso be removed.
In Example 3, the term ??????
is in accord with the definitionof suffix redundancy information, but this term is a correct candidate.
Thus, the prob-lem of prefix/suffix redundancy information is so complex that we need an evaluationmethod to decide to retain or drop this candidate.Fig.
3.
The description of the sort-based subset deletion algorithm4.2.1   Sort-Based Subset Deletion MethodAiming at subset redundancy information, we propose sort-based subset deletionmethod to handle it.
Because subset redundancy information is an intermediate ofestimating terminology translations, its information is basically contained by the1.
Sort by entropy value2.
Sort by boundary[*] for the same entropy3.
Sort by length and lexical sort for the same entropy and boundary4.
int nNum = 0;    //record the number of remained candidates5.
for(int i=0; i<m_nDataNum; i++)  {6.  int nIsSubString = FALSE;7.   if(nNum == 0)    //for the first item to be remained8.
Judge whether to remain this item using boundary and length proportioninformation;9.   else  {10.         for(int j=0; j< nNum; j++)  {11.
Judge if the ith candidate is a subset of the jth, and doesn?t emerge inthe isolated form, if yes12.
{    nIsSubString = TRUE;    break;    }13.
}14.
}15.  if(!nIsSubString)  {16.
Move the ith candidate information to nNum position, and save;17.
The saved number nNum++;18.
}19.
}20. m_nDataNum = nNum; //Save the total number.
[*]Note: refer to the case that the string has the distinct left and right boundary in the Web1012 G. Fang, H. Yu, and F. Nishinolonger string candidate with higher frequency.
Therefore, this problem can be wellsolved by first sorting and then judging if this item is a subset of the preceding candi-dates.
The detailed algorithm is described in Fig.
3.4.2.2   Mutual Information Based MethodPrefix/suffix redundancy information is very complicated to deal with.
In some cases,previous candidate is a correct translation and should be retained, while in other cases,it is a noise and should be deleted.
In this paper, mutual information based method isproposed to decide if the candidate should be retained or deleted.The concept of information entropy is first proposed by Shannon in 1948.
Entropyis a measure of uncertainty of a random variable, and defined as:?
?==kiii xpxpXH12 )(log)()( ,                                       (2)where )( ixp  is a probability function of a random variable X=xi.Mutual information is a concept of information theory, and is a measure of theamount of information that one random variable contains about another variable.
Themutual information of two events X and Y is defined as:),()()(),( YXHYHXHYXI ?+= ,                                (3)where H(X) and H(Y) are respectively the entropies of the random variables of X andY, and H(X,Y) is the co-occurrence entropy of X and Y.Mutual information reflects a closeness degree of the combination of X and Y. Ifthere is no interesting relationship between X and Y, I(X,Y)=0, that is, X and Y areindependent each other.
If there is a genuine association between X and Y, the co-occurrence of XY will be bigger than the random individual occurrence chance of Xor Y, and consequently I>>0.
In this case, the possibility as a fixed compound phraseof XY becomes very big.
Small mutual information hints that the combination of Xand Y is very loose, and therefore there is a great possibility of a boundary betweentwo words X, Y.String frequency estimation is performed on different Web pages.
In each Webpage there is more than one occurrence for a candidate translation.
Mapping this esti-mation process to the entropy calculation, we define Nnxp ii /)( = , where ni denotesthe number of a translation candidate in one Web page, and N represents the totalnumber of this candidate.
We define k as the number of the estimated Web pages.
Thecalculation of entropy is rewritten as:NnnNNnNnXHkiiikiii21212 loglog1log)( +??=??===.
(4)Through this formula, the candidate entropy can be computed directly rather thanafter counting all Web data.
Therefore, it can reduce the time of statistics.Entropy can not only reflect the frequency information N, but also the distributioninformation in different Webs.
The higher the frequency is, and the larger the entropyis.
If the distribution is more uniform, this entropy value will become bigger.
This isalso in accord with our intuition.Web-Based Terminology Translation Mining 1013Given two candidate patterns of 1t , 2t  in the set of translation candidates,)()( 21 tCtC > , where C denotes the frequency of estimation.
For suffix redundancyinformation, )( 21 tsufft = ; for prefix redundancy information, )( 21 tpreft = .
Accord-ing to the definition of mutual information, )()()()( 21212 tHttHtHtI ?
?+= .The mutual information based method for prefix/suffix redundancy information isdescribed as follows.
First, judge if the condition of 95.0)(/)( 11 ??
tCttCii  or95.0)(/)( 11 ??
tCttCii  is satisfied, where the candidates itt1  represent the items thatdo not contained each other in the windows of 10 candidates after the candidate 1t .
Ifthe condition is met, then delete 1t .
In an example of ?Dendritic Cell ??
(62) ?????
(40) ????
(15) ?????
(4)?, because (40+15+4)/62=0.952>0.95, thecandidate ????
is deleted.
If prefix/suffix redundancy information don?t satisfy thecondition above, then judge the condition of )()( 21 tItI <?
, if yes, then delete 1t ,otherwise retain it.
The value of ?
is determined by the experiments, and the followingexperimental results demonstrate that ?=0.85 is the best parameter.5   ExperimentsOur experimental database consists of two sets of 401 English-Chinese term pairs and3511 English-Chinese term pairs in the financial domain.
There is no intersectionbetween two sets.
Each terminology often consists of 1-6 English words, and theassociated translation contains 2-8 Chinese characters.
In the test set of 401 terms,there are more than one Chinese translation for one English term, and only one Chi-nese translation for 3511 term pairs.
The top n accuracy is defined as the percentageof terms whose top n translations include correct translation in the term pairs.556065707580859095100Top1 Top3 Top5 Top10 Top30The calculated numberAc curac y?=0.9?=0.85?=0.82?=0.8?=0.7Fig.
4.
The relationship between the parameter ?
and the accuracyFor testing in what condition, mutual information based method is the best to dealwith the prefix/suffix redundancy information.
The parameter of ?
is respectively set1014 G. Fang, H. Yu, and F. Nishinoto 0.7, 0.8, 0.82, 0.85, and 0.9 in the experiment on the test set of 401 terms.
Experi-mental results are shown in Fig.
4.
From the figure, we know that ?=0.85 is thebest parameter.606570758085909510050 100 150 200 250 300The number of Web pagesAc cu ra cyN=1N=3N=5Fig.
5.
The relationship between the number of Web pages and the accuracyA second experiment is to analyze the number of Web pages influencing the termtranslation accuracy.
The experiments are respectively performed on 50, 100, 150,200, 250, and 300 Web pages retrieved from the Web.
Experimental results are illus-trated in Fig.
5, where N=1, 3, 5 represent the results of top 1, top 3, and top 5.
Asseen from the figure, the result of using 200 Web pages is best.
When the Web pagesincrease more than 200 Web pages, the performance isn?t improved distinctly, whilethe computation cost grows.
In the case of 200 Web pages, the Chinese translationaccuracy of top 1 is 71.8%, and top 3 is 94.5%, and top 5 is 97% on the test set of 401English terms (see Table 1).Table 1.
Experimental results on a test set of 401 termsCandidates Top30 Top10 Top5 Top3 Top1Accuracy 99.5% 99% 97% 94.5% 71.8%Using the previous trained parameters, we perform term translation mining ex-periments in the test set of 3511 terms.
Experimental results are listed in Table 2.From this table, the accuracy of top 3 is 83.6%.
Experiments also validate that theaccuracy of top 30 is nearly equal to the coverage of translations (the percentage ofterm translations found by our system).
This is because there is no change on theaccuracy when increasing the candidate number after top 30.Table 2.
Experimental results on a test set of 3511 termsCandidates Top30 Top10 Top5 Top3 Top1Accuracy 95.4% 93.8% 89.1% 83.6% 56.4%Web-Based Terminology Translation Mining 10156   ConclusionsIn this paper, after reviewing and analyzing all possible methods of acquiring trans-lations, a statistics-based method is proposed to mine terminology translation fromthe Web.
In the proposed method, character-based string frequency estimation isfirst presented to construct term translation candidates, and then sort-based subsetdeletion and mutual information methods are respectively proposed to deal with tworedundancy information formed in the process of estimation.
Experiments on twovocabularies of 401 and 3511 English terms show that our system has better per-formance, about 94.5% and 83.6% in the top 3 Chinese candidates.
The contribu-tions of this paper focus on the following two aspects: 1) On the basis of reviewingall possible methods of acquiring translations and analyzing different forms of termtranslation distribution, a statistics-based method is proposed to mine terminologytranslation from the Web.
2) The candidate noises are defined as two categories:subset redundancy information and prefix/suffix redundancy information.
Sort-based subset deletion and mutual information methods are respectively proposed todeal with two redundancy information.References1.
Somers, H.: Bilingual Parallel Corpora and Language Engineering.
Proc.
Anglo-IndianWorkshop "Language Engineering for South-Asian languages", (2001)2.
V?ronis, J.: Parallel Text Processing - Alignment and Use of Translation Corpora.
TheNetherlands: Kluwer Academic Publishers, (2000)3.
Grefenstette, G.: The WWW as a Resource for Example-Based MT Tasks.
Proc.
ASLIBTranslating and the Computer 21 Conference, (1999)4.
Cao, Y., Li, H.: Base Noun Phrase Translation Using Web Data and the EM Algorithm.Proc.
19th Int?l Conf.
Computational Linguistics, (2002) 127-1335.
Li, H., Cao, Y., Li, C.: Using Bilingual Web Data to Mine and Rank Translations.
IEEEIntelligent Systems.
4 (2003) 54-596.
Navigli, R., Velardi, P., Gangemi, A.: Ontology Learning and Its Application to Auto-mated Terminology Translation.
IEEE Intelligent Systems.
1 (2003) 22-317.
Nagata, M., Saito, T., Suzuki, K.: Using the Web as a Bilingual Dictionary.Proc.
ACL 2001 Workshop Data-Driven Methods in Machine Translation, (2001)95?1028.
Rapp, R.: Identifying Word Translations in Nonparallel Texts.
Proc.
33th Annual Meetingof the Association for Computational Linguistics, (1995) 320-3229.
Tanaka, K., Iwasaki, H.: Extraction of Lexical Translation from Non-Aligned Corpora,Proc.
16th Int?l Conf.
Computational Linguistics, (1996) 580-58510.
Rapp, R.: Automatic Identification of Word Translations from Unrelated English andGerman Corpora.
Proc.
37th Annual Meeting Assoc.
Computational Linguistics, (1999)519-52611.
Fung, P.: Compiling Bilingual Lexicon Entries from a Non-Parallel English-Chinese Cor-pus.
Proc.
Third Annual Workshop on Very Large Corpora, (1995) 173-1831016 G. Fang, H. Yu, and F. Nishino12.
Fung, P.: Finding Terminology Translations from Nonparallel Corpora.
Proc.
Fifth AnnualWorkshop on Very Large Corpora (WVLC'97), (1997) 192-20213.
Fung P., Yee, L.P.: An IR Approach for Translation New Words from Nonparallel, Com-parable Texts.
Proc.
17th Int?l Conf.
Computational Linguistics and 36th Annual Meetingof the Association for Computational Linguistics, (1998) 414-42014.
Shahzad, I., Ohtake, K., Masuyama, S., Yamamoto, K.: Identifying Translations of Com-pound Nouns Using Non-Aligned Corpora.
Proc.
Workshop on Multilingual InformationProcessing and Asian Language Processing, (1999) 108-113
