Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 651?656,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsAnnotation and Classification of an Email Importance CorpusFan ZhangComputer Science DepartmentUniversity of PittsburghPittsburgh, PA 15260zhangfan@cs.pitt.eduKui XuResearch and Technology CenterRobert Bosch LLCPalo Alto, CA 94304Kui.Xu2@us.bosch.comAbstractThis paper presents an email importancecorpus annotated through Amazon Me-chanical Turk (AMT).
Annotators anno-tate the email content type and email im-portance for three levels of hierarchy (se-nior manager, middle manager and em-ployee).
Each email is annotated by5 turkers.
Agreement study shows thatthe agreed AMT annotations are close tothe expert annotations.
The annotateddataset demonstrates difference in propor-tions of content type between different lev-els.
An email importance prediction sys-tem is trained on the dataset and identifiesthe unimportant emails at minimum 0.55precision with only text-based features.1 IntroductionIt is common that people receive tens or hundredsof emails everyday.
Reading and managing allthese emails consume significant time and atten-tion.
Many efforts have been made to address theemail overload problem.
There are studies mod-eling the email importance and the recipients?
ac-tions in order to help with the user?s interactionwith emails (Dabbish and Kraut, 2006; Dabbishet al., 2005).
Meanwhile, there are NLP studieson spam message filtering, email intention classi-fication, and priority email selection to reduce thenumber of emails to read (Schneider, 2003; Co-hen et al., 2004; Jeong et al., 2009; Dredze etal., 2009).
In our project, we intend to build anemail briefing system which extracts and summa-rizes important email information for the users.However, we believe there are critical com-ponents missing from the current research work.First, to the extent of our knowledge, there is lit-tle public email corpus with email importance la-beled.
Most of the prior works were either basedon surveys or private commercial data (Dabbishand Kraut, 2006; Aberdeen et al., 2010).
Second,little attention has been paid to study the differenceof emails received by people at different levels ofhierarchy.
Third, most of the prior works chosethe user?s action to the email (e.g.
replies, opens)as the indicator of email importance.
However, weargue that the user action does not necessarily in-dicate the importance of the email.
For example,a work-related reminder email can be more impor-tant than a regular social greeting email.
However,a user is more likely to reply to the later and keepthe information of the former in mind.
Specificallyfor the goal of our email briefing system, impor-tance decided upon the user?s action is insufficient.This paper proposes to annotate email impor-tance on the Enron email corpus (Klimt and Yang,2004).
Emails are grouped according to the re-cipient?s levels of hierarchy.
The importance ofan email is annotated not only according to theuser?s action but also according to the importanceof the information contained in the email.
Thecontent type of the emails are also annotated forthe email importance study.
Section 3 describe theannotation and analysis of the dataset.
Section 4describes our email importance prediction systemtrained on the annotated corpus.2 Related workThe most relevant work is the email corpus an-notated by Dredze et al.
(Dredze et al., 2008a;Dredze et al., 2008b).
2391 emails from inboxesof 4 volunteers were included.
Each volunteermanually annotated whether their own emails needto be replied or not.
The annotations are reliableas they come from the emails?
owners.
However, itlacks diversity in the user distribution with only 4volunteers.
Also, whether an email gets responseor not does not always indicate its importance.While commercial products such as Gmail PriorityInbox (Aberdeen et al., 2010) has a better cover-651age of users and decides the importance of emailsupon more factors1, it is unlikely to have their dataaccessible to public due to user privacy concerns.The Enron corpus is a public email corpuswidely researched (Klimt and Yang, 2004).
Lam-pert et al.
(2010) annotated whether an email con-tains action request or not based on the agreed an-notations of three annotators.
We followed sim-ilar ideas and labeled the email importance andcontent type with the agreed Amazon MechanicalTurk annotations.
Emails are selected from En-ron employees at different levels of hierarchy andtheir importance are labeled according to the im-portance of their content.
While our corpus canbe less reliable without the annotations from theemails?
real recipients, it is more diverse and hasbetter descriptions of email importance.3 Data annotation3.1 Annotation schemeAnnotators are required to select the importance ofthe email from three levels: Not important, Nor-mal and Important.
Not important emails con-tain little useful information and require no actionfrom the recipient.
It can be junk emails missedby the spam filter or social greeting emails that donot require response from the recipient.
Importantemails either contain very important informationto the recipient or contain urgent issues that re-quire immediate action (e.g.
change of meetingtime/place).
Normal emails contain less impor-tant information or contain less urgent issues thanImportant emails.
For example, emails discussingabout plans of social events after work would typ-ically be categorized as Normal.We also annotate the email content type as itreveals the semantic information contained in theemails.
There are a variety of email content typedefinitions (Jabbari et al., 2006; Goldstein et al.,2006; Dabbish et al., 2005).
We choose Dabbish etal.
?s definition for our work.
Eight categories areincluded: Action Request, Info Request, Info At-tachment, Status Update, Scheduling, Reminder,Social, and Other.
While an email can containmore than one type of content, annotators are re-quired to select one primary type.1Including user actions and action time, the user actionsnot only include the Reply action but also includes actionssuch as opens, manual corrections, etc.3.2 Annotation with AMTAmazon Mechanical Turk is widely used in dataannotation (Lawson et al., 2010; Marge et al.,2010).
It is typically reliable for simple tasks.
Ob-serving the fact that it takes little time for a user todecide an email?s importance, we choose AMT todo the annotations and manage to reduce the an-notation noise through redundant annotation.Creamer et al.
categorized the employees of theEnron dataset to 4 groups: senior managers, mid-dle managers, traders and employees2(Creameret al., 2009).
We hypothesized that the types ofemails received by different groups were differentand annotated different groups separately.
Basedon Creamer et al?s work, we identified 23 seniormanagers with a total of 21728 emails, 20 middlemanagers with 13779 emails and 17 regular em-ployees with 12137 emails.
The trader group wasnot annotated as it was more specific to Enron.
Foreach group, one batch of 750 assignments (email)was released.
The emails were randomly selectedfrom all the group members?
received emails (toor cc?ed).
Turkers were presented with all de-tails available in the Enron dataset, including sub-ject, sender, recipients, cclist, date and the con-tent (with history of forwards and replies).
Turkerswere required to make their choices as they werein the position.3Each assignment was annotatedby 5 turkers at the rate of $0.06 per Turker assign-ment.
The email type and the email importanceare decided according to the majority votes.
If anemail has 3 agreed votes or higher, we call thisemail agreed.
Table 1 demonstrates the averagetime per assignment (Time), the effectively hourlyrate (Ehr), the number of emails with message typeagreed (#TypeAgreed), importance agreed (#Im-poAgreed) and both agreed (#AllAgreed).
We findthat #AllAgreed is close to #TypeAgreed, whichindicates a major overlap between the agreed typeannotation and the agreed importance annotation.3.3 Data discussionIn this paper we focus on the AllAgreed emails tomitigate the effects of annotation noise.
Table 2demonstrates the contingency table of the corpus.2Senior managers include CEO, presidents, vice presi-dents, chief risk officer, chief operating officer and manag-ing directors.
The other employees at management level arecategorized to middle managers3E.g.
instruction of the senior manager batch: Imagineyou were the CEO/president/vice president/managing direc-tor of the company, categorize the emails into the three cate-gories [Not Important], [Normal], [Important].652Level Time (s) Ehr ($) #All #TypeAgreed #ImpoAgreed #AllAgreedSenior (23) 40 5.400 750 589 656 574Middle (20) 33 6.545 750 556 622 550Employee (17) 31 6.968 750 593 643 586Table 1: AMT annotation results, notice that #AllAgreed is close to #TypeAgreedAct.Req Info.Req Info Status Schedule Reminder Social Other AllSenior 60 49 255 57 43 4 68 38 574Not 0 0 0 0 0 0 33 30 63Normal 38 37 231 51 37 4 35 8 441Important 22 12 24 6 6 0 0 0 70Middle 82 53 261 22 49 0 37 46 550Not 0 0 1 0 0 0 10 32 43Normal 64 47 247 22 49 0 27 14 470Important 18 6 13 0 0 0 0 0 37Employee 61 65 326 22 29 1 52 30 586Not 0 0 1 0 0 0 8 26 35Normal 43 62 315 22 27 1 44 4 518Important 18 3 10 0 2 0 0 0 33Table 2: Contingency table of content type and importance of AllAgreed emails; bold indicates theproportions of this category is significantly different between groups (p<0.05)A potential issue of the corpus is that the impor-tance of the email is not decided by the real emailrecipient.
To address this concern, we comparedthe AllAgreed results with the annotations froman expert annotator.
50 emails were randomly se-lected from AllAgreed emails for each level.
Theannotator was required to check the backgroundof each recipient (e.g.
the recipient?s position inthe company at the time, his/her department infor-mation and the projects he/she was involved in ifthese information were available online) and judgethe relationship between the email?s contacts be-fore annotation (e.g.
if the contact is a familymember or a close friend of the recipient).
Agree-ment study shows a Kappa score of 0.7970 for thesenior manager level, 0.6420 for the middle man-ager level and 0.7845 for the employee level.
Itdemonstrates that the agreed Turker annotationsare as reliable as well-prepared expert annotations.We first tested whether the content type pro-portions were significantly different between dif-ferent levels of hierarchy.
Recipients with morethan 20 emails sampled were selected.
A vector ofcontent type proportions was built for each recipi-ent on his/her sampled emails.
Then we appliedmultivariate analysis of variance (MANOVA) totest the difference in the means of the vectorsbetween levels4.
We found that there were sig-nificant differences in proportions of status up-date (p=0.042) and social emails (p=0.035).
Thisagrees with the impression that the senior man-agers spend more time on project management andsocial relationship development.
Following thesame approach, we tested whether there were sig-nificant differences in importance proportions be-tween levels.
However, no significant differencewas found while we can observe a higher portionof Important emails in the Senior group in Table 2.In the next section, we further investigate the rela-tionship between content type and message impor-tance using the content type as a baseline featurein email importance prediction.4 Email importance predictionIn this section we present a preliminary study ofautomatic email importance prediction.
Two base-lines are compared, including a Majority baselinewhere the most frequent class is chosen and a Typebaseline where the only feature used for classifica-tion is the email content type.4We cannot use Chi-square to test the difference betweengroups directly on Table 2 as the emails sampled do not sat-isfy the independence consumption if they come from thesame recipient653Features Acc Kappa P(U) R(I)Sr. MgrsMajority 76.83 0 0 0Type 68.78 37.93 58.76 44.81Text 76.34 26.96 71.83?
14.67?Text+Type 78.43 33.80 75.99?
12.13?MgrsMajority 85.45 0 0 0Type 69.81 32.75 50.47 49.80Text 87.09 26.64 54.67 4.17?Text+Type 88.55 36.42 63.80?
7.59?EmpMajority 88.39 0 0 0Type 80.34 38.63 40.21 45.12Text 88.83 30.98 63.83?
1.67?Text+Type 89.16 36.71 72.50?
1.67?Table 3: Results of Experiment 1; ?
indicates sig-nificantly better than the Type baseline; ?
indicatessignificantly worse than the Type baseline; boldindicates better than all other methods.
With onlytext-based features, the system achieves at least54.67 precision in identifying unimportant emails.Groups Acc Kappa P(U) R(I)Sr. Mgrs 77.70 19.24 65.22 10.00Mgrs 83.27 30.03 61.90 2.70Emp 83.10 33.89 46.94 33.33Table 4: Cross-group results of Experiment 24.1 Feature extractionWhile prior works have pointed out that the so-cial features such as contacting frequency are re-lated to the user?s action on emails (Lampert et al.,2010; Dredze et al., 2008a), in this paper we onlyfocus on features that can be extracted from text.N-gram features Binary unigram features areextracted from the email subject and the emailcontent separately.
Stop words are not filtered asthey might also hint the email importance.Part-of-speech tags According to our observa-tion, the work-related emails have more contentwords than greeting emails.
Thus, POS tag fea-tures are extracted from the email content, includ-ing the total numbers of POS tags in the text andthe average numbers of tags in each sentence.55The Part-of-speech (POS) tags are tagged with the Stan-ford CoreNLP toolkit (Manning et al., 2014; Toutanova et al.,2003), containing 36 POS tags as defined in the Penn Tree-bank annotation.Length features We observe that work-relatedemails tend to be more succinct than unimpor-tant emails such as advertisements.
Thus, lengthfeatures are extracted including the length of theemail subject and email content, and the averagelength of sentences in the email content.Content features Inspired by prior works(Lampert et al., 2010; Dredze et al., 2008a), fea-tures that provide hints of the email content are ex-tracted, including the number of question marks,date information and capitalized words, etc.4.2 ExperimentsWe treat our task as a multi-class classificationproblem.
We test classifications within-level andcross-level with only text-based features.Experiment 1 Each level is tested with 10-foldcross-validation.
SVM of the Weka toolkit (Hall etal., 2009) is chosen as the classifier.
To address thedata imbalance problem, the minority classes ofthe training data are oversampled with the WekaSMOTE package (Chawla et al., 2002).
The pa-rameters of SMOTE are decided according to theclass distribution of the training data.Experiment 2 The classifiers are trained on twolevels and tested on the other level.
Again, SVM ischosen as the model and SMOTE is used to over-sample the training data.4.3 EvaluationKappa6and accuracy are chosen to evaluate theoverall performance in prediction.
For our emailbriefing task specifically, precision in unimpor-tant email prediction P(U) (avoid the false recog-nition of unimportant emails) and recall in impor-tant email prediction R(I) (cover as many impor-tant emails as possible) are evaluated.
Paired t-tests are utilized to compare whether there are sig-nificant differences in performance (p < 0.05).As demonstrated in Table 3, the text-based fea-tures are useful for the prediction of unimportantemail classification but not as useful for the recog-nition of important emails.
It also shows thatthe content type is an important indicator of theemail?s importance.
While the content type is notalways accessible in real life settings, the resultsdemonstrate the necessity of extracting semanticinformation for email importance prediction.
InTable 4, precision of unimportant email prediction6The agreement between the system and the majority la-bels from the Mechanical Turk654is higher on the manager levels but lower on theemployee level.
This indicates a potential differ-ence of email features between the manager levelsand the employee level.5 Conclusion and future workIn this paper we present an email importance cor-pus collected through AMT.
The dataset focuseson the importance of the information contained inthe email instead of the email recipient?s action.The content type of the email is also annotated andwe find differences in content type proportions be-tween different levels of hierarchy.
Experimentsdemonstrate that the content type is an importantindicator of email importance.
The system basedon only text-based features identifies unimportantemails at minimum 0.5467 precision.Agreement study shows that the agreed Turkerannotations are as good as annotations of well-prepared expert annotators.
We plan to increasethe size of our dataset through AMT.
We expectthe dataset to be helpful for studies on email over-load problems.
Meanwhile, we are aware that thecurrent corpus lacks social and personal informa-tion.
We believe features regarding such informa-tion (e.g.
the recipient?s email history with thecontact, the recipient?s personal preference in cat-egorizing emails, etc.)
should also be incorporatedfor importance prediction.AcknowledgmentsWe would like to thank Zhe Feng, Doo Soon Kim,Lin Zhao, Sai Sudharsanan and other employeesof the Bosch RTC 1.2 group for their helpful feed-back, Prof. Diane Litman for her instructions tothe first author and all the anonymous reviewersfor their suggestions.This research is supported by the Research andTechnology Center of Robert Bosch LLC.ReferencesDouglas Aberdeen, Ondrej Pacovsky, and AndrewSlater.
2010.
The learning behind gmail priorityinbox.
In LCCC: NIPS 2010 Workshop on Learningon Cores, Clusters and Clouds.Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall,and W Philip Kegelmeyer.
2002.
Smote: syntheticminority over-sampling technique.
Journal of artifi-cial intelligence research, 16(1):321?357.William W. Cohen, Vitor R. Carvalho, and Tom M.Mitchell.
2004.
Learning to classify email into?speech acts?.
In Dekang Lin and Dekai Wu, edi-tors, Proceedings of EMNLP 2004, pages 309?316,Barcelona, Spain, July.
Association for Computa-tional Linguistics.Germ?an Creamer, Ryan Rowe, Shlomo Hershkop, andSalvatore J Stolfo.
2009.
Segmentation and auto-mated social hierarchy detection through email net-work analysis.
In Advances in Web Mining and WebUsage Analysis, pages 40?58.
Springer.Laura A Dabbish and Robert E Kraut.
2006.
Emailoverload at work: an analysis of factors associatedwith email strain.
In Proceedings of the 2006 20thanniversary conference on Computer supported co-operative work, pages 431?440.
ACM.Laura A Dabbish, Robert E Kraut, Susan Fussell, andSara Kiesler.
2005.
Understanding email use: pre-dicting action on a message.
In Proceedings of theSIGCHI conference on Human factors in computingsystems, pages 691?700.
ACM.Mark Dredze, Tova Brooks, Josh Carroll, Joshua Ma-garick, John Blitzer, and Fernando Pereira.
2008a.Intelligent email: reply and attachment prediction.In Proceedings of the 13th international conferenceon Intelligent user interfaces, pages 321?324.
ACM.Mark Dredze, Hanna M Wallach, Danny Puller, TovaBrooks, Josh Carroll, Joshua Magarick, John Blitzer,Fernando Pereira, et al.
2008b.
Intelligent email:Aiding users with ai.
In AAAI, pages 1524?1527.Mark Dredze, Bill N Schilit, and Peter Norvig.
2009.Suggesting email view filters for triage and search.In IJCAI, pages 1414?1419.Jade Goldstein, Andres Kwasinksi, Paul Kingsbury,Roberta Evans Sabin, and Albert McDowell.
2006.Annotating subsets of the enron email corpus.
InCEAS.
Citeseer.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H Witten.2009.
The weka data mining software: an update.ACM SIGKDD explorations newsletter, 11(1):10?18.Sanaz Jabbari, Ben Allison, David Guthrie, and LouiseGuthrie.
2006.
Towards the orwellian nightmare:separation of business and personal emails.
InProceedings of the COLING/ACL on Main confer-ence poster sessions, pages 407?411.
Associationfor Computational Linguistics.Minwoo Jeong, Chin-Yew Lin, and Gary Geunbae Lee.2009.
Semi-supervised speech act recognition inemails and forums.
In Proceedings of the 2009 Con-ference on Empirical Methods in Natural LanguageProcessing: Volume 3-Volume 3, pages 1250?1259.Association for Computational Linguistics.Bryan Klimt and Yiming Yang.
2004.
The enron cor-pus: A new dataset for email classification research.In Machine learning: ECML 2004, pages 217?226.Springer.655Andrew Lampert, Robert Dale, and Cecile Paris.
2010.Detecting emails containing requests for action.
InHuman Language Technologies: The 2010 AnnualConference of the North American Chapter of theAssociation for Computational Linguistics, pages984?992.
Association for Computational Linguis-tics.Nolan Lawson, Kevin Eustice, Mike Perkowitz, andMeliha Yetisgen-Yildiz.
2010.
Annotating largeemail datasets for named entity recognition with me-chanical turk.
In Proceedings of the NAACL HLT2010 Workshop on Creating Speech and LanguageData with Amazon?s Mechanical Turk, pages 71?79.Association for Computational Linguistics.Christopher D. Manning, Mihai Surdeanu, John Bauer,Jenny Finkel, Steven J. Bethard, and David Mc-Closky.
2014.
The Stanford CoreNLP natural lan-guage processing toolkit.
In Proceedings of 52ndAnnual Meeting of the Association for Computa-tional Linguistics: System Demonstrations, pages55?60.Matthew Marge, Satanjeev Banerjee, and Alexander IRudnicky.
2010.
Using the amazon mechanical turkto transcribe and annotate meeting speech for extrac-tive summarization.
In Proceedings of the NAACLHLT 2010 Workshop on Creating Speech and Lan-guage Data with Amazon?s Mechanical Turk, pages99?107.
Association for Computational Linguistics.Karl-Michael Schneider.
2003.
A comparison of eventmodels for naive bayes anti-spam e-mail filtering.In Proceedings of the tenth conference on Euro-pean chapter of the Association for ComputationalLinguistics-Volume 1, pages 307?314.
Associationfor Computational Linguistics.Kristina Toutanova, Dan Klein, Christopher D Man-ning, and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In Proceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology-Volume 1, pages 173?180.
Association for Compu-tational Linguistics.656
