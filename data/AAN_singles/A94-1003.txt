Language Determination: Natural Language Processing from ScannedDocument ImagesPenelope Sibun & A. Lawrence SpitzFuji Xerox Palo Alto Laboratory3400 HiUview AvenuePalo Alto, CA 94304 USAsibun@ pal.
xerox.cornAbstractMany documents are available to a computeronly as images from paper.
However, most nat-ural language processing systems expect heirinput as character-coded text, which may bedifficult or expensive to extract accuratelyfrom the page.
We describe amethod for con-verting a document image into character shapecodes and word shape tokens.
We believe thatthis representation, which is both cheap androbust, is sufficient for many NLP tasks.
In thispaper, we show that the representation is suffi-cient for determining which of 23 languagesthe document is written in, using only a smallnumber of features, with greater than 90%accuracy overall.1 I n t roduct ionComputational linguists work with texts.
Computationallin!mistic applications range from natural languageunderstanding to information retrieval to machine trans-lation.
Such systems usually assume the language of thetext that is being processed.
However, as corporabecome larger and more diverse this assumptionbecomes less warranted.
Attention is now turning to theissue of determining the language or languages of a textbefore further processing is done.
Several sources ofinformation for language determination have been tried:short words (Kulikowski 1991, Ingle 1976); n-grams ofwords (Batchelder 1992); n-grams of characters (Cavner& Trenkle 1994); diacritics and special characters(Beesley 1988, Newman 1987); syllable characteristics(Mustonen 1965); morphology and syntax (Ziegler1991).
F~ch of these approaches i  prGmising althoughnone is completely accurate.
More fundamentally, manyrely on relatively large amounts of text data and all relyon data in the form of character codes (e.g., ASCID.In today's world of text-based information, how-ever, not all sources of text will be character coded.Many documents such as incoming faxes, patent appli-cations, and office memos are only accessible on paper.Processes such as Optical Character Recognition (OCR)have been developed for mapping paper documents intocharacter-coded t xt.However, for applications like OCR, it is desirableto know the language adocument is in before trying todecode its characters.
There appears to be a fundamentalCatch-22: natural anguage processing systems want tobe able to work automatically with arbitrary documents,many of which may be available only on paper, and inthe process, they minimally need to know which lan-guage or languages are present.
The algorithms citedabove can determine a document's language, but theyrequire a character-coded representation f the text.OCR can produce such a representation, but OCR doesnot work well unless the language(s) of the documentare known.
So how can the language of a paper docu-ment be determined?We have developed a method which reliably deter-mines the language or lan?xlages of a document image.In this paper, we discuss Roman-alphabet languagessuch as English, Polish, and Swahili; see Spitz (1994)for a discussion of the determination f Asian-script lan-guages.
Our method finesses the problems inherent inmapping from an image to a character-coded r presenta-tion: we map instead from the image to a shape-basedrepresentation.
The basal representation is the charactershape code of which there are a small number.
Theseshape codes are aggregated into word shape tokenswhich are delimited by white space.
From examiningthese word shape tokens we can determine the languageof the document.
An example of the transformation fromcharacter codes to character shape codes is shown in fig-urel.Character codesConfidence in the internationalmonetary system was shaky enough be-fore last week's action.Character shape codesAxxAAxxxx ix AAx ixAxxxxAixxxAxxxxAxxg xgxAxx xxx xAxAg xxxxgA Ax-Axxx AxxA xxxA'x xxAixx.Figure 1: Character code representation a d charactershape code representation.The shape-based representation f a document isproving to be a remarkably rich source of information.While our initial goal has been to use it for languageidentification, in support of downstream OCR pro-15cesses, we are finding that his representation may itselfbe sufficient for natural language applications such asdocument indexing and content characterization (seeNakayama (this volume), Sibun & Farrar 1994).
We fredthese indications exciting because OCR is an expensive,slow, and often inaccurate process, especially inthe pres-ence of printing and scanning artifacts uch as broken ortouching characters or skew or curvature of text lines.Thus, if our technique allows natural language process-ing systems to apply OCR selectively or to side-stepOCR entirely, such systems will become faster, lessexpensive, and more robust.In this paper, we first explain the background ofoursystem that constructs character shape codes and wordshape tokens from a document image.
We next describeour method for language determination from this shape-based representation, and demonstrate our approachusing only the three languages F.nglish, French, and Ger-man.
We then describe an automated version of this pro-cess that allows us to apply our techniques to an arbitraryset of lan~ruages and show its performance on23 Roman-alphabet languages.2 Character  shape codes  and wordshape tokensOur determinations about document characteristics aremade neither on the raw image I nor on the charactercodes by which the document can be represented.
Thedeterminations are made on a shape-based representationbuilt of a novel component, the character shape code(Spitz 1993).Four horizontal lines define the boundaries of threesignificant zones on each text line (see figure 2).
Thearea between the bottom and the baseline is thedescender zone; the area between the baseline and thetop of such characters as x is the x zone; and the areabetween the x-height level and the top is the ascenderzone.Top x-heightFigure 2: A text image showing the text line parameterpositions: Top, x-height, Baseline and Bottom.Characterizations of the number of connected com-pouents in a character cell and, in some instances, theiraspect ratios, contribute to the coding.
Thus most charac-ters can be readily mapped from their positions relativeto the baseline and x-height to a small number of distinctcodes (see figure 3).
2Charactershape codeACharacterA -Zbdfhk l t~0-9#$&/@\[x acemnor  suvwxzi i~ad66?6fif.Leig gPqY9j JU a~zt~O0Figure 3: Character shape codes.2.1 Typesetting effectsTypesetters use different conventions.
For example, inGerman text 0 may be set as ue and 8 may be set as.Therefore, there may be several-to-one mappings oftypeset information to character shape codes, since iimaps to U andue toxx.If this shape mapping can be done from documentimages, it can more trivially be accomplished from char-acter-coded documents (e.g., ASCII, ISO-Latin-1, JIS,Unicode), providing, of course, that the method ofencoding is known.2.2 Computational complexityOur approach takes on a much less difficult problem thandoes OCR.
There is no need to investigate he free struc-ture of character images, the number of classes is small,and measurements are largely independent of font ortypeface.
As a result, the process of classifying text intocharacter shape codes and aggregating those codes intoword shape tokens is two to three orders of magnitudefaster than current OCR technology.3 Language determlnat,on"We have found that we can readily distinguish the lan-guage of a document for 23 Roman-alphabet (mostlyEuropean) languages from a relatively small text.
Thistechnique xploits the high frequency of short words insuch languages and the diversity of their word shapetoken representations.In this section, we describe our method for deter-mining a document's language from the shape-basedrepresentation derived from the image (some of this1.
Document images may be obtained by scanning ofpaper documents, by retrieval from a document image data-base, or by digital rendering of a high level representation ofthe document.2.
This paper adepts the following conventions: mono-spaced  to represent input characters, boldface to representthe character shape codes (A, x, i, g, J, U), and sans-serif torepresent typographic conventions.16work has been reported in Nakayama & Spitz 1993).
Oursystem learns how to discriminate a set of languages;then, for any input document, he system determines towhich language it belongs.
Our method uses the statisti-cal technique of Linear Discriminate Analysis (LDA).First, we demonstrate the method using a hand-selectedset of distinguishing features for a small set of lan?uages.In section 4, we describe our process for automating theselection of distinguishing features across an arbitrarynumber of lan?uages, and show the results on a corpusthat includes documents from 23 languages.Our initial set of discriminable anguages comprisedEnglish, French, and German.
To ascertain the set of dis-criminating features, we built a training corpus ofapproximately 15 scanned images of one-page docu-ments for each language.
We tokenized these images fol-lowing the procedure described in section 2.
Thisresulted in 7621 tokens from l~.,glisla, 6826 tokens fromFrench, and 5472 tokens from German.
We then rankedthe frequency of word shape tokens across each corpusand noted the ten most frequent tokens.
By comparingthese top ten word shape tokens for each of the lan-guages, we were able to select one per language that wasboth frequent in that lanouage and less frequent in theother languages.
Intuitively, each of these tokens is char-acteristic of its language; therefore, we call these charac-teristic tokens (see figure 4).
The characteristic oken forEnglish is AAx; AAx constitutes 7% of the tokens in theToken:,:-.:.:.r.r.'.'.'.':-:-:-:-:..r.r."
: : : : : : : : : : : : : : : : : : : : : : :  : : : : : : ' :< ' : ' : ' : ' : ' :xA 2 of?x 4 isxxA 5 andxx 6Axx 9xxx 8gxxEnglish French GermanRank!
Word Rank Word Rank Word7: : : : : : :  : :  : : : : : : : : : :  : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :2 en3 les4 aux5 pas6103 auf2 an1 der5 werFigure 4: Most frequent word shape tokens in English,French and German: the top five for each language areshown; rankings of these are shown for the otherlanguages when they fall in the top ten; shadingindicates the characteristic token for each language; andcommon words that map to the top five tokens for eachlanguage are shown.Axx constitutes 6%.
However, of the five, only Aix israre in the other languages.
While Ax is frequent in allthree corpora, it is overwhelmingly frequent in French,where it makes up 11% of the tokens (vs. 4% for Englishand 2% for German).
These differences in the distribu-tion of the characteristic tokens in the three corpora resufficient for LDA to correctly identify each languagealmost every time (see figure 5).
3 The documents arefrom the training corpus: by a process called cross-vali-dation, each was removed from the training corpus oneat a time and classified based on the discriminatingresults from training on the rest of the corpus.Language assignedEnglish French GermanLanguage English 13of French 17 1document German I 14Figure 5: Number of documents from each cot msassigned to each language.It may be noted that each of the top five word shapetokens in each of the English, French, and German cor-pora is a mapping of dosed class words such as deter-miners, conjunctions, and pronouns.
This is notsurprising, since dosed class words are frequent in Euro-pean languages.
Of course, other words map to theseword shape tokens too.
For example, in English, theword f lu  maps to AAx.
However the overwhelmingproportion of AAx tokens in the English corpus are map-pings of the.
Since the  is such a common word inEnglish, we can expect AAx to be characteristic of anyshape-level representation f an F.nglish document.
Sim-ilar situations obtain in the other languages.While it may seem fortuitous that in English AAx isvirtually always a mapping of the,  unique word shapetokens are more common in Roman-alphabet languagesthan one might suppose.
We mapped an English lexiconof surface ft~ms into word shape tokens and discoveredthat 20% of the resulting word ~ape tokens were unique;examples include the surface forms apple  andapples.4 Automated language determina-tionIn the previous ection, we discussed the selection of dis-criminating word shape tokens by hand.
We nowdescribe our method for automating this process.
Wehave been able to use this technique to discover a dis-criminating set of tokens for a large fraction of thelanguages written in the Roman alphabet.
We initiallytested this automated technique by recapitulating ourEnglish corpus and is quite rare in the others.
In the Ger-man corpus, Aix is not the most frequent token: xx, xxA,Aix, and xxx each make up about 3% of the corpus while3.
In the case of the German document that was mis-classified, examination of the image reveals that, due toprinting and scanning artifacts, many  characters axe artifac-tually touching each other.17work done by hand in discriminating English, French,and German.
We then applied the technique to a 755-document corpus comprising 23 languages.4.1 The automated methodWhile it is easy to hand-select a single discriminatingtoken for each of a few languages, the task becomesmore complex as the number of languages grows.
Fur-ther, a single feature per language may no longer besufficient; a profile, or vector of features, for each lan-guage would be more robust.For the automated method, acorpus for each of thelanguages i scanned and tokeuized, and the tokens aresorted by frequency.
The n most frequent tokens for eachcorpus are selected.
We apply stepwise discriminantanalysis, avariant of LDA, to this token set: variables areselected one by one according to their ability to discrim-inate between languages.
The optimal value of n has notyet been determined.
We need to gather enough discrim-inating tokens to characterize the languages as com-pletely as possible.
However, if we use too many, theaccuracy of the classification may actually be degraded;further, relatively uncommon tokens may improve per-formance on test data but may not work well in general.As we discuss below, n = 5 suffices for three languages,but may not be optimal for 23.There are several considerations for ensuring thatthis process is robust.
The size of the corpus for each lan-guage must be sufficiently large in terms of both thenumber of documents and the total number of wordshape tokens.
The number of documents must be largeenough to enable the LDA testing procedure to system-atically eliminate some of them for cross validation with-out skewing the overall characteristics of the corpus.
Thenumber of word shape tokens must be large enough to bereflective of the language in which the documents arewritten to allow for accurate comparison between lan-guages.
A further consideration is that he number of dis-criminating tokens used by the LDA system should beconsiderably smaller than the number of documents.For our initial test we selected the five most frequentword shape tokens from each of English, F~nch, andGerman; this fo~aied a set of ten tokens (because of over-lap between corpora).
Using stepwise discriminant anal-ysis, the system fonnd the best way to use the tokens byselecting the single token that was most discriminatingand then for each of the remaining tokens adding the nextmost discriminating tokens given the ones that hadalready been selected.
This resulted in a ranking of ninediscriminating tokens (Ax, xA, ix, AIX, Axx, xx, AAx,xxA, xxx).
The tenth was not found to improve the reli-ability of the discrimination; in fact accuracy peaked atfour tokens.We compared the performance of the automatedsystem with that using the hand-selected tokens.
Whenthe top three automatically-selected tokens were used,performance was comparable to that of the three hand-selected tokens.
Interestingly, there is no overlap in themisclassification of documents.
Using four automati-cally-selected tokens, the system classified all but onedocument correctly (see figure 6).Language Englishof Frenchdocument GermanLanguage AssignedEnglish13French German1814Figure 6: Assignment of documents o language usingfour automatically-selected discriminating tokens.4.2 Automated determination for manylanguagesWe have constructed a database of 755 one-page docu-ments in 23 languages including virtually everyEuropean language written in the Roman alphabet.
Thereare 18 Indo-European languages: Afrikaans, Croatian,Czech/Slovak 4, Danish, Dutch, English, French, Gaelic,German, Icelandic, Italian, Norwegian, Polish, Portu-guese, Rumanian, Spanish, Swedish, and Welsh.
Thereare two Uralic languages: Finni.~h and Hungarian.Finally, we include three languages from disparate fam-ilies: Turkish, Swahili~ and Viemamese.To construct a set of discriminating features, weselected the five most frequent word shape tokens fromeach language.
Because of overlap, this resulted in 23tokens.
Some of these discriminating tokens have a highfrequency across languages; in fact, xx appears in the topfive of 22 of the languages we examined.
However, evenwhen we consider 23 languages, there are eight tokensappearing in the top five of one language which do notappear in the top five of any others.
(This does not meanof course, that these tokens do not appear in other lan-guages at all, but simply that they are relatively muchless frequent.)
The 23 tokens comprise the set (x, xx,xxx, xxxx, i, ix, xi, xix, A, AAx, Ax, AxA, AxAx, Axx,Axxx, xA, xxA, Ai, AIX, g, gx, xg, xxg, jx).As before, we used LDA to build a statistical modelof the language categorizations, and by cross validationtested the accuracy of the model (see figure 7).
Our over-all accuracy is better than 90%, while the accuracy forindividual anguages varies between 100% and 75%,with an outlier of 44% for Czech/Slovak.
Examination ofmisclassifications proves somewhat instructive, as canbe seen in the confusion matrix in figure 8.
For example,Dutch and Afrikaans are closely related languages, andthe only error in either language is the categorization fone Afrikaans document as Dutch.
Among the five4.
We initially considered Czech and Slovak as separatelanguages, but this yielded worse results than combiningthem.
We feel our decision was legitimate because ~Slovak issimilar enough to Czech to be considered by some as merelya dialect" despite "the existence of slightly different alpha-bets, as well as distinct litoratures ~ (Katzner 1986, p 91).18Romance languages - French, Italian, Spanish, Portu-guese, Rumaulan - nine of the ten classification errorsare within that language family.
For the Scandinavianlanguage family - Danish, Norwegian, Swedish, and Ice-landic - the pattern is less clear.
Two Norwegian docu-ments are classified as Icelandic, but the three othererrors in that family are classifications outside of thefamily.Ace Ace Language abbr (%) Language abbr (%)Afrikaans af 97 Italian it 95Croatian cr 100 Norwegian no 95Czech/Slovak cs 44 Polish po 100Danish da 96 Portuguese pt 96Dutch du 100 Rumanian ru 93English en 95 Spanish sp 95Finnish fi 75 Swahili sa 97French fr 92 Swedush sw 98Gaelic ga 86 Turkish tu 93German ge 97 Vietnamese vi 100Hungarian hu 94 Welsh we 97Icelandic ic 96Figure 7: Language detection accuracy.
The abbre-viations hown are used as indices in figure 8.Croatian, Czech/Slovak, and Polish are all Slaviclanguages; Hungarian and Finnish are related to eachother but not to any other European languages.
However,there is a large cluster of errors within the set of thesefive languages.
Most of these errors are for Czech/Slo-yak documents; in fact, Czech/Slovak was recognizedfar less accurately than any other language and it isunclear why.
It may be the case that many of these doe-uments are of poor quality.
Seventeen of the 69 errorsseem to be random; while we are working to reduce sucherrors, it is unlikely that we can eliminate them entirely.It is possible that 23 discriminating tokens is not suffi-cient; since the accuracy has been improved by the addi-tion of each new token, adding several more maycontinue the improvement.4.3 Discussion of methodologyWhile LDA has proved adequate, there are some draw-backs to this technique.
We are somewhat disappointedby the system's accuracy.
Examination of token frequen-cies suggests that the profiles for each language aredistinct enough that 90% should be a lower bound onclassification accuracy.
However, for several languagesthe accuracy was much lower, and for many more it wasnot much better than 90%.
A more troubling problem isthe instability of the model.
When we add or delete lan-guages, overall accuracy fluctuates between 80% and93%.
This suggests that removing al~_nguage affects thetypical distribution across all lan?uages, which shouldnot be the case.
It is difficult o identify the underlyingcauses of both of these observations.
Finally, the resultsof LDA are difficult o interpret.
All these considerationssuggest that LDA may not be the best technique to use.Therefore, we are exploring alternative statistical mod-els, such as classification trees, to fred an approach thatis more robust for our task.5 Compar ison with other methodsIt is difficult for us to compare our approach to othermethods of language determination.
Most sources wehave found are simply guides for librarians or translators.For example, Ingle (1976) found that the presence orabsence of specific one- or two-character words suff~,esto distinguish among 17 Roman-alphabet languages.There are several implemented systems, ome of whichreport on their accuracy, but none is addressing exactlythe same problem as ours: all work from character-codedtext.
However, it is useful to get a ballpark estimate ofthe accuracy to be expected of character-based systems.Batchelder (1992) trained neural networks to recog-nize 3-6 character words from 10 languages.
While hernetworks had high accuracy in recognizing words fromthe training set, their best-case performance on untrainedwords was 53%, thus making accurate determination fa document's language highly onlikely.Cavner and Trenlde (1994) used n-grams of charac-ters for n = 1 to 5.
Their task was not language determi-nation per se, but determining to which country'snewsgroup (in the netnews oc.culture hierarchy) a doc-ument belonged.
In each newsgroup, the documentswere written in either English or other language(s).
Fordocuments longer than 300 characters, the system deter-mined the correct newsgroup with 97% accuracy whenusing the 100 most frequent n-grams.
These results aregood, but the technique should be tested on a set of doc-uments for which the l~nguages are known and the topicsare varied.Kulikowski (1991) used a semi-automatic method todetermine a profile of frequent 2-3 character words fornine languages.
He claims at least 95% accuracy fordetermining that a single-language document is in one ofthe nine languages or in none of them.
Unforttmately hedoes not expand on this claim?
Henfich (1989) used cri-teria such as language-specific word-boundary charactersequences and common short words to determine the lan-guage of sentences inEnglish, French, or German.Mustonen (1965) used discriminant analysis to dis-tinguish English, Swedish, and Finnish words.
His sys-tem, which used 43 discriminating features, such asparticular letters and syllable types, performed with 76%accuracy.
This relatively poor performance is probablydue to the data being isolated words rather than docu-ments, though it may also be due to overfitting of the testdata by too many features (see section 4.1).We would like to emphasize that our statistics onword shape token distribution across the various lan-19Detected Language ~en ge du af fr it sp pt ru da no se ic galwe cr cs  pl hu fi tu sa vi ~ \ [~en 36 1 1 2ge 29 1 1du 28 0af 1 29 1fr 23 1 1 2it 35 1 1 2sp 39 2 2pt 1 25 1ru 3 38 3da 1 25 1no 39 2 2se 40!
1 1ic 23 1 1ga 2 2 1 31 5we 30 1 1cr 33 0cs 1 6 24 16 3 5 31pl 28 0hu 2 29 2fi 6 2 24 8tu 1 1 28 2sa 1 37 1vi 13 00 1 1 0 2 5 2 4 4 0 0 0 3 2 0 6 6 18 3 9 1 1 0 69Figure 8: Confusion matrix showing detection accuracy between l_a_nomaages.
Numbers On the majordiagonal indicate the number of correct classificaticms for each language.
Numbers off the diagonalindicate classification errors.guages are generated entirely from scanned images oftext.
We feel this is important because the text whoselanguage we are trying to identify should not be system-atically different in any way from the texts from whichthe discriminate analysis was generated.
For example,typographic conventions such as a ligature between avowel and an acute accent (as in characters like ~t) causethe character shape code recognizer to classify thesecharacters as A.
However, if we were working fromencoded on-line corpora we would "know" that such acharacter should be classified as i.6 Conc lus ionWe have described our method for generating wordshape tokens from images and have shown how thisshape-level representation f the text can be used forimportant tasks such as determining the language or lan-guages of a document.
We have shown that the methodcan discriminate among 23 languages with highaccuracy.Since our approach is statistical, the more text oursystem sees in a document image, the more reliably it candetermine the document's language.
So far, we have nottried to determine the language of a document shorterthan 27 words, and most of the documents we work withare a few hundred words long (2000-3000 characters).We are investigating the lower bound on the length oftexts whose language we can reliably determine.
In theideal case we would be able to detect he presence of avery few words of a secondary language interpolated intoa document predominated byanother language.In other work, we axe using the shape-level repre-sentation as input to higher-level natural language pro-cessing systems for rudimentary content analysis.However, many sorts of information, particularly stylecharacteristics, an be derived from the shape-level rep-20resentation directly.
For instance, since the number ofcharacter shape codes extracted form a document is com-parable to the number of characters, characterizationsabout word length in a shape-level representation applyas well to the character-coded version of the document.This word length characterization s ot perfect: ligaturesintroduce some uncertainty.
Additionally, braces, brack-ets, and parentheses which are typically set contiguouswith words, are currently mapped to A, this will affectword length counts.
We are refining the mapping toaccount for these delimiting characters.AcknowledgmentsWe thank David Hull for his expertise in developing sta-tistical methods and help in explaining them, ArleneHolloway for patiently scanning and processing most ofour document image database and helping to analyze theresults, Marti Hearst and Michael Berch for commentson drafts of this paper, and Jussi Karlgren for a last-minute reference.BibliographyBatchelder, Eleanor Olds, A Learning Experience:Training an Artificial Neural Network to Discrimi-nate Languages, Unpublished Technical Report,1992.Beesley, Kenneth R., Language Identifier: A ComputerProgram for Automatic Natural-Language Id ntifi-cation of On-line Text, Language at Crossroads:Proceedings of the 29th Annual Conference of theAmerican Translators Association, 12-16 Oct 1988,pp 47-54.Caviler, William B.
& Trenkle, John M., N-Gram BasedText Categorization, Proceedings of the ThirdAnnual Symposium on Document Analysis andInfccmation Retrieval, 11-13 April 1994, pp 161-169.Hem'ida, Peter, Language Identification for the Auto-matic Grapheme-to-Phoneme Conversion of For-eign Words in a German Text-to-Speech System,Proceedings of Eurospeech 1989.
European SpeechCommunication a d Technology, Paris, Sept. 1989,pp 220-223.Ingle, Norman C. A Language Identification Table, TheIncorporated Linguist vol.
15 no.
4 pp 98-101, 1976.Katzner, Kenneth, The Languages of the World, London:Routledge, 1986.KuKkowsld, Start, Using Short Words: A Language Iden-tification Algorithm, Unpublished Technical Report,1991.Mustonen, Seppo, Multiple Discriminant Analysis inLinguistic Problems, Statistical Methods in Linguis-tics, No.
4, Skriptor Fack, Stockholm, 1965, pp 37-44.Nakayama, Takehiro & Spitz, A. Lawrence, EuropeanLanguage Determination from Image, Proceedingsof the International Conference on Document Anal-ysis and Recognition, 20-22 Oct 1993, pp 159-162.Nakayama, Takehiro, Modeling Content Identificationfrom Document Images, this volume.Newman, Patricia, Foreign Language Identification:First Step in the Translation Process, Proceedingsof the 28th Annual Conference of the AmericanTranslators Association, 8-11 October 1987, pp509-516.Sibun, Penelope & David S. Farrar, Content Character-ization Using Word Shape Tokens, Procedings of the15th International Conference on ComputationalLinguistics, Kyoto, Japan, 1994, pp 686-690.Spitz, A. Lawrence, Generalized Line, Word and Char-acter Finding, Progress in Image Analysis and pro-cessing HI, Impedovo, Ed., World Scientific, 1993,pp 377-383.Spitz, A. Lawrence, Script and Language Determinationfrom Document Images, Proceexfmgs of the ThirdAnnual Symposium on Document Analysis andInformation Retrieval, 11-13 April 1994, pp 229-235.Ziegler, Douglas-Val, The Automatic Identification ofLanguages Using Linguistic Recognition Signals.Dissertation, State University of New York at Buf-falo, 1991.21
