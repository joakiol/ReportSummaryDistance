Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 129?137,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsImproving Translation Lexicon Induction from Monolingual Corpora viaDependency Contexts and Part-of-Speech EquivalencesNikesh Garera, Chris Callison-Burch, David YarowskyDepartment of Computer Science, Johns Hopkins UniversityBaltimore MD, USA{ngarera,ccb,yarowsky}@cs.jhu.eduAbstractThis paper presents novel improvementsto the induction of translation lexiconsfrom monolingual corpora using multilin-gual dependency parses.
We introduce adependency-based context model that in-corporates long-range dependencies, vari-able context sizes, and reordering.
It pro-vides a 16% relative improvement overthe baseline approach that uses a fixedcontext window of adjacent words.
ItsTop 10 accuracy for noun translation ishigher than that of a statistical translationmodel trained on a Spanish-English par-allel corpus containing 100,000 sentencepairs.
We generalize the evaluation toother word-types, and show that the per-formance can be increased to 18% rela-tive by preserving part-of-speech equiva-lencies during translation.1 IntroductionRecent trends in machine translation illustrate thathighly accurate word and phrase translations can belearned automatically given enough parallel trainingdata (Koehn et al, 2003; Chiang, 2007).
However,large parallel corpora exist for only a small frac-tion of the world?s languages, leading to a bottleneckfor building translation systems in low-density lan-guages such as Swahili, Uzbek or Punjabi.
Whileparallel training data is uncommon for such lan-guages, more readily available resources includesmall translation dictionaries, comparable corpora,and large amounts of monolingual data.The marked difference in the availability ofmonolingual vs parallel corpora has led severalresearchers to develop methods for automaticallylearning bilingual lexicons, either by using mono-lingual corpora (Rapp, 1999; Koehn and Knight,2002; Schafer and Yarowsky, 2002; Haghighi et al,2008) or by exploiting the cross-language evidenceof closely related ?bridge?
languages that have moreresources (Mann and Yarowsky, 2001).This paper investigates new ways of learningtranslations from monolingual corpora.
We extendthe Rapp (1999) model of context vector projectionusing a seed lexicon.
It is based on the intuition thattranslations will have similar lexical context, even inunrelated corpora.
For example, in order to translatethe word ?airplane?, the algorithm builds a contextvector which might contain terms such as ?passen-gers?, ?runway?, ?airport?, etc.
and words in tar-get language that have their translations (obtainedvia seed lexicon) in surrounding context can be con-sidered as likely translations.
We extend the basicapproach by formulating a context model that usesdependency trees.
The use of dependencies has thefollowing advantages:?
Long distance dependencies allow associatedwords to be included in the context vector evenif they fall outside of the fixed-window used inthe baseline model.?
Using relationships like parent and child in-stead of absolute positions alleviates problemswhen projecting vectors between languageswith different word orders.?
It achieves better performance than baselinecontext models across the board, and betterperformance than statistical translation modelson Top-10 accuracy for noun translation whentrained on identical data.129We further show that an extension based on part-of-speech clustering can give similar accuracy gainsfor learning translations of all word-types, deepen-ing the findings of previous literature which mainlyfocused on translating nouns (Rapp, 1999; Koehnand Knight, 2002; Haghighi et al, 2008).2 Related WorkThe literature on translation lexicon induction forlow-density languages falls in to two broad cate-gories: 1) Effectively utilizing similarity betweenlanguages by choosing a high-resource ?bridge?
lan-guage for translation (Mann and Yarowsky, 2001;Schafer and Yarowsky, 2002) and 2) Extractingnoisy clues (such as similar context) from mono-lingual corpora with help of a seed lexicon (Rapp,1999; Koehn and Knight, 2002; Schafer andYarowsky, 2002, Haghighi et al, 2008).
The lat-ter category is more relevant to this work and is ex-plained in detail below.The idea of words with similar meaning havingsimilar contexts in the same language comes fromthe Distributional Hypothesis (Harris, 1985) andRapp (1999) was the first to propose using context ofa given word as a clue to its translation.
Given a Ger-man word with an unknown translation, a Germancontext vector is constructed by counting its sur-rounding words in a monolingual German corpus.Using an incomplete bilingual dictionary, the countsof the German context words with known transla-tions are projected onto an English vector.
The pro-jected vector for the German word is compared tothe vectors constructed for all English words usinga monolingual English corpus.
The English wordswith the highest vector similarity are treated as trans-lation candidates.
The original work employed a rel-atively large bilingual dictionary containing approx-imately 16,000 words and tested only on a small col-lection of 100 manually selected nouns.Koehn and Knight (2002) tested this idea on alarger test set consisting of the 1000 most frequentwords from a German-English lexicon.
They alsoincorporated clues such as frequency and ortho-graphic similarity in addition to context.
Schaferand Yarowsky, (2002) independently proposed us-ing frequency, orthographic similarity and alsoshowed improvements using temporal and word-burstiness similarity measures, in addition to con-text.
Haghighi et al, (2008) made use of contex-tual and orthographic clues for learning a generativemodel from monolingual corpora and a seed lexicon.All of the aforementioned work defines contextsimilarity in terms of the adjacent words over a win-dow of some arbitary size (usually 2 to 4 words), asinitially proposed by Rapp (1999).
We show that themodel for surrounding context can be improved byusing dependency information rather than strictly re-lying on adjacent words, based on the success of de-pendency trees for monolingual clustering and dis-ambiguation tasks (Lin and Pantel, 2002; Pado andLapata, 2007) and the recent developments in multi-lingual dependency parsing literature (Buchholz andMarsi, 2006; Nivre et al, 2007).We further differentiate ourselves from previouswork by conducting a second evaluation which ex-amines the accuracy of translating all word types,rather than just nouns.
While the straightforward ap-plication of context-based model gives a lower over-all accuracy than nouns alone, we show how learn-ing a mapping of part-of-speech tagsets between thesource and target language can result in comparableperformance to that of noun translation.3 Translation by Context VectorProjectionThis section details how translations are discoveredfrom monolingual corpora through context vectorprojection.
Section 3.1 defines alternative ways ofmodeling context vectors, and including baselinemodels and our dependency-based model.The central idea of Rapp?s method for learningtranslations is that of context vector projection andvector similarity.
The goodness of semantic ?fit?
ofcandidate translations is measured as the vector sim-ilarity between two words.
Those vectors are drawnfrom two different languages, so the vector for oneword must first be projected onto the language spaceof the other.
The algorithm for creating, projectingand comparing vectors is described below, and illus-trated in Figure 1.Algorithm:1.
Extract context vectors:Given a word in source language, say sw, createa vector using the surrounding context wordsand call this reference source vector rssw for130Figure 1: Illustration of (Rapp, 1999) model for translating spanish word ?crecimiento (growth)?
via dependency context vectorsextracted from respective monolingual corpora as explained in Section 3.1.2source word sw.
The actual composition of thisvector varies depending on how the surround-ing context is modeled.
The context model isindependent of the algorithm, and various mod-els are explained in later sections.2.
Project reference source vector:Project all the source vector words contained inthe projection dictionary onto the vector spacefor the target language, retaining the countsfrom source corpus.
This vector now exists inthe target language space and is called the ref-erence target vector rtsw .
This vector may besparse, depending on how complete the bilin-gual dictionary is, because words without dic-tionary entires will receive zero counts in thereference target vector.3.
Rank candidates by vector similarity:For each word twi in the target language a con-text vector is created using the target languagemonolingual corpora as in Step 1.
Compute asimilarity score between the context vector oftwi = ?ci1, ci2, ...., cin?
and reference target vec-tor rtsw = ?r1, r2, ...., rn?.
The word with themaximum similarity score t?wi is chosen as thecandidate translation of sw.The vector similarity can be computed in anumber of ways.
Our setup we used cosinesimilarity:t?wi = argmaxtwi ci1?r1+ci2?r2+....+cin?rn?c2i1+c2i2+...+c2in?r21+r22+...+r2nRapp (1999) used l1-norm metric after nor-malizing the vectors to unit length, Koehn andKnight (2002) used Spearman rank order cor-relation, and Schafer and Yarowsky (2002) usecosine similarity.
We found that cosine simi-larity gave the best results in our experimentalconditions.
Other similarity measures may beused equally well.3.1 Models of ContextWe compared several context models.
Empirical re-sults for their ability to find accurate translations aregiven in Section 5.3.1.1 Baseline modelIn the baseline model, the context is computedusing adjacent words as in (Rapp,1999; Koehnand Knight, 2002; Schafer and Yarowsky, 2002;Haghighi et al, 2008).
Given a word in source lan-guage, say sw, count all its immediate context wordsappearing in a window of four words.
The countsare collected seperately for each position by keepingtrack of four seperate vectors for positions -2, -1, +1and +2.
Thus each vector is a sparse vector, havingthe # of dimensions as the size of source languagevocabulary.
Each dimension is also reweighted bymultiplying the inverse document frequency (IDF)131Figure 2: Illustration of using dependency trees to model richer contexts for projectionas in the standard TF.IDF weighting scheme1.
Thesevectors are then concatenated into a single vector,having dimension four times the size of the vocabu-lary.
This vector is called the reference source vectorrssw for source word sw.3.1.2 Modeling context using dependency treesWe use dependency parsing to extend the con-text model.
Our context vectors use contexts derivedfrom head-words linked by dependency trees insteadof using the immediate adjacent lexical words.
Theuse of dependency trees for modeling contexts hasbeen shown to help in monolingual clustering tasksof finding words with similar meaning (Lin and Pan-tel, 2002) and we show how they can be effectivelyused for translation lexicon induction.Position Adjacent DependencyContext Context-2 para camino-1 el para+1 y prosperidad, y, el+2 la econo?micaTable 1: Contrasting context words derived from the adjacentvs dependency models for the above exampleThe four vectors for positions -1, +1, -2 and +2in the baseline model get mapped to immediate par-ent (-1), immediate child (+1), grandparent (-2) andgrandchild (+2).
An example of using the depen-dency tree context is shown in Figure 2, and the de-pendency context is shown in contrast with the ad-jacent context in Table 1, showing the selection ofmore salient words by using the dependencies.Note that while we are limiting to four positionsin the tree, it does not imply that only a maximum offour context words are selected since the word canhave multiple immediate children depending uponthe dependency parse of the sentence.
Hence, thisapproach allows for a dynamic context size, with the1In order to compute the IDF, while there were no clear doc-ument boundaries in our corpus, a virtual document boundarywas created by binning after every 1000 words.number of context words varying with the number ofchildren and parents at the two levels.Another advantage of this method is that it al-leviates the reordering problem as we use tree po-sitions (consisting of head-words) as compared tothe adjacent position in the baseline context model.For example, if the source spanish word to be trans-lated was ?prosperidad?, then in the example shownin Figure 2, in case of adjacent context, the con-text word ?econo?mica?
will show up in +1 positionin Spanish and -1 position in English (as adjectivescome before nouns in English) but in case of depen-dency context, the adjective will be the child of nounand hence will show up in +1 position in both lan-guages.
Thus, we do not need to use a bag of wordmodel as in Section 3 in order to avoid learning theexplicit mapping that adjectives and nouns in Span-ish and English are reversed.4 Experimental DesignFor our initial set of experiments we compared sev-eral different vector-based context models:?
Adjbow ?
A baseline model which used bag ofwords model with a fixed window of 4 words,two on either side of the word to be translated.?
Adjposn ?
A second baseline that used a fixedwindow of 4 words but which took positionalinto account.?
Depbow ?
A dependency model which did notdistinguish between grandparent, parent, childand grandparent relations, analogous to the bagof words model.?
Depposn ?
A dependency model which did in-clude such relationships, and was analogous tothe position-based baseline.?
Depposn + rev ?
The above Depposn model ap-plied in both directions (Spanish-to-Englishand English-to-Spanish) using their sum as thefinal translation score.We contrasted the accuracy of the above methods,which use monolingual corpora, with a statistical132model trained on bilingual parallel corpora.
We re-fer to that model as Mosesen-es-100k, because it wastrained using the Moses toolkit (Koehn et al, 2007).4.1 Training DataAll context models were trained on a Spanish cor-pus containing 100,000 sentences with 2.13 millionwords and an English corpus containing 100,000sentences with 2.07 million words.
The Spanish cor-pus was parsed using the MST dependency parser(McDonald et al, 2005) trained using dependencytrees generated from the the English Penn Treebank(Marcus et al, 1993) and Spanish CoNLL-X data(Buchholz and Marsi, 2006).So that we could directly compare against sta-tistical translation models, our Spanish and Englishmonolingual corpora were drawn from the Europarlparallel corpus (Koehn, 2005).
The fact that ourtwo monolingual corpora are taken from a parallelcorpus ensures that the assumption that similar con-texts are a good indicator of translation holds.
Thisassumption underlies in all work of translation lex-icon induction from comparable monolingual cor-pora, and here we strongly bias toward that assump-tion.
Despite the bias, the comparison of differentcontext models holds, since all models are trainedon the same data.4.2 Evaluation CriterionThe models were evaluated in terms of exact-matchtranslation accuracy of the 1000 most frequentnouns in a English-Spanish dictionary.
The accuracywas calculated by counting how many mappings ex-actly match one of the entries in the dictionary.
Thisevaluation criterion is similar to the setup used byKoehn and Knight (2002).
We compute the Top Naccuracy in the standard way as the number of Span-ish test words whose Top N English translation can-didates contain a lexicon translation entry out of thetotal number of Spanish words that can be mappedcorrectly using the lexicon entries.
Thus if ?crec-imiento, growth?
is the correct mapping based on thelexicon entries, the translation for ?crecimiento?
willbe counted as correct if ?growth?
occurs in the TopN English translation candidates for ?crecimiento?.Note that the exact-match accuracy is a conser-vative estimate as it is possible that the algorithmmay propose a reasonable translation for the givencaminoDepposn Cntxt Model Adjbow Cntxt Modelway 0.124 intentions 0.22solution 0.097 way 0.21steps 0.094 idea 0.20path 0.093 thing 0.20debate 0.085 faith 0.18account 0.082 steps 0.17means 0.080 example 0.17work 0.079 news 0.16approach 0.074 work 0.16issue 0.073 attitude 0.15Table 2: Top 10 translation candidates for the spanish word?camino (way)?
for the best adjacent context model (Adjbow)and best dependency context model (Depposn).
The bold Englishterms show the acceptable translations.Figure 3: Precision/Recall curve showing superior perfor-mance of dependency context model as compared to adjacentcontext at different recall points.
Precision is the fraction oftested Spanish words with Top 1 translation correct and Recallis fraction of the 1000 Spanish words tested upon.Spanish word but is marked incorrect if it does notexist in the lexicon.
Because it would be intractableto compare each projected vector against the vectorsfor all possible English words, we limited ourselvesto comparing the projected vector from each Spanishword against the vectors for the 1000 most frequentEnglish nouns, following along the lines of previ-ous work (Koehn and Knight, 2002; Haghighi et al,2008).5 ResultsTable 3 gives the Top 1 and Top 10 accuracy foreach of the models on their ability to translate Span-ish nouns into English.
Examples of the top 10translations using the best performing baseline anddependency-based models are shown in Table 2.
Thebaseline models Adjposn and Adjbow differ in that the133Model AccTop 1 AccTop 10Adjbow 35.3% 59.8%Adjposn 20.9% 46.9%Depbow 41.0% 62.0%Depposn 41.0% 64.1%Depposn + rev 42.9% 65.5%Mosesen-es-100k 56.4% 62.7%Table 3: Performance of various context-based modelslearned from monolingual corpora and phrase-table learnedfrom parallel corpora on Noun translation.latter disregards the position information in the con-text vector and simply uses a bag of words instead.Table 3 shows that Adjbow gains using this simplifi-cation.
A bag of words vector approach pools countstogether, which helps to reduce data sparsity.
Inthe position based model the vector is four times aslong.
Additionally, the bag of words model can helpwhen there is local re-ordering between the two lan-guages.
For instance, Spanish adjectives often fol-low nouns whereas in English the the ordering isreversed.
Thus, one can either learn position map-pings, that is, position +1 for adjectives in Spanish isthe same as position -1 in English or just add the theword counts from different positions into one com-mon vector as considered in the bag of words ap-proach.Using dependency trees also alleviates the prob-lem of position mapping between source and targetlanguage.
Table 3 shows the performance using thedependency based models outperforms the baselinemodels substantially.
Comparing Depbow to Depposnshows that ignoring the tree depth and treating it asa bag of words does not increase the performance.This contrasts with the baseline models.
The de-pendency positions account for re-ordering automat-ically.
The precision-recall curve in Figure 3 showsthat the dependency-based context performs betterthan adjancet context at almost all recall levels.The Mosesen-es-100k model shows the performanceof the statistical translation model trained on a bilin-gual parallel corpus.
While the system performs bestin Top 1 accuracy, the dependency context-basedmodel that ignores the sentence alignments surpris-ingly performs better in case of Top 10 accuracy,showing substantial promise.While computing the accuracy using the phrase-table learned from parallel corpora (Mosesen-es-100k),the translation probabilities from both directions(p(es|en) and p(en|es)) were used to rank the can-didates.
We also apply the monolingual context-based model in the reverse direction (from Englishto Spanish) and the row with label Depposn + rev inTable 3 shows further gains using both directions.Spanish English Sim Is presentScore in lexiconsen?ores gentlemen 0.99 NOxenofobia xenophobia 0.87 YESdiversidad diversity 0.73 YESchipre cyprus 0.66 YESmujeres women 0.65 YESalemania germany 0.65 YESexplotacio?n exploitation 0.63 YEShombres men 0.62 YESrepu?blica republic 0.60 YESracismo racism 0.59 YEScomercio commerce 0.58 YEScontinente continent 0.53 YESgobierno government 0.52 YESisrael israel 0.52 YESfrancia france 0.52 YESfundamento certainty 0.51 NOsuecia sweden 0.50 YEStra?fico space 0.49 NOtelevisio?n tv 0.48 YESfrancesa portuguese 0.48 NOTable 4: List of 20 most confident mappings using the de-pendency context based model for noun translation.
Note thatalthough the first mapping is the correct one, it was not presentin the lexicon used for evaluation and hence is marked as incor-rect.6 Further Extensions: Generalizing toother word types via tagset mappingMost of the previous literature on this problem fo-cuses on evaluating on nouns (Rapp, 1999; Koehnand Knight 2002; Haghighi et al, 2008).
Howeverthe vector projection approach is general, and shouldbe applicable to other word-types as well.
We eval-uated the models with new test set containing 1000most frequent words (not just nouns) in the English-Spanish lexicon.We used the dependency-based context model tocreate translations for this new set.
The row labeledDepposn in Table 5 shows that the accuracy on thisset is lower when compared to evaluating only onnouns.
The main reason for lower accuracy is thatclosed class words are often the most frequent andtend to have a wide range of contexts resulting inreasonable translation for most words include openclass words via the context model.
For instance, theEnglish preposition ?to?
appears as the most confi-dent translation for 147 out of the 1000 Spanish test134Figure 4: Illustration of using part-of-speech tag mapping torestrict candidate space of translationswords and in none (rightly so) after restricting thetranslations by part-of-speech categories.This problem can be greatly reduced by makinguse of the intuition that part-of-speech is often pre-served in translation, thus the space of possible can-didate translation can be largely reduced based onthe part-of-speech restrictions.
For example, a nounin source language will usually be translated as nounin target language, determiner will be translated asdeterminer and so on.
This idea is more clearly il-lustrated in in Figure 4.
We do not impose a hardrestriction but rather compute a ranking based onthe conditional probability of candidate translation?spart-of-speech tag given source word?s tag.An interesting problem in using part-of-speech re-strictions is that corpora in different languages havebeen tagged using widely different tagsets and thefollowing subsection explains this problem in detail:6.1 Mapping Part-of-Speech tagsets indifferent languagesThe English tagset was derived from the Penn tree-bank consisting of 53 tags (including punctuationmarkers) and the Spanish tagset was derived fromthe Cast3LB dataset consisting of 57 tags but thereis a large difference in the morphological and syn-tactic features marked by the tagset.
For example,the Spanish tagset as different tags for masculine andfeminine nouns and also has a different tag for coor-dinated nouns, all of which need to be mapped to thesingular or plural noun category available in Englishtagset.
Figure 5 shows an illustration of the mappingproblem between the Spanish and English POS tags.Figure 5: Illustration of mapping Spanish part-of-speechtagset to English tagset.
The tagsets vary greatly in notation andthe morphological/syntactic constituents represented and needto be mapped first, using the algorithm described in Section 6.1.We now describe an empirical approach for learn-ing the mapping between tagsets using the English-Spanish projection dictionary used in the monolin-gual context-based models for translation.
Given asmall English-Spanish bilingual dictionary and a n-best list of part-of-speech tags for each word in thedictionary2, we compute conditional probability oftranslating a source word with pos tag sposi to a tar-get with pos tag tposj as follows:p(tposj |sposi) =c(sposi , tposj )c(sposi) =?sw?S, tw?T p(sposi |sw) ?
p(tposj |tw) ?
Idict(sw, tw)?sw?S p(sposi |sw)where?
S and T are the source and target vocabulary inthe seed dictionary, with sw and tw being anyof the words in the respective sets.?
p(sposi |sw), p(tposj |tw) are obtained using rel-ative frequencies in a part-of-speech taggedcorpus in the source/target languages respec-tively, and are used as soft counts.?
Idict(sw, tw) is the indicator function withvalue 1 if the pair (sw, tw) occurs in the seeddictionary and 0 otherwise.In essence, the mapping between tagsets islearned using the known translations from a smalldictionary.Given a source word sw to translate, its mostlikely tag s?pos, and the most likely mapping of thistag into English t?pos computed as above, the transla-tion candidates with part-of-speech tag t?pos are con-sidered for comparison with vector similarity and2The n-best part-of-speech tag list for any word in the dic-tionary was derived using the relative frequencies in a part-of-speech annotated corpora in the respective languages135Figure 6: Precision/Recall curve showing superior perfor-mance of using part-of-speech equivalences for translating allword-types.
Precision is the fraction of tested Spanish wordswith Top 1 translation correct and Recall is fraction of the 1000Spanish words tested upon.the other candidates with tposj 6= t?pos are discardedfrom the candidate space.
Figure 4 shows an exam-ple of restricting the candidate space using POS tags.Model AccTop 1 AccTop 10Depposn 35.1% 62.9%+ POS 41.3% 66.4%Table 5: Performance of dependency context-based modelalong with addition of part-of-speech mapping model on trans-lating all word-types.The row labeled +POS in Table 5 shows the part-of-speech tags provides substantial gain as com-pared to direct application of dependency context-based model and is also comparable to the accuracyobtained evaluating just on nouns in Table 3.7 ConclusionThis paper presents a novel contribution to the stan-dard context models used when learning transla-tion lexicons from monolingual corpora by vectorprojection.
We show that using contexts based ondependency parses can provide more salient con-texts, allow for dynamic context size, and accountfor word reordering in the source and target lan-guage.
An exact-match evaluation shows 16% rela-tive improvement by using a dependency-based con-text model over the standard approach.
Furthermore,we show that our model, which is trained only onmonolingual corpora, outperforms the standard sta-Spanish English Sim Is presentScore in lexiconsen?ores gentlemen 0.99 NOchipre cyprus 0.66 YESmujeres women 0.65 YESalemania germany 0.65 YEShombres men 0.62 YESexpresar express 0.60 YESracismo racism 0.59 YESinterior internal 0.55 YESgobierno government 0.52 YESfrancia france 0.52 YEScultural cultural 0.51 YESsuecia sweden 0.50 YESfundamento basis 0.48 YESfrancesa french 0.48 YESentre between 0.47 YESorigen origin 0.46 YEStra?fico traffic 0.45 YESde of 0.44 YESsocial social 0.43 YESruego thank 0.43 NOTable 6: List of 20 most confident mappings using the depen-dency context with the part-of-speech mapping model translat-ing all word-types.
Note that although the second best mappingin Table4 for noun-translation is for xenofobia with score 0.87,xenofobia is not among the 1000 most frequent words (of allword-types) and thus is not in this test set.tistical MT approach to learning phrase tables whentrained on the same amount of sentence-aligned par-allel corpora, when evaluated on Top 10 accuracy.As a second contribution, we go beyond previ-ous literature which evaluated only on nouns.
Weshowed how preserving a word?s part-of-speech intranslation can improve performance.
We furtherproposed a solution to an interesting sub-problemencountered on the way.
Since part-of-speeechtagsets are not identical across two languages, wepropose a way of learning their mapping automat-ically.
Restricting candidate space based on thislearned tagset mapping resulted in 18% improve-ment over the direct application of context-basedmodel to all word-types.Dependency trees help improve the context fortranslation substantially and their use opens up thequestion of how the context can be enriched furthermaking use of the hidden structure that may provideclues for a word?s translation.
We also believe thatthe problem of learning the mapping between tagsetsin two different languages can be used in general forother NLP tasks making use of projection of wordsand its morphological/syntactic properties betweenlanguages.136ReferencesS.
Buchholz and E. Marsi.
2006.
Conll-X shared taskon multilingual dependency parsing.
Proceedings ofCoNLL, pages 189?210.Y.
Cao and H. Li.
2002.
Base Noun Phrase translationusing web data and the EM algorithm.
Proceedings ofCOLING-Volume 1, pages 1?7.D.
Chiang.
2007.
Hierarchical Phrase-Based Transla-tion.
Computational Linguistics, 33(2):201?228.P.
Fung and L.Y.
Yee.
1998.
An IR Approach forTranslating New Words from Nonparallel, Compara-ble Texts.
Proceedings of ACL, 36:414?420.A.
Haghighi, P. Liang, T. Berg-Kirkpatrick, and D. Klein.2008.
Learning bilingual lexicons from monolingualcorpora.
Proceedings of ACL-HLT, pages 771?779.Z.
Harris.
1985.
Distributional structure.
Katz, J. J.
(ed.
),The Philosophy of Linguistics, pages 26?47.P.
Koehn and K. Knight.
2002.
Learning a translationlexicon from monolingual corpora.
Proceedings ofACL Workshop on Unsupervised Lexical Acquisition,pages 9?16.P.
Koehn, F.J. Och, and D. Marcu.
2003.
Statisticalphrase-based translation.
In Proceedings of NAACL-HLT, pages 48?54.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, et al 2007.
Moses: Open sourcetoolkit for statistical machine translation.
Proceedingsof ACL, companian volume, pages 177?180.P.
Koehn.
2005.
Europarl: A parallel corpus for statisti-cal machine translation.
MT Summit X.D.
Lin and P. Pantel.
2002.
Discovery of inference rulesfor question-answering.
Natural Language Engineer-ing, 7(04):343?360.G.S.
Mann and D. Yarowsky.
2001.
Multipath transla-tion lexicon induction via bridge languages.
Proceed-ings of NAACL, pages 151?158.M.P.
Marcus, M.A.
Marcinkiewicz, and B. Santorini.1993.
Building a large annotated corpus of En-glish: the Penn treebank.
Computational Linguistics,19(2):313?330.R.
McDonald, F. Pereira, K. Ribarov, and J. Hajic.
2005.Non-projective dependency parsing using spanningtree algorithms.
Proceedings of EMNLP-HLT, pages523?530.J.
Nivre, J.
Hall, S. Kubler, R. McDonald, J. Nilsson,S.
Riedel, and D. Yuret.
2007.
The conll 2007shared task on dependency parsing.
Proceedings ofthe CoNLL Shared Task Session of EMNLP-CoNLL,pages 915?932.S.
Pado and M. Lapata.
2007.
Dependency-Based Con-struction of Semantic Space Models.
ComputationalLinguistics, 33(2):161?199.R.
Rapp.
1999.
Automatic identification of word trans-lations from unrelated English and German corpora.Proceedings of ACL, pages 519?526.C.
Schafer and D. Yarowsky.
2002.
Inducing translationlexicons via diverse similarity measures and bridgelanguages.
Proceedings of COLING, pages 1?7.137
