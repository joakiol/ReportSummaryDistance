Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 65?72,October 25, 2014, Doha, Qatar.
c?2014 Association for Computational LinguisticsAutomatic Arabic diacritics restoration based on deep netsAbstractIn this paper, Arabic diacritics restorationproblem is tackled under the deep learn-ing framework presenting Confused Sub-set Resolution (CSR) method to improvethe classification accuracy, in addition toArabic Part-of-Speech (PoS) taggingframework using deep neural nets.
Spe-cial focus is given to syntactic diacritiza-tion, which still suffer low accuracy asindicated by related works.
Evaluation isdone versus state-of-the-art systems re-ported in literature, with quite challeng-ing datasets, collected from different do-mains.
Standard datasets like LDC Arab-ic Tree Bank is used in addition to cus-tom ones available online for results rep-lication.
Results show significant im-provement of the proposed techniquesover other approaches, reducing the syn-tactic classification error to 9.9% andmorphological classification error to 3%compared to 12.7% and 3.8% of the bestreported results in literature, improvingthe error by 22% over the best reportedsystems1 IntroductionArabic is a wide spread language spoken by over350 million people on the planet.
Arabic alphabetand vocabulary are very rich, with the same wordmorphology being a candidate of different mean-ings and pronunciations.
For example the word???
might bear the meaning of the person name?Omar?
??????
or the meaning of ?age?
??????.
Whatdistinguish them is the diacritization signs as-signed to each character of the word.Diacritics are marks added on the character toreflect its correct pronunciation, according togrammatical, syntactical and morphological rulesof the language.Nowadays, Modern Standard Arabic (MSA)transcripts are written without diacritics, left tothe ability of the reader to restore them from thecontext and knowledge.
Diacritics restoration isnot an easy task even for knowledgeable, nativeArabic speakers.
On the other hand, there aremany machine learning tasks, like Text-To-Speech (TTS), translation, spelling correction,word sense disambiguation,?etc, that  requirediacritizing the script as a pre-processing stepbefore applying the core application technique.In its basic form, the problem can be reducedto a pattern classification problem, with sevendiacritics classes being the targets.
In addition,the diacritics classification can be divided intosyntactical diacritization, caring about case-ending and morphological diacritization, caringabout the rest of the word diacritics.
So far, mor-phological part of the problem is almost solved,leaving a marginal error of around 3-4%, Rash-wan et al.
(2009, 2011).
On the other hand, syn-tactical diacritization errors are still high, hittinga ceiling that is claimed to be asymptotic andcannot be squeezed any further, Rashwan et al.
(2009, 2011).
For this reason, we focus our effortto squeeze this error beyond the least 12.5% er-ror obtained in Rashwan et al.
(2009, 2011).Recently, a significant advancement in the areaof deep learning has been witnessed, with thedevelopment of a generative model; Deep BeliefNets (DBN), with a fast algorithm for inferenceof the model parameters.
Deep Neural Networks(DNN) shall be the basic machine learning clas-sifier used in this work, employing the latest re-sults reached in the deep learning field.
An effi-cient features?
vector is designed under the um-brella of deep learning to distinguish differentwords diacritics.
Features that are tested in thecurrent work are: PoS, morphological quadrupleof lexemes, last character and word identity.
Inaddition, context features are essential to the dia-critization problem.
Context features include, theMohsen A.
A. RashwanELC Dept., Cairo Uni-versity,Ahmad A. Al SallabELC Dept., Cairo Uni-versityHazem M. RaafatComputer ScienceDept., Kuwait Uni-versityAhmed RafeaComputer ScienceDept., AmericanUniversity in Cairomohsen_rashwan@rdi-eg.comahmad.elsallab@gmail.comhazem@cs.ku.edu.kwRafea@aucegypt.edu65previous word features, as well as the previousword diacritic.Part-of-Speech (PoS) features are critical tosyntactic diacritization, which is the focus of thiswork.
For some datasets PoS tags are manuallyannotated by professional linguistics, while forthe real case and most datasets, they are notavailable.
For this reason, standalone PoS taggersare built under the deep learning framework,which can reused in Arabic PoS tagging systems,needed for many other applications, not only forArabic diacritization.The deep learning model often hit a perfor-mance barrier which cannot be crossed.
Hence,error analysis and diagnosis is run on the confu-sion matrix results, proposing the Confused Sub-set Resolution (CSR) method to train sub-classifiers to resolve the identified confusionsand automatically generate a deep network-of-networks composed of the main classifier and thesub-classifiers working together to offer im-proved accuracy system purified of the identifiedconfusions, offering around 2% error enhance-ment.Evaluation of the proposed techniques is done ontwo datasets; the first is a custom one collectedfrom many different sources, which is availableonline at (http://www.RDI-eg.com/RDI/TrainingData is where to downloadTRN_DB_II).
Manually extracted PoS and mor-phological quadruples are available for only apart of this dataset.
The PoS tags of this part ofthe dataset were used to build the DNN PoS tag-gers to tag the rest of the dataset.
The corres-ponding test set is available online at(http://www.RDI-eg.com/RDI/TestData is whereto download TST_DB), which is quite challeng-ing and collected from different sources thantraining ones.
The second dataset is the standardLDC Arabic Tree Bank dataset LDC Arabic TreeBank Part 3,(http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?catalogId=LDC2005T20) used to benchmark the system against state-of-the art systemsin Arabic diacritization area.The rest of the paper is organized as follows:first the related works in literature are surveyed,followed by a formulation of the CSR method.The next section is dedicated to describing thefeatures used in the system, and how they areencoded and represented in the features?
vectorfollowed by the details of building the DNN PoStagger for Arabic.
The datasets used for evalua-tion are then described.
The next section de-scribes the system evaluation experiments.
Ex-perimental results include an error analysis studyof the effect of each feature and method on thesystem performance, in addition to benchmark-ing against state-of-the art systems in literature,evaluated on standard datasets.
Finally, the paperis concluded with the main results and conclu-sion.2 Related workThere have been many attempts to approach theArabic diacritization problem by different tech-niques.
Focus will be around three works strong-ly related to what is proposed here, and havingthe best results in literature.
Zitouni et al.
(2006)apply Maximum Entropy classification to theproblem taking the advantage of the MaxEntframework to combine different features togeth-er, like lexical, segment-based, and PoS features.Segmentation involves getting the prefix, suffix,and stem of the word.
PoS features are also gen-erated under the MaxEnt framework.
Habashand Rambow (2007)  perform MorphologicalAnalysis and Disambiguation of Arabic (MA-DA) system, and then apply SVM classification.Last, Rashwan et al.
(2009 and 2011) propose ahybrid approach composed of two stages: first,maximum marginal probability via A* latticesearch and n-grams probability estimation.
Whenfull-form words are OOV, the system switches tothe second mode which factorizes each Arabicword into all its possible morphological constitu-ents, then uses also the same techniques used bythe first mode to get the most likely sequence ofmorphemes, hence the most likely diacritization.The latter system shall be our baseline, since itgives the best results in literature so far, and thedataset used to evaluate it is available at ourhand, and hence fair comparison is possible.
Al-so, comparison to the three systems is made onthe LDC Arabic Tree Bank data set.3 System architectureIn this section the overall system is presented.The raw text input is fed to the system word byword.
According to the configured context depth,a number of succeeding and preceding words arestored in a context memory.
In our system thecontext is experimentally taken as three preced-ing words and one succeeding word (N=3, M=1),which is found to give the best accuracy resultsversus other tunings: (N=1, M=1), (N=2, M=2),(N=3, M=2), (N=1, M=3) and (N=1, M=3).
If theword is the first or last one in a sentence, the pre-66ceding or succeeding context is zero padded.Word context serves in case of syntactic diacriti-zation, while for morphological case, characterscontext is also needed, which is directly presentin the character sequence of the single inputword itself.Features extraction procedure depends on thefeature itself.
For PoS tags, a special DNN istrained for that purpose, which also makes use ofthe context of the word.
For other features, likesequence of characters forming the word, the lastcharacter of the word and the morphologicalquadruples are directly extracted from the singleword.The framework in Figure 2 is employed.
Threelayers network architecture is used for each fea-tures extraction subnet or classification network.For the classification network a 20-20-20 archi-tecture was used, while for PoS-tagging net-works a 60-60-60 is used.
The network architec-ture is determined empirically.
By experiments itwas found that the best architecture is the sym-metric one, with the same width for all layers.The best width is found to be the same as theaverage number of ones in the training set fea-tures vectors.The neural network training undergoes DBN pretraining as in Hinton et al.
(2006) for 20 epochsper layer, with batch size of 1000 examples eachwithout mini batches.
Momentum is used initial-ly with 0.5 for the first 5 epochs and then raisedto 0.9 for the next epochs.
The discriminativefine tuning is performed using conjugate gradientminimization for 30 epochs.
For the first 6epochs, only the upper layer is adjusted, then therest of the layers are trained for the next epochs.Once the features are ready of a certain raw wordit is fed to the DNN classifier.
The resulting con-fusion matrix from the training phase is then fedto the CSR method to generate the tree structurethat improves the system accuracy.
During test-ing phase, the raw input features are fed to theDNN classifier to obtain an initial guess of thetarget diacritic.
This guess is then improved inthe next CSR stage to obtain the final diacriticdecision.Figure 1 Overall Arabic diacritization system4 Deep learning frameworkThe Arabic diacritics restoration task can beformulated as pattern classification problem.
Thetarget classes shall be the diacritics themselves,described in TABLE I.
The input is the raw MSAtranscript.
The task is to classify the input basedon well-designed features?
vector and restore theoriginal diacritics of the raw text.
The outputshall be the full diacritized text.
All these diacrit-ics can exist on case-ending character, whileFathten, Dammeten and Kasreten can never oc-cur on non-ending character of the word root.TABLE I ARABIC DIACRITICS CLASSESDiacritics formon Arabic letter?Class name Pronunciation??
Fatha ????
/a/??
Damma ???
/u/??
Kasra ????
/i/?
??
Fathten ??????
/an/??
Dammeten?????/un/??
Kasreten ??????
/in/??
Sukun ????
No vowel??
Shadda ????
Double conso-nantThe machine learning classifier tool chosen inthis paper is the Deep Neural Network (DNN),under the framework of learning deep architec-ture proposed by Hinton et al.
(2006).
The rawtext is presented to the classifier, and a group ofsub-nets work to extract the desired features, likePoS tags.
The network architecture is shown inFigure 2.
Each sub-net is trained to extract a cer-tain kind of features, and the obtained features?vectors are concatenated together to form theinput that is represented to the classifier network.In fact the training of features extraction nets isguided by certain desired features, like PoS tags.67This enables building a standalone system thatoperates on the raw text only.Figure 2 Deep network framework5 Confused sub-set resolution methodThe Confused Sub-Classes Resolution (CSR) isbased on confusion matrix analysis and the me-thod by Raafat and Rashwan (1993).
The outputof this analysis shall be a network architecturecomposed of the original classifier operatingwith sub-classifiers to resolve confusions thatwere identified through confusion matrix analy-sis.The method starts with training a global classifi-er, then evaluating its performance.
To enhanceits accuracy, the sources of errors are analyzedby building the confusion matrix for the trainingset.
The position of the off diagonal elementidentifies the pair of classes that are confusedtogether.5.1 AlgorithmThe flow chart of the CSR method is shown inFigure 3.
The following steps describe the algo-rithm:1.
Train a basic global classifier in DNNframework and obtain the confusion matrixCon the training set2.
Identify the confusion domains }{ iDD =that have confusions more than a threshold?
,which is a parameter of the algorithm ob-tained from confusion matrix analysis.
It canbe set to the highest confusion figures in theoff diagonal elements of the confusion ma-trix.3.
Train sub-classifiers for each confusion do-main iD .4.
Determine the architecture of the model hav-ing nm?
sub-classifiers.
The superscript ndenote the index in the layer, while m de-notes the layer depth in which this domain isresolved.
When a sub classifier is a completesubset of another one, it is placed in a deeperlayer of the architecture.
In this case, the msuperscript is incremented to denote extradepth in the model.Figure 3 CSR algorithm flow chartTABLE  II shows the confusion results for DNNclassifier (vertically: true, horizontally: pre-dicted).1.
Fatha, Damma, Kasra: }6,5,4{11 =D 11?2.
Fathten, Dammeten, Kasreten:}3,2,1{21 =D  21?3.
Kasra, Kasreten: }6,3{12 =D  12?Each domain has its own nm?
classifier to re-solve its confusion.
The final model shall be asshown in Figure 4.68TABLE  II CONFUSION MATRIX RESULTS FOR DNN CLASSIFIER ON SYNTACTIC DIACRITIZATIONFathten Dammeten Kasreten Fatha Damma Kasra Shadda SukkunFathten 4762 2179 2455 336 389 197 0 120Dammeten 2647 6976 2720 660 1144 408 0 231Kasreten 4560 3378 32588 801 303 4868 0 951Fatha 438 475 1458 92755 11671 8340 0 1980Damma 262 727 579 5858 72994 14995 0 952Kasra 59 184 3275 2682 3657 220357 0 1970Shadda 2 78 86 51 75 0 416 4Sukkun 3 128 271 1150 630 1565 0 73983Figure 4 CSR model for syntactic Arabic diacritization task6 FeaturesThe input to text processing tasks is a tran-script or document containing raw text.
ForArabic diacritization task specifically, a set offeatures have proved good performance in litera-ture, such as morphological lexemes, PoS, wordidentity,?etc see Rashwan et al.
(2009, 2011),Zitouni et al.
(2006) and Habash and Rambow(2007).
In this section the features employed inour features vector are described.Last character identity: case-ending diacriti-zation is about adding diacritics on the last cha-racter of the word.
Arabic language prohibitssome diacritics from being placed over somecharacters.
For example fatha on???
is phoneti-cally forbidden.
Also, it favors some diacriticsover some character like fatheten on ???.
A rulebased system would have set a rule for that,however, in DNN framework, the system is leftto learn such rules.
Hence, the last characteridentity is an effective feature for syntactic dia-critization task.The raw word identity: is another type ofpossible features.
The system proposed byRasshwan et al.
(2009, 2011) uses this feature.There are two possibilities of encoding such fea-ture, the first would be to use a raw indexrepresenting the word index from a vocabularyvector built from training set.
However, thiscould lead to many out of vocabulary (OOV)cases, in addition to long vector.
On the otherhand, a word can be encoded as sequence of theidentities of its composing characters, which ismore efficient under the DNN framework toavoid OOV, because even if a word is not en-countered during training, at least a similar onewith a character less or more was encounteredduring training, generating nearly similar activa-tion of the stochastic binary units and leading tosimilar result as the original word.
The same ex-act word need not be present during trainingphase, instead only a similar word is enough sothat the word is not considered OOV.
This is adirect result of encoding words as sequence oftheir constituting characters.Context features: Considering the featuresvector of the preceding and/or succeeding wordsor characters can improve significantly the clas-sification accuracy.
This is what we refer to ascontext features.
Context features are essential tosyntactic and morphological diacritization tasks.For morphological diacritization context is justthe surrounding characters, while for syntacticdiacritization context is represented by the sur-69rounding words.
We denote the depth of the pre-ceding context by N and the succeeding contextelements by M.Context class labels: The context does not on-ly include the features?
vectors of inputs, it canalso include the context of class labels.
For ex-ample, the decision of the classifier for the pre-vious diacritic can be re-considered as an inputfeature for the current diacritic classification.This results in something like an auto-regressivemodel, where the previous decisions affect thenext one recursivelyPart-of-Speech tags: are essential features todiscriminate syntactic diacritics cases, wheresyntactic diacritics restoration is strongly relatedto grammatically parsing and analyzing the sen-tence into its syntactic language units or PoS.There are many models for Arabic PoS tags.
Inthis work we adopt the one in Rashwan et al.
(2011), which sets 62 context-free atomic unitsto represent all possible Arabic language PoStags.
A very rich dataset of Arabic words, ex-tracted from different sources, is used to train thesystem (available on http://www.RDI-eg.com/RDI/TrainingData is where to downloadTRN_DB_II).
PoS tags are manually annotatedfor this dataset by expert Arabic linguistics.
ADNN is trained on this dataset to identify differ-ent PoS tags.7 DatasetsIn all the coming experiments one of the fol-lowing datasets is used:- TRN_DB_I: This is a 750,000 words dataset,collected from different sources and manual-ly annotated by expert linguistics with everyword PoS and Morphological quadruples.- TRN_DB_II: This is 2500,000 words trainset.- TST_DB: This is 11,000 words test data set.For more information refer to Rashwan et al.
(2009, 2011).- ATB: LDC Arabic Tree Bank.For TRN_DB_I, PoS tags are available as ready fea-tures added manually.
When the manually PoS tagsare used as input features, the dataset is referred to asTRN_DB_I ?
Ready PoS.
While, when our PoS-DNNnets are used, a derivative dataset with only raw textis referred as TRN_DB_I ?
Raw text.8 System evaluation8.1 Effect of CSR methodThe objective of this experiment is to show theeffect of CSR method.
The test set is TST_DB.Results in TABLE  III  show improvementaround 2% in all tested datasets.
This represents17.09% improvement of error.TABLE  III EFFECT OF CSRDataset Accuracy with CSR (%)Accuracywithout CSR(%)TRN_DB_I ?Ready PoS90.2 88.2TRN_DB_I ?Raw text88.2 86.28.2 Effect of class context learningThe objective of this experiment is to evaluatethe effect of employing sequential class labelsmodel.
Test set is TST_DB.
The results in TA-BLE  IV show that employing this feature offers1% to 2% improvement of accuracy over basicDBN model alone.
This represents 15.625% im-provement of error.TABLE  IV EFFECT OF CLASS LABELSCONTEXT ON SYNTACTIC DIACRITIZA-TIONDatasetAccuracywithclass labelscontext (%)Accuracywithout classlabels context(%)TRN_DB_I ?
ReadyPoS88.3 87.2TRN_DB_I ?
Rawtext86.7 85.1TRN_DB_I  +TRN_DB_II /TST_DB86.3 84.38.3 Effect of last character feature for syn-tactic caseThe identity of the last character of a word is acritical feature for syntactic diacritization task.The dataset used for training is TRN_DB_I andfor testing TST_DB.
TABLE  V shows the effectof utilizing this feature.
A significant error im-provement of about 4% is witnessed with thisnew feature.TABLE  V EFFECT OF LAST CHARACTER FEATUREON SYNTACTIC DIACRITIZATIONAccuracy (%)With last character 88.2Without last character 84.570Justification to this strong improvement is that;Arabic language prohibits some diacritics frombeing placed over some characters.
For examplefatha on???
is prohibited phonetically.
Also, itfavors some diacritics over some character likefatheten on ???.
A rule based system would haveset a rule for that, however, in DNN framework,the system is left to learn such rules.8.4 Effect of character level encoding of thewordThe word identity is an important feature fordiacritization task.
The dataset used for trainingis TRN_DB_I and for testing TST_DB.
TABLEVI shows the effect of utilizing this feature.
Asignificant error improvement of about 2% iswitnessed with this feature.TABLE  VI EFFECT OF CHARACTER LEVEL WORDENCODING ON SYNTACTIC DIACRITIZATIONEncoding Accuracy (%)Word level 88.2Character level 86.3?Word level?
could lead to many out of voca-bulary (OOV) cases, in addition to long vector.On the other hand, ?Character level?
is more ef-ficient under the DNN framework to avoid OOVsuffered in Rashwan et al.
(2009, 2011), becauseeven if a word is not encountered during training,but a similar one with a character less or morewas encountered, then a nearly similar activationof the stochastic binary units would be generated,leading to similar result to the most similar wordexisting in training data set.8.5 Comparison to other systemsThe objective of this experiment is to evaluatethe performance of the proposed system forArabic diacritization versus the architecture inRashwan et al.
(2009, 2011)., the MaxEnt modelproposed in Zitouni et al.
(2006) and the MADAsystem Habash and Rambow (2007).
These sys-tems represent the state of the art Arabic diacriti-zation systems, with the best reported accuracyin literature.
The evaluation was done on all thedatasets as explained in Rashwan et al.
(2011).The PoS features are extracted using the DNN-PoS tagger, since TRN_DB_II / TST_DB datasetcontains only raw text without ready PoS fea-tures.Results in TABLE VIII show that the proposedsystem achieves improved performance byaround 1.2% over the system in 0Rashwan et al.
(2011), which represents 9.23% of the error, eva-luated on the (TRN_DB_I  + TRN_DB_II /TST_DB) dataset.
Also, on ATB standard data-set, the proposed system achieves 0.9% im-provement over the best result in literature usingthe same training and testing data same as evalu-ation in Rashwan et al.
(2011) was done.Another comparison is done when the datasetTRN_DB_I is used with ready PoS features.
Re-sults in  show that the proposed system achievesbetter performance by 3.2% over the system inRashwan et al.
(2011), which represents 24.6%of the error.
The importance of this experiment isto isolate the automatic PoS tagging errors fromthe evaluation.TABLE VII COMPARISON TO HYBRID AR-CHITECTURE WITH READY PoS FEATURESSystem Syntacticalaccuracy (%)Deep network + CSR 90.2Hybrid Architecture0Rashwan et al.
(2011)88.3TABLE VIII COMPARISON TO OTHER SYSTEMSSystem Dataset Case-endingaccuracy (%)Morphologicalaccuracy (%)Deep network + CSR(This paper)TRN_DB_I  +TRN_DB_II / TST_DB88.297ATB 88.4 97Hybrid Architecture?
Rashwan et al.
(2009, 2011)TRN_DB_I  +TRN_DB_II / TST_DB8796.4ATB 87.5 96.2MaxEnt - Zitouni etal.
(2006)ATB 82 94.5MADA - Habash andRambow (2007)ATB 85.1 95.2719 ConclusionIn this paper the problem of Arabic diacritiza-tion restoration is tackled under the deep learningframework taking advantage of DBN modeltraining.
As part of the proposed deep system, aPoS tagger for Arabic transcript is proposed aswell using deep networks.
The first contributionis the introduction of the Confused Sub-set Reso-lution (CSR) architecture to enhance the accura-cy.Design of features vector played a key role inerror improvement.
Specifically, using featureslike last character identity had valuable contribu-tion to error improvement by about 4%.
Includ-ing class labels context features in auto-regressive fashion has also good impact of 1.1%on error improvement.
Finally, encoding of wordas sequence of characters enables to reduce OOVcases and enhance the accuracy.CSR enables to purify the cross confusions be-tween diacritics.
A network-of-network architec-ture formed of group of classifiers, each workingto resolve a set of confusions, is directly generat-ed to enhance the overall accuracy by about 2%.The identified confusions and the architecture gosmoothly with the grammatical and syntacticalrules of the Arabic language.Evaluation of the proposed system is made ontwo different datasets; custom and standard, bothavailable online to enable replicating the experi-ments.
Details of features vectors formatting andthe used features are presented to facilitate re-sults re-generation.
The standard LDC ArabicTree Bank dataset is used to bench mark the sys-tem against the best three systems in literature,showing that our system outperforms all pre-viously published baselines.
The effect of eachproposed method is presented separately.
Resultsshow improvements ranging from 1.2% to 2.8%over the best reported results representing 22%improvement of the error.ReferenceRashwan, Mohsen A A; Attia, Mohamed; Abdou,Sherif M.; Abdou, S.; Rafea, Ahmed A.
2009.
?AHybrid System for Automatic Arabic Diacritization?,International Conference on Natural LanguageProcessing and Knowledge Engineering, pp 1-8, 24-27.Rashwan, Mohsen A A , Al-Badrashiny, Mohamed AS A A; Attia, Mohamed; Abdou, Sherif M.; Rafea,Ahmed A.
2011.
?A Stochastic Arabic DiacritizerBased on a Hybrid of Factorized and UnfactorizedTextual Features?, IEEE Transactions on Audio,Speech, and Language Processing, vol.
19, issue 1, pp166-175.I.
Zitouni; J. S. Sorensen; R. Sarikaya, 2006.
?Maxi-mum Entropy Based Restoration of Arabic Diacrit-ics?, Proceedings of the 21st International Conferenceon Computational Linguistics and 44th Annual Meet-ing of the Association for Computational Linguistics(ACL); Workshop on Computational Approaches toSemitic Languages; Sydney-AustraliaN.
Habash; O. Rambo.
2007.
?Arabic Diacritizationthrough Full Morphological Tagging?, Proceedings ofthe 8th Meeting of the North American Chapter of theAssociation for Computational Linguistics (ACL);Human Language Technologies Conference (HLT-NAACL).G.
E. Hinton; S. Osindero; Y. Teh, ?A fast learningalgorithm for deep belief nets?
Neural Computation,vol.
18, pp.
1527?1554, 2006.Ruslan Salakhutdinov.
2009.?Learning Deep Genera-tive Models?
PhD thesis, Graduate Department ofComputer Science, University of Toronto,Raafat, H.; Rashwan, M.A.A.
1993.
?A tree structuredneural network, Proceedings of the Second Interna-tional Conference on Document Analysis and Recog-nition?
, pp.
939 ?
941, ISBN: 0-8186-4960-7Hai-Son Le ; Oparin, I. ; Allauzen, A. ; Gauvain, J.,2011.
?Structured Output Layer neural network lan-guage model?
2011 IEEE International Conference onAcoustics, Speech and Signal Processing (ICASSP),pp.
5524 - 5527, ISBN: 978-1-4577-0537-3http://www.RDI-eg.com/RDI/TrainingData is whereto download TRN_DB_II.http://www.RDI-eg.com/RDI/TestData is where todownload TST_DBLDC Arabic Tree Bank Part 3,http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?catalogId=LDC2005T2072
