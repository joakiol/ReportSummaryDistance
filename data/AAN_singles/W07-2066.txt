Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 304?307,Prague, June 2007. c?2007 Association for Computational LinguisticsSW-AG: Local Context Matching for English Lexical SubstitutionGeorge Dahl, Anne-Marie Frassica, Richard WicentowskiDepartment of Computer ScienceSwarthmore CollegeSwarthmore, PA 19081 USA{george.dahl, afrassi1}@gmail.com, richardw@cs.swarthmore.eduAbstractWe present two systems that pick the tenmost appropriate substitutes for a markedword in a test sentence.
The first systemscores candidates based on how frequentlytheir local contexts match that of the markedword.
The second system, an enhancementto the first, incorporates cosine similarity us-ing unigram features.
The core of both sys-tems bypasses intermediate sense selection.Our results show that a knowledge-light, di-rect method for scoring potential replace-ments is viable.1 IntroductionAn obvious way to view the problem of lexical sub-stitution is as a sense disambiguation task.
Forexample, one possible approach is to identify thesense of the target word and then to pick a synonymbased on the identified sense.
Following Dagan etal.
(2006), we refer to this as an indirect approach.A system using an indirect approach must have ac-cess to a list of senses for each target word, and eachsense must have a corresponding list of synonyms.Though one can use a predefined sense inventory,such as WordNet, the granularity of the sense inven-tory may not be appropriate for the task.
If the senseinventory is too fine-grained, then picking the cor-rect sense may be needlessly difficult.
Conversely,if it is too coarse, picking the correct sense may notnarrow down the list of potential substitutions suffi-ciently.To avoid these problems, we propose a direct ap-proach, which will break the problem into two steps:for each target word, generate a list of candidate syn-onyms; then rank each synonym for its quality asa replacement.
Although our second system makesuse of some sense information, it is used only to re-rank candidates generated using a direct approach.We describe two systems: the first is a purely di-rect method based on local context matching, andthe second is a hybrid of local context matchingand wider context bag-of-words matching.
Both areknowledge-light and unsupervised.2 MethodsAs mentioned above, we divide the task into twosteps: compiling a list of synonyms and then, foreach test instance, ranking the list of appropriatesynonyms.
Both of our systems create lists of can-didate synonyms in the same way and only differ inthe way they arrive at a ranking for these candidates.2.1 Compiling a substitution lexiconWe begin by compiling a list of candidate synonymsfor each target word.
Following Dagan et al (2006),we will refer to this list of synonyms as our substitu-tion lexicon.
The performance of our system is lim-ited by the substitution lexicon because it can onlypick the correct replacements if they are in the lexi-con.
The substitution lexicon available to our scor-ing system therefore determines both the maximumattainable recall and the baseline probability of ran-domly guessing a correct replacement.One approach to generating a substitution lexiconis to query WordNet for lists of synonyms groupedby the senses of each word.
While WordNet has itsadvantages, we aimed to create a knowledge-light304system.
A more knowledge-free system would haveused a machine readable dictionary or a large nat-ural language sample to retrieve its synonyms (see,for example, Lin (1998)), but our system falls shortof this, relying on Roget?s New Millennium The-saurus1 (henceforth RT) as a source of synonyms.Though this thesaurus is similar to WordNet in someways, it does not contain semantic relationships be-yond synonyms and antonyms.
One important ad-vantage of a thesaurus over WordNet is that it is eas-ier to obtain for languages other than English.We used the trial data to ensure that the qualityof the list compiled from RT would be satisfactoryfor this task.
We found that by using the synonymsin WordNet synsets2 as our substitution lexicon, wecould achieve a maximum recall of 53% when usingan oracle to select the correct synonyms.
However,using the synonyms from RT as the substitution lex-icon led to a maximum recall of 85%.Querying RT for the synonyms of a word returnsmultiple entries.
For our purposes, each entry con-sists of a Main Entry, a part of speech, a definition,and a list of synonyms.
Many of the returned entriesdo not list the query word as the Main Entry.
Forinstance, given the query ?tell?, RT returns 115 en-tries: 7 whose Main Entry is ?tell?, an additional 3that contain ?tell?
(e.g.
?show and tell?
), and the re-maining 105 entries are other words (e.g.
?gossip?
)that list ?tell?
as a synonym.
Where the Main Entrymatches the query, RT entries roughly correspond tothe traditional notion of ?sense?.In order to reduce the number of potentially spu-rious synonyms that could be picked, we createda simple automatic filtering system.
For each RTquery, we kept only those entries whose Main Entryand part of speech matched the target word exactly3.In addition, we removed obscure words which webelieved human annotators would be unlikely topick.
We used the unigram counts from the Web1T 5-gram corpus (Brants and Franz, 2006) to de-termine the frequency of use of each candidate syn-onym.
We experimented with discarding the leastfrequent third of the candidates.
Although this fil-tering reduced our maximum attainable recall from1http://thesaurus.reference.com2excluding the extended relations such as hyponyms, etc.3Since RT was not always consistent in labeling adjectivesand adverbs, we conflated these in filtering.85% to 75% on the trial data, it significantly raisedour precision.2.2 Ranking substitutionsWe created two systems (and submitted two sets ofresults) for this task.
The first system is fully de-scribed in Section 2.2.1.
The second system includesthe first system and is fully described in the remain-der of Section 2.2.2.2.1 Local context matching (LCM)Our first system matches the context of targetwords to the context of candidate synonyms in alarge, unannotated corpus.
If the context of a candi-date synonym exactly matches the context of a targetword, it is considered a good replacement synonym.Context matches are made against the Web 1T cor-pus?
list of trigrams.
Though this corpus providesus with a very large amount of data4, to increasethe likelihood of finding an appropriate match, wemapped inflected words to their roots in both the cor-pus and the test data (Baayen et al, 1996).The context of a target word consists of a set of upto 3 trigrams, specifically those trigrams in the testsentence that contain the target word.
For example,the context of ?bright?
in the sentence5 ?...
who wasa bright boy only ...?
is the set {?was a bright?, ?abright boy?, ?bright boy only?
}.Once we identified the set of context trigrams,we filtered this set by removing all trigrams whichdid not include content words.
To identify contentwords, we used the NLTK-Lite tagger to assign apart of speech to each word (Loper and Bird, 2002).We considered open class words (with the exceptionof the verb to be) and pronouns to be content words.We call the filtered set of trigrams the test trigrams.From the above example, we would remove the tri-gram ?was a bright?
since it does not contain a con-tent word other than the target word.We match the test trigrams against trigrams in theWeb 1T corpus.
A corpus trigram is said to matchone of the test trigrams if the only difference be-tween them is that the target word is replaced with acandidate synonym.A scoring algorithm is then applied to each can-didate.
The scoring algorithm relies on the test tri-4There are over 967 million unique trigrams in this corpus5Excerpted from trial instance 1.305grams, denoted by T , the set of candidate synonyms,C, and the frequencies of the trigrams in the corpus.Let m(t, c) be the frequency of the corpus trigramthat matches test trigram t, where the target word int is replaced with candidate c. The score of a candi-date c is given by:score(c) =?t?Tm(t, c)?x?C m(t, x).The normalization factor prevents high frequencytest trigrams from dominating the score of candi-dates.
The candidates are ranked by score, and thetop ten candidates are returned as substitutions.2.2.2 Nearest ?synonym?
neighborIn some cases, the words in the local context didnot help identify a replacement synonym.
For ex-ample, in test instance 391 the trigrams used in thelocal context model were: ?by a coach?, ?a coachand?, and ?coach and five?.
The first two trigramswere removed because they did not contain contentwords.
The final trigram does not provide conclu-sive evidence: the correct synonym in this case canbe determined by knowing whether the next word is?players?
(coach = instructor) or ?horses?
(coach =vehicle).
Without backoff, extending the local con-text model to 5-grams led to sparse data problems.Rather than match exact n-grams, we use a near-est neighbor cosine model with unigram (bag ofwords) features for all words in the target sentence.For each instance, an ?instance vector?
was createdby counting the unigrams in the target sentence.Since the Web 1T corpus does not contain fullsentences, we matched each of the instance vectorsagainst vectors derived from the British NationalCorpus6.
For each candidate synonym, we createda single vector by summing the unigram counts inall sentences containing the candidate synonym (orone of its morphological variants).
We ranked eachcandidate by the cosine similarity between the can-didate vector and the instance vector.2.2.3 Nearest ?sense?
neighborManual inspection of the trial data key revealedthat, for many instances, a large majority (if notall) of the human-selected synonyms in that instance6http://www.natcorp.ox.ac.uk/were found in just one or two RT entries.
This notaltogether unexpected insight led to the creation of asecond nearest neighbor cosine model.We first created instance vectors, following themethod described above.
However, instead of cre-ating a single vector for each candidate synonym,we created a single vector for each ?sense?
(RT en-try): for each RT entry, we created a single vector bysumming the unigram counts in all BNC sentencescontaining any of that entry?s candidate synonyms(or morphological variants).
We ranked each candi-date sense by the cosine similarity between the sensevector and the instance vector.This method is not used on its own but rather tofilter the results (Section 2.2.4) of the nearest ?syn-onym?
neighbor method.
Also note that while weused the ?senses?
provided by RT for this method,we could have used an automatic method, e.g.
Linand Pantel (2001), to achieve the same goal.2.2.4 Filtering by senseThe nearest synonym neighbor method underper-formed the local context matching method on thetrial data.
This result led us to filter the nearestneighbor results by keeping only those words listedas synonyms of the highest ranked senses, as deter-mined by the nearest sense neighbor model.
Thisproved successful, increasing accuracy from .41 to.44 (for instances which had a mode) when we keptonly those synonyms found in the top half of thesenses returned by the nearest sense model.7We attempted the sense filtering method on thelocal context model but found that it was less suc-cessful.
No matter what threshold we set for filter-ing, we always did best by not doing the filteringat all.
However, applying the filtering to only nouninstances, keeping only those synonyms belongingto the single most highly ranked sense, increasedour accuracy on nouns from .51 to .57 (for instanceswhich had a mode).
This surprising result, used inthe following section, requires further investigationwhich was not possible in the limited time provided.2.2.5 Model CombinationA straightforward model combination using therelative ranking of synonyms by the filtered local7We rounded up if there were an odd number of senses, andwe always kept a minimum of two senses.306P R Mode P Mode Rall 35.53 32.83 47.41 43.82Further AnalysisNMWT 37.49 34.64 49.11 45.35NMWS 38.36 35.67 49.41 45.70RAND 36.94 34.52 48.94 45.72MAN 33.83 30.85 45.63 41.67Table 1: SWAG1:OOT resultsP R Mode P Mode Rall 37.80 34.66 50.18 46.02Further AnalysisNMWT 39.95 36.51 52.28 47.78NMWS 40.97 37.75 52.25 47.98RAND 39.74 36.36 53.61 48.78MAN 35.56 32.79 46.34 42.88Table 2: SWAG2:OOT resultscontext matching (FLCM) model8 and the filterednearest neighbor (FNN) model yielded results whichwere inferior to those provided by the FLCM modelon its own.
Examination of the results of each modelshowed that the FLCMmodel was best on nouns andadjectives, the FNN model was best on adverbs, andthe combination model was best on verbs.
Thoughlimited time prohibited us from doing a more thor-ough evaluation, we decided to use this unorthodoxcombination as the basis for our second system.3 ResultsWe submitted two sets of results to this task: the firstwas our local context matching system (SWAG1)and the second was the combined FLCM and FNNhybrid system (SWAG2).Our systems consistently perform better when amode exists, which makes sense because those areinstances in which the annotators are in agreement(McCarthy and Navigli, 2007).
In these cases it ismore likely that the most appropriate synonym isclear from the context and therefore easier to pick.It is hard to say exactly why SWAG2 outperformsSWAG1 because we haven?t had enough time tofully analyze our results.
Our decision to choose dif-ferent systems for each part of speech may have been8Filtering was done only on nouns as described above.partially responsible.
For example, both LCM (usedin SWAG1 and SWAG2) and the nearest neigh-bor cosine comparison algorithm (used in SWAG2)performed poorly on verbs on the trial data.
Thevoter described in the SWAG2 discussion alwaysperformed better on verbs than either system did in-dividually, so this may account for part of the higherprecision and recall.4 ConclusionsOur results show that direct methods of lexical sub-stitution deserve more investigation.
It does indeedseem possible to successfully do lexical substitutionwithout doing sense disambiguation.
Furthermore,this task can be accomplished in a knowledge-lightway.
Further investigation of this method could in-clude generating the list of synonyms using a com-pletely knowledge-free approach.ReferencesR.H.
Baayen, R. Piepenbrock, and L. Gulikers.
1996.CELEX2.
LDC96L14, Linguistic Data Consortium,Philadelphia.T.
Brants and A. Franz.
2006.
Web 1T 5-gram, ver.
1.LDC2006T13, Linguistic Data Consortium, Philadel-phia.I.
Dagan, O. Glickman, A. Gliozzo, E. Marmorshtein,and C. Strapparava.
2006.
Direct word sense match-ing for lexical substitution.
In Proceedings of the 44thAnnual Meeting of the Association for ComputationalLinguistics.D.
Lin and P. Pantel.
2001.
Induction of semanticclasses from natural language text.
In Proceedings ofthe 7th ACM SIGKDD International Conference onKnowledge Discovery and Data Mining.D.
Lin.
1998.
Automatic retrieval and clustering of simi-lar words.
In Proceedings of the 36th Annual Meetingof the Association for Computational Linguistics.E.
Loper and S. Bird.
2002.
NLTK: The NaturalLanguage Toolkit.
In Proceedings of the ACL-02Workshop on Effective Tools and Methodologiesfor Teaching Natural Language Processing andComputational Linguistics.D.
McCarthy and R. Navigli.
2007.
SemEval-2007 Task10: English lexical substitution task.
In Proceedingsof SemEval-2007.307
