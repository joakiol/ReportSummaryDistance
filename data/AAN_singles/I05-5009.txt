Evaluating Contextual Dependency of Paraphrasesusing a Latent Variable ModelKiyonroi OHTAKESpoken Language Communication Research LaboratoriesAdvanced Telecommunications Research Institute InternationalKyoto 619-0288 Japankiyonori.ohtake + @atr.jpAbstractThis paper presents an evaluationmethod employing a latent variablemodel for paraphrases with their con-texts.
We assume that the context of asentence is indicated by a latent vari-able of the model as a topic and thatthe likelihood of each variable can beinferred.
A paraphrase is evaluatedfor whether its sentences are used inthe same context.
Experimental re-sults showed that the proposed methodachieves almost 60% accuracy and thatthere is not a large performance differ-ence between the two models.
The re-sults also revealed an upper bound ofaccuracy of 77% with the method whenusing only topic information.1 IntroductionThis paper proposes a method to evaluate whethera paraphrasing pair is contextually independent.Evaluating a paraphrasing pair is important whenwe extract paraphrases from a corpus or apply aparaphrase to a sentence, since wemust guaranteethat the paraphrase carries almost the same mean-ing.
However, the meaning carried by a sentenceis affected by its context.
Thus, we focus on thecontextual dependency of paraphrases.A thing can be expressed by various expres-sions, and a single idea can be paraphrased inmany ways to enrich its expression or to increaseunderstanding.
Paraphrasing plays a very impor-tant role in natural language expressions.
How-ever, it is very hard for machines to handle differ-ent expressions that carry the same meaning.The importance of paraphrasing has beenwidely acknowledged, and many paraphrasingstudies have been carried out.
Using only sur-face similarity is insufficient for evaluating para-phrases because there are not only surface dif-ferences but many other kinds of differences be-tween paraphrased sentences.
Thus, it is not easyto evaluate whether two sentences carry almostthe same meaning.Some studies have constructed and evaluatedhand-made rules (Takahashi et al, 2001; Ohtakeand Yamamoto, 2001).
Others have tried toextract paraphrases from corpora (Barzilay andMcKeown, 2001; Lin and Pantel, 2001), whichare very useful because they enable us to con-struct paraphrasing rules.
In addition, we can con-struct an example-based or a Statistical MachineTranslation (SMT)-like paraphrasing system thatutilizes paraphrasing examples.
Thus, collect-ing paraphrased examples must be continued toachieve high-performance paraphrasing systems.Several methods of acquiring paraphrases havebeen proposed (Barzilay and McKeown, 2001;Shimohata and Sumita, 2002; Yamamoto, 2002).Some use parallel corpora as resources to obtainparaphrases, which seems a promising way to ex-tract high-quality paraphrases.However, unlike translation, there is no obvi-ous paraphrasing direction.
Given paraphrasingpair E1:E2, we have to know the paraphrasingdirection to paraphrase from E1 to E2 and viceversa.
When extracting paraphrasing pairs fromcorpora, whether the paraphrasing pairs are con-65textually dependent paraphrases is a serious prob-lem, and thus there is a specific paraphrase direc-tion for each pair.
In addition, it is also importantto evaluate a paraphrasing pair not only when ex-tracting but also when applying a paraphrase.Consider this example, automatically extractedfrom a corpus: Can I pay by traveler?s check?/ Do you take traveler?s checks?
This exampleseems contextually independent.
On the otherhand, here is another example: I want to buy apair of sandals.
/ I?m looking for sandals.
Thisexample seems to be contextually dependent, be-cause we don?t know whether the speaker is onlylooking for a single pair of sandals.
In some con-texts, the latter sentence means that the speaker isseeking or searching for sandals.
In other words,the former sentence carries specific meaning, butthe latter carries generic meaning.
Thus, the para-phrasing sentences are contextually dependent,and although the paraphrasing direction from spe-cific to generic might be acceptable, the oppositedirection may not be.We can solve part of this problem by inferringthe contexts of the paraphrasing sentences.
A textmodel with latent variables can be used to inferthe topic of a text, since latent variables corre-spond to the topics indicated by texts.
We as-sume that a topic indicated by a latent variableof a text model can be used as an approximationof context.
Needless to say, however, such an ap-proximation is very rough, and a more complexmodel or more powerful approach must be devel-oped to achieve performances that match humanjudgement in evaluating paraphrases.The final goal of this study is the evaluationof paraphrasing pairs based on the following twofactors: contextual dependency and paraphras-ing direction.
In this paper, however, as a firststep to evaluate paraphrasing pairs, we focus onthe evaluation of contextual dependency by us-ing probabilistic Latent Semantic Indexing (pLSI)(Hofmann, 1999) and Latent Dirichlet Allocation(LDA) (Blei et al, 2003) as text models with la-tent variables.2 Latent Variable Models and TopicInferenceIn this section, we introduce two latent variablemodels, pLSI and LDA, and also explain how toinfer a topic with the models.In addition to pLSI and LDA, there are other la-tent variable models such as mixture of unigrams.We used pLSI and LDA because Blei et al havealready demonstrated that LDA outperforms mix-ture of unigrams and pLSI (Blei et al, 2003), anda toolkit has been developed for each model.From a practical viewpoint, we want to deter-mine how much performance difference exists be-tween pLSI and LDA through evaluations of con-textual paraphrase dependency.
The time com-plexity required to infer a topic by LDA is largerthan that by pLSI, and thus it is valuable to knowthe performance difference.2.1 Probabilistic LSIPLSI is a latent variable model for general co-occurrence data that associates an unobservedtopic variable z ?
Z = {z1, ?
?
?
, zK} with eachobservation, i.e., with each occurrence of wordw ?
W = {w1, ?
?
?
, wM} in document d ?
D ={d1, ?
?
?
dN}.PLSI gives joint probability for a word and adocument as follows:P (d,w) = P (d)P (w|d), (1)whereP (w|d) =?z?ZP (w|z)P (z|d).
(2)However, to infer a topic indicated by a docu-ment, we have to obtain P (z|d).
From (Hofmann,1999), we can derive the following formulas:P (z|d,w) ?
P (z)P (d|z)P (w|z) (3)andP (d|z) ?
?wn(d,w)P (z|d,w), (4)where n(d,w) denotes term frequency, which isthe number of times w occurs in d. Assumingthat P (d|z) = ?w?d P (w|z), the probability of atopic under document (P (z|d)) is proportional tothe following formula:P (z)2?w?dP (w|z)?wn(d,w)P (w|z).
(5)After a pLSI model is constructed with a learn-ing corpus, we can infer topic z ?
Z indicated66by given document d = w1, ?
?
?
, wM(d) with For-mula 5.
A topic z that maximizes Formula 5 isinferred as the topic of document d.2.2 Latent Dirichlet AllocationLatent Dirichlet Allocation (LDA) is a generativeprobabilistic model of a corpus.
The basic ideais that documents are represented as random mix-tures over latent topics, where each topic is char-acterized by a distribution over words.LDA gives us the marginal distribution of adocument (p(d|?, ?
), d = (w1, w2, ?
?
?wN )) bythe following formula:?p(?|?
)( N?n=1?znp(zn|?
)p(wn|zn, ?
))d?, (6)where ?
parameterizes Dirichlet random vari-able ?
and ?
parameterizes the word probabili-ties, and zn indicates a topic variable zn ?
Z ={z1, z2, ?
?
?
, zN}.
To obtain the probability of acorpus, we take the product of the marginal prob-abilities of single documents.Here, we omit the details of parameter estima-tion and the inference of LDA due to space lim-itations.
However, the important point is that theDirichlet parameters used to infer the probabilityof a document can be seen as providing a repre-sentation of the document in the topic simplex.In other words, these parameters indicate a pointin the topic simplex.
Thus, in this paper, we usethe largest elements of the parameters to infer thetopic (as an approximation of context) to which agiven text belongs.3 Evaluating Paraphrases with LatentVariable ModelsTo evaluate a paraphrasing pair of sentences, wemust prepare a learning corpus for constructinglatent variable models.
It must be organized sothat it consist of documents, and each documentmust be implicated in a specific context.Both latent variable models pLSI and LDA re-quire vector format data for their learning.
In thispaper, we follow the bag-of-words approach andprepare vector data that consist of words and theirfrequency for each document in the learning cor-pus.After constructing the pLSI and LDA models,we can infer a topic by using the models with vec-tor data that correspond to a target sentence.Thevector data for the target sentence are constructedby using the target sentence and the sentences thatsurround it.
From these sentences, the vector datathat correspond to the target sentence are con-structed.
We call the number of sentences usedto construct vector data ?window size.
?Evaluating a paraphrasing pair (P1:P2) issimple.
Construct vector data (vec(P1) andvec(P2)) and infer contexts (T (P1) and T (P2))by using a latent variable model.
Using pLSI, thetopic that indicates the highest probability is usedas the inferred result, and using LDA, the largestparameter that corresponds to the topic is used asthe inferred result.
If topics T (P1) and T (P2)are different, the sentences might be used in dif-ferent contexts, and the paraphrasing pair wouldbe contextually dependent; otherwise, the para-phrasing pair would be contextually independent.4 ExperimentsWe carried out several experiments that automati-cally evaluated extracted paraphrases with pLSIand LDA.
To carry out these experiments, weused plsi-0.031 by Kudo for pLSI and lda-c2toolkit by Blei (Blei et al, 2003) for LDA.4.1 Data setWe used a bilingual corpus of travel conversationcontaining Japanese sentences and correspond-ing English translations (Takezawa et al, 2002).Since the translations were made sentence by sen-tence, this corpus was sentence-aligned from itsorigin and consisted of 162,000 sentence pairs.The corpus was manually and roughly anno-tated with topics.
Each topic had a two-levelhierarchical structure whose first level consistedof 19 topics.
Each first-level topic had severalsubtopics.
The second level consisted of 218 top-ics, after expanding all subtopics of each topicin the first level.
A rough annotation exampleis shown in Table 1; the hierarchical structure ofthis topic seems unorganized.
For example, in thefirst-level topic, there are topics labeled basic andcommunication, which seem to overlap.1http://chasen.org/?taku/software/plsi/2http://www.cs.berkeley.edu/?blei/lda-c/67Table 1: Examples of manually annotated topicssentence 1st topic 2nd topicWhere is the nearest department store?
shopping buying somethingThat?s too flashy for me.
shopping choosing somethingThere seems to be a mistake on my bill.
staying checkoutThere seems to be a mistake on my bill.
staying complainingIn the corpus, however, there is an obvioustextual cohesion such that sentences of the sametopic are locally gathered.
Each series of sen-tences can be used as a document for a text model.Under the assumption that each series of sen-tences is a document, the average number of sen-tences included in a document is 18.7, and the av-erage number of words included in a document is44.9.4.2 Extracting paraphrasesA large collection of parallel texts contains manysentences in one language that correspond tothe same expression in the other language fortranslation.
For example, if Japanese sentencesJi1, ..., Jim correspond to English sentence Ei,then these Japanese sentences would be para-phrases.We utilized a very simple method to extractJapanese paraphrases from the corpus.
First, weextracted duplicate English sentences by exactmatching.
From the learning set, 18,505 sen-tences were extracted.
Second, we collectedJapanese sentences that correspond to each ex-tracted English sentence.
Next, we obtained setsof Japanese sentences collected by using Englishsentences as pivots.
In the corpus, one Englishsentence averaged almost 4.5 Japanese sentences,but this number included duplicate sentences.If duplicate sentences are excluded, the averagenumber of Japanese sentences corresponding toan English sentence becomes 2.4.
Finally, weobtained 944,547 Japanese paraphrasing pairs bycombining sentences in each group of Japanesesentences.4.3 Comparing human judgement andinference by latent variable modelsIn this section, we determine the difference be-tween manually annotated topics and inferenceresults using pLSI and LDA.
We originally con-sidered evaluating each paraphrase as a binaryclassification problem that determines whetherboth sentences of the paraphrase are used in thesame context.
We evaluated the inferred resultsby comparison with the manually annotated top-ics, and thus accuracy could be calculated whenthe manually annotated topics were correct.
How-ever, accuracy is inappropriate for evaluating re-sults inferred by a latent variable model, since thetopics were roughly annotated by humans as men-tioned in Section 4.1.
Accordingly, we employedKappa statistics as a rough guide for the correct-ness of the inferred results by latent variable mod-els.Tables 2 and 3 show the comparison results,where the window size is 11 (the target sentence+ the previous five and the following five sen-tences).
When constructing pLSI models, the pa-rameter for tempered EM (TEM) is set to 0.9 (weuse this value in all of the experiments in this pa-per), because it showed the best performance inpreliminary experiments.
We performed the ex-periments on several topics.Table 2: Comparing results of first-level topic(19)# of topics ?
by pLSI ?
by LDA10 0.4812 0.479820 0.5085 0.518530 0.5087 0.509440 0.5392 0.524550 0.5185 0.4897window size = 11As mentioned in Sections 2.1 and 2.2, we cantreat inference results as vector data.
Thus, wecan use a metric to classify the two vectors thatcorrespond to the inferred results of any two givensentences.
We use cosine as a metric and con-68Table 3: Comparing results of second-level topic(218)# of topicss ?
by pLSI ?
by LDA30 0.3523 0.388340 0.3663 0.409350 0.4122 0.411160 0.4184 0.418670 0.4196 0.413380 0.3665 0.370290 0.3437 0.3596100 0.3076 0.3526window size = 11ducted comparison experiments for the first- andsecond-level topics, as shown in Tables 4 and 5.The threshold values used to judge whether topicsare the same are indicated in the parentheses.Table 4: Comparing results of first-level topic(19) with cosine metric# of topics ?
by pLSI ?
by LDA10 0.4873(0.5) 0.5042(0.5)20 0.5230(10?6) 0.5841(0.5)30 0.5502(10?6) 0.5672(0.5)40 0.5808(10?6) 0.5871(0.5)50 0.5611(10?6) 0.5573(0.5)window size = 11Table 5: Comparing results of second-level topic(218) with cosine metric# of topics ?
by pLSI ?
by LDA30 0.3536(0.5) 0.3726(0.5)40 0.3679(0.5) 0.4006(0.5)50 0.4127(0.5) 0.4085(0.5)60 0.4186(0.5) 0.4218(0.5)70 0.4202(0.5) 0.4202(0.5)80 0.3733(0.5) 5.2 ?
10?7(0.5)window size = 11We also performed an experiment to confirmthe relationship between Kappa statistics andwindow-size context.
Experiments were done un-der the following conditions: the number of topicswas 20 for both pLSI and LDA, Kappa statisticswere calculated for the first-level topic, and win-dow sizes were 5, 11, 15, 21, 25, and 31.
Table 6Table 6: Window size and Kappa statistics forfirst-level annotationwindow pLSI LDAsize (20 topics) (20 topics)5 0.4580 0.252711 0.5085 0.518515 0.5165 0.544021 0.4613 0.539625 0.3286 0.528631 0.1730 0.5157shows the experimental results.The actual computing time needed to evaluate944,547 paraphrases with a Pentium M 1.4-GHz,1-GB memory computer is shown in Table 7.
It isimportant to note that the inference program forpLSI was written in Perl, but for LDA it was writ-ten in C.Table 7: Computing time to evaluate paraphrases# of topics pLSI LDA20 665 sec.
996 sec.60 1411 sec.
2223 sec.window size = 154.4 Experiments from paraphrasingperspectivesTo investigate the upper bound of our method, wecarried out several experiments.
So far in this pa-per, we have discussed topic information as anapproximation of contextual information by com-paring topics annotated by humans and automati-cally inferred by pLSI and LDA.
However, sinceour goal is to evaluate paraphrases, we need todetermine whether latent variable models detect adifference of topics for sentences of paraphrases.First, we randomly selected 1% of the Englishseed sentences.
Each sentence corresponds toseveral Japanese sentences, so we could produceJapanese paraphrasing pairs.
The number of se-lected English sentences was 185.Second, we generated 9,091 Japanese para-phrasing pairs from the English seed sentences.However, identical sentences existed in some gen-erated paraphrasing pairs.
In other words, thesesentences were simply collected from different69places in the corpus.
From a paraphrasing per-spective, such pairs are useless.
Thus we removedthem and randomly selected one pair from oneEnglish seed sentence.Finally, we sampled 117 paraphrasing pairsand evaluated them based on a paraphrasing per-spective: whether a paraphrase is contextuallyindependent.
There were 71 contextually inde-pendent paraphrases and 37 contextually depen-dent paraphrases.
Nine paraphrases had prob-lems, all of which were caused by translation er-rors.
The phrase ?contextually independent para-phrases?
means that the paraphrases can be usedin any context and can be applied as two-wayparaphrases.
On the other hand, ?contextually de-pendent paraphrases?
means that the paraphrasesare one-way, and so we have to give considerationto the direction of each paraphrase.Table 8: Evaluation with manually annotated la-belsindependent dependentsame diff.
same diff.1st level 46 25 18 192nd level 25 46 11 26We removed the nine problematic paraphras-ing pairs and evaluated the remaining sampleswith manually annotated topic labels, as shownin Table 8.
According to the basic idea of thismethod, a contextually independent paraphras-ing pair should be judged as having the sametopic, and a contextually dependent pair shouldbe judged as having a different topic.
Thus, weintroduced a criterion to evaluate labeling resultsin terms of an error rate, defined as follows:Error rate = |Dindep|+ |Sdep|# of judged pairs , (7)where Dindep denotes a set that consists of para-phrasing pairs that are judged as having differ-ent topics but are contextually independent.
Onthe other hand, Sdep denotes a set that consists ofparaphrasing pairs that are judged as having thesame topic, but are contextually dependent.For example, from the results in Table 8, theerror rate of the results for the first-level topic is0.398 ((25 + 18)/108), and that for the second-level topic is 0.528 ((46 + 11)/108).To estimate the upper bound of this method, wealso investigated potentially unavoidable errors.Several paraphrasing pairs are used for the exactsame topic, but they seem contextually dependentbecause several words are different.
On the otherhand, some paraphrasing pairs seem to be usedin obviously different topics but are contextuallyindependent.
Table 9 shows the investigation re-sults; at least ten paraphrasing pairs seem contex-tually independent but are actually used in differ-ent topics.
In addition, there are at least 15 para-phrasing pairs whose topic is obviously the same,but several differences of words make them con-textually dependent.
Moreover, in this case, theerror rate is 0.231 ((15+10)/108), meaning that itis difficult to judge all of the paraphrasing pairscorrectly by using only topic (contextual) infor-mation.
Thus, this method?s upper bound of ac-curacy when using only topic information is esti-mated to be around 77%.Table 9: Potential upper bound of this methodhuman judgement human judgementfrom paraphrasing based on topicperspective same differentindependent 61 10dependent 15 22We prepared several latent variable modelsto investigate the performance of the proposedmethod and applied it to the sampled paraphras-ing sentences mentioned above.
Table 10 showsthe evaluation results.5 DiscussionFirst, there is no major performance differencebetween pLSI and LDA in paraphrasing evalu-ation.
On average, LDA is slightly better thanpLSI.
Blei et al showed that LDA outperformspLSI in (Blei et al, 2003); however, in some ofthe cases shown in Tables 2 and 3, pLSI outper-forms LDA.
On the contrary, using a cosine met-ric, LDA has a significant problem: it loses its dis-tinguishing ability when the number of topics (la-tent variables) becomes large.
With such a largenumber of topics, LDA always infers a point nearthe gravity point of the topic simplex.
In addition,using a cosine metric also requires a threshold to70Table 10: Evaluating contextual dependency of paraphrases by latent variable modelsmodel window independent dependent corrected(threshold) size same diff.
same diff.
err.
rate err.
ratepLSI20 11 43 28 14 23 0.3889 0.2048pLSI20 15 39 32 14 23 0.4259 0.2530pLSI40 11 33 38 12 25 0.4630 0.3012pLSI40 15 34 37 16 21 0.4907 0.3373pLSI20cos(10?6) 11 45 26 17 20 0.3981 0.2169pLSI20cos(10?6) 15 31 40 15 22 0.5093 0.3614pLSI40cos(10?6) 11 43 28 17 20 0.4167 0.2410pLSI40cos(10?6) 15 29 42 13 24 0.5093 0.3614LDA20 11 39 32 19 18 0.4722 0.3133LDA20 15 42 29 16 21 0.4167 0.2410LDA40 11 40 31 14 23 0.4167 0.2410LDA40 15 35 36 15 22 0.4722 0.3133LDA20cos(0.5) 11 49 22 23 14 0.4167 0.2410LDA20cos(0.5) 15 51 20 21 16 0.3796 0.1928LDA40cos(0.5) 11 47 24 18 19 0.3889 0.2048LDA40cos(0.5) 15 43 28 17 20 0.4167 0.24101st-level topic ?
46 25 18 19 0.3981 0.2169judge a pair of paraphrasing sentences.From Table 6, LDA seems robust against theinclusion of noisy sentences with a large window,but it is easily affected by a small window.
Onthe other hand, pLSI seems robust against infor-mation shortages due to a small window, but it isnot effective with a large window.
The best per-formances were shown at window size 15 for bothpLSI and LDA, since the average number of sen-tences in a document (segment) is 18.7, as shownin Section 4.1.Table 7 shows that in spite of the difference inprograming language, pLSI is faster than LDA inpractice.
In addition, Table 8 reveals that judgingthe contextual dependency of paraphrasing pairsdoes not require fine-grained topics.From the results shown in Table 10, we canconclude that topic inference by latent variablemodels resembles context judgement by humansas recorded in error rate.
However, we note thatthe error rate was not weighted for contextuallyindependent or dependent results.
Error rate issimply a relative index.
For example, if there isa result in which all of the inferences reflect thesame topic, then the error rate becomes 0.3426.Thus it is important to detect a contextually de-pendent paraphrase.
Considering these points,pLSI20 with window size 11 shows very good re-sults in Table 10.In Section 4.4, we showed the potential upperbound of this method.
The smallest error rate is0.231, and we can estimate a corrected error bythe following formula:|Dindep|+ |Sdep| ?C# of judged pairs?
C , (8)where C denotes the correction value that cor-responds to the number of paraphrasing pairsjudged incorrectly with only contextual informa-tion.
In our experiments, from the results shownin Table 9, C is set to 25.
From the results shownin Table 10, we can conclude that the performanceof our method is almost the same as that by themanually annotated topics, and the accuracy ofour method is almost 80% for paraphrasing pairsthat can be judged by contextual information.There are several possibilities for improvingaccuracy.
One is using a fixed window to ob-tain contextual information.
Irrelevant sentencesare sometimes included in fixed windows, and la-tent variable models fail on inference.
If we couldinfer a boundary of topics with high accuracy,71we would be able to dynamically detect a pre-cise window using some other reliable text mod-els specialized to text segmentation.So far, we have mainly discussed the contex-tual dependency of paraphrasing pairs.
However,when a paraphrasing pair is contextually depen-dent, it is also important to infer its specific para-phrasing direction.
Unfortunately, we concludethat inferring the paraphrasing direction with con-textual information is difficult.
In the experimen-tal results, however, there were several exampleswhose direction could be inferred from their con-textual information.
Thus, contextual informationmay benefit the inference of paraphrasing direc-tion.
Actually, in the experiments, 11 of 37 con-textual dependent pairs had obvious paraphrasingdirections.
In most of the paraphrasing pairs, dif-ferent words were used or inserted, or some wordswere deleted.
Thus, to infer a paraphrasing direc-tion, we need more specific information for wordsor sentences; for example what words carry spe-cific or generic meaning and so on.One might consider a supervised learningmethod, such as Support Vector Machine, to in-fer topics (e.g., (Lane et al, 2004)).
However, wecannot know the best number of topics for an ap-plication in advance.
Thus, a supervised learningmethod is promising only if we already know thebest number of topics for which we can preparean appropriate learning set.6 ConclusionWe proposed an evaluation method for the contex-tual dependency of paraphrasing pairs using twolatent variable models, pLSI and LDA.
To eval-uate a paraphrasing pair, we used sentences sur-rounding the given sentence as contextual infor-mation and approximated context by topics thatcorrespond to a latent variable of a text model.The experimental results with paraphrases auto-matically extracted from a corpus showed that theproposed method achieved almost 60% accuracy.In addition, there is no major performance differ-ence between pLSI and LDA.
However, they haveslightly different characteristics: LDA is robustagainst noisy sentences with long context, whilepLSI is robust against information shortage dueto short context.
The results also revealed thatany method?s upper bound of accuracy using onlycontextual information is almost 77%.AcknowledgementsThis research was supported in part by the Min-istry of Public Management, Home Affairs, Postsand Telecommunications.ReferencesRegina Barzilay and Kathleen R. McKeown.
2001.Extracting paraphrases from a parallel corpus.
InProceedings of the 39th Annual Meeting of the ACL,pages 50?57.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet alocation.
Journal of Ma-chine Learning Research, 3:993?1022, January.Thomas Hofmann.
1999.
Probabilistic Latent Seman-tic Indexing.
In Proceedings of the 22nd AnnualACM Conference on Research and Development inInformation Retrieval, pages 50?57.Ian R. Lane, Tatsuya Kawahara, Tomoko Matsui, andSatoshi Nakamura.
2004.
Topic classification andverification modeling for out-of-domain utterancedetection.
In Proceedings of ICSLP, pages 2197?2200.Dekang Lin and Patrick Pantel.
2001.
Discoveryof inference rule for question-answering.
NaturalLanguage Engineering, 7(4):343?360.Kiyonori Ohtake and Kazuhide Yamamoto.
2001.Paraphrasing honorifics.
In Workshop Proceedingsof Automatic Paraphrasing: Theories and Appli-cations (NLPRS2001 Post-Conference Workshop),pages 13?20.Mitsuo Shimohata and Eiichiro Sumita.
2002.
Auto-matic paraphrasing based on parallel corpus for nor-malization.
In Proceedings of LREC 2002, pages453?457.Tetsuro Takahashi, Tomoya Iwakura, Ryu Iida, At-sushi Fujita, and Kentaro Inui.
2001.
KURA:A transfer-based lexico-structural paraphrasing en-gine.
In Proceedings of Automatic Paraphras-ing: Theories and Applications (NLPRS2001 Work-shop), pages 37?46.Toshiyuki Takezawa, Eiichiro Sumita, Fumiaki Sug-aya, Hirofumi Yamamoto, and Seiichi Yamamoto.2002.
Toward a broad-coverage bilingual corpusfor speech translation of travel conversations in thereal world.
In Proceedings of LREC 2002, pages147?152.Kazuhide Yamamoto.
2002.
Acquisition of lexicalparaphrases from texts.
In Proceedings of the 2ndInternational Workshop on Computational Termi-nology (Computerm 2002, in conjunction with Col-ing 2002), pages 22?28.72
