Speed and Accuracy in Shallow and Deep Stochastic ParsingRonald M. Kaplan , Stefan Riezler , Tracy Holloway KingJohn T. Maxwell III, Alexander Vasserman and Richard CrouchPalo Alto Research Center, 3333 Coyote Hill Rd., Palo Alto, CA 94304{kaplan|riezler|king|maxwell|avasserm|crouch}@parc.comAbstractThis paper reports some experiments that com-pare the accuracy and performance of twostochastic parsing systems.
The currently pop-ular Collins parser is a shallow parser whoseoutput contains more detailed semantically-relevant information than other such parsers.The XLE parser is a deep-parsing system thatcouples a Lexical Functional Grammar to a log-linear disambiguation component and providesmuch richer representations theory.
We mea-sured the accuracy of both systems against agold standard of the PARC 700 dependencybank, and also measured their processing times.We found the deep-parsing system to be moreaccurate than the Collins parser with only aslight reduction in parsing speed.11 IntroductionIn applications that are sensitive to the meanings ex-pressed by natural language sentences, it has becomecommon in recent years simply to incorporate publiclyavailable statistical parsers.
A state-of-the-art statisticalparsing system that enjoys great popularity in researchsystems is the parser described in Collins (1999) (hence-forth ?the Collins parser?).
This system not only is fre-quently used for off-line data preprocessing, but alsois included as a black-box component for applicationssuch as document summarization (Daume and Marcu,2002), information extraction (Miller et al, 2000), ma-chine translation (Yamada and Knight, 2001), and ques-tion answering (Harabagiu et al, 2001).
This is be-1This research has been funded in part by contract #MDA904-03-C-0404 awarded from the Advanced Research andDevelopment Activity, Novel Intelligence from Massive Dataprogram.
We would like to thank Chris Culy whose original ex-periments inspired this research.cause the Collins parser shares the property of robustnesswith other statistical parsers, but more than other suchparsers, the categories of its parse-trees make grammati-cal distinctions that presumably are useful for meaning-sensitive applications.
For example, the categories ofthe Model 3 Collins parser distinguish between heads,arguments, and adjuncts and they mark some long-distance dependency paths; these distinctions can guideapplication-specific postprocessors in extracting impor-tant semantic relations.In contrast, state-of-the-art parsing systems based ondeep grammars mark explicitly and in much more de-tail a wider variety of syntactic and semantic dependen-cies and should therefore provide even better support formeaning-sensitive applications.
But common wisdom hasit that parsing systems based on deep linguistic grammarsare too difficult to produce, lack coverage and robustness,and also have poor run-time performance.
The Collinsparser is thought to be accurate and fast and thus to repre-sent a reasonable trade-off between ?good-enough?
out-put, speed, and robustness.This paper reports on some experiments that put thisconventional wisdom to an empirical test.
We investi-gated the accuracy of recovering semantically-relevantgrammatical dependencies from the tree-structures pro-duced by the Collins parser, comparing these dependen-cies to gold-standard dependencies which are availablefor a subset of 700 sentences randomly drawn from sec-tion 23 of the Wall Street Journal (see King et al (2003)).We compared the output of the XLE system, adeep-grammar-based parsing system using the EnglishLexical-Functional Grammar previously constructed aspart of the Pargram project (Butt et al, 2002), to thesame gold standard.
This system incorporates sophisti-cated ambiguity-management technology so that all pos-sible syntactic analyses of a sentence are computed inan efficient, packed representation (Maxwell and Ka-plan, 1993).
In accordance with LFG theory, the outputincludes not only standard context-free phrase-structuretrees but also attribute-value matrices (LFG?s f(unctional)structures) that explicitly encode predicate-argument re-lations and other meaningful properties.
XLE selects themost probable analysis from the potentially large candi-date set by means of a stochastic disambiguation com-ponent based on a log-linear (a.k.a.
maximum-entropy)probability model (Riezler et al, 2002).
The stochas-tic component is also ?ambiguity-enabled?
in the sensethat the computations for statistical estimation and selec-tion of the most probable analyses are done efficientlyby dynamic programming, avoiding the need to unpackthe parse forests and enumerate individual analyses.
Theunderlying parsing system also has built-in robustnessmechanisms that allow it to parse strings that are outsidethe scope of the grammar as a shortest sequence of well-formed ?fragments?.
Furthermore, performance parame-ters that bound parsing and disambiguation work can betuned for efficient but accurate operation.As part of our assessment, we also measured the pars-ing speed of the two systems, taking into account allstages of processing that each system requires to produceits output.
For example, since the Collins parser dependson a prior part-of-speech tagger (Ratnaparkhi, 1996), weincluded the time for POS tagging in our Collins mea-surements.
XLE incorporates a sophisticated finite-statemorphology and dictionary lookup component, and itstime is part of the measure of XLE performance.Performance parameters of both the Collins parser andthe XLE system were adjusted on a heldout set consist-ing of a random selection of 1/5 of the PARC 700 depen-dency bank; experimental results were then based on theother 560 sentences.
For Model 3 of the Collins parser, abeam size of 1000, and not the recommended beam sizeof 10000, was found to optimize parsing speed at littleloss in accuracy.
On the same heldout set, parameters ofthe stochastic disambiguation system and parameters forparsing performance were adjusted for a Core and a Com-plete version of the XLE system, differing in the size ofthe constraint-set of the underlying grammar.For both XLE and the Collins parser we wrote con-version programs to transform the normal (tree or f-structure) output into the corresponding relations ofthe dependency bank.
This conversion was relativelystraightforward for LFG structures (King et al, 2003).However, a certain amount of skill and intuition wasrequired to provide a fair conversion of the Collinstrees: we did not want to penalize configurations in theCollins trees that encoded alternative but equally legit-imate representations of the same linguistic properties(e.g.
whether auxiliaries are encoded as main verbs oraspect features), but we also did not want to build intothe conversion program transformations that compensatefor information that Collins cannot provide without ap-pealing to additional linguistic resources (such as identi-fying the subjects of infinitival complements).
We did notinclude the time for dependency conversion in our mea-sures of performance.The experimental results show that stochastic parsingwith the Core LFG grammar achieves a better F-scorethan the Collins parser at a roughly comparable parsingspeed.
The XLE system achieves 12% reduction in errorrate over the Collins parser, that is 77.6% F-score for theXLE system versus 74.6% for the Collins parser, at a costin parsing time of a factor of 1.49.2 Stochastic Parsing with LFG2.1 Parsing with Lexical-Functional GrammarThe grammar used for this experiment was developed inthe ParGram project (Butt et al, 2002).
It uses LFG as aformalism, producing c(onstituent)-structures (trees) andf(unctional)-structures (attribute value matrices) as out-put.
The c-structures encode constituency and linear or-der.
F-structures encode predicate-argument relations andother grammatical information, e.g., number, tense, state-ment type.
The XLE parser was used to produce packedrepresentations, specifying all possible grammar analysesof the input.In our system, tokenization and morphological analy-sis are performed by finite-state transductions arranged ina compositional cascade.
Both the tokenizer and the mor-phological analyzer can produce multiple outputs.
For ex-ample, the tokenizer will optionaly lowercase sentenceinitial words, and the morphological analyzer will pro-duce walk +Verb +Pres +3sg and walk +Noun +Pl forthe input form walks.
The resulting tokenized and mor-phologically analyzed strings are presented to the sym-bolic LFG grammar.The grammar can parse input that has XML de-limited named entity markup: <company>ColumbiaSavings</company> is a major holder of so-called junkbonds.
To allow the grammar to parse this markup,the tokenizer includes an additional tokenization of thestrings whereby the material between the XML markupis treated as a single token with a special morphologi-cal tag (+NamedEntity).
As a fall back, the tokenizationthat the string would have received without that markupis also produced.
The named entities have a single mul-tiword predicate.
This helps in parsing both because itmeans that no internal structure has to be built for thepredicate and because predicates that would otherwise beunrecognized by the grammar can be parsed (e.g., Cie.Financiere de Paribas).
As described in section 5, it wasalso important to use named entity markup in these ex-periments to more fairly match the analyses in the PARC700 dependency bank.To increase robustness, the standard grammar is aug-mented with a FRAGMENT grammar.
This allows sen-tences to be parsed as well-formed chunks specified bythe grammar, in particular as Ss, NPs, PPs, and VPs, withunparsable tokens possibly interspersed.
These chunkshave both c-structures and f-structures corresponding tothem.
The grammar has a fewest-chunk method for de-termining the correct parse.The grammar incorporates a version of OptimalityTheory that allows certain (sub)rules in the grammar to beprefered or disprefered based on OT marks triggered bythe (sub)rule (Frank et al, 1998).
The Complete versionof the grammar uses all of the (sub)rules in a multi-passsystem that depends on the ranking of the OT marks inthe rules.
For example, topicalization is disprefered, butthe topicalization rule will be triggered if no other parsecan be built.
A one-line rewrite of the Complete grammarcreates a Core version of the grammar that moves the ma-jority of the OT marks into the NOGOOD space.
This ef-fectively removes the (sub)rules that they mark from thegrammar.
So, for example, in the Core grammar there isno topicalization rule, and sentences with topics will re-ceive a FRAGMENT parse.
This single-pass Core grammaris smaller than the Complete grammar and hence is faster.The XLE parser also allows the user to adjust per-formance parameters bounding the amount of work thatis done in parsing for efficient but accurate operation.XLE?s ambiguity management technology takes advan-tage of the fact that relatively few f-structure constraintsapply to constituents that are far apart in the c-structure,so that sentences are typically parsed in polynomial timeeven though LFG parsing is known to be an NP-completeproblem.
But the worst-case exponential behavior doesbegin to appear for some constructions in some sentences,and the computational effort is limited by a SKIMMINGmode whose onset is controlled by a user-specified pa-rameter.
When skimming, XLE will stop processing thesubtree of a constituent whenever the amount of work ex-ceeds that user-specified limit.
The subtree is discarded,and the parser will move on to another subtree.
This guar-antees that parsing will be finished within reasonable lim-its of time and memory but at a cost of possibly loweraccuracy if it causes the best analysis of a constituentto be discarded.
As a separate parameter, XLE also letsthe user limit the length of medial constituents, i.e., con-stituents that do not appear at the beginning or the endof a sentence (ignoring punctuation).
The rationale be-hind this heuristic is to limit the weight of constituents inthe middle of the sentence but still to allow sentence-finalheavy constituents.
This discards constituents in a some-what more principled way as it tries to capture the psy-cholinguistic tendency to avoid deep center-embedding.When limiting the length of medial constituents, cubic-time parsing is possible for sentences up to that length,even with a deep, non-context-free grammar, and linearparsing time is possible for sentences beyond that length.The Complete grammar achieved 100% coverage ofsection 23 as unseen unlabeled data: 79% as full parses,21% FRAGMENT and/or SKIMMED parses.2.2 Dynamic Programming for Estimation andStochastic DisambiguationThe stochastic disambiguation model we employ definesan exponential (a.k.a.
log-linear or maximum-entropy)probability model over the parses of the LFG grammar.The advantage of this family of probability distributionsis that it allows the user to encode arbitrary propertiesof the parse trees as feature-functions of the probabilitymodel, without the feature-functions needing to be inde-pendent and non-overlapping.
The general form of con-ditional exponential models is as follows:p?
(x|y) = Z?(y)?1e?
?f(x)where Z?
(y) =?x?X(y) e?
?f(x) is a normalizing con-stant over the set X(y) of parses for sentence y, ?
isa vector of log-parameters, f is a vector of feature-values, and ?
?
f(x) is a vector dot product denoting the(log-)weight of parse x.Dynamic-programming algorithms that allow the ef-ficient estimation and searching of log-linear mod-els from a packed parse representation without enu-merating an exponential number of parses havebeen recently presented by Miyao and Tsujii (2002)and Geman and Johnson (2002).
These algorithms canbe readily applied to the packed and/or-forests ofMaxwell and Kaplan (1993), provided that each conjunc-tive node is annotated with feature-values of the log-linear model.
In the notation of Miyao and Tsujii (2002),such a feature forest ?
is defined as a tuple ?C,D, r, ?, ?
?where C is a set of conjunctive nodes, D is a set of dis-junctive nodes, r ?
C is the root node, ?
: D ?
2C isa conjunctive daughter function, and ?
: C ?
2D is adisjunctive daughter function.A dynamic-programming solution to the problem offinding most probable parses is to compute the weight?d of each disjunctive node as the maximum weight ofits conjunctive daugher nodes, i.e.,?d = maxc??
(d)?c (1)and to recursively define the weight ?c of a conjunctivenode as the product of the weights of all its descendantdisjunctive nodes and of its own weight:?c =?d??
(c)?d e?
?f(c) (2)Keeping a trace of the maximally weighted choices in acomputaton of the weight ?r of the root conjunctive noder allows us to efficiently recover the most probable parseof a sentence from the packed representation of its parses.The same formulae can be employed for an effi-cient calculation of probabilistic expectations of feature-functions for the statistical estimation of the parameters?.
Replacing the maximization in equation 1 by a sum-mation defines the inside weight of disjunctive node.
Cor-respondingly, equation 2 denotes the inside weight of aconjunctive node.
The outside weight ?c of a conjunctivenode is defined as the outside weight of its disjunctivemother node(s):?c =?{d|c??
(d)}?d (3)The outside weight of a disjunctive node is the sum ofthe product of the outside weight(s) of its conjunctivemother(s), the weight(s) of its mother(s), and the insideweight(s) of its disjunctive sister(s):?d =?{c|d??
(c)}{?c e??f(c)?{d?|d???(c),d?
6=d}?d?}
(4)From these formulae, the conditional expectation of afeature-function fi can be computed from a chart withroot node r for a sentence y in the following way:?x?X(y)e??f(x)fi(x)Z?
(y)=?c?C?c?cfi(c)?r(5)Formula 5 is used in our system to compute expectationsfor discriminative Bayesian estimation from partially la-beled data using a first-order conjugate-gradient routine.For a more detailed description of the optimization prob-lem and the feature-functions we use for stochastic LFGparsing see Riezler et al (2002).
We also employed acombined `1 regularization and feature selection tech-nique described in Riezler and Vasserman (2004) thatconsiderably speeds up estimation and guarantees smallfeature sets for stochastic disambiguation.
In the experi-ments reported in this paper, however, dynamic program-ming is crucial for efficient stochastic disambiguation,i.e.
to efficiently find the most probable parse from apacked parse forest that is annotated with feature-values.There are two operations involved in stochastic disam-biguation, namely calculating feature-values from a parseforest and calculating node weights from a feature forest.Clearly, the first one is more expensive, especially forthe extraction of values for non-local feature-functionsover large charts.
To control the cost of this compu-tation, our stochastic disambiguation system includesa user-specified parameter for bounding the amount ofwork that is done in calculating feature-values.
When theuser-specified threshold for feature-value calculation isreached, this computation is discontinued, and the dy-namic programming calculation for most-probable-parsesearch is computed from the current feature-value anno-tation of the parse forest.
Since feature-value computa-tion proceeds incrementally over the feature forest, i.e.for each node that is visited all feature-functions that ap-ply to it are evaluated, a complete feature annotation canbe guaranteed for the part of the and/or-forest that is vis-ited until discontinuation.
As discussed below, these pa-rameters were set on a held-out portion of the PARC700which was also used to set the Collins parameters.In the experiments reported in this paper, we used athreshold on feature-extraction that allowed us to cut offfeature-extraction in 3% of the cases at no loss in accu-racy.
Overall, feature extraction and weight calculationaccounted for 5% of the computation time in combinedparsing and stochastic selection.3 The Gold-Standard Dependency BankWe used the PARC 700 Dependency Bank (DEPBANK)as the gold standard in our experiments.
The DEPBANKconsists of dependency annotations for 700 sentences thatwere randomly extracted from section 23 of the UPennWall Street Journal (WSJ) treebank.
As described by(King et al, 2003), the annotations were boot-strappedby parsing the sentences with a LFG grammar and trans-forming the resulting f-structures to a collection of depen-dency triples in the DEPBANK format.
To prepare a truegold standard of dependencies, the tentative set of depen-dencies produced by the robust parser was then correctedand extended by human validators2.
In this format eachtriple specifies that a particular relation holds between ahead and either another head or a feature value, for ex-ample, that the SUBJ relation holds between the headsrun and dog in the sentence The dog ran.
Average sen-tence length of sentences in DEPBANK is 19.8 words, andthe average number of dependencies per sentence is 65.4.The corpus is freely available for research and evaluation,as are documentation and tools for displaying and prun-ing structures.3In our experiments we used a Reduced version of theDEPBANK, including just the minimum set of dependen-cies necessary for reading out the central semantic rela-tions and properties of a sentence.
We tested against thisReduced gold standard to establish accuracy on a lowerbound of the information that a meaning-sensitive appli-cation would require.
The Reduced version contained allthe argument and adjunct dependencies shown in Fig.1, and a few selected semantically-relevant features, asshown in Fig.
2.
The features in Fig.
2 were chosen be-2The resulting test set is thus unseen to the grammar andstochastic disambiguation system used in our experiments.
Thisis indicated by the fact that the upperbound of F-score for thebest matching parses for the experiment grammar is in the rangeof 85%, not 100%.3http://www2.parc.com/istl/groups/nltt/fsbank/Function Meaningadjunct adjunctsaquant adjectival quantifiers (many, etc.
)comp complement clauses (that, whether)conj conjuncts in coordinate structuresfocus int fronted element in interrogativesmod noun-noun modifiersnumber numbers modifying nounsobj objectsobj theta secondary objectsobl obliqueobl ag demoted subject of a passiveobl compar comparative than/as clausesposs possessives (John?s book)pron int interrogative pronounspron rel relative pronounsquant quantifiers (all, etc.
)subj subjectstopic rel fronted element in relative clausesxcomp non-finite complementsverbal and small clausesFigure 1: Grammatical functions in DEPBANK.cause it was felt that they were fundamental to the mean-ing of the sentences, and in fact they are required by thesemantic interpreter we have used in a knowledge-basedapplication (Crouch et al, 2002).Feature Meaningadegree degree of adjectives and adverbs(positive, comparative, superlative)coord form form of a coordinatingconjunction (e.g., and, or)det form form of a determiner (e.g., the, a)num number of nouns (sg, pl)number type cardinals vs. ordinalspassive passive verb (e.g., It was eaten.
)perf perfective verb (e.g., have eaten)precoord form either, neitherprog progressive verb (e.g., were eating)pron form form of a pronoun (he, she, etc.
)prt form particle in a particle verb(e.g., They threw it out.
)stmt type statement type (declarative,interrogative, etc.
)subord form subordinating conjunction (e.g.
that)tense tense of the verb (past, present, etc.
)Figure 2: Selected features for Reduced DEPBANK.As a concrete example, the dependency list in Fig.
3 isthe Reduced set corresponding to the following sentence:He reiterated his opposition to such funding,but expressed hope of a compromise.An additional feature of the DEPBANK that is relevantto our comparisons is that dependency heads are rep-resented by their standard citation forms (e.g.
the verbswam in a sentence appears as swim in its dependencies).We believe that most applications will require a conver-sion to canonical citation forms so that semantic relationscan be mapped into application-specific databases or on-tologies.
The predicates of LFG f-structures are alreadyrepresented as citation forms; for a fair comparison weran the leaves of the Collins tree through the same stem-mer modules as part of the tree-to-dependency transla-tion.
We also note that proper names appear in the DEP-BANK as single multi-word expressions without any in-ternal structure.
That is, there are no dependencies hold-ing among the parts of people names (A. Boyd Simpson),company names (Goldman, Sachs & Co), and organiza-tion names (Federal Reserve).
This multiword analysiswas chosen because many applications do not requirethe internal structure of names, and the identification ofnamed entities is now typically carried out by a separatenon-syntactic pre-processing module.
This was capturedfor the LFG parser by using named entity markup and forthe Collins parser by creating complex word forms witha single POS tag (section 5).conj(coord?0, express?3)conj(coord?0, reiterate?1)coord form(coord?0, but)stmt type(coord?0, declarative)obj(reiterate?1, opposition?6)subj(reiterate?1, pro?7)tense(reiterate?1, past)obj(express?3, hope?15)subj(express?3, pro?7)tense(express?3, past)adjunct(opposition?6, to?11)num(opposition?6, sg)poss(opposition?6, pro?19)num(pro?7, sg)pron form(pro?7, he)obj(to?11, funding?13)adjunct(funding?13, such?45)num(funding?13, sg)adjunct(hope?15, of?46)num(hope?15, sg)num(pro?19, sg)pron form(pro?19, he)adegree(such?45, positive)obj(of?46, compromise?54)det form(compromise?54, a)num(compromise?54, sg)Figure 3: Reduced dependency relations for He reiteratedhis opposition to such funding, but expressed hope of acompromise.4 Conversion to Dependency Bank FormatA conversion routine was required for each system totransform its output so that it could be compared to theDEPBANK dependencies.
While it is relatively straightfor-ward to convert LFG f-structures to the dependency bankformat because the f-structure is effectively a dependencyformat, it is more difficult to transform the output trees ofthe Model 3 Collins parser in a way that fairly allocatesboth credits and penalties.LFG Conversion We discarded the LFG tree structuresand used a general rewriting system previously developedfor machine translation to rewrite the relevant f-structureattributes as dependencies (see King et al (2003)).
Therewritings involved some deletions of irrelevant features,some systematic manipulations of the analyses, and sometrivial respellings.
The deletions involved features pro-duced by the grammar but not included in the PARC 700such as negative values of PASS, PERF, and PROG andthe feature MEASURE used to mark measure phrases.
Themanipulations are more interesting and are necessary tomap systematic differences between the analyses in thegrammar and those in the dependency bank.
For example,coordination is treated as a set by the LFG grammar but asa single COORD dependency with several CONJ relationsin the dependency bank.
Finally, the trivial rewritingswere used to, for example, change STMT-TYPE decl inthe grammar to STMT-TYPE declarative in the de-pendency bank.
For the Reduced version of the PARC700 substantially more features were deleted.Collins Model 3 Conversion An abbreviated represen-tation of the Collins tree for the example above is shownin Fig.
4.
In this display we have eliminated the head lex-ical items that appear redundantly at all the nonterminalsin a head chain, instead indicating by a single numberwhich daughter is the head.
Thus, S?2 indicates that thehead of the main clause is its second daughter, the VP,and its head is its first VP daughter.
Indirectly, then, thelexical head of the S is the first verb reiterated.
(TOP?1(S?2 (NP-A?1 (NPB?1 He/PRP))(VP?1 (VP?1 reiterated/VBD(NP-A?1 (NPB?2 his/PRP$opposition/NN)(PP?1 to/TO(NPB?2 such/JJfunding/NN))))but/CC(VP?1 expressed/VBD(NP-A?1 (NPB?1 hope/NN)(PP?1 of/IN(NP-A?1 (NPB?2 a/DTcompromise/NN))))))))Figure 4: Collins Model 3 tree for He reiterated his op-position to such funding, but expressed hope of a compro-mise.The Model 3 output in this example includes standardphrase structure categories, indications of the heads, andthe additional -A marker to distinguish arguments fromadjuncts.
The terminal nodes of this tree are inflectedforms, and the first phase of our conversion replaces themwith their citation forms (the verbs reiterate and express,and the decapitalized and standardized he for He and his).We also adjust for systematic differences in the choice ofheads.
The first conjunct tends to be marked as the headof a coordination in Model 3 output, whereas the depen-dency bank has a more symmetric representation: it in-troduces a new COORD head and connects that up to theconjunction, and it uses a separate CONJ relation for eachof the coordinated items.
Similarly, Model 3 identifiesthe syntactic markers to and that as the heads of com-plements, whereas the dependency bank treats these asselectional features and marks the main predicate of thecomplements as the head.
These adjustments are carriedout without penalty.
We also compensate for the differ-ences in the representation of auxiliaries: Model 3 treatsthese as main verbs with embedded complements insteadof the PERF, PROG, and PASSIVE features of the DEP-BANK, and our conversion flattens the trees so that thefeatures can be read off.The dependencies are read off after these and a fewother adjustments are made.
NPs under VPs are read offeither as objects or adjuncts, depending on whether ornot the NP is annotated with the argument indicator (-A)as in this example; the -A presumably would be miss-ing in a sentence like John arrived Friday, and Fridaywould be treated as an ADJUNCT.
Similarly, NP-As un-der S are read off as subject.
In this example, however,this principle of conversion does not lead to a match withthe dependency bank: in the DEPBANK grammatical rela-tions that are factored out of conjoined structures are dis-tributed back into those structures, to establish the correctsemantic dependencies (in this case, that he is the subjectof both reiterate and express and not of the introducedcoord).
We avoided the temptation of building coordinatedistribution into the conversion routine because, first, it isnot always obvious from the Model 3 output when dis-tribution should take place, and second, that would bea first step towards building into the conversion routinethe deep lexical and syntactic knowledge (essentially thefunctional component of our LFG grammar) that the shal-low approach explicitly discounts4.For the same reasons our conversion routine does notidentify the subjects of infinitival complements with par-ticular arguments of matrix verbs.
The Model 3 trees pro-vide no indication of how this is to be done, and in manycases the proper assignment depends on lexical informa-tion about specific predicates (to capture, for example, thewell-known contrast between promise and persuade).Model 3 trees also provide information about certain4However, we did explore a few of these additional transfor-mations and found only marginal F-score increases.long-distance dependencies, by marking with -g annota-tions the path between a filler and a gap and marking thegap by an explicit TRACE in the terminal string.
The filleritself is not clearly identified, but our conversion treatsall WH categories under SBAR as potential fillers andattempts to propagate them down the gap-chain to linkthem up to appropriate traces.In sum, it is not a trivial matter to convert a Model 3tree to an appropriate set of dependency relations, and theprocess requires a certain amount of intuition and skill.For our experiments we tried to define a conversion thatgives appropriate credit to the dependencies that can beread from the trees without relying on an undue amountof sophisticated linguistic knowledge5.5 ExperimentsWe conducted our experiments by preparing versions ofthe test sentences in the form appropriate to each sys-tem.
We used a configuration of the XLE parser that ex-pects sentences conforming to ordinary text conventionsto appear in a file separated by double line-feeds.
A cer-tain amount of effort was required to remove the part-of-speech tags and labeled brackets of the WSJ corpus in away that restored the sentences to a standard English for-mat (for example, to remove the space between wo and n?tthat remains when the POS tags are removed).
Since thePARC 700 treats proper names as multiword expressions,we then augmented the input strings with XML markupof the named entities.
These are parsed by the grammaras described in section 2.
We used manual named entitymarkup for this experiment because our intent is to mea-sure parsing technology independent of either the timeor errors of an automatic named-entity extractor.
How-ever, in other experiments with an automatic finite-stateextractor, we have found that the time for named-entityrecognition is negligible (on the order of seconds acrossthe entire corpus) and makes relatively few errors, so thatthe results reported here are good approximations of whatmight be expected in more realistic situations.As input to the Collins parser, we used the part-of-speech tagged version of section 23 that was providedwith the parser.
From this we extracted the 700 sentencesin the PARC 700.
We then modified them to producenamed entity input so that the parses would match thePARC 700.
This was done by putting underscores be-tween the parts of the named entity and changing the finalpart of speech tag to the appropriate one (usually NNP)if necessary.
(The number of words indicated at the be-ginning of the input string was also reduced accordingly.
)An example is shown in (1).5The results of this conversion are available athttp://www2.parc.com/istl/groups/nltt/fsbank/(1) Sen. NNP Christopher NNP Dodd NNP ??Sen.
Christopher Dodd NNPAfter parsing, the underscores were converted to spacesto match the PARC 700 predicates.Before the final evaluation, 1/5 of the PARC 700 de-pendency bank was randomly extracted as a heldout set.This set was used to adjust the performance parameters ofthe XLE system and the Collins parser so as to optimizeparsing speed without losing accuracy.
For example, thelimit on the length of medial phrases was set to 20 wordsfor the XLE system (see Sec.
2), and a regularizer penaltyof 10 was found optimal for the `1 prior used in stochas-tic disambiguation.
For the Collins parser, a beam sizeof 1000 was found to improve speed considerably at lit-tle cost in accuracy.
Furthermore, the np-bracketing flag(npbflag) was set to 0 to produce an extended set of NPlevels for improved argument/adjunct distinction6.
The fi-nal evaluation was done on the remaining 560 examples.Timing results are reported in seconds of CPU time7.
POStagging of the input to the Collins parser took 6 secondsand this was added to the timing result of the Collinsparser.
Time spent for finite-state morphology and dictio-nary lookup for XLE is part of the measure of its timingperformance.
We did not include the time for dependencyextraction or stemming the Collins output.Table 1 shows timing and accuracy results for the Re-duced dependency set.
The parser settings compared areModel 3 of the Collins parser adjusted to beam size 1000,and the Core and Complete versions of the XLE sys-tem, differing in the size of the grammar?s constraint-set.
Clearly, both versions of the XLE system achieve asignificant reduction in error rate over the Collins parser(12% for the core XLE system and 20% for the completesystem) at an increase in parsing time of a factor of only1.49 for the core XLE system.
The complete version givesan overall improvement in F-score of 5% over the Collinsparser at a cost of a factor of 5 in parsing time.Table 1: Timing and accuracy results for Collins parserand Complete and Core versions of XLE system on Re-duced version of PARC 700 dependency bank.time prec.
rec.
F-scoreLFG core 298.88 79.1 76.2 77.6LFG complete 985.3 79.4 79.8 79.6Collins 1000 199.6 78.3 71.2 74.66A beam size of 10000 as used in Collins (1999) improvedthe F-score on the heldout set only by .1% at an increase of pars-ing time by a factor of 3.
Beam sizes lower than 1000 decreasedthe heldout F-score significantly.7All experiments were run on one CPU of a dual proces-sor AMD Opteron 244 with 1.8 GHz and 4GB main memory.Loading times are included in CPU times.6 ConclusionWe presented some experiments that compare the accu-racy and performance of two stochastic parsing systems,the shallow Collins parser and the deep-grammar-basedXLE system.
We measured the accuracy of both systemsagainst a gold standard derived from the PARC 700 de-pendency bank, and also measured their processing times.Contrary to conventional wisdom, we found that the shal-low system was not substantially faster than the deepparser operating on a core grammar, while the deep sys-tem was significantly more accurate.
Furthermore, ex-tending the grammar base of the deep system results inmuch better accuracy at a cost of a factor of 5 in speed.Our experiment is comparable to recent work on read-ing off Propbank-style (Kingsbury and Palmer, 2002)predicate-argument relations from gold-standard tree-bank trees and automatic parses of the Collins parser.Gildea and Palmer (2002) report F-score results in the55% range for argument and boundary recognition basedon automatic parses.
From this perspective, the nearly75% F-score that is achieved for our deterministic rewrit-ing of Collins?
trees into dependencies is remarkable,even if the results are not directly comparable.
Our scoresand Gildea and Palmer?s are both substantially lower thanthe 90% typically cited for evaluations based on labeledor unlabeled bracketing, suggesting that extracting se-mantically relevant dependencies is a more difficult, butwe think more valuable, task.ReferencesMiriam Butt, Helge Dyvik, Tracy Holloway King, Hi-roshi Masuichi, and Christian Rohrer.
2002.
Theparallel grammar project.
In Proceedings of COL-ING2002, Workshop on Grammar Engineering andEvaluation, pages 1?7.Michael Collins.
1999.
Head-Driven Statistical Modelsfor Natural Language Parsing.
Ph.D. thesis, Univer-sity of Pennsylvania.D.
Crouch, C. Condoravdi, R. Stolle, T.H.
King,V.
de Paiva, J. Everett, and D. Bobrow.
2002.
Scal-ability of redundancy detection in focused documentcollections.
In Proceedings of Scalable Natural Lan-guage Understanding, Heidelberg.Hal Daume and Daniel Marcu.
2002.
A noisy-channelmodel for document compression.
In Proceedings ofthe 40th Annual Meeting of the Association for Com-putational Linguistics (ACL?02), Philadelphia, PA.Anette Frank, Tracy H. King, Jonas Kuhn, and JohnMaxwell.
1998.
Optimality theory style constraintranking in large-scale LFG grammars.
In Proceedingsof the Third LFG Conference.Stuart Geman and Mark Johnson.
2002.
Dynamicprogramming for parsing and estimation of stochaticunification-based grammars.
In Proceedings of the40th Annual Meeting of the Association for Compu-tational Linguistics (ACL?02), Philadelphia, PA.Dan Gildea and Martha Palmer.
2002.
The necessityof parsing for predicate argument recognition.
In Pro-ceedings of the 40th Annual Meeting of the Associationfor Computational Linguistics (ACL?02), Philadelphia.Sanda Harabagiu, Dan Moldovan, Marius Pas?ca, RadaMihalcea, Mihai Surdeanu, Ra?zvan Bunescu, RoxanaG?
?rju, Vasile Rus, and Paul Mora?rescu.
2001.
Therole of lexico-semantic feedback in open-domain tex-tual question-answering.
In Proceedings of the 39thAnnual Meeting and 10th Conference of the EuropeanChapter of the Asssociation for Computational Lin-guistics (ACL?01), Toulouse, France.Tracy H. King, Richard Crouch, Stefan Riezler, MaryDalrymple, and Ronald M. Kaplan.
2003.
The PARC700 dependency bank.
In Proceedings of the Work-shop on ?Linguistically Interpreted Corpora?
at the10th Conference of the European Chapter of the Asso-ciation for Computational Linguistics (LINC?03), Bu-dapest, Hungary.Paul Kingsbury and Martha Palmer.
2002.
From tree-bank to propbank.
In Proceedings of the 3rd Interna-tional Conference on Language Resources and Evalu-ation (LREC?02), Las Palmas, Spain.John Maxwell and Ron Kaplan.
1993.
The interface be-tween phrasal and functional constraints.
Computa-tional Linguistics, 19(4):571?589.Scott Miller, Heidi Fox, Lance Ramshaw, and RalphWeischedel.
2000.
A novel use of statistical parsingto extract information from text.
In Proceedings ofthe 1st Conference of the North American Chapter ofthe Association for Computational Linguistics (ANLP-NAACL 2000), Seattle, WA.Yusuke Miyao and Jun?ichi Tsujii.
2002.
Maximumentropy estimation for feature forests.
In Proceed-ings of the Human Language Technology Conference(HLT?02), San Diego, CA.Adwait Ratnaparkhi.
1996.
A maximum entropy modelfor part-of-speech tagging.
In Proceedings of EMNLP-1.Stefan Riezler and Alexander Vasserman.
2004.
Gradi-ent feature testing and `1 regularization for maximumentropy parsing.
Submitted for publication.Stefan Riezler, Tracy H. King, Ronald M. Kaplan,Richard Crouch, John T. Maxwell, and Mark John-son.
2002.
Parsing the Wall Street Journal using aLexical-Functional Grammar and discriminative esti-mation techniques.
In Proceedings of the 40th AnnualMeeting of the Association for Computational Linguis-tics (ACL?02), Philadelphia, PA.Kenji Yamada and Kevin Knight.
2001.
A syntax-basedstatistical translation model.
In Proceedings of the39th Annual Meeting and 10th Conference of the Eu-ropean Chapter of the Asssociation for ComputationalLinguistics (ACL?01), Toulouse, France.
