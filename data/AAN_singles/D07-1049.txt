Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
468?476, Prague, June 2007. c?2007 Association for Computational LinguisticsSmoothed Bloom filter language models: Tera-Scale LMs on the CheapDavid Talbot and Miles OsborneSchool of Informatics, University of Edinburgh2 Buccleuch Place, Edinburgh, EH8 9LW, UKd.r.talbot@sms.ed.ac.uk, miles@inf.ed.ac.ukAbstractA Bloom filter (BF) is a randomised datastructure for set membership queries.
Itsspace requirements fall significantly belowlossless information-theoretic lower boundsbut it produces false positives with somequantifiable probability.
Here we presenta general framework for deriving smoothedlanguage model probabilities from BFs.We investigate how a BF containing n-gramstatistics can be used as a direct replacementfor a conventional n-gram model.
Recentwork has demonstrated that corpus statisticscan be stored efficiently within a BF, herewe consider how smoothed language modelprobabilities can be derived efficiently fromthis randomised representation.
Our pro-posal takes advantage of the one-sided errorguarantees of the BF and simple inequali-ties that hold between related n-gram statis-tics in order to further reduce the BF stor-age requirements and the error rate of thederived probabilities.
We use these modelsas replacements for a conventional languagemodel in machine translation experiments.1 IntroductionLanguage modelling (LM) is a crucial component instatistical machine translation (SMT).
Standard n-gram language models assign probabilities to trans-lation hypotheses in the target language, typicallyas smoothed trigram models (Chiang, 2005).
Al-though it is well-known that higher-order languagemodels and models trained on additional monolin-gual corpora can significantly improve translationperformance, deploying such language models is nottrivial.
Increasing the order of an n-gram model canresult in an exponential increase in the number ofparameters; for the English Gigaword corpus, forinstance, there are 300 million distinct trigrams andover 1.2 billion distinct five-grams.
Since a languagemodel is potentially queried millions of times persentence, it should ideally reside locally in memoryto avoid time-consuming remote or disk-based look-ups.Against this background, we consider a radicallydifferent approach to language modelling.
Insteadof explicitly storing all distinct n-grams from ourcorpus, we create an implicit randomised represen-tation of these statistics.
This allows us to drasticallyreduce the space requirements of our models.
Inthis paper, we build on recent work (Talbot and Os-borne, 2007) that demonstrated how the Bloom filter(Bloom (1970); BF), a space-efficient randomiseddata structure for representing sets, could be used tostore corpus statistics efficiently.
Here, we proposea framework for deriving smoothed n-gram modelsfrom such structures and show via machine trans-lation experiments that these smoothed Bloom filterlanguage modelsmay be used as direct replacementsfor standard n-gram models in SMT.The space requirements of a Bloom filter are quitespectacular, falling significantly below information-theoretic error-free lower bounds.
This efficiency,however, comes at the price of false positives: the fil-ter may erroneously report that an item not in the setis a member.
False negatives, on the other hand, will468never occur: the error is said to be one-sided.
Ourframework makes use of the log-frequency Bloomfilter presented in (Talbot and Osborne, 2007), anddescribed briefly below, to compute smoothed con-ditional n-gram probabilities on the fly.
It takesadvantage of the one-sided error guarantees of theBloom filter and certain inequalities that hold be-tween related n-gram statistics drawn from the samecorpus to reduce both the error rate and the compu-tation required in deriving these probabilities.2 The Bloom filterIn this section, we give a brief overview of theBloom filter (BF); refer to Broder andMitzenmacher(2005) for a more in detailed presentation.
A BF rep-resents a set S = {x1, x2, ..., xn} with n elementsdrawn from a universe U of size N .
The structure isattractive when N  n. The only significant stor-age used by a BF consists of a bit array of size m.This is initially set to hold zeroes.
To train the filterwe hash each item in the set k times using distincthash functions h1, h2, ..., hk.
Each function is as-sumed to be independent from each other and to mapitems in the universe to the range 1 to m uniformlyat random.
The k bits indexed by the hash valuesfor each item are set to 1; the item is then discarded.Once a bit has been set to 1 it remains set for the life-time of the filter.
Distinct items may not be hashedto k distinct locations in the filter; we ignore col-lisons.
Bits in the filter can, therefore, be shared bydistinct items allowing significant space savings butintroducing a non-zero probability of false positivesat test time.
There is no way of directly retrieving orennumerating the items stored in a BF.At test time we wish to discover whether a givenitem was a member of the original set.
The filter isqueried by hashing the test item using the same khash functions.
If all bits referenced by the k hashvalues are 1 then we assume that the item was amember; if any of them are 0 then we know it wasnot.
True members are always correctly identified,but a false positive will occur if all k correspondingbits were set by other items during training and theitem was not a member of the training set.The probability of a false postive, f , is clearly theprobability that none of k randomly selected bits inthe filter are still 0 after training.
Letting p be theproportion of bits that are still zero after these n ele-ments have been inserted, this gives,f = (1?
p)k.As n items have been entered in the filter by hashingeach k times, the probability that a bit is still zero is,p?=(1?1m)kn?
e?knmwhich is the expected value of p. Hence the falsepositive rate can be approximated as,f = (1?
p)k ?
(1?
p?
)k ?(1?
e?knm)k.By taking the derivative we find that the number offunctions k?
that minimizes f is,k?
= ln 2 ?mn,which leads to the intuitive result that exactly halfthe bits in the filter will be set to 1 when the optimalnumber of hash functions is chosen.The fundmental difference between a Bloom fil-ter?s space requirements and that of any lossless rep-resentation of a set is that the former does not dependon the size of the (exponential) universe N fromwhich the set is drawn.
A lossless representationscheme (for example, a hash map, trie etc.)
must de-pend on N since it assigns a distinct representationto each possible set drawn from the universe.3 Language modelling with Bloom filtersRecent work (Talbot and Osborne, 2007) presented ascheme for associating static frequency informationwith a set of n-grams in a BF efficiently.13.1 Log-frequency Bloom filterThe efficiency of the scheme for storing n-gramstatistics within a BF presented in Talbot and Os-borne (2007) relies on the Zipf-like distribution ofn-gram frequencies: most events occur an extremelysmall number of times, while a small number arevery frequent.
We assume that raw counts are quan-tised and employ a logarithmic codebook that mapscounts, c(x), to quantised counts, qc(x), as follows,qc(x) = 1 + blogb c(x)c. (1)1Note that as described the Bloom filter is not an associativedata structure and provides only a Boolean function character-ising the set that has been stored in it.469Algorithm 1 Training frequency BFInput: Strain, {h1, ...hk} and BF = ?Output: BFfor all x ?
Strain doc(x)?
frequency of n-gram x in Strainqc(x)?
quantisation of c(x) (Eq.
1)for j = 1 to qc(x) dofor i = 1 to k dohi(x)?
hash of event {x, j} under hiBF [hi(x)]?
1end forend forend forreturn BFThe precision of this codebook decays exponentiallywith the raw counts and the scale is determined bythe base of the logarithm b; we examine the effectof this parameter on our language models in experi-ments below.Given the quantised count qc(x) for an n-gramx, the filter is trained by entering composite eventsconsisting of the n-gram appended by an integercounter j that is incremented from 1 to qc(x) intothe filter.
To retrieve an n-gram?s frequency, the n-gram is first appended with a counter set to 1 andhashed under the k functions; if this tests positive,the counter is incremented and the process repeated.The procedure terminates as soon as any of the khash functions hits a 0 and the previous value of thecounter is reported.
The one-sided error of the BFand the training scheme ensure that the actual quan-tised count cannot be larger than this value.
As thecounts are quantised logarithmically, the counter isusually incremented only a small number of times.We can then approximate the original frequencyof the n-gram by taking its expected value given thequantised count retrieved,E[c(x)|qc(x) = j] =bj?1 + bj ?
12.
(2)These training and testing routines are repeated hereas Algorithms 1 and 2 respectively.As noted in Talbot and Osborne (2007), errors forthis log-frequency BF scheme are one-sided: fre-quencies will never be underestimated.
The prob-ability of overestimating an item?s frequency decaysAlgorithm 2 Test frequency BFInput: x, MAXQCOUNT , {h1, ...hk} and BFOutput: Upper bound on c(x) ?
Strainfor j = 1 to MAXQCOUNT dofor i = 1 to k dohi(x)?
hash of event {x, j} under hiif BF [hi(x)] = 0 thenreturn E[c(x)|qc(x) = j ?
1] (Eq.
2)end ifend forend forexponentially with the size of the overestimation er-ror d (i.e.
as fd for d > 0) since each erroneousincrement corresponds to a single false positive andd such independent events must occur together.The efficiency of the log-frequency BF schemecan be understood from an entropy encoding per-spective under the distribution over frequencies ofn-gram types: the most common frequency (the sin-gleton count) is assigned the shortest code (length k)while rarer frequencies (those for more common n-grams) are assigned increasingly longer codes (k ?qc(x)).3.2 Smoothed BF language modelsA standard n-gram language model assigns condi-tional probabilities to target words given a certaincontext.
In practice, most standard n-gram languagemodels employ some form of interpolation wherebyprobabilities conditioned on the most specific con-text consisting usually of the n ?
1 preceding to-kens are combined with more robust estimates basedon less specific conditioning events.
To computesmoothed language model probabilities, we gener-ally require access to the frequencies of n-grams oflength 1 to n in our training corpus.
Depending onthe smoothing scheme, we may also need auxiliarystatistics regarding the number of distinct suffixesfor each n-gram (e.g., Witten-Bell and Kneser-Neysmoothing) and the number of distinct prefixes orcontexts in which they appear (e.g., Kneser-Ney).We can use a single BF to store these statistics butneed to distinguish each type of event (e.g., rawcounts, suffix counts, etc.).
Here we use a distinctset of k hash functions for each such category.Our motivation for storing the corpus statistics470directly rather than precomputed probabilities istwofold: (i) the efficiency of the scheme describedabove for storing frequency information togetherwith items in a BF relies on the frequencies hav-ing a Zipf-like distribution; while this is definitelytrue for corpus statistics, it may well not hold forprobabilities estimated from them; (ii) as will be-come apparent below, by using the corpus statisticsdirectly, we will be able to make additional savingsin terms of both space and error rate by using simpleinequalities that hold for related information drawnconsistently from the same corpus; it is not clearwhether such bounds can be established for proba-bilities computed from these statistics.3.2.1 Proxy itemsThere is a potential risk of redundancy if we rep-resent related statistics using the log-frequency BFscheme presented in Talbot and Osborne (2007).
Inparticular, we do not need to store information ex-plicitly that is necessarily implied by the presenceof another item in the training set, if that item canbe identified efficiently at query time when needed.We use the term proxy item to refer to items whosepresence in the filter implies the existence of anotheritem and that can be efficiently queried given the im-plied item.
In using a BF to store corpus statisticsfor language modelling, for example, we can use theevent corresponding to an n-gram and the counterset to 1 as a proxy item for a distinct prefix, suffix orcontext count of 1 for the same n-gram since (ignor-ing sentence boundaries) it must have been precededand followed by at least one distinct type, i.e.,qc(w1, ..., wn) ?
1 ?
BF ?
s(w1, ..., wn) ?
1,where s(?)
is the number of the distinct types follow-ing this n-gram in the training corpus.
We show be-low that such lower bounds allow us to significantlyreduce the memory requirements for a BF languagemodel.3.2.2 Monotonicity of n-gram event spaceThe error analysis in Section 2 focused on thefalse positive rate of a BF; if we deploy a BF withinan SMT decoder, however, the actual error rate willalso depend on the a priori membership probabilityof items presented to it.
The error rate Err is,Err = Pr(x /?
Strain|Decoder)f.This implies that, unlike a conventional lossless datastructure, the model?s accuracy depends on othercomponents in system and how it is queried.Assuming that statistics are entered consistentlyfrom the same corpus, we can take advantage of themonotonicity of the n-gram event space to place up-per bounds on the frequencies of events to be re-trieved from the filter prior to querying it, therebyreducing the a priori probability of a negative andconsequently the error rate.Specifically, since the log-frequency BF schemewill never underestimate an item?s frequency, wecan apply the following inequality recursively andbound the frequency of an n-gram by that of its leastfrequent subsequence,c(w1, ..., wn) ?
min {c(w1, ..., wn?1), c(w2, ..., wn)}.We use this to reduce the error rate of an interpolatedBF language model described below.3.3 Witten-Bell smoothed BF LMAs an example application of our framework, wenow describe a scheme for creating and queryinga log-frequency BF to estimate n-gram languagemodel probabilities using Witten-Bell smoothing(Bell et al, 1990).
Other smoothing schemes, no-tably Kneser-Ney, could be described within thisframework using additional proxy relations for infixand prefix counts.In Witten-Bell smoothing, an n-gram?s probabil-ity is discounted by a factor proportional to the num-ber of times that the n ?
1-gram preceding the cur-rent word was observed preceding a novel type inthe training corpus.
It is defined recursively as,Pwb(wi|wi?1i?n+1) = ?wi?1i?n+1Pml(wi|wi?1i?n+1)+(1?
?wi?1i?n+1)Pwb(wi|wi?1i?n+2)where ?x is defined via,1?
?x =c(x)s(x) + c(x),and Pml(?)
is the maximum likelihood estimator cal-culated from relative frequencies.The statistics required to compute the Witten-Bellestimator for the conditional probability of an n-gram consist of the counts of all n-grams of length4711 to n as well as the counts of the number of distincttypes following all n-grams of length 1 to n ?
1.In practice we use the c(w1, ..., wi) = 1 event as aproxy for s(w1, ..., wi) = 1 and thereby need notstore singleton suffix counts in the filter.Distinct suffix counts of 2 and above are storedby subtracting this proxy count and converting to thelog quantisation scheme described above, i.e.,qs(x) = 1 + blogb(s(x)?
1)cIn testing for a suffix count, we first query the itemc(w1, ..., wn) = 1 as a proxy for s(w1, ..., wn) =1 and, if found, query the filter for incrementallylarger suffix counts, taking the reconstructed suffixcount of an n-gram with a non-zero n-gram count tobe the expected value, i.e.,E[s(x)|qs(x) = j ?
j > 0] = 1 +(bj?1 + bj ?
1)2Having created a BF containing these events, thealgorithm we use to compute the interpolated WBestimate makes use of the inequalities describedabove to reduce the a priori probability of queryingfor a negative.
In particular, we bound the count ofeach numerator in the maximum likelihood term bythe count of the corresponding denominator and thecount of distinct suffixes of an n-gram by its respec-tive token frequency.Unlike more traditional LM formulations thatback-off from the highest-order to lower-order mod-els, our algorithm works up from the lowest-ordermodel.
Since the conditioning context increases inspecificity at each level, each statistic is bound fromabove by its corresponding value at the previous lessspecific level.
The bounds are applied by passingthem as the parameter MAXQCOUNT to the fre-quency test routine shown as Algorithm 2.
We ana-lyze the effect of applying such bounds on the per-formance of the model within an SMT decoder inthe experiments below.
Working upwards from thelower-order models also allows us to truncate thecomputation before the highest level if the denomi-nator in the maximum likelihood term is found witha zero count at any stage (no higher-order terms canbe non-zero given this).4 ExperimentsWe conducted a range of experiments to explorethe error-space trade-off of using a BF-based modelas a replacement for a conventional n-gram modelwithin an SMT system and to assess the benefits ofspecific features of our framework for deriving lan-guage model probabilities from a BF.4.1 Experimental set-upAll of our experiments use publically available re-sources.
Our main experiments use the French-English section of the Europarl (EP) corpus for par-allel data and language modelling (Koehn, 2003).Decoding is carried-out using the Moses decoder(Koehn and Hoang, 2007).
We hold out 1,000 testsentences and 500 development sentences from theparallel text for evaluation purposes.
The parame-ters for the feature functions used in this log-lineardecoder are optimised using minimum error rate(MER) training on our development set unless other-wise stated.
All evaluation is in terms of the BLEUscore on our test set (Papineni et al, 2002).Our baseline language models were created us-ing the SRILM toolkit (Stolcke, 2002).
We built 3,4 and 5-gram models from the Europarl corpus us-ing interpolated Witten-Bell smoothing (WB); no n-grams are dropped from these models or any of theBF-LMs.
The number of distinct n-gram types inthese baseline models as well as their sizes on diskand as compressed by gzip are given in Table 1; thegzip figures are given as an approximate (and opti-mistic) lower bound on lossless representations ofthese models.2The BF-LM models used in these experimentswere all created from the same corpora following thescheme outlined above for storing n-gram statistics.Proxy relations were used to reduce the number ofitems that must be stored in the BF; in addition, un-less specified otherwise, we take advantage of thebounds described above that hold between relatedstatistics to avoid presenting known negatives to thefilter.
The base of the logarithm used in quantizationis specified on all figures.The SRILM and BF-based models are bothqueried via the same interface in the Moses decoder.2Note, in particular, that gzip compressed files do not sup-port direct random access as required by in language modelling.472n Types Mem.
Gzip?d BLEU3 5.9M 174Mb 51Mb 28.544 14.1M 477Mb 129Mb 28.995 24.2M 924Mb 238Mb 29.07Table 1: WB-smoothed SRILM baseline models.We assign a small cache to the BF-LM models (be-tween 1 and 2MBs depending on the order of themodel) to store recently retrieved statistics and de-rived probabilities.
Translation takes between 2 to 5times longer using the BF-LMs as compared to thecorresponding SRILM models.4.2 Machine translation experimentsOur first set of experiments examines the relation-ship between memory allocated to the BF-LM andtranslation performance for a 3-gram and a 5-gramWB smoothed BF-LM.
In these experiments we usethe log-linear weights of the baseline model to avoidvariation in translation performance due to differ-ences in the solutions found by MER training: thisallows us to focus solely on the quality of each BF-LM?s approximation of the baseline.
These exper-iments consider various settings of the base for thelogarithm used during quantisation (b in Eq.
(1)).We also analyse these results in terms of the re-lationships between BLEU score and the underlyingerror rate of the BF-LM and the number of bits as-signed per n-gram in the baseline model.MER optimised BLEU scores on the test set arethen given for a range of BF-LMs.4.3 Mean squared error experimentsOur second set of experiments focuses on the accu-racy with which the BF-LM can reproduce the base-line model?s distribution.
Unfortunately, perplex-ity or related information-theoretic quantities are notapplicable in this case since the BF-LM is not guar-anteed to produce a properly normalised distribu-tion.
Instead we evaluate the mean squared error(MSE) between the log-probabilites assigned by thebaseline model and by BF-LMs to n-grams in theEnglish portion of our development set; we also con-sider the relation between MSE and the BLEU scorefrom the experiments above.2224262830320.020.01750.0150.01250.010.00750.0050.0025BLEU ScoreMemory in GBWB-smoothed BF-LM 3-grammodelBF-LMbase1.1BF-LMbase1.5BF-LMbase3SRILMWitten-Bell3-gram (174MB)Figure 1: WB-smoothed 3-gram model (Europarl).4.4 Analysis of BF-LM frameworkOur third set of experiments evaluates the impact ofthe use of upper bounds between related statistics ontranslation performance.
Here the standard modelthat makes use of these bounds to reduce the a pri-ori negative probability is compared to a model thatqueries the filter in a memoryless fashion.3We then present details of the memory savings ob-tained by the use of proxy relations for the modelsused here.5 Results5.1 Machine translation experimentsFigures 1 and 2 show the relationship between trans-lation performance as measured by BLEU and thememory assigned to the BF respectively for WB-smoothed 3-gram and 5-gram BF-LMs.
There is aclear degradation in translation performance as thememory assigned to the filter is reduced.
Modelsusing a higher quantisation base approach their opti-mal performance faster; this is because these morecoarse-grained quantisation schemes store feweritems in the filter and therefore have lower underly-ing false positive rates for a given amount of mem-ory.Figure 3 presents these results in terms of the re-lationship between translation performance and thefalse positive rate of the underlying BF.
We can seethat for a given false positive rate, the more coarse-grained quantisation schemes (e.g., base 3) perform3In both cases we apply ?sanity check?
bounds to ensure thatnone of the ratios in the WB formula (Eq.
3) are greater than 1.4732224262830320.070.060.050.040.030.020.01BLEU ScoreMemory in GBWB-smoothed BF-LM 5-grammodelBF-LMbase1.1BF-LMbase1.5BF-LMbase3SRILMWitten-Bell5-gram (924MB)Figure 2: WB-smoothed 5-gram model (Europarl).222324252627282930  0.010.11BLEU ScoreFalsepositive rate (probability)WB-smoothed BF-LM 3-grammodelBF-LMbase1.1BF-LMbase1.5BF-LMbase3Figure 3: False positive rate vs. BLEU .worse than the more fine-grained schemes.4Figure 4 presents the relationship in terms of thenumber of bits per n-gram in the baseline model.This suggests that between 10 and 15 bits is suf-ficient for the BF-LM to approximate the baselinemodel.
This is a reduction of a factor of between 16and 24 on the plain model and of between 4 and 7on gzip compressed model.The results of a selection of BF-LM models withdecoder weights optimised using MER training aregiven in Table 2; these show that the models performconsistently close to the baseline models that theyapproximate.5.2 Mean squared error experimentsFigure 5 shows the relationship between memory as-signed to the BF-LMs and the mean squared error4Note that in this case the base 3 scheme will use approxi-mately two-thirds the amount of memory required by the base1.5 scheme.20222426283032191715131197531BLEU ScoreBits per n-gramWB-smoothed BF-LM 3-grammodelBF-LMbase1.1BF-LMbase1.5BF-LMbase3Figure 4: Bits per n-gram vs. BLEU.n Memory Bits / n-gram base BLEU3 10MB 14 bits 1.5 28.333 10MB 14 bits 2.0 28.474 20MB 12 bits 1.5 28.634 20MB 12 bits 2.0 28.635 40MB 14 bits 1.5 28.535 40MB 14 bits 2.0 28.725 50MB 17 bits 1.5 29.315 50MB 17 bits 2.0 28.67Table 2: MERT optimised WB-smoothed BF-LMS.
(MSE) of log-probabilities that these models assignto the development set compared to those assignedby the baseline model.
This shows clearly that themore fine-grained quantisation scheme (e.g.
base1.1) can reach a lower MSE but also that the morecoarse-grained schemes (e.g., base 3) approach theirminimum error faster.Figure 6 shows the relationship between MSEbetween the BF-LM and the baseline model andBLEU.
The MSE appears to be a good predictor ofBLEU score across all quantisation schemes.
Thissuggests that it may be a useful tool for optimisingBF-LM parameters without the need to run the de-coder assuming a target (lossless) LM can be builtand queried for a small test set on disk.
An MSE ofbelow 0.05 appears necessary to achieve translationperformance matching the baseline model here.5.3 Analysis of BF-LM frameworkWe refer to (Talbot and Osborne, 2007) for empiri-cal results establishing the performance of the log-frequency BF-LM: overestimation errors occur with4740.010.025 0.05 0.1 0.25 0.50.030.020.010.0050.00250.001Mean squared error of log probabilitesMemory in GBMSEbetween WB3-gram SRILM and BF-LMsBase3Base1.5Base1.1Figure 5: MSE between SRILM and BF-LMs222324252627282930  0.010.11BLEU ScoreMeansquared errorWB-smoothed BF-LM 3-grammodelBF-LMbase1.1BF-LMbase1.5BF-LMbase3Figure 6: MSE vs. BLEU for WB 3-gram BF-LMsa probability that decays exponentially in the size ofthe overestimation error.Figure 7 shows the effect of applying upperbounds to reduce the a priori probability of pre-senting a negative event to the filter in our in-terpolation algorithm for computing WB-smoothedprobabilities.
The application of upper bounds im-proves translation performance particularly whenthe amount of memory assigned to the filter is lim-ited.
Since both filters have the same underlyingfalse positive rate (they are identical), we can con-clude that this improvement in performance is dueto a reduction in the number of negatives that arepresented to the filter and hence errors.Table 3 shows the amount of memory saved bythe use of proxy items to avoid storing singletonsuffix counts for the Witten-Bell smoothing scheme.The savings are given as ratios over the amount ofmemory needed to store the statistics without proxyitems.
These models have the same underlying false2224262830320.010.00750.0050.0025BLEU ScoreMemory in GBWB-smoothed BF-LM 3-grammodelBF-LMbase2 with boundsBF-LMbase2 without boundsFigure 7: Effect of upper bounds on BLEUn-gram order Proxy space saving3 0.8854 0.7835 0.708Table 3: Space savings via proxy items .positive rate (0.05) and quantisation base (2).
Sim-ilar savings may be anticipated when applying thisframework to infix and prefix counts for Kneser-Neysmoothing.6 Related WorkPrevious work aimed at reducing the size of n-gramlanguage models has focused primarily on quanti-sation schemes (Whitaker and Raj, 2001) and prun-ing (Stolcke, 1998).
The impact of the former seemslimited given that storage for the n-gram types them-selves will generally be far greater than that neededfor the actual probabilities of the model.
Pruningon the other hand could be used in conjunction withthe framework proposed here.
This holds also forcompression schemes based on clustering such as(Goodman and Gao, 2000).
Our approach, however,avoids the significant computational costs involvedin the creation of such models.Other schemes for dealing with large languagemodels include per-sentence filtering of the modelor its distribution over a cluster.
The former requirestime-consuming adaptation of the model for eachsentence in the test set while the latter incurs sig-nificant overheads for remote calls during decoding.Our framework could, however, be used to comple-ment either of these approaches.4757 Conclusions and Future WorkWe have proposed a framework for computingsmoothed language model probabilities efficientlyfrom a randomised representation of corpus statis-tics provided by a Bloom filter.
We have demon-strated that models derived within this frameworkcan be used as direct replacements for equivalentconventional language models with significant re-ductions in memory requirements.
Our empiricalanalysis has also demonstrated that by taking advan-tage of the one-sided error guarantees of the BF andsimple inequalities that hold between related n-gramstatistics we are able to further reduce the BF stor-age requirements and the effective error rate of thederived probabilities.We are currently implementing Kneser-Neysmoothing within the proposed framework.
We hopethe present work will, together with Talbot and Os-borne (2007), establish the Bloom filter as a practi-cal alternative to conventional associative data struc-tures used in computational linguistics.
The frame-work presented here shows that with some consider-ation for its workings, the randomised nature of theBloom filter need not be a significant impediment tois use in applications.AcknowledgementsReferencesT.C.
Bell, J.G.
Cleary, and I.H.
Witten.
1990.
Text Compres-sion.
Prentice Hall, Englewood Cliffs, NJ.B.
Bloom.
1970.
Space/time tradeoffs in hash coding withallowable errors.
CACM, 13:422?426.A.
Broder and M. Mitzenmacher.
2005.
Network applicationsof Bloom filters: A survey.
Internet Mathematics, 1(4):485?509.David Chiang.
2005.
A hierarchical phrase-based model forstatistical machine translation.
In Proceedings of the 43rdAnnual Meeting of the Association for Computational Lin-guistics (ACL?05), pages 263?270, Ann Arbor, Michigan,June.
Association for Computational Linguistics.J.
Goodman and J. Gao.
2000.
Language model size reductionby pruning and clustering.
In ICSLP?00, Beijing, China.Philipp Koehn and Hieu Hoang.
2007.
Factored translationmodels.
In Proc.
of the 2007 Conference on Empirical Meth-ods in Natural Language Processing (EMNLP/Co-NLL).P.
Koehn.
2003.
Europarl: A multilingual corpus forevaluation of machine translation, draft.
Availableat:http://people.csail.mit.edu/ koehn/publications/europarl.ps.K.
Papineni, S. Roukos, T. Ward, and W.J.
Zhu.
2002.
BLEU:a method for automatic evaluation of machine translation.In ACL-2002: 40th Annual meeting of the Association forComputational Linguistics.Andreas Stolcke.
1998.
Entropy-based pruning of back-off lan-guage models.
In Proc.
DARPA Broadcast News Transcrip-tion and Understanding Workshop, pages 270?274.A.
Stolcke.
2002.
SRILM ?
an extensible language modelingtoolkit.
In Proc.
Intl.
Conf.
on Spoken Language Processing.D.
Talbot and M. Osborne.
2007.
Randomised language mod-elling for statistical machine translation.
In 45th AnnualMeeting of the Association of Computational Linguists (Toappear).E.
Whitaker and B. Raj.
2001.
Quantization-based languagemodel compression (tr-2001-41).
Technical report, Mit-subishi Electronic Research Laboratories.476
