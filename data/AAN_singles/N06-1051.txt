Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 399?406,New York, June 2006. c?2006 Association for Computational LinguisticsA Machine Learning based Approach to Evaluating Retrieval SystemsHuyen-Trang Vu and Patrick GallinariLaboratory of Computer Science (LIP6)University of Pierre and Marie Curie8 rue du capitaine Scott, 75015 Paris, France{vu,gallinar}@poleia.lip6.frAbstractTest collections are essential to evaluateInformation Retrieval (IR) systems.
Therelevance assessment set has been recog-nized as the key bottleneck in test col-lection building, especially on very largesized document collections.
This paperaddresses the problem of efficiently se-lecting documents to be included in theassessment set.
We will show how ma-chine learning techniques can fit this task.This leads to smaller pools than tradi-tional round robin pooling, thus reducessignificantly the manual assessment work-load.
Experimental results on TREC col-lections1 consistently demonstrate the ef-fectiveness of our approach according todifferent evaluation criteria.1 IntroductionThe effectiveness of retrieval systems is often justi-fied by benchmark test collections.
A standard testcollection consists of lots of documents, a set of in-formation needs, called topics and human judgmentabout the relevance status of each document for atopic.
Nowadays, it is relatively easy to gather hugeset of millions of documents and hundreds of topics.The key obstacle for forming large sized test col-lections lies therefore in the topic assessment pro-cedure.
Assessing the whole document sets is un-feasible, even for small sized collection of 800,000documents (Voorhees and Harman, 1999).
In orderto keep the assessment process practical, one often1http://trec.nist.govselects a certain number of documents for judgment.This is called (document) pooling and the outcomethe pool or the qrels (query relevance set).
The col-lected documents are then judged by humans, doc-uments outside the pool are assumed non relevant.A representative pool is therefore essential to thewhole evaluation process.This paper proposes a method to form the assess-ment set with the support of machine learning algo-rithms.
Based on relevance judgments of relativelyshallow pools, a ranking algorithm will attempt togive priority for relevant documents so that the as-sessment set can be fixed at a feasible size withoutskewing the system evaluation result.
The judgmentprocess is indeed kept as much subjective-free aspossible: the first relevance feeback step is designedappropriately so that the assessor cannot give anybias towards any particular rank or any system, thelearning process is completely transparent to the as-sessors and parameters of the ranking function arecollection-tailored rather than exported from previ-ous collections.The method will then be evaluated on TRECad-hoc collections.
Results from our comprehensiveexperiment confirm that the qrels generated by ourmethod are much more representative than those ofthe same size by the TREC method.
The outcomeqrels is substantially smaller, so much cheaper toproduce than the official TREC qrels, yet their con-clusions about system effectiveness are quite com-patible.The remaining of this paper is organized as fol-lows.
We review related work in Section 2.
Sec-tion 3 presents the general framework of apply-ing machine learning techniques to forming testcollections.
We also give a brief introduction399about RankBoost (Freund et al, 2003) and Rank-ing SVM (Joachims, 2002b), the two learning algo-rithms used in our experiment.
Section 4 introducesdata sets and experimental setup.
Section 5 is ded-icated to present experimental results according todifferent evaluation criteria.
Precisely, Section 5.1shows the capacity of small pools on identifying rel-evant documents and Section 5.2 illustrates their im-pact on system comparison; Section 5.3 presents sta-tistical validation tests.
We conclude and discussperspectives in Section 6.2 Related work2.1 TREC methodologySince the seminal work of test collection formingin 1975 (Sparck Jones and Van Rijsbergen, 1975),pooling has been outlined as the main approachto form the assessment set.
The simple solutionof round robin pooling from different systems pro-posed in that report has been adopted in most exist-ing IR evaluation forums such as TREC, CLEF2 orNTCIR3.
For convenience, we will denote that strat-egy as TREC-style pooling.
To have the assessmentset, from submissions (restricted length L = 1000for most TREC tracks), only n top documents persubmission are pooled.
Despite different technicaltricks to control the final pool size such as gatheringonly principal runs or reducing the value of n, theassessment procedure is still quite time-consuming.In TREC 8 ad-hoc track, for example, despite lim-iting the pool depth n at 100 and gathering only 71of 129 submissions, each assessor has to work withapproximately 1737 documents per topic (precisely,between 1046 and 2992 documents).
Assuming thatit takes on average 30 seconds to read and judge adocument, the whole judgment procedure for thistopic set can therefore only terminate after a round-the-clock month.
Meanwhile, a simple analysis onthe ad-hoc collections from TREC-3 to TREC-8 re-vealed that there are on average 94% documentsjudged as non relevant.
Since most of existing ef-fectiveness measures do not take into account thesenon relevant documents, it would be bettter to notwaste effort on judging non relevant documents pro-vided that the quality of test collections is always2http://www.clef-campaign.org/3http://research.nii.ac.jp/ntcir/conserved.
Several advanced pool sampling meth-ods have been proposed but due to some commondrawbacks, none of them has been used in practice.2.2 Topic adaptive poolingZobel (Zobel, 1998) forms the shallow pools accord-ing to the TREC methodology.
When there areenough judged documents (up to the set of 30 topdocuments per run in his experiment), an extrapo-lation function will then be estimated to predict thenumber of unpooled relevant documents.
The ideais to judge more documents for topics that have highpotential to have relevant documents else.
Carteretteand Allan (Carterette and Allan, 2005) have recentlyreplaced that extrapolation function by statisticaltests to distinguish runs.
This method produced in-teresting empirical outcomes on TREC ad-hoc col-lections, lack however a sound theoretical basis andis clearly of very high complexity due to iterativestatistical tests of every run pairs.
Furthermore, thisincremental/online pooling approach raises a majorconcern about the unbiasness requirement from thehuman judgment as the assessors know well thatdocuments come later are of lower ranks, thus oflower relevance possibility.2.3 System adaptive poolingCormack et al (Cormack et al, 1998) propose theso-called Move-To-Front (MTF) heuristic to givepriority for documents based on the correspond-ing system performance.
In their experiment,the latter factor has been simply the number ofnon relevant documents this system has intro-duced to the pool since the last relevant document.Aslam et al (Aslam et al, 2003) formulate this pri-ority rule by adopting an online learning algorithmcalled Hedged (Freund and Schapire, 1997).Our method relies on this idea of pushing aheadrelevant documents by weighting retrieval systems.There are however two major differences.
Whilstall aforementioned proposals favor online paradigmwith a series of human interaction rounds, ourmethod works in batch mode.
We believe that thelatter is more suitable for this task since it elimi-nates as much as possible the bias introduced byhuman assessor towards any document.
Moreover,the batch mode enables us to exploit intuitively theinter-topic relationship what is not the case of on-400line paradigm.
The second difference lies in the wayof estimating the ranking function.
It is widely ac-cepted that machine learning techniques can delivermore reliable model on previously unseen data givenmuch less training instances than any classical statis-tical techniques or expert rules can.2.4 Generate pseudo assessment setSeveral evaluation methodologies, especially forweb search engines, have been proposed to evaluatesystems without relevance judgment.
These propos-als can be grouped into two main categories.
Thefirst (Soboroff et al, 2001; Wu and Crestani, 2003;Nuray and Can, 2006) exploits internal informationof submissions.
The second (Can et al, 2004;Joachims, 2002a; Beitzel et al, 2003) benefits ex-ternal resources such as document and query con-tent, or those of web environment.
We skip the sec-ond category since these resources are not availablein generic situations.Soboroff et al (Soboroff et al, 2001) sam-ple documents of a shallow pool (top tendocuments returned by retrieval systems)based on statistics from past qrels.
Wu andCrestani (Wu and Crestani, 2003), Nuray and Can(Nuray and Can, 2006) adopt metasearch strategieson document position.
A certain number of top out-come documents will then be considered as relevantwithout any human verification.
Different votingschemes have been tried in the two aformentionedpapers.
Their empirical experiment illustrated howthe quality of these pseudo-qrels is sensible to thechosen voting scheme and to other parameters suchas the pool depth or the diversity of systems usedfor fusion.
They also confirm that pseudo-qrels areoften unable to identify best systems.In sum, the thorough literature review confirmedthe importance of relevance assessment sets in IRevaluation yet the lack of an appropriate solution tohave a reliable set given a moderate amount of judg-ment resource.3 Machine learning based Pooling3.1 General frameworkLet M denote the topic set size available for thetraining purpose, N the number of participating sys-tems, k1 the pool depth to get the training data fromany participating system and K the final pool size.The training process consists of two main steps.Firstly, for each training topic, k1 first documentsof all N systems are gathered and the assessors areasked to assess all of these documents.
Let T denotethe outcome of this assessing step on all M topics.From the information of T , a function f will then belearned which assigns to each document a value cor-responding to its relevance degree for a given query.At the usage time, for each given topic, the wholeretrieved list of N systems will be fused.
These doc-uments will then be sorted in the decreasing order oftheir values according to f and the K top documentswill be sent to the assessor for judgment.
This lastset of judgements will be the qrels used for the sys-tem evaluation.In the training framework, it is clear that the sec-ond step plays the major role.
An effective scoringfunction can substantially save the workload at thelast assessment step.
We will now focus on methodsfor estimating such scoring function.3.2 Document ranking principleThe scoring function f can be estimated in differ-ent ways as seen in the last section.
In this study,we adopt the learning-to-rank paradigm for estimat-ing this scoring function.
The principle of documentranking will be sketched in this section.
The nextsub-section will introduce the two specific rankingalgorithms used in our experiment.A ranking algorithm aims at estimating a functionwhich describes correctly all partial orders inside aset of elements.
An ideal ranking in information re-trieval must be able to place all relevant documentsabove non relevant ones for a given topic.
The prob-lem can be described as follows.
For each topic, thedocument collection is decomposed into two disjointsets S+ and S?
for relevant (non relevant respec-tively) documents, R and NR are their cardinality.
Aranking function H(d) assigns to each document dof the document collection a score value.
We seekfor a function H(d) so that the document rankinggenerated from the scores respect the relevance rela-tionship, that is any relevant document has a higherscore than any non relevant one.
Let ?d ?
d??
sig-nify that d is ranked higher than d?.
The learning401objective can therefore be stated as follows.d+ ?
d?
?
H(d+) > H(d?),?
(d+, d?)
?
S+ ?
S?There are different ways to measure the rankingerror of a scoring function h. The natural criterionmight be the proportion of misordered pairs (a rele-vant document is below a non relevant one) over thetotal pair number R.NR.
This criterion is an estimateof the probability of misordering a pair P(d?
?
d+).RLoss(H) =?d+?S+d?
?S?D(d+, d?)d?
?
d+(1)=??
(d+ ,d?
)D(d+, d?)H(d?)
> H(d+)(2)where ~? is 1 if ?
holds, 0 otherwise; D(d+, d?)
de-scribes the importance of the pair in consideration,it will be uniform(1R.NR)if the information is un-known.In practice, we have to average RLoss over thetraining topic set.
This can be done by either macro-averaging at topic level or micro-averaging at docu-ment pair level.
For presentation simplification, thisoperation has been implicit.3.3 Discriminative ranking algorithmsSince RLoss is neither continuous nor differentiable,its direct use as a training criterion raises practicaldifficulties.
Also, in order to provide reliable predic-tions on previously unseen data, the prediction errorof the learning function has to be bounded with asignificant confidence.
For both practical and theo-retical reasons, RLoss is then often approximated bya smooth error function.In this study, we will explore the per-formance of two ranking algorithms, theyare RankBoost (Freund et al, 2003) andRanking SVM (Joachims, 2002b).
As far aswe know, these algorithms are actually among afew state-of-the-art ranking learning algorithmswhose convergence and generalization propertieshave been theoretically proved (Freund et al, 2003;Joachims, 2002b; Cle?menc?on et al, 2005).3.3.1 RankBoostRankBoost (aka RBoost) (Freund et al, 2003) re-turns a scoring function for each document d by min-imizing the following exponential upper bound ofthe ranking error RLoss (Eq.
(2)):ELoss(H) =?(d+,d?
)D(d+, d?)eH(d?
)?H(d+) (3)This is an iterative algorithm like all other boostingmethods (Freund and Schapire, 1997).
The globalranking function of a document d is a linear combi-nation of all base functions H(d) = ?Tt=1 ?tht(d).
Ateach iteration t, a new training data sample is gener-ated by putting more weight D(., .)
on difficult pairs(d+, d?).
A scoring function ht is proposed (it caneven be chosen among the features used to describedocuments) and the weight ?t is estimated in orderto minimize the ELoss at that iteration.RBoost has virtues particularly fitting the pool-ing task.
First, it can operate on relative values.Second, it does not impose any independence as-sumption between combined systems.
Finally, inthe case of binary relevance judgment which usu-ally occurs in IR, there is an efficient implementa-tion of RBoost whose complexity is linear in termsof the training instance number (cf: the originaltext (Freund et al, 2003)).3.3.2 Ranking SVMRanking SVM (Joachims, 2002b), rSVM forshort, is a straightforward adaptation of the max-margin principle (Vapnik, 2000) to pairwise objectranking.
The score function is often assumed to belinear in some feature space, that is H(d) = wT?
(d)where w is the vector of weights to be estimated and?
is a feature mapping.
The max-margin approachminimizes the following approximation of RLoss:rSVMLoss(H) = max{1 +(H(d?)
?
H(d+)), 0}(4)for all pairs (d+, d?)
while at the same time control-ling the complexity of function space described viathe norm of vector w for generalization objective.Notice that rSVM does not explicitly support rankvalues as does RBoost.
Nevertheless, we will seelater that the discriminative nature allows rSVM towork quite well on features merely deduced fromrank values.
Its behavior difference is in fact ignor-able in comparison with RBoost.4 Experimental setupOur method is general enough to be applicable toany ad-hoc retrieval information task where pooling402could be useful.
In this paper, we will however fo-cus on TREC traditional ad-hoc retrieval collections.Experiments have been performed on the three cor-pora TREC-6, TREC-7 and TREC-8.
Statisticsabout the number of runs, of judgments, of rele-vant documents are shown in Tab.
1.
Due to limitof space, we will detail results on the TREC-8 caseand only mention the results on the two others.#runs #judgments #rel.
docs Depth-5TREC 6 79 1445.4 92.2 144.3TREC 7 103 1606.9 93.5 114.6TREC 8 129 1736.6 94.6 143.4Table 1: Information about three TREC ad-hoc col-lections.
The three last columns are averaged overthe topic set size (50 topics/collection).Training data is gathered from the top five an-swers of each run.
The pool depth of five has beenarbitrarily chosen to have both sufficient trainingdata and to eliminate potential bias from assessorstowards a particular system or towards early iden-tified answers while judging a shallow pool.
Fur-thermore, this training data set is large enough fortesting the ranking algorithm efficiency.Each document is described by an N-dimensionalfeature vector where N is the number of participat-ing systems.
The jth feature value for a documentis a function of its position in the retrieved list, tiesare arbitrary broken.
A document at rank i is as-signed a feature value of (L + 1 ?
i) where L is theTREC limit of submission run (L is usually set up at1000).
Documents outside submission runs receivethe zero feature value (i.e.
it is assumed to be at rank(L + 1)).
For implementation speed, the input forrSVM is further scaled down to the interval [0, 1].Due to the small topic set size, we use a leave-one-out training strategy: a model will be trainedfor each topic by using judgments of all other top-ics.
The training data set size is presented in the lastcolumn of Tab.
1.
The workload for training datasetdoes not exceed the effort for assessing 5 topics inthe full pool of TREC.We employ SVMlight package4 for rSVM.We adopt the efficient RBoost version for bi-nary feedback and binary base functions ht(cf.
(Freund et al, 2003)), boosting is iterated 1004http://svmlight.joachims.orgtimes and we impose positive weighting for all coef-ficients ?t.The non-interpolated average precision (MAP)has been chosen to measure system perfor-mance5.
This metric has been shown to behighly stable and reliable with both smalltopic set size (Buckley and Voorhees, 2000)and very large document collec-tions (Hawking and Robertson, 2003).RBoost and rSVM pools will be compared to theTREC-style pools of the same size.
We also include?local MTF?
(Cormack et al, 1998) in the experi-ment.
The ?global MTF?
has been shown to slightlyoutperform the local version in the aforementionedpaper.
However, we believe that the global modeis merely for demonstration but unlikely practicalof online judgment since it insists that all queriesare judged simultaneously with a strict synchroni-sation among all assessors.
Hereafter, for simplic-ity, the TREC-style pool of the first n documentsretrieved by each submission will be denoted byDepth-n, the equivalent pool (with the same aver-age final pool size m over the topic set) produced byRBoost, rSVM or MTF will be RBoost-m, rSVM-mor MTF-m respectively.
In all figures in the next sec-tion, the abscissa denotes the pool size m and valuesof n will be present along the Depth-n curve.5 Experimental resultsThis section will examine small pools produced ei-ther by the TREC method or by RBoost/rSVM/MTFfrom two angles: their pooling performance andtheir influence on system comparison result.5.1 Identify relevant documentsFig.
1 shows the ratio of relevant documents re-trieved by different pooling methods (i.e.
the re-call).
The curves obtained by RBoost and rSVM arequite similar and much higher than that by TRECmethodology.
The curve of MTF is in the middleof RBoost/rSVM and Depth-n at the beginning andthen catches that of RBoost at the pools of about 600documents.5http://trec.nist.gov/trec eval/40300.10.20.30.40.50.60.70.80.910  200  400  600  800  1000%rel.
docs found#docs judgedrecall: TREC8 ad-hoc, 129 runs1101520 253040RBoostrSVMMTFDepth-nFigure 1: Along the incrementally enlarged pools:relevant documents identified in comparison withthe full assessment set.5.2 Correlation of system rankingsOnce the pool is obtained by a given method, theassessor will give relevance judgment for all docu-ments of that pool, called qrels for the outcome.
Thisqrels will be used as the ground truth to measure ef-fectiveness of a retrieval system.0.70.750.80.850.90.9510  200  400  600  800  1000Kendall?s?qrels sizesys.
ranking, MAP: TREC8 ad-hoc, 129 runs1271015 2025RBoostrSVMMTFDepth-nFigure 2: Kendall?s ?
correlation of system rankingaccording to different qrels methods in comparisonwith that produced by the full assessment set.The simplest way to compare different sys-tems is to sort them by the decreasing effective-ness values.
The correlation of each two sys-tem rankings will then be quantified through acorrelation statistic.
In this study, we followTREC convention (Buckley and Voorhees, 2004;Carterette and Allan, 2005), that is taking the 0.9value of Kendall?s ?
as the sufficient threshold toconclude that the difference of two system rankingsis ignorable.
We compare here the system rankingobtained by the official TREC qrels with those byDepth-n where n varies from 1 to 100.
We then re-place Depth-n by RBoost-m, rSVM-m and MTF-m.The results are shown in Fig.
2 for TREC-8 and inTab.
2 for the 7 first pool depths.
We observe fromthe figure the similar order of pooling methods asseen in the previous section.
The MTF curve meetsthose of RBoost and rSVM from qrels of more than400 documents.
The results obtained on the two col-lections of TREC-6 and TREC-7 are in line withthose observed on TREC-8 (Tab.
2).It is clear that system ranking correlation quan-tified by any rank correlation statistics providesnecessary but not sufficient information about sys-tem comparison.
Ranking systems by their sam-ple means is indeed the simplest way with at leasttwo implicit assumptions.
First, runs have simi-lar variances, this usually does not hold in practiceeven after discarding poorest runs.
Second, all runswaps have the same importance without taking intoaccount their statistically significant difference andtheir positions in the system ranking.
In practice,swap of adjacent systems does not make much senseif they are not significantly different to each otheraccording to statstical tests.
The next section will bedevoted to further statistical validations.5.3 Statistical Validations5.3.1 Significant difference detectionWe register for a given qrels all system pairswhich are significantly different on the topic set.
Thequality of a qrels can be measured by the similarityof this significant difference detection in compari-son with that obtained by the official TREC qrels.We carry out the paired t-test for each pair of runswith 95% significance level.
The recall and the falsealarm rate of these detections are shown in Fig.
3.
Interms of recall, RBoost and rSVM qrels are muchmore better than its TREC-style counterparts andMTF is in the middle.
In terms of false alarm rate,there are some changes concerning rSVM and MTF.Precisely, rSVM at small qrels of less than 100 doc-uments is the best whilst that is MTF qrels of morethan 150 documents.5.3.2 Tukey groupingThis multicomparison test6 aims to group runsbased on their statistical difference.
We concentrate6IR-STAT-PAK (Tague-Sutcliffe and Blustein, 1995)404TREC-6 (79 sys.)
TREC-7 (103 sys.)
TREC-8 (129 sys.
)n m D-n MTF SVM RBst m D-n MTF SVM RBst m D-n MTF SVM RBst1 37 .835 .843 .888 .914 32 .788 .809 .888 .891 40 .733 .805 .927 .9092 66 .875 .899 .925 .934 56 .831 .890 .920 .922 68 .829 .877 .939 .9333 93 .892 .925 .939 .956 76 .851 .918 .931 .935 95 .864 .903 .948 .9464 118 .903 .940 .949 .967 95 .876 .926 .935 .947 119 .877 .921 .951 .9535 144 .907 .949 .958 .972 115 .884 .936 .942 .954 143 .896 .933 .959 .9556 170 .915 .953 .961 .974 133 .894 .942 .951 .957 168 .898 .940 .963 .9637 195 .925 .959 .967 .977 152 .903 .950 .956 .962 191 .901 .946 .967 .966Table 2: Kendall?s ?
obtained on small qrels.
D-n: TREC-style Depth-n qrels, SVM: rSVM-m; RBst:RBoost-m.657075808590951000  200  400  600  800  1000%#docs judgedrecall: TREC8 ad-hoc, 129 runs1210 1520 25RBoostrSVMMTFDepth-n051015202530350  200  400  600  800  1000%#docs judgedfallout: TREC8 ad-hoc, 129 runs110 15 20 25RBoostrSVMMTFDepth-nFigure 3: Comparing qrels of RBoost-m, rSVM-m,MTF-m and Depth-n in terms of pairs of signifi-cantly different systems: recall (top) and false alarmrate (bottom)particularly on the top group, called group A whichconsists of runs on which there is not enough evi-dence to conclude that they are statistically signifi-cantly worse than the top run.
In practice, this fig-ure will be meaningful if it is around 10 (one oftensays about the top 10 runs).
It will however becomemeaningless if the group A is too large, for exam-ple contains more than half of systems in consider-ation.
Note that Tukey test relies on the assump-tion of Equality of Variances.
This requirement cannot be completely satisfied in practice, even after010203040506070800  200  400  600  800  1000card.ofgroup A#docs judgedsys.
grouping, MAP: TREC8 ad-hoc, 129 runs147 (57)10152025RBoostrSVMMTFDepth-nFigure 4: Cardinality of group A (95% confidencelevel) after the arcsine-root data transformation.some data transformation such as arcsine-root or us-ing rank values.The size of group A on TREC-8 collection isshown in Fig.
4.
We observe from that figure thestability of the two curves of RBoost and rSVM, thisimplies that the two qrels RBoost-35 and rSVM-35which have both satisfied the 0.9 requirement ofKendall?s ?
can replace the official TREC qrels.
Theeffort saving is therefore a factor of 50 (if ignoringthe cost of training data set preparation) and of 10.5otherwise.
MTF needs qrels of at least 168 docu-ments to produce comparable group A?s with that ofthe official TREC qrels.
The Depth-n pools how-ever should not be recommended with less than 1000documents in total (i.e.
pooling more than 40 topdocuments per run).6 Conclusions and DiscussionThis study has well illustrated that two algorithms ofRBoost and rSVM are quite suitable for qrels con-struction task.
The final qrels are not only smallenough to ask for human judgment but also resultin reliable conclusion about system effectiveness in405comparison with the counterpart of TREC method-ology and that of MTF.It is necessary to include other metasearch meth-ods for further study.
This will allow us to validatenot only the impact of the metasearch training prin-ciple based on pairwise ranking error RLoss but alsothe capacity of automatic feature selection of the tworanking algorithms used in this paper.This method needs to be further verified on chal-lenging ad-hoc retrieval scenarios such as Terabyte,Web Topic Distillation or Robust Tracks in TRECcontext.
The hardness of these scenarios involvestwo main issues.
First, the number of documentjudged relevant varies largely across the whole topicset.
Second, some topics might even have no rele-vant document in shallow pools.
These matter anystatistical inference on shallow pools.Acknowledgement The authors thank M.-R. Amini,B.
Piwowarski, J. Zobel and the anonymous re-viewers for their thorough comments.
We ac-knowledge NIST to make accessible the TRECsubmissions.
This work was supported in partby the IST Programme of the European Commu-nity, under the PASCAL Network of Excellence,IST-2002-506778.
The publication only reflects theauthors?
views.References[Aslam et al2003] J.A.
Aslam, V. Pavlu, and R. Savell.2003.
A unified model for metasearch, pooling, andsystem evaluation.
In Proc.
CIKM?03.
[Beitzel et al2003] S. M. Beitzel, E. C. Jensen,A.
Chowdhury, and D. Grossman.
2003.
Usingtitles and category names from editor-driven tax-onomies for automatic evaluation.
In Proc.
CIKM?03.
[Buckley and Voorhees2000] C. Buckley and E.M.Voorhees.
2000.
Evaluating evaluation measurestability.
In Proc.
SIGIR?00.
[Buckley and Voorhees2004] C. Buckley and E.M.Voorhees.
2004.
Retrieval evaluation with incompleteinformation.
In Proc.
SIGIR?04.
[Can et al2004] F. Can, R. Nuray, and A.
B. Sevdik.2004.
Automatic performance evaluation of websearch engines.
Info.
Process.
Management,40(3):495?514, May.
[Carterette and Allan2005] B. Carterette and J. Allan.2005.
Incremental Test Collections.
In CIKM?05.
[Cle?menc?on et al2005] S. Cle?menc?on, G. Lugosi, andN.
Vayatis.
2005.
Ranking and scoring using empiri-cal risk minimization.
In Proc.
COLT?05.
[Cormack et al1998] G.V.
Cormack, Christopher R.Palmer, and C.L.A.
Clarke.
1998.
Efficient construc-tion of large test collections.
In Proc.
SIGIR?98.
[Freund and Schapire1997] Y. Freund and R.E.
Schapire.1997.
A decision-theoretic generalization of on-linelearning and an application to boosting.
J. Compt.
Sys.Sci., 55(1):119?139, August.
[Freund et al2003] Y. Freund, R. Iyer, R.E.
Schapire, andY.
Singer.
2003.
An efficient boosting algorithmfor combining preferences.
J. Mach.
Learning Res.,4:933?969, November.
[Hawking and Robertson2003] D. Hawking andS.
Robertson.
2003.
On collection size and retrievaleffectiveness.
Information Retrieval, 6(1):99?105.
[Joachims2002a] Th.
Joachims.
2002a.
Evaluating re-trieval performance using clickthrough data.
In Proc.SIGIR wshop on Math./Formal Methods in IR.
[Joachims2002b] Th.
Joachims.
2002b.
Optimizingsearch engines using clickthrough data.
In KDD?02.
[Nuray and Can2006] R. Nuray and F. Can.
2006.
Au-tomatic ranking of information retrieval systems usingdata fusion.
Info.
Process.
Management, 42(3):595?614, May.
[Soboroff et al2001] I. Soboroff, Ch.
Nicholas, and P. Ca-han.
2001.
Ranking Retrieval Systems without Rele-vance Judgments.
In Proc.
SIGIR?01.
[Sparck Jones and Van Rijsbergen1975] K. Sparck Jonesand C. J.
Van Rijsbergen.
1975.
Report on the need forand provision of an ideal information retrieval test col-lection.
Technical Report 5266, Computer Lab., Univ.Cambridge.
[Tague-Sutcliffe and Blustein1995] J. Tague-Sutcliffe andJ.
Blustein.
1995.
A statistical analysis of the TREC-3data.
In Proc.
TREC-3.
[Vapnik2000] N. V. Vapnik.
2000.
The Nature of Statisti-cal Learning Theory.
Springer-Verlag.
[Voorhees and Harman1999] E.M. Voorhees and D. Har-man.
1999.
Overview of the Eighth Text REtrievalConference (TREC-8).
In Proc.
TREC 8.
[Wu and Crestani2003] Sh.
Wu and F. Crestani.
2003.Methods for Ranking Information Retrieval SystemsWithout Relevance Judgements.
In SAC?03.
[Zobel1998] J. Zobel.
1998.
How reliable are the resultsof large-scale information retrieval experiments?
InProc.
SIGIR?98.406
