Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 324?332,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPTopological Ordering of Function Wordsin Hierarchical Phrase-based TranslationHendra Setiawan1and Min-Yen Kan2and Haizhou Li3and Philip Resnik11University of Maryland Institute for Advanced Computer Studies2School of Computing, National University of Singapore3Human Language Technology, Institute for Infocomm Research, Singapore{hendra,resnik}@umiacs.umd.edu,kanmy@comp.nus.edu.sg, hli@i2r.a-star.edu.sgAbstractHierarchical phrase-based models are at-tractive because they provide a consis-tent framework within which to character-ize both local and long-distance reorder-ings, but they also make it difcult todistinguish many implausible reorderingsfrom those that are linguistically plausi-ble.
Rather than appealing to annotation-driven syntactic modeling, we address thisproblem by observing the inuential roleof function words in determining syntac-tic structure, and introducing soft con-straints on function word relationships aspart of a standard log-linear hierarchi-cal phrase-based model.
Experimentationon Chinese-English and Arabic-Englishtranslation demonstrates that the approachyields signicant gains in performance.1 IntroductionHierarchical phrase-based models (Chiang, 2005;Chiang, 2007) offer a number of attractive bene-ts in statistical machine translation (SMT), whilemaintaining the strengths of phrase-based systems(Koehn et al, 2003).
The most important of theseis the ability to model long-distance reordering ef-ciently.
To model such a reordering, a hierar-chical phrase-based system demands no additionalparameters, since long and short distance reorder-ings are modeled identically using synchronouscontext free grammar (SCFG) rules.
The samerule, depending on its topological ordering ?
i.e.its position in the hierarchical structure ?
can af-fect both short and long spans of text.
Interest-ingly, hierarchical phrase-based models providethis benet without making any linguistic commit-ments beyond the structure of the model.However, the system's lack of linguistic com-mitment is also responsible for one of its great-est drawbacks.
In the absence of linguistic knowl-edge, the system models linguistic structure usingan SCFG that contains only one type of nontermi-nal symbol1.
As a result, the system is susceptibleto the overgeneration problem: the grammar maysuggest more reordering choices than appropriate,and many of those choices lead to ungrammaticaltranslations.Chiang (2005) hypothesized that incorrect re-ordering choices would often correspond to hier-archical phrases that violate syntactic boundariesin the source language, and he explored the useof a ?constituent feature?
intended to reward theapplication of hierarchical phrases which respectsource language syntactic categories.
Althoughthis did not yield signicant improvements, Mar-ton and Resnik (2008) and Chiang et al (2008)extended this approach by introducing soft syn-tactic constraints similar to the constituent feature,but more ne-grained and sensitive to distinctionsamong syntactic categories; these led to substan-tial improvements in performance.
Zollman et al(2006) took a complementary approach, constrain-ing the application of hierarchical rules to respectsyntactic boundaries in the target language syn-tax.
Whether the focus is on constraints from thesource language or the target language, the mainingredient in both previous approaches is the ideaof constraining the spans of hierarchical phrases torespect syntactic boundaries.In this paper, we pursue a different approachto improving reordering choices in a hierarchicalphrase-based model.
Instead of biasing the modeltoward hierarchical phrases whose spans respectsyntactic boundaries, we focus on the topologi-cal ordering of phrases in the hierarchical struc-ture.
We conjecture that since incorrect reorder-ing choices correspond to incorrect topological or-derings, boosting the probability of correct topo-1In practice, one additional nonterminal symbol is used in?glue rules?.
This is not relevant in the present discussion.324logical ordering choices should improve the sys-tem.
Although related to previous proposals (cor-rect topological orderings lead to correct spansand vice versa), our proposal incorporates broadercontext and is structurally more aware, since welook at the topological ordering of a phrase relativeto other phrases, rather than modeling additionalproperties of a phrase in isolation.
In addition, ourproposal requires no monolingual parsing or lin-guistically informed syntactic modeling for eitherthe source or target language.The key to our approach is the observation thatwe can approximate the topological ordering ofhierarchical phrases via the topological orderingof function words.
We introduce a statistical re-ordering model that we call the pairwise domi-nance model, which characterizes reorderings ofphrases around a pair of function words.
In mod-eling function words, our model can be viewed asa successor to the function words-centric reorder-ing model (Setiawan et al, 2007), expanding onthe previous approach by modeling pairs of func-tion words rather than individual function wordsin isolation.The rest of the paper is organized as follows.
InSection 2, we briey review hierarchical phrase-based models.
In Section 3, we rst describe theovergeneration problem in more detail with a con-crete example, and then motivate our idea of us-ing the topological ordering of function words toaddress the problem.
In Section 4, we developour idea by introducing the pairwise dominancemodel, expressing function word relationships interms of what we call the the dominance predi-cate.
In Section 5, we describe an algorithm to es-timate the parameters of the dominance predicatefrom parallel text.
In Sections 6 and 7, we describeour experiments, and in Section 8, we analyze theoutput of our system and lay out a possible futuredirection.
Section 9 discusses the relation of ourapproach to prior work and Section 10 wraps upwith our conclusions.2 Hierarchical Phrase-based SystemFormally, a hierarchical phrase-based SMT sys-tem is based on a weighted synchronous contextfree grammar (SCFG) with one type of nonter-minal symbol.
Synchronous rules in hierarchicalphrase-based models take the following form:X ?
?
?, ?,??
(1)where X is the nonterminal symbol and ?
and ?are strings that contain the combination of lexicalitems and nonterminals in the source and targetlanguages, respectively.
The ?
symbol indicatesthat nonterminals in ?
and ?
are synchronizedthrough co-indexation; i.e., nonterminals with thesame index are aligned.
Nonterminal correspon-dences are strictly one-to-one, and in practice thenumber of nonterminals on the right hand side isconstrained to at most two, which must be sepa-rated by lexical items.Each rule is associated with a score that is com-puted via the following log linear formula:w(X ?
?
?, ?,??)
=?if?ii (2)where fi is a feature describing one particular as-pect of the rule and ?i is the corresponding weightof that feature.
Given e?
and f?
as the sourceand target phrases associated with the rule, typi-cal features used are rule's translation probabilityPtrans(f?
|e?)
and its inverse Ptrans(e?|f?
), the lexi-cal probability Plex(f?
|e?)
and its inverse Plex(e?|f?
).Systems generally also employ a word penalty, aphrase penalty, and target language model feature.
(See (Chiang, 2005) for more detailed discussion.
)Our pairwise dominance model will be expressedas an additional rule-level feature in the model.Translation of a source sentence e using hier-archical phrase-based models is formulated as asearch for the most probable derivationD?
whosesource side is equal to e:D?
= argmax P (D),where source(D)=e.D = Xi, i ?
1...|D| is a set of rules following acertain topological ordering, indicated here by theuse of the superscript.3 Overgeneration and TopologicalOrdering of Function WordsThe use of only one type of nonterminal allows aexible permutation of the topological ordering ofthe same set of rules, resulting in a huge number ofpossible derivations from a given source sentence.In that respect, the overgeneration problem is notnew to SMT: Bracketing Transduction Grammar(BTG) (Wu, 1997) uses a single type of nontermi-nal and is subject to overgeneration problems, aswell.22Note, however, that overgeneration in BTG can beviewed as a feature, not a bug, since the formalism was origi-325The problem may be less severe in hierarchi-cal phrase-based MT than in BTG, since lexicalitems on the rules' right hand sides often limit thespan of nonterminals.
Nonetheless overgenerationof reorderings is still problematic, as we illustrateusing the hypothetical Chinese-to-English exam-ple in Fig.
1.Suppose we want to translate the Chinese sen-tence in Fig.
1 into English using the following setof rules:1.
Xa ?
?
?Z X1, computers andX1?2.
Xb ?
?X14 X2, X1 are X2?3.
Xc ?
?C?
, cell phones ?4.
Xd ?
?X1{?
, inventions of X1?5.
Xe ?
??
?- , the last century ?Co-indexation of nonterminals on the right handside is indicated by subscripts, and for our ex-amples the label of the nonterminal on the lefthand side is used as the rule's unique identier.To correctly translate the sentence, a hierarchicalphrase-based system needs to model the subjectnoun phrase, object noun phrase and copula con-structions; these are captured by rulesXa,Xd andXb respectively, so this set of rules represents ahierarchical phrase-based system that can be usedto correctly translate the Chinese sentence.
Notethat the Chinese word order is correctly preservedin the subject (Xa) as well as copula constructions(Xb), and correctly inverted in the object construc-tion (Xd).However, although it can generate the correcttranslation in Fig.
2, the grammar has no mech-anism to prevent the generation of an incorrecttranslation like the one illustrated in Fig.
3.
Ifwe contrast the topological ordering of the rulesin Fig.
2 and Fig.
3, we observe that the differenceis small but quite signicant.
Using precede sym-bol (?)
to indicate the rst operand immediatelydominates the second operand in the hierarchicalstructure, the topological orderings in Fig.
2 andFig.
3 are Xa ?
Xb ?
Xc ?
Xd ?
Xe andXd ?
Xa ?
Xb ?
Xc ?
Xe, respectively.
Theonly difference is the topological ordering of Xd:in Fig.
2, it appears below most of the other hier-archical phrases, while in Fig.
3, it appears aboveall the other hierarchical phrases.nally introduced for bilingual analysis rather than generationof translations.Modeling the topological ordering of hierarchi-cal phrases is computationally prohibitive, sincethere are literally millions of hierarchical rules inthe system's automatically-learned grammar andmillions of possible ways to order their applica-tion.
To avoid this computational problem andstill model the topological ordering, we proposeto use the topological ordering of function wordsas a practical approximation.
This is motivated bythe fact that function words tend to carry crucialsyntactic information in sentences, serving as the?glue?
for content-bearing phrases.
Moreover, thepositional relationships between function wordsand content phrases tends to be xed (e.g., in En-glish, prepositions invariably precede their objectnoun phrase), at least for the languages we haveworked with thus far.In the Chinese sentence above, there are threefunction words involved: the conjunctionZ (and),the copula 4 (are), and the noun phrase marker{ (of).3 Using the function words as approximaterepresentations of the rules in which they appear,the topological ordering of hierarchical phrases inFig.
2 is Z(and) ?
4(are) ?
{(of), while thatin Fig.
3 is {(of) ?
Z(and) ?
4(are).4 Wecan distinguish the correct and incorrect reorder-ing choices by looking at this simple information.In the correct reordering choice,{(of) appears atthe lower level of the hierarchy while in the incor-rect one,{(of) appears at the highest level of thehierarchy.4 Pairwise Dominance ModelOur example suggests that we may be able to im-prove the translation model's sensitivity to correctversus incorrect reordering choices by modelingthe topological ordering of function words.
We doso by introducing a predicate capturing the domi-nance relationship in a derivation between pairs ofneighboring function words.5Let us dene a predicate d(Y ?, Y ??)
that takestwo function words as input and outputs one of3We use the term ?noun phrase marker?
here in a generalsense, meaning that in this example it helps tell us that thephrase is part of an NP, not as a technical linguistic term.
Itserves in other grammatical roles, as well.
Disambiguatingthe syntactic roles of function words might be a particularlyuseful thing to do in the model we are proposing; this is aquestion for future research.4Note that for expository purposes, we designed our sim-ple grammar to ensure that these function words appear inseparate rules.5Two function words are considered neighbors iff no otherfunction word appears between them in the source sentence.326?Z C?
4 ?{??-?XXXXXz?????9??
?
?arecomputers and cell phones inventions of the last centuryFigure 1: A running example of Chinese-to-English translation.Xa??
?Z Xb, computers andXb???
?Z Xc4 Xd, computers andXc are Xd???
?ZC?4 Xd, computers and cell phones areXd???
?ZC?4 Xe{?
, computers and cell phones are inventions ofXe????ZC?4??-{?
, computers and cell phones are inventions of the last century?Figure 2: The derivation that leads to the correct translationXd??Xa{?
, inventions of Xa???
?Z Xb{?
, inventions of computers andXb???
?Z Xc4 Xe{?
, inventions of computers andXc are Xe???
?ZC?4 Xe{?
, inventions of computers and cell phones areXe????ZC?4??-{?
, inventions of computers and cell phones are the last century?Figure 3: The derivation that leads to the incorrect translationfour values: {leftFirst, rightFirst, dontCare, nei-ther}, where Y ?
appears to the left of Y ??
in thesource sentence.
The value leftFirst indicates thatin the derivation's topological ordering, Y ?
pre-cedes Y ??
(i.e.
Y ?
dominates Y ??
in the hierarchi-cal structure), while rightFirst indicates that Y ?
?dominates Y ?.
In Fig.
2, d(Y ?, Y ??)
= leftFirstfor Y ?
= the copula 4 (are) and Y ??
= the nounphrase marker{ (of).The dontCare and neither values capture twoadditional relationships: dontCare indicates thatthe topological ordering of the function words isexible, and neither indicates that the topologi-cal ordering of the function words is disjoint.
Theformer is useful in cases where the hierarchicalphrases suggest the same kind of reordering, andtherefore restricting their topological ordering isnot necessary.
This is illustrated in Fig.
2 by thepairZ(and) and the copula 4(are), where puttingeither one above the other does not change the -nal word order.
The latter is useful in cases wherethe two function words do not share a same parent.Formally, this model requires several changes inthe design of the hierarchical phrase-based system.1.
To facilitate topological ordering of functionwords, the hierarchical phrases must be sub-categorized with function words.
Taking Xbin Fig.
2 as a case in point, subcategorizationusing function words would yield:6Xb(4 ?
{) ?
Xc4 Xd({) (3)The subcategorization (indicated by theinformation in parentheses following thenonterminal) propagates the function word4(are) of Xb to the higher level structure to-gether with the function word {(of) of Xd.This propagation process generalizes to otherrules by maintaining the ordering of the func-tion words according to their appearance inthe source sentence.
Note that the subcate-gorized nonterminals often resemble genuinesyntactic categories, for instance X({) canfrequently be interpreted as a noun phrase.2.
To facilitate the computation of the domi-nance relationship, the coindexing in syn-chronized rules (indicated by the ?
symbolin Eq.
1) must be expanded to include infor-mation not only about the nonterminal corre-spondences but also about the alignment ofthe lexical items.
For example, adding lexi-cal alignment information to rule Xd wouldyield:Xd ?
?X1{2?3, inventions3 of2 X1?
(4)6The target language side is concealed for clarity.327The computation of the dominance relation-ship using this alignment information will bediscussed in detail in the next section.Again takingXb in Fig.
2 as a case in point, thedominance feature takes the following form:fdom(Xb) ?
dom(d(4,{)|4, {)) (5)dom(d(YL, YR)|YL, YR)) (6)where the probability of4 ?
{ is estimated ac-cording to the probability of d(4,{).In practice, both 4(are) and {(of) may ap-pear together in one same rule.
In such a case, adominance score is not calculated since the topo-logical ordering of the two function words is un-ambiguous.
Hence, in our implementation, adominance score is only calculated at the pointswhere the topological ordering of the hierarchicalphrases needs to be resolved, i.e.
the two functionwords always come from two different hierarchi-cal phrases.5 Parameter EstimationLearning the dominance model involves extract-ing d values for every pair of neighboring func-tion words in the training bitext.
Such statisticsare not directly observable in parallel corpora, soestimation is needed.
Our estimation method isbased on two facts: (1) the topological orderingof hierarchical phrases is tightly coupled with thespan of the hierarchical phrases, and (2) the spanof a hierarchical phrase at a higher level is al-ways a superset of the span of all other hierarchicalphrases at the lower level of its substructure.
Thus,to establish soft estimates of dominance counts,we utilize alignment information available in therule together with the consistent alignment heuris-tic (Och and Ney, 2004) traditionally used to guessphrase alignments.Specically, we dene the span of a functionword as a maximal, consistent alignment in thesource language that either starts from or endswith the function word.
(Requiring that spans bemaximal ensures their uniqueness.)
We will re-fer to such spans as Maximal Consistent Align-ments (MCA).
Note that each function word hastwo such Maximal Consistent Alignments: onethat ends with the function word (MCAR)and an-other that starts from the function word (MCAL).Y ?
Y ??
left- right- dont- nei-First First Care therZ (and) 4 (are) 0.11 0.16 0.68 0.054 (are) { (of) 0.57 0.15 0.06 0.22Table 1: The distribution of the dominance valuesof the function words involved in Fig.
1.
The valuewith the highest probability is in bold.Given two function words Y ?
and Y ?
?, with Y ?preceding Y ?
?, we dene the value of d by exam-ining the MCAs of the two function words.d(Y ?, Y ??)
=??????????
?leftFirst, Y ?
6?
MCAR(Y ??)
?
Y ???
MCAL(Y ?
)rightFirst, Y ??
MCAR(Y ??)
?
Y ??
6?
MCAL(Y ?
)dontCare, Y ??
MCAR(Y ??)
?
Y ???
MCAL(Y ?
)neither, Y ?
6?
MCAR(Y ??)
?
Y ??
6?
MCAL(Y ?)(6)Fig.
4a illustrates the leftFirst dominance valuewhere the intersection of the MCAs contains onlythe second function word ({(of)).
Fig.
4b illus-trates the dontCare value, where the intersectioncontains both function words.
Similarly, rightFirstand neither are represented by an intersection thatcontains only Y ?, or by an empty intersection, re-spectively.
Once all the d values are counted, thepairwise dominance model of neighboring func-tion words can be estimated simply from countsusing maximum likelihood.
Table 1 illustrates es-timated dominance values that correctly resolvethe topological ordering for our running example.6 Experimental SetupWe tested the effect of introducing the pairwisedominance model into hierarchical phrase-basedtranslation on Chinese-to-English and Arabic-to-English translation tasks, thus studying its effectin two languages where the use of function wordsdiffers signicantly.
Following Setiawan et al(2007), we identify function words as the N mostfrequent words in the corpus, rather than identify-ing them according to linguistic criteria; this ap-proximation removes the need for any additionallanguage-specic resources.
We report resultsfor N = 32, 64, 128, 256, 512, 1024, 2048.7 For7We observe that even N = 2048 represents less than1.5% and 0.8% of the words in the Chinese and Arabic vo-cabularies, respectively.
The validity of the frequency-basedstrategy, relative to linguistically-dened function words, isdiscussed in Section 8328nanbjjjzjzjthe last centuryofinnovationsarecell phonesandcomputers?ZC?
4?- {?jzjzjjjthe last centuryofinnovationsarecell phonesandcomputers?ZC?
4?-{?Figure 4: Illustrations for: a) the leftFirst value,and b) the dontCare value.
Thickly borderedboxes are MCAs of the function words while solidcircles are the alignment points of the functionwords.
The gray boxes are the intersections of thetwo MCAs.all experiments, we report performance using theBLEU score (Papineni et al, 2002), and we assessstatistical signicance using the standard boot-strapping approach introduced by (Koehn, 2004).Chinese-to-English experiments.
We trainedthe system on the NIST MT06 Eval corpus ex-cluding the UN data (approximately 900K sen-tence pairs).
For the language model, we used a 5-gram model with modied Kneser-Ney smoothing(Kneser and Ney, 1995) trained on the English sideof our training data as well as portions of the Giga-word v2 English corpus.
We used the NIST MT03test set as the development set for optimizing inter-polation weights using minimum error rate train-ing (MERT; (Och and Ney, 2002)).
We carried outevaluation of the systems on the NIST 2006 eval-uation test (MT06) and the NIST 2008 evaluationtest (MT08).
We segmented Chinese as a prepro-cessing step using the Harbin segmenter (Zhao etal., 2001).Arabic-to-English experiments.
We trainedthe system on a subset of 950K sentence pairsfrom the NIST MT08 training data, selected by?subsampling?
from the full training data using amethod proposed by Kishore Papineni (personalcommunication).
The subsampling algorithm se-lects sentence pairs from the training data in away that seeks reasonable representation for all n-grams appearing in the test set.
For the languagemodel, we used a 5-gram model trained on the En-glish portion of the whole training data plus por-tions of the Gigaword v2 corpus.
We used theNIST MT03 test set as the development set foroptimizing the interpolation weights using MERT.We carried out the evaluation of the systems on theNIST 2006 evaluation set (MT06) and the NIST2008 evaluation set (MT08).
Arabic source textwas preprocessed by separating clitics, the de-niteness marker, and the future tense marker fromtheir stems.7 Experimental ResultsChinese-to-English experiments.
Table 2 sum-marizes the results of our Chinese-to-English ex-periments.
These results conrm that the pairwisedominance model can signicantly increase per-formance as measured by the BLEU score, with aconsistent pattern of results across the MT06 andMT08 test sets.
Modeling N = 32 drops the per-formance marginally below baseline, suggestingthat perhaps there are not enough words for thepairwise dominance model to work with.
Dou-bling the number of words (N = 64) producesa small gain, and dening the pairwise dominancemodel using N = 128 most frequent words pro-duces a statistically signicant 1-point gain overthe baseline (p < 0.01).
Larger values of Nyield statistically signicant performance abovethe baseline, but without further improvementsover N = 128.Arabic-to-English experiments.
Table 3 sum-marizes the results of our Arabic-to-English ex-periments.
This set of experiments shows a pat-tern consistent with what we observed in Chinese-to-English translation, again generally consistentacross MT06 and MT08 test sets although mod-eling a small number of lexical items (N = 32)brings a marginal improvement over the baseline.In addition, we again nd that the pairwise dom-inance model with N = 128 produces the mostsignicant gain over the baseline in the MT06,although, interestingly, modeling a much largernumber of lexical items (N = 2048) yields thestrongest improvement for the MT08 test set.329MT06 MT08baseline 30.58 24.08+dom(N = 32) 30.43 23.91+dom(N = 64) 30.96 24.45+dom(N = 128) 31.59 24.91+dom(N = 256) 31.24 24.26+dom(N = 512) 31.33 24.39+dom(N = 1024) 31.22 24.79+dom(N = 2048) 30.75 23.92Table 2: Experimental results on Chinese-to-English translation with the pairwise dominancemodel (dom) of different N .
The baseline (therst line) is the original hierarchical phrase-basedsystem.
Statistically signicant results (p < 0.01)over the baseline are in bold.MT06 MT08baseline 41.56 40.06+dom(N = 32) 41.66 40.26+dom(N = 64) 42.03 40.73+dom(N = 128) 42.66 41.08+dom(N = 256) 42.28 40.69+dom(N = 512) 41.97 40.95+dom(N = 1024) 42.05 40.55+dom(N = 2048) 42.48 41.47Table 3: Experimental results on Arabic-to-English translation with the pairwise dominancemodel (dom) of different N .
The baseline (therst line) is the original hierarchical phrase-basedsystem.
Statistically signicant results over thebaseline (p < 0.01) are in bold.8 Discussion and Future WorkThe results in both sets of experiments show con-sistently that we have achieved a signicant gainsby modeling the topological ordering of functionwords.
When we visually inspect and comparethe outputs of our system with those of the base-line, we observe that improved BLEU score oftencorresponds to visible improvements in the sub-jective translation quality.
For example, the trans-lations for the Chinese sentence ?<1?2 :3?4 ?5 ?6 8?7 8 9 ?10 ?11 ?12?13?, taken from Chinese MT06 test set, are asfollows (co-indexing subscripts represent recon-structed word alignments):?
baseline: ?military1 intelligence2 un-der observation8 in5 u.s.6 air raids7 :3 iran4to9 how11 long12 ?13 ??
+dom(N=128): ?
military1 survey2 :3 how11long12 iran4 under8 air strikes7 of the u.s6can9 hold out10 ?13 ?In addition to some lexical translation errors(e.g.
?6 should be translated to U.S. Army),the baseline system also makes mistakes in re-ordering.
The most obvious, perhaps, is its fail-ure to capture the wh-movement involving the in-terrogative word ?11 (how); this should moveto the beginning of the translated clause, consis-tent with English wh-fronting as opposed to Chi-nese wh in situ.
The pairwise dominance modelhelps, since the dominance value between the in-terrogative word and its previous function word,the modal verb 9(can) in the baseline system'soutput, is neither, rather than rightFirst as in thebetter translation.The fact that performance tends to be best us-ing a frequency threshold of N = 128 strikesus as intuitively sensible, given what we knowabout word frequency rankings.8In English,for example, the most frequent 128 words in-clude virtually all common conjunctions, deter-miners, prepositions, auxiliaries, and comple-mentizers ?
the crucial elements of ?syntacticglue?
that characterize the types of linguisticphrases and the ordering relationships betweenthem ?
and a very small proportion of con-tent words.
Using Adam Kilgarriff's lemma-tized frequency list from the British National Cor-pus, http://www.kilgarriff.co.uk/bnc-readme.html,the most frequent 128 words in English are heav-ily dominated by determiners, ?functional?
ad-verbs like not and when, ?particle?
adverbs likeup, prepositions, pronouns, and conjunctions, withsome arguably ?functional?
auxiliary and lightverbs like be, have, do, give, make, take.
Con-tent words are generally limited to a small numberof frequent verbs like think and want and a verysmall handful of frequent nouns.
In contrast, ranks129-256 are heavily dominated by the traditionalcontent-word categories, i.e.
nouns, verbs, adjec-tives and adverbs, with a small number of left-overfunction words such as less frequent conjunctionswhile, when, and although.Consistent with these observations for English,the empirical results for Chinese suggest that our8In fact, we initially simply choseN = 128 for our exper-imentation, and then did runs with alternative N to conrmour intuitions.330approximation of function words using word fre-quency is reasonable.
Using a list of approxi-mately 900 linguistically identied function wordsin Chinese extracted from (Howard, 2002), we ob-serve that that the performance drops when in-creasing N above 128 corresponds to a large in-crease in the number of non-function words usedin the model.
For example, with N = 2048, theproportion of non-function words is 88%, com-pared to 60% when N = 128.9One natural extension of this work, therefore,would be to tighten up our characterization offunction words, whether statistically, distribution-ally, or simply using manually created resourcesthat exist for many languages.
As a rst step, wedid a version of the Chinese-English experimentusing the list of approximately 900 genuine func-tion words, testing on the Chinese MT06 set.
Per-haps surprisingly, translation performance, 30.90BLEU, was around the level we obtained whenusing frequency to approximate function words atN = 64.
However, we observe that many ofthe words in the linguistically motivated functionword list are quite infrequent; this suggests thatdata sparseness may be an additional factor worthinvestigating.Finally, although we believe there are strongmotivations for focusing on the role of functionwords in reordering, there may well be value inextending the dominance model to include contentcategories.
Verbs and many nouns have subcat-egorization properties that may inuence phraseordering, for example, and this may turn out to ex-plain the increase in Arabic-English performancefor N = 2048 using the MT08 test set.
More gen-erally, the approach we are taking can be viewedas a way of selectively lexicalizing the automati-cally extracted grammar, and there is a large rangeof potentially interesting choices in how such lex-icalization could be done.9 Related WorkIn the introduction, we discussed Chiang's (2005)constituency feature, related ideas explored byMarton and Resnik (2008) and Chiang et al(2008), and the target-side variation investigatedby Zollman et al (2006).
These methods differfrom each other mainly in terms of the specic lin-9We plan to do corresponding experimentation and anal-ysis for Arabic once we identify a suitable list of manuallyidentied function words.guistic knowledge being used and on which sidethe constraints are applied.Shen et al (2008) proposed to use lin-guistic knowledge expressed in terms of a de-pendency grammar, instead of a syntactic con-stituency grammar.
Villar et al (2008) attemptedto use syntactic constituency on both the sourceand target languages in the same spirit as the con-stituency feature, along with some simple pattern-based heuristics ?
an approach also investigated byIglesias et al (2009).
Aiming at improving the se-lection of derivations, Zhou et al (2008) proposedprior derivation models utilizing syntactic annota-tion of the source language, which can be seen assmoothing the probabilities of hierarchical phrasefeatures.A key point is that the model we have intro-duced in this paper does not require the linguisticsupervision needed in most of this prior work.
Weestimate the parameters of our model from paralleltext without any linguistic annotation.
That said,we would emphasize that our approach is, in fact,motivated in linguistic terms by the role of func-tion words in natural language syntax.10 ConclusionWe have presented a pairwise dominance modelto address reordering issues that are not handledparticularly well by standard hierarchical phrase-based modeling.
In particular, the minimal lin-guistic commitment in hierarchical phrase-basedmodels renders them susceptible to overgenera-tion of reordering choices.
Our proposal han-dles the overgeneration problem by identifyinghierarchical phrases with function words and byusing function word relationships to incorporatesoft constraints on topological orderings.
Ourexperimental results demonstrate that introducingthe pairwise dominance model into hierarchicalphrase-based modeling improves performance sig-nicantly in large-scale Chinese-to-English andArabic-to-English translation tasks.AcknowledgmentsThis research was supported in part by theGALE program of the Defense Advanced Re-search Projects Agency, Contract No.
HR0011-06-2-001.
Any opinions, ndings, conclusions orrecommendations expressed in this paper are thoseof the authors and do not necessarily reect theview of the sponsors.331ReferencesDavid Chiang, Yuval Marton, and Philip Resnik.
2008.Online large-margin training of syntactic and struc-tural translation features.
In Proceedings of the2008 Conference on Empirical Methods in Natu-ral Language Processing, pages 224?233, Honolulu,Hawaii, October.David Chiang.
2005.
A hierarchical phrase-basedmodel for statistical machine translation.
In Pro-ceedings of the 43rd Annual Meeting of the Associa-tion for Computational Linguistics (ACL'05), pages263?270, Ann Arbor, Michigan, June.
Associationfor Computational Linguistics.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2):201?228.Jiaying Howard.
2002.
A Student Handbook for Chi-nese Function Words.
The Chinese University Press.Gonzalo Iglesias, Adria de Gispert, Eduardo R. Banga,and William Byrne.
2009.
Rule ltering by patternfor efcient hierarchical translation.
In Proceedingsof the 12th Conference of the European Chapter ofthe Association of Computational Linguistics (to ap-pear).R.
Kneser and H. Ney.
1995.
Improved backing-off for m-gram language modeling.
In Proceed-ings of IEEE International Conference on Acoustics,Speech, and Signal Processing95, pages 181?184,Detroit, MI, May.Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In Proceedingsof the 2003 Human Language Technology Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics, pages 127?133,Edmonton, Alberta, Canada, May.
Association forComputational Linguistics.Philipp Koehn.
2004.
Statistical signicance tests formachine translation evaluation.
In Proceedings ofEMNLP 2004, pages 388?395, Barcelona, Spain,July.Yuval Marton and Philip Resnik.
2008.
Soft syntac-tic constraints for hierarchical phrased-based trans-lation.
In Proceedings of The 46th Annual Meet-ing of the Association for Computational Linguis-tics: Human Language Technologies, pages 1003?1011, Columbus, Ohio, June.Franz Josef Och and Hermann Ney.
2002.
Discrim-inative training and maximum entropy models forstatistical machine translation.
In Proceedings of40th Annual Meeting of the Association for Com-putational Linguistics, pages 295?302, Philadelphia,Pennsylvania, USA, July.Franz Josef Och and Hermann Ney.
2004.
The align-ment template approach to statistical machine trans-lation.
Computational Linguistics, 30(4):417?449.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automaticevaluation of machine translation.
In Proceedingsof 40th Annual Meeting of the Association for Com-putational Linguistics, pages 311?318, Philadelphia,Pennsylvania, USA, July.Hendra Setiawan, Min-Yen Kan, and Haizhou Li.2007.
Ordering phrases with function words.
InProceedings of the 45th Annual Meeting of the As-sociation of Computational Linguistics, pages 712?719, Prague, Czech Republic, June.Libin Shen, Jinxi Xu, and Ralph Weischedel.
2008.A new string-to-dependency machine translation al-gorithm with a target dependency language model.In Proceedings of The 46th Annual Meeting of theAssociation for Computational Linguistics: HumanLanguage Technologies, pages 577?585, Columbus,Ohio, June.David Vilar, Daniel Stein, and Hermann Ney.
2008.Analysing soft syntax features and heuristics for hi-erarchical phrase based machine translation.
Inter-national Workshop on Spoken Language Translation2008, pages 190?197, October.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?404, Sep.Tiejun Zhao, Yajuan Lv, Jianmin Yao, Hao Yu, MuyunYang, and Fang Liu.
2001.
Increasing accuracyof chinese segmentation with strategy of multi-stepprocessing.
Journal of Chinese Information Pro-cessing (Chinese Version), 1:13?18.Bowen Zhou, Bing Xiang, Xiaodan Zhu, and YuqingGao.
2008.
Prior derivation models for formallysyntax-based translation using linguistically syntac-tic parsing and tree kernels.
In Proceedings ofthe ACL-08: HLT Second Workshop on Syntax andStructure in Statistical Translation (SSST-2), pages19?27, Columbus, Ohio, June.Andreas Zollmann and Ashish Venugopal.
2006.
Syn-tax augmented machine translation via chart parsing.In Proceedings on the Workshop on Statistical Ma-chine Translation, pages 138?141, New York City,June.332
