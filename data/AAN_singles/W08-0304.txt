Proceedings of the Third Workshop on Statistical Machine Translation, pages 26?34,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsRegularization and Search for Minimum Error Rate TrainingDaniel Cer, Daniel Jurafsky, and Christopher D. ManningStanford UniversityStanford, CA 94305cerd,jurafsky,manning@stanford.eduAbstractMinimum error rate training (MERT) is awidely used learning procedure for statisticalmachine translation models.
We contrast threesearch strategies for MERT: Powell?s method,the variant of coordinate descent found in theMoses MERT utility, and a novel stochasticmethod.
It is shown that the stochastic methodobtains test set gains of +0.98 BLEU on MT03and +0.61 BLEU on MT05.
We also presenta method for regularizing the MERT objec-tive that achieves statistically significant gainswhen combined with both Powell?s methodand coordinate descent.1 IntroductionOch (2003) introduced minimum error rate training(MERT) as an alternative training regime to the con-ditional likelihood objective previously used withlog-linear translation models (Och & Ney, 2002).This approach attempts to improve translation qual-ity by optimizing an automatic translation evalua-tion metric, such as the BLEU score (Papineni et al,2002).
This is accomplished by either directly walk-ing the error surface provided by an evaluation met-ric w.r.t.
the model weights or by using gradient-based techniques on a continuous approximation ofsuch a surface.
While the former is piecewise con-stant and thus cannot be optimized using gradienttechniques, Och (2003) provides an approach thatperforms such training efficiently.In this paper we explore a number of variations onMERT.
First, it is shown that performance gains canbe had by making use of a stochastic search strategyas compare to that obtained by Powell?s method andcoordinate descent.
Subsequently, results are pre-sented for two regularization strategies1.
Both allowcoordinate descent and Powell?s method to achieveperformance that is on par with stochastic search.In what follows, we briefly review minimum er-ror rate training, introduce our stochastic search andregularization strategies, and then present experi-mental results.2 Minimum Error Rate TrainingLet F be a collection of foreign sentences to betranslated, with individual sentences f0, f1, .
.
.
,fn.
For each fi, the surface form of an indi-vidual candidate translation is given by ei withhidden state hi associated with the derivation ofei from fi.
Each ei is drawn from E , whichrepresents all possible strings our translation sys-tem can produce.
The (ei,hi, fi) triples are con-verted into vectors of m feature functions by?
: E ?H ?F ?
Rm whose dot product with theweight vector w assigns a score to each triple.The idealized translation process then is to find thehighest scoring pair (ei,hi) for each fi, or rather(ei,hi) = argmax(e?E,h?H)w ??
(e,h, f).The aggregate argmax for the entire data set F isgiven by equation (1)2.
This gives Ew which repre-sents the set of translations selected by the model fordata set F when parameterized by the weight vec-tor w. Let?s assume we have an automated mea-sure of translation quality ` that maps the collec-1While we prefer the term regularization, the strategies pre-sented here could also be referred to as smoothing methods.2Here, the translation of the entire data set is treated as asingle structured prediction problem using the feature functionvector ?
(E,H,F) =Pni ?
(ei,hi, fi)26id Translation log(PTM(f |e)) log(PLM(e)) BLEU-2e1 This is it -1.2 -0.1 29.64e2 This is small house -0.2 -1.2 63.59e3 This is miniscule building -1.6 -0.9 31.79e4 This is a small house -0.1 -0.9 100.00ref This is a small houseTable 1: Four hypothetical translations and their corresponding log model scores from a translation model PTM (f |e)and a language model PLM (e), along with their BLEU-2 scores according to the given reference translation.
TheMERT error surface for these translations is given in figure 1.tion of translations Ew onto some real valued loss,` : En ?
R. For instance, in the experiments thatfollow, the loss corresponds to 1 minus the BLEUscore assigned to Ew for a given collection of refer-ence translations.
(Ew,Hw) = argmax(E?En,H?Hn)w ??
(E,H,F) (1)Using n-best lists produced by a decoder to ap-proximate En and Hn, MERT searches for theweight vector w?
that minimizes the loss `.
Let-ting E?w denote the result of the translation argmaxw.r.t.
the approximate hypothesis space, the MERTsearch is then expressed by equation (2).
Notice theobjective function being optimized is equivalent tothe loss assigned by the automatic measure of trans-lation quality, i.e.
O(w) = `(E?w).w?
= argminw`(E?w) (2)After performing the parameter search, the de-coder is then re-run using the weights w?
to producea new set of n-best lists, which are then concate-nated with the prior n-best lists in order to obtain abetter approximation of En and Hn.
The parametersearch given in (2) can then be performed over theimproved approximation.
This process repeats un-til either no novel entries are produced for the com-bined n-best lists or the weights change by less thansome ?
across iterations.Unlike the objective functions associated withother popular learning algorithms, the objective Ois piecewise constant over its entire domain.
Thatis, while small perturbations in the weights, w, willchange the score assigned by w ??
(e,h, f) to eachtriple, (e,h, f), such perturbations will generally notchange the ranking between the pair selected by theargmax, (e?,h?)
= argmaxw ??
(e,h, f), and anygiven competing pair (e?,h?).
However, at certaincritical points, the score assigned to some compet-ing pair (e?,h?)
will exceed that assigned to theprior winner (e?wold ,h?wold).
At this point, the pairreturned by argmaxw ??
(e,h, f) will change andloss ` will be evaluated using the newly selected e?.Figure 1: MERT objective for the translations givenin table 1.
Regions are labeled with the translationthat dominates within it, i.e.
argmaxw ??
(e, f),and with their corresponding objective values,1?
`(argmaxw ??
(e, f)).This is illustrated in figure (1), which plots theMERT objective function for a simple model withtwo parameters, wtm & wlm, and for which thespace of possible translations, E , consists of the foursentences given in table 13.
Here, the loss ` is de-3For this example, we ignore the latent variables, h, associ-27fined as 1.0?BLEU-2(e).
That is, ` is the differ-ence between a perfect BLEU score and the BLEUscore calculated for each translation using unigramand bi-gram counts.The surface can be visualized as a collection ofplateaus that all meet at the origin and then extendoff into infinity.
The latter property illustrates thatthe objective is scale invariant w.r.t.
the weight vec-tor w. That is, since any vector w?
= ?w ?
?>0 willstill result in the same relative rankings of all pos-sible translations according to w ??
(e,h, f), suchscaling will not change the translation selected bythe argmax.
At the boundaries between regions, theobjective is undefined, as 2 or more candidates areassigned identical scores by the model.
Thus, it isunclear what should be returned by the argmax forsubsequent scoring by `.Since the objective is piecewise constant, it can-not be minimized using gradient descent or even thesub-gradient method.
Two applicable methods in-clude downhill simplex and Powell?s method (Presset al, 2007).
The former attempts to find a lo-cal minimum in an n dimensional space by itera-tively shrinking or growing an n+ 1 vertex simplex4based on the objective values of the current vertexpoints and select nearby points.
In contrast, Pow-ell?s method operates by starting with a single pointin weight space, and then performing a series of lineminimizations until no more progress can be made.In this paper, we focus on line minimization basedtechniques, such as Powell?s method.2.1 Global minimum along a lineEven without gradient information, numerous meth-ods can be used to find, or approximately find, localminima along a line.
However, by exploiting the factthat the underlying scores assigned to competing hy-potheses, w ??
(e,h, f), vary linearly w.r.t.
changesin the weight vector, w, Och (2003) proposed a strat-egy for finding the global minimum along any givensearch direction.The insight behind the algorithm is as follows.Let?s assume we are examining two competingated with the derivation of each e from the foreign sentence f .If included, such variables would only change the graph in thatmultiple different derivations would be possible for each ej .
Ifpresent, the graph could then include disjoint regions that allmap to the same ej and thus the same objective value.4A simplex can be thought of as a generalization of a triangleto arbitrary dimensional spaces.Figure 2: Illustration of how the model score assignedto each candidate translation varies during a line searchalong the coordinate direction wlm with a starting pointof (wtm, wlm) = (1.0, 0.5).
Each plotted line corre-sponds to the model score for one of the translation candi-dates.
The vertical bands are labeled with the hypothesisthat dominates in that region.
The transitions betweenbands result from the dotted intersections between 1-bestlines.translation/derivation pairs, (e1,h1) & (e2,h2).Further, let?s say the score assigned by themodel to (e1,h1) is greater than (e2,h2), i.e.w ??
(e1,h1, f) > w ??
(e2,h2, f).
Since thescores of the two vary linearly along any searchdirection, d, we can find the point at which themodel?s relative preference for the competingpairs switches as p = w??(e1,h1,f)?w??(e2,h2,f)d??(e2,h2,f)?d??
(e1,h1,f) .At this particular point, we have the equality(pd+w) ??
(e1,h1, f) = (pd+w) ??
(e2,h2, f),or rather the point at which the scores assignedby the model to the candidates intersect alongsearch direction d5.
Such points correspond tothe boundaries between adjacent plateaus in theobjective, as prior to the boundary the loss function` is computed using the translation, e1, and after theboundary it is computed using e2.To find the global minimum for a search direc-tion d, we move along d and for each plateau we5Notice that, this point only exists if the slopes of thecandidates?
model scores along d are not equivalent, i.e.
ifd ??
(e2,h2, f) 6= d ??
(e1,h1, f).28Translation m b 1-beste1 -0.1 -1.25 (0.86,+?
]e2 -1.2 -0.8 (-0.83,0.88)e3 -0.9 -2.05 n/ae4 -0.9 -0.55 [?
?,-0.83]Table 2: Slopes, m, intercepts, b, and 1-best rangesfor the 4 translations given in table 1 during a linesearch along the coordinate wlm, with a starting point of(wtm, wlm) = (1.0, 0.5).
This line search in illustratedin figure(2).identify all the points at which the score assignedby the model to the current 1-best translation inter-sects the score assigned to competing translations.At the closest such intersection, we have a new 1-best translation.
Moving to the plateau associatedwith this new 1-best, we then repeat the search forthe nearest subsequent intersection.
This continuesuntil we know what the 1-best translations are for allpoints along d. The global minimum can then befound by examining ` once for each of these.Let?s return briefly to our earlier example given intable 1.
Starting at position (wtm, wlm) = (1.0, 0.5)and searching along the wlm coordinate, i.e.
(dtm, dlm) = (0.0, 1.0), table 2 gives the linesearch slopes, m = d ??
(e,h, f), and intercepts,b = w ??
(e,h, f), for each of the four candidatetranslations.
Using the procedure just described, wecan then find what range of values along d eachcandidate translation is assigned the highest rela-tive model score.
Figure 2 illustrates how the scoreassigned by the model to each of the translationschanges as we move along d. Each of the banded re-gions corresponds to a plateau in the objective, andeach of the top most line intersections represents thetransition from one plateau to the next.
Note that,while the surface that is defined by the line segmentswith the highest classifier score for each region isconvex, this is not a convex optimization problem aswe are optimizing over the loss ` rather than classi-fier score.Pseudocode for the line search is given in algo-rithm 1.
Letting n denote the number of foreign sen-tences, f , in a dataset, and having m denote the sizeof the individual n-best lists, |l|, the time complexityof the algorithm is given by O(nm2).
This is seenin that each time we check for the nearest intersec-tion to the current 1-best for some n-best list l, weAlgorithm 1 Och (2003)?s line search method tofind the global minimum in the loss, `, when start-ing at the point w and searching along the directiond using the candidate translations given in the col-lection of n-best lists L.Input: L, w, d, `I ?
{}for l ?
L dofor e ?
l dom{e} ?
e.features ?
db{e} ?
e.features ?
wend forbestn ?
argmaxe?l m{e} {b{e} breaks ties}loopbestn+1 = argmine?l max(0, b{bestn}?b{e}m{e}?m{bestn})intercept ?
max(0, b{bestn}?b{bestn+1}m{bestn+1}?m{bestn})if intercept > 0 thenadd(I, intercept)elsebreakend ifend loopend foradd(I, max(I) + 2?
)ibest = argmini?I eval`(L,w + (i?
?)
?
d)return w + (ibest ?
?)
?
dmust calculate its intersection with all other candi-date translations that have yet to be selected as the1-best.
And, for each of the n n-best lists, this mayhave to be done up to m?
1 times.2.2 Search StrategiesIn this section, we review two search strategies that,in conjunction with the line search just described,can be used to drive MERT.
The first, Powell?smethod, was advocated by Och (2003) when MERTwas first introduced for statistical machine transla-tion.
The second, which we call Koehn-coordinatedescent (KCD)6, is used by the MERT utility pack-aged with the popular Moses statistical machinetranslation system (Koehn et al, 2007).6Moses uses David Chiang?s CMERT package.
Within thesource file mert.c, the function that implements the overallsearch strategy, optimize koehn(), is based on Philipp Koehn?sPerl script for MERT optimization that was distributed withPharaoh.292.2.1 Powell?s MethodPowell?s method (Press et al, 2007) attempts toefficiently search the objective by constructing a setof mutually non-interfering search directions.
Thebasic procedure is as follows: (i) A collection ofsearch directions is initialized to be the coordinatesof the space being searched; (ii) The objective isminimized by looping through the search directionsand performing a line minimization for each; (iii) Anew search direction is constructed that summarizesthe cumulative direction of the progress made dur-ing step (ii) (i.e., dnew = wpreii ?wpostii).
Aftera line minimization is performed along dnew, it isused to replace one of the existing search directions.
(iv) The process repeats until no more progress canbe made.
For a quadratic function of n variables,this procedure comes with the guarantee that it willreach the minimum within n iterations of the outerloop.
However, since Powell?s method is usually ap-plied to non-quadratic optimization problems, a typ-ical implementation will forego the quadratic con-vergence guarantees in favor of a heuristic schemethat allows for better navigation of complex sur-faces.2.2.2 Koehn?s Coordinate DescentKCD is a variant of coordinate descent that, ateach iteration, moves along the coordinate which al-lows for the most progress in the objective.
In or-der to determine which coordinate this is, the rou-tine performs a trial line minimization along each.
Itthen updates the weight vector with the one that itfound to be most successful.
While much less so-phisticated that Powell, our results indicate that thismethod may be marginally more effective at opti-mizing the MERT objective7.3 ExtensionsIn this section we present and motivate two novelextensions to MERT.
The first is a stochastic alterna-tive to the Powell and KCD search strategies, whilethe second is an efficient method for regularizing theobjective.7While we are not aware of any previously published resultsthat demonstrate this, it is likely that we were not the first tomake this discovery as even though Moses?
MERT implemen-tation includes a vestigial implementation of Powell?s method,the code is hardwired to call optimize koehn rather than the rou-tine for Powell.3.1 Random Search DirectionsOne significant advantage of Powell?s algorithmover coordinate descent is that it can optimize alongdiagonal search directions in weight space.
That is,given a model with a dozen or so features, it canexplore gains that are to be had by simultaneouslyvarying two or more of the feature weights.
In gen-eral, the diagonals that Powell?s method constructsallow it to walk objective functions more efficientlythan coordinate descent (Press et al, 2007).
How-ever, given that we have a line search algorithmthat will find the global minima along any givensearch direction, diagonal search may be of evenmore value.
That is, similar to ridge phenomenonthat arise in traditional hill climbing search, it is pos-sible that there are points in the objective that are theglobal minimum along any given coordinate direc-tion, but are not the global minimum along diagonaldirections.However, one substantial disadvantage for Pow-ell is that the assumptions it uses to build up the di-agonal search directions do not hold in the presentcontext.
Specifically, the search directions are builtup under the assumption that near a minimum thesurface looks approximately quadratic and that weare performing local line minimizations within suchregions.
However, since we are performing globalline minimizations, it is possible for the algorithm tojump from the region around one minima to another.If Powell?s method has already started to tune itssearch directions for the prior minima, it will likelybe less effective in its efforts to search the new re-gion.
To this extent, coordinate descent will be morerobust than Powell as it has no assumptions that areviolated when such a jump occurs.One way of salvaging Powell?s algorithm in thiscontext would be to incorporate additional heuris-tics that detect when the algorithm has jumped fromthe region around one minima to another.
Whenthis occurs, the search directions could be reset tothe coordinates of the space.
However, we opt for asimpler solution, which like Powell?s algorithm per-forms searches along diagonals in the space, but thatlike coordinate descent is sufficiently simple that thealgorithm will not be confused by sudden jumps be-tween regions.Specifically, the search procedure chooses di-rections at random such that each component30Figure 3: Regularization during line search - using, from left to right: (i) the maximum loss of adjacent plateaus, (ii)the average loss of adjacent plateaus, (iii) no regularization.
Each set of bars represents adjacent plateaus along the linebeing searched, with the height of the bars representing their associated loss.
The vertical lines indicate the surrogateloss values used for the center region under each of the schemes (i-iii).is distributed according to a Gaussian8, d s.t.di ?
N(0, 1).
This allows the procedure to mini-mize along diagonal search directions, while makingessentially no assumptions regarding the character-istics of the objective or the relationship between aseries of sequential line minimizations.
In the resultsthat follow, we show that, perhaps surprisingly, thissimple procedure outperforms both KCD and Pow-ell?s method.3.2 RegularizationOne potential drawback of MERT, as it is typicallyimplemented, is that it attempts to find the best pos-sible set of parameters for a training set withoutmaking any explicit efforts to find a set of param-eters that can be expected to generalize well.
Forexample, let?s say that for some objective there isa very deep but narrow minima that is surroundedon all sides by very bad objective values.
Thatis, the BLEU score at the minima might be 39.1while all surrounding plateaus have a BLEU scorethat is < 10.
Intuitively, such a minima would be avery bad solution, as the resulting parameters wouldlikely exhibit very poor generalization to other datasets.
This could be avoided by regularizing the sur-face in order to eliminate such spurious minima.One candidate for performing such regularizationis the continuous approximation of the MERT objec-tive, O = Epw(`).
Och (2003) claimed that this ap-proximation achieved essentially equivalent perfor-mance to that obtained when directly using the lossas the objective, O = `.
However, Zens et al (2007)found that O = Epw(`) achieved substantially bettertest set performance than O = `, even though it per-forms slightly worse on the data used to train theparameters.
Similarly, Smith and Eisner (2006) re-ported test set gains for the related technique of min-imum risk annealing, which incorporates a temper-8However, we speculate that similar results could be ob-tained using a uniform distribution over (?1, 1)ature parameter that trades off between the smooth-ness of the objective and the degree it reflects theunderlying piecewise constant error surface.
How-ever, the most straightforward implementation ofsuch methods requires a loss that can be applied atthe sentence level.
If the evaluation metric of inter-est does not have this property (e.g.
BLEU), the lossmust be approximated using some surrogate, withsuccessful learning then being tied to how well thesurrogate captures the critical properties of the un-derlying loss.The techniques of Zens et al (2007) & Smithand Eisner (2006) regularize by implicitly smooth-ing over nearby plateaus in the error surface.
Wepropose an alternative scheme that operates directlyon the piecewise constant objective and that miti-gates the problem of spurious local minima by ex-plicitly smoothing over adjacent plateaus during theline search.
That is, when assessing the desirabil-ity of any given plateau, we examine a fixed win-dow w of adjacent plateaus along the direction be-ing searched and combine their evaluation scores.We explore two combination methods, max andaverage.
The former, max, assigns each plateau anobjective value that is equal to the maximum objec-tive value in its surrounding window, while averageassigns a plateau an objective value that is equal toits window?s average.
Figure 3 illustrates both meth-ods for regularizing the plateaus and contrasts themwith the case where no regularization is used.
No-tice that, while both methods discount spurious pitsin the objective, average still does place some valueon isolated deep plateaus, and max discounts themcompletely.Note that one potential weakness of this schemeis the value assigned by the regularized objectiveto any given point differs depending on the direc-tion being searched.
As such, it has the potential towreak havoc on methods such as Powell?s, which ef-fectively attempt to learn about the curvature of the31objective from a sequence of line minimizations.4 ExperimentsThree sets of experiments were performed.
For thefirst set, we compare the performance of Powell?smethod, KCD, and our novel stochastic search strat-egy.
We then evaluate the performance of all threemethods when the objective is regularized using theaverage of adjacent plateaus for window sizes vary-ing from 3 to 7.
Finally, we repeat the regularizationexperiment, but using the maximum objective valuefrom the adjacent plateaus.
These experiments wereperformed using the Chinese English evaluation dataprovided for NIST MT eval 2002, 2003, and 2005.MT02 was used as a dev set for MERT learning,while MT03 and MT05 were used as our test sets.For all experiments, MERT training was per-formed using n-best lists from the decoder of size100.
During each iteration, the MERT search wasperformed once with a starting point of the weightsused to generate the most recent set of n-best listsand then 5 more times using randomly selected start-ing points9.
Of these, we retain the weights fromthe search that obtained the lowest objective value.Training continued until either decoding producedno novel entries for the combined n-best lists or noneof the parameter values changed by more than 1e-5across subsequent iterations.4.1 SystemExperiments were run using a right-to-left beamsearch decoder that achieves a matching BLEUscore to Moses (Koehn et al, 2007) over a varietyof data sets.
Moreover, when using the same under-lying model, the two decoders only produce transla-tions that differ by one or more words 0.2% of thetime.
We made use of a stack size of 50 as it al-lowed for faster experiments while only performingmodestly worse than a stack of 200.
The distortionlimit was set to 6.
And, we retrieved 20 translationoptions for each unique source phrase.Our phrase table was built using 1, 140, 693 sen-tence pairs sampled from the GALE Y2 training9Only 5 random restarts were used due to time constraints.Ideally, a sizable number of random restarts should be used inorder to minimize the degree to which the results are influencedby some runs receiving starting points that are better in generalor perhaps better/worse w.r.t.
their specific optimization strat-egy.Method Dev Test TestMT02 MT03 MT05KCD 30.967 30.778 29.580Powell 30.638 30.692 29.780Random 31.681 31.754 30.191Table 3: BLEU scores obtained by models trained usingthe three different parameter search strategies: Powell?smethod, KCD, and stochastic search.data.
The Chinese data was word segmented us-ing the GALE Y2 retest release of the StanfordCRF segmenter (Tseng et al, 2005).
Phrases wereextracted using the typical approach described inKoehn et al (2003) of running GIZA++ (Och &Ney, 2003) in both directions and then mergingthe alignments using the grow-diag-final heuristic.From the merged alignments we also extracted a bi-directional lexical reordering model conditioned onthe source and the target phrases (Tillmann, 2004)(Koehn et al, 2007).
A 5-gram language modelwas created using the SRI language modeling toolkit(Stolcke, 2002) and trained using the Gigaword cor-pus and English sentences from the parallel data.5 ResultsAs illustrated in table 3, Powell?s method and KCDachieve a very similar level of performance, withKCD modestly outperforming Powell on the MT03test set while Powell modestly outperforms coordi-nate descent on the MT05 test set.
Moreover, thefact that Powell?s algorithm did not perform betterthan KCD on the training data10, and in fact actuallyperformed modestly worse, suggests that Powell?sadditional search machinery does not provide muchbenefit for MERT objectives.Similarly, the fact that the stochastic search ob-tains a much higher dev set score than either Pow-ell or KCD indicates that it is doing a better jobof optimizing the objective than either of the twoalternatives.
These gains suggest that stochasticsearch does make better use of the global minimumline search than the alternative methods.
Or, alter-natively, it strengthens the claim that the methodsucceeds at combining one of the critical strengths10This indicates that Powell failed to find a deeper minimain the objective, since recall that the unregularized objective isequivalent to the model?s dev set performance.32Method Window Dev Test TestAvg MT02 MT03 MT05Coordinate none 30.967 30.778 29.5803 31.665 31.675 30.2665 31.317 31.229 30.1827 31.205 31.824 30.149Powell none 30.638 30.692 29.7803 31.333 31.412 29.8905 31.748 31.777 30.3347 31.249 31.571 30.161Random none 31.681 31.754 30.1913 31.548 31.778 30.2635 31.336 31.647 30.4157 30.501 29.336 28.372Method Window Dev Test TestMax MT02 MT03 MT05Coordinate none 30.967 30.778 29.5803 31.536 31.927 30.3345 31.484 31.702 29.6877 31.627 31.294 30.199Powell none 30.638 30.692 29.7803 31.428 30.944 29.5985 31.407 31.596 30.0907 30.870 30.911 29.620Random none 31.681 31.754 30.1913 31.179 30.898 29.5295 30.903 31.666 29.9637 31.920 31.906 30.674Table 4: BLEU scores obtained when regularizing using the average loss of adjacent plateaus, left, and the maximumloss of adjacent plateaus, right.
The none entry for each search strategy represents the baseline where no regularizationis used.
Statistically significant test set gains, p < 0.01, over the respective baselines are in bold face.of Powell?s method, diagonal search, with coordi-nate descent?s robustness to the sudden jumps be-tween regions that result from global line minimiza-tion.
Using an approximate randomization test forstatistical significance (Riezler & Maxwell, 2005),and with KCD as a baseline, the gains obtainedby stochastic search on MT03 are statistically sig-nificant (p = 0.002), as are the gains on MT05(p = 0.005).Table 4 indicates that performing regularizationby either averaging or taking the maximum of adja-cent plateaus during the line search leads to gains forboth Powell?s method and KCD.
However, no reli-able additional gains appear to be had when stochas-tic search is combined with regularization.It may seem surprising that the regularizationgains for Powell & KCD are seen not only in the testsets but on the dev set as well.
That is, in typical ap-plications, regularization slightly decreases perfor-mance on the data used to train the model.
However,this trend can in part be accounted for by the fact thatduring training, MERT is using n-best lists for objec-tive evaluations rather than the more expensive pro-cess of running the decoder for each point that needsto be checked.
As such, during each iteration oftraining, the decoding performance of the model ac-tually represents its generalization performance rel-ative to what was learned from the n-best lists cre-ated during prior iterations.
Moreover, better gen-eralization from the prior n-best lists can also helpdrive subsequent learning as there will then be morehigh quality translations on the n-best lists used forfuture iterations of learning.
Additionally, regular-ization can reduce search errors by reducing the riskof getting stuck in spurious low loss pits that are inotherwise bad regions of the space.6 ConclusionsWe have presented two methods for improving theperformance of MERT.
The first is a novel stochas-tic search strategy that appears to make better use ofOch (2003)?s algorithm for finding the global min-imum along any given search direction than eithercoordinate descent or Powell?s method.
The sec-ond is a simple regularization scheme that leads toperformance gains for both coordinate descent andPowell?s method.
However, no further gains are ob-tained by combining the stochastic search with reg-ularization of the objective.One quirk of the regularization scheme presentedhere is that the regularization applied to any givenpoint in the objective varies depending upon whatdirection the point is approached from.
We arecurrently looking at other similar regularizationschemes that maintain consistent objective valuesregardless of the search direction.AcknowledgmentsWe extend our thanks to our three anonymous reviewers,33particularly for the depth of analysis provided.
This paperis based on work funded in part by the Defense AdvancedResearch Projects Agency through IBM.ReferencesKoehn, P., Hoang, H., Birch, A., Callison-Burch, C.,Federico, M., Bertoldi, N., Cowan, B., Shen, W.,Moran, C., Zens, R., Dyer, C., Bojar, O., Con-stantin, A., & Herbst, E. (2007).
Moses: Opensource toolkit for statistical machine translation.In ACL.Koehn, P., Och, F. J., & Marcu, D. (2003).
Statisticalphrase-based translation.
In HLT-NAACL.Och, F.-J.
(2003).
Minimum error rate training instatistical machine translation.
In ACL.Och, F. J., & Ney, H. (2002).
Discriminative train-ing and maximum entropy models for statisticalmachine translation.
In ACL.Och, F. J., & Ney, H. (2003).
A systematic compari-son of various statistical alignment models.
Com-putational Linguistics, 29, 19?51.Papineni, K., Roukos, S., Ward, T., & Zhu, W.-J.(2002).
Bleu: a method for automatic evaluationof machine translation.
In ACL.Press, W. H., Teukolsky, S. A., Vetterling, W. T., &Flannery, B. P. (2007).
Numerical recipes 3rd edi-tion: The art of scientific computing.
CambridgeUniversity Press.Riezler, S., & Maxwell, J. T. (2005).
On some pit-falls in automatic evaluation and significance test-ing for mt.
In ACL.Smith, D. A., & Eisner, J.
(2006).
Minimum riskannealing for training log-linear models.
In ACL.Stolcke, A.
(2002).
Srilm ?
an extensible languagemodeling toolkit.
In ICSLP.Tillmann, C. (2004).
A unigram orientation modelfor statistical machine translation.
In ACL.Tseng, H., Chang, P., Andrew, G., Jurafsky, D.,& Manning, C. (2005).
A conditional randomfield word segmenter for sighan bakeoff 2005.
InSIGHAN Workshop on Chinese Language Pro-cessing.Zens, R., Hasan, S., & Ney, H. (2007).
A system-atic comparison of training criteria for statisticalmachine translation.
In EMNLP.34
