Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 176?184,The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational LinguisticsNon-humanlike Spoken Dialogue: A Design PerspectiveKotaro FunakoshiHonda Research Institute Japan Co., Ltd.8-1 Honcho, WakoSaitama, Japanfunakoshi@jp.honda-ri.comMikio NakanoHonda Research Institute Japan Co., Ltd.8-1 Honcho, WakoSaitama, Japannakano@jp.honda-ri.comKazuki KobayashiShinshu University4-17-1 Wakasato, NaganoNagano, Japankby@shinshu-u.ac.jpTakanori KomatsuShinshu University3-15-1 Tokida, UedaNagano, Japantkomat@shinshu-u.ac.jpSeiji YamadaNational Institute of Informatics2-1-2 Hitotsubashi, ChiyodaTokyo, Japanseiji@nii.ac.jpAbstractWe propose a non-humanlike spoken di-alogue design, which consists of two el-ements: non-humanlike turn-taking andnon-humanlike acknowledgment.
Two ex-perimental studies are reported in this pa-per.
The first study shows that the pro-posed non-humanlike spoken dialogue de-sign is effective for reducing speech colli-sions.
It also presents pieces of evidencethat show quick humanlike turn-taking isless important in spoken dialogue systemdesign.
The second study supports a hy-pothesis found in the first study that userpreference on response timing varies de-pending on interaction patterns.
Upon re-ceiving these results, this paper suggests apractical design guideline for spoken dia-logue systems.1 IntroductionSpeech and language are owned by humans.Therefore, spoken dialogue researchers tend topursue a humanlike spoken dialogue.
Only a fewresearchers positively investigate restricted (i.e.,non-humanlike) spoken dialogue design such as(Ferna?ndez et al, 2007).Humanlikeness is a very important concept andsometimes it is really useful to design machines /interactions.
Machines are, however, not humans.We believe humanlikenss cannot be the dominantfactor, or gold-standard, for designing spoken dia-logues.Pursuing humanlikeness has at least five criti-cal problems.
(1) Cost: in general, humanlikenessdemands powerful and highly functional hardwareand software, and highly integrated systems re-quiring top-grade experts both for developmentand maintenance.
All of them lead to cost over-run.
(2) Performance: sometimes, humanlikenessforces performance to be compromised.
For ex-ample, achieving quick turn-taking which humansdo in daily conversations forces automatic speechrecognizers, reasoners, etc.
to be compromised toenable severe real-time processing.
(3) Applicabil-ity: differences in cultures, genders, generations,situations limit the applicability of a humanlikedesign because it often accompanies a rigid char-acter.
For example, Shiwa et al (2008) succeededin improving users?
impression for slow responsesfrom a robot by using a filler but obviously useof such a filler is limited by social appropriate-ness.
(4) Expectancy: humanlike systems inducetoo much expectancy of users that they are as in-telligent as humans.
It will result in disappoint-ments (Komatsu and Yamada, 2010) and may re-duce users?
willingness to use systems.
Keepinghigh willingness is quite important from the view-point of both research (for collecting data fromusers to improve systems) and business (for con-tinuously selling systems with limited functional-ity).
(5) Risk: Although it is not verified, what iscalled the uncanny valley (Bartneck et al, 2007)probably exists.
It is commonly observed that peo-ple hate imperfect humanlike systems.We try to avoid these problems rather than over-come them.
Our position is positively exploringnon-humanlike spoken dialogue design.
This pa-176per focuses on its two elements, i.e., decelerateddialogues as non-humanlike turn-taking and an ar-tificial subtle expression (ASE) as non-humanlikeacknowledgment1, and presents two experimentalstudies regarding these two elements.
ASEs, de-fined by the authors in (Komatsu et al, 2010), aresimple expressions suitable for artifacts, which in-tuitively notify users about artifacts?
internal stateswhile avoiding the above five problems.In Section 2, the first study, which was pre-viously reported in (Funakoshi et al, 2010), issummarized and shows that the proposed non-humanlike spoken dialogue design is effective forreducing speech collisions.
It also presents piecesof evidence that shows quick humanlike turn-taking is less important in designing spoken dia-logue systems (SDSs).
In Section 3, the secondstudy, which is newly reported in this paper, showsa tendency supporting a hypothesis found in thefirst study that user preference on response timingvaries depending on interaction patterns.
Upon re-ceiving the results of the two experiments, a designguideline for SDSs is suggested in Section 4.2 Study 1: Reducing Speech Collisionswith an Artificial Subtle Expressionin a Decelerated DialogueAn important issue in SDSs is the management ofturn-taking.
Failures of turn-taking due to sys-tems?
end-of-turn misdetection cause undesiredspeech collisions, which harm smooth communi-cation and degrade system usability.There are two approaches to reducing speechcollisions due to end-of-turn misdetection.
Thefirst approach is using machine learning tech-niques to integrate information from multiplesources for accurate end-of-turn detection in earlytiming.
The second approach is to make a long in-terval after the user?s speech signal ends and be-fore the system replies simply because a longerinterval means no continued speech comes.
Asfar as the authors know, all the past work takesthe first approach (e.g., (Kitaoka et al, 2005;Raux and Eskenazi, 2009)) because the second ap-proach deteriorates responsiveness of SDSs.
Thischoice is based on the presumption that users pre-fer a responsive system to less responsive systems.The presumption is true in most cases if the sys-1In this paper, acknowledgment denotes that at the level 1of the joint action ladder (Clark, 1996), which communicatesthe listener?s identifying the signal presented by the speaker.B l i n k i n g L E DFigure 1: Interface robot with an embedded LEDtem?s performance is at human level.
However, ifthe system?s performance is below human level,high responsiveness might not be vital or even beharmful.
For instance, Hirasawa et al (1999) re-ported that immediate overlapping backchannelscan cause users to have negative impressions.
Ki-taoka et al (2005) also reported that the familiarityof an SDS with backchannels was inferior to thatwithout backchannels due to a small portion of er-rors even though the overall timing and frequencyof backchannels was fairly good (but did not comeup to human operators).
Technologies are advanc-ing but they are still below human level.
We chal-lenge the past work that took the first approach.The second approach is simple and sta-ble against user differences and environmentalchanges.
Moreover, it can afford to employ morepowerful but computationally expensive speechprocessing or to build systems on small deviceswith limited resources.
A concern with this ap-proach is debasement of user experience due topoor responsiveness as stated above.
Another is-sue is speech collisions due to users?
following-up utterances such as repetitions.
Slow responsestend to induce such collision-eliciting speech.This section shows the results of the experimentin which participants engaged in hotel reservationtasks with an SDS equipped with an ASE-basedacknowledging method, which intuitively notifieda user about the system?s internal state (process-ing).
The results suggest that the method can re-duce speech collisions and provide users with pos-itive impressions.
The comparisons of evaluationsbetween systems with a slow reply speed and amoderate reply speed suggest that users of SDSsdo not care about slow replies.
These results in-dicate that decelerating spoken dialogues is not abad idea.2.1 ExperimentSystem An SDS that can handle a hotel reserva-tion domain was built.
The system was equipped177USERSYSTEMVAD tail margin wait intervalprocessing delayblinking LED (artificial subtle expression)short pausesdetected speech onset detected end-of-turnX Ysystem speechuser speechtimeFigure 2: Behavior of the dialogue system along a timelinewith an interface robot with an LED attached toits chest (see Figure 1).
Participants?
utteranceswere recognized by an automatic speech recog-nizer Julius2, and interpreted by an in-house lan-guage understander.
The robot?s utterances werevoiced by a commercial speech synthesizer.
TheLCD monitor in Figure 1 was used only to showreservation details at last.Julius output a recognition result to the systemat 400 msec after an input speech signal ended, butthe system awaited the next input for a fixed inter-val (wait interval, whose length is given as an ex-perimental factor).
If the system received an addi-tional input, it awaited the next input for the sameinterval again.
Otherwise, the system replied.The LED started blinking at 1/30 sec even-intervals when a speech signal was detected andstopped when the system started replying.
Thebasic function of the blinking light expression issimilar to hourglass icons used in GUIs.
A bigdifference is that basically GUIs can ignore any in-put while they are showing those icons, but SDSsmust accept successive speech while it is blink-ing an LED.
What we intend to do is to suppressonly collision-eliciting speech such as repetitions(we call them follow-ups) which are negligiblebut difficult to be automatically distinguished frombarge-ins.
Barge-ins are not negligible.Conditions and participants Two experimen-tal factors were set-up, that is, the reply speedfactor (moderate or slow reply speed) and theblinking light factor (with or without a blinkinglight), resulting in four conditions:A: slow reply speed, with a blinking light,B: slow reply speed, without a blinking light,C: moderate reply speed, with a blinking light,D: moderate reply speed, without a blinking light.We randomly assigned 48 Japanese participants2http://julius.sourceforge.jp/(mean age 30.9) to one of the four conditions.A reply speed depends on a wait interval forwhich the dialogue system awaits the next input.Shiwa et al (2008) showed that the best replyspeed for a conversational robot was one second.Thus we chose 800 msec as the wait interval forthe moderate reply speed because an actual replyspeed was the accumulation of the wait intervaland a delay for processing a user request, and 800msec is simply twice the default length (the VADtail margin) by which the Julius speech recognizerrecognizes the end of a speech.
For the slow replyspeed, we chose 4 sec as the wait interval.
Waitintervals include the VAD tail margin.Figure 2 shows how the system and the LEDwork along with user speech.
In this figure, a userutters a continuous speech with a rather long pausethat is longer than the VAD tail margin but shorterthan the wait interval.
If the system detects theend of the user?s turn and starts speaking withinthe interval marked with an ?X?, a speech collisionwould occur.
If the user utters a follow-up withinthe interval marked with a ?Y?, a speech collisionwould occur, too.
We try to suppress the formerspeech collision by decelerating dialogues and thelatter by using a blinking light as an ASE.Method The experiment was conducted in aroom for one participant at one time.
Participantsentered the room and sat on a chair in front of adesk as shown in Figure 1.The experimenter gave the participants instruc-tions so as to reserve hotel rooms five times bytalking with the robot in front of them.
All of themwere given the same five tasks which require themto reserve several rooms (one to three) at the sametime.
The meaning of the blinking light expres-sion was not explained to them.
After giving theinstructions, the experimenter left the participants,and they began tasks when the robot started to talkto them.
Each task was limited to up to three min-utes.
After finishing the tasks, the participants an-178swered a questionnaire.
Figure 5 and Figure 6 inthe appendix show one of the five task instructions,and a dialogue on that task, respectively.2.2 ResultsReply speeds Averages of observed replyspeeds were calculated from the timestamps intranscripts.
They were 4.53 sec for the slow con-ditions and 1.42 sec for the moderate conditions.Task completion The average number of com-pleted tasks in the four conditions A, B, C, and Dwere 4.00, 3.83, 3.83, and 4.33, respectively.
AnANOVA did not find any significant difference.Speech collisions We counted speech collisionsfor which the SDS was responsible, that is, thecases where the robot spoke while participantswere talking (i.e., end-of-turn misdetections).
Ofcourse, there were speech collisions for which par-ticipants were responsible, that is, the cases whereparticipants intentionally spoke while the robotwas talking (i.e., barge-ins).
These speech colli-sions were not the targets, hence they were not in-cluded in the counts.Speech collisions due to participants?
back-channel feedbacks were not included, either.
Wethink that it is possible to filter out such feedbackbecause feedback utterances are usually very shortand variations are small.
On the other hand, aswe mentioned above, it is not easy to automat-ically distinguish negligible speech such as rep-etitions from barge-ins.
We want to suppressonly such speech negligible but hard to distinguishfrom other not negligible speech.The number of observed speech collisions inthe four conditions A, B, C, and D were 5, 11,45, and 30, respectively.
First we performed anANOVA on the number of collisions.
The interac-tion effect was not significant (p = 0.24).
A sig-nificant difference on the reply speed factor wasfound (p < 0.005).
This result confirms that de-celerating dialogues reduces collisions.
The ef-fect of the blinking light factor was not significant(p = 0.60).Next we performed a Fisher?s exact test (one-side) on the number of participants who hadspeech collisions between the two conditions ofthe slow reply speed (3 out of 12 for A and 8 outof 12 for B).
The test found a significant difference(p < 0.05).
This result indicates that the blinkinglight can reduce speech collisions by suppressingusers?
follow-ups in decelerated dialogues.Impression on the dialogue and robot The par-ticipants rated 38 positive-negative adjective pairs(such as smooth vs. rough) for evaluating both thedialogue and the robot.
The ratings are based on aseven-point Likert scale.An ANOVA found a positive marginal signifi-cance (p = 0.07) for the blinking light in the com-fortableness factor extracted by a factor analysisfor the impression on the dialogue.
In addition,an ANOVA found a positive marginal significance(p = 0.07) for the slow reply speed in the mod-esty factor extracted by a factor analysis for theimpression on the robot.
Surprisingly, no signifi-cant negative effect for the slow reply speed wasfound.System evaluations The participants evaluatedthe SDS in two measures on a scale from 1 to 7,that is, the convenience of the system and theirwillingness to use the system.
The greater theevaluation value is, the higher the degree of con-venience or willingness.The average scores of convenience in the fourconditions A, B, C, and D were 3.50, 3.17, 3.17,and 3.92, respectively.
Those of willingness were3.58, 2.58, 2.83, and 3.42, respectively.
ANOVAsdid not find any significant difference among thefour conditions both for the two measures.Discussion on user preference The analysis ofthe questionnaire suggests that the blinking lightexpression gives users a comfortable impressionon the dialogue.
The analysis also suggests thatthe slow reply speed gives users a modest impres-sion on the interface robot.
Meanwhile, no neg-ative impression with a statistical significance isfound on the slow reply speed.Although no statistically significant differenceis found between the four conditions, numbersof completed tasks and convenience are stronglycorrelated.
However, users?
willingness to usethe systems, which is the most important mea-sure for systems, is inverted between conditionA and D. Convenience will be primarily domi-nated by what degree a user?s purpose (reservingrooms) is achieved, thus, it is reasonable that con-venience scores correlate with the number of com-pleted tasks.
On the other hand, willingness willbe dominated by not only practical usefulness butalso overall usability or experience.
Therefore,we can interpret that the improvements in impres-sions and reduction in aversive speech collisions179let condition A have the highest score for willing-ness.
These results indicate that decelerating spo-ken dialogues is not a bad idea in contradictionto the common design policy in human-computerinterfaces (HCIs), and they suggest to exploit mer-its provided by decelerating dialogues rather thanpursuing quickly responding humanlike systems.Our finding contradicts not only the com-mon design policy in HCIs but also the de-sign policy in human-robot interaction found byShiwa et al (2008), that is, the best response tim-ing of a communication robot is at one second.
Wethink this contradiction is superficial and is ascrib-able to the following four major differences be-tween their study and our study.?
They adopted a within-subjects experimentaldesign while we adopted a between-subjectsdesign.
A within-subjects design makes sub-jects do relative evaluations and tends to em-phasis differences.?
Their question was specific in terms of re-sponse timing.
Our questions were overallratings of the system such as convenience.?
They assumed a perfect machine (Wizard-of-Oz experiment).
Our system was elaboratelycrafted but still far from perfect.?
Our system quickly returns non-verbal re-sponses even if verbal responses are delayed.From these differences, we hypothesize that re-sponse timing has no significant impact on the us-ability of SDSs in an absolute and holistic contextat least in the current state of the art spoken dia-logue technology, even though users prefer a sys-tem which responds quickly to a system which re-sponds slowly when they compare them with eachother directly, given an explicit comparison metricon response timing with perfect machines.3 Study 2: Uncovering Comfortablenessof Response Timing under DifferentInteraction PatternsOur conclusion in Section 2 is that SDSs do notneed to quickly respond verbally as long as theyquickly respond non-verbally by showing their in-ternal states with an ASE, while many researcherstry to make them verbally respond as fast as pos-sible.
Decelerating a dialogue has many practicaladvantages as stated above.However, through the experiment, we have alsosuspected that this conclusion is not valid in somespecific cases.
That is, we think in some situa-tions users feel uncomfortable with slow verbal re-sponses primordially, and those situations are suchas when users simply reply to systems?
yes-no-questions or greetings.
Our hypothesis is that usersexpect quick verbal responses (and hate slow ver-bal responses) only when users expect that it is notdifficult for systems to understand their responsesor to decide next actions.
This section reports theexperiment validating this hypothesis.3.1 ExperimentTo validate the hypothesis described above, weconducted a Wizard-of-Oz experiment using fixedscenarios.
Participants engaged in short interac-tions with an interface robot and evaluated re-sponse timing of the robot.
Three experimentalfactors were interaction patterns, response timing(wait interval), and existence of a blinking light.Interaction patterns Five interaction patternswere setup to see the differences between situa-tions.
Each pattern consisted of three utterances.The first utterance was from the system.
Upon re-ceiving the utterance, a participant as a user of thesystem replied with the second utterance.
Thenthe system responded after the given wait interval(1 sec or 4 sec) with the third utterance.
Partic-ipants evaluated this interval between the secondutterance and the third utterance in a measure ofcomfortableness.The patterns with scenarios are shown in Fig-ure 3.
They will be referred to by abbreviations(PGG, QYQ, QNQ, PSQ, PLQ) in what follows.Note that the scenarios are originally in Japanese.Here, RequestS and RequestL mean a short re-quest and a long request, respectively.
YNQues-tion and WhQuestion mean a yes-no-question anda wh-question, respectively.
According to the hy-pothesis, we can predict that the reported com-fortableness for the longer wait interval (4 sec)are worse for short and formulaic cases such asPGG and QYQ than for the long request case (i.e.,PLQ).
In addition, we can predict that the reportedcomfortableness for longer intervals improves forPLQ if the robot?s light blinks, while that does notimprove for PGG and QYQ.System We used the same interface robot andthe LCD monitor as study 1.
The experiment inthis study, however, was conducted using a WOZsystem.180Prompt-Greeting-Greeting (PGG)S: Welcome to our Hotel.
May I help you?U: Hello.S: Hello.YNQuestion-Yes-WhQuestion (QYQ)S: Welcome to our Hotel.
Will you stay tonight?U: Yes.S: Can I ask your name?YNQuestion-No-WhQuestion (QNQ)S: Welcome to our Hotel.
Will you stay tonight?U: No.S: How may I help you?Prompt-RequestS-WhQuestion (PSQ)S: Welcome to our Hotel.
May I help you?U: I would like to reserve a room from tomorrow.S: How long will you stay?Prompt-RequestL-WhQuestion (PLQ)S: Welcome to our Hotel.
May I help you?U: I would like to reserve rooms with breakfast from to-morrow, one single room and one double room, non-smoking and smoking, respectively.S: How long will you stay?Figure 3: Interaction patterns and scenariosFirst the WOZ system presents an instruction tothe participant on the LCD monitor, which revealsthe robot?s first utterance of the given scenario(e.g., ?Welcome to our Hotel.
May I help you??
)and indicates the participant?s second utterance(e.g., ?Hello.?).
Two seconds after the participantclicks the OK button on the monitor with a com-puter mouse, the system makes the robot utter thefirst utterance.
Then, the participant replies, andthe operator of the system end-points the end ofparticipant?s speech by clicking a button shown inanother monitor for the operator in the room nextto the participant?s room.
After the end-pointing,the system waits for the wait interval (one secondor four seconds) and makes the robot utter the thirdutterance of the scenario.
One second after, thesystem asks the participant to evaluate the com-fortableness of the response timing of the robot?sthird utterance on a scale from 1 to 7 (1:very un-comfortable, 4:neutral, 7:very comfortable) on theLCD monitor.Conditions and participants Forty participants(mean age 28.8, 20 males and 20 females) engagedin the experiment.
No participant had engaged instudy 1.
They were randomly assigned to one oftwo groups (gender was balanced).
The groupscorrespond to one of two levels of the experi-mental factor of the existence of a blinking light.For one group, the robot blinked its LED when itwas waiting.
For the other group, the robot didnot blink the LED.
We refer to the former group(condition) as BL (Blinking Light, n=20) and thelater as NL (No Light, n=20).
In summary, thisexperiment is within-subjects design with regardto interaction patterns and response timing and isbetween-subjects design with regard to the blink-ing light.Method The experiment was conducted in aroom for one participant at one time.
Participantsentered the room and sat on a chair in front of adesk as shown in Figure 1, but they did not wearheadphones this time.The experimenter gave the participants instruc-tions so as to engage in short dialogues with therobot in front of them.
They engaged in each offive scenarios shown in Figure 3 six times (threetimes with a 1 sec wait interval and three with4 sec), resulting in 30 dialogues (5?
3?
2 = 30).The order of scenarios and intervals was random-ized.
The existence and meaning of the blinkinglight expression was not explained to them.
Theywere not told that the systemwas operated by a hu-man operator, either.
After giving the instructions,the experimenter left the participants, and theypracticed one time.
This practice used a Prompt-RequestM-WhQuestion3 type scenario with a waitinterval of two seconds.
Then, thirty dialogueswere performed.
Short breaks were inserted af-ter ten dialogues.
Each dialogue proceeded as ex-plained above.3.2 ResultsEnd-pointing errors End-pointing was done bya fixed operator.
We obtained 1,184 dialogues outof 1,200 (= 30 ?
40) after removing dialoguesin which end-pointing failed (failures were self-reported by the operator).
We sampled 30 dia-logues from the 1,184 dialogues and analyzed end-pointing errors in the recorded speech data.
Theaverage error was 84.6 msec (SD=89.6).Comfortableness This experiment was de-signed to grasp a preliminary sense on ourhypothesis as much as possible with a limitednumber of participants in exchange for aban-donment of use of statistical tests, because thisstudy involved multiple factors and the interactionpattern factor was complex by itself.
Therefore,in the following discussion on comfortableness,we do not refer to statistical significances.3The request utterance is longer than that of RequestS andshorter than that of RequestL.181!"#$"%&'()*+,,,+!
,+!!"#$"%&'()*+,,,+!
,+!Figure 4: Comfortableness (Left: without a blinking light (NL), right: with a blinking light (BL))Figure 4 shows regression lines obtained fromthe 1,184 dialogues in the two graphs for NL andBL (Detailed values are shown in Table 1).
TheX axes in the graphs correspond to response tim-ing, that is, the two wait intervals of 1 sec and4 sec.
The Y axes correspond to comfortablenessreported in a scale from 1 to 7.
Obviously, with orwithout a blinking light effected comfortableness.The results shown in the graphs support the pre-dictions made in Section 3.1.
The scores of PGGand QYQ are worse than that of PLQ at 4 sec.PGG and QYQ show no difference between NLand BL.
QNQ and PSQ show differences.
PLQshows the biggest difference.
In case of PLQ, thereported comfortableness at 4 sec shifted to al-most the neutral position (score 4) by presenting ablinking light.
This indicates that a blinking lightASE can allay the debasement of impression dueto slow responses only in non-formulaic cases.Interestingly, the blinking light expression at-tracted comfortableness scores to neutral both at1 sec and at 4 sec.
We can make two hypotheseson this result.
One is that the blinking light expres-sion has a negative effect which degrades comfort-ableness at 1 sec.
The other is that the blinkinglight expression makes participants difficult to seedifferences between 1 sec and 4 sec, therefore, re-ported scores converge to neutral.
At this stage wethink that the later is more probable than the for-mer because the scores of PGG and QYQ shouldbe degraded at 1 sec if the former is true.4 A Practical Design Guideline for SDSsSummarizing the results of the experiments pre-sented in Section 2 and Section 3, we suggest atwofold design guideline for SDSs, especially fortask-oriented systems.
Some interaction-orientedsystems such as chatting systems are out of scopeof this guideline.
In what follows, first the guide-line is presented and then a commentary on theguideline is described.The guideline(1) Never be obsessed with quick turn-takingbut acknowledge users immediatelyQuick turn-taking will not recompense your ef-forts, resources inputted, etc.
Pursue it only af-ter accomplishing all you can do without compro-mising performance in other elements of dialoguesystems and only if it does not make system devel-opment and maintenance harder.
However, quick(possibly non-verbal) acknowledgment is a requi-site.
You can compensate for the debasement ofuser experience due to slow verbal responses justby using an ASE such as a tiny blinking LED toacknowledge user speech.
No instruction aboutthe ASE is needed for users.
(2) Think of users?
expectationsUsers expect rather quick verbal responses to theirgreetings and yes-answers.
ASEs will be ineffec-tive for them.
Thus it is recommended to enableyour systems to quickly respond verbally to suchutterances.
Fortunately it is easy to anticipate suchutterances.
Greetings usually occur only at the be-ginning of dialogues or after tasks were accom-plished.
Yes-answers will come only after yes-no-questions.
Therefore it will be able to implementan SDS that quickly responds verbally to greetingand yes-answers both without increasing develop-ment / maintenance costs and without decreasing182recognition performance, etc.However, you should keep in mind that tooquick verbal responses (0 sec interval or overlap-ping) may not be welcomed (Hirasawa et al, 1999;Shiwa et al, 2008).
They may also induce toomuch expectancy in users and result in disappoint-ments to your systems after some interactions.Commentary on the guidelineThe guideline was constructed so as to avoid thefive problems pointed out in Section 1.
The firstpoint of the guideline is induced mainly from theresults of study 1, and the second point is inducedmainly from the results of study 2.Although the results of study 2 indicate usersprefer quick responses to slow ones as presup-posed in past literature, note that the experimentin study 2 is within-subjects design with regard tothe response timing factor and that within-subjectsdesign tends to emphasis differences as discussedat the end of Section 2.
The results of study 1suggested that such an emphasized difference (i.e.,preference for quick responses) has no significantimpact on the usability of SDSs on the whole.5 ConclusionThis paper proposed a non-humanlike spoken di-alogue design, which consists of two elements:non-humanlike turn-taking and acknowledgment.Two experimental studies were reported regardingthese two elements.
The first study showed that theproposed non-humanlike spoken dialogue designis effective for reducing speech collisions.
Thisstudy also presented pieces of evidence that showquick humanlike turn-taking is less important inspoken dialogue system (SDS) design.
The secondstudy showed a tendency supporting a hypothesisfound in the first study that user preference on re-sponse timing varies depending on interaction pat-terns in terms of comfortableness.
Upon receivingthese results, a practical design guideline for SDSswas suggested, that is, (1) never be obsessed withquick turn-taking but acknowledge users immedi-ately and (2) think of users?
expectations.Our non-humanlike acknowledging method us-ing an LED-based artificial subtle expression(ASE) can apply to any interfaces on wearable /handheld devices, vehicles, whatever.
It is, how-ever, difficult to directly apply it to call-centers(i.e., telephone interfaces), which occupy a bigportion of the deployed SDSs pie.
Yet, the un-derlying concept: decelerated dialogues accom-panied by an ASE will be applicable even to tele-phone interfaces by using an auditory ASE, whichis to be explored in future work.The guideline is supported by findings in arather hypothetical stage.
More experiments arenecessary to confirm these findings.
In addition,the guideline is for the current transitory periodin which intelligence technologies such as auto-matic recognition, language processing, reasoningetc.
are below human level.
In that sense, the con-tribution of this paper might be limited.
However,this period will last until a decisive paradigm shiftoccurs in intelligence technologies.
It may comeafter a year, a decade, or a century.ReferencesC.
Bartneck, T. Kanda, H. Ishiguro, and N. Hagita.2007.
Is the uncanny valley an uncanny cliff?
InProc.
RO-MAN 2007.H.
Clark.
1996.
Using Language.
Cambridge U. P.R.
Ferna?ndez, D. Schlangen, and T. Lucht.
2007.Push-to-talk ain?t always bad!
comparing differentinteractivity settings in task-oriented dialogue.
InProc.
DECALOG 2007.K.
Funakoshi, K. Kobayashi, M. Nakano, T. Komatsu,and S. Yamada.
2010.
Reducing speech collisionsby using an artificial subtle expression in a deceler-ated spoken dialogue.
In Proc.
2nd Intl.
Symp.
NewFrontiers in Human-Robot Interaction.J.
Hirasawa, M. Nakano, T. Kawabata, and K. Aikawa.1999.
Effects of system barge-in responses on userimpressions.
In Proc.
EUROSPEECH?99.N.
Kitaoka, M. Takeuchi, R. Nishimura, and S. Nak-agawa.
2005.
Response timing detection us-ing prosodic and linguistic information for human-friendly spoken dialog systems.
Journal of TheJapanese Society for AI, 20(3).T.
Komatsu and S. Yamada.
2010.
Effects of adapta-tion gap on user?s variation of impressions of artifi-cial agents.
In Proc.
WMSCI 2010.T.
Komatsu, S. Yamada, K. Kobayashi, K. Funakoshi,and M. Nakano.
2010.
Artificial subtle expressions:Intuitive notification methodology of artifacts.
InProc.
CHI 2010.A.
Raux and M. Eskenazi.
2009.
A finite-state turn-taking model for spoken dialog systems.
In Proc.NAACL-HLT 2009.T.
Shiwa, T. Kanda, M. Imai, H. Ishiguro, andN.
Hagita.
2008.
How quickly should communi-cation robots respond?
In Proc.
HRI 2008.183Hotel Reservation Task 3Reserve rooms as belowStayRoomTwin, 1 room, non-smokingDouble, 1 room, non-smokingAs specified with the orange-colored frameon the calendarFigure 5: One of the five task instructions used in study 1S: Welcome to Hotel Wakamatsu-Kawada.
May I help you?U: I want to stay from March 10th to 11th.S: What kind of room would you like?U: One non-smoking twin room and one non-smoking double room.S: Are your reservation details correctly shown on the screen?U: Yes.
No problem.S: Your reservation has been accepted.
Thank you for using us.Figure 6: A successful dialogue observed with the task shown in Figure 5 (translated into English)Table 1: Detailed comfortableness scores in study 2Interaction pattern PGG QYQ QNQ PSQ PLQCondition NL BL NL BL NL BL NL BL NL BL1 secmean 5.34 5.36 5.55 5.56 5.48 5.25 5.09 4.73 5.13 4.41s.d.
1.00 1.17 1.10 1.00 1.02 1.04 1.12 1.09 1.14 1.20p-value 0.93 0.96 0.23 0.09 0.0014 secmean 3.12 3.16 3.37 3.36 3.28 3.52 3.43 3.52 3.54 3.83s.d.
0.94 1.04 0.78 0.93 0.76 0.93 0.81 0.87 0.95 0.87p-value 0.83 0.98 0.14 0.59 0.08p-values were obtained by two-sided t-tests between NL and BL.
Those are shown just for reference.184
