Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1210?1219,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsProbabilistic Soft Logic for Semantic Textual SimilarityIslam Beltagy?Katrin Erk?Raymond Mooney?
?Department of Computer Science?Department of LinguisticsThe University of Texas at AustinAustin, Texas 78712?
{beltagy,mooney}@cs.utexas.edu?katrin.erk@mail.utexas.eduAbstractProbabilistic Soft Logic (PSL) is a re-cently developed framework for proba-bilistic logic.
We use PSL to combinelogical and distributional representationsof natural-language meaning, where distri-butional information is represented in theform of weighted inference rules.
We ap-ply this framework to the task of Seman-tic Textual Similarity (STS) (i.e.
judg-ing the semantic similarity of natural-language sentences), and show that PSLgives improved results compared to a pre-vious approach based on Markov LogicNetworks (MLNs) and a purely distribu-tional approach.1 IntroductionWhen will people say that two sentences are sim-ilar?
This question is at the heart of the SemanticTextual Similarity task (STS)(Agirre et al, 2012).Certainly, if two sentences contain many of thesame words, or many similar words, that is a goodindication of sentence similarity.
But that can bemisleading.
A better characterization would be tosay that if two sentences use the same or similarwords in the same or similar relations, then thosetwo sentences will be judged similar.1Interest-ingly, this characterization echoes the principle ofcompositionality, which states that the meaning ofa phrase is uniquely determined by the meaning ofits parts and the rules that connect those parts.Beltagy et al (2013) proposed a hybrid ap-proach to sentence similarity: They use a very1Mitchell and Lapata (2008) give an amusing example oftwo sentences that consist of all the same words, but are verydifferent in their meaning: (a) It was not the sales managerwho hit the bottle that day, but the office worker with theserious drinking problem.
(b) That day the office manager,who was drinking, hit the problem sales worker with a bottle,but it was not serious.deep representation of sentence meaning, ex-pressed in first-order logic, to capture sentencestructure, but combine it with distributional sim-ilarity ratings at the word and phrase level.
Sen-tence similarity is then modelled as mutual entail-ment in a probabilistic logic.
This approach is in-teresting in that it uses a very deep and preciserepresentation of meaning, which can then be re-laxed in a controlled fashion using distributionalsimilarity.
But the approach faces large hurdlesin practice, stemming from efficiency issues withthe Markov Logic Networks (MLN) (Richardsonand Domingos, 2006) that they use for performingprobabilistic logical inference.In this paper, we use the same combined logic-based and distributional framework as Beltagy etal., (2013) but replace Markov Logic Networkswith Probabilistic Soft Logic (PSL) (Kimmig etal., 2012; Bach et al, 2013).
PSL is a proba-bilistic logic framework designed to have efficientinference.
Inference in MLNs is theoretically in-tractable in the general case, and existing approxi-mate inference algorithms are computationally ex-pensive and sometimes inaccurate.
Consequently,the MLN approach of Beltagy et al (2013) wasunable to scale to long sentences and was onlytested on the relatively short sentences in the Mi-crosoft video description corpus used for STS(Agirre et al, 2012).
On the other hand, inferencein PSL reduces to a linear programming problem,which is theoretically and practically much moreefficient.
Empirical results on a range of prob-lems have confirmed that inference in PSL is muchmore efficient than in MLNs, and frequently moreaccurate (Kimmig et al, 2012; Bach et al, 2013).We show how to use PSL for STS, and describechanges to the PSL framework that make it moreeffective for STS.
For evaluation, we test on threeSTS datasets, and compare our PSL system withthe MLN approach of Beltagy et al, (2013) andwith distributional-only baselines.
Experimental1210results demonstrate that, overall, PSL models hu-man similarity judgements more accurately thanthese alternative approaches, and is significantlyfaster than MLNs.The rest of the paper is organized as follows:section 2 presents relevant background material,section 3 explains how we adapted PSL for theSTS task, section 4 presents the evaluation, andsections 5 and 6 discuss future work and conclu-sions, respectively.2 Background2.1 Logical SemanticsLogic-based representations of meaning have along tradition (Montague, 1970; Kamp and Reyle,1993).
They handle many complex semantic phe-nomena such as relational propositions, logicaloperators, and quantifiers; however, their binarynature prevents them from capturing the ?graded?aspects of meaning in language.
Also, it is difficultto construct formal ontologies of properties and re-lations that have broad coverage, and semanticallyparsing sentences into logical expressions utilizingsuch an ontology is very difficult.
Consequently,current semantic parsers are mostly restricted toquite limited domains, such as querying a specificdatabase (Kwiatkowski et al, 2013; Berant et al,2013).
In contrast, our system is not limited to anyformal ontology and can use a wide-coverage toolfor semantic analysis, as discussed below.2.2 Distributional SemanticsDistributional models (Turney and Pantel, 2010),on the other hand, use statistics on contextualdata from large corpora to predict semantic sim-ilarity of words and phrases (Landauer and Du-mais, 1997; Mitchell and Lapata, 2010).
They arerelatively easier to build than logical representa-tions, automatically acquire knowledge from ?bigdata,?
and capture the ?graded?
nature of linguis-tic meaning, but do not adequately capture logicalstructure (Grefenstette, 2013).Distributional models are motivated by the ob-servation that semantically similar words occur insimilar contexts, so words can be represented asvectors in high dimensional spaces generated fromthe contexts in which they occur (Landauer andDumais, 1997; Lund and Burgess, 1996).
Suchmodels have also been extended to compute vec-tor representations for larger phrases, e.g.
byadding the vectors for the individual words (Lan-dauer and Dumais, 1997) or by a component-wiseproduct of word vectors (Mitchell and Lapata,2008; Mitchell and Lapata, 2010), or more com-plex methods that compute phrase vectors fromword vectors and tensors (Baroni and Zamparelli,2010; Grefenstette and Sadrzadeh, 2011).
We usevector addition (Landauer and Dumais, 1997), andcomponent-wise product (Mitchell and Lapata,2008) as baselines for STS.
Vector addition waspreviously found to be the best performing sim-ple distributional method for STS (Beltagy et al,2013).2.3 Markov Logic NetworksMarkov Logic Networks (MLN) (Richardson andDomingos, 2006) are a framework for probabilis-tic logic that employ weighted formulas in first-order logic to compactly encode complex undi-rected probabilistic graphical models (i.e., Markovnetworks).
Weighting the rules is a way of soft-ening them compared to hard logical constraintsand thereby allowing situations in which not allclauses are satisfied.
MLNs define a probabilitydistribution over possible worlds, where a world?sprobability increases exponentially with the to-tal weight of the logical clauses that it satisfies.A variety of inference methods for MLNs havebeen developed, however, developing a scalable,general-purpose, accurate inference method forcomplex MLNs is an open problem.
Beltagy etal.
(2013) use MLNs to represent the meaning ofnatural language sentences and judge textual en-tailment and semantic similarity, but they were un-able to scale the approach beyond short sentencesdue to the complexity of MLN inference.2.4 Probabilistic Soft LogicProbabilistic Soft Logic (PSL) is a recently pro-posed alternative framework for probabilistic logic(Kimmig et al, 2012; Bach et al, 2013).
It useslogical representations to compactly define largegraphical models with continuous variables, andincludes methods for performing efficient proba-bilistic inference for the resulting models.
A keydistinguishing feature of PSL is that ground atomshave soft, continuous truth values in the interval[0, 1] rather than binary truth values as used inMLNs and most other probabilistic logics.
Givena set of weighted logical formulas, PSL builds agraphical model defining a probability distributionover the continuous space of values of the randomvariables in the model.1211A PSL model is defined using a set of weightedif-then rules in first-order logic, as in the followingexample:?x, y, z. friend(x, y) ?
votesFor(y, z)?votesFor(x, z) | 0.3 (1)?x, y, z. spouse(x, y) ?
votesFor(y, z)?votesFor(x, z) | 0.8 (2)In our notation, we use lower case letters likex, y, z to represent variables and upper case let-ters for constants.
The first rule states that a per-son is likely to vote for the same person as his/herfriend.
The second rule encodes the same regular-ity for a person?s spouse.
The weights encode theknowledge that a spouse?s influence is greater thana friend?s in this regard.In addition, PSL includes similarity functions.Similarity functions take two strings or two sets asinput and return a truth value in the interval [0, 1]denoting the similarity of the inputs.
For example,in our application, we generate inference rules thatincorporate the similarity of two predicates.
Thiscan be represented in PSL as:?x.
similarity(?predicate1?, ?predicate2?)
?predicate1(x)?
predicate2(x)As mentioned above, each ground atom, a,has a soft truth value in the interval [0, 1],which is denoted by I(a).
To compute soft truthvalues for logical formulas, Lukasiewicz?s re-laxation of conjunctions(?
), disjunctions(?)
andnegations(?)
are used:I(l1?
l1) = max{0, I(l1) + I(l2)?
1}I(l1?
l1) = min{I(l1) + I(l2), 1}I(?l1) = 1?
I(l1)Then, a given rule r ?
rbody?
rhead, is said to besatisfied (i.e.
I(r) = 1) iff I(rbody) ?
I(rhead).Otherwise, PSL defines a distance to satisfactiond(r) which captures how far a rule r is from beingsatisfied: d(r) = max{0, I(rbody) ?
I(rhead)}.For example, assume we have the set of evidence:I(spouse(B,A)) = 1, I(votesFor(A,P )) =0.9, I(votesFor(B,P )) = 0.3, and that ris the resulting ground instance of rule (2).Then I(spouse(B,A) ?
votesFor(A,P )) =max{0, 1 + 0.9 ?
1} = 0.9, and d(r) =max{0, 0.9?
0.3} = 0.6.Using distance to satisfaction, PSL defines aprobability distribution over all possible interpre-tations I of all ground atoms.
The pdf is definedas follows:p(I) =1Zexp [?
?r?R?r(d(r))p]; (3)Z =?Iexp [?
?r?R?r(d(r))p]where Z is the normalization constant, ?ris theweight of rule r, R is the set of all rules, and p ?
{1, 2} provides two different loss functions.
Forour application, we always use p = 1PSL is primarily designed to support MPE in-ference (Most Probable Explanation).
MPE infer-ence is the task of finding the overall interpretationwith the maximum probability given a set of evi-dence.
Intuitively, the interpretation with the high-est probability is the interpretation with the lowestdistance to satisfaction.
In other words, it is theinterpretation that tries to satisfy all rules as muchas possible.
Formally, from equation 3, the mostprobable interpretation, is the one that minimizes?r?R?r(d(r))p. In case of p = 1, and giventhat all d(r) are linear equations, then minimizingthe sum requires solving a linear program, which,compared to inference in other probabilistic logicssuch as MLNs, can be done relatively efficientlyusing well-established techniques.
In case p = 2,MPE inference can be shown to be a second-ordercone program (SOCP) (Kimmig et al, 2012).2.5 Semantic Textual SimilaritySemantic Textual Similarity (STS) is the task ofjudging the similarity of a pair of sentences ona scale from 0 to 5, and was recently introducedas a SemEval task (Agirre et al, 2012).
Goldstandard scores are averaged over multiple hu-man annotations and systems are evaluated usingthe Pearson correlation between a system?s out-put and gold standard scores.
The best perform-ing system in 2012?s competition was by B?ar etal.
(2012), a complex ensemble system that inte-grates many techniques including string similarity,n-gram overlap, WordNet similarity, vector spacesimilarity and MT evaluation metrics.
Two of thedatasets we use for evaluation are from the 2012competition.
We did not utilize the new datasetsadded in the 2013 competition since they did notcontain naturally-occurring, full sentences, whichis the focus of our work.12122.6 Combining logical and distributionalmethods using probabilistic logicThere are a few recent attempts to combine log-ical and distributional representations in order toobtain the advantages of both.
Lewis and Steed-man (2013) use distributional information to deter-mine word senses, but still produce a strictly log-ical semantic representation that does not addressthe ?graded?
nature of linguistic meaning that isimportant to measuring semantic similarity.Garrette et al (2011) introduced a frameworkfor combining logic and distributional models us-ing probabilistic logic.
Distributional similaritybetween pairs of words is converted into weightedinference rules that are added to the logical repre-sentation, and Markov Logic Networks are used toperform probabilistic logical inference.Beltagy et al (2013) extended this frameworkby generating distributional inference rules fromphrase similarity and tailoring the system to theSTS task.
STS is treated as computing the prob-ability of two textual entailments T |= H andH |= T , where T and H are the two sentenceswhose similarity is being judged.
These two en-tailment probabilities are averaged to produce ameasure of similarity.
The MLN constructed todetermine the probability of a given entailmentincludes the logical forms for both T and H aswell as soft inference rules that are constructedfrom distributional information.
Given a similar-ity score for all pairs of sentences in the dataset,a regressor is trained on the training set to mapthe system?s output to the gold standard scores.The trained regressor is applied to the scores inthe test set before calculating Pearson correlation.The regression algorithm used is Additive Regres-sion (Friedman, 2002).To determine an entailment probability, first,the two sentences are mapped to logical repre-sentations using Boxer (Bos, 2008), a tool forwide-coverage semantic analysis that maps a CCG(Combinatory Categorial Grammar) parse into alexically-based logical form.
Boxer uses C&C forCCG parsing (Clark and Curran, 2004).Distributional semantic knowledge is then en-coded as weighted inference rules in the MLN.A rule?s weight (w) is a function of the cosinesimilarity (sim) between its antecedent and con-sequent.
Rules are generated on the fly for eachT and H .
Let t and h be the lists of all wordsand phrases in T and H respectively.
For allpairs (a, b), where a ?
t, b ?
h, it generatesan inference rule: a ?
b | w, where w =f(sim(?
?a ,?
?b )).
Both a and b can be words orphrases.
Phrases are defined in terms of Boxer?soutput.
A phrase is more than one unary atomsharing the same variable like ?a little kid?
whichin logic is little(K) ?
kid(K).
A phrase also canbe two unary atoms connected by a relation like?a man is driving?
which in logic is man(M) ?agent(D,M) ?
drive(D).
The similarity func-tion sim takes two vectors as input.
Phrasal vec-tors are constructed using Vector Addition (Lan-dauer and Dumais, 1997).
The set of generatedinference rules can be regarded as the knowledgebase KB.Beltagy et al (2013) found that the logical con-junction in H is very restrictive for the STS task,so they relaxed the conjunction by using an aver-age evidence combiner (Natarajan et al, 2010).The average combiner results in computationallycomplex inference and only works for short sen-tences.
In case inference breaks or times-out, theyback off to a simpler combiner that leads to muchfaster inference but loses most of the structure ofthe sentence and is therefore less accurate.Given T , KB and H from the previoussteps, MLN inference is then used to computep(H|T,KB), which is then used as a measure ofthe degree to which T entails H .3 PSL for STSFor several reasons, we believe PSL is a more ap-propriate probabilistic logic for STS than MLNs.First, it is explicitly designed to support efficientinference, therefore it scales better to longer sen-tences with more complex logical forms.
Sec-ond, it was also specifically designed for com-puting similarity between complex structured ob-jects rather than determining probabilistic logicalentailment.
In fact, the initial version of PSL(Broecheler et al, 2010) was called Probabilis-tic Similarity Logic, based on its use of similar-ity functions.
This initial version was shown to bevery effective for measuring the similarity of noisydatabase records and performing record linkage(i.e.
identifying database entries referring to thesame entity, such as bibliographic citations refer-ring to the same paper).
Therefore, we have devel-oped an approach that follows that of Beltagy etal.
(2013), but replaces Markov Logic with PSL.This section explains how we formulate the STS1213task as a PSL program.
PSL does not work verywell ?out of the box?
for STS, mainly becauseLukasiewicz?s equation for the conjunction is veryrestrictive.
Therefore, we use a different interpre-tation for conjunction that uses averaging, whichrequires corresponding changes to the optimiza-tion problem and the grounding technique.3.1 RepresentationGiven the logical forms for a pair of sentences,a text T and a hypothesis H , and given a set ofweighted rules derived from the distributional se-mantics (as explained in section 2.6) composingthe knowledge base KB, we build a PSL modelthat supports determining the truth value of H inthe most probable interpretation (i.e.
MPE) givenT and KB.Consider the pair of sentences is ?A man is driv-ing?, and ?A guy is walking?.
Parsing into logicalform gives:T : ?x, y. man(x) ?
agent(y, x) ?
drive(y)H : ?x, y. guy(x) ?
agent(y, x) ?
walk(y)The PSL program is constructed as follows:T : The text is represented in the evidence set.
Forthe example, after Skolemizing the existentialquantifiers, this contains the ground atoms:{man(A), agent(B,A), drive(B)}KB: The knowledge base is a set of lexical andphrasal rules generated from distributionalsemantics, along with a similarity score foreach rule (section 2.6).
For the exam-ple, we generate the rules: ?x.
man(x) ?vs sim(?man?, ?guy?)?
guy(x) ,?x.drive(x)?vs sim(?drive?, ?walk?
)?walk(x)where vs sim is a similarity function thatcalculates the distributional similarity scorebetween the two lexical predicates.
All rulesare assigned the same weight because allrules are equally important.H: The hypothesis is represented as H ?result(), and then PSL is queried for thetruth value of the atom result().
Forour example, the rule is: ?x, y. guy(x) ?agent(y, x) ?
walk(y)?
result().Priors: A low prior is given to all predicates.
Thisencourages the truth values of ground atomsto be zero, unless there is evidence to the con-trary.For each STS pair of sentences S1, S2, we runPSL twice, once where T = S1, H = S2andanother where T = S2, H = S1, and output thetwo scores.
To produce a final similarity score, wetrain a regressor to learn the mapping between thetwo PSL scores and the overall similarity score.As in Beltagy et al, (2013) we use Additive Re-gression (Friedman, 2002).3.2 Changing ConjunctionAs mentioned above, Lukasiewicz?s formula forconjunction is very restrictive and does not workwell for STS.
For example, for T: ?A man is driv-ing?
and H: ?A man is driving a car?, if we use thestandard PSL formula for conjunction, the outputvalue is zero because there is no evidence for a carand max(0, X + 0 ?
1) = 0 for any truth value0 ?
X ?
1.
However, humans find these sen-tences to be quite similar.Therefore, we introduce a new averaging inter-pretation of conjunction that we use for the hy-pothesis H .
The truth value for a conjunctionis defined as I(p1?
.... ?
pn) =1n?ni=1I(pi).This averaging function is linear, and the result isa valid truth value in the interval [0, 1], thereforethis change is easily incorporated into PSL with-out changing the complexity of inference whichremains a linear-programming problem.It would perhaps be even better to use aweighted average, where weights for differentcomponents are learned from a supervised train-ing set.
This is an important direction for futurework.3.3 Grounding ProcessGrounding is the process of instantiating the vari-ables in the quantified rules with concrete con-stants in order to construct the nodes and links inthe final graphical model.
In principle, ground-ing requires instantiating each rule in all possibleways, substituting every possible constant for eachvariable in the rule.
However, this is a combinato-rial process that can easily result in an explosion inthe size of the final network.
Therefore, PSL em-ploys a ?lazy?
approach to grounding that avoidsthe construction of irrelevant groundings.
If thereis no evidence for one of the antecedents in a par-ticular grounding of a rule, then the normal PSLformula for conjunction guarantees that the rule is1214Algorithm 1 Heuristic GroundingInput: rbody= a1?
....?an: antecedent of a rulewith average interpretation of conjunctionInput: V : set of variables used in rbodyInput: Ant(vi): subset of antecedents ajcon-taining variable viInput: Const(vi): list of possible constants ofvariable viInput: Gnd(ai): set of ground atoms of ai.Input: GndConst(a, g, v): takes an atom a,grounding g for a, and variable v, and returnsthe constant that substitutes v in gInput: gnd limit: limit on the number ofgroundings1: for all vi?
V do2: for all C ?
Const(vi) do3: score(C) =?a?Ant(vi)(max I(g))for g ?
Gnd(a) ?GndConst(a, g, vi) = C4: end for5: sort Const(vi) on scores, descending6: end for7: return For all vi?
V , take the Cartesian-product of the sortedConst(vi) and return thetop gnd limit resultstrivially satisfied (I(r) = 1) since the truth valueof the antecedent is zero.
Therefore, its distance tosatisfaction is also zero, and it can be omitted fromthe ground network without impacting the result ofMPE inference.However, this technique does not work oncewe switch to using averaging to interpret conjunc-tions.
For example, given the rule ?x.
p(x) ?q(x) ?
t() and only one piece of evidence p(C)there are no relevant groundings because there isno evidence for q(C), and therefore, for normalPSL, I(p(C) ?
q(C)) = 0 which does not affectI(t()).
However, when using averaging with thesame evidence, we need to generate the groundingp(C)?q(C) because I(p(C)?q(C)) = 0.5 whichdoes affect I(t()).One way to solve this problem is to eliminatelazy grounding and generate all possible ground-ings.
However, this produces an intractably largenetwork.
Therefore, we developed a heuristic ap-proximate grounding technique that generates asubset of the most impactful groundings.Pseudocode for this heuristic approach is shownin algorithm 1.
Its goal is to find constants thatparticipate in ground propositions with high truthvalue and preferentially use them to construct alimited number of groundings of each rule.The algorithm takes the antecedents of a ruleemploying averaging conjunction as input.
It alsotakes the grounding limit which is a threshold onthe number of groundings to be returned.
The al-gorithm uses several subroutines, they are:?
Ant(vi): given a variable vi, it returns the setof rule antecedent atoms containing vi.
E.g,for the rule: a(x) ?
b(y) ?
c(x), Ant(x) re-turns the set of atoms {a(x), c(x)}.?
Const(vi): given a variable vi, it returns thelist of possible constants that can be used toinstantiate the variable vi.?
Gnd(ai): given an atom ai, it returns the setof all possible ground atoms generated for ai.?
GndConst(a, g, v): given an atom a andgrounding g for a, and a variable v, it findsthe constant that substitutes for v in g. E.g,assume there is an atom a = ai(v1, v2), andthe ground atom g = ai(A,B) is one of itsgroundings.
GndConst(a, g, v2) would re-turn the constant B since it is the substitutionfor the variable v2in g.Lines 1-6 loop over all variables in the rule.
Foreach variable, lines 2-5 construct a list of constantsfor that variable and sort it based on a heuristicscore.
In line 3, each constant is assigned a scorethat indicates the importance of this constant interms of its impact on the truth value of the overallgrounding.
A constant?s score is the sum, over allantedents that contain the variable in question, ofthe maximum truth value of any grounding of thatantecedent that contains that constant.Pushing constants with high scores to the topof each variable?s list will tend to make the over-all truth value of the top groundings high.
Line7 computes a subset of the Cartesian product ofthe sorted lists of constants, selecting constants inranked order and limiting the number of results tothe grounding limit.One point that needs to be clarified about thisapproach is how it relies on the truth values ofground atoms when the goal of inference is to ac-tually find these values.
PSL?s inference is ac-tually an iterative process where in each itera-tion a grounding phase is followed by an opti-mization phase (solving the linear program).
Thisloop repeats until convergence, i.e.
until the truth1215values stop changing.
The truth values used ineach grounding phase come from the previous op-timization phase.
The first grounding phase as-sumes only the propositions in the evidence pro-vided have non-zero truth values.4 EvaluationThis section evaluates the performance of PSL onthe STS task.4.1 DatasetsWe evaluate our system on three STS datasets.?
msr-vid: Microsoft Video Paraphrase Cor-pus from STS 2012.
The dataset consistsof 1,500 pairs of short video descriptionscollected using crowdsourcing (Chen andDolan, 2011) and subsequently annotated forthe STS task (Agirre et al, 2012).
Half ofthe dataset is for training, and the second halfis for testing.?
msr-par: Microsoft Paraphrase Corpus fromSTS 2012 task.
The dataset is 5,801pairs of sentences collected from newssources (Dolan et al, 2004).
Then, for STS2012, 1,500 pairs were selected and anno-tated with similarity scores.
Half of thedataset is for training, and the second half isfor testing.?
SICK: Sentences Involving CompositionalKnowledge is a dataset collected for SemEval2014.
Only the training set is available at thispoint, which consists of 5,000 pairs of sen-tences.
Pairs are annotated for RTE and STS,but we only use the STS data.
Training andtesting was done using 10-fold cross valida-tion.4.2 Systems ComparedWe compare our PSL system with several others.In all cases, we use the distributional word vec-tors employed by Beltagy et al (2013) based oncontext windows from Gigaword.?
vec-add: Vector Addition (Landauer andDumais, 1997).
We compute a vector rep-resentation for each sentence by adding thedistributional vectors of all of its words andmeasure similarity using cosine.
This is asimple yet powerful baseline that uses onlydistributional information.?
vec-mul: Component-wise Vector Multipli-cation (Mitchell and Lapata, 2008).
Thesame as vec-add except uses component-wise multiplication to combine word vectors.?
MLN: The system of Beltagy et al (2013),which uses Markov logic instead of PSL forprobabilistic inference.
MLN inference isvery slow in some cases, so we use a 10minute timeout.
When MLN times out, itbacks off to a simpler sentence representationas explained in section 2.6.?
PSL: Our proposed PSL system for combin-ing logical and distributional information.?
PSL-no-DIR: Our PSL system without dis-tributional inference rules(empty knowledgebase).
This system uses PSL to compute sim-ilarity of logical forms but does not use dis-tributional information on lexical or phrasalsimilarity.
It tests the impact of the proba-bilistic logic only?
PSL+vec-add: PSL ensembled with vec-add.
Ensembling the MLN approach with apurely distributional approach was found toimprove results (Beltagy et al, 2013), so wealso tried this with PSL.
The methods are en-sembled by using both entailment scores ofboth systems as input features to the regres-sion step that learns to map entailment scoresto STS similarity ratings.
This way, the train-ing data is used to learn how to weight thecontribution of the different components.?
PSL+MLN: PSL ensembled with MLN inthe same manner.4.3 ExperimentsSystems are evaluated on two metrics, Pearsoncorrelation and average CPU time per pair of sen-tences.?
Pearson correlation: The Pearson correlationbetween the system?s similarity scores andthe human gold-standards.?
CPU time: This metric only applies to MLNand PSL.
The CPU time taken by the infer-ence step is recorded and averaged over allpairs in each of the test datasets.
In manycases, MLN inference is very slow, so wetimeout after 10 minutes and report the num-ber of timed-out pairs on each dataset.1216msr-vid msr-par SICKvec-add 0.78 0.24 0.65vec-mul 0.76 0.12 0.62MLN 0.63 0.16 0.47PSL-no-DIR 0.74 0.46 0.68PSL 0.79 0.53 0.70PSL+vec-add 0.83 0.49 0.71PSL+MLN 0.79 0.51 0.70Best Score (B?aret al, 2012)0.87 0.68 n/aTable 1: STS Pearson CorrelationsPSL MLNtime time timeouts/totalmsr-vid 8s 1m 31s 132/1500msr-par 30s 11m 49s 1457/1500SICK 10s 4m 24s 1791/5000Table 2: Average CPU time per STS pair, andnumber of timed-out pairs in MLN with a 10minute time limit.
PSL?s grounding limit is set to10,000 groundings.We also evaluated the effect of changing thegrounding limit on both Pearson correlation andCPU time for the msr-par dataset.
Most of thesentences in msr-par are long, which results islarge number of groundings, and limiting the num-ber of groundings has a visible effect on the over-all performance.
In the other two datasets, thesentences are fairly short, and the full number ofgroundings is not large; therefore, changing thegrounding limit does not significantly affect the re-sults.4.4 Results and DiscussionTable 1 shows the results for Pearson correlation.PSL out-performs the purely distributional base-lines (vec-add and vec-mul) because PSL is ableto combine the information available to vec-addand vec-mul in a better way that takes sentencestructure into account.
PSL also outperformsthe unaided probabilistic-logic baseline that doesnot use distributional information (PSL-no-DIR).PSL-no-DIR works fairly well because there issignificant overlap in the exact words and struc-ture of the paired sentences in the test data, andPSL combines the evidence from these similari-ties effectively.
In addition, PSL always does sig-nificantly better than MLN, because of the largeFigure 1: Effect of PSL?s grounding limit on thecorrelation score for the msr-par datasetnumber of timeouts, and because the conjunction-averaging in PSL is combining evidence bet-ter than MLN?s average-combiner, whose perfor-mance is sensitive to various parameters.
Theseresults further support the claim that using prob-abilistic logic to integrate logical and distribu-tional information is a promising approach tonatural-language semantics.
More specifically,they strongly indicate that PSL is a more effectiveprobabilistic logic for judging semantic similaritythan MLNs.Like for MLNs (Beltagy et al, 2013), en-sembling PSL with vector addition improved thescores a bit, except for msr-par where vec-add?sperformance is particularly low.
However, this en-semble still does not beat the state of the art (B?ar etal., 2012) which is a large ensemble of many dif-ferent systems.
It would be informative to add oursystem to their ensemble to see if it could improveit even further.Table 2 shows the CPU time for PSL and MLN.The results clearly demonstrate that PSL is an or-der of magnitude faster than MLN.Figures 1 and 2 show the effect of changing thegrounding limit on Pearson correlation and CPUtime.
As expected, as the grounding limit is in-creased, accuracy improves but CPU time alsoincreases.
However, note that the difference inscores between the smallest and largest groundinglimit tested is not large, suggesting that the heuris-tic approach to limiting grounding is quite effec-tive.5 Future WorkAs mentioned in Section 3.2, it would be goodto use a weighted average to compute the truth1217Figure 2: Effect of PSL?s grounding limit on CPUtime for the msr-par datasetvalues for conjunctions, weighting some predi-cates more than others rather than treating themall equally.
Appropriate weights for different com-ponents could be learned from training data.
Forexample, such an approach could learn that thetype of an object determined by a noun should beweighted more than a property specified by an ad-jective.
As a result, ?black dog?
would be appro-priately judged more similar to ?white dog?
thanto ?black cat.
?One of the advantages of using a probabilis-tic logic is that additional sources of knowledgecan easily be incorporated by adding additionalsoft inference rules.
To complement the soft in-ference rules capturing distributional lexical andphrasal similarities, PSL rules could be added thatencode explicit paraphrase rules, such as thosemined from monolingual text (Berant et al, 2011)or multi-lingual parallel text (Ganitkevitch et al,2013).This paper has focused on STS; however, asshown by Beltagy et al (2013), probabilistic logicis also an effective approach to recognizing tex-tual entailment (RTE).
By using the appropriatefunctions to combine truth values for various log-ical connectives, PSL could also be adapted forRTE.
Although we have shown that PSL outper-forms MLNs on STS, we hypothesize that MLNsmay still be a better approach for RTE.
However, itwould be good to experimentally confirm this in-tuition.
In any case, the high computational com-plexity of MLN inference could mean that PSL isstill a more practical choice for RTE.6 ConclusionThis paper has presented an approach that usesProbabilistic Soft Logic (PSL) to determine Se-mantic Textual Similarity (STS).
The approachuses PSL to effectively combine logical seman-tic representations of sentences with soft infer-ence rules for lexical and phrasal similarities com-puted from distributional information.
The ap-proach builds upon a previous method that usesMarkov Logic (MLNs) for STS, but replaces theprobabilistic logic with PSL in order to improvethe efficiency and accuracy of probabilistic infer-ence.
The PSL approach was experimentally eval-uated on three STS datasets and was shown to out-perform purely distributional baselines as well asthe MLN approach.
The PSL approach was alsoshown to be much more scalable and efficient thanusing MLNsAcknowledgmentsThis research was supported by the DARPA DEFTprogram under AFRL grant FA8750-13-2-0026.Any opinions, findings, and conclusions or recom-mendations expressed in this material are those ofthe author and do not necessarily reflect the viewof DARPA, DoD or the US government.
Some ex-periments were run on the Mastodon Cluster sup-ported by NSF Grant EIA-0303609.ReferencesEneko Agirre, Daniel Cer, Mona Diab, and AitorGonzalez-Agirre.
2012.
Semeval-2012 task 6: Apilot on semantic textual similarity.
In Proceedingsof Semantic Evaluation (SemEval-12).Stephen H. Bach, Bert Huang, Ben London, and LiseGetoor.
2013.
Hinge-loss Markov random fields:Convex inference for structured prediction.
In Pro-ceedings of Uncertainty in Artificial Intelligence(UAI-13).Daniel B?ar, Chris Biemann, Iryna Gurevych, andTorsten Zesch.
2012.
UKP: Computing seman-tic textual similarity by combining multiple contentsimilarity measures.
In Proceedings of SemanticEvaluation (SemEval-12).Marco Baroni and Roberto Zamparelli.
2010.
Nounsare vectors, adjectives are matrices: Representingadjective-noun constructions in semantic space.
InProceedings of Conference on Empirical Methods inNatural Language Processing (EMNLP-10).Islam Beltagy, Cuong Chau, Gemma Boleda, Dan Gar-rette, Katrin Erk, and Raymond Mooney.
2013.1218Montague meets Markov: Deep semantics withprobabilistic logical form.
In Proceedings of theSecond Joint Conference on Lexical and Computa-tional Semantics (*SEM-13).Jonathan Berant, Ido Dagan, and Jacob Goldberger.2011.
Global learning of typed entailment rules.
InProceedings of Association for Computational Lin-guistics (ACL-11).Jonathan Berant, Andrew Chou, Roy Frostig, and PercyLiang.
2013.
Semantic parsing on Freebase fromquestion-answer pairs.
In Proceedings of Confer-ence on Empirical Methods in Natural LanguageProcessing (EMNLP-13).Johan Bos.
2008.
Wide-coverage semantic analysiswith Boxer.
In Proceedings of Semantics in TextProcessing (STEP-08).Matthias Broecheler, Lilyana Mihalkova, and LiseGetoor.
2010.
Probabilistic Similarity Logic.
InProceedings of Uncertainty in Artificial Intelligence(UAI-20).David L. Chen and William B. Dolan.
2011.
Collect-ing highly parallel data for paraphrase evaluation.
InProceedings of Association for Computational Lin-guistics (ACL-11).Stephen Clark and James R. Curran.
2004.
Parsingthe WSJ using CCG and log-linear models.
In Pro-ceedings of Association for Computational Linguis-tics (ACL-04).Bill Dolan, Chris Quirk, and Chris Brockett.
2004.Unsupervised construction of large paraphrase cor-pora: Exploiting massively parallel news sources.In Proceedings of the International Conference onComputational Linguistics (COLING-04).Jerome H Friedman.
2002.
Stochastic gradient boost-ing.
Journal of Computational Statistics & DataAnalysis (CSDA-02).Juri Ganitkevitch, Benjamin Van Durme, and ChrisCallison-Burch.
2013.
PPDB: The paraphrasedatabase.
In Proceedings of North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies (NAACL-HLT-13).Dan Garrette, Katrin Erk, and Raymond Mooney.2011.
Integrating logical representations with prob-abilistic information using Markov logic.
In Pro-ceedings of International Conference on Computa-tional Semantics (IWCS-11).Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011.Experimental support for a categorical composi-tional distributional model of meaning.
In Proceed-ings of Conference on Empirical Methods in NaturalLanguage Processing (EMNLP-11).Edward Grefenstette.
2013.
Towards a formal distri-butional semantics: Simulating logical calculi withtensors.
In Proceedings of Second Joint Conferenceon Lexical and Computational Semantics (*SEM2013).Hans Kamp and Uwe Reyle.
1993.
From Discourse toLogic.
Kluwer.Angelika Kimmig, Stephen H. Bach, MatthiasBroecheler, Bert Huang, and Lise Getoor.
2012.A short introduction to Probabilistic Soft Logic.In Proceedings of NIPS Workshop on ProbabilisticProgramming: Foundations and Applications (NIPSWorkshop-12).Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and LukeZettlemoyer.
2013.
Scaling semantic parsers withon-the-fly ontology matching.
In Proceedings ofConference on Empirical Methods in Natural Lan-guage Processing (EMNLP-13).T.
K. Landauer and S. T. Dumais.
1997.
A solution toPlato?s problem: The Latent Semantic Analysis the-ory of the acquisition, induction, and representationof knowledge.
Psychological Review.Mike Lewis and Mark Steedman.
2013.
Combineddistributional and logical semantics.
Transactionsof the Association for Computational Linguistics(TACL-13).Kevin Lund and Curt Burgess.
1996.
Producinghigh-dimensional semantic spaces from lexical co-occurrence.
Behavior Research Methods, Instru-ments, and Computers.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
In Proceedingsof Association for Computational Linguistics (ACL-08).Jeff Mitchell and Mirella Lapata.
2010.
Compositionin distributional models of semantics.
Journal ofCognitive Science.Richard Montague.
1970.
Universal grammar.
Theo-ria, 36:373?398.Sriraam Natarajan, Tushar Khot, Daniel Lowd, PrasadTadepalli, Kristian Kersting, and Jude Shavlik.2010.
Exploiting causal independence in Markovlogic networks: Combining undirected and directedmodels.
In Proceedings of European Conference inMachine Learning (ECML-10).Matthew Richardson and Pedro Domingos.
2006.Markov logic networks.
Machine Learning,62:107?136.Peter Turney and Patrick Pantel.
2010.
From fre-quency to meaning: Vector space models of se-mantics.
Journal of Artificial Intelligence Research(JAIR-10).1219
