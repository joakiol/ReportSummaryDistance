Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 1402?1413, Dublin, Ireland, August 23-29 2014.Deep-Syntactic ParsingMiguel Ballesteros1, Bernd Bohnet2, Simon Mille1, Leo Wanner1,31Natural Language Processing Group, Pompeu Fabra University, Barcelona, Spain2School of Computer Science, University of Birmingham, United Kingdom3Catalan Institute for Research and Advanced Studies (ICREA)1,3{name.lastname}@upf.edu2bohnetb@cs.bham.ac.ukAbstract?Deep-syntactic?
dependency structures that capture the argumentative, attributive and coordi-native relations between full words of a sentence have a great potential for a number of NLP-applications.
The abstraction degree of these structures is in-between the output of a syntacticdependency parser (connected trees defined over all words of a sentence and language-specificgrammatical functions) and the output of a semantic parser (forests of trees defined over indi-vidual lexemes or phrasal chunks and abstract semantic role labels which capture the argumentstructure of predicative elements, dropping all attributive and coordinative dependencies).
Wepropose a parser that delivers deep syntactic structures as output.1 IntroductionSurface-syntactic structures (SSyntSs) as produced by data-driven syntactic dependency parsers are perforce idiosyncratic in that they contain governed prepositions, determiners, support verb constructionsand language-specific grammatical functions such as, e.g., SBJ, OBJ, PRD, PMOD, etc.
(Johansson andNugues, 2007).
For many NLP-applications, including machine translation, paraphrasing, text simpli-fication, etc., such a high idiosyncrasy is obstructive because of the recurrent divergence between thesource and the target structures.
Therefore, the use of more abstract ?syntactico-semantic?
structuresseems more appropriate.
Following Mel?
?cuk (1988), we call these structures deep-syntactic structures(DSyntSs).
DSyntSs are situated between SSyntSs and PropBank- (Palmer et al., 2005) or SemanticFrame-like structures (Fillmore et al., 2002).
Compared to SSyntSs, they have the advantage to ab-stract from language-specific grammatical idiosyncrasies.
Compared to PropBank and Semantic Framestuctures, they have the advantage to be connected and complete, i.e., capture all argumentative, attribu-tive and coordinative dependencies between the meaningful lexical items of a sentence, while PropBankand Semantic Frame structures are not always connected, may contain either individual lexical items orphrasal chunks as nodes, and discard attributive and coordinative relations (be they within the chunks orsentential).
In other words, they constitute incomplete structures that drop not only idiosyncratic, func-tional but also meaningful elements of a given sentence and often contain dependencies between chunksrather than individual tokens.
Therefore, we propose to put on the research agenda the task of deep-syntactic parsing and show how a DSyntS is obtained from a SSynt dependency parse using data-driventree transduction in a pipeline with a syntactic parser.1In Section 2, we introduce SSyntSs and DSyntSsand discuss the fundamentals of SSyntS?DSyntS transduction.
Section 3 describes the experiments thatwe carried out on Spanish material, and Section 4 discusses their outcome.
Section 5 summarizes therelated work, before in Section 6 some conclusions and plans for future work are presented.2 Fundamentals of SSyntS?DSyntS transductionBefore we set out to discuss the principles of the SSyntS?DSynt transduction, we must specify theDSyntSs and SSyntSs as used in our experiments.This work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/1The term ?tree transduction?
is used in this paper in the sense of Rounds (1970) and Thatcher (1970) to denote an extensionof finite state transduction (Aho, 1972) to trees.14022.1 Defining SSyntS and DSyntSSSyntSs and DSyntSs are directed, node- and edge-labeled dependency trees with standard feature-valuestructures (Kasper and Rounds, 1986) as node labels and dependency relations as edge labels.The features of the node labels in SSyntSs are lexssynt, and ?syntactic grammemes?
of the value oflexssynt, i.e., number, gender, case, definiteness, person for nouns and tense, aspect, mood and voice forverbs.
The value of lexssyntcan be any (either full or functional) lexical item; in graphical representationsof SSyntSs, usually only the value of lexssyntis shown.
The edge labels of a SSyntS are grammaticalfunctions ?subj?, ?dobj?, ?det?, ?modif?, etc.
In other words, SSyntSs are syntactic structures of the kindas encountered in the standard dependency treebanks; cf., e.g., dependency version of the Penn TreeBank(Johansson and Nugues, 2007) for English, Prague Dependency Treebank for Czech (Haji?c et al., 2006),Ancora for Spanish (Taul?e et al., 2008), Copenhagen Dependency Treebank for Danish (Buch-Kromann,2003), etc.
In formal terms that we need for the outline of the transduction below, a SSyntS is defined asfollows:Definition 1 (SSyntS) An SSyntS of a language L is a quintuple TSS= ?N,A, ?ls?n, ?rs?a, ?n?g?defined over all lexical items L of L, the set of syntactic grammemes Gsynt, and the set of grammaticalfunctions Rgr, where?
the set N of nodes and the set A of directed arcs form a connected tree,?
?ls?nassigns to each n ?
N an ls?
L,?
?rs?aassigns to each a ?
A an r ?
Rgr, and?
?n?gassigns to each ?ls?n(n) a set of grammemes Gt?
Gsynt.The features of the node labels in DSyntSs as worked with in this paper are lexdsyntand ?seman-tic grammemes?
of the value of lexdsynt, i.e., number and determination for nouns and tense, aspect,mood and voice for verbs.2In contrast to lexssyntin SSyntS, DSyntS?s lexdsyntcan be any full, butnot a functional lexeme.
In accordance with this restriction, in the case of look after a person, AFTERwill not appear in the corresponding DSyntS; it is a functional (or governed) preposition (so are TO orBY, in Figure 1).3In contrast, AFTER in leave after the meeting is a full lexeme; it will remain in theDSyntS because there it has its own meaning of ?succession in time?.
The edge labels of a DSyntS arelanguage-independent ?deep-syntactic?
relations I,.
.
.
,VI, ATTR, COORD, APPEND.
?I?,.
.
.
,?VI?
areargument relations, analogous to A0, A1, etc.
in the PropBank annotation.
?ATTR?
subsumes all (cir-cumstantial) ARGM-x PropBank relations as well as the modifier relations not captured by the PropBankand FrameNet annotations.
?COORD?
is the coordinative relation as in: John-COORD?and-II?Mary,publish-COORD?or-II?perish, and so on.
APPEND subsumes all parentheticals, interjections, directaddresses, etc., as, e.g., in Listen, John!
: listen-APPEND?John.
DSyntSs thus show a strong similaritywith PropBank structures, with four important differences: (i) their lexical labels are not disambiguated;(ii) instead of circumstantial thematic roles of the kind ARGM-LOC, ARGM-DIR, etc.
they use a uniqueATTR relation; (iii) they capture all existing dependencies between meaningful lexical nodes; and (iv)they are connected.4A number of other annotations have resemblance with DSyntSs; cf.
(Ivanova et al.,2012) for an overview of deep dependency structures.
Formally, a DSyntS is defined as follows:Definition 2 (DSyntS) An DSyntS of a language L is a quintuple TDS= ?N,A, ?ls?n, ?rs?a, ?n?g?defined over the full lexical items Ldof L, the set of semantic grammemes Gsem, and the set of deep-syntactic relations Rdsynt, where?
the set N of nodes and the set A of directed arcs form a connected tree,?
?ls?nassigns to each n ?
N an ls?
Ld,?
?rs?aassigns to each a ?
A an r ?
Rdsynt, and?
?n?gassigns to each ?ls?n(n) a set of grammemes Gt?
Gsem.Consider in Figure 1 an example for an SSyntS and its corresponding DSyntS.2Most of the grammemes have a semantic and a surface interpretation; see (Mel?
?cuk, 2013).3Functional lexemes also include auxiliaries (e.g.
HAVE, or BE when it is not a copula), and definite and indefinite deter-miners (THE, A); see Figure 1).4Our DSyntSs are thus DSyntSs as used in the Meaning-Text Theory (Mel?
?cuk, 1988), only that our DSyntSs do notdisambiguate lexical items and do not use lexical functions (Mel?
?cuk, 1996).1403(a) almost 1.2 million jobs have been created by the state thanks to their endeavoursrestrquantquantsubjanalyt perfanalyt passagentadvpreposdetobl objpreposdet(b) almost 1.2 million job create state thanks their endeavourATTRATTRATTRIIIATTRIIIFigure 1: An SSyntS (a) and its corresponding DSyntS (b)2.2 Fleshing out the SSyntS?DSyntS transductionIt is clear that the SSyntS and DSyntS of the same sentence are not isomorphic.
The following corre-spondences between the SSyntS Sssand DSyntS Sdsof a sentence need to be taken into account duringSSyntS?DSyntS transduction:(i) a node in Sssis a node in Sds;(ii) a relation in Ssscorresponds to a relation in Sds;(iii) a fragment of the Ssstree corresponds to a single node in Sds;(iv) a relation with a dependent node in Sssis a grammeme in Sds;(v) a grammeme in Sssis a grammeme in Sds;(vi) a node in Sssis conflated with another node in Sds; and(vii) a node in Sdshas no correspondence in Sss.The grammeme correspondences (iv) and (v) and the ?pseudo?
correspondences in (vi) and (vii)5arefew or idiosyncratic and are best handled in a rule-based post-processing stage.
The main task of theSSyntS?DSyntS transducer is thus to cope with the correspondences (i)?(iii).
For this purpose, we canview both SSyntS and DSyntS as vectors indexed in terms of two-dimensional matrices I = N ?N (Nbeing the set of nodes of a given tree 1, .
.
.
,m), with I(i, j) = ?
(ni, nj), if ni, nj?
N and (ni, nj) ?
Aand I(i, j) = 0 otherwise (where ??
(ni, nj)?
is the function that assigns to an edge a relation label andi, j = 1, .
.
.
,m; i 6= j are nodes of the tree).
That is, for a given SSyntS, the matrix I(i, j) contains inthe cells (i, j), i, j = 1, .
.
.
,m, the names of the SSynt-relations between the nodes niand nj, and ?0?otherwise, while for a given DSyntS, the cells of its matrix IDcontain DSyntS-relations.Starting from the matrix ISof a given SSyntS, the task is therefore to obtain the matrix IDof thecorresponding DSyntS, that is, to identify correspondences between i/j, (i, j) and groups of (i, j) ofISwith i?/j?and (i?, j?)
of ID; see (i)?
(iii) above.
In other words, the task consists in identifying andremoving all functional lexemes, and attach correctly the remaining nodes between them.6As a ?token chain?surface-syntactic tree?
projection, this task can be viewed as a classification task.However, while the former is isomorphic, we know that the SSyntS?DSyntS projection is not.
In orderto approach the task to an isomorphic projection (and thus simplify its modelling), it is convenient tointerpret SSyntS and the targeted DSyntS as collections of hypernodes:Definition 3 (Hypernode) Given a SSyntS Sswith its index matrix IS(a DSyntS Sdwith its index matrixID), a node partition p (with |p |?
1) of IS(ID) is a hypernode hsi(hdi) iff p corresponds to a partitionp?
(with |p?|?
1) of Sd(Ss).In this way, the SSyntS?DSyntS correspondence boils down to a correspondence between individualhypernodes and between individual arcs, and the transduction embraces the following three (classifica-tion) subtasks: 1.
Hypernode identification, 2.
DSynt tree construction, and 3.
DSynt arc labeling, whichare completed by a post-processing stage.5(vi) covers, e.g., reflexive verb particles such as se in Spanish, which are conflated in the DSyntS with the verb:se?aux refl dir-conocer vs. CONOCERSE ?know each other?
; (vii) covers, e.g., the zero subject in pro-drop languages (whichis absent in the SSyntS and present in the DSyntS).6What is particularly challenging is the identification of functional prepositions: based on the information found in thecorpus only, our system must decide if a given preposition is a full or a functional lexeme.
That is, we do not resort to anyexternal lexical resources.14041.
Hypernode identification.
The hypernode identification consists of a binary classification of thenodes of a given SSyntS as nodes that form a hypernode of cardinality 1 (i.e., nodes that have a one-to-one correspondence to a node in the DSyntS) vs. nodes that form part of a hypernode of cardinality> 1.
In practice, hypernodes of type one will be formed by: 1) noun nodes that do not govern determineror functional preposition nodes, 2) full verb nodes that are not governed by any auxiliary verb nodesand that do not govern any functional preposition node, adjective nodes, adverbial nodes, and semanticpreposition nodes.
Hypernodes of type two will be formed by: 1) noun nodes + determiner / func-tional preposition nodes they govern, 2) verb nodes + auxiliary nodes they are governed by + functionalpreposition nodes they govern.2.
DSynt tree reconstruction.
The outcome of the hypernode identification stage is thus the set Hs=Hs|p|=1?Hs|p|>1of hypernodes of two types.
With this set at hand, we can define an isomorphy function?
: Hs?
Hd|p|=1(with hd?
Hd|p|=1consisting of nd?
Nds, i.e., the set of nodes of the target DSyntS).?
is the identity function for hs?
Hs|p|=1.
For hs?
Hs|p|>1, ?
maps the functional nodes in hsontogrammemes (attribute-value pairs) of the lexically meaningful node in hdand identifies the lexicallymeaningful node as head.
Some of the dependencies of the obtained nodes nd?
Ndscan be recoveredfrom the dependencies of their sources.
Due to the projection of functional nodes to grammemes (whichcan be also seen as node removal), some dependencies will be also missing and must be introduced.Algorithm 1 recalculates the dependencies for the target DSyntS Sd, starting from the index matrix ISofSSyntS Ssto obtain a connected tree.Algorithm 1: DSyntS tree reconstructionfor ?ni?
Nddoif ?nj: (nj, ni) ?
Ss?
?
(nj) ?
Ndthen(nj, ni)?
Sd// the equivalent of the head node of niis included in DSyntSelse if ?nj, na: (nj, ni) ?
Ss?
?
(nj) 6?
Nd??
(na) ?
Ndthen//nais the first ancestor of njthat has an equivalent in DSyntS//the equivalent of the head node of niis not included in DSyntS, but the ancestor nais(na, ni)?
Sdelse//the equivalent of the head node of niis not included in DSyntS, but several ancestors of it arenb:= BestHead(ni, Ss, Sd)(nb, ni)?
SdendforBestHead recursively ascends Ssfrom a given node niuntil it encounters one or several head nodesnd?
Nds.
In case of several encountered head nodes, the one which governs the highest frequencydependency is returned.3.
Label Classification.
The tree reconstruction stage produces a ?hybrid?
connected dependency treeSs?dwith DSynt nodes Nds, and arcs Aslabelled by SSynt relation labels, i.e., an index matrix wecan denote as I?, whose cells (i, j) contain SSynt labels for all ni, nj?
Nds: (ni, nj) ?
Asand?0?
otherwise.
The next and last stage of SSynt-to-DSyntS transduction is thus the projection of SSyntrelation labels of Ss?dto their corresponding DSynt labels, or, in other words, the mapping of I?to IDof the target DSyntS.4.
Postprocessing.
As mentioned in Section 2, there is a limited number of idiosyncratic correspon-dences between elements of SSyntS and DSyntS (the correspondences (iv?vii) which can be straight-forwardly handled by a rule-based postprocessor because (a) they are non-ambiguous, i.e., a ?
b, c ?d ?
a = b ?
c = d, and (b) they are few.
Thus, only determiners and auxiliaries in SSyntS map onto agrammeme in DSyntS, both SSyntS and DSyntS count with less than a dozen grammemes, etc.3 ExperimentsIn order to validate the outlined SSyntS?DSyntS transduction and to assess its performance in combi-nation with a surface dependency parser, i.e., starting from plain sentences, we carried out a number of1405experiments in which we implemented the transducer and integrated it into a pipeline shown in Figure 2.JointPoS TaggerSSynt parserSSynt?DSyntTransducerPlainSentencesDSyntTreebankSSyntTreebankSSyntS DSynSFigure 2: Setup of a deep-syntactic parserFor our experiments, we use the AnCora-UPF SSyntS and DSyntS treebanks of Spanish (Mille etal., 2013) in CoNLL format, adjusted for our needs.
In particular, we removed from the 79-tag SSyntStreebank the semantically and information structure influenced relation tags to obtain an annotation gran-ularity closer to the ones used for previous parsing experiments (55 relation tags, see (Mille et al., 2012)).Our development set consisted of 219 sentences (3271 tokens in the DSyntS treebank and 4953 tokensin the SSyntS treebank), the training set of 3036 sentences (57665 tokens in the DSyntS treebank and86984 tokens in the SSyntS treebank), and the test set held-out for evaluation of 258 sentences (5641tokens in the DSyntS treebank and 8955 tokens in the SSyntS treebank).To obtain the SSyntS, we use Bohnet and Nivre (2012)?s transition-based parser, which combineslemmatization, PoS tagging, and syntactic dependency parsing?tuned and trained on the respective setsof the SSyntS treebank.
Cf.
Table 1 for the performance of the parser on the development set.POS LEMMA LAS UAS96.14 91.10 78.64 86.49Table 1: Results of Bohnet and Nivre?s surface-syntactic parser on the development setIn what follows, we first present the realization of the SSyntS?DSyntS transducer and then the real-ization of the baseline.3.1 SSyntS?DSyntS transducerAs outlined in Section 2.2, the SSyntS?DSyntS transducer is composed of three submodules and a post-processing stage:1.
Hypernode identification.
For the hypernode identification, we trained a binary polynomial (degree2) SVM from LIBSVM (Chang and Lin, 2001).
The SVM allows both features related to the processednode and higher-order features, which can be related to the head node of the processed node or to itssibling nodes.
After several feature selection trials, we chose the following features for each node n:?
lemma or stem of the label of n,?
label of the relation between n and its head,?
surface PoS of n?s label (the SSynt and DSyntS treebanks distinguish between surface and deepPoS),?
label of the relation between n?s head to its own head,?
surface PoS of the label of n?s head node.After an optimization round of the parameters available in the SVM implementation, the hypernodeidentification achieved over the gold development set 99.78% precision and 99.02% recall (and thus99.4% F1).
That is, only very few hypernodes are not identified correctly.
The main error source aregoverned prepositions: the classifier has to learn when to assign a preposition an own hypernode (i.e.,when it is lexically meaningful) and when it should be included into the hypernode of the governor (i.e.,when it is functional).
Our interpretation is that the features we use for this task are appropriate, butthat the training data set is too small.
As a result, some prepositions are erroneously left out from orintroduced into the DSyntS.14062.
Tree reconstruction.
The implementation of the tree reconstruction module shows an unlabelleddependency attachment precision of 98.18% and an unlabelled dependency attachment recall of 97.43%over the gold development set.
Most of the errors produced by this module have their origin in theprevious module, i.e., hypernode identification.
When a node has been incorrectly removed, the moduleerrs in the attachment because it cannot use the node in question as the destination or the origin of adependency, as it is the case in the gold-standard annotation:Gold-standard: ser como e?nebe like letter-nIIIIPredicted: ser e?neIIWhen a node has erroneously not been removed, no dependencies between its governor and its depen-dent can be established since DSyntS must remain a tree (which gives the same LAS and UAS errors aswhen a node has been erroneously removed):Gold-standard: y Michael JacksonIIPredicted: y a Michael Jacksonand to Michael JacksonIIII3.
Relation label classification.
For relation label classification, we use a multiclass linear SVM.
Thelabel classification depends on the concrete annotation schemata of the SSyntS and DSyntS treebankson which the parser is trained.
Depending on the schemata, some DSynt relation labels may be easier toderive from the original SSyntS relation labels than others.
Table 2 lists all SSynt relation labels that havea straightforward mapping to DSyntS relation labels in the used treebanks, i.e., neither their dependentnor their governor are removed, and the SSyntS label always maps to the same DSynt label.SSynt DSyntabbrev ATTRabs pred ATTRadv ATTRadv mod ATTRagent Iappos ATTRattr ATTRaux phras ?aux refl dir IISSynt DSyntaux refl indir IIIbin junct ATTRcompl1 IIcompl2 IIIcompl adnom ATTRcoord COORDcopul IIcopul clitic IIcopul quot IISSynt DSyntdobj clitic IIdobj quot IIelect ATTRjuxtapos APPENDmodal IImodif ATTRnum junct COORDobj copred ATTRprepos IISSynt DSyntprepos quot IIprolep APPENDquant ATTRquasi coord COORDquasi subj Irelat ATTRrestr ATTRsequent ATTRsubj Isubj copred ATTRTable 2: Straightforward SSynt to DSyntS mappingsTable 3 shows SSyntS relation?DSyntS relation label correspondences that are not straightforward.SSynt DepRelAMapping to DSyntanalyt fut remove Gov and Dep; add tense=FUTanalyt pass remove Gov; invert I and II; add voice=PASSanalyt perf remove Gov; add tense=PASTanalyt progr remove Gov; add tem constituency=PROGRaux refl lex remove Dep; add se at the end of Gov?s lemmaaux refl pass remove Dep; invert I and II; add voice=PASScompar remove Dep if conjunctioncompar /coord /sub conj remove Dep if governed prepositiondetIF Dep=el?un THEN remove Dep; add definiteness=DEF/INDEFIF Dep=possessive THEN DepRel ATTR?I?II?IIIIF Dep=other THEN DepRel ATTRdobj remove Dep if governed prepositioniobj remove Dep if governed preposition; DepRel II?III?IV?V?VIiobj clitic DepRel II?III?IV?V?VIobl compl remove Dep if governed preposition; DepRel I?II?III?IV?V?VIobl obj remove Dep if governed preposition; DepRel II?III?IV?V?VIpunc ?punc init ?Table 3: Complex SSynt to DSynt mappings1407The final set of features selected for label classification includes: (i) lemma of the dependent node, (ii)dependency relation to the head of the dependent node, (iii) dependency relation label of the head nodeto its own head, (iv) dependency relation to the head of the sibling nodes of the dependent node, if any.After an optimization round of the parameter set of the SVM-model, relation labelling achieved94.00% label precision and 93.28% label recall on the development set.
The recall is calculated con-sidering all the nodes that are included in the gold standard.
The error sources for relation labellingwere mostly the dependencies that involved possessives and the various types of objects (see Table3) due to their differing valency.
For instance, the relation det in su?det?coche ?his/her car?
andsu?det?llamada ?his/her phone call?
have different correspondences in DSyntS: su?ATTR?cochevs.
su?I?llamada.
That is, the DSyntS relation depends on the lexical properties of the governor.7Once again, more training data is needed in order to classify better those cases.4.
Postprocessing In the postprocessing stage for Spanish, the following rules capture non-ambiguouscorrespondences between elements of the SSynt-index matrix IS= Ns?Nsand DSyntS index matrixID= Nd?Nd, with ns?
Nsand nd?
Nd, and nsand ndcorresponding to each other (we do not listhere identity correspondences such as between the number grammemes of nsand nd):?
if nsis dependent of analyt pass or analyt refl pass relation, then the voice grammeme in ndisPASS;?
if nsis dependent of analyt progr, then the voice grammeme in ndis PROGR;?
if nsis dependent of analyt refl lex, then add the particle -SE as suffix of node label (word) of dd;?
if any of the children of nsis labelled by one of the tokens UN ?amasc?, UNA ?afem?, UNOS?somemasc?
or UNAS ?somefem?, then the definiteness grammeme in ndINDEF, otherwise it isDEF;?
if the nslabel is a finite verb and nsdoes not govern a subject relation, then add to I?the relationnd?
I?n?d, with n?dbeing a newly introduced node.3.2 BaselineAs point of reference for the evaluation of the performance of our SSyntS?DSyntS transducer, we use arule-based baseline that carries out the most direct transformations extracted from Tables 2 and 3.
Thebaseline detects hypernodes by directly removing all the nodes that we are sure need to be removed, i.e.punctuation and auxiliaries.
The nodes that are only potentially to be removed, i.e., all dependents ofDepRels that have a possibly governed preposition or conjunction in Table 3, are left in the DSyntS.
Thenew relation labels in the DSyntS are obtained by selecting the label that is most likely to substitute theSSyntS relation label according to classical grammar studies.
The rules of the rule-based baseline lookas follows:1 if (deprel==abbrev) then deep deprel=ATTR2 if (deprel==obl obj) then deep deprel=II.
.
.n if (deprel==punc) then remove(current node)4 Results and DiscussionLet us look in this section at the performance figures of the SSyntS parser, the SSyntS?DSyntS trans-ducer, and the sentence?DSyntS pipeline obtained in the experiments.4.1 SSyntS?DSyntS transducer resultsIn Table 4, the performance of the subtasks of the SSyntS?DSyntS transducer is contrasted to the per-formance of the baselines; the evaluation of the postprocessing subtask is not included because the one-to-one projection of SSyntS elements to DSyntS guarantees an accuracy of 100% of the operationsperformed.
The transducer has been applied to the gold standard test set, which is the held-out test set,with gold standard PoS tags, lemmas and dependency trees.
It outputs in total 5610 nodes; the rule-basedbaseline outputs 8653 nodes.
As mentioned in Section 3, our gold standard includes 5641 nodes.7Note that lexemes are not generalized: a verb and its corresponding noun (e.g., construct/construction) are considereddistinct lexemes.1408Hyper-Node DetectionMeasure Rule-based Baseline Tree Transducerp 64.31 (5565/8653) 99.79 (5598/5610)r 98.65 (5565/5641) 99.24 (5598/5641)F1 77.86 99.51Attachment and LabellingMeasure Rule-based Baseline Tree TransducerLAP 50.02 (4328/8653) 91.07 (5109/5610)UAP 53.05 (4590/8653) 98.32 (5516/5610)LA-P 57.66 (4989/8653) 92.37 (5182/5610)LAR 76.72 (4328/5641) 90.57 (5109/5641)UAR 81.37 (4590/5641) 97.78 (5516/5641)LA-R 88.44 (4989/5641) 91.86 (5182/5641)Table 4: Performance of the SSyntS?DSyntS transducer and of the rule-based baseline over the gold-standard held-out test set (LAP: labelled attachment precision, UAP: unlabelled attachment precision, LA-P: label assign-ment precision, LAR: labelled attachment recall, UAR: Unlabelled attachment recall and LA-R: Label assignment recall)Our data-driven SSyntS?DSyntS transducer is much better than the baseline with respect to all eval-uation measures.8The transducer relies on distributional patterns identified in the training data set, andmakes thus use of information that is not available for the rule-based baseline, which studies one nodeat a time.
However, the rule-based baseline results also show that transduction that would remove a fewnodes would provide results close to a 100% recall for the hypernode detection because a DSynt tree is asubtree of the SSynt tree (if we ignore the nodes introduced by post-processing).
This is also evidencedby the labeled and attachment recall scores.
The results of the transducer on the test and developmentsets are quite comparable.
The hypernode detection is even better on the test set.
The label accuracysuffers most from using unseen data during the development of the system.
The attachment figures areapproximately equivalent on both sets.4.2 Results of deep-syntactic parsingLet us consider now the performance of the complete DSynt parsing pipeline (PoS-tagger+surface-dependency parser?
SSyntS?DSyntS transducer) on the held-out test set.
Table 5 displays the figuresof the Bohnet and Nivre parser.
The figures are in line with the performance of state-of-the-art parsersfor Spanish (Mille et al., 2012).POS LEMMA LAS UAS96.05 92.10 81.45 88.09Table 5: Performance of Bohnet and Nivre?s joint PoS-tagger+dependency parser trained on Ancora-UPFTable 6 shows the performance of the pipeline when we feed the output of the syntactic parser to therule-based baseline SSyntS?DSyntS module and the tree transducer.
We observe a clear error propaga-tion from the dependency parser (which provides 81.45% LAS) to the SSyntS?DSyntS transducer, whichloses in tree quality more than 18%.Hyper-Node DetectionMeasure Baseline Tree Transducerp 63.87 (5528/8655) 97.07 (5391/5554)r 98.00 (5528/5641) 95.57 (5391/5641)F1 77.33 96.31Labelling and AttachmentMeasure Baseline Tree TransducerLAP 38.75 (3354/8655) 68.31 (3794/5554)UAP 44.69 (3868/8655) 77.31 (4294/5554)LA-P 49.66 (4298/8655) 80.47 (4469/5554)LAR 59.46 (3354/5641) 67.26 (3794/5641)UAR 68.57 (3868/5641) 76.12 (4294/5641)LA-R 76.19 (4298/5641) 79.22 (4469/5641)Table 6: Performance of the deep-syntactic parsing pipeline5 Related WorkTo the best of our knowledge, data-driven deep-syntactic parsing as proposed in this paper is novel.
Assemantic role labeling and frame-semantic analysis, it has the goal to obtain more semantically orientedstructures than those delivered by state-of-the-art syntactic parsing.
Semantic role labeling receivedconsiderable attention in the CoNLL shared tasks for syntactic dependency parsing in 2006 and 20078We also ran MaltParser by training it on the DSynt-treebank to parse the SSynt-test set; however, the outcome was tooweak to be used as baseline.1409(Buchholz and Marsi, 2006; Nivre et al., 2007), the CoNLL shared task for joint parsing of syntactic andsemantic dependencies in 2008 (Surdeanu et al., 2008) and the shared task in 2009 (Haji?c et al., 2009).The top ranked systems were pipelines that started with a syntactic analysis (as we do) and continuedwith predicate identification, argument identification, argument labeling, and word sense disambigua-tion; cf.
(Johansson and Nugues, 2008; Che et al., 2009).
At the end, a re-ranker that considers jointlyall arguments to select the best combination was applied.
Some of the systems were based on integratedsyntactic and semantic dependency analysis; cf., e.g., (Gesmundo et al., 2009); see also (Llu?
?s et al.,2013) for a more recent proposal along similar lines.
However, all of them lack the ability to performstructural changes?as, e.g., introduction of nodes or removal of nodes necessary to obtain a DSyntS.Klime?s (2006)?s parser removes nodes (producing tectogrammatical structures as in the Prague Depen-dency Treebank), but is based on rules instead of classifiers, as in our case.
The same applies to earlierworks in the TAG-framework, as, e.g., in (Rambow and Joshi, 1997).However, this is not to say that the idea of the surface?surface syntax?deep syntax pipeline is new.It goes back at least to Curry (1961) and is implemented in a number of more recent works; see, e.g., (deGroote, 2001; Klime?s, 2006; Bojar et al., 2008).6 Conclusions and Future WorkWe have presented a deep-syntactic parsing pipeline which consists of a state-of-the-art dependencyparser and a novel SSyntS?DSyntS transducer.
The obtained DSyntSs can be used in different applica-tions since they abstract from language-specific grammatical idiosyncrasies of the SSynt structures asproduced by state-of-the art dependency parsers, but still avoid the complexities of genuine semanticanalysis.9DSyntS-treebanks needed for data-driven applications can be bootstrapped by the pipeline.If required, a SSyntS?DSyntS structure pair can be also mapped to a pure predicate-argument graphsuch as the DELPH-IN structure (Oepen, 2002) or to an approximation thereof (as the Enju conversion(Miyao, 2006), which keeps functional nodes), to an DRS (Kamp and Reyle, 1993), or to a PropBankstructure.
On the other hand, DSyntS-treebanks can be used for automatic extraction of deep grammars.As shown by Cahill et al.
(2008), automatically obtained resources can be of an even better quality thanmanually-crafted resources.
In this context, especially research in the context of CCGs (Hockenmeier,2003; Clark and Curran, 2007) and TAGs (Xia, 1999) should be also mentioned.To validate our approach with languages other than Spanish, we carried out an experiment on a Chi-nese SSyntS-DSyntS Treebank (training the DSynt-transducer on the outcome of the SSynt-parser).
Theresults over predicted input showed an accuracy of about 75%, i.e., an accuracy comparable to the accu-racy achieved for Spanish.
We are also investigating multilingual approaches, such as the one proposedby McDonald et al.
(2013).In the future, we will carry out further in-depth feature engineering for the task of DSynt-parsing.
Itproved to be crucial in semantic role labelling and dependency parsing (Che et al., 2009; Ballesteros andNivre, 2012); we expect it be essential for our task as well.
Furthermore, we will join surface syntacticand deep-syntactic parsing we kept so far separate; see, e.g., (Zhang and Clark, 2008; Llu?
?s et al., 2013;Bohnet and Nivre, 2012) for analogous proposals.
Further research is required here since although jointmodels avoid error propagation from the first stage to the second, overall, pipelined models still provedto be competitive; cf.
the outcome of CoNLL shared tasks.The deep-syntactic parser described in this paper is available for downloading at https://code.google.com/p/deepsyntacticparsing/.AcknowledgementsThis work has been supported by the European Commission under the contract number FP7-ICT-610411.Many thanks to the three anonymous COLING reviewers for their very helpful comments and sugges-tions.9The motivation to work with DSyntS instead of SSyntS is thus similiar to the motivation of the authors of the AbstractMeaning Representation (AMR) for Machine Translation (Banarescu et al., 2013), only that AMRs are considerably moresemantic than DSyntSs.1410ReferencesAlfred V. Aho.
1972.
The theory of parsing, translation and, compiling.
Prentice Hall, Upper Saddle River, NJ.Miguel Ballesteros and Joakim Nivre.
2012.
MaltOptimizer: A System for MaltParser Optimization.
In Proceed-ings of the Eighth International Conference on Language Resources and Evaluation (LREC 12).L.
Banarescu, C. Bonial, S. Cai, M. Georgescu, K. Griffitt, U. Hermjakob, K. Knight, P. Koehn, M. Palmer, andN.
Schneider.
2013.
Abstract Meaning Representation for Sembanking.
In Proceedings of the 7th LinguisticAnnotation Workshop & Interoperability with Discourse, pages 178?186, Sofia, Bulgaria.Bernd Bohnet and Joakim Nivre.
2012.
A transition-based system for joint part-of-speech tagging and labelednon-projective dependency parsing.
In EMNLP-CoNLL.O.
Bojar, S. Cinkov?a, and J. Pt?a?cek.
2008.
Towards English-to-Czech MT via Tectogrammatical Layer.
ThePrague Bulletin of Mathematical Linguistics, 90:57?68.Mathias Buch-Kromann.
2003.
The Danish dependency treebank and the dtag treebank tool.
In 2nd Workshop onTreebanks and Linguistic Theories (TLT), Sweden, pages 217?220.Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-X shared task on multilingual dependency parsing.
In Proceed-ings of the 10th Conference on Computational Natural Language Learning (CoNLL), pages 149?164.Aoife Cahill, Michael Burke, Ruth O?Donovan, Stefan Riezler, Josef van Genabith, and Andy Way.
2008.
Wide-coverage deep statistical parsing using automatic dependency structure annotation.
Computational Linguistics,34(1):81?124.Chih-Chung Chang and Chih-Jen Lin, 2001.
LIBSVM: A Library for Support Vector Machines.
Software availableat http://www.csie.ntu.edu.tw/?cjlin/libsvm.Wanxiang Che, Zhenghua Li, Yongqiang Li, Yuhang Guo, Bing Qin, and Ting Liu.
2009.
Multilingualdependency-based syntactic and semantic parsing.
In Proceedings of the Thirteenth Conference on Compu-tational Natural Language Learning (CoNLL 2009): Shared Task, pages 49?54, Boulder, Colorado, June.
As-sociation for Computational Linguistics.Stephen Clark and James R. Curran.
2007.
Wide-coverage efficient statistical parsing with CCG and log-linearmodels.
Computational Linguistics, 33:493?552.R.
Curry.
1961.
Some logical aspects of grammatical structure.
In R. Jakobson, editor, Structure of Language andIts Mathematical Aspects, pages 56?68.
American Mathematical Society, Providence, RI.Ph.
de Groote.
2001.
Towards abstract categorial grammar.
In Proceedings of the 39th Annual Meeting of theAssociation for Computational Linguistics (ACL).Charles J. Fillmore, Collin F. Baker, and Hiroaki Sato.
2002.
The FrameNet database and software tools.
InProceedings of the Third International Conference on Language Resources and Evaluation, volume IV, LasPalmas.
LREC, LREC.A.
Gesmundo, J. Henderson, P. Merlo, and I.Titov.
2009.
Latent variable model of synchronous syntactic-semanticparsing for multiple languages.
In CoNLL 2009 Shared Task., Conf.
on Computational Natural LanguageLearning, pages 37?42, Boulder, Colorado, USA.Jan Haji?c, Jarmila Panevov?a, Eva Haji?cov?a, Petr Sgall, Petr Pajas, Jan?St?ep?anek, Ji?r??
Havelka, Marie Mikulov?a,and Zden?k?Zabokrtsk?y.
2006.
Prague Dependency Treebank 2.0.
Linguistic Data Consortium, Philadelphia.Jan Haji?c, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria Ant`onia Mart?
?, Llu?
?s M`arquez,Adam Meyers, Joakim Nivre, Sebastian Pad?o, Jan?St?ep?anek, Pavel Stra?n?ak, Mihai Surdeanu, Nianwen Xue,and Yi Zhang.
2009.
The conll-2009 shared task: Syntactic and semantic dependencies in multiple languages.In Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): SharedTask, pages 1?18.J.
Hockenmeier.
2003.
Parsing with generative models of predicate-argument structure.
In Proceedings of the 41stAnnual Meeting of the Association for Computational Linguistics (ACL), pages 359?366, Sapporo, Japan.Angelina Ivanova, Stephan Oepen, Lilja ?vrelid, and Dan Flickinger.
2012. Who did what to whom?
a contrastivestudy of syntacto-semantic dependencies.
In Proceedings of the Sixth Linguistic Annotation Workshop, pages2?11, Jeju, Republic of Korea, July.
Association for Computational Linguistics.1411R.
Johansson and P. Nugues.
2007.
Extended constituent-to-dependency conversion for english.
In J. Nivre, H.-J.Kaalep, K. Muischnek, and M. Koit, editors, Proceedings of NODALIDA 2007, pages 105?112, Tartu, Estonia.Richard Johansson and Pierre Nugues.
2008.
Dependency-based syntactic?semantic analysis with PropBank andNomBank.
In CoNLL 2008: Proceedings of the Twelfth Conference on Natural Language Learning, pages183?187, Manchester, United Kingdom.H.
Kamp and U. Reyle.
1993.
From Discourse to Logic.
Kluwer Academic Publishers, Dordrecht, NL.R.T.
Kasper and W.C. Rounds.
1986.
A logical semantics for feature structures.
In Proceedings of the 24th annualmeeting on Association for Computational Linguistics, pages 257?266.V?aclav Klime?s.
2006.
Analytical and Tectogrammatical Analysis of a Natural Language.
Ph.D. thesis, UFAL,MFF UK, Prague, Czech Republic.Xavier Llu?
?s, Xavier Carreras, and Llu?
?s M`arquez.
2013.
Joint arc-factored parsing of syntactic and semanticdependencies.
Transactions of the Association for Computational Linguistics, pages 219?230.Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-Brundage, Yoav Goldberg, Dipanjan Das, Kuzman Ganchev,Keith Hall, Slav Petrov, Hao Zhang, Oscar T?ackstr?om, Claudia Bedini, N?uria Bertomeu Castell?o, and JungmeeLee.
2013.
Universal dependency annotation for multilingual parsing.
In Proceedings of the 51st AnnualMeeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 92?97.Igor Mel??cuk.
1988.
Dependency Syntax: Theory and Practice.
State University of New York Press.Igor Mel??cuk.
1996.
Lexical functions: A tool for the description of lexical relations in the lexicon.
In L. Wanner,editor, Lexical functions in lexicography and natural language processing, pages 37?102.
Benjamins AcademicPublishers, Amsterdam.Igor Mel??cuk.
2013.
Semantics: From meaning to text, Volume 2.
Benjamins Academic Publishers, Amsterdam.Simon Mille, Alicia Burga, Gabriela Ferraro, and Leo Wanner.
2012.
How does the granularity of an annotationscheme influence dependency parsing performance?
In Conference on Computational Linguistics, COLING2012.Simon Mille, Alicia Burga, and Leo Wanner.
2013.
AnCora-UPF: A Multi-Level Annotation of Spanish .
InProceedings of the Second International Conference on Dependency Linguistics (DEPLING 2013).Yusuke Miyao.
2006.
From Linguistic Theory to Syntactic Analysis: Corpus-Oriented Grammar Developmentand Feature Forest Model.
Ph.D. thesis, University of Tokyo.J.
Nivre, J.
Hall, S. K?ubler, R. McDonald, J. Nilsson, S. Riedel, and D. Yuret.
2007.
The CoNLL 2007 Shared Taskon Dependency Parsing.
In Proceedings of the CoNLL Shared Task of EMNLP-CoNLL 2007, pages 915?932.Stephan Oepen.
2002.
Collaborative Language Engineering: A Case Study in Efficient Grammar-based Process-ing.
Stanford Univ Center for the Study.Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005.
The proposition bank.
Computational Linguistics,31:71?106.Owen Rambow and Aravind Joshi.
1997.
A formal look at dependency grammar and phrase structure grammars,with special consideration of word-order phenomena.
In L. Wanner, editor, Recent Trends in Meaning-TextTheory, pages 167?190.
Benjamins Academic Publishers, Amsterdam.W.C.
Rounds.
1970.
Mappings and grammars on trees.
Mathematical Systems Theory, 4(3):257?287.Mihai Surdeanu, Richard Johansson, Adam Meyers, Llu?
?s M`arquez, and Joakim Nivre.
2008.
The conll 2008shared task on joint parsing of syntactic and semantic dependencies.
In CoNLL 2008: Proceedings of theTwelfth Conference on Computational Natural Language Learning, pages 159?177.M.
Taul?e, M. Ant`onia Mart?
?, and Marta Recasens.
2008.
Ancora: Multilevel annotated corpora for Catalan andSpanish.
In Proceedings of the Sixth International Language Resources and Evaluation (LREC?08), Marrakech,Morocco, may.
European Language Resources Association (ELRA).J.W.
Thatcher.
1970.
Generalized sequential machine maps.
Journal of Computer and System Sciences, 4(4):339?367.1412F.
Xia.
1999.
Extracting tree adjoining grammars from bracketed corpora.
In Proceedings of the 5th NaturalLanguage Processing Pacific Rim Symposium, pages 398?403, Beijing, China.Yue Zhang and Stephen Clark.
2008.
Joint word segmentation and POS tagging using a single perceptron.
In Pro-ceedings of ACL-08: HLT, pages 888?896, Columbus, Ohio, June.
Association for Computational Linguistics.1413
