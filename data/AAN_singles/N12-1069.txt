2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 577?581,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsConcavity and Initialization for Unsupervised Dependency ParsingKevin Gimpel and Noah A. SmithLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213, USA{kgimpel,nasmith}@cs.cmu.eduAbstractWe investigate models for unsupervised learn-ing with concave log-likelihood functions.
Webegin with the most well-known example,IBM Model 1 for word alignment (Brownet al, 1993) and analyze its properties, dis-cussing why other models for unsupervisedlearning are so seldom concave.
We thenpresent concave models for dependency gram-mar induction and validate them experimen-tally.
We find our concave models to be effec-tive initializers for the dependency model ofKlein and Manning (2004) and show that wecan encode linguistic knowledge in them forimproved performance.1 IntroductionIn NLP, unsupervised learning typically implies op-timization of a ?bumpy?
objective function riddledwith local maxima.
However, one exception is IBMModel 1 (Brown et al, 1993) for word alignment,which is the only model commonly used for unsu-pervised learning in NLP that has a concave log-likelihood function.1 For other models, such asthose used in unsupervised part-of-speech taggingand grammar induction, and indeed for more sophis-ticated word alignment models, the log-likelihoodfunction maximized by EM is non-concave.
As aresult, researchers are obligated to consider initial-ization in addition to model design (Klein and Man-ning, 2004; Goldberg et al, 2008).For example, consider the dependency grammarinduction results shown in Table 1 when training the1It is not strictly concave (Toutanova and Galley, 2011).widely used dependency model with valence (DMV;Klein and Manning, 2004).
Using uniform distri-butions for initialization (UNIF) results in an accu-racy of 17.6% on the test set, well below the base-line of attaching each word to its right neighbor(ATTACHRIGHT, 31.7%).
Furthermore, when usinga set of 50 random initializers (RAND), the standarddeviation of the accuracy is an alarming 8.3%.In light of this sensitivity to initialization, it iscompelling to consider unsupervised models withconcave log-likelihood functions, which may pro-vide stable, data-supported initializers for morecomplex models.
In this paper, we explore the issuesinvolved with such an expedition and elucidate thelimitations of such models for unsupervised NLP.We then present simple concave models for depen-dency grammar induction that are easy to implementand offer efficient optimization.
We also show howlinguistic knowledge can be encoded without sacri-ficing concavity.
Using our models to initialize theDMV, we find that they lead to an improvement inaverage accuracy across 18 languages.2 IBM Model 1 and ConcavityIBM Model 1 is a conditional model of a target-language sentence e of length m and an alignmenta given a source-language sentence f of length l.The generation of m is assumed to occur with some(inconsequential) uniform probability .
The align-ment vector a, a hidden variable, has an entry foreach element of e that contains the index in f ofthe aligned word.
These entries are used to definewhich translation parameters t(ej | faj ) are active.Model 1 assumes that the probability of the ith ele-577ment in a, denoted a(i | j, l,m), is simply a uni-form distribution over all l source words plus thenull word.
These assumptions result in the follow-ing log-likelihood for a sentence pair ?f , e?
underModel 1 (marginalizing a):log p(e | f) = log (l+1)m+?mj=1 log?li=0 t(ej | fi)(1)The only parameters to be learned in the model aret = {t(e | f)}e,f .
Since a parameter is concave initself, the sum of concave functions is concave, andthe log of a concave function is concave, Eq.
1 isconcave in t (Brown et al, 1993).IBM Model 2 involves a slight change to Model1 in which the probability of a word link dependson the word positions.
However, this change rendersit no longer concave.
Consider the log-likelihoodfunction for Model 2:log +?mj=1 log?li=0 t(ej | fi) ?a(i | j, l,m) (2)Eq.
2 is not concave in the parameters t(ej | fi) anda(i | j, l,m) because a product is neither convex norconcave in its vector of operands.
This can be shownby computing the Hessian matrix of f(x, y) = xyand showing that it is indefinite.In general, concavity is lost when the log-likelihood function contains a product of model pa-rameters enclosed within a log?.
If the sum is notpresent, the log can be used to separate the prod-uct of parameters, making the function concave.
Itcan also be shown that a ?featurized?
version (Berg-Kirkpatrick et al, 2010) of Model 1 is not con-cave.
More generally, any non-concave function en-closed within log?will cause the log-likelihoodfunction to be non-concave, though there are fewother non-concave functions with a probabilistic se-mantics than those just discussed.3 Concave, Unsupervised ModelsNearly every other model used for unsupervisedlearning in NLP has a non-concave log-likelihoodfunction.
We now proceed to describe the conditionsnecessary to develop concave models for two tasks.3.1 Part-of-Speech TaggingConsider a standard first-order hidden Markovmodel for POS tagging.
Letting y denote the tagsequence for a sentence e with m tokens, the single-example log-likelihood is:log?y p(stop | ym)?mj=1 p(yj | yj?1) ?
p(ej | yj)(3)where y0 is a designated ?start?
symbol.
Unlike IBMModels 1 and 2, we cannot reverse the order of thesummation and product here because the transitionparameters p(yj | yj?1) cause each tag decision toaffect its neighbors.
Therefore, Eq.
3 is non-concavedue to the presence of a product within a log?.However, if the tag transition probabilities p(yj |yj?1) are all constants and also do not depend onthe previous tag yj?1, then we can rewrite Eq.
3 asthe following concave log-likelihood function (usingC(y) to denote a constant function of tag y, e.g., afixed tag prior distribution):logC(stop) + log?mj=1?yjC(yj) ?
p(ej | yj)Lacking any transition modeling power, this modelappears weak for POS tagging.
However, we notethat we can add additional conditioning informationto the p(ej | yj) distributions and retain concavity,such as nearby words and tag dictionary informa-tion.
We speculate that such a model might learnuseful patterns about local contexts and provide aninitializer for unsupervised part-of-speech tagging.3.2 Dependency Grammar InductionTo develop dependency grammar induction models,we begin with a version of Model 1 in which a sen-tence e is generated from a copy of itself (denotede?
): log p(e | e?
)= log (m+1)m +?mj=1 log?mi=0,i 6=j c(ej | e?i) (4)If a word ej is ?aligned?
to e?0, ej is a root.
Thisis a simple child-generation model with no tree con-straint.
In order to preserve concavity, we are forbid-den from conditioning on other parent-child assign-ments or including any sort of larger constraints.However, we can condition the child distributionson additional information about e?
since it is fullyobserved.
This conditioning information may in-clude the direction of the edge, its distance, andany properties about the words in the sentence.
Wefound that conditioning on direction improved per-formance: we rewrite the c distributions as c(ej |e?i, sign(j ?
i)) and denote this model by CCV1.578We note that we can also include constraints in thesum over possible parents and still preserve concav-ity.
Naseem et al (2010) found that adding parent-child constraints to a grammar induction system canimprove performance dramatically.
We employ onesimple rule: roots are likely to be verbs.2 We mod-ify CCV1 to restrict the summation over parents toexclude e?0 if the child word is not a verb.3 We onlyemploy this restriction during EM learning for sen-tences containing at least one verb.
For sentenceswithout verbs, we allow all words to be the root.
Wedenote this model by CCV2.In related work, Brody (2010) also developedgrammar induction models based on the IBM wordalignment models.
However, while our goal is todevelop concave models, Brody employed Bayesiannonparametrics in his version of Model 1, whichmakes the model non-concave.4 ExperimentsWe ran experiments to determine how well our con-cave grammar induction models CCV1 and CCV2 canperform on their own and when used as initializersfor the DMV (Klein and Manning, 2004).
The DMVis a generative model of POS tag sequences and pro-jective dependency trees over them.
It is the foun-dation of most state-of-the-art unsupervised gram-mar induction models (several of which are listed inTab.
1).
The model includes multinomial distribu-tions for generating each POS tag given its parentand the direction of generation: where ei is the par-ent POS tag and ej the child tag, these distributionstake the form c(ej | ei, sign(j ?
i)), analogous tothe distributions used in our concave models.
TheDMV also has multinomial distributions for decid-ing whether to stop or continue generating childrenin each direction considering whether any childrenhave already been generated in that direction.The majority of researchers use the original ini-tializer from Klein and Manning (2004), denotedhere K&M.
K&M is a deterministic harmonic initial-izer that sets parent-child token affinities inversely2This is similar to the rule used by Marec?ek and Z?abokrtsky?
(2011) with empirical success.3As verbs, we take all tags that map to V in the universal tagmappings from Petrov et al (2012).
Thus, to apply this con-straint to a new language, one would have to produce a similartag mapping or identify verb tags through manual inspection.Train ?
10 Train ?
20Test TestModel Init.
?10 ??
?10 ?
?ATTRIGHT N/A 38.4 31.7 38.4 31.7CCV1 UNIF 31.4 25.6 31.0 23.7CCV2 UNIF 43.1 28.6 43.9 27.1UNIF 21.3 17.6 21.3 16.4RAND?
41.0 31.8 - -DMV K&M 44.1 32.9 51.9 37.8CCV1 45.3 30.9 53.9 36.7CCV2 54.3 43.0 64.3 53.1Shared LN K&M 61.3 41.4L-EVG RAND?
68.8 -Feature DMV K&M 63.0 -LexTSG-DMV K&M 67.7 55.7Posterior Reg.
K&M 64.3 53.3Punc/UTags K&M?
- 59.1?Table 1: English attachment accuracies on Section 23, forshort sentences (?10 words) and all (??).
We includeselected results on this same test set: Shared LN = Cohenand Smith (2009), L-EVG = Headden III et al (2009),Feature DMV = Berg-Kirkpatrick et al (2010), LexTSG-DMV = Blunsom and Cohn (2010), Posterior Reg.
=Gillenwater et al (2010), Punc/UTags = Spitkovsky etal.
(2011a).
K&M?
is from Spitkovsky et al (2011b).
?Accuracies are averages over 50 random initializers;?
= 10.9 for test sentences ?
10 and 8.3 for all.
?Usedmany random initializers with unsupervised run selec-tion.
?Used staged training with sentences ?
45 words.proportional to their distances, then normalizes toobtain probability distributions.
K&M is often de-scribed as corresponding to an initial E step for anunspecified model that favors short attachments.Procedure We run EM for our concave models for100 iterations.
We evaluate the learned models di-rectly as parsers on the test data and also use themto initialize the DMV.
When using them directly asparsers, we use dynamic programming to ensure thata valid tree is recovered.
When using the concavemodels as initializers for the DMV, we copy the cparameters over directly since they appear in bothmodels.
We do not have the stop/continue parame-ters in our concave models, so we simply initializethem uniformly for the DMV.
We train each DMVfor 200 iterations and use minimum Bayes risk de-coding with the final model on the test data.
We useseveral initializers for training the DMV, includingthe uniform initializer (UNIF), K&M, and our trainedconcave models CCV1 and CCV2.579Init.
eu bg ca zh cs da nl en de el huUNIF 24/21 32/26 27/29 44/40 32/30 24/19 21/21 21/18 31/24 37/32 23/18K&M 32/26 48/40 24/25 38/33 31/29 34/23 39/33 44/33 47/37 50/41 23/20CCV1 22/21 34/27 44/51 46/45 33/31 19/14 24/24 45/31 46/31 51/45 32/28CCV2 26/25 34/26 29/35 46/44 50/40 29/18 50/43 54/43 49/33 50/45 60/46it ja pt sl es sv tr avg.
accuracy avg.
log-likelihoodUNIF 31/24 35/30 49/36 20/20 29/24 26/22 33/30 29.8 / 25.7 -15.05K&M 32/24 39/31 44/28 33/27 19/11 46/33 39/36 36.7 / 29.4 -14.84CCV1 34/25 42/27 50/38 30/25 41/33 45/33 37/29 37.5 / 30.9 -14.93CCV2 55/48 49/31 50/38 22/21 57/50 46/32 31/22 43.7 / 35.5 -14.45Table 2: Test set attachment accuracies for 18 languages; first number in each cell is accuracy for sentences ?
10words and second is for all sentences.
For training, sentences ?
10 words from each treebank were used.
In order,languages are Basque, Bulgarian, Catalan, Chinese, Czech, Danish, Dutch, English, German, Greek, Hungarian,Italian, Japanese, Portuguese, Slovenian, Spanish, Swedish, and Turkish.Data We use data prepared for the CoNLL2006/07 shared tasks (Buchholz and Marsi, 2006;Nivre et al, 2007).4 We follow standard practicein removing punctuation and using short sentences(?
10 or ?
20 words) for training.
For all experi-ments, we train on separate data from that used fortesting and use gold POS tags for both training andtesting.
We report accuracy on (i) test set sentences?10 words and (ii) all sentences from the test set.Results Results for English are shown in Tab.
1.We train on ?2?21 and test on ?23 in the Penn Tree-bank.
The constraint on sentence roots helps a greatdeal, as CCV2 by itself is competitive with the DMVwhen testing on short sentences.
The true benefit ofthe concave models, however, appears when usingthem as initializers.
The DMV initialized with CCV2achieves a substantial improvement over all others.When training on sentences of length ?
20 words(bold), the performance even rivals that of severalmore sophisticated models shown in the table, de-spite only using the DMV with a different initializer.Tab.
2 shows results for 18 languages.
On av-erage, CCV2 performs best and CCV1 does at leastas well as K&M.
This shows that a simple, concavemodel can be as effective as a state-of-the-art hand-designed initializer (K&M), and that concave mod-els can encode linguistic knowledge to further im-prove performance.4In some cases, we did not use official CoNLL test sets butinstead took the training data and reserved the first 80% of thesentences for training, the next 10% for development, and thefinal 10% as our test set; dataset details are omitted for spacebut are the same as those given by Cohen (2011).Average log-likelihoods (micro-averaged acrosssentences) achieved by EM training are shown in thefinal column of Tab.
2.
CCV2 leads to substantially-higher likelihoods than the other initializers, sug-gesting that the verb-root constraint is helping EMto find better local optima.55 DiscussionStaged training has been shown to help unsupervisedlearning in the past, from early work in grammar in-duction (Lari and Young, 1990) and word alignment(Brown et al, 1993) to more recent work in depen-dency grammar induction (Spitkovsky et al, 2010).While we do not yet offer a generic procedure forextracting a concave approximation from any modelfor unsupervised learning, our results contribute evi-dence in favor of the general methodology of stagedtraining in unsupervised learning, and provide a sim-ple and powerful initialization method for depen-dency grammar induction.AcknowledgmentsWe thank Shay Cohen, Dipanjan Das, Val Spitkovsky,and members of the ARK research group for helpful com-ments that improved this paper.
This research was sup-ported in part by the NSF through grant IIS-0915187, theU.
S. Army Research Laboratory and the U. S. Army Re-search Office under contract/grant number W911NF-10-1-0533, and Sandia National Laboratories (fellowship toK.
Gimpel).5However, while CCV1 leads to a higher average accuracythan K&M, the latter reaches slightly higher likelihood, sug-gesting that the success of the concave initializers is only par-tially due to reaching high training likelihood.580ReferencesT.
Berg-Kirkpatrick, A.
Bouchard-Co?te?, J. DeNero, andD.
Klein.
2010.
Painless unsupervised learning withfeatures.
In Proc.
of NAACL.P.
Blunsom and T. Cohn.
2010.
Unsupervised inductionof tree substitution grammars for dependency parsing.In Proc.
of EMNLP.S.
Brody.
2010.
It depends on the translation: Unsu-pervised dependency parsing via word alignment.
InProc.
of EMNLP.P.
F. Brown, S. A. Della Pietra, V. J. Della Pietra, andR.
L. Mercer.
1993.
The mathematics of statisticalmachine translation: Parameter estimation.
Computa-tional Linguistics, 19(2):263?311.S.
Buchholz and E. Marsi.
2006.
CoNLL-X sharedtask on multilingual dependency parsing.
In Proc.
ofCoNLL.S.
Cohen and N. A. Smith.
2009.
Shared logistic normaldistributions for soft parameter tying in unsupervisedgrammar induction.
In Proc.
of NAACL.S.
Cohen.
2011.
Computational Learning of Probabilis-tic Grammars in the Unsupervised Setting.
Ph.D. the-sis, Carnegie Mellon University.J.
Gillenwater, K. Ganchev, J. Grac?a, F. Pereira, , andB.
Taskar.
2010.
Posterior sparsity in unsuperviseddependency parsing.
Journal of Machine LearningResearch.Y.
Goldberg, M. Adler, and M. Elhadad.
2008.
EMcan find pretty good HMM POS-taggers (when givena good start).
In Proc.
of ACL.W.
Headden III, M. Johnson, and D. McClosky.
2009.Improving unsupervised dependency parsing withricher contexts and smoothing.
In Proc.
of NAACL.D.
Klein and C. D. Manning.
2004.
Corpus-based induc-tion of syntactic structure: Models of dependency andconstituency.
In Proc.
of ACL.K.
Lari and S. J.
Young.
1990.
The estimation of stochas-tic context-free grammars using the inside-outside al-gorithm.
Computer Speech and Language, 4:35?56.D.
Marec?ek and Z.
Z?abokrtsky?.
2011.
Gibbs samplingwith treeness constraint in unsupervised dependencyparsing.
In Proc.
of Workshop on Robust Unsuper-vised and Semisupervised Methods in Natural Lan-guage Processing.T.
Naseem, H. Chen, R. Barzilay, and M. Johnson.
2010.Using universal linguistic knowledge to guide gram-mar induction.
In Proc.
of EMNLP.J.
Nivre, J.
Hall, S. Ku?bler, R. McDonald, J. Nils-son, S. Riedel, and D. Yuret.
2007.
The CoNLL2007 shared task on dependency parsing.
In Proc.
ofCoNLL.S.
Petrov, D. Das, and R. McDonald.
2012.
A universalpart-of-speech tagset.
In Proc.
of LREC.V.
I. Spitkovsky, H. Alshawi, and D. Jurafsky.
2010.From Baby Steps to Leapfrog: How ?Less is More?in unsupervised dependency parsing.
In Proc.
ofNAACL-HLT.V.
I. Spitkovsky, H. Alshawi, A. X. Chang, and D. Juraf-sky.
2011a.
Unsupervised dependency parsing with-out gold part-of-speech tags.
In Proc.
of EMNLP.V.
I. Spitkovsky, H. Alshawi, and D. Jurafsky.
2011b.Punctuation: Making a point in unsupervised depen-dency parsing.
In Proc.
of CoNLL.K.
Toutanova and M. Galley.
2011.
Why initializationmatters for IBM Model 1: Multiple optima and non-strict convexity.
In Proc.
of ACL.581
