Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 220?229,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsStatistical Script Learning with Multi-Argument EventsKarl PichottaDepartment of Computer ScienceThe University of Texas at Austinpichotta@cs.utexas.eduRaymond J. MooneyDepartment of Computer ScienceThe University of Texas at Austinmooney@cs.utexas.eduAbstractScripts represent knowledge of stereotyp-ical event sequences that can aid text un-derstanding.
Initial statistical methodshave been developed to learn probabilis-tic scripts from raw text corpora; how-ever, they utilize a very impoverished rep-resentation of events, consisting of a verband one dependent argument.
We presenta script learning approach that employsevents with multiple arguments.
Unlikeprevious work, we model the interactionsbetween multiple entities in a script.
Ex-periments on a large corpus using the taskof inferring held-out events (the ?narrativecloze evaluation?)
demonstrate that mod-eling multi-argument events improves pre-dictive accuracy.1 IntroductionScripts encode knowledge of stereotypical events,including information about their typical orderedsequences of sub-events and corresponding argu-ments (Schank and Abelson, 1977).
The clas-sic example is the ?restaurant script,?
which en-codes knowledge about what normally happenswhen dining out.
Such knowledge can be usedto improve text understanding by supporting in-ference of missing actions and events, as well asresolution of lexical and syntactic ambiguities andanaphora (Rahman and Ng, 2012).
For example,given the text ?John went to Olive Garden and or-dered lasagna.
He left a big tip and left,?
an infer-ence that scripts would ideally allow us to make is?John ate lasagna.
?There is a small body of recent research on auto-matically learning probabilistic models of scriptsfrom large corpora of raw text (Manshadi et al.,2008; Chambers and Jurafsky, 2008; Chambersand Jurafsky, 2009; Jans et al., 2012).
However,this work uses a very impoverished representationof events that only includes a verb and a single de-pendent entity.
We propose a more complex multi-argument event representation for use in statisticalscript models, capable of directly capturing inter-actions between multiple entities.
We present amethod for learning such a model, and provide ex-perimental evidence that modeling entity interac-tions allows for better prediction of events in docu-ments, compared to previous single-entity ?chain?models.
We also compare to a competitive base-line not used in previous work, and introduce anovel evaluation metric.2 BackgroundThe idea of representing stereotypical event se-quences for textual inference originates in theseminal work of Schank and Abelson (1977).Early scripts were manually engineered for spe-cific domains; however, Mooney and DeJong(1985) present an early knowledge-based methodfor learning scripts from a single document.
Theseearly scripts (and methods for learning them) werenon-statistical and fairly brittle.Chambers and Jurafsky (2008) introduced amethod for learning statistical scripts that, using amuch simpler event representation that allows forefficient learning and inference.
Jans et al.
(2012)use the same simple event representation, but in-troduce a new model that more accurately predictstest data.
These methods only model the actions ofa single participant, called the protagonist.
Cham-bers and Jurafsky (2009) extended their approachto the multi-participant case, modeling the eventsin which all of the entities in a document are in-volved; however, their method cannot represent in-teractions between multiple entities.Balasubramanian et al.
(2012; 2013) describethe Rel-gram system, a Markov model similar tothat of Jans et al.
(2012), but with tuples insteadof (verb, dependency) pairs.
Our approach is sim-220ilar, but instead of modeling a distribution over co-occurring verbs and nominal arguments, we modelinteractions between entities directly by incorpo-rating coreference information into the model.Previous statistical script learning systems pro-ceed broadly as follows.
For a document D:1.
Run a dependency parser on D, to match upverbs with their argument NPs.2.
Run a coreference resolver onD to determinewhich NPs likely refer to the same entity.3.
Construct a sequence of event objects, usingsyntactic and coreference information.One can then build a statistical model of the eventsequences produced by Step 3.
Such a model maybe evaluated using the narrative cloze evaluation,described in Section 4.1, in which we hold out anevent from a sequence and attempt to infer it.The major difference between the current workand previous work is that the event sequences pro-duced in Step 3 are of a different sort from thosein other models.
Our events are more structured,as described in Section 3.1, and we produce oneevent sequence per document, instead of one eventsequence per entity.
This requires a different sta-tistical model, as described in Section 3.2.3 Script ModelsIn Section 3.1, we describe the multi-argumentevents we use as the basis of our script models.Section 3.2 describes a script model using theseevents, and Section 3.3 describes the baseline sys-tems to which we compare.3.1 Multi-Argument EventsStatistical scripts are models of stereotypical se-quences of events.
In Chambers and Juraf-sky (2008; 2009) and Jans et al.
(2012), eventsare (verb, dependency) pairs, forming ?chains,?grouped according to the entity involved.
For ex-ample, the text(1) Mary emailed Jim and he responded to herimmediately.yields two chains.
First, there is a chain for Mary:(email, subject)(respond, object)indicating that Mary was the subject of an email-ing event and the object of a responding event.Second, there is a chain for Jim:(email, object)(respond, subject)indicating that Jim was the object of an emailingevent and the subject of a responding event.
Thus,one document produces many chains, each cor-responding to an entity.
Note that a single verbmay produce multiple pair events, each presentin a chain corresponding to one of the verb?s ar-guments.
Note also that there is no connectionbetween the different events produced by a verb:there is nothing connecting (email, subject) inMary?s chain with (email, object) in Jim?s chain.We propose a richer event representation, inwhich a document is represented as a single se-quence of event tuples, the arguments of which areentities.
Each entity may be mentioned in manyevents, and, unlike previous work, each event mayinvolve multiple entities.
For example, sentence(1) will produce a single two-event sequence, thefirst event representing Mary emailing Jim, and thesecond representing Jim responding to Mary.Formally, an entity is represented by a con-stant, and noun phrases are mapped to entities,where two noun phrases are mapped to the sameconstant if and only if they corefer.
A multi-argument event is a relational atom v(es, eo, ep),where v is a verb lemma, and es, eo, and eparepossibly-null entities.
The first entity, es, standsin a subject relation to the verb v; the second, eo,is the direct object of v; the third epstands ina prepositional relation to v. One of these enti-ties is null (written as ???)
if and only if no nounphrase stands in the appropriate relation to v. Forexample, Mary hopped would be represented ashop(mary, ?, ?
), while Mary gave the book to Johnwould be give(mary, book, john).
In this formula-tion, Example (1) produces the sequenceemail(m, j, ?
)respond(j,m, ?
)where m and j are entity constants representingall mentions of Mary and Jim, respectively.
Notethat this formulation is capable of capturing inter-actions between entities: we directly encode thefact that after one person emails another, the lat-ter responds to the former.
In contrast, pair eventscan capture only that after an entity emails, theyare responded to (or after they are emailed, theyrespond).
Multi-argument events capture more ofthe basic event structure of text, and are thereforewell-suited as a representation for scripts.2213.2 Multi-argument Statistical ScriptsWe now describe our script model.
Section 3.2.1describes our method of estimating a joint prob-ability distribution over pairs of events, modelingevent co-occurrence, and Section 3.2.2 shows howthis co-occurrence probability can be used to infernew events from a set of known events.3.2.1 Estimating Joint ProbabilitiesSuppose we have a sequence of multi-argumentevents, each of which is a verb with entities as ar-guments.
We are interested in predicting whichevent is most likely to have happened at somepoint in the sequence.
Our model will requirea conditional probability P (a|a?
), the probabilityof seeing event a after event a?, given we haveobserved a?.
However, as described below, di-rectly estimating this probability is more compli-cated than in previous work because events nowhave additional structure.By definition, we haveP (a2|a1) =P (a1, a2)P (a1)where P (a1, a2) is the probability of seeing a1and a2, in order.
The most straightforward wayto estimate P (a1, a2) is, if possible, by countingthe number of times we observe a1and a2co-occurring and normalizing the function to sum to1 over all pairs (a1, a2).
For Chambers and Ju-rafsky (2008; 2009) and Jans et al.
(2012), such aMaximum Likelihood Estimate is straightforwardto arrive at: events are (verb, dependency) pairs,and two events co-occur when they are in the sameevent chain, relating to the same entity (Jans et al.
(2012) further require a1and a2to be near eachother).
One need simply traverse a training corpusand count the number of times each pair (a1, a2)co-occurs.
The Rel-grams of Balasubramanian etal.
(2012; 2013) admit a similar strategy: to arriveat a joint distribution of pairwise co-occurrence,one can simply count co-occurrence of ground re-lations in a corpus and normalize.However, given two multi-argument events ofthe form v(es, eo, ep), this strategy will not suffice.For example, if during training we observe the twoco-occurring events(2) ask(mary, bob, question)answer(bob, ?, ?
)we would like this to lend evidence to theco-occurrence of events ask(x, y, z) andAlgorithm 1 Learning with entity substitution1: for a1, a2?
evs do2: N(a1, a2)?
03: end for4: for D ?
documents do5: for a1, a2?
coocurEvs(D) do6: for ?
?
subs(a1, a2) do7: N(?
(a1), ?
(a2)) += 18: end for9: end for10: end foranswer(y, ?, ?)
for all distinct entities x, y,and z.
If we were to simply keep the entities asthey are and calculate raw co-occurrence counts,we would get evidence only for x = mary,y = bob, and z = question.One approach to this problem would be to de-ploy one of many previously described StatisticalRelational Learning methods, for example Logi-cal Hidden Markov Models (Kersting et al., 2006)or Relational Markov Models (Anderson et al.,2002).
These methods can learn various statisti-cal relationships between relational logical atomswith variables, of the sort considered here.
How-ever, we investigate a simpler option.The most important relationship between theentities in two multi-argument events concernstheir overlapping entities.
For example, to de-scribe the relationship between the three entitiesin (2), it is most important to note that the objectof the first event is identical with the subject of thesecond (namely, both are bob).
The identity of thenon-overlapping entities mary and question is notimportant for capturing the relationship betweenthe two events.We note that two multi-argument eventsv(es, eo, ep) and v?
(e?s, e?o, e?p), share at most threeentities.
We thus introduce four variables x, y, z,and O.
The three variables x, y, and z repre-sent arbitrary distinct entities, and the fourth, O,stands for ?Other,?
for entities not shared betweenthe two events.
We can rewrite the entities in ourtwo multi-argument events using these variables,with the constraint that two identical (i.e.
corefer-ent) entities must be mapped to the same variablein {x, y, z}, and no two distinct entities may mapto the same variable in {x, y, z}.
This formulationsimplifies calculations while still capturing pair-wise entity relationships between events.Algorithm 1 gives the pseudocode for the learn-222ing method.
This populates a co-occurrencematrix N , where entry N(a1, a2) gives the co-occurrence count of events a1and a2.
The vari-able evs in line 1 is the set of all events in ourmodel, which are of the form v(es, eo, ep), with va verb lemma and es, eo, ep?
{x, y, z, O}.
Thevariable documents in line 4 is the collectionof documents in our training corpus.
The func-tion cooccurEvs in line 5 takes a document Dand returns all ordered pairs of co-occurring eventsin D, where, following the 2-skip bigram modelof Jans et al.
(2012), and similar to Balasubrama-nian et al.
(2012; 2013), two events a1and a2aresaid to co-occur if they occur in order, in the samedocument, with at most two intervening events be-tween them.1The function subs in line 6 takestwo events and returns all variable substitutions ?mapping from entities mentioned in the events a1and a2to the set {x, y, z, O}, such that two coref-erent entities map to the same element of {x, y, z}.A substitution ?
applied to an event v(es, eo, ep),as in line 7, is defined as v(?
(es), ?
(eo), ?
(ep)),with the null entity mapped to itself.Once we have calculatedN(a1, a2) using Algo-rithm 1, we may define P (a1, a2) for two eventsa1and a2, giving an estimate for the probabilityof observing a2occurring after a1, asP (a1, a2) =N(a1, a2)?a?1,a?2N(a?1, a?2).
(3)We may then define the conditional probability ofseeing a2after a1, given an observation of a1:P (a2|a1) =P (a1, a2)?a?P (a1, a?
)=N(a1, a2)?a?N(a1, a?).
(4)3.2.2 Inferring EventsSuppose we have a sequence of multi-argumentevents extracted from a document.
A natural taskfor a statistical script model is to infer what otherevents likely occurred, given the events explic-itly stated in a document.
Chambers and Jurafsky(2008; 2009) treat the events involving an entityas an unordered set, inferring the most likely ad-ditional event, with no relative ordering betweenthe inferred event and known events.
We adoptthe model of Jans et al.
(2012), which was demon-strated to give better empirical performance.
This1Other notions of co-occurrence could easily be substi-tuted here.model takes an ordered sequence of events anda position in that sequence, and guesses eventsthat likely occurred at that position.
In that work,events are (verb, dependency) pairs, and an eventsequence consists of all such pairs involving a par-ticular entity.
We use this model in the multi-argument event setting, in which a document pro-duces a single sequence of multi-argument events.LetA be an ordered list of events, and let p be aninteger between 1 and |A|, the length ofA.
For i =1, .
.
.
, |A|, define aito be the ith element of A.We follow Jans et al.
(2012) by scoring a candidateevent a according to its probability of following allof the events before position p, and preceding allevents after position p. That is, we rank candidateevents a by maximizing S(a), defined asS(a) =p?1?i=1logP (a|ai) +|A|?i=plogP (ai|a) (5)with conditional probabilities P (a|a?)
calculatedusing (4).
Each event in ai?
A independentlycontributes to a candidate a?s score; the orderingbetween a and aiis taken into account, but the or-dering between the different events ai?
A doesnot directly affect a?s score.3.3 Baseline SystemsWe describe the baseline systems against whichwe compare the performance of the multi-argument script system described in section 3.2.These systems infer new events (either multi-argument or pair events) given the events con-tained in a document.Performance of these systems is measured usingthe narrative cloze task, in which we hold out a sin-gle event (either a multi-argument or pair event),and rate a system by its ability to infer this event,given the other events in a document.
The narra-tive cloze task is described in detail in Section 4.1.3.3.1 Random ModelThe simplest baseline we compare to is the ran-dom baseline, which outputs randomly selectedevents observed during training.
This model canguess either multi-argument or pair events.3.3.2 Unigram ModelThe unigram system guesses events ordered byprior probability, as calculated from the train-ing set.
If scripts are viewed as n-gram models223over events, this baseline corresponds to a bag-of-words unigram model.
In this model, events areassumed to occur independently, drawn from a sin-gle distribution.
This model can be used to guesseither multi-argument or pair events.3.3.3 Single Protagonist ModelWe refer to the system of Jans et al.
(2012) as thesingle protagonist system.
This model takes asingle sequence of (verb, dependency) pair events,all relating to a single entity.
It then producesa list of pair events, giving the model?s top pre-dictions for additional events involving the entity.This model maximizes the objective given in (5),with the sequence A (and the candidate guesses a)comprised of pair events.3.3.4 Multiple Protagonist ModelThe multiple protagonist system infers multi-argument events.
While this method is not de-scribed in previous work, it is the most direct wayof guessing a full multi-argument event using asingle protagonist script model.The multiple protagonist system uses a single-protagonist model, which models pair events, topredict multi-argument events, given a sequenceof known multi-argument events.
Suppose wehave a non-empty set E of entities mentioned inthe known events.
We describe the most directmethod of using a single-protagonist system to in-fer additional multi-argument events involving E.A multi-argument event a = v(es, eo, ep) repre-sents three pairs: (v, es), (v, eo), and (v, ep).
Themultiple protagonist model scores an event a ac-cording to the score the single protagonist modelassigns to these three pairs individually.For entity e ?
E in some multi-argument eventin a document, we first extract the sequence of(verb, dependency) pairs corresponding to e fromall known multi-argument events.
For a pair d,we calculate the score Se(d), the score the sin-gle protagonist system assigns the pair d, given theknown pairs corresponding to e. If e has no knownpairs corresponding to it (in the cloze evaluationdescribed below, this will happen if e occurs onlyin the held-out event), we fall back to calculatingSe(d) with a unigram model, as described in Sec-tion 3.3.2, over (verb, dependency) pair events.We then rank a multi-argument event a =v(es, eo, ep), with es, eo, ep?
E, with the follow-ing objective function:M(a) =Ses((v, subj)) + Seo((v, obj))+Sep((v, prep)) (6)where, for null entity e, we define Se(d) = 0 forall d. In the cloze evaluation, E will be the entitiesin the held-out event.
Each entity in a contributesindependently to the score M(a), based on theknown (verb, dependency) pairs involving that en-tity.
This model scores a multi-argument event aby combining one independent single-protagonistmodel for every entity in a.This model is similar to the multi-participantnarrative schemas described in Chambers and Ju-rafsky (2009), but whereas they infer bare verbs,we infer an entire multi-argument event.4 Evaluation4.1 Evaluation TaskWe follow previous work in using the narrativecloze task to evaluate statistical scripts (Chambersand Jurafsky, 2008; Chambers and Jurafsky, 2009;Jans et al., 2012).
The task is as follows: givena sequence of events a1, .
.
.
, anfrom a document,hold out some event apand attempt to predict thatevent, given the other events in the sequence.
Aswe cannot automatically evaluate the prediction oftruly unmentioned events in a document, this eval-uation acts as a straightforward proxy.In the aforementioned work, the cloze task isto guess a pair event, given the other events inwhich the held-out pair?s entity occurs.
In Section4.2.2, we evaluate directly on this task of guess-ing pair events.
However, in Section 4.2.1, weevaluate on the task of guessing a multi-argumentevent, given all other events in a document and theentities mentioned in the held-out event.
This is,we argue, the most natural way to adapt the clozeevaluation to the multi-argument event setting: in-stead of guessing a held-out pair event based onthe other events involving its lone entity, we willguess a held-out multi-argument event based onthe other events involving any of its entities.A document may contain arbitrarily many enti-ties.
The script model described in Section 3.2.1,however, only models events involving entitiesfrom a closed class of four variables {x, y, z, O}.We therefore rewrite entities in a document?s se-quences of events to the variables {x, y, z, O} ina way that maintains all pairwise relationships be-tween the held-out event and others.
That is, if the224held-out event shares an entity with another event,this remains true after rewriting.We perform entity rewriting relative to a singleheld-out event, proceeding as follows:?
Any entity in the held-out event that is men-tioned at least once in another event getsrewritten consistently to one of x, y, or z,such that distinct entities never get rewrittento the same variable.?
Any entity mentioned only in the held-outevent is rewritten as O.?
All entities not present in the held-out eventare rewritten as O.This simplification removes structure from theoriginal sequence, but retains the important pair-wise entity relationships between the held-outevent and the other events.4.2 Experimental EvaluationFor each document, we use the Stanford depen-dency parser (De Marneffe et al., 2006) to get syn-tactic information about the document; we thenuse the Stanford coreference resolution engine(Raghunathan et al., 2010) to get (noisy) equiva-lence classes of coreferent noun phrases in a doc-ument.2We train on approximately 1.1M arti-cles from years 1994-2006 of the NYT portionof the Gigaword Corpus, Third Edition (Graff etal., 2007), holding out a random subset of the arti-cles from 1999 for development and test sets.
Ourtest set consists of 10,000 randomly selected held-out events, and our development set is 500 disjointrandomly selected held-out events.
To remove du-plicate documents, we hash the first 500 charactersof each article and remove any articles with hashcollisions.
We use add-one smoothing on all jointprobabilities.
To reduce the size of our model, weremove all events that occur fewer than 50 times.3We evaluate performance using the followingtwo metrics:1.
Recall at 10: Following Jans et al.
(2012),we measure performance by outputting thetop 10 guesses for each held-out event andcalculating the percentage of such lists con-2We use version 1.3.4 of the Stanford CoreNLP system.3A manual inspection reveals that the majority of theseremoved events come from noisy text or parse errors.taining the correct answer.4This value willbe between 0 and 1, with 1 indicating perfectsystem performance.2.
Accuracy: A multi-argument eventv(es, eo, ep) has four components; a pairevent has two components.
For a held-outevent, we may judge the accuracy of asystem?s top guess by giving one point forgetting each of its components correct anddividing by the number of possible points.We average this value over the test set,yielding a value between 0 and 1, with 1indicating perfect system performance.
Thisis a novel evaluation metric for the scriptlearning task.These metrics target a system?s most confidentpredicted events: we argue that a script system isbest evaluated by its top inferences.In Section 4.2.1, we evaluate on the task of in-ferring multi-argument events.
In Section 4.2.2,we evaluate on the task of guessing pair events.4.2.1 System Comparison on Multi-argumentEventsWe first compare system performance on inferringmulti-argument events, evaluated on the narrativecloze task as described in Section 4.1, using thecorpora and metrics described in Section 4.2.
Wecompare against three baselines: the uninformedrandom baseline from Section 3.3.1, the unigramsystem from 3.3.2, and the multiple protagonistsystem from Section 3.3.4.The joint system guesses the held-out event,given the other events in the document that involvethe entities in that held-out tuple.
The system or-ders candidate events a by their scores S(a), asgiven in Equation (5).
This is the primary sys-tem described in this paper, modeling full multi-argument events directly.Table 1 gives the recall at 10 (?R@10?)
and ac-curacy scores for the different systems.
The uni-gram system is quite competitive, achieving per-formance comparable to the multiple protagonistsystem on accuracy, and superior performance onrecall at 10.Evaluating by the recall at 10 metric, the jointsystem provides a 2.9% absolute (13.2% relative)improvement over the unigram system, and a 3.6%4Jans et al.
(2012) instead use recall at 50, but we observe,as they also report, that the comparative differences betweensystems using recall at k for various values of k is similar.225Method R@10 AccuracyRandom 0.001 0.334Unigram 0.216 0.507Multiple Protagonist 0.209 0.504Joint 0.245 0.549Table 1: Results for multi-argument events.absolute (17.2% relative) improvement over themultiple protagonist system.
These differencesare statistically significant (p < 0.01) by McNe-mar?s test.
By accuracy, the joint system providesa 4.2% absolute (8.3% relative) improvement overthe unigram model, and a 4.5% absolute (8.9%relative) improvement over the multiple protago-nist model.
Accuracy differences are significant(p < 0.01) by a Wilcoxon signed-rank test.These results provide evidence that directlymodeling full multi-argument events, as opposedto modeling chains of (verb, dependency) pairs forsingle entities, allows us to better infer held-outverbs with all participating entities.4.2.2 System Comparison on Pair EventsIn Section 4.2.1, we adapted a baseline pair-eventsystem to the task of guessing multi-argumentevents.
We may also do the converse, adapting ourmulti-argument event system to the task of guess-ing the simpler pair events.
That is, we infer a fullmulti-argument event and extract from it a (sub-ject,verb) pair relating to a particular entity.
Thisallows us to compare directly to previously pub-lished methods.The random, unigram, and single protagonistsystems are pair-event systems described in Sec-tions 3.3.1, 3.3.2, and 3.3.3, respectively.
Thejoint pair system takes the multi-argument eventsguessed by the joint system of Section 4.2.1 andconverts them to pair events by discarding any in-formation not related to the target entity; that is, ifthe held-out pair event relates to an entity e, thenevery occurrence of e as an argument of a guessedmulti-argument event will be converted into a sin-gle pair event, scored identically to its originalmulti-argument event.
Ties are broken arbitrarily.Table 2 gives the comparative results for thesefour systems.
The test set is constructed by ex-tracting one pair event from each of the 10,000multi-argument events in the test set used in Sec-tion 4.2.1, such that the extracted pair event relatesto an entity with at least one additional known pairMethod R@10 AccuracyRandom 0.001 0.495Unigram 0.297 0.552Single Protagonist 0.282 0.553Joint Pair 0.336 0.561Table 2: Results for pair events.event.
Evaluating by recall at 10, the joint sys-tem provides a 3.9% absolute (13.1% relative) im-provement over the unigram baseline, and a 5.4%absolute (19.1% relative) improvement over thesingle protagonist system.
These differences aresignificant (p < 0.01) by McNemar?s test.
Byaccuracy, the joint system provides a 0.9% abso-lute (1.6% relative) improvement over the unigrammodel, and a 0.8% absolute (1.4% relative) im-provement over the single protagonist model.
Ac-curacy differences are significant (p < 0.01) by aWilcoxon signed-rank test.These results indicate that modeling multi-argument event sequences allows better inferenceof simpler pair events.
These performance im-provements may be due to the fact that the jointmodel conditions on information not representablein the single protagonist model (namely, all of theevents in which a multi-argument event?s entitiesare involved).5 Related WorkThe procedural encoding of common situationsfor automated reasoning dates back decades.
Theframes of Minsky (1974), schemas of Rumelhart(1975), and scripts of Schank and Abelson (1977)are early examples.
These models use quite com-plex representations for events, with many differ-ent relations between events.
They are not statis-tical, and use separate models for different scenar-ios (e.g.
the ?restaurant script?
is different fromthe ?bank script?).
Generally, they require humansto encode procedural information by hand; see,however, Mooney and DeJong (1985) for an earlymethod for learning scripts automatically from adocument.
Miikkulainen (1990; 1993) gives a hi-erarchical Neural Network system which storessequences of events from text in episodic memory,capable of simple question answering.Regneri et al.
(2010) and Li et al.
(2012)give methods for using crowdsourcing to cre-ate situation-specific scripts.
These methods226help alleviate the bottleneck of the knowledge-engineering required for traditionally conceivedscript systems.
These systems are precision-oriented: they create small, highly accurate scriptsfor very limited scenarios.
The current work,in contrast, focuses on building high-recall mod-els of general event sequences.
There are also anumber of systems addressing the related problemof modeling domain-specific human-human dia-log for building dialog systems (Bangalore et al.,2006; Chotimongkol, 2008; Boyer et al., 2009).There have been a number of recent approachesto learning statistical scripts.
Chambers and Ju-rafsky (2008) and Jans et al.
(2012) give methodsfor learning models of (verb, dependency) pairs,as described above.
Manshadi et al.
(2008) givean n-gram model for sequences of verbs and theirpatients.
McIntyre and Lapata (2009; 2010) usescript objects learned from corpora of fairy talesto automatically generate stories.
Chambers andJurafsky (2009) extend their previous model toincorporate multiple entities, but do not directlymodel the different arguments of an event.
Bam-man et al.
(2013) learn latent character personasfrom film summaries, associating character typeswith stereotypical actions; they focus on identify-ing persona types, rather than event inference.Manshadi et al.
(2008) and Balasubramanian etal.
(2012; 2013) give approaches similar to thecurrent work for modeling sequences of events asn-grams.
These methods differ from the currentwork in that they do not model entities directly, in-stead modeling co-occurrence of particular nounsstanding as arguments to particular verbs.
Lewisand Steedman (2013) build clusters of relationssimilar to these events, finding such clusters help-ful to question answering and textual inference.There has also been recent interest in the relatedproblem of automatically learning event frames(Bejan, 2008; Chambers and Jurafsky, 2011; Che-ung et al., 2013; Chambers, 2013).
These ap-proaches focus on identifying frames for infor-mation extraction tasks, as opposed to inferringevents directly.
Balasubramanian et al.
(2013) givean event frame identification method, developed inparallel with the current work, using sequences oftuples similar to our multi-argument events, notingcoherence issues with pair events.
Their formu-lation differs from ours primarily in that they donot incorporate coreference information into theirevent co-occurrence distribution, and evaluate us-ing human judgments of frame coherence ratherthan a narrative cloze test.6 Future WorkWe have evaluated only one type of multi-argument event inference, in which a script infersan event given a set of entities and the events in-volving those entities.
We claim that this is themost natural adaptation of the cloze evaluation tothe multi-argument event setting.
However, othertypes of inferences would be useful as well forquestion-answering.
Additional script inferences,and their applications to question answering, areworth investigating more fully.The evaluation methodology used here has twoserious benefits: it is totally automatic, and it doesnot require labeled data.
The cloze evaluation isintuitively reasonable: a good script system shouldbe able to predict stated events as having takenplace.
Basic pragmatic reasoning, however, tellsus that the most obvious inferable events are nottypically stated in text.
This evaluation thus failsto capture some of the most important common-sense inferences.
Further investigation into evalu-ation methodologies for script systems is needed.7 ConclusionWe described multi-argument events for statisti-cal scripts, which can directly encode the pair-wise entity relationships between events in a doc-ument.
We described a script model that can han-dle the important aspects of the additional com-plexity introduced by these events, and a baselinemodel that can infer multi-argument events usingsingle-protagonist chains instead of directly mod-eling full relations.
We introduced the novel uni-gram baseline model for comparison, as well asthe novel accuracy metric, and provided empir-ical evidence that modeling full multi-argumentevents provides more predictive power than mod-eling event chains individually.AcknowledgmentsThanks to Katrin Erk, Amelia Harrison, and theDEFT group at UT Austin for helpful discussions.Thanks also to the anonymous reviewers for theirhelpful comments.
This research was supported inpart by the DARPA DEFT program under AFRLgrant FA8750-13-2-0026.
Some of our experi-ments were run on the Mastodon Cluster, sup-ported by NSF Grant EIA-0303609.227ReferencesCorin R Anderson, Pedro Domingos, and Daniel SWeld.
2002.
Relational Markov models and theirapplication to adaptive web navigation.
In Proceed-ings of the Eighth ACM SIGKDD International Con-ference on Knowledge Discovery and Data Mining(KDD-2002), pages 143?152.Niranjan Balasubramanian, Stephen Soderland,Mausam, and Oren Etzioni.
2012.
Rel-grams: aprobabilistic model of relations in text.
In Proceed-ings of the Joint Workshop on Automatic KnowledgeBase Construction and Web-scale KnowledgeExtraction at NAACL-HLT 2012 (AKBC-WEKEX2012), pages 101?105.Niranjan Balasubramanian, Stephen Soderland,Mausam, and Oren Etzioni.
2013.
Generatingcoherent event schemas at scale.
In Proceedingsof the 2013 Conference on Empirical Methods inNatural Language Processing (EMNLP-2013).David Bamman, Brendan O?Connor, and Noah A.Smith.
2013.
Learning latent personas of film char-acters.
In Proceedings of the 51st Annual Meet-ing of the Association for Computational Linguistics(ACL-13), pages 352?361.Srinivas Bangalore, Giuseppe Di Fabbrizio, andAmanda Stent.
2006.
Learning the structure of task-driven human?human dialogs.
In Proceedings ofthe 21st International Conference on ComputationalLinguistics and 44th Annual Meeting of the Associa-tion for Computational Linguistics (COLING/ACL-06), pages 201?208.Cosmin Adrian Bejan.
2008.
Unsupervised discov-ery of event scenarios from texts.
In Prodeedings ofthe 21st International Florida Artificial IntelligenceResearch Society Conference (FLAIRS-2008), pages124?129.Kristy Elizabeth Boyer, Robert Phillips, Eun YoungHa, Michael D. Wallis, Mladen A. Vouk, andJames C. Lester.
2009.
Modeling dialogue structurewith adjacency pair analysis and Hidden MarkovModels.
In Proceedings of Human Language Tech-nologies: The Conference of the North AmericanChapter of the Association for Computational Lin-guistics, Companion Volume: Short Paper (NAACL-HLT-09 Short), pages 49?52.Nathanael Chambers and Daniel Jurafsky.
2008.
Un-supervised learning of narrative event chains.
InProceedings of the 46th Annual Meeting of the As-sociation for Computational Linguistics (ACL-08),pages 789?797.Nathanael Chambers and Dan Jurafsky.
2009.
Un-supervised learning of narrative schemas and theirparticipants.
In Joint Conference of the 47th An-nual Meeting of the Association for ComputationalLinguistics and the 4th International Joint Confer-ence on Natural Language Processing of the AsianFederation of Natural Language Processing (ACL-IJCNLP), pages 602?610.Nathanael Chambers and Dan Jurafsky.
2011.Template-based information extraction without thetemplates.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguis-tics: Human Language Technologies (ACL-HLT-11), pages 976?986.Nathanael Chambers.
2013.
Event schema induc-tion with a probabilistic entity-driven model.
InProceedings of the 2013 Conference on EmpiricalMethods in Natural Language Processing (EMNLP-2013).Jackie Chi Kit Cheung, Hoifung Poon, and Lucy Van-derwende.
2013.
Probabilistic frame induction.
InProceedings of the 2013 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies(NAACL-13).Ananlada Chotimongkol.
2008.
Learning the struc-ture of task-oriented conversations from the corpusof in-domain dialogs.
Ph.D. thesis, Carnegie MellonUniversity.Marie-Catherine De Marneffe, Bill MacCartney,Christopher D Manning, et al.
2006.
Generat-ing typed dependency parses from phrase structureparses.
In Proceedings of the 5th InternationalConference on Language Resources & Evaluation(LREC-2006), volume 6, pages 449?454.David Graff, Junbo Kong, Ke Chen, and KazuakiMaeda.
2007.
English Gigaword Third Edition.Linguistic Data Consortium.Bram Jans, Steven Bethard, Ivan Vuli?c, andMarie Francine Moens.
2012.
Skip n-gramsand ranking functions for predicting script events.In Proceedings of the 13th Conference of the Euro-pean Chapter of the Association for ComputationalLinguistics (EACL-12), pages 336?344.Kristian Kersting, Luc De Raedt, and Tapani Raiko.2006.
Logical Hidden Markov Models.
Journal ofArtificial Intelligence Research, 25:425?456.Mike Lewis and Mark Steedman.
2013.
Combineddistributional and logical semantics.
Transactionsof the Association for Computational Linguistics,1:179?192.Boyang Li, Stephen Lee-Urban, Darren Scott Appling,and Mark O Riedl.
2012.
Crowdsourcing narrativeintelligence.
Advances in Cognitive Systems, 2:25?42.Mehdi Manshadi, Reid Swanson, and Andrew S Gor-don.
2008.
Learning a probabilistic model of eventsequences from internet weblog stories.
In Prodeed-ings of the 21st International Florida Artificial In-telligence Research Society Conference (FLAIRS-2008), pages 159?164.228Neil McIntyre and Mirella Lapata.
2009.
Learn-ing to tell tales: A data-driven approach to storygeneration.
In Joint Conference of the 47th An-nual Meeting of the Association for ComputationalLinguistics and the 4th International Joint Confer-ence on Natural Language Processing of the AsianFederation of Natural Language Processing (ACL-IJCNLP), pages 217?225.Neil McIntyre and Mirella Lapata.
2010.
Plot induc-tion and evolutionary search for story generation.
InProceedings of the 48th Annual Meeting of the As-sociation for Computational Linguistics (ACL-10),pages 1562?1572.Risto Miikkulainen.
1990.
DISCERN: A DistributedArtificial Neural Network Model of Script Process-ing and Memory.
Ph.D. thesis, University of Cali-fornia.Risto Miikkulainen.
1993.
Subsymbolic Natural Lan-guage Processing: An Integrated Model of Scripts,Lexicon, and Memory.
MIT Press, Cambridge, MA.Marvin Minsky.
1974.
A framework for representingknowledge.
Technical report, MIT-AI Laboratory.Raymond J. Mooney and Gerald F. DeJong.
1985.Learning schemata for natural language processing.In Proceedings of the Ninth International Joint Con-ference on Artificial Intelligence (IJCAI-85), pages681?687, Los Angeles, CA, August.Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-garajan, Nathanael Chambers, Mihai Surdeanu, DanJurafsky, and Christopher Manning.
2010.
A multi-pass sieve for coreference resolution.
In Proceed-ings of the Conference on Empirical Methods in Nat-ural Language Processing (EMNLP-2010), pages492?501.Altaf Rahman and Vincent Ng.
2012.
Resolvingcomplex cases of definite pronouns: the Winogradschema challenge.
In Proceedings of the 2012 Con-ference on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning (EMNLP-CoNLL-12), pages 777?789.Michaela Regneri, Alexander Koller, and ManfredPinkal.
2010.
Learning script knowledge with webexperiments.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Lin-guistics (ACL-10), Uppsala, Sweden, July.David Rumelhart.
1975.
Notes on a schema for sto-ries.
Representation and Understanding: Studies inCognitive Science.Roger C. Schank and Robert P. Abelson.
1977.
Scripts,Plans, Goals and Understanding: An Inquiry intoHuman Knowledge Structures.
Lawrence Erlbaumand Associates, Hillsdale, NJ.229
