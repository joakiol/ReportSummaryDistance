Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 154?157,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsTask 5: Single document keyphrase extraction using sentence clusteringand Latent Dirichlet AllocationClaude PasquierInstitute of Developmental Biology & CancerUniversity of Nice Sophia-AntipolisUNSA/CNRS UMR-6543Parc Valrose06108 NICE Cedex 2, Franceclaude.pasquier@unice.frAbstractThis paper describes the design of a sys-tem for extracting keyphrases from a sin-gle document The principle of the algo-rithm is to cluster sentences of the doc-uments in order to highlight parts of textthat are semantically related.
The clustersof sentences, that reflect the themes of thedocument, are then analyzed to find themain topics of the text.
Finally, the mostimportant words, or groups of words, fromthese topics are proposed as keyphrases.1 IntroductionKeyphrases are words, or groups of words, thatcapture the key ideas of a document.
They repre-sent important information concerning a documentand constitute an alternative, or a complement, tofull-text indexing.
Pertinent keyphrases are alsouseful to potential readers who can have a quickoverview of the content of a document and can se-lect easily which document to read.Currently, the most powerful keyphrases extrac-tion algorithms are based on supervised learning.These methods address the problem of associat-ing keyphrases to documents as a classificationtask.
However, the fact that this approach requiresa corpus of similar documents, which is not al-ways readily available, constitutes a major draw-back.
For example, if one encounters a new Webpage, one might like to know quickly the main top-ics addressed.
In this case, a domain-independentkeyword extraction system that applies to a singledocument is needed.Several methods have been proposed for ex-tracting keywords from a single document (Mat-suo and Ishizuka, 2004; Palshikar, 2007).
The re-ported performances were slightly higher than thatobtained using a corpus and selecting the wordswith the highest TF-IDF 1 measure (Salton et al,1975).The paper describes a new keyphrase extractionalgorithm from a single document.
We show thatour system performs well without the need for acorpus.The paper is organized as follows.
The next sec-tion describes the principles of our keyphrase ex-traction system.
We present the main parts of thealgorithm in section 3, we detail the methods insection 4 and we conclude the paper.2 PrinciplesWhen authors write documents, they have to thinkfirst at the way they will present their ideas.
Mostof the time, they establish content summaries thathighlight the main topics of their texts.
Then, theywrite the content of the documents by carefullyselecting the most appropriate words to describeeach topic.
In this paper, we make the assumptionthat the words, or the set of words, that are repre-sentative of each topic constitute the keyphrases ofthe document.
In the following of this paper, wecall terms, the components of a document that con-stitute the vocabulary (see the detail of the identi-fication of terms in subsection 4.3).In statistical natural language processing, onecommon way of modeling the contributions of dif-ferent topics to a document is to treat each topic asa probability distribution over words.
Therefore, adocument is considered as a probabilistic mixtureof these topics (Griffiths and Steyvers, 2004).Generative models can be used to relate a set ofobservations (in our case, the terms used in a doc-ument) to a set of latent variables (the topics).
Aparticular generative model, which is well suitedfor the modeling of text, is called Latent Dirichlet1The TF-IDF weight gives the degree of importance of aword in a collection of documents.
The importance increasesif the word is frequently used in the set of documents butdecreases if it is used by too many documents.154Allocation (LDA) (Blei et al, 2003).
Given a setof documents, the algorithms describes each doc-ument as a mixture over topics, where each topicis characterized by a distribution over words.The idea is to perform first a clustering of thesentences of the document based on their semanticsimilarity.
Intuitively, one can see each cluster asa part of the text dealing with semantically relatedcontent.
Therefore, the initial document is dividedinto a set of clusters and LDA can then be appliedon this new representation.3 AlgorithmThe algorithm is composed of 8 steps:1.
Identification and expansion of abbrevia-tions.2.
Splitting the content of the document into msentences.3.
Identification of the n unique terms in thedocument that are potential keyphrases.4.
Creation of a m ?
n sentence-term matrixX to identify the occurrences of the n termswithin a collection of m sentences.5.
Dimensionality reduction to transform data inthe high-dimensional matrix X to a space offewer dimensions.6.
Data clustering performed in the reducedspace.
The result of the clustering is used tobuild a new representation of the source doc-ument, which is now considered as a set ofclusters, with each cluster consisting of a bagof terms.7.
Execution of LDA on the new document rep-resentation.8.
Selection of best keyphrases by analyzingLDA?s results.4 MethodsOur implementation is build on UIMA (Un-structured Information Management Architecture)(http://incubator.apache.org/uima/ ), a robust andflexible framework that facilitates interoperabilitybetween tools dedicated to unstructured informa-tion processsing.
The method processes one doc-ument at a time by performing the steps describedbelow.4.1 Abbreviation ExpansionThe program ExtractAbbrev (Schwartz and Hearst,2003) is used to identify abbreviations (shortforms) and their corresponding definitions (longforms).
Once abbreviations have been identified,each short form is replaced by its correspondinglong form in the processed document.4.2 Sentence DetectionSplitting the content of a document into sentencesis an important step of the method.
To per-form this task, we used the OpenNLP?s sentencedetector module (http://opennlp.sourceforge.net/ )trained on a corpus of general English texts.4.3 Term IdentificationWord categories are identified by using the Ling-Pipe?s general English part-of-speech (POS) tag-ger trained on the Brown Corpus (http://alias-i.com/lingpipe/ ).
We leverage POS information tocollect, for each sentence, nominal groups that arepotential keyphrases.4.4 Matrix CreationLet D = {d1, d2, .
.
.
, dn} be the complete vo-cabulary set of the document identified in subsec-tion 4.3 above.
We build a m?n matrix X = [xij]where m is the number of sentences in the doc-ument, n is the number of terms and xijis theweight of the jthterm in the ithsentence.
Theweight of a term in a sentence is the product of alocal and global weight given by xij= lij?
gj,where lijis the local weight of term j within sen-tence i, and gjis the global weight of term j inthe document.
The local weighting function mea-sures the importance of a term within a sentenceand the global weighting function measures theimportance of a term across the entire document.Three local weighting functions were investigated:term frequency, log of term frequency and binary.Five global weighting functions were also inves-tigated: Normal, GFIDF (Global frequency ?
In-verse document frequency, IDF (Inverse documentfrequency), Entropy and none (details of calcula-tion can be found in Dumais (1991) paper).4.5 Dimensionality ReductionThe matrix X is a representation of a document ina high-dimensional space.
Singular Value Decom-position (SVD) (Forsythe et al, 1977) and Non-Negative Matrix Factorization (NMF) (Lee and155Seung, 1999) are two matrix decomposition tech-niques that can be used to transform data in thehigh-dimensional space to a space of fewer dimen-sions.With SVD, the original matrix X is decom-posed as a factor of three other matrices U , ?
andV such as:X = U?VTwhere U is an m?m matrix, ?
is a m?n diagonalmatrix with nonnegative real numbers on the diag-onal, and V T denotes the transpose of V , an n?nmatrix.
It is often useful to approximate X usingonly r singular values (with r < min(m,n)), sothat we have X = Ur?kVTr+ E, where E is anerror or residual matrix, Uris an m ?
r matrix,?ris a k ?
r diagonal matrix, and Vris an n ?
rmatrix.NMF is a matrix factorization algorithm thatdecomposes a matrix with only positive elementsinto two positive elements matrices, with X =WH+E.
Usually, only r components are fit, so Eis an error or residual matrix, W is a non-negativem ?
r matrix and H is a non-negative r ?
n ma-trix.
There are several ways in which W and Hmay be found.
In our system, we use Lee and Se-ung?s multiplicative update method (Lee and Se-ung, 1999).4.6 Sentence ClusteringThe clustering of sentences is performed in thereduced space by using the cosine similarity be-tween sentence vectors.
Several clustering tech-niques have been investigated: k-means cluster-ing, Markov Cluster Process (MCL) (Dongen,2008) and ClassDens (Gue?noche, 2004).The latent semantic space derived by SVD doesnot provide a direct indication of the data par-titions.
However, with NMF, the cluster mem-bership of each document can be easily identi-fied directly using the W matrix (Xu et al, 2003).Each value wijof matrix W , indicates, indeed, towhich degree sentence i is associated with clus-ter j.
If NMF was calculated with the rank r,then r clusters are represented on the matrix.
Weuse a simple rule to determine the content of eachcluster: sentence i belongs to cluster j if wij>a maxk?{1...m}wik.
In our system, we fixed a = 0.1.4.7 Applying Latent Dirichlet AllocationBy using the result of the clustering, the sourcedocument is now represented by c clusters ofterms.
The terms associated with a clus-ter ciis the sum of the terms belonging toall the sentences in the cluster.
JGibbLDA(http://jgibblda.sourceforge.net/ ) is used to exe-cute LDA on the new dataset.
We tried to ex-tract different numbers of topics t (with t ?
{2, 5, 10, 20, 50, 100}) and we choose the Dirich-let hyperparameters such as ?
= 0.1 and ?
=50/t.
LDA inferences a topic model by estimatingthe cluster-topic distribution ?
and the topic-worddistribution ?
(Blei et al, 2003).4.8 Term Ranking and Keyphrase SelectionWe assume that topics covering a significant por-tion of the document content are more importantthan those covering little content.
To reflect thisassumption, we calculate the importance of a termin the document (its score) with a function thattakes into account the distribution of topics overclusters given by ?, the distribution of terms overtopics given by ?
and the clusters?
size.score(i) = maxj?
{1...n}(?jic?k=1(?kjp(s(k)))where score(i) represents the score of term i ands(k) is the size of the cluster k. We tested threedifferent functions for p: the constant functionp(i) = 1, the linear function p(i) = i and theexponential function p(i) = i2.When a score is attributed to each term ofthe vocabulary, our system simply selects the topterms with the highest score and proposes them askeyphrases.4.9 Setting Tuning ParametersNumerous parameters have influence on themethod: the weighting of the terms in the doc-ument matrix, the dimension reduction methodused, the number of dimension retained, the clus-tering algorithm, the number of topics used to ex-ecute LDA and the way best keyphrases are se-lected.The parameter that most affects the perfor-mance is the method used to perform the dimen-sion reduction.
In all cases, whatever the otherparameters, NMF performs better than SVD.
Wefound that using only 10 components for the fac-torization is sufficient.
There was no significantperformance increase by using more factors.The second most important parameter is theclustering method used.
When NMF is used, the156best results were achieved by retrieving clustersfrom the W matrix.
With SVD, ClassDens getsthe best results.
We tested the performance of k-means clustering by specifying a number of clus-ters varying from 5 to 100.
The best performanceswere achieved with a number of clusters ?
20.However, k-means scores a little bit below Class-Dens and MCL is found to be the worst method.The choice of the global weighting function isalso important.
In our experiments, the use of IDFand no global weighting gave the worst results.Entropy and normal weighting gave the best re-sults but, on average, entropy performs a little bet-ter than normal weight.
In the final version, theglobal weighting function used is entropy.The last parameter that has a visible influenceon the quality of extracted keyphrases is the selec-tion of keyphrases from LDA?s results.
In our ex-periments, the exponential function performs best.The remaining parameters do not have notableinfluence on the results.
As already stated by Leeet al (2005), the choice of local weighting func-tion makes relatively little difference.
Similarly,the number of topics used for LDA has little in-fluence.
In our implementation we used term fre-quency as local weighting and executed LDA witha number of expected topics of 10.5 Results and ConclusionIn Task 5, participants are invited to provide thekeyphrases for 100 scientific papers provided bythe organizers.
Performances (precision, recalland F-score) are calculated by comparing the pro-posed keyphrases to keywords given by the au-thors of documents, keywords selected by inde-pendant readers and a combination of both.
Com-pared to other systems, our method gives thebest results on the keywords assigned by read-ers.
By performing the calculation on the first 5keyphrases, our system ranks 9th out of 20 submit-ted systems, with an F-score of 14.7%.
This is be-low the best method that obtains 18.2%, but abovethe TD-IDF baseline of 10.44%.
The same calcu-lation performed on the first 15 kephrases gives aF-score of 17.80% for our method (10th best F-score).
This is still below the best method, whichobtains an F-score of 23.50%, but a lot better thanthe TD-IDF baseline (F-score=12.87%).The evaluation shows that the performance ofour system is near the average of other submittedsystems.
However, one has to note that our systemuses only the information available from a singledocument.
Compared to a selection of keywordsbased on TF-IDF, which is often used as a refer-ence, our system provides a notable improvement.Therefore, the algorithm described here is an inter-esting alternative to supervised learning methodswhen no corpus of similar documents is available.ReferencesDavid M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alocation.
J. Mach.
Learn.Res., 3:993?1022.Stijn Van Dongen.
2008.
Graph clustering via a dis-crete uncoupling process.
SIAM J. Matrix Anal.Appl., 30(1):121?141.Susan T. Dumais.
1991.
Improving the retrieval of in-formation from external sources.
Behavior ResearchMethods, Instruments, & Comp., 23(2):229?236.George Forsythe, Michael Malcolm, and Cleve Moler.1977.
Computer Methods for Mathematical Com-putations.
Englewood Cliffs, NJ: Prentice Hall.Thomas L. Griffiths and Mark Steyvers.
2004.
Find-ing scientific topics.
Proceedings of the NationalAcademy of Sciences, 101:5228?5235.Alain Gue?noche.
2004.
Clustering by vertex densityin a graph.
In Classification, Clustering and DataMining, D. Banks et al (Eds.
), Springer, 15-23.Daniel D. Lee and H. Sebastian Seung.
1999.
Learningthe parts of objects by non-negative matrix factoriza-tion.
Nature, 401:788.Michael D. Lee, Brandon Pincombe, and MatthewWelsh.
2005.
A comparison of machine measuresof text document similarity with human judgments.In proceedings of CogSci2005, pages 1254?1259.Yutaka Matsuo and Mitsuru Ishizuka.
2004.
Keywordextraction from a single document using word co-occurrence statistical information.
Int.
Journal onArtificial Intelligence Tools, 13(1):157?169.Girish Keshav Palshikar.
2007.
Keyword extractionfrom a single document using centrality measures.LNCS, 4815/2007:503?510.G Salton, C. S. Yang, and C. T. Yu.
1975.
A theoryof term importance in automatic text analysis.
Jour-nal of the American Society for Information Science,26(1):33?44.Ariel S. Schwartz and Marti A. Hearst.
2003.
A simplealgorithm for identifying abbreviation definitions inbiomedical text.
In proceedings of PSB 2003, pages451?462.Wei Xu, Xin Liu, and Yihong Gong.
2003.
Documentclustering based on non-negative matrix factoriza-tion.
In proceedings of SIGIR 03, pages 267?273.157
