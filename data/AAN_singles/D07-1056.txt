Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
533?540, Prague, June 2007. c?2007 Association for Computational LinguisticsPhrase Reordering Model Integrating Syntactic Knowledge for SMTDongdong Zhang, Mu Li, Chi-Ho Li, Ming ZhouMicrosoft Research AsiaBeijing, China{dozhang,muli,chl,mingzhou}@microsoft.comAbstractReordering model is important for the sta-tistical machine translation (SMT).
Currentphrase-based SMT technologies are good atcapturing local reordering but not globalreordering.
This paper introduces syntacticknowledge to improve global reorderingcapability of SMT system.
Syntactic know-ledge such as boundary words, POS infor-mation and dependencies is used to guidephrase reordering.
Not only constraints insyntax tree are proposed to avoid the reor-dering errors, but also the modification ofsyntax tree is made to strengthen the capa-bility of capturing phrase reordering.
Fur-thermore, the combination of parse treescan compensate for the reordering errorscaused by single parse tree.
Finally, expe-rimental results show that the performanceof our system is superior to that of thestate-of-the-art phrase-based SMT system.1 IntroductionIn the last decade, statistical machine translation(SMT) has been widely studied and achieved goodtranslation results.
Two kinds of SMT system havebeen developed, one is phrase-based SMT and theother is syntax-based SMT.In phrase-based SMT systems (Koehn et al,2003; Koehn, 2004), foreign sentences are firstlysegmented into phrases which consists of adjacentwords.
Then source phrases are translated into tar-get phrases respectively according to knowledgeusually learned from bilingual parallel corpus.
Fi-nally the most likely target sentence based on acertain statistical model is inferred by combiningand reordering the target phrases with the aid ofsearch algorithm.
On the other hand, syntax-basedSMT systems (Liu et al, 2006; Yamada et al,2001) mainly depend on parse trees to completethe translation of source sentence.Figure 1: A reordering exampleAs studied in previous SMT projects, languagemodel, translation model and reordering model arethe three major components in current SMT sys-tems.
Due to the difference between the source andtarget languages, the order of target phrases in thetarget sentence may differ from the order of sourcephrases in the source sentence.
To make the trans-lation results be closer to the target language style,a mathematic model based on the statistic theory isconstructed to reorder the target phrases.
This sta-tistic model is called as reordering model.
Asshown in Figure 1, the order of the translations of????
and ???
is changed.
The order of theIPVPADVPNPDNP VPNPNNAD DEG??VV?
??
?
?the Euro the significant appreciation of533translation of ???/??
and ???/???
is al-tered as well.
The former reordering case with thesmaller distance is usually referred as local reor-dering and the latter with the longer distance reor-dering as global reordering.
Phrase-based SMTsystem can effectively capture the local word reor-dering information which is common enough to beobserved in training data.
But it is hard to modelglobal phrase reordering.
Although syntacticknowledge used in syntax-based SMT systems canhelp reorder phrases, the resulting model is usuallymuch more complicated than a phrase-based sys-tem.There have been considerable amount of effortsto improve the reordering model in SMT systems,ranging from the fundamental distance-based dis-tortion model (Och and Ney, 2004; Koehn et al,2003), flat reordering model (Wu, 1996; Zens et al,2004; Kumar et al, 2005), to lexicalized reorderingmodel (Tillmann, 2004; Kumar et al, 2005; Koehnet al, 2005), hierarchical phrase-based model(Chiang, 2005), and maximum entropy-basedphrase reordering model (Xiong et al, 2006).
Dueto the absence of syntactic knowledge in these sys-tems, the ability to capture global reordering know-ledge is not powerful.
Although syntax-based SMTsystems (Yamada et al, 2001; Quirk et al, 2005;Liu et al, 2006) are good at modeling global reor-dering, their performance is subject to parsing er-rors to a large extent.In this paper, we propose a new method to im-prove reordering model by introducing syntacticinformation.
Syntactic knowledge such as boun-dary of sub-trees, part-of-speech (POS) and depen-dency relation is incorporated into the SMT systemto strengthen the ability to handle global phrasereordering.
Our method is different from previoussyntax-based SMT systems in which the translationprocess was modeled based on specific syntacticstructures, either phrase structures or dependencyrelations.
In our system, syntactic knowledge isused just to decide where we should combine adja-cent phrases and what their reordering probabilityis.
For example, according to the syntactic infor-mation in Figure 1, the phrase translation combina-tion should take place between ????
and ???
?rather than between ???
and ????.
Moreover,the non-monotone phrase reordering should occurbetween ???/??
and ???/???
rather thanbetween ???/??
and ????.
We train a maxi-mum entropy model, which is able to integrate richsyntactic knowledge, to estimate phrase reorderingprobabilities.
To enhance the performance ofphrase reordering model, some modification on thesyntax trees are also made to relax the phrase reor-dering constraints.
Additionally, the combinationof other kinds of syntax trees is introduced to over-come the deficiency of single parse tree.
The expe-rimental results show that the performance of oursystem is superior to that of the state-of-art phrase-based SMT system.The roadmap of this paper is: Section 2 gives therelated work.
Section 3 introduces our model.
Sec-tion 4 explains the generalization of reorderingknowledge.
The procedures of training and decod-ing are described in Section 5 and Section 6 re-spectively.
The experimental results are shown inSection 7.
Section 8 concludes the paper.2 Related WorkThe Pharaoh system (Koehn et al, 2004) is wellknown as the typical phrase-based SMT system.
Itsreordering model is designed to penalize transla-tion according to jump distance regardless of lin-guistic knowledge.
This method just works well forlanguage pairs that trend to have similar word-orders and it has nothing to do with global reorder-ing.A straightforward reordering model used in (Wu,1996; Zens et al, 2004; Kumar et al, 2005) is toassign constant probabilities to monotone reorder-ing and non-monotone reordering, which can beflexible depending on the different language pairs.This method is also adopted in our system for non-peer phrase reordering.The lexicalized reordering model was studied in(Tillmann, 2004; Kumar et al, 2005; Koehn et al,2005).
Their work made a step forward in integrat-ing linguistic knowledge to capture reordering.
Buttheir methods have the serious data sparsenessproblem.Beyond standard phrase-based SMT system, aCKY style decoder was developed in (Xiong et al,2006).
Their method investigated the reordering ofany two adjacent phrases.
The limited linguisticknowledge on the boundary words of phrases isused to construct the phrase reordering model.
Thebasic difference to our method is that no syntacticknowledge is introduced to guide the global phrasereordering in their system.
Besides boundary534words, our phrase reordering model also integratesmore significant syntactic knowledge such as POSinformation and dependencies from the  syntax tree,which can avoid some intractable phrase reorder-ing errors.A hierarchical phrase-based model was pro-posed by (Chiang, 2005).
In his method, a syn-chronous CFG is used to reorganize the phrasesinto hierarchical ones and grammar rules are auto-matically learned from corpus.
Different from hiswork, foreign syntactic knowledge is introducedinto the synchronous grammar rules in our methodto restrict the arbitrary phrase reordering.Syntax-based SMT systems (Yamada et al,2001; Quirk et al, 2005; Liu et al, 2006) totallydepend on syntax structures to complete phrasetranslation.
They can capture global reordering bysimply swapping the children nodes of a parse tree.However, there are also reordering cases which donot agree with syntactic structures.
Furthermore,their model is usually much more complex than aphrase-based system.
Our method exactly attemptsto integrate the advantages of phrase-based SMTsystem and syntax-based SMT system to improvethe phrase reordering model.
Phrase translation inour system is independent of syntactic structures.3 The ModelIn our work, we focus on building a better reorder-ing model with the help of source parsing informa-tion.
Although we borrow some fundamental ele-ments from a phrase-based SMT system such asthe use of bilingual phrases as basic translation unit,we are more interested in introducing syntacticknowledge to strengthen the ability to handle glob-al reordering phenomena in translation.3.1 DefinitionsGiven a foreign sentence f and its syntactic parsetree T, each leaf in T corresponds to a single wordin f and each sub-tree of T exactly covers a phrasefi in f which is called as linguistic phrase.
Exceptlinguistic phrases, any other phrase is regarded asnon-linguistic phrase.
The height of phrase fi isdefined as the distance between the root node of Tand the root node of the maximum sub-tree whichexactly covers fi.
For example, in Figure 1 thephrase ????
has the maximum sub-tree rootingat ADJP and its height is 3.
The height of phrase???
is 4 since its maximum sub-tree roots atADBP instead of AD.
If two adjacent phrases havethe same height, we regard them as peer phrases.In our model, we make use of bilingual phrasesas well, which refer to source-target algned phrasepairs extracted using the same criterion as mostphrase-based systems (Och and Ney, 2004).3.2 ModelSimilar to the work in Chiang (2005), our transla-tion model can be formulated as a weighted syn-chronous context free grammar derivation process.Let D be a derivation that generates a bilingualsentence pair ?f, e?, in which f is the given sourcesentence, the statistical model that is used to pre-dict the translation probability p(e|f) is defined overDs as follows:?
?
?
?
?
?
?
???
?????
??
?
?
??,????????
,????
?where plm(e) is the language model, ?i(X ???,??
)is a feature function defined over the derivationrule X???,?
?, and ?i is its weight.Although theoretically it is ideal for translationreorder modeling by constructing a synchronouscontext free grammar based on bilingual linguisticparsing trees, it is generally a very difficult task inpractice.
In this work we propose to use a smallsynchronous grammar constructed on the basis ofbilingual phrases to model translation reorderprobability and constraints by referring to thesource syntactic parse trees.
In the grammar, thesource / target words serve as terminals, and thebilingual phrases and combination of bilingualphrases are presented with non-terminals.
Thereare two non-terminals in the grammar except thestart symbol S: Y and Z.
The general derivationrules are defined as follows:a) Derivations from non-terminal to non-terminals are restricted to binary branchingforms;b) Any non-terminals that derives a list of termin-als, or any combination of two non-terminals,if the resulting source string won?t cause anycross-bracketing problems in the source parsetree (it exactly corresponds to a linguisticphrase in binary parse trees), are reduced to Y;c) Otherwise, they are reduced to Z.Table 1 shows a complete list of derivation rulesin our synchronous context grammar.
The first ninegrammar rules are used to constrain phrase reor-535dering during phrase combination.
The last tworules are used to represent bilingual phrases.
Rule(10) is the start grammar rule to generate the entiresentence translation.Rule Name Rule ContentRule (1) Y?
?Y1Y2, Y1Y2?Rule (2) Y?
?Y1Y2, Y2Y1?Rule (3) Y?
?Z1Z2, Z1Z2?Rule (4) Y?
?Y1Z2,Y1Z2?Rule (5) Y?
?Z1Y2, Z1Y2?Rule (6) Z?
?Y1Z2, Y1Z2?Rule (7) Z?
?Z1Y2, Z1Y2?Rule (8) Z?
?Z1Z2, Z1Z2?Rule (9) Z?
?Y1Y2, Y1Y2?Rule (10) S?
?Y1,Y1?Rule (11) Z?
?Z1, Z1?Rule (12) Y?
?Y1,Y1?Table 1: Synchronous grammar rulesRule (1) and Rule (2) are only applied to two ad-jacent peer phrases.
Note that, according to theconstraints of foreign syntactic structures, onlyRule (2) among all rules in Table 1 can be appliedto conduct non-monotone phrase reordering in ourframework.
This can avoid arbitrary phrase reor-dering.
For example, as shown in Figure 1, Rule (1)is applied to the monotone combination of phrases????
and ??
?, and Rule (2) is applied to thenon-monotone combination of phrases ???/?
?and ???
/???.
However, the non-monotonecombination of ???
and ????
is not allowed inour method since there is no proper rule for it.Non-linguistic phrases are involved in Rule(3)~(9).
We do not allow these grammar rules fornon-monotone combination of non-peer phrases,which really harm the translation results as provedin experimental results.
Although these rules vi-olate the syntactic constraints, they not only pro-vide the option to leverage non-linguistic transla-tion knowledge to avoid syntactic errors but alsotake advantage of phrase local reordering capabili-ties.
Rule (3) and Rule (8) are applied to the com-bination of two adjacent non-linguistic phrases.Rule (4)~(7) deal with the situation where one is alinguistic phrase and the other is a non-linguisticphrase.
Rule (9) is applied to the combination oftwo adjacent linguistic phrases but their combina-tion result is not a linguistic phrase.Rule (11) and Rule (12) are applied to generatebilingual phrases learned from training corpus.Table 2 demonstrates an example how theserules are applied to translate the foreign sentence???/?/??/???
into the English sentence?the significant appreciation of the Euro?.Step Partial derivations Rule1 S?
?Y1, Y1?
(10)2 ?
?Y2Y3, Y3Y2?
(2)3 ?
?Y4Y5Y3, Y3Y5Y4?
(2)4 ????
Y5Y3, Y3Y5 the Euro?
(12)5 ????
?
Y3, Y3 of the Euro?
(12)6 ????
?
Y6Y7, Y6Y7 of the Euro?
(1)7 ????
?
??
Y7, the significantY7 of  the Euro?
(12)8 ????
?
??
?
?, the signifi-cant appreciation of  the Euro?
(12)Table 2: Example of application for rulesHowever, there are always other kinds of bilin-gual phrases extracted directly from training cor-pus, such as ??
?, the Euro?
and ??
??
?
?, ?s significant appreciation?, which can producedifferent candidate sentence translations.
Here, thephrase ??
??
???
is a non-linguistic phrase.The above derivations can also be rewritten asS?
?Y1, Y1???Y2Z3,Y2Z3???
??
Z3, the EuroZ3??????
??
?
?, the Euro ?s significantappreciation?, where Rule (10), (4), (12) and (11)are applied respectively.3.3 FeaturesSimilar to the default features in Pharaoh (Koehn,Och and Marcu 2003), we used following featuresto estimate the weight of our grammar rules.
Note536that different rules may have different features inour model.?
The lexical weights plex(?|?)
and plex(?|?)
esti-mating how well the words in ?
translate thewords in ?.
This feature is only applicable toRule (11) and Rule (12).?
The phrase translation weights pphr(?|?)
andpphr(?|?)
estimating how well the terminalwords of ?
translate the terminal words of ?,This feature is only applicable to Rule (11) andRule (12).?
A word penalty exp(|?|), where |?| denotes thecount of terminal words of ?.
This feature isonly applicable to Rule (11) and Rule (12).?
A penalty exp(1) for grammar rules analogousto Pharaoh?s penalty which allows the model tolearn a preference for longer or shorter deriva-tions.
This feature is applicable to all rules inTable 1.?
Score for applying the current rule.
This featureis applicable to all rules in Table 1.
We will ex-plain the score estimation in detail in Section3.4.3.4 Scoring of RulesBased on the syntax constraints and involved non-terminal types, we separate the grammar rules intothree groups to estimate their application scoreswhich are also treated as reordering probabilities.For Rule (1) and Rule (2), they strictly complywith the syntactic structures.
Given two peerphrases, we have two choices to use one of them.Thus, we use maximum entropy (ME) model algo-rithm to estimate their reordering probabilities sep-arately, where the boundary words of foreignphrases and candidate target translation phrases,POS information and dependencies are integratedas features.
As listed in Table 3, there are totallytwelve categories of features used to train the MEmodel.
In fact, the probability of Rule (1) is justequal to the supplementary probability of Rule (2),and vice versa.For Rule (3)~(9), according to the syntacticstructures, their application is determined sincethere is only one choice to complete reordering,which is similar to the ?glue rules?
in Chiang(2005).
Due to the appearance of non-linguisticphrases, non-monotone phrase reordering is notallowed in these rules.
We just assign these rules aconstant score trained using our implementation ofMinimum Error Rate Training (Och, 2003b),which is 0.7 in our system.For Rule (10)~(12), they are also determinedrules since there is no other optional rules compet-ing with them.
Constant score is simply assigned tothem as well, which is 1.0 in our system.Fea.
DescriptionLS1 First word of first foreign phraseLS2 First word of second foreign phraseRS1 Last word of first foreign phraseRS2 Last word of second foreign phraseLT1 First word of first target phraseLT2 First word of second target phraseRT1 Last word of first target phraseRT2 Last word of second target phraseLPosPOS of the node covering first foreignphraseRPosPOS of the node covering second foreignphraseCposPOS of the node covering the combina-tion of foreign phrasesDPDependency between the nodes coveringtwo single foreign phrases respectivelyTable 3: Feature categories used for ME model4 The Generalization of ReorderingKnowledge4.1 Enriching Parse TreesThe grammar rules proposed in Section 3 are onlyapplied to binary syntax tree nodes.
For n-ary syn-tax trees (n>2), some modification is needed togenerate more peer phrases.
As shown in Figure2(a), the syntactic tree of Chinese sentence ????
/????
/??
/??
?
(Guangdong/high-tech/products/export), parsed by the Stanford Pars-er (Klein, 2003), has a 3-ary sub-tree.
Referring toits English translation result ?export of high-techproducts in Guangdong?, we understand thereshould be a non-monotone combination betweenthe phrases ?????
and ?????/???.
How-ever, ?????/???
is not a linguistic phrase537though its component phrases ??????
and ????
are peer phrases.
To avoid the conflict with theRule (2), we just add some extra virtual nodes inthe n-ary sub-trees to make sure that only binarysub-trees survive in the modified parse tree.
Figure2(b) is the modification result of the syntactic treefrom Figure 2(a), where two virtual nodes with thenew distinguishable POS of M are added.In general, we add virtual nodes for each set ofthe continuous peer phrases and let them have thesame height.
Thus, for a n-ary sub-tree, there are?
??
?11 )(ni in= (n?1)2/2 virtual nodes being addedwhere n>2.
The phrases exactly covered by thevirtual nodes are called as virtual peer phrases.Figure 2: Example of syntax tree modification4.2 Combination of Parse TreesIt is well known that parse errors in syntactic treesalways are inescapable even if the state-of-the-artparser is used.
Incorrect syntactic knowledge mayharm the reordering probability estimation.
To mi-nimize the impact of parse error of a single tree,more parse trees are introduced.
To support thecombination of parse trees, the synchronousgrammar rules are applied independently, but theywill compete against each other with the effect ofother models such as language model.In our system, we combine the parse trees gen-erated respectively by Stanford parser (Klein, 2003)and a dependency parser developed by (Zhou,2000).
Compared with the Stanford parser, the de-pendency parser only conducts shallow syntacticanalysis.
It is powerful to identify the base NPs andbase VPs and their dependencies.
Additionally,dependency parser runs much faster.
For example,it took about three minutes for the dependencyparser to parse one thousand sentences with aver-age length of 25 words, but the Stanford parserneeds about one hour to complete the same work.More importantly, as shown in the experimentalresults, the dependency parser can achieve thecomparable quality of final translation results withStanford parser in our system.5 The DecoderWe developed a CKY style decoder to completethe sentence translation.
A two-dimension arrayCA is constructed to store all the local candidatephrase translation and each valid cell CAij in CAcorresponds to a foreign phrase where i is thephrase start position and j is the phrase end posi-tion.
The cells in CA are filled in a bottom-up way.Firstly we fill in smaller cells with the translationin bilingual phrases learned from corpus.
Then thecandidate translation in the larger cell CAij is gen-erated based on the content in smaller adjacentcells CAik and CAk+1j with the monotone combina-tion and non-monotone combination, where i?k?j.To reduce the cost of system resources, the wellknown pruning methods, such as histogram prun-ing, threshold pruning and recombination, are usedto only keep the top N candidate translation in eachcell.6 TrainingSimilar to most state-of-the-art phrase-based SMTsystems, we use the SRI toolkit (Stolcke, 2002) forlanguage model training and Giza++ toolkit (Ochand Ney, 2003) for word alignment.
For reorderingmodel training, two kinds of parse trees for eachforeign sentence in the training corpus were ob-tained through the Stanford parser (Klein, 2003)and a dependency parser (Zhou, 2000).
After that,we picked all the foreign linguistic phrases of thesame sentence according to syntactic structures.Based on the word alignment results, if the alignedtarget words of any two adjacent foreign linguisticphrases can also be formed into two valid adjacentphrase according to constraints proposed in thephrase extraction algorithm by Och (2003a), theywill be extracted as a reordering training sample.Finally, the ME modeling toolkit developed byZhang (2004) is used to train the reordering modelover the extracted samples.?
?NPNPNPNNNPNR JJADJPNNNP????
?M M?
?NPNPNPNNNPNR JJADJPNNNP?????????
(a) (b)???
?5387 Experimental Results and AnalysisWe conducted our experiments on Chinese-to-English translation task of NIST MT-05 on a3.0GHz system with 4G RAM memory.
The bilin-gual training data comes from the FBIS corpus.The Xinhua news in GIGAWORD corpus is usedto train a four-gram language model.
The devel-opment set used in our system is the NIST MT-02evaluation test data.For phrase extraction, we limit the maximumlength of foreign and English phrases to 3 and 5respectively.
But there is no phrase length con-straint for reordering sample extraction.
About1.93M and 1.1M reordering samples are extractedfrom the FBIS corpus based on the Stanford parserand the dependency parser respectively.
To reducethe search space in decoder, we set the histogrampruning threshold to 20 and relative pruning thre-shold to 0.1.In the following experiments, we compared oursystem performance with that of the other state-of-the-art systems.
Additionally, the effect of somestrategies on system performance is investigated aswell.
Case-sensitive BLEU-4 score is adopted toevaluate system performance.7.1 Comparing with Baseline SMT systemOur baseline system is Pharaoh (Koehn, 2004).Xiong?s system (Xiong, et al, 2006) which usedME model to train the reordering model is alsoregarded as a competitor.
To have a fair compari-son, we used the same language model and transla-tion model for these three systems.
The experimen-tal results are showed in Table 4.System Bleu ScorePharaoh 0.2487Xiong?s System 0.2616Our System 0.2737Table 4: Performance against baseline systemThese three systems are the same in that the fi-nal sentence translation results are generated by thecombination of local phrase translation.
Thus, theyare capable of local reordering but not global reor-dering.
The phrase reordering in Pharaoh dependsonly on distance distortion information which doesnot contain any linguistic knowledge.
The experi-mental result shows that the performance of bothXiong?s system and our system is better than thatof Pharaoh.
It proves that linguistic knowledge canhelp the global reordering probability estimation.Additionally, our system is superior to Xiong?ssystem in which only use phrase boundary wordsto guide global reordering.
It indicates that syntac-tic knowledge is more powerful to guide globalreordering than boundary words.
On the other hand,it proves the importance of syntactic knowledgeconstraints in avoiding the arbitrary phrase reorder-ing.7.2 Syntactic Error AnalysisRule (3)~(9) in Section 3 not only play the role tocompensate for syntactic errors, but also take theadvantage of the capability of capturing localphrase reordering.
However, the non-monotonecombination for non-peer phrases is really harmfulto system performance.
To prove these ideas, weconducted experiments with different constrains.Constraints Bleu ScoreAll rules in Table 1 used  0.2737Allowing the non-monotonecombination of non-peer phrases0.2647Rule (3)~(9) are prohibited 0.2591Table 5:  About non-peer phrase combinationFrom the experimental results shown in Table 5,just as claimed in other previous work, the combi-nation between non-linguistic phrases is useful andcannot be abandoned.
On the other hand, if we re-lax the constraint of non-peer phrase combination(that is, allowing non-monotone combination foron-peer phrases), some more serious errors in non-syntactic knowledge is introduced, thereby degrad-ing performance from 0.2737 to 0.2647.7.3 Effect of Virtual Peer PhrasesAs discussed in Section 4, for n-ary nodes (n>2) inthe original syntax trees, the relationship among n-ary sub-trees is always not clearly captured.
Togive them the chance of free reordering, we add thevirtual peer nodes to make sure that the combina-tion of a set of peer phrases can still be a peerphrase.
An experiment was done to compare withthe case where the virtual peer nodes were notadded to n-ary syntax trees.
The Bleu score539dropped to 26.20 from 27.37, which shows the vir-tual nodes have great effect on system performance.7.4 Effect of Mixed Syntax TreesIn this section, we conducted three experiments toinvestigate the effect of constituency parse tree anddependency parse tree.
Over the same platform, wetried to use only one of them to complete the trans-lation task.
The experimental results are shown inTable 6.Surprisingly, there is no significant difference inperformance.
The reason may be that both parsersproduce approximately equivalent parse results.However, the combination of syntax trees outper-forms merely only one syntax tree.
This suggeststhat the N-best syntax parse trees may enhance thequality of reordering model.Situation Bleu ScoreDependency parser only 0.2667Stanford parser only 0.2670Mixed parsing trees 0.2737Table 6: Different parsing tree8 Conclusion and Future WorkIn this paper, syntactic knowledge is introducedto capture global reordering of SMT system.
Thismethod can not only inherit the advantage of localreordering ability of standard phrase-based SMTsystem, but also capture the global reordering asthe syntax-based SMT system.
The experimentalresults showed the effectiveness of our method.In the future work, we plan to improve the reor-dering model by introducing N-best syntax treesand exploiting richer syntactic knowledge.ReferencesDavid Chiang.
2005.
A hierarchical phrase-based mod-el for statistical machine translation.
In Proceedingsof ACL 2005.Franz Josef Och.
2003a.
Statistical Machine Translation:From Single-Word Models to Alignment TemplatesThesis.Franz Josef Och.
2003b.
Minmum Error Rate Trainingin Statistical Machine Translation.
In Proceedingsfor ACL 2003.Franz Josef Och, Hermann Ney.
2003.
A SystematicComparison of Various Statistical Alignment Models.Computational Linguistics, 29:19-51.Franz Josef Och and Hermann Ney.
2004.
The align-ment template approach to statistical machine trans-lation.
Computational Linguistics, 30:417-449.Dan Klein and Christopher D. Manning.
2003.
AccurateUnlexicalized Parsing.
In Proceedings of ACL 2003.Philipp Koehn, Franz Joseph Och, and Daniel Marcu.2003.
Statistical Phrase-Based Translation.
In Pro-ceedings of HLT/NAACL 2003.Philipp Koehn.
2004.
Pharaoh: a Beam Search Decoderfor Phrased-Based Statistical Machine TranslationModels.
In Proceedings of AMTA 2004.Shankar Kumar and William Byrne.
2005.
Local phrasereordering models for statistical machine translation.In Proceedings of HLT-EMNLP 2005.Yang Liu, Qun Liu, Shouxun Lin.
2006.
Tree-to-StringAlignment Template for Statistical Machine Transla-tion.
In Proceedings of COLING-ACL 2006.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.Dependency treelet translation: Syntactically in-formed phrasal SMT.
In Proceedings of ACL 2005.Andreas Stolcke.
2002.
SRILM-An Extensible LanguageModeling Toolkit.
In Proceedings of ICSLP 2002.Christoph Tillmann.
2004.
A block orientation modelfor statistical machine translation.
In Proceedings ofHLT-NAACL 2004.Dekai Wu.
1996.
A Polynomial-Time Algorithm for Sta-tistical Machine Translation.
In Proceedings of ACL1996.Deyi Xiong, Qun Liu, and Shouxun Lin.
2006.
Maxi-mum Entropy Based Phrase Reordering Model forStatistical Machine Translation.
In Proceedings ofCOLING-ACL 2006.Kenji Yamada and Kevin Knight.
2001.
A syntax basedstatistical translation model.
In Proceedings of ACL2001.Le Zhang.
2004.
Maximum Entropy Modeling Toolkitfor Python and C++.
Available at http://homepages.inf.ed.ac.uk/s0450736/maxent_toolkit.html.R.
Zens, H. Ney, T. Watanabe, and E. Sumita.
2004.Reordering Constraints for Phrase-Based StatisticalMachine Translation.
In Proceedings of CoLing 2004.Ming Zhou.
2000.
A block-based robust dependencyparser for unrestricted Chinese text.
The secondChinese Language Processing Workshop attached toACL2000.540
