Proceedings of the Workshop on Distributional Semantics and Compositionality (DiSCo?2011), pages 21?28,Portland, Oregon, 24 June 2011. c?2011 Association for Computational LinguisticsDistributional Semantics and Compositionality 2011:Shared Task Description and ResultsChris BiemannUKP lab, Technical University of DarmstadtHochschulstr.
1064289 Darmstadt, Germanybiemann@tk.informatik.tu-darmstadt.deEugenie GiesbrechtFZI Forschungszentrum InformatikHaid-und-Neu-Str.
10-1476131 Karlsruhe, Germanygiesbrecht@fzi.deAbstractThis paper gives an overview of the sharedtask at the ACL-HLT 2011 DiSCo (Distribu-tional Semantics and Compositionality) work-shop.
We describe in detail the motivationfor the shared task, the acquisition of datasets,the evaluation methodology and the resultsof participating systems.
The task of assign-ing a numerical score for a phrase accord-ing to its compositionality showed to be hard.Many groups reported features that intuitivelyshould work, yet showed no correlation withthe training data.
The evaluation reveals thatmost systems outperform simple baselines, yethave difficulties in reliably assigning a compo-sitionality score that closely matches the goldstandard.
Overall, approaches based on wordspace models performed slightly better thanmethods relying solely on statistical associa-tion measures.1 IntroductionAny NLP system that does semantic processing re-lies on the assumption of semantic compositionality:the meaning of a phrase is determined by the mean-ings of its parts and their combination.
However,this assumption does not hold for lexicalized phrasessuch as idiomatic expressions, which causes troublesnot only for semantic, but also for syntactic process-ing (Sag et al, 2002).
In particular, while distribu-tional methods in semantics have proved to be veryefficient in tackling a wide range of tasks in naturallanguage processing, e.g., document retrieval, clus-tering and classification, question answering, queryexpansion, word similarity, synonym extraction, re-lation extraction, textual advertisement matching insearch engines, etc.
(see Turney and Pantel (2010)for a detailed overview), they are still strongly lim-ited by being inherently word-based.
While dictio-naries and other lexical resources contain multiwordentries, these are expensive to obtain and not avail-able for all languages to a sufficient extent.
Fur-thermore, the definition of a multiword varies acrossresources, and non-compositional phrases are oftenmerely a subclass of multiword units.This shared task addressed researchers that areinterested in extracting non-compositional phrasesfrom large corpora by applying distributional mod-els that assign a graded compositionality score to aphrase, as well as researchers interested in express-ing compositional meaning with such models.
Thescore denotes the extent to which the composition-ality assumption holds for a given expression.
Thelatter can be used, for example, to decide whetherthe phrase should be treated as a single unit in ap-plications.
We emphasized that the focus is on au-tomatically acquiring semantic compositionality andexplicitly did not invite approaches that employ pre-fabricated lists of non-compositional phrases.It is often the case that compositionality of aphrase depends on the context.
Though we haveused a sentence context in the process of construct-ing the gold standard, we have decided not to pro-vide it with the dataset: we have asked for a sin-gle compositionality score per phrase.
In an appli-cation, this could play the role of a compositional-ity prior that could, e.g., be stored in a dictionary.There is a long-living tradition within the research21community working on multiword units (MWUs) toautomatically classify MWUs into either composi-tional or non-compositional ones.
However, it hasbeen often noted that compositionality comes in de-grees, and a binary classification is not valid enoughin many cases (Bannard et al, 2003; Katz and Gies-brecht, 2006).
To the best of our knowledge, this hasbeen the first attempt to offer a dataset and a sharedtask that allows to explicitly evaluate the models ofgraded compositionality.2 Shared Task DescriptionFor the shared task, we aimed to get composition-ality scores for phrases frequently occurring in cor-pora.
Since distributional models need large corporato perform reliable statistics, and these statistics aremore reliable for frequent items, we chose to restrictthe candidate set to the most frequent phrases fromthe freely available WaCky1 web corpora (Baroni etal., 2009).
Those are currently downloadable for En-glish, French, German and Italian.
They have al-ready been automatically sentence-split, tokenized,part-of-speech (POS) tagged and lemmatized, whichreduces the load on both organizers and participantsthat decide to make use of these corpora.
Further,WaCky corpora provide a good starting point for ex-perimenting with distributional models due to theirsize, ranging between 1-2 billion tokens, and exten-sive efforts to make these corpora as clean as possi-ble.2.1 Candidate SelectionThere is a wide range of subsentential units that canfunction as a non-compositional construction.
Theseunits do not have to be realized continuously in thesurface realization and can consist of an arbitrarynumber of lexical items.
While it would be interest-ing to examine unrestricted forms of multiwords andcompositional phrases, we decided to restrict candi-date selection to certain grammatical constructionsto make the task more tangible.
Specifically, we useword pairs in the following relations:?
ADJ NN: Adjective modifying a noun, e.g.
?red herring?
or ?blue skies?1http://wacky.sslmit.unibo.it?
V SUBJ: Noun in subject position and verb,e..g. ?flies fly?
or ?people transfer (sth.)??
V OBJ: Noun in object position and verb, e.g.
?lose keys?, ?play song?While it is possible to extract the relations fairly ac-curately from parsed English text, there is ?
to ourknowledge ?
no reliable, freely available methodthat can tell verb-subjects from verb-objects for Ger-man.
Thus, we employed a three-step selectionprocedure for producing a set of candidate phrasesper grammatical relation and language that involvedheavy manual intervention.1.
Extract candidates using (possibly over-generating) patterns over part-of-speechsequences and sort by frequency2.
Manually select plausible candidates for thetarget grammatical relation in order of decreas-ing frequency3.
Balance the candidate set to select enough non-compositional phrasesFor English, we used the following POS pat-terns: ADJ NN: ?JJ* NN*?
; V SUBJ: ?NN* VV*?
;V OBJ: ?VV* DT|CD NN*?
and ?VV* NN*?.
Thestar * denotes continuation of tag labels: e.g.
VV*matches all tags starting with ?VV?, such as VV,VVD, VVG, VVN, VVP and VVZ.For German, we used ?ADJ* NN*?
for ADJ NN.For relations involving nouns and verbs, we ex-tracted all noun-verb pairs in a window of 4 tokensand manually filtered by relation on the aggregatedfrequency list.
Frequencies were computed on thelemma forms.This introduces a bias on the possible construc-tions that realize the target relations, especially forthe verb-noun pairs.
Further, the selection procedureis biased by the intuition of the person that performsthe selection.
We only admitted what we thoughtwere clear-cut cases (only nouns that are typicallyfound in subject respectively object position) to thecandidate set at this stage.Since non-compositional phrases are much lessin numbers than compositional phrases, we tried tosomewhat balance this in the third step in the se-lection.
If the candidates would have been randomly22selected, an overwhelming number of compositionalphrases would have rendered the task very hard toevaluate, since a baseline system predicting highcompositionality in all cases would have achieveda very high score.
We argue that since we are es-pecially interested in non-compositional phrases inthis competition, it is valid to bias the dataset in thisway.After we collected a candidate list, we randomlyselected seven sentences per candidate from the cor-pus.
Through manual filtering, we checked whetherthe target word pair was in fact found in the targetrelation in these sentences.
Further we removed in-complete and too long sentences, so that we endedup with five sentences per target phrase.
Some can-didate phrases that only occurred in very fixed con-texts (e.g.
disclaimers) or did not have enough well-formed sentences were removed in this step.Figure 1 shows the sentences for ?V OBJ: bucktrend?
as an example output of this procedure.2.2 AnnotationThe sample usages of target phrases now had to beannotated for compositionality.
We employed thecrowdsourcing service Amazon Turk2 for realizingthese annotations.
The advantage of crowdsourc-ing is its scalability through the large numbers ofworkers that are ready to perform small tasks forpay.
The disadvantage is that tasks usually cannot bevery complex, since quality issues (scammers) haveto be addressed either with test items or redundancyor both ?
mechanisms that only work for types oftasks where there is clearly a correct answer.Previous experiences in constructing linguisticannotations with Amazon Turk (Biemann and Ny-gaard, 2010) made us stick to the following two-stepprocedure that more or less ensured the quality ofannotation by hand-picking workers:1.
Gather high quality workers: In an open taskfor a small data sample with unquestionable de-cisions, we collected annotations from a largenumber of workers.
Workers were asked toprovide reasons for their decisions.
Workersthat performed well, gave reasons that demon-strated their understanding of the task and com-pleted a significant amount of the examples2http://www.mturk.comwere invited for a closed task.
Net pay was 2US cents for completing a HIT.2.
Get annotations for the real task: In the closedtask, only invited workers were admitted andredundancy was reduced to four workers perHIT.
Net pay was 3 US cents for completinga HIT.Figure 2 shows a sample HIT (human intelligencetask) for English on Amazon turk, including in-structions.
Workers were asked to enter a judgmentfrom 0-10 about the literacy of the highlighted tar-get phrase in the respective context.
For the Germandata, we used an equivalent task definition in Ger-man.All five contexts per target phrase were scored byfour workers each.
A few items were identified asproblematic by the workers (e.g.
missing highlight-ing, too little context), and one worker was excludedduring the English experiment for starting to delib-erately scam.
For this worker, all judgments were re-moved and not repeated.
Thus, the standard numberof judgments per target phrase was 20, with sometargets receiving less judgments because of theseproblems.
The minimum number of judgments pertarget phrase was 12: four HITs with three judg-ments each.From this, we computed a score by averaging overall judgments per phrase and multiplying the over-all score by 10 to get scores in the range of 0-100.This score cannot help in discriminating moderatelycompositional phrases like ?V OBJ: make decision?from phrases that are dependent on the context like?V OBJ: wait minute?
which had two HITs for theidiomatic use of ?wait a minute!?
and three HITswith literally minutes to spend idling.As each HIT was annotated by a possibly differ-ent set of workers, it is not possible to compute inter-annotator agreement.
Eyeballing the scores revealedthat some workers generally tend to give higher re-spectively lower scores than others.
Overall, work-ers agreed more for clearly compositional or clearlynon-compositional HITs.
We believe that using thiscomparatively high number of judgments per target,averaged over several contexts, should give us fairlyreliable judgments, as worker biases should cancelout each other.23?
I would like to buck the trend of complaint !?
One company that is bucking the trend is Flowcrete Group plc located in Sandbach , Cheshire .?
?
We are now moving into a new phase where we are hoping to buck the trend .?
With a claimed 11,000 customers and what look like aggressive growth plans , including recent acquisitions ofInfinium Software , Interbiz and earlier also Max international , the firm does seem to be bucking the trend ofdifficult times .?
Every time we get a new PocketPC in to Pocket-Lint tower , it seems to offer more features for less money andthe HP iPaq 4150 is n?t about to buck the trend .Figure 1: sentences for V OBJ: buck trend after manual filtering and selection.
The target is highlighted.How literal is this phrase?Can you infer the meaning of a given phrase by only considering their parts literally, or does the phrase carry a ?special?
meaning?In the context below, how literal is the meaning of the phrase in bold?Enter a number between 0 and 10.?
0 means: this phrase is not to be understood literally at all.?
10 means: this phrase is to be understood very literally.?
Use values in between to grade your decision.
Please, however, try to take a stand as often as possible.In case the context is unclear or nonsensical, please enter ?66?
and use the comment field to explain.
However, please try to make sense of it even if the sentences are incomplete.Example 1 :There was a red truck parked curbside.
It looked like someone was living in it.YOUR ANSWER: 10reason: the color of the truck is red, this can be inferred from the parts ?red?
and ?truck?
only - without any special knowledge.?
Example 2 :What a tour!
We were on cloud nine when we got back to headquarters but we kept our mouths shut.YOUR ANSWER: 0reason: ?cloud nine?
means to be blissfully happy.
It does NOT refer to a cloud with the number nine.Example 3 :Yellow fever is found only in parts of South America and Africa.YOUR ANSWER: 7reason: ?yellow fever?
refers to a disease causing high body temperature.
However, the fever itself is not yellow.
Overall, this phrase is fairly literal, but not totally, hence answering with a valuebetween 5 and 8 is appropriate.We take rejection seriously and will not reject a HIT unless done carelessly.
Entering anything else but numbers between 0 and 10 or 66 in the judgment field will automatically trigger rejection.YOUR CONTEXT with big daySpecial Offers : Please call FREEPHONE 0800 0762205 to receive your free copy of ?
Groom ?
the fullcolour magazine dedicated to dressing up for the big day and details of Moss Bros Hire rates .How literal is the bolded phrase in the context above between 0 and 10?
[ ]OPTIONAL: leave a comment, tell us about what is broken, help us to improve this type of HIT:[ ]Figure 2: Sample Human Intelligence Task on Amazon Turk with annotation instructions24EN ADJ NN V SUBJ V OBJ SumTrain 58 (43) 30 (23) 52 (41) 140 (107)Vali.
10 (7) 9 (6) 16 (13) 35 (26)Test 77 (52) 35 (26) 62 (40) 174 (118)All 145 (102) 74 (55) 130 (94) 349 (251)Table 1: English dataset: number of target phrases (withcoarse scores)DE ADJ NN V SUBJ V OBJ SumTrain 49 (42) 26 (23) 44 (33) 119 (98)Vali.
11 (8) 9 (8) 9 (7) 29 (23)Test 63 (48) 29 (28) 57 (44) 149 (120)All 123 (98) 64 (59) 110 () 297 (241)Table 2: German dataset: number of target phrases (withcoarse scores)Additionally to the numerical scores, we?ve alsoprovided coarse-grained labels.
This is motivatedby the following: for some applications, it is prob-ably enough to decide whether a phrase is alwayscompositional, somewhat compositional or usuallynot compositional, without the need of more fine-grained distinctions.
For this, we?ve transformed thenumerical scores in the range of 0-25 to coarse la-bel ?low?, those between 38-62 have been labeledas ?medium?, and the ones from 75 to 100 have re-ceived the value ?high?.
All other phrases have beenexcluded from the corresponding training and testdatasets for ?coarse evaluation?
(s. Section 2.4.2):28.1% of English and 18.9% of German phrases.2.3 DatasetsNow we describe the datasets in detail.
Table 1 sum-marizes the English data, Table 2 describes the Ger-man data quantitatively.
Per language and relation,the data was randomly split in approximatively 40%training, 10% validation and 50% test.2.4 Scoring of system responsesWe provided evaluation scripts along with the train-ing and validation data.
Additionally, we report cor-relation values (Spearman?s rho and Kendall?s tau)in Section 4.2.4.1 Numerical ScoringFor numerical scoring, the evaluation script com-putes the distance between the system responsesS = {starget1, starget2, ...stargetN} and the goldstandard G = {gtarget1, gtarget2, ...gtargetN} inpoints, averaged over all items:NUMSCORE(S,G) = 1N?i=1..N |gi ?
si|.Missing values in the system scores are filledwith the default value of 50.
A perfect score is0, indicating no difference between the systemresponses and the gold standard.2.4.2 Coarse ScoringWe use precision on coarse label predictions forcoarse scoring:COARSE(S,G) =1N?i=1..N{ si == gi : 1otherwise : 0.As with numerical scoring, missing system re-sponses are filled with a default value, in this case?medium?.
A perfect score would be 1.00, connot-ing complete congruence of gold standard and sys-tem response labels.3 ParticipantsSeven teams participated in the shared task.
Table 3summarizes the participants and their systems.
Fourof the teams (Duluth, UoY, JUCSE, SCSS-TCD)submitted three runs for the whole English test set.One team participated with two systems, one ofwhich was for the entire English dataset and an-other one included entries only for English V SUBJand V OBJ relations.
A team from UNED providedscores solely for English ADJ NN pairs.
UCPH wasthe only team that delivered results for both Englishand German.Systems can be split into approaches based on sta-tistical association measures and approaches basedon word space models.
On top, some systems useda machine-learned classifier to predict numericalscores or coarse labels.4 ResultsThe results of the official evaluation for English areshown in Tables 4 and 5.Table 4 reports the results for numerical scor-ing.
UCPH-simple.en performed best with the scoreof 16.19.
The second best system UoY: Exm-Bestachieved 16.51, and the third was UoY:Pro-Bestwith 16.79.
It is worth noting that the top six systems25Systems Institution Team ApproachDuluth-1 Dept.
of Computer Science, Ted Pedersen statisticalDuluth-2 University of Minnesota association measures:Duluth-3 t-score and pmiJUCSE-1 Jadavpur University Tanmoy Chakraborty, Santanu Pal mix of statisticalJUCSE-2 Tapabrata Mondal, Tanik Saikh, association measuresJUCSE-3 Sivaju BandyopadhyaySCSS-TCD:conf1 SCSS, Alfredo Maldonado-Guerra, unsupervised WSM,SCSS-TCD:conf2 Trinity College Dublin Martin Emms cosine similaritySCSS-TCD:conf3submission-ws Gavagai Hillevi Ha?gglo?f, random indexingsubmission-pmi Lisa Tengstrand association measures (pmi)UCPH-simple.en University of Copenhagen Anders Johannsen, Hector Martinez, support vector regressionChristian Rish?j, Anders S?gaard with COALS-basedendocentricity featuresUoY: Exm University of York, UK; Siva Reddy, Diana McCarthy, exemplar-based WSMsUoY: Exm-Best Lexical Computing Ltd., UK Suresh Manandhar,UoY: Pro-Best Spandana Gella prototype-based WSMUNED-1: NN NLP and IR Group at UNED Guillermo Garrido, syntactic VSM,UNED-2: NN Anselmo Peas dependency-parsed UKWaC,UNED-3: NN SVM classifierTable 3: Participants of DiSCo?2011 Shared Taskin the numerical evaluation are all based on differentvariations of word space models.The outcome of evaluation for coarse scores isdisplayed in Table 5.
Here, Duluth-1 performs high-est with 0.585, followed closely by UoY:ExmBestwith 0.576 and UoY: ProBest with 0.567.
Duluth-1 is an approach purely based on association mea-sures.Both tables also report ZERO-response andRANDOM-response baselines.
ZERO-responsemeans that, if no score is reported for a phrase, itgets a default value of 50 (fifty) points in numericalevaluation and ?medium?
in coarse evaluation.
Ran-dom baselines were created by using random labelsfrom a uniform distribution.
Most systems beat theRANDOM-response baseline, only about half of thesystems are better than ZERO-response.Apart from the officially announced scoring meth-ods, we provide Spearman?s rho and Kendall?s taurank correlations for numerical scoring.
Rank cor-relation scores that are not significant are noted inparentheses.
With correlations, the higher the score,the better is the system?s ability to order the phrasesaccording to their compositionality scores.
Here,systems UoY: Exm-Best, UoY: Pro-Best / JUCSE-1 and JUCSE-2 achieved the first, second and thirdbest results respectively.Overall, there is no clear winner for the Englishdataset.
However, across different scoring mecha-nisms, UoY: Exm-Best is the most robust of the sys-tems.
The UCPH-simple.en system has a stellar per-formance on V OBJ but apparently uses a subopti-mal way of assigning coarse labels.
The Duluth-1system, on the other hand, is not able to produce anumerical ranking that is significant according to thecorrelation measures, but excels in the coarse scor-ing.When comparing word space models and asso-ciation measures, it seems that the former do aslightly better job on modeling graded composition-ality, which is especially obvious in the numericalevaluation.Since word space models and statistical associa-tion measures are language-independent approachesand most teams have not used syntactic preprocess-ing other than POS tagging, it is a pity that only oneteam has tried the German task (see Tables 6 and7).
The comparison to the baselines shows that theUCPH system is robust across languages and per-forms (relatively speaking) equally well in the nu-merical scoring both for the German and the Englishtasks.26numerical scores responses ?
?
EN all EN ADJ NN EN V SUBJ EN V OBJnumber of phrases 174 77 35 620-response baseline 0 - - 23.42 24.67 17.03 25.47random baseline 174 (0.02) (0.02) 32.82 34.57 29.83 32.34UCPH-simple.en 174 0.27 0.18 16.19 14.93 21.64 14.66UoY: Exm-Best 169 0.35 0.24 16.51 15.19 15.72 18.6UoY: Pro-Best 169 0.33 0.23 16.79 14.62 18.89 18.31UoY: Exm 169 0.26 0.18 17.28 15.82 18.18 18.6SCSS-TCD: conf1 174 0.27 0.19 17.95 18.56 20.8 15.58SCSS-TCD: conf2 174 0.28 0.19 18.35 19.62 20.2 15.73Duluth-1 174 (-0.01) (-0.01) 21.22 19.35 26.71 20.45JUCSE-1 174 0.33 0.23 22.67 25.32 17.71 22.16JUCSE-2 174 0.32 0.22 22.94 25.69 17.51 22.6SCSS-TCD: conf3 174 0.18 0.12 25.59 24.16 32.04 23.73JUCSE-3 174 (-0.04) (-0.03) 25.75 30.03 26.91 19.77Duluth-2 174 (-0.06) (-0.04) 27.93 37.45 17.74 21.85Duluth-3 174 (-0.08) (-0.05) 33.04 44.04 17.6 28.09submission-ws 173 0.24 0.16 44.27 37.24 50.06 49.72submission-pmi 96 - - - - 52.13 50.46UNED-1: NN 77 - - - 17.02 - -UNED-2: NN 77 - - - 17.18 - -UNED-3: NN 77 - - - 17.29 - -Table 4: Numerical evaluation scores for English: average point difference and correlation measures (not significantvalues in parentheses)coarse values responses EN all EN ADJ NN EN V SUBJ EN V OBJnumber of phrases 118 52 26 40zero-response baseline 0 0.356 0.288 0.654 0.250random baseline 118 0.297 0.288 0.308 0.300Duluth-1 118 0.585 0.654 0.385 0.625UoY: Exm-Best 114 0.576 0.692 0.500 0.475UoY: Pro-Best 114 0.567 0.731 0.346 0.500UoY: Exm 114 0.542 0.692 0.346 0.475SCSS-TCD: conf2 118 0.542 0.635 0.192 0.650SCSS-TCD: conf1 118 0.534 0.64 0.192 0.625JUCSE-3 118 0.475 0.442 0.346 0.600JUCSE-2 118 0.458 0.481 0.462 0.425SCSS-TCD: conf3 118 0.449 0.404 0.423 0.525JUCSE-1 118 0.441 0.442 0.462 0.425submission-ws 117 0.373 0.346 0.269 0.475UCPH-simple.en 118 0.356 0.346 0.500 0.275Duluth-2 118 0.322 0.173 0.346 0.500Duluth-3 118 0.322 0.135 0.577 0.400submission-pmi - - - 0.346 0.550UNED-1-NN 52 - 0.289 - -UNED-2-NN 52 - 0.404 - -UNED-3-NN 52 - 0.327 - -Table 5: Coarse evaluation scores for English27numerical scores responses ?
?
DE all DE ADJ NN DE V SUBJ DE V OBJnumber of phrases 149 63 29 570-response baseline 0 - - 32.51 32.21 38.00 30.05random baseline 149 (0.005) (0.004) 37.79 36.27 47.45 34.54UCPH-simple.de 148 0.171 0.116 24.03 27.09 15.55 24.06Table 6: Numerical evaluation scores for Germanheightcoarse values responses DE all DE ADJ NN DE V SUBJ DE V OBJnumber of phrases 120 48 28 440-response baseline 0 0.158 0.208 0.071 0.159random baseline 120 0.283 0.313 0.214 0.295UCPH-simple.de 119 0.283 0.375 0.286 0.182Table 7: Coarse evaluation scores for GermanFor more details on the systems as well as fine-grained analysis of the results, please consult thecorresponding system description papers.5 ConclusionDiSCo Shared Task attracted seven groups that sub-mitted results for 19 systems.
We consider thisa success, taking into consideration that the taskis new and difficult.
The opportunity to evaluatelanguage-independent models for languages otherthan English was unfortunately not taken up by mostparticipants.The teams applied a variety of approaches thatcan be classified into lexical association measuresand word space models of various flavors.
Fromthe evaluation, it is hard to decide what method iscurrently more suited for the task of automatic ac-quisition of compositionality, with a slight favor forapproaches based on word space model.A takeaway message is that a pure corpus-basedacquisition of graded compositionality is a hard task.While some approaches clearly outperform base-lines, further advances are needed for automatic sys-tems to be able to reproduce semantic composition-ality.AcknowledgmentsWe thank Emiliano Guevara for helping with thepreparation of the evaluation scripts and the ini-tial task description.
This work was partially sup-ported by the German Federal Ministry of Eco-nomics (BMWi) under the project Theseus (number01MQ07019).ReferencesColin Bannard, Timothy Baldwin, and Alex Lascarides.2003.
A statistical approach to the semantics of verb-particles.
In Proc.
of the ACL-SIGLEX Workshopon Multiword Expressions: Analysis, Acquisition andTreatment, pages 65?72, Sapporo, Japan.Marco Baroni, Silvia Bernardini, Adriano Ferraresi, andEros Zanchetta.
2009.
The WaCky wide web: acollection of very large linguistically processed web-crawled corpora.
Language Resources and Evalua-tion.Chris Biemann and Valerie Nygaard.
2010.
Crowdsourc-ing WordNet.
In Proc.
of the 5th International Confer-ence of the Global WordNet Association (GWC-2010),Mumbai, India.Graham Katz and Eugenie Giesbrecht.
2006.
Auto-matic identification of non-compositional multi-wordexpressions using latent semantic analysis.
In Pro-ceedings of the ACL/COLING-06 Workshop on Multi-word Expressions: Identifying and Exploiting Under-lying Properties, pages 12?19, Sydney, Australia.Ivan A.
Sag, Timothy Baldwin, Francis Bond, AnnCopestake, and Dan Flickinger.
2002.
Multiword Ex-pressions: A Pain in the Neck for NLP.
In Proc.
ofthe 3rd International Conference on Intelligent TextProcessing and Computational Linguistics (CICLing-2002), pages 1?15, Mexico City, Mexico.Peter D. Turney and Patrick Pantel.
2010.
From Fre-quency to Meaning: Vector Space Models of Se-mantics.
Journal of Artificial Intelligence Research,37:141?188.28
