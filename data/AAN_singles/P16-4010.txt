Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics?System Demonstrations, pages 55?60,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsMMFEAT: A Toolkit for Extracting Multi-Modal FeaturesDouwe KielaComputer LaboratoryUniversity of Cambridgedouwe.kiela@cl.cam.ac.ukAbstractResearch at the intersection of languageand other modalities, most notably vision,is becoming increasingly important in nat-ural language processing.
We introduce atoolkit that can be used to obtain featurerepresentations for visual and auditory in-formation.
MMFEAT is an easy-to-usePython toolkit, which has been developedwith the purpose of making non-linguisticmodalities more accessible to natural lan-guage processing researchers.1 IntroductionDistributional models are built on the assumptionthat the meaning of a word is represented as adistribution over others (Turney and Pantel, 2010;Clark, 2015), which implies that they suffer fromthe grounding problem (Harnad, 1990).
That is,they do not account for the fact that human se-mantic knowledge is grounded in the perceptualsystem (Louwerse, 2008).
There has been a lotof interest within the Natural Language Processingcommunity for making use of extra-linguistic per-ceptual information, much of it in a subfield calledmulti-modal semantics.
Such multi-modal modelsoutperform language-only models on a range oftasks, including modelling semantic similarity andrelatedness (Bruni et al, 2014; Silberer and La-pata, 2014), improving lexical entailment (Kielaet al, 2015b), predicting compositionality (Rollerand Schulte im Walde, 2013), bilingual lexiconinduction (Bergsma and Van Durme, 2011) andmetaphor identification (Shutova et al, 2016).
Al-though most of this work has relied on visionfor the perceptual input, recent approaches havealso used auditory (Lopopolo and van Miltenburg,2015; Kiela and Clark, 2015) and even olfactory(Kiela et al, 2015a) information.In this demonstration paper, we describe MM-FEAT, a Python toolkit that makes it easy to ob-tain images and sound files and extract visualor auditory features from them.
The toolkit in-cludes two standalone command-line tools thatdo not require any knowledge of the Python pro-gramming language: one that can be used forautomatically obtaining files from a variety ofsources, including Google, Bing and FreeSound(miner.py); and one that can be used for extract-ing different types of features from directories ofdata files (extract.py).
In addition, the packagecomes with code for manipulating multi-modalspaces and several demos to illustrate the widerange of applications.
The toolkit is open sourceunder the BSD license and available at https://github.com/douwekiela/mmfeat.2 Background2.1 Bag of multi-modal wordsAlthough it is possible to ground distributional se-mantics in perception using e.g.
co-occurrencepatterns of image tags (Baroni and Lenci, 2008)or surrogates of human semantic knowledge suchas feature norms (Andrews et al, 2009), the defacto method for grounding representations in per-ception has relied on processing raw image data(Baroni, 2016).
The traditional method for ob-taining visual representations (Feng and Lapata,2010; Leong and Mihalcea, 2011; Bruni et al,2011) has been to apply the bag-of-visual-words(BoVW) approach (Sivic and Zisserman, 2003).The method can be described as follows:1. obtain relevant images for a word or set ofwords;2. for each image, get local feature descriptors;3. cluster feature descriptors with k-means tofind the centroids, a.k.a.
the ?visual words?;554.
quantize the local descriptors by comparingthem to the cluster centroids; and5.
combine relevant image representations intoan overall visual representation for a word.The local feature descriptors in step (2) tendto be variants of the dense scale-invariant featuretransform (SIFT) algorithm (Lowe, 2004), wherean image is laid out as a dense grid and featuredescriptors are computed for each keypoint.A similar method has recently been applied tothe auditory modality (Lopopolo and van Mil-tenburg, 2015; Kiela and Clark, 2015), usingsound files from FreeSound (Font et al, 2013).Bag-of-audio-words (BoAW) uses mel-frequencycepstral coefficients (MFCCs) (O?Shaughnessy,1987) for the local descriptors, although other lo-cal frame representations may also be used.
InMFCC, frequency bands are spaced along the melscale (Stevens et al, 1937), which has the advan-tage that it approximates human auditory percep-tion more closely than e.g.
linearly-spaced fre-quency bands.2.2 Convolutional neural networksIn computer vision, the BoVW method has beensuperseded by deep convolutional neural networks(CNNs) (LeCun et al, 1998; Krizhevsky et al,2012).
Kiela and Bottou (2014) showed that suchnetworks learn high-quality representations thatcan successfully be transfered to natural languageprocessing tasks.
Their method works as follows:1. obtain relevant images for a word or set ofwords;2. for each image, do a forward pass througha CNN trained on an image recognition taskand extract the pre-softmax layer;3. combine relevant image representations intoan overall visual representation for a word.They used the pre-softmax layer (referred to asFC7) from a CNN trained by Oquab et al (2014),which was an adaptation of the well-known CNNby Krizhevsky et al (2012) that played a key rolein the deep learning revolution in computer vision(Razavian et al, 2014; LeCun et al, 2015).
SuchCNN-derived representations perform much betterthan BoVW features and have since been used ina variety of NLP applications (Kiela et al, 2015c;Lazaridou et al, 2015; Shutova et al, 2016; Bulatet al, 2016).2.3 Related workThe process for obtaining perceptual representa-tions thus involves three distinct steps: obtainingfiles relevant to words or phrases, obtaining repre-sentations for the files, and aggregating these intovisual or auditory representations.
To our knowl-edge, this is the first toolkit that spans this entireprocess.
There are libraries that cover some ofthese steps.
Notably, VSEM (Bruni et al, 2013)is a Matlab library for visual semantics represen-tation that implements BoVW and useful func-tionality for manipulating visual representations.DISSECT (Dinu et al, 2013) is a toolkit for dis-tributional compositional semantics that makes iteasy to work with (textual) distributional spaces.Lopopolo and van Miltenburg (2015) have also re-leased their code for obtaning BoAW representa-tions1.3 MMFeat OverviewThe MMFeat toolkit is written in Python.
Thereare two command-line tools (described below) forobtaining files and extracting representations thatdo not require any knowledge of Python.
ThePython interface maintains a modular structureand contains the following modules:?
mmfeat.miner?
mmfeat.bow?
mmfeat.cnn?
mmfeat.spaceSource files (images or sounds) can be obtainedwith the miner module, although this is not a re-quirement: it is straightforward to build an in-dex of a data directory that matches words orphrases with relevant files.
The miner module au-tomatically generates this index, a Python dictio-nary mapping labels to lists of filenames, whichis stored as a Python pickle file index.pkl in thedata directory.
The index is used by the bow andcnn modules, which together form the core of thepackage for obtaining perceptual representations.The space package allows for the manipulationand combination of multi-modal spaces.miner Three data sources are currently sup-ported: Google Images2(GoogleMiner), Bing Im-ages3(BingMiner) and FreeSound4(FreeSound-Miner).
All three of them require API keys,1https://github.com/evanmiltenburg/soundmodels-iwcs2https://images.google.com3https://www.bing.com/images4https://www.freesound.org56which can be obtained online and are stored in theminer.yaml settings file in the root folder.bow The bag-of-words methods are contained inthis module.
BoVW and BoAW are accessiblethrough the mmfeat.bow.vw and mmfeat.bow.awmodules respectively, through the BoVW andBoAW classes.
These classes obtain feature de-scriptors and perform clustering and quantizationthrough a standard set of methods.
BoVW usesdense SIFT for its local feature descriptors; BoAWuses MFCC.
The modules also contain an inter-face for loading local feature descriptors fromMatlab, allowing for simple integraton with e.g.VLFeat5.
The centroids obtained by the clustering(sometimes also called the ?codebook?)
are storedin the data directory for re-use at a later stage.cnn The CNN module uses Python bindingsto the Caffe deep learning framework (Jia etal., 2014).
It supports the pre-trained referenceadaptation of AlexNet (Krizhevsky et al, 2012),GoogLeNet (Szegedy et al, 2015) and VGGNet(Simonyan and Zisserman, 2015).
The interface isidentical to the bow interface.space An additional module is provided formaking it easy to manipulate perceptual represen-tations.
The module contains methods for aggre-gating image or sound file representations into vi-sual or auditory representations; combining per-ceptual representations with textual representa-tions into multi-modal ones; computing nearestneighbors and similarity scores; and calculatingSpearman ?scorrelation scores relative to humansimilarity and relatedness judgments.3.1 DependenciesMMFeat has the following dependencies: scipy,scikit-learn and numpy.
These are standard Pythonlibraries that are easy to install using your favoritepackage manager.
The BoAW module addition-ally requires librosa6to obtain MFCC descriptors.The CNN module requires Caffe7.
It is recom-mended to make use of Caffe?s GPU support, ifavailable, for increased processing speeds.
Moredetailed installation instructions are provided inthe readme file online and in the documentationof the respective projects.5http://www.vlfeat.org6https://github.com/bmcfee/librosa7http://caffe.berkeleyvision.org4 ToolsMMFeat comes with two easy-to-use command-line tools for those unfamiliar with the Python pro-gramming language.4.1 Mining: miner.pyThe miner.py tool takes three arguments: the datasource (bing, google or freesound), a query filethat contains a line-by-line list of queries, and adata directory to store the mined image or soundfiles in.
Its usage is as follows:miner.py {bing,google,freesound} \query_file data_dir [-n int]The -n option can be used to specify the number ofimages to download per query.
The following ex-amples show how to use the tool to get 10 imagesfrom Bing and 100 sound files from FreeSound forthe queries ?dog?
and ?cat?
:$ echo -e "dog\ncat" > queries.txt$ python miner.py -n 10 bing \queries.txt ./img_data_dir$ python miner.py -n 100 freesound \queries.txt ./sound_data_dir4.2 Feature extraction: extract.pyThe extract.py tool takes three arguments: the typeof model to apply (boaw, bovw or cnn), the datadirectory where relevant files and the index arestored, and the output file where the representa-tions are written to.
Its usage is as follows:extract.py [-k int] [-c string] \[-o {pickle,json,csv}] [-s float] \[-m {vgg,alexnet,googlenet}] \{boaw,bovw,cnn} data_dir out_fileThe -k option sets the number of clusters to use inthe bag of words methods (the k in k-means).
The-c option allows for pointing to an existing code-book, if available.
The -s option allows for sub-sampling the number of files to use for the cluster-ing process (which can require significant amountsof memory) and is in the range 0-1.
The tool canoutput representation in Python pickle, JSON andCSV formats.
The following examples show howthe three models can easily be applied:python extract.py -k 100 -s 0.1 bovw \./img_data_dir ./output_vectors.pklpython extract.py -gpu -o json cnn \./img_data_dir ./output_vectors.jsonpython extract.py -k 300 -s 0.5 -o csv \boaw ./sound_data_dir ./out_vecs.csv575 Getting StartedThe command-line tools mirror the Python in-terface, which allows for more fine-grained con-trol over the process.
In what follows, we walkthrough an example illustrating the process.
Thecode should be self-explanatory.Mining The first step is to mine some imagesfrom Google Images:datadir = ?/path/to/data?words = [?dog?, ?cat?
]n_images = 10from mmfeat.miner import*miner = GoogleMiner(datadir, \?/path/to/miner.yaml?
)miner.getResults(words, n_images)miner.save()Applying models We then apply both theBoVW and CNN models, in a manner familiar toscikit-learn users, by calling the fit() method:from mmfeat.bow import*from mmfeat.cnn import*b = BoVW(k=100, subsample=0.1)c = CNN(modelType=?alexnet?, gpu=True)b.load(data_dir)b.fit()c.load(data_dir)c.fit()Building the space We subsequently constructthe aggregated space of visual representations andprint these to the screen:from mmfeat.space import*for lkp in [b.toLookup(), c.toLookup()]:vs = AggSpace(lkp, ?mean?
)print vs.spaceThese short examples are meant to show how onecan straightforwardly obtain perceptual represen-tations that can be applied in a wide variety of ex-periments.6 DemosTo illustrate the range of possible applications, thetoolkit comes with a set of demonstrations of itsusage.
The following demos are available:1-Similarity and relatedness The demo down-loads images for the concepts in the well-knownMEN (Bruni et al, 2012) and SimLex-999 (Hillet al, 2014) datasets, obtains CNN-derived vi-sual representations and calculates the Spearman?scorrelations for textual, visual and multi-modalrepresentations.2-ESP game To illustrate that it is not necessaryto mine images or sound files and that an exist-ing data directory can be used, this demo buildsan index for the ESP Game dataset (Von Ahn andDabbish, 2004) and obtains and stores CNN rep-resentations for future use in other applications.3-Matlab interface To show that local fea-ture descriptors from Matlab can be used, thisdemo contains Matlab code (run dsift.m) that usesVLFeat to obtain descriptors, which are then usedin the BoVW model to obtain visual representa-tions.4-Instrument clustering The demo downloadssound files from FreeSound for a set of instru-ments and applies BoAW.
The mean auditory rep-resentations are clustered and the cluster assign-ments are reported to the screen, showing similarinstruments in similar clusters.5-Image dispersion This demo obtains imagesfor the concepts of elephant and happiness and ap-plies BoVW.
It then shows that the former has alower image dispersion score and is consequentlymore concrete than the latter, as described in Kielaet al (2014).7 ConclusionsThe field of natural language processing hasbroadened in scope to address increasingly chal-lenging tasks.
While the core NLP tasks will re-main predominantly focused on linguistic input, itis important to address the fact that humans ac-quire and apply language in perceptually rich en-vironments.
Moving towards human-level AI willrequire the integration and modeling of multiplemodalities beyond language.Advances in multi-modal semantics show howtextual information can fruitfully be combinedwith other modalities, opening up many avenuesfor further exploration.
Some NLP researchersmay consider non-textual modalities challengingor outside of their area of expertise.
We hope thatthis toolkit enables them in carrying out researchthat uses extra-linguistic input.AcknowledgmentsThe author was supported by EPSRC grantEP/I037512/1 and would like to thank Anita Ver?o,Stephen Clark and the reviewers for helpful sug-gestions.58ReferencesMark Andrews, Gabriella Vigliocco, and David Vin-son.
2009.
Integrating experiential and distribu-tional data to learn semantic representations.
Psy-chological review, 116(3):463.Marco Baroni and Alessandro Lenci.
2008.
Conceptsand properties in word spaces.
Italian Journal ofLinguistics, 20(1):55?88.Marco Baroni.
2016.
Grounding distributional seman-tics in the visual world.
Language and LinguisticsCompass, 10(1):3?13.Shane Bergsma and Benjamin Van Durme.
2011.Learning bilingual lexicons using the visual similar-ity of labeled web images.
In IJCAI, pages 1764?1769.Elia Bruni, Giang Binh Tran, and Marco Baroni.
2011.Distributional semantics from text and images.
InProceedings of the GEMS 2011 workshop on ge-ometrical models of natural language semantics,pages 22?32.
Association for Computational Lin-guistics.Elia Bruni, Gemma Boleda, Marco Baroni, and Nam-Khanh Tran.
2012.
Distributional semantics in tech-nicolor.
In ACL, pages 136?145.Elia Bruni, Ulisse Bordignon, Adam Liska, Jasper Ui-jlings, and Irina Sergienya.
2013.
Vsem: An openlibrary for visual semantics representation.
In Pro-ceedings of the 51st Annual Meeting of the Associa-tion for Computational Linguistics, pages 187?192,Sofia, Bulgaria.Elia Bruni, Nam-Khanh Tran, and Marco Baroni.2014.
Multimodal distributional semantics.
Journalof Artifical Intelligence Research, 49:1?47.Luana Bulat, Douwe Kiela, and Stephen Clark.
2016.Vision and Feature Norms: Improving automaticfeature norm learning through cross-modal maps.
InProceedings of NAACL-HLT 2016, San Diego, CA.Stephen Clark.
2015.
Vector Space Models of LexicalMeaning.
In Shalom Lappin and Chris Fox, editors,Handbook of Contemporary Semantics, chapter 16.Wiley-Blackwell, Oxford.Georgiana Dinu, Nghia The Pham, and Marco Baroni.2013.
DISSECT - DIStributional SEmantics Com-position Toolkit.
In Proceedings of the 51st AnnualMeeting of the Association for Computational Lin-guistics, pages 31?36, Sofia, Bulgaria.Yansong Feng and Mirella Lapata.
2010.
Visual infor-mation in semantic representation.
In Proceedingsof NAACL, pages 91?99.Frederic Font, Gerard Roma, and Xavier Serra.
2013.Freesound technical demo.
In Proceedings of the21st acm international conference on multimedia,pages 411?412.
ACM.Stevan Harnad.
1990.
The symbol grounding problem.Physica D, 42:335?346.Felix Hill, Roi Reichart, and Anna Korhonen.2014.
SimLex-999: Evaluating semantic mod-els with (genuine) similarity estimation.
CoRR,abs/1408.3456.Yangqing Jia, Evan Shelhamer, Jeff Donahue, SergeyKarayev, Jonathan Long, Ross B. Girshick, SergioGuadarrama, and Trevor Darrell.
2014.
Caffe: Con-volutional architecture for fast feature embedding.In ACM Multimedia, pages 675?678.Douwe Kiela and L?eon Bottou.
2014.
Learning imageembeddings using convolutional neural networks forimproved multi-modal semantics.
In Proceedings ofEMNLP, pages 36?45.Douwe Kiela and Stephen Clark.
2015.
Multi- andcross-modal semantics beyond vision: Groundingin auditory perception.
In Proceedings of the 2015Conference on Empirical Methods in Natural Lan-guage Processing, pages 2461?2470, Lisbon, Portu-gal, September.
Association for Computational Lin-guistics.Douwe Kiela, Felix Hill, Anna Korhonen, and StephenClark.
2014.
Improving multi-modal representa-tions using image dispersion: Why less is sometimesmore.
In Proceedings of ACL, pages 835?841.Douwe Kiela, Luana Bulat, and Stephen Clark.
2015a.Grounding semantics in olfactory perception.
InProceedings of ACL, pages 231?236, Beijing, China,July.Douwe Kiela, Laura Rimell, Ivan Vuli?c, and StephenClark.
2015b.
Exploiting image generality for lex-ical entailment detection.
In Proceedings of the53rd Annual Meeting of the Association for Compu-tational Linguistics and the 7th International JointConference on Natural Language Processing (Vol-ume 2: Short Papers), pages 119?124, Beijing,China, July.
Association for Computational Linguis-tics.Douwe Kiela, Ivan Vuli?c, and Stephen Clark.
2015c.Visual bilingual lexicon induction with transferredconvnet features.
In Proceedings of the 2015Conference on Empirical Methods in Natural Lan-guage Processing, pages 148?158, Lisbon, Portugal,September.
Association for Computational Linguis-tics.Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hin-ton.
2012.
ImageNet classification with deep con-volutional neural networks.
In Proceedings of NIPS,pages 1106?1114.Angeliki Lazaridou, Dat Tien Nguyen, RaffaellaBernardi, and Marco Baroni.
2015.
Unveiling thedreams of word embeddings: Towards language-driven image generation.
CoRR, abs/1506.03500.59Yann LeCun, L?eon Bottou, Yoshua Bengio, and PatrickHaffner.
1998.
Gradient-based learning applied todocument recognition.
Proceedings of the IEEE,86(11):2278?2324.Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.2015.
Deep learning.
Nature, 521(7553):436?444.Chee Wee Leong and Rada Mihalcea.
2011.
Goingbeyond text: A hybrid image-text approach for mea-suring word relatedness.
In Proceedings of IJCNLP,pages 1403?1407.A.
Lopopolo and E. van Miltenburg.
2015.
Sound-based distributional models.
In Proceedings of the11th International Conference on ComputationalSemantics (IWCS 2015).Max M. Louwerse.
2008.
Symbol interdependency insymbolic and embodied cognition.
Topics in Cogni-tive Science, 59(1):617?645.David G. Lowe.
2004.
Distinctive image features fromscale-invariant keypoints.
International Journal ofComputer Vision, 60(2):91?110.Maxime Oquab, L?eon Bottou, Ivan Laptev, and JosefSivic.
2014.
Learning and transferring mid-levelimage representations using convolutional neuralnetworks.
In Proceedings of CVPR, pages 1717?1724.D.
O?Shaughnessy.
1987.
Speech communication: hu-man and machine.
Addison-Wesley series in electri-cal engineering: digital signal processing.
Universi-ties Press (India) Pvt.
Limited.Ali Razavian, Hossein Azizpour, Josephine Sullivan,and Stefan Carlsson.
2014.
CNN features off-the-shelf: an astounding baseline for recognition.
InProceedings of the IEEE Conference on ComputerVision and Pattern Recognition Workshops, pages806?813.Stephen Roller and Sabine Schulte im Walde.
2013.A multimodal LDA model integrating textual, cog-nitive and visual modalities.
In Proceedings ofEMNLP, pages 1146?1157.Ekaterina Shutova, Douwe Kiela, and Jean Maillard.2016.
Black holes and white rabbits: Metaphoridentification with visual features.
In Proceedingsof NAACL-HTL 2016, San Diego.
Association forComputational Linguistics.Carina Silberer and Mirella Lapata.
2014.
Learn-ing grounded meaning representations with autoen-coders.
In Proceedings of ACL, pages 721?732.Karen Simonyan and Andrew Zisserman.
2015.
Verydeep convolutional networks for large-scale imagerecognition.
In Proceedings of ICLR.Josef Sivic and Andrew Zisserman.
2003.
Videogoogle: A text retrieval approach to object match-ing in videos.
In Proceedings of ICCV, pages 1470?1477.Stanley Smith Stevens, John Volkmann, and Edwin B.Newman.
1937.
A scale for the measurement ofthe psychological magnitude pitch.
Journal of theAcoustical Society of America, 8(3):185?190.Christian Szegedy, Wei Liu, Yangqing Jia, PierreSermanet, Scott Reed, Dragomir Anguelov, Du-mitru Erhan, Vincent Vanhoucke, and Andrew Rabi-novich.
2015.
Going deeper with convolutions.
InProceedings of the IEEE Conference on ComputerVision and Pattern Recognition, pages 1?9.Peter D. Turney and Patrick Pantel.
2010.
FromFrequency to Meaning: vector space models of se-mantics.
Journal of Artifical Intelligence Research,37(1):141?188, January.Luis Von Ahn and Laura Dabbish.
2004.
Labelingimages with a computer game.
In Proceedings of theSIGCHI conference on human factors in computingsystems, pages 319?326.
ACM.60
