Proceedings of the 12th Conference of the European Chapter of the ACL, pages 692?700,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsTagging Urdu Text with Parts of Speech: A Tagger ComparisonHassan SajjadUniversit?t StuttgartStuttgart.
Germanysajjad@ims.uni-stuttgart.deHelmut SchmidUniversit?t StuttgartStuttgart, Germanyschmid@ims.uni-stuttgart.deAbstractIn this paper, four state-of-art probabilistictaggers i.e.
TnT tagger, TreeTagger, RF taggerand SVM tool, are applied to the Urdu lan-guage.
For the purpose of the experiment, asyntactic tagset is proposed.
A training corpusof 100,000 tokens is used to train the models.Using the lexicon extracted from the trainingcorpus, SVM tool shows the best accuracy of94.15%.
After providing a separate lexicon of70,568 types, SVM tool again shows the bestaccuracy of 95.66%.1 Urdu LanguageUrdu belongs to the Indo-Aryan language family.It is the national language of Pakistan and is oneof the official languages of India.
The majorityof the speakers of Urdu spread over the area ofSouth Asia, South Africa and the United King-dom1.Urdu is a free order language with generalword order SOV.
It shares its phonological, mor-phological and syntactic structures with Hindi.Some linguists considered them as two differentdialects of one language (Bhatia and Koul,2000).
However, Urdu is written in Perso-arabicscript and inherits most of the vocabulary fromArabic and Persian.
On the other hand, Hindi iswritten in Devanagari script and inherits vocabu-lary from Sanskrit.Urdu is a morphologically rich language.Forms of the verb, as well as case, gender, andnumber are expressed by the morphology.
Urdurepresents case with a separate character after thehead noun of the noun phrase.
Due to their sepa-rate occurrence and their place of occurrence,they are sometimes considered as postpositions.Considering them as case markers, Urdu has no-1 http://www.ethnologue.com/14/show_language.asp?code=URDminative, ergative, accusative, dative, instrumen-tal, genitive and locative cases (Butt, 1995: pg10).
The Urdu verb phrase contains a main verb,a light verb describing the aspect, and a tenseverb describing the tense of the phrase (Hardie,2003; Hardie, 2003a).2 Urdu TagsetThere are various questions that need to be ans-wered during the design of a tagset.
The granu-larity of the tagset is the first problem in this re-gard.
A tagset may consist either of general partsof speech only or it may consist of additionalmorpho-syntactic categories such as number,gender and case.
In order to facilitate the taggertraining and to reduce the lexical and syntacticambiguity, we decided to concentrate on the syn-tactic categories of the language.
Purely syntacticcategories lead to a smaller number of tags whichalso improves the accuracy of manual tagging2(Marcus et al, 1993).Urdu is influenced from Arabic, and canbe considered as having three main parts ofspeech, namely noun, verb and particle (Platts,1909; Javed, 1981; Haq, 1987).
However, somegrammarians proposed ten main parts of speechfor Urdu (Schmidt, 1999).
The work of Urdugrammar writers provides a full overview of allthe features of the language.
However, in theperspective of the tagset, their analysis is lackingthe computational grounds.
The semantic, mor-phological and syntactic categories are mixed intheir distribution of parts of speech.
For example,Haq (1987) divides the common nouns into sit-uational (smile, sadness, darkness), locative(park, office, morning, evening), instrumental(knife, sword) and collective nouns (army, data).In 2003, Hardie proposed the first com-putational part of speech tagset for Urdu (Hardie,2 A part of speech tagger for Indian languages, available athttp://shiva.iiit.ac.in/SPSAL2007 /iiit_tagset_guidelines.pdf6922003a).
It is a morpho-syntactic tagset based onthe EAGLES guidelines.
The tagset contains 350different tags with information about number,gender, case, etc.
(van Halteren, 2005).
TheEAGLES guidelines are based on three levels,major word classes, recommended attributes andoptional attributes.
Major word classes includethirteen tags: noun, verb, adjective, pro-noun/determiner, article, adverb, adposition, con-junction, numeral, interjection, unassigned, resi-dual and punctuation.
The recommendedattributes include number, gender, case, finite-ness, voice, etc.3 In this paper, we will focus onpurely syntactic distributions thus will not gointo the details of the recommended attributes ofthe EAGLES guidelines.
Considering theEAGLES guidelines and the tagset of Hardie incomparison with the general parts of speech ofUrdu, there are no articles in Urdu.
Due to thephrase level and semantic differences, pronounand demonstrative are separate parts of speech inUrdu.
In the Hardie tagset, the possessive pro-nouns like  /mera/ (my), /tumhara/(your), 	 /humara/ (our) are assigned to thecategory of possessive adjective.
Most of the Ur-du grammarians consider them as pronouns(Platts, 1909; Javed, 1981; Haq, 1987).
However,all these possessive pronouns require a noun intheir noun phrase, thus show a similar behavioras demonstratives.
The locative and temporaladverbs (/yahan/ (here),  /wahan/ (there), /ab/ (now), etc.)
and, the locative and tempor-al nouns ( /subah/ (morning),    /sham/(evening),    /gher/ (home)) appear in a verysimilar syntactic context.
In order to keep thestructure of pronoun and noun consistent, loca-tive and temporal adverbs are treated as pro-nouns.
The tense and aspect of a verb in Urdu isrepresented by a sequence of auxiliaries.
Consid-er the example4:             Hai raha Ja kerta kam JanIs  Doing  Kept  Work JohnJohn is kept on doing work?Table 1: The aspect of the verb  /kerta/(doing) is represented by two separate words /ja/ and  /raha/ and the last word of the sen-tence  /hai/ (is) shows the tense of the verb.
?3 The details on the EAGLES guidelines can be found at:http://www.ilc.cnr.it/EAGLES/browse.html4 Urdu is written in right to left direction.The above considerations lead to the followingtagset design for Urdu.
The general parts ofspeech are noun, pronoun, demonstrative, verb,adjective, adverb, conjunction, particle, numberand punctuation.
The further refinement of thetagset is based on syntactic properties.
The mor-phologically motivated features of the languageare not encoded in the tagset.
For example, anUrdu verb has 60 forms which are morphologi-cally derived from its root form.
All these formsare annotated with the same category i.e.
verb.During manual tagging, some words arehard for the linguist to disambiguate reliably.
Inorder to keep the training data consistent, suchwords are assigned a separate tag.
For instance,the semantic marker  /se/ gets a separate tagdue to its various confusing usages such as  loca-tive and instrumental (Platts, 1909).The tagset used in the experiments reportedin this paper contains 42 tags including threespecial tags.
Nouns are divided into noun (NN)and proper name (PN).
Demonstratives are di-vided into personal (PD), KAF (KD), adverbial(AD) and relative demonstratives (RD).
All fourcategories of demonstratives are ambiguous withfour categories of pronouns.
Pronouns are di-vided into six types i.e.
personal (PP), reflexive(RP), relative (REP), adverbial (AP), KAF (KP)and adverbial KAF (AKP) pronouns.
Based onphrase level differences, genitive reflexive (GR)and genitive (G) are kept separate from pro-nouns.
The verb phrase is divided into verb, as-pectual auxiliaries and tense auxiliaries.
Numer-als are divided into cardinal (CA), ordinal (OR),fractional (FR) and multiplicative (MUL).
Con-junctions are divided into coordinating (CC) andsubordinating (SC) conjunctions.
All semanticmarkers except   /se/ are kept in one category.Adjective (ADJ), adverb (ADV), quantifier (Q),measuring unit (U), intensifier (I), interjection(INT), negation (NEG) and question words(QW) are handled as separate categories.
Adjec-tival particle (A), KER (KER), SE (SE) andWALA (WALA) are ambiguous entities whichare annotated with separate tags.
A complete listof the tags with the examples is given in appen-dix A.
The examples of the weird categories suchas WALA, KAF pronoun, KAF demonstratives,etc.
are given in appendix B.3 Tagging MethodologiesThe work on automatic part of speech taggingstarted in early 1960s.
Klein and Simmons693(1963) rule based POS tagger can be consideredas the first automatic tagging system.
In the rulebased approach, after assigning each word itspotential tags, a list of hand written disambigua-tion rules are used to reduce the number of tagsto one (Klein and Simmons, 1963; Green andRubin, 1971; Hindle, 1989; Chanod and Tapa-nainen 1994).
A rule based model has the disad-vantage of requiring lots of linguistic efforts towrite rules for the language.Data-driven approaches resolve this prob-lem by automatically extracting the informationfrom an already tagged corpus.
Ambiguity be-tween the tags is resolved by selecting the mostlikely tag for a word (Bahl and Mercer, 1976;Church, 1988; Brill, 1992).
Brill?s transformationbased tagger uses lexical rules to assign eachword the most frequent tag and then applies con-textual rules over and over again to get a highaccuracy.
However, Brill?s tagger requires train-ing on a large number of rules which reduces theefficiency of machine learning process.
Statistic-al approaches usually achieve an accuracy of96%-97% (Hardie, 2003: 295).
However, statis-tical taggers require a large training corpus toavoid data sparseness.
The problem of low fre-quencies can be resolved by applying differentmethods such as smoothing, decision trees, etc.In the next section, an overview of the statisticaltaggers is provided which are evaluated on theUrdu tagset.3.1 Probabilistic DisambiguationThe Hidden Markov model is the most widelyused method for statistical part of speech tag-ging.
Each tag is considered as a state.
States areconnected by transition probabilities whichrepresent the cost of moving from one state toanother.
The probability of a word having a par-ticular tag is called lexical probability.
Both, thetransitional and the lexical probabilities are usedto select the tag of a particular word.As a standard HMM tagger, The TnTtagger is used for the experiments.
The TnT tag-ger is a trigram HMM tagger in which the transi-tion probability depends on two preceding tags.The performance of the tagger was tested onNEGRA corpus and Penn Treebank corpus.
Theaverage accuracy of the tagger is 96% to 97%(Brants, 2000).The second order Markov model used bythe TnT tagger requires large amounts of taggeddata to get reasonable frequencies of POS tri-grams.
The TnT tagger smooths the probabilitywith linear interpolation to handle the problem ofdata sparseness.
The Tags of unknown words arepredicted based on the word suffix.
The longestending string of an unknown word having one ormore occurrences in the training corpus is consi-dered as a suffix.
The tag probabilities of a suffixare evaluated from all the words in the trainingcorpus (Brants, 2000).In 1994, Schmid proposed a probabilisticpart of speech tagger very similar to a HMMbased tagger.
The transition probabilities are cal-culated by decision trees.
The decision treemerges infrequent trigrams with similar contextsuntil the trigram frequencies are large enough toget reliable estimates of the transition probabili-ties.
The TreeTagger uses an unknown wordPOS guesser similar to that of the TnT tagger.The TreeTagger was trained on 2 million wordsof the Penn-Treebank corpus and was evaluatedon 100,000 words.
Its accuracy is comparedagainst a trigram tagger built on the same data.The TreeTagger showed an accuracy of 96.06%(Schmid, 1994a).In 2004, Gim?nez and M?rquez pro-posed a part of speech tagger (SVM tool) basedon support vector machines and reported accura-cy higher than all state-of-art taggers.
The aim ofthe development was to have a simple, efficient,robust tagger with high accuracy.
The supportvector machine does a binary classification of thedata.
It constructs an N-dimensional hyperplanethat separates the data into positive and negativeclasses.
Each data element is considered as avector.
Those vectors which are close to the se-parating hyperplane are called support vectors5.A support vector machine has to betrained for each tag.
The complexity is controlledby introducing a lexicon extracted from the train-ing data.
Each word tag pair in the training cor-pus is considered as a positive case for that tagclass and all other tags in the lexicon are consi-dered negative cases for that word.
This featureavoids generating useless cases for the compari-son of classes.The SVM tool was evaluated on theEnglish Penn Treebank.
Experiments were con-ducted using both polynomial and linear kernels.When using n-gram features, the linear kernelshowed a significant improvement in speed andaccuracy.
Unknown words are considered as themost ambiguous words by assigning them allopen class POS tags.
The disambiguation of un-knowns uses features such as prefixes, suffixes,5 Andrew Moore:http://www.autonlab.org/tutorials/svm.html694upper case, lower case, word length, etc.
On thePenn Treebank corpus, SVM tool showed an ac-curacy of 97.16% (Gim?nez and M?rquez,2004).In 2008, Schmid and Florian proposed aprobabilistic POS tagger for fine grained tagsets.The basic idea is to consider POS tags as sets ofattributes.
The context probability of a tag is theproduct of the probabilities of its attributes.
Theprobability of an attribute given the previous tagsis estimated with a decision tree.
The decisiontree uses different context features for the predic-tion of different attributes (Schmid and Laws,2008).The RF tagger is well suited for lan-guages with a rich morphology and a large finegrained tagset.
The RF tagger was evaluated onthe German Tiger Treebank and Czech Academ-ic corpus which contain 700 and 1200 POS tags,respectively.
The RF tagger achieved a higheraccuracy than TnT and SVMTool.Urdu is a morphologically rich language.Training a tagger on a large fine grained tagsetrequires a large training corpus.
Therefore, thetagset which we are using for these experimentsis only based on syntactic distributions.
Howev-er, it is always interesting to evaluate new dis-ambiguation ideas like RF tagger on differentlanguages.4 ExperimentsA corpus of approx 110,000 tokens was takenfrom a news corpus (www.jang.com.pk).
In thefiltering phase, diacritics were removed from thetext and normalization was applied to keep theUnicode of the characters consistent.
The prob-lem of space insertion and space deletion wasmanually solved and space is defined as the wordboundary.
The data was randomly divided intotwo parts, 90% training corpus and 10% test cor-pus.
A part of the training set was also used asheld out data to optimize the parameters of thetaggers.
The statistics of the training corpus andtest corpus are shown in table 2 and table 3.
Theoptimized parameters of the TreeTagger are con-text size 2, with minimum information gain fordecision tree 0.1 and information gain at leafnode 1.4.
For TnT, a default trigram tagger isused with suffix length of 10, sparse data mode 4with lambda1 0.03 and lambda2 0.4.
The RFtagger uses a context length of 4 with thresholdof suffix tree pruning 1.5.
The SVM tool istrained at right to left direction with model 4.Model 4 improves the detection of unknownwords by artificially marking some known wordsas unknown words and then learning the model.Training corpus Test corpusTokens 100,000 9000Types 7514 1931UnknownTokens-- 754UnknownTypes-- 444?Table 2: Statistics of training and test data.
?Tag Total Un-knownTag To-talUn-knownNN 2537 458 PN 459 101P 1216 0 AA 379 0VB 971 81 TA 285 0ADJ 510 68 ADV 158 21?Table 3: Eight most frequent tags in the testcorpus.
?In the first experiment, no external lexicon wasprovided.
The types from the training corpuswere used as the lexicon by the tagger.
SVM toolshowed the best accuracy for both known andunknown words.
Table 4 shows the accuracies ofall the taggers.
The baseline result where eachword is annotated with its most frequent tag, ir-respective of the context, is 88.0%.TnTtaggerTreeTagger RF tagger SVMtagger93.40% 93.02% 93.28% 94.15%Known95.78% 95.60% 95.68% 96.15%Unknown68.44% 65.92% 68.08% 73.21%?Table 4: Accuracies of the taggers without us-ing any external lexicon.
SVM tool shows thebest result for both known and unknown words.
?The taggers show poor accuracy while detectingproper names.
In most of the cases, proper nameis confused with adjective and noun.
This is be-cause in Urdu, there is no clear distinction be-tween noun and proper name.
Also, the usage ofan adjective as a proper name is a frequent phe-nomenon in Urdu.
The accuracies of open classtags are shown in table 5.
The detailed discussionon the results of the taggers is done after provid-ing an external lexicon to the taggers.695Tag TnTtaggerTree-TaggerRFtaggerSVMtaggerVB 93.20% 91.86% 92.68% 94.23%NN 94.12% 96.21% 93.89% 96.45%PN 73.20% 66.88% 72.77% 68.62%ADV 75.94% 72.78% 74.68% 72.15%ADJ 85.67% 80.78% 86.5% 85.88%?Table 5: Accuracies of open class tags withouthaving an external lexicon?In the second stage of the experiment, a largelexicon consisting of 70,568 types was pro-vided6.
After adding the lexicon, there are 112unknown tokens and 81 unknown types in thetest corpus7.
SVM tool again showed the bestaccuracy of 95.66%.
Table 6 shows the accuracyof the taggers.
The results of open class wordssignificantly improve due to the smaller numberof unknown words in the test corpus.
The totalaccuracy of open class tags and their accuracy onunknown words are given in table 7 and table 8respectively.TnT tag-gerTree-TaggerRF tagger SVMtool94.91% 95.17% 95.26% 95.66%Known95.42% 95.65% 95.66% 96.11%Unknown56.25% 58.04% 64.60% 61.61%?Table 6: Accuracies of the taggers after addingthe lexicon.
SVM tool shows the best accuracyfor known word disambiguation.
RF taggershows the best accuracy for unknown words.
?Tag TnTtaggerTree-TaggerRFtaggerSVMtoolVB 95.88% 95.88% 96.58% 96.80%NN 94.64% 95.85% 94.79% 96.64%PN 86.92% 79.73% 84.96% 81.70%ADV 82.28% 79.11% 81.64% 81.01%ADJ 91.59% 89.82% 92.37% 88.26%?Table 7: Accuracies of open class tags afteradding an external lexicon.
?6 Additional lexicon is taken from CRULP, Lahore, Paki-stan (www.crulp.org).7 The lexicon was added by using the default settings pro-vided by each tagger.
No probability distribution informa-tion was given with the lexicon.Tag TnTtaggerTree-TaggerRFtaggerSVMtoolVB 28.57% 0.00% 42.86% 42.86%NN 74.47% 95.74% 80.85% 80.85%PN 68.18% 54.54% 63.63% 50.00%ADV 8.33% 0.00% 8.33% 0.00%ADJ 30.00% 20.00% 70.00% 80.00%?Table 8: Accuracies of open class tags on un-known words.
The number of unknown wordswith tag VB and ADJ are less than 10 in this ex-periment.
?The results of the taggers are analyzed by findingthe most frequently confused pairs for all thetaggers.
It includes both the known and unknownwords.
Only those pairs are added in the tablewhich have an occurrence of more than 10.
Table9 shows the results.ConfusedpairTnTtaggerTree-TaggerRFtaggerSVMtoolNN ADJ 85 87 87 95NN PN 118 140 129 109NN ADV 12 15 13 15NN VB 14 17 12 12VB TA 12 0 0 0KER P 14 14 14 0ADV ADJ 11 14 13 11PD PP 26 26 30 14?Table 9: Most frequently confused tag pairswith total number of occurrences.
?5 DiscussionThe output of table 9 can be analyzed in manyways e.g.
ambiguous tags, unknown words, openclass tags, close class tags, etc.
In the close classtags, the most frequent errors are between de-monstrative and pronoun, and between KER tagand semantic marker (P).
The difference betweendemonstrative and pronoun is at the phrase level.Demonstratives are followed by a noun whichbelongs to the same noun phrase whereas pro-nouns form a noun phrase by itself.
Taggers ana-lyze the language in a flat structure and are una-ble to handle the phrase level differences.
It isinteresting to see that the SVM tool shows aclear improvement in detecting the phrase leveldifferences over the other taggers.
It might bedue to the SVM tool ability to look not only at696the neighboring tags but at the neighboringwords as well.
(a)      !"
#Gay gayain Gana log VohTA  VB  NN  NN  PDWill sing Song people ThoseThose people will sing a song.
)b(      #Gay Gayain gana VohTA  VB  NN  PPWill  Sing  Song  thoseThose will sing a song.
?Table 10: The word # /voh/ is occurring both aspronoun and demonstrative.
In both of the cases,it is followed by a noun.
But looking at thephrases, demonstrative # has the noun inside thenoun phrase.
?The second most frequent error among the closedclass tags is the distinction between the KER tag /kay/ and the semantic marker  /kay/.
TheKER tag always takes a verb before it and thesemantic marker always takes a noun before it.The ambiguity arises when a verbal noun occurs.In the tagset, verbal nouns are handled as verb.Syntactically, verbal nouns occur at the place ofa noun and can also take a semantic marker afterthem.
This decreases the accuracy in two ways;the wrong disambiguation of KER tag and thewrong disambiguation of unknown verbal nouns.Due to the small amount of training data, un-known words are frequent in the test corpus.Whenever an unknown word occurs at the placeof a noun, the most probable tag for that wordwill be noun which is wrong in our case.
Table11 shows an example of such a scenario.
)a($&'   baad Kay kernay kamNN P VB NNafter -- doing workAfter doing work)b(  kay ker kamKER VB NN-- Doing work(After) doing work?Table 11: (a) Verbal noun with semantic mark-er, (b) syntactic structure of KER tag.
?8All the taggers other than the SVM tool havedifficulties to disambiguate between KER tagsand semantic markers.
)a(* +!< ! !!"
$>Xdo khoraak Ko log zarorat-mandVB NN P NN ADJgive food To people needyGive food to the needy people(b)* +!< ! $>Xdo khoraak ko zaroratmandVB NN P NNgive food To needyGive food to the needy?Table 12: (a) Occurrence of adjective withnoun, (b) dropping of main noun from the nounphrase.
In that case, adjective becomes thenoun.
?Coming to open class tags, the most frequenterrors are between noun and the other open classtags in the noun phrase like proper noun, adjec-tive and adverb.
In Urdu, there is no clear dis-tinction between noun and proper noun.
Thephenomenon of dropping of words is also fre-quent in Urdu.
If a noun in a noun phrase isdropped, the adjective becomes a noun in thatphrase (see table 12).
The ambiguity betweennoun and verb is due to verbal nouns as ex-plained above (see table 11).6 ConclusionIn this paper, probabilistic part of speech taggingtechnologies are tested on the Urdu language.The main goal of this work is to investigatewhether general disambiguation techniques andstandard POS taggers can be used for the taggingof Urdu.
The results of the taggers clearly answerthis question positively.
With the small trainingcorpus, all the taggers showed accuracies around95%.
The SVM tool shows the best accuracy in8 One possible solution to this problem could be to intro-duce a separate tag for verbal nouns which will certainlyremove the ambiguity between the KER tag and the seman-tic marker and reduce the ambiguity between verb andnoun.697disambiguating the known words and the RFtagger shows the best accuracy in detecting thetags of unknown words.AppendicesAppendix A. Urdu part of speech tagsetFollowing is the complete list of the tags of Ur-du.
There are some occurrences in which twoUrdu words are mapped to the same translationof English.
There are two reasons for that, ei-ther the Urdu words have different case or thereis no significant meaning difference betweenthe two words which can be described by dif-ferent English translations.Tag ExamplePersonal demonstra-tive (PD)Y (we) YZ (you) [\ Z(you9)]Z(this) # Z(that)^ Z (that)Relative demonstra-tive (RD)! (that)` Z(that) Z!>(that)Kaf demonstrative(KD)` (whose){! Z(someone)Adverbial demonstr-ative (AD) (now) |Z (then)  Z}* (here) Z (here)Noun (NN)~ (ship) `~ Z (earth)" Z (boy) ? Z(above)$ Z (inside)  Z?	 (with) ??
Z (like)Proper noun (PN){> (Germany)  Z???
(Pakistan)Personal pronoun(PP) (I)Y Z (we) YZ (you) Z[\ (you) ]Z (he) # Z(he) ^ Z (he)Reflexive pronoun(RP)*!< (myself) [\ Z(myself)Relative pronoun(REP)!(that)` Z(that) Z!>(that)Adverbial pronoun(AD) (now) |Z (then)  Z}* (here) Z (here)Kaf pronoun (KP)! (who) {! Z(someone) ` Z Z (which)Adverbial kaf pro(AKP)}$ (where) | Z(when) ? Z (how)Genitive reflexive(GR)>? (my)Genitives (G) (my) Z (your)  Z	 (our) Z (your)Verb (VB)>?"
(write)  Z (eat)  Z (go)  Z (do)9 Polite form of you which is used while talking with the elders andwith the strangersAspectual auxiliary(AA)]??
Z Z10Tense auxiliary (TA) (is)  Z (are) Z(was) Z (were)Adjective (ADJ)Y"?
(cruel) ?!?
'!< Z(beautiful) ?
  Z(weak)Adverb (ADV)?' (very) ? Z (very)  Z' (very)Quantifier (Q)? (some) Z (all)  Z> (this much) ? Z(total)Cardinal (CA)? (one)* Z (two) `Z(three)Ordinal (OR)??
(first) * Z(second) ?<\ Z (last)Fractional (FR){!?
(one fourth) Z{}?
(two and a half)Multiplicative(MUL)> (times)>* Z (twotimes)Measuring unit (U) !
?(kilo)Coordinating (CC) , (and) (or)Subordinating (SC)   ],(that) ]?
! (because)Intensifier (I) !Z{' Z{Adjectival particle  (like)KER  ZPre-title (PRT) ???
(Mr.) Z (Mr.)Post-title (POT) { |? Z (Mr.)Case marker (P)Z Z  Z { Z !  Z ?Z  Z?
Z ?
?SE (SE) WALA (WALA) " Z{" Z?Negation (NEG) ]]  Z[ (not/no)Interjection (INT)#(hurrah)  , Z?
?? (Good)Question word(QW) (what) ! Z (why)Sentence marker(SM)?.
?, ??
?Phrase marker (PM) ?,?
, ?
;?DATE 2007, 1999Expression (Exp): Any word or symbol whichis not handled in the tagset will be catered un-der expression.
It can be mathematical sym-bols, digits, etc.
?Table 13: Tagset of Urdu?10 They always occur with a verb and can not be translated stand-alone.698Appendix B.
Examples of WALA, Noun withlocative behavior, KAF pronoun and KAFdemonstrative and multiplicative.WALA ?:Attributive Demonstrative Occupation? ???
? ]? }**Respectable This one Milk manManner Possession Time? ]??
\  !?
?!?
?  < ? The one with themanner ?slow?Flower withthornsMorningnewspaperPlace Doer --! ? ' ? >}?
--Shoes which isbought fromsome othercountryThe one whosestudy--?Table 14: Examples of tag WALA?Noun with locative behavior:Adverb Noun* {" ? \  ?Down shop Coming fromdownstairsPostposition Noun?  ?  ?Under the table Goes down?Table 15: Examples of noun with locative be-haviorMultiplicative:>*  ? #)>*( ? ?
!He is two times fatter than me.
?Table 16: Example of MultiplicativeKAF pronoun and KAF demonstrative:KAF pronoun! !!"
` \ ?  ??"
?Which people like mangoes?KAF Demonstrative! ` \ ?  ??"
?Which one like mangoes?Adverbial KAF pronoun#   }$ ?Where did he go?
?Table 17: Examples of KAF pronoun and KAFdemonstrativeReferencesBahl, L. R. and Mercer, R. L. 1976.
Part ofspeech assignment by a statistical decision algo-rithm, IEEE International Symposium on Infor-mation Theory, pp.
88-89.Bhatia, TK and Koul, A.
2000.
Colloquial Urdu.London: Routledge.Brants, Thorsten.
2000.
TnT ?
a statistical part-of-speech tagger.
In Proceedings of the Sixth Ap-plied Natural Language Processing ConferenceANLP-2000 Seattle, WA.Brill, E. 1992.
A simple rule-based part ofspeech tagger, Department of Computer Science,University of Pennsylvania.Butt, M. 1995.
The structure of complex predi-cates in Urdu.
CSLI, Stanford.Chanod, Jean-Pierre and Tapananinen, Pasi1994.
Statistical and constraint-Based taggers forFrench, Technical report MLTT-016, RXRCGrenoble.Church, K. W. 1988.
A stochastic parts programand noun phrase parser for unrestricted test, Inthe proceedings of 2nd conference on AppliedNatural Language Processing, pp.
136-143.Gim?nez and M?rquez.
2004.
SVMTool: A gen-eral POS tagger generator based on support vec-tor machines.
In Proceedings of the IV Interna-tional Conference on Language Resources andEvaluation (LREC?
04), Lisbon, Portugal.Green, B. and Rubin, G. 1971.
Automatedgrammatical tagging of English, Department ofLinguistics, Brown University.699Haq, M. Abdul.
1987.
* !?
 ?, Amju-man-e-Taraqqi Urdu (Hind).Hardie, A.
2003.
Developing a tag-set for auto-mated part-of-speech tagging in Urdu.
In Archer,D, Rayson, P, Wilson, A, and McEnery, T (eds.
)Proceedings of the Corpus Linguistics 2003 con-ference.
UCREL Technical Papers Volume 16.Department of Linguistics, Lancaster University,UK.Hardie, A.
2003a.
The computational analysis ofmorphosyntactic categories in Urdu, PhD thesis,Lancaster University.Hindle, D. 1989.
Acquiring disambiguation rulesfrom text, Proceedings of 27th annual meeting ofAssociation for Computational Linguistics.van Halteren, H, 2005.
Syntactic Word ClassTagging, Springer.Javed, Ismat.
1981. ?
$?!?
*, Taraqqi UrduBureau, New Delhi.Klein, S. and Simmons, R.F.
1963.
A computa-tional approach to grammatical coding of Englishwords, JACM 10: pp.
334-347.Marcus, M. P., Santorini, B. and Marcinkiewicz,M.
A.
1993.
Building a large annotated corpus ofEnglish: the Penn Treebank Computational Lin-guistics 19, pp.
313-330Platts, John T 1909.
A grammar of the Hindusta-ni or Urdu language, London.Schmid, H. 1994.
Probabilistic part-of-speechtagging using decision tree, Institut f?r Maschi-nelle Sprachverarbeitung, Universit?t Stuttgart,Germany.Schmid, H. 1994a.
Part-of-speech tagging withneural networks, In the Proceedings of Interna-tional Conference on Computational Linguistics,pp.
172-176, Kyoto, Japan.Schmid, H. and Laws, F. 2008.
Estimation ofconditional Probabilities with Decision Trees andan Application to Fine-Grained POS tagging,COLING 2008, Manchester, Great Britain.Schmidt, RL 1999.
Urdu: an essential grammar,London: Routledge.700
