The Comlex Syntax Project: The First YearCatherine Macleod, Ralph Grishman, and Adam MeyersComputer Science DepartmentNew York University715 Broadway, 7th FloorNew York, NY 10003ABSTRACTWe describe the design of Comlex Syntax, a computational lexiconproviding detailed syntactic information for approximately 38,000EnglJish headwords.
We consider the types of errors which arise increagng such a lexicon, and how such errors can be measured andcontrolled.1.
GoalsThe goal of the Comlex Syntax project is to create a moderately-broad-coverage lexicon recording the syntactic features of Englishwords for purposes of computational language analysis.
This dic-tionary is being developed at New York University and is to bedistributed by the Linguistic Data Consortium, and to be freely us-able for both research and commercial purposes by members of theConsortium.In order to meet the needs of a wide range of analyzers, we haveincluded a rich set of syntactic features and have aimed to charac-terize these features in a relatively theory-neutral way.
In particular,the feature set is more detailed than those of the major commer-cial dictionaries, uch as the Oxford Advanced Learner's Dictionary(OALD) \[4\] and the Longman Dictionary of Contemporary English(LDOCE) \[8\], which have been widely used as a source of lexicalinformation in language analyzers?
In addition, we have aimedto be more comprehensive in capturing features, and in particularsubcategorization features, than commercial dictionaries.2.
StructureThe structure of COMLEX has been discussed at length in our reportto the 1993 HLT Workshop so we will briefly touch on the detailsof our dictionary entry.
The major classes (adjectives, nouns andverbs) are marked for features and complements ( ubcategorizationframes), examples of which can be seen in Figure 1.Nouns have 9 possible features and 9 possible complements; adjec-tives have 7 features and 14 complements; and verbs have 5 featuresand 92 complements.
Figure 2 shows some actual dictionary entries,including some entries for adverbs and prepositions.In order to insure the completeness of our codes, we studied the cod-ing employed by several other major lexicons, including tire BrandeisVerb Lexicon 2, the ACQUILEX Project \[10\], the NYU LinguisticString Project \[9\], the OALD, and the LDOCE, and, whenever fea-sible, have sought o incorporate distinctions made in any of these1To facilitate the transition to COMLEX by current users of these dic-tionaries, we have prepared mappings from COMLEX classes to those ofseveral other dictionaries.2Developed by J. Grimshaw and R. Jackendoff.dictionaries.
The names for the different complement types are basedon the conventions used in the the Brandeis Verb Lexicon.The nota-tion indicates the type and order of the elements (NP = noun phrase,PP = prepositional phrase, NP-PP = a noun phrase followed by aprepositional phrase, :pval = the selected prepositions).The subcategofization types are defined by frames.
These frameswhich appear in our reference manual (see Figure 3) include theconstituent structure :es, the grammatical structure :gs, optional:foaturo$ and one or more examples :ox.
The features in the sub-categorization frames are not those in the dictionary but refer to thecontrol or raising properties of the verb where applicable.
In par-ticular, they capture four different ypes of control: subject control,object control, variable control, and arbitrary control.
Furthermore,the notation allows us to indicate that a verb may have different con-trol features for different complement s ructures, or even for differentprepositions within the complement.
We record, for example, that"blame ... on" involves arbitrary control ("He blamed the problemon going too fast.
"), whereas "blame for" involves object control("He blamed John for going too fast.
").There are two complements represented by the frames in Figure3, possing and ing-sz, possing stands for a frame group whichincludes two frames *possing (where the subject of the gerund ispresent) and *ing-ae (where the subjectis interpreted tobe arbitrary).A verb which is assigned possing must be able to occur in both ofthese frames, ing-sc also stands for a frame group.
It includesbo-ing-s?
and *possing.
Here the subject of the gerund must be thesame as the surface subject and the possessive subject of *possingwill be co-referential with the surface subject.3.
MethodsOur basic approach is to create an initial lexicon manually and thento use a variety of resources, both commercial nd corpus-derived, torefine this lexicon.
Although methods have been developed over thelast few years for automatically identifying some subcategorizationconstraints through corpus analysis \[2,5\], these methods are stilllimited in the range of distinctions they can identify and their abilityto deal with low-frequency words.
Consequently we have chosen touse manual entry for creation of our initial dictionary.The entry of lexical information is being performed by four graduatelinguistics tudents, referred to as elves ("elf" = enterer of lexicalfeatures).
The elves are provided with a menu-basedinterface codedin Common Lisp using the Garnet GUI package, and running onSun workstations.
This interface also provides access to a largetext corpus; as a word is being entered, instances of the word canbe viewed in one of the windows.
Elves rely on citations fromthe corpus, definitions and citations from any of several printed8Noun feature NUNIT: a noun which can occur in a quantifier-noun measure expressionex: "two FOOT long pipe"/"a pipe which is two FEET in length"Noun complement NOUN-THAT-S: the noun complement is a full sentenceex:"the assumption that he will go to school (is wrong.
)"Adj feature ATTRIBUTIVE: an adjective that occurs only attributively (ie before thenoun) and never predicatively (after "be")ex: "The LONE man rode through the desert"/*"the man was lone.
"Adj complement ADJ-FOR-TO-INF: includes three infinitival complements of adjectivesex: "it is PRACTICAL for Evan to go to school."
(extrap-adj-for-to-inf)''the race was easy for her to win."
(extrap-adj-for-to-inf-np-omit)"Joan was kind to invite me."
(extrap-adj-for-to-inf-rs)Verb feature VMOTION: a verb which occurs with a locative adverbial complement.ex: "he ran in" (which may permute to "in he ran.
")Verb complement: NP a verb which takes a direct object noun phrase.ex: "he ran a gambling den.
"Figure 1: Some features and complements.
(verb(noun(adverb(adjective(verb(prep(adjective:orth "build" :subc ((np) (np-for-np) (part-np :adval ("up")))):orth "day" :plural "days":features ((nunit))):orth "even"):orth "even" :features ((apreq))) ;no noun (poetic eventide):orth "even" :subc ((np) (part-np :adval ("up" "out")))):orth "to"):orth "wonderful" :subc ((extrap-adj-s) (extrap-adj-for-to-inf-np-omit)):features ((gradable)))Figure 2: Sample COMLEX Syntax dictionary entries.
(frame-group possing(vp-frame *possing(vp-frame *ing-ac(frame-group ing-sc(vp-frame be-ing-sc(*possing *ing-ac):cs ((poss 2) (vp 3 :mood prespart :subject 2)):gs (:subject 1, :comp 3):ex "he discussed their writing novels.
"):cs (vp 2 :mood prespart :subject anyone):features (:control arbitrary):gs (:subject 1, :comp 2):ex "he discussed writing novels.
")(*possing be-ing-sc)):cs (vp 2 :mood prespart :subject 1):features (:control subject):gs (:subject 1, :comp 2):ex "she began drinking at 9:00 every night.
")Figure 3: Sample COMLEX Syntax subcategorization frames.9dictionaries and their own linguistic intuitions in assigning featuresto words.Entry of the initial dictionary began in April 1993.
To date, entrieshave be, en created for all the nouns and adjectives, and 60% of theverbs3; the initial dictionary is scheduled for completion i  the springof 1994.We expect to check this dictionary against several sources.
Weintend to compare the manual subeategorizations for verbs againstthose in the OALD, and would be pleased to make comparisonsagainst other broad-coverage dictionaries if those can be made avail-able for this purpose.
We also intend to make comparisons againstseveral corpus-derived lists: at the very least, with verb/prepositionand verb/particle pairs with high mutualinformation \[3\] and, if possi-ble, with the results of recently-developed procedures for extractingsubcategorization frames from corpora \[2,5\].
While this corpus-derived information may not be detailed or accurate nough forfully-automated lexicon creation, it should be most valuable as abasis for comparisons.4.
Types and Sources of ErrorAs part of the process of refining the dictionary and assuring its qual-ity, we have spent considerable resources on reviewing dictionaryentries and on occasion have had sections coded by two or even fourof the elves.
This process has allowed us to make some analysis ofthe sources and types of error in the lexicon, and how they might bereduced.
We can divide the sources of error and inconsistency intofour classes:1. errors of elassUieation: where an instance of a word is im-properly analyzed, and in particular where the words follow-ing a verb are not properly identified with regard to comple-ment type.
Specific types of problems include misclassify-ing adjuncts as arguments (or vice versa) and identifying thewrong control features.
Our primary defenses against sucherrors have been a steady refinement of the feature descrip-tions in our manual and regular group review sessions with allthe elves.
In particular, we have developed etailed criteriafor making adjunct/argument distinctions \[6\].A preliminary study, conducted on examples (drawn at ran-dom from a corpus not used for our concordance) of verbsbeginning with "j", indicated that elves were consistent 93%to 94% of the time in labeling argument/adjunct distinctionsfollowing our criteria and, when they were consistent in ar-gument/adjunct labeling, rarely disagreed on the subcatego-rization.
In more than half of the cases where there was dis-agreement, the elves separately flagged these as difficult, am-biguous, or figurative uses of the verbs (and therefore wouldprobably not use them as the basis for assigning lexical fea-tures).
The agreement rate for examples which were notflagged was 96% to 98%.2.
omitted features: where an elf omits a feature because it isnot suggested by an example in the concordance, a citationin the dictionary, or the elf's introspection.
In order to getan estimate of the magnitude of this problem we decided toestablish ameasure of coverage or "recall" for the subcatego-rization features assigned by our elves.
To do this, we tagged3No features are being assigned to adverbs or prepositions in the initiallexicon.the first 150 "j" verbs from a randomly selected corpus from apart of the San Diego Mercury which was not included in ourconcordance and then compared the dictionary entries createdby our lexicographers against he tugged corpus.
The resultsof this comparison are shown in Figure 4.The "Complements only" is the percentage of instances in thecorpus covered by the subcategorization tugs assigned by theelves and does not include the identification of any prepo-sitions or adverbs.
The "Complements only" would corre-spond roughly to the type of information provided by OALDand LDOCE 4.
The "Complements + Prepositions/Particles"column includes all the features, that is it considers thecorrect identification of the complement plus the specificprepositions and adverbs required by certain complements.The two columns of figures under "Complements + Prepo-sitions/Particles" how the results with and without the enu-meration of directional prepositions.We have recently changed our approach to the classificatonof verbs (like "run", "send", "jog", "walk", "jump") whichtake a long list of directional prepositions, by providing ourentering program with a P-DIR option on the prepositionlist.
This option will automatically assign a list of directionalprepositions to the verb and thus will save time and eliminateerrors Of missing prepositions.
Figure 5 shows the dictionaryentry for"jump", taken from the union of the four elves.
If younote the large number of directional prepositions listed underPP (prepositional phrase), you can see how easy it would befor a single elf to miss one or more.
The addition of P-DIRhas eliminated that problem.In some cases this approach will provide a preposition listthat is a little rich for a given verb but we have decided toerr on the side of a slight overgeneration rather than riskmissing any prepositions which actually occur.
As you cansee, the removal of the P-DIRs from consideration improvesthe individual elf scores.The elf union score is the union of the lexical entries for allfour elves.
Theseare certainly numbers to be proud of, butrealistically, having the verbs done four separate times is notpractical.
However, in our original proposal we stated thatbecause of the complexity of the verb entries we would liketo have them done twice.
As can be seen in Figure 6, withtwo passes we succeed in raising individual percentages in allcases.We would like to make clear that even in the two cases whereour individual lexicographers miss 18% and 13% of the com-plements, there was only one instance in which this mighthave resulted in the inability to parse a sentence.
This wasa missing intransitive.
Otherwise, the missed complementswould have been analyzed as adjuncts ince they were a com-bination ofprepositionalphrases andadverbials with one caseof a subordinate conjunction "as".We endeavored to make a comparison with LDOCE on themeasurement.
This was a bit difficult since LDOCE lackssome complements we have and combines others, not alwaysconsistently.
For instance, our PP roughly corresponds to ei-ther L9 (our PP/ADVP) or prep/adv + T1 (e.g.
"on" + T1)(our PP/PART-NP) but in some cases a preposition is men-tioned but the verb is classified as intransitive.
The straightforward comparison has LDOCE finding 73% of the tagged4LDOCE does provide some prepositions and particles.10elf # Complements only Complements + Prepositions/Particleswithout P-DIR using P-DIR96% 89% 90%2 82% 63% 79%3 95% 83% 92%4 87% 69% 81%eft av 90% 76% 84%elf union 100% 93% 94%Figure 4: Number of subcategorization features assigned to "j" verbs by different elves.
(verb :orth "jump" :subc ((pp :pval ("up" "around" "along" "across" "at""down" "in" "from" "into" "through""out" "off of" "past" "over" "out of""onto" "off" "on" "under" '"towards""toward" '`to"))(pp-pp :pval ("about" '"from" "on" "off of" "off""onto" "to"))(np-pp :pval ("through" over" "to")) (intrans) (np)(part-pp :adval ("up" "down" "off" "back" "away""out"):pval ("on" "from" "to"))(part :adval ("off" "on" "across" "aside" "down" "back""away" "in" "up"))):features ((vmotion)))Figure 5: Dictionary entry for "jump" showing proliferation ofpvals.elf# Complements only Complements + Prepositions/Particleswithout P-DIR using P-DIR1 + 2 100%1 + 3 97%1 + 4 96%2 + 3 99%2 + 4 95%3 + 4 97%2-elf av91%91%91%89%79%85%93%92%91%90%86%92%97% I 88% 91%Figure 6: Number of subcategorization features assigned to "j" verbs by pairs of elves.113.4.comp'~\[ements bu  asofter measure liminating complementsthat LDOCE seems to be lacking (PART-NP-PP, P-POSSING,PP-PP) and allowing for app complement for"joke", althoughit is not specified, results in a percentage of 79.We have adopted two lines of defense against the problemof omitted features.
First, critical entries (particularly highfrequency verbs) will be done independently by two or moreelves.
Second, we are developing a more balanced corpus forthe elves to consult.
Recent studies (e.g., \[1\]) confirm ourobservations that features uch as subcategorization patternsmay differ substantially between corpora.
We began with acorpus from a single newspaper (San Jose Mercury News),but have since added the Brown corpus, several literary worksfrom the Library of America, scientific abstracts from the U.S.Department of Energy, and an additional newspaper (the WallStreet Journal).
In extending the corpus, we have limited our-selves to texts which would be readily available to membersof the Linguistic Data Consortium.excess features: when an elf assigns a spurious featurethrough incorrect extrapolation r analogy from available x-amples or introspection.
Because of our desire to obtain rel-atively complete feature sets, even for infrequent verbs, wehave permitted elves to extrapolate from the citations found.Such a process is bound to be less certain than the assignmentof features from extant examples.
However, this problemdoes not appear to be very severe.
A review of the "j" verbenlries produced by all four elves indicates that the fractionof spurious entries ranges from 2% to 6%.fuzzy features: feature assignmentis defined in terms of theacceptability of words in particular syntactic frames.
Accept-ability, however, is often not absolute but a matter of degree.A verb may occur primarily with particular complements, butwill be "acceptable" with others.This problem is compounded bywords which take on partic-ular features only in special contexts.
Thus, we don't ordi-narily think of"dead" as being gradable (*"Fred is more deadthan Mary.
"), but we do say "deader than a door nail".
It isalso compounded by our decision ot to make sense distinc-tions initially.
For example, many words which are countable(require a determiner before the singular form) also have ageneric sense in which the determiner is not required (*"Fredbought apple."
but "Apple is a wonderful f avor.").
For eachsuch problematic feature we have prepared guidelines for theelves, but these still require considerable discretion on theirpart.These problems have emphasized for us the importance of devel-oping a tagged corpus in conjunction with the dictionary, so thatfrequency of occurrence of a feature (and frequency by text type)will be available.
We are planning to do such tagging beginning inMarch 1994, in parallel with the completion ofour initial dictionary.Our plan is to begin by tagging verbs in the Brown corpus, in orderto be able to correlate our tagging with the word sense tagging beingdone by the WordNet group on the same corpus \[7\].5.
AcknowledgementsDesign and preparation fCOMLEX Syntax has been supported bythe Advanced Research Projects Agency through the Office of NavalResearch under Awards No.
MDA972-92-J-1016 and N00014-90-J-1851, and The Trustees of the University of Pennsylvania.12References1.
Douglas.Biber.
Using register-diversified corpora for generallanguage studies.
Computational Linguistics, 19(2):219-242,1993.2.
MichaelBrent.
From grammarto lexicon: Unsupervisedlearn-ing of lexical syntax.
Computational Linguistics, 19(2):243-262, 1993.3.
Donald Hindle andMats Rooth.
Structural mbiguity and lexi-cal relations.
In Proceedings ofthe 29th Annual Meeting of theAssn.for ComputationalLinguistics, pages 229-236, Berkeley,CA, 1une 1991.4.
A. S. Hornby, editor.
Oxford Advanced Learner's Dictionaryof Current English.
1980.5.
Christopher Manning.
Automatic acquisition of a large sub-categorization dictionary from corpora.In Proceedings of the 31st Annual Meeting of the Assn.
forComputational Linguistics, pages 235-242, Columbus, OH,June 1993.6.
Adam Meyers, Catherine Macleod, and Ralph Grishman.
Stan-dardization of the complement-adjunct distinction.
Submittedto the 1994 Annual Meeting of the Assn.
for ComputationalLinguistics.7.
George Miller, Claudia Leacock, Randee Tengi, and RossBunker.
A semantic concordance.
InProceedings of the Hu-man Language Technology Workshop, ages 303-308, Prince-ton, NJ, March 1993.
Morgan Kaufmann.8.
P. Proctor, editor.
Longman Dictionary of Contemporary En-glish.
Longman, 1978.9.
Eileen Fitzpatrick and Naomi Sager.
The Lexical Subclassesof the LSP English Grammar Appendix 3.
In Naomi SagerNatural Language Information Processing.
Addison-Wesley,Reading, MA, 1981.10.
Antonio Sanfilippo.
LKB encoding of lexical knowledge.
InT.
Briscoe, A. Copestake, and V. de Pavia, editors, DefaultInheritance in Unification-Based Approaches to the Lexicon.Cambridge University Press, 1992.
