Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 799?808,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsImproving the Inference of Implicit Discourse Relations via ClassifyingExplicit Discourse ConnectivesAttapol T. RutherfordDepartment of Computer ScienceBrandeis UniversityWaltham, MA 02453, USAtet@brandeis.eduNianwen XueDepartment of Computer ScienceBrandeis UniversityWaltham, MA 02453, USAxuen@brandeis.eduAbstractDiscourse relation classification is an im-portant component for automatic discourseparsing and natural language understanding.The performance bottleneck of a discourseparser comes from implicit discourse rela-tions, whose discourse connectives are notovertly present.
Explicit discourse connec-tives can potentially be exploited to collectmore training data to collect more data andboost the performance.
However, using themindiscriminately has been shown to hurt theperformance because not all discourse con-nectives can be dropped arbitrarily.
Based onthis insight, we investigate the interaction be-tween discourse connectives and the discourserelations and propose the criteria for selectingthe discourse connectives that can be droppedindependently of the context without chang-ing the interpretation of the discourse.
Extratraining data collected only by the freely omis-sible connectives improve the performance ofthe system without additional features.1 IntroductionThe analysis of discourse-level structure has re-ceived increasing attention from the field in recentyears (Feng and Hirst, 2012; Patterson and Kehler,2013; Li et al, 2014).
Discourse-level analysis istypically concerned with relations between clausesand sentences, linguistic units that go beyond sen-tence boundaries.
There are a few conceptions ofthe discourse structure representation of a text suchas a tree (Mann and Thompson, 1988), or a graph(Wolf et al, 2005).
In the work we describe here,we adopt the view of the Penn Discourse Treebank(PDTB) (Prasad et al, 2008), which views a text asa series of local discourse relations, each of whichconsists of a discourse connective as a predicate tak-ing two arguments.
Syntactically, these two argu-ments are typically realized as clauses or sentences.The discourse connective (underlined) can either beexplicit, as in (1), or implicit, as in (2):(1) [The city?s Campaign Finance Board has re-fused to pay Mr Dinkins $95,142 in matchingfunds]Arg1because [his campaign records areincomplete]Arg2.
(2) [So much of the stuff poured into its Austin,Texas, offices that its mail rooms there simplystopped delivering it]Arg1.
Implicit=so [Now,thousands of mailers, catalogs and sales pitchesgo straight into the trash]Arg2.Determining the sense of an explicit discourse re-lation such as (1) is straightforward since ?because?is a strong indicator that the relation between thetwo arguments is CONTINGENCY.CAUSE.
This taskeffectively amounts to disambiguating the sense ofdiscourse connective, which can be done with highaccuracy (Pitler et al, 2008).However, in the absence of an explicit discourseconnective, inferring the sense of a discourse rela-tion has proved to a very challenging task (Park andCardie, 2012; Rutherford and Xue, 2014).
The senseis no longer localized on one or two discourse con-nectives and must now be inferred solely based onits two textual arguments.
Given the limited amountof annotated data in comparison to the number offeatures needed, the process of building a classi-fier is plagued by the data sparsity problem (Li andNenkova, 2014).
As a result, the classification ac-curacy of implicit discourse relations remains much799lower than that of explicit discourse relations (Pitleret al, 2008).One potential method for reducing the data spar-sity problem is through a distantly supervised learn-ing paradigm, which is the direction we take inthis work.
Distant supervision approaches makeuse of prior knowledge or heuristics to cheaply ob-tain weakly labeled data, which potentially containa small number of false labels.
Weakly labeled datacan be collected from unannotated data and incor-porated in the model training process to supplementmanually labeled data.
For our task, we can collectinstances of explicit discourse relations from unan-notated data by some simple heuristics.
After drop-ping the discourse connectives, we should be able totreat them as additional implicit discourse relations.The approach assumes that when the discourseconnective is omitted, the discourse relation remainsthe same, which is a popular assumption in discourseanalysis (Fraser, 2006; Schourup, 1999).
This as-sumption turns out to be too strong in many cases asillustrated in (3):(3) [I want to go home for the holiday]Arg1.Nonetheless, [I will book a flight toHawaii]Arg2.If ?Nonetheless?
is dropped in (3), one can nolonger infer the COMPARISON relation.
Instead,one would naturally infer a CONTINGENCY relation.Dropping the connective and adding the relation as atraining sample adds noise to the training set and canonly hurt the performance.
In addition, certain typesof explicit discourse relations have no correspondingimplicit discourse relations.
For example, discourserelations of the type CONTINGENY.CONDITION arealmost always expressed with an explicit discourseconnective and do not exist in implicit relations.
Webelieve this also explains the lack of success in pre-vious attempts to boost the performance of implicitdiscourse relation detection with this approach.
(Bi-ran and McKeown, 2013; Pitler et al, 2009).
Thissuggests that in order for this approach to work, weneed to identify instances of explicit discourse re-lations that closely match the characteristics of im-plicit discourse relations.In this paper, we propose two criteria for selectingsuch explicit discourse relation instances: omissionrate and context differential.
Our selection criteriafirst classify discourse connectives by their distribu-tional properties and suggest that not all discourseconnectives are truly optional and not all implicitand explicit discourse relations are equivalent, con-trary to commonly held beliefs in previous studiesof discourse connectives.
We show that only thefreely omissible discourse connectives gather addi-tional training instances that lead to significant per-formance gain against a strong baseline.
Our ap-proach improves the performance of implicit dis-course relations without additional feature engineer-ing in many settings and opens doors to more so-phisticated models that require more training data.The rest of the paper is structured as follows.
InSection 2, we describe the discourse connective se-lection criteria.
In Section 3, we present our dis-course connective classification method and experi-mental results that demonstrate its impact on infer-ring implicit discourse relations.
We discuss relatedwork and conclude our findings in Section 4 and 5respectively.2 Discourse Connective Classification andDiscourse Relation Extraction2.1 Datasets used for selectionWe use two datasets for the purposes of extractingand selecting weakly labeled explicit discourse re-lation instances: the Penn Discourse Treebank 2.0(Prasad et al, 2008) and the English Gigaword cor-pus version 3 (Graff et al, 2007).The Penn Discourse Treebank (PDTB) is thelargest manually annotated corpus of discourse re-lations on top of one million word tokens from theWall Street Journal (Prasad et al, 2008; Prasad etal., 2007).
Each discourse relation in the PDTB isannotated with a semantic sense in the PDTB sensehierarchy, which has three levels: CLASS, TYPE andSUBTYPE.
In this work, we are primarily concernedwith the four top-level CLASS senses: EXPANSION,COMPARISON, CONTINGENCY, and TEMPORAL.The distribution of top-level senses of implicit dis-course relations is shown in Table 2.
The spansof text that participate in the discourse relation arealso explicitly annotated.
These are called ARG1 orARG2, depending on its relationship with the dis-course connective.The PDTB is our corpus of choice for its lexical800groundedness.
The existence of a discourse relationmust be linked or grounded to a discourse connec-tive.
More importantly, this applies to not only ex-plicit discourse connectives that occur naturally aspart of the text but also to implicit discourse rela-tions where a discourse connective is added by an-notators during the annotation process.
This is cru-cial to the work reported here in that it allows us tocompare the distribution of the same connective inexplicit and implicit discourse relations.
In the nextsubsection, we will explain in detail how we com-pute the comparison measures and apply them to theselection of explicit discourse connectives that canbe used for collecting good weakly labeled data.We use the Gigaword corpus, a large unannotatednewswire corpus, to extract and select instances ofexplicit discourse discourse relations to supplementthe manually annotated instances from the PDTB.The Gigaword corpus is used for its large size of2.9 billion words and its similarity to the Wall StreetJournal data from the PDTB.
The source of the cor-pus is drawn from six distinct international sourcesof English newswire dating from 1994 - 2006.
Weuse this corpus to extract weakly labeled data for theexperiment.2.2 Discourse relation extraction patternWe extract instances of explicit discourse relationsfrom the Gigaword Corpus that have the same pat-terns as the implicit discourse relations in the PDTB,using simple regular expressions.
We first sentence-segment the Gigaword Corpus using the NLTK sen-tence segmenter (Bird, 2006).
We then write a set ofrules to prevent some common erroneous cases suchas because vs because of from being included.If a discourse connective is a subordinating con-junction, then we use the following pattern:(Clause 1) (connective) (clause 2).Clause 1 and capitalized clause 2 are then used as Arg1and Arg2 respectively.If a discourse connective is a coordinating conjunctionor discourse adverbial, we use the following pattern:(Sentence 1).
(Connective),(clause 2).Sentence 1 and Clause 2 with the first word capitalizedare used as Arg1 and Arg2 respectively.Although there are obviously many other syntactic pat-terns associated with explicit discourse connectives, weuse these two patterns because these are the only patternsthat are also observed in the implicit discourse relations.We want to select instances of explicit discourse relationsthat match the argument patterns of implicit discourse re-lations as much as possible.
As restrictive as this mayseem, these two patterns along with the set of rules allowus to extract more than 200,000 relation instances fromthe Gigaword corpus, so the coverage is not an issue.2.3 Discourse connective selection andclassification criteriaWe hypothesize that connectives that are omitted oftenand in a way that is insensitive to the semantic contextare our ideal candidates for extracting good weakly la-beled data.
We call this type of connectives freely omissi-ble discourse connectives.
To search for this class of con-nectives, we need to characterize connectives by the rateat which they are omitted and by the similarity betweentheir context, in this case their arguments, in explicit andimplicit discourse relations.
This is possible because im-plicit discourse connectives are inserted during annota-tion in the PDTB.
For each discourse connective, we cancompute omission rate and context differential from an-notated explicit and implicit discourse relation instancesin the PDTB and use those measures to classify and selectdiscourse connectives.2.3.1 Omission rate (OR)We use omission rates (OR) to measure the level ofoptionality of a discourse connective.
The omission rateof a type of discourse connective (DC) is defined as:# occurrences of DC in implicit relations# total occurrences of DCOur intuition is that the discourse connectives that havea high level of omission rate are more suitable as sup-plemental training data to infer the sense of implicit dis-course relations.2.3.2 Context differentialThe omission of a freely omissible discourse connec-tive should also be context-independent.
If the omissionof a discourse connective leads to a different interpreta-tion of the discourse relation, this means that the explicitand implicit discourse relations bound by this discourseconnective are not equivalent, and the explicit discourserelation instance cannot be used to help infer the senseof the implicit discourse relation.
Conversely, if the con-texts for the discourse connective in explicit and implicitdiscourse relations do not significantly differ, then the ex-plicit discourse relation instance can be used as weaklylabeled data.To capture this intuition, we must quantify the con-text differential of explicit and implicit discourse rela-tions for each discourse connective.
We represent the801semantic context of a discourse connective through a un-igram distribution over words in its two arguments, withArg1 and Arg2 combined.
We use Jensen-Shannon Di-vergence (JSD) as a metric for measuring the differencebetween the contexts of a discourse connective in implicitand explicit discourse relations.
Computing a context dif-ferential of the discourse connective therefore involvesfitting a unigram distribution from all implicit discourserelations bound by that discourse connective and fittinganother from all explicit discourse relations bound by thesame discourse connective.
We choose this method be-cause it has been shown to be exceptionally effective incapturing similarities of discourse connectives (Hutchin-son, 2005) and statistical language analysis in general(Lee, 2001; Ljubesic et al, 2008).The Jensen-Shannon Divergence (JSD) metric for dif-ference between Po, the semantic environments (unigramdistribution of words in Arg1 and Arg2 combined) in im-plicit discourse relations, and Pr, the semantic environ-ments in explicit discourse relations, is defined as:JSD(Po||Pr) =12D(Po||M) +12D(Pr||M)where M =12(Po+ Pr) is a mixture of the two distribu-tions and D(.||.)
is Kullback-Leibler divergence functionfor discrete probability distributions:D(P ||Q) =?iln(P (i)Q(i))P (i)2.4 Discourse Connective ClassificationUsing the two metrics, we can classify discourse connec-tives into the following classes:1.
Freely omissible: High OR and low JSD2.
Omissible: Low non-zero OR and low JSD.3.
Alternating I: High OR and high JSD.4.
Alternating II: Low non-zero OR and high JSD.5.
Non-omissible: Zero OR.
JSD cannot be computedbecause the connectives are never found in any im-plicit discourse relations.Classifying the connectives into these classes allow usto empirically investigate which explicit discourse rela-tions are useful as supplemental training data for deter-mining the sense of implicit discourse relations.
We dis-cuss each type of connectives below.2.4.1 Freely omissible discourse connectivesThese are connectives whose usage in implicit and ex-plicit discourse relations is indistinguishable and there-fore suitable as a source of supplemental training data.These connectives are defined as having high omissionrate and low context differential.
This definition impliesthat the omission is frequent and insensitive to the con-text.
?Because?
and ?in particular?
in (4) and (5) are suchconnectives.
Dropping them has minimal impact on theunderstanding the discourse relation between their twoarguments and one might argue they even make the sen-tences sound more natural.
(4) We cleared up questions and inconsistencies veryquickly because the people who had the skills andperspective required to resolve them were part ofthe task team.
(WSJ0562)(5) Both companies are conservative marketers that relyon extensive market research.
P&G, in particular,rarely rolls out a product nationally before extensivetest-marketing.
(WSJ0589)2.4.2 Omissible discourse connectivesThey are connectives whose usage in implicit and ex-plicit discourse relations is indistinguishable, yet they arenot often omitted because the discourse relation might behard to interpret without them.
These connectives are de-fined as having low omission rate and low context differ-ential.
For example,(6) Such problems will require considerable skill to re-solve.
However, neither Mr. Baum nor Mr. Harperhas much international experience.
(WSJ0109)One can infer from the discourse that the problems re-quire international experience, but Mr. Baum and Mr.Harper don?t have that experience even without the dis-course connective ?however?.
In other words, the truthvalue of this proposition is not affected by the presence orabsence of this discourse connective.
The sentence mightsound a bit less natural, and the discourse relation seemsa bit more difficult to infer if ?however?
is omitted.2.4.3 Alternating discourse connectivesThey are connectives whose usage in implicit and ex-plicit discourse relations is substantially different andthey are defined as having high context differential.
Hav-ing high context differential means that the two argu-ments of an explicit discourse connective differ substan-tially from those of an implicit discourse.
An example ofsuch discourse connectives is ?nevertheless?
in (7).
If thediscourse connective is dropped, one might infer EXPAN-SION or CONTINGENCY relation instead of COMPARI-SON indicated by the connective.
(7) Plant Genetic?s success in creating genetically en-gineered male steriles doesn?t automatically meanit would be simple to create hybrids in all crops.Nevertheless, he said, he is negotiating with PlantGenetic to acquire the technology to try breedinghybrid cotton.
(WSJ0209)802We hypothesize that this type of explicit discourse re-lations would not be useful as extra training instances forinferring implicit discourse relations because they willonly add noise to the training set.2.4.4 Non-omissible discourse connectivesThey are defined as discourse connectives whose omis-sion rate is close to zero as they are never found in im-plicit discourse relations.
For example, conditionals cannot be easily expressed without the use of an explicitdiscourse connective like ?if?.
We hypothesize that in-stances of explicit discourse relations with such discourseconnectives would not be useful as additional trainingdata for inferring implicit discourse relations becausethey represent discourse relation senses that do not existin the implicit discourse relations.3 Experiments3.1 Partitioning the discourse connectivesWe only include the discourse connectives that appearin both explicit and implicit discourse connectives in thePDTB to make the comparison and classification possi-ble.
As a result, we only analyze 69 out of 134 connec-tives for the purpose of classification.
We also leave out15 connectives whose most frequent sense acccounts forless than 90% of their instances.
For example, since canindicate a TEMPORAL sense or a CONTINGENCY senseof almost equal chance, so it is not readily useful for gath-ering weakly labeled data.
Ultimately, we have 54 con-nectives as our candidates for freely omissible discourseconnectives.We first classify the discourse connectives based ontheir omission rates and context differentials as discussedin the previous section and partition all of the explicit dis-course connective instances based on this classification.The distributions of omission rates and context differen-tials show substantial amount of variation among differ-ent connectives.
Many connectives are rarely omitted andnaturally form its own class of non-omissible discourseconnectives (Figure 1).
We run the agglomerative hier-chical clustering algorithm using Euclidean distance onthe rest of the connectives to divide them into two groups:high omission and low omission rates.
The boundary be-tween the two groups is around 0.65.The distribution of discourse connectives with respectto the context differential suggests two distinct groupsacross the two corpora (Figure 2).
The analysis only in-cludes connectives that are omitted at least twenty timesin the PDTB corpus, so that JSD can be computed.
Thehierarchical clustering algorithm divides the connectivesinto two groups with the boundary at around 0.32, asshould be apparent from the histogram.
The JSD?s com-puted from the explicit discourse relations from the two0102030400.00 0.25 0.50 0.75 1.00Omission rateCountFigure 1: Omission rates of the discourse connectivetypes vary drastically, suggesting that connectives varyin their optionality.
Some connectives are never omitted.Explicit PDTB Weakly Labeled Explicit Gigaword0510150.1 0.2 0.3 0.4 0.5 0.6 0.1 0.2 0.3 0.4 0.5 0.6Context Differential (Jensen?Shannon Divergence)CountFigure 2: The distributions of Jensen-Shannon Diver-gence from both corpora shows two potential distinctclusters of discourse connectives.corpora are highly correlated (?
= 0.80, p < 0.05), so wecan safely use the Gigaword corpus for the analysis andevaluation.The omission rate boundary and context differentialboundary together classify the discourse connectives intofour classes in addition to the non-omissible connectives.When plotted against each other, omission rates and con-text differential together group the discourse connectivesnicely into clusters (Figure 3).
For the purpose of eval-uation, we combine Alternating I and II into one classbecause each individual class is too sparse on its own.The complete discourse connective classification result isdisplayed in Table 1.Sense Train Dev TestComparison 1855 189 145Contingency 3235 281 273Expansion 6673 638 538Temporal 582 48 55Total 12345 1156 1011Table 2: The distribution of senses of implicit discourserelations in the PDTB803Class Name OR JSD ConnectivesAlternating I High High further, in sum, in the end, overall, similarly, whereasAlternating II Low High earlier, in turn, nevertheless, on the other hand, ultimatelyFreely Omissible High Low accordingly, as a result, because, by comparison, by contrast, consequently, for exam-ple, for instance, furthermore, in fact, in other words, in particular, in short, indeed,previously, rather, so, specifically, therefore,Omissible Low Low also, although, and, as, but, however, in addition, instead, meanwhile, moreover, rather,since, then, thus, whileNon-omissible zero NA as long as, if, nor, now that, once, otherwise, unless, untilTable 1: Classification of discourse connectives based on omission rate (OR) and Jensen-Shannon Divergence contextdifferential (JSD).lllllllllllllllllllllllllllllll llllll lllllll0.200.250.300.350.25 0.50 0.75 1.00Omission Rate (OR)Context Differential (JSD)Figure 3: The scattergram of the discourse connectivessuggest three distinct classes.
Each dot represents a dis-course connective.3.2 Evaluation resultsWe formulate the implicit relation classification task asa 4-way classification task in a departure from previ-ous practice where the task is usually set up as four onevs other binary classification tasks so that the effect ofadding the distant supervision from the weakly labeleddata can be more easily studied.
We also believe thissetup is more natural in realistic settings.
Each classifica-tion instance consists of the two arguments of an implicitdiscourse relation, typically adjacent pairs of sentences ina text.
The distribution of the sense labels is shown in Ta-ble 2.
We follow the data split used in previous work for aconsistent comparison (Rutherford and Xue, 2014).
ThePDTB corpus is split into a training set, development set,and test set.
Sections 2 to 20 are used to train classifiers.Sections 0 and 1 are used for developing feature sets andtuning models.
Section 21 and 22 are used for testing thesystems.To evaluate our method for selecting explicit discourserelation instances, we extract weakly labeled discourserelations from the Gigaword corpus for each class of dis-course connective such that the discourse connectives areequally represented within the class.
We train and testMaximum Entropy classifiers by adding varying num-ber (1000, 2000, .
.
.
, 20000) of randomly selected ex-plicit discourse discourse relation instances to the man-ually annotated implicit discourse relations in the PDTBas training data.
We do this for each class of discourseconnectives as presented in Table 1.
We perform 30 trialsof this experiment and compute average accuracy ratesto smooth out the variation from random shuffling of theweakly labeled data.The statistical models used in this study are from theMALLET implementation with its default setting (Mc-Callum, 2002).
Features used in all experiments are takenfrom the state-of-the-art implicit discourse relation classi-fication system (Rutherford and Xue, 2014).
The featureset consists of combinations of various lexical features,production rules, and Brown cluster pairs.
These featuresare described in greater detail by Pitler et al (2009) andRutherford and Xue (2014).Instance reweighting is required when using weaklylabeled data because the training set no longer representsthe natural distribution of the labels.
We reweight eachinstance such that the sums of the weights of all the in-stances of the same label are equal.
More precisely, if aninstance i is from class j, then the weight for the instancewijis equal to the inverse proportion of class j:wij=Number of total instancesSize of class j ?
Number of classes=?kj?cj?cj?
k=ncj?
kwhere cjis the total number of instances from class j andk is the number of classes in the dataset of size n. It istrivial to show that the sum of the weights for all instancesfrom class j is exactlynkfor all classes.The impact of different classes of weakly labeled ex-plicit discourse connective relations is illustrated in Fig-ure 4.
The results show that expicit discourse relationswith freely omissible discourse connectives (high OR andlow JSD) improve the performance on the standard testset and outperform the other classes of discourse connec-tives and the naive approach where all of the discourse804lllll lllllllll l llll0.540.550 5000 10000 15000 20000Number of samples from distant supervisionMeanaccuracyrateDiscourseConnective Type lAllNon?omissibleHigh JSDLow OR.
Low JSDHigh OR.
Low JSDFigure 4: Discourse connectives with high omission ratesand low context differentials lead to highest performanceboost over the state-of-the-art baseline (dotted line).
Eachpoint is an average over multiple trials.
The solid lines areLOESS smoothing curves.connectives are used.
In addition, it shows that on av-erage, the system with weakly labeled data from freelyomissible discourse connectives continues to rise as weincrease the number of samples unlike the other classesof discourse connectives, which show the opposite trend.This suggests that discourse connectives must have bothhigh omission rates and low context differential betweenimplicit and explicit use of the connectives in order to behelpful to the inference of implicit discourse relations.Table 3 presents results that show, overall, our best per-forming system, the one using distant supervision fromfreely omissible discourse connectives, raises the accu-racy rate from 0.550 to 0.571 (p < 0.05; bootstrap test)and the macro-average F1score from 0.384 to 0.405.We achieve such performance after we tune the subset ofweakly labeled data to maximize the performance on thedevelopment set.
Our distant supervision approach im-proves the performance by adding more weakly labeleddata and no additional features.For a more direct comparison with previous results,we also replicated the state-of-the-art system describedin Rutherford and Xue (2014), who follows the practiceof the first work on this topic (Pitler et al, 2009) in settingup the task as four binary one vs. other classifiers.
Theresults are presented in Table 4.
The results show that theextra data extracted from the Gigaword Corpus is particu-larly helpful for minority classes such as Comparison vs.Others and Temporal vs Others, where our current sys-tem significantly outperforms that of Rutherford and Xue(2014).
Interestingly, the Expansion vs. Others classifierBaseline Baselinefeatures + extra dataExpansion Precision 0.608 0.614Recall 0.751 0.788F10.672 0.691Comparison Precision 0.398 0.449Recall 0.228 0.276F10.290 0.342Contingency Precision 0.465 0.493Recall 0.418 0.396F10.440 0.439Temporal Precision 0.263 0.385Recall 0.091 0.091F10.135 0.147Accuracy 0.550 0.571Macro-Average F10.384 0.405Table 3: Our current 4-way classification system outper-forms the baseline overall.
The difference in accuracy isstatistically significant (p < 0.05; bootstrap test).R&X Baseline Baseline(2014) + extra dataComparison vs Others 0.397 0.410 0.380Contingency vs Others 0.544 0.538 0.539Expansion vs Others 0.702 0.694 0.679Temporal vs Others 0.287 0.333 0.246Table 4: The performance of our approach on the binaryclassification task formulation.did not improve as the Expansion class in the four-wayclassification (Table 3).3.3 Just how good is the weakly labeled data?We performed additional experiments to get a sense ofjust how good the weakly labeled data extracted from anunlabeled corpus are.
Table 5 presents four-way classifi-cation results using just the weakly labeled data from theGigaword Corpus.
The results show that the same trendholds when the implicit relations from the PDTB are notincluded in the training process.
The freely omissible dis-course connectives achieves the accuracy rate of 0.505,which is significantly higher than the other classes, butthey are weaker than the manually labeled data, whichachieves the accuracy rate of 0.550 for the same numberof training instances.Weakly labeled data are not perfectly equivalent tothe true implicit discourse relations, but they do providestrong enough additional signal.
Figure 5 presents experi-mental results that compare the impact of weakly labeleddata from Gigaword Corpus vs gold standard data fromthe PDTB for the freely omissible class.
The mean ac-curacy rates from the PDTB data are significantly higherthan those from the Gigaword Corpus (p <0.05; t-test805Gigaword GigawordClass only + Implicit PDTBFreely omissible 0.505 0.571Omissible 0.313 0.527Alternating I + II 0.399 0.546Non-Omissible 0.449 0.554All of above 0.490 0.547Table 5: The accuracy rates for the freely omissible classare higher than the ones for the other classes both whenusing the Gigaword data alone and when using it in con-junction with the implicit relations in the PDTB.0.5500.5550.5600.5650.5702500 3000 3500 4000 OptimizedNumber of samples from distant supervisionMeanaccuracyrateCorpusGigawordPDTB ExplicitFigure 5: The PDTB corpus leads to more improvementfor the same amount of the data.
However, Gigaword cor-pus achieves significantly better performance (p < 0.05;bootstrap test) when both models are tuned on the devel-opement set.and bootstrap test) for the same number of training in-stances combined with the implicit discourse relations.However, when the number of introduced weakly labeleddata exceeds a certain threshold of around 12,000 in-stances, the performance of the Gigaword corpus risessignificantly above the baseline and the explicit PDTB(Figure 4).The relative superiority of our approach derives pre-cisely from the two selection criteria that we propose.The performance gain does not come from the fact thatfreely omissible discourse connectives have better cov-erage of all four senses (Table 6).
When all classes arecombined equally, the system performs worse as we addmore samples although all four senses are covered.
Thecoverage of all four senses is not sufficient for a class ofdiscourse connectives to boost the performance.
The twoselection criteria are both necessary for the success of thisparadigm.4 Related workPrevious work on implicit discourse relation classifica-tion have focused on supervised learning approaches (Linet al, 2010; Rutherford and Xue, 2014), and the distantlysupervised approach using explicit discourse relationsSenseClass Comp.
Cont.
Exp.
Temp.Freely omissible 2 6 10 1Omissible 4 2 5 3Alternating I 1 0 5 0Alternating II 2 0 0 3Non-omissible 0 3 3 2Table 6: The sense distribution by connective class.has not shown satisfactory results (Pitler et al, 2009; Parkand Cardie, 2012; Wang et al, 2012; Sporleder and Las-carides, 2008) Explicit discourse relations have been usedto remedy the sparsity problem or gain extra features withlimited success (Biran and McKeown, 2013; Pitler et al,2009).
Our heuristics for extracting discourse relationshas been explored in the unsupervised setting (Marcu andEchihabi, 2002), but it has never been evaluated on thegold standard data to show its true efficacy.
Our distantsupervision approach chooses only certain types of dis-course connectives to extract weakly labeled data and isthe first of its kind to improve the performance in this tasktested on the manually annotated data.Distant supervision approaches have recently been ex-plored in the context of natural language processing dueto the recent capability to process large amount of data.These approaches are known to be particularly usefulfor relation extraction tasks because training data pro-vided do not suffice for the task and are difficult to ob-tain (Riloff et al, 1999; Yao et al, 2010).
For example,Mintz et al (2009) acquire a large amount of weakly la-beled data based on the Freebase knowledge base and im-proves the performance of relation extraction.
Distantlysupervised learning has also recently been demonstratedto be useful for text classification problems (Speriosu etal., 2011; Marchetti-Bowick and Chambers, 2012).
Forexample, Thamrongrattanarit et al (2013) use simpleheuristics to gather weakly labeled data to perform textclassification with no manually annotated training data.Discourse connectives have been studied and classi-fied based on their syntactic properties such subordinat-ing conjunction, adverbials, etc.
(Fraser, 2006; Fraser,1996).
While providing a useful insight into how dis-course connectives fit into utterances, the syntactic clas-sification does not seem suitable for selecting useful dis-course connectives for our purposes of distant supervi-sion for our task.5 Conclusion and Future DirectionsWe propose two selection criteria for discourse connec-tives that can be used to gather weakly labeled data forimplicit discourse relation classifiers and improve theperformance of the state-of-the-art system without furtherfeature engineering.
As part of this goal, we classify dis-806course connectives based on their distributional semanticproperties and found that certain classes of discourse con-nectives cannot be omitted in every context, which plaguethe weakly labeled data used in previous studies.
Our dis-course connective classification allows for the better se-lection of data points for distant supervision.More importantly, this work presents a new directionin distantly supervised learning paradigm for implicit dis-course relation classification.
This virtual dramatic in-crease in the training set size allows for more feature en-gineering and more sophisticated models.
Implicit dis-course relation classification is now no longer limited tostrictly supervised learning approaches.AcknowledgmentsThis work was funded partially by the National ScienceFoundation via Grant No.
0910532 entitled ?Richer Rep-resentations for Machine Translation?.
All views ex-pressed in this paper are those of the authors and do notnecessarily represent the view of the National ScienceFoundation.
We also would like to thank Karl Pichottaand Gary Patterson for feedback on the manuscript andthe three anonymous reviewers for their suggestions andcomments.ReferencesOr Biran and Kathleen McKeown.
2013.
Aggregatedword pair features for implicit discourse relation dis-ambiguation.
In Proceedings of the 51st Annual Meet-ing of the Association for Computational Linguistics,pages 69?73.
The Association for Computational Lin-guistics.Steven Bird.
2006.
Nltk: the natural language toolkit.
InProceedings of the COLING/ACL on Interactive pre-sentation sessions, pages 69?72.
Association for Com-putational Linguistics.Vanessa Wei Feng and Graeme Hirst.
2012.
Text-leveldiscourse parsing with rich linguistic features.
In Pro-ceedings of the 50th Annual Meeting of the Associationfor Computational Linguistics: Long Papers-Volume1, pages 60?68.
Association for Computational Lin-guistics.Bruce Fraser.
1996.
Pragmatic markers.
Pragmatics,6:167?190.Bruce Fraser.
2006.
Towards a theory of discourse mark-ers.
Approaches to discourse particles, 1:189?204.D Graff, J Kong, K Chen, and K Maeda.
2007.
Englishgigaword third edition, 2007, ldc 2007t07.Ben Hutchinson.
2005.
Modelling the similarity ofdiscourse connectives.
In Proceedings of the the27th Annual Meeting of the Cognitive Science Society(CogSci2005).Lillian Lee.
2001.
On the effectiveness of the skew di-vergence for statistical language analysis.
In ArtificialIntelligence and Statistics, volume 2001, pages 65?72.Junyi Jessy Li and Ani Nenkova.
2014.
Reducing spar-sity improves the recognition of implicit discourse re-lations.
In Proceedings of the 15th Annual Meeting ofthe Special Interest Group on Discourse and Dialogue(SIGDIAL), pages 199?207, Philadelphia, PA, U.S.A.,June.
Association for Computational Linguistics.Jiwei Li, Rumeng Li, and Eduard Hovy.
2014.
Recursivedeep models for discourse parsing.
In Proceedings ofthe 2014 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), pages 2061?2069.Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan.2010.
A PDTB-Styled End-to-End Discourse Parser.arXiv.org, November.N Ljubesic, Damir Boras, Nikola Bakaric, and JasminaNjavro.
2008.
Comparing measures of semantic simi-larity.
In Information Technology Interfaces, 2008.
ITI2008.
30th International Conference on, pages 675?682.
IEEE.William C Mann and Sandra A Thompson.
1988.Rhetorical structure theory: Toward a functional the-ory of text organization.
Text, 8(3):243?281.Micol Marchetti-Bowick and Nathanael Chambers.2012.
Learning for microblogs with distant supervi-sion: Political forecasting with twitter.
In Proceedingsof the 13th Conference of the European Chapter ofthe Association for Computational Linguistics, pages603?612.
Association for Computational Linguistics.Daniel Marcu and Abdessamad Echihabi.
2002.
Anunsupervised approach to recognizing discourse rela-tions.
In Proceedings of the 40th Annual Meetingon Association for Computational Linguistics, pages368?375.
Association for Computational Linguistics.Andrew Kachites McCallum.
2002.
Mal-let: A machine learning for language toolkit.http://www.cs.umass.edu/ mccallum/mallet.Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.2009.
Distant supervision for relation extraction with-out labeled data.
In Proceedings of the Joint Confer-ence of the 47th Annual Meeting of the ACL and the4th International Joint Conference on Natural Lan-guage Processing of the AFNLP: Volume 2-Volume 2,pages 1003?1011.
Association for Computational Lin-guistics.Joonsuk Park and Claire Cardie.
2012.
Improving im-plicit discourse relation recognition through feature setoptimization.
In Proceedings of the 13th Annual Meet-ing of the Special Interest Group on Discourse andDialogue, pages 108?112.
Association for Computa-tional Linguistics.Gary Patterson and Andrew Kehler.
2013.
Predictingthe presence of discourse connectives.
In Proceedings807of the Conference on Empirical Methods in NaturalLanguage Processing.
Association for ComputationalLinguistics.Emily Pitler, Mridhula Raghupathy, Hena Mehta, AniNenkova, Alan Lee, and Aravind K Joshi.
2008.
Eas-ily identifiable discourse relations.
Technical Reports(CIS), page 884.Emily Pitler, Annie Louis, and Ani Nenkova.
2009.
Au-tomatic sense prediction for implicit discourse rela-tions in text.
In Proceedings of the Joint Conferenceof the 47th Annual Meeting of the ACL and the 4thInternational Joint Conference on Natural LanguageProcessing of the AFNLP: Volume 2-Volume 2, pages683?691.
Association for Computational Linguistics.Rashmi Prasad, Eleni Miltsakaki, Nikhil Dinesh, AlanLee, Aravind Joshi, Livio Robaldo, and Bonnie LWebber.
2007.
The penn discourse treebank 2.0 an-notation manual.Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-sakaki, Livio Robaldo, Aravind K Joshi, and Bonnie LWebber.
2008.
The penn discourse treebank 2.0.
InLREC.
Citeseer.Ellen Riloff, Rosie Jones, et al 1999.
Learning dictio-naries for information extraction by multi-level boot-strapping.
In AAAI/IAAI, pages 474?479.Attapol T. Rutherford and Nianwen Xue.
2014.
Discov-ering implicit discourse relations through brown clus-ter pair representation and coreference patterns.
InProceedings of the 14th Conference of the EuropeanChapter of the Association for Computational Linguis-tics (EACL 2014), Gothenburg, Sweden, April.Lawrence Schourup.
1999.
Discourse markers.
Lingua,107(3):227?265.Michael Speriosu, Nikita Sudan, Sid Upadhyay, and Ja-son Baldridge.
2011.
Twitter polarity classificationwith label propagation over lexical links and the fol-lower graph.
In Proceedings of the First workshop onUnsupervised Learning in NLP, pages 53?63.
Associ-ation for Computational Linguistics.Caroline Sporleder and Alex Lascarides.
2008.
Usingautomatically labelled examples to classify rhetoricalrelations: An assessment.
Natural Language Engi-neering, 14(03):369?416.Attapol Thamrongrattanarit, Colin Pollock, BenjaminGoldenberg, and Jason Fennell.
2013.
A distant su-pervision approach for identifying perspectives in un-structured user-generated text.
In Proceedings of theSixth International Joint Conference on Natural Lan-guage Processing, pages 922?926, Nagoya, Japan, Oc-tober.
Asian Federation of Natural Language Process-ing.Xun Wang, Sujian Li, Jiwei Li, and Wenjie Li.
2012.
Im-plicit discourse relation recognition by selecting typ-ical training examples.
In Proceedings of COLING2012, pages 2757?2772, Mumbai, India, December.The COLING 2012 Organizing Committee.Florian Wolf, Edward Gibson, Amy Fisher, and MeredithKnight.
2005.
The discourse graphbank: A databaseof texts annotated with coherence relations.
LinguisticData Consortium.Limin Yao, Sebastian Riedel, and Andrew McCallum.2010.
Collective cross-document relation extractionwithout labelled data.
In Proceedings of the 2010 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 1013?1023.
Association for Com-putational Linguistics.808
