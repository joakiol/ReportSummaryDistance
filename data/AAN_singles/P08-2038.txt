Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 149?152,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsA Linguistically Annotated Reordering Modelfor BTG-based Statistical Machine TranslationDeyi Xiong, Min Zhang, Aiti Aw and Haizhou LiHuman Language TechnologyInstitute for Infocomm Research21 Heng Mui Keng Terrace, Singapore 119613{dyxiong, mzhang, aaiti, hli}@i2r.a-star.edu.sgAbstractIn this paper, we propose a linguistically anno-tated reordering model for BTG-based statis-tical machine translation.
The model incorpo-rates linguistic knowledge to predict orders forboth syntactic and non-syntactic phrases.
Thelinguistic knowledge is automatically learnedfrom source-side parse trees through an an-notation algorithm.
We empirically demon-strate that the proposed model leads to a sig-nificant improvement of 1.55% in the BLEUscore over the baseline reordering model onthe NIST MT-05 Chinese-to-English transla-tion task.1 IntroductionIn recent years, Bracketing Transduction Grammar(BTG) proposed by (Wu, 1997) has been widelyused in statistical machine translation (SMT).
How-ever, the original BTG does not provide an effec-tive mechanism to predict the most appropriate or-ders between two neighboring phrases.
To addressthis problem, Xiong et al (2006) enhance the BTGwith a maximum entropy (MaxEnt) based reorder-ing model which uses boundary words of bilingualphrases as features.
Although this model outper-forms previous unlexicalized models, it does not uti-lize any linguistically syntactic features, which haveproven useful for phrase reordering (Wang et al,2007).
Zhang et al (2007) integrates source-sidesyntactic knowledge into a phrase reordering modelbased on BTG-style rules.
However, one limita-tion of this method is that it only reorders syntac-tic phrases because linguistic knowledge from parsetrees is only carried by syntactic phrases as far as re-ordering is concerned, while non-syntactic phrasesare combined monotonously with a flat reorderingscore.In this paper, we propose a linguistically anno-tated reordering model for BTG-based SMT, whichis a significant extension to the work mentionedabove.
The new model annotates each BTG nodewith linguistic knowledge by projecting source-sideparse trees onto the corresponding binary trees gen-erated by BTG so that syntactic features can be usedfor phrase reordering.
Different from (Zhang et al,2007), our annotation algorithm is able to label bothsyntactic and non-syntactic phrases.
This enablesour model to reorder any phrases, not limited to syn-tactic phrases.
In addition, other linguistic informa-tion such as head words, is also used to improve re-ordering.The rest of the paper is organized as follows.
Sec-tion 2 briefly describes our baseline system whileSection 3 introduces the linguistically annotated re-ordering model.
Section 4 reports the experimentson a Chinese-to-English translation task.
We con-clude in Section 5.2 Baseline SMT SystemThe baseline system is a phrase-based system whichuses the BTG lexical rules (A ?
x/y) to translatesource phrase x into target phrase y and the BTGmerging rules (A ?
[A,A]|?A,A?)
to combine twoneighboring phrases with a straight or inverted or-der.
The BTG lexical rules are weighted with severalfeatures, such as phrase translation, word penaltyand language models, in a log-linear form.
For themerging rules, a MaxEnt-based reordering modelusing boundary words of neighboring phrases as fea-tures is used to predict the merging order, similar to(Xiong et al, 2006).
We call this reordering model149boundary words based reordering model (BWR).
Inthis paper, we propose to incorporate a linguisticallyannotated reordering model into the log-linear trans-lation model, so as to strengthen the BWR?s phrasereordering ability.
We train all the model scaling fac-tors on the development set to maximize the BLEUscore.
A CKY-style decoder is developed to gener-ate the best BTG binary tree for each input sentence,which yields the best translation.3 Linguistically Annotated ReorderingModelThe linguistically annotated reorderingmodel (LAR) is a MaxEnt-based classifica-tion model which predicts the phrase ordero ?
{inverted, straight} during the applicationof merging rules to combine their left and rightneighboring phrases Al and Ar into a larger phraseA.
1 The model can be formulated asLAR = exp(?i ?ihi(o,Al, Ar, A))?o?
exp(?i ?ihi(o?, Al, Ar, A))(1)where the functions hi ?
{0, 1} are reordering fea-tures and ?i are weights of these features.
We definethe features as linguistic elements which are anno-tated for each BTG node through an annotation al-gorithm, which comprise (1) head word hw, (2) thepart-of-speech (POS) tag ht of head word and (3)syntactic label sl.Each merging rule involves 3 nodes (A,Al, Ar)and each node has 3 linguistic elements (hw, ht, sl).Therefore, the model has 9 features in total.
Takingthe left node Al as an example, the model could useits head word w as feature as followshi(o,A,Al, Ar) ={ 1, Al.hw = w, o = straight0, otherwise3.1 Annotation AlgorithmThere are two steps to annotate a phrase or a BTGnode using source-side parse tree information: (1)determining the span on the source side which isexactly covered by the node or the phrase, then(2) annotating the span according to the source-sideparse tree.
If the span is exactly covered by a sin-gle subtree in the source-side parse tree, it is called1Each phrase is also a node in the BTG tree generated by thedecoder.1: Annotator (span s = ?i, j?, source-side parse tree t)2: if s is a syntactic span then3: Find the subtree c in t which exactly covers s4: s.{ } := {c.hw, c.ht, c.sl}5: else6: Find the smallest subtree c?
subsuming s in t7: if c?.hw ?
s then8: s.hw := c?.hw and s.ht := c?.ht9: else10: Find the word w ?
s which is nearest to c?.hw11: s.hw := w and s.ht := w.t /*w.t is the POStag of w*/12: end if13: Find the left boundary node ln of s in c?14: Find the right boundary node rn of s in c?15: s.sl := ln.sl-c?.sl-rn.sl16: end ifFigure 1: The Annotation Algorithm.syntactic span, otherwise it is non-syntactic span.One of the challenges in this annotation algorithmis that phrases (BTG nodes) are not always cover-ing syntactic span, in other words, they are not al-ways aligned to all constituent nodes in the source-side tree.
To solve this problem, we use heuristicrules to generate pseudo head word and compositelabel which consists of syntactic labels of three rel-evant constituents for the non-syntactic span.
In thisway, our annotation algorithm is capable of labellingboth syntactic and non-syntactic phrases and there-fore providing linguistic information for any phrasereordering.The annotation algorithm is shown in Fig.
1.
Fora syntactic span, the annotation is trivial.
Annotationelements directly come from the subtree that coversthe span exactly.
For a non-syntactic span, the pro-cess is much complicated.
Firstly, we need to locatethe smallest subtree c?
subsuming the span (line 6).Secondly, we try to identify the head word/tag of thespan (line 7-12) by using its head word directly if itis within the span.
Otherwise, the word within thespan which is nearest to hw will be assigned as thehead word of the span.
Finally, we determine thecomposite label of the span (line 13-15), which isformulated as L-C-R. L/R means the syntactic labelof the left/right boundary node of s which is thehighest leftmost/rightmost sub-node of c?
not over-lapping the span.
If there is no such boundary node150IP(??)?????HHHHHNP(??)??
HHNP(??)NR??1TibetNP(??)?
HNN??2financialNN??3workVP(??)?????HHHHHVV??4gainAS?5NP(??)??
HHADJP(??)JJ??6remarkableNP(??)NN?
?7achievementFigure 2: A syntactic parse tree with head word annotatedfor each internal node.
The superscripts of leaf nodesdenote their surface positions from left to right.span hw ht sl?1, 2?
??
NN NULL-NP-NN?2, 3?
??
NN NP?2, 4?
??
VV NP-IP-NP?3, 4?
??
VV NP-IP-NPTable 1: Annotation samples according to the tree shownin Fig.
2. hw/ht represents the head word/tag, respec-tively.
sl means the syntactic label.
(the span s is exactly aligned to the left/right bound-ary of c?
), L/R will be set to NULL.
C is the label ofc?.
L, R and C together define the external syntacticcontext of s.Fig.
2 shows a syntactic parse tree for a Chinesesentence, with head word annotated for each internalnode.
Some sample annotations are given in Table 1.3.2 Training and DecodingTraining an LAR model takes three steps.
Firstly, weextract annotated reordering examples from source-side parsed, word-aligned bilingual data using theannotation algorithm and the reordering exampleextraction algorithm of (Xiong et al, 2006).
Wethen generate features using linguistic elements ofthese examples and finally estimate feature weights.This training process flexibly learns rich syntacticreordering information without explicitly construct-ing BTG tree or forest for each sentence pair.During decoding, each input source sentence isfirstly parsed to obtain its syntactic tree.
Then theCKY-style decoder tries to generate the best BTGtree using the lexical and merging rules.
When twoneighboring nodes are merged in a specific order, thetwo embedded reordering models, BWR and LAR,evaluate this merging independently with individualscores.
The former uses boundary words as featureswhile the latter uses the linguistic elements as fea-tures, annotated on the BTG nodes through the anno-tation algorithm according to the source-side parsetree.4 ExperimentsAll experiments in this section were carried out onthe Chinese-to-English translation task of the NISTMT-05.
The baseline system and the new systemwith the LAR model were trained on the FBIS cor-pus.
We removed 15,250 sentences, for which theChinese parser (Xiong et al, 2005) failed to pro-duce syntactic parse trees.
The parser was trainedon the Penn Chinese Treebank with a F1 score of79.4%.
The remaining FBIS corpus (224,165 sen-tence pairs) was used to obtain standard bilingualphrases for the systems.We extracted 2.8M reordering examples fromthese sentences.
From these examples, we gener-ated 114.8K reordering features for the BWR modelusing the right boundary words of phrases and 85Kfeatures for the LAR model using linguistic annota-tions.
We ran the MaxEnt toolkit (Zhang, 2004) totune reordering feature weights with iteration num-ber being set to 100 and Gaussian prior to 1 to avoidoverfitting.We built our four-gram language model usingXinhua section of the English Gigaword corpus(181.1M words) with the SRILM toolkit (Stol-cke, 2002).
For the efficiency of minimum-error-rate training (Och, 2003), we built our developmentset (580 sentences) using sentences not exceeding50 characters from the NIST MT-02 evaluation testdata.4.1 ResultsWe compared various reordering configurations inthe baseline system and new system.
The base-line system only has BWR as the reordering model,while the new system employs two reordering mod-els: BWR and LAR.
For the linguistically anno-tated reordering model LAR, we augment its featurepool incrementally: firstly using only single labels1512(SL) as features (132 features in total), then con-structing composite labels for non-syntactic phrases(+BNL) (6.7K features), and finally introducinghead words and their POS tags into the feature pool(+BNL+HWT) (85K features).
This series of exper-iments demonstrate the impact and degree of con-tribution made by each feature for reordering.
Wealso conducted experiments to investigate the ef-fect of restricting reordering to syntactic phrases inthe new system using the best reordering featureset (SL+BNL+HWT) for LAR.
The experimentalresults (case-sensitive BLEU scores together withconfidence intervals) are presented in Table 2, fromwhich we have the following observations:(1) The LAR model improves the performancestatistically significantly.
Even we only use the base-line feature set SL with only 132 features for theLAR, the BLEU score improves from 0.2497 to0.2588.
This is because most of the frequent reorder-ing patterns between Chinese and English have beencaptured using syntactic labels.
For example, thepre-verbal modifier PP in Chinese is translated intopost-verbal counterpart in English.
This reorderingcan be described by a rule with an inverted order:V P ?
?PP, V P ?, and captured by our syntacticreordering features.
(2) Context information, provided by labels ofboundary nodes (BNL) and head word/tag pairs(HWT), also improves phrase reordering.
Produc-ing composite labels for non-syntactic BTG nodes(+BNL) and integrating head word/tag pairs intothe LAR as reordering features (+BNL+HWT) areboth effective, indicating that context informationcomplements syntactic label for capturing reorder-ing patterns.
(3) Restricting phrase reordering to syntacticphrases is harmful.
The BLEU score plummets from0.2652 to 0.2512.5 ConclusionIn this paper, we have presented a linguistically an-notated reordering model to effectively integrate lin-guistic knowledge into phrase reordering by merg-ing source-side parse trees with BTG binary trees.Our experimental results show that, on the NIST2For non-syntactic node, we only use the single label C,without constructing composite label L-C-R.Reordering Configuration BLEU (%)BWR 24.97 ?
0.90BWR + LAR (SL) 25.88 ?
0.95BWR + LAR (+BNL) 26.27 ?
0.98BWR + LAR (+BNL+HWT) 26.52 ?
0.96Only allowed SPs reordering 25.12 ?
0.87Table 2: The effect of the linguistically annotated reorder-ing model.
BWR denotes the boundary word based re-ordering model while LAR denotes the linguistically an-notated reordering model.
(SL) is the baseline feature set,(+BNL) and (+BNL+HWT) are extended feature sets forthe LAR.
SP means syntactic phrase.MT-05 task of Chinese-to-English translation, theproposed reordering model leads to BLEU improve-ment of 1.55%.
We believe that our linguisticallyannotated reordering model can be further improvedby using better annotation which transfers moreknowledge (morphological, syntactic or semantic)to the model.ReferencesFranz Josef Och.
2003.
Minimum Error Rate Training in Sta-tistical Machine Translation.
In Proceedings of ACL 2003.Andreas Stolcke.
2002.
SRILM - an extensible language mod-eling toolkit.
In Proceedings of International Conference onSpoken Language Processing, volume 2, pages 901-904.Chao Wang, Michael Collins and Philipp Koehn.
2007.
Chi-nese Syntactic Reordering for Statistical Machine Transla-tion.
In Proceedings of EMNLP-CoNLL 2007.Dekai Wu.
1997.
Stochastic Inversion Transduction Grammarsand Bilingual Parsing of Parallel Corpora.
ComputationalLinguistics, 23(3):377-403.Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin, YueliangQian.
2005.
Parsing the Penn Chinese Treebank with Se-mantic Knowledge.
In Proceedings of IJCNLP, Jeju Island,Korea.Deyi Xiong, Qun Liu and Shouxun Lin.
2006.
MaximumEntropy Based Phrase Reordering Model for Statistical Ma-chine Translation.
In Proceedings of ACL-COLING 2006.Dongdong Zhang, Mu Li, Chi-Ho Li and Ming Zhou.
2007.Phrase Reordering Model Integrating Syntactic Knowledgefor SMT.
In Proceedings of EMNLP-CoNLL 2007.Le Zhang.
2004.
Maximum Entropy Model-ing Tooklkit for Python and C++.
Available athttp://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html.152
