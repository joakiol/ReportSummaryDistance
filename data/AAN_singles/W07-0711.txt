Proceedings of the Second Workshop on Statistical Machine Translation, pages 80?87,Prague, June 2007. c?2007 Association for Computational LinguisticsUsing Word Dependent Transition Models in HMM based WordAlignment for Statistical Machine TranslationXiaodong HeMicrosoft ResearchOne Microsoft WayRedmond, WA 98052 USAxiaohe@microsoft.comAbstractIn this paper, we present a Bayesian Learn-ing based method to train word dependenttransition models for HMM based wordalignment.
We present word alignment re-sults on the Canadian Hansards corpus ascompared to the conventional HMM andIBM model 4.
We show that this methodgives consistent and significant alignmenterror rate (AER) reduction.
We also con-ducted machine translation (MT) experi-ments on the Europarl corpus.
MT resultsshow that word alignment based on thismethod can be used in a phrase-based ma-chine translation system to yield up to 1%absolute improvement in BLEU score,compared to a conventional HMM, and0.8% compared to a IBM model 4 basedword alignment.1 IntroductionWord alignment is an important step of mostmodern approaches to statistical machinetranslation (Koehn et al, 2003).
The classicalapproaches to word alignment are based on IBMmodels 1-5 (Brown et al, 1994) and the HMMbased alignment model (Vogel et al, 1996) (Ochand Ney, 2000a, 2000b), while recentlydiscriminative approaches (Moore, 2006) andsyntax based approaches (Zhang and Gildea, 2005)for word alignment are also studied.
In this paper,we present improvements to the HMM basedalignment model originally proposed by (Vogel etal., 1996, Och and Ney, 2000a).Although HMM based word alignment ap-proaches give good performance, one weakness ofit is the coarse transition models.
In the HMMbased alignment model (Vogel et al, 1996), it isassumed that the HMM transition probabilities de-pend only on the jump width from the last state tothe next state.
Therefore, the knowledge of transi-tion probabilities given a particular source word eis not sufficiently modeled.In order to improve transition models in theHMM based alignment, Och and Ney (2000a) ex-tended the transition models to be word-class de-pendent.
In that approach, words of the source lan-guage are first clustered into a number of wordclasses, and then a set of transition parameters isestimated for each word class.
In (2002), Toutano-va et al modeled self-transition (i.e., jump width iszero) probability separately from other transitionprobabilities.
A word dependent self-transitionmodel P(stay|e) is introduced to decide whether tostay at the current source word e at the next step, orjump to a different word.
It was also shown thatwith the assumption that a source word with fertili-ty greater than one generates consecutive words inthe target language, this probability approximatesfertility modeling.
Deng and Byrne in (2005) im-proved this idea.
They proposed a word-to-phraseHMM in which a source word dependent phraselength model is used to model the approximatefertility, i.e., the length of consecutive target wordsgenerated by the source word.
It provides morepowerful modeling of approximate fertility thanthe single P(stay|e) parameter.However, these methods only model the proba-bility of state occupancy rather than a full set oftransition probabilities.
Important knowledge ofjumping from e to another position, e.g., jumping80forward (monotonic alignment) or jumping back-ward (non-monotonic alignment), is not modeled.In this paper, we present a method to further im-prove the transition models for HMM alignmentmodel.
For each source word e, we not only modelits self-transition probability, but also the probabil-ity of jumping from word e to a different word.
Forthis purpose, we estimate a full transition modelfor each source word.A key problem for detailed word-dependenttransition modeling is data sparsity.
In (Toutanovaet al, 2002), the word dependent self-transitionprobability P(stay|e) is interpolated with the globalHMM self-transition probability to alleviate thedata sparsity problem, where an interpolationweight is used for all words and that weight istuned on a hold-out set.
In the proposed word de-pendent transition model, because there are a largenumber of parameters to estimate, the data sparsityproblem is even more severe.
Moreover, since thesparsity of different words are very different, it isdifficult to find a one-size-fits-all interpolationweight, and therefore simple linear interpolation isnot optimal.
In order to address this problem, weuse Bayesian learning so that the transition modelparameters are estimated by maximum a posteriori(MAP) training.
With the help of the prior distribu-tion of the model, the training is regularized andresults in robust models.In the next section we briefly review modelingof transition probabilities in a conventional HMMalignment model (Vogel et al, 1996, Och and Ney,2000a).
Then we describe the equations of MAPtraining for word dependent transition models.
Insection 5, we present word alignment results thatshow significant alignment error rate reductionscompared to the baseline HMM and IBM model 4.We also conducted phrase-based machine transla-tion experiments on the Europarl corpus, English ?French track, and shown that the proposed methodcan lead to significant BLEU score improvementcompared to the HMM and IBM model 4.2 Baseline HMM alignment modelWe briefly review the HMM based word alignmentmodels (Vogel, 1996, Och and Ney, 2000a).
Let?sdenote by 1 1( ,..., )J Jf f f=  as the French sentence,1 1( ,..., )I Ie e e=  as the English sentence, and1 1( ,..., )J Ja a a= as the alignment that specifies theposition of the English word aligned to eachFrench word.
In the HMM based word alignment,a HMM is built at English side, i.e., each (position,word) pair, ( , )jj aa e , is a HMM state, which emitsthe French word fj.
In order to mitigate the sparsedata problem, it is assumed that the emission prob-ability only depends on the English word, i.e.,( | , ) ( | )j jj j a j ap f a e p f e= , and the transition prob-ability only  depends on the position of the laststate and the length of the English sentence, i.e.,11 1( | , , ) ( | , )jj j a j jp a a e I p a a I??
?= .
Then, Vogel etal.
(1996) give11 1 11( | ) ( | , ) ( | )jJJJ Ij j j ajap f e p a a I p f e?=?
?= ?
???
(1)In the HMM of (Vogel et al, 1996), it is furtherassumed these transition probabilities1( | , )?
?= =j jp a i a i I  depend only on the jumpwidth (i - i'), i.e.,1( )( | , )( )Ilc i ip i i Ic l i=???
=???
(2)Therefore, the transition probability1( | , )j jp a a I?
depends on aj-1 but only through thedistortion set {c(i - i')}.In (Och and Ney, 2000a), the word null is intro-duced to generate the French words that don't alignto any English words.
If we denote by j_ the posi-tion of the last French word before j that aligns to anon-null English word, the transition probabilities1( | , )j jp a i a i I?
?= =  in (1) is computed as_( | , ) ( | , )j jp a i a i I p i i I?
?= = = % , where00if    0( | , ) (1 ) ( | , )  otherwisep ip i i Ip p i i I=??
= ???
?
?%state i=0 denotes the state of a null word at theEnglish side, and p0 is the probability of jumpingto state 0, which is estimated from hold-out data.For convenience, we denote by{ }( | , ), ( | )j ip i i I p f e??
=  the HMM parameter set.81In the training stage, ?
are usually estimatedthrough maximum likelihood (ML) training, i.e.,1 1arg max ( | , )J IML p f e??
= ?
(3)and the efficient Expectation-Maximization al-gorithm can be used to optimize ?
iteratively untilconvergence (Rabiner 1989).For the interest of this paper, we elaborate tran-sition parameter estimation with more details.These transition probabilities { }( | , )p i i I?
is a mul-tinomial distribution estimated according to (2),where at each iteration the distortion set {c(i - i')}is the fractional count of transitions with jumpwidth d = i - i', i.e.,11 1 11 1( ) Pr( , | , , )J IJ Ij jj ic d a i a i d f e?+= =?= = = + ???
(4)where ?'
is the model obtained from the immediateprevious iteration and these terms in (4) can beefficiently computed by using the Forward-Backward algorithm (Rabiner 1989).
In practice,we can bucket the distortion parameters {c(d)} intoa few buckets as implemented in (Liang et al,2006).
In our implementation, 15 buckets are usedfor c(?-7), c(-6), ... c(0), ..., c(?7).
The probabilitymass for transitions with jump width larger than 6is uniformly divided.
As suggested in (Liang et al,2006), we also use two separate sets of distortionparameters for transitioning into the first state, andfor transitioning out of the last state, respectively.Finally, we further smooth transition probabilitieswith a uniform distribution as described in (Ochand Ney, 2000a),_ _1( | , ) (1 ) ( | , )j j j jp a a I p a a II?
??
= ?
+ ?
?
.After training, Viterbi decoding is used to findthe best alignment sequence 1?Ja .
i.e.,11 _1?
arg max ( | , ) ( | )jJJJj j j aa ja p a a I p f e=?
?= ?
??
.3 Word-dependent transition models inHMM based alignment modelAs discussed in the previous sections, conventionaltransition models that only depend on source wordpositions are not accurate enough.
There are onlylimited distortion parameters to model the transi-tion between HMM states for all English words,and the knowledge of transition probabilities givena particular source word is not represented.
In or-der to improve the transition model in HMM, weextend the transition probabilities to be word de-pendent so that the probability of jumping fromstate aj_to aj not only depends on aj_, but also de-pends on the English word at position aj_.
Thisgives_11 1 _1( | ) ( | , , ) ( | )j jJJJ Ij j a j ajap f e p a a e I p f e=?
?= ?
???
.Compared to (1), we need to estimate the transitionparameter__( | , , )jj j ap a a e I  which is _jae  depen-dent.
Correspondingly, the HMM parameters weneed to estimate are { }( | , , ), ( | )i j ip i i e I p f e???
= ,which provides a much richer set of free parame-ters to model transition probabilities.4 Bayesian Learning for word-dependenttransition models4.1 Maximum a posteriori trainingUsing ML training, we can obtain the estimationformula for word dependent transition probabilities{ }( | , , )p i i e I?
similar as (2), i.e.,1( ; )( | , , )( ; )ML Ilc i i ep i i e Ic l i e=???
=???
(5)where at each training iteration the word dependentdistortion set {c(i - i';e)} is computed by11 1 11 1( ; )( ) Pr( , | , , )jJ IJ Ia j jj ic d ee e a i a i d f e?
?+= ==?= = = + ???
(6)where d = i - i' is the jump width, and ( )jae e?
= isthe Kronecker delta function that equals one ifjae e= , and zero otherwise.However, for many non-frequent words, thedata samples for c(d;e) is very limited and there-fore may lead to a biased model that severely over-fits to the sparse data.
In order to address this issue,maximum a posteriori (MAP) framework is ap-plied (Gauvain and Lee, 1994).
In MAP training,an appropriate prior distribution is used to incorpo-82rate prior knowledge into the model parameter es-timation,1 1 1arg max ( | , ) ( | )J I IMAP p f e g e??
= ?
?
(7)where the prior distribution 1( | )Ig e?
characterizesthe distribution  of the model parameter set ?
giv-en the English sentence.
The relation between MLand MAP estimation is through the Bayes' theoremwhere the posterior distribution1 1 1 1 1( | , ) ( | , ) ( | )J I J I Ip f e p f e g e?
?
?
?
, and1 1( | , )J Ip f e ?
is the likelihood function.In transition model estimation, the transitionmodel { }( | , , )ip i i e I??
is a multinomial distribution.Its conjugate prior distribution is a Dirichlet distri-bution taking the following form (Bishop 2006),( ) , 111( | , , ) | ( | , , ) i iIvIi iig p i i e I e p i i e I ?
??
?=?
??
?
(8)where{ },i iv ?
is the set of hyper-parameters of theprior distribution.
Note that for mathematic tracta-bility,,i iv ?
needs to be greater than 1, which isusually the case in practice.Substitute (8) into (7) and using EM algorithm,we can obtain the iterative MAP training formulafor transition models (Gauvain and Lee, 1994),,1 1( ; ) 1( | , , )( ; )i iMAP I Ii ll lc i i e vp i i e Ic l i e v I?
?= =??
+ ??
=??
+ ??
?
(9)4.2 Setting hyper-parameters for the priordistributionIn Bayesian learning, the hyper-parameter set{ },i iv ?
of the prior distribution is assumed knownbased on a subjective knowledge about the model.In our method, we set the prior with word-independent transition probabilities.,( | , ) 1i iv p i i I??
?= ?
+     (10)where ?
is a positive parameter that needs to tuneon a hold-out data set.
We will investigate the ef-fect of ?
with experimental results in later sections.Substituting (10) into (9), the MAP based transi-tion model training formula becomes1( ; ) ( | , )( | , , )( ; )MAP Ilc i i e p i i Ip i i e Ic l i e??=?
??
+ ??
=??
+?
(11)Note that for frequent words that have a largeamount of data samples for c(d;e), the sum of1,...,( ; )=???
l I c l i e  is large, so that ( | , , )MAPp i i e I?
isdominated by the data distribution.
For rare wordsthat have low counts of c(d;e), ( | , , )MAPp i i e I?
willapproach to the word independent model.
On theother hand, for the same word, when a small ?
isused, a weak prior is applied, and the transitionprobability is more dependent on the training dataof that word.
When ?
becomes larger and larger, astronger prior knowledge is applied, and the worddependent transition model will approach to theword-independent transition model.
Therefore, wecan vary the parameter ?
to control the contributionof prior distribution in model training and tune theword alignment performance.5 Experimental Results5.1 Word alignment on the Canadian Han-sards English-French corpusWe evaluated our word dependent transition mod-els for HMM based word alignment on the Eng-lish-French Hansards corpus.
Only a subset of500K sentence pairs was used in our experimentsincluding 447 test sentence-pairs.
Tests sentence-pairs were manually aligned and were marked withboth sure and possible alignments (Och and Ney2000a).
Using this annotation, we report the wordalignment performance in terms of alignment errorrate (AER) as defined by Och and Ney (2000a):| | | |1 | | | |A S A PAERA S?
+ ?= ?+(12)where S denotes the set of sure gold alignments, Pdenotes the set of possible gold alignments, A de-notes the set of alignments generated by the wordalignment method under test.We first trained the IBM model 1 and then abaseline HMM model as described in section 2 onthe Hansards corpus.
As the common practice, weinitialized the translation probabilities of model 1with uniform distribution over word pairs occurtogether in a same sentence pair.
HMM was initia-83lized with uniform transition probabilities andmodel 1 translation probabilities.
Both model 1 andHMM were trained with 5 iterations.
For the pro-posed word dependent transition model basedHMM (WDHMM), we used the same settings asthe HMM baseline except that the transition prob-ability is computed according to (11).
We alsotrained  IBM model 4 using GIZA++ provided byOch and Ney (2000c), where 5 iterations of  model4 training was performed after 5 iterations of mod-el 1 plus 5 iterations of HMM.The effect of hyper-parameters in the prior dis-tribution for WDHMM is shown in Figure 1.
Thehorizontal dot line represents the AER given by thebaseline HMM.
The dash-line curve represents theAERs of WDHMM given different ??s.
We varythe value of ?
in the range from 0 to 1E5 andpresent that range in a log-scale in the figure.
Since?
= 0 is not a valid value in the log domain, we ac-tually use the left-most point in the figure torepresent the case of ?
= 0.
From Fig.
1 it is shownthat when ?
is zero, we actually use the ML trainedword-dependent transition model.
Due to thesparse data problem, the model is poorly estimatedand lead to a high AER.
When increase ?
to a larg-er value, a stronger prior is applied to give a morerobust model.
Then in a large rangeof [100,2000]?
?
, WDHMM outperforms baselineHMM significantly.
When ?
gets even larger, MAPmodel training becomes being over-dominated bythe prior distribution, and that eventually results ina performance approaching to that of the baselineHMM.
Fig.
1 only presents AER results that arecalculated after combination of word alignments ofboth E?F and F?E directions based on a set ofheuristics proposed by Och and Ney (2000b).
Wehave observed the similar trend of AER change forthe E?F and F?E alignment directions, respec-tively.
However, due to the limit of the space, wedidn?t include them in this paper.In table 1-3, we give a detailed comparison be-tween baseline HMM, WDHMM (with ?
= 1000),and IBM model 4.
Compared to the baselineHMM, the proposed WDHMM can reduce AER bymore than 13%.
It even outperforms IBM model 4after two direction word alignment combination.Meanwhile we noticed  that although IBM model 4gives superior performance over the baselineHMM on both of the two alignment directions, itsAER after combination is almost the same as thatof the baseline HMM.
We hypothesize that it maydue to the modeling mechanism difference be-tween HMM and model 4.0 1 2 3 4 588.599.51010.511log10(tau)AER%WDHMMHMM baselineFigure 1: The AER of HMM baseline and the AERof WDHMM as the prior parameter ?
is varied from 0 to1E5.
Note that the x axis is in log scale and we use theleft-most point in the figure to represent the case of ?
=0.
These results are calculated after combination ofword alignments of both E?F and F?E directions.model E ?
F F ?
E combinedbaseline HMM  12.7 13.7 9.8WDHMM(?
= 1000)11.6 12.7 8.5IBM model 4(GIZA++)11.3 12.1 9.7Table 1: Comparison of test set AER between vari-ous models trained on 500K sentence pairs.
All numbersare in percentage.model E ?
F F ?
E combinedbaseline HMM  85.2 83.1 91.7WDHMM(?
= 1000)86.1 83.8 93.3IBM model 4(GIZA++)87.2 86.2 91.6Table 2: Comparison of test set Precision betweenvarious models trained on 500K sentence pairs.
Allnumbers are in percentage.model E ?
F F ?
E combinedbaseline HMM  90.6 91.4 88.3WDHMM(?
= 1000)91.9 92.6 89.1IBM model 4(GIZA++)91.1 90.8 88.4Table 3: Comparison of test set Recall between vari-ous models trained on 500K sentence pairs.
All numbersare in percentage.845.2 Machine translation on Europarl corpusWe further tested our WDHMM on a phrase-basedmachine translation system to see whether our im-provement on word alignment can also improveMT accuracy measured by BLEU score (Papineniet al, 2002).
The machine translation experimentwas conducted on the English-to-French track ofNAACL 2006 Europarl evaluation workshop.
Thesupplied training corpus contains 688K sentencepairs.
Text data are already tokenized.
In our expe-riment, we first lower-cased all text, then wordclustering was performed to cluster words of Eng-lish and French into 32 word classes respectivelyusing the tool provided by (J. Goodman).
Thenword alignment was performed.
Both baselineHMM and IBM model 4 use word-class basedtransition models, and in WDHMM the word-classbased transition model was used for prior distribu-tion.
The IBM model 4 is trained by GIZA++ witha regimen of 5 iterations of Model 1, 5 iterations ofHMM, and 5 iterations of Model 4.
Alignments ofboth directions are generated and then are com-bined by heuristic rules described in (Och and Ney2000b).
Then phrase table was extracted from theword aligned bilingual texts.
The maximum phraselength was set to 7.
In the phrase-based MT system,there are four channel models.
They are directmaximum likelihood estimate of the probability oftarget phrase given source phrase, and the sameestimate of source given target; we also computethe lexicon weighting features for source giventarget and target given source, respectively.
Othermodels include word count and phrase count, and a3-gram language model provided by the workshop.These models are combined in a log-linear frame-work with different weights (Och and Ney, 2002).The model weight vector is trained on a dev setwith 2000 English sentences, each of which hasone French translation reference.
In the experiment,only the first 500 sentences were used to train thelog-linear model weight vector, where minimumerror rate (MER) training was used (Och, 2003).After MER training, the weight vector that givesthe best accuracy on the development set was se-lected.
We then applied it to tests.
There are 2000sentences in the development-test set devtest, 2000sentences in a test set test, and 1064 out-of-domainsentences called nc-test.
The Pharaoh phrase-baseddecoder (Koehn 2004b) was used for decoding.The maximum re-ordering limit for decoding wasset to 7.
We used default settings for all other pa-rameters.We present BLEU scores of MT systems usingdifferent word alignments on all three test sets,where Fig 2 shows BLEU scores of the two in-domain tests, and Fig 3 shows MT results on theout-of-domain test set.
In testing, the prior parame-ter ?
of WDHMM was varied in the range of [20,5000].In Fig.
2, the horizontal dash line and the hori-zontal dot line represent BLEU scores of the base-line HMM on devtest set and test set, respectively.The dash-line curve and dot-line curve representthe BLEU scores of WDHMM on these two tests.It is shown in the figure that WDHMM canachieve the best BLEU scores on both devtest andtest when the prior parameter ?
is set to 100.
Fur-thermore, WDHMM also gives considerable im-provement on BLEU score over the baseline HMMin a broad range of ?
from 50 to 1000, which indi-cates that WDHMM works pretty stable within areasonable range of prior distributions.In Fig.
3, the horizontal dash line represents theBLEU score of baseline HMM on nc-test set andthe dash-line curve represents BLEU scores ofWDHMM on the out-of-domain test.
The bestBLEU is obtained at ?
= 500.
It is interesting to seethat the best ?
for the out-of-domain test is largerthan that of an in-domain test.
One possible expla-nation is that for out-of-domain data, we needmore robust modeling for outliers other than moreaccurate (in-domain) modeling.
However, since thedifference between ?
= 500 and ?
= 100 are verysmall, further experiments are needed before wecan draw a conclusion.We gives a detailed BLEU-wise comparison be-tween baseline HMM and WDHMM in Table 4,where for WDHMM, ?
=100 is used since it givesthe best performance on the development-test setdevtest.
In the same table, we also provide BLEUresults of using IBM model 4.
Compared to base-line HMM alignment model, WDHMM can im-prove the BLEU score nearly 1% on in-domain testsets, and the improvement reduces to about 0.5%on the out-of-domain test.
When compared to IBMmodel 4, WDHMM still gives higher BLEUscores, and outperform model 4 by about 0.8% onthe test set.
However the gain is reduced to 0.3%on devtest and 0.5% on the out-of-domain nc-test.851 1.5 2 2.5 3 3.5 42929.53030.531log10(tau)BLEU%devtesttestFigure 2: Machine translation results on Europarl,English to French track, devtest and test sets.
TheBLEU score of HMM baseline and the BLEU score ofWDHMM as the prior parameter ?
is varied from 20 to5000.
Note that the x axis is in log scale.1 1.5 2 2.5 3 3.5 42020.52121.522log10(tau)BLEU%nc-testFigure 3: Machine translation results on Europarl,English to French track, out-of-domain test sets.
TheBLEU score of HMM baseline and the BLEU score ofWDHMM as the prior parameter ?
is varied from 20 to5000.
Note that the x axis is in log scale.model devtest test nc-testbaseline HMM  29.69 29.65 20.51WDHMM (?
= 100) 30.59 30.65 20.96IBM model 4 30.29 29.86 20.51Table 4: Comparison of BLEU scores on devtest, test,and nc-test set between various word alignment models.All numbers are in percentage.In order to verify whether these gains fromWDHMM are statistically significant, we imple-mented paired bootstrap resampling method pro-posed by Koehn (2004b) to compute statistical sig-nificance of the above test results.
In table 5, it isshown that BLEU gains of WDHMM over HMMand IBM-4 on different test sets, except the gainover IBM model 4 on the devtest set, are statistical-ly significant with a significance level > 95%.significance level  devtest test nc-testWDHMM (?=100)vs. HMM99.9% 99.9% 99.5%WDHMM (?=100)vs. IBM model 493.7% 99.9% 99.3%Table 5: Statistical significance test of the BLEU im-provement of WDHMM  (?
= 100) vs. HMM baseline,and WDHMM  (?
= 100) vs. IBM model 4 on devtest,test, and nc-test sets.5.3 Runtime performance of WDHMMWDHMM runs as fast as a normal HMM, andthe extra memory needed for the word dependenttransition model is proportional to the vocabularysize of the source language given that the distortionsets of {c(d;e)}  are bucketed.
Runtime speed ofWDHMM and IBM-model 4 using GIZA++ is ta-bulated in table 6.
The results are based on Euro-parl English to French alignment and these testswere conducted on a fast PC with 3.0GHz CPUand 16GB memory.
In Table 6, WDHMM includes5 iterations of model 1 training followed by 5 itera-tions of WDHMM, while "IBM model 4" includes5 iterations for model 1, 5 iterations for HMM, and5 iterations for model 4.
It is shown in Table 6 thatWDHMM is more than four times faster to pro-duce the end-to-end word alignment.model   runtime(min)WDHMM   121IBM model 4    537Table 6: comparison of runtime performance bew-teen WDHMM training and IBM model 4 training usingGIZA++.6 DiscussionOther works have been done to improve transitionmodels in HMM based word alignment.
Och andNey (2000a) have suggested estimating word-classbased transition models so as to provide more de-tailed transition probabilities.
However, due to thesparse data problem, only a small number of wordclasses are usually used and the many words in thesame class still have to share the same transitionmodel.
Toutanova et al (2002) has proposed to86estimate a word-dependent self-transition modelP(stay|e) so that each word can have its own prob-ability to decide whether to stay or jump to a dif-ferent word.
Later Deng and Byrne (2005) pro-posed a word dependent phrase length model tobetter model state occupancy.
However, thesemodel can only model the probability of self-jumping.
Important knowledge of jumping from eto a different position should also be word depen-dent but is not modeled.Another interesting comparison is betweenWDHMM and the fertility-based models, e.g.,IBM model 3-5.
Compared to these models, a ma-jor disadvantage of HMM is the absence of a mod-el of source word fertility.
However, as discussedin (Toutanova et al 2002),the word dependent self-transition model can be viewed as an approxima-tion of fertility model.
i.e., it models the number ofconsecutive target words generated by the sourceword with a geometric distribution.
Therefore, witha well estimated word dependent transition model,this weakness of HMM is alleviated.In this work, we proposed estimating a fullword-dependent transition models in HMM basedword alignment, and with Bayesian learning wecan achieve robust model estimation under thesparse data condition.
We have conducted a seriesof experiments to evaluate this method on wordalignment and machine translation tests, and showsignificant improvement over baseline HMM interms of AER and BLEU.
It also performs betterthan the much more complicated IBM model 4based word alignment model on various wordalignment and machine translation tasks.Acknowledgments  The author is grateful to ChrisQuirk and Arul Menezes for assistance with theMT system and for the valuable suggestions anddiscussions.ReferencesC.
M. Bishop, 2006.
Pattern Recognition and MachineLearning.
Springer.P.
Brown, S. D. Pietra, V. J. D. Pietra, and R. L. Mercer.1994.
The Mathematics of Statistical Machine Trans-lation: Parameter Estimation.
Computational Linguis-tics, 19:263?311.Y.
Deng and W. Byrne, 2005, HMM Word and PhraseAlignment For Statistical Machine Translation, inProceedings of HLT/EMNLP.J.
Gauvain and C.-H. Lee, 1994, Maximum a PosterioriEstimation For Multivariate Gaussian Mixture Ob-servations Of Markov Chains, IEEE Trans on Speechand Audio Processing.J.
Goodman, http://research.microsoft.com/~joshuago/P.
Koehn, F. J. Och, and D. Marcu.
2003.
StatisticalPhrase-Based Translation.
In Proceedings of HLT-NAACL.P.
Koehn, 2004a, Statistical Significance Tests for Ma-chine Translation Evaluation, in Proceedings ofEMNLP.P.
Koehn.
2004b.
Pharaoh: A Beam Search Decoder ForPhrase Based Statistical Machine Translation Mod-els.
In Proceedings of AMTA.P.
Liang, B. Taskar, and D. Klein, 2006, Alignment byAgreement, in Proceedings of NAACL.R.
Moore, W. Yih and A.
Bode, 2006, Improved Dis-criminative Bilingual Word Alignment, In Proceed-ings of COLING/ACL.F.
J. Och and H. Ney.
2000a.
A comparison of Align-ment Models for Statistical Machine Translation.
InProceedings of COLING.F.
J. Och and H. Ney.
2000b.
Improved StatisticalAlignment Models.
In Proceedings of ACL.F.
J. Och and H. Ney.
2000c.
Giza++: Training of statis-tical translation models.
http://www-i6.informatik.rwthaachen.de/och/software/GIZA++.html.F.
J. Och and H. Ney.
2002.
Discriminative training andMaximum Entropy Models for Statistical MachineTranslation, In Proceedings of ACL.F.
J. Och, 2003, Minimum Error Rate Training in Statis-tical Machine Translation.
In Proceedings of ACL.K.
A. Papineni, S. Roukos, T. Ward, and W.-J.
Zhu.2002.
Bleu: A Method For Automatic Evaluation OfMachine Translation.
in Proceedings of ACL.L.
R. Rabiner, 1989 A tutorial on hidden Markov mod-els and selected applications in speech recognition.Proceedings of the IEEE.K.
Toutanova, H. T. Ilhan, and C. D. Manning.
2002.Extensions to HMM-based Statistical Word Align-ment Models.
In Proceedings of EMNLP.S.
Vogel, H. Ney, and C. Tillmann.
1996.
HMM-basedWord Alignment In Statistical Translation.
In Pro-ceedings of COLING.H.
Zhang and D. Gildea, 2005, Stochastic LexicalizedInversion Transduction Grammar for Alignment, InProceedings of ACL.87
