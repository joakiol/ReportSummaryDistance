Proceedings of EACL '99Exploring the Use of Linguistic Features in Domain and GenreClassificationMar ia  Wolters '  and Math ias  K i rs ten  21Inst.
f. Kommunikationsforschung .
Phonetik, Bonn; wolters@ikp.uni-bonn.de2German Natl.
Res.
Center for IT-AiS.KD-, St. Augustin; mathias.kirsten~gmd.deAbst rac tThe central questions are: How usefulis information about part-of-speech fre-quency for text categorisation?
Is it fea-sible to limit word features to contentwords for text classifications?
This isexamined for 5 domain and 4 genre clas-sification tasks using LIMAS, the Ger-man equivalent of the Brown corpus.
Be-cause LIMAS is too heterogeneous, nei-ther question can be answered reliablyfor any of the tasks.
However, the re-sults suggest that both questions haveto be examined separately for each taskat hand, because in some cases, the ad-ditional information can indeed improveperformance.1 In t roduct ionThe greater the amounts of text people can ac-cess and have to process, the more important effi-cient methods for text categorisation become.
Sofar, most research has concentrated on content-based categories.
But determining the genre  ofa text can also be very important, for examplewhen having to distinguish an EU press releaseon the introduction of the euro from a newspapercommentary on the same topic.The results of e.g.
(Lewis, 1992; Yang and Ped-ersen, 1997) indicate that for good content clas-sification, we basically need a vector which con-tains the most relevant words of the text.
Usingn-grams hardly yields significant improvements,because the dimension of the document represen-tation space increases exponentially.
But do word-based vectors also work well for genre detection?Or do we need additional inguistically motivatedfeatures to capture the different styles of writingassociated with different genres?In this paper, we present a pilot study basedon a set of easily computable linguistic features,namely the frequency of part-of-speech (POS)tags, and a corpus of German, LIMAS (Glas,1975), which contains a wide range of differentgenres.
LIMAS is described briefly in Sac.
3, whilesections 2 and 4 motivate the choice of features.The text categorisation experiments are describedin Sec.
5.2 L inguist ic  Cues to Genre2.1 What  is genre?The term "genre" is more frequent in philologyand media studies than in mainstream linguistics(Swales, 1990, p.38).
When it is not used synony-mously with the terms "register" or "style", genreis defined on the basis of non-linguistic criteria.For example, (Biber, 1988) characterises genres interms of author/speaker purpose, while text typesclassify texts on the basis of text-internal criteria.Swales phrases this more precisely: Genres arecollections of communicative events with sharedcommunicative purposes which can vary in theirprototypicality.
These communicative purposesare determined by the discourse community whichproduces and reads texts belonging to a genre.But how can we extract its communicative pur-pose from a given text?
First of all, we need todefine the genres we want to detect.
The defi-nitions which were used in this study are sum-marised in section 3.1.
If we assume that theculture-specific onventions which form the ba-sis for assigning a given text to a certain genreare reflected in the style of the text, and if thatstyle can be characterised quantitatively as a ten-dency to favour certain linguistic options over oth-ers (Herdan, 1960), we can then proceed to searchfor linguistic features which both discriminate wellbetween our genres and can also be computed reli-ably from unannotated text.
Potential sources forsuch options are comparative genre studies (Biber,1988), authorship attribution research (Holmes,1998; Forsyth and Holmes, 1996), content analy-142Proceedings of EACL '99sis (Martindale and MacKenzie, 1995), and quan-titative stylistics (Pieper, 1979).
For the last step,classification, we need a robust statistical methodwhich should preferably work well on sparse andnoisy data.
This aspect will be discussed in moredetail in section 5.In their paper on genre categorization, (Kessleret al, 1997) take a somewhat different approach.They classify texts according to generic facets.Those facets express distinctions that "answer tocertain practical interests" (p. 33).
The "brow"facet roughly corresponds to register, and the"narrative" facet is taken from text type theory,while the "genre" facet most closely correspond toour usage of the term.2.2 Choice of  featuresThere are two basic types of features: ratios andfrequencies.
Typical ratios are the type/token ra-tio, sentence length (in words per sentence), orword length (in characters per words).
More elab-orate ratios which have been found to be useful inquantitative stylistics (Ross and Hunter, 1994) aree.g.
the ratio of determiners to nouns or that ofauxiliaries to VP heads.The most common features to be counted arewords, or, more precisely, word stems.
Whilemost text categorisation research focusses on con-tent words, function words have proved valuablein authorship attribution.
The rationale behindthis is that authors monitor their use of the mostfrequent words less carefully than that of otherwords.
But this is not the reason why functionwords might prove to be useful in genre analy-sis.
Rather, they indicate dimensions such as per-sonal involvement (heavy use of first and secondperson pronouns), or argumentativity (high fre-quency of specific conjunctions).
Content anal-ysis counts the frequency of words which belongto certain diagnostic lasses, such as for exam-ple aggressivity markers.
The frequency of otherlinguistic features uch as part-0f-speech (POS),noun phrases, or infinitive clauses, has been ex-amined selectively in quantitative stylistics.
In hiscomparative analysis of written and spoken genresin English, Biber (Biber, 1988) lists an impressivearray of 67 linguistically motivated features whichcan be extracted reliably from text.
However, hesometimes relies heavily on the fixed word order ofEnglish for their computation, which makes themdifficult to transfer to a language with a more flex-ible word order, such as German.
(Karlgren andCutting, 1994) reports good results in a genre clas-sification task based on a subset of these features,while (Kessler et al, 1997) show that a prudentselection of cues based on words, characters, andratios can perform at least equally well.In our paper, we explore a hybrid approach.Starting from the classical information retrievalrepresentation f texts as vectors of word frequen-cies (Salton and McGill, 1983), we explore howperformance is affected if we includefunction word frequencies.
For example, textswhich aim at generalisable statements maycontain more indefinite articles and pronounsand less definite articles.POS frequencies.
(This essentially condensesinformation implicitly available in the wordvector.)
For example, nominal style shouldlead to a higher frequency of nouns, whereasdescriptive texts may show more adjectivesand adverbials than others.Note that we do not experiment with sophisti-cated feature selection strategies, which might beworthwhile for the POS information (cf.
Sec.
4).POS frequency information is the only higher-level linguistic information which is encoded ex-plicitly.
Most current POS-taggers are reliableenough (at least for English) for their output tobe used as the basis for a classification, whereasrobust, reliable parsers are hard to find.
Anothersource of information would have been the posi-tion of a word in a sentence, but incorporatingthis would have lead to substantially arger featurespaces and will be left to future work.
Seman-tic classes were not examined, because defining,building, fine-tuning, and maintaining such wordlists can be an arduous task (cf.
e.g.
(Klavans andKan, 1998)), which should therefore only be un-dertaken for corpora with both well-defined andwell-represented genres, where inherently fuzzyclass boundaries are less likely to counteract theeffect of careful feature selection.3 The  L IMAS corpus  o f  GermanSince our focus is on genre detection, we decidednot to use common benchmark collections uchas Reuters 1 and OHSUMED 2 because they arerather homogenous with respect o genre.LIMAS is a comprehensive corpus of contem-porary written German, modelled on the Browncorpus (Ku~era and Francis, 1967) and collectedin the early 1970s.
It consists of 500 sources witharound 2000 words each.
It has been completelytagged with POS tags using the MALAGA sys-tem (Beutel, 1998).
MALAGA is based on the1 http://www.research.att.com/lewis/reuters21578.html2 ftp://medir.ohsu.edu/pub/ohsumed143Proceedings of EACL '99STTS tagset for German which consists of 54 cat-egories (Schiller et al, 1995).
The corpus has at-ready been used for text classification by (vonderGrfin, 1999).Since the corpus is rather heterogeneous, we de-fined two sets of tasks, one based on the full cor-pus (CL), the other based on all texts from thecategories law, politics, and economy (LPE) (104sources in all).
In the LPE experiments, empha-sis was on searching for good parameters for thevarious learning algorithms as well as on the con-tribution of POS and punctuation information toclassification accuracy.
The experiments on thecomplete corpus, on the other hand, focus moreon composition of the feature vectors.3.1 Genre  ClassesLIMAS is based on the 33 main categories ofthe Deutsche Bibliographie (German bibliogra-phy).
Each of the bibliography's categories i rep-resented according to its frequency in the textspublished in 1970/1971, so that the corpus can beconsidered representative of the written Germanof that time (Bergenholtz and Mugdan, 1989).Furthermore, the corpus designers took care tocover a wide range of genres within each subcat-egory.
As a result, groups of more than 10 doc-uments taken from LIMAS will be rather hetero-geneous.
For example, press reports can be takenfrom broadsheets or tabloids, they can be com-mentaries, news reports, or reviews of culturalevents.Many  of the main  categories correspond todomains  such as "mathematics"  or "history".A l though not evident f rom the category label,genre distinctions can also be quite importantfor domain  classification, because some domainshave developed specific genres for communicat ionwithin the associated communi ty .
There are threesuch domain categories in our experiments, poli-tics (P), law (L), and economy (E).
Two furthercategories are academic texts from the humani-ties (H) and from the field of science and technol-ogy (S).
In the LPE corpus, this distinction is col-lapsed into "academic" (A), the set of all scholarlytexts in the corpus.
Four categories are based ongenre only.
On one hand, we have press texts (N),and more specifically NH, press texts from highquality broadsheets and magazines, on the otherhand, fiction (F) and FL, a low-quality subset ofF.
For LPE, we defined a category D consistingof articles from quality broadsheets.
Table 1 givesan overview of the categories and the number ofdocuments in each category for each corpus.
Inall subsequent experiments, we assume as base-line the classification accuracy which we get whenL P E H SCL n 20 44 40 109 72CL acc.
96 91,2 92 78 85,6F FL N NHCL n 60 26 53 30CL acc.
88 94,8 89,4 94L P E A DLPE n 20 43 40 45 26LPE acc.
80 58,7 61,5 56,7 75Table 1: Number of documents n in each categoryand classification accuracy acc.
if each documentis judged not  to belong to that category.all documents are assigned to the majority class.The baselines are specified in Tab.
I.4 Va l idat ing  the  FeaturesIf the frequency of POS features does not varysignificantly between categories, adding such in-formation increases both random variation in thedata as well as its dimensionality.
To check forthis, we conducted a series of non-parametric testson CL for each POS tag.In addition, binary classification trees weregrown on the complete set of documents for eachcategory, and the structure of the tree was subse-quently examined.
Classification trees basicallyrepresent an ordered series of tests.
Each treenode corresponds to one test, and the order ofthe tests is specified by the tree's branches.
Alltests are binary.
The outcome of a test higher upin the tree determines which test to perform next.A data item which reaches a leaf is assigned theclass of the majority of the items which reachedit during training.
The trees were grown usingrecursive partitioning; the splitting criterion wasreduction in deviance.
Using the Gini index ledto larger trees and higher misclassification rates.Since the primary purpose of the trees was notprediction of unseen, but analysis of seen data,they were not pruned.
There were no separatetest sets.We tested for 12 categories and all STTS POStags if the distribution of a tag significantly differsbetween documents in a given category and docu-ments not in that category.
These categories con-sist of the nine defined in Sec.
3 plus the content-based domains (Hi) and religion (R), and textsfrom tabloids and similar publications (PL).Choice of  Feature  Values:  The value of a fea-ture is its relative frequency in a given text.
Thefrequencies were standardised using z-scores, sothat the resulting random variables have a mean of0 and a variance of 1.
The z-scores were rounded144Proceedings of EACL '99down to the next integer, so that all featureswhose frequency does not deviate greatly from themean have a value of 0.
Z-scores were computedon the basis of all documents to be compared.This makes ense if we view style as deviation froma default, and such defaults hould be computedrelative to the complete corpus of documents used,not relative to specific classification tasks.Results:  In general, only 7 of all 54 tags showsignificant differences in distribution for morethan half of the categories, and the actual differ-ences are far smaller than a standard eviation.However, for most tasks, there are at least 15 POStags with characteristic distributions, o that in-cluding POS frequency information might well bebeneficial.The four most important content word classesare VVFIN (finite forms of full verbs), NN(nouns), ADJD (adverbial adjectives), and ADJA(attributive adjectives).
Importance is measuredby the number of significant differences in dis-tribution.
A higher incidence of VVFIN char-acterises F, FL, and NL, whereas texts fromacademia or about politics and law show signif-icantly less VVFIN.
The difference between themeans is around 0.2 for F and FL, and below 0.1for the rest.
(Numbers relate to the z-scores).Note that we cannot claim that more VVFINmeans less nouns (NN): scholarly texts both showless VVFIN and less NN than the rest of the cor-pus.
For adjectives, we find that academic textsare significantly richer in ADJA (differences be-tween 0.02-0.04), while FL contains more adver-bial adjectives (difference 0.04).But function words can be equally important in-dicators, especially personal pronouns, which areusually part of the stop word list.
They are sig-nificantly less frequent in academic texts and cat-egories E, L, NH, and P, and more frequent infiction, NL, and R. Again, all differences are at orbelow 0.1.
A lower frequency of personal pronounscan indicate both less interpersonal involvementand shorter eference chains.Other valuable categories are, for example,pronominal adverbs (PAV) and infinitives of auxil-iary verbs (VAINF), where the difference betweenthe means usually lies between 0.2 and 0.4 for sig-nificant differences.
(We restrict ourselves to dis-cussing these in more detail for reasons of space.
)Pronominal adverbs uch as "deswegen" (becauseof this) are especially frequent in texts from lawand science, both of which tend to contain textsof argumentative types.
The frequency of infini-tives of auxiliaries reflects both the use of passivevoice, which is formed with the auxiliary "war-den" in German, and the use of present perfect orpluperfect tense (auxiliary "haben').
In this cor-pus, texts from the domains of law and economycontain more VAINF than others.The potential meaning of common punctuationmarks is quite clear: the longer the sentences anauthor constructs, the fewer full stops and themore commata nd subordinating conj unctions wefind.
However, the frequency of full stops is dis-tinctive only for four categories: L, E, and H havesignificantly fewer full stops, NL has significantlymore.
We also find significantly more commatain fiction than in non-fiction, Possible sources forthis are infinitive clauses and lists of adjectives.With regard to the trees, we examined onlythose splits that actually discriminate well be-tween positive and negative examples with lessthan 40% false positives or negatives.
We willnot present our analyses in detail, but illus-trate the type of information provided by suchtrees with the category F. For this category,PPER, KOMMA, PTKZU ("to" before infinitive),PTKNEG (negation particle), an~t PWS (substi-tuting interrogative pronoun) discriminate well inthe tree.
In the case of PTKZU and PTKNEG,this difference in distribution is conditional, it wasnot observed in the significance tests and surfacedonly through the tree experiments.5 Text  Categor i sa t ion  Exper imentsFor our categorisation experiments, we chose arelational k-nearest-neighbour (k-NN) classifier,RIBL (Emde and Wettschereek, 1996; Bohnebecket al, 1998), and two feature-based k-NN algo-rithms, learning vector quantisation (LVQ, (Ko-honen et al, 1996)), and IBLI(-IG) (Daelemanset al, 1997; Aha et al, 1991).
The reason forchoosing k-NN-based approaches i that this al-gorithm has been very successful in text categori-sation (Yang, 1997).We first ran the experiments on the LPE-corpus, which had mainly exploratory character,then on the complete corpus.In the LPE-experiments, we distinguished sixfeature sets: CW, CWPOS, CWPP, WS, WS-POS, and WSPP, where CW stands for contentword lemmata, WS for all lemmata, POS for POSinformation, and PP for POS and punctuation i -formation.In the CL-experiments, we did not control forthe potential contribution of punctuation featuresto the results, but on the type of lemma fromwhich the features were derived.
We again ex-plored 6 feature sets, CW, CWPOS, WS, WSPOS,FW, and FWPOS, where FW stands for function145Proceedings of EACL '99word lemmata.
Punctuation was included in con-ditions WS, WSPOS, FW, and FWPOS, but notin CW and CWPOS.
In addition to feature type,we also varied the length of the feature vectors.In the following subsections, we outline our gen-eral method for feature selection and evaluationand give a brief description of the algorithms used.We then report on the results of the two suites ofexperiments.5.1 Feature SelectionThe set of all potential features is large - there aremore than 29000 lemmata in the LPE corpus, andmore than 80000 in the full corpus.In a first step we excluded for the LPE corpus,all lemmata occuring less than 5 times in the texts,and for the CL corpus, all lemmata occurring inless than 10 sources, which left us with 4857 lem-mata for LPE and 5440 lemmata nd punctuationmarks for CL.
We then determined the relevanceof each of these lemmata for a given classifica-tion task by their gain ratio (Yang and Pedersen,1997).
From this ranked list of lemmata, we con-structed the final feature sets.5.2 The  A lgor i thmsRIBL: RIBL is a k-NN classification algo-rithm where each object is represented as a setof ground facts, which makes encoding highlystructured data easier.
The underlying first-order logic distance measure is described in(Emde and Wettschereck, 1996; Bohnebeck etal., 1998).
Features were not weighted be-cause using Kononenko's Relief feature weight-ing (Kononenko, 1994) did not significantly af-fect performance in preliminary experiments.The input for RIBL consists of three relationslemma(di,lemma,v), pos(di,POS-Tag,v), and doc-ument(all), with di the document index and v thestandardised frequency, rounded to the next inte-ger value.
In the CL experiments, the lemma tagcovers both real lemmata nd punctuation marks,in LPE, punctuation marks had a separate pre-cidate.
Relations with a feature value of 0 areomitted, reducing the size of the input consider-ably.
For these features, a true relational repre-sentation is not necessary, but that might changefor more complex features uch as syntactic rela-tions.IBL: IBL stores all training set vectors in aninstance base.
New feature vectors are assignedthe class of the most similar instancc.
We use theFuclidean distance metric for determining nearestncighbours.
All experiments were run with (IBL-IG) or without (IBL) weighting the contributionof each feature with its gain ratio.LVQ: LVQ also classifies incoming data basedon prototype vectors.
However, the prototypesare not selected, but interpolated from the trainingdata so as to maximise the accuracy of a nearest-neighbour classifier based on these vectors.
Dur-ing learning, the prototypes are shifted graduallytowards members of the class they represent andaway from members of different classes.
Thereare three main variants of the algorihm, two ofwhich only modify codebook vectors at the deci-sion boundary between classes.5.3 LPE-Exper iments5.3.1 P rocedureFrom the complete set of documents, we con-structed three pairs of training and test sets fortraining the feature classifiers.
The test sets aremutually disjunct; each of them contains 5 posi-tive and 5 negative xamples.
The correspondingtraining sets contain the remaining 95 documents.For RIBL, test set performance is determined us-ing leave-one-out cross validation.
Feature vectorscontained either 100,500, or 1000 lemma features.On the basis of test set performance, we deter-mined precision, recall, and accuracy.
Instead ofdetermining recall/precision breakeven point as in(Joachims, I998) or average precision over differ-ent recall values as in (Yang, 1997), we provideboth values to determine which type of error analgorithm is more susceptible to.
Tab.
2 summa-rizes the results.5.3.2 A lgor i thm-spec l f ic  resu l tsCondition IBL-IG resulted in significantlyhigher precision (+0.5%) than IBL, but lower re-call and accuracy (difference not significant).
Thenumber of neighbouring vectors was also varied(k = 1,3, 5, 7).
For precision, recall, and accuracy,best results were achieved with k = 3.
A purenearest-neighbour approach led to classifying allexamples as negative.
The number of neighboursk was also varied for RIBL.
Contrary to 1BL, itperforms best for k = 1.For the LVQ runs, we used the variant OLVQI.In this algorithm, one codebook vector is adaptedat a time; the rate of codebook vector adaptationis optimised for fast convergence.
The resultingcodebook was not tuned afterwards to avoid over-fitting.
We varied both the number of codebookvectors (10,20,50,90) and the initialisation proce-dure: during one set of runs, each class receivesthe same number of vectors, during the other,the number of codebook vectors is proportional toclass size.
Performance increases if codebook w~.c-146Proceedings of EACL '99Task  Alg.A RIBLIBLLVQE FtlBLIBLLVQL I:tIBLIBLLVQN RIBLIBLLVQP I:tIBLIBLLVQPrec.
RRecall FN FS92,9 94,05 I00 wspos75 75 I000 ws*99,67 I00 500 cwpos97,59 77,18 500 ws75 75 10O0 all100 100 1000 all95,45 I00 I00 wspos75 75 I00 / I000  allI00 I00 I00 ws*I00 I00 100 wspos75 75 I00 all100 I00 I00 all96,93 89,09 500 ws75 75 100/1000 allI00 I00 I00 ws =Table 2: Test set performance averaged over allruns for each task and for the best combination offeature set and number of features, precision andrecall having equal weight.Key: all: ws/wspos/wspp/cw/cwpos/cwpp, cw*:cw/cwpos/cwpp, ws*: ws/wspos/wspptors are assigned proportionally to each class anddeteriorates with the number of codebook vectors,a clear sign of overfitting.LVQ achieves a performance ceiling of 100%precision and recall on nearly all tasks except forgenre task A.
The low average performance of IBLis due to bad results for k = 1; for higher k, IBLperforms as well as LVQ.
Overall, performance de-creases with increasing number of features.
IBL israther robust regarding the choice of feature set.LVQ tends to perform better on data sets derivedfrom both content and function words, with theexception of task A.
Because of the ceiling effect,it almost never matters if the additional linguisticfeatures are included or not.
Recall is significantlybetter than precision for most tasks.RIBL shows the greatest variation in perfor-mance.
Although it performs fairly well, Tab.
2shows differences of up to -5% on precision and-23% on recall.
Overall, ws-based feature setsoutperform cw-based ones.
Performance declinessharply with the number of features.
POS fea-tures almost always have a clear positive effect onrecall (on average +28%, cw* and +16%, ws*),but an even larger negative ffect on precision (-38%, cw* and -39%,ws*), which only shows for 500and 1000 lemma features.
Lemma and POS fre-quency information apparently conflict, with POSfrequency leading to overgeneralization.
Maybesemantic features describe the class boundariesmore adequately.
They may be covered implic-itly in large vectors containing lemmata from thatclass.
For 100 lemmafeatures, where the represen-tation is extremely sparse, we find that includingPOS information does indeed boost performance,especially for the two genre tasks, as we wouldhave predicted.5.4 CL Exper iments5.4.1 P rocedureIn this set of experiments, RIBL and IBL wereboth evaluated using leave-one-out cross valida-tion.
The performance of LVQ is reported onthe basis of ten-fold cross validation for reasonsof computing time.
Training and test sets werealso constructed somewhat differently.
The testset contained the same proportion of positive ex-amples as the training set.
If we had balancedthe test set as above, this would have resulted in4 pairs of sets instead of 10, and much smallertest sets, because some classes, such as L, arevery small.
This problem was not so grave for theLPE experiments because of the ceiling effect andthe small size of the complete data set, therefore,we did not rerun the corresponding experiments.Furthermore, the number of codebook vectors forLVQ was now varied between 10, 50, 100, and 200in order to take into account he increased train-ing set sizes.5.4.2 Resu l tsThe results on the larger corpus differ substan-tially from that on the smaller corpus.
It is fareasier to determine if a text belongs to one of thethree major domains covered in a corpus than toassign a text to a minor domain which covers only4% of the complete corpus.
If the class itself is notconsiderably more homogeneous (with respect othe classifier used) than the rest of the corpus,this will be a difficult task indeed.
Our results ug-gest that the classes were indeed not homogeneousenough to ensure reliable classification.
The rea-son for this is that LIMAS was designed to be asrepresentative as possible, and consequently to beas heterogeneous a possible.
This explains whywe never achieved 100% precision and recall onany data set again.
In fact, results became muchworse, and varied a tot depending mainly on thetype of classifier and the task.
Again, if classes arevery inhomogeneous, any change in the way sim-ilarity between data items is computed can havestrong effects on the composition of the neighbour-hood, and the erratic behaviour observed here is avivid testimony of this.
We therefore chose not topresent general summaries, but to document sometypical patterns of variation.Parameter  sett ings: LVQ gives best results interms of both precision and recall for even initial-isation of codebook vectors, which makes sensebecause the number of positive examples has nowbecome rather small in comparison to the rest ofthe corpus.
A good codebook size appears to be50 vectors.147Proceedings of EACL '99CWCWPOSFWFWPOSWSPOSWSH S50 200 50 20065.2 33.6 42.24 47.1565.2 29.5 42.24 47.1519.6 54 59.79 17.319.6 54 74.4 17.388.3 100 62.45 45.956.6 68 62.45 45.9Table 3: Average LVQ results (precision) for cate-gories H and S, 50 codebook vectors, even initial-ization.For RIBL, restricting the size of the relevantneighbourhood to 1 or 2 gives by far the best re-sults in terms of both precision and recall, but notin terms of accuracy - the negative ffect of falsepositives is too strong.IBL is also sensitive to the size of the neigh-bourhood; again, precision and recall are highestfor k--1.
For this size, incorporating informationgain into the distance measure leads to a clear de-crease in performance.Overal l  per fo rmance:  Unsurprisingly, perfor-mance in terms of precision and recall is ratherpoor.
Average LVQ performance under the bestparameter settings in terms of precision and re-call only improves on the baseline for two genres:H (baseline 78%, accuracy for feature set WSPOS88%) and FL (feature sets CONT and CONTPOS,baseline 94%, accuracy 95%).
Under matchedconditions (same genre, same feature set, samenumber of features, optimal settings), IBL andRIBL both perform significantly worse than LVQ,which can interpolate between data points and sosmooth out at least some of the noise.
For exam-ple, IBL accuracy on task H is 69,1% for both WSand WSPOS, while accuracy on FL never muchexceeds 92% and thus remains just below baseline.RIBL performs best on FL for condition CWPOS,but even then accuracy is only 90%.Size of Feature  Vector:  The number of fea-tures used did not significantly affect the perfor-mance of IBL.
For LVQ, both precision and re-call decrease sharply as the number of featuresincreases (average precision for 50 lemma features29.5%, for 200 24.8%; average recall for 50 9.1%,for 200 7.1%).
But this was not the case for allgenres, as Tab.
3 shows.
The categories H andS are chosen for comparison because they are thelargest.
For H, the precision under conditions CWand CWPOS decreases, all others increase; for S,it is exactly the other way around.Compos i t ion  of  feature  vectors:  Anotherlesson of Tab.
3 is that the effect of the com-position of the feature vectors can vary depend-ing both on the task and on the size of the fea-ture vector.
The dramatic fall in precision forcondition FWPOS, category S, shows that veryclearly.
Here, additional function word informa-tion has blurred the class boundaries, whereas forH, it has sharpened them considerably.
Because ofthe large amount of noise in the results, we wouldbe very hesitant o identify any condition as op-timal or indeed claim that our hypotheses aboutthe role of POS information or content vs. func-tion words could be verified.
However, what theseresults do confirm is that sometimes, comparingdifferent representations might well pay off, as wehave seen in the case of task H, where WSPOSindeed emerges as optimal feature set choice.6 Conc lus ionIn this paper, we examined ifferent linguisticallymotivated inputs for training text classification al-gorithms, focussing on domain- and genre-basedtasks.The most clear-cut result is the influence of thetraining corpus on classifier performance.
If wewant general-purpose classifiers for large genres orcollections of genres, "small" representative cor-pora such as LIMAS will in the end provide toolittle training material, because the emphasis ison capturing the extent of potential variation ina language, and less on providing sufficient num-bers of prototypical instances for text categorisa-tion algorithms.
In addition, genre boundaries arenotoriously fuzzy, and if this inherent variabilityis compounded by sparse data, we indeed havea problem, as Sec.
5.4 showed.
Therefore, fur-ther work into genre classification should focus onwell-defined genres and corpora large enough tocontain a sufficient number of prototypical docu-ments.
In our opinion, further investigations intothe utility of linguistic features for textcategoriza-tion tasks should best be conducted on such cor-pora.Our results neither support nor refute the hy-potheses advanced in Sec.
2.
However, note thatin some cases, the additional non-content wordinformation did indeed improve performance (cf.Tab.
3), so that such representations should atleast be experimented with before settling on con-tent words.AcknowledgementsWe would like to thank Stefan Wrobel, ThomasPortele, and two anonymous reviewers for their148Proceedings of EACL '99comments.
All statistical analyses were con-ducted with R (http://www.ci.tuwien.ac.at/R).Oliver Lorenz added the POS tags to LIMAS.Re ferencesD.
Aha, D. Kibler, and M. Albert.
1991.Instance-based learning algorithms.
MachineLearning, 6:37-66.H.
Bergenholtz and J. Mugdan.
1989.
Zur Kor-pusproblematik in der Computerlinguistik.
InI.
B?tori, W. Lenders, and W. Putschke, edi-tors, Handbuch Computerlinguistik.
deGruyter,Berlin/New York.B.
Beutel.
1998.
Malaga UserManual.
http://www.linguistik.uni-erlangen.de/Malaga.de.html.D.
Biber.
1988.
Variation across Speech andWriting.
Cambridge University Press, Cam-bridge.U.
Bohnebeck, T. Horvath, and S. Wrobel.
1998.Term comparisons in first-order similarity mea-sures.
In Proc.
8th Intl.
Conf.
Ind.
Logic Progr.,pages 65-79.W.
Daelemans, A. van den Bosch, and T. Weijters.1997.
IGTtree: Using trees for compression andclassification in lazy learning algorithms.
AIReview, 11:407-423.W.
Emde and D. Wettschereck.
1996.
Relationalinstance based learning.
In Proc.
13th Intl.Conf.
Machine Learning, pages 122-130.R.I.
Forsyth and D. Holmes.
1996.
Feature~finding for text classification.
Literary and Lin-guistic Computing, 11:163-174.R.
Glas.
1975.
Das LIMAS-Korpus, ein Textkor-pus f/it die deutsche Gegenwartssprache.
Lin-gustische Berichte, 40:63-66.G.
Herdan.
1960.
Type-token mathematics: atextbook of mathematical linguistics.
Mouton,The Hague.D.
Holmes.
1998.
The evolution of stylometry inhumanities scholarschip.
Literary and Linguis-tic Computing, 13:111-117.T.
Joachims.
1998.
Text categorization with Sup-port Vector Machines: Learning with many rel-evant features.
Technical Report LS-8 23, Dept.of Computer Science, Dortmund University.,I.
Karlgren and D. Cutting.
1994.
Recognizingtext genres with simple metrics using discrimi-nant analysis.
In Proc.
COLING Kyoto.B.
Kessler, G. Nunberg, and H. Schiitze.
1997.Automatic lassification of text genre.
In Proc.35th A CL/Sth EACL Madrid, pages 32-38.J.
Klavans and Min-Yen Kan. 1998.
Role of verbsin document analysis.
In Proc.
COLING/ACLMontrdal.T.
Kohonen, J. Kangas, J. Laaksonen, andK.
Torkkola.
1996.
LVQ-PAK - the learningvector quantization package v. 3.0.
TechnicalReport A30, Helsinki University of Technology.I.
Kononenko.
1994.
Estimating attributes: Anal-ysis and extensions ofrelief.
In Proc.
7th Europ.Conf.
Machine Learning, pages 171 - 182.H.
Ku~era nd W Francis.
1967.
Frequency anal-ysis of English usage: lexicon and grammar.Houghton Mifflin, Boston.D.
Lewis.
1992.
Feature selection and feature x-traction for text categorization.
I  Proc.
Speechand Natural Language Workshop, pages 212-217.
Morgan Kaufman.C.
Martindale and D. MacKenzie.
1995.
On theutility of content analysis in author attribution:The Federalist.
Computers and the Humanities,29:259-270.U.
Pieper.
1979.
Uber die Aussagekraft s atistis-chef Methoden fi~r die linguistische Stilanalyse.Narr, Tfibingen.D.
Ross and D. Hunter.
1994. p-EYEBALL:An interactive system for producing stylistic de-scriptions and comparisons.
Computers and theHumanities, 28:1-11.G.
Salton and M.J. McGill.
1983.
Introductionto Modern Information Retrieval.
McGrawHill,New York.A.
Schiller, S. Teufel, and C. Thielen.
1995.Guidelines ftir das Tagging deutscher Textcor-pora mit STTS.
Technical report, IMSStuttgart/Seminar f. Sprachwiss.
Ttibingen.J.
Swales.
1990.
Genre Analysis.
Cambridge Uni-versity Press, Cambridge.A.
yon der Gr/in.
1999.
Wort-, Morphem- und Al-lomorphhgufigkeit in dom~nenspezifischen Kor-pora des Deutschen.
Master's thesis, Insti-tute of Computational Linguistics, Universityof Erlangen-Ntirnberg.Y.
Yang and J. Pedersen.
1997.
A comparativestudy on feature selection in text categorization.In Proc.
14th ICML.Y.
Yang.
1997.
An evaluation of statistical ap-proaches to text categorization.
Technical Re-port CMU-CS-97-127, Dept.
of Computer Sci-ence, Carnegie Mellon University.149
