Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 217?225,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPLearning to Tell Tales: A Data-driven Approach to Story GenerationNeil McIntyre and Mirella LapataSchool of Informatics, University of Edinburgh10 Crichton Street, Edinburgh, EH8 9AB, UKn.d.mcintyre@sms.ed.ac.uk, mlap@inf.ed.ac.ukAbstractComputational story telling has sparkedgreat interest in artificial intelligence,partly because of its relevance to educa-tional and gaming applications.
Tradition-ally, story generators rely on a large repos-itory of background knowledge contain-ing information about the story plot andits characters.
This information is detailedand usually hand crafted.
In this paper wepropose a data-driven approach for gen-erating short children?s stories that doesnot require extensive manual involvement.We create an end-to-end system that real-izes the various components of the gen-eration pipeline stochastically.
Our systemfollows a generate-and-and-rank approachwhere the space of multiple candidate sto-ries is pruned by considering whether theyare plausible, interesting, and coherent.1 IntroductionRecent years have witnessed increased interest inthe use of interactive language technology in ed-ucational and entertainment applications.
Compu-tational story telling could play a key role in theseapplications by effectively engaging learners andassisting them in creating a story.
It could also al-low teachers to generate stories on demand thatsuit their classes?
needs.
And enhance the enter-tainment value of role-playing games1.
The major-ity of these games come with a set of pre-specifiedplots that the players must act out.
Ideally, the plotshould adapt dynamically in response to the play-ers?
actions.Computational story telling has a longstandingtradition in the field of artificial intelligence.
Earlywork has been largely inspired by Propp?s (1968)1A role-playing game (RPG) is a game in which the par-ticipants assume the roles of fictional characters and act outan adventure.typology of narrative structure.
Propp identified inRussian fairy tales a small number of recurringunits (e.g., the hero is defeated, the villain causesharm) and rules that could be used to describe theirrelation (e.g., the hero is pursued and the rescued).Story grammars (Thorndyke, 1977) were initiallyused to capture Propp?s high-level plot elementsand character interactions.
A large body of morerecent work views story generation as a form ofagent-based planning (Theune et al, 2003; Fass,2002; Oinonen et al, 2006).
The agents act ascharacters with a list of goals.
They form plansof action and try to fulfill them.
Interesting storiesemerge as agents?
plans interact and cause failuresand possible replanning.Perhaps the biggest challenge faced by compu-tational story generators is the amount of worldknowledge required to create compelling stories.A hypothetical system must have informationabout the characters involved, how they inter-act, what their goals are, and how they influencetheir environment.
Furthermore, all this informa-tion must be complete and error-free if it is to beused as input to a planning algorithm.
Tradition-ally, this knowledge is created by hand, and mustbe recreated for different domains.
Even the sim-ple task of adding a new character requires a wholenew set of action descriptions and goals.A second challenge concerns the generationtask itself and the creation of stories character-ized by high-quality prose.
Most story genera-tion systems focus on generating plot outlines,without considering the actual linguistic structuresfound in the stories they are trying to mimic (butsee Callaway and Lester 2002 for a notable ex-ception).
In fact, there seems to be little com-mon ground between story generation and naturallanguage generation (NLG), despite extensive re-search in both fields.
The NLG process (Reiter andDale, 2000) is often viewed as a pipeline consist-ing of content planning (selecting and structuringthe story?s content), microplanning (sentence ag-217gregation, generation of referring expressions, lex-ical choice), and surface realization (agreement,verb-subject ordering).
However, story generationsystems typically operate in two phases: (a) creat-ing a plot for the story and (b) transforming it intotext (often by means of template-based NLG).In this paper we address both challenges fac-ing computational story telling.
We propose adata-driven approach to story generation that doesnot require extensive manual involvement.
Ourgoal is to create stories automatically by leverag-ing knowledge inherent in corpora.
Stories withinthe same genre (e.g., fairy tales, parables) typicallyhave similar structure, characters, events, and vo-cabularies.
It is precisely this type of informationwe wish to extract and quantify.
Of course, build-ing a database of characters and their actions ismerely the first step towards creating an automaticstory generator.
The latter must be able to selectwhich information to include in the story, in whatorder to present it, how to convert it into English.Recent work in natural language generation hasseen the development of learning methods for re-alizing each of these tasks automatically with-out much hand coding.
For example, Duboue andMcKeown (2002) and Barzilay and Lapata (2005)propose to learn a content planner from a paral-lel corpus.
Mellish et al (1998) advocate stochas-tic search methods for document structuring.
Stentet al (2004) learn how to combine the syntacticstructure of elementary speech acts into one ormore sentences from a corpus of good and bad ex-amples.
And Knight and Hatzivassiloglou (1995)use a language model for selecting a fluent sen-tence among the vast number of surface realiza-tions corresponding to a single semantic represen-tation.
Although successful on their own, thesemethods have not been yet integrated together intoan end-to-end probabilistic system.
Our work at-tempts to do this for the story generation task,while bridging the gap between story generatorsand NLG systems.Our generator operates over predicate-argumentand predicate-predicate co-occurrence statisticsgathered from corpora.
These are used to pro-duce a large set of candidate stories which aresubsequently ranked based on their interesting-ness and coherence.
The top-ranked candidateis selected for presentation and verbalized us-ing a language model interfaced with RealPro(Lavoie and Rambow, 1997), a text generationengine.
This generate-and-rank architecture cir-cumvents the complexity of traditional generationThis is a fat hen.The hen has a nest in the box.She has eggs in the nest.A cat sees the nest, and can get the eggs.The sun will soon set.The cows are on their way to the barn.One old cow has a bell on her neck.She sees the dog, but she will not run.The dog is kind to the cows.Figure 1: Children?s stories from McGuffey?sEclectic Primer Reader; it contains primary read-ing matter to be used in the first year of schoolwork.systems, where numerous, often conflicting con-straints, have to be encoded during developmentin order to produce a single high-quality output.As a proof of concept we initially focus onchildren?s stories (see Figure 1 for an example).These stories exhibit several recurrent patterns andare thus amenable to a data-driven approach.
Al-though they have limited vocabulary and non-elaborate syntax, they nevertheless present chal-lenges at almost all stages of the generation pro-cess.
Also from a practical point of view, chil-dren?s stories have great potential for educationalapplications (Robertson and Good, 2003).
For in-stance, the system we describe could serve as anassistant to a person who wants suggestions as towhat could happen next in a story.
In the remain-der of this paper, we first describe the componentsof our story generator (Section 2) and explain howthese are interfaced with our story ranker (Sec-tion 3).
Next, we present the resources and evalu-ation methodology used in our experiments (Sec-tion 4) and discuss our results (Section 5).2 The Story GeneratorAs common in previous work (e.g., Shim and Kim2002), we assume that our generator operates in aninteractive context.
Specifically, the user suppliesthe topic of the story and its desired length.
Bytopic we mean the entities (or characters) aroundwhich the story will revolve.
These can be a listof nouns such as dog and duck or a sentence, suchas the dog chases the duck.
The generator nextconstructs several possible stories involving theseentities by consulting a knowledge base containinginformation about dogs and ducks (e.g., dogs bark,ducks swim) and their interactions (e.g., dogschase ducks, ducks love dogs).
We conceptualize218the dog chases the duckthe dog barks the duck runs awaythe dog catches the duck the duck escapesFigure 2: Example of a simplified story tree.the story generation process as a tree (see Figure 2)whose levels represent different story lengths.
Forexample, a tree of depth 3 will only generate sto-ries with three sentences.
The tree encodes manystories efficiently, the nodes correspond to differ-ent sentences and there is no sibling order (thetree in Figure 2 can generate three stories).
Eachsentence in the tree has a score.
Story generationamounts to traversing the tree and selecting thenodes with the highest scoreSpecifically, our story generator applies twodistinct search procedures.
Although we are ul-timately searching for the best overall story atthe document level, we must also find the mostsuitable sentences that can be generated from theknowledge base (see Figure 4).
The space of pos-sible stories can increase dramatically dependingon the size of the knowledge base so that an ex-haustive tree search becomes computationally pro-hibitive.
Fortunately, we can use beam search toprune low-scoring sentences and the stories theygenerate.
For example, we may prefer sentencesdescribing actions that are common for their char-acters.
We also apply two additional criteria in se-lecting good stories, namely whether they are co-herent and interesting.
At each depth in the treewe maintain the N-best stories.
Once we reach therequired length, the highest scoring story is pre-sented to the user.
In the following we describethe components of our system in more detail.2.1 Content PlanningAs mentioned earlier our generator has access toa knowledge base recording entities and their in-teractions.
These are essentially predicate argu-ment structures extracted from a corpus.
In our ex-periments this knowledge base was created usingthe RASP relational parser (Briscoe and Carroll,2002).
We collected all verb-subject, verb-object,verb-adverb, and noun-adjective relations from theparser?s output and scored them with the mutualdog:SUBJ:bark whistle:OBJ:dogdog:SUBJ:bite treat:OBJ:dogdog:SUBJ:see give:OBJ:dogdog:SUBJ:like have: OBJ:doghungry:ADJ:dog lovely:ADJ:dogTable 1: Relations for the noun dog with highMI scores (SUBJ is a shorthand for subject-of,OBJ for object-of and ADJ for adjective-of).information-based metric proposed in Lin (1998):MI = ln(?
w,r,w?
?
?
?
?,r,?
??
w,r,?
?
?
?
?,r,w?
?
)(1)where w and w?
are two words with relation type r.?
denotes all words in that particular relation and?
w,r,w?
?
represents the number of times w,r,w?occurred in the corpus.
These MI scores are usedto inform the generation system about likely entityrelationships at the sentence level.
Table 1 showshigh scoring relations for the noun dog extractedfrom the corpus used in our experiments (see Sec-tion 4 for details).Note that MI weighs binary relations which insome cases may be likely on their own withoutmaking sense in a ternary relation.
For instance, al-though both dog:SUBJ:run and president:OBJ:runare probable we may not want to create the sen-tence ?The dog runs for president?.
Ditransitiveverbs pose a similar problem, where two incongru-ent objects may appear together (the sentence Johngives an apple to the highway is semantically odd,whereas John gives an apple to the teacher wouldbe fine).
To help reduce these problems, we needto estimate the likelihood of ternary relations.
Wetherefore calculate the conditional probability:p(a1,a2 | s,v) =?
s,v,a1,a2 ??
s,v,?,?
?
(2)where s is the subject of verb v, a1 is the first argu-ment of v and a2 is the second argument of v andv,s,a1 6= ?.
When a verb takes two arguments, wefirst consult (2), to see if the combination is likelybefore backing off to (1).The knowledge base described above can onlyinform the generation system about relationshipson the sentence level.
However, a story createdsimply by concatenating sentences in isolationwill often be incoherent.
Investigations into theinterpretation of narrative discourse (Asher andLascarides, 2003) have shown that lexical infor-mation plays an important role in determining219SUBJ:chaseOBJ:chaseSUBJ:runSUBJ:escapeSUBJ:fallOBJ:catch SUBJ:frightenSUBJ:jump1 2265815Figure 3: Graph encoding (partially ordered)chains of eventsthe discourse relations between propositions.
Al-though we don?t have an explicit model of rhetor-ical relations and their effects on sentence order-ing, we capture the lexical inter-dependencies be-tween sentences by focusing on events (verbs)and their precedence relationships in the corpus.For every entity in our training corpus we extractevent chains similar to those proposed by Cham-bers and Jurafsky (2008).
Specifically, we identifythe events every entity relates to and record their(partial) order.
We assume that verbs sharing thesame arguments are more likely to be semanticallyrelated than verbs with no arguments in common.For example, if we know that someone steals andthen runs, we may expect the next action to be thatthey hide or that they are caught.In order to track entities and their associatedevents throughout a text, we first resolve entitymentions using OpenNLP2.
The list of events per-formed by co-referring entities and their gram-matical relation (i.e., subject or object) are sub-sequently stored in a graph.
The edges betweenevent nodes are scored using the MI equationgiven in (1).
A fragment of the action graphis shown in Figure 3 (for simplicity, the edgesin the example are weighted with co-occurrencefrequencies).
Contrary to Chambers and Juraf-sky (2008) we do not learn global narrativechains over an entire corpus.
Currently, we con-sider local chains of length two and three (i.e.,chains of two or three events sharing gram-matical arguments).
The generator consults thegraph when selecting a verb for an entity.
Itwill favor verbs that are part of an event chain(e.g., SUBJ:chase ?
SUBJ:run ?
SUBJ:fall inFigure 3).
This way, the search space is effectivelypruned as finding a suitable verb in the current sen-tence is influenced by the choice of verb in the nextsentence.2See http://opennlp.sourceforge.net/.2.2 Sentence PlanningSo far we have described how we gather knowl-edge about entities and their interactions, whichmust be subsequently combined into a sentence.The backbone of our sentence planner is a gram-mar with subcategorization information which wecollected from the lexicon created by Korhonenand Briscoe (2006) and the COMLEX dictionary(Grishman et al, 1994).
The grammar rules actas templates.
They each take a verb as their headand propose ways of filling its argument slots.
Thismeans that when generating a story, the choice ofverb will affect the structure of the sentence.
Thesubcategorization templates are weighted by theirprobability of occurrence in the reference dictio-naries.
This allows the system to prefer less elab-orate grammatical structures.
The grammar ruleswere converted to a format compatible with oursurface realizer (see Section 2.3) and include in-formation pertaining to mood, agreement, argu-ment role, etc.Our sentence planner aggregates together infor-mation from the knowledge base, without how-ever generating referring expressions.
Althoughthis would be a natural extension, we initiallywanted to assess whether the stochastic approachadvocated here is feasible at all, before venturingtowards more ambitious components.2.3 Surface RealizationThe surface realization process is performed byRealPro (Lavoie and Rambow (1997)).
The sys-tem takes an abstract sentence representation andtransforms it into English.
There are several gram-matical issues that will affect the final realizationof the sentence.
For nouns we must decide whetherthey are singular or plural, whether they are pre-ceded by a definite or indefinite article or with noarticle at all.
Adverbs can either be pre-verbal orpost-verbal.
There is also the issue of selectingan appropriate tense for our generated sentences,however, we simply assume all sentences are inthe present tense.
Since we do not know a prioriwhich of these parameters will result in a gram-matical sentence, we generate all possible combi-nations and select the most likely one according toa language model.
We used the SRI toolkit to traina trigram language model on the British NationalCorpus, with interpolated Kneser-Ney smoothingand perplexity as the scoring metric for the gener-ated sentences.220rootdog.
.
.
barkbark(dog) bark at(dog,OBJ)bark at(dog,duck) bark at(dog,cat)bark(dog,ADV)bark(dog,loudly)hide runduckquack.
.
.run.
.
.fly.
.
.Figure 4: Simplified generation example for the in-put sentence the dog chases the duck.2.4 Sentence Generation ExampleIt is best to illustrate the generation procedure witha simple example (see Figure 4).
Given the sen-tence the dog chases the duck as input, our gen-erator assumes that either dog or duck will be thesubject of the following sentence.
This is a some-what simplistic attempt at generating coherent sto-ries.
Centering (Grosz et al, 1995) and other dis-course theories argue that topical entities are likelyto appear in prominent syntactic positions such assubject or object.
Next, we select verbs from theknowledge base that take the words duck and dogas their subject (e.g., bark, run, fly).
Our beamsearch procedure will reduce the list of verbs toa small subset by giving preference to those thatare likely to follow chase and have duck and dogas their subjects or objects.The sentence planner gives a set of possibleframes for these verbs which may introduce ad-ditional entities (see Figure 4).
For example, barkcan be intransitive or take an object or adver-bial complement.
We select an object for bark,by retrieving from the knowledge base the setof objects it co-occurs with.
Our surface real-izer will take structures like ?bark(dog,loudly)?,?bark at(dog,cat)?, ?bark at(dog,duck)?
and gen-erate the sentences the dog barks loudly, the dogbarks at the cat and the dog barks at the duck.
Thisprocedure is repeated to create a list of possiblecandidates for the third sentence, and so on.As Figure 4 illustrates, there are many candidatesentences for each entity.
In default of generatingall of these exhaustively, our system utilizes theMI scores from the knowledge base to guide thesearch.
So, at each choice point in the generationprocess, e.g., when selecting a verb for an entity ora frame for a verb, we consider the N best alterna-tives assuming that these are most likely to appearin a good story.3 Story RankingWe have so far described most modules of ourstory generator, save one important component,namely the story ranker.
As explained earlier, ourgenerator produces stories stochastically, by rely-ing on co-occurrence frequencies collected fromthe training corpus.
However, there is no guaran-tee that these stories will be interesting or coher-ent.
Engaging stories have some element of sur-prise and originality in them (Turner, 1994).
Ourstories may simply contain a list of actions typi-cally performed by the story characters.
Or in theworst case, actions that make no sense when col-lated together.Ideally, we would like to be able to discern in-teresting stories from tedious ones.
Another im-portant consideration is their coherence.
We haveto ensure that the discourse smoothly transitionsfrom one topic to the next.
To remedy this, wedeveloped two ranking functions that assess thecandidate stories based on their interest and coher-ence.
Following previous work (Stent et al, 2004;Barzilay and Lapata, 2007) we learn these rankingfunctions from training data (i.e., stories labeledwith numeric values for interestingness and coher-ence).Interest Model A stumbling block to assessinghow interesting a story may be, is that the very no-tion of interestingness is subjective and not verywell understood.
Although people can judge fairlyreliably whether they like or dislike a story, theyhave more difficulty isolating what exactly makesit interesting.
Furthermore, there are virtually noempirical studies investigating the linguistic (sur-face level) correlates of interestingness.
We there-fore conducted an experiment where we asked par-ticipants to rate a set of human authored stories interms of interest.
Our stories were Aesop?s fablessince they resemble the stories we wish to gener-ate.
They are fairly short (average length was 3.7sentences) and with a few characters.
We askedparticipants to judge 40 fables on a set of crite-ria: plot, events, characters, coherence and interest(using a 5-point rating scale).
The fables were splitinto 5 sets of 8; each participant was randomly as-signed one of the 5 sets to judge.
We obtained rat-221ings (440 in total) from 55 participants, using theWebExp3 experimental software.We next investigated if easily observable syn-tactic and lexical features were correlated with in-terest.
Participants gave the fables an average in-terest rating of 3.05.
For each story we extractedthe number of tokens and types for nouns, verbs,adverbs and adjectives as well as the numberof verb-subject and verb-object relations.
Usingthe MRC Psycholinguistic database4 tokens werealso annotated along the following dimensions:number of letters (NLET), number of phonemes(NPHON), number of syllables (NSYL), writtenfrequency in the Brown corpus (Kucera and Fran-cis 1967; K-F-FREQ), number of categories in theBrown corpus (K-F-NCATS), number of samplesin the Brown corpus (K-F-NSAMP), familiarity(FAM), concreteness (CONC), imagery (IMAG),age of acquisition (AOA), and meaningfulness(MEANC and MEANP).Correlation analysis was used to assess the de-gree of linear relationship between interest ratingsand the above features.
The results are shown inTable 2.
As can be seen the highest predictor is thenumber of objects in a story, followed by the num-ber of noun tokens and types.
Imagery, concrete-ness and familiarity all seem to be significantlycorrelated with interest.
Story length was not asignificant predictor.
Regressing the best predic-tors from Table 2 against the interest ratings yieldsa correlation coefficient of 0.608 (p < 0.05).
Thepredictors account uniquely for 37.2% of the vari-ance in interest ratings.
Overall, these results indi-cate that a model of story interest can be trainedusing shallow syntactic and lexical features.
Weused the Aesop?s fables with the human ratings astraining data fromwhich we extracted features thatshown to be significant predictors in our correla-tion analysis.
Word-based features were summedin order to obtain a representation for the en-tire story.
We used Joachims?s (2002) SVMlightpackage for training with cross-validation (all pa-rameters set to their default values).
The modelachieved a correlation of 0.948 (Kendall?s tau)with the human ratings on the test set.Coherence Model As well as being interestingwe have to ensure that our stories make senseto the reader.
Here, we focus on local coher-ence, which captures text organization at the level3See http://www.webexp.info/.4http://www.psy.uwa.edu.au/mrcdatabase/uwa_mrc.htmInterest InterestNTokens 0.188??
NLET 0.120?NTypes 0.173??
NPHON 0.140?
?VTokens 0.123?
NSYL 0.125?
?VTypes 0.154??
K-F-FREQ 0.054AdvTokens 0.056 K-F-NCATS 0.137?
?AdvTypes 0.051 K-F-NSAMP 0.103?AdjTokens 0.035 FAM 0.162?
?AdjTypes 0.029 CONC 0.166?
?NumSubj 0.150??
IMAG 0.173?
?NumObj 0.240??
AOA 0.111?MEANC 0.169??
MEANP 0.156?
?Table 2: Correlation values for the human ratingsof interest against syntactic and lexical features;?
: p < 0.05, ??
: p < 0.01.of sentence to sentence transitions.
We created amodel of local coherence using using the EntityGrid approach described in Barzilay and Lapata(2007).
This approach represents each documentas a two-dimensional array in which the columnscorrespond to entities and the rows to sentences.Each cell indicates whether an entity appears in agiven sentence or not and whether it is a subject,object or neither.
This entity grid is then convertedinto a vector of entity transition sequences.
Train-ing the model required examples of both coher-ent and incoherent stories.
An artificial training setwas created by permuting the sentences of coher-ent stories, under the assumption that the originalstory is more coherent than its permutations.
Themodel was trained and tested on the Andrew Langfairy tales collection5 on a random split of the data.It ranked the original stories higher than their cor-responding permutations 67.40% of the time.4 Experimental SetupIn this section we present our experimental set-upfor assessing the performance of our story genera-tor.
We give details on our training corpus, system,parameters (such as the width of the beam), thebaselines used for comparison, and explain howour system output was evaluated.Corpus The generator was trained on 437 sto-ries from the Andrew Lang fairy tale corpus.6 Thestories had an average length of 125.18 sentences.The corpus contained 15,789 word tokens.
We5Aesop?s fables were too short to learn a coherencemodel.6See http://www.mythfolklore.net/andrewlang/.222discarded word tokens that did not appear in theChildren?s Printed Word Database7, a database ofprinted word frequencies as read by children agedbetween five and nine.Story search When searching the story space,we set the beam width to 500.
This means thatwe allow only 500 sentences to be considered ata particular depth before generating the next set ofsentences in the story.
For each entity we select thefive most likely events and event sequences.
Anal-ogously, we consider the five most likely subcate-gorization templates for each verb.
Considerablelatitude is available when applying the rankingfunctions.
We may use only one of them, or oneafter the other, or both of them.
To evaluate whichsystem configuration was best, we asked two hu-man evaluators to rate (on a 1?5 scale) stories pro-duced in the following conditions: (a) score thecandidate stories using the interest function firstand then coherence (and vice versa), (b) score thestories simultaneously using both rankers and se-lect the story with the highest score.
We also ex-amined how best to prune the search space, i.e., byselecting the highest scoring stories, the lowestscoring one, or simply at random.
We created tenstories of length five using the fairy tale corpus foreach permutation of the parameters.
The resultsshowed that the evaluators preferred the versionof the system that applied both rankers simultane-ously and maintained the highest scoring stories inthe beam.Baselines We compared our system against twosimpler alternatives.
The first one does not usea beam.
Instead, it decides deterministically howto generate a story on the basis of the mostlikely predicate-argument and predicate-predicatecounts in the knowledge base.
The second onecreates a story randomly without taking any co-occurrence frequency into account.
Neither ofthese systems therefore creates more than onestory hypothesis whilst generating.Evaluation The system generated stories for10 input sentences.
These were created using com-monly occurring sentences in the fairy tales corpus(e.g., The family has the baby, The monkey climbsthe tree, The giant guards the child).
Each sys-tem generated one story for each sentence result-ing in 30 (3?10) stories for evaluation.
All sto-ries had the same length, namely five sentences.Human judges (21 in total) were asked to rate the7http://www.essex.ac.uk/psychology/cpwd/System Fluency Coherence InterestRandom 1.95?
2.40?
2.09?Deterministic 2.06?
2.53?
2.09?Rank-based 2.20 2.65 2.20Table 3: Human evaluation results: mean story rat-ings for three versions of our system; ?
: signifi-cantly different from Rank-based.stories on a scale of 1 to 5 for fluency (was thesentence grammatical?
), coherence (does the storymake sense overall?)
and interest (how interestingis the story?).
The stories were presented in ran-dom order.
Participants were told that all storieswere generated by a computer program.
They wereinstructed to rate more favorably interesting sto-ries, stories that were comprehensible and overallgrammatical.5 ResultsOur results are summarized in Table 3 which liststhe average human ratings for the three systems.We performed an Analysis of Variance (ANOVA)to examine the effect of system type on the storygeneration task.
Statistical tests were carried outon the mean of the ratings shown in Table 3 forfluency, coherence, and interest.
We observed a re-liable effect of system type by subjects and itemson all three dimensions.
Post-hoc Tukey tests re-vealed that the stories created with our rank-based system are perceived as significantly betterin terms of fluency, interest, and coherence thanthose generated by both the deterministic and ran-dom systems (?
< 0.05).
The deterministic systemis not significantly better than the random one ex-cept in terms of coherence.These results are not entirely surprising.
Thedeterministic system maintains a local restrictedview of what constitutes a good story.
It createsa story by selecting isolated entity-event relation-ships with high MI scores.
As a result, the storiesare unlikely to have a good plot.
Moreover, it tendsto primarily favor verb-object or verb-subject re-lations, since these are most frequent in the cor-pus.
The stories thus have little structural varia-tion and feel repetitive.
The random system useseven less information in generating a story (entity-action relationships are chosen at random withouttaking note of the MI scores).
In contrast to thesebaselines, the rank-based system assesses candi-date stories more globally.
It thus favors coher-ent stories, with varied word choice and structure.223The family has the baby The giant guards the childRandom The family has the baby.
The family is how to emptyup to a fault.
The baby vanishes into the cave.
The fam-ily meets with a stranger.
The baby says for the boy tofancy the creature.The giant guards the child.
The child calls for thewindow to order the giant.
The child suffers from apleasure.
The child longer hides the forest.
The childreaches presently.Determ The family has the baby.
The family rounds up thewaist.
The family comes in.
The family wonders.
Thefamily meets with the terrace.The giant guards the child.
The child rescues the clutch.The child beats down on a drum.
The child feels out ofa shock.
The child hears from the giant.Rank-based The family has the baby.
The baby is to seat the lady atthe back.
The baby sees the lady in the family.
The fam-ily marries a lady for the triumph.
The family quicklywishes the lady vanishes.The giant guards the child.
The child rescues the sonfrom the power.
The child begs the son for a pardon.The giant cries that the son laughs the happiness out ofdeath.
The child hears if the happiness tells a story.Table 4: Stories generated by the random, deterministic, and rank-based systems.A note of caution here concerns referring expres-sions which our systems cannot at the momentgenerate.
This may have disadvantaged the storiesoverall, rendering them stylistically awkward.The stories generated by both the determinis-tic and random systems are perceived as less in-teresting in comparison to the rank-based system.This indicates that taking interest into account is apromising direction even though the overall inter-estingness of the stories we generate is somewhatlow (see third column in Table 3).
Our interestranking function was trained on well-formed hu-man authored stories.
It is therefore possible thatthe ranker was not as effective as it could be sim-ply because it was applied to out-of-domain data.An interesting extension which we plan for thefuture is to evaluate the performance of a rankertrained on machine generated stories.Table 4 illustrates the stories generated by eachsystem for two input sentences.
The rank-basedstories read better overall and are more coherent.Our subjects also gave them high interest scores.The deterministic system tends to select simplis-tic sentences which although read well by them-selves do not lead to an overall narrative.
Interest-ingly, the story generated by the random systemfor the input The family has the baby, scored highon interest too.
The story indeed contains interest-ing imagery (e.g.
The baby vanishes into the cave)although some of the sentences are syntacticallyodd (e.g.
The family is how to empty up to a fault).6 Conclusions and Future WorkIn this paper we proposed a novel method tocomputational story telling.
Our approach hasthree key features.
Firstly, story plot is createddynamically by consulting an automatically cre-ated knowledge base.
Secondly, our generator re-alizes the various components of the generationpipeline stochastically, without extensive manualcoding.
Thirdly, we generate and store multiplestories efficiently in a tree data structure.
Storycreation amounts to traversing the tree and select-ing the nodes with the highest score.
We developtwo scoring functions that rate stories in termsof how coherent and interesting they are.
Experi-mental results show that these bring improvementsover versions of the system that rely solely onthe knowledge base.
Overall, our results indicatethat the overgeneration-and-ranking approach ad-vocated here is viable in producing short storiesthat exhibit narrative structure.
As our system canbe easily rertrained on different corpora, it can po-tentially generate stories that vary in vocabulary,style, genre, and domain.An important future direction concerns a moredetailed assessment of our search procedure.
Cur-rently we don?t have a good estimate of the type ofstories being overlooked due to the restrictions weimpose on the search space.
An appealing alterna-tive is the use of Genetic Algorithms (Goldberg,1989).
The operations of mutation and crossoverhave the potential of creating more varied andoriginal stories.
Our generator would also bene-fit from an explicit model of causality which iscurrently approximated by the entity chains.
Sucha model could be created from existing resourcessuch as ConceptNet (Liu and Davenport, 2004),a freely available commonsense knowledge base.Finally, improvements such as the generation ofreferring expressions and the modeling of selec-tional restrictions would create more fluent stories.Acknowledgements The authors acknowledgethe support of EPSRC (grant GR/T04540/01).We are grateful to Richard Kittredge for his helpwith RealPro.
Special thanks to Johanna Moorefor insightful comments and suggestions.224ReferencesAsher, Nicholas and Alex Lascarides.
2003.
Logics of Con-versation.
Cambridge University Press.Barzilay, Regina and Mirella Lapata.
2005.
Collective con-tent selection for concept-to-text generation.
In Proceed-ings of the HLT/EMNLP.
Vancouver, pages 331?338.Barzilay, Regina and Mirella Lapata.
2007.
Modeling localcoherence: An entity-based approach.
Computational Lin-guistics 34(1):1?34.Briscoe, E. and J. Carroll.
2002.
Robust accurate statisti-cal annotation of general text.
In Proceedings of the 3rdLREC.
Las Palmas, Gran Canaria, pages 1499?1504.Callaway, Charles B. and James C. Lester.
2002.
Narrativeprose generation.
Artificial Intelligence 2(139):213?252.Chambers, Nathanael and Dan Jurafsky.
2008.
Unsupervisedlearning of narrative event chains.
In Proceedings of ACL-08: HLT .
Columbus, OH, pages 789?797.Duboue, Pablo A. and Kathleen R. McKeown.
2002.
Con-tent planner construction via evolutionary algorithms anda corpus-based fitness function.
In Proceedings of the 2ndINLG.
Ramapo Mountains, NY.Fass, S. 2002.
Virtual Storyteller: An Approach to Compu-tational Storytelling.
Master?s thesis, Dept.
of ComputerScience, University of Twente.Goldberg, David E. 1989.
Genetic Algorithms in Search, Op-timization and Machine Learning.
Addison-Wesley Long-man Publishing Co., Inc., Boston, MA.Grishman, Ralph, Catherine Macleod, and Adam Meyers.1994.
COMLEX syntax: Building a computational lexi-con.
In Proceedings of the 15th COLING.
Kyoto, Japan,pages 268?272.Grosz, Barbara J., Aravind K. Joshi, and Scott Weinstein.1995.
Centering: A framework for modeling the lo-cal coherence of discourse.
Computational Linguistics21(2):203?225.Joachims, Thorsten.
2002.
Optimizing search engines us-ing clickthrough data.
In Proceedings of the 8th ACMSIGKDD.
Edmonton, AL, pages 133?142.Knight, Kevin and Vasileios Hatzivassiloglou.
1995.
Two-level, many-paths generation.
In Proceedings of the 33rdACL.
Cambridge, MA, pages 252?260.Korhonen, Y. Krymolowski, A. and E.J.
Briscoe.
2006.
Alarge subcategorization lexicon for natural language pro-cessing applications.
In Proceedings of the 5th LREC.Genova, Italy.Kucera, Henry and Nelson Francis.
1967.
ComputationalAnalysis of Present-day American English.
Brown Uni-versity Press, Providence, RI.Lavoie, Benoit and Owen Rambow.
1997.
A fast and portablerealizer for text generation systems.
In Proceedings of the5th ANCL.
Washington, D.C., pages 265?268.Lin, Dekang.
1998.
Automatic retrieval and clustering of sim-ilar words.
In Proceedings of the 17th COLING.
Montre?al,QC, pages 768?774.Liu, Hugo and Glorianna Davenport.
2004.
ConceptNet: apractical commonsense reasoning toolkit.
BT TechnologyJournal 22(4):211?226.Mellish, Chris, Alisdair Knott, Jon Oberlander, and MickO?Donnell.
1998.
Experiments using stochastic search fortext planning.
In Eduard Hovy, editor, Proceedings of the9th INLG.
New Brunswick, NJ, pages 98?107.Oinonen, K.M., M. Theune, A. Nijholt, and J.R.R.
Uijlings.2006.
Designing a story database for use in automaticstory generation.
In R. Harper, M. Rauterberg, andM.
Combetto, editors, Entertainment Computing ?
ICEC2006.
Springer Verlag, Berlin, volume 4161 of LectureNotes in Computer Science, pages 298?301.Propp, Vladimir.
1968.
The Morphology of Folk Tale.
Uni-versity of Texas Press, Austin, TX.Reiter, E and R Dale.
2000.
Building Natural-Language Gen-eration Systems.
Cambridge University Press.Robertson, Judy and Judith Good.
2003.
Ghostwriter: A nar-rative virtual environment for children.
In Proceedings ofIDC2003.
Preston, England, pages 85?91.Shim, Yunju and Minkoo Kim.
2002.
Automatic short storygenerator based on autonomous agents.
In Proceedings ofPRIMA.
London, UK, pages 151?162.Stent, Amanda, Rashmi Prasad, and Marilyn Walker.
2004.Trainable sentence planning for complex information pre-sentation in spoken dialog systems.
In Proceedings of the42nd ACL.
Barcelona, Spain, pages 79?86.Theune, M., S. Faas, D.K.J.
Heylen, and A. Nijholt.
2003.The virtual storyteller: Story creation by intelligent agents.In S. Gbel, N. Braun, U. Spierling, J. Dechau, and H. Di-ener, editors, TIDSE-2003.
Fraunhofer IRB Verlag, Darm-stadt, pages 204?215.Thorndyke, Perry W. 1977.
Cognitive structures in compre-hension and memory of narrative discourse.
CognitivePsychology 9(1):77?110.Turner, Scott T. 1994.
The creative process: A computermodel of storytelling and creativity.
Erlbaum, Hillsdale,NJ.225
