Accenting unknown words in a specialized languagePierre Zweigenbaum and Natalia GrabarDIAM ?
STIM/DSI, Assistance Publique ?
H?pitaux de Paris& D?partement de Biomath?matiques, Universit?
Paris 6{ngr,pz}@biomath.jussieu.frAbstractWe propose two internal methods for ac-centing unknown words, which both learnon a reference set of accented words thecontexts of occurrence of the various ac-cented forms of a given letter.
One methodis adapted from POS tagging, the other isbased on finite state transducers.We show experimental results for lettere on the French version of the MedicalSubject Headings thesaurus.
With thebest training set, the tagging method ob-tains a precision-recall breakeven pointof 84.24.4% and the transducer method83.84.5% (with a baseline at 64%) forthe unknown words that contain this let-ter.
A consensus combination of both in-creases precision to 92.03.7% with a re-call of 75%.
We perform an error analysisand discuss further steps that might helpimprove over the current performance.1 IntroductionThe ISO-latin family, Unicode or the UniversalCharacter Set have been around for some time now.They cater, among other things, for letters which canbear different diacritic marks.
For instance, Frenchuses four accented es (????)
besides the unaccentedform e. Some of these accented forms correspond tophonemic differences.
The correct handling of suchaccented letters, beyond US ASCII, has not beenimmediate and general.
Although suitable charac-ter encodings are widely available and used, sometexts or terminologies are still, for historical rea-sons, written with unaccented letters.
For instance,in the French version of the US National Libraryof Medicine?s Medical Subject Headings thesaurus(MeSH, (INS, 2000)), all the terms are written inunaccented uppercase letters.
This causes difficul-ties when these terms are used in Natural Languageinterfaces or for automatically indexing textual doc-uments: a given unaccented word may match severalwords, giving rise to spurious ambiguities such as,e.g., marche matching both the unaccented marche(walking) and the accented march?
(market).Removing all diacritics would simplify match-ing, but would increase ambiguity, which is al-ready pervasive enough in natural language pro-cessing systems.
Another of our aims, besides,is to build language resources (lexicons, morpho-logical knowledge bases, etc.)
for the medi-cal domain (Zweigenbaum, 2001) and to learn lin-guistic knowledge from terminologies and cor-pora (Grabar and Zweigenbaum, 2000), includingthe MeSH.
We would rather work, then, with lin-guistically sound data in the first place.We therefore endeavored to produce an accentedversion of the French MeSH.
This thesaurus in-cludes 19,971 terms and 9,151 synonyms, with21,475 different word forms.
Human reaccentua-tion of the full thesaurus is a time-consuming, error-prone task.
As in other instances of preparation oflinguistic resources, e.g., part-of-speech-tagged cor-pora or treebanks, it is generally more efficient for ahuman to correct a first annotation than to produceit from scratch.
This can also help obtain better con-sistency over volumes of data.
The issue is then tofind a method for (semi-)automatic accentuation.The CISMeF team of the Rouen University Hos-Association for Computational Linguistics.the Biomedical Domain, Philadelphia, July 2002, pp.
21-28.Proceedings of the Workshop on Natural Language Processing inpital already accented some 5,500 MeSH termsthat are used as index terms in the CISMeF onlinecatalog of French-language medical Internet sites(Darmoni et al, 2000) (www.chu-rouen.fr/cismef).This first means that less material has to be reac-cented.
Second, this accented portion of the MeSHmight be usable as training material for a learningprocedure.However, the methods we found in the literaturedo not address the case of ?unknown?
words, i.e.,words that are not found in the lexicon used by theaccenting system.
Despite the recourse to both gen-eral and specialized lexicons, a large number of theMeSH words are in this case, for instance those intable 1.
One can argue indeed that the compila-cryomicroscopie dactylolysedecarboxylases decoquinatedenitrificans deoxyribonucleasedesmodonte desoxyadrenalinedextranase dichlorobenzidinedicrocoeliose diiodotyrosinedimethylamino dimethylcysteinedioctophymatoidea diosgenineTable 1: Unaccented words not in lexicon.tion of a larger lexicon should reduce the propor-tion of unknown words.
But these are for the mostpart specialized, rare words, some of which we didnot find even in a large reference medical dictionary(Garnier and Delamare, 1992).
It is then reasonableto try to accentuate automatically these unknownwords to help human domain experts perform fasterpost-editing.
Moreover, an automatic accentuationmethod will be reusable for other unaccented textualresources.
For instance, the ADM (Medical Diagno-sis Aid) knowledge base online at Rennes University(Seka et al, 1997) is another large resource which isstill in unaccented uppercase format.We first review existing methods (section 2).
Wethen present two trainable accenting methods (sec-tion 3), one adapted from part-of-speech tagging, theother based on finite-state transducers.
We show ex-perimental results for letter e on the French MeSH(section 4) with both methods and their combina-tion.
We finally discuss these results (section 5) andconclude on further research directions.2 BackgroundPrevious work has addressed text accentuation, withan emphasis on the cases where all possible wordsare assumed to be known (listed in a lexicon).
Theissue in that case is to disambiguate unaccentedwords when they match several possible accentedword forms in the lexicon ?
the marche/march?
ex-amples in the introduction.Yarowsky (1999) addresses accent restoration inSpanish and in French, and notes that they can belinked to part-of-speech ambiguities and to seman-tic ambiguities which context can help to resolve.He proposes three methods to handle these: N-gramtagging, Bayesian classification and decision lists,which obtain the best results.
These methods relyeither on full words, on word suffixes or on parts-of-speech.
They are tested on ?the most problem-atic cases of each ambiguity type?, extracted fromthe Spanish AP Newswire.
The agreement with hu-man accented words reaches 78.4?98.4% dependingon ambiguity type.Spriet and El-B?ze (1997) use an N-gram modelon parts-of-speech.
They evaluate this method on a19,000 word test corpus consisting of news articlesand obtain a 99.31% accuracy.
In this corpus, only2.6% of the words were unknown, among which89.5% did not need accents.
The resulting error rate(0.3%) accounts for nearly one half of the total er-ror rate, but is so small that it is not worth trying toguess accentuation for unknown words.The same kind of approach is used in projectR?ACC (Simard, 1998).
Here again, unknownwords are left untouched, and account for one fourthof the errors.
We typed the words in table 1through the demonstration interface of R?ACC on-line at www-rali.iro.umontreal.ca/Reacc/: none ofthese words was accented by the system (7 out of16 do need accentuation).When the unaccented words are in the lexicon,the problem can also be addressed as a spelling cor-rection task, using methods such as string edit dis-tance (Levenshtein, 1966), possibly combined withthe previous approach (Ruch et al, 2001).However, these methods have limited power whena word is not in the lexicon.
At best, they might saysomething about accented letters in grammatical af-fixes which mark contextual, syntactic constraints.We found no specific reference about the accentua-tion of such ?unknown?
words: a method that, whena word is not listed in the lexicon, proposes an ac-cented version of that word.
Indeed, in the aboveworks, the proportion of unknown words is too smallfor specific steps to be taken to handle them.
The sit-uation is quite different in our case, where about onefourth of the words are ?unknown?.
Moreover, con-textual clues are scarce in our short, often ungram-matical terms.We took obvious measures to reduce the numberof unknown words: we filtered out the words thatcan be found in accented lexicons and corpora.
Butthis technique is limited by the size of the corpus thatwould be necessary for such ?rare?
words to occur,and by the lack of availability of specialized Frenchlexicons for the medical domain.We then designed two methods that can learn ac-centing rules for the remaining unknown words: (i)adapting a POS-tagging method (Brill, 1995) (sec-tion 3.3); (ii) adapting a method designed for learn-ing morphological rules (Theron and Cloete, 1997)(section 3.4).3 Accenting unknown words3.1 Filtering out know wordsThe French MeSH was briefly presented in the in-troduction; we work with the 2001 version.
The partwhich was accented and converted into mixed caseby the CISMeF team is that of November 2001.
Asmore resources are added to CISMeF on a regularbasis, a larger number of these accented terms mustnow be available.
The list of word forms that oc-cur in these accented terms serves as our base lex-icon (4861 word forms).
We removed from thislist the ?words?
that contain numbers, those that areshorter than 3 characters (abbreviations), and con-verted them in lower case.
The resulting lexicon in-cludes 4054 words (4047 once unaccented).
Thislexicon deals with single words.
It does not try toregister complex terms such as myocardial infarc-tion, but instead breaks them into the two words my-ocardial and infarction.A word is considered unknown when it is notlisted in our lexicon.
A first concern is to filter outfrom subsequent processing words that can be foundin larger lexicons.
The question is then to find suit-able sources of additional words.We used various specialized word lists found onthe Web (lexicon on cancer, general medical lex-icon) and the ABU lexicon (abu.cnam.fr/DICO),which contains some 300,000 entries for ?gen-eral?
French.
Several corpora provided accentedsources for extending this lexicon with some med-ical words (cardiology, haematology, intensive care,drawn from the current state of the CLEF corpus(Habert et al, 2001), and drug monographs).
Wealso used a word list extracted from the French ver-sions of two other medical terminologies: the In-ternational Classification of Diseases (ICD-10) andthe Microglossary for Pathology of the SystematizedNomenclature of Medicine (SNOMED).
This wordlist contains 8874 different word forms.
The totalnumber of word forms of the final word list was276 445.After application of this list to the MeSH, 7407words were still not recognized.
We converted thesewords to lower case, removed those that did not in-clude the letter e, were shorter than 3 letters (mainlyacronyms) or contained numbers.
The remaining5188 words, among which those listed in table 1,were submitted to the following procedure.3.2 Representing the context of a letterThe underlying hypotheses of this method are thatsufficiently regular rules determine, for most words,which letters are accented, and that the context ofoccurrence of a letter (its neighboring letters) is agood basis for making accentuation decisions.
Weattempted to compile these rules by observing theoccurrences of e????
in a reference list of words(the training set, for instance, the part of the FrenchMeSH accented by the CISMeF team).
In the fol-lowing, we shall call pivot letter a letter that is partof the confusion set e????
(set of letters to discrimi-nate).An issue is then to find a suitable description ofthe context of a pivot letter in a word, for instancethe letter ?
in excis?e.
We explored and comparedtwo different representation schemes, which under-lie two accentuation methods.3.3 Accentuation as contextual taggingThis first method is based on the use of a part-of-speech tagger: Brill?s (1995) tagger.
We considereach word as a ?string of letters?
: each letter makesone word, and the sequence of letters of a wordmakes a sentence.
The ?tag?
of a letter is the ex-pected accented form of this letter (or the same letterif it is not accented).
For instance, for the word en-dometre (endometer), to be accented as endom?tre,the ?tagged sentence?
is e/e n/n d/d o/o m/m e/?
t/tr/r e/e (in the format of Brill?s tagger).
The regularprocedure of the tagger then learns contextual accen-tuation rules, the first of which are shown on table 2.Brill Format Gloss(1) e ?
NEXT2TAG i e.i) e!
?
(2) e ?
NEXT1OR2TAG o e.?o) e!
?
(3) e ?
NEXT1OR2TAG a e.?a) e!
?
(4) e ?
NEXT1OR2WD e e.?e) e!
?
(5) e ?
NEXT2TAG h e.h) e!
?
(6) ?
?
NEXTBIGRAM n e ?ne) ?!
?
(7) ?
e NEXTBIGRAM m e ?me) ?!
e(8) e ?
NEXTBIGRAM t r etr ) e!
?
(9) ?
e NEXT1OR2OR3TAG x ?.?.
?x) ?!
e(10) e ?
NEXT1OR2TAG y e.?y) e!
?
(11) e ?
NEXT2TAG u e.u) e!
?
(12) e ?
SURROUNDTAG t i tei) e!
?
(13) ?
?
NEXTBIGRAM s e ?se) ?!
?Table 2: Accentuation correction rules, of the form?change t1to t2if test true on x [y]?.
NEXT2TAG =second next tag, NEXT1OR2TAG = one of next 2 tags,NEXTBIGRAM = next 2 words, NEXT1OR2OR3TAG = oneof next 3 tags, SURROUNDTAG = previous and nexttags,Given a new ?sentence?, Brill?s tagger first assignseach ?word?
its mots frequent tag: this consists inaccenting no e. The contextual rules are then ap-plied and successively correct the current accentu-ation.
For instance, when accenting the word flex-ion, rule (1) first applies (if e with second next tag= i, change to ?)
and accentuates the e to yield fl?x-ion (as in ...?mie).
Rule (9) applies next (if ?
withone of next three tags = x, change to e) to correctthis accentuation before an x, which finally resultsin flexion.
These rules correspond to representationsof the contexts of occurrence of a letter.
This rep-resentation is mixed (left and right contexts can becombined, e.g., in SURROUNDTAG, where both imme-diate left and right tags are examined), and can ex-tend to a distance of three letters left and right, butin restricted combinations.3.4 Mixed context representationThe ?mixed context?
representation used byTheron and Cloete (1997) folds the letters of a wordaround a pivot letter: it enumerates alternatelythe next letter on the right then on the left, until itreaches the word boundaries, which are marked withspecial symbols (here, ^ for start of word, and $ forend of word).
Theron & Cloete additionally repeatan out-of-bounds symbol outside the word, whereaswe dispense with these marks.
For instance, thefirst e in excis?e (excised) is represented as themixed context in the right column of the first row oftable 3.
The left column shows the order in whichthe letters of the word are enumerated.
The next tworows explain the mixed context representations forthe two other es in the word.
This representationWord Mixed Context=Output^ e x c i s ?
e $2 .
1 3 4 5 6 7 8 x^ c i s e e $=e^ e x c i s ?
e $8 7 6 5 4 2 .
1 3 e s $ i c x e^=?^ e x c i s ?
e $8 7 6 5 4 3 2 .
1 $ e s i c x e^=eTable 3: Mixed context representations.caters for contexts of different sizes and facilitatestheir comparison.Each of these contexts is unaccented (it is meantto be matched with representations of unaccentedwords) and the original form of the pivot letter isassociated to the context as an output (we use thesymbol ?=?
to mark this output).
Each context isthus converted into a transducer: the input tape is themixed context of a pivot letter, and the output tape isthe appropriate letter in the confusion set e???
?.The next step is to determine minimal discrimi-nating contexts (figure 1).
To obtain them, we joinall these transducers (OR operator) by factoring theircommon prefixes as a trie structure, i.e., a determin-istic transducer that exactly represents the trainingset.
We then compute, for each state of this trans-ducer and for each possible output (letter in the con-fusion set) reachable from this state, the number ofpaths starting from this state that lead to this output.^allergie$, ^chirurgie$i^r?fugi?$^cytologie$^?chographie$^lipoatrophie$rhuoe?ee$g3 ?505 e,65 e6 e1 ?63 e1 ?86 e,Figure 1: Trie of mixed contexts, each state showingthe frequency of each possible output.We call a state unambiguous if all the paths fromthis state lead to the same output.
In that case, forour needs, these paths may be replaced with a short-cut to an exit to the common output (see figure 1).This amounts to generalizing the set of contexts byreplacing them with a set of minimal discriminatingcontexts.Given a word that needs to be accented, the firststep consists in representing the context of each ofits pivot letters.
For instance, the word biologie:$igoloib^ .
Each context is matched with the trans-ducer in order to find the longest path from the startstate that corresponds to a prefix of the context string(here, $igo).
If this path leads to an output state, thisoutput provides the proposed accented form of thepivot letter (here, e).
If the match terminates earlier,we have an ambiguity: several possible outputs canbe reached (e.g., h?morragie matches $ig).We can take absolute frequencies into account toobtain a measure of the support (confidence level)for a given output O from the current state S: howmuch evidence there is to support this decision.
Itis computed as the number of contexts of the train-ing set that go through S to an output state labelledwith O (see figure 1).
The accenting procedure canchoose to make a decision only when the supportfor that decision is above a given threshold.
Table 4Context Support Gloss Examples$igo=e 65 ?ogie cytologie$ih=e 63 ?hie lipoatrophie$uqit=e 77 ?tique am?lanotiqueu=e 247 -eu- activateur, calleuxx=e 68 -ex- excis?eTable 4: Some minimal discriminating contexts.shows some minimal discriminating contexts learntfrom the accented part of the French MeSH with ahigh support threshold.
However, in previous exper-iments (Zweigenbaum and Grabar, 2002), we testeda range of support thresholds and observed that thegain in precision obtained by raising the supportthreshold was minor, and counterbalanced by a largeloss in recall.
We therefore do not use this devicehere and accept any level of support.Instead, we take into account the relative frequen-cies of occurrence of the paths that lead to the dif-ferent outputs, as marked in the trie.
A probabilistic,majority decision is made on that basis: if one of thecompeting outputs has a relative frequency above agiven threshold, this output is chosen.
In the presentexperiments, we tested two thresholds: 0.9 (90% ormore of the examples must support this case; thismakes the correct decision for h?morragie) and 1(only non-ambiguous states lead to a decision: nodecision for the first e in hemorragie, which weleave unaccented).Simpler context representations of the same fam-ily can also be used.
We examined right contexts(a variable-length string of letters on the right of thepivot letter) and left contexts (idem, on the left).3.5 Evaluating the rulesWe trained both methods, Brill and contexts (mixed,left and right), on three training sets: the 4054 wordsof the accented part of the MeSH, the 54,291 lem-mas of the ABU lexicon and the 8874 words in theICD-SNOMED word list.
To check the validity ofthe rules, we applied them to the accented part ofthe MeSH.
The context method knows when it canmake a decision, so that we can separate the wordsthat are fully processed (f , all es have lead to deci-sions) from those that are partially (p) processed ornot (n) processed at all.
Let fcthe number of correctaccentuations in f .
If we decide to only propose anaccented form for the words that get fully accented,we can compute recall Rfand precision Pffiguresas follows: Rf=fcf+p+nand Pf=fcf.
Similarmeasures can be computed for p and n, as well asfor the total set of words.We then applied the accentuation rules to the 5188accentable ?unknown?
words of the MeSH.
No goldstandard is available for these words: human vali-dation was necessary.
We drew from that set a ran-dom sample containing 260 words (5% of the total)which were reviewed by the CISMeF team.
Becauseof sampling, precision measures must include a con-fidence interval.We also tested whether the results of several meth-ods can be combined to increase precision.
We sim-ply applied a consensus rule (intersection): a wordis accepted only if all the methods considered agreeon its accentuation.The programs were developed in the Perl5 lan-guage.
They include a trie manipulation packagewhich we wrote by extending the Tree::Trie pack-age, online on the Comprehensive Perl Archive Net-work (www.cpan.org).4 ResultsThe baseline of this task consists in accenting no e.On the accented part of the MeSH, it obtains an ac-curacy of 0.623, and on the test sample, 0.642.
TheBrill tagger learns 80 contextual rules with MeSHtraining (208 on ABU and 47 on CIM-SNOMED).The context method learns 1,832 rules on the MeSHtraining set (16,591 on ABU and 3,050 on CIM-SNOMED).Tables 5, 6 and 7 summarize the validation resultsobtained on the accented part of the MeSH.
Set de-notes the subset of words as explained in section 3.5.Cor.
stands for the number of correctly accentedwords.Not surprizingly, the best global precision is ob-tained with MeSH training (table 6).
The mixedcontext method obtains a perfect precision, whereasBrill reaches 0.901 (table 5).
ABU and CIM-SNOMED training also obtain good results (table 7),again better with the mixed context method (0.912?0.931) than with Brill (0.871?0.895).
We performedthe same tests with right and left contexts (table 6):precision can be as good for fully processed words(set f ) as that of mixed contexts, but recall is alwayslower.
The results of these two context variants aretherefore not kept in the following tables.
Both pre-cision and recall are generally slightly better withthe majority decision variant.
If we concentrate onthe fully processed words (f ), precision is alwayshigher than the global result and than that of wordswith no decision (n).
The n class, whose wordsare left unaccented, generally obtain a precision wellover the baseline.
Partially processed words (p) arealways those with the worst precision.training set cor.
recall precisionciMeSH 3646 0.899 0.9010.009ABU 3524 0.869 0.8710.010CIM-SNOMED 3621 0.893 0.8950.009Table 5: Validation: Brill, 4054 words of accentedMeSH.context set cor.
recall precisionciright n 1906 0.470 0.7470.017p 943 0.233 0.8040.023f 324 0.080 1.0000.000tot 3173 0.783 0.7840.013left n 743 0.183 0.6490.028p 500 0.123 0.4280.028f 1734 0.428 1.0000.000tot 2977 0.734 0.7360.014mixed n 7 0.002 1.0000.000p 0 0.000 0.0000.000f 4040 0.997 1.0000.000tot 4047 0.998 1.0000.000majority decision (0.9)mixed n 2 0.000 1.0000.000p 0 0.000 0.0000.000f 4045 0.998 1.0000.000tot 4047 0.998 1.0000.000Table 6: Validation: different context methods,MeSH training, 4054 words of accented MeSH.Precision and recall for the unaccented part ofthe MeSH are showed on tables 8 and 9.
Theglobal results with the different training sets atbreakeven point, with their confidence intervals, arenot really distinguishable.
They are clustered from0.8190.047 to 0.8420.044, except the unambigu-ous decision method trained on MeSH which standsa bit lower at 0.8000.049 and the Brill taggertrained on ABU (0.785).
If we only consider fullyprocessed words, precision can reach 0.8840.043(ICD-SNOMED training, majority decision), with arecall of 0.731 (or 0.8760.043 / 0.758 with MeSHtraining, majority decision).Consensus combination of several methods (ta-ble 8) does increase precision, at the expense ofrecall.
A precision/recall of 0.9200.037/0.750 isABU training (strict)set cor.
recall precisioncin 368 0.091 0.8640.033p 227 0.056 0.6680.050f 3164 0.780 0.9640.006tot 3759 0.927 0.9290.008majority decision (0.9)cor.
recall precisionci111 0.027 0.8600.06077 0.019 0.5240.0813585 0.884 0.9510.0073773 0.931 0.9320.008CIM-SNOMED trainingn 176 0.043 0.7520.055p 114 0.028 0.4250.059f 3400 0.839 0.9590.007tot 3690 0.910 0.9120.009majority decision (0.9)57 0.014 0.8030.09351 0.013 0.3000.0693607 0.890 0.9480.0073715 0.916 0.9180.008Table 7: Validation: mixed contexts, strict (thresh-old = 1) and majority (threshold = 0.9) decisions,4054 words of accented MeSH.training set cor.
recall precisionciMeSH 219 0.842 0.8420.044ABU 204 0.785 0.7850.050CIM-SNOMED 218 0.838 0.8380.045Combined methodsmesh/Brill + mesh/majority 195 0.750 0.9200.037mesh/Brill + mesh/majorityf185 0.712 0.9300.036mesh+abu+cim-snomed/Brill 178 0.685 0.9270.037+ mesh/majorityTable 8: Evaluation on the rest of the MeSH: Brill,estimate on 5% sample (260 words).obtained by combining Brill and the mixed contextmethod (majority decision), with MeSH training onboth sides.
The same level of precision is obtainedwith other combinations, but with lower recalls.5 Discussion and ConclusionWe showed that a higher precision, which shouldmake human post-editing easier, can be obtained intwo ways.
First, within the mixed context method,three sets of words are separated: if only the ?fullyprocessed?
words f are considered (table 9), preci-sion/recall can reach 0.884/0.731 (CIM-SNOMED,majority) or 0.876/0.758 (MeSH, majority).
Second,the results of several methods can be combined witha consensus rule: a word is accepted only if all thesemethods agree on its accentuation.
The combinationof Brill mixed contexts (majority decision), for in-stance with MeSH training on both sides, increasesprecision to 0.9200.037 with a recall still at 0.750(table 8).The results obtained show that the methods pre-sented here obtain not only good performance ontheir training set, but also useful results on the tar-MeSH training (strict)set cor.
recall precisioncin 19 0.073 0.7310.170p 15 0.058 0.4290.164f 174 0.669 0.8740.046tot 208 0.800 0.8000.049majority decisioncor.
recall precisionci8 0.031 0.7270.26311 0.042 0.4580.199197 0.758 0.8760.043216 0.831 0.8310.046ABU training (strict)n 30 0.115 0.8820.108p 32 0.123 0.7110.132f 153 0.588 0.8450.053tot 215 0.827 0.8270.046majority decision13 0.050 0.9290.13511 0.042 0.7860.215194 0.746 0.8360.048218 0.838 0.8380.045CIM-SNOMED trainingn 27 0.104 0.8180.132p 19 0.073 0.4870.157f 168 0.646 0.8940.044tot 214 0.823 0.8230.046majority decision14 0.054 0.8240.1819 0.035 0.3210.173190 0.731 0.8840.043213 0.819 0.8190.047Table 9: Evaluation on the rest of the MeSH: mixedcontexts, estimate on same 5% sample.get data.
We believe these methods will allow us toreduce dramatically the final human time needed toaccentuate useful resources such as the MeSH the-saurus and ADM knowledge base.It is interesting that a general-language lexiconsuch as ABU can be a good training set for accent-ing specialized-language unknown words, althoughthis is true with the mixed context method and thereverse with the Brill tagger.A study of the 44 errors made by the mixed con-text method (table 9, MeSH training, majority deci-sion: 216 correct out of 260) revealed the follow-ing errors classes.
MeSH terms contain some En-glish words (academy, cleavage) and many Latinwords (arenaria, chrysantemi, denitrificans), someof which built over proper names (edwardsiella).These loan words should not bear accents; some oftheir patterns are correctly processed by the meth-ods presented here (i.e., unaccented eae$, ella$), butothers are not distinguishable from normal Frenchwords and get erroneously accented (rena of are-naria is erroneously processed as in r?nal; acad?myas in acad?mie).
A first-stage classifier might helphandle this issue by categorizing Latin (and English)words and excluding them from processing.
Ourfirst such experiments are not conclusive and add asmany errors as are removed.Another class of errors are related with mor-pheme boundaries: some accentuation rules whichdepend on the start-of-word boundary would needto apply to morpheme boundaries.
For in-stance, pilo/erection fails to receive the ?
of r^e=?
(^?rection), apic/ectomie erroneously receives an ?as in cc=?
(c?cit?).
An accurate morpheme seg-menter would be needed to provide suitable inputto this process without again adding noise to it.In some instances, no accentuation decision couldbe made because no example had been learnt for aspecific context (e.g., accentuation of c?falo in ce-faloglycine).We also uncovered accentuation inconsistenciesin both the already accented MeSH words and thevalidated sample (e.g., bacterium or bact?rium indifferent compounds).
Cross-checking on the Webconfirmed the variability in the accentuation of rarewords.
This shows the difficulty to obtain consistenthuman accentuation across large sets of complexwords.
One potential development of the present au-tomated accentuation methods could be to check theconsistency of word lists.
In addition, we discoveredspelling errors in some MeSH terms (e.g., bethane-chol instead of betanechol prevents the proper ac-centuation of beta).Finally, further testing is necessary to check therelevance of these methods to other accented lettersin French and in other languages.AcknowledgementsWe wish to thank Magaly Douy?re, Beno?t Thirionand St?fan Darmoni, of the CISMeF team, for pro-viding us with accented MeSH terms and patientlyreviewing the automatically accented word samples.References[Brill1995] Eric Brill.
1995.
Transformation-based error-driven learning and natural language processing: Acase study in part-of-speech tagging.
ComputationalLinguistics, 21(4):543?565.
[Darmoni et al2000] St?fan J. Darmoni, J.-P. Leroy,Beno?t Thirion, F. Baudic, Magali Douyere, andJ.
Piot.
2000.
CISMeF: a structured health resourceguide.
Methods Inf Med, 39(1):30?35.
[Garnier and Delamare1992] M. Garnier and V. Dela-mare.
1992.
Dictionnaire des Termes de M?decine.Maloine, Paris.
[Grabar and Zweigenbaum2000] Natalia Grabar andPierre Zweigenbaum.
2000.
Automatic acquisitionof domain-specific morphological resources from the-sauri.
In Proceedings of RIAO 2000: Content-BasedMultimedia Information Access, pages 765?784,Paris, France, April.
C.I.D.
[Habert et al2001] Beno?t Habert, Natalia Grabar, PierreJacquemart, and Pierre Zweigenbaum.
2001.
Build-ing a text corpus for representing the variety of medi-cal language.
In Corpus Linguistics 2001, Lancaster.
[INS2000] Institut National de la Sant?
et de la RechercheM?dicale, Paris, 2000.
Th?saurus Biom?dicalFran?ais/Anglais.
[Levenshtein1966] V. I. Levenshtein.
1966.
Binary codescapable of correcting deletions, insertions, and rever-sals.
Soviet Physics-Doklandy, pages 707?710.
[Ruch et al2001] Patrick Ruch, Robert H. Baud, AntoineGeissbuhler, Christian Lovis, Anne-Marie Rassinoux,and A. Rivi?re.
2001.
Looking back or looking allaround: comparing two spell checking strategies fordocuments edition in an electronic patient record.
JAm Med Inform Assoc, 8(suppl):568?572.
[Seka et al1997] LP Seka, C Courtin, and P Le Beux.1997.
ADM-INDEX: an automated system for index-ing and retrieval of medical texts.
In Stud Health Tech-nol Inform, volume 43 Pt A, pages 406?410.
Reidel.
[Simard1998] Michel Simard.
1998.
Automatic inser-tion of accents in French text.
In Proceedings of theThird Conference on Empirical Methods in NaturalLanguage Processing, Grenade.
[Spriet and El-B?ze1997] Thierry Spriet and Marc El-B?ze.
1997.
R?accentuation automatique de textes.In FRACTAL 97, Besan?on.
[Theron and Cloete1997] Pieter Theron and Ian Cloete.1997.
Automatic acquisition of two-level morpholog-ical rules.
In Ralph Grishman, editor, Proceedingsof the Fifth Conference on Applied Natural LanguageProcessing, pages 103?110, Washington, DC, March-April.
ACL.
[Yarowsky1999] David Yarowsky.
1999.
Corpus-basedtechniques for restoring accents in Spanish and Frenchtext.
In Natural Language Processing Using VeryLarge Corpora, pages 99?120.
Kluwer Academic Pub-lishers.
[Zweigenbaum and Grabar2002] Pierre Zweigenbaumand Natalia Grabar.
2002.
Accenting unknown words:application to the French version of the MeSH.
InWorkshop NLP in Biomedical Applications, pages69?74, Cyprus, March.
EFMI.
[Zweigenbaum2001] Pierre Zweigenbaum.
2001.
Re-sources for the medical domain: medical terminolo-gies, lexicons and corpora.
ELRA Newsletter, 6(4):8?11.
