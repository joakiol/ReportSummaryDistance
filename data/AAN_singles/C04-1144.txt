A Multiple-Document Summarization System with UserInteractionHiroyuki SAKAIToyohashi University of Technology1-1 Hibarigaoka, Tempaku,Toyohashi 441-8580,Japan,sakai@smlab.tutkie.tut.ac.jpShigeru MASUYAMAToyohashi University of Technology1-1 Hibarigaoka, Tempaku,Toyohashi 441-8580,Japan,masuyama@tutkie.tut.ac.jpAbstractWe propose a multiple-document summa-rization system with user interaction.
Oursystem extracts keywords from sets of docu-ments to be summarized and shows the key-words to a user on the screen.
Among them,the user selects some keywords reflectinghis/her needs.
Our system controls the pro-duced summary by using these selected key-words.
For evaluation of our method, weparticipated in TSC3 of NTCIR4 workshopby letting our system select 12 best key-words regarding scoring by the system.
Ourparticipated system attained the best per-formance in content evaluation among sys-tems not using sets of questions.
Moreover,we evaluated effectiveness of user interac-tion in our system.
With user interaction,our system attained both higher coverageand precision than that without user inter-action.1 IntroductionRecent rapid progress of computer and com-munication technologies enabled us to accessenormous amount of machine-readable informa-tion easily.
However, this has caused the in-formation overload problem.
In order to solvethis problem, automatic summarization meth-ods have been studied (Mani and T.Maybury,1999).
In particular, the necessity for amultiple-document summarization has been in-creasing and the multiple-document summa-rization technology has been intensively studiedrecently (Mani, 2001).In this paper, we define multiple-documentsummarization as a process for producing asummary from a relevant document set.
Such adocument set may be very large and may con-tain a number of topics.
It is preferable thata summary produced by a multiple-documentsummarization system from the document setcovers all topics contained in the document set.However, it is difficult to produce a summarythat covers all the topics in the document setwith a small number of characters.
For example,a document set relevant to ?releasing AIBO?contains some topics, e.g., what is AIBO?, howto sell AIBO?, etc.
Moreover, sentences recog-nized as important sentences considerably dif-fer person to person (Nomoto and Matsumoto,2001).
This is because ?summarization need?,i.e., topics a different person wants to read, maydiffer.
Hence, we propose a multiple-documentsummarization system with user interaction forcoping appropriately with user?s summarizationneed.
Our system extracts keywords from a doc-ument set to be summarized and shows the key-words to a user.
Among them, the user selectskeywords reflecting user?s summarization need.Our system controls a produced summary byusing the keywords selected by the user.
For re-alizing our purpose, we have devised a scoringmethod for keywords extraction specialized toour purpose.
We would like to emphasize herethe fact that scoring of words for extracting key-words shown to a user is crucial for the systemperformance as well as different from those usedin usual automatic indexing.We participated in TSC3 (Text Summariza-tion Challenge - 3) of NTCIR4 workshop 1 andattained the best performance in content evalu-ation among systems not using sets of questions.Note that our system participated in TSC3 is anautomatic summarization system without userinteraction by letting our system with user in-teraction select 12 best keywords regarding scor-ing by the system.
Moreover, we evaluated ef-fectiveness of user interaction and that with userinteraction attained both higher coverage andprecision than that without user interaction.2 Feature of our multiple-documentsummarization systemOur multiple-document summarization systemproposed in this paper is different from previ-1http://www.lr.pi.titech.ac.jp/tsc/index-en.htmlously proposed multiple-document summariza-tion methods (see, e.g.,(Barzilay et al, 1999),(Mani and Bloedorn, 1999), (Goldstein et al,2000), (Ando et al, 2000), (Lin and Hovy,2002), (Nobata and Sekine, 2002), (Hirao et al,2003)) in that: (1) Our system can produce asummary coping appropriately with each user?ssummarization need by letting a user select key-words reflecting user?s summarization need.
(2)The keywords are extracted automatically froma document set to be summarized by calculatinga score to each noun contained in the documentset.
The formula to calculate scores consists ofnot only frequency of nouns and document fre-quency used in tf ?
idf but also distribution ofnouns in the document set and location of nounsin documents or the document set.
The rea-son why such factors are used will be explainedin the next section.
(3) Our system deletes re-dundant adnominal verb phrases in sentences toreduce the number of characters in a sentence.The deletable adnominal verb phrases are de-cided statistically by using entropy based on aprobability that verbs modify noun, etc.
Ourprevious method (Sakai and Masuyama, 2002)adjusted to multiple-document summarizationso that more deletable adnominal verb phrasesare recognized, is used in this system.The interactive summarization system hasbeen introduced for the first time by (Saggionand Lapalme, 2002).
The system proposed in(Saggion and Lapalme, 2002) is based on shal-low syntactic and semantic analysis, conceptualidentification and text re-generation, while, oursystem is based on a statistical method.3 The method to extract relevantkeywordsA relevant document set S to be summarizedmay be regarded as a document set obtainedby a hypothetical query from the entire doc-ument set ?
to be considered.
In TSC3, theentire document set consists of newspaper arti-cles, Mainichi newspaper and Yomiuri newspa-per, Japanese daily newspapers, from January1 to December 31, 1998, 1999.
We explain amethod to extract keywords relevant to such ahypothetical query from document set S. Here,we define such keywords as relevant keywords ti,i = 1, 2, .
.
.
, k. We assign scores to nouns con-tained in document set S and nouns assigned alarge score are extracted as relevant keywords.A large score is assigned if a noun fulfills thefollowing four conditions.1.
The noun that appears frequently in thedocument set S to be summarized.2.
The noun that appears uniformly in eachdocument d ?
S.3.
The noun that appears in the beginningof a document (i.e., the 1st sentence) andin the beginning of the document set inchronological order (i.e., the 1st docu-ment).4.
The noun that does not appear frequentlyin entire document set ?.Our method for extracting relevant keywordsconsists of the following two steps.Step 1: Calculate score W (ti, S) of noun ti(i = 1, .
.
.
, n) contained in document setS.Step 2: Extract the nouns with k largest scoreW (ti, S) as relevant keywords.The score W (ti, S) is calculated by formula 1.W (ti, S) = (0.5 + Tf(ti, S)maxi=1,...,nTf(ti, S))?
(0.5 + En(ti, S)maxi=1,...,nEn(ti, S))?maxd?S 1 + nl(d)?
nlf(ti, d)nl(d)?maxd?S 1 + |S| ?
rt(ti, d)|S|?idf(ti,?)
(1)where,Tf(ti, S): frequency of noun ti contained indocument set S. This is calculated by for-mula 2.Tf(ti, S) =?d?Stf(ti, d) (2)where, tf(ti, d) is a frequency of noun ti indocument d.En(ti, S): entropy based on the probabilitythat noun ti appears in document d ?
S.This is calculated by formula 3 to be intro-duced later.nl(d): the number of sentences in documentd ?
S.nlf(ti, d): the line number of a sentence con-taining noun ti for the first time in docu-ment d ?
S.rt(ti, d): the document number of documentd containing noun ti for the first time indocument set S in chronological order.idf(ti,?
): idf(Baeza-Yates and Ribeiro-Neto,1999) value assigned to noun ti in entiredocument set ?.En(ti, S) is an entropy based on a probabilitythat noun ti appears in document d ?
S. For ex-ample, En(ti, S) assigned to noun ti containedonly in one document d ?
S is 0.
Though suchnoun ti may be an important noun for documentd, it may be an irrelevant noun for document setS.
Hence, noun ti that is assigned small entropyvalue should not be extracted as a relevant key-word.
However, a noun that appears uniformlyin each document contained in document set Shas a large entropy value.
En(ti, S) is calcu-lated by formula 3.En(ti, S) = ?
?d?SP (ti, d) log2(P (ti, d)) (3)where, P (ti, d) = tf(ti, d)Tf(ti, S) (4)The 3rd term in formula 1 is to assign a largevalue to a noun appearing in the beginning of adocument.
The 4th term in formula 1 is to as-sign a large value to a noun appearing in thebeginning of a document set in chronologicalorder.
The reason why these members are in-cluded is that the 1st sentence in the 1st docu-ment frequently contains important information(see, e.g.,(Nobata and Sekine, 2002)).4 The method to extract importantsentencesThe method to extract important sentencesmeasures similarity between a sentence and theset of relevant keywords selected by a user, andextracts sentences assigned large similarity asimportant sentences.
The similarity is calcu-lated as cosine metric between a vector of asentence and a vector of the set of relevant key-words.
If the same noun as relevant keywordsis contained frequently in a sentence, the co-sine metric assigned to the sentence has a largevalue.
The method to extract important sen-tences is summarized as follows: Here, we definerelevant keywords shown to a user as keywordset K and define relevant keywords selected bya user as keyword set U .Step 1: Re-calculate score of relevant key-words ti?s by the following formula.
Here,we define the number of keywords shown toa user to be k.W ?
(ti, S) ={ (1 + 0.5k)W (ti, S), ti ?
UW (ti, S), otherwise (5)Step 2: Generate relevant keyword vector VKconsisting of W ?
(ti, S) (i = 1, .
.
.
, k) as-signed to each relevant keyword (ti ?
K).VK = (W ?
(t1, S), W ?
(t2, S), .
.
.
, W ?
(tk, S))Step 3: Generate sentence vector Vs consist-ing of W ?
(tj , S) (j = 1, .
.
.
,m) assigned toeach noun contained in sentence s (tj ?
s).Vs = (W ?
(t1, S), W ?
(t2, S), .
.
.
, W ?
(tm, S))Step 4: Calculate a cosine metric betweenvector VK and vector Vs as similaritysim(s,K) by formula 6.sim(s,K) = VK ?Vs|VK||Vs| (6)Step 5: Extract the sentences with m largestsimilarity sim(s,K) as important sen-tences and output these m sentences inchronological order.In document set S, the 1st sentence containedin the 1st document containing important sen-tences in chronological order is always adoptedas an important sentence in order to improvethe readability.5 The method to delete redundantinformationIn the multiple-document summarization, it isnecessary to measure the degree of closeness ofcontents in extracted sentences (or documents)and to delete redundant information.
This isbecause, the documents including the same con-tents may exist in a document set to be summa-rized.
Our multiple-document summarizationsystem identifies close sentences in extractedimportant sentences set and close documents inthe document set, and deletes redundant infor-mation contained therein.First, redundant information contained in thesentences set is deleted as follows.Step 1: Measure the difference d(s1, s2) be-tween cosine metric sim(s1,K) assigned tosentence s1 and sim(s2,K) assigned to sen-tence s2.d(s1, s2) = |sim(s1,K)?
sim(s2,K)| (7)Step 2: If d(s1, s2) has a value smaller than athreshold value, delete sentence si having asmaller cosine metric sim(si,K).We determined the threshold value to be 0.0001in Step 2.
This is a sufficiently small value toregard contents of s1 identical to contents of s2.Next, redundant information contained in thedocument set is deleted as follows.
Here, wedefine a set of important sentences contained indocument di as sdi.
The method is as follows.Step 1: Generate vector Vsd1 , consisting ofW ?
(ti, S) (i = 1, .
.
.
, n) assigned to nounscontained in sd1.Vsd1 = (W ?
(t1, S),W ?
(t2, S), .
.
.
,W ?
(tn, S))Step 2: Generate vector Vsd2 , consisting ofW ?
(tj , S) (j = 1, .
.
.
,m) assigned to nounscontained in sd2.Vsd2 = (W ?
(t1, S),W ?
(t2, S), .
.
.
,W ?
(tm, S))Step 3: Calculate a cosine metric between vec-tor Vsd1 and vector Vsd2 as similaritysim(sd1, sd2).Step 4: If sim(sd1, sd2) has a value larger thana threshold value, delete document di (i =1 or 2) having a smaller score W (sdi) (sdiis in di).
Score W (sdi) is calculated by thefollowing formula 8.W (sdi) =?s?sdisim(s,K) (8)Here, documents d1 and d2 are newspaper arti-cles issued on the same day.
We determined thethreshold value to be 0.85 in Step 4 by trial anderror using sample data provided by the orga-nizer of TSC3.
Note that this sample data hasnot used in the formal run as a document setto be summarized.
Note that if di is deleted,sentences contained in document di are not ex-tracted and the important sentences extractedby our system are changed.
Hence, our systemexecutes this algorithm to delete documents andthe algorithm to extract important sentences it-eratively until no document is deleted by thisalgorithm.6 The method to reduce the numberof characters in a sentenceOur system deletes redundant adnominal verbphrases in sentences to reduce the number ofcharacters in a sentence.
We define adnomi-nal verb phrases as phrases that modify a nounand include a verb modifying the noun.
For ex-ample, in the case of ?SONY ga kaihatsu shitaaibo( : the AIBO devel-oped by SONY?, ?SONY ga kaihatsu shita(: developed by SONY)?
is anadnominal verb phrase, which modifies noun?aibo( : AIBO)?.
Here, the adnominalverb phrase ?SONY ga kaihatsu shita(: developed by SONY)?
may be deletedif a user has known that AIBO was developedby SONY.
We define an adnominal verb phrasemodifying a noun n as V P (n).
Redundantadnominal verb phrases are deleted by an im-proved method of (Sakai and Masuyama, 2002)proposed by us in order to apply to multipledocuments summarization.
For more details,please refer to reference (Sakai and Masuyama,2002) 2.
The method is as follows.Step 1: Calculate score endf(n) to assign tonoun n modified by adnominal verb phraseV P (n) by formula 9.Step 2: Calculate score W (V P (n), s) for ad-nominal verb phrase V P (n) by formula 12.Step 3: Delete adnominal verb phrase V P (n)if the score endf(n) has a value smallerthan threshold value ?
(endf(n)) and thescore W (V P (n), s) has a value smaller thanthreshold value ?
(W (V P (n), s)).We decided threshold value ?
(endf(n)) as 0.7and threshold value ?
(W (V P (n), s)) as 8.7 inStep 3.
These threshold values are decided byexperiments with training corpus not to be sum-marized in the experiments.
Score endf(n) ex-presses the degree of modifier necessity of nounn and is calculated by formula 9.endf(n) = 1 +H(n)idf(n,?)
(9)Here, H(n) is an entropy based on a probabil-ity that verbs modify noun n. It reflects ?fre-quency of modification of noun n by adnomi-nal verb phrases?, ?variety of adnominal verbphrases modifying noun n?.
H(n) is calculatedby formula 10:H(n) = ?
?v?V (n)P (v, n) log2(P (v, n)) (10)2The method of deleting adnominal verb phrases pro-posed in (Sakai and Masuyama, 2002) attained precision79.3%.P (v, n) = f(v, n)?v?V (n) f(v, n)(11)where,V (n): set of verbs contained in adnominal verbphrases modifying noun n in entire docu-ment set ?,f(v, n): frequency of verb v modifying noun nin entire document set ?.Next, W (V P (n), s) is calculated by formula 12.W (V P (n), s) = NM(n)IM(V P (n), s)0.5 + 0.5CV (n, s) (12)NM(n) = 0.5 + endf(n)J(n) (13)where,IM(V P (n), s): a factor to reflect rating of con-text in adnominal verb phrase V P (n) con-tained in sentence s.CV (n, s): the number of occurrences of noun nmodified by adnominal verb phrases fromthe 1st sentence in the 1st document to sen-tence s in document d ?
S in document setS in chronological order.J(n): the number of common nouns containedin noun n if noun n is a compound noun.The IM(V P (n), s) is calculated by formula 14.IM(V P (n), s) = 0.5 +R?c?V P (n)I(c, s) (14)I(c, s) = W?
(c, S)0.5 + 0.5CT (c, s) (15)where,R: the number of segments composing adnom-inal verb phrase V P (n),W ?
(c, S): the score calculated by formula 5 tonoun c contained in adnominal verb phraseV P (n).CT (c, s): the number of occurrences of noun ccontained in adnominal verb phrases fromthe 1st sentence in the 1st document to sen-tence s in document d ?
S in document setS in chronological order.We introduced CV (n, s) in formula 12 andCT (c, s) in formula 15 in order to recognizemore deletable adnominal verb phrases than ourprevious method applied directly to multiple-document summarization.Figure 1: A summary produced by our systemFigure 2: A summary produced by changing rel-evant keywords7 ImplementationWe implemented our method and developed amultiple-document summarization system.
Weemployed JUMAN 3 as a morphological ana-lyzer, and KNP4 as a parser.
We show a sum-mary produced by our system in Figure 1.
Thedocument set to be summarized contains 9 doc-uments relevant to ?releasing AIBO?
and thesummary consists of less than 236 characters.Moreover, we show a summary in Figure 2 whena user selects keywords relevant to the move-ment and performance of AIBO (e.g., ?
(artificial intelligence)?)
and deletes keywordsrelevant to the way to sell (e.g., ?
(Reser-vation)?).
Comparing Figure 1 with Figure 2,we can make sure that summaries have beenchanged by keywords selected by a user.8 Evaluations of our system in TSC3We participate in TSC3 (Text SummarizationChallenge - 3) of NTCIR4 workshop for evalu-ation of information access technologies.
Thepurpose of TSC3 is to evaluate performanceof automatic multiple-document summarizationthat summarizes newspaper articles from twosources (Mainichi newspaper and Yomiuri news-paper from January 1 to December 31, 1998,3http://www-lab25.kuee.kyoto-u.ac.jp/nl-resource/juman.html4http://www-lab25.kuee.kyoto-u.ac.jp/nl-resource/knp.htmlFigure 3: Content evaluation1999.)
Our system participated in TSC3 isnot a system with user interaction for realizingautomatic multiple documents summarization.Hence, we define the following execution of oursystem to be ?Auto?
for realizing an automaticmultiple-document summarization system with-out user interaction.Auto: The execution of our system where 12best keywords regarding scoring by the sys-tem are selected.The number of keywords selected by the systemis determined by trial and error using sampledata provided by the organizer of TSC3.
Themain evaluation method of TSC3 is :Content evaluation: Human judges matchsummaries they produced with system re-sults at sentence level, and evaluate the re-sults based on the degree of the matching(how well they match).
The sentences inthe human-produced summaries have val-ues that show the degree of importance,and these values are taken into account atthe final evaluation 5.8.1 Evaluation results of TSC3The result of content evaluation is shown inFigure 3 6.
Here, ?AUTO?
shows our systemthat participated in TSC3.
?Lead?
is the leadmethod, a baseline method.
In TSC3, we aregiven the sets of questions about important in-formation of the document sets by the organizerof TSC3.
Note that these sets of questions areproduced from summaries made by human ascorrect data.
(For example: when will AIBO5http://www.lr.pi.titech.ac.jp/tsc/cfp3/task description e.html6About 383 characters are involved in a summary of?short?
and about 742 characters are involved in a sum-mary of ?long?.be released ?
etc.)
Here, we exclude evalua-tion results of a system that uses the sets ofquestions for producing summaries of multipledocuments 7.
The reason is as follows.
Asmentioned above, the sets of questions are pro-duced from summaries made by human as cor-rect data.
Hence, we consider that using thesets of questions as machine-readable informa-tion for producing summaries is not realistic.Moreover, we consider that comparing systemsusing the sets of questions with systems not us-ing them by ranking is unfair.By the result shown in figure 3, our systemthat implemented ?AUTO?
has attained thebest performance among the systems not usingthe sets of questions.8.2 Evaluation of user interactionOur system is essentially a multiple-documentsummarization system with user interaction.Hence, we evaluate effectiveness of user interac-tion of our system in this subsection.
For eval-uating it, we consider the following execution ofour system:Interaction: Execution of our system whererelevant keywords contained in the set ofquestions are selected, and relevant key-words not contained in the set of questionsare deleted.The ?Interaction?
simulates user interaction onour system.
(i.e., we regard the set of questionsmentioned at the beginning of Sec.8.1 as user?ssummarization need.
Since the set of questionsproduced from summaries by human (i.e., user),we will be able to regard the questions as user?ssummarization need.)
The coverage and pre-cision of ?Interaction?
is shown in Figure 4.Moreover, the coverage and precision of ?Auto?and ?Lead?
are shown for comparison.
Here,the coverage and precisions which take redun-dancy into account are obtained by using thescoring tool provided for the subtask in TSC3.9 DiscussionFrom the result shown in figure 3, our systemparticipating in TSC3 as ?Auto?
attained thebest performance among the systems not usingthe sets of questions.
We think the reason whythe good performance was attained is that thefirst 12 keywords extracted from a documentset to be summarized by scoring by our method7In TSC3, systems not using the sets of questions andsystems using them were evaluated together.Figure 4: Evaluation of user interactionwere appropriate.
Sentences extracted by usingkeywords irrelevant to the document set maynot probably be important.From the result shown in figure 4, we concludethat the ?Interaction?
is more effective than the?AUTO?.
Moreover, the effectiveness of user in-teraction in the case of ?long?
is more remark-able than that of ?short?.
The reason why theeffectiveness of user interaction in the case of?long?
is more remarkable is as follows.
In thecase of ?short?, our system has to extract sen-tences fewer than that of ?long?.
Even if a userhad changed relevant keywords to use for sen-tence extraction, the sentences extracted by oursystem are not necessarily changed in the caseof ?short?.
However, the extracted sentencesare greatly changed in the case of ?long?
whena user had changed relevant keywords.
Hence,we consider that sentences are extracted wellby changing relevant keywords in the case of?long?.AcknowledgmentThis work was supported in part by The21st Century COE Program ?Intelligent HumanSensing?, from the ministry of Education, Cul-ture, Sports, Science and Technology of Japanand The Grant-in-Aid from the Japan Societyfor the Promotion of Science.ReferencesR.
Ando, B. Boguraev, R. Byrd, and M. Neff.2000.
Multi-document summarization by vi-sualizing topical content.
In Proceedings ofthe ANLP/NAACL 2000 Workshop on Auto-matic Summarization, pages 79?88.R.
Baeza-Yates and B. Ribeiro-Neto.
1999.Modern Information Retrieval.
Addison Wes-ley.R.
Barzilay, K. McKeown, and M. Elhadad.1999.
Information fusion in the context ofmulti-document summarization.
In Proceed-ings of the 37th Annual Meeting of the Asso-ciation for Computatonal Linguistics, pages550?557.J.
Goldstein, V. Mittal, J. Carbonel, andM.
Kantrowitz.
2000.
Multi-document sum-marization by sentence extraction.
In Pro-ceedings of the ANLP/NAACL 2000 Work-shop on Automatic Summarization, pages 40?48.T.
Hirao, K. Takeushi, H. Isozaki, Y. Sasaki,and E. Maeda.
2003.
Svm-based multi-document summarization integrating sen-tence extraction with bunsetsu eliminate.
IE-ICE Trans.
on Information and Systems,E86-D(9):1702?1709.C-Y.
Lin and E. Hovy.
2002.
From single tomulti-document summarizaton: A prototypesystem and its evaluation.
In Proceedings ofthe 40th Anniversary Meeting of the Asso-ciation for Computational Linguistics (ACL-02), pages 457?464.I.
Mani and E. Bloedorn.
1999.
Summariz-ing similarities and differences among relateddocuments.
Information Retrieval, 1(1):35?67.I.
Mani and M. T.Maybury.
1999.
Advancesin Automatic Text Summarization.
the MITPress.I.
Mani.
2001.
Automatic Summarization.John Benjamins Publishing Company.C.
Nobata and S. Sekine.
2002.
A summariza-tion system with categorization of documentsets.
In Working Notes of the Third NTCIRWorkshop Meeting, pages 33?38.T.
Nomoto and Y. Matsumoto.
2001.
An ex-perimental comparison of supervised and un-supervised approaches to text summariza-tion.
In Proceedings of the 2001 IEEE Inter-national Conference on Data Mining, pages630?632.H.
Saggion and G. Lapalme.
2002.
Generat-ing indicative-informative summaries with su-mum.
Computational Linguistics, 28(4):497 ?526.H.
Sakai and S. Masuyama.
2002.
Unsupervisedknowledge acquisition about the deletion pos-sibility of adnominal verb phrases.
In Pro-ceedings of Workshop on Multilingual Sum-marization and Question Answering 2002(post-conference workshop to be held in con-junction with COLING-2002), pages 49?56.
