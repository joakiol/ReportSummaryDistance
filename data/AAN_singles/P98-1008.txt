Time Mapping with HypergraphsJ an  W.  Amtrup  Vo lker  WeberComput ing  Research Laboratory University of Hamburg,New Mexico State University Computer  Science Department,Las Cruces, NM 88003,USA Vogt-K611n-Str.
30, D-22527 Hamburg, Germanyemail: j amtrup~cr l .nmsu,  edu email: weber@in format ik .un i -hamburg .deAbst rac tWord graphs are able to represent a large num-ber of different utterance hypotheses in a verycompact manner.
However, usually they con-tain a huge amount of redundancy in termsof word hypotheses that cover almost identi-cal intervals in time.
We address this problemby introducing hypergraphs for speech process-ing.
Hypergraphs can be classified as an ex-tension to word graphs and charts, their edgespossibly having several start and end vertices.By converting ordinary word graphs to hyper-graphs one can reduce the number of edgesconsiderably.
We define hypergraphs formally,present an algorithm to convert word graphsinto hypergraphs and state consistency proper-ties for edges and their combination.
Finally, wepresent some empirical results concerning raphsize and parsing efficiency.1 In t roduct ionThe interface between a word recognizer andlanguage processing modules is a crucial issuewith modern speech processing systems.
Givena sufficiently high word recognition rate, it suf-fices to transmit the most probable word se-quence from the recognizer to a subsequentmodule (e.g.
a parser).
A slight extension overthis best chain mode would be to deliver n-bestchains to improve language processing results.However, it is usually not enough to deliverjust the best 10 or 20 utterances, at least notfor reasonable sized applications given todaysspeech recognition technology.
To overcome thisproblem, in most current systems word graphsare used as speech-language interface.
Wordgraphs offer a simple and efficient means to rep-resent a very high number of utterance hypothe-ses in a extremely compact way (Oerder andNey, 1993; Aubert and Ney, 1995).und (and) dann (then)Figure 1: Two families of edges in a word graphAlthough they are compact, the use of wordgraphs leads to problems by itself.
One of themis the current lack of a reasonable measure forword graph size and evaluation of their contents(Amtrup et al, 1997).
The problem we want toaddress in this paper is the presence of a largenumber of almost identical word hypotheses.
Byalmost identical we mean that the start and endvertices of edges differ only slightly.Consider figure 1 as an example section of aword graph.
There are several word hypothe-ses representing the words und (and) and dann(then).
The start and end points of them differby small numbers of frames, each of them 10mslong.
The reasons for the existence of these fam-ilies of edges are at least twofold:* Standard HMM-based word recognizers tryto start (and finish) word models at eachindividual frame.
Since the resolution isquite high (10ms, in many cases shorterthan the word onset), a word model mayhave boundaries at several points in time.?
Natural speech (and in particular spon-taneously produced speech) tends to blurword boundaries.
This effect is in part re-sponsible for the dramatic decrease in wordrecognition rate, given fluent speech as in-put in contrast to isolated words as in-put.
Figure 1 demonstrates the inaccuracy55of word boundaries by containing severalmeeting points between und and dann, em-phasized by the fact that both words endresp.
start with the same consonant.Thus, for most words, there is a whole setof word hypotheses in a word graph which re-sults in several meets between two sets of hy-potheses.
Both facts are disadvantageous forspeech processing: Many word edges result ina high number of lexical lookups and basic op-erations (e.g.
bottom-up roposals of syntacticcategories); many meeting points between edgesresult in a high number of possibly complex op-erations (like unifications in a parser).The most obvious way to reduce the numberof neighboring, identically labeled edges is toreduce the time resolution provided by a wordrecognizer (Weber, 1992).
If a word edge isto be processed, the start and end vertices aremapped to the more coarse grained points intime used by linguistic modules and a redun-dancy check is carried out in order to preventmultiple copies of edges.
This can be easilydone, but one has to face the drawback on in-troducing many more paths through the graphdue to artificially constructed overlaps.
Fur-thermore, it is not simple to choose a correctresolution, as the intervals effectively appearingwith word onsets and offsets change consider-ably with words spoken.
Also, the introductionof cycles has to be avoided.A more sophisticated schema would use in-terval graphs to encode word graphs.
Edges ofinterval graphs do not have individual start andend vertices, but instead use intervals to denotethe range of applicability of an edge.
The majorproblem with interval graphs lies with the com-plexity of edge access methods.
However, manyformal statements shown below will use intervalarithmetics, as the argument will be easier tofollow.The approach we take in this paper is to usehypergraphs a  representation medium for wordgraphs.
What one wants is to carry out oper-ations only once and record the fact that thereare several start and end points of words.
Hy-pergraphs (Gondran and Minoux, 1984, p. 30)are generalizations of ordinary graphs that al-low multiple start and end vertices of edges.We extend the approach of H. Weber (Weber,1995) for time mapping.
Weber considered setsof edges with identical start vertices but slightlydifferent end vertices, for which the notion fam-ily was introduced.
We use full hypergraphs asrepresentation a d thus additionally allow sev-eral start vertices, which results in a further de-crease of 6% in terms of resulting chart edgeswhile parsing (cf.
section 3).
Figure 2 showsthe example section using hyperedges for thetwo families of edges.
We adopt the way ofdealing with different acoustical scores of wordhypotheses from Weber.
(then)lOmsFigure 2: Two hyperedges representing familiesof edges2 Word  Graphs  and  HypergraphsAs described in the introduction, word graphsconsist of edges representing word hypothesesgenerated by a word recognizer.
The start andend point of edges usually denote points in time.Formally, a word graph is a directed, acyclic,weighted, labeled graph with distinct root andend vertices.
It is a quadruple G = (V, g, YV,/:)with the following components:?
A nonempty set of graph vertices Y --{vl , .
.
.
,Vn}.
To associate vertices withpoints in time, we use a function t : 1) >N that returns the frame number for agiven vertex.?
A nonempty set of weighted, labeled, di-rected edges g = {e l , .
.
.
,em} C_ V x ~2 x14) ?
E. To access the components of anedge e = (v, v', w, l), we use functions a,~3, w and l, which return the start vertex(~(e) = v), the end vertex (/~(e) = v'), theweight (w(e) = w) and the label (l(e) = l)of an edge, respectively.?
A nonempty set of edge weights ~ --{wi , .
.
.
,wp}.
Edge weights normally rep-resent a the acoustic score assigned to theword hypothesis by a HMM based wordrecognizer.56?
A nonempty set of Labels ?
= {t l , .
.
.
,lo},which represents information attached toan edge, usually words.We define the relation of teachability for ver-tices (--r) as Vv, w E V : v --+ w ~ 3e E $ := v ^ = wThe transitive hull of the reachability relation---r is denoted by -~.We already stated that a word graph is acyclicand distinctly rooted and ended.2.1 I - IypergraphsHypergraphs differ from graphs by allowing sev-eral start and end vertices for a single edge.
Inorder to apply this property to word graphs, thedefinition of edges has to be changed.
The set ofedges C becomes a nonempty set of weighted, la-beled, directed hyperedges $ = {e l , .
.
.
,em} C_V*\O x V*\O x W x ?.Several notions and functions defined for ordi-nary word graphs have to be adapted to reflectedges having sets of start and end vertices.?
The accessor functions for start and endvertices have to be adapted to return sets ofvertices.
Consider an edge e = (V, V', w,/) ,then we redefinea:c  > := v (1)E > := v '  (2)?
Two hyperedges e,e' are adjacent, if theyshare a common vertex:fl(e) A a(e') # ~ (3)?
The reachability relation is now Vv, w E )2 :v-+ w ~ 9e e $ : v e a(e) ^ w  e ~(e)Additionally, we define accessor functions forthe first and last start and end vertex of anedge.
We recur to the association of verticeswith frame numbers, which is a slight simplifi-cation (in general, there is no need for a totalordering on the vertices in a word graph) 1.
Fur-thermore, the intervals covered by start and endvertices are defined.a<(e) := argmin{t(v)lv E V} (4)1The total ordering on vertices is naturally giventhrough the linearity of speech.a>Ce) := argmax{t(v)lv e V} (5)/3<(e) := argmin{t(v)lv e V'} (6)/3>(e) := argmax{t(v)lv E V'} (7)au(e ) := \[t(a<(e)),t(a>(e))\] (8)~D(e) := \[t(~<(e)),t(~>(e))\] (9)In contrast o interval graphs, we do not re-quire the sets of start and end vertices to be con-tiguous, i.e.
there may be vertices that fall inthe range of the start or end vertices of an edgewhich are not members of that set.
If we are notinterested in the individual members of a(e) or~(e), we merely talk about interval graphs.2.2 Edge  Cons is tencyJust like word graphs, we demand that hyper-graphs are acyclic, i.e.
Vv -5, w : v # w.In terms of edges, this corresponds to Ve :t (a>Ce) )  <2.3 Add ing  Edges  to HypergraphsAdding a simple word edge to a hypergraph is asimplification of merging two hyperedges bear-ing the same label into a new hyperedge.
There-fore we are going to explain the more generalcase for hyperedge merging first.
We analyzewhich edges of a hypergraph may be merged toform a new hyperedge without loss of linguisticinformation.
This process has to follow threemain principles:?
Edge labels have to be identical?
Edge weights (scores) have to be combinedto a single value?
Edges have to be compatible in their startand end vertices and must not introducecycles to the resulting graphS imple  Ru le  Set for Edge  Merg ingLet ei, e2 E E be two hyperedges to be checkedfor merging, where el = (V1, VI', wt, 11) and e2 =(V2, V~, w2,/2).
Then el and e2 could be mergedinto a new hyperedge 3 = (V3, V~, w3,/3) ifft (e l )  = t(e2) (10)min(t(/3< (el)), t(/3< (e2))) >max(t(a> (el)), t (a> (e2))) (11)where e3 is:13 = ll (=/2) (12)w3 = scorejoin(wi,w2)  (13)57V3 = VI UV2 (14)= Vl' u (15)el and e2 have to be removed from the hyper-graph while e3 has to be inserted.Suf f ic iency of  the  Ru le -SetWhy is this set of two conditions ufficient forhyperedge merging?
First of all it is clear thatwe can merge only hyperedges with the samelabel (this is prescribed by condition 10).
Con-dition 11 gives advice which hyperedges couldbe combined and prohibits cycles to be intro-duced in the hypergraph.
An analysis of theoccuring cases shows that this condition is rea-sonable.
Without loss of generality, we assumethat t(~>(el)) ~ t(;3>(e2)).1. aO(el)n 13D(e2 ) # 0 V a0(e2 ) n ~0(el) # 0:This is the case where either the start ver-tices of el and the end vertices of e2 or thestart vertices of e2 and end vertices of eloverlap each other.
The merge of two suchhyperedges of this case would result in ahyperedge 3 where t(a>(e3)) > t(~< (e3)).This could introduce cycles to the hyper-graph.
So this case is excluded by condi-tion 11.2. aD(el ) n ~B(e2) = O A a\[l(e2 ) ?3 ;3\[\](el) = O:This is the complementary case to 1.
(a) t(a<(e2)) >_ t(~>(el))This is the case where all vertices ofhyperedge l occur before all verticesof hyperedge 2 or in other words thecase where two individual independentword hypotheses with same label oc-cur in the word graph.
This case mustalso not result in an edge merge since~H(el) C_ \[t(a<(el)),t(a>(e2))\] in themerged edge.
This merge is prohib-ited by condition 11 since all verticesof ~(el) have to be smaller than allvertices of a(e2).
(b) t(a<(e2)) < t(~>(el))This is the complementary case to (a).i.
t (o~<(el ) )  ~ t (~>(e2) )This is only a theoretical case be-cause tC <(el)) < << _<2Examples for the scorejoin operation are given laterin the paragraph about score normalization.is required (e2 contains the last endvertex).ii.
t (a<(el))  < t(;3>(e2))This is the complementary case toi.
As a result of the empty in-tersections and the cases (b) andii we get t(c~>(el)) < t(~<(e2))and t(oz>(e2)) < t(~<(el)).
Thatis in other words Vta E a0(el ) U~n(e2),t~ e ;3U(el)u~D(e2) : t~ <t~ and just the case demanded bycondition 2.After analyzing all cases of merging of inter-sections between start and end vertices of twohyperedges we turn to insertion of word hy-potheses to a hypergraph.
Of course, a wordhypothesis could be seen as interval edge withtrivial intervals or as a hyperedge with only onestart and one end vertex.
Since this case ofadding an edge to a hypergraph is rather easyto depict and is heavily used while parsing wordgraphs incrementally we discuss it in more de-tail.e g e I) (~ + e 3C ) ( )( ) ( )~ e 5Figure 3: Cases for adding an edge to the graphThe speech decoder we use delivers word hy-potheses incrementally and ordered by the timestamps of their end vertices.
For practical rea-sons we further sort the start vertices with equalend vertex of a hypergraph by time.
Under thisprecondition we get the cases shown in figure 3.The situation is such that eg is a hyperedge al-ready constructed and el -e5  are candidates forinsertion.58funct ion  AddEdge(G:Hypergraph,en :Wordhypothesis)-~ HyperGraphbeg in\[1\] i f  3e~ E ?
(G) withl (ek)  = l (en)A  t (~<(elc) )  > t (a (en) )  then(2\] Modify edge e&e t := (aCek) u (a (e . )}
,~(ek) u (/~(e.)),max(t~(ek),normal ize(w(en ), a (en),  ~(en ))),l(e~))re turnC' := ( r  U {a(en),~(en)),e I\[3\] else\[4\] Add edge e' neln := ({c~(en)}, {~(en)} ,normalize(w(en ), Q(en ), ~(en )),l (en) )returnG' := (1) U {~(en) ,~(en)} ,E  U {e~},wu (w(e ' )} ,  L u ( t (e ' ) ) ),andFigure 4: An algorithm for adding edges to hy-pergraphsof a matching hyperdege.
In practice this is notneeded and we could check a smaller amount ofvertices.
We do this by introducing a maximaltime gap which gives us advice how far (in mea-sures of time) we look backwards from the startvertex of a new edge to be inserted into the hy-pergraph to determine a compatible hyperedgeof the hypergraph.Additional PathsCCFigure 5: Additional paths by time mappingIt is not possible to add el and e2 to the hy-peredge g since they would introduce an over-lap between the sets of start and end verticesof the potential new hyperedge.
The resultinghyperedges ofadding e3 -- e5 are depicted below.Score Normal izat ionScore normalization is a necessary means ifone wants to compare hypotheses of differentlengths.
Thus, edges in word graphs are as-signed normalized scores that account for wordsof different extensions in time.
The usual mea-sure is the score per .frame, which is computedScore per wordby taking Length of word in frames"When combining several word edges as wedo by constructing hyperedges, the combina-tion should be assigned a single value that re-flects a certain useful aspect of the originatingedges.
In order not to exclude certain hypothe-ses from consideration i score-driven languageprocessing modules, the score of the hyperedgeis inherited from the best-rated word hypothe-sis (cf.
(Weber, 1995)).
We use the minimum ofthe source acoustic scores, which corresponds toa highest recognition probability.Introducing a Maximal  T ime GapThe algorithm depicted in figure 4 can bespeeded up for practical reasons.
Each vertexbetween the graph root and the start vertex ofthe new edge could be one of the start verticesIt is possible to introduce additional pathsinto a graph by performing time mapping.
Con-sider fig.
5 as an example.
Taken as a normalword graph, it contains two label sequences,namely a -c -d  and b-c-e.
However, if timemapping is performed for the edges labelled c,two additional sequences are introduced: a -c -eand b-c-d.
Thus, time mapping by hyper-graphs is not information preserving in a strongsense.
For practical applications this does notpresent any problems.
The situations in whichadditional abel sequences are introduced arequite rare, we did not observe any linguistic dif-ference in our experiments.2.4 Edge Combinat ionBesides merging the combination of hyperedges,to construct edges with new content is an impor-tant task within any speech processing module,e.g.
for parsing.
The assumption we will adopthere is that two hyperedges el, e2 E ?
may becombined if they are adjacent, i.e.
they share acommon vertex: /~(el) N ol(e2) ~ 0.
The labelof the new edge en (which may be used to rep-resent linguistic content) is determined by thecomponent building it, whereas the start andend vertices are determined byQc(en) c (el) (16)J (en) :=   (e2) (17)59This approach is quite analogous to edge com-bination methods for normal graphs, e.g.
inchart parsing, where two edges are equally re-quired to have a meeting point.
However, scorecomputation for hyperedge combination is moredifficult.
The goal is to determine a score perframe by selecting the smallest possible scoreunder all possible meeting vertices.
It is de-rived by examining all possible connecting ver-tices (all elements of I := f~(el) CI a(e2)) andcomputing the resulting score of the new edge:If w(el) < w(e2), we usew(e,~) := WC~l)(t< -t(~> (~1)))+~(~2).
(t(~> (e2))-t<)tC~>(e2))-t(~>(~l))where t< = min{t(v)lv e I}.
If, on the otherhand, w(el) > w(e2), we usew(el).(t>-t(~<(~l)))+~C~2).
(t(~<(~2))-t>) w(en) := t(~< (e:))-t(~< (~1)) 'where t> = max{t(v)\[v e I}.3 Experiments with HypergraphsThe method of converting word graphs to hy-pergraphs has been used in two experiments sofar.
One of them is devoted to the study ofconnectionist unification in speech applications(Weber, forthcoming).
The other one, fromwhich the performance figures in this sectionare drawn, is an experimental speech transla-tion system focusing on incremental operationand uniform representation (Amtrup, 1997).10000 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.# Edgesgraphs produced by the Hamburg speech recog-nition system (Huebener et al, 1996).
Thetest data consisted of one dialogue within theVerbmobil domain.
There were 41 turns withan average length of 4.65s speaking time perturn.
The word graphs contained 1828 edgeson the average.
Figure 6 shows the amount ofreduction in the number of edges by convertingthe graphs into hypergraphs.
On the average,1671 edges were removed (mapped), leaving 157edges in hypergraphs, approximately 91% lessthan the original word graphs.Next, we used both sets of graphs (the orig-inal word graphs and hypergraphs) as inputto the speech parser used in (Amtrup, 1997).This parser is an incremental ctive chart parserwhich uses a typed feature formalism to describelinguistic entities.
The grammar is focussed onpartial parsing and contains rules mainly fornoun phrases, prepositional phrases and such.The integration of complete utterances is ne-glected.
Figure 7 shows the reduction in termsof chart edges at completion time.Figure 6: Word edge reductionWe want to show the effect of hypergraphs re-garding edge reduction and parsing effort.
In or-der to provide real-world figures, we used wordlO000 -IO00--4 -  Maximum gaps100# EdgesFigure 7: Chart edge reductionThe amount of reduction concerning parsingeffort is much less impressive than pure edgereduction.
On the average, parsing of completegraphs resulted in 15547 chart edges, while pars-ing of hypergraphs produced 3316 chart edges,a reduction of about 79%.
Due to edge combi-nations, one could have expected a much highervalue.
The reason for this fact lies mainly withthe redundancy test used in the parser.
There60are many instances of edges which are not in-serted into the chart at all, because identicalhypotheses are already present.Consequently, the amount of reduction inparse time is within the same bounds.
Pars-ing ordinary graphs took 87.7s, parsing of hy-pergraphs 6.4s, a reduction of 93%.
There aresome extreme cases of word graphs, where hy-pergraph parsing was 94 times faster than wordgraph parsing.
One of the turns had to be ex-cluded from the test set, because it could notbe fully parsed as word graph.I0001001~ 1.617--II- With mapping0.10o.ol Io 4&o# EdgesFigure 8: Parsing time reduction4 Conc lus ionIn this paper, we have proposed the applicationof hypergraph techniques to word graph pars-ing.
Motivated by linguistic properties of spon-taneously spoken speech, we argued that bun-dles of edges in word graphs hould be treated inan integrated manner.
We introduced intervalgraphs and directed hypergraphs as representa-tion devices.
Directed hypergraphs extend thenotion of a family of edges in that they are ableto represent edges having several start and endvertices.We gave a formal definition of word graphsand the necessary extensions to cover hyper-graphs.
The conditions that have to be fulfilledin order to merge two hyperedges and to com-bine two adjacent hyperedges were stated in aformal way; an algorithm to integrate a wordhypothesis into a hypergraph was presented.We proved the applicability of our mecha-nisms by parsing one dialogue of real-world spo-ken utterances.
Using hypergraphs resulted in a91% reduction of initial edges in the graph anda 79% reduction in the total number of chartedges.
Parsing hypergraphs instead of ordinaryword graphs reduced the parsing time by 93%.ReferencesJan W. Amtrup, Henrik Heine, and Uwe Jost.1997.
What's in a Word Graph - -  Evaluationand Enhancement of Word Lattices.
In Proc.of Eurospeech 1997, Rhodes, Greece, Septem-ber.Jan W. Amtrup.
1997.
Layered Charts forSpeech Translation.
In Proceedings of theSeventh International Conference on Theo-retical and Methodological Issues in MachineTranslation, TMI '97, Santa Fe, NM, July.Xavier Aubert and Hermann Ney.
1995.
LargeVocabulary Continuous Speech RecognitionUsing Word Graphs.
In ICASSP 95.Michel Gondran and Michel Minoux.
1984.Graphs and algorithms.
Wiley-InterscienceSeries in Discrete Mathematics.
John Wiley& Sons, Chichester.Kai Huebener, Uwe Jost, and Henrik Heine.1996.
Speech Recognition for SpontaneouslySpoken German Dialogs.
In ICSLP96,Philadelphia.Martin Oerder and Hermann Ney.
1993.Word Graphs: An Efficient Interface Be-tween Continuous-Speech Recognition andLanguage Understanding.
In Proceedingsof the 1993 IEEE International Conferenceon Acoustics, Speech ~ Signal Processing,ICASSP, pages II/119-II/122, Minneapolis,MN.Hans Weber.
1992.
Chartparsing in ASL-Nord:Berichte zu den Arbeitspaketen P1 bis P9.Technical Report ASL-TR-28-92/UER, Uni-versit/it Erlangen-Niirnberg, Erlangen, De-cember.Hans Weber.
1995.
LR-inkrementelles, Prob-abilistisches Chartparsing yon Worthypothe-sengraphen mit Unifikationsgrammatiken:Eine Enge Kopplung yon Suche und Analyse.Ph.D.
thesis, Universit/it Hamburg.Volker Weber.
forthcoming.
Funktionales Kon-nektionistisches Unifikationsbasiertes Pars-ing.
Ph.D. thesis, Univ.
Hamburg.61
