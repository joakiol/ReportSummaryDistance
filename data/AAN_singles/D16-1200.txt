Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1936?1942,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsTimeline extraction using distant supervision and joint inferenceSavelie CornegrutaDepartment of Biomedical EngineeringKing?s College London, UKsavelie.cornegruta@kcl.ac.ukAndreas VlachosDepartment of Computer ScienceUniversity of Sheffield, UKa.vlachos@sheffield.ac.ukAbstractIn timeline extraction the goal is to order allthe events in which a target entity is involvedin a timeline.
Due to the lack of explic-itly annotated data, previous work is primar-ily rule-based and uses pre-trained temporallinking systems.
In this work, we propose adistantly supervised approach by heuristicallyaligning timelines with documents.
The noisytraining data created allows us to learn mod-els that anchor events to temporal expressionsand entities; during testing, the predictionsof these models are combined to produce thetimeline.
Furthermore, we show how to im-prove performance using joint inference.
Inexperiments in the SemEval-2015 TimeLinetask we show that our distantly supervisedapproach matches the state-of-the-art perfor-mance while joint inference further improveson it by 3.2 F-score points.1 IntroductionTemporal information extraction focuses on extract-ing relations and events along with the time whenthey were true or happened.
In this work we focuson timeline extraction, following the recent SemEvalTimeLine shared task (Minard et al, 2015).
The aimof the task is to extract timelines from multiple doc-uments consisting of events in which a given targetentity is the main participant.
An example timelinefor the entity Steve Jobs extracted from 4 documentsis given in Fig.1.The development data provided by the TimeLineshared task does not contain annotations for the var-ious intermediate processing stages needed, only aset of documents with annotated event mentions (in-put) and the timelines extracted for a few target en-tities (output).
No training data was provided, thusparticipating systems used rules combined with tem-poral linking systems trained on related tasks in or-der to anchor events to temporal expressions and en-tities to construct the timelines.We propose a new approach to timeline extractionthat uses the development data provided as distantsupervision to generate noisy training data (Cravenand Kumlien, 1999; Mintz et al, 2009).
Morespecifically, we heuristically align the target entityand the timestamps from the timelines with auto-matically recognized entities and temporal expres-sions in the documents.
This noisy labeled data setallows us to learn models for the subtasks of an-choring events to temporal expressions and to en-tities, without requiring training models on addi-tional data.
Also, we improve the performance us-ing joint inference for both anchoring subtasks.
Inour experiments, we show that our distantly super-vised approach matches the state-of-the-art perfor-mance while joint inference further improves on itby 3.2 F-score points.
Our code is publicly availableat http://github.com/savac/timeline.2 Timeline extractionThe task of timeline extraction given a target entityand a set of documents can be decomposed as fol-lows.
The initial stages are event mention extraction,target entity recognition, and temporal expressionidentification and resolution.
The next stages are an-choring event mentions to target entities and tempo-ral expressions.
The final stages are event corefer-1936Documents:DocId: 16844, DCT: 2010-06-08, Sentence: 2,3,4,5: Yesterday2010-06-07 , at this year ?s Apple Worldwide Developers Conference (WWDC ) , company CEO Steve JobsSteve Jobs unveiled iPhone 4iPhone 4 , along with the new iOS 4 operating system for Apple mobiledevices .
The announcement was long-awaited but not a very big surprise .
In April , the technology blog Gizmodo obtaineda prototype of the new phoneiPhone 4 and published details of itiPhone 4 online .
While introducing iPhone 4iPhone 4 , at the annualconference , JobsSteve Jobs started by hinting at the incident , saying , ?
Stop me if you ?ve already seen this .DocId: 17036, DCT: 2010-07-17, Sentence: 6,15: Rather than recall the devices or offer a hardware fix , JobsSteve Jobs saidyesterday2010-07-16 that Apple will offer a free case to anyone who has purchased an iPhone 4iPhone 4.
[...] However ,JobsSteve Jobsadmitted that the percentage of calls dropped on the iPhone 4iPhone 4 was slightly greater than the percentage of calls dropped onthe 3GSiPhone 3GS.DocId: 16900, DCT: 2010-06-16, Sentence: 6: The newest iPhoneiPhone 4 , iPhone 4iPhone 4 was introduced by Apple CEOSteve JobsSteve JobsSteve Jobs at the company ?s 2010 Worldwide Developer ?s Conference less than two weeks ago2010-06 .DocId 16983, 2010-10-23, Sentence 10: In hisSteve Jobs keynote address Wednesday2010-10-20 , JobsSteve Jobs announced the release ofApple ?s iLife ?11 software suite , which includes the iPhoto , iMovie , and GarageBand programs .Timeline: Steve Jobs1 2010-06-07 16844-2-unveiled1 2010-06-07 16844-5-introducing 16900-11-introduced1 2010-06-07 16844-5-hinting1 2010-06-07 16844-5-saying2 2010-07-16 17036-6-said2 2010-07-16 17036-15-admitted3 2010-10-20 16983-10-address3 2010-10-20 16983-10-announcedFigure 1: Example timeline for target entity Steve Jobs.
The input to the system is the documents annotated with event mentions an-notations and their Document Creation Time (DCT).
The event mentions appearing in the timeline are identified by their documentid-sentence index.
The annotations for the target entities and temporal expression mentions need to be done by the system.ence resolution and ordering of the events in a time-line, which rely largely on their anchoring to tem-poral expressions.
The TimeLine shared task hadtwo tracks, A and B, the only difference being thatin Track B the event mentions are provided in theinput.
We consider this track in this paper and fo-cus on learning the anchoring of events to temporalexpressions and entities.The development data provided in the context ofthe shared task consisted of documents related toApple and gold timelines for six target entities.
Eval-uation was performed by extracting timelines fromthree document sets, each related to Airbus, GM andStock market respectively.
We used the official eval-uation which is based on the metric introduced byUzZaman and Allen (2011) which assesses a pre-dicted timeline versus the gold standard one usingprecision, recall and F-score over binary temporalrelations between the events.3 Distant supervisionIn order to generate training data for anchoring eventmentions to target entities and temporal expressionsvia distant supervision, we first need to identifythem.
For entity recognition we use approximatestring matching combined with the Stanford Coref-erence Resolution System (Lee et al, 2013).
Fortemporal expression identification and resolution toabsolute timestamps we use the UWTime temporalparser (Lee et al, 2014).Next we generate labeled instances as follows.For anchoring events to entities, we consider foreach event mention the correct entity mention to bethe nearest mention of the target entity in the samesentence, and all others to be incorrect.
Similarly,for anchoring events to timestamps, we consider foreach event mention the correct temporal expressionto be the nearest temporal expression that exactlymatches the timestamp according to the timeline(but not necessarily in the same sentence), and allothers to be incorrect.
The datasets generated willbe noisy since correct anchors may be entity men-tions and temporal expressions that are not the near-est ones.
Further noise is expected due to errors inthe entity recognition and temporal expression iden-tification and resolution stages.1937Features typeMeasure distance in tokens between event andtarget entity mentionslocalSyntactic dependencies between event and targetentity mentions (extracted from training corpus)localCheck if subsequent events have the same stemand are attributed to the same target entityglobalCheck if subsequent events are in the same sen-tence and are attributed to the same target entityglobalCheck if subsequent events are both communica-tion events and are attributed to the same targetentityglobalTable 1: Features to encode dependencies between events andtarget entities4 Event anchoringAfter generating training data for anchoring eventmentions to target entities and to temporal expres-sions with distant supervision, we now proceed todeveloping linear models for each of these tasks.4.1 ClassificationUsing distant supervision we obtained examples ofcorrect and incorrect anchoring of event mentions toentities and temporal expressions.
Thus we learn foreach of the two tasks a binary linear classifier of theform:score(x, y,w) = w ?
?
(x, y) (1)where x is an event mention, y is the anchor (ei-ther the target entity or the temporal expression)and w are the parameters to be learned.
The fea-tures extracted by ?
represent various distance mea-sures and syntactic dependencies between the eventmention and the anchor obtained using StanfordCoreNLP (Manning et al, 2014).
The temporal ex-pression anchoring model also uses a few featuretemplates that depend on the timestamp of the tem-poral expression.
The full list of features extractedby ?
are denoted as local in Tables 1 and 2.4.2 AlignmentThe classification approach described is limited toanchoring each event mention to an entity or a tem-poral expression in isolation.
However it would bepreferable to infer the decisions for each task jointlyat the document level and take into account the de-pendencies in anchoring different events, e.g.
thatconsecutive events in text are likely to be anchoredFeatures typeMeasure distance in sentences between eventmention and temporal expressionlocalMeasure distance in tokens between event men-tion and temporal expressionlocalSyntactic dependencies between event mentionand temporal expression (extracted from trainingcorpus)localCheck if temporal expression is before of afterthe event mentionlocalCheck if timestamp is in the future wrt the DCT localCheck if timestamp is undefined (i.e.
XX-XX-XXXX)localCheck if timestamp is incomplete localCheck if subsequent events and are linked to thesame temporal expressionglobalCheck if subsequent events have the same stemand are linked to the same temporal expressionglobalCheck if subsequent events are in the same sen-tence and are linked to the same temporal expres-sionglobalCheck if subsequent events are communicationevents and are linked to the same temporal ex-pressionglobalTable 2: Features to encode dependencies between events andtemporal expressionsto the same entity, as shown in Figure 2, or to thesame temporal expression.
Capturing such depen-dencies can be crucial when the correct anchor is notexplicitly signalled in the text but can be inferredconsidering other relations and/or their ordering intext (Derczynski, 2013).Defining our joint model formally, let x be a vec-tor containing all event mentions in a document andy be the vector of all anchors (target entity mentionsor temporal expressions) in the same document.
Theorder of the events in x is as they appear in the doc-ument.
Let z be a vector of the same length as x thatdefines the alignment between x and y by containingpointers to elements in y, thus allowing for multipleevents to share the same anchor.
The scoring func-tion is defined asscore(x,y, z,w) = w ?
?
(x,y, z) (2)where the global feature function ?, in addition tothe features returned by the local scoring function(Eq.
1), also returns features taking into accountanchoring predictions across the document.
Apartfrom features encoding subsequences of anchoring1938Documents:DocId: 17036, DCT: 2010-07-17, Sentence: 9,10,11:JobsSteve Jobs also said that those who had already purchaseda bumper will receive a full refund for the accessory.
Forconsumers still dissatisfied with iPhone 4iPhone 4, JobsSteve Jobssaid that the phones can be returned for a refund as well.JobsSteve Jobs acknowledged that ?a very small percentage ofusers?
were experiencing antenna issues, but dismissed theexistence of an ?Antennagate,?
saying that similar problemsplague all cellular phones and that the iPhoneiPhone 4 issue?has been blown so out of proportion that it is incredible.
?Figure 2: The correct alignment of events and target entitymentions is shown with the numbers in brackets denoting the in-dex of the sentence in which the mention is found.
The consec-utive events acknowledged, dismissed and saying are anchoredto entity Steve Jobs that was only mentioned once in the begin-ning of the sentence.predictions, it also makes possible to make them de-pendent on the events, e.g.
a binary indicator encod-ing whether two consecutive events with the samestem share the same anchor or not.
The full listof local and global features extracted by ?
are pre-sented in Tables 1 and 2.
Predicting with the scoringfunction in Eq.2 amounts to finding the anchoringsequence vector z that maximizes it.
To be able toperform exact inference efficiently, we impose a firstorder Markov assumption and use the Viterbi algo-rithm (Viterbi, 1967).
Similar approaches have beensuccessful in word alignment for machine transla-tion (Blunsom and Cohn, 2006).4.3 Post-processingDuring testing, we need to construct the timeline foreach target entity using the events that were pre-dicted to be anchored to it and the timestamps ofthe temporal expressions each event was anchoredto.
Thus, we need to perform two additional tasks,event coreference and ordering.
For the former wedefine a simple heuristic by which if two mentionshave the same stems and timestamps then they re-fer to the same event.
The only exception is that iftwo mentions represent communication events (said,announced etc.
), then they are resolved to differentevents when in the same document.
We finally orderthe events according to their timestamp.5 ResultsWe evaluate our system using the setup provided bythe TimeLine task ensuring that the training and val-idation are performed only using the developmentdata i.e.
the Apple collection.
All linear models weretrained with the perceptron update rule (Pedregosa etal., 2011).
We tuned the number of perceptron iter-ations by performing cross-validation using the de-velopment data by holding out the timeline for onetarget entity and training on the timelines for the re-maining ones.In Table 3 we compare the binary classificationmodel (Our System Binary) against the alignmentmodel (Our System Alignment) and show that thelatter outperforms the former by a margin of 3.2points in F-score, achieving a micro F1-score of28.58 across the three test corpora, thus confirmingthe benefits of joint inference.
The only corpus inwhich joint inference did not help was Stock whichhas on average shorter event chains per document(Minard et al, 2015) and thus renders joint anchor-ing less likely to be useful.We now compare our approach to the two par-ticipants in the TimeLine shared task with tworuns each.
The best-performing GPLSIUA team(Navarro and Saquete, 2015) used the TIPSem tooldeveloped by Llorens et al (2010) for temporal rela-tion processing which extracts events and temporalexpressions and uses a Conditional Random Fieldmodel to anchor them against each other.
How-ever, TIPSem only considers anchoring of eventsto temporal expressions that are in the same sen-tence.
GPLSIUA also used the semantic role labelerfrom SENNA (Collobert et al, 2011) and Open-NER and anchored entities to events using a rule-based approach.
The HeidelToul team (Moulahi etal., 2015) used HeidelTime (Stro?tgen et al, 2013)to identify and resolve temporal expressions and de-1939Airbus GM Stock TotalSystem F1 F1 F1 P R F1GPLSIUA 1 22.35 19.28 33.59 21.73 30.46 25.36GPLSIUA 2 20.47 16.17 29.90 20.08 26.00 22.66HeidelToul 1 19.62 7.25 20.37 20.11 14.76 17.03HeidelToul 2 16.50 10.82 25.89 13.58 28.23 18.34Our System Binary 17.99 20.97 34.95 25.97 24.79 25.37Our System Alignment 25.65 26.64 32.35 29.05 28.12 28.58Table 3: Results for our system and other participants in the SemEval 2015 Task 4: TimeLine.veloped a target entity mention identification toolsimilar to ours using Stanford CoreNLP (Manninget al, 2014).
However, they rely on a rule-basedapproach for event anchoring.
Our binary modelmatches the performance of the best system, and ouralignment model exceeds it by 3.2 F1-score pointsacross, even though we do not use any off-the-shelfcomponents developed for temporal relation extrac-tion.
Instead we rely on training data generated withdistant supervision, and UWTime for temporal ex-pression identification and resolution, for which theparticipants also used similar components.6 Related workIn recent work, Laparra et al (2015) also consid-ered anchoring at the document-level in the contextof the Track A of the TimeLine shared task, howeverthey developed a rule-based approach.
The structurefeatures used in our joint inference approach encodesimilar intuitions, but we are learning model weightsusing distant supervision so that we can combinethem more flexibly.
And even though the noise in thetrainng data generated with distant supervision is aconcern, manual annotation of temporal relations isknown to have low inter-annotator agreement rates1and thus also likely to be noisy.Prior to the TimeLine shared task, TempEval(Verhagen et al, 2007) was the original task that fo-cused on categorising the relations between events,temporal expressions and Document Creation Timeusing the the TimeML annotation language.
Thetask classified only the relations between mentionsin the same or consecutive sentences.
The two fol-lowing tasks, TempEval-2 (Verhagen et al, 2010)and TempEval-3 (UzZaman et al, 2013), addedtasks for event and temporal expression identifica-1http://www.timeml.org/timebank/documentation-1.2.htmltion as well as an end-to-end temporal relation pro-cessing task that was performed on raw text.Beyond TempEval, McClosky and Manning(2012) used distant supervision in order to learn howto extract the temporal bounds for events in the con-text of the TAC temporal knowledge base populationtask (Ji et al, 2011).
However they focus on learn-ing real-world event ordering constraints (e.g.
peo-ple go to school before university) instead of howevents are reported in text.7 ConclusionsIn this paper we proposed a timeline extraction ap-proach in which we generate noisy training data foranchoring events to entities and temporal expres-sions using distant supervision.
By learning a binaryclassifier we match the state-of-the-art F1-score forthe Track B of the TimeLine shared task.
We furtherimprove this result by 3.2 F1-score points using jointinference.AcknowledgmentsPart of this work was conducted while both authorswere at University College London.
The authorswould like to thank Leon Derczynski for his feed-back on an earlier version.
Andreas Vlachos is sup-ported by the EU H2020 SUMMA project (grantagreement number 688139).ReferencesPhil Blunsom and Trevor Cohn.
2006.
Discriminativeword alignment with conditional random fields.
InProceedings of the 21st International Conference onComputational Linguistics and the 44th annual meet-ing of the Association for Computational Linguistics,pages 65?72.
Association for Computational Linguis-tics.1940Ronan Collobert, Jason Weston, Le?on Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011.Natural language processing (almost) from scratch.
J.Mach.
Learn.
Res., 12:2493?2537, November.Mark Craven and Johan Kumlien.
1999.
Constructingbiological knowledge bases by extracting informationfrom text sources.
In Proceedings of the Seventh Inter-national Conference on Intelligent Systems for Molec-ular Biology, pages 77?86.
AAAI Press.Leon Derczynski.
2013.
Determining the Types of Tem-poral Relations in Discourse.
Ph.D. thesis, Universityof Sheffield.Heng Ji, Ralph Grishman, and Hoa Trang Dang.
2011.Overview of the tac 2011 knowledge base populationtrack.
In Proceedings of Text Analysis Conference(TAC).Egoitz Laparra, Itziar Aldabe, and German Rigau.
2015.Document level time-anchoring for timeline extrac-tion.
In ACL.Heeyoung Lee, Angel Chang, Yves Peirsman, NathanaelChambers, Mihai Surdeanu, and Dan Jurafsky.
2013.Deterministic coreference resolution based on entity-centric, precision-ranked rules.
Comput.
Linguist.,39(4):885?916, December.Kenton Lee, Yoav Artzi, Jesse Dodge, and Luke Zettle-moyer.
2014.
Context-dependent semantic parsing fortime expressions.
In Proceedings of the 52nd AnnualMeeting of the Association for Computational Linguis-tics (Volume 1: Long Papers), pages 1437?1447, Balti-more, Maryland, June.
Association for ComputationalLinguistics.Hector Llorens, Estela Saquete, and Borja Navarro.2010.
Tipsem (english and spanish): Evaluating crfsand semantic roles in tempeval-2.
In Proceedings ofthe 5th International Workshop on Semantic Evalua-tion, pages 284?291.
Association for ComputationalLinguistics.Christopher D. Manning, Mihai Surdeanu, John Bauer,Jenny Finkel, Steven J. Bethard, and David McClosky.2014.
The Stanford CoreNLP natural language pro-cessing toolkit.
In Association for Computational Lin-guistics (ACL) System Demonstrations, pages 55?60.David McClosky and Christopher D Manning.
2012.Learning constraints for consistent timeline extrac-tion.
In Proceedings of the 2012 Joint Conferenceon Empirical Methods in Natural Language Process-ing and Computational Natural Language Learning,pages 873?882.
Association for Computational Lin-guistics.Anne-Lyse Minard, Manuela Speranza, Eneko Agirre,Itziar Aldabe, Marieke van Erp, Bernardo Magnini,German Rigau, and Ruben Urizar.
2015.
Semeval-2015 task 4: Timeline: Cross-document event order-ing.
In Proceedings of the 9th International Workshopon Semantic Evaluation (SemEval 2015), pages 778?786, Denver, Colorado, June.
Association for Compu-tational Linguistics.Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.2009.
Distant supervision for relation extraction with-out labeled data.
In Proceedings of the Joint Confer-ence of the 47th Annual Meeting of the ACL and the 4thInternational Joint Conference on Natural LanguageProcessing of the AFNLP: Volume 2 - Volume 2, ACL?09, pages 1003?1011, Stroudsburg, PA, USA.
Asso-ciation for Computational Linguistics.Bilel Moulahi, Jannik Stro?tgen, Michael Gertz, andLynda Tamine.
2015.
Heideltoul: A baseline ap-proach for cross-document event ordering.
In Pro-ceedings of the 9th International Workshop on Seman-tic Evaluation (SemEval 2015), pages 825?829, Den-ver, Colorado, June.
Association for ComputationalLinguistics.Borja Navarro and Estela Saquete.
2015.
Gplsiua: Com-bining temporal information and topic modeling forcross-document event ordering.
In Proceedings of the9th International Workshop on Semantic Evaluation(SemEval 2015), pages 820?824, Denver, Colorado,June.
Association for Computational Linguistics.F.
Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,B.
Thirion, O. Grisel, M. Blondel, P. Prettenhofer,R.
Weiss, V. Dubourg, J. Vanderplas, A. Passos,D.
Cournapeau, M. Brucher, M. Perrot, and E. Duches-nay.
2011.
Scikit-learn: Machine learning in Python.Journal of Machine Learning Research, 12:2825?2830.Jannik Stro?tgen, Julian Zell, and Michael Gertz.
2013.Heideltime: Tuning english and developing spanishresources for tempeval-3.
In Second Joint Conferenceon Lexical and Computational Semantics (*SEM), Vol-ume 2: Proceedings of the Seventh International Work-shop on Semantic Evaluation (SemEval 2013), pages15?19, Atlanta, Georgia, USA, June.
Association forComputational Linguistics.Naushad UzZaman and James F. Allen.
2011.
Temporalevaluation.
In Proceedings of the 49th Annual Meetingof the Association for Computational Linguistics: Hu-man Language Technologies: Short Papers - Volume2, HLT ?11, pages 351?356, Stroudsburg, PA, USA.Association for Computational Linguistics.Naushad UzZaman, Hector Llorens, Leon Derczynski,James Allen, Marc Verhagen, and James Pustejovsky.2013.
Semeval-2013 task 1: Tempeval-3: Evaluat-ing time expressions, events, and temporal relations.In Second Joint Conference on Lexical and Compu-tational Semantics (*SEM), Volume 2: Proceedings ofthe Seventh International Workshop on Semantic Eval-uation (SemEval 2013), pages 1?9, Atlanta, Georgia,1941USA, June.
Association for Computational Linguis-tics.Marc Verhagen, Robert Gaizauskas, Frank Schilder,Mark Hepple, Graham Katz, and James Pustejovsky.2007.
Semeval-2007 task 15: Tempeval temporal re-lation identification.
In Proceedings of the 4th Interna-tional Workshop on Semantic Evaluations, pages 75?80.
Association for Computational Linguistics.Marc Verhagen, Roser Sauri, Tommaso Caselli, andJames Pustejovsky.
2010.
Semeval-2010 task 13:Tempeval-2.
In Proceedings of the 5th internationalworkshop on semantic evaluation, pages 57?62.
Asso-ciation for Computational Linguistics.Andrew J. Viterbi.
1967.
Error bounds for convolutionalcodes and an asymptotically optimum decoding algo-rithm.
IEEE Transactions on Information Theory, IT-13(2):260?269, April.1942
