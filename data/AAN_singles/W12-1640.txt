Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 290?294,Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational LinguisticsImproving Sentence Completion in Dialogues with Multi-Modal FeaturesAnruo Wang, Barbara Di Eugenio, Lin ChenDepartment of Computer ScienceUniversity of Illinois at Chicago851 S Morgan ST, Chicago, IL 60607, USAawang28, bdieugen, lchen43@uic.eduAbstractWith the aim of investigating how humans un-derstand each other through language and ges-tures, this paper focuses on how people un-derstand incomplete sentences.
We trained asystem based on interrupted but resumed sen-tences, in order to find plausible completionsfor incomplete sentences.
Our promising re-sults are based on multi-modal features.1 IntroductionOur project, called RoboHelper, focuses on devel-oping an interface for elderly people to effectivelycommunicate with robotic assistants that can helpthem perform Activities of Daily Living (ADLs)(Krapp, 2002), so that they can safely remain livingin their home (Di Eugenio et al, 2010; Chen et al,2011).
We are developing a multi-modal interfacesince people communicate with each other using avariety of verbal and non-verbal signals, includinghaptics, i.e., force exchange (as when one personhands a bowl to another person, and lets go onlywhen s/he senses that the other is holding it).
Wecollected a medium size multi-modal human-humandialogue corpus, then processed and analyzed it.
Weobserved that a fair number of sentences are incom-plete, namely, the speaker does not finish the utter-ance.
Because of that, we developed a core compo-nent of our multi-modal interface, a sentence com-pletion system, trained on the set of interrupted buteventually completed sentences from our corpus.
Inthis paper, we will present the component of the sys-tem that predicts reasonable completion structuresfor an incomplete sentence.Sentence completion has been addressed withininformation retrieval, to satisfy user?s informationneeds (Grabski and Scheffer, 2004).
Completingsentences in human-human dialogue is more diffi-cult than in written text.
First, utterances may be in-formal, ungrammatical or dis-fluent; second, peopleinterrupt each other during conversations (DeVaultet al, 2010; Yang et al, 2011).
Additionally, theinteraction is complex, as people spontaneously usehand gestures, body language and gaze besides spo-ken language.
As noticed by (Bolden, 2003), duringface-to-face interaction, the completion problem isnot only an exclusively verbal phenomenon but ?anaction embedded within a complex web of differ-ent meaning-making fields?.
Accordingly, amongour features, we will include pointing gestures, andhaptic-ostensive (H-O) actions, e.g., referring to anobject by manipulating it in the real world (Landra-gin et al, 2002; Foster et al, 2008).The paper is organized as follows.
In Section 2 wedescribe our data collection and multi-modal anno-tation.
In Section 3 we discuss how we generate ourtraining data, and in Section 4 the model we trainfor sentence completion, and the results we obtain.2 DatasetIn contrast with other sentence completion systemsthat focus on text input, the dataset we use in thispaper is a subset of the ELDERLY-AT-HOME cor-pus, a multi-modal corpus in the domain of elderlycare, which includes collaborative human-human di-alogues, pointing gestures and haptic-ostensive (H-O) actions.
Our experiments were conducted ina fully functional apartment and included a helper290(HEL) and an elderly person (ELD).
HEL helpsELD to complete several realistic tasks, such asputting on shoes, finding a pot, cooking pasta andsetting the table for dinner.
We used 7 web camerasto videotape the whole experiment, one microphoneeach to record the audio and one data glove each tocollect haptics data.
We ran 20 realistic experimentsin total, and then imported the videos and audios (inavi format), haptics data (in csv format) and tran-scribed utterances (in xml format) into Anvil (Kipp,2001) to build the multi-modal corpus.Among other annotations (for example DialogueActs) we have annotated these dialogues for Point-ing gestures and H-O actions.
Due to the settingof our experiments, the targets of pointing gesturesand H-O actions are real life objects, thus we de-signed a reference index system to annotate them.We give pre-defined indices to targets which can-not be moved, such as cabinets, draws, and fridge.We also assign runtime indices to targets which canbe moved, like pots, glasses, and plates.
For exam-ple, ?Glass1?
refers to the first glass that appears inone experiment.
In our annotation, a ?Pointing?
ges-ture is defined as a hand gesture without any phys-ical contact between human and objects.
Handgestures with physical contact to objects are anno-tated as H-O actions.
H-O actions are further subdi-vided into 7 subtypes, including ?Holding?, ?Touch-ing?,?Open?
and ?Close?.
In order to verify the reli-ability of our annotations, we double coded 15% ofthe pointing gestures and H-O actions.
Kappa val-ues of 0.751 for pointing gestures, and of 0.703 forH-O actions, are considered acceptable, especiallyconsidering the complexity of these real life tasks(Chen and Di Eugenio, 2012).In this paper, we focus on specific sub-dialoguesin the corpus, which we call interruptions.
An inter-ruption can occur at any point in human-human dia-logues: it happens when presumably the interrupter(ITR) thinks s/he has already understood what thespeaker (SPK) means before listening to the entiresentence.
By observing the data from our corpus,we conclude that there are generally three cases ofinterruptions.
First, the speaker (SPK) stops speak-ing and does not complete the sentence ?
these arethe incomplete sentences whose completion a robotwould need to infer.
In the second type of inter-ruption, after being interrupted SPK continues with(a) few words, and then stops without finishing thewhole sentence: hence, there is a short time over-lap between two sentences (7 cases).
The third caseoccurs when the SPK ignores the ITR and finishesthe entire sentence.
In this case, the SPK and theITR speak simultaneously (198 cases).
The numberof interruptions ranges from 1 to 37 in each experi-ment.
An excerpt from an interruption with a subse-quent completion (an example of case 3) is shownbelow.
The interruption occurs at the start of theoverlap between the two speakers, marked by < and>.
This example also includes annotations for point-ing gestures and for H-O actions.Elder: I need some glasses from < that cabinet >.
[Point (Elder, Cabinet1)]Helper: < From this > cabinet?
[Point (Helper, Cabinet2)]Helper: Is this the glass you < ?re looking for?
>[Touching (Helper, Glass1)]Elder: < No, that one.>[Point (Elder, Cabinet1, Glass2)]As concerns annotation for interruptions, it proceedsfrom identifying interrupted sentences to finding<interrupted sentences, candidate structure> pairswhich will be used for generating grammatical com-pletion for an incomplete sentence.
Each in-terrupted sentence is marked with two categories:incomplete form, from the start of the sentenceto where it is interrupted, such as ?I need someglasses?
; complete form, from the start of a sentenceto where the speaker stops, ?I need some glassesfrom that cabinet.
?Table 2 shows distribution statistics for ourELDERLY-AT-HOME corpus.
It contains a total of4839 sentences, which in turn contain 7219 clauses.320 sentences are incomplete in the sense of case 1(after interruption SPK never completes his/her sen-tence); whereas 205 sentences are completed afterinterruption (cases 2 and 3).Sentences 4,839Clauses 7,219Pointing Gestures 362H-O Actions 629Incomplete sentences 320Interrupted sentences 205Table 1: Corpus Distributions2913 Candidate Pairs GenerationThe question is now, how to generate plausible train-ing instances to predict completions for incompletesentences.
We use the 205 sentences that havebeen interrupted but for which we have comple-tions; however, we cannot only use those pairs fortraining, since we would run the risk of overfit-ting, and not being able to infer appropriate com-pletions for other sentences.
To generate addi-tional<Interrupted sentences, candidate structure>pairs, we need to match an interrupted sentence IntSwith its potential completions ?
basically, to checkwhether IntS can match the prefix of other sentencesin the corpus.
We do so by comparing the POS se-quence and parse tree of IntS with the POS sequenceand parse tree of the prefix of another sentence.
BothIntS and other sentences in the corpus are parsed viathe Stanford Parser (Klein and Manning, 2003).Before discussing the details though, we needto deal with one potential problem: the POS se-quence for the incomplete portion of IntS may notbe correctly assigned.
For example, when the sen-tence ?The/DT, top/JJ, cabinet/NN.?
is interrupted as?The/DT, top/NN?, the POS tag of NN is assignedto ?top?
; this is incorrect, and engenders noise forfinding correct completions.We first pre-process a dialogue by splitting turnsinto sentences, tokenizing sentences into tokens, andPOS tagging tokens.
Although for the interruptedsentences, we could obtain a correct POS tag se-quence by parsing the incomplete and resumed por-tions together, this would not work for a truly incom-plete sentence (whose completion is our goal).
Thus,to treat both interrupted sentences and incompletesentences in the same way, we train a POS tag Cor-rection Model to correct fallaciously assigned POStags.
The POS tag Correction Model?s feature setincludes the POS tag of the token, the word, and theprevious tokens?
POS tags in a window size of 3.The model outputs the corrected POS tags.The POS tag Correction model described abovewas implemented using the Weka package (Hall etal., 2009).
Specifically, we experimented with J48(a decision tree implementation), Naive Bayes (NB),and LibSVM (a Support Vector Machine implemen-tation).
All the results reported below are calculatedusing 10 fold cross-validation.J48 NB LibSVMAccuracy 0.829 0.680 0.532Table 2: POS tag Correction Model PerformanceThe results in Table 2 are not surprising, since de-tecting the POS tag of a known word is a simpletask.
Additionally, it is not surprising that J48 ismore accurate than NB, since NB is known to of-ten behave as a baseline method.
What is surprisingthough is the poor performance of SVMs, which aregenerally among the top performers for a broad va-riety of tasks.
We are investigating why this may bethe case.
At any rate, by applying the J48 model, weobtain more accurate POS tag assignments for inter-rupted sentences (and in our future application, forthe incomplete sentence we need to complete).Once we have corrected the POS assignments foreach interrupted sentence IntS, we retrieve poten-tial grammatical structures for IntS, by comparingIntS with the prefixes of all complete sentences inthe corpus via POS tags and parse trees.
Note thatdue to the complexity of building a parse tree cor-rection model in our corpus, we only build a modelto correct the POS tags, but ignore the possible in-correct parse trees of the incomplete portion of aninterrupted sentence.
The matching starts from thelast word in IntS back to the first word, with weightsassigned to each position in decreasing order.
Due tothe size of our corpus, it is not possible to find ex-actly matched POS tag sequences for every incom-plete sentence; thus, we also consider the parsed treestructures and mismatched POS tags between IntS?sand complete sentences by reducing weights accord-ing to the size of the matched phrases and distancesof mismatched POS tags.
After this, a matchingscore is calculated for each incomplete and candi-date structure pair.Due to the large number of candidate structures,only the top 150 candidate structures for each IntSare selected and manually annotated with threeclassifications: ?R?, when the candidate structureprovides a grammatically ?reasonable?
structure,which can be used as a template for completion;?U?, which means the candidate structure givesan ?ungrammatical?
structure, thus this candidatestructure cannot be used as template for completion;292?T?, the candidate structure is exactly the same aswhat the speaker was originally saying, as judgedbased on the video and audio records.
An exampleof an incomplete sentence with candidate structuresin each of the three categories is shown below.It/PRP, feels/VBZ | It/PRP, feels/VBZ, good/JJR[R] It/PRP, ?s/VBZ, fine/JJ, like/IN, this/DT][U] We/PRP, did/VBD, n?t/RB[T] It/PRP, is/VBZ, better/JJR10543 interrupted sentences and candidate pairsare generated.
5268 of those 10543 pairs(49.97%) were annotated as ?Reasonable?, 4727pairs (44.85%) were annotated as ?Unreasonable?,and 545 pairs (5.17%) were annotated as ?Same withoriginal sentence?.Incomplete Sentence and Structure pairs 10,543Reasonable structures (R) 5,268Unreasonable structures (U) 4,729Exactly same structures (T) 545Table 3: Distribution of completion classifications4 Results and DiscussionOn the basis of the annotation, we trained a ?Rea-sonable Structure Selection (RSS)?
model via su-pervised learning methods.
For each pair <IntS,Candidate>, the feature set includes word and POStag of the tokens of IntS and its candidate structuresentence.
Co-occurring pointing gestures and H-Oactions for both IntS and Candidate are also includedin the model.
Co-occurrence is defined as tempo-ral overlap between the gesture (pointing or H-O ac-tion) and the duration of the utterance.
For eachtraining instance, we include the following features:IntS: <words, POS tags>, <Pointing (Person / Ob-ject / Location)>, <H-O action (Person / Object /Location / Type)>;Candidate: <words/POS tags)>, <Pointing (Per-son / Object / Location)>, <H-O action (Person /Object / Location / Type)>;<Matching Score>;<Classification: R, U, or T>.We trained the RSS model also using the Wekapackage.
The same methods mentioned earlier(J48, NB and SVM) are used, with 10-fold cross-validations.
Results are shown in Table 4.
WeJ48 NB LibSVMPrecision R, U, T 0.822 0.724 0.567R, U 0.843 0.761 0.600Recall R, U, T 0.820 0.725 0.512R, U 0.842 0.762 0.563F-Measure R, U, T 0.818 0.711 0.390R, U 0.841 0.761 0.440Table 4: Reasonable Structure Selection modelsran two different sets of experiments using two ver-sions of training instances: Classification with threeclasses, R, U and T, and classification with twoclasses, R and U.
When training with only twoclasses, the T instances are marked as R. We exper-imented with collapsing R and T candidates since Tcandidates may lead to overfitting, and some R can-didates might even provide better structures for anincomplete sentence than what exactly one speakerhad originally said.
Not surprisingly, results im-prove for two-way classification.
Based on the J48model, we observed that the POS tag features playa significant part in classification, whereas the wordfeatures are redundant.
Further, pointing gesturesand H-O actions do appear in some subtrees of thelarger decision tree, but not on every branch.
Wespeculate that this is due to the fact that pointing ges-tures or H-O actions do not accompany every utter-ance.5 Conclusions and Future WorkIn this paper, we introduced our multi-modal sen-tence completion schema which includes pointinggestures and H-O actions in the corpus ELDERLY-AT-HOME.
Our data shows that it is possible to pre-dict what people will say, even if the utterance isnot complete.
Our promising results include multi-modal features, which as we have shown elsewhere(Chen and Di Eugenio, 2012) improve traditionalco-reference resolution models.
In the near future,we will implement the last module of our sentencecompletion system, the one that fills the chosen can-didate structure with actual words.293ReferencesG.B.
Bolden.
2003.
Multiple modalities in collaborativeturn sequences.
Gesture, 3(2):187?212.L.
Chen and B.
Di Eugenio.
2012.
Co-reference viapointing and haptics in multi-modal dialogues.
In The2012 Conference of the North American Chapter ofthe Association for Computational Linguistics: Hu-man Language Technologies.
Association for Compu-tational Linguistics.
short paper, to appear.L.
Chen, A. Wang, and B.
Di Eugenio.
2011.
Im-proving pronominal and deictic co-reference resolu-tion with multi-modal features.
In Proceedings of theSIGDIAL 2011 Conference, pages 307?311.
Associa-tion for Computational Linguistics.David DeVault, Kenji Sagae, and David Traum.
2010.Incremental interpretation and prediction of utterancemeaning for interactive dialogue.
Dialogue and Dis-course, 2(1):143170.B.
Di Eugenio, M. Zefran, J. Ben-Arie, M. Foreman,L.
Chen, S. Franzini, S. Jagadeesan, M. Javaid, andK.
Ma.
2010.
Towards effective communication withrobotic assistants for the elderly: Integrating speech,vision and haptics.
In 2010 AAAI Fall Symposium Se-ries.M.E.
Foster, E.G.
Bard, M. Guhe, R.L.
Hill, J. Ober-lander, and A. Knoll.
2008.
The roles of haptic-ostensive referring expressions in cooperative, task-based human-robot dialogue.
In Proceedings of the3rd ACM/IEEE international conference on Humanrobot interaction, pages 295?302.
ACM.K.
Grabski and T. Scheffer.
2004.
Sentence completion.In Proceedings of the 27th annual international ACMSIGIR conference on Research and development in in-formation retrieval, pages 433?439.
ACM.Mark Hall, Eibe Frank, Geoffrey Holmes, Bern-hard Pfahringer, Peter Reutemann, and Ian H. Wit-ten.
2009.
The WEKA data mining soft-ware: An update.
SIGKDD Explorations, 11(1).http://www.cs.waikato.ac.nz/ml/weka/.M.
Kipp.
2001.
Anvil-a generic annotation tool for mul-timodal dialogue.
In Seventh European Conference onSpeech Communication and Technology.D.
Klein and C.D.
Manning.
2003.
Accurate unlexical-ized parsing.
In Proceedings of the 41st Annual Meet-ing on Association for Computational Linguistics-Volume 1, pages 423?430.
Association for Computa-tional Linguistics.K.M.
Krapp.
2002.
The Gale Encyclopedia of Nursing& Allied Health: DH, volume 2.
Gale Cengage.F.
Landragin, N. Bellalem, L. Romary, et al 2002.
Re-ferring to objects with spoken and haptic modalities.F.
Yang, P.A.
Heeman, and A.L.
Kun.
2011.
Aninvestigation of interruptions and resumptions inmulti-tasking dialogues.
Computational Linguistics,37(1):75?104.294
