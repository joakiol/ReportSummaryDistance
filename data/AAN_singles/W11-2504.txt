Proceedings of the GEMS 2011 Workshop on Geometrical Models of Natural Language Semantics, EMNLP 2011, pages 33?42,Edinburgh, Scotland, UK, July 31, 2011. c?2011 Association for Computational LinguisticsReranking Bilingually Extracted ParaphrasesUsing Monolingual Distributional SimilarityTsz Ping Chan, Chris Callison-Burch and Benjamin Van DurmeCenter for Language and Speech Processing, and HLTCOEJohns Hopkins UniversityAbstractThis paper improves an existing bilingualparaphrase extraction technique using mono-lingual distributional similarity to rerank can-didate paraphrases.
Raw monolingual dataprovides a complementary and orthogonalsource of information that lessens the com-monly observed errors in bilingual pivot-based methods.
Our experiments reveal thatmonolingual scoring of bilingually extractedparaphrases has a significantly stronger cor-relation with human judgment for grammat-icality than the probabilities assigned by thebilingual pivoting method does.
The resultsalso show that monolingual distribution simi-larity can serve as a threshold for high preci-sion paraphrase selection.1 IntroductionParaphrasing is the rewording of a phrase suchthat meaning is preserved.
Data-driven paraphraseacquisition techniques can be categorized by thetype of data that they use (Madnani and Dorr,2010).
Monolingual paraphrasing techniques clus-ter phrases through statistical characteristics suchas dependency path similarities or distributional co-occurrence information (Lin and Pantel, 2001; Pascaand Dienes, 2005).
Bilingual paraphrasing tech-niques use parallel corpora to extract potential para-phrases by grouping English phrases that share thesame foreign translations (Bannard and Callison-Burch, 2005).
Other efforts blur the lines betweenthe two, applying techniques from statistical ma-chine translation to monolingual data or extract-ing paraphrases from multiple English translationsof the same foreign text (Barzilay and McKeown,2001; Pang et al, 2003; Quirk et al, 2004).We exploit both methodologies, applying amonolingually-derived similarity metric to the out-put of a pivot-based bilingual paraphrase model.
Inthis paper we investigate the strengths and weak-nesses of scoring paraphrases using monolingualdistributional similarity versus the bilingually calcu-lated paraphrase probability.
We show that monolin-gual cosine similarity calculated on large volumesof text ranks bilingually-extracted paraphrases bet-ter than the paraphrase probability originally definedby Bannard and Callison-Burch (2005).
While ourcurrent implementation shows improvement mainlyin grammaticality, other contextual features are ex-pected to enhance the meaning preservation of para-phrases.
We also show that monolingual scores canprovide a reasonable threshold for picking out highprecision paraphrases.2 Related Work2.1 Paraphrase Extraction from BitextsBannard and Callison-Burch (2005) proposed iden-tifying paraphrases by pivoting through phrases in abilingual parallel corpora.
Figure 1 illustrates theirparaphrase extraction process.
The target phrase,e.g.
thrown into jail, is found in a German-Englishparallel corpus.
The corresponding foreign phrase(festgenommen) is identified using word alignmentand phrase extraction techniques from phrase-basedstatistical machine translation (Koehn et al, 2003).Other occurrences of the foreign phrase in the par-allel corpus may align to a distinct English phrase,such as jailed.
As the original phrase occurs sev-eral times and aligns with many different foreignphrases, each of these may align to a variety of otherEnglish paraphrases.
Thus, thrown into jail not onlyparaphrases as jailed, but also as arrested, detained,imprisoned, incarcerated, locked up, and so on.
Badparaphrases, such as maltreated, thrown, cases, cus-tody, arrest, and protection, may also arise due topoor word alignment quality and other factors.33... letzteWoche wurden in Irland f?nf Landwirte festgenommen , weil sie verhindern wollten... last week five farmers were thrown into jail in Ireland because they resisted ......Zahlreiche Journalisten sind verschwunden oder wurden festgenommen , gefoltert und get?tet .Quite a few journalists have disappeared or have been imprisoned , tortured and killed .Figure 1: Using a bilingual parallel corpus to extractparaphrases.Bannard and Callison-Burch (2005) defined aparaphrase probability to rank these paraphrase can-didates, as follows:e?2 = arg maxe2 6=e1p(e2|e1) (1)p(e2|e1) =?fp(e2, f |e1) (2)=?fp(e2|f, e1)p(f |e1) (3)?
?fp(e2|f)p(f |e1) (4)where p(e2|e1) is the paraphrase probability, andp(e|f) and p(f |e) are translation probabilities froma statistical translation model.Anecdotally, this paraphrase probability some-times seems unable to discriminate between goodand bad paraphrases, so some researchers disregardit and treat the extracted paraphrases as an unsortedset (Snover et al, 2010).
Callison-Burch (2008)attempts to improve the ranking by limiting para-phrases to be the same syntactic type.We attempt to rerank the paraphrases using otherinformation.
This is similar to the efforts of Zhaoet al (2008), who made use of multiple resources toderive feature functions and extract paraphrase ta-bles.
The paraphrase that maximizes a log-linearcombination of various feature functions is then se-lected as the optimal paraphrase.
Feature weightsin the model are optimized by minimizing a phrasesubstitution error rate, a measure proposed by theauthors, on a development set.2.2 Monolingual Distributional SimilarityPrior work has explored the acquisition of para-phrases using distributional similarity computedfrom monolingual resources, such as in the DIRTresults of Lin and Pantel (2001).
In these models,phrases are judged to be similar based on the cosinedistance of their associated context vectors.
In somecases, such as by Lin and Pantel, or the seminal workof Church and Hanks (1991), distributional contextis defined using frequencies of words appearing invarious syntactic relations with other lexical items.For example, the nouns apple and orange are con-textually similar partly because they both often ap-pear as the object of the verb eat.
While syntac-tic contexts provide strong evidence of distributionalpreferences, it is computationally expensive to parsevery large corpora, so it is also common to representcontext vectors with simpler representations like ad-jacent words and n-grams (Lapata and Keller, 2005;Bhagat and Ravichandran, 2008; Lin et al, 2010;Van Durme and Lall, 2010).
In these models, ap-ple and orange might be judged similar because bothtend to be one word to the right of some, and one tothe left of juice.Here we calculate distributional similarity using aweb-scale n-gram corpus (Brants and Franz, 2006;Lin et al, 2010).
Given both the size of the collec-tion, and that the n-grams are sub-sentential (the n-grams are no longer than 5 tokens by design), it wasnot feasible to parse, which led to the use of n-gramcontexts.
Here we use adjacent unigrams.
For eachphrase x we wished to paraphrase, we extracted thecontext vector of x from the n-gram collection assuch: every (n-gram, frequency) pair of the form:(ax, f ), or (xb, f ), gave rise to the (feature, value)pair: (wi?1=a, f ), or (wi+1=b, f ), respectively.
Inorder to scale to this size of a collection, we reliedon Locality Sensitive Hashing (LSH), as was donepreviously by Ravichandran et al (2005) and Bha-gat and Ravichandran (2008).
To avoid computingfeature vectors explicitly, which can be a memoryintensive bottleneck, we employed the online LSHvariant described by Van Durme and Lall (2010).This variant, based on the earlier work of Indykand Motwani (1998) and Charikar (2002), approxi-mates the cosine similarity between two feature vec-tors based on the Hamming distance in a reduced bit-wise representation.
In brief, for the feature vectors~u, ~v, each of dimension d, then the cosine similarityis defined as: ~u?~v|~u||~v| .
If we project ~u and ~v througha d by b random matrix populated with draws from34huge amount ofBiP SyntBiP BiP-MonoDSlarge number of, .33 large number of, .38 huge amount of, 1.0in large numbers, .11 great number of, .09 large quantity of, .98great number of, .08 huge amount of, .06 large number of, .98large numbers of, .06 vast number of, .06 great number of, .97vast number of, .06 vast number of, .94huge amount of, .06 in large numbers, .10large quantity of, .03 large numbers of, .08Table 1: Paraphrases for huge amount of according to thebilingual pivoting (BiP), syntactic-constrainted bilingualpivoting (SyntBiP) translation score and the monolingualsimilarity score via LSH (MonoDS), ranked by corre-sponding scores listed next to each paraphrase.
Syntactictype of the phrase is [JJ+NN+IN].N(0, 1), then we convert our feature vectors to bitsignatures of length b, by setting each bit of the sig-nature conditioned on whether or not the respectiveprojected value is greater than or equal to 0.
Giventhe bit signatures h(~u) and h(~v), we approximatecosine with the formula: cos(D(h(~u),h(~v))b pi), whereD() is Hamming distance.3 Ranking ParaphrasesWe use several different methods to rank candidatesets of paraphrases that are extracted from bilingualparallel corpora.
Our three scoring methods are:?
MonoDS ?
monolingual distributional similar-ity calculated over the Google n-gram corpusvia LSH, as described in Section 2.2.?
BiP ?
bilingual pivoting is calculated as inEquation 4 following Bannard and Callison-Burch (2005).
The translation model probabili-ties are estimated from a French-English paral-lel corpus.?
SyntBiP ?
syntactically-constrained bilingualpivoting.
This refinement to BiP, proposed inCallison-Burch (2008), constrains paraphrasesto be the same syntactic type as the originalphrase in the pivoting step of the paraphrase ta-ble construction.When we use MonoDS to re-score a candidate set,we indicate which bilingual paraphrase extractionmethod was used to extract the candidates as prefix,as in BiP-MonoDS or SyntBiP-MonoDS.reluctantMonoDShand?selected BiP*willing, .99 not, .56loath, .98 unwilling, .04*eager, .98 reluctance, .03somewhat reluctant, .98 reticent, .03unable, .98 hesitant, .02denied access, .98 reticent about, .01disinclined, .98 reservations, .01very unwilling, .97 reticence, .01conducive, .97 hesitate, .01linked, .97 are reluctant, .01Table 2: Ordered reranked paraphrase candidates for thephrase reluctant according to monolingual distributionalsimilarity (MonoDShand?selected) and bilingual pivotingparaphrase (BiP) method.
Two hand-selected phrases arelabeled with asterisks.3.1 Example Paraphrase ScoresTable 1 shows the paraphrase candidates for thephrase huge amount of along with the values for eachof our three scoring methods.
Although MonoDSdoes not explicitly impose syntactic restrictions, thesyntactic structure of the paraphrase in large num-bers contributes to the large difference in the leftand right context of the paraphrase and of the orig-inal phrase.
Hence, the paraphrase was assigned alow score of 0.098 as compared to other paraphrasecandidates with the correct syntactic type.
Note thatthe SyntBiP produced significantly fewer paraphrasecandidates, since its paraphrase candidates must bethe same syntactic type as the original phrase.
Iden-tity paraphrases are excluded for the rest of the dis-cussion in this paper.3.2 Susceptibility to AntonymsMonolingual distributional similarity is widelyknown to conflate words with opposite meaning andhas motivated a large body of prior work on antonymdetection (Lin and Zhao, 2003; Lin and Pantel,2001; Mohammad et al, 2008a; Mohammad et al,2008b; Marneffe et al, 2008; Voorhees, 2008).
Incontrast, the antonyms of a phrase are rarely pro-duced during pivoting of the BiP methods becausethey tend not to share the same foreign translations.Since the reranking framework proposed here be-gins with paraphrases acquired by the BiP methodol-35ogy, MonoDS can considerably enhance the qualityof ranking while sidestepping the antonym problemthat arises from using MonoDS alone.To support this intuition, an example of a para-phrase list with inserted hand-selected phrasesranked by each reranking methods is shown in Ta-ble 21.
Hand-selected antonyms of reluctant are in-serted into the paraphrase candidates extracted byBiP before they are reranked by MonoDS.
This isanalogous to the case without pre-filtering of para-phrases by BiP and all phrases are treated equallyby MonoDS alone.
BiP cannot rank these hand-selected paraphrases since, by construction, they donot share any foreign translation and hence theirparaphrase scores are not defined.
As expected fromthe drawbacks of monolingual-based statistics, will-ing and eager are assigned top scores by MonoDS,although good paraphrases such as somewhat reluc-tant and disinclined are also ranked highly.
Thisillustrates how BiP complements the monolingualreranking technique by providing orthogonal infor-mation to address the issue of antonyms for Mon-oDS.3.3 Implementation DetailsFor BiP and SyntBiP, the French-English paralleltext from the Europarl corpus (Koehn, 2005) wasused to train the paraphrase model.
The parallelcorpus was extracted from proceedings of the Eu-ropean parliament with a total of about 1.3 millionsentences and close to 97 million words in the En-glish text.
Word alignments were generated withthe Berkeley aligner.
For SyntBiP, the English sideof the parallel corpus was parsed using the Stan-ford parser (Klein and Manning, 2003).
The transla-tion models were trained with Thrax, a grammar ex-tractor for machine translation (Weese et al, 2011).Thrax extracts phrase pairs that are labeled withcomplex syntactic labels following Zollmann andVenugopal (2006).For MonoDS, the web-scale n-gram collection ofLin et al (2010) was used to compute the mono-lingual distributional similarity features, using 512bits per signature in the resultant LSH projection.Following Van Durme and Lall (2010), we implic-1Generating a paraphrase list by MonoDS alone requiresbuilding features for all phrases in the corpus, which is com-putationally impractical and hence, was not considered here.itly represented the projection matrix with a pool ofsize 10,000.
In order to expand the coverage of thecandidates scored by the monolingual method, theLSH signatures are obtained only for the phrases inthe union set of the phrase-level outputs from theoriginal and from the syntactically constrained para-phrase models.
Since the n-gram corpus consistsof at most 5-gram and each distributional similar-ity feature requires a single neighboring token, theLSH signatures are generated only for phrases thatare 4-gram or less.
Phrases that didn?t appear in then-grams with at least one feature were discarded.4 Human EvaluationThe different paraphrase scoring methods were com-pared through a manual evaluation conducted onAmazon Mechanical Turk.
A set of 100 test phraseswere selected and for each test phrase, five distinctsentences were randomly sampled to capture the factthat paraphrases are valid in some contexts but notothers (Szpektor et al, 2007).
Judges evaluated theparaphrase quality through a substitution test: Foreach sampled sentence, the test phrase is substitutedwith automatically-generated paraphrases.
The sen-tences and the phrases are drawn from the Englishside of the Europarl corpus.
Judges indicated theamount of the original meaning preserved by theparaphrases and the grammaticality of the resultingsentences.
They assigned two values to each sen-tence using the 5-point scales defined in Callison-Burch (2008).The 100 test phrases consisted of 25 unigrams,25 bigrams, 25 trigrams and 25 4-grams.
These 25phrases were randomly sampled from the paraphrasetable generated by the bilingual pivoting method,with the following restrictions:?
The phrase must have occurred at least 5 timesin the parallel corpus and must have appearedin the web-scale n-grams.?
The size of the union of paraphrase candidatesfrom BiP and SyntBiP must be 10 or more.4.1 Calculating CorrelationIn addition to their average scores on the 5-pointscales, the different paraphrase ranking methodswere quantitatively evaluated by calculating theircorrelation with human judgments.
Their correla-tion is calculated using Kendall?s tau coefficient, a36Reranking Method Meaning GrammarBiP 0.14 0.04BiP-MonoDS 0.14 0.24?SyntBiP 0.19 0.08SyntBip-MonoDS 0.15 0.22?SyntBiPmatched 0.20 0.15SyntBiPmatched-MonoDS 0.17 0.16SyntBiP* 0.21 0.09SyntBiP-MonoDS* 0.16 0.22?Table 3: Kendall?s Tau rank correlation coefficients be-tween human judgment of meaning and grammaticalityfor the different paraphrase scoring methods.
Bottompanel: SyntBiPmatched is the same as SyntBiP exceptparaphrases must match with the original phrase in syn-tactic type.
SyntBiP* and MonoDS* are the same asbefore except they share the same phrase support withSyntBiPmatched.
(?
: MonoDS outperforms the corre-sponding BiP reranking at p-value?0.01, and ?
at?0.05)common measure of correlation between two rankedlists.
Kendall?s tau coefficient ranges between -1 and1, where 1 indicates a perfect agreement between apair of ranked lists.Since tied rankings occur in the human judgmentsand reranking methods, Kendall?s tau b, which ig-nores pairs with ties, is used in our analysis.
Anoverall Kendall?s tau coefficient presented in the re-sults section is calculated by averaging all Kendall?stau coefficients of a particular reranking methodover all phrase-sentence combinations.5 Experimental Results5.1 CorrelationThe Kendall?s tau coefficients for the three para-phrase ranking methods are reported Table 3.
Atotal of 100 phrases and 5 sentence per phrase areselected for the experiment, resulting in a max-imum support size of 500 for Kendall?s tau co-efficient calculation.
The overall sizes of sup-port are 500, 335, and 304 for BiP, SyntBiP andSyntBiPmatched, respectively.
The positive values ofKendall?s tau confirm both monolingual and bilin-gual approaches for paraphrase reranking are posi-tively correlated with human judgments overall.
Forgrammaticality, monolingual distributional simi-larity reranking correlates stronger with humanjudgments than bilingual pivoting methods.
Forexample, in the top panel, given a paraphrase ta-ble generated through bilingual pivoting, Kendall?stau for monolingual distributional similarity (BiP-MonoDS) achieves 0.24 while that of the bilin-gual pivoting ranking (BiP) is only 0.04.
Simi-larly, reranking of the paraphrases extracted withsyntactically-constrained bilingual pivoting shows astronger correlation between SyntBiP-MonoDS andgrammar judgments (0.22) than the SyntBiP (0.08).This result further supports the intuition of distri-butional similarity being suitable for paraphrasereranking in terms of grammaticality.In terms of meaning preservation, the Kendall?stau coefficient for MonoDS is often lower than thebilingual approaches, suggesting that paraphraseprobability from the bilingual approach correlatesbetter with phrasal meaning than the monolingualmetric.
For instance, SyntBiP reaches a Kendall?stau of 0.19, which is a slightly stronger correlationthan that of SyntBiP-MonoDS.
Although paraphrasecandidates were generated by bilingual pivoting,distributional similarity depends only on contextualsimilarity and does not guarantee paraphrases thatmatch with the original meaning; whereas Bilingualpivoting methods are derived based on shared for-eign translations which associate meaning.In the bottom panel of Table 3, only paraphrasesof the same syntactic type as the source phrase areincluded in the ranked list for Kendall?s tau calcula-tion.
The phrases associated with these paraphrasesare used for calculating Kendall?s tau for the orig-inal reranking methods (labeled as SyntBiP* andSyntBiP-MonoDS*).
Comparing only the bilingualmethods across panels, syntactic matching increasesthe correlation of bilingual pivoting metrics withhuman judgments in grammaticality (e.g., 0.15 forSyntBiPmatched and 0.08 for SyntBiP) but with onlyminimal effects on meaning.
The maximum valuesin the bottom panel for both categories are roughlythe same as that in the corresponding category inthe upper panel ({0.21,0.19} in meaning and {0.22,0.24} in grammar for lower and upper panels, re-spectively.)
This suggests that syntactic type match-ing offers similar improvement in grammaticalityas MonoDS, although syntactically-constrained ap-proaches have more confined paraphrase coverage.We performed a one-tailed sign test on theKendall?s Tau values across phrases to examine37Figure 2: Averaged scores in the top K paraphrase can-didates as a function of K for different reranking metrics.All methods performs similarly in meaning preservation,but SyntBiP-MonoDS outperforms other scoring meth-ods in grammaticality, as shown in the bottom graph.the statistical significance of the performance gaindue to MonoDS.
For grammaticality, except for thecase of syntactic type matching (SyntBiPmatched), p-values are less than 0.05, confirming the hypothesisthat MonoDS outperforms BiP.
The p-value for com-paring MonoDS and SyntBiPmatched exceeds 0.05,agreeing with our conclusion from Table 3 that thetwo methods perform similarly.5.2 Thresholding Using MonoDS ScoresOne possible use for the paraphrase scores would beas a cutoff threshold where any paraphrases exceed-ing that value would be selected.
Ideally, this wouldretain only high precision paraphrases.To verify whether scores from each method corre-spond to human judgments for paraphrases extractedby BiP, human evaluation scores are averaged formeaning and grammar within each range of para-phrase score for BiP and approximate cosine dis-tance for MonoDS, as shown in Table 4.
The BiPparaphrase score bin sizes are linear in log scale.BiP Paraphrase Score MonoDS LSH ScoreRegion M G Region M G1.00 ?
x > 0.37 3.6 3.7 1 ?
x > 0.95 4.0 4.40.37 ?
x > 0.14 3.6 3.7 0.95 ?
x > 0.9 3.2 4.00.14 ?
x > 0.05 3.4 3.6 0.9 ?
x > 0.85 3.3 4.00.05 ?
x > 1.8e-2 3.4 3.6 0.85 ?
x > 0.8 3.3 4.01.8e-2 ?
x > 6.7e-3 3.4 3.6 0.8 ?
x > 0.7 3.2 3.96.7e-3 ?
x > 2.5e-3 3.2 3.7 0.7 ?
x > 0.6 3.3 3.82.5e-3 ?
x > 9.1e-4 3.0 3.6 0.6 ?
x > 0.5 3.1 3.79.1e-4 ?
x > 3.4e-4 3.0 3.8 0.5 ?
x > 0.4 3.1 3.63.4e-4 ?
x > 1.2e-4 2.6 3.6 0.4 ?
x > 0.3 3.1 3.51.2e-4 ?
x > 4.5e-5 2.7 3.6 0.3 ?
x > 0.2 2.9 3.4x ?
4.5e-5 2.5 3.7 0.2 ?
x > 0.1 3.0 3.30.1 ?
x > 0 2.9 3.2Table 4: Averaged human judgment scores as a func-tion of binned paraphrase scores and binned LSH scores.MonoDS serves as much better thresholding score for ex-tracting high precision paraphrases.MonoDS LSH BiP Paraphrase ThresholdThreshold ?
0.05 ?
0.01 ?
6.7e-3?
0.9 4.2 / 4.4 4.1 / 4.4 4.0 / 4.4?
0.8 4.0 / 4.3 3.9 / 4.3 3.9 / 4.2?
0.7 3.9 / 4.1 3.8 / 4.2 3.8 / 4.1Table 5: Thresholding using both the MonoDS and BiPscores further improves the average human judgment ofMeaning / Grammar.Observe that for the BiP paraphrase scores on theleft panel, no trend on the averaged grammar scoresacross all score bins is present.
While a mild cor-relation exists between the averaged meaning scoresand the paraphrase scores, the top score region (1> x?
0) corresponds to merely an averaged value of 3.6on a 5-point scale.
Therefore, thresholding on BiPscores among a set of candidates would not guaran-tee accurate paraphrases in grammar or meaning.On the right panel, MonoDS LSH scores on para-phrase candidates produced by BiP are uniformlyhigher in grammar than meaning across all scorebins, similar to the correlation results in Table 3.The averaged grammar scores decreases monoton-ically and proportionally to the change in LSH val-ues.
With regard to meaning scores, the averagedvalues roughly correspond to the decrease of LSHvalues, implying distributional similarity correlatesweakly with human judgment in the meaning preser-38vation of paraphrase.
Note that the drop in averagedscores is the largest from the top bin (1?
x > 0.95)to the second bin (0.95 ?
x > 0.9) is the largestwithin both meaning and grammar.
This suggeststhat thresholding on top tiered MonoDS scorescan be a good filter for extracting high precisionparaphrases.
BiP scores, by comparison, are not asuseful for thresholding grammaticality.Additional performance gain attained by combin-ing the two thresholding are illustrated in Table 5,where averaged meaning and grammar scores arelisted for each combination of thresholding.
At athreshold of 0.9 for MonoDS LSH score and 0.05for BiP paraphrase score, the averaged meaning ex-ceeds the highest value reported in Table 4, whereasthe grammar scores reaches the value in the top binin Table 4.
General trends of improvement from uti-lizing the two reranking methods are observed bycomparing Tables 4 and 5.5.3 Top K AnalysisFigure 2 shows the mean human assigned scorewithin the top K candidates averaged across allphrases.
Compared across the two categories,meaning scores have lower range of score anda more uniform trend of decreasing values as Kgrows.
In grammaticality, BiP clearly underper-forms whereas the SyntBiP-MonoDS maintains thebest score among all methods over all values of K.In addition, a slow drop-off up until K = 4 in thecurve for SyntBiP-MonoDS implies that the qualityof paraphrases remains relatively high going fromtop 1 to top 4 candidates.In applications such as question answering orsearch, the order of answers presented is importantbecause the lower an answer is ranked, the less likelyit would be looked at by a user.
Based on this intu-ition, the paraphrase ranking methods are evaluatedusing the maximum human judgment score amongthe top K candidates obtained by each method.
Asshown in Table 6, when only the top candidateis considered, the averaged score corresponding tothe monolingual reranking methods are roughly thesame as that to the bilingual methods in meaning, butas K grows, the bilingual methods outperforms themonolingual methods.
In terms of grammaticality,scores associated with monolingual reranking meth-ods are consistently higher than the bilingual meth-Reranking MethodK BiP BiP-MonoDS SyntBiP SyntBiP-MonoDSM1 3.62 3.67 3.58 3.583 4.13 4.07 4.13 4.015 4.26 4.19 4.20 4.0910 4.39 4.30 4.25 4.23G1 3.83 4.11 4.04 4.233 4.22 4.45 4.47 4.545 4.38 4.54 4.55 4.6210 4.52 4.62 4.63 4.67Table 6: Average of the maximum human evaluationscore from top K candidates for each reranking method.Support sizes for BiP- and SyntBiP-based metrics are 500and 335, respectively.
(M = Meaning, G = Grammar)ods but the difference tapers off as K increases.
Thissuggests that when only limited top paraphrase can-didates can be evaluated, MonoDS is likely to pro-vide better quality of results.6 Detailed Examples6.1 MonoDS Filters Bad BiP ParaphrasesThe examples in the top panel of Table 7 illustrates afew disadvantages of the bilingual paraphrase scoresand how monolingual reranking complements thebilingual methods.
Translation models based onbilingual corpora are known to suffer from misalign-ment of the parallel text (Bannard and Callison-Burch, 2005), producing incorrect translations thatpropagate through in the paraphrase model.
This is-sue is exemplified in the phrase pairs {considerablechanges, caused quite}, {always declared, alwaysbeen}, and {significantly affected, known} listed Ta-ble 7.
The paraphrases are clearly unrelated to thecorresponding phrases as evident from the low rank-ings from human judges.
Nonetheless, they were in-cluded as candidates likely due to misalignment andwere ranked relatively high by BiP metric.
For ex-ample, considerable changes was aligned to modifieconside?rablement correctly.
However, due to a com-bination of loose translations and difficulty in align-ing multiple words that are spread out in a sentence,the French phrase was inaccurately matched withcaused quite by the aligner, inducing a bad para-phrase.
Note that in these cases LSH produces theresults that agrees with the human rankings.39RankingPhrase Paraphrase Sizepool Meaning Grammar BiP BiP-MonoDSsignificantly affected known 20 19 18.5 1 17considerable changes caused quite 23 23 23 2.5 23always declared always been 20 20 20 2 13hauled delivered 23 7 5.5 21.5 5.0fiscal burden?
taxes 18 13.5 18 6 16fiscal burden?
taxes 18 2 8 6 16legalise legalize 23 1 1 10 1to deal properly with address 35 4.5 5.5 4 29.5you have just stated you have just suggested 31 13.5 8.5 4 30Table 7: Examples of phrase pair rankings by different reranking methods and human judgments in terms of meaningand grammar.
Higher rank (smaller numbers) corresponds to more favorable paraphrases by the associated metric.(?
: Phrases are listed twice to show the ranking variation when substitutions are evaluated in different sentences.
)6.2 Context MattersOccasionally, paraphrases are context-dependent,meaning the relevance of the paraphrase depends onthe context in a sentence.
Bilingual methods cancapture limited context through syntactic constraintsif the POS tags of the paraphrases and the sentenceare available, while the distributional similarity met-ric, in its current implementation, is purely based onthe pattern of co-occurrence with neighboring con-text n-grams.
As a result, LSH scores should beslightly better at gauging the paraphrases defined bycontext, as suggested by some examples in Table 7.The phrase pair {hauled, delivered} differ slightlyin how they describe the manner that an object ismoved.
However, in the context of the followingsentence, they roughly correspond to the same idea:countries which do not comply with communitylegislation should be hauled before the court ofjustice and i think mrs palacio will do so .As a result, out of 23 candidates, human judgesranked delivered 7 and 5.5 for meaning and gram-mar, respectively.
The monolingual-based metricalso assigns a higher rank to the paraphrase whileBiP puts it near the lowest rank.Another example of context-dependency is thephrase pair {fiscal burden, taxes}, which could havesome foreign translations in common.
The originalphrase appears in the following sentence:... the member states can reduce the fiscal burdenconsisting of taxes and social contributions .The paraphrase candidate taxes is no longer ap-propriate with the consideration of the context sur-rounding the original phrase.
As such, taxes re-ceived rankings of 13.5, 18 and 16 out of 18for meaning, grammar, and MonoDS, respectively,whereas BiP assigns a 6 to the paraphrase.
The samephrase pair but a different sentence, the context in-duces opposite effects on the paraphrase judgments,where the paraphrase received 2 and 8 in the twocategories as shown in Table 7:the economic data for our eu as regards employ-ment and economic growth are not particularlygood , and , in addition , the fiscal burden in eu-rope , which is to be borne by the citizen , hasreached an all-time high of 46 % .Hence, distributional similarity offers additionaladvantages over BiP only when the paraphrase ap-pears in a context that also defines most of the non-zero dimensions of the LSH signature vector.The phrase pair {legalise, legalize} exemplifiesthe effect of using different corpora to train 2 para-phrase reranking models as shown in Table 7.
Mean-ing, grammar and MonoDS all received top rank outof all paraphrases, whereas BiP ranks the paraphrase10 out of 23.
Since the BiP method was trainedwith Europarl data, which is dominated by BritishEnglish, BiP fails to acknowledge the Americanspelling of the same word.
On the other hand, dis-tributional similarity feature vectors were extractedfrom the n-gram corpus with different variations ofEnglish, which was informative for paraphrase rank-ing.
This property can be exploited for adaptation ofspecific domain of paraphrases selection.406.3 Limitations of MonoDS ImplementationWhile the monolingual distributional similarityshows promise as a paraphrase ranking method,there are a number of additional drawbacks associ-ated with the implementation.The method is currently limited to phrases withup to 4 contiguous words that are present in then-gram corpus for LSH feature vector extraction.Since cosine similarity is a function of the anglebetween 2 vectors irrespective of the vector mag-nitudes, thresholding on low occurrences of highern-grams in the corpus construction causes larger n-grams to suffer from feature sparsity and be sus-ceptible to noise.
A few examples from the exper-iment demonstrate such scenario.
For a phrase todeal properly with, a paraphrase candidate addressreceives rankings of 4.5, 5.5 and 4 out of 35 formeaning, grammar and BiP, respectively, it is ranked29.5 by BiP-MonoDS.
The two phrases are expectedto have similar neighboring context in regular En-glish usage, but it might be misrepresented by theLSH feature vector due to the lack of occurrences ofthe 4-gram in the corpus.Another example of how sparsity affects LSH fea-ture vectors is the phrase you have just stated.
Anacceptable paraphrase you have just suggested wasranked 13.5, 8.5 and 6.5 out of a total of 31 can-didates by meaning, grammar and BiP, respectively,but MonoDS only ranks it at 30.
The cosine sim-ilarity between the phrases are 0.05, which is verylow.
However, the only tokens that differentiate the4-gram phrases, i.e.
{stated,suggested}, have a sim-ilarity score of 0.91.
This suggests that even thoughthe additional words in the phrase don?t alter themeaning significantly, the feature vectors are mis-represented due to the sparsity of the 4-gram.
Thishighlights a weakness of the current implementa-tion of distributional similarity, namely that contextwithin a phrase is not considered for larger n-grams.7 Conclusions and Future WorkWe have presented a novel paraphrase ranking met-ric that assigns a score to paraphrase candidates ac-cording to their monolingual distributional similar-ity to the original phrase.
While bilingual pivoting-based paraphrase models provide wide coverageof paraphrase candidates and syntactic constraintson the model confines the structural match, addi-tional contextual similarity information provided bymonolingual semantic statistics increases the accu-racy of paraphrase ranking within the target lan-guage.
Through a manual evaluation, it was shownthat monolingual distributional scores strongly cor-relate with human assessment of paraphrase qualityin terms of grammaticality, yet have minimal effectson meaning preservation of paraphrases.While we speculated that MonoDS would im-prove both meaning and grammar scoring for para-phrases, we found in the results that only gram-maticality was improved from the monolingual ap-proach.
This is likely due to the choice of how con-text is represented, which in this case is only singleneighboring words.
A consideration for future workto enhance paraphrasal meaning preservation wouldbe to explore other contextual representations, suchas syntactic dependency parsing (Lin, 1997), mu-tual information between co-occurences of phrasesChurch and Hanks (1991), or increasing the numberof neighboring words used in n-gram based repre-sentations.In future work we will make use of other com-plementary bilingual and monolingual knowledgesources by combining other features such as n-gramlength, language model scores, etc.
One approachwould be to perform minimum error rate trainingsimilar to Zhao et al (2008) in which linear weightsof a feature function for a set of paraphrases candi-date are trained iteratively to minimize the phrasal-substitution-based error rate.
Instead of phrasal sub-stitution in Zhao?s method, quantitative measure ofcorrelation with human judgment can be used asthe objective function to be optimized during train-ing.
Other techniques such as SVM-rank (Joachims,2002) may also be investigated for aggregating re-sults from multiple ranked lists.8 AcknowledgementsThanks to Courtney Napoles for advice regardinga pilot version of this work.
Thanks to JonathanWeese, Matt Post and Juri Ganitkevitch for their as-sistance with Thrax.
This research was supported bythe EuroMatrixPlus project funded by the EuropeanCommission (7th Framework Programme), and bythe NSF under grant IIS-0713448.
Opinions, inter-pretations, and conclusions are the authors?
alone.41ReferencesColin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with bilingual parallel corpora.
In Proceed-ings of ACL.Regina Barzilay and Kathleen McKeown.
2001.
Extract-ing paraphrases from a parallel corpus.
In Proceedingsof ACL.Rahul Bhagat and Deepak Ravichandran.
2008.
Largescale acquisition of paraphrases for learning surfacepatterns.
In Proceedings of ACL-HLT.Thorsten Brants and Alex Franz.
2006.
Web 1T 5-gramversion 1.Chris Callison-Burch.
2008.
Syntactic constraints onparaphrases extracted from parallel corpora.
In Pro-ceedings of EMNLP.Moses Charikar.
2002.
Similarity estimation techniquesfrom rounding algorithms.
In Proceedings of STOC.Kenneth Church and Patrick Hanks.
1991.
Word asso-ciation norms, mutual information and lexicography.Computational Linguistics, 6(1):22?29.Piotr Indyk and Rajeev Motwani.
1998.
Approximatenearest neighbors: towards removing the curse of di-mensionality.
In Proceedings of STOC.Thorsten Joachims.
2002.
Optimizing search enginesusing clickthrough data.
In Proceedings of the ACMConference on Knowledge Discovery and Data Min-ing.Dan Klein and Christopher D. Manning.
2003.
Fast exactinference with a factored model for natural languageparsing.
Advances in NIPS, 15:3?10.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proceed-ings of HLT/NAACL.Philipp Koehn.
2005.
A parallel corpus for statisticalmachine translation.
In Proceedings of MT-Summit.Mirella Lapata and Frank Keller.
2005.
Web-based mod-els for natural language processing.
ACM Transac-tions on Speech and Language Processing, 2(1).Dekang Lin and Patrick Pantel.
2001.
Discovery of infer-ence rules for question answering.
Natural LanguageEngineering, 7:343?360.Dekang Lin and Shaojun Zhao.
2003.
Identifying syn-onyms among distributionally similar words.
In Pro-ceedings of IJCAI-03, pages 1492?1493.Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,David Yarowsky, Shane Bergsma, Kailash Patil, EmilyPitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani,and Sushant Narsale.
2010.
New tools for web-scalen-grams.
In Proceedings of LREC.Dekang Lin.
1997.
Using syntactic dependency as localcontext to resolve word sense ambiguity.
In Proceed-ings of ACL.Nitin Madnani and Bonnie Dorr.
2010.
Generatingphrasal and sentential paraphrases: A survey of data-driven methods.
Computational Linguistics, 36(3).Marie-Catherine De Marneffe, Anna N. Rafferty, andChristopher D. Manning.
2008.
Finding contradic-tions in text.
In Proceedings of ACL 2008.Saif Mohammad, Bonnie Dorr, and Graeme Hirst.2008a.
Computing word-pair antonymy.
In Proceed-ings of the Conference on Empirical Methods in Natu-ral Language Processing, pages 982?991.
Associationfor Computational Linguistics.Saif Mohammad, Bonnie J. Dorr, Melissa Egan, NitinMadnani, David Zajic, and Jimmy Lin.
2008b.
Mul-tiple alternative sentence compressions and word-pairantonymy for automatic text summarization and rec-ognizing textual entailment.Bo Pang, Kevin Knight, and Daniel Marcu.
2003.Syntax-based alignment of multiple translations: Ex-tracting paraphrases and generating new sentences.
InProceedings of HLT/NAACL.Marius Pasca and Peter Dienes.
2005.
Aligning needlesin a haystack: Paraphrase acquisition across the web.In Proceedings of IJCNLP, pages 119?130.Chris Quirk, Chris Brockett, and William Dolan.
2004.Monolingual machine translation for paraphrase gen-eration.
In Proceedings of EMNLP, pages 142?149.Deepak Ravichandran, Patrick Pantel, and Eduard Hovy.2005.
Randomized Algorithms and NLP: Using Lo-cality Sensitive Hash Functions for High Speed NounClustering.
In Proceedings of ACL.Matthew Snover, Nitin Madnani, Bonnie Dorr, andRichard Schwartz.
2010.
TER-plus: paraphrase,semantic, and alignment enhancements to translationedit rate.
Machine Translation, 23(2-3):117?127.Idan Szpektor, Eyal Shnarch, and Ido Dagan.
2007.Instance-based evaluation of entailment rule acquisi-tion.
In Proceedings of ACL.Benjamin Van Durme and Ashwin Lall.
2010.
Onlinegeneration of locality sensitive hash signatures.
InProceedings of ACL, Short Papers.Ellen M. Voorhees.
2008.
Contradictions and justifica-tions: Extensions to the textual entailment task.Jonathan Weese, Juri Ganitkevitch, Chris Callison-Burch, Matt Post, and Adam Lopez.
2011.
Joshua3.0: Syntax-based machine translation with the thraxgrammar extractor.
EMNLP 2011 - Workshop on sta-tistical machine translation.Shiqi Zhao, Cheng Niu, Ming Zhou, Ting Liu, and ShengLi.
2008.
Combining multiple resources to improveSMT-based paraphrasing model.
In Proceedings ofACL/HLT.Andreas Zollmann and Ashish Venugopal.
2006.
Syntaxaugmented machine translation via chart parsing.
InProceedings of WMT06.42
