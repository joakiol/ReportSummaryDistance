2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 263?273,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsEvery sensible extended top-down tree transduceris a multi bottom-up tree transducerAndreas Maletti?Institute for Natural Language Processing, Universit?t StuttgartPfaffenwaldring 5b, 70569 Stuttgart, Germanyandreas.maletti@ims.uni-stuttgart.deAbstractA tree transformation is sensible if the size ofeach output tree is uniformly bounded by alinear function in the size of the correspond-ing input tree.
Every sensible tree transfor-mation computed by an arbitrary weighted ex-tended top-down tree transducer can also becomputed by a weighted multi bottom-up treetransducer.
This further motivates weightedmulti bottom-up tree transducers as suitabletranslation models for syntax-based machinetranslation.1 IntroductionSeveral different translation models are used insyntax-based statistical machine translation.
Koehn(2010) presents an introduction to statistical ma-chine translation, and Knight (2007) presents anoverview of syntax-based statistical machine trans-lation.
The oldest and best-studied tree transfor-mation device is the top-down tree transducer ofRounds (1970) and Thatcher (1970).
G?cseg andSteinby (1984) and F?l?p and Vogler (2009) presentthe existing results on the unweighted and weightedmodel, respectively.
Knight (2007) promotes theuse of weighted extended top-down tree transduc-ers (XTOP), which have also been implemented inthe toolkit TIBURON by May and Knight (2006)[more detail is reported by May (2010)].
In the con-text of bimorphisms, Arnold and Dauchet (1976) in-vestigated XTOP, and Lilin (1978) and Arnold andDauchet (1982) investigated multi bottom-up tree?The author was supported by the German Research Foun-dation (DFG) grant MA 4959/1-1.transducers (MBOT) [as k-morphisms].
Recently,weighted XTOP and MBOT, which are the cen-tral devices in this contribution, were investigatedby Maletti (2011a) in the context of statistical ma-chine translation.Several tree transformation devices are used astranslation models in statistical machine translation.Chiang (2007) uses synchronous context-free gram-mars, which force translations to be very similaras observed by Eisner (2003) and Shieber (2004).This deficiency is overcome by synchronous treesubstitution grammars, which are state-less linearand nondeleting XTOP.
Recently, Maletti (2010b)proposed MBOT, and Zhang et al (2008b) andSun et al (2009) proposed the even more powerfulsynchronous tree-sequence substitution grammars.Those two models allow certain translation discon-tinuities, and the former device also offers computa-tional benefits over linear and nondeleting XTOP asargued by Maletti (2010b).The simplicity of XTOP makes them very appeal-ing as translation models.
In 2010 the ATANLP par-ticipants [workshop at ACL] identified ?copying?
asthe most exciting and promising feature of XTOP,but unrestricted copying can lead to an undesirableexplosion of the size of the translation.
Accordingto Engelfriet and Maneth (2003) a tree transforma-tion has linear size-increase if the size of each outputtree is linearly bounded by the size of its correspond-ing input tree.
The author believes that this is a verysensible restriction that intuitively makes sense andat the same time suitably limits the copying powerof XTOP.We show that every sensible tree transformation263that can be computed by an XTOP can also be com-puted by an MBOT.
For example, linear XTOP (i.e.,no copying) compute only sensible tree transforma-tions, and Maletti (2008) shows that for each linearXTOP there exists an equivalent MBOT.
Here, wedo not make any restrictions on the XTOP besidessome sanity conditions (see Section 3).
In particu-lar, we consider copying XTOP.
If we accept the re-striction to linear size-increase tree transformation,then our main result further motivates MBOT as asuitable translation model for syntax-based machinetranslation because MBOT can implement each rea-sonable (even copying) XTOP.
In addition, our re-sult allows us to show that each reasonable XTOPpreserves regularity under backward application.
Asdemonstrated by May et al (2010) backward appli-cation is the standard application of XTOP in themachine translation pipeline, and preservation ofregularity is the essential property for several of theevaluation algorithms of May et al (2010).2 NotationWe start by introducing our notation for trees, whosenodes are labeled by elements of an alphabet ?
anda set V .
However, only leaves can be labeled byelements of V .
For every set T , we let?
(T ) = {?
(t1, .
.
.
, tk) | ?
?
?, t1, .
.
.
, tk ?
T} ,which contains all trees with a ?-labeled rootand direct successors in T .
The set T?
(V ) of?-trees with V -leaves is the smallest set T such thatV ?
?
(T ) ?
T .
We use X = {x1, x2, .
.
. }
as a setof formal variables.Each node of the tree t ?
T?
(V ) is identified bya position p ?
N+, which is a sequence of posi-tive integers.
The root is at position ?
(the emptystring), and the position ip with i ?
N+ and p ?
N?+is the position p in the i-th direct subtree.
Theset pos(t) contains all positions of t, and the sizeof t is |t| = |pos(t)|.
For each p ?
pos(t), the labelof t at p is t(p).
Given a set L ?
?
?V of labels, welet posL(t) = {p ?
pos(t) | t(p) ?
L} be the posi-tions with L-labels.
We write posl(t) for pos{l}(t)for each l ?
L. Finally, we write t[u]p for the treeobtained from t by replacing the subtree at position pby the tree u ?
T?
(V ).The following notions refer to the variables X .The tree t ?
T?
(V ) [potentially V ?
X = ?]
isS?NP1PP11x1112VP2VBD21ran211RB22away221Figure 1: The tree t (with positions indicated as super-scripts) is linear and var(t) = {x2}.
The tree t[He]111 isthe same tree with x2 replaced by ?He?.linear if every x ?
X occurs at most once in t (i.e.,|posx(t)| ?
1).
Moreover,var(t) = {x ?
X | posx(t) 6= ?
}contains the variables that occur in t. A substitu-tion ?
is a mapping ?
: X ?
T?
(V ).
When appliedto t, it returns the tree t?, which is obtained from tby replacing all occurrences of x ?
X in t by ?
(x).Our notions for trees are illustrated in Figure 1.Finally, we present weighted tree grammars(WTG) as defined by F?l?p and Vogler (2009), whodefined it for arbitrary semirings as weight struc-tures.
In contrast, our weights are always nonneg-ative reals, which form the semiring (R+,+, ?, 0, 1)and are used in probabilistic grammars.
For eachweight assignment f : T ?
R+, we letsupp(f) = {t ?
T | f(t) 6= 0} .WTG offer an efficient representation of weightedforests (i.e., set of weighted trees), which is evenmore efficient than the packed forests of Mi et al(2008) because they can be minimized efficiently us-ing an algorithm of Maletti and Quernheim (2011).In particular, WTG can share more than equivalentsubtrees and can even represent infinite sets of trees.A WTG is a system G = (Q,?, q0, P,wt) with?
a finite set Q of states (nonterminals),?
an alphabet ?
of symbols,?
a starting state q0 ?
Q,?
a finite set P of productions q ?
r, whereq ?
Q and r ?
T?
(Q) \Q, and?
a mapping wt: P ?
R+ that assigns produc-tion weights.Without loss of generality, we assume that we candistinguish states and symbols (i.e., Q ?
?
= ?
).For all ?, ?
?
T?
(Q) and a production ?
= q ?
r,264St1 VPt2 t37?St2 t1 t3Figure 2: Example rotation.
In principle, such rotationsare required in the translation from English to Arabic.we write ?
?
?G ?
if ?
= ?
[q]p and ?
= ?
[r]p, wherep is the lexicographically least element of posQ(?
).The WTG G generates the weighted tree lan-guage LG : T?
?
R+ such thatLG(t) =?n?N,?1,...,?n?Pq0?
?1G ????
?nG twt(?1) ?
.
.
.
?
wt(?n)for every t ?
T?.
Each such language is regular, andReg(?)
contains all those languages over the alpha-bet ?.
A thorough introduction to tree languages ispresented by G?cseg and Steinby (1984) and G?c-seg and Steinby (1997) for the unweighted case andby F?l?p and Vogler (2009) for the weighted case.3 Extended top-down tree transducersWe start by introducing the main model of thiscontribution.
Extended top-down tree transducers(XTOP) are a generalization of the top-down treetransducers (TOP) of Rounds (1970) and Thatcher(1970).
XTOP allow rules with several (non-stateand non-variable) symbols in the left-hand side (asin the rule of Figure 3), whereas a TOP rule containsexactly one symbol in the left-hand side.
Shieber(2004) and Knight (2007) identified that this exten-sion is essential for many NLP applications becausewithout it linear (i.e., non-copying) cannot computerotations (see Figure 2).
In the form of bimorphismsXTOP were investigated by Arnold and Dauchet(1976) and Arnold and Dauchet (1982) in the 1970s,and Knight (2007) invigorated research.As demonstrated by Graehl et al (2009) themost general XTOP model includes copying, dele-tion, and regular look-ahead in the spirit of En-gelfriet (1977).
More powerful models (such assynchronous tree-sequence substitution grammarsand multi bottom-up tree transducers) can handletranslation discontinuities naturally as evidencedby Zhang et al (2008a) and Maletti (2011b), butq0Sx1 VPx2 x3?SqVBx2qNPx1qNPx3Figure 3: Example XTOP rule by Graehl et al (2008).XTOP need copying and deletion to handle them.Copying essentially allows an XTOP to translatecertain parts of the input several times and was iden-tified by the ATANLP 2010 participants as one of themost interesting and promising features of XTOP.Currently, the look-ahead feature is not used in ma-chine translation, but we need it later on in the theo-retical development.Given an alphabet Q and a set T , we letQ[T ] = {q(t) | q ?
Q, t ?
T},in which the root always has exactly one succes-sor from T in contrast to Q(T ).
We treat elementsof Q[T?
(V )] as special trees of T?
?Q(V ).
More-over, we let 1??
(t) = 1 for every t ?
T?.
XTOPwith regular look-ahead (XTOPR) were also stud-ied by Knight and Graehl (2005) and Graehl et al(2008).
Formally, an XTOPR is a systemM = (Q,?,?, q0, R, c,wt)with?
a finite set Q of states,?
alphabets ?
and ?
of input and output symbols,?
a starting state q0 ?
Q,?
a finite set R of rules of the form ` ?
r withlinear ` ?
Q[T?
(X)] and r ?
T?(Q[var(`)]),?
c : R ?
X ?
Reg(?)
assigns a regular look-ahead to each deleted variable of a rule [i.e.,c(` ?
r, x) = 1??
for all ` ?
r ?
R andx ?
X \ (var(`) \ var(r))], and?
wt: R?
R+ assigns rule weights.The XTOPR M is linear [respectively, nondeleting]if r is linear [respectively, var(`) = var(r)] for ev-ery rule ` ?
r ?
R. It has no look-ahead (XTOP)if c(?, x) = 1??
for all ?
?
R and x ?
X .
Figure 3shows a rule of a linear and nondeleting XTOP.The look-ahead can be used to restrict rule appli-cations.
It can inspect subtrees that are deleted by a265uq0St1VPt2 t3?
?,.5MuSqVBt2qNPt1qNPt3Figure 4: Rewrite step using rule ?
of Figure 3.rule application, so for each rule ?
= ` ?
r, we letdel(?)
= var(`) \ var(r) be the set of deleted vari-ables in ?.
If we suppose that a variable x ?
del(?
)matches to an input subtree t, then the weight of thelook-ahead c(?, x)(t), which we also write c?,x(t),is applied to the derivation.
If it is 0, then this look-ahead essentially prohibits the application of ?.
It isimportant that the look-ahead is regular (i.e., thereexists a WTG accepting it).
The toolkit TIBURONby May and Knight (2006) implements XTOP to-gether with a number of essential operations.
Look-ahead is not implemented in TIBURON, but it canbe simulated using a composition of two XTOP, inwhich the first XTOP performs the look-ahead andmarks the results, so that the second XTOP can ac-cess the look-ahead information.As for WTG the semantics for the XTOPRM = (Q,?,?, I, R, c,wt) is presented usingrewriting.
Without loss of generality, we again sup-pose that Q ?
(?
??)
= ?.
Let ?, ?
?
T?(Q[T?
]),w ?
R+, and ?
= ` ?
r be a rule of R. We write?
?
?,wM ?
if there exists a substitution ?
: X ?
T?such that?
?
= ?[`?]p,?
?
= ?[r?
]p, and?
w = wt(?)
??x?del(?)
c?,x(x?
),where p ?
posQ(?)
is the lexicographically leastQ-labeled position in ?.
Figure 4 illustrates a deriva-tion step.The XTOPR M computes a weighted tree trans-formation by applying rewrite steps to the tree q0(t),where t ?
T?
is the input tree, until an outputtree u ?
T?
has been produced.
The weight of aparticular derivation is obtained by multiplying theweights of the rewrite steps.
The weight of the trans-formation from t to u is obtained by summing allweights of the derivations from q0(t) to u. For-mally1, the weighted tree transformation computedby M in state q ?
Q is?
qM (t, u) =?n?N,?1,...,?n?Rq(t)?
?1,w1M ????
?n,wnM uw1 ?
.
.
.
?
wn (1)for every t ?
T?
and u ?
T?.
The XTOPR Mcomputes the weighted tree transformation ?
q0M .
TwoXTOPR M and N are equivalent, if ?M = ?N .The sum (1) can be infinite, which we avoid bysimply requiring that all our XTOPR are produc-ing, which means that r /?
Q[X] for every rule` ?
r ?
R.2 In a producing XTOPR each rule ap-plication produces at least one output symbol, whichlimits the number n of rule applications to the size ofthe output tree u.
A detailed exposition to XTOPR ispresented by Arnold and Dauchet (1982) and Graehlet al (2009) for the unweighted case and by F?l?pand Vogler (2009) for the weighted case.Example 1.
LetMex = (Q,?,?, q, R, c,wt) be thenondeleting XTOP with?
Q = {q},?
?
= {?, ?, ?},?
the two rulesq(?)?
?
(?)q(?(x1))?
?
(q(x1), q(x1)) (??)?
trivial look-ahead (i.e., c(?, x) = 1??
), and?
wt(?)
= 2 and wt(??)
= 1.The XTOPR Mex computes the tree transformationthat turns the input tree ?n(?)
into the fully balancedbinary tree u of the same height with weight 2(2n).An example derivation is presented in Figure 5.Unrestricted copying (as in Example 1) yieldsvery undesirable phenomena and is most likely notneeded in the machine translation task.
In fact, itis almost universally agreed that a translation modelshould be ?linear-size increase?, which means that1There is an additional restriction that is discussed in thenext paragraph.2This is a convenience requirement.
We can use other con-ditions on the XTOPR or the used weight structures to guaranteea well-defined semantics.266q??????,1Mex?q??q?????,1Mex??q?q?q?????,1Mex??q?q??q?q???,2Mex???
q??q?q??
?,2Mex ?
?
?
??,2Mex???
???
?Figure 5: Example derivation using the XTOP Mex with weight 13 ?
24 = 16.the size of each output tree should be linearlybounded in the size of the corresponding input treeaccording to Aho and Ullman (1971) and Engelfrietand Maneth (2003).Definition 2.
A mapping ?
: T?
?
T?
?
R+ islinear-size increase if there exists an integer n ?
Nsuch that |u| ?
n ?
|t| for all (t, u) ?
supp(?
).An XTOPR M is sensible if ?M is linear-size in-crease.?Sensible?
is not a syntactic property of anXTOPR as it does not depend on the actual rules,but only on its computed weighted tree transforma-tion.
The XTOP Mex of Example 1 is not sensiblebecause |u| = 2|t| ?
1 for every (t, u) ?
?Mex .
In-tuitively, the number of times that Mex can use thecopying rule ??
is not uniformly bounded.We need an auxiliary result in the main part.Let ?
: T?
?
T?
?
R+ be a weighted treetransformation.
We need the weighted tree lan-guage ?
?1(u) : T?
?
R+ of input trees weightedby their translation weight to a given outputtree u ?
T?.
Formally,(?
?1(u))(t) = ?
(t, u) forevery t ?
T?.Theorem 3.
For every producing XTOPR M andoutput tree u?
?
T?, the weighted tree lan-guage ?
?1M (u?)
is regular.Proof sketch.
We use some properties that are onlydefined in the next sections (for proof economy).
Itis recommended to skip this proof on the first read-ing and revisit it later.
Maletti (2010a) shows thatwe can construct an XTOPR M ?
such that?M ?
(t, u) ={?M (t, u) if u?
= u0 otherwisefor every t ?
T?
and u ?
T?.
This operation iscalled ?output product?
by Maletti (2010a).
The ob-tained XTOPR M ?
is also producing, so we knowthat M ?
can take at most |u?| rewrite steps to de-rive u?.
Since M ?
can only produce the outputtree u?, this also limits the total number of rule appli-cations in any successful derivation.
Consequently,M ?
can only apply a copying rule at most |u?| times,which shows that M ?
is finitely copying (see Def-inition 8).
By Theorem 11 we can implement M ?by an equivalent MBOT M ??
(i.e., ?M ??
= ?M ?
;see Section 5), for which we know by Theorem 14of Maletti (2011a) that ?
?1M ??
(u) = ?
?1M ?
(u) is regu-lar.Finally, let us illustrate the overall structure of ourarguments to show that every sensible XTOPR canbe implemented by an equivalent MBOT.
We firstnormalize the given XTOPR such that the seman-tic property ?sensible?
yields a syntactic propertycalled ?finitely copying?
(see Section 4).
In a secondstep, we show that each finitely copying XTOPR canbe implemented by an equivalent MBOT (see Sec-tion 5).
Figure 6 illustrates these steps towards ourmain result.
In the final section, we derive some con-sequences from our main result (see Section 6).4 From sensible to finite copyingFirst, we adjust a normal form of Engelfriet andManeth (2003) to our needs.
This section bor-rows heavily from Aho and Ullman (1971) and En-gelfriet and Maneth (2003), where ?sensible?
(unweighted) deterministic macro tree transduc-ers (MAC) [see Engelfriet and Vogler (1985)] areconsidered.
Our setting is simpler on the one handbecause XTOPR do not have context parametersas MAC, but more difficult on the other hand be-cause we consider nondeterministic and weightedtransducers.Intuitively, a sensible XTOPR cannot copy a lotsince the size of each output tree is linearly boundedin the size of the corresponding input tree.
However,the actual presentation of the XTOPR M might con-267sensible XTOPRsensible proper XTOPRfinitely copying XTOPRlinear and nondeleting MBOTFigure 6: Overview of the proof steps.tain rules that allow unbounded copying.
This un-bounded copying might not manifest due to the look-ahead restrictions or due to the fact that those rulescannot be used in a successful derivation.
The pur-pose of the normal form is the elimination of thoseartifacts.
To this end, we eliminate all states (exceptthe initial state) that can only produce finitely manyoutputs.
Such a state can simply be replaced by oneof the output trees that it can produce and an ad-ditional look-ahead that checks whether the currentinput tree indeed allows that translation (and insertsthe correct translation weight).Normalized XTOPR are called ?proper?, and wedefine this property next.
For the rest of this section,let M = (Q,?,?, q0, R, c,wt) be the consideredsensible XTOPR.
Without loss of generality, we as-sume that the state q0 does not occur in the right-hand sides of rules.
Moreover, we write ?
?
?M ?
ifthere exist nonzero weights w1, .
.
.
, wn ?
R+ \ {0}and rules ?1, .
.
.
, ?n ?
R with?
?
?1,w1M ?
?
?
?
?n,wnM ?
.In essence, ?
?
?M ?
means that M can transform ?into ?
(in the unweighted setting).Definition 4.
A state q ?
Q is proper if there are in-finitely many u?
?
T?
such that there exists a deriva-tionq0(t)?
?M ?
[q(s)]p ?
?M u[u?
]pwhere s, t ?
T?
are input trees, ?
?
T?(Q[T?
]),p ?
pos(?
), and u ?
T?
is an output tree.The derivation in Definition 4 is illustrated in Fig-ure 7.
In other words, a proper state is reachablefrom the initial state and can transform infinitelymany input trees into infinitely many output trees.The latter is an immediate consequence of Defini-tion 4 since each input tree can be transformed intoonly finitely many output trees due to sensibility.The restriction includes the look-ahead (because werequire that the rewrite step weights are nonzero),which might further restrict the input trees.Example 5.
The state q of the XTOP Mex is properbecause we already demonstrated that it can trans-form infinitely many input trees into infinitely manyoutput trees.The XTOPR M is proper if all its states exceptthe initial state q0 are proper.
Next, we show thateach XTOPR can be transformed into an equivalentproper XTOPR using a simplified version of the con-struction of Lemma 5.4 by Engelfriet and Maneth(2003).
Mind that we generally assume that all con-sidered XTOPR are producing.Theorem 6.
For every XTOPR there exists an equiv-alent proper XTOPR.Proof sketch.
The construction is iterative.
Sup-pose that M is not yet proper.
Then there existsa state q ?
Q, which can produce only finitelymany outputs U .
It can be decided whether a stateis proper using Theorem 4.5 of Drewes and Engel-friet (1998), and in case it is proper, the set U canalso be computed effectively.
The cited theorem ap-plies to unweighted XTOPR, but it can be appliedalso in our setting because?
?M in Definition 4 dis-regards weights.
Now we consider each u ?
U in-dividually.
Clearly, (?
qM )?1(u) is regular by The-orem 3.
For each u and each occurrence of q inthe right-hand side of a rule ?
?
R of M , we cre-ate a copy ??
of ?, in which the selected occur-rence of q(x) is replaced by u and the new look-ahead is c(?
?, x) = c(?, x) ?
(?
qM )?1(u), which re-stricts the input tree appropriately and includes theadjustment of the weights.
Since regular weightedtree languages are closed under HADAMARD prod-ucts [see F?l?p and Vogler (2009)], the look-aheadc(?, x) ?
(?
qM )?1(u) is again regular.Essentially, we precompute the action of q asmuch as possible, and immediately output one ofthe finitely many output trees, check that the inputtree has the required shape using the look-ahead,and charge the weight for the precomputed trans-formation again using the look-ahead.
This pro-cess is done for each occurrence, so if a rule con-tains two occurrences of q, then the process must be268q0t ??M......qs?
?M......u?Figure 7: Illustration of the derivation in Definition 4.done twice to this rule.
In this way, we eventuallypurge all occurrences of q from the right-hand sidesof rules of M without changing the computed trans-formation.
Since q 6= q0 and q is now unreachable,it is useless and can be deleted, which removes onenon-proper state.
This process is repeated until allstates except the initial state q0 are proper.Clearly, the construction of Theorem 6 appliedto a sensible XTOPR M yields a sensible properXTOPR M ?
since the property ?sensible?
refers tothe computed transformation and ?M = ?M ?
.
Let usillustrate the construction on a small example.Example 7.
Let ?
be the rule displayed in Figure 3,and let us assume that the state qVB is not proper.Moreover, suppose that qVB can yield the outputtree u and that we already computed the translationoptions that yield u.
Let t1, .
.
.
, tn ?
T?
be thosetranslation options.
Then we create the copy ?
?q0(S(x1,VP(x2, x3)))?
S(u, qNP(x1), qNP(x3))of the rule ?
with look-ahead c?(?
?, x) such thatc??
?,x(t) ={c?,x(t) if x 6= x2?
qVBM (t, u) if x = x2 .In general, there can be infinitely many inputtrees ti that translate to a selected output tree u, sowe cannot simply replace the variable in the left-hand side by all the options for the input tree.
Thisis the reason why we use the look-ahead because theset ?
?1M (u) is a regular weighted tree language.From now on, we assume that the XTOPR M isproper.
Next, we want to invoke Theorem 7.1 of En-gelfriet and Maneth (2003) to show that a propersensible XTOPR is finitely copying.
Engelfriet andManeth (2003) present a formal definition of finitecopying, but we only present a high-level descrip-tion of it.Definition 8.
The XTOPR M is finitely copying ifthere is a copying bound n ?
N such that no inputsubtree is copied more than n times in any derivationq(t)?
?M u with q ?
Q, t ?
T?, and u ?
T?.Example 9.
The XTOP of Example 1 is not finitelycopying as the input subtree ?
is copied 2n times ifthe input tree is ?n(?).
Clearly, this shows that thereis no uniform bound on the number of copies.It is worth noting that the properties ?sensible?
and?finitely copying?
are essentially unweighted prop-erties.
They largely disregard the weights and aweighted XTOPR does have one of those propertiesif and only if its associated unweighted XTOPR hasit.
We now use this tight connection to lift Theo-rem 7.1 of Engelfriet and Maneth (2003) from theunweighted (and deterministic) case to the weighted(and nondeterministic) case.Theorem 10.
If a proper XTOPR is sensible, then itis finitely copying.Proof.
Let M be the input XTOPR.
Since M is sen-sible, its associated unweighted XTOPR N , whichis obtained by setting all weights to 1 and comput-ing in the BOOLEAN semiring, is sensible.
Conse-quently,N is finitely copying by Theorem 7.1 of En-gelfriet and Maneth (2003).
Thus, also M is finitelycopying, which concludes the proof.
We remarkthat Theorem 7.1 of Engelfriet and Maneth (2003)only applies to deterministic XTOPR, but the essen-tial pumping argument, which is Lemma 6.2 of En-gelfriet and Maneth (2003) also works for nonde-terministic XTOPR.
Essentially, the pumping argu-ment shows the contraposition.
If M is not finitelycopying, then M can copy a certain subtree an arbi-trarily often.
Due to the properness of M , all thesecopies have an impact on the output tree, whichyields that its size grows beyond any uniform lin-ear bound, which in turn demonstrates that M is notsensible.269We showed that each sensible XTOPR can be im-plemented by a finitely copying XTOPR via the con-struction of the proper normal form.
This approachactually yields a characterization because finitelycopying XTOPR are trivially sensible by Theo-rem 4.19 of Engelfriet and Maneth (2003).5 From finite copying to an MBOTWe complete the argument by showing how to im-plement a finitely copying XTOPR by a weightedmulti bottom-up tree transducer (MBOT).
First, werecall the MBOT, which was introduced by Arnoldand Dauchet (1982) and Lilin (1978) in the un-weighted case.
Engelfriet et al (2009) give an En-glish presentation.
We present the linear and non-deleting MBOT of Engelfriet et al (2009).A weighted multi bottom-up tree transducer is asystem M = (Q,?,?, F,R,wt) with?
an alphabet Q of states,?
alphabets ?
and ?
of input and output symbols,?
a set F ?
Q of final states,?
a finite set R of rules of the form ` ?
r where` ?
T?
(Q(X)) and r ?
Q(T?
(X)) are linearand var(`) = var(r), and?
wt: R?
R+ assigning rule weights.We now use T?
(Q(X)) and Q(T?
(X)) instead ofT?
(Q[X]) and Q[T?
(X)], which highlights the dif-ference between XTOPR and MBOT.
First, MBOTare a bottom-up device, which yields that ?
and ?as well as ` and r exchange their place.
More impor-tantly, MBOT can use states with more than 1 suc-cessor (e.g, Q(X) instead of Q[X]).
An examplerule is displayed in Figure 8.Let M = (Q,?,?, F,R,wt) be an MBOT suchthatQ?(???)
= ?.3 We require that r /?
Q(X) foreach rule ` ?
r ?
R to guarantee finite derivationsand thus a well-defined semantics.4 As before, wepresent a rewrite semantics.
Let ?, ?
?
T?(Q(T?
)),and let ?
= ` ?
r be a rule.
We write ?
?
?M ?if there exists a substitution ?
: X ?
T?
such that?
= ?[`?
]p and ?
= ?[r?
]p, where p ?
pos(?)
be isthe lexicographically least reducible position in ?.
Arewrite step is illustrated in Figure 8.3This restriction can always be achieved by renaming thestates.4Again this could have been achieved with the help of otherconditions on the MBOT or the used weight structure.The weighted tree transformation computed byMin state q ?
Q is?
qM (t, u1 ?
?
?uk) =?n?N,?1,...,?n?Rt?
?1M ????
?nM q(u1,...,uk)wt(?1) ?
.
.
.
?
wt(?n)for all t ?
T?
and u1, .
.
.
, uk ?
T?.
The semanticsof M is ?M (t, u) =?q?F ?qM (t, u) for all t ?
T?and u ?
T?.We move to the last step for our main result,in which we show how to implement each finitelycopying XTOPR by an MBOT using a weighted ver-sion of the construction in Lemma 15 of Maletti(2008).
The computational benefits (binarization,composition, efficient parsing, etc.)
of MBOT overXTOPR are described by Maletti (2011a).Theorem 11.
Every finitely copying XTOPR can beimplemented by an MBOT.Proof sketch.
We plan to utilize Theorem 18 of En-gelfriet et al (2009), which proves the same state-ment in the unweighted and deterministic case.Again, the weights are not problematic, but we needto remove the nondeterminism before we can applyit.
This is achieved by a decomposition into twoXTOPR.
The first XTOPR annotates the input treewith the rules that the second XTOPR is supposed touse.
Thus, the first XTOPR remains nondeterminis-tic, but the second XTOPR, which simply executesthe annotated rules, is now deterministic.
This stan-dard approach due to Engelfriet (1975) is used inmany similar constructions.Suppose that n is a copying bound for the inputXTOPR M , which means that no more than n rulesare applied to each input symbol.
The first XTOPRis actually a nondeterministic linear and nondeletingXTOP that annotates each input tree symbol with ex-actly n rules of M that are consistent with the statebehavior of M .
Moreover, the annotation also pre-scribes with which of n rules the processing shouldcontinue at each subtree.
Since we know all the rulesthat will potentially be applied for a certain symbol,we can make the assignment such that no annotatedrule is used twice in the same derivation.
The de-tails for this construction can be found in Lemma 15of Maletti (2008).In this way, we obtain a weighted linear and non-deleting XTOP M1, which includes the look-ahead,270SqNPx1VPqVBx2qNPx3 x4?qSSx2 x1 x3x4tSqNPu1VPqVBu2qNPu3 u4?
?MtqSSu2 u1 u3u4Figure 8: Example MBOT rule ?
[left] and its use in a rewrite step [right].and an unweighted deterministic XTOP M2.
Onlythe weight and look-ahead of rules that are actu-ally executed are applied (e.g., although we anno-tate n rules at the root symbol, we only execute thefirst rule and thus only apply its weight and look-ahead).
The look-ahead of different rules is eitherresolved (i.e., pushed to the next rules) or multi-plied using the HADAMARD product [see F?l?p andVogler (2009)], which preserves regularity.
Thisprocess is also used by Seemann et al (2012).
Nowwe can use Theorem 4 of Maletti (2011a) to obtainan MBOT N1 that is equivalent to M1.
Similarly,we can use Theorem 18 of Engelfriet et al (2009)to obtain an MBOT N2 that is equivalent to M2.Since MBOT are closed under composition by The-orem 23 of Engelfriet et al (2009), we can composeN1 andN2 to obtain a single MBOTN that is equiv-alent to M .Corollary 12.
For every sensible producing XTOPRthere exists an equivalent MBOT.Proof.
Theorem 6 shows that there exists an equiva-lent proper XTOPR, which must be finitely copyingby Theorem 10.
This last fact allows us to constructan equivalent MBOT by Theorem 11.6 Preservation of regularityFinally, we present an application of Corollary 12 tosolve an open problem.
The translation model is of-ten used in a backwards manner in a machine trans-lation system as demonstrated, for example, by Mayet al (2010), which means that an output tree is sup-plied and the corresponding input trees are sought.This starting output tree is typically the best parseof the string that we want to translate.
However, in-stead of a single tree, we want to use all parses ofthis sentence together with their parse scores.
Thoseparses form a regular weighted tree language, andapplying them backwards to the translation modelyields another weighted tree language L of corre-sponding input trees.
For an efficient representationand efficient modification algorithms (such a k-bestextraction) we would like L to be regular.
However,F?l?p et al (2011) demonstrate that the backwardapplication of a regular weighted tree language toan XTOPR is not necessarily regular.
The counterex-ample uses a variant of the XTOP of Example 1 andis thus not sensible.
Theorem 14 of Maletti (2011a)shows that MBOT preserve regularity under back-ward application.Corollary 13.
Sensible XTOPR preserve regularityunder backward application.ConclusionWe demonstrated that each sensible XTOPR can beimplemented by an MBOT.
The latter formalism of-fers many computational advantages, so that the au-thor believes that MBOT should be used instead ofXTOP.
We used real number weights, but the authorbelieves that our results carry over to at least all zero-sum and zero-divisor free semirings [see Hebischand Weinert (1998) and Golan (1999)], which aresemirings such that (i) a+ b = 0 implies a = 0 and(ii) a ?
b = 0 implies 0 ?
{a, b}.
Whether our resultshold in other semirings (such as the semiring of allreals where ?1 + 1 = 0) remains an open question.271ReferencesAlfred V. Aho and Jeffrey D. Ullman.
1971.
Transla-tions on a context-free grammar.
Inform.
and Control,19(5):439?475.Andr?
Arnold and Max Dauchet.
1976.
Bi-transductionsde for?ts.
In Proc.
3th Int.
Coll.
Automata, Languagesand Programming, pages 74?86.
University of Edin-burgh.Andr?
Arnold and Max Dauchet.
1982.
Morphismeset bimorphismes d?arbres.
Theoret.
Comput.
Sci.,20(1):33?93.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Comput.
Linguist., 33(2):201?228.Frank Drewes and Joost Engelfriet.
1998.
Decidabilityof the finiteness of ranges of tree transductions.
In-form.
and Comput., 145(1):1?50.Jason Eisner.
2003.
Learning non-isomorphic tree map-pings for machine translation.
In Proc.
41st Ann.Meeting Association for Computational Linguistics,pages 205?208.
Association for Computational Lin-guistics.Joost Engelfriet and Sebastian Maneth.
2003.
Macro treetranslations of linear size increase are MSO definable.SIAM J.
Comput., 32(4):950?1006.Joost Engelfriet and Heiko Vogler.
1985.
Macro treetransducers.
J. Comput.
System Sci., 31(1):71?146.Joost Engelfriet, Eric Lilin, and Andreas Maletti.
2009.Extended multi bottom-up tree transducers ?
compo-sition and decomposition.
Acta Inform., 46(8):561?590.Joost Engelfriet.
1975.
Bottom-up and top-down treetransformations ?
a comparison.
Math.
Systems The-ory, 9(3):198?231.Joost Engelfriet.
1977.
Top-down tree transducerswith regular look-ahead.
Math.
Systems Theory,10(1):289?303.Zolt?n F?l?p and Heiko Vogler.
2009.
Weighted treeautomata and tree transducers.
In Manfred Droste,Werner Kuich, and Heiko Vogler, editors, Handbookof Weighted Automata, EATCS Monographs on Theo-ret.
Comput.
Sci., chapter 9, pages 313?403.
Springer.Zolt?n F?l?p, Andreas Maletti, and Heiko Vogler.
2011.Weighted extended tree transducers.
Fundam.
Inform.,111(2):163?202.Ferenc G?cseg and Magnus Steinby.
1984.
Tree Au-tomata.
Akad?miai Kiad?, Budapest.Ferenc G?cseg and Magnus Steinby.
1997.
Tree lan-guages.
In Grzegorz Rozenberg and Arto Salomaa,editors, Handbook of Formal Languages, volume 3,chapter 1, pages 1?68.
Springer.Jonathan S. Golan.
1999.
Semirings and their Applica-tions.
Kluwer Academic, Dordrecht.Jonathan Graehl, Kevin Knight, and Jonathan May.2008.
Training tree transducers.
Comput.
Linguist.,34(3):391?427.Jonathan Graehl, Mark Hopkins, Kevin Knight, and An-dreas Maletti.
2009.
The power of extended top-downtree transducers.
SIAM J.
Comput., 39(2):410?430.Udo Hebisch and Hanns J. Weinert.
1998.
Semirings?Algebraic Theory and Applications in Computer Sci-ence.
World Scientific.Kevin Knight and Jonathan Graehl.
2005.
An overviewof probabilistic tree transducers for natural languageprocessing.
In Proc.
6th Int.
Conf.
Computational Lin-guistics and Intelligent Text Processing, volume 3406of LNCS, pages 1?24.
Springer.Kevin Knight.
2007.
Capturing practical natu-ral language transformations.
Machine Translation,21(2):121?133.Philipp Koehn.
2010.
Statistical Machine Translation.Cambridge University Press.Eric Lilin.
1978.
Une g?n?ralisation des transduc-teurs d?
?tats finis d?arbres: les S-transducteurs.
Th?se3?me cycle, Universit?
de Lille.Andreas Maletti and Daniel Quernheim.
2011.
Pushingfor weighted tree automata.
In Proc.
36th Int.
Symp.Mathematical Foundations of Computer Science, vol-ume 6907 of LNCS, pages 460?471.
Springer.Andreas Maletti.
2008.
Compositions of extended top-down tree transducers.
Inform.
and Comput., 206(9?10):1187?1196.Andreas Maletti.
2010a.
Input and output productsfor weighted extended top-down tree transducers.
InProc.
14th Int.
Conf.
Developments in Language The-ory, volume 6224 of LNCS, pages 316?327.
Springer.Andreas Maletti.
2010b.
Why synchronous tree substitu-tion grammars?
In Proc.
Human Language Technolo-gies: Conf.
North American Chapter of the ACL, pages876?884.
Association for Computational Linguistics.Andreas Maletti.
2011a.
An alternative to synchronoustree substitution grammars.
J. Natur.
Lang.
Engrg.,17(2):221?242.Andreas Maletti.
2011b.
How to train your multi bottom-up tree transducer.
In Proc.
49th Ann.
Meeting Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, pages 825?834.
Association forComputational Linguistics.Jonathan May and Kevin Knight.
2006.
Tiburon: Aweighted tree automata toolkit.
In Proc.
11th Int.Conf.
Implementation and Application of Automata,volume 4094 of LNCS, pages 102?113.
Springer.Jonathan May, Kevin Knight, and Heiko Vogler.
2010.Efficient inference through cascades of weighted treetransducers.
In Proc.
48th Ann.
Meeting Associationfor Computational Linguistics, pages 1058?1066.
As-sociation for Computational Linguistics.272Jonathan May.
2010.
Weighted Tree Automata andTransducers for Syntactic Natural Language Process-ing.
Ph.D. thesis, University of Southern California,Los Angeles.Haitao Mi, Liang Huang, and Qun Liu.
2008.
Forest-based translation.
In Proc.
46th Ann.
Meeting Associ-ation for Computational Linguistics, pages 192?199.Association for Computational Linguistics.William C. Rounds.
1970.
Mappings and grammars ontrees.
Math.
Systems Theory, 4(3):257?287.Nina Seemann, Daniel Quernheim, Fabienne Braune, andAndreas Maletti.
2012.
Preservation of recognizabil-ity for weighted linear extended top-down tree trans-ducers.
In Proc.
2nd Workshop Applications of TreeAutomata in Natural Language Processing, pages 1?10.
Association for Computational Linguistics.Stuart M. Shieber.
2004.
Synchronous grammars as treetransducers.
In Proc.
7th Int.
Workshop Tree AdjoiningGrammars and Related Formalisms, pages 88?95.Jun Sun, Min Zhang, and Chew Lim Tan.
2009.
Anon-contiguous tree sequence alignment-based modelfor statistical machine translation.
In Proc.
47th Ann.Meeting Association for Computational Linguistics,pages 914?922.
Association for Computational Lin-guistics.James W. Thatcher.
1970.
Generalized2 sequential ma-chine maps.
J. Comput.
System Sci., 4(4):339?367.Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,Chew Lim Tan, and Sheng Li.
2008a.
A tree se-quence alignment-based tree-to-tree translation model.In Proc.
46th Ann.
Meeting Association for Compu-tational Linguistics, pages 559?567.
Association forComputational Linguistics.Min Zhang, Hongfei Jiang, Haizhou Li, Aiti Aw, andSheng Li.
2008b.
Grammar comparison study fortranslational equivalence modeling and statistical ma-chine translation.
In Proc.
22nd Int.
Conf.
Computa-tional Linguistics, pages 1097?1104.
Association forComputational Linguistics.273
