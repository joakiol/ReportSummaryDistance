COMBINAT ION OF N-GRAMS AND STOCHASTICCONTEXT-FREE GRAMMARS FOR LANGUAGE MODEL ING*J os6 -Migue l  Benedf  and  Joan-Andreu  SSnchezDe, i )artmnento de Sistemas Informgti(:os y Coml)utaci6nUniversidad Polil;(!ClfiCa (te ValenciaCmnino de \Sera s/n,  d6022 Valencia (Slmin)e-mail: {.jt)enedi,j andreu} (@dsic.ul)v.esAbst rac tThis t)al)t;r de, scribes a hybrid prol)osal tocombine n-grams and Stochastic Context-FreeGrammars (SCFGs) tbr language modeling.
Aclassical n-gram model is used to cat)lure thelocal relations between words, while a stochas-tic grammatical inodel is considered to repre-sent the hmg-term relations between syntacticalstru(:tm'es.
In order to define this granmlaticalmodel, which will 1)e used on large-vo(:almlarycomph'~x tasks, a eategory-t)ased SCFG and aprol)abilisti(" model of' word (tistrilmtion in thecategories have been 1)rol)osed.
Methods forleanfing these stochastic models tTor complextasks are described, and algorithms for con>puting the word transition probal)ilities are also1)resented.
Filmily, ext)erilnents using the PennTreel)ank corpus improved by 30% the test; set;l)erph~xity with regard to the classical n-grammodels.1 I n t roduct ionLanguage modeling is an important asl)e('t toconsider in large-vocabulary Sl)eeeh recognitionsystenls (Bahl et al, 1983; ,lelinek, 1998).
Then--grain models are the most widely-used for awide range of domains (Bahl et al, 1983).
Then-grams are simple and robust models and ad-equately capture the local restrictions betweenwords.
Moreover, it is well-known how to es-timate the parameters of the lnodet and howto integrate them in a speech recognition sys-tem.
However, the n-grmn models cannot ad-equately characterize the long-term constraintsof the sentences of the tasks.On the other hand, Stochastic Context-FreeGrammars (SCI)'Gs) allow us a better model-* This work has been partially SUl)l)<)rted by the S1)anishCICYT under contract (TIC98/0423-C(16).ing of long-term relations and work well onlhnited-domain tasks of low perplexity.
How-ever, SCFGs work poorly for large-vocabulary,general-purpose tasks because learning SCFGsand the (:Olnlmtation of word transition 1)roba -bilities present serious 1)roblenls tLr ('olnplex realtasks.In the literature, a nulnber of works have pro-posed ways to generalize the n-gram models (.le-linek, 1998; Siu and Ostendorf, 2000) or com-1)ining with other structural models (Bellegarda,1998; Gilet and Ward, 1998; Chellm and Jelinek,1998).In this iml)er, we present a confl)ined lan-guage model defined as a linear combination ofn-grams, whk'h are llse(t to capture the local re-lations between words, and a stoehasti(: gram-matieal model whi(:h is used to represent heglottal relation 1)etw(x'dl synl:aetie strll(;tllrt~s, hior(ter to (:at)turc, these lollg-terltl relations an(t tosolve the main 1)rolflems derived Dora the large-vocabulary complex tasks, we 1)l'Ol)ose here todetine: a eategory--ba,~ed SCFG and a prolmbilis-tic model of word distrilmt;ion in the categories.Taking into a(:count his proposal, we also de-scribe here how to solve the learning of thesestochastic models and their integrati(m prol>1CIlIS.With regard to the learning problem, severalalgorithms that learn SCFGs by means of es-timation algorithms have been 1)reposed (Lariand Young, 1990; Pereira and Schal)es, 1992;Sfinehez and Benedi, 1998), and pronfising re-suits have been achieved with category-basedSCFGs on real tasks (Sfi.nchez and Benedi,\]999).In relation to the integration problem, wel)resent wo algorithms that compute the wordtransition 1)robability: the first algorithm isbased on the l~efl;-to-ll,ight Inside algorithln55(LRI) (Jelinek and Lafferty, 1991), and the sec-ond is based on an application of a Viterbischeme to the LRI algorithm (the VLRI algo-rithm) (S~nehez and Benedf, 1997).Finally, in order to evaluate the behavior ofthis proposal, experiments with a part of theWall Street Journal processed in the Penn Tree-bank project were carried out and significant im-provements with regard to the classical n-grammodels were achieved.2 The  language mode lAn important problem related to language mod-eling is the evaluation of Pr(wk I w l .
.
.
wk-1).In order to compute this probability, we pro-pose a hybrid language model defined as a sire-ple linear combination of n-gram models and astochastic grammatical model G~:Pr(wklwl .
.
.w~_l)  = c~Pr(~klwk-n.. .wt~-l)+(1 - c~) P"(wklw~... wk-,, G~), (1)where 0 < c~ < 1 is a weight factor which de-pends on the task.The expression Pr(w/~lwk_n...wk-,) is theword probability of occurrence of w/~ given bythe n-gram model.
The parameters of thismodel can be easily estinmted, and the ex-pression Pr(wl~lWl~_n...  wtc-1) can be efficientlycomputed (Bahl et al, 1983; Jelinek, 1998).In order to define the stochastic gram-matical model G~ of the expressionPr(wk\]w~ .
.
.
wk_ j ,  G~) for large-vocabularycomplex tasks, we propose a combination oftwo different stochastic models: a category-based SCFG (G~), that allows us to representthe long-term relations between these syntacti-cal structures and a probabilistic model of worddistribution into categories (Cw).This proposal introduces two imlmrtant as-peels, which are the estimation of the parame-ters of the stochastic models, Gc and Cw, andthe computation of the following expression:Pr(~klWl... Wk-1, ac, Cw)= Pr(wl .
.
.
wk.. .
la~, c~) (2)IC , C,,,)3 Training of the modelsThe parameters of the described model are es-timated fi'om a training sample, that is, froma set of sentences.
Each word of the sentencehas a part-of  speech tag (POStag) associatedto it.
These POStags are considered as wordcategories and are the terminal symbols of theSCFG.
h 'om this training sample, the parame-ters of G~ and C~ can be estimated as tbllows.First, tile parameters of Cw, represented byPr(w\[c), are computed as:=E,o,  (3)where N(w,c) is the number of times that theword w has been labeled with the POStagc.
Itis important o note that a word w can belong todifferent categories.
In addition, it may hapt)enthat a word in a test set does not appear inthe training set, and therefore some smoothingtechnique has to be carried out.With regard to the estimation of the category-based SCFGs, one of the most widely-knownmethods is the Inside-Outside (IO) algo-rithln (Lari and Young, 1990).
The applicationof this algorithm presents important problemswhich are accentuated in real tasks: the timecomplexity per iteration and the large numberof iterations that are necessary to converge.
Analternative to the IO algorithm is a.n algorithmbased on the Viterbi score (VS algorithm) (Ney,1992).
The convergence of the VS algorithmis faster than the IO algorithm.
However, theSCFGs obtained are, in genera.l, not as welllearned (Simchez et al, 1996).Another possibility for estimating SCFCs,which is somewhere between the IO and VS al-gorithms, has recently been proposed.
This ap-proach considers only a certain subset of deriva-tions in the estimation process.
In order toselect this subset of derivations, two alterna-tives have been considered: froln structural in-formation content in a bracketed corpus (Pereiraand Schabes, 1992; Amaya et al, 1999), andfrom statistical information content in the k-best derivations (Sgmchez and Benedl, 1998).In the first alternative, the IOb and VSb algo-rithms which learn SCFGs from partially brack-eted corpora were defined (Pereira and Schabes,1992; Amaya et al, 1999).
In the second alter-native, the kVS algorithm for the estimation ofthe probability distributions of a SCFG fl'om thek-best derivations was proposed (Shnchez andBenedi, 1998).56All of these algorithms have a tilne (:omi)lexityO('n,a\[PI), where 'n is the length of the inputst;ring, and \[1)1 is the size.
of the SCFG.These algorithms have been tested inreal tasks fl)r estimating cat(,gory-1)asedSCFOs (Sfinchez and Benedf, 1999) and theresults obtained justify their applicatiol, incomplex real tasks.4 In tegrat ion  o f  the  mode ll?rom exl)ression (2), it can bee se(m that in or-der to integrate the too(M, it is necessary toefli(:iently ('oml)ute the expression:P~0,,~... ',,,k... la,.., <,,).
(4)In order to describo how this computation (:anl)e m~de, we tirst introduce some notation.A Court:el-Free, Grammar G is a four-tul)le(N, E, P, S), wher(; N is the tinit(; set of nont(;r-minals, )2 is the tinite sol; of terminals (N ~-/E =0), S ~ N is the axiom or initial symbol and1' is the finite set of t)rodu(:tions or ruh;s of thetbrm A -+ it, where A c N a.nd c~ C (N U E) +(only grmmmtrs with non (;mt)ty rules ar(; con-sidered).
FOI" siml)li('ity (but without loss of g('.n-erality) only (:ontext-iYee grammars in Ch, om.s'kyNormal Form are.
considere(l, that is, grammarswith rules of the form A -+ HC or A -> v wh(n'(:A ,B ,C  C N and v ~ )2.A Stoch, a.stic Contcxt-l';rc.c U'raw, w, wl" G.~ is apair (G,p), where G is a (:ontext-fr(,.
(; grain-mar and p : P -+\]0,1\] is a 1)robal)ility tim(:-{;ion of rule ai)l)li('al;ion su(:h that VA ~ N:}~,c(Nu>~)+ p(A --+ ,~) - -  i.Now, we pr(:sent two algorithms ill order tocompute the word transition 1)rol)at)ility.
Thefirst algorithm is based on the Ll/i algorithm,a.nd the second is based on an apt)li('atiou of aViterbi s(:heme to the LRI algorithln (the VLI/\]a.lgorithm).P robab i l i ty  of  generat ing  an init ialsubst r ingThe COmlmtation of (4) is l)as('.d on an algo-rithm which is a modith:ation of the I,RI algo-rithm (aelinek and Lafl'erty, 1991).
This newalgorithln is based on the detlnition of Pr(A <<i , j ) )  = Pr(A ~ wi .
.
.w j  .
.
.
Ic .
.
c,,,) as th(,1)robability that A generates the initial sul)stringwi .
.
.
w j .
.
.
given Gc and C.,,,.
This can l)e com-puted with the following (lynamic 1)rogrmmnings(:henl(;:c+ ~ Q(A ~ D)p(D ~ ~)P"(~"~l~)),D.i-1Pr (d<<i , j )=  E ~ Q(A ~ BC)B,CGN l=ipl.
(J~ < ~:, 1 >) p,.
(c << 1 + 1, j ) .
::: this way, Pr (~,~.
.
.~,k .
.
.
IG, : ,6 ' , , , )  =Pr(S << l,k).In this cxi)ression, Q(A ~ D) is the proba-bility that D is the leftmost nol:terminal in allsentential fOHllS which are derived from A. Thevahu; Q(A ~ BC)  is the probability that BCis th(; initial substring of all sentential forms de-riv(;d from i\.
Pr(H < i, l  >) is th{; probabilitythat the substring "wi... wz is generated from/~given G,: and C.,,,.
Its contlmt;ation will be de-fined \]ater.It shouh:l be noted that th(; combination ofthe models G,.
and C~,, in carried out in the vah:eP'r(A << i, i).
This is the lnain difl:'crcnce withresp(wt the \]A/I algorithm.P robab i l i ty  of  the  best  der ivat iongenerat ing  an init ial  subst r ingAn algorithm whi(:\]l is similar to the previous(>he (-m~ l)e (l(~fin(~d t)ased on the \;iterl)i ,~(:lmme.In this way, it is l)onsil)le to obtain the \])cst; pars-ing of an initial sul)string.
This new algorithm isalso related to the \/'Lll.I algol'ithni (Shn(:hez andB('aw, di, 1997) and is 1)ased on the (lciinition ofP,~'(A << ',:, J)) = P,~'(A ~ ",,i .
.
.
',,j .
.
.
IG~:, Cw)as the probability of the most probal)le 1)arsingwhich generates wi .
.
.w j .
.
,  from A given G,:and C,,.
This can 1)(i (:omputcd as follows:p:(A << i , , : )=  m:?x(p(A ~ c)l),(~,,~l,.
'),m~Lx(Q(A => D)'p(D ~ c)Pr('wi\[c))),DPI~'(A << i , j )  = max :mix  (Q(A ~ \]3C)H,CcN l=i...j-IAPr(\]3 < i, l  >)Pr (C  << l + 1,j)) .A AThorcfo,o >4",~...'wk... \[a,:, <,,)  -- p , .
(s  <<1,k).In this expression, Q(A ~ D) is the t)rob -al)ility that D is the leftmost nontermina, l in57the most t)robable sentential form which is de-rived ti'om d. The value Q(A ~ BC) is theprobability that BC is the initial substring ofmost the probable sentential form derived fromA.
Pr(B < i, 1 >) is the probability of the mostprobable parse which generates wi ?
?
?
wl froli1 B.P robab i l i ty  of generating a stringThe wflue Pr(A < i , j  >) = Pr(A d>wi...'u;jlG~,Go) is defined as the probabilitythat the substring wi... wj is generated fromA given G~ and C,~.
To calculate this proba-bility a modification of the well-known Insidealgorithm (Lari and Young, 1990) is proposed.This computation is carried out by using thefollowing dynamic progralmning scheme:Pr(A < i,i >) -- ~p(A-~ c) Pr(wilc) ,cj - - IPr (A<i , j>)  : E E p(A -+ BC)B,CcN l=ipr(B < i,l >)Pr (C < 1 + 1,j >).In this way, Pr(w~ ...whiGs, C,,) = Pr(S <1,n >).As we have commented above, the combina-tion of the two parts of the grammatical modelis carried out in the value Pr(A < i, i >).Probabi l i ty of the best derivationgenerating a stringThe t)rol)abitity of the best derivation thatgenel'~-gtes a s t r ing ,  P r ( 'u ,1 .
.
.
~t/2,~l~c, 6 'w)  , canbe evaluated using a Viterbi-like scheme (Ney,1992).
As in the previous case, the computationof this probability is based on the definition ofp .
(A < g,j >) = pU-(A <,,) asthe probability of the best derivation that gen-erates the substring wi... wj fi'om A given Gcand Cw.
Similarly:P r (A<i , i>)  =Pr(A < i , j  >) =Therefore,1, n >).i nax  \ ] ) (a  -9  C)I)I'('//)i lC) , Cmax n ,ax  -+Be)B,CCN ...Pr(B < i, l  >)Pr (C  < 1 + 1, j  >) .nla , C,,o) = P -(X <Finally, the time complexity of these algo-rithms is the same as the algorithms they arerelated to, there%re the time colnplexity isO(k:alrl), where tc is the length of the inputstring and IPI is the size of the SCFG.5 Exper iments  w i th  the  PennTreebank  CorpusThe corpus used in the experiments was the partof the Wall Street Journal which had been pro-cessed in the Petal %'eebank project 1 (Marcus el:al., 1993).
This corpus consists of English textscollected from the Wall Street Journal from edi-tions of the late eighties.
It contains approx-imately one million words.
This corpus wasautomatically labelled, analyzed and manuallychecked as described in (Marcus et 31., 1993).There are two kinds of labelling: a POStag la-belling and a syntactic labelling.
The size ofthe vocalmlary is greater than 25,000 diil'erentwords, the POStag vocabulary is composed of45 labels 2 and the syntactic vocabulary is com-posed of 14 labels.The corpus was divided into sentences accord-ing to the bracketing.
In this way, we obtaineda corpus whose main characteristics are shownin Table 1.Table 1: Characteristics of the Petal Treebankcorpus once it; was divided into sentences.No.
of Av.
Std.
Min.
Max.senten, length deviation length length49,207 23.61 11.13 1 249We took advantage of the category-basedSCFGs estimated in a previous work (Simchezand Benedf, 1998).
These SCFGs were esti-mated with sentences which had less than 15words.
Therefore, in this work, we assumed suchrestriction.
The vocabulary size of the new cor-pus was 6,333 different words.
For the exper-iments, the corpus was divided into a trainingcorpus (directories 00 to 1.9) and a test corpus(directories 20 to 24).
The characteristics ofthese sets can be seen in Table 2.
The part of the1Release 2 of this data set can be ob-tained t'rmn the Linguistic Data Consor-tium with Catalogue number LDC94T4B(http://www.ldc.upenn.edu/ldc/nofranm.html)2There are 48 labels defined in (Marcus et al, 1993),however, three of ttmm do not appear in the corpus.58(-orlms lal)(:led with l)()Stags was used to (:st;i-mate the p~wameters of tlm grammati(:al me(M,while the non-lad)e\](;(l part was u,s(',d i;o estimateth(; parameters (it" the n-grmn lnodc.l.
\?
(~ nowdes('ribe the estinmtion l)roec,,~s in (l('%ail.Table 2: Chm'acteristics of th(: data.
s('A;s (h~iinedfor the eXl)eriments wh(',n the senl:en(:(~s wi(;hmore l;lmn 15 l)OSl;ags were r(;moved.Da.ta \[ No.
of I Av.
Std.
\]l(,ngl h deviation S(~, J; SellI;ell.
\ ] _ .~l.i;st .
2,295 1 .1~ 3.55The 1)a.rmn(%er,q of a 3-grmn too(l(;1 were ('~s-t imatcd with the softw~re tool des('rit)('.
(1 in(l/,osenfehl, 1995) :t. W(~ u,qed tlm linear ini;(',rl/o-la.tion ~qmooth t('~(:lmiqu(~ SUpl)orted by ,;hi,~ tool.Th('~ o1:l;-of-v(/(:al)lflary words wcr('~ groul/e(l inthe same (:\]as,~ and w(u'e used in th(~ ('omt)ula-1;ion of i;\]~('~ perl)h~'xity.
~.l'h(,.
I:(~sl ,~(:I l)(~rl)l('~xitywith t:his mo(lel was 180.4.T\]w, values ()f ('.xt)r(~,qsi()n (3) wure (:()ml)ut(!
(tfrom the t~:.gged and l:on-l:agg(:(t i)alq; O1' \[;lie,l:raining corpus.
In or(h'a' to avoid mill val-ues, the m~seen (~'vents wer('~ lal)ele(1 with ~ Sl)C-cial symbol 'w' wlfich did not a pl)ear in (;hei ,  s.,:h -?
0,Vc C (/, whtu'(~ (/ was I:h('~ set ()f (:at('.g()ri(~s.That  is, all th(: ('at(',gori(~s could g(',n(:rat(', i;\]:(: uu--,q('x'~n evenI, This l)rolml)ility took a. v(~ry smallvalu(; (s(',v('.ral ()rd(;rs of magnii;u(l('~ h'~,qs tlmnminw~v,c(:c l)r('wlc), where V was the \.
'o('alm-lary of the tra.ining corpus), and (liffer(mI; vahte.~of this i)robability did not chang('~ tlm r(~sults.The i)aramet('~rs of an initial ergodic SCFGwere estimated with each one of the estimationmethods mentioned in Se('tion 3.
This SCFGhad 3,374 rules, (:omt)osed fl'om 45 terminalsyml)ols (the numl)er of l)()Stags) and \]d non-terminal symbols (the nmnber of synl;a('l;i(: la-bels).
The prol)z~l)ilitics were rmidolnly gem'a'-ated mid t;hree different seeds were tested, lintonly one of them is reported given that the re-suits were very similar.
The training (:orlms wasthe labele(l part of the des('ril)ed (:orlm,q.
The1)erl)lexity of the labeled part of the test; s(:t forall.clcas(~ 2.04 is availal)le at htl;l)://svr-www.cng.cmn.ac.uk/~ 1)rcl4/toolkit.html.diti'(n'('~nt (~stimation algorithms (;a.n l)c. ,~cen inTal)le 3.\[I.
'abl(~ 3: PCxl)lexity of the labeled 1)?tl't; Of I;\]lOtest set with the SCFC, est imated with themethods mentioned in Section 3.7 1 /'~ vs l a,\ s l ,ot, I \  sb IOnce we had est imated the lmramei;ers of thedefined model, we applied expression (1) 1)y us-ing the IAI,\] algorithm m~d the VI.\[/,I algorithmin ('~xt)w, ssion (d).
Th(; test set lWa'l)lexitly thatwas ol)I;ained in flmction t)f (t %r difl't'a'(:nt; esti-nm~tion algorithms (VS, kVS, lOb mid VSb) canbe seen in \]rig.
1.In the best case, the tn'ot)osed l~mguage modelel)rained more than a.
30% inlI)rOVellle:l|; OVerre,~ults ol)taincd 1)y the 3-gram lmlguagc motM(s(w. ~l'at)le d).
This result wa,q ol)t;ainc.d wh('~nth(: SCFG usl;imat(~d with the lOb algorit;hmwa,~ u,~(;(1.
The SCFGs ('.stimat('.
(l with ()ther al-g()ril;hms also ()l)tain('.d iml)ortanl; ilnlirovt,.nw.lfl;,~(:Oinl)ar(;d l;o \[;he 3-grain.
In ~(t(lition, ii; can beoliserv('.d i;hat t)oth th(,.
LI-(.I algorithm and theVIA/I algoril;hm obtained good results.Tal)le d: \]3(~,st tx~st lW, rl)lexity for difl'(~,rem; SCFG(:.%imation algori thins, and I;h(~ \])er(:cntage of im-i)rovt'.mc~lll; with resi)(wl; I;o i;hc 3-gram model.VSm kVS lOb VSI)\]All 133.6 130.3 124.6 136.3i % improv.
25.9% 27.8% 30.9% 24.5%\[ V\]~l:l I ~ 137.2 I 13Z4 I V,9.7 I\[ %iml)rov.
20.5% 23.0% 26.6% 17.0%An important aspect to note is (;hat theweight; of the grmmnatical part was approxi-mat;ely 50%, which means that this part pro-vided iml)ori;mlI; inform~tion to the languagemodel.6 Conc lus ionsA nc'w language model has been introduced.This new language model is detined as a~ lin-ear ('olnl)in~ttion of an n-gram which repre-s(mts relations betwe('~n words, and a stochastic5920O190180>"~ 17(116015014(\]13(\]12020O19(\]180'~ 17(\]16015(\]14(1130120;,, /l:j~3-ffrgtn:VSbkVS VSIOb0 3 0 4 0.5 0.6 0.1 0.2~/3~gt'anlVSbVSkVSlOb0.7 0,8 0.9/0.1 0 2 0.3 0.4 0.5 0.6 0 7 0.8 0.9 0 (tFigure 1: Test set perplexity obtained withthe proposed language models in function ofgamma.
Different curves correspond to SCFGsestimated with different algorithms.
The up-per graphic correst)onds to the results obtainedwhen the LRI algorithm was used in the lan-guage models, and the lower graphic corre-sponds to the results obtained with the VLRIalgorithm.grammatical model which is used to representthe global relation between syntactic structures.The stochastic graminatical model is composedof a category-based SCFG and a probabilisticmodel of word distribution in the categories.Several algorithms have been described to esti-mate the parameters of the model flom a thesmnple.
In addition, efficient algorithms tbrsolving the problem of the interpretation withthis model have been presented.The proposed model has been tested on thepart of Wall Street .Journal processed in thePenn Treebank project, and the results obtainedimproved by more tlmn 30% the test set; per-plexity over results obtained by a simple 3-grainmodel.Re ferencesF.
Amaya, J.M.
Benedi, and J.A.
Shuchez.1999.
Learning of stochastic context-freegrammars from bracketed corpora by meansof reestimation algorithms.
In M.I.
Torresand A. Sanfeliu, editors, Proc.
VIII SpanishSymposium on Pattern Recognition and Im-age Analysis, pages 119 126, Bilbao, Est)afia,May.
AERFAI.L.R.
Bahl, F. Jelinek, and ILL. Mercer.
1983.A maximmn likelihood approach to continu-ous speech recognition.
IEEE Trans.
PatternAnalysis and Machine Intelligence, PAMI-5(2):179 190.J.R.
Bellegarda.
1998.
A multispan languagemodeling frmnework tbr large vocabularyspeech recognition.
IEEE Trans.
Speech andAudio Processing, 6(5):456-476.C.
Chelba and F. Jelinek.
1998.
Exploiting syn-tactic structure for lm~guage modeling.
InProc.
COLING, Montreal, Canada.
Univer-sity of Montreal.J.
Gilet and W. Ward.
1998.
A language modelcombining trigrams and stochastic ontext-fl'ee grammars.
In In 5th International Con-.fercnce on Spoken Language Processing, pages2319 2322, Sidney, Australia.F.
Jelinek and J.D.
Lafferty.
1991.
Coml)uta-tion of the probability of initial substring en-eration by stochastic ontext-free grammars.Computational Linguistics, 17(3):315 323.F.
Jelinek.
1998.
Statistical Meth, ods for SpeechRecognition.
MIT Press.K.
Lari and S.J.
Young.
1990.
The estimationof stochastic ontext-fl'ee grmnmars using theinside-outside algorithm.
Computer, Speechand Language, 4:35 56.M.P.
Marcus, B. Santorini, and M.A.Marcinkiewicz.
1993.
Buikting a large anno-tated corpus of english: the penn treebank.Computational Linguistics, 19 (2):313-330.H.
Ney.
1992.
Stochastic grmmnars and patternrecognition.
In P. Laface and R. De Mori, ed-itors, Speech Recognition and Understanding.Recent Advances, pages 319 344.
Springer-Verlag.F.
Pereira and Y. Schabes.
1992.
Inside-outside reestimation from partially brack-60cted corporm In Pwcccding,~' of the 30th An-n'ual A4ectin9 of the As,~ociatiou for Comp'uta-tional Linguistics, 1)ages 128 135.
Universityof l)elawarc.ll,.
Roscnfl'hl.
1.995.
Th(' cmu sta.tisl:i(:al an-guage mo(Ming toolkit and its use in the 1994art)a csr evaluation.
In ARPA Spoken Lan-guage Technology Workshop, Austin, Texas,USA.M.
Siu and M. Osto.ndorf.
2000.
\;al'iablcn-grams mid (;xl;(~.n,siolLs for convcrsatiomtlspeech langu~g(' mo(h;ling.
IEEI~' Tm,',,s.
onSpeech and A'udio P'roc(;s.sing, 8(1) :63 75..I.A.
Sfi.nch(;z and ,J.M.
B(;ncdf.
11997.
()Olnl)llta-tion of the probability of the best (hwiw~t;ion ofan initial substring fi'om a stochastic (:ontcxt-fl'ec; grammar.
In A. Smffeliu, .\]..l. Villa.mleVa,and .J.
\;itri;t, editors, Prm:.
VII Spanish, Sym-posi'um, on Pattern Rccogu, itio'n and imageAnalysi.
L pages 181-186, Barco.hma., Eslmfia,April.
AERFAI.J.A.
SSn(:h?
;z mid J.M.
Bcn('(lf.
1998.
Esti-mation of the l)robability di,~tributions ofsto(:ha,sti(: (;oni;('~xt-frc(; grammar,s from th(; \],:-1)(;st derivations.
In b~, 5th, b~,ter'national Co'n-.f('r(:nc(: on Spoke',, Langua9(: l)'ro(:c,ssing, imgc,s2d95 2498, Sidney, Australia..J.A.
Sgmchcz mid ,\].M.
Bcn(;(li.
1999. lx~rn-ing of ,%ochast, ic cont(~xt-fl'(~'c grammm's bymemos of ('stima.tion ~flgol'ithms.
In 1)'roe.
EU-H, OSl)EECl\]'99, volume 4, lmges 1799 1802,Budal)cSt , Hungary.J.A.
S~nchez, J.M.
B(;nedf, and F. Casacu-berta.
1996.
Comparison lmi,ween the insi(h~-outside algorithm mid the vitcrl)i a.lgorithmfor stochastic context-fl'ee grammars.
InP.
Perncr, P. Wang, amd A. I{osenfe.ld, edi-tors, Advances in Str'uct'm'al and SyntacticalPattcrn ll, ccogu, ition, pages 50 59.
Springer-Verlag.61
