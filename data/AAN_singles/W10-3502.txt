Proceedings of the 2nd Workshop on ?Collaboratively Constructed Semantic Resources?, Coling 2010, pages 10?18,Beijing, August 2010Using the Wikipedia Link Structure to Correct the Wikipedia LinkStructureBenjamin Mark PatemanUniversity of KentEnglandbmp7@kent.ac.ukColin JohnsonUniversity of KentEnglandC.G.Johnson@kent.ac.ukAbstractOne of the valuable features of any col-laboratively constructed semantic resource(CSR) is its ability to ?
as a system ?
con-tinuously correct itself.
Wikipedia is an ex-cellent example of such a process, with van-dalism and misinformation being removedor reverted in astonishing time by a coali-tion of human editors and machine bots.However, some errors are harder to spotthan others, a problem which can lead topersistent unchecked errors, particularly onmore obscure, less viewed article pages.
Inthis paper we discuss the problems of incor-rect link targets in Wikipedia, and proposea method of automatically highlighting andcorrecting them using only the semanticinformation found in this encyclopaedia?slink structure.1 IntroductionWikipedia, despite initial scepticism, is an incredi-bly robust semantic resource.
Armed with a sharedset of standards, legions of volunteers make posi-tive changes to the pages of this vast encyclopae-dia every day.
Some of these editors may be ca-sual ?
perhaps noticing an error in a page they werereading and being motivated to correct it ?
whileothers actively seek to improve the quality of awide variety of pages that interest them.
Facilitatedby a relatively minimalist set of editing mechan-ics and incentives, Wikipedia has reached a statein which it is, for the most part, a reliable and sta-ble encyclopaedia.
Just enough regulation to pre-vent widespread vandalism or inaccuracy (includ-ing, on occasion, the temporary locking of particu-larly controversial pages), and enough editing free-dom to maintain accuracy and relevance.There are a number of potential approaches tominimizing misinformation and vandalism, fallinginto two broad categories: adding human incen-tives, and creating Wiki-crawling bots.
There al-ready exists a wide variety of natural and Wiki-based incentives (Kuznetsov, 2006) that have beencrucial to the encyclopaedia?s success.
By imple-menting additional incentives, it may be possibleto, for example, increase editor coverage of less-viewed articles.
There are many avenues to exploreregarding this, from additional community featuressuch as a reputation system (Adler and de Alfaro,2007), to ideas building upon recent work relat-ing to games with a purpose (von Ahn, 2006), pro-viding a form of entertainment that simultaneouslyaids page maintenance.Wikipedia also benefits from a wide variety ofbots and user-assistance tools.
Some make the livesof dedicated editors easier (such as WikiCleaner1),providing an interface that facilitates the detectionand correction of errors.
Others carry out repeti-tive but important tasks, such as ClueBot2, an anti-vandalism bot that reverts various acts of vandalismwith surprising speed.
Similar bots have been ofgreat use in not only maintaining existing pages butalso in adding new content (such as RamBot3, a botresponsible for creating approximately 30,000 U.Scity articles).In recent years, researchers have taken an in-creasing interest in harnessing the semantic datacontained in Wikipedia (Medelyan et al, 2009).To this end, the encyclopaedia now serves as notonly a quick-lookup source for millions of peopleacross the world, but also as an important semanticresource for a wide range of information retrieval,natural language processing and ontology buildingapplications.
With all this utility, it is increasinglybeneficial for Wikipedia to be as accurate and reli-able as possible.In this paper, we will discuss an algorithm thataims to use Wikipedia?s inherent link structure todetect and correct errors within that very same1https://launchpad.net/wikicleaner2http://en.wikipedia.org/wiki/User:ClueBot3http://en.wikipedia.org/wiki/User:Rambot10structure.
In Section 2 we will explore the natureand causes of this error, outlining the motivationsfor our algorithm.
Section 3 discusses the inspi-rations for our approach, as well as our reasonsfor choosing it.
We will then describe its methodin detail, before evaluating its effectiveness andanalysing its strengths and weaknesses.2 A Reliable Encyclopaedia?It?s the blind leading the blind ?
infinite monkeysproviding infinite information for infinite readers,perpetuating the cycle of misinformation and ig-norance?
(Keen, 2007).
There has been much de-bate over the value of Wikipedia as a reliable en-cyclopaedia.
Fallis (2008) talks at length about itsepistemic consequences, acknowledging these crit-icisms but ultimately reaching a positive conclu-sion.
In particular, he emphasizes the merits ofWikipedia in comparison with other easily accessi-ble knowledge sources: If Wikipedia did not exist,people would turn to a selection of alternatives forquick-lookups, the collection of which are likely tobe much less consistent, less verifiable and less cor-rectable.The fallacies of Wikipedia come from twosources: disinformation (an attempt to deceive ormislead) and misinformation (an honest mistakemade by an editor).
These can exist both in the tex-tual content of an article, as well as the structuralform of the encyclopaedia as a whole (e.g.
the linkstructure or category hierarchy).
The consequencescan be measured in terms of the lifespan of sucherrors: a fairly harmless issue would be one thatcan be noticed and corrected easily, while those thatare harder to detect and correct must be consideredmore troublesome.For this reason, to be more potent on less fre-quently visited pages, as mentioned in Section 1.However, (Fallis, 2008) argues that ?because theydo not get a lot of readers, the potential epistemiccost of errors in these entries is correspondinglylower as well?, suggesting that a balance is struckbetween misinformation and page traffic that stayssomewhat consistent across all traffic levels.
Whileinaccuracies may linger for longer on these less vis-ited pages, it follows that fewer people are at risk ofassuming false beliefs as a result.An interesting pitfall of Wikipedia pointed outby Fallis (2008) comes as a result of the nature ofits correctability.
As readers of any piece of writ-ten information, certain factors can make us lesstrustworthy of its content; for example, grammat-ical or spelling mistakes, as well as blatant false-hoods.
However, these are the first things to be cor-rected by Wikipedia editors, leaving what appearsto be ?
on the surface ?
a credible article, but poten-tially one that embodies subtle misinformation thatwas not so quickly rectified.2.1 Ambiguous DisambiguationsIt is therefore important that methods of detect-ing and resolving the not-so-obvious inaccuraciesare developed.
One such not-so-obvious error canoccur in Wikipedia?s link structure.
This prob-lem stems from the polysemous nature of language(that is, that one word can map to multiple differ-ent meanings).
In Wikipedia, different meaningsof a word are typically identified by adding addi-tional information in the relevant page?s name.
Forexample, the article ?Pluto (Disney)?
distinguishesitself from the article ?Pluto?
to avoid confusionbetween the Disney character and the dwarf planet.Adding extra information in brackets after the arti-cle name itself is Wikipedia?s standard for explic-itly disambiguating a word.
Note that the article onthe dwarf planet Pluto has no explicit disambigua-tion, because it is seen as the primary topic for thisword.
In other cases, no primary topic is assumed,and the default page for the word will instead leaddirectly to the disambiguation page (for example,see the Wikipedia page on ?Example?
).This system, while effective, is susceptibleto human error when links are added or mod-ified.
The format for a link in WikiTextis: ?
[[PageName | AnchorText]]?
(the an-chor text being optional).
It is not hard to imag-ine, therefore, how a slightly careless editor mightattempt to link to the article on Pluto (the Disneycharacter) by typing ?
[[Pluto]]?, assuming thatthis will link to the correct article, and not some-thing completely different.Is ?Jaguar?, generally the name of a fast feline,more likely to make you think of cars?
?Python?is a genus of snake, but also a programming lan-guage to those involved in software development.Apple, a common fruit, but to a lot of people willbe heavily associated with a well-known multina-tional corporation.
These examples suggest thatwhen a word takes on a new meaning, this newmeaning ?
as long as it remains relevant ?
can be-11come more recognizable than the original one (asyet another example, consider how your reaction tothe word ?Avatar?
fluctuated in meaning as JamesCameron?s film went by).
One particular potentialproblem is that someone editing an article will befocused on the context of that particular article, andwill therefore be likely to not consider the poly-semous nature of a word that they are using.
Forexample, someone editing the article on the Ap-ple iPad will have the company name Apple promi-nently in their mind, and therefore may momentar-ily forget about the existence of a particular kind ofsmall round fruit.The effects of these blunders can vary greatly de-pending on the word in question.
For example, justabout anyone who ?
expecting to be directed to apage on a Disney character ?
instead finds them-selves at a page about a well-known dwarf planetin our Solar System, is going to know that thereis an error in the source article.
In this example,then, the error would be fixed very quickly indeed?
faster still if the source page was popular (suchas the article on Disney itself).
However, there arecases where linking to the wrong sense of a poly-semous word may not be as obvious an error for alot of users.
Someone following a link to ?Jagu?ar?
(the band) is less likely to notice a mistake if they?retaken to the incorrect page of ?Jaguar (band)?
(adifferent band) than if they?re taken to the incor-rect page ?Jaguar?
(the feline).
We argue that theextent of this problem depends on the difficulty ofdistinguishing between two different meanings ofthe same word.
This difficulty is based upon twofactors: the reader?s level of background knowl-edge about the expected article, and the semanticsimilarity between it and the incorrect article beinglinked to.
If the reader has absolutely no knowl-edge concerning the subject in question, they can-not be certain that they are viewing the correct pagewithout further investigation.
Furthermore, a readerwith some relevant knowledge may still be unawarethat they have been taken to the wrong page if theincorrectly linked-to page is semantically very sim-ilar to the page they were expecting.
If these arecommon responses to a particular pair of polyse-mous articles, then it follows that a link error con-cerning them is likely to persist for longer withoutbeing corrected.3 The Semantic Significance ofWikipedia?s Link StructureWikipedia consists of, for the most part, unstruc-tured text.
Originally constructed with only the hu-man user in mind, its design makes machine inter-pretations of its content difficult at best.
However,the potential use of Wikipedia in a wide range ofcomputational tasks has driven a strong research ef-fort into ways of enriching and structuring its infor-mation to make it more suitable for these purposes.For example, DBpedia4 takes data from Wikipediaand structures it into a consistent ontology, allow-ing all its information to be harnessed for variouspowerful applications, and is facilitating efforts to-wards realizing a semantic web (Bizer et al, 2009).At the same time, research has also been car-ried out in ways of making use of the exist-ing structure of Wikipedia for various natural lan-guage processing applications.
For example, Shon-hofen (2006) proposed using the hierarchical cate-gory structure of Wikipedia to categorize text doc-uments.
Another example of a system which makesuse of word-sense disambiguation in the contextof Wikipedia is the Wikify!
system (Mihalcea andCsomai, 2007), which takes a piece of raw textand adds links to Wikipedia articles for significantterms.
One of the biggest challenges for the authorsof that system was linking to polysemous termswithin the raw text.
A combination of methods wasused to determine the best disambiguation: over-lap between concepts in the neighbourhood of theterm and dictionary definitions of the various possi-ble link targets, combined with a machine learningapproach based on linguistic features.In this paper we are concerned with anothermethod of using Wikipedia without prior modifi-cations: exploiting the nature of its network oflinks.
This approach was pioneered by Milne andWitten (2007; 2008a; 2008b), responsible for de-veloping the Wikipedia Link-Based Measure, anoriginal measure of semantic relatedness that usesthe unmodified network of links existing withinWikipedia.Indeed, the link structure is one of the few ele-ments of Wikipedia that can be easily interpretedby a machine without any restructuring.
It containswithin it informal ?
often vague ?
relationships be-tween concepts.
Whereas, ideally, we would like to4http://dbpedia.org/12be dealing with labelled relationships, being ableto directly analyse collections of untyped relation-ships is still very useful.
Importantly, however, wemust not concern ourselves with the significanceof a single link (relationship), due to its class be-ing unknown.
In an article there may be links thatare more significant ?
semantically speaking ?
thanothers, but this information cannot be retrieved di-rectly.
For example, the article on a famous singermight have a link to the village in which she grewup, but this is arguably ?
in most contexts ?
lesssemantically significant than the link to her first al-bum, or the genre that describes her music.Instead, then, we would like to look at collec-tions of links, as these loosely summarize seman-tic information and de-emphasize the importanceof knowing what type of relationship each link, in-dividually, might express.
Every single page onWikipedia can be seen as a collection of links inthis way; ignoring the raw, unstructured text withinan article, we are still able to determine a great dealabout its meaning just by looking at the underlyinglink structure.
In doing this, comparing the simi-larity of two articles is as simple as comparing theoutgoing links that each has.
The more outgoinglinks that are common between the two articles, themore similar we can gauge them to be.Looking at the links pointing to an article alsoprovides us with additional cheap information.
Ofparticular interest is deriving an estimated ?com-monness?
of a concept by counting the number oflinks pointing in to it.
The Wikipedia Link-BasedMeasure uses this information to weight each link,giving additional strength to links that have a lowerprobability of occurring.
This accounts for the factthat two articles are less likely to share uncommonlinks; if they do, then this link overlap accounts fora higher degree of similarity.
Conversely, two arti-cles sharing a very common link (such as a page ona country or capital city) should not be consideredvery similar on that fact alone.The motivations behind taking this approach forour link checking algorithm come largely from theinexpensive nature of this measure.
While a largeamount of potential information is ignored ?
suchas the content of an article itself ?
the computa-tional cost is an order of magnitude lower, andminimal preprocessing is required.
With the En-glish Wikipedia consisting of several million pages,and the search for incorrect links being essentiallyblind, processing speed is an important factor inproviding useful page coverage.4 Detecting Incorrect LinksThe detection of incorrectly targeted links inWikipedia is a trial of semantics; by estimating howsimilar in meaning a linked page is to the theme ofan article, we can determine whether there mightbe an alternative page that would be more suitable.In finding significantly more suitable alternatives tothese semantically unrelated links, we are able tohypothesise that the original link was incorrect.
Inthe following subsections, we will describe the de-tails of this algorithm.4.1 Preparing the DatabaseSnapshots of Wikipedia can be downloaded fromits database dump page5, and then loaded into a lo-cal database.
While this database is used by thealgorithm, the practicality of such an applicationdemands that live Wikipedia pages be used as theinput.
Checking a week old snapshot of Wikipediafor incorrect links will be less effective, as a num-ber of them may well have been already fixed onthe website itself.
For this reason, the algorithm ac-cepts a URL input of the page to be analysed, andwill extract its current links directly.4.2 Determining the Theme of an ArticleThe first step is to compute the semantic theme ofthe original article in question.
This is done us-ing an algorithm loosely based on that of Milne andWitten (2008a), which was discussed in section 3.To begin with, the original article is arranged as alist of linked pages (pages that it links directly to).Each of these derived pages is considered as a se-mantic ?concept?.We represent each concept as a further list of itsoutgoing page links, creating a wide tree structureof depth 2, with the original article at the root (seeFigure 1).
The theme of this article is determinedby propagating link information up the tree, essen-tially consolidating the individual themes of each ofits concepts.
As new links are discovered, they areassigned a commonness weighting (see section 3),and multiple encounters with the same link are tal-lied.
For each link, this information (the common-ness rating and link frequency) is used to sculpt the5http://dumps.wikimedia.org/backup-index.html13Figure 1: A simplified link structure diagram for the article on ?JProfiler?.overall theme of the article.4.3 Semantic ContributionWe use the phrase ?semantic contribution?
to de-scribe how much meaning a particular concept?contributes?
to the theme of the article in ques-tion.
This is based on the nature of each of its linksand how frequently they occur amongst the rest ofthe article?s concepts.
We therefore quantify the se-mantic contribution of a given concept by using theformula:Sc =n?l=1{log(fl)wl if fl >= 20 if fl < 2In other words, for each link l with a frequency(f , the number of times this link appears across allconcepts) of 2 or more, its semantic contributionis a product of its frequency and its weight (w), asdefined by:w = 1log(il + 1)Where il is the total number of incoming links(Wikipedia-wide) pointing to the same target aslink l. The total semantic contribution of a conceptis the summation of all of the contributions of itsoutgoing links.
By quantifying each concept in thismanner, we can immediately see which conceptscontribute a lot, and which contribute very little, tothe theme of an article.4.4 Extracting Dissimilar LinksWith an aggregated theme established for an article,it is a simple task to flag up those concepts that havea low semantic contribution.
Due to how semanticinformation was propagated up the tree (see the pre-vious section), each concept represents some subsetof the article?s theme.
Qualitatively speaking, thisessentially equates to looking at how much of itstheme overlaps with the most accentuated aspectsof the article?s theme.
The dominant features of anarticle?s theme will come from those links that areuncommon and frequently occurring, so any con-cept that consists of a good number of these linkswill be have a high semantic contribution.By scoring each concept in terms of its contri-bution to the article theme, we are able to exam-ine those concepts that scored particularly low.
Thevalue to use as a threshold for flagging potential er-rors is somewhat arbitrary, but in our experimentswe have found best results using a simple variablethreshold:Threshold = average contribution2Any concepts with a semantic contribution belowthis value are considered as candidate errors, al-though it?s important to note that, in many cases,a perfectly valid link can have a low contribution.For example, a link from a famous film director to acountry he once filmed in.
In these cases, however,we expect that it is unlikely for a more relevant al-ternative to be found.4.5 Finding Possible AlternativesWith one or more potentially incorrect links found,the algorithm must now search for alternative tar-gets that are more suitable.
This method is built onthe assumption that the link is in error due to point-ing towards the wrong disambiguation, accountingfor the typical scenario of an editor linking to thewrong sense of a polysemous word.An editor who has accidentally pointed to the ar-ticle ?Pluto?
rather than ?Pluto (Disney)?
has notmade any spelling errors.
As we discussed in Sec-tion 2.1, the error is typically a result of a presump-tion being made on the most typical meaning of14the target article.
With this in mind, an error ofthis nature is likely to be resolved by looking atother articles that share the same name.
There are anumber of ways to do this, such as simply search-ing the database for all articles containing the word?Pluto?.
However, we chose instead to locate therelevant disambiguation page, if it exists (in this ex-ample, ?Pluto (disambiguation)?).
For the type oferror we are targeting, this disambiguation page canbe expected to contain the correct, intended page asone of its outgoing links.4.6 Choosing the Best AlternativeWith a list of possible alternatives for a particularweakly related concept, we then go about calculat-ing their potential semantic contribution to the orig-inal article (using the same formula as was seen insection 4.4.
To continue the example, the semanticcontribution of ?Terry Pluto?
is unlikely to be at allhigh when considering the original article on Dis-ney.
The same goes for other possible alternatives,such as ?Pluto (newspaper)?
or ?Pluto Airlines?.However, the concept ?Pluto (Disney)?
contributesconsiderably more than the original link, and thisbecomes evidence to suggest it as a likely correc-tion.For each plausible alternative, a score is assignedbased on the increased semantic contribution it pro-vides over the original link.
By doing this, the sug-gestions can be ordered from best to worst, express-ing a degree of confidence in each option.5 EvaluationWe evaluated the effectiveness of this algorithm bytesting it on a snapshot of Wikipedia from Novem-ber 2009.
By using old Wikipedia pages we can,in most cases, easily validate our results against thenow-corrected pages of live Wikipedia.
However,finding examples of incorrectly linked articles is nosimple task.
Indeed, much of the justification forthe algorithm this paper describes stems from thefact that finding these incorrect links is not easy,and actively searching for them is a somewhat te-dious task.
While we would like to leave our scriptcrawling across Wikipedia detecting incorrect linksby itself, in order to evaluate its performance weneed to evaluate how well it performs on a set ofpages that are known to contain broken links.
Itis impossible to generate such a set automatically,as by their nature these broken links are concernedwith the meaning of the text on the pages.We gauge the performance of our algorithm bylooking at how many of the ?best?
suggestions(those with the highest calculated semantic contri-bution) given for a particular link are, in fact, cor-rect.5.1 Gathering Test DataWe found that a satisfactory method for finding in-correct links was to examine the incoming linkspointing to a particularly ambiguous page.
How-ever, pages can have hundreds or thousands of in-coming links, so we need to choose ones that arelikely to be linked to in error, using ideas discussedin section 2.1.
For example, if we look at the longlist of links pointing towards the article ?Jaguar?,we will mostly see articles relating to the animal:geographical locations, ecological information orpages concerning biology, for example.
If, amongthese pages, we notice an out of place page ?
re-lating, perhaps, to cars, racing or business ?
wehave reason to believe this article was supposed tobe linking to something different (most likely, inthis case, ?Jaguar Cars?).
After basic investiga-tion we can confirm this and add it to our collec-tion of pages for evaluation.
While still not fast byany means, this method is considerably more effec-tive than randomly meandering around the pages ofWikipedia in search of link errors.
For this eval-uation, we used the first 50 error-containing pagesthat were encountered using this method.Another potentially effective method would be todownload two chronologically separate snapshotsof the Wikipedia database (for example, one takena week before the other).
We could then comparethe incoming links to a particular article across bothsnapshots: If there are more incoming links in thenewer snapshot than the old, then we can attempt tofind them in the older snapshot and check their out-going links.
For example, the new snapshot mighthave a link from the article ?Jim Clark?
to ?JaguarCars?
that does not exist in the old snapshot.
Uponchecking the old snapshot?s version of the page on?Jim Clark?, we see it has a link to ?Jaguar?
andhave immediately found a suitable error.
This en-ables us to quickly find links that have been re-paired in the time between the two snapshots, pro-viding a fast, systematic method of gathering testdata.15Nonetheless, finding a substantial set of exam-ples of incorrectly linked pages is a significant chal-lenge for work in this area.
It is an important task,however, as without such a set it is impossible todetermine a number of important features of a pro-posed correction algorithm.
Firstly, without such aset it is impossible to determine which wrongly al-located links have been ignored by the algorithm,which is an important measure of the algorithm?ssuccess.
Secondly, determining whether the algo-rithm has suggested the correct link requires thatthese correct links have been specified by a humanuser.
As a result, the development of a substantialdatabase of examples is an important priority forthe development of this area of work.5.2 DiscussionOverall, the results (given in Table 1) show that thealgorithm performs well on this test set, with thebest suggestion being the correct choice in 76.1%of cases.As expected, the algorithm works best on largerarticles with a well-established theme.
For ex-ample, the articles on ?Austin Rover Group?
and?CyberVision?
were riddled with links to incorrectpages, but with a total of 194 and 189 outgoinglinks respectively, there was sufficient informationto confidently and accurately find the most suitablecorrections, despite the number of errors.
Con-versely, ?Video motion analysis?, with only 7 out-going links, fails to form a strong enough theme toeven be able to highlight potential errors.One might argue that the accurate result for thearticle on ?Synapse (disambiguation)?
is somewhatof an anomaly.
Being a disambiguation page, thereis inherently no common theme; typically, each linkwill point to a completely different area of seman-tic space.
Correctly repairing the link to ?Java?comes as somewhat of a coincidence, therefore, andit should be noted that disambiguation pages are notsuited to this algorithm.
Conversely, due to the na-ture of disambiguation pages, we might assume thatusers editing them are ?
in general ?
more carefulabout the target of their links, minimizing the oc-currence of these sorts of errors.There is a unique limitation with the algorithmthat these results clearly highlight, however.
An ex-ample of this lies in the results from programming-themed pages dealing with the link to ?Java?
:There are a handful of recurring concepts be-ing suggested, such as ?Java (programming lan-guage)?, ?Java (software platform)?
or ?Java Vir-tual Machine?.
These suggestions are often accom-panied by very similar values of semantic contribu-tion, simply because they are all very semanticallyrelated to one another.
As a result, if the theme ofan article is related to one, it will be typically be re-lated to them all.
Which is the correct choice, if allare semantically valid?
The one that fits best withthe context of the sentence in which it is found.This reveals an important limitation of this algo-rithm, in that the position of links within the text ?and the surrounding text itself ?
is completely un-known to it.
Dealing only with what is essentiallya ?bag of links?, there is no information to discernwhich article (from a selection of strongly relatedarticles) would be most appropriate for that partic-ular link to point to.
Indeed, in these isolated caseswe observed the algorithms accuracy drop to 47%,although it should be noted that in almost all casesthe correct link was suggested, just not as the bestchoice.6 ConclusionThe results of our evaluation not only display theeffectiveness of this algorithm at detecting and cor-recting typical link errors, but also clearly markits limitations when dealing with multiple seman-tically similar suggestions.
When considering theimpact of these limitations, however, we must notforget that the algorithm was still able to recognizean invalid link, and was still able to offer the correctsolution (often as the best choice).
The impacts,then, are just on the consistency of the best choicebeing correct in these situations.
However, the aimof this work was to build an algorithm that can be ofsignificant assistance to a human editor?s efficiency,and not to replace the editor.
With that in mind, theoutput of the algorithm provides enough informa-tion to enable the editor to promptly pick the mostappropriate suggestion, based on their own judg-ment.While carrying out the evaluation on these 6month old Wikipedia pages, we checked the resultsagainst the live pages of Wikipedia.
A surprisinglylarge number (as many as 40%) of errors found hadyet to be corrected half a year later, which, ulti-mately, is highly indicative of the potential bene-fits of this utility in repairing the errors that nobodyknew existed.16Table 1: Counts of the correct link being given as the best suggestion.Page Name Best Correct Page Name Best CorrectAcropolis Rally 2/2 JProfiler 0/1Austin Rover Group 6/6 KJots 0/1Barabanki district 2/2 Lady G 0/1Batch file 0/1 List of rapid application development tools 3/3Belong (band) 1/1 Video motion analysis 0/1Comparison of audio synthesis environments 3/4 Logo (programming language) 1/3Comparison of network monitoring systems 2/3 Maria Jotuni 0/1Computer-assisted translation 0/1 Mickey?s delayed date 1/1Convention over configuration 1/1 Neil Barret (Fashion Designer) 1/1CyberVision 18/21 Ninja Gaiden (Nintendo Entertainment System) 2/3Daimler 2.5 & 4.5 litre 1/2 Planetary mass 1/1Dance music 3/3 Population-based incremental learning 1/1Deiopea 1/1 Streaming Text Oriented Messaging Protocol 1/2David Permut 3/3 Spiritual Warfare (video game) 1/2Demon (video game) 1/1 Sonic Heroes 1/1Disney dollar 1/1 Soulseek Records 2/2DJ Hyper 1/1 Synapse (disambiguation) 1/1DJ Qbert 1/2 Tellurium (software) 2/2Eliseo Salazar 1/2 Testwell CTC++ 1/1Fixed point combinator 0/1 The Flesh Eaters (band) 3/3Gravity Crash 1/1 Trans-Am Series 3/4Hyphenation algorithm 2 Ultima IV: Quest of the Avatar 1/1IBM Lotus Notes 1/2 Uma Thurman 4/6Jaguar XFR 2/2 Unlabel 1/1Jim Clark 0/1 Virtual World 1/2Total: 86/1137 Further WorkIn continuing this work, there are a number of av-enues to explore.
Fundamentally, there is room tofine tune various aspects of the algorithm, such asthe threshold value used to determine candidate er-rors, or the relationship between a link?s frequencyand its commonness.
In doing so we might in-clude additional variables, in particular investigat-ing how the size of an article affects the algorithm,or the distribution of a central theme amongst itsconcepts.Additionally, there is work to be done on con-structing a practical application from this; adding,for example, an accessible GUI as well as directWikipedia integration to allow for users to easilycommit corrected links to the Wikipedia server it-self.
This could lead to a further evaluation stepin which we analyse the effectiveness of these cor-rections after the system has been running ?in thewild?
for a number of months.
In order to usethis system to correct the live Wikipedia it wouldbe important to have an up-to-date local copy ofWikipedia in order to rapidly access the up-to-datelink structure.As mentioned earlier, an important challengefor the accurate evaluation of systems of this kindwould be the development of a substantial, anno-tated database of examples of this kind of brokenlink.
Clearly, it is difficult for a single develop-ment team to curate such a database, as the discov-ery process is time consuming.
One approach tothis would be through some form of crowdsourc-ing effort to gather a large number of examples.This could be as simple as encouraging readers ofWikipedia to report such corrections, for exampleby using a specific keyword in the revision notesmade on that change.
A more sophisticated ap-proach could be to draw on the concept of gameswith a purpose (von Ahn, 2006), as exemplifiedby the Google Image Labeler6 which uses a two-player game to find new tags for images.
A gamecould be created based on the notion of present-ing the user with a choice of links for a particu-lar Wikipedia page, and rewarding them when theyagree with another user on a target that is not cur-rently pointed at by that link.One further useful measure would be to devise abaseline algorithm to compare against.
One possi-bility for this baseline would be to select the mostheavily referenced choice from the list of candi-dates.
This is similar to the approach used in datamining, where classifiers are compared against thenaive classifier that classifies every instance as themost frequent item in the training set.Finally, taking the reverse approach to the algo-rithm and looking primarily at incoming links ?
fol-lowing the intuition behind our method of selectingtest data (see section 5.1) ?
may prove very usefulin locating articles that potentially contain incorrectlinks, allowing the algorithm to accurately and ef-ficiently seek out pages to repair without having tocrawl blindly across the entire encyclopaedia.6http://images.google.com/imagelabeler/17ReferencesAdler, B. Thomas and Luca de Alfaro.
2007.
A content-driven reputation system for the wikipedia.
In WWW?07: Proceedings of the 16th international conferenceon World Wide Web, pages 261?270, New York, NY,USA.
ACM.Bizer, Christian, Jens Lehmann, Georgi Kobilarov,So?ren Auer, Christian Becker, Richard Cyganiak, andSebastian Hellmann.
2009.
Dbpedia - a crystalliza-tion point for the web of data.
Web Semantics: Sci-ence, Services and Agents on the World Wide Web,7(3):154?165, September.Fallis, Don.
2008.
Toward an epistemology ofwikipedia.
Journal of the American Society for Infor-mation Science and Technology, 59(10):1662?1674.Keen, Andrew.
2007.
The Cult of the Amateur: HowToday?s Internet is Killing Our Culture.
BroadwayBusiness, June.Kuznetsov, Stacey.
2006.
Motivations of contributors towikipedia.
SIGCAS Comput.
Soc., 36(2), June.Medelyan, Olena, David Milne, Catherine Legg, andIan H. Witten.
2009.
Mining meaning fromwikipedia.
May.Mihalcea, Rada and Andras Csomai.
2007.
Wikify!
:linking documents to encyclopedic knowledge.
InCIKM ?07: Proceedings of the sixteenth ACM con-ference on Conference on information and knowledgemanagement, pages 233?242, New York, NY, USA.ACM.Milne, David and Ian H. Witten.
2008a.
An effec-tive, low-cost measure of semantic relatedness ob-tained from wikipedia links.
In Proceedings of thefirst AAAI Workshop on Wikipedia and Artificial In-telligence.Milne, David and Ian H. Witten.
2008b.
Learning tolink with wikipedia.
In CIKM ?08: Proceeding of the17th ACM conference on Information and knowledgemanagement, pages 509?518, New York, NY, USA.ACM.Milne, David.
2007.
Computing semantic relatednessusing wikipedia link structure.
In Proceedings ofthe New Zealand Computer Science Research StudentConference.Schonhofen, Peter.
2006.
Identifying document top-ics using the wikipedia category network.
In WI?06: Proceedings of the 2006 IEEE/WIC/ACM Inter-national Conference on Web Intelligence, pages 456?462, Washington, DC, USA.
IEEE Computer Society.von Ahn, L. 2006.
Games with a purpose.
Computer,39(6):92?94.18
