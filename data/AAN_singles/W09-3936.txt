Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 253?261,Queen Mary University of London, September 2009. c?2009 Association for Computational LinguisticsTurn-Yielding Cues in Task-Oriented DialogueAgust?
?n GravanoDepartment of Computer ScienceColumbia UniversityNew York, NY, USAagus@cs.columbia.eduJulia HirschbergDepartment of Computer ScienceColumbia UniversityNew York, NY, USAjulia@cs.columbia.eduAbstractWe examine a number of objective, au-tomatically computable TURN-YIELDINGCUES ?
distinct prosodic, acoustic andsyntactic events in a speaker?s speech thattend to precede a smooth turn exchange ?in the Columbia Games Corpus, a largecorpus of task-oriented dialogues.
Weshow that the likelihood of occurrence ofa turn-taking attempt from the interlocu-tor increases linearly with the number ofcues conjointly displayed by the speaker.Our results are important for improvingthe coordination of speaking turns in in-teractive voice-response systems, so thatsystems can correctly estimate when theuser is willing to yield the conversationalfloor, and so that they can produce theirown turn-yielding cues appropriately.1 Introduction and Previous ResearchUsers of state-of-the-art interactive voice response(IVR) systems often find interactions with thesesystems to be unsatisfactory.
Part of this reac-tion is due to deficiencies in speech recognitionand synthesis technologies, but some can also betraced to coordination problems in the exchangeof speaking turns between system and user (Wardet al, 2005; Raux et al, 2006).
Users are not surewhen the system is ready to end its turn, and sys-tems are not sure when users are ready to relin-quish theirs.
Currently, the standard method fordetermining when a user is willing to yield theconversational floor is to wait for a silence longerthan a prespecified threshold, typically rangingfrom 0.5 to 1 second (Ferrer et al, 2003).
How-ever, this strategy is rarely used by humans, whorely instead on cues from sources such as syntax,acoustics and prosody to anticipate turn transitions(Yngve, 1970).
If such TURN-YIELDING CUEScould be modeled and incorporated in IVR sys-tems, it should be possible to make faster, moreaccurate turn-taking decisions, thus leading to amore fluent interaction.
Additionally, a better un-derstanding of the mechanics of turn-taking couldbe used to vary the speech output of IVR systemsto (i) produce turn-yielding cues when the sys-tem is finished speaking and the user is expectedto speak next, and (ii) avoid producing such cueswhen the system has more things to say.
In thispaper we examine the existence of turn-yieldingcues in a large corpus of task-oriented dialoguesin Standard American English (SAE).The question of what types of cues humans ex-ploit for engaging in synchronized conversationhas been addressed by several studies.
Duncan(1972, inter alia) conjectures that speakers dis-play complex signals at turn endings, composedof one or more discrete turn-yielding cues, suchas the completion of a grammatical clause, or anyphrase-final intonation other than a plateau.
Dun-can also hypothesizes that the likelihood of a turn-taking attempt by the listener increases linearlywith the number of such cues conjointly displayedby the speaker.
Subsequent studies have investi-gated some of these hypotheses (Ford and Thomp-son, 1996; Wennerstrom and Siegel, 2003).
Morerecent studies have investigated how to improveIVR system?s the turn-taking decisions by incor-porating some of the features found to correlatewith turn endings (Ferrer et al, 2003; Atterer etal., 2008; Raux and Eskenazi, 2008).
All of thesemodels are shown to improve over silence-basedtechniques for predicting turn endings, motivatingfurther research.
In this paper we present results253of a large, corpus-based study of turn-yieldingcues in the Columbia Games Corpus which veri-fies some of Duncan?s hypotheses and adds addi-tional cues to turn-taking behavior.2 Materials and MethodThe materials for our study are taken from theColumbia Games Corpus (Gravano, 2009), a col-lection of 12 spontaneous task-oriented dyadicconversations elicited from 13 native speakers ofSAE.
In each session, two subjects were paid toplay a series of computer games requiring verbalcommunication to achieve joint goals of identify-ing and moving images on the screen, while seatedin a soundproof booth divided by a curtain to en-sure that all communication was verbal.
The sub-jects?
speech was not restricted in any way, andthe games were not timed.
The corpus contains9 hours of dialogue, which were orthographicallytranscribed; words were time-aligned to the sourceby hand.
Around 5.4 hours have also been into-nationally transcribed using the ToBI framework(Beckman and Hirschberg, 1994).We automatically extracted a number of acous-tic features from the corpus using the Praat toolkit(Boersma and Weenink, 2001), including pitch,intensity and voice quality features.
Pitch slopeswere computed by fitting least-squares linear re-gression models to the F0 track extracted fromgiven portions of the signal.
Part-of-speech(POS) tags were labeled automatically using Rat-naparkhi?s maxent tagger trained on a subset of theSwitchboard corpus in lower-case with all punctu-ation removed, to simulate spoken language tran-scripts.
All speaker normalizations were calcu-lated using z-scores: z = (x ?
?
)/?, where xis a raw measurement, and ?
and ?
are the meanand standard deviation for a speaker.For our turn-taking studies, we define anINTER-PAUSAL UNIT (IPU) as a maximal se-quence of words surrounded by silence longer than50 ms.1 A TURN then is defined as a maximal se-quence of IPUs from one speaker, such that be-tween any two adjacent IPUs there is no speechfrom the interlocutor.
Boundaries of IPUs andturns are computed automatically from the time-aligned transcriptions.
Two trained annotatorsclassified each turn transition in the corpus using alabeling scheme adapted from Beattie (1982) thatidentifies, inter alia, SMOOTH SWITCHES ?
tran-150 ms was identified empirically to avoid stopgaps.sitions from speaker A to speaker B such that (i)A manages to complete her utterance, and (ii) nooverlapping speech occurs between the two con-versational turns.
Additionally, all continuationsfrom one IPU to the next within the same turnwere labeled automatically as HOLD transitions.The complete labeling scheme is shown in the Ap-pendix.Our general approach consists in contrastingIPUs immediately preceding smooth switches(S) with IPUs immediately preceding holds (H).
(Note that in this paper we consider only non-overlapping exchanges.)
We hypothesize thatturn-yielding cues are more likely to occur beforeS than before H. It is important to emphasize theoptionality of all turn-taking phenomena and de-cisions: For H, turn-yielding cues ?
whatevertheir nature ?
may still be present; and for S, theymay sometimes be absent.
However, we hypothe-size that their likelihood of occurrence should bemuch higher before S. Finally, note that we donot make claims regarding whether speakers con-sciously produce turn-yielding cues, or whetherlisteners consciously perceive and/or use them toaid their turn-taking decisions.3 Individual Turn-Yielding CuesFigures 1 and 2 show the speaker-normalizedmean of a number of objective, automaticallycomputed variables for IPUs preceding S and H.In all cases, one-way ANOVA and Kruskal-Wallistests reveal significant differences (at p < 0.001)between the two groups.
We we discuss these re-sults in detail below.3.1 IntonationThe literature contains frequent mention of thepropensity of speaking turns to end in any into-nation contour other than a plateau (a sustainedpitch level, neither rising nor falling).
We firstanalyze the categorical prosodic labels in the por-tion of the Columbia Games Corpus annotated us-ing the ToBI annotations.
We tabulate the phraseS HH-H% 484 22.1% 513 9.1%[!
]H-L% 289 13.2% 1680 29.9%L-H% 309 14.1% 646 11.5%L-L% 1032 47.2% 1387 24.7%No boundary tone 16 0.7% 1261 22.4%Other 56 2.6% 136 2.4%Total 2186 100% 5623 100%Table 1: ToBI phrase accents and boundary tones.254Figure 1: Individual turn-yielding cues: intonation, speaking rate and IPU duration.accent and boundary tone labels assigned to theend of each IPU, and compare their distributionfor the S and H turn exchange types, as shownin Table 1.
A chi-square test indicates that thereis a significant departure from a random distribu-tion (?2=1102.5, df=5, p?0).
Only 13.2% ofall IPUs immediately preceding a smooth switch(S) ?
where turn-yielding cues are most likelypresent ?
end in a plateau ([!
]H-L%); most of theremaining ones end in either a falling pitch (L-L%)or a high rise (H-H%).
For IPUs preceding a hold(H) the counts approximate a uniform distribution,with the plateau contours being the most common,supporting the hypothesis that this contour func-tions as a TURN-HOLDING CUE (that is, a cue thattypically prevents turn-taking attempts from thelistener).
The high counts for the falling contourpreceding a hold (24.7%) may be explained by thefact that, as discussed above, taking the turn isoptional for the listener, who may choose not toact despite hearing some turn-yielding cues.
It isnot entirely clear what the role is of the low-risingcontour (L-H%), as it occurs in similar proportionsbefore S and before H. Finally, we note that the ab-sence of a boundary tone works as a strong indi-cation that the speaker has not finished speaking,since nearly all (98%) IPUs without a boundarytone precede a hold transition.Next, we examine four objective acoustic ap-proximations of this perceptual feature: the ab-solute value of the speaker-normalized F0 slope,both raw and stylized, computed over the final 200and 300 ms of each IPU.
The case of a plateaucorresponds to a value of F0 slope close to zero;the other case, of either a rising or a falling pitch,corresponds to a high absolute value of F0 slope.As shown in Figure 1, we find that the final slopebefore S is significantly higher than before H inall four cases.
These findings provide additionalsupport to the hypothesis that turns tend to end infalling and high-rising final intonations, and pro-vide automatically identifiable indicators of thisturn-yielding cue.3.2 Speaking rateDuncan (1972) hypothesizes a ?drawl on the fi-nal syllable or on the stressed syllable of a termi-nal clause?
[p. 287] as a turn-yielding cue, whichwould probably correspond to a noticeable de-crease in speaking rate.
We examine this hypothe-sis in our corpus using two common definitions ofspeaking rate: syllables per second and phonemesper second.
Syllable and phoneme counts wereestimated from dictionary lookup, and word dura-tions were extracted from the manual orthographicalignments.
Figure 1 shows that both measures,computed over either the whole IPU or its finalword, are significantly higher before S than be-fore H, which indicates an increase in speakingrate before turn boundaries rather than Duncan?shypothesized drawl.Furthermore, the speaking rate is, in both cases(before S and before H), significantly slower onthe final word than over the whole IPU, a findingthat is in line with phonological theories that pre-dict a segmental lengthening near prosodic phraseboundaries (Wightman et al, 1992).
This findingmay indeed correspond to the drawl or lengthen-ing described by Duncan before turn boundaries.However, it seems to be the case ?
at least forour corpus ?
that the final lengthening tends tooccur at all phrase final positions, not just at turnendings.
In fact, our results indicate that the fi-nal lengthening is more prominent in turn-medialIPUs than in turn-final ones.255Figure 2: Individual turn-yielding cues: intensity, pitch and voice quality.3.3 IPU duration and acoustic cuesIn the Columbia Games Corpus, we find that turn-final IPUs tend to be significantly longer than turn-medial ones, both when measured in seconds andin number of words (Figure 1).
This suggests thatIPU duration could function as a turn-yielding cue,supporting similar findings in perceptual experi-ments by Cutler and Pearson (1986).We also find that IPUs followed by S have amean intensity significantly lower than those fol-lowed by H (computed over the IPU-final 500 and1000 ms, see Figure 2).
Also, the differences in-crease when moving towards the end of the IPU.This suggests that speakers tend to lower theirvoices when approaching potential turn bound-aries, whereas they reach turn-internal pauses witha higher intensity.Phonological theories conjecture a declinationin the pitch level, which tends to decrease grad-ually within utterances, and across utteranceswithin the same discourse segment, as a conse-quence of a gradual compression of the pitch range(Pierrehumbert and Hirschberg, 1990).
For con-versational turns, then, we would expect to findthat speakers tend to lower their pitch level as theyreach potential turn boundaries.
This hypothesisis verified by the dialogues in our corpus, wherewe find that IPUs preceding S have a significantlylower mean pitch than those preceding H (Figure2).
In consequence, pitch level may also work as aturn-yielding cue.Next we examine three acoustic features asso-ciated with the perception of voice quality: jit-ter, shimmer and noise-to-harmonics ratio (NHR)(Bhuta et al, 2004), computed over the IPU-final500 and 1000 ms (Figure 2).
We compute jit-ter and shimmer only over voiced frames for im-proved robustness.
For all three features, the meanvalue for IPUs preceding S is significantly higherthan for IPUs preceding H, with the difference in-creasing towards the end of the IPU.
Therefore,voice quality seems to play a clear role as a turn-yielding cue.3.4 Lexical cuesStereotyped expressions such as you know or Ithink have been proposed in the literature as lex-ical turn-yielding cues.
However, in the GamesCorpus we find that none of the most frequentIPU-final unigrams and bigrams, both precedingS and H, correspond to such expressions (see Ta-ble A.1 in the Appendix).
Instead, such unigramsand bigrams are specific to the computer gamesin which the subjects participated.
For example,the game objects tended to be spontaneously de-scribed by subjects from top to bottom and fromleft to right, as shown in the following excerpt(pauses are indicated with #):A: I have a blue lion on top # with a lemonin the bottom left # and a yellow crescentmoon in- # i- # in the bottom rightB: oh okay [...]In consequence, bigrams such as lower right andbottom right are common before S, while on topor bottom left are common before H. These are alltask-specific lexical constructions and do not con-stitute stereotyped expressions in the traditionalsense.Also very common among the most frequentIPU-final expressions are AFFIRMATIVE CUEWORDS ?
heavily overloaded words, such asokay or yeah, that are used both to initiate andto end discourse segments, among other functions(Gravano et al, 2007).
The occurrence of thesewords does not constitute a turn-yielding or turn-holding cue per se; rather, additional contextual,acoustic and prosodic information is needed to dis-ambiguate their meaning.256While we do not find clear examples of lexicalturn-yielding cues in our task-oriented corpus, wedo find two lexical turn-holding cues: word frag-ments (e.g., incompl-) and filled pauses (e.g., uh,um).
Of the 8123 IPUs preceding H, 6.7% endin a word fragment, and 9.4% in a filled pause.By constrast, only 0.3% of the 3246 IPUs preced-ing S end in a word fragment, and 1% in a filledpause.
These differences suggest that, after eithera word fragment or a filled pause, the speaker ismuch more likely to intend to continue holdingthe floor.
This notion of disfluencies functioningas a turn-taking cue has been studied by Goodwin(1981), who shows that they may be used to securethe listener?s attention at turn beginnings.3.5 Textual completionSeveral authors (Duncan, 1972; Ford and Thomp-son, 1996; Wennerstrom and Siegel, 2003) claimthat some form of syntactic or semantic comple-tion, independent of intonation and interactionalimport, functions as a turn-yielding cue.
Althoughsome call this syntactic completion, since all au-thors acknowledge the need for semantic and dis-course information in judging it, we choose themore neutral term TEXTUAL COMPLETION forthis phenomenon.
We annotated a portion ofour corpus with respect to textual completion andtrained a machine learning (ML) classifier to auto-matically label the whole corpus.
From these an-notations we then examined how textual comple-tion labels relate to turn-taking categories in thecorpus.3.5.1.
Manual labeling: In conversation, lis-teners judge textual completion incrementally andwithout access to later material.
To simulate theseconditions in the labeling task, annotators wereasked to judge the textual completion of a turn upto a target pause from the written transcript alone,without listening to the speech.
They were al-lowed to read the transcript of the full previousturn by the other speaker (if any), but they werenot given access to anything after the target pause.These are two sample tokens:A: the lion?s left paw our frontB: yeah and it?s th- right so theA: and then a tea kettle and then the wineB: okay well I have the big shoe and the wineWe selected 400 tokens at random from the GamesCorpus; the target pauses were also chosen at ran-dom.
Three annotators labeled each token inde-pendently as either complete or incomplete ac-cording to these guidelines: Determine whetheryou believe what speaker B has said up to thispoint could constitute a complete response to whatspeaker A has said in the previous turn/segment.Note: If there are no words by A, then B is begin-ning a new task, such as describing a card or thelocation of an object.
To avoid biasing the results,annotators were not given the turn-taking labels ofthe tokens.
Inter-annotator reliability is measuredby Fleiss?
?
at 0.814, which corresponds to the?almost perfect?
agreement category.
The meanpairwise agreement between the three subjects is90.8%.
For the cases in which there is disagree-ment between the three annotators, we adopt theMAJORITY LABEL as our gold standard; that is,the label chosen by two annotators.3.5.2.
Automatic classification: Next, wetrained a ML model using the 400 manually anno-tated tokens as training data to automatically clas-sify all IPUs in the corpus as either complete or in-complete.
For each IPU we extracted a number oflexical and syntactic features from the current turnup to the IPU itself: lexical identity of the IPU-final word (w); POS tags and simplified POS tags(N, V, Adj, Adv, Other) of w and of the IPU-finalbigram; number of words in the IPU; a binary flagindicating if w is a word fragment; size and type ofthe biggest (bp) and smallest (sp) phrase that endin w; binary flags indicating if each of bp and sp isa major phrase (NP, VP, PP, ADJP, ADVP); binaryflags indicating if w is the head of each of bp andsp.
We chose these features in order to capture asmuch lexical and syntactic information as possiblefrom the transcripts.
The syntactic features werecomputed using two different parsers: the Collinsstatistical parser (Collins, 2003) and CASS, a par-tial parser especially designed for use with noisytext (Abney, 1996).
We experimented with thelearners listed in Table 2, using the implementa-tions provided in the WEKA ML toolkit (Wittenand Frank, 2000).
Table 2 shows the accuracy ofthe majority-class baseline and of each classifier,using 10-fold cross validation on the 400 train-ing data points, and the mean pairwise agreementby the three human labelers.
The linear-kernelsupport-vector-machine (SVM) classifier achievesthe highest accuracy, significantly outperformingthe baseline, and approaching the mean agreementof human labelers.257Classifier AccuracyMajority-class (?complete?)
55.2%C4.5 (decision trees) 55.2%Ripper (propositional rules) 68.2%Bayesian networks 75.7%SVM, RBF kernel (c = 1, ?
= 10?12) 78.2%SVM, linear kernel (c = 1, ?
= 10?12) 80.0%Human labelers (mean agreement) 90.8%Table 2: Textual completion: ML results.3.5.3.
Results: First we examine the tokens thatwere manually labeled by the human annotators.Of the 100 tokens followed by S, 91 were labeledtextually complete, a significantly higher propor-tion than the 42% followed by H that were labeledcomplete (?2=51.7, df=1, p?0).
Next, we usedour highest performing classifier, the linear-kernelSVM, to automatically label all IPUs in the cor-pus.
Of the 3246 IPUs preceding S, 2649 (81.6%)were labeled textually complete, and about half ofall IPUs preceding H (4272/8123, or 52.6%) werelabeled complete.
The difference is also signifi-cant (?2 =818.7, df = 1, p?
0).
These resultssuggest that textual completion as defined aboveconstitutes a necessary, but not sufficient, turn-yielding cue.4 Combining Turn-Yielding CuesSo far, we have shown strong evidence supportingthe existence of individual acoustic, prosodic andtextual turn-yielding cues.
Now we shift our atten-tion to the manner in which they combine togetherto form more complex turn-yielding signals.
Foreach individual cue type, we choose two or threefeatures shown to correlate strongly with smoothswitches, as shown in Table 3 (e.g., the speakingrate cue is represented by two automatic features:syllables and phonemes per second over the wholeIPU).
We consider a cue c to be PRESENT on IPUu if, for any feature f modeling c, the value of fon u is closer to fS than to fH , where fS and fHare the mean values of f across all IPUs preced-ing S and H, respectively.
Otherwise, we say c isABSENT on u.
Also, we automatically annotate allIPUs in the corpus for textual completion using thelinear-kernel SVM classifier described in Section3.5.
IPUs classified as complete are considered tobear the textual completion turn-yielding cue.We first analyze the frequency of occurrenceof conjoined individual turn-yielding cues.
Ta-ble 4 shows the top frequencies of complex turn-yielding cues for IPUs immediately before smoothIndividual cues Automatic featuresIntonationAbs(F0 slope) over IPU-final 200 msAbs(F0 slope) over IPU-final 300 msSpeaking rateSyllables per second over whole IPUPhonemes per second over whole IPUIntensity levelMean intensity over IPU-final 500 msMean intensity over IPU-final 1000 msPitch levelMean pitch over IPU-final 500 msMean pitch over IPU-final 1000 msIPU durationIPU duration in msNumber of words in IPUVoice qualityJitter over IPU-final 500 msShimmer over IPU-final 500 msNHR over IPU-final 500 msTable 3: Features used to estimate the presence ofindividual turn-yielding cues.switches (S) and holds (H).
The most frequentcases before S correspond to all, or almost all, cuespresent at once.
For IPUs preceding a hold (H),the opposite is true: those with no cues, or withjust one or two, represent the most frequent cases.S HCues Count Cues Count1234567 267 ...4... 392.234567 226 ......7 2471234.67 138 ....... 223.234.67 109 ...4..7 218.23..67 98 ...45.. 178..34567 94 .2....7 166123..67 93 1234.67 163.2.4567 73 .2..5.7 157... ...Total 3246 Total 8123Table 4: Top frequencies of complex turn-yieldingcues for IPUs preceding S and H. A digit indicatesthe presence of a specific cue; a dot, its absence.1: Intonation; 2: Speaking rate; 3: Intensity level;4: Pitch level; 5: IPU duration; 6: Voice quality;7: Textual completion.Table 5 shows the same results, now groupingtogether all IPUs with the same number of cues,independently of the cue types.
Again, we observethat larger proportions of IPUs preceding S presentmore conjoined cues than IPUs preceding H.Next we look at how the likelihood of a turn-taking attempt varies with respect to the numberof individual cues displayed by the speaker, a rela-tion hypothesized to be linear by Duncan (1972).Figure 3 shows the proportion of IPUs with 0-7cues present that are followed by a turn-taking at-tempt from the interlocutor.2 The dashed line cor-2 The proportion of turn-taking attempts is computed foreach cue count as the number of S and PI divided by the num-ber of S, PI, H and BC, according to our labeling scheme.258Cue count S H0 4 0.1% 223 2.7%1 52 1.6% 970 11.9%2 241 7.4% 1552 19.1%3 518 16.0% 1829 22.5%4 740 22.8% 1666 20.5%5 830 25.6% 1142 14.1%6 594 18.3% 611 7.5%7 267 8.2% 130 1.6%Total 3246 100% 8123 100%Table 5: Distribution of the number of turn-yielding cues displayed in IPUs preceding smoothswitches (S) and hold transitions (H).Figure 3: Percentage of turn-taking attempts fromthe listener (either S or PI) following IPUs con-taining 0-7 turn-yielding cues.responds to a linear model fitted to the data (Pear-son?s correlation test: r2 = 0.969), and the contin-uous line, to a quadratic model (r2 = 0.995).
Thehigh correlation coefficient of the linear modelsupports Duncan?s hypothesis, that the likelihoodof a turn-taking attempt by the interlocutor in-creases linearly with the number of individual cuesdisplayed by the speaker.
However, an ANOVA testreveals that the quadratic model fits the data sig-nificantly better than the linear model (F (1, 5) =23.01; p = 0.005), even though the curvature ofthe quadratic model is only moderate, as can beobserved in the figure.5 Speaker VariationTo investigate possible speaker dependence in ourturn-yielding cues, we examine evidence for eachcue for each of our thirteen speakers.
Table 6summarizes this data.
For each speaker, a check(?)
indicates that there is significant evidence ofthe speaker producing the corresponding individ-ual turn-yielding cue (at p < 0.05, using the samestatistical tests described in the previous sections).Five speakers show evidence of all seven cues,Speaker 101 102 103 104 105 106 107 108 109 110 111 112 113Intonation?
?
?
?
?
?
?
?
?
?
?Spk.
rate?
?
?
?
?
?
?
?
?
?
?
?
?Intensity?
?
?
?
?
?
?
?
?
?
?
?Pitch?
?
?
?
?
?
?Completion?
?
?
?
?
?
?
?
?
?
?
?
?Voice quality?
?
?
?
?
?
?
?
?
?
?
?
?IPU duration?
?
?
?
?
?
?
?
?
?
?
?
?LM r2 .92 .93 .82 .88 .97 .96 .95 .95 .97 .91 .95 .97 .89QM r2 .98 .95 .95 .92 .98 .98 .96 .95 .99 .94 .98 .99 .90Table 6: Summary of results for each individualspeaker.while the remaining eight speakers show eitherfive or six cues.
Pitch level is the least reliablecue, present only for seven subjects.
Notably, thecues related to speaking rate, textual completion,voice quality, and IPU duration are present for allthirteen speakers.The two bottom rows in Table 6 show the cor-relation coefficients (r2) of linear and quadraticregressions performed on the data from eachspeaker.
In all cases, the coefficients are very high.The fit of the quadratic model is significantly bet-ter for six speakers (shown in bold typeface); forthe remaining seven speakers, both models pro-vide statistically indistinguishable explanations ofthe data.6 DiscussionWe have examined seven turn-yielding cues ?i.e., seven measurable events that take place witha significantly higher frequency on IPUs preced-ing smooth turn switches than on IPUs precedinghold transitions.
These events may be summarizedas follows: (i) a falling or high-rising intonation atthe end of the IPU; (ii) an increased speaking rate;(iii) a lower intensity level; (iv) a lower pitch level;(v) a longer IPU duration; (vi) a higher value ofthree voice quality features: jitter, shimmer, andNHR; and (vii) a point of textual completion.
Wehave also shown that, when several turn-yieldingcues occur simultaneously, the likelihood of a sub-sequent turn-taking attempt by the interlocutor in-creases in an almost linear fashion.We propose that these findings can be used toimprove some turn-taking decisions of state-of-the-art IVR systems.
For example, if a systemwishes to yield the floor to a user, it should in-clude in its output as many of the described cuesas possible.
Conversely, when the user is speak-ing, the system may detect appropriate momentsto take the turn by estimating the presence of turn-259yielding cues at every silence.
If the number of de-tected cues is high enough, then the system shouldtake the turn; otherwise, it should remain silent.Two assumptions of our study are that turn-yielding cues are binary and all contribute equallyto the overall ?count?.
In future research wewill explore alternative methods of combining andweighting the different features ?
by means ofmultiple linear regression, for example ?
in or-der to experiment with more sophisticated modelsof turn-yielding behavior.
We also plan to exam-ine new turn-yielding cues, paying special atten-tion to additional voice quality features, given thepromising results obtained for jitter, shimmer andnoise-to-harmonics ratio.7 AcknowledgementsThis work was funded in part by NSF IIS-0307905.
We thank Stefan Benus, Enrique Hen-estroza, Elisa Sneed and Gregory Ward, for valu-able discussion and for their help in collecting andlabeling the data, and the anonymous reviewers forhelpful comments and suggestions.ReferencesS.
Abney.
1996.
Partial parsing via finite-state cas-cades.
Journal of Natural Language Engineering,2(4):337?344.M.
Atterer, T. Baumann, and D. Schlangen.
2008.
To-wards incremental end-of-utterance detection in di-alogue systems.
In Proceedings of Coling, Manch-ester, UK.G.
W. Beattie.
1982.
Turn-taking and interruptionin political interviews: Margaret Thatcher and JimCallaghan compared and contrasted.
Semiotica,39(1/2):93?114.M.
E. Beckman and J. Hirschberg.
1994.
The ToBIannotation conventions.
Ohio State University.T.
Bhuta, L. Patrick, and J. D. Garnett.
2004.
Per-ceptual evaluation of voice quality and its correla-tion with acoustic measurements.
Journal of Voice,18(3):299?304.P.
Boersma and D. Weenink.
2001.
Praat: Doing pho-netics by computer.
http://www.praat.org.M.
J. Collins.
2003.
Head-driven statistical models fornatural language parsing.
Computational Linguis-tics, 29(4):589?637.E.
A. Cutler and M. Pearson.
1986.
On the analysis ofprosodic turn-taking cues.
In C. Johns-Lewis, Ed.,Intonation in Discourse, pp.
139?156.
College-Hill.S.
Duncan.
1972.
Some signals and rules for takingspeaking turns in conversations.
Journal of Person-ality and Social Psychology, 23(2):283?292.L.
Ferrer, E. Shriberg, and A. Stolcke.
2003.
Aprosody-based approach to end-of-utterance detec-tion that does not require speech recognition.
InProceedings of ICASSP.C.
E. Ford and S. A. Thompson.
1996.
Interactionalunits in conversation: Syntactic, intonational andpragmatic resources for the management of turns.
InE.
Ochs, E. A. Schegloff, and S. A. Thompson, Eds.,Interaction and Grammar, pp.
134?184.
CambridgeUniversity Press.C.
Goodwin.
1981.
Conversational Organization:Interaction between Speakers and Hearers.
Aca-demic Press.A.
Gravano, S. Benus, J. Hirschberg, S. Mitchell, andI.
Vovsha.
2007.
Classification of discourse func-tions of affirmative words in spoken dialogue.
InProceedings of Interspeech.A.
Gravano.
2009.
Turn-Taking and Affirmative CueWords in Task-Oriented Dialogue.
Ph.D. thesis,Columbia University, New York.J.
Pierrehumbert and J. Hirschberg.
1990.
The mean-ing of intonational contours in the interpretation ofdiscourse.
In P. Cohen, J. Morgan, and M. Pol-lack, Eds., Intentions in Communication, pp.
271?311.
MIT Pr.A.
Raux and M. Eskenazi.
2008.
Optimizing endpoint-ing thresholds using dialogue features in a spokendialogue system.
In Proceedings of SIGdial.A.
Raux, D. Bohus, B. Langner, A. W. Black, andM.
Eskenazi.
2006.
Doing research on a deployedspoken dialogue system: One year of Let?s Go!
ex-perience.
In Proceedings of Interspeech.N.
G. Ward, A. G. Rivera, K. Ward, and D. G. Novick.2005.
Root causes of lost time and user stress ina simple dialog system.
In Proceedings of Inter-speech.A.
Wennerstrom and A. F. Siegel.
2003.
Keeping thefloor in multiparty conversations: Intonation, syn-tax, and pause.
Discourse Processes, 36(2):77?107.C.
W. Wightman, S. Shattuck-Hufnagel, M. Ostendorf,and P. J.
Price.
1992.
Segmental durations in thevicinity of prosodic phrase boundaries.
The Journalof the Acoustical Society of America, 91:1707.I.
H. Witten and E. Frank.
2000.
Data Mining: Prac-tical Machine Learning Tools and Techniques withJava Implementations.
Morgan Kaufmann.V.
H. Yngve.
1970.
On getting a word in edgewise.Sixth Regional Meeting of the Chicago LinguisticSociety, 6:657?677.260For each turn by speaker S2, where S1 is the other speaker, label S2?s turn as follows:Is S2?s utterance in response to S1?s utterance and indicates only?I?m still here / I hear you and please continue??
(1)noSimultaneous speech present?yesS2 is successful?yesS1?s utterancecomplete?
(2)yesOverlap(O)HHHHjnoInterruption(I)HHHHjnoButting-in(BI)HHHHjnoS1?s utterancecomplete?
(2)yesSmoothswitch (S)HHHHjnoPause interruption(PI)HHHHjyesSimultaneous speech present?yesBackchannelwith overlap(BC O)HHHHjnoBackchannel(BC)Figure A.1: Turn-taking labeling scheme.Appendix: Turn-Taking Labeling SchemeWe adopt a slightly modified version of Beat-tie?s (1982) labeling scheme, depicted in Fig-ure A.1.
We incorporate backchannels (excludedfrom Beattie?s study) by adding the decisionmarked (1) at the root of the decision tree, forwhich we use the annotations described in Gra-vano et al (2007).
For the decision marked (2), weuse Beattie?s informal definition of utterance com-pleteness: ?Completeness [is] judged intuitively,taking into account the intonation, syntax, andmeaning of the utterance?
[p. 100].
All continu-ations from one IPU to the next within the sameturn are labeled automatically H, for ?hold?.
Also,we identify three special cases that do not corre-spond to actual turn exchanges:Task beginnings: Turns beginning a new gametask are labeled X1.Continuations after BC or BC O: If a turn t is acontinuation after a backchannel b from the otherspeaker, it is labeled X2 O if t and b overlap, orX2 if not.Simultaneous starts: Fry (1975) reports that hu-mans require at least 210 ms to react verbally to averbal stimulus.3 Thus, if two turns begin within210 ms of each other, they are most probably con-nected to preceding events than to one another.
InFigure A.2, A1, A2 and B1 represent turns fromspeakers A and B.
Most likely, A2 is simply acontinuation from A1, and B1 occurs in response3D.
B. Fry.
1975.
Simple reaction-times to speech andnon-speech stimuli.
Cortex, 11(4):355-60.to A1.
Thus, B1 is labeled with respect to A1 (notA2), and A2 is labeled X3.A1 A2xB1yFigure A.2: Simultaneous start (|y?x| < 210ms).S Count H Countokay 241 okay 402yeah 167 on top 172lower right 85 um 136bottom right 74 the top 117the right 59 of the 67hand corner 52 blue lion 57lower left 43 bottom left 56the iron 37 with the 54the onion 33 the um 54bottom left 31 yeah 53the ruler 30 the left 48mm-hm 30 and 48right 28 lower left 46right corner 27 uh 45the bottom 26 oh 45the left 24 and a 45crescent moon 23 alright 44the lemon 22 okay um 43the moon 20 the uh 42tennis racket 20 the right 41blue lion 19 the bottom 39the whale 18 I have 39the crescent 18 yellow lion 37the middle 17 the middle 37of it 17 I?ve got 34... ...Total 3246 Total 8123Table A.1: 25 most frequent final bigrams preced-ing smooth turn switches (S) and hold transitions(H).
(See Section 3.4.
)261
