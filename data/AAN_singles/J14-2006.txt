Practical Linguistic Steganography usingContextual Synonym Substitution and aNovel Vertex Coding MethodChing-Yun Chang?University of CambridgeStephen Clark?
?University of CambridgeLinguistic steganography is concerned with hiding information in natural language text.One of the major transformations used in linguistic steganography is synonym substitution.However, few existing studies have studied the practical application of this approach.
In thisarticle we propose two improvements to the use of synonym substitution for encoding hiddenbits of information.
First, we use the Google n-gram corpus for checking the applicability of asynonym in context, and we evaluate this method using data from the SemEval lexical substitu-tion task and human annotated data.
Second, we address the problem that arises from words withmore than one sense, which creates a potential ambiguity in terms of which bits are representedby a particular word.
We develop a novel method in which words are the vertices in a graph,synonyms are linked by edges, and the bits assigned to a word are determined by a vertex codingalgorithm.
This method ensures that each word represents a unique sequence of bits, withoutcutting out large numbers of synonyms, and thus maintains a reasonable embedding capacity.1.
IntroductionIn order to transmit information through an open channel without detection by anyoneother than the receiver, a covert channel can be used.
In information theory, a covertchannel is a parasitic communications channel that is hidden within the medium ofa legitimate communication channel (Lampson 1973).
For example, steganography is aform of covert channel in which certain properties of the cover medium are manipulatedin an unexpected, unconventional, or unforeseen way so that, with steganographictransmission, the encrypted messages can be camouflaged in a seemly innocent mediumand sent to the receiver with less chance of being suspected and attacked.
Ideally,because the changes to the medium are so subtle, anyone not specifically looking fora hidden message is unlikely to notice the changes (Fridrich 2009).?
15 JJ Thomson Avenue, Cambridge CB3 0FD, UK.
E-mail: Ching-Yun.Chang@cl.cam.ac.uk.??
15 JJ Thomson Avenue, Cambridge CB3 0FD, UK.
E-mail: Stephen.Clark@cl.cam.ac.uk.Submission received: 18 January 2013; revised version received: 3 June 2013; accepted for publication:1 August 2013.doi:10.1162/COLI a 00176?
2014 Association for Computational LinguisticsComputational Linguistics Volume 40, Number 2In this article, we aim at concealing secret information in natural language text bymanipulating cover words.
The proposed steganography system replaces selected coverwords with their synonyms, which is the mechanism used to embed information.
Inorder to ensure the lexical substitutions in the cover text are imperceptible, the systemuses the Google n-gram corpus (Brants and Franz 2006) for checking the applicabilityof a synonym in context.
In addition, the system assigns codes to acceptable substitutesusing a novel vertex coding method in which words are represented as vertices in agraph, synonyms are linked by edges, and the bits assigned to a vertex represent thecode of a particular word.
Our lexical substitution-based steganography system waspreviously published in Chang and Clark (2010b), in which the system was evaluatedautomatically by using data from the English lexical substitution task for SemEval-2007.1 In this article, we extend the previous work by more closely addressing thepractical application of the proposed system.
We present results from a new humanevaluation of the system?s output and a simple computational steganalysis of word-frequency statistics which is a more direct evaluation for linguistic steganography.
Wealso give an extended literature review which will serve as a useful introduction tolinguistic steganography for those Computational Linguistics readers not familiar withthe problem.1.1 SteganographyThe word steganography has Greek origins and means ?concealed writing.?
The originalpractice can be traced back to around 440 BC when the ancient Greeks hid messageswithin wax tablets by writing messages on the wood before applying a wax surface(Herodotus 1987).
Another early recorded use of steganography occurred in ancientGreece when messengers tattooed messages on their shaved heads and concealed themessages with the hair that grew over them afterwards, a technique also used byGerman spies in the early 20th century (Newman 1940).
With the advent of tiny images,in the Russo-Japanese War (1905) microscopic images were hidden in ears, nostrils, orunder fingernails (Stevens 1957); during both World Wars messages were reduced tomicrodots and stuck on top of printed periods or commas in innocent cover materialsuch as magazines, or inserted into slits of the edges of postcards (Newman 1940;Hoover 1946).
In both World Wars invisible inks were also used extensively to writemessages under visible text (Kahn 1967).
The application of special inks is still usedtoday in the field of currency security to write a hidden message on bank notes or othersecure documents.Since the 1980s, with the advent of computer technologies, digital equivalents ofthese camouflage techniques were invented to hide messages in digital cover me-dia, such as images, video, and audio signals (Fridrich 2009).
For example, in 2010,the United States Department of Justice documented that more than 100 text fileswere retrieved from images posted on publicly accessible Web sites.2 According tothe Steganography Analysis and Research Centre,3 there have been over 1,100 digi-tal steganography applications identified.
Most of the digital steganography systemsexploit the redundancy of the cover media and rely on the limitations of the humanauditory or visual systems.
For example, a standard image steganography system uses1 http://www.dianamccarthy.co.uk/task10index.html.2 http://www.justice.gov/opa/documents/062810complaint2.pdf.3 http://www.sarc-wv.com/.404Chang and Clark Practical Linguistic Steganographythe least-significant-bit substitution technique.
Because the difference between 11111111and 11111110 in the value for red/green/blue intensity is likely to be undetectable bythe human eye, the least-significant-bit can be used to hide information other than color,without being perceptable by a human observer.4Simmons (1984) formulated steganography as the ?Prisoners?
Problem.?
The prob-lem describes a scenario where two prisoners named Alice and Bob are locked up inseparate cells far apart from each other and wish to hatch an escape plan.
All theircommunications have to pass through the warden, Willie.
If Willie detects any signof a conspiracy, he will thwart their plan by throwing them into high-security cellsfrom which nobody has ever escaped; as long as Willie does not suspect anything,the communication can be put through.
So Alice and Bob must find some way forembedding hidden information into their seemingly innocent messages.
Alice and Bobcan succeed if they are able to exchange information allowing them to coordinate theirescape without arousing Willie?s suspicion.
According to information-hiding terminol-ogy (Pfitzmann 1996), a legitimate communication among the prisoners is called a coverobject, and a message with embedded hidden information is called a stego object,where object stands for ?text,?
?image,?
?audio,?
or whatever media is being used.The algorithms that Alice uses for creating the stego object and Bob uses for decodingthe message are collectively called a stegosystem.A stegosystem has to fulfil two fundamental requirements.
The first and foremostrequirement is security.
This means that the stegomedia in which the secret messageis hidden must be unsuspicious according to a human or a computer.
The secondrequirement is payload capacity.
The payload is the size of the secret message thatthe sender wishes to conceal and transport relative to the size of the cover media.Because steganography aims at covert information transmission, it requires sufficientembedding capacity.
An ideal stegosystem would have a high level of security andlarge payload capacity.
However, there is a fundamental trade-off between security andpayload because any attempt to embed additional information in the cover media islikely to increase the chance of introducing anomalies into the media, thus degradingthe security level.A related area to steganography is digital watermarking, in which changes aremade to a cover medium in order to verify its authenticity or to show the identity ofits owners?for example, for copyright purposes (Cox et al.
2008; Shih 2008).
An inter-esting watermarking application is ?traitor tracing,?
in which documents are changedin order to embed individual watermarks.
These marks can then be used to lateridentify particular documents and, if necessary, to trace the source of the documents;for example, if a set of documents?identical except for the changes used to embedthe watermarks?has been sent to a group of individuals, and one of the documents hasbeen leaked to a newspaper.
Both steganography and watermarking use steganographictechniques to embed information in cover media.
However, steganography aims for theimperceptibility of a secret message to an observer, whereas watermarking tries to markcover media with information that is robust against modifications or for the purpose oftamperproofing.
For steganography a user can have the freedom to choose the covermedium to carry messages, whereas for watermarking the cover medium is alreadydecided.4 The observer may also be a computer program, designed to detect statistical anomalies in the imagerepresentation that may indicate the presence of hidden information.405Computational Linguistics Volume 40, Number 2Figure 1The linguistic steganography framework.1.2 Linguistic SteganographyA key question for any stegosystem is the choice of cover medium.
Given the ubiquitousnature of natural language and the omnipresence of text, text is an obvious mediumto consider.
For example, a Nazi spy in World War II sent the following message(Kahn 1967):Apparently neutral?s protest is thoroughly discounted and ignored.
Isman hard hit.
Blockadeissue affects pretext for embargo on by-products, ejecting suets and vegetable oils.By taking the second letter from each word the following message emerges:Pershing sails from NY June IThe advantage of this method is that the secret message appears as some normalcommunication that may not arouse suspicion.
However, given the current state-of-the-art of Natural Language Processing (NLP) technology, NLP techniques are not capable ofcreating meaningful and natural text from scratch and of hiding messages in it.
There-fore, most of the existing linguistic stegosystems take already-existing text as the covertext, and linguistic properties of the text are used to modify it and hide information.Figure 1 shows the general linguistic steganography framework.
First, some secretmessage, represented as a sequence of bits, is hidden in a cover text using the embeddingalgorithm, resulting in the stego text.5 Next, the stego text passes the observer (humanor computer), who does not object to innocuous messages passing between the senderand receiver, but will examine the text for any suspicious looking content.
Once thestego text reaches the receiver, the hidden message is recovered using the extractingalgorithm.In order to embed messages, a cover text must provide information carriers that canbe modified to represent the secret.
For example, a lexical substitution-based stegosys-tem substitutes selected words (the information carriers) with their synonyms so thatthe concatenation of the bitstrings represented by the synonyms is identical to thesecret.
Note that an unmodifiable text cannot carry information.
So far, the literature on5 The message may have been encrypted initially also, as in the figure, but this is not important in thisarticle; the key point is that the hidden message is a sequence of bits.406Chang and Clark Practical Linguistic Steganographylinguistic steganography is small compared with other media (Bergmair 2007).
One ofthe likely reasons is that it is easier to make changes to images and other non-linguisticmedia that are undetectable by an observer.
Language has the property that even smalllocal changes to a text (e.g., replacing a word with a word with similar meaning) mayresult in text that is anomalous at the document level, or anomalous with respect to thestate of the world.
Hence finding linguistic transformations that can be applied reliablyand frequently is a challenging problem for linguistic steganography.An additional challenge for linguistic steganography is that evaluation of linguisticstegosystems is much more difficult than that of image, audio, or video stegosystems be-cause such evaluation requires us to consider many controversial linguistic issues, suchas meaning, grammaticality, fluency, and style.
The current state-of-the-art techniquesfor automatically evaluating the fluency and grammaticality of natural language gener-ation systems are based on techniques for evaluating the output of machine translationsystems, such as comparing the n-grams in the machine translation output with thosein a reference translation (Papineni et al.
2002; Zhang and Clark 2011).
Although somecomputational steganalysis systems have been developed to identify stego text frominnocent text using statistical methods, these systems can only be applied to text thatundergoes certain linguistic transformations such as translation and lexical substitution,and they are not accurate enough for practical evaluation.
Therefore, most of the currentlinguistic stegosystems were evaluated by human judges (Murphy and Vogel 2007a,2007b; Meral et al.
2007, 2009; Kim 2008, 2009; Chang and Clark 2010a, 2012a, 2012b),where a human assessor was provided with stego text and was asked to rate or improvethe naturalness of the stego text.1.3 Linguistic StegosystemMost existing stegosystems consist of three independent modules?linguistic transfor-mation, encoder generation, and text selection?as shown in Figure 2.
As explainedearlier, in order to embed messages, a cover text must provide information carriers thatcan be modified to represent the secret, and the modification must be imperceptibleto an observer.
This first step of modification is called linguistic transformation.
Forexample, suppose there is a cover sentence I like biscuits with a cup of tea, in whichbiscuits can be replaced by its synonym cookies without degrading the naturalness ofthe original sentence; hence biscuits can serve as an information carrier in this example(using synonym substitution as the transformation).
After the linguistic transformationstage, there are two alternative sentences I like biscuits with a cup of tea and I like cookieswith a cup of tea.
According to the linguistic transformation used in a stegosystem, wecan classify existing work into three major categories, which will be discussed in detailFigure 2Three modules in a linguistic stegosystem.407Computational Linguistics Volume 40, Number 2in Section 2.1: lexical or phrase substitutions, syntactic transformations, and semantictransformations.After generating different versions of the cover text, an encoding method is used toassign bitstrings to the alternatives, which is called encoder generation.
To continue theprevious example, a simple encoding method would first sort the alternative sentencesalphabetically and then encode the first alternative, namely, the one containing biscuitswith bit 0, and the other sentence containing cookies with bit 1.
The final phase is textselection, which chooses the alternative representing the secret bitstring as the stegotext.
Let us assume the secret bit is 1; therefore during the text selection phase, I likecookies with a cup of tea is chosen as the stego sentence in order to embed the secret.
Ifthere is no alternative associated with the secret bitstring, the secret embedding fails.Therefore, it is important to generate sufficient alternatives as well as to efficientlyencode each option.To recover the secret in the example, the sender and receiver must share the samelinguistic transformation and encoder generation modules.
The receiver first uses thetransformation method to determine that, in the stego sentence, cookies can be replacedby its synonym biscuits; hence, two alternative sentences are derived.
Next, the receiveruses the encoding method to assign codes to the two alternatives.
Because the stegosentence I like cookies with a cup of tea is encoded by bit 1, the receiver knows that thesecret is 1.The convenient modularity between the linguistic transformation and encoder gen-eration allows a transformation to be combined with different encoding algorithms,although the transformation may put some constraints on what encoding method can beused.
For example, suppose we replace the synonym substitution-based transformationin the example with a translation-based transformation that takes a non-English coversentence as input and outputs two different English translations as the alternatives.Assume the two translations are I like biscuits with a cup of tea and I like cookies with acup of tea, and that the second alternative is the stego sentence.
Now the receiver hasto recover the secret from this stego sentence.
However, the receiver does not know theoriginal cover sentence, so it is unlikely that the receiver will be able to obtain the othertranslations used by the sender to derive the code assigned to the stego sentence.
Inthis case, the translation-based transformation cannot work with block code encoding.A possible solution is to use hash function encoding, as explained in Section 2.2.4.1.4 OverviewIn this article we focus on linguistic steganography rather than watermarking, becausewe are interested in the requirement that any changes to a text must be imperceptibleto an observer, as this makes for a strong test of the NLP technology used to modifythe cover text.
There are some practical security issues in the steganography applicationthat we have chosen to ignore or simplify in order to focus on the underlying NLP tech-niques.
For example, we assume the adversary in the espionage scenario is acting pas-sively rather than actively.
A passive warden examines all messages exchanged betweenAlice and Bob but crucially does not modify any message.
In other words, we haveignored the possibility of steganographic attacks (Fridrich 2009), via an active wardenwho deliberately modifies messages in order to thwart any hidden communication.
Inaddition, for the human evaluation of the security level of our stegosystem, we evaluatethe naturalness of generated stego sentences.
In other words, we do not investigate thedocument-level coherence of stego text because this requires sophisticated knowledgeof natural language semantics and pragmatics which we consider to be outside the408Chang and Clark Practical Linguistic Steganographyscope of this work.
Therefore, it is possible that a totally natural stego sentence canstill raise suspicion if it contains words which stand out as being completely differentfrom the rest of the paragraph.
For the computational steganalysis, we use a simplestatistical analysis proposed in Meng et al.
(2010), which compares the frequency ofhigh-frequency words in a stego text and in its original text.The main objective of this article is to explore the applicability of lexical substitutionto steganography.
Lexical substitution is a relatively straightforward modification oftext.
It replaces selected words with the same part of speech (POS) synonyms, anddoes not involve operating on the sentence structure, so the modification is likely tobe grammatical.
Another advantage of this transformation is that many languages areprofuse in synonyms so there is a rich source of information carriers in a cover text.In this work we focus on hiding information in English text.
However, the proposedmethods can also be applied to other languages as long as the same resources and toolsare available for the other language, such as synonym dictionaries and n-gram corpora.There are two practical difficulties associated with hiding bits using lexical sub-stitution.
The first is that words can have more than one sense.
In terms of WordNet(Fellbaum 1998), which is the electronic dictionary we use, words can appear in morethan one synonym set (synset).
This is a problem because a word may be assigneddifferent secret bitstrings in the different synsets, and the receiver does not know whichof the senses to use, and hence does not know which hidden bitstring to recover.
Oursolution to this problem is a novel vertex coding method which ensures that words arealways assigned the same bitstring, even when they appear in different synsets.The second problem is that many synonyms are only applicable in certain contexts.For example, the words in the WordNet synset {bridge, span} share the meaning of ?astructure that allows people or vehicles to cross an obstacle such as a river or canalor railway etc.?
However, bridge and span cannot be substituted for each other in thesentence suspension bridges are typically ranked by the length of their main span, and doing sowould likely raise the suspicion of an observer due to the resulting anomaly in the text.Our solution to this problem is to perform a contextual check that utilizes the Googlen-gram corpus (Brants and Franz 2006).
We evaluate the substitution checker using thedata from the English lexical substitution task for SemEval-2007 and a human judgmentcorpus created specifically for this work.For the proposed lexical substitution checker, the higher quality the passed substi-tutions are, the less suspicious the stego text may be.
In addition, the more substitutionsthat pass the check, the more information carriers the stegosystem can use.
Hence theevaluation of the proposed substitution checker can be seen as an indirect evaluation ofthe proposed stegosystem.
For this reason, the performance of the proposed substitu-tion checker is evaluated in terms of precision and recall in our automatic evaluation.Precision is the percentage of substitutions judged acceptable by the checker that arealso in the gold standard of the SemEval-2007 English lexical substitution task; recall isthe percentage of substitutions in the gold standard that are also passed by the checker.The interpretation of the measures for a stegosystem is that a higher precision valueimplies a better security level, whereas a larger recall value means a greater payloadcapacity.
Apart from precision and recall evaluations, we also asked human judges toevaluate the naturalness of the substitutions that pass the proposed checker, which is amore direct evaluation of the imperceptibility of the steganography application.A significant contribution of this article is to advertise the linguistic steganographyproblem to the NLP community.
The requirement that any linguistic transformationsmaintain the grammaticality and meaning of the cover text makes the problem a strongtest for existing NLP technology.
In addition, the proposed substitution checker for409Computational Linguistics Volume 40, Number 2certifying sentence naturalness potentially benefits not only the steganography ap-plication, but also other NLP applications that require a measure of how natural aword is in a particular context.
Another contribution of the work is the evaluation ofthe proposed stegosystems.
The results suggest that it is possible to develop a prac-tical linguistic steganography system based on lexical substitution with current NLPtechniques.The rest of this article is organized as follows.
Section 2 reviews the current state ofthe art in linguistic steganography and the various linguistic transformations that havebeen used in existing stegosystems.
This is a substantial literature review designed tointroduce the problem to the Computational Linguistics reader.
In Section 3, we describeour lexical substitution-based stegosystem, along with the method for checking substi-tution quality together with an empirical evaluation.
In Section 4 we propose a novelvertex coding algorithm to solve the decoding ambiguity problem and demonstrate theproposed stegosystem using an example.2.
BackgroundThis section reviews existing linguistic stegosystems.
Under our interpretation of theterm linguistic steganography, we are only concerned with stegosystems that makechanges that are linguistic in nature, rather than operating on superficial properties ofthe text, for example, the amount of white space between words (Por, Fong, and Delina2008), font colors (Khairullah 2009), or relying on specific file formats, such as ASCII orHTML (Bennett 2004; Shahreza 2006).2.1 Linguistic TransformationsIn the following, we describe the three linguistic transformation categories?lexicalor phrase substitutions, syntactic transformations, and semantic transformations?thathave been used in existing stegosystems to modify cover text.
For each transformation,some examples are provided to demonstrate the text manipulation.2.1.1 Lexical and Phrase Transformations.
There are a few electronic dictionaries availablethat are designed to capture various lexical relationships between words and serve aslexical reference systems (Fellbaum 1998; Schuler 2005).
These can be used to performlexical substitution.
One of the most well-known electronic dictionaries is WordNet(Fellbaum 1998), in which English nouns, verbs, adjectives, and adverbs are categorizedinto synonym sets (synsets).
Words in the same synset have the same or similar meaningTable 1Synsets of the word marry in WordNet 3.1.marry (verb)gloss: take in marriagesynset: marry, get married, wed, conjoin, hook up with, get hitched with, espousegloss: perform a marriage ceremonysynset: marry, wed, tie, splice410Chang and Clark Practical Linguistic Steganographyand in principle can be substituted with each other.
For example, a search result of theword marry in WordNet 3.1 is summarized in Table 1.
According to this table, we canchange the sentence The minister will marry us on Sunday to The minister will wed us onSunday without introducing much semantic difference because marry and wed express asimilar lexical concept in this context.There are three main challenges when using lexical substitution as the linguistictransformation.
The first is word-category disambiguation, which marks up a wordwith a particular POS based on both its definition as well as the context.
For example,fast is an adverb in the phrase hold fast to the rope, an adjective in the phrase a fast car,and a verb in the phrase Catholics fast during Lent.
Existing POS taggers have achieved97% accuracy on the Penn Treebank (Toutanova et al.
2003; Shen, Satta, and Joshi 2007;Spoustova?
et al.
2009; S?gaard 2010) and are widely used in lexical substitution-basedstegosystems (Chapman, Davida, and Rennhard 2001; Bolshakov 2004; Taskiran, Top-kara, and Delp 2006; Topkara, Topkara, and Atallah 2006b; Topkara et al.
2006; Changand Clark 2010b).The second challenge is word-sense disambiguation, which identifies the sense ofa word in context (if the word has more than one meaning) so the correct synset canbe used.
For example, according to the context, bottom means ?a cargo ship?
ratherthan ?the lower side of anything?
in the sentence we did our overseas trade in foreignbottoms, and therefore it can be replaced with freighter but not undersurface.
The firstlexical substitution stegosystem was proposed by Winstein (1999).
In order to han-dle the fact that a word may appear in more than one synset in WordNet, Winsteindefines ?interchangeable?
words as words that belong to exactly the same synsets,and only uses these words for substitution.
For example, marry and wed in Table 1are interchangeable words because they are always synonyms even under differentmeanings.
Any words that are not interchangeable are discarded and not available forcarrying information.
Winstein calculates that only 30% of WordNet can be used in sucha system.The main purpose of linguistic transformations is to generate unsuspicious alterna-tives for a cover sentence.
Although replacing a word with its synonym that conveys thesame concept may preserve the meaning of the sentence, much of the time there are stillsemantic and pragmatic differences among synonyms.
For example, the synset {chase,trail, tail, tag, dog, track}means ?go after with the intent to catch.?
However, an awkwardsentence would be generated if we replaced chase with dog in the sentence the dogs chasethe rabbit.
Hence, it is important to check the acceptability of a synonym in context.Bolshakov (2004) used a collocation-based test to determine whether a substitutionis applicable in context.
Taskiran, Topkara, and Delp (2006) attempted to use contextby prioritizing the alternatives using an n-gram language model; that is, rather thanrandomly choose an option from the synset, the system relies on the language model toselect the synonym.
In Section 3, we describe how our proposed lexical substitution-based stegosystem uses the Google n-gram corpus to certify the naturalness of theproposed substitution.Similar to synonym substitution, text paraphrasing restates a phrase using differentwords while preserving the essential meaning of the source material being paraphrased.In other words, text paraphrasing is multi-word substitution.
For example, we can para-phrase a high percentage of by a large number of in the sentence a form of asbestos has caused ahigh percentage of cancer deaths.
However, text paraphrasing may have more effect on thegrammaticality of a sentence than lexical substitution.
In our earlier work (Chang andClark 2010a) we developed a stegosystem exploiting a paraphrase dictionary (Callison-Burch 2008) to find potential information carriers, and used the Google n-gram corpus411Computational Linguistics Volume 40, Number 2Table 2Some common syntactic transformations in English.Transformation Original sentence Transformed sentencePassivization The dog kissed Peter.
Peter was kissed by the dog.Topicalization I like pasta.
Pasta, I like.Clefting He won a new bike.
It was a new bike that he won.Extraposition To achieve that is impossible.
It is impossible to achieve that.Preposing I like cheese bagels.
Cheese bagels are what I like.There-construction A cat is in the garden.
There is a cat in the garden.Pronominalization I put the cake in the fridge.
I put it there.Fronting ?What!?
Peter said.
?What!?
said Peter.and a combinatory categorial grammar (CCG) parser (Clark and Curran 2007) to certifythe paraphrasing grammaticality.2.1.2 Syntactic Transformations.
Syntactic transformation methods are based on the factthat a sentence can be transformed into more than one semantically equivalent syntac-tic structure, using transformations such as passivization, topicalization, and clefting.Table 2 lists some of the common syntactic transformations in English.6The first syntactic transformation method was presented by Atallah et al.
(2000).Later, Atallah et al.
(2001) generated alternative sentences by adjusting the structuralproperties of intermediate representations of a cover sentence.
In other words, insteadof performing lexical substitution directly on the text, the modifications are performedon the syntactic parse tree of a cover sentence.
Murphy (2001), Liu, Sun, and Wu(2005), Topkara, Topkara, and Atallah (2006a), Meral et al.
(2007), Murphy and Vogel(2007b), and Meral et al.
(2009) all belong to this syntactic transformation category.
Aftermanipulating the syntactic parse tree, the modified deep structure form is convertedinto the surface structure format via language generation tools.Aside from these systems, Wayner (1995) and Chapman and Davida (1997) pro-posed mimicry text approaches associated with linguistic syntax.
These two stegosys-tems generate stego text from scratch instead of modifying an existing text.
Wayner(1995) proposed a method with his context-free mimic function (Wayner 1992) to gen-erate a stego text that has statistical properties close to natural language.
The context-free mimic function uses a probabilistic grammar-based model to structure the stegotext.
Because the mimicry method only puts emphasis on the syntactic structure of asentence, it is likely to generate nonsensical stego text which is perceptible by humans.Chapman and Davida (1997) developed a stegosystem called NICETEXT that generatesstego sentences using style sources and context-free grammars to simulate certain as-pects of writing style.
Compared to Wayner?s mimicry method, the stego text generatedby NICETEXT is more natural in terms of the semantics, but still not at a level thatwould be suitable for practical steganography.2.1.3 Semantic Transformations.
The semantic transformation is the most sophisticatedapproach for linguistic steganography, and perhaps impractical given the current state-of-the-art for NLP technology.
It requires some sophisticated tools and knowledge to6 The categories of transformations are adopted from Topkara, Taskiran, and Delp (2005).412Chang and Clark Practical Linguistic SteganographyAfghanistan (nation)borders-on China, Iran, Pakistan, Tajikistan, Uzbekistanhas-currency afghanihas-member Pashtun, Tajik, Hazara, Uzbekhas-representative Mullah Mohammad OmarFigure 3Parts of the ontological semantics for Afghanistan.assault?|?agent?nation?
?United States?|?theme?nation??Afghanistan?(a)assault?|?agent?nation?
?United States?|?theme?nation??Afghanistan??|?has-representative?politician?
?Mullah Mohammad Omar?
(b)Figure 4An example of the TMR tree modification taken from Atallah et al.
(2002).model natural language semantics and to evaluate equivalence between texts in order toperform deep semantic manipulations.
For example, consider the following sentences:Bond takes revenge for Vesper?s death.Vesper?s death is avenged by Bond.007 takes revenge for Vesper?s death.The idea is to define the semantic representation in such a way that the translation fromany of these sentences to their semantic representations would yield the same form.In this manner, the meaning of the cover sentence can be expressed in another naturallanguage text.
For this to be successful for the example, we would have to understandthe sentences in different voices, such as active and passive, and make use of someworld knowledge, such as the fact that the codename of James Bond is 007.The work of Atallah et al.
(2002) used semantic transformations and aimed tooutput alternatives by modifying the text-meaning representation (TMR) tree of acover sentence.
The modifications include pruning, grafting, or substituting the treestructure with information available from ontological semantic resources.
A linguisticontology is a formal knowledge representation of the world; a conceptualization ofentities, events, and their relationships in an abstract way.
For example, Figure 3, takenfrom Atallah et al.
(2002), shows parts of the ontological semantics for Afghanistanthat are structured in a tree-like hierarchy.7 An ontology provides concepts that areused to define propositions in TMR.
The TMR of a natural language expression canshow information such as clause relationship, author attitude, and topic composition.It is constructed through mapping lexical items and events that are referred in theexpression to their ontology concepts.
Figure 4(a) shows the TMR of the sentence theUnited States are attacking Afghanistan.
A modification of the tree can be performedby grafting additional semantic information of Afghanistan as shown in Figure 4(b),yielding the alternative sentence the United States are attacking Afghanistan, which is ruledby Mullah Mohammed Omar.
Vybornova and Macq (2007) also exploited the linguistic7 Afghanistan was ruled by Mullah Mohammed Omar at the time of Atallah et al.
(2002).413Computational Linguistics Volume 40, Number 2phenomenon of presupposition, with the idea that some presuppositional informationcan be removed without changing the meaning of a sentence.Another group of studies aims to use machine-translated sentences as the alterna-tives.
The main advantage of using machine-translated text is that translations are notperfect and therefore it is hard to determine whether the anomalies are introduced by atranslation system or due to the camouflage of secret information.The first translation-based stegosystem was proposed by Grothoff et al.
(2005).
Intheir method, the sender uses a set of machine translation systems to generate multipletranslations for a given cover sentence.
Stutsman et al.
(2006) also utilized multipletranslation systems to output alternatives for a cover sentence.
Because Grothoff et al.
(2005) and Stutsman et al.
(2006) used multiple machine translation systems to generatealternative translations, which leads to a stego text containing a mixture of translationsgenerated from different systems and each stego sentence may have different statisticaldistribution of features (e.g., percentage of high-frequency words), a simple comparisonof the statistical distribution of features obtained from a normal text and from a stegotext might be able to detect the existence of the secret message (Meng et al.
2010;Chen et al.
2011).
Instead of obtaining alternative translations from multiple translationsystems, Meng et al.
(2011) and Venugopal et al.
(2011) used a statistical machinetranslation system to generate the n-best translations for a given cover sentence.
Becausetranslations are from one system, each of them is more similar to the rest than thatderived from another translation system.Another of our papers (Chang and Clark 2012b) proposed a word-ordering-basedstegosystem, where the word-ordering technique can be seen as a ?monolingual trans-lation?
that translates a cover sentence into different permutations.
Because not allthe sentence permutations generated by a word-ordering system are grammatical andsemantically meaningful, we developed a maximum entropy classifier to distinguishnatural word orders from awkward ones.Another possible semantic transformation for linguistic steganography is sentencecompression (Dorr, Zajic, and Schwartz 2003; Cohn and Lapata 2008; Zhu, Bernhard,and Gurevych 2010).
Different compressed versions of the original text provide variousalternatives for a stegosystem.
We developed a compression-based stegosystem thatgenerates alternatives for a cover sentence by removing unnecessary adjectives in nounphrases (Chang and Clark 2012a).
For example, he spent only his own money and hespent only his money almost express the same meaning.
In order to certify the deletiongrammaticality, we only accept a deletion that does not change the CCG categories ofthe words in the rest of the sentence.
In addition, we propose two methods to determinewhether the adjective in a noun phrase is necessary to the context.
The first methoduses the Google n-gram corpus, and the second method, which performs better, trainsa support vector machine model that combines n-gram statistics, lexical associationmeasures, entropy-based measures, and an n-gram divergence statistic.2.2 Encoding MethodsIn the previous sections we have explained different transformation methods for gen-erating alternatives for an input text.
This procedure is seen as the linguistic transfor-mation module in Figure 2.
After deriving alternatives for a cover text, the encodergeneration module maps each alternative to a code that can be used to represent a secretbitstring.
In this section, we introduce four encoding methods that assign bitstringsto the alternative candidates and have been used in existing stegosystems.
In orderto demonstrate each encoding method, we assume the cover sentence is we finish the414Chang and Clark Practical Linguistic Steganographycharitable project and the transformation applied to the text consists of simply replacinga word with its synonym.
The alternatives for the cover text arise from replacingfinish with complete, and replacing project with labor, task, or undertaking.
Note that, asmentioned earlier, linguistic transformations are largely independent of the encodingmethods and therefore the encoding methods explained here are not restricted to lexicalsubstitutions.
After encoding the alternatives, the secret can be embedded by selectingalternatives that directly associate with the secret bitstring.2.2.1 Block Code Method.
For a set with cardinality n, the block code method assignsm-bit binary codes from 0 to 2m ?
1 to the elements in the set, where 2m ?
n. For exam-ple, the synonym set {complete, finish} has cardinality n = 2 so 1-bit binary codes 0 and 1are assigned to complete and finish, respectively.
Because the synonym set {labor, project,task, undertaking} has cardinality n = 4, the block code method can use either one-bit or2-bit codes to encode the words as shown in Figure 5.
When one-bit codes are used, bothlabor and task represent code 0, and both project and undertaking represent code 1; whentwo-bit codes are used, the four words are assigned different codes 00, 01, 10, and 11.The advantage of using one-bit codes is that the cover word project needs to be replacedwith its synonym only 50% of the time, whereas the two-bit scheme has a 75% chance ofmodifying the cover word, assuming the secret is a random bitstring.
However, one-bitcodes embed less information.
Hence, there is a trade-off between security and payloadcapacity.
It is worth noting that, in this simple scheme, each block code representationhas the same probability of being chosen, even though native speakers might have apreference for the choice of synonyms, which would be security-relevant.
For example,if the block code method is applied to Table 1, wed, tie, and splice would have the sameprobability of replacing marry in the cover sentence The minister will marry us on Sunday.However, of these three alternatives only wed is allowed in this context; hence choosingtie or splice may arouse suspicion in others.2.2.2 Mixed-Radix Number Method.
In a mixed-radix number system, the numerical basediffers from position to position.
For example, 8 hours, 41 minutes, and 21 secondscan be presented relative to seconds in mixed-radix notation as: 8(24)41(60)21(60), whereeach digit is written above its associated base.
The numerical interpretation of a mixed-radix number an(bn )an?1(bn?1 )...a0(b0 ) is anbn?1bn?2...b0 + an?1bn?2bn?3...b0 + ...+ a1b0 +a0, and any number can be uniquely expressed in mixed-radix form (Soderstrand et al.1986).Figure 6 shows the use of the mixed-radix number method with the lexical substi-tution example which is described in Bergmair (2004).
Firstly, the words in the synsets{complete, finish} are encoded with 0 and 1 with base 2, and the words in the synset{labor, project, task, undertaking} are encoded with 0, 1, 2, and 3 with base 4.
Therefore,the combinations of the substitutions yield the two-digit mixed-radix numbers from0204 to 1234, which are equal to the decimal numbers 0 to 7.
Assume the secret bitstring1-bit Word 1-bit 2-bit WordWe 0 complete the charitable 0 00 labor .1 finish 1 01 project0 10 task1 11 undertakingFigure 5An example of the block code method.415Computational Linguistics Volume 40, Number 2Code Word Code WordWe 02 complete the charitable 04 labor .12 finish 14 project24 task34 undertakingFigure 6An example of the mixed-radix number method.to be embedded is 110, which can be seen as the binary number for six.
Because wefinish the charitable task represents the mixed-radix number 1224, which is the decimalnumber 6, this sentence will be the stego sentence that embeds the secret.
Like theBlock code method, each mixed-radix number representation has the same probabilityof being chosen, which may have security implications.
To solve this issue, one canutilize variable-length code methods described in the next section.2.2.3 Huffman Code Method.
Figure 7 demonstrates the use of variable-length codes, in theform of the Huffman code (Huffman 1952), for encoding words in a synset.
Assumingthere is a utility score for each word, then the Huffman algorithm determines a wayto produce a variable-length binary string for each word.
More importantly, it does soin such a way that an optimal encoding is created; that is, words with higher utilityhave shorter codes whereas words with lower utility get longer codes.
Thus, wordsfrequently used by native speakers are more likely to be chosen by the stegosystem(assuming utility corresponds to frequency).
In addition, when making a low-utilitychoice, the tree ensures that maximal (bitrate) benefit will be derived from that choice.The process shown in Figure 8 begins with leaf nodes each containing a word along withits associated probability.
The two nodes with the smallest probabilities are then chosento become the children of a new node whose probability is the sum of the probabilitiesof its children.
The newly created left and right branches are assigned bit 0 and 1,respectively.
Now only the newly created node is taken into consideration instead of itschildren.
The procedure is repeated until only one node remains, thereby constructingthe Huffman tree.
To determine the binary code assigned to a particular word, we startfrom the root node and gather the bits on the path to the leaf node connected to thatword.
In this example we can see that project has the highest probability among wordsin the same synset and is encoded with the shortest code.
Thus, it is more likely to matchthe secret bitstring.One existing stegosystem that uses Huffman code encoding is proposed by Grothoffet al.
(2005).
Their system takes different translations of a cover sentence as alternatives,each of which is assigned a probability that represents the quality of the translation.Then a Huffman tree is built for the translations based on the probabilities, wherepoorer translations are at the bottom of the tree (with lower probabilities), and qualityCode Word Prob.
Code Word Prob.We 0 complete 0.77 the charitable 110 labor 0.05 .1 finish 0.23 0 project 0.6910 task 0.25111 undertaking 0.01Figure 7An example of the Huffman code method.416Chang and Clark Practical Linguistic SteganographyFigure 8The process of constructing a Huffman tree.translations are higher in the tree.
In other words, poor translations have longer codeswhile better translations have shorter codes.
This ensures that quality translations ap-pear more often, and when a poorer translation (and thus potentially more perceptiblesentence) appears, it transmits a maximal number of bits.As described in Section 2.1.2, Wayner (1995) generates stego text by exploiting aprobabilistic context-free grammar.
His method creates a Huffman tree for each set ofproductions that expand the same non-terminal symbol.
In this way, each productionhas its own Huffman code representation, as shown in Figure 9(a).
Then, we beginwith a designated start-symbol S, and expand a non-terminal symbol by choosingthe production whose Huffman code representation is identical to the portion of theRule No.
Rule Code Probability1.
S?AB 0 0.32.
S?AC 1 0.73.
A?I 0 0.44.
A?You 10 0.35.
A?He 110 0.156.
A?She 111 0.157.
B?lost 0 0.48.
B?won 1 0.69.
C?lost the D 0 0.410.
C?won the D 1 0.611.
D?game 0 0.412.
D?match 10 0.313.
D?championship 110 0.214.
D?competition 111 0.1(a)Position Prefix Rule Output?1101110 1 2.
AC1?101110 10 4.
You C110?1110 1 10.
You won the D1101?110 110 13.
You won the championship(b)Figure 9An example of the Wayner (1995) mimicry method.417Computational Linguistics Volume 40, Number 2secret bitstring.
The procedure is repeated until a grammatical message is generated.In the embedding example given in Figure 9(b), the secret bitstring is 1101110 and asymbol ???
is used to indicate the current bit in reading the string.
At the beginning,the prefix string of the secret message ?1101110 is ?1?
which is associated with thesecond production, so the start-symbol S is expanded to AC.
Now, the prefix string ofthe message 1?101110 becomes ?10?.
The fourth production is applied, and a string?You C?
is generated.
Next, we see the prefix string ?1?
in the message 101?110,and therefore, the output string turns into ?You won the D?.
Finally, the end of thesecret message 1101110?
is reached, and a stego sentence You won the championship isgenerated.
Theoretically, the block code representation or the mixed-radix techniqueexplained in the previous sections can be utilized in Wayner?s stegosystem.2.2.4 Hash Function Method.
For the block code method, the mixed-radix number ap-proach, and the Huffman code representation, the encoding process is dependent onknowing all the alternatives (e.g., the synset).
Hence, in order to extract the codeassigned to the stego text during the secret recovery process, all the alternatives must beknown to the receiver as well.
Note that the receiver does not need to know the originalcover text.
However, not all the linguistic transformations can meet this requirement.For example, if the sender encodes the four best machine translations of the coversentence using block coding and sends the translation that represents the secret bits tothe receiver, it is unlikely that the receiver can retrieve the four best machine translationswithout knowing the original cover sentence.
Thus, the secret recovery fails.
For thisreason, Stutsman et al.
(2006), Meng et al.
(2011), Venugopal et al.
(2011), and Chang andClark (2012b) used a hash function to map a translation to a code, which is independentof the rest of the alternatives.Venugopal et al.
(2011) defined a random hashing operation that maps a translationto a bit sequence of fixed length.
Venugopal et al.
stated that a good hash functionshould produce a bitstring whose 0s and 1s are generated with equal probability.Stutsman et al.
(2006) proposed a hash function encoding scheme which uses the firsth bits of a translation hash bitstring as the header bits and the next b bits as the coderepresented by the translation, where h is shared between the sender and the receiver,and b is the integer represented by the header bits.
For example, assume h = 2; a hashbitstring ?1011.
.
.
?
has header bits 10 to indicate a 10(2)-bit code is carried by thistranslation, and the two-bits are 11.
Among all possible translations of a cover sentence,the one with the combination of header and information-carrying bits for the given hrepresenting the next b bits of the message is chosen as the stego sentence.2.3 Stegosystem EvaluationsSo far we have introduced different linguistic transformations used to produce alter-natives for a cover text as well as some encoding methods that are used to assign abitstring to a candidate.
The final procedure is text selection, in which an alternativethat represents the secret bits is chosen as the stego text.
We can see that the qualityof a stego text mainly relies on the quality of the applied linguistic transformation,typically requiring sophisticated NLP tools and resources to produce a realistic stegotext.
However, given the current state-of-the-art, such NLP techniques cannot guaranteethe transformation?s imperceptibility.
Hence it is important to evaluate a stegosystem.A stegosystem can be evaluated from two aspects: the security level and the embed-ding capacity.
The security assessment methods used so far can be classified into twocategories: automatic evaluation and human evaluation.
Topkara, Topkara, and Atallah418Chang and Clark Practical Linguistic Steganography(2006a) and Topkara et al.
(2006) used machine translation evaluation metrics BLEU(Papineni et al.
2002) and NIST (Doddington 2002), automatically measuring how closea stego sentence is to the original.
Topkara, Topkara, and Atallah (2006a) admitted thatmachine translation evaluation metrics are not sufficient for evaluating stegosystems;for example, BLEU relies on word sequences in the stego sentence matching those inthe cover sentence and thus is not suitable for evaluating transformations that changethe word order significantly.The other widely adopted evaluation method is based on human judgments.
Meralet al.
(2007, 2009) and Kim (2008, 2009) asked participants to edit stego text for im-proving intelligibility and style.
The fewer edit-hits a transformed text received, thehigher the reported security level.
Murphy and Vogel (2007a, 2007b) first asked subjectsto rate the acceptability (in terms of plausibility, grammaticality, and style) of the stegosentences on a seven-point scale.
Then participants were provided with the originalsand asked to judge to what extent meaning was preserved, also on a seven-point scale.In Chang and Clark (2010a) we asked participants to judge whether a paraphrasedsentence is grammatical and whether the paraphrasing retains the meaning of theoriginal.
In Chang and Clark (2012a) we asked participants to annotate the naturalnessof the resulting sentences after adjective deletions; and in Chang and Clark (2012b) weasked participants to rate the naturalness of sentence permutations on a four-point scale.For the work presented in this article, we also use human judgments to evaluate theproposed stegosystem, as this is close to the linguistic steganography scenario wherewe assume the adversary is a human acting passively.The other aspect of the stegosystem evaluation is to calculate the amount of datacapable of being embedded in a stego text, which can be quantified in terms of bitsof hidden message per bit transmitted or per language unit (e.g., per word or persentence).
Payload measurements can be theoretical or empirical.
The theoretical pay-load measurement only depends on an encoding method and is independent of thequality of a stego text; the empirical measurement takes the applicability of a linguistictransformation, namely, the security of a stego text, into consideration and measuresthe payload capacity while a certain security level is achieved.
Most of the payloadrates reported in existing work are based on empirical measurements.For the lexical substitution transformation, Topkara, Taskiran, and Delp (2005) andTopkara, Topkara, and Atallah (2006b) achieved an average embedding payload of0.67 bits per sentence, despite the large number of synonyms in English.
In Changand Clark (2012a) we showed that the payload upper bound of using the adjectivedeletion technique is around 0.4 bits per sentence if a deletion represents a secretbit.
The payload attained by syntactic transformations was around 0.5 to 1.0 bits persentence.
For example, both Atallah et al.
(2001) and Topkara, Topkara, and Atallah(2006a) achieved an embedding payload of 0.5 bits per sentence, and Meral et al.
(2009)reported the data embedding rate of their system as 0.81 bits per sentence.
Because theontological semantic transformation is currently impractical, the empirical payload isnot available for this transformation type.
Another semantic method (Vybornova andMacq 2007) that aims at modifying presuppositional information in text achieved apayload of 1 bit per sentence through the use of a secret key to indicate sentenceswith or without presupposition information.
Stutsman et al.
(2006) showed that theirtranslation-based stegosystem has a payload of 0.33 bits of hidden message for every100 bits of data transmitted.Not only the linguistic transformation and the encoding method, but also the choiceof cover text, can affect the security level and the payload capacity of a stegosystem.For example, if a newspaper article were chosen as the cover text, then any changes419Computational Linguistics Volume 40, Number 2could be easily found in practice by comparing the stego text with the original article,which is likely to be readily available.
In addition, an anomaly introduced by a linguistictransformation may be more noticeable in a newspaper article than in a blog article.
Interms of payload capacity, a synonym substitution?based stegosystem may find morewords that can be substituted in a storybook than in a car repair manual because thereare usually many terminologies in a manual which cannot be changed or even cannot befound in a standard dictionary (assuming the system does not happen to have a detailedontology of car parts).
To the best of our knowledge, there is no study on the practicalissue of using different types of cover text for the steganography application.3.
Lexical SubstitutionIn the following sections we introduce our linguistic stegosystem based on lexicalsubstitution.
In the original work on linguistic steganography, Winstein (1999) proposedan information-hiding algorithm using a block coding method to encode synonyms, sothat the selection of a word from a synset directly associates with part of the secretbitstring.
An example of Winstein?s system can be found in Figure 5.
In his system, asender and a receiver share the same coded synonym dictionary as the secret key.
Torecover the hidden message, the receiver first seeks words in the stego text that can befound in the shared dictionary.
Those words are information carriers, and therefore thecodes assigned to them are secret bitstrings.
Note that the receiver does not need theoriginal cover text to recover the secret message.One of the problems faced by a synonym-based stegosystem is that many words arepolysemous, having more than one sense, and this may cause ambiguities during thesecret recovery stage.
In WordNet a synset contains words expressing a similar concept,and a word may appear in more than one synset.
For example, both marry and wedappear in the two synsets in Table 1.
Figure 10 shows what happens when the blockcoding method is applied to the two overlapping synsets, assuming the stego sentencereceived by the receiver is the minister will marry us on Sunday.
Note that we only takesingle word substitution into consideration in order to avoid the confusion of findinginformation carriers during the secret recovering phase.
For example, if the cover wordespouse is replaced by hook up with, the receiver would not know whether the secretmessage is embedded in the word hook or the phrase hook up with.
After deleting multi-word synonyms, words in the two synsets are sorted alphabetically and assigned two-bit codes.
As can be seen in Figure 10, marry is encoded by two different codewordsand thus the secret bitstring cannot be reliably recovered, because the receiver does notknow the original cover word or the sense of the word.In order to solve the problem of words appearing in more than one synonymset, Winstein defines interchangeable words as words that are always synonyms toeach other even under different meanings (i.e., they always appear together in theSynset 1 Synset 2Word Code Word Codeconjoin 00 marry 00espouse 01 splice 01marry 10 tie 10wed 11 wed 11Figure 10An example of decoding ambiguity using lexical substitution.420Chang and Clark Practical Linguistic Steganographysame synsets).
For example, marry and wed are interchangeable words under Winstein?sdefinition.
The advantage in this approach is that interchangeable words always receivethe same codeword.
The disadvantage is that many synonyms need to be discardedin order to achieve this property.
As mentioned previously, Winstein reported thatonly 30% of words in WordNet are interchangeable words.
In addition, as explainedin Section 2.1.1, many synonyms are only applicable in certain contexts.
However, inWinstein?s steganography scheme there is no method to filter out unacceptable substi-tutions so the generated stego text may be unnatural and arouse suspicion in others.Another synonym substitution-based stegosystem was proposed by Bolshakov(2004), who applies transitive closure to overlapping synsets to avoid the decodingambiguity.
Applying transitive closure leads to a merger of all the overlapping synsetsinto one set which is then seen as the synset of a target word.
Consider the overlappingsynsets in Figure 10 as an example.
After applying transitive closure, the resulting set is{conjoin, espouse, marry, splice, tie, wed}.
The disadvantage of Bolshakov?s system is thatall words in a synonym transitive closure chain need to be considered, which can lead tovery large sets of synonyms, many of which are not synonymous with the original targetword.
For this reason, Bolshakov used a collocation-based test to remove unsuitablewords after merging the synsets.
Finally, the collocationally verified synonyms areencoded using the block coding method.
Note that in Bolshakov?s system it is possibleto replace an original word with a non-synonymous word if the non-synonymousword passes the collocation-based test.Similar to Bolshakov?s method, our approach takes words in a synonym transitiveclosure chain into consideration and assigns a score to each word using the proposedsubstitution checker.
A score threshold is applied to eliminate low-score words; thatis, the remaining words are both in the synonym transitive closure chain as well asacceptable to the context.
More details of the proposed substitution checker will bedescribed later.
We then construct a synonym graph that has a vertex for each remainingword and an undirected edge for every pair of words that share the same meaning.After constructing the synonym graph, we use a novel vertex coding method inspiredby vertex coloring to assign codes to every word in the graph.A crucial difference from Bolshakov?s method is that in our approach the senderonly considers words that are synonymous with the cover word as alternatives, eventhough the other words in the synonym graph can also fit into the context.
The reasonfor also including non-synonymous words during the encoding is because the receiverdoes not know the cover word and, therefore, we need a method to ensure that thereceiver is encoding the same list of words, namely, the same synonym graph, as thesender during the secret recovery.
In other words, the sender and the receiver mustderive the same synonym graph so that the sender knows the cover word and thereceiver knows the stego word.Figure 11(a) shows a synonym graph constructed from a synonym transitive closurechain that contains six synsets: {bind, tie}, {tie, draw}, {tie, wed, splice, marry}, {marry,wed, espouse, conjoin}, {conjoin, join}, {join, link, unite, connect}.
Assume the cover wordis conjoin.
In Bolshakov?s system, there is a chance of replacing conjoin with draw, whichis three steps away from the original word in the graph; in our method, however,we only consider a cover word?s synonyms as alternatives?that is, conjoin is onlyallowed to be replaced by wed, espouse, marry, or join.
Note that we have not appliedthe substitution check in this example.Now let us apply the substitution check to words in the synonym transitive closurechain, and suppose join and marry do not pass the check.
Figure 11(b) shows thetwo disconnected synonym graphs G1 and G2 derived from the checked pool.
The two421Computational Linguistics Volume 40, Number 2(a) An unchecked synonym graph(b) Synonym graphs derived after substitution checking(c) Another example of checked synonym graphsFigure 11Synonym graphs with and without the substitution check.synonym graphs are then encoded independently.
In other words, the encoding of G1does not affect the codes assigned to the words in G2.
Because conjoin is the cover word,the system may replace conjoin with either wed or espouse, or keep the original worddepending on the encoding of G1 and the secret bits.
Assume wed is chosen as the stegoword.
In order to work out the embedded message, the receiver needs to construct andencode the same graphs as those generated by the sender.
The decoding process startsfrom extracting the synonym transitive closure chain of wed, and then applying thesubstitution checker to the pool to filter out unacceptable words.
Because the remainingwords are the same as those used by the sender, the receiver can successfully extractthe secret bits after constructing and encoding the synonym graphs.Because the proposed substitution checker measures the acceptability of a wordaccording to the context, the synonym graph for a target word varies depending on itscontext.
Let us consider another case where the cover word is still conjoin, but this timethe substitution checker determines that conjoin, espouse, and marry are not acceptable tothe context.
Figure 11(c) shows the corresponding synonym graphs of the remainingwords.
In this case, the applicable alternatives are either wed or join because theyare synonyms of conjoin.
As mentioned previously, disconnected graphs are encodedindependently.
Therefore, it is possible that both wed and join are assigned the samecodeword which does not match the secret bits.
If neither of the synonyms can be usedas the stego word, the sender will keep the original word and send conjoin to the receiver.During the decoding process, the receiver should be able to know that conjoin fails thecheck and thus does not carry any message.
In contrast, if wed and join are encoded422Chang and Clark Practical Linguistic Steganographyby different codewords, say 0 and 1, respectively, the system can choose the one thatrepresents the secret bit as the stego word.3.1 Substitution CheckersThe aim of the proposed checkers is to filter out inapplicable substitutes given the orig-inal word in context.
The substitution checkers must not only work with the proposedlinguistic stegosystem, but can also be integrated into other synonym substitution-based applications to certify the transformation quality.
The following sections areorganized so that the basic substitution checker using the Google n-gram corpus (Brantsand Franz 2006) is described first.
Then we introduce the ?-skew divergence measure(Lee 1999) that can be combined with the basic n-gram method.
The proposed checkersare evaluated using data from the SemEval lexical substitution task (McCarthy andNavigli 2007), which is independent of the steganography application.
We also performa more direct evaluation of the imperceptibility of the steganography application byasking human judges to evaluate the naturalness of sentences.
After explaining thelinguistic transformation module in our stegosystem, we proceed with the encodergeneration module and present the vertex coding method.
Finally, we use an exampleto demonstrate the complete stegosystem.3.1.1 n-Gram Count Method (NGM).
The basic checking method, referred to as NGM,utilizes the Google n-gram corpus to calculate a substitution score for a candidate wordin context based on Bergsma, Lin, and Goebel (2009).
The Google n-gram corpus wascollected by Google Research for statistical language modeling, and has been used formany tasks such as spelling correction (Carlson, Mitchell, and Fette 2008; Islam andInkpen 2009), multi-word expression classification (Kummerfeld and Curran 2008), andlexical disambiguation (Bergsma, Lin, and Goebel 2009).
It contains frequency countsfor n-grams from uni-grams through to 5-grams obtained from over 1 trillion wordtokens of English Web text.
Only n-grams appearing more than 40 times were kept inthe corpus.The checking method first extracts contextual bi- to 5-grams around the word to betested and uses the Minnen, Carroll, and Pearce (2001) tools for correcting the form ofan indefinite and a verb?s tense.
For example, if the word to be tested is maverick and it isgoing to replace unorthodox in the phrase the help of an unorthodox speech therapist namedLionel, the indefinite an will be corrected as a when extracting contextual n-grams.
Asanother example, assume the word to be replaced is bleach in the original phrase hemight be bleaching his skin; then a verb substitute decolor will be corrected as decoloringbecause the original word is in the progressive tense.After extracting contextual bi- to 5-grams, the checking method queries the n-gramfrequency counts from the Google n-gram corpus.
For each n, the total count fn iscalculated by summing up individual n-gram frequencies, for every contextual n-gramcontaining the candidate word.
We define a count function:Count(w) =5?n=2log(fn),where log(0) is defined as zero.
If Count(w)=0, we assume the word w is unrelated tothe context and therefore is eliminated from the synonym transitive closure chain.
Aftercalculating Count(w) for each word in the pool, the word that has the highest count423Computational Linguistics Volume 40, Number 2is called the most likely word and its count is referred as maxcount.
The main purpose ofhaving maxcount is to score each word relative to the most likely substitute in the chain,so even in less frequent contexts which lead to smaller frequency counts, the score ofeach word can still indicate the degree of feasibility.
We also need to use the most likelyword, rather than the original cover word, because the receiver does not have access tothe cover text when applying the check.
The most likely word in the context may be theoriginal word or another word in the synonym transitive closure chain.
The substitutionscore is defined as:ScoreNGM(w) =Count(w)maxcountThe hypothesis is that a word with a high score is more suitable for the context, and weapply a threshold so that words having a score lower than the threshold are discarded.Consider as an example the calculation of the substitution score for the candidateword clever as a possible replacement for the word bright in the cover sentence hewas bright and independent and proud.
First of all, various contextual n-grams are ex-tracted from the sentence and the Google n-gram corpus is consulted to obtain theirfrequency counts, as shown in Figure 12.
The derived fn values can then be used tocalculate Count(clever), which is 27.5 in this example.
Suppose the threshold is 0.9, andthe maxcount is 30 from the synonym transitive closure chain.
The substitution score isas follows:ScoreNGM(clever) =Count(clever)maxcount =27.530 = 0.92which is greater than the threshold (0.9), and so the word clever is determined asacceptable for this context and is kept in the pool.One disadvantage of using n-gram statistics is that high-frequency n-grams maydominate the substitution score, especially lower-order n-grams.
For example, even isnot a good substitute for eve in the sentence on the eve of the wedding, Miranda tellsMr.
Big that marriage ruins everything, but it still has a reasonably high score of 0.74since the bigrams the even and even of have high frequency counts compared with thoseof the 4-grams and 5-grams.
As a way of overcoming this problem, we consider then-gram frequency fnwas clever 40,726 f2 = 302,492clever and 261,766He was clever 1,798 f3 = 8,072was clever and 6,188clever and independent 86He was clever and 343 f4 = 343was clever and independent 0clever and independent and 0He was clever and independent 0 f5 = 0was clever and independent and 0clever and independent and proud 0Figure 12n-grams and their frequency counts for calculating ScoreNGM(clever).424Chang and Clark Practical Linguistic SteganographyC21 C22 C31 C32 C33 C41 C42 C43 C51 C52 C53bright 0.081 0.892 0.002 0.024 0.0002 0 0 0 0 0 0clever 0.130 0.843 0.006 0.020 0.0002 0.001 0 0 0 0 0Figure 13n-gram frequency distributions of bright and clever for calculating the contextual divergence.n-gram distributional similarity between a most likely word and a candidate substitutein context using ?-skew divergence as explained in the next section.
We assume that anacceptable substitute should have a similar n-gram distribution to the most likely wordacross the various n-gram counts.3.1.2 Contextual ?-skew Divergence.
The ?-skew divergence is a non-symmetric measureof the difference between two probability distributions P and Q.
Typically, P representsthe observations, in our case the n-gram count distribution of the most likely word, andQ represents a model, in our case the candidate?s distribution.
The ?-skew divergencemeasure is defined as:S?
(Q, P) = D(P??
?Q + (1?
?
)?P)where 0 ?
?
?
1 and D is the Kullback-Leibler divergence (Kullback 1959):D(P?Q) =?vP(v) logP(v)Q(v)The ?
parameter allows us to avoid the problem of zero probabilities, and in ourmethod we use ?
= 0.99.
The value of the ?-skew divergence measure is zero if thetwo probability distributions are identical and increases positively as the distributionsbecome less similar.We use the following example to demonstrate how to calculate the contextualdivergence between two words.
Assume the most likely word is bright and a substituteto be considered is clever.
First we need to derive the n-gram frequency distributionsof both words.
We divide each n-gram frequency by the total frequency to get Cni, asshown in Figure 13, where i means the ith n-gram (e.g., C32 is the second trigram).
For aword, Cni should sum up to 1 (over all n, i).
Then we can calculate the?-skew divergenceof these two distributions:S?
(clever,bright) =?n?iCbrightni ?
log(Cbrightni?Ccleverni + (1?
?
)Cbrightni) = 0.014Similar to the NGM method, we define a score function:ScoreDVG(w) = 1?S?(?
?w ,?????????????
?the most likely word)maxdivergencewhere ?
?w and?????????????
?the most likely word are the probability distributions of n-gram countsof the target substitute and the most likely word, respectively, and maxdivergence is themaximum divergence between the most likely word and another word in the synonym425Computational Linguistics Volume 40, Number 2transitive closure chain.
In this example, suppose maxdivergence is 0.15 and therefore wecan derive the contextual divergence-based substitution score:ScoreDVG(clever) = 1?0.0140.15 = 0.91The reason to calculate S?(?
?w ,???????????
?the most likely word)maxdivergence is to spread the divergence score between0 and 1.
Note that the higher the divergence S?(?
?w ,?????????????
?the most likely word) is, the lowerthe score ScoreDVG(w).
Finally we combine the distributional similarity with the NGMmethod, referred to as NGM DVG method, by modifying the score function as follows:ScoreNGM DVG(w) = ?
?ScoreNGM(w) + (1?
?
)?ScoreDVG(w)where 0 ?
?
?
1.
The value of ?
determines the relative weights of ScoreNGM(w) andScoreDVG(w).3.2 Ranking Task EvaluationBoth NGM and NGM DVG assign a score to a word according to the context and themost likely word in the group of alternatives.
In order to evaluate the performanceof the proposed scoring methods, we apply our approaches to a ranking task thatrequires a system to rank a list of substitute words given an original word and itscontext.
The task can test whether the proposed methods are capable of assigning higherscores to appropriate substitutes than to unacceptable ones and thus is useful for thesteganography application.
The gold standard data is derived from the English lexicalsubstitution task for SemEval-2007 (McCarthy and Navigli 2007) and the evaluationmeasure used is Generalized Average Precision (Kishida 2005).
In this section we firstdescribe the gold standard data used in this evaluation and then provide the results.
Wecompare our results with three other models developed by Erk and Pado?
(2010), Dinuand Lapata (2010), and O?
Se?aghdha and Korhonen (2011), all of which are designedfor measuring word meaning similarity in context.
Note that our substitution checkersdo not aim at modeling word similarity, and therefore the result comparison is justtrying to show that our substitution checkers are competitive.
Later, we will evaluatethe proposed checkers with the human annotated data and see whether our methodswould be practical for linguistic steganography.3.2.1 Data.
For this evaluation, we use the SemEval-2007 lexical substitution data set asthe gold standard.
The original purpose of the data set was to develop systems thatcan automatically find feasible substitutes given a target word in context.
The humanannotation data comprises 2,010 sentences selected from the English Internet Corpus(Sharoff 2006), and consists of 201 target words: nouns, verbs, adjectives, and adverbs,each with ten sentences containing that word.
The five annotators were asked to provideup to three substitutes for a target word in the context of a sentence, and were permittedto consult a dictionary or thesaurus of their choosing.
After filtering out annotationsentences where the target word is part of a proper name and for which annotatorscould not think of a good substitute, the data was separated into 298 trial sentencesand 1,696 test sentences.
Table 3 illustrates two examples from the gold standard, bothfeaturing the target word bright.
The right column lists appropriate substitutes of bright426Chang and Clark Practical Linguistic Steganographyin each context, and the numbers in parentheses indicate the number of annotators whoprovided that substitute.To allow comparison with previous results reported on the substitution rankingtask, following Erk and Pado?
(2010), Dinu and Lapata (2010) and O?
Se?aghdha andKorhonen (2011), we pool together the positive substitutes for each target word, con-sidering all contexts, and rank the substitutes using our scoring methods.
For instance,assume in the gold standard there are only two sentences containing the target wordbright as shown in Table 3.
We merge all the substitutes of bright given by the annotatorsand derive a large candidate pool {intelligent, clever, colorful, brilliant, gleam, luminous}.We expect intelligent and clever to be ranked at the top of the list for the first sentence,with colorful, brilliant, gleam, and luminous ranked at the top for the second sentence.3.2.2 Experiments and Results.
In the SemEval-2007 lexical substitution task participantswere asked to discover possible replacements of a target word so the evaluation metricsprovided are designed to give credit for each correct guess and do not take the orderingof the guesses into account.
In contrast, in the ranking task a system is already given afixed pool of substitutes and is asked to recover the order of the list.
Therefore, we usethe Generalized Average Precision (GAP) to evaluate the ranked lists rather than themetrics provided in the SemEval-2007 lexical substitution task.
GAP rewards correctlyranked items with respect to their gold standard weights while the traditional averageprecision is only sensitive to the relative positions of correctly and incorrectly rankeditems.
Let G = ?g1, g2, ..., gm?
be the list of gold substitutions with weights ?y1, y2, ..., ym?for a target word in context.
In our task, the weight is the frequency of a substitutein the gold standard.
Let S = ?s1, s2, ..., sn?
be the system ranked substitute list and?x1, x2, ..., xn?
be the weights associated with them, where m ?
n and xi = 0 if si is not inthe gold list and G ?
S. ThenGAP(S, G) = 1?mj=1 I(yj)y?jn?i=1I(xi)x?i and x?i =1ii?k=1xkwhere I(xi) = 1 if xi is larger than zero, zero otherwise; x?i is the average gold weight ofthe first i system ranked items; y?i is defined analogously.After experimenting on the trial data, we decided a ?
value of 0.6 for the NGM DVGmethod.
We then applied the proposed NGM and NGM DVG methods to rank pooledsubstitutes for each sentence in the test data.
Table 4 summarizes the performancesof our approaches, where mean GAP values are reported on the whole test data aswell as for different POSs.
We can see that the NGM DVG performs better than theNGM system on the ranking task and achieved a mean GAP of 50.8% on the whole testTable 3Two sentences in the SemEval-2007 lexical substitution gold standard.Sentence SubstitutesHe was bright and independent and proud.
intelligent(3), clever(3)The roses have grown out of control, wild and carefree, theirbright blooming faces turned to bathe in the early autumn sun.colorful(2), brilliant(1),gleam(1), luminous(1)427Computational Linguistics Volume 40, Number 2Table 4GAP values (%) of the ranking task evaluation.System test set noun verb adj advNGM 49.7 48.5 44.3 53.2 64.7NGM DVG 50.8 50.9 44.6 53.7 66.2Dinu and Lapata (2010) 42.9 n/a n/a n/a n/aErk and Pado?
(2010) 38.6 n/a n/a n/a n/aO?
Se?aghdha and Korhonen (2011) 49.5 50.7 45.1 48.8 55.9data.
We then compare our results with those achieved by Erk and Pado?
(2010), Dinuand Lapata (2010), and O?
Se?aghdha and Korhonen (2011).
Erk and Pado?
developed anexemplar-based model for capturing word meaning in context, where the meaning of aword in context is represented by a set of exemplar sentences most similar to it.
Dinuand Lapata proposed a vector-space model that models the meaning of a word as aprobability distribution over a set of latent senses.
O?
Se?aghdha and Korhonen also useprobabilistic latent variable models to describe patterns of syntactic interaction.
The bestmean GAP values reported by Erk and Pado?, Dinu and Lapata, and O?
Se?aghdha andKorhonen are 38.6%, 42.9%, and 49.5% on the test data, respectively.3.3 Classification Task EvaluationAlthough the ranking task evaluation gives some indication of how reliable the pro-posed scoring methods are, for the steganography application we require a systemthat can correctly distinguish acceptable substitutes from unacceptable ones.
Thus, weconduct a classification task evaluation which is more related to the steganographyapplication.
The task requires a system to determine acceptable substitutes from a groupof candidates given the word to be replaced and its context.
Those passed substitutescan then carry different codes and be used as stego words.
Similar to the previoussection, we first describe the data and then explain the experimental setup and theevaluation results.3.3.1 Data.
We use the sentences in the gold standard of the SemEval-2007 lexicalsubstitution task as the cover text in our experiments so that the substitutes providedby the annotators can be the positive data.
Because we only take into considerationthe single word substitutions, multi-word substitutes are removed from the positivedata.
Moreover, we use WordNet as the source of providing candidate substitutes inour stegosystem, so if a human-provided substitute does not appear in any synsets ofits target word in WordNet, there is no chance for our stegosystem to replace the targetword with the substitute; therefore, the substitute can be eliminated.
Table 5 presentsthe statistics of the positive data for our experiments.In addition to the positive data, we also need some negative data to test whether ourmethods have the ability to filter out bad substitutions.
We extract the negative data forour experiments by first matching positive substitutes of a target word to all the synsetsthat contain the target word in WordNet.
The synset that includes the most positivesubstitutes is used to represent the meaning of the target word.
If there is more thanone synset containing the highest number of positives, all of those synsets are taken428Chang and Clark Practical Linguistic SteganographyTable 5Statistics of experimental data.noun verb adj advnumber of target words 59 54 57 35number of sentences 570 527 558 349number of positives 2,343 2,371 2,708 1,269number of negatives 1,914 1,715 1,868 884into consideration.
We then randomly select up to six single-word synonyms other thanpositive substitutes from the chosen synset(s) as negative instances of the target word.Let us use an example to demonstrate our automatic negative data collection.
Inthis example, we need to generate bad substitutions for a cover word remainder in thesentence if we divide any number by 4, we would get 1 or 2 or 3, as the remainders, giventhat the annotator-provided positives are leftover and residual.
Table 6 lists the synsets ofremainder found in WordNet 3.1.
Because the synset {remainder, balance, residual, residue,residuum, rest} contains one of the positives whereas the other synsets do not, this synsetis selected for our negative data collection.
We assume the selected synset representsthe meaning of the original word, and those synonyms in the synset which are notannotated as positives must have a certain degree of mismatch to the context.
Therefore,from this example, balance, residue, residuum, and rest are extracted as negatives to testwhether our checking methods can pick out bad substitutions from a set of wordssharing similar or the same meaning.In order to examine whether the automatically collected instances are true negativesand hence form a useful test set, a sample of automatically generated negatives wasselected for human evaluation.
For each POS one sentence of each different target wordwas selected, which results in roughly 13% of the collected negative data, and every neg-ative substitute of the selected sentences was judged by the second author of this article.As can be seen from the annotation results shown in Table 7, most of the instances aretrue negatives, and only a few cases are incorrectly chosen as false negatives.
Becausethe main purpose of the data set is to test whether the proposed checking methods canguard against inappropriate lexical substitutions and be integrated in the stegosystem,it is reasonable to have a few false negatives in our experimental data.
Also, it isTable 6Synsets of remainder in WordNet 3.1.remainder (noun)gloss: something left after other parts have been taken awaysynset: remainder, balance, residual, residue, residuum, restgloss: the part of the dividend that is left over when the dividend is not evenly divisible bythe divisorsynset: remaindergloss: the number that remains after subtraction; the number that when added to thesubtrahend gives the minuendsynset: remainder, differencegloss: a piece of cloth that is left over after the rest has been used or soldsynset: end, remainder, remnant, oddment429Computational Linguistics Volume 40, Number 2Table 7Annotation results for negative data.noun verb adj advnumber of true negatives 234 201 228 98number of false negatives 9 20 28 16more harmless to rule out a permissible substitution than to include an inappropriatereplacement for a stegosystem in terms of its security.
Table 5 gives the statistics of theautomatically collected negative data for our experiments.3.3.2 Experiments and Results.
We evaluate the classification performance of the NGMsystem and the NGM DVG system in terms of accuracy, precision, and recall.
Accuracyis the percentage of correct classification decisions over all acceptable and unacceptablesubstitutes; precision is the percentage of system accepted substitutes being human-provided; recall is the percentage of human-provided substitutes being accepted by thesystem.
Accuracy is less important for the steganography application, and the reasonsfor using precision and recall were explained in Section 1.4: A higher precision valueimplies a better security level, and a larger recall value means a greater payload capacity.It is worth noting that, although there will be a decrease in recall if more false negativesare obtained from a system, there will not be a negative effect on the value of precision.That is, from a security perspective, rejecting an acceptable substitute does not damagethe quality of stego text.
However, it will lower the payload capacity so more stego texttransmission is needed in order to send the secret message, which may raise a securityconcern.Both the NGM system and the NGM DVG system require a threshold to decidewhether a word is acceptable in context.
In order to derive sensible threshold values foreach POS, five-fold cross validation was used for the experiments.
For each fold, 80%of the data is used to find the threshold value which maximizes the accuracy, and thatthreshold is then applied to the remaining 20% to get the final result.We first test whether the proposed methods would benefit from using only longern-grams.
We compare the performance of different combinations of n-gram counts,which are frequency counts of bi- to five-grams, tri- to five-grams, four- to five-grams,and five-grams only.
The results show that for both methods the accuracy, precision,and recall values drop when using fewer n-grams.
In other words, among the four com-binations, the one including bigram to five-gram frequency counts performs the bestacross different POS and, therefore, is adopted in the NGM system and the NGM DVGsystem.
Next, we try weighting different sized n-grams in the proposed methods, ashave Bergsma, Lin, and Goebel (2009) in related work.
According to the preliminaryexperiments we conducted and the conclusion given by Bergsma, Lin, and Goebel, sucha method does not do much better than the simple method using uniform weights fordifferent sized n-grams.Table 8 gives the results for the two checking methods and the average thresholdvalues over the five folds.
In addition, for each POS, a simple baseline is derived by al-ways saying a substitute is acceptable.
From the table we can see that both the NGM andthe NGM DVG systems have higher precision than the baseline, which performs wellin terms of embedding capacity (100% recall) but at the expense of a lower security level(lower precision).
However, in contrast to the results of the ranking task evaluation, the430Chang and Clark Practical Linguistic SteganographyTable 8Performance of the NGM and NGM DVG systems on the classification task.NGM NGM DVG BaselinePOS Acc % Pre % Rec % Thr Acc % Pre % Rec % Thr Acc, Pre % Rec %noun 70.2 70.0 80.2 0.58 68.1 66.5 67.3 0.70 55.0 100verb 68.1 69.7 79.5 0.56 64.8 65.7 66.7 0.70 58.0 100adj 72.5 72.7 85.7 0.48 70.2 68.8 77.7 0.63 59.2 100adv 73.7 76.4 80.1 0.54 68.0 66.4 75.9 0.63 58.9 100NGM system slightly outperforms the NGM DVG system.
Because imperceptibility isan important issue for steganography, we would prefer a system with a higher precisionvalue.
Thus we adopt the NGM method as the linguistic transformation checker in ourlexical substitution-based stegosystem.In addition, we are interested in the effect of the threshold value on the performanceof the NGM method.
Figure 14 shows the precision and recall values with respect todifferent thresholds for each POS.
From the charts we can clearly see the trade-offbetween precision and recall.
Although a higher precision can be achieved by usinga higher threshold value?for example, noun substitutions reach almost 90% precisionwith threshold equal to 0.9?the large drop in recall means many applicable substitutesare being eliminated.
In other words, the trade-off between precision and recall impliesthe trade-off between imperceptibility and payload capacity for linguistic steganogra-phy.
Therefore, the practical threshold setting would depend on how steganographyusers want to trade off imperceptibility for payload.So far we have presented the performance of our checking methods using twodifferent automatic evaluations, the ranking task and the classification task.
From theranking task evaluation we can see that the n-gram distributional similarity does havethe ability to further eliminate some bad substitutes after applying the basic n-gramFigure 14The performance of the NGM method under various thresholds.431Computational Linguistics Volume 40, Number 2method.
However, when facing the classification task, which is more related to thesteganography application, we find that the checking method simply based on countsfrom the Google n-gram corpus is hard to beat.
In addition, from the results of differentorder n-gram combinations we can conclude that the more information we includein the checking method (i.e., using all counts from bi- to five-grams) the better theperformance.
This is similar to the conclusion given by Bergsma, Lin, and Goebel (2009).3.4 Human EvaluationWe want to test how reliable the proposed NGM method is if it is used in a lexicalsubstitution-based stegosystem to guard against inappropriate substitutions.
Therefore,apart from the automatic evaluations, we conducted a more direct evaluation of theimperceptibility for the steganography application by asking human judges to evaluatethe naturalness of sentences.
In the following sections, we explain the evaluation datafirst and then describe the evaluation setup and results.3.4.1 Data.
We collected a total of 60 sentences from Robert Peston?s BBC blog.8 Foreach noun, verb, adjective, and adverb in a sentence, we first group the target word?ssynset(s) in WordNet and apply the NGM method with a score threshold equal to 0.95to eliminate bad substitutes.
If more than one substitute passes the check, the one withthe lowest score is used to replace the original word.
The reason for choosing the wordwith the lowest score is because this makes the test more challenging.
This process isapplied to a sentence where possible and results in around two changes being made persentence.We also generated another version of a sentence changed by random choice of atarget word and random choice of a substitute from a target word?s synset(s) (in orderto provide a baseline comparison).
The number of changes made to a sentence usingthis random method is the same as that in the version generated by the NGM method.In this way, it is fair to compare the qualities of the two modified versions because bothof them receive the same number of substitutions.
Table 9 shows lexical substitutedsentences generated by our method and by the random method.
We can see that oursystem replaces four words (in boldface) in the original sentence so the same numberof words (in boldface) are randomly selected when applying the random method.
Notethat the random method just happens to pick the word big in the original sentence whichis also replaced by our system.
We refer to an original sentence as COVER, a versiongenerated by our method as SYSTEM, and a version modified by the random methodas RANDOM.3.4.2 Evaluation Setup and Results.
The experimental setup follows a Latin square design(Kirk 2012) with three groups of 10 native English speakers as shown in Table 10.
Inthis table, each row represents a set of annotation sentences for a group of judges,and we can see that each sentence is presented in three different conditions: COVER,SYSTEM, and RANDOM, as shown in a column.
Subjects in the same group receivethe 60 sentences under the same set of conditions, and each subject sees each sentenceonly once in one of the three conditions.
The annotation process is Web-based.
At thebeginning of the annotation task, we describe the aim of the annotation as shown inFigure A1 in the Appendix.
Subjects are asked to rate the naturalness of each sentence8 http://www.bbc.co.uk/news/correspondents/robertpeston/.432Chang and Clark Practical Linguistic SteganographyTable 9Different versions of a cover sentence.Version SentenceCOVER Apart from anything else, big companies have the size and muscle to derive gainsby forcing their suppliers to cut prices (as shown by the furore highlighted inyesterday?s Telegraph over Serco?s demand - now withdrawn - for a 2.5% rebatefrom its suppliers); smaller businesses lower down the food chain simply don?thave that opportunity.SYSTEM Apart from anything else, large companies have the size and muscle to derive gainsby pushing their suppliers to cut prices (as evidenced by the furore highlighted inyesterday?s Telegraph over Serco?s need - now withdrawn - for a 2.5% rebate fromits suppliers); smaller businesses lower down the food chain simply don?t have thatopportunity.RANDOM Apart from anything else, self-aggrandizing companies have the size and muscleto derive gains by forcing their suppliers to foreshorten prices (as shown by thefurore highlighted in yesterday?s Telegraph over Serco?s demand - now with-drawn - for a 2.5% rebate from its suppliers); smaller businesses lower down thefood chain simply don?t birth that chance.Table 10Latin square design with three groups of judges.s1, s2, .
.
.
, s20 s21, s22, .
.
.
, s40 s41, s42, .
.
.
, s60Group 1 COVER SYSTEM RANDOMGroup 2 RANDOM COVER SYSTEMGroup 3 SYSTEM RANDOM COVERon a scale from 1 to 4 with score 1 meaning Poor English and score 4 meaning PerfectEnglish.
Each judgment score is explained followed by an example sentence.
Figure A2in the Appendix shows a screen capture of an annotation example presented to a subject.It is worth mentioning that we do not ask judges to spot changes in a text that has beenrun through our system because a spotted change does not necessarily imply that asentence is unnatural; a spotted change might just be the result of the word preferencesand style of an individual judge.The annotation results show that our judges gave an average score of 3.67 outof 4 for the original sentences; 3.33 for the sentences checked by the NGM system;and 2.82 for the randomly changed sentences.
We measure the significance level ofour annotation results using the Wilcoxon signed-rank test (Wilcoxon 1945).
The teststatistic shows that the differences between the three versions (original, system changed,and randomly changed) are highly significant (p < 0.01).
The payload capacity forthis level of imperceptibility is around two information carriers per sentence and eachinformation carrier guarantees to represent at least one bit.
These results show that ourstegosystem achieves better payload capacity than existing lexical substitution-basedstegosystems which have achieved 0.67 bits per sentence (Topkara, Taskiran, and Delp2005; Topkara, Topkara, and Atallah 2006b).
In addition, in terms of security, the resultssuggest that, with the proposed checking method, the quality of stego sentences isimproved compared with the random substitution baseline.433Computational Linguistics Volume 40, Number 2Table 11The average number of words for each word frequency rank (rank 1 is the lowest frequency andrank 15 is the highest frequency).rank 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15cover 961.1 56.7 35.5 29.3 36.7 45.5 55.4 49.2 48.4 39.6 31.2 23.9 19.8 14.5 239.2stego 973.8 56.2 35.2 29.2 36.3 46.0 55.7 49.2 47.9 40.3 30.6 23.6 20.6 14.7 238.73.5 Computational Analysis of Word-Frequency DistributionIn the previous sections we evaluated our system using human judges and now wetest the system using a simple statistical analysis that compares the number of high-frequency words in stego text with that in cover text.
We followed the methodologyused by Meng et al.
(2010), who show that there are fewer high-frequency wordsin translation-based stego text than in normal text.
In the following sections we firstdescribe the evaluation data and then give the evaluation setup and results.3.5.1 Data.
We randomly collected 1,000 pieces of text from 1,000 sections of the BritishNational Corpus9 as the cover text, each of which is about 20k bytes.
Then for each pieceof cover text, the corresponding stego text was generated using the same stegosystemsetting as that used to generate the human evaluation data described in Section 3.4.1.This results in a total of 152,268 cover words being replaced with synonyms in the1,000 pieces of stego text.
As mentioned in Section 2.3, one of the standard payloadmeasurements is to calculate the number of hidden bits per bit transmitted.
From thisdata, we can calculate the lower bound embedding rate of our stegosystem as 0.09 bits ofhidden message per every 100 bits of data transmitted if one substitution only embedsone bit.3.5.2 Evaluation Setup and Results.
In order to calculate word frequency counts, words inboth the cover text and the stego text are first lemmatized using the Minnen, Carroll,and Pearce (2001) tools.
In each piece of text, the obtained word frequency count is thenmapped to a rank r between 1 and 15, where rank 1 corresponds to low frequency and15 corresponds to high frequency.
The mapping is defined as:r =?frequency?minmax?min15?+ 1where max and min is the maximum and minimum word frequency counts in the text,respectively, and the rank of max is 15.
Next, we calculate the number of words for eachfrequency rank in each text.
Finally, we calculate the average number of words for eachfrequency rank for both the cover text and the stego text as shown in Table 11, where theaverage number of words has been multiplied by r2 like the results reported by Menget al.
(2010).From Table 11 we can see the two vectors are very close, and the only apparentdifference is that in the stego text there are more low-frequency words than in the covertext.
However, unlike the conclusion derived by Meng et al.
(2010), our results do not9 http://www.natcorp.ox.ac.uk/docs/URG/.434Chang and Clark Practical Linguistic Steganographyshow a substantial difference in the frequency of high-frequency words between thecover text and the stego text.
A possible explanation is that a translation-based stegosys-tem may combine translations output from different machine translation systems, hencethe usage of frequent words may not be consistent, whereas our stegosystem uses thesame substitution checker for each synonym replacement so there is a certain consis-tency achieved in stego text.After giving both the results of human and computational evaluations in the previ-ous sections, now we would like to show an example of what a typical stego text willlook like.
The following paragraphs are generated by the proposed NGM method withthe substitution score threshold equal to 0.9, where 24 words have been substituted:The whistleblower, who yesterday gave me the entire recording, told me that the Telegraph?sdeletion of these sections about Mr Murdoch was a commercial decision, prompted by the factthat the Telegraph - like Mr. Cable - would rather News Corporation does not end up as 100%owner of BskyB.I of course set this to the Telegraph.
And quite late in the day, at 19:19 last night to beaccurate, the Telegraph?s external media adviser sent me a statement attributed to anunidentified ?spokesman for the Daily Telegraph.?
The statement reads:?It is complete nonsense to suggest that the Daily Telegraph did not publish commentsfrom Vince Cable on the Rupert Murdoch takeover of BSkyB for commercial reasons.
It was aneditorial decision to focus this morning on Cable?s comments on the Coalition because they costof wider interest to our readers.
?Well, some would say that was a somewhat eccentric editorial decision for an editor, TonyGallagher, widely regarded as one of the sharpest in the business.
I rang Mr. Gallagher todiscuss this, but he directed me to the Telegraph?s national PR spokesperson.Also, you may have found that the Telegraph has not even put out any clear andunequivocal statement that it was ever planning to publish Mr Cable?s remarks about MrMurdoch (though it has now released them, after they were set out by the BBC).Maybe I am being a bit naive and ridiculous to think any of this matters.
Maybe most ofyou believe that what we do as reporters is so plain and constantly subject to commercialinterference that there is no special benefit to be gained from asking the Telegraph to explainitself in this case.But really that?s not been my experience in 27 years as a hack.
And I still think the questionof what news organisations put into the public domain, and how they do it, matters.Readers can consult Appendix B for those 24 changes made by the proposed NGMmethod.4.
Vertex Coding MethodAs described earlier, the proposed stegosystem extends the original synset by addingwords in a synonym transitive closure chain while retaining the synonymous rela-tionships between words using a synonym graph representation.
The proposed NGMchecker effectively controls the size of a synonym graph according to the context.
Afterconstructing the graph, namely, obtaining all the good alternatives for a cover word, theencoder generation module needs to assign codes to every word in the graph.
In thissection, we explain the coding method used in our stegosystem.435Computational Linguistics Volume 40, Number 2The aim of the proposed coding method is to convert an input synonym graph intoa coded graph so that each vertex, namely, word, is encoded by a particular code.
Themethod is inspired by the classic vertex coloring problem in graph theory (Gould 1988),where a coloring of a graph is a labeling of the graph?s vertices with colors subject to thecondition that no two adjacent vertices share the same color.
However, in our proposedcoding method, adjacent vertices are allowed to have the same code as long as eachvertex is able to handle the prefix string of any secret message.
A vertex can achieve thisby either using its own code or a neighbor?s code, as long as there is a guarantee thatat least the first secret bit of the prefix can be embedded no matter which word in thegraph is the cover word.Let us first consider coding the synonym graph of the two joint synsets fromFigure 10.
Because both of the synsets have a size of four, which means the synsetscan exhaust up to a two-bit coding space, four different two-bit codewords 00, 01, 10,and 11 are used to code the graph, as shown in Figure 15(a).
As we can see, each word(a)(b)(c)Figure 15Examples of coded synonym graphs.436Chang and Clark Practical Linguistic Steganographyin the graph has access to all the two-bit codewords.
This means that the fundamentalrequirement of the coding method is satisfied: No matter what the target word is inthe graph, any two-digit prefix of a secret message can be accommodated.
In addition,the problematic word marry receives a unique codeword no matter which synset isconsidered, which means the secret recovery process will not encounter an ambiguitybecause the receiver can apply the same coding method to derive identical codewordsused by the sender.Next, let us consider coding the synonym graph in Figure 11(a).
Again, four two-bitcodewords are used because the maximum synset size is four in the synsets that makeup this graph, and a coded version of the graph is shown in Figure 15(b).
Note that itis acceptable to have conjoin and join encoded by the same codeword 00 because bothof them have access to all the two-bit codewords.
However, both bind and draw haveonly one neighbor, which means that only two codewords can be accommodated bythese nodes, namely, bits 0 and 1.
Therefore, instead of using two-bit codewords, themost significant bits are used to code these words and the neighbor, a process we callcodeword reduction.
In this example, the codewords of bind, draw, and tie are reducedto 0, 0, and 1, respectively.
After codeword reduction, the vertex draw can only accesscodeword 1 so a further change is needed: The vertex?s codeword is changed to 0 inorder to accommodate either secret bit 0 or 1, a process we call codeword correction.Note that the final coded graph, after codeword reduction and correction, satisfiesthe fundamental requirement that all vertices can represent some prefix of the secretmessage.
Note also that some vertices can represent a longer prefix than others.
Forexample, if the next part of the secret message to be embedded is 11, and the targetword is splice, then tie would be chosen as the stego word, covering only the first bit ofthe prefix.
However, if the target word is wed, then espouse would be chosen as the stegoword, covering two-bits of the prefix.
In general the secret embedding procedure willchoose to cover as many bits of the secret message as possible at each point.99.6% of synsets in WordNet have a size of less than eight, which means that mostof the synsets cannot exhaust more than a two-bit coding space (i.e., we can only encodeat most two bits using a typical synset).
Therefore, we restrict the maximum codewordsize in our coding method to two bits.
The proposed method always starts coding agraph with two-bit codewords even if the maximum synset size of a graph is less thanfour, and then adjusts the assigned codewords by codeword reduction and codewordcorrection.Figure 15(c) shows a coded synonym graph where the maximum synset size is three.We first want to make sure that wed, tie, conjoin, and join have access to all the two-bitcodewords because they all have at least three neighboring vertices.
Those vertices thathave less than three neighbors are randomly assigned one of the four codewords suchthat no two adjacent vertices have the same codeword.
After the two-bit codewordencoding, for a vertex that has only two neighbors, we first check whether the twoneighbors are encoded by the same codeword.
If they are, which means the vertexand its neighbors can accommodate only two codewords, then codeword reduction isapplied to the vertex and both of its neighbors.
For example, both the neighbors of linkare encoded by codeword 11 so the codewords of link and its neighbors are replacedby the most significant bits.
Then codeword correction is applied to link to ensure theaccess of both bit 0 and bit 1.
Similarly, the codeword of unite is replaced by its mostsignificant bit, but unite does not need codeword correction in this case.However, if the two neighbors have different codewords, then the vertex has accessto only three two-bit codewords and one of the two-bit codewords is missing.
In thiscase the codeword that has the same most significant bit as the missing one must be437Computational Linguistics Volume 40, Number 2reduced by undergoing codeword reduction.
For example, the two neighbors of splicehave different two-bit codewords, and the missing codeword is 10.
Among splice, wed,and tie, tie has the same significant bit from the missing codeword, so the codeword oftie is reduced to bit 1.
Note that splice can now handle any message prefix: If the first bitis a 0, then two bits will be embedded (using either splice or wed); if the first bit is a 1then only that one bit will be embedded (using tie).
Similarly, the codeword of espouse ischanged to bit 1.Finally, bind and draw, which have only one neighbor, are adjusted as describedfor Figure 15(b).
It is worth noting that, even though the maximum synset size in thisexample is three, it is possible to have a word carrying a two-bit codeword in the codedgraph.Figure 16 describes an algorithm for assigning two-bit codewords to each nodein an input synonym graph (which is run before applying the processes of codewordcorrection and reduction).
The node data structure has four fields: word is the label ofINPUT: a synonym graph G that has its nodes stored in Q1 and Q2OUTPUT: a coded synonym graph Gcoded using 2-bit codewordsstruct nodestring wordstring code = nullset of codes cannot access = {00, 01, 10, 11}set of nodes neighborsfunction AssignCode(w, code set)IF code set is empty THENw.code = random({00, 01, 10, 11})ELSEw.code = random(code set);END IFdelete w.code from w.cannot accessFOR every n in w.neighborsdelete w.code from n.cannot accessEND FORQ = [Q1, Q2]WHILE Q is not emptyw = pop(Q)IF w.code is null THENAssignCode(w, w.cannot access)END IFFOR every n in w.neighborsIF n.code is null THENIF w.cannot access is empty THENAssignCode(n, n.cannot access)ELSE IF n.cannot access is empty THENAssignCode(n, w.cannot access)ELSEAssignCode(n, w.cannot access ?
n.cannot access)END IFEND IFEND FOREND WHILEFigure 16Algorithm for coding a synonym graph using two-bit codewords.438Chang and Clark Practical Linguistic SteganographyFigure 17Framework of the proposed lexical substitution-based stegosystem.the node corresponding to a word in a synset; code is the codeword of the node andis initialized to null; neighbors contains a set of neighboring nodes; and cannot accessrecords the two-bit codewords that cannot be accessed by the node and is initialized to00, 01, 10, 11.
Because we want a node to have access to all the two-bit codewords wherepossible, nodes that have at least three neighbors are stored in a priority queue Q1.
Inthis group of nodes, a node having three neighbors does not allow any of its neighborsto carry the same code so we assign codes to three-neighbor nodes and their neighborsfirst.
Therefore, nodes in Q1 are sorted by the number of neighbors such that nodeshaving the least neighbors are at the front of the queue.
The other nodes are stored in aqueue Q2 such that nodes having two neighbors are at the front of the queue.The function AssignCode(w, code set) randomly chooses a codeword from code set,or from the four two-bit codewords if code set is empty.
Then the function removes thechosen codeword from w.cannot access and from the cannot access sets of w?s neighbors.The two-bit codeword assigning procedure first loops through all the nodes in Q1 andthen Q2.
For each node, the procedure first checks if a codeword has already beenassigned; if not, it calls the AssignCode function.
This assigns an appropriate code, asdescribed earlier, and modifies the cannot access field of both the node and the node?sneighbors (since the new code is now accessible to both the node and its neighbors).
Itthen loops through each of the node?s neighbors, using the AssignCode function to assigna code to each neighbor if it does not already have one.
Note that the set of availablecodes passed to the function depends on the cannot access sets from both the node andthe neighbor.After all the nodes have been assigned two-bit codes using this algorithm, code-word reduction and codeword correction as previously described are applied to reviseimproper codewords.4.1 Proposed Lexical StegosystemFigure 17 illustrates the framework of our lexical substitution-based stegosystem.
Notethat we asume that WordNet has been pre-processed by excluding multi-word syn-onyms and single-entry synsets.
Table 12 shows the statistics of synsets used in ourTable 12Statistics of synsets used in our stegosystem.noun verb adjective adverbnumber of synsets 16,079 4,529 6,655 964number of words 30,933 6,495 14,151 2,025average synset size 2.56 2.79 2.72 2.51max synset size 25 16 21 8439Computational Linguistics Volume 40, Number 2stegosystem.
A possible information carrier is first found in the cover sentence.
Wedefine a possible information carrier as a word in the cover sentence that belongs to atleast one synset in the pre-processed WordNet.
Starting from the cover word?s synset,all words in the synonym transitive closure chain are examined by the NGM method.
Asynonym graph(s) is then built based on the remaining words.
Next, we assign codes toeach word in the synonym graph(s).
During the encoder generation procedure, if wordsin the synonym graph all belong to the same synset, the block coding method is usedto encode the words; otherwise the vertex coding method is applied to the synonymgraph.
Finally, according to the secret bitstring, the system selects a substitute that issynonymous with the cover word and has as its codeword the longest potential matchwith the secret bitstring.Repeating a comment made earlier, we use the transitive closure chain of WordNetcontaining the target word as a simple method to ensure that both sender and receiverencode the same graph.
It is important to note, however, that the sender only considersthe synonyms of the target word as potential substitutes; the transitive closure chain isonly used to consistently assign the codes.For the decoding process, the receiver does not need the original text for extractingsecret data.
An information carrier can be found in the stego text by referring to Word-Net in which related synonyms are extracted.
Those words in the related sets undergothe NGM checking method, and the words passing the check form a synonym graph(s).The synonym graph(s) are encoded by either block coding or the vertex coding schemedepending on whether the remaining words are in the same synset.
Finally, the secretbitstring is implicit in the codeword of the information carrier and therefore can beextracted.We demonstrate how to embed secret bit 1 in the sentence it is a shame that wecould not reach the next stage.
A possible information carrier shame is first found in thesentence.
Table 13 lists the synsets in the synonym transitive closure chain extractedfrom WordNet.
The score of each word calculated by the NGM method is given inparentheses.
For the purpose of demonstrating the use of vertex coding, we select alow threshold score of 0.27.
The output of the synonym graph is shown in Figure 18(a).Because the remaining words do not belong to the same synset, the vertex codingmethod is then used to encode the words.
Figure 18(b) shows the coded synonym graphin which each vertex is assigned one of the four two-bit codewords; Figure 18(c) isthe graph after applying codeword reduction and codeword correction.
Although bothdisgrace and pity are encoded by 1, pity is chosen to replace the cover word because ithas a higher score.
Finally, the stego text is generated, it is a pity that we could not reachTable 13Synsets of shame in the synonym transitive closure chain with substitution scores.cover sentence: It is a shame that we could not reach the next stage.
{pity (0.97), shame (1.0)}{shame (1.0), disgrace (0.84), ignominy (0.24)}{commiseration (0.28), pity (0.97), ruth (0.13), pathos (0.31)}{compassion (0.49), pity (0.97)}{condolence (0.27), commiseration (0.28)}{compassion (0.49), compassionateness (0)}{pathos (0.31), poignancy (0.31)}{poignance (0.12), poignancy (0.31)}440Chang and Clark Practical Linguistic Steganography(a) The synonym graph of the synsets in Table 13(b) Coded synonym graph using the four 2-bit codewords(c) Coded synonym graph after codeword reduction and code-word correctionFigure 18Synonym graphs generated by the proposed stegosystem.the next stage.
As explained previously, even if a cover word does not pass the NGMcheck, the proposed stegosystem can still use its synonyms to embed secret bits.
Forexample, assume the cover sentence is it is an ignominy that we could not reach the nextstage.
The same coded synonym graph as Figure 18(c) will be constructed because boththe context and the synonym transitive closure chain are the same as that in the originalexample.
This time, the replacement of shame represents secret bit 0, and the replacementwith disgrace represents secret bit 1.
In other words, a change must be made in order toembed a secret bit in this case.In cryptography, Kerckhoffs?s principle (Kerckhoffs 1883) states that a method ofsecretly coding and transmitting information should be secure even if everything aboutthe system, except the key and any private randomizer, is public knowledge.
In oursteganography scheme, the secret key is the score threshold in the NGM method, and441Computational Linguistics Volume 40, Number 2the private randomizer is the one that assigns codes in the AssignCode function in theproposed vertex coding methods.
The score threshold decides the size of a synonymgraph, and the randomizer controls the encoding of a synonym graph.
To extract thesecret message, the enemy needs to generate the same coded synonym graphs asconstructed by the sender.
Therefore, it is difficult to recover the secret bits withoutknowing the score threshold and the code randomizer.5.
Conclusions and Future WorkOne of the contributions of this work is to develop a novel lexical substitution-basedstegosystem using vertex coding that improves the data embedding capacity com-pared to existing systems.
The vertex coding method represents synonym substitu-tion as a synonym graph so the relations between words can be clearly observed.
Inaddition, the NGM method, an automatic system for checking synonym acceptabil-ity in context, is integrated in our stegosystem to ensure information security.
Theproposed stegosystem was automatically evaluated using the gold standard from theSemEval2007 lexical substitution task as well as a human evaluation.
From the evalu-ation results we may conclude that our substitution-based stegosystem has achieveda reasonable level of security while reaching the payload capacity of around two bitsper sentence.In this work, we only evaluated the lexical substitution in terms of the sentence-level naturalness rather than meaning retention and document-level coherence.
There-fore, it would be interesting to see to what extent the proposed substitution checkers areuseful for the security of linguistic steganography at the document-level.
In addition,apart from the linguistic transformations discussed in Section 2.1, we would like toexplore more manipulations that can meet the requirements of linguistic steganography.As mentioned in Section 2.2, there is no research on the practical issue of using differenttypes of cover text for the steganography application.
Thus, it would be interestingto see whether some types of cover text are better suited to linguistic steganographythan others.
Another interesting question that we have not addressed is whether somelanguages are easier to be modified than others, or whether some languages work betterwith particular linguistic transformations than others.Research in linguistic steganography requires experience and knowledge of bothinformation security and NLP.
In addition, due to the complexity of language, therehave been more studies on image or video steganography than linguistic steganog-raphy.
Hence linguistic steganography is a relatively new research area, and furtherefforts are needed to develop more secure and efficient systems.
The novel and originalideas provided in this article can benefit research in both computational linguistics andinformation security.
It is hoped that our work can form the basis for more researchdevoted to linguistic steganography.442Chang and Clark Practical Linguistic SteganographyAppendix A. Screenshots of the Web-Based Human AnnotationFigure A1The introduction and guidelines for the lexical substitution annotation.Appendix B.
Original Text and the Substituted WordsThe following text is part of the Robert Peston?s BBC blog article titled ?Unansweredquestions about Cable?10 where the 24 words in boldface are selected by the proposedNGM method as information carriers and are replaced with their synonyms as shownin the example in Section 3.4.2:The whistleblower, who yesterday gave me the full recording, told me that the Telegraph?somission of these sections about Mr Murdoch was a commercial decision, motivated by thefact that the Telegraph - like Mr Cable - would rather News Corporation does not end up as100% owner of BskyB.I of course put this to the Telegraph.
And rather late in the day, at 19:19 last night to beprecise, the Telegraph?s external media adviser sent me a statement attributed to an unnamed?spokesman for the Daily Telegraph.?
The statement says:?It is utter nonsense to suggest that the Daily Telegraph did not publish comments fromVince Cable on the Rupert Murdoch takeover of BSkyB for commercial reasons.
It was aneditorial decision to focus this morning on Cable?s comments on the Coalition because they wereof wider interest to our readers.
?10 http://www.bbc.co.uk/blogs/thereporters/robertpeston/2010/12/unanswered questionsabout cab.html.443Computational Linguistics Volume 40, Number 2Well, some would say that was a slightly eccentric editorial decision for an editor, TonyGallagher, widely regarded as one of the sharpest in the business.
I rang Mr Gallagher to discussthis, but he directed me to the Telegraph?s internal PR spokesperson.Also, you may have noticed that the Telegraph has not yet put out any clear andunambiguous statement that it was ever planning to publish Mr Cable?s remarks about MrMurdoch (though it has now published them, after they were put out by the BBC).Maybe I am being a bit naive and silly to think any of this matters.
Maybe most of youthink that what we do as reporters is so obviously and constantly subject to commercialinterference that there is no particular benefit to be gained from asking the Telegraph to explainitself in this case.But actually that?s not been my experience in 27 years as a hack.
And I still think thequestion of what news organisations put into the public domain, and how they do it, matters.Figure A2A screen capture of the lexical substitution annotation.AcknowledgmentsWe would like to thank Dr. Laura Rimell,the anonymous reviewers, and all thecontributing volunteer annotators foruseful comments and their effort andtime.
This work was largely carried outwhile Ching-Yun Chang was a Ph.D. studentat the Cambridge University ComputerLaboratory, where she was supported bythe Studying Abroad Scholarship from theMinistry of Education in Taiwan.ReferencesAtallah, Mikhail J., Craig J. McDonough,Victor Raskin, and Sergei Nirenburg.2000.
Natural language processing forinformation assurance and security: Anoverview and implementations.
InProceedings of the 2000 Workshop on NewSecurity Paradigms, pages 51?65,Ballycotton.Atallah, Mikhail J., Victor Raskin, Michael C.Crogan, Christian Hempelmann, Florian444Chang and Clark Practical Linguistic SteganographyKerschbaum, Dina Mohamed, andSanket Naik.
2001.
Natural languagewatermarking: Design, analysis, and aproof-of-concept implementation.In Proceedings of the 4th InternationalInformation Hiding Workshop,pages 185?199, Pittsburgh, PA.Atallah, Mikhail J., Victor Raskin,Christian F. Hempelmann, MercanKarahan, Umut Topkara, Katrina E.Triezenberg, and Radu Sion.
2002.
Naturallanguage watermarking andtamperproofing.
In Proceedings of the 5thInternational Information Hiding Workshop,pages 196?212, Noordwijkerhout.Bennett, Krista.
2004.
Linguisticsteganography: Survey, analysis, androbustness concerns for hidinginformation in text.
CERIAS TechnicalReport 2004-13, Purdue University,Lafayette, IN.Bergmair, Richard.
2004.
Towards linguisticsteganography: A systematic investigationof approaches, systems, and issues.
Finalyear thesis, B.Sc.
(Hons.)
in ComputerStudies, The University of Derby.Bergmair, Richard.
2007.
A comprehensivebibliography of linguistic steganography.In Proceedings of the SPIE InternationalConference on Security, Steganography, andWatermarking of Multimedia Contents,pages W1?W6, San Jose, CA.Bergsma, Shane, Dekang Lin, and RandyGoebel.
2009.
Web-scale n-gram models forlexical disambiguation.
In Proceedings ofthe 21st International Joint Conference onArtifical Intelligence, pages 1,507?1,512,Pasadena, CA.Bolshakov, Igor A.
2004.
A method oflinguistic steganography based oncollocationally-verified synonym.
InInformation Hiding: 6th InternationalWorkshop, pages 180?191, Toronto.Brants, Thorsten and Alex Franz.
2006.Web 1T 5-gram corpus version 1.1.Linguistic Data Consortium,Philadelphia, PA.Callison-Burch, Chris.
2008.
Syntacticconstraints on paraphrases extracted fromparallel corpora.
In Proceedings of theEMNLP Conference, pages 196?205,Honolulu, HI.Carlson, Andrew, Tom M. Mitchell, andIan Fette.
2008.
Data analysis project:Leveraging massive textual corpora usingn-gram statistics.
Technical ReportCMU-ML-08-107, School of ComputerScience, Carnegie Mellon University,Pittsburgh, PA.Chang, Ching-Yun and Stephen Clark.2010a.
Linguistic steganography usingautomatically generated paraphrases.In Proceedings of the Annual Meeting ofthe North American Association forComputational Linguistics, pages 591?599,Los Angeles, CA.Chang, Ching-Yun and Stephen Clark.
2010b.Practical linguistic steganography usingcontextual synonym substitution andvertex color coding.
In Proceedings of the2010 Conference on Empirical Methodsin Natural Language Processing,pages 1,194?1,203, Cambridge, MA.Chang, Ching-Yun and Stephen Clark.2012a.
Adjective deletion for linguisticsteganography and secret sharing.In Proceedings of the 24th InternationalConference on Computational Linguistics,pages 493?510, Mumbai.Chang, Ching-Yun and Stephen Clark.
2012b.The secret?s in the word order: Text-to-textgeneration for linguistic steganography.In Proceedings of the 24th InternationalConference on Computational Linguistics,pages 511?528, Mumbai.Chapman, Mark and George I. Davida.1997.
Hiding the hidden: A softwaresystem for concealing ciphertext asinnocuous text.
In Proceedings of the FirstInternational Conference on Information andCommunication Security, pages 335?345,Beijing.Chapman, Mark, George I. Davida, andMarc Rennhard.
2001.
A practical andeffective approach to large-scaleautomated linguistic steganography.In Proceedings of the 4th InternationalConference on Information Security,pages 156?165, Malaga.Chen, Zhili, Liusheng Huang, Peng Meng,Wei Yang, and Haibo Miao.
2011.
Blindlinguistic steganalysis against translationbased steganography.
In Proceedings of the9th International Conference on DigitalWatermarking, pages 251?265, Seoul.Clark, Stephen and James R. Curran.
2007.Wide-coverage efficient statistical parsingwith CCG and log-linear models.Computation Linguistics, 33(4):493?552.Cohn, Trevor and Mirella Lapata.
2008.Sentence compression beyond worddeletion.
In Proceedings of the 22ndInternational Conference on ComputationalLinguistics (Coling 2008), pages 137?144,Manchester.Cox, Ingemar, Matthew Miller, Jeffrey Bloom,Jessica Fridrich, and Ton Kalker.
2008.Digital Watermarking and Steganography.445Computational Linguistics Volume 40, Number 2Morgan Kaufmann Publishers Inc.,second edition.Dinu, Georgiana and Mirella Lapata.
2010.Measuring distributional similarity incontext.
In Proceedings of the 2010Conference on Empirical Methods in NaturalLanguage Processing, pages 1,162?1,172,Cambridge, MA.Doddington, George.
2002.
Automaticevaluation of machine translation qualityusing n-gram co-occurrence statistics.In Proceedings of the Second InternationalConference on Human Language TechnologyResearch, pages 138?145, San Diego, CA.Dorr, Bonnie, David Zajic, and RichardSchwartz.
2003.
Hedge trimmer:A parse-and-trim approach to headlinegeneration.
In Proceedings of theHLT-NAACL 03 on Text SummarizationWorkshop - Volume 5, pages 1?8, Edmonton.Erk, Katrin and Sebastian Pado?.
2010.Exemplar-based models for word meaningin context.
In Proceedings of the ACL 2010Conference Short Papers, pages 92?97,Uppsala.Fellbaum, Christiane.
1998.
WordNet: AnElectronic Lexical Database.
MIT Press,Cambridge, MA.Fridrich, Jessica.
2009.
Steganography inDigital Media: Principles, Algorithms, andApplications.
Cambridge University Press.Gould, Ronald J.
1988.
Graph Theory.Benjamin/Cummings Publishing Co.,Menlo Park, CA.Grothoff, Christian, Krista Grothoff,Ludmila Alkhutova, Ryan Stutsman, andMikhail J. Atallah.
2005.
Translation-basedsteganography.
In Proceedings of the2005 Information Hiding Workshop,pages 219?233, Barcelona.Herodotus.
1987.
The History.
University ofChicago Press.
Translated by David Grene.Hoover, J. Edgar.
1946.
The enemy?smasterpiece of espionage.
The Reader?sDigest, 48:49?53.
London edition.Huffman, David A.
1952.
A method for theconstruction of minimum-redundancy codes.Proceedings of the IRE, 40(9):1098?1101.Islam, Aminul and Diana Inkpen.
2009.Real-word spelling correction usingGoogle Web IT 3-grams.
In EMNLP ?09:Proceedings of the 2009 Conference onEmpirical Methods in Natural LanguageProcessing, pages 1,241?1,249, Singapore.Kahn, David.
1967.
The Codebreakers: TheStory of Secret Writing.
Macmillan.Kerckhoffs, Auguste.
1883.
La cryptographiemilitaire.
Journal des Sciences Militaires,IX:5?83.Khairullah, M. D. 2009.
A novel textsteganography system using font color ofthe invisible characters in Microsoft Worddocuments.
In Second InternationalConference on Computer and ElectricalEngineering, pages 482?484, Dubai.Kim, Mi-Young.
2008.
Natural languagewatermarking for Korean using adverbialdisplacement.
In Multimedia and UbiquitousEngineering, pages 576?581, Busan.Kim, Mi-Young.
2009.
Natural languagewatermarking by morphemesegmentation.
In First Asian Conference onIntelligent Information and Database Systems,pages 144?149, Dong Hoi.Kirk, Roger E. 2012.
Experimental Design:Procedures for the Behavioral Sciences.SAGE Publications, Inc, fourth edition.Kishida, Kazuaki.
2005.
Property of averageprecision and its generalization: Anexamination of evaluation indicatorfor information retrieval experiments.Technical Report NII-2005-014E, NationalInstitute of Informatics, Tokyo.Kullback, Solomon.
1959.
Information Theoryand Statistics.
John Wiley and Sons,New York.Kummerfeld, Jonathan K. and James R.Curran.
2008.
Classification of verb particleconstructions with the Google Web 1TCorpus.
In Proceedings of the AustralasianLanguage Technology Association Workshop2008, pages 55?63, Hobart.Lampson, Butler W. 1973.
A note on theconfinement problem.
Communications ofthe ACM, 16(10):613?615.Lee, Lillian.
1999.
Measures of distributionalsimilarity.
In Proceedings of the 37th AnnualMeeting of the Association for ComputationalLinguistics on Computational Linguistics,pages 25?32, College Park, MD.Liu, Yuling, Xingming Sun, and Yong Wu.2005.
A natural language watermarkingbased on Chinese syntax.
In Advances inNatural Computation, volume 3612,pages 958?961, Changsha.McCarthy, Diana and Roberto Navigli.
2007.SemEval-2007 task 10: English lexicalsubstitution task.
In Proceedings of the4th International Workshop on SemanticEvaluations (SemEval-2007), pages 48?53,Prague.Meng, Peng, Liusheng Hang, Zhili Chen,Yuchong Hu, and Wei Yang.
2010.
STBS:A statistical algorithm for steganalysisof translation-based steganography.In Proceedings of the 12th InternationalConference, Information Hiding,pages 208?220, Calgary.446Chang and Clark Practical Linguistic SteganographyMeng, Peng, Yun-Qing Shi, Liusheng Huang,Zhili Chen, Wei Yang, and AbdelrahmanDesoky.
2011.
LinL: Lost in n-best list.In Proceedings of the 13th InternationalConference, Information Hiding,pages 329?341, Prague.Meral, Hasan M., Emre Sevinc, Ersin Unkar,Bulent Sankur, A. Sumru Ozsoy, andTunga Gungor.
2007.
Syntactic tools fortext watermarking.
In Proceedings of theSPIE International Conference on Security,Steganography, and Watermarking ofMultimedia Contents, pages X1?X12,San Jose, CA.Meral, Hasan Mesut, Bu?lent Sankur,A.
Sumru O?zsoy, Tunga Gu?ngo?r, andEmre Sevinc?.
2009.
Natural languagewatermarking via morphosyntacticalterations.
Computer Speech and Language,23(1):107?125.Minnen, Guido, John Carroll, and DarrenPearce.
2001.
Applied morphologicalprocessing of English.
Natural LanguageEngineering, 7:207?223.Murphy, Brian.
2001.
Syntactic informationhiding in plain text.
Masters Thesis,Trinity College Dublin.Murphy, Brian and Carl Vogel.
2007a.Statistically-constrained shallow textmarking: Techniques, evaluation paradigmand results.
In Proceedings of the SPIEInternational Conference on Security,Steganography, and Watermarking ofMultimedia Contents, pages Z1?Z9,San Jose, CA.Murphy, Brian and Carl Vogel.
2007b.
Thesyntax of concealment: Reliable methodsfor plain text information hiding.
InProceedings of the SPIE InternationalConference on Security, Steganography, andWatermarking of Multimedia Contents,volume 6505, pages Y1?Y12, San Jose, CA.Newman, Bernard.
1940.
Secrets of GermanEspionage.
Robert Hale Ltd.O?
Se?aghdha, Diarmuid and Anna Korhonen.2011.
Probabilistic models of similarityin syntactic context.
In Proceedings of theConference on Empirical Methods in NaturalLanguage Processing, pages 1,047?1,057,Edinburgh.Papineni, Kishore, Salim Roukos, ToddWard, and Wei-Jing Zhu.
2002.
BLEU:A method for automatic evaluation ofmachine translation.
In Proceedings of the40th Annual Meeting of the Association forComputational Linguistics, pages 311?318,Philadelphia, PA.Pfitzmann, Birgit.
1996.
Information hidingterminology: Results of an informalplenary meeting and additional proposals.In Proceedings of the First InternationalWorkshop on Information Hiding,pages 347?350, Cambridge.Por, Lip Y., Ang T. Fong, and B. Delina.2008.
WhiteSteg: A new scheme ininformation hiding using textsteganography.
WSEAS Transactionson Computers, 7:735?745.Schuler, Karin Kipper.
2005.
Verbnet: ABroad-coverage, Comprehensive Verb Lexicon.Ph.D.
thesis, University of Pennsylvania,Philadelphia, PA.Shahreza, Mohammad Shirali.
2006.
A newmethod for steganography in HTML files.Advances in Computer, Information, andSystems Sciences, and Engineering,pages 247?252.Sharoff, Serge.
2006.
Open-source corpora:Using the net to fish for linguistic data.International Journal of Corpus Linguistics,11(4):435?462.Shen, Libin, Giorgio Satta, and AravindJoshi.
2007.
Guided learning forbidirectional sequence classification.In Proceedings of the 45th Annual Meetingof the Association of ComputationalLinguistics, pages 760?767, Prague.Shih, Frank Y.
2008.
Digital Watermarking andSteganography: Fundamentals and Techniques.CRC Press.Simmons, Gustavus J.
1984.
The prisoners?problem and the subliminal channel.In Advances in Cryptology: Proceedingsof CRYPTO ?83, pages 51?67,Santa Barbara, CA.Soderstrand, Michael A., Kenneth W.Jenkins, Graham A. Jullien, and Fred J.Taylor.
1986.
Residue Number SystemArithmetic: Modern Applications inDigital Signal Processing.
IEEE Press.S?gaard, Anders.
2010.
Simplesemi-supervised training of part-of-speechtaggers.
In Proceedings of the ACL 2010Conference Short Papers, pages 205?208,Uppsala.Spoustova?, Drahom?
?ra ?Johanka,?
Jan Hajic?,Jan Raab, and Miroslav Spousta.
2009.Semi-supervised training for the averagedperceptron POS tagger.
In Proceedings of the12th Conference of the European Chapter of theAssociation for Computational Linguistics,pages 763?771, Athens.Stevens, Guy William Willis.
1957.Microphotography: Photography at ExtremeResolution.
Chapman & Hall.Stutsman, Ryan, Christian Grothoff, MikhailAtallah, and Krista Grothoff.
2006.
Lost injust the translation.
In Proceedings of the447Computational Linguistics Volume 40, Number 22006 ACM Symposium on AppliedComputing, pages 338?345, Dijon.Taskiran, Cuneyt M., Mercan Topkara, andEdward J. Delp.
2006.
Attacks on lexicalnatural language steganography systems.In Proceedings of the SPIE InternationalConference on Security, Steganography, andWatermarking of Multimedia Contents,pages 97?105, San Jose, CA.Topkara, Mercan, Giuseppe Riccardi, DilekHakkani-Tu?r, and Mikhail J. Atallah.2006.
Natural language watermarking:Challenges in building a practical system.In Proceedings of the SPIE InternationalConference on Security, Steganography, andWatermarking of Multimedia Contents,pages 106?117, San Jose, CA.Topkara, Mercan, Cuneyt M. Taskiran, andEdward J. Delp.
2005.
Natural languagewatermarking.
In Proceedings of the SPIEInternational Conference on Security,Steganography, and Watermarking ofMultimedia Contents, volume 5681,pages 441?452, San Jose, CA.Topkara, Mercan, Umut Topkara, andMikhail J. Atallah.
2006a.
Words are notenough: Sentence level natural languagewatermarking.
In Proceedings of the ACMWorkshop on Content Protection and Security,pages 37?46, Santa Barbara, CA.Topkara, Umut, Mercan Topkara, andMikhail J. Atallah.
2006b.
The hidingvirtues of ambiguity: Quantifiablyresilient watermarking of natural languagetext through synonym substitutions.In Proceedings of the 8th Workshop onMultimedia and Security, pages 164?174,Geneva.Toutanova, Kristina, Dan Klein,Christopher D. Manning, and YoramSinger.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.In Proceedings of the 2003 Conference of theNorth American Chapter of the Associationfor Computational Linguistics on HumanLanguage Technology, pages 173?180,Edmonton.Venugopal, Ashish, Jakob Uszkoreit,David Talbot, Franz Och, and JuriGanitkevitch.
2011.
Watermarking theoutputs of structured prediction with anapplication in statistical machinetranslation.
In Proceedings of the 2011Conference on Empirical Methods in NaturalLanguage Processing, pages 1,363?1,372,Edinburgh.Vybornova, M. Olga and Benoit Macq.
2007.A method of text watermarking usingpresuppositions.
In Proceedings of theSPIE International Conference on Security,Steganography, and Watermarking ofMultimedia Contents, pages R1?R10,San Jose, CA.Wayner, Peter.
1992.
Mimic functions.Cryptologia, XVI(3):193?214.Wayner, Peter.
1995.
Strong theoreticalsteganography.
Cryptologia,XIX(3):285?299.Wilcoxon, Frank.
1945.
Individualcomparisons by ranking methods.Biometrics Bulletin, 1(6):80?83.Winstein, Keith.
1999.
Tyrannosauruslex.
Open source.
Available athttp://web.mit.edu/keithw/tlex.Zhang, Yue and Stephen Clark.
2011.Syntax-based grammaticalityimprovement using CCG and guidedsearch.
In Proceedings of the 2011Conference on Empirical Methods in NaturalLanguage Processing, pages 1,147?1,157,Edinburgh.Zhu, Zhemin, Delphine Bernhard, andIryna Gurevych.
2010.
A monolingualtree-based translation model for sentencesimplification.
In Proceedings of the23rd International Conference onComputational Linguistics, COLING ?10,pages 1,353?1,361, Beijing.448
