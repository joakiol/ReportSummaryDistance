Bunsetsu  Ident i f i ca t ion  Us ing  Category -Exc lus ive  Ru lesMasaki Murata Kiyotaka Uchimoto Qing Ma Hitoshi IsaharaC()mmunications Research Laboratory, Ministry of Posts and ~I~lecommunications588-2, \]waoka, Nishi-ku, Kobe, 651-2d92, Japan?
- j  ( ' .
tel:-k81- 78-969-2 \]81 tax: +81- 78-369-2189 http://www-karc.crl, go.j p/ips/murata{ murata,u(:himoto,qma,isahara}(@crl.go.ji)AbstractThis pal>or describes two new bunsetsu identificatkmmethods using supervised learning.
Sin(:e ,Jat)anesesyntactic analysis ix usnally done after bunsetsuidentification, lmnsetsu identiiieation is iml)orl;antfor analyzing Japanese sentences.
In experimentscomparing the four previously available machine-learning methods (decision tree, maximmn-entropymethod, example-based apI)roaeh and deeiskm list,)an(l two new methods llSing categot'y-exclusive rul s~the new method using l;he category-exclusive ruleswith the highest similarity t)erformetl best.1 IntroductionThis paper is about machine learning methods foridentifying bwnsr'ts'~zs, which correspond to Englishphrasal units such as noun phrases and t)rel)ositionalphrases.
Since .Japanes(.'
syntactic analysis ix usu-ally done after lmnselisu identitication (Uchimot;o el;a\].. 1999), i(lentitlying lmnsetsu is important l'or an-alyzing ,J;~p~tnese ltt(}tl(:es.
The  conventional stud-ies on lmnsetsu  identitieation ~ have used hand-maderules (Kameda, \]995; Kurohashi, 3998), })ill; bun-sel;su identification is not an easy task.
Conventionalstudies used many hand-nmde rules develot)ed at thecost of many man-hours.
Kurohashi, tbr examl)le,made 146 rules for lmnsetsu identification (Kuro-hashi, 1998).Itl }l.tl a .
t te t l lp t  to reduce the mnnber of man-hours, we used machine-learning methods for bun-setsu identitication.
Because it; was not clear whichmachine-learning method would 1)e the one most al)-propriate for bunsctsu identification, so we tried avariety of them.
In this paper we rei)orl; ext)er-inlel lts comparing tbur inachine-learning me, thods(decision tree, maximmn entropy, example-based,and decision list; methods) and our new methods us-ing category-exclusive rules.l lhmsetsu ideni,illcation is a ln'oblem similar to ohm,king(lLamshaw and Marcus, 1995; Sang and \h;ellsl;ra, 1999) inother l;mguages.2 Bunsetsu identification problemWe conducted experiments (m the following super-vised learning methods tbr idel~tiflying }mnsetsu:?
\])eeision {;l'ee method?
Max i lnun l  ent ropy  method?
Examt)le-based method (use of sinfilarity)?
Decision list (use of probability and frequency)?
Method 1 (use of exclusive rules)?
Method 2 (use of exclusive rules with the high-est similarity).In general, t)misetsu identification is (tone afl;ermorl)hological and l)efore syntactic analysis.
Mor-1)hological analysis correst)onds to part-of-st)ee(:htagging ill English.
Japanese syntactic structures areusually ref)resented by the.
relations between lmn-setsus, which correspond to l)hrasal units such as anoml l)hrase or a t)repositional 1)hrase in \]r, nglish.St), 1)unsetsu identification is imi)ortant in .lnpanesesentence mmlysis.In this paper, we identit\[y a bunsetsu by usingintbrmation Dora a morl~hological nalysis.
Bun-setsu identitication is treated as the task of decidingwhether to insert a "\[" mark to indicate the partitionbetween two hunsetsus as in Figure 1.
There, fore,bunsetsu identilical;ion is done by judging whether apartition mark should be inserted between two adja-cent nlorphemes or not.
(We.
do not use l;he insertedpartition mark in the tbllowing analysis ill this paperfor the sake of simplicity.
)Our lmnsetsu identification method uses i;t1(} lilOr-phok)gk:al intbrmation of the two preceding and twosucceeding morphemes ofan analyzed space bel;weentwo adjacent morphemes.
We use the following mor-phological information:(i) Major part-of  speech (POS) category, 2(ii) Minor P()S category or intlection tYl)e,(iii) Semantic information (the first three-digit nun>bet of a category nmnlmr as used ill "BGIt"(NLI{,I, 1964:)),2Part-of-spec.ch ~ttegories follow those of 3 \[/MAN (Kuro-hashi and N~tgao, 1998).565bohu .qa(I) nominative-case particle(I identify bunsetsu.
)\[ bunsetsu wo(bunsetsu) objective-case particleI matomeagcru(identify)Figure 1: Example of identified bunsetsusMajor POSMinor POSSemanticsWordbun wo ~ugiru(sentence) (obj) (divide)((I) divide sentences)Noun Particle VerbNormal Noun Case-Particle Normal Formx None 217x wo ku.qiruSymbolPunctuationXXFigure 2: hfformation used in bunsetsu identification(iv) Word (lexical iifformation).For simplicity we do not use the "Semmltic infor-matioif' and "Word" in either of the two outsidemorphemes.Figure 2 shows the information used to judgewhether or not to insert a partition mark in the spacebetween two adjacent morphemes, "wo (obj)" and"kugiru (divide)," in the sentence "bun wo kugiru.
((I) divide sentences).
"3 Bunsetsu  ident i f i ca t ion  process  fo reach  mach ine- learn ing  methoda.1 Deeision-tree methodIn this work we used the program C4.5 (Quinlan,1.995) for the decision-tree l arning method.
Thefour types of information, (i) major POS, (ii) mi-nor POS, (iii) semmltic information, and (iv) word,mentioned in the previous section were also usedas features with the decision-tree l arning method.As shown in Figure 3, the number of features is 12(2 + 4 + 4 + 2) because we do not use (iii) semanticinformation and (iv) word information from the twooutside morphemes.In Figure 2, for example, the value of the feature'the major POS of the far left morpheme' is 'Noun.
'a.2 Maximum-entropy methodThe maximum-entropy method is useful with sparsedata conditions and has been used by many re-searchers (Berger et al, 1996; Ratnaparkhi, 1996;Ratnaparkhi, 1997; Borthwick el; al., 1998; Uchi-moto et al, 1999).
In our maximuln-entropy exper-iment we used Ristad's system (Ristad, 1998).
Theanalysis is performed by calculating the probabilityof inserting or not inserting a partition mark, fromthe output of the system.
Whichever probability ishigher is selected as the desired answer.In the maximum-entropy method, we use the samefour types of morI)hological information, (i) majorPOS, (ii) minor POS, (iii) semantic information, and(iv) word, as in the decision-tree method.
However,it, does not consider a combination of features.
Un-like the decision-tree method, as a result, we had tocombine features mmmally.First we considered a combination of the bits ofeach morphological information.
Because there werefour types of information, the total number of com-binations was 2 ~-  1.
Since this number is largeand intractable, we considered that (i) major POS,(ii) minor POS, (iii) semantic information, aim (iv)word information gradually becolne inore specific inthis order, and we coml)ined the four types of infor-mation in the following way:Information A: (i) major POSIntbrmation B: (i) major POS and (ii) minor POShfformat, ion C: (i) major POS, (ii) minor POS and(iii) semantic informationInformation D: (i) major POS, (ii) minor POS,(iii) semantic informa~aion a d (iv) word(~)We used only Information A and B for the two out-side morphemes because we (lid not use semanticand word information in the same way it is used inthe decision-tree inethod.Next, we considered the combinations ofeach typeof information.
As shown in Figure 4, the numberof combinations was 64 (2 x 4 x 4 x 2).For data sparseness, in addition to the above com-binations, we considered the cases in which frst, oneof the two outside morphemes was not used, sec-ondly, neither of the two outside ones were used, m~dthirdly, only one of the two middle ones is used.
Thenmnber of features used in the maximum-entropymethod is 152, which is obtained as follows: a3When we extr~,cted features  f rom all of  the  ar t ic les  on566Far M't mort)henm Left morl)heme( Ma.ior POS '}~'Major POS'\[ ,J Minor POSI, Minor POgJ +/S,mmntic Information +( \?ord ,2 4Right morl)hemc Far right mort)heineMinor POS f Ma.ior POS ~Semantic Infi)rmation ' + \[ Minor POS jWord4 2Figure 3: Features used in the decision-tree methodFar left morl)hemeInformationhffbrnmtion A }2Left morpheme Right morl)hcme( In format ion!}
(Infbrmation AJ hfformation J Intbrmation B/hfformation & ~ hfformation CI, Infbrmation 1, hffbrmation D4 4Far right morphemeJ" Information& \[hlformation B A }2Figure 4: lJ~,atm'es used in the maximunt-entrol)y met.hod.No.
of t>atures= 2 x 4 x 4 x 2+2 x 4 x 4+ 4 x 4 x 2+ 4 x 4+ 4+ 4= 152In Figure 2, (;lie feature that uses InfornultionB in the far left morl)heme, Infbrnmtion D in theleft mort)heine, Information C in the right mor-pheme, and Information A in the fa.r right mopl/heme is "Noun: Nornml Noun; Particle: Case-Particle: none: wo; Verl): Nornml Form: 217; Sym-bol".
In tim maximmn-entrol)y method we used foreach space 152 ligatures uch as this ()tie.3.3  Example -based  method (use ofs imi lar i ty)An example-based method was t)rollosed t) 3, Nagao(Nagao, 1984) in an attempt to solve I)roblenls inmachine translation.
To resolve a. l)rol)h'm, it; usesthe most similar (;xami)le.
In the i)resent work, theexamt)le-1)ased method imt)artially used the samefour types of information (see Eli.
(1)) as used inthe maxinmm-entrotly method,To use tills method, we must define the similarityof an ini)ut to an example.
We use the 152 1)atternsfl'om the maximum-entropy method to establish thelevel of similarity.
We define the similarity S be-tween all input and an exmnl)le according to whichone of these 152 levels is the lnatching level, as fol-lows.
(The equation reflects the importance of thetwo middle morphemes.
)January 1, 1995 of a Kyoto University corpus (l;hc mnnber ofspaces between mrl)henms was 25,81d) by using this method,the nunfl)e,r of types of features was 1,534,701.S = s(m_t) x s(m-H) x 10,000+ s(m_2) x s(.q.~) (2)Here m- l ,  m4-\], m-2, and m+2 refer respectively tothe left;, rigid;, far M't, ;rod far righl; mortflmnms, ands(x) is the mort)hological similarity of a ll lOl't)hell lex, which is defined as follows:s(x) =1 (when no information of x is matched)2 (when Information A of x is matdmd)3 (when hfl~)rmal:ion B of x is mate, heal)4 (when Information C of x is mat;cited)5 (when Information D of x is matched)(a)Figure 5 shows an exmnple of the levels of sim-ilarity.
When a pattern matches Information A ofall four lnort)henies , uch as "Noun; Particle; Verb;Symbol", its similarity is 40,004 (2 x 2 x 10,000 +2 x 2).
When a pattern matches a pattern, such as" ; Particle: Case-Particle: none: wo; ; ", itssimilarity is 50,001 (5 x 1 x 10,000 + 1 x 1).The exmnl)le-1)ased method extracts the exam-ple with the highest level of similm'ity and checkswhether or not that exami)le is marked.
A partitionmarl{ is inserted in tile input data only when the ex~amt)le iv marked.
When multit)le exalnl)les have thesame highest level of similarity, the selection of tilebest example is ambiguous, hi this case, we counttile number of nlarked and mlinarked sl)aces in allof the examples and choose the larger.a .4  Decis ion- l ist  method (use of p robab i l i tyand f l ' equeney)T i le  decision-list method was proposed by Rivest(Rivest, 1987), in which tile rules are not expressedas a tree structure like in the decision-tree method,567No informationhffornmtion AInformation BInformation CInformation Dbun wo kugiru(senWnce) (obj) (divide)S(X) ?~\]'- 2 7D,--1 ?/Zq-1 '11~'+21 .
.
.
.2 Noun Particle Verb Symbol3 Normal Noun Case-Particle Normal Form Punctuation4 x None 217 x5 x wo kugiru xFigure 5: Exmnple of levels of similaritybut are expanded by combining all the features, andare stored in a one-dimensional list.
A priority or-der is defined in a certain way and all of the rulesare arranged in this order.
The decision-list methodsearches for rules Dora tile top of the list and an-alyzes a particular problem by using only the firstapplicable rule.In this study we used ill the decision-list methodthe same 152 types of patterns that were used in/;liema.ximuln-entropy method.To determine the priority order of the rules, we re-ferred to Yarowsky's method (Yarowsky, 1994) andNishiokwama's method (Nishiokaymna et al, 1998)and used the probability a.nd frequency of each ruleas measures of this priority order.
When nnlltiplerifles had the same probability, the rules were ar-ranged in order of their frequency.Suppose, for example, that Pattern A "Noun:Normal Noun; Particle: Case-Particle: none: wo;Verb: Normal Form: 217; Symhol: Punctuatioif'occurs 13 times in a learlfing set and that tell ofthe occurrences include the inserted partition Inal:k.Suppose also thai; Pattern B "Noun; Particle; Verb;Symbol" occurs 12a times in a learning set and that90 of the occurrences include the mark.This exmnple is recognized by the following rules:Pattern A ~ Partition 76.9% (10/ 13), Freq.
23Pattern B => Partition 73.2% (90/123), Freq.
123Many similar rules were made and were then listedin order of their probabilities and, for any one prob-ability, in order of their frequencies.
This list wassearched from tile top ml(:l the answer was obtainedby using the first, ai)plicable rule.3.5 Method  I (use of eategory-exe lus iverules)So far, we have described the four existing machinelearning methods.
In the next two sections we de-scribe our methods.It is reasoimble to consider tile 152 patterns usedin three of the previous methods.
Now, let us sup-pose that the 152 patterns fl'om the learning set yieldthe statistics of Figure 6.
"Partition" means that the rule determines that apartition mark should be inserted in the input dataand "non-t)arl:ition" ineans that tile rule determinesthat a partition mark should not be inserted.Suppose that when we solve a hypothetical prob-lem Patterns A to G are apt)licable.
If we use thedecision-list inethod, only Rule A is used, which isapplied first, and this determines that a partitionmark should not be inserted.
For Rules B, C, andD, although the fl'equency of each rule is lower thmlthat of Rule A, tile suln of their frequencies of therules is higher, so we think that it is better to useRules B, C, ml(t D than Rule A.
Method 1 followsthis idea, but we do not simply sum up tile frequen-cies.
Instead, we count the munber of exalnples usedill Rules B~ C, and D and judge the category havingtile largest number of exmnplcs that satisfy the pat-tern with the highest probability to be the desiredai1swer.For exmnple, suppose that in the above examt)lethe number of examples atis(ying Rules B, C, andD is 65.
(Because some exmnples overlq) in multi-pie rules, the total nunfl)er of exalnples is actuallysmaller than the total number of tile frequencies ofthe three rules.)
In this case, among the examplesused by the rules having 100% probability, tile nmn-ber of examples of partition is 65, m~d the numberof examt)les of non-t)artitioi~ s 34.
So, we deternlinethat tile desired answer is to partition.A rule having 100% probability is called acategory-exclusive rule because all the data satist~y-ing it belong to one category, which is either parti-tion or noi>partition.
Because for any given spacethe number of rules used call be as large as 152,category-exclusive rules are applie(t often ~.
Method1 uses all of these category-exclusive rules, so we callit tile method using category-exclusive rules.Solving problems by using rules whose prol)abili-l;ies are nol; 100% may result ill the wrong solutions.Almost all of the traditional machine learning meth-ods solve problelns by usiug rules whose i)robabilities4'l'he ratio of the spaces analyzed by using category-exclusive rules is 99.30% (16864/16983) in Experiinent 1 ofSection d. This indicates that ahnost all of the spaces areanalyzed by category-exclusive rules.568\]{,uIe A:l{,uh.'
B:Rule C:Rule D:Rule E:Rule F:Rule G:l 'attern APattern \]Pattern CPatl;crn \])Patt;ern 1'3Pal;l;erll FPat tern  G-?- I)rol)ability of non-i)al|;ition=> probability of partition=> i)rolml)ility of partition=> probal)ility of 1)artition:~ i)robability of partition:~ probability of parl:ition=> probability of non-partitionm0% (a4/34)100% (,33/a3)\]oo% (25/2s)J()0(X, ( 1!
)/ 19)8J.3% (1?)?
}/123)76.9% ( 10/ ~a)57.4% (31{)/540)Figm'e 6: ;Ill (',Xaml)h; of rules used in Method :l1,?equency 34Frequency 33Frequency 25Frequency 19Frequcncy 123Fl'o, qucncy \] 3lq'equc, ncy 540are not J(}0%.
By using such methods, we cannothol)e to improve, a,:curacy.
If we want to improve ac-(;llra(;y~ we nlllst use catt;gory-excltl,qive l'lllCS.
The l 'eare some eases, however, tbr which, even if we takethis at)l)r()ach, eategory-exchlsive rules aye rarely al>plied.
In such cases, we lilllSl; add new feai;ures t()I;he mlalysis to create a situation in which manyc i~tegory -exeh ls ive  ru les  Call \])(; appli(~d.I\]owever, il; is not suflieient to use  t ;~/tt~ory-exclusive rules.
There arc, many nmaningless ruleswhich \]la,1)l)ell to  1)e c~tl;egor3~-ex(;lll,qive o l l ly  ill ~tlearning set.
We lllllSt consider how to (~Iimim/tesuch meaningh;ss rule,q.3.6  Method  2 (r ising category -exc lus iveru les  w i th  the  h ighest  sint i lar it ;y)Method 2 combines the, exami)h>based method andMethod 1.
That  is, it; combines the.
method usingsimilarity m~d the method usint'~ category-exchlsiverules in order to eliminate the meaningless (:art;gory-exclusive rules ment;ion(;(l i l lhe 1)revious t;el;ion.Mel;ho(1 2 also uses 152 patl(!rus for i(tentillving\])llllS(',|;,qll.
q~h(!s(, ~ t)~li;l,(!l'll~q ~/l'(?
llF,(RI ;IS rules i~ the,q;lllle ~,\;;ty }is ill Met\ ] lo( \ ]  \].
l)esir('d HIISWtH',q ill'(; (h;t(; l-mined by using the rule.
having the high(>t probabil-ity.
When mull;iple rules have the same 1)rolmbility,M(;thod 2 uses the wdue of the similarity describedin the section of the examl)h>based m(;thod and }lll-alyzes the 1)robk:m with the rule having the highestsimihu'ity.
When multiple rules have th(; stone prob-nbilil;y and similm'ity, the method takes the exam-pies used by the rules having the highest probabil ityand the higlmst si,nilarity, and chooses the (:ategorywith the larger llllllI))CF Of exami)les as t:hc desiredanswer~ in the same way as in Method 1.Itowever, when (:ategory-c.xchlsive rules havingl l lt iro tha l l  Olte fre(l l lel lCy exist, the a})ove t ) roccdl l r ( ;is performed after el iminating all of the category-exclusive rules having one frequency, in el;her words,category-exclusive rules having more than one fl'e-quency are giwm a higher priority than category-exclusive rules having only one.
flTo.qll(;nc.y })lit hav-ing ~ high(w similarity.
This is 1)c(:ause eategory-(;xclusivc rules having only one fl'equen(:y are not soreliabh',.4 Experiments and discussionIn our experiments we used a Kyoto  University texteorlms (Knrohashi and Nagao, 1997), which is atagged corpus made Ul) of articles fi'om the Mainichinewspaper.
All exl)eriments reported in this paperwe.re performed using art, ielcs dated fi'om ,\]mmary\] to 5, 1995.
We obtained the correct infi)rnmtion()n morphoh)gy and }mnse.t;su identiticathm from thetagged corpus.The following experiments were conducted to de-termine which supervised \]earnillg~ lnethod achievesthe high<'.st a(:Cllra(:y l~tl;e.?
Exlmriment \]\[,(;arninl,; set: ,Janllary 1, 1.995~J?t;Sl; st?t: ,\]atlll~/l'y 3, 1.995?
\]';xt)eriment 2Learning set: 3ammry 4, 1995Test set: .\]a.nuary 5, 1995ltecm>e we used F, xlmriment \] in maki,lg MethodI and Method 2, \]i;Xl)erinieut 71 is a ch)sc'd data..~etfor Mel:l~od \] and Method 2.
So, we l)crformed Ex-lmriment 2.The ,'(;suits arc.
listed in '12fl)lt;,q I to d. \Ve usedKNP2.0b4 (Kurohashi, 11997) mM KNP2.0t/6 (Kuro-hashi: 1998), which are bmlsetsu identitication andsyntael;i(" analysis systems using tmmy hand-maderules in addition 1;o the six methods des(:ribed inSection 3.
Be('mtse KNP is not based on a machinelearning inethod but :many hand-made rules, in theKNP results "Learning selY and '~'.Test et" in the ta-llies have nt) meanings.
In the eXll(wiment of KNP,we also uses morphological information in a corpus.~\].~hc ';F': ill l;\]le tables indicates the F-measure~ whichis the.
harmonic mean of a recall and a precision.
Arecall is l;he fl'action of correctly identilied partit ionsout of all the partitions.
A t)reeision is the ffae-th)n of correctly identitied partit ions out of all theSlmCeS which were judged to have a partit ion markinserted.Tables I to -/I show the.
following results:?
In the test set I;he dc.cision-tree method wasa little better thmt the maximmn-entropy569Table 1: Results of learning set of Exper iment  1Method D ~Decision %-ee 99.58%Maximum Entropy 99.20%Example-Based 99.98%Decision List 99.98%Method 1 99.98%Method 2 99.98%KNP 2.0114 99.23%KNP 2.0116 99.73%Recall Precision99.66% 99.51%99.35% 99.06%100.00% 99.97%100.00% 99.97%100.00% 99.97%100.00% 99.97%99.78% 98.69%99.77% 99.69%The number of spaces between two )nor flmmes is25,814.
The number of lmrtitions is 9,523.Table 3: Results of learning set of Exi)er iment 2MethodDecision TreeMaximum EntropyExample-BasedDecision ListMethod 1Method 2KNP 2.01)4KNP 2.06699.07% I99.99% I99.99% I99.99% I99.99% I98.94% I99.47% IRecaU Precision99.71% 99.69%99.23% 98.92%100,00% 99.98%100.00% 99.98%100.00% 99.98%100.00% 99.98%99.50% 98.39%99.47% 99.48%The mlmber of spaces between two mor )heines is27,665.
The number of partitions is 10 143.Table 2: Results of test set of Exper iment  1Method~ ion  TreeMaximmn EntropyExample-BasedDecision ListMethod 1Method 2KNP 2.0b4KNP 2.066- F Recall98.87% 98.67% 99.08%98.90% 98.75% 99.06%99.02% 198.69% 99.36%98.95% i 98.43% !
99.48%98.98% 198.54%!
99.43%99.16% I 98.88% ' 99.45%99.13% 99.72% !
98.54%99.66% ~_99.68% !
99.64%:)heroes is The lmmber of spaces between two morPrecision16,983.
The mmiber of partitions is 6,166.T~ble 4: Results of test set of Exper iment  2MethodDecision TreeM aximmn EntropyExample-BasedDecision ListMethod 1Method 2KNP 2.0b4KNP 2.01)6P RTcad I Precision98.50% 98.51% I 98.49%-98.57% 98.55%1i 98.82% 98.71%1198.75% 98.27%1i 98.79% 98.54% Ii 98.9\[1% 98.65% I199.
(/7% 99.43%1L 99.51% 99.40% ~_98.59%98.93%99.23%99.43%99.15%98.71%99.61%The nmnber of" spaces between two morphemes i32,3o4.
The number of partitions is 11,756.method.
Al though the maximuln-entropymethod has a weak point  in that  it, does notlearn the combinat ions of features, we couldovercome this weakness by malting almost all ofthe combilmtions of features to produce a higheraccuracy rate.?
Tile decision-list n lethod was bet ter  t itan themaximum-entropy method in this experinmnt.?
Tile example-based nlethod obtained the high-est accuracy rate among the four exist ing meth-ods.?
Altt lough Method 1, which uses tim category-exclusive rule, was worse than the exmnple-based method,  it was better  than ti le decision-list method.
One reason for this was thatti le decision-l ist metl lod chooses rules rmldomlywhen mult iple rules have identical probabi l i t iesmid fl'equeneies.?
Method 2, which uses the category-exchlsiverule with the highest similarity, achieved thehighest accuracy rate among ti le supervisedlearning methods.?
Tim example-based method,  tim decision-l istinethod, Method 1 and Method 2 obta ined ac-curacy rates of about  100% for the leanfing set.This indicates that  these methods m:e especial lystrong for learning sets.?
Tile two methods using similarity example-based method mid Method 2) were always bet-ter than the other methods, indicat ing that  theuse of s imilar ity is eflective if we can define itapprol)r iately.?
We carried out experinmnts by using KNP,  asystem that  uses ninny ha.nd-made rules.
TheF-measure of KNP was highest in the test set.?
We used two versions of KNP, KNP 2.0b4 andKNP 2.0b6.
The lat ter  was mud l  better  t lmntlm former, iudicat ing tha.t the improvementsmade by hand are  effective.
But, the mainte-nance of rules by hand has a l imit, so the im-provements made by hand are not always effec-tive.Tlle above exper iments indicate that  Method 2 isbest among the machine learning methods '5.In Table 5 we show some cases which were par-t i t ioned incorrectly with KNP but correctly with51n these experiments, the.
differences were very small.But, we think that the differences are significant to some ex-tent because we performed Experiment 1 and Experiment 2,the data we used are a large corplls containing about a fewten thousand morphemes and tagged objectively in advance,and the difference of about 0.1% is large in the precisions of99%.570Table 5: Cases when KNP was incorrect and Method2 wan correctko tsukotsu \[ N H1,,'Tr 9aman.sh i(steadily) (lm prurient wit;h)L_{"" 1)e patient with ... steadily)lyoyuu wo \] motte \]~xrT;~l~ shirizokei(enough Stl'engi;h) obj (have) (1)eat off)(... beat off ... having enough sl;rength)~aisha wo I gurupu-wake \ [ ~,.o.,v,,,,y obj (~r,,,,pi,,~) (do)(... 11o grouping companies)Method 2.
A partition with "NEED" indicates thatKNP missed inserting the i)artition mark, and a par-tition with "WRONG" indicates that KNP insertedthe partitiol~ mark incorrectly.
In the test set of Ex-periment 1, the F-measure of KNP2.0b6 was 99.66%.The F-measur(.
~ increases to 99.83%, ml(ler the as-sumption that when KNP2.0t)6 or Method 2 in cor-rect, the answer is correct.
Although the accuracyrate for KNP2.0b6 was high, there were some casesin which KNP t)artitioned incorrectly and Method2 partitioned correctly, A combination of Method2 with KNP2.0b6 may be able to iml)rove the F-lile~lsllrO.The only 1)revious research resolving Imnnetsuidentification by machine learning methods, in thework by Zhang (Zhang and Ozeki, 1998).
Thedecision-tree, ine, thod was used in this work.
Butthis work used only a small mmther of intor-l l l;ttion for t)llllsetsll identification" and (lid notachieve ll igh accuracy rat;es.
(The  recall  ratewas 97 .6%(=2502/ (2502+62) ) ,  the 1)recision ratewas 92 .4%(=2502/ (2502+205) ) ,  and F -measure  was94.2%.
)5 Conc lus ionTo solve tile t)roblem of aecm'ate btmsetsu iden-tification, we carried out ext)eriments comt)aringtbur existing machine-learning methods (decision-tree method, maxilnum-entrol)y method, examI)le-based method and decision-list method).
We ob-tained the following order of acem'acy in bunsetsuidentification.Example-Based > Decision List >Maximum Entropy > Decision TreeWe also described a new method which usescategory-exclusive rules with the highest similarity.This method performed better than the other learn-ing methods in ore" exi)eriments.
(>l'his work used oifly the POS information of the two roofphemes of an analyzed space.ReferencesAdam L. Berger, Stephen A. Della Pietra, and VincentJ.
\])ella Pietra.
1996.
A Maximum Entropy Approach toNatural Language Processing.
Computational Linguistics,22(l):ag-rl.Andrew Borthwicl% John Sterling, Eugene Agichtein, aimRalph Grishlnan.
1998.
Exploiting Diverse KnowledgeSources via Maximum I';ntropy in Named Entity ll.ecoglfi-tion.
In Proceedings of the Sixth Workshop on Very LaT~leCorpora, pages 152 -160.Masayuki Kameda.
1995.
Simple Jalmnese analysis tool q jp.The Association for Natural Lan, guage Processing, the IslNational Convention, pages 349-352.
(ill .
)~ti)~tlleSe ).Sadao Kurohashi and Makot<) Nagao.
1997.
Kyoto Universitytext corpus 1)ro./ect.
pages 115-118.
(in .lapanese).Sadao lgurohashi and MM~oto Nagao, 1998.
Japanese Mofphological Analysis System JUMAN version 3.5.
\])ei)art-mcnt of Informatics, Kyoto University.
(in Japanese).Sadao Kurohashi, 1997.
Japanese Dependency/Case Struc-ture Anahdzer KNI ) version 2.Obj.
Department of lnfofmatics, Kyoto Ulfiversity.
(in Japmmse).Sadao Kurohashi, 1!)!18.
Japanese Dependency/Uase Struc-ture Analyzer KNI' version 2.0b6.
Del)artment of Infor-m~tics, l(yoto University.
(in .lapmmse.
).Makoto Nagao.
1984.
A I,'ralneworl( of a Mechanical Transh>ti(m between Jai)anese alld English 1)y Analogy lh'incit)le.Artificial a\]l(l Iluman hitelligence~ pages 173 q80.Shigeyuld Nishiokayama, Takehito Utsuro, and Yu.ii Mat-sumoto.
119!18.
Extracting preference of dependency be-twee/t ,Japanese subordinate clauses from corlms.
IE ICE-WGNL098- ll, pages 31-38.
(in .lapnese).NI,I{,\].
1964.
(National Language Resemvh Institute).Word List b?l Semantic Principles.
Syuei SyuI)pan.
(inJapanese)..\].
1{.. Quinl;m. 1995.
Pro qrams for machine learning.Lmme A. I{;mlshaw ;uld Mitchell 1'.
Marcus.
1(.)(.15.
Textclmnking using transformationq)ased l arning.
\]11 Proceed-ira.IS of th, e Th, ird Workshop on Very La*#e Corpora, l)ages82 (.
)4.Adwait l{.atnat)arklfi.
1996.
A Maximum l,;nl;ropy Model tin"l'art-OlCSt)eech Tagging.
1)rocecdin9 s of P)mpirieal Methodfor Natural Language l'roeessings, pages 133 1,12.Adwait ll,atnaparkhi, 19!17.
A l,inear Observed Time Statis-ticaI l'm'ser Based ou Maximum \]~ntropy Models.
ht t~l'o -ceedings of Empirical Method for Natural l, an.quage l'ro-cessings.Eric Sven Ristad.
1998.
Maximum Elltropy ModelingToolkit, Release 1.6 beta.
http://www,mnemonic.com/software/metal.Ronald L. Rivest.
1987. lmarning l)ecision lasts.
MachineLearning~ 2:229 -246.Erik P. Tjong Kim Sang and .lorn \reenstra.
1999. ll.el)re-senting text chunks, in EA CL'99.Kiyotaka Uehimoto, Satoshi Sekiim, and IIitoshi Isalmra.1999.. Japanese dependency structure analysis based onmaximmn entrol)y models.
In Proceedings of the NinthCoTtference of the 15"ulwpeau Chapter of the Associationfor Computational Linguistics (P)A CL), pages 196-203.\])avid Yarowsky.
1!)94.
Decision lists fo," lexieal ambiguityresolution: Application to accent restoration in Spanishand l,Yench.
In 22th Alt, Tt~tal Meeting of the Assoeitationof the Computational Linguistics, pages 88-95.Yujie Zlmng and Kazuhiko Ozeki.
1998.
The applica-tion of classitieation trees to bunsetsu segmentation ofJat)anese sentences.
Journal of Natural Language Process-ing, 5(4):17-33.571
