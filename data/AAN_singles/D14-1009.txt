Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 78?89,October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational LinguisticsStrongly Incremental Repair DetectionJulian Hough1,21Dialogue Systems GroupFaculty of Linguisticsand LiteratureBielefeld Universityjulian.hough@uni-bielefeld.deMatthew Purver22Cognitive Science Research GroupSchool of Electronic Engineeringand Computer ScienceQueen Mary University of Londonm.purver@qmul.ac.ukAbstractWe present STIR (STrongly Incremen-tal Repair detection), a system that de-tects speech repairs and edit terms ontranscripts incrementally with minimal la-tency.
STIR uses information-theoreticmeasures from n-gram models as its prin-cipal decision features in a pipeline ofclassifiers detecting the different stages ofrepairs.
Results on the Switchboard dis-fluency tagged corpus show utterance-finalaccuracy on a par with state-of-the-art in-cremental repair detection methods, butwith better incremental accuracy, fastertime-to-detection and less computationaloverhead.
We evaluate its performance us-ing incremental metrics and propose newrepair processing evaluation standards.1 IntroductionSelf-repairs in spontaneous speech are annotatedaccording to a well established three-phase struc-ture from (Shriberg, 1994) onwards, and as de-scribed in Meteer et al.
(1995)?s Switchboard cor-pus annotation handbook:John [ likes?
??
?reparandum+ {uh}?
??
?interregnumloves ]?
??
?repairMary (1)From a dialogue systems perspective, detecting re-pairs and assigning them the appropriate structureis vital for robust natural language understanding(NLU) in interactive systems.
Downgrading thecommitment of reparandum phases and assigningappropriate interregnum and repair phases permitscomputation of the user?s intended meaning.Furthermore, the recent focus on incrementaldialogue systems (see e.g.
(Rieser and Schlangen,2011)) means that repair detection should oper-ate without unnecessary processing overhead, andfunction efficiently within an incremental frame-work.
However, such left-to-right operability onits own is not sufficient: in line with the princi-ple of strong incremental interpretation (Milward,1991), a repair detector should give the best re-sults possible as early as possible.
With one ex-ception (Zwarts et al., 2010), there has been nofocus on evaluating or improving the incrementalperformance of repair detection.In this paper we present STIR (Strongly In-cremental Repair detection), a system which ad-dresses the challenges of incremental accuracy,computational complexity and latency in self-repair detection, by making local decisions basedon relatively simple measures of fluency and sim-ilarity.
Section 2 reviews state-of-the-art methods;Section 3 summarizes the challenges and explainsour general approach; Section 4 explains STIR indetail; Section 5 explains our experimental set-upand novel evaluation metrics; Section 6 presentsand discusses our results and Section 7 concludes.2 Previous workQian and Liu (2013) achieve the state of the art inSwitchboard corpus self-repair detection, with anF-score for detecting reparandum words of 0.841using a three-step weighted Max-Margin Markovnetwork approach.
Similarly, Georgila (2009)uses Integer Linear Programming post-processingof a CRF to achieve F-scores over 0.8 for reparan-dum start and repair start detection.
However nei-ther approach can operate incrementally.Recently, there has been increased interestin left-to-right repair detection: Rasooli andTetreault (2014) and Honnibal and Johnson (2014)present dependency parsing systems with reparan-dum detection which perform similarly, the latterequalling Qian and Liu (2013)?s F-score at 0.841.However, while operating left-to-right, these sys-tems are not designed or evaluated for their incre-mental performance.
The use of beam search over78different repair hypotheses in (Honnibal and John-son, 2014) is likely to lead to unstable repair labelsequences, and they report repair hypothesis ?jit-ter?.
Both of these systems use a non-monotonicdependency parsing approach that immediately re-moves the reparandum from the linguistic anal-ysis of the utterance in terms of its dependencystructure and repair-reparandum correspondence,which from a downstream NLU module?s perspec-tive is undesirable.
Heeman and Allen (1999) andMiller and Schuler (2008) present earlier left-to-right operational detectors which are less accu-rate and again give no indication of the incremen-tal performance of their systems.
While Heemanand Allen (1999) rely on repair structure templatedetection coupled with a multi-knowledge-sourcelanguage model, the rarity of the tail of repairstructures is likely to be the reason for lower per-formance: Hough and Purver (2013) show thatonly 39% of repair alignment structures appearat least twice in Switchboard, supported by the29% reported by Heeman and Allen (1999) onthe smaller TRAINS corpus.
Miller and Schuler(2008)?s encoding of repairs into a grammar alsocauses sparsity in training: repair is a general pro-cessing strategy not restricted to certain lexicalitems or POS tag sequences.The model we consider most suitable for in-cremental dialogue systems so far is Zwarts etal.
(2010)?s incremental version of Johnson andCharniak (2004)?s noisy channel repair detector,as it incrementally applies structural repair anal-yses (rather than just identifying reparanda) andis evaluated for its incremental properties.
Fol-lowing (Johnson and Charniak, 2004), their sys-tem uses an n-gram language model trained onroughly 100K utterances of reparandum-excised(?cleaned?)
Switchboard data.
Its channel model isa statistically-trained S-TAG parser whose gram-mar has simple reparandum-repair alignment rulecategories for its non-terminals (copy, delete, in-sert, substitute) and words for its terminals.
Theparser hypothesises all possible repair structuresfor the string consumed so far in a chart, beforepruning the unlikely ones.
It performs equallywell to the non-incremental model by the end ofeach utterance (F-score = 0.778), and can makedetections early via the addition of a speculativenext-word repair completion category to their S-TAG non-terminals.
In terms of incremental per-formance, they report the novel evaluation met-ric of time-to-detection for correctly identified re-pairs, achieving an average of 7.5 words from thestart of the reparandum and 4.6 from the start ofthe repair phase.
They also introduce delayed ac-curacy, a word-by-word evaluation against gold-standard disfluency tags up to the word before thecurrent word being consumed (in their terms, theprefix boundary), giving a measure of the stabilityof the repair hypotheses.
They report an F-scoreof 0.578 at one word back from the current prefixboundary, increasing word-by-word until 6 wordsback where it reaches 0.770.
These results are thepoint-of-departure for our work.3 Challenges and ApproachIn this section we summarize the challenges forincremental repair detection: computational com-plexity, repair hypothesis stability, latency of de-tection and repair structure identification.
In 3.1we explain how we address these.Computational complexity Approaches to de-tecting repair structures often use chart storage(Zwarts et al., 2010; Johnson and Charniak, 2004;Heeman and Allen, 1999), which poses a com-putational overhead: if considering all possibleboundary points for a repair structure?s 3 phasesbeginning on any word, for prefixes of length nthe number of hypotheses can grow in the orderO(n4).
Exploring a subset of this space is nec-essary for assigning entire repair structures as in(1) above, rather than just detecting reparanda:the (Johnson and Charniak, 2004; Zwarts et al.,2010) noisy-channel detector is the only systemthat applies such structures but the potential run-time complexity in decoding these with their S-TAG repair parser is O(n5).
In their approach,complexity is mitigated by imposing a maximumrepair length (12 words), and also by using beamsearch with re-ranking (Lease et al., 2006; Zwartsand Johnson, 2011).
If we wish to include fulldecoding of the repair?s structure (as argued byHough and Purver (2013) as necessary for full in-terpretation) whilst taking a strictly incrementaland time-critical perspective, reducing this com-plexity by minimizing the size of this search spaceis crucial.Stability of repair hypotheses and latency Us-ing a beam search of n-best hypotheses on a word-by-word basis can cause ?jitter?
in the detector?soutput.
While utterance-final accuracy is desired,79for a truly incremental system good intermedi-ate results are equally important.
Zwarts et al.
(2010)?s time-to-detection results show their sys-tem is only certain about a detection after process-ing the entire repair.
This may be due to the stringalignment-inspired S-TAG that matches repair andreparanda: a ?rough copy?
dependency only be-comes likely once the entire repair has been con-sumed.
The latency of 4.6 words to detection anda relatively slow rise to utterance-final accuracy upto 6 words back is undesirable given repairs havea mean reparandum length of ?1.5 words (Houghand Purver, 2013; Shriberg and Stolcke, 1998).Structural identification Classifying repairshas been ignored in repair processing, despite thepresence of distinct categories (e.g.
repeats, sub-stitutions, deletes) with different pragmatic effects(Hough and Purver, 2013).1 This is perhaps due tolack of clarity in definition: even for human anno-tators, verbatim repeats withstanding, agreementis often poor (Hough and Purver, 2013; Shriberg,1994).
Assigning and evaluating repair (not justreparandum) structures will allow repair interpre-tation in future; however, work to date evaluatesonly reparandum detection.3.1 Our approachTo address the above, we propose an alternativeto (Johnson and Charniak, 2004; Zwarts et al.,2010)?s noisy channel model.
While the modelelegantly captures intuitions about parallelism inrepairs and modelling fluency, it relies on string-matching, motivated in a similar way to automaticspelling correction (Brill and Moore, 2000): it as-sumes a speaker chooses to utter fluent utteranceX according to some prior distribution P (X), buta noisy channel causes them instead to utter anoisy Y according to channel model P (Y |X).Estimating P (Y |X) directly from observed datais difficult due to sparsity of repair instances, so atransducer is trained on the rough copy alignmentsbetween reparandum and repair.
This approachsucceeds because repetition and simple substitu-tion repairs are very common; but repair as a psy-chological process is not driven by string align-ment, and deletes, restarts and rarer substitutionforms are not captured.
Furthermore, the noisychannel model assumes an inherently utterance-global process for generating (and therefore find-1Though see (Germesin et al., 2008) for one approach,albeit using idiosyncratic repair categories.ing) an underlying ?clean?
string ?
much as sim-ilar spelling correction models are word-global ?we instead take a very local perspective here.In accordance with psycholinguistic evidence(Brennan and Schober, 2001), we assume charac-teristics of the repair onset allow hearers to detectit very quickly and solve the continuation prob-lem (Levelt, 1983) of integrating the repair intotheir linguistic context immediately, before pro-cessing or even hearing the end of the repair phase.While repair onsets may take the form of inter-regna, this is not a reliable signal, occurring inonly ?15% of repairs (Hough and Purver, 2013;Heeman and Allen, 1999).
Our repair onset de-tection is therefore driven by departures from flu-ency, via information-theoretic features derivedincrementally from a language model in line withrecent psycholinguistic accounts of incrementalparsing ?
see (Keller, 2004; Jaeger and Tily, 2011).Considering the time-linear way a repair is pro-cessed and the fact speakers are exponentially lesslikely to trace one word further back in repair asutterance length increases (Shriberg and Stolcke,1998), backwards search seems to be the most ef-ficient reparandum extent detection method.2 Fea-tures determining the detection of the reparan-dum extent in the backwards search can also beinformation-theoretic: entropy measures of dis-tributional parallelism can characterize not onlyrough copy dependencies, but distributionally sim-ilar or dissimilar correspondences between se-quences.
Finally, when detecting the repair endand structure, distributional information allowscomputation of the similarity between reparan-dum and repair.
We argue a local-detection-with-backtracking approach is more cognitivelyplausible than string-based left-to-right repair la-belling, and using this insight should allow an im-provement in incremental accuracy, stability andtime-to-detection over string-alignment driven ap-proaches in repair detection.4 STIR: Strongly Incremental RepairdetectionOur system, STIR (Strongly Incremental Repairdetection), therefore takes a local incremental ap-2We acknowledge a purely position-based model forreparandum extent detection under-estimates prepositions,which speakers favour as the retrace start and over-estimatesverbs, which speakers tend to avoid retracing back to, prefer-ring to begin the utterance again, as (Healey et al., 2011)?sexperiments also demonstrate.80?John?
?likes?S0S1S2T0?John?
?likes?
?uh?edS0S1S2S3edT1?John?
?likes?
?uh?ed?loves?rpstartS0S1S2S3ed?S4rpstartT2?John?
?likes?rmstartrmend?uh?ed?loves?rpstartS0S1S2rmstartrmendS3edS4rpstartT3?John?
?likes?rmstartrmend?uh?ed?loves?rpstartrpsubendS0S1S2rmstartrmendS3edS4rpstartrpsubendT4?John?
?likes?rmstartrmend?uh?ed?loves?rpstartrpsubend?Mary?S0S1S2rmstartrmendS3edS4rpstartrpsubendS5T5Figure 1: Strongly Incremental Repair Detectionproach to detecting repairs and isolated edit terms,assigning words the structures in (2).
We in-clude interregnum recognition in the process, dueto the inclusion of interregnum vocabulary withinedit term vocabulary (Ginzburg, 2012; Hough andPurver, 2013), a useful feature for repair detection(Lease et al., 2006; Qian and Liu, 2013).{...
[rmstart...rmend+ {ed}rpstart...rpend]......{ed}...(2)Rather than detecting the repair structure in itsleft-to-right string order as above, STIR functionsas in Figure 1: first detecting edit terms (possiblyinterregna) at step T1; then detecting repair onsetsrpstartat T2; if one is found, backwards searchingto find rmstartat T3; then finally finding the re-pair end rpendat T4.
Step T1 relies mainly onlexical probabilities from an edit term languagemodel; T2 exploits features of divergence from afluent language model; T3 uses fluency of hypoth-esised repairs; and T4 the similarity between dis-tributions after reparandum and repair.
However,each stage integrates these basic insights via mul-tiple related features in a statistical classifier.4.1 Enriched incremental language modelsWe derive the basic information-theoretic featuresrequired using n-gram language models, as theyhave a long history of information theoretic anal-ysis (Shannon, 1948) and provide reproducible re-sults without forcing commitment to one partic-ular grammar formalism.
Following recent workon modelling grammaticality judgements (Clarket al., 2013), we implement several modificationsto standard language models to develop our basicmeasures of fluency and uncertainty.For our main fluent language models we traina trigram model with Kneser-Ney smoothing(Kneser and Ney, 1995) on the words and POStags of the standard Switchboard training data(all files with conversation numbers beginningsw2*,sw3* in the Penn Treebank III release), con-sisting of ?100K utterances, ?600K words.
Wefollow (Johnson and Charniak, 2004) by clean-ing the data of disfluencies (i.e.
edit terms andreparanda), to approximate a ?fluent?
languagemodel.
We call these probabilities plexkn, pposknbe-low.33We suppress the pos and lex superscripts below where werefer to measures from either model.81We then derive surprisal as our principal defaultlexical uncertainty measurement s (equation 3) inboth models; and, following (Clark et al., 2013),the (unigram) Weighted Mean Log trigram prob-ability (WML, eq.
4)?
the trigram logprob of thesequence divided by the inverse summed logprobof the component unigrams (apart from the firsttwo words in the sequence, which serve as thefirst trigram history).
As here we use a local ap-proach we restrict the WML measures to singletrigrams (weighted by the inverse logprob of thefinal word).
While use of standard n-gram prob-ability conflates syntactic with lexical probability,WML gives us an approximation to incrementalsyntactic probability by factoring out lexical fre-quency.s(wi?2.
.
.
wi) = ?
log2pkn(wi| wi?2, wi?1) (3)WML(w0.
.
.
wn) =?i=ni=2log2pkn(wi| wi?2, wi?1)?
?nj=2log2pkn(wj)(4)Distributional measures To approximate un-certainty, we also derive the entropy H(w | c) ofthe possible word continuations w given a contextc, from p(wi| c) for all words wiin the vocabu-lary ?
see (5).
Calculating distributions over theentire lexicon incrementally is costly, so we ap-proximate this by constraining the calculation towords which are observed at least once in contextc in training, wc= {w|count(c, w) ?
1} , assum-ing a uniform distribution over the unseen suffixesby using the appropriate smoothing constant, andsubtracting the latter from the former ?
see eq.
(6).Manual inspection showed this approximationto be very close, and the trie structure of our n-gram models allows efficient calculation.
We alsomake use of the Zipfian distribution of n-gramsin corpora by storing entropy values for the 20%most common trigram contexts observed in train-ing, leaving entropy values of rare or unseen con-texts to be computed at decoding time with littlesearch cost due to their small or empty wcsets.H(w | c) = ?
?w?V ocabpkn(w | c) log2pkn(w | c) (5)H(w | c) ?[?
?w?wcpkn(w | c) log2pkn(w | c)]?
[n?
?
log2?
]where n = |V ocab| ?
|wc|and ?
=1?
?w?wcpkn(w | c)n(6)Given entropy estimates, we can also sim-ilarly approximate the Kullback-Leibler (KL)divergence (relative entropy) between distribu-tions in two different contexts c1and c2, i.e.?
(w|c1) and ?
(w|c2), by pair-wise computingp(w|c1) log2(p(w|c1)p(w|c2)) only for words w ?
wc1?wc2, then approximating unseen values by assum-ing uniform distributions.
Using pknsmoothed es-timates rather than raw maximum likelihood es-timations avoids infinite KL divergence values.Again, we found this approximation sufficientlyclose to the real values for our purposes.
All suchprobability and distribution values are stored inincrementally constructed directed acyclic graph(DAG) structures (see Figure 1), exploiting theMarkov assumption of n-gram models to allow ef-ficient calculation by avoiding re-computation.4.2 Individual classifiersThis section details the features used by the 4 indi-vidual classifiers.
To investigate the utility of thefeatures used in each classifier we obtain valueson the standard Switchboard heldout data (PTB IIIfiles sw4[5-9]*: 6.4K utterances, 49K words).4.2.1 Edit term detectionIn the first component, we utilise the well-knownobservation that edit terms have a distinctivevocabulary (Ginzburg, 2012), training a bigrammodel on a corpus of all edit words annotated inSwitchboard?s training data.
The classifier simplyuses the surprisal slex from this edit word model,and the trigram surprisal slex from the standardfluent model of Section 4.1.
At the current positionwn, one, both or none of words wnand wn?1areclassified as edits.
We found this simple approacheffective and stable, although some delayed deci-sions occur in cases where slex and WMLlex arehigh in both models before the end of the edit, e.g.
?I like?
?
?I {like} want...?.
Words classified ased are removed from the incremental processinggraph (indicated by the dotted line transition inFigure 1) and the stack updated if repair hypothe-ses are cancelled due to a delayed edit hypothesisof wn?1.4.2.2 Repair start detectionRepair onset detection is arguably the most crucialcomponent: the greater its accuracy, the better theinput for downstream components and the lesserthe overhead of filtering false positives required.82i havent hadanygood reallyverygood experience with childcare?1.4?1.2?1.0?0.8?0.6?0.4?0.20.0WMLFigure 2: WMLlex values for trigrams for a repaired utterance exhibiting the drop at the repair onsetWe use Section 4.1?s information-theoretic fea-tures s,WML,H for words and POS, and intro-duce 5 additional information-theoretic features:?WML is the difference between the WML val-ues at wn?1and wn; ?H is the difference in en-tropy between wn?1and wn; InformationGainis the difference between expected entropy atwn?1and observed s at wn, a measure thatfactors out the effect of naturally high entropycontexts; BestEntropyReduce is the best reduc-tion in entropy possible by an early rough hy-pothesis of reparandum onsets within 3 words;and BestWMLBoost similarly speculates on thebest improvement of WML possible by positingrmstartpositions up to 3 words back.
We also in-clude simple alignment features: binary featureswhich indicate if the word wi?xis identical to thecurrent word wifor x ?
{1, 2, 3}.
With 6 align-ment features, 16 N-gram features and a singlelogical feature edit which indicates the presenceof an edit word at position wi?1, rpstartdetectionuses 23 features?
see Table 1.We hypothesised repair onsets rpstart wouldhave significantly lower plex (lower lexical-syntactic probability) and WMLlex (lower syntac-tic probability) than other fluent trigrams.
Thiswas the case in the Switchboard heldout datafor both measures, with the biggest differenceobtained for WMLlex (non-repair-onsets: -0.736(sd=0.359); repair onsets: -1.457 (sd=0.359)).
Inthe POS model, entropy of continuation Hpos wasthe strongest feature (non-repair-onsets: 3.141(sd=0.769); repair onsets: 3.444 (sd=0.899)).
Thetrigram WMLlex measure for the repaired utter-ance ?I haven?t had any [ good + really very good] experience with child care?
can be seen in Fig-ure 2.
The steep drop at the repair onset shows theusefulness of WML features for fluency measures.To compare n-gram measures against other lo-cal features, we ranked the features by Informa-tion Gain using 10-fold cross validation over theSwitchboard heldout data?
see Table 1.
The lan-guage model features are far more discriminativethan the alignment features, showing the potentialof a general information-theoretic approach.4.2.3 Reparandum start detectionIn detecting rmstartpositions given a hypothe-sised rpstart(stage T3 in Figure 1), we use thenoisy channel intuition that removing the reparan-dum (from rmstartto rpstart) increases fluencyof the utterance, expressed here as WMLboost asdescribed above.
When using gold standard in-put we found this was the case on the heldoutdata, with a mean WMLboost of 0.223 (sd=0.267)for reparandum onsets and -0.058 (sd=0.224) forother words in the 6-word history- the negativeboost for non-reparandum words captures the in-tuition that backtracking from those points wouldmake the utterance less grammatical, and con-versely the boost afforded by the correct rmstartdetection helps solve the continuation problem forthe listener (and our detector).Parallelism in the onsets of rpstartandrmstartcan also help solve the continuationproblem, and in fact the KL divergence be-tween ?pos(w | rmstart, rmstart?1) and ?pos(w |rpstart, rpstart?1) is the second most useful fea-ture with average merit 0.429 (+- 0.010) in cross-83validation.
The highest ranked feature is ?WML(0.437 (+- 0.003)) which here encodes the drop inthe WMLboost from one backtracked position tothe next.
In ranking the 32 features we use, againinformation-theoretic ones are higher ranked thanthe logical features.average merit average rank attribute0.139 (+- 0.002) 1 (+- 0.00) Hpos0.131 (+- 0.001) 2 (+- 0.00) WMLpos0.126 (+- 0.001) 3.4 (+- 0.66) WMLlex0.125 (+- 0.003) 4 (+- 1.10) spos0.122 (+- 0.001) 5.9 (+- 0.94) wi?1= wi0.122 (+- 0.001) 5.9 (+- 0.70) BestWMLBoostlex0.122 (+- 0.002) 5.9 (+- 1.22) InformationGainpos0.119 (+- 0.001) 7.9 (+- 0.30) BestWMLBoostpos0.098 (+- 0.002) 9 (+- 0.00) H lex0.08 (+- 0.001) 10.4 (+- 0.49) ?WMLpos0.08 (+- 0.003) 10.6 (+- 0.49) ?Hpos0.072 (+- 0.001) 12 (+- 0.00) POSi?1= POSi0.066 (+- 0.003) 13.1 (+- 0.30) slex0.059 (+- 0.000) 14.2 (+- 0.40) ?WMLlex0.058 (+- 0.005) 14.7 (+- 0.64) BestEntropyReducepos0.049 (+- 0.001) 16.3 (+- 0.46) InformationGainlex0.047 (+- 0.004) 16.7 (+- 0.46) BestEntropyReducelex0.035 (+- 0.004) 18 (+- 0.00) ?H lex0.024 (+- 0.000) 19 (+- 0.00) wi?2= wi0.013 (+- 0.000) 20 (+- 0.00) POSi?2= POSi0.01 (+- 0.000) 21 (+- 0.00) wi?3= wi0.009 (+- 0.000) 22 (+- 0.00) edit0.006 (+- 0.000) 23 (+- 0.00) POSi?3= POSiTable 1: Feature ranker (Information Gain) forrpstartdetection- 10-fold x-validation on Switch-board heldout data.4.2.4 Repair end detection and structureclassificationFor rpenddetection, using the notion of paral-lelism, we hypothesise an effect of divergence be-tween ?lex at the reparandum-final word rmendand the repair-final word rpend: for repetition re-pairs, KL divergence will trivially be 0; for substi-tutions, it will be higher; for deletes, even higher.Upon inspection of our feature ranking this KLmeasure ranked 5th out of 23 features (merit=0.258 (+- 0.002)).We introduce another feature encoding paral-lelism ReparandumRepairDifference : the differ-ence in probability between an utterance cleanedof the reparandum and the utterance with itsrepair phase substituting its reparandum.
Inboth the POS (merit=0.366 (+- 0.003)) and word(merit=0.352 (+- 0.002)) LMs, this was the mostdiscriminative feature.4.3 Classifier pipelineSTIR effects a pipeline of classifiers as in Fig-ure 3, where the ed classifier only permits noned words to be passed on to rpstartclassificationand for rpendclassification of the active repairhypotheses, maintained in a stack.
The rpstartclassifier passes positive repair hypotheses to thermstartclassifier, which backwards searches upto 7 words back in the utterance.
If a rmstartisclassified, the output is passed on for rpendclas-sification at the end of the pipeline, and if not re-jected this is pushed onto the repair stack.
Repairhypotheses are are popped off when the string is7 words beyond its rpstartposition.
Putting limitson the stack?s storage space is a way of controllingfor processing overhead and complexity.
Embed-ded repairs whose rmstartcoincide with another?srpstartare easily dealt with as they are added tothe stack as separate hypotheses.4Classifiers Classifiers are implemented usingRandom Forests (Breiman, 2001) and we use dif-ferent error functions for each stage using Meta-Cost (Domingos, 1999).
The flexibility affordedby implementing adjustable error functions in apipelined incremental processor allows control ofthe trade-off of immediate accuracy against run-time and stability of the sequence classification.Processing complexity This pipeline avoids anexhaustive search all repair hypotheses.
If we limitthe search to within the ?rmstart, rpstart?
possibil-ities, this number of repairs grows approximatelyin the triangular number series?
i.e.
n(n+1)2, anested loop over previous words as n gets incre-mented ?
which in terms of a complexity class isa quadratic O(n2).
If we allow more than one?rmstart, rpstart?
hypothesis per word, the com-plexity goes up to O(n3), however in the tests thatwe describe below, we are able to achieve good de-tection results without permitting this extra searchspace.
Under our assumption that reparandum on-set detection is only triggered after repair onset de-tection, and repair extent detection is dependenton positive reparandum onset detection, a pipelinewith accurate components will allow us to limitprocessing to a small subset of this search space.4We constrain the problem not to include embeddeddeletes which may share their rpstartword with another re-pair ?
these are in practice very rare.84Figure 3: Classifier pipeline5 Experimental set-upWe train STIR on the Switchboard data describedabove, and test it on the standard Switchboard testdata (PTB III files 4[0-1]*).
In order to avoid over-fitting of classifiers to the basic language models,we use a cross-fold training approach: we dividethe corpus into 10 folds and use language mod-els trained on 9 folds to obtain feature values forthe 10th fold, repeating for all 10.
Classifiers arethen trained as standard on the resulting feature-annotated corpus.
This resulted in better featureutility for n-grams and better F-score results fordetection in all components in the order of 5-6%.5Training the classifiers Each Random Forestclassifier was limited to 20 trees of maximumdepth 4 nodes, putting a ceiling on decoding time.In making the classifiers cost-sensitive, MetaCostresamples the data in accordance with the costfunctions: we found using 10 iterations over a re-sample of 25% of the training data gave the mosteffective trade-off between training time and accu-racy.6 We use 8 different cost functions in rpstartwith differing costs for false negatives and posi-tives of the form below, where R is a repair ele-ment word and F is a fluent onset:(RhypFhypRgold0 2Fgold1 0)We adopt a similar technique in rmstartusing 5different cost functions and in rpendusing 8 dif-ferent settings, which when combined gives a to-tal of 320 different cost function configurations.We hypothesise that higher recall permitted in thepipeline?s first components would result in betteroverall accuracy as these hypotheses become re-fined, though at the cost of the stability of the hy-5Zwarts and Johnson (2011) take a similar approach onSwitchboard data to train a re-ranker of repair analyses.6As (Domingos, 1999) demonstrated, there are only rela-tively small accuracy gains when using more than this, withtraining time increasing in the order of the re-sample size.potheses of the sequence and extra downstreamprocessing in pruning false positives.We also experiment with the number of repairhypotheses permitted per word, using limits of 1-best and 2-best hypotheses.
We expect that allow-ing 2 hypotheses to be explored per rpstartshouldallow greater final accuracy, but with the trade-offof greater decoding and training complexity, andpossible incremental instability.As we wish to explore the incrementality versusfinal accuracy trade-off that STIR can achieve wenow describe the evaluation metrics we employ.5.1 Incremental evaluation metricsFollowing (Baumann et al., 2011) we divide ourevaluation metrics into similarity metrics (mea-sures of equality with or similarity to a gold stan-dard), timing metrics (measures of the timing ofrelevant phenomena detected from the gold stan-dard) and diachronic metrics (evolution of incre-mental hypotheses over time).Similarity metrics For direct comparison toprevious approaches we use the standard measureof overall accuracy, the F-score over reparandumwords, which we abbreviate Frm(see 7):precision = rmcorrectrmhyprecall = rmcorrectrmgoldFrm= 2 ?precision ?
recallprecision + recall(7)We are also interested in repair structural clas-sification, we also measure F-score over all repaircomponents (rm words, ed words as interregnaand rp words), a metric we abbreviate Fs.
Thisis not measured in standard repair detection onSwitchboard.
To investigate incremental accuracywe evaluate the delayed accuracy (DA) introducedby (Zwarts et al., 2010), as described in section2 against the utterance-final gold standard disflu-ency annotations, and use the mean of the 6 wordF-scores.85Input and current repair labels editsJohnJohn likesrm rp(?rm) (?rp)John likes uhed(?rm) (?rp)?edJohn likes uh lovesrm ed rp?rm?rpJohn likes uh loves Maryrm ed rpFigure 4: Edit Overhead- 4 unnecessary editsTiming and resource metrics Again for com-parative purposes we use Zwarts et al?s time-to-detection metrics, that is the two average distances(in numbers of words) consumed before first de-tection of gold standard repairs, one from rmstart,TDrmand one from rpstart, TDrp.
In our 1-bestdetection system, before evaluation we know a pri-ori TDrpwill be 1 token, and TDrmwill be 1 morethan the average length of rmstart?
rpstartrepairspans correctly detected.
However when we in-troduce a beam where multiple rmstarts are pos-sible per rpstartwith the most likely hypothesiscommitted as the current output, the latency maybegin to increase: the initially most probable hy-pothesis may not be the correct one.
In additionto output timing metrics, we account for intrinsicprocessing complexity with the metric processingoverhead (PO), which is the number of classifica-tions made by all components per word of input.Diachronic metrics To measure stability of re-pair hypotheses over time we use (Baumann et al.,2011)?s edit overhead (EO) metric.
EO measuresthe proportion of edits (add, revoke, substitute) ap-plied to a processor?s output structure that are un-necessary.
STIR?s output is the repair label se-quence shown in Figure 1, however rather thanevaluating its EO against the current gold stan-dard labels, we use a new mark-up we term the in-cremental repair gold standard: this does not pe-nalise lack of detection of a reparandum word rmas a bad edit until the corresponding rpstartof thatrm has been consumed.
While Frm, Fsand DAevaluate against what Baumann et al.
(2011) callthe current gold standard, the incremental goldstandard reflects the repair processing approachwe set out in 3.
An example of a repaired utterancewith an EO of 44% (49) can be seen in Figure 4: ofthe 9 edits (7 repair annotations and 2 correct flu-ent words), 4 are unnecessary (bracketed).
NoteFigure 6: Delayed Accuracy Curvesthe final ?rm is not counted as a bad edit for thereasons just given.6 Results and DiscussionWe evaluate on the Switchboard test data; Ta-ble 2 shows results of the best performing settingsfor each of the metrics described above, togetherwith the setting achieving the highest total score(TS)?
the average % achieved of the best per-forming system?s result in each metric.7 The set-tings found to achieve the highest Frm(the metricstandardly used in disfluency detection), and thatfound to achieve the highest TS for each stage inthe pipeline are shown in Figure 5.Our experiments showed that different systemsettings perform better in different metrics, andno individual setting achieved the best result inall of them.
Our best utterance-final Frmreaches0.779, marginally though not significantly exceed-ing (Zwarts et al., 2010)?s measure and STIRachieves 0.736 on the previously unevaluated Fs.The setting with the best DA improves on (Zwartset al., 2010)?s result significantly in terms of meanvalues (0.718 vs. 0.694), and also in terms of thesteepness of the curves (Figure 6).
The fastest av-erage time to detection is 1 word for TDrpand 2.6words for TDrm(Table 3), improving dramaticallyon the noisy channel model?s 4.6 and 7.5 words.Incrementality versus accuracy trade-off Weaimed to investigate how well a system could doin terms of achieving both good final accuracy andincremental performance, and while the best Frmsetting had a large PO and relatively slow DA in-crease, we find STIR can find a good trade-off set-7We do not include time-to-detection scores in TS as itdid not vary enough between settings to be significant, how-ever there was a difference in this measure between the 1-beststack condition and the 2-best stack condition ?
see below.86??
?rphypstartFhyprpgoldstart0 64Fgold1 0?????
?rmhypstartFhyprmgoldstart0 8Fgold1 0?????
?rphypendFhyprpgoldend0 2Fgold1 0??
?Stack depth = 2??
?rphypstartFhyprpgoldstart0 2Fgold1 0?????
?rmhypstartFhyprmgoldstart0 16Fgold1 0?????
?rphypendFhyprpgoldend0 8Fgold1 0??
?Stack depth = 1Figure 5: The cost function settings for the MetaCost classifiers for each component, for the best Frmsetting (top row) and best total score (TS) setting (bottom row)FrmFsDA EO POBest Final rm F-score (Frm) 0.779 0.735 0.698 3.946 1.733Best Final repair structure F-score (Fs) 0.772 0.736 0.707 4.477 1.659Best Delayed Accuracy of rm (DA) 0.767 0.721 0.718 1.483 1.689Best (lowest) Edit Overhead (EO) 0.718 0.674 0.675 0.864 1.230Best (lowest) Processing Overhead (PO) 0.716 0.671 0.673 0.875 1.229Best Total Score (mean % of best scores) (TS) 0.754 0.708 0.711 0.931 1.255Table 2: Comparison of the best performing system settings using different measuresFrmFsDA EO PO TDrpTDrm1-best rmstart0.745 0.707 0.699 3.780 1.650 1.0 2.62-best rmstart0.758 0.721 0.701 4.319 1.665 1.1 2.7Table 3: Comparison of performance of systems with different stack capacitiesting: the highest TS scoring setting achieves anFrmof 0.754 whilst also exhibiting a very goodDA (0.711) ?
over 98% of the best recorded score?
and low PO and EO rates ?
over 96% of the bestrecorded scores.
See the bottom row of Table 2.As can be seen in Figure 5, the cost functions forthese winning settings are different in nature.
Thebest non-incremental Frmmeasure setting requireshigh recall for the rest of the pipeline to work on,using the highest cost, 64, for false negative rpstartwords and the highest stack depth of 2 (similar to awider beam); but the best overall TS scoring sys-tem uses a less permissive setting to increase in-cremental performance.We make a preliminary investigation into theeffect of increasing the stack capacity by com-paring stacks with 1-best rmstarthypotheses perrpstartand 2-best stacks.
The average differencesbetween the two conditions is shown in Table 3.Moving to the 2-stack condition results in gain inoverall accuracy in Frmand Fs, but at the cost ofEO and also time-to-detection scores TDrmandTDrp.
The extent to which the stack can be in-creased without increasing jitter, latency and com-plexity will be investigated in future work.7 ConclusionWe have presented STIR, an incremental repairdetector that can be used to experiment with in-cremental performance and accuracy trade-offs.
Infuture work we plan to include probabilistic anddistributional features from a top-down incremen-tal parser e.g.
Roark et al.
(2009), and use STIR?sdistributional features to classify repair type.AcknowledgementsWe thank the three anonymous EMNLP review-ers for their helpful comments.
Hough is sup-ported by the DUEL project, financially supportedby the Agence Nationale de la Research (grantnumber ANR-13-FRAL-0001) and the DeutscheForschungsgemainschaft.
Much of the work wascarried out with support from an EPSRC DTAscholarship at Queen Mary University of Lon-don.
Purver is partly supported by ConCreTe:the project ConCreTe acknowledges the financialsupport of the Future and Emerging Technologies(FET) programme within the Seventh FrameworkProgramme for Research of the European Com-mission, under FET grant number 611733.87ReferencesT.
Baumann, O.
Bu?, and D. Schlangen.
2011.
Eval-uation and optimisation of incremental processors.Dialogue & Discourse, 2(1):113?141.Leo Breiman.
2001.
Random forests.
Machine learn-ing, 45(1):5?32.S.E.
Brennan and M.F.
Schober.
2001.
How listenerscompensate for disfluencies in spontaneous speech.Journal of Memory and Language, 44(2):274?296.Eric Brill and Robert C Moore.
2000.
An improved er-ror model for noisy channel spelling correction.
InProceedings of the 38th Annual Meeting on Associa-tion for Computational Linguistics, pages 286?293.Association for Computational Linguistics.Alexander Clark, Gianluca Giorgolo, and Shalom Lap-pin.
2013.
Statistical representation of grammat-icality judgements: the limits of n-gram models.In Proceedings of the Fourth Annual Workshop onCognitive Modeling and Computational Linguistics(CMCL), pages 28?36, Sofia, Bulgaria, August.
As-sociation for Computational Linguistics.Pedro Domingos.
1999.
Metacost: A general methodfor making classifiers cost-sensitive.
In Proceed-ings of the fifth ACM SIGKDD international con-ference on Knowledge discovery and data mining,pages 155?164.
ACM.Kallirroi Georgila.
2009.
Using integer linear pro-gramming for detecting speech disfluencies.
In Pro-ceedings of Human Language Technologies: The2009 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, Companion Volume: Short Papers, pages109?112.
Association for Computational Linguis-tics.Sebastian Germesin, Tilman Becker, and Peter Poller.2008.
Hybrid multi-step disfluency detection.In Machine Learning for Multimodal Interaction,pages 185?195.
Springer.Jonathan Ginzburg.
2012.
The Interactive Stance:Meaning for Conversation.
Oxford UniversityPress.P.
G. T. Healey, Arash Eshghi, Christine Howes, andMatthew Purver.
2011.
Making a contribution: Pro-cessing clarification requests in dialogue.
In Pro-ceedings of the 21st Annual Meeting of the Societyfor Text and Discourse, Poitiers, July.Peter Heeman and James Allen.
1999.
Speech repairs,intonational phrases, and discourse markers: model-ing speakers?
utterances in spoken dialogue.
Com-putational Linguistics, 25(4):527?571.Matthew Honnibal and Mark Johnson.
2014.
Jointincremental disfluency detection and dependencyparsing.
Transactions of the Association of Com-putational Linugistics (TACL), 2:131?142.Julian Hough and Matthew Purver.
2013.
Modellingexpectation in the self-repair processing of annotat-,um, listeners.
In Proceedings of the 17th SemDialWorkshop on the Semantics and Pragmatics of Di-alogue (DialDam), pages 92?101, Amsterdam, De-cember.T Florian Jaeger and Harry Tily.
2011.
On languageutility: Processing complexity and communicativeefficiency.
Wiley Interdisciplinary Reviews: Cogni-tive Science, 2(3):323?335.Mark Johnson and Eugene Charniak.
2004.
A TAG-based noisy channel model of speech repairs.
InProceedings of the 42nd Annual Meeting on Asso-ciation for Computational Linguistics, pages 33?39,Barcelona.
Association for Computational Linguis-tics.Frank Keller.
2004.
The entropy rate principle as apredictor of processing effort: An evaluation againsteye-tracking data.
In EMNLP, pages 317?324.Reinhard Kneser and Hermann Ney.
1995.
Im-proved backing-off for m-gram language modeling.In Acoustics, Speech, and Signal Processing, 1995.ICASSP-95., 1995 International Conference on, vol-ume 1, pages 181?184.
IEEE.Matthew Lease, Mark Johnson, and Eugene Charniak.2006.
Recognizing disfluencies in conversationalspeech.
Audio, Speech, and Language Processing,IEEE Transactions on, 14(5):1566?1573.W.J.M.
Levelt.
1983.
Monitoring and self-repair inspeech.
Cognition, 14(1):41?104.M.
Meteer, A. Taylor, R. MacIntyre, and R. Iyer.
1995.Disfluency annotation stylebook for the switchboardcorpus.
ms. Technical report, Department of Com-puter and Information Science, University of Penn-sylvania.Tim Miller and William Schuler.
2008.
A syntactictime-series model for parsing fluent and disfluentspeech.
In Proceedings of the 22nd InternationalConference on Computational Linguistics-Volume 1,pages 569?576.
Association for Computational Lin-guistics.David Milward.
1991.
Axiomatic Grammar, Non-Constituent Coordination and Incremental Interpre-tation.
Ph.D. thesis, University of Cambridge.Xian Qian and Yang Liu.
2013.
Disfluency detectionusing multi-step stacked learning.
In Proceedings ofNAACL-HLT, pages 820?825.Mohammad Sadegh Rasooli and Joel Tetreault.
2014.Non-monotonic parsing of fluent umm I mean dis-fluent sentences.
EACL 2014, pages 48?53.Hannes Rieser and David Schlangen.
2011.
Introduc-tion to the special issue on incremental processing indialogue.
Dialogue & Discourse, 2(1):1?10.88Brian Roark, Asaf Bachrach, Carlos Cardenas, andChristophe Pallier.
2009.
Deriving lexical andsyntactic expectation-based measures for psycholin-guistic modeling via incremental top-down parsing.In Proceedings of the 2009 Conference on EmpiricalMethods in Natural Language Processing: Volume1-Volume 1, pages 324?333.
Association for Com-putational Linguistics.Claude E. Shannon.
1948.
A mathematical theoryof communication.
technical journal.
AT & T BellLabs.Elizabeth Shriberg and Andreas Stolcke.
1998.
Howfar do speakers back up in repairs?
A quantitativemodel.
In Proceedings of the International Confer-ence on Spoken Language Processing, pages 2183?2186.Elizabeth Shriberg.
1994.
Preliminaries to a Theoryof Speech Disfluencies.
Ph.D. thesis, University ofCalifornia, Berkeley.Simon Zwarts and Mark Johnson.
2011.
The impact oflanguage models and loss functions on repair disflu-ency detection.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Lin-guistics: Human Language Technologies - Volume1, HLT ?11, pages 703?711, Stroudsburg, PA, USA.Association for Computational Linguistics.Simon Zwarts, Mark Johnson, and Robert Dale.
2010.Detecting speech repairs incrementally using a noisychannel approach.
In Proceedings of the 23rd Inter-national Conference on Computational Linguistics,COLING ?10, pages 1371?1378, Stroudsburg, PA,USA.
Association for Computational Linguistics.89
