Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 702?707,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsA Lightweight and High Performance Monolingual Word AlignerXuchen Yao and Benjamin Van DurmeJohns Hopkins UniversityBaltimore, MD, USAChris Callison-Burch?University of PennsylvaniaPhiladelphia, PA, USAPeter ClarkVulcan Inc.Seattle, WA, USAAbstractFast alignment is essential for many nat-ural language tasks.
But in the setting ofmonolingual alignment, previous work hasnot been able to align more than one sen-tence pair per second.
We describe a dis-criminatively trained monolingual wordaligner that uses a Conditional RandomField to globally decode the best align-ment with features drawn from source andtarget sentences.
Using just part-of-speechtags and WordNet as external resources,our aligner gives state-of-the-art result,while being an order-of-magnitude fasterthan the previous best performing system.1 IntroductionIn statistical machine translation, alignment is typ-ically done as a one-off task during training.
How-ever for monolingual tasks, like recognizing tex-tual entailment or question answering, alignmenthappens repeatedly: once or multiple times pertest item.
Therefore, the efficiency of the aligner isof utmost importance for monolingual alignmenttasks.
Monolingual word alignment also has a va-riety of distinctions than the bilingual case, for ex-ample: there is often less training data but morelexical resources available; semantic relatednessmay be cued by distributional word similarities;and, both the source and target sentences share thesame grammar.These distinctions suggest a model design thatutilizes arbitrary features (to make use of wordsimilarity measure and lexical resources) and ex-ploits deeper sentence structures (especially in thecase of major languages where robust parsers areavailable).
In this setting the balance betweenprecision and speed becomes an issue: while wemight leverage an extensive NLP pipeline for a?Performed while faculty at Johns Hopkins University.language like English, such pipelines can be com-putationally expensive.
One earlier attempt, theMANLI system (MacCartney et al, 2008), usedroughly 5GB of lexical resources and took 2 sec-onds per alignment, making it hard to be deployedand run in large scale.
On the other extreme, a sim-ple non-probabilistic Tree Edit Distance (TED)model (c.f.
?4.2) is able to align 10, 000 pairsper second when the sentences are pre-parsed, butwith significantly reduced performance.
Trying toembrace the merits of both worlds, we introduce adiscriminative aligner that is able to align tens tohundreds of sentence pairs per second, and needsaccess only to a POS tagger and WordNet.This aligner gives state-of-the-art performanceon the MSR RTE2 alignment dataset (Brockett,2007), is faster than previous work, and we re-lease it publicly as the first open-source monolin-gual word aligner: Jacana.Align.12 Related WorkThe MANLI aligner (MacCartney et al, 2008)was first proposed to align premise and hypothe-sis sentences for the task of natural language in-ference.
It applies perceptron learning and han-dles phrase-based alignment of arbitrary phraselengths.
Thadani and McKeown (2011) opti-mized this model by decoding via Integer LinearProgramming (ILP).
Benefiting from modern ILPsolvers, this led to an order-of-magnitude speedup.With extra syntactic constraints added, the exactalignment match rate for whole sentence pairs wasalso significantly improved.Besides the above supervised methods, indirectsupervision has also been explored.
Among them,Wang and Manning (2010) extended the work ofMcCallum et al (2005) and modeled alignmentas latent variables.
Heilman and Smith (2010)used tree kernels to search for the alignment that1http://code.google.com/p/jacana/702yields the lowest tree edit distance.
Other treeor graph matching work for alignment includesthat of (Punyakanok et al, 2004; Kouylekov andMagnini, 2005; Chambers et al, 2007; Mehdad,2009; Roth and Frank, 2012).Finally, feature and model design in monolin-gual alignment is often inspired by bilingual work,including distortion modeling, phrasal alignment,syntactic constraints, etc (Och and Ney, 2003;DeNero and Klein, 2007; Bansal et al, 2011).3 The Alignment Model3.1 Model DesignOur work is heavily influenced by the bilingualalignment literature, especially the discriminativemodel proposed by Blunsom and Cohn (2006).Given a source sentence s of length M , and a tar-get sentence t of length N , the alignment from sto t is a sequence of target word indices a, wheream?
[1,M ] ?
[0, N ].
We specify that when am = 0,source word st is aligned to a NULL state, i.e.,deleted.
This models a many-to-one alignmentfrom source to target.
Multiple source words canbe aligned to the same target word, but not viceversa.
One-to-many alignment can be obtainedby running the aligner in the other direction.
Theprobability of alignment sequence a conditionedon both s and t is then:p(a | s, t) =exp(?m,k ?kfk(am?1, am, s, t))Z(s, t)This assumes a first-order Conditional RandomField (Lafferty et al, 2001).
The word alignmenttask is evaluated over F1.
Instead of directly op-timizing F1, we employ softmax-margin training(Gimpel and Smith, 2010) and add a cost functionto the normalizing function Z(s, t) in the denom-inator, which becomes:?a?exp(?m,k?kfk(a?m?1, a?m, s, t) + cost(at, a?
))where at is the true alignments.
cost(at, a?
)can be viewed as special ?features?
with uniformweights that encourage consistent with true align-ments.
It is only computed during training in thedenominator because cost(at,at) = 0 in the nu-merator.
Hamming cost is used in practice.One distinction of this alignment model com-pared to other commonly defined CRFs is thatthe input is two dimensional: at each position m,the model inspects both the entire sequence ofsource words (as the observation) and target words(whose offset indices are states).
The other dis-tinction is that the size of its state space is notfixed (e.g., unlike POS tagging, where states arefor instance 45 Penn Treebank tags), but dependson N , the length of target sentence.
Thus we cannot ?memorize?
what features are mostly associ-ated with what states.
For instance, in the task oftagging mail addresses, a feature of ?5 consecu-tive digits?
is highly indicative of a POSTCODE.However, in the alignment model, it does not makesense to design features based on a hard-codedstate, say, a feature of ?source word lemma match-ing target word lemma?
fires for state index 6.To avoid this data sparsity problem, all featuresare defined implicitly with respect to the state.
Forinstance:fk(am?1, am, s, t) ={1 lemmas match: sm, tam0 otherwiseThus this feature fires for, e.g.
:(s3 = sport, t5 = sports, a3 = 5), and:(s2 = like, t10 = liked, a2 = 10).3.2 Feature DesignString Similarity Features include the followingsimilarity measures: Jaro Winkler, Dice Sorensen,Hamming, Jaccard, Levenshtein, NGram overlap-ping and common prefix matching.2 Also, twobinary features are added for identical match andidentical match ignoring case.POS Tags Features are binary indicators ofwhether the POS tags of two words match.
Also,a ?possrc2postgt?
feature fires for each word pair,with respect to their POS tags.
This would capture,e.g., ?vbz2nn?, when a verb such as arrests alignswith a noun such as custody.Positional Feature is a real-valued feature for thepositional difference of the source and target word(abs(mM ?
amN )).WordNet Features indicate whether two wordsare of the following relations of each other: hyper-nym, hyponym, synonym, derived form, entailing,causing, members of, have member, substances of,have substances, parts of, have part; or whether2Of these features the trained aligner preferred DiceSorensen and NGram overlapping.703their lemmas match.3Distortion Features measure how far apart thealigned target words of two consecutive sourcewords are: abs(am + 1 ?
am?1).
This learns ageneral pattern of whether these two target wordsaligned with two consecutive source words areusually far away from each other, or very close.We also added special features for corner caseswhere the current word starts or ends the sourcesentence, or both the previous and current wordsare deleted (a transition from NULL to NULL).Contextual Features indicate whether the left orthe right neighbor of the source word and alignedtarget word are identical or similar.
This helpsespecially when aligning functional words, whichusually have multiple candidate target functionalwords to align to and string similarity features can-not help.
We also added features for neighboringPOS tags matching.3.3 SymmetrizationTo expand from many-to-one alignment to many-to-many, we ran the model in both directions andapplied the following symmetrization heuristics(Koehn, 2010): INTERSECTION, UNION, GROW-DIAG-FINAL.4 Experiments4.1 SetupSince no generic off-the-shelf CRF software is de-signed to handle the special case of dynamic stateindices and feature functions (Blunsom and Cohn,2006), we implemented this aligner model in theScala programming language, which is fully in-teroperable with Java.
We used the L2 regular-izer and LBFGS for optimization.
OpenNLP4 pro-vided the POS tagger and JWNL5 interfaced withWordNet (Fellbaum, 1998).To make results directly comparable, we closelyfollowed the setup of MacCartney et al (2008) andThadani and McKeown (2011).
Training and testdata (Brockett, 2007) each contains 800 manuallyaligned premise and hypothesis pairs from RTE2.Note that the premises contain 29 words on av-erage, and the hypotheses only 11 words.
We takethe premise as the source and hypothesis as the tar-get, and use S2T to indicate the model aligns from3We found that each word has to be POS tagged to get anaccurate relation, otherwise this feature will not help.4http://opennlp.apache.org/5http://jwordnet.sf.net/source to target and T2S from target to source.4.2 Simple BaselinesWe additionally used two baseline systems forcomparison.
One was GIZA++, with the IN-TERSECTION tricks post-applied, which workedthe best among all other symmetrization heuris-tics.
The other was a Tree Edit Distance (TED)model, popularly used in a series of NLP appli-cations (Punyakanok et al, 2004; Kouylekov andMagnini, 2005; Heilman and Smith, 2010).
Weused uniform cost for deletion, insertion and sub-stitutions, and applied a dynamic program algo-rithm (Zhang and Shasha, 1989) to decode thetree edit sequence with the minimal cost, basedon the Stanford dependency tree (De Marneffeand Manning, 2008).
This non-probabilistic ap-proach turned out to be extremely fast, processingabout 10,000 sentence pairs per second with pre-parsed trees, performing quantitatively better thanthe Stanford RTE aligner (Chambers et al, 2007).4.3 MANLI BaselinesMANLI was first developed by MacCartney et al(2008), and then improved by Thadani and McKe-own (2011) with faster and exact decoding via ILP.There are four versions to be compared here:MANLI the original version.MANLI-approx.
re-implemented version byThadani and McKeown (2011).MANLI-exact decoding via ILP solvers.MANLI-constraint MANLI-exact with hardsyntactic constraints, mainly on common ?light?words (determiners, prepositions, etc.)
attachmentto boost exact match rate.4.4 ResultsFollowing Thadani and McKeown (2011), perfor-mance is evaluated by macro-averaged precision,recall, F1 of aligned token pairs, and exact (per-fect) match rate for a whole pair, shown in Ta-ble 1.
As our baselines, GIZA++ (with align-ment intersection of two directions) and TED areon par with previously reported results using theStanford RTE aligner.
The MANLI-family of sys-tems provide stronger baselines, notably MANLI-constraint, which has the best F1 and exact matchrate among themselves.We ran our aligner in two directions: S2T andT2S, then merged the results with INTERSECTION,UNION and GROW-DIAG-FINAL.
Our system beats704System P % R % F1 % E %GIZA++, ?
82.5 74.4 78.3 14.0TED 80.6 79.0 79.8 13.5Stanford RTE?
82.7 75.8 79.1 -MANLI?
85.4 85.3 85.3 21.3MANLI-approx./ 87.2 86.3 86.7 24.5MANLI-exact/ 87.2 86.1 86.8 24.8MANLI-constraint/ 89.5 86.2 87.8 33.0this work, S2T 91.8 83.4 87.4 25.9this work, T2S 93.7 84.0 88.6 35.3S2T ?
T2S 95.4 80.8 87.5 31.3S2T ?
T2S 90.3 86.6 88.4 29.6GROW-DIAG-FINAL 94.4 81.8 87.6 30.8Table 1: Results on the 800 pairs of test data.
E% standsfor exact (perfect) match rate.
Systems marked with ?
arereported by MacCartney et al (2008), with / by Thadani andMcKeown (2011).the weak and strong baselines6 in all measures ex-cept recall.
Some patterns are very clearly shown:Higher precision, lower recall is due to thehigher-quality and lower-coverage of WordNet,where the MANLI-family systems used addi-tional, automatically derived lexical resources.Imbalance of exact match rate between S2T andT2S with a difference of 9.4% is due to the many-to-one nature of the aligner.
When aligning fromsource (longer) to target (shorter), multiple sourcewords can align to the same target word.
Thisis not desirable since multiple duplicate ?light?words are aligned to the same ?light?
word in thetarget, which breaks perfect match.
When align-ing T2S, this problem goes away: the shorter tar-get sentence contains less duplicate words, and inmost cases there is an one-to-one mapping.MT heuristics help, with INTERSECTION andUNION respectively improving precision and re-call.4.5 Runtime TestTable 2 shows the runtime comparison.
Since theRTE2 corpus is imbalanced, with premise length(words) of 29 and hypothesis length of 11, wealso compare on the corpus of FUSION (McKeownet al, 2010), with both sentences in a pair aver-aging 27.
MANLI-approx.
is the slowest, withquadratic growth in the number of edits with sen-tence length.
MANLI-exact is in second place, re-lying on the ILP solver.
This work has a preciseO(MN2) decoding time, with M the source sen-tence length and N the target sentence length.6Unfortunately both MacCartney and Thadani no longerhave their original output files (personal communication), sowe cannot run a significance test against their result.corpus sent.
pairlengthMANLI-approx.MANLI-exactthisworkRTE2 29/11 1.67 0.08 0.025FUSION 27/27 61.96 2.45 0.096Table 2: Alignment runtime in seconds per sentence pair ontwo corpora: RTE2 (Cohn et al, 2008) and FUSION (McKe-own et al, 2010).
The MANLI-* results are from Thadaniand McKeown (2011), on a Xeon 2.0GHz with 6MB Cache.The runtime for this work takes the longest timing from S2Tand T2S, on a Xeon 2.2GHz with 4MB cache (the closestwe can find to match their hardware).
Horizontally in a real-world application where sentences have similar length, thiswork is roughly 20x faster (0.096 vs. 2.45).
Vertically, thedecoding time for our work increases less dramatically whensentence length increases (0.025?0.096 vs. 0.08?2.45).features P % R % F1 % E %full (T2S) 93.7 84.0 88.6 35.3- POS 93.2 83.5 88.1 31.4- WordNet 93.2 83.7 88.2 33.5- both 93.1 83.2 87.8 30.1Table 3: Performance without POS and/or Word-Net features.While MANLI-exact is about twenty-fold fasterthan MANLI-approx., our aligner is at least an-other twenty-fold faster than MANLI-exact whenthe sentences are longer and balanced.
We alsobenefit from shallower pre-processing (no parsing)and can store all resources in main memory.74.6 Ablation TestSince WordNet and the POS tagger is the only usedexternal resource, we removed them8 from the fea-ture sets and reported performance in Table 3.
Thissomehow reflects how the model would performfor a language without a suitable POS tagger, ormore commonly, WordNet in that language.
Atthis time, the model falls back to relying on stringsimilarities, distortion, positional and contextualfeatures, which are almost language-independent.A loss of less than 1% in F1 suggests that thealigner can still run reasonably well without a POStagger and WordNet.7WordNet (?30MB) is a smaller footprint than the 5GB ofexternal resources used by MANLI.8per request of reviewers.
Note that WordNet is less pre-cise without a POS tagger.
When we removed the POS tag-ger, we enumerated all POS tags for a word to find its hyper-nym/synonym/... synsets.7054.7 Error AnalysisThere were three primary categories of error:91.
Token-based paraphrases that are not coveredby WordNet, such as program and software,business and venture.
This calls for broader-coverage paraphrase resources.2.
Words that are semantically related but notexactly paraphrases, such as married andwife, beat and victory.
This calls for re-sources of close distributional similarity.3.
Phrases of the above kinds, such as electedand won a seat, politician and presidentialcandidate.
This calls for further work onphrase-based alignment.10There is a trade-off using WordNet vs. larger,noisier resources in exchange of higher preci-sion vs. recall and memory/disk allocation.
Wethink this is an application-specific decision; otherresources could be easily incorporated into ourmodel, which we may explore in the future to ex-plore the trade-off in addressing items 1 and 2.5 ConclusionWe presented a model for monolingual sentencealignment that gives state-of-the-art performance,and is significantly faster than prior work.
We re-lease our implementation as the first open-sourcemonolingual aligner, which we hope to be of ben-efit to other researchers in the rapidly expandingarea of natural language inference.AcknowledgementWe thank Vulcan Inc. for funding this work.
Wealso thank Jason Smith, Travis Wolfe, Frank Fer-raro for various discussion, suggestion, commentsand the three anonymous reviewers.ReferencesMohit Bansal, Chris Quirk, and Robert Moore.
2011.Gappy phrasal alignment by agreement.
In Proceed-ings of ACL, Portland, Oregon, June.9We submitted a browser in JavaScript(AlignmentBrowser.html) in the supporting materialthat compares the gold alignment and test output; readers areencouraged to try it out.10Note that MacCartney et al (2008) showed that in theMANLI system setting phrase size to larger than one therewas only a 0.2% gain in F1, while the complexity becamemuch larger.P.
Blunsom and T. Cohn.
2006.
Discriminative wordalignment with conditional random fields.
In Pro-ceedings of ACL2006, pages 65?72.Chris Brockett.
2007.
Aligning the RTE 2006 corpus.Technical report, Microsoft Research.N.
Chambers, D. Cer, T. Grenager, D. Hall, C. Kid-don, B. MacCartney, M.C.
de Marneffe, D. Ramage,E.
Yeh, and C.D.
Manning.
2007.
Learning align-ments and leveraging natural logic.
In Proceedingsof the ACL-PASCAL Workshop on Textual Entail-ment and Paraphrasing, pages 165?170.Trevor Cohn, Chris Callison-Burch, and Mirella Lap-ata.
2008.
Constructing corpora for the develop-ment and evaluation of paraphrase systems.
Com-put.
Linguist., 34(4):597?614, December.Marie-Catherine De Marneffe and Christopher D Man-ning.
2008.
The stanford typed dependencies rep-resentation.
In Coling 2008: Proceedings of theworkshop on Cross-Framework and Cross-DomainParser Evaluation, pages 1?8.John DeNero and Dan Klein.
2007.
Tailoring wordalignments to syntactic machine translation.
In Pro-ceedings of ACL2007.C.
Fellbaum.
1998.
WordNet: An Electronical LexicalDatabase.Kevin Gimpel and Noah A. Smith.
2010.
Softmax-margin crfs: training log-linear models with costfunctions.
In NAACL 2010, pages 733?736.Michael Heilman and Noah A. Smith.
2010.
Tree editmodels for recognizing textual entailments, para-phrases, and answers to questions.
In Proceedings ofNAACL 2010, pages 1011?1019, Los Angeles, Cali-fornia, June.Philipp Koehn.
2010.
Statistical Machine Translation.Cambridge University Press, New York, NY, USA.Milen Kouylekov and Bernardo Magnini.
2005.
Rec-ognizing textual entailment with tree edit distancealgorithms.
In PASCAL Challenges on RTE, pages17?20.John D. Lafferty, Andrew McCallum, and FernandoC.
N. Pereira.
2001.
Conditional random fields:Probabilistic models for segmenting and labeling se-quence data.
In Proceedings of the Eighteenth Inter-national Conference on Machine Learning, ICML?01, pages 282?289, San Francisco, CA, USA.B.
MacCartney, M. Galley, and C.D.
Manning.
2008.A phrase-based alignment model for natural lan-guage inference.
In Proceedings of EMNLP2008,pages 802?811.Andrew McCallum, Kedar Bellare, and FernandoPereira.
2005.
A Conditional Random Fieldfor Discriminatively-trained Finite-state String EditDistance.
In Proceedings of the 21st Conferenceon Uncertainty in Artificial Intelligence (UAI 2005),July.706Kathleen McKeown, Sara Rosenthal, Kapil Thadani,and Coleman Moore.
2010.
Time-efficient creationof an accurate sentence fusion corpus.
In ACL2010short, pages 317?320.Y.
Mehdad.
2009.
Automatic cost estimation for treeedit distance using particle swarm optimization.
InProceedings of the ACL-IJCNLP 2009 ConferenceShort Papers, pages 289?292.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational linguistics, 29(1):19?51.Vasin Punyakanok, Dan Roth, and Wen T. Yih.
2004.Mapping Dependencies Trees: An Application toQuestion Answerin.
In Proceedings of the 8th In-ternational Symposium on Artificial Intelligence andMathematics, Fort Lauderdale, Florida.Michael Roth and Anette Frank.
2012.
Aligning pred-icates across monolingual comparable texts usinggraph-based clustering.
In Proceedings of the 2012Joint Conference on Empirical Methods in Natu-ral Language Processing and Computational Natu-ral Language Learning, pages 171?182, Jeju Island,Korea, July.Kapil Thadani and Kathleen McKeown.
2011.
Opti-mal and syntactically-informed decoding for mono-lingual phrase-based alignment.
In Proceedings ofACL short.Mengqiu Wang and Christopher D. Manning.
2010.Probabilistic tree-edit models with structured latentvariables for textual entailment and question an-swering.
In Proceedings of the 23rd InternationalConference on Computational Linguistics, COLING?10, pages 1164?1172, Stroudsburg, PA, USA.K.
Zhang and D. Shasha.
1989.
Simple fast algorithmsfor the editing distance between trees and relatedproblems.
SIAM J.
Comput., 18(6):1245?1262, De-cember.707
