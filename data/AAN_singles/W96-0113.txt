A Re-estimation Method for Stochastic LanguageModel ing from Ambiguous ObservationsMik io  YamamotoInstitute of Information Sciences and ElectronicsUniversity of Tsukuba1-1-1 Tennodai, Tsukuba, Ibaraki 305, Japanmyama@is, t sukuba, ac.
j pAbstractThis paper describes a reestimation method for stochastic language models uch as theN-gram model and the Hidden Maxkov Model(HMM) from ambiguous observations.
It isapplied to model estimation for a tagger from a~ untagged corpus.
We make extensionsto a previous algorithm that reestimates the N-gram model from an untagged segmentedlanguage (e.g., English) text as training data.
The new method can estimate not only theN-gram model, but also the HMM from untagged, unsegmented language (e.g., Japanese)text.
Credit factors for training data to improve the reliability of the estimated modelsaxe also introduced.
In experiments, the extended algorithm could estimate the HMM aswell as the N-gram model from an untagged, unsegmented Japanese corpus and the creditfactor was effective in improving model accuracy.
The use of credit factors is a usefulapproach to estimating a reliable stochastic language model from untagged corpora whichaxe noisy by nature.1 In t roduct ionStochastic language models are useful for many language processing applications such as speechrecognition, natural anguage processing and so on.
However, in order to build an accuratestochastic language model, large amounts of tagged text are needed and a tagged corpus maynot always match a target application because of, for example, differences between the tagsystems.
If the language model can be estimated from untagged corpora nd the dictionary ofa target application, then the above two problems would be resolved because large amountsof untagged corpora could be easily used and untagged corpora are neutral toward any appli-cations.Kupiec (1992) has proposed an estimation method for the N-gram language model usingthe Baum-Welch reestimation algorithm (Rabiner et al, 1994) from an untagged corpus andCutting et al (1992) have applied this method to an English tagging system.
Takeuchi andMatsumoto (1995) also have developed an extended method for unsegmented languages (e.g.,Japanese) and applied it to their Japanese tagger.However, Merialdo (1994) and Elworthy (1994) have criticized methods of estimation froman untagged corpus based on the maximum likelihood principle.
They pointed out limitationof such methods revealed by their experiments and said that the optimization of likelihooddidn't necessarily improve tagging accuracy.
In other words, the training data extracted froman untagged corpus using only a dictionary are, by nature, too noisy to build a reliable model.I would like to know whether or not the noise problem occurs in other language models uchas the HMM.
Zhou and Nakagawa (1994) have shown, in the experiments of word prediction155from the previous word sequence, that the HMM is more powerful than the bigram modeland is nearly equivalent to the trigram model, though the number of parameters of the HMMis less than that in the N-gram model.
In general, models with fewer parameters are morerobust.
Here, I investigate a method that can estimate HMM parameters from an untaggedcorpus and also a general technique for supressing noise in untagged training data.
The goalsof this paper are as follows.?
Extension of Baum-Welch algorithm: I formulate an algorithm that can be applied tountagged, unsegmented language corpora and estimate not only the N-gram model, butthe HMM.
Also, a scaling procedure is defined in the algorithm.?
Credit factor: In order to overcome the noise of untagged corpora, I introduce creditfactors that are assigned to training data.
The estimation algorithm can approximatelymaximize the modified likelihood that is weighted by the credit factors.The problem of stochastic tagging is formulated in the next section(2) and the extendedreestimation method in section 3.
A way of determining the credit factor based on a rule-based tagger is described in section 4.
Experiments which evaluate the proposed method arereported in section 5.2 Stochastic Tagging FormulationIn general, the stochastic tagging problem can be formulated as a search problem in thestochastic space of sequences of tags and words.
In this formulation, the tagger searches forthe best sequence that maximizes the probability (Nagata, 1994):(1~ r, T) = arg maxp(W, TIS ) = arg maxp(W, T) (1)W,T W,Twhere W is a word sequence (wl,w2, ...,Wn), T is a tag sequence (tl,t2,...,tn) and S is aninput sentence.
Since Japanese sentences have no delimiters (e.g., spaces) between words, amorphological nalyzer (tagger) must decide word segmentation in addition to part-of-speechassignment.
The number of segmentation ambiguities of Japanese sentences is large and theseambiguities complicate the work of a Japanese tagger.Although all possible p(W, T)s on combinations of W and T cannot be estimated, thereare some particularly useful approximations such as the N-gram model and the HMM.
Thefollowing formulae are straightforward formulations whose observed variables are pairs of wordsand tags:np(W, T) ~ ~I p(wi, tilwi-N+l, ..., wi-1, ti-N+l, .
.
.
,  ti--1) (2)i=1n--1p(W,T) '~' ~ I~ ax(i),x(i+l)bz(i+l) (wi+l'ti+l) (3)x i=0Formula 2 is the N-gram model and formula 3 is the HMM.
When N of formula 2 is two,the model is called the bigram, when N is three, it is the trigrarm Symbol x of formula 3denotes a possible path of states of the HMM and x(i) denotes a state of the HMM thatis visited at the i-th transition in the path x .
ax( i ) ,x ( i+ l  ) is the transition probability fromx(i) to x(i + i).
In particular, ax(0),x(1 ) represents he initial state probability (Trx(1)) of x(1).b~(i)(w , t) is an output probability of a pair of word w and tag t on the state x(i).
A state ofthe HMM represents an abstract class of a part of the input symbol sequence.
That is, we canregard the HMM as a mixed model of unigram, bigram, trigram, and so on.156We can also decrease the number of model parameters by separating the tag model fromformulae 2 and 3.
In the models, the N-gram and the HMM are used to model tag sequenceand p(wlt ) is used for another part of the model.nv(w, T) II v(ti Iti-N...ti-1)v(wilt ) (4)i=1n- -1p(W,T) ~ ~ ~I = ax(i),x(i+l)b~(i+l)(ti+l)p(wi+l\]ti+l ) (5)x i=0The PAIR-HMM, TAG-bigram model, and TAG-HMM based on formulae 3, 4 (whereN = 2) and 5, respectively, will be investigated in section 5.
In the next section, I describean extension to the forward-backward algorithm for determining HMM parameters fi'om am-biguous observations.3 Re-es t imat ion  Method  from Ambiguous  Observat ions3.1 Ambiguous  Observat ion  St ructureHere, we define an ambiguous observation as a lattice structure with a credit factor for eachbranch.
In unsegmented languages that have no delimiter between words, such as Japanese,candidates for alignment of tag and word have different segmentation.
That is, they must berepresented by a lattice.
We can create a lattice structure from untagged Japanese sentencesand a Japanese dictionary.The following is the definition of the lattice of candidates representing ambiguous wordand tag sequences called the morpheme network.
All morphemes on the morpheme networkare numbered.w, or word(s): The spelling of the s-th morpheme.ts or tag(s): The tag of the s-th morpheme.suc(s): The set of morpheme numbers that the s-th morpheme connects to.pre(s): The set of morpheme numbers that connect o the s-th morpheme.credit(r, s): The credit factor of the connection between the r-th and the s-th morphemes.For example, a morpheme network can be derieved from the input sentence "~ L ~3 v~"which means "not to sail" (Fig.
1).
The real and dotted lines in Figure 1 represent the correctand incorrect paths of morphemes, respectively.
Of course, any algorithm for estimationfrom untagged corpora cannot determine whether the connections are correct or not.
Theconnections of dotted lines constitute noise for the estimation algorithm.
The numbers on thelines show the credit factor of each connection that is assigned by the method described insection 4.
The numbers at the right of colons are morpheme numbers.
In Figure 1, word(3)is ' ~ b ', tag(3) is 'verb', pre(3) is the set {1}, sue(3) is the set {6, 7} and the credit factorcredit(l, 3) is 0.8.3.2 Re-es t imat ion  A lgor i thmGiven a morpheme network, we can formulate the reestimation algorithm for the HMM pa-rameters.
The original forward-backward algorithm calculates the probability of the partialobservation sequence given the state of the HMM at the time (position of word in the inputsentence).
The original algorithm does this by a time synchronous procedure operating onunambiguous observation sequence.
The extended algorithm calculates the probability of the157INPUT SENTENCE:(not to sa i l )., ~ ~r~(noun): 1 ..... .0.~8.
.. .. * ~ L,(verb):3,:--0-'-2----:- ?
'0?P~(adjective):6 ,,.0.:-I"" (ship) (putout) "..0.5,-"*(not) ",0.90"" :< "" 0.~ 0.3/ .
.
.
.
"., ",,u.= ~ ~J~(noun):2 .0.7 ?
\[.,(verb):4 -'"'" 0.7 ~ ~'=?~(post-fix):7 0.9 ~ ".
"(symbol):8 0~90(saling) ""..., (do) (not) ,.,-'"o:?'-.
.
.
.
.
."
' "  L ~'0?
~(noun):5 .... ""(bamboo sword)Figure 1: An example of the morpheme network.partial ambiguous equence given the state of the HMM at the node (morpheme candidate)in the morpheme network by a node synchronous procedure.
The algorithm formulation is asfollows:initial:c~u(j) = lrjbj(wu, tu)credit(#, u) wherefly(i) = 1 whererecursion:Nc~(j) = ~\] E a~(i)aljbj(wr,t~)credit(s,r)sEpre(r)  i=1Nfls(i) = ~ ~ aijbj(wr, tr)flr(j)credit(s,r)u e on(l)v E on(B)rEsuc(s)  j= lwhere on(l)  is the set of numbers of the left most morphemes in the morpheme network andon(B) is the set of numbers of the right most morphemes.
The '# '  in credit(#, u) means thebeginning-of-text indicator.The trellis, that is often used to explain the originM forward-backward algorithm, is ex-tended into a network trellis.
Figure 2 is an example of the network trellis that is generatedfrom the morpheme network example given above (Fig.
1).
In this example, c~7(1) means aforward probability of the 7th morpheme at the 1st state of the HMM.Using the extended forward-backward probabilities we can formulate the reestimation al-gorithm from ambiguous observations:K 1 k~__ 1~ ~ czrk(i)aijbj(ts)fl~(j)credit(r,s)---- rEpre(s)a i J  ---- Kk=lKk=l  t'i(w, t) = wora(sl=~,t,g(~l=t (7/k=l(6)158noun:  1 verb:3 ad ject ive :6o1:8n"<- .
.
.
.
.
.
.
.~ ,  .
:\[" (X7(1)noun:5  "~7(3)Figure 2: An example of the network trellisK1 O?~(/)~sk (i) E~ Ek=l 8Eon(1) ~ = g N (8)k=l sEon(1)j=lwhere k represents the k-th input sentence and Pk is sum of the probabilities of possiblesequences in the k-th morpheme network weighted by the credit factors.3.3 Sca l ingIn the calculation of forward-backward probabilities, under-flow sometimes occurs if the dic-tionaxy for making the morpheme network is large and/or the length of the input sentenceis long, because the forward-backward algorithm multiplies many small transition and outputprobabilities together.
This problem is native to speech modeling, but in general, the modelingof text is free from this problem.
However, since Japanese sentences tend to be relatively longand the recent Japanese dictionary for research is large, under-flow is sometimes a problem.For example, the EDIt Japanese corpus (EDR, 1994) includes entences that consist of morethan fifty words at a frequency of one percent.
In fact, we experienced the underflow problemin preliminary experiments with the EDR corpus.Application of the scaling technique of the original backward-forward algorithm (Rabineret al, 1994) to our reestimation method would solve the under-flow problem.
The originaltechnique is based on synchronous calculation with positions of words in the input sentencein left-to-right fashion.
However, since word boundaries in the morpheme network may ormay not cross on the input character sequence, we cannot directly apply this method to theextended algorithm.Let us introduce synchronous points on a~ input characters sequence to facilitate synchro-nization of the calculation of forward-backward probabilities.
All possible paths of a morpheme159Syncronouspoints!
!
I !I I o !o !
l li i i i~t~ :2 ?
L:4 ~:7 j~!
!!
!!
I!
!| i2 34".
":8Figure 3: An example of syncronous pointsnetwork have one morpheme on each synchronous point.
The synchronous points are definedas positions of the head character of all morphemes in a morpheme network and are numberedfrom left to right.
The synchronous point number of the left most word is defined as 1.
Amorpheme is associated with the synchronous points which are located in the flow of charactersof the morpheme.The symbols and on(q) function are defined as follows:B: The maximum number of synchronous points in a morpheme network.on(q): The set of morpheme numbers that are associated with synchronous point q.L,: The left most synchronous point that is associated with the s-th morpheme.R,: The right most synchronous point that is associated with the s-th morpheme.Figure 3 is an example of the syncronous points for the morpheme network example givenabove (Fig.
1).
The values of the symbols and function defined above are as follows in thisexample; B = 5, on(2) = {2, 3}, L5 = 3, R5 = 4 and so on.The scaled forward probabilities are defined with the above definitions.
The notation ~st(i)is used to denote the unscaled forward probabilities of the s-th morpheme on the syncronouspoint l, &sl(i) to denote the scaled forward probabilities, and &,l(i) to denote the local versionof c~ before scaling, cl is the scaling factor of synchronous point I.initial:&sl(i) = ~sl(i) = ~ribi(ws,t,)credit(~,s) where s E on(l)N ^C 1 : 1 /  E E &sl(/)sEon(1) i=1^&sl ( i )  = C l&s l ( i )  where s E on(l)160cost 0 1-10 11-20 21-50 51-100 101-200 201-500 501-1000precision 0.84 0.16 0.13 0.069 0.074 0.0083 0.0017 0Table 1: The precision on each cost of Jumanrecursion:&s,/-l(i) i f  L, 7~lN& ,l_l(i)aijbtj(w,,t )credit(r,s) i f  L ,  = lrEpre(s )  i=1N ^C l -~ 1 E E &sl(i)sEon(l) i=1&st(i) = Cl&t(i)The scaled forward probabilities can be calculated synchronizing with the synchronouspoints from left to right.
The scaled backward probabilities are defined in the same way usingthe scaling factors obtained in the calculation of the forward probabilities.The scaled forward-backward probabilities have the following property:1&s(i)aijbj(wr, tr)flr(j) = --as(i)aijbj(wr, tr)fl~(j) (9)Pkwhere &8 = &~R~ and fls = fl~ns.
Using this property, the reestimation formulae can bereplaced with the scaled versions.
The replaced formulae are free of the under-flow problemand their use also obviates the need to calculate the weighted sum of path probabilities of thek-th ambiguous observation, Pk.4 Cred i t  FactorIn the estimation of a Japanese language model from an untagged corpus, the segmentationambiguity of Japanese sentences severely degrades the model reliability.
I will show that modelestimations excluding the credit factors cannot overcome the noise problem in section 5.
Creditfactors play a very important role by supressing noise in the training data.
However, a way ofcalculating the optimal value of credit is not yet available, so a preliminary method escribedin this section was used for the experiments.The 'costs' of candidates outputted by a rule-based tagger were used as the source ofinformation related to the credit.
Juman (Matsumoto et al, 1994) was used in our experimentsto generate the morpheme network.
Juman is a rule-based Japanese tagging system whichuses hand-coding cost values that represent the implausibility of morpheme connections, andword- and tag-occurences.
Given a cost-width, Juman outputs the candidates of morphemesequences pruned by this cost-width.
A larger cost-width would result in a larger number ofoutput candidates.We evaluated the precision of a set of morpheme candidates that have a certain cost.The precision value was used as the credit factor of each branch in the morpheme networkto be outputted by Juman (Table 1).
In the experiments described in the next section, weapproximated the results from this example (see Table 1) by the formula 1/(a* cost + b), wherea was 0.5 and b 1.19.1615 Experiments5.1 ImplementationThe experimental system for model estimation was implemented using the extended reestima-tion method.
A morpheme network of each input sentence was generated with Juman (Mat-sumoto et al, 1994) and the credit factor was attached to each branch as described above.
Thesystem can estimate three kinds of models; the PAIRoHMM (formula 3) with output symbolsas pairs of words and tags, the TAG-bigram model (formula 4, where N = 2) and TAG-HMM(formula 5) with output symbols as tags and p(w\]t).
The scaling technique was used with allestimations.The numbers of parameters of the TAG-bigram model, the TAG-HMM and the PAIR-HMMare approximated by the equations NT 2 + ND, NS  2 + NS * NT  + ND, and NS 2 + NS * ND,respectively, where NT is the number of tags, NS is the number of states of the HMM, andND is the number of entries in the dictionary.
In all experiments, NT, NS  and ND were fixedat 104, 10, and 130,000, respectively.
The numbers of parameters of the TAG-bigram model,TAG-HMM, PAIR-HMM were 10816 + ND, 1140 + ND, and 100+ IOND, respectively.
Notethat the number of parameters of the tag model of the TAG-HMM is one tenth that of theTAG-bigram model.For the model evaluation, a stochastic tagger was implemented.
Given a morpheme networkgenerated by Juman with a cost-width, the implemented tagger selects the most probable pathin the network using each stochastic model.
The best path was calculated by the Viterbi-algorithm on the paths of the morpheme network.5.2 Data and EvaluationI used 26108 Japanese untagged sentences as training data and 100 hand-tagged sentences astest data, both from the Nikkei newspaper 1994 corpus (Nihon Keizai Shimbun, Inc., 1995).The test sentences include about 2500 Japanese morphemes.
The tags were defined as thecombination of part-of-speech, conjugation, and class of conjugation.
The number of kinds oftags was 104.In the precision evaluation, the correct morpheme was defined as that matching the seg-mentation, tag, and spelling of the base form of the hand-tagged morpheme.
The precisionwas defined as the proportion of correct morphemes relative to the total number of morphemesin the sequence which the tagger outputted as the best alignment of tags and words.5.3 Resu l tsThree kinds of models were estimated using the untagged training data with the initial pa-rameters set to the equivalent probabilities.
Each model was estimated both with and withoutuse of the credit factor.
The reestimation algorithm was iterated for five to twenty times.The precision of the most plausible segmentation a d tag assignment was outputted bythe tagger based on each stochastic model estimated either without (Figs.
4 and 5) or with(Fig.
6) the credit factor assignment function described in the previous ection.
Two versionsof the morpheme network for the estimations were used; one limited by a cost-width of 500(Fig.
4) and the other by a cost-width of 70 (Figs.
5 and 6).
The cost-width of 500 requiredalmost all of the morphemes to be used for the estimation.
In other words, a morphemenetwork of cost-width 500 was equivalent to that extracted from the input sentence with adictionary only.
Although one experiment (Fig.
5) didn't use the credit factor assignmentfunction, it is regarded as using a special function of the credit factor that returns 0 or 1, that162cost-width 0 10 20 50 100 200 500 1000precisionrecall84.4 79.8 79.3 71.0 66.3 43.6 36.5 36.394.7 96.0 96.1 97.2 98.0 98.7 98.7 98.7Table 2: The precision and recall of Juman on each cost-width.is a step function, with a cost threshold of 70.
However, this function doesn't differentiateamong morphemes whose costs are 0 and 70.The cost-widths (see horizontal axes in Figs.
4, 5 and 6) were provided to Juman to generatethe morpheme network used in the stochastic tagger for model evaluation.
The tagger chosethe best morpheme sequence from the network by each stochastic model.
A larger cost-widthwould result in a larger network, lower precision, and higher recall (Table 2).
Note thatthe precision of any model will never exceed the recall of Juman (see Table 2).
If a modelis correctly estimated, then a larger cost-width will improve precision.
Therefore, we canestimate model accuracy from the precision at cost-width 500 or 1000.When estimated without the credit factor (Fig.
4), neither the HMM nor the TAG-bigrammodel was robust against noisy training data.
It was also observed in the experiments hatthe accuracy of tagging was degraded by excessive iterations of reestimation.
I conclude thatit is hard to estimate the Japanese model from only an untagged corpus and a dictionary.Precision was improved by the step credit factor function whose threshold is 70 (Fig.
5).The precision of the HMMs are better than the precision of the TAG-bigram model, despitethe number of parameters of the TAG-HMM being smaller than that for the TAG-bigrammodel.
The HMM is very capable of modeling language, if the training data is reliable.Including the variable credit factor in these models is an effective way to improve precision(Fig.
6).
In particular, the results of the TAG-bigram model were dramatically improvedby using the variable credit factor.
Although incorporating the credit factor into the HMMimproved the results, they remained at a level similar to that of the TAG-bigram model withthe credit factor.
Although it is not clear exactly why the HMM did not improved more,there are at least three possible explanations: (1) theoretical limitation of estimation usinga~ untagged corpus, (2) using an untagged corpus, estimation of the HMM is harder thanestimation of the bigram model, therefore more corpora are needed to train the HMM or(3) the credit factor in this experiment matched to the bigram model but not to the HMM.Investigation of these possibilities in the future is needed.6 D iscuss ion  and Future WorkMerialdo (1994) and Elworthy (1994) have insisted, based on their experimental results, thatthe maximum likelihood training using an untagged corpus does not necessarily improve tag-ging accuracy.
However, their likelihood was the probability with all paths weighted equiva-lently.
Since more than half of the symbols in the observations may be noise, models estimatedin this way are not reliable.
The credit factor was introduced to redefine the likelihood of train-ing data.
The new likelihood was based on the probability with each possible path weightedby the credit factor.
The extended reestimation algorithm can approximately maximize themodified likelihood and improve the model accuracy.The Baum-Welch reestimation algorithm was also extended in two ways.
The algorithm canbe applied to an unsegmented language (e.g., Japanese), because of the extension for copingwith lattice-based observations as training data.
The other extension is that the algorithm can163precision94.0093.0092.0091.0090.0089.0088.0087.0086.0085.0084.0083.0082.0081.0080.00.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~ ,  .
.
.
.
.
4 .
.
.
.
2"-.,.4 .
.
.
.
\ -~PAIRqrAG-bigram-HM.M Iiofi.~na_l Juman.
.
.
.
.
.
- - I I -  .
.
.
.
.
", - .
TAG-bigram%%I %", TAG-HMN%b .
.
.
.
.
((0 ) )  10 30 100 300cost-width1000Figure 4: The precision of the models estimated without the credit factor164precision94.0093.5093.00i h92.5092.0091.50 .
.
.
.91.0090.50- - -' .
.
.
.
.
.
.
.
.
.
PAIR-I-\] MMTAG-HMM,XfE:i-ffq_Cf" ' ' - - e  .
.
.
.
.
- tITAG-HMM.
.
.
.
.
.
.
.
O .
.
.
.
.
.
.
.
.
.original JunqTAG-bigrar~0 10 30 100 300 1000cost-widthanFigure 5: The  precision of the models est imated with the step credit factor.precision94.0093.5093.0092.50 ,92.00 - - - -91.5091.0090.50 I0"POS-bigramr- daG.
.
.
.
.
- J ' - -  - .
.
.
.
.
.
4" ' " ? "
-B -  .
.
.
.
.
.
.
.
.
.TAG-I-IMMop i_n J.u_  an10 30 100 300 1000cost-widthFigure 6: The precision of the models est imated with the variable credit factor.165train the HMM in addition to the N-gram model.
Takeuchi and Matsumoto (1995) proposedthe bigram estimation method from an untagged Japanese corpus.
Their algorithm dividesa morpheme network into possible sequences that are then used for the normal Baum-Welchalgorithm.
This algorithm cannot ake advantage of the scaling procedure, because it requiresthe synchronous calculation of all possible sequences in the morpheme network.
Nagata (1996)recently proposed a generalized forward-backward algorithm that is a character synchronousmethod for unsegmented languages.
He applied this algorithm to bigram model training fromuntagged Japanese text for new word extraction.
However, he did not apply this algorithm tothe estimation of HMM parameters.Two additional experiments have been planned.
One is related to the limitations of esti-mation using untagged corpora.
The other is related to assignment of the credit factor withouta rule-based tagger.The credit factor improved the upper bound of the estimation accuracy from an untaggedcorpus.
However, at higher levels of tagging accuracy, the reestimation method based on theBaum-Welch algorithm is limited by the noise of untagged corpora.
On this point, I agreewith Merialdo (1994) and Elworthy (1994).
One promising direction for future work wouldbe an integration of models estimated from tagged and untagged corpora.
Although the totalmodel estimated from an untagged corpus is worse than that from a model using a taggedcorpus, a part of the model using the untagged corpus may be better, because stimationsfrom untagged corpora can use very extensive training material.
In the bigram model, we canweight each probability of a pair of tags in both models estimated from tagged or untaggedcorpora.
A smoothing method, such as deleted interpolation (Jelinek, 1985), can be used forweighting.Another promising avenue for research is the development of improved methods to assignthe credit factor without using rule-based taggers.
Any chosen rule-based tagger will impartits own characteristic errors to credit factors it has been used to assign.
Such errors can bemisleading in the modeling of language.
In order to assign more neutral values to the creditfactor, we can use the estimated model itself.
In the initial estimation of a model, an equivalentcredit factor is used for estimation.
After several iterations of reestimation, development datatagged by hand is used to evaluate the estimated model.
The credit factors can be assignedfrom this evaluation process and be used in the second phase of estimation.
Following thesecond phase of estimation, new credit factors would be decided by evaluation of the newmodel.
Such a global iteration is a special version of error correcting learning.7 ConclusionWe have proposed an estimation method from ambiguous observations and a credit factor.This estimation method can use untagged, unsegmented language corpora as training dataand build not only the N-gram model, but also the HMM.
A credit factor can improve thereliability of the model estimated from an untagged corpus.This method can be further improved and integrated with other language models.
Inparticular, it is important to formulate a dynamic method to assign the credit factor based onsmall sets of tagged data for development.AcknowledgementThanks are due to the members of both the Itahashi laboratory at the University of Tsukubaand the Nakagawa laboratory at the Toyohashi University of Technology for their help andcriticism at various stages of this research.166ReferencesCutting, D., J. Kupiec, J. Pedersen and P. Sibun.
1992.
A practical part-of-speech tagger.In Proceedings off the Second Conference on Applied Natural Language Processing, pages133-140.
Association for Computational Linguistics, Morristown, New Jersey.Elworthy, David.
1994.
Does Baum-Welch re-estimation help taggers?
In Proceedings ofthe 4th Conference on Applied Natural Language Processing, pages 53-58.
Association forComputational Linguistics, Morristown, New Jersey.Japan Electronic Dictionary Research Institute.
1995.
EDR Electronic Dictionary Version 2Technical Guide.
http://www.iijnet.or.jp/edr.Jelinek, Frederick.
1985.
Self-organized language modeling for speech recognition.
IBM Re-port.
(Reprinted in Readings in Speech Recognition, pages 450-506, Morgan Kaufmann).Kupiec, Julian.
1992.
Robust part-of-speech tagging using a hidden Markov model.
ComputerSpeech and Language, 6, pages 225-242.Matsumoto, Y., S. Kurohashi, T. Utsuro, Y. Nyoki and M. Nagao 1994.
Japanese morpho-logical analysis ystem JUMAN manual (in Japanese).Merialdo, Bernard.
1994.
Tagging English text with a probabilistic model.
ComputationalLinguistics, 20(2), pages 155-171.Nagata, Masaaki.
1994.
A stochastic Japanese morphological nalyzer using a forward-DPbackward-A* N-best search Mgorithm.
In Proceedings of COLING-94, pages 201-207.Nagata, Masaaki.
1996.
Automatic extraction of new words from Japanese texts using gener-alized forward-backward search.
In Proceedings ofEmpirical Methods in Natural LanguageProcessing, pages 48-59.Nihon Keizai Shimbun, Inc. 1995.
Nikkei newspaper database 1994, CD-ROM version.Rabiner, Lawrence and Biing-Hwang Juang.
1994.
Fundamentals ofSpeech Recognition.
PTRPrentice-Hall, Inc.Takeuchi, Kouichi and Yuji Matsumoto.
1995.
Learning parameters of Japanese morphologicalanalyzer based-on hidden Markov model.
IPSJ Technical Report SIG-NL, 108-3, pages 13-19 (in Japanese).Zhou, Min and Seiichi Nakagawa.
1994.
A study of stochastic language models for Japaneseand English.
In Proceedings of Symposium on Learning in Natural Language Processing,pages 57-64 (in Japanese).167
