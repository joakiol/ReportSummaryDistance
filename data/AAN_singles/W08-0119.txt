Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 112?119,Columbus, June 2008. c?2008 Association for Computational LinguisticsTraining and Evaluation of the HIS POMDP Dialogue System in NoiseM.
Gas?ic?, S. Keizer, F. Mairesse, J. Schatzmann, B. Thomson, K. Yu, S. YoungMachine Intelligence LaboratoryEngineering DepartmentCambridge UniversityUnited KingdomAbstractThis paper investigates the claim that a di-alogue manager modelled as a Partially Ob-servable Markov Decision Process (POMDP)can achieve improved robustness to noisecompared to conventional state-based dia-logue managers.
Using the Hidden Infor-mation State (HIS) POMDP dialogue man-ager as an exemplar, and an MDP-based dia-logue manager as a baseline, evaluation resultsare presented for both simulated and real dia-logues in a Tourist Information Domain.
Theresults on the simulated data show that theinherent ability to model uncertainty, allowsthe POMDP model to exploit alternative hy-potheses from the speech understanding sys-tem.
The results obtained from a user trialshow that the HIS system with a trained policyperformed significantly better than the MDPbaseline.1 IntroductionConventional spoken dialogue systems operate byfinding the most likely interpretation of each userinput, updating some internal representation of thedialogue state and then outputting an appropriate re-sponse.
Error tolerance depends on using confidencethresholds and where they fail, the dialogue managermust resort to quite complex recovery procedures.Such a system has no explicit mechanisms for rep-resenting the inevitable uncertainties associated withspeech understanding or the ambiguities which natu-rally arise in interpreting a user?s intentions.
The re-sult is a system that is inherently fragile, especiallyin noisy conditions or where the user is unsure ofhow to use the system.It has been suggested that Partially ObservableMarkov Decision Processes (POMDPs) offer a nat-ural framework for building spoken dialogue sys-tems which can both model these uncertaintiesand support policies which are robust to their ef-fects (Young, 2002; Williams and Young, 2007a).The key idea of the POMDP is that the underlyingdialogue state is hidden and dialogue managementpolicies must therefore be based not on a single stateestimate but on a distribution over all states.Whilst POMDPs are attractive theoretically, inpractice, they are notoriously intractable for any-thing other than small state/action spaces.
Hence,practical examples of their use were initially re-stricted to very simple domains (Roy et al, 2000;Zhang et al, 2001).
More recently, however, a num-ber of techniques have been suggested which do al-low POMDPs to be scaled to handle real world tasks.The two generic mechanisms which facilitate thisscaling are factoring the state space and perform-ing policy optimisation in a reduced summary statespace (Williams and Young, 2007a; Williams andYoung, 2007b).Based on these ideas, a number of real-worldPOMDP-based systems have recently emerged.
Themost complex entity which must be represented inthe state space is the user?s goal.
In the BayesianUpdate of Dialogue State (BUDS) system, the user?sgoal is further factored into conditionally indepen-dent slots.
The resulting system is then modelledas a dynamic Bayesian network (Thomson et al,2008).
A similar approach is also developed in112(Bui et al, 2007a; Bui et al, 2007b).
An alterna-tive approach taken in the Hidden Information State(HIS) system is to retain a complete representationof the user?s goal, but partition states into equiva-lence classes and prune away very low probabilitypartitions (Young et al, 2007; Thomson et al, 2007;Williams and Young, 2007b).Whichever approach is taken, a key issue in a realPOMDP-based dialogue system is its ability to berobust to noise and that is the issue that is addressedin this paper.
Using the HIS system as an exem-plar, evaluation results are presented for a real-worldtourist information task using both simulated andreal users.
The results show that a POMDP systemcan learn noise robust policies and that N-best out-puts from the speech understanding component canbe exploited to further improve robustness.The paper is structured as follows.
Firstly, in Sec-tion 2 a brief overview of the HIS system is given.Then in Section 3, various POMDP training regimesare described and evaluated using a simulated user atdiffering noise levels.
Section 4 then presents resultsfrom a trial in which users conducted various tasksover a range of noise levels.
Finally, in Section 5,we discuss our results and present our conclusions.2 The HIS System2.1 Basic PrinciplesA POMDP-based dialogue system is shown in Fig-ure 1 where sm denotes the (unobserved or hidden)machine state which is factored into three compo-nents: the last user act au, the user?s goal su andthe dialogue history sd.
Since sm is unknown, ateach time-step the system computes a belief statesuch that the probability of being in state sm givenbelief state b is b(sm).
Based on this current beliefstate b, the machine selects an action am, receivesa reward r(sm, am), and transitions to a new (un-observed) state s?m, where s?m depends only on smand am.
The machine then receives an observationo?
consisting of an N-best list of hypothesised useractions.
Finally, the belief distribution b is updatedbased on o?
and am as follows:b?
(s?m) = kP (o?|s?m, am)?sm?SmP (s?m|am, sm)b(sm)(1)where k is a normalisation constant (Kaelbling et al,1998).
The first term on the RHS of (1) is called theobservation model and the term inside the summa-tion is called the transition model.
Maintaining thisbelief state as the dialogue evolves is called beliefmonitoring.SpeechUnderstandingSpeechGenerationUserauam~suamBeliefEstimatorDialogPolicysdb(     )smsm=<au,su,sd>au1..auNFigure 1: Abstract view of a POMDP-based spoken dia-logue systemAt each time step t, the machine receives a rewardr(bt, am,t) based on the current belief state bt and theselected action am,t.
Each action am,t is determinedby a policy ?
(bt) and building a POMDP system in-volves finding the policy ??
which maximises thediscounted sum R of the rewardsR =?
?t=0?tr(bt, am,t) (2)where ?t is a discount coefficient.2.2 Probability ModelsIn the HIS system, user goals are partitioned andinitially, all states su ?
Su are regarded as beingequally likely and they are placed in a single par-tition p0.
As the dialogue progresses, user inputsresult in changing beliefs and this root partition isrepeatedly split into smaller partitions.
This split-ting is binary, i.e.
p ?
{p?, p ?
p?}
with probabilityP (p?|p).
By replacing sm by its factors (su, au, sd)and making reasonable independence assumptions,it can be shown (Young et al, 2007) that in parti-113tioned form (1) becomesb?
(p?, a?u, s?d) = k ?
P (o?|a?u)?
??
?observationmodelP (a?u|p?, am)?
??
?user actionmodel?
?sdP (s?d|p?, a?u, sd, am)?
??
?dialoguemodelP (p?|p)b(p, sd)?
??
?partitionsplitting(3)where p is the parent of p?.In this equation, the observation model is approx-imated by the normalised distribution of confidencemeasures output by the speech recognition system.The user action model allows the observation prob-ability that is conditioned on a?u to be scaled by theprobability that the user would speak a?u given thepartition p?
and the last system prompt am.
In thecurrent implementation of the HIS system, user dia-logue acts take the form act(a = v) where act is thedialogue type, a is an attribute and v is its value [forexample, request(food=Chinese)].
The user actionmodel is then approximated byP (a?u|p?, am) ?
P (T (a?u)|T (am))P (M(a?u)|p?
)(4)where T (?)
denotes the type of the dialogue actand M(?)
denotes whether or not the dialogue actmatches the current partition p?.
The dialoguemodel is a deterministic encoding based on a simplegrounding model.
It yields probability one when theupdated dialogue hypothesis (i.e., a specific combi-nation of p?, a?u, sd and am) is consistent with thehistory and zero otherwise.2.3 Policy RepresentationPolicy representation in POMDP-systems is non-trivial since each action depends on a complex prob-ability distribution.
One of the simplest approachesto dealing with this problem is to discretise the statespace and then associate an action with each dis-crete grid point.
To reduce quantisation errors, theHIS model first maps belief distributions into a re-duced summary space before quantising.
This sum-mary space consists of the probability of the toptwo hypotheses plus some status variables and theuser act type associated with the top distribution.Quantisation is then performed using a simple dis-tance metric to find the nearest grid point.
Ac-tions in summary space refer specifically to the toptwo hypotheses, and unlike actions in master space,they are limited to a small finite set: greet, ask, ex-plicit confirm, implicit confirm, select confirm, of-fer, inform, find alternative, query more, goodbye.A simple heuristic is then used to map the selectednext system action back into the full master beliefspace.1ObservationFromUserOntology Rules2Nuama~FromSystem112221ds2ds1ds2ds3ds1up2up3upPOMDPPolicy2h3h4h5h1h~au~au~au~au~au~au~auBeliefStateApplication DatabaseActionRefinement(heuristic)ma^StrategicActionSpecificActionMap toSummarySpacemabb^Summary SpaceFigure 2: Overview of the HIS system dialogue cycleThe dialogue manager is able to support nega-tions, denials and requests for alternatives.
When theselected summary action is to offer the user a venue,the summary-to-master space mapping heuristicswill normally offer a venue consistent with the mostlikely user goal hypothesis.
If this hypothesis is thenrejected its belief is substantially reduced and it willno longer be the top-ranking hypothesis.
If the nextsystem action is to make an alternative offer, thenthe new top-ranking hypothesis may not be appro-priate.
For example, if an expensive French restau-rant near the river had been offered and the user asksfor one nearer the centre of town, any alternative of-fered should still include the user?s confirmed de-sire for an expensive French restaurant.
To ensurethis, all of the grounded features from the rejectedhypothesis are extracted and all user goal hypothe-ses are scanned starting at the most likely until analternative is found that matches the grounded fea-tures.
For the current turn only, the summary-to-master space heuristics then treat this hypothesis asif it was the top-ranking one.
If the system then of-fers a venue based on this hypothesis, and the useraccepts it, then, since system outputs are appendedto user inputs for the purpose of belief updating, the114alternative hypothesis will move to the top, or nearthe top, of the ranked hypothesis list.
The dialoguethen typically continues with its focus on the newlyoffered alternative venue.2.4 Summary of OperationTo summarise, the overall processing performed bythe HIS system in a single dialogue turn (i.e.
one cy-cle of system output and user response) is as shownin Figure 2.
Each user utterance is decoded into anN-best list of dialogue acts.
Each incoming act plusthe previous system act are matched against the for-est of user goals and partitions are split as needed.Each user act au is then duplicated and bound toeach partition p. Each partition will also have aset of dialogue histories sd associated with it.
Thecombination of each p, au and updated sd forms anew dialogue hypothesis hk whose beliefs are eval-uated using (3).
Once all dialogue hypotheses havebeen evaluated and any duplicates merged, the mas-ter belief state b is mapped into summary space b?and the nearest policy belief point is found.
The as-sociated summary space machine action a?m is thenheuristically mapped back to master space and themachine?s actual response am is output.
The cyclethen repeats until the user?s goal is satisfied.3 Training and Evaluation with aSimulated User3.1 Policy optimisationPolicy optimisation is performed in the discretesummary space described in the previous section us-ing on-line batch ?-greedy policy iteration.
Givenan existing policy ?, dialogs are executed and ma-chine actions generated according to ?
except thatwith probability ?
a random action is generated.
Thesystem maintains a set of belief points {b?i}.
At eachturn in training, the nearest stored belief point b?k tob?
is located using a distance measure.
If the distanceis greater than some threshold, b?
is added to the setof stored belief points.
The sequence of points b?ktraversed in each dialogue is stored in a list.
As-sociated with each b?i is a function Q(b?i, a?m) whosevalue is the expected total reward obtained by choos-ing summary action a?m from state b?i.
At the endof each dialogue, the total reward is calculated andadded to an accumulator for each point in the list,discounted by ?
at each step.
On completion of abatch of dialogs, the Q values are updated accord-ing to the accumulated rewards, and the policy up-dated by choosing the action which maximises eachQ value.
The whole process is then repeated untilthe policy stabilises.In our experiments, ?
was fixed at 0.1 and ?
wasfixed at 0.95.
The reward function used attemptedto encourage short successful dialogues by assign-ing +20 for a successful dialogue and ?1 for eachdialogue turn.3.2 User SimulationTo train a policy, a user simulator is used to gen-erate responses to system actions.
It has two maincomponents: a User Goal and a User Agenda.
Atthe start of each dialogue, the goal is randomlyinitialised with requests such as ?name?, ?addr?,?phone?
and constraints such as ?type=restaurant?,?food=Chinese?, etc.
The agenda stores the di-alogue acts needed to elicit this information in astack-like structure which enables it to temporarilystore actions when another action of higher priorityneeds to be issued first.
This enables the simulatorto refer to previous dialogue turns at a later point.
Togenerate a wide spread of realistic dialogs, the sim-ulator reacts wherever possible with varying levelsof patience and arbitrariness.
In addition, the sim-ulator will relax its constraints when its initial goalcannot be satisfied.
This allows the dialogue man-ager to learn negotiation-type dialogues where onlyan approximate solution to the user?s goal exists.Speech understanding errors are simulated at the di-alogue act level using confusion matrices trained onlabelled dialogue data (Schatzmann et al, 2007).3.3 Training and EvaluationWhen training a system to operate robustly in noisyconditions, a variety of strategies are possible.
Forexample, the system can be trained only on noise-free interactions, it can be trained on increasing lev-els of noise or it can be trained on a high noise levelfrom the outset.
A related issue concerns the gener-ation of grid points and the number of training itera-tions to perform.
For example, allowing a very largenumber of points leads to poor performance due toover-fitting of the training data.
Conversely, havingtoo few point leads to poor performance due to a lack115of discrimination in its dialogue strategies.After some experimentation, the following train-ing schedule was adopted.
Training starts in anoise free environment using a small number of gridpoints and it continues until the performance of thepolicy levels off.
The resulting policy is then takenas an initial policy for the next stage where the noiselevel is increased, the number of grid points is ex-panded and the number of iterations is increased.This process is repeated until the highest noise levelis reached.
This approach was motivated by the ob-servation that a key factor in effective reinforcementlearning is the balance between exploration and ex-ploitation.
In POMDP policy optimisation whichuses dynamically allocated grid points, maintainingthis balance is crucial.
In our case, the noise intro-duced by the simulator is used as an implicit mech-anism for increasing the exploration.
Each time ex-ploration is increased, the areas of state-space thatwill be visited will also increase and hence the num-ber of available grid points must also be increased.At the same time, the number of iterations must beincreased to ensure that all points are visited a suf-ficient number of times.
In practice we found thataround 750 to 1000 grid points was sufficient andthe total number of simulated dialogues needed fortraining was around 100,000.A second issue when training in noisy conditionsis whether to train on just the 1-best output from thesimulator or train on the N-best outputs.
A limit-ing factor here is that the computation required forN-best training is significantly increased since therate of partition generation in the HIS model in-creases exponentially with N. In preliminary tests,it was found that when training with 1-best outputs,there was little difference between policies trainedentirely in no noise and policies trained on increas-ing noise as described above.
However, policiestrained on 2-best using the incremental strategy didexhibit increased robustness to noise.
To illustratethis, Figures 3 and 4 show the average dialogue suc-cess rates and rewards for 3 different policies, alltrained on 2-best: a hand-crafted policy (hdc), a pol-icy trained on noise-free conditions (noise free) anda policy trained using the incremental scheme de-scribed above (increm).
Each policy was tested us-ing 2-best output from the simulator across a rangeof error rates.
In addition, the noise-free policy wasalso tested on 1-best output.Figure 3: Average simulated dialogue success rate as afunction of error rate for a hand-crafted (hdc), noise-freeand incrementally trained (increm) policy.Figure 4: Average simulated dialogue reward as a func-tion of error rate for a hand-crafted (hdc), noise-free andincrementally trained (increm) policy.As can be seen, both the trained policies improvesignificantly on the hand-crafted policies.
Further-more, although the average rewards are all broadlysimilar, the success rate of the incrementally trainedpolicy is significantly better at higher error rates.Hence, this latter policy was selected for the usertrial described next.4 Evaluation via a User TrialThe HIS-POMDP policy (HIS-TRA) that was incre-mentally trained on the simulated user using 2-bestlists was tested in a user trial together with a hand-crafted HIS-POMDP policy (HIS-HDC).
The strat-egy used by the latter was to first check the mostlikely hypothesis.
If it contains sufficient grounded116keys to match 1 to 3 database entities, then offer isselected.
If any part of the hypothesis is inconsis-tent or the user has explicitly asked for another sug-gestion, then find alternative action is selected.
Ifthe user has asked for information about an offeredentity then inform is selected.
Otherwise, an un-grounded component of the top hypothesis is identi-fied and depending on the belief, one of the confirmactions is selected.In addition, an MDP-based dialogue manager de-veloped for earlier trials (Schatzmann, 2008) wasalso tested.
Since considerable effort has been put inoptimising this system, it serves as a strong baselinefor comparison.
Again, both a trained policy (MDP-TRA) and a hand-crafted policy (MDP-HDC) weretested.4.1 System setup and confidence scoringThe dialogue system consisted of an ATK-basedspeech recogniser, a Phoenix-based semantic parser,the dialogue manager and a diphone based speechsynthesiser.
The semantic parser uses simple phrasalgrammar rules to extract the dialogue act type and alist of attribute/value pairs from each utterance.In a POMDP-based dialogue system, accuratebelief-updating is very sensitive to the confidencescores assigned to each user dialogue act.
Ideallythese should provide a measure of the probability ofthe decoded act given the true user act.
In the evalu-ation system, the recogniser generates a 10-best listof hypotheses at each turn along with a compact con-fusion network which is used to compute the infer-ence evidence for each hypothesis.
The latter is de-fined as the sum of the log-likelihoods of each arcin the confusion network and when exponentiatedand renormalised this gives a simple estimate of theprobability of each hypothesised utterance.
Each ut-terance in the 10-best list is passed to the semanticparser.
Equivalent dialogue acts output by the parserare then grouped together and the dialogue act foreach group is then assigned the sum of the sentence-level probabilities as its confidence score.4.2 Trial setupFor the trial itself, 36 subjects were recruited (allBritish native speakers, 18 male, 18 female).
Eachsubject was asked to imagine himself to be a touristin a fictitious town called Jasonville and try to findparticular hotels, bars, or restaurants in that town.Each subject was asked to complete a set of pre-defined tasks where each task involved finding thename of a venue satisfying a set of constraints suchas food type is Chinese, price-range is cheap, etc.,and getting the value of one or more additional at-tributes of that venue such as the address or thephone number.For each task, subjects were given a scenario toread and were then asked to solve the task via a di-alogue with the system.
The tasks set could eitherhave one solution, several solutions, or no solutionat all in the database.
In cases where a subject foundthat there was no matching venue for the given task,he/she was allowed to try and find an alternativevenue by relaxing one or more of the constraints.In addition, subjects had to perform each task atone of three possible noise levels.
These levels cor-respond to signal/noise ratios (SNRs) of 35.3 dB(low noise), 10.2 dB (medium noise), or 3.3 dB(high noise).
The noise was artificially generatedand mixed with the microphone signal, in additionit was fed into the subject?s headphones so that theywere aware of the noisy conditions.An instructor was present at all times to indicateto the subject which task description to follow, andto start the right system with the appropriate noise-level.
Each subject performed an equal number oftasks for each system (3 tasks), noise level (6 tasks)and solution type (6 tasks for each of the types 0, 1,or multiple solutions).
Also, each subject performedone task for all combinations of system and noiselevel.
Overall, each combination of system, noiselevel, and solution type was used in an equal numberof dialogues.4.3 ResultsIn Table 1, some general statistics of the corpus re-sulting from the trial are given.
The semantic errorrate is based on substitutions, insertions and dele-tions errors on semantic items.
When tested after thetrial on the transcribed user utterances, the semanticerror rate was 4.1% whereas the semantic error rateon the ASR input was 25.2%.
This means that 84%of the error rate was due to the ASR.Tables 2 and 3 present success rates (Succ.)
andaverage performance scores (Perf.
), comparing thetwo HIS dialogue managers with the two MDP base-117Number of dialogues 432Number of dialogue turns 3972Number of words (transcriptions) 18239Words per utterance 4.58Word Error Rate 32.9Semantic Error Rate 25.2Semantic Error Rate transcriptions 4.1Table 1: General corpus statistics.line systems.
For the success rates, also the stan-dard deviation (std.dev) is given, assuming a bino-mial distribution.
The success rate is the percentageof successfully completed dialogues.
A task is con-sidered to be fully completed when the user is able tofind the venue he is looking for and get al the addi-tional information he asked for; if the task has no so-lution and the system indicates to the user no venuecould be found, this also counts as full completion.A task is considered to be partially completed whenonly the correct venue has been given.
The results onpartial completion are given in Table 2, and the re-sults on full completion in Table 3.
To mirror the re-ward function used in training, the performance foreach dialogue is computed by assigning a reward of20 points for full completion and subtracting 1 pointfor the number of turns up until a successful recom-mendation (i.e., partial completion).Partial Task Completion statisticsSystem Succ.
(std.dev) #turns Perf.MDP-HDC 68.52 (4.83) 4.80 8.91MDP-TRA 70.37 (4.75) 4.75 9.32HIS-HDC 74.07 (4.55) 7.04 7.78HIS-TRA 84.26 (3.78) 4.63 12.22Table 2: Success rates and performance results on partialcompletion.Full Task Completion statisticsSystem Succ.
(std.dev) #turns Perf.MDP-HDC 64.81 (4.96) 5.86 7.10MDP-TRA 65.74 (4.93) 6.18 6.97HIS-HDC 63.89 (4.99) 8.57 4.20HIS-TRA 78.70 (4.25) 6.36 9.38Table 3: Success rates and performance results on fullcompletion.The results show that the trained HIS dialoguemanager significantly outperforms both MDP baseddialogue managers.
For success rate on partial com-pletion, both HIS systems perform better than theMDP systems.4.3.1 Subjective ResultsIn the user trial, the subjects were also asked fora subjective judgement of the systems.
After com-pleting each task, the subjects were asked whetherthey had found the information they were lookingfor (yes/no).
They were also asked to give a scoreon a scale from 1 to 5 (best) on how natural/intuitivethey thought the dialogue was.
Table 4 shows theresults for the 4 systems used.
The performance ofthe HIS systems is similar to the MDP systems, witha slightly higher success rate for the trained one anda slightly lower score for the handcrafted one.System Succ.
Rate (std.dev) ScoreMDP-HDC 78 (4.30) 3.52MDP-TRA 78 (4.30) 3.42HIS-HDC 71 (4.72) 3.05HIS-TRA 83 (3.90) 3.41Table 4: Subjective performance results from the usertrial.5 ConclusionsThis paper has described recent work in training aPOMDP-based dialogue manager to exploit the ad-ditional information available from a speech under-standing system which can generate ranked lists ofhypotheses.
Following a brief overview of the Hid-den Information State dialogue manager and pol-icy optimisation using a user simulator, results havebeen given for both simulated user and real user di-alogues conducted at a variety of noise levels.The user simulation results have shown that al-though the rewards are similar, training with 2-bestrather than 1-best outputs from the user simulatoryields better success rates at high noise levels.
Inview of this result, we would have liked to inves-tigate training on longer N-best lists, but currentlycomputational constraints prevent this.
We hope inthe future to address this issue by developing moreefficient state partitioning strategies for the HIS sys-tem.118The overall results on real data collected from theuser trial clearly indicate increased robustness by theHIS system.
We would have liked to be able toplot performance and success scores as a functionof noise level or speech understanding error rate,but there is great variability in these kinds of com-plex real-world dialogues and it transpired that thetrial data was insufficient to enable any statisticallymeaningful presentation of this form.
We estimatethat we need at least an order of magnitude moretrial data to properly investigate the behaviour ofsuch systems as a function of noise level.
The trialdescribed here, including transcription and analysisconsumed about 30 man-days of effort.
Increasingthis by a factor of 10 or more is not therefore anoption for us, and clearly an alternative approach isneeded.We have also reported results of subjective suc-cess rate and opinion scores based on data obtainedfrom subjects after each trial.
The results were onlyweakly correlated with the measured performanceand success rates.
We believe that this is partly dueto confusion as to what constituted success in theminds of the subjects.
This suggests that for subjec-tive results to be meaningful, measurements such asthese will only be really useful if made on live sys-tems where users have a real rather than imaginedinformation need.
The use of live systems wouldalso alleviate the data sparsity problem noted earlier.Finally and in conclusion, we believe that despitethe difficulties noted above, the results reported inthis paper represent a first step towards establish-ing the POMDP as a viable framework for develop-ing spoken dialogue systems which are significantlymore robust to noisy operating conditions than con-ventional state-based systems.AcknowledgementsThis research was partly funded by the UK EPSRCunder grant agreement EP/F013930/1 and by theEU FP7 Programme under grant agreement 216594(CLASSIC project: www.classic-project.org).ReferencesTH Bui, M Poel, A Nijholt, and J Zwiers.
2007a.
Atractable DDN-POMDP Approach to Affective Dia-logue Modeling for General Probabilistic Frame-basedDialogue Systems.
In Proc 5th Workshop on Knowl-edge and Reasoning in Practical Dialogue Systems,pages 34?57.TH Bui, B van Schooten, and D Hofs.
2007b.
Practi-cal dialogue manager development using POMDPs .In 8th SIGdial Workshop on Discourse and Dialogue,Antwerp.LP Kaelbling, ML Littman, and AR Cassandra.
1998.Planning and Acting in Partially Observable StochasticDomains.
Artificial Intelligence, 101:99?134.N Roy, J Pineau, and S Thrun.
2000.
Spoken DialogueManagement Using Probabilistic Reasoning.
In ProcACL.J Schatzmann, B Thomson, and SJ Young.
2007.
ErrorSimulation for Training Statistical Dialogue Systems.In ASRU 07, Kyoto, Japan.J Schatzmann.
2008.
Statistical User and Error Mod-elling for Spoken Dialogue Systems.
Ph.D. thesis, Uni-versity of Cambridge.B Thomson, J Schatzmann, K Weilhammer, H Ye, andSJ Young.
2007.
Training a real-world POMDP-basedDialog System.
In HLT/NAACL Workshop ?Bridgingthe Gap: Academic and Industrial Research in DialogTechnologies?, Rochester.B Thomson, J Schatzmann, and SJ Young.
2008.Bayesian Update of Dialogue State for Robust Dia-logue Systems.
In Int Conf Acoustics Speech and Sig-nal Processing ICASSP, Las Vegas.JD Williams and SJ Young.
2007a.
Partially ObservableMarkov Decision Processes for Spoken Dialog Sys-tems.
Computer Speech and Language, 21(2):393?422.JD Williams and SJ Young.
2007b.
Scaling POMDPsfor Spoken Dialog Management.
IEEE Audio, Speechand Language Processing, 15(7):2116?2129.SJ Young, J Schatzmann, K Weilhammer, and H Ye.2007.
The Hidden Information State Approach to Dia-log Management.
In ICASSP 2007, Honolulu, Hawaii.SJ Young.
2002.
Talking to Machines (StatisticallySpeaking).
In Int Conf Spoken Language Processing,Denver, Colorado.B Zhang, Q Cai, J Mao, E Chang, and B Guo.
2001.Spoken Dialogue Management as Planning and Actingunder Uncertainty.
In Eurospeech, Aalborg, Denmark.119
