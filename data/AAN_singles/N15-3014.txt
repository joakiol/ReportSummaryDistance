Proceedings of NAACL-HLT 2015, pages 66?70,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsLean Question Answering over Freebase from ScratchXuchen Yaokitt.ai?2157 N Northlake WaySeattle, WA 98103, USAAbstractFor the task of question answering (QA) overFreebase on the WEBQUESTIONS dataset(Berant et al, 2013), we found that 85% of allquestions (in the training set) can be directlyanswered via a single binary relation.
Thus weturned this task into slot-filling for <questiontopic, relation, answer> tuples: predicting re-lations to get answers given a question?s topic.We design efficient data structures to identifyquestion topics organically from 46 millionFreebase topic names, without employing anyNLP processing tools.
Then we present a leanQA system that runs in real time (in offlinebatch testing it answered two thousand ques-tions in 51 seconds on a laptop).
The systemalso achieved 7.8% better F1score (harmonicmean of average precision and recall) than theprevious state of the art.1 IntroductionLarge-scale open-domain question answering fromstructured Knowledge Base (KB) provides a goodbalance of precision and recall in everyday QAtasks, executed by search engines and personal assis-tant applications.
The release of WEBQUESTIONSdataset (Berant et al, 2013) has drawn a lot of inter-est from both academia and industry.
One tendencyto notice is that the general trend of research is be-coming more complex, utilizing various techniquessuch as semantic parsing and deep neural networks.We took a radically different approach by head-ing for the other direction: simplifying the task asmuch as possible with no compromise on speed andaccuracy.
We treat the task of QA from Freebase?Incubated by the Allen Institute for Artificial Intelligence.as a two-step problem: identifying the correct topic(search problem) and predicting the correct answer(prediction problem).
The common approach to thefirst problem is applying basic linguistic processing,such as part-of-speech (POS) tagging and chunkingto identify noun phrases, and named entity recog-nition (NER) for interesting topics.
The commonapproach to the second problem is detailed ques-tion analysis, which usually involves parsing.
In anycase, various components from the natural languageprocessing (NLP) pipeline are usually applied.With an emphasis on real-time prediction (usu-ally making a prediction within 100 milliseconds af-ter seeing the question), we chose not to use anyNLP preprocessing ?
not even POS tagging.
Insteadwe design efficient data structures to help identifynamed entities to tackle the search problem.For the prediction problem, we found that givena question and its topic, simply predicting the KBrelation between the topic and the answer is suffi-cient.
In other words, we turned QA from Freebaseinto a slot-filling problem in the form of <topic, re-lation, answer> tuples: given a question, the taskis to find the answer, while the search problem is tofind the topic and the prediction problem is to findthe relation.
For instance, given the question what?ssweden?s currency?, the task can be turned into a tu-ple of <Sweden, /location/country/currency_used,Swedish krona>.
In Section 3 we address howto identify the topic (Sweden) and in Section 4how to predict the relation (/location/country/cur-rency_used).
There are obvious limitations in thistask format, which are discussed in Section 6.Going beyond reporting evaluation scores, we de-scribe in details our design principle and also reportperformance in speed.
This paper makes the follow-66ing technical contributions to QA from KB:?
We design and compare several data structuresto help identify question topics using the KBresource itself.
The key to success is to searchthrough 46 million Freebase topics efficientlywhile still being robust against noise (such astypographical or speech recognition errors).?
Our algorithm is high-performance, real-time,and simple enough to replicate.
It achievedstate-of-the-art result on the WEBQUESTIONSdataset.
Training time in total is less than 5minutes and testing on 2032 questions takesless than 1 minute.
There are no external NLPlibrary dependencies: the only preprocessing islowercasing.2 Related WorkThe task of question answering from Freebase wasfirst proposed by Berant et al (2013), who crawledGoogle Suggest and annotated 5810 questions thathad answers from Freebase with Amazon Mechan-ical Turk, thus the WEBQUESTIONS dataset.
Re-searchers have approached this problem from differ-ent angles.
Semantic parsing (Berant et al, 2013;Berant and Liang, 2014) aims to predict the logicforms of the question given the distant supervisionof direct answers.
Their logic forms were derivedfrom dependency parses and then converted intodatabase queries.
Reddy et al (2014) conceptual-ized semantic parsing as a graph matching prob-lem by building graphs with Combinatory Catego-rial Grammar parses.
Edges and nodes in parsinggraphs were grounded with respect to Freebase re-lations and entities.
Other research explored thegraph nature of Freebase.
For instance, Bordes et al(2014) learned low-dimensional word embeddingsfor both the question and related topic subgraph.A scoring function was defined over these embed-dings so that correct answers yielded a higher score.Yao and Van Durme (2014) treated this task as adirect information extraction problem: each entitynode from a topic graph was ranked against othersby searching a massively generated feature space.All of the above work resorted to using the Free-base annotation of ClueWeb (Gabrilovich et al,2013) to gain extra advantage of paraphrasing QApairs or dealing with data sparsity problem.
How-ever, ClueWeb is proprietary data and costs hun-dreds of dollars to purchase.
Moreover, even thoughthe implementation systems from (Berant et al,2013; Yao and Van Durme, 2014; Reddy et al,2014) are open-source, they all take considerabledisk space (in tens of gigabytes) and training time(in days).
In this paper we present a system that canbe easily implemented in 300 lines of Python codewith no compromise in accuracy and speed.3 SearchGiven a question, we need to find out all named en-tities (or topics in Freebase terms).
For instance, forthe question what character did natalie portman playin star wars?, we are mainly interested in the topicsof natalie portman and star wars.
Note that all sen-tences in WEBQUESTIONS are lowercased.Normal approaches require a combination of ba-sic NLP processing.
For instance, an NER taggermight recognize natalie portman as a PERSON, butwould not recognize star wars as a movie, unlessthere is a pre-defined gazetteer.
Then one needs toresort to basic chunking to at least identify star warsas a noun phrase.
Moreover, these NLP tools needto be trained to better adapt lowercased sentences.Even though, one is still limited to a small numberof recognizable types: noun phrases, person, loca-tion, organization, time, date, etc.Freebase contains 46 million topics, each ofwhich is annotated with one or more types.
Thusa natural idea is to use these 46 million topics asa gazetteer and recognizes named entities from thequestion (with ambiguities), with two steps:1. enumerate all adjacent words (of variouslength) of the question, an O(N2) operationwhere N is the length of question in words;2. check whether each adjacent word block existsin the gazetteer.We use two common data structures to search effi-ciently, with three design principles:1. compact and in-memory, to avoid expensivehard disk or solid state drive I/O;2. fuzzy matching, to be robust against noise;3. easily extensible, to accommodate new topics.673.1 Fuzzy Matching and GenerationTo check whether one string is a Freebase topic, theeasiest way is to use a hash set.
However, this is notrobust against noise unless a fuzzy matching hash-ing function (e.g., locality sensitive hashing) is de-signed.
Moreover, 46 million keys in a giant hash setmight cause serious problems of key collision or setresizing in some programming languages.
Instead,we propose to use two common data structures forthe purpose of fuzzy matching or generation.Fuzzy Matching with Sorted List1: a sorted listcan provide basic fuzzy matching while avoiding thekey collision problem with slightly extra computingtime.
The search is done via 3 steps:1. build a sorted list of 46 million topics;2. to identify whether a string appears in the list,do a binary search.
Since 46 million is between225and 226, a search would require in the worstcase 26 steps down the binary random accessladder, which is a trivial computation on mod-ern CPUs;3.
For each string comparison during the binarysearch, also compute the edit distance.
Thischecks whether there is a similar string withinan edit distance of d in the list given anotherstring.Note that a sorted list does not compute all similarstrings within an edit distance of d efficiently.
Ad-jacent strings in the list also wastes space since theyare highly similar.
Thus we also propose to use aprefix tree:Fuzzy Generation with Prefix Tree (Trie): aprefix tree builds a compact representation of allstrings where common prefixes are shared towardsthe root of the tree.
By careful back tracing, a prefixtree can also output all similar strings within a fixededit distance to a given string.
This efficiently solvesthe wasted space and generation problems.3.2 Implementation and Speed ComparisonWe maximally re-used existing software for robust-ness and quick implementation:1We mix the notion of array vs. list as long as the actualimplementation satisfies two conditions: O(1) random accesstime and O(1) appending(resizing) time.d = 0 d = 1 d = 2Fuzzy Matching<0.01ms 7.9ms 7.5ms(Sorted List, PyPy)Fuzzy Generation29ms 210ms 1969ms(Trie, Elasticsearch)Table 1: Fuzzy query time per question.
d is the editdistance while d = 0 means strict matching.
HTTProundtrip overhead from Elasticsearch was also counted.Sorted List was implemented with vanilla Pythonlist, compiled with the PyPy just-in-time compiler.Prefix Tree was implemented with Elasticsearch,written in Java.Both implementations held 46 million topicnames (each topic name is 20 characters long onaverage) in memory.
Specifically, sorted list took2.06GB RAM while prefix tree took 1.62GB RAM.Then we tested how fast it was to find out alltopics from a question.
To do this, we used theDEV set of WEBQUESTIONS.
Enumerating all ad-jacent words of various length is an O(N2) opera-tion where N is a sentence length.
In practice wecounted 27 adjacent words on average for one ques-tion, thus 27 queries per question.
Elasticsearchfollows the client-server model where client sendsHTTP queries to the backend database server.
Toreduce HTTP roundtrip overhead, we queried theserver in burst mode: client only sends one ?mega?query to the server per question where each ?mega?query contains 27 small queries on average.
Exper-iments were conducted with an Intel Core i5-4278UCPU @ 2.60GHz.Table 1 shows the query time per question.
Notethat this is an evaluation of real-world computingsituation, not how efficiently either search struc-ture was implemented (or in what programming lan-guage).
Thus the purpose of comparison is to helpchoose the best implementation solution.3.3 RankingAfter identifying all possible topic names in a ques-tion, we send them to the official Freebase SearchAPI to rank them.
For instance, for the ques-tion what character did natalie portman play in starwars?, possible named entities include character,natalie, natalie portman, play, star, and star wars.68But in general natalie portman and star wars shouldbe ranked higher.
Due to the crowd-sourced nature,many topics have duplicate entries in Freebase.
Forinstance, we counted 20 different natalie portman?s(each one has a unique machine ID), but only oneis extensively edited.
One can either locally rankthem by the number of times each topic is cross-referenced with others, or use the Freebase SearchAPI in an online fashion.
In our experiments the lat-ter yielded significantly better results.
The FreebaseSearch API returns a ranked list of topic candidates.Our next job is to predict answers from this list.4 PredictionGiven a question and its topic, we directly predictthe relation that connects the topic with the answer.Our features and model are extremely simple: wetook unigram and bigram words from the questionas our features and used logistic regression to learna model that associates lexical words with relations.The training set of WEBQUESTIONS contains3778 question and answer pairs.
Each question isalso annotated with the Freebase topic used to iden-tify the answers.
Then for each of the topic-answerpairs, we extracted a direct relation from the topicto the answer, for instance (TOPIC: Sweden, RELA-TION: /location/country/currency_used, ANSWER:Swedish krona).
If there were more than one rela-tions between the topic and the answer (mostly dueto dummy ?compound?
nodes in between), we chosethe nearest one to the answer node as the direct re-lation.
To be more specific: we first selected thethe shortest path between the topic and answer node,then chose the relation from the answer node to itsparent node, regardless of whether the parent nodewas the topic node.
In this way we found direct re-lations for 3634 of these questions, which count as96.2% of the whole training set.Note that we ?reverse-engineered?
the slot-fillingrelations that would predict the correct answersbased on annotated gold answers.
It does not meanthat these relations will predict the answers with100% accuracy.
For instance, for the question whatwas the first book dr. seuss wrote?, the directrelation was /book/author/book_editions_published.However, this relation would predict all books Dr.Seuss wrote, instead of just the first one.
Thus inthe training set, we further counted the number ofrelations that point to the exact gold answers.
In all,62% of questions out of the whole training set canbe exactly answered by a single relation.The remaining 38% presented a complicated case.We sampled 100 questions and did a manual analy-sis.
There were mainly two reasons that contributedto the 38%:1.
Noisy Annotation: questions with incompleteanswers.
For instance,(a) for the question what does bob dylan sing?,the annotated answer was only ?like a rollingstone?, while the direct relation /music/artist/-track gave a full list;(b) for the question what kind of currency doescuba use?, the annotated answer was CubanPeso, while the direct relation /location/coun-try/currency_used led to two answers: CubanPeso and Cuban Convertible Peso.2.
Complex Questions: questions with con-straints that cannot be answered by binary re-lations.
For instance:(a) who does david james play for 2011?
(b) which province in canada is the most pop-ulated?
(c) who does jodelle ferland play in eclipse?For category 1, the answers provided by direct bi-nary relations will only hurt evaluation scores, butnot user experience.
For category 2, we countedabout 40% of them from the samples.
Thus in total,complex questions constructed 38% ?
40% = 15%of the whole training set.
In other words, 85% ofquestions can be answered by predicting a single bi-nary relation.
This provides statistical evidence thatthe task of QA on WEBQUESTIONS can be effec-tively simplified to a tuple slot-filling task.5 ResultsWe applied Liblinear (Fan et al, 2008) via its Scikit-learn Python interface (Pedregosa et al, 2011) totrain the logistic regression model with L2 regular-ization.
Testing on 2032 questions took 51 seconds.22This excluded the time used to call the Freebase SearchAPI, which is highly dependent on the network and server node.69F1(Berant)F1(Yao)Yao and Van Durme (2014) 33.0 42.0Berant and Liang (2014) 39.9 43.0Reddy et al (2014) 41.3 -Bordes et al (2014) 41.8 45.7this work 44.3 53.5Table 2: Results on the WEBQUESTIONS test set.We found no difference in quality but only slightlyin speed in the search results between using sortedlist and prefix tree.
Moreover, in specifically theWEBQUESTIONS dataset, there was no difference instrict matching and fuzzy matching ?
the dataset issomehow void of typographical errors.3We evaluated both the average F1over all ques-tions (Berant) and the F1of average precision andrecall values (Yao) following Bordes et al (2014),shown in Table 2.
Our method outperformed allprevious systems in both F1measures, with possi-bly two reasons: 1, the simplicity of this methodminimizes error propagation down the processingpipeline; 2, we used direct supervision while mostprevious work used distant supervision.6 Limitation and DiscussionThe limitation of our method comes from the as-sumption: most questions can be answered by pre-dicting a direct binary relation.
Thus it cannot han-dle complex questions that require to resolve a chainof relations.
These complex questions appear about15% of the time.Note that WEBQUESTIONS is a realistic dataset:it was mined off Google Suggest, which reflectspeople?s everyday searches.
Our manual analysisshowed that these complex questions usually onlycontain one type of constraint that comes from eithera ranking/superlative describer (first, most, etc) or apreposition phrase (in 1998, in some movie, etc).
Toadapt to these questions, we can take a further stepof learning to filter a returned list of results.
For in-3This is likely due to the fact that the dataset was crawledwith the Google Suggest API, which aggregates commonqueries and common queries are mostly free of typos.
For real-world everyday queries, fuzzy matching should still be applied.stance, first (first husband, first novel, etc) requireslearning a time ordering; a prepositional constraintusually picks out a single result from a list of results.To go beyond to ?crossword-like?
questions withmultiple constraints, more powerful mechanisms arecertainly needed.In summary, we have presented a system witha focus on efficiency and simplicity.
Computationtime is minimized to allow more time for networktraffic, while still being able to respond in real time.The system is based on a simpler assumption: mostquestions can be answered by directly predicting abinary relation from the question topic to the answer.The assumption is supported by both statistics andobservation.
From this simple but verified assump-tion we gained performance advantages of not onlyspeed, but also accuracy: the system achieved thebest result so far on this task.ReferencesJonathan Berant and Percy Liang.
2014.
Semantic pars-ing via paraphrasing.
In Proceedings of ACL.Jonathan Berant, Andrew Chou, Roy Frostig, and PercyLiang.
2013.
Semantic Parsing on Freebase fromQuestion-Answer Pairs.
In Proceedings of EMNLP.Antoine Bordes, Sumit Chopra, and Jason Weston.
2014.Question answering with subgraph embeddings.
InProceedings of EMNLP 2014, pages 615?620.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-RuiWang, and Chih-Jen Lin.
2008.
LIBLINEAR: A li-brary for large linear classification.
The Journal ofMachine Learning Research, 9:1871?1874.Evgeniy Gabrilovich, Michael Ringgaard, , andAmarnag Subramanya.
2013.
FACC1:Freebase annotation of ClueWeb corpora.http://lemurproject.org/clueweb09/FACC1/.Fabian Pedregosa, Ga?l Varoquaux, Alexandre Gram-fort, Vincent Michel, Bertrand Thirion, Olivier Grisel,Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vin-cent Dubourg, et al 2011.
Scikit-learn: Machinelearning in Python.
The Journal of Machine LearningResearch, 12:2825?2830.Siva Reddy, Mirella Lapata, and Mark Steedman.
2014.Large-scale semantic parsing without question-answerpairs.
Transactions of the Association for Computa-tional Linguistics, 2:377?392.Xuchen Yao and Benjamin Van Durme.
2014.
Informa-tion extraction over structured data: Question answer-ing with freebase.
In Proceedings of ACL.70
