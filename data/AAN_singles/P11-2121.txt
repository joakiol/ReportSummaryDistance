Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 687?692,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsGetting the Most out of Transition-based Dependency ParsingJinho D. ChoiDepartment of Computer ScienceUniversity of Colorado at Boulderchoijd@colorado.eduMartha PalmerDepartment of LinguisticsUniversity of Colorado at Bouldermpalmer@colorado.eduAbstractThis paper suggests two ways of improvingtransition-based, non-projective dependencyparsing.
First, we add a transition to an exist-ing non-projective parsing algorithm, so it canperform either projective or non-projectiveparsing as needed.
Second, we present a boot-strapping technique that narrows down dis-crepancies between gold-standard and auto-matic parses used as features.
The new ad-dition to the algorithm shows a clear advan-tage in parsing speed.
The bootstrappingtechnique gives a significant improvement toparsing accuracy, showing near state-of-the-art performance with respect to other parsingapproaches evaluated on the same data set.1 IntroductionDependency parsing has recently gained consider-able interest because it is simple and fast, yet pro-vides useful information for many NLP tasks (Shenet al, 2008; Councill et al, 2010).
There are twomain dependency parsing approaches (Nivre andMcDonald, 2008).
One is a transition-based ap-proach that greedily searches for local optima (high-est scoring transitions) and uses parse history as fea-tures to predict the next transition (Nivre, 2003).The other is a graph-based approach that searchesfor a global optimum (highest scoring tree) froma complete graph in which vertices represent wordtokens and edges (directed and weighted) representdependency relations (McDonald et al, 2005).Lately, the usefulness of the transition-based ap-proach has drawn more attention because it gener-ally performs noticeably faster than the graph-basedapproach (Cer et al, 2010).
The transition-based ap-proach has a worst-case parsing complexity of O(n)for projective, and O(n2) for non-projective pars-ing (Nivre, 2008).
The complexity is lower for pro-jective parsing because it can deterministically dropcertain tokens from the search space whereas thatis not advisable for non-projective parsing.
Despitethis fact, it is possible to perform non-projectiveparsing in linear time in practice (Nivre, 2009).
Thisis because the amount of non-projective dependen-cies is much smaller than the amount of projectivedependencies, so a parser can perform projectiveparsing for most cases and perform non-projectiveparsing only when it is needed.
One other advan-tage of the transition-based approach is that it canuse parse history as features to make the next pre-diction.
This parse information helps to improveparsing accuracy without hurting parsing complex-ity (Nivre, 2006).
Most current transition-based ap-proaches use gold-standard parses as features dur-ing training; however, this is not necessarily whatparsers encounter during decoding.
Thus, it is desir-able to minimize the gap between gold-standard andautomatic parses for the best results.This paper improves the engineering of differentaspects of transition-based, non-projective depen-dency parsing.
To reduce the search space, we add atransition to an existing non-projective parsing algo-rithm.
To narrow down the discrepancies betweengold-standard and automatic parses, we present abootstrapping technique.
The new addition to thealgorithm shows a clear advantage in parsing speed.The bootstrapping technique gives a significant im-provement to parsing accuracy.687LEFT-POPL( [?1|i], ?2, [j|?
], E ) ?
( ?1 , ?2, [j|?
], E ?
{iL?
j} )?i 6= 0, j. i 6??
j ?
@k ?
?.
i?
kLEFT-ARCL( [?1|i], ?2 , [j|?
], E )?
( ?1 , [i|?2], [j|?
], E ?
{iL?
j} )?i 6= 0, j. i 6??
jRIGHT-ARCL( [?1|i], ?2 , [j|?
], E )?
( ?1 , [i|?2], [j|?
], E ?
{iL?
j} )?i, j. i 6??
jSHIFT( ?1 , ?2, [j|?
], E ) ?
( [?1 ?
?2|j], [ ] , ?
, E )DT: ?1 = [ ], NT: @k ?
?1.
k ?
j ?
k ?
jNO-ARC( [?1|i], ?2 , [j|?
], E )?
( ?1 , [i|?2], [j|?
], E )default transitionTable 1: Transitions in our algorithm.
For each row, the first line shows a transition and the second line showspreconditions of the transition.2 Reducing search spaceOur algorithm is based on Choi-Nicolov?s approachto Nivre?s list-based algorithm (Nivre, 2008).
Themain difference between these two approaches is intheir implementation of the SHIFT transition.
Choi-Nicolov?s approach divides the SHIFT transition intotwo, deterministic and non-deterministic SHIFT?s,and trains the non-deterministic SHIFT with a classi-fier so it can be predicted during decoding.
Choi andNicolov (2009) showed that this implementation re-duces the parsing complexity from O(n2) to lineartime in practice (a worst-case complexity is O(n2)).We suggest another transition-based parsing ap-proach that reduces the search space even more.The idea is to merge transitions in Choi-Nicolov?snon-projective algorithm with transitions in Nivre?sprojective algorithm (Nivre, 2003).
Nivre?s projec-tive algorithm has a worst-case complexity of O(n),which is faster than any non-projective parsing al-gorithm.
Since the number of non-projective depen-dencies is much smaller than the number of projec-tive dependencies (Nivre and Nilsson, 2005), it isnot efficient to perform non-projective parsing forall cases.
Ideally, it is better to perform projectiveparsing for most cases and perform non-projectiveparsing only when it is needed.
In this algorithm, weadd another transition to Choi-Nicolov?s approach,LEFT-POP, similar to the LEFT-ARC transition inNivre?s projective algorithm.
By adding this tran-sition, an oracle can now choose either projective ornon-projective parsing depending on parsing states.11We also tried adding the RIGHT-ARC transition fromNivre?s projective algorithm, which did not improve parsingperformance for our experiments.Note that Nivre (2009) has a similar idea of per-forming projective and non-projective parsing selec-tively.
That algorithm uses a SWAP transition toreorder tokens related to non-projective dependen-cies, and runs in linear time in practice (a worst-casecomplexity is still O(n2)).
Our algorithm is distin-guished in that it does not require such reordering.Table 1 shows transitions used in our algorithm.All parsing states are represented as tuples (?1, ?2,?, E), where ?1, ?2, and ?
are lists of word tokens.E is a set of labeled edges representing previouslyidentified dependencies.
L is a dependency label andi, j, k represent indices of their corresponding wordtokens.
The initial state is ([0], [ ], [1,. .
.
,n], ?).
The0 identifier corresponds to an initial token, w0, intro-duced as the root of the sentence.
The final state is(?1, ?2, [ ], E), i.e., the algorithm terminates whenall tokens in ?
are consumed.The algorithm uses five kinds of transitions.
Alltransitions are performed by comparing the last to-ken in ?1, wi, and the first token in ?, wj .
BothLEFT-POPL and LEFT-ARCL are performed whenwj is the head of wi with a dependency relation L.The difference is that LEFT-POP removes wi from?1 after the transition, assuming that the token is nolonger needed in later parsing states, whereas LEFT-ARC keeps the token so it can be the head of sometoken wj<k?n in ?.
This wi ?
wk relation causesa non-projective dependency.
RIGHT-ARCL is per-formed whenwi is the head ofwj with a dependencyrelation L. SHIFT is performed when ?1 is empty(DT) or there is no token in ?1 that is either the heador a dependent ofwj (NT).
NO-ARC is there to movetokens around so each token in ?
can be comparedto all (or some) tokens prior to it.688It1was2in3my4interest5to6Root0see7you8SBJROOTPRD NMODPMODIMNMODOBJTransition ?1 ?2 ?
E0 [0] [ ] [1|?]
?1 SHIFT (NT) [?1|1] [ ] [2|?
]2 LEFT-ARC [0] [1] [2|?]
E ?
{1?SBJ?
2}3 RIGHT-ARC [ ] [0|?2] [2|?]
E ?
{0 ?ROOT?
2}4 SHIFT (DT) [?1|2] [ ] [3|?
]5 RIGHT-ARC [?1|1] [2] [3|?]
E ?
{2 ?PRD?
3}6 SHIFT (NT) [?1|3] [ ] [4|?
]7 SHIFT (NT) [?1|4] [ ] [5|?
]8 LEFT-POP [?1|3] [ ] [5|?]
E ?
{4?NMOD?
5}9 RIGHT-ARC [?1|2] [3] [5|?]
E ?
{3 ?PMOD?
5}10 SHIFT (NT) [?1|5] [ ] [6|?
]11 NO-ARC [?1|3] [5] [6|?
]12 NO-ARC [?1|2] [3|?2] [6|?
]13 NO-ARC [?1|1] [2|?2] [6|?
]14 RIGHT-ARC [0] [1|?2] [6|?]
E ?
{1 ?NMOD?
6}15 SHIFT (NT) [?1|6] [ ] [7|?
]16 RIGHT-ARC [?1|5] [6] [7|?]
E ?
{6 ?IM?
7}17 SHIFT (NT) [?1|7] [ ] [8|?
]18 RIGHT-ARC [?1|6] [7] [8|?]
E ?
{7 ?OBJ?
8}19 SHIFT (NT) [?1|8] [ ] [ ]Table 2: Parsing states for the example sentence.
After LEFT-POP is performed (#8), [w4 = my] is removed from thesearch space and no longer considered in the later parsing states (e.g., between #10 and #11).During training, the algorithm checks for the pre-conditions of all transitions and generates traininginstances with corresponding labels.
During decod-ing, the oracle decides which transition to performbased on the parsing states.
With the addition ofLEFT-POP, the oracle can choose either projectiveor non-projective parsing by selecting LEFT-POP orLEFT-ARC, respectively.
Our experiments show thatthis additional transition improves both parsing ac-curacy and speed.
The advantage derives from im-proving the efficiency of the choice mechanism; it isnow simply a transition choice and requires no addi-tional processing.3 Bootstrapping automatic parsesTransition-based parsing has the advantage of usingparse history as features to make the next prediction.In our algorithm, when wi and wj are compared,subtree and head information of these tokens is par-tially provided by previous parsing states.
Graph-based parsing can also take advantage of using parseinformation.
This is done by performing ?higher-order parsing?, which is shown to improve parsingaccuracy but also increase parsing complexity (Car-reras, 2007; Koo and Collins, 2010).2 Transition-based parsing is attractive because it can use parseinformation without increasing complexity (Nivre,2006).
The qualification is that parse informationprovided by gold-standard trees during training isnot necessarily the same kind of information pro-vided by automatically parsed trees during decod-ing.
This can confuse a statistical model trained onlyon the gold-standard trees.To reduce the gap between gold-standard and au-tomatic parses, we use bootstrapping on automaticparses.
First, we train a statistical model using gold-2Second-order, non-projective, graph-based dependencyparsing is NP-hard without performing approximation.689standard trees.
Then, we parse the training data us-ing the statistical model.
During parsing, we ex-tract features for each parsing state, consisting ofautomatic parse information, and generate a train-ing instance by joining the features with the gold-standard label.
The gold-standard label is achievedby comparing the dependency relation between wiand wj in the gold-standard tree.
When the parsingis done, we train a different model using the traininginstances induced by the previous model.
We repeatthe procedure until a stopping criteria is met.The stopping criteria is determined by performingcross-validation.
For each stage, we perform cross-validation to check if the average parsing accuracyon the current cross-validation set is higher than theone from the previous stage.
We stop the procedurewhen the parsing accuracy on cross-validation setsstarts decreasing.
Our experiments show that thissimple bootstrapping technique gives a significantimprovement to parsing accuracy.4 Related workDaume?
et al (2009) presented an algorithm, calledSEARN, for integrating search and learning to solvecomplex structured prediction problems.
Our boot-strapping technique can be viewed as a simplifiedversion of SEARN.
During training, SEARN itera-tively creates a set of new cost-sensitive examplesusing a known policy.
In our case, the new examplesare instances containing automatic parses inducedby the previous model.
Our technique is simpli-fied because the new examples are not cost-sensitive.Furthermore, SEARN interpolates the current policywith the previous policy whereas we do not per-form such interpolation.
During decoding, SEARNgenerates a sequence of decisions and makes a fi-nal prediction.
In our case, the decisions are pre-dicted dependency relations and the final predictionis a dependency tree.
SEARN has been successfullyadapted to several NLP tasks such as named entityrecognition, syntactic chunking, and POS tagging.To the best of our knowledge, this is the first timethat this idea has been applied to transition-basedparsing and shown promising results.Zhang and Clark (2008) suggested a transition-based projective parsing algorithm that keeps B dif-ferent sequences of parsing states and chooses theone with the best score.
They use beam search andshow a worst-case parsing complexity ofO(n) givena fixed beam size.
Similarly to ours, their learn-ing mechanism using the structured perceptron al-gorithm involves training on automatically derivedparsing states that closely resemble potential statesencountered during decoding.5 Experiments5.1 Corpora and learning algorithmAll models are trained and tested on English andCzech data using automatic lemmas, POS tags,and feats, as distributed by the CoNLL?09 sharedtask (Hajic?
et al, 2009).
We use Liblinear L2-L1SVM for learning (L2 regularization, L1 loss; Hsiehet al (2008)).
For our experiments, we use the fol-lowing learning parameters: c = 0.1 (cost), e = 0.1(termination criterion), B = 0 (bias).5.2 Accuracy comparisonsFirst, we evaluate the impact of the LEFT-POP tran-sition we add to Choi-Nicolov?s approach.
To makea fair comparison, we implemented both approachesand built models using the exact same feature set.The ?CN?
and ?Our?
rows in Table 3 show accuraciesachieved by Choi-Nicolov?s and our approaches, re-spectively.
Our approach shows higher accuraciesfor all categories.
Next, we evaluate the impact ofour bootstrapping technique.
The ?Our+?
row showsaccuracies achieved by our algorithm using the boot-strapping technique.
The improvement from ?Our?to ?Our+?
is statistically significant for all categories(McNemar, p < .0001).
The improvment is evenmore significant in a language like Czech for whichparsers generally perform more poorly.English CzechLAS UAS LAS UASCN 88.54 90.57 78.12 83.29Our 88.62 90.66 78.30 83.47Our+ 89.15?
91.18?
80.24?
85.24?Merlo 88.79 (3) - 80.38 (1) -Bohnet 89.88 (1) - 80.11 (2) -Table 3: Accuracy comparisons between different pars-ing approaches (LAS/UAS: labeled/unlabeled attachmentscore).
?
indicates a statistically significant improvement.
(#) indicates an overall rank of the system in CoNLL?09.690Finally, we compare our work against other state-of-the-art systems.
For the CoNLL?09 shared task, Ges-mundo et al (2009) introduced the best transition-based system using synchronous syntactic-semanticparsing (?Merlo?
), and Bohnet (2009) introduced thebest graph-based system using a maximum span-ning tree algorithm (?Bohnet?).
Our approach showsquite comparable results with these systems.35.3 Speed comparisonsFigure 1 shows average parsing speeds for eachsentence group in both English and Czech eval-uation sets (Table 4).
?Nivre?
is Nivre?s swapalgorithm (Nivre, 2009), of which we use theimplementation from MaltParser (maltparser.org).
The other approaches are implemented inour open source project, called ClearParser (code.google.com/p/clearparser).
Note that fea-tures used in MaltParser have not been optimizedfor these evaluation sets.
All experiments are testedon an Intel Xeon 2.57GHz machine.
For general-ization, we run five trials for each parser, cut offthe top and bottom speeds, and average the middlethree.
The loading times for machine learning mod-els are excluded because they are independent fromthe parsing algorithms.
The average parsing speedsare 2.86, 2.69, and 2.29 (in milliseconds) for Nivre,CN, and Our+, respectively.
Our approach showslinear growth all along, even for the sentence groupswhere some approaches start showing curves.0 10 20 30 40 50 60 702610141822Sentence lengthParsingspeed(inms)Our+CNNivreFigure 1: Average parsing speeds with respect to sentencegroups in Table 4.3Later, ?Merlo?
and ?Bohnet?
introduced more advancedsystems, showing some improvements over their previous ap-proaches (Titov et al, 2009; Bohnet, 2010).< 10 < 20 < 30 < 40 < 50 < 60 < 701,415 2,289 1,714 815 285 72 18Table 4: # of sentences in each group, extracted from bothEnglish/Czech evaluation sets.
?< n?
implies a groupcontaining sentences whose lengths are less than n.We also measured average parsing speeds for ?Our?,which showed a very similar growth to ?Our+?.
Theaverage parsing speed of ?Our?
was 2.20 ms; it per-formed slightly faster than ?Our+?
because it skippedmore nodes by performing more non-deterministicSHIFT?s, which may or may not have been correctdecisions for the corresponding parsing states.It is worth mentioning that the curve shown by?Nivre?
might be caused by implementation detailsregarding feature extraction, which we included aspart of parsing.
To abstract away from these im-plementation details and focus purely on the algo-rithms, we would need to compare the actual num-ber of transitions performed by each parser, whichwill be explored in future work.6 Conclusion and future workWe present two ways of improving transition-based,non-projective dependency parsing.
The additionaltransition gives improvements to both parsing speedand accuracy, showing a linear time parsing speedwith respect to sentence length.
The bootstrappingtechnique gives a significant improvement to parsingaccuracy, showing near state-of-the-art performancewith respect to other parsing approaches.
In the fu-ture, we will test the robustness of these approachesin more languages.AcknowledgmentsWe gratefully acknowledge the support of the Na-tional Science Foundation Grants CISE-IIS-RI-0910992,Richer Representations for Machine Translation, a sub-contract from the Mayo Clinic and Harvard Children?sHospital based on a grant from the ONC, 90TR0002/01,Strategic Health Advanced Research Project Area 4: Nat-ural Language Processing, and a grant from the DefenseAdvanced Research Projects Agency (DARPA/IPTO) un-der the GALE program, DARPA/CMO Contract No.HR0011-06-C-0022, subcontract from BBN, Inc. Anyopinions, findings, and conclusions or recommendationsexpressed in this material are those of the authors and donot necessarily reflect the views of the National ScienceFoundation.691ReferencesBernd Bohnet.
2009.
Efficient parsing of syntactic andsemantic dependency structures.
In Proceedings of the13th Conference on Computational Natural LanguageLearning: Shared Task (CoNLL?09), pages 67?72.Bernd Bohnet.
2010.
Top accuracy and fast depen-dency parsing is not a contradiction.
In The 23rd In-ternational Conference on Computational Linguistics(COLING?10).Xavier Carreras.
2007.
Experiments with a higher-order projective dependency parser.
In Proceedings ofthe CoNLL Shared Task Session of EMNLP-CoNLL?07(CoNLL?07), pages 957?961.Daniel Cer, Marie-Catherine de Marneffe, Daniel Juraf-sky, and Christopher D. Manning.
2010.
Parsingto stanford dependencies: Trade-offs between speedand accuracy.
In Proceedings of the 7th InternationalConference on Language Resources and Evaluation(LREC?10).Jinho D. Choi and Nicolas Nicolov.
2009.
K-best, lo-cally pruned, transition-based dependency parsing us-ing robust risk minimization.
In Recent Advances inNatural Language Processing V, pages 205?216.
JohnBenjamins.Isaac G. Councill, Ryan McDonald, and Leonid Ve-likovich.
2010.
What?s great and what?s not: Learn-ing to classify the scope of negation for improved sen-timent analysis.
In Proceedings of the Workshop onNegation and Speculation in Natural Language Pro-cessing (NeSp-NLP?10), pages 51?59.Hal Daume?, Iii, John Langford, and Daniel Marcu.
2009.Search-based structured prediction.
Machine Learn-ing, 75(3):297?325.Andrea Gesmundo, James Henderson, Paola Merlo, andIvan Titov.
2009.
A latent variable model of syn-chronous syntactic-semantic parsing for multiple lan-guages.
In Proceedings of the 13th Conference onComputational Natural Language Learning: SharedTask (CoNLL?09), pages 37?42.Jan Hajic?, Massimiliano Ciaramita, Richard Johans-son, Daisuke Kawahara, Maria Anto`nia Mart?
?, Llu?
?sMa`rquez, Adam Meyers, Joakim Nivre, SebastianPado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,Nianwen Xue, and Yi Zhang.
2009.
The conll-2009shared task: Syntactic and semantic dependencies inmultiple languages.
In Proceedings of the 13th Con-ference on Computational Natural Language Learning(CoNLL?09): Shared Task, pages 1?18.Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin, S. SathiyaKeerthi, and S. Sundararajan.
2008.
A dual coordinatedescent method for large-scale linear svm.
In Proceed-ings of the 25th international conference on Machinelearning (ICML?08), pages 408?415.Terry Koo and Michael Collins.
2010.
Efficient third-order dependency parsers.
In Proceedings of the 48thAnnual Meeting of the Association for ComputationalLinguistics (ACL?10).Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic?.
2005.
Non-projective dependency pars-ing using spanning tree algorithms.
In Proceedings ofthe Conference on Human Language Technology andEmpirical Methods in Natural Language Processing(HLT-EMNLP?05), pages 523?530.Joakim Nivre and Ryan McDonald.
2008.
Integratinggraph-based and transition-based dependency parsers.In Proceedings of the 46th Annual Meeting of the As-sociation for Computational Linguistics: Human Lan-guage Technologies (ACL:HLT?08), pages 950?958.Joakim Nivre and Jens Nilsson.
2005.
Pseudo-projectivedependency parsing.
In Proceedings of the 43rd An-nual Meeting of the Association for ComputationalLinguistics (ACL?05), pages 99?106.Joakim Nivre.
2003.
An efficient algorithm for pro-jective dependency parsing.
In Proceedings of the8th International Workshop on Parsing Technologies(IWPT?03), pages 23?25.Joakim Nivre.
2006.
Inductive Dependency Parsing.Springer.Joakim Nivre.
2008.
Algorithms for deterministic incre-mental dependency parsing.
Computational Linguis-tics, 34(4):513?553.Joakim Nivre.
2009.
Non-projective dependency parsingin expected linear time.
In Proceedings of the JointConference of the 47th Annual Meeting of the ACL andthe 4th International Joint Conference on Natural Lan-guage Processing of the AFNLP (ACL-IJCNLP?09),pages 351?359.Libin Shen, Jinxi Xu, and Ralph Weischedel.
2008.
Anew string-to-dependency machine translation algo-rithm with a target dependency language model.
InProceedings of the 46th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies (ACL:HLT?08), pages 577?585.Ivan Titov, James Henderson, Paola Merlo, and GabrieleMusillo.
2009.
Online graph planarisation for syn-chronous parsing of semantic and syntactic depen-dencies.
In Proceedings of the 21st InternationalJoint Conference on Artificial Intelligence (IJCAI?09),pages 1562?1567.Yue Zhang and Stephen Clark.
2008.
A tale oftwo parsers: investigating and combining graph-based and transition-based dependency parsing usingbeam-search.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing(EMNLP?08), pages 562?571.692
