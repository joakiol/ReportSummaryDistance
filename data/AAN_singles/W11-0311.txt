Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 87?96,Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational LinguisticsImproving the Impact of Subjectivity Word Sense Disambiguation onContextual Opinion AnalysisCem Akkaya, Janyce Wiebe, Alexander ConradUniversity of PittsburghPittsburgh PA, 15260, USA{cem,wiebe,conrada}@cs.pitt.eduRada MihalceaUniversity of North TexasDenton TX, 76207, USArada@cs.unt.eduAbstractSubjectivity word sense disambiguation(SWSD) is automatically determining whichword instances in a corpus are being used withsubjective senses, and which are being usedwith objective senses.
SWSD has been shownto improve the performance of contextualopinion analysis, but only on a small scale andusing manually developed integration rules.In this paper, we scale up the integration ofSWSD into contextual opinion analysis andstill obtain improvements in performance,by successfully gathering data annotated bynon-expert annotators.
Further, by improvingthe method for integrating SWSD into con-textual opinion analysis, even greater benefitsfrom SWSD are achieved than in previouswork.
We thus more firmly demonstrate thepotential of SWSD to improve contextualopinion analysis.1 IntroductionOften, methods for opinion, sentiment, and sub-jectivity analysis rely on lexicons of subjective(opinion-carrying) words (e.g., (Turney, 2002;Whitelaw et al, 2005; Riloff and Wiebe, 2003; Yuand Hatzivassiloglou, 2003; Kim and Hovy, 2004;Bloom et al, 2007; Andreevskaia and Bergler, 2008;Agarwal et al, 2009)).
Examples of such words arethe following (in bold):(1) He is a disease to every team he has gone to.Converting to SMF is a headache.The concert left me cold.That guy is such a pain.However, even manually developed subjectiv-ity lexicons have significant degrees of subjectivitysense ambiguity (Su and Markert, 2008; Gyamfi etal., 2009).
That is, many clues in these lexicons haveboth subjective and objective senses.
This ambiguityleads to errors in opinion and sentiment analysis, be-cause objective instances represent false hits of sub-jectivity clues.
For example, the following sentencecontains the keywords from (1) used with objectivesenses:(2) Early symptoms of the disease include severeheadaches, red eyes, fevers and cold chills, bodypain, and vomiting.Recently, in (Akkaya et al, 2009), we introducedthe task of subjectivity word sense disambiguation(SWSD), which is to automatically determine whichword instances in a corpus are being used with sub-jective senses, and which are being used with objec-tive senses.
We developed a supervised system forSWSD, and exploited the SWSD output to improvethe performance of multiple contextual opinion anal-ysis tasks.Although the reported results are promising, thereare three obvious shortcomings.
First, we were ableto apply SWSD to contextual opinion analysis onlyon a very small scale, due to a shortage of anno-tated data.
While the experiments show that SWSDimproves contextual opinion analysis, this was onlyon the small amount of opinion-annotated data thatwas in the coverage of our system.
Two questionsarise: is it feasible to obtain greater amounts ofthe needed data, and do SWSD performance im-provements on contextual opinion analysis hold on a87larger scale.
Second, the annotations in (Akkaya etal., 2009) are piggy-backed on SENSEVAL sense-tagged data, which are fine-grained word sense an-notations created by trained annotators.
A concernis that SWSD performance improvements on con-textual opinion analysis can only be achieved usingsuch fine-grained expert annotations, the availabilityof which is limited.
Third, (Akkaya et al, 2009) usesmanual rules to apply SWSD to contextual opinionanalysis.
Although these rules have the advantagethat they transparently show the effects of SWSD,they are somewhat ad hoc.
Likely, they are not opti-mal and are holding back the potential of SWSD toimprove contextual opinion analysis.To address these shortcomings, in this paper, weinvestigate (1) the feasibility of obtaining a substan-tial amount of annotated data, (2) whether perfor-mance improvements on contextual opinion analy-sis can be realized on a larger scale, and (3) whetherthose improvements can be realized with subjectiv-ity sense tagged data that is not built on expert full-inventory sense annotations.
In addition, we explorebetter methods for applying SWSD to contextualopinion analysis.2 Subjectivity Word Sense Disambiguation2.1 Annotation TasksWe adopt the definitions of subjective (S) and ob-jective (O) from (Wiebe et al, 2005; Wiebe and Mi-halcea, 2006; Wilson, 2007).
Subjective expressionsare words and phrases being used to express mentaland emotional states, such as speculations, evalua-tions, sentiments, and beliefs.
A general coveringterm for such states is private state (Quirk et al,1985), an internal state that cannot be directly ob-served or verified by others.
Objective expressionsinstead are words and phrases that lack subjectivity.The contextual opinion analysis experiments de-scribed in Section 3 include both S/O and polar-ity (positive,negative, neutral) classifications.
Theopinion-annotated data used in those experiments isfrom the MPQA Corpus (Wiebe et al, 2005; Wilson,2007),1 which consists of news articles annotated forsubjective expressions, including polarity.1Available at http://www.cs.pitt.edu/mpqa2.1.1 Subjectivity Sense LabelingFor SWSD, we need the notions of subjectiveand objective senses of words in a dictionary.
Weadopt the definitions from (Wiebe and Mihalcea,2006), who describe the annotation scheme as fol-lows.
Classifying a sense as S means that, whenthe sense is used in a text or conversation, one ex-pects it to express subjectivity, and also that thephrase or sentence containing it expresses subjectiv-ity.
As noted in (Wiebe and Mihalcea, 2006), sen-tences containing objective senses may not be objec-tive.
Thus, objective senses are defined as follows:Classifying a sense as O means that, when the senseis used in a text or conversation, one does not expectit to express subjectivity and, if the phrase or sen-tence containing it is subjective, the subjectivity isdue to something else.Both (Wiebe and Mihalcea, 2006) and (Su andMarkert, 2008) performed agreement studies of thescheme and report that good agreement can beachieved between human annotators labeling thesubjectivity of senses (?
values of 0.74 and 0.79, re-spectively).
(Akkaya et al, 2009) followed the same annota-tion scheme to annotate the senses of the words usedin the experiments.
For this paper, we again usethe same scheme and annotate WordNet senses of90 new words (the process of selecting the words isdescribed in Section 2.4).2.1.2 Subjectivity Sense TaggingThe training and test data for SWSD consists ofword instances in a corpus labeled as S or O, in-dicating whether they are used with a subjective orobjective sense.Because there was no such tagged data at the time,(Akkaya et al, 2009) created a data set by com-bining two types of sense annotations: (1) labels ofsenses within a dictionary as S or O (i.e., the subjec-tivity sense labels of the previous section), and (2)sense tags of word instances in a corpus (i.e., SEN-SEVAL sense-tagged data).2 The subjectivity senselabels were used to collapse the sense labels in thesense-tagged data into the two new senses, S and O.The target words (Akkaya et al, 2009) chose are thewords tagged in SENSEVAL that are also members2Please see the paper for details on the SENSEVAL dataused in the experiments.88Sense Set1 (Subjective){ attack, round, assail, lash out, snipe, assault } ?
attack inspeech or writing; ?The editors attacked the House Speaker?
{ assail, assault, set on, attack } ?
attack someone emotionally;?Nightmares assailed him regularly?Sense Set2 (Objective){ attack } ?
begin to injure; ?The cancer cells are attacking hisliver?
; ?Rust is attacking the metal?
{ attack, aggress } ?
take the initiative and go on the offensive;?The visiting team started to attack?Figure 1: Sense sets for target word ?attack?
(abridged).of the subjectivity lexicon of (Wilson et al, 2005;Wilson, 2007).3 There are 39 such words.
(Akkayaet al, 2009) chose words from a subjectivity lexiconbecause such words are known to have subjectiveusages.For this paper, subjectivity sense-tagged data wasobtained from the MTurk workers using the anno-tation scheme of (Akkaya et al, 2010).
A goal is tokeep the annotation task as simple as possible.
Thus,the workers are not directly asked if the instance ofa target word has a subjective or an objective sense,because the concept of subjectivity would be diffi-cult to explain in this setting.
Instead the workersare shown two sets of senses ?
one subjective set andone objective set ?
for a specific target word and atext passage in which the target word appears.
Theirjob is to select the set that best reflects the meaningof the target word in the text passage.
The set theychoose gives us the subjectivity label of the instance.A sample annotation task is shown below.
AnMTurk worker has access to two sense sets of thetarget word ?attack?
as seen in Figure 1.
The S andO labels appear here only for the purpose of this pa-per; the workers do not see them.
The worker is pre-sented with the following text passage holding thetarget word ?attack?
:Ivkovic had been a target of intra-partyfeuding that has shaken the party.
He wasattacked by Milosevic for attempting tocarve out a new party from the Socialists.In this passage, the use of ?attack?
is most similarto the first entry in sense set one; thus, the correctanswer for this problem is Sense Set-1.3Available at http://www.cs.pitt.edu/mpqa(Akkaya et al, 2010) carried out a pilot studywhere a subjectivity sense-tagged dataset was cre-ated for eight SENSEVAL words through MTurk.
(Akkaya et al, 2010) evaluated the non-expert la-bel quality against gold-standard expert labels whichwere obtained from (Akkaya et al, 2009) relyingon SENSEVAL.
The non-expert annotations are reli-able, achieving ?
scores around 0.74 with the expertannotations.For some words, there may not be a clean split be-tween the subjective and objective senses.
For these,we opted for another strategy for obtaining MTurkannotations.
Rather than presenting the workerswith WordNet senses, we show them a set of objec-tive usages, a set of subjective usages, and a text pas-sage in which the target word appears.
The workers?job is to judge which set of usages the target instanceis most similar to.2.2 SWSD SystemWe follow the same approach as in (Akkaya et al,2009) to build our SWSD system.
We train a differ-ent supervised SWSD classifier for each target wordseparately.
This means the overall SWSD systemconsists of as many SWSD classifiers as there aretarget words.
We utilize the same machine learningfeatures as in (Akkaya et al, 2009), which are com-monly used in Word Sense Disambiguation (WSD).2.3 Expert SWSD vs. Non-expert SWSDBefore creating a large subjectivity sense-taggedcorpus via MTurk, we want to make sure that non-expert annotations are good enough to train reliableSWSD classifiers.
Thus, we decided to comparethe performance of a SWSD system trained on non-expert annotations and on expert annotations.
Forthis purpose, we need a subjectivity sense-taggedcorpus where word instances are tagged both by ex-pert and non-expert annotations.
Fortunately, wehave such a corpus.
As discussed in Section 3,(Akkaya et al, 2009) created a subjecvitivity sense-tagged corpus piggybacked on SENSEVAL.
Thisgives us a gold-standard corpus tagged by experts.There is also a small subjectivity sense-tagged cor-pus consisting of eight target words obtained fromnon-expert annotators in (Akkaya et al, 2010).
Thiscorpus is a subset of the gold-standard corpus from(Akkaya et al, 2009) and it consists of 60 tagged89Acc p-valueSWSDGOLD 79.2 -SWSDMJL 78.4 0.542SWSDMJC 78.8 0.754Table 1: Comparison of SWSD systemsinstances for each target word.Actually, (Akkaya et al, 2010) gathered three la-bels for each instance.
This gives us two optionsto train the non-expert SWSD system: (1) trainingthe system on the majority vote labels (SWSDMJL)(2) training three systems on the three separate la-bel sets and taking the majority vote prediction(SWSDMJC).
Additionally, we train an expert SWSDsystem (SWSDGOLD) ?
a system trained on goldstandard expert annotations.
All these systems aretrained on 60 instances of the eight target words forwhich we have both non-expert and expert annota-tions and are evaluated on the remaining instancesof the gold-standard corpus.
This makes a total of923 test instances for the eight target words with amajority class baseline of 61.8.Table 1 reports micro-average accuracy of eachsystem and the two-tailed p-value between the ex-pert SWSD system and the two non-expert SWSDsystems.
The p-value is calculated with McNemar?stest.
It shows that there is no statistically signif-icant difference between classifiers trained on ex-pert gold-standard annotations and non-expert anno-tations.
We adopt SWSDMJL in all our following ex-periments, because it is more efficient.2.4 Corpus CreationFor our experiments, we have multiple goals, whicheffect our decisions on how to create the subjectiv-ity sense-tagged corpus via MTurk.
First, we wantto be able to disambiguate more target words than(Akkaya et al, 2009).
This way, SWSD will be ableto disambiguate a larger portion of the MPQA Cor-pus allowing us to evaluate the effect of SWSD oncontextual opinion analysis on a larger scale.
Thiswill also allow us to investigate additional integra-tion methods of SWSD into contextual opinion anal-ysis rather than simple ad hoc manual rules utilizedin (Akkaya et al, 2009).
Second, we want to showthat we can rely on non-expert annotations instead ofexpert annotations, which will make an annotationeffort on a larger-scale both practical and feasible,timewise and costwise.
Optimally, we could haveannotated via MTurk the same subjectivity sense-tagged corpus from (Akkaya et al, 2009) in order tocompare the effect of a non-expert SWSD system oncontextual opinion analysis directly with the resultsreported for an expert SWSD system in (Akkaya etal., 2009).
But, this would have diverted our re-sources to reproduce the same corpus and contradictour goal to extend the subjectivity sense-tagged cor-pus to new target words.
Moreover, we have alreadyshown in Section 2.3 that non-expert annotations canbe utilized to train reliable SWSD classifiers.
It isreasonable to believe that similar performance onthe SWSD task will reflect to similar improvementson contextual opinion analysis.
Thus, we decidedto prioritize creating a subjectivity sense-tagged cor-pus for a totally new set of words.
We aim to showthat the favourable results reported in (Akkaya et al,2009) will still hold on new target words relying onnon-expert annotations.We chose our target words from the subjectivitylexicon of (Wilson et al, 2005), because we knowthey have subjective usages.
The contextual opin-ion systems we want to improve rely on this lexicon.We call the words in the lexicon subjectivity clues.At this stage, we want to concentrate on the fre-quent and ambiguous subjectivity clues.
We chosefrequent ones, because they will have larger cov-erage in the MPQA Corpus.
We chose ambiguousones, because these clues are the ones that are mostimportant for SWSD.
Choosing most frequent andambiguous subjectivity clues guarantees that we uti-lize our limited resources in the most efficient way.We judge a clue to be ambiguous if it appears morethan 25% and less than 75% of the times in a sub-jective expression.
We get these statistics by simplycounting occurrences in the MPQA Corpus insideand outside of subjective expressions.There are 680 subjectivity clues that appear in theMPQA Corpus and are ambiguous.
Out of those, weselected the 90 most frequent that have to some ex-tent distinct objective and subjective senses in Word-Net, as judged by the co-authors.
The co-authors an-notated the WordNet senses of those 90 target words.For each target word, we selected approximately 120instances randomly from the GIGAWORD Corpus.In a first phase, we collected three sets of MTurk an-90notations for the selected instances.
In this phase,MTurk workers base their judgements on two sensesets they observe.
This way, we get training data tobuild SWSD classifiers for these 90 target words.The quality of these classifiers is important, be-cause we will exploit them for contextual opinionanalysis.
Thus, we evaluate them by 10-fold cross-validation.
We split the target words into threegroups.
If the majority class baseline of a word ishigher than 90%, it is considered as skewed (skewedwords have a performance at least as good as the ma-jority class baseline).
If a target word improves overits majority class baseline by 25% in accuracy, it isconsidered as good.
Otherwise, it is considered asmediocre.
This way, we end up with 24 skewed, 35good, and 31 mediocre words.
There are many pos-sible reasons for the less reliable performance forthe mediocre group.
We hypothesize that a majorproblem is the similarity between the objective andsubjective sense sets of a word, thus leading to poorannotation quality.
To check this, we calculate theagreement between three annotation sets and reportaverages.
The agreement in the mediocre group is78.68%, with a ?
value of 0.57, whereas the aver-age agreement in the good group is 87.51%, witha ?
value of 0.75.
These findings support our hy-pothesis.
Thus, the co-authors created usage inven-tories for the words in the mediocre group as de-scribed in Section 2.1.1.
We initiated a second phaseof MTurk annotations.
We collect for the mediocregroup another three sets of MTurk annotations for120 instances, this time utilizing usage inventories.The 10-fold cross-validation experiments show thatnine of the 31 words in the mediocre group shift tothe good group.
Only for these nine words, we ac-cept the annotations collected via usage inventories.For all other words, we use the annotations collectedvia sense inventories.
From now on, we will referto this non-expert subjectivity sense-tagged corpusconsisting of the tagged data for all 90 target wordsas the MTurkSWSD Corpus (agreement on the entireMTurkSWSD corpus is 85.54%, ?
:0.71).3 SWSD IntegrationNow that we have the MTurkSWSD Corpus, weare ready to evaluate the effect of SWSD on con-textual opinion analysis.
In this section, we ap-ply our SWSD system trained on MTurkSWSD toboth expression-level classifiers from (Akkaya et al,2009): (1) the subjective/objective (S/O) classifierand (2) the contextual polarity classifier.
Both clas-sifiers are introduced in Section 3.1Our SWSD system can disambiguate 90 targetwords, which have 3737 instances in the MPQACorpus.
We refer to this subset of the MPQA Corpusas MTurkMPQA.
This subset makes up the cover-age of our SWSD system.
Note that MTurkMPQAis 5.2 times larger than the covered MPQA subsetin (Akkaya et al, 2009) referred as senMPQA.
Wetry different strategies to integrate SWSD into thecontextual classifiers.
In Section 3.2, we follow thesame rule-based strategy as in (Akkaya et al, 2009)for completeness.
In Section 3.3, we introduce twonew learning strategies for SWSD integration out-performing existing rule-based strategy.
We evalu-ate the improvement gained by SWSD on MTurkM-PQA.3.1 Contextual ClassifiersThe original contextual polarity classifier is intro-duced in (Wilson et al, 2005).
We use the same im-plementation as in (Akkaya et al, 2009).
This classi-fier labels clue instances in text as contextually neg-ative/positive/neutral.
The gold standard is definedon the MPQA Corpus as follows.
If a clue instanceappears in a positive expression, it is contextuallypositive (Ps).
If it appears in a negative expression,it is contextually negative (Ng).
If it is in an objec-tive expression or in a neutral subjective expression,it is contextually neutral (N).
The contextual polar-ity classifier consists of two separate steps.
The firststep is an expression-level neutral/polar (N/P) clas-sifier.
The second step classifies only polar instancesfurther into positive and negative classes.
This way,the overall system performs a three-way classifica-tion (Ng/Ps/N).The subjective/objective classifier is introduced in(Akkaya et al, 2009).
It relies on the same machinelearning features as the N/P classifier (i.e.
the firststep of the contextual polarity classifier).
The onlydifference is that the classes are S/O instead of N/P.The gold standard is defined on the MPQA Corpusin the following way.
If a clue instance appears ina subjective expression, it is contextually S. If it ap-pears in an objective expression, it is contextually O.Both contextual classifiers are supervised.91Baseline Acc OF SFMTurkMPQA 52.4% (O)OS/O 67.1 68.9 65.0R1R2 71.1 72.7 69.2senMPQA 63.1% (O)OS/O 75.4 65.4 80.9R1R2 81.3 75.9 84.8Table 2: S/O classifier with and without SWSD.3.2 Rule-Based SWSD Integration(Akkaya et al, 2009) integrates SWSD into a con-textual classifier by simple rules.
The rules flip theoutput of the contextual classifier if some conditionshold.
They make use of following information: (1)SWSD output, (2) the contextual classifier?s confi-dence and (3) the presence of another subjectivityclue ?
any clue from the subjectivity lexicon ?
in thesame expression.For the contextual S/O classifier, (Akkaya et al,2009) defines two rules: one flipping the S/O classi-fier?s output from O to S (R1) and one flipping fromS to O (R2).
R1 is defined as follows : if the contex-tual classifier decides a target word instance is con-textually O and SWSD decides that it is used in a Ssense, then SWSD overrules the contextual S/O clas-sifier?s output and flips it from O to S, because aninstance in a S sense will make the surrounding ex-pression subjective.
R2 is a little bit more complex.It is defined as follows: If the contextual classifier la-bels a clue instance as S but (1) SWSD decides thatit is used in an O sense, (2) the contextual classifier?sconfidence is low, and (3) there is no other subjec-tivity clue in the same expression, then R2 flips thecontextual classifier?s output from S to O.
The ra-tionale behind R2 is that even if the target word in-stance has an O sense, there might be another reason(e.g.
the presence of another subjectivity clue in thesame expression) for the expression enclosing it tobe subjective.We use the exact same rules and adopt the sameconfidence threshold.
Table 2 holds the comparisonof the original contextual classifier and the classi-fier with SWSD support on senMPQA as reported in(Akkaya et al, 2009) and on MTurkMPQA.
OS/O isthe original S/O classifier; R1R2 is the system withSWSD support utilizing both rules.
We report onlyR1R2, since (Akkaya et al, 2009) gets highest im-provement utilizing both rules.Baseline Acc NF PFMTurkMPQA 70.6% (P)ON/P 72.3 82.0 39.8R4 74.5 84.0 37.8senMPQA 73.9% (P)ON/P 79.0 86.7 50.3R4 81.6 88.6 52.3Table 3: N/P classifier with and without SWSDIn Table 2 we see that R1R2 achieves 4% percent-age points improvement in accuracy over OS/O onMTurkMPQA.
The improvement is statistically sig-nificant at the p < .01 level with McNemar?s test.
Itis accompanied with improvements both in subjec-tive F-measure (SF) and objective F-measure (OF).It is not possible to directly compare improvementson senMPQA and MTurkMPQA since they are dif-ferent subsets of the MPQA Corpus.
SWSD supportbrings 24% error reduction on senMPQA over theoriginal S/O classifier.
In comparison, on MTurkM-PQA, the error reduction is 12%.
We see that the im-provements on the large MTurkMPQA set still hold,but not as strong as in (Akkaya et al, 2009).
(Akkaya et al, 2009) uses a similar rule tomake the contextual polarity classifier sense-aware.Specifically, the rule is applied to the output of thefirst step (N/P classifier).
The rule, R4, flips P to Nand is analogous to R2.
If the contextual classifierlabels a clue instance as P but (1) SWSD decidesthat it is used in an O sense, (2) the contextual clas-sifier?s confidence is low, and (3) there is no otherclue instance in the same expression, then R4 flipsthe contextual classifier?s output from P to N.Table 3 holds the comparison of the original N/Pclassifier with and without SWSD support on sen-MPQA as reported in (Akkaya et al, 2009) and onMTurkMPQA.
ON/P is the original N/P classifier; R4is the system with SWSD support utilizing rule R4.Since our main focus is not rule-based integration,we did not run the second step of the polarity classi-fier.
We report the second step result below for thelearning-based SWSD integration in section 3.4.In Table 3, we see that R4 achieves 2.2 percent-age points improvement in accuracy over ON/P onMTurkMPQA.
The improvement is statistically sig-nificant at the p < .01 level with McNemar?s test.It is accompanied with improvement only in objec-tive F-measure (OF).
SWSD support brings 12.4%error reduction on senMPQA (Akkaya et al, 2009).92On MTurkMPQA, the error reduction is 8%.
We seethat the rule-based SWSD integration still improvesboth contextual classifiers on MTurkMPQA, but thegain is not as large as on senMPQA.
This might bedue to the brittleness of the rule-based integration.3.3 Learning SWSD IntegrationNow that we can disambiguate a larger portion ofthe MPQA Corpus than in (Akkaya et al, 2009),we can investigate machine learning methods forSWSD integration to deal with the brittleness of therule-based integration.
In this section, we introducetwo learning methods to apply SWSD to the contex-tual classifiers.
For the learning methods, we rely onexactly the same information as the rule-based inte-gration: (1) SWSD output, (2) the contextual clas-sifier?s output, (3) the contextual classifier?s confi-dence, and (4) the presence of another clue instancein the same expression.
The rationale is the same asfor the rule-based integration, namely to relate sensesubjectivity and contextual subjectivity.3.3.1 Method1In the first method, we extend the machine learn-ing features of the underlying contextual classifiersby adding (1) and (4) from above.
We evaluate theextended contextual classifiers on MTurkMPQA via10-fold cross-validation.
Tables 4 and 5 hold thecomparison of Method1 (EXTS/O, EXTN/P) to theoriginal contextual classifiers (OS/O, ON/P) and to therule-based SWSD integration (R1R2, R4).
We seesubstantial improvement for Method1.
It achieves39% error reduction over OS/O and 25% error reduc-tion over ON/P.
For both classifiers, the improvementin accuracy over the rule-based integration is statisti-cally significant at the p< .01 level with McNemar?stest.3.3.2 Method2This method defines a third classifier that acceptsas input the contextual classifier?s output and theSWSD output and predicts what the contextual clas-sifier?s output should have been.
We can think ofthis third classifier as the learning counterpart ofthe manual rules from Section 3.2, since it actu-ally learns when to flip the contextual classifier?soutput considering SWSD evidence.
Specifically,this merger classifier relies on four machine learn-ing features (1), (2), (3), (4) from above (the ex-Acc OF SFOS/O 67.1 68.9 65.0R1R2 71.1 72.7 69.2EXTS/O 80.0 81.4 78.3MERGERS/O 78.2 80.3 75.5Table 4: S/O classifier with learned SWSD integrationAcc NF PFON/P 72.3 82.0 39.8R4 74.5 84.0 37.8EXTN/P 79.1 85.7 61.1MERGERN/P 80.4 86.7 62.8Table 5: N/P classifier with learned SWSD integrationact same information used in rule-based integration).Because it is a supervised classifier, we need train-ing data where we have clue instances with cor-responding contextual classifier and SWSD predic-tions.
Fortunately, we can use senMPQA for thispurpose.
We train our merger classifier on senM-PQA (we get contextual classifier predictions via 10-fold cross-validation on the MPQA Corpus) and ap-ply it to MTurkMPQA.
We use SVM classifier fromthe Weka package (Witten and Frank., 2005) withits default settings.
Tables 4 and 5 hold the com-parison of Method2 (MERGERS/O, MERGERN/P) tothe original contextual classifiers (Oo/s, ON/P) andthe rule-based SWSD integration (R1R2, R4).
Itachieves 29% error reduction over OS/O and 29% er-ror reduction over ON/P.
The improvement on therule-based integration is statistically significant atthe p < .01 level with McNemar?s test.
Method2performs better (statistically significant at the p <.05 level) than Method1 for the N/P classifier butworse (statistically significant at the p < .01 level)for the S/O classifier.3.4 Improving Contextual PolarityClassificationWe have seen that Method2 is the best method toimprove the N/P classifier, which is the first stepof the contextual polarity classifier.
To assess theoverall improvement in polarity classification, werun the second step of the contextual polarity clas-sifier after correcting the first step with Method2.Table 6 summarizes the improvement propagated to93Acc NF NgF PsFMTurkMPQAOPs/Ng/N 72.1 83.0 34.2 15.0MERGERN/P 77.8 87.4 53.0 27.7senMPQAOPs/Ng/N 77.6 87.2 39.5 40.0R4 80.6 89.1 43.2 44.0Table 6: Polarity classifier with and without SWSD.Ps/Ng/N classification.
For comparison, we alsoinclude results from (Akkaya et al, 2009) on sen-MPQA.
Method2 results in 20% error reduction inaccuracy over OPs/Ng/N (R4 achieves 13.4% errorreduction on senMPQA).
The improvement on therule-based integration is statistically significant atthe p < .01 level with McNemar?s test.
More im-portantly, the F-measure for all the labels improves.This indicates that non-expert MTurk annotationscan replace expert annotations for our end-goal ?
im-proving contextual opinion analysis ?
while reduc-ing time and cost requirements by a large margin.Moreover, we see that the improvements in (Akkayaet al, 2009) scale up to new subjectivity clues.4 Related WorkOne related line of research is to automaticallyassign subjectivity and/or polarity labels to wordsenses in a dictionary (Valitutti et al, 2004; An-dreevskaia and Bergler, 2006; Wiebe and Mihalcea,2006; Esuli and Sebastiani, 2007; Su and Markert,2009).
In contrast, the task in our paper is to auto-matically assign labels to word instances in a corpus.Recently, some researchers have exploited fullword sense disambiguation in methods for opinion-related tasks.
For example, (Mart?
?n-Wanton et al,2010) exploit WSD for recognizing quotation polar-ities, and (Rentoumi et al, 2009; Mart?
?n-Wanton etal., 2010) exploit WSD for recognizing headline po-larities.
None of this previous work investigates per-forming a coarse-grained variation of WSD such asSWSD to improve their application results, as we doin this work.A notable exception is (Su and Markert, 2010),who exploit SWSD to improve the performance ona contextual NLP task, as we do.
While the taskin our paper is subjectivity and sentiment analy-sis, their task is English-Chinese lexical substitu-tion.
As (Akkaya et al, 2009) did, they anno-tated word senses, and exploited SENSEVAL dataas training data for SWSD.
They did not directly an-notate words in context with S/O labels, as we do inour work.
Further, they did not separately evaluate aSWSD system component.Many researchers work on reducing the granular-ity of sense inventories for WSD (e.g., (Palmer et al,2004; Navigli, 2006; Snow et al, 2007; Hovy et al,2006)).
Their criteria for grouping senses are syn-tactic and semantic similarities, while the groupingsin work on SWSD are driven by the goals to improvecontextual subjectivity and sentiment analysis.5 Conclusions and Future WorkIn this paper, we utilized a large pool of non-expertannotators (MTurk) to collect subjectivity sense-tagged data for SWSD.
We showed that non-expertannotations are as good as expert annotations fortraining SWSD classifiers.
Moreover, we demon-strated that SWSD classifiers trained on non-expertannotations can be exploited to improve contextualopinion analysis.The additional subjectivity sense-tagged data en-abled us to evaluate the benefits of SWSD on con-textual opinion analysis on a corpus of opinion-annotated data that is five times larger.
Using thesame rule-based integration strategies as in (Akkayaet al, 2009), we found that contextual opinion anal-ysis is improved by SWSD on the larger datasets.We also experimented with new learning strategiesfor integrating SWSD into contextual opinion analy-sis.
With the learning strategies, we achieved greaterbenefits from SWSD than the rule-based integrationstrategies on all of the contextual opinion analysistasks.Overall, we more firmly demonstrated the poten-tial of SWSD to improve contextual opinion analy-sis.
We will continue to gather subjectivity sense-tagged data, using sense inventories for words thatare well represented in WordNet for our purposes,and with usage inventories for those that are not.6 AcknowledgmentsThis material is based in part upon work supportedby National Science Foundation awards #0917170and #0916046.94ReferencesApoorv Agarwal, Fadi Biadsy, and Kathleen Mckeown.2009.
Contextual phrase-level polarity analysis us-ing lexical affect scoring and syntactic N-grams.
InProceedings of the 12th Conference of the EuropeanChapter of the ACL (EACL 2009), pages 24?32.
Asso-ciation for Computational Linguistics.Cem Akkaya, Janyce Wiebe, and Rada Mihalcea.
2009.Subjectivity word sense disambiguation.
In Proceed-ings of the 2009 Conference on Empirical Methods inNatural Language Processing, pages 190?199, Singa-pore, August.
Association for Computational Linguis-tics.Cem Akkaya, Alexander Conrad, Janyce Wiebe, andRada Mihalcea.
2010.
Amazon mechanical turk forsubjectivity word sense disambiguation.
In Proceed-ings of the NAACL HLT 2010 Workshop on CreatingSpeech and Language Data with Amazon?s Mechani-cal Turk, pages 195?203, Los Angeles, June.
Associa-tion for Computational Linguistics.Alina Andreevskaia and Sabine Bergler.
2006.
Miningwordnet for a fuzzy sentiment: Sentiment tag extrac-tion from wordnet glosses.
In Proceedings of the 11rdConference of the European Chapter of the Associa-tion for Computational Linguistics (EACL-2006).Alina Andreevskaia and Sabine Bergler.
2008.
Whenspecialists and generalists work together: Overcom-ing domain dependence in sentiment tagging.
In Pro-ceedings of ACL-08: HLT, pages 290?298, Columbus,Ohio, June.
Association for Computational Linguis-tics.Kenneth Bloom, Navendu Garg, and Shlomo Argamon.2007.
Extracting appraisal expressions.
In HLT-NAACL 2007, pages 308?315, Rochester, NY.Andrea Esuli and Fabrizio Sebastiani.
2007.
Pagerank-ing wordnet synsets: An application to opinion min-ing.
In Proceedings of the 45th Annual Meeting ofthe Association of Computational Linguistics, pages424?431, Prague, Czech Republic, June.
Associationfor Computational Linguistics.Yaw Gyamfi, Janyce Wiebe, Rada Mihalcea, and CemAkkaya.
2009.
Integrating knowledge for subjectiv-ity sense labeling.
In Proceedings of Human Lan-guage Technologies: The 2009 Annual Conference ofthe North American Chapter of the Association forComputational Linguistics (NAACL-HLT 2009), pages10?18, Boulder, Colorado, June.
Association for Com-putational Linguistics.E.
Hovy, M. Marcus, M. Palmer, L. Ramshaw, andR.
Weischedel.
2006.
Ontonotes: The 90% solution.In Proceedings of the Human Language TechnologyConference of the NAACL, Companion Volume: ShortPapers, New York City.Soo-Min Kim and Eduard Hovy.
2004.
Determining thesentiment of opinions.
In Proceedings of the Twen-tieth International Conference on Computational Lin-guistics (COLING 2004), pages 1267?1373, Geneva,Switzerland.Tamara Mart?
?n-Wanton, Aurora Pons-Porrata, Andre?sMontoyo-Guijarro, and Alexandra Balahur.
2010.Opinion polarity detection - using word sense disam-biguation to determine the polarity of opinions.
InICAART 2010 - Proceedings of the International Con-ference on Agents and Artificial Intelligence, Volume1, pages 483?486.R.
Navigli.
2006.
Meaningful clustering of senses helpsboost word sense disambiguation performance.
InProceedings of the Annual Meeting of the Associationfor Computational Linguistics, Sydney, Australia.M.
Palmer, O. Babko-Malaya, and H. T. Dang.
2004.Different sense granularities for different applications.In HLT-NAACL 2004 Workshop: 2nd Workshop onScalable Natural Language Understanding, Boston,Massachusetts.Randolph Quirk, Sidney Greenbaum, Geoffry Leech, andJan Svartvik.
1985.
A Comprehensive Grammar of theEnglish Language.
Longman, New York.Vassiliki Rentoumi, George Giannakopoulos, VangelisKarkaletsis, and George A. Vouros.
2009.
Sentimentanalysis of figurative language using a word sensedisambiguation approach.
In Proceedings of the In-ternational Conference RANLP-2009, pages 370?375,Borovets, Bulgaria, September.
Association for Com-putational Linguistics.Ellen Riloff and Janyce Wiebe.
2003.
Learning extrac-tion patterns for subjective expressions.
In Proceed-ings of the Conference on Empirical Methods in Natu-ral Language Processing (EMNLP-2003), pages 105?112, Sapporo, Japan.R.
Snow, S. Prakash, D. Jurafsky, and A. Ng.
2007.Learning to merge word senses.
In Proceedings ofthe Joint Conference on Empirical Methods in Natu-ral Language Processing and Computational NaturalLanguage Learning (EMNLP-CoNLL), Prague, CzechRepublic.Fangzhong Su and Katja Markert.
2008.
From wordto sense: a case study of subjectivity recognition.
InProceedings of the 22nd International Conference onComputational Linguistics (COLING-2008), Manch-ester.Fangzhong Su and Katja Markert.
2009.
Subjectivityrecognition on word senses via semi-supervised min-cuts.
In Proceedings of Human Language Technolo-gies: The 2009 Annual Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics, pages 1?9, Boulder, Colorado, June.
Associ-ation for Computational Linguistics.95Fangzhong Su and Katja Markert.
2010.
Word sensesubjectivity for cross-lingual lexical substitution.
InHuman Language Technologies: The 2010 AnnualConference of the North American Chapter of the As-sociation for Computational Linguistics, pages 357?360, Los Angeles, California, June.
Association forComputational Linguistics.Peter Turney.
2002.
Thumbs up or thumbs down?
Se-mantic orientation applied to unsupervised classifica-tion of reviews.
In Proceedings of the 40th AnnualMeeting of the Association for Computational Linguis-tics (ACL-02), pages 417?424, Philadelphia, Pennsyl-vania.Alessandro Valitutti, Carlo Strapparava, and OlivieroStock.
2004.
Developing affective lexical resources.PsychNology, 2(1):61?83.Casey Whitelaw, Navendu Garg, and Shlomo Argamon.2005.
Using appraisal taxonomies for sentiment anal-ysis.
In Proceedings of CIKM-05, the ACM SIGIRConference on Information and Knowledge Manage-ment, Bremen, DE.Janyce Wiebe and Rada Mihalcea.
2006.
Word senseand subjectivity.
In Proceedings of the 21st Interna-tional Conference on Computational Linguistics and44th Annual Meeting of the Association for Compu-tational Linguistics, pages 1065?1072, Sydney, Aus-tralia, July.
Association for Computational Linguistics.Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005.Annotating expressions of opinions and emotionsin language.
Language Resources and Evaluation,39(2/3):164?210.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-level sentiment analysis.
In Proceedings of the Hu-man Language Technologies Conference/Conferenceon Empirical Methods in Natural Language Process-ing (HLT/EMNLP-2005), pages 347?354, Vancouver,Canada.Theresa Wilson.
2007.
Fine-grained Subjectivity andSentiment Analysis: Recognizing the Intensity, Polar-ity, and Attitudes of private states.
Ph.D. thesis, Intel-ligent Systems Program, University of Pittsburgh.I.
Witten and E. Frank.
2005.
Data Mining: PracticalMachine Learning Tools and Techniques, Second Edi-tion.
Morgan Kaufmann, June.Hong Yu and Vasileios Hatzivassiloglou.
2003.
Towardsanswering opinion questions: Separating facts fromopinions and identifying the polarity of opinion sen-tences.
In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing (EMNLP-2003), pages 129?136, Sapporo, Japan.96
