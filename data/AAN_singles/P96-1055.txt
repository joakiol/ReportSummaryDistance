The Select ion of the Most  Probable Dependency  Structure inJapanese Using Mutual  InformationEduardo  de Pa iva  A lvesUniversity of Electro-Connnunications1-5-1 Chofugaoka Chofushi Tokyo Japanealves@phaeton.cs.uec.ac.jpAbst rac tWe use a statistical method to select themost probable structure or parse for a givensentence.
It takes as input the dependencystructures generated for the sentence bya dependency grammar, finds all triple ofmodifier, particle and modificant relations,calculates mutual information of each re-lation and chooses the structure for whichthe product of the mutual information ofits relations is the highest.1 IntroductionComputer Aided Instruction (CAI) systems are im-portant and effective tools, especially for teachingforeign languages.
Many students of Japanese as aforeign language are aware of the Computer AssistedTEchnical Reading System (CATERS) that provideshelpful information for reading texts in science andtechnology fields (Kano a~ld Yamamoto, 1995).One of the difficulties in learning Japanese lies inrecognizing dependency relations in Japallese sen-tences.
This is because the language allows relativelyfree word orders.
Take an example from a leadingnewspaper:We would like to expect a prompt  study of the causes, basedon a national investigationTo understand this sentence it is necessa~'y toknow that ~1:~=~= (investigation) modifies ~o ~ -'3~,~$: (based) but not ~L~.=~ ~ (expect); ~0 ~"3~fa(based) modifies ~\ ]  (study) but not ~,~ (cause).CATERS is useful because it provides such infor-mation through several user-friendly functions.As effective as it is for foreign students, however,the texts in CATERS are fixed and the dependencystructure of every sentence in them is all hand-coded.This inability to handle new text poses a seriousproblem in its general applicability and extensibility.This paper describes a method for selecting theright or most probable structure for a Japanese sen-tence among multiple probable structures generatedby Restricted Dependency Grammar (RDG) (Fuku-moto, 1992).
If this method works, then its resultswill be quite valuable for facilitating the develop-ment of new texts for CAI systems like CATERS.2 BackgroundAs pointed out earlier, the dependency relation ofelements in Japanese sentences are fairly compli-cated due to relatively free word orders.
RDG isdesigned to determine dependency relations amongwords and phrases in sentences.
To do so, it clas-sifies the phrases according to grammatical cate-gories and syntactic attributes.
However, it fails toreject semantically unacceptable dependency struc-tures.
The inevitable consequence is that RDG oftenproduces multiple parses even for a simple sentence.Kurohashi and Nagao (1993) try to determine thedependency relations of a sentence by means of usingsample sentences.
When the sentence is structurallyambiguous, they determine its structure by compar-ing it to structurally similax patterns taken from amanually generated set of examples and calculatingsimilarity values.Our method, on the contrary, uses a statistical ap-proach to select he most probable structure or parseof a given sentence.
It takes as input dependencystructures generated by RDG for a sentence, finds allof modifier-particle-modificant relations, calculatestheir mutual information and chooses the structurefor which the product of the nmtual information ofits relations is the highest.In order to calculate the mutual informationfor any modifier-particle-modificant pattern, we usethe Conceptual Dictionary: (CD) to build a tax-onomic hierarchy of the modifiers which occur1The Co-occurrence Dictionary and Conceptual Dic-tionary used in the process are part of a set of machinereadable :\]apanese dictionaries compiled by the .JapanElectronic Dictionary Research Institute (EDIt, 1993).The Conceptual Dictionary is a set of graphs consistingof 400,000 concepts and a number of taxonomic as well asfunctional relations between them.
The Co-occurrenceDictionary consist of a list of 1,100,000 dependency re-lations (modifier, particle and modificant) taken from acorpus.
Each entry includes yntactic information, con-cept identifiers (a numerical code) and the number ofoccurrences in the corpus.372with the particle-modificant sub-pattern in the Co-occurrence Dictionary (COD).
The mutual informa-tion for any pattern is the maximum mutual infor-mation between the sub-pattern and the concepts inthe taxonomic hierarchy which generalize the modi-tier in the pattern.Resnik and Hearst (1993) use a similar approachto calculate preferences for prepositional phrase at-tachment.
While they use data on word groups, ourmethod directly uses word co-occurrence data to es-timate the preferences using the CD to identify themost adequate grouping for each relation.While Kurohashi and Nagao compare the sentencewith a single sample of patterns, we use all occur-rehces of the pattern in COD to calculate the mu-tual information.
Our approach automatically ex-tracts the occurrences from the dictionary as well asbuilds the taxonomic hierarchy.
Unlike Kurohashiand Nagao (1993), which uses only verb and adjec-tive patterns, we cover all dependency relations.3 Se lec t ing  the  Most  P robab leS t ructureRDG identifies all possible dependency structureswhich consist of modifier-modificant relations be-tween elements in a sentence.
The arcs in the fol-lowing example show modifier-modificant relationswhich can be combined into six different dependencystructures.I I, , , In J  ,n&tional investig&tion based cause pro - -~ s - ~ ~ - -Our objective is to develop a method to automat-ically select the correct dependency structures accu-rately or at least those which have the highest prob-ability of being correct.
We evaluate the variouspossible structures according to the mutual infor-mation between modifiers and particle-modificants.In some cases there is no particle and the modifi-cant directly precedes the modifier (see example insection 3.2).
To calculate the mutual informationfor each relation, we obtain form the COD the con-ceptual identifiers (a numerical code) for the mod-ifiers that appear with the particle-modifica~t andthe number of their occurrences in the corpus.
If thepattern is not present, backing off, we search this in-formation for the modificant only.
For each of thoseconcept identifiers we obtain from the CD all gen-eralizers (concept identifiers that express a similarmeaning in a more general way) and build a taxo-nomic hierarchy with them.
Using the number ofoccurrences obtained, we calculate the mutual infor-mation for the concepts in the taxonomic hierarchy.We also build a taxonomic hierarchy for the modi-fier that appears with the particle-modificant i  thesentence.
Then comparing these two taxonomic hi-erarchies (one for the modifiers in the COD, one forthe modifers in the sentence), we look for the con-cept identifier common to both hierarchies that hasthe highest mutual information.
This is the mutualinformation for the relation itself.
For each depen-dency structure we calculate a score by multiplyingthe mutual information for all ambiguous relations(the non-ambiguous do not contribute to the evalua-tion).
The dependency structure with highest prob-ability of being correct is the one with the highestscore.
Since all structures have the same number ofrelations, this multiplication reflects the likelyhoodof the structure.3.1 The  A lgor i thmThe process described above is written in an algo-rithmic form as follows:1.
Select the ambiguous relations (those with morethan one modificant) for each structure.2.
Search COD for the particle-nmdificant sub-pattern, in the corresponding positions.
If thereis no entry, search for the modificant only.3.
Obtain from the COD the concept identifiers forthe modificant (there may be multiple mean-ings) and the concept identifiers with the num-ber of their occurrences in the corpus forthe modifiers which occur with the particle-modificant pattern.4.
For each modificant concept identifer, build ataxonomic hierarchy with its modifiers usingCD to find the generalizer for each concept iden-tifier.5.
Calculate the mutual information 2for all the concept identifiers in the taxonomichierarchies.6.
For the modifiers in the sentence, extract theirconcept identifiers from COD and build the tax-onomic hierarchies using CD to find the gener-alizers for each concept identifier.7.
For each relation (modifier-particle-modificantpattern), search the concept identifier that gen-eralizes the modifier word and has maximumnmtual information.
This value is the mutualinformation for the relation.8.
For each dependency structure, multiply themutual information of its ambiguous depen-dency relations to obtain the score for thatstructure.9.
Arrange the structures according to their scores.2The mutual information tells how much informationone outcome &ives about the other and is given by theformula:I(Wl, w2) ---- In kp - ( -~)  \] (1)3733.2 ExamplesThe following figure shows the output from RDG fora given sentence.
The arrows in the figure indicatethe dependency relations.~ ~ - -  work people stress s t ructure  - lllIIOY~lOn progressgrow worseThe ambiguous relations are ~$i~g~ ~J~./v'C,and ~ A~-~ ?)
~ "~.
Accordingly the occurrences forthe modificants in these relations (~O~O, ~ ( ,( ,  ~ ,  and ~?7o) are extracted from COD, ob-taining a list of modifier concept identifiers with thenumber of their occurrences.
Note that in the pat-tern ~ ( A and /~ ( ~ b l/7, the modificant pre-cedes the modifier.
The following figure shows somemodifiers for ~ ( (work)  with their number of oc-currences.person wom~n mother dr ive  each  person  f~ctory  w i fe  f~ct  worker32 18 6 6 3 3 3 2 2 2Next, the taxonomic hierarchy for each particle-modificant is built and the mutual information cal-culated for each concept identifier.
An extra?t of thehierarchy for ~ ( is shown in the following figure.~ (0.0)-~ ~2~2 n ~  pseudo-stilq life~,~'~7o:TF~ 71 life ~.bstract producthuma,  n or  s imi la r/~  l ive  body  re la t ive  to  ac t ionhuman #)~ (3.61) ~ (3.40)person  fo rceNext the generalizers for (~,  A, and ;~ b P~)are searched in the hierarchies for their modificantsto obtain the mutual information for the relations.For ~ (A  (working person) it happened to be theconcept A (person) itself with mutual informationof 3.61.
For ~ ( 5~ b l~ 5~ (working stress) the matchoccurred for ~ (force) giving a mutual informationof 0.69.Multiply the mutual information for all the depen-dency relations in each structure.
For the examplesentence the mutual information for the ambiguousrelations are as follows:~-~.95 .~- - - '~")?~'C'~\]'zFrom this the algorithm selects the parse withhighest score which is drawn in thick lines.
The nextfigure shows the result for the first example sentence.1.60 3.40sudden relat ion deep heart  disease pressure more than 10"/0 was4 Resu l ts  and  Eva luat ionWe have applied our method to 35 sentences takenfrom a leading newspaper and included with RDGsoftware.
The average number of dependency struc-tures per sentence is 8.68.
The method we used se-lected the correct structures for 25 sentences.
Thecorrect structures for 8 sentences were found as thesecond most probable structure by the method.In another experiment, we parsed 70 sentences us-ing a grammar similar to the one used in Kurohashiand Nagao (1993).
Our method selected the mostlikely relation among the multiple generated in 95~,of the cases.Although the size of the test data is small, we saythat our method provided a way to identify the mostprobable structure more efficiently than RDG.
Sincethe sentences used are extracted from a newspaper,it's also general in its applicability.
Therefore it canbe used in preparing teaching materials uch as thestructures used by a CAI system such as CATERS,saving the instructor of hand-coding them.
In futurework we shall extract the co-occurrences directlyfrom the corpora, and use other grouping techniquesto replace the CD.5 AcknowledgmentsI am thankful to my thesis advisor Dr. T. Furugoriand the anonymous referees for their suggestions andcomments.Re ferencesFukumoto, F.; Sano H., Saitoh, Y.; and FukumotoJ.
1992.
A Framework for Dependency Gram-mar based on the word's modifiability level -Restricted Dependency Grammar.
In Trans.
IPSJapan, 33(10), (in Japanese).Resnik,P.
and Hearst M. 1993.
Structural Ambiguityand Conceptual Relations.
In Proceedings of theWorkshop on Very Large Corpora: Academic andIndustrial Perspectives.
Ohio State University.Japan Electronic Dictiona~'y Research Institute, Ltd.1993.
EDR Electronic DictionaiT SpecificationsGuide (in Japanese).Kano, C. and Yanlamoto, H. 1995.
A System forReading Scientific and Technical Texts, Class-room, Instruction and Evaluation.
In Jinbunka-gaku to computer 27(1) (in Japanese).Kurohashi, S., and Nagao, M. 1993.
StructuralDisambiguation i Japanese by Evaluating CaseStructures based on Examples in Case Frame Dic-tionary.
In Proceedings of IVCPT93.374
