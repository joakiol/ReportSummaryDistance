The ATIS Spoken Language SystemsPilot CorpusCharles T. Hemphil l ,  John J. Godfrey, George R. DoddingtonTexas Instruments IncorporatedPO Box 655474, MS 238Dallas, Texas 75265AbstractSpeech research has made tremendous progress in thepast using the following paradigm:?
define the research problem,?
collect a corpus to objectively measure progress, and?
solve the research problem.Natural language research, on the other hand, has typ-ically progressed without the benefit of any corpus ofdata with which to test research hypotheses.
We de-scribe the Air Travel Information System (ATIS) pilotcorpus, a corpus designed to measure progress in Spo-ken Language Systems that include both a speech andnatural anguage component.
This pilot marks the firstfull-scale attempt o collect such a corpus and providesguidelines for future efforts.IntroductionThe ATIS corpus provides an opportunity to developand evaluate speech systems that understand sponta-neous speech.
This corpus differs from its predecessor,the Resource Management corpus (Price eg al, 1988), inat least four significant ways.1.
Instead of being read, the speech has many ofthe characteristics of spontaneous spoken language(e.g., dysfiuencies, false starts, and colloquial pro-nunciations).2.
The speech collection occurs in an office environ-ment rather than a sound booth.3.
The grammar becomes part of the system underevaluation rather than a given part of the experi-ment.4.
The reference answer consists of the actual replyfor the utterance rather than an orthographic tran-scription of the speech.The evaluation methodology supported by ATIS de-pends on having a comparable representation f the an-swer for each utterance.
This is accomplished by limitingthe utterances to database queries~ and the answers toa ground set of tuples from a fixed relational database.The ATIS corpus comprises the acoustic speech data fora query, transcriptions of that query, a set of tuples thatconstitute the answer, and the SQL expression for thequery that produced the answer tuples.The ATIS database consists of data obtained fromthe Official Airline Guide (OAG, 1990), organized un-der a relational schema.
The database remained fixedthroughout he pilot phase.
It contains informationabout flights, fares, airlines, cities, airports, and groundservices, and includes twenty-five supporting tables.
Thelarge majority of the questions posed by subjects canbe answered from the database with a single relationalquery.To collect the kind of English expected in a real work-ing system, we simulate one.
The subject, or "travelplanner," is in one room, with those running the simula-tion in another.
The subject speaks requests over a mi-crophone and receives both a transcription of the speechand the answer on a computer screen.
A session lastsapproximately one hour, including detailed preliminaryinstructions and an exit questionnaire.Two "wizards" carry out the simulation: one tran-scribes the query while the other produces the answer.The transcriber interprets any verbal editing by the sub-ject and removes dysfluencies in order to produce an or-thographic transcription of what the subject intendedto say.
At the same time, the answerer uses a natu-ral language-oriented command language to produce anSQL expression that elicits the correct answer for thesubject.
On-line utilities maintain a complete log of thesession, including time stamps.At the conclusion of the session, the utterances aresorted into categories to determine those utterances suit-able for objective evaluation.
Finally, each utterancereceives three different ranscriptions.
First, a checkedversion of the transcription produced uring the sessionprovides an appropriate input string for evaluating text-based natural language systems.
Second, a slightly ex-panded version of this serves as a prompt in collectinga read version of the spontaneously spoken sentences.Finally, a more detailed orthographic transcription rep-resents the speech actually uttered by the subject, ap-propriate for use in acoustic modeling.96Corpus Col lect ionAbout one session a day was conducted, using subjectsrecruited from within Texas Instruments.
A typical ses-sion included approximately 20 minutes of introduction,40 minutes of query time and 10 minutes for follow-up.Each session resulted in two speech files for each queryand a complete log of the session.
Figure 1 depicts thesession procedure.I I Introductionsubj.o, l l__.
t  o,,ow.
?p II Tran'?r'p"?n H Answer L._~.
J i i~t~ ii I Wizard Wizard I 71ii  !iiiiiii !llliiiiiilN?i ilNiiiiiiiillFigure 1: Subject Session ProcedureSess ion  In t roduct ionThe subjects were given the following instructions, bothorally and in writing:The Air Travel Information System (ATIS) is aprototype of a voice-input information retrievalsystem.
It has the same information that iscontained in the Official Airline Guide (OAG)to help you make air travel plans.
We wouldlike you to participate in a trial use of this ex-perimental system.Subjects were not told whether that the "experimentalsystem" was totally automated or involved human inter-vention.
It was hoped that most subjects would believethat the system was real to elicit natural speech.Subjects were informed about the contents of the re-lational database in a one page summary.
The summarydescribed the major database ntities in fairly generalterms to avoid influencing the vocabulary used duringthe session.
To avoid some misconceptions in advance,subjects were told that the database did not contain in-formation about hotels or rental cars.The subject was next assigned a travel planning sce-nario, systematically chosen from a set of six scenar-ios designed to exercise various aspects of the database.For example, some scenarios focused on flight time con-straints while others concentrated on fares.
The scenar-ios did not specify particular times or cities in an effortto make the scenario more personal to the subject.
Thefollowing example illustrates this:Plan the travel arrangements for a small fam-ily reunion.
First pick a city where the get-together will be held.
From 3 different cities(of your choice), find travel arrangements hatare suitable for the family members who typifythe "economy", "high class", and "adventur-ous" life styles.After receiving the scenario, subjects were left with theinstructions and given five minutes to plan the detailsof the scenarios.
Subjects were given pen and paper onwhich to write the details and to take notes during thesession.Finally, subjects were given instructions regarding theoperation of the system.
The "system", from the sub-jects perspective, consisted of a 19 inch color monitorrunning the X Window System, and a head-mountedSennheiser (HMD 410-6) microphone.
A desk mountedCrown (PCC-160 phase coherent cardioid) microphonewas also used to record the speech.
The "office" con-tained a spare-station cpu and disk to replicate officenoise, and a wall map of the United States to help sub-jects solve their scenarios.The monitor screen was divided into two regions: alarge, scrollable window for system output and a smallerwindow for speech interaction.
The system used a "push-to-talk" input mechanism, whereby speech collection oc-curred while a suitably marked mouse button was de-pressed.
Subjects were given the opportunity to cancelan utterance for a period of time equal to the length ofthe utterance.A single sentence was used for all subjects to illus-trate the push-to-talk mechanism and interaction withthe system:Show me all the nonstop flights between At-lanta and Philadelphia.This sentence was processed as if the system actuallyresponded to the utterance, including a transcription ofthe speech on the subject's display followed by the an-swer in table format.Session QueriesAfter the introduction, subjects were given approxi-mately 40 minutes to complete the task described in thescenario.
If they finished early, subjects were instructedto select another scenario or to explore the capabilitiesof the system.
After the 40 minutes, subjects were giventhe opportunity to continue, finally ending the sessionby saying "all done".Once the actual session started, subjects cycledthrough thinking, querying, waiting, and writing.
Whilethe thinking portion of the session actually requiredthe most time, the query portion required the most re-sources.Several things happened at once as a given subjectspoke a query.
While speech from both the head-mounted and desk-mounted microphones was recorded,one wizard began to transcribe the speech and the otherwizard began to answer the query.
A playback capa-bility could be used if needed by the transcription wiz-ard.
The answer wizard was constrained not to sendthe answer before the transcription wizard finished thetranscription.
Typically, the subject received the typed97transcription a few seconds after speaking and the an-swer approximately 20 seconds later.Each wizard each had their own X Window termi-nal.
The transcription wizard used a gnuemacs-basedtool that checked the spelling of the transcription andsent the transcription to both the answer wizard and thesubject.
Despite the transcription wizard's best efforts,some transcription mistakes did reach the subject: oc-casionally words were omitted, inserted, or substituted(e.g., "fight" for "flight").The answer wizard used a tool called NLParse(Hemphill et al 1987) to form the answer to the sub-jects queries.
This tool used a natural anguage-orientedcommand language to produce a set of tuples for the an-swer.
NLParse provides a set of menus to help convey thelimited coverage to the wizard.
In practice, the answerwizard knew the coverage and used typing with escapecompletion to enter the appropriate NLParse command.NLParse provides several advantages as a wizard tool:?
every answerable query (with respect to thedatabase) receives an answer,?
the NLParse query language avoids ambiguity,?
the wizard formulates the answer in terms ofdatabase ntities, and?
the wizard can easily discern the correctness of theanswer.However, the NLParse query language was not originallydesigned for rapid query entry, prompting several smallgrammar enhancements during the pilot.The answer wizard's terminal also included agnuemacs-based utility that created a session log.
Thisincluded the transcription, the NLParse input, the re-sulting SQL expression, and the set of tuples constitut-ing the answer.
The answer wizard sent only the set oftuples to the subject.The  AT IS  DatabaseThe ATIS database was designed to model as much ofa real-world resource as possible.
In particular, we triedto model the printed OAG in a straightforward manner.With this approach, we could rely on travel data exper-tise from Official Airline Guides, Incorporated.
We alsoused the data directly from the OAG and did not inventany data - -  something that is difficult to accomplish ina realistic manner.
Additionally, the printed OAG wasavailable to all sites and provided a form of documenta-tion for the database.The relational schema were designed to help answerqueries in an intuitive manner, with no attempt o max-imize the speech collected (e.g., by supplying narrow ta-bles as answers).
Toward this end, entities were repre-sented with simple sets or lists in the most direct way.Sess ion  Fo l low-UpAfter the query phase of the session, subjects were givena brief questionnaire to let us know what they thought ofthe system.
This consisted of the following ten questionswith possible answers of "yes" "maybe/sometimes","no" or "no opinion":1.
Were you able to get the travel information youneeded?2.
Were you satisfied with the way the information waspresented?3.
Did the responses contain the kinds of informationyou were seeking?4.
Were the answers provided quickly enough?5.
Would you prefer this method to looking up theinformation in a book?6.
Did the system understand your requests the firsttime?7.
If the system did not understand you, could youeasily find another way to get the information on alater try?8.
Was the travel planning scenario appropriate for atrial use of the system?9.
Do you think a person unfamiliar with computerscould use the system easily?10.
Do you think a human was interpreting your ques-tions?After the questionnaire, the subjects were given achance to ask questions, and were informed that the sys-tem was a simulation involving human intervention.
Fi-nally, we thanked our subjects with their choice of eithera mug or a T-shirt.Corpus ProcessingAfter data collection, a rather elaborate series of pro-cessing steps was required before the subject's utterancesactually became part of the corpus.
A session resultedin a set of speech files and a session log that formed theraw materials for the corpus.
Figure 2 illustrates theprocessing steps.T ranscr ip t ionsTo facilitate use of the corpus, three transcriptions wereprovided with each query.
A more detailed transcriptiondocument specifies the details of these, with the rationaleexplained below.?
NL - input :  This transcription is a corrected ver-sion of the on-the-fly session transcription, cor-rected" while reviewing the subject's peech off-line.This transcription reflects the speech as the sub-ject meant to say it, that is, dysfluencies corrected98TranscriptionDocumentClassificationDocumentI nterpretationDocument\[ ANSI SQLDocumentCAS FormatJ File FormatDocument~, , , .
, , ,n????
.
.
?o , ?
, ?
.
, .
, ?
, , , , , , , .
, , .
.
?
.
?????
.
.
???????
.
.o .o??
, , .oo .
.
?
.
?
?o .
?
.
.
!i|1 iE ----i-~J ClassificationReference  InterpretationReference SOL IReference Answer ..........
.,.,., .
.
: .
.
.
.
.
.
.
.
?
?
.
.
I .
.
, .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
?
.
?
?
.
.
?
????
.
?
.
.
?
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
|Corpus F'es Iation and dysfluencies were removed, resulting in some-thing resembling the NL_ input  transcription, but withabbreviations and numbers expanded.
Example: ||WHERE IS THE STOP FOR U S AIRFL IGHT NUMBER THIRTY SEVEN FROMPHILADELPHIA TO SAN FRANCISCOFigure 2: Corpus Processing Stepsverbally by the subject were corrected.
The orthog-raphy of the transcription obeys common Englishconventions.
It is suitable as input to a text-basednatural anguage systems.
Example:Where is the stop for USAir flight number37 from Philadelphia to San Francisco??
p rompt ing_text :  This transcription expands anyacoustically ambiguous lexical tokens found in theNLAnput  transcription while listening to the sub-ject's speech.
This transcription serves as a promptin a later read-speech session, allowing a compar-isons of read and spontaneous speech.
Example:Where is the stop for USAir flight num-ber thirty-seven from Philadelphia to SanFrancisco??
SR-output :  This transcription includes a detaileddescription of the major acoustic events in thequery.
It is created from the prompt lng- - text  whilelistening to the subject's peech and includes all thedysfluencies previously ignored.
Abbreviations andnumbers are expanded to eliminate open-class lex-ical items.
This transcription serves as a point ofcomparison for speech recognition systems that out-put an orthographic transcription.
Example:Where is the stop \[uh\] for U S, Air flightnumber thirty seven, from Philadelphia toSan FranciscoFor interim testing purposes, a Standard, NormalizedOrthographic Representation (SNOR) was created algo-rithmically from the SR_output  transcription.
Punctu-ClassificationNot all queries were equally suited for evaluating spokenlanguage systems.
Accordingly, each query received aclassification to help circumscribe the types of queries de- |i sired for training and testing.
The classifications them-|!
selves were determined through a committee and defined \]several dimensions:?
context-dependent/context-removable/context-independent?
ambiguous (vague)/clear?
unanswerable/answerable?
ill-formed (grossly)/well-formed?
noncooperative/cooperativeThe committee defined evaluable queries (for June,1990) as those not classified by the first term in eachset.
In addition to these, the following simple classifica-tions were supplied to help sites analyze results:?
ungrammatical/grammatical?
multi-sentence/single-sentenceReference InterpretationAn interpretation document was defined, which speci-fies the details of how to interpret an ATIS query, bothfor the answer wizard and for the SLS sites.
For exam-ple, for consistency it was ruled that a flight serving asnack would be considered as a flight with a meal.
Thedocument provides a mapping of concepts expressed inEnglish to concepts encoded in the relational database.The NLParse commands reflect these conventions andwere included in the corpus to facilitate maintenancesince it was usually easier to determine the correctnessof the reference answer by looking at the NLParse com-mand rather than the resulting SQL expression.
In theevent of an erroneous answer, correction occurs by sim-ply amending the NLParse command.Reference SQLThe pilot corpus includes the ANSI-standard SQL ex-pression that produced the reference answer, which is the"final word" on the interpretation of a subject's query.It also provides ome degree of database independence.For example, as long as the relational schema remainfixed, we can add new cities to the database, rerun theSQL against the database, and produce a new corpusthat includes the new cities.
This works as long as theevaluation criteria excludes context-dependent queries.99Reference  AnswerThe reference answer consists of the set of tuples result-ing from the evaluation of the reference SQL with respectto the official ATIS database.
This is actually redundant,but makes scoring easier for most sites.
The tuples areformatted according the Common Answer Specification(GAS) format (Boisen et al 1989).
This format amountsto representing the answer in Lisp syntax to aid in au-tomatic scoring.Corpus  F i lesAll of the items mentioned above were formatted intofiles and shipped to the National Institute of Standardsand Technology (NIST).
NIST then distributed the cor-pus to interested sites.
A file format document exists tohelp sites install the data.Resu l tsForty-one sessions containing 1041 utterances were col-lected over 8 weeks, nine of which were designated astraining material by NIST.
Each session consisted of 25.4queries per session on average.
Table 1 describes the ut-terance statistics for each Pilot Distribution (PD).PD \[ Weeks I Sessions Utt Utt/Sess1 2 9 234 26.02 2 10 245 24.53 2 10 236 23.64 1 7 197 28.15 1 5!
129 25.8\[ total I 8\[ 4111041 25.4Table 1: Session Utterance StatisticsTable 2 describes the time statistics for each PD.
Eachsession consisted of approximately 40 minutes of querytime with an average rate of 39.1 queries per hour.
Theaverage time between queries of 1.5 minutes includedsubject hinking time, and about 22 seconds for the wiz-ard to send the answer to the subject after the transcrip-tion.PD I Min Ave1 355 39.42 354 35.43 391 39.14 302 43.15 196 39.1total I 1598 39.0Min/Utt  i Sec/Ans Utt /Hr1.5 23.5 39.61.4 21.2 41.51.7 24.2 36.21.5 19.6 39.11.51.521.622.139.539.1Table 2: Session Time StatisticsThe average utterance length (in words) varied ac-cording to the transcription: 10.2 for NLAnput ,  11.7for Sl : t_output (expanded lexical items and dysfluen-cies), and 11.3 for NL_SNOR (expanded lexical items).Eighteen percent of the utterances contained some formof dysfluency.Of the 1041 utterances collected, 740 were judgedevaluable according to the June 1990 criteria: not classi-fied as context-dependent, ambiguous, ill-formed, unan-swerable, or noncooperative.
These results are shown inTable 3, broken down according to PD.
The table alsoshows that if we relax these criteria to exclude only am-biguous and unanswerable utterances, the yield wouldincrease from 71% to 80%.PD UttlJ-unevll%J-evllrel x %evl1 234 88 62 73 682 245 73 70 52 793 236 47 80 32 864 197 58 70 27 865 129 35 73 19 85total 1041 301 I 71 I 203 80 ITable 3: Session Yield of Evaluable UtterancesSubjects generally enjoyed the sessions, as reflected inTable 4 (the tally includes two subjects not included inthe corpus).
The answers to questions were typically notprovided quickly enough, as might be expected in a sim-ulation.
Some subjects defined an acceptable responsetime as under 5 seconds.
Of the subjects that thought ahuman was interpreting the questions, some knew in ad-vance, some misinterpreted the question ("Did the sys-tem behave as if a human was interpreting your ques-tions?
"), and some were tipped-off by the amazing abil-ity of the system to recognize speech in the face of grossdysfluencies.I Q \[ Yes I Maybe/Sometimes \[ No \ ]No  Opinion1 27 16 0 02 32 10 1 03 31 9 2 04 2 19 22 05 29 10 4 06 26 15 i 07 24 4 4 78 40 1 1 1i9 26 13 3 1i0 8 7 22~ 5Table 4: Answers to the QuestionnaireSubjects also supplied general comments.
Some sub-jects felt uncomfortable with computers or the system:"The topic was not my thing, but the voiceactivation was fascinating.
"while other subjects were more enthusiastic:i00"The system looks like a winner.
It needs omefine-tuning and perhaps faster response, butotherwise it's a very promising tool.
"ConclusionsThe ATIS SLS pilot corpus has proved that objectiveevaluation of spoken language systems is both possibleand beneficial.
The pilot corpus has also served to clar-ify many points in the data collection procedure.
In thiseffort, we have learned that a spontaneous speech cor-pus is more expensive to collect han a read speech one,but provides an opportunity to evaluate spoken languagesystems under realistic onditions.AcknowledgmentsThis work was supported by the Defense Advanced Re-search Projects Agency and monitored by the NavalSpace and Warfare Systems Command under ContractNo.
N00039-85-C-0338.
The views and conclusions ex-pressed here do not represent the official policies, eitherexpressed or implied, of the Defense Advanced ResearchProjects Agency or the United States Government.We gratefully acknowledge the publishers of the Of-ficial Airline Guide for travel data and consulting help.We thank the subjects for their participation, Jane Mc-Daniel for her invaluable assistance in all phases of thecorpus collection, and the many members of the variouscommittees for their expert advice.References[1] Boisen, Sean, Lance A. Ramshaw, Damaris Ayuso,and Madeleine Bates, "A Proposal for SLS evalu-ation," in Proceedings of $he DARPA Speech andNatural Language Workshop, October 1989.
[2] Hemphill, Charles T., Inderjeet Mani, and StevenL.
Bossie, "A Combined Free-Form and Menu-ModeNatural Language Interface", Abridged Proceedingsof ~he Second International Conference on Human-Computer Interaction, Honolulu, Hawaii, 1987.
[3] Official Airline Guides, Official Airline Guide,North American Edition with Pares, Oakbrook, Illi-nois, Volume 16, No.
7, January 1, 1990.
[4] Price, P.J., W.M.
Fisher, J. Bernstein, D.S.
Pallett,"The DARPA 1000-Word Resource ManagementDatabase for Continuous Speech Recognition", Pro-ceedings of ICASSP, 1988.i01
