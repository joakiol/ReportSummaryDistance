Proceedings of NAACL HLT 2007, Companion Volume, pages 165?168,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsVirtual Evidence for Training Speech Recognizers using Partially LabeleddataAmarnag SubramanyaDepartment of Electrical EngineeringUniversity of WashingtonSeattle, WA 98195-2500asubram@u.washington.eduJeff BilmesDepartment of Electrical EngineeringUniversity of WashingtonSeattle, WA 98195-2500bilmes@ee.washington.eduAbstractCollecting supervised training data for au-tomatic speech recognition (ASR) sys-tems is both time consuming and expen-sive.
In this paper we use the notion of vir-tual evidence in a graphical-model basedsystem to reduce the amount of supervi-sory training data required for sequencelearning tasks.
We apply this approach toa TIMIT phone recognition system, andshow that our VE-based training schemecan, relative to a baseline trained withthe full segmentation, yield similar resultswith only 15.3% of the frames labeled(keeping the number of utterances fixed).1 IntroductionCurrent state-of-the-art speech recognizers use thou-sands of hours of training data, collected from alarge number of speakers with various backgroundsin order to make the models more robust.
It is wellknown that one of the simplest ways of improv-ing the accuracy of a recognizer is to increase theamount of training data.
Moreover, speech recog-nition systems can benefit from being trained onhand-transcribed data where all the appropriate wordlevel segmentations (i.e., the exact time of the wordboundaries) are known.
However, with increasingamounts of raw speech data being made available, itis both time consuming and expensive to accuratelysegment every word for every given sentence.
More-over, for languages for which only a small amountof training data is available, it can be expensive andchallenging to annotate with precise word transcrip-tions ?
the researcher may have no choice but to usepartially erroneous training data.There are a number of different ways to labeldata used to train a speech recognizer.
First, themost expensive case (from an annotation perspec-tive) is fully supervised training, where both wordsequences and time segmentations are completelyspecified1 .
A second case is most commonly usedin speech recognition systems, where only the wordsequences of utterances are given, but their precisesegmentations are unknown.
A third case falls un-der the realm of semi-supervised approaches.
Asone possible example, a previously trained recog-nizer is used to generate transcripts for unlabeleddata, which are then used to re-train the recog-nizer based on some measure of recognizer confi-dence (Lamel et al, 2002).The above cases do not exhaust the set of possibletraining scenarios.
In this paper, we show how thenotion of virtual evidence (VE) (Pearl, 1988) maybe used to obtain the benefits of data with time seg-mentations but using only partially labeled data.
Ourmethod lies somewhere between the first and sec-ond cases above.
This general framework has beensuccessfully applied in the past to the activity recog-nition domain (Subramanya et al, 2006).
Here wemake use of the TIMIT phone recognition task as anexample to show how VE may be used to deal withpartially labeled speech training data.
To the best ofour knowledge, this paper presents the first system toexpress training uncertainty using VE in the speechdomain.2 Baseline SystemFigure 1 shows two consecutive time slices of a dy-namic Bayesian network (DBN) designed for con-1This does not imply that all variables are observed duringtraining.
While the inter-word segmentations are known, themodel is not given information about intra-word segmentations.165TransitionTransitionPhonePhonePositionStatePhoneStateC   =1 C =1SHARPOObservationt?1t?1PHS t?1Rt?1At?1Ot?1 tttttttt?1VE observed childVE appliedvia this CPTVE appliedvia this CPTFigure 1: Training Graph.text independent (CI) phone recognition.
All ob-served variables are shaded, deterministic depen-dences are depicted using solid black lines, valuespecific dependences are shown using a dot-dashlines, and random dependencies are represented us-ing dashed lines.
In this paper, given any randomvariable (rv) X , x denotes a particular value of thatrv, DX is the domain of X (x ?
DX ), and |DX |represents its cardinality.In the above model, Pt is the rv representingthe phone variable, Ht models the current po-sition within a phone, St is the state, Ot theacoustic observations, At and Rt indicate stateand phone transitions respectively.
Here, DXt =DXt?1 , ?t,?X .
In our implementation here,DHt , DAt ?
{0, 1, 2}, DRt ?
{0, 1}.
Also?
{c1, .
.
.
, cn} is an indicator function that turns onwhen all the conditions {c1, .
.
.
, cn} are true (i.e.a conjunction over all the conditions).
The distri-bution for Ht is given by p(ht|ht?1, rt?1, at?1) =?
{ht=0,rt?1=1} + ?
{ht=at?1+ht?1,rt?1=0}, which im-plies that we always start a phone with Ht = 0.We allow skips in each phone model, and At=0,indicates no transition, At=1 implies you transi-tion to the next state, At=2 causes a state to skip(Ht+1 = Ht + 2).
As the TIMIT corpus pro-vides phone level segmentations, Pt is observed dur-ing training.
However, for reasons that will be-come clear in the next section, we treat Pt as hid-den but make it the parent of a rv Ct, with, p(ct =1|pt) = ?lt=pt where lt is obtained from the tran-scriptions (lt ?
DPt).
The above formulation hasexactly the same effect as making Pt observed andsetting it equal to lt (Bilmes, 2004).
Additional de-tails on other CPTs in this model may be found in(Bilmes and Bartels, 2005).
We provide more de-tails on the baseline system in section 4.1.Our main reason for choosing the TIMIT phonerecognition task is that TIMIT includes both se-quence and segment transcriptions (something raret t t1 4 7p2p1Labeled Unlabeledt5t2 t6tt 8t30Figure 2: Illustration showing our rendition of Vir-tual Evidence.for LVCSR corpora such as Switchboard andFisher).
This means that we can compare againsta model that has been trained fully supervised.
It isalso well known that context-dependent (CD) mod-els outperform CI models for the TIMIT phonerecognition task (Glass et al, 1996).
We usedCI models primarily for the rapid experimentalturnaround time and since it still provides a rea-sonable test-bed for evaluating new ideas.
Wedo note, however, that our baseline CI system iscompetitive with recently published CD systems(Wang and Fosler-Lussier, 2006), albeit which usesmany fewer components per mixture (see Sec-tion 4.1).3 Soft-supervised Learning With VEGiven a joint distribution over n variablesp(x1, .
.
.
, xn), ?evidence?
simply means thatone of the variables (w.l.o.g.
x1) is known.
Wedenote this by x?1, so the probability distributionbecomes p(x?1, .
.
.
, xn) (no longer a function of x1).Any configuration of the variables where x1 6= x?1is never considered.
We can mimic this behaviorby introducing a new virtual child variable c intothe joint distribution that is always observed to beone (so c = 1), and have c interact only with x1via the CPT p(c = 1|x1) = ?x1=x?1 .
Therefore,?x1 p(c = 1, x1, .
.
.
, xn) = p(x?1, .
.
.
, xn).
Nowconsider setting p(c = 1|x1) = f(x1), wheref() is an arbitrary non-negative function.
Withthis, different treatment can be given to differentassignments to x1, but unlike hard evidence, weare not insisting on only one particular value.
Thisrepresents the general notion of VE.
In a certainsense, the notion of VE is similar to the priordistribution in Bayesian inference, but it is differentin that VE expresses preferences over combinationsof values of random variables whereas a Bayesianprior expresses preferences over combinations ofmodel parameter values.
For a more information onVE, see (Bilmes, 2004; Pearl, 1988).VE can in fact be used when accurate phone levelsegmentations are not available.
Consider the illus-tration in Figure 2.
As shown, t1 and t4 are the166start and end times respectively for phone p1, whilet4 and t7 are the start and end times for phone p2.When the start and end times for each phone aregiven, we have information about the identity ofthe phone that produced each and every observation.The general training scenario in most large vocabu-lary speech recognition systems, however, does nothave access to these starting/ending times, and theyare trained knowing only the sequence of phone la-bels (e.g., that p2 follows p1).Consider a new transcription based on Figure 2,where we know that p1 ended at some time t3 ?
t4and that p2 started at sometime t5 > t4.
In theregion between t3 and t5 we have no informationon the identity of the phone variable for eachacoustic frame, except that it is either p1 or p2.
Asimilar case occurs at the start of phone p1 andthe end of phone p2.
The above information canbe used in our model (Figure 1) in the followingway (here given only for t2 ?
t ?
t6): p(Ct =1|pt) = ?
{pt=p1,t2?t?t3} + ?
{pt=p2,t5?t?t6} +ft(p1)?
{pt=p1,t3?t?t5} + gt(p2)?
{pt=p2,t3?t?t5}.Here ft(p1) and gt(p2) represent our relative beliefsat time t in whether the value of Pt is either p1or p2.
It is important to highlight that rather thanthe absolute values of these functions, it is theirrelative values that have an effect on inference(Bilmes, 2004).
There are number of differentways of choosing these functions.
First, we can setft(p1) = gt(p2) = ?, ?
> 0.
This encodes ouruncertainty regarding the identity of the phone inthis region while still forcing it to be either p1 orp2, and equal preference is given for both (referredto as ?uniform over two phones?).
Alternatively,other functions could take into account the fact that,in the frames ?close?
to t3, it is more likely to bep1, whereas in the frames ?close?
to t5, it is morelikely to be p2.
This can be represented by usinga decreasing function for ft(p1) and an increasingfunction for gt(p2) (for example linearly increasingor decreasing with time).As more frames are dropped around transitions(e.g., as t3 ?
t2 decreases), we use lesser amountsof labeled data.
In an extreme situation, we can dropall the labels (t3 < t2) to recover the case where onlysequence and not segment information is available.Alternatively, we can have t3 = t2 +1, which meansthat only one frame is labeled for every phone in anutterance ?
all other frames of a phone are left un-transcribed.
From the perspective of a transcriber,this simulates the task of going through an utter-ance and identifying only one frame that belongs to0 10 20 30 40 50 60 70 80 90 10052545658606264% of Unused Segmentation DataPhone AccuracyBaselineUniform over 2 phonesLinear InterpolationFigure 3: Virtual Evidence Resultseach particular phone without having to identify thephone boundary.
In contrast to the task of determin-ing the phone boundary, identifying one frame perword unit is much simpler, less prone to error or dis-agreement, and less costly (Greenberg, 1995).4 Experimental Results4.1 Baseline SystemWe trained a baseline TIMIT phone recognition sys-tem that made full use of all phone level segmen-tations (the fully supervised case).
To obtain theacoustic observations, the signal was first preem-phasized (?
= 0.97) and then windowed using aHamming window of size 25ms at 100Hz.
We thenextracted MFCC?s from these windowed features.Deltas and double deltas were appended to the aboveobservation vector.
Each phone is modeled using 3states, and 64 Gaussians per state.
We follow thestandard practice of building models for 48 differentphones and then mapping them down to 39 phonesfor scoring purposes (Halberstadt and Glass, 1997).The decoding DBN graph is similar to the traininggraph (Figure 1) except that the variable Ct is re-moved when decoding.
We test on the NIST Coretest set (Glass et al, 1996).
All results reported inthis paper were obtained by computing the stringedit (Levenshtein) distance between the hypothesisand the reference.
All models in this paper wereimplemented using the Graphical Models Toolkit(GMTK) (Bilmes and Bartels, 2005).4.2 VE Based Training and ResultsWe tested various cases of VE-based training byvarying the amount of ?dropped?
frame labels oneither side of the transition (the dropped labels be-came the unlabeled frames of Figure 2).
We did thisuntil there was only one frame left labeled for ev-ery phone.
Moreover, in each of the above cases,we tested a number of different functions to gener-167ate the VE scores (see section 3).
The results of ourVE experiments are shown in Figure 3.
The curveswere obtained by fitting a cubic spline to the pointsshown in the figure.
The phone accuracy (PA) of ourbaseline system (trained in a fully supervised man-ner) is 61.4%.
If the total number of frames in thetraining set is NT , and we drop labels on N frames,the amount of unused data is given by U = NNT ?100(the x-axis in the figure).
Thus U = 0% is the fullysupervised case, whereas U = 100% correspondsto using only the sequence information.
Droppingthe label for one frame on either side of every phonetransition yielded U = 24.5%.It can be seen that in the case of both ?uniformover 2 phones?
and linear interpolation, the PA ac-tually improves when we drop a small number (?5 frames) of frames on either side of the transition.This seems to suggest that there might be some in-herent errors in the frame level labels near the phonetransitions.
The points on the plot at U=84.7% cor-respond to using a single labeled frame per phonein every utterance in the training set (average phonelength in TIMIT is about 7 frames).
The PA of thesystem using a single label per phone is 60.52%.
Inthis case, we also used a trapezoidal function definedas follows: if t = ti were the labeled frames forphone p1, then ft(p1) = 1, ti ?
1 ?
t ?
ti + 1, anda linear interpolation function for the other valuest during the transition to generate the VE weights.This system yielded a PA of 61.29% (baseline accu-racy 61.4%).
We should highlight that even thoughthis system used only 15.3% of the labels used bythe baseline, the results were similar!
The figurealso shows the PA of the system that used onlythe sequence information was about 53% (compareagainst baseline accuracy of 61.4%).
This lends ev-idence to the claim that training recognizers usingdata with time segmentation information can lead toimproved performance.Given the procedure we used to drop the framesaround transitions, the single labeled frame for ev-ery phone is usually located on or around the mid-point of the phone.
This however cannot be guaran-teed if a transcriber is asked to randomly label oneframe per phone.
To simulate such a situation, werandomly choose one frame to be labeled for everyphone in the utterance.
We then trained this systemusing the ?uniform over 2 phones?
technique andtested it on the NIST core test set.
This experimentwas repeated 10 times, and the PA averaged over the10 trails was found to be 60.5% (standard deviation0.402), thus showing the robustness of our techniqueeven for less carefully labeled data.5 DiscussionIn this paper we have shown how VE can be usedto train a TIMIT phone recognition system usingpartially labeled data.
The performance of this sys-tem is not significantly worse than the baseline thatmakes use of all the labels.
Further, though thismethod of data transcription is only slightly moretime consuming that sequence labeling, it yeilds sig-nificant gains in performance (53% v/s 60.5%).
Theresults also show that even in the presence of fullylabaled data, allowing for uncertainity at the tran-sitions during training can be beneficial for ASRperformance.
It should however be pointed outthat while phone recognition accuracy is not al-ways a good predictor of word accuracy, we stillexpect that our method will ultimately generalizeto word accuracy as well, assuming we have ac-cess to a corpus where at least one frame of eachword has been labeled with the word identity.
Thiswork was supported by an ONR MURI grant, No.N000140510388.References[Bilmes and Bartels2005] J. Bilmes and C. Bartels.
2005.Graphical model architectures for speech recognition.
IEEESignal Processing Magazine, 22(5):89?100, September.
[Bilmes2004] J. Bilmes.
2004.
On soft evidence in Bayesiannetworks.
Technical Report UWEETR-2004-0016, Univer-sity of Washington, Dept.
of EE.
[Glass et al1996] J.
Glass, J. Chang, and M. McCandless.
1996.A probabilistic framework for feature-based speech recogni-tion.
In Proc.
ICSLP ?96, volume 4, Philadelphia, PA.[Greenberg1995] S Greenberg.
1995.
The Switchboard tran-scription project.
Technical report, The Johns Hopkins Uni-versity (CLSP) Summer Research Workshop.
[Halberstadt and Glass1997] A. K. Halberstadt and J. R. Glass.1997.
Heterogeneous acoustic measurements for phoneticclassification.
In Proc.
Eurospeech ?97, pages 401?404,Rhodes, Greece.
[Lamel et al2002] L. Lamel, J. Gauvian, and G. Adda.
2002.Lightly supervised and unsupervised acoustic model train-ing.
Computer Speech and Language.
[Pearl1988] J. Pearl.
1988.
Probabilistic Reasoning in Intel-ligent Systems: Networks of Plausible Inference.
MorganKaufmann Publishers, Inc.[Subramanya et al2006] A. Subramanya, A. Raj, J. Bilmes, andD.
Fox.
2006.
Recognizing activities and spatial contextusing wearable sensors.
In Proc.
of the Conference on Un-certainty in Articial Intelligence (UAI).
[Wang and Fosler-Lussier2006] Y. Wang and E. Fosler-Lussier.2006.
Integrating phonetic boundary discrimination explic-ity into HMM systems.
In Proc.
of the Interspeech.168
