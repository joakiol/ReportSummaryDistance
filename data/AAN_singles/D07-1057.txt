Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
541?550, Prague, June 2007. c?2007 Association for Computational LinguisticsIdentification and Resolution of Chinese Zero Pronouns:A Machine Learning ApproachShanheng Zhao and Hwee Tou NgDepartment of Computer ScienceNational University of Singapore3 Science Drive 2Singapore 117543{zhaosh, nght}@comp.nus.edu.sgAbstractIn this paper, we present a machine learn-ing approach to the identification and reso-lution of Chinese anaphoric zero pronouns.We perform both identification and resolu-tion automatically, with two sets of easilycomputable features.
Experimental resultsshow that our proposed learning approachachieves anaphoric zero pronoun resolutionaccuracy comparable to a previous state-of-the-art, heuristic rule-based approach.
Toour knowledge, our work is the first to per-form both identification and resolution ofChinese anaphoric zero pronouns using amachine learning approach.1 IntroductionCoreference resolution is the task of determiningwhether two or more noun phrases refer to the sameentity in a text.
It is an important task in discourseanalysis, and successful coreference resolution ben-efits many natural language processing applicationssuch as information extraction, question answering,etc.In the literature, much of the work on corefer-ence resolution is for English text (Soon et al, 2001;Ng and Cardie, 2002b; Yang et al, 2003; McCal-lum and Wellner, 2005).
Publicly available cor-pora for coreference resolution are mostly in En-glish, e.g., the Message Understanding Conferencetasks (MUC6 and MUC7)1.
Relatively less work has1http://www-nlpir.nist.gov/related_projects/muc/been done on coreference resolution for Chinese.Recently, the ACE Entity Detection and Tracking(EDT) task2 included annotated Chinese corpora forcoreference resolution.
Florian et al (2004) andZhou et al (2005) reported research on Chinesecoreference resolution.A prominent phenomenon in Chinese coreferenceresolution is the prevalence of zero pronouns.
A zeropronoun (ZP) is a gap in a sentence which refersto an entity that supplies the necessary informationfor interpreting the gap.
An anaphoric zero pro-noun (AZP) is a zero pronoun that corefers to oneor more overt noun phrases present in the preced-ing text.
Zero pronouns occur much more frequentlyin Chinese compared to English, and pose a uniquechallenge in coreference resolution for Chinese.
Forexample, Kim (2000) conducted a study to comparethe use of overt subjects in English, Chinese, andother languages.
He found that the use of overt sub-jects in English is over 96%, while this percentage isonly 64% for Chinese, indicating that zero pronouns(lack of overt subjects) are much more prevalent inChinese.Chinese zero pronouns have been studied in lin-guistics research (Li and Thompson, 1979; Li,2004), but only a small body of prior work in com-putational linguistics deals with Chinese zero pro-noun identification and resolution (Yeh and Chen,2004; Converse, 2006).
To our knowledge, all pre-vious research on zero pronoun identification andresolution in Chinese uses hand-engineered rules orheuristics, and our present work is the first to per-form both identification and resolution of Chinese2http://www.nist.gov/speech/tests/ace/541anaphoric zero pronouns using a machine learningapproach.The rest of this paper is organized as follows.
InSection 2, we give the task definition, and describethe corpus used in our evaluation and the evaluationmetrics.
We then give an overview of our approachin Section 3.
Anaphoric zero pronoun identificationand resolution are presented in Section 4 and 5, re-spectively.
We present the experimental results inSection 6 and related work in Section 7, and con-clude in Section 8.2 Task Definition2.1 Zero PronounsAs mentioned in the introduction, a zero pronoun(ZP) is a gap in a sentence which refers to an en-tity that supplies the necessary information for in-terpreting the gap.
A coreferential zero pronoun is azero pronoun that corefers to one or more overt nounphrases present in the same text.Just like a coreferential noun phrase, a coreferen-tial zero pronoun can also corefer to a noun phrasein the preceding or following text, called anaphoricor cataphoric, respectively.
Most coreferential zeropronouns in Chinese are anaphoric.
In the corpusused in our evaluation, 98% of the coreferential zeropronouns have antecedents.
Hence, for simplicity,we only consider anaphoric zero pronouns (AZP)in this work.
That is, we only attempt to resolve acoreferential zero pronoun to noun phrases preced-ing it.Here is an example of an anaphoric zero pronounfrom the Penn Chinese TreeBank (CTB) (Xue et al,2005) (sentence ID=300):[?)
??
??
?
?=[China electronic products import and export?4]1 ?
 ?
?2trade]1 continues increasing , ?23  ?
?= {represents total import and export ?s?
?
?
ratio continues increasing .The anaphoric zero pronoun ?2 is coreferring tonoun phrase 1.
The corresponding parse tree isshown in Figure 1.
In CTB, IP refers to a simpleclause that does not have complementizers.
CP, onthe other hand, refers to a clause introduced by acomplementizer.Resolving an anaphoric zero pronoun to its cor-rect antecedent in Chinese is a difficult task.
Al-though gender and number information is availablefor an overt pronoun and has proven to be usefulin pronoun resolution in prior research, a zero pro-noun in Chinese, unlike an overt pronoun, providesno such gender or number information.
At the sametime, identifying zero pronouns in Chinese is also adifficult task.
There are only a few overt pronounsin English, Chinese, and many other languages, andstate-of-the-art part-of-speech taggers can success-fully recognize most of these overt pronouns.
How-ever, zero pronouns in Chinese, which are not ex-plicitly marked in a text, are hard to be identified.Furthermore, even if a gap is a zero pronoun, it maynot be coreferential.
All these difficulties make theidentification and resolution of anaphoric zero pro-nouns in Chinese a challenging task.2.2 CorpusWe use an annotated third-person pronoun and zeropronoun coreference corpus from Converse (2006)3.The corpus contains 205 texts from CTB 3.0, withannotations done directly on the parse trees.
Inthe corpus, coreferential zero pronouns, third-personpronouns, and noun phrases are annotated as coref-erence chains.
If a noun phrase is not in any coref-erence chain, it is not annotated.
If a coreferencechain does not contain any third-person pronoun orzero pronoun, the whole chain is not annotated.A zero pronoun is not always coreferential withsome noun phrases.
In the corpus, if a zero pronounis not coreferential with any overt noun phrases, itis assigned one of the following six categories: dis-course deictic (#DD), existential (#EXT), inferrable(#INFR), ambiguity between possible referents inthe text (#AMB), arbitrary reference (#ARB), andunknown (#UNK).
For example, in the followingsentence, ?3 refers to an event in the preceding text,with no corresponding antecedent noun phrase.
Sono antecedent is annotated, and ?3 is labeled as#DD.&?
??
cL ?THong Kong famous syndicate Cheung Kong3The data set we obtained is a subset of the one used in Con-verse (2006).542IP1HHHHHHHHHHHHHHHHIP2HHHHHHHHHNP1HHHHHNP2NR?)NP3HNN??NN?
?NP4 HNN?
?=NN?4VP1HVV?VP2VVPU?IP3HHHNP5HHHCP1CP2HHHIP4VP3 HHVV3NP6 HADJPJJNP7NN?
?=DEC{NP8NN?VP4HVV?VP5VV?PUFigure 1: The parse tree which corresponds to the anaphoric zero pronoun example in Section 2.1.
"  ?
?- *?
4QuHoldings , Peregrine as strategic=?V .
??
?investors already purchased LE ? ;?
 ?I??
Shenye Holdings ?
twenty percent{ ?Y ?
?3 ?I 'n ?
?s share , ?3 fully reflects out=?V { fe investors ?s confidence .Converse (2006) assumed that all correctly identi-fied AZPs and the gold standard parse trees are givenas input to her system.
She applied the Hobbs algo-rithm (Hobbs, 1978) to resolve antecedents for thegiven AZPs.In our case, we are only interested in zero pro-nouns with explicit noun phrase referents.
If a coref-erence chain does not contain AZPs, we discard thechain.
We also discard the 6 occurrences of zeropronouns with split antecedents, i.e., a zero pronounwith an antecedent that is split into two separatenoun phrases.
A total of 383 AZPs remain in thedata set used in our experiments.Among the 205 texts in the data set, texts 1?155are reserved for training, while the remaining texts(156?205) are used for blind test.
The statistics ofthe data set are shown in Table 1.Training TestDoc ID 1?155 156?205# Docs 155 50# Characters 96,338 15,710# Words 55,348 9,183# ZPs 665 87# AZPs 343 40Table 1: Statistics of training and test data sets.2.3 Evaluation MetricsAs in previous work on pronoun resolution, we eval-uate the accuracy in terms of recall, precision, and F-measure.
The overall recall and precision on the testset are computed by micro-averaging over all test in-stances.
The overall F-measure is then computed.For AZP identification, recall and precision are543defined as:RecallAZP =# AZP Hit# AZP in KeyPrecisionAZP =# AZP Hit# AZP in ResponseAn ?AZP Hit?
occurs when an AZP as reported inthe response (system output) has a counterpart in thesame position in the gold standard answer key.For AZP resolution, recall and precision are de-fined as:RecallResol =# Resol Hit# AZP in KeyPrecisionResol =# Resol Hit# AZP in ResponseA ?Resol Hit?
occurs when an AZP is correctly iden-tified, and it is correctly resolved to a noun phrasethat is in the same coreference chain as provided inthe answer key.3 Overview of Our ApproachIn this section, we give an overview of our approachfor Chinese AZP identification and resolution.Typically, the input raw texts need to be pro-cessed by a Chinese word segmenter, a part-of-speech (POS) tagger, and a parser sequentially.
Al-though our approach can apply directly to machine-generated parse trees from raw text, in order to min-imize errors introduced by preprocessing, and focusmainly on Chinese zero pronoun resolution, we usethe gold standard word segmentation, POS tags, andparse trees provided by CTB.
However, we removeall null categories and functional tags from the CTBgold standard parse trees.
Figure 1 shows a parsetree after such removal.A set of zero pronoun candidates and a set of nounphrase candidates are then extracted.
If W is the left-most word in the word sequence that is spanned bysome VP node, the gap G that is immediately to theleft of W qualifies as a ZP candidate.
For example,in Figure 1, gaps immediately to the left of the twooccurrences of?, and,3,?
are all ZPcandidates.
All noun phrases4 that are either maxi-mal NPs or modifier NPs qualify as NP candidates.4A noun phrase can either be NP or QP in CTB.
We simplyuse NP hereafter.For example, in Figure 1, NP1, NP2, NP3, NP5, andNP6 are all NP candidates.
With these ZP and NPcandidate extractions, the recalls of ZPs and NPs are100% and 98.6%, respectively.After the ZP and NP candidates are determined,we perform AZP identification and resolution in asequential manner.
We build two classifiers, theAZP identification classifier and the AZP resolutionclassifier.
The AZP identification classifier deter-mines the position of AZPs, while the AZP resolu-tion classifier finds an antecedent noun phrase foreach AZP identified by the AZP identification clas-sifier.
Both classifiers are built using machine learn-ing techniques.
The features of both classifiers arelargely syntactic features based on parse trees andare easily computed.We perform 5-fold cross validation on the train-ing data set to tune parameters and to pick the bestmodel.
We then retrain the best model with all datain the training data set, and apply it to the blind testset.
In the following sections, all accuracies reportedon the training data set are based on 5-fold cross val-idation.4 Anaphoric Zero Pronoun IdentificationWe use machine learning techniques to build theAZP identification classifier.
The features are de-scribed in Table 2.In the feature description, Z is the ZP candidate.Let Wl and Wr be the words immediately to the leftand to the right of Z , respectively, P the parse treenode that is the lowest common ancestor node of Wland Wr, Pl and Pr the child nodes of P that are an-cestor nodes of Wl and Wr, respectively.
If Z is thefirst gap of the sentence, Wl, P , Pl, and Pr are allNA.
Furthermore, let V be the highest VP node inthe parse tree that is immediately to the right of Z ,i.e., the leftmost word in the word sequence that isspanned by V is Wr.
If Z is not the first gap in thesentence, define the ceiling node C to be P , other-wise to be the root node of the parse tree.
In theexample shown in Figure 1, for the ZP candidate ?2(which is immediately to the left of 3), Wl, Wr,P , Pl, Pr, V , and C are ??
?, 3, IP1, IP2, IP3,VP3, and IP1, respectively.
Its feature values are alsoshown in Table 2.To train an AZP identification classifier, we gen-544Feature Description ?2First Gap If Z is the first gap in the sentence, T; else F. FPl Is NP If Z is the first gap in the sentence, NA; otherwise, if Pl is an NP node,T; else F.FPr Is VP If Z is the first gap in the sentence, NA; otherwise, if Pr is a VP node, T;else F.FPl Is NP & Pr Is VP If Z is the first gap in the sentence, NA; otherwise, if Pl is an NP nodeand Pr is a VP node, T; else F.FP Is VP If Z is the first gap in the sentence, NA; otherwise, if P is a VP node, T;else F.FIP-VP If in the path from Wr to C , there is a VP node such that its parent nodeis an IP node, T; else F.THas Ancestor NP If V has an NP node as ancestor, T; else F. THas Ancestor VP If V has a VP node as ancestor, T; else F. FHas Ancestor CP If V has a CP node as ancestor, T; else F. TLeft Comma If Z is the first gap, NA; otherwise if Wl is a comma, T; else F. TSubject Role If the grammatical role of Z is subject, S; else X. XClause If V is in a matrix clause, an independent clause, a subordinate clause, ornone of the above, the value is M, I, S, X, respectively.IIs In Headline If Z is in the headline of the text, T; else F. FTable 2: Features for anaphoric zero pronoun identification.
The feature values of ?2 are shown in the lastcolumn.erate training examples from the training data set.All ZP candidates in the training data set generatetraining examples.
Whether a training example ispositive or negative depends on whether the ZP can-didate is an AZP.After generating all training examples, we trainan AZP identification classifier using the J48 deci-sion tree learning algorithm in Weka5.
During test-ing, each ZP candidate is presented to the learnedclassifier to determine whether it is an AZP.
We con-duct experiments to measure the performance of themodel learned.
The results of 5-fold cross validationon the training data set are shown in Table 3.Model R P FHeuristic 99.7 15.0 26.1AZP Ident 19.8 51.1 28.6AZP Ident (r = 8) 59.8 44.3 50.9Table 3: Accuracies of AZP identification on thetraining data set under 5-fold cross validation.We use heuristic rules as a baseline for compar-5http://www.cs.waikato.ac.nz/ml/weka/ison.
The rules used by the heuristic model are asfollows.
For a node T in the parse tree, if1.
T is a VP node; and2.
T ?s parent node is not a VP node; and3.
T has no left sibling, or its left sibling is not anNP node,then the gap that is immediately to the left of theword sequence spanned by T is an AZP.
This simpleAZP identification heuristic achieves an F-measureof 26.1%.Imbalanced Training DataFrom Table 3, one can see that the F-measureof the machine-learned AZP identification model is28.6%, which is only slightly higher than baselineheuristic model.
It has a relatively high precision,but much lower recall.
The problem lies in thehighly imbalanced number of positive and negativetraining examples.
Among all the 155 texts in thetraining set, there are 343 positive and 10,098 neg-ative training examples.
The ratio r of the number545of negative training examples to the number of pos-itive training examples is 29.4.
A classifier trainedon such highly imbalanced training examples tendsto predict more testing examples as negative exam-ples.
This explains why the precision is high, but therecall is low.To overcome this problem, we vary r by varyingthe weight of the positive training examples, whichis equivalent to sampling more positive training ex-amples.
The values of r that we have tried are1, 2, 3, .
.
.
, 29.
The larger the value of r, the higherthe precision, and the lower the recall.
By tuningr, we get a balance between precision and recall,and hence an optimal F-measure.
Figure 2 showsthe effect of tuning r on AZP identification.
Whenr = 8, the optimal F-measure is 50.9%, which ismuch higher than the F-measure without tuning r.0 5 10 15 20 25 30102030405060708090100rScoreRecallPrecisionF?measureFigure 2: Effect of tuning r on AZP identificationNg and Cardie (2002a) reported that the accura-cies of their noun phrase anaphoricity determinationclassifier were 86.1% and 84.0% for the MUC6 andMUC7 data sets, respectively.
Noun phrases providemuch fruitful information for anaphoricity identifi-cation.
However, useful information such as gen-der, number, lexical string, etc, is not available inthe case of zero pronouns.
This makes AZP identifi-cation a much more difficult task, and hence it has arelatively low accuracy.5 Anaphoric Zero Pronoun ResolutionIn anaphoric zero pronoun resolution, we also usemachine learning techniques to build a classifier.The features are described in Table 4.In the feature description, Z is the anaphoric zeropronoun that is under consideration, and A is the po-tential NP antecedent for Z .
V is the same as in AZPidentification.
The feature values of the pair NP1 and?2 (the gap immediately to the left of3) in Figure1 are shown in Table 4.To train the AZP resolution classifier, we generatetraining examples in the following way.
An AZP Zand its immediately preceding coreferential NP an-tecedent A in the gold standard coreference chainform a positive training example.
Between A and Z ,there are other NP candidates.
Each one of these NPcandidates, together with Z , form a negative trainingexample.
This is similar to the approach adopted inSoon et al (2001).
We also train the AZP resolutionclassifier using the J48 decision tree learning algo-rithm.After building both AZP identification and resolu-tion classifiers, we perform AZP identification andresolution in a sequential manner.
For a ZP candi-date Z , the AZP identification classifier determineswhether Z is an AZP.
If it is an AZP, all NP can-didates that are to the left of Z in textual order areconsidered as potential antecedents.
These potentialantecedents are tested from right to left.
We startfrom the NP candidate A1 that is immediately to theleft of Z .
A1 and Z form a pair.
If the pair is classi-fied as positive by the resolution classifier, A1 is theantecedent for Z .
If it is classified as negative, weproceed to the NP candidate A2 that is immediatelyto the left of A1, and test again.
The process contin-ues until we find an antecedent for Z , or there is nomore NP candidate to test.This right-to-left search attempts to find the clos-est correct antecedent for an AZP.
We do not choosethe best-first search strategy proposed by Ng andCardie (2002b).
This is because we generate train-ing examples and build the resolution classifier bypairing each zero pronoun with its closest precedingantecedent.
In addition, a zero pronoun is typicallynot too far away from its antecedent.
In our data set,92.6% of the AZPs have antecedents that are at most2 sentences apart.
Our experiment shows that thisclosest-first strategy performs better than the best-first strategy for Chinese AZP resolution.Table 5 shows the experimental results of 5-foldcross validation on the training data set.
For com-546Feature Description NP1-?2Features between Z and ADist Sentence If Z and A are in the same sentence, 0; if they are one sentenceapart, 1; and so on.0Dist Segment If Z and A are in the same segment (where a segment is a se-quence of words separated by punctuation marks including ???,??
?, ??, ??
?, and ???
), 0; if they are one segment apart, 1; andso on.1Sibling NP VP If Z and A are in different sentences, F; Otherwise, if both A andZ are child nodes of the root node, and they are siblings (or at mostseparated by one comma), T; else F.FClosest NP If A is the closest preceding NP candidate to Z , T; else F. TFeatures on AA Has Anc NP If A has an ancestor NP node, T; else F. FA Has Anc NP In IP If A has an ancestor NP node which is a descendant of A?s lowestancestor IP node, T; else F.FA Has Anc VP If A has an ancestor VP node, T; else F. FA Has Anc VP In IP If A has an ancestor VP node which is a descendant of A?s lowestancestor IP node, T; else F.FA Has Anc CP If A has an ancestor CP node, T; else F. FA Grammatical Role If the grammatical role of A is subject, object, or others, the value isS, O, or X, respectively.SA Clause If A is in a matrix clause, an independent clause, a subordinateclause, or none of the above, the value is M, I, S, X, respectively.MA Is ADV If A is an adverbial NP, T; else F. FA Is TMP If A is a temporal NP, T; else F. FA Is Pronoun If A is a pronoun, T; else F. FA Is NE If A is a named entity, T; else F. FA In Headline If A is in the headline of the text, T; else F. FFeatures on ZZ Has Anc NP If V has an ancestor NP node, T; else F. TZ Has Anc NP In IP If V has an ancestor NP node which is a descendant of V?s lowestancestor IP node, T; else F.FZ Has Anc VP If V has an ancestor VP node, T; else F. FZ Has Anc VP In IP If V has an ancestor VP node which is a descendant of V?s lowestancestor IP node, T; else F.FZ Has Anc CP If V has an ancestor CP node, T; else F. TZ Grammatical Role If the grammatical role of Z is subject, S; else X. XZ Clause If V is in a matrix clause, an independent clause, a subordinateclause, or none of the above, the value is M, I, S, X, respectively.IZ Is First ZP If Z is the first ZP candidate in the sentence, T; else F. FZ Is Last ZP If Z is the last ZP candidate in the sentence, T; else F. FZ In Headline If Z is in the headline of the text, T; else F. FTable 4: Features for anaphoric zero pronoun resolution.
The feature values of the pair NP1 and ?2 areshown in the last column.547parison, we show three baseline systems.
In all threebaseline systems, we do not perform AZP identifica-tion, but directly apply the AZP resolution classifier.In the first baseline, we apply the AZP resolutionclassifier on all ZP candidates.
In the second base-line, we apply the classifier only on ZPs annotatedin the gold standard, instead of all ZP candidates.In the third baseline, we further restrict it to resolveonly AZPs.
The F-measures of the three baselinesare 2.5%, 27.6%, and 40.6% respectively.Model R P FAll ZP Candidates 40.5 1.3 2.5Gold ZP 40.5 20.9 27.6Gold AZP 40.5 40.6 40.6AZP Ident (r=8 t=0.5) 23.6 17.5 20.1AZP Ident (r=11 t=0.6) 22.4 20.3 21.3Table 5: Accuracies of AZP resolution on the train-ing data set under 5-fold cross validation.Tuning of ParametersNg (2004) showed that an NP anaphoricity iden-tification classifier with a cut-off threshold t =0.5 pruned away many correct anaphoric NPs andharmed the overall recall.
By varying t, the overallresolution F-measure was improved.
We adopt thesame tuning strategy and accept a ZP candidate ZPias an AZP and proceed to find its antecedent only ifP (ZPi) ?
t. The possible values for t that we havetried are 0, 0.05, 0.1, .
.
.
, 0.95.In Section 4, we show that r = 8 yields the bestAZP identification F-measure.
When we fix r = 8and vary t, the overall F-measure for AZP resolutionis the best at t = 0.65, as shown in Figure 3.
We thentry tuning r and t at the same time.
An overall op-timal F-measure of 21.3% is obtained when r = 11and t = 0.6.
We compare this tuned F-measure withthe F-measure of 20.1% at r = 8 and t = 0.5, ob-tained without tuning t. Although the improvementis modest, it is statistically significant (p < 0.05).6 Experimental ResultsIn the previous section, we show that when r = 11and t = 0.6, our sequential AZP identification andresolution achieves the best F-measure under 5-foldcross validation on the 155 training texts.
In or-der to utilize all available training data, we generate0 0.2 0.4 0.6 0.8 1051015202530354045tScoreRecallPrecisionF?measureFigure 3: Effect of tuning t on AZP resolutiontraining examples for the AZP identification classi-fier with r = 11, and generate training examplesfor the AZP resolution classifier, on all 155 train-ing texts.
Both classifiers are trained again with thenewly generated training examples.
We then applyboth classifiers with anaphoricity identification cut-off threshold t = 0.6 to the blind test data.
Theresults are shown in Table 6.R P F27.5 24.4 25.9Table 6: Accuracies of AZP resolution on blind testdata.By utilizing all available information on the goldstandard parse trees, Converse (2006) finds an an-tecedent for each AZP given that all AZPs are cor-rectly input to her system.
The accuracy of her rule-based approach is 43.0%.
For comparison, we de-termine the antecedents for AZPs in the gold stan-dard annotation, under 5-fold cross validation on all205 texts in the corpus.
The recall, precision, and F-measure are 42.3%, 42.7%, and 42.5%, respectively.This shows that our proposed machine learning ap-proach for Chinese zero pronoun resolution is com-parable to her state-of-the-art rule-based approach.7 Related WorkConverse (2006) assumed that the gold standardChinese anaphoric zero pronouns and the gold stan-dard parse trees of the texts in Penn Chinese Tree-548Bank (CTB) were given as input to her system,which performed resolution of the anaphoric zeropronouns using the Hobbs algorithm (Hobbs, 1978).Her system did not identify the anaphoric zero pro-nouns automatically.Yeh and Chen (2004) proposed an approach forChinese zero pronoun resolution based on the Cen-tering Theory (Grosz et al, 1995).
Their systemused a set of hand-engineered rules to perform zeropronoun identification, and resolved zero pronounswith a set of hand-engineered resolution rules.In Iida et al (2006), they proposed a ma-chine learning approach to resolve zero pronouns inJapanese using syntactic patterns.
Their system alsodid not perform zero pronoun identification, and as-sumed that correctly identified zero pronouns weregiven as input to their system.The probabilistic model of Seki et al (2002) bothidentified and resolved Japanese zero pronouns, withthe help of a verb dictionary.
Their model neededlarge-scale corpora to estimate the probabilities andto prevent data sparseness.Ferra?ndez and Peral (2000) proposed a hand-engineered rule-based approach to identify and re-solve zero pronouns that are in the subject grammat-ical position in Spanish.8 ConclusionIn this paper, we present a machine learning ap-proach to the identification and resolution of Chi-nese anaphoric zero pronouns.
We perform bothidentification and resolution automatically, with twosets of easily computable features.
Experimen-tal results show that our proposed learning ap-proach achieves anaphoric zero pronoun resolutionaccuracy comparable to a previous state-of-the-art,heuristic rule-based approach.
To our knowledge,our work is the first to perform both identificationand resolution of Chinese anaphoric zero pronounsusing a machine learning approach.Obviously, there is much room for improvement.In future, we plan to apply our model directly onmachine-generated parse trees.
We also plan to clas-sify non-coreferential zero pronouns into the six cat-egories.AcknowledgementsWe thank Susan Converse and Martha Palmer forsharing their Chinese third-person pronoun and zeropronoun coreference corpus.ReferencesSusan Converse.
2006.
Pronominal Anaphora Resolu-tion in Chinese.
Ph.D. thesis, Department of Com-puter and Information Science, University of Pennsyl-vania.Antonio Ferra?ndez and Jesu?s Peral.
2000.
A computa-tional approach to zero-pronouns in Spanish.
In Pro-ceedings of the 38th Annual Meeting of the Associa-tion for Computational Linguistics (ACL2000), pages166?172.Radu Florian, Hany Hassan, Abraham Ittycheriah,Hongyan Jing, Nanda Kambhatla, Xiaoqiang Luo,Nicolas Nicolov, and Salim Roukos.
2004.
A statisti-cal model for multilingual entity detection and track-ing.
In Proceedings of the Human Language Tech-nology Conference and North American Chapter ofthe Association for Computational Linguistics AnnualMeeting 2004 (HLT-NAACL2004), pages 1?8.Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.1995.
Centering: A framework for modeling the lo-cal coherence of discourse.
Computational Linguis-tics, 21(2):203?225.Jerry R. Hobbs.
1978.
Resolving pronoun references.Lingua, 44:311?338.Ryu Iida, Kentaro Inui, and Yuji Matsumoto.
2006.
Ex-ploiting syntactic patterns as clues in zero-anaphoraresolution.
In Proceedings of the 21st Interna-tional Conference on Computational Linguistics and44th Annual Meeting of the Association for Compu-tational Linguistics (COLING-ACL2006), pages 625?632.Young-Joo Kim.
2000.
Subject/object drop in the acqui-sition of Korean: A cross-linguistic comparison.
Jour-nal of East Asian Linguistics, 9(4):325?351.Charles N. Li and Sandra A. Thompson.
1979.
Third-person pronouns and zero-anaphora in Chinese dis-course.
Syntax and Semantics, 12:311?335.Wendan Li.
2004.
Topic chains in Chinese discourse.Discourse Processes, 37(1):25?45.Andrew McCallum and Ben Wellner.
2005.
Conditionalmodels of identity uncertainty with application to nouncoreference.
In Advances in Neural Information Pro-cessing Systems 17 (NIPS), pages 905?912.549Vincent Ng and Claire Cardie.
2002a.
Identifyinganaphoric and non-anaphoric noun phrases to improvecoreference resolution.
In Proceedings of the 19th In-ternational Conference on Computational Linguis-tics (COLING2002), pages 1?7.Vincent Ng and Claire Cardie.
2002b.
Improving ma-chine learning approaches to coreference resolution.In Proceedings of the 40th Annual Meeting of the As-sociation for Computational Linguistics (ACL2002),pages 104?111.Vincent Ng.
2004.
Learning noun phrase anaphoricity toimprove coreference resolution: Issues in representa-tion and optimization.
In Proceedings of the 42nd An-nual Meeting of the Association for ComputationalLinguistics (ACL2004), pages 152?159.Kazuhiro Seki, Atsushi Fujii, and Tetsuya Ishikawa.2002.
A probabilistic method for analyzing Japaneseanaphora integrating zero pronoun detection and reso-lution.
In Proceedings of the 19th International Con-ference on Computational Linguistics (COLING2002),pages 911?917.Wee Meng Soon, Hwee Tou Ng, and Daniel Chung YongLim.
2001.
A machine learning approach to corefer-ence resolution of noun phrases.
Computational Lin-guistics, 27(4):521?544.Nianwen Xue, Fei Xia, Fu-Dong Chiou, and MarthaPalmer.
2005.
The Penn Chinese TreeBank: Phrasestructure annotation of a large corpus.
Natural Lan-guage Engineering, 11(2):207?238.Xiaofeng Yang, Guodong Zhou, Jian Su, and Chew LimTan.
2003.
Coreference resolution using competitionlearning approach.
In Proceedings of the 41st AnnualMeeting of the Association for Computational Linguis-tics (ACL2003), pages 176?183.Ching-Long Yeh and Yi-Chun Chen.
2004.
Zeroanaphora resolution in Chinese with shallow parsing.Journal of Chinese Language and Computing.Yaqian Zhou, Changning Huang, Jianfeng Gao, and LideWu.
2005.
Transformation based Chinese entity de-tection and tracking.
In Proceedings of the SecondInternational Joint Conference on Natural LanguageProcessing (IJCNLP 2005), pages 232?237.550
