Proceedings of NAACL HLT 2007, pages 468?475,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsAutomatic Assessment of Student Translationsfor Foreign Language TutoringChao Wang and Stephanie SeneffSpoken Language Systems GroupMIT Computer Science and Artificial Intelligence LaboratoryThe Stata Center, 32 Vassar Street, Cambridge, MA 02139{wangc,seneff}@csail.mit.eduAbstractThis paper introduces the use of speechtranslation technology for a new type ofvoice-interactive Computer Aided Lan-guage Learning (CALL) application.
Wedescribe a computer game we have devel-oped, in which the system presents sen-tences in a student?s native language toelicit spoken translations in the target newlanguage.
A critical technology is an al-gorithm to automatically verify the ap-propriateness of the student?s translationusing linguistic analysis.
Evaluation re-sults are presented on the system?s abil-ity to match human judgment of the cor-rectness of a student?s translation, for a setof 1115 utterances collected from 9 learn-ers of Mandarin Chinese translating flightdomain sentences.
We also demonstratethe effective use of context information toimprove both recognition performance onnon-native speech as well as the system?saccuracy in judging the translation quality.1 IntroductionIt is widely recognized that one of the best waysto learn a foreign language is through spoken di-alogue with native speakers (Ehsani and Knodt,1998).
However, this is not a practical method in theclassroom setting.
A potential solution to this prob-lem is to rely on computer spoken dialogue systemsto role play a tutor and/or a conversational partner.Ideally, a voice-interactive system can provide thelearner with endless opportunities for practice andfeedback.
However, while a number of dialogue sys-tems have been developed (or adapted) for languagelearning purposes (Seneff et al, 2004; Johnson etal., 2004), the issues of speech understanding of theaccented and disfluent utterances of a foreign lan-guage student typically lead to unacceptable perfor-mance (Eskenazi, 1999).A relatively successful application of speech pro-cessing technology is in the area of pronunciationtraining (Eskenazi, 1999; Witt, 1999; Hardison,2004).
In this case, a learner repeats words or sen-tences prompted by the computer, and receives feed-back on the segmental and suprasegmental qualityof their speech.
While such systems can be useful,they do not help the student learn how to formulatesentences within the language on their own.In this paper, we introduce a novel speech trans-lation game which aims to provide a fun and reli-able environment for language learners to gain pro-ficiency in communication.
Our application is com-plementary to pronunciation training in that we pro-vide the opportunity for the students to practice ex-pressing themselves in the new language.
Whilewe don?t explicitly evaluate their pronunciations, weprovide implicit feedback in that they must speakwell enough for the speech recognizer to success-fully process their utterances.
The translation exer-cise can also serve as a preparation stage in whichstudents can practice relevant vocabulary and sen-tence constructs, in order to prepare themselves forthe more challenging task of interacting with a dia-logue system in the same domain.468System: (greet and prompt user) Welcome!
You are playing at level four.
Okay, here?s your first sentence totranslate: The return date is Tuesday October 25.User: (translate with error) fan3 hui2 ri4 qi1 shi4 si4 yue4 er4 shi2 wu3 hao4 xing1 qi1 er4.System: (paraphrase in Chinese) fan3 hui2 ri4 qi1 si4 yue4 er4 shi2 wu3 hao4 xing1 qi1 er4.System: (paraphrase in English) The return date is Tuesday April 25.User: (repeat date) shi2 yue4 er4 shi2 wu3 hao4.System: (paraphrase in Chinese) shi2 yue4 er4 shi2 wu3 hao4.System: (paraphrase in English) October 25.System: (respond to user) ?Great!
You successfully translated the sentence in separate fragments.
Let?s moveon to the next sentence: I need to fly to Hong Kong the day after tomorrow.User: (click on the help button)System: (repeat English prompt) I need to fly to Hong Kong the day after tomorrow.System: (offer example translation) wo3 xu1 yao4 hou4 tian1 fei1 xiang1 gang3.User: (imitate the translation) wo3 xu1 yao4 hou4 tian1 fei1 xiang1 gang3.... ...System: (respond to user) You translated nine out of ten sentences.
You took on average 1.6 turns per sentence.You have advanced to level five.
Would you like to play another round?Figure 1: Example interaction between a user and the system.Our prototype centers on the task of translatingphrases and sentences from English into Chinese, inthe flight reservation domain.
As illustrated by theexample dialogue in Figure 1, the system role playsa language tutor interacting with a Mandarin learner.The system prompts the student with randomly gen-erated English sentences to elicit spoken Chinesetranslations from the learner.
The system para-phrases each user utterance in both languages, to im-plicitly inform the user of the system?s internal un-derstanding, and judges whether the student has suc-ceeded in the task.
The system keeps track of howmany turns a user takes to complete all the sentencesin a game session, and rewards good performance byadvancing the student towards higher difficulty lev-els.
A convenient ?help?
button allows the student torequest a translation of the current game sentence,to help them overcome gaps in their knowledge ofthe linguistic constructs or the vocabulary.
The stu-dent can also type any English sentences within thedomain to obtain a reference translation.
The sys-tem utilizes an interlingua-based bidirectional trans-lation capability, described in detail in (Wang andSeneff, 2006; Seneff et al, 2006).
Both Chinese andEnglish sentences are parsed into a common mean-ing representation, which we loosely refer to as an?interlingua,?
from which paraphrases in both lan-guages can be automatically generated using formalgeneration rules.The key to a successful tutoring system lies inits ability to provide immediate and pertinent feed-back on the student?s performance, similar to a hu-man tutor.
A central focus of this paper is to ad-dress the challenging problem of automatically as-sessing the appropriateness of a student?s transla-tion.
At first glance, our task appears to share muchin common with machine translation (MT) evalua-tion (Hovy et al, 2002).
Indeed, both are trying toassess the quality of the translation output, whetherit is produced by a computer or by a foreign lan-guage student.
Nevertheless, there also exist sev-eral fundamental distinctions.
Automatic MT eval-uation methods, as represented by the well-knownBleu metric (Papineni et al, 2001), assume the avail-ability of human reference translations.
The algo-rithms typically compare MT outputs with referencetranslations with the goal of producing a quality in-dicator (on a numeric scale) that correlates with hu-man judgement.
In contrast, our algorithm operatesin the absence of human generated reference trans-lations1 .
Furthermore, our application requires theevaluation algorithm to make accept/reject decisionson each individual translation, in the same way as alanguage tutor determines whether a translation isacceptable or not.
While our task is more demand-ing, it is made possible by operating in restricted do-mains.The remainder of the paper is organized as fol-lows.
In Section 2, we present an interlingua-basedapproach for verifying the correctness of the stu-dent?s spoken translation.
Section 3 describes the1We employ a grammar of recursive rewrite rules to generatea very large number of English prompt sentences.
It would betoo costly and time-consuming to generate human translationsto cover this space.469evaluation framework, followed by results and dis-cussions in Section 4.
Finally, we discuss futureplans for extending our work.2 MethodologyThe two most important aspects in the human eval-uation of translation quality are fluency and fi-delity (Hovy et al, 2002).
In our case, we con-sider a student?s translation to be acceptable if it iswell-formed (high fluency) and conveys the samemeaning as the input sentence (high fidelity).
Wedesigned our interlingua-based evaluation algorithmfollowing these two principles.
The algorithm usesparsability to verify fluency.
Fidelity is examinedby extracting and comparing semantic informationfrom the translation pairs.
In the following, we beginby describing the basic steps involved in our transla-tion verification algorithm.
We then discuss differ-ent strategies for integrating with the speech recog-nition system.2.1 ParsingOur framework depends strongly on an ability toparse both the English and Chinese sentences into acommon interlingual meaning representation.
Pars-ing is critical both for producing the two paraphrasesof the student?s utterance and for judging the qual-ity of their provided translation.
Both English andChinese grammars are needed to analyze the sourceand target sides of each translation pair.
The gram-mars have been carefully constructed so that mean-ing representations derived from both languages areas similar as feasible.We utilized a parser (Seneff, 1992) that is basedon an enhanced probabilistic context-free gram-mar (PCFG), which captures dependencies beyondcontext-free rules by conditioning on the externalleft-context parse categories when predicting thefirst child of each parent node.
While we use a spe-cific grammar for analyzing flight domain sentences,we emphasize domain portability of the grammar byusing mainly syntactic information in the majority ofthe parse tree rules.
Semantics are introduced nearthe terminals, mostly involving adjectives, verbs,nouns and proper noun classes.
Rules for generalsemantic concepts such as dates and times are orga-nized into sub-grammars that are easily embeddedinto any domain.
We have successfully applied thesame strategy in developing both the Chinese andEnglish grammars.
Once a parse tree is obtained, se-lected parse categories are extracted to form a hier-archical meaning representation encoding both syn-tactic and semantic information.2.2 Semantic Information ComparisonIn principle, we can directly compare the meaningrepresentations derived from the source and targetsides of the translation pair to determine their equiv-alence.
In practice, the meaning representation stillcaptures too much language-specific detail, whichmakes the comparison prone to failure.
Even thepair of English utterances, ?How much is the secondflight??
and ?What is the price of the second flight?
?have essentially the same meaning, but would notproduce identical meaning representations.
Acrosslanguages, this situation becomes much worse.We adopted two complementary strategies to in-crease the chance of a match between the Englishprompt and the student translation.
First, the Englishprompt is translated into a reference Chinese trans-lation using the existing interlingua translation capa-bility.
This extra step aims at reducing discrepanciescaused by syntactic structure differences betweenthe two languages.
Secondly, we abstract from theoriginal meaning representation into a simple en-coding of key-value (KV) pairs.
This is accom-plished using a language generation system (Baptistand Seneff, 2000), with generation rules determin-ing what information to extract from the original hi-erarchical meaning representation.
Figure 2 showsa couple of examples of the KV representation thatwe used for scoring.Another important role of the KV generation stepis to bring in a flexible mechanism for definingequivalence, which is a tricky task even for humanevaluators.
For example, while it is somewhat ob-vious that ?
(1) Give me flights leaving around ninep m?
is equivalent to ?
(2) Give me flights depart-ing around nine p m,?
it is unclear whether thesetwo sentences are equivalent to ?
(3) Give me flightsaround nine p m?
or even ?
(4) I would like to leavearound nine p m.?
From a pragmatic point of view,the same speaker intention can be inferred from thefour sentences.
On the other hand, it can be ar-gued that (1) and (2) are completely interchangeable470{c eform:topic "fare":airline_name "united":trace "how much" }lian2 he2 hang2 kong1 de5 piao4 jia4 shi4 duo1 shao3?
(What is the fare on United airlines?
){c eform:topic "flight":source "paris":departure_time {c eform:clock_hour 9:xm "a m" }}shang4 wu3 jiu3 dian3 cong2 ba1 li2 chu1 fa1 de5 ban1 ji1.
(Flights from Paris leaving at nine o?clock in the morning.
)Figure 2: Frame representation of the key-value in-formation for two example Chinese sentences.while (3) and (4) could not substitute for (1) or (2)in some circumstances.
Criteria for equivalence canbe controlled by what is extracted from the mean-ing representation.
If only a departure timekey is generated for the sentences, then all four sen-tences will be equivalent.
However, if more infor-mation is preserved in the KV pairs, for example, atopic key with value flight, then sentence (4)will not be considered as equivalent to sentences (1)-(3).
Considering that our intended application is lan-guage tutoring, we lean towards a stricter criterionfor defining equivalence.
The KV generation rulesare developed manually, guided by human-rated de-velopment data.
The KV inventory includes over 80unique keys.Once the KV pairs are obtained from the prompt(reference) and the student translation (hypothesis),a recursive procedure is applied to compare all thekeys in the reference and hypothesis KV frames.Mismatches are tabulated into substitutions (differ-ent values for the same key), deletions (extra keys inthe reference), and insertions (extra keys in the hy-pothesis).
A perfect match is achieved if there areno mismatch errors.
Figure 3 summarizes the proce-dure to evaluate students?
spoken translations.Partial match for a good student translation is acommon problem caused by speech recognition er-rors, particularly on dates and times.
It is naturalfor the student to just repeat the ?incorrect?
pieceafter noticing the error in the system?s paraphrases.Hence, in the tutoring application, we added a sub-match mode to the comparison algorithm, whichworks in a divide-and-conquer manner.
All match-ing KV pairs in each turn are checked off from thereference, and a subsequent submatch succeeds oncethere are no remaining KV pairs unaccounted for.One limitation of the incremental comparison algo-rithm is that it ignores insertion errors.
The tutoringsystem provides a special reply message when a sen-tence is translated via partial matches accomplishedover a series of utterances, to distinguish from thecase of a perfect match in a single turn, as illustratedin the example dialogue.2.3 Integration with Speech RecognitionA user?s utterance is first processed by the speechrecognizer to produce word hypotheses.
Therecognizer is configured from a segment-basedspeech recognition system (Glass, 2003), using Chi-nese acoustic models trained on native speakers?data (Wang et al, 2000a; Wang et al, 2000b).
Tonefeatures are ignored in the acoustic models; how-ever, the language model implicitly captures sometone constraints.
This is preferred over modelingtone explicitly, considering that non-native speak-ers typically make many tone errors.
The languagemodel was initially trained on Chinese translationsof English sentences generated from the templatesused in the game, and later augmented with addi-tional data collected from users.
The recognizer canoutput multiple hypotheses in the form of an N-bestlist.
The parser is able to convert the N-best list intoa lattice, and re-select a best hypothesis based on acombination of recognition and parsing scores.Poor recognition on non-native speech is a ma-jor performance issue for CALL application.
In ourdomain, dates, times, and flight numbers are particu-larly challenging entities for the recognizer.
Recog-nition error typically results in false rejection, caus-ing frustration to the user.
Since the system hasexplicit knowledge of the sentence the student istrying to produce, it should be feasible to exploitthis knowledge to improve speech understanding.
Aplausible strategy is to dynamically adjust the rec-ognizer?s language model in anticipation of whatthe user is likely to say, as exemplified by dialoguecontext dependent language models (Solsona et al,2002).In theory, we could use the automatically gener-471Key?ValueRulesEnglishGrammarChineseRulesChineseGrammarRecognitionPromptEnglishTranslationChineseHypothesisASRInterlingua MTKV ExtractionLanguageUnderstandingLanguageGenerationLanguageUnderstandingSpeechInterlinguaInterlinguaLanguageGeneration ComparatorKey?ValueRef KVHyp KVDecisionStudent?sSpoken TranslationFigure 3: Schematic of procedure to evaluate students?
spoken translations.ated reference translation to explicitly bias the lan-guage model.
However, one has to take care not tobias towards the correct response so strongly that thestudent is allowed to make mistakes with impunity.Furthermore, this strategy would not generalize tocover all the possible legitimate translations a stu-dent might produce for that prompt.
Instead, we de-vised a simple strategy that overcomes these issues.We select a preferred hypothesis from the N-best listif its KV representation matches the reference.
Thusthe student has to speak well enough for a correct an-swer to appear somewhere in the N -best list, with-out any manipulations of the recognizer?s languagemodel.
If the parser fails to find a perfect match inthe N-best list, it will choose the hypothesis with thebest score, or fall back to the recognizer?s top hy-pothesis if no parse theory could succeed.3 Evaluation FrameworkGiven a translation pair, the goal of our algorithm isto make the same accept/reject decision as a humanevaluator.
Hence, we can evaluate our algorithm ina classification framework.
In this section, we firstpresent the data collection and labeling effort.
Wethen describe a baseline system based on a variantof the Bleu metric.
Finally we briefly describe themetrics we used to evaluate our algorithms.3.1 Data Collection and LabelingDuring the course of developing a prototype gamesystem, two developers and two student testers inter-acted extensively with the system.
A total of 2527Chinese waveforms, recorded during this process,became development data for finding gaps in theinterlingua-based matching method and for tuningparameters for the baseline method.For evaluation, we use 1115 utterances collectedfrom 9 users with varying degrees of Chinese expo-sure.
These subjects were asked to play the transla-tion game over the Web and fill out a survey after-wards.
They came from a rich background of Chi-nese exposure, include advanced ?heritage?
speak-ers of Chinese (including dialects such as Cantoneseand Shanghainese), as well as novices who justcompleted two semesters of a college-level Chineseclass.The speech waveforms recorded from the interac-tions were manually transcribed with orthography,gender, and speaker information.
The transcriberwas instructed to transcribe spontaneous speech ef-fects, such as false starts and filled pauses.
However,tonal mispronunciations are completely ignored, andsegmental errors are largely ignored to the extentthat they do not result in a different syllable.The translation pairs (the English prompt and theorthographic transcription of the student translation)were rated independently by two bilingual speakersto provide reference labels for evaluating the verifi-cation algorithm.
The two raters, both native in Chi-nese and fluent in English, labelled each translationwith either an ?accept?
or a ?reject?
label.
Transla-tions can be rejected because of bad language usage(including false starts) or because of mismatches inmeaning.
One labeller rated both development andtest data, while the second labeller only rated the testdata.
The interlabeller agreement on the test data hasa kappa score (Uebersax, 1998) of 0.85.
The subsetof data for which there was disagreement were rela-belled by the two raters jointly to reach a consensus.3.2 BaselineThe Bleu metric has been widely accepted as aneffective means to automatically evaluate the qual-ity of machine translation outputs (Papineni et al,4722001).
An interesting question is whether it wouldbe useful for the purpose of assessing the appro-priateness of translations produced by non-nativespeakers at a sentence by sentence granularity level.We developed a simple baseline algorithm using theNIST score, which is a slight variation of Bleu2.Given an English prompt, the interlingua-based ma-chine translation system first produces a referencetranslation.
The student?s translation is then com-pared against the machine output to obtain a NISTscore.
The translation is accepted if the score ex-ceeds a certain threshold optimized on the develop-ment data.Figure 4 plots the Receiver Operating Character-istics (ROC) curve of the baseline algorithm, ob-tained by varying the NIST score acceptance thresh-old.
Each point on the curve represents a tradeoffbetween accepting an erroneous translation (FalseAccept) and rejecting a good one (False Reject).
Asshown in the plot, the NIST score based ROC curveis far from reaching the ideal top-left corner.
Forlanguage tutoring purposes, it is desirable to oper-ate in the low false acceptance region.
However, a20% false acceptance rate will result in the systemrejecting over 35% of correct student translations.The operating point that minimizes overall classifi-cation error turns out to be biased towards leniency,falsely accepting over 60% of translations that arerejected by human raters.
The resulting minimumerror rate on development data transcripts is 23.0%,with a NIST score threshold of 3.16.
The thresh-old for automatic speech recognition (ASR) outputswas optimized separately using the 1-best hypothe-ses of utterances in the development data.
The opti-mal threshold on ASR outputs is 1.60, resulting in aclassification error rate of 24.1%.
The majority clas-sifier, corresponding to the (1, 1) point on the curve,translates into a 31.6% error rate on the developmentdata.3.3 Evaluation MetricsWe evaluated the overall system performance on testdata using human decisions as ground truth.
Al-2We determined empirically that the NIST score worksslightly better than the Bleu score in our application.
Thescores are computed using the NIST MT scoring tool from:ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl0 0.2 0.4 0.6 0.8 100.20.40.60.81False Positive (False Accept)TruePostive(1?
False Reject)ROC on Dev Set TranscriptsROC curveMin Error PointFigure 4: ROC curve by changing acceptancethreshold on the NIST score on transcriptions of de-velopment data.though we can not generate an ROC curve for ourproposed algorithm (because it is a non-parametricmethod), we plot its performance along with theROC curve of the baseline system for a more thor-ough comparison.We evaluated the different ASR integration strate-gies (1-best hypothesis, 10-best hypotheses, usingcontextual constraints from reference KV) based onsentence classification error rates as well as speechrecognition performance.4 Results and DiscussionsTable 1 summarizes the false accept, false reject, andoverall classification error rates on unseen test data.With manual transcripts as inputs, the baseline al-gorithm using the NIST score achieved a classifica-tion error rate of 19.3%, as compared with 25.0%for the trivial case of always accepting the user sen-tence (Majority classifier).
The KV-based algorithmachieved a much better performance, with only a7.1% classification error rate.
This translates intoa kappa score of 0.86, which is slightly above thelevel of agreement initially achieved by the two la-bellers.
Note that the performance difference com-pared to the baseline system is mostly attributed to alarge reduction in the ?False Accept?
category.Interestingly, the NIST method degrades onlyslightly when it is applied to the speech recognition1-best output rather than the transcript.
However,this result is deceptive, as it is now even more bi-473False False ClassificationTranscript Reject Accept ErrorMajority 0.0% 100% 25.0%NIST 8.0% 54.5% 19.6%KV 7.3% 6.8% 7.2%False False ClassificationASR Reject Accept ErrorNIST 4.2% 77.1% 22.4%KV 1-best 32.1% 4.3% 25.1%KV 10-best 27.0% 7.2% 22.1%KV Context 13.5% 14.7% 13.8%Table 1: Classification results for various evaluationsystems, on both transcripts and automatic speechrecognition (ASR) outputs.
Note that the ?KV Con-text?
condition favors a hypothesis that matches theprompt KV.ased towards a ?False Accept?
strategy, causing overthree quarters of the students?
erroneous utterancesto be accepted.
The KV method is much more sus-ceptible to speech recognition error because of itsdeep linguistic analysis.
For instance, any recog-nition errors causing a parse failure will result ina ?reject?
decision, which explains the high errorrate when only the 1-best hypothesis is used.
How-ever, the KV algorithm can improve substantially bysearching the full N-best list (N = 10) for a plau-sible analysis.
When contextual information (KVContext) is used, our simple strategy of favoring thehypothesis matching the reference KV reduces theclassification error rate dramatically.A plot of the receiver operating characteristics ofthese methods in Figure 5 reveals a clear picture ofthe performance differences.
All of the KV pointsare clustered in the upper left corner of the plot,above the ROC curve of the NIST-based method.The NIST-score based classifier (represented by thesquare marker on the ROC curve) is heavily biasedtowards making the acceptance decision (the major-ity class).
In contrast, the KV method operates in thelow ?False Accept?
area.
It achieves a much lowerfalse rejection rate when compared with the NISTmethod operating at an equivalent false acceptancepoint.Although the classification error rate clearly im-0 0.2 0.4 0.6 0.8 100.20.40.60.81False Positive (False Accept)TruePostive(1?
False Reject)ROC on Test Set Transcripts and ASR OutputsNIST?transcriptKV?transcriptNIST?ASR (1?best)KV?ASR (1?best)KV?ASR (10?best)KV?ASR (context)Figure 5: Comparison of ROC of different methods.Syllable SentenceER(%) RR(%) ER(%) RR(%)1-best 11.6 - 40.4 -10-best 10.7 7.8 38.7 4.2Context 8.7 25.0 30.0 25.7Table 2: Comparison of speech recognition per-formance in syllable error rates and sentence errorrates, for three different strategies of utterance selec-tion from an N -best list.
(ER stands for error rate,RR stands for relative reduction.
)proves when the KV method makes use of the N-best list and incorporates contextual constraints, theROC plot seems to suggest that the error reductionmight simply be attributed to a shift in the operat-ing point: the improvements are caused by a biastowards making the majority class decision.
We useimprovements in speech recognition to demonstratethat this is not the case (at least not entirely).
Table 2summarizes the syllable and sentence error rates onthe test data, for the three configurations discussedpreviously (1-best, 10-best, and Context).
By usinga tighter integration with the parser with contextualconstraints, we greatly improved speech recognitionperformance, marked by reductions of syllable andsentence error rates by 25% and 25.7% respectively.5 Conclusions and Future WorkIn this paper, we have presented an algorithm for au-tomatically assessing spoken translations producedby language learners.
The evaluation results demon-474strated that our method involving deep linguisticanalysis of the translation pair can achieve high con-sistency with human decisions, and our strategy ofincorporating contextual constraints is effective atimproving speech recognition on non-native speech.While our solution is domain specific, we emphasizedomain portability in the linguistic analysis mod-ules, so that similar capabilities in other domains canbe quickly developed even in the absence of train-ing data.
Our interlingua framework also makes themethodology agnostic to the direction of source andtarget languages.
A similar application for nativeMandarin speakers learning English could be instan-tiated by using the same components for linguisticanalysis.A major challenge in our problem is in determin-ing equivalence between the meanings of a transla-tion pair.
While our approach of using a rule-basedgeneration system gives the developer great flexibil-ity in deriving an appropriate KV representation, thecomparison algorithm is somewhat primitive: it re-lies entirely on the generation rules to produce theright KV representation.
In future work, we planto apply machine learning techniques to this prob-lem.
With the data we have collected and labelled(and the effort is ongoing), it becomes feasible toexamine the use of data-driven methods.
As alludedto in our evaluation methodology, we can cast theproblem into a classification framework.
Lexical,n-gram, and alignment based features can be ex-tracted from the translation pairs, which can be fur-ther enhanced by features obtained from deep lin-guistic analysis.
This will relieve the burden on thesemantic analysis component, and improve the over-all portability of our approach.We also plan to expand our application to manyother domains appropriate for language learning,and test the effectiveness of the translation game asa means for language learning.6 AcknowledgementsThis research is supported in part by ITRI and theCambridge MIT Initiative.
The authors would like toacknowledge Yushi Xu for annotating the data.
Weare also grateful to Michael Collins and the anony-mous reviews for their helpful comments and sug-gestions.ReferencesL.
Baptist, S. Seneff.
2000.
Genesis-II: A versatile sys-tem for language generation in conversational systemapplications.
In Proc.
ICSLP, Beijing, China.D.
Ehsani, E. Knodt.
1998.
Speech technology incomputer-aided language learnings: Strengths andlimitations of a new call paradigm.
Language Learn-ing & Technology, 2(1):54?73.M.
Eskenazi.
1999.
Using automatic speech process-ing for foreign language pronunciation tutoring: Someissues and a prototype.
Language Learning & Tech-nology, 2(2):62?76.J.
Glass.
2003.
A probabilistic framework for segment-based speech recognition.
Computer Speech and Lan-guage, 17:137?152.D.
Hardison.
2004.
Generalization of computer-assistedprosody training: quantitative and qualitative findings.Language Learning & Technology, 8(1):34?52.E.
Hovy, M. King, A. Popescu-Belis.
2002.
Principlesof context-based machine translation evaluation.
Ma-chine Translation, 7(1):43?75.W.
L. Johnson, S. Marsella, H. Vihjalmsson.
2004.The DARWARS tactical language training system.
InProc.
I/ITSEC.K.
Papineni, S. Roukos, T. Ward, W.-J.
Zhu.
2001.BLEU: a method for automatic evaluation of machinetranslation.
In Proc.
ACL.S.
Seneff, C. Wang, J. Zhang.
2004.
Spoken conversa-tional interaction for language learning.
In Proc.
IN-STIL/CALL.S.
Seneff, C. Wang, J. Lee.
2006.
Combining linguis-tic and statistical methods for bi-directional EnglishChinese translation in the flight domain.
In Proc.
ofAMTA.S.
Seneff.
1992.
TINA: A natural language systemfor spoken language applications.
Computational Lin-guistics, 18(1).R.
A. Solsona, E. Fosler-Lussier, H.-K. J. Kuo,A.
Potamianos, I. Zitouni.
2002.
Adaptive languagemodels for spoken dialogue systems.
In ICASSP.J.
S. Uebersax.
1998.
Diversity of decision-making mod-els and the measurement of interrater agreement.
Psy-chological Bulletin, 101:140?146.C.
Wang, S. Seneff.
2006.
High-quality speech transla-tion in the flight domain.
In Proc.
of InterSpeech.C.
Wang, D. S. Cyphers, X. Mou, J. Polifroni, S. Sen-eff, J. Yi, V. Zue.
2000a.
MUXING: A telephone-access Mandarin conversational system.
In Proc.
IC-SLP, 715?718, Beijing, China.H.
C. Wang, F. Seide, C. Y. Tseng, L. S. Lee.
2000b.MAT2000 ?
Design, collection, and validation on aMandarin 2000-speaker telephone speech database.
InProc.
ICSLP, Beijing, China.S.
M. Witt.
1999.
Use of Speech Recognition inComputer-assisted Language Learning.
Ph.D. thesis,Department of Engineering, University of Cambridge,Cambridge, UK.475
