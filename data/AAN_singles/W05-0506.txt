Proceedings of the Second Workshop on Psychocomputational Models of Human Language Acquisition, pages 45?52,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsA Second Language Acquisition Model UsingExample Generalization and Concept CategoriesAri Rappoport Vera SheinmanInstitute of Computer Science Institute of Computer ScienceThe Hebrew University The Hebrew UniversityJerusalem, Israel Jerusalem, Israelarir@cs.huji.ac.il vera46@cl.cs.titech.ac.jpAbstractWe present a computational model of ac-quiring a second language from examplesentences.
Our learning algorithms build aconstruction grammar language model,and generalize using form-based patternsand the learner?s conceptual system.
Weuse a unique professional language learn-ing corpus, and show that substantial reli-able learning can be achieved even thoughthe corpus is very small.
The model is ap-plied to assisting the authoring of Japa-nese language learning corpora.1 IntroductionSecond Language Acquisition (SLA) is a centraltopic in many of the fields of activity related tohuman languages.
SLA is studied in cognitive sci-ence and theoretical linguistics in order to gain abetter understanding of our general cognitive abili-ties and of first language acquisition (FLA)1.
Gov-ernments, enterprises and individuals investheavily in foreign language learning due to busi-ness, cultural, and leisure time considerations.
SLAis thus vital for both theory and practice and shouldbe seriously examined in computational linguistics(CL), especially when considering the close rela-tionship to FLA and the growing attention devotedto the latter by the CL community.In this paper we present a computational modelof SLA.
As far as we could determine, this is thefirst model that simulates the learning process1 Note that the F stands here for ?First?, not ?Foreign?.computationally.
Learning is done from examples,with no reliance on explicit rules.
The model isunique in the usage of a conceptual system by thelearning algorithms.
We use a unique professionallanguage learning corpus, showing effective learn-ing from a very small number of examples.
Weevaluate the model by applying it to assisting theauthoring of Japanese language learning corpora.We focus here on basic linguistic aspects ofSLA, leaving other aspects to future papers.
In par-ticular, we assume that the learner possesses per-fect memory and is capable of invoking theprovided learning algorithms without errors.In sections 2 and 3 we provide relevant back-ground and discuss previous work.
Our input,learner and language models are presented in sec-tion 4, and the learning algorithms in section 5.Section 6 discusses the authoring application.2 BackgroundWe use the term ?second language acquisition?
torefer to any situation in which adults learn a newlanguage2.
A major concept in SLA theory[Gass01, Mitchell03] is that of interlanguage:when learning a new language (L2), at any givenpoint in time the learner has a valid partial L2 lan-guage system that differs from his/her native lan-guage(s) (L1) and from the L2.
The SLA process isthat of progressive enhancement and refinement ofinterlanguage.
The main trigger for interlanguagemodification is when the learner notices a gap be-tween interlanguage and L2 forms.
In order for thisto happen, the learner must be provided with com-2 Some SLA texts distinguish between ?second?
and ?foreign?and between ?acquisition?
and ?learning?.
We will not makethose distinctions here.45prehensible input.
Our model directly supports allof these notions.A central, debated issue in language acquisitionis whether FLA mechanisms [Clark03] are avail-able in SLA.
What is clear is that SL learners al-ready possess a mature conceptual system and arecapable of explicit symbolic reasoning and abstrac-tion.
In addition, the amount of input and timeavailable for FLA are usually orders of magnitudelarger than those for SLA.The general linguistic framework that we utilizein this paper is that of Construction Grammar(CG) [Goldberg95, Croft01], in which the buildingblocks of language are words, phrases and phrasetemplates that carry meanings.
[Tomasello03] pre-sents a CG theory of FLA in which children learnwhole constructions as ?islands?
that are graduallygeneralized and merged.
Our SLA model is quitesimilar to this process.In language education, current classroom meth-ods use a combination of formal rules and commu-nicative situations.
Radically different is thePimsleur method [Pimsleur05], an audio-basedself-study method in which rules and explanationsare kept to a minimum and most learning occurs byletting the learner infer L2 constructs from transla-tions of contextual L1 sentences.
Substantial anec-dotal evidence (as manifested by learner commentsand our own experience) suggests that the methodis highly effective.
We have used a Pimsleur cor-pus in our experiments.
One of the goals of ourmodel is to assist the authoring of such corpora.3 Previous WorkThere is almost no previous CL work explicitlyaddressing SLA.
The only one of which we areaware is [Maritxalar97], which represents interlan-guage levels using manually defined symbolicrules.
No language model (in the CL sense) orautomatic learning are provided.Many aspects of SLA are similar to first lan-guage acquisition.
Unsupervised grammar induc-tion from corpora is a growing CL research area([Clark01, Klein05] and references there), mostlyusing statistical learning of model parameters orpattern identification by distributional criteria.
Theresulting models are not easily presentable to hu-mans, and do not utilize semantics.
[Edelman04] presents an elegant FLA system inwhich constructions and word categories are iden-tified iteratively using a graph.
[Chang04] presentsan FLA system that truly supports constructiongrammar and is unique in its incorporation of gen-eral cognitive concepts and embodied semantics.SLA is related to machine translation (MT),since learning how to translate is a kind of acquisi-tion of the L2.
Most relevant to us here is modernexample-based machine translation (EBMT) [So-mers01, Carl03], due to its explicit computation oftranslation templates and to the naturalness oflearning from a small number of examples[Brown00, Cicekli01].The Computer Assisted Language Learning(CALL) literature [Levy97, Chapelle01] is rich inproject descriptions, and there are several commer-cial CALL software applications.
In general,CALL applications focus on teacher, environment,memory and automatization aspects, and are thuscomplementary to the goals that we address here.4 Input, Learner and Language Knowl-edge ModelsOur ultimate goal is a comprehensive computa-tional model of SLA that covers all aspects of thephenomenon.
The present paper is a first step inthat direction.
Our goals here are to: Explore what can be learned from exam-ple-based, small, beginner-level inputcorpora tailored for SLA; Model a learner having a mature concep-tual system; Use an L2 language knowledge modelthat supports sentence enumeration; Identify cognitively plausible and effectiveSL learning algorithms; Apply the model in assisting the author-ing of corpora tailored for SLA.In this section we present the first three compo-nents; the learning algorithms and the applicationare presented in the next two sections.4.1 Input ModelThe input potentially available for SL learners is ofhigh variability, consisting of meta-linguistic rules,usage examples isolated for learning purposes, us-age examples partially or fully understood in con-text, dictionary-like word definitions, free-formexplanations, and more.46One of our major goals is to explore the rela-tionship between first and second language acqui-sition.
Methodologically, it therefore makes senseto first study input that is the most similar linguis-tically to that available during FLA, usage exam-ples.
As noted in section 2, a fundamental propertyof SLA is that learners are capable of mature un-derstanding.
Input in our model will thus consist ofan ordered set of comprehensible usage exam-ples, where an example is a pair of L1, L2 sen-tences such that the former is a translation of thelatter in a certain understood context.We focus here on modeling beginner-level pro-ficiency, which is qualitatively different from na-tive-like fluency [Gass01] and should be studiedbefore the latter.We are interested in relatively small input cor-pora (thousands of examples at most), because thisis an essential part of SLA modeling.
In addition, itis of great importance, in both theoretical andcomputational linguistics, to explore the limits ofwhat can be learned from meager input.One of the main goals of SLA modeling is todiscover which input is most effective for SLA,because a substantial part of learners?
input can becontrolled, while their time capacity is small.
Wethus allow our input to be optimized for SLA, bycontaining examples that are sub-parts of otherexamples and whose sole purpose is to facilitatelearning those (our corpus is also optimized in thesense of covering simpler constructs and wordsfirst, but this issue is orthogonal to our model).
Weutilize two types of such sub-examples.
First, werequire that new words are always presented firston their own.
This is easy to achieve in controlledteaching, and is actually very frequent in FLA aswell [Clark03].
In the present paper we will as-sume that this completely solves the task of seg-menting a sentence into words, which is reasonablefor a beginner level corpus where the total numberof words is relatively small.
Word boundaries arethus explicitly and consistently marked.Second, the sub-example mechanism is also use-ful when learning a construction.
For example, ifthe L2 sentence is ?the boy went to school?
(wherethe L2 here is English), it could help learning algo-rithms if it were preceded by ?to school?
or ?theboy?.
Hence we do not require examples to becomplete sentences.In this paper we do not deal with phonetics orwriting systems, assuming L2 speech has beenconsistently transcribed using a quasi-phoneticwriting system.
Learning L2 phonemes is certainlyan important task in SLA, but most linguistic andcognitive theories view it as separable from the restof language acquisition [Fromkin02, Medin05].The input corpus we have used is a transcribedPimsleur Japanese course, which fits the inputspecification above.4.2 Learner ModelA major aspect of SLA is that learners already pos-sess a mature conceptual system (CS), influencedby their life experience (including languages theyknow).
Our learning algorithms utilize a CS model.We opted for being conservative: the model is onlyallowed to contain concepts that are clearly pos-sessed by the learner before learning starts.
Con-cepts that are particular to the L2 (e.g., ?noungender?
for English speakers learning Spanish) arenot allowed.
Examples for concept classes includefruits, colors, human-made objects, physical activi-ties and emotions, as well as meta-linguistic con-cepts such as pronouns and prepositions.
A singleconcept is simply represented by a prototypicalEnglish word denoting it (e.g., ?child?, ?school?).
Aconcept class is represented by the concepts it con-tains and is conveniently named using an Englishword or phrase (e.g., ?types of people?, ?buildings?,?language names?
).Our learners can explicitly reason about conceptinter-relationships.
Is-a relationships betweenclasses are represented when they are beyond anydoubt (e.g., ?buildings?
and ?people?
are both?physical things?
).A basic conceptual system is assumed to existbefore the SLA process starts.
When the input iscontrolled and small, as in our case, it is bothmethodologically valid and practical to prepare theCS manually.
CS design is discussed in detail insection 6.In the model described in the present  paper wedo not automatically modify the CS during thelearning process; CS evolution will be addressed infuture models.As stated in section 1, in this paper we focus onlinguistic SLA aspects and do not address issuessuch as human errors, motivation and attention.We thus assume that our learner possesses perfectmemory and can invoke our learning algorithmswithout any mistakes.474.3 Language Knowledge ModelWe require our model to support a basic capabilityof a grammar: enumeration of language sentences(parsing will be reported in other papers).
In addi-tion, we provide a degree of certainty for each.
Themodel?s quality is evaluated by its applicability forlearning corpora authoring assistance (section 6).The representation is based on constructiongrammar (CG), explicitly storing a set of construc-tions and their inter-relationships.
CG is ideallysuited for SLA interlanguage because it enables therepresentation of partial knowledge: every lan-guage form, from concrete words and sentences tothe most abstract constructs, counts as a construc-tion.
The generative capacity of language is ob-tained by allowing constructions to replacearguments.
For example, (child), (the child goes toschool), (<x> goes to school), (<x> <v> to school)and (X goes Z) are all constructions, where <x>,<v> denote word classes and X, Z denote otherconstructions.SL learners can make explicit judgments as totheir level of confidence in the grammaticality ofutterances.
To model this, our learning algorithmsassign a degree of certainty (DOC) to each con-struction and to the possibility of it being an argu-ment of another construction.
The certainty of asentence is a function (e.g., sum or maximum) ofthe DOCs present in its derivation path.Our representation is equivalent to a graphwhose nodes are constructions and whose directed,labeled arcs denote the possibility of a node fillinga particular argument of another node.
When thegraph is a-cyclic the resulting language contains afinite number of concrete sentences, easily com-puted by graph traversal.
This is similar to [Edel-man04]; we differ in our partial support forsemantics through a conceptual system (section 5)and in the notion of a degree of certainty.5 Learning AlgorithmsOur general SLA scheme is that of incrementallearning ?
examples are given one by one, eachcausing an update to the model.
A major goal ofour model is to identify effective, cognitively plau-sible learning algorithms.
In this section we presenta concrete set of such algorithms.Structured categorization is a major drivingforce in perception and other cognitive processes[Medin05].
Our learners are thus driven by the de-sire to form useful generalizations over the input.A generalization of two or more examples is possi-ble when there is sufficient similarity of form andmeaning between them.
Hence, the basic ingredi-ent of our learning algorithms is identifying suchsimilarities.To identify concrete effective learning algo-rithms, we have followed our own inference proc-esses when learning a foreign language from anexample-based corpus (section 6).
The set of algo-rithms described below are the result of this study.The basic form similarity algorithm is SingleWord Difference (SWD).
When two examplesshare all but a single word, a construction isformed in which that word is replaced by an argu-ment class containing those words.
For example,given ?eigo ga wakari mas?
and ?nihongo ga wakarimas?, the construction (<eigo, nihongo> ga wakarimas) (?I understand English/Japanese?
), containingone argument class, is created.
In itself, SWD onlycompresses the input, so its degree of certainty ismaximal.
It does not create new sentences, but itorganizes knowledge in a form suitable for gener-alization.The basic meaning-based similarity algorithm isExtension by Conceptual Categories (ECC).
Foran argument class W in a construction C, ECC at-tempts to find the smallest concept category U?that contains W?, the set of concepts correspondingto the words in W. If no such U?
exists, C is re-moved from the model.
If U?
was found, W is re-placed by U, which contains the L2 wordscorresponding to the concepts in U?.
When the re-placement occurs, it is possible that not all suchwords have already been taught; when a new wordis taught, we add it to all such classes U (easilyimplemented using the new word?s translation,which is given when it is introduced.
)In the above example, the words in W are ?eigo?and ?nihongo?, with corresponding concepts ?Eng-lish?
and ?Japanese?.
Both are contained in W?, the?language names?
category, so in this case U?equals W?.
The language names category containsconcepts for many other language names, includ-ing Korean, so it suffices to teach our learner theJapanese word for Korean (?kankokugo?)
at somepoint in the future in order to update the construc-tion to be (<eigo, nihongo, kankokugo> ga wakarimas).
This creates a new sentence ?kankokugo gawakari mas?
meaning ?I understand Korean?.
An48example in which U?
does not equal W?
is given inTable 1 by ?child?
and ?car?.L2 words might be ambiguous ?
several con-cepts might correspond to a single word.
Becauseexample semantics are not explicitly represented,our system has no way of knowing which conceptis the correct one for a given construction, so itconsiders all possibilities.
For example, the Japa-nese ?ni?
means both ?two?
and ?at/in?, so whenattempting to generalize a construction in which?ni?
appears in an argument class, ECC would con-sider both the ?numbers?
and ?prepositions?
con-cepts.The degree of certainty assigned to the new con-struction by ECC is a function of the quality of thematch between W and U?.
The more abstract is U,the lower the certainty.The main form-based induction algorithm isShared Prefix, Generated Suffix (SPGS).
Givenan example ?x y?
(x, y are word sequences), if thereexist (1) an example of the form ?x z?, (2) an ex-ample ?x?, and (3) a construction K that derives ?z?or ?y?, we create the construction (x K) having adegree of certainty lower than that of K. A SharedSuffix version can be defined similarly.
Require-ment (2) ensures that the cut after the prefix willnot be arbitrary, and assumes that the lesson authorpresents constituents as partial examples before-hand (as indeed is the case in our corpus).SPGS utilizes the learner?s current generativecapacity.
Assume input ?watashi wa biru o nomimas?
(?I drink beer?
), previous inputs ?watashi waamerica jin des?
(?I am American?
), ?watashi wa?
(?as to me...?)
and an existing construction K =(<biru, wain> o nomi mas).
SPGS would create theconstruction (watashi wa K), yielding the new sen-tence ?watashi wa wain o nomi mas?
(?I drinkwine?
).To enable faster learning of more abstract con-structions, we use generalized versions of SWDand SPGS, which allow the differing or sharedelements to be a construction rather than a word ora word sequence.The combined learning algorithm is: given anew example, iteratively invoke each of the abovealgorithms at the given order until nothing new canbe learned.
Our system is thus a kind of inductiveprogramming system (see [Thompson99] for a sys-tem using inductive logic programming for seman-tic parsing).Note that the above algorithms treat words asatomic units, so they can only learn morphologicalrules if boundaries between morphemes aremarked in the corpus.
They are thus more usefulfor languages such as Japanese than, say, for Ro-mance or Semitic languages.Our algorithms have been motivated by generalcognitive considerations.
It is possible to refinethem even further, e.g.
by assigning a higher cer-tainty when the focus element is a prefix or a suf-fix, which are more conspicuous cognitively.6 Results and Application to Authoring ofLearning CorporaWe have experimented with our model using thePimsleur Japanese I (for English speakers) course,which comprises 30 half-hour lessons, 1823 differ-ent examples, and about 350 words.
We developeda simple set of tools to assist transcription, using anarbitrary, consistent Latin script transliterationbased on how the Japanese phonemes are pre-sented in the course, which differs at places fromcommon transliterations (e.g., we use ?mas?, not?masu?).
Word boundaries were marked duringtransliteration, as justified in section 4.Example sentences from the corpus are ?nani oshi mas kaa ?
/ what are you going to do?
?, ?wa-tashi ta chi wa koko ni i mas / we are here?, ?kyowa kaeri masen / today I am not going back?,?demo hitori de kaeri mas / but I am going to returnalone?, etc.
Sentences are relatively short and ap-propriate for a beginner level learner.Evaluating the quality of induced languagemodels is notoriously difficult.
Current FLA prac-tice favors comparison of predicted parses withones in human annotated corpora.
We have fo-cused on another basic task of a grammar, sentenceenumeration, with the goal of showing that ourmodel is useful for a real application, assistance forauthoring of learning corpora.The algorithm has learned 113 constructionsfrom the 1823 examples, generating 525 new sen-tences.
These numbers do not include construc-tions that are subsumed by more abstract ones(generating a superset of their sentences) or thoseinvolving number words, which would distort thecount upwards.
The number of potential new sen-tences is much higher: these numbers are basedonly on the 350 words present, organized in arather flat CS.
The constructions contain many49placeholders for concepts whose words would betaught in the future, which could increase the num-ber exponentially.In terms of precision, 514 of the 525 sentenceswere judged (by humans) to be syntactically cor-rect (53 of those were problematic semantically).Regarding recall, it is very difficult to assess for-mally.
Our subjective impression is that the learnedconstructions do cover most of what a reasonableperson would learn from the examples, but this isnot highly informative ?
as indicated, the algo-rithms were discovered by following our own in-herence processes.
In any case, our algorithmshave been deliberately designed to be conservativeto ensure precision, which we consider more im-portant than recall for our model and application.There is no available standard benchmark toserve as a baseline, so we used a simpler version ofour own system as a baseline.
We modified ECC tonot remove C in case of failure of concept match(see ECC?s definition in section 5).
The number ofconstructions generated after seeing 1300 exam-ples is 3,954 (yielding 35,429 sentences), almostall of which are incorrect.The applicative scenario we have in mind is thefollowing.
The corpus author initially specifies thedesired target vocabulary and the desired syntacti-cal constructs, by writing examples (the easiestinterface for humans).
Vocabulary is selected ac-cording to linguistic or subject  (e.g., tourism,sports) considerations.
The examples are fed oneby one into the model (see Table 1).
For a singleword example, its corresponding concepts are firstmanually added to the CS.The system now lists the constructions learned.For a beginner level and the highest degree of cer-tainty, the sentences licensed by the model can beeasily grasped just by looking at the constructions.The fact that our model?s representations can beeasily communicated to people is also an advan-tage from an SLA theory point of view, where ?fo-cus on form?
is a major topic [Gass01].
Foradvanced levels or lower certainties, viewing thesentences themselves (or a sample, when theirnumber gets too large) might be necessary.The author can now check the learned items forerrors.
There are two basic error types, errorsstemming from model deficiencies and errors thathuman learners would make too.
As an example ofthe former, wrong generalizations may result fromdiscrepancies between the modeled conceptual sys-tem and that of a real person.
In this case the au-thor fixes the modeled CS.
Discovering errors ofthe second kind is exactly the point where themodel is useful.
To address those, the author usu-ally introduces new full or partial examples thatwould enable the learner to induce correct syntax.In extreme cases there is no other practical choicebut to provide explicit linguistic explanations inorder to clarify examples that are very far from thelearner?s current knowledge.
For example, Englishspeakers might be confused by the variability ofthe Japanese counting system, so it might be usefulto insert an explanation of the sort ?X is usuallyused when counting long and thin objects, but beaware that there are exceptions?.
In the scenario ofTable 1, the author might eventually notice that thelearner is not aware that when speaking of some-body else?s child a more polite reference is in or-der, which can be fixed by giving examplesfollowed by an explanation.
The DOC can be usedto draw the author?s attention to potential prob-lems.Preparation of the CS is a sensitive issue in ourmodel, because it is done manually while it is notclear at all what kind of CS people have (WordNetis sometimes criticized for being arbitrary, too fine,and omitting concepts).
We were highly conserva-tive in that only concepts that are clearly part of theconceptual system of English speakers before anyexposure to Japanese were included.
Our task ismade easier by the fact that it is guided by wordsactually appearing in the corpus, whose number isnot large, so that it took only about one hour toproduce a reasonable CS.
Example categories arenames (for languages, places and people), places(park, station, toilet, hotel, restaurant, shop, etc),people (person, friend, wife, husband, girl, boy),food, drink, feelings towards something (like,need, want), self motion activities (arrive, come,return), judgments of size, numbers, etc.
We alsoincluded language-related categories such as pro-nouns and prepositions.7 DiscussionWe have presented a computational model of sec-ond language acquisition.
SLA is a central subjectin linguistics theory and practice, and our maincontribution is in addressing it in computationallinguistics.
The model?s learning algorithms areunique in their usage of a conceptual system, and50its generative capacity is unique in its support fordegrees of certainty.
The model was tested on aunique corpus.The dominant trend in CL in the last years hasbeen the usage of ever growing corpora.
We haveshown that meaningful learning can be achievedfrom a small corpus when the corpus has been pre-pared by a ?good teacher?.
Automatic identification(and ordering) of corpora subsets from whichlearning is effective should be a fruitful researchdirection for CL.We have shown that using a simple conceptualsystem can greatly assist language learning algo-rithms.
Previous FLA algorithms have in effectcomputed a CS simultaneously with the syntax;decoupling the two stages could be a promisingdirection for FLA.The model presented here is the first computa-tional SLA model and obviously needs to be ex-tended to address more SLA phenomena.
It is clearthat the powerful notion of certainty is only used ina rudimentary manner.
Future research should alsoaddress constraints (e.g.
for morphology and agree-ment), recursion, explicit semantics (e.g.
parsinginto a semantic representation), word segmenta-tion, statistics (e.g.
collocations), and induction ofnew concept categories that result from the learnedlanguage itself (e.g.
the Japanese counting system).An especially important SLA issue is L1 trans-fer, which refers to the effect that the L1 has on thelearning process.
In this paper the only usage of theL1 part of the examples was for accessing a con-ceptual system.
Using the L1 sentences (and theexisting conceptual system) to address transfer isan interesting direction for research, in addition tousing the L1 sentences for modeling sentence se-mantics.Many additional important SLA issues will beaddressed in future research, including memory,errors, attention, noticing, explicit learning, andmotivation.
We also plan additional applications,such as automatic lesson generation.Acknowledgement.
We would like to thank DanMelamed for his comments on a related document.ReferencesBrown Ralf, 2000, Automated Generalization of Trans-lation Examples, COLING ?00.Carl Michael, Way Andy, (eds), 2003, Recent Advancesin Example Based Machine Translation, Kluwer.Chang Nancy, Gurevich Olya, 2004.
Context-DrivenConstruction Learning.
Proceedings, Cognitive Sci-ence ?04.Chapelle Carol, 2001.
Computer Applications in SLA.Cambridge University Press.
.Cicekli Ilyas, Gu?venir Altay, 2001, Learning Transla-tion Templates from Bilingual Translational Exam-ples.
Applied Intelligence 15:57-76, 2001.Clark Alexander, 2001.
Unsupervised Language Acqui-sition: Theory and Practice.
PhD thesis, University ofSussex.Clark Eve Vivienne, 2003.
First Language Acquisition.Cambridge University Press.Croft, William, 2001.
Radical Construction Grammar.Oxford University Press.Edelman Shimon, Solan Zach, Horn David, RuppinEytan, 2004.
Bridging Computational, Formal andPsycholinguistic Approaches to Language.
Proceed-ings, Cognitive Science ?04.Fromkin Victoria, Rodman Robert, Hyams Nina, 2002.An Introduction to Language, 7th ed.
Harcourt.Gass Susan M, Selinker Larry, 2001.
Second LanguageAcquisition: an Introductory Course.
2nd ed.
LEAPublishing.Goldberg Adele, 1995.
Constructions: a ConstructionGrammar Approach to Argument Structure.
ChicagoUniversity Press.Klein Dan, 2005.
The Unsupervised Learning of NaturalLanguage Structure.
PhD Thesis, Stanford.Levy Michael, 1997.
Computer-Assisted LanguageLearning.
Cambridge University Press.Maritxalar Montse, Diaz de Ilarraza Arantza, OronozMaite, 1997.
From Psycholinguistic Modelling of In-terlanguage in SLA to a Computational Model.CoNLL ?97.Medin Douglas, Ross Brian, Markman Arthur, 2005.Cognitive Psychology, 4th ed.
John Wiley & Sons.Mitchell Rosamond, Myles Florence, 2003.
SecondLanguage Learning Theories.
2nd ed.
Arnold Publica-tion.Pimsleur 2005. www.simonsays.com, under ?foreignlanguage instruction?.Somers Harold, 2001.
Example-based Machine Transla-tion.
Machine Translation 14:113-158.Thompson Cynthia, Califf Mary Elaine, Mooney Ray-mond, 1999.
Active Learning for Natural LanguageParsing and Information Extraction.
ICML ?99.Tomasello Michael, 2003.
Constructing a Language: aUsage Based Theory of Language Acquisition.
Har-vard University Press.51Construction  DOC Source  Comment1 anata / you 0 example2 watashi / I  0 example3 anata no / your 0 example4 watashi no / my 0 example5 (<anata,watashi> no ) 0 SWD(3,4) The first words of 3 and 4 are different, therest is identical.6 (W no), where W is <anata,watashi, Japanese word for?we?>-1 ECC(5)  The concept category W?={I, you, we} wasfound in the CS.
We know how to say ?I?
and?you?, but not ?we?.7 watashi ta chi / we  0 example8 (W no), where W is<anata, watashi, watashi tachi>-2 ECC(6,7) We were taught how to say ?we?, and anempty slot for it was found in 6.Now we can generate a new sentence: ?wa-tashi ta chi no?, whose meaning (?our?)
isinferred from the meaning of construction 6.9 chiisai / small  0 example10 kuruma / car 0 example11 chiisai kuruma / a small car 0 example12 watashi ta chi no kuruma / ourcar0 example13 ((W no) kuruma) -3 SSGP (12,11, 10, 8)Shared Suffix Generated Prefix:(0) new example 12 = ?y x?
(x: kuruma)(1) existing example 11 = ?z x?
(2) existing example 10 = ?x?
(3) construction K (#8) deriving ?y?learns the new construction (K x)Now we can generate a new sentence: ?wa-tashi no kuruma?, meaning ?my car?.14 kodomo / child 0 example... ... 0 examples Skipping a few examples...20 ((W no) kodomo) -3 ...
This construction was learned using theskipped examples.21 ((W no) <kuruma, kodomo>) -3 SWD (13,20)Note that the shared element is a constructionthis time, not a sub-sentence.22 ((W no) P), where P is the setof Japanese words for physi-cal things (animate or inani-mate)-4 ECC (21) The smallest category that contains the con-cepts ?car?
and ?child?
is P?=PhysicalThings.Now we can generate many new sen-tences, meaning ?my X?
where X is anyJapanese word we will learn in the futuredenoting a physical thing.Table 1: A learning scenario.
For simplicity, the degree of certainty here is computed by adding that of the algorithmtype to that of the most uncertain construction used.
Note that the notation used was designed for succinct presen-tation and is not the optimal one for authors of learning corpora (for example, it is probably easier to visualize thesentences generated by construction #22 if it were shown as ((<watashi, anata, watashi ta chi> no) <kuruma,kodomo>).
)52
