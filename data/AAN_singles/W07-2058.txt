Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 268?271,Prague, June 2007. c?2007 Association for Computational LinguisticsPSNUS: Web People Name Disambiguation by Simple Clusteringwith Rich FeaturesErgin Elmacioglu1 Yee Fan Tan2 Su Yan1 Min-Yen Kan2 Dongwon Lee11The Pennsylvania State University, USA2National University of Singapore, Singapore{ergin,syan,dongwon}@psu.edu, {tanyeefa,kanmy}@comp.nus.edu.sgAbstractWe describe about the system description ofthe PSNUS team for the SemEval-2007 WebPeople Search Task.
The system is basedon the clustering of the web pages by us-ing a variety of features extracted and gen-erated from the data provided.
This systemachieves F?=0.5 = 0.75 and F?=0.2 = 0.78for the final test data set of the task.1 IntroductionWe consider the problem of disambiguating personnames in a Web searching scenario as described bythe Web People Search Task in SemEval 2007 (Ar-tiles et al, 2007).
Here, the system receives as in-put a set of web pages retrieved from a search en-gine using a given person name as a query.
The goalis to determine how many different people are rep-resented for that name in the input web pages, andcorrectly assign each namesake to its correspondingsubset of web pages.There are many challenges towards an effectivesolution.
We are to correctly estimate the number ofnamesakes for a given person name and group doc-uments referring to the same individual.
Moreover,the information sources to be processed are unstruc-tured web pages and there is no certain way of cor-rectly establishing a relation between any two webpages belonging to the same or different individuals.We have taken several approaches to analyze dif-ferent sources of information provided with the in-put data, and also compared strategies to combinethese individual features together.
The configurationthat achieved the best performance (which were sub-mitted for our run) used a single named entity fea-ture as input to clustering.
In the remainder of thispaper, we first describe our system in terms of theclustering approach used and alternative features in-vestigated.
We then analyze the results on the train-ing set before concluding the paper.2 Clustering AlgorithmClustering is the key part for such a task.
We havechosen to view the problem as an unsupervised hardclustering problem.
First, we view the problem asunsupervised, using the training data for parametervalidation, to optimally tune the parameters in theclustering algorithm.
Secondly, we observed that themajority of the input pages reference a single indi-vidual, although there are a few that reference mul-tiple individuals sharing the same name.
Hence, weview the problem as hard clustering, assigning inputpages to exactly one individual, so that the producedclusters do not overlap.Hard clustering algorithms can be classified as ei-ther partitive or hierarchical.
Agglomerative hierar-chical clustering generates a series of nested clustersby merging simple clusters into larger ones, whilepartitive methods try to find a pre-specified num-ber of clusters that best capture the data.
As thecorrect number of clusters is not given a priori, wechose a method from the second group.
We use theHierarchical Agglomerative Clustering (HAC) algo-rithm (Jain et al, 1999) for all experiments reportedin this paper.
HAC views each input web page asa separate cluster and iteratively combines the mostsimilar pair of clusters to form a new cluster that re-268places the pair.3 FeaturesAs input to the clustering, we consider several dif-ferent representations of the input documents.
Eachrepresentation views the input web pages as a vectorof features.
HAC then computes the cosine similar-ity between the feature vectors for each pair of clus-ters to determine which clusters to merge.
We nowreview the inventory of features studied in our work.Tokens (T).
Identical to the task baseline by (Ar-tiles et al, 2005), we stemmed the words in the webpages using the Porter stemmer (Porter, 1980), toconflate semantically similar English words with thestem.
Each stemmed word is considered to be a fea-ture and weighted by its Term Frequency ?
InverseDocument Frequency (TF?IDF).Named Entities (NE).
We extract the named enti-ties from the web pages using the Stanford NamedEntity Recognizer (Finkel et al, 2005).
This taggeridentifies and labels names of places, organizationsand people in the input.
Each named entity tokenis treated as a separate feature, again weighted byTF?IDF.
We do not perform stemming for NE fea-tures.We also consider a more target-centric form ofthe NE feature, motivated by the observation thatperson names can be differentiated using their mid-dle names or titles.
We first discard all named enti-ties that do not contain any token of the search tar-get, and then discard any token from the remain-ing named entities that appears in the search tar-get.
The remaining tokens are then used as features,and weighted by their TF?IDF.
For example, for thesearch target ?Edward Fox?, the features generatedfrom the name ?Edward Charles Morrice Fox?
are?Charles?
and ?Morrice?.
We call this variation NEtargeted (NE-T).Hostnames and domains (H and D).
If twoweb pages have links pointing to the exact sameURL, then there is a good chance that these twoweb pages refer the same person.
However, wefind such exact matches of URLs are rare, sowe relax the condition and consider their host-names or domain names instead.
For example, theURL http://portal.acm.org/guide.cfm has host-name portal.acm.org and domain name acm.org.As such, for each web page, we can extract the listof hostnames from the links in this page.We observe that some host/domain names serveas more discriminative evidence than others (e.g.,a link to a university homepage is more tellingthan a link to the list of publications page ofGoogle Scholar when disambiguating computer sci-ence scholars).
To model this, we weight eachhost/domain name by its IDF.
Note that we do notuse TF as web pages often contain multiple inter-nal links in the form of menus or navigation bars.Using IDF and cosine similarity has been proveneffective for disambiguating bibliographic citationrecords sharing a common author name (Tan et al,2006).We also considered a variant where we includethe URL of the input web page itself as a ?link?.
Wetried this variation only with hostnames, calling thisHost with Self URL (H-S).Page URLs (U).
Uniform resource locations(URLs) themselves contain a rich amountof information.
For example, the URLhttp://www.cs.ualberta.ca/?lindek/ itself sug-gests a home page of ?lindek?
in the ComputerScience department, University of Alberta, Canada.We used the MeURLin system (Kan and NguyenThi, 2005) to segment the URL of each web pageinto tokens as well as to generate additional fea-tures.
These features include (a) segmentation oftokens such as ?www.allposters.com?
to ?www?,?all?, ?posters?
and ?com?
; (b) the parts in the URLwhere the tokens occur, e.g., protocol, domain name,and directory paths; (c) length of the tokens; (d) or-thographic features; (e) sequential n-grams; and (f)sequential bigrams.
As each of these features can beseen as a ?token?, the output of the MeURLin seg-menter for a web page can be seen as a ?document?,and hence it is possible to compute the TF?IDF co-sine similarity between two such documents.3.1 Feature CombinationThe features described above represent largely or-thogonal sources of information in the input: inputcontent, hyperlinks, and source location.
We hy-pothesize that by combining these different featureswe can obtain better performance.
To combine thesefeatures for use with HAC, we consider simply con-catenating individual feature vectors together to cre-269ate a single feature vector, and compute cosine sim-ilarity.
We used this method in two configurations:namely, (T + NE + H-S), (T + D + NE + NE-T + U).We also tried using the maximum and averagecomponent-wise similarities of individual features.
(max(NE, H-S)) uses the maximum value of theNamed Entity and Host with Self features.
For the(avg(T, H-S)) and (avg(T, D, NE, NE-T, U)) runs,we compute the average similarity over the two andfive sets of individual features, respectively.4 ResultsWe present the clustering performances of the var-ious methods in our system based on the differentfeatures that we extracted.
Each experiment usesHAC with single linkage clustering.
Since the num-ber of clusters is not known, when to terminate theagglomeration process is a crucial point and signifi-cantly affects the quality of the clustering result.
Weempirically determine the best similarity thresholdsto be 0.1 and 0.2 for all the experiments on the threedifferent data sets provided.
We found that largervalues for these data sets do not allow the HAC algo-rithm to create enough clustering hierarchy by caus-ing it to terminate early, and therefore result in manysmall clusters increasing purity but dramatically suf-fering from inverse purity performance.Table 1 shows the results of our experiments onthe training data sets (ECDL, Wikipedia and Cen-sus).
Two different evaluation measures are reportedas described by the task: F?=0.5 is a harmonic meanof purity and inverse purity of the clustering result,and F?=0.2 is a version of F that gives more impor-tance to inverse purity (Artiles et al, 2007).Among the individual features, Tokens andNamed Entity features consistently show close tobest performance for all training data sets.
In mostcases, NE is better than Tokens because some webpages contain lots of irrelevant text for this task (e.g.,headers and footers, menus etc).
Also, we found thatthe NEs have far more discriminative power thanmost other tokens in determining similarity betweenweb pages.
The NE variation, NE targeted, performsworse among the token based methods.
AlthoughNE targeted aims for highly precise disambiguation,it seems that it throws away too much informationso that inverse purity is very much reduced.
Theother NEs, such as locations and organizations arealso very helpful for this task.
For example, the or-ganization may indicate the affiliation of a particularname.
This explains the superiority of NE over NEtargeted for all three data sets.Among the link based features, Domain gives bet-ter performance over Host as it leads to better in-verse purity.
The reason is that there are usuallymany pages on different hosts from a single domainfor a given name (e.g., the web pages belonging toa researcher from university domain).
This greatlyhelps in resolving the name while results in a slightdrop in purity.
Using a web page?s URL itself in thefeatures Host+Self and Domain+Self shows a largerincrease in inverse purity at a smaller decrease in pu-rity, hence these have improved F-measure in com-parison to Domain and Host.
Not surprisingly, theselink based features perform very well for the ECDLdata set, compared to the other two.
A significantportion of the people in the ECDL data set are mostlikely present-day computer scientists, likely havingextensive an web presence, which makes the taskmuch easier.
Although the other two data sets mayhave popular people with many web pages, theirweb presence are usually created by others and oftenscatter across many domains with little hyperlink-age between them.
This explains why our link basedmethods are not very effective for such data sets.Our final individual feature URL performs worstamong all.
Although highly precise, its resulting in-verse purity is poor.
While the features generatedby MeURLin do improve the performance over purehost name and domain on the page URLs, its incor-poration in a richer feature set does not lead to betterresults, as the other features which have richer infor-mation to process.Each of the individual features has different de-gree of discriminative power in many differentcases.
By combining them, we expect to get bet-ter performance than individually.
However, we donot obtain significant improvement in any of the datasets.
Furthermore, in the Census data set, the com-bined features fail to outperform the individual NEand Tokens features.
The relatively poor perfor-mance of the remaining features also degrades theperformance of Tokens and NE when combined.Considering the performances using the harmonicmean, we do not see any clear winner in all of three270Feature ECDL Wikipedia CensusF?=0.5 F?=0.2 F?=0.5 F?=0.2 F?=0.5 F?=0.2Tokens (T) .72 / .77 .83 / .84 .72 / .76 .85 / .84 .82 / .84 .88 / .86Named Entities (NE) .75 / .80 .84 / .79 .75 / .77 .85 / .78 .89 / .78 .89 / .73NE targeted (NE-T) .54 / .55 .49 / .47 .66 / .64 .60 / .57 .64 / .64 .57 / .58Host (H) .72 / .57 .64 / .48 .67 / .51 .58 / .41 .67 / .63 .59 / .55Host + Self (H-S) .73 / .59 .66 / .49 .68 / .54 .60 / .43 .68 / .63 .60 / .56Domain (D) .78 / .69 .72 / .60 .71 / .59 .66 / .50 .69 / .65 .61 / .58Domain + Self (D-S) .79 / .70 .74 / .61 .72 / .62 .67 / .52 .70 / .66 .62 / .59URL (U) .50 / .43 .43 / .35 .56 / .42 .50 / .33 .64 / .58 .56 / .51(T + NE + H-S) .71 / .77 .83 / .83 .72 / .76 .85 / .83 .65 / .67 .78 / .76(T + D + NE + NE-T + U) .72 / .76 .83 / .80 .72 / .77 .84 / .83 .66 / .66 .78 / .74(max(NE, H-S)) .74 / .80 .84 / .82 .74 / .77 .86 / .82 .71 / .66 .80 / .70(avg(T, H-S)) .77 / .81 .86 / .76 .75 / .77 .86 / .76 .70 / .64 .80 / .67(avg(T, D, NE, NE-T, U)) .78 / .77 .86 / .73 .75 / .78 .86 / .76 .69 / .61 .77 / .62Table 1: Experimental results for each training data set of the task: ECDL, Wikipedia and Census.
Eachexperiment uses single link HAC with the similarity threshold values of 0.1 / 0.2.
Best F?=0.5 performancesare shown in bold.training data sets.
In addition, the method showingthe best performance does not result in a win witha large margin in each data set.
Relatively com-plicated methods do not always perform better oversimpler, single featured based methods on all train-ing data sets.
Considering the results and Occam?srazor (Thorburn, 1915), we conclude that a simplemethod should most likely work relatively well inmany other different settings as well.
Therefore, weselected the method based on the individual NE fea-ture with the similarity threshold value of 0.2 for thefinal test submission run.
We are able to achievethe following results for this submission run: pu-rity = 0.73, inverse purity = 0.82, F?=0.5 = 0.75,F?=0.2 = 0.78.5 ConclusionWe described our PSNUS system that disambiguatespeople mentions in web pages returned by a websearch scenario, as defined in the inaugural WebPeople Search Task.
As such, we mainly focus onextracting various kinds of information from webpages and utilizing them in the similarity computa-tion of the clustering algorithm.
The experimentalresults show that a simple Hierarchical Agglomera-tive Clustering approach using a single named entityfeature seems promising as a robust solution for thevarious datasets.ReferencesJavier Artiles, Julio Gonzalo, and Felisa Verdejo.
2005.A testbed for people searching strategies in the WWW.In ACM SIGIR, pages 569?570, August.Javier Artiles, Julio Gonzalo, and Satoshi Sekine.
2007.The SemEval-2007 WePS evaluation: Establishing abenchmark for the Web People Search Task.
In Se-mEval 2007, ACL, June.Jenny R. Finkel, Trond Grenager, and Christopher Man-ning.
2005.
Incorporating non-local information intoinformation extraction systems by Gibbs sampling.
InACL, pages 363?370, June.Anil K. Jain, M. Narasimha Murty, and Patrick J. Flynn.1999.
Data clustering: A review.
ACM ComputingSurveys, 31(3):264?323, September.Min-Yen Kan and Hoang Oanh Nguyen Thi.
2005.
Fastwebpage classification using URL features.
In CIKM,pages 325?326, October/November.Martin F. Porter.
1980.
An algorithm for suffix stripping.Program, 14(3):130?137, July.Yee Fan Tan, Min-Yen Kan, and Dongwon Lee.
2006.Search engine driven author disambiguation.
InACM/IEEE JCDL, pages 314?315, June.William M. Thorburn.
1915.
Occam?s razor.
Mind,24:287?288.271
