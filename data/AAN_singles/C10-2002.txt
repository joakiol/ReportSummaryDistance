Coling 2010: Poster Volume, pages 9?17,Beijing, August 2010Document Expansion Based on WordNetfor Robust IREneko AgirreIXA NLP GroupUniv.
of the Basque Countrye.agirre@ehu.esXabier ArregiIXA NLP GroupUniv.
of the Basque Countryxabier.arregi@ehu.esArantxa OtegiIXA NLP GroupUniv.
of the Basque Countryarantza.otegi@ehu.esAbstractThe use of semantic information to im-prove IR is a long-standing goal.
This pa-per presents a novel Document Expansionmethod based on a WordNet-based systemto find related concepts and words.
Ex-pansion words are indexed separately, andwhen combined with the regular index,they improve the results in three datasetsover a state-of-the-art IR engine.
Consid-ering that many IR systems are not robustin the sense that they need careful fine-tuning and optimization of their parame-ters, we explored some parameter settings.The results show that our method is spe-cially effective for realistic, non-optimalsettings, adding robustness to the IR en-gine.
We also explored the effect of doc-ument length, and show that our methodis specially successful with shorter docu-ments.1 IntroductionSince the earliest days of IR, researchers notedthe potential pitfalls of keyword retrieval, suchas synonymy, polysemy, hyponymy or anaphora.Although in principle these linguistic phenom-ena should be taken into account in order to ob-tain high retrieval relevance, the lack of algo-rithmic models prohibited any systematic studyof the effect of this phenomena in retrieval.
In-stead, researchers resorted to distributional se-mantic models to try to improve retrieval rele-vance, and overcome the brittleness of keywordmatches.
Most research concentrated on QueryExpansion (QE) methods, which typically ana-lyze term co-occurrence statistics in the corpusand in the highest scored documents for the orig-inal query in order to select terms for expandingthe query terms (Manning et al, 2009).
Docu-ment expansion (DE) is a natural alternative toQE, but surprisingly it was not investigated un-til very recently.
Several researchers have useddistributional methods from similar documents inthe collection in order to expand the documentswith related terms that do not actually occur in thedocument (Liu and Croft, 2004; Kurland and Lee,2004; Tao et al, 2006; Mei et al, 2008; Huanget al, 2009).
The work presented here is com-plementary, in that we also explore DE, but useWordNet instead of distributional methods.Lexical semantic resources such as WordNet(Fellbaum, 1998) might provide a principled andexplicit remedy for the brittleness of keywordmatches.
WordNet has been used with successin psycholinguistic datasets of word similarity andrelatedness, where it often surpasses distributionalmethods based on keyword matches (Agirre et al,2009b).
WordNet has been applied to IR before.Some authors extended the query with relatedterms (Voorhees, 1994; Liu et al, 2005), whileothers have explicitly represented and indexedword senses after performing word sense disam-biguation (WSD) (Gonzalo et al, 1998; Stokoeet al, 2003; Kim et al, 2004).
More recently,a CLEF task was organized (Agirre et al, 2008;Agirre et al, 2009a) where queries and docu-ments were semantically disambiguated, and par-ticipants reported mixed results.This paper proposes to use WordNet for docu-ment expansion, proposing a new method: given9a full document, a random walk algorithm overthe WordNet graph ranks concepts closely relatedto the words in the document.
This is in con-trast to previous WordNet-based work which fo-cused on WSD to replace or supplement wordswith their senses.
Our method discovers impor-tant concepts, even if they are not explicitly men-tioned in the document.
For instance, given a doc-ument mentioning virus, software and DSL, ourmethod suggests related concepts and associatedwords such us digital subscriber line, phone com-pany and computer.
Those expansion words areindexed separately, and when combined with theregular index, we show that they improve the re-sults in three datasets over a state-of-the-art IR en-gine (Boldi and Vigna, 2005).
The three datasetsused in this study are ResPubliQA (Pen?as et al,2009), Yahoo!
Answers (Surdeanu et al, 2008)and CLEF-Robust (Agirre et al, 2009a).Considering that many IR systems are not ro-bust in the sense that they need careful fine-tuningand optimization of their parameters, we decidedto study the robustness of our method, explor-ing some alternative settings, including default pa-rameters, parameters optimized in developmentdata, and parameters optimized in other datasets.The study reveals that the additional semantic ex-pansion terms provide robustness in most cases.We also hypothesized that semantic documentexpansion could be most profitable when docu-ments are shorter, and our algorithm would bemost effective for collections of short documents.We artificially trimmed documents in the Robustdataset.
The results, together with the analysis ofdocument lengths of the three datasets, show thatdocument expansion is specially effective for veryshort documents, but other factors could also playa role.The paper is structured as follows.
We first in-troduce the document expansion technique.
Sec-tion 3 introduces the method to include the expan-sions in a retrieval system.
Section 4 presents theexperimental setup.
Section 5 shows our main re-sults.
Sections 6 and 7 analyze the robustness andrelation to document length.
Section 8 comparesto related work.
Finally, the conclusions and fu-ture work are mentioned.2 Document Expansion Using WordNetOur key insight is to expand the document withrelated words according to the background infor-mation in WordNet (Fellbaum, 1998), which pro-vides generic information about general vocabu-lary terms.
WordNet groups nouns, verbs, adjec-tives and adverbs into sets of synonyms (synsets),each expressing a distinct concept.
Synsets are in-terlinked with conceptual-semantic and lexical re-lations, including hypernymy, meronymy, causal-ity, etc.In contrast with previous work, we select thoseconcepts that are most closely related to the doc-ument as a whole.
For that, we use a techniquebased on random walks over the graph represen-tation of WordNet concepts and relations.We represent WordNet as a graph as fol-lows: graph nodes represent WordNet concepts(synsets) and dictionary words; relations amongsynsets are represented by undirected edges; anddictionary words are linked to the synsets asso-ciated to them by directed edges.
We used ver-sion 3.0, with all relations provided, including thegloss relations.
This was the setting obtaining thebest results in a word similarity dataset as reportedby Agirre et al (2009b).Given a document and the graph-based repre-sentation of WordNet, we obtain a ranked list ofWordNet concepts as follows:1.
We first pre-process the document to obtainthe lemmas and parts of speech of the opencategory words.2.
We then assign a uniform probability distri-bution to the terms found in the document.The rest of nodes are initialized to zero.3.
We compute personalized PageR-ank (Haveliwala, 2002) over the graph,using the previous distribution as the resetdistribution, and producing a probabilitydistribution over WordNet concepts Thehigher the probability for a concept, themore related it is to the given document.Basically, personalized PageRank is computedby modifying the random jump distribution vec-tor in the traditional PageRank equation.
In ourcase, we concentrate all probability mass in theconcepts corresponding to the words in the docu-10ment.Let G be a graph with N vertices v1, .
.
.
, vNand di be the outdegree of node i; let M be a N ?N transition probability matrix, where Mji = 1diif a link from i to j exists, and zero otherwise.Then, the calculation of the PageRank vector Prover G is equivalent to resolving Equation (1).Pr = cMPr + (1?
c)v (1)In the equation, v is a N ?
1 vector and c is theso called damping factor, a scalar value between0 and 1.
The first term of the sum on the equa-tion models the voting scheme described in thebeginning of the section.
The second term repre-sents, loosely speaking, the probability of a surferrandomly jumping to any node, e.g.
without fol-lowing any paths on the graph.
The damping fac-tor, usually set in the [0.85..0.95] range, modelsthe way in which these two terms are combined ateach step.The second term on Eq.
(1) can also be seen as asmoothing factor that makes any graph fulfill theproperty of being aperiodic and irreducible, andthus guarantees that PageRank calculation con-verges to a unique stationary distribution.In the traditional PageRank formulation thevector v is a stochastic normalized vector whoseelement values are all 1N , thus assigning equalprobabilities to all nodes in the graph in case ofrandom jumps.
In the case of personalized PageR-ank as used here, v is initialized with uniformprobabilities for the terms in the document, and0 for the rest of terms.PageRank is actually calculated by applying aniterative algorithm which computes Eq.
(1) suc-cessively until a fixed number of iterations areexecuted.
In our case, we used a publicly avail-able implementation1, with default values for thedamping value (0.85) and the number of iterations(30).
In order to select the expansion terms, wechose the 100 highest scoring concepts, and getall the words that lexicalize the given concept.Figure 1 exemplifies the expansion.
Given theshort document from Yahoo!
Answers (cf.
Sec-tion 4) shown in the top, our algorithm producesthe set of related concepts and words shown in the1http://ixa2.si.ehu.es/ukb/bottom.
Note that the expansion produces syn-onyms, but also other words related to conceptsthat are not mentioned in the document.3 Including Expansions in a RetrievalSystemOnce we have the list of words for document ex-pansion, we create one index for the words in theoriginal documents and another index with the ex-pansion terms.
This way, we are able to use theoriginal words only, or to also include the expan-sion words during the retrieval.The retrieval system was implemented usingMG4J (Boldi and Vigna, 2005), as it providesstate-of-the-art results and allows to combine sev-eral indices over the same document collection.We conducted different runs, by using only the in-dex made of original words (baseline) and also byusing the index with the expansion terms of therelated concepts.BM25 was the scoring function of choice.
It isone of the most relevant and robust scoring func-tions available (Robertson and Zaragoza, 2009).wBM25Dt := (2)tfDtk1((1?
b) + b dlDavdlD)+ tfDtidftwhere tfDt is the term frequency of term t in doc-ument D, dlD is the document length, idft is theinverted document frequency (or more specificallythe RSJ weight, (Robertson and Zaragoza, 2009)),and k1 and b are free parameters.The two indices were combined linearly, as fol-lows (Robertson and Zaragoza, 2009):score(d, e, q) := (3)?t?q?dwBM25Dt + ?
?t?q?ewBM25Etwhere D and E are the original and expanded in-dices, d, e and q are the original document, theexpansion of the document and the query respec-tively, t is a term, and ?
is a free parameter for therelative weight of the expanded index.11You should only need to turn off virus and anti-spy not uninstall.
And that?sdone within each of the softwares themselves.
Then turn them back on later afterinstalling any DSL softwares.06566077-n?
computer software, package, software, software package, software program, software system03196990-n?
digital subscriber line, dsl01569566-v?
instal, install, put in, set up04402057-n?
line, phone line, suscriber line, telephone circuit, telephone line08186221-n?
phone company, phone service, telco, telephone company, telephone service03082979-n?
computer, computing device, computing machine, data processor, electronic computerFigure 1: Example of a document expansion, with original document on top, and some of the relevantWordNet concepts identified by our algorithm, together with the words that lexicalize them.
Words inthe original document are shown in bold, synonyms in italics, and other related words underlined.4 Experimental SetupWe chose three data collections.
The first is basedon a traditional news collection.
DE could bespecially interesting for datasets with short docu-ments, which lead our choice of the other datasets:the second was chosen because it contains shorterdocuments, and the third is a passage retrieval taskwhich works on even shorter paragraphs.
Table 1shows some statistics about the datasets.One of the collections is the English datasetof the Robust task at CLEF 2009 (Agirre et al,2009a).
The documents are news collections fromLA Times 94 and Glasgow Herald 95.
The top-ics are statements representing information needs,consisting of three parts: a brief title statement; aone-sentence description; a more complex narra-tive describing the relevance assessment criteria.We use only the title and the description parts ofthe topics in our experiments.The Yahoo!
Answers corpus is a subset of adump of the Yahoo!
Answers web site2 (Surdeanuet al, 2008), where people post questions andanswers, all of which are public to any web userwilling to browse them.
The dataset is a smallsubset of the questions, selected for their linguis-tic properties (for example they all start with ?how{to?do?did?does?can?would?could?should}?
).Additionally, questions and answers of obviouslow quality were removed.
The document set wascreated with the best answer of each question(only one for each question).2Yahoo!
Webscope dataset ?ydata-yanswers-manner-questions-v1 0?
http://webscope.sandbox.yahoo.com/docs length q. train q. testRobust 166,754 532 150 160Yahoo!
89610 104 1000 88610ResPubliQA 1,379,011 20 100 500Table 1: Number of documents, average docu-ment length, number of queries for train and testin each collection.The other collection is the English dataset ofResPubliQA exercise at the Multilingual Ques-tion Answering Track at CLEF 2009 (Pen?as et al,2009).
The exercise is aimed at retrieving para-graphs that contain answers to a set of 500 natu-ral language questions.
The document collectionis a subset of the JRC-Acquis Multilingual Paral-lel Corpus, and consists of 21,426 documents forEnglish which are aligned to a similar number ofdocuments in other languages3.
For evaluation,we used the gold standard released by the orga-nizers, which contains a single correct passage foreach query.
As the retrieval unit is the passage,we split the document collection into paragraphs.We applied the expansion strategy only to pas-sages which had more than 10 words (half of thepassages), for two reasons: the first one was thatmost of these passages were found not to containrelevant information for the task (e.g.
?Article 2?or ?Having regard to the proposal from the Com-mission?
), and the second was that we thus savedsome computation time.In order to evaluate the quality of our expansionin practical retrieval settings, the next Section re-3Note that Table 1 shows the number of paragraphs,which conform the units we indexed.12base.
expa.
?Robust MAP .3781 .3835*** 1.43%Yahoo!
MRR .2900 .2950*** 1.72%P@1 .2142 .2183*** 1.91%ResPubliQA MRR .3931 .4077*** 3.72%P@1 .2860 .3000** 4.90%Table 2: Results using default parameters.port results with respect to several parameter set-tings.
Parameter optimization is often neglectedin retrieval with linguistic features, but we think itis crucial since it can have a large effect on rele-vance performance and therefore invalidate claimsof improvements over the baseline.
In each settingwe assign different values to the free parameters inthe previous section, k1, b and ?.5 ResultsThe main evaluation measure for Robust is meanAverage Precision (MAP), as customary.
In two ofthe datasets (Yahoo!
and ResPubliQA) there is asingle correct answer per query, and therefore weuse Mean Reciprocal Rank (MRR) and Mean Pre-cision at rank 1 (P@1) for evaluation.
Note that inthis setting MAP is identical to MRR.
Statisticalsignificance was computed using Paired Random-ization Test (Smucker et al, 2007).
In the tablesthroughout the paper, we use * to indicate statis-tical significance at 90% confidence level, ** for95% and *** for 99%.
Unless noted otherwise,base.
refers to MG4J with the standard index, andexpa.
refers to MG4J using both indices.
Bestresults per row are in bold when significant.
?
re-ports relative improvement respect to the baseline.5.1 Default Parameter SettingThe values for k1 and b are the default values asprovided in the wBM25 implementation of MG4J,1.2 and 0.5 respectively.
We could not think of astraightforward value for ?.
A value of 1 wouldmean that we are assigning equal importance tooriginal and expanded terms, which seemed anoverestimation, so we used 0.1.
Table 2 showsthe results when using the default setting of pa-rameters.
The use of expansion is beneficial in alldatasets, with relative improvements ranging from1.43% to 4.90%.base.
expa.
?Robust MAP .3740 .3823** 2.20%Yahoo!
MRR .3070 .3100*** 0.98%P@1 .2293 .2317* 1.05%ResPubliQA MRR .4970 .4942 -0.56%P@1 .3980 .3940 -1.01%Table 3: Results using optimized parameters.Setting System k1 b ?Default base.
1.20 0.50 -expa.
1.20 0.50 0.100Robust base.
1.80 0.64 -expa.
1.66 0.55 0.075Yahoo!
basel.
0.99 0.82 -expa.
0.84 0.87 0.146ResPubliQA base.
0.09 0.56 -expa.
0.13 0.65 0.090Table 4: Parameters as in the default setting or asoptimized in each dataset.
The ?
parameter is notused in the baseline systems.5.2 Optimized Parameter SettingWe next optimized all three parameters using thetrain part of each collection.
The optimization ofthe parameters followed a greedy method called?promising directions?
(Robertson and Zaragoza,2009).
The comparison between the baseline andexpansion systems in Table 3 shows that expan-sion helps in Yahoo!
and Robust, with statisticalsignificance.
The differences in ResPubliQA arenot significant, and indicate that expansion termswere not helpful in this setting.Note that the optimization of the parametersyields interesting effects in the baseline for eachof the datasets.
If we compare the results of thebaseline with default settings (Table 2) and withoptimized setting (Table 3), the baseline improvesMRR dramatically in ResPubliQA (26% relativeimprovement), significantly in Yahoo!
(5.8%) anddecreases MAP in Robust (-0.01%).
This dis-parity of effects could be explained by the factthat the default values are often approximated us-ing TREC-style news collections, which is exactlythe genre of the Robust documents, while Yahoouses shorter documents, and ResPubliQA has theshortest documents.Table 4 summarizes the values of the parame-ters in both default and optimized settings.
For k1,the optimization yields very different values.
InRobust the value is similar to the default value, but13base.
expa.
?
?Rob MAP .3781 .3881*** 2.64% 0.18Y!
MRR .2900 .2980*** 2.76% 0.27P@1 .2142 .2212*** 3.27%ResP.
MRR .3931 .4221*** 7.39% 0.61P@1 .2860 .3180** 11.19%Table 5: Results obtained using the ?
optimizedsetting, including actual values of ?.in ResPubliQA the optimization pushes it downbelow the typical values cited in the literature(Robertson and Zaragoza, 2009), which might ex-plain the boost in performance for the baseline inthe case of ResPubliQA.
When all three param-eters are optimized together, the values ?
in thetable range from 0.075 to 0.146.
The values of theoptimized ?
can be seem as an indication of theusefulness of the expanded terms, so we exploredthis farther.5.3 Exploring ?As an additional analysis experiment, we wantedto know the effect of varying ?
keeping k1 and bconstant at their default values.
Table 5 shows thebest values in each dataset, which that the weightof the expanded terms and the relative improve-ment are highly correlated.5.4 Exploring Number of ExpansionConceptsOne of the free parameters of our system is thenumber of concepts to be included in the docu-ment expansion.
We have performed a limitedstudy with the default parameter setting on theRobust setting, using 100, 500 and 750 concepts,but the variations were not statistically significant.Note that with 100 concepts we were actually ex-panding with 268 words, with 500 concepts weadd 1247 words and with 750 concepts we add1831 words.6 RobustnessThe results in the previous section indicate thatoptimization is very important, but unfortunatelyreal applications usually lack training data.
In thisSection we wanted to study whether the param-eters can be carried over from one dataset to theother, and if not, whether the extra terms found bytrain base.
expa.
?Rob.def.
MAP .3781 .3835*** 1.43%Rob.
MAP .3740 .3823** 2.20%Y!
MAP .3786 .3759 -0.72%Res.
MAP .3146 .3346*** 6.35%Y!def.
MRR .2900 .2950*** 1.72%Rob.
MRR .2920 .2920 0.0%Y!
MRR .3070 .3100** 0.98%Res.
MRR .2600 .2750*** 5.77%ResP.def.
MRR .3931 .4077*** 3.72%Rob.
MRR .3066 .3655*** 19.22%Y!
MRR .3010 .3459*** 14.93%Res.
MRR .4970 .4942 -0.56%Table 6: Results optimizing parameters with train-ing from other datasets.
We also include defaultand optimization on the same dataset for compar-ison.
Only MRR and MAP results are given.DE would make the system more robust to thosesub-optimal parameters.Table 6 includes a range of parameter set-tings, including defaults, and optimized parame-ters coming from the same and different datasets.The values of the parameters are those in Table4.
The results show that when the parameters areoptimized in other datasets, DE provides improve-ment with statistical significance in all cases, ex-cept for the Robust dataset when using parametersoptimized from Yahoo!
and vice-versa.Overall, the table shows that our DE method ei-ther improves the results significantly or does notaffect performance, and that it provides robustnessacross different parameter settings, even with sub-optimal values.7 Exploring Document LengthThe results in Table 6 show that the perfor-mance improvements are best in the collectionwith shortest documents (ResPubliQA).
But theresults for Robust and Yahoo!
do not show any re-lation to document length.
We thus decided to doan additional experiment artificially shrinking thedocument in Robust to a certain percentage of itsoriginal length.
We create new pseudo-collectionwith the shrinkage factors of 2.5%, 10%, 20% and50%, keeping the first N% words in the documentand discarding the rest.
In all cases we used thesame parameters, as optimized for Robust.Table 7 shows the results (MAP), with someclear indication that the best improvements are ob-14tained for the shortest documents.length base.
expa.
?2.5% 13 .0794 .0851 7.18%10% 53 .1757 .1833 4.33%20% 107 .2292 .2329 1.61%50% 266 .3063 .3098 1.14%100% 531 .3740 .3823 2.22%Table 7: Results (MAP) on Robust when arti-ficially shrinking documents to a percentage oftheir length.
In addition to the shrinking rate weshow the average lengths of documents.8 Related WorkGiven the brittleness of keyword matches, mostresearch has concentrated on Query Expansion(QE) methods.
These methods analyze the userquery terms and select automatically new relatedquery terms.
Most QE methods use statistical(or distributional) techniques to select terms forexpansion.
They do this by analyzing term co-occurrence statistics in the corpus and in the high-est scored documents of the original query (Man-ning et al, 2009).
These methods seemed to im-prove slightly retrieval relevance on average, butat the cost of greatly decreasing the relevance ofdifficult queries.
But more recent studies seemto overcome some of these problems (Collins-Thompson, 2009).An alternative to QE is to perform the expan-sion in the document.
Document Expansion (DE)was first proposed in the speech retrieval commu-nity (Singhal and Pereira, 1999), where the taskis to retrieve speech transcriptions which are quitenoisy.
Singhal and Pereira propose to enhance therepresentation of a noisy document by adding tothe document vector a linearly weighted mixtureof related documents.
In order to determine re-lated documents, the original document is used asa query into the collection, and the ten most rele-vant documents are selected.Two related papers (Liu and Croft, 2004; Kur-land and Lee, 2004) followed a similar approachon the TREC ad-hoc document retrieval task.They use document clustering to determine simi-lar documents, and document expansion is carriedout with respect to these.
Both papers report sig-nificant improvements over non-expanded base-lines.
Instead of clustering, more recent work (Taoet al, 2006; Mei et al, 2008; Huang et al, 2009)use language models and graph representations ofthe similarity between documents in the collec-tion to smooth language models with some suc-cess.
The work presented here is complementary,in that we also explore DE, but use WordNet in-stead of distributional methods.
They use a tighterintegration of their expansion model (compared toour simple two-index model), which coupled withour expansion method could help improve resultsfurther.
We plan to explore this in the future.An alternative to statistical expansion methodsis to use lexical semantic knowledge bases such asWordNet.
Most of the work has focused on queryexpansion and the use of synonyms from Word-Net after performing word sense disambiguation(WSD) with some success (Voorhees, 1994; Liuet al, 2005).
The short context available inthe query when performing WSD is an impor-tant problems of these techniques.
In contrast,we use full document context, and related wordsbeyond synonyms.
Another strand of WordNetbased work has explicitly represented and indexedword senses after performing WSD (Gonzalo etal., 1998; Stokoe et al, 2003; Kim et al, 2004).The word senses conform a different space fordocument representation, but contrary to us, theseworks incorporate concepts for all words in thedocuments, and are not able to incorporate con-cepts that are not explicitly mentioned in the doc-ument.
More recently, a CLEF task was orga-nized (Agirre et al, 2009a) where terms were se-mantically disambiguated to see the improvementthat this would have on retrieval; the conclusionswere mixed, with some participants slightly im-proving results with information from WordNet.To the best of our knowledge our paper is the firston the topic of document expansion using lexical-semantic resources.We would like to also compare our performanceto those of other systems as tested on the samedatasets.
The systems which performed best inthe Robust evaluation campaign (Agirre et al,2009a) report 0.4509 MAP, but note that they de-ployed a complex system combining probabilis-tic and monolingual translation-based models.
InResPubliQA (Pen?as et al, 2009), the official eval-15uation included manual assessment, and we can-not therefore reproduce those results.
Fortunately,the organizers released all runs, but only the firstranked document for each query was included, sowe could only compute P@1.
The P@1 of bestrun was 0.40.
Finally (Surdeanu et al, 2008) re-port MRR figure around 0.68, but they evaluateonly in the questions where the correct answeris retrieved by answer retrieval in the top 50 an-swers, and is thus not comparable to our setting.Regarding the WordNet expansion techniquewe use here, it is implemented on top of publiclyavailable software4, which has been successfullyused in word similarity (Agirre et al, 2009b) andword sense disambiguation (Agirre and Soroa,2009).
In the first work, a single word was in-put to the random walk algorithm, obtaining theprobability distribution over all WordNet synsets.The similarity of two words was computed as thesimilarity of the distribution of each word, obtain-ing the best results for WordNet-based systems onthe word similarity dataset, and comparable to theresults of a distributional similarity method whichused a crawl of the entire web.
Agirre et al (2009)used the context of occurrence of a target word tostart the random walk, and obtained very good re-sults for WordNet WSD methods.9 Conclusions and Future WorkThis paper presents a novel Document Expan-sion method based on a WordNet-based systemto find related concepts and words.
The docu-ments in three datasets were thus expanded withrelated words, which were fed into a separate in-dex.
When combined with the regular index wereport improvements over MG4J usingwBM25 forthose three datasets across several parameter set-tings, including default values, optimized param-eters and parameters optimized in other datasets.In most of the cases the improvements are sta-tistically significant, indicating that the informa-tion in the document expansion is useful.
Similarto other expansion methods, parameter optimiza-tion has a stronger effect than our expansion strat-egy.
The problem with parameter optimization isthat in most real cases there is no tuning dataset4http://ixa2.si.ehu.es/ukbavailable.
Our analysis shows that our expansionmethod is more effective for sub-optimal param-eter settings, which is the case for most real-liveIR applications.
A comparison across the threedatasets and using artificially trimmed documentsindicates that our method is particularly effectivefor short documents.As document expansion is done at indexingtime, it avoids any overhead at query time.
Italso has the advantage of leveraging full documentcontext, in contrast to query expansion methods,which use the scarce information present in themuch shorter queries.
Compared to WSD-basedmethods, our method has the advantage of nothaving to disambiguate all words in the document.Besides, our algorithm picks the most relevantconcepts, and thus is able to expand to conceptswhich are not explicitly mentioned in the docu-ment.
The successful use of background informa-tion such as the one in WordNet could help closethe gap between semantic web technologies andIR, and opens the possibility to include other re-sources like Wikipedia or domain ontologies likethose in the Unified Medical Language System.Our method to integrate expanded terms usingan additional index is simple and straightforward,and there is still ample room for improvement.A tighter integration of the document expansiontechnique in the retrieval model should yield bet-ter results, and the smoothed language models of(Mei et al, 2008; Huang et al, 2009) seem anatural choice.
We would also like to comparewith other existing query and document expan-sion techniques and study whether our techniqueis complementary to query expansion approaches.AcknowledgmentsThis work has been supported by KNOW2(TIN2009-14715-C04-01) and KYOTO (ICT-2007-211423) projects.
Arantxa Otegi?s work isfunded by a PhD grant from the Basque Govern-ment.
Part of this work was done while ArantxaOtegi was visiting Yahoo!
Research Barcelona.ReferencesAgirre, E. and A. Soroa.
2009.
Personalizing PageR-ank for Word Sense Disambiguation.
In Proc.
of16EACL 2009, Athens, Greece.Agirre, E., G. M. Di Nunzio, N. Ferro, T. Mandl,and C. Peters.
2008.
CLEF 2008: Ad-Hoc TrackOverview.
In Working Notes of the Cross-LingualEvaluation Forum.Agirre, E., G. M. Di Nunzio, T. Mandl, and A. Otegi.2009a.
CLEF 2009 Ad Hoc Track Overview: Ro-bust - WSD Task.
In Working Notes of the Cross-Lingual Evaluation Forum.Agirre, E., A. Soroa, E. Alfonseca, K. Hall, J. Kraval-ova, and M. Pasca.
2009b.
A Study on Similarityand Relatedness Using Distributional and WordNet-based Approaches.
In Proc.
of NAACL, Boulder,USA.Boldi, P. and S. Vigna.
2005.
MG4J at TREC 2005.In The Fourteenth Text REtrieval Conference (TREC2005) Proceedings, number SP 500-266 in SpecialPublications.
NIST.Collins-Thompson, Kevyn.
2009.
Reducing the riskof query expansion via robust constrained optimiza-tion.
In Proceedings of CIKM ?09, pages 837?846.Fellbaum, C., editor.
1998.
WordNet: An Elec-tronic Lexical Database and Some of its Applica-tions.
MIT Press, Cambridge, Mass.Gonzalo, J., F. Verdejo, I. Chugur, and J. Cigarran.1998.
Indexing with WordNet synsets can improvetext retrieval.
In Proceedings ACL/COLING Work-shop on Usage of WordNet for Natural LanguageProcessing.Haveliwala, T. H. 2002.
Topic-sensitive PageRank.
InProceedings of WWW ?02, pages 517?526.Huang, Yunping, Le Sun, and Jian-Yun Nie.
2009.Smoothing document language model with localword graph.
In Proceedings of CIKM ?09, pages1943?1946.Kim, S. B., H. C. Seo, and H. C. Rim.
2004.
Informa-tion retrieval using word senses: root sense taggingapproach.
In Proceedings of SIGIR ?04, pages 258?265.Kurland, O. and L. Lee.
2004.
Corpus structure, lan-guage models, and ad hoc information retrieval.
InProceedings of SIGIR ?04, pages 194?201.Liu, X. and W. B. Croft.
2004.
Cluster-based retrievalusing language models.
In Proceedings of SIGIR?04, pages 186?193.Liu, S., C. Yu, and W. Meng.
2005.
Word sense dis-ambiguation in queries.
In Proceedings of CIKM?05, pages 525?532.Manning, C. D., P. Raghavan, and H. Schu?tze.
2009.An introduction to information retrieval.
Cam-bridge University Press, UK.Mei, Qiaozhu, Duo Zhang, and ChengXiang Zhai.2008.
A general optimization framework forsmoothing language models on graph structures.
InProceedings of SIGIR ?08, pages 611?618.Pen?as, A., P. Forner, R. Sutcliffe, A. Rodrigo,C.
Fora?scu, I. Alegria, D. Giampiccolo, N. Moreau,and P. Osenova.
2009.
Overview of ResPubliQA2009: Question Answering Evaluation over Euro-pean Legislation.
In Working Notes of the Cross-Lingual Evaluation Forum.Robertson, S. and H. Zaragoza.
2009.
The Proba-bilistic Relevance Framework: BM25 and Beyond.Foundations and Trends in Information Retrieval,3(4):333?389.Singhal, A. and F. Pereira.
1999.
Document expansionfor speech retrieval.
In Proceedings of SIGIR ?99,pages 34?41, New York, NY, USA.
ACM.Smucker, M. D., J. Allan, and B. Carterette.
2007.
Acomparison of statistical significance tests for infor-mation retrieval evaluation.
In Proc.
of CIKM 2007,Lisboa, Portugal.Stokoe, C., M. P. Oakes, and J. Tait.
2003.
Word sensedisambiguation in information retrieval revisited.
InProceedings of SIGIR ?03, page 166.Surdeanu, M., M. Ciaramita, and H. Zaragoza.
2008.Learning to Rank Answers on Large Online QACollections.
In Proceedings of ACL 2008.Tao, T., X. Wang, Q. Mei, and C. Zhai.
2006.
Lan-guage model information retrieval with documentexpansion.
In Proceedings of HLT/NAACL, pages407?414, June.Voorhees, E. M. 1994.
Query expansion using lexical-semantic relations.
In Proceedings of SIGIR ?94,page 69.17
