Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
868?876, Prague, June 2007. c?2007 Association for Computational LinguisticsFactored Translation ModelsPhilipp Koehn and Hieu Hoangpkoehn@inf.ed.ac.uk, H.Hoang@sms.ed.ac.ukSchool of InformaticsUniversity of Edinburgh2 Buccleuch Place, Edinburgh EH8 9LWScotland, United KingdomAbstractWe present an extension of phrase-basedstatistical machine translation models thatenables the straight-forward integration ofadditional annotation at the word-level ?may it be linguistic markup or automati-cally generated word classes.
In a num-ber of experiments we show that factoredtranslation models lead to better transla-tion performance, both in terms of auto-matic scores, as well as more grammaticalcoherence.1 IntroductionThe current state-of-the-art approach to statisticalmachine translation, so-called phrase-based models,is limited to the mapping of small text chunks with-out any explicit use of linguistic information, mayit be morphological, syntactic, or semantic.
Suchadditional information has been demonstrated to bevaluable by integrating it in pre-processing or post-processing steps.However, a tighter integration of linguistic infor-mation into the translation model is desirable for tworeasons:?
Translation models that operate on more gen-eral representations, such as lemmas insteadof surface forms of words, can draw on richerstatistics and overcome the data sparsenessproblems caused by limited training data.?
Many aspects of translation can be best ex-plained on a morphological, syntactic, or se-mantic level.
Having such information avail-able to the translation model allows the directmodeling of these aspects.
For instance: re-ordering at the sentence level is mostly drivenword wordpart-of-speechOutputInputmorphologypart-of-speechmorphologyword classlemmaword classlemma......Figure 1: Factored representations of input and out-put words incorporate additional annotation into thestatistical translation model.by general syntactic principles, local agreementconstraints show up in morphology, etc.Therefore, we extended the phrase-based ap-proach to statistical translation to tightly integrateadditional information.
The new approach allowsadditional annotation at the word level.
A word inour framework is not only a token, but a vector offactors that represent different levels of annotation(see Figure 1).We report on experiments with factors such assurface form, lemma, part-of-speech, morphologi-cal features such as gender, count and case, auto-matic word classes, true case forms of words, shal-low syntactic tags, as well as dedicated factors to en-sure agreement between syntactically related items.This paper describes the motivation, the modelingaspects and the computationally efficient decodingmethods of factored translation models.
We presentbriefly results for a number of language pairs.
How-ever, the focus of this paper is the description of theapproach.
Detailed experimental results will be de-scribed in forthcoming papers.8682 Related WorkMany attempts have been made to add richer in-formation to statistical machine translation models.Most of these focus on the pre-processing of the in-put to the statistical system, or the post-processingof its output.
Our framework is more general andgoes beyond recent work on models that back offto representations with richer statistics (Nie?en andNey, 2001; Yang and Kirchhoff, 2006; Talbot andOsborne, 2006) by keeping a more complex repre-sentation throughout the translation process.Rich morphology often poses a challenge to sta-tistical machine translation, since a multitude ofword forms derived from the same lemma fragmentthe data and lead to sparse data problems.
If the in-put language is morphologically richer than the out-put language, it helps to stem or segment the inputin a pre-processing step, before passing it on to thetranslation system (Lee, 2004; Sadat and Habash,2006).Structural problems have also been addressed bypre-processing: Collins et al (2005) reorder the in-put to a statistical system to closer match the wordorder of the output language.On the other end of the translation pipeline, addi-tional information has been used in post-processing.Och et al (2004) report minor improvements withlinguistic features on a Chinese-English task, Koehnand Knight (2003) show some success in re-rankingnoun phrases for German-English.
In their ap-proaches, first, an n-best list with the best transla-tions is generated for each input sentence.
Then,the n-best list is enriched with additional features,for instance by syntactically parsing each candidatetranslation and adding a parse score.
The additionalfeatures are used to rescore the n-best list, resultingpossibly in a better best translation for the sentence.The goal of integrating syntactic informationinto the translation model has prompted many re-searchers to pursue tree-based transfer models (Wu,1997; Alshawi et al, 1998; Yamada and Knight,2001; Melamed, 2004; Menezes and Quirk, 2005;Galley et al, 2006), with increasingly encouragingresults.
Our goal is complementary to these efforts:we are less interested in recursive syntactic struc-ture, but in richer annotation at the word level.
Infuture work, these approaches may be combined.lemma lemmapart-of-speechOutputInputmorphologypart-of-speechword wordmorphologyFigure 2: Example factored model: morphologi-cal analysis and generation, decomposed into threemapping steps (translation of lemmas, translation ofpart-of-speech and morphological information, gen-eration of surface forms).3 Motivating Example: MorphologyOne example to illustrate the short-comings of thetraditional surface word approach in statistical ma-chine translation is the poor handling of morphol-ogy.
Each word form is treated as a token in it-self.
This means that the translation model treats,say, the word house completely independent of theword houses.
Any instance of house in the trainingdata does not add any knowledge to the translationof houses.In the extreme case, while the translation of housemay be known to the model, the word housesmay beunknown and the system will not be able to translateit.
While this problem does not show up as stronglyin English ?
due to the very limited morphologi-cal inflection in English ?
it does constitute a sig-nificant problem for morphologically rich languagessuch as Arabic, German, Czech, etc.Thus, it may be preferably to model translationbetween morphologically rich languages on the levelof lemmas, and thus pooling the evidence for differ-ent word forms that derive from a common lemma.In such a model, we would want to translate lemmaand morphological information separately, and com-bine this information on the output side to ultimatelygenerate the output surface words.Such a model can be defined straight-forward asa factored translation model.
See Figure 2 for anillustration of this model in our framework.Note that while we illustrate the use of factoredtranslation models on such a linguistically motivated869example, our framework also applies to models thatincorporate statistically defined word classes, or anyother annotation.4 Decomposition of Factored TranslationThe translation of factored representations of in-put words into the factored representations of out-put words is broken up into a sequence of mappingsteps that either translate input factors into outputfactors, or generate additional output factors fromexisting output factors.Recall the example of a factored model motivatedby morphological analysis and generation.
In thismodel the translation process is broken up into thefollowing three mapping steps:1.
Translate input lemmas into output lemmas2.
Translate morphological and POS factors3.
Generate surface forms given the lemma andlinguistic factorsFactored translation models build on the phrase-based approach (Koehn et al, 2003) that breaks upthe translation of a sentence into the translation ofsmall text chunks (so-called phrases).
This approachimplicitly defines a segmentation of the input andoutput sentences into phrases.
See an example inFigure 3.Our current implementation of factored transla-tion models follows strictly the phrase-based ap-proach, with the additional decomposition of phrasetranslation into a sequence of mapping steps.
Trans-lation steps map factors in input phrases to factorsin output phrases.
Generation steps map outputfactors within individual output words.
To reiter-ate: all translation steps operate on the phrase level,while all generation steps operate on the word level.Since all mapping steps operate on the same phrasesegmentation of the input and output sentence intophrase pairs, we call these synchronous factoredmodels.Let us now take a closer look at one example, thetranslation of the one-word phrase ha?user into En-glish.
The representation of ha?user in German is:surface-form ha?user | lemma haus | part-of-speechNN | count plural | case nominative | gender neutral.neue h?user werden gebautnew houses are builtFigure 3: Example sentence translation by a stan-dard phrase model.
Factored models extend this ap-proach.The three mapping steps in our morphologicalanalysis and generation model may provide the fol-lowing applicable mappings:1.
Translation: Mapping lemmas?
haus ?
house, home, building, shell2.
Translation: Mapping morphology?
NN|plural-nominative-neutral ?NN|plural, NN|singular3.
Generation: Generating surface forms?
house|NN|plural ?
houses?
house|NN|singular ?
house?
home|NN|plural ?
homes?
...We call the application of these mapping stepsto an input phrase expansion.
Given the multi-ple choices for each step (reflecting the ambigu-ity in translation), each input phrase may be ex-panded into a list of translation options.
The Germanha?user|haus|NN|plural-nominative-neutral may beexpanded as follows:1.
Translation: Mapping lemmas{ ?|house|?|?, ?|home|?|?, ?|building|?|?,?|shell|?|?
}2.
Translation: Mapping morphology{ ?|house|NN|plural, ?|home|NN|plural,?|building|NN|plural, ?|shell|NN|plural,?|house|NN|singular, ... }3.
Generation: Generating surface forms{ houses|house|NN|plural,homes|home|NN|plural,buildings|building|NN|plural,shells|shell|NN|plural,house|house|NN|singular, ... }8705 Statistical ModelFactored translation models follow closely the sta-tistical modeling approach of phrase-based models(in fact, phrase-based models are a special case offactored models).
The main difference lies in thepreparation of the training data and the type of mod-els learned from the data.5.1 TrainingThe training data (a parallel corpus) has to be anno-tated with the additional factors.
For instance, if wewant to add part-of-speech information on the inputand output side, we need to obtain part-of-speechtagged training data.
Typically this involves runningautomatic tools on the corpus, since manually anno-tated corpora are rare and expensive to produce.Next, we need to establish a word-alignmentfor all the sentences in the parallel training cor-pus.
Here, we use the same methodology asin phrase-based models (typically symmetrizedGIZA++ alignments).
The word alignment methodsmay operate on the surface forms of words, or on anyof the other factors.
In fact, some preliminary ex-periments have shown that word alignment based onlemmas or stems yields improved alignment quality.Each mapping step forms a component of theoverall model.
From a training point of view thismeans that we need to learn translation and gener-ation tables from the word-aligned parallel corpusand define scoring methods that help us to choosebetween ambiguous mappings.Phrase-based translation models are acquiredfrom a word-aligned parallel corpus by extracting allphrase-pairs that are consistent with the word align-ment.
Given the set of extracted phrase pairs withcounts, various scoring functions are estimated,such as conditional phrase translation probabilitiesbased on relative frequency estimation or lexicaltranslation probabilities based on the words in thephrases.In our approach, the models for the translationsteps are acquired in the same manner from a word-aligned parallel corpus.
For the specified factors inthe input and output, phrase mappings are extracted.The set of phrase mappings (now over factored rep-resentations) is scored based on relative counts andword-based translation probabilities.The generation distributions are estimated on theoutput side only.
The word alignment plays norole here.
In fact, additional monolingual data maybe used.
The generation model is learned on aword-for-word basis.
For instance, for a genera-tion step that maps surface forms to part-of-speech,a table with entries such as (fish,NN) is constructed.One or more scoring functions may be defined overthis table, in our experiments we used both condi-tional probability distributions, e.g., p(fish|NN) andp(NN|fish), obtained by maximum likelihood esti-mation.An important component of statistical machinetranslation is the language model, typically an n-gram model over surface forms of words.
In theframework of factored translation models, such se-quence models may be defined over any factor, orany set of factors.
For factors such as part-of-speechtags, building and using higher order n-gram models(7-gram, 9-gram) is straight-forward.5.2 Combination of ComponentsAs in phrase-based models, factored translationmodels can be seen as the combination of severalcomponents (language model, reordering model,translation steps, generation steps).
These compo-nents define one or more feature functions that arecombined in a log-linear model:p(e|f) =1Zexpn?i=1?ihi(e, f) (1)Z is a normalization constant that is ignored inpractice.
To compute the probability of a translatione given an input sentence f, we have to evaluate eachfeature function hi.
For instance, the feature func-tion for a bigram language model component is (mis the number of words ei in the sentence e):hLM(e, f) = pLM(e)= p(e1) p(e2|e1)..p(em|em?1)(2)Let us now consider the feature functions intro-duced by the translation and generation steps of fac-tored translation models.
The translation of the inputsentence f into the output sentence e breaks down toa set of phrase translations {(f?j , e?j)}.For a translation step component, each featurefunction hT is defined over the phrase pairs (f?j , e?j)871given a scoring function ?
:hT(e, f) =?j?
(f?j , e?j) (3)For a generation step component, each featurefunction hG given a scoring function ?
is definedover the output words ek only:hG(e, f) =?k?
(ek) (4)The feature functions follow from the scoringfunctions (?
, ?)
acquired during the training oftranslation and generation tables.
For instance, re-call our earlier example: a scoring function for ageneration model component that is a conditionalprobability distribution between input and outputfactors, e.g., ?
(fish,NN,singular) = p(NN|fish).The feature weights ?i in the log-linear modelare determined using a minimum error rate trainingmethod, typically Powell?s method (Och, 2003).5.3 Efficient DecodingCompared to phrase-based models, the decomposi-tion of phrase translation into several mapping stepscreates additional computational complexity.
In-stead of a simple table look-up to obtain the possibletranslations for an input phrase, now multiple tableshave to be consulted and their content combined.In phrase-based models it is easy to identify theentries in the phrase table that may be used for aspecific input sentence.
These are called translationoptions.
We usually limit ourselves to the top 20translation options for each input phrase.The beam search decoding algorithm starts withan empty hypothesis.
Then new hypotheses are gen-erated by using all applicable translation options.These hypotheses are used to generate further hy-potheses in the same manner, and so on, until hy-potheses are created that cover the full input sen-tence.
The highest scoring complete hypothesis in-dicates the best translation according to the model.How do we adapt this algorithm for factoredtranslation models?
Since all mapping steps operateon the same phrase segmentation, the expansions ofthese mapping steps can be efficiently pre-computedprior to the heuristic beam search, and stored astranslation options.
For a given input phrase, all pos-sible translation options are thus computed beforeword wordpart-of-speechOutputInput3gram7gramFigure 4: Syntactically enriched output: By gener-ating additional linguistic factors on the output side,high-order sequence models over these factors sup-port syntactical coherence of the output.decoding (recall the example in Section 4, where wecarried out the expansion for one input phrase).
Thismeans that the fundamental search algorithm doesnot change.However, we need to be careful about combina-torial explosion of the number of translation optionsgiven a sequence of mapping steps.
In other words,the expansion may create too many translation op-tions to handle.
If one or many mapping steps resultin a vast increase of (intermediate) expansions, thismay be become unmanageable.
We currently ad-dress this problem by early pruning of expansions,and limiting the number of translation options perinput phrase to a maximum number, by default 50.This is, however, not a perfect solution.
We are cur-rently working on a more efficient search for the top50 translation options to replace the current brute-force approach.6 ExperimentsWe carried out a number of experiments using thefactored translation model framework, incorporatingboth linguistic information and automatically gener-ated word classes.This work is implemented as part of the opensource Moses1 system (Koehn et al, 2007).
We usedthe default settings for this system.6.1 Syntactically Enriched OutputIn the first set of experiments, we translate surfaceforms of words and generate additional output fac-tors from them (see Figure 4 for an illustration).
Byadding morphological and shallow syntactic infor-1available at http://www.statmt.org/moses/872English?GermanModel BLEUbest published result 18.15%baseline (surface) 18.04%surface + POS 18.15%surface + POS + morph 18.22%English?SpanishModel BLEUbaseline (surface) 23.41%surface + morph 24.66%surface + POS + morph 24.25%English?CzechModel BLEUbaseline (surface) 25.82%surface + all morph 27.04%surface + case/number/gender 27.45%surface + CNG/verb/prepositions 27.62%Table 1: Experimental results with syntactically en-riched output (part of speech, morphology)mation, we are able to use high-order sequence mod-els (just like n-gram language models over words) inorder to support syntactic coherence of the output.Table 1 summarizes the experimental results.The English?German systems were trained on thefull 751,088 sentence Europarl corpus and evaluatedon the WMT 2006 test set (Koehn and Monz, 2006).Adding part-of-speech and morphological factors onthe output side and exploiting them with 7-gramsequence models results in minor improvements inBLEU.
The model that incorporates both POS andmorphology (18.22% BLEU vs. baseline 18.04%BLEU) ensures better local grammatical coherence.The baseline system produces often phrases suchas zur(to) zwischenstaatlichen(inter-governmental)methoden(methods), with a mismatch between thedeterminer (singular) and the noun (plural), whilethe adjective is ambiguous.
In a manual evaluationof intra-NP agreement we found that the factoredmodel reduced the disagreement error within nounphrases of length ?
3 from 15% to 4%.English?Spanish systems were trained on a40,000 sentence subset of the Europarl corpus.
Here,we also used morphological and part-of-speech fac-tors on the output side with an 7-gram sequencemodel, resulting in absolute improvements of 1.25%(only morph) and 0.84% (morph+POS).
Improve-ments on the full Europarl corpus are smaller.English-Czech systems were trained on a 20,000sentence Wall Street Journal corpus.
Morphologi-cal features were exploited with a 7-gram languagemodel.
Experimentation suggests that it is benefi-cial to carefully consider which morphological fea-tures to be used.
Adding all features results inlower performance (27.04% BLEU), than consider-ing only case, number and gender (27.45% BLEU)or additionally verbial (person, tense, and aspect)and prepositional (lemma and case) morphology(27.62% BLEU).
All these models score well abovethe baseline of 25.82% BLEU.An extended description of these experiments isin the JHU workshop report (Koehn et al, 2006).6.2 Morphological Analysis and GenerationThe next model is the one described in our motivat-ing example in Section 4 (see also Figure 2).
Insteadof translating surface forms of words, we translateword lemma and morphology separately, and gener-ate the surface form of the word on the output side.We carried out experiments for the language pairGerman?English, using the 52,185 sentence NewsCommentary corpus2.
We report results on the de-velopment test set, which is also the out-of-domaintest set of the WMT06 workshop shared task (Koehnand Monz, 2006).
German morphological analysisand POS tagging was done using LoPar Schmidt andSchulte im Walde (2000), English POS tagging wasdone with Brill?s tagger (Brill, 1995), followed by asimple lemmatizer based on tagging results.Experimental results are summarized in Table 2.For this data set, we also see an improvement whenusing a part-of-speech language model ?
the BLEUscore increases from 18.19% to 19.05% ?
consis-tent with the results reported in the previous section.However, moving from a surface word translationmapping to a lemma/morphology mapping leads toa deterioration of performance to a BLEU score of14.46%.Note that this model completely ignores the sur-face forms of input words and only relies on the2Made available for the WMT07 workshop shared taskhttp://www.statmt.org/wmt07/873German?EnglishModel BLEUbaseline (surface) 18.19%+ POS LM 19.05%pure lemma/morph model 14.46%backoff lemma/morph model 19.47%Table 2: Experimental results with morphologicalanalysis and generation model (Figure 2), usingNews Commentary corpusmore general lemma and morphology information.While this allows the translation of word forms withknown lemma and unknown surface form, on bal-ance it seems to be disadvantage to throw away sur-face form information.To overcome this problem, we introduce an al-ternative path model: Translation options in thismodel may come either from the surface form modelor from the lemma/morphology model we just de-scribed.
For surface forms with rich evidence inthe training data, we prefer surface form mappings,and for surface forms with poor or no evidence inthe training data we decompose surface forms intolemma and morphology information and map theseseparately.
The different translation tables form dif-ferent components in the log-linear model, whoseweights are set using standard minimum error ratetraining methods.The alternative path model outperforms the sur-face form model with POS LM, with an BLEU scoreof 19.47% vs. 19.05%.
The test set has 3276 un-known word forms vs 2589 unknown lemmas (outof 26,898 words).
Hence, the lemma/morph modelis able to translate 687 additional words.6.3 Use of Automatic Word ClassesFinally, we went beyond linguistically motivatedfactors and carried out experiments with automati-cally trained word classes.
By clustering words to-gether by their contextual similarity, we are able tofind statistically similarities that may lead to moregeneralized and robust models.We trained models on the IWSLT 2006 task(39,953 sentences).
Compared to a baselineEnglish?Chinese system, adding word classes on theoutput side as additional factors (in a model as pre-English?ChineseModel BLEUbaseline (surface) 19.54%surface + word class 21.10%Table 3: Experimental result with automatic wordclasses obtained by word clusteringChinese?EnglishRecase Method BLEUStandard two-pass: SMT + recase 20.65%Integrated factored model (optimized) 21.08%OutputInputmixed-casedlower-cased lower-casedTable 4: Experimental result with integrated recas-ing (IWSLT 2006 task)viously illustrated in Figure 4) to be exploited bya 7-gram sequence model, we observe a gain 1.5%BLEU absolute.
For more on this experiment, see(Shen et al, 2006).6.4 Integrated RecasingTo demonstrate the versatility of the factored trans-lation model approach, consider the task of recas-ing (Lita et al, 2003; Wang et al, 2006).
Typicallyin statistical machine translation, the training data islowercased to generalize over differently cased sur-face forms ?
say, the, The, THE ?
which neces-sitates a post-processing step to restore case in theoutput.With factored translation models, it is possibleto integrate this step into the model, by adding ageneration step.
See Table 4 for an illustration ofthis model and experimental results on the IWSLT2006 task (Chinese-English).
The integrated recas-ing model outperform the standard approach with anBLEU score of 21.08% to 20.65%.
For more on thisexperiment, see (Shen et al, 2006).8746.5 Additional ExperimentsFactored translation models have also been usedfor the integration of CCG supertags (Birch et al,2007), domain adaptation (Koehn and Schroeder,2007) and for the improvement of English-Czechtranslation (Bojar, 2007).7 Conclusion and Future WorkWe presented an extension of the state-of-the-artphrase-based approach to statistical machine trans-lation that allows the straight-forward integration ofadditional information, may it come from linguistictools or automatically acquired word classes.We reported on experiments that showed gainsover standard phrase-based models, both in termsof automatic scores (gains of up to 2% BLEU), aswell as a measure of grammatical coherence.
Theseexperiments demonstrate that within the frameworkof factored translation models additional informa-tion can be successfully exploited to overcome someshort-comings of the currently dominant phrase-based statistical approach.The framework of factored translation models isvery general.
Many more models that incorporatedifferent factors can be quickly built using the ex-isting implementation.
We are currently exploringthese possibilities, for instance use of syntactic in-formation in reordering and models with augmentedinput information.We have not addressed all computational prob-lems of factored translation models.
In fact, compu-tational problems hold back experiments with morecomplex factored models that are theoretically pos-sible but too computationally expensive to carry out.Our current focus is to develop a more efficient im-plementation that will enable these experiments.Moreover, we expect to overcome the constraintsof the currently implemented synchronous factoredmodels by developing a more general asynchronousframework, where multiple translation steps mayoperate on different phrase segmentations (for in-stance a part-of-speech model for large scale re-ordering).AcknowledgmentsThis work was supported in part under the GALEprogram of the Defense Advanced Research ProjectsAgency, Contract No NR0011-06-C-0022 and inpart under the EuroMatrix project funded by the Eu-ropean Commission (6th Framework Programme).We also benefited greatly from a 2006 sum-mer workshop hosted by the Johns Hopkins Uni-versity and would like thank the other workshopparticipants for their support and insights, namelyNicola Bertoldi, Ondrej Bojar, Chris Callison-Burch, Alexandra Constantin, Brooke Cowan, ChrisDyer, Marcello Federico, Evan Herbst ChristineMoran, Wade Shen, and Richard Zens.ReferencesAlshawi, H., Bangalore, S., and Douglas, S. (1998).
Automaticacquisition of hierarchical transduction models for machinetranslation.
In Proceedings of the 36th Annual Meeting ofthe Association of Computational Linguistics (ACL).Birch, A., Osborne, M., and Koehn, P. (2007).
CCG supertagsin factored statistical machine translation.
In Proceedingsof the Second Workshop on Statistical Machine Translation,pages 9?16, Prague, Czech Republic.
Association for Com-putational Linguistics.Bojar, O.
(2007).
English-to-Czech factored machine transla-tion.
In Proceedings of the Second Workshop on StatisticalMachine Translation, pages 232?239, Prague, Czech Repub-lic.
Association for Computational Linguistics.Brill, E. (1995).
Transformation-based error-driven learningand natural language processing: A case study in part ofspeech tagging.
Computational Linguistics, 21(4).Collins, M., Koehn, P., and Kucerova, I.
(2005).
Clause re-structuring for statistical machine translation.
In Proceed-ings of the 43rd Annual Meeting of the Association for Com-putational Linguistics (ACL?05), pages 531?540, Ann Arbor,Michigan.
Association for Computational Linguistics.Galley, M., Graehl, J., Knight, K., Marcu, D., DeNeefe, S.,Wang, W., and Thayer, I.
(2006).
Scalable inference andtraining of context-rich syntactic translation models.
In Pro-ceedings of the 21st International Conference on Computa-tional Linguistics and 44th Annual Meeting of the Associa-tion for Computational Linguistics, pages 961?968, Sydney,Australia.
Association for Computational Linguistics.Koehn, P., Federico, M., Shen, W., Bertoldi, N., Hoang, H.,Callison-Burch, C., Cowan, B., Zens, R., Dyer, C., Bojar,O., Moran, C., Constantin, A., and Herbst, E. (2006).
Opensource toolkit for statistical machine translation: Factoredtranslation models and confusion network decoding.
Tech-nical report, John Hopkins University Summer Workshop.Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Federico,M., Bertoldi, N., Cowan, B., Shen, W., Moran, C., Zens, R.,Dyer, C., Bojar, O., Constantin, A., and Herbst, E. (2007).Moses: Open source toolkit for statistical machine transla-tion.
In Proceedings of the Annual Meeting of the Associa-tion for Computational Linguistics, demonstation session.Koehn, P. and Knight, K. (2003).
Feature-rich translation ofnoun phrases.
In 41st Annual Meeting of the Association ofComputational Linguistics (ACL).875Koehn, P. and Monz, C. (2006).
Manual and automatic evalua-tion of machine translation between European languages.
InProceedings on the Workshop on Statistical Machine Trans-lation, pages 102?121, NewYork City.
Association for Com-putational Linguistics.Koehn, P., Och, F. J., and Marcu, D. (2003).
Statistical phrasebased translation.
In Proceedings of the Joint Conference onHuman Language Technologies and the Annual Meeting ofthe North American Chapter of the Association of Computa-tional Linguistics (HLT-NAACL).Koehn, P. and Schroeder, J.
(2007).
Experiments in domainadaptation for statistical machine translation.
In Proceed-ings of the Second Workshop on Statistical Machine Trans-lation, pages 224?227, Prague, Czech Republic.
Associationfor Computational Linguistics.Lee, Y.-S. (2004).
Morphological analysis for statistical ma-chine translation.
In Proceedings of the Joint Conference onHuman Language Technologies and the Annual Meeting ofthe North American Chapter of the Association of Computa-tional Linguistics (HLT-NAACL).Lita, L. V., Ittycheriah, A., Roukos, S., and Kambhatla, N.(2003).
tRuEcasIng.
In Hinrichs, E. and Roth, D., editors,Proceedings of the 41st Annual Meeting of the Associationfor Computational Linguistics, pages 152?159.Melamed, I. D. (2004).
Statistical machine translation by pars-ing.
In Proceedings of the 42nd Meeting of the Associa-tion for Computational Linguistics (ACL?04), Main Volume,pages 653?660, Barcelona, Spain.Menezes, A. and Quirk, C. (2005).
Microsoft research treelettranslation system: IWSLT evaluation.
In Proc.
of the Inter-national Workshop on Spoken Language Translation.Nie?en, S. and Ney, H. (2001).
Toward hierarchical modelsfor statistical machine translation of inflected languages.
InWorkshop on Data-Driven Machine Translation at 39th An-nual Meeting of the Association of Computational Linguis-tics (ACL), pages 47?54.Och, F. J.
(2003).
Minimum error rate training for statistical ma-chine translation.
In Proceedings of the 41st Annual Meetingof the Association of Computational Linguistics (ACL).Och, F. J., Gildea, D., Khudanpur, S., Sarkar, A., Yamada, K.,Fraser, A., Kumar, S., Shen, L., Smith, D., Eng, K., Jain,V., Jin, Z., and Radev, D. (2004).
A smorgasbord of fea-tures for statistical machine translation.
In Proceedings ofthe Joint Conference on Human Language Technologies andthe Annual Meeting of the North American Chapter of theAssociation of Computational Linguistics (HLT-NAACL).Sadat, F. and Habash, N. (2006).
Combination of arabic pre-processing schemes for statistical machine translation.
InProceedings of the 21st International Conference on Com-putational Linguistics and 44th Annual Meeting of the As-sociation for Computational Linguistics, pages 1?8, Sydney,Australia.
Association for Computational Linguistics.Schmidt, H. and Schulte im Walde, S. (2000).
Robust Germannoun chunking with a probabilistic context-free grammar.
InProceedings of the International Conference on Computa-tional Linguistics (COLING).Shen, W., Zens, R., Bertoldi, N., and Federico, M. (2006).
TheJHU Workshop 2006 IWSLT System.
In Proc.
of the Inter-national Workshop on Spoken Language Translation, pages59?63, Kyoto, Japan.Talbot, D. and Osborne, M. (2006).
Modelling lexical redun-dancy for machine translation.
In Proceedings of the 21stInternational Conference on Computational Linguistics and44th Annual Meeting of the Association for ComputationalLinguistics, pages 969?976, Sydney, Australia.
Associationfor Computational Linguistics.Wang, W., Knight, K., and Marcu, D. (2006).
Capitalizing ma-chine translation.
In Proceedings of the Joint Conference onHuman Language Technologies and the Annual Meeting ofthe North American Chapter of the Association of Computa-tional Linguistics (HLT-NAACL).Wu, D. (1997).
Stochastic inversion transduction grammars andbilingual parsing of parallel corpora.
Computational Lin-guistics, 23(3).Yamada, K. and Knight, K. (2001).
A syntax-based statisticaltranslation model.
In Proceedings of the 39th Annual Meet-ing of the Association of Computational Linguistics (ACL).Yang, M. and Kirchhoff, K. (2006).
Phrase-based backoff mod-els for machine translation of highly inflected languages.
InProceedings of the 11th Conference of the European Chapterof the Association for Computational Linguistics (EACL).876
