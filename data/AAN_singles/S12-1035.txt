First Joint Conference on Lexical and Computational Semantics (*SEM), pages 265?274,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics*SEM 2012 Shared Task: Resolving the Scope and Focus of NegationRoser MoranteCLiPS - University of AntwerpPrinsstraat 13, B-2000 Antwerp, BelgiumRoser.Morante@ua.ac.beEduardo BlancoLymba CorporationRichardson, TX 75080 USAeduardo@lymba.comAbstractThe Joint Conference on Lexical and Compu-tational Semantics (*SEM) each year hosts ashared task on semantic related topics.
In itsfirst edition held in 2012, the shared task wasdedicated to resolving the scope and focus ofnegation.
This paper presents the specifica-tions, datasets and evaluation criteria of thetask.
An overview of participating systems isprovided and their results are summarized.1 IntroductionSemantic representation of text has received consid-erable attention these past years.
While early shal-low approaches have been proven useful for severalnatural language processing applications (Wu andFung, 2009; Surdeanu et al, 2003; Shen and La-pata, 2007), the field is moving towards analyzingand processing complex linguistic phenomena, suchas metaphor (Shutova, 2010) or modality and nega-tion (Morante and Sporleder, 2012).The *SEM 2012 Shared Task is devoted to nega-tion, specifically, to resolving its scope and focus.Negation is a grammatical category that comprisesdevices used to reverse the truth value of proposi-tions.
Broadly speaking, scope is the part of themeaning that is negated and focus the part of thescope that is most prominently or explicitly negated(Huddleston and Pullum, 2002).
Although negationis a very relevant and complex semantic aspect oflanguage, current proposals to annotate meaning ei-ther dismiss negation or only treat it in a partial man-ner.The interest in automatically processing nega-tion originated in the medical domain (Chapmanet al, 2001), since clinical reports and dischargesummaries must be reliably interpreted and indexed.The annotation of negation and hedge cues and theirscope in the BioScope corpus (Vincze et al, 2008)represented a pioneering effort.
This corpus boostedresearch on scope resolution, especially since it wasused in the CoNLL 2010 Shared Task (CoNLLST 2010) on hedge detection (Farkas et al, 2010).Negation has also been studied in sentiment analy-sis (Wiegand et al, 2010) as a means to determinethe polarity of sentiments and opinions.Whereas several scope detectors have been de-veloped using BioScope (Morante and Daelemans,2009; Velldal et al, 2012), there is a lack of cor-pora and tools to process negation in general domaintexts.
This is why we have prepared new corporafor scope and focus detection.
Scope is annotatedin Conan Doyle stories (CD-SCO corpus).
For eachnegation, the cue, its scope and the negated event, ifany, are marked as shown in example (1a).
Focus isannotated on top of PropBank, which uses the WSJsection of the Penn TreeBank (PB-FOC corpus).
Fo-cus annotation is restricted to verbal negations an-notated with MNEG in PropBank, and all the wordsbelonging to a semantic role are selected as focus.An annotated example is shown in (1b)1.
(1) a.
[John had] never [said as much before]b. John had never said {as much} beforeThe rest of this paper is organized as follows.The two proposed tasks are described in Section 2,and the corpora in Section 3.
Participating systemsand their results are summarized in Section 4.
Theapproaches used by participating systems are de-scribed in Section 5, as well as the analysis of re-sults.
Finally, Section 6 concludes the paper.1Throughout this paper, negation cues are marked in boldletters, scopes are enclosed in square brackets and negatedevents are underlined; focus is enclosed in curly brackets.2652 Task descriptionThe *SEM 2012 Shared Task2 was dedicated to re-solving the scope and focus of negation (Task 1 and2 respectively).
Participants were allowed to engagein any combination of tasks and submit at most tworuns per task.
A pilot task combining scope andfocus detection was initially planned, but was can-celled due to lack of participation.
We received atotal of 14 runs, 12 for scope detection (7 closed, 5open) and 2 for focus detection (0 closed, 2 open).Submissions fall into two tracks:?
Closed track.
Systems are built using exclusivelythe annotations provided in the training set and aretuned with the development set.
Systems that donot use external tools to process the input text orthat modify the annotations provided (e.g., simplifyparse tree, concatenate lists of POS tags, ) fall underthis track.?
Open track.
Systems can make use of any externalresource or tool.
For example, if a team uses an ex-ternal semantic parser, named entity recognizer orobtains the lemma for each token by querying ex-ternal resources, it falls under the open track.
Thetools used cannot have been developed or tuned us-ing the annotations of the test set.Regardless of the track, teams were allowed tosubmit their final results on the test set using a sys-tem trained on both the training and developmentsets.
The data format is the same as in several pre-vious CoNLL Shared Tasks (Surdeanu et al, 2008).Sentences are separated by a blank line.
Each sen-tence consists of a sequence of tokens, and a newline is used for each token.2.1 Task 1: Scope ResolutionTask 1 aimed at resolving the scope of negation cuesand detecting negated events.
The task is dividedinto 3 subtasks:1.
Identifying negation cues, i.e., words that expressnegation.
Cues can be single words (e.g., never),multiwords (e.g., no longer, by no means), or affixes(e.g.l im-, -less).
Note that negation cues can bediscontinuous, e.g., neither [.
.
. ]
nor.2.
Resolving the scope of negation.
This subtask ad-dresses the problem of determining which tokenswithin a sentence are affected by the negation cue.A scope is a sequence of tokens that can be discon-tinuous.2www.clips.ua.ac.be/sem2012-st-neg/3.
Identifying the negated event or property, if any.The negated event or property is always within thescope of a cue.
Only factual events can be negated.For the sentence in (2), systems have to identifyno and nothing as negation cues, after his habit hesaid and after mine I asked questions as scopes, andsaid and asked as negated events.
(2) [After his habit he said] nothing, and after mine Iasked no questions.After his habit he said nothing, and [after mine Iasked] no [questions].2.1.1 Evaluation measuresPreviously, scope resolvers have been evaluated ateither the token or scope level.
The token level eval-uation checks whether each token is correctly la-beled (inside or outside the scope), while the scopelevel evaluation checks whether the full scope is cor-rectly labeled.
The CoNLL 2010 ST introduced pre-cision and recall at scope level as performance mea-sures and established the following requirements: Atrue positive (TP) requires an exact match for boththe negation cue and the scope.
False positives (FP)occur when a system predicts a non-existing scopein gold, or when it incorrectly predicts a scope exist-ing in gold because: (1) the negation cue is correctbut the scope is incorrect; (2) the cue is incorrectbut the scope is correct; (3) both cue and scope areincorrect.
These three scenarios also trigger a falsenegative (FN).
Finally, FN also occur when the goldannotations specify a scope but the system makes nosuch prediction (Farkas et al, 2010).As we see it, the CONLL 2010 ST evaluationrequirements were somewhat strict because for ascope to be counted as TP, the negation cue hadto be correctly identified (strict match) as well asthe punctuation tokens within the scope.
Addi-tionally, this evaluation penalizes partially correctscopes more than fully missed scopes, since partiallycorrect scopes count as FP and FN, whereas missedscopes count only as FN.
This is a standard prob-lem when applying the F measures to the evaluationof sequences.
For this shared task we have adopteda slightly different approach based on the followingcriteria:?
Punctuation tokens are ignored.?
We provide a scope level measure that does not re-quire strict cue match.
To count a scope as TP this266measure requires that only one cue token is cor-rectly identified, instead of all cue tokens.?
To count a negated event as TP we do not requirecorrect identification of the cue.?
To evaluate cues, scopes and negated events, partialmatches are not counted as FP, only as FN.
This is toavoid penalizing partial matches more than missedmatches.The following evaluation measures have beenused to evaluate the systems:?
Cue-level F1-measures (Cue).?
Scope-level F1-measures that require only partialcue match (Scope NCM).?
Scope-level F1-measures that require strict cuematch (Scope CM).
In this case, all tokens of thecue have to be correctly identified.?
F1-measure over negated events (Negated), com-puted independently from cues and from scopes.?
Global F1-measure of negation (Global): the threeelements of the negation ?
cue, scope and negatedevent ?
all have to be correctly identified (strictmatch).?
F1-measure over scope tokens (Scope tokens).
Thetotal of scope tokens in a sentence is the sum of to-kens of all scopes.
For example, if a sentence hastwo scopes, one of five tokens and another of seventokens, then the total of scope tokens is twelve.?
Percentage of correct negation sentences (CNS).A second version of the measures (Cue/ScopeCM/Scope NCM/Negated/Global-B) was calculatedand provided to participants, but was not used torank the systems, because it was introduced in thelast period of the development phase following therequest of a participant team.
In the B version of themeasures, precision is not counted as (TP/(TP+FP)),but as (TP / total of system predictions), counting inthis way the percentage of perfect matches amongall the system predictions.
Providing this version ofthe measures also allowed us to compare the resultsof the two versions and to check if systems wouldbe ranked in a different position depending on theversion.Even though we believe that relaxing scope eval-uation by ignoring punctuation marks and relaxingthe strict cue match requirement is a positive featureof our evaluation, we need to explore further in orderto define a scope evaluation measure that capturesthe impact of partial matches in the scores.2.2 Task 2: Focus DetectionThis task tackles focus of negation detection.
Bothscope and focus are tightly connected.
Scope is thepart of the meaning that is negated and focus is thatpart of the scope that is most prominently or explic-itly negated (Huddleston and Pullum, 2002).
Focuscan also be defined as the element of the scope that isintended to be interpreted as false to make the over-all negative true.Detecting focus of negation is useful for retriev-ing the numerous words that contribute to implicitpositive meanings within a negation.
Consider thestatement The government didn?t release the UFOfiles {until 2008}.
The focus is until 2008, yieldingthe interpretation The government released the UFOfiles, but not until 1998.
Once the focus is resolved,the verb release, its AGENT The government and itsTHEME the UFO files are positive; only the TEMPO-RAL information until 2008 remains negated.We only target verbal negations and focus is al-ways the full text of a semantic role.
Some examplesof annotation and their interpretation (Int) using fo-cus detection are provided in (3?5).
(3) Even if that deal isn?t {revived}, NBC hopes tofind another.Int: Even if that deal is suppressed, NBC hopes tofind another.
(4) A decision isn?t expected {until some time nextyear}.Int: A decision is expected at some time next year.
(5) .
.
.
it told the SEC it couldn?t provide financialstatements by the end of its first extension?
{without unreasonable burden or expense}?.Int: It could provide them by that time with a hugeoverhead.2.2.1 Evaluation measuresTask 2 is evaluated using precision, recall and F1.Submissions are ranked by F1.
For each negation,the predicted focus is considered correct if it is a per-fect match with the gold annotations.3 Data SetsWe have released two datasets, which will be avail-able from the web site of the task: CD-SCO forscope detection and PB-FOC for focus detection.The next two sections introduce the datasets.267WL2 108 0 After After IN (S(S(PP* AfterWL2 108 1 his his PRP$ (NP* hisWL2 108 2 habit habit NN *)) habitWL2 108 3 he he PRP (NP*) heWL2 108 4 said say VBD (VP* said saidWL2 108 5 nothing nothing NN (NP*))) nothingWL2 108 6 , , , *WL2 108 7 and and CC *WL2 108 8 after after IN (S(PP* afterWL2 108 9 mine mine NN (NP*)) mineWL2 108 10 I I PRP (NP*) IWL2 108 11 asked ask VBD (VP* asked askedWL2 108 12 no no DT (NP* noWL2 108 13 questions question NNS *))) questionsWL2 108 14 .
.
.
*)Figure 1: Example sentence from CD-SCO.3.1 CD-SCO: Scope AnnotationThe corpus for Task 1 is CD-SCO, a corpus of Co-nan Doyle stories.
The training corpus contains TheHound of the Baskervilles, the development corpus,The Adventure of Wisteria Lodge, and the test corpusThe Adventure of the Red Circle and The Adventureof the Cardboard Box.
The original texts are freelyavailable from the Gutenberg Project.3CD-SCO is annotated with negation cues andtheir scope, as well as the event or property that isnegated.
The cues are the words that express nega-tion and the scope is the part of a sentence that isaffected by the negation cues.
The negated eventor property is the main event or property actuallynegated by the negation cue.
An event can be a pro-cess, an action, or a state.Figure 1 shows an example sentence.
Column 1contains the name of the file, column 2 the sentence#, column 3 the token #, column 4 the word, column5 the lemma, column 6 the PoS, column 7 the parsetree information and columns 8 to end the negationinformation.
If a sentence does not contain a nega-tion, column 8 contains ?***?
and there are no morecolumns.
If it does contain negations, the informa-tion for each one is encoded in three columns: nega-tion cue, scope, and negated event respectively.The annotation of cues and scopes is inspired bythe BioScope corpus, but there are several differ-ences.
First and foremost, BioScope does not an-notate the negated event or property.
Another im-3http://www.gutenberg.org/browse/authors/d\#a37238Training Dev.
Test# tokens 65,450 13,566 19,216# sentences 3644 787 1089# negation sent.
848 144 235% negation sent.
23.27 18.29 21.57# cues 984 173 264# unique cues 30 20 20# scopes 887 168 249# negated 616 122 173Table 1: CD-SCO Corpus statistics.portant difference concerns the scope model itself:in CD-SCO, the cue is not considered to be part ofthe scope.
Furthermore, scopes can be discontinu-ous and all arguments of the negated event are con-sidered to be part of the scope, including the subject,which is kept out of the scope in BioScope.
A finaldifference is that affixal negation is annotated in CD-SCO, as in (6).
(6) [He] declares that he heard cries but [is] un[{able}to state from what direction they came].Statistics for the corpus is presented in Table 1.More information about the annotation guidelines isprovided by Morante et al (2011) and Morante andDaelemans (2012), including inter-annotator agree-ment.The corpus was preprocessed at the Universityof Oslo.
Tokenization was obtained by the PTB-compliant tokenizer that is part of the LinGO En-glish Resource Grammar.
44http://moin.delph-in.net/268Apart from the gold annotations, the corpus wasprovided to participants with additional annotations:?
Lemmatization using the GENIA tagger (Tsuruokaand Tsujii, 2005), version 3.0.1, with the ?-nt?
com-mand line option.
GENIA PoS tags are comple-mented with TnT PoS tags for increased compati-bility with the original PTB.?
Parsing with the Charniak and Johnson (2005) re-ranking parser.5 For compatibility with PTB con-ventions, the top-level nodes in parse trees (?S1?
),were removed.
The conversion of PTB-style syntaxtrees into CoNLL-style format was performed usingthe CoNLL 2005 Shared Task software.63.2 PB-FOC: Focus AnnotationWe have adapted the only previous annotation efforttargeting focus of negation for PB-FOC (Blanco andMoldovan, 2011).
This corpus provides focus an-notation on top of PropBank.
It targets exclusivelyverbal negations marked with MNEG in PropBankand selects as focus the semantic role containing themost likely focus.
The motivation behind their ap-proach, annotation guidelines and examples can befound in the aforementioned paper.We gathered all negations from sections 02?21,23 and 24 and discarded negations for which the fo-cus or PropBank annotations were not sound, leav-ing 3,544 instances.7 For each verbal negation, PB-FOC provides the current sentence, and the previousand next sentences as context.
For each sentence,along with the gold focus annotations, PB-FOC con-tains the following additional annotations:?
Token number;?
POS tags using the Brill tagger (Brill, 1992);?
Named Entities using the Stanford named en-tity recognizer recognizer (Finkel et al, 2005);?
Chunks using the chunker by Phan (2006);?
Syntactic tree using the Charniak parser (Char-niak, 2000);?
Dependency tree derived from the syntactictree (de Marneffe et al, 2006);ErgTokenization, http://moin.delph-in.net/ReppTop5November 2009 release available from Brown University.6http://www.lsi.upc.edu/?srlconll/srlconll-1.1.tgz7The original focus annotation targeted the 3,993 negationsmarked with MNEG in the whole PropBank.Train Devel Test1 role 2,210 515 6722 roles 89 15 383 roles 3 0 2All 2,302 530 712SemanticrolesfocusbelongstoA1 980 222 309AM-NEG 592 138 172AM-TMP 161 35 46AM-MNR 127 27 38A2 112 28 36A0 94 23 31None 88 19 35AM-ADV 78 23 26C-A1 46 6 16AM-PNC 33 8 12AM-LOC 25 4 10A4 11 2 5R-A1 10 2 2Other 40 8 16Table 2: Basic numeric analysis for PB-FOC.
The first 4rows indicate the number of unique roles each negationbelongs to, the rest indicate the counts for each role.?
Semantic roles using the labeler described by(Punyakanok et al, 2008); and?
Verbal negation, indicates with ?N?
if that tokencorrespond to a verbal negation for which focusmust be predicted.Figure 2 provides a sample of PB-FOC.
Know-ing that the original focus annotations were done ontop of PropBank and that focus corresponds to a sin-gle role, semantic role information is key to predictthe focus.
In Table 2, we show some basic numericanalysis regarding focus annotation and the automat-ically obtained semantic role labels.
Most instancesof focus belong to a single role in the three splitsand the most common role focus belongs to is A1,followed by AM-NEG, M-TMP and M-MNR.
Notethat some instances have at least one word that doesnot belong to any role (88 in training, 19 in develop-ment and 35 in test).4 Submissions and resultsA total of 14 runs were submitted: 12 for scope de-tection and 2 for focus detection.
The unbalancednumber of submissions might be due to the fact thatboth tasks are relatively new and the tight timeline(six weeks) under which systems were developed.269Marketers 1 NNS O B-NP (S1(S(NP*) 2 nsubj (A0*) * - *believe 2 VBP O B-VP (VP* 0 root (V*) * - *most 3 RBS O B-NP (SBAR(S(NP* 4 amod (A1* (A0* - FOCUSAmericans 4 NNPS O I-NP *) 7 nsubj * *) - FOCUSwo 5 MD O B-VP (VP* 7 aux * (AM-MOD*) - *n?t 6 RB O I-VP * 7 neg * (AM-NEG*) - *make 7 VB O I-VP (VP* 2 ccomp * (V*) N *the 8 DT O B-NP (NP* 10 det * (A1* - *convenience 9 NN O I-NP * 10 nn * * - *trade-off 10 NN O I-NP *)))))) 7 dobj *) *) - *... 11 : O O * 2 punct * * - *.
12 .
O O *)) 2 punct * * - *Figure 2: Example sentence from PB-FOC.Team Prec.
Rec.
F1Open UConcordia, run 1 60.00 56.88 58.40UConcordia, run 2 59.85 56.74 58.26Table 3: Official results for Task 2.Some participants showed interest in the second taskand expressed that they did not participate becauseof lack of time.
In this section, we present the resultsfor each task.4.1 Task 1Six teams (UiO1, UiO2, FBK, UWashington,UMichigan, UABCoRAL) submitted results for theclosed track with a total of seven runs, and fourteams (UiO2, UGroningen, UCM-1, UCM-2) sub-mitted results for the open track with a total of fiveruns.
The evaluation results are provided in Ta-ble 4, which contains the official results, and Table 5,which contains the results for evaluation measuresB.The best Global score in the closed track was ob-tained by UiO1 (57.63 F1).
The best score for Cueswas obtained by FBK (92.34 F1), for Scopes CMby UiO2 (73.39 F1), for Scopes NCM by UWash-ington (72.40 F1), and for Negated by UiO1 (67.02F1).
The best Global score in the open track was ob-tained by UiO2 (54.82 F1), as well as the best scoresfor Cues (91.31 F1), Scopes CM (72.39 F1), ScopesNCM (72.39 F1), and Negated (61.79 F1).4.2 Task 2Only one team participated in Task 2, UConcordiafrom CLaC Lab at Concordia University.
They sub-mitted two runs and the official results are summa-rized in Table 3.
Their best run scored 58.40 F1.5 Approaches and analysisIn this section we summarize the methodologies ap-plied by participants to solve the tasks and we ana-lyze the results.5.1 Task 1To solve Task 1 most teams develop a three modulepipeline with a module per subtask.
Scope resolu-tion and negated event detection are independent ofeach other and both depend on cue detection.
Anexception is the UiO1 system, which incorporates amodule for factuality detection.
Most systems ap-ply machine learning algorithms, either ConditionalRandom Fields (CRFs) or Support Vector Machines(SVMs), while less systems implement a rule-basedapproach.
Syntax information is widely employed,either in the form of rules or incorporated in thelearning model.
Multi-word and affixal negationcues receive a special treatment in most cases, andscopes are generally postprocessed.The systems that participate in the closed trackare machine learning based.
The UiO1 system is anadaptation of another system (Velldal et al, 2012),which combines SVM cue classification with SVM-based ranking of syntactic constituents for scoperesolution.
The approach is extended to identifynegated events by first classifying negations as fac-tual or non-factual, and then applying an SVMranker over candidate events.
The original treat-ment of factuality in this system results in the high-est score for both the negated event subtask and theglobal task.The UiO2 system combines SVM cue classifica-tion with CRF-based sequence labeling.
An originalaspect of the UiO2 approach is the model represen-270OfficialresultsforTask1 CuesScopesCMScopesNCMScopeTokensNegatedGlobal%CNSPrec.Rec.F1Prec.Rec.F1Prec.Rec.F1Prec.Rec.F1Prec.Rec.F1Prec.Rec.F1ClosedtrackUiO1r289.1793.5691.3183.8960.6470.3983.8960.6470.3975.8790.0882.3760.5875.0067.0279.8745.0857.6343.83UiO1r191.4292.8092.1087.4361.4572.1787.4361.4572.1781.9988.8185.2660.5072.8966.1283.4543.9457.5742.13UiO289.1793.5691.3185.7162.6572.3985.7162.6572.3986.0381.5583.7368.1852.6359.4078.2640.9153.7340.00FBK93.4191.2992.3488.9658.2370.3988.9658.2370.3981.5382.4481.9864.1456.7160.2084.9636.3650.9335.74UWashington88.0492.0590.0082.7263.4571.8182.9064.2672.4083.2683.7783.5158.0450.9254.2574.0235.6148.0934.04UMichigan94.3187.8890.9890.0050.6064.7890.0050.6064.7884.8580.6682.7050.0052.2451.1084.2728.4142.4927.23UABCoRAL85.9385.6185.7779.0453.0163.4679.5354.6264.7685.3768.8676.2365.0038.4648.3366.3627.6539.0426.81OpentrackUiO289.1793.5691.3185.7162.6572.3985.7162.6572.3982.2582.1682.2066.9057.4061.7978.7242.0554.8241.28UGroningenr288.8984.8586.8276.1240.9653.2676.1240.9653.2669.2082.2775.1756.6365.2960.6572.0027.2739.5627.23UCM-189.2691.2990.2682.8646.5959.6482.8646.5959.6485.3768.5376.0366.6712.7221.3666.2821.5932.5718.72UCM-281.3464.3971.8867.1338.5548.9866.9038.9649.2458.3067.7062.6546.1521.1829.0342.6510.9817.4611.91UGroningenr186.9082.9584.8846.3812.8520.1246.3812.8520.1269.6970.3069.9953.9452.0552.9837.747.5812.627.66Table4:Officialresults.?r1?standsforrun1nd?r2?forrun2.CNSstandsforCorrectNegationSentences.?CM?standsforCueMatchand?NCM?standsforNoCueMatch.CuesBScopesBCMScopesBNCMNegatedBGlobalBPrec.Rec.F1Prec.Rec.F1Prec.Rec.F1Prec.Rec.F1Prec.Rec.F1ClosedtrackUiO1r286.9793.5690.1456.5560.6458.5256.5560.6458.5258.6075.0065.7941.9045.0843.43UiO1r189.0992.8090.9159.3061.4560.3659.3061.4560.3657.6272.8964.3642.1843.9443.04UiO286.9793.5690.1459.3262.6560.9459.3262.6560.9467.1652.6359.0138.0340.9139.42FBK91.6391.2991.4658.2358.2358.2358.2358.2358.2360.3956.7158.4938.0340.9139.42UWashington85.2692.0588.5258.5263.4560.8959.2664.2661.6653.9050.9252.3732.9835.6134.24UMichigan92.8087.8890.2755.5150.6052.9455.5150.6052.9438.2552.2444.1630.0028.4129.18UABCoRAL79.5885.6182.4855.2353.0154.1056.9054.6255.7462.5038.4647.6225.7027.6526.64OpentrackUiO286.9793.5690.1459.5462.6561.0659.5462.6561.0663.8257.4060.4439.0842.0540.51UGroningenr285.8284.8585.3339.8440.9640.3939.8440.9640.3955.2265.2959.8327.5927.2727.43UCM-186.6991.2988.9345.6746.5946.1345.6746.5946.1366.6712.7221.3620.5021.5921.03UCM-272.3464.3968.1341.2038.5539.8341.6338.9640.2544.4421.1828.6912.3410.9811.62UGroningenr183.9182.9583.4312.2612.8512.5512.2612.8512.5552.6652.0552.357.667.587.62Table5:ResultswithevaluationmeasuresB.Precisioniscalculatedas:truepositives/totalofsystempredictions.?r1?standsforrun1nd?r2?forrun2.
?CM?standsforCueMatchand?NCM?standsforNoCueMatch.Participatinginstitutions:UiO:UniversityofOslo;FBK:FondazioneBrunoKessler&UniversityofTrento;UWashington:UniversityofWashington;UMichigan:UniversityofMichigan;UABCoRAL:CoRALLabUniversityofAlabama;UGroningen:UniversityofGroningen;UCM:ComplutenseUniversityofMadrid.271tation for scopes and negated events, where tokensare assigned a set of labels that attempts to de-scribe their behavior within the mechanics of nega-tion.
After unseen sequences are labeled, in-scopeand negated tokens are assigned to their respectivecues using simple post-processing heuristics.The FBK system consists of three different CRFclassifiers, as well as the UMichigan.
A character-istic of the cue model of the UMichigan system isthat tokens are assigned five labels in order to rep-resent the different types of negation.
Similarly, theUWashington system has a CRF sequence tagger forscope and negated event detection, while the cue de-tector learns regular expression matching rules fromthe training set.
The UABCoRAL system followsthe same strategy, but instead of CRFs it employsSVM Light.The resources utilized by participants in the opentrack are diverse.
UiO2 reparsed the data with Malt-Parser in order to obtain dependency graphs.
For therest, the system is the same as in the closed track.The global results obtained by this system in theclosed track are higher than the results obtained inthe open track, which is mostly due to a higher per-formance of the scope resolution module.
This is theonly machine learning system in the open track andthe highest performing one.The UGroningen system is based on tools thatproduce complex semantic representations.
The sys-tem employs the C&C tools8 for parsing and Boxer9to produce semantic representations in the form ofDiscourse Representation Structures (DRSs).
Forcue detection, the DRSs are converted to flat, non-recursive structures, called Discourse Representa-tion Graphs (DRGs).
These DRGs allow for cue de-tection by means of labelled tuples.
Scope detectionis done by gathering the tokens that occur within thescope of the negated DRSs.
For negated event detec-tion, a basic algorithm takes the detected scope andreturns the negated event based on information fromthe syntax tree within the scope.UCM-1 and UCM-2 are rule-based systems thatrely heavily on information from the syntax tree.The UCM-1 system was initially designed for pro-8http://svn.ask.it.usyd.edu.au/trac/candc/wiki/Documentation9http://svn.ask.it.usyd.edu.au/trac/candc/wiki/boxercessing opinionated texts.
It applies a dictionary ap-proach to cue detection, with the detection of affixalcues being performed using WordNet.
Non-affixalcue detection is performed by consulting a prede-fined list of cues.
It then uses information from thesyntax tree in order to get a first approximation tothe scope, which is later refined using a set of post-processing rules.
In the case of the UCM-2 systeman algorithm detects negation cues and their scopeby traversing Minipar dependency structures.
Fi-nally, the scope is refined with post-processing rulesthat take into account the information provided bythe first algorithm and linguistic clause boundaries.If we compare tracks, the Global best results ob-tained in the closed track (57.63 F1) are higher thanthe Global best results obtained in the open track(54.82 F1).
If we compare approaches, the best re-sults in the two tracks are obtained with machinelearning-based systems.
The rule-based systemsparticipating in the open track clearly score lower(39.56 F1 the best) than the machine learning-basedsystem (54.82 F1).Regarding subtasks, systems achieve higher re-sults in the cue detection task (92.34 F1 the best) andlower results in the scope resolution (72.40 F1 thebest) and negated event detection (67.02 F1 the best)tasks.
This is not surprising, not only because ofthe error propagation effect, but also because the setof negation cues is closed and comprises mostly sin-gle tokens, whereas scope sequences are longer.
Thebest results in cue detection are obtained by the FBKsystem that uses CRFs and applies a special proce-dure to detect the negation cues that are subtokens.The best scores for scope resolution (72.40, 72.39F1) are obtained by two machine learning compo-nents.
UWashington uses CRFs with features de-rived from the syntax tree.
UiO2 uses CRFs mod-els with syntactic and lexical features for scopes, to-gether with a set of labels aimed at capturing thebehavior of certain tokens within the mechanics ofnegation.
The best scores for negated events (67.02F1) are obtained by the UiO1 system that first clas-sifies negations as factual or non-factual, and thenapplies an SVM ranker over candidate events.Finally, we would like to draw the attention to thedifferent scores obtained depending on the evalua-tion measure used.
When scope resolution is evalu-ated with the Scope (NCM, CM) measure, results272are much lower than when using the Scope To-kens measure, which does not reflect the ability ofsystems to deal with sequences.
Another observa-tion is related to the difference in precision scoresbetween the two versions of the evaluation mea-sures.
Whereas for Cues and Negated the differ-ences are not so big because most cues and negatedevents span over a single token, for Scopes they are.The best Scope NCM precision score is 90.00 %,whereas the best Scope NCM B precision score is59.54 %.
This shows that the scores can changeconsiderably depending on how partial matches arecounted (as FP and FN, or only as FN).
As a finalremark it is worth noting that the ranking of systemsdoes not change when using the B measures.5.2 Task 2UConcordia submitted two runs in the open track.Both of them follow the same three component ap-proach.
First, negation cues are detected.
Second,the scope of negation is extracted based on depen-dency relations and heuristics defined by Kilicogluand Bergler (2011).
Third, the focus of negationis determined within the elements belonging to thescope following three heuristics.6 ConclusionsIn this paper we presented the description of the first*SEM Shared Task on Resolving the Scope and Fo-cus of Negation, which consisted of two differenttasks related to different aspects of negation: Task 1on resolving the scope of negation, and Task 2 ondetecting the focus of negation.
Task 1 was di-vided into three subtasks: identifying negation cues,resolving their scope, and identifying the negatedevent.
Two new datasets have been produced for thisShared Task: the CD-SCO corpus of Conan Doylestories annotated with scopes, and the PB-FOC cor-pus, which provides focus annotation on top of Prop-Bank.
New evaluation software was also developedfor this task.
The datasets and the evaluation soft-ware will be available on the web site of the SharedTask.
As far as we know, this is the first task that fo-cuses on resolving the focus and scope of negation.A total of 14 runs were submitted, 12 for scopedetection and 2 for focus detection.
Of these, fourruns are from systems that take a rule-based ap-proach, two runs from hybrid systems, and the restfrom systems that take a machine learning approachusing SVMs or CRFs.
Most participants designed athree component architecture.For a future edition of the shared task we wouldlike to unify the annotation schemes of the two cor-pora, namely the annotation of focus in PB-FOC andnegated events in CD-SCO.
The annotation of moredata with both scope and focus would allow us tostudy the two aspects jointly.
We would also like toprovide better evaluation measures for scope reso-lution.
Currently, scopes are evaluated in terms ofF1, which demands a division of errors into the cat-egories TP/FP/TN/FN borrowed from the evaluationof information retrieval systems.
These categoriesare not completely appropriate to be assigned to se-quence tasks, such as scope resolution.AcknowledgementsWe are very grateful to Vivek Srikumar for pre-processing the PB-FOC corpus with the Illinois se-mantic role labeler, and to Stephan Oepen for pre-processing the CD-SCO corpus.
We also thank the*SEM organisers and the ST participants.
RoserMorante?s research was funded by the University ofAntwerp (GOA project BIOGRAPH).ReferencesEduardo Blanco and Dan Moldovan.
2011.
SemanticRepresentation of Negation Using Focus Detection.
InProceedings of the 49th Annual Meeting of the Asso-ciation for C omputational Linguistics: Human Lan-guage Technologies, pages 581?589, Portland, Ore-gon, USA.
Association for Computational Linguistics.Eric Brill.
1992.
A simple rule-based part of speech tag-ger.
In Proceedings of the third conference on Appliednatural language processing, ANLC ?92, pages 152?155, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Wendy W. Chapman, Will Bridewell, Paul Hanbury, Gre-gory F. Cooper, and Bruce G. Buchanan.
2001.
Asimple algorithm for identifying negated findings anddiseases in discharge summaries.
J Biomed Inform,34:301?310.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminative rerank-ing.
In Proceedings of the 43rd Annual Meeting ofthe Association for Computational Linguistics, pages173?180, Ann Arbor.273Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proceedings of the 1st North Americanchapter of the Association for Computational Lin-guistics conference, NAACL 2000, pages 132?139,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating TypedDependency Parses from Phrase Structure Parses.
InProceedings of the IEEE / ACL 2006 Workshop onSpoken Language Technology.
The Stanford NaturalLanguage Processing Group.Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nosCsirik, and Gyo?rgy Szarvas.
2010.
The CoNLL-2010Shared Task: Learning to Detect Hedges and theirScope in Natural Language Text.
In Proceedings ofthe Fourteenth Conference on Computational NaturalLanguage Learning, pages 1?12, Uppsala, Sweden.Association for Computational Linguistics.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by gibbs sam-pling.
In Proceedings of the 43rd Annual Meeting onAssociation for Computational Linguistics, ACL ?05,pages 363?370, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Rodney D. Huddleston and Geoffrey K. Pullum.
2002.The Cambridge Grammar of the English Language.Cambridge University Press.Halil Kilicoglu and Sabine Bergler.
2011.
Effective bio-event extraction using trigger words and syntactic de-pendencies.
Computational Intelligence, 27(4):583?609.Roser Morante and Walter Daelemans.
2009.
A met-alearning approach to processing the scope of nega-tion.
In Proceedings of the 13th Conference on Natu-ral Language Learning, pages 21?29, Boulder, CO.Roser Morante and Walter Daelemans.
2012.ConanDoyle-neg: Annotation of negation cues andtheir scope in Conan Doyle stories.
In Proceedingsof LREC 2012, Istambul.Roser Morante and Caroline Sporleder.
2012.
Special is-sue on modality and negation: An introduction.
Com-putational Linguistics.Roser Morante, Sarah Schrauwen, and Walter Daele-mans.
2011.
Annotation of negation cues and theirscope.
guidelines v1.0.
Technical Report Series CTR-003, CLiPS, University of Antwerp, Antwerp, April.Xuan-Hieu Phan.
2006.
Crfchunker: Crf english phrasechunker.Vasin Punyakanok, Dan Roth, and Wen-tau Yih.
2008.The importance of syntactic parsing and inference insemantic role labeling.
Computational Linguistics,34(2):257?287, June.Dan Shen and Mirella Lapata.
2007.
Using SemanticRoles to Improve Question Answering.
In Proceed-ings of the 2007 Joint Conference on Empirical Meth-ods in Natural Language Processing and Computa-tional Natural Language Learning (EM NLP-CoNLL),pages 12?21.Ekaterina Shutova.
2010.
Models of Metaphor in NLP.In Proceedings of the 48th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 688?697,Uppsala, Sweden.
ACL.Mihai Surdeanu, Sanda Harabagiu, John Williams, andPaul Aarseth.
2003.
Using Predicate-Argument Struc-tures for Information Extraction.
In Proceedings of the41st Annual Meeting of the Association for Computa-tional Linguistics, pages 8?15, Sapporo, Japan.
Asso-ciation for Computational Linguistics.Mihai Surdeanu, Richard Johansson, Adam Meyers,Llu?
?s Ma`rquez, and Joakim Nivre.
2008.
The conll-2008 shared task on joint parsing of syntactic and se-mantic dependencies.
In CoNLL 2008: Proceedings ofthe 12th Conference on Computational Natural Lan-guage Learning, page 159177, Manchester.Yoshimasa Tsuruoka and Jun?ichi Tsujii.
2005.
Bidi-rectional inference with the easiest-first strategy fortagging sequence data.
In Proceedings of of HumanLanguage Technology Conference and Conference onEmpirical Methods in Natural Language Processing,pages 467?474, Vancouver.Erik Velldal, Lilja ?vrelid, Jonathon Read, and StephanOepen.
2012.
Speculation and negation: Rules,rankers, and the role of syntax.
Computational Lin-guistics.Veronika Vincze, Gyorgy Szarvas, Richard Farkas, Gy-orgy Mora, and Janos Csirik.
2008.
The Bio-Scope corpus: biomedical texts annotated for uncer-tainty, negation and their scopes.
BMC Bioinformat-ics, 9(Suppl 11):S9+.Michael Wiegand, Alexandra Balahur, Benjamin Roth,Dietrich Klakow, and Andre?s Montoyo.
2010.
A sur-vey on the role of negation in sentiment analysis.
InProceedings of the Workshop on Negation and Specu-lation in Natural Language Processing, pages 60?68,Uppsala, Sweden.
University of Antwerp.Dekai Wu and Pascale Fung.
2009.
Semantic Rolesfor SMT: A Hybrid Two-Pass Model.
In Proceedingsof Human Language Technologies: The 2009 AnnualConference of the North American Chapter of the As-sociation for Computational Linguistics, CompanionVolume: Short Papers, pages 13?16, Boulder, Col-orado.
Association for Computational Linguistics.274
