Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1488?1497,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsFine-grained Semantic Typing of Emerging EntitiesNdapandula Nakashole, Tomasz Tylenda, Gerhard WeikumMax Planck Institute for InformaticsSaarbru?cken, Germany{nnakasho,ttylenda,weikum}@mpi-inf.mpg.deAbstractMethods for information extraction (IE)and knowledge base (KB) constructionhave been intensively studied.
However, alargely under-explored case is tapping intohighly dynamic sources like news streamsand social media, where new entities arecontinuously emerging.
In this paper, wepresent a method for discovering and se-mantically typing newly emerging out-of-KB entities, thus improving the freshnessand recall of ontology-based IE and im-proving the precision and semantic rigorof open IE.
Our method is based on a prob-abilistic model that feeds weights into in-teger linear programs that leverage typesignatures of relational phrases and typecorrelation or disjointness constraints.
Ourexperimental evaluation, based on crowd-sourced user studies, show our methodperforming significantly better than priorwork.1 IntroductionA large number of knowledge base (KB) con-struction projects have recently emerged.
Promi-nent examples include Freebase (Bollacker 2008)which powers the Google Knowledge Graph, Con-ceptNet (Havasi 2007), YAGO (Suchanek 2007),and others.
These KBs contain many millions ofentities, organized in hundreds to hundred thou-sands of semantic classes, and hundred millionsof relational facts between entities.
However, de-spite these impressive advances, there are still ma-jor limitations regarding coverage and freshness.Most KB projects focus on entities that appear inWikipedia (or other reference collections such asIMDB), and very few have tried to gather entities?in the long tail?
beyond prominent sources.
Vir-tually all projects miss out on newly emerging en-tities that appear only in the latest news or socialmedia.
For example, the Greenlandic singer NiveNielsen has gained attention only recently and isnot included in any KB (a former Wikipedia articlewas removed because it ?does not indicate the im-portance or significance of the subject?
), and theresignation of BBC director Entwistle is a recentlynew entity (of type event).Goal.
Our goal in this paper is to discover emerg-ing entities of this kind on the fly as they becomenoteworthy in news and social-media streams.
Asimilar theme is pursued in research on open infor-mation extraction (open IE) (Banko 2007; Fader2011; Talukdar 2010; Venetis 2011; Wu 2012),which yields higher recall compared to ontology-style KB construction with canonicalized and se-mantically typed entities organized in prespecifiedclasses.
However, state-of-the-art open IE meth-ods extract all noun phrases that are likely to de-note entities.
These phrases are not canonical-ized, so the same entity may appear under manydifferent names, e.g., ?Mr.
Entwistle?, ?GeorgeEntwistle?, ?the BBC director?, ?BBC head En-twistle?, and so on.
This is a problem becausenames and titles are ambiguous, and this hampersprecise search and concise results.Our aim is for all recognized and newly dis-covered entities to be semantically interpretableby having fine-grained types that connect themto KB classes.
The expectation is that this willboost the disambiguation of known entity namesand the grouping of new entities, and will alsostrengthen the extraction of relational facts aboutentities.
For informative knowledge, new entitiesmust be typed in a fine-grained manner (e.g., gui-tar player, blues band, concert, as opposed to crudetypes like person, organization, event).Strictly speaking, the new entities that we cap-1488ture are typed noun phrases.
We do not attemptany cross-document co-reference resolution, asthis would hardly work with the long-tail na-ture and sparse observations of emerging entities.Therefore, our setting resembles the establishedtask of fine-grained typing for noun phrases (Fleis-chmann 2002), with the difference being that wedisregard common nouns and phrases for promi-nent in-KB entities and instead exclusively focuson the difficult case of phrases that likely denotenew entities.
The baselines to which we compareour method are state-of-the-art methods for noun-phrase typing (Lin 2012; Yosef 2012).Contribution.
The solution presented in thispaper, called PEARL, leverages a repository ofrelational patterns that are organized in a type-signature taxonomy.
More specifically, we har-ness the PATTY collection consisting of morethan 300,000 typed paraphrases (Nakashole 2012).An example of PATTY?s expressive phrases is:?musician?
* cover * ?song?
for a musician per-forming someone else?s song.
When extract-ing noun phrases, PEARL also collects the co-occurring PATTY phrases.
The type signatures ofthe relational phrases are cues for the type of theentity denoted by the noun phrase.
For example,an entity named Snoop Dogg that frequently co-occurs with the ?singer?
* distinctive voice in *?song?
pattern is likely to be a singer.
Moreover,if one entity in a relational triple is in the KB andcan be properly disambiguated (e.g., a singer), wecan use a partially bound pattern to infer the typeof the other entity (e.g., a song) with higher confi-dence.In this line of reasoning, we also leverage thecommon situation that many input sentences con-tain one entity registered in the KB and one novelor unknown entity.
Known entities are recognizedand mapped to the KB using a recent tool fornamed entity disambiguation (Hoffart 2011).
Forcleaning out false hypotheses among the type can-didates for a new entity, we devised probabilisticmodels and an integer linear program that consid-ers incompatibilities and correlations among entitytypes.In summary, our contribution in this paper isa model for discovering and ontologically typ-ing out-of-KB entities, using a fine-grained typesystem and harnessing relational paraphrases withtype signatures for probabilistic weight computa-tion.
Crowdsourced quality assessments demon-strate the accuracy of our model.2 Detection of New EntitiesTo detect noun phrases that potentially refer to en-tities, we apply a part-of-speech tagger to the in-put text.
For a given noun phrase, there are fourpossibilities: a) The noun phrase refers to a gen-eral concept (a class or abstract concept), not anindividual entity.
b) The noun phrase is a knownentity that can be directly mapped to the knowl-edge base.
c) The noun phrase is a new name fora known entity.
d) The noun phrase is a new entitynot known to the knowledge base at all.
In this pa-per, our focus is on case d); all other cases are outof the scope of this paper.We use an extensive dictionary of surface formsfor in-KB entities (Hoffart 2012), to determine ifa name or phrase refers to a known entity.
If aphrase does not have any match in the dictionary,we assume that it refers to a new entity.
To decideif a noun phrase is a true entity (i.e., an individ-ual entity that is a member of one or more lexi-cal classes) or a non-entity (i.e., a common nounphrase that denotes a class or a general concept),we base the decision on the following hypothesis(inspired by and generalizing (Bunescu 2006): Agiven noun phrase, not known to the knowledgebase, is a true entity if its headword is singularand is consistently capitalized (i.e., always spelledwith the first letter in upper case).3 Typing Emerging EntitiesTo deduce types for new entities we propose toalign new entities along the type signatures of pat-terns they occur with.
In this manner we use thepatterns to suggest types for the entities they occurwith.
In particular, we infer entity types from pat-tern type signatures.
Our approach builds on thefollowing hypothesis:Hypothesis 3.1 (Type Alignment Hypothesis)For a given pattern such as ?actor?
?s characterin ?movie?, we assume that an entity pair (x, y)frequently occurring with the pattern in textimplies that x and y are of the types ?actor?
and?movie?, respectively.Challenges and Objective.
While the type align-ment hypothesis works as a starting point, it in-troduces false positives.
Such false positives stem1489from the challenges of polysemy, fuzzy patternmatches, and incorrect paths between entities.With polysemy, the same lexico-syntactic patterncan have different type signatures.
For example,the following are three different patterns: ?singer?released ?album?, ?music band?
released ?album?,?company?
released ?product?.
For an entity pair(x, y) occurring with the pattern ?released?, x canbe one of three different types.We cannot expect that the phrases we extract intext will be exact matches of the typed relationalpatterns learned by PATTY.
Therefore, for betterrecall, we must accept fuzzy matches.
Quite oftenhowever, the extracted phrase matches multiple re-lational patterns to various degrees.
Each of thematched relational patterns has its own type sig-nature.
The type signatures of the various matchedpatterns can be incompatible with one another.The problem of incorrect paths between entitiesemerges when a pair of entities occurring in thesame sentence do not stand in a true subject-objectrelation.
Dependency parsing does not adequatelysolve the issue.
Web sources contain a plethoraof sentences that are not well-formed.
Such sen-tences mislead the dependency parser to extractwrong dependencies.Our solution takes into account polysemy, fuzzymatches, as well as issues stemming from poten-tial incorrect-path limitations.
We define and solvethe following optimization problem:Definition 1 (Type Inference Optimization)Given all the candidate types for x, find thebest types or ?strongly supported?
types for x.The final solution must satisfy type disjointnessconstraints.
Type disjointness constraints areconstraints that indicate that, semantically, a pairof types cannot apply to the same entity at thesame time.
For example, a ?university?
cannot bea ?person?.We also study a relaxation of type disjointnessconstraints through the use of type correlation con-straints.
Our task is therefore twofold: first, gen-erate candidate types for new entities; second, findthe best types for each new entity among its can-didate types.4 Candidate Types for EntitiesFor a given entity, candidate types are types thatcan potentially be assigned to that entity, based onthe entity?s co-occurrences with typed relationalpatterns.Definition 2 (Candidate Type) Given a new en-tity x which occurs with a number of patternsp1, p2, ..., pn, where each pattern pi has a type sig-nature with a domain and a range: if x occurs onthe left of pi, we pick the domain of pi as a candi-date type for x; if x occurs on the right of pi, wepick the range of pi as a candidate type for x.For each candidate type, we compute confi-dence weights.
Ideally, if an entity occurs witha pattern which is highly specific to a given typethen the candidate type should have high con-fidence.
For example ?is married to?
is morespecific to people then ?expelled from?.
A per-son can be expelled from an organization but acountry can also be expelled from an organizationsuch as NATO.
There are various ways to com-pute weights for candidate types.
We first intro-duce a uniform weight approach and then present amethod for computing more informative weights.4.1 Uniform WeightsWe are given a new entity x which occurs withphrases (x phrase1 y1), (x phrase2 y2), ..., (xphrasen yn).
Suppose these occurrences leadto the facts (x, p1, y1), (x, p2, y2),..., (x, pn, yn).The pis are the typed relational patterns extractedby PATTY.
The facts are generated by matchingphrases to relational patterns with type signa-tures.
The type signature of a pattern is denotedby:sig(pi) = (domain(pi), range(pi))We allow fuzzy matches, hence each fact comeswith a match score.
This is the similarity degreebetween the phrase observed in text and the typedrelational pattern.Definition 3 (Fuzzy Match Score) Suppose weobserve the surface string: (x phrase y) whichleads to the fact: x, pi, y.
The fuzzy match similar-ity score is: sim(phrase, pi), where similarity isthe n-gram Jaccard similarity between the phraseand the typed pattern.The confidence that x is of type domain is de-fined as follows:Definition 4 (Candidate Type Confidence)For a given observation (x phrase y), where1490phrase matches patterns p1, ..., pn, with domainsd1, ..., db which are possibly the same:typeConf(x, phrase, d) =?
{pi:domain(pi)=d}(sim(phrase, pi))Observe that this sums up over all patterns thatmatch the phrase.To compute the final confidence fortypeConf(x, domain), we aggregate theconfidences over all phrases occurring with x.Definition 5 (Aggregate Confidence) Fora set of observations (x, phrase1, y1),(x, phrase2, y2), ..., (x, phrasen, yn), theaggregate candidate type confidence is given by:aggTypeConf(x, d) =?phraseitypeConf(x, phrasei, d)=?phrasei?
{pj :domain(pj)=d}(sim(phrasei, pj))The confidence for the rangetypeConf(x, range) is computed analogously.All confidence weights are normalized to valuesin [0, 1].The limitation of the uniform weight approachis that each pattern is considered equally good forsuggesting candidate types.
Thus this approachdoes not take into account the intuition that an en-tity occurring with a pattern which is highly spe-cific to a given type is a stronger signal that theentity is of the type suggested.
Our next approachaddresses this limitation.4.2 Co-occurrence Likelihood WeightComputationWe devise a likelihood model for computingweights for entity candidate types.
Central to thismodel is the estimation of the likelihood of a giventype occurring with a given pattern.Suppose using PATTY methods we mined atyped relational pattern ?t1?
p ?t2?.
Suppose thatwe now encounter a new entity pair (x, y) occur-ring with a phrase that matches p. We can com-pute the likelihood of x and y being of types t1and t2, respectively, from the likelihood of p co-occurring with entities of types t1, t2.
Thereforewe are interested in the type-pattern likelihood,defined as follows:Definition 6 (Type-Pattern Likelihood) Thelikelihood of p co-occurring with an entity pair(x, y) of the types (t1, t2) is given by:P [t1, t2|p] (1)where t1 and t2 are the types of the arguments ob-served with p from a corpus such as Wikipedia.P [t1, t2|p] is expanded as follows:P [t1, t2|p] =P [t1, t2, p]P [p] .
(2)The expressions on the right-hand side of Equa-tion 2 can be directly estimated from a corpus.We use Wikipedia (English), for corpus-based es-timations.
P [t1, t2, p] is the relative occurrencefrequency of the typed pattern among all entity-pattern-entity triples in a corpus (e.g., the frac-tion of ?musican?
plays ?song?
among all triples).P[p] is the relative occurrence frequency of the un-typed pattern (e.g., plays) regardless of the argu-ment types.
For example, this sums up over both?musican?
plays ?song?
occurrences and ?actor?plays ?fictional character?.
If we observe a factwhere one argument name can be easily disam-biguated to a knowledge-base entity so that its typeis known, and the other argument is considered tobe an out-of-knowledge-base entity, we conditionthe joint probability of t1, p, and t2 in a differentway:Definition 7 (Conditional Type-PatternLikelihood)The likelihood of an entity of type t1 occurringwith a pattern p and an entity of type t2 is givenby:P [t1|t2, p] =P [t1, t2, p]P [p, t2](3)where the P [p, t2] is the relative occurrence fre-quency of a partial triple, for example, ?*?
plays?song?.Observe that all numbers refer to occurrencefrequencies.
For example, P [t1, p, t2] is a frac-tion of the total number of triples in a corpus.Multiple patterns can suggest the same type foran entity.
Therefore, the weight of the assertionthat y is of type t, is the total support strength fromall phrases that suggest type t for y.Definition 8 (Aggregate Likelihood) The aggre-gate likelihood candidate type confidence is given1491by:typeConf(x, domain)) =?phrasei?pj(sim(phrasei, pj) ??
)Where ?
= P [t1, t2|p] or P [t1|t2, p] or P [t2|t1, p]The confidence weights are normalized to valuesin [0, 1].
So far we have presented a way of gener-ating a number of weighted candidate types for x.In the next step we pick the best types for an entityamong all its candidate types.4.3 Integer Linear Program FormulationGiven a set of weighted candidate types, our goalis to pick a compatible subset of types for x. Theadditional asset that we leverage here is the com-patibility of types: how likely is it that an entitybelongs to both type ti and type tj .
Some typesare mutually exclusive, for example, the type loca-tion rules out person and, at finer levels, city rulesout river and building, and so on.
Our approachharnesses these kinds of constraints.
Our solutionis formalized as an Integer Linear Program (ILP).We have candidate types for x: t1, .., tn.
First, wedefine a decision variable Ti for each candidatetype i = 1, .
.
.
, n. These are binary variables:Ti = 1 means type ti is selected to be includedin the set of types for x, Ti = 0 means we discardtype ti for x.In the following we develop two variants of thisapproach: a ?hard?
ILP with rigorous disjointnessconstraints, and a ?soft?
ILP which considers typecorrelations.?Hard?
ILP with Type Disjointness Con-straints.
We infer type disjointness constraintsfrom the YAGO2 knowledge base using occur-rence statistics.
Types with no overlap in entitiesor insignificant overlap below a specified thresh-old are considered disjoint.
Notice that this intro-duces hard constraints whereby selecting one typeof a disjoint pair rules out the second type.
We de-fine type disjointness constraints Ti + Tj ?
1 forall disjoint pairs ti, tj (e.g.
person-artifact, movie-book, city-country, etc.).
The ILP is defined asfollows:objectivemax?i Ti ?
witype disjointness constraint?
(ti, tj)disjoint Ti + Tj ?
1The weights wi are the aggregrated likelihoodsas specified in Definition 8.?Soft?
ILP with Type Correlations.
In manycases, two types are not really mutually exclusivein the strict sense, but the likelihood that an en-tity belongs to both types is very low.
For exam-ple, few drummers are also singers.
Conversely,certain type combinations are boosted if they arestrongly correlated.
An example is guitar playersand electric guitar players.
Our second ILP con-siders such soft constraints.
To this end, we pre-compute Pearson correlation coefficients for alltype pairs (ti, tj) based on co-occurrences of typesfor the same entities.
These values vij ?
[?1, 1]are used as weights in the objective function ofthe ILP.
We additionally introduce pair-wise deci-sion variables Yij , set to 1 if the entity at hand be-longs to both types ti and tj , and 0 otherwise.
Thiscoupling between the Yij variables and the Ti, Tjvariables is enforced by specific constraints.
Forthe objective function, we choose a linear combi-nation of per-type evidence, using weights wi asbefore, and the type-compatibility measure, usingweights vij .
The ILP with correlations is definedas follows:objectivemax ?
?i Ti ?
wi + (1?
?
)?ij Yij ?
vijtype correlation constraints?i,j Yij + 1 ?
Ti + Tj?i,j Yij ?
Ti?i,j Yij ?
TjNote that both ILP variants need to be solvedper entity, not over all entities together.
The ?soft?ILP has a size quadratic in the number of candidatetypes, but this is still a tractable input for modernsolvers.
We use the Gurobi software package tocompute the solutions for the ILP?s.
With this de-sign, PEARL can efficiently handle a typical newsarticle in less than a second, and is well geared forkeeping up with high-rate content streams in realtime.
For both the ?hard?
and ?soft?
variants ofthe ILP, the solution is the best types for entity xsatisfying the constraints.14925 EvaluationTo define a suitable corpus of test data, we ob-tained a stream of news documents by subscrib-ing to Google News RSS feeds for a few topicsover a six-month period (April 2012 ?
Septem-ber 2012).
This produced 318, 434 documents.The topics we subscribed to are: Angela Merkel,Barack Obama, Business, Entertainment, HillaryClinton, Joe Biden, Mitt Romney, Newt Gingrich,Rick Santorum, SciTech and Top News.
All our ex-periments were carried out on this data.
The typesystem used is that of YAGO2, which is derivedfrom WordNet.
Human evaluations were carriedout on Amazon Mechanical Turk (MTurk), whichis a platform for crowd-sourcing tasks that requirehuman input.
Tasks on MTurk are small question-naires consisting of a description and a set of ques-tions.Baselines.
We compared PEARL against twostate-of-the-art baselines: i).
NNPLB (No NounPhrase Left Behind), is the method presented in(Lin 2012), based on the propagation of typesfor known entities through salient patterns occur-ring with both known and unknown entities.
Weimplemented the algorithm in (Lin 2012) in ourframework, using the relational patterns of PATTY(Nakashole 2012) for comparability.
For assess-ment we sampled from the top-5 highest rankedtypes for each entity.
In our experiments, our im-plementation of NNPLB achieved precision valuescomparable to those reported in (Lin 2012).
ii).HYENA (Hierarchical tYpe classification for En-tity NAmes), the method of (Yosef 2012), basedon a feature-rich classifier for fine-grained, hierar-chical type tagging.
This is a state-of-the-art rep-resentative of similar methods such as (Rahman2010; Ling 2012).Evaluation Task.
To evaluate the quality of typesassigned to emerging entities, we presented turk-ers with sentences from the news tagged with out-of-KB entities and the types inferred by the meth-ods under test.
The turkers task was to assess thecorrectness of types assigned to an entity mention.To make it easy to understand the task for the turk-ers, we combined the extracted entity and type intoa sentence.
For example if PEARL inferred thatBrussels Summit is an political event, we generateand present the sentence: Brussels Summit is anevent.
We allowed four possible assessment val-ues: a) Very good output corresponds to a perfectresult.
b) Good output exhibits minor errors.
Forinstance, the description G20 Summit is an orga-nization is wrong, because the summit is an event,but G20 is indeed an organization.
The problem inthis example is incorrect segmentation of a namedentity.
c) Wrong for incorrect types (e.g., BrusselsSummit is a politician).
d) Not sure / do not knowfor other cases.Comparing PEARL to Baselines.
Per method,turkers evaluated 105 entity-type pair test sam-ples.
We first sampled among out-of-KB entitiesthat were mentioned frequently in the news cor-pus: in at least 20 different news articles.
Eachtest sample was given to 3 different turkers for as-sessment.
Since the turkers did not always agreeif the type for a sample is good or not, we ag-gregate their answers.
We use voting to decidewhether the type was assigned correctly to an en-tity.
We consider the following voting variants:i) majority ?very good?
or ?good?, a conservativenotion of precision: precisionlower.
ii) at leastone ?very good?
or ?good?, a liberal notion ofprecision: precisionupper.
Table 1 shows preci-sion for PEARL-hard, PEARL-soft, NNPLB, andHYENA, with a 0.9-confidence Wilson score in-terval (Brown 2001).
PEARL-hard outperformedPEARL-soft and also both baselines.
HYENA?srelatively poor performance can be attributed tothe fact that its features are mainly syntactic suchas bi-grams and part-of-speech tags.
Web data ischallenging, it has a lot of variations in syntac-tic formulations.
This introduces a fair amountof ambiguity which can easily mislead syntacticfeatures.
Leveraging semantic features as doneby PEARL could improve HYENA?s performance.While the NNPLB method performs better thanHYENA, in comparison to PEARL-hard, there isroom for improvement.
Like HYENA, NNPLBassigns negatively correlated types to the same en-tity.
This limitation could be addressed by apply-ing PEARL?s ILPs and probabilistic weights to thecandidate types suggested by NNPLB.To compute inter-judge agreement we calcu-lated Fleiss?
kappa and Cohen?s kappa ?, whichare standard measures.
The usual assumption forFleiss??
is that labels are categorical, so that eachdisagreement counts the same.
This is not the casein our settings, where different labels may indicatepartial agreement (?good?, ?very good?).
There-1493Precisionlower PrecisionupperPEARL-hard 0.77?0.08 0.88?0.06PEARL-soft 0.53?0.09 0.77?0.09HYENA 0.26?0.08 0.56?0.09NNPLB 0.46?0.09 0.68?0.09Table 1: Comparison of PEARL to baselines.?
F leiss Cohen0.34 0.45Table 2: Lower bound estimations for inter-judgeagreement kappa: Fleiss?
?
& adapted Cohen?s ?.fore the ?
values in Table 2 are lower-bound esti-mates of agreement in our experiments; the ?trueagreement?
seems higher.
Nevertheless, the ob-served Fleiss ?
values show that the task was fairlyclear to the turkers; values > 0.2 are generallyconsidered as acceptable (Landis 1977).
Cohen?s?
is also not directly applicable to our setting.
Weapproximated it by finding pairs of judges who as-sessed a significant number of the same entity-typepairs.Precisionlower PrecisionupperFreq.
mentions 0.77?0.08 0.88?0.06All mentions 0.65?0.09 0.77?0.08Table 3: PEARL-hard performance on a sample offrequent entities (mention frequency?
20) and ona sample of entities of all mention frequencies.Mention Frequencies.
We also studied PEARL-hard?s performance on entities of different men-tion frequencies.
The results are shown in Ta-ble 3.
Frequently mentioned entities providePEARL with more evidence as they potentially oc-cur with more patterns.
Therefore, as expected,precision when sampling over all entities dropsa bit.
For such infrequent entities, PEARL doesnot have enough evidence for reliable type assign-ments.Variations of PEARL.
To quantify how variousaspects of our approach affect performance, westudied a few variations.
The first method is thefull PEARL-hard.
The second method is PEARLwith no ILP (denoted No ILP), only using theprobabilistic model.
The third variation is PEARLwithout probabilistic weights (denoted UniformFigure 1: Variations of the PEARL method.Weights).
From Figure 1, it is clear that both theILP and the weighting model contribute signifi-cantly to PEARL?s ability to make precise type as-signments.
Sample results from PEARL-hard areshown in Table 4.NDCG.
For a given entity mention e, an entity-typing system returns a ranked list of types{t1, t2, ..., tn}.
We evaluated ranking quality us-ing the top-5 ranks for each method.
These assess-ments were aggregated into the normalized dis-counted cumulative gain (NDCG), a widely usedmeasure for ranking quality.
The NDCG valuesobtained are 0.53, 0.16, and 0.16, for PEARL-hard, HYENA, and NNPLB, respectively.
PEARLclearly outperforms the baselines on ranking qual-ity, too.6 Related WorkTagging mentions of named entities with lexicaltypes has been pursued in previous work.
Mostwell-known is the Stanford named entity recog-nition (NER) tagger (Finkel 2005) which assignscoarse-grained types like person, organization, lo-cation, and other to noun phrases that are likely todenote entities.
There is fairly little work on fine-grained typing, notable results being (Fleischmann2002; Rahman 2010; Ling 2012; Yosef 2012).These methods consider type taxonomies similarto the one used for PEARL, consisting of severalhundreds of fine-grained types.
All methods usetrained classifiers over a variety of linguistic fea-tures, most importantly, words and bigrams withpart-of-speech tags in a mention and in the textualcontext preceding and following the mention.
Inaddition, the method of (Yosef 2012) (HYENA)utilizes a big gazetteer of per-type words that oc-cur in Wikipedia anchor texts.
This method out-performs earlier techniques on a variety of test1494Entity Inferred Type Sample Source Sentence (s)Lochte medalist Lochte won America?s lone gold ...Malick director ... the red carpet in Cannes for Malick?s 2011 movie ...Bonamassa musician Bonamassa recorded Driving Towards the Daylight in Las Vegas ...... Bonamassa opened for B.B.
King in Rochester , N.Y.Analog Man album Analog Man is Joe Walsh?s first solo album in 20 years.Melinda Liu journalist ... in a telephone interview with journalist Melinda Liu of the Daily Beast.RealtyTrac publication Earlier this month, RealtyTrac reported that ...Table 4: Sample types inferred by PEARL.cases; hence it served as one of our baselines.Closely related to our work is the recent ap-proach of (Lin 2012) (NNPLB) for predictingtypes for out-of-KB entities.
Noun phrases in thesubject role in a large collection of fact triplesare heuristically linked to Freebase entities.
Thisyields type information for the linked mentions.For unlinkable entities the NNPLB method (in-spired by (Kozareva 2011)) picks types based onco-occurrence with salient relational patterns bypropagating types of linked entities to unlinkableentities that occur with the same patterns.
UnlikePEARL, NNPLB does not attempt to resolve in-consistencies among the predicted types.
In con-trast, PEARL uses an ILP with type disjointnessand correlation constraints to solve and penalizesuch inconsistencies.
NNPLB uses untyped pat-terns, whereas PEARL harnesses patterns withtype signatures.
Furthermore, PEARL computesweights for candidate types based on patterns andtype signatures.
Weight computations in NNPLBare only based on patterns.
NNPLB only assignstypes to entities that appear in the subject role ofa pattern.
This means that entities in the objectrole are not typed at all.
In contrast, PEARL in-fers types for entities in both the subject and objectrole.Type disjointness constraints have been studiedfor other tasks in information extraction (Carlson2010; Suchanek 2009), but using different formu-lations.7 ConclusionThis paper addressed the problem of detecting andsemantically typing newly emerging entities, tosupport the life-cycle of large knowledge bases.Our solution, PEARL, draws on a collection ofsemantically typed patterns for binary relations.PEARL feeds probabilistic evidence derived fromoccurrences of such patterns into two kinds ofILPs, considering type disjointness or type corre-lations.
This leads to highly accurate type predic-tions, significantly better than previous methods,as our crowdsourcing-based evaluation showed.ReferencesS.
Auer, C. Bizer, G. Kobilarov, J. Lehmann, R. Cyga-niak, Z.G.
Ives: DBpedia: A Nucleus for a Web ofOpen Data.
In Proceedings of the 6th InternationalSemantic Web Conference (ISWC), pages 722?735,Busan, Korea, 2007.M.
Banko, M. J. Cafarella, S. Soderland, M. Broad-head, O. Etzioni: Open Information Extraction fromthe Web.
In Proceedings of the 20th InternationalJoint Conference on Artificial Intelligence (IJCAI),pages 2670?2676, Hyderabad, India, 2007.K.
D. Bollacker, C. Evans, P. Paritosh, T. Sturge, J.Taylor: Freebase: a Collaboratively Created GraphDatabase for Structuring Human Knowledge.
InProceedings of the ACM SIGMOD InternationalConference on Management of Data (SIGMOD),pages, 1247-1250, Vancouver, BC, Canada, 2008.Lawrence D. Brown, T.Tony Cai, Anirban Dasgupta:Interval Estimation for a Binomial Proportion.
Sta-tistical Science 16: pages 101?133, 2001.R.
C. Bunescu, M. Pasca: Using Encyclopedic Knowl-edge for Named entity Disambiguation.
In Proceed-ings of the 11th Conference of the European Chap-ter of the Association for Computational Linguistics(EACL), Trento, Italy, 2006.A.
Carlson, J. Betteridge, R.C.
Wang, E.R.
Hruschka,T.M.
Mitchell: Coupled Semi-supervised Learningfor Information Extraction.
In Proceedings of theThird International Conference on Web Search andWeb Data Mining (WSDM), pages 101?110, NewYork, NY, USA, 2010.S.
Cucerzan: Large-Scale Named Entity Disambigua-tion Based on Wikipedia Data.
In Proceedings ofthe 2007 Joint Conference on Empirical Meth-ods in Natural Language Processing and Com-putational Natural Language Learning (EMNLP-1495CoNLL), pages 708?716, Prague, Czech Republic,2007.A.
Fader, S. Soderland, O. Etzioni: Identifying Rela-tions for Open Information Extraction.
In Proceed-ings of the 2011 Conference on Empirical Methodsin Natural Language Processing (EMNLP), pages1535?1545, Edinburgh, UK, 2011.J.R.
Finkel, T. Grenager, C. Manning.
2005.
Incorpo-rating Non-local Information into Information Ex-traction Systems by Gibbs Sampling.
In Proceedingsof the 43rd Annual Meeting of the Association forComputational Linguistics (ACL), pages 363?370,Ann Arbor, Michigan, 2005.Michael Fleischman, Eduard H. Hovy: Fine GrainedClassification of Named Entities.
In Proceedingsthe International Conference on Computational Lin-guistics, COLING 2002.X.
Han, J. Zhao: Named Entity Disambiguation byLeveraging Wikipedia Semantic Knowledge.
In Pro-ceedings of 18th ACM Conference on Informationand Knowledge Management (CIKM), pages 215 ?224,Hong Kong, China, 2009.C.
Havasi, R. Speer, J. Alonso.
ConceptNet 3: a Flex-ible, Multilingual Semantic Network for CommonSense Knowledge.
In Proceedings of the Recent Ad-vances in Natural Language Processing (RANLP),Borovets, Bulgaria, 2007.Sebastian Hellmann, Claus Stadler, Jens Lehmann,Sren Auer: DBpedia Live Extraction.
OTM Confer-ences (2) 2009: 1209-1223.J.
Hoffart, M. A. Yosef, I.Bordino and H. Fuerstenau,M.
Pinkal, M. Spaniol, B.Taneva, S.Thater, GerhardWeikum: Robust Disambiguation of Named Entitiesin Text.
In Proceedings of the 2011 Conference onEmpirical Methods in Natural Language Processing(EMNLP), pages 782?792, Edinburgh, UK, 2011.J.
Hoffart, F. Suchanek, K. Berberich, E. Lewis-Kelham, G. de Melo, G. Weikum: YAGO2: Ex-ploring and Querying World Knowledge in Time,Space, Context, and Many Languages.
In Proceed-ings of the 20th International Conference on WorldWide Web (WWW), pages 229?232, Hyderabad, In-dia.
2011.J.
Hoffart, F. Suchanek, K. Berberich, G. Weikum:YAGO2: A Spatially and Temporally EnhancedKnowledge Base from Wikipedia.
Artificial Intelli-gence 2012.Z.
Kozareva, L. Voevodski, S.-H.Teng: Class LabelEnhancement via Related Instances.
EMNLP 2011:118-128J.
R. Landis, G. G. Koch: The measurement of observeragreement for categorical data in Biometrics.
Vol.33, pp.
159174, 1977.C.
Lee, Y-G. Hwang, M.-G. Jang: Fine-grainedNamed Entity Recognition and Relation Extractionfor Question Answering.
In Proceedings of the 30thAnnual International ACM SIGIR Conference onResearch and Development in Information Retrieval(SIGIR), pages 799?800, Amsterdam, The Nether-lands, 2007.T.
Lin, Mausam , O. Etzioni: No Noun Phrase LeftBehind: Detecting and Typing Unlinkable Entities.In Proceedings of the 2012 Joint Conference onEmpirical Methods in Natural Language Process-ing and Computational Natural Language Learning(EMNLP-CoNLL), pages 893?903, Jeju, South Ko-rea, 2012.Xiao Ling, Daniel S. Weld: Fine-Grained EntityRecognition.
In Proceedings of the Conference onArtificial Intelligence (AAAI), 2012D.
N. Milne, I. H. Witten: Learning to Link with Wi-kipedia.
In Proceedings of 17th ACM Conference onInformation and Knowledge Management (CIKM),pages 509-518, Napa Valley, California, USA, 2008.N.
Nakashole, G. Weikum, F. Suchanek: PATTY:A Taxonomy of Relational Patterns with Seman-tic Types.
In Proceedings of the 2012 Joint Con-ference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL), pages 1135 -1145, Jeju, South Korea, 2012.V.
Nastase, M. Strube, B. Boerschinger, Ca?cilia Zirn,Anas Elghafari: WikiNet: A Very Large ScaleMulti-Lingual Concept Network.
In Proceedings ofthe 7th International Conference on Language Re-sources and Evaluation(LREC), Malta, 2010.H.
T. Nguyen, T. H. Cao: Named Entity Disambigua-tion on an Ontology Enriched by Wikipedia.
In Pro-ceedings of the IEEE International Conference onResearch, Innovation and Vision for the Future inComputing & Communication Technologies (RIVF),pages 247?254, Ho Chi Minh City, Vietnam, 2008.Feng Niu, Ce Zhang, Christopher Re, Jude W. Shav-lik: DeepDive: Web-scale Knowledge-base Con-struction using Statistical Learning and Inference.
Inthe VLDS Workshop, pages 25-28, 2012.A.
Rahman, Vincent Ng: Inducing Fine-Grained Se-mantic Classes via Hierarchical and Collective Clas-sification.
In Proceedings the International Con-ference on Computational Linguistics (COLING),pages 931-939, 2010.F.
M. Suchanek, G. Kasneci, G. Weikum: Yago: aCore of Semantic Knowledge.
In Proceedings of the16th International Conference on World Wide Web(WWW) pages, 697-706, Banff, Alberta, Canada,2007.1496F.
M. Suchanek, M. Sozio, G. Weikum: SOFIE: ASelf-organizing Framework for Information Extrac-tion.
InProceedings of the 18th International Con-ference on World Wide Web (WWW), pages 631?640,Madrid, Spain, 2009.P.P.
Talukdar, F. Pereira: Experiments in Graph-BasedSemi-Supervised Learning Methods for Class-Instance Acquisition.
In Proceedings of the AnnualMeeting of the Association for Computational Lin-guistics (ACL), pages 1473-1481, 2010.P.
Venetis, A. Halevy, J. Madhavan, M. Pasca, W. Shen,F.
Wu, G. Miao, C. Wu: Recovering Semantics ofTables on the Web.
In Proceedings of the VLDB En-dowment, PVLDB 4(9), pages, 528?538.
2011.W.
Wu, H. Li, H. Wang, K. Zhu: Probase: AProbabilistic Taxonomy for Text Understanding.
InProceedings of the International Conference onManagement of Data (SIGMOD), pages 481?492,Scottsdale, AZ, USA, 2012.M.
A. Yosef, S. Bauer, J. Hoffart, M. Spaniol, G.Weikum: HYENA: Hierarchical Type Classifica-tion for Entity Names.
In Proceedings the In-ternational Conference on Computational Linguis-tics(COLING), to appear, 2012.1497
