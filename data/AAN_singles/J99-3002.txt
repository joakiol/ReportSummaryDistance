The Computational Complexity of theCorrect-Prefix Property for TAGsMark-Jan Nederhof*German Research Center for ArtificialIntelligenceA new upper bound is presented for the computational complexity of the parsing problem forTAGs, under the constraint hat input is read from left to right in such a way that errors in theinput are observed as soon as possible, which is called the "correct-prefix property."
The formerupper bound, O(n9), is now improved to O(n6), which is the same as that of practical parsingalgorithms for TAGs without he additional constraint of the correct-prefix property.1.
IntroductionTraditionally, parsers and recognizers for regular and context-free languages processinput from left to right.
If a syntax error occurs in the input they often detect haterror immediately after its position is reached.
The position of the syntax error canbe defined as the rightmost symbol of the shortest prefix of the input that cannot beextended to be a correct sentence in the language L.In formal notation, this prefix for a given erroneous input w ~ L is defined as thestring va, where w = vax, for some x, such that vy E L, for some y, but vaz ~ L, forany z.
(The symbols v, w .
.
.
.
denote strings, and a denotes an input symbol.)
Theoccurrence of a in w indicates the error position.If the error is detected as soon as it is reached, then all prefixes of the input thathave been processed at preceding stages are correct prefixes, or more precisely, they areprefixes of some correct strings in the language.
Hence, we speak of the correct-prefixproperty.
1An important application can be found in the area of grammar checking: uponfinding an ungrammatical sentence in a document, a grammar checker may report tothe user the presumed position of the error, obtained from a parsing algorithm withthe correct-prefix property.For context-free and regular languages, the correct-prefix property can be satis-fied without additional costs of space or time.
Surprisingly, it has been claimed bySchabes and Waters (1995) that this property is problematic for the mildly context-sensitive languages represented by tree-adjoining grammars (TAGs): the best practicalparsing algorithms for TAGs have time complexity Cg(n 6) (Vijay-Shankar and Joshi\[1985\]; see Satta \[1994\] and Rajasekaran and Yooseph \[1995\] for lower theoretical upperbounds), whereas the only published algorithm with the correct-prefix property--thatby Schabes and Joshi (1988)--has complexity O(n9).In this paper we present an algorithm that satisfies the correct-prefix property andoperates in Cq(n 6) time.
This algorithm merely recognizes input, but it can be extended* DFKI, Stuhlsatzenhausweg 3, D-66123 Saarbriicken, Germany.
E-mail: nederhof@dfki.de1 We adopt this term from Sippu and Soisalon-Soininen (1988).
In some publications, the term validprefix property is used.
(~) 1999 Association for Computational LinguisticsComputational Linguistics Volume 25, Number 3to be a parsing algorithm with the ideas from Schabes (1994), which also suggest howit can be extended to handle substitution in addition to adjunction.
The complexityresults carry over to linear indexed grammars, combinatory categorial grammars, andhead grammars, since these formalisms are equivalent o TAGs (Vijay-Shanker andWeir 1993, 1994).We present he actual algorithm in Section 3, after the necessary notation has beendiscussed in Section 2.
The correctness proofs are discussed in Section 4, and the timecomplexity in Section 5.
The ideas in this paper give rise to a number of questions forfurther research, as discussed in Section 6.2.
DefinitionsOur definition of TAGs simplifies the explanation of the algorithm, but differs slightlyfrom standard treatment such as that of Joshi (1987).A tree-adjoining grammar is a 4-tuple (~, NT, L A), where ~ is the set of terminals,I is the set of initial trees, and A is the set of auxil iary trees.
We refer to the trees inI U A as elementary trees.
The set NT, the set of nonterminals, does not play any rolein this paper.We refer to the root of an elementary tree t as Rt.
Each auxiliary tree has exactlyone distinguished leaf, which is called the foot.
We refer to the foot of an auxiliarytree t as Ft.We use variables N and M to range over nodes in elementary trees.
We assumethat the sets of nodes belonging to distinct elementary trees are pairwise disjoint.For each leaf N in an elementary tree, except when it is a foot, we define label(N)to be the label of the node, which is either a terminal from ~ or the empty string e.For all other nodes, label is undefined.For each node N that is not a leaf or that is a foot, Adj(N) is the set of auxiliary treesthat can be adjoined at N, plus possibly the special element nil.
For all other nodes,Adj is undefined.
If a set Adj(N) contains nil, then this indicates that adjunction at Nis not obligatory.For each nonleaf node N we define children(N) as the (nonempty) list of daughternodes.
For all other nodes, children is undefined.
An example of a TAG is given inFigure 1.The language described by a TAG is given by the set of strings that are the yieldsof derived trees.
A derived tree is obtained from an initial tree by performing thefollowing operation on each node N, except when it is a leaf: The tree is excised at N,and between the two halves a fresh instance of an auxiliary tree, which is taken fromthe set Adj(N), is inserted, or the element nil is taken from Adj(N), in which case nonew nodes are added to the tree.
Insertion of the new auxiliary tree, which from nowon will be called adjunction, is done in such a way that the bottom half of the excisedtree is connected to the foot of the auxiliary tree.
The new nodes that are added to thetree as a result are recursively subjected to the same operation.
This process ends in acomplete derived tree once all nodes have been treated.An example of the derivation of a string is given in Figure 2.
We start with initialtree al and treat Ral, for which we find Adj(Ral) = {b2, nil}.
We opt to select nil,so that no new nodes are added.
However in the figure we do split Ral in order tomark it as having been treated.
Next we treat Nail, and we opt to adjoin bl, takenfrom Adj(N~I ) = {bl, b3}.
After another "nil-adjunction" at Rbl, we adjoin b2 at N~I.Note that this is an obligatory adjunction, since Adj(N~I ) does not contain nil.
Somemore nil-adjunctions lead to a derived tree with yield acdb, which is therefore in thelanguage described by the TAG.346Nederhof Correct-Prefix Property for TAGsInitial trees(al) (a2)@a/~ Ra l C~~@b e eAdj(Ral) = Adj(R,~2) ={62, nil} {b3, nil}Adj(N:,)  = Adj(g:~) ={bl,b3} {bl,b2}Adj ( Y2~) ={nil}Figure 1A tree-adjoining grammar.Auxiliary treesi(bl) (b2)Fbl OFb2 ~CAdj(nbl) = Adj(Rb2) ={bl, b2, nil} {b3, nil}Ady(NdI) = Adj(Fb~) ={b~} {nil}Adj(Fb~) ={nil}Initial t re~/~ Ralbnil-adjunction'~ adjoineaq  bl at N~Ibo .~ adjoinb2 at NilslC?-~Derived o /~tree:three nil-adjunctions4?.%eFigure 2Derivation of the string acdb.
(b3)l Rb3"a" oFb3  N:3bAdj(Rb3) = {bl, nil}Adj(N~) = (b2}A~j(N~) = {b3, n i l}Adj(Fb3) = (nil}ac bJ nil-adjunctiona ~ F b lc bIn order to avoid cluttering the picture with details, we have omitted the namesof nodes at which (nil-)adjunction has been applied.
We will reintroduce these nameslater.
A further point worth mentioning is that here we treat the nodes in preorder: wetraverse the tree top-down and left-to-right, and perform adjunction at each node thefirst time it is encountered.
2 Any other strategy would lead to the same set of derivedtrees, but we chose preorder treatment since this matches the algorithm we presentbelow.2 The tree that is being traversed grows in size dur ing the traversal, contrary to traditional usage of thenotion of "traversal.
"347Computational Linguistics Volume 25, Number 33.
The AlgorithmThe input to the recognition algorithm is given by the string ala2.
.
?
an, where n is thelength of the input.
Integers i such that 0 < i < n will be used to indicate "positions"in the input string.
Where we refer to the input between positions i and j we meanthe string ai+l .
.
.
aj.The algorithm operates by means of least fixed-point iteration: a table is graduallyfilled with elements derived from other elements, until no more new ones can be found.A number of "steps" indicate how table elements are to be derived from others.
3For the description of the steps we use a pseudoformal notation.
Each step consistsof a list of antecedents and a consequent.
The antecedents are the conditions underwhich an incarnation of the step is executed.
The consequent is a new table elementthat the step then adds to the parse table, unless of course it is already present.
Anantecedent may be a table element, in which case the condition that it represents imembership in the table.The main table elements, or items, are 6-tuples \[h, N --* c~ ?
t ,  i, j, f l , f2\].
Here, Nis a node from some elementary tree t, and o~fl is the list of the daughter nodes of N.The daughters in o~ together generate the input between positions i and j.
The wholeelementary tree generates input from position h onwards.Internal to the elementary tree, there may be adjunctions; in fact, the traversal ofthe tree (implying (nil-)adjunctions at all nodes) has been completed up to the endof c~.
Furthermore, tree t may itself be an auxiliary tree, in which case it is adjoinedin another tree.
Then, the foot may be dominated by one of the daughters in a, andthe foot generates the part of the input between positions fl  and f2.
When the tree isnot an auxiliary tree, or when the foot is not dominated by one of the daughters in c~,then fl  and f2 both have the dummy value " - "Whether t is an initial or an auxiliary tree, it is part of a derived tree of whicheverything to the left of the end of c~ generates the input between positions 0 and j.The traversal has been completed up to the end of c~.See Figure 3 for an illustration of the meaning of items.
We assume Rt and Ftare the root and foot of the elementary tree t to which N belongs; Ft may not exist,as explained above.
R is the root of some initial tree.
The solid lines indicate whathas been established; the dashed lines indicate what is merely predicted.
If Ft ex-ists, the subtree below Ft indicates the lower half of the derived tree in which t wasadjoined.The shaded areas labeled by I, II, and III have not yet been traversed.
In particularit has not yet been established that these parts of the derived tree together generatethe input between positions j and n.For technical reasons, we assume an additional node for each elementary tree t,which we denote by T. This node has only one daughter, viz.
the actual root node Rt.We also assume an additional node for each auxiliary tree t, which we denote by 3_.This is the unique daughter of the actual foot node Ft; we set children(Ft) = _1_.In summary, an item indicates how a part of an elementary tree contributes to therecognition of some derived tree.Figure 4 illustrates the items needed for recognition of the derived tree from therunning example.
We have simplified the notation of items by replacing the names ofleaves (other than foot nodes) by their labels.3 A "step" is more accurately called an "inference rule" in the literature on deductive parsing (Shieber,Schabes, and Pereira 1995).
For the sake of convenience we will apply the shorter term.348Nederhof Correct-Prefix Property for TAGs0 h i f  1 f2 jFigure 3An item \[h, N -* a .
fl, i, \], fl, f2\].01: \[0, T -+ ,Ra l ,  0,0 , - , - \ ]  =T 2: \[0, R~I -+ ?
a N~I, 0, 0 , - , - \ ]  =I , 23 3: \[0, Ral -+ a ?
N~I, 0, 1, - ,  - \ ]  =~Rnl 4: \[1, T --+ ?
Rbl, 1, 1, - ,  -\] =~4T 212~ 1 5 : \ [ I 'Rb I -+ ?Nd lFb l '  1 '1 ' - ' - \ ]=  6: \[1, T -+ ?Rb2, 1,1,--,--\] = 21 7: \[1, Rb2 -+ ?
Fb2 d, 1, 1, - ,  - \]  =~Rbl 8: \[1, Fb2 -+ ?
3-, 1, 1 , - , - \ ]  =g~/ Rbl 9: \[1, Nil --+ ?
c, 1, 1 , - , -1  ----a 1 ;~20 10: \[1,NIl -+ c ?, 1 ,2 , - , - \ ]  =Nbl:4 / ~Fbl 11: \[1, Ub2 -+ 3_ ?, 1,2,1,21 =6,T14 yFbl 12: \[1, Rb2 --~ Fb2 ?
d, 1, 2, 1, 2\] =~Rb2 16119 13: \[1, Rb2 --+ Fb2 d ?, 1, 3, 1, 2\] = Rb27~,12~3 7~lN// 14: \[1, T __+ Rb2 ?, 1,3, 1,21 = u52 I 8 15a: \[N~I -+ c ?, 1, 3 , - ,  -1 =15: \[1, Rbl ~ N~I " Fbl, 1, 3, --, -1 =yFb2 d b 16: \[1, Fbl -+ ?
3_, 3, 3, --, --1 =8-!-111 17: \[0, Ni l  --+ ?
b, 3, 3, , \] =1 Nbl 18: \[0, Nail -+ b ?, 3, 4, - ,  - \]  =19: \[1,Ybl ~ ?
?, 3,4,3,4\] =20: \[1, Rbl --+ Ni l  Fbl ?, 1, 4, 3, 4\] =C 21: \[1, T --+ Rbl ?, 1, 4, 3, 4\] =a c d b 22a: \[Nil -+ b ?, 1, 4 , - ,  - \]  =22: \[0, R~I --+ a N~I ?, 0 ,4 , - , - \ ]  =1 2 3 4 23: \[0, 7- -+ Ral ", 0, 4 , - , - \ ]  =( In i t )(Pred 2)1(Scan  1)2(Pred 1)3(Pred 2)4(Pred 1)5(Pred 2)6(Pred 2)7(Pred 3)8 + 5(Scan 1)9(Comp 1)10 + 8 + 5(Comp 2111 + 7(Scan 1)12(Comp 2)13 + 6(nd j  0)14 + 10(Adj 2)15a + 5(P red  2)15(P red  3)16 + 3(Scan 1)17(Comp 1)18 + 16 + 3(Comp 2)19 + 15(Comp 2)20 + 4(Adj 0)21 + 18(Adj 2)22a + 3(Comp 3)22 + 1Figure 4The items needed for recognition of a derived tree.There is one special kind of item, with only five fields instead of six.
This isused as an intermediate result in the adjunctor steps to be discussed in Section3.5.349Computational Linguistics Volume 25, Number 3RL .
.
.
.
.
.
.
.
.
.
.
.
.
.
x0 nInitFigure 5The initialization.h i fl f2 J j+lScan 1Figure 6The first scanner step.3.1 InitializerThe initializer step predicts initial trees t starting at position 0; see Figure 5.t?
I\[0, 7----~ .Rt,  O, O, , \](Init)For the running example, item 1 in Figure 4 results from this step.3.2 ScannerThe scanner steps try to shift the dot rightward in case the next node in line is labeledwith a terminal or ?, which means the node is a leaf but not a foot.
Figure 6 sketchesthe situation with respect o the input positions mentioned in the step.
The depictedstructure is part of at least one derived tree consistent with the input between positions0 and j + 1, as explained earlier.\[h, N Mg, i, j, A, ;Ca\],label(M) = aj+l\[h, N --* ccM .
fl, i, j + l ,  A ,  f2 \](Scan 1)\[h, N- -*c~,Mf l ,  i, j, f l ,  f2\],label(M) = ?\[h, N --* aM.
fl, i, j, A, f2\](Scan 2)For the running example in Figure 4, Scan 1 derives, among others, item 3 fromitem 2, and item 13 from item 12.350Nederhof Correct-Prefix Property for TAGs",,, ",,,,h i h i jPred 1 Pred 2Figure 7The three predictor steps.i!~, "'"',," / ///,~,,:,, ' ,  ,,',,/ // / / JM ' i  ~,', ,,, ,/ / / / / I  ~ ,i <, ,,', ,/ // //~ '__~_, i~ ~,;, ,, ,,/ f f t~___:~L~_~__',h i jPred 33.3 PredictorThe first predictor step predicts a fresh occurrence of an auxiliary tree t, indicated inFigure 7.
The second predicts a list of daughters "7 lower down in the tree, abstainingfrom adjunction at the current node M. The third predicts the lower half of a tree inwhich the present ree t was adjoined.\[h, N --~ a .
Mfl, i, j, f l ,  f2\],t E Adj(M) (Pred 1)~', T~ .R , ,  j, j, - ,  -\]\[h, N- -+a.Mf l ,  i, j, A, f2\],nil E Adj(M),children(M) = "7(Pred 2)\[h,M--+ *"7, j, j, - ,  -\]~, ~,~ .
?,  k, k, - ,  - \ ] ,\[h, N ---+ a .
Mfl, i, j, f l ,  f2\],t E Adj(M),children(M) = "7(Pred 3)\[h, M--+ .
"7, k, k, - ,  -\]For the running example, Pred 1 derives item 4 from item 3 and item 6 fromitem 5.
Pred 2 derives, among others, item 5 from item 4.
Pred 3 derives item 9 fromitems 8 and 5, and item 17 from items 16 and 3.3.4 CompleterThe first completer step completes recognition of the lower half of a tree in which anauxiliary tree t was adjoined, and asserts recognition of the foot of t; see Figure 8.
Thesecond and third completer steps complete recognition of a list of daughter nodes '7,and initiate recognition of the list of nodes fl to the right of the mother node of %\[h, M--* '7.
,  k, l , f~,fd\] ,t E Adj(M),~, Ft--~ ,_L, k, k, - ,  -\],\[h, N-+a.Mf l ,  i, j, f~, f2\]~, F t ---+ l .
,  k, l, k, I\] (Comp 1)351Computational Linguistics Volume 25, Number 3Lh i j k lComp 1Figure 8h i jTwo of the completer steps.f2 kComp 2\[h, M--*3"?, j, k, fl, f2\],\[h, N --~ c~oMfl, i, j, - ,  -\],M dominates foot of tree\[h, N-- ,c~M?f l ,  i, k, fl, f2\](Comp 2)\[h, M--~',/ .
,  j, k, - ,  -\],\[h, N---~c~?Mfl, i, j, f l, f2\]\[h, N -* c~M ?
fl, i, k, A, f2\](Comp 3)See Figure 4 for use of these three steps in the running example.3.5 AdjunctorThe adjunctor steps perform the actual recognition of an adjunction of an auxiliarytree t in another tree at some node M. The first adjunctor step deals with the case inwhich the other tree is again adjoined in a third tree (the two darkly shaded areas inFigure 9) and M dominates the foot node.
The second adjunctor step deals with thecase in which the other tree is either an initial tree, or has its foot elsewhere, i.e., notdominated by M.The two respective cases of adjunction are realized by step Adj 0 plus step Adj 1,and by step Adj 0 plus step Adj 2.
The auxiliary step Adj 0 introduces items of asomewhat different form than those considered up to now, viz.
\[M ~ 3' o, j, k, f~, f~\].The interpretation is suggested in Figure 10: at M a tree has been adjoined.
The ad-joined tree and the lower half of the tree that M occurs in together generate the inputfrom j to k. The depicted structure is part of at least one derived tree consistent withthe input between positions 0 and k. In the case in which M dominates a foot node,as suggested in the figure, f~ and fd have a value other than " - "~, T---~ Rt.
,  j, k, fl, f2\],\[h, M--*3",, f~, f2, f~, fd\],t E Adj(M)\[M --~ 3' ?, j, k, f~, f2\] (Adj 0)352Nederhof Correct-Prefix Property for TAGsh i j f~f(  f ; f2Adj 1Figure 9~ M 3' ,h i fl 4 kAdj 2The two adjunctor steps, implicitly combined with Adj 0.\[M--* 7?, j, k,f~,fd\],M dominates foot of tree t',\[h, F t, --+ ?
o, f~, f~, f~, f~\],\[h, N--+ c~oMfl, i, j, - ,  -\]\[h, N- - .c~M?f l ,  i, k, f~, f~\](Adj 1)\[M ~-yo ,  j, k, - ,  - \] ,\[h, N- -*c~.Mf l ,  i, j, f~, f~\]\[h, N --~ aM.
fl, i, k, f~, f~\] (Adj 2)For the running example, Adj 0 derives the intermediate item 15a from items 14and 10 and from this and item 5, Adj 2 derives item 15.
Similarly, Adj 0 and Adj 2together derive item 22.
There are no applications of Adj 1 in this example.An alternative formulation of the adjunctor steps, without Adj 0, could be thefollowing:~, T--* Rt .
,  j, k, f l ,  d2\],\[h, M~, ,  fl, fa, f~, fd\],t C Adj(M),M dominates foot of tree t',\[h, Ft, ~ ?.
,  f~, fd, f~, f~l,\[h, N--~ c~ ?Mfl,  i, j, - ,  -1\[h, N --* aM ?
fl, i, k, f~, fd\] (Adj 1')~, T"+ at?, j, k, f~, fal,\[h, M--~7, ,  fl, f2, - ,  -1,t 6 Adj(M),\[h, N---~a,Mfl ,  i, j, f~, fd\]\[h, N ~ aM, f l ,  i, k, f~, f~\](Adj 2')353Computational Linguistics Volume 25, Number 3MJ f l '~ '  kFigure 10An item \[M --~ 3' ", j, k, fi, fd\].That this formulation is equivalent to the original combination of the three stepsAdj 0, Adj 1, and Adj 2 can be argued in two stages.First, the h in \[h, M --~ "7 ,, A ,  f2, f~, f~\] or \[h, M --+ 31 ,, A ,  f2, - ,  -\] occurring assecond antecedent of Adj I t or Adj 2 ~, respectively, can be replaced by a fresh variableh ~ without affecting the correctness of the algorithm.
In particular, the occurrence of hin the second antecedent of Adj 1 ~ is redundant because of the inclusion of the fifthantecedent \[h, Ft, --+ J - , ,  f~, f~, f~, f~\].
Note that, conversely, this fifth antecedent isredundant with respect o the second antecedent, since existence of an item \[h, M --+"7 ", f l ,  f2, f~, fd\], such that M dominates the foot of a tree t', implies the existence ofan item \[h, Ft, ~ _L ,, f~, f~, f~, f~\].
For further explanation, see Section 4.Second, the first three antecedents of Adj 1 ~ and Adj 2 ~ can be split off to obtainAdj 0, Adj 1, and Adj 2, justified by principles that are the basis for optimization ofdatabase queries (Ullman 1982).The rationale for the original formulation of the adjunction steps as opposed tothe alternative formulation by Adj 1 ~ and Adj 2 ~ lies in the consideration of timecomplexity, as will be discussed in Section 5.4.
PropertiesThe first claim we make about he algorithm pertains to its correctness a  a recognizer:ClaimAfter completion of the algorithm, the item \[0, T --* a t e, 0, n, - ,  -\], for some t E Lis in the table if and only if the input is in the language described by the grammar.Note that the input is in the language if and only if the input is the yield of aderived tree.The idea behind the proof of the "if" part is that for any derived tree constructedfrom the grammar we can indicate a top-down and left-to-right tree traversal that ismatched by corresponding items that are computed by steps of the algorithm.
Thetree traversal and the corresponding items are exemplified by the numbers 1. .
.
.
,23in Figure 4.For the "only if" part, we can show for each step separately that the invariantsuggested in Figure 3 is preserved.
To simplify the proof one can look only at the lastfive fields of items \[h, N --+ c~, fl, i, j, fl, f2\], h being irrelevant for the above claim.We do, however, need h for the proof of the following claim:354Nederhof Correct-Prefix Property for TAGs0 h i  d(a)Figure 11Pred 1 preserves the invariant.0 h id(b)ClaimThe algorithm satisfies the correct-prefix property, provided the grammar is reduced.A TAG is reduced if it does not contain any elementary trees that cannot be partof any derived tree.
(One reason why an auxiliary tree might not be a part of anyderived tree is that at some node it may have obligatory adjunction of itself, leadingto "infinite adjunction.
")Again, the proof relies on the invariant sketched in Figure 3.
The invariant can beproven correct by verifying that if the items in the antecedents of some step satisfythe invariant, then so does the item in the consequent.A slight technical problem is caused by the obligatory adjunctions.
The shadedareas in Figure 3, for example, represent not merely subtrees of elementary trees, butsubtrees of a derived tree, which means that at each node either adjunction or nil-adjunction has been performed.This issue arises when we prove that Pred 1 preserves the invariant.
Figure 11(a)represents he interpretation f the first antecedent of this step, \[h, N --+ e~ ?
Mf l ,  i, j, f l ,f2\]; without loss of generality we only consider the case that fl = f2 = - .
We mayassume that below M some subtree xists, and that at M itself either adjunction withauxiliary tree t ~ or nil-adjunction has been applied; the figure shows the former case.In order to justify the item from the consequent, ~, T --* ?
Rt, j, j, - ,  -\], weconstruct he tree in Figure 11(b), which is the same as that in Figure 11(a), exceptthat t ~ is replaced by auxiliary tree t, which has been traversed so that at all nodeseither adjunction or nil-adjunction has been applied, including the nodes introducedrecursively through adjunctions.
Such a finite traversal must exist since the grammaris reduced.For the other steps we do not need the assumption that the grammar is reducedin order to prove that the invariant is preserved.
For example, for Adj 1 we mayreason as follows: The item \[M --+ ~, ?, j, k, f~, f~\], the first antecedent, informs us ofthe existence of the structure in the shaded area of Figure 12(a).
Similarly, the items\[h, Ft, --~ / ?
,  jc~, jcd, f~, fd\] and \[h, N ~ c~ ?
Mf l ,  i, j, - ,  -\] provide the shaded areasof Figures 12(b) and 12(c).
Note that in the case of the first or third item, we do notuse all the information that the item provides.
In particular, the information that thestructures are part of a derived tree consistent with the input between positions 0 andk (in the case of (a)) or j (in the case of (c)) is not needed.355Computational Linguistics Volume 25, Number 3MJ fl' f2' kO"" """ / ",i ",h fl' .f2' h i j(a) (b) (c)Rt~"""' '!\0 h i j fl fl" f2' f2 kFigure 12Adj 1 preserves the invariant.
(d)The combined information from these three items ensures the existence of thederived tree depicted in Figure 12(d), which justifies the consequent of Adj 1, viz.\[h, N --* aM.
fl, i, k, f~, fd\].The other steps can be proven to preserve the invariant in similar ways.Now the second claim follows: if the input up to position j has been read resultingin an item of the form \[h, N --* aa * fl, i, j, fl, f2\], then there is a string y such thata l .
.
.
ajy is in the language.
This y is the concatenation of the yields of the subtreeslabeled I, II, and III in Figure 3.The full proofs of the two claims above are straightforward but tedious.
Further-more, our new algorithm is related to many existing recognition algorithms for TAGs(Vijay-Shankar and Joshi 1985; Schabes and Joshi 1988; Lang 1988; Vijay-Shanker andWeir 1993; Schabes and Shieber 1994; Schabes 1994), some of which were published356Nederhof Correct-Prefix Property for TAGstogether with proofs of correctness.
Therefore, including full proofs for our new algo-rithm does not seem necessary.5.
ComplexityThe steps presented in pseudoformal notation in Section 3 can easily be composedinto an actual algorithm (Shieber, Schabes, and Pereira 1995).
This can be done in sucha way that the order of the time complexity is determined by the maximal number ofdifferent combinations of antecedents per step.
If we restrict ourselves to the order ofthe time complexity expressed in the length of the input, this means that the complexityis given by O(nP), where p is the largest number of input positions in any step.However, a better realization of the algorithm exists that allows us to excludethe variables for input positions that occur only once in a step, which we will callirrelevant input positions.
This realization relies on the fact that an intermediate stepImay be applied that reduces an item I with q input positions to another item I' withq' < q input positions, omitting those that are irrelevant.
That reduced item I' thentakes the place of I in the antecedent of the actual step.
This has a strong relationshipto optimization of database queries (Ullman 1982).For example, there are nine variables in Comp 1, of which i,fl,f2,f~,f~ are allirrelevant, since they occur only once in that step.
An alternative formulation of thisstep is therefore given by the combination of the following three steps:\[h, M~' ) , .
,  k, l, f{, f~\]\[h, M--*'y., k, l, ?, ?\](Omit 5-6)\[h, N ---* c~.
Mfl, i, j, A, f2\]\[h, N--* o~ .Mf l ,  ?, j, ?, ?\](Omit 3-5-6)\[h, M--~'y ., k, 1, ?, ?\],t E Adj(M),~,Ft---+ -?,  k, k , - ,  -\],\[h, N--* o~.
Mfl, ?, j, ?, ?1~, F t - -~ l .
,  k, l, k, I\](Comp 1')The question marks indicate omitted input positions.
Items containing questionmarks are distinguished from items without hem, and from items with question marksin different fields.In Comp 1' there are now only four input positions left.
The contribution of thisstep to the overall time complexity is therefore O(n 4) rather than C9(n9).
The contribu-tion of Omit 5-6 and Omit 3-5-6 to the time complexity is O(n5).For the entire algorithm, the maximum number of relevant input positions perstep is six.
Thereby, the complexity of left-to-right recognition for TAGs under theconstraint of the correct-prefix property is CO(n6).
There are five steps that contain sixrelevant input positions, viz.
Comp 2, Comp 3, Adj 0, Adj 1, and Adj 2.357Computational Linguistics Volume 25, Number 3In terms of the size of the grammar G, the complexity is (Q(IG\[2), since at mosttwo elementary trees are simultaneously considered in a single step.
Note that insome steps we address everal parts of a single elementary tree, such as the two partsrepresented by the items \[h, Ft, ---+ 3_., f i ,  f~, f~, f~\] and \[h, N ~ c~, Mf l ,  i, j, - ,  - \]in Adj 1.
However, the second of these items uniquely identifies the second field ofthe first item, and therefore this pair of items amounts to only one factor of IG\] in thetime complexity.The complexity of (.9(n 6) that we have achieved epends on two ideas: first, theuse of Adj 0, Adj 1, and Adj 2 instead of Adj 1 / and Adj 2 I, and second, the exclusionof irrelevant variables above.
Both are needed.
The exclusion of irrelevant variablesalone, in combination with Adj 1 t and Adj 2 t, leads to a complexity of O(n8).
Withoutexcluding irrelevant variables, we obtain a complexity of 0(//9) due to Comp 1, whichuses nine input positions.The question arises where the exact difference lies between our algorithm andthat of Schabes and Joshi (1988), and whether their algorithm could be improved toobtain the same time complexity as ours, using techniques similar to those discussedabove.
This question is difficult to answer precisely because of the significant differencebetween the types of items that are used in the respective algorithms.
However, somegeneral considerations suggest hat the algorithm from Schabes and Joshi (1988) isinherently more expensive.First, the items from the new algorithm have five input positions, which impliesthat storage of the parse table requires a space complexity of O(n5).
The items from theolder algorithm have effectively six input positions, which leads to a space complexityof 0(/76).Second, the "Right Completor" from Schabes and Joshi (1988), which roughlycorresponds with our adjunctor steps, has nine relevant input positions.
This step canbe straightforwardly broken up into smaller steps that each have fewer relevant inputpositions, but it seems difficult to reduce the maximal number of positions to six.A final remark on Schabes and Joshi (1988) concerns the time complexity in termsof the size of the grammar that they report, viz.
O(\]GI2).
This would be the same upperbound as in the case of the new algorithm.
However, the correct complexity seems tobe O(\]G\]3), since each item contains references to two nodes of the same elementarytree, and the combination i  "Right Completor" of two items entails the simultaneoususe of three distinct nodes from the grammar.6.
Further ResearchThe algorithm in the present paper operates in a top-down manner, being very similarto Earley's algorithm (Earley 1970), which is emphasized by the use of the "dotted"items.
As shown by Nederhof and Satta (1994), a family of parsing algorithms (top-down, left-corner, PLR, ELR, and LR parsing \[Nederhof 1994\]) can be carried overto head-driven parsing.
An obvious question is whether such parsing techniques canalso be used to produce variants of left-to-right parsing for TAGs.
Thus, one mayconjecture, for example, the existence of an LR-like parsing algorithm for arbitraryTAGs that operates in (_9(n 6) and that has the correct-prefix property.Note that LR-like parsing algorithms were proposed by Schabes and Vijay-Shanker(1990) and Nederhof (1998).
However, for these algorithms the correct-prefix propertyis not satisfied.Development of advanced parsing algorithms for TAGs with the correct-prefixproperty is not at all straightforward.
In the case of context-free grammars, the addi-tional benefit of LR parsing, in comparison to, for example, top-down parsing, lies in358Nederhof Correct-Prefix Property for TAGsthe ability to process multiple grammar ules simultaneously.
If this is to be carriedover to TAGs, then multiple elementary trees must be handled simultaneously.
Thisis difficult to combine with the mechanism we used to satisfy the correct-prefix prop-erty, which relies on filtering out hypotheses with respect o "left context."
Filteringout such hypotheses requires detailed investigation of that left context, which, how-ever, precludes treating multiple elementary trees simultaneously.
An exception maybe the case when a TAG contains many, almost identical, elementary trees.
It is notclear whether this case occurs often in practice.Therefore, further esearch is needed not only to precisely define advanced parsingalgorithms for TAGs with the correct-prefix property, but also to determine whetherthere are any benefits for practical grammars.AcknowledgmentsMost of the presented research was carriedout within the framework of the PriorityProgramme Language and SpeechTechnology (TST) while the author wasemployed at the University of Groningen.The TST-Programme is sponsored by NWO(Dutch Organization for Scientific Research).An error in a previous version of this paperwas found and corrected with the help ofGiorgio Satta.ReferencesEarley, Jay.
1970.
An efficient context-freeparsing algorithm.
Communications of theACM, 13(2):94-102, February.Joshi, Aravind K. 1987.
An introduction totree adjoining grammars.
In AlexisManaster-Ramer, ditor, Mathematics ofLanguage.
John Benjamins PublishingCompany, Amsterdam, pages 87-114.Lang, Bernard.
1988.
The systematicconstruction of Earley parsers:Application to the production of C9(n 6)Earley parsers for tree adjoininggrammars.
Unpublished paper, December.Nederhof, Mark-Jan. 1994.
An optimaltabular parsing algorithm.
In Proceedingsof the 32nd Annual Meeting, pages 117-124,Las Cruces, NM, June.
Association forComputational Linguistics.Nederhof, Mark-Jan. 1998.
An alternativeLR algorithm for TAGs.
In COLING-ACL'98 36th Annual Meeting o/the Association/orComputational Linguistics and 17thInternational Conference on ComputationalLinguistics, volume 2, pages 946-952,Montreal, Quebec, Canada, August.Nederhof, Mark-Jan and Giorgio Satta.
1994.An extended theory of head-drivenparsing.
In Proceedings o/the 32nd AnnualMeeting, pages 210-217, Las Cruces, NM,June.
Association for ComputationalLinguistics.Rajasekaran, Sanguthevar nd ShibuYooseph.
1995.
TAL recognition iO(M(n2) time.
In Proceedings ofthe 33rdAnnual Meeting, pages 166-173,Cambridge, MA, June.
Association forComputational Linguistics.Satta, Giorgio.
1994.
Tree-adjoininggrammar parsing and Boolean matrixmultiplication.
Computational Linguistics,20(2):173-191.Schabes, Yves.
1994.
Left to right parsing oflexicalized tree-adjoining grammars.Computational Intelligence, 10(4):506-524.Schabes, Yves and Aravind K. Joshi.
1988.An Earley-type parsing algorithm for treeadjoining grammars.
In Proceedings o/the26th Annual Meeting, pages 258-269,Buffalo, NY, June.
Association forComputational Linguistics.Schabes, Yves and Stuart M. Shieber.
1994.An alternative conception oftree-adjoining derivation.
ComputationalLinguistics, 20(1):91-124.Schabes, Yves and K. Vijay-Shanker.
1990.Deterministic left to right parsing of treeadjoining languages.
In Proceedings o/the28th Annual Meeting, pages 276-283,Pittsburgh, PA, June.
Association forComputational Linguistics.Schabes, Yves and Richard C. Waters.
1995.Tree insertion grammar: A cubic-time,parsable formalism that lexicalizescontext-free grammar without changingthe trees produced.
ComputationalLinguistics, 21(4):479-513.Shieber, Stuart M., Yves Schabes, andFernando C. N. Pereira.
1995.
Principlesand implementation f deductive parsing.Journal o/Logic Programming, 24:3-36.Sippu, Seppo and Eljas Soisalon-Soininen.1988.
Parsing Theory, Vol.
h Languages andParsing.
Volume 15 of EATCS Monographson Theoretical Computer Science.Springer-Verlag.Ullman, Jeffrey D. 1982.
Principles o/DatabaseSystems.
Computer Science Press.Vijay-Shankar, K. and Aravind K. Joshi.359Computational Linguistics Volume 25, Number 31985.
Some computational properties oftree adjoining grammars.
In Proceedings ofthe 23rd Annual Meeting, pages 82-93,Chicago, IL, July.
Association forComputational Linguistics.Vijay-Shanker, K. and David J. Weir.
1993.Parsing some constrained grammarformalisms.
Computational Linguistics,19(4):591-636.Vijay-Shanker, K. and David J. Weir.
1994.The equivalence of four extensions ofcontext-free grammars.
MathematicalSystems Theory, 27:511-546.360
