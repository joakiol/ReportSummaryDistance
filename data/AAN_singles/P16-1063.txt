Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 666?675,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsGenerative Topic Embedding: a Continuous Representation of DocumentsShaohua Li1,2Tat-Seng Chua1Jun Zhu3Chunyan Miao2shaohua@gmail.com dcscts@nus.edu.sg dcszj@tsinghua.edu.cn ascymiao@ntu.edu.sg1.
School of Computing, National University of Singapore2.
Joint NTU-UBC Research Centre of Excellence in Active Living for the Elderly (LILY)3.
Department of Computer Science and Technology, Tsinghua UniversityAbstractWord embedding maps words into a low-dimensional continuous embedding spaceby exploiting the local word collocationpatterns in a small context window.
Onthe other hand, topic modeling maps docu-ments onto a low-dimensional topic space,by utilizing the global word collocationpatterns in the same document.
Thesetwo types of patterns are complementary.In this paper, we propose a generativetopic embedding model to combine thetwo types of patterns.
In our model, topicsare represented by embedding vectors, andare shared across documents.
The proba-bility of each word is influenced by bothits local context and its topic.
A variationalinference method yields the topic embed-dings as well as the topic mixing propor-tions for each document.
Jointly they rep-resent the document in a low-dimensionalcontinuous space.
In two document clas-sification tasks, our method performs bet-ter than eight existing methods, with fewerfeatures.
In addition, we illustrate with anexample that our method can generate co-herent topics even based on only one doc-ument.1 IntroductionRepresenting documents as fixed-length featurevectors is important for many document process-ing algorithms.
Traditionally documents are rep-resented as a bag-of-words (BOW) vectors.
How-ever, this simple representation suffers from beinghigh-dimensional and highly sparse, and loses se-mantic relatedness across the vector dimensions.Word Embedding methods have been demon-strated to be an effective way to represent wordsas continuous vectors in a low-dimensional em-bedding space (Bengio et al, 2003; Mikolov et al,2013; Pennington et al, 2014; Levy et al, 2015).The learned embedding for a word encodes itssemantic/syntactic relatedness with other words,by utilizing local word collocation patterns.
Ineach method, one core component is the embed-ding link function, which predicts a word?s distri-bution given its context words, parameterized bytheir embeddings.When it comes to documents, we wish to find amethod to encode their overall semantics.
Giventhe embeddings of each word in a document, wecan imagine the document as a ?bag-of-vectors?.Related words in the document point in similar di-rections, forming semantic clusters.
The centroidof a semantic cluster corresponds to the most rep-resentative embedding of this cluster of words, re-ferred to as the semantic centroids.
We could usethese semantic centroids and the number of wordsaround them to represent a document.In addition, for a set of documents in a partic-ular domain, some semantic clusters may appearin many documents.
By learning collocation pat-terns across the documents, the derived semanticcentroids could be more topical and less noisy.Topic Models, represented by Latent DirichletAllocation (LDA) (Blei et al, 2003), are able togroup words into topics according to their colloca-tion patterns across documents.
When the corpusis large enough, such patterns reflect their seman-tic relatedness, hence topic models can discovercoherent topics.
The probability of a word is gov-erned by its latent topic, which is modeled as acategorical distribution in LDA.
Typically, only asmall number of topics are present in each docu-ment, and only a small number of words have highprobability in each topic.
This intuition motivatedBlei et al (2003) to regularize the topic distribu-tions with Dirichlet priors.666Semantic centroids have the same nature as top-ics in LDA, except that the former exist in the em-bedding space.
This similarity drives us to seek thecommon semantic centroids with a model similarto LDA.
We extend a generative word embeddingmodel PSDVec (Li et al, 2015), by incorporatingtopics into it.
The new model is named TopicVec.In TopicVec, an embedding link function modelsthe word distribution in a topic, in place of the cat-egorical distribution in LDA.
The advantage of thelink function is that the semantic relatedness is al-ready encoded as the cosine distance in the em-bedding space.
Similar to LDA, we regularize thetopic distributions with Dirichlet priors.
A varia-tional inference algorithm is derived.
The learningprocess derives topic embeddings in the same em-bedding space of words.
These topic embeddingsaim to approximate the underlying semantic cen-troids.To evaluate how well TopicVec represents doc-uments, we performed two document classifica-tion tasks against eight existing topic modeling ordocument representation methods.
Two setups ofTopicVec outperformed all other methods on twotasks, respectively, with fewer features.
In addi-tion, we demonstrate that TopicVec can derive co-herent topics based only on one document, whichis not possible for topic models.The source code of our implementation is avail-able at https://github.com/askerlee/topicvec.2 Related WorkLi et al (2015) proposed a generative word em-bedding method PSDVec, which is the precur-sor of TopicVec.
PSDVec assumes that the con-ditional distribution of a word given its contextwords can be factorized approximately into inde-pendent log-bilinear terms.
In addition, the wordembeddings and regression residuals are regular-ized by Gaussian priors, reducing their chance ofoverfitting.
The model inference is approached byan efficient Eigendecomposition and blockwise-regression method (Li et al, 2016b).
TopicVecdiffers from PSDVec in that in the conditional dis-tribution of a word, it is not only influenced by itscontext words, but also by a topic, which is an em-bedding vector indexed by a latent variable drawnfrom a Dirichlet-Multinomial distribution.Hinton and Salakhutdinov (2009) proposed tomodel topics as a certain number of binary hiddenvariables, which interact with all words in the doc-ument through weighted connections.
Larochelleand Lauly (2012) assigned each word a uniquetopic vector, which is a summarization of the con-text of the current word.Huang et al (2012) proposed to incorporateglobal (document-level) semantic information tohelp the learning of word embeddings.
The globalembedding is simply a weighted average of theembeddings of words in the document.Le and Mikolov (2014) proposed ParagraphVector.
It assumes each piece of text has a la-tent paragraph vector, which influences the distri-butions of all words in this text, in the same wayas a latent word.
It can be viewed as a special caseof TopicVec, with the topic number set to 1.
Typ-ically, however, a document consists of multiplesemantic centroids, and the limitation of only onetopic may lead to underfitting.Nguyen et al (2015) proposed Latent FeatureTopic Modeling (LFTM), which extends LDA toincorporate word embeddings as latent features.The topic is modeled as a mixture of the con-ventional categorical distribution and an embed-ding link function.
The coupling between thesetwo components makes the inference difficult.They designed a Gibbs sampler for model infer-ence.
Their implementation1is slow and infeasi-ble when applied to a large corpous.Liu et al (2015) proposed Topical Word Em-bedding (TWE), which combines word embed-ding with LDA in a simple and effective way.They train word embeddings and a topic modelseparately on the same corpus, and then averagethe embeddings of words in the same topic to getthe embedding of this topic.
The topic embeddingis concatenated with the word embedding to formthe topical word embedding of a word.
In the end,the topical word embeddings of all words in a doc-ument are averaged to be the embedding of thedocument.
This method performs well on our twoclassification tasks.
Weaknesses of TWE include:1) the way to combine the results of word embed-ding and LDA lacks statistical foundations; 2) theLDA module requires a large corpus to derive se-mantically coherent topics.Das et al (2015) proposed Gaussian LDA.
Ituses pre-trained word embeddings.
It assumes thatwords in a topic are random samples from a mul-tivariate Gaussian distribution with the topic em-bedding as the mean.
Hence the probability that a1https://github.com/datquocnguyen/LFTM/667Name DescriptionS Vocabulary {s1, ?
?
?
, sW}V Embedding matrix (vs1, ?
?
?
,vsW)D Document set {d1, ?
?
?
, dM}vsiEmbedding of word siasisj,A Bigram residualstik,TiTopic embeddings in doc dirik, riTopic residuals in doc dizijTopic assignment of the j-th word j in doc di?iMixing proportions of topics in doc diTable 1: Table of notationsword belongs to a topic is determined by the Eu-clidean distance between the word embedding andthe topic embedding.
This assumption might beimproper as the Euclidean distance is not an opti-mal measure of semantic relatedness between twoembeddings2.3 Notations and DefinitionsThroughout this paper, we use uppercase bold let-ters such as S,V to denote a matrix or set, low-ercase bold letters such as vwito denote a vector,a normal uppercase letter such as N,W to denotea scalar constant, and a normal lowercase letter assi, wito denote a scalar variable.Table 1 lists the notations in this paper.In a document, a sequence of words is referredto as a text window, denoted by wi, ?
?
?
, wi+l,or wi:wi+l.
A text window of chosen size cbefore a word widefines the context of wiaswi?c, ?
?
?
, wi?1.
Here wiis referred to as the fo-cus word.
Each context word wi?jand the focusword wicomprise a bigram wi?j, wi.We assume each word in a document is seman-tically similar to a topic embedding.
Topic embed-dings reside in the same N -dimensional space asword embeddings.
When it is clear from context,topic embeddings are often referred to as topics.Each document has K candidate topics, arrangedin the matrix form Ti= (ti1?
?
?
tiK), referred toas the topic matrix.
Specifically, we fix ti1= 0,referring to it as the null topic.In a document di, each word wijis assigned toa topic indexed by zij?
{1, ?
?
?
,K}.
Geometri-cally this means the embedding vwijtends to align2Almost all modern word embedding methods adopt theexponentiated cosine similarity as the link function, hence thecosine similarity may be assumed to be a better estimate ofthe semantic relatedness between embeddings derived fromthese methods.with the direction of ti,zij.
Each topic tikhas adocument-specific prior probability to be assignedto a word, denoted as ?ik= P (k|di).
The vector?i= (?i1, ?
?
?
, ?iK) is referred to as the mixingproportions of these topics in document di.4 Link Function of Topic EmbeddingIn this section, we formulate the distribution of aword given its context words and topic, in the formof a link function.The core of most word embedding methods is alink function that connects the embeddings of a fo-cus word and its context words, to define the distri-bution of the focus word.
Li et al (2015) proposedthe following link function:P (wc| w0: wc?1)?P (wc) exp{v>wcc?1?l=0vwl+c?1?l=0awlwc}.
(1)Here awlwcis referred as the bigram resid-ual, indicating the non-linear part not captured byv>wcvwl.
It is essentially the logarithm of the nor-malizing constant of a softmax term.
Some litera-ture, e.g.
(Pennington et al, 2014), refers to sucha term as a bias term.
(1) is based on the assumption that the con-ditional distribution P (wc| w0: wc?1) canbe factorized approximately into independent log-bilinear terms, each corresponding to a contextword.
This approximation leads to an efficient andeffective word embedding algorithm PSDVec (Liet al, 2015).
We follow this assumption, and pro-pose to incorporate the topic of wcin a way like alatent word.
In particular, in addition to the con-text words, the corresponding embedding tikis in-cluded as a new log-bilinear term that influencesthe distribution of wc.
Hence we obtain the fol-lowing extended link function:P (wc| w0:wc?1, zc, di) ?
P (wc)?exp{v>wc(c?1?l=0vwl+ tzc)+c?1?l=0awlwc+rzc}, (2)where diis the current document, and rzcis thelogarithm of the normalizing constant, named thetopic residual.
Note that the topic embeddings tzcmay be specific to di.
For simplicity of notation,we drop the document index in tzc.
To restrictthe impact of topics and avoid overfitting, we con-strain the magnitudes of all topic embeddings, sothat they are always within a hyperball of radius ?.668w1?
?
?w0wczc?d?vsi?iWord EmbeddingsasisjhijResidualsGaussian GaussianMultDirtTopic Embeddingswc?
dTd ?
DDocumentsV AFigure 1: Graphical representation of TopicVec.It is infeasible to compute the exact value of thetopic residual rk.
We approximate it by the contextsize c = 0.
Then (2) becomes:P (wc| k, di) = P (wc) exp{v>wctk+ rk}.
(3)It is required that?wc?S P (wc | k) = 1 tomake (3) a distribution.
It follows thatrk= ?
log(?sj?SP (sj) exp{v>sjtk}).
(4)(4) can be expressed in the matrix form:r = ?
log(u exp{V>T }), (5)whereu is the row vector of unigram probabilities.5 Generative Process and LikelihoodThe generative process of words in documents canbe regarded as a hybrid of LDA and PSDVec.Analogous to PSDVec, the word embedding vsiand residual asisjare drawn from respective Gaus-sians.
For the sake of clarity, we ignore their gen-eration steps, and focus on the topic embeddings.The remaining generative process is as follows:1.
For the k-th topic, draw a topic embedding uni-formly from a hyperball of radius ?, i.e.
tk?Unif(B?);2.
For each document di:(a) Draw the mixing proportions ?ifrom theDirichlet prior Dir(?
);(b) For the j-th word:i.
Draw topic assignment zijfrom the cate-gorical distribution Cat(?i);ii.
Draw word wijfrom S according toP (wij| wi,j?c:wi,j?1, zij, di).The above generative process is presented in platenotation in Figure (1).5.1 Likelihood FunctionGiven the embeddings V , the bigram residualsA, the topics Tiand the hyperparameter ?, thecomplete-data likelihood of a single document diis:p(di,Zi,?i|?,V ,A,Ti)=p(?i|?
)p(Zi|?i)p(di|V ,A,Ti,Zi)=?(?Kk=1?k)?Kk=1?(?k)K?j=1?
?j?1ij?Li?j=1(?i,zijP (wij)?
exp{v>wij(j?1?l=j?cvwil+ tzij)+j?1?l=j?cawilwij+ ri,zij}), (6)where Zi= (zi1, ?
?
?
, ziLi), and ?(?)
is theGamma function.Let Z,T ,?
denote the collection of all thedocument-specific {Zi}Mi=1, {Ti}Mi=1, {?i}Mi=1,respectively.
Then the complete-data likelihoodof the whole corpus is:p(D,A,V ,Z,T ,?|?, ?,?
)=W?i=1P (vsi;?i)W,W?i,j=1P (asisj; f(hij))K?kUnif(B?)?M?i=1{p(?i|?
)p(Zi|?i)p(di|V ,A,Ti,Zi)}=1Z(H,?)UK?exp{?W,W?i,j=1f(hi,j)a2sisj?W?i=1?i?vsi?2}?M?i=1{?(?Kk=1?k)?Kk=1?(?k)K?j=1?
?j?1ij?Li?j=1(?i,zijP (wij)?
exp{v>wij(j?1?l=j?cvwil+tzij)+j?1?l=j?cawilwij+ri,zij})},(7)where P (vsi;?i) and P (asisj; f(hij)) are the twoGaussian priors as defined in (Li et al, 2015).669Following the convention in (Li et al, 2015),hij,H are empirical bigram probabilities, ?
arethe embedding magnitude penalty coefficients,andZ(H,?)
is the normalizing constant for wordembeddings.
U?is the volume of the hyperball ofradius ?.Taking the logarithm of both sides, we obtainlog p(D,A,V ,Z,T ,?|?, ?,?)=C0?
logZ(H,?)?
?A?2f(H) ?W?i=1?i?vsi?2+M?i=1{K?k=1log ?ik(mik+ ?k?
1) +Li?j=1(ri,zij+v>wij(j?1?l=j?cvwil+ tzij)+j?1?l=j?cawilwij)}, (8)wheremik=?Lij=1?
(zij= k) counts the numberof words assigned with the k-th topic in di, C0=M log?(?Kk=1?k)?Kk=1?
(?k)+?M,Lii,j=1logP (wij)?K logU?is constant given the hyperparameters.6 Variational Inference Algorithm6.1 Learning Objective and ProcessGiven the hyperparameters ?, ?,?, the learningobjective is to find the embeddings V , the topicsT , and the word-topic and document-topic distri-butions p(Zi,?i|di,A,V ,T ).
Here the hyperpa-rameters ?, ?,?
are kept constant, and we makethem implicit in the distribution notations.However, the coupling between A,V andT ,Z,?
makes it inefficient to optimize them si-multaneously.
To get around this difficulty, welearn word embeddings and topic embeddings sep-arately.
Specifically, the learning process is di-vided into two stages:1.
In the first stage, considering that the topicshave a relatively small impact to word dis-tributions and the impact might be ?averagedout?
across different documents, we simplifythe model by ignoring topics temporarily.
Thenthe model falls back to the original PSDVec.The optimal solution V?,A?is obtained ac-cordingly;2.
In the second stage, we treat V?,A?asconstant, plug it into the likelihood func-tion, and find the corresponding optimalT?, p(Z,?|D,A?,V?,T?)
of the full model.As in LDA, this posterior is analytically in-tractable, and we use a simpler variational dis-tribution q(Z,?)
to approximate it.6.2 Mean-Field Approximation andVariational GEM AlgorithmIn this stage, we fix V = V?,A = A?, andseek the optimal T?, p(Z,?|D,A?,V?,T?).
AsV?,A?are constant, we also make them implicitin the following expressions.For an arbitrary variational distributionq(Z,?
), the following equalities holdEqlog[p(D,Z,?|T )q(Z,?
)]=Eq[log p(D,Z,?|T )] +H(q)= log p(D|T )?
KL(q||p), (9)where p = p(Z,?|D,T ), H(q) is the entropy ofq.
This impliesKL(q||p)= log p(D|T )?
(Eq[log p(D,Z,?|T )] +H(q))= log p(D|T )?
L(q,T ).
(10)In (10), Eq[log p(D,Z,?|T )] + H(q) is usu-ally referred to as the variational free energyL(q,T ), which is a lower bound of log p(D|T ).Directly maximizing log p(D|T ) w.r.t.
T is in-tractable due to the hidden variables Z,?, so wemaximize its lower bound L(q,T ) instead.
Weadopt a mean-field approximation of the true pos-terior as the variational distribution, and use avariational algorithm to find q?,T?maximizingL(q,T ).The following variational distribution is used:q(Z,?;pi,?)
= q(?;?)q(Z;pi)=M?i=1???Dir(?i;?i)Li?j=1Cat(zij;piij)???.
(11)We can obtain (Li et al, 2016a)L(q,T )=M?i=1{K?k=1(Li?j=1pikij+ ?k?
1)(?(?ik)?
?
(?i0))+ Tr(T>iLi?j=1vwijpi>ij) + r>iLi?j=1piij}+H(q) + C1, (12)670where Tiis the topic matrix of the i-th docu-ment, and riis the vector constructed by con-catenating all the topic residuals rik.
C1=C0?logZ(H,?)??A?2f(H)?
?Wi=1?i?vsi?2+?M,Lii,j=1(v>wij?j?1k=j?cvwik+?j?1k=j?cawikwij)isconstant.We proceed to optimize (12) with a General-ized Expectation-Maximization (GEM) algorithmw.r.t.
q and T as follows:1.
Initialize all the topics Ti= 0, and correspond-ingly their residuals ri= 0;2.
Iterate over the following two steps until con-vergence.
In the l-th step:(a) Let the topics and residuals be T =T(l?1), r = r(l?1), find q(l)(Z,?)
that max-imizes L(q,T(l?1)).
This is the Expectationstep (E-step).
In this step, log p(D|T ) is con-stant.
Then the q that maximizes L(q,T(l))will minimize KL(q||p), i.e.
such a q is theclosest variational distribution to p measuredby KL-divergence;(b) Given the variational distribution q(l)(Z,?
),find T(l), r(l)that improve L(q(l),T ), usingGradient descent method.
This is the gener-alized Maximization step (M-step).
In thisstep, pi,?,H(q) are constant.6.2.1 Update Equations of pi,?
in E-StepIn the E-step, T = T(l?1), r = r(l?1)are con-stant.
Taking the derivative of L(q,T(l?1)) w.r.t.pikijand ?ik, respectively, we can obtain the opti-mal solutions (Li et al, 2016a) at:pikij?
exp{?
(?ik) + v>wijtik+ rik}.
(13)?ik=Li?j=1pikij+ ?k.
(14)6.2.2 Update Equation of Tiin M-StepIn the Generalized M-step, pi = pi(l),?
= ?(l)areconstant.
For notational simplicity, we drop theirsuperscripts (l).To update Ti, we first take the derivative of(12) w.r.t.
Ti, and then take the Gradient Descentmethod.The derivative is obtained as (Li et al, 2016a):?L(q(l),T )?Ti=Li?j=1vwijpi>ij+K?k=1m?ik?rik?Ti, (15)where m?ik=?Lij=1pikij= E[mik], the sum ofthe variational probabilities of each word being as-signed to the k-th topic in the i-th document.
?rik?Tiis a gradient matrix, whose j-th column is?rik?tij.Remind that rik= ?
log(EP (s)[exp{v>stik}]).When j 6= k, it is easy to verify that?rik?tij= 0.When j = k, we have?rik?tik= e?rik?
EP (s)[exp{v>stik}vs]= e?rik?
?s?Wexp{v>stik}P (s)vs= e?rik?
exp{t>ikV }(u ?
V ), (16)where u?V is to multiply each column of V withu element-by-element.Therefore?rik?Ti= (0, ?
?
?
?rik?tik, ?
?
?
,0).
Plug-ging it into (15), we obtain?L(q(l),T )?Ti=Li?j=1vwijpi>ij+(m?i1?ri1?ti1, ?
?
?
, m?iK?riK?tiK).We proceed to optimize Tiwith a gradient de-scent method:T(l)i= T(l?1)+ ?
(l, Li)?L(q(l),T )?Ti,where ?
(l, Li) =L0?0l?max{Li,L0}is the learning ratefunction, L0is a pre-specified document lengththreshold, and ?0is the initial learning rate.
Asthe magnitude of?L(q(l),T )?Tiis approximately pro-portional to the document length Li, to avoid thestep size becoming too big a on a long document,if Li> L0, we normalize it by Li.To satisfy the constraint that ?t(l)ik?
?
?, whent(l)ik> ?, we normalize it by ?/?t(l)ik?.After we obtain the new T , we update r(m)ius-ing (5).Sometimes, especially in the initial few itera-tions, due to the excessively big step size of thegradient descent, L(q,T ) may decrease after theupdate of T .
Nonetheless the general direction ofL(q,T ) is increasing.6.3 Sharing of Topics across DocumentsIn principle we could use one set of topics acrossthe whole corpus, or choose different topics fordifferent subsets of documents.
One could choosea way to best utilize cross-document information.For instance, when the document category in-formation is available, we could make the docu-ments in each category share their respective set671of topics, so that M categories correspond to Msets of topics.
In the learning algorithm, only theupdate of pikijneeds to be changed to cater for thissituation: when the k-th topic is relevant to thedocument i, we update pikijusing (13); otherwisepikij= 0.An identifiability problem may arise when wesplit topic embeddings according to documentsubsets.
In different topic groups, some highlysimilar redundant topics may be learned.
If weproject documents into the topic space, portionsof documents in the same topic in different docu-ments may be projected onto different dimensionsof the topic space, and similar documents mayeventually be projected into very different topicproportion vectors.
In this situation, directly us-ing the projected topic proportion vectors couldcause problems in unsupervised tasks such as clus-tering.
A simple solution to this problem would beto compute the pairwise similarities between topicembeddings, and consider these similarities whencomputing the similarity between two projectedtopic proportion vectors.
Two similar documentswill then still receive a high similarity score.7 Experimental ResultsTo investigate the quality of document represen-tation of our TopicVec model, we compared itsperformance against eight topic modeling or doc-ument representation methods in two documentclassification tasks.
Moreover, to show the topiccoherence of TopicVec on a single document, wepresent the top words in top topics learned on anews article.7.1 Document Classification Evaluation7.1.1 Experimental SetupCompared Methods Two setups of TopicVecwere evaluated:?
TopicVec: the topic proportions learned byTopicVec;?
TV+WV: the topic proportions, concate-nated with the mean word embedding of thedocument (same as the MeanWV below).We compare the performance of our methodsagainst eight methods, including three topic mod-eling methods, three continuous document repre-sentation methods, and the conventional bag-of-words (BOW) method.
The count vector of BOWis unweighted.The topic modeling methods include:?
LDA: the vanilla LDA (Blei et al, 2003) inthe gensim library3;?
sLDA: Supervised Topic Model4(McAuliffeand Blei, 2008), which improves the predic-tive performance of LDA by modeling classlabels;?
LFTM: Latent Feature Topic Modeling5(Nguyen et al, 2015).The document-topic proportions of topic modelingmethods were used as their document representa-tion.The document representation methods are:?
Doc2Vec: Paragraph Vector (Le andMikolov, 2014) in the gensim library6.?
TWE: Topical Word Embedding7(Liu etal., 2015), which represents a documentby concatenating average topic embeddingand average word embedding, similar to ourTV+WV;?
GaussianLDA: Gaussian LDA8(Das et al,2015), which assumes that words in a topicare random samples from a multivariateGaussian distribution with the mean as thetopic embedding.
Similar to TopicVec, wederived the posterior topic proportions as thefeatures of each document;?
MeanWV: The mean word embedding of thedocument.Datasets We used two standard document clas-sification corpora: the 20 Newsgroups9and theApteMod version of the Reuters-21578 corpus10.The two corpora are referred to as the 20News andReuters in the following.20News contains about 20,000 newsgroup doc-uments evenly partitioned into 20 different cate-gories.
Reuters contains 10,788 documents, whereeach document is assigned to one or more cate-gories.
For the evaluation of document classifi-cation, documents appearing in two or more cate-gories were removed.
The numbers of documentsin the categories of Reuters are highly imbalanced,and we only selected the largest 10 categories,leaving us with 8,025 documents in total.3https://radimrehurek.com/gensim/models/ldamodel.html4http://www.cs.cmu.edu/?chongw/slda/5https://github.com/datquocnguyen/LFTM/6https://radimrehurek.com/gensim/models/doc2vec.html7https://github.com/largelymfs/topical word embeddings/8https://github.com/rajarshd/Gaussian LDA9http://qwone.com/?jason/20Newsgroups/10http://www.nltk.org/book/ch02.html672The same preprocessing steps were applied toall methods: words were lowercased; stop wordsand words out of the word embedding vocabulary(which means that they are extremely rare) wereremoved.Experimental Settings TopicVec used the wordembeddings trained using PSDVec on a March2015 Wikipedia snapshot.
It contains the most fre-quent 180,000 words.
The dimensionality of wordembeddings and topic embeddings was 500.
Thehyperparameters were ?
= (0.1, ?
?
?
, 0.1), ?
= 5.For 20news and Reuters, we specified 15 and 12topics in each category on the training set, respec-tively.
The first topic in each category was al-ways set to null.
The learned topic embeddingswere combined to form the whole topic set, whereredundant null topics in different categories wereremoved, leaving us with 281 topics for 20Newsand 111 topics for Reuters.
The initial learningrate was set to 0.1.
After 100 GEM iterationson each dataset, the topic embeddings were ob-tained.
Then the posterior document-topic distri-butions of the test sets were derived by performingone E-step given the topic embeddings trained onthe training set.LFTM includes two models: LF-LDA and LF-DMM.
We chose the better performing LF-LDAto evaluate.
TWE includes three models, and wechose the best performing TWE-1 to compare.LDA, sLDA, LFTM and TWE used the spec-ified 50 topics on Reuters, as this is the optimaltopic number according to (Lu et al, 2011).
Onthe larger 20news dataset, they used the specified100 topics.
Other hyperparameters of all com-pared methods were left at their default values.GaussianLDA was specified 100 topics on20news and 70 topics on Reuters.
As each sam-pling iteration took over 2 hours, we only had timefor 100 sampling iterations.For each method, after obtaining the documentrepresentations of the training and test sets, wetrained an `-1 regularized linear SVM one-vs-allclassifier on the training set using the scikit-learnlibrary11.
We then evaluated its predictive perfor-mance on the test set.Evaluation metrics Considering that the largestfew categories dominate Reuters, we adoptedmacro-averaged precision, recall and F1 measuresas the evaluation metrics, to avoid the average re-sults being dominated by the performance of the11http://scikit-learn.org/stable/modules/svm.html20News ReutersPrec Rec F1 Prec Rec F1BOW 69.1 68.5 68.6 92.5 90.3 91.1LDA 61.9 61.4 60.3 76.1 74.3 74.8sLDA 61.4 60.9 60.9 88.3 83.3 85.1LFTM 63.5 64.8 63.7 84.6 86.3 84.9MeanWV 70.4 70.3 70.1 92.0 89.6 90.5Doc2Vec 56.3 56.6 55.4 84.4 50.0 58.5TWE 69.5 69.3 68.8 91.0 89.1 89.9GaussianLDA 30.9 26.5 22.7 46.2 31.5 35.3TopicVec 71.4 71.3 71.2 91.8 92.0 91.7TV+WV172.1 71.9 71.8 91.4 91.9 91.51Combined features of TopicVec topic proportions andMeanWV.Table 2: Performance on multi-class text classifi-cation.
Best score is in boldface.Avg.
Features BOW MeanWV TWE TopicVec TV+WV20News 50381 500 800 281 781Reuters 17989 500 800 111 611Table 3: Number of features of the five best per-forming methods.top categories.Evaluation Results Table 2 presents the perfor-mance of the different methods on the two clas-sification tasks.
The highest scores were high-lighted with boldface.
It can be seen that TV+WVand TopicVec obtained the best performance onthe two tasks, respectively.
With only topic pro-portions as features, TopicVec performed slightlybetter than BOW, MeanWV and TWE, and sig-nificantly outperformed four other methods.
Thenumber of features it used was much lower thanBOW, MeanWV and TWE (Table 3).GaussianLDA performed considerably inferiorto all other methods.
After checking the generatedtopic embeddings manually, we found that the em-beddings for different topics are highly similar toeach other.
Hence the posterior topic proportionswere almost uniform and non-discriminative.
Inaddition, on the two datasets, even the fastest Aliassampling in (Das et al, 2015) took over 2 hours forone iteration and 10 days for the whole 100 itera-tions.
In contrast, our method finished the 100 EMiterations in 2 hours.673Figure 2: Topic Cloud of the pharmaceutical com-pany acquisition news.7.2 Qualitative Assessment of Topics Derivedfrom a Single DocumentTopic models need a large set of documents to ex-tract coherent topics.
Hence, methods dependingon topic models, such as TWE, are subject to thislimitation.
In contrast, TopicVec can extract co-herent topics and obtain document representationseven when only one document is provided as in-put.To illustrate this feature, we ran TopicVec ona New York Times news article about a pharma-ceutical company acquisition12, and obtained 20topics.Figure 2 presents the most relevant words inthe top-6 topics as a topic cloud.
We first calcu-lated the relevance between a word and a topic asthe frequency-weighted cosine similarity of theirembeddings.
Then the most relevant words wereselected to represent each topic.
The sizes ofthe topic slices are proportional to the topic pro-portions, and the font sizes of individual wordsare proportional to their relevance to the topics.Among these top-6 topics, the largest and small-est topic proportions are 26.7% and 9.9%, respec-tively.As shown in Figure 2, words in obtained topicswere generally coherent, although the topics were12http://www.nytimes.com/2015/09/21/business/a-huge-overnight-increase-in-a-drugs-price-raises-protests.htmlonly derived from a single document.
The reasonis that TopicVec takes advantage of the rich se-mantic information encoded in word embeddings,which were pretrained on a large corpus.The topic coherence suggests that the derivedtopic embeddings were approximately the seman-tic centroids of the document.
This capacity mayaid applications such as document retrieval, wherea ?compressed representation?
of the query docu-ment is helpful.8 Conclusions and Future WorkIn this paper, we proposed TopicVec, a generativemodel combining word embedding and LDA, withthe aim of exploiting the word collocation patternsboth at the level of the local context and the globaldocument.
Experiments show that TopicVec canlearn high-quality document representations, evengiven only one document.In our classification tasks we only explored theuse of topic proportions of a document as its rep-resentation.
However, jointly representing a doc-ument by topic proportions and topic embeddingswould be more accurate.
Efficient algorithms forthis task have been proposed (Kusner et al, 2015).Our method has potential applications in vari-ous scenarios, such as document retrieval, classifi-cation, clustering and summarization.AcknowlegementWe thank Xiuchao Sui and Linmei Hu for theirhelp and support.
We thank the anonymous men-tor provided by ACL for the careful proofread-ing.
This research is funded by the National Re-search Foundation, Prime Minister?s Office, Sin-gapore under its IDM Futures Funding Initiativeand IRC@SG Funding Initiative administered byIDMPO.ReferencesYoshua Bengio, R?ejean Ducharme, Pascal Vincent, andChristian Jauvin.
2003.
A neural probabilistic lan-guage model.
Journal of Machine Learning Re-search, pages 1137?1155.David M Blei, Andrew Y Ng, and Michael I Jordan.2003.
Latent dirichlet alocation.
the Journal of ma-chine Learning research, 3:993?1022.Rajarshi Das, Manzil Zaheer, and Chris Dyer.
2015.Gaussian LDA for topic models with word embed-dings.
In Proceedings of the 53rd Annual Meet-ing of the Association for Computational Linguistics674and the 7th International Joint Conference on Natu-ral Language Processing (Volume 1: Long Papers),pages 795?804, Beijing, China, July.
Association forComputational Linguistics.Geoffrey E Hinton and Ruslan R Salakhutdinov.
2009.Replicated softmax: an undirected topic model.
InAdvances in neural information processing systems,pages 1607?1614.Eric H Huang, Richard Socher, Christopher D Man-ning, and Andrew Y Ng.
2012.
Improving wordrepresentations via global context and multiple wordprototypes.
In Proceedings of the 50th Annual Meet-ing of the Association for Computational Linguis-tics: Long Papers-Volume 1, pages 873?882.
Asso-ciation for Computational Linguistics.Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian Q.Weinberger.
2015.
From word embeddings to docu-ment distances.
In David Blei and Francis Bach, ed-itors, Proceedings of the 32nd International Confer-ence on Machine Learning (ICML-15), pages 957?966.
JMLR Workshop and Conference Proceedings.Hugo Larochelle and Stanislas Lauly.
2012.
A neuralautoregressive topic model.
In Advances in NeuralInformation Processing Systems, pages 2708?2716.Quoc Le and Tomas Mikolov.
2014.
Distributed repre-sentations of sentences and documents.
In Proceed-ings of the 31st International Conference on Ma-chine Learning (ICML-14), pages 1188?1196.Omer Levy, Yoav Goldberg, and Ido Dagan.
2015.
Im-proving distributional similarity with lessons learnedfrom word embeddings.
Transactions of the Associ-ation for Computational Linguistics, 3:211?225.Shaohua Li, Jun Zhu, and Chunyan Miao.
2015.
Agenerative word embedding model and its low rankpositive semidefinite solution.
In Proceedings ofthe 2015 Conference on Empirical Methods in Natu-ral Language Processing, pages 1599?1609, Lisbon,Portugal, September.
Association for ComputationalLinguistics.Shaohua Li, Tat-Seng Chua, Jun Zhu, and Chun-yan Miao.
2016a.
Generative topic em-bedding: a continuous representation of docu-ments (extended version with proofs).
Technicalreport.
https://github.com/askerlee/topicvec/blob/master/topicvec-ext.pdf.Shaohua Li, Jun Zhu, and Chunyan Miao.
2016b.
PS-DVec: a toolbox for incremental and scalable wordembedding.
To appear in Neurocomputing.Yang Liu, Zhiyuan Liu, Tat-Seng Chua, and MaosongSun.
2015.
Topical word embeddings.
In AAAI,pages 2418?2424.Yue Lu, Qiaozhu Mei, and ChengXiang Zhai.
2011.Investigating task performance of probabilistic topicmodels: an empirical study of PLSA and LDA.
In-formation Retrieval, 14(2):178?203.Jon D McAuliffe and David M Blei.
2008.
Super-vised topic models.
In Advances in neural informa-tion processing systems, pages 121?128.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013.
Distributed representa-tions of words and phrases and their compositional-ity.
In Proceedings of NIPS 2013, pages 3111?3119.Dat Quoc Nguyen, Richard Billingsley, Lan Du, andMark Johnson.
2015.
Improving topic models withlatent feature word representations.
Transactionsof the Association for Computational Linguistics,3:299?313.Jeffrey Pennington, Richard Socher, and Christopher DManning.
2014.
GloVe: Global vectors forword representation.
Proceedings of the EmpiricialMethods in Natural Language Processing (EMNLP2014), 12.675
