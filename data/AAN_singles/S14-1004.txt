Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 30?39,Dublin, Ireland, August 23-24 2014.Sense and Similarity: A Study of Sense-level Similarity MeasuresNicolai Erbs?, Iryna Gurevych?
?and Torsten Zesch??
UKP Lab, Technische Universit?at Darmstadt?
Information Center for Education, DIPF, Frankfurt?
Language Technology Lab, University of Duisburg-Essenhttp://www.ukp.tu-darmstadt.deAbstractIn this paper, we investigate the differ-ence between word and sense similaritymeasures and present means to converta state-of-the-art word similarity measureinto a sense similarity measure.
In or-der to evaluate the new measure, we cre-ate a special sense similarity dataset andre-rate an existing word similarity datasetusing two different sense inventories fromWordNet and Wikipedia.
We discoverthat word-level measures were not ableto differentiate between different sensesof one word, while sense-level measuresactually increase correlation when shift-ing to sense similarities.
Sense-level sim-ilarity measures improve when evaluatedwith a re-rated sense-aware gold standard,while correlation with word-level similar-ity measures decreases.1 IntroductionMeasuring similarity between words is a very im-portant task within NLP with applications in taskssuch as word sense disambiguation, informationretrieval, and question answering.
However, mostof the existing approaches compute similarity onthe word-level instead of the sense-level.
Conse-quently, most evaluation datasets have so far beenannotated on the word level, which is problem-atic as annotators might not know some infrequentsenses and are influenced by the more probablesenses.
In this paper, we provide evidence that thisprocess heavily influences the annotation process.For example, when people are presented the wordpair jaguar - gamepad only few people know thatJaguarGamepadZoo.0070.0016.0000Figure 1: Similarity between words.jaguar is also the name of an Atari game console.1People rather know the more common senses ofjaguar, i.e.
the car brand or the animal.
Thus, theword pair receives a low similarity score, whilecomputational measures are not so easily fooledby popular senses.
It is thus likely that existingevaluation datasets give a wrong picture of the trueperformance of similarity measures.Thus, in this paper we investigate whether sim-ilarity should be measured on the sense level.
Weanalyze state-of-the-art methods and describe howthe word-based Explicit Semantic Analysis (ESA)measure (Gabrilovich and Markovitch, 2007) canbe transformed into a sense-level measure.
Wecreate a sense similarity dataset, where senses areclearly defined and evaluate similarity measureswith this novel dataset.
We also re-annotate an ex-isting word-level dataset on the sense level in orderto study the impact of sense-level computation ofsimilarity.2 Word-level vs. Sense-level SimilarityExisting measures either compute similarity (i) onthe word level or (ii) on the sense level.
Similarityon the word level may cover any possible sense ofthe word, where on the sense level only the actualsense is considered.
We use Wikipedia Link Mea-1If you knew that it is a certain sign that you are gettingold.30Atari Jaguar Jaguar (animal)GamepadZoo.0000.0321 .0341.0000.0000Figure 2: Similarity between senses.sure (Milne, 2007) and Lin (Lin, 1998) as exam-ples of sense-level similarity measures2and ESAas the prototypical word-level measure.3The Lin measure is a widely used graph-basedsimilarity measure from a family of similar ap-proaches (Budanitsky and Hirst, 2006; Seco et al.,2004; Banerjee and Pedersen, 2002; Resnik, 1999;Jiang and Conrath, 1997; Grefenstette, 1992).
Itcomputes the similarity between two senses basedon the information content (IC) of the lowest com-mon subsumer (lcs) and both senses (see For-mula 1).simlin=2 IC(lcs)IC(sense1) + IC(sense2)(1)Another type of sense-level similarity measureis based on Wikipedia that can also be considered asense inventory, similar to WordNet.
Milne (2007)uses the link structure obtained from articles tocount the number of shared incoming links of ar-ticles.
Milne and Witten (2008) give a more effi-cient variation for computing similarity (see For-mula 2) based on the number of links for each ar-ticle, shared links |A ?B| and the total number ofarticles in Wikipedia|W |.simLM=logmax(|A| ,|B|)?
log|A ?B|log|W | ?
logmin(|A| ,|B|)(2)All sense-level similarity measures can be con-verted into a word similarity measure by comput-ing the maximum similarity between all possiblesense pairs.
Formula 3 shows the heuristic, withSnbeing the possible senses for word n, simwtheword similarity, and simsthe sense similarity.simw(w1, w2) = maxs1?S1,s2?S2sims(s1, s2) (3)Explicit Semantic Analysis (ESA) (Gabrilovichand Markovitch, 2007) is a widely used word-level2We selected these measures because they are intuitive butstill among the best performing measures.3Hassan and Mihalcea (2011) classify these measures ascorpus-based and knowledge-based.similarity measure based on Wikipedia as a back-ground document collection.
ESA constructs a n-dimensional space, where n is the number of arti-cles in Wikipedia.
A word is transformed in a vec-tor with the length n. Values of the vector are de-termined by the term frequency in the correspond-ing dimension, i.e.
in a certain Wikipedia article.The similarity of two words is then computed asthe inner product (usually the cosine) of the twoword vectors.We now show how ESA can be adapted success-fully to work on the sense-level, too.2.1 DESA: Disambiguated ESAIn the standard definintion, ESA computes theterm frequency based on the number of times aterm?usually a word?appears in a document.
Inorder to make it work on the sense level, we willneed a large sense-disambiguated corpus.
Sucha corpus could be obtained by performing wordsense disambiguating (Agirre and Edmonds, 2006;Navigli, 2009) on all words.
However, as thisis an error-prone task and we are more inter-ested to showcase the overall principle, we rely onWikipedia as an already manually disambiguatedcorpus.
Wikipedia is a highly linked resource andarticles can be considered as senses.4We ex-tract all links from all articles, with the link tar-get as the term.
This approach is not restrictedto Wikipedia, but can be applied to any resourcecontaining connections between articles, such asWiktionary (Meyer and Gurevych, 2012b).
An-other reason to select Wikipedia as a corpus is thatit will allow us to directly compare similarity val-ues with the Wikipedia Link Measure as describedabove.After this more high-level introduction, we nowfocus on the mathematical foundation of ESA anddisambiguated ESA (called ESA on senses).
ESAand ESA on senses count the frequency of eachterm (or sense) in each document.
Table 1 showsthe corresponding term-document matrix for theexample in Figure 1.
The term Jaguar appears inall shown documents, but the term Zoo appears inthe articles Dublin Zoo and Wildlife Park.5A man-ual analysis shows that Jaguar appears with differ-ent senses in the articles D-pad6and Dublin Zoo.4Wikipedia also contains pages with a list of possiblesenses called disambiguation pages, which we filter.5In total it appears in 30 articles but we shown only fewexample articles.6A D-pad is a directional pad for playing computer games.31Articles TermsJaguar Gamepad Zoo# articles 3,496 30 7,553Dublin Zoo 1 0 25Wildlife Park 1 0 3D-pad 1 0 0Gamepad 4 1 0... ... ... ...Table 1: Term-document-matrix for frequencies ina corpus if words are used as termsArticles TermsAtariGamepadJaguarZooJaguar (animal)# articles 156 86 578 925Dublin Zoo 0 0 2 1Wildlife Park 0 0 1 1D-pad 1 1 0 0Gamepad 1 0 0 0... ... ... ... ...Table 2: Term-document-matrix for frequencies ina corpus if senses are used as termsBy comparing the vectors without any modifi-cation, we see that the word pairs Jaguar?Zooand Jaguar?Gamepad have vector entries for thesame document, thus leading to a non-zero simi-larity.
Vectors for the terms Gamepad and Zoo donot share any documents, thus leading to a simi-larity of zero.Shifting from words to senses changes term fre-quencies in the term-document-matrix in Table 2.The word Jaguar is split in the senses Atari Jaguarand Jaguar (animal).
Overall, the term-document-matrix for the sense-based similarity shows lowerfrequencies, usually zero or one because in mostcases one article does not link to another article orexactly once.
Both senses of Jaguar do not appearin the same document, hence, their vectors are or-thogonal.
The vector for the term Gamepad dif-fers from the vector for the same term in Table 1.This is due to two effects: (i) There is no link fromthe article Gamepad to itself, but the term is men-tioned in the article and (ii) there exists a link fromthe article D-pad to Gamepad, but using anotherterm.The term-document-matrices in Table 1 and 2show unmodified frequencies of the terms.
Whencomparing two vectors, both are normalized in aprior step.
Values can be normalized by the inverselogarithm of their document frequency.
Term fre-quencies can also be normalized by weightingthem with the inverse frequency of links pointingto an article (document or articles with many linkspointing to them receive lower weights as docu-ments with only few incoming links.)
We normal-ize vector values with the inverse logarithm of ar-ticle frequencies.Besides comparing two vectors by measuringthe angle between them (cosine), we also experi-ment with a language model variant.
In the lan-guage model variant we calculate for both vec-tors the ratio of links they both share.
The fi-nal similarity value is the average for both vec-tors.
This is somewhat similar to the approach ofWikipedia Link Measure by Milne (2007).
Bothrely on Wikipedia links and are based on frequen-cies of these links.
We show that?although, ESAand Link Measure seem to be very different?theyboth share a general idea and are identical with acertain configuration.2.2 Relation to the Wikipedia Link MeasureLink Measure counts the number of incominglinks to both articles and the number of sharedlinks.
In the originally presented formula by Milne(2007) the similarity is the cosine of vectors forincoming or outgoing links from both articles.
In-coming links are also shown in term-document-matrices in Table 1 and 2, thus providing the samevector information.
In Milne (2007), vector valuesare weighted by the frequency of each link normal-ized by the logarithmic inverse frequency of linkspointing to the target.
This is one of the earlier de-scribed normalization approaches.
Thus, we arguethat the Wikipedia Link Measure is a special caseof our more general ESA on senses approach.3 Annotation Study I: Rating SenseSimilarityWe argue that human judgment of similarity be-tween words is influenced by the most probablesense.
We create a dataset with ambiguous termsand ask annotators to rank the similarity of sensesand evaluate similarity measures with the noveldataset.3.1 Constructing an Ambiguous DatasetIn this section, we discuss how an evaluationdataset should be constructed in order to correctlyasses the similarity of two senses.
Typically, eval-uation datasets for word similarity are constructedby letting annotators rate the similarity between32both words without specifying any senses for thesewords.
It is common understanding that anno-tators judge the similarity of the combination ofsenses with the highest similarity.We investigate this hypothesis by constructinga new dataset consisting of 105 ambiguous wordpairs.
Word pairs are constructed by adding oneword with two clearly distinct senses and a secondword, which has a high similarity to only one ofthe senses.
We first ask two annotators7to rate theword pairs on a scale from 0 (not similar at all) to 4(almost identical).
In the second round, we ask thesame annotators to rate 277 sense8pairs for theseword pairs using the same scale.The final dataset thus consists of two levels:(i) word similarity ratings and (ii) sense similarityratings.
The gold ratings are the averaged ratingsof both annotators, resulting in an agreement9of.510 (Spearman: .598) for word ratings and .792(Spearman: .806) for sense ratings.Table 3 shows ratings of both annotators for twoword pairs and ratings for all sense combinations.In the given example, the word bass has the sensesof the fish, the instrument, and the sound.
Anno-tators compare the words and senses to the wordsFish and Horn, which appear only in one sense(most frequent sense) in the dataset.The annotators?
rankings contradict the assump-tion that the word similarity equals the similar-ity of the highest sense.
Instead, the highestsense similarity rating is higher than the wordsimilarity rating.
This may be caused?amongothers?by two effects: (i) the correct sense is notknown or not recalled, or (ii) the annotators (un-consciously) adjust their ratings to the probabil-ity of the sense.
Although, the annotation manualstated that Wikipedia (the source of the senses)could be used to get informed about senses andthat any sense for the words can be selected, wesee both effects in the annotators?
ratings.
Bothannotators rated the similarity between Bass andFish as very low (1 and 2).
However, when askedto rate the similarity between the sense Bass (Fish)and Fish, both annotators rated the similarity ashigh (4).
Accordingly, for the word pair Bass and7Annotators are near-native speakers of English and haveuniversity degrees in cultural anthropology and computer sci-ence.8The sense of a word is given in parentheses but annota-tors have access to Wikipedia to get information about thosesenses.9We report agreement as Krippendorf ?
with a quadraticweight function.Horn, word similarity is low (1) while the highestsense frequency is medium to high (3 and 4).3.2 Results & DiscussionWe evaluated similarity measures with the previ-ously created new dataset.
Table 4 shows corre-lations of similarity measures with human ratings.We divide the table into measures computing sim-ilarity on word level and on sense level.
ESAworks entirely on a word level, Lin (WordNet)uses WordNet as a sense inventory, which meansthat senses differ across sense inventories.10ESAon senses and Wikipedia Link Measure (WLM)compute similarity on a sense-level, however, sim-ilarity on a word-level is computed by taking themaximum similarity of all possible sense pairs.Results in Table 4 show that word-level mea-sures return the same rating independent from thesense being used, thus, they perform good whenevaluated on a word-level, but perform poorlyon a sense-level.
For the word pair Jaguar?Zoo, there exist two sense pairs Atari Jaguar?Zoo and Jaguar (animal)?Zoo.
Word-level mea-sures return the same similarity, thus leading toa very low correlation.
This was expected, asonly sense-based similarity measures can discrim-inate between different senses of the same word.Somewhat surprisingly, sense-level measures per-form also well on a word-level, but their per-formance increases strongly on sense-level.
Ournovel measure ESA on senses provides the bestresults.
This is expected as the ambiguous datasetcontains many infrequently used senses, which an-notators are not aware of.Our analysis shows that the algorithm for com-paring two vectors (i.e.
cosine and languagemodel) only influences results for ESA on senseswhen computed on a word-level.
Correlation forWikipedia Link Measure (WLM) differs depend-ing on whether the overlap of incoming or outgo-ing links are computed.
WLM on word-level usingincoming links performs better, while the differ-ence on sense-level evaluation is only marginal.Results show that an evaluation on the level ofwords and senses may influence performance ofmeasures strongly.3.3 Pair-wise EvaluationIn a second experiment, we evaluate how wellsense-based measures can decide, which one of10Although, there exists sense alignment resources, we didnot use any alignment.33Annotator 1 Annotator 2Word 1 Word 2 Sense 1 Sense 2 Words Senses Words SensesBass FishBass (Fish)Fish (Animal) 1414Bass (Instrument) 1 1Bass (Sound) 1 1Bass HornBass (Fish)Horn (Instrument) 2111Bass (Instrument) 3 4Bass (Sound) 3 3Table 3: Examples of ratings for two word pairs and all sense combinations with the highest ratingsmarked boldWord-level Sense-levelmeasure Spearman Pearson Spearman PearsonWord measuresESA .456 .239 -.001 .017Lin (WordNet) .298 .275 .038 .016Sense measuresESA on senses (Cosine) .292 .272 .642 .348ESA on senses (Lang.
Mod.)
.185 .256 .642 .482WLM (out) .190 .193 .537 .372WLM (in) .287 .279 .535 .395Table 4: Correlation of similarity measures with a human gold standard of ambiguous word pairs.two sense pairs for one word pair have a highersimilarity.
We thus create for every word pair allpossible sense pairs11and count cases where onemeasure correctly decides, which is the sense pairwith a higher similarity.Table 5 shows evaluation results based on aminimal difference between two sense pairs.
Weremoved all sense pairs with a lower differenceof their gold similarity.
Column #pairs gives thenumber of remaining sense pairs.
If a measureclassifies two sense pairs wrongly, it may eitherbe because it rated the sense pairs with an equalsimilarity or because it reversed the order.Results show that accuracy increases with in-creasing minimum difference between sense pairs.Figure 3 emphasizes this finding.
Overall, accu-racy for this task is high (between .70 and .83),which shows that all the measures can discrim-inate sense pairs.
WLM (out) performs best formost cases with a difference in accuracy of up to.06.When comparing these results to results fromTable 4, we see that correlation does not implyaccurate discrimination of sense pairs.
Although,ESA on senses has the highest correlation to hu-man ratings, it is outperformed by WLM (out) onthe task of discriminating two sense pairs.
We seethat results are not stable across both evaluation11For one word pair with two senses for one word, there aretwo possible sense pairs.
Three senses result in three sensepairs.0.5 1 1.5 2 2.5 3 3.5 40.70.750.80.850.9Min.
judgement differenceAccuracyESA on sensesWLM (in)WLM (out)Figure 3: Accuracy distribution depending onminimum difference of similarity ratingsscenarios, however, ESA on senses achieves thehighest correlation and performs similar to WLM(out) when comparing sense pairs pair-wise.4 Annotation Study II: Re-rating ofRG65We performed a second evaluation study where weasked three human annotators12to rate the similar-ity of word-level pairs in the dataset by Rubensteinand Goodenough (1965).
We hypothesize thatmeasures working on the sense-level should have adisadvantage on word-level annotated datasets dueto the effects described above that influence anno-tators towards frequent senses.
In our annotation12As before, all three annotators are near-native speakers ofEnglish and have a university degree in physics, engineering,and computer science.34Min.
Wrongdiff.
#pairs measure Correct Reverse Values equal Accuracy0.5420ESA on senses 296 44 80 .70WLM (in) 296 62 62 .70WLM (out) 310 76 34 .741.0390ESA on senses 286 38 66 .73WLM (in) 282 52 56 .72WLM (out) 294 64 32 .751.5360ESA on senses 264 34 62 .73WLM (in) 260 48 52 .72WLM (out) 280 54 26 .782.0308ESA on senses 232 28 48 .75WLM (in) 226 36 46 .73WLM (out) 244 46 18 .792.5280ESA on senses 216 22 42 .77WLM (in) 206 32 42 .74WLM (out) 224 38 18 .803.0174ESA on senses 134 10 30 .77WLM (in) 128 20 26 .74WLM (out) 136 22 16 .783.5068ESA on senses 56 4 8 .82WLM (in) 50 6 12 .74WLM (out) 52 6 10 .764.012ESA on senses 10 2 0 .83WLM (in) 10 2 0 .83WLM (out) 10 2 0 .83Table 5: Pair-wise comparison of measures: Results for ESA on senses (language model) and ESA onsenses (cosine) do not differstudies, our aim is to minimize the effect of senseweights.In previous annotation studies, human annota-tors could take sense weights into account whenjudging the similarity of word pairs.
Addition-ally, some senses might not be known by anno-tators and, thus receive a lower rating.
We min-imize these effects by asking annotators to selectthe best sense for a word based on a short summaryof the corresponding sense.
To mimic this pro-cess, we created an annotation tool (see Figure 4),for which an annotator first selects senses for bothwords, which have the highest similarity.
Then theannotator ranks the similarity of these sense pairsbased on the complete sense definition.A single word without any context cannot bedisambiguated properly.
However, when wordpairs are given, annotators first select senses basedon the second word, e.g.
if the word pair is Jaguarand Zoo, an annotator will select the wild animalfor Jaguar.
After disambiguating, an annotatorassigns a similarity score based on both selectedsenses.
To facilitate this process, a definition ofeach possible sense is shown.As in the previous experiment, similarity is an-notated on a five-point-scale from 0 to 4.
Al-though, we ask annotators to select senses forword pairs, we retrieve only one similarity ratingfor each word pair, which is the sense combinationwith the highest similarity.No sense inventory To compare our results withthe original dataset from Rubenstein and Goode-nough (1965), we asked annotators to rate similar-ity of word pairs without any given sense reposi-tory, i.e.
comparing words directly.
The annota-tors reached an agreement of .73.
The resultinggold standard has a high correlation with the orig-inal dataset (.923 Spearman and .938 Pearson).This is in line with our expectations and previouswork that similarity ratings are stable across time(B?ar et al., 2011).Wikipedia sense inventory We now use the fullfunctionality of our annotation tool and ask an-notators to first, select senses for each word andsecond, rate the similarity.
Possible senses anddefinitions for these senses are extracted fromWikipedia.13The same three annotators reached13We use the English Wikipedia version from June 15th,2010.35Figure 4: User interface for annotation studies: The example shows the word pair glass?tumbler withno senses selected.
The interface shows WordNet definitons of possible senses in the text field below thesense selection.
The highest similarity is selected as sense 4496872 for tumbler is a drinking glass.an agreement of .66.
The correlation to the orig-inal dataset is lower than for the re-rating (.881Spearman, .896 Pearson).
This effect is dueto many entities in Wikipedia, which annotatorswould typically not know.
Two annotators ratedthe word pair graveyard?madhouse with a ratherhigh similarity because both are names of musicbands (still no very high similarity because one isa rock and the other a jazz band).WordNet sense inventory Similar to the previ-ous experiment, we list possible senses for eachword from a sense inventory.
In this experiment,we use WordNet senses, thus, not using any namedentity.
The annotators reached an agreement of .73and the resulting gold standard has a high correla-tion with the original dataset (.917 Spearman and.928 Pearson).Figure 5 shows average annotator ratings incomparison to similarity judgments in the origi-nal dataset.
All re-rating studies follow the generaltendency of having higher annotator judgments forsimilar pairs.
However, there is a strong fluctua-tion in the mid-similarity area (1 to 3).
This is dueto fewer word pairs with such a similarity.4.1 Results & DiscussionWe evaluate the similarity measures using Spear-man and Pearson correlation with human similar-0 1 2 3 4024Original similaritySimilarityjudgementsNoneWikipediaWordNetFigure 5: Correlation curve of rerating studiesity judgments.
We calculate correlations to fourhuman judgments: (i) from the original dataset(Orig.
), (ii) from our re-rating study (Rerat.
), (iii)from our study with senses from Wikipedia (WP),and (iv) with senses from WordNet (WN).
Ta-ble 6 shows results for all described similaritymeasures.ESA14achieves a Spearman correlation of .751and a slightly higher correlation (.765) on ourre-rating gold standard.
Correlation then dropswhen compared to gold standards with sensesfrom Wikipedia and WordNet.
This is expectedas the gold standard becomes more sense-aware.Lin is based on senses in WordNet but still out-14ESA is used with normalized text frequencies, a constantdocument frequency, and a cosine comparison of vectors.36Spearman Pearsonmeasure Orig.
Rerat.
WP WN Orig.
Rerat.
WP WNESA .751 .765 .704 .705 .647 .694 .678 .625Lin .815 .768 .705 .775 .873 .840 .798 .846ESA on senses (lang.
mod.)
.733 .765 .782 .751 .703 .739 .739 .695ESA on senses (cosine) .775 .810 .826 .795 .694 .712 .736 .699WLM (in) .716 .745 .754 .733 .708 .712 .740 .707WLM (out) .583 .607 .652 .599 .548 .583 .613 .568Table 6: Correlation of similarity measures with a human gold standard on the word pairs by Rubensteinand Goodenough (1965).
Best results for each gold standard are marked bold.performs all other measures on the original goldstandard.
Correlation reaches a high value forthe gold standard based on WordNet, as the samesense inventory for human annotations and mea-sure is applied.
Values for Pearson correlation em-phasizes this effect: Lin reaches the maximum of.846 on the WordNet-based gold standard.Correspondingly, the similarity measures ESAon senses and WLM reach their maximum onthe Wikipedia-based gold standard.
As for theambiguous dataset in Section 3 ESA on sensesoutperforms both WLM variants.
Cosine vectorcomparison again outperforms the language modelvariant for Spearman correlation but impairs it interms of Pearson correlation.
As before WLM (in)outperforms WLM (out) across all datasets andboth correlation metrics.Is word similarity sense-dependent?
In gen-eral, sense-level similarity measures improvewhen evaluated with a sense-aware gold standard,while correlation with word-level similarity mea-sures decreases.
A further manual analysis showsthat sense-level measures perform good when rat-ing very similar word pairs.
This is very useful forapplications such as information retrieval where auser is only interested in very similar documents.Our evaluation thus shows that word similar-ity should not be considered without consideringthe effect of the used sense inventory.
The sameannotators rate word pairs differently if they canspecify senses explicitly (as seen in Table 3).
Cor-respondingly, results for similarity measures de-pend on which senses can be selected.
Wikipediacontains many entities, e.g.
music bands or ac-tors, while WordNet contains fine-grained sensesfor things (e.g.
narrow senses of glass as shown inFigure 4).
Using the same sense inventory as theone, which has been used in the annotation pro-cess, leads to a higher correlation.5 Related WorkThe work by Schwartz and Gomez (2011) is theclosest to our approach in terms of sense anno-tated datasets.
They compare several sense-levelsimilarity measures based on the WordNet taxon-omy on sense-annotated datasets.
For their ex-periments, annotators were asked to select sensesfor every word pair in three similarity datasets.Annotators were not asked to re-rate the similar-ity of the word pairs, or the sense pairs, respec-tively.
Instead, similarity judgments from the orig-inal datasets are used.
Possible senses are given byWordNet and the authors report an inter-annotatoragreement of .93 for the RG dataset.The authors then compare Spearman correlationbetween human judgments and judgments fromWordNet-based similarity measures.
They focuson differences between similarity measures usingthe sense annotations and the maximum value forall possible senses.
The authors do not report im-provements across all measures and datasets.
Often measures and three datasets, using sense an-notations, improved results in nine cases.
In 16cases, results are higher when using the maxi-mum similarity across all possible senses.
In fivecases, both measures yielded an equal correlation.The authors do not report any overall tendencyof results.
However, these experiments show thatswitching from words to senses has an effect onthe performance of similarity measures.The work by Hassan and Mihalcea (2011) isthe closest to our approach in terms of similaritymeasures.
They introduce Salient Semantic Anal-ysis (SAS), which is a sense-level measure basedon links and disambiguated senses in Wikipediaarticles.
They create a word-sense-matrix and37compute similarity with a modified cosine met-ric.
However, they apply additional normaliza-tion factors to optimize for the evaluation metricswhich makes a direct comparison of word-leveland sense-level variants difficult.Meyer and Gurevych (2012a) analyze verb sim-ilarity with a corpus from Yang and Powers(2006) based on the work by Zesch et al.
(2008).They apply variations of the similarity measureESA by Gabrilovich and Markovitch (2007) us-ing Wikipedia, Wiktionary, and WordNet.
Meyerand Gurevych (2012a) report improvements us-ing a disambiguated version of Wiktionary.
Linksin Wiktionary articles are disambiguated and thustransform the resource to a sense-based resource.In contrast to our work, they focus on the simi-larity of verbs (in comparison to nouns in this pa-per) and it applies disambiguation to improve theunderlying resource, while we switch the level,which is processed by the measure to senses.Shirakawa et al.
(2013) apply ESA for compu-tation of similarities between short texts.
Textsare extended with Wikipedia articles, which is onestep to a disambiguation of the input text.
Theyreport an improvement of the sense-extended ESAapproach over the original version of ESA.
In con-trast to our work, the text itself is not changed andsimilarity is computed on the level of texts.6 Summary and Future WorkIn this work, we investigated word-level andsense-level similarity measures and investigatedtheir strengths and shortcomings.
We evaluatedhow correlations of similarity measures with agold standard depend on the sense inventory usedby the annotators.We compared the similarity measures ESA(corpus-based), Lin (WordNet), and WikipediaLink Measure (Wikipedia), and a sense-enabledversion of ESA and evaluated them with a datasetcontaining ambiguous terms.
Word-level mea-sures were not able to differentiate between dif-ferent senses of one word, while sense-level mea-sures could even increase correlation when shift-ing to sense similarities.
Sense-level measures ob-tained accuracies between .70 and .83 when decid-ing which of two sense pairs has a higher similar-ity.We performed re-rating studies with three an-notators based on the dataset by Rubenstein andGoodenough (1965).
Annotators were asked tofirst annotate senses from Wikipedia and Word-Net for word pairs and then judge their similar-ity based on the selected senses.
We evaluatedwith these new human gold standards and foundthat correlation heavily depends on the resourceused by the similarity measure and sense reposi-tory a human annotator selected.
Sense-level sim-ilarity measures improve when evaluated with asense-aware gold standard, while correlation withword-level similarity measures decreases.
Usingthe same sense inventory as the one, which hasbeen used in the annotation process, leads to ahigher correlation.
This has implications for cre-ating word similarity datasets and evaluating sim-ilarity measures using different sense inventories.In future work we would like to analyze howwe can improve sense-level similarity measures bydisambiguating a large document collection andthus retrieving more accurate frequency values.This might reduce the sparsity of term-document-matrices for ESA on senses.
We plan to useword sense disambiguation components as a pre-processing step to evaluate whether sense simi-larity measures improve results for text similarity.Additionally, we plan to use sense alignments be-tween WordNet and Wikipedia to enrich the term-document matrix with additional links based onsemantic relations.The datasets, annotation guidelines, and our ex-perimental framework are publicly available in or-der to foster future research for computing sensesimilarity.15AcknowledgmentsThis work has been supported by the Volk-swagen Foundation as part of the Lichtenberg-Professorship Program under grant No.
I/82806,by the Klaus Tschira Foundation under project No.00.133.2008, and by the German Federal Min-istry of Education and Research (BMBF) withinthe context of the Software Campus project openwindow under grant No.
01IS12054.
The au-thors assume responsibility for the content.
Wethank Pedro Santos, Mich`ele Spankus and MarkusB?ucker for their valuable contribution.
We thankthe anonymous reviewers for their helpful com-ments.15www.ukp.tu-darmstadt.de/data/text-similarity/sense-similarity/38ReferencesEneko Agirre and Philip Edmonds.
2006.
WordSense Disambiguation: Algorithms and Applica-tions.
Springer.Satanjeev Banerjee and Ted Pedersen.
2002.
AnAdapted Lesk Algorithm for Word Sense Disam-biguation using WordNet.
In Computational Lin-guistics and Intelligent Text, pages 136?-145.Daniel B?ar, Torsten Zesch, and Iryna Gurevych.
2011.A Reflective View on Text Similarity.
In Proceed-ings of the International Conference on Recent Ad-vances in Natural Language Processing, pages 515?520, Hissar, Bulgaria.Alexander Budanitsky and Graeme Hirst.
2006.
Eval-uating WordNet-based Measures of Lexical Se-mantic Relatedness.
Computational Linguistics,32(1):13?47.Evgeniy Gabrilovich and Shaul Markovitch.
2007.Computing Semantic Relatedness using Wikipedia-based Explicit Semantic Analysis.
In Proceedings ofthe 20th International Joint Conference on ArtificalIntelligence, pages 1606?1611.Gregory Grefenstette.
1992.
Sextant: Exploring Unex-plored Contexts for Semantic Extraction from Syn-tactic Analysis.
In Proceedings of the 30th An-nual Meeting of the Association for ComputationalLinguistics, pages 324?-326, Newark, Delaware,USA.
Association for Computational Linguistics.Samer Hassan and Rada Mihalcea.
2011.
SemanticRelatedness Using Salient Semantic Analysis.
InProceedings of the 25th AAAI Conference on Artifi-cial Intelligence, (AAAI 2011), pages 884?889, SanFrancisco, CA, USA.Jay J Jiang and David W Conrath.
1997.
Seman-tic Similarity based on Corpus Statistics and Lexi-cal Taxonomy.
In Proceedings of 10th InternationalConference Research on Computational Linguistics,pages 1?15.Dekang Lin.
1998.
An Information-theoretic Defini-tion of Similarity.
In In Proceedings of the Interna-tional Conference on Machine Learning, volume 98,pages 296?-304.Christian M. Meyer and Iryna Gurevych.
2012a.
ToExhibit is not to Loiter: A Multilingual, Sense-Disambiguated Wiktionary for Measuring Verb Sim-ilarity.
In Proceedings of the 24th InternationalConference on Computational Linguistics, pages1763?1780, Mumbai, India.Christian M. Meyer and Iryna Gurevych.
2012b.
Wik-tionary: A new rival for expert-built lexicons?
Ex-ploring the possibilities of collaborative lexicogra-phy.
In Sylviane Granger and Magali Paquot, ed-itors, Electronic Lexicography, chapter 13, pages259?291.
Oxford University Press, Oxford, UK,November.David Milne and Ian H Witten.
2008.
Learning to Linkwith Wikipedia.
In Proceedings of the 17th ACMConference on Information and Knowledge Man-agement, pages 509?-518.David Milne.
2007.
Computing Semantic Relatednessusing Wikipedia Link Structure.
In Proceedings ofthe New Zealand Computer Science Research Stu-dent Conference.Roberto Navigli.
2009.
Word Sense Disambiguation:A Survey.
ACM Computing Surveys, 41(2):1?69.Philip Resnik.
1999.
Semantic Similarity in a Tax-onomy: An Information-based Measure and its Ap-plication to Problems of Ambiguity in Natural Lan-guage.
Journal of Artificial Intelligence Research,11:95?130.Herbert Rubenstein and John B Goodenough.
1965.Contextual Correlates of Synonymy.
Communica-tions of the ACM, 8(10):627?-633.Hansen A Schwartz and Fernando Gomez.
2011.
Eval-uating Semantic Metrics on Tasks of Concept Simi-larity.
In FLAIRS Conference.Nuno Seco, Tony Veale, and Jer Hayes.
2004.
AnIntrinsic Information Content Metric for SemanticSimilarity in WordNet.
In Proceedings of EuropeanConference for Artificial Intelligence, number Ic,pages 1089?1093.Masumi Shirakawa, Kotaro Nakayama, Takahiro Hara,and Shojiro Nishio.
2013.
Probabilistic Seman-tic Similarity Measurements for Noisy Short Textsusing Wikipedia Entities.
In Proceedings of the22nd ACM International Conference on Information& Knowledge Management, pages 903?908, NewYork, New York, USA.
ACM Press.Dongqiang Yang and David MW Powers.
2006.
VerbSimilarity on the Taxonomy of WordNet.
In Pro-ceedings of GWC-06, pages 121?-128.Torsten Zesch, Christof M?uller, and Iryna Gurevych.2008.
Using Wiktionary for Computing SemanticRelatedness.
In Proceedings of the Twenty-ThirdAAAI Conference on Artificial Intelligence, pages861?867, Chicago, IL, USA.39
