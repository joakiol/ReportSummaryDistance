Coling 2010: Poster Volume, pages 108?116,Beijing, August 2010Benchmarking of Statistical Dependency Parsers for FrenchMarie Candito!, Joakim Nivre!, Pascal Denis!
and Enrique Henestroza Anguiano!!
Alpage (Universit?
Paris 7/INRIA)!
Uppsala University, Department of Linguistics and Philologymarie.candito@linguist.jussieu.fr {pascal.denis, henestro}@inria.fr joakim.nivre@ling?l.uu.seAbstractWe compare the performance of threestatistical parsing architectures on theproblem of deriving typed dependencystructures for French.
The architecturesare based on PCFGs with latent vari-ables, graph-based dependency parsingand transition-based dependency parsing,respectively.
We also study the in?u-ence of three types of lexical informa-tion: lemmas, morphological features,and word clusters.
The results show thatall three systems achieve competitive per-formance, with a best labeled attachmentscore over 88%.
All three parsers bene?tfrom the use of automatically derived lem-mas, while morphological features seemto be less important.
Word clusters have apositive effect primarily on the latent vari-able parser.1 IntroductionIn this paper, we compare three statistical parsersthat produce typed dependencies for French.
Asyntactic analysis in terms of typed grammaticalrelations, whether encoded as functional annota-tions in syntagmatic trees or in labeled depen-dency trees, appears to be useful for many NLPtasks including question answering, informationextraction, and lexical acquisition tasks like collo-cation extraction.This usefulness holds particularly for French,a language for which bare syntagmatic treesare often syntactically underspeci?ed becauseof a rather free order of post-verbal comple-ments/adjuncts and the possibility of subject in-version.
Thus, the annotation scheme of theFrench Treebank (Abeill?
and Barrier, 2004)makes use of ?at syntagmatic trees without VPnodes, with no structural distinction betweencomplements, adjuncts or post-verbal subjects,but with additional functional annotations on de-pendents of verbs.Parsing is commonly enhanced by using moreabstract lexical information, in the form of mor-phological features (Tsarfaty, 2006), lemmas(Seddah et al, 2010), or various forms of clusters(see (Candito and Seddah, 2010) for references).In this paper, we explore the integration of mor-phological features, lemmas, and linear contextclusters.Typed dependencies can be derived using manydifferent parsing architectures.
As far as statisticalapproaches are concerned, the dominant paradigmfor English has been to use constituency-basedparsers, the output of which can be convertedto typed dependencies using well-proven conver-sion procedures, as in the Stanford parser (Kleinand Manning, 2003).
In recent years, it hasalso become popular to use statistical dependencyparsers, which are trained directly on labeled de-pendency trees and output such trees directly, suchas MSTParser (McDonald, 2006) and MaltParser(Nivre et al, 2006).
Dependency parsing has beenapplied to a fairly broad range of languages, espe-cially in the CoNLL shared tasks in 2006 and 2007(Buchholz and Marsi, 2006; Nivre et al, 2007).We present a comparison of three statisticalparsing architectures that output typed dependen-cies for French: one constituency-based architec-ture featuring the Berkeley parser (Petrov et al,2006), and two dependency-based systems usingradically different parsing methods, MSTParser(McDonald et al, 2006) and MaltParser (Nivre etal., 2006).
These three systems are compared bothin terms of parsing accuracy and parsing times, inrealistic settings that only use predicted informa-tion.
By using freely available software packagesthat implement language-independent approaches108and applying them to a language different fromEnglish, we also hope to shed some light on thecapacity of different methods to cope with thechallenges posed by different languages.Comparative evaluation of constituency-basedand dependency-based parsers with respect to la-beled accuracy is rare, despite the fact that parserevaluation on typed dependencies has been ad-vocated for a long time (Lin, 1995; Carroll etal., 1998).
Early work on statistical dependencyparsing often compared constituency-based anddependency-based methods with respect to theirunlabeled accuracy (Yamada and Matsumoto,2003), but comparison of different approacheswith respect to labeled accuracy is more recent.Cer et al (2010) present a thorough analysis ofthe best trade-off between speed and accuracy inderiving Stanford typed dependencies for English(de Marneffe et al, 2006), comparing a number ofconstituency-based and dependency-based parserson data from the Wall Street Journal.
They con-clude that the highest accuracy is obtained usingconstituency-based parsers, although some of thedependency-based parsers are more ef?cient.For German, the 2008 ACL workshop on pars-ing German (K?bler, 2008) featured a shared taskwith two different tracks, one for constituency-based parsing and one for dependency-based pars-ing.
Both tracks had their own evaluation metrics,but the accuracy with which parsers identi?edsubjects, direct objects and indirect objects wascompared across the two tracks, and the resultsin this case showed an advantage for dependency-based parsing.In this paper, we contribute results for athird language, French, by benchmarking bothconstituency-based and dependency-based meth-ods for deriving typed dependencies.
In addi-tion, we investigate the usefulness of morphologi-cal features, lemmas and word clusters for each ofthe different parsing architectures.
The rest of thepaper is structured as follows.
Section 2 describesthe French Treebank, and Section 3 describes thethree parsing systems.
Section 4 presents the ex-perimental evaluation, and Section 5 contains acomparative error analysis of the three systems.Section 6 concludes with suggestions for futureresearch.2 TreebanksFor training and testing the statistical parsers, weuse treebanks that are automatically convertedfrom the French Treebank (Abeill?
and Barrier,2004) (hereafter FTB), a constituency-based tree-bank made up of 12, 531 sentences from the LeMonde newspaper.
Each sentence is annotatedwith a constituent structure and words bear thefollowing features: gender, number, mood, tense,person, de?niteness, wh-feature, and clitic case.Nodes representing dependents of a verb are la-beled with one of 8 grammatical functions.1We use two treebanks automatically obtainedfrom FTB, both described in Candito et al(2010).
FTB-UC is a modi?ed version of theoriginal constituency-based treebank, where therich morphological annotation has been mappedto a simple tagset of 28 part-of-speech tags, andwhere compounds with regular syntax are bro-ken down into phrases containing several simplewords while remaining sequences annotated ascompounds in FTB are merged into a single token.Function labels are appended to syntactic categorysymbols and are either used or ignored, dependingon the task.FTB-UC-DEP is a dependency treebank de-rived from FTB-UC using the classic technique ofhead propagation rules, ?rst proposed for Englishby Magerman (1995).
Function labels that arepresent in the original treebank serve to label thecorresponding dependencies.
The remaining un-labeled dependencies are labeled using heuristics(for dependents of non-verbal heads).
With thisconversion technique, output dependency trees arenecessarily projective, and extracted dependen-cies are necessarily local to a phrase, which meansthat the automatically converted trees can be re-garded as pseudo-projective approximations to thecorrect dependency trees (Kahane et al, 1998).Candito et al (2010) evaluated the converted treesfor 120 sentences, and report a 98% labeled at-tachment score when comparing the automaticallyconverted dependency trees to the manually cor-rected ones.1These are SUJ (subject), OBJ (object), A-OBJ/DE-OBJ(indirect object with preposition ?
/ de), P-OBJ (indirectobject with another preposition / locatives), MOD (modi?er),ATS/ATO (subject/object predicative complement).109SNP-SUJDETuneNClettreVNVavaitVPP?t?VPPenvoy?eNP-MODDETlaNCsemaineADJderni?rePP-A_OBJP+DauxNPNCsalari?sune lettre avait ?t?
envoy?e la semaine derni?re aux salari?sdetsujaux-tpsaux-pass moddet moda_objobjFigure 1: An example of constituency tree of the FTB-UC (left), and the corresponding dependency tree(right) for A letter had been sent the week before to the employees.Figure 1 shows two parallel trees from FTB-UCand FTB-UC-DEP.
In all reported experiments inthis paper, we use the usual split of FTB-UC: ?rst10% as test set, next 10% as dev set, and the re-maining sentences as training set.3 ParsersAlthough all three parsers compared are statis-tical, they are based on fairly different parsingmethodologies.
The Berkeley parser (Petrov etal., 2006) is a latent-variable PCFG parser, MST-Parser (McDonald et al, 2006) is a graph-baseddependency parser, and MaltParser (Nivre et al,2006) is a transition-based dependency parser.The choice to include two different dependencyparsers but only one constituency-based parser ismotivated by the study of Seddah et al (2009),where a number of constituency-based statisti-cal parsers were evaluated on French, includingDan Bikel?s implementation of the Collins parser(Bikel, 2002) and the Charniak parser (Charniak,2000).
The evaluation showed that the Berke-ley parser had signi?cantly better performance forFrench than the other parsers, whether measuredusing a parseval-style labeled bracketing F-scoreor a CoNLL-style unlabeled attachment score.Contrary to most of the other parsers in that study,the Berkeley parser has the advantage of a strictseparation of parsing model and linguistic con-straints: linguistic information is encoded in thetreebank only, except for a language-dependentsuf?x list used for handling unknown words.In this study, we compare the Berkeley parserto MSTParser and MaltParser, which have thesame separation of parsing model and linguisticrepresentation, but which are trained directly onlabeled dependency trees.
The two dependencyparsers use radically different parsing approachesbut have achieved very similar performance for awide range of languages (McDonald and Nivre,2007).
We describe below the three architecturesin more detail.23.1 The Berkeley ParserThe Berkeley parser is a freely available imple-mentation of the statistical training and parsingalgorithms described in (Petrov et al, 2006) and(Petrov and Klein, 2007).
It exploits the fact thatPCFG learning can be improved by splitting sym-bols according to structural and/or lexical proper-ties (Klein and Manning, 2003).
Following Mat-suzaki et al (2005), the Berkeley learning algo-rithm uses EM to estimate probabilities on sym-bols that are automatically augmented with la-tent annotations, a process that can be viewedas symbol splitting.
Petrov et al (2006) pro-posed to score the splits in order to retain only themost bene?cial ones, and keep the grammar sizemanageable: the splits that induce the smallestlosses in the likelihood of the treebank are mergedback.
The algorithm starts with a very generaltreebank-induced binarized PCFG, with order hhorizontal markovisation.
created, where at eachlevel a symbol appears without track of its orig-inal siblings.
Then the Berkeley algorithm per-forms split/merge/smooth cycles that iterativelyre?ne the binarized grammar: it adds two latentannotations on each symbol, learns probabilitiesfor the re?ned grammar, merges back 50% of thesplits, and smoothes the ?nal probabilities to pre-vent over?tting.
All our experiments are run us-ing BerkeleyParser 1.0,3 modi?ed for handling2For replicability, models, preprocessing tools and ex-perimental settings are available at http://alpage.inria.fr/statgram/frdep.html.3http://www.eecs.berkeley.edu/\~petrov/berkeleyParser110French unknown words by Crabb?
and Candito(2008), with otherwise default settings (order 0horizontal markovisation, order 1 vertical marko-visation, 5 split/merge cycles).The Berkeley parser could in principle betrained on functionally annotated phrase-structuretrees (as shown in the left half of ?gure 1), butCrabb?
and Candito (2008) have shown that thisleads to very low performance, because the split-ting of symbols according to grammatical func-tions renders the data too sparse.
Therefore, theBerkeley parser was trained on FTB-UC withoutfunctional annotation.
Labeled dependency treeswere then derived from the phrase-structure treesoutput by the parser in two steps: (1) function la-bels are assigned to phrase structure nodes thathave functional annotation in the FTB scheme;and (2) dependency trees are produced using thesame procedure used to produce the pseudo-golddependency treebank from the FTB (cf.
Section 2).The functional labeling relies on the MaximumEntropy labeler described in Candito et al (2010),which encodes the problem of functional label-ing as a multiclass classi?cation problem.
Specif-ically, each class is of the eight grammatical func-tions used in FTB, and each head-dependent pairis treated as an independent event.
The featureset used in the labeler attempt to capture bilexi-cal dependencies between the head and the depen-dent (using stemmed word forms, parts of speech,etc.)
as well as more global sentence propertieslike mood, voice and inversion.3.2 MSTParserMSTParser is a freely available implementationof the parsing models described in McDonald(2006).
These models are often described asgraph-based because they reduce the problemof parsing a sentence to the problem of ?ndinga directed maximum spanning tree in a densegraph representation of the sentence.
Graph-basedparsers typically use global training algorithms,where the goal is to learn to score correct treeshigher than incorrect trees.
At parsing time aglobal search is run to ?nd the highest scoringdependency tree.
However, unrestricted globalinference for graph-based dependency parsingis NP-hard, and graph-based parsers like MST-Parser therefore limit the scope of their featuresto a small number of adjacent arcs (usually two)and/or resort to approximate inference (McDon-ald and Pereira, 2006).
For our experiments, weuse MSTParser 0.4.3b4 with 1-best projective de-coding, using the algorithm of Eisner (1996), andsecond order features.
The labeling of dependen-cies is performed as a separate sequence classi?-cation step, following McDonald et al (2006).To provide part-of-speech tags to MSTParser,we use the MElt tagger (Denis and Sagot, 2009),a Maximum Entropy Markov Model tagger en-riched with information from a large-scale dictio-nary.5 The tagger was trained on the training setto provide POS tags for the dev and test sets, andwe used 10-way jackkni?ng to generate tags forthe training set.3.3 MaltParserMaltParser6 is a freely available implementationof the parsing models described in (Nivre, 2006)and (Nivre, 2008).
These models are often char-acterized as transition-based, because they reducethe problem of parsing a sentence to the prob-lem of ?nding an optimal path through an abstracttransition system, or state machine.
This is some-times equated with shift-reduce parsing, but infact includes a much broader range of transitionsystems (Nivre, 2008).
Transition-based parserslearn models that predict the next state given thecurrent state of the system, including features overthe history of parsing decisions and the input sen-tence.
At parsing time, the parser starts in an ini-tial state and greedily moves to subsequent states?
based on the predictions of the model ?
until aterminal state is reached.
The greedy, determinis-tic parsing strategy results in highly ef?cient pars-ing, with run-times often linear in sentence length,and also facilitates the use of arbitrary non-localfeatures, since the partially built dependency treeis ?xed in any given state.
However, greedy in-ference can also lead to error propagation if earlypredictions place the parser in incorrect states.
Forthe experiments in this paper, we use MaltParser4http://mstparser.sourceforge.net5Denis and Sagot (2009) report a tagging accuracy of97.7% (90.1% on unknown words) on the FTB-UC test set.6http://www.maltparser.org1111.3.1 with the arc-eager algorithm (Nivre, 2008)and use linear classi?ers from the LIBLINEARpackage (Fan et al, 2008) to predict the next statetransitions.
As for MST, we used the MElt taggerto provide input part-of-speech tags to the parser.4 ExperimentsThis section presents the parsing experiments thatwere carried out in order to assess the state of theart in labeled dependency parsing for French andat the same time investigate the impact of differenttypes of lexical information on parsing accuracy.We present the features given to the parsers, dis-cuss how they were extracted/computed and inte-grated within each parsing architecture, and thensummarize the performance scores for the differ-ent parsers and feature con?gurations.4.1 Experimental SpaceOur experiments focus on three types of lexicalfeatures that are used either in addition to or assubstitutes for word forms: morphological fea-tures, lemmas, and word clusters.
In the caseof MaltParser and MSTParser, these features areused in conjunction with POS tags.
Motivationsfor these features are rooted in the fact that Frenchhas a rather rich in?ectional morphology.The intuition behind using morphological fea-tures like tense, mood, gender, number, and per-son is that some of these are likely to provide ad-ditional cues for syntactic attachment or functiontype.
This is especially true given that the 29 tagsused by the MElt tagger are rather coarse-grained.The use of lemmas and word clusters, on theother hand, is motivated by data sparseness con-siderations: these provide various degrees of gen-eralization over word forms.
As suggested by Kooet al (2008), the use of word clusters may also re-duce the need for annotated data.All our features are automatically produced:no features except word forms originate from thetreebank.
Our aim was to assess the performancecurrently available for French in a realistic setting.Lemmas Lemmatized forms are extracted us-ing Lefff (Sagot, 2010), a large-coverage morpho-syntactic lexicon for French, and a set of heuristicsfor unknown words.
More speci?cally, Lefff isqueried for each (word, pos), where pos is thetag predicted by the MElt tagger.
If the pair isfound, we use the longest lemma associated withit in Lefff.
Otherwise, we rely on a set of simplestemming heuristics using the form and the pre-dicted tag to produce the lemma.
We use the formitself for all other remaining cases.7Morphological Features Morphological fea-tures were extracted in a way similar to lemmas,again by querying Lefff and relying on heuristicsfor out-of-dictionary words.
Here are the mainmorphological attributes that were extracted fromthe lexicon: mood and tense for verbs; personfor verbs and pronouns; number and gender fornouns, past participles, adjectives and pronouns;whether an adverb is negative; whether an adjec-tive, pronoun or determiner is cardinal, ordinal,de?nite, possessive or relative.
Our goal was topredict all attributes found in FTB that are recov-erable from the word form alone.Word Form Clusters Koo et al (2008) haveproposed to use unsupervised word clusters asfeatures in MSTParser, for parsing English andCzech.
Candito and Crabb?
(2009) showed that,for parsing French with the Berkeley parser, us-ing the same kind of clusters as substitutes forword forms improves performance.
We now ex-tend their work by comparing the impact of suchclusters on two additional parsers.We use the word clusters computed by Can-dito and Crabb?
(2009) using Percy Liang?s im-plementation8 of the Brown unsupervised cluster-ing algorithm (Brown et al, 1992).
It is a bottom-up hierarchical clustering algorithm that uses a bi-gram language model over clusters.
The result-ing cluster ids are bit-strings, and various lev-els of granularity can be obtained by retainingonly the ?rst x bits.
Candito and Crabb?
(2009)used the L?Est R?publicain corpus, a 125 mil-lion word journalistic corpus.9 To reduce lexi-7Candito and Seddah (2010) report the following cover-age for the Lefff : around 96% of the tokens, and 80.1% ofthe token types are present in the Lefff (leaving out punctua-tion and numeric tokens, and ignoring case differences).8http://www.eecs.berkeley.edu/~pliang/software9http://www.cnrtl.fr/corpus/estrepublicain112cal data sparseness caused by in?ection, they rana lexicon-based stemming process on the corpusthat removes in?ection marks without adding orremoving lexical ambiguity.
The Brown algo-rithm was then used to compute 1000 clusters ofstemmed forms, limited to forms that appeared atleast 20 times.We tested the use of clusters with different val-ues for two parameters: nbbits = the cluster pre-?x length in bits, to test varying granularities, andminocc = the minimum number of occurrences inthe L?Est R?publicain corpus for a form to be re-placed by a cluster or for a cluster feature to beused for that form.4.2 Parser-Specific ConfigurationsSince the three parsers are based on different ma-chine learning algorithms and parsing algorithms(with different memory requirements and parsingtimes), we cannot integrate the different featuresdescribed above in exactly the same way.
For theBerkeley parser we use the setup of Candito andSeddah (2010), where additional information isencoded within symbols that are used as substi-tutes for word forms.
For MaltParser and MST-Parser, which are based on discriminative modelsthat permit the inclusion of interdependent fea-tures, additional information may be used eitherin addition to or as substitutes for word forms.Below we summarize the con?gurations that havebeen explored for each parser:?
Berkeley:1.
Morphological features: N/A.2.
Lemmas: Concatenated with POS tagsand substituted for word forms.3.
Clusters: Concatenated with morpho-logical suf?xes and substituted for wordforms; grid search for optimal values ofnbbits and minocc.?
MaltParser and MSTParser:1.
Morphological features: Added asfeatures.2.
Lemmas: Substituted for word formsor added as features.3.
Clusters: Substituted for word forms oradded as features; grid search for opti-mal values of nbbits and minocc.4.3 ResultsTable 1 summarizes the experimental results.
Foreach parser we give results on the developmentset for the baseline (no additional features), thebest con?guration for each individual feature type,and the best con?guration for any allowed combi-nation of the three features types.
For the ?naltest set, we only evaluate the baseline and the bestcombination of features.
Scores on the test setwere compared using a ?2-test to assess statisti-cal signi?cance: unless speci?ed, all differencestherein were signi?cant at p ?
0.01.The MSTParser system achieves the best la-beled accuracy on both the development set andthe test set.
When adding lemmas, the best con-?guration is to use them as substitutes for wordforms, which slightly improves the UAS results.For the clusters, their use as substitutes for wordforms tends to degrade results, whereas usingthem as features alone has almost no impact.
Thismeans that we could not replicate the positive ef-fect10 reported by Koo et al (2008) for Englishand Czech.
However, the best combined con-?guration is obtained using lemmas instead ofwords, a reduced set of morphological features,11and clusters as features, with minocc=50, 000 andnbbits=10.MaltParser has the second best labeled accu-racy on both the development set and the test set,although the difference with Berkeley is not sig-ni?cant on the latter.
MaltParser has the lowestunlabeled accuracy of all three parsers on bothdatasets.
As opposed to MSTParser, all three fea-ture types work best for MaltParser when used inaddition to word forms, although the improvementis statistically signi?cant only for lemmas andclusters.
Again, the best model uses all three typesof features, with cluster features minocc=600 andnbbits=7.
MaltParser shows the smallest discrep-ancy from unlabeled to labeled scores.
This mightbe because it is the only architecture where label-ing is directly done as part of parsing.10Note that the two experiments cannot be directly com-pared.
Koo et al (2008) use their own implementation of anMST parser, which includes extra second-order features (e.g.grand-parent features on top of sibling features).11As MSTParser training is memory-intensive, we re-moved the features containing information already encodedpart-of-speech tags.113Development Set Test SetBaseline Morpho Lemma Cluster Best Baseline BestParser LAS UAS LAS UAS LAS UAS LAS UAS LAS UAS LAS UAS LAS UASBerkeley 85.1 89.3 ?
?
85.9 90.0 86.5 90.8 86.5 90.8 85.6 89.6 86.8 91.0MSTParser 87.2 90.0 87.2 90.2 87.2 90.1 87.2 90.1 87.5 90.3 87.6 90.3 88.2 90.9MaltParser 86.2 89.0 86.3 89.0 86.6 89.2 86.5 89.2 86.9 89.4 86.7 89.3 87.3 89.7Table 1: Experimental results for the three parsing systems.
LAS=labeled accuracy, UAS=unlabeled accuracy, for sentencesof any length, ignoring punctuation tokens.
Morpho/Lemma/Cluster=best con?guration when using morphological featuresonly (resp.
lemmas only, clusters only), Best=best con?guration using any combination of these.For Berkeley, the lemmas improve the resultsover the baseline, and its performance reaches thatof MSTParser for unlabeled accuracy (althoughthe difference between the two parsers is not sig-ni?cant on the test set).
The best setting is ob-tained with clusters instead of word forms, usingthe full bit strings.
It also gives the best unlabeledaccuracy of all three systems on both the devel-opment set and the test set.
For the more impor-tant labeled accuracy, the point-wise labeler usedis not effective enough.Overall, MSTParser has the highest labeled ac-curacy and Berkeley the highest unlabeled ac-curacy.
However, results for all three systemson the test set are roughly within one percent-age point for both labeled and unlabeled ac-curacy, which means that we do not ?nd thesame discrepancy between constituency-basedand dependency-based parser that was reportedfor English by Cer et al (2010).Table 2 gives parsing times for the best con?g-uration of each parsing architecture.
MaltParserruns approximately 9 times faster than the Berke-ley system, and 10 times faster than MSTParser.The difference in ef?ciency is mainly due to thefact that MaltParser uses a linear-time parsing al-gorithm, while the other two parsers have cubictime complexity.
Given the rather small differ-ence in labeled accuracy, MaltParser seems to bea good choice for processing very large corpora.5 Error AnalysisWe provide a brief analysis of the errors made bythe best performing models for Berkeley, MST-Parser and MaltParser on the development set, fo-cusing on labeled and unlabeled attachment fornouns, prepositions and verbs.
For nouns, Berke-Bky Malt MSTTagging _ 0:27 0:27Parsing 12:19 0:58 (0:18) 14:12 (12:44)Func.
Lab.
0:23 _ _Dep.
Conv.
0:4 _ _Total 12:46 1:25 14:39Table 2: Parsing times (min:sec) for the dev set, for thethree architectures, on an imac 2.66GHz.
The ?gures withinbrackets show the pure parsing time without the model load-ing time, when available.ley has the best unlabeled attachment, followed byMSTParser and then MaltParser, while for labeledattachment Berkeley and MSTParser are on a parwith MaltParser a bit behind.
For prepositions,MSTParser is by far the best for both labeled andunlabeled attachment, with Berkeley and Malt-Parser performing equally well on unlabeled at-tachment and MaltParser performing better thanBerkeley on labeled attachment.12 For verbs,Berkeley has the best performance on both labeledand unlabeled attachment, with MSTParser andMaltParser performing about equally well.
Al-though Berkeley has the best unlabeled attach-ment overall, it also has the worst labeled attach-ment, and we found that this is largely due to thefunctional role labeler having trouble assigningthe correct label when the dependent is a prepo-sition or a clitic.For errors in attachment as a function of worddistance, we ?nd that precision and recall on de-pendencies of length > 2 tend to degrade fasterfor MaltParser than for MSTParser and Berkeley,12In the dev set, for MSTParser, 29% of the tokens thatdo not receive the correct governor are prepositions (883 outof 3051 errors), while these represent 34% for Berkeley (992out of 2914), and 30% for MaltParser (1016 out of 3340).114with Berkeley being the most robust for depen-dencies of length > 6.
In addition, Berkeley isbest at ?nding the correct root of sentences, whileMaltParser often predicts more than one root for agiven sentence.
The behavior of MSTParser andMaltParser in this respect is consistent with the re-sults of McDonald and Nivre (2007).6 ConclusionWe have evaluated three statistical parsing ar-chitectures for deriving typed dependencies forFrench.
The best result obtained is a labeled at-tachment score of 88.2%, which is roughly on apar with the best performance reported by Cer etal.
(2010) for parsing English to Stanford depen-dencies.
Note two important differences betweentheir results and ours: First, the Stanford depen-dencies are in a way deeper than the surface de-pendencies tested in our work.
Secondly, we ?ndthat for French there is no consistent trend fa-voring either constituency-based or dependency-based methods, since they achieve comparable re-sults both for labeled and unlabeled dependencies.Indeed, the differences between parsing archi-tectures are generally small.
The best perfor-mance is achieved using MSTParser, enhancedwith predicted part-of-speech tags, lemmas, mor-phological features, and unsupervised clusters ofword forms.
MaltParser achieves slightly lowerlabeled accuracy, but is probably the best optionif speed is crucial.
The Berkeley parser has highaccuracy for unlabeled dependencies, but the cur-rent labeling method does not achieve a compara-bly high labeled accuracy.Examining the use of lexical features, we ?ndthat predicted lemmas are useful in all three ar-chitectures, while morphological features have amarginal effect on the two dependency parsers(they are not used by the Berkeley parser).
Unsu-pervised word clusters, ?nally, give a signi?cantimprovement for the Berkeley parser, but have arather small effect for the dependency parsers.Other results for statistical dependency pars-ing of French include the pilot study of Canditoet al (2010), and the work ofSchluter and vanGenabith (2009), which resulted in an LFG sta-tistical French parser.
However, the latter?s re-sults are obtained on a modi?ed subset of the FTB,and are expressed in terms of F-score on LFG f-structure features, which are not comparable toour attachment scores.
There also exist a num-ber of grammar-based parsers, evaluated on goldtest sets annotated with chunks and dependen-cies (Paroubek et al, 2005; de la Clergerie et al,2008).
Their annotation scheme is different fromthat of the FTB, but we plan to evaluate the statis-tical parsers on the same data in order to comparethe performance of grammar-based and statisticalapproaches.AcknowledgmentsThe ?rst, third and fourth authors?
work was sup-ported by ANR Sequoia (ANR-08-EMER-013).We are grateful to our anonymous reviewers fortheir comments.ReferencesAbeill?, A. and N. Barrier.
2004.
Enriching a frenchtreebank.
In LREC?04.Bikel, D. M. 2002.
Design of a multi-lingual, parallel-processing statistical parsing engine.
In HLT-02.Brown, P., V. Della Pietra, P. Desouza, J. Lai, andR.
Mercer.
1992.
Class-based n-gram models ofnatural language.
Computational linguistics, 18(4).Buchholz, S. and E. Marsi.
2006.
CoNLL-X sharedtask on multilingual dependency parsing.
In CoNLL2006.Candito, M. and B. Crabb?.
2009.
Improving gener-ative statistical parsing with semi-supervised wordclustering.
In IWPT?09.Candito, M. and D. Seddah.
2010.
Parsing word clus-ters.
In NAACL/HLT Workshop SPMRL 2010.Candito, M., B.
Crabb?, and P. Denis.
2010.
Statis-tical french dependency parsing : treebank conver-sion and ?rst results.
In LREC 2010.Carroll, J., E. Briscoe, and A. San?lippo.
1998.
Parserevaluation: A survey and a new proposal.
In LREC1998.Cer, D., M.-C. de Marneffe, D. Jurafsky, and C. Man-ning.
2010.
Parsing to stanford dependencies:Trade-offs between speed and accuracy.
In LREC2010.Charniak, E. 2000.
A maximum entropy inspiredparser.
In NAACL 2000.115Crabb?, B. and M. Candito.
2008.
Exp?riencesd?analyse syntaxique statistique du fran?ais.
InTALN 2008.de la Clergerie, E. V., C. Ayache, G. de Chalendar,G.
Francopoulo, C. Gardent, and P. Paroubek.
2008.Large scale production of syntactic annotations forfrench.
In First International Workshop on Auto-mated Syntactic Annotations for Interoperable Lan-guage Resources.de Marneffe, M.-C., B. MacCartney, and C. D. Man-ning.
2006.
Generating typed dependency parsesfrom phrase structure parses.
In LREC 2006.Denis, P. and B. Sagot.
2009.
Coupling an an-notated corpus and a morphosyntactic lexicon forstate-of-the-art pos tagging with less human effort.In PACLIC 2009.Eisner, J.
1996.
Three new probabilistic models fordependency parsing: An exploration.
In COLING1996.Fan, R.-E., K.-W. Chang, C.-J.
Hsieh, X.-R. Wang, andC.-J.
Lin.
2008.
LIBLINEAR: A library for largelinear classi?cation.
Journal of Machine LearningResearch, 9.Kahane, S., A. Nasr, and O. Rambow.
1998.Pseudo-projectivity: A polynomially parsable non-projective dependency grammar.
In ACL/COLING1998.Klein, D. and C. D. Manning.
2003.
Accurate unlexi-calized parsing.
In ACL 2003.Koo, T., X. Carreras, and M. Collins.
2008.
Sim-ple semi-supervised dependency parsing.
In ACL-08:HLT.K?bler, S. 2008.
The PaGe 2008 shared task on pars-ing german.
In ACL-08 Workshop on Parsing Ger-man.Lin, D. 1995.
A dependency-based method for evalu-ating broad-coverage parsers.
In IJCAI-95.Magerman, D. M. 1995.
Statistical decision-tree mod-els for parsing.
In ACL 1995.Matsuzaki, T., Y. Miyao, and J. Tsujii.
2005.
Proba-bilistic cfg with latent annotations.
In ACL 2005.McDonald, R. and J. Nivre.
2007.
Characterizingthe errors of data-driven dependency parsing mod-els.
In EMNLP-CoNLL 2007.McDonald, R. and F. Pereira.
2006.
Online learningof approximate dependency parsing algorithms.
InEACL 2006.McDonald, R., K. Lerman, and F. Pereira.
2006.
Mul-tilingual dependency analysis with a two-stage dis-criminative parser.
In CoNLL 2006.McDonald, R. 2006.
Discriminative Learning andSpanning Tree Algorithms for Dependency Parsing.Ph.D.
thesis, University of Pennsylvania.Nivre, J., Johan Hall, and Jens Nilsson.
2006.
Malt-parser: A data-driven parser-generator for depen-dency parsing.
In LREC 2006.Nivre, J., J.
Hall, S. K?bler, R. McDonald, J. Nils-son, S. Riedel, and D. Yuret.
2007.
The CoNLL2007 shared task on dependency parsing.
In CoNLLShared Task of EMNLP-CoNLL 2007.Nivre, J.
2006.
Inductive Dependency Parsing.Springer.Nivre, J.
2008.
Algorithms for deterministic incre-mental dependency parsing.
Computational Lin-guistics, 34.Paroubek, P., L.-G. Pouillot, I. Robba, and A. Vilnat.2005.
Easy : Campagne d?
?valuation des analy-seurs syntaxiques.
In TALN 2005, EASy workshop :campagne d?
?valuation des analyseurs syntaxiques.Petrov, S. and D. Klein.
2007.
Improved inference forunlexicalized parsing.
In NAACL-07: HLT.Petrov, S., L. Barrett, R. Thibaux, and D. Klein.
2006.Learning accurate, compact, and interpretable treeannotation.
In ACL 2006.Sagot, B.
2010.
The Lefff, a freely available and large-coverage morphological and syntactic lexicon forfrench.
In LREC 2010.Schluter, N. and J. van Genabith.
2009.
Dependencyparsing resources for french: Converting acquiredlfg f-structure.
In NODALIDA 2009.Seddah, D., M. Candito, and B. Crabb?.
2009.
Crossparser evaluation and tagset variation: a french tree-bank study.
In IWPT 2009.Seddah, D., G. Chrupa?a, O. Cetinoglu, J. van Gen-abith, and M. Candito.
2010.
Lemmatization andstatistical lexicalized parsing of morphologically-rich languages.
In NAACL/HLT Workshop SPMRL2010.Tsarfaty, R. 2006.
Integrated morphological and syn-tactic disambiguation for modern hebrew.
In COL-ING/ACL 2006 Student Research Workshop.Yamada, H. and Y. Matsumoto.
2003.
Statistical de-pendency analysis with support vector machines.
InIWPT 2003.116
