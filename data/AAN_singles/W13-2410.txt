Proceedings of the 4th Biennial International Workshop on Balto-Slavic Natural Language Processing, pages 63?68,Sofia, Bulgaria, 8-9 August 2013. c?2010 Association for Computational LinguisticsImproving English-Russian sentence alignment through POS tagging andDamerau-Levenshtein distanceAndrey KutuzovNational Research University Higher School of EconomicsMoscow, RussiaMyasnitskaya str.
20akutuzov72@gmail.comAbstractThe present paper introduces approach toimprove English-Russian sentence align-ment, based on POS-tagging of automat-ically aligned (by HunAlign) source andtarget texts.
The initial hypothesis istested on a corpus of bitexts.
Sequencesof POS tags for each sentence (exactly,nouns, adjectives, verbs and pronouns)are processed as ?words?
and Damerau-Levenshtein distance between them iscomputed.
This distance is then normal-ized by the length of the target sentenceand is used as a threshold between sup-posedly mis-aligned and ?good?
sentencepairs.
The experimental results show pre-cision 0.81 and recall 0.8, which allowsthe method to be used as additional datasource in parallel corpora alignment.
Atthe same time, this leaves space for furtherimprovement.1 IntroductionParallel multilingual corpora have long ago be-come a valuable resource both for academic andfor industrial computational linguistics.
They areemployed for solving problems of machine trans-lation, for research in comparative language stud-ies and many more.One of difficult tasks in parallel multilingualcorpora building is alignment of its elements witheach other, that is establishing a set of links be-tween words and phrases of source and target lan-guage segments (Tiedemann, 2003).
Alignmentcan be done on the level of words, sentences,paragraphs or whole documents in text collection.Most widely used are word and sentence align-ment, and the present paper deals with the latterone.Word alignment is an essential part of statisti-cal machine translation workflow.
However, usu-ally it can only be done after sentence alignmentis already present.
Accordingly, there have beenextensive research on the ways to improve it.Basic algorithm of sentence alignment simplylinks sentences from source and target text in or-der of their appearance in the texts.
E.g., sentencenumber 1 in the source corresponds to sentencenumber 1 in the target etc.
But this scheme by de-sign can?t handle one-to-many, many-to-one andmany-to-many links (a sentence translated by twosentences, two sentences translated by one, etc)and is sensitive to omissions in source or trans-lated text.Mainstream ways of coping with these prob-lems and increasing alignment quality includeconsidering sentence length (Gale and Church,1991) and using bilingual dictionaries (Och andNey, 2000) or cognates (Simard et al 1992) toestimate the possibility of sentences being linked.Kedrova and Potemkin (2008) showed that theseways provide generally good results for Russianas well.But often this is not enough.
Sentence lengthcan vary in translation, especially when translationlanguage is typologically different from the sourceone.
As for bilingual dictionaries, it is sometimesproblematic to gather and compile a useful set ofthem.Thus, various additional methods were pro-posed, among them using part-of speech data fromboth source and target texts.
It is rather com-monplace in word alignment (Tiedemann, 2003;Toutanova et al 2002).
Using part-of speech tag-ging to improve sentence alignment for Chinese-English parallel corpus is presented in (Chen andChen, 1994).
In the current paper we propose touse similar approach in aligning English-Russiantranslations.632 Setting up the ExperimentWe test the part-of-speech based approach to im-prove quality of sentence alignment in our par-allel corpus of learner translations available athttp://rus-ltc.org.
Only English to Russian trans-lations were selected, as of now.
The workflowwas as follows.All source and target texts were automat-ically aligned with the help of HunAlignsoftware (Varga et al 2005) together withits wrapper LF Aligner by Andra?s Farkas(http://sourceforge.net/projects/aligner).
Thechoice of aligner was based on high estimationby researchers (cf.
(Kaalep and Veskis, 2007))and its open-source code.
Sentence splitting wasdone with a tool from Europarl v3 PreprocessingTools (http://www.statmt.org/europarl) written byPhilipp Koehn and Josh Schroeder.
Proper lists ofnon-breaking prefixes were used for both Russianand English.HunAlign uses both bilingual dictionaries andGale-Church sentence-length information.
Its re-sults are quite good, considering the noisiness ofour material.
However, about 30 percent of sen-tences are still mis-aligned.
The reasons behindthis are different, but mostly it is sentence split-ter errors (notwithstanding its preparation for Rus-sian), omissions or number of sentences changingduring translation.
Here is a typical example:?And these two fuels are superior to ethanol,Liao says, because they have a higher energy den-sity, do not attract water, and are noncorrosive?.||| ????
???
????
???????
????
?????????????????
??
?????
?????????.
?0 ||| ???
??????
??
?, ???
?????????????
???????
??????????????
??????????,??
????????
???
?, ?
??????
?????????????.
?The translator transformed one English sen-tence into two Russian sentences.
Consequently,aligner linked the first Russian sentence to thesource one, and the second sentence is left with-out its source counterpart (null link).
It should besaid that in many cases HunAlign manages to copewith such problems, but not always, as we can seein the example above.The cases of mis-alignment must be human cor-rected, which is very time-expensive, especiallybecause there is no way to automatically assess thequality of alignment.
HunAlign?s internal measureof quality is often not very helpful.
For exam-ple, for the first row of the table above it assignedrather high quality mark of 0.551299.
Trying topredict alignment correctness with the help of Hunquality mark only for the whole our data set re-sulted in precision 0.727 and recall 0.548, whichis much lower than our results presented below.We hypothesize that source and target sentenceshould in most cases correspond in the number andorder of content parts of speech (POS).
This datacan be used to trace mis-aligned sentences and per-haps to find correct equivalents for them.
In orderto test this hypothesis, our source and target textswere POS-tagged using Freeling 3.0 suite of lan-guage analyzers (Padro?
and Stanilovsky, 2012).Freeling gives comparatively good results in En-glish and Russian POS-tagging, using Markov tri-gram scheme trained on large disambiguated cor-pus.Freeling tag set for English follows that ofPenn TreeBank, while Russian tag set, ac-cording to Freeling manual, corresponds toEAGLES recommendations for morphosyn-tactic annotation of corpora described onhttp://www.ilc.cnr.it/EAGLES96/home.html(Monachini and Calzolari, 1996).
It is not trivialto project one scheme onto another completely,except for the main content words ?
nouns, verbsand adjectives.
Moreover, these three parts ofspeech are the ones used in the paper by Chenand Chen (1994), mentioned above.
So, thedecision was made to take into consideration onlythe aforementioned lexical classes, with optionalinclusion of pronouns (in real translations theyoften replace nouns and vice versa).Thus, each sentence was assigned a ?POS wa-termark?, indicating number and order of contentwords in it.
Cf.
the following sentence:?Imagine three happy people each win $1 mil-lion in the lottery.
?and its ?POS watermark?
:VANVNN,where N is noun, A is adjective and V is verb.Here is the same analysis for its Russian trans-lation counterpart:???????????
????
????
??????????
?????,???????
????????
?
???????
??
????????????????.
?Corresponding ?POS watermark?
:VPANVNNN,where N is noun, V is verb, A is adjective and Pis pronoun.Nouns and verbs are marked identically in Penn64and EAGLES schemes.
Adjectives in Penn aremarked as JJ, so this mark was corrected to A,which is also the mark for adjectives in EAGLES.We considered to be ?pronouns?
(P) those wordswhich are marked as ?E?
in EAGLES and ?PRP?in Penn.Thus, each content word is represented asone letter strictly corresponding to one lexi-cal class.
Therefore our ?POS watermark?can be thought of as a kind of ?word?.
Thedifference between these ?words?
is computedusing Damerau-Levenshtein distance (Damerau,1964).
Basically, it is the number of cor-rections, deletions, additions and transpositionsneeded to transform one character sequenceinto another.
We employ Python implementa-tion of this algorithm by Michael Homer (pub-lished at http://mwh.geek.nz/2009/04/26/python-damerau-levenshtein-distance).According to it, the distance between POS wa-termarks of two sentence above is 2.
It means weneed only two operations ?
adding one pronounand one noun ?
to get target POS structure fromsource POS structure.
At the same time, the dis-tance between VPVNANNNNNNNNNNVN andNVNNANANANN is as high as 10, which meansthat POS structures of these sentences are quitedifferent.
Indeed, the sentences which generatedthese structures are obviously mis-aligned:?If a solar panel ran its extra energy into avat of these bacteria, which could use the en-ergy to create biofuel, then the biofuel effectivelybecomes a way to store solar energy that oth-erwise would have gone to waste.?
||| ??????????
????????????
???????
?????
?, ????????????.
?One can suppose that there is correlationbetween Damerau-Levenshtein distance and thequality of alignment: the more is the distance themore is the possibility that the alignment of thesetwo sentences has failed in one or the other way.In the following chapter we present the results ofthe preliminary experiment on our parallel texts.3 The ResultsWe performed testing of the hypothesis over 170aligned English-Russian bi-texts containing 3263sentence pairs.
As of genres of original texts, theyincluded essays, advertisements and informationalpassages from mass media.
The dataset was hand-annotated and mis-aligned sentence pairs marked(663 pairs, 20 percent of total dataset).Damerau-Levenshtein distances for all sen-tences were computed and we tried to find opti-mal distance threshold to cut ?bad?
sentence pairsfrom ?good?
ones.For this we used Weka software (Hall etal., 2009) and its Threshold Selector ?
a meta-classifier that selects a mid-point threshold on theprobability output by a classifier (logistic regres-sion in our case).
Optimization was performedfor ?bad?
class, and we used both precision andF-measure for determining the threshold, with dif-ferent results presented below.
The results wereevaluated with 3-fold cross-validation over the en-tire dataset.Initially, on the threshold 7 we achieved pre-cision 0.78, recall 0.77 and F-measure 0.775 forthe whole classifier.
F-measure for detecting onlymis-aligned sentences was as low as 0.464.In order to increase the quality of detection wetried to change the settings: first, to change thenumber of ?features?, i.e., parts of speech consid-ered.
?Minimalist?
approach with only nouns andadjectives lowered F-measure to 0.742.
However,considering nouns, adjectives and verbs withoutpronouns seemed more promising: using the samedistance threshold 7 we got precision 0.787 andrecall 0.78 with F-measure 0.783.
F-measure fordetecting mis-aligned sentences also got slightlyhigher, up to 0.479.
So, general estimate is evenhigher than when using pronouns.Moving further in an effort to improve the al-gorithm, we found that Damerau-Levenshtein dis-tance shows some kind of dis-balance when com-paring short and long ?words?.
Short ?words?
re-ceive low distance estimates simply because thenumber of characters is small and it?s ?easier?
totransform one into another, even if the ?words?
arerather different.
At the same time, long ?words?tend to receive higher distance estimates becauseof higher probability of some variance in them,even if the ?words?
represent legitimate sentencepairs.
Cf.
the following pairs:?
distance between PVPVAA and ANAN is es-timated as 5,?
distance between NNNNVAANNVVN-NVNNNVV and NNNNVANANPAN-NANVN is estimated as 7.Meanwhile, the first sentence pair is in fact mis-aligned, and the second one is quite legitimate.
It65is obvious that ?word?
length influences resultsof distance estimation and it should be somehowcompensated.Thus, the penalty was assigned to all distances,depending on the length of original sentences.Then this ?normalized?
distance was used as athreshold.
We tried employing the length of thesource sentence, of target sentence and the aver-age of both.
The length of the target (translated)sentence gave the best results.So, the equation is as follows:DLnorm = DL(sP,tP )LEN(tP ) ,where DLnorm is ?normalized?
distance, DLis original Damerau-Levenshtein distance, sP is?POS watermark?
for source sentence, tP is ?POSwatermark?
for target sentence and LEN is lengthin characters.With nouns, verbs, adjectives and pronouns thisnormalization gives considerably better results:Precision 0.813Recall 0.802F-Measure 0.807After removing pronouns from consideration,at the optimal threshold of 0.21236, recall getsslightly higher:Precision 0.813Recall 0.803F-Measure 0.808Even ?minimalist?
nouns-and-adjectives ap-proach improves after normalization:Precision: 0.792Recall: 0.798F-Measure: 0.795Overall results are presented in the Table 1.Methods without target length penalty provideconsiderably lower overall performance, thus,methods with the penalty should be used.Depending on particular aim, one can vary thethreshold used in classification.
In most cases,mis-aligned pairs are of more interest than ?goodpairs?.
If one?s aim is to improve precision of ?badpairs?
detection, the threshold of 0.8768 will give0.851 precision for this, at the expense of recallas low as 0.1.
If one wants more balanced out-put, the already mentioned threshold of 0.21236is optimal, providing mis-aligned pairs detectionprecision of 0.513 and recall of 0.584.Figure 1 presents distribution of ?good?
and?bad?
pairs in our data set in relation to Damerau-Levenshtein distance (X axis).
Correctly alignedpairs are colored gray and incorrectly aligned onesMethod Precision Recall F-MeasureNouns, adjec-tives, verbsand pronounswithout lengthpenalty.0.78 0.77 0.775Nouns, adjec-tives and verbswithout lengthpenalty.0.787 0.78 0.783Nouns andadjectiveswithout lengthpenalty.0.764 0.728 0.742Nouns andadjectives withtarget lengthpenalty.0.792 0.798 0.795Nouns, adjec-tives, verbs andpronouns withtarget lengthpenalty.0.813 0.802 0.807Nouns, ad-jectives andverbs withtarget lengthpenalty.0.813 0.803 0.808Table 1.
Overall performance of pairs classifierdepending on the method.black.
Correlation between alignment correctnessand Levenshtein value can be clearly seen.
At thesame time, internal HunAlign quality measure (Yaxis) does not show any stable influence on align-ment correctness, as we already mentioned above.4 Discussion and Further ResearchThe experimental results presented above showthat number and order of POS in source and tar-get sentences in English-Russian translations aresimilar to the degree that makes possible to usethis similarity in order to check alignment cor-rectness.
The method of calculating Damerau-Levenshtein distance between POS ?watermarks?of source and target sentences can be applied fordetecting mis-aligned sentence pairs as an addi-tional factor, influencing the decision to mark thepair as ?bad?.66Figure 1.
Levenshtein distance (X axis) and alignment correctness (color)However, some pairs show anomalies in thisaspect.
For example, the pair below is charac-terized by normalized POS Damerau-Levenshteindistance of enormous 2.6, however, human asses-sor marked it as ?good?
:?An opinion poll released by the independentLevada research group found that only 6 per centof Russians polled sympathised with the womenand 51 per cent felt either indifference, irritationor hostility.?
||| ??
???
51 ???????
????????????????????
?
???
??????????
?
????????????????.
?Translator omitted some information she con-sidered irrelevant, but the pair itself is aligned cor-rectly.On the other hand, cf.
two consecutive pairsbelow:?The British Museum?
The Louvre??
|||???????????
???????
?The Metropolitan??
||| ???????
?Normalized distance for the first pair is 0.3333,and this correctly classifies it as ?bad?.
The sec-ond target sentence must have belonged to the firstpair and the second pair is obviously bad, but itsdistance equals to zero (because both part containexactly one noun), so it will be incorrectly classi-fied as ?good?
with any threshold.Such cases are not detected with the method de-scribed in this paper.Our plans include enriching this method withheuristic rules covering typical translational trans-formations for particular language pair.
For exam-ple, English construction ?verb + pronominal di-rect object?
is regularly translated to ?pronominaldirect object + verb?
in Russian:?She loves him?
||| ????
???
?????
?.Also we plan to move from passively markingmis-aligned pairs and leaving the actual correc-tion to human to actively searching for possibleequivalent candidates among other sentence pairs,especially among those with null links.
The diffi-cult part here is designing the method to deal with?partially correct?
alignment, for example, like inthe pair below:?The magic number that defines this ?com-fortable standard?
varies across individuals andcountries, but in the United States, it seems tofall somewhere around $75,000.?
||| ??????????????
?, ???????
????????????
??????????????
?, ???????
??
??????
???????
?, ??????
??
?????
?, ?
???????
??
?????????.
?In the experiment above we considered suchpairs to be mis-aligned.
But ideally, the sec-ond part of the source sentence should be de-tached and start ?looking for?
appropriate equiv-alent.
Whether this can be done with the help ofPOS-tagging (or, perhaps, syntactic parsing), fur-ther research will show.The same is true about the possibility to ap-ply this method to Russian-English translationsor translations between typologically distant lan-guages.675 ConclusionIn this paper, approach to improve English-Russian sentence alignment was introduced, basedon part-of-speech tagging of automatically alignedsource and target texts.
Sequences of POS-marksfor each sentence (exactly, nouns, adjectives, verbsand pronouns) are processed as ?words?
andDamerau-Levenshtein distance between them iscomputed.
This distance is then normalized bythe length of the target sentence and is used asa threshold between supposedly mis-aligned and?good?
sentence pairs.The experimental results show precision 0.81and recall 0.8 for this method.
This performancealone allows the method to be used in parallel cor-pora alignment, but at the same time leaves spacefor further improvement.AcknowledgmentsThe study was implemented in the framework ofthe Basic Research Program at the National Re-search University Higher School of Economics(HSE) in 2013.ReferencesKuang-hua Chen and Hsin-Hsi Chen.
1994.
A part-of-speech-based alignment algorithm.
In Proceedingsof the 15th conference on Computational linguistics- Volume 1, COLING ?94, pages 166?171, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.William A. Gale and Kenneth W. Church.
1991.
Aprogram for aligning sentences in bilingual corpora.In Proceedings of the 29th annual meeting on As-sociation for Computational Linguistics, ACL ?91,pages 177?184, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The weka data mining software: an update.SIGKDD Explor.
Newsl., 11(1):10?18, November.Heiki-Jaan Kaalep and Kaarel Veskis.
2007.
Compar-ing parallel corpora and evaluating their quality.
InProceedings of MT Summit XI, pages 275?279.G.E.
Kedrova and S.B.
Potemkin.
2008.
Alignmentof un-annotated parallel corpora.
In Papers fromthe annual international conference ?Dialogue?, vol-ume 7, pages 431?436, Moscow, Russia.Monica Monachini and Nicoletta Calzolari.
1996.Eagles synopsis and comparison of morphosyntac-tic phenomena encoded in lexicons and corpora.
acommon proposal and applications to european lan-guages.
Technical report, Paris, France.Franz Josef Och and Hermann Ney.
2000.
A com-parison of alignment models for statistical machinetranslation.
In Proceedings of the 18th conferenceon Computational linguistics - Volume 2, COLING?00, pages 1086?1090, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.Llu?
?s Padro?
and Evgeny Stanilovsky.
2012.
Freel-ing 3.0: Towards wider multilinguality.
In Nico-letta Calzolari (Conference Chair), Khalid Choukri,Thierry Declerck, Mehmet Ug?ur Dog?an, BenteMaegaard, Joseph Mariani, Jan Odijk, and SteliosPiperidis, editors, Proceedings of the Eight Interna-tional Conference on Language Resources and Eval-uation (LREC?12), Istanbul, Turkey, may.
EuropeanLanguage Resources Association (ELRA).Michel Simard, George F. Foster, and Pierre Isabelle.1992.
Using cognates to align sentences in bilingualcorpora.
In Proceedings of the Fourth InternationalConference on Theoretical and Methodological Is-sues in Machine Translation, pages 67?81.Jo?rg Tiedemann.
2003.
Combining clues for wordalignment.
In Proceedings of the tenth conferenceon European chapter of the Association for Compu-tational Linguistics - Volume 1, EACL ?03, pages339?346, Stroudsburg, PA, USA.
Association forComputational Linguistics.Kristina Toutanova, H. Tolga Ilhan, and Christopher D.Manning.
2002.
Extensions to hmm-based statisti-cal word alignment models.
In Proceedings of theACL-02 conference on Empirical methods in natu-ral language processing - Volume 10, EMNLP ?02,pages 87?94, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.D.
Varga, L. Ne?meth, P. Hala?csy, A. Kornai, V. Tro?n,and V. Nagy.
2005.
Parallel corpora for mediumdensity languages.
In Recent Advances in NaturalLanguage Processing (RANLP 2005), pages 590?596.68
