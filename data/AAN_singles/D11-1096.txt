Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1034?1046,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsStructured Lexical Similarity via Convolution Kernels on Dependency TreesDanilo CroceDIIUniversity of Tor Vergata00133 Roma, Italycroce@info.uniroma2.itAlessandro MoschittiDISIUniversity of Trento38123 Povo (TN), Italymoschitti@disi.unitn.itRoberto BasiliDIIUniversity of Tor Vergata00133 Roma, Italybasili@info.uniroma2.itAbstractA central topic in natural language process-ing is the design of lexical and syntactic fea-tures suitable for the target application.
In thispaper, we study convolution dependency treekernels for automatic engineering of syntacticand semantic patterns exploiting lexical simi-larities.
We define efficient and powerful ker-nels for measuring the similarity between de-pendency structures, whose surface forms ofthe lexical nodes are in part or completely dif-ferent.
The experiments with such kernels forquestion classification show an unprecedentedresults, e.g.
41% of error reduction of the for-mer state-of-the-art.
Additionally, semanticrole classification confirms the benefit of se-mantic smoothing for dependency kernels.1 IntroductionA central topic in Natural Language Processing isthe design of lexical and syntactic features suitablefor the target application.
The selection of effectivepatterns composed of syntactic dependencies andlexical constraints is typically a complex task.Additionally, the availability of training data isusually scarce.
This requires the development ofgeneralized features or the definition of seman-tic similarities between them, e.g.
as proposed in(Resnik, 1995; Jiang and Conrath, 1997; Schtze,1998; Pedersen et al, 2004a; Bloehdorn and Mos-chitti, 2007b; Davis et al, 2007) or in semi-supervised settings, e.g.
(Chapelle et al, 2006).A semantic similarity can be defined at structurallevel over a graph, e.g.
(Freeman, 1977; Bunke andShearer, 1998; Brandes, 2001; Zhao et al, 2009), aswell as combining structural and lexical similarityover semantic networks, e.g.
(Cowie et al, 1992; Wuand Palmer, 1994; Resnik, 1995; Jiang and Conrath,1997; Schtze, 1998; Leacock and Chodorow, 1998;Pedersen et al, 2004a; Budanitsky and Hirst, 2006).More recent research also focuses on mechanismsto define if two structures, e.g.
graphs, are enoughsimilar, as explored in (Mihalcea, 2005; Zhao et al,2009; Fu?rstenau and Lapata, 2009; Navigli and La-pata, 2010).On one hand, previous work shows that there isa substantial lack of automatic methods for engi-neering lexical/syntactic features (or more in gen-eral syntactic/semantic similarity).
On the otherhand, automatic feature engineering of syntactic orshallow semantic structures has been carried outby means of structural kernels, e.g.
(Collins andDuffy, 2002; Kudo and Matsumoto, 2003; Cumbyand Roth, 2003; Cancedda et al, 2003; Daume?
IIIand Marcu, 2004; Toutanova et al, 2004; Shen et al,2003; Gliozzo et al, 2005; Kudo et al, 2005; Titovand Henderson, 2006; Zelenko et al, 2002; Bunescuand Mooney, 2005; Zhang et al, 2006).
The mainidea of structural kernels is to generate structuresthat in turn represent syntactic or shallow semanticfeatures.
Most notably, the work in (Bloehdorn andMoschitti, 2007b) encodes lexical similarity in suchkernels.
This is essentially the syntactic tree ker-nel (STK) proposed in (Collins and Duffy, 2002) inwhich syntactic fragments from constituency treescan be matched even if they only differ in the leafnodes (i.e.
they have different surface forms).
Thisimplies matching scores lower than 1, depending onthe semantic similarity of the corresponding leavesin the syntactic fragments.Although this kernel achieves state-of-the-art per-formance in NLP tasks, such as Question Classifica-1034tion (Bloehdorn and Moschitti, 2007b) and TextualEntailment (Mehdad et al, 2010), it offers clearlypossibility of improvement: (i) better possibility toexploit semantic smoothing since, e.g., trivially STKonly matches the syntactic structure apple/orangewhen comparing the big beautiful apple to a nicelarge orange; and (ii) STK cannot be effectively ap-plied to dependency structures, e.g.
see experimentsand motivation in (Moschitti, 2006a).
Additionally,to our knowledge, there is no previous study thatclearly describes how dependency structures shouldbe converted in trees to be fully and effectively ex-ploitable by convolution kernels.
Indeed, althoughthe work in (Culotta and Sorensen, 2004) defines adependency tree also using node similarity, it is nota convolution kernel: this results in a much poorerfeature space.In this paper, we propose a study of convolutionkernels for dependency structures aiming at jointlymodeling syntactic and lexical semantic similarity.More precisely, we define several dependency treesexploitable by the Partial Tree Kernel (PTK) (Mos-chitti, 2006a) and compared them with STK overconstituency trees.
Most importantly, we definean innovative and efficient class of kernels, i.e.
theSmoothed Partial Tree Kernels (SPTKs), which canmeasure the similarity of structural similar treeswhose nodes are associated with different but re-lated lexicals.
Given the convolution nature of suchkernels any possible node path of lexicals providea contribution smoothed by the similarity accountedby its nodes.The extensive experimentation on two datasets ofquestion classification (QC) and semantic role label-ing (SRL), shows that: (i) PTK applied to our depen-dency trees outperforms STK, demonstrating thatdependency parsers are fully exploitable for featureengineering based on structural kernels; (ii) SPTKoutperforms any previous kernels achieving an un-precedented result of 41% of error reduction with re-spect to the former state-of-the-art on QC; and (iii)the experiments on SRL confirm that the approachcan be applied to different tasks without any tuningand again achieving state-of-the-art accuracy.In the reminder of this paper, Section 2 providesthe background for structural and lexical similar-ity kernels.
Section 3 introduces SPTK.
Section 4provides our representation models for dependencytrees.
Section 5 presents the experimental evaluationfor QC and SRL.
Section 6 derives the conclusions.2 Kernel BackgroundIn kernel-based machines, both learning and classi-fication algorithms only depend on the inner prod-uct between instances.
This in several cases can beefficiently and implicitly computed by kernel func-tions by exploiting the following dual formulation:?i=1..l yi?i?(oi)?
(o) + b = 0, where oi and o aretwo objects, ?
is a mapping from the objects to fea-ture vectors ~xi and ?(oi)?
(o) = K(oi, o) is a ker-nel function implicitly defining such mapping.
Incase of structural kernels,K determines the shape ofthe substructures describing the objects above.
Themost general kind of kernels used in NLP are stringkernels, e.g.
(Shawe-Taylor and Cristianini, 2004),the Syntactic Tree Kernels (Collins and Duffy, 2002)and the Partial Tree Kernels (Moschitti, 2006a).2.1 String KernelsThe String Kernels (SK) that we consider countthe number of subsequences shared by two stringsof symbols, s1 and s2.
Some symbols during thematching process can be skipped.
This modifiesthe weight associated with the target substrings asshown by the following SK equation:SK(s1, s2) =?u???
?u(s1) ?
?u(s2) =?u???
?~I1:u=s1[~I1]?~I2:u=s2[~I2]?d(~I1)+d(~I2)where, ??
= ?
?n=0 ?n is the set of all strings, ~I1 and~I2 are two sequences of indexes ~I = (i1, ..., i|u|),with 1 ?
i1 < ... < i|u| ?
|s|, such that u = si1 ..si|u| ,d(~I) = i|u| ?
i1 + 1 (distance between the first andlast character) and ?
?
[0, 1] is a decay factor.It is worth noting that: (a) longer subsequencesreceive lower weights; (b) some characters can beomitted, i.e.
gaps; (c) gaps determine a weight sincethe exponent of ?
is the number of characters andgaps between the first and last character; and (c)the complexity of the SK computation is O(mnp)(Shawe-Taylor and Cristianini, 2004), where m andn are the lengths of the two strings, respectively andp is the length of the largest subsequence we want toconsider.10352.2 Tree KernelsConvolution Tree Kernels compute the numberof common substructures between two trees T1and T2 without explicitly considering the wholefragment space.
For this purpose, let the setF = {f1, f2, .
.
.
, f|F|} be a tree fragment space and?i(n) be an indicator function, equal to 1 if thetarget fi is rooted at node n and equal to 0 oth-erwise.
A tree-kernel function over T1 and T2 isTK(T1, T2) =?n1?NT1?n2?NT2?
(n1, n2), NT1and NT2 are the sets of the T1?s and T2?s nodes,respectively and ?
(n1, n2) = ?|F|i=1 ?i(n1)?i(n2).The latter is equal to the number of common frag-ments rooted in the n1 and n2 nodes.
The ?
func-tion determines the richness of the kernel space andthus different tree kernels.
Hereafter, we considerthe equation to evaluate STK and PTK 1.2.2.1 Syntactic Tree Kernels (STK)To compute STK is enough to compute?STK(n1, n2) as follows (recalling that sinceit is a syntactic tree kernels, each node can beassociated with a production rule): (i) if theproductions at n1 and n2 are different then?STK(n1, n2) = 0; (ii) if the productions atn1 and n2 are the same, and n1 and n2 haveonly leaf children then ?STK(n1, n2) = ?
; and(iii) if the productions at n1 and n2 are thesame, and n1 and n2 are not pre-terminals then?STK(n1, n2) = ?
?l(n1)j=1 (1 + ?STK(cjn1 , cjn2)),where l(n1) is the number of children of n1 and cjnis the j-th child of the node n. Note that, since theproductions are the same, l(n1) = l(n2) and thecomputational complexity of STK is O(|NT1 ||NT2 |)but the average running time tends to be linear,i.e.O(|NT1 |+ |NT2 |), for natural language syntactictrees (Moschitti, 2006a).2.2.2 The Partial Tree Kernel (PTK)The computation of PTK is carried out by thefollowing ?PTK function: if the labels of n1and n2 are different then ?PTK(n1, n2) = 0; else?PTK(n1, n2) =?
(?2 +?~I1,~I2,l(~I1)=l(~I2)?d(~I1)+d(~I2)l(~I1)?j=1?PTK(cn1(~I1j), cn2(~I2j)))1To have a similarity score between 0 and 1, a normalizationin the kernel space, i.e.
TK(T1,T2)?TK(T1,T1)?TK(T2,T2)is applied.where d(~I1) = ~I1l(~I1)?~I11+1 and d(~I2) = ~I2l(~I2)?~I21 + 1.
This way, we penalize both larger trees andchild subsequences with gaps.
PTK is more generalthan the STK as if we only consider the contribu-tion of shared subsequences containing all childrenof nodes, we implement the STK kernel.
The com-putational complexity of PTK is O(p?2|NT1 ||NT2 |)(Moschitti, 2006a), where p is the largest subse-quence of children that we want consider and ?
is themaximal outdegree observed in the two trees.
How-ever the average running time again tends to be lin-ear for natural language syntactic trees (Moschitti,2006a).2.3 Lexical Semantic KernelGiven two text fragments d1 and d2 ?
D (the textfragment set), a general lexical kernel (Basili et al,2005) defines their similarity as:K(d1, d2) =?w1?d1,w2?d2(?1?2)?
?
(w1, w2) (1)where ?1 and ?2 are the weights of the words (fea-tures) w1 and w2 in the documents d1 and d2, re-spectively, and ?
is a term similarity function, e.g.
(Pedersen et al, 2004b; Sahlgren, 2006; Corley andMihalcea, 2005; Mihalcea et al, 2005).
Technically,any ?
can be used, provided that the resulting Grammatrix, G = K(d1, d2) ?d1, d2 ?
D is positivesemi-definite (Shawe-Taylor and Cristianini, 2004)(D is typically the training text set).We determine the term similarity function throughdistributional analysis (Pado and Lapata, 2007), ac-cording to the idea that the meaning of a word canbe described by the set of textual contexts in which itappears (Distributional Hypothesis, (Harris, 1964)).The contexts are words appearing in a n-windowwith target words: such a space models a genericnotion of semantic relatedness, i.e.
two wordsclose in the space are likely to be either in paradig-matic or syntagmatic relation as in (Sahlgren, 2006).The original word-by-word context matrix M is de-composed through Singular Value Decomposition(SVD) (Golub and Kahan, 1965) into the productof three new matrices: U , S, and V so that S is di-agonal and M = USV T .
M is approximated byMl = UlSlV Tl in which only the first l columns ofU and V are used, and only the first l greatest singu-lar values are considered.
This approximation sup-plies a way to project a generic term wi into the l-1036dimensional space using W = UlS1/2l , where eachrow corresponds to the representation vectors ~wi.Therefore, given two words w1 and w2, the termsimilarity function ?
is estimated as the cosine simi-larity between the corresponding projections ~w1, ~w2,i.e ?
(w1, w2) = ~w1?
~w2?
~w1??
~w2?
.
The latent semantic ker-nels (Siolas and d?Alch Buc, 2000; Cristianini et al,2001) derive G by applying LSA, resulting in a validkernel.Another methods to design a valid kernel is to rep-resent words as word vectors and compute ?
as theirscalar product between such vectors.
For example,in (Bloehdorn et al, 2006), bag of hyponyms andhypernyms (up to a certain level of WordNet hierar-chy) were used to build such vectors.
We will referto such similarity as WL (word list).3 Smoothing Partial Tree Kernel (SPTK)Combining lexical and structural kernels providesclear advantages on all-vs-all words similarity,which tends to semantically diverge.
Indeed syn-tax provides the necessary restrictions to com-pute an effective semantic similarity.
Followingthis idea, Bloedhorn & Moschitti (2007a) mod-ified step (i) of ?STK computation as follows:(i) if n1 and n2 are pre-terminal nodes withthe same number of children, ?STK(n1, n2) =?
?nc(n1)j=1 ?
(lex(n1), lex(n2)), where lex returnsthe node label.
This allows to match fragments hav-ing same structure but different leaves by assigning ascore proportional to the product of the lexical sim-ilarities of each leaf pair.
Although it is an inter-esting kernel, the fact that lexicals must belong tothe leaf nodes of exactly the same structures limitsits applications.
Trivially, it cannot work on depen-dency trees.
Hereafter, we define a much more gen-eral smoothed tree kernel that can be applied to anytree and exploit any combination of lexical similari-ties, respecting the syntax enforced by the tree.3.1 SPTK DefinitionIf n1 and n2 are leaves then ??
(n1, n2) =???
(n1, n2); else??
(n1, n2) = ??
(n1, n2)?
(?2 +?~I1,~I2,l(~I1)=l(~I2)?d(~I1)+d(~I2)l(~I1)?j=1??
(cn1(~I1j), cn2(~I2j))), (2)where ?
is any similarity between nodes, e.g.
be-tween their lexical labels, and the other variables arethe same of PTK.3.2 SoundnessA completely formal proof of the validity of theEq.
2 is beyond the purpose of this paper (mainlydue to space reason).
Here we give a first sketch:let us consider ?
as a string matching betweennode labels and ?
= ?
= 1.
Each recursivestep of Eq.
2 can be seen as a summation of (1 +?l(~I1)j=1 ?STK(cn1(~I1j), cn2(~I2j))), i.e.
the ?STKrecursive equation (see Sec.
2.2.1), for all subse-quences of children cn1(~I1j).
In other words, PTKis a summation of an exponential number of STKs,which are valid kernels.
It follows that PTK is a ker-nel.
Note that the multiplication by ?
and ?
elevatedto any power only depends on the target fragment.Thus, it just gives an additional weight to the frag-ment and does not violate the Mercer?s conditions.In contrast, the multiplication by ?
(n1, n2) does de-pend on both comparing examples, i.e.
on n1 and n2.However, if the matrix [?
(n1, n2)]?n1, n2 ?
f ?
Fis positive semi-definite, a decomposition existssuch that ?
(n1, n2) = ?(n1)?
(n2) ?
??
(n1, n2)can be written as ?|F|i=1 ?(n1)?i(n1)?
(n2)?i(n2)= ?|F|i=1 ??(n1)??
(n2) (see Section 2.2), whichproves SPTK to be a valid kernel.3.3 Efficient EvaluationWe followed the idea in (Moschitti, 2006a) for effi-ciently computing SPTK.
We consider Eq.
2 evalu-ated with respect to sequences of different length p;it follows that?
(n1, n2) = ??
(n1, n2)(?2 +m?p=1?p(cn1 , cn2)),where ?p evaluates the number of common sub-trees rooted in subsequences of exactly p children(of n1 and n2) and m = min{l(cn1), l(cn2)}.Given the two child sequences s1a = cn1 ands2b = cn2 (a and b are the last children)?p(s1a, s2b) = ?
(a, b)?|s1|?i=1|s2|?r=1?|s1|?i+|s2|?r ??
?p?1(s1[1 : i], s2[1 : r])where s1[1 : i] and s2[1 : r] are the child subse-quences from 1 to i and from 1 to r of s1 and s2.
Ifwe name the double summation term as Dp, we can1037S1SBARQ.?
::.SQVPNPPPNPNNfield::nNNfootball::nDTa::dINof::iNPNNwidth::nDTthe::dAUXbe::vWHNPWPwhat::wFigure 1: Constituent Tree (CT)rewrite the relation as:?p(s1a, s2b) ={?
(a, b)Dp(|s1|, |s2|) if ?
(a, b) > 0;0 otherwise.Note that Dp satisfies the recursive relation:Dp(k, l) = ?p?1(s1[1 : k], s2[1 : l]) + ?Dp(k, l ?
1)+?Dp(k ?
1, l)?
?2Dp(k ?
1, l ?
1)By means of the above relation, we can compute thechild subsequences of two sequences s1 and s2 inO(p|s1||s2|).
Thus the worst case complexity of theSPTK is identical to PTK, i.e.
O(p?2|NT1 ||NT2 |),where ?
is the maximum branching factor of the twotrees.
The latter is very small in natural languageparse trees and we also avoid the computation ofnode pairs with non similar labels.We note that PTK generalizes both (i) SK, allow-ing the similarity between sequences (node children)structured in a tree and (ii) STK, allowing the com-putation of STK over any possible pair of subtreesextracted from the original tree.
For this reason,we do not dedicate additional space on the defini-tion of the smoothed SK or smoothed STK, whichare in any case important corollary findings of ourresearch.3.4 Innovative Features of SPTKThe most similar kernel to SPTK is the SyntacticSemantic Tree Kernel (SSTK) proposed in (Bloe-hdorn and Moschitti, 2007a; Bloehdorn and Mos-chitti, 2007b).
However, the following aspects showthe remarkable innovativeness of SPTK:?
SSTK can only work on constituency treesand not on dependency trees (see (Moschitti,2006a)).?
The lexical similarity in SSTK is only appliedto leaf nodes in exactly the same syntacticconstituents.
Only complete matching of thestructure of subtrees is allowed: there is abso-lutely no flexibility, e.g.
the NP structure ?ca-ble television system?
has no match with theNP ?video streaming system?.
SPTK providesmatches between all possible relevant subparts,e.g.
?television system?
and ?video system?
(soalso exploiting the meaningful similarity be-tween ?video?
and ?television?).?
The similarity in the PTK equation is addedsuch that SPTK still corresponds to a scalarproduct in the semantic/structure space2.?
We have provided a fast evaluation of SPTKwith dynamic programming (otherwise thecomputation would have required exponentialtime).4 Dependency Tree StructuresThe feature space generated by the structural ker-nels, presented in the previous section, obviously de-pends on the input structures.
In case of PTK andSPTK different tree representations may lead to en-gineer more or less effective syntactic/semantic fea-ture spaces.
The next two sections provide our repre-sentation models for dependency trees and their dis-cussion.4.1 Proposed Computational StructuresGiven the following sentence:(s1) What is the width of a football field?The representation tree for a phrase structureparadigm leaves little room for variations as shownby the constituency tree (CT) in Figure 1.
We ap-ply lemmatization to the lexicals to improve gener-alization and, at the same time, we add a generalizedPoS-tag, i.e.
noun (n::), verb (v::), adjective (::a), de-terminer (::d) and so on, to them.
This is useful tomeasure similarity between lexicals belonging to thesame grammatical category.In contrast, the conversion of dependency struc-tures in computationally effective trees (for theabove kernels) is not straightforward.
We need todecide the role of lexicals, their grammatical func-tions (GR), PoS-tags and dependencies.
It is natural2This is not trivial: for example if sigma is added in Eq.
2 byonly multiplying the ?d1+d2 term, no valid space is generated.1038ROOTVBZP.?
::.PRDNNNMODINPMODNNfield::nNMODNNfootball::nNMODDTa::dof::iwidth::nNMODDTthe::dbe::vSBJWPwhat::wFigure 2: PoS-Tag Centered Tree (PCT)ROOTP.?
::.PRDNMODPMODNNfield::nNMODNNfootball::nNMODDTa::dINof::iNNwidth::nNMODDTthe::dVBZbe::vSBJWPwhat::wFigure 3: Grammatical Relation Centered Tree (GRCT)be::vVBZROOT?
::..Pwidth::nNNPRDof::iINNMODfield::nthe::dDTNMODwhat::wWPSBJNNPMODfootball::nNNNMODa::dDTNMODFigure 4: Lexical Centered Tree (LCT)to associate edges with dependencies but, since ourkernels cannot process labels on the arcs, they mustbe associated with tree nodes.
The basic idea of ourstructures is to use (i) one of the three kinds of infor-mation above as central node, from which depen-be::v?
::.width::nof::ifield::nfootball::na::dthe::dwhat::wFigure 5: Lexical Only Centered Tree (LOCT)TOP.?
::.NNfield::nNNfootball::nDTa::dINof::iNNwidth::nDTthe::dVBZbe::vWPwhat::wFigure 6: Lexical and PoS-Tag Sequences Tree (LPST)TOP?
::.field::nfootball::na::dof::iwidth::nthe::dbe::vwhat::wFigure 7: Lexical Sequences Tree (LST)dencies are drawn and (ii) all the other informationas features (in terms of additional nodes) attached tothe central nodes.We define three main trees: the PoS-Tag CenteredTree (PCT), e.g.
see Figure 2, where the GR is addedas father and the lexical as a child; the GR CenteredTree (GRCT), e.g.
see Figure 3, where the PoS-Tagsare children of GR nodes and fathers of their associ-ated lexicals; and the Lexical Centered Tree (LCT),e.g.
see Figure 4, in which both GR and PoS-Tag areadded as the rightmost children.TOPROOTP.?
::.PRDNMODPMODNNgoal::nNMODNNhockey::nNMODNNice::nNMODDTan::dINof::iNNdimension::nNMODDTthe::dVBPbe::vSBJWPwhat::wFigure 8: Grammatical Relation Centered Tree of (s2)4.2 Comparative StructuresTo better study the role of the above dependencystructures, especially from a performance perspec-tive, we define additional structures: the LexicalOnly Centered Tree (LOCT), e.g.
see Figure 5,which is an LCT only containing lexical nodes; theLexical and PoS-Tag Sequences Tree (LPST), e.g.see Figure 6, which ignores the syntactic structureof the sentence being a simple sequence of PoS-Tagnodes, where lexicals are simply added as children;and the Lexical Sequence Tree (LST), where onlylexical items are leaves of a single root node.
PTK1039and PSTK applied to it simulates a standard SK andan SK with smoothing, respectively.4.3 Structural FeaturesSection 2 has already described the kind of featuresgenerated by SK, STK and PTK.
However, it isinteresting to analyze what happens when SPTK isapplied.
For example, given the following sentencesyntactically and semantically similar to s1:(s2) What is the dimension of an ice hockey goal?Figure 8 shows the corresponding GRCT, whoselargest PTK fragment shared with the GRTC of s1(Fig.
3) is: (ROOT (SBJ (WP (what::w))) (PRD (NMOD(DT (the::d))) (NN) (NMOD (IN (of::i)) (PMOD (NMOD (DT))(NMOD (NN)) (NN)))) (P (.
(?::.)))).
If smoothing is ap-plied the matching is almost total, i.e.
also the chil-dren: width::n/dimension::n, football::n/hockey::nand field::n/goal::n will be matched (with a smooth-ing equal to the product of their similarities).The matching using LCT is very interesting:without smoothing, the largest subtree is: (be::v(what::w (SBJ) (WP)) (ROOT)); when smoothing is usedonly the fragment (NMOD (NN (ice::n)) will not be partof the match.
This suggests that LCT will probablyreceive the major benefit from smoothing.
Addition-ally, with respect to all the above structures, LCT isthe only one that can produce only lexical fragments,i.e.
paths only composed by similar lexical nodesconstrained by syntactic dependencies.
All the othertrees produce fragments in which lexicals play therole of features of GR or PoS-Tag nodes.5 ExperimentsThe aim of the experiments is to analyze differentlevels of representation, i.e.
structure, for syntacticdependency parses.
At the same time, we comparewith the constituency trees and different kernels toderive the best syntactic paradigm for convolutionkernels.
Most importantly, the role of lexical simi-larity embedded in syntactic structures will be inves-tigated.
For this purpose, we first carry out extensiveexperiments on coarse and fine grained QC and thenwe verify our findings on a completely different task,i.e.
Argument Classification in SRL.5.1 General experimental setupTools: for SVM learning, we extended the SVM-LightTK software3 (Moschitti, 2006a) (which in-3http://disi.unitn.it/moschitti/Tree-Kernel.htmcludes structural kernels in SVMLight (Joachims,2000)) with the smooth match between tree nodes.For generating constituency trees, we used the Char-niak parser (Charniak, 2000) whereas we appliedLTH syntactic parser (described in (Johansson andNugues, 2008a)) to generate dependency trees.Lexical Similarity: we used the Eq.
1 with ?1 =?2 = 1 and ?
is derived with both approaches de-scribed in Sec.
2.3.
The first approach is LSA-based:LSA was applied to ukWak (Baroni et al, 2009),which is a large scale document collection made by2 billion tokens.
More specifically, to build the ma-trix M, POS tagging is first applied to build rowswith pairs ?lemma, ::POS?, or lemma::POS in brief.The contexts of such items are the columns of Mand are short windows of size [?3,+3], centered onthe items.
This allows for better capturing syntacticproperties of words.
The most frequent 20,000 itemsare selected along with their 20k contexts.
The en-tries of M are the point-wise mutual information be-tween them.
The SVD reduction is then applied toM, with a dimensionality cut of l = 250.
The sec-ond approach uses the similarity based on word list(WL) as provided in (Li and Roth, 2002).Models: SVM-LightTK is applied to the differenttree representations discussed in Section 4.
SincePTK and SPTK are typically used in our experi-ments, to have a more compact acronym for eachmodel, we associate the latter with the name of thestructure, i.e.
this indicates that PTK is applied toit.
Then the presence of the subscript WL and LSAindicates that SPTK is applied along with the corre-sponding similarity, e.g.
LCTWL is the SPTK ker-nel applied to LCT structure, using WL similarity.We experiment with multi-classification, which wemodel through one-vs-all scheme by selecting thecategory associated with the maximum SVM mar-gin.
The quality of such classification is measuredwith accuracy.
We determine the statistical signi-cance by using the model described in (Yeh, 2000)and implemented in (Pado?, 2006).The parameterization of each classifier is carried ona held-out set (30% of the training) and concernswith the setting of the trade-off parameter (option -c) and the Leaf Weight (LeW ) (see Sec.
5.2), whichis used to linearly scale the contribution of the leafnodes.
In contrast, the cost-factor parameter of theSVM-LightTK is set as the ratio between the num-104080%82%84%86%88%90%92%0 1000 2000 3000 4000 5000AccuracyNumber of ExamplesPCT LPST CT LOCT GRCT LCT BOWFigure 9: Learning curves: comparison with no similarity80%82%84%86%88%90%92%94%0 1000 2000 3000 4000 5000AccuracyNumber of ExamplesPCT-WL LPST-WL CT-WL LOCT-WL GRCT-WL LCT-WL PCTFigure 10: Learning curves: comparison with similarityber of negative and positive examples for attemptingto have a balanced Precision/Recall.5.2 QC experimentsFor these experiments, we used the UIUC dataset(Li and Roth, 2002).
It is composed by a trainingset of 5,452 questions and a test set of 500 ques-tions4.
Question classes are organized in two levels:6 coarse-grained classes (like ENTITY or HUMAN)and 50 fine-grained sub-classes (e.g.
Plant, Foodas subclasses of ENTITY).The outcome of the several kernels applied to sev-eral structures for the coarse and fine grained QCis reported in Table 1.
The first column showsthe experimented models, obtained by applyingPTK/SPTK to the structures described in Sec.
4.
Thelast two rows are: CT-STK, i.e.
STK applied to aconstituency tree and BOW, which is a linear ker-4http://cogcomp.cs.illinois.edu/Data/QA/QC/nel applied to lexical vectors.
Column 2, 3 and 4report the accuracy using no, LSA and WL similar-ity, where LeW is the amplifying parameter, i.e.
aweight associated with the leaves in the tree.
Thelast three columns refer to the fine grained task.It is worth nothing that when no similarity is ap-plied: (i) BOW produces high accuracy, i.e.
88.8%but it is improved by STK (the current state-of-the-art5 in QC (Zhang and Lee, 2003; Moschitti et al,2007)); (ii) PTK applied to the same tree of STKproduces a slightly lower value (non-statisticallysignificant difference); (iii) interestingly, when PTKis instead applied to dependency structures, it im-proves STK, i.e.
91.60% vs 91.40% (although notsignificantly); and (iv) LCT, strongly based on lexi-cal nodes, is the least accurate, i.e 90.80% since it isobviously subject to data sparseness (fragments onlycomposed by lexicals are very sparse).The very important results can be noted when lex-ical similarity is used, i.e.
SPTK is applied: (a) allthe syntactic-base structures using both LSA or WLimprove the classification accuracy.
(b) CT gets thelowest improvement whereas LCT achieves an im-pressive result of 94.80%, i.e more than 41% of rel-ative error reduction.
It seems that the lexical similarpaths when driven by syntax produces accurate fea-tures.
Indeed, when syntax is missing such as for theunstructured lexical path of LSTLSA, the accuracydoes not highly improve or may also decrease.
Ad-ditionally, the result of our best model is so high thatits errors only refer to questions like What did JesseJackson organize ?, where the classifier selected En-tity instead ofHuman category.
These refer to clearcases where a huge amount of background knowl-edge is needed for deriving the exact solution.Finally, on the fine grained experiments LCTstill produces the most accurate outcome again ex-ceeding the state-of-the-art (Zhang and Lee, 2003),where WL significantly improves on all models (CTincluded).5.3 Learning curvesIt is interesting to study the impact of syntac-tic/semantic kernels on the learning generalization.For this purpose, Fig.
9 reports the learning curve5Note that in (Bloehdorn and Moschitti, 2007b), higher ac-curacy values for smoothed STK are shown for different param-eters but the best according to a validation set is not highlighted.1041COARSE FINENO LSA WL NO LSA WLLeW Acc.
LeW Acc.
LeW Acc.
LeW Acc.
LeW Acc.
LeW Acc.CT 4 90.80% 2 91.00% 5 92.20% 4 84.00% 5 83.00% 7 86.60%GRCT 3 91.60% 4 92.60% 2 94.20% 3 83.80% 4 83.20% 2 85.00%LCT 1 90.80% 1 94.80% 1 94.20% 0.33 85.40% 1 86.20% 0.33 87.40%LOCT 1 89.20% 1 93.20% 1 91.80% 1 85.40% 1 86.80% 1 87.00%LST 1 88.20% 1 85.80% 1 89.60% 1 84.00% 1 80.00% 1 85.00%LPST 3 89.40% 1 89.60% 1 92.40% 3 84.20% 4 82.20% 1 84.60%PCT 4 91.20% 4 92.20% 5 93.40% 4 84.80% 5 84.00% 5 85.20%CT-STK - 91.20% - - - - - 82.20% - - - -BOW - 88.80% - - - - - 83.20% - - - -Table 1: Accuracy of structural several kernels on different structures for coarse and fine grained QCy = 0.051x2.005y = 0.030x1.609y = 0.068x1.213y = 0.081x1.7050204060801001200 10 20 30 40 50 60microsecondsNumber of NodesLPST-WL GRCT-WL GRCT LCT-WL LCT LPSTFigure 11: Micro-seconds for each kernel computationof the previous models without lexical similaritywhereas Fig.
10 shows the complete SPTK behaviorthrough the different structures.
We note that whenno similarity is used the dependency trees bettergeneralize than constituency trees or non-syntacticstructures like LPST or BOW.
When WL is acti-vated, all models outperform the best kernel of theprevious pool, i.e.
PCT (see dashed line of Fig.
10or the top curve in Fig.
9).5.4 Kernel EfficiencyWe plotted the average running time of each compu-tation of PTK/SPTK applied to the different struc-tures.
We divided the examples from QC basedon the number of nodes in each example.
Fig-ure 11 shows the elapsed time in function of thenumber of nodes for different tree representations.We note that: (i) when the WL is not active, LCTand GRCT are very fast as they impose hierarchicalmatching of subtrees; (ii) when the similarity is ac-tivated, LCTWL and GRCTWL tend to match manymore tree fragments thus their complexity increases.However, the equations of the curve fit, shown in thefigure, suggests that the trend is sub-quadratic (x1.7).Only LPSTWL, which has no structure, matches avery large number of sequences of nodes, when thesimilarity is active.
This increases the complexity,which results in an order higher than 2.5.5 FrameNet Role Classification ExperimentsTo verify that our findings are general and that oursyntactic/semantic dependency kernels can be effec-tively exploited for diverse NLP tasks, we experi-mented with a completely different application, i.e.FrameNet SRL classification (gold standard bound-aries).
We used the FrameNet version 1.3 withthe 90/10% split between training and test set (i.e271,560 and 30,173 examples respectively), as de-fined in (Johansson and Nugues, 2008b), one of thebest system for FrameNet parsing.
We used the LTHdependency parser.
LSA was applied to the BNCcorpus, the source of the FrameNet annotations.For each of 648 frames, we applied SVM alongwith the best models for QC, i.e.
GRCT and LCT, tolearn its associated binary role classifiers (RC) fora total of 4,254 classifiers.
For example, Figure 12shows the LCT representation of the first two rolesof the following sentence:[Bootleggers]CREATOR, then copy [the film]ORIGINAL[onto hundreds of V HS tapes]GOALTable 2 shows the results of the different multi-classifiers.
GRCT and LCT show a large ac-curacy, i.e.
87.60.
This improves up to 88.74by activating the LSA similarity.
The combina-tion GRCTLSA+LCTLSA significantly improves theabove model, achieving 88.91%.
This is very closeto the state-of-the-art of SRL for classification (us-ing a single classifier, i.e.
no joint model), i.e.89.6%, achieved in (Johansson and Nugues, 2008b).1042copy::vVBPROOTbootlegger::nNNSSBJcopy::vVBPROOTfilm::nNNOBJthe::dDTNMODFigure 12: LCT Examples for argument rolesKernel AccuracyGRCT 87.60%GRCTLSA 88,61%LCT 87.61%LCTLSA 88.74%GRCT + LCT 87.99%GRCTLSA + LCTLSA 88.91%Table 2: Argument Classification AccuracyFinally, it should be noted that, to learn and test theSELF MOTION multi-classifier, containing 14,584examples, distributed on 22 roles, SVM-SPTK em-ployed 1.5 h and 10 minutes, respectively6.6 Final Remarks and ConclusionIn this paper, we have proposed a study on repre-sentation of dependency structures for the design ofeffective structural kernels.
Most importantly, wehave defined a new class of kernel functions, i.e.
SP-TKs, that carry out syntactic and lexical similaritieson the above structures.
SPTK exploits the latterby providing generalization trough lexical similar-ities constrained in them.
This allows for automat-ically generating feature spaces of generalized syn-tactic/semantic dependency substructures.To test our models, we carried out experimentson QC and SRL.
These show that by exploiting thesimilarity between two sets of words carried out ac-cording to their dependency structure leads to an un-precedented result for QC, i.e.
94.8% of accuracy.In contrast, when no structure is used the accuracydoes not significantly improves.
We have also pro-vided a fast algorithm for the computation of SPTKand empirically shown that it can easily scale.It should be noted that our models are not abso-lutely restricted to QC and SRL.
Indeed, since mostof the NLP applications are based on syntactic andlexical representations, SPTK will have a major im-pact in most of them, e.g.
:6Using one of the 8 processors of an Intel(R) Xeon(R) CPUE5430 @ 2.66GHz machine, 32Gb Ram.?
Question Answering, the high results for QCwill positively impact on the overall task.?
SRL, SPTK alone reaches the state-of-the-art(SOA) (only 0.7% less) in FrameNet role clas-sification.
This is very valuable as previouswork showed that tree kernels (TK) alone per-form lower than models based on manually en-gineered features for SRL tasks, e.g., (Mos-chitti, 2004; Giuglea and Moschitti, 2004; Giu-glea and Moschitti, 2006; Moschitti, 2006b;Che et al, 2006; Moschitti et al, 2008).
Thusfor the first time in an SRL task, a generaltree kernel reaches the same accuracy of heavymanual feature design.
This also suggests animprovement when used in combinations withmanual feature vectors.?
Relation Extraction and Pronominal Corefer-ence, whose state-of-the-art for some tasks isachieved with the simple STK-CT (see (Zhanget al, 2006) and (Yang et al, 2006; Versley etal., 2008), respectively).?
In word sense disambiguation tasks, SPTK cangeneralize context according to syntactic andsemantic constraints (selectional restrictions)making very effective distributional semanticapproaches.?
In Opinion Mining SPTK will allow to matchsentiment words within their correspondingsyntactic counterparts and improve the state-of-the-art (Johansson and Moschitti, 2010b; Jo-hansson and Moschitti, 2010a).?
Experiments on Recognizing Textual Entail-ment (RTE) tasks, the use of SSTK (in-stead of STK-CT) improved the state-of-the-art(Mehdad et al, 2010).
SPTK may provide fur-ther enhancement and innovative and effectivedependency models.The above points also suggest many promising fu-ture research directions, which we would like to ex-plore.AcknowledgementsThis work has been partially supported by the ECproject FP247758: Trustworthy Eternal Systems viaEvolving Software, Data and Knowledge (EternalS).1043ReferencesMarco Baroni, Silvia Bernardini, Adriano Ferraresi, andEros Zanchetta.
2009.
The wacky wide web: acollection of very large linguistically processed web-crawled corpora.
Language Resources and Evalua-tion, 43(3):209?226.Roberto Basili, Marco Cammisa, and Alessandro Mos-chitti.
2005.
Effective use of WordNet semanticsvia kernel-based learning.
In Proceedings of CoNLL-2005, pages 1?8, Ann Arbor, Michigan.
Associationfor Computational Linguistics.Stephan Bloehdorn and Alessandro Moschitti.
2007a.Combined syntactic and semantic kernels for text clas-sification.
In Proceedings of ECIR 2007, Rome, Italy.Stephan Bloehdorn and Alessandro Moschitti.
2007b.Structure and semantics for expressive text kernels.
InIn Proceedings of CIKM ?07.Stephan Bloehdorn, Roberto Basili, Marco Cammisa, andAlessandro Moschitti.
2006.
Semantic kernels for textclassification based on topological measures of featuresimilarity.
In Proceedings of ICDM 06, Hong Kong,2006.Ulrik Brandes.
2001.
A Faster Algorithm for Between-ness Centrality.
Journal of Mathematical Sociology,25:163?177.Alexander Budanitsky and Graeme Hirst.
2006.
Eval-uating WordNet-based measures of semantic distance.Computational Linguistics, 32(1):13?47.Razvan Bunescu and Raymond Mooney.
2005.
A short-est path dependency kernel for relation extraction.
InProceedings of HLT and EMNLP, pages 724?731,Vancouver, British Columbia, Canada, October.Horst Bunke and Kim Shearer.
1998.
A graph distancemetric based on the maximal common subgraph.
Pat-tern Recogn.
Lett., 19(3-4):255?259, March.Nicola Cancedda, Eric Gaussier, Cyril Goutte, andJean Michel Renders.
2003.
Word sequence kernels.Journal of Machine Learning Research, 3:1059?1082.O.
Chapelle, B. Schlkopf, and A. Zien.
2006.
Semi-Supervised Learning.
Adaptive computation and ma-chine learning.
MIT Press, Cambridge, MA, USA, 09.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proceedings of NAACL?00.Wanxiang Che, Min Zhang, Ting Liu, and Sheng Li.2006.
A hybrid convolution tree kernel for semanticrole labeling.
In Proceedings of the COLING/ACL onMain conference poster sessions, COLING-ACL ?06,pages 73?80, Stroudsburg, PA, USA.
Association forComputational Linguistics.Michael Collins and Nigel Duffy.
2002.
New Rank-ing Algorithms for Parsing and Tagging: Kernels overDiscrete Structures, and the Voted Perceptron.
In Pro-ceedings of ACL?02.Courtney Corley and Rada Mihalcea.
2005.
Measur-ing the semantic similarity of texts.
In Proceedings ofthe ACL Workshop on Empirical Modeling of SemanticEquivalence and Entailment, pages 13?18, Ann Arbor,Michigan, June.
Association for Computational Lin-guistics.Jim Cowie, Joe Guthrie, and Louise Guthrie.
1992.
Lex-ical disambiguation using simulated annealing.
In inCOLING, pages 359?365.Nello Cristianini, John Shawe-Taylor, and Huma Lodhi.2001.
Latent semantic kernels.
In Carla Brodley andAndrea Danyluk, editors, Proceedings of ICML-01,18th International Conference on Machine Learning,pages 66?73, Williams College, US.
Morgan Kauf-mann Publishers, San Francisco, US.Aron Culotta and Jeffrey Sorensen.
2004.
Dependencytree kernels for relation extraction.
In Proceedings ofACL, pages 423?429, Barcelona, Spain, July.Chad Cumby and Dan Roth.
2003.
Kernel Methods forRelational Learning.
In Proceedings of ICML 2003.Hal Daume?
III and Daniel Marcu.
2004.
Np bracketingby maximum entropy tagging and SVM reranking.
InProceedings of EMNLP?04.Jason V. Davis, Brian Kulis, Prateek Jain, Suvrit Sra, andInderjit S. Dhillon.
2007.
Information-theoretic met-ric learning.
In Proceedings of the 24th internationalconference on Machine learning, ICML ?07, pages209?216, New York, NY, USA.
ACM.Linton C. Freeman.
1977.
A Set of Measures of Central-ity Based on Betweenness.
Sociometry, 40(1):35?41.Hagen Fu?rstenau and Mirella Lapata.
2009.
Graph align-ment for semi-supervised semantic role labeling.
InIn Proceedings of EMNLP ?09, pages 11?20, Morris-town, NJ, USA.Ana-Maria Giuglea and Alessandro Moschitti.
2004.Knowledge Discovering using FrameNet, VerbNet andPropBank.
In In Proceedings of the Workshop on On-tology and Knowledge Discovering at ECML 2004,Pisa, Italy.A.-M. Giuglea and A. Moschitti.
2006.
Semantic rolelabeling via framenet, verbnet and propbank.
In Pro-ceedings of ACL, Sydney, Australia.Alfio Gliozzo, Claudio Giuliano, and Carlo Strapparava.2005.
Domain kernels for word sense disambiguation.In Proceedings of ACL?05, pages 403?410.G.
Golub and W. Kahan.
1965.
Calculating the singularvalues and pseudo-inverse of a matrix.
Journal of theSociety for Industrial and Applied Mathematics: Se-ries B, Numerical Analysis, 2(2):pp.
205?224.Zellig Harris.
1964.
Distributional structure.
In Jer-rold J. Katz and Jerry A. Fodor, editors, The Philos-ophy of Linguistics.
Oxford University Press.1044J.
J. Jiang and D. W. Conrath.
1997.
Semantic SimilarityBased on Corpus Statistics and Lexical Taxonomy.
InInternational Conference Research on ComputationalLinguistics (ROCLING X).T.
Joachims.
2000.
Estimating the generalization per-formance of a SVM efficiently.
In Proceedings ofICML?00.Richard Johansson and Alessandro Moschitti.
2010a.Reranking models in fine-grained opinion analysis.
InProceedings of the 23rd International Conference ofComputational Linguistics (Coling 2010), pages 519?527, Beijing, China.Richard Johansson and Alessandro Moschitti.
2010b.Syntactic and semantic structure for opinion expres-sion detection.
In Proceedings of the Fourteenth Con-ference on Computational Natural Language Learn-ing, pages 67?76, Uppsala, Sweden.Richard Johansson and Pierre Nugues.
2008a.Dependency-based syntactic?semantic analysis withPropBank and NomBank.
In CoNLL 2008: Proceed-ings of the Twelfth Conference on Natural LanguageLearning, pages 183?187, Manchester, United King-dom.Richard Johansson and Pierre Nugues.
2008b.
The effectof syntactic representation on semantic role labeling.In Proceedings of COLING, Manchester, UK, August18-22.Taku Kudo and Yuji Matsumoto.
2003.
Fast methods forkernel-based text analysis.
In Proceedings of ACL?03.Taku Kudo, Jun Suzuki, and Hideki Isozaki.
2005.Boosting-based parse reranking with subtree features.In Proceedings of ACL?05.Claudia Leacock and Martin Chodorow, 1998.
Combin-ing Local Context and WordNet Similarity for WordSense Identification, chapter 11, pages 265?283.
TheMIT Press.X.
Li and D. Roth.
2002.
Learning question classifiers.In Proceedings of ACL?02.Yashar Mehdad, Alessandro Moschitti, and Fabio Mas-simo Zanzotto.
2010.
Syntactic/semantic structuresfor textual entailment recognition.
In HLT-NAACL,pages 1020?1028.Rada Mihalcea, Courtney Corley, and Carlo Strappar-ava.
2005.
Corpus-based and knowledge-based mea-sures of text semantic similarity.
In Proceedings of theAmerican Association for Artificial Intelligence (AAAI2006), Boston, July.Rada Mihalcea.
2005. unsupervised large-vocabularyword sense disambiguation with graph-based algo-rithms for sequence data labeling.
In In HLT/EMNLP2005, pages 411?418.Alessandro Moschitti, Silvia Quarteroni, Roberto Basili,and Suresh Manandhar.
2007.
Exploiting syntacticand shallow semantic kernels for question/answer clas-sification.
In Proceedings of ACL?07.Alessandro Moschitti, Daniele Pighin, and RobertoBasili.
2008.
Tree kernels for semantic role labeling.Computational Linguistics, 34(2):193?224.A.
Moschitti.
2004.
A study on convolution kernelsfor shallow semantic parsing.
In Proceedings of ACL,Barcelona, Spain.Alessandro Moschitti.
2006a.
Efficient convolution ker-nels for dependency and constituent syntactic trees.
InProceedings of ECML?06, pages 318?329.Alessandro Moschitti.
2006b.
Making tree kernels prac-tical for natural language learning.
In Proccedings ofEACL?06.Roberto Navigli and Mirella Lapata.
2010.
An Experi-mental Study of Graph Connectivity for UnsupervisedWord Sense Disambiguation.
IEEE Transactions onPattern Analysis and Machine Intelligence, 32(4):678?692.Sebastian Pado and Mirella Lapata.
2007.
Dependency-based construction of semantic space models.
Compu-tational Linguistics, 33(2).Sebastian Pado?, 2006.
User?s guide to sigf: Signifi-cance testing by approximate randomisation.Ted Pedersen, Siddharth Patwardhan, and Jason Miche-lizzi.
2004a.
WordNet::Similarity - Measuring the Re-latedness of Concept.
In Proc.
of 5th NAACL, Boston,MA.Ted Pedersen, Siddharth Patwardhan, and Jason Miche-lizzi.
2004b.
Wordnet::similarity - measuring the re-latedness of concepts.
In Daniel Marcu Susan Du-mais and Salim Roukos, editors, HLT-NAACL 2004:Demonstration Papers, pages 38?41, Boston, Mas-sachusetts, USA, May 2 - May 7.
Association forComputational Linguistics.Philip Resnik.
1995.
Using information content to eval-uate semantic similarity in a taxonomy.
In In Proceed-ings of the 14th International Joint Conference on Ar-tificial Intelligence, pages 448?453.Magnus Sahlgren.
2006.
The Word-Space Model.
Ph.D.thesis, Stockholm University.Hinrich Schtze.
1998.
Automatic word sense discrimi-nation.
Journal of Computational Linguistics, 24:97?123.John Shawe-Taylor and Nello Cristianini.
2004.
KernelMethods for Pattern Analysis.
Cambridge UniversityPress.Libin Shen, Anoop Sarkar, and Aravind k. Joshi.
2003.Using LTAG Based Features in Parse Reranking.
InEmpirical Methods for Natural Language Processing(EMNLP), pages 89?96, Sapporo, Japan.Georges Siolas and Florence d?Alch Buc.
2000.
Sup-port vector machines based on a semantic kernel for1045text categorization.
In Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Net-works (IJCNN?00)-Volume 5, page 5205.
IEEE Com-puter Society.Ivan Titov and James Henderson.
2006.
Porting statisti-cal parsers with data-defined kernels.
In Proceedingsof CoNLL-X.Kristina Toutanova, Penka Markova, and ChristopherManning.
2004.
The Leaf Path Projection View ofParse Trees: Exploring String Kernels for HPSG ParseSelection.
In Proceedings of EMNLP 2004.Yannick Versley, Alessandro Moschitti, Massimo Poe-sio, and Xiaofeng Yang.
2008.
Coreference sys-tems based on kernels methods.
In The 22nd Interna-tional Conference on Computational Linguistics (Col-ing?08), Manchester, England.Zhibiao Wu and Martha Palmer.
1994.
Verb semanticsand lexical selection.
In 32nd.
Annual Meeting of theAssociation for Computational Linguistics, pages 133?138, New Mexico State University, Las Cruces, NewMexico.Xiaofeng Yang, Jian Su, and Chewlim Tan.
2006.Kernel-based pronoun resolution with structured syn-tactic knowledge.
In Proc.
COLING-ACL 06.Alexander S. Yeh.
2000.
More accurate tests for the sta-tistical significance of result differences.
In COLING,pages 947?953.Dmitry Zelenko, Chinatsu Aone, and AnthonyRichardella.
2002.
Kernel methods for relationextraction.
In Proceedings of EMNLP-ACL, pages181?201.Dell Zhang and Wee Sun Lee.
2003.
Question classifica-tion using support vector machines.
In Proceedings ofthe 26th annual international ACM SIGIR conferenceon Research and development in informaion retrieval,pages 26?32.
ACM Press.Min Zhang, Jie Zhang, and Jian Su.
2006.
Explor-ing Syntactic Features for Relation Extraction using aConvolution tree kernel.
In Proceedings of NAACL.Peixiang Zhao, Jiawei Han, and Yizhou Sun.
2009.
P-Rank: a comprehensive structural similarity measureover information networks.
In CIKM ?09: Proceed-ing of the 18th ACM conference on Information andknowledge management, pages 553?562, New York,NY, USA.
ACM.1046
