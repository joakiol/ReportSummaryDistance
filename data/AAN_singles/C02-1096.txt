Wordform- and class-based prediction of the componentsof German nominal compounds in an AAC systemMarco Baroni Johannes MatiasekAustrian Research Institute forArtificial IntelligenceSchottengasse 3,A-1010 Vienna, Austria{marco,john}@oefai.atHarald TrostDepartment of Medical Cybernetics andArtificial Intelligence, University of ViennaFreyung 6/2A-1010 Vienna, Austriaharald@ai.univie.ac.atAbstractIn word prediction systems for augmentative and al-ternative communication (AAC), productive word-formation processes such as compounding pose aserious problem.
We present a model that predictsGerman nominal compounds by splitting them intotheir modifier and head components, instead of try-ing to predict them as a whole.
The model is im-proved further by the use of class-based modifier-head bigrams constructed using semantic classesautomatically extracted from a corpus.
The eval-uation shows that the split compound model withclass bigrams leads to an improvement in keystrokesavings of more than 15% over a no split compoundbaseline model.
We also present preliminary resultsobtained with a word prediction model integratingcompound and simple word prediction.1 IntroductionN-gram language modeling techniques have beensuccessfully embedded in a number of natural lan-guage processing applications, including word pre-dictors for augmentative and alternative communi-cation (AAC).
N-gram based techniques rely cru-cially on the assumption that the large majority ofwords to be predicted have also occurred in the cor-pus used to train the models.Productive word-formation by compounding inlanguages such as German, Dutch, the Scandina-vian languages and Greek, where compounds arecommonly written as single orthographic words, isproblematic for this assumption.Productive compounding implies that a sizeablenumber of new words will constantly be added tothe language.
Such words cannot, in principle, becontained in any already existing training corpus,no matter how large.
Moreover, the training cor-pus itself is likely to contain a sizeable number ofnewly formed compounds that, as such, will havean extremely low frequency, causing data sparse-ness problems.New compounds, however, differ from othertypes of new/rare words in that, while they are rare,they can typically be decomposed into more com-mon smaller units (the words that were put togetherto form them).
For example, in the corpus we an-alyzed, Abend ?evening?
and Sitzung ?session?, thetwo components of the German compound Abend-sitzung ?evening session?, are much more frequentwords than the latter.
Thus, a natural way to handleproductively formed compounds is to treat them notas primitive units, but as the concatenation of theircomponents.A model of this sort will be able to predict newlyformed compounds that never occurred in the train-ing corpus, as long as they can be analyzed as theconcatenation of constituents that did occur in thetraining corpus.
Moreover, a model of this sortavoids the specific type of data sparseness problemscaused by newly formed compounds in the trainingcorpus, since it collects statistics based on their (typ-ically more frequent) components.Building upon previous work (Spies, 1995;Carter et al, 1996; Fetter, 1998; Larson et al,2000), Baroni et al (2002) reported encouragingresults obtained with a model in which two-elementnominal German compounds are predicted by treat-ing them as the concatenation of a modifier (left el-ement) and a head (right element).Here, we report of further improvements to thismodel that we obtained by adding a class-based bi-gram term to head prediction.
As far as we know,this it the first time that semantic classes auto-matically extracted from the training corpus havebeen used to enhance compound prediction, inde-pendently of the domain of application of the pre-diction model.Moreover, we present the results of preliminaryexperiments we conducted in the integration ofcompound predictions and simple word predictionswithin the AAC word prediction task.The remainder of this paper is organized as fol-lows.
In section 2, we describe the AAC word pre-diction task.
In section 3, we describe the basicproperties of German compounds.
In section 4, wepresent our split compound prediction model, focus-ing on the new class-based head prediction compo-nent.
In section 5, we report the results of simu-lations run with the enhanced compound predictionmodel.
In section 6, we report about our prelimi-nary experiments with the integration of compoundand simple word prediction.
Finally, in section 7,we summarize the main results we obtained and in-dicate directions for further work.2 Word prediction for AACWord prediction systems based on n-gram statisticsare an important component of AAC devices, i.e.,software and possibly hardware typing aids for dis-abled users (Copestake, 1997; Carlberger, 1998).Word predictors provide the user with a predic-tion window, i.e.
a menu that, at any time, lists themost likely next word candidates, given the inputthat the user has typed until the current character.If the word that the user intends to type next is inthe prediction window, the user can select it fromthere.
Otherwise, the user will keep typing letters,until the target word appears in the prediction win-dow (or until she finishes typing the word).The (percentage) keystroke savings rate (ksr) is astandard measure used in AAC research to evaluateword predictors.
The ksr can be thought of as thepercentage of keystrokes that a ?perfect?
user wouldsave by employing the relevant word predictor totype the test set, over the total number of keystrokesthat are needed to type the test set without using theword predictor.Usually, the ksr is defined byksr = (1?
ki + kskn) ?
100 (1)where: ki is the number of input characters actuallytyped, ks is the number of keystrokes needed to se-lect among the predictions presented by the modeland kn is the number of keystrokes that would beneeded if the whole text was typed without anyprediction aid.
Typically, the user will need onekeystroke to select among the predictions , and thuswe assume that ks equals 1.11In the split compound model, the user needs one keystroketo select the modifier and one keystroke to select the head.The ksr is influenced not only by the quality ofthe prediction model but also by the size of the pre-diction window.
In our simulations, we use a 7 wordprediction window.Ksr is not a function of perplexity, but it is gener-ally true that there is an inverse correlation betweenksr and perplexity (Carlberger, 1998).3 Compounding in GermanCompounding is an extremely common and produc-tive mean to form words in German.In an analysis of the APA newswire corpus (acorpus of over 28 million words), we found that al-most half (47%) of the word types were compounds.However, the compounds accounted for a small por-tion of the overall token count (7%).
This suggeststhat, as expected, many of them are productivelyformed hapax legomena or very rare words (83%of the compounds had a corpus frequency of 5 orlower).By far the most common type of German com-pound is the N+N type, i.e., a sequence of twonouns (62% of the compounds in our corpus havethis shape).
Thus, we decided to limit ourselves, fornow, to handling compounds of this shape.In German, nominal compounds, including theN+N type, are right-headed, i.e., the rightmost ele-ment of the compound determines its basic semanticand morphosyntactic properties.Thus, the context of a compound is often moreinformative about its right element (the head) thanabout its left element (the modifier).In modifier context, nouns are sometimes fol-lowed by a linking suffix (Krott, 2001; Dressler etal., 2001), or they take other special inflectionalshapes.As a consequence of the presence of linking suf-fixes and related patterns, the forms that nouns takein modifier position are sometimes specific to thisposition only, i.e., they are bound forms that do notoccur as independent words.We did not parse special modifier forms in or-der to reconstruct their independent nominal forms.Thus, we treat all inflected modifier forms, includ-ing bound forms, as unanalyzed primitive nominalwordforms.4 The split compound prediction modelIn Baroni et al (2002), we present and evaluate asplit compound model in which N+N compoundsare predicted by treating them as the sequence of amodifier and a head.Modifiers are predicted on the basis of weighedprobabilities deriving from the following threeterms: the unigram and bigram training corpus fre-quency of nominal wordforms as modifiers or in-dependent words, and the training corpus type fre-quency of nominal wordforms as modifiers:2Pmod(w) = ?1P (w) + ?2P (w|c) + ?3Pismod(w) (2)The type frequency of nouns as modifiers is de-termined by the number of distinct compounds inwhich a noun form occurs as modifier.Heads are predicted on the basis of weightedprobabilities deriving from three terms analogous tothe ones used for modifiers: the unigram and bigramfrequency of nouns as heads or independent words,and the type frequency of nouns as heads:Phead(w) = ?1P (w) +?2P (w|c) +?3Pishead(w) (3)The type frequency of nouns as heads is de-termined by the number of distinct compounds inwhich a noun form occurs as head.Given that compound heads determine the syn-tactic properties of compounds, bigrams for headprediction are collected by considering not the im-mediate left context of heads (i.e., their modifiers),but the word preceding the compound (e.g., dieAbendsitzung is counted as an instance of the bi-gram die Sitzung).For reasons of size and efficiency, single uni- andbigram count lists are used for predicting modifiersand heads.3 For the same reasons, and to minimizethe chances of over-fitting to the training corpus, alln-gram/frequency tables are trimmed by removingelements that occur only once in the training corpus.We currently use a simple interpolation model, inwhich all terms are assigned equal weight.4.1 Improving head predictionWhile we obtained encouraging results with it (Ba-roni et al, 2002), we feel that a particularly unsat-isfactory aspect of the model described in the previ-ous section is that information on the modifier is not2Here and below, c stands for the last word in the left con-text of w; w is the suffix of the word to be predicted minusthe (possibly empty) prefix typed by the user up to the currentpoint.3This has a distorting effect on the bigram counts (wordsoccurring before compounds are counted twice, once as the leftcontext of the modifier and once as the left context of the head).However, preliminary experiments indicated that the empiricaleffect of this distortion is minimal.exploited when trying to predict the head of a com-pound.
Intuitively, knowing what the modifier isshould help us in guessing the head of a compound.However, constructing a plausible head-predictionterm based on modifier-head dependencies is notstraightforward.The word-form-based compound-bigram fre-quency of a head, i.e., the number of times a specifichead occurs after a specific modifier, is not a veryuseful measure: Counting how often a modifier-head pair occurs in the training corpus is equiv-alent to collecting statistics on unanalyzed com-pounds, and it will not help us to generalize beyondthe compounds encountered in the training corpus.Moreover, if a specific modifier-head bigram is fre-quent, i.e., the corresponding compound is a fre-quent word, it is probably better to treat the wholecompound as an unanalyzed lexical unit anyway.POS-based head-modifier bigrams are not goingto be of any help either, since we are consideringonly N+N compounds, and thus we would collect asingle POS bigram (N N) with probability 1.4We decided instead to try to exploit asemantically-driven route.
It seems plausiblethat modifiers that are semantically related willtend to co-occur with heads that are, in turn,semantically related.
Consider for example therelationship between the class of fruits and theclass of sweets in English compounds.
It is easyto think of compounds in which a member ofthe class of fruits (bananas, cherries, apricots...)modifies a member of the class of sweets (pies,cakes, muffins...).
Thus, if you have to predict thehead of a compound given a fruit modifier, it wouldbe reasonable, all else being equal, to guess somekind of sweet.4.1.1 Class-based modifier-head bigramsWhile semantically-driven prediction makes sensein principle, clustering nouns into semantic classesis certainly not a trivial job, and, if a large input lex-icon must be partitioned, it is not a task that couldbe accomplished by a human expert.
Drawing inspi-ration from Brown et al (1990), we constructed in-stead semantic classes using a clustering algorithmextracting them from a corpus, on the basis of theaverage mutual information (MI) between pairs ofwords (Rosenfeld, 1996).54Even if the model handled other compound types, very fewPOS combinations are attested within compounds.5We are aware of the fact that other measures of lexical as-sociation have been proposed (Evert and Krenn, 2001, andMI values were computed using Adam Berger?strigger toolkit (Berger, 1997).6 The same trainingcorpus of about 25.5M words (and with N+N com-pounds split) that we describe below was used tocollect MI values for noun pairs.
All modifiers andheads of N+N compounds and all corpus words thatwere parsed as nouns by the Xerox morphologicalanalyzer (Karttunen et al, 1997) were counted asnouns for this purpose.MI was computed only for pairs that co-occurredat least three times in the corpus (thus, only a subsetof the input nouns appears in the output list).
Validco-occurrences were bound by a maximal distancebetween elements of 500 words, and a minimal dis-tance of 2 words (to avoid lexicalized phrases, suchas proper names or phrasal loanwords).Having obtained a list of pairs from the toolkit,the next step was to cluster them into classes, bygrouping together nouns with a high MI.
For spacereasons, we do not discuss our clustering algorithmin detail here (we motivate and analyze the algo-rithm in a paper currently in preparation).In short, the algorithm starts by building classesout of nouns that occur with very few other nouns inthe MI pair list, and thus their assignment to classesis relatively unambiguous, and it then adds progres-sively more ambiguous nouns (ambiguous in thesense that they occur in a progressively larger num-ber of MI pairs, and thus it becomes harder to deter-mine with which other nouns they should be clus-tered).
Each input word is assigned to a single class(thus, we do not try to capture polysemy).
More-over, not all words in the input are clustered (seestep 5 below).7Schematically, the algorithm works as follows(the input vocabulary of step 1 is simply a list ofall the words that occur at least once in the MI pairreferences quoted there) and are sometimes claimed to be morereliable than MI, and we are planning to run our clustering al-gorithm using alternative measures.6The trigger toolkit returns directional MI values (i.e., sepa-rate MI values for the pairs N1 N2 and N2 N1).
Since we werenot interested in directional information, we merged pairs con-taining identical nouns by summing their MI.
We realize thatthis is not mathematically equivalent to computing symmetricMI values, but it is a practical approximation that allowed us touse the trigger toolkit for our purposes.7We also experimented with an iterative version of the al-gorithm that tried to cluster all words, through multiple passes.The classes generated by the non-iterative procedure describedin the text, however, gave better results, when integrated in thehead prediction task, than those generated with the iterative ver-sion.list):?
step 1: Rank words in input vocabulary on thebasis of how often they occur in the MI pair list(from least to most frequent);?
step 2: Shift top word from ranked list and de-termine with the members of which existingclass it has the highest average mutual infor-mation;?
step 3: If highest value found in step 2 is 0,assign current word to new class; else, assignit to class corresponding to highest value;?
step 4: If ranked list is not empty, go back tostep 2;?
step 5: Discard all classes that have only onemember.This is a heuristic clustering procedure and thereis no guarantee that it will construct classes thatmaximize MI.
A cursory inspection of the outputlist indicates that most classes constructed by ouralgorithm are intuitively reasonable, while there arealso, undoubtedly, classes that contain heteroge-neous elements, and missed generalizations.
Table1 reports a list of ten randomly selected classes thatwere constructed using this procedure.Alleinstehende, Singles, Alben, Platten, Platte, Sound,Hits, Hit, Live, Songs, Single, Album, Pop, Studio,Rock, Fans, BandAtrophie, Hartung, NeurologeMagische, MagieBilgen, Tivoli, Baur, Scharrer, Streiter, Winkel, Pfeffer,Schmid, MEffizienz, TransparenzHarm, Radar, Jets, Flugzeugen, Typs, Abwehr, Raketen,Maschinen, Angriffen, Flugzeuge, KampfRelegation, Birmingham, StephenPartnerschafts, Partnerschaft, Kooperation,Bereichen, Aktivita?tenImporteure, Zo?lleLabyrinths, LabyrinthTable 1: Randomly selected noun classesThe algorithm generated 3744 classes, containinga total of 14059 nouns (about one third of the nounsin the training corpus).Class-based modifier-head bigrams were thencollected by labeling all the modifiers and heads inthe training corpus with their semantic classes, andcounting how often each combination of modifierand head class occurred.Like the other tables, class-based bigrams weretrimmed by removing elements with a frequency of1.4.1.2 The class-based head prediction modelWe compute the class-based probability of a com-pound head given its modifier in the following way:Pclass(h|m) = P (Cl(h)|Cl(m))P (h|Cl(h)) (4)whereP (Cl(h)|Cl(m)) =count(Cl(m), Cl(h))count(Cl(m))(5)and P (h|Cl(h)) = 1|Cl(h)|(6)The latter term assigns equal probability to allmembers of a class, but lower probability to mem-bers of larger classes.Class-based probability is added to thewordform-based terms of equation 3 obtainingthe following formula to compute head probability:Phead(w) = (7)?1P (w) +?2P (w|c) +?3Pishead(w) +?4Pclass(w|m)5 EvaluationThe new split compound model and a baselinemodel with no compound processing were eval-uated in a series of simulations, using the APAnewswire articles from January to September 1999(containing 25,466,500 words) as the training cor-pus, and all the 90,643 compounds found in theFrankfurter Rundschau newspaper articles fromJune 29 to July 12 of 1992 (in bigram context) asthe testing targets.8In order to train and test the split compoundmodel, all words in both sets were run though themorphological analyzer, and all N+N compoundswere split into their modifier and head surfaceforms.We first ran simulations in which compoundheads were predicted using each of the terms inequation 7 separately.
The results are reported intable 2.As an independent predictor, the class-based termperforms slightly worse than wordform-based bi-gram prediction.We then simulated head and compound predic-tion using the head prediction model of equation 7.8In other experiments, including those reported in Baroniet al (2002), we tested on another section of the APA corpusfrom the same year.
Not surprisingly, ksr?s in the experimentswith the APA corpus were overall higher, and the differencebetween the split compound and baseline models was less dra-matic (because many compounds in the test set were already inthe training corpus).model P (w) P (w|c) Pishead Pclass(w|m)head ksr 42.2 30.0 47.1 29.4Table 2: Predicting heads with single term modelsThe results of this simulation are reported in table3, together with the results of a simulation in whichclass-based prediction was not used, and the re-sults obtained with the baseline no-split-compoundmodel.Model split split no splitw/ classes no classeshead ksr 51.2 48.8 N/Acompound ksr 50.1 48.8 34.9Table 3: Predicting heads and compoundsWhen used in conjunction with the other terms,class bigrams lead to an improvement in head pre-diction of more than 2% over the split compoundmodel without class-based prediction.
This trans-lates into an improvement of 1.3% in the predictionof whole compounds.
Overall, the split compoundmodel with class bigrams leads to an improvementof more than 15% over the baseline model.The results of these experiments confirm the use-fulness of the split compound model, and theyalso show that the addition of class-based predic-tion improves the performance of the model, evenif this improvement is not dramatic.
Clearly, fu-ture research should concentrate on whether alterna-tive measures of association, clustering techniquesand/or integration strategies can make class-basedprediction more effective.6 Preliminary experiments in integrationIn a working word prediction system, compoundsare obviously not the only type of words that theuser needs to type.
Thus, the predictions providedby the compound model must be integrated withpredictions of simple words.
In this section, we re-port preliminary results we obtained with a modellimited to the integration of N+N compound predic-tion with simple noun prediction.In our approach to compound/simple predictionintegration, candidate modifiers are presented to-gether and in competition with simple word so-lutions as soon as the user starts typing a newword.
The user can distinguish modifiers from sim-ple words in the prediction window because the for-mer are suffixed with a special symbol (for exam-ple an underscore).
If the user selects a modifier,the head prediction model is activated, and the usercan start typing the prefix of the desired compoundhead, while the system suggests completions basedon the head prediction model.For example, if the user has just typed Abe,the prediction window could contain, among otherthings, the candidates Abend and Abend .
If theuser selects the latter, possible head completions fora compound having Abend as its modifier are pre-sented.Modifier candidates are proposed on the basis ofPmod(w) computed as in equation 2 above.
Simplenoun candidates are proposed on the basis of theirunigram and bigram probabilities (interpolated withequal weights).We experimented with two versions of the inte-grated model.In one, modifier and simple noun candidates areranked directly on the basis of their probabilities.This risks to lead to over-prediction of modifier can-didates (recall that, from the point of view of tokenfrequency, compounds are much rarer than simplewords; the prediction window should not be clut-tered by too many modifier candidates when, mostof the time, users will want to type simple words).Thus, we constructed a second version of the in-tegrated model in which Pmod(w) is multiplied by apenalty term.
This term discounts the probability ofmodifier candidates built from nominal wordformsthat occur more frequently in the training corpus asindependent nouns than as modifiers (forms that areequally or more frequent in modifier position are notaffected by the penalty).The same training corpus and procedures de-scribed in section 5 above were used to train the twoversions of the integrated model, and the baselinemodel that does not use compound prediction.These models were tested by treating all thenouns in the test corpus as prediction targets.
Theintegrated test set contained 90,643 N+N tokens and395,731 more nouns.
The results of the simulationsare reported in table 4.Model integrated integrated simple predno penalty w/ penalty onlycompound ksr 47.6 45.9 34.9simple n ksr 40.5 42.5 45.6combined ksr 42.5 43.5 42.6Table 4: Integrated predictionBecause of the simple noun predictions getting inthe way, the integrated models perform compoundprediction worse than the non-integrated split com-pound model of table 3.
However the integratedmodels still perform compound prediction consid-erably better than the baseline model.The integrated model with modifier penalties per-forms worse than the model without penalties whenpredicting compounds.
This is expected, since themodifier penalties make this model more conserva-tive in proposing modifier candidates.However, the model with penalties outperformsthe model without penalties in simple noun predic-tion.
Given that in our test set (and, we expect, inmost German texts) simple noun tokens greatly out-number compound tokens, this results in an overallbetter performance of the model with penalties.The integrated model with penalties achievesan overall ksr that is about 1% higher than thatachieved by the baseline model.Thus, these preliminary experiments indicate thatan approach to integrating compound and simpleword predictions along the lines sketched at the be-ginning of this section, and in particular the versionof the model in which modifier predictions are pe-nalized, is feasible.
However, the model is clearly inneed of further refinement, given that the improve-ment over the baseline model is currently minimal.7 ConclusionThe main result concerning German compound pre-diction that was reported in this paper pertains to theintroduction of class-based modifier-head bigramsto enhance head prediction.We presented a procedure to cluster nominalwordforms into semantic classes and to extractclass-based modifier-head bigrams, and then amodel to calculate the class-based probability ofcandidate heads using these bigrams.While we evaluated our system in the contextof the AAC word prediction task, we believe thatthe class-based prediction model we proposed couldbe extended to any other domain in which n-gram-based compound prediction must be performed.The addition of class-based head prediction to thesplit compound model of Baroni et al (2002) leadsto an improvement in head prediction (from a ksr of48.8% to a ksr of 51.2%).
This translates into animprovement of 1.3% in whole compound predic-tion (from 48.8% to 50.1%).
Overall, the split com-pound model with class bigrams led to an improve-ment of more than 15% over a no split compoundbaseline model.This result was presented in the context of theAAC word prediction task, but we believe that theclass-based prediction model we proposed could beextended to any other domain in which n-gram-based compound prediction must be performed.While the results we report are encouraging,the improvement obtained with the addition of theclass-based model is hardly dramatic.
It is clear thatfurther work in this area is required.In particular, we plan to experiment with differentmeasures of association to determine the degree ofrelatedness of words, and with alternative clusteringtechniques.Moreover, we hope to improve the overall perfor-mance of the compound predictor by resorting to abetter interpolation strategy than the uniform weightassignment model we are currently using.We also reported results obtained with a prelim-inary model in which split compound prediction isintegrated with simple noun prediction.
This modeloutperforms the baseline model without compoundprediction, but only of about 1% ksr.
Clearly, fur-ther work in this area is also necessary.
In partic-ular, as suggested by a reviewer, we will try to ex-ploit morpho-syntactic differences between simplenouns and modifiers to help distinguishing betweenthe two types.AcknowledgementsWe would like to thank an anonymous reviewer forhelpful comments and the Austria Presse Agenturfor kindly making the APA corpus available to us.This work was supported by the European Unionin the framework of the IST programme, projectFASTY (IST-2000-25420).
Financial support for?OFAI is provided by the Austrian Federal Ministryof Education, Science and Culture.ReferencesM.
Baroni, J. Matiasek, and H. Trost, ?Predict-ing the Components of German Nominal Com-pounds?, to appear in Proc.
ECAI 2002.A.
Berger: Trigger Toolkit, publicly available soft-ware, 1997.http://www-2.cs.cmu.edu/ aberger/software.htmlP.
Brown, V. Della Pietra, P. DeSouza, J. Lai, andR.
Mercer, ?Class-based n-gram models of nat-ural language?, Computational Linguistics 18(4),pp.467-479, 1990.J.
Carlberger, Design and Implementation of a Prob-abilistic Word Prediction Program, Royal Insti-tute of Technology (KTH), 1998.D.
Carter, J. Kaja, L. Neumeyer, M. Rayner, F.Weng, and M. Wire`n, ?Handling Compounds ina Swedish Speech-Understanding System?, Proc.ICSLP-96.A Copestake, ?Augmented and alternative NLPtechniques for augmentative and alternative com-munication?, Proceedings of the ACL workshopon Natural Language Processing for Communi-cation Aids, 1997.W.
Dressler, G. Libben, J. Stark, C. Pons, and G.Jarema, ?The processing of interfixed Germancompounds?, Yearbook of Morphology 1999, pp.185-220, 2001.S.
Evert and B. Krenn, ?Methods for the Qual-itative Evaluation of Lexical Association Mea-sures?, Proceedings of the 39th Annual Meetingof the Association for Computational Linguistics,Toulouse, France, 2001.P.
Fetter, Detection and Transcription of OOVWords, Verbmobil Report 231, 1998.L.
Karttunen, K. Gal, and A. Kempe, XeroxFinite-State Tool, Xerox Research Centre Europe,Grenoble, 1997.A.
Krott, Analogy in Morphology, Max Planck In-stitute for Psycholinguistics, Nijmegen, 2001.M.
Larson, D. Willett, J. Kohler, and G. Rigoll,?Compound splitting and lexical unit recombi-nation for improved performance of a speechrecognition system for German parliamentaryspeeches?, Proceedings of the 6th Interna-tional Conference of Spoken Language Pro-cessing (ICSLP-2000), October 16-20., Peking,China, 2000.R.
Rosenfeld, ?A Maximum Entropy Approach toAdaptive Statistical Language Modeling?, Com-puter Speech and Language 10, 187?228, 1996.M.
Spies, ?A Language Model for CompoundWords?, Proc.
Eurospeech ?95, pp.1767-1779,1995.
