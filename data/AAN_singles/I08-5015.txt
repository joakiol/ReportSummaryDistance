Proceedings of the IJCNLP-08 Workshop on NER for South and South East Asian Languages, pages 105?110,Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language ProcessingExperiments in Telugu NER: A Conditional Random Field ApproachPraneeth M Shishtla, Karthik Gali, Prasad Pingali and Vasudeva Varma{praneethms,karthikg}@students.iiit.ac.in,{pvvpr,vv}@iiit.ac.inLanguage Technologies Research CentreInternational Institute of Information TechnologyHyderabad, IndiaAbstractNamed Entity Recognition(NER) is the taskof identifying and classifying tokens in atext document into predefined set of classes.In this paper we show our experimentswith various feature combinations for Tel-ugu NER.
We also observed that the prefixand suffix information helps a lot in find-ing the class of the token.
We also showthe effect of the training data on the perfor-mance of the system.
The best performingmodel gave an F?=1 measure of 44.91.
Thelanguage independent features gave an F?=1measure of 44.89 which is close to F?=1measure obtained even by including the lan-guage dependent features.1 IntroductionThe objective of NER is to identify and classify alltokens in a text document into predefined classessuch as person, organization, location, miscella-neous.
The Named Entity information in a documentis used in many of the language processing tasks.NER was created as a subtask in Message Under-standing Conference (MUC) (Chinchor, 1997).
Thisreflects the importance of NER in the area of Infor-mation Extraction (IE).
NER has many applicationsin the areas of Natural Language Processing, Infor-mation Extraction, Information Retrieval and speechprocessing.
NER is also used in question answer-ing systems (Toral et al, 2005; Molla et al, 2006),and machine translation systems (Babych and Hart-ley, 2003).
It is also a subtask in organizing and re-trieving biomedical information (Tsai, 2006).The process of NER consists of two steps?
identification of boundaries of proper nouns.?
classification of these identified proper nouns.The Named Entities(NEs) should be correctly iden-tified for their boundaries and later correctly classi-fied into their class.
Recognizing NEs in an Englishdocument can be done easily with a good amountof accuracy(using the capitalization feature).
IndianLanguages are very much different from the Englishlike languages.Some challenges in named entity recognition thatare found across various languages are: Manynamed entities(NEs) occur rarely in the corpus i.ethey belong to the open class of nouns.
Ambiguityof NEs.
Ex Washington can be a person?s name or aplace name.
There are many ways of mentioning thesame Named Entity(NE).
In case of person names,Ex: Abdul Kalam, A.P.J.Kalam, Kalam refer to thesame person.
And, in case of place names Waran-gal, WGL both refer to the same location.
NamedEntities mostly have initial capital letters.
This dis-criminating feature of NEs can be used to solve theproblem to some extent in English.Indian Languages have some additional chal-lenges: We discuss the challenges that are specificto Telugu.
Absence of capitalization.
Ex: The con-densed form of the person name S.R.Shastry is writ-ten as S.R.S in English and is represented as srs inTelugu.
Agglutinative property of the Indian Lan-guages makes the identification more difficult.
Ag-glutinative languages such as Turkish or Finnish,Telugu etc.
differ from languages like English in105the way lexical forms are generated.
Words areformed by productive affixations of derivational andinflectional suffixes to roots or stems.
For example:warangal, warangal ki, warangalki, warangallo,warangal ni etc .. all refer to the place Waran-gal.
where lo, ki, ni are all postpostion markersin Telugu.
All the postpositions get added to thestem hyderabad.
There are many ways of represent-ing acronyms.
The letters in acronyms could be theEnglish alphabet or the native alphabet.
Ex: B.J.Pand BaJaPa both are acronyms of Bharatiya JanataParty.
Telugu has a relatively free word order whencompared with English.
The morpohology of Tel-ugu is very complex.
The Named Entity Recogni-tion algorithm must be able handle most of theseabove variations which otherwise are not found inlanguages like English.
There are not rich and robusttools for the Indian Languages.
For Telugu, thougha Part Of Speech(POS) Tagger for Telugu, is avail-able, the accuracy is less when compared to Englishand Hindi.2 Problem StatementNER as sequence labelling taskNamed entity recognition (NER) can be modelledas a sequence labelling task (Lafferty et al, 2001).Given an input sequence of words W n1 = w1w2w3...wn, the NER task is to construct a label sequenceLn1 = l1l2l3 ...ln , where label li either belongs tothe set of predefined classes for named entities oris none(representing words which are not propernouns).
The general label sequence ln1 has the high-est probability of occuring for the word sequenceW n1 among all possible label sequences, that isL?n1 = argmax {Pr (Ln1 |Wn1 ) }3 Conditional Random FieldsConditional Random Fields (CRFs) (Wallach, 2004)are undirected graphical models used to calculate theconditional probability of values on designated out-put nodes given values assigned to other designatedinput nodes.
In the special case in which the outputnodes of the graphical model are linked by edges in alinear chain, CRFs make a first-order Markov inde-pendence assumption, and thus can be understood asconditionally-trained finite state machines(FSMs).Let o = ?
O1,O2,...OT ?
be some observed inputdata sequence, such as a sequence of words in textin a document,(the values on n input nodes of thegraphical model).
Let S be a set of FSM states, eachof which is associated with a label, l ?L .Let s = ?
s1,s2,... sT ,?
be some sequence of states,(thevalues on T output nodes).
By the Hammersley-Clifford theorem CRFs define the conditional prob-ability of a state sequence given an input sequenceto beP(s|o) =1Zo?
exp(T?t=1?k?k fk (st?1,st ,o, t))where Zo is a normalization factor over all statesequences, is an arbitrary feature function over its ar-guments, and ?k is a learned weight for each featurefunction.
A feature function may, for example, bedefined to have value 0 or 1.
Higher ?
weights maketheir corresponding FSM transitions more likely.CRFs define the conditional probability of a la-bel sequence based on total probability over the statesequences, P(l|o) = ?s:l(s)=l P(s|o) where l(s) is thesequence of labels corresponding to the labels of thestates in sequence s. Note that the normalization fac-tor, Zo, (also known in statistical physics as the parti-tion function) is the sum of the scores of all possiblestate sequences,Zo = ?s?ST?exp(T?t=1?k?k fk (st?1,st ,o, t))and that the number of state sequences is expo-nential in the input sequence length,T.
In arbitrarily-structure CRFs, calculating the partition function inclosed form is intractable, and approximation meth-ods such as Gibbs sampling, or loopy belief propa-gation must be used.4 FeaturesThere are many types of features used in generalNER systems.
Many systems use binary featuresi.e.
the word-internal features, which indicate thepresence or absence of particular property in theword.
(Mikheev, 1997; Wacholder et al, 1997;Bikel et al, 1997).
Following are examples ofbinary features commonly used.
All-Caps (IBM),Internal capitalization (eBay), initial capital (AbdulKalam), uncapitalized word (can), 2-digit number106(83, 28), 4-digit number (1273, 1984), all digits (8,31, 1228) etc.
The features that correspond to thecapitalization are not applicable to Telugu.
We havenot used any binary features in our experiments.Gazetteers are used to check if a part of thenamed entity is present in the gazetteers.
We don?thave proper gazetteers for Telugu.Lexical features like a sliding window[w?2,w?1,wo,w1,w2] are used to create a lexi-cal history view.
Prefix and suffix tries were alsoused previously(Cucerzan and Yarowsky,1999).Linguistics features like Part Of Speech, Chunk,etc are also used.4.1 Our FeaturesWe donot have a highly accurate Part OfSpeech(POS) tagger.
In order to obtain somePOS and chunk information, we ran a POS Taggerand chunker for telugu (PVS and G, 2007) on thedata.
And from that, we used the following featuresin our experiments.Language Independent Featurescurrent token: w0previous 3 tokens: w?3,w?2,w?1next 3 tokens: w1,w2,w3compound feature:w0 w1compound feature:w?1 w0prefixes (len=1,2,3,4) of w0: pre0suffixes (len=1,2,3,4) of w0: su f0Language Dependent FeaturesPOS of current word: POS0Chunk of current word: Chunk0Each feature is capable of providing some infor-mation about the NE.The word window helps in using the context in-formation while guessing the tag of the token.
Theprefix and suffix feature to some extent help in cap-turing the variations that may occur due to aggluti-nation.The POS tag feature gives a hint whether the wordis a proper noun.
When this is a proper noun it hasa chance of being a NE.
The chunk feature helps infinding the boundary of the NE.In Indian Languages suffixes and other inflectionsget attached to the words increasing the length of theword and reducing the number of occurences of thatword in the entire corpus.
The character n-grams cancapture these variations.5 Experimental Setup5.1 CorpusWe conducted the experiments on the developementdata released as a part of NER for South and South-East Asian Languages (NERSSEAL) Competetion.The corpus in total consisted of 64026 tokens outof which 10894 were Named Entities(NEs).
We di-vided the corpus into training and testing sets.
Thetraining set consisted of 46068 tokens out of which8485 were NEs.
The testing set consisted of 17951tokens out of which 2407 were NEs.
The tagset asmentioned in the release, was based on AUKBC?sENAMEX,TIMEX and NAMEX, has the follow-ing tags: NEP (Person), NED (Designation), NEO(Organization), NEA (Abbreviation), NEB (Brand),NETP (Title-Person), NETO (Title-Object), NEL(Location), NETI (Time), NEN (Number), NEM(Measure) & NETE (Terms).5.2 Tagging SchemeThe corpus is tagged using the IOB tagging scheme(Ramshaw and Marcus, 1995).
In this scheme eachline contains a word at the beginning followed byits tag.
The tag encodes the type of named entityand whether the word is in the beginning or insidethe NE.
Empty lines represent sentence(document)boundaries.
An example is given in table 1.Words tagged with O are outside of named en-tities and the I-XXX tag is used for words inside anamed entity of type XXX.
Whenever two entitiesof type XXX are immediately next to each other,the first word of the second entity will be tagged B-XXX in order to show that it starts another entity.This tagging scheme is the IOB scheme originallyput forward by Ramshaw and Marcus (1995).5.3 ExperimentsTo evaluate the performance of our Named EntityRecognizer, we used three standard metrics namelyprecision, recall and f-measure.
Precision measuresthe number of correct Named Entities(NEs) in the107Token Named Entity TagSwami B-NEPVivekananda I-NEPwas Oborn Oon OJanuary B-NETI, I-NETI12 I-NETIin OCalcutta B-NEL.
OTable 1: IOB tagging scheme.machine tagged file over the total number of NEs inthe machine tagged file and the recall measures thenumber of correct NEs in the machine tagged fileover the total number of NEs in the golden standardfile while F-measure is the weighted harmonic meanof precision and recall:F =(?
2+1)RP?
2R+Pwith?
= 1where P is Precision, R is Recall and F is F-measure.W?n+n: A word window :w?n, w?n+1, .., w?1, w0,w1, .., wn?1, wn.POSn: POS nth token.Chn: Chunk of nth token.pren: Prefix information of nth token.
(prefixlength=1,2,3,4)su fn: Suffix information of nth token.
(suffixlength=1,2,3,4)The more the features, the better is the perfor-mance.
The inclusion of the word window, prefixand suffix features have increased the F?=1 mea-sure significantly.
Whenever the suffix feature isincluded, the performance of the system increased.This shows that the system is able to caputure thoseagglutinative language variations.
We also have ex-perimented changing the training data size.
Whilevarying the training data size, we have tested theperformance on the same amount of testing data of17951 tokens.6 Conclusion & Future WorkThe inclusion of prefix and suffix feature helps inimproving the F?=1 measure (also recall) of the sys-tem.
As the size of the training data is increased,the F?=1 measure is increased.
Even without thelanguage specific information the system is able toperform well.
The suffix feature helped improve therecall.
This is due to the fact that the POS taggeralso uses the same features in predicting the POStags.
Prefix, suffix and word are three non-linguisticfeatures that resulted in good performance.
We planto experiment with the character n-gram approach(Klein et al, 2003) and include gazetteer informa-tion.ReferencesBogdan Babych and Anthony Hartley.
2003.
Improv-ing machine translation quality with automatic namedentity recognition.
In Proceedings of Seventh Inter-national EAMT Workshop on MT and other languagetechnology tools, Budapest, Hungary.Daniel M. Bikel, Scott Miller, Richard Schwartz, andRalph Weischedel.
1997.
Nymble: a high-performance learning name-finder.
In Proceedings ofthe fifth conference on Applied natural language pro-cessing, pages 194?201, San Francisco, CA, USA.Morgan Kaufmann Publishers Inc.Nancy Chinchor.
1997.
Muc-7 named entity task defini-tion.
Technical Report Version 3.5, Science Applica-tions International Corporation, Fairfax, Virginia.Dan Klein, Joseph Smarr, Huy Nguyen, and Christo-pher D. Manning.
2003.
Named entity recognitionwith character-level models.
In Proceedings of theseventh conference on Natural language learning atHLT-NAACL 2003, pages 180?183, Morristown, NJ,USA.
Association for Computational Linguistics.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional random fields: Probabilistic modelsfor segmenting and labeling sequence data.
In Proc.18th International Conf.
on Machine Learning, pages282?289.
Morgan Kaufmann, San Francisco, CA.Andrei Mikheev.
1997.
Automatic rule inductionfor unknown-word guessing.
Comput.
Linguist.,23(3):405?423.108Features Precision Recall F?=1Ch0 51.41% 9.19% 15.59POS0 46.32% 9.52% 15.80POS0.Ch0 46.63% 9.69% 16.05W?3+3.Ch0 59.08% 19.50% 29.32W?3+3.POS0 58.43% 19.61% 29.36Ch0.pren 53.97% 24.76% 33.95POS0.pren 53.94% 24.93% 34.10POS0.Ch0.pren 53.94% 25.32% 34.46POS0.su fn 47.51% 29.36% 36.29POS0.Ch0.su fn 48.02% 29.24% 36.35Ch0.su fn 48.55% 29.13% 36.41W?3+3.POS0.pren 62.98% 27.45% 38.24W?3+3.POS0.Ch0.pren 62.95% 27.51% 38.28W?3+3.Ch0.pren 62.88% 27.62% 38.38W?3+3.POS0.su fn 60.09% 30.53% 40.49W?3+3.POS0.Ch0.su fn 59.93% 30.59% 40.50W?3+3.Ch0.su fn 61.18% 30.81% 40.98POS0.Ch0.pren.su fn 57.83% 34.57% 43.27POS0.pren.su fn 57.41% 34.73% 43.28Ch0.pren.su fn 57.80% 34.68% 43.35W?3+3.Ch0.pren.su fn 64.12% 34.34% 44.73W?3+3.POS0.pren.su fn 64.56% 34.29% 44.79W?3+3.POS0.Ch0.pren.su fn 64.07% 34.57% 44.91Table 2: Average Precision,Recall and F?=1 measure for different language dependent feature combinations.Features Precision Recall F?=1w 57.05% 20.62% 30.29pre 53.65% 23.87% 33.04suf 47.75% 29.19% 36.23w.pre 63.08% 27.56% 38.36w.suf 60.93% 30.76% 40.88pre.suf 57.94% 34.96% 43.61w.pre.suf 64.80% 34.34% 44.89Table 3: Average Precision,Recall and F?=1 measure for different language independent feature combina-tions.Diego Molla, Menno van Zaanen, and Daniel Smith.2006.
Named entity recognition for question answer-ing.
In Proceedings of Australasian Language Tech-nology Workshop 2006, Sydney, Australia.Avinesh PVS and Karthik G. 2007.
Part-of-speech tag-ging and chunking using conditional random fields andtransformation based learning.
In In Proceedings ofSPSAL-2007 Workshop.Lance Ramshaw and Mitch Marcus.
1995.
Text chunk-ing using transformation-based learning.
In DavidYarovsky and Kenneth Church, editors, Proceedingsof the Third Workshop on Very Large Corpora, pages82?94, Somerset, New Jersey.
Association for Compu-tational Linguistics.Antonio Toral, Elisa Noguera, Fernando Llopis, andRafael Mun?oz.
2005.
Improving question answeringusing named entity recognition.
In Proceedings of the10th NLDB congress, Lecture notes in Computer Sci-ence, Alicante, Spain.
Springer-Verlag.109Number of Words Precision Recall F?=12500 51.37% 9.47% 15.995000 64.74% 11.93% 20.157500 61.32% 13.50% 22.1310000 66.88% 23.31% 34.5712500 63.42% 27.39% 38.2615000 63.55% 31.26% 41.9117500 60.58% 30.64% 40.7020000 58.32% 30.03% 39.6422500 57.72% 29.75% 39.2625000 59.33% 29.92% 39.7827500 60.91% 30.03% 40.2330000 62.77% 30.42% 40.9832500 62.66% 30.64% 41.1635000 62.08% 30.81% 41.1837500 61.02% 30.87% 41.0040000 61.60% 31.09% 41.3342500 62.12% 32.44% 42.6245000 62.70% 32.77% 43.0547500 63.20% 32.72% 43.1250000 64.29% 34.29% 44.72Table 4: The effect of training data size on the performance of the NER.Richard Tzong-Han Tsai.
2006.
A hybrid approach tobiomedical named entity recognition and semantic rolelabeling.
In Proceedings of the 2006 Conference of theNorth American Chapter of the Association for Com-putational Linguistics on Human Language Technol-ogy, pages 243?246, Morristown, NJ, USA.
Associa-tion for Computational Linguistics.Nina Wacholder, Yael Ravin, and Misook Choi.
1997.Disambiguation of proper names in text.
In Proceed-ings of the fifth conference on Applied natural lan-guage processing, pages 202?208, San Francisco, CA,USA.
Morgan Kaufmann Publishers Inc.HannaM.Wallach.
2004.
Conditional random fields: Anintroduction.
Technical Report MS-CIS-04-21, Uni-versity of Pennsylvania, Department of Computer andInformation Science, University of Pennsylvania.110
