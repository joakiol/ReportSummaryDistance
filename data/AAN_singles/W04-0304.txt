Incremental Parsing with Reference InteractionScott C. Stoness, Joel Tetreault, James AllenDepartment of Computer ScienceUniversity of RochesterRochester, NY, USAstoness@cs.rochester.edutetreaul@cs.rochester.edu, james@cs.rochester.eduAbstractWe present a general architecture for incremen-tal interaction between modules in a speech-to-intention continuous understanding dialogue sys-tem.
This architecture is then instantiated in theform of an incremental parser which receives suit-ability feedback on NP constituents from a refer-ence resolution module.
Oracle results indicatethat perfect NP suitability judgments can provide alabelled-bracket error reduction of as much as 42%and an efficiency improvement of 30%.
Prelimi-nary experiments in which the parser incorporatesfeedback judgments based on the set of referentsfound in the discourse context achieve a maximumerror reduction of 9.3% and efficiency gain of 4.6%.The parser is also able to incrementally instantiatethe semantics of underspecified pronouns based onmatches from the discourse context.
These resultssuggest that the architecture holds promise as a plat-form for incremental parsing supporting continuousunderstanding.1 IntroductionHumans process language incrementally, as hasbeen shown by classic psycholinguistic discussionssurrounding the garden-path phenomenon and pars-ing preferences (Altmann and Steedman, 1988;Konieczny, 1996; Phillips, 1996).
Moreover, a va-riety of eye-tracking experiments (Cooper, 1974;Tanenhaus and Spivey, 1996; Allopenna et al,1998; Sedivy et al, 1999) suggest that complex se-mantic and referential constraints are incorporatedon an incremental basis in human parsing decisions.Computational parsers, however, still tend to op-erate an entire sentence at a time, despite the ad-vent of speech-to-intention dialogue systems suchas Verbmobil (Kasper et al, 1996; Noth et al, 2000;Pinkal et al, 2000), Gemini (Dowding et al, 1993;Dowding et al, 1994; Moore et al, 1995) and TRIPS(Allen et al, 1996; Ferguson et al, 1996; Fergu-son and Allen, 1998).
Naturalness, robustness, andinteractivity are goals of such systems, but controlflow is typically the sequential execution of mod-ules, each operating on the output of its predeces-sor; only after the entire sentence has been parseddo higher-level modules such as intention recogni-tion and reference resolution get involved.In contrast to this sequential model is the con-tinuous understanding approach, in which all lev-els of language analysis occur simultaneously, fromspeech recognition to intention recognition.
As wellas being psycholinguistically motivated, continuousunderstanding models offer potential computationaladvantages, including accuracy and efficiency im-provements for real-time spoken language under-standing and better support for the spontaneities ofnatural human speech.
Continuous understandingis necessary if the system is to respond before theentire utterance is analyzed, a prerequisite for in-cremental confirmation and clarification.
The majorcomputational advantage of continuous understand-ing models is that high-level expectations and feed-back should be able to influence the search of lower-level processes, thus leading to a focused searchthrough hypotheses that are plausible at all levelsof processing.One of the major current applications of parsersthat operate incrementally is for language modellingin speech recognition (Brill et al, 1998; Jelinek andChelba, 1999).
This work is important not onlyfor its ability to improve performance on the speechrecognition task; it also models the interactions be-tween speech recognition and parsing in a contin-uous understanding system.
Our research attemptsto further the quest for continuous understanding bymoving one step up the hierarchy, building an incre-mental parser which is the advisee rather than theadvisor.We begin by presenting a general architecturefor incremental interaction between the parser andhigher-level modules, and then discuss a specific in-stantiation of this general architecture in which areference resolution module provides feedback tothe parser on the suitability of noun phrases.
Ex-periments with incremental feedback from a refer-Client(parser)MediatorAdvisor(reference)InformInform FeedbackModifyChartFigure 1: A General Architecture for IncrementalParsingence resolution module and an NP suitability oracleare reported, and the ability of the implementationto incrementally instantiate semantically underspec-ified pronouns is outlined.
We believe this researchprovides an important start towards developing end-to-end continuous understanding models.2 An Incremental Parsing ArchitectureMany current parsers fall into the class of history-based grammars (Black et al, 1992).
The indepen-dence assumptions of these models make the pars-ing problem both stochastically and computation-ally tractable, but represent a simplification and maytherefore be a source of error.
In a continuous un-derstanding framework, higher-level modules mayhave additional information that suggests loci forimprovement, recognizing either invalid indepen-dence assumptions or errors in the underlying prob-ability model.We have designed a general incremental parsingarchitecture (Figure 1) in which the Client, a dy-namic programming parser, performs its calcula-tions, the results of which are incrementally passedon via a Mediator to an Advisor with access tohigher-level information.
This higher-level Advi-sor sends feedback to the Mediator which has ac-cess to the Client?s chart, and which then surrepti-tiously changes and/or adds to the chart in order tomake the judgments conform more closely to thoseof the Advisor.
The parser, whose chart has (unbe-knownst to it) been changed, then simply calculateschart expansions for the next word, na?
?vely expand-ing the currently available (and possibly modified)hypotheses.This architecture is general in that neither the Me-diator nor the Advisor have been specified; eitherof these modules can be instantiated in any numberof ways within the general framework.
The typicaldynamic programming component will function invery much the same way that it does in the vanillaalgorithm, except that the chart in which partial re-sults are recorded may be modified between timesteps.
The Client can be any system which uses dy-namic programming to efficiently encode indepen-dence assumptions, so long as it provides the Me-diator with the ability to modify chart probabilitiesand add chart entries; otherwise the original parsercan remain untouched.
By having the Mediator per-form these modifications rather than the Advisor,we preserve modularity: in this architecture the Ad-visor need not be aware of the specific implementa-tion of the Client, although depending on the typeof advice provided, it may need access to the under-lying grammar.
The Mediator isolates the Advisorand Client from each other as well as determininghow the feedback will be introduced into into theClient?s chart.Stoness (2004) identifies two broad categories ofsubversion - our term for the Mediator?s surrepti-tious modification of the Client?s chart - as outlinedbelow:?
Heuristic Subversion: the Mediator uses theAdvisor?s feedback as heuristic information,affecting the search sequence but not the prob-abilities calculated for a given hypothesis; and?
Chart Subversion: the Mediator is free tomodify the Client?s chart as necessary, but doesnot directly affect the search sequence of theClient (except insofar as this is accomplishedby the modifications to the chart).The two types of subversion have very differentproperties.
Heuristic subversion will affect the setof analyses which is output by the parser, but eachof those analyses will have exactly the same proba-bility score as under the original parser; the effectsof the Advisor are essentially limited to determin-ing which hypotheses remain within the beam, orthe order in which hypotheses are expanded, de-pending on whether the underlying parser uses abeam search or an agenda.
Chart subversion, on theother hand, will actually change the scores assignedanalyses, resulting in a new probability distribution.Heuristic subversion is considerably less powerful,but more stable; the effects of chart subversion canbe fairly chaotic, especially if care is not taken toavoid feedback loops.
Stoness (2004) outlines con-ditions under which the effects of chart subversionare predictable, becoming broadly equivalent to anincremental version of a post-hoc re-ranking of theClient?s output hypotheses.Further details on the general architecture, in-cluding properties of various modes of feedback in-tegration, a discussion of the relationship betweenincremental parsing and parse re-ranking, the pos-sibilities of multiple Advisors working in combina-tion, and provisions in the model for asynchronousfeedback are available in a University of RochesterTechnical Report (Stoness, 2004).3 Instantiating the ArchitectureWorking in the context of TRIPS, an existing task-oriented dialogue system, we have modified theexisting parser and reference resolution modulesso that they communicate incrementally with eachother.
This models the early incorporation of refer-ence resolution information seen in humans (Cham-bers et al, 1999; Allopenna et al, 1998), and al-lows reference resolution information to affect pars-ing decisions.For example, in ?Put the apple in the box in thecorner?
there is an attachment ambiguity.
Referenceresolution can determine the number of matches forthe noun phrase ?the apple?
incrementally; if thereis a single match, the parser would expect this tobe a complete NP, and prefer the reading where thebox is in the corner.
If reference returns multiplematches for ?the apple?, the parser would expectdisambiguating information, and prefer a readingwhere additional information about the apple is pro-vided: in this case, an the NP ?the apple in the box?.With solid feedback from reference, it should bepossible to remove some of the ambiguity inherentin the search process within the parser.
This willsimultaneously guide the search to the most likelyregion of the search space, improving accuracy, anddelay the search of unlikely regions, improving effi-ciency.
Of course, this comes at the cost of somecommunication overhead and additional referenceresolution.
Ideally, the overall improvement in theparser?s search space would be enough to coverthe additional incremental operation costs of othermodules.3.1 An Incremental ParserThe pre-existing parser in the dialogue system was apure bottom-up chart parser with a hand-built gram-mar suited for parsing task-oriented dialogue.
Thegrammar consisted of a context-free backbone witha set of associated features and semantic restric-tions, including agreement, hard subcategorizationconstraints, and soft selectional restriction prefer-ences.
The parser has been modified so that when-ever a constituent is built, it can be sent forward tothe Mediator, allowing for the possibility of feed-back.
The architecture and experiments described inthis paper were performed in a synchronous mode,but the parser can also operate in an incrementallyasynchronous mode, where it continues to build thechart in parallel with other modules?
operations;probability adjustments to the chart then cascade todependent constituents.3.2 Interaction with ReferenceWhen the parser builds a potential referring expres-sion (e.g.
any NP), it is immediately passed on to theAdvisor, the reference resolution module describedin Tetreault et.
al.
(2004) modified for incrementalinteraction.
This module then determines all pos-sible discourse referents, providing the parser witha ranked classification based on the salience of thereferents and the (incremental) syntactic environ-ment.The reference module keeps a dynamically up-dated list of currently salient discourse entitiesagainst which incoming incrementally constructedNP constituents are matched.
Before any utterancesare processed, the module loads a static databaseof relevant place names in the domain; all otherpossible referents are discourse entities which havebeen spoken of during the course of the dialogue.For efficiency, the dynamic portion of the contextlist is limited to the ten most recent contentful ut-terances; human-annotated antecedent data for thiscorpus shows that 99% of all pronoun antecendentsfall within this threshold.
After each sentence isfully parsed the context list is updated with new dis-course entities introduced in the utterance; ideally,these context updates would also be incremental,but this feature was omitted in the current versionfor simplicity.The matching process is based on that describedby Byron (2000), and differs from that of manyother reference modules in that every entity andNP-constituent has a (possibly underspecified) se-mantic feature vector, and it is both the logical andsemantic forms which determine successful match-ings.
Adding semantic information increases the ac-curacy of the reference resolution from 44% to 58%(Tetreault and Allen, 2004), and consequently im-proves the feedback provided to the parser.The Mediator receives the set of all possible ref-erents, including the semantic content of the refer-ent and a classification of whether the referent is thesingle salient entity in focus, has previously beenmentioned, or is a relevant place name.3.3 MediatorThe Mediator interprets the information receivedfrom reference and determines how the parser?schart should be modified.
If the NP matches noth-ing in the discourse context, no match is returned;otherwise each referent is annotated with its typeand discourse distance, and this set is run through aclassifier to reduce it to a single tag.
The resultingtag is the reference resolution tag, or R. The NPconstituents are also classified by definiteness andnumber, giving an NP tag N .For each classifier, we trained a probability modelwhich calculated Pr, the probability that a nounphrase constituent c would be in the final parse, con-ditioned on R and N , orPr = p(c in final parse|R,N).This probability was then linearly combined withthe parser?s constituent probability,Pp = p(c ?
wnm),according to the equationP (c) = (1?
?)
?
Pp + ?
?
Prfor various values of ?.
Evaluation using held-outdata suggested that a value of ?
= 0.2 would beoptimal.
This style of feedback is an example ofchart subversion, as it is a direct modification ofconstituent probabilities by the Mediator, defininga new probability distribution.4 ExperimentsThe Monroe domain (Tetreault et al, 2004; Stent,2001) is a series of task-oriented dialogues betweenhuman participants set in a simulated rescue op-eration domain, where participants collaborativelyplan responses to emergency calls.
Dialogues wererecorded, broken up into utterances, and then tran-scribed by hand, removing speech repairs from theparser input.
These transcriptions served as inputfor all experiments reported below.A probabilistic grammar was trained from su-pervised data, assigning PCFG probabilities for therule expansions in the CFG backbone of the hand-crafted, semantically constrained grammar.
Theparser was run using this grammar, but without anyincremental interaction whatsoever, in order to es-tablish baseline accuracy and efficiency numbers.The corpus consists of six task-oriented dialogues;four were used for the PCFG training, one washeld out to establish appropriate parameter values,and one was selected for testing.
The held-out andtest dialogues contain hand-checked gold standardparses.Under normal operation of the sequential dia-logue system, the parser is run in best-first mode,providing only a single analysis to higher-levelmodules, and has a constituent construction limit inBase All NPs Def-SingPrecision 94.6 97.2 96.3Recall 71.1 83.1 78.8F-statistic 82.9 90.2 87.6Improvement N/A 7.3 4.7Error Red.
N/A 42.4 27.2Work Red.
N/A 30.3 18.7Perfect S 224 241 236Parsed S 270 282 279Table 1: Results for (a) The baseline parser withoutreference feedback, (b) An Oracle Advisor correctlydetermining status of all NPs, (c) An Oracle Advi-sor correctly determining status of definite singularNPs.an attempt to simulate the demands of a real-timesystem.
When the parser reaches the constituentlimit, appropriate partial analyses are collected andforwarded to higher-level modules.
These con-straints were kept in place during our experiments,because they would be necessary under normal op-eration of the system.
Thus, the inability to parse asentence does not necessarily indicate a lack of cov-erage of the grammar, but rather a lack of efficiencyin the parsing process.As can be seen in Table 1, the parser achieves a94.6% labelled bracket precision, and a 71.1% la-belled bracket recall.
Note that only constituentsof complete parses were checked against the goldstandard, to avoid any bias introduced by the partialparse evaluation metric.
Of the 290 gold standardutterances in the test data, 270 could be parsed, and224 were parsed perfectly.4.1 Oracle EvaluationWe began with a feasibility study to determinehow significant the effects of incremental advice onnoun phrases could be in principle.
The feedbackfrom the reference module is designed to determinewhether particular NPs are good or bad from a refer-ence standpoint.
We constructed a simple feedbackoracle from supervised data which determined, foreach NP, whether or not the final parse of the sen-tence contained an NP constituent which spannedthe same input.
Those NPs marked ?good?, whichdid appear in the parse, were added to the chart asnew constituents.
NPs marked ?bad?
were added tothe chart with a probability of zero1.
A second or-1In some sense, this style of feedback is an example ofheuristic subversion, as it has the effect of keeping ?good?
anal-yses around while removing ?bad?
analyses from the searchspace.
Technically, this is also chart subversion, as each hy-pothesis has its score multiplied by 1 or 0, depending onacle evaluation performed this same task, but onlyproviding feedback on definite singular NPs.The results of both oracles are shown in Table1.
The first five rows give the precision, recall, f-statistic, the raw f-statistic improvement, and the f-statistic error reduction percentage, all determinedin terms of labelled bracket accuracy.
There is amarked increase in both precision and recall, withan overall error reduction of 42.4% with the fulloracle and 27.2% with the definite singular oracle.Thus, in this domain over a quarter of all incorrectlylabelled constituents are attributable to syntacticallyincorrect definite singular NPs.
The number of con-stituents built during the parse is used as a measureof efficiency, and the work reduction is reported inthe sixth row of the table, showing an efficiency im-provement of 30.3% or 18.7%, depending on the or-acle.
The final two lines of the table show that boththe number of sentences which can be parsed andthe number of sentences which are perfectly parsedincrease under both models.The nature of the oracle experiment ensures somereduction in error and complexity, but the magni-tude of the improvement is surprising, and certainlyencouraging for the prospects of incremental refer-ence.
Definite singular NPs typically have a uniquereferent, providing a locus for effective feedback,and we believe that incremental interaction with anaccurate reference module might approach the ora-cle performance.4.2 Dialogue ExperimentsFor these experiments the parser interacted with theactual reference module, incorporating feedback ac-cording to the model discussed in Section 3.3.
Thefirst data column of Table 2 repeats the baseline re-sults of the parser without reference feedback.
Thenext two columns show statistics for a run of theparser with incremental feedback from reference,using a probability model based on a classificationscheme which distinguished only whether or not theset of referent matches was empty.
The second datacolumn shows the results for the estimated interpo-lation parameter value of ?
= 0.2, while the thirddata column shows results for the empirically deter-mined optimal ?
value of 0.1.The results are encouraging, with an error reduc-tion of 8.2% or 9.3% on the test dialogue, althoughthe amount of work the parser performed was re-duced by only 4.0% and 3.6%.
A further encour-aging sign is that for every exploratory ?
value wewhether it is ?good?
or ?bad?.
In this degenerate case of all-or-nothing feedback, chart subversion and heuristic subversionare equivalent.Base SC SC CC?
= N/A 0.2 0.1 0.2Precision 94.6 94.5 94.8 93.9Recall 71.1 74.1 74.2 73.9F-statistic 82.9 84.3 84.5 83.9F-stat Imp.
N/A 1.4 1.6 1.0Error Red.
N/A 8.2 9.3 5.8Work Red.
N/A 3.6 4.0 4.6Perfect S 224 225 228 223Parsed S 270 273 273 273Table 2: Results for Discourse Experiment withSimple (SC) and Complex (CC) Classifierstried in either the held-out or the test data, both theaccuracy and efficiency improved.
Reference infor-mation also helped increase both the number of sen-tences that could be parsed and the number of sen-tences that were parsed perfectly, although the im-provements were small.The estimated value of ?
= 0.2 produced an errorreduction that was approximately 20% of the orac-ular, which is a very good start, especially consider-ing that this experiment used only the information ofwhether there was a referent match or not.
The effi-ciency gains were more modest at just above 10% ofthe oracular results, although one would expect lessradical efficiency improvements from this experi-ment, since under the linear interpolation of the ex-periment, even extremely dispreferred analyses maybe expanded, whereas the oracle simply drops alldispreferred NPs off the beam immediately.We performed a second experiment that mademore complete use of the reference data, break-ing down referent sets according to when and howoften they were mentioned, whether they matchedthe focus, and whether they were in the set ofrelevant place names.
We expected that this in-formation would provide considerably better re-sults than the simple match/no-match classificationabove.
For example, consider a definite singularNP: if it matches a single referent, one would expectit to be in the parse with high probability, but multi-ple matches would indicate that the referent was notunique, and that the base noun probably requires ad-ditional discriminating information (e.g.
a preposi-tional phrase or restrictive relative clause).Unfortunately, as the final column of Table 2shows, the additional information did not providemuch of an advantage.
The amount of work donewas reduced by 4.6%, the largest of any efficiencyimprovement, but error reduction was only 5.8%,and the number of sentences parsed perfectly actu-ally decreased by one.We conjecture that co-reference chains may be asignificant source of confusion in the reference data.Ideally, if several entities in the discourse contextall refer to the same real-world entity, they shouldbe counted as a single match.
The current refer-ence module does construct co-referential chains,but a single error in co-reference identification willcause all future NPs to match both the chain and themisidentified item, instead of producing the singlematch desired.The reference module has to rely on the parserto provide the correct context, so there is somethingof a bootstrapping problem at work, which indicatesboth a drawback and a potential of this type of in-cremental interaction.
The positive feedback loopbodes well for the potential benefits of the incre-mental system, because as the incremental referenceinformation begins to improve the performance ofthe parser, the context provided to the referenceresolution module improves, which provides evenmore accurate reference information.
Of course, inthe early stages of such a system, this works againstus; many of the reference resolution errors could bea result of the poor quality of the discourse context.Our current efforts aim to identify and correctthese and other reference resolution issues.
Not onlywill this improve the performance of the ReferenceAdvisor from an incremental parsing standpoint, butit should also further our understanding of referenceresolution itself.We have shown efficiency improvements in termsof the overall number of constituents constructed bythe parser; however, one might ask whether this im-provement in parsing speed comes at a large cost tothe overall efficiency of the system.
We suggest thatthis is in some sense the wrong question to ask, be-cause for a real-time interactive system the primaryconcern is to keep up with the human interlocutor,and the incremental approach offers a far greater op-portunity for parallelism between modules.
In termsof time elapsed from speech to analysis, the systemas a whole should benefit from the incremental ar-chitecture.5 Semantic ReplacementWhen the word ?it?
is parsed as a referential NP, it isgiven highly underspecified semantics.
We have im-plemented a Mediator which, for each possible ref-erent for ?it?, adds a new item to the parser?s chartwith the underspecified semantics of ?it?
instanti-ated to the semantics of the referent.Consider the sentence sequence ?Send the bus tothe hospital?, ?Send it to the mall?.
At the pointthat the NP ?it?
is encountered in the second sen-tence, it has not yet been connected to the verb,so the incremental reference resolution determinesthat ?the bus?
and ?the hospital?
are both possi-ble referents.
We add two new constituents to thechart: ?it?
[the hospital] and ?it?
[the bus].
Theyare given probabilities infinitesimally higher thanthe ?it?
[underspecified] which already exists on thechart.
Thus, if either of the new versions of ?it?match the semantic restrictions inherent in the restof the parse, they will be featured in parses with ahigher probability than the underspecified version.?It?
[the bus] matches the mobility required of theobject of ?send?, while ?it?
[the hospital] does not.This results in a parse where the semantics of ?it?are instantiated early and incrementally.This sort of capability is key for an end-to-endincremental system, because neither the referencemodule nor the parser is capable, by itself, of deter-mining incrementally that the reference in questionmust be ?the bus?.
If we want an end-to-end systemwhich can interact incrementally with the user, thistype of decision-making must be made in an incre-mental fashion.This ability is also key in the presence of soft con-straints or other Advisors which prefer one possi-ble moveable referent to another; under incrementalparsing, these constraints would have the chance tobe applied during the parsing process, whereas a se-quential system has no alternatives to the default,underspecified pronoun, and so cannot apply theserestrictions to discriminate between referents.Our implementation performs the semantic vet-ting discussed above, but we have done no large-scale experiments in this area.6 Related WorkThere are instances in the literature of incrementalparsers that pass forward information to higher-levelmodules, but none, to our knowledge, are designedas continuous understanding systems, where all lev-els of language analysis occur (virtually) simultane-ously.For example, there are a number of robust seman-tic processing systems (Pinkal et al, 2000; Rose,2000; Worm, 1998; Zechner, 1998) which containincremental parsers that pass on partial results im-mediately to the robust semantic analysis compo-nent, which begins to work on combining thesesentence fragments.
If the parser cannot find aparse, then the semantic analysis program has al-ready done at least part of its work.
However, noneof the above systems have a feedback loop betweenthe semantic analysis component and the incremen-tal parser.
So, while all of these are in some senseexamples of incremental parsing, they are not con-tinuous understanding models.Schuler (2002) describes a parser which buildsboth a syntactic tree and a denotation-based seman-tic analysis as it parses.
The denotations of con-stituents in the environment are used to inform pars-ing decisions, much as we use the static database ofplace names.
However, the feedback in our systemis richer, based on the context provided by the pre-ceding discourse.
Furthermore, as an instantiationof the general architecture presented in Section 2,our system is more easily extensible to other formsof feedback.7 Future WorkThere is a catch-22 in that the accurate reference in-formation necessary to improve parsing accuracy isdependent on an accurate discourse context whichis reliant on accurate parsing.
One way to cut thisGordian Knot is to use supervised data to ensure thatthe discourse context in the reference module is up-dated with the gold standard parse of the sentencerather than the parse chosen by the parser; a contextoracle, if you will.A major undertaking necessary to advance thiswork is an error analysis of the reference moduleand of the parser?s response to feedback; when doesfeedback lead to additional work or decreased ac-curacy on the part of the incremental parser, and isthe feedback that leads to these errors correct froma reference standpoint?Currently, the accuracy of the parser is couchedin syntactic terms.
The precision of the baselinePCFG is fairly high at 94.6%, but that could concealsemantic errors, which could be corrected with ref-erence information.
Assessing semantic accuracy isone of a number of alternative evaluation metricsthat we are exploring.We intend to gather timing data and investigateother efficiency metrics to determine to what extentthe efficiency gains in the parser offset the commu-nication overhead and the extra work performed bythe reference module.We also plan to do experiments with differentfeedback regimes, experimenting both with the ac-tual reference results and with the oracle data.
Fur-ther experiments with this oracle data should enableus to appropriately parameterize the linear interpo-lation, and indeed, to investigate whether linear in-terpolation itself is a productive feedback scheme,or whether an integrated probability distributionover parser and reference judgments is more effec-tive.
The latter scheme is not only more elegant, butcan also be shown to produce probabilities equiva-lent to those assigned parses in the parse re-rankingtask (Stoness, 2004).We?ve shown (Stoness, 2004) that feedbackwhich punishes constituents that are not in the fi-nal parse cannot result in reduced accuracy or effi-ciency; under certain restrictions, the same holds ofrewarding constituents that will be in the final parse.However, it is not clear how quickly the efficiencyand accuracy gains drop off as errors mount.
By in-troducing random mistakes into the Oracle Advisor,we can artificially achieve any desired level of accu-racy, which will enable us to explore the character-istics of this curve.
The accuracy and efficiency re-sponse under error has drastic consequences on thetypes of Advisors that will be suitable under this ar-chitecture.Finally, it is clear that finding only the discoursecontext referents of a noun phrase is not sufficient;intuitively, and as shown by Schuler (2002), real-world referents can also aid in the parsing task.
Weintend to enhance the reference resolution compo-nent of the system to identify both discourse andreal-world referents.8 ConclusionThese preliminary experiments, using the coars-est grain of reference information possible, achievea significant fraction of the oracular accuracy im-provements, highlighting the potential benefits ofincremental interaction between the parser and ref-erence in a continuous understanding system.The Oracle feedback for NPs shows that it is pos-sible to simultaneously improve both the accuracyand efficiency of an incremental parser, providing aproof-in-principle for the general incremental pro-cessing architecture we introduced.
This architec-ture holds great promise as a platform for instantiat-ing the wide range of interactions necessary for truecontinuous understanding.9 AcknowledgementsPartial support for this project was provided byONR grant no.
N00014-01-1-1015, ?Portable Di-alog Interfaces?
and NSF grant 0328810 ?Continu-ous Understanding?.ReferencesJ.
Allen, B. Miller, E. Ringger, and T. Sikorski.1996.
Robust understanding in a dialogue sys-tem.
In Proc.
of ACL-96, pages 62?70.P.
D. Allopenna, J. S. Magnuson, and M. K. Tanen-haus.
1998.
Tracking the time course of spo-ken word recognition using eye movements: ev-idence for continuous mapping models.
Journalof Memory and Language, 38:419?439.G.
Altmann and M. Steedman.
1988.
Interactionwith context during human sentence processing.Cognition, 30:191?238.E.
Black, F. Jelinkek, J. Lafferty, D. Magerman,R.
Mercer, and S. Roukos.
1992.
Towardshistory-based grammars: using richer modelsfor probabilistic parsing.
In Proc.
of the FifthDARPA Speech and Natural Language Workshop.E.
Brill, R. Florian, J. C. Henderson, and L. Mangu.1998.
Beyond n-grams: Can linguistic sophisti-cation improve language modeling?
In Proc.
ofCOLING-ACL-98, pages 186?190.D.
K. Byron.
2000.
Semantically enhanced pro-nouns.
In Proc.
of DAARC2000: 3rd Interna-tional Conference on Discourse Anaphora andAnaphor Resolution.C.
G. Chambers, M. K. Tanenhaus, and J. S. Magnu-son.
1999.
Real world knowledge modulates ref-erential effects on pp-attachment: Evidence fromeye movements in spoken language comprehen-sion.
Conference Abstract.
Architechtures andMechanisms for Language Processing.R.
M. Cooper.
1974.
The control of eye fixationby the meaning of spoken language.
CognitivePsychology, 6:84?107.J.
Dowding, J. M. Gawron, D. Appelt, J. Bear,L.
Cherny, R. Moore, and D. Moran.
1993.Gemini: A natural language system for spoken-language understanding.
In Proc.
of ACL-93,pages 54?61.J.
Dowding, R. Moore, F. Andry, and D. Moran.1994.
Interleaving syntax and semantics in anefficient bottom-up parser.
In Proc.
of ACL-94,pages 110?116.G.
Ferguson and J. Allen.
1998.
Trips: An inte-grated intelligent problem-solving assistant.
InProc.
of AAAI-98, pages 567?572.G.
Ferguson, J. Allen, and B. Miller.
1996.
Trains-95: Towards a mixed-initiative planning assis-tant.
In Proc.
of the 3rd International Conferenceon Artificial Intelligence Planning Systems, pages70?77.Frederick Jelinek and Ciprian Chelba.
1999.Putting language into language modeling.
InProc.
of Eurospeech-99.W.
Kasper, H.-U.
Krieger, J. Spilker, and H. Weber.1996.
From word hypotheses to logical form: Anefficient interleaved approach.
In Natural Lan-guage Processing and Speech Technology: Re-sults of the 3rd Konvens Conference, pages 77?88.Lars Konieczny.
1996.
Human Sentence Process-ing: A Semantics-Oriented Parsing Approach.Ph.D.
thesis, Universitat Freiburg.R.
Moore, D. Appelt, J. Dowding, J. M. Gawron,and D. Moran.
1995.
Combining linguistic andstatistical knowledge sources in natural-languageprocessing for atis.
In Proc.
ARPA Spoken Lan-guage Systems Technology Workshop.E.
Noth, A. Batliner, A. Kiessling, R. Kompe, andH.
Niemann.
2000.
Verbmobil: The use ofprosody in the linguistic components of a speechunderstanding system.
IEEE Transactions onSpeech and Audio Processing, 8(5):519?531.Colin Phillips.
1996.
Order and Structure.
Ph.D.thesis, MIT.M.
Pinkal, C.J.
Rupp, and K. Worm.
2000.
Ro-bust semantic processing of spoken language.In Verbmobil: Foundations of Speech-to-SpeechTranslation, pages 321?335.C.
P. Rose.
2000.
A framework for robust semanticinterpretation.
In Proc.
of the Sixth Conferenceon Applied Natural Language Processing.W.
Schuler.
2002.
Interleaved semantic interpreta-tion in environment-based parsing.
In Proc.
ofCOLING-02.J.
C. Sedivy, M. K. Tanenhaus, C. G. Chambers,and G. N. Carlson.
1999.
Achieving incrementalsemantic interpretation through contextual repre-sentation.
Cognition, 71:109?147.A.
Stent.
2001.
Dialogue Systems as Conver-sational Partners.
Ph.D. thesis, University ofRochester.S.
C. Stoness.
2004.
A general architecture forincremental parsing.
Technical report, TR 838,University of Rochester.M.
K. Tanenhaus and M. Spivey.
1996.
Eye-tracking.
Language and Cognition Processes,11(6):583?588.J.
Tetreault and J. Allen.
2004.
Semantics, dia-logue, and reference resolution.
In Catalog-04:8th Workshop on the Semantics and Pragmaticsof Dialogue.J.
Tetreault, M. Swift, P. Prithviraj, M. Dzikovska,and J. Allen.
2004.
Discourse annotation in themonroe corpus.
In ACL-04 Discourse AnnotationWorkshop.K.
Worm.
1998.
A model for robust process-ing of spontaneous speech by integrating viablefragments.
In Proc.
of COLING-ACL-98, pages1403?1407.K.
Zechner.
1998.
Automatic construction of framerepresentations for spontaneous speech in unre-stricted domains.
In Proc.
of COLING-ACL-98,pages 1448?1452.
