Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 1223?1233, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsFirst-order vs. higher-order modification in distributional semanticsGemma BoledaLinguistics DepartmentUniversity of Texas at Austingemma.boleda@utcompling.comEva Maria VecchiCenter for Mind/Brain SciencesUniversity of Trentoevamaria.vecchi@unitn.itMiquel Cornudella and Louise McNallyDepartament de Traduccio?
i Cie`ncies del LlenguatgeUniversitat Pompeu Fabramiquel.cornudellagaya@gmail.com, louise.mcnally@upf.eduAbstractAdjectival modification, particularly by ex-pressions that have been treated as higher-order modifiers in the formal semantics tradi-tion, raises interesting challenges for semanticcomposition in distributional semantic mod-els.
We contrast three types of adjectival mod-ifiers ?
intersectively used color terms (as inwhite towel, clearly first-order), subsectivelyused color terms (white wine, which have beenmodeled as both first- and higher-order), andintensional adjectives (former bassist, clearlyhigher-order) ?
and test the ability of differentcomposition strategies to model their behav-ior.
In addition to opening up a new empir-ical domain for research on distributional se-mantics, our observations concerning the at-tested vectors for the different types of adjec-tives, the nouns they modify, and the resultingnoun phrases yield insights into modificationthat have been little evident in the formal se-mantics literature to date.1 IntroductionOne of the most appealing aspects of so-called dis-tributional semantic models (see Turney and Pan-tel (2010) for a recent overview) is that they af-ford some hope for a non-trivial, computationallytractable treatment of the context dependence of lex-ical meaning that might also approximate in inter-esting ways the psychological representation of thatmeaning (Andrews et al 2009).
However, in or-der to have a complete theory of natural languagemeaning, these models must be supplied with orconnected to a compositional semantics; otherwise,we will have no account of the recursive potentialthat natural language affords for the construction ofnovel complex contents.In the last 4-5 years, researchers have begunto introduce compositional operations on distribu-tional semantic representations, for instance to com-bine verbs with their arguments or adjectives withnouns (Erk and Pado?, 2008; Mitchell and Lapata,2010; Baroni and Zamparelli, 2010; Grefenstetteand Sadrzadeh, 2011; Socher et al 2011)1.
Al-though the proposed operations have shown vary-ing degrees of success in a number of tasks such asdetecting phrase similarity and paraphrasing, it re-mains unclear to what extent they can account forthe full range of meaning composition phenomenafound in natural language.
Higher-order modifica-tion (that is, modification that cannot obviously bemodeled as property intersection, in contrast to first-order modification, which can) presents one suchchallenge, as we will detail in the next section.The goal of this paper is twofold.
First, we exam-ine how the properties of different types of adjecti-val modifiers, both in isolation and in combinationwith nouns, are represented in distributional mod-els.
We take as a case study three groups of adjec-tives: 1) color terms used to ascribe true color prop-erties (referred to here as intersective color terms),as prototypical representative of first-order modi-fiers; 2) color terms used to ascribe properties otherthan simple color (here, subsective color terms), asrepresentatives of expressions that could in principle1In a complementary direction, Garrette et al(2011) con-nect distributional representations of lexical semantics to logic-based compositional semantics.1223be given a well-motivated first-order or higher-orderanalysis; and 3) intensional adjectives (e.g.
former),as representative of modifiers that arguably require ahigher-order analysis.
Formal semantic models tendto group the second and third groups together, de-spite the existence of some natural language datathat questions this grouping.
However, our resultsshow that all three types of modifiers behave differ-ently from each other, suggesting that their semantictreatment needs to be differentiated.Second, we test how five different compositionfunctions that have been proposed in recent literaturefare in predicting the attested properties of nominalsmodified by each type of adjective.
The model byBaroni and Zamparelli (2010) emerges as a suitablemodel of adjectival composition, while multiplica-tion and addition shed mixed results.The paper is structured as follows.
Section 2 pro-vides the necessary background on the semantics ofadjectival modification.
Section 3 presents the meth-ods used in our study.
Section 4 describes the char-acteristics of the different types of adjectival modifi-cation, and Section 5, the results of the compositionoperations.
The paper concludes with a general dis-cussion of the results and prospects for future work.2 The semantics of adjectival modificationAccounting for inference in language is an impor-tant concern of semantic theory.
Perhaps for this rea-son, within the formal semantics tradition the mostinfluential classification of adjectives is based onthe inferences they license (see (Parsons, 1970) and(Kamp, 1975) for early discussion).
We very brieflyreview this classification here.First, so called intersective adjectives, such as (theliterally used) white in white dress, yield the infer-ence that both the property contributed by the ad-jective and that contributed by the noun hold of theindividual described; in other words, a white dressis white and is a dress.
The semantics for such mod-ifiers is easily characterized in terms of the intersec-tion of two first-order properties, that is, propertiesthat can be ascribed to individuals.On the other extreme, intensional adjectives, suchas former or alleged in former/alleged criminal, donot license the inference that either of the propertiesholds of the individual to which the modified nom-inal is ascribed.
Indeed, such adjectives cannot beused as predicates at all:(1) ?
?The criminal was former/alleged.The infelicity of (1) is generally attributed to thefact that these adjectives do not describe individu-als directly but rather effect more complex opera-tions on the meaning of the modified noun.
It is forthis reason that these adjectives can be consideredhigher-order modifiers: they behave as properties ofproperties.
Though rather abstract, the higher-orderanalysis is straightforwardly implementable in for-mal semantic models and captures a range of lin-guistic facts successfully.Finally, subsective adjectives such as (the non-literally-used) white in white wine, consitute an in-termediate case: they license the inference that theproperty denoted by the noun holds of the indi-vidual being described, but not the property con-tributed by the adjective.
That is, white wine isnot white but rather a color that we would proba-bly call some shade of yellow.
This use of colorterms, in general, is distinguished primarily by thefact that color serves as a proxy for another prop-erty that is related to color (e.g.
type of grape),though the color in question may or may not matchthe color identified by the adjective on the intersec-tive use (see (Ga?rdenfors, 2000) and (Kennedy andMcNally, 2010) for discussion and analysis).
Theeffect of the adjective, rather than to identify a valuefor an incidental COLOR attribute of an object, is of-ten to characterize a subclass of the class describedby the noun (white wine is a kind of wine, brownrice a kind of rice, etc.
).This use of color terms can be modeled by prop-erty intersection in formal semantic models only ifthe term is previously disambiguated or allowed todepend on context for its precise denotation.
How-ever, it is easily modeled if the adjective denotes a(higher-order) function from properties (e.g.
that de-noted by wine) to properties (that denoted by whitewine), since the output of the function denoted bythe color term can be made to depend on the input itreceives from the noun meaning.
Nonetheless, thereis ample evidence in natural language that a first-order analysis of the subsective color terms wouldbe preferable, as they share more features with pred-1224icative adjectives such as happy than they do withadjectives such as former.The trio of intersective color terms, subsectivecolor terms, and intensional adjectives provides fer-tile ground for exploring the different compositionfunctions that have been proposed for distributionalsemantic representations.
Most of these functionsstart from the assumption that composition takespairs of vectors (e.g.
a verb vector and a noun vec-tor) and returns another vector (e.g.
a vector forthe verb with the noun as its complement), usuallyby some version of vector addition or multiplication(Erk and Pado?, 2008; Mitchell and Lapata, 2010;Grefenstette and Sadrzadeh, 2011).
Such func-tions, insofar as they yield representations whichstrengthen distributional features shared by the com-ponent vectors, would be expected to model inter-sective modification.Consider the example of white dress.
We mightexpect the vector for dress to include non-zero fre-quencies for words such as wedding and funeral.The vector for white, on the other hand, is likelyto have higher frequencies for wedding than for fu-neral, at least in corpora obtained from the U.S. andthe U.K.
Combining the two vectors with an addi-tive or multiplicative operation should rightly yielda vector for white dress which assigns a higher fre-quency to wedding than to funeral.Additive and multiplicative functions might alsobe expected to handle subsective modification withsome success because these operations provide anatural account for how polysemy is resolved inmeaning composition.
Thus, the vector that resultsfrom adding or multiplying the vector for white withthat for dress should differ in crucial features fromthe one that results from combining the same vectorfor white with that for wine.
For example, depend-ing on the details of the algorithm used, we shouldfind the frequencies of words such as snow or milkyweakened and words like straw or yellow strength-ened in combination with wine, insofar as the formerwords are less likely than the latter to occur in con-texts where white describes wine than in those whereit describes dresses.
In contrast, it is not immedi-ately obvious how these operations would fare withintensional adjectives such as former.
In particular,it is not clear what specific distributional features ofthe adjective would capture the effect that the ad-jective has on the meaning of the resulting modifiednominal.Interestingly, recent approaches to the semanticcomposition of adjectives with nouns such as Baroniand Zamparelli (2010) and Guevara (2010) draw onthe classical analysis of adjectives within the Mon-tagovian tradition of formal semantic theory (Mon-tague, 1974), on which they are treated as higher or-der predicates, and model adjectives as matrices ofweights that are applied to noun vectors.
On suchmodels, the distributional properties of observed oc-currences of adjective-noun pairs are used to inducethe effect of adjectives on nouns.
Insofar as it isgrounded in the intuition that adjective meaningsshould be modeled as mappings from noun mean-ings to adjective-noun meanings, the matrix anal-ysis might be expected to perform better than ad-ditive or multiplicative models for adjective-nouncombinations when there is evidence that the adjec-tive denotes only a higher-order property.
There isalso no a priori reason to think that it would faremore poorly at modeling the intersective and subsec-tive adjectives than would additive or multiplicativeanalyses, given its generality.In this paper, we present the first studies that weknow of that explore these expectations.3 MethodWe built a semantic space and tested the composi-tion functions as specified in what follows.3.1 Semantic spaceThe semantic space we used for our experimentsconsists of a matrix where each row vector repre-sents an adjective, noun or adjective-noun phrase(henceforth, AN).
We first introduce the source cor-pus, then the vocabulary that we represent in thespace, and finally the procedure to build the vectorsrepresenting the vocabulary items from corpus data.3.1.1 Source corpusOur source corpus is the concatenation of theukWaC corpus2, a mid-2009 dump of the EnglishWikipedia3 and the British National Corpus4.
Thecorpus is tokenized, POS-tagged and lemmatized2http://wacky.sslmit.unibo.it/3http://en.wikipedia.org4http://www.natcorp.ox.ac.uk/1225with TreeTagger (Schmid, 1995) and contains about2.8 billion tokens.
We extracted all statistics at thelemma level, ignoring inflectional information.3.1.2 VocabularyThe core vocabulary of the semantic space con-sists of the 8K most frequent nouns and the 4K mostfrequent adjectives from the corpus.
By crossing theset of 700 most frequent adjectives (reduced to 663after removing questionable items like above, lessand very) and the 4K most frequent nouns and se-lecting those ANs that occured at least 100 timesin the corpus, we obtained a set of 179K ANs thatwe added to the semantic space, for a total of 191Krows.
These ANs were used for training the linearmodels as well as for providing a basis for the anal-ysis of the results.3.1.3 Semantic space parametersThe dimensions (columns) of our semantic spaceare the top 10K most frequent content words in thecorpus (nouns, adjectives, verbs and adverbs), ex-cluding the 300 most frequent words of all parts ofspeech.For each word or AN, we collected raw co-occurrence counts by recording their sentence-internal co-occurrence with each of words in the di-mensions.
The counts were then transformed intoLocal Mutual Information (LMI) scores, an associ-ation measure that closely approximates the com-monly used Log-Likelihood Ratio but is simpler tocompute (Evert, 2005).
Specifically, given a row el-ement r, a column element c and a counting functionC(r, c), thenLMI = C(r, c) ?
logC(r, c)C(?, ?
)C(r, ?
)C(?, c)(1)where C(r, c) is how many times r cooccurs withc, C(r, ?)
is the total count of r, C(?, c) is the to-tal count of c, and C(?, ?)
is the cumulative co-occurrence count of any r with any c.The dimensionality of the space was reduced us-ing Singular Value Decomposition (SVD), as in La-tent Semantic Analysis and related distributionalsemantic methods (Landauer and Dumais, 1997;Rapp, 2003; Schu?tze, 1997).
Both LMI and SVDwere used for the core vocabulary, and the AN vec-tors were computed based on the values for thecore vocabulary.
All of the results discussed in thearticle are based on the SVD-reduced space, be-cause it yielded consistently better results, except forthose involving multiplicative composition, whichwas carried out on the non-reduced model becauseSVD reduction introduces negative values for the la-tent dimensions used for the reduced space.Some of the parameters of the space and com-position functions were set based on performanceon independent word similarity and AN similaritytasks (Rubenstein and Goodenough, 1965; Mitchelland Lapata, 2010).
In addition to LMI, we testedthe performance using log-transformed frequenciesand found very poor performance in the aforemen-tioned tasks.
The number of latent dimensions forthe SVD-reduced space was set at 300 after testingthe performance using 300, 600 and 900 latent di-mensions.In the discussion, we use the cosine of two vectorsas a measure of similarity.
This is the most commonchoice in related work, as it has shown to be robustacross different tasks and settings, though other op-tions (in particular, measures that are not symmetricor do not normalize) could be explored (Widdows,2004).3.2 Composition modelsThe experiments described below were carried outusing five compositional methods that have been ex-plored in recent studies of compositionality in dis-tributional semantic spaces (Mitchell and Lapata,2010; Guevara, 2010; Baroni and Zamparelli, 2010).For each function, we define p as the compositionof the adjective vector, u, and the noun vector, v,a nomenclature that follows Mitchell and Lapata(2010).Additive (add) AN vectors were obtained bysumming the corresponding adjective and noun vec-tors.
We also explored the effects of the additivemodel with normalized component adjective andnoun vectors (addn).p = u + v (2)Multiplicative (mult) AN vectors were obtainedby component-wise multiplication of the adjectiveand noun vectors in the non-reduced semantic space.p = u v (3)1226Dilation (dl) AN vectors were obtained by calcu-lating the dot products of u?u and u?v and stretchingv by a factor ?
(in our case, 16.7) in the direction ofu (Clark et al 2008; Mitchell and Lapata, 2010).The effect of this operation is to ?stretch?
the headvector v (noun, in our case) in the direction of themodifying vector u (adjective).p = (u ?
u)v + (??
1)(u ?
v) (4)The factor ?
was selected based on the optimal pa-rameters presented in Mitchell and Lapata (2010).We tested both reported values (16.7 and 2.2) andfound ?
= 16.7 to perform better in terms of rank ofobserved equivalent (see Section 5).The preceding functions produce an AN vectorfrom the component A and N vectors.
The remain-ing two functions do not use the vector for the ad-jective, but learn a matrix representation for it.
Thecomposed AN vector is obtained by multiplying thematrix by the noun vector.
The general equation forthe two functions is the following, where B is a ma-trix of weights that is multiplied by the noun vectorv to produce the AN vector p.p = Bv (5)In the linear map (lim) approach proposed byGuevara (2010), one single matrix B is learnt thatrepresents all adjectives.
An AN vector is obtainedby multiplying the weight matrix by the concate-nation of the adjective and noun vectors, so thateach dimension of the generated AN vector is a lin-ear combination of dimensions of the correspond-ing adjective and noun vectors.
In our implementa-tion, B is an 300 x 300 weight matrix representingan adjective, and v is a 300-dimension noun vec-tor.
Following Guevara (2010), we estimate the co-efficients of the equation using (multivariate) partialleast squares regression (PLSR) as implemented inthe R pls package (Mevik and Wehrens, 2007), set-ting the latent dimension parameter of PLSR to 300.This value was chosen after testing values 100, 200and 300 on the AN similarity tasks (Mitchell andLapata, 2010).
Coefficient matrix estimation is per-formed by feeding PLSR a set of input-output exam-ples, where the input is given by concatenated ad-jective and noun vectors, and the output is the vectorof the corresponding AN directly extracted from oursemantic space.
The matrix is estimated using a ran-dom sample of 2.5K adjective-noun-AN tuples.5In the adjective-specific linear map (alm) model,proposed by Baroni and Zamparelli (2010), a dif-ferent matrix B is learnt for each adjective.
Theweights of each of the rows of the weight matrixare the coefficients of a linear equation predictingthe values of one of the dimensions of the normal-ized AN vector as a linear combination of the di-mensions of the normalized component noun.
Thelinear equation coefficients are estimated again us-ing PLSR, and in the present implementation we useridge regression generalized cross-validation (GCV)to automatically choose the optimal ridge parameterfor each adjective (Golub et al 1979).
This pro-cedure drastically outperforms setting a fixed num-ber of dimensions.
The model is trained on all N-AN vector pairs available in the semantic space foreach adjective, and range from 100 to over 1K itemsacross the adjectives we tested.3.3 DatasetsWe built two datasets of adjective-noun phrases forthe present research, one with color terms and onewith intensional adjectives.6Color terms.
This dataset is populated with a ran-domly selected set of adjective-noun pairs from thespace presented above.
From the 11 colors in the ba-sic set proposed by Berlin and Kay (1969), we cover7 (black, blue, brown, green, red, white, and yel-low), since the remaining (grey, orange, pink, andpurple) are not in the 700 most frequent set of ad-jectives in the corpora used.
From an original setof 412 ANs, 43 were manually removed because ofsuspected parsing errors (e.g.
white photograph, forblack and white photograph) or because the headnoun was semantically transparent (white variety).The remaining 369 ANs were tagged independentlyby the second and fourth authors of this paper, bothnative English speaker linguists, as intersective (e.g.white towel), subsective (e.g.
white wine), or id-iomatic, i.e.
compositionally non-transparent (e.g.black hole).
They were allowed the assignment of at52.5K ANs is the upper bound of the software package used.6Available at http://dl.dropbox.com/u/513347/resources/data-emnlp2012.zip.
See Bruni et al(toappear) for an analysis of the color term dataset from a multi-modal perspective.1227most two labels in case of polysemy, for instance forblack staff for the person vs. physical object sensesof the noun or yellow skin for the race vs. literallypainted interpretations of the AN.
In this paper, onlythe first label (most frequent interpretation, accord-ing to the judges) has been used.
The ?
coefficient ofthe annotation on the three categories (first interpre-tation only) was 0.87 (conf.
int.
0.82-0.92, accordingto Fleiss et al(1969)), observed agreement 0.96.7There were too few instances of idioms (17) for aquantitative analysis of the sort presented here, sothese are collapsed with the subsective class in whatfollows.8 The dataset as used here consists of 239intersective and 130 subsective ANs.Intensional adjectives.
The intensional datasetcontains all ANs in the semantic space with a pre-selected list of 10 intensional adjectives, manuallypruned by one of the authors of the paper to elimi-nate erroneous examples and to ensure that the ad-jective was being intensionally used.
Examples ofthe ANs eliminated on these grounds include pasttwelve (cp.
accepted past president), former girl(probably former girl friend or similar), false rumor(which is a real rumor that is false, vs. e.g.
falsefloor, which is not a real floor), or theoretical work(which is real work related to a theory, vs. e.g.
theo-retical speed, which is a speed that should have beenreached in theory).
Other AN pairs were excludedon the grounds that the noun was excessively vague(e.g.
past one) or because the AN formed a fixedexpression (e.g.
former USSR).
The final datasetcontained 1,200 ANs, distributed as follows: former(300 examples), possible (244), future (243), poten-tial (183), past (87), false (44), apparent (39), arti-ficial (36), likely (18), theoretical (6).9Table 1 contains examples of each type of AN weare considering.7Code for the computation of inter-annotator agreement byStefan Evert, available at http://www.collocations.de/temp/kappa_example.zip.8An alternative would have been to exclude idiomatic ANsfrom the analysis.9Alleged, one of the most prototypical intensional adjectives,is not considered here because it was not among the 700 mostfrequent adjectives in the space.
We will consider it in futurework.Intersective Subsective Intensionalwhite towel white wine artificial legblack sack black athlete former bassistgreen coat green politics likely suspectred disc red ant possible delayblue square blue state theoretical limitTable 1: Example ANs in the datasets.4 Observed vectorsWe began by exploring the empirically observedvectors for the adjectives (A), nouns (N), andadjective-noun phrases (AN) in the datasets, as theyare represented in the semantic space.
Note thatwe are working with the AN vectors directly har-vested from the corpora (that is, based on the co-occurrence of, say, the phrase white towel with eachof the 10K words in the space dimensions), with-out doing any composition.
AN vectors obtained bycomposition will be examined in the following sec-tion.
Though observed AN vectors should not beregarded as a gold standard in the sense of, for in-stance, Machine Learning approaches, because theyare typically sparse10 and thus the vectors of theircomponent adjective and noun will be richer, theyare still useful for exploration and as a compari-son point for the composition operations (Baroni andLenci, 2010; Guevara, 2010).Figure 1 shows the distribution of the cosines be-tween A, N, and AN vectors with intensional adjec-tives (I, white box), intersective uses of color terms(IE, lighter gray box), and subsective uses of colorterms (S, darker gray box).In general, the similarity of the A and N vectors isquite low (cosine < 0.2, left graph of Figure 1), andmuch lower than the similarities between both theAN and A vectors and the AN and N vectors.
Thisis not surprising, given that adjectives and nouns de-scribe rather different sorts of things.We find significant differences between the threetypes of adjectives in the similarity between AN andA vectors (middle graph of Figure 1).
The adjec-tive and adjective-noun phrase vectors are nearer for10The frequency of the adjectives in the datasets range from3.5K to 3.7M, with a median frequency of 109,114.
The nounsrange from 4.9K to 2.5M, with a median frequency of 148,459.While the frequency of the ANs range from 100 to 18.5K, witha median frequency of 239.1228lllllllllllllllllllllllllllllI L N0.00.20.40.60.81.0cos(A,N)lI L N0.00.20.40.60.81.0cos(AN,A)I L N0.00.20.40.60.81.0cos(AN,N)Figure 1: Cosine distance distribution in the different types of AN.
We report the cosines between the componentadjective and noun vectors (cos(A,N)), between the observed AN and adjective vectors (cos(AN,A)), and between theobserved AN and noun vectors (cos(AN,N)).
Each chart contains three boxplots with the distribution of the cosinescores (y-axis) for the intensional (I), intersective (IE), and subsective (S) types of ANs.
The boxplots represent thevalue distribution of the cosine between two vectors.
The horizontal lines in the rectangles represent the first quartile,median, and third quartile.
Larger rectangles correspond to a more spread distribution, and their (a)symmetry mirrorsthe (a)symmetry of the distribution.
The lines above and below the rectangle stretch to the minimum and maximumvalues, at most 1.5 times the length of the rectangle.
Values outside this range (outliers) are represented as points.intersective uses than for subsective uses of colorterms, a pattern that parallels the difference in thedistance between component A and N vectors.
Sinceintersective uses correspond to the prototypical useof color terms (a white dress is the color white, whilewhite wine is not), the greater similarity for the in-tersective cases is unsurprising ?
it suggests that inthe case of subsective adjectival modifiers, the noun?pulls?
the AN further away from the adjective thanhappens with the cases of intersective modification.This is compatible with the intuition (manifest in theformal semantics tradition in the treatment of sub-sective adjectives as higher-order rather than first-order, intersective modifiers) that the adjective?s ef-fect on the AN in cases of subsective modificationdepends heavily on the interpretation of the nounwith which the adjective combines, whereas that isless the case when the adjective is used intersec-tively.As for intensional adjectives, the middle graphshows that their AN vectors are quite distant fromthe corresponding A vectors, in sharp contrast towhat we find with both intersective and subsectivecolor terms.
We hypothesize that the results for theintensional adjectives are due to the fact that theycannot plausibly be modeled as first order attributes(i.e.
being potential or apparent is not a propertyin the same sense that being white or yellow is) andthus typically do not restrict the nominal descriptionper se, but rather provide information about whetheror when the nominal description applies.
The re-sult is that intensional adjectives should be evenweaker than subsectively used adjectives, in com-parison with the nouns with which they combine, intheir ability to ?pull?
the AN vector in their direc-tion.
Note, incidentally, that an alternative expla-nation, namely that the effect mentioned could bedue to the fact that most nouns in the intensionaldataset are abstract and that adjectives modifyingabstract nouns might tend to be further away fromtheir nouns altogether, is ruled out by the compari-son between the A and N vectors: the A-N cosinesof the intensional and intersective ANs are similar.We thus conclude that here we see an effect of thetype of modification involved.An examination of the average distances among1229the nearest neighbors of the intensional and of thecolor adjectives in the distributional space supportsour hypothesized account of their contrasting be-haviors.
We predict that the nearest neighbors aremore dispersed for adjectives that cannot be mod-eled as first-order properties (i.e., intensional adjec-tives), than for those that can (here, the color terms).We find that the average cosine distance among thenearest ten neighbors of the intensional adjectives is0.74 with a standard deviation of 0.13, which is sig-nificantly lower (t-test, p<0.001) than the averagesimilarity among the nearest neighbors of the coloradjectives, 0.96 with astandard deviation of 0.04.Finally, with respect to the distances between theadjective-noun and head noun vectors (right graphof Figure 1), there is no significant difference for theintersective vs. subsective color terms.
This can beexplained by the fact that both kinds of modifiersare subsective, that is, the fact that a white dress is adress and that white wine is wine.In contrast, intensional ANs are closer to theircomponent Ns than are color ANs (the differenceis qualitatively quite small, but significant even forthe intersective vs. intensional ANs according to at-test, p-value = 0.015).
This effect, the inverse ofwhat we find with the AN-A vectors, can similarlybe explained by the fact that intensional adjectivesdo not restrict the descriptive content of the nounthey modify, in contrast to both the intersective andsubsective color ANs.
Restriction of the nominaldescription may lead to significantly restricted dis-tributions (e.g.
the phrase red button may appearin distinctively different contexts than does button;similarly for green politics and politics), while wedo not expect the contexts in which former bassistand bassist appear to diverge in a qualitatively dif-ferent way because the basic nominal descriptionsare identical, though further research will be neces-sary to confirm these explanations.Finally, note that, contrary to predictions fromsome approaches in formal semantics, subsectivecolor ANs and intensional ANs do not pattern to-gether: subsective ANs are closer to their compo-nent As, and intensional ANs closer to their compo-nent Ns.
This unexpected behavior underscores thefact highlighted in the previous paragraph: that thedistributional properties of modified expressions aremore sensitive to whether the modification restrictsthe nominal description than to whether the modifieris intersective in the strictest sense of term.We now discuss the extent to which the differentcomposition functions account for these patterns.5 Composed vectorsSince intersective modification is the point of com-parison for both subsective and intensional modifi-cation, we first discuss the composed vectors for theintersective vs. subsective uses of color terms, andthen turn to intersective vs. intensional modification.5.1 Intersective and subsective modificationwith color termsTo adequately model the differences between inter-sective and subsective modification observed in theprevious section, a successful composition functionshould yield a significantly smaller distance betweenthe adjective and AN vectors for intersectively usedadjectives, whereas it should yield no significant dif-ference for the distances between the noun and ANvectors.Table 2 provides a summary of the results withthe observed data (obs) and the composition func-tions discussed in Section 3.2.
The median rank ofobserved equivalent (ROE) is provided as a generalmeasure of the quality of the composition function.It is computed by finding the cosine between thecomposed AN vectors and all rows in the semanticspace and then determining the rank in which the ob-served ANs are found.11 The remaining columns re-port the differences in standardized (z-score) cosinesbetween the vector built with each of the composi-tion functions and the observed AN, A, and N vec-tors.
A positive value means that the cosines forintersective uses are higher, while a negative valuemeans that the cosines for subsective uses are higher.The first row (obs) contains a numerical summaryof the tendencies for observed ANs explained in theprevious section.
This is the behavior that we expectto model.Two composition functions come close to mod-eling the observed behavior: alm and mult, thoughalm is better in terms of ROE, consistent with the11The ROE is provided as a general guide; however, recallthat the ROE was taken into account to tune the ?
parameter inthe dilation model, and that the ANs of the color dataset wereincluded when training the matrices for the alm model.1230model ROE ?
:AN ?
:A ?
:Nobs - - .54 ???
.10add 286 .40 ???
.14 .15addn 11 .40 ???
.65 ???
.65 ??
?mult 111 .40 ???
.74 ???
.29 ?dl 298 .63 ???
.85 ???
-.66 ??
?lim 1,940 .46 ???
.20 .38 ?
?alm 1 .16 .52 ???
.27 ?Table 2: Intersective vs. subsective uses of color terms.The first column reports the rank of the observed equiva-lent (ROE), the rest report the differences (?)
betwen theintersective and subsective uses of color terms when com-paring the composed AN with the observed vectors for:AN, adjective (A), noun (N).
See text for details.
Signifi-cances according to a t-test: *** for p< 0.001, **< 0.01,* < 0.05.results reported in Baroni and Zamparelli (2010).In both cases, we find that these functions yieldhigher similarities for AN-A for the intersective thanfor the subsective uses of color terms, and a veryslight (though still mildly significant) difference forthe distance to the head noun.
The addn functionperforms very good in terms of ROE (median 11).This suggests that, for adjectival modification, pro-viding a vector that is in the middle of the twocomponent vectors (which is what normalized ad-dition does) is a reasonable approximation of theobserved vectors.
However, precisely because theresulting vector is in the middle of the two com-ponent vectors, this function cannot account for theasymmetries in the distances found in the observeddata.
The non-normalized version also cannot ac-count for these effects because the adjective vec-tor, being much longer (as color terms are very fre-quent), totally dominates the AN, which results inno difference across uses when comparing to the ad-jective or to the noun.The dilation model shows a strange pattern, as ityields a strongly significant negative difference inthe AN-N distance.
The lim function exhibits the op-posite pattern as predicted, yielding no difference forthe AN-A similarities and a difference for the AN-N similarities.
A possible explanation for the AN-A results is that lim learns from such a broad rangeof AN pairs that the impact of the distance betweenintersective vs. subsective uses of color terms fromtheir component adjectives is dampened.
Moreover,lim is by far the worst function in terms of ROE.All composition functions except for alm find in-tersective uses easier to model.
This is shown in thepositive values in column ?
:AN, which mean thatthe similarity between observed and composed ANvectors is greater for intersective than for subsectiveANs.
This is consistent with expectations.
The sub-sective uses are specific to the nouns with which thecolor terms combine, and the exact interpretation ofthe adjective varies across those nouns.
In contrast,the interpretation associated with intersective use isconsistent across a larger variety of nouns, and inthat sense should be predominantly reflected in theadjective?s vector.
The exception in this respect isthe alm function, since the weights for each adjec-tive matrix are estimated in relation to the noun vec-tors with which the adjective combines, on the onehand, and the related observed AN vectors, on theother; thus, the basic lexical representation of theadjective is inherently reflective of the distributionsof the ANs in which it appears in a way that is notthe case for the adjective representations used in theother composition models.
And indeed, alm is theonly function that shows no difference in difficulty(distance) between the predicted and observed ANvectors for intersective vs. subsective ANs.Both mult and alm seem to account for the ob-served patterns in color terms.
However, an exam-ination of the nearest neighbors of the composedANs suggest that alm captures the semantics of ad-jective composition in this case to a larger extentthan mult.
For instance, the NN for blue square (in-tersective) are the following according to mul: blue,red, official colour, traditional colour, blue num-ber, yellow; while alm yields the following: bluesquare, red square, blue circle, blue triangle, bluepattern, yellow circle.
Similarly, for green poli-tics (subsective) mul yields: pleasant land, greenbusiness, green politics, green issue, green strategy,green product, while alm yields green politics, greenmovement, political agenda, environmental move-ment, progressive government, political initiative.5.2 Intensional modificationTable 3 contains the results of the composition func-tions comparing the behavior of intersective colorANs and intensional ANs.
The tendencies in theROE are as in Table 2, so we will not comment on1231model ROE ?
:AN ?
:A ?
:Nobs - - 1.39 ???
-.27 ??
?add 198 .66 ???
.71 ???
-.81 ??
?addn 40 .93 ???
.20 ?
.20 ?mult 110 .58 ???
1.09 ???
-.25 ??
?dl 354 .97 ???
-.27 ??
.47 ??
?lim 7,943 .27 ???
.65 ???
-.47 ??
?alm 1 .81 ???
1.43 ???
-.59 ??
?Table 3: Intersective vs. intensional ANs.
Information asin Table 2.them further (note the very poor performance of lim,though).
As noted above, we expect more difficultyin modeling intensional modification vs. other kindsof modification, and this is verified in the results(cf.
the positive values in second column).
The dif-ference with the results in the previous subsectionis that in this case the alm function does present ahigher difficulty in modeling intensional ANs, un-like with the color terms.
This points to a qualitativedifference between subsective and intensional adjec-tives that could be evidence for a first-order analysisof subsective color terms.A good composition function should provide alarge positive difference when comparing the ANto the A, and a small negative difference (becausethe effect is very small in the observed data) whencomparing the AN to the N. The functions that bestmatch the observed data are again alm and mult.Add and lim show the predicted pattern, but to amuch lesser degree (cf.
smaller differences in col-umn ?:A).
Dl yields the exact opposite effect andaddn, though good in terms of ROE, is subject tothe problems discussed in the previous section.Again, alm seems to be capturing relevant seman-tic aspects of composition with intensional adjec-tives.
For instance, the nearest neighbors of artificialleg according to alm are artificial leg, artificial limb,artificial joint, artificial hip, scar, small wound.6 Discussion and conclusionsThe present research provides some evidence fortreating adjectives as matrices or functions, ratherthan vectors, although simple operations on vectorssuch as addition (for its excellent approximation toobserved vectors) and multiplication (for its abilityto reproduce the observed trends in the data) still ac-count for some aspects of adjectival modification.The dilation model, in contrast, is not suitable foradjectival modification.Our results also show that alm performs betterthan lim, but it is worth observing that it does soat the expense of modeling each adjective as a com-pletely different function.
We consider lim very at-tractive in principle because it generalizes across ad-jectives and is thus more parsimonious.
Part of thepoor results on lim were due to limitations of ourimplementation, as we trained the matrices on only2.5K ANs, while our semantic space contains morethan 170K ANs.
However, the linguistic literatureand the present results suggest that it might be use-ful to try a compromise between alm and lim, train-ing one matrix for each subclass of adjectives underanalysis.Beyond the new data it offers regarding the com-parative ability of the different composition func-tions to account for different kinds of adjectivalmodification, the study presented here underscoresthe complexity of modification as a semantic phe-nomenon.
The role of adjectival modifiers as restric-tors of descriptive content is reflected differently indistributional data than is their role in providing in-formation about whether or when a description ap-plies to some individual.
Formal semantic models,thanks to their abstractness, are able to handle thesetwo roles with little difficulty, but also with limitedinsight.
Distributional models, in contrast, offer thepromise of greater insight into each of these roles,but face serious challenges in handling both of themin a unified manner.AcknowledgmentsThis research was funded by the Spanish MICINN(FFI2010-09464-E, FFI2010-15006, TIN2009-14715-C04-04), the Catalan AGAUR (2010BP-A00070), and the EU (PASCAL2; FP7-ICT-216886).
Eva Maria Vecchi was partially fundedby ERC Starting Grant 283554.
We thank MarcoBaroni, Roberto Zamparelli, and six anonymousreviewers for valuable feedback, and Yao-zhongZhang for the code for the alm function.1232ReferencesMark Andrews, Gabriella Vigliocco, and David Vinson.2009.
Integrating experiential and distributional datato learn semantic represenations.
Psychological Re-view, 116(3):463?498.Marco Baroni and Alessandro Lenci.
2010.
Dis-tributional Memory: A general framework forcorpus-based semantics.
Computational Linguistics,36(4):673?721.Marco Baroni and Roberto Zamparelli.
2010.
Nounsare vectors, adjectives are matrices: Representingadjective-noun constructions in semantic space.
InProceedings of EMNLP, pages 1183?1193, Boston,MA.Brent Berlin and Paul Kay.
1969.
Basic Color Terms:Their Universality an Evolution.
University of Cali-fornia Press, Berkeley and Los Angeles, CA.E.
Bruni, G. Boleda, M. Baroni, and N. K. Tran.
to ap-pear.
Distributional semantics in technicolor.
In Pro-ceedings of ACL 2012.Stephen Clark, Bob Coecke, and Mehrnoosh Sadrzadeh.2008.
A compositional distributional model of mean-ing.
In Proceedings of the AAAI Spring Symposium onQuantum Interaction, pages 52?55, Stanford, CA.Katrin Erk and Sebastian Pado?.
2008.
A structured vec-tor space model for word meaning in context.
In Pro-ceedings of EMNLP, pages 897?906, Honolulu, HI,USA.Stefan Evert.
2005.
The Statistics of Word Cooccur-rences.
Dissertation, Stuttgart University.Joseph L. Fleiss, Jacob Cohen, and B. S. Everitt.
1969.Large sample standard errors of kappa and weightedkappa.
Psychological Bulletin, 72(5):323?327.Peter Ga?rdenfors.
2000.
Conceptual Spaces: The Geom-etry of Thought.
MIT Press, Cambridge, MA.Dan Garrette, Katrin Erk, and Raymond Mooney.
2011.Integrating logical representations with probabilisticinformation using markov logic.
In Proceedings ofIWCS 2011.G.H.
Golub, M. Heath, and G. Wahba.
1979.
General-ized cross-validation as a method for choosing a goodridge parameter.
Technometrics, pages 215?223.Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011.Experimenting with transitive verbs in a discocat.
InProceedings of the GEMS 2011 Workshop on GEomet-rical Models of Natural Language Semantics.Emiliano Guevara.
2010.
A regression model ofadjective-noun compositionality in distributional se-mantics.
In Proceedings of the ACL GEMS Workshop,pages 33?37, Uppsala, Sweden.H.
Kamp.
1975.
Two theories about adjectives.
Formalsemantics of natural language, pages 123?155.Christopher Kennedy and Louise McNally.
2010.
Color,context, and compositionality.
Synthese, 174:79?98.Thomas Landauer and Susan Dumais.
1997.
A solu-tion to Plato?s problem: The latent semantic analysistheory of acquisition, induction, and representation ofknowledge.
Psychological Review, 104(2):211?240.Bjo?rn-Helge Mevik and Ron Wehrens.
2007.
Thepls package: Principal component and partial leastsquares regression in R. Journal of Statistical Soft-ware, 18(2).
Published online: http://www.jstatsoft.org/v18/i02/.Jeff Mitchell and Mirella Lapata.
2010.
Composition indistributional models of semantics.
Cognitive Science,34(8):1388?1429.Richard Montague.
1974.
Formal philosophy: SelectedPapers of Richard Montague.
Yale University Press,New Haven.Terence Parsons.
1970.
Some problems concerning thelogic of grammatical modifiers.
Synthese, 21:320?334.Reinhard Rapp.
2003.
Word sense discovery based onsense descriptor dissimilarity.
In Proceedings of the9th MT Summit, pages 315?322, New Orleans, LA,USA.Herbert Rubenstein and John Goodenough.
1965.
Con-textual correlates of synonymy.
Communications ofthe ACM, 8(10):627?633.Helmut Schmid.
1995.
Improvements in part-of-speechtagging with an application to German.
In Proceed-ings of the EACL-SIGDAT Workshop, Dublin, Ireland.Hinrich Schu?tze.
1997.
Ambiguity Resolution in NaturalLanguage Learning.
CSLI, Stanford, CA.R.
Socher, J. Pennington, E.H. Huang, A.Y.
Ng, and C.D.Manning.
2011.
Semi-supervised recursive autoen-coders for predicting sentiment distributions.
In Pro-ceedings of the Conference on Empirical Methods inNatural Language Processing, pages 151?161, Edin-burgh, UK.Peter Turney and Patrick Pantel.
2010.
From frequencyto meaning: Vector space models of semantics.
Jour-nal of Artificial Intelligence Research, 37:141?188.Dominic Widdows.
2004.
The Geometry of Meaning.CSLI Publications, Stanford, CA.1233
