Proceedings of the of the EACL 2014 Workshop on Dialogue in Motion (DM), pages 63?67,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsConversational Strategies for Robustly Managing Dialog in Public SpacesAasish Pappu Ming SunLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh PA, USA{aasish,mings,seshadrs,air}@cs.cmu.eduSeshadri Sridharan Alexander I. RudnickyAbstractOpen environments present an attentionmanagement challenge for conversationalsystems.
We describe a kiosk system(based on Ravenclaw?Olympus) that usessimple auditory and visual information tointerpret human presence and manage thesystem?s attention.
The system robustlydifferentiates intended interactions fromunintended ones at an accuracy of 93%and provides similar task completion ratesin both a quiet room and a public space.1 IntroductionDialog systems designers try to minimize disrup-tive influences by introducing physical and be-havioral constraints to create predictable environ-ments.
This includes using a closed-talking mi-crophone or limiting interaction to one user at atime.
But such constraints are difficult to applyin public environments such as kiosks (Bohus andHorvitz, 2010; Foster et al., 2012; Nakashima etal., 2014), in-car assistants (Kun et al., 2007; Hof-mann et al., 2013; Misu et al., 2013) or on mo-bile robots (Haasch et al., 2004; Sabanovic et al.,2006; Kollar et al., 2012).
To implement dialogsystems that operate in public spaces, we have torelax some of these constraints and deal with addi-tional challenges.
For example, the system needsto select the correct interlocutor, who may be onlyone of several possible ones in the vicinity, thendetermine whether they are initiating the processof engaging with the system.In this paper we focus on the problems ofidentifying a potential interlocutor in the environ-ment, engaging them in conversation and provid-ing suitable channel-maintenance cues (Bruce etal., 2002; Fukuda et al., 2002; Al Moubayed andSkantze, 2011).
We address these problems in thecontext of a simple application, a kiosk agent thatFigure 1: Ravenclaw?Olympus augmented withmultimodal input and output functions.accepts tasks such as taking a message to a namedrecipient.
To evaluate the effectiveness of our ap-proach we compared the system?s ability to man-age conversations in a quiet room and in a publicarea.The remainder of this paper is organized as fol-lows: we first describe the system architecture,then present the evaluation setup and the results,then review related work and finally conclude withan analysis of the study.2 System ArchitectureFigure 1 shows the architecture; it incorporatesRavenclaw/Olympus (Bohus et al., 2007) stan-dard components (in white), new components (inblack) and modified ones (shaded).
In the systempipeline, the Audio Server receives audio from amicrophone, endpoints it and sends it to the ASR63Figure 2: Face states; some are animations.engine (PocketSphinx); the decoding is passed toNLU (Phoenix parser).
ICE (Input Confidence Es-timation) (Helios) assigns confidence scores forthe input concepts.
Based on user?s input andthe context, the Dialog Manager (DM) determineswhat to do next, perhaps using data from theDomain Reasoner (DR).
An Interaction Manager(IM) initiates a spoken response using NaturalLanguage Generation (NLG) and Text-to-Speech(TTS) component.Three components were added: (1) MultimodalCapture acquires audio and human position datausing a Kinect device1.
(2) Awareness deter-mines whether there is a potential interlocutor inthe vicinity and their current position, using skele-tal and azimuth information.
(3) Talking Headthat conveys the system?s state (as shown in Fig-ure 2): whether it?s active (conversing and hint-ing) or idle (asleep and doze) and whether fo-cused concepts are grounded (conversing and non-understanding); certain state representations (e.g.,conversing) are coordinated with the TTS compo-nent.3 EvaluationA robust system should be able to function as wellin a difficult situation as in a controlled one.
Wecompare the system?s performance in two environ-ments, public and quiet, and evaluate the (a) sys-tem?s awareness of intended users, and its (b) end-to-end performance.The same twenty subjects participated in both1See http://www.microsoft.com/en-us/Kinectforwindows/develop/.
Three sources aretapped: the beam-formed audio, the sound source azimuthand skeleton coordinates.
Video data are not used.experiments: a mix of American, Indian, Chineseand Hispanic with different fluency levels of En-glish.
None of them had previously interacted withthis system prior to this study.The subjects were told that they would interactwith a virtual agent displayed on a screen.
Theirtask for the awareness experiment was to make theagent aware that they wished to interact.
For theend-to-end system performance, the task was toinstruct the agent to send a message to a namedrecipient.3.1 Situated AwarenessWe define situated awareness as correctly engag-ing the intended interlocutor (i.e., verbally ac-knowledge the user?s presence) under two con-ditions.
When the user is positioned (i) insidethe visual range of the Kinect at LOC-0 in Fig-ure 3(a); and (ii) outside the visual range of theKinect at LOC-1 in Figure 3(a).
We used the effec-tive range of the camera?s documented horizontalfield of view (57?
); hereafter referred as its cone-of-awareness.We conducted the awareness experiment in apublic space, a lounge at a hub connecting mul-tiple corridors.
The area has tables and seating,self-serve coffee, a microwave oven, etc.
The ex-periment was conducted during regular hours, be-tween 10am to 6pm on weekdays.
During thesetimes we observed occupants discussing projects,preparing food, making coffee, etc.
No direct at-tempt was made to influence their behavior and webelieve that they made no attempt to accommo-date our activities.
Accordingly, the natural soundlevel in the room varied in unpredictable ways.
Tosupplement naturally-occurring sounds, we playedaudio of a conversation between two humans, anextract from the SwitchBoard corpus (Graff et al.,2001).
It was played using a loudspeaker placed atLOC-2 in Figure 3(a).
The locations (0, 1, and 2)are all 1.5m from the Kinect, which we deemed tobe a comfortable distance for the subjects.
LOC-1 and LOC-2 are 70?to the left and right of theKinect, outside its cone.To detect the presence of an intended user, webuild an awareness model that uses three sensorystreams viz., voice activity, skeleton, and soundsource azimuth.
This model relies on the co-incidence of azimuth angle and the skeleton angle(along with voice activity) to determine the pres-ence of an intended user.
We compare the pro-64Figure 3: (a) Plan of Public Space (lounge);(b) Plan of Quiet Room (lab).
Dark circled markers indicatelocations (LOC-0, LOC-1, LOC-2), discussed in the text.Condition Voice +Skeleton +AzimuthOutsidethe cone 28% ?
93%Insidethe cone ?
25% 93%Table 1: Accuracy for the Awareness Detectionposed model with two baselines: (1) conventionalvoice-activity-detection (VAD): once speech is de-tected the system responds as if a conversation isinitiated and (2) based on skeleton plus VAD: oncethe skeleton appears in front of the Kinect and avoice is heard, the system engages in conversation.Table 1 shows the combination of sensorystreams we used under two conditions.
For theoutside-the-cone condition, the participants standin LOC-1 as shown in Figure 3(a) and follow theinstructions from the agent.
Initially, the sub-ject?s skeleton is invisible to the agent; howeverthe subject is audible to the agent.
Therefore, incertain combinations of sensors (e.g., voice +skeleton model and voice + skeleton+ azimuth model) the system attempts to guidethem to move in front of it, i.e.
to LOC-0, anideal position for interacting with the system.
Forinside-the-cone condition, subjects stand at LOC-0 where the agent can sense their skeleton.When user stands at LOC-1 i.e., outside-the-cone voice + skeleton model andvoice + skeleton + azimuth modelsare functionally the same since the source ofdistraction has no skeleton in the cone.
Whenuser stands at LOC-0, i.e., inside-the-cone voicealone is the same as voice + skeletonmodel since the agent always sees a skeleton infront of it.
Therefore, this variant was not used.We treated awareness detection as a binary de-cision.
An utterance is classified either as ?in-tended?
or ?unintended?.
We manually labeled theutterances whether they were directed at the sys-tem (?intended?
), ?unintended?
otherwise.
Accu-racy on ?intended?
speech is reported in the Ta-ble 1.
Within each condition, the order of the ex-periments with different awareness strategies wasrandomized.We observe that the voice + skeleton +azimuth model proves to be robust in the pub-lic space.
Its performance is significantly better,t(38) = 8.1, p ?
0.001, compared to the otherbaselines in both conditions.
This result agreeswith previous research (Haasch et al., 2004; Bohusand Horvitz, 2009) showing that a fusion of multi-modal features improves performance over a uni-modal approach.
Our result indicates that a sim-ple heuristic approach, using minimal visual andaudio features, provides usable attention manage-ment in open environments.
This approach helpedthe system handle a complex interaction scenariosuch as out-of-cone speech directed to the sys-tem.
If the speaker is out of range but is producingpossibly system-directed utterances, system urgesthem to step to the front.
We believe it can be ex-tended to other complex cases by introducing ad-ditional logic.3.2 End-to-End System PerformanceTo investigate the effect of the environment, wecompare the system?s performance in public spaceand quiet room.
The average noise level in thequiet room is about 47dB(A) with computers as65Metric PublicSpaceQuietRoomSuccess Ratio 15/20 16/20Avg # Turns 14.2 16.4Concept Acc 67% 68%Table 2: Public Space vs Quiet Room Performancethe primary source of noise.
The backgroundsound level in the public space was 46dB; othernatural sources ranged up to 57dB.
The audio dis-tractor measured 57dB.
The same ASR acousticmodels and processing parameters were used inboth environments.
The participant stood at LOC-0 in Figure 3(a) during the public space experi-ment and Figure 3(b) during the quiet room ex-periment.
In both experiments, LOC-0 is 1.5maway from the system.
We used the voice +skeleton + azimuth model to discriminateuser speech from distractions in the environment.We gave each participant a randomized seriesof message-sending tasks, e.g.
?send a messageto ?person?
who is in room ?number??.
Subjectshad a maximum of 3 minutes to complete; eachtask required 7 turns.
The number of tasks com-pleted (over the group) is reported in terms oftask ?success-ratio?.
Table 2 shows the success-ratio of the task, the average number of turnsneeded to complete the task, and the system?s per-utterance concept accuracy (Boros et al., 1996).There were no statistically significant differencesbetween quiet room and public space, (t(38) <2, p > 0.5, on any metric).
We conclude thatthe channel maintenance technique we tested wasequally effective in both environments.4 Related WorkThe problem of deploying social agents in publicspaces has been of enduring interest; (Bohus andHorvitz, 2010) list engagement as a challenge fora physically situated agent in open-world interac-tions.
But the problem was noted earlier and solu-tions were proposed; e.g a ?push-to-talk?
protocolto signal the onset of intended user speech (Stentet al., 1999).
(Sharp et al., 1997; Hieronymus etal., 2006) described the use of attention phrase asa required prefix to each user input.
Although ex-plicit actions are effective, they need to be learnedby users.
This may not be practical for systems inpublic areas engaged by casual users.A more robust approach involves fusing sev-eral sources of information such as audio, gazeand pose(Horvitz et al., 2003; Bohus and Horvitz,2009) (Hosoya et al., 2009; Nakano and Ishii,2010).
Previous works have shown that fusionof different sensory information can improve at-tention management.
The drawback of such ap-proaches is in the complexity of the sensor equip-ment.
Our work attempts to create the rele-vant capabilities using a simple sensing deviceand relying on explicitly modeled conversationalstrategies.
Others are also using the MicrosoftKinect device for research in dialog.
For example,(Skantze and Al Moubayed, 2012) and (Foster etal., 2012) presented a multiparty interaction sys-tems that use Kinect for face tracking and skeletontracking combined with speech recognition.In our current work, we show that situationalawareness can be integrated into an existing dia-log framework, Ravenclaw?Olympus, that was notoriginally designed with this functionality in mind.The source code of the framework presented inthis work is publicly available for download1andthe acoustic models that have been adapted to theKinect audio channel25 ConclusionWe found that a conventional spoken dialog sys-tem can be adapted to a public space with mini-mal modifications to accommodate additional in-formation sources.
Investigating the effectivenessof different awareness strategies, we found that asimple heuristic approach that uses a combinationof sensory streams viz., voice, skeleton and az-imuth, can reliably identify the likely interlocutor.End-to-end system performance in a public spaceis similar to that observed in a quiet room, indi-cating that, at least under the conditions we cre-ated, usable performance can be achieved.
Thisis a useful finding.
We believe that on this level,channel maintenance is a matter of articulating amodel that specifies appropriate behavior in dif-ferent states defined by a small number of dis-crete features (presence, absence, coincidence).We conjecture that such a framework is likely tobe extensible to more complex situations, for ex-ample ones involving multiple humans in the en-vironment.1http://trac.speech.cs.cmu.edu/repos/olympus/tags/KinectOly2.0/2http://trac.speech.cs.cmu.edu/repos/olympus/tags/KinectOly2.0/Resources/DecoderConfig/AcousticModels/Semi_Kinect.cd_semi_5000/66References[Al Moubayed and Skantze2011] S. Al Moubayed andG.
Skantze.
2011.
Turn-taking control using gaze inmultiparty human-computer dialogue: Effects of 2d and3d displays.
In Proceedings of AVSP, Florence, Italy,pages 99?102.
[Bohus and Horvitz2009] D. Bohus and E. Horvitz.
2009.Dialog in the open world: platform and applications.
InProceedings of the 2009 international conference on Mul-timodal interfaces, pages 31?38.
ACM.
[Bohus and Horvitz2010] D. Bohus and E. Horvitz.
2010.
Onthe challenges and opportunities of physically situated dia-log.
In 2010 AAAI Fall Symposium on Dialog with Robots.AAAI.
[Bohus et al.2007] D. Bohus, A. Raux, T.K.
Harris, M. Eske-nazi, and A.I.
Rudnicky.
2007.
Olympus: an open-sourceframework for conversational spoken language interfaceresearch.
In Proceedings of the workshop on bridging thegap: Academic and industrial research in dialog technolo-gies, pages 32?39.
Association for Computational Lin-guistics.
[Boros et al.1996] M. Boros, W. Eckert, F. Gallwitz, G. Gorz,G.
Hanrieder, and H. Niemann.
1996.
Towards under-standing spontaneous speech: Word accuracy vs. conceptaccuracy.
In Spoken Language, 1996.
ICSLP 96.
Pro-ceedings., Fourth International Conference on, volume 2,pages 1009?1012.
IEEE.
[Bruce et al.2002] A. Bruce, I. Nourbakhsh, and R. Simmons.2002.
The role of expressiveness and attention in human-robot interaction.
In Proceedings of 2002 IEEE Interna-tional Conference on Robotics and Automation, volume 4,pages 4138?4142.
IEEE.
[Foster et al.2012] M.E.
Foster, A. Gaschler, M. Giuliani,A.
Isard, M. Pateraki, and R.P.A.
Petrick.
2012.
?twopeople walk into a bar?
: Dynamic multi-party social inter-action with a robot agent.
In Proc.
of the 14th ACM Inter-national Conference on Multimodal Interaction ICMI.
[Fukuda et al.2002] T. Fukuda, J. Taguri, F. Arai,M.
Nakashima, D. Tachibana, and Y. Hasegawa.2002.
Facial expression of robot face for human-robotmutual communication.
In Proceedings of 2002 IEEEInternational Conference on Robotics and Automation,volume 1, pages 46?51.
IEEE.
[Graff et al.2001] D. Graff, K. Walker, and D. Miller.
2001.Switchboard cellular part 1 transcribed audio.
In Linguis-tic Data Consortium, Philadelphia.
[Haasch et al.2004] A. Haasch, S. Hohenner, S. H?uwel,M.
Kleinehagenbrock, S. Lang, I. Toptsis, GA Fink,J.
Fritsch, B. Wrede, and G. Sagerer.
2004.
Biron?thebielefeld robot companion.
In Proc.
Int.
Workshop on Ad-vances in Service Robotics, pages 27?32.
Stuttgart, Ger-many: Fraunhofer IRB Verlag.
[Hieronymus et al.2006] J. Hieronymus, G. Aist, andJ.
Dowding.
2006.
Open microphone speech under-standing: correct discrimination of in domain speech.
InProceedings of 2006 IEEE international conference onacoustics, speech, and signal processing, volume 1.
IEEE.
[Hofmann et al.2013] H. Hofmann, U. Ehrlich, A. Berton,A.
Mahr, R. Math, and C. M?uller.
2013.
Evaluation ofspeech dialog strategies for internet applications in the car.In Proceedings of the SIGDIAL 2013 Conference, pages233?241, Metz, France, August.
Association for Compu-tational Linguistics.
[Horvitz et al.2003] E. Horvitz, C. Kadie, T. Paek, andD.
Hovel.
2003.
Models of attention in computing andcommunication: from principles to application.
In Com-munications of the ACM, volume 46, pages 52?59.
[Hosoya et al.2009] K. Hosoya, T. Ogawa, and T. Kobayashi.2009.
Robot auditory system using head-mounted squaremicrophone array.
In Intelligent Robots and Systems,2009.
IROS 2009.
IEEE/RSJ International Conference on,pages 2736?2741.
IEEE.
[Kollar et al.2012] T. Kollar, A. Vedantham, C. Sobel,C.
Chang, V. Perera, and M. Veloso.
2012.
A multi-modalapproach for natural human-robot interaction.
In Proceed-ings of 2012 International Conference on Social Robots.
[Kun et al.2007] A. Kun, T. Paek, and Z. Medenica.
2007.The effect of speech interface accuracy on driving perfor-mance.
In INTERSPEECH, pages 1326?1329.
[Misu et al.2013] T. Misu, A. Raux, I.
Lane, J. Devassy, andR.
Gupta.
2013.
Situated multi-modal dialog system invehicles.
In Proceedings of the 6th Workshop on Eye Gazein Intelligent Human Machine Interaction: Gaze in Multi-modal Interaction, pages 25?28.
ACM.
[Nakano and Ishii2010] Y. Nakano and R. Ishii.
2010.
Es-timating user?s engagement from eye-gaze behaviors inhuman-agent conversations.
In Proceedings of the 15thinternational conference on Intelligent user interfaces,pages 139?148.
ACM.
[Nakashima et al.2014] Taichi Nakashima, Kazunori Ko-matani, and Satoshi Sato.
2014.
Integration of multiplesound source localization results for speaker identifica-tion in multiparty dialogue system.
In Natural Interactionwith Robots, Knowbots and Smartphones, pages 153?165.Springer New York.
[Sabanovic et al.2006] S. Sabanovic, M.P.
Michalowski, andR.
Simmons.
2006.
Robots in the wild: Observinghuman-robot social interaction outside the lab.
In Ad-vanced Motion Control, 2006.
9th IEEE InternationalWorkshop on, pages 596?601.
IEEE.
[Sharp et al.1997] R.D.
Sharp, E. Bocchieri, C. Castillo,S.
Parthasarathy, C. Rath, M. Riley, and J. Rowland.
1997.The watson speech recognition engine.
In Proceedings of1997 IEEE international conference on acoustics, speech,and signal processing, volume 5, pages 4065?4068.
IEEE.
[Skantze and Al Moubayed2012] G. Skantze andS.
Al Moubayed.
2012.
Iristk: a statechart-basedtoolkit for multi-party face-to-face interaction.
In Proc.of the 14th ACM International Conference on MultimodalInteraction ICMI.
[Stent et al.1999] A. Stent, J. Dowding, J. Gawron, E. Bratt,and R. Moore.
1999.
The commandtalk spoken dialoguesystem.
In Proceedings of the 37th annual meeting of theAssociation for Computational Linguistics on Computa-tional Linguistics, pages 183?190.
ACL.67
