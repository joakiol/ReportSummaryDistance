Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC) @ EACL 2014, pages 21?30,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsA Systematic Study of Semantic Vector Space Model ParametersDouwe KielaUniversity of CambridgeComputer Laboratorydouwe.kiela@cl.cam.ac.ukStephen ClarkUniversity of CambridgeComputer Laboratorysc609@cam.ac.ukAbstractWe present a systematic study of parame-ters used in the construction of semanticvector space models.
Evaluation is car-ried out on a variety of similarity tasks, in-cluding a compositionality dataset, usingseveral source corpora.
In addition to rec-ommendations for optimal parameters, wepresent some novel findings, including asimilarity metric that outperforms the al-ternatives on all tasks considered.1 IntroductionVector space models (VSMs) represent the mean-ings of lexical items as vectors in a ?semanticspace?.
The benefit of VSMs is that they can eas-ily be manipulated using linear algebra, allowinga degree of similarity between vectors to be com-puted.
They rely on the distributional hypothesis(Harris, 1954): the idea that ?words that occur insimilar contexts tend to have similar meanings?
(Turney and Pantel, 2010; Erk, 2012).
The con-struction of a suitable VSM for a particular task ishighly parameterised, and there appears to be littleconsensus over which parameter settings to use.This paper presents a systematic study of thefollowing parameters:?
vector size;?
window size;?
window-based or dependency-based context;?
feature granularity;?
similarity metric;?
weighting scheme;?
stopwords and high frequency cut-off.A representative set of semantic similaritydatasets has been selected from the literature, in-cluding a phrasal similarity dataset for evaluatingcompositionality.
The choice of source corpus islikely to influence the quality of the VSM, and sowe use a selection of source corpora.
Hence thereare two additional ?superparameters?:?
dataset for evaluation;?
source corpus.Previous studies have been limited to investigat-ing only a small number of parameters, and us-ing a limited set of source corpora and tasks forevaluation (Curran and Moens, 2002a; Curran andMoens, 2002b; Curran, 2004; Grefenstette, 1994;Pado and Lapata, 2007; Sahlgren, 2006; Turneyand Pantel, 2010; Schulte im Walde et al., 2013).Rohde et al.
(2006) considered several weightingschemes for a large variety of tasks, while Weedset al.
(2004) did the same for similarity metrics.Stone et al.
(2008) investigated the effectivenessof sub-spacing corpora, where a larger corpus isqueried in order to construct a smaller sub-spacedcorpus (Zelikovitz and Kogan, 2006).
Blacoe andLapata (2012) compare several types of vector rep-resentations for semantic composition tasks.
Themost comprehensive existing studies of VSM pa-rameters ?
encompassing window sizes, featuregranularity, stopwords and dimensionality reduc-tion ?
are by Bullinaria and Levy (2007; 2012)and Lapesa and Evert (2013).Section 2 introduces the various parameters ofvector space model construction.
We then attempt,in Section 3, to answer some of the fundamen-tal questions for building VSMs through a numberof experiments that consider each of the selectedparameters.
In Section 4 we examine how thesefindings relate to the recent development of dis-tributional compositional semantics (Baroni et al.,2013; Clark, 2014), where vectors for words arecombined into vectors for phrases.2 Data and ParametersTwo datasets have dominated the literature withrespect to VSM parameters: WordSim353 (Finkel-stein et al., 2002) and the TOEFL synonym dataset21Dataset Pairings WordsRG 65 48MC 30 39W353 353 437MEN 3000 751TOEFL 80 400M&L10 324 314Table 1: Datasets for evaluation(Landauer and Dumais, 1997).
There is a riskthat semantic similarity studies have been overfit-ting to their idiosyncracies, so in this study weevaluate on a variety of datasets: in addition toWordSim353 (W353) and TOEFL, we also usethe Rubenstein & Goodenough (RG) (1965) andMiller & Charles (MC) (1991) data, as well asa much larger set of similarity ratings: the MENdataset (Bruni et al., 2012).
All these datasets con-sist of human similarity ratings for word pairings,except TOEFL, which consists of multiple choicequestions where the task is to select the correctsynonym for a target word.
In Section 4 we ex-amine our parameters in the context of distribu-tional compositional semantics, using the evalua-tion dataset from Mitchell and Lapata (2010).
Ta-ble 1 gives statistics for the number of words andword pairings in each of the datasets.As well as using a variety of datasets, we alsoconsider three different corpora from which tobuild the vectors, varying in size and domain.These include the BNC (Burnard, 2007) (106word types, 108tokens) and the larger ukWaC(Baroni et al., 2009) (107types, 109tokens).We also include a sub-spaced Wikipedia corpus(Stone et al., 2008): for all words in the eval-uation datasets, we build a subcorpus by query-ing the top 10-ranked Wikipedia documents usingthe words as search terms, resulting in a corpuswith 106word types and 107tokens.
For examin-ing the dependency-based contexts, we include theGoogle Syntactic N-gram corpus (Goldberg andOrwant, 2013), with 107types and 1011tokens.2.1 ParametersWe selected the following set of parameters for in-vestigation, all of which are fundamental to vectorspace model construction1.1Another obvious parameter would be dimensionality re-duction, which we chose not to include because it does notrepresent a fundamental aspect of VSM construction: di-mensionality reduction relies on some original non-reducedmodel, and directly depends on its quality.Vector size Each component of a vector repre-sents a context (or perhaps more accurately a ?con-textual element?, such as second word to the leftof the target word).2The number of componentsvaries hugely in the literature, but a typical valueis in the low thousands.
Here we consider vec-tor sizes ranging from 50,000 to 500,000, to seewhether larger vectors lead to better performance.Context There are two main approaches to mod-elling context: window-based and dependency-based.
For window-based methods, contexts aredetermined by word co-occurrences within a win-dow of a given size, where the window simplyspans a number of words occurring around in-stances of a target word.
For dependency-basedmethods, the contexts are determined by wordco-occurrences in a particular syntactic relationwith a target word (e.g.
target word dog is thesubject of run, where run subj is the context).We consider different window sizes and comparewindow-based and dependency-based methods.Feature granularity Context words, or ?fea-tures?, are often stemmed or lemmatised.
We in-vestigate the effect of stemming and lemmatisa-tion, in particular to see whether the effect varieswith corpus size.
We also consider more fine-grained features in which each context word ispaired with a POS tag or a lexical category fromCCG (Steedman, 2000).Similarity metric A variety of metrics can beused to calculate the similarity between two vec-tors.
We consider the similarity metrics in Table 2.Weighting Weighting schemes increase the im-portance of contexts that are more indicative of themeaning of the target word: the fact that cat co-occurs with purr is much more informative thanits co-occurrence with the.
Table 3 gives defini-tions of the weighting schemes considered.Stopwords, high frequency cut-off Functionwords and stopwords are often considered too un-informative to be suitable context words.
Ignor-ing them not only leads to a reduction in modelsize and computational effort, but also to a moreinformative distributional vector.
Hence we fol-lowed standard practice and did not use stopwordsas context words (using the stoplist in NLTK (Birdet al., 2009)).
The question we investigated is2We will use the term ?feature?
or ?context?
or ?contextword?
to refer to contextual elements.22Measure DefinitionEuclidean11+??ni=1(ui?vi)2Cityblock11+?ni=1|ui?vi|Chebyshev11+maxi|ui?vi|Cosineu?v|u||v|Correlation(u??u)?(v?
?v)|u||v|Dice2?ni=0min(ui,vi)?ni=0ui+viJaccardu?v?ni=0ui+viJaccard2?ni=0min(ui,vi)?ni=0max(ui,vi)Lin?ni=0ui+vi|u|+|v|Tanimotou?v|u|+|v|?u?vJensen-Shannon Div 1?12(D(u||u+v2)+D(v||u+v2))?2 log 2?-skew 1?D(u||?v+(1??
)u)?2 log 2Table 2: Similarity measures between vectors vand u, where viis the ith component of vwhether removing more context words, based ona frequency cut-off, can improve performance.3 ExperimentsThe parameter space is too large to analyse ex-haustively, and so we adopted a strategy for howto navigate through it, selecting certain parame-ters to investigate first, which then get fixed or?clamped?
in the remaining experiments.
Unlessspecified otherwise, vectors are generated with thefollowing restrictions and transformations on fea-tures: stopwords are removed, numbers mappedto ?NUM?, and only strings consisting of alphanu-meric characters are allowed.
In all experiments,the features consist of the frequency-ranked first nwords in the given source corpus.Four of the five similarity datasets (RG, MC,W353, MEN) contain continuous scales of sim-ilarity ratings for word pairs; hence we followstandard practice in using a Spearman correlationcoefficient ?sfor evaluation.
The fifth dataset(TOEFL) is a set of multiple-choice questions,for which an accuracy measure is appropriate.Calculating an aggregate score over all datasetsis non-trivial, since taking the mean of correla-tion scores leads to an under-estimation of per-formance; hence for the aggregate score we usethe Fisher-transformed z-variable of the correla-Scheme DefinitionNone wij= fijTF-IDF wij= log(fij)?
log(Nnj)TF-ICF wij= log(fij)?
log(Nfj)Okapi BM25 wij=fij0.5+1.5?fjfjj+fijlogN?nj+0.5fij+0.5ATC wij=(0.5+0.5?fijmaxf) log(Nnj)?
?Ni=1[(0.5+0.5?fijmaxf) log(Nnj)]2LTU wij=(log(fij)+1.0) log(Nnj)0.8+0.2?fj?jfjMI wij= logP (tij|cj)P (tij)P (cj)PosMI max(0,MI)T-Test wij=P (tij|cj)?P (tij)P (cj)?P (tij)P (cj)?2see (Curran, 2004, p. 83)Lin98a wij=fij?ffi?fjLin98b wij= ?1?
lognjNGref94 wij=log fij+1lognj+1Table 3: Term weighting schemes.
fijdenotes thetarget word frequency in a particular context, fithe total target word frequency, fjthe total contextfrequency, N the total of all frequencies, njthenumber of non-zero contexts.
P (tij|cj) is definedasfijfjand P (tij) asfijN.tion datasets, and take the weighted average ofits inverse over the correlation datasets and theTOEFL accuracy score (Silver and Dunlap, 1987).3.1 Vector sizeThe first parameter we investigate is vector size,measured by the number of features.
Vectors areconstructed from the BNC using a window-basedmethod, with a window size of 5 (2 words eitherside of the target word).
We experiment with vec-tor sizes up to 0.5M features, which is close to thetotal number of context words present in the en-tire BNC according to our preprocessing scheme.Features are added according to frequency in theBNC, with increasingly more rare features beingadded.
For weighting we consider both PositiveMutual Information and T-Test, which have beenfound to work best in previous research (Bullinariaand Levy, 2012; Curran, 2004).
Similarity is com-puted using Cosine.23Figure 1: Impact of vector size on performanceacross different datasetsThe results in Figure 1 show a clear trend: forboth weighting schemes, performance no longerimproves after around 50,000 features; in fact, forT-test weighting, and some of the datasets, perfor-mance initially declines with an increase in fea-tures.
Hence we conclude that continuing to addmore rare features is detrimental to performance,and that 50,000 features or less will give good per-formance.
An added benefit of smaller vectors isthe reduction in computational cost.3.2 Window sizeRecent studies have found that the best windowsize depends on the task at hand.
For example,Hill et al.
(2013) found that smaller windows workbest for measuring similarity of concrete nouns,whereas larger window sizes work better for ab-stract nouns.
Schulte im Walde et al.
(2013) foundthat a large window size worked best for a com-positionality dataset of German noun-noun com-pounds.
Similar relations between window sizeand performance have been found for similar ver-sus related words, as well as for similar versus as-sociated words (Turney and Pantel, 2010).We experiment with window sizes of 3, 5, 7, 9and a full sentence.
(A window size of n impliesn?12words either side of the target word.)
Weuse Positive Mutual Information weighting, Co-sine similarity, and vectors of size 50,000 (basedon the results from Section 3.1).
Figure 2 showsthe results for all the similarity datasets, with theaggregated score at the bottom right.Performance was evaluated on three corpora,in order to answer three questions: Does win-dow size affect performance?
Does corpus sizeinteract with window size?
Does corpus sub-Figure 2: Impact of window size across three cor-poraspacing interact with window size?
Figure 2clearly shows the answer to all three questions is?yes?.
First, ukWaC consistently outperforms theBNC, across all window sizes, indicating that alarger source corpus leads to better performance.Second, we see that the larger ukWaC performsbetter with smaller window sizes compared to theBNC, with the best ukWaC performance typicallybeing found with a window size of only 3.
Forthe BNC, it appears that a larger window is able tooffset the smaller size of corpus to some extent.We also evaluated on a sub-spaced Wikipediasource corpus similar to Stone et al.
(2008), whichperforms much better with larger window sizesthan the BNC or ukWaC.
Our explanation for thisresult is that sub-spacing, resulting from search-ing for Wikipedia pages with the appropriate tar-get terms, provides a focused, less noisy corpus inwhich context words some distance from the targetword are still relevant to its meaning.In summary, the highest score is typicallyachieved with the largest source corpora andsmallest window size, with the exception of themuch smaller sub-spaced Wikipedia corpus.3.3 ContextThe notion of context plays a key role in VSMs.Pado and Lapata (2007) present a comparison ofwindow-based versus dependency-based methodsand conclude that dependency-based contexts givebetter results.
We also compare window-based anddependency-based models.Dependency-parsed versions of the BNC andukWaC were used to construct syntactically-informed vectors, with a single, labelled arc be-24Figure 3: Window versus dependency contextstween the target word and context word.3Sincethis effectively provides a window size of 3, wealso use a window size of 3 for the window-basedmethod (which provided the best results in Sec-tion 3.2 with the ukWaC corpus).
As well asthe ukWaC and BNC source corpora, we use theGoogle syntactic N-gram corpus (Goldberg andOrwant, 2013), which is one of the largest cor-pora to date, and which consists of syntactic n-grams as opposed to window-based n-grams.
Weuse vectors of size 50,000 with Positive Mutual In-formation weighting and Cosine similarity.
Dueto its size and associated computational cost, weused only 10,000 contexts for the vectors gener-ated from the syntactic N-gram corpus.
The re-sults are shown in Figure 3.In contrast to the idea that dependency-basedmethods outperform window-based methods, wefind that the window-based models outperformdependency-based models when they are con-structed from the same corpus using the smallwindow size.
However, Google?s syntactic N-gram corpus does indeed outperform window-based methods, even though smaller vectors wereused for the Google models (10,000 vs. 50,000features).
We observe large variations acrossdatasets, with window-based methods performingparticularly well on some, but not all.
In partic-ular, window-based methods clearly outperformdependency-based methods on the RG dataset (forthe same source corpus), whereas the oppositetrend is observed for the TOEFL synonym dataset.The summary is that the model built from the syn-tactic N-grams is the overall winner, but when we3The Clark and Curran (2007) parser was used to providethe dependencies.compare both methods on the same corpus, thewindow-based method on a large corpus appearsto work best (given the small window size).3.4 Feature granularityStemming and lemmatisation are standard tech-niques in NLP and IR to reduce data sparsity.However, with large enough corpora it may bethat the loss of information through generalisa-tion hurts performance.
In fact, it may be that in-creased granularity ?
through the use of grammat-ical tags ?
can lead to improved performance.
Wetest these hypotheses by comparing four types ofprocessed context words: lemmatised, stemmed,POS-tagged, and tagged with CCG lexical cate-gories (which can be thought of as fine-grainedPOS tags (Clark and Curran, 2007)).4The sourcecorpora are BNC and ukWaC, using a window-based method with windows of size 5, PositiveMutual Information weighting, vectors of size50,000 and Cosine similarity.
The results are re-ported in Figure 4.The ukWaC-generated vectors outperform theBNC-generated ones on all but a single instancefor each of the granularities.
Stemming yieldsthe best overall performance, and increasing thegranularity does not lead to better results.
Evenwith a very large corpus like ukWaC, stemmingyields signficantly better results than not reduc-ing the feature granularity at all.
Conversely, apartfrom the results on the TOEFL synonym dataset,increasing the feature granularity of contexts byincluding POS tags or CCG categories does notyield any improvement.3.5 Similarity-weighting combinationThere is contrasting evidence in the literature re-garding which combination of similarity metricand weighting scheme works best.
Here we inves-tigate this question using vectors of size 50,000,no processing of the context features (i.e., ?nor-mal?
feature granularity), and a window-basedmethod with a window size of 5.
Aggregatedscores across the datasets are reported in Tables4 and 5 for the BNC and ukWaC, respectively.There are some clear messages to be taken fromthese large tables of results.
First, two weightingschemes perform better than the others: PositiveMutual Information (PosMI) and T-Test.
On theBNC, the former yields the best results.
There are4Using NLTK?s Porter stemmer and WordNet lemmatiser.25Figure 4: Feature granularity: stemmed (S), lem-matised (L), normal (N), POS-tagged (T) andCCG-tagged (C)RG MC W353 MEN TOEFLP+COS 0.74 0.64 0.50 0.66 0.76P+COR 0.74 0.65 0.58 0.71 0.83T+COS 0.78 0.69 0.54 0.68 0.78T+COR 0.78 0.71 0.54 0.68 0.78Table 6: Similarity scores on individual datasetsfor positive mutual information (P) and T-test(T) weighting, with cosine (COS) and correlation(COR) similaritythree similarity metrics that perform particularlywell: Cosine, Correlation and the Tanimoto coef-ficient (the latter also being similar to Cosine; seeTable 2).
The Correlation similarity metric has themost consistent performance across the differentweighting schemes, and yields the highest scorefor both corpora.
The most consistent weightingscheme across the two source corpora and similar-ity metrics appears to be PosMI.The highest combined aggregate score is that ofPosMI with the Correlation metric, in line withthe conclusion of Bullinaria and Levy (2012) thatPosMI is the best weighting scheme5.
However,for the large ukWaC corpus, T-Test achieves sim-ilarly high aggregate scores, in line with the workof Curran (2004).
When we look at these twoweighting schemes in more detail, we see that T-Test works best for the RG and MC datasets, whilePosMI works best for the others; see Table 6.
Cor-relation is the best similarity metric in all cases.5In some cases, the combination of weighting scheme andsimilarity metric results in a division by zero or leads to tak-ing the logarithm of a negative number, in which cases wereport the aggregate scores as nan (not-a-number).Figure 5: Finding the optimal ?contiguous subvec-tor?
of size 10,0003.6 Optimal subvectorStopwords are typically removed from vectors andnot used as features.
However, Bullinaria andLevy (2012) find that removing stopwords has noeffect on performance.
A possible explanationis that, since they are using a weighting scheme,the weights of stopwords are low enough thatthey have effectively been removed anyhow.
Thisraises the question: are we removing stopwordsbecause they contribute little towards the meaningof the target word, or are we removing them be-cause they have high frequency?The experiment used ukWaC, with a window-based method and window size of 5, normal fea-ture granularity, Cosine similarity and a slidingvector of size 10,000.
Having a sliding vector im-plies that we throw away up to the first 40,000 con-texts as we slide across to the 50,000 mark (replac-ing the higher frequency contexts with lower fre-quency ones).
In effect, we are trying to find thecut-off point where the 10,000-component ?con-tiguous subvector?
of the target word vector isoptimal (where the features are ordered by fre-quency).
Results are given for PosMI, T-Test andno weighting at all.The results are shown in Figure 5.
T-test outper-forms PosMI at the higher frequency ranges (to theleft of the plots) but PosMI gives better results forsome of the datasets further to the right.
For bothweighting schemes the performance decreases ashigh frequency contexts are replaced with lowerfrequency contexts.A different picture emerges when no weight-ing is used, however.
Here the performance canincrease as high-frequency contexts are replaced26British National CorpusCOS COR DIC JC1 JC2 TAN LIN EUC CIB CHS JSD ASKnone 0.49 0.50 0.34 0.35 0.27 0.22 0.30 0.09 0.11 0.08 0.45 0.36tfidf 0.43 0.44 0.33 0.34 0.22 0.16 0.27 0.13 0.12 0.16 0.38 0.32tficf 0.47 0.48 0.34 0.36 0.23 0.16 0.27 0.13 0.12 0.15 0.40 0.33okapi 0.40 0.42 0.37 0.42 0.22 0.23 0.26 0.25 0.15 0.14 0.37 0.26atc 0.40 0.43 0.25 0.24 0.16 0.34 0.30 0.10 0.13 0.08 0.33 0.23ltu 0.44 0.45 0.35 0.36 0.22 0.23 0.26 0.22 0.13 0.21 0.37 0.27mi 0.58 0.61 0.31 0.56 0.29 -0.07 0.45 0.15 0.10 0.09 0.16 -0.04posmi 0.63 0.66 0.52 0.58 0.35 -0.08 0.45 0.15 0.11 0.06 0.54 0.46ttest 0.63 0.62 0.11 0.34 0.08 0.63 0.17 0.18 0.14 0.11 nan nanchisquared 0.50 0.50 0.46 0.42 0.42 0.42 nan 0.06 0.07 0.08 0.57 0.52lin98b 0.47 0.52 0.35 0.40 0.21 -0.10 0.29 0.10 0.11 nan 0.38 0.29gref94 0.46 0.49 0.35 0.37 0.23 0.06 0.28 0.12 0.11 0.09 0.41 0.30Table 4: Aggregated scores for combinations of weighting schemes and similarity metrics using the BNC.The similarity metrics are Cosine (COS), Correlation (COR), Dice (DIC), Jaccard (JC1), Jaccard2 (JC2),Tanimoto (TAN), Lin (LIN), Euclidean (EUC), CityBlock (CIB), Chebyshev (CHS), Jensen-ShannonDivergence (JSD) and ?-skew (ASK)ukWaCCOS COR DIC JC1 JC2 TAN LIN EUC CIB CHS JSD ASKnone 0.55 0.55 0.28 0.35 0.24 0.41 0.31 0.06 0.09 0.08 0.56 0.49tfidf 0.45 0.47 0.26 0.30 0.20 0.28 0.22 0.14 0.12 0.16 0.37 0.27tficf 0.45 0.49 0.27 0.33 0.20 0.29 0.24 0.13 0.11 0.09 0.37 0.28okapi 0.37 0.42 0.33 0.37 0.18 0.27 0.26 0.26 0.17 0.12 0.34 0.20atc 0.34 0.42 0.13 0.13 0.08 0.15 0.28 0.10 0.09 0.07 0.28 0.15ltu 0.43 0.48 0.30 0.34 0.19 0.26 0.25 0.26 0.16 0.24 0.36 0.23mi 0.51 0.53 0.18 0.51 0.16 0.28 0.37 0.18 0.10 0.09 0.12 nanposmi 0.67 0.70 0.56 0.62 0.42 0.59 0.52 0.23 0.15 0.06 0.60 0.49ttest 0.70 0.70 0.16 0.48 0.10 0.70 0.22 0.16 0.11 0.15 nan nanchisquared 0.57 0.58 0.52 0.56 0.44 0.52 nan 0.08 0.06 0.10 0.63 0.60lin98b 0.43 0.63 0.31 0.37 0.20 0.23 0.26 0.09 0.10 nan 0.34 0.24gref94 0.48 0.54 0.27 0.33 0.20 0.17 0.23 0.13 0.11 0.09 0.38 0.25Table 5: Aggregated scores for combinations of weighting schemes and similarity metrics using ukWaCwith lower-frequency ones, with optimal perfor-mance comparable to when weighting is used.There are some scenarios where it may be ad-vantageous not to use weighting, for example inan online setting where the total set of vectors isnot fixed; in situations where use of a dimension-ality reduction technique does not directly allowfor weighting, such as random indexing (Sahlgren,2006); as well as in settings where calculatingweights is too expensive.
Where to stop the slid-ing window varies with the datasets, however, andso our conclusion is that the default scheme shouldbe weighting plus high frequency contexts.4 CompositionalityIn order to examine whether optimal parame-ters carry over to vectors that are combined intophrasal vectors using a composition operator, weperform a subset of our experiments on the canoni-cal compositionality dataset from Mitchell and La-pata (2010), using vector addition and pointwisemultiplication (the best performing operators inthe original study).We evaluate using two source corpora (the BNCand ukWaC) and two window sizes (small, witha window size of 3; and big, where the full sen-tence is the window).
In addition to the weight-ing schemes from the previous experiment, we in-clude Mitchell & Lapata?s own weighting scheme,which (in our notation) is defined as wij=fij?Nfi?fj.While all weighting schemes and similarity met-rics were tested, we report only the best perform-ing ones: correlations below 0.5 were ommittedfor the sake of brevity.
Table 7 shows the results.We find that many of our findings continue tohold.
PosMI and T-Test are the best performingweighting schemes, together with Mitchell & La-pata?s own weighting scheme.
We find that ad-dition outperforms multiplication (contrary to theoriginal study) and that small window sizes workbest, except in the VO case.
Performance acrosscorpora is comparable.
The best performing simi-larity metrics are Cosine and Correlation, with thelatter having a slight edge over the former.27BNC - Small windowAN NN VO ALLadd-posmi-cosine 0.57 0.56 0.52 0.55add-posmi-correlation 0.66 0.60 0.53 0.60add-ttest-cosine 0.59 0.54 0.53 0.56add-ttest-correlation 0.60 0.54 0.53 0.56add-mila-correlation 0.64 0.38 0.51 0.51ukWaC - Small windowAN NN VO ALLadd-posmi-correlation 0.64 0.59 0.56 0.59add-ttest-cosine 0.61 0.55 0.53 0.56add-ttest-correlation 0.61 0.55 0.53 0.56add-mila-correlation 0.64 0.48 0.57 0.56mult-mila-correlation 0.52 0.44 0.63 0.53BNC - Large windowAN NN VO ALLadd-posmi-correlation 0.47 0.49 0.57 0.51add-ttest-cosine 0.50 0.53 0.60 0.54add-ttest-correlation 0.50 0.53 0.60 0.54add-mila-correlation 0.51 0.49 0.61 0.54mult-posmi-correlation 0.48 0.48 0.66 0.54mult-mila-correlation 0.53 0.51 0.67 0.57ukWaC - Large windowAN NN VO ALLadd-posmi-correlation 0.46 0.44 0.60 0.50add-ttest-cosine 0.46 0.46 0.59 0.50add-ttest-correlation 0.47 0.46 0.60 0.51add-mila-correlation 0.47 0.46 0.64 0.52mult-posmi-correlation 0.44 0.46 0.65 0.52mult-mila-correlation 0.56 0.49 0.70 0.58Table 7: Selected Spearman ?
scores on theMitchell & Lapata 2010 compositionality dataset5 ConclusionOur experiments were designed to investigate awide range of VSM parameters, using a varietyof evaluation tasks and several source corpora.Across each of the experiments, results are com-petitive with the state of the art.
Some importantmessages can be taken away from this study:Experiment 1 Larger vectors do not always leadto better performance.
As vector size increases,performance stabilises, and a vector size of around50,000 appears to be optimal.Experiment 2 The size of the window has aclear impact on performance: a large corpus witha small window size performs best, but high per-formance can be achieved on a small subspacedcorpus, if the window size is large.Experiment 3 The size of the source corpusis more important than whether the model iswindow- or dependency-based.
Window-basedmethods with a window size of 3 yield better re-sults than dependency-based methods with a win-dow of 3 (i.e.
having a single arc).
The GoogleSyntactic N-gram corpus yields very good perfor-mance, but it is unclear whether this is due to beingdependency-based or being very large.Experiment 4 The granularity of the contextwords has a relatively low impact on performance,but stemming yields the best results.Experiment 5 The optimal combination ofweighting scheme and similarity metric is Posi-tive Mutual Information with a mean-adjusted ver-sion of Cosine that we have called Correlation.Another high-performing weighting scheme is T-Test, which works better for smaller vector sizes.The Correlation similarity metric consistently out-performs Cosine, and we recommend its use.Experiment 6 Use of a weighting scheme ob-viates the need for removing high-frequency fea-tures.
Without weighting, many of the high-frequency features should be removed.
However,if weighting is an option we recommend its use.Compositionality The best parameters forindividual vectors generally carry over to a com-positional similarity task where phrasal similarityis evaluated by combining vectors into phrasalvectors.Furthermore, we observe that in general perfor-mance increases as source corpus size increases,so we recommend using a corpus such as ukWaCover smaller corpora like the BNC.
Likewise,since the MEN dataset is the largest similaritydataset available and mirrors our aggregate scorethe best across the various experiments, we rec-ommend evaluating on that similarity task if onlya single dataset is used for evaluation.Obvious extensions include an analysis of theperformance of the various dimensionality reduc-tion techniques, examining the importance of win-dow size and feature granularity for dependency-based methods, and further exploring the relationbetween the size and frequency distribution of acorpus together with the optimal characteristics(such as the high-frequency cut-off point) of vec-tors generated from that source.AcknowledgmentsThis work has been supported by EPSRC grantEP/I037512/1.
We would like to thank LauraRimell, Tamara Polajnar and Felix Hill for help-ful comments and suggestions.28ReferencesMarco Baroni, Silvia Bernardini, Adriano Ferraresi,and Eros Zanchetta.
2009.
The WaCky Wide Web:A collection of very large linguistically processedWeb-crawled corpora.
Language Resources andEvaluation, 43(3):209?226.Marco Baroni, Raffaella Bernardi, and Roberto Zam-parelli.
2013.
Frege in Space: A program for com-positional distributional semantics.
Linguistic Is-sues in Language Technologies (LiLT).Steven Bird, Ewan Klein, and Edward Loper.2009.
Natural Language Processing with Python.O?Reilly Media.William Blacoe and Mirella Lapata.
2012.
A Com-parison of Vector-based Representations for Seman-tic Composition.
In Proceedings of the 2012 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning, pages 546?556, Jeju Island, Korea,July.
Association for Computational Linguistics.Elia Bruni, Gemma Boleda, Marco Baroni, and N. K.Tran.
2012.
Distributional Semantics in Techni-color.
In Proceedings of the ACL 2012.John A. Bullinaria and Joseph P. Levy.
2007.
Ex-tracting Semantic Representations from Word Co-occurrence Statistics: A computational study.
Be-havior Research Methods, 39:510?526.John A. Bullinaria and Joseph P. Levy.
2012.
Ex-tracting Semantic Representations from Word Co-occurrence Statistics: Stop-lists, Stemming andSVD.
Behavior Research Methods, 44:890?907.L.
Burnard.
2007.
Reference Guidefor the British National Corpus.http://www.natcorp.ox.ac.uk/docs/URG/.Stephen Clark and James R. Curran.
2007.
Wide-Coverage Efficient Statistical Parsing with CCG andLog-Linear Models.
Computational Linguistics,33(4):493?552.Stephen Clark.
2014.
Vector Space Models of LexicalMeaning (to appear).
In Shalom Lappin and ChrisFox, editors, Handbook of Contemporary Semantics.Wiley-Blackwell, Oxford.James R. Curran and Marc Moens.
2002a.
Improve-ments in Automatic Thesaurus Extraction.
In Pro-ceedings of the ACL-02 workshop on Unsupervisedlexical acquisition-Volume 9, pages 59?66.
Associa-tion for Computational Linguistics.James R. Curran and Marc Moens.
2002b.
ScalingContext Space.
In Proceedings of the 40th AnnualMeeting on Association for Computational Linguis-tics, pages 231?238.
Association for ComputationalLinguistics.James R. Curran.
2004.
FromDistributional to Seman-tic Similarity.
Ph.D. thesis, University of Edinburgh.Katrin Erk.
2012.
Vector Space Models of WordMeaning and Phrase Meaning: A Survey.
Languageand Linguistics Compass, 6(10):635?653.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-tan Ruppin.
2002.
Placing Search in Context: TheConcept Revisited.
ACM Transactions on Informa-tion Systems, 20(1):116?131.Yoav Goldberg and Jon Orwant.
2013.
A Datasetof Syntactic-Ngrams over Time from a Very LargeCorpus of English Books.
In Second Joint Con-ference on Lexical and Computational Semantics(*SEM), Volume 1: Proceedings of the Main Con-ference and the Shared Task: Semantic Textual Simi-larity, pages 241?247, Atlanta, Georgia, USA, June.Association for Computational Linguistics.Gregory Grefenstette.
1994.
Explorations in Auto-matic Thesaurus Discovery.
Kluwer Academic Pub-lishers, Norwell, MA, USA.Z.
Harris.
1954.
Distributional Structure.
Word,10(23):146?162.F.
Hill, D. Kiela, and A. Korhonen.
2013.
Con-creteness and Corpora: A Theoretical and PracticalAnalysis.
In Proceedings of ACL 2013, Workshopon Cognitive Modelling and Computational Linguis-tics, Sofia, Bulgaria.Thomas K. Landauer and Susan T. Dumais.
1997.A solution to Platos problem: The latent semanticanalysis theory of acquisition, induction, and rep-resentation of knowledge.
Psychological Review,104(2):211?240.Gabriella Lapesa and Stefan Evert.
2013.
Evaluat-ing neighbor rank and distance measures as predic-tors of semantic priming.
In In Proceedings of theACL Workshop on Cognitive Modeling and Compu-tational Linguistics (CMCL 2013), Sofia, Bulgaria.G.A.
Miller and W.G.
Charles.
1991.
Contextual Cor-relates of Semantic Similarity.
Language and Cog-nitive Processes, 6(1):1?28.Jeff Mitchell and Mirella Lapata.
2010.
Compositionin Distributional Models of Semantics.
CognitiveScience, 34(8):1388?1429.Sebastian Pado and Mirella Lapata.
2007.Dependency-based Construction of SemanticSpace Models.
Computational Linguistics,33(2):161?199.Douglas L. T. Rohde, Laura M. Gonnerman, andDavid C. Plaut.
2006.
An Improved Model of Se-mantic Similarity based on Lexical Co-occurence.Communciations of the ACM, 8:627?633.Herbert Rubenstein and John B. Goodenough.
1965.Contextual Correlates of Synonymy.
Commun.ACM, 8(10):627?633, October.29Magnus Sahlgren.
2006.
The Word-Space Model: Us-ing distributional analysis to represent syntagmaticand paradigmatic relations between words in high-dimensional vector spaces.
Ph.D. thesis, Depart-ment of Linguistics, Stockholm University.Sabine Schulte im Walde, Stefan M?uller, and StephenRoller.
2013.
Exploring Vector Space Models toPredict the Compositionality of German Noun-NounCompounds.
In Proceedings of the 2nd Joint Con-ference on Lexical and Computational Semantics,pages 255?265, Atlanta, GA.N.
Clayton Silver and William P. Dunlap.
1987.
Av-eraging Correlation Coefficients: Should Fisher?s zTransformation Be Used?
Journal of Applied Psy-chology, 72(1):146?148, February.Mark Steedman.
2000.
The Syntactic Process.
MITPress, Cambridge, MA, USA.Benjamin P. Stone, Simon J. Dennis, and Peter J.Kwantes.
2008.
A Systematic Comparison of Se-mantic Models on Human Similarity Rating Data:The Effectiveness of Subspacing.
In The Proceed-ings of the Thirtieth Conference of the Cognitive Sci-ence Society.Peter D. Turney and Patrick Pantel.
2010.
From Fre-quency to Meaning: vector space models of seman-tics.
J. Artif.
Int.
Res., 37(1):141?188, January.Julie Weeds, David Weir, and Diana McCarthy.
2004.Characterising Measures of Lexical DistributionalSimilarity.
In Proceedings of Coling 2004, pages1015?1021, Geneva, Switzerland, Aug 23?Aug 27.COLING.S.
Zelikovitz and M. Kogan.
2006.
Using WebSearches on Important Words to create BackgroundSets for LSI Classification.
In In Proceedings ofthe 19th International FLAIRS Conference, pages598?603, Menlo Park, CA.
AAAI Press.30
