Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 536?543,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsSemantic Class Induction and Coreference ResolutionVincent NgHuman Language Technology Research InstituteUniversity of Texas at DallasRichardson, TX 75083-0688vince@hlt.utdallas.eduAbstractThis paper examines whether a learning-based coreference resolver can be improvedusing semantic class knowledge that is au-tomatically acquired from a version of thePenn Treebank in which the noun phrasesare labeled with their semantic classes.
Ex-periments on the ACE test data show that aresolver that employs such induced semanticclass knowledge yields a statistically signif-icant improvement of 2% in F-measure overone that exploits heuristically computed se-mantic class knowledge.
In addition, the in-duced knowledge improves the accuracy ofcommon noun resolution by 2-6%.1 IntroductionIn the past decade, knowledge-lean approaches havesignificantly influenced research in noun phrase(NP) coreference resolution ?
the problem of deter-mining which NPs refer to the same real-world en-tity in a document.
In knowledge-lean approaches,coreference resolvers employ only morpho-syntacticcues as knowledge sources in the resolution process(e.g., Mitkov (1998), Tetreault (2001)).
While theseapproaches have been reasonably successful (seeMitkov (2002)), Kehler et al (2004) speculate thatdeeper linguistic knowledge needs to be made avail-able to resolvers in order to reach the next level ofperformance.
In fact, semantics plays a crucially im-portant role in the resolution of common NPs, allow-ing us to identify the coreference relation betweentwo lexically dissimilar common nouns (e.g., talksand negotiations) and to eliminate George W. Bushfrom the list of candidate antecedents of the city, forinstance.
As a result, researchers have re-adoptedthe once-popular knowledge-rich approach, investi-gating a variety of semantic knowledge sources forcommon noun resolution, such as the semantic rela-tions between two NPs (e.g., Ji et al (2005)), theirsemantic similarity as computed using WordNet(e.g., Poesio et al (2004)) or Wikipedia (Ponzettoand Strube, 2006), and the contextual role played byan NP (see Bean and Riloff (2004)).Another type of semantic knowledge that hasbeen employed by coreference resolvers is the se-mantic class (SC) of an NP, which can be used to dis-allow coreference between semantically incompat-ible NPs.
However, learning-based resolvers havenot been able to benefit from having an SC agree-ment feature, presumably because the method usedto compute the SC of an NP is too simplistic: whilethe SC of a proper name is computed fairly accu-rately using a named entity (NE) recognizer, manyresolvers simply assign to a common noun the first(i.e., most frequent) WordNet sense as its SC (e.g.,Soon et al (2001), Markert and Nissim (2005)).
Itis not easy to measure the accuracy of this heuristic,but the fact that the SC agreement feature is not usedby Soon et al?s decision tree coreference classifierseems to suggest that the SC values of the NPs arenot computed accurately by this first-sense heuristic.Motivated in part by this observation, we exam-ine whether automatically induced semantic classknowledge can improve the performance of alearning-based coreference resolver, reporting eval-uation results on the commonly-used ACE corefer-536ence corpus.
Our investigation proceeds as follows.Train a classifier for labeling the SC of an NP.In ACE, we are primarily concerned with classify-ing an NP as belonging to one of the ACE seman-tic classes.
For instance, part of the ACE Phase 2evaluation involves classifying an NP as PERSON,ORGANIZATION, GPE (a geographical-political re-gion), FACILITY, LOCATION, or OTHERS.
We adopta corpus-based approach to SC determination, re-casting the problem as a six-class classification task.Derive two knowledge sources for coreferenceresolution from the induced SCs.
The firstknowledge source (KS) is semantic class agreement(SCA).
Following Soon et al (2001), we representSCA as a binary value that indicates whether the in-duced SCs of the two NPs involved are the same ornot.
The second KS is mention, which is representedas a binary value that indicates whether an NP be-longs to one of the five ACE SCs mentioned above.Hence, the mention value of an NP can be readilyderived from its induced SC: the value is NO if itsSC is OTHERS, and YES otherwise.
This KS couldbe useful for ACE coreference, since ACE is con-cerned with resolving only NPs that are mentions.Incorporate the two knowledge sources in acoreference resolver.
Next, we investigate whetherthese two KSs can improve a learning-based base-line resolver that employs a fairly standard featureset.
Since (1) the two KSs can each be repre-sented in the resolver as a constraint (for filteringnon-mentions or disallowing coreference betweensemantically incompatible NPs) or as a feature, and(2) they can be applied to the resolver in isolation orin combination, we have eight ways of incorporatingthese KSs into the baseline resolver.In our experiments on the ACE Phase 2 coref-erence corpus, we found that (1) our SC induc-tion method yields a significant improvement of 2%in accuracy over Soon et al?s first-sense heuristicmethod as described above; (2) the coreference re-solver that incorporates our induced SC knowledgeby means of the two KSs mentioned above yieldsa significant improvement of 2% in F-measure overthe resolver that exploits the SC knowledge com-puted by Soon et al?s method; (3) the mention KS,when used in the baseline resolver as a constraint,improves the resolver by approximately 5-7% in F-measure; and (4) SCA, when employed as a featureby the baseline resolver, improves the accuracy ofcommon noun resolution by about 5-8%.2 Related WorkMention detection.
Many ACE participants havealso adopted a corpus-based approach to SC deter-mination that is investigated as part of the mentiondetection (MD) task (e.g., Florian et al (2006)).Briefly, the goal of MD is to identify the boundaryof a mention, its mention type (e.g., pronoun, name),and its semantic type (e.g., person, location).
Un-like them, (1) we do not perform the full MD task,as our goal is to investigate the role of SC knowl-edge in coreference resolution; and (2) we do notuse the ACE training data for acquiring our SC clas-sifier; instead, we use the BBN Entity Type Corpus(Weischedel and Brunstein, 2005), which consists ofall the Penn Treebank Wall Street Journal articleswith the ACE mentions manually identified and an-notated with their SCs.
This provides us with a train-ing set that is approximately five times bigger thanthat of ACE.
More importantly, the ACE participantsdo not evaluate the role of induced SC knowledgein coreference resolution: many of them evaluatecoreference performance on perfect mentions (e.g.,Luo et al (2004)); and for those that do report per-formance on automatically extracted mentions, theydo not explain whether or how the induced SC infor-mation is used in their coreference algorithms.Joint probabilistic models of coreference.
Re-cently, there has been a surge of interest in im-proving coreference resolution by jointly modelingcoreference with a related task such as MD (e.g.,Daume?
and Marcu (2005)).
However, joint modelstypically need to be trained on data that is simulta-neously annotated with information required by allof the underlying models.
For instance, Daume?
andMarcu?s model assumes as input a corpus annotatedwith both MD and coreference information.
On theother hand, we tackle coreference and SC inductionseparately (rather than jointly), since we train our SCdetermination model on the BBN Entity Type Cor-pus, where coreference information is absent.3 Semantic Class InductionThis section describes how we train and evaluate aclassifier for determining the SC of an NP.5373.1 Training the ClassifierTraining corpus.
As mentioned before, we usethe BBN Entity Type Corpus for training the SCclassifier.
This corpus was originally developed tosupport the ACE and AQUAINT programs and con-sists of annotations of 12 named entity types andnine nominal entity types.
Nevertheless, we willonly make use of the annotations of the five ACEsemantic types that are present in our ACE Phase 2coreference corpus, namely, PERSON, ORGANIZA-TION, GPE, FACILITY, and LOCATION.Training instance creation.
We create one train-ing instance for each proper or common NP (ex-tracted using an NP chunker and an NE recognizer)in each training text.
Each instance is representedby a set of lexical, syntactic, and semantic features,as described below.
If the NP under consideration isannotated as one of the five ACE SCs in the corpus,then the classification of the associated training in-stance is simply the ACE SC value of the NP.
Other-wise, the instance is labeled as OTHERS.
This resultsin 310063 instances in the training set.Features.
We represent the training instance for anoun phrase, NPi, using seven types of features:(1) WORD: For each word w in NPi, we create aWORD feature whose value is equal to w. No fea-tures are created from stopwords, however.
(2) SUBJ VERB: If NPi is involved in a subject-verb relation, we create a SUBJ VERB feature whosevalue is the verb participating in the relation.
Weuse Lin?s (1998b) MINIPAR dependency parser toextract grammatical relations.
Our motivation hereis to coarsely model subcategorization.
(3) VERB OBJ: A VERB OBJ feature is created ina similar fashion as SUBJ VERB if NPi participatesin a verb-object relation.
Again, this represents ourattempt to coarsely model subcategorization.
(4) NE: We use BBN?s IdentiFinder (Bikel et al,1999), a MUC-style NE recognizer to determine theNE type of NPi.
If NPi is determined to be a PERSONor ORGANIZATION, we create an NE feature whosevalue is simply its MUC NE type.
However, if NPiis determined to be a LOCATION, we create a featurewith value GPE (because most of the MUC LOCA-TION NEs are ACE GPE NEs).
Otherwise, no NEfeature will be created (because we are not interestedin the other MUC NE types).ACE SC KeywordsPERSON personORGANIZATION social groupFACILITY establishment, construction, building, facil-ity, workplaceGPE country, province, government, town, city,administration, society, island, communityLOCATION dry land, region, landmass, body of water,geographical area, geological formationTable 1: List of keywords used in WordNet searchfor generating WN CLASS features.
(5) WN CLASS: For each keyword w shown in theright column of Table 1, we determine whether thehead noun of NPi is a hyponym of w in WordNet,using only the first WordNet sense of NPi.1 If so,we create a WN CLASS feature with w as its value.These keywords are potentially useful features be-cause some of them are subclasses of the ACE SCsshown in the left column of Table 1, while othersappear to be correlated with these ACE SCs.2(6) INDUCED CLASS: Since the first-sense heuris-tic used in the previous feature may not be accuratein capturing the SC of an NP, we employ a corpus-based method for inducing SCs that is motivated byresearch in lexical semantics (e.g., Hearst (1992)).Given a large, unannotated corpus3 , we use Identi-Finder to label each NE with its NE type and MINI-PAR to extract all the appositive relations.
An ex-ample extraction would be <Eastern Airlines, thecarrier>, where the first entry is a proper noun la-beled with either one of the seven MUC-style NEtypes4 or OTHERS5 and the second entry is a com-mon noun.
We then infer the SC of a commonnoun as follows: (1) we compute the probabilitythat the common noun co-occurs with each of theeight NE types6 based on the extracted appositiverelations, and (2) if the most likely NE type has aco-occurrence probability above a certain threshold(we set it to 0.7), we create a INDUCED CLASS fea-1This is motivated by Lin?s (1998c) observation that a coref-erence resolver that employs only the first WordNet sense per-forms slightly better than one that employs more than one sense.2The keywords are obtained via our experimentation withWordNet and the ACE SCs of the NPs in the ACE training data.3We used (1) the BLLIP corpus (30M words), which con-sists of WSJ articles from 1987 to 1989, and (2) the ReutersCorpus (3.7GB data), which has 806,791 Reuters articles.4Person, organization, location, date, time, money, percent.5This indicates the proper noun is not a MUC NE.6For simplicity, OTHERS is viewed as an NE type here.538ture for NPi whose value is the most likely NE type.
(7) NEIGHBOR: Research in lexical semantics sug-gests that the SC of an NP can be inferred from itsdistributionally similar NPs (see Lin (1998a)).
Mo-tivated by this observation, we create for each ofNPi?s ten most semantically similar NPs a NEIGH-BOR feature whose value is the surface string ofthe NP.
To determine the ten nearest neighbors, weuse the semantic similarity values provided by Lin?sdependency-based thesaurus, which is constructedusing a distributional approach combined with aninformation-theoretic definition of similarity.Learning algorithms.
We experiment with fourlearners commonly employed in language learning:Decision List (DL): We use the DL learner as de-scribed in Collins and Singer (1999), motivated byits success in the related tasks of word sense dis-ambiguation (Yarowsky, 1995) and NE classifica-tion (Collins and Singer, 1999).
We apply add-onesmoothing to smooth the class posteriors.1-Nearest Neighbor (1-NN): We use the 1-NN clas-sifier as implemented in TiMBL (Daelemans et al,2004), employing dot product as the similarity func-tion (which defines similarity as the number of com-mon feature-value pairs between two instances).
Allother parameters are set to their default values.Maximum Entropy (ME): We employ Lin?s MEimplementation7 , using a Gaussian prior for smooth-ing and running the algorithm until convergence.Naive Bayes (NB): We use an in-house implementa-tion of NB, using add-one smoothing to smooth theclass priors and the class-conditional probabilities.In addition, we train an SVM classifier for SCdetermination by combining the output of five clas-sification methods: DL, 1-NN, ME, NB, and Soonet al?s method as described in the introduction,8with the goal of examining whether SC classifica-tion accuracy can be improved by combining theoutput of individual classifiers in a supervised man-ner.
Specifically, we (1) use 80% of the instancesgenerated from the BBN Entity Type Corpus to trainthe four classifiers; (2) apply the four classifiers andSoon et al?s method to independently make predic-7See http://www.cs.ualberta.ca/?lindek/downloads.htm8In our implementation of Soon?s method, we label an in-stance as OTHERS if no NE or WN CLASS feature is generated;otherwise its label is the value of the NE feature or the ACE SCthat has the WN CLASS features as its keywords (see Table 1).PER ORG GPE FAC LOC OTHTraining 19.8 9.6 11.4 1.6 1.2 56.3Test 19.5 9.0 9.6 1.8 1.1 59.0Table 2: Distribution of SCs in the ACE corpus.tions for the remaining 20% of the instances; and (3)train an SVM classifier (using the LIBSVM pack-age (Chang and Lin, 2001)) on these 20% of the in-stances, where each instance, i, is represented by aset of 31 binary features.
More specifically, let Li ={li1, li2, li3, li4, li5} be the set of predictions that weobtained for i in step (2).
To represent i, we generateone feature from each non-empty subset of Li.3.2 Evaluating the ClassifiersFor evaluation, we use the ACE Phase 2 coreferencecorpus, which comprises 422 training texts and 97test texts.
Each text has its mentions annotated withtheir ACE SCs.
We create our test instances fromthe ACE texts in the same way as the training in-stances described in Section 3.1.
Table 2 shows thepercentages of instances corresponding to each SC.Table 3 shows the accuracy of each classifier (seerow 1) for the ACE training set (54641 NPs, with16414 proper NPs and 38227 common NPs) and theACE test set (13444 NPs, with 3713 proper NPs and9731 common NPs), as well as their performance onthe proper NPs (row 2) and the common NPs (row3).
We employ as our baseline system the Soon et almethod (see Footnote 8), whose accuracy is shownunder the Soon column.
As we can see, DL, 1-NN,and SVM show a statistically significant improve-ment over the baseline for both data sets, whereasME and NB perform significantly worse.9 Addi-tional experiments are needed to determine the rea-son for ME and NB?s poor performance.In an attempt to gain additional insight into theperformance contribution of each type of features,we conduct feature ablation experiments using theDL classifier (DL is chosen simply because it is thebest performer on the ACE training set).
Results areshown in Table 4, where each row shows the accu-racy of the DL trained on all types of features exceptfor the one shown in that row (All), as well as accu-racies on the proper NPs (PN) and the common NPs(CN).
For easy reference, the accuracy of the DL9We use Noreen?s (1989) Approximate Randomization testfor significance testing, with p set to .05 unless otherwise stated.539Training Set Test SetSoon DL 1-NN ME NB SVM Soon DL 1-NN ME NB SVM1 Overall 83.1 85.0 84.0 54.5 71.3 84.2 81.1 82.9 83.1 53.0 70.3 83.32 Proper NPs 83.1 84.1 81.0 54.2 65.5 82.2 79.6 82.0 79.8 55.8 64.4 80.43 Common NPs 83.1 85.4 85.2 54.6 73.8 85.1 81.6 83.3 84.3 51.9 72.6 84.4Table 3: SC classification accuracies of different methods for the ACE training set and test set.Training Set Test SetFeature Type PN CN All PN CN AllAll features 84.1 85.4 85.0 82.0 83.3 82.9- WORD 84.2 85.4 85.0 82.0 83.1 82.8- SUBJ VERB 84.1 85.4 85.0 82.0 83.3 82.9- VERB OBJ 84.1 85.4 85.0 82.0 83.3 82.9- NE 72.9 85.3 81.6 74.1 83.2 80.7- WN CLASS 84.1 85.9 85.3 81.9 84.1 83.5- INDUCED C 84.0 85.6 85.1 82.0 83.6 83.2- NEIGHBOR 82.8 84.9 84.3 80.2 82.9 82.1Table 4: Results for feature ablation experiments.Training Set Test SetFeature Type PN CN All PN CN AllWORD 64.0 83.9 77.9 66.5 82.4 78.0SUBJ VERB 24.0 70.2 56.3 28.8 70.5 59.0VERB OBJ 24.0 70.2 56.3 28.8 70.5 59.0NE 81.1 72.1 74.8 78.4 71.4 73.3WN CLASS 25.6 78.8 62.8 30.4 78.9 65.5INDUCED C 25.8 81.1 64.5 30.0 80.3 66.3NEIGHBOR 67.7 85.8 80.4 68.0 84.4 79.8Table 5: Accuracies of single-feature classifiers.classifier trained on all types of features is shownin row 1 of the table.
As we can see, accuracy dropssignificantly with the removal of NE and NEIGHBOR.As expected, removing NE precipitates a large dropin proper NP accuracy; somewhat surprisingly, re-moving NEIGHBOR also causes proper NP accuracyto drop significantly.
To our knowledge, there are noprior results on using distributionally similar neigh-bors as features for supervised SC induction.Note, however, that these results do not implythat the remaining feature types are not useful forSC classification; they simply suggest, for instance,that WORD is not important in the presence of otherfeature types.
To get a better idea of the utility ofeach feature type, we conduct another experiment inwhich we train seven classifiers, each of which em-ploys exactly one type of features.
The accuraciesof these classifiers are shown in Table 5.
As we cansee, NEIGHBOR has the largest contribution.
Thisagain demonstrates the effectiveness of a distribu-tional approach to semantic similarity.
Its superiorperformance to WORD, the second largest contribu-tor, could be attributed to its ability to combat datasparseness.
The NE feature, as expected, is crucialto the classification of proper NPs.4 Application to Coreference ResolutionWe can now derive from the induced SC informa-tion two KSs ?
semantic class agreement and men-tion ?
and incorporate them into our learning-basedcoreference resolver in eight different ways, as de-scribed in the introduction.
This section examineswhether our coreference resolver can benefit fromany of the eight ways of incorporating these KSs.4.1 Experimental SetupAs in SC induction, we use the ACE Phase 2 coref-erence corpus for evaluation purposes, acquiring thecoreference classifiers on the 422 training texts andevaluating their output on the 97 test texts.
We re-port performance in terms of two metrics: (1) the F-measure score as computed by the commonly-usedMUC scorer (Vilain et al, 1995), and (2) the accu-racy on the anaphoric references, computed as thefraction of anaphoric references correctly resolved.Following Ponzetto and Strube (2006), we consideran anaphoric reference, NPi, correctly resolved if NPiand its closest antecedent are in the same corefer-ence chain in the resulting partition.
In all of ourexperiments, we use NPs automatically extracted byan in-house NP chunker and IdentiFinder.4.2 The Baseline Coreference SystemOur baseline coreference system uses the C4.5 deci-sion tree learner (Quinlan, 1993) to acquire a classi-fier on the training texts for determining whether twoNPs are coreferent.
Following previous work (e.g.,Soon et al (2001) and Ponzetto and Strube (2006)),we generate training instances as follows: a positiveinstance is created for each anaphoric NP, NPj , andits closest antecedent, NPi; and a negative instance iscreated for NPj paired with each of the interveningNPs, NPi+1, NPi+2, .
.
., NPj?1.
Each instance is rep-resented by 33 lexical, grammatical, semantic, and540positional features that have been employed by high-performing resolvers such as Ng and Cardie (2002)and Yang et al (2003), as described below.Lexical features.
Nine features allow differenttypes of string matching operations to be performedon the given pair of NPs, NPx and NPy10, including(1) exact string match for pronouns, proper nouns,and non-pronominal NPs (both before and after de-terminers are removed); (2) substring match forproper nouns and non-pronominal NPs; and (3) headnoun match.
In addition, one feature tests whetherall the words that appear in one NP also appear inthe other NP.
Finally, a nationality matching featureis used to match, for instance, British with Britain.Grammatical features.
22 features test the gram-matical properties of one or both of the NPs.
Theseinclude ten features that test whether each of the twoNPs is a pronoun, a definite NP, an indefinite NP, anested NP, and a clausal subject.
A similar set offive features is used to test whether both NPs arepronouns, definite NPs, nested NPs, proper nouns,and clausal subjects.
In addition, five features deter-mine whether the two NPs are compatible with re-spect to gender, number, animacy, and grammaticalrole.
Furthermore, two features test whether the twoNPs are in apposition or participate in a predicatenominal construction (i.e., the IS-A relation).Semantic features.
Motivated by Soon et al(2001), we have a semantic feature that tests whetherone NP is a name alias or acronym of the other.Positional feature.
We have a feature that com-putes the distance between the two NPs in sentences.After training, the decision tree classifier is usedto select an antecedent for each NP in a test text.Following Soon et al (2001), we select as the an-tecedent of each NP, NPj , the closest preceding NPthat is classified as coreferent with NPj .
If no suchNP exists, no antecedent is selected for NPj .Row 1 of Table 6 and Table 7 shows the resultsof the baseline system in terms of F-measure (F)and accuracy in resolving 4599 anaphoric references(All), respectively.
For further analysis, we also re-port the corresponding recall (R) and precision (P)in Table 6, as well as the accuracies of the system inresolving 1769 pronouns (PRO), 1675 proper NPs(PN), and 1155 common NPs (CN) in Table 7.
As10We assume that NPx precedes NPy in the associated text.we can see, the baseline achieves an F-measure of57.0 and a resolution accuracy of 48.4.To get a better sense of how strong our baselineis, we re-implement the Soon et al (2001) corefer-ence resolver.
This simply amounts to replacing the33 features in the baseline resolver with the 12 fea-tures employed by Soon et al?s system.
Results ofour Duplicated Soon et al system are shown in row2 of Tables 6 and 7.
In comparison to our baseline,the Duplicated Soon et al system performs worseaccording to both metrics, and although the drop inF-measure seems moderate, the performance differ-ence is in fact highly significant (p=0.002).114.3 Coreference with Induced SC KnowledgeRecall from the introduction that our investigation ofthe role of induced SC knowledge in learning-basedcoreference resolution proceeds in three steps:Label the SC of each NP in each ACE document.If a noun phrase, NPi, is a proper or common NP,then its SC value is determined using an SC classi-fier that we acquired in Section 3.
On the other hand,if NPi is a pronoun, then we will be conservative andposit its SC value as UNCONSTRAINED (i.e., it is se-mantically compatible with all other NPs).12Derive two KSs from the induced SCs.
Recall thatour first KS, Mention, is defined on an NP; its valueis YES if the induced SC of the NP is not OTHERS,and NO otherwise.
On the other hand, our secondKS, SCA, is defined on a pair of NPs; its value isYES if the two NPs have the same induced SC thatis not OTHERS, and NO otherwise.Incorporate the two KSs into the baseline re-solver.
Recall that there are eight ways of incor-porating these two KSs into our resolver: they caneach be represented as a constraint or as a feature,and they can be applied to the resolver in isolationand in combination.
Constraints are applied dur-ing the antecedent selection step.
Specifically, whenemployed as a constraint, the Mention KS disallowscoreference between two NPs if at least one of themhas a Mention value of NO, whereas the SCA KS dis-allows coreference if the SCA value of the two NPsinvolved is NO.
When encoded as a feature for theresolver, the Mention feature for an NP pair has the11Again, we use Approximate Randomization with p=.05.12The only exception is pronouns whose SC value can be eas-ily determined to be PERSON (e.g., he, him, his, himself).541System Variation R P F R P F R P F R P F1 Baseline system 60.9 53.6 57.0 ?
?
?
?
?
?
?
?
?2 Duplicated Soon et al 56.1 54.4 55.3 ?
?
?
?
?
?
?
?
?Add to the Baseline Soon?s SC Method Decision List SVM Perfect Information3 Mention(C) only 56.9 69.7 62.6 59.5 70.6 64.6 59.5 70.7 64.6 61.2 83.1 70.54 Mention(F) only 60.9 54.0 57.2 61.2 52.9 56.7 60.9 53.6 57.0 62.3 33.7 43.85 SCA(C) only 56.4 70.0 62.5 57.7 71.2 63.7 58.9 70.7 64.3 61.3 86.1 71.66 SCA(F) only 62.0 52.8 57.0 62.5 53.5 57.6 63.0 53.3 57.7 71.1 33.0 45.17 Mention(C) + SCA(C) 56.4 70.0 62.5 57.7 71.2 63.7 58.9 70.8 64.3 61.3 86.1 71.68 Mention(C) + SCA(F) 58.2 66.4 62.0 60.9 66.8 63.7 61.4 66.5 63.8 71.1 76.7 73.89 Mention(F) + SCA(C) 56.4 69.8 62.4 57.7 71.3 63.8 58.9 70.6 64.3 62.7 85.3 72.310 Mention(F) + SCA(F) 62.0 52.7 57.0 62.6 52.8 57.3 63.2 52.6 57.4 71.8 30.3 42.6Table 6: Coreference results obtained via the MUC scoring program for the ACE test set.System Variation PRO PN CN All PRO PN CN All PRO PN CN All1 Baseline system 59.2 54.8 22.5 48.4 ?
?
?
?
?
?
?
?2 Duplicated Soon et al 53.4 45.7 16.9 41.4 ?
?
?
?
?
?
?
?Add to the Baseline Soon?s SC Method Decision List SVM3 Mention(C) only 58.5 51.3 16.5 45.3 59.1 54.1 20.2 47.5 59.1 53.9 20.6 47.54 Mention(F) only 59.2 55.0 22.5 48.5 59.2 56.1 22.4 48.8 59.4 55.2 22.6 48.65 SCA(C) only 58.1 50.1 16.4 44.7 58.1 51.8 17.1 45.5 58.5 52.0 19.6 46.36 SCA(F) only 59.2 54.9 27.8 49.7 60.4 56.7 30.1 51.5 60.8 56.4 29.4 51.37 Mention(C) + SCA(C) 58.1 50.1 16.4 44.7 58.1 51.8 17.1 45.5 58.5 51.9 19.5 46.38 Mention(C) + SCA(F) 58.9 52.0 22.3 47.2 60.2 55.9 28.1 50.6 60.7 55.3 27.4 50.49 Mention(F) + SCA(C) 58.1 50.3 16.3 44.8 58.1 52.4 16.7 45.6 58.6 52.4 19.7 46.610 Mention(F) + SCA(F) 59.2 55.0 27.6 49.7 60.4 56.8 30.1 51.5 60.8 56.5 29.5 51.4Table 7: Resolution accuracies for the ACE test set.value YES if and only if the Mention value for bothNPs is YES, whereas the SCA feature for an NP pairhas its value taken from the SCA KS.Now, we can evaluate the impact of the two KSson the performance of our baseline resolver.
Specifi-cally, rows 3-6 of Tables 6 and 7 show the F-measureand the resolution accuracy, respectively, when ex-actly one of the two KSs is employed by the baselineas either a constraint (C) or a feature (F), and rows7-10 of the two tables show the results when bothKSs are applied to the baseline.
Furthermore, eachrow of Table 6 contains four sets of results, each ofwhich corresponds to a different method for deter-mining the SC value of an NP.
For instance, the firstset is obtained by using Soon et al?s method as de-scribed in Footnote 8 to compute SC values, servingas sort of a baseline for our results using induced SCvalues.
The second and third sets are obtained basedon the SC values computed by the DL and the SVMclassifier, respectively.13 The last set corresponds toan oracle experiment in which the resolver has ac-cess to perfect SC information.
Rows 3-10 of Table13Results using other learners are not shown due to space lim-itations.
DL and SVM are chosen simply because they achievethe highest SC classification accuracies on the ACE training set.7 can be interpreted in a similar manner.From Table 6, we can see that (1) in comparison tothe baseline, F-measure increases significantly in thefive cases where at least one of the KSs is employedas a constraint by the resolver, and such improve-ments stem mainly from significant gains in preci-sion; (2) in these five cases, the resolvers that useSCs induced by DL and SVM achieve significantlyhigher F-measure scores than their counterparts thatrely on Soon?s method for SC determination; and (3)none of the resolvers appears to benefit from SCA in-formation whenever mention is used as a constraint.Moreover, note that even with perfectly computedSC information, the performance of the baseline sys-tem does not improve when neither MD nor SCA isemployed as a constraint.
These results provide fur-ther evidence that the decision tree learner is not ex-ploiting these two semantic KSs in an optimal man-ner, whether they are computed automatically or per-fectly.
Hence, in machine learning for coreferenceresolution, it is important to determine not only whatlinguistic KSs to use, but also how to use them.While the coreference results in Table 6 seem tosuggest that SCA and mention should be employedas constraints, the resolution results in Table 7 sug-542gest that SCA is better encoded as a feature.
Specifi-cally, (1) in comparison to the baseline, the accuracyof common NP resolution improves by about 5-8%when SCA is encoded as a feature; and (2) wheneverSCA is employed as a feature, the overall resolutionaccuracy is significantly higher for resolvers that useSCs induced by DL and SVM than those that rely onSoon?s method for SC determination, with improve-ments in resolution observed on all three NP types.Overall, these results provide suggestive evidencethat both KSs are useful for learning-based corefer-ence resolution.
In particular, mention should be em-ployed as a constraint, whereas SCA should be usedas a feature.
Interestingly, this is consistent with theresults that we obtained when the resolver has accessto perfect SC information (see Table 6), where thehighest F-measure is achieved by employing men-tion as a constraint and SCA as a feature.5 ConclusionsWe have shown that (1) both mention and SCA canbe usefully employed to improve the performanceof a learning-based coreference system, and (2) em-ploying SC knowledge induced in a supervised man-ner enables a resolver to achieve better performancethan employing SC knowledge computed by Soonet al?s simple method.
In addition, we found thatthe MUC scoring program is unable to reveal theusefulness of the SCA KS, which, when encodedas a feature, substantially improves the accuracy ofcommon NP resolution.
This underscores the im-portance of reporting both resolution accuracy andclustering-level accuracy when analyzing the perfor-mance of a coreference resolver.ReferencesD.
Bean and E. Riloff.
2004.
Unsupervised learning of contex-tual role knowledge for coreference resolution.
In Proc.
ofHLT/NAACL, pages 297?304.D.
M. Bikel, R. Schwartz, and R. M. Weischedel.
1999.
Analgorithm that learns what?s in a name.
Machine Learning34(1?3):211?231.C.-C. Chang and C.-J.
Lin, 2001.
LIBSVM: a libraryfor support vector machines.
Software available athttp://www.csie.ntu.edu.tw/?cjlin/libsvm.M.
Collins and Y.
Singer.
1999.
Unsupervised models fornamed entity classification.
In Proc.
of EMNLP/VLC.W.
Daelemans, J. Zavrel, K. van der Sloot, and A. van denBosch.
2004.
TiMBL: Tilburg Memory Based Learner, ver-sion 5.1, Reference Guide.
ILK Technical Report.H.
Daume?
III and D. Marcu.
2005.
A large-scale explorationof effective global features for a joint entity detection andtracking model.
In Proc.
of HLT/EMNLP, pages 97?104.R.
Florian, H. Jing, N. Kambhatla, and I. Zitouni.
2006.
Fac-torizing complex models: A case study in mention detection.In Proc.
of COLING/ACL, pages 473?480.M.
Hearst.
1992.
Automatic acquisition of hyponyms fromlarge text corpora.
In Proc.
of COLING.H.
Ji, D. Westbrook, and R. Grishman.
2005.
Using seman-tic relations to refine coreference decisions.
In Proc.
ofHLT/EMNLP, pages 17?24.A.
Kehler, D. Appelt, L. Taylor, and A. Simma.
2004.
The(non)utility of predicate-argument frequencies for pronouninterpretation.
In Proc.
of NAACL, pages 289?296.D.
Lin.
1998a.
Automatic retrieval and clustering of similarwords.
In Proc.
of COLING/ACL, pages 768?774.D.
Lin.
1998b.
Dependency-based evaluation of MINIPAR.
InProc.
of the LREC Workshop on the Evaluation of ParsingSystems, pages 48?56.D.
Lin.
1998c.
Using collocation statistics in information ex-traction.
In Proc.
of MUC-7.X.
Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S. Roukos.2004.
A mention-synchronous coreference resolution algo-rithm based on the Bell tree.
In Proc.
of the ACL.K.
Markert and M. Nissim.
2005.
Comparing knowledgesources for nominal anaphora resolution.
ComputationalLinguistics, 31(3):367?402.R.
Mitkov.
2002.
Anaphora Resolution.
Longman.R.
Mitkov.
1998.
Robust pronoun resolution with limitedknowledge.
In Proc.
of COLING/ACL, pages 869?875.V.
Ng and C. Cardie.
2002.
Improving machine learning ap-proaches to coreference resolution.
In Proc.
of the ACL.E.
W. Noreen.
1989.
Computer Intensive Methods for TestingHypothesis: An Introduction.
John Wiley & Sons.M.
Poesio, R. Mehta, A. Maroudas, and J. Hitzeman.
2004.Learning to resolve bridging references.
In Proc.
of the ACL.S.
P. Ponzetto and M. Strube.
2006.
Exploiting semantic rolelabeling, WordNet and Wikipedia for coreference resolution.In Proc.
of HLT/NAACL, pages 192?199.J.
R. Quinlan.
1993.
C4.5: Programs for Machine Learning.Morgan Kaufmann, San Mateo, CA.W.
M. Soon, H. T. Ng, and D. Lim.
2001.
A machine learningapproach to coreference resolution of noun phrases.
Compu-tational Linguistics, 27(4):521?544.J.
Tetreault.
2001.
A corpus-based evaluation of centering andpronoun resolution.
Computational Linguistics, 27(4).M.
Vilain, J. Burger, J. Aberdeen, D. Connolly, andL.
Hirschman.
1995.
A model-theoretic coreference scor-ing scheme.
In Proc.
of MUC-6, pages 45?52.R.
Weischedel and A. Brunstein.
2005.
BBN pronoun corefer-ence and entity type corpus.
Linguistica Data Consortium.X.
Yang, G. Zhou, J. Su, and C. L. Tan.
2003.
Coreferenceresolution using competitive learning approach.
In Proc.
ofthe ACL, pages 176?183.D.
Yarowsky.
1995.
Unsupervised word sense disambiguationrivaling supervised methods.
In Proc.
of the ACL.543
