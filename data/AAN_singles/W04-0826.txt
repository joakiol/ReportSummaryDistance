UBBNBC WSD System DescriptionCsomai ANDR?SDepartment of Computer ScienceBabes-Bolyai UniversityCluj-Napoca, Romaniacsomaia@personal.roAbstractThe Na?ve Bayes classification proves tobe a good performing tool in word sensedisambiguation, although it has not yetbeen applied to the Romanian language.The aim of this paper is to present ourWSD system, based on the NBC algorithm,that performed quite well in Senseval 3.1 IntroductionAccording to the literature, the NBC algorithm isvery efficient, in many cases it outperforms moresophisticated methods (Pedersen 1998).
Therefore,this is the approach we used in our research.
Theword sense disambiguating process has three majorsteps, therefore, the application has three maincomponents as follows:Stemming ?
removal of suffixes, and the filter-ing out of the irrelevant information fromthe corpora.
A simple dictionary based ap-proach.Learning ?
the training of the classifier, basedon the sense tagged corpora.
A databasecontaining the number of co-occurrences isbuilt.Disambiguating ?on the basis of the database,the correct sense of a word in a given con-text is estimated.In the followings the previously mentioned threesteps are described in detail.2 StemmingThe preprocessing of the corpora is one of the mostresult-influential steps.
The preprocessing consistsof the removal of suffixes and the elimination ofthe irrelevant data.
The removal of suffixes is per-formed trough a simple dictionary based method.For every wi word the possible wj candidates areselected from the dictionary containing the wordstems.
Then a similarity score is calculated be-tween the word to be stemmed and the candidates,as follows:li, lj is the length of word i, respectively j.scorei=jiilll+2if li ?
lj   andscorej=0, otherwise.The result is the candidate with the highest score ifits score is above a certain threshold, otherwise theword is leaved untouched.In the preprocessing phase we also erase the pro-nouns and prepositions from the examined context.This exclusion was made upon a list of stop words.3 LearningThe training is conducted according to the NBCalgorithm.
First a database is built, with the follow-ing tables:words ?
contains all the words found in the cor-pora.
Its role is to assign a sense id to everyword.wordsenses ?
contains all the tagged words inthe corpora linked with their possible senses.One entry for a given sense and word.Association for Computational Linguisticsfor the Semantic Analysis of Text, Barcelona, Spain, July 2004SENSEVAL-3: Third International Workshop on the Evaluation of Systemsnosenses - number of tagged contexts, with agiven sensenocontexts  - number of tagged contexts of agiven wordoccurrences ?
number of co-occurrences of agiven word with a given senseFigure1: The tables of the databaseThe training of the system is nothing but filling upthe tables of the database.fill NoSensesfill NoContextsfill Wordsensesscan corporacakt=actual entry in corpora (a context)w=actual word in entry (the ambiguous word)sk=actual sense of entryscan caktvj=actual word in entryif vj<>w thenif vj in words thenvi=wordid from words where w=vjelseadd words vjendifif (exists entry in occurrences wherewordid=vi and senseid=sk) thenincrement C(wordid,senseid) in occurrences,where wordid=vi and senseid=skelseadd occurrences(wordid, senseid, 1)endifstep to next wordendscanstep to next entryendscan corporaAs it is obvious, the database is filled up (so thesystem is trained) only upon the training corpusprovided for the Senseval3 Romanian LexicalSample task.4 DisambiguationThe basic assumption of the Na?ve Bayes methodis that the contextual features are not dependent oneach other.
In this particular case, we assume thatthe probability of co-occurrence of a word vi withthe ambiguous word w of sense s is not dependenton other co-occurrences.The goal is to find the correct sense s?
, of  theword w, for a given context.
This s?
sense maxi-mizes the following equation.
)()|(maxarg)()()|(maxarg)|(maxargkkskkskssPscPsPcPscPcsPskkk===?At this point we make the simplifying ?na?ve?
as-sumption:?
?=cvkjkjsvPscP )|()|(The algorithm (T?tar, 2003) for estimating the cor-rect sense of word w according to its c context isthe following:for every sk sense of w doscore(sk)=P(sk)for every vj from context c doscore(sk)= score(sk)*P(vj  | sk)s?= ))((maxarg ks sscorekwhere s?
is the estimated sense, vj is the j-th wordof the context, sk is the k-th possible sense for wordw.P(sk) and P(vj  | sk) are calculated as follows:where  C(w) is the number of contexts for word w,C(vj , sk) is the number of occurrences of word vj in)()C(s)P(s kk wC=)s()s,C(v)s|P(vkkjkj C=contexts tagged with sense sk , and C(sk) is thenumber of contexts tagged with sense skThe values are obtained from the database, as fol-lows:C(w)- from nocontexts,C(vj , sk)- from occurrences,C(sk)- from nosenses.wordsenses is being used to determine the possiblesenses of a given word.5 EvaluationThe described system was evaluated at Senseval 3.The output was not weighted, therefore for everyambiguous word, at most 1 solution (estimatedsense) was provided.
The results achieved, are thefollowings:score correct/attemptedprecision 0.710 2415 correct of3403 attemptedrecall 0.682 2415 correct of3541 in totalattempted 96.10% 3403 attemptedof 3541 in totalFigure2: Fine-grained scorescore correct/attemptedprecision 0.750 2551 correct of3403 attemptedrecall 0.720 2551 correct of3541 in totalattempted 96.10% 3403 attemptedof 3541 in totalFigure2: Coarse-grained scoreA simple test was made, before the Senseval 3evaluation.
The system was trained on 90% of theRomanian Lexical Sample training corpus, andtested on the remaining 10%.
The selection wasrandom, with a uniform distribution.
A coarsegrained score was computed and compared to thebaseline score.
A baseline method consists of  de-termining the most frequent sense for every word(based upon the training corpus) and in the evalua-tion phase always this sense is assigned.UBBNBC  Baselinerecall 0.66 0.56precision 0.69 0.56Figure3: baseline UBBNBC comparisonReferencesTed Pedersen.
1998.
Na?ve Bayes as a SatisficingModel.
Working Notes of the AAAI Spring Sympo-sium on Satisficing Models, Palo Alto, CADoina T?tar.
2003.
Inteligen??
artificial?
- Aplica?ii ?nprelucrarea limbajului natural.
Editura Albastra,Cluj-Napoca, Romania.Manning, C. D., Sch?tze, H. 1999.
Foundations of sta-tistical natural language processing.
MIT Press,Cambridge, Massachusetts.
