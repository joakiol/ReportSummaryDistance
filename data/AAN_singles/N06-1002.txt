Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 9?16,New York, June 2006. c?2006 Association for Computational LinguisticsDo we need phrases?
Challenging the conventional wisdom in StatisticalMachine TranslationChris Quirk and Arul MenezesMicrosoft ResearchOne Microsoft WayRedmond, WA  98052  USA{chrisq,arulm}@microsoft.comAbstractWe begin by exploring theoretical andpractical issues with phrasal SMT, severalof which are addressed by syntax-basedSMT.
Next, to address problems nothandled by syntax, we propose theconcept of a Minimal Translation Unit(MTU) and develop MTU sequencemodels.
Finally we incorporate thesemodels into a syntax-based SMT systemand demonstrate that it improves on thestate of the art translation quality within atheoretically more desirable framework.1.
IntroductionThe last several years have seen phrasal statisticalmachine translation (SMT) systems outperformword-based approaches by a wide margin (Koehn2003).
Unfortunately the use of phrases in SMT isbeset by a number of difficult theoretical andpractical problems, which we attempt tocharacterize below.
Recent research into syntax-based SMT (Quirk and Menezes 2005; Chiang2005) has produced promising results inaddressing some of the problems; researchmotivated by other statistical models has helpedto address others (Banchs et al 2005).
We refineand unify two threads of research in an attempt toaddress all of these problems simultaneously.Such an approach proves both theoretically moredesirable and empirically superior.In brief, Phrasal SMT systems employ phrasepairs automatically extracted from parallelcorpora.
To translate, a source sentence is firstpartitioned into a sequence of phrases I = s1?sI.Each source phrase si is then translated into atarget phrase ti.
Finally the target phrases arepermuted, and the translation is read off in order.Beam search is used to approximate the optimaltranslation.
We refer the reader to Keohn et al(2003) for a detailed description.
Unlessotherwise noted, the following discussion isgenerally applicable to Alignment Templatesystems (Och and Ney 2004) as well.1.1.
Advantages of phrasal SMTNon-compositionalityPhrases capture the translations of idiomatic andother non-compositional fixed phrases as a unit,side-stepping the need to awkwardly reconstructthem word by word.
While many words can betranslated into a single target word, commoneveryday phrases such as the English passwordtranslating as the French mot de passe cannot beeasily subdivided.
Allowing such translations tobe first class entities simplifies translationimplementation and improves translation quality.Local re-orderingPhrases provide memorized re-ordering decisions.As previously noted, translation can beconceptually divided into two steps: first, findinga set of phrase pairs that simultaneously coversthe source side and provides a bag of translatedtarget phrases; and second, picking an order forthose target phrases.
Since phrase pairs consist ofmemorized substrings of the training data, theyare very likely to produce correct local re-orderings.Contextual informationMany phrasal translations may be easilysubdivided into word-for-word translation, forinstance the English phrase the cabbage may betranslated word-for-word as le chou.
However wenote that la is also a perfectly reasonable word-for-word translation of the, yet la chou is not agrammatical French string.
Even when a phraseappears compositional, the incorporation ofcontextual information often improves translation9quality.
Phrases are a straightforward means ofcapturing local context.1.2.
Theoretical problems with phrasal SMTExact substring match; no discontiguityLarge fixed phrase pairs are effective when anexact match can be found, but are uselessotherwise.
The alignment template approach(where phrases are modeled in terms of wordclasses instead of specific words) provides asolution at the expense of truly fixed phrases.Neither phrasal SMT nor alignment templatesallow discontiguous translation pairs.Global re-orderingPhrases do capture local reordering, but provideno global re-ordering strategy, and the number ofpossible orderings to be considered is notlessened significantly.
Given a sentence of nwords, if the average target phrase length is 4words (which is unusually high), then the re-ordering space is reduced from n!
to only (n/4)!
:still impractical for exact search in mostsentences.
Systems must therefore impose somelimits on phrasal reordering, often hard limitsbased on distance as in Koehn et al (2003) orsome linguistically motivated constraint, such asITG (Zens and Ney, 2004).
Since these phrasesare not bound by or even related to syntacticconstituents, linguistic generalizations (such asSVO becoming SOV, or prepositions becomingpostpositions) are not easily incorporated into themovement models.Probability estimationTo estimate the translation probability of a phrasepair, several approaches are used, oftenconcurrently as features in a log-linear model.Conditional probabilities can be estimated bymaximum likelihood estimation.
Yet the phrasesmost likely to contribute important translationaland ordering information?the longest ones?arethe ones most subject to sparse data issues.Alternately, conditional phrasal models can beconstructed from word translation probabilities;this approach is often called lexical weighting(Vogel et al 2003).
This avoids sparse dataissues, but tends to prefer literal translationswhere the word-for-word probabilities are highFurthermore most approaches model phrases asbags of words, and fail to distinguish betweenlocal re-ordering possibilities.Partitioning limitationA phrasal approach partitions the sentence intostrings of words, making several questionableassumptions along the way.
First, the probabilityof the partitioning is never considered.
Longphrases tend to be rare and therefore have sharpprobability distributions.
This adds an inherentbias toward long phrases with questionable MLEprobabilities (e.g.
1/1 or 2/2).
1Second, the translation probability of eachphrase pair is modeled independently.
Such anapproach fails to model any phenomena that reachacross boundaries; only the target language modeland perhaps whole-sentence bag of words modelscross phrase boundaries.
This is especiallyimportant when translating into languages withagreement phenomena.
Often a single phrase doesnot cover all agreeing modifiers of a headword;the uncovered modifiers are biased toward themost common variant rather than the one agreeingwith its head.
Ideally a system would consideroverlapping phrases rather than a singlepartitioning, but this poses a problem forgenerative models: when words are generatedmultiple times by different phrases, they areeffectively penalized.1.3.
Practical problem with phrases: sizeIn addition to the theoretical problems withphrases, there are also practical issues.
Whilephrasal systems achieve diminishing returns due1The Alignment Template approach differs slightly here.Phrasal SMT estimates the probability of a phrase pair as:=')',(),()|(ttscounttscountst?The Alignment Template method incorporates a loosepartitioning probability by instead estimating the probabilityas (in the special case where each word has a unique class):)(),()|(scounttscountstp =Note that these counts could differ significantly.
Picture asource phrase that almost always translates into adiscontiguous phrase (e.g.
English not becoming French ne?
pas), except for the rare occasion where, due to analignment error or odd training data, it translates into acontiguous phrase (e.g.
French ne parle pas).
Then the firstprobability formulation of ne parle pas given not would beunreasonably high.
However, this is a partial fix since itagain suffers from data sparsity problems, especially onlonger templates where systems hope to achieve the bestbenefits from phrases.10to sparse data, one does see a small incrementalbenefit with increasing phrase lengths.
Given thatstoring all of these phrases leads to very largephrase tables, many research systems simply limitthe phrases gathered to those that could possiblyinfluence some test set.
However, this is notfeasible for true production MT systems, since thedata to be translated is unknown.2.
Previous work2.1.
Delayed phrase constructionTo avoid the major practical problem of phrasalSMT?namely large phrase tables, most of whichare not useful to any one sentence?one caninstead construct phrase tables on the fly using anindexed form of the training data (Zhang andVogel 2005; Callison-Burch et al 2005).However, this does not relieve any of thetheoretical problems with phrase-based SMT.2.2.
Syntax-based SMTTwo recent systems have attempted to address thecontiguity limitation and global re-orderingproblem using syntax-based approaches.Hierarchical phrasesRecent work in the use of hierarchical phrases(Chiang 2005) improves the ability to capturelinguistic generalizations, and also removes thelimitation to contiguous phrases.
Hierarchicalphrases differ from standard phrases in oneimportant way: in addition to lexical items, aphrase pair may contain indexed placeholders,where each index must occur exactly once oneach side.
Such a formulation leads to a formallysyntax-based translation approach, wheretranslation is viewed as a parallel parsing problemover a grammar with one non-terminal symbol.This approach significantly outperforms a phrasalSMT baseline in controlled experimentation.Hierarchical phrases do address the need fornon-contiguous phrases and suggest a powerfulordering story in the absence of linguisticinformation, although this reordering informationis bound in a deeply lexicalized form.
Yet they donot address the phrase probability estimationproblem; nor do they provide a means ofmodeling phenomena across phrase boundaries.The practical problems with phrase-basedtranslation systems are further exacerbated, sincethe number of translation rules with up to twonon-adjacent non-terminals in a 1-1 monotonesentence pair of n source and target words isO(n6), as compared to O(n2) phrases.Treelet TranslationAnother means of extending phrase-basedtranslation is to incorporate source languagesyntactic information.
In Quirk and Menezes(2005) we presented an approach to phrasal SMTbased on a parsed dependency tree representationof the source language.
We use a sourcedependency parser and project a targetdependency tree using a word-based alignment,after which we extract tree-based phrases(?treelets?)
and train a tree-based ordering model.We showed that using treelets and a tree-basedordering model results in significantly bettertranslations than a leading phrase-based system(Pharaoh, Koehn 2004), keeping all other modelsidentical.Like the hierarchical phrase approach, treelettranslation succeeds in improving the global re-ordering search and allowing discontiguousphrases, but does not solve the partitioning orestimation problems.
While we found our treeletsystem more resistant to degradation at smallerphrase sizes than the phrase-based system, itnevertheless suffered significantly at very smallphrase sizes.
Thus it is also subject to practicalproblems of size, and again these problems areexacerbated since there are potentially anexponential number of treelets.2.3.
Bilingual n-gram channel modelsTo address on the problems of estimation andpartitioning, one recent approach transformschannel modeling into a standard sequencemodeling problem (Banchs et al 2005).
Considerthe following aligned sentence pair in Figure 1a.In such a well-behaved example, it is natural toconsider the problem in terms of sequencemodels.
Picture a generative process thatproduces a sentence pair in left to right, emitting apair of words in lock step.
Let M = ?
m1, ?, mn ?be a sequence of word pairs mi = ?
s, t ?.
Then onecan generatively model the probability of analigned sentence pair using techniques from n-gram language modeling:11=??=?
?==kiiniikiiimmPmmPMPATSP11111)|()|()(),,(When an alignment is one-to-one andmonotone, this definition is sufficient.
Howeveralignments are seldom purely one-to-one andmonotone in practice; Figure 1b displays commonbehavior such as one-to-many alignments,inserted words, and non-monotone translation.
Toaddress these problems, Banchs et al (2005)suggest defining tuples such that:(1) the tuple sequence is monotone,(2) there are no word alignment links betweentwo distinct tuples,(3) each tuple has a non-NULL source side,which may require that target wordsaligned to NULL are joined with theirfollowing word, and(4) no smaller tuples can be extracted withoutviolating these constraints.Note that M is now a sequence of phrase pairsinstead of word pairs.
With this adjusteddefinition, even Figure 1b can be generated usingthe same process using the following tuples:m1 = ?
the, l?
?m2 = ?
following example, exemple suivant ?m3 = ?
renames, change le nom ?m4 = ?
the, de la ?m5 = ?
table, table ?There are several advantages to such anapproach.
First, it largely avoids the partitioningproblem; instead of segmenting into potentiallylarge phrases, the sentence is segmented intomuch smaller tuples, most often pairs of singlewords.
Furthermore the failure to model apartitioning probability is much more defensiblewhen the partitions are much smaller.
Secondly,n-gram language model probabilities provide arobust means of estimating phrasal translationprobabilities in context that models interactionsbetween all adjacent tuples, obviating the need foroverlapping mappings.These tuple channel models still must addresspractical issues such as model size, though muchwork has been done to shrink language modelswith minimal impact to perplexity (e.g.
Stolcke1998), which these models could immediatelyleverage.
Furthermore, these models do notaddress the contiguity problem or the globalreordering problem.3.
Translation by MTUsIn this paper, we address all four theoreticalproblems using a novel combination of oursyntactically-informed treelet approach (Quirkand Menezes 2005) and a modified version ofbilingual n-gram channel models (Banchs et al2005).
As in our previous work, we first parse thesentence into a dependency tree.
After this initialparse, we use a global search to find a candidatethat maximizes a log-linear model, where thesecandidates consist of a target word sequenceannotated with a dependency structure, a wordalignment, and a treelet decomposition.We begin by exploring minimal translationunits and the models that concern them.3.1.
Minimal Translation UnitsMinimal Translation Units (MTUs) are related tothe tuples of Banchs et al (2005), but differ inseveral important respects.
First, we relieve therestriction that the MTU sequence be monotone.This prevents spurious expansion of MTUs toincorporate adjacent context only to satisfymonotonicity.
In the example, note that theprevious algorithm would extract the tuple?following example, exemple suivant?
even thoughthe translations are mostly independent.
Theirpartitioning is also context dependent: if thesentence did not contain the words following orsuivant, then ?
example, exemple ?
would be asingle MTU.
Secondly we drop the requirementthat no MTU have a NULL source side.
Whilesome insertions can be modeled in terms ofadjacent words, we believe more robust modelscan be obtained if we consider insertions as(a) Monotone aligned sentence pair(b) More common non-monotone aligned sentence pairFigure 1.
Example aligned sentence pairs.12independent units.
In the end our MTUs aredefined quite simply as pairs of source and targetword sets that follow the given constraints:(1) there are no word alignment links betweendistinct MTUs, and(2) no smaller MTUs can be extracted withoutviolating the previous constraint.Since our word alignment algorithm is able toproduce one-to-one, one-to-many, many-to-one,one-to-zero, and zero-to-one translations, theseact as our basic units.
As an example, let usconsider example (1) once again.
Using this newalgorithm, the MTUs would be:m1 = ?
the, l?
?m2 = ?
following, suivant ?m3 = ?
example, exemple ?m4 = ?
renames, change le nom ?m5 = ?
NULL, de ?m6 = ?
the, la ?m7 = ?
table, table ?A finer grained partitioning into MTUs furtherreduces the data sparsity and partitioning issuesassociated with phrases.
Yet it poses issues inmodeling translation: given a sequence of MTUsthat does not have a monotone segmentation, howdo we model the probability of an alignedtranslation pair?
We propose several solutions,and use each in a log-linear combination ofmodels.First, one may walk the MTUs in source order,ignoring insertion MTUs altogether.
Such amodel is completely agnostic of the target wordorder; instead of generating an aligned sourcetarget pair, it generates a source sentence alongwith a bag of target phrases.
This approachexpends a great deal of modeling effort inregenerating the source sentence, which may notbe altogether desirable, though it does conditionon surrounding translations.
Also, it can beevaluated on candidates before orderings areconsidered.
This latter property may be useful intwo-stage decoding strategies where translationsare considered before orderings.Secondly, one may walk the MTUs in targetorder, ignoring deletion MTUs.
Where the source-order MTU channel model expends probabilitymass generating the source sentence, this modelexpends a probability mass generating the targetsentence and therefore may be somewhatredundant with the target language model.Finally, one may walk the MTUs independency tree order.
Let us assume that inaddition to an aligned source-target candidatepair, we have a dependency parse of the sourceside.
Where the past models conditioned onsurface adjacent MTUs, this model conditions ontree adjacent MTUs.
Currently we condition onlyon the ancestor chain, where parent1(m) is theparent MTU of m, parent2(m) is the grandparentof m, and so on:))(|()(),,( 11 mparentmPMPATSP nMm???
?=This model hopes to capture informationcompletely distinct from the other two models,such as translational preferences contingent on thehead, even in the presence of long distancedependencies.
Note that it generates unordereddependency tree pairs.All of these models can be trained from aparallel corpus that has been word aligned and thesource side dependency parsed.
We walk througheach sentence extracting MTUs in source, target,and tree order.
Standard n-gram languagemodeling tools can be used to train MTUlanguage models.3.2.
DecodingWe employ a dependency tree-based beam searchdecoder to search the space of translations.
Firstthe input is parsed into a dependency treeEnglish French English JapaneseTraining Sentences 300,000 500,000Words 4,441,465 5,198,932 7,909,198 9,379,240Vocabulary 63,343 59,290 79,029 95,813Singletons 35,328 29,448 44,111 52,911Development test Sentences 200 200Words 3,045 3,456 3,436 4,095Test Sentences 2,000 2,000Words 30,010 34,725 35,556 3,855OOV rate 5.5% 4.6% 6.9% 6.8%Table 4.1 Data characteristics13structure.
For each input node in the dependencytree, an n-best list of candidates is produced.Candidates consist of a target dependency treealong with a treelet and word alignment.
Thedecoder generally assumes phrasal cohesion:candidates covering a substring (not subsequence)of the input sentence produce a potential substring(not subsequence) of the final translation.
Inaddition to allowing a DP / beam decoder, thisallows us to evaluate string-based models (such asthe target language model and the source andtarget order MTU n-gram models) on partialcandidates.
This decoder is unchanged from ourprevious work: the MTU n-gram models aresimply incorporated as feature functions in thelog-linear combination.
In the experiments sectionthe MTU models are referred to as model set (1).3.3.
Other translation modelsPhrasal channel modelsWe can estimate traditional channel models usingmaximum likelihood or lexical weighting:?
??
????
?
??
?
??
?====)(),(InverseM1)(),(DirectM1)(),(InverseMLE)(),(DirectMLE)|(),,()|(),,()(*,),(),,(,*)(),(),,(Atreelets s tAtreelets t sAtreeletsAtreeletstspATSfstpATSfccATSfccATSf??
?
???
?
??????????
?We use word probability tables p(t | s) and p(s | t)estimated by IBM Model 1 (Brown et al 1993).Such models can be built over phrases if used in aphrasal decoder or over treelets if used in a treeletdecoder.
These models are referred to as set (2).Word-based modelsA target language model using modified Kneser-Ney smoothing captures fluency; a word countfeature offsets the target LM preference forshorter selections; and a treelet/phrase count helpsbias toward translations using fewer phrases.These models are referred to as set (3).|)(|),,(||),,()|(),,(tphrasecounwordcount||11targetLMAtreeletsATSfTATSfttPATSfTiinii=== ?=?
?Syntactic modelsAs in Quirk and Menezes (2005), we include alinguistically-informed order model that predictsthe head-relative position of each nodeindependently, and a tree-based bigram targetlanguage model; these models are referred to asset (4).????==TtTttparenttPATSfATStpositionPATSf))(|(),,(),,|)((),,(treeLMorder4.
Experimental setupWe evaluate the translation quality of the systemusing the BLEU metric (Papineni et al, 02) undera variety of configurations.
As an additionalbaseline, we compare against a phrasal SMTdecoder, Pharaoh (Koehn et al 2003).4.1.
DataTwo language pairs were used for thiscomparison: English to French, and English toJapanese.
The data was selected from technicalsoftware documentation including softwaremanuals and product support articles; Table 4.1presents the major characteristics of this data.4.2.
TrainingWe parsed the source (English) side of thecorpora using NLPWIN, a broad-coverage rule-based parser able to produce syntactic analyses atvarying levels of depth (Heidorn 2002).
For thepurposes of these experiments we used adependency tree output with part-of-speech tagsand unstemmed surface words.
Word alignmentswere produced by GIZA++ (Och and Ney 2003)with a standard training regimen of five iterationsof Model 1, five iterations of the HMM Model,and five iterations of Model 4, in both directions.These alignments were combined heuristically asdescribed in our previous work.We then projected the dependency trees andused the aligned dependency tree pairs to extracttreelet translation pairs, train the order model, andtrain MTU models.
The target language modelswere trained using only the target side of thecorpus.
Finally we trained model weights bymaximizing BLEU (Och 2003) and set decoderoptimization parameters (n-best list size, timeouts14etc) on a development test set of 200 held-outsentences each with a single reference translation.Parameters were individually estimated for eachdistinct configuration.PharaohThe same GIZA++ alignments as above wereused in the Pharaoh decoder (Koehn 2004).
Weused the heuristic combination described in (Ochand Ney 2003) and extracted phrasal translationpairs from this combined alignment as describedin (Koehn et al, 2003).
Aside from MTU modelsand syntactic models (Pharaoh uses its ownordering approach), the same models were used:MLE and lexical weighting channel models,target LM, and phrase and word count.
Modelweights were also trained following Och (2003).5.
ResultsWe begin with a broad brush comparison ofsystems in Table 5.1.
Throughout this section,treelet and phrase sizes are measured in terms ofMTUs, not words.
By default, all systems(including Pharaoh) use treelets or phrases of upto four MTUs, and MTU bigram models.
The firstresults reiterate that the introduction ofdiscontiguous mappings and especially alinguistically motivated order model (model set(4)) can improve translation quality.
Replacingthe standard channel models (model set (2)) withMTU bigram models (model set (1)) does notappear to degrade quality; it even seems to boostquality on EF.
Furthermore, the information in theMTU models appears somewhat orthogonal to thephrasal models; a combination results inimprovements for both language pairs.The experiments in Table 5.2 compare qualityusing different orders of MTU n-gram models.
(Treelets containing up to four MTUs were stillused as the basis for decoding; only the order ofthe MTU n-gram models was adjusted.)
Aunigram model performs surprisingly well.
Thissupports our intuition that atomic handling ofnon-compositional multi-word translations is amajor contribution of phrasal SMT.
Furthermorebigram models increase translation qualitysupporting the claim that local context is anothercontribution.
Models beyond bigrams had littleimpact presumably due to sparsity and smoothing.Table 5.3 explores the impact of using differentphrase/treelet sizes in decoding.
We see thatadding MTU models makes translation moreresilient given smaller phrases.
The poorperformance at size 1 is not particularlysurprising: both systems require insertions to belexically anchored: the only decoding operationallowed is translation of some visible sourcephrase, and insertions have no visible trace.6.
ConclusionsIn this paper we have teased apart the role ofEF EJPhrasal decoder (Pharaoh)Model sets (2),(3) 45.8?2.0 32.9?0.9Treelet decoder, without discontiguous mappingsModel sets (2),(3) 45.1?2.1 33.2?0.9Model sets (2),(3),(4) 48.4?2.0 34.8?0.9Treelet decoder, with discontiguous mappingsModel sets (2),(3) 46.4?2.1 34.3?0.9Model sets (2),(3),(4) 48.7?2.1 34.9?0.9Model sets (1),(3),(4) 49.6?2.1 33.9?0.8Model sets (1)-(4) 50.5?2.1 36.2?0.9Table 5.1.
Broad system comparison.EF EJTreelet decoder, model sets (1),(3),(4)MTU unigram 47.8?2.1 33.2?0.9MTU bigram 49.6?2.1 33.9?0.8MTU trigram 49.9?2.0 34.0?0.9MTU 4-gram 49.6?2.1 34.1?0.9Treelet decoder, model sets (1)-(4)MTU unigram 48.6?2.1 34.3?1.0MTU bigram 50.5?2.1 36.2?0.9MTU trigram 48.9?2.0 36.1?0.9MTU 4-gram 50.4?2.0 36.2?1.0Table 5.2.
Varying MTU n-gram model order.Table 5.3.
Varying phrase / treelet size.Phrasal decodermodel sets (2),(3)Treelet decoder: MTU bigrammodel sets (1),(3),(4)Treelet decoder: MTU bigrammodel sets (1)-(4)Size EF EJ EF EJ EF EJ1 32.6?1.8 20.5?0.7 26.3?1.3 15.4?0.7 29.8?1.4 16.7?0.72 40.4?1.9 29.7?0.7 48.7?2.1 32.4?0.9 47.7?2.1 33.8?0.83 44.3?2.1 30.7?0.9 48.5?2.0 34.6?0.9 48.5?2.0 35.1?0.94 45.8?2.0 32.9?0.9 49.6?2.1 33.9?0.8 50.5?2.1 36.2?0.915phrases and handled each contribution via adistinct model best suited to the task.
Non-compositional translations stay as MTU phrases.Context and robust estimation is provided byMTU-based n-gram models.
Local and globalordering is handled by a tree-based model.The first interesting result is that at normalphrase sizes, augmenting an SMT system withMTU n-gram models improves quality; whereasreplacing the standard phrasal channel models bythe more theoretically sound MTU n-gramchannel models leads to very similarperformance.Even more interesting are the results on smallerphrases.
A system using very small phrases (size2) and MTU bigram models matches (English-French) or at least approaches (English-Japanese)the performance of the baseline system usinglarge phrases (size 4).
While this work does notyet obviate the need for phrases, we consider it apromising step in that direction.An immediate practical benefit is that it allowssystems to use much smaller phrases (and hencesmaller phrase tables) with little or no loss inquality.
This result is particularly important forsyntax-based systems, or any system that allowsdiscontiguous phrases.
Given a fixed length limit,the number of surface phrases extracted from anysentence pair of length n where all words areuniquely aligned is O(n), but the number oftreelets is potentially exponential in the number ofchildren; and the number of rules with two gapsextracted by Chiang (2005) is potentially O(n3).Our results using MTUs suggest that suchsystems can avoid unwieldy, poorly estimatedlong phrases and instead anchor decoding onshorter, more tractable knowledge units such asMTUs, incorporating channel model informationand contextual knowledge with an MTU n-grammodel.Much future work does remain.
Frominspecting the model weights of the best systems,we note that only the source order MTU n-grammodel has a major contribution to the overallscore of a given candidate.
This suggests that thethree distinct models, despite their different walkorders, are somewhat redundant.
We plan toconsider other approaches for conditioning oncontext.
Furthermore phrasal channel models, inspite of the laundry list of problems presentedhere, have a significant impact on translationquality.
We hope to replace them with effectivemodels without the brittleness and sparsity issuesof heavy lexicalization.ReferencesBanchs, Rafael, Josep Crego, Adri?
de Gispert, PatrikLambert, and Jose Mari?o.
2005.
Statistical machinetranslation of Euparl data by using bilingual n-grams.
InProceedings of ACL Workshop on Building and UsingParallel Texts.Brown, Peter, Vincent Della Pietra, Stephen Della Pietra, andRobert Mercer.
1993.
The mathematics of statisticalmachine translation: parameter estimation.
ComputationalLinguistics 19(2): 263-311.Callison-Burch, Chris, Colin Bannard, and Josh Schroeder.2005.
Scaling phrase-based machine translation to largercorpora and longer phrases.
In Proceedings of ACL.Chiang, David.
2005.
A hierarchical phrase-based model forstatistical machine translation.
In Proceedings of ACL.Heidorn, George.
2000.
?Intelligent writing assistance?.
InDale et al Handbook of Natural Language Processing,Marcel Dekker.Koehn, Philipp, Franz Josef Och, and Daniel Marcu.
2003.Statistical phrase based translation.
In Proceedings ofNAACL.Koehn, Philipp.
2004.
Pharaoh: A beam search decoder forphrase-based statistical machine translation models.
InProceedings of AMTA.Och, Franz Josef and Hermann Ney.
2003.
A systematiccomparison of various statistical alignment models.Computational Linguistics, 29(1): 19-51.Och, Franz Josef and Hermann Ney.
2004.
The AlignmentTemplate approach to statistical machine translation,Computational Linguistics, 30(4):417-450.Och, Franz Josef.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of ACL.Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-JingZhu.
2002.
BLEU: a method for automatic evaluation ofmachine translation.
In Proceedings of ACL.Quirk, Chris and Arul Menezes.
2005.
Dependency treetranslation: syntactically-informed phrasal SMT.
InProceedings of ACL.Stolcke, Andreas.
1998.
Entropy-based pruning of backofflanguage models.
In Proceedings of DARPA BroadcastNews Transcription and Understanding.Vogel, Stephan, Ying Zhang, Fei Huang, Alicia Tribble,Ashish Venugopal, Bing Zhao, Alex Waibel.
2003.
TheCMU statistical machine translation system.
InProceedings of MT Summit.Zens, Richard, and Hermann Ney.
2003.
A comparativestudy on reordering constraints in statistical machinetranslation.
In Proceedings of ACL.Zhang, Ying and Stephan Vogel.
2005.
An efficient phrase-to-phrase alignment model for arbitrarily long phrase andlarge corpora.
In Proceedings of EAMT.16
