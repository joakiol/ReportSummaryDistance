Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 109?117,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPKnowing the Unseen: Estimating Vocabulary Size over Unseen SamplesSuma BhatDepartment of ECEUniversity of Illinoisspbhat2@illinois.eduRichard SproatCenter for Spoken Language UnderstandingOregon Health & Science Universityrws@xoba.comAbstractEmpirical studies on corpora involve mak-ing measurements of several quantities forthe purpose of comparing corpora, creat-ing language models or to make general-izations about specific linguistic phenom-ena in a language.
Quantities such as av-erage word length are stable across sam-ple sizes and hence can be reliably esti-mated from large enough samples.
How-ever, quantities such as vocabulary sizechange with sample size.
Thus measure-ments based on a given sample will needto be extrapolated to obtain their estimatesover larger unseen samples.
In this work,we propose a novel nonparametric estima-tor of vocabulary size.
Our main result isto show the statistical consistency of theestimator ?
the first of its kind in the lit-erature.
Finally, we compare our proposalwith the state of the art estimators (bothparametric and nonparametric) on largestandard corpora; apart from showing thefavorable performance of our estimator,we also see that the classical Good-Turingestimator consistently underestimates thevocabulary size.1 IntroductionEmpirical studies on corpora involve making mea-surements of several quantities for the purpose ofcomparing corpora, creating language models orto make generalizations about specific linguisticphenomena in a language.
Quantities such as av-erage word length or average sentence length arestable across sample sizes.
Hence empirical mea-surements from large enough samples tend to bereliable for even larger sample sizes.
On the otherhand, quantities associated with word frequencies,such as the number of hapax legomena or the num-ber of distinct word types changes are strictly sam-ple size dependent.
Given a sample we can ob-tain the seen vocabulary and the seen number ofhapax legomena.
However, for the purpose ofcomparison of corpora of different sizes or lin-guistic phenomena based on samples of differentsizes it is imperative that these quantities be com-pared based on similar sample sizes.
We thus needmethods to extrapolate empirical measurements ofthese quantities to arbitrary sample sizes.Our focus in this study will be estimators ofvocabulary size for samples larger than the sam-ple available.
There is an abundance of estima-tors of population size (in our case, vocabularysize) in existing literature.
Excellent survey arti-cles that summarize the state-of-the-art are avail-able in (Bunge and Fitzpatrick, 1993) and (Gan-dolfi and Sastri, 2004).
Of particular interest tous is the set of estimators that have been shownto model word frequency distributions well.
Thisstudy proposes a nonparametric estimator of vo-cabulary size and evaluates its theoretical and em-pirical performance.
For comparison we considersome state-of-the-art parametric and nonparamet-ric estimators of vocabulary size.The proposed non-parametric estimator for thenumber of unseen elements assumes a regimecharacterizing word frequency distributions.
Thiswork is motivated by a scaling formulation to ad-dress the problem of unlikely events proposed in(Baayen, 2001; Khmaladze, 1987; Khmaladze andChitashvili, 1989; Wagner et al, 2006).
We alsodemonstrate that the estimator is strongly consis-tent under the natural scaling formulation.
Whilecompared with other vocabulary size estimates,we see that our estimator performs at least as wellas some of the state of the art estimators.2 Previous WorkMany estimators of vocabulary size are availablein the literature and a comparison of several non109parametric estimators of population size occurs in(Gandolfi and Sastri, 2004).
While a definite com-parison including parametric estimators is lacking,there is also no known work comparing methodsof extrapolation of vocabulary size.
Baroni andEvert, in (Baroni and Evert, 2005), evaluate theperformance of some estimators in extrapolatingvocabulary size for arbitrary sample sizes but limitthe study to parametric estimators.
Since we con-sider both parametric and nonparametric estima-tors here, we consider this to be the first studycomparing a set of estimators for extrapolating vo-cabulary size.Estimators of vocabulary size that we comparecan be broadly classified into two types:1.
Nonparametric estimators- here word fre-quency information from the given samplealone is used to estimate the vocabulary size.A good survey of the state of the art is avail-able in (Gandolfi and Sastri, 2004).
In thispaper, we compare our proposed estimatorwith the canonical estimators available in(Gandolfi and Sastri, 2004).2.
Parametric estimators- here a probabilisticmodel capturing the relation between ex-pected vocabulary size and sample size is theestimator.
Given a sample of size n, thesample serves to calculate the parameters ofthe model.
The expected vocabulary for agiven sample size is then determined usingthe explicit relation.
The parametric esti-mators considered in this study are (Baayen,2001; Baroni and Evert, 2005),(a) Zipf-Mandelbrot estimator (ZM);(b) finite Zipf-Mandelbrot estimator (fZM).In addition to the above estimators we considera novel non parametric estimator.
It is the nonpara-metric estimator that we propose, taking into ac-count the characteristic feature of word frequencydistributions, to which we will turn next.3 Novel Estimator of Vocabulary sizeWe observe (X1, .
.
.
,Xn), an i.i.d.
sequencedrawn according to a probability distribution Pfrom a large, but finite, vocabulary ?.
Our goalis in estimating the ?essential?
size of the vocabu-lary ?
using only the observations.
In other words,having seen a sample of size n we wish to know,given another sample from the same population,how many unseen elements we would expect tosee.
Our nonparametric estimator for the numberof unseen elements is motivated by the character-istic property of word frequency distributions, theLarge Number of Rare Events (LNRE) (Baayen,2001).
We also demonstrate that the estimator isstrongly consistent under a natural scaling formu-lation described in (Khmaladze, 1987).3.1 A Scaling FormulationOur main interest is in probability distributions Pwith the property that a large number of words inthe vocabulary ?
are unlikely, i.e., the chance anyword appears eventually in an arbitrarily long ob-servation is strictly between 0 and 1.
The authorsin (Baayen, 2001; Khmaladze and Chitashvili,1989; Wagner et al, 2006) propose a natural scal-ing formulation to study this problem; specifically,(Baayen, 2001) has a tutorial-like summary of thetheoretical work in (Khmaladze, 1987; Khmaladzeand Chitashvili, 1989).
In particular, the authorsconsider a sequence of vocabulary sets and prob-ability distributions, indexed by the observationsize n. Specifically, the observation (X1, .
.
.
,Xn)is drawn i.i.d.
from a vocabulary ?n according toprobability Pn.
If the probability of a word, say?
?
?n is p, then the probability that this specificword ?
does not occur in an observation of size nis(1 ?
p)n .For ?
to be an unlikely word, we would like thisprobability for large n to remain strictly between0 and 1.
This implies thatc?n ?
p ?c?n , (1)for some strictly positive constants 0 < c?
< c?
<?.
We will assume throughout this paper that c?and c?
are the same for every word ?
?
?n.
Thisimplies that the vocabulary size is growing lin-early with the observation size:nc?
?
|?n| ?nc?
.This model is called the LNRE zone and its appli-cability in natural language corpora is studied indetail in (Baayen, 2001).3.2 ShadowsConsider the observation string (X1, .
.
.
,Xn) andlet us denote the quantity of interest ?
the number110of word types in the vocabulary ?n that are notobserved ?
by On.
This quantity is random sincethe observation string itself is.
However, we notethat the distribution of On is unaffected if one re-labels the words in ?n.
This motivates studyingof the probabilities assigned by Pn without refer-ence to the labeling of the word; this is done in(Khmaladze and Chitashvili, 1989) via the struc-tural distribution function and in (Wagner et al,2006) via the shadow.
Here we focus on the latterdescription:Definition 1 Let Xn be a random variable on ?nwith distribution Pn.
The shadow of Pn is de-fined to be the distribution of the random variablePn({Xn}).For the finite vocabulary situation we are con-sidering, specifying the shadow is exactly equiv-alent to specifying the unordered components ofPn, viewed as a probability vector.3.3 Scaled Shadows ConvergeWe will follow (Wagner et al, 2006) and sup-pose that the scaled shadows, the distribution ofn ?Pn(Xn), denoted by Qn converge to a distribu-tion Q.
As an example, if Pn is a uniform distribu-tion over a vocabulary of size cn, then n ?
Pn(Xn)equals 1c almost surely for each n (and hence itconverges in distribution).
From this convergenceassumption we can, further, infer the following:1.
Since the probability of each word ?
is lowerand upper bounded as in Equation (1), weknow that the distribution Qn is non-zeroonly in the range [c?, c?].2.
The ?essential?
size of the vocabulary, i.e.,the number of words of ?n on which Pnputs non-zero probability can be evaluated di-rectly from the scaled shadow, scaled by 1n as?
c?c?1y dQn(y).
(2)Using the dominated convergence theorem,we can conclude that the convergence of thescaled shadows guarantees that the size of thevocabulary, scaled by 1/n, converges as well:|?n|n ??
c?c?1y dQ(y).
(3)3.4 Profiles and their LimitsOur goal in this paper is to estimate the size of theunderlying vocabulary, i.e., the expression in (2),?
c?c?ny dQn(y), (4)from the observations (X1, .
.
.
,Xn).
We observethat since the scaled shadow Qn does not de-pend on the labeling of the words in ?n, a suf-ficient statistic to estimate (4) from the observa-tion (X1, .
.
.
,Xn) is the profile of the observation:(?n1 , .
.
.
, ?nn), defined as follows.
?nk is the num-ber of word types that appear exactly k times inthe observation, for k = 1, .
.
.
, n. Observe thatn?k=1k?nk = n,and thatV def=n?k=1?nk (5)is the number of observed words.
Thus, the objectof our interest is,On = |?n| ?
V. (6)3.5 Convergence of Scaled ProfilesOne of the main results of (Wagner et al, 2006) isthat the scaled profiles converge to a deterministicprobability vector under the scaling model intro-duced in Section 3.3.
Specifically, we have fromProposition 1 of (Wagner et al, 2006):n?k=1???
?k?kn ?
?k?1??????
0, almost surely, (7)where?k :=?
c?c?yk exp(?y)k!
dQ(y) k = 0, 1, 2, .
.
.
.
(8)This convergence result suggests a natural estima-tor for On, expressed in Equation (6).3.6 A Consistent Estimator of OnWe start with the limiting expression for scaledprofiles in Equation (7) and come up with a natu-ral estimator for On.
Our development leading tothe estimator is somewhat heuristic and is aimedat motivating the structure of the estimator for thenumber of unseen words, On.
We formally stateand prove its consistency at the end of this section.1113.6.1 A Heuristic DerivationStarting from (7), let us first make the approxima-tion thatk?kn ?
?k?1, k = 1, .
.
.
, n. (9)We now have the formal calculationn?k=1?nkn ?n?k=1?k?1k (10)=n?k=1?
c?c?e?yyk?1k!
dQ(y)??
c?c?e?yy( n?k=1ykk!
)dQ(y) (11)??
c?c?e?yy (ey ?
1) dQ(y) (12)?
|?n|n ??
c?c?e?yy dQ(y).
(13)Here the approximation in Equation (10) followsfrom the approximation in Equation (9), the ap-proximation in Equation (11) involves swappingthe outer discrete summation with integration andis justified formally later in the section, the ap-proximation in Equation (12) follows becausen?k=1ykk!
?
ey ?
1,as n ?
?, and the approximation in Equa-tion (13) is justified from the convergence in Equa-tion (3).
Now, comparing Equation (13) withEquation (6), we arrive at an approximation forour quantity of interest:Onn ??
c?c?e?yy dQ(y).
(14)The geometric series allows us to write1y =1c???
?=0(1 ?
yc?
)?, ?y ?
(0, c?)
.
(15)Approximating this infinite series by a finite sum-mation, we have for all y ?
(c?, c?
),1y ?1c?M?
?=0(1 ?
yc?
)?=(1 ?
yc?)My?
(1 ?
c?c?)Mc?
.
(16)It helps to write the truncated geometric series asa power series in y:1c?M?
?=0(1 ?
yc?
)?= 1c?M??=0??k=0(?k)(?1)k(yc?
)k= 1c?M?k=0( M??=k(?k))(?1)k(yc?
)k=M?k=0(?1)k aMk yk, (17)where we have writtenaMk :=1c?k+1( M?
?=k(?k)).Substituting the finite summation approximationin Equation 16 and its power series expression inEquation (17) into Equation (14) and swapping thediscrete summation with the integral, we can con-tinueOnn ?M?k=0(?1)k aMk?
c?c?e?yyk dQ(y)=M?k=0(?1)k aMk k!?k.
(18)Here, in Equation (18), we used the definition of?k from Equation (8).
From the convergence inEquation (7), we finally arrive at our estimate:On ?M?k=0(?1)k aMk (k + 1)!
?k+1.
(19)3.6.2 ConsistencyOur main result is the demonstration of the consis-tency of the estimator in Equation (19).Theorem 1 For any ?
> 0,limn????
?On ?
?Mk=0 (?1)k aMk (k + 1)!
?k+1??
?n ?
?almost surely, as long asM ?
c?
log2 e + log2 (?c?
)log2 (c??
c?)
?
1 ?
log2 (c?).
(20)112Proof: From Equation (6), we haveOnn =|?n|n ?n?k=1?kn= |?n|n ?n?k=1?k?1k ?n?k=11k(k?kn ?
?k?1).
(21)The first term in the right hand side (RHS) ofEquation (21) converges as seen in Equation (3).The third term in the RHS of Equation (21) con-verges to zero, almost surely, as seen from Equa-tion (7).
The second term in the RHS of Equa-tion (21), on the other hand,n?k=1?k?1k =?
c?c?e?yy( n?k=1ykk!)dQ(y)??
c?c?e?yy (ey ?
1) dQ(y), n ?
?,=?
c?c?1y dQ(y) ??
c?c?e?yy dQ(y).The monotone convergence theorem justifies theconvergence in the second step above.
Thus weconclude thatlimn?
?Onn =?
c?c?e?yy dQ(y) (22)almost surely.
Coming to the estimator, we canwrite it as the sum of two terms:M?k=0(?1)k aMk k!
?k (23)+M?k=0(?1)k aMk k!
((k + 1)?k+1n ?
?k).The second term in Equation (23) above is seen toconverge to zero almost surely as n ?
?, usingEquation (7) and noting that M is a constant notdepending on n. The first term in Equation (23)can be written as, using the definition of ?k fromEquation (8),?
c?c?e?y( M?k=0(?1)k aMk yk)dQ(y).
(24)Combining Equations (22) and (24), we have that,almost surely,limn?
?On ?
?Mk=0 (?1)k aMk (k + 1)!
?k+1n =?
c?c?e?y(1y ?M?k=0(?1)k aMk yk)dQ(y).
(25)Combining Equation (16) with Equation (17), wehave0 < 1y ?M?k=0(?1)k aMk yk ?
(1 ?
c?c?)Mc?
.
(26)The quantity in Equation (25) can now be upperbounded by, using Equation (26),e?c?
(1 ?
c?c?)Mc?
.For M that satisfy Equation (20) this term is lessthan ?.
The proof concludes.3.7 Uniform Consistent EstimationOne of the main issues with actually employingthe estimator for the number of unseen elements(cf.
Equation (19)) is that it involves knowing theparameter c?.
In practice, there is no natural way toobtain any estimate on this parameter c?.
It wouldbe most useful if there were a way to modify theestimator in a way that it does not depend on theunobservable quantity c?.
In this section we see thatsuch a modification is possible, while still retain-ing the main theoretical performance result of con-sistency (cf.
Theorem 1).The first step to see the modification is in ob-serving where the need for c?
arises: it is in writingthe geometric series for the function 1y (cf.
Equa-tions (15) and (16)).
If we could let c?
along withthe number of elements M itself depend on thesample size n, then we could still have the geo-metric series formula.
More precisely, we have1y ?1c?nMn??=0(1?
yc?n)?= 1y(1 ?
yc?n)Mn?
0, n ?
?,as long asc?nMn?
0, n ?
?.
(27)This simple calculation suggests that we can re-place c?
and M in the formula for the estimator (cf.Equation (19)) by terms that depend on n and sat-isfy the condition expressed by Equation (27).1134 Experiments4.1 CorporaIn our experiments we used the following corpora:1.
The British National Corpus (BNC): A cor-pus of about 100 million words of written andspoken British English from the years 1975-1994.2.
The New York Times Corpus (NYT): A cor-pus of about 5 million words.3.
The Malayalam Corpus (MAL): A collectionof about 2.5 million words from varied ar-ticles in the Malayalam language from theCentral Institute of Indian Languages.4.
The Hindi Corpus (HIN): A collection ofabout 3 million words from varied articles inthe Hindi language also from the Central In-stitute of Indian Languages.4.2 MethodologyWe would like to see how well our estimator per-forms in terms of estimating the number of unseenelements.
A natural way to study this is to ex-pose only half of an existing corpus to be observedand estimate the number of unseen elements (as-suming the the actual corpus is twice the observedsize).
We can then check numerically how wellour estimator performs with respect to the ?true?value.
We use a subset (the first 10%, 20%, 30%,40% and 50%) of the corpus as the observed sam-ple to estimate the vocabulary over twice the sam-ple size.
The following estimators have been com-pared.Nonparametric: Along with our proposed esti-mator (in Section 3), the following canonical es-timators available in (Gandolfi and Sastri, 2004)and (Baayen, 2001) are studied.1.
Our proposed estimator On (cf.
Section 3):since the estimator is rather involved we con-sider only small values of M (we see empir-ically that the estimator converges for verysmall values of M itself) and choose c?
= M.This allows our estimator for the number ofunseen elements to be of the following form,for different values of M :M On1 2 (?1 ?
?2)2 32 (?1 ?
?2) + 34?33 43 (?1 ?
?2) + 89(?3 ?
?43)Using this, the estimator of the true vocabu-lary size is simply,On + V. (28)Here (cf.
Equation (5))V =n?k=1?nk .
(29)In the simulations below, we have consideredM large enough until we see numerical con-vergence of the estimators: in all the cases,no more than a value of 4 is needed for M .For the English corpora, very small values ofM suffice ?
in particular, we have consideredthe average of the first three different estima-tors (corresponding to the first three valuesof M ).
For the non-English corpora, we haveneeded to consider M = 4.2.
Gandolfi-Sastri estimator,VGS def=nn?
?1(V + ?1?2), (30)where?2 = ?1 ?
n?
V2n +?5n2 + 2n(V ?
3?1) + (V ?
?1)22n ;3.
Chao estimator,VChao def= V +?212?2; (31)4.
Good-Turing estimator,VGT def=V(1?
?1n) ; (32)5.
?Simplistic?
estimator,VSmpl def= V(nnewn); (33)here the supposition is that the vocabularysize scales linearly with the sample size (herennew is the new sample size);6.
Baayen estimator,VByn def= V +(?1n)nnew; (34)here the supposition is that the vocabularygrowth rate at the observed sample size isgiven by the ratio of the number of hapaxlegomena to the sample size (cf.
(Baayen,2001) pp.
50).114% error of top 2 and Good?Turing estimates compared%error?40?30?20?10010Our GT ZM Our GT ZM Our GT ZM Our GT ZMBNC NYT Malayalam HindiFigure 1: Comparison of error estimates of the 2best estimators-ours and the ZM, with the Good-Turing estimator using 10% sample size of all thecorpora.
A bar with a positive height indicatesand overestimate and that with a negative heightindicates and underestimate.
Our estimator out-performs ZM.
Good-Turing estimator widely un-derestimates vocabulary size.Parametric: Parametric estimators use the ob-servations to first estimate the parameters.
Thenthe corresponding models are used to estimate thevocabulary size over the larger sample.
Thus thefrequency spectra of the observations are only in-directly used in extrapolating the vocabulary size.In this study we consider state of the art paramet-ric estimators, as surveyed by (Baroni and Evert,2005).
We are aided in this study by the availabil-ity of the implementations provided by the ZipfRpackage and their default settings.5 Results and DiscussionThe performance of the different estimators as per-centage errors of the true vocabulary size usingdifferent corpora are tabulated in tables 1-4.
Wenow summarize some important observations.?
From the Figure 1, we see that our estima-tor compares quite favorably with the best ofthe state of the art estimators.
The best of thestate of the art estimator is a parametric one(ZM), while ours is a nonparametric estima-tor.?
In table 1 and table 2 we see that our esti-mate is quite close to the true vocabulary, atall sample sizes.
Further, it compares very fa-vorably to the state of the art estimators (bothparametric and nonparametric).?
Again, on the two non-English corpora (ta-bles 3 and 4) we see that our estimator com-pares favorably with the best estimator of vo-cabulary size and at some sample sizes evensurpasses it.?
Our estimator has theoretical performanceguarantees and its empirical performance iscomparable to that of the state of the art es-timators.
However, this performance comesat a very small fraction of the computationalcost of the parametric estimators.?
The state of the art nonparametric Good-Turing estimator wildly underestimates thevocabulary; this is true in each of the fourcorpora studied and at all sample sizes.6 ConclusionIn this paper, we have proposed a new nonpara-metric estimator of vocabulary size that takes intoaccount the LNRE property of word frequencydistributions and have shown that it is statisticallyconsistent.
We then compared the performance ofthe proposed estimator with that of the state of theart estimators on large corpora.
While the perfor-mance of our estimator seems favorable, we alsosee that the widely used classical Good-Turingestimator consistently underestimates the vocabu-lary size.
Although as yet untested, with its com-putational simplicity and favorable performance,our estimator may serve as a more reliable alter-native to the Good-Turing estimator for estimatingvocabulary sizes.AcknowledgmentsThis research was partially supported by AwardIIS-0623805 from the National Science Founda-tion.ReferencesR.
H. Baayen.
2001.
Word Frequency Distributions,Kluwer Academic Publishers.Marco Baroni and Stefan Evert.
2001.
?Testing the ex-trapolation quality of word frequency models?, Pro-ceedings of Corpus Linguistics , volume 1 of TheCorpus Linguistics Conference Series, P. Danielssonand M. Wagenmakers (eds.).J.
Bunge and M. Fitzpatrick.
1993.
?Estimating thenumber of species: a review?, Journal of the Amer-ican Statistical Association, Vol.
88(421), pp.
364-373.115Sample True % error w.r.t the true value(% of corpus) value Our GT ZM fZM Smpl Byn Chao GS10 153912 1 -27 -4 -8 46 23 8 -1120 220847 -3 -30 -9 -12 39 19 4 -1530 265813 -2 -30 -9 -11 39 20 6 -1540 310351 1 -29 -7 -9 42 23 9 -1350 340890 2 -28 -6 -8 43 24 10 -12Table 1: Comparison of estimates of vocabulary size for the BNC corpus as percentage errors w.r.t thetrue value.
A negative value indicates an underestimate.
Our estimator outperforms the other estimatorsat all sample sizes.Sample True % error w.r.t the true value(% of corpus) value Our GT ZM fZM Smpl Byn Chao GS10 37346 1 -24 5 -8 48 28 4 -820 51200 -3 -26 0 -11 46 22 -1 -1130 60829 -2 -25 1 -10 48 23 1 -1040 68774 -3 -25 0 -10 49 21 -1 -1150 75526 -2 -25 0 -10 50 21 0 -10Table 2: Comparison of estimates of vocabulary size for the NYT corpus as percentage errors w.r.t thetrue value.
A negative value indicates an underestimate.
Our estimator compares favorably with ZM andChao.Sample True % error w.r.t the true value(% of corpus) value Our GT ZM fZM Smpl Byn Chao GS10 146547 -2 -27 -5 -10 9 34 82 -220 246723 8 -23 4 -2 19 47 105 530 339196 4 -27 0 -5 16 42 93 -140 422010 5 -28 1 -4 17 43 95 -150 500166 5 -28 1 -4 18 44 94 -2Table 3: Comparison of estimates of vocabulary size for the Malayalam corpus as percentage errorsw.r.t the true value.
A negative value indicates an underestimate.
Our estimator compares favorably withZM and GS.Sample True % error w.r.t the true value(% of corpus) value Our GT ZM fZM Smpl Byn Chao GS10 47639 -2 -34 -4 -9 25 32 31 -1220 71320 7 -30 2 -1 34 43 51 -730 93259 2 -33 -1 -5 30 38 42 -1040 113186 0 -35 -5 -7 26 34 39 -1350 131715 -1 -36 -6 -8 24 33 40 -14Table 4: Comparison of estimates of vocabulary size for the Hindi corpus as percentage errors w.r.t thetrue value.
A negative value indicates an underestimate.
Our estimator outperforms the other estimatorsat certain sample sizes.116A.
Gandolfi and C. C. A. Sastri.
2004.
?Nonparamet-ric Estimations about Species not Observed in aRandom Sample?, Milan Journal of Mathematics,Vol.
72, pp.
81-105.E.
V. Khmaladze.
1987.
?The statistical analysis oflarge number of rare events?, Technical Report, De-partment of Mathematics and Statistics., CWI, Am-sterdam, MS-R8804.E.
V. Khmaladze and R. J. Chitashvili.
1989.
?Statis-tical analysis of large number of rate events and re-lated problems?, Probability theory and mathemati-cal statistics (Russian), Vol.
92, pp.
196-245.. P. Santhanam, A. Orlitsky, and K. Viswanathan, ?Newtricks for old dogs: Large alphabet probability es-timation?, in Proc.
2007 IEEE Information TheoryWorkshop, Sept. 2007, pp.
638?643.A.
B. Wagner, P. Viswanath and S. R. Kulkarni.
2006.?Strong Consistency of the Good-Turing estimator?,IEEE Symposium on Information Theory, 2006.117
