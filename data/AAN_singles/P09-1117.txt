Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 1039?1047,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPSemi-Supervised Active Learning for Sequence LabelingKatrin Tomanek and Udo HahnJena University Language & Information Engineering (JULIE) LabFriedrich-Schiller-Universita?t Jena, Germany{katrin.tomanek|udo.hahn}@uni-jena.deAbstractWhile Active Learning (AL) has alreadybeen shown to markedly reduce the anno-tation efforts for many sequence labelingtasks compared to random selection, ALremains unconcerned about the internalstructure of the selected sequences (typ-ically, sentences).
We propose a semi-supervised AL approach for sequence la-beling where only highly uncertain sub-sequences are presented to human anno-tators, while all others in the selected se-quences are automatically labeled.
For thetask of entity recognition, our experimentsreveal that this approach reduces annota-tion efforts in terms of manually labeledtokens by up to 60 % compared to the stan-dard, fully supervised AL scheme.1 IntroductionSupervised machine learning (ML) approaches arecurrently the methodological backbone for lots ofNLP activities.
Despite their success they create acostly follow-up problem, viz.
the need for humanannotators to supply large amounts of ?golden?annotation data on which ML systems can betrained.
In most annotation campaigns, the lan-guage material chosen for manual annotation is se-lected randomly from some reference corpus.Active Learning (AL) has recently shaped as amuch more efficient alternative for the creation ofprecious training material.
In the AL paradigm,only examples of high training utility are selectedfor manual annotation in an iterative manner.
Dif-ferent approaches to AL have been successfullyapplied to a wide range of NLP tasks (Engel-son and Dagan, 1996; Ngai and Yarowsky, 2000;Tomanek et al, 2007; Settles and Craven, 2008).When used for sequence labeling tasks such asPOS tagging, chunking, or named entity recogni-tion (NER), the examples selected by AL are se-quences of text, typically sentences.
Approachesto AL for sequence labeling are usually uncon-cerned about the internal structure of the selectedsequences.
Although a high overall training util-ity might be attributed to a sequence as a whole,the subsequences it is composed of tend to ex-hibit different degrees of training utility.
In theNER scenario, e.g., large portions of the text donot contain any target entity mention at all.
Tofurther exploit this observation for annotation pur-poses, we here propose an approach to AL wherehuman annotators are required to label only uncer-tain subsequences within the selected sentences,while the remaining subsequences are labeled au-tomatically based on the model available from theprevious AL iteration round.
The hardness of sub-sequences is characterized by the classifier?s con-fidence in the predicted labels.
Accordingly, ourapproach is a combination of AL and self-trainingto which we will refer as semi-supervised ActiveLearning (SeSAL) for sequence labeling.While self-training and other bootstrapping ap-proaches often fail to produce good results on NLPtasks due to an inherent tendency of deteriorateddata quality, SeSAL circumvents this problem andstill yields large savings in terms annotation de-cisions, i.e., tokens to be manually labeled, com-pared to a standard, fully supervised AL approach.After a brief overview of the formal underpin-nings of Conditional Random Fields, our baseclassifier for sequence labeling tasks (Section 2),a fully supervised approach to AL for sequencelabeling is introduced and complemented by oursemi-supervised approach in Section 3.
In Section4, we discuss SeSAL in relation to bootstrappingand existing AL techniques.
Our experiments arelaid out in Section 5 where we compare fully andsemi-supervised AL for NER on two corpora, thenewspaper selection of MUC7 and PENNBIOIE, abiological abstracts corpus.10392 Conditional Random Fields forSequence LabelingMany NLP tasks, such as POS tagging, chunking,or NER, are sequence labeling problems where asequence of class labels ~y = (y1, .
.
.
,yn) ?
Ynare assigned to a sequence of input units~x = (x1, .
.
.
,xn) ?
X n. Input units xj are usuallytokens, class labels yj can be POS tags or entityclasses.Conditional Random Fields (CRFs) (Lafferty etal., 2001) are a probabilistic framework for label-ing structured data and model P~?(~y|~x).
We focuson first-order linear-chain CRFs, a special form ofCRFs for sequential data, whereP~?
(~y|~x) =1Z~?(~x)?
exp(n?j=1m?i=1?ifi(yj?1,yj ,~x, j))(1)with normalization factor Z~?
(~x), feature functionsfi(?
), and feature weights ?i.Parameter Estimation.
The model parameters?i are set to maximize the penalized log-likelihoodL on some training data T :L(T ) =?
(~x,~y)?Tlog p(~y|~x) ?m?i=1?2i2?2 (2)The partial derivations of L(T ) are?L(T )?
?i= E?
(fi) ?
E(fi) ?
?i?2 (3)where E?
(fi) is the empirical expectation of fea-ture fi and can be calculated by counting the oc-currences of fi in T .
E(fi) is the model expecta-tion of fi and can be written asE(fi) =?
(~x,~y)?T?~y ??YnP~?
(~y?|~x)?n?j=1fi(y?j?1, y?j , ~x,j) (4)Direct computation of E(fi) is intractable due tothe sum over all possible label sequences ~y ?
?
Yn.The Forward-Backward algorithm (Rabiner, 1989)solves this problem efficiently.
Forward (?)
andbackward (?)
scores are defined by?j(y|~x) =?y?
?T?1j (y)?j?1(y?|~x) ?
?j(~x, y?, y)?j(y|~x) =?y?
?Tj(y)?j+1(y?|~x) ?
?j(~x, y, y?
)where ?j(~x,a,b) = exp(?mi=1 ?ifi(a,b,~x, j)),Tj(y) is the set of all successors of a state y at aspecified position j, and, accordingly, T?1j (y) isthe set of predecessors.Normalized forward and backward scoresare inserted into Equation (4) to replace?~y ?
?Yn P~?
(~y?|~x) so that L(T ) can be opti-mized with gradient-based or iterative-scalingmethods.Inference and Probabilities.
The marginalprobabilityP~?
(yj = y?|~x) = ?j(y?|~x) ?
?j(y?|~x)Z~?
(~x)(5)specifies the model?s confidence in label y?
at po-sition j of an input sequence ~x.
The forwardand backward scores are obtained by applying theForward-Backward algorithm on ~x.
The normal-ization factor is efficiently calculated by summingover all forward scores:Z~?
(~x) =?y?Y?n(y|~x) (6)The most likely label sequence~y ?
= argmax~y?Ynexp(n?j=1m?i=1?ifi(yj?1,yj ,~x, j))(7)is computed using the Viterbi algorithm (Rabiner,1989).
See Equation (1) for the conditional prob-ability P~?
(~y?|~x) with Z~?
calculated as in Equa-tion (6).
The marginal and conditional probabili-ties are used by our AL approaches as confidenceestimators.3 Active Learning for Sequence LabelingAL is a selective sampling technique where thelearning protocol is in control of the data to beused for training.
The intention with AL is to re-duce the amount of labeled training material byquerying labels only for examples which are as-sumed to have a high training utility.
This section,first, describes a common approach to AL for se-quential data, and then presents our approach tosemi-supervised AL.3.1 Fully Supervised Active LearningAlgorithm 1 describes the general AL framework.A utility function UM(pi) is the core of each ALapproach ?
it estimates how useful it would be for1040Algorithm 1 General AL frameworkGiven:B: number of examples to be selectedL: set of labeled examplesP : set of unlabeled examplesUM: utility functionAlgorithm:loop until stopping criterion is met1.
learn modelM from L2.
for all pi ?
P : upi ?
UM(pi)3. select B examples pi ?
P with highest utility upi4.
query human annotator for labels of all B examples5.
move newly labeled examples from P to Lreturn La specific base learner to have an unlabeled exam-ple labeled and, subsequently included in the train-ing set.In the sequence labeling scenario, such an ex-ample is a stream of linguistic items ?
a sentenceis usually considered as proper sequence unit.
Weapply CRFs as our base learner throughout this pa-per and employ a utility function which is basedon the conditional probability of the most likelylabel sequence ~y ?
for an observation sequence ~x(cf.
Equations (1) and (7)):U~?
(~x) = 1 ?
P~?
(~y?|~x) (8)Sequences for which the current model is leastconfident on the most likely label sequence arepreferably selected.1 These selected sentences arefully manually labeled.
We refer to this AL modeas fully supervised Active Learning (FuSAL).3.2 Semi-Supervised Active LearningIn the sequence labeling scenario, an examplewhich, as a whole, has a high utility U~?
(~x), canstill exhibit subsequences which do not add muchto the overall utility and thus are fairly easy for thecurrent model to label correctly.
One might there-fore doubt whether it is reasonable to manually la-bel the entire sequence.
Within many sequencesof natural language data, there are probably largesubsequences on which the current model alreadydoes quite well and thus could automatically gen-erate annotations with high quality.
This might, inparticular, apply to NER where larger stretches ofsentences do not contain any entity mention at all,or merely trivial instances of an entity class easilypredictable by the current model.1There are many more sophisticated utility functions forsequence labeling.
We have chosen this straightforward onefor simplicity and because it has proven to be very effective(Settles and Craven, 2008).For the sequence labeling scenario, we accord-ingly modify the fully supervised AL approachfrom Section 3.1.
Only those tokens remain to bemanually labeled on which the current model ishighly uncertain regarding their class labels, whileall other tokens (those on which the model is suf-ficiently certain how to label them correctly) areautomatically tagged.To select the sequence examples the same util-ity function as for FuSAL (cf.
Equation (8)) is ap-plied.
To identify tokens xj from the selected se-quences which still have to be manually labeled,the model?s confidence in label y?j is estimated bythe marginal probability (cf.
Equation (5))C~?
(y?j ) = P~?
(yj = y?j |~x) (9)where y?j specifies the label at the respective po-sition of the most likely label sequence ~y ?
(cf.Equation (7)).
If C~?
(y?j ) exceeds a certain con-fidence threshold t, y?j is assumed to be the correctlabel for this token and assigned to it.2 Otherwise,manual annotation of this token is required.
So,compared to FuSAL as described in Algorithm 1only the third step step is modified.We call this semi-supervised Active Learning(SeSAL) for sequence labeling.
SeSAL joins thestandard, fully supervised AL schema with a boot-strapping mode, namely self-training, to combinethe strengths of both approaches.
Examples withhigh training utility are selected using AL, whileself-tagging of certain ?safe?
regions within suchexamples additionally reduces annotation effort.Through this combination, SeSAL largely evadesthe problem of deteriorated data quality, a limitingfactor of ?pure?
bootstrapping approaches.This approach requires two parameters to be set:Firstly, the confidence threshold t which directlyinfluences the portion of tokens to be manuallylabeled.
Using lower thresholds, the self-taggingcomponent of SeSAL has higher impact ?
presum-ably leading to larger amounts of tagging errors.Secondly, a delay factor d can be specified whichchannels the amount of manually labeled tokensobtained with FuSAL before SeSAL is to start.Only with d = 0, SeSAL will already affect thefirst AL iteration.
Otherwise, several iterations ofFuSAL are run until a switch to SeSAL will hap-pen.2Sequences of consecutive tokens xj for which C~?
(y?j ) ?t are presented to the human annotator instead of single, iso-lated tokens.1041It is well known that the performance of boot-strapping approaches crucially depends on the sizeof the seed set ?
the amount of labeled examplesavailable to train the initial model.
If class bound-aries are poorly defined by choosing the seed settoo small, a bootstrapping system cannot learnanything reasonable due to high error rates.
If, onthe other hand, class boundaries are already toowell defined due to an overly large seed set, noth-ing to be learned is left.
Thus, together with lowthresholds, a delay rate of d > 0 might be crucialto obtain models of high performance.4 Related WorkCommon approaches to AL are variants of theQuery-By-Committee approach (Seung et al,1992) or based on uncertainty sampling (Lewisand Catlett, 1994).
Query-by-Committee uses acommittee of classifiers, and examples on whichthe classifiers disagree most regarding their pre-dictions are considered highly informative andthus selected for annotation.
Uncertainty sam-pling selects examples on which a single classi-fier is least confident.
AL has been successfullyapplied to many NLP tasks; Settles and Craven(2008) compare the effectiveness of several ALapproaches for sequence labeling tasks of NLP.Self-training (Yarowsky, 1995) is a form ofsemi-supervised learning.
From a seed set of la-beled examples a weak model is learned whichsubsequently gets incrementally refined.
In eachstep, unlabeled examples on which the currentmodel is very confident are labeled with their pre-dictions, added to the training set, and a newmodel is learned.
Similar to self-training, co-training (Blum and Mitchell, 1998) augments thetraining set by automatically labeled examples.It is a multi-learner algorithm where the learnershave independent views on the data and mutuallyproduce labeled examples for each other.Bootstrapping approaches often fail when ap-plied to NLP tasks where large amounts of trainingmaterial are required to achieve acceptable perfor-mance levels.
Pierce and Cardie (2001) showedthat the quality of the automatically labeled train-ing data is crucial for co-training to perform wellbecause too many tagging errors prevent a high-performing model from being learned.
Also, thesize of the seed set is an important parameter.When it is chosen too small data quality gets dete-riorated quickly, when it is chosen too large no im-provement over the initial model can be expected.To address the problem of data pollution by tag-ging errors, Pierce and Cardie (2001) propose cor-rected co-training.
In this mode, a human is putinto the co-training loop to review and, if neces-sary, to correct the machine-labeled examples.
Al-though this effectively evades the negative side ef-fects of deteriorated data quality, one may find thecorrection of labeled data to be as time-consumingas annotations from the scratch.
Ideally, a humanshould not get biased by the proposed label butindependently examine the example ?
so that cor-rection eventually becomes annotation.In contrast, our SeSAL approach which also ap-plies bootstrapping, aims at avoiding to deterioratedata quality by explicitly pointing human annota-tors to classification-critical regions.
While thoseregions require full annotation, regions of highconfidence are automatically labeled and thus donot require any manual inspection.
Self-trainingand co-training, in contradistinction, select exam-ples of high confidence only.
Thus, these boot-strapping methods will presumably not find themost useful unlabeled examples but require a hu-man to review data points of limited training util-ity (Pierce and Cardie, 2001).
This shortcoming isalso avoided by our SeSAL approach, as we inten-tionally select informative examples only.A combination of active and semi-supervisedlearning has first been proposed by McCallum andNigam (1998) for text classification.
Committee-based AL is used for the example selection.
Thecommittee members are first trained on the labeledexamples and then augmented by means of Expec-tation Maximization (EM) (Dempster et al, 1977)including the unlabeled examples.
The idea isto avoid manual labeling of examples whose la-bels can be reliably assigned by EM.
Similarly,co-testing (Muslea et al, 2002), a multi-view ALalgorithms, selects examples for the multi-view,semi-supervised Co-EM algorithm.
In both works,semi-supervision is based on variants of the EMalgorithm in combination with all unlabeled ex-amples from the pool.
Our approach to semi-supervised AL is different as, firstly, we aug-ment the training data using a self-tagging mech-anism (McCallum and Nigam (1998) and Musleaet al (2002) performed semi-supervision to aug-ment the models using EM), and secondly, we op-erate in the sequence labeling scenario where anexample is made up of several units each requiring1042a label ?
partial labeling of sequence examples isa central characteristic of our approach.
Anotherwork also closely related to ours is that of Krist-jansson et al (2004).
In an information extractionsetting, the confidence per extracted field is cal-culated by a constrained variant of the Forward-Backward algorithm.
Unreliable fields are high-lighted so that the automatically annotated corpuscan be corrected.
In contrast, AL selection of ex-amples together with partial manual labeling of theselected examples are the main foci of our work.5 Experiments and ResultsIn this section, we turn to the empirical assessmentof semi-supervised AL (SeSAL) for sequence la-beling on the NLP task of named entity recogni-tion.
By the nature of this task, the sequences ?in this case, sentences ?
are only sparsely popu-lated with entity mentions and most of the tokensbelong to the OUTSIDE class3 so that SeSAL canbe expected to be very beneficial.5.1 Experimental SettingsIn all experiments, we employ the linear-chainCRF model described in Section 2 as the baselearner.
A set of common feature functions wasemployed, including orthographical (regular ex-pression patterns), lexical and morphological (suf-fixes/prefixes, lemmatized tokens), and contextual(features of neighboring tokens) ones.All experiments start from a seed set of 20 ran-domly selected examples and, in each iteration,50 new examples are selected using AL.
The ef-ficiency of the different selection mechanisms isdetermined by learning curves which relate the an-notation costs to the performance achieved by therespective model in terms of F1-score.
The unit ofannotation costs are manually labeled tokens.
Al-though the assumption of uniform costs per tokenhas already been subject of legitimate criticism(Settles et al, 2008), we believe that the numberof annotated tokens is still a reasonable approxi-mation in the absence of an empirically more ade-quate task-specific annotation cost model.We ran the experiments on two entity-annotatedcorpora.
From the general-language newspaperdomain, we took the training part of the MUC7corpus (Linguistic Data Consortium, 2001) whichincorporates seven different entity types, viz.
per-3The OUTSIDE class is assigned to each token that doesnot denote an entity in the underlying domain of discourse.corpus entity classes sentences tokensMUC7 7 3,020 78,305PENNBIOIE 3 10,570 267,320Table 1: Quantitative characteristics of the chosen corporasons, organizations, locations, times, dates, mone-tary expressions, and percentages.
From the sub-language biology domain, we used the oncologypart of the PENNBIOIE corpus (Kulick et al,2004) and removed all but three gene entity sub-types (generic, protein, and rna).
Table 1 summa-rizes the quantitative characteristics of both cor-pora.4 The results reported below are averages of20 independent runs.
For each run, we randomlysplit each corpus into a pool of unlabeled examplesto select from (90 % of the corpus), and a comple-mentary evaluation set (10 % of the corpus).5.2 Empirical EvaluationWe compare semi-supervised AL (SeSAL) withits fully supervised counterpart (FuSAL), usinga passive learning scheme where examples arerandomly selected (RAND) as baseline.
SeSALis first applied in a default configuration with avery high confidence threshold (t = 0.99) with-out any delay (d = 0).
In further experiments,these parameters are varied to study their impacton SeSAL?s performance.
All experiments wererun on both the newspaper (MUC7) and biological(PENNBIOIE) corpus.
When results are similar toeach other, only one data set will be discussed.Distribution of Confidence Scores.
The lead-ing assumption for SeSAL is that only a small por-tion of tokens within the selected sentences consti-tute really hard decision problems, while the ma-jority of tokens are easy to account for by the cur-rent model.
To test this stipulation we investigatethe distribution of the model?s confidence valuesC~?
(y?j ) over all tokens of the sentences (cf.
Equa-tion (9)) selected within one iteration of FuSAL.Figure 1, as an example, depicts the histogramfor an early AL iteration round on the MUC7 cor-pus.
The vast majority of tokens has a confidencescore close to 1, the median lies at 0.9966.
His-tograms of subsequent AL iterations are very sim-ilar with an even higher median.
This is so because4We removed sentences of considerable over and underlength (beyond +/- 3 standard deviations around the averagesentence length) so that the numbers in Table 1 differ fromthose cited in the original sources.1043confidence scorefrequency0.2 0.4 0.6 0.8 1.0050010001500Figure 1: Distribution of token-level confidence scores in the5th iteration of FuSAL on MUC7 (number of tokens: 1,843)the model gets continuously more confident whentrained on additional data and fewer hard cases re-main in the shrinking pool.Fully Supervised vs. Semi-Supervised AL.Figure 2 compares the performance of FuSAL andSeSAL on the two corpora.
SeSAL is run witha delay rate of d = 0 and a very high confi-dence threshold of t = 0.99 so that only thosetokens are automatically labeled on which the cur-rent model is almost certain.
Figure 2 clearlyshows that SeSAL is much more efficient thanits fully supervised counterpart.
Table 2 depictsthe exact numbers of manually labeled tokens toreach the maximal (supervised) F-score on bothcorpora.
FuSAL saves about 50 % compared toRAND, while SeSAL saves about 60 % comparedto FuSAL which constitutes an overall saving ofover 80 % compared to RAND.These savings are calculated relative to thenumber of tokens which have to be manually la-beled.
Yet, consider the following gedanken ex-periment.
Assume that, using SeSAL, every sec-ond token in a sequence would have to be labeled.Though this comes to a ?formal?
saving of 50 %,the actual annotation effort in terms of the timeneeded would hardly go down.
It appears thatonly when SeSAL splits a sentence into largerCorpus Fmax RAND FuSAL SeSALMUC7 87.7 63,020 36,015 11,001PENNBIOIE 82.3 194,019 83,017 27,201Table 2: Tokens manually labeled to reach the maximal (su-pervised) F-score0 10000 30000 500000.600.700.800.90MUC7manually labeled tokensF?scoreSeSALFuSALRAND0 10000 30000 500000.600.700.800.90PennBioIEmanually labeled tokensF?scoreSeSALFuSALRANDFigure 2: Learning curves for Semi-supervised AL (SeSAL),Fully Supervised AL (FuSAL), and RAND(om) selectionwell-packaged, chunk-like subsequences annota-tion time can really be saved.
To demonstrate thatSeSAL comes close to this, we counted the num-ber of base noun phrases (NPs) containing one ormore tokens to be manually labeled.
On the MUC7corpus, FuSAL requires 7,374 annotated NPs toyield an F-score of 87%, while SeSAL hit thesame F-score with only 4,017 NPs.
Thus, also interms of the number of NPs, SeSAL saves about45% of the material to be considered.5Detailed Analysis of SeSAL.
As Figure 2 re-veals, the learning curves of SeSAL stop early (onMUC7 after 12,800 tokens, on PENNBIOIE after27,600 tokens) because at that point the whole cor-pus has been labeled exhaustively ?
either manu-ally, or automatically.
So, using SeSAL the com-plete corpus can be labeled with only a smallfraction of it actually being manually annotated(MUC7: about 18%, PENNBIOIE: about 13%).5On PENNBIOIE, SeSAL also saves about 45% com-pared to FuSAL to achieve an F-score of 81%.1044Table 3 provides additional analysis results onMUC7.
In very early AL rounds, a large ratio oftokens has to be manually labeled (70-80 %).
Thisnumber decreases increasingly as the classifier im-proves (and the pool contains fewer informativesentences).
The number of tagging errors is quitelow, resulting in a high accuracy of the created cor-pus of constantly over 99 %.labeled tokensmanual automatic ?
AR (%) errors ACC1,000 253 1,253 79.82 6 99.515,000 6,207 11,207 44.61 82 99.2710,000 25,506 34,406 28.16 174 99.5112,800 57,371 70,171 18.24 259 99.63Table 3: Analysis of SeSAL on MUC7: Manually and auto-matically labeled tokens, annotation rate (AR) as the portionof manually labeled tokens in the total amount of labeled to-kens, errors and accuracy (ACC) of the created corpus.The majority of the automatically labeled to-kens (97-98 %) belong to the OUTSIDE class.This coincides with the assumption that SeSALworks especially well for labeling tasks wheresome classes occur predominantly and can, inmost cases, easily be discriminated from the otherclasses, as is the case in the NER scenario.
Ananalysis of the errors induced by the self-taggingcomponent reveals that most of the errors (90-100 %) are due to missed entity classes, i.e., whilethe correct class label for a token is one of theentity classes, the OUTSIDE class was assigned.This effect is more severe in early than in later ALiterations (see Table 4 for the exact numbers).labeled error types (%)corpus tokens errors E2O O2E E2EMUC7 10,000 75 100 ?
?70,000 259 96 1.3 2.7Table 4: Distribution of errors of the self-tagging component.Error types: OUTSIDE class assigned though an entity classis correct (E2O), entity class assigned but OUTSIDE is cor-rect (O2E), wrong entity class assigned (E2E).Impact of the Confidence Threshold.
We alsoran SeSAL with different confidence thresholds t(0.99, 0.95, 0.90, and 0.70) and analyzed the re-sults with respect to tagging errors and the modelperformance.
Figure 3 shows the learning and er-ror curves for different thresholds on the MUC7corpus.
The supervised F-score of 87.7% is onlyreached by the highest and most restrictive thresh-old of t = 0.99.
With all other thresholds, SeSAL0 2000 6000 100000.600.700.800.90learning curvesmanually labeled tokensF?scoret=0.99t=0.95t=0.90t=0.700 20000 40000 60000050010002000error curvesall labeled tokenserrorst=0.99t=0.95t=0.90t=0.70Figure 3: Learning and error curves for SeSAL with differentthresholds on the MUC7 corpusstops at much lower F-scores and produces labeledtraining data of lower accuracy.
Table 5 containsthe exact numbers and reveals that the poor modelperformance of SeSAL with lower thresholds ismainly due to dropping recall values.threshold F R P Acc0.99 87.7 85.9 89.9 99.60.95 85.4 82.3 88.7 98.80.90 84.3 80.6 88.3 98.10.70 69.9 61.8 81.1 96.5Table 5: Maximum model performance on MUC7 in terms ofF-score (F), recall (R), precision (P) and accuracy (Acc) ?
thelabeled corpus obtained by SeSAL with different thresholdsImpact of the Delay Rate.
We also measuredthe impact of delay rates on SeSAL?s efficiencyconsidering three delay rates (1,000, 5,000, and10,000 tokens) in combination with three confi-dence thresholds (0.99, 0.9, and 0.7).
Figure 4 de-picts the respective learning curves on the MUC7corpus.
For SeSAL with t = 0.99, the delay10450 5000 10000 15000 200000.600.700.800.90threshold 0.99manually labeled tokensF?scoreFuSALSeSAL, d=0SeSAL, d=1000SeSAL, d=5000SeSAL, d=10000F=0.8770 5000 10000 15000 200000.600.700.800.90threshold 0.9manually labeled tokensF?scoreFuSALSeSAL, d=0SeSAL, d=1000SeSAL, d=5000SeSAL, d=10000F=0.843F=0.8770 2000 6000 100000.600.700.800.90threshold 0.7manually labeled tokensF?scoreFuSALSeSAL, d=0SeSAL, d=1000SeSAL, d=5000SeSAL, d=10000F=69.9F=0.877Figure 4: SeSAL with different delay rates and thresholds on MUC7.
Horizontal lines mark the supervised F-score (upper line)and the maximal F-score achieved by SeSAL with the respective threshold and d = 0 (lower line).has no particularly beneficial effect.
However,in combination with lower thresholds, the delayrates show positive effects as SeSAL yields F-scores closer to the maximal F-score of 87.7%,thus clearly outperforming undelayed SeSAL.6 Summary and DiscussionOur experiments in the context of the NERscenario render evidence to the hypothesis thatthe proposed approach to semi-supervised AL(SeSAL) for sequence labeling indeed strongly re-duces the amount of tokens to be manually anno-tated ?
in terms of numbers, about 60% comparedto its fully supervised counterpart (FuSAL), andover 80% compared to a totally passive learningscheme based on random selection.For SeSAL to work well, a high and, by this,restrictive threshold has been shown to be crucial.Otherwise, large amounts of tagging errors lead toa poorer overall model performance.
In our ex-periments, tagging errors in such a scenario wereOUTSIDE labelings, while an entity class wouldhave been correct ?
with the effect that the result-ing models showed low recall rates.The delay rate is important when SeSAL is runwith a low threshold as early tagging errors canbe avoided which otherwise reinforce themselves.Finding the right balance between the delay factorand low thresholds requires experimental calibra-tion.
For the most restrictive threshold (t = 0.99)though such a delay is unimportant so that it canbe set to d = 0 circumventing this calibration step.In summary, the self-tagging component ofSeSAL gets more influential when the confidencethreshold and the delay factor are set to lower val-ues.
At the same time though, under these con-ditions negative side-effects such as deteriorateddata quality and, by this, inferior models emerge.These problems are major drawbacks of manybootstrapping approaches.
However, our experi-ments indicate that as long as self-training is cau-tiously applied (as is done for SeSAL with restric-tive parameters), it can definitely outperform anentirely supervised approach.From an annotation point of view, SeSAL effi-ciently guides the annotator to regions within theselected sentence which are very useful for thelearning task.
In our experiments on the NER sce-nario, those regions were mentions of entity namesor linguistic units which had a surface appearancesimilar to entity mentions but could not yet be cor-rectly distinguished by the model.While we evaluated SeSAL here in terms oftokens to be manually labeled, an open issue re-mains, namely how much of the real annotationeffort ?
measured by the time needed ?
is savedby this approach.
We here hypothesize that hu-man annotators work much more efficiently whenpointed to the regions of immediate interest in-stead of making them skim in a self-paced waythrough larger passages of (probably) semanticallyirrelevant but syntactically complex utterances ?a tiring and error-prone task.
Future research isneeded to empirically investigate into this area andquantify the savings in terms of the time achiev-able with SeSAL in the NER scenario.AcknowledgementsThis work was funded by the EC within theBOOTStrep (FP6-028099) and CALBC (FP7-231727) projects.
We want to thank Roman Klin-ger (Fraunhofer SCAI) for fruitful discussions.1046ReferencesA.
Blum and T. Mitchell.
1998.
Combining labeledand unlabeled data with co-training.
In COLT?98 ?Proceedings of the 11th Annual Conference on Com-putational Learning Theory, pages 92?100.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.Maximum likelihood from incomplete data via theEM algorithm.
Journal of the Royal Statistical So-ciety, 39(1):1?38.S.
Engelson and I. Dagan.
1996.
Minimizing man-ual annotation cost in supervised training from cor-pora.
In ACL?96 ?
Proceedings of the 34th AnnualMeeting of the Association for Computational Lin-guistics, pages 319?326.T.
Kristjansson, A. Culotta, and P. Viola.
2004.
Inter-active information extraction with constrained Con-ditional Random Fields.
In AAAI?04 ?
Proceed-ings of 19th National Conference on Artificial Intel-ligence, pages 412?418.S.
Kulick, A. Bies, M. Liberman, M. Mandel, R. T. Mc-Donald, M. S. Palmer, and A. I. Schein.
2004.
Inte-grated annotation for biomedical information extrac-tion.
In Proceedings of the HLT-NAACL 2004 Work-shop ?Linking Biological Literature, Ontologies andDatabases: Tools for Users?, pages 61?68.J.
D. Lafferty, A. McCallum, and F. Pereira.
2001.Conditional Random Fields: Probabilistic modelsfor segmenting and labeling sequence data.
InICML?01 ?
Proceedings of the 18th InternationalConference on Machine Learning, pages 282?289.D.
D. Lewis and J. Catlett.
1994.
Heterogeneousuncertainty sampling for supervised learning.
InICML?94 ?
Proceedings of the 11th InternationalConference on Machine Learning, pages 148?156.Linguistic Data Consortium.
2001.
Message Under-standing Conference (MUC) 7.
LDC2001T02.
FTPFILE.
Philadelphia: Linguistic Data Consortium.A.
McCallum and K. Nigam.
1998.
Employing EMand pool-based Active Learning for text classifica-tion.
In ICML?98 ?
Proceedings of the 15th Interna-tional Conference on Machine Learning, pages 350?358.I.
A. Muslea, S. Minton, and C. A. Knoblock.
2002.Active semi-supervised learning = Robust multi-view learning.
In ICML?02 ?
Proceedings of the19th International Conference on Machine Learn-ing, pages 435?442.G.
Ngai and D. Yarowsky.
2000.
Rule writing or anno-tation: Cost-efficient resource usage for base nounphrase chunking.
In ACL?00 ?
Proceedings of the38th Annual Meeting of the Association for Compu-tational Linguistics, pages 117?125.D.
Pierce and C. Cardie.
2001.
Limitations of co-training for natural language learning from largedatasets.
In EMNLP?01 ?
Proceedings of the 2001Conference on Empirical Methods in Natural Lan-guage Processing, pages 1?9.L.
R. Rabiner.
1989.
A tutorial on Hidden MarkovModels and selected applications in speech recogni-tion.
Proceedings of the IEEE, 77(2):257?286.B.
Settles and M. Craven.
2008.
An analysis of ActiveLearning strategies for sequence labeling tasks.
InEMNLP?08 ?
Proceedings of the 2008 Conferenceon Empirical Methods in Natural Language Pro-cessing, pages 1069?1078.B.
Settles, M. Craven, and L. Friedland.
2008.
ActiveLearning with real annotation costs.
In Proceedingsof the NIPS 2008 Workshop on ?Cost-Sensitive Ma-chine Learning?, pages 1?10.H.
S. Seung, M. Opper, and H. Sompolinsky.
1992.Query by committee.
In COLT?92 ?
Proceedings ofthe 5th Annual Workshop on Computational Learn-ing Theory, pages 287?294.K.
Tomanek, J. Wermter, and U. Hahn.
2007.
An ap-proach to text corpus construction which cuts anno-tation costs and maintains corpus reusability of an-notated data.
In EMNLP-CoNLL?07 ?
Proceedingsof the 2007 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalLanguage Learning, pages 486?495.D.
Yarowsky.
1995.
Unsupervised word sense disam-biguation rivaling supervised methods.
In ACL?95 ?Proceedings of the 33rd Annual Meeting of the As-sociation for Computational Linguistics, pages 189?196.1047
