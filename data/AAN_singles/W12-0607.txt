Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 53?60,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsPredicting the 2011 Dutch Senate Election Results with TwitterErik Tjong Kim Sang and Johan BosAlfa-informaticaUniversity of GroningenGroningen, The Netherlands{e.f.tjong.kim.sang,johan.bos}@rug.nlAbstractTo what extend can one use Twitter in opin-ion polls for political elections?
Merelycounting Twitter messages mentioning po-litical party names is no guarantee for ob-taining good election predictions.
By im-proving the quality of the document col-lection and by performing sentiment anal-ysis, predictions based on entity counts intweets can be considerably improved, andbecome nearly as good as traditionally ob-tained opinion polls.1 IntroductionPredicting the future is one of human?s great-est desires.
News companies are well aware ofthis, and try to predict tomorrow?s weather andchanges on the stock markets.
Another case inpoint are the opinion polls, of which the newsis abundant in the period before political elec-tions.
Such polls are traditionally based on ask-ing a (representative) sample of voters what theywould vote on the day of election.The question we are interested in, is whetheropinion polls could be conducted on the basisof the information collected by Twitter, a popu-lar microblog website, used by millions to broad-cast messages of no more than 140 characters,known as tweets.
Over the last two years, we havecollected a multi-billion-word corpus of Dutch1The data and software used for the experiments de-scribed in this paper can be retrieved from http://ifarm.nl/ps2011/p2011.ziptweets, with the general aim of developing nat-ural language processing tools for automaticallyanalyzing the content of the messages in this newsocial medium, which comes with its own chal-lenges.
When the Dutch Senate elections tookplace in 2011, we took this as an opportunity toverify the predictive power of tweets.More concretely, we wanted to test whether bysimply counting Twitter messages mentioning po-litical party names we could accurately predict theelection outcome.
Secondly, we wanted to inves-tigate factors that influence the predictions basedon the Dutch tweets.In this paper we present the results of our exper-iments.
We first summarize related work in Sec-tion 2.
Then we outline our data collection pro-cess (Section 3).
The methods we used for pre-dicting election results and the obtained results,are presented in Sections 4, 5 and 6.
We discussthe results of the experiments in Section 7 andconclude in Section 8.2 Related workTumasjan et al (2010) investigate how Twitteris used in political discourse and check if polit-ical sentiment on Twitter reflects real-life senti-ments about parties and politicians.
As a part oftheir study, they compare party mentions on Twit-ter with the results of the 2009 German parliamentelection.
They conclude that the relative numberof tweets mentioning a party is a good predictorfor the number of votes of that party in an elec-tion.
A similar finding was earlier reported byJean Ve?ronis in a series of blogposts: the number53Figure 1: Overview of our collection of Dutch tweets of the year 2011.
The data set contains almost 700 milliontweets.
Both the number of tweets (about two million per day) and the number of unique users (about onemillion) increase almost every month.
The collection is estimated to contain about 37% of the total volume ofDutch tweets.of times a French presidential candidate was men-tioned in the press was a good prediction for his orher election results (Ve?ronis, 2007).
This predic-tion task involved only two candidates, so it waseasier than predicting the outcome of a multipartyelection.Jungherr et al (2011) criticize the work of Tu-masjan et al (2010).
They argue that the choiceof included parties in the evaluation was not wellmotivated and show that the inclusion of a seventhparty, the Pirate Party, would have had a large neg-ative effect on accuracy of the predictions.
Fur-thermore, Jungherr et al question the time periodwhich was used by Tumasjan et al for collectingthe tweets and show that including the tweets ofthe week right before the election would also havehad a significant negative effect on the predictionaccuracy.Using Twitter data for predicting election re-sults was popular in 2010 and 2011.
Chungand Mustafaraj (2011) found that merely count-ing tweets is not enough to obtain good predic-tions and measure the effect of sentiment analysisand spam filtering.
O?Connor et al (2010) dis-covered that while volumes of mentions of obamaon Twitter before the US presidential election of2008 correlated with high poll ratings for BarackObama, volumes of mentions of his rival mccainalso correlated with high poll ratings of the elec-tion winner.
Gayo-Avello et al (2011) show thatpredictions based on Twitter only predicted halfof the winners of US congressional elections withtwo candidates correctly, a performance which isnot better than chance.3 Data collectionWe collect Dutch Twitter messages (tweets) withthe filter stream provided by Twitter.
We continu-ously search for messages that contain at least oneof a list of about a hundred high-frequent Dutchwords and a dozen frequent Dutch subject tags(hashtags).
The results of this process also con-tain some false positives: tweets that contain ap-parent Dutch words but are actually written in an-other language.
In order to get rid of these mes-sages, we apply a language guesser developed byThomas Mangin (Mangin, 2007).
It ranks lan-guages by comparing character n-grams of an in-put text to n-gram models of texts in known lan-guages.
We use a set of 74 language models de-veloped by our students in 2007.In order to estimate the coverage of our selec-tion with respect to all tweets in Dutch, we col-lected all tweets of one month from 1,017 ran-domly selected users which predominantly postmessages in Dutch.
We compared the two datastreams and found that the first contained 37% ofthe data found in the second.
This suggests thatwe collect about 37% of all Dutch tweets.
Ourdata collection process contains two filters: one isbased on a word list and the other is the languageguesser.
The first filter lost 62% of the data whilethe second lost another 1%.54Short Long Seats Seats Seats AverageParty name name Total Twitter PB MdH pollsPVV 2226 1 2227 18 12 12 12VVD 1562 0 1562 13 14 16 15CDA 1504 0 1504 12 9 10 9.5PvdA 1056 1 1057 9 13 13 13SP 839 0 839 7 8 7 7.5GL 243 505 748 6 5 3 4D66 610 0 610 5 6 5 5.5CU 159 79 238 2 3 3 3PvdD 103 51 154 1 1 1 1SGP 139 0 139 1 2 2 250+ 6 43 49 0 1 2 1.5OSF - - - 1 1 1 1offset 21 4 4 -Table 1: Frequencies of tweets mentioning one of 11 main political parties from one day, Wednesday 16 February2011, converted to Senate seats (column Seats Twitter) and compared with the predictions of two polls from thesame week: from Politieke Barometer of 17 February (Synovate.nl, 2011b) and from Maurice de Hond of 15February (Peil.nl, 2011b).
The offset value is the sum of the differences between the Twitter predictions and theaverage poll predictions.
The OSF group is a cooperation of 11 local parties which were not tracked on Twitter.4 Counting party namesThe Dutch Senate elections are held once ev-ery four years.
The elections are preceded bythe Dutch Provincial Election in which the vot-ers choose 566 representatives for the States-Provincial.
Three months later the new repre-sentatives elect the new Senate.
In the secondelection, each of the representatives has a weightwhich is proportional to the number of peoplehe or she represents.
The 2011 Dutch provincialelections were held on Wednesday 2 March 2011and the corresponding Senate elections were heldon Monday 23 May 2011.
In the Senate elections75 seats are contested.Our work on predicting the results of this elec-tion was inspired by the work of Tumasjan et al(2010), who report that basic counts of tweetsmentioning a political party provided good pre-dictions for the results of the 2009 German parlia-ment election.
We decided to replicate their workfor the Dutch Senate Elections of 2011.We started with examining the Dutch tweets ofWednesday 16 February 2011, two weeks priorto the Provincial elections.
This data set con-sisted of 1.7 million tweets.
From this data setwe extracted the tweets containing names of po-litical parties.
This resulted in 7,000 tweets.
Thisnumber was lower than we had expected.
Origi-nally we had planned to use the tweets for predict-ing local election results.
However, further filter-ing of the tweets to require location informationwould have left us with a total of about 70 polit-ical tweets per day, far too few to make reliablepredictions for twelve different provinces.In the data, we searched for two variants ofeach party: the abbreviated version and the fullname, allowing for minor punctuation and capi-talization variation.
For nearly all parties, the ab-breviated name was used more often on Twitterthan the full name.
The two exceptions are Groen-Links/GL and 50Plus/50+ (Table 1).
Party namescould be identified with a precision close to 100%except for the party ChristenUnie: its abbreviationCU is also used as slang for see you.
This was thecase for 11% of the tweets containing the phraseCU.
In this paper, the 11% of tweets have alreadybeen removed from the counts of this party.Apart from the eleven regular parties shown inTable 1, there was a twelfth party with a chanceof winning a Senate seat: the Independent SenateGroup (OSF), a cooperation of 11 regional par-55ties.
These parties occur infrequently in our Twit-ter data (less than five times per party per day),too infrequent to allow for a reliable base for pre-dicting election results.
Therefore we decided touse a baseline prediction for them.
We assumedthat the group would win exactly one Senate seat,just like in the two previous elections.We converted the counts of the party names onTwitter to Senate seats by counting every tweetmentioning a party name as a vote for that party.The results can be found in the column Seats Twit-ter in Table 1.
The predicted number of seatswere compared with the results of two polls of thesame week: one by the polling company PolitiekeBarometer of 17 February (Synovate.nl, 2011b)and another from the company Peil.nl, commonlyreferred to as Maurice de Hond, from 15 February(Peil.nl, 2011b).
The predicted numbers of seatsby Twitter were reasonably close to the numbersof the polling companies.
However, there is roomfor improvement: for the party PVV, tweets pre-dicted a total of 18 seats while the polling com-panies only predicted 12 and for the party 50+,Twitter predicted no seats while the average of thepolling companies was 1.5 seats.5 Normalizing party countsThe differences between the Twitter predictionand prediction of the polling companies couldhave been caused by noise.
However, the differ-ences could also have resulted from differencesbetween the methods for computing the predic-tions.
First, in the polls, like in an election, every-one has one vote.
In the tweet data set this is notthe case.
One person may have send out multipletweets or may have tweeted about different politi-cal parties.
This problem of the data is easy to fix:we can keep only one political tweet per user inthe data set and remove all others.A second problem is that not every messagecontaining a party name is necessarily positiveabout the party.
For example:Wel triest van de vvd om de zondagennu te schrappen wat betreft het shop-pen, jammer!
Hierbij dus een #failSadly, the VVD will ban shopping onSundays, too bad!
So here is a #failOne party One tweet BothParty per tweet per user constraintsPVV 22 17 19VVD 12 13 13CDA 12 12 12PvdA 8 8 8SP 6 8 7GL 6 7 7D66 5 5 5CU 1 2 2PvdD 1 1 1SGP 1 1 050+ 0 0 0OSF 1 1 1offset 29 22 25Table 2: Senate seat predictions based on normalizedtweets: keeping only tweets mentioning one party,keeping only the first tweet of each user and keeping ofeach user only the first tweet which mentioned a singleparty.
The offset score is the seat difference betweenthe predictions and the average poll prediction of Ta-ble 1.While the tweet is mentioning a political party,the sender does not agree with the policy of theparty and most likely will not vote for the party.These tweets need to be removed as well.A third problem with the data is that the demo-graphics of Dutch Twitter users are probably quitedifferent from the demographics of Dutch voters.Inspection of Dutch tweets revealed that Twitter isvery popular among Dutch teens but they are noteligible to vote.
User studies for other countrieshave revealed that senior citizens are underrepre-sented on the Internet (Fox, 2010) but this grouphas a big turnout in elections (Epskamp and vanRhee, 2010).
It would be nice if we could as-sign weights to tweets based on the representa-tiveness of certain groups of users.
Unfortunatelywe cannot determine the age and gender of indi-vidual Twitter users because users are not requiredto specify this information in their profile.Based on the previous analysis, we tested twonormalization steps for the tweet data.
First, weremoved all tweets that mentioned more than oneparty name.
Next, we kept only the first tweet ofeach user.
Finally we combined both steps: keep-56ing of each user only the first tweet which men-tioned a single political party.
We converted allthe counts to party seats and compared them withthe poll outcomes.
The results can be found inTable 2.
The seat predictions did not improve.
Infact, the offsets of the three methods proved to belarger than the corresponding number of the base-line approach without normalization (29, 25 and22 compared to 21).
Still, we believe that normal-ization of the tweet counts is a good idea.Next, we determined the sentiments of thetweets.
Since we do not have reliable automaticsentiment analysis software for Dutch, we de-cided to build a corpus of political tweets withmanual sentiment annotation.
Each of the two au-thors of this paper manually annotated 1,678 po-litical tweets, assigning one of two classes to eachtweet: negative towards the party mentioned inthe tweet or nonnegative.
The annotators agreedon the sentiment of 1,333 tweets (kappa score:0.59).We used these 1,333 tweets with unanimousclass assignment for computing sentiment scoresper party.
We removed the tweets that mentionedmore than one party and removed duplicate tweetsof users that contributed more than one tweet.
534nonnegative tweets and 227 negative tweets wereleft.
Then we computed weights per party by di-viding the number of nonnegative tweets per partyby the associated total number of tweets.
For ex-ample, there were 42 negative tweets for the VVDparty and 89 nonnegative, resulting in a weight of89/(42+89) = 0.68.
The resulting party weightscan be found in Table 3.We multiplied the weights with the tweetcounts obtained after the two normalization stepsand converted these to Senate seats.
As a resultthe difference with the poll prediction droppedfrom 25 to 23 (see Table 3).
Incorporating sen-timent analysis improved the results of the pre-diction.After sentiment analysis, the tweets still did notpredict the same number of seats as the polls forany party.
For nine parties, the difference wastwo and a half seats or lower but the differencewas larger for two parties: GL (5) and PvdA (6).A possible cause for these differences is a mis-match between the demographics of Twitter usersTweet Sentiment SeatsParty count weight TwitterPVV 811 0.49 13VVD 552 0.68 13CDA 521 0.70 12PvdA 330 0.69 7SP 314 0.90 9GL 322 0.81 9D66 207 0.94 6CU 104 0.67 2PvdD 63 1.00 2SGP 39 0.86 150+ 17 0.93 0OSF - - 1offset 23Table 3: Sentiment weights per party resulting froma manual sentiment analysis, indicating what fractionof tweets mentioning the party is nonnegative and theresulting normalized seat predictions after multiplyingtweet counts with these weights.
The second columncontains the number of tweets per party after the nor-malization steps of Table 2.and the Dutch population.
We have no data de-scribing this discrepancy.
We wanted to build amodel for this difference so we chose to model thedifference by additional correction weights basedon the seats differences between the two predic-tions.
We based the expected number of seats onthe two poll results of the same time period asthe tweets (Synovate.nl, 2011b; Peil.nl, 2011b).For example, after normalization, there were 811tweets mentioning the PVV party.
The party has asentiment weight of 0.49 so the adjusted numberof tweets is 0.49*811 = 397.
The polls predicted12 of 74 seats for this party.
The associated pop-ulation weight is equal to the average number ofpoll seats divided by the total number of seats di-vided by the adjusted number of tweets dividedby the total number of adjusted tweets (2,285):(12/74)/(397/2285) is 0.93.The population weights can be found in Table4.
They corrected most predicted seat numbersof Twitter to the ones predicted by the polls.
Adrawback of this approach is that we have tunedthe prediction system to the results of polls ratherthan to the results of elections.
It would have been57Population Seats AverageParty weight Twitter pollsPVV 0.93 12 12VVD 1.23 15 15CDA 0.80 10 9.5PvdA 1.76 13 13SP 0.82 8 7.5GL 0.47 4 4D66 0.87 5 5.5CU 1.33 3 3PvdD 0.49 1 1SGP 1.84 2 250+ 2.93 1 1.5OSF - 1 1offset 2 -Table 4: Population weights per party resulting fromdividing the percentage of the predicted poll seats(Synovate.nl, 2011b; Peil.nl, 2011b) by the percent-age of nonnegative tweets (Table 3), and the associatedseat predictions from Twitter, which are now closer tothe poll predictions.
Offsets are measured by compar-ing with the average number of poll seats from Table 1.better to tune the system to the results of past elec-tions but we do not have associated Twitter datafor these elections.
Adjusting the results of thesystem to get them as close to the poll predictionsas possible, is the best we can do at this moment.6 Predicting election outcomesThe techniques described above were applied toDutch political tweets collected in the week be-fore the election: 23 February 2011 ?
1 March2011: 64,395 tweets.
We used a week of datarather than a day because we expected that usingmore data would lead to better predictions.
Wechose for a week of tweets rather than a monthbecause we assumed that elections were not animportant discussion topic on Twitter one monthbefore they were held.After the first two normalization steps, oneparty per tweet and one tweet per user, 28,704tweets were left.
The parties were extracted fromthe tweets, and counted, and the counts were mul-tiplied with the sentiment and population weightsand converted to Senate seats.
The results areshown in Table 5 together with poll predictionsSeats Seats SeatsParty Result PB MdH TwitterVVD 16 14 16 14PvdA 14 12 11 16CDA 11 9 9 8PVV 10 11 12 10SP 8 9 9 6D66 5 7 5 8GL 5 4 4 3CU 2 3 3 350+ 1 2 2 2SGP 1 2 2 2PvdD 1 1 2 2OSF 1 1 0 1offset - 14 14 18Table 5: Twitter seat prediction for the 2 March 2011Dutch Senate elections compared with the actual re-sults (Kiesraad.nl, 2012a) and the predictions of twopolling companies of 1 March 2011: PB: PolitiekeBarometer (Synovate.nl, 2011a) and MdH: Maurice deHond (Peil.nl, 2011a).
(Synovate.nl, 2011a; Peil.nl, 2011a) and the re-sults of the elections of 2 March 2011 (Kies-raad.nl, 2012a).The seat numbers predicted by the tweets wereclose to the election results.
Twitter predictedthe correct number of seats for the party PVVwhile the polling companies predicted an in-correct number.
However the companies pre-dicted other seat numbers correctly and they hada smaller total error: 14 seats compared to 18 forour approach.In Dutch elections, there is no strict linear rela-tion between the number of votes for a party andthe number seats awarded to a party.
Seats thatremain after truncating seat numbers are awardedto parties by a system which favors larger par-ties (Kiesraad.nl, 2012b).
Furthermore, in 2011there was a voting incident in the Senate electionswhich caused one party (D66) to loose one of itsseats to another party (SP).
In our evaluation wehave compared seat numbers because that is theonly type of data that we have available from thepolling companies.
The election results allow acomparison based on percentages of votes.
Thiscomparison is displayed in Table 6.58Party Result Twitter offsetVVD 19.6% 17.3% -2.3%PvdA 17.3% 20.8% +3.5%CDA 14.1% 11.0% -3.1%PVV 12.4% 13.3% +0.9%SP 10.2% 8.5% -1.7%D66 8.4% 10.1% +1.7%GL 6.3% 4.8% -1.5%CU 3.6% 4.0% +0.4%50+ 2.4% 3.1% +0.7%SGP 2.4% 3.1% +0.7%PvdD 1.9% 2.7% +0.8%OSF 1.4% 1.3% -0.1%offset - 17.4%Table 6: Twitter vote prediction for the 2 March 2011Dutch Provincial elections compared with the actualresults in percentages2.With the exception of the three largest par-ties, all predicted percentages are within 1.7%of the numbers of the election.
The percentagesmight prove to be more reliable than seat num-bers as a base for a election prediction method.We hope to use percentage figures when the pre-dicting the outcome of next parliament elections:one of the polling companies publishes such fig-ures with their predictions of parliament elections.7 DiscussionAlthough we are happy about the accuracy ob-tained by the Twitter predictions, we have someconcerns about the chosen approach.
In Table 4,we introduced poll-dependent weights to correctthe demographic differences between the Twitterusers and the Dutch electorate.
This was neces-sary because we did not have information aboutthe demographics of Twitter users, for exampleabout their gender and age.
As already men-tioned, this choice led to tuning the system topredicting poll results rather than election results.But do the population weights not also minimizethe effect that tweet counts have on the predic-tions?
Does the system still use the tweet counts2CU and SGP were awarded an additional 0.3% and 0.2%for the 0.5% they won as an alliance.Seats PopulationParty Result Twitter weightVVD 16 16 2.23PvdA 14 13 1.93CDA 11 10 1.41PVV 10 12 1.78SP 8 7 1.11D66 5 5 0.82GL 5 4 0.59CU 2 3 0.4550+ 1 1 0.22SGP 1 2 0.30PvdD 1 1 0.15OSF 1 1 -offset - 8Table 7: Seat prediction for the 2 March 2011 DutchSenate elections based on an uniform distribution oftweets mentioning political parties.for the election prediction?In order to answer the latter question, we de-signed an additional experiment.
Suppose thetweets per party were uniformly distributed suchthat each party name appeared in the same numberof tweets each day.
This would make tweet countsuninteresting for predicting elections.
However,how would our system deal with this situation?The results of this experiment are shown in Ta-ble 7.Since we did not have data to base sentimentweights on, we assumed that all the sentimentweights had value 1.0.
Since the tweet countswere different from those in the earlier exper-iments, we needed to compute new populationweights (see Table 7).
The seat numbers predictedby the system were equal to the average of the seatnumbers of the two polls in Table 4 plus or mi-nus a half in case the two numbers added up toan odd number.
The VVD party gained one seat,as a consequence of the system of awarding re-mainder seats to larger parties.
We assume thatthe tweet distribution will be uniform at all timesand this means that the system will always predictthe seat distribution.
The offset of the new predic-tion was 3 seats for the test distribution of Table 4and 8 seats for the election results (see Table 7), a59smaller error than either of the polling companies(compare with Table 5).This experiment has produced a system whichgenerates the average of the predictions of thetwo polling companies from the week of 16/17February as an election prediction.
It does not re-quire additional input.
This is not a good methodfor predicting election outcome but by chance itgenerated a better prediction than our earlier ap-proach and those of two polling companies.
Weare not sure what conclusions to draw from this.Is the method of using population weights flawed?Is our evaluation method incorrect?
Are tweetsbad predictors of political sentiment?
Is the mar-gin of chance error large?
It would be good to testwhether the measured differences are statisticallysignificant but we do not know how to do that forthis data.8 Concluding remarksWe have collected a large number of Dutch Twit-ter messages (hundreds of millions) and showedhow they can be used for predicting the results ofthe Dutch Senate elections of 2011.
Counting thetweets that mention political parties is not suffi-cient to obtain good predictions.
We tested theeffects of improving the quality of the data col-lection by removing certain tweets: tweets men-tioning more than one party name, multiple tweetsfrom a single user and tweets with a negative sen-timent.
Despite having no gold standard trainingdata, the total error of our final system was only29% higher than that of two experienced pollingcompanies (Table 5).
We hope to improve theseresults in the future, building on the knowledgewe have obtained in this study.AcknowledgementsWe would like to thank the two reviewers of thispaper for valuable comments.ReferencesJessica Chung and Eni Mustafaraj.
2011.
Cam col-lective sentiment expressed on twitter predict po-litical elections?
In Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence.San Francisco, CA, USA.Martijn Epskamp and Marn van Rhee.
2010.
Analyseopkomst gemeenteraadsverkiezingen 2010.Susannah Fox.
2010.
Four in ten seniors go online.Pew Research Center, http://www.pewinternet.org/Commentary/2010/January/38-of-adults-age-65-go-online.aspx (Retrieved 8 March 2012).Daniel Gayo-Avello, Panagiotis Metaxas, and EniMustafaraj.
2011.
Limits of electoral predictionsusing social media data.
In Proceedings of the In-ternational AAAI Conference on Weblogs and So-cial Media.
Barcelona, Spain.Andreas Jugherr, Pascal Ju?rgens, and Harald Schoen.2011.
Why the pirate party won the german elec-tion of 2009 or the trouble with predictions: A re-sponse to tumasjan, a., sprenger, t. o., sander, p.g., & welpe, i. m. ?predicting elections with twit-ter: What 140 characters reveal about political sen-timent?.
Social Science Computer Review.Kiesraad.nl.
2012a.
Databank verkiezingsuitslagen.http://www.verkiezingsuitslagen.nl/Na1918/Verkie-zingsuitslagen.aspx?VerkiezingsTypeId=2 (retrie-ved 27 February 2012).Kiesraad.nl.
2012b.
Toewijzing zetels.
http://www.kiesraad.nl/nl/Onderwerpen/Uitslagen/Toewijzingzetels.html (retrieved 27 February 2012).Thomas Mangin.
2007. ngram: Textcat implementa-tion in python.
http://thomas.mangin.me.uk/.Brendan O?Connor, Ramnath Balasubramanyan,Bryan R. Routledge, and Noah A. Smith.
2010.From tweets to polls: Linking text sentiment topublic opinion time series.
In Proceedings of theInternational AAAI Conference on Weblogs andSocial Media.
Washington DC, USA.Peil.nl.
2011a.
Nieuw haags peil 1 maart 2011.http://www.peil.nl/?3182 (retrieved 5 March 2012).Peil.nl.
2011b.
Nieuw haags peil 15 februari 2011.http://www.peil.nl/?3167 (retrieved 1 March 2012).Synovate.nl.
2011a.
Nieuws 2011 - peiling eerstekamer - week 9. http://www.synovate.nl/con-tent.asp?
targetid=721 (retrieved 5 March 2012).Synovate.nl.
2011b.
Peiling eerste kamer - week 7.http://www.synovate.nl/content.asp?targetid=713(retrieved 5 March 2012).Andranik Tumasjan, Timm Sprenger, Philipp Sandner,and Isabell Welpe.
2010.
Predicting elections withtwitter: What 140 characters reveal about politicalsentiment.
In Proceedings of the Fourth AAAI con-ference on Weblogs and Social Media, pages 178?185.Jean Ve?ronis.
2007.
2007: La presse fait a?
nouveaumieux que les sondeurs.
http://blog.veronis.fr/2007/05/2007-la-presse-fait-nouveau-mieux-que.html.60
