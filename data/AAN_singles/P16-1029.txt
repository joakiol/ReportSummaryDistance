Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 301?310,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsSentiment Domain Adaptation with Multiple SourcesFangzhao Wu and Yongfeng Huang?Tsinghua National Laboratory for Information Science and TechnologyDepartment of Electronic EngineeringTsinghua University, Beijing, Chinawufangzhao@gmail.com, yfhuang@tsinghua.edu.cnAbstractDomain adaptation is an important re-search topic in sentiment analysis area.Existing domain adaptation methods usu-ally transfer sentiment knowledge fromonly one source domain to target do-main.
In this paper, we propose a newdomain adaptation approach which canexploit sentiment knowledge from mul-tiple source domains.
We first extrac-t both global and domain-specific senti-ment knowledge from the data of multi-ple source domains using multi-task learn-ing.
Then we transfer them to target do-main with the help of words?
sentimen-t polarity relations extracted from the un-labeled target domain data.
The similar-ities between target domain and differentsource domains are also incorporated intothe adaptation process.
Experimental re-sults on benchmark dataset show the ef-fectiveness of our approach in improvingcross-domain sentiment classification per-formance.1 IntroductionSentiment classification is a hot research topic innatural language processing field, and has manyapplications in both academic and industrial ar-eas (Pang and Lee, 2008; Liu, 2012; Wu et al,2015; Wu and Huang, 2016).
Sentiment classi-fication is widely known as a domain-dependenttask (Blitzer et al, 2007; Glorot et al, 2011).
Thesentiment classifier trained in one domain may notperform well in another domain.
This is becausesentiment expressions used in different domain-s are usually different.
For example, ?boring?
?Corresponding author.and ?lengthy?
are frequently used to express neg-ative sentiments in Book domain.
However, theyrarely appear in Electronics domain (Bollegala etal., 2011).
Thus a sentiment classifier trained inElectronics domain cannot accurately predict theirsentiments in Book domain.
In addition, the sameword may convey different sentiments in differen-t domains.
For example, in Electronics domain?easy?
is usually used in positive reviews, e.g.,?this digital camera is easy to use.?
However, itis frequently used as a negative word in Moviedomain.
For instance, ?the ending of this movieis easy to guess.?
Thus, the sentiment classifiertrained in one domain usually cannot be appliedto another domain directly (Pang and Lee, 2008).In order to tackle this problem, sentiment do-main adaptation has been widely studied (Liu,2012).
For example, Blitzer et al (2007) pro-posed to compute the correspondence among fea-tures from different domains using their associa-tions with pivot features based on structural corre-spondence learning (SCL).
Pan et al (2010) pro-posed a spectral feature alignment (SFA) algorith-m to align the domain-specific words from differ-ent domains in order to reduce the gap betweensource and target domains.
However, all of thesemethods transfer sentiment information from onlyone source domain.
When the source and targetdomains have significant difference in feature dis-tributions, the adaptation performance will heav-ily decline.
In some cases, the performance ofsentiment domain adaptation is even worse thanthat without adaptation, which is usually knownas negative transfer (Pan and Yang, 2010).In this paper we propose a new domain adapta-tion approach for cross-domain sentiment classi-fication.
Our approach can exploit the sentimen-t information in multiple source domains to re-duce the risk of negative transfer effectively.
Ourapproach consists of two steps, i.e., training and301adaptation.
At the training stage, we extract twokinds of sentiment models, i.e., the global mod-el and the domain-specific models, from the da-ta of multiple source domains using multi-tasklearning.
The global sentiment model can capturethe common sentiment knowledge shared by var-ious domains, and has better generalization per-formance than the sentiment model trained in asingle source domain.
The domain-specific sen-timent model can capture the specific sentimentknowledge in each source domain.
At the adap-tation stage, we transfer both kinds of sentimentknowledge to target domain with the help of thewords?
sentiment graph of target domain.
The sen-timent graph contains words?
domain-specific sen-timent polarity relations extracted from the syntac-tic parsing results of the unlabeled data in targetdomain.
Since sentiment transfer between similardomains is more effective than dissimilar domains,we incorporate the similarities between target do-main and different source domains into the adap-tation process.
In order to estimate the similaritybetween two domains, we propose a novel domainsimilarity measure based on their sentiment graph-s. Extensive experiments were conducted on thebenchmark Amazon product review dataset.
Theexperimental results show that our approach canimprove the performance of cross-domain senti-ment classification effectively.2 Related workSentiment classification is widely known as adomain-dependent task, since different expres-sions are used to express sentiments in differentdomains (Blitzer et al, 2007).
The sentiment clas-sifier trained in one domain may not perform wellin another domain.
Since there are massive do-mains, it is impractical to annotate enough datafor each new domain.
Thus, domain adaptation,or so called cross-domain sentiment classification,which transfers the sentiment knowledge from do-mains with sufficient labeled data (i.e., source do-main) to a new domain with no or scarce labeleddata (i.e., target domain), has been widely stud-ied.
Existing domain adaptation methods main-ly transfer sentiment information from only onesource domain.
For example, Blitzer et al (2007)proposed a domain adaptation method based onstructural correspondence learning (SCL).
In theirmethod, a set of pivot features are first selected ac-cording to their associations with source domainlabels.
Then the correspondence among featuresfrom source and target domains is computed usingtheir associations with pivot features.
In order toreduce the gap between source and target domain-s, Pan et al (2010) proposed a spectral featurealignment (SFA) algorithm to align the domain-specific sentiment words from different domainsinto clusters.
He et al (2011) proposed to extractpolarity-bearing topics using joint sentiment-topic(JST) model to expand the feature representation-s of texts from both source and target domain-s. Li et al (2009) proposed to transfer sentimentknowledge from source domain to target domainusing nonnegative matrix factorization.
A com-mon shortcoming of above methods is that if thesource and target domains have significantly dif-ferent distributions of sentiment expressions, thenthe domain adaptation performance will heavilydecline (Li et al, 2013).Using multiple source domains in cross-domainsentiment classification has also been explored.Glorot et al (2011) proposed a sentiment domainadaptation method based on a deep learning tech-nique, i.e., Stacked Denoising Auto-encoders.
Thecore idea of their method is to learn a high-levelrepresentation that can capture generic conceptsusing the unlabeled data from multiple domains.Yoshida et al (2011) proposed a probabilistic gen-erative model for cross-domain sentiment classi-fication with multiple source and target domains.In their method, each word is assigned three at-tributes, i.e., the domain label, the domain depen-dence/independence label, and sentiment polari-ty.
Bollegala et al (2011) proposed to constructa sentiment sensitive thesaurus for cross-domainsentiment classification using data from multiplesource domains.
This thesaurus is used to expandthe feature vectors for both training and classifi-cation.
However, the similarities between targetdomain and different source domains are not con-sidered in these methods.
In addition, although un-labeled data is utilized in these methods, the usefulword-level sentiment knowledge in the unlabeledtarget domain data is not exploited.General-purpose multiple source domain adap-tation methods have also been studied.
For ex-ample, Mansour et al (2009) proposed a distribu-tion weighted hypothesis combination approach,and gave theoretical guarantees for it.
However,this method is based on the assumption that tar-get distribution is some mixture of source distri-302butions, which may not hold in sentiment domainadaptation scenario.
Duan et al (2009) proposeda Domain Adaptation Machine (DAM) method tolearn a Least-Squares SVM classifier for target do-main by leveraging the classifiers independentlytrained in multiple source domains.
Chattopad-hyay et al (2011) explored to assign psuedo label-s to unlabeled samples in the target domain usingthe classifiers from multiple source domains.
Thentarget domain classifier is trained on these psuedolabeled samples.
Compared with these general-purpose domain adaptation methods with multi-ple source domains, our approach is more suit-able for sentiment domain adaptation because ourapproach exploits more sentiment-related charac-teristics and knowledge, such as the general senti-ment knowledge shared by different domains andthe word-level sentiment polarity relations, whichis validated by experiments.3 Sentiment Graph Extraction andDomain Similarity MeasureIn this section we introduce two important com-ponents used in our sentiment domain adaptationapproach, i.e., the words?
sentiment graph and do-main similarity.3.1 Sentiment Graph ExtractionCompared with labeled data, unlabeled data isusually much easier and cheaper to collect on alarge scale.
Although unlabeled samples are notassociated with sentiment labels, they can stil-l provide a lot of useful sentiment information fordomain adaptation.
For example, if ?great?
and?quick?
are frequently used to describe the sametarget in the same review of Kitchen domain, thenthey probably convey the same sentiment polarityin this domain.
Since ?great?
is a general positiveword in both Book and Kitchen domains, we caninfer that ?quick?
is also a positive word in Kitchendomain when transferring from Book domain to K-itchen domain.Motivated by above observations, in this paperwe propose to extract sentiment polarity relationsamong words from massive unlabeled data for sen-timent domain adaptation.
Two kinds of polarityrelations are explored, i.e., sentiment coherent re-lation and sentiment opposite relation.
The formermeans that two words convey the same sentimentpolarity while the latter indicates opposite senti-ment polarities.
These polarity relations are ex-durablesmallbatteryThe cameraof thisnotconj_andnsubjneg det nmod_ofcase det  smalldurableoppositensubjFigure 1: An illustrative example of extractingsentiment polarity relations from syntactic parsingresults.tracted from the syntactic parsing results accord-ing to manually selected rules.
Two rules are usedto extract sentiment coherent relations.
The firstone is that two words are connected by coordi-nating conjunctions such as ?and?
and ?as wellas?.
For example, a review in Kitchen domain maybe ?it is so high-quality and professional.?
Since?high-quality?
and ?professional?
are connectedby the coordinating conjunction ?and?, we inferthat they probably convey the same sentiment po-larity.
The second rule is that two words are not di-rectly connected but are used to describe the sametarget in the same sentence.
For example, a re-view in Electronics domain may be ?It is a beau-tiful, durable, easy-to-use camera.?
Since ?beauti-ful?, ?durable?, and ?easy-to-use?
are all used todescribe the same camera in the same review, theytend to convey the same sentiment polarity.
We al-so propose two rules for extracting sentiment op-posite relations.
The first rule is that two wordsare connected by adversative conjunctions such as?but?
and ?however?.
The second rule is that twowords are connected by coordinating conjunctionsbut there is a negation symbol before one of them.For example, a review may be ?The battery of thiscamera is small and not durable.?
We can inferthat ?small?
and ?durable?
may convey oppositesentiments when they are used to describe cam-era battery.
An illustrative example of extractingsentiment polarity relations from syntactic parsingresults is shown in Fig.
1.Based on the sentiment polarity relations amongwords extracted from the unlabeled data, we canbuild a words?
sentiment graph for each domain.The nodes of the sentiment graph represent wordsand the edges stand for sentiment polarity relation-s. We denote R ?
RD?Das the words?
sentimen-t graph of a specific domain.
Ri,jrepresents thesentiment polarity relation score between words i303and j.
In this paper we define Ri,jasnCi,j?nOi,jnCi,j+nOi,j,where nCi,jand nOi,jrepresent the frequencies ofwords i and j sharing coherent and opposite senti-ment polarity relations respectively in all the unla-beled samples.
Thus, Ri,j?
[?1, 1].
If Ri,j> 0,then words i and j tend to convey the same sen-timent polarity.
Similarly, if Ri,j< 0, then thesetwo words are more likely to convey opposite sen-timents.
The absolute value of Ri,jrepresents theconfidence of this sentiment polarity relation.3.2 Domain SimilarityDifferent pairs of domains have different senti-ment relatedness (Remus, 2012; Wu and Huang,2015).
Researchers have found that sentiment do-main adaptation between similar domains, such asKitchen and Electronics, is much more effectivethan that between dissimilar domains, such as K-itchen and Book (Blitzer et al, 2007; Pan et al,2010).
Thus, it is beneficial if we take the similar-ity between source and target domains into consid-eration when transferring sentiment knowledge.In this paper we explore two methods to mea-sure domain similarity.
The first one is basedon term distribution.
The assumption behind thismethod is that similar domains usually share morecommon terms than dissimilar domains.
For ex-ample, Smart Phone and Digital Camera domainsshare many common terms such as ?screen?, ?bat-tery?, ?light?, and ?durable?, while the term dis-tributions of Digital Camera and Book domainsmay have significant difference.
Term distribu-tion based domain similarity measures, such asA-distance, have been explored in previous work-s (Blitzer et al, 2007).
Inspired by (Remus, 2012),here we apply Jensen-Shannon divergence to mea-sure domain similarity based on term distribution-s, which is more easy to compute thanA-distance.Denote tm?
RD?1as the term distribution of do-main m, where tmwis the probability of term w ap-pearing in domain m. Then the similarity betweendomains m and n is formulated as:TermSim(m,n) = 1?DJS(tm, tn)= 1?12(DKL(tm, t) +DKL(tn, t)),(1)where t =12(tm+ tn) is the average distribution,and DKL(?, ?)
is the Kullback-Leibler divergence:DKL(p,q) =D?i=1pilog2(piqi).
(2)We can verify that DJS(tm, tn) ?
[0, 1].
Thus,the range of TermSim(m,n) is also [0, 1].The term distribution based domain similaritycan measure whether similar words are used in t-wo domains.
However, sharing similar terms doesnot necessarily mean that sentiment expressionsare used similarly in these domains.
For exam-ple, CPU and Battery are both related to electron-ics.
The word ?fast?
is positive when used to de-scribe CPU.
However, it is frequently used as anegative word in Battery domain.
For example,?this battery runs out fast.?
Thus, it is more usefulto measure domain similarity based on sentimentword distributions.
However, although we can in-fer the sentiment word distributions of source do-mains according to labeled samples, it is difficultto compute the sentiment word distribution of tar-get domain, since the labeled data does not existor is very scarce in target domain.In order to tackle this problem, in this paper wepropose to estimate the similarity between two do-mains based on their sentiment graphs.
Similardomains usually share more common sentimen-t words and sentiment word pairs than dissimilardomains.
In addition, the polarity relation scoresof a pair of words in the sentiment graphs of simi-lar domains are also more similar.
In other words,they tend to be both positive or negative in thesetwo domains.
Motivated by above observations,the domain similarity based on sentiment graph isformulated as follows:SentiSim(m,n) =D?w=1?v 6=w|Rmw,v+Rnw,v| ?Nm?nw,vD?w=1?v 6=w(|Rmw,v| ?Nmw,v+ |Rnw,v| ?Nnw,v),(3)where Rmw,vis the sentiment polarity relation s-core between words w and v in domain m,and Nmw,vis its frequency in this domain.Nm?nw,v= min{Nmw,v, Nnw,v}.
We can verify thatSentiSim(m,n) ?
[0, 1].
If two domains havemore common sentiment word pairs and the po-larity relation scores of these word pairs are moresimilar, then these two domains share higher do-main similarity according to Eq.
(3).4 Sentiment Domain Adaptation withMultiple SourcesIn this section we introduce our sentiment domainadaptation approach in detail.
First we introduceseveral notations that will be used in following dis-cussions.
Assume there are M source domains.304Denote {Xm?
RNm?D,ym?
RNm?1} as thelabeled data in source domain m, where Nmis thenumber of labeled samples and D is the size offeature vector.
xmi?
RD?1is the feature vec-tor of the ithsample in domain m, and its senti-ment label is ymi.
In this paper we focus on senti-ment polarity classification, and ymi?
{+1,?1}.Denote w ?
RD?1as the global sentiment mod-el extracted from multiple source domains andwm?
RD?1as the domain-specific sentimen-t model of source domain m. Denote wt?
RD?1as the domain-specific sentiment model of targetdomain.
Denote f(x, y,w) as the loss of clas-sifying sample x into label y under linear clas-sification model w. Our approach is flexible tothe selection of loss function f , which can besquare loss, logistic loss, and hinge loss.
DenoteRm?
RD?Das the sentiment graph knowledgeof domain m, and Sm,t?
[0, 1] as the similaritybetween source domain m and target domain.Our sentiment domain adaptation with multiplesources approach (SDAMS) consists of two step-s, i.e., training and adaptation.
At the trainingstage, the global and domain-specific sentimen-t knowledge are extracted from the data of mul-tiple source domains.
And at the adaptation stage,these two kinds of sentiment knowledge are trans-ferred to target domain by incorporating the sen-timent graph knowledge of target domain and thesimilarities between target and source domains.4.1 TrainingGiven the labeled data and the sentiment graphknowledge of multiple source domains, at thetraining stage, our goal is to train a robust glob-al sentiment model to capture the general senti-ment knowledge shared by various domains and adomain-specific sentiment model for each sourcedomain.
The model of the training process is mo-tivated by multi-task learning (Evgeniou and Pon-til, 2004; Liu et al, 2009) and is formulated as:arg minw,wmL(w,wm) =M?m=11NmNm?i=1f(xmi, ymi,w +wm)+ ?M?m=1D?i=1?j 6=iRmi,j|(wi+ wm,i)?
(wj+ wm,j)|+ ?1?w?22+ ?1M?m=1?wm?22+ ?2?w?1+ ?2M?m=1?wm?1,(4)where?, ?1, and ?2are nonnegative regularizationcoefficients.
The sentiment classification model ofeach source domain is decomposed into two com-ponents, i.e., a global one and a domain-specificone.
The global sentiment model is shared by al-l source domains and is trained in these domainssimultaneously.
It is used to capture the generalsentiment knowledge, such as the general senti-ment words ?great?, ?worst?, ?perfect?
and so on.The domain-specific sentiment model is trained onthe labeled data within one source domain and isused to capture the specific sentiment knowledgeof this domain.
For example, the domain-specificsentiment word ?easy?
is a positive word in Elec-tronics domain but is used as a negative word inMovie domain.In Eq.
(4), the first term means minimizing theempirical classification loss on the labeled data ofmultiple source domains.
In this way we incorpo-rate the sentiment information in labeled samplesinto sentiment classifier learning.
In the secondterm we incorporate the sentiment graph knowl-edge of each source domain.
It is motivated bygraph-guided fused Lasso (Chen et al, 2012).
Iftwo words have strong coherent (or opposite) sen-timent polarity relations, then we constrain thattheir sentiment scores are more similar (or dissim-ilar) with each other in the final classification mod-el.
The L1-norm regularization terms are motivat-ed by Lasso (Tibshirani, 1996).
It can set manyminor sentiment scores in the models to exact ze-ros.
Since not all the words convey sentiments,these terms can help conduct sentiment word se-lection.
We also incorporate the L2-norm regular-ization terms in order to improve model stability inhigh-dimensional problems, which is inspired byelastic net regularization (Zou and Hastie, 2003).4.2 AdaptationAt the adaptation stage, we incorporate the glob-al sentiment knowledge, the domain-specific sen-timent knowledge of multiple source domains, thesentiment graph knowledge of target domain, andthe domain similarities between target and sourcedomains into a unified framework to learn an ac-curate sentiment classifier for target domain.
Themodel of our adaptation framework is formulatedas follows:argminwtL(wt) =M?m=1Sm,t?wm?wt?22+ ?1?wt?22+ ?2?wt?1+ ?D?i=1?j 6=iRti,j|(wi+ wt,i)?
(wj+ wt,j)|,(5)305where ?, ?1, and ?2are nonnegative regularizationcoefficients.
The final sentiment classifier of thetarget domain is a linear combination of w and wt,i.e., w+wt, where w is the global sentiment mod-el extracted from multiple source domains at thetraining stage, and wtis the domain-specific sen-timent model of target domain learned at the adap-tation stage.
In the first term of Eq.
(5), we transferthe knowledge in domain-specific sentiment mod-els from multiple source domains to wt.
Since thesentiment knowledge transfer between similar do-mains is more effective, the transfer of domain-specific sentiment knowledge is weighted by thesimilarities between target domain and differen-t source domains.
If target domain is more similarwith source domain m than source domain n (i.e.,Sm,t> Sn,t), then more domain-specific senti-ment knowledge will be transferred to wtfromwmthan wn.
Through the last term we incorpo-rate the sentiment graph knowledge extracted frommassive unlabeled data of target domain into theadaptation process.
If two words share strong co-herent (or opposite) sentiment polarity relations inthe target domain, then we constrain that their sen-timent scores in the sentiment classification modelof target domain are more similar (or dissimilar).This term can help transfer the sentiment knowl-edge from source domains to target domain moreeffectively.
For example, if we know that ?great?is a positive word in the global sentiment modeland there is a strong coherent polarity relation be-tween ?easy?
and ?great?
in Electronics domain,then we can infer that ?easy?
is also a positiveword in this domain.5 Experiments5.1 Dataset and Experimental SettingsThe dataset used in our experiments is the fa-mous Amazon product review dataset collected byBlitzer et al (2007).
It is widely used as a bench-mark dataset for cross-domain sentiment classifi-cation.
Four domains, i.e., Book, DVD, Electron-ics and Kitchen, are included in this dataset.
Eachdomain contains 1,000 positive and 1,000 negativereviews.
Besides, a large number of unlabeled re-views are provided.
The detailed statistics of thisdataset are shown in Table 1.In our experiments, each domain was select-ed in turn as target domain, and remaining do-mains as source domains.
In each experiment,we randomly selected N labeled samples from theTable 1: The statistics of the dataset.Domain Book DVD Electronics Kitchenpositive 1,000 1,000 1,000 1,000negative 1,000 1,000 1,000 1,000unlabeled 973,194 122,438 21,009 17,856source domains to train sentiment models at thetraining stage.
These samples were balanced a-mong different source domains.
In order to per-form fair comparisons with baseline methods, fol-lowing (Bollegala et al, 2011), we limited the to-tal number of training samples, i.e., N , to 1,600.The target domain sentiment classifier was test-ed on all the labeled samples of target domain.Following (Blitzer et al, 2007), unigrams and bi-grams were used as features.
The sentiment po-larity relations of bigrams were extracted by ex-panding the polarity relations between unigram-s using modifying relations.
For example, fromthe review ?this phone is very beautiful and notexpensive,?
we extract not only sentiment polari-ty relation between ?beautiful?
and ?expensive?,but also polarity relation between ?beautiful?
and?not expensive?
(coherent sentiment), and that be-tween ?very beautiful?
and ?expensive?
(oppositesentiment), since ?very?
and ?not?
are used tomodify ?beautiful?
and ?expensive?
respectively.Classification accuracy was selected as the eval-uation metric.
We manually set ?
in Eq.
(5) to0.01.
The values of ?, ?1, and ?2in Eq.
(4) wereselected using cross validation.
The optimizationproblems in Eq.
(4) and Eq.
(5) were solved usingalternating direction method of multipliers (ADM-M) (Boyd et al, 2011).
Each experiment was re-peated 10 times independently and average resultswere reported.5.2 Comparison of Domain SimilarityMeasuresIn this section, we conducted experiments to com-pare the effectiveness of the two kinds of domainsimilarity measures introduced in Section 3.2 insentiment domain adaptation task.
The experi-mental results are summarized in Fig.
2.
Theclassification loss function used in our approachis hinge loss.
The results of other loss functionsshow similar patterns.From Fig.
2 we can see that the domain simi-larity measure based on sentiment graph performsconsistently better than that based on term distri-bution in our approach.
This result validates our306Book DVD Electronics Kitchen0.70.720.740.760.780.80.820.840.86AccuracyTermSimSentiSimFigure 2: The performance of our approach withdifferent kinds of domain similarity measure.assumption in Section 3.2 that the sentiment graphbased domain similarity can better model the sen-timent relatedness between different domains thanthat based on term distribution in sentiment do-main adaptation task.
In all the following exper-iments, the sentiment graph based domain similar-ities were used in our approach.5.3 Performance EvaluationIn this section we conducted experiments to evalu-ate the performance of our approach by comparingit with a series of baseline methods.
The meth-ods to be compared are: 1) SCL, domain adap-tation based on structural correspondence learn-ing (Blitzer et al, 2007); 2) SFA, domain adap-tation based on spectral feature alignment (Panet al, 2010); 3) SCL-com and SFA-com, adapt-ing SCL and SFA to multiple source domain sce-nario by first training a cross-domain sentimen-t classifier in each source domain and then com-bining their classification results using majorityvoting; 4) SST, cross-domain sentiment classifi-cation by using multiple source domains to con-struct a sentiment sensitive thesaurus for featureexpansion (Bollegala et al, 2011); 5) IDDIWP,multiple-domain sentiment analysis by identify-ing domain dependent/independent word polari-ty (Yoshida et al, 2011); 6) DWHC, DAM and CP-MDA, three general-purpose multiple source do-main adaptation methods proposed in (Mansour etal., 2009), (Duan et al, 2009) and (Chattopadhyayet al, 2011) respectively; 7) SDAMS-LS, SDAMS-SVM, and SDAMS-Log, our proposed sentimen-t domain adaptation approaches with square loss,hinge loss, and logistic loss respectively; 8) All-Training, all the domains were involved in thetraining phase of our approach and there is noadaptation phase.
This method is introduced toprovide an upper bound for the performance of ourapproach.
The experimental results of these meth-ods are summarized in Table 2.Table 2: The performance of different methods.Book DVD Electronics KitchenSCL 0.7457 0.7630 0.7893 0.8207SFA 0.7598 0.7848 0.7808 0.8210SCL-com 0.7523 0.7675 0.7918 0.8247SFA-com 0.7629 0.7869 0.7864 0.8258SST 0.7632 0.7877 0.8363 0.8518IDDIWP 0.7524 0.7732 0.8167 0.8383DWHC 0.7611 0.7821 0.8312 0.8478DAM 0.7563 0.7756 0.8284 0.8419CP-MDA 0.7597 0.7792 0.8331 0.8465SDAMS-LS 0.7795 0.7880 0.8398 0.8596SDAMS-SVM 0.7786 0.7902 0.8418 0.8578SDAMS-Log 0.7829 0.7913 0.8406 0.8629All-Training 0.7983 0.8104 0.8463 0.8683From Table 2 we can see that our approachachieves the best performance among all the meth-ods compared here.
SCL and SFA are famouscross-domain sentiment classification methods.
Inthese methods, the sentiment knowledge is trans-ferred from one source domain to target domain.According to Table 2, our approach performs sig-nificantly better than them.
This result indicatesthat the sentiment knowledge extracted from onesource domain may contain heavy domain-specificbias and may be inappropriate for the target do-main.
Our approach can tackle this problem by ex-tracting the global sentiment model from multiplesource domains.
This global model can capturethe general sentiment knowledge shared by vari-ous domains and has better generalization perfor-mance.
It can reduce the risk of negative transfereffectively.
Our approach also outperforms SCL-com and SFA-com.
In SCL-com and SFA-com, thesentiment information in different source domain-s is combined at the classification stage, while inour approach it is combined at the learning stage.The superior performance of our approach com-pared with SCL-com and SFA-com shows that ourapproach is a more appropriate way to exploit thesentiment knowledge in different source domain-s. SST and IDDIWP also utilize data from mul-tiple source domains as our approach.
But ourapproach can still outperform them.
This is be-cause in these methods, the similarities betweentarget domain and different source domains arenot considered.
Since different domains usually307easy goodexcellent hard simplequickworth expensivepriceylack reliabledurable1.00.881.01.01.0 1.01.01.0 1.0 -1.0-1.0 1.0-1.0-1.01.0 cheap 0.56Figure 3: An illustrative example of the sentimentgraph of Electronics domain.
The value on the linerepresents the sentiment polarity relation score.have different sentiment relatedness, our approachcan exploit the sentiment information in multi-ple source domains more accurately by incorpo-rating the similarities between target domain andeach source domain into the adaptation process.Our approach also outperforms the state-of-the-art general-purpose multiple source domain adap-tation methods, such as DWHC, DAM, and CP-MDA.
This is because our approach can exploitmore sentiment-related characteristics and knowl-edge for sentiment domain adaptation, such as thegeneral sentiment knowledge shared by variousdomains, the sentiment graph based domain sim-ilarities, and the word-level sentiment polarity re-lations.
Thus, our approach is more suitable forsentiment domain adaptation than these general-purpose multiple source domain adaptation meth-ods.
Another observation from Table 2 is that theperformance of our approach is quite close to theupper bound, i.e., All-Training, especially in Elec-tronics and Kitchen domains.
This result validatesthe effectiveness of our approach in sentiment do-main adaptation.5.4 Case StudyIn this section we conducted several case studies tofurther explore how our sentiment domain adapta-tion approach works.
As an illustrative example,we selected Electronics domain as the target do-main and remaining domains as source domains.The top sentiment words in the global and domain-specific sentiment models learned from the dataof multiple source domains are shown in Table 3.A subgraph of the sentiment graph extracted fromthe unlabeled data of target domain (Electronic-s) is shown in Fig.
3.
The top words in the finaldomain-specific sentiment model of target domainreturned by our approach are shown in Table 3.From Table 3 we have following observations.First, the global sentiment model extracted frommultiple source domains can capture the gener-al sentiment knowledge quite well.
It contain-s many general sentiment words, such as ?excel-lent?, ?great?, ?waste?
and so on.
These generalsentiment words convey strong sentiment orienta-tions.
In addition, their sentiment polarities areconsistent in different domains.
Thus, the glob-al sentiment model extracted from multiple sourcedomains has good generalization ability and ismore suitable for domain adaptation than the sen-timent model trained in a single source domain,which may contain heavy domain-specific senti-ment bias.
Second, the domain-specific sentimentmodels can capture rich specific sentiment expres-sions in each source domain.
For example, ?easy?is a positive word in Kitchen domain while ?re-turn?
is a negative word in this domain.
Third,different domains have different domain-specificsentiment expressions.
For example, ?read?
isfrequently used as a positive word in Book do-main, while it is a negative word in DVD domain.Thus, it is important to separate the global and thedomain-specific sentiment knowledge.
In addi-tion, although different sentiment expressions areused in different domains, similar domains mayshare many common domain-specific sentimen-t expressions.
For example, ?easy?
and ?works?are positive words in both Electronics and Kitchendomains, and ?return?
and ?broken?
are both nega-tive words in them.
Thus, transferring the domain-specific sentiment models from similar source do-mains to target domain is helpful.
From Fig.
3we can see that the sentiment polarity relation-s in the sentiment graph extracted from massiveunlabeled data are reasonable.
Words with pos-itive relation scores tend to convey similar sen-timents, and words with negative relation scoresusually convey opposite sentiments.
In addition,this sentiment graph contains rich domain-specificsentiment information in target domain, which isuseful to transfer the sentiment knowledge frommultiple source domains to target domain.
For ex-ample, ?excellent?, ?easy?, ?simple?, and ?quick?share the same sentiment polarity in Electronic-s domain according to Fig.
3.
We can infer that?easy?
is positive in this domain using the senti-ment of ?excellent?
in the global sentiment mod-el and the sentiment relation between ?easy?
and?excellent?.
Then we can further infer the sen-timents of the domain-specific sentiment words308Table 3: The top words in the global and domain-specific sentiment models.GlobalPositive excellent, great, best, perfect, love, wonderful, the best, loved, well, fantastic, enjoy, favoriteNegative bad, waste, boring, disappointed, worst, poor, disappointing, disappointment, terrible, poorlyBookPositive excellent, wonderful, easy, loved, enjoyable, life, fun, favorite, a must, read, important, novelNegative no, boring, disappointing, bad, instead, waste, little, writing, poorly, pages, unfortunatelyDVDPositive enjoy, hope, loved, season, better than, best, a must, first, superman, classic, times, backNegative worst, boring, bad, the worst, terrible, waste, awful, book, horrible, dull, lame, read, hardKitchenPositive easy, great, perfect, love, works, easy to, best, little, well, good, nice, long, durable, cleanNegative disappointed, back, poor, broken, too, return, off, returned, broke, waste, tried, times, doesn?tElectronicsPositive excellent, great, perfect, best, love, easy to, easy, little, the best, works, good, nice, wonderfulNegative disappointed, poor, waste, too, bad, worst, back, broken, return, horrible, off, tried, poorly?simple?
and ?quick?
in target domain using thepolarity of ?easy?
and their sentiment relationswith it, even if they may be covered by no sourcedomain.6 ConclusionThis paper presents a sentiment domain adaptationapproach which transfers the sentiment knowledgefrom multiple source domains to target domain.Our approach consists of two steps.
First, we ex-tract both global and domain-specific sentimentknowledge from the data of multiple source do-mains.
Second, we transfer these two kinds of sen-timent knowledge to target domain with the helpof the words?
sentiment graph.
We proposed tobuild words?
sentiment graph for target domain byextracting their sentiment polarity relations frommassive unlabeled data.
Besides, we proposed anovel domain similarity measure based on senti-ment graphs, and incorporated the domain similar-ities between target and different source domainsinto the domain adaptation process.
The experi-mental results on a benchmark dataset show thatour approach can effectively improve the perfor-mance of cross-domain sentiment classification.AcknowledgementsThis research is supported by the Key Programof National Natural Science Foundation of China(Grant nos.
U1536201 and U1405254), the Na-tional Natural Science Foundation of China (Grantno.
61472092), the National High Technology Re-search and Development Program of China (863Program) (Grant no.
2015AA020101), the Na-tional Science and Technology Support Programof China (Grant no.
2014BAH41B00), and theInitiative Scientific Research Program of TsinghuaUniversity.ReferencesJohn Blitzer, Mark Dredze, Fernando Pereira, et al2007.
Biographies, bollywood, boom-boxes andblenders: Domain adaptation for sentiment classi-fication.
In ACL, volume 7, pages 440?447.Danushka Bollegala, David Weir, and John Carroll.2011.
Using multiple sources to construct a senti-ment sensitive thesaurus for cross-domain sentimentclassification.
In ACL:HLT, pages 132?141.Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato,and Jonathan Eckstein.
2011.
Distributed optimiza-tion and statistical learning via the alternating direc-tion method of multipliers.
Foundations and Trendsin Machine Learning, 3(1):1?122.Rita Chattopadhyay, Jieping Ye, Sethuraman Pan-chanathan, Wei Fan, and Ian Davidson.
2011.Multi-source domain adaptation and its applicationto early detection of fatigue.
In KDD, pages 717?725.
ACM.Xi Chen, Qihang Lin, Seyoung Kim, Jaime G Car-bonell, Eric P Xing, et al 2012.
Smoothingproximal gradient method for general structured s-parse regression.
The Annals of Applied Statistics,6(2):719?752.Lixin Duan, Ivor W Tsang, Dong Xu, and Tat-SengChua.
2009.
Domain adaptation from multiplesources via auxiliary classifiers.
In ICML, pages289?296.
ACM.Theodoros Evgeniou and Massimiliano Pontil.
2004.Regularized multi?task learning.
In KDD, pages109?117.
ACM.Xavier Glorot, Antoine Bordes, and Yoshua Bengio.2011.
Domain adaptation for large-scale sentimentclassification: A deep learning approach.
In ICML,pages 513?520.Yulan He, Chenghua Lin, and Harith Alani.
2011.Automatically extracting polarity-bearing topics forcross-domain sentiment classification.
In ACL:HLT,pages 123?131.Tao Li, Vikas Sindhwani, Chris Ding, and Yi Zhang.2009.
Knowledge transformation for cross-domainsentiment classification.
In SIGIR, pages 716?717.ACM.309Shoushan Li, Yunxia Xue, Zhongqing Wang, andGuodong Zhou.
2013.
Active learning for cross-domain sentiment classification.
In IJCAI, pages2127?2133.Jun Liu, Shuiwang Ji, and Jieping Ye.
2009.
Multi-task feature learning via efficient l2,1-norm mini-mization.
In UAI, pages 339?348.Bing Liu.
2012.
Sentiment analysis and opinion min-ing.
Synthesis Lectures on Human Language Tech-nologies, 5(1):1?167.Yishay Mansour, Mehryar Mohri, and Afshin Ros-tamizadeh.
2009.
Domain adaptation with multiplesources.
In NIPS, pages 1041?1048.Sinno Jialin Pan and Qiang Yang.
2010.
A survey ontransfer learning.
TKDE, 22(10):1345?1359.Sinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, QiangYang, and Zheng Chen.
2010.
Cross-domain sen-timent classification via spectral feature alignment.In WWW, pages 751?760.
ACM.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and trends in infor-mation retrieval, 2(1-2):1?135.Robert Remus.
2012.
Domain adaptation using do-main similarity-and domain complexity-based in-stance selection for cross-domain sentiment analy-sis.
In 2012 IEEE 12th International Conference onData Mining Workshops, pages 717?723.
IEEE.Robert Tibshirani.
1996.
Regression shrinkage and se-lection via the lasso.
Journal of the Royal StatisticalSociety.
Series B (Methodological), pages 267?288.Fangzhao Wu and Yongfeng Huang.
2015.
Collabora-tive multi-domain sentiment classification.
In ICD-M, pages 459?468.
IEEE.Fangzhao Wu and Yongfeng Huang.
2016.
Person-alized microblog sentiment classification via multi-task learning.
In AAAI, pages 3059?3065.Fangzhao Wu, Yangqiu Song, and Yongfeng Huang.2015.
Microblog sentiment classification with con-textual knowledge regularization.
In AAAI, pages2332?2338.Yasuhisa Yoshida, Tsutomu Hirao, Tomoharu Iwata,Masaaki Nagata, and Yuji Matsumoto.
2011.
Trans-fer learning for multiple-domain sentiment analy-sis?identifying domain dependent/independent wordpolarity.
In AAAI, pages 1286?1291.Hui Zou and Trevor Hastie.
2003.
Regularizationand variable selection via the elastic net.
Journalof the Royal Statistical Society: Series B (StatisticalMethodology), 67(2):301?320.310
