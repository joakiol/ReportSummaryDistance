Ident i f icat ion of  Coreference Between Names  and FacesKoich i  Yamada and Kazunar i  Sug iyamaYasunor i  Yonamine  and H i rosh i  NakagawaFaculty of Engineering, Yokohama National University79-5 Tokiwadai Hodogaya-ku Yokohama City, Kanagawa 240-8501 JapanPhone: +81-45-339-4137{aron, ksugi, yasunet, nakagawa}@naklab.dnj.ynu.ac.jpAbst ractTo retrieve multimedia contents by their mean-ing, it is necessary to use not only the contentsof distinct media, such as image or language,but also a certain semantic relation holding be-tween them.
For this purpose, in this paper, wepropose a method to find coreferences betweenhuman names in the article of newspaper andhuman faces in the accompanying photograph.The method we proposed is based on the ma-chine learning and the hypothesis driven com-bining method for identifying names and corre-sponding faces.
Our experimental results showthat the recall and precision rate of our methodare better than those of the system which usesinformation exclusively from either text mediaor image media.1 IntroductionIn multimedia contents retrieval, almost allof researches have ibcused on information ex-tracted from single media, e.g.
(Han andMyaeng, 1996) (Smeaton and Quigley, 1996).These methods don't take into account seman-tic relations, like coreference between faces andnames, holding between the contents of individ-ual media.
In order to retrieve multimedia con-tents with this kind of relations, it is necessaryto find out such relations.In this research, we use photograph news ar-ticles distributed on the Internet(Mai, 1997)and develop a system which identifies a person'sname in texts of this type of news articles andher/his face on the accompanying photographimage, based on 1) the machine learning tech-nology applied to individual media contents tobuild decision trees which extract face regionsand human names, and 2) hypothesis basedcombining method for the results extracted bydecision trees of 1).
Since, in general, the num-ber of candidates from image and that from lan-guage are more than one, the output of our sys-tem is the coreference between a set of face re-gions and a set of names.There are many researches in the areaof human face recognition (Rowley et al,1996)(Hunke, 1994)(Yang et al, 1997)(Turkand Pentland, 1991) and human name extrac-tion, e.g.
(MUC, 1995).
However, almost allof them deal with the contents of single mediaand don't take into account he combinationof multimedia contents.
As a case of combin-ing multimedia contents, there is a research ofcaptioned images (Srihari and Burhans, 1994)(Srihari, 1995).
Their system analyzes an im-age and the corresponding caption to identifythe coreference between faces in the image andnames in the caption.
The text in their researchis restricted to captions, which describes con-tents of the corresponding images.
However, innewspapers or photo news, captions don't al-ways exist and long captions like the captionsused in their research are rare.
Therefore, ingeneral, we have to develop amethod to captureeffective linguistic expressions not from captionsbut from the body of text itself.In the research field of the video contentsretrieval, although there are many researches((Flickner et al, 1995),etc), few researches havebeen done to combine image and language me-dia (Satoh et al, 1997)(Satoh and Kanade,1997)(Smith and Kanade, 1997)(Wactlar et al,1996)(Smoliar and Zhang, 1994) .
In this field,as language media, there are soundtracks orcaptions in the video or sometimes in its tran-scriptions.
For analysis of video contents, theinformation which consists along the time axisis effective and is used in such systems.
On theother hand, for analysis of still images, someother methods that are different from the meth-ods for video contents retrieval are required be-cause the relatively small amount of and lim-ited information than information from videosare provided.In section 2, the background and our system'soverview are stated.
In section 3 and 4, we de-scribe the language module and the image mod-ule, respectively.
Section 5 describes the com-1"1bining method of the results of the languagemodule and the image module.
In section 6,the experimental results are shown.
Section 7 isour conclusions.2 System arch i tec ture  for combin ingTo find coreferences between ames in the textand faces in the image of the same photographnews article, we have to extract human namesfrom the text and recognize faces in the image(Figure 1).Photograph news articleimage :iiii~\]~i:i::" ""::!ili:i" "i:i:i:i~ .
.
.
.
.
.
.
.
.
.
.
L J .... .,X.
.
.
.
.
.
.
/ .
.
,~ .
.
.
.
.
.
.
.
.
P .
.
.
.
.. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.Face recogn~ionO0ABC DCorrespondenceI relationAName extractionFigure 1: Human name extraction and facerecognition.The problem is that the face of the personwhose name is appearing in a text is not alwaysappearing in the image, and vice versa.
There-fore, we have to develop a method by whichwe automatically extracts a person whose nameappears in the text and simultaneously his/herface appears on the image of the same article.For the convenience, we define common person,common name and common face as follows.Def init ion 1 A person whose name and faceappear in the text of the article and in the photoimage of the same article respectively, is calledcommon person.
The name of the common per-son is called common ame, and the face of thecommon person is called common face.This research is initiated by the intuition thatis state as assumptions as follows:Assumption 1 The name of a common personhas a certain linguistic feature in the text dis-tinct from that of a non common person.Assumption 2 The face of a common personhas a certain image feature distinct from that ofa non common person.These two assumptions are our starting pointto seek out a method to identify the differencebetween tlle way of appearing of common amesor faces in each media and the way of appear-ing of non common names or faces, and assigncertainties of commonness to names and facesrespectively based on the above assumptions.Since each media requires its proper process-ing methodology, our system has the languagemodule to process the text and the image mod-ule to process the image.
Our system also hasthe combining module which derives the finalcertainty of a name and a face from the cer-tainty of name calculated by the language mod-ule and the certainty of face calculated by theimage module respectively.For the image module, it is necessary to usethe resulting information given by the languagemodule, such as the number of names of highcertainty, because the features of regions likewhere and how large they are, depend on thenumber of common persons.
For example, theimage module should select the largest regionif the language module extracts only one name.On the other hand, for the language module, itis also necessary to use the result we get fromthe image module, such as the number of facesof high certainty, to select names of the commonperson.However, if we consider the nature of these in-teractive procedures between the language mod-ule and the image module, it is easily knownthat one module cannot wait until the comple-tion of analysis of the other module.
To resolvethis situation, we consider two kinds of method.Method 1: First, the image (or language)module analyzes contents to proceed theprocess and outputs the partial results.Then assuming the result of the image (orlanguage) module is correct, the language(or image ) module analyzes the text (or im-age).Needless to say, the assumed partial resultsmight be wrong.
In that case, the image(or language) module has to backtrack toresolve the conflict between the result ofthe image module and that of the languagemodule.
Namely, this method is a kind ofsearch with backtrack and it also requiresthe threshold value by which the system de-cides whether the situation needs to back-track or not.
Moreover, the result dependson which media is analyzed first.Method 2: Before combining of the results ofimage processing and those of languageprocessing, the system works out all thehypotheses about the number of common18persons.
Using all of these hypotheses, thesystem selects the best combination of theresults.
Its strong advantages are 1) the op-timal solution is always found, and 2) eachmodule can process independently.Considering the advantages and the short-comings of two of the above described meth-ods, it is reasonable to adopt Method 2.
In thisresearch, the hypotheses of the number of com-mon persons are "one", "two" and "more thantwo."
The reasons of introducing "more thantwo" are the followings: the images containingfour or more persons are very rare, and suchimages have similar features to the images con-taining three persons.ImagemodulearticleLanguagemoduleOutputs under each of 3 hypotheses" ~ erson = "t ", "2"\] "more than 2"Combiningmodule )common personsJohn: 0.8Paul: 0.6Figure 2: Overview of our system.3 Ext ract ion  o f  human namecand idatesThe language module extracts the human namecandidates from all human names appearing inthe text and assigns certainty of commonnessto each of the candidates of a common name.When the extracted name is a common name,the person is regarded as the important personin the article.
Therefore, the linguistic expres-sions around the name probably have the spe-cific linguistic features.
Thus, our system de-cides whether an extracted name is a commonname or not with information of the linguisticexpressions around the human name.
To selecteffective features for this purpose from the allfeatures generated from the text, we employ amachine learning technique, because some im-portant features could be fallen out if selectedby hand.
Moreover, machine learning techniquemight be able to learn incomprehensible phe-nomena for human.It is hard to recognize meaningful linguisticfeatures without morphological analysis.
Onthe other hand, if the system does the syntaxanalysis, the handling of the ambiguity becomesa big problem.
Furthermore, on the practicaluse, high processing cost becomes a problem toprocess huge amount of news articles.
As theconsequence, we adopt a word sequence patternbased approach.
For this, firstly, we analyzetexts of news articles with morphological ana-lyzer JUMAN(version 3.6)(Kurohashi and Na-gao, 1998) to extract the part of speech tags asthe features in machine learning.
Note that acompound noun is treated as one noun becauseif we treat component words of the compoundnoun individually, the patterns we have to dealwith become too complicated for machine learn-ing systems.
The features to be used for learn-ing are the followings.Compound noun wh ich  conta ins  ahuman nameThe human name appearing in the news articlesmight have the adjacent words which describeadditional information about the name such astitle, age, year of birth and so on.
The namewith some kind of words, like title, sometimesbecomes one compound noun and treated as onemorpheme in our system.
Our system tries tofind this type of information as features for ma-chine learning.Par t  o f  speech  tags  around a humannameAs well known, syntactic parsing is computa-tionally heavy and usually has high ambiguities.Thus, instead of syntactic parsing, we extractthe combination of a word, its part of speechtag and its relative position to the focused namefor learning.
Especially we focus on the wordsaround the human name to capture the charac-teristic linguistic expressions about the humanname.
Our system employs two levels of thepart of speech tag defined by the morphologicalanalyzer JUMAN.Since our system is for Japanese, object is de-scribed by a case particle.
In pattern matching,instead of the sophisticated case analysis doneby syntactic parsing, our system first applies theparticle followed to the word as a feature.
Asfor a predicate, we choose the predicate whoseposition is after the name and nearest o thename because in Japanese a predicate comes af-ter subject, object, and other syntactic ompo-nents.19Location and frequency of  a humannameLocation of the word is important because itreflects structures of documents.
Our systemuses features as follows: 1) whether the wordis in the title or not, 2) the line number of theline the word is in, and 3) the number of theparagraph the word is in.
Our system also usesthe order of the occurrence of the name in allthe name occurrences and the frequency of thename in the text.Using linguistic features described above ex-tracted from training data as inputs, we useC5.0 (Rul, 1998) to generate decision trees.
Foreach case in test data, C5.0 outputs th~ classpredicted by the decision tree with the confi-dence of the prediction.
We use the confidenceas the output of this module.Another factor for selecting feature for learn-ing is how many morphemes around the nameare used.
In our experiment, ten morphemesaround the name are used.
The experimentalresults will be shown in section 6.4 Ext ract ion  o f  human facecand idatesTo identify coreferences between the face in theimage and the name in the text, this moduleshould extract regions that are candidates ofcommon face.
In this section, we describe theimage module which extracts face candidatesfrom the image.
The face candidates are thefaces of persons who might be common persons.Next, as same as the language module does, thismodule learns the characteristic features of theregion of a common face that are used to de-cide whether an extracted region as a face is acommon face or not.4.1 Extract ion o f  face regionsTo extract face regions, this module uses the fol-lowing methods: 1) Filtering to remove noise,and 2) RGB based modeling of skin color toextract face region.
Furthermore, this mod-ule generates features of each region and learnscharacteristics of the common face by C5.0.
Thevalue of each feature, e.g.
location of face re-gion, region size, depends upon the number ofthe persons appearing in the image as shownin Figure 3 and the text.
To optimize fea-ture based recognition, this module proceedsthe processes corresponding to three hypothe-ses, say the number of common person is one,two, or more than two.?
O0 00one two three0Figure 3: Differences in the features accordingto the number of the person.4.2 Skin co lor  mode l ingThe advantage of using color for face detectionis robust against orientation, occlusion and in-tensities, and able to process fast, but the de-merit is the difficulty in detecting only a facefi'om a human body or other parts like hands,and to locate it accurately.Darrell et al(Darrell et al, 1998) con-vert (R,G,B) tuples into tuples of the form(log(G),log(R) - log(G), log(B) - (log(R) +log(G))/2) which is called "log color-opponentspace", and detect skin color by using a clas-sifier with an empirically estimated Gaussianprobability model of "skin" and "not-skin" in'the space.
Yang et al(Yang and Waibel, 1995)develop a real-time face tracking system, andthey propose an adaptive skin color model un-der different lighting condition based on the factthat its distribution under a certain lightingcondition can be Characterized by a multivari-ate Gaussian distribution(Yang et al, 1997).The variables are chromatic olors, that is, r =R/(R+G+B), and g = G/(R+G+B).
On theother hand, Satoh et al(Satoh et al, 1997) usethe Gaussian distribution in (R, G, B) space intheir face detection system because this modelis more sensitive to brightness of skin color.The picture of the newspaper we treat is ascene picture that includes not only a commonface but also other faces, and a face doesn't al-ways look straight forward.
Thus, we use colorinformation to detect a face because the colordoesn't depend on its orientation.
Suppose thatthe skin color distribution complies with theGaussian distribution in (R,G, B) space(Satohet al, 1997).
Then, we introduce the Maha-lanobis distance.
That is the distance fi'olnthe center of gravity of the group consider-ing variance-covariance of data.
We calculatethe mean intensity M(= (/~,G,/})T), variance-covariance matrix V and Mahalanobis distanced from skin color data of 5pixel ?
5pixel blocks,which are extracted from the cheek colored ar-eas of 85 persons (Satoh et al, 1997).
The al-most all of cheek colored areas express natural20skin color and they are rarely in a. shadow evenif the people wear hat, etc.
Suppose I be in-tensities of a pixel of the input image.
Then, ifthat pixel satisfies (1), we take that pixel as thecandidate pixel with skin color.d 2 > ( I -  M)Tv - I ( I  - M)  (1)where value d is experimentally optinfized.The method we described above is not so ac-curate in some cases.
Some extra, non-facialregions would also be extracted simultaneously.To achieve higher accuracies, we examine thedistribution of (R + G + B) - (R - B), and drawborder lines in order to contain more than 80%of the sample.
We decide the triangle manuallyby observing the various output images.
We ex-tract the pixels which is in the triangle shownin Figure 4.100 200 300 400 500 600 lO0 800R+G+BFigure 4: Skin color in (R -I- G + B) - (R - B)space.Some results by this method is shown in Fig-ure 5.
As you can see, not only faces but alsohands and other regions whose color is similar toskin color are also extracted.
To elilninate theseundesirable r gions, we use a decision trees builtby C5.0 as stated in 4.3.Figure 5: Result of face candidate region ex-traction.4.3 Features of extracted regionsIn this research, we use the following 17 fea-tures including the composition information ofthe whole image, in addition to the form andcolor of the region that is used with conven-tional image retrieval(Han and Myaeng, 1996).The following five features are used to'expressthe form of skin color region: 1) Ratio of re-gion to the largest region, 2) Ratio between thelength of X-axis direction and the length of Y-axis direction, 3) Rectangularity, 4) Ellipticity,5) Eccentricity.The feature about the color is the followings:6-9) Each of the mean of R, G, B and intensityY.The following eight features are positional in-formation of the region.10) Aspect ratio of the whole image.11,12) x,y  coordinates of the center of gravityof the region.13) Distance between the center of gravity ofthe region and the center of the whole im-age, normalized with a half of the length ofthe diagonM line of the image.14) The order of the region in descending orderof 13).15) Distance between the center of gravity ofthe region and the center of the upper endof the whole image, normalized with thelength from the center of the upper end tothe left lower end (or the right lower end).16) The order of the region in descending orderof 15).17) Suppose that the image is divided into 3 ?3 sub-areas.
Which of these sub-areas thecenter of gravity is in.Using these 17 features extracted from trMn-ing data as input features, we use C5.0 to learndecision trees, which extract candidates of com-mon face with different certainties as describedin section 3.
The experhnental results will beshown in section 6.5 Combin ing  candidates from imageand languageIn this section, we describe the combining mod-ule whose inputs are the candidates extractedby the language module and the image moduledescribed in section 3 and 4, respectively.
Itsoutput is the result of the whole system.215.1 InputAs already said, since the language module andthe image module process under hypothesis of"one", "two", or  " lnore  than  two"  persons ,  re-spectively, one module outputs three results ac-cording to these three hypotheses.
Then out-puts from both modules are expressed a.s fol-lows:(output of language ntodule )(output of image module)= .fta,~g(n,x) (2)= f im~g~(m,y)(3)Note that n and m are the number of the com-mon persons adopted a.s the hypothesis, x and yare orders in ascending order of certainty aboutthe person being common.
The certainty of thedecision in the language module and the im-age module is the confidence output by C5.0.For example, fl,,~a(n, 2) expresses the certaintyof the person'who has the second highest cer-tainty.
Each output is something like the graphon Figure 6.
In this figure, all of the extractedf,,,,~\[ |" \] ,f image 1.0 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.0 first second third Candidate ofperson's name or faceFigure 6: Output from each module under onehypothesis.candidate names or faces are sorted in descend-ing order of calculated certainties by distinctdecision trees of  the language module or the im-age module because the nmnber of the commonperson might be more than one.
By introducingcertainties, as later described, we obtain enor-mous flexibility in combining ca,ndidates fromthe language module and those from the imagemodule.5.2 Combination of hypothesesSince the language module and the image mod-ule process under each of three hypotheses,there are 3 ?
3 combinations of the results.This combining module selects the best pairfrom those combinations and outputs the re-sults based on the selected pair.
To select thebest pair, we introduce some kinds of distancedescribed as follows.Distance between outputs of two mediaThe distance between the result of the imagemodule and the result of the language modulefu is defined by (4).Mf .
( .
, , .
,4  = ~ I/to~(.,, z) - k,,~ag~(m, ~)1z=l fta~g(n, z) 7 fimagdm, z) (4)where 114 is the maximum number of the personsknown from the results of both modules.
As youknow from (4), the'nearer the certainties of thecandidates from the language module and theimage module which have the same order z are,the smaller the fu(n, m) is.Distance between output of media andhypothesisIf there is difference between a hypothesis andthe output calculated under the hypothesis, ayft~ng and fimag~, the hypothesis hould not beconsidered to be valid.
Therefore, we intro-duce the distance between the hypothesis andthe output of the language module: ft~g or thatof the image module: fimoee.
A hypothesis of ncommon persons is defined in (5).1 (x _< n) fa(n,x) = 0 (x > n) (5)where x is the order of certainty of candidates.Since each of the language module and the im-age module has its own hypothesis, the combin-ing module calculates the distance fat definedby (6) between the hypothesis used in the la.n-guage module and the result fl'om the languagemodule.
It also calculates the distance fai de-fined by (7) between the hypothesis used in theimage module and the result from the imagemodule.3fo~(',) = F_.
Ifto,~g(,~, z) - A ( .
, ,  z)lz=l ?,n~(n, z) 7 fa(,~, z) (6)3 Ikma,e(m, Z) -- fa(m,Z)\[L i(m) = ~ f im~(m, z) 7 L('~, z) (r)z--~\]In the case that the hypothesis i "more thantwo", the certMnty of candidates whose order isfourth or larger are ignored.Decreasing factor for each inconsistenthypothesisDifferent hypotheses of the language moduleand the image module indical~e inconsistency.However, since the analysis of each module isnot perfect, our system does not exclude such22inconsistent coml)inations of hypotheses.
In-stead, we decrease the certainty of such in-consistent combinations.
For this, we use de-creasing factors D(m,n)  where n and m meanthe hypothesized nmnber of person in the lan-guage module and the image module, respec-tively.
We empirically tuned up the actual val-ues of D(m, n) as shown in Table 1.Table 1: Decreasing factor D(m, n) for each in-consistent hypothesis.n1 2 3 or more1 1.0 0.9 0.5m 2 0.9 1.0 0.63 or more 0.5 0.6 0.8In tegrat ion  of  the measuresUsing these three distances, namely fti, f~t andfoi, and D(n,m) , the combining module fi-nally calculates total certainty f (n ,m)  definedby (8) for each combination of hypotheses.
Thesmaller the f (n,  m) is, the nearer the result fromthe language module is the result of the imagemodule.f (n,  m) ={fli(n, ?,~) n L 1} {fa/(n) + 1} {f~i(m) + 1}1?
D(n, m) (8)5.3 Combin ing  the resultsWhen a combination which has the smallestf (n,  m) has been selected, the results from thelanguage module and the image module arefixed.
The system combines these results intoone result funion(n, m, z), where the person cor-responding to z is expected to be a commonperson, funion(n, m, z) is the final output of thewhole system.
For this combining, we investi-gate two methods as follows.
In (9), the con-sistency on the number of common persons isregarded as an important factor.
On the otherhand, in (10), when at least one of two mod-ule, namely time language module or the imagemodule, assigns high certainty to a candidateperson, the whole system finally assigns highcertainty to the candidate person.w,  m, z) =.hamAn, x (9)Vz, m, =l - { 1 - h .n ,A , , .
,  z )}  {l  -(10)The final outputs of whole system are somethinglike these: "John: common person (certainty:0.8)", "Paul: common person (certainty: 0.4)"and so on.
These results are used to find theface on the image if We specify a certain namein the text to retrieve his/her face image, or viceversa..6 Exper imentsWe have experimentally evaluated the systemwe proposed by comparing with the simple sys-tems which contain only the language moduleor the image module respectively to confirm theeffect of the combining process.
The languagemodule and the image module work under threekinds of hypothesis in the simple systems aswell.
Thus, we use the system's result whichhas the minimum distance between the outputof media and the hypothesis defined by formula(6),(7) a.s the baseline of evaluation.
In our ex-periments, we use the photograph news in theweb page called "AULOS" distributed by TheMainichi Newspapers(Mai, 1997).
The averagelength of the text of the article is about 300characters or 100 words.
The almost all of theimages are full colored, and the average size ofthem is about 250 x 200 pixels.
Moreover, theimages are not accompanied with captions.
Onthis evaluation, we use articles with full coloredimages published on May and June 1997.
Asfor common name extraction, we did four foldcross-validation for 228 articles of this periodwhich contains common human names.
As forcommon face extraction, we did three fold cross-validation for the set of color photograph im-ages which are contained by the articles usedby the language module.
To evaluate how accu-rate the system identifies the given person beinga common person, we calculated the recall andprecision rate of the system's decision about aperson being common.
Since the outputs of oursystem are certainties, recall and precision ratesare defined as follows.Recall =Precision -Eiecc W(i)Number of the common persons(11)w(i)Ew w(i)where W(i)  is the certainty of person i, and ccmeans a set of all correctly identified persons.23Table 2: The evaluation results of the outputsfrom each module.Recall PrecisionLanguage module 0.68 0.67Image module 0.52 0.64Combining module based on (9) 0.42 0.74Combining module based on (10) 0.76 0.69The evaluation results of each module isshown in Table 2.For the language module and the combiningmodule, we evaluate names and its certainties.On the other hand, for the image module, weevaluate only certainties under the assumptionthat the human name of the face which was as-signed higher certainty is correct because theimage module doesn't output human names.The effect of combining appears as the differ-ence between the results of the combining mod-ule and the results of the language module orthe image module.
The combining module hastwo variations.
The module based on (10) im-proved both recall and precision rates by com-bining.
The reason of high recall rate is thatone module picks up  the person whom the othermodule fails to pick up.
Since high precisionrate is maintained, this compensation is reallyeffective.
On the other hand, the combiningmodule based on (9) improves the precision ratemore than the module based on (10).
The rea-son of this phenomena is that the module is ableto cancel the noise which appears in one mediacontents by the other media contents.
However,the recall rate was decreased as expected from(9).7 ConclusionsWe have developed the system which identifiescoreferences between the human face in the im-age and the human name in the text by selectingcombinations of hypotheses and the combiningof the results from the language module and theimage module.
The experimental result is thatrecall is 42% to 76% and precision is 69% to74%.
This result indicates that the practical useof semi-automatic extraction of common per-son from multimedia contents for IR purposeswould come into our sight with some technicalimprovement along this line of research strategy.ReferencesT.
Darrell, G. Gordon, M. H'arville, and J. Wood-fill.
1998.
Integrated person tracking using stereo,color, and pattern detection.
CVPR'98, pages601-609.Myron Flickner, Harpreet Sawhney, et al 1995.Query by image and video content: The QBICsystem.
Compuler, 28(9):23-32.Kyung-Ah Han and Sung-Hyun Myaeng.
1996. hn-age organization and retrieval with automaticallyconstructed feature vectors.
SIGIR'96, pages157-165.H.
M. Hunke.
1994.
Locating and tracking of humanfaces with neural networks.
Tech.
Report CMU-CS-94-155, Carnegie Mellon University.Sadao Kurohashi and Makoto Nagao.
1998.Japanese morphological nalysis ystem JUMANlnanual (versidn 3.6).
Kyoto University.The Mainichi Newspapers, 1997.
A ULOS PhotoNews.
http ://www.mainichi.co.jp/.DARPA, 1995.
Proceedings ofthe Sixth Message Un-derstanding Conference (MUC-6).H.
A. Rowley, S. Baluja, and Takeo Kanade.
1996.Neural network-based face detection.
CVPR'96,pages 203-208.RuleQuest Research Pty Ltd, 1998.
See5/ C5.0.http://www.rulequest.com/.Shin'ichi Satoh and Takeo Kanade.
1997.
Name-It:Association of face and name in video.
CVPR'97,pages 368-373.Shin'ichi Satoh, Yuichi Nakamura, and TakeoKanade.
1997.
Name-It: Naming and detectingfaces in video by the integration of image and nat-ural language processing.
IJCAI-97, pages 1488-1493.Alan F. Smeaton and Fan Quigley.
1996.
Experi-ments on using semantic distances between wordsin image caption retrieval.
SIGIR'96, pages 174-180.Michael A. Smith and Takeo Kanade.
1997.
Videoskimming and characterization through the com-bination of image and language understandingtechniques.
CVPR'97, pages 775-781.Stephen W. Smoliar and HongJiang Zhang.
1994.Content-based video indexing and retrieval.
Mul-limedia, 1(2):62-72.Rohini K. Srihari and Debra T. Burhans.
1994.Visual semantics: Extracting visual informa-tion frol/l text accompanying pictures.
AAAI-94,1:793-798.Rohini K. Srihari.
1995.
Automatic indexingand content-based retrieval of captioned images.Computer, 28(9):49-56.M.
T~rk and A. Pentland.
1991.
Eigenfaces forrecognition.
Journal of Cognitive Neuroscience,3(1):71-86.Howard D. Wactlar, Takeo Kanade, Michael A.Smith, and Scott M. Stevens.
1996.
Intelligentaccess to digital video: Informedia project.
Com-puter, 29(5):46-52.Jie Yang and Alex Waibel.
1995.
Tracking humanfaces in real-time.
Tech.
Report CMU-CS-95-210,Carnegie Mellon University.Jie Yang, Weier Lu, and Alex Waibel.
1997.
Skin-color modeling and adaptation.
Tech.
ReportCMU-CS-97-146, Carnegie Mellon University.24
