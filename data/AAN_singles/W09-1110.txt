Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 66?74,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsInteractive Feature Space Construction using Semantic InformationDan Roth and Kevin SmallDepartment of Computer ScienceUniversity of Illinois at Urbana-ChampaignUrbana, IL 61801{danr,ksmall}@illinois.eduAbstractSpecifying an appropriate feature space is animportant aspect of achieving good perfor-mance when designing systems based uponlearned classifiers.
Effectively incorporat-ing information regarding semantically relatedwords into the feature space is known to pro-duce robust, accurate classifiers and is one ap-parent motivation for efforts to automaticallygenerate such resources.
However, naive in-corporation of this semantic information mayresult in poor performance due to increasedambiguity.
To overcome this limitation, weintroduce the interactive feature space con-struction protocol, where the learner identi-fies inadequate regions of the feature spaceand in coordination with a domain expert addsdescriptiveness through existing semantic re-sources.
We demonstrate effectiveness on anentity and relation extraction system includ-ing both performance improvements and ro-bustness to reductions in annotated data.1 IntroductionAn important natural language processing (NLP)task is the design of learning systems which per-form well over a wide range of domains with limitedtraining data.
While the NLP community has a longtradition of incorporating linguistic information intostatistical systems, machine learning approaches tothese problems often emphasize learning sophisti-cated models over simple, mostly lexical, features.This trend is not surprising as a primary motivationfor machine learning solutions is to reduce the man-ual effort required to achieve state of the art perfor-mance.
However, one notable advantage of discrimi-native classifiers is the capacity to encode arbitrarilycomplex features, which partially accounts for theirpopularity.
While this flexibility is powerful, it oftenoverwhelms the system designer causing them to re-sort to simple features.
This work presents a methodto partially automate feature engineering through aninteractive learning protocol.While it is widely accepted that classifier perfor-mance is predicated on feature engineering, design-ing good features requires significant effort.
One un-derutilized resource for descriptive features are ex-isting semantically related word lists (SRWLs), gen-erated both manually (Fellbaum, 1998) and automat-ically (Pantel and Lin, 2002).
Consider the follow-ing named entity recognition (NER) example:His father was rushed to [WestlakeHospital]ORG, an arm of [ResurrectionHealth Care]ORG, in west suburban[Chicagoland]LOC.For such tasks, it is helpful to know that west isa member of the SRWL [Compass Direction] andother such designations.
If extracting features usingthis information, we would require observing onlya subset of the SRWL in the data to learn the cor-responding parameter.
This statement suggests thatone method for learning robust classifiers is to in-corporate semantic information through features ex-tracted from the more descriptive representation:His father was rushed to Westlake [HealthCare Institution], an [Subsidiary] of Resur-rection Health Care, [Locative Preposition][Compass Direction] suburban Chicagoland.66Deriving discriminative features from this rep-resentation often results in more informative fea-tures and a correspondingly simpler classificationtask.
Although effective approaches along this veinhave been shown to induce more accurate classi-fiers (Boggess et al, 1991; Miller et al, 2004; Li andRoth, 2005), naive approaches may instead result inhigher sample complexity due to increased ambi-guity introduced through these semantic resources.Features based upon SRWLs must therefore balancethe tradeoff between descriptiveness and noise.This paper introduces the interactive featurespace construction (IFSC) protocol, which facil-itates coordination between a domain expert andlearning algorithm to interactively define the featurespace during training.
This paper describes the par-ticular instance of the IFSC protocol where seman-tic information is introduced through abstraction oflexical terms in the feature space with their SRWLlabels.
Specifically, there are two notable contri-butions of this work: (1) an interactive method forthe expert to directly encode semantic knowledgeinto the feature space with minimal effort and (2) aquerying function which uses both the current stateof the learner and properties of the available SRWLsto select informative instances for presentation tothe expert.
We demonstrate the effectiveness of thisprotocol on an entity and relation extraction task interms of performance and labeled data requirements.2 PreliminariesFollowing standard notation, let x ?
X representmembers of an input domain and y ?
Y representmembers of an output domain where a learning al-gorithm uses a training sample S = {(xi, yi)}mi=1to induce a prediction function h : X ?
Y .
Weare specifically interested in discriminative classi-fiers which use a feature vector generating procedure?
(x) ?
x, taking an input domain member x andgenerating a feature vector x.
We further assume theoutput assignment of h is based upon a scoring func-tion f : ?
(X ) ?
Y ?
R such that the prediction isstated as y?
= h(x) = argmaxy?
?Y f(x, y?
).The feature vector generating procedure is com-posed of a vector of feature generation functions(FGFs), ?
(x) = ?
?1(x),?2(x), .
.
.
,?n(x)?, whereeach feature generation function, ?i(x) ?
{0, 1},takes the input x and returns the appropriate fea-ture vector value.
Consider the text ?in west sub-urban Chicagoland?
where we wish to predict theentity classification for Chicagoland.
In this case,example active FGFs include ?text=Chicagoland,?isCapitalized, and ?text(?2)=west while FGFs suchas ?text=and would remain inactive.
Since we areconstructing sparse feature vectors, we use the infi-nite attribute model (Blum, 1992).Semantically related word list (SRWL) featureabstraction begins with a set of variable sizedword lists {W} such that each member lexicalelement (i.e.
word, phrase) has at least onesense that is semantically related to the conceptrepresented by W (e.g.
Wcompass direction =north, east, .
.
.
, southwest).
For the purpose offeature extraction, whenever the sense of a lexical el-ement associated with a particularW appears in thecorpus, it is replaced by the name of the correspond-ing SRWL.
This is equivalent to defining a FGF forthe specified W which is a disjunction of the func-tionally related FGFs over the member lexical ele-ments (e.g.
?text?Wcompass direction = ?text=north ?
?text=east ?
.
.
.
?
?text=southwest).3 Interactive Feature Space ConstructionThe machine learning community has become in-creasingly interested in protocols which allow inter-action with a domain expert during training, such asthe active learning protocol (Cohn et al, 1994).
Inactive learning, the learning algorithm reduces thelabeling effort by using a querying function to in-crementally select unlabeled examples from a datasource for annotation during learning.
By care-fully selecting examples for annotation, active learn-ing maximizes the quality of inductive informationwhile minimizing label acquisition cost.While active learning has been shown to reducesample complexity, we contend that it significantlyunderutilizes the domain expert ?
particularly forcomplex annotation tasks.
More precisely, when adomain expert receives an instance, world knowl-edge is used to reason about the instance and sup-ply an annotation.
Once annotated and provided fortraining, the learner must recover this world knowl-edge and incorporate it into its model from a smallnumber of instances, exclusively through induction.67Learning algorithms generally assume that thefeature space and model are specified before learn-ing begins and remain static throughout learning,where training data is exclusively used for parameterestimation.
Conversely, the interactive feature spaceconstruction (IFSC) protocol relaxes this static fea-ture space assumption by using information aboutthe current state of the learner, properties of knowl-edge resources (e.g.
SRWLs, gazetteers, unlabeleddata, etc.
), and access to the domain expert duringtraining to interactively improve the feature space.Whereas active learning focuses on the labeling ef-fort, IFSC reduces sample complexity and improvesperformance by modifying the underlying represen-tation to simplify the overall learning task.The IFSC protocol for SRWL abstraction is pre-sented in Algorithm 1.
Given a labeled data set S,an initial feature vector generating procedure ?0, aquerying function Q : S ?
h ?
Sselect, and anexisting set of semantically related word lists, {W}(line 1), an initial hypothesis is learned (line 3).
Thequerying function scores the labeled examples andselects an instance for interaction (line 6).
The ex-pert selects lexical elements from this instance forwhich feature abstractions may be performed (line8).
If the expert doesn?t deem any elements vi-able for interaction, the algorithm returns to line 5.Once lexical elements are selected for interaction,the SRWLW associated with each selected elementis retrieved (line 11) and refined by the expert (line12).
Using the validated SRWL definition W? , thelexical FGFs are replaced with the SRWL FGF (line14).
This new feature vector generating procedure?t+1 is used to train a new classifier (line 18) andthe algorithm is repeated until the annotator halts.3.1 Method of Expert InteractionThe method of interaction for active learning isvery natural; data annotation is required regardless.To increase the bandwidth between the expert andlearner, a more sophisticated interaction must be al-lowed while ensuring that the expert task of remainsreasonable.
We require the interaction be restrictedto mouse clicks.
When using this protocol to in-corporate semantic information, the primary tasksof the expert are (1) selecting lexical elements forSRWL feature abstraction and (2) validating mem-bership of the SRWL for the specified application.Algorithm 1 Interactive Feature Space Construction1: Input: Labeled training data S, feature vectorgenerating procedure ?0, querying function Q,set of known SRWLs {W}, domain expert A?2: t?
03: ht ?
A(?t,S); learn initial hypothesis4: Sselected ?
?5: while annotator is willing do6: Sselect ?
Q(S\Sselected, ht); Q proposes(labeled) instance for interaction7: Sselected ?
Sselected ?
Sselect; mark selectedexamples to prevent reselection8: Eselect ?
A?
(Sselect); the expert selects lex-ical elements for semantic abstraction9: ?t+1 ?
?t; initialize new FGF vector withexisting FGFs10: for each  ?
Eselect do11: Retrieve word listW12: W? ?
A?
(W); the expert refines the ex-isting semantic classW for this task13: for each ?
?
 do14: ?t+1 ?
(?t+1\?)
?
?W? ; re-place features with SRWL features (e.g.
?text= ?
?text?W? )15: end for16: end for17: t?
t+ 118: ht ?
A(?t,S); learn new hypothesis19: end while20: Output: Learned hypothesis hT , final featurespace ?T , refined semantic classes {W?
}3.1.1 Lexical Feature Selection (Line 8)Once an instance is selected by the querying func-tion (line 6), the the domain expert selects lexical el-ements (i.e.
words, phrases) believed appropriate forSRWL feature abstraction.
This step is summarizedby Figure 1 for the example introduced in Section 1.For this NER example, features extracted includethe words and bigrams which form the named en-tity and those within a surrounding two word win-dow.
All lexical elements which have membershipto at least one SRWL and are used for feature ex-traction are marked with a box and may be selectedby the user for interaction.
In this particular case,the system has made a mistake in classification of68His father was rushed to [WestlakeHospital ]ORG, an arm of [ResurrectionHealth Care ]ORG, in west suburban[Chicagoland]ORG.Figure 1: Lexical Feature Selection ?
All lexical ele-ments with SRWL membership used to derive featuresare boxed.
Elements used for the incorrect prediction forChicagoland are double-boxed.
The expert may selectany boxed element for SRWL validation.Chicagoland and the lexical elements used to derivefeatures for this prediction are emphasized with adouble-box for expository purposes.
The expert se-lects lexical elements which they believe will resultin good feature abstractions; the querying functionmust present examples believed to have high impact.3.1.2 Word List Validation (Lines 11 &12)Once the domain expert has selected a lexical el-ement for SRWL feature abstraction, they are pre-sented with the SRWL W to validate membershipfor the target application as shown in Figure 2.
Inthis particular case, the expert has chosen to performtwo interactions, namely for the lexical elementswest and suburban.
Once they have chosen whichwords and phrases will be included in this particularfeature abstraction,W is updated and the associatedfeatures are replaced with their SRWL counterpart.For example, ?text=west, ?text=north, etc.
would allbe replaced with ?text?WA1806 later in lines 13 & 14.A1806: southeast, northeast, southsoutheast, northeast, south, north, south-west, west, east, northwest, inland, outsideA1558: suburban, nearby, downtownsuburban, nearby, downtown, urban,metropolitan, neighboring, near, coastalFigure 2: Word List Validation ?
Completing two domainexpert interactions.
Upon selecting either double-boxedelement in Figure 1, the expert validates the respectiveSRWL for feature extraction.Accurate sense disambiguation is helpful for ef-fective SRWL feature abstraction to manage situa-tions where lexical elements belong to multiple lists.In this work, we first disambiguate by predicted partof speech (POS) tags.
In cases of multiple SRWLsenses for a POS, the given SRWLs (Pantel and Lin,2002) rank list elements according their semanticrepresentativeness which we use to return the high-est ranked sense for a particular lexical element.Also, as SRWL resources emphasize recall over pre-cision, we reduce expert effort by using the Googlen-gram counts (Brandts and Franz, 2006) to auto-matically prune SRWLs.3.2 Querying Function (Line 6)A primary contribution of this work is designing anappropriate querying function.
In doing so, we lookto maximize the impact of interactions while min-imizing the total number.
Therefore, we look toselect instances for which (1) the current hypoth-esis indicates the feature space is insufficient and(2) the resulting SRWL feature abstraction will helpimprove performance.
To account for these twosomewhat orthogonal goals, we design two query-ing functions and aggregate their results.3.2.1 Hypothesis-Driven QueryingTo find areas of the feature space which are be-lieved to require more descriptiveness, we look toemphasize those instances which will result in thelargest updates to the hypothesis.
To accomplishthis, we adopt an idea from the active learningcommunity and score instances according to theirmargin relative to the current learned hypothesis,?
(ft, xi, yi) (Tong and Koller, 2001).
This resultsin the hypothesis-driven querying functionQmargin = argsorti=1,...,m?
(ft, xi, yi)where the argsort operator is used to sort the inputelements in ascending order (for multiple instanceselection).
Unlike active learning, where selectionis from an unlabeled data source, the quantity of la-beled data is fixed and labeled data is selected duringeach round.
Therefore, we use the true margin andnot the expected margin.
This means that we willfirst select instances which have large mistakes, fol-lowed by those instances with small mistakes, andfinally instances that make correct predictions in theorder of their confidence.693.2.2 SRWL-Driven QueryingAn equally important goal of the querying func-tion is to present examples which will result inSRWL feature abstractions of broad usability.
Intu-itively, there are two criteria distinguishing desirableSRWLs for this purpose.
First of all, large lists aredesirable as there are many lists of cities, countries,corporations, etc.
which are extremely informative.Secondly, preference should be given to lists wherethe distribution of lexical elements within a particu-lar word list,  ?
W , is more uniform.
For example,consider W = {devour, feed on, eat, consume}.While all of these terms belong to the same SRWL,learning features based on eat is sufficient to covermost examples.
To derive a SRWL-driven queryingfunction based on these principles, we use the wordlist entropy, H(W) = ?
??W p() log p().
Thequerying score for a sentence is determined by itshighest entropy lexical element used for feature ex-traction, resulting in the querying functionQentropy = argsorti=1,...,m[argmin?
?xi?H(W)]This querying function is supported by the under-lying assumption of SRWL abstraction is that thereexists a true feature space ??
(x) which is built uponSRWLs and lexical elements but is being approxi-mated by ?
(x), which doesn?t use semantic infor-mation.
In this context, a lexical feature providesone bit of information to the prediction functionwhile a SRWL feature provides information contentproportional to its SRWL entropy H(W).To study one aspect of this phenomena empiri-cally, we examine the rate at which words are firstencountered in our training corpus from Section 4,as shown by Figure 3.
The first observation isthe usefulness of SRWL feature abstraction in gen-eral as we see that when including an entire SRWLfrom (Pantel and Lin, 2002) whenever the first ele-ment of the list is encountered, we cover the unigramvocabulary much more rapidly.
The second observa-tion is that when sentences are presented in the or-der of the average SRWL entropy of their words, thiscoverage rate is further accelerated.
Figure 3 helpsexplain the recall focused aspect of SRWL abstrac-tion while we rely on hypothesis-driven querying totarget interactions for the specific task at hand.00.20.40.60.810  200  400  600  800  1000unigramtypecoverage (%)sentencesSRWL - EntropySRWL - SequenceLexical - SequenceFigure 3: The Impact of SRWL Abstraction and SRWL-driven Querying ?
The first occurrence of words occur ata much lower rate than the first occurrence of words whenabstracted through SRWLs, particularly when sentencesare introduced as ranked by average SRWL entropy cal-culated using (Brandts and Franz, 2006).3.2.3 Aggregating Querying FunctionsTo combine these two measures, we use the Bordacount method of rank aggregation (Young, 1974) tofind a consensus between the two querying func-tions without requiring calibration amongst the ac-tual ranking scores.
Defining the rank position ofan instance by r(x), the Borda count based queryingfunction is stated byQBorda = argsorti=1,...,m[rmargin(xi) + rentropy(xi)]QBorda selects instances which consider both wideapplicability through rentropy and which focus onthe specific task through rmargin.4 Experimental EvaluationTo demonstrate the IFSC protocol on a practical ap-plication, we examine a three-stage pipeline modelfor entity and relation extraction, where the task isdecomposed into sequential stages of segmentation,entity classification, and relation classification (Rothand Small, 2008).
Extending the standard classifi-cation task, a pipeline model decomposes the over-all classification into a sequence of D stages suchthat each stage d = 1, .
.
.
, D has access to the in-put instance along with the classifications from allprevious stages, y?(d).
Each stage of the pipelinemodel uses a feature vector generating procedure70?
(d)(x, y?
(0), .
.
.
, y?
(d?1)) ?
x(d) to learn a hypoth-esis h(d).
Once each stage of the pipelined classifieris learned, predictions are made sequentially, wherey?
= h(x) =?argmaxy?
?Y(d)f (d)(x(d), y?
)?Dd=1Each pipeline stage requires a classifier whichmakes multiple interdependent predictions based oninput from multiple sentence elements x ?
X1 ??
?
?
?
Xnx using a structured output space, y(d) ?Y(d)1 ?
?
?
?
?
Y(d)ny .
More specifically, segmenta-tion makes a prediction for each sentence word overY ?
{begin, inside, outside} and constraints areenforced between predictions to ensure that an in-side label can only follow a begin label.
Entity clas-sification begins with the results of the segmenta-tion classifier and classifies each segment into Y ?
{person, location, organization}.
Finally, rela-tion classification labels each predicted entity pairwith Y ?
{located in, work for, org based in,live in, kill} ?
{left, right}+ no relation.The data used for empirical evaluation was takenfrom (Roth and Yih, 2004) and consists of 1436 sen-tences, which is split into a 1149 (80%) sentencetraining set and a 287 (20%) sentence testing setsuch that all have at least one active relation.
SR-WLs are provided by (Pantel and Lin, 2002) andexperiments were conducted using a custom graphi-cal user interface (GUI) designed specifically for theIFSC protocol.
The learning algorithm used for eachstage of the classification task is a regularized vari-ant of the structured Perceptron (Collins, 2002).
Re-sources used to perform experiments are available athttp://L2R.cs.uiuc.edu/?cogcomp/.We extract features in a method similar to (Rothand Small, 2008), except that we do not includegazetteer features in ?
(d)0 as we will include thistype of external information interactively.
Secondly,we use SRWL features as introduced.
The segmen-tation features include the word/SRWL itself alongwith the word/SRWL of three words before and twowords after, bigrams of the word/SRWL surround-ing the word, capitalization of the word, and capi-talization of its neighbor on each side.
Entity clas-sification uses the segment size, the word/SRWLmembers within the segment, and a window of twoword/SRWL elements on each side.
Relation clas-sification uses the same features as entity classifica-tion along with the entity labels, the length of theentities, and the number of tokens between them.4.1 Interactive Querying FunctionWhen using the interactive feature space construc-tion protocol for this task, we require a queryingfunction which captures the hypothesis-driven as-pect of instance selection.
We observed that basingQmargin on the relation stage performs best, whichis not surprising given that this stage makes the mostmistakes, benefits the most from semantic informa-tion, and also has many features which are similar tofeatures from previous stages.
Therefore, we adaptthe querying function described by (Roth and Small,2008) for the relation classification stage and defineour margin for the purposes of instance selection as?relation = mini=1,...,ny[fy+(x, i)?
fy?+(x, i)]where y?
= argmaxy?
?Y\y fy?
(x), the highest scor-ing class which is not the true label, and Y+ =Y\no relation.4.2 Interactive Protocol on Entire Data SetThe first experiments we conduct uses all availabletraining data (i.e.
|S| = 1149) to examine the im-provement achieved with a fixed number of IFSCinteractions.
A single interaction is defined by theexpert selecting a lexical element from a sentencepresented by the querying function and validatingthe associated word list.
Therefore, it is possible thata single sentence may result in multiple interactions.The results for this experimental setup are sum-marized in Table 1.
For each protocol configura-tion, we report F1 measure for all three stages ofthe pipeline.
As our simplest baseline, we first trainusing the default feature set without any semanticfeatures (Lexical Features).
The second baselineis to replace all instances of any lexical elementwith its SRWL representation as provided by (Pan-tel and Lin, 2002) (Semantic Features).
The nexttwo baselines attempt to automatically increase pre-cision by defining each semantic class using only thetop fraction of the elements in each SRWL (PrunedSemantic (top {1/2,1/4})).
This pruning procedureoften results in smaller SRWLs with a more precisespecification of the semantic concept.71Pruned Pruned 50 interactionsLexical Semantic Semantic Semantic Interactive InteractiveFeatures Features (top 1/2) (top 1/4) (select only) (select & validate)Segmentation 90.23 90.14 90.77 89.71 92.24 93.43Entity Class.
82.17 83.28 83.93 83.04 85.81 88.76Relation Class.
54.67 55.20 56.34 56.21 59.14 62.08Table 1: Relative performance of the stated experiments conducted over the entire available dataset.
The interactivefeature construction protocol outperforms all non-interactive baselines, particularly for later stages of the pipelinewhile requiring only 50 interactions.Finally, we consider the interactive feature spaceconstruction protocol at two different stages.
Wefirst consider the case where 50 interactions are per-formed such that the algorithm assumes W?
= W ,that is, the expert selects features for abstraction,but doesn?t perform validation (Interactive (selectonly)).
The second experiment performs the entireprotocol, including validation (Interactive (select &validate)) for 50 interactions.
On the relation ex-traction task, we observe a 13.6% relative improve-ment over the lexical model and a 10.2% relative im-provement over the best SRWL baseline F1 score.4.3 Examination of the Querying FunctionAs stated in section 3.2, an appropriate queryingfunction presents sentences which will result in theexpert selecting features from that example and forwhich the resulting interactions will result in a largeperformance increase.
The former is difficult tomodel, as it is dependent on properties of the sen-tence (such as length), will differ from user to user,and anecdotally is negligibly different for the threequerying functions for earlier interactions.
How-ever, we are able to measure the performance im-provement of interactions associated with differentquerying functions.
For our second experiment, weevaluate the relative performance of the three query-ing functions defined after every ten interactions interms of the F1 measure for relation extraction.
Theresults of this experiment are shown in figure 4,where we first see that the Qrandom generally leadsto the least useful interactions.
Secondly, whileQentropy performs well early, Qmargin works bet-ter as more interactions are performed.
Finally, wealso observe that QBorda exceeds the performanceenvelope of the two constituent querying functions.0.540.550.560.570.580.590.60.610.620.630  10  20  30  40  50relationextraction(F1)interactionsQBordaQentropyQmarginQrandomFigure 4: Relative performance of interactions generatedthrough the respective querying functions.
We see thatQentropy performs well for a small number of interac-tions, Qmargin performs well as more interactions areperformed and QBorda outperforms both consistently.4.4 Robustness to Reduced AnnotationThe third set of experiments consider the relativeperformance of the configurations from the first setof experiments as the amount of available trainingdata is reduced.
To study this scenario, we per-form the same set of experiments with 50 interac-tions while varying the size of the training set (e.g.|S| = {250, 500, 600, 675, 750, 1000}), summariz-ing the results in Figure 5.
One observation is thatthe interactive feature space construction protocoloutperforms all other configurations at all annota-tion levels.
A second important observation is madewhen comparing these results to those presented in(Roth and Small, 2008), where this data is labeledusing active learning.
In (Roth and Small, 2008),once 65% of the labeled data is observed, a perfor-mance level is achieved comparable to training onthe entire labeled dataset.
In this work, an interpo-72lation of the performance at 600 and 675 labeled in-stances implies that we achieve a performance levelcomparable to training on all of the data of the base-line learner while about 55% of the labeled data isobserved at random.
Furthermore, as more labeleddata is introduced, the performance continues to im-prove with only 50 interactions.
This supports thehypothesis that a good representation is often moreimportant than additional training data, even whenthe data is carefully selected.0.350.40.450.50.550.6300  400  500  600  700  800  900  1000relationextraction(F1)labeled dataInteractive (select & verify)Pruned Semantic (top 1/2)Semantic FeaturesLexical FeaturesBaseline (Lexical Features)Figure 5: Relative performance of several baseline al-gorithm configurations and the interactive feature spaceconstruction protocol with variable labeled dataset sizes.The interactive protocol outperforms other baseline meth-ods in all cases.
Furthermore, the interactive protocol (In-teractive) outperforms the baseline lexical system (Base-line) trained on all 1149 sentences even when trainedwith a significantly smaller subset of labeled data.5 Related WorkThere has been significant recent work on designinglearning algorithms which attempt to reduce annota-tion requirements through a more sophisticated an-notation method.
These methods allow the annota-tor to directly specify information about the featurespace in addition to providing labels, which is thenincorporated into the learning algorithm (Huang andMitchell, 2006; Raghavan and Allan, 2007; Zaidanet al, 2007; Druck et al, 2008; Zaidan and Eisner,2008).
Additionally, there has been recent work us-ing explanation-based learning techniques to encodea more expressive feature space (Lim et al, 2007).Amongst these works, the only interactive learningprotocol is (Raghavan and Allan, 2007) where in-stances are presented to an expert and features arelabeled which are then emphasized by the learningalgorithm.
Thus, in this case, although additionalinformation is provided the feature space itself re-mains static.
To the best of our knowledge, this isthe first work that interactively modifies the featurespace by abstracting the FGFs.6 Conclusions and Future WorkThis work introduces the interactive feature spaceconstruction protocol, where the learning algorithmselects examples for which the feature space is be-lieved to be deficient and uses existing semanticresources in coordination with a domain expert toabstract lexical features with their SRWL names.While the power of SRWL abstraction in terms ofsample complexity is evident, incorporating this in-formation is fraught with pitfalls regarding the in-troduction of additional ambiguity.
This interactiveprotocol finds examples for which the domain ex-pert will recognize promising semantic abstractionsand for which those semantic abstraction will signif-icantly improve the performance of the learner.
Wedemonstrate the effectiveness of this protocol on anamed entity and relation extraction system.As a relatively new direction, there are manypossibilities for future work.
The most immedi-ate task is effectively quantifying interaction costswith a user study, including the impact of includ-ing users with varying levels of expertise.
Recentwork on modeling the costs of the active learn-ing protocol (Settles et al, 2009; Haertel et al,2009) provides some insight on modeling costs as-sociated with interactive learning protocols.
A sec-ond potentially interesting direction would be toincorporate other semantic resources such as lexi-cal patterns (Hearst, 1992) or Wikipedia-generatedgazetteers (Toral and Mun?oz, 2006).AcknowledgmentsThe authors would like to thank Ming-Wei Chang,Margaret Fleck, Julia Hockenmaier, Alex Klemen-tiev, Ivan Titov, and the anonymous reviewers fortheir valuable suggestions.
This work is supportedby DARPA funding under the Bootstrap LearningProgram and by MIAS, a DHS-IDS Center for Mul-timodal Information Access and Synthesis at UIUC.73ReferencesAvrim Blum.
1992.
Learning boolean functions in aninfinite attribute space.
Machine Learning, 9(4):373?386.Lois Boggess, Rajeev Agarwal, and Ron Davis.
1991.Disambiguation of prepositional phrases in automat-ically labelled technical text.
In Proceedings of theNational Conference on Artificial Intelligence (AAAI),pages 155?159.Thorsten Brandts and Alex Franz.
2006.
Web 1T 5-gramVersion 1.David Cohn, Les Atlas, and Richard Ladner.
1994.
Im-proving generalization with active learning.
MachineLearning, 15(2):201?222.Michael Collins.
2002.
Discriminative training methodsfor hidden markov models: Theory and experimentswith perceptron algorithms.
In Proc.
of the Conferenceon Empirical Methods for Natural Language Process-ing (EMNLP), pages 1?8.Gregory Druck, Gideon Mann, and Andrew McCallum.2008.
Learning from labeled features using general-ization expectation criteria.
In Proc.
of InternationalConference on Research and Development in Informa-tion Retrieval (SIGIR), pages 595?602.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
MIT Press.Robbie Haertel, Kevin D. Seppi, Eric K. Ringger, andJames L. Carroll.
2009.
Return on investment foractive learning.
In NIPS Workshop on Cost SensitiveLearning.Marti A. Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In Proc.
the In-ternational Conference on Computational Linguistics(COLING), pages 539?545.Yifen Huang and Tom M. Mitchell.
2006.
Text clusteringwith extended user feedback.
In Proc.
of InternationalConference on Research and Development in Informa-tion Retrieval (SIGIR), pages 413?420.Xin Li and Dan Roth.
2005.
Learning question clas-sifiers: The role of semantic information.
Journal ofNatural Language Engineering, 11(4).Siau Hong Lim, Li-Lun Wang, and Gerald DeJong.
2007.Explanation-based feature construction.
In Proc.
ofthe International Joint Conference on Artificial Intelli-gence (IJCAI), pages 931?936.Scott Miller, Jethran Guinness, and Alex Zamanian.2004.
Name tagging with word clusters and discrimi-native training.
In Proc.
of the Annual Meeting of theNorth American Association of Computational Lin-guistics (NAACL), pages 337?342.Patrick Pantel and Dekang Lin.
2002.
Discovering wordsenses from text.
In Proc.
of the International Con-ference on Knowledge Discovery and Data Mining(KDD), pages 613?619.Hema Raghavan and James Allan.
2007.
An interactivealgorithm for asking and incorporating feature feed-back into support vector machines.
In Proc.
of Inter-national Conference on Research and Development inInformation Retrieval (SIGIR), pages 79?86.Dan Roth and Kevin Small.
2008.
Active learning forpipeline models.
In Proceedings of the National Con-ference on Artificial Intelligence (AAAI), pages 683?688.Dan Roth and Wen-Tau Yih.
2004.
A linear program-ming formulation for global inference in natural lan-guage tasks.
In Proc.
of the Annual Conference onComputational Natural Language Learning (CoNLL),pages 1?8.Burr Settles, Mark Craven, and Lewis Friedland.
2009.Active learning with real annotation costs.
In NIPSWorkshop on Cost Sensitive Learning.Simon Tong and Daphne Koller.
2001.
Support vec-tor machine active learning with applications to textclassification.
Journal of Machine Learning Research,2:45?66.Antonio Toral and Rafael Mun?oz.
2006.
A proposalto automatically build and maintain gazetteers usingwikipedia.
In Proc.
of the Annual Meeting of theEuropean Association of Computational Linguistics(EACL), pages 56?61.H.
Peyton Young.
1974.
An axiomatization of borda?srule.
Journal of Economic Theory, 9(1):43?52.Omar F. Zaidan and Jason Eisner.
2008.
Modeling anno-tators: A generative approach to learning from annota-tor rationales.
In Proc.
of the Conference on EmpiricalMethods for Natural Language Processing (EMNLP),pages 31?40.Omar Zaidan, Jason Eisner, and Christine Piatko.
2007.Using ?annotator rationales?
to improve machinelearning for text categorization.
In Proc.
of the AnnualMeeting of the North American Association of Compu-tational Linguistics (NAACL), pages 260?267.74
