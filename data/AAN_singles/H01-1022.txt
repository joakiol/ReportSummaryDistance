An Evaluation Corpus For Temporal SummarizationVikash Khandelwal, Rahul Gupta, and James AllanCenter for Intelligent Information RetrievalDepartment of Computer ScienceUniversity of MassachusettsAmherst, MA 01003fvikas,rgupta,allang@cs.umass.eduABSTRACTIn recent years, a lot of work has been done in the eld ofTopic Tracking.
The focus of this work has been on iden-tifying stories belonging to the same topic.
This might re-sult in a very large number of stories being reported to theuser.
It might be more useful to a user if a summary of themain events in the topic rather than the entire collection ofstories related to the topic were presented.
Though workon such a ne-grained level has been started, there is cur-rently no standard evaluation testbed available to measurethe accuracy of such techniques.
We describe a scheme fordeveloping a testbed of user judgments which can be usedto evaluate the above mentioned techniques.
The corpusthat we have created can also be used to evaluate single ormulti-document summaries.1.
THE PROBLEMIn recent years, a lot of progress has been made in theeld of Topic Tracking ([2], [3], [8], etc).
The focus of thiswork has been on identifying news stories belonging to thesame topic.
This might result in a very large number ofstories being reported to the user.
It would be more usefulto a user if a summary of the main events/developments inthe topic rather than the entire collection of stories relatedto the topic were presented.
We can formulate the problemas follows.We are given a stream of chronologically ordered and top-ically related stories.
We strive to identify the shifts in thetopic which represent the developments within the topic.For example, consider the topic \2000 Presidential Elec-tions".
On the night of November 7, there were reports ofGore conceding defeat to Bush.
The next morning, therewere reports claiming his retraction of the previous conces-sion.
Most of the stories on the next day would also containold information including details of Gore's rst phone callto Bush.
We want to present only the new development(i.e., Gore's retraction) on the next day..We assume that sentence extracts can identify such topicshifts.
At the very least, they can convey enough informa-tion to a user to keep track of the developments within thattopic.
For example, in Figure 1, the mappings indicate howthe sentences in a story correspond to events.Human judgments are required to evaluate accuracy ofextracts.
The approach usually taken is to have each suchextract evaluated by human beings but such a process is ex-pensive and time consuming.
We need an evaluation corpussimilar to the TDT or TREC corpora that can be used overand over again to do such evaluations automatically.
Wepropose a new scheme for building such a corpus.Summarization evaluation is di?cult because summariescan be created for a range of purposes.
The Tipster SUM-MAC evaluation [7] required human assessors to evaluateeach summary, and most other evaluations have also re-quired human checking of every summary [6].
There areothers who have attempted automatic evaluations ([5], [9])but none of these evaluation schemes captures all the desir-able properties in a summary.The particular problem of summarizing shifts in a newstopic was attacked slightly dierently at a Summer 1999workshop on Novelty Detection [4].
Those eorts towards\new information detection" were a dead end because thegranularity of new information was too small, e.g., a men-tion of a person's age might count as new information evenwhen it is not the focus of the story.
Swan and Allan alsocreated an event-level summary \timeline" ([10], [11]) butthey did not develop any evaluation corpus for their work.This paper is organised as follows.
In Section 2, we dis-cuss the desirable properties of such an evaluation corpus.Section 3 discusses the entire annotation process, as wellas the interesting practical issues, the problems faced andthen the statistics of the corpus that we have built.
Finally,in Section 4, we discuss one possible way of utilising thiscorpus.2.
DESIRABLE PROPERTIES OF THEEVALUATION CORPUSAny evaluation corpus of sentence extracts and eventswhich is to be used for the purpose of evaluating summariesof topic shifts in a news stream should have the followingproperties: It should be possible to identify all new events on aperiodic basis.
This would be required to estimate therecall of a system.The Navy has ordered the discharge of sailor Timothy Mcveighafter he described himself gay in his America online user profile.Civil rights groups and gay campaigners are outraged.Mcveigh, who?s no relation of the convicted Oklahoma bomber,is Lodging an appeal.Paul Miller has more.Timothy R. Mcveigh put "gay" in the marital status part of an aolFor "the world," I am Paul Miller in Washington.user profile.He did not use his full name or say he was in the Navy, referringto himself only as "Tim of Honolulu".The Navy?s personnel department says that?s violation of theClinton administration?s Don?t ask/Don?t tell policy of Homo-sexuals in the military and Mcveigh has been dismissed.Many people are upset that the Navy asked aol for informationabout the supposedly anonymous user and, a Naval investigat-or says, the online service provided it.Gay rights groups say it?s discrimination.Privacy advocates say it?s a breach of confidentiality.Sailor Mcveigh dis-charged from NavyNavy claims that heviolated "Don?t ask/Don?t tell" policy.Discrimination againstgays.Breach of privacy byNavy.eventNot related to anyFigure 1: An example showing how sentence extracts can indicate events It should be possible to quantify the precision of asummary, i.e., it should be possible to nd the pro-portion of relevant sentences in the summary, It should be possible to identify redundancy in thesystem output being evaluated.
There should be someway of assigning a marginal utility to sentences con-taining relevant but redundant information It should be possible to quantify the \usefulness" ofa summary taking recall, precision as well as redun-dancy into account. Sentence boundaries should be uniquely identied(though they need not be perfect) because the aimof the system is to identify the relevant portions inthe summary.3.
BUILDING AN EVALUATION CORPUS3.1 The annotation processWe collect a stream of stories related to a certain topicfrom the TDT-2 corpus of stories from January 1 to June30 1998.
We used stories that were judged \on-topic" byannotators from LDC.
The topics were selected from theTDT 1998 and 1999 evaluations.
The stories are parsed toobtain sentence boundaries and all the sentences are givenunique identiers.
We proceed with collecting the humanjudgments in the following four steps.1.
Each judge reads all the stories and identies the im-portant events.2.
The judges sit together to merge the events identiedby them, to form a single list of events for that topic.All the events are given unique identiers.3.
Each judge goes through the stories again, connectingsentences to the relevant events.
Obviously, not allsentences need to be related to any event.
However,if some sentence is relevant to more than one event, itis linked to all those events.4.
Another judge now veries the mapping between thesentences and the events.
This gives us the nal map-ping from sentences to events.This way we obtain all the events mentioned within astory and we can also nd out the events which nd theirrst mention within this story.
The advantage of buildingthe evaluation corpus in this way is that these judgmentscan be used both for summarizing topic shifts as well assummarizing any given story by itself.We have built a user interface in Java to allow judges to dothe above work systematically.
Figure 2 shows a snapshotof the interface used by the judges.3.2 Statistics of the judgments obtainedWe have obtained judgments for 22 topics.
Three judgesworked on each topic.
We summarize the results of theannotation process for a subset of the topics in Table 1.We dene the interjudge agreement for an event to be theratio of the number of sentences linked to that event, asagreed upon by the third judge, to the number of sentencesin the union of the sentences individually marked by therst two judges for that event.
For a topic, the interjudgeagreement is dened to be the average of the agreement forall the events in that topic.
It is to be noted that the Kappastatistic is not applicable here in any standard form.We found a large variance in the number of sentenceslinked to dierent events.
As an example, in Table 2, weshow the statistics for a group of news stories describingFigure 2: A snapshot of the user interface used for annotating the topicsTopic id # of # of Time taken Inter-judgestories events (in hours) Agreement20008 49 10 4.5 0.9120020 34 23 4.5 0.9820021 48 9 2.5 0.9720022 27 10 3.5 0.8520024 38 12 2.75 0.9820026 68 11 2.5 0.8720031 34 15 2.5 0.6220041 24 11 2 0.9420042 28 14 2.5 0.6620057 19 9 2 0.6620065 57 16 2.33 0.9420074 51 13 3 0.96Average 39.75 12.75 2.88 0.86Table 1: Annotation statistics for some of the topicsthe damage due to tornados in Florida.
We see that event 5(\Relief agencies needed more than $300,000 to provide re-lief") is linked to 4 sentences while event 1 (\At least 40 peo-ple died in Florida due to 10-15 tornados.")
is linked to 43sentences.
We may be able to use the number of sentenceslinked to a event as an indicator of the weight/importanceof the event.We have divided our corpus into two parts - one each fortraining and testing respectively.
Each part consists of 11topics.
Care was taken to ensure that both the parts hadtopics of roughly the same size and time of occurrence.
Thestatistics of both parts of the corpus are given in Table 3.3.3 Problems faced Sometimes our sentence parser broke up a valid sen-tence into multiple parts.
One judge linked only theEvent id # of Inter-judgesentences Agreement1 43 1.02 9 1.03 33 0.974 8 1.05 4 0.86 5 1.07 14 1.08 19 1.09 9 1.0Table 2: Variance in the number of sentences linkedto dierent events for topic 20021relevant part of the sentence to the correspondingevent while another linked all the parts to that event.This happened in the case of three of the topics (top-ics 20031, 20042 and 20057) before we detected theproblem. Sometimes when similar sentences occur in dierentstories, one of the judges neglected the later occur-rences of the sentence.3.4 Interesting issues/judges?
commentsWe asked the judges for feedback on the annotation pro-cess and the di?culties faced.
Here are some of the inter-esting issues which cropped up : Some ideas/events cannot be covered by any singlesentence but only by a group of sentences.
By them-selves, none of the sentences might be relevant to theevent.
For example, Suppose, the event is The Navyand AOL contradict each other and we have two sen-tences - \the navy has said in sworn testimony thatTraining Test AllNumber of topics 11 11 22Number of stories 474 470 944per topic 43.1 42.7 42.9Number of events 162 181 343per topic 14.7 16.5 15.6Number of sentences 8043 9006 17049per topic 731.2 818.7 775.0per story 17.0 19.2 18.1O-event sentences 72% 70% 71%Single-event sentences 24% 26% 25%Multi-event sentences 4% 4% 4%Table 3: Characteristics of the corpus.
All numbersexcept for the number of topics are averaged overall topics included in that column.this did happen."
and \america online is saying thisnever happened."
Clearly, any one sentence does notadequately represent the event.
This can be easilytaken care of by considering groups of sentences ratherthan single sentences. Abstract ideas : Sometimes the meaning of individ-ual sentences is totally dierent from overall idea theyconvey.
Satirical articles are an example of this.
Thesekind of ideas cannot be represented by sentence ex-tracts.
We omitted such events. Sometimes dierent stories totally contradict eachother.
For example, some stories (on the same day)claim a lead for Bush while others claim Gore to be farahead.
This is more of a summarization issue thoughand need not be dealt with while building the evalua-tion corpus.4.
USING THE EVALUATION CORPUSWe have used the corpus for evaluating our system whichproduces temporal summaries in news stream ([1]).
Theproblem of temporal summarization can be formalized asfollows.
A news topic is made up of a set of events andis discussed in a sequence of news stories.
Most sentencesof the news stories discuss one or more of the events in thetopic.
Some sentences are not germane to any of the events.Those sentences are called \o-event" sentences and con-trast with \on-event" sentences.
The task of the system isto assign a score to every sentence that indicates the impor-tance of the sentence in the summary.
This scoring yieldsa ranking on all sentences in the topic, including o- andon-event sentences.We will use measures that are analogues of recall andprecision.
We are interested in multiple properties: Useful sentences are those that have the potential tobe a meaningful part of the summary.
O-event sen-tences are not useful, but all other sentences are. Novel sentences are those that are not redundant|i.e., are new in the presentation.
The rst sentenceabout an event is clearly novel, but all following sen-tences discussing the same event are not.Figure 3: nu-recall vs nu-precision plot for the taskof summarizing topic shifts in a news stream Size of the summary is a typical measure used in sum-marization research and we include it here.Based on those properties, we could dene the followingmeasure to capture the combination of usefulness and nov-elty:nu   recall =PI(r(ei) > 0)Enu  precision =PI(r(ei) > 0)Srwhere Sris the number of sentences retrieved, E is thenumber of events in the topic, eiis event number i (1  i E), r(ei) is the number of sentences retrieved for event ei,I(exp) is 1 if exp is true and 0 if not.
All summations areas i ranges over the set of events.
Note that Sr6=Pr(ei)since completely o-topic sentences might be retrieved.The nu-recall measure is the proportion of the events thathave been mentioned in the summary, and nu-precision isthe proportion of sentences retrieved that are the rst men-tions of an event.We used this measure to evaluate the performance ofour system over the entire training corpus.
The results forthe training corpus are shown in the nu-recall/nu-precisiongraph in gure 3.
This work is described in detail else-where([1]).This is just one of the possible ways of using the corpus.We can dene a number of other similar measures whichcould be easily computed using the data provided by such acorpus.
These same measures can also be used to evaluate asystem producing single or multi-document summaries too.5.
FUTURE WORKWe intend to complete collecting user judgments for moretopics soon.
After analyzing the reliablity of these judg-ments and correcting the few mistakes that we had madeinitially, we will collect annotations for more topics.
Ini-tially, we had used a simple barebones sentence parser, sincethat is mostly su?cient for the work such a corpus would beput to.
Nevertheless, in future annotations, we will need toimprove the sentence parser.
We intend to continue usingthese judgments to evaluate the performance of the systemsthat we are currently building to identify and summarizetopic shifts in news streams.AcknowledgementsThis material is based on work supported in part by theLibrary of Congress and Department of Commerce undercooperative agreement number EEC-9209623 and in partby SPAWARSYSCEN-SD grant number N66001-99-1-8912.Any opinions, ndings and conclusions or recommendationsexpressed in this material are the author(s) and do not nec-essarily reect those of the sponsor.6.
REFERENCES[1] J. Allan, R. Gupta, and V. Khandelwal.
TemporalSummaries of News Topics.
Proceedings of SIGIR2001 Conference, New Orleans, LA., 2001.
[2] J. Allan, V. Lavrenko, D. Frey, and V. Khandelwal.UMASS at TDT2000.
TDT 2000 Workshop notebook,2000.
[3] J. Allan, R. Papka, and V. Lavrenko.
On-line NewEvent Detection and Tracking.
Proceedings of SIGIR1998, pp.
37-45, 1998.
[4] J. Allan et al Topic-based novelty detection.
1999Summer Workshop at CLSP Final Report.
Availableat http://www.clsp.jhu.edu/ws99/tdt, 1999.
[5] J. Goldstein, M. Kantrowitz, V. Mittal, andJ.
Carbonell.
Summarizing text documents: SentenceSelection and Evaluation Metrics.
Proceedings ofSIGIR 1999, August 1999.
[6] H. Jing, R. Barzilay, K. McKeown, and M. Elhadad.Summarization Evaluation Methods: Experimentsand Analysis.
Working notes, AAAI SpringSymposium on Intelligent Text Summarization,Stanford, CA, April, 1998.
[7] Inderjeet Mani and et al The TIPSTER SUMMACText Summarization Evaluation Final Report.
1998.
[8] R. Papka, J. Allan, and V. Lavrenko.
UMASSApproaches to Detection and Tracking at TDT2.Proceedings of the DARPA Broadcast NewsWorkshop, Herndon,VA, pp.
111-125, 1999.
[9] D. R. Radev, H. Jing, and M. Budzikowska.Summarization of multiple documents: clustering,sentence extraction, and evaluation.
ANLP/NAACLWorkshop on Summarization, Seattle, WA, 2000.
[10] R. Swan and J. Allan.
Extracting Signicant TimeVarying Features from Text.
Proceedings of the EighthInternational Conference on Information andKnowledge Management, pp.38-45, 1999.
[11] R. Swan and J. Allan.
Automatic Generation ofOverview Timelines.
Proceedings of SIGIR 2000Conference, Athens, pp.49-56, 2000.
