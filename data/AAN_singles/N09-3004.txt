Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 19?24,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsSentence Realisation from Bag of Words with dependency constraintsKarthik Gali, Sriram VenkatapathyLanguage Technologies Research Centre,IIIT-Hyderabad, Hyderabad, India{karthikg@students,sriram@research}.iiit.ac.inAbstractIn this paper, we present five models for sentencerealisation from a bag-of-words containing mini-mal syntactic information.
It has a large varietyof applications ranging from Machine Translationto Dialogue systems.
Our models employ simpleand efficient techniques based on n-gram Languagemodeling.We evaluated the models by comparing the syn-thesized sentences with reference sentences usingthe standard BLEU metric(Papineni et al, 2001).We obtained higher results (BLEU score of 0.8156)when compared to the state-of-art results.
In fu-ture, we plan to incorporate our sentence realiser inMachine Translation and observe its effect on thetranslation accuracies.1 IntroductionIn applications such as Machine Translation (MT)and Dialogue Systems, sentence realisation is a ma-jor step.
Sentence realisation involves generating awell-formed sentence from a bag of lexical items.These lexical items may be syntactically related toeach other.
The level of syntactic information at-tached to the lexical items might vary with applica-tion.
In order to appeal to the wide range of applica-tions that use sentence realisation, our experimentsassume only basic syntactic information, such as un-labeled dependency relationships between the lexi-cal items.In this paper, we present different models for sen-tence realisation.
These models consider a bag ofwords with unlabelled dependency relations as inputand apply simple n-gram language modeling tech-niques to get a well-formed sentence.We now present the role of a sentence realiserin the task of MT.
In transfer-based approaches forMT1 (Lavie et al, 2003), the source sentence isfirst analyzed by a parser (a phrase-structure or adependency-based parser).
Then the source lexicalitems are transferred to the target language using abi-lingual dictionary.
The target language sentenceis finally realised by applying transfer-rules that mapthe grammar of both the languages.
Generally, thesetransfer rules make use of rich analysis on the sourceside such as dependency labels etc.
The accuracy ofhaving such rich analysis (dependency labeling ) islow and hence, might affect the performance of thesentence realiser.
Also, the approach of manuallyconstructing transfer rules is costly, especially fordivergent language pairs such as English and Hindior English and Japanese.
Our models can be usedin this scenario, providing a robust alternative to thetransfer rules.A sentence realiser can also be used in the frame-work of a two-step statistical machine translation.In the two-step framework, the semantic transferand sentence realisation are decoupled into indepen-dent modules.
This provides an opporunity to de-velop simple and efficient modules for each of thesteps.
The model for Global Lexical Selection andSentence Re-construction (Bangalore et al, 2007)is one such approach.
In this approach, discrimi-native techniques are used to first transfer semanticinformation of the source sentence by looking at thesource sentence globally, this obtaining a accuratebag-of-words in the target language.
The words inthe bag might be attached with mild syntactic infor-mation (ie., the words they modify) (Venkatapathyand Bangalore, 2007).
We propose models that take1http://www.isi.edu/natural-language/mteval/html/412.html19this information as input and produce the target sen-tence.
We can also use our sentence realiser as anordering module in other approaches such as (Quirket al, 2005), where the goal is to order an unorderedbag (of treelets in this case) with dependency links.In Natural Language Generation applicationssuch as Dialogue systems etc, the set of conceptsand the dependencies between the concepts is ob-tained first which is known as text planning.
Theseconcepts are then realized into words resulting in abag of words with syntactic relations (Bangalore andRambow, 2000).
This is known as sentence plan-ning.
In the end, the surface string can be obtainedby our models.In this paper, we do not test our models with anyof the applications mentioned above.
However, weplan to test our models with these applications, es-pecially on the two-stage statistical MT approachusing the bag-of-words obtained by Global Lexi-cal Selection (Bangalore et al, 2007),(Venkatapathyand Bangalore, 2007).
Here, we test our models in-dependent of any application, by beginning with agiven bag-of-words (with dependency links).The structure of the paper is as follows.
We givean overview of the related work in section 2.
In sec-tion 3, we talk about the effect of dependency con-straints and gives details of the experimental setup insection 4.
In section 5, we describe about the exper-iments that have been conducted.
In section 6, ourexperimental results are presented.
In section 7, wetalk about the possible future work and we concludewith section 8.2 Related WorkThere have been approaches for sentence realisationsuch as FUF/SURGE (Elhadad, 1991), OpenCCG(White, 2004) and XLE (Crouch et al, 2007)that apply hand-crafted grammars based on partic-ular linguistic theories.
These approaches expectrich syntactic information as input in order to re-alise the sentence.
There are other approaches inwhich the generation grammars are extracted semi-automatically (Belz, 2007) or automatically (such asHPSG (Nakanishi and Miyao, 2005), LFG (Cahilland van Genabith, 2006; Hogan et al, 2007) andCCG (White et al, 2007)).
The limitation of theseapproaches is that these cannot be incorporated intoa wide range of applications as they rely on richsyntactic information for generation.
On the con-trary, we use simple n-gram models to realise (or lin-earize) a bag-of-words where the only informationavailable is the presence of various links between thewords.Our work is similar to a recently published workby Guo (Guo et al, 2008).
They use n-gram modelsto realise sentences from the f-structures of HPSG(equivalent to labeled dependency structure).
Theirmodels rely heavily on the dependency relation la-bels (also called grammatical roles) available inHPSG.
However, the dependency role information(of any dependency formalism) is either not read-ily available in a variety of applications in NLP.
Wepropose to explore the realisation of a sentence us-ing minimal syntactic information.
Apart from de-pendency links, we also make use of part-of-speechtags which are easily available and hence, our sen-tence realiser can be plugged much easily into var-ious applications.
Guo (Guo et al, 2008) conducttheir experiments by considering gold data as input.Apart from using gold data as input, we also con-duct experiments by assuming noisy input data totest the robustness of our models.
The search al-gorithm used by both Guo and us is locally greedyi.e., we compute the best string at every node.
Guouses the Viterbi algorithm to get best string whereaswe consider and score all permutations to obtain thebest string.There has been burgeoning interest in the prob-abilistic models for sentence realisation, especiallyfor realisation ranking in a two stage sentence real-isation architecture where in the first stage a set ofsentence realisations are generated and then a real-isation ranker will choose the best of them (Banga-lore and Rambow, 2000).One major observation in our experiments wasthat the POS tags held immensely in the task of sen-tence realisation.3 Effect of Dependency ConstraintsThere is a major advantage in using dependencyconstraints for sentence realisation.
The searchspace reduces drastically when the constraints areapplied.
These constraints state that the realised sen-tences should be projective with respect to the de-20pendency structure (unordered) of the input bag-of-words ie.., any word and its children in the depen-dency tree should project as a contiguous unit in therealised sentence.
This is a safe assumption to makeas the non-projectivity in English is only used toaccount for Long-Distance Dependencies and suchcases are low in number (Guo et al, 2008).is goingRam to schoolFigure 1: Bag of words with dependency constraintsand head markedWe now present an example to show how the de-pendency constraints reduce the search space.
Forexample, consider an unordered dependency tree inFigure 1, which has five words.
If we don?t use theconstraints provided by the dependency tree then thesearch space is 5!
(120).
But, if we use the con-straints provided by the dependency tree then thesearch space is 2!
+ 4!
= 28.
There is a huge reduc-tion in the search space if we use the constraints pro-vided by the dependency tree.
Further, it has beenshown in (Chang and Toutanova, 2007) that apply-ing the constraints also aids for the synthesis of bet-ter constructed sentences.4 Experimental Set-upFor the experiments, we use the WSJ portion of thePenn tree bank (Marcus et al, 1993), using the stan-dard train/development/test splits, viz 39,832 sen-tences from 2-21 sections, 2416 sentences from sec-tion 23 for testing and 1,700 sentences from sec-tion 22 for development.
The input to our sen-tence realiser are bag of words with dependencyconstraints which are automatically extracted fromthe Penn treebank using head percolation rules usedin (Magerman, 1995), which do not contain any or-der information.
We also use the provided part-of-speech tags in some experiments.In a typical application, the input to the sentencerealiser is noisy.
To test the robustness of our modelsin such scenarios, we also conduct experiments withnoisy input data.
We parse the test data with an un-labelled projective dependency parser (Nivre et al,2006) and drop the order information to obtain theinput to our sentence realiser.
However we still usethe correct bag of words.
We propose to test this as-pect in future by plugging our sentence realiser inMachine Translation.Table 1 shows the number of nodes having a par-ticular number of children in the test data.Children countNodes Children countNodes0 30219 5 10171 13649 6 6852 5887 7 2693 3207 8 1064 1526 > 8 119Table 1: The number of nodes having a particularnumber of children in the test dataFrom Table 1, we can see that more than 96% ofthe internal nodes of the trees contain five or lesschildren.
It means that for almost all the nodes, thereordering complexity is minimal.
This makes thisapproach very feasible if the order of a sub-tree iscomputed after the order of the sub-trees of its chil-dren is fixed.
Hence, the approaches that we presentin the next section use bottom-up traversal of thetree.
During the traversal, the appropriate order ofevery sub-tree is fixed.5 ExperimentsThe task here is to realise a well formed sentencefrom a bag of words with dependency constraints(unordered dependency tree) for which we proposefive models using n-gram based Language modelingtechinque.
We train the language models of order 3using Good-Turning smoothing on the training dataof Penn Treebank.5.1 Model 1 : Sentential Language ModelWe traverse the tree in bottom up manner and findthe best phrase at each subtree.
The best phrase cor-responding to the subtree is assigned to the root nodeof the sub-tree during the traversal.Let the node n have N children represented as ci(1 < i < N ).
During the bottom up traversal, the21children ci are assigned best phrases before process-ing node n. Let the best phrases corresponding to thechildren be p(ci).
The best phrase corresponding tothe node n is computed by exploring the permuta-tions of n and the best phrases p(ci) correspondingto the children ci.
The total number of permutationsthat are explored are (N+1)!.
A sentential languagemodel is applied on each of the candidate phrases toselect the best phrase.p(n) = bestPhrase ( perm (n, ?
i p(ci)) o LM )(1)In Sentential Language Model, we used a LM thatis trained on complete sentences of the training cor-pus to score the permutations.5.2 Model 2 : Subtree-type based LanguageModels(STLM)The major problem with model 1 is that we are us-ing a common sentential language model (trained oncomplete sentences) to score phrases correspondingto various sub-tree types.
In this model, we builddifferent LMs for phrases corresponding to differentsubtree-types.To build STLMs, the training data is parsed first.Each subtree in the parse structure is representedby the part-of-speech tag of its head.
Different lan-guage models are created for each of the POS tags.We have 44 different language models each corre-sponding to a particular POS tag.
For example, aIN language model contains phrases like in hour, ofchaos, after crash, in futures, etc and VBD languagemodel contains phrases like were criticized, neverresumed while training.So, in this model we realise a sentence from aunordered dependency tree by traversing the depen-dency tree in bottom-up manner as we did in model1; but while scoring the permuted phrases we usedifferent language models for subtrees headed bywords of various pos tags.p(n) = bestPhrase ( perm (n, ?
i p(ci)) o LMPOS(n) )(2)Here, LMPOS(n) represents the language modelassociated with the part-of-speech of the node n.5.3 Model 3 : Head-word STLMIn the models presented earlier, a node and its chil-dren are ordered using the best phrases of the chil-dren.
For example, the best phrase assigned to thenode ?was?
is computed by taking of the permutationof ?was?
and its children ?The equity market?, ?illiq-uid?
and ?.?
and then applying the language model.In model 3, instead of considering best phrases whileordering, the heads of the the children ci are consid-ered.
For example, the best phrase assigned to thenode ?was?
is computed by first permuting the nodes?was?, ?market?, ?illiquid?
and ?.?
and then apply-ing the language models trained on the treelets (headand children) and not on entire sub-trees.The major advantage of using this model is thatorder at a node is independent of the best phrases ofits descendants and also any mistakes in computa-tion of best phrases of descendants doesn?t effect thechoice of reordering decision at a particular node.5.4 Model 4 : POS based STLMWe now experiment by using Part-Of-Speech (POS)tags of words for ordering the nodes.
In the previ-ous approaches, the language models were trainedon the words which were then used to compute thebest strings associated with various nodes.
Here,we order the node and its children using a languagemodel trained on POS tag sequences.
The motiva-tion behind buliding such kind of Language modelsis that it deals with unseen words effectively.
Hence,in this model, the best phrase corresponding to thenode ?was?
is obtained by permuting the POS tagsof the words ?was?, ?market?, ?illiquid and ?.?
whichare ?VBZ?, ?NN?, ?NN?
and ?.?
respectively.
As thebest POS tag sequence might correspond to severalorderings of the treelet, a word based STLM is ap-plied to choose the correct ordering.The major advantages of this model is that it ismore general and it deals with unseen words effec-tively.
Also, it is much faster than earlier models asthis model is a POS tag based model.5.5 Model 5: Head-marked POS based STLMIn POS based STLM, the head of a particular nodeisn?t marked while applying the language model.Hence, all the nodes of the treelet are treated equallywhile applying the LM.
For example, in Figure 2, thestructures of treelets is not taken into account whileapplying the head-POS based language model.
Bothare treated in the same manner while applying TLM.In this model, we experiment by marking the head22information for the POS of the head word whichtreats the treelets in Figure 2 in a different manner toobtain the best phrase.
As the best POS tag sequencemight correspond to several orderings of the treelet,we test various word-based approaches to choose thebest ordering among the many possibilities.
The bestapproach was the one where head-word of the treelethad the POS tag attached to it.VBVBP NNVBPVB NNFigure 2: Two different treelets which would havesame best POS tag sequence6 Results and DiscussionTo evaluate our models, we compare the system gen-erated sentences with reference sentences and getthe BLEU score.
As mentioned in section 4, Weevaluate our models on two different types of in-put.
In the first input type, we have bag of wordswith dependency constraints extracted from tree-bank and in the second input type, the dependencyconstraints among the bag of words are extractedfrom the parser which are noisy.
Table 2 shows theresults of model 1-5.Model Treebank(gold) Parser(noisy)Model 1 0.5472 0.5514Model 2 0.6886 0.6870Model 3 0.7284 0.7227Model 4 0.7890 0.7783Model 5 0.8156 0.8027Table 2: The results of Model 1-5We can observe that in model 1, BLEU score ofthe parser input is high when compared to Treebankinput.
This might be because, the parser input is pro-jective (as we used projective parsing) whereas thetreebank input might contain some non-projectivecases.
In general, for all the models, the results withnoisy dependency links are comparable to the caseswhere gold dependency links are used which is en-couraging.We have taken the Table-3 from (Guo et al,2008), which shows the BLEU scores of differentPaper BLEU scoreLangkilde(2002) 0.757Nakanishi(2005) 0.705Cahill(2006) 0.6651Hogan(2007) 0.6882White(2007) 0.5768Guo(2008) 0.7440Our Model 0.8156Table 3: Comparsion of results for English WSJ sec-tion 23systems on section 23 of PTB.
Its really difficult tocompare sentence realisers as the information con-tained in the input vaires greatly between systems.But, we can clearly see that the our system performsbetter than all the systems.
The main observationsfrom the results are, (1) Searching the entire space ofO(n!)
helps, (2) Treelet LM capture characteristicsof phrases headed by various POS tags, in contrast tosentential LM which is a general LM, (3) POS tagscan play an important role in ordering nodes of a de-pendency structure, (4) The head models performedbetter than the models that used all the nodes of thesub-tree, and (5) Marking the head of a treelet pro-vides vital clues to the language model for reorder-ing.7 Future ExperimentsAlthough the results of the proposed models aremuch higher when compared to other methods, themajor constraint with our models is the computa-tional complexity, which is O(n!).
However, our ap-proach is still tractable because of the low values ofn.
We plan to reduce the search space complexity byusing Viterbi search (Guo et al, 2008), and examinethe drop in results because of that.The models proposed in paper, consider only thelocally best phrases (local to the sub-tree) at everystep.
In order to retain the globally best possibilitiesat every step, we plan to use beam search, where weretain K-best best phrases for every sub-tree.Also, the goal is to test the approach formorphologically-rich languages such as Hindi.Also, it would require us to expand our features set.We also plan to test the factored models.The most important experiment that we plan to23perform is to test our system in the context of MT,where the input is more real and noisy.To train more robust language models, we plan touse the much larger data on a web scale.8 ConclusionIn this paper, we had experimented with five ngrambased models for sentence realisation from bag ofwords with dependency constraints.
We have evalu-ated our models on two different types of input(goldand noisy).
From the results, we can conclude thatthe model ?Marked Head-POS based LM?
worksbest in both cases.AcknowledgmentsThe authors of this work were supported by ILMTgrant 11(10)/2006-HCC(TDIL) and EILMT grant11(9)/2006HCC(TDIL).
We would also like to thankthe four reviewers for their valuable reviews.ReferencesS.
Bangalore and O. Rambow.
2000.
Exploiting a proba-bilistic hierarchical model for generation.
Proceedingsof the 18th conference on Computational linguistics.S.
Bangalore, P. Haffner, and S. Kanthak.
2007.
Statisti-cal Machine Translation through Global Lexical Selec-tion and Sentence Reconstruction.
In Annual Meeting- ACL, volume 45.A.
Belz.
2007.
Probabilistic Generation of WeatherForecast Texts.
In Proceedings of NAACL HLT.A.
Cahill and J. van Genabith.
2006.
RobustPCFG-Based Generation Using Automatically Ac-quired LFG Approximations.
In ANNUAL MEETING-ASSOCIATION FOR COMPUTATIONAL LINGUIS-TICS, volume 44.P.C.
Chang and K. Toutanova.
2007.
A DiscriminativeSyntactic Word Order Model for Machine Translation.Proceedings of the 45th Annual Meeting of the ACL.D.
Crouch, M. Dalrymple, R. Kaplan, T. King,J.
Maxwell, and P. Newman.
2007.
XLE documen-tation.
Available on-line.M.
Elhadad.
1991.
FUF: The universal unifier user man-ual version 5.0.
Department of Computer Science,Columbia University.
New York.Y.
Guo, J. van Genabith, and H. Wang.
2008.Dependency-Based N-Gram Models for General Pur-pose Sentence Realisation.
Proceedings of the 22ndconference on Computational linguistics.D.
Hogan, C. Cafferkey, A. Cahill, and J. van Gen-abith.
2007.
Exploiting Multi-Word Units in History-Based Probabilistic Generation.
In Proceedings of the2007 Joint Conference on Empirical Methods in Natu-ral Language Processing and Computational NaturalLanguage Learning (EMNLP-CoNLL).A.
Lavie, S. Vogel, L. Levin, E. Peterson, K. Probst,A.F.
Llitjo?s, R. Reynolds, J. Carbonell, and R. Cohen.2003.
Experiments with a Hindi-to-English transfer-based MT system under a miserly data scenario.
ACM-TALIP, 2(2).D.M.
Magerman.
1995.
Statistical decision-tree modelsfor parsing.
In Proceedings of the 33rd annual meetingon ACL.
ACL Morristown, NJ, USA.M.P.
Marcus, M.A.
Marcinkiewicz, and B. Santorini.1993.
Building a large annotated corpus of English:the penn treebank.
Computational Linguistics, 19(2).H.
Nakanishi and Y. Miyao.
2005.
Probabilistic modelsfor disambiguation of an HPSG-based chart generator.In Proceedings of the International Workshop on Pars-ing Technology.J.
Nivre, J.
Hall, J. Nilsson, G. Eryigit, and S. Marinov.2006.
Labeled pseudo-projective dependency parsingwith support vector machines.
In Proceedings of theTenth CoNLL.K.
Papineni, S. Roukos, T. Ward, and W.J.
Zhu.
2001.BLEU: a method for automatic evaluation of machinetranslation.
Proceedings of the 40th Annual Meetingon ACL.C.
Quirk, A. Menezes, and C. Cherry.
2005.
De-pendency treelet translation: syntactically informedphrasal SMT.
Proceedings of the 43rd Annual Meet-ing of ACL.S.
Venkatapathy and S. Bangalore.
2007.
Three mod-els for discriminative machine translation using GlobalLexical Selection and Sentence Reconstruction.
InProceedings of SSST, NAACLHLT/AMTA Workshop onSyntax and Structure in Statistical Translation, pages152?159.M.
White, R. Rajkumar, and S. Martin.
2007.
TowardsBroad Coverage Surface Realization with CCG.
InProceedings of the Workshop on Using Corpora forNLG: Language Generation and Machine Translation(UCNLG+ MT).M.
White.
2004.
Reining in CCG Chart Realization.LECTURE NOTES IN COMPUTER SCIENCE.24
