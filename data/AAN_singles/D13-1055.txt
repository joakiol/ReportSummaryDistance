Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 578?589,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsAutomatically Classifying Edit Categories in Wikipedia RevisionsJohannes Daxenberger?
and Iryna Gurevych???
Ubiquitous Knowledge Processing LabDepartment of Computer Science, Technische Universita?t Darmstadt?
Information Center for EducationGerman Institute for Educational Research and Educational Informationhttp://www.ukp.tu-darmstadt.deAbstractIn this paper, we analyze a novel set of fea-tures for the task of automatic edit categoryclassification.
Edit category classification as-signs categories such as spelling error correc-tion, paraphrase or vandalism to edits in a doc-ument.
Our features are based on differencesbetween two versions of a document includ-ing meta data, textual and language propertiesand markup.
In a supervised machine learningexperiment, we achieve a micro-averaged F1score of .62 on a corpus of edits from the En-glish Wikipedia.
In this corpus, each edit hasbeen multi-labeled according to a 21-categorytaxonomy.
A model trained on the samedata achieves state-of-the-art performance onthe related task of fluency edit classification.We apply pattern mining to automatically la-beled edits in the revision histories of differentWikipedia articles.
Our results suggest thathigh-quality articles show a higher degree ofhomogeneity with respect to their collabora-tion patterns as compared to random articles.1 IntroductionDue to its ever-evolving and collaboratively builtcontent, Wikipedia has been the subject of manyNLP studies.
While the number of newly createdarticles in the online encyclopedia declined in thelast few years (Suh et al 2009), the number of editsin existing articles is rather stable.1 It is reasonableto assume that the latter will not change in the near1http://stats.wikimedia.org/EN/TablesDatabaseEdits.htmfuture.
One of the major reasons for the popular-ity of Wikipedia is its up-to-dateness (Keegan et al2013), which in turn requires constant editing activ-ity.
Wikipedia?s revision history stores all changesmade to any page in the encyclopedia in separaterevisions.
Previous studies have exploited revisionhistory data in tasks such as preposition error cor-rection (Cahill et al 2013), spelling error correc-tion (Zesch, 2012) or paraphrasing (Max and Wis-niewski, 2010).
However, they all use different ap-proaches to extract the information needed for theirtask.
Ferschke et al(2013) outline several appli-cations benefiting from revision history data.
Theyargue for a unified approach to extract and classifyedits from revision histories based on a predefinededit category taxonomy.In this work, we show how the extraction andautomatic multi-label classification of any edit inWikipedia can be handled with a single approach.Therefore, we use the 21-category edit classificationtaxonomy developed in previous work (Daxenbergerand Gurevych, 2012).
This taxonomy enables a fine-grained analysis of edit activity in revision histories.We present the results from an automatic classifica-tion experiment, based on an annotated corpus of ed-its in the English Wikipedia.
Additional informationnecessary to reproduce our results, including wordlists and training, development and test data, is re-leased online.2 To the best of our knowledge, thisis the first approach allowing to classify each singleedit in Wikipedia into one or more of 21 differentedit categories using a supervised machine learning2http://www.ukp.tu-darmstadt.de/data/edit-classification578approach.We define our task as edit category classification.An edit is a coherent, local change which modifies adocument and which can be related to certain metadata (e.g.
its author, time stamp etc.).
In edit ca-tegory classification, we aim to detect all n editsekv?1,v with 0 ?
k < n in adjacent versions rv?1, rvof a document (we refer to the older revision as rv?1and to the newer as rv) and assign each of themto one or more edit categories.
There exist at leasttwo main applications of edit category classification:First, a fine-grained classification of edits in collab-oratively created documents such as Wikipedia ar-ticles, scientific papers or research proposals, wouldhelp us to better understand the collaborative writingprocess.
This includes answers to questions aboutthe kind of contribution of individual authors (Whohas added substantial contents?, Who has improvedstylistic issues?)
and about the kind of collabora-tion which characterizes different articles (Liu andRam, 2011).
Second, automatic classification of ed-its generates huge amounts of training data for theabove mentioned NLP systems.Edit category classification is related to the bet-ter known task of document pair classification.In document pair classification, a pair of docu-ments has to be assigned to one or more categories(e.g.
paraphrase/non-paraphrase, plagiarism/non-plagiarism).
Here, the document may be a very shorttext, such as a sentence or a single word.
Appli-cations of document pair classification include pla-giarism detection (Potthast et al 2012), paraphrasedetection (Madnani et al 2012) or text similaritydetection (Ba?r et al 2012).
In edit category clas-sification, we also have two documents.
However,these documents are different versions of the sametext.
This scenario implies certain characteristics fora well-designed feature set as we will demonstrate inthis study.The main contributions of this paper are: First,we introduce a novel feature set for edit categoryclassification.
Second, we evaluate the performanceof this feature set on different tasks within a cor-pus of Wikipedia edits.
We propose the new task ofedit category classification and show that our modelis able to classify edits from a 21-category taxon-omy.
Furthermore, our model achieves state-of-the-art performance in a fluency edit classification task(Bronner and Monz, 2012).
Third, we analyze col-laboration patterns based on edit categories on twosubsets of Wikipedia articles, namely featured andnon-featured articles.
We detect correlations be-tween collaboration patterns and high-quality arti-cles.
This is demonstrated by the fact that featuredarticles have a higher degree of homogeneity withrespect to their collaboration patterns as comparedto random articles.The rest of this paper is structured as follows.
InSection 2, we motivate our experiments based onprevious work.
Section 3 explains our training dataand the features we use for the machine learning ex-periments.
In Section 4, we present and discuss theresults of our experiments.
We also demonstrate anapplication of our classifier model in Section 5 bymining frequent collaboration patterns in the revi-sion histories of different articles.
Finally, we drawa conclusion in Section 6.2 Related WorkWikipedia is a huge data source for generating train-ing data for edit category classification, as all pre-vious versions of each page in the encyclopediaare stored in its revision history.
Unsurprisingly,the number of studies extracting certain kinds ofWikipedia edits keeps growing.
Most of these usemanually defined rules or filters find the right kindof edits.
Among the latter, there are NLP applica-tions such as the detection of lexical errors (Nelkenand Yamangil, 2008), spelling error correction (Maxand Wisniewski, 2010; Zesch, 2012), preposition er-ror correction (Cahill et al 2013), sentence com-pression (Nelken and Yamangil, 2008; Yamangiland Nelken, 2008), summarization (Nelken and Ya-mangil, 2008), simplification (Yatskar et al 2010;Woodsend and Lapata, 2011), paraphrasing (Maxand Wisniewski, 2010; Dutrey et al 2011), tex-tual entailment (Zanzotto and Pennacchiotti, 2010;Cabrio et al 2012), information retrieval (Aji et al2010; Nunes et al 2011) and bias detection (Re-casens et al 2013).Bronner and Monz (2012) define features for thesupervised classification of factual and fluency edits.Their features are calculated both on character- andword-level.
Furthermore, they use features basedon POS tags, named entities, acronyms, and a lan-579Figure 1: An example edit from WPEC labeled with REFERENCE-M, as displayed by Wikimedia?s diff page tool.guage model (word n-grams).
In their experiments,character-level features and named entity featuresshow the highest improvement over the baseline.Vandalism detection in Wikipedia has mostlybeen defined as a binary machine learning task,where the goal is to classify a pair of adjacent re-visions as vandalized or not-vandalized based onedit category features.
In Adler et al(2011), theauthors group these features into meta data (au-thor, comment and time stamp of a revision), rep-utation (author and article reputation), textual (lan-guage independent, i.e.
token- and character-based)and language features (language dependent, mostlydictionary-based).
They carry out cross-validationexperiments on the PAN-WVC-10 corpus (Potthastand Holfeld, 2011).
Classifiers based on reputa-tion and text performed best.
Adler et al(2011)use Random Forests as classifier (Breiman, 2001)in their experiments.
This classifier was also usedin the vandalism detection study of Javanmardi et al(2011) where it outperformed the classifiers basedon Logistic Regression and Naive Bayes.Different to the approach of Bronner and Monz(2012) and previous vandalism classification stud-ies, we built a model which accounts for multi-labeling and a fine-grained edit category system.Our feature set builds upon existing work whileadding a substantial number of new features.3 Experiments3.1 Wikipedia Edit Category CorpusFor our experiments, we used the freely avail-able Wikipedia Edit Category Corpus (WPEC) com-piled in previous work (Daxenberger and Gurevych,2012).
In this corpus, each pair of adjacent revisionsis segmented into one or more edits.
This enablesan accurate picture of the editing process, as an au-thor may perform several independent edits in thesame revision.
Furthermore, edits are multi-labeled,i.e.
each edit is assigned one or more categories.This is important for a precise description of majoredits, e.g.
when an entire new paragraph includingtext, references and markup is added.
There are fourbasic types of edits, namely Insertions, Deletions,Modifications and Relocations.
These are calculatedvia a line-based diff comparison on the source text(including wiki markup).
As previously suggested(Daxenberger and Gurevych, 2012), inside modifiedlines, only the span of text which has actually beenchanged is marked as edit (either Insertion, Dele-tion or Modification), not the entire line.
We ex-tracted the data which is not contained in WPEC(meta data and plain text of rv?1 and rv) using theJava Wikipedia Library (JWPL) with the RevisionToolkit (Ferschke et al 2011).In Daxenberger and Gurevych (2012), we dividethe 21-category taxonomy into text-base (meaning-changing edits), surface (non meaning-changing ed-its) and Wikipedia policy (VANDALISM and RE-VERT) edits.
Among the text-base edits, we includecategories for templates, references (internal and ex-ternal links), files and information, each of whichis further divided into an insertion (I), deletion (D)and modification (M) category.
Surface edits con-sist of paraphrases, spelling and grammar correc-tions, relocations and markup edits.
The latter cate-gory contains all edits which affect markup elementsthat are not covered by any of the other categoriesand is divided into insertions, deletions and modifi-cations.
This includes, for example, apostrophes in'''bold text'''.
We also suggested an OTHER category,which is intended for edits which cannot be labeleddue to segmentation errors.
Figure 1 shows an exam-ple edit from WPEC, labeled with the REFERENCE-580Feature Value ExplanationMetaDataAuthor group user Wikimedia user group of the authorAuthor is registered* true Author is registered (otherwise: IP user)Same author* false Authors of rv and rv?1 are the sameComment length* 0 Number of characters in the commentVulgarism in comment false Comment contains a word from in the vulgarism word listComment is auto-generated false Entire comment has been auto-generatedAuto-generated comment ratio 0 Auto-generated part of comment divided by length of the commentIncorrect comment ratio 0 Out-of-dictionary word count divided by word count in the commentComment n-grams1 ?
Presence or absence of token n-grams in the commentIs revert* false Comment contains a word from in the revert word listIs minor false Revision has been marked as minor changeTime difference* 505 Time difference between rv?1 and rv (in minutes)Number of edits 1 Absolute number of edits in the (rv?1, rv)-pairTextualDiff capitals* 0 Difference in the number of capitalsDiff digits* 0 Difference in the number of digitsDiff special characters* 2 Difference in the number of non-alphanumeric charactersDiff whitespace characters 1 Difference in the number of whitespace charactersDiff characters* 9 Difference in the number of charactersDiff tokens* 1 Difference in the number of whitespace-separated tokensDiff repeated characters 0 Difference in the number of repeated charactersDiff repeated tokens 0 Difference in the number of repeated white-space separated tokensCosine similarity 0 Cosine similarityLevenshtein distance* 9 Levenshtein distanceOptimal string alignment distance 9 Optimal string alignment distance (Damerau-Levenshtein distance)Ratio diff to paragraph characters 0.02 Diff characters divided by the length of the edited paragraphRatio diff to revision characters 0.0005 Diff characters divided by the length of rv?1Ratio diff to paragraph tokens 0.04 Diff tokens divided by the length of the edited paragraphRatio diff to revision tokens 0.0003 Diff tokens divided by the length of rv?1Ratio old to new paragraph 0 Difference in the number of characters in the edited paragraphCharacter n-grams1 p,o,e,t,r,y2 Presence or absence of n-grams of edited charactersToken n-grams1 poetry2 Presence or absence of n-grams of edited tokensSimple edit type Insertion Modification, Insertion, Deletion or RelocationMarkupDiff number m 0 Difference in the number of mDiff type m false Different types of mDiff type context m true3 Different types of m within the immediate context of the editIs covered by m true3 Edit is covered by m in rv?1Covers m false Edit covers m in rv?1Language Diff spelling errors* 0 Difference in the number of out-of-dictionary wordsDiff vulgar words* 0 Difference in the number of tokens contained in vandalism word listSemantic similarity -1 Explicit Semantic Analysis with vector indexes from WiktionaryDiff POS tags* false POS tag sets are symmetrically differentDiff type POS tags* 0 Number of distinct POS tags1 N-gram features are represented as boolean features.2 In this example, n = 1 (unigrams).3 True if m corresponds to internal link, false otherwise.Table 1: List of edit category classification features with explanations.
The values correspond to the the example editfrom Figure 1. m may refer to internal link, external link, image, template or markup element.
Features marked with* have previously been mentioned in Adler et al(2011), Javanmardi et al(2011) or Bronner and Monz (2012).581M category.
WPEC was created in a manual anno-tation study with three annotators.
The overall inter-annotator agreement measured as Krippendorf?s ?
is.67 (Daxenberger and Gurevych, 2012).
The exper-iments in this study are based on the gold standardannotations in WPEC, which have been derived bymeans of a majority vote for each edit.WPEC consists of 981 revision pairs, segmentedinto 1,995 edits.
We define edit category classifica-tion as a multi-label classification task.
For the sakeof readability, in the following we will refer to anedit ekv?1,v as ei, with ei ?
E, where 0 ?
i < 1995and E is the set of all edits.
An edit ei is the basicclassification unit in our task.
Each ei has to be la-beled with a set of categories y ?
C, where C is theset of all edit categories, |C| = 21.3.2 Features for Edit Category ClassificationWe grouped our features into meta data, textual,markup and language features.
An overview andexplanation of all features can be found in Table 1.The scheme we apply to group edit category clas-sification features is similar to the system used byAdler et al(2011).
We re-use some of the featuressuggested by Adler et al(2011), Javanmardi et al(2011) and Bronner and Monz (2012), as marked inTable 1.
Features are calculated on edited text spans.We label the edited text span corresponding to ei inrv?1 as tv?1 and the edited text span in rv as tv.
Inedits which are insertions, we consider tv?1 to beempty, while tv is considered empty for deletions.For Relocations, tv?1 = tv.Table 1 includes the value of each feature for theexample edit from Figure 1.
This edit modifies thelink [[Dactyl|Dactylic]] by adding a speci-fication to the target of that link.
For spell-checking,we use British and US-American English Jazzy dic-tionaries.3 Markup elements are detected by theSweble Wikitext parser (Dohrn and Riehle, 2011).Meta data features We consider the comment,author, time stamp or any other flag (?minorchange?)
of rv as meta data.
The Wikimedia usergroup4 of an author specifies the edit permissions3http://sourceforge.net/projects/jazzydicts4http://meta.wikimedia.org/wiki/User_classesof this user (e.g.
bot, administrator, blocked user).We indicate whether the revision comments or partsof it have been auto-generated.
This happens whena page is blanked, i.e.
all of its content has beendeleted or replaced or when a new page or redirect iscreated (denoted by the Comment is auto-generatedfeature).
Furthermore, edits within a specific sec-tion of an article are automatically marked by addinga prefix with the name of this section to the com-ment of the revision (denoted by the Auto-generatedcomment ratio feature).
Meta data features have thesame value for all edits in a (rv?1, rv)-pair.Textual features Textual features are calculatedbased on a certain property of the changed text.
Ina preprocessing step, any wiki markup inside tv?1and tv is deleted.
As for the example edit from Fig-ure 1, tv?1 would correspond to an empty string andtv would be represented as ?
(poetry)?.
The n-gramfeature spaces are composed of n-grams that arepresent either in tv?1 but not tv, or vice verse.
Char-acter n-grams only contain English alphabet charac-ters, token n-grams consist of words excluding spe-cial characters.Markup features As opposed to textual features,wiki markup features account for the Wikimediaspecific markup elements.
Markup features are cal-culated based on the number and type of a markupelement m and the surrounding context of an edit.Here, m can be a template, an external or internallink, an image or any other element used to describemarkup including HTML tags.
The type of m isdefined by the link target for internal and externallinks and images, by the name of the template fortemplates and by the wiki markup element name formarkup elements.
Markup features are calculated ontext spans tv?1 and tv.
Naturally, wiki markup is notdeleted beforehand.
The edited text spans tv?1 andtv may be located inside a markup elementm (e.g.
alink or a template).
In such cases, our diff algorithmwill not label the entire element m, but rather theactually modified text.
However, such an edit maychange the name of a template or the target of a link(as in the example edit from Figure 1).
We there-fore include the immediate context sv?1 and sv ofeach edit and compare the type of potential markupelements m in sv?1 and sv.
Here, sv (sv?1) is de-fined as tv (tv?1) including all preceding and follow-582Revisions Edits CardinalityTrain 713 1,597 1.20Test 89 229 1.24Dev 89 169 1.21Table 2: Statistics of the training, test and developmentset.
Cardinality is the average number of edit categoriesassigned to an edit.ing characters in rv (rv?1) which are not separatedfrom tv (tv?1) by a boundary character (whitespaceor line break).
The above described features modelwhat is actually edited in the text.
A number of fea-tures are calculated on tv?1 only.
These features aremore likely to inform about where an edit is con-ducted.
They specify whether tv?1 covers (i.e.
con-tains) a certain wiki markup element and vice versa,i.e.
whether tv?1 is located inside a text span thatbelongs to a markup element.Language Language features are calculated on thecontext sv?1 and sv of edits, any wiki markup isdeleted.
For the Explicit Semantic Analysis, we useWiktionary (Zesch et al 2008) and not Wikipediaassuming that the former has a better coverage withrespect to different lexical classes.
POS tagging wascarried out using the OpenNLP POS tagger.5 Thevandalism word list contains a hand-crafted set ofaround 100 vandalism and spam words from variousplaces in the web.3.3 Experimental SetupWe extract features with the help of ClearTK (Ogrenet al 2008).
For the machine learning part, we useWeka (Hall et al 2009) with the Meka6 and Mu-lan (Tsoumakas et al 2010) extensions for multi-label classification.
We use DKPro Lab (Eckart deCastilho et al 2011) to test different parameter com-binations.
We randomly split the gold standard datafrom WPEC into 80% training, 10% test and 10%development set, as shown in Table 2.Multi-label Classification We report the perfor-mance of various machine learning algorithms.
Acomprehensive overview of multi-label classifica-tion algorithms and evaluation measures can be5Maxent model for English, http://opennlp.apache.org6http://meka.sourceforge.netRandomMajorityBRHOMERRAKELThreshold ?
?
.10 .25 .33ExampleAccuracy .09 .13 .50 .44 .53Exact Match .06 .13 .35 .36 .44F1 .09 .13 .55 .47 .56Precision .10 .13 .54 .46 .56Recall .10 .13 .61 .50 .60LabelMacro-F1 .10 .06 .49 .35 .51Micro-F1 .10 .12 .59 .49 .62Ranking One Error .90 .87 .42 .48 .34Table 3: Overall classification results with 3 multi-labelclassifiers and a C4.5 decision tree base classifier, as com-pared to random and majority category baselines.found in Madjarov et al(2012).
Multi-label classi-fication problems are solved by either transformingthe multi-label classification task into one or moresingle-label classification tasks (problem transfor-mation method) or by adapting single-label clas-sification algorithms (algorithm adaption method).Several algorithms have been developed on top ofthe former methods and use ensembles of such clas-sifiers (ensemble methods).
We applied the Bi-nary Relevance approach (BR), a simple transfor-mation method which converts the multi-label prob-lem into |C| binary single-label problems, where |C|is the number of categories.
Hence, this methodtrains a classifier for each category in the corpus(one-against-all).
It is the most straightforward ap-proach when dealing with multi-labeled data.
How-ever, it does not consider possible relationships ordependencies between categories.
Therefore, wetested two more sophisticated methods.
Hierar-chy of multi-label classifiers HOMER (Tsoumakaset al 2008) is a problem transformation method.It accounts for possibly hierarchical relationshipsamong categories by dividing the overall categoryset into a tree-like structure with nodes of small ca-tegory sets of size k and leaves of single categories.Subsequently, a multi-label classifier is applied toeach node in the tree.
Random k-labelsets RAKEL(Tsoumakas et al 2011) is an ensemble method,which randomly chooses l typically small subsetswith k categories from the overall set of catego-ries.
Subsequently, all k-labelsets which are foundin the multi-labeled data set are converted into newcategories in a single-labeled data set using the la-583bel powerset transformation (Trohidis et al 2008).HOMER and BR are among the multi-label clas-sifiers, which Madjarov et al(2012) recommendas benchmark methods.
As underlying single-labelclassification algorithm, we used a C4.5 decisiontree classifier (Quinlan, 1993), as decision tree clas-sifiers yield state-of-the-art performance in the re-lated work.Multi-label Evaluation We denote the set of rel-evant categories for each edit ei ?
E as yi ?C and the set of predicted categories as h(ei).Evaluation measures for multi-label classificationsystems are based on either bipartitions or rank-ings.
Among the former, we report example-based (weighting each edit equally) and label-based(weighting each edit category equally) measures.The accuracy of a multi-label classifier is definedas 1|E|?|E|i=1|h(ei)?yi||h(ei)?yi|, which corresponds to the Jac-card similarity of h(ei) and yi averaged over all ed-its.
We report subset accuracy (exact match), cal-culated as 1|E|?|E|i=1 I , with I = 1 if h(ei) =yi and I = 0 otherwise.
Example-based pre-cision is defined as 1|E|?|E|i=1|h(ei)?yi||h(ei)|, recall as1|E|?|E|i=1|h(ei)?yi||yi|, and F1 as 1|E|?|E|i=12?|h(ei)?yi||h(ei)|+|yi|.For the label-based measures, we report macro-and micro-averaged F1 scores.
As a ranking-basedmeasure, we report one error, which is defined as1|E|?|E|i=1J[arg maxc?Cf(ei, c)] /?
yiK, JexprK = 1 ifexpr is true and JexprK = 0 otherwise.
f(ei, c) de-notes the rank of category c ?
C as predicted bythe classifier.
The one error measure evaluates thenumber of edits where the highest ranked categoryin the predictions is not in the set of relevant cate-gories.
It becomes smaller when the performance ofthe classifier increases.Table 3 shows the overall classification scores.We calculated a random baseline, which multi-labelsedits at random considering the label powerset fre-quencies it has learned from the training data.
Fur-thermore, we calculated a majority category base-line, which labels all edits with the most frequentedit category in the training data.
In Figure 2, welist the results for each category, together with theaverage pair-wise inter-rater agreement (F1 scores).The F1 scores are calculated based on the study wecarried out in Daxenberger and Gurevych (2012).Parameters and Feature selection All parame-ters have been adjusted on the development set us-ing the RAKEL classifier, aiming to optimize accu-racy.
With respect to the n-gram features, we testedvalues for n = 1, 2 and 3.
For comment n-grams,unigrams turned out to yield the best overall per-formance, and bigrams for character and token n-grams.
The word and character n-gram spaces arelimited to the 500 most frequent items, the commentn-gram space is limited to the 1,500 most frequentitems.
To transform ranked output into bipartitions,it is necessary to set a threshold.
This threshold isreported in Table 3 and has been optimized for eachclassifier with respect to label cardinality (averagenumber of labels assigned to edits) on the develop-ment set.
Since most of the traditional feature se-lection methods cannot be applied directly to multi-labeled data, we used the label powerset approach totransform the multi-labeled data into single-labeleddata and subsequently applied ?2.
Feature reduc-tion to the highest-ranked features clearly improvedthe classifier performance on the development set.We therefore limited the feature space to the 150highest-ranked features in our experiments.For the RAKEL classifier, we set l = 42 (twicethe size of the category set) and k = 3.
In HOMER,we used BR as transformation method, random dis-tribution of categories to the children nodes andk = 3.
For all other classifier parameters, we usedthe default settings as configured in Meka respectiveMulan.4 DiscussionThe classifiers significantly outperformed both base-lines.
RAKEL shows best performance for almostall measures in Table 3.
The simpler BR approach,which assumes no dependencies between categories,still outperforms HOMER.We trained and tested the classifier with differentfeature groups (see Table 1), to analyze the impor-tance of single types of features.
As shown in Fig-ure 2, textual features had the highest impact on clas-sification performance.
On the opposite, languagefeatures played a minor role in our experiments.Among the highest ranked individual features for theentire set of categories, we find textual (Levenshteindistance, Simple edit type), markup (Diff number584Revert(30)Vandalism(19)Paraphrase(5)Relocation(2)Markup-I (27)Markup-M(2)Markup-D(10)Spelling/Gr.
(20)Reference-I(31)Reference-M(12)Reference-D(12)Information-I(35)Information-M(28)Information-D(18)Template-I(10)Template-D(2)File-I (2)File-D(1)Other (16)00.20.40.60.81F1scoreHuman Classifier Textual Markup Meta Data LanguageFigure 2: F1 scores of RAKEL with C4.5 as base classifier for individual categories.
We add human inter-annotatoragreement as average pair-wise F1 scores as well as F1 scores for classifiers trained and tested on single featuregroups, cf.
Table 1.
The number of edits labeled with each category in the test set is given in brackets.
The FILE-Mand TEMPLATE-M categories are omitted in this Figure, as they had no examples in the development or test set.markup elements) and meta data (Number of edits)features.Bronner and Monz (2012) report an accuracyof .88 for their best performing system on the bi-nary classification task of distinguishing fluency andfactual edits.
The best performing classifier intheir study was Random Forests (Breiman, 2001).To compare our features with their approach, wemapped the 21 edit categories from Daxenberger andGurevych (2012) to the binary category set (factualvs.
fluency) of Bronner and Monz (2012).
Edits la-beled as SPELLING/GRAMMAR, MARKUP, RELO-CATION and PARAPHRASE are considered fluencyedits, the remaining categories factual edits.
We re-moved all edits labeled as OTHER, REVERT or VAN-DALISM from WPEC.
After applying the categorymapping, we deleted all edits which were labeledwith both the fluency and factual category.
The lat-ter may happen due to multi-labeling.
This resultedin 1,262 edits labeled as either fluency or factual.On the 80% training split from Table 2, we traineda Random Forests classifier with the optimized fea-ture set and feature reduction as described in Sec-tion 3.3.
The number of trees was set to 100, withunlimited depth.
On the remaining data (test anddevelopment split), we achieved an accuracy of .90.Although we did not use the same data set as Bron-ner and Monz (2012), this result suggests that ourfeature set is suited for related tasks such as fluencydetection.With respect to vandalism detection in Wikipedia,state-of-the-art systems have a performance ofaround .82 to .85 AUC-PR on the English Wikipedia(Adler et al 2011).
We suspect that the low perfor-mance of our system for Vandalism edits is mostlydue to a lower amount of training data, a higher skewin the training and test data and the fact that we didnot include features which inform about future ac-tions (e.g.
whether a revision is reverted).Error Analysis Sparseness is a major problemfor some of the 21 categories, as shown in Fig-ure 2 by categories such as FILE-D, TEMPLATE-D, MARKUP-M or PARAPHRASE which have onlyvery few examples in training, development andtest set.
Categories with low inter-annotator agree-ment in WPEC such as MARKUP-M, PARAPHRASEor OTHER also yielded low classification accuracy.We analyzed frequent errors of the classifier withthe help of a confusion matrix.
PARAPHRASE ed-its have been confused with INFORMATION-M bythe classifier.
Furthermore, the classifier had prob-lems to distinguish between VANDALISM and RE-VERT as well as INFORMATION-I.
Generally, modi-fications as compared to insertions or deletions per-form worse.
All of the classifiers we tested, build585their predictions by thresholding over a ranking, cf.Table 3.
This generates a source of errors, becausethe classifier is not able to make a prediction, if itdoes not have enough confidence for any of the cate-gories.
The imbalance of the data, because of thehigh skew in the category distribution, is anotherreason for classification errors.
In ambiguous cases,the classifier will be biased toward the category withmore examples in the training data.5 A closer look at edit sequences: Miningcollaboration patternsAn edit category classifier allows us to label en-tire article revision histories.
We applied the best-performing model from Section 3.3 trained on theentire WPEC to automatically classify all edits in theWikipedia Quality Assessment Corpus (WPQAC)as presented in previous work (Daxenberger andGurevych, 2012).
WPQAC consists of 10 fea-tured and 10 non-featured articles7, with an over-all number of 21,578 revisions (9,986 revisionsfrom featured articles and 11,592 from non-featuredarticles), extracted from the April 2011 EnglishWikipedia dump.
The articles in WPQAC are care-fully chosen to form comparable pairs of featuredand non-featured articles, which should reduce thenoise of external influences on edit activity suchas popularity or visibility.
In Daxenberger andGurevych (2012), we have shown significant dif-ferences in the edit category distribution of arti-cles with featured status before and after the articleswere featured.
We concluded that articles becomemore stable after being featured, as shown by thehigher number of surface edits and lower number ofmeaning-changing edits.Different to our previous approach which is basedon the mere distribution of edit categories, in thepresent study we include the chronological order ofedits and use a 10 times larger amount of data for ourexperiments.
We segmented all adjacent revisionsin WPQAC into edits, following the approach ex-plained in Daxenberger and Gurevych (2012).
Dur-ing the classification process, we discarded revisionswhere the classifier could not assign any of the 21edit categories with a confidence higher than the7http://en.wikipedia.org/wiki/Wikipedia:FAthreshold, cf.
Table 3.
This resulted in 17,640 re-maining revisions.
We applied a sequential patternmining algorithm with time constraints (Hirate andYamana, 2006; Fournier-Viger et al 2008) to thedata.
The latter is based on the PrefixSpan algorithm(Pei et al 2004).
Calculations have been carried outwithin the open-source SPMF Java data mining plat-form.8We created one time-extended sequence databasefor the 10 featured articles and one for the 10 non-featured articles.
The sequence databases consistof one row per article.
Each row is a chronologi-cally ordered list of revisions.
Each revision is rep-resented by the itemset of all edit categories for alledits in that revision (in alphabetical order).The output of the algorithm are sequential pat-terns with time constraints.
To obtain meaningfulresults, we constrained the output with the follow-ing parameters:?
Minimum support: 1 (the patterns have to bepresent in each article)?
Time interval allowed between two successiveitemsets in the patterns: 1 (patterns are ex-tracted only from adjacent revisions)?
Minimum time interval between the first item-set and the last itemset in the patterns: 1 (thelength of the patterns is 2 or higher)As this output reflects recurring sequences of ad-jacent revisions labeled with edit categories, we re-fer to it as collaboration patterns.
With these pa-rameters, the algorithm discovered 1,358 sequen-tial patterns for featured articles and 968 for non-featured articles.
The number of shared patterns infeatured and non-featured articles is 427, this corre-sponds to the number of frequent patterns in a se-quence database which contains all 20 featured andnon-featured articles.
The maximum length of pat-terns which were found was 6 for featured articles,and 5 for non-featured articles.
These numbers showthat the defined collaboration patterns seem to havediscriminative power for different kinds of articles.Featured articles can be characterized by a higher8http://www.philippe-fournier-viger.com/spmf586Featured1 INFORMATION-I 2 INFORMATION-I 3 INFORMATION-I 4 INFORMATION-I 5 INFORMATION-I1 INFORMATION-D, INFORMATION-I 2 INFORMATION-I 3 INFORMATION-I 4 REFERENCE-I1 TEMPLATE-D 2 REFERENCE-INon-Featured1 INFORMATION-I 2 INFORMATION-I, REFERENCE-I 3 INFORMATION-I 4 REFERENCE-I 5 MARKUP-I1 MARKUP-I 2 REFERENCE-D 3 MARKUP-I1 VANDALISM 2 REVERTTable 4: Examples of collaboration patterns which have been found in either all featured or all non-featured articles ofWPQAC.degree of homogeneity with respect to their collab-orative patterns due to a higher number and lengthof frequent sequential patterns in featured articles ascompared to non-featured articles.In Table 4, we list some examples of collabora-tion patterns with a minimum support of 1 whichwe found in featured, but not non-featured arti-cles, or vice verse.
Unsurprisingly, patterns whichcontain combinations of the most frequent catego-ries (INFORMATION-I, REFERENCE-I), have a highoverall frequency.
The diversity inside collaborationpatterns measured by the number of different editcategories was higher in non-featured articles.
Forexample, the VANDALISM - REVERT pattern wasonly found in non-featured articles.
Patterns in fea-tured articles tended to be more homogeneous, asshown by the first pattern in Table 4, a repetitionof additions of information.
We conclude that dis-tinguished, high-quality articles, show a higher de-gree of homogeneity as compared to a subset of non-featured articles and the overall corpus.6 ConclusionIn this study, we evaluated a novel feature setfor building a model to automatically classifyWikipedia edits.
Using a freely available cor-pus (Daxenberger and Gurevych, 2012), our modelachieved a micro-averaged F1 score of .62 classify-ing edits within a range of 21 categories.
Textualfeatures had the highest impact on classifier perfor-mance, whereas language features play a minor role.The same classifier model obtained state-of-the-artperformance on the related task of fluency edit clas-sification.
Applications which potentially benefitfrom our work include the analysis of the writingprocess in collaboratively created documents, suchas wikis or research papers.
We have demonstratedhow our model can be used to detect collaborationpatterns in article revision histories.
On a subsetof articles from the English Wikipedia, we foundthat high-quality articles show a higher degree ofhomogeneity in their collaborative patterns as com-pared to random articles.
Furthermore, automaticedit category classification allows to generate hugeamounts of category-filtered training data for NLPtasks, e.g.
spelling and grammar correction or van-dalism detection.
With respect to future work, weplan to include more resources, e.g.
the PAN-WVC-10 (Potthast and Holfeld, 2011) or WiCoPaCo (Maxand Wisniewski, 2010) to increase the size of train-ing data.
A larger amount of labeled data wouldcertainly help to improve the classifier performancefor weak categories (e.g.
VANDALISM and PARA-PHRASE) and sparse categories (e.g.
TEMPLATE-D,MARKUP-M).
Based on our trained classifier, anno-tating more examples can be alleviated with the helpof active learning.AcknowledgmentsThis work has been supported by the VolkswagenFoundation as part of the Lichtenberg-ProfessorshipProgram under grant No.
I/82806, and by theHessian research excellence program ?Landes-Offensive zur Entwicklung Wissenschaftlich-o?konomischer Exzellenz?
(LOEWE) as part of theresearch center ?Digital Humanities?.
We thank theanonymous reviewers for their valuable feedback.ReferencesB Thomas Adler, Luca Alfaro, Santiago M Mola-Velasco, Paolo Rosso, and Andrew G West.
2011.Wikipedia Vandalism Detection: Combining NaturalLanguage, Metadata, and Reputation Features.
InAlexander Gelbukh, editor, Computational Linguistics587and Intelligent Text Processing, Lecture Notes in Com-puter Science, pages 277?288.
Springer.Ablimit Aji, Yu Wang, and Eugene Agichtein.
2010.
Us-ing the Past To Score the Present: Extending TermWeighting Models Through Revision History Analy-sis.
ReCALL, pages 629?638.Daniel Ba?r, Chris Biemann, Iryna Gurevych, and TorstenZesch.
2012.
UKP: Computing Semantic TextualSimilarity by Combining Multiple Content SimilarityMeasures.
In Proceedings of the 6th InternationalWorkshop on Semantic Evaluation, held in conjunctionwith the 1st Joint Conference on Lexical and Compu-tational Semantics, pages 435?40, Montreal, Canada,USA.Leo Breiman.
2001.
Random Forests.
Machine Learn-ing, 45(1):5?32.Amit Bronner and Christof Monz.
2012.
User EditsClassification Using Document Revision Histories.
InEuropean Chapter of the Association for Computa-tional Linguistics (EACL 2012), pages 356?366, Avi-gnon, France.Elena Cabrio, Bernardo Magnini, and Angelina Ivanova.2012.
Extracting Context-Rich Entailment Rules fromWikipedia Revision History.
In Proceedings of the 3rdWorkshop on The People?s Web meets NLP, pages 34?43, Jeju Island, Republic of Korea.Aoife Cahill, Nitin Madnani, Joel Tetreault, and DianeNapolitano.
2013.
Robust Systems for PrepositionError Correction Using Wikipedia Revisions.
In Pro-ceedings of the 2013 Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics: Human Language Technologies, pages 507?517, Atlanta, GA, USA.Johannes Daxenberger and Iryna Gurevych.
2012.
ACorpus-Based Study of Edit Categories in Featuredand Non-Featured Wikipedia Articles.
In Proceed-ings of the 24th International Conference on Compu-tational Linguistics, pages 711?726, Mumbai, India.Hannes Dohrn and Dirk Riehle.
2011.
Design and imple-mentation of the Sweble Wikitext parser.
In Proceed-ings of the 7th International Symposium on Wikis andOpen Collaboration, pages 72?81, Mountain View,CA, USA.Camille Dutrey, Houda Bouamor, Delphine Bernhard,and Aure?lien Max.
2011.
Local modifications andparaphrases in Wikipedia?s revision history.
Proce-samiento del Lenguaje Natural, 46:51?58.Richard Eckart de Castilho, Iryna Gurevych, andRichard Eckart de Castilho.
2011.
A LightweightFramework for Reproducible Parameter Sweeping inInformation Retrieval.
In Proceedings of the Work-shop on Data Infrastructures for Supporting Informa-tion Retrieval Evaluation, pages 7?10, Glasgow, UK.Oliver Ferschke, Torsten Zesch, and Iryna Gurevych.2011.
Wikipedia Revision Toolkit: Efficiently Access-ing Wikipedia?s Edit History.
In Proceedings of the49th Annual Meeting of the Association for Compu-tational Linguistics: Human Language Technologies.System Demonstrations, pages 97?102, Portland, OR,USA.Oliver Ferschke, Johannes Daxenberger, and IrynaGurevych.
2013.
A Survey of NLP Methods and Re-sources for Analyzing the Collaborative Writing Pro-cess in Wikipedia.
In Iryna Gurevych and Jungi Kim,editors, The Peoples Web Meets NLP: CollaborativelyConstructed Language Resources, Theory and Appli-cations of Natural Language Processing, chapter 5.Springer.Philippe Fournier-Viger, Roger Nkambou, and Engel-bert Mephu Nguifo.
2008.
A Knowledge DiscoveryFramework for Learning Task Models from User Inter-actions in Intelligent Tutoring Systems.
In AlexanderGelbukh and Eduardo F. Morales, editors, Proceedingsof the 7th Mexican International Conference on Artifi-cial Intelligence, Lecture Notes in Computer Science,pages 765?778.
Springer.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The WEKA Data Mining Software: An Update.SIGKDD Explorations, 11(1):10?18.Yu Hirate and Hayato Yamana.
2006.
Generalized Se-quential Pattern Mining with Item Intervals.
Journalof Computers, 1(3):51?60.Sara Javanmardi, David W. McDonald, and Cristina V.Lopes.
2011.
Vandalism Detection in Wikipedia: AHigh-Performing, Feature-Rich Model and its Reduc-tion Through Lasso.
In Proceedings of the 7th Interna-tional Symposium on Wikis and Open Collaboration,pages 82?90, Mountain View, CA, USA.Brian Keegan, Darren Gergle, and Noshir Contractor.2013.
Hot Off the Wiki: Structures and Dynamicsof Wikipedia?s Coverage of Breaking News Events.American Behavioral Scientist, 57(5):595?622, May.Jun Liu and Sudha Ram.
2011. Who does what: Col-laboration patterns in the wikipedia and their impacton article quality.
ACM Trans.
Management Inf.
Syst.,2(2):11.Gjorgji Madjarov, Dragi Kocev, Dejan Gjorgjevikj, andSas?o Dz?eroski.
2012.
An extensive experimentalcomparison of methods for multi-label learning.
Pat-tern Recognition, 45(9):3084?3104.Nitin Madnani, Joel Tetreault, and Martin Chodorow.2012.
Re-examining machine translation metrics forparaphrase identification.
In Proceedings of the 2012Conference of the North American Chapter of theAssociation for Computational Linguistics: Human588Language Technologies, pages 182?190, Montre?al,Canada.Aure?lien Max and Guillaume Wisniewski.
2010.
MiningNaturally-occurring Corrections and Paraphrases fromWikipedias Revision History.
In Proceedings of the7th Conference on International Language Resourcesand Evaluation, Valletta, Malta.Rani Nelken and Elif Yamangil.
2008.
MiningWikipedia?s Article Revision History for TrainingComputational Linguistics Algorithms.
In Proceed-ings of the 1st AAAI Workshop on Wikipedia and Arti-ficial Intelligence, pages 31?36, Chicago, IL, USA.Se?rgio Nunes, Cristina Ribeiro, and Gabriel David.
2011.Term weighting based on document revision history.Journal of the American Society for Information Sci-ence and Technology, 62(12):2471?2478.Philip V. Ogren, Philipp G. Wetzler, and Steven Bethard.2008.
ClearTK: A UIMA toolkit for statistical naturallanguage processing.
In Towards Enhanced Interoper-ability for Large HLT Systems: UIMA for NLP work-shop at Language Resources and Evaluation Confer-ence (LREC), pages 32?38, Marrakech, Morocco.Jian Pei, Jiawei Han, Behzad Mortazavi-Asl, JianyongWang, Helen Pinto, Qiming Chen, Umeshwar Dayal,and Mei-Chun Hsu.
2004.
Mining sequential patternsby pattern-growth: the PrefixSpan approach.
IEEETransactions on Knowledge and Data Engineering,16(11):1424?1440.Martin Potthast and Teresa Holfeld.
2011.
Overview ofthe 2nd International Competition on Wikipedia Van-dalism Detection.
In Notebook Papers of CLEF 2011Labs and Workshops, Amsterdam, Netherlands.Martin Potthast, Tim Gollub, Matthias Hagen, JohannesKiesel, Maximilian Michel, Arnd Oberla?nder, Mar-tin Tippmann, Alberto Barro?n-Ceden?o, Parth Gupta,Paolo Rosso, and Benno Stein.
2012.
Overview ofthe 4th International Competition on Plagiarism De-tection.
In CLEF 2012 Evaluation Labs and WorkshopWorking Notes Papers, Rome, Italy.J.
Ross Quinlan.
1993.
C4.5: programs for machinelearning.
Morgan Kaufmann Publishers.Marta Recasens, Cristian Danescu-Niculescu-Mizil, andDan Jurafsky.
2013.
Linguistic Models for Analyz-ing and Detecting Biased Language.
In Proceedings ofthe 51st Annual Meeting on Association for Computa-tional Linguistics, pages 1650?1659, Sofia, Bulgaria.Bongwon Suh, Gregorio Convertino, Ed H. Chi, and Pe-ter Pirolli.
2009.
The singularity is not near: slow-ing growth of Wikipedia.
In Proceedings of the 5thInternational Symposium on Wikis and Open Collabo-ration, Orlando, FL, USA.Konstantinos Trohidis, Grigorios Tsoumakas, GeorgeKalliris, and Ioannis Vlahavas.
2008.
Multi-labelclassification of music into emotions.
In 9th Inter-national Conference on Music Information Retrieval,pages 325?330, Philadelphia, PA, USA.Grigorios Tsoumakas, Ioannis Katakis, and Ioannis Vla-havas.
2008.
Effective and Efficient Multilabel Classi-fication in Domains with Large Number of Labels.
InProceedings of the ECML/PKDD 2008 Workshop onMining Multidimensional Data, Antwerp, Belgium.Grigorios Tsoumakas, Ioannis Katakis, and Ioannis Vla-havas.
2010.
Mining multi-label data.
In OdedMaimon and Lior Rokach, editors, Data Mining andKnowledge Discovery Handbook, chapter 34, pages667?685.
Springer.Grigorios Tsoumakas, Ioannis Katakis, and Ioannis Vla-havas.
2011.
Random k-Labelsets for Multi-LabelClassification.
IEEE Transactions on Knowledge andData Engineering, 23(7):1079?1089.Kristian Woodsend and Mirella Lapata.
2011.
Learningto Simplify Sentences with Quasi-Synchronous Gram-mar and Integer Programming.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing, pages 409?420, Edinburgh, Scot-land, UK.Elif Yamangil and Rani Nelken.
2008.
MiningWikipedia Revision Histories for Improving SentenceCompression.
In Proceedings of the 46th AnnualMeeting of the Association for Computational Linguis-tics: Human Language Technologies.
Short Papers,pages 137?140, Columbus, OH, USA.Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-Mizil, and Lillian Lee.
2010.
For the sake of simplic-ity: unsupervised extraction of lexical simplificationsfrom Wikipedia.
In Human Language Technologies:The 2010 Annual Conference of the North AmericanChapter of the Association for Computational Linguis-tics, HLT ?10, pages 365?368, Los Angeles, CA, USA.Fabio Massimo Zanzotto and Marco Pennacchiotti.2010.
Expanding textual entailment corpora fromWikipedia using co-training.
In Proceedings ofthe COLING-Workshop on The People?s Web MeetsNLP: Collaboratively Constructed Semantic Re-sources, pages 28?36, Beijing, China.Torsten Zesch, Christof Mu?ller, and Iryna Gurevych.2008.
Using Wiktionary for Computing Semantic Re-latedness.
In Proceedings of the Twenty-Third AAAIConference on Artificial Intelligence, pages 861?866,Chicago, IL, USA.Torsten Zesch.
2012.
Measuring Contextual Fitness Us-ing Error Contexts Extracted from the Wikipedia Revi-sion History.
In Proceedings of the 13th Conference ofthe European Chapter of the Association for Compu-tational Linguistics, pages 529?538, Avignon, France.589
