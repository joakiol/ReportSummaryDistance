Learning to Recognize Tables in Free TextHwee Tou NgChung Yong L imJess ica  Li  Teng  KooDSO National  Laborator ies20 Science Park Drive, Singapore 118230{nhweetou, ichungyo, kliteng}@dso, org.
sgAbst rac tMany real-world texts contain tables.
In order toprocess these texts correctly and extract he infor-mation contained within the tables, it is importantto identify the presence and structure of tables.
Inthis paper, we present a new approach that learnsto recognize tables in free text, including the bound-ary, rows and columns of tables.
When tested onWall Street Journal news documents, our learningapproach outperforms a deterministic table recogni-tion algorithm that identifies tables based on a fixedset of conditions.
Our learning approach isalso moreflexible and easily adaptable to texts in different do-mains with different table characteristics.1 Introduct ionTables are present in many reai-world texts.
Someinformation such as statistical data is bestpresentedin tabular form.
A check on the more than 100,000Wall Street Journal (WSJ) documents collected inthe ACL/DCI CD-ROM reveals that at least an es-timated one in 30 documents contains tables.Tables present a unique challenge to informationextraction systems.
At the very least, the presence oftables must be detected so that they can be skippedover.
Otherwise, processing the lines that consti-tute tables as if they are normal "sentences" is atbest misleading and at worst may lead to erroneousanalysis of the text.As tables contain important data and information,it is critical for an information extraction system tobe able to extract he information embodied in ta-bles.
This can be accomplished only if the structureof a table, including its rows and columns, are iden-tified.That table recognition is an important step in in-formation extraction has been recognized in (Appeltand Israel, 1997).
Recently, there is also a greaterrealization within the computational linguistics com-munity that the layout and types of information(such as tables) contained in a document are im-portant considerations in text processing (see thecall for participation (Power and Scott, 1999) forthe 1999 AAAI Fail Symposium Series).However, despite the omnipresence of tables andtheir importance, there is surprisingly very littlework in computational linguistics on algorithms torecognize tables.
The only research that we areaware of is the work of (Hurst and Douglas, 1997;Douglas and Hurst, 1996; Douglas et al, 1995).Their method is essentially a deterministic algorithmthat relies on spaces and special punctuation sym-bols to identify the presence and structure of tables.However, tables are notoriously idiosyncratic.
Themain difficulty in table recognition is that there axeso many different and varied ways in which tablescan show up in real-world texts.
Any deterministicalgorithm based on a fixed set of conditions i boundto fail on tables with unforeseen layout and structurein some domains.In contrast, we present a new approach in this pa-per that learns to recognize tables in free text.
Asour approach is adaptive and trainable, it is moreflexible and easily adapted to texts in different do-mains with different table characteristics.2 Task Definit ionThe input to our table recognition program consistsof plain texts in ASCII characters.
Examples of in-put texts are shown in Figure I to 3.
They are docu-ment fragments hat contain tables.
Figure 1 and 2are taken from the Wall Street Journal documents inthe ACL/DCI CD-ROM, whereas Figure 3 is takenfrom the patent documents in the TIPSTER IR TextResearch Collection Volume 3 CD-ROM.
1In Figure 1, we added horizontal 2-digit line num-bers "Line nn:" and vertical single-digit line num-bers "n" for ease of reference to any line in this doc-ument.
We will use this document to illustrate thedetails of our learning approach throughout this pa-per.
We refer to a horizontal line as hline and avertical line as vline in the rest of this paper.Each input text may contain zerQ, one or moretables.
A table consists of one or more hlines.
Forexample, in Figure 1, hlines 13-18 constitute a ta-ble.
Ear~ table is subdivided into columns and rows.1 The extracted document  f ragments  appear in a sl ightlyedited form in this  paper due to space constraint.443LineLineLineLineLineLineLineLineLineLineLineLineLineLineLineLineLineLineLineLineLine123456789012345678901234567890123456789012345678901234567890123456789001: Raw-steel production by the nation's mills increased 4~ last week to02:1,833,000 tons from 1,570,000 tons the previous week, the AmericanIron and Steel Institute said.
03:04:05:06:07:08:09:I0:Last week's output fe l l  9.5~ from the 1,804,000 tons produced a yearear l ie r .The industry used 75.8X of its capability last week, compared with71.9~ the previous week and 72.3~ a year earlier.11: The American Iron and Steel Institute reported:12:13: Net tons Capability14: produced utilization15: Week to March 14 .............. 1,633,000 75.8~16: Week to March 7 ............... 1,570,000 71.9~17: Year to date .................. 15,029,000 66.9~18: Year earlier to date .......... 18,431,000 70.8~19: The capability utilization rate is a calculation designed20:to indicate at what percent of its production capability the21:industry is operating in a given week.Figure l:Wail Street Journ~ document fragmentHow Some.
Highly Conditional 'Bids' FaredStock's'Bid'* InitialDate** Reaction***Bidder (Target Company)TWAICarl Ic~h- (USAir Group)$52 +5 3/8 to 49 1/83/4/87OutcomeBid, seen a ploy to getUSAir to buy TWA, isshelved Monday withUSAir at 45 i/4; closedWed.
at 44 1/2Columbia Ventures (Harnischfeger)$19 +1/2 to 18 1/4 Harnischfeger re jec ts2/23/87 bid Feb. 26 with stockat 18 3/8; closed Wed.at 17 5/8Figure 2: Wail Street Journal document fragmentEach column of a table consists of one or more vlines.For example, there are three columns in the table inFigure 1: vlines 4-23, 36-45, and 48-58.
Each rowof a table consists of one or more hlines.
For ex-ample, there are five rows in the table in Figure 1:hlines 13-14, 15, 16, 17, and 18.More specifically, the task of table recognition isto identify the boundaries, columns and rows of ta-bles within an input text.
For example, given the in-put text in Figure 1, our table recognition programwill identify one table with the following boundary,columns and rows:I. Boundary: Mines 13-182.
Columns: vlines 4-23, 36--45, and 48-583.
Rows: hlines 13-14, 15, 16, 17, and 18Figure 1 to 3 illustrate some of the dh~iculties oftable recognition.
The table in Figure I uses a stringof contiguous punctuation symbols "."
instead ofblank space characters in between two columns.
InFigure 2, the rows of the table can contain captionor title information, like "How Some Highly Con-ditionai 'Bids' Fared", or header information like"Stock's Initial Reaction***" and "Outcome", or444side walls of the tray to provide even greater protection from convectiveheat transfer.
Preferred construction materials are shown in Table 1:TABLE 1ComponentMaterialStiffenerPaperboard having a thickness of about 6 and 30mil (between about 6 and 30 point chip board).InsulationMineral wool, having a density of between 2.5and 6.0 pounds per cubic foot and a thickness ofbetween 1/4 and 1 and 1/4 inch.Plastic sheetsPolyethylene, having a thickness of between 1and 4 mil; coated with a reflective finish on theexterior surfaces, such as aluminum having athickness of between 90 and 110 Angstromsapplied using a standard technique such asvacuum deposition.The stiffener 96 makes a smaller contribution to the insulation propertiesof the blanket 92, than does the insulator 98.
As stated above, theFigure 3: Patentbody content information like "$52" and "+5 3/8to 49 1/8".
Each row containing body content infor-mation consists of several hlines - -  information on"Outcome" spans several hlines.
In Figure 3, stringsof contiguous dashes "-" occur within the table.
Fur-thermore, the two columns within the table appearright next to each other - -  there are no blank vlinesseparating the two columns.
Worse still, some wordsfrom the first column like "Insulation" and "Plasticsheets" spill over to the second column.
Notice thatthere may or may not be any blank lines or delimitersthat immediately precede or follow a table within aninput text.In this paper, we assume that our input texts areplain texts that do not contain any formatting codes,such as those found in an SGML or HTML docu-ment.
There is a large number of documents thatfall under the plain text category, and these are thekinds of texts that our approach to table recognitionhandles.
The work of (Hurst and Douglas, 1997;Douglas and Hurst, 1996; Douglas et al, 1995) alsodeals with plain texts.3 ApproachA table appearing in plain text is essentially a twodimensional entity.
Typically, the author of the textuses the <newline> character to separate adjacenthlines and a row is formed from one or more of suchhlines.
Similarly, blank space characters or somedocument fragmentspecial punctuation characters are used to delimitthe columns.
2 However, the specifics of how exactlythis is done can vary widely across texts, as exem-plified by the tables in Figure 1 to 3.Instead of resorting to an ad-hoc method to rec-ognize tables, we present a new approach in this pa-per that learns to recognize tables in plain text.
Ourlearning method uses purely surface features like theproportion of the kinds of characters and their rela-tive locations in a line and across lines to recognizetables.
It is domain independent and does not relyon any domain-specific knowledge.
We want to in-vestigate how high an accuracy we can achieve basedpurely on such surface characteristics.The problem of table recognition is broken downinto 3 subproblems: recognizing table boundary, col-umn, and row, in that order.
Our learning approachtreats eac~ subproblem as a separate classificationproblem and relies on sample training texts in whichthe table boundaries, columns, and rows have beencorrectly identified.
We built a graphical user inter-face in which such markup by human annotators canbe readily done.
With our X-window based GUI, atypical table can be annotated with its boundary,column, and row demarcation within a minute.From these sample annotated texts, training ex-2We assume that any <tab> character has been replacedby the appropriate number of blank space characters in theinput text.445amples in the form of feature-value vectors withcorrectly assigned classes are generated.
One setof training examples i generated for each subprob-lem of recognizing table boundary, column, and row.Machine learning algorithms are used to build clas-sifters from the training examples, one classifier persubproblem.
After training is completed, the tablerecognition program will use the learned classifiersto recognize tables in new, previously unseen inputtexts.We now describe in detail the feature xtractionprocess, the learning algorithms, and how tables innew texts are recognized.
The following classes ofcharacters are referred to throughout the rest of thissection:?
Space character: the character " " (i.e., thecharacter obtained by typing the space bar onthe keyboard).?
Alphanumeric character: one of the followingcharacters: "A" to "Z', "a" to "z', and "0" to"9".?
Special character: any character that is not aspace character and not an alphanumeric char-acter.?
Separator character: one of the following char-acters: ".
", "*', and %".3.1 Feature Extraction3.1.1 BoundaryEvery hline in an input text generates one train-ing example for the subproblem of table boundaryrecognition.
Every hline H within (outside) a tablegenerates a positive (negative) example.
Each train-ing example consists of a set of 27 feature values.The first nine feature values are derived from theimmediately preceding hline H- l ,  the second ninefrom the current hline Ho, and the last nine fromthe immediately following//1.3For a given hline H, its nine features and theirassociated values are given in Table 1.To illustrate, the feature values of the training ex-ample generated by line 16 in Figure 1 are:f, 3, N, %, N, 4, 3, I, I,f, 3, N, %, N, 4, 3,1, 1,f, 3, N, %, N, 3, 3, I, 1Line 16 generated the feature valuesf, 3, N, %, N, 4, 3,1, 1.
Since line 16 does notconsist of only space characters, the value of F1 isf.
There are three space characters before the word3For the purpose of generating the feature values for thefirst and last hline in a text, we assume that the text is paddedwith a line of blank space characters before the first line andafter the last line.
"Week" in line 16, so the value of F2 is 3.
Since thefirst non-space character in line 16 is "W" and it isnot one of the listed special characters, the valueof F3 is "N".
The last non-space character in line16 is "%", which becomes the value of F4.
Sinceline 16 does not consist of only special characters,the value of F5 is "N".
There are four segmentsin line 16 such that each segment consists of twoor more contiguous pace characters: a segmentof three contiguous pace characters before theword "Week"; a segment of two contiguous pacecharacters after the punctuation characters "..."and before the number "1,570,000"; a segment ofthree contiguous space characters between the twonumbers "1,570,000" and "71.9%"; and the lastsegment of contiguous pace characters trailingthe number "71.9%".
The values of the remainingfeatures of line 16 are similarly determined.
Fi-nally, line 15 and 17 generated the feature valuesf,3,N,%,N,4,3,1,1 and f,3,N,%,N,3,3,1,1,respectively.The features attempt o capture some recurringcharacteristics of lines that constitute tables.
Lineswith only space characters orspecial characters tendto delimit tables or are part of tables.
Lines withina table tend to begin with some number of leadingspace characters.
Since columns within a table areseparated by contiguous space characters or specialcharacters, we use segments ofsuch contiguous char-acters as features indicative of the presence of tables.3.1.2 ColumnEvery vline within a table generates one training ex-ample for the subproblem of table column recogni-tion.
Each vline can belong to exactly one of fiveclasses:1.
Outside any column2.
First line of a column3.
Within a column (but neither the first nor lastline)4.
Last line of a column5.
First and last line of a column (i.e., the columnconsists of only one line)Note that it is possible for one column to imme-diately follow another (as is the case in Figure 3).Thus a two-class representation is ot adequate here,since there would be no way to distinguish betweentwo adjoining columns versus one contiguous columnusing only two classes.
4The start and end of a column in a table is typ-ically characterized by a transition from a vline of4For the identification of table boundary, we assume inthis paper that there is some hline separating any two tables,and so a two-class representation fortable boundary suffices.446Feature DescriptionF1F2F3Whether H consists of only space characters.
Possible values are t (if H is a blankline) or f (otherwise).The number of leading (or initial) space characters in H.The first non-space character in H. Possible values are one of the following specialcharacters: 0\[ \ ]{}<> +-* /=~!
@#$%A& or N (if the first non-space character isnot one of the above special characters).F4 The last non-space character in H. Possible values are the same as F3.F5 Whether H consists entirely of one special character only.
Possible values are eitherone of the special characters listed in F3 (if H only consists of that special character)or N (if H does not consist of one special character only).F6 The number of segments in H with two or more contiguous space characters.F7 The number of segments in H with three or more contiguous space characters.F8 The number of segments in H with two or more contiguous separator characters.F9 The number of segments in H with three or more contiguous separator characters.Table 1: Feature values for table boundaryspace (or special) characters to a vline with mixed al-phanumeric and space characters.
That is, the tran-sition of character types across adjacent vlines givesan indication of the demarcation of table columns.Thus, we use character type transition as the fea-tures to identify table columns.Each training example consists of a set of six fea-ture values.
The first three feature values are derivedfrom comparing the immediately preceding vline V-zand the current vline V0, while the last three featurevalues are derived from comparing V0 with the im-mediately following vline Vl.SLet Vj and Vj+ 1 be any two adjacent vlines.Suppose Vj = C l j .
.
.
c i , j .
.
.
c~,#,  and Vj+I =Czj+l .
.
.
ci j+l .
.
.
cm,j+z where m is the number ofhlines that constitute a table.Then the three feature values that are derivedfrom the two vlines Vj and 1~+1 are determinedby counting the proportion of two horizontally ad-jacent characters c~,j and ci j+l (1 < i < m) thatsatisfy some condition on the type of the two char-acters.
The precise conditions on the three featuresare given in Table 2.To illustrate, the feature values of vline 4 in Fig-ure 1 are:0.333, 0, 0.667, 0.333, 0, 0and its class is 2 (first line of a column).
In de-riving the feature values, only hlines 13-18, thelines that constitute the table, are considered (i.e.,m = 6).
For the first three feature values, F1 =2/6 since there are two space-character-to-space-character transitions from vline 3 to 4 (namely, onhlines 13 and 14); F2 = 0 since there is no al-phanumeric haracter or special character in vline5For the purpose of generating the feature values for thefirst and last vline in a table, we assume that the table ispadded with a vline of blank space characters before the firstvline and after the last vline.3; F3 = 4/6, since there are four space-character-to-alphanumeric-character transitions from vline 3 to 4(namely, on hlines 15-18).
Similarly, the last 3 fea-ture values are derived by examining the charactertransitions from vline 4 to 5.3.1.3 RowEvery hline within a table generates one training ex-ample for the subproblem of table row recognition.Unlike table columns, every hline within a table be-longs to some row in our formulation of the rowrecognition problem.
As such, each hline belongsto exactly one of two classes:1.
First hline of a row2.
Subsequent hline of a row (not the first line)The layout of a typical table is such that its rowstend to record repetitive or similar data or informa-tion.
We use this clue in designing the features fortable row recognition.
Since the information withina row may span multiple hlines, as the "Outcome"information in Figure 2 illustrates, we use the firsthline of a row as the basis for comparison acrossrows.
If two hlines are similar, then they belongto two separate rows; otherwise, they belong to thesame row.
Similarity is measured by character typetransitions, as in the case of table column recogni-tion.More specifically, to generate a training examplefor a hline H, we compare H with H ~, where H ~ isthe first hline of the immediately preceding row ifH is the first hline of the current row, and H ~ isthe first hline of the current row if H is not the firsthline of the current row.
6Each training example consists of a set of fourfeature values F1,..., F4.
F1, F2, and F3 are de-termined by comparing H and H ~ while F4  is de-termined solely from H. Let H = Ci,l ... cid.., ci,n~H ~ = H for the very first hline within a table.447Feature DescriptionF1F2ci j  is a space character and ei,jq_ 1 is a space character; or ci,j is a special characterand ci,j+l is a special characterci j  is an alphanumeric character or a special character, and ci,j+l is a space char-acterF3 ci,j is a space character, and cl,j+l is an alphanumeric character or a special char-acterTable 2: Feature values for table columnand H'  = Ci',1 .
.
.
Ci',j... Ci',n, where n is the numberof vlines of the table.
The values of F1 , .
.
.
,  F3 aredetermined by counting the proportion of the pairsof characters ci, j and cl,j (1 _< j < n) that satisfysome condition on the type of the two characters,as listed in Table 3.
Let ci,k be the first non-spacecharacter in H. Then the value of F4 is kin.To illustrate, the feature values of hline 16 in Fig-ure 1 are:0.236, 0.018, 0.018, 0.018and its class is 1 (first line of a row).
There are 55vlines in the table, so n = 55.
7 Since hline 16 is thefirst line of a row, it is compared with hline 15, thefirst hline of the immediately preceding row, to gen-erate F1, F2, and F3 .
F1 = 13/55 since there are 13space-character-to-space-character transitions fromhline 15 to 16.
F2 = F3 = 1/55 since there isonly one alphanumeric-character-to-space-charactertransition ("4" to space character in vline 19) andone space-character-to-special-character transition(space character to "."
in vline 20).
The first non-space character is "W" in the first vline within thetable, so k = 1.3.2 Learn ing A lgor i thmsWe used the C4.5 decision tree induction algorithm(Quinlan, 1993) and the backpropagation algorithmfor artificial neural nets (Rumelhart et al, 1986) asthe learning algorithms to generate the classifiers.Both algorithms are representative state-of-the-artlearning algorithms for symbolic and connectionistlearning.We used all the default learning parameters in theC4.5 package.
For backpropagation, the learningparameters are: hidden units : 2, epochs = 1000,learning rate = 0.35 and momentum term = 0.5.
Wealso used log n-bit encoding for the symbolic featuresand normalized the numeric features to \[0... 1\] forbackpropagation.3.3 Recogn iz ing  Tables in New Texts3.3.1 BoundaryEvery hline generates a test example and a classi-fier assigns the example as either positive (within a~'In generating the feature values for table row recognition,only the vlines enclosed within the identified first and lastcolumn of the table are considered.table) or negative (outside a table).3.3.2 Co lumnAfter the table boundary has been identified, clas-sification proceeds from the first (leftmost) vline tothe last (rightmost) vline in a table.
For each vline,a classifier will return one of five classes for the testexample generated from the current vline.Sometimes, the class assigned by a classifier to thecurrent vline may not be logically consistent withthe classes assigned up to that point.
For instance,it is not logically consistent if the previous vline is ofclass 1 (outside any column) and the current vlineis assigned class 4 (last line of a column).
Whenthis happens, for the backpropagation algorithm, theclass which is logically consistent and has the highestscore is assigned to the current vline; for C4.5, one ofthe logically consistent classes is randomly chosen.3.3.3 RowThe first hline of a table always starts a new activerow (class 1).
Thereafter, for a given hline, it iscompared with the first hline of the current activerow.
If the classifier eturns class 1 (first hline ofa row), then a new active row is started and thecurrent hline is the first hline of this new row.
Ifthe classifier eturns class 2 (subsequent hline of arow), then the current active row grows to includethe current hline.4 EvaluationTo determine how well our learning approach per-forms on the task of table recognition, we selected100 Wall Street Journal (WSJ) news documentsfrom the ACL/DCI CD-ROM.
After removing theSGML markups on the original documents, we man-ually annotated the plain-text documents with tableboundary, column, and row information.
The docu-ments shown in Figure 1 and 2 are part of the 100documents used for evaluation.4.1 Accuracy  Def in i t ionTo measure the accuracy of recognizing table bound-ary of a new text, we compare the class assigned bythe human annotator to the class assigned by our ta-ble recognition program on every hline of the text.Let A be the number of hlines identified by the hu-man annotator as being part of some table.
Let B448Feature DescriptionF1 cl, j is a space character and ci,j is a space characterF2F3F4ci,,j is an alphanumeric character or a special character, and ci,j is a space characterci,,j is a space character, and ci,j is an alphanumeric character or a special characterk inTable 3: Feature values for table rowbe the number of Mines identified by the program asbeing part of some table.
Let C be the number ofMines identified by both the human annotator andthe program as being part of some table.
Then recallR = C/A  and precision P = C/B .
The accuracy oftable boundary recognition is defined as the F mea-sure, where F = 2RP/ (R  + P).
The accuracy ofrecognizing table column (row) is defined similarly,by comparing the class assigned by the human anno-tator and the program to every vline (hline) withina table.4.2 Determin is t i c  A lgor i thmsTo determine how well our learning approach per-forms, we also implemented deterministic algorithmsfor recognizing table boundary, column, and row.The intent is to compare the accuracy achieved byour learning approach to that of the baseline deter-ministic algorithms.
These deterministic algorithmsare described below.4.2.1 BoundaryA Mine is considered part of a table if at least onecharacter of Mine is not a space character and if anyof the following conditions is met:* The ratio of the position of the first non-spacecharacter in hline to the length of hline exceedssome pre-determined threshold (0.25)?
Hline consists entirely of one special character.. Hline contains three or more segments, eachconsisting of two or more contiguous space char-acters.?
Hline contains two or more segments, each con-sisting of two or more contiguous eparatorcharacters.4.2.2 ColumnAll vlines within a table that consist of entirelyspace characters are considered not part of any col-umn.
The remaining vlines within the table are thengrouped together to form the columns.4.2.3 RowThe deterministic algorithm to recognize table rowis similar to the recognition algorithm of the learn-ing approach given in Section 3.3.3, except hat theclassifier is replaced by one that computes the pro-portion of character type transitions.
All charactersin the two hlines under consideration are groupedinto four types: space characters, pecial characters,alphabetic haracters, or digits.
If the proportionof characters that change type exceeds ome pre-determined threshold (0.5), then the two Mines be-long to the same row.4.3 ResultsWe evaluated the accuracy of our learning approachon each subproblem of table boundary, column, androw recognition.
For each subproblem, we conductedten random trials and then averaged the accuracyover the ten trials.
In each random trial, 20% of thetexts are randomly chosen to serve as the texts fortesting, and the remaining 80% texts are used fortraining.
We plot the learning curve as each clas-sifter is given increasing number of training texts.Figure 4 to 6 summarize the average accuracy overten random trials for each subproblem.
Besides theaccuracy for the C4.5 and backpropagation classi-tiers, we also show the accuracy of the deterministicalgorithms.The results indicate that our learning approachoutperforms the deterministic algorithms for all sub-problems.
The accuracy of the deterministic algo-rithms is about 70%, whereas the maximum accu-racy achieved by the learning approach ranges over85% - 95%.
No one learning algorithm clearly out-performs the other, with C4.5 giving higher accu-racy on recognizing table boundary and column, andbackpropagation performing better at recognizingtable row.To test the generality of our learning approach,we also evaluated it on 50 technical patent docu-ments from the T IPSTER Volume 3 CD-ROM.
Totest how well a classifier that is trained on one do-main of texts will generalize to work on a differentdomain, we also tested the accuracy of our learn-ing approach on patent exts after training on WSJtexts only, and vice versa.
Space constraint does notpermit us to present the detailed empirical results inthis paper, but suffice to say that we found that ourlearning approach is able to generalize well to workon different domains of texts.5 Future  WorkCurrently, our table row recognition does not dis-tinguish among the different ypes of rows, such astitle (or caption) row, header ow, and content row.We would like to extend our method to make such4499590858O~ ~?656O55500, , , , , & ................ Y ...............e ....,,"~ ....... .......~ ..........TC4.5 -'---e .....~,~ Bp -..---x. .
...I ?-i ...........10 20 30 40 50 60 70 80Number of training examples908580757065605550I I i i I I i/ .. ~ .....~ .
.
.
.
.  '
" " .
.
.
.~  ................ ,X .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
"X .
.
.
.
.
.
.
.
.
.
.
.
.
.
~:!
~:' " .
.
.
.
.
.
.
.
.
.
.
.
.
.  "
" "  ?
................. ?
................. "~"  " -0  "" " " - 'Q  .
.
.
.
.
.
.
.
.
.
.C4.5 ----e .....Bp "-"~ .....Det ----'~ .....I0 I I I , ,  I I I 1 20 30 40 50 60 70 80Number of training examplesFigure 4: Learning curve of boundary identificationaccuracy on WSJ textsFigure 6: Learning curve of row identification accu-racy on WSJ texts90858O7~70555O45 ~0 10, , , , , & ............... 4 ............... .
-0  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.. , .
.
..... - ............ Q ....t ' "  .... X ....," . "
.... .
/C4.5 ..-..-o .....Bp --'-~ .....Det --.-~ .....' 3'o ' 6'0 ' 20 40 70 80Number of training examplesFigure 5: Learning curve of column identificationaccuracy on WSJ textsdistinction.
We would also like to investigate theeffectiveness of other learning algorithms, such asexemplar-based methods, on the task of table recog-nition.6 Conc lus ionIn this paper, we present a new approach that learnsto recognize tables in free text, including the bound-ary, rows and columns of tables.
When tested onWall Street Journal news documents, our learningapproach outperforms a deterministic table recogni-tion algorithm that identifies tables based on a fixedset of conditions.
Our learning approach isalso moreflexible and easily adaptable to texts in different do-mains with different able characteristics.ReferencesDouglas Appelt and David Israel.
1997.
Tutorialnotes on building information extraction systems.Tutorial held at the Fifth Conference on AppliedNatural Language Processing.Shona Douglas and Matthew Hurst.
1996.
Layout& language: Lists and tables in technical doc-uments.
In Proceedings o.f the A CL SIGPARSEWorkshop on Punctuation in Computational Lin-guistics, pages 19-24.Shona Douglas, Matthew Hurst, and David Quinn.1995.
Using natural anguage processing for iden-tifying and interpreting tables in plain text.
InFourth Annual ~qymposium on Document Analy-sis and Information Retrieval, pages 535-545.Matthew Hurst and Shona Douglas.
1997.
Layout& language: Preliminary experiments in assigninglogical structure to table cells.
In Proceedings ofthe Fifth Conference on Applied Natural LanguageProcessing, pages 217-220.Richard Power and Donia Scott.
1999.
Using lay-out for the generation, understanding or retrievalof documents.
Call for participation at the 1999AAAI Fall Symposium Series.John Ross Quinlan.
1993.
C4.5: Programs for Ma-chine Learning.
Morgan Kaufmann, San Fran-cisco, CA.David E. Rumelhart, Geoffrey E. Hinton, andRonald J. Williams.
1986.
Learning internal rep-resentation by error propagation.
In David E.Rumelhart and James L. McClelland, editors,Parallel Distributed Processing, Volume 1, pages318-362.
MIT Press, Cambridge, MA.450
