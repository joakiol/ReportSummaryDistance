Proceedings of the NAACL HLT 2010 Second Workshop on Computational Approaches to Linguistic Creativity, pages 49?57,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsComputational Creativity Tools for SongwritersBurr SettlesMachine Learning DepartmentCarnegie Mellon UniversityPittsburgh, PA 15213 USAbsettles@cs.cmu.eduAbstractThis paper describes two natural languageprocessing systems designed to assist song-writers in obtaining and developing ideas fortheir craft.
Titular is a text synthesis algo-rithm for automatically generating novel songtitles, which lyricists can use to back-formconcepts and narrative story arcs.
LyriCloudis a word-level language ?browser?
or ?ex-plorer,?
which allows users to interactively se-lect words and receive lyrical suggestions inreturn.
Two criteria for creativity tools arealso presented along with examples of howthey guided the development of these systems,which were used by musicians during an inter-national songwriting contest.1 IntroductionWriting lyrics for popular music is challenging.Even apart from musical considerations, well-crafted lyrics should succinctly tell a story, expressemotion, or create an image in the listener?s mindwith vivid and original language.
Finding the rightwords, from an evocative title to a refrain, hook, ornarrative detail, is often difficult even for full-timeprofessional songwriters.This paper considers the task of creating ?intel-ligent?
interactive lyric-writing tools for musicians.As a preliminary case study, I discuss two systemsthat (1) suggest novel song titles and (2) find inter-esting words given a seed word as input, and de-ployed them online for participants in an interna-tional songwriting event called FAWM1 (February1http://fawm.orgAlbum Writing Month).
The goal of this event?which attracts both professional and amateur musi-cians worldwide?is to compose 14 new works ofmusic (roughly an album?s worth) during the monthof February.
Working at a rate of one new song ev-ery two days on average is taxing, described by someparticipants as a ?musical marathon.?
To maintainpace, songwriters are constantly looking for newideas, which makes them good candidates for us-ing computational creativity tools.
Typical lyric-writing tools consist of rhyme or phrase dictionarieswhich are searchable but otherwise static, passive re-sources.
By contrast, we wish to develop advancedsoftware which uses learned linguistic knowledge toactively help stimulate creative thought.To formalize the task of developing computa-tional creativity tools, let us first define creativityas ?the ability to extrapolate beyond existing ideas,rules, patterns, interpretations, etc., and to generatemeaningful new ones.?
By this working definition,which is similar to Zhu et al (2009), tools that assisthumans in creative endeavors should:1.
Suggest instances unlike the majority that ex-ist.
If one were to model instances statistically,system proposals should be ?outliers.?2.
Suggest instances that are meaningful.
Purelyrandom proposals might be outliers, but theyare not likely to be interesting or useful.Previous approaches to linguistic lyric-modelinghave generally not focused on creativity, but ratheron quantifying ?hit potential?
(Yang et al, 2007),which is arguably the opposite, or classifying mu-sical genre (Li and Ogihara, 2004; Neumayer and49Rauber, 2007).
There has been some work on auto-matically generating percussive lyrics to accompanya given piece of musical input (Oliveira et al, 2007;Ramakrishnan et al, 2009), and there exists a richbody of related work on natural language generationfor fiction (Montfort, 2006; Solis et al, 2009), po-etry (Gerva?s, 2001; Manurung, 2004; Netzer et al,2009), and even jokes (Binstead, 1996).
However,the goal of these systems is to be an ?artificial artist?which can create complete works of language au-tonomously, rather than interactive tools for assist-ing humans in their creative process.A few computational lyric-writing tools havebeen developed outside of academia, such as Ver-basizer, which was famously co-created by rock starDavid Bowie to help him brainstorm ideas (Thomp-son, 2007).
These types of systems take a smallamount of seed text as input, such as a newspa-per article, and generate novel phrases by iteratingthrough random word permutations.
However, theseapproaches fail the second criterion for creativitytools, since the majority of output is not meaning-ful.
Many other so-called lyric generators exist onthe Internet2, but these are by and large ?Mad-Lib?style fill-in-the-blanks meant for amusement ratherthan more serious artistic exploration.The contribution of this work is to develop andstudy methods that satisfy both criteria for compu-tational creativity tools?that their suggestions areboth unlikely and meaningful?and to demonstratethat these methods are useful to humans in prac-tice.
To do this, I employ statistical natural languagemodels induced from a large corpus of song lyrics toproduce real-time interactive programs for exploringsongwriting ideas.
The rest of this paper is organizedas follows.
Section 2 describes the overall approachand implementation details for these tools.
Section 3presents some empirical and anecdotal system eval-uations.
Section 4 summarizes my findings, dis-cusses limitations of the current tools, and proposesfuture directions for work in this vein.2 MethodologyDavis (1992) describes the discipline of songwritingas a multi-step process, beginning with activationand association.
Activation, according to Davis, in-2http://www.song-lyrics-generator.org.ukvolves becoming hyper-observant, embracing spon-taneous ideas, and then choosing a title, theme, orprogression around which to build a song.
Associa-tion is the act of expanding on that initial theme oridea as much as possible.
Subsequent steps are todevelop, organize, and winnow down the ideas gen-erated during the first two stages.Following this philosophy, I decided to designtwo natural language processing systems aimed atstimulating songwriters during the early stages ofthe process, while adhering to the criteria for cre-ativity tools defined in Section 1.
I call these toolsTitular, which suggests novel song titles as a startingpoint, and LyriCloud, which expands lyrical ideas bysuggesting related words in the form of a ?cloud.
?To encourage songwriters to actually adopt thesetools, they were deployed online as part of the of-ficial FAWM website for participants to use.
Thisposed some development constraints, namely thatthe user interface be implemented in the PHP lan-guage and run in a shared web-hosting environment.Because of this setup, complex inference methodsbased on a large number of statistics had to beavoided in order to maintain speed and interactiv-ity.
Thus, I was limited to approaches where suffi-cient statistics could be pre-computed and stored ina database to be accessed quickly as needed.To induce linguistic models for Titular and Lyri-Cloud, existing song data were needed for training.For this, I used a ?screen scraper?
to extract songtitle, artist, and lyric fields from ubiquitous onlinelyrics websites.
In this way, I collected a corpus of137,787 songs by 15,940 unique artists.
The collec-tion spans multiple genres (e.g., pop/rock, hip-hop,R&B, punk, folk, blues, gospel, showtunes), and hasgood coverage of mainstream charting artists (e.g.,Beyonce?, R.E.M., Van Halen) as well as more ob-scure independent performers (e.g., Over the Rhine,Dismemberment Plan).
An estimated 85% of thesongs are primarily in English.2.1 TitularFigure 1 shows example output from Titular, whichpresents the user with five suggested song titles at atime.
Suggestions often combine visceral and con-tradictory images, like ?Bleeding in the Laughter,?while others lend themselves to more metaphori-cal interpretation, such as ?Heads of Trick?
(which50Figure 1: A screenshot of the Titular user interface.evokes some sort of executive committee for deceit).Occasionally the output is syntactically valid, buta little too awkward to interpret sensibly, such as?Starting in the Sisters.
?To accomplish this, I adopted a template-basedapproach (Deemter et al, 2005) to title synthesis.
In-stead of using hand-crafted templates, however, Tit-ular learns its own using the following method:1.
Source titles in the training corpus are tok-enized and tagged for part-of-speech (POS).
In-put is first lowercased to discourage the taggerfrom labeling everything NNP (proper noun).2.
The open word classes (i.e., various forms ofadjectives, adverbs, nouns, and verbs) are sub-stituted with their assigned POS tag, whileclosed words classes (i.e., conjunctions, de-terminers, prepositions, pronouns and punctu-ation) remain intact.
Titular also rememberswhich words were substituted with which tag,in order to use them in generating new titles inthe future.
Vulgar and offensive words are fil-tered out using regular expressions.3.
The induced templates and words are culled ifthey occur in the corpus below a certain fre-quency.
This step helps to counterbalance tag-ging errors, which is important because song ti-tles are often too short to infer POS informationaccurately.
Thresholds are set to 3 for templatesand 2 for POS-word pairs.Table 1 shows several example templates inducedusing this method.
To generate new titles, TitularLearned Template Freq.
Example Source TitleJJ NN 3531 Irish RoverVB 783 BreatheVBN NN 351 Born YesterdayNN and NN 205 Gin And JuiceNNP POS NN 167 Tom?s DinerFW NN 65 Voodoo WomanVB and VB 57 Seek And DestroyCD JJ NNS 49 99 Red BalloonsJJ , JJ NN 14 Bad, Bad Girlyou ?re so JJ 8 You?re So VainNN ( the NN ) 7 Silver (The Hunger)VBG with a NN 4 Walking With A ZombieTable 1: Example title templates induced by Titular.first randomly selects a template in proportion toits empirical frequency, and likewise selects wordsto replace each POS tag in the template (drawnfrom the set of words for the corresponding tag).This model can be thought of as a simple stochasticcontext-free grammar (Lari and Young, 1990) withonly a small set of production rules.
Specifically, theroot nonterminal S goes to complete templates, e.g.,S ?
VBG , VBG | UH , JJ NN !
| .
.
.
,and POS nonterminals go to words that have beentagged accordingly in the corpus, e.g.,VBG?
waiting | catching | going | .
.
.
,JJ?
good | little | sacrificial | .
.
.
,NN?
time | life | dream | .
.
.
,UH?
hey | oh | yeah | .
.
.
,all with corresponding probabilities based on fre-quency.
Using these production rules, Titular cangenerate novel titles like ?Going, Waiting?
or ?Oh,Little Life!?
The system learned 2,907 template pro-duction rules and 11,247 word production rules fromthe lyrics corpus, which were stored with frequencyinformation in a MySQL database for deployment.Post-processing heuristics are implemented in theuser interface to strip white-spaces from punctuationand contractions to improve readability.
Output re-mains lowercased as an aesthetic decision.An alternative approach to title generation isan n-gram model based on Markov chains, whichare common for both natural language generation(Jurafsky and Martin, 2008) and music synthesis(Farbood and Schoner, 2001).
For titles, eachword wi is generated with conditional probability51P (wi|wi?n, .
.
.
, wi?1) based on the n words gener-ated previously, using statistics gathered from titlesin the lyrics corpus.
However, this approach tends tosimply recreate titles in the training data.
In a pre-liminary study using n = {1, 2, 3, 4} and 200 titleseach, the proportion of suggestions that were verba-tim recreations of existing titles ranged from 35%to 97%: .
When n-gram titles do manage to benovel, they tend to be long and unintelligible, suchas ?Too Many A Woman First the Dark Clouds WillIt Up?
or ?Born of Care 4 My Paradise.?
These twoextremes satisfy one or the other, but not both of thecriteria for creativity tools.
By contrast, none of the200 template-generated titles existed in the sourcedatabase, and they tend to be more intelligible aswell (see results in Section 3.1).The template grammar offers several other advan-tages over the n-gram approach, including fewerinference steps (which results in fewer databasequeries) and helping to ensure that titles are well-formed with respect to longer-range syntactic depen-dencies (like matching parentheses).
Constrainingwords by part-of-speech rather than immediate con-text also allows the system to create novel and unex-pected word juxtapositions, which satisfies the firstcriterion for creativity tools, while remaining rela-tively coherent and meaningful, which helps satisfythe second criterion.2.2 LyriCloudFigure 2 shows example output from LyriCloud,which takes a seed (in this case, the word ?dream?highlighted near the center) and suggests up to 25related words arranged visually in a cloud (Batemanet al, 2008).
The size of each word is intended tocommunicate its specificity to the cloud.Notice that LyriCloud?s notion of related wordscan be quite loose.
Some suggestions are modi-fiers of the seed, like ?broken dream?
and ?deepdream,?
although these invoke different senses ofdream (metaphorical vs. literal).
Some suggestionsare part of an idiom involving the seed, such as ?ful-fill a dream,?
while others are synonyms/antonyms,or specializations/generalizations, such as ?night-mare.?
Still other words might be useable as rhymesfor the seed, like ?smithereen?
and even ?thing?(loosely).
The goal is to help the user see the seedword in a variety of senses and perspectives.
UsersFigure 2: A screenshot of the LyriCloud user interface.may also interact with the system by clicking witha pointer device on words in the cloud that they findinteresting, and be presented with a new cloud basedon the new seed.
In this way, LyriCloud is a sort of?language browser?
for lyrical ideas.I liken the problem of generating interesting wordclouds to an information retrieval task: given a seedword s, return a set of words that are related to s,with the constraint that they not be overly generalterms.
To do this, the corpus was pre-processed byfiltering out common stop-words and words that oc-cur in only one song, or fewer than five times inthe entire corpus (this catches most typos and mis-spellings).
As with Titular, vulgar and offensivewords are filtered using regular expressions.For a potential seed word s, we can compute asimilarity score with every other word w in the cor-pus vocabulary using the following measure:sim(s, w) =(1 + log c(s, w))?
logNu(w),where c(s, w) is the number of times s and w co-occur in the same line of a lyric in the corpus, u(w)is the number of unique words with which w oc-curs in any line of the corpus, and N is the size ofthe overall vocabulary.
This is essentially the well-known log-tempered tf?idf measure from the infor-mation retrieval literature, if we treat each seed s asa ?document?
by concatenating all the lines of textin which s appears.I also experimented with the co-occurence fre-quency c(s, w) and point-wise mutual information(Church and Hanks, 1989) as similarity functions.52The former still used overly common words (e.g.,?love,?
?heart,?
?baby?
), which fails the first crite-rion for creativity tools, and the latter yielded overlyseed-specific results (and often typos not filtered bythe pre-processing step), which can fail the secondcriterion.
The log-tempered tf?idf metric provided areasonable balance between the two extremes.To generate a new cloud from a given seed, up to25 words are randomly sampled from the top 300ranked by similarity, which are pre-computed andstored in the database for efficient lookup.
The sam-pled words are then sorted alphabetically for displayand scaled using a polynomial equation:size(w) = f0 + f1 ?
(1?rank(w)K)4,where f0 and f1 are constants that bound the min-imum and maximum font size, and K is the num-ber of words in the cloud (usually 25, unless therewere unusually few words co-occuring with s).
Thischoice of scaling equation is ad-hoc, but producesaesthetically pleasing results.As a point of comparison, the original approachfor LyriCloud was to employ a topic model such asLatent Dirichlet Allocation (Blei et al, 2003).
Thisis an unsupervised machine learning algorithm thatattempts to automatically organize words accordingto empirical regularities in how they are used in data.Table 2 shows 15 example ?topics?
induced on thelyrics corpus.
Intuitively, these models treat doc-uments (song lyrics) as a probabilistic mixture oftopics, which in turn are probabilistic mixtures ofwords.
For example, a religious hymn that employsnature imagery might be a mixture of topics #11 and#13 with high probability, and low probability forother topics (see Blei et al for more details).To generate clouds, I trained a model of 200 top-ics on the lyrics corpus, and let the system choosethe most probable topic for a user-provided seed.
Itthen randomly sampled 25 of the 300 most proba-ble words for the corresponding topic, which werescaled in proportion to topic probability.
The intu-ition was that clouds based on the top-ranked wordsfor a topic should be semantically coherent, but sam-pling at random from a large window would also al-low for more interesting and unusual words.In a small pilot launch of the system, songwrit-ers rejected the topic-based version of LyriCloud for# Most Probable Words By Topic1 love baby give true sweet song girl ...2 la big black white hair wear hot ...3 hold hand hands head free put back ...4 love find nothing something everything wrong give ...5 yeah baby hey man girl rock ooh ...6 feel make eyes gonna cry makes good ...7 fall turn light face sun again world ...8 night day cold long sleep dream days ...9 mind world lose nothing left gonna shake ...10 time life long live day end die ...11 god lord man hell king heaven jesus ...12 hear play call people good talk heard ...13 sky eyes fire fly blue high sea ...14 heart inside pain soul break broken deep ...15 go back home again time gonna coming ...Table 2: Example words from a topic model induced byLatent Dirichlet Allocation on the lyrics corpus.two reasons.
First, the top-ranked words within atopic tend to be high frequency words in general(consider Table 2).
These were deemed by users tobe unoriginal and in violation the first criterion forcreativity tools.
Tweaking various system param-eters, such as the number of topics in the model,did not seem to improve performance in this re-spect.
Second, while words were thought to be co-herent overall, users had trouble seeing the connec-tion between the chosen topic and the highlightedseeds themselves (see also the results in Section 3.1).Instead, users wanted to receive interesting sug-gestions that were more specifically tailored to theseeds they chose, which motivated the information-retrieval view of the problem.
While this is plau-sible using topic models with a more sophisticatedsampling scheme, it was beyond the capabilities of asimple PHP/MySQL deployment setup.3 ResultsThis section discusses some results and observationsabout Titular and LyriCloud.3.1 Empirical EvaluationI conducted an empirical study of these systems us-ing Amazon Mechanical Turk3, which is being usedincreasingly to evaluate several systems on open-ended tasks for which gold-standard evaluation datadoes not exist (Mintz et al, 2009; Carlson et al,3http://www.mturk.com53template bigram real title12345inspiration (Titular)tfidf topic random12345inspiration (LyriCloud)tfidf topic random12345relatedness (LyriCloud)meanevaluatorratingFigure 3: Box plots illustrating the distribution of ?inspiration?
and ?relatedness?
ratings assigned by Mechanical Turkevaluators to Titular and LyriCloud output.
Boxes represent the middle 50% of ratings, with the medians indicated bythick black lines.
Whiskers on either side span the first and last quartiles of each distribution; circles indicate possibleoutliers.
The methods actually deployed for use online are are highlighted on the left of each plot.Method Good Title?
Grammatical?
Offensive?template 68% 67% 6%bigram 62% 52% 7%real title 93% 88% 9%Table 3: Titular results from Mechanical Turk evaluators.2010).
This was done because (1) we would likesome quantification of how well these systems per-form, and (2) using Mechanical Turk workers shouldprovide more stringent and objective feedback thanthe songwriters for whom the tools were developed.For each tool, I generated 200 instances (i.e., ti-tles for Titular and word clouds for LyriCloud) andhad evaluators fill out a simple form for each, com-posed of yes/no questions and ratings on a five-starscale.
Each instance was given to three unique eval-uators in order to break ties in the event of disagree-ment, and to smooth out variance in the ratings.
Tit-ular is compared against titles generated by a bi-gram Markov chain model (n = 1), as well as ac-tual song titles randomly selected from the lyricsdatabase.
LyriCloud is compared against the topicmodel method, and a baseline where both words andtheir sizes are generated at random.Table 3 summarizes Titular results for the yes/noquestions in the Mechanical Turk evaluation.
Atleast two of three evaluators agreed that 68% ofthe suggested template-based phrases would makegood song titles, which is higher than the bigrammethod, but lower than the 93% rate for real songtitles (as expected).
Similarly, template-based sug-gestions were deemed more grammatical than thebigram approach and less grammatical than actualsongs.
Somewhat surprisingly, 12% of all titles inthe evaluation were judged to be good titles despitegrammatical errors, including ?Dandelion Cheatin?and ?Dressed in Fast.?
This is an interesting ob-servation, indicating that people sometimes enjoytitles which they find syntactically awkward (per-haps even because they are awkward).
All methodsyielded a few potentially offensive titles, which weremostly due to violent or sexual interpretations.Evaluators were also asked to rate titles on a scalefrom boring (1) to inspiring (5), the results of whichare shown on the left of Figure 3.
The template ap-proach does fairly well in this respect: half its sug-gestions are rated 3 or higher, which is better thanthe bigram method but slightly worse than real titles.Interestingly, distributional differences between theratings for template-based output and actual song ti-tles are not statistically significant, while all otherdifferences are significant4.
This suggests that Titu-lar?s titles are nearly as good as real song titles, froman inspirational perspective.
Unlike other methods,no single template-based title received a unanimousrating of 5 from all three evaluators (although ?TheLungs Are Pumping?
and ?Sure Spider Vengeance?both received average scores of 4.7).For LyriCloud, evaluators were asked to rate boththe level of inspiration and whether or not theyfound the words in a cloud to be unrelated (1) or4Kolmogorov-Smirnov tests with p < 0.05.
Multiple testsin this paper are corrected for using the Bonferroni method.54related (5).
These results are shown in the centerand right plots of Figure 3.
In both measures, thetf?idf approach outperforms the topic model, whichin turn outperforms a random collection of words.All differences are statistically significant.
Interest-ingly, the R2 coefficient between these two mea-sures is only 0.25 for all word clouds in the evalu-ation, meaning that relatedness ratings only explain25% of the variance in inspiration ratings (and viceversa).
This curious result implies that there areother, more subtle factors at play in how ?inspiring?humans find a set of words like this to be.
Addi-tionally, only 36% of evaluators reported that theycould find meaning in the size/scale of words in tf?idfclouds (compared to 9% for the topic model and 1%for random), which is lower than anticipated.3.2 Anecdotal ResultsThe implementations described in this paper weredeveloped over a four-day period, and made avail-able to FAWM participants mid-way through thesongwriting challenge on February 16, 2010.
Theywere promoted as part of a suite of online toolscalled The Muse5, which also includes two otherprograms, Struxxure and Plot Spline, aimed at help-ing songwriters consider various novel song struc-tures and plot constraints.
These other tools donot use any language modeling, and a discussion ofthem is beyond the scope of this paper.Table 4 summarizes the internet traffic that TheMuse received between its launch and the end of theFAWM challenge on March 1, 2010 (thirteen days).Encouragingly, Titular and LyriCloud were the mostpopular destinations on the website.
These statis-tics include visitors from 36 countries, mostly fromthe United States (55%), Germany (15%) and theUnited Kingdom (9%).
News of these tools spreadwell beyond the original FAWM community, as 29%of website visitors were referred via hyperlinks fromunaffiliated music forums, songwriting blogs, andsocial websites like Facebook and Twitter.FAWM participants typically post their songs on-line after completion to track their progress, so theywere asked to ?tag?
the songs they wrote with helpfrom these creativity tools as a way to measure howmuch the tools were being used in practice.
By the5http://muse.fawm.orgTool Pageviews % TrafficTitular 11,408 42.9%LyriCloud 5,313 20.0%Struxxure 4,371 16.5%Plot Spline 3,219 12.1%Home Page 2,248 8.5%Total 26,559 100.0%Table 4: Website activity for The Muse tools betweenFebruary 16 and March 1, 2010.
The creativity tools dis-cussed in this paper are italicized.end of the challenge, 66 songs were tagged ?titular?and 29 songs were tagged ?lyricloud.?
Note thatthese figures are conservative lower-bounds for ac-tual usage, since not all participants tag their songsand, as previously stated, a significant portion of theinternet traffic for these tools came from outside theFAWM community.The tools were published without detailed instruc-tions, so songwriters were free to interpret themhowever they saw fit.
Several users found inspirationin Titular titles that are interpretable as metaphor orsynecdoche.
For example, Expendable Friend (thestage name of British singer/songwriter Jacqui Car-nall) wrote a song around the suggestion ?I Am YourAdult,?
which she interpreted this way:So, this is a song about the little voice insideyou that stops you from doing fun things be-cause you?re not a child any more and youcan?t really get away with doing them.Surprisingly, even non-lyricists adopted Titular.
?Mexican of No Breakup?
led guitarist Ryan Dayto compose a latin-style instrumental, and ?What aSunset!?
became an ambient electronic piece by anartist by the pseudonym of Vom Vorton.While LyriCloud was about half as popular asTitular, it was arguably open to a wider range ofinterpretation.
Some songwriters used it as origi-nally envisioned, i.e., a ?browser?
for interactivelyexploring lyrical possibilities.
New York lawyer andfolksinger Mike Skliar wrote two songs this way.
Hesaid this about the process:I had a few images in mind, and I would typein the key word of the image, which generatedother words sometimes slightly related... thenkind of let my mind free associate on whatthose words might mean juxtaposed against55the first image, and came up with about halfthe song that way.Unexpectedly, some took LyriCloud as a challengeto write a song using all the words from a singlecloud (or as many as possible), since it chooses aseed word at random if none is provided as input.Songwriter James Currey, who actually used Lyri-Cloud and Titular together to write ?For the Bethle-hem of Manhattan,?
described the process this way:It was like doing a puzzle, and the result is ac-tually quite surprisingly coherent AND good.As a final anecdote, middle school Englishteacher Keith Schumacher of California was aFAWM 2010 participant.
He shared these tools withfaculty members at his school, who designed an in-class creative writing exercise for students involvingwords generated by LyriCloud, projected overheadin the classroom.
This demonstrates the utility ofthese and similar tools to a broad range of age groupsand writing styles.4 Conclusions and Future WorkIn this paper, I introduced the task of designing com-putational creativity tools for songwriters.
I de-scribed two such tools, Titular and LyriCloud, andpresented an empirical evaluation as well as anecdo-tal results based on actual usage during an interna-tional songwriting event.
I also presented two cri-teria for creativity tools?that their suggestions beboth unlikely and meaningful to the human artistsinteracting with them?and showed how these prin-ciples guided the development of the tools.This preliminary foray into creativity toolsbased on language modeling shows the potentialfor creativity-oriented human-computer interaction.However, there is still much that can be improvedon.
For example, Titular might benefit from train-ing with a more traditional context-free grammar(i.e., one with recursive production rules), whichmight yield more complex and interesting possibili-ties.
LyriCloud could be extended to include bigramand trigram phrases in addition to single words.Also, the vocabularies of both systems might cur-rently suffer due to the limited and domain-specifictraining data (the lyrics corpus), which could be sup-plemented with other sources.Perhaps more importantly, neither of the currenttools incorporate any explicit notion of wordplay(e.g., rhyme, alliteration, assonance, puns) or anyother device of creative writing (meter, repetition,irony, etc.).
The systems occasionally do suggest in-stances that embody these properties, but they arewholly by chance at this stage.
One can, however,imagine future versions that take preference param-eters as input from the user, and try to suggest in-stances based on these constraints.AcknowledgmentsThanks to Jacob Eisenstein for helpful discussionsduring the development of these tools, and to theanonymous reviewers for valuable suggestions thatmuch improved the paper.
To all the ?fawmers?
whoinspired the project, used the tools, and providedfeedback?particularly those who gave permissionto be discussed in Section 3.2?you rock.ReferencesS.
Bateman, C. Gutwin, and M. Nacenta.
2008.
Seeingthings in the clouds: the effect of visual features on tagcloud selections.
In Proceedings of the ACM Confer-ence on Hypertext and Hypermedia, pages 193?202.ACM.K.
Binstead.
1996.
Machine humour: An implementedmodel of puns.
Ph.D. thesis, University of Edinburgh.D.M.
Blei, A.Y.
Ng, and M. Jordan.
2003.
Latent Dirich-let alocation.
Journal of Machine Learning Research,3:993?1022.A.
Carlson, J. Betteridge, R. Wang, E.R.
Hruschka Jr, andT.
Mitchell.
2010.
Coupled semi-supervised learningfor information extraction.
In Proceedings of the In-ternational Conference on Web Search and Data Min-ing (WSDM), pages 101?110.
ACM Press.K.
Church and P. Hanks.
1989.
Word associate norms,mutual information and lexicography.
In Proceed-ings of the Association for Computational Linguistics(ACL), pages 76?83.
ACL Press.S.
Davis.
1992.
The Songwriters Idea Book.
Writer?sDigest Books.K.
Deemter, M. Theune, and E. Krahmer.
2005.
Realversus template-based natural language generation:a false opposition?
Computational Linguistics,31(1):15?24.M.
Farbood and B. Schoner.
2001.
Analysis and syn-thesis of Palestrina-style counterpoint using Markovchains.
In Proceedings of the International Computer56Music Conference, pages 471?474.
International Com-puter Music Association.P.
Gerva?s.
2001.
An expert system for the composition offormal spanish poetry.
Journal of Knowledge-BasedSystems, 14:181?188.D.
Jurafsky and J.H.
Martin.
2008.
Speech and Lan-guage Processing.
Prentice Hall.K.
Lari and S. J.
Young.
1990.
The estimation of stochas-tic context-free grammars using the inside-outside al-gorithm.
Computer Speech and Language, 4:35?56.T.
Li and M. Ogihara.
2004.
Music artist style identi-fication by semi-supervised learning from both lyricsand content.
In Proceedings of the ACM InternationalConference on Multimedia, pages 364?367.
ACM.H.
Manurung.
2004.
An evolutionary algorithm ap-proach to poetry generation.
Ph.D. thesis, Universityof Edinburgh.M.
Mintz, S. Bills, R. Snow, and D. Jurafsky.
2009.Distant supervision for relation extraction without la-beled data.
In Proceedings of the Association for Com-putational Linguistics (ACL), pages 1003?1011.
ACLPress.N.
Montfort.
2006.
Natural language generation and nar-rative variation in interactive fiction.
In Proceedings ofthe AAAI Workshop on Computational Aesthetics.Y.
Netzer, D. Gabay, Y. Goldberg, and M. Elhadad.
2009.Gaiku : Generating haiku with word associationsnorms.
In Proceedings of the Workshop on Compu-tational Approaches to Linguistic Creativity (CALC),pages 32?39.
ACL Press.R.
Neumayer and A. Rauber.
2007.
Integration of textand audio features for genre classification in music in-formation retrieval.
In Proceedings of the EuropeanConference on Information Retrieval (ECIR), pages724?727.
Springer.H.R.G.
Oliveira, F.A.
Cardoso, and F.C.
Pereira.
2007.Tra-la-lyrics: An approach to generate text based onrhythm.
In Proceedings of the International JointWorkshop on Computational Creativity, London.A.
Ramakrishnan, S. Kuppan, and S. Lalitha Devi.
2009.Automatic generation of tamil lyrics for melodies.
InProceedings of the Workshop on Computational Ap-proaches to Linguistic Creativity (CALC), pages 40?46.
ACL Press.C.
Solis, J.T.
Siy, E. Tabirao, and E. Ong.
2009.
Plan-ning author and character goals for story generation.In Proceedings of the Workshop on Computational Ap-proaches to Linguistic Creativity (CALC), pages 63?70.
ACL Press.D.
Thompson.
2007.
Hallo Spaceboy: The Rebirth ofDavid Bowie.
ECW Press.J.M.
Yang, C.Y.
Lai, and Y.H.
Hsieh.
2007.
A quantita-tive measurement mechanism for lyrics evaluation: Atext mining approach.
In Proceedings of the Interna-tional Conference on Business and Information (BAI).ATISR.X.
Zhu, Z. Xu, and T. Khot.
2009.
How creative is yourwriting?
In Proceedings of the Workshop on Compu-tational Approaches to Linguistic Creativity (CALC),pages 87?93.
ACL Press.57
