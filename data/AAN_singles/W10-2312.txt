Proceedings of the 2010 Workshop on Graph-based Methods for Natural Language Processing, ACL 2010, pages 74?82,Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational LinguisticsExperiments with CST-based Multidocument SummarizationMaria Luc?a del Rosario Castro Jorge, Thiago Alexandre Salgueiro PardoN?cleo Interinstitucional de Ling?
?stica Computacional (NILC)Instituto de Ci?ncias Matem?ticas e de Computa?
?o, Universidade de S?o PauloAvenida Trabalhador s?o-carlense, 400 - CentroP.O.Box 668.
13560-970, S?o Carlos/SP, Brazil{mluciacj,taspardo}@icmc.usp.brAbstractRecently, with the huge amount of growinginformation in the web and the littleavailable time to read and process all thisinformation, automatic summaries havebecome very important resources.
In thiswork, we evaluate deep content selectionmethods for multidocument summarizationbased on the CST model (Cross-documentStructure Theory).
Our methods considersummarization preferences and focus on theoverall main problems of multidocumenttreatment: redundancy, complementarity, andcontradiction among different informationsources.
We also evaluate the impact of theCST model over superficial summarizationsystems.
Our results show that the use ofCST model helps to improve informativenessand quality in automatic summaries.1 IntroductionIn the last years there has been a considerableincrease in the amount of online information andconsequently the task of processing thisinformation has become more difficult.
Just tohave an idea, recent studies conducted by IDCshowed that 800 exabytes of information wereproduced in 2009, and it is estimated that in 2012it will be produced 3 times more.
Among all ofthis information, there is a lot of related contentthat comes from different sources and thatpresents similarities and differences.
Readingand dealing with this is not straightforward.
Inthis scenario, multidocument summarization hasbecome an important task.Multidocument summarization consists inproducing a unique summary from a set ofdocuments on the same topics (Mani, 2001).
Amultidocument summary must contain the mostrelevant information from the documents.
Forexample, we may want to produce amultidocument summary from all the documentstelling about the recent world economical crisisor the terrorism in some region.
As an example,Figure 1 reproduces a summary from Radev andMckeown (1998), which contains the main factsfrom 4 news sources.Figure 1: Example of multidocument summary(Radev and Mckeown, 1998, p. 478)Multidocument summarization has to deal notonly with the fact of showing relevantinformation but also with some multidocumentphenomena such as redundancy,complementarity, contradiction, informationordering, source identification, temporalresolution, etc.
It is also interesting to notice that,instead of only generic summaries (as the one inthe example), summaries may be producedconsidering user preferences.
For example, onemay prefer summaries including informationattributed to particular sources (if one trusts morein some sources) or more context information(considering a reader that has not accompaniedsome recent important news), among otherpossibilities.Reuters reported that 18 people were killed in aJerusalem bombing Sunday.
The next day, a bombin Tel Aviv killed at least 10 people and wounded30 according to Israel radio.
Reuters reported thatat least 12 people were killed and 105 wounded.Later the same day, Reuters reported that theradical Muslim group Hamas had claimedresponsibility for the act.74There are two main approaches formultidocument summarization (Mani andMaybury, 1999): the superficial and the deepapproaches.
Superficial approach uses littlelinguistic knowledge to produce summaries.
Thisapproach usually has low cost and is morerobust, but it produces poor results.
On the otherhand, deep approaches use more linguisticknowledge to produce summaries.
In generalterms, in this approach it is commonly usedsyntactical, semantic and discourse parsers toanalyze the original documents.
A very commonway to analyze documents consists inestablishing semantic relations among thedocuments parts, which helps identifyingcommonalities and differences in information.Within this context, discourse models as CST(Cross-document Structure Theory) (Radev,2000) are useful (see, e.g., Afantenos et al,2004; Afantenos, 2007; Jorge and Pardo, 2009,2010; Radev and Mckeown, 1998; Radev et al,2001; Zhang et al, 2002).It was proposed in Mani and Maybury (1999)a general architecture for multidocumentsummarization, with analysis, transformation,and synthesis stages.
The first stage consists inanalyzing and formally representing the contentof the original documents.
The second stageconsists mainly in transforming the representedcontent into a condensed content that will beincluded in the final summary.
One of the mostimportant tasks in this stage is the contentselection process, which consists in selecting themost relevant information.
Finally, the thirdstage expresses the condensed content in naturallanguage, producing the summary.In this paper, we explore a CST-basedsummarization method and evaluate thecorresponding prototype system formultidocument summarization.
Our system,called CSTSumm (CST-based SUMMarizer),produces multidocument summaries from inputCST-analyzed news documents.
We mainlyinvestigate content selection methods forproducing both generic and preference-basedsummaries.
Particularly, we formalize and codifyour content selection strategies as operators thatperform the previously cited transformationstage.
We run our experiments with BrazilianPortuguese news texts (previously analyzedaccording to CST by human experts) and showthat we produce more informative summaries incomparison with some superficial summarizers(Pardo, 2005; Radev et al, 2000).
We also useCST to enrich these superficial summarizers,showing that the results also improve.
Ourgeneral hypothesis for this work is that the deepknowledge provided by CST helps to improveinformation and quality in summaries.This work is organized as follows.
In Section2, the main concepts of the CST model areintroduced and the works that have already usedCST for multidocument summarization arereviewed.
In Section 3, we present CSTSumm,while its evaluation is reported in Section 4.Some final remarks are presented in Section 5.2 Related Work2.1 Cross-document Structure TheoryRadev (2000) proposed CST model with a set of24 relations for multidocument treatment in anydomain.
Table 1 lists these relations.Table 1: CST original relationsIdentity JudgmentEquivalence FulfillmentTranslation DescriptionSubsumption Reader profileContradiction ContrastHistorical background ParallelModality Cross-referenceAttribution CitationSummary RefinementFollow-up AgreementElaboration GeneralizationIndirect speech Change of perspectiveThe established relations may have (or not)directionality, e.g., the equivalence relation(which states that two text segments have similarcontent) has no directionality while the historicalbackground relation (which states that a segmentprovides historical information about other) has.Figure 2 shows examples of these two relationsamong sentences from different sources.As part of the model, the author proposes ageneral schema that reveals the possibility ofrelationship at any level of linguistic analysis.Figure 3 (reproduced from Radev, 2000)illustrates this schema.
According to this schema,the documents with CST relations arerepresented as a graph, whose nodes are textsegments (of possibly any level) and the edgesare relations.
This graph is possiblydisconnected, since not all segments presentrelations with other segments.
It is important tosay that, in general, only one analysis level istreated.
In this work, we only deal with sentencesfrom the input documents, since sentences are75well delimited and are standard segments indiscourse analysis.Figure 2: Examples of CST relationsFigure 3: CST general schema (Radev, 2000, p. 78)2.2 Multidocument SummarizationA few works explored CST for multidocumentsummarization.
A 4-stage multidocumentsummarization methodology was proposed inRadev (2000).
In this methodology, the firststage consists in clustering documents accordingto their topics.
In the second stage, internalanalysis (syntactical and semantic, for instance)of the texts may be performed.
In the third stage,CST relations are established among texts.Finally, in the fourth stage, information isselected to produce the final summary.
For thismethodology the author suggests using operatorsactivated by user summarization preferencessuch as authorship (i.e., reporting the informationsources) or contradictory information preference.The author also says that it may be possible toproduce generic summaries without consideringa particular preference.
In this case the criterionused to select information is based on the numberof CST relations that a segment has.
Thiscriterion is based on the idea that relevantinformation is more repeated/elaborated andrelated to other segments across documents.
Thismay be easily verified in practice.
In this paperwe follow such ideas.A methodology for enriching multidocumentsummaries produced by superficial summarizerswas proposed by Zhang et al (2002).
Theauthors incorporated the information given byCST relations to MEAD (Radev et al, 2000)summarization process, showing that givingpreference to segments with CST relationsproduces better summaries.
Otterbacher et al(2002) investigated how CST relations mayimprove cohesion in summaries, which wastested by ordering sentences in summariesaccording to CST relations.
The idea used behindthis ordering is that sentences related by CSTrelations should appear closer in the finalsummaries as well as should respect possibletemporal constraints indicated by some relations.Afantenos et al (2004) proposed anothersummarization methodology that extractsmessage templates from the texts (usinginformation extraction tools) and, according tothe type of CST relation between two templates,produces a unified message that would representthe summary content.
The authors did not fullyimplement this method.3 CSTSummIn this paper we evaluate a CST-basedmultidocument summarization method byimplementing and testing a prototype system,called CSTSumm.
It performs content selectionoperations over a group of texts on the sametopic that were previously annotated according toCST.
For the moment, we are using manuallyannotated texts, i.e., the analysis stage ofmultidocument summarization is only simulated.In the future, texts may be automaticallyannotated, since a CST parser is underdevelopment for Brazilian Portuguese language(Maziero et al, 2010).Initially, the system receives as input theCST-annotated texts, which are structured as agraph.
An initial rank of sentences is then built:the sentences are ordered according to thenumber of CST relations they present; the moreEquivalence relationSentence 1: Nine people died, three of themchildren, and 25 others were wounded lastMonday in a blast at a market in Moscow,police said.Sentence 2: Nine people died, including threechildren, and 25 others were injured lastMonday in an explosion that happened at amarket in Moscow, police of Moscowinformed.Historical background relation(directionality: from Sentence 2 to 1)Sentence 1: An airplane accident in Bukavu,east of Democratic Republic of Congo, killed13 people this Thursday in the afternoon.Sentence 2: Congo has a history of more than30 airplane tragedies.76relations a sentence presents, better ranked it willbe.
Having the initial rank, content selection isperformed.
In this work, following the idea ofJorge and Pardo (2010), we represent and codifyeach content selection strategy as an operator.
Acontent selection operator tells how to rearrangethe sentences in the rank in order to producesummaries that better satisfy the correspondinguser preferences.
For instance, if a user requiresmore context information in the summary, thecorresponding operator is activated.
Suchoperator will (i) select in the rank all thesentences that present historical background andelaboration CST relations with better rankedsentences and (ii) improve their position in therank by putting them immediately after the betterranked sentences with which they are related.This final action would give to these?contextual?
sentences more preference for beingin the summary, since they are better positionedin the refined rank.
Figure 4 shows an exampleof a hypothetical CST graph (derived from agroup of texts), the corresponding initial rank(with relations preserved for clarification) andthe transformation that the context operatorwould do for producing the new/refined rank.
Itis possible to see that sentence 1, that presentshistorical information about the sentence 4, getsa better position in the rank (immediately aftersentence 4), receiving some privilege to be in thesummary.Besides the context operator, we also haveother 3 operators: the contradiction operator(which looks for the contradiction CST relationin order to include in the summary everycontradiction in the texts), the authorshipoperator (which looks for the citation andattribution CST relations in order to include inthe summary possible sources that provided theavailable information), and the evolving eventsoperator (which looks for historical backgroundand follow-up CST relations in order to presentthe development of the events during a timeperiod).Independently from the user preference, anextra operator is always applied: the redundancyoperator.
It removes from the rank all sentenceswhose information is already expressed in otherbetter ranked sentences.
Redundancy isrepresented by the identity, equivalence, andsubsumption CST relations.After the content selection process, in the laststage ?
the synthesis stage ?
the system selects asmany sentences from the rank as allowed by thespecified compression rate.
The compression rate(provided by the user) informs the size of thesummary.
For instance, a 70% rate indicates thatthe summary must have at most 30% of thenumber of words in a text.
In this work, given themultidocument nature of the task, we computethe compression rate over the size of the longesttext in the group.Hypothetical CST graphInitial rankRefined rank (after applying the operator)Figure 4: Example of context operator applicationSynthesis stage also orders the selected sentencesaccording to a simple criterion that onlyconsiders the position of the sentences in theoriginal documents: first sentences appear first inthe summary.
If two sentences have the sameposition but in different documents, then thesentences are ordered according to the documentnumber.
Finally, we apply a sentence fusionsystem (Seno and Nunes, 2009) to some selectedsentences.
This is done when sentences withoverlap CST relation among them are selected tothe summary.
The overlap relation indicates thatthe sentences have similar content, but also thatboth present unique content.
In this case, it isdesired that the sentences become only one withthe union of their contents.
The fusion systemthat we use does that.
Figure 5 illustrates thefusion process, with the original sentences and aresulting fusion.Figure 6 shows the general architecture ofCSTSumm, which summarizes the whole processdescribed before.
Each operator is codified in77XML, where it is specified which relationsshould be looked in the rank in order to have thecorrespondent sentences better ranked.
It isimportant to notice that, excepting theredundancy operator, our system was designed toallow the application of only one contentselection operator at a time.
If more than oneoperator is applied, the application of thefollowing operator may probably rewrite themodifications in the rank that the previousoperator has done.
For instance, the applicationof the contradiction operator after the contextoperator might include sentences withcontradiction above sentences with contextinformation in the rank, altering therefore therank produced by the context operator.
Onesimple alternative to this design choice is to askthe user to rank his preferences and, then, toapply the corresponding operators in the oppositeorder, so that the rank produced by the mostimportant preference will not be further altered.Other alternative is to produce more complexoperators that combine preferences (and thecorresponding CST relations), but somepreference on the relations should still bespecified.Figure 5: Example of sentence fusionFigure 6: CSTSumm architectureIn Figure 7 we show the algorithm for theapplication of operators during content selectionprocess.
It is important to notice that the selectedoperator looks for its relations in all pairs ofsentences in the rank.
Once it finds the relations,it rearranges the rank appropriately, by puttingthe related sentence more above in the rank.Figure 7: Algorithm for application of contentselection operatorsAs an illustration of the results of our system,Figure 8 shows an automatic summary producedfrom a group of 3 texts with the application ofthe context operator (after redundancy operatorwas applied) and a 70% compression rate.
Thesummary was translated from Portuguese, thelanguage with which the summarizer was tested.Figure 8: Example of multidocument summary withcontext information4 EvaluationOur main research question in this work was howhelpful CST would be for producing bettersummaries.
CSTSumm enables us to assess thesummaries and content selection strategies, but acomparison of these summaries with summariesproduced by superficial methods is stillnecessary.
In fact, we not only proceeded to suchSentence 1: According to a spokesman fromUnited Nations, the plane was trying to land atthe airport in Bukavu in the middle of a storm.Sentence 2: Everyone died when the plane,hampered by bad weather, failed to reach therunway and crashed in a forest 15 kilometersfrom the airport in Bukavu.Fusion: According to a spokesman for the UnitedNations, everyone died when a plane that wastrying to land at Bukavu airport, hampered by badweather, failed to reach the runway and crashedin a forest 15 kilometers from the airport.procedure for application of content selectionoperatorsinput data: initial rank, user summarizationpreference, operatorsoutput data: refined rankapply the redundancy operatorselect one operator according to the usersummarization preferencefor i=sentence at the first position in the rank to thelast but one sentencefor j=sentence at position i+1 in the rank to thelast sentenceif the operator relations happen amongsentences i and j, rearrange the rankappropriatelyThe Brazilian volleyball team has won on Fridaythe seventh consecutive victory in the WorldLeague, defeating Finland by 3 sets to 0 - partialsof 25/17, 25/22 and 25/21 - in a match in theTampere city, Finland.
The first set remainedbalanced until the middle, when Andr?
Hellerwent to serve.
In the last part, Finland againpaired the game with Brazil, but after a sequenceof Brazilians points Finland failed to respond andlost by 25 to 21.
The Brazilian team has won fivetimes the World League in 1993, 2001, 2003,2004 and 2005.78comparison, but also improved the superficialmethods with CST knowledge.As superficial summarizers, we selectedMEAD (Radev et al, 2000) and GistSumm(Pardo et al, 2003; Pardo, 2005) summarizers.MEAD works as follows.
Initially, MEAD buildsan initial rank of sentences according to a scorebased on three parameters: position of thesentence in the text, lexical distance of thesentence to the centroid of the text, and the sizeof the sentence.
These three elements are linearlycombined for producing the score.
GistSumm, onthe other side, is very simple: the systemjuxtaposes all the source texts and gives a scoreto each sentence according to the presence offrequent words (following the approach of Luhn,1958) or by using TF-ISF (Term Frequency ?Inverse Sentence Frequency, as proposed inLarroca et al, 2000).
Following the work ofZhang et al (2002), we decided to use CST torearrange (and supposedly improve) the sentenceranks produced by MEAD and GistSumm.
Wesimply add to each sentence score the number ofCST relations that the sentence presents:new sentence score = old sentence score + number ofCST relationsThe number of sentences is retrieved from theCST graph.
This way, the sentence positions inthe rank are changed.For our experiments, we used the CSTNewscorpus (Aleixo and Pardo, 2008), which is acorpus of news texts written in BrazilianPortuguese.
The corpus contains 50 clusters oftexts.
Each group has from 2 to 4 texts on thesame topic annotated according to CST byhuman experts, as well as a manual genericsummary with 70% compression rate (in relationto the longest text).
The annotation process wascarried out by 4 humans, with satisfactoryagreement, which demonstrated that theannotation task was well defined and performed.More details about the corpus and its annotationprocess are presented by Maziero et al (2010).For each cluster of CSTNews corpus, it wasproduced a set of automatic summariescorresponding to each method that was exploredin this work.
To evaluate the informativity andquality of the summaries, we used two types ofevaluation: automatic evaluation and humanevaluation.
For the automatic evaluation we usedROUGE (Lin, 2004) informativity measure,which compares automatic summaries withhuman summaries in terms of the n-grams thatthey have in common, resulting in precision,recall and f-measure numbers between 0 (theworst) and 1 (the best), which indicate how muchinformation the summary presents.
Precisionindicates the amount of relevant information thatthe automatic summary contains; recall indicateshow much information from the human summaryis reproduced in the automatic summary; f-measure is a unique performance measure thatcombines precision and recall.
Although it lookssimple, ROUGE author has showed that itperforms as well as humans in differentiatingsummary informativeness, which caused themeasure to be widely used in the area.
Inparticular, for this work, we considered onlyunigram comparison, since the author of themeasure demonstrated that unigrams are enoughfor differentiating summary quality.
Forcomputing ROUGE, we compared eachautomatic summary with the correspondinghuman summary in the corpus.We computed ROUGE for every summary weproduced through several strategies: using onlythe initial rank, only the redundancy operator,and the remaining preference operators (appliedafter the redundancy operator).
Is is important tonotice that it is only fair to use ROUGE toevaluate the summaries produced by the initialrank and by the redundancy operator, since thehuman summary (to which ROUGE comparesthe automatic summaries) are generic, producedwith no preference in mind.
We only computedROUGE for the preference-biased summaries inorder to have a measure of how informative theyare.
Ideally, these preference-biased summariesshould not only mirror the user preference, butalso contain the main information from thesource texts.On the other hand, we used human evaluationto measure the quality of the summaries in termsof coherence, cohesion and redundancy, factorsthat ROUGE is not sensitive enough to capture.By coherence, we mean the characteristic of atext having a meaning and being understandable.By cohesion, we mean the superficial makers ofcoherence, i.e., the sequence of text elements thatconnect the ideas in the text, as punctuation,discourse markers, anaphors, etc.For each one of the above evaluation factors,a human evaluator was asked to assign one offive values: very bad (score 0), bad (score 1),regular (score 2), good (score 3), and excellent(score 4).
We also asked humans to evaluateinformativity in the preference-biased summariesproduced by our system, which is a more fair79evaluation than the automatic one describedabove.
The user should score each summary(using the same values above) according to howmuch he was satisfied with the actual content ofthe summary in face of the preference made.
Theuser had access to the source texts for performingthe evaluation.Table 2 shows the ROUGE scores for thesummaries produced by the initial rank, by theapplication of the operators, by the superficialsummarizers, and by the CST-enrichedsuperficial summarizers.
It is important to saythat these results are the average results obtainedfor the automatic summaries generated for all theclusters in the CSTNews corpus.Table 2: ROUGE resultsContent selection method Precision Recall F-measureInitial rank 0.5564 0.5303 0.5356Redundancy treatment (only) 0.5761 0.5065 0.5297Context information 0.5196 0.4938 0.4994Authorship information 0.5563 0.5224 0.5310Contradiction information 0.5503 0.5379 0.5355Evolving events information 0.5159 0.5222 0.5140MEAD without CST 0.5242 0.4602 0.4869MEAD with CST 0.5599 0.4988 0.5230GistSumm without CST 0.3599 0.6643 0.4599GistSumm with CST 0.4945 0.5089 0.4994As expected, it may be observed that the bestresults were achieved by the initial rank (since itproduces generic summaries, as happens to thehuman summaries to which they are compared),which does not consider any summarizationpreference at all.
It is also possible to see that: (a)the superficial summarizers are outperformed bythe CST-based methods and (b) CST-enrichedsuperficial summarizers produced better resultsthan the superficial summarizers.Results for human evaluation are shown inTable 3.
These results show the average value foreach factor evaluated for a sample group of 48texts randomly selected from the corpus.
We alsoassociated to each value the closest concept inour evaluation.
We could not perform theevaluation for the whole corpus due to the highcost and time-demanding nature of the humanevaluation.
Six humans carried out thisevaluation.
Each human evaluated eightsummaries, and each summary was evaluated bythree humans.Table 3: Results for human evaluationContent selection method Coherence Cohesion Redundancy InformativityInitial rank 3.6Excellent3.2Good1.8Regular3.6ExcellentContext  2.1Regular2.7Good3.6Excellent2.2RegularAuthorship3.3Good2.4Regular2.8Good3GoodContradiction  2.4Regular2.7Good2.5Regular3.7ExcellentEvolving events  2.1Regular2.5Regular2.6Good3.2GoodIt may be observed that informativity factorresults are quite satisfactory, since more than50% of the judges considered that theperformance was excellent.
For coherence,cohesion and redundancy factors, results werenot excellent in all the cases, but they were notbad either.
We consider that one of the thingsthat could have had an influence in this case isthe performance of the fusion system, since itmay generate sentences with some problems ofcoherence and cohesion.
There are also otherthings that may influence these results, such as80the method for ordering sentences that we usedin this work.
This method does not follow anydeep criteria to order sentences and may alsolead to coherence and cohesion problems.These results show that CSTSumm is capableof producing summaries with good informativityand quality.
In fact, the results validate ourhypothesis that deep knowledge may improve theresults, since it deals better with themultidocument phenomena, as the presence ofredundant, complementary and contradictoryinformation.5 Final RemarksAlthough we consider that very good resultswere achieved, there is still room forimprovements.
Future works include theinvestigation of better sentence orderingmethods, as well as more investigation on how tojointly apply more than one content selectionoperator.For the moment, CSTSumm assumes that thetexts to be summarized must be alreadyannotated with CST.
In the future, as soon as anautomatic CST parser is available forPortuguese, it should provide the suitable inputto the summarizer.Finally, it is interesting to notice that,although we have tested our methods withBrazilian Portuguese texts, they are robust andgeneric enough to be applied to any otherlanguage, since both our methods and CSTmodel are language independent.AcknowledgmentsThe authors are grateful to FAPESP and CNPqfor supporting this work.ReferencesAfantenos, S.D.
; Doura, I.; Kapellou, E.;Karkaletsis, V. 2004.
Exploiting Cross-Document Relations for Multi-documentEvolving Summarization.
In the Proceedingsof SETN, pp.
410-419.Afantenos, S.D.
2007.
Reflections on the Task ofContent Determination in the Context ofMulti-Document Summarization of EvolvingEvents.
In Recent Advances on NaturalLanguage Processing.
Borovets, Bulgaria.Aleixo, P. and Pardo, T.A.S.
2008.
CSTNews:Um C?rpus de Textos Journal?sticos Anotadossegundo a Teoria Discursiva CST (Cross-Document Structure Theory).
S?rie deRelat?rios T?cnicos do Instituto de Ci?nciasMatem?ticas e de Computa?
?o, Universidadede S?o Paulo no.
326.
S?o Carlos, Brazil .Jorge, M.L.C and Pardo, T.A.S.
2009.
ContentSelection Operators for MultidocumentSummarization based on Cross-documentStructure Theory.
In the Proceedings of the 7thBrazilian Symposium in Information andHuman Language Technology.
S?o Carlos,Brazil.Jorge, M.L.C.
and Pardo, T.A.S.
2010.Formalizing CST-based Content SelectionOperations.
In the Proceedings of the 9thInternational Conference on ComputationalProcessing of Portuguese Language (LectureNotes in Artificial Intelligence 6001), pp.
25-29.
Porto Alegre, Brazil.Larocca Neto, J.; Santos, A.D.; Kaestner, A.A.;Freitas, A.A. 2000.
Generating TextSummaries through the Relative Importance ofTopics.
In M.C.
Monard and J.S.
Sichman(eds.
), Lecture Notes in Artificial Intelligence,N.
1952, pp.
300-309.
Springer, Verlag.Lin, C-Y.
2004.
ROUGE: a Package forAutomatic Evaluation of Summaries.
In theProceedings of the Workshop on TextSummarization Branches Out.
Barcelona,Spain.Luhn, H.P.
1958.
The automatic creation ofliterature abstracts.
IBM Journal of Researchand Development, Vol.
2, pp.
159-165.Barcelona, Spain.Mani, I. and Maybury, M. T. 1999.
Advances inautomatic text summarization.
MIT Press,Cambridge, MA.Mani, I.
2001.
Automatic Summarization.
JohnBenjamins Publishing Co. Amsterdam.Maziero, E.G.
; Jorge, M.L.C.
; Pardo, T.A.S.2010.
Identifying Multidocument Relations.
Inthe Proceedings of the 7th InternationalWorkshop on Natural Language Processingand Cognitive Science.
June 8-12,Funchal/Madeira, Portugal.Otterbacher, J.C.; Radev, D.R.
; Luo, A.
2002.Revisions that improve cohesion in multi-document summaries: a preliminary study.
Inthe Proceedings of the Workshop onAutomatic Summarization, pp 27-36.Philadelphia.81Pardo, T.A.S.
; Rino, L.H.M.
; Nunes, M.G.V.2003.
GistSumm: A Summarization ToolBased on a New Extractive Method.
In N.J.Mamede, J. Baptista, I. Trancoso, M.G.V.Nunes (eds.
), 6th Workshop on ComputationalProcessing of the Portuguese Language -Written and Spoken (Lecture Notes inArtificial Intelligence 2721), pp.
210-218.
Faro, Portugal.Pardo, T.A.S.
2005.
GistSumm - GISTSUMMarizer: Extens?es e NovasFuncionalidades.
S?rie de Relat?rios doNILC.
NILC-TR-05-05.
S?o Carlos, Brazil.Radev, D. and McKeown, K. 1998.
Generatingnatural language summaries from multiple on-line sources.
Computational Linguistics, Vol.24, N. 3, pp.
469-500.Radev, D.R.
2000.
A common theory ofinformation fusion from multiple text sources,step one: Cross-document structure.
In theProceedings of the 1st ACL SIGDIALWorkshop on Discourse and Dialogue.
HongKong.Radev, D.R.
; Jing, H.; Budzikowska, M. 2000.Centroid-based summarization of multipledocuments: sentence extraction, utility-basedevaluation and user studies.
In theProceedings of the ANLP/NAACL Workshop,pp.
21-29.Radev, D.R.
; Blair-Goldensohn, S.; Zhang, Z.2001.
Experiments in single and multi-document summarization using MEAD.
In theProceedings of the 1st DocumentUnderstanding Conference.
New Orleans, LA.Seno, E.R.M.
and Nunes, M.G.V.
2009.Reconhecimento de Informa?
?es Comuns paraa Fus?o de Senten?as Compar?veis doPortugu?s.
Linguam?tica, Vol.
1, pp.
71-87.Zhang, Z.; Goldenshon, S.B.
; Radev, D.R.
2002.Towards CST-Enhanced Sumarization.
In theProceedings of the 18th National Conferenceon Artificial Intelligence.
Edmonton.82
