SMOOTHING OF AUTOMATICALLY GENERATEDSELECTIONAL CONSTRAINTSRalph Grishman and John SterlingDepartment ofComputer ScienceNew York UniversityNew York, NY 10003ABSTRACTFrequency information on co-occurrence patterns can be automati-cally collected from a syntactically analyzed corpus; this informationcan then serve as the basis for selectional constraints when analyz-ing new text from the same domain.
Better coverage of the domaincan be obtained by appropriate g neralization f the specific wordpatterns which are collected.
We report here on an approach to au-tomatically make suitable generalizations: using the co-occurrencedata to compute aconfusion matrix relating individual words, andthen using the confusion matrix to smooth the original frequencydata.1.
INTRODUCTIONSemantic (selectional) constraints are necessary for the accu-rate analysis of natural language t xt.
Accordingly, the acqui-sition of these constraints i an essential yet time-consumingpart of porting a natural anguage system to a new domain.Several research groups have attempted toautomate this pro-cess by collecting co-occurrence patterns (e.g., subject-verb-object patterns) from a large training corpus.
These patternsare then used as the source of selectional constraints in ana-lyzing new text.However, the patterns collected in this way involve specificword combinations from the training corpus.
Unless the train-ing corpus is very large, this will provide only limited cover-age of the range of acceptable semantic ombinations, evenwithin a restricted omain.
In order to obtain better coverage,it will be necessary to generalize from the patterns collectedso that patterns with semantically related words will also beconsidered acceptable.
In most cases this has been done bymanually assigning words to semantic lasses and then gen-eralizing from specific words to their classes.
This approachstill implies a substantial manual burden in moving to a newdomain, since at least some of the semantic word classes willbe domain-specific.In order to fully automate the process of semantic onstraintacquisition, we would like to be able to automatically identifysemantically related words.
This can be done using the co-occurrence data, by identifying words which occur in thesame contexts (for example, verbs which occur with the samesubjects and objects).
From the co-occurrence data one cancompute a similarity relation between words, and then clusterwords of high similarity.
This approach was taken by Sekineet al at UMIST, who then used these clusters to generalizesemantic patterns \[6\].
A similar approach to word clusteringwas reported by Hirschman et al in 1975 \[5\].For our current experiments, we have adopted a slightly dif-ferent approach.
We compute from the co-occurrence data aconfusion matrix, which also measures the interchangeabilityof words in particular contexts.
We then use the confusionmatrix directly to generalize the semantic patterns.2.
THE NATURE OF THE CONSTRAINTSThe constraints we wish to acquire are local semantic on-straints; more specifically, constraints on which words canoccur together in specific syntactic relations.
These includehead-argument relations (e.g., subject-verb-object) and head-modifier elations.
Some constraints may be general (domainindependent), but others will be specific to a particular do-main.
Because it is not practical to state all the allowableword combinations, we normally place words into (seman-tic) word classes and then state the constraints in terms ofallowable combinations of these classes.When these constraints were encoded by hand, they were nor-mally stated as absolute constraints--aparticular combinationof words was or was not acceptable.
With corpus-derivedconstraints, on the other hand, it becomes possible to thinkin terms of a probabilistic model.
For example, based ona training corpus, we would estimate the probability that aparticular verb occurs with a particular subject and object (orwith subject and object from particular classes), or that a verboccurs with a particular modifier.
Then, using the (obviouslycrude) assumption of independent probabilities, we would es-timate the probability of a particular sentence derivation asthe product of the probabilities of all the operations (addingarguments to heads, adding modifiers to heads) required toproduce the sentence, and the probability of a sentence as thesum of the probabilities of its derivations.3.
ACQUIRING SEMANTIC PATTERNSBased on a series of experiments over the past year (as reportedat COLING-92) we have developed the following procedurefor acquiring semantic patterns from a text corpus:2541.
Using unsupervised training methods, create a stochas-tic grammar f om a (non-stochastic) augmented context-free grammar.
Use this stochastic grammar to parse thetraining corpus, taking only the most probable parse(s)of each sentence.2.
Regularize the parses to produce something akin to anLFG f-structure, with explicitly labeled syntactic rela-tions such as SUBJECT and OBJECT.
l3.
Extract from the regularized parse a series of triples ofthe formhead syntactic-relation argwhere arg is the head of the argument or modifier.
Wewill use the notation < wi r wj > for such a triple, and< r w i > for a relation-argument pair.4.
Compute the frequency F of each head and each triplein the corpus.
If a sentence produces N parses, a triplegenerated from a single parse has weight 1/N in the total.For example, the sentenceMary likes young linguists from Limerick.would produce the regularized syntactic structure(s like (subject (np Mary))(object (np linguist (a-pos young)(from (np Limerick)))))from which the following four triples are generated:like subject Marylike object linguistlinguist a-pos younglinguist from LimerickGiven the frequency information F, we can then estimate theprobability that a particular head wi appears with a particularargument or modifier < v wj >:2F (< wi r wj >)F(wi appears as a head in a parse tree)This probability information would then be used in scoringalternative parse trees.
For the evaluation below, however,we will use the frequency data F directly.1 But with somewhat more regulafization than is done in LFG; in particular,passive structures are converted to corresponding active forms.2Note that F(wl appears as a head in a parse tree) is different fromF(wi appears as a head in a triple) since a single head in a parse tree mayproduce several such triples, one for each argument or modifier of that head.Step 3 (the triples extraction) includes a number of specialcases :(a)(h)(c)(d)(e)if a verb has a separable particle (e.g., "out" in"carry out"), this is attached to the head (to createthe head carry-out) and not treated as a separaterelation.
Different particles often correspond tovery different senses of a verb, so this avoidsconflating the subject and object distributions ofthese different senses.if the verb is "be", we generate a relation be-complement between the subject and the predi-cate complement.triples in which either the head or the argumentis a pronoun are discardedtriples in which the argument is a subordinateclause are discarded (this includes subordinateconjunctions and verbs taking clausal arguments)triples indicating negation (with an argument of"not" or "never") are ignored4.
GENERALIZING SEMANTIC PATTERNSThe procedure described above produces a set of frequenciesand probability estimates based on specific words.
The "tradi-tional" approach to generalizing this information has been toassign the words to a set of semantic lasses, and then to col-lect the frequency information on combinations of semanticclasses \[7,3\].Since at least some of these classes will be domain specific,there has been interest in automating the acquisition of theseclasses as well.
This can be done by clustering together wordswhich appear in the same context.
Starting from the file oftriples, this involves:1. collecting for each word the frequency with which itoccurs in each possible context; for example, for a nounwe would collect the frequency with which it occurs asthe subject and the object of each verb2.
defining a similarity measure between words, which re-flects the number of common contexts in which theyappear3.
forming clusters based on this similarity measureSuch a procedure was performed by Sekine et al at UMIST\[6\]; these clusters were then manually reviewed and the re-sulting clusters were used to generalize selectional patterns.255A similar approach to word cluster formation was describedby Hirschman et al in 1975 \[5\].Cluster creation has the advantage that the clusters areamenable to manual review and correction.
On the otherhand, our experience indicates that successful cluster gener-ation depends on rather delicate adjustment of the clusteringcriteria.
We have therefore lected to try an approach whichdirectly uses a form of similarity measure to smooth (gener-alize) the probabilities.Co-occurrence smoothing isa method which has been recentlyproposed for smoothing n-gram models \[4\] .3 The core of thismethod involves the computation of a co-occurrence matrix(a matrix of confusion probabilities) Pc(wj \]wi), which in-dicates the probability of word wj occurring in contexts inwhich word wi occurs, averaged over these contexts.Pc(wj \[w,) = E P(wj ls)P(slw,)$~ P(w~ Is)P(w~ls)P(s)P(wi)where the sum is over the set of all possible contexts .
Foran n-gram model, for example, the context might be the setof n - 1 prior words.
This matrix can be used to take a basictrigram model PB (wn Iw,~-2, wn-0  and produce a smoothedmodelPs(w.lw.-2, w,-x)  = ~ Pc(w.lw'.)PB(w'.
1~.-2, Wn-1)We have used this method in a precisely analogous way tocompute smoothed semantic triples frequencies, Fs.
In triplesof the form wordl relation word2 we have initially chosen tosmooth over wordl ,treating relation and word2 as the context.Pc(wilw}) = ~ P(wil < r wj >) .
P (< r wj > Iw~)r~Wj~w F(< wi r wj >) :  z;E>3F(< w~ r wj >)F(w~ appears as a head of a triple)!
rs(< wi r >)  = Pc(wil  ) ..r(< >)In order to avoid the generation of confusion table entriesfrom a single shared context (which quite often is the resultof an incorrect parse), we apply a filter in generating Pc:for i ?
j, we generate a non-zero Pc(wj Iwi) only if the wiand wj appear in at least two common contexts, and thereis some common context in which both words occur at least3We wish to thank Richard Schwartz of BBN for referring us to thismethod and article.twice.
Furthermore, if the value computed by the formulafor Pc is less than some threshold ~'c, the value is takento be zero; we have used rc = 0.001 in the experimentsreported below.
(These filters are not applied for the casei = j; the diagonal elements of the confusion matrix arealways computed exactly.)
Because these filters may yeild anun-normalized confusion matrix (i.e., ~o j  Pc(wj Iwi) < 1),we renormalize the matrix so that ~w Pc(wj Iwi) = 1.5.
EVALUATION5.1.
Evaluation MetricWe have previously (at COLING-92) described two methodsfor the evaluation of semantic onstraints.
For the currentexperiments, we have used one of these methods, where theconstraints are evaluated against a set of manually classifiedsemantic triples.For this evaluation, we select a small test corpus separatefrom the training corpus.
We parse the corpus, regularizethe parses, and extract riples just as we did for the semanticacquisition phase (with the exception that we use the non-stochastic grammar in order to generate all grammaticallyvalid parses of each sentence).
We then manually classify eachtriple as semantically valid or invalid (a triple is counted asvalid if we believe that this pair of words could meaningfullyoccur in this relationship, even if this was not the intendedrelationship n this particular text).We then establish a threshold T for the weighted triples countsin our training set, and defineV+V_i+i_number of triples in test set which were classifiedas valid and which appeared in training set withcount > Tnumber of triples in test set which were classifiedas valid and which appeared in training set withcount _< Tnumber of triples in test set which were classifiedas invalid and which appeared in training set withcount > Tnumber of triples in test set which were classifiedas invalid and which appeared in training set withcount < Tand then definerecallerror ratev+v++v_i+i+ +i_256w Pc(attac&lw)hardenattackassaultdislodgetortureharassmachinegunmassacrereinforceboardabductspecializeoccupyengageblow-upblow0.2520.2510.1780.1310.1230.1140.0960.0940.0930.0910.0860.0760.0720.0680.0640.063to Pc( terroristlw)terroristallyjobworldceasefirecommandoguerrillaurban commandocoupassassinindividualjournalistoffensivehistoryrebelfighter0.3090.1370.1190.0910.0690.0580.0450.0430.0430.0410.0350.0290.0290.0260.0250.023Figure 1: Verbs closely related to the verb "attack" and nouns closely related to the noun "terrorist", ranked by Pc.
("harden"appears at the top of the list for "attack" because both appear with the object "position".
)At a given threshold T, our smoothing process hould increaserecall but in practice will also increase the error rate.
Howcan we tell if our smoothing isdoing any good?
We can viewthe smoothing process as moving some triples from v_ to v+and from i_ to i+.4 Is it doing so better than some randomprocess?
I.e., is it preferentially raising valid items above thethreshold?
To assess this, we compute (for a fixed threshold)the quality measureV~--V+i_where the values with superscript S represent the values withsmoothing, and those without superscripts represent the valueswithout smoothing.
If Q > 1, then smoothing is doing betterthan a random process in identifying valid triples.5.2.
Test DataThe training corpus was the set of 1300 messages (with atotal of 18,838 sentences) which constituted the developmentcorpus for Message Understanding Conferences - 3 and 4\[1,2\].
These messages are news reports from the ForeignBroadcast Information Service concerning terrorist activity inCentral and South America.
The average sentence length isabout 24 words.
In order to get higher-quality parses of thesesentences, we disabled several of the recovery mechanisms4In fact, some triples will move above the threshold and other will movebelow the threshold, but in the regions we are considering, the net movementwill be above the threshold.normally used in parsing, such as longest-substring parsing;with these mechanisms disabled, we obtained parses for 9,903of the 18,838 sentences.
These parses were then regularizedand reduced to triples.
We generated a total of 46,659 distincttriples from this test corpus.The test corpus--used togenerate the triples which were man-ually classified---consisted of 10 messages of similar style,taken from one of the test corpora for Message UnderstandingConference - 3.
These messages produced a test set contain-ing a total of 636 distinct riples, of which 456 were valid and180 were invalid.5.3.
ResultsIn testing our smoothing procedure, we first generated theconfusion matrix Pc and examined some of the entries.
Fig-ure 1 shows the largest entries in Pc for the verb "attack" andthe noun "terrorist", two very common words in the terroristdomain.
It is clear that (with some odd exceptions) most ofthe words with high Pc values are semantically related to theoriginal word.To evaluate the effectiveness of smoothing, we have comparedthree sets of triples frequency data:1. the original (unsmoothed) am2.
the data as smoothed using Pc3.
the data as generalized using a manually-prepared classi-fication hierarchy for a subset of the words of the domain257generalization strategy T v+ v_ i+ i_ recall error rate Q1.
no smoothing 0 139 317 13 167 30% 7%2.
confusion matrix 0 237 219 50 130 52% 28% 1.393. classification hierarchy 0 154 302 18 162 34% 10% 1.584. confusion matrix 0.29 154 302 17 163 34% 9% 1.90Table 1: A comparison of the effect of different generalization strategies.For the: third method, we employed a classification hierar-chy which had previously been prepared as part of the in-formation extraction system used for Message UnderstandingConference-4.
This hierarchy included only the subset ofthe vocabulary thought relevant to the information extractiontask (not counting proper names, roughly 10% of the wordsin the vocabulary).
From this hierarchy we identified the 13classes which were most frequently referred to in the lexico-semantic models used by the extraction system.
If the head(first element) of a semantic triple was a member of one ofthese classes, the generalization process replaced that wordby the most specific class to which it belongs (since we havea hierarchy with nested classes, a word will typically belongto several classes); to make the results comparable to thosewith confusion-matrix smoothing, we did not generalize theargument (last element) of the triple.The basic results are shown in rows 1, 2, and 3 of  Table 1.For all of these we used a threshold (T) of 0, so a triple withany frequency > 0 would go into the v+ or i+ category.
Ineach case the quality measure Q is relative to the run withoutsmoothing, entry 1 in the table.
Both the confusion matrix andthe classification hierarchy ield Qs substantially above 1, in-dicating that both methods are performing substantially betterthan random.
The Q is higher with the classification hierar-chy, as might be expected since it has been manually checked;on the other hand, the improvement in recall is substantiallysmaller, since the hierarchy covers only a small portion of thetotal vocabulary.
As the table shows, the confusion matrixmethod produces a large increase in recall (about 73% overthe base run).These comparisons all use a T (frequency threshold) of 0,which yields the highest recall and error rate.
Differentrecall/error-rate trade-offs can be obtained by varying T. Forexample, entry 4 of the table shows the result for T=0.29,the point at which the recall using the confusion matrix andthe classification hierarchy is the same (the values withoutsmoothing and the values using the classification hierarchyare essentially unchanged at T=0.29), We observe that, forthe same recall, the automatic smoothing does as well as themanually generated hierarchy with regard to error rate.
(Infact, the Q value with smoothing (line 4) is much higher thanwith the classification hierarchy (line 3), but this reflects adif-ference of only 1 in i+ and should not be seen as significant.)6.
D ISCUSSIONWe have demonstrated that automated smoothing methodscan be of some benefit in increasing the coverage of auto-matically acquired selectional constraints.
This is potentiallyimportant as a step in developing tools for porting naturallanguage systems to new domains.
It is still too early toassess the relative merits of different approaches to general-izing these selectional constraints, given our limited testingand the different evaluation metrics of the few others groupsexperimenting with such acquisition procedures.Our experimental results are not uniformly positive.
We didachieve substantially higher ecall evels with smoothing.
Onthe other hand, over the range of recalls obtainable withoutsmoothing, smoothing did not consistently improve the errorrate.
Therefore at present the principal benefit of the smooth-ing technique is to raise the recall beyond that possible usingunsmoothed data.In addition, preliminary experiments with smoothing appliedto the argument position in a triple indicate that the compari-son between automated smoothing and manual classificationhierarchies is not so favorable.
This is not too surprisingbecause when the classification hierarchy was initially cre-ated, its primary use was to specify the allowable values ofarguments and modifiers in semantic ase flames; as a re-sult, while the hierarchy is of benefit in generalizing heads(as described above), it is more effective in generalizing theargument position.We recognize that the size of the corpus we have used is quiteminimal for the task of computing similarities, since to get afully populated similarity matrix we would require each pairof semantically related words to occur in several commoncontexts.
We hope therefore to repeat hese experiments witha substantially larger corpus in the near future.
A largercorpus will also allow us to use larger patterns, including inparticular subject-verb-object patterns, and thus reduce theconfusion due to treating different words senses as commoncontexts.2587.
AcknowledgementThis material is based upon work supported by the AdvancedResearch Projects Agency through the Office of Naval Re-search under Grant No.
N00014-904-1851.References1.
Proceedings ofthe Third Message Understanding Conference(MUC-3).
Morgan Kaufmann, May 1991.2.
Proceedingsof the FourthMessage UnderstandingConference(MUC-4).
Morgan Kaufmann, June 1992.3.
Jing-Shin Chang, Yih-Fen Luo, and Keh-Yih Su.
GPSM: Ageneralized probabilistic semantic model for ambiguity reso-lution.
In Proceedings ofthe 30th Annual Meeting of the Assn.for Computational Linguistics, pages 177-184, Newark, DE,June 1992.4.
U. Essen and V. Steinbiss.
Cooccurrence smoothing forstochastic language modefing.
In ICASSP92, pages 1-161 -1-164, San Francisco, CA, May 1992.5.
Lynette Hirschman, Ralph Gfishman, and Naomi Sager.Grammatically-based automatic word class formation.
Infor-mation Processing and Management, 11(1/2):39-57, 1975.6.
Satoshi Sekine, Sofia Ananiadou, Jeremy Carroll, and Jun'ichiTsujfi.
Linguistic knowledge generator.
In Proc.
14th lnt'lConf.
Computational Linguistics (COLING 92), pages 560-566, Nantes, France, July 1992.7.
Paola Velardi, Maria Teresa Pazienza, and Michela Fasolo.How to encode semantic knowledge: A method for meaningrepresentation a d computer-aided acquisition.
ComputationalLinguistics, 17(2):153-170, 1991.259
