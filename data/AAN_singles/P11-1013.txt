Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 123?131,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsAutomatically Extracting Polarity-Bearing Topics for Cross-DomainSentiment ClassificationYulan He Chenghua Lin?
Harith AlaniKnowledge Media Institute, The Open UniversityMilton Keynes MK7 6AA, UK{y.he,h.alani}@open.ac.uk?
School of Engineering, Computing and MathematicsUniversity of Exeter, Exeter EX4 4QF, UKcl322@exeter.ac.ukAbstractJoint sentiment-topic (JST) model was previ-ously proposed to detect sentiment and topicsimultaneously from text.
The only super-vision required by JST model learning isdomain-independent polarity word priors.
Inthis paper, we modify the JST model by in-corporating word polarity priors through mod-ifying the topic-word Dirichlet priors.
Westudy the polarity-bearing topics extracted byJST and show that by augmenting the originalfeature space with polarity-bearing topics, thein-domain supervised classifiers learned fromaugmented feature representation achieve thestate-of-the-art performance of 95% on themovie review data and an average of 90% onthe multi-domain sentiment dataset.
Further-more, using feature augmentation and selec-tion according to the information gain criteriafor cross-domain sentiment classification, ourproposed approach performs either better orcomparably compared to previous approaches.Nevertheless, our approach is much simplerand does not require difficult parameter tun-ing.1 IntroductionGiven a piece of text, sentiment classification aimsto determine whether the semantic orientation of thetext is positive, negative or neutral.
Machine learn-ing approaches to this problem (?
; ?
; ?
; ?
; ?
; ?)
typ-ically assume that classification models are trainedand tested using data drawn from some fixed distri-bution.
However, in many practical cases, we mayhave plentiful labeled examples in the source do-main, but very few or no labeled examples in thetarget domain with a different distribution.
For ex-ample, we may have many labeled books reviews,but we are interested in detecting the polarity ofelectronics reviews.
Reviews for different producesmight have widely different vocabularies, thus clas-sifiers trained on one domain often fail to producesatisfactory results when shifting to another do-main.
This has motivated much research on sen-timent transfer learning which transfers knowledgefrom a source task or domain to a different but re-lated task or domain (?
; ?
; ?
; ?
).Joint sentiment-topic (JST) model (?
; ?)
was ex-tended from the latent Dirichlet alocation (LDA)model (?)
to detect sentiment and topic simultane-ously from text.
The only supervision required byJST learning is domain-independent polarity wordprior information.
With prior polarity words ex-tracted from both the MPQA subjectivity lexicon1and the appraisal lexicon2, the JST model achievesa sentiment classification accuracy of 74% on themovie review data3 and 71% on the multi-domainsentiment dataset4.
Moreover, it is also able to ex-tract coherent and informative topics grouped underdifferent sentiment.
The fact that the JST modeldoes not required any labeled documents for trainingmakes it desirable for domain adaptation in senti-ment classification.
Many existing approaches solvethe sentiment transfer problem by associating words1http://www.cs.pitt.edu/mpqa/2http://lingcog.iit.edu/arc/appraisal_lexicon_2007b.tar.gz3http://www.cs.cornell.edu/people/pabo/movie-review-data4http://www.cs.jhu.edu/?mdredze/datasets/sentiment/index2.html123from different domains which indicate the same sen-timent (?
; ?).
Such an association mapping problemcan be naturally solved by the posterior inference inthe JST model.
Indeed, the polarity-bearing topicsextracted by JST essentially capture sentiment asso-ciations among words from different domains whicheffectively overcome the data distribution differencebetween source and target domains.The previously proposed JST model uses the sen-timent prior information in the Gibbs sampling in-ference step that a sentiment label will only be sam-pled if the current word token has no prior sentimentas defined in a sentiment lexicon.
This in fact im-plies a different generative process where many ofthe word prior sentiment labels are observed.
Themodel is no longer ?latent?.
We propose an alter-native approach by incorporating word prior polar-ity information through modifying the topic-wordDirichlet priors.
This essentially creates an informedprior distribution for the sentiment labels and wouldallow the model to actually be latent and would beconsistent with the generative story.We study the polarity-bearing topics extracted bythe JST model and show that by augmenting theoriginal feature space with polarity-bearing topics,the performance of in-domain supervised classifierslearned from augmented feature representation im-proves substantially, reaching the state-of-the-art re-sults of 95% on the movie review data and an aver-age of 90% on the multi-domain sentiment dataset.Furthermore, using simple feature augmentation,our proposed approach outperforms the structuralcorrespondence learning (SCL) (?)
algorithm andachieves comparable results to the recently proposedspectral feature alignment (SFA) method (?).
Never-theless, our approach is much simpler and does notrequire difficult parameter tuning.We proceed with a review of related work onsentiment domain adaptation.
We then briefly de-scribe the JST model and present another approachto incorporate word prior polarity information intoJST learning.
We subsequently show that wordsfrom different domains can indeed be grouped un-der the same polarity-bearing topic through an illus-tration of example topic words extracted by JST be-fore proposing a domain adaptation approach basedon JST.
We verify our proposed approach by con-ducting experiments on both the movie review dataand the multi-domain sentiment dataset.
Finally, weconclude our work and outline future directions.2 Related WorkThere has been significant amount of work on algo-rithms for domain adaptation in NLP.
Earlier worktreats the source domain data as ?prior knowledge?and uses maximum a posterior (MAP) estimation tolearn a model for the target domain data under thisprior distribution (?).
Chelba and Acero (?)
alsouses the source domain data to estimate prior dis-tribution but in the context of a maximum entropy(ME) model.
The ME model has later been studiedin (?)
for domain adaptation where a mixture modelis defined to learn differences between domains.Other approaches rely on unlabeled data in thetarget domain to overcome feature distribution dif-ferences between domains.
Motivated by the alter-nating structural optimization (ASO) algorithm (?
)for multi-task learning, Blitzer et al (?)
proposedstructural correspondence learning (SCL) for do-main adaptation in sentiment classification.
Givenlabeled data from a source domain and unlabeleddata from target domain, SCL selects a set of pivotfeatures to link the source and target domains wherepivots are selected based on their common frequencyin both domains and also their mutual informationwith the source labels.There has also been research in exploring care-ful structuring of features for domain adaptation.Daume?
(?)
proposed a kernel-mapping functionwhich maps both source and target domains data toa high-dimensional feature space so that data pointsfrom the same domain are twice as similar as thosefrom different domains.
Dai et al(?)
proposed trans-lated learning which uses a language model to linkthe class labels to the features in the source spaces,which in turn is translated to the features in thetarget spaces.
Dai et al (?)
further proposed us-ing spectral learning theory to learn an eigen fea-ture representation from a task graph representingfeatures, instances and class labels.
In a similarvein, Pan et al (?)
proposed the spectral featurealignment (SFA) algorithm where some domain-independent words are used as a bridge to con-struct a bipartite graph to model the co-occurrencerelationship between domain-specific words anddomain-independent words.
Feature clusters are124generated by co-align domain-specific and domain-independent words.Graph-based approach has also been studied in(?)
where a graph is built with nodes denotingdocuments and edges denoting content similaritybetween documents.
The sentiment score of eachunlabeled documents is recursively calculated untilconvergence from its neighbors the actual labels ofsource domain documents and pseudo-labels of tar-get document documents.
This approach was laterextended by simultaneously considering relationsbetween documents and words from both source andtarget domains (?
).More recently, Seah et al (?)
addressed the issuewhen the predictive distribution of class label giveninput data of the domains differs and proposed Pre-dictive Distribution Matching SVM learn a robustclassifier in the target domain by leveraging the la-beled data from only the relevant regions of multiplesources.3 Joint Sentiment-Topic (JST) ModelAssume that we have a corpus with a collection ofDdocuments denoted by C = {d1, d2, ..., dD}; eachdocument in the corpus is a sequence of Nd wordsdenoted by d = (w1, w2, ..., wNd), and each wordin the document is an item from a vocabulary indexwith V distinct terms denoted by {1, 2, ..., V }.
Also,let S be the number of distinct sentiment labels, andT be the total number of topics.
The generativeprocess in JST which corresponds to the graphicalmodel shown in Figure ??
(a) is as follows:?
For each document d, choose a distributionpid ?
Dir(?).?
For each sentiment label l under document d,choose a distribution ?d,l ?
Dir(?).?
For each word wi in document d?
choose a sentiment label li ?
Mult(pid),?
choose a topic zi ?
Mult(?d,li),?
choose a word wi from ?lizi , a Multino-mial distribution over words conditionedon topic zi and sentiment label li.Gibbs sampling was used to estimate the posteriordistribution by sequentially sampling each variableof interest, zt and lt here, from the distribution overw!z"NdS*T# $DlS(a) JST model.w!z"NdS*T# $DlSS!S(b) Modified JST model.Figure 1: JST model and its modified version.that variable given the current values of all othervariables and data.
Letting the superscript ?t de-note a quantity that excludes data from tth position,the conditional posterior for zt and lt by marginaliz-ing out the random variables ?, ?, and pi isP (zt = j, lt = k|w, z?t, l?t, ?, ?, ?)
?N?twt,j,k + ?N?tj,k + V ?
?N?tj,k,d + ?j,kN?tk,d +?j ?j,k?N?tk,d + ?N?td + S?.
(1)where Nwt,j,k is the number of times word wt ap-peared in topic j and with sentiment label k, Nj,kis the number of times words assigned to topic jand sentiment label k, Nj,k,d is the number of timesa word from document d has been associated withtopic j and sentiment label k, Nk,d is the number oftimes sentiment label k has been assigned to someword tokens in document d, andNd is the total num-ber of words in the document collection.In the modified JST model as shown in Fig-ure ??
(b), we add an additional dependency link of?
on the matrix ?
of size S?V which we use to en-code word prior sentiment information into the JSTmodel.
For each word w ?
{1, ..., V }, if w is foundin the sentiment lexicon, for each l ?
{1, ..., S}, theelement ?lw is updated as follows?lw ={1 if S(w) = l0 otherwise, (2)where the function S(w) returns the prior sentimentlabel of w in a sentiment lexicon, i.e.
neutral, posi-125Book DVD Book Elec.
Book Kitch.
DVD Elec.
DVD Kitch.
Elec.
Kitch.Pos.recommend funni interest pictur interest qualiti concert sound movi recommend sound pleashighli cool topic clear success easili rock listen stori highli excel lookeasi entertain knowledg paper polit servic favorit bass classic perfect satisfi worthdepth awesom follow color clearli stainless sing amaz fun great perform materistrong worth easi accur popular safe talent acoust charact qulati comfort professionNeg.mysteri cop abus problem bore return bore poorli horror cabinet tomtom eliminfbi shock question poor tediou heavi plot low alien break region regardlessinvestig prison mislead design cheat stick stupid replac scari install error cheaplideath escap point case crazi defect stori avoid evil drop code plainreport dirti disagre flaw hell mess terribl crap dead gap dumb incorrectTable 1: Extracted polarity words by JST on the combined data sets.tive or negative.The matrix ?
can be considered as a transforma-tion matrix which modifies the Dirichlet priors ?
ofsize S ?
T ?
V , so that the word prior polarity canbe captured.
For example, the word ?excellent?
withindex i in the vocabulary has a positive polarity.
Thecorresponding row vector in ?
is [0, 1, 0] with its el-ements representing neutral, positive, and negative.For each topic j, multiplying ?li with ?lji, only thevalue of ?lposji is retained, and ?lneuji and ?lnegjiare set to 0.
Thus, the word ?excellent?
can onlybe drawn from the positive topic word distributionsgenerated from a Dirichlet distribution with param-eter ?lpos .4 Polarity Words Extracted by JSTThe JST model allows clustering different termswhich share similar sentiment.
In this section, westudy the polarity-bearing topics extracted by JST.We combined reviews from the source and targetdomains and discarded document labels in both do-mains.
There are a total of six different combi-nations.
We then run JST on the combined datasets and listed some of the topic words extracted asshown in Table ??.
Words in each cell are groupedunder one topic and the upper half of the table showstopic words under the positive sentiment label whilethe lower half shows topic words under the negativesentiment label.We can see that JST appears to better capture sen-timent association distribution in the source and tar-get domains.
For example, in the DVD+Elec.
set,words from the DVD domain describe a rock con-cert DVD while words from the Electronics domainare likely relevant to stereo amplifiers and receivers,and yet they are grouped under the same topic by theJST model.
Checking the word coverage in each do-main reveals that for example ?bass?
seldom appearsin the DVD domain, but appears more often in theElectronics domain.
Likewise, in the Book+Kitch.set, ?stainless?
rarely appears in the Book domainand ?interest?
does not occur often in the Kitchendomain and they are grouped under the same topic.These observations motivate us to explore polarity-bearing topics extracted by JST for cross-domainsentiment classification since grouping words fromdifferent domains but bearing similar sentiment hasthe effect of overcoming the data distribution differ-ence of two domains.5 Domain Adaptation using JSTGiven input data x and a class label y, labeled pat-terns of one domain can be drawn from the jointdistribution P (x, y) = P (y|x)P (x).
Domain adap-tation usually assume that data distribution are dif-ferent in source and target domains, i.e., Ps(x) 6=Pt(x).
The task of domain adaptation is to predictthe label yti corresponding to xti in the target domain.We assume that we are given two sets of trainingdata, Ds and Dt, the source domain and target do-main data sets, respectively.
In the multiclass clas-sification problem, the source domain data consistof labeled instances, Ds = {(xsn; ysn) ?
X ?
Y :1 ?
n ?
N s}, where X is the input space and Yis a finite set of class labels.
No class label is givenin the target domain, Dt = {xtn ?
X : 1 ?
n ?N t, N t  N s}.
Algorithm ??
shows how to per-form domain adaptation using the JST model.
Thesource and target domain data are first merged withdocument labels discarded.
A JST model is then126learned from the merged corpus to generate polarity-bearing topics for each document.
The original doc-uments in the source domain are augmented withthose polarity-bearing topics as shown in Step 4 ofAlgorithm ?
?, where li zi denotes a combination ofsentiment label li and topic zi for word wi.
Finally,feature selection is performed according to the infor-mation gain criteria and a classifier is then trainedfrom the source domain using the new documentrepresentations.
The target domain documents arealso encoded in a similar way with polarity-bearingtopics added into their feature representations.Algorithm 1 Domain adaptation using JST.Input: The source domain data Ds = {(xsn; ysn) ?
X ?Y : 1 ?
n ?
Ns}, the target domain data, Dt ={xtn ?
X : 1 ?
n ?
Nt, N t  Ns}Output: A sentiment classifier for the target domain Dt1: Merge Ds and Dt with document labels discarded,D = {(xsn, 1 ?
n ?
Ns;xtn, 1 ?
n ?
Nt}2: Train a JST model on D3: for each document xsn = (w1, w2, ..., wm) ?
Ds do4: Augment document with polarity-bearing topicsgenerated from JST,xs?n = (w1, w2, ..., wm, l1 z1, l2 z2, ..., lm zm)5: Add {xs?n ; ysn} into a document pool B6: end for7: Perform feature selection using IG on B8: Return a classifier, trained on BAs discussed in Section ??
that the JST model di-rectly models P (l|d), the probability of sentimentlabel given document, and hence document polar-ity can be classified accordingly.
Since JST modellearning does not require the availability of docu-ment labels, it is possible to augment the source do-main data by adding most confident pseudo-labeleddocuments from the target domain by the JST modelas shown in Algorithm ?
?.6 ExperimentsWe evaluate our proposed approach on the twodatasets, the movie review (MR) data and the multi-domain sentiment (MDS) dataset.
The movie re-view data consist of 1000 positive and 1000 neg-ative movie reviews drawn from the IMDB moviearchive while the multi-domain sentiment datasetcontains four different types of product reviews ex-tracted from Amazon.com including Book, DVD,Electronics and Kitchen appliances.
Each categoryAlgorithm 2 Adding pseudo-labeled documents.Input: The target domain data, Dt = {xtn ?
X :1 ?
n ?
N t, N t  N s}, document sentimentclassification threshold ?Output: A labeled document pool B1: Train a JST model parameterized by ?
on Dt2: for each document xtn ?
Dt do3: Infer its sentiment class label from JST asln = arg maxs P (l|xtn; ?
)4: if P (ln|xtn; ?)
> ?
then5: Add labeled sample (xtn, ln) into a docu-ment pool B6: end if7: end forof product reviews comprises of 1000 positive and1000 negative reviews and is considered as a do-main.
Preprocessing was performed on both of thedatasets by removing punctuation, numbers, non-alphabet characters and stopwords.
The MPQA sub-jectivity lexicon is used as a sentiment lexicon in ourexperiments.6.1 Experimental SetupWhile the original JST model can produce reason-able results with a simple symmetric Dirichlet prior,here we use asymmetric prior ?
over the topic pro-portions which is learned directly from data using afixed-point iteration method (?
).In our experiment, ?
was updated every 25 itera-tions during the Gibbs sampling procedure.
In termsof other priors, we set symmetric prior ?
= 0.01 and?
= (0.05?L)/S, where L is the average documentlength, and the value of 0.05 on average allocates 5%of probability mass for mixing.6.2 Supervised Sentiment ClassificationWe performed 5-fold cross validation for the per-formance evaluation of supervised sentiment clas-sification.
Results reported in this section are av-eraged over 10 such runs.
We have tested severalclassifiers including Na?
?ve Bayes (NB) and supportvector machines (SVMs) from WEKA5, and maxi-mum entropy (ME) from MALLET6.
All parametersare set to their default values except the Gaussian5http://www.cs.waikato.ac.nz/ml/weka/6http://mallet.cs.umass.edu/127prior variance is set to 0.1 for the ME model train-ing.
The results show that ME consistently outper-forms NB and SVM on average.
Thus, we only re-port results from ME trained on document vectorswith each term weighted according to its frequency.859095100ccuracy (%)Movie Review Book DVD Electronics Kitchen75801 5 10 15 30 50 100 150 200Accuracy (%)No.
of TopicsFigure 2: Classification accuracy vs. no.
of topics.The only parameter we need to set is the numberof topics T .
It has to be noted that the actual num-ber of feature clusters is 3 ?
T .
For example, whenT is set to 5, there are 5 topic groups under eachof the positive, negative, or neutral sentiment labelsand hence there are altogether 15 feature clusters.The generated topics for each document from theJST model were simply added into its bag-of-words(BOW) feature representation prior to model train-ing.
Figure ??
shows the classification results on thefive different domains by varying the number of top-ics from 1 to 200.
It can be observed that the bestclassification accuracy is obtained when the numberof topics is set to 1 (or 3 feature clusters).
Increas-ing the number of topics results in the decrease ofaccuracy though it stabilizes after 15 topics.
Never-theless, when the number of topics is set to 15, us-ing JST feature augmentation still outperforms MEwithout feature augmentation (the baseline model)in all of the domains.
It is worth pointing out thatthe JST model with single topic becomes the stan-dard LDA model with only three sentiment topics.Nevertheless, we have proposed an effective way toincorporate domain-independent word polarity priorinformation into model learning.
As will be shownlater in Table ??
that the JST model with word po-larity priors incorporated performs significantly bet-ter than the LDA model without incorporating suchprior information.For comparison purpose, we also run the LDAmodel and augmented the BOW features with theMethod MRMDSBook DVD Elec.
Kitch.Baseline 82.53 79.96 81.32 83.61 85.82LDA 83.76 84.32 85.62 85.4 87.68JST 94.98 89.95 91.7 88.25 89.85[YE10] 91.78 82.75 82.85 84.55 87.9[LI10] - 79.49 81.65 83.64 85.65Table 2: Supervised sentiment classification accuracy.generated topics in a similar way.
The best accu-racy was obtained when the number of topics is setto 15 in the LDA model.
Table ??
shows the clas-sification accuracy results with or without featureaugmentation.
We have performed significance testand found that LDA performs statistically signifi-cant better than Baseline according to a paired t-testwith p < 0.005 for the Kitchen domain and withp < 0.001 for all the other domains.
JST performsstatistically significant better than both Baseline andLDA with p < 0.001.We also compare our method with other recentlyproposed approaches.
Yessenalina et al (?)
ex-plored different methods to automatically generateannotator rationales to improve sentiment classifica-tion accuracy.
Our method using JST feature aug-mentation consistently performs better than their ap-proach (denoted as [YE10] in Table ??).
They fur-ther proposed a two-level structured model (?)
fordocument-level sentiment classification.
The bestaccuracy obtained on the MR data is 93.22% withthe model being initialized with sentence-level hu-man annotations, which is still worse than ours.
Liet al (?)
adopted a two-stage process by first clas-sifying sentences as personal views and impersonalviews and then using an ensemble method to per-form sentiment classification.
Their method (de-noted as [LI10] in Table ??)
performs worse than ei-ther LDA or JST feature augmentation.
To the bestof our knowledge, the results achieved using JSTfeature augmentation are the state-of-the-art for boththe MR and the MDS datasets.6.3 Domain AdaptationWe conducted domain adaptation experiments onthe MDS dataset comprising of four different do-mains, Book (B), DVD (D), Electronics (E), andKitchen appliances (K).
We randomly split each do-128main data into a training set of 1,600 instances and atest set of 400 instances.
A classifier trained on thetraining set of one domain is tested on the test set ofa different domain.
We preformed 5 random splitsand report the results averaged over 5 such runs.Comparison with Baseline ModelsWe compare our proposed approaches with twobaseline models.
The first one (denoted as ?Base?
inTable ??)
is an ME classifier trained without adapta-tion.
LDA results were generated from an ME clas-sifier trained on document vectors augmented withtopics generated from the LDA model.
The numberof topics was set to 15.
JST results were obtainedin a similar way except that we used the polarity-bearing topics generated from the JST model.
Wealso tested with adding pseudo-labeled examplesfrom the JST model into the source domain for MEclassifier training (following Algorithm ??
), denotedas ?JST-PL?
in Table ??.
The document sentimentclassification probability threshold ?
was set to 0.8.Finally, we performed feature selection by selectingthe top 2000 features according to the informationgain criteria (?JST-IG?
)7.There are altogether 12 cross-domain sentimentclassification tasks.
We showed the adaptation lossresults in Table ??
where the result for each domainand for each method is averaged over all three pos-sible adaptation tasks by varying the source domain.The adaptation loss is calculated with respect to thein-domain gold standard classification result.
Forexample, the in-domain goal standard for the Bookdomain is 79.96%.
For adapting from DVD to Book,baseline achieves 72.25% and JST gives 76.45%.The adaptation loss is 7.71 for baseline and 3.51 forJST.It can be observed from Table ??
that LDA onlyimproves slightly compared to the baseline with anerror reduction of 11%.
JST further reduces the er-ror due to transfer by 27%.
Adding pseudo-labeledexamples gives a slightly better performance com-pared to JST with an error reduction of 36%.
Withfeature selection, JST-IG outperforms all the otherapproaches with a relative error reduction of 53%.7Both values of 0.8 and 2000 were set arbitrarily after an ini-tial run on some held-out data; they were not tuned to optimizetest performance.Domain Base LDA JST JST-PL JST-IGBook 10.8 9.4 7.2 6.3 5.2DVD 8.3 6.1 4.8 4.4 2.9Electr.
7.9 7.7 6.3 5.4 3.9Kitch.
7.6 7.6 6.9 6.1 4.4Average 8.6 7.7 6.3 5.5 4.1Table 3: Adaptation loss with respect to the in-domaingold standard.
The last row shows the average loss overall the four domains.Parameter SensitivityThere is only one parameters to be set in the JST-IG approach, the number of topics.
We plot the clas-sification accuracy versus different topic numbers inFigure ??
with the number of topics varying between1 and 200, corresponding to feature clusters varyingbetween 3 and 600.
It can be observed that for therelatively larger Book and DVD data sets, the accu-racies peaked at topic number 10, whereas for therelatively smaller Electronics and Kitchen data sets,the best performance was obtained at topic number50.
Increasing topic numbers results in the decreaseof classification accuracy.
Manually examining theextracted polarity topics from JST reveals that whenthe topic number is small, each topic cluster containswell-mixed words from different domains.
How-ever, when the topic number is large, words undereach topic cluster tend to be dominated by a singledomain.Comparison with Existing ApproachesWe compare in Figure ??
our proposed approachwith two other domain adaptation algorithms forsentiment classification, SCL and SFA.
Each set ofbars represent a cross-domain sentiment classifica-tion task.
The thick horizontal lines are in-domainsentiment classification accuracies.
It is worth not-ing that our in-domain results are slightly differentfrom those reported in (?
; ?)
due to different ran-dom splits.
Our proposed JST-IG approach outper-forms SCL in average and achieves comparable re-sults to SFA.
While SCL requires the construction ofa reasonable number of auxiliary tasks that are use-ful to model ?pivots?
and ?non-pivots?, SFA relieson a good selection of domain-independent featuresfor the construction of bipartite feature graph beforerunning spectral clustering to derive feature clusters.12970758085uracy (%)D >B E >B K >B B >D E >D K >D60651 5 10 15 30 50 100 150 200Accuracy (%)No.
of topics(a) Adapted to Book and DVD data sets.8085uracy (%)B >E D >E K >E B >K D >K E >K70751 5 10 15 30 50 100 150 200Accuracy (%)No.
of topics(b) Adapted to Electronics and Kitchen data sets.Figure 3: Classification accuracy vs. no.
of topics.On the contrary, our proposed approach based onthe JST model is much simpler and yet still achievescomparable results.7 ConclusionsIn this paper, we have studied polarity-bearing top-ics generated from the JST model and shown that byaugmenting the original feature space with polarity-bearing topics, the in-domain supervised classi-fiers learned from augmented feature representationachieve the state-of-the-art performance on both themovie review data and the multi-domain sentimentdataset.
Furthermore, using feature augmentationand selection according to the information gain cri-teria for cross-domain sentiment classification, ourproposed approach outperforms SCL and gives sim-ilar results as SFA.
Nevertheless, our approach ismuch simpler and does not require difficult parame-ter tuning.There are several directions we would like to ex-plore in the future.
First, polarity-bearing topicsgenerated by the JST model were simply added intothe original feature space of documents, it is worthinvestigating attaching different weight to each topic79.96 81.32758085uracy (%)baseline SCL MI SFA JST IG6570D >B E >B K >B B >D E >D K >DAccuracy (%)(a) Adapted to Book and DVD data sets.83.61 85.82808590uracy (%)baseline SCL MI SFA JST IG657075B >E D >E K >E B >K D >K E >KAccuracy (%)(b) Adapted to Electronics and Kitchen data sets.Figure 4: Comparison with existing approaches.maybe in proportional to the posterior probability ofsentiment label and topic given a word estimated bythe JST model.
Second, it might be interesting tostudy the effect of introducing a tradeoff parameterto balance the effect of original and new features.Finally, our experimental results show that addingpseudo-labeled examples by the JST model does notappear to be effective.
We could possibly explore in-stance weight strategies (?)
on both pseudo-labeledexamples and source domain training examples inorder to improve the adaptation performance.AcknowledgementsThis work was supported in part by the EC-FP7projects ROBUST (grant number 257859).ReferencesR.K.
Ando and T. Zhang.
2005.
A framework for learn-ing predictive structures from multiple tasks and un-labeled data.
The Journal of Machine Learning Re-search, 6:1817?1853.A.
Aue and M. Gamon.
2005.
Customizing sentimentclassifiers to new domains: a case study.
In Proceed-ings of Recent Advances in Natural Language Process-ing (RANLP).David M. Blei, Andrew Y. Ng, and Michael I. Jordan.1302003.
Latent Dirichlet alocation.
J. Mach.
Learn.Res., 3:993?1022.J.
Blitzer, M. Dredze, and F. Pereira.
2007.
Biographies,bollywood, boom-boxes and blenders: Domain adap-tation for sentiment classification.
In ACL, page 440?447.C.
Chelba and A. Acero.
2004.
Adaptation of maxi-mum entropy classifier: Little data can help a lot.
InEMNLP.W.
Dai, Y. Chen, G.R.
Xue, Q. Yang, and Y. Yu.
2008.Translated learning: Transfer learning across differentfeature spaces.
In NIPS, pages 353?360.W.
Dai, O. Jin, G.R.
Xue, Q. Yang, and Y. Yu.
2009.Eigentransfer: a unified framework for transfer learn-ing.
In ICML, pages 193?200.H.
Daume?
III and D. Marcu.
2006.
Domain adaptationfor statistical classifiers.
Journal of Artificial Intelli-gence Research, 26(1):101?126.H.
Daume?.
2007.
Frustratingly easy domain adaptation.In ACL, pages 256?263.J.
Jiang and C.X.
Zhai.
2007.
Instance weighting fordomain adaptation in NLP.
In ACL, pages 264?271.A.
Kennedy and D. Inkpen.
2006.
Sentiment clas-sification of movie reviews using contextual valenceshifters.
Computational Intelligence, 22(2):110?125.S.
Li, C.R.
Huang, G. Zhou, and S.Y.M.
Lee.
2010.Employing personal/impersonal views in supervisedand semi-supervised sentiment classification.
In ACL,pages 414?423.C.
Lin and Y.
He.
2009.
Joint sentiment/topic model forsentiment analysis.
In Proceedings of the 18th ACMinternational conference on Information and knowl-edge management (CIKM), pages 375?384.C.
Lin, Y.
He, and R. Everson.
2010.
A Compara-tive Study of Bayesian Models for Unsupervised Sen-timent Detection.
In Proceedings of the 14th Confer-ence on Computational Natural Language Learning(CoNLL), pages 144?152.Ryan McDonald, Kerry Hannan, Tyler Neylon, MikeWells, and Jeff Reynar.
2007.
Structured models forfine-to-coarse sentiment analysis.
In ACL, pages 432?439.T.
Minka.
2003.
Estimating a Dirichlet distribution.Technical report.S.J.
Pan, X. Ni, J.T.
Sun, Q. Yang, and Z. Chen.
2010.Cross-domain sentiment classification via spectral fea-ture alignment.
In Proceedings of the 19th interna-tional conference on World Wide Web (WWW), pages751?760.Bo Pang and Lillian Lee.
2004.
A sentimental educa-tion: sentiment analysis using subjectivity summariza-tion based on minimum cuts.
In ACL, page 271?278.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
: sentiment classification using ma-chine learning techniques.
In EMNLP, pages 79?86.B.
Roark and M. Bacchiani.
2003.
Supervised and un-supervised PCFG adaptation to novel domains.
InNAACL-HLT, pages 126?133.C.W.
Seah, I. Tsang, Y.S.
Ong, and K.K.
Lee.
2010.
Pre-dictive Distribution Matching SVM for Multi-domainLearning.
In ECML-PKDD, pages 231?247.Casey Whitelaw, Navendu Garg, and Shlomo Argamon.2005.
Using appraisal groups for sentiment analysis.In Proceedings of the ACM international conferenceon Information and Knowledge Management (CIKM),pages 625?631.Q.
Wu, S. Tan, and X. Cheng.
2009.
Graph ranking forsentiment transfer.
In ACL-IJCNLP, pages 317?320.Q.
Wu, S. Tan, X. Cheng, and M. Duan.
2010.
MIEA:a Mutual Iterative Enhancement Approach for Cross-Domain Sentiment Classification.
In COLING, page1327-1335.A.
Yessenalina, Y. Choi, and C. Cardie.
2010a.
Auto-matically generating annotator rationales to improvesentiment classification.
In ACL, pages 336?341.A.
Yessenalina, Y. Yue, and C. Cardie.
2010b.
Multi-Level Structured Models for Document-Level Senti-ment Classification.
In EMNLP, pages 1046?1056.Jun Zhao, Kang Liu, and Gen Wang.
2008.
Adding re-dundant features for CRFs-based sentence sentimentclassification.
In EMNLP, pages 117?126.131
