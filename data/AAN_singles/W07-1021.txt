BioNLP 2007: Biological, translational, and clinical language processing, pages 161?162,Prague, June 2007. c?2007 Association for Computational LinguisticsExploring the Use of NLP in the Disclosure of Electronic Patient RecordsDavid HardcastleFaculty of Mathematics and ComputingThe Open Universityd.w.hardcastle@open.ac.ukCatalina HallettFaculty of Mathematics and ComputingThe Open Universityc.hallett@open.ac.ukAbstractThis paper describes a preliminary analysisof issues involved in the production of re-ports aimed at patients from Electronic Pa-tient Records.
We present a system proto-type and discuss the problems encountered.1 IntroductionAllowing patient access to Electronic PatientRecords (EPR) in a comprehensive format is a le-gal requirement in most European countries.
Apartfrom this legal aspect, research shows that the provi-sion of clear information to patients is instrumentalin improving the quality of care (Detmer and Sin-gleton, 2004).
Current work on generating expla-nations of EPRs to patients suffer from two majordrawbacks.
Firstly, existing report generation sys-tems have taken an intuitive approach to the gener-ation of explanation: there is no principled way ofselecting the information that requires further expla-nation.
Secondly, most work on medical report gen-eration systems has concentrated on explaining thestructured part of an EPR; there has been very lit-tle work on providing automatic explanations of thenarratives (such as letters between health practition-ers) which represent a considerable part of an EPR.Attempting to rewrite narratives in a patient-friendlyway is in many ways more difficult than providingsuggestions for natural language generation systemsthat take as input data records.
In narratives, ambi-guity can arise from a combination of aspects overwhich NLG systems have full control, such as syn-tax, discourse structure, sentence length, formattingand readability.This paper introduces a pilot project that attemptsto address this gap by addressing the following re-search questions:1.
Given the text-based part of a patient record,which segments require explanation before being re-leased to patients?2.
Which types of explanation are appropriate forvarious types of segment?3.
Which subparts of a segment require explanation?The prototype system correctly selects the seg-ments that require explanation, but we have yet tosolve the problem of accurately identifiying the fea-tures that contribute to the ?expertness?
of a doc-ument.
We discuss the underlying issues in moredetail in section 3 below.2 Feature identification methodTo identify a set of features that differentiate med-ical expert and lay language, we compared a cor-pus of expert text with a corpus of lay texts.
Wethen used the selected features on a corpus of nar-ratives extracted from a repository of Electronic Pa-tient Records to attempt to answer the three ques-tions posed above.
First, paragraphs that containfeatures characteristic to expert documents are high-lighted using a corpus of patient information leafletsas a background reference.
Second, we prioritise theexplanations required by decomposing the classifi-cation data.
Finally, we identify within those sec-tions the features that contribute to the classificationof the section as belonging to the expert register, andprovide suggestions for text simplification.2.1 FeaturesThe feature identification was performed on two cor-pora of about 200000 words each: (a) an expertcorpus, containing clinical case studies and med-ical manuals produced for doctors and (b) a laycorpus, containing patient testimonials and infor-mational materials for patients.
Both corpora were161sourced from a variety of online sources.
In com-paring the corpora we considered a variety of fea-tures in the following categories: medical content,syntactic structure, discourse structure, readabilityand layout.
The features that proved to be best dis-criminators were the frequency of medical terms,readability indices, average NP length and the rela-tive frequency of loan words against English equiva-lents1.
The medical content analysis is based on theMeSH terminology (Canese, 2003) and consists ofassessing: (a) the frequency of MeSH primary con-cepts and alternative descriptions, (b) the frequencyof medical terms types and occurences and (c) thefrequency of MeSH terms in various top-level cate-gories.
The readability features consist of two stan-dard readability indices (FOG and Flesch-Kincaid).Although some discourse and layout features alsoproved to have a high discriminatory power, theyare strongly dependent on the distribution mediumof the analysed materials, hence not suitable for ouranalysis of EPR narratives.2.2 Analysing EPR narrativesWe performed our analysis on a corpus of 11000narratives extracted from a large repository of Elec-tronic Patient Records, totalling almost 2 millionwords.
Each segment of each narrative was then as-sessed on the basis of the features described above,such as Fog, sentence length, MeSH primary con-cepts etc.
We then smoothed all of the scores forall segments for each feature forcing the minimumto 0.0, the maximum to 1.0 and the reference corpusscore for that feature to 0.5.
This made it possible tocompare scores with different gradients and scalesagainst a common baseline in a consistent way.3 Evaluation and discussionWe evaluated our segment identification method ona set of 10 narratives containing 27 paragraphs, ex-tracted from the same repository of EPRs .
The seg-ment identification method proved succesful, with26/27 (96.3%) segments marked correctly are re-quiring/not requiring explanation.
However, thisonly addresses the first of the three questions setout above, leaving the following research questions1An in-depth analysis of unfamiliar terms in medical docu-ments can be found in (Elhadad, 2006)open to further analysis.Quantitative vs qualitative analysisMany of the measures that discriminate expert fromlay texts are based on indicative features; for exam-ple complex words are indicative of text that is dif-ficult to read.
However, there is no guarantee thatindividual words or phrases that are indicative arealso representative - in other words a given complexword or long sentence will contribute to the readabil-ity score of the segment, but may not itself be prob-lematic.
Similarly, frequency based measures, suchas a count of medical terminology, discriminate at asegment level but do not entail that each occurrencerequires attention.TerminologyWe used the MeSH terminology to analyse med-ical terms in patient records, however (as with prac-tically all medical terminologies) it contains manynon-expert medical terms.
We are currently investi-gating the possibility of mining a list of expert termsfrom MeSH or of making use of medical-lay alignedontologies.ClassificationNarratives in the EPR are written in a completely dif-ferent style from both our training expert corpus andthe reference patient information leaflets corpus.
Itis therefore very difficult to use the reference corpusas a threshold for feature values which can producegood results on the corpus of narratives, suggest-ing that a statistical thresholding technique might bemore effective.Feature dependenciesMost document features are not independent.
There-fore, the rewriting suggestions the system providesmay themselves have an unwanted impact on therewritten text, leading to a circular process for theend-user.ReferencesKathi Canese.
2003.
New Entrez Database: MeSH.NLM Technical Bulletin, March-April.D.
Detmer and P. Singleton.
2004.
The informed pa-tient.
Technical Report TIP-2, Judge Institute of Man-agement, University of Cambridge, Cambridge.Noemi Elhadad.
2006.
Comprehending technical texts:Predicting and defining unfamiliar terms.
In Proceed-ing of AMIA?06, pages 239?243.162
