Speaker-Independent Phone Recognition Using BREFJean-Luc Gauvain and Lori F. LamelLIMSI -CNRS,  BP 13391403 Orsay cedex, FRANCEABSTRACTA series of experiments on speaker-independent phone recogni-tion of continuous peech ave been carried out using the recentlyrecorded BREF corpus.
These experiments are the first to use thislarge corpus, and are meant to provide abaseline performance eval-uation for vocabulary-independent phone recognition of French.The HMM-based recognizer was trained with hand-verified atafrom 43 speakers.
Using 35 context-independent phone models,a baseline phone accuracy of 60% (no phone grammar) was ob-tained on an independent test set of 7635 phone segments from 19new speakers.
Including phone bigram probabilities as phonotacticconstraints resulted in a performance of 63.5%.
A phone accuracyof 68.6% was obtained with 428 context dependent models andthe bigram phone language model.
Vocabulary-independent wordrecognition results with no grammar are also reported for the sametest data.INTRODUCTIONThis paper eports on a series of experiments for speaker-independent, continuous speech phone recognition of French,using the recently recorded BREF corpus\[4, 6\].
BREFwas designed to provide speech data for the developmentof dictation machines, the evaluation of continuous peechrecognition systems (both speaker-dependent a d speaker-independent), and to provide a large corpus of continuousspeech to study phonological variations.
These experimentsare the first to use this corpus, and are meant o provide abaseline performance evaluation for vocabulary-independent(VI) phone recognition, as well as the development of a pro-cedure for automatic segmentation a d labeling of the corpus.First a brief description of BREF is given, along with theprocedure for semi-automatic (verified) labeling and auto-matic segmentation f the speech data.
The ability to accu-rately predict he phone labels from the text is assessed, as isthe accuracy of the automatic segmentation.
Next the phonerecognition experiments performed using speech data from62 speakers (43 for training, 19 for test) are described.
A hid-den Markov model (HMM) based recognizer has beeen eval-uated with context-independent (CI) and context-dependent(CD) model sets, both with and without a duration model.Results are also given with and without the use of 1-gramand 2-gram statistics to provide phonotactic constraints.
Pre-liminary VI word recognition results are presented with nogrammar.
The final section provides a discussion and sum-mary, and a comparison of these results to the performanceof other phone recognizers.THE BREF  CORPUSBREF is a large read-speech corpus, containing over 100hours of speech material, from 120 speakers.
The text ma-terials were selected verbatim from the French newspaperLe Monde, so as to provide a large vocabulary (over 20,000words) and a wide range of phonetic environments\[4\].
Con-taining 11 i5 distinct diphones and over 17,500 triphones,BREF can be used to train VI phonetic models.
Hon andLee\[5\] concluded that for VI recognition, the coverage oftriphones i  crucial.
Separate text materials, with similar dis-tributionalproperties were selected for training, developmenttest, and evaluation purposes.
The selected texts consist of 18"all phoneme" sentences, and approximately 840 paragraphs,3300 short sentences (12.4 words/sentence), and 3800 longersentences (21 words/sentence).
The distributional propertiesfor the 3 sets of texts, and the combined total, are shown inTable 1.
The sets are distributionally comparable in terms oftheir coverage of word and subword units and quite similar intheir phone and diphone distributions.
For comparison, thelast column of the table gives the distributional properties forthe original text of Le Monde.Each of 80 speakers read approximately 10,000 words(about 650 sentences) of text, and an additional 40 speakerseach read about half that amount.
The speakers, chosen froma subject pool of over 250 persons in the Paris area, werepaid for their participation.
Potential subjects were givena short reading test, containing selected sentences from LeMonde representative of the type of material to be recorded\[6\]and subjects judged to be incapable of the task were notrecorded.
The recordings were made in stereo in a sound-isolated room, and were monitored to assure the contents.Thus far, 80 training, 20 test, and 20 evaluation speakers havebeen recorded.
The number of male and female speakers foreach subcorpus i given in Table 2.
The ages of the speakersrange from 18 to 73 years, with 75% between the ages of20 and 40 years.
In these experiments only a subset of thetraining and development test data was used, reserving theevaluation data for future use.3,44Unit \[ Train Development Evaluation Total I Le Monde#sentences 3,877 3,624 3,501 11,002 167,359#words (total) 55,760 50,946 49,040 115,746 4,244,810#distinct words 14,089 12,803 12,280 20,055 92,185#phonemic words 11,215 10,177 9,757 15,460 63,981#syllables 3,339 3,040 2,976 3,977 9,571#dissyllables 11,297 10,472 10,072 14,066 37,636#phones (total) 252,369 230,102 222,250 726,988 16,416,738#distinct phones 35 35 35 35 35#diphones 1,107 1,092 1,082 1,115 1,160#triphones 15,704 14,769 14,399 17,552 25,999Table 1: Distributional properties of selected text subsets: training, development test, and evaluation, and of the original text.Number of Speakers ICorpus Male I Female I Total Itraining 37 43 80development 9 11 20evaluation 9 11 20total 55 65 I 120Table 2: Speakers in each corpus etLabeling of BREFIn order to be used effectively for phonetic recognition,time-aligned phonetic transcriptions of the utterances inBREF are needed.
Since hand-transcription of such a largeamount of data is a formidable task, and inherently subjec-tive, an automated procedure for labeling and segmentationis being investigated.The procedure for providing atime-aiigned broad phonetictranscription for an utterance has two steps.
First, a text-to-phoneme module\[10\] generates the phone sequence fromthe text prompt.
The 35 phones (including silence) usedby the text-to-phoneme system are given in Table 3.
Sincethe automatic phone sequence generation can not alwaysaccurately predict what the speaker said, the transcriptionsmust be verified.
The most common errors in translationoccur with foreign words and names, and acronyms.
Othermispredictions arise in the reading of dates: for examplethe year "1972" may be spoken as "mille neuf cent soixantedouze" or as "dix neuf cent soixante douze."
In the secondstep, the phone sequence is aligned with the speech signalusing Viterbi segmentation.The training and test sentences used in these experimentshave been processed automatically and manually verifiedprior to segmentation.
'nae manual verification only cor-rected "blatant errors" and did not attempt o make fine-phonetic distinctions.
Comparing the predicted and verifiedphone strings, 97.5% of the 38,397 phone labels 1 were as-sessed to be correct, with an accuracy of 96.6%.
However,during verification about 67% of the automatically generatedphone strings were modified.
This indicates that verification1 Silence segmems were disregarded.eho  I Example I Phone \] ExampleVowels Consonantsi li_t s s_ote bl_6 z _z~breE s_el S c__haty su_c Z jourX leur f foux petit v _vin@ feu m motea p atte, p.~te n n_otec so_l N dign__eo seu_le 1 !au fe__q r rendNasal Vowels p p_ontI bri~, bru_..n b benA chan_._t t tonO bo.__n d donSemivowels k c ouh l_m g g_ondw o_ui English phonesj _yole G thin_gsilence D ~eT Smi~H h_otTable 3: The 35 phone symbol set.is a necessary step for accurate labeling.
The exception dic-tionary used by the text-to-phoneme system has been updatedaccordingly to correct some of the prediction errors, therebyreducing the work entailed in verification.Table 4 summarizes the phone prediction accuracy of thetext-to-phone translation.
86% of the errors are due to inser-tions and deletions by the text-to-phone system.
Liaison andthe pronunciation ofmute-e account for about 70% of these.Liaison is almost always optional and thus bard to accuratelypredict.
While most speakers are likely to pronounce mute-ebefore apause, it is not always poken.
Whether or not mute-e is pronounced depends on the context in which it occursand upon the dialect of the speaker.
Substitutions account for345Prediction PercentCorrectSubstitutionsDeletionsInsertionsAccuracy97.50.50.92.095.5Table 4: Phone prediction errors.only 14% of the errors, with the most common substitutionsbetween/z/and/s/, and between/e/and/E/.A problem that was unanticipated was that some of theFrench speakers actually pronounced the English wordspresent in the text prompt using the correct Englishphonemes, phonemes that do not exist in French.
Thesesegments were transcribed using the "English phones" listedin Table 3, which were added to the 35 phone set.
However,so few occurrences of these phones were observed that fortraining they were mapped to the "closest" French phone.In addition, afew cases were found where what the speakersaid did not agree with the prompt ext, and the orthographictext needed to be modified.
These variations were typi-cally the insertion or deletion of a single word, and usuallyoccurred when the text was almost, but not quite, a verycommon expression.Validation of automatic segmentationA subset of the training data (roughly 12 minutes of speech,from 20 of the training speakers) was manually segmentedto bootstrap the training and segmentation procedures.
Inorder to evaluate the Viterbi segmentation, the phone recog-nition accuracy using the manual segmentation for trainingwas compared to the recognition accuracy obtained usingViterbi resegmentation (3 iterations) on the same subset oftraining data.
For this comparison 35 context-independentphone models with 8 mixture components and no durationmodel, were used.
The recognizer was tested on data from11 speakers in the development test speaker set, and theaveraged results are given in Table 5.
The performance isestimated by the phone accuracy given by: 1 - (subs + del +ins) I correct number of phones.
The recognition accuraciesare seen to be comparable, indicating that, at least for thepurposes of speech recognition, the Viterbi algorithm can beused to segment the BREF corpus once the segment labelshave been verified.
Including a duration model increases thephone accuracy to 58.0% with the Viterbi segmentation.\[ Condition \[ Correct\[Subs.
Bet.
\[Ins.
Accuracymanual \[ 60.4 \[ 27.3 12.3 I 3.8 56.7Viterbi 61.8 27.7 10.5 5.0 56.8Table 5: Training based on manual vs. Viterbi resegmentationThe segmentations determined by the Viterbi algorithmhave been compared to the manual segmentations ona newindependent set of test data.
To do so the offset in numberof frames was counted, using the manual segmentation asthe reference.
Silence segments were ignored.
The test dataconsisted of 115 sentences from 10 speakers (4nff6f) and con-talned 6517 segments.
71% of the segment boundaries werefound to be identical.
91% of the automatically found bound-aty locations were within I frame (96% within 2 frames) ofthe hand boundary location.
The automatic boundaries werelocated later than the hand location for 23% of the segments,and earlier for 5% of the segments.
This assyrnmetry may bedue to the minimum duration imposed by the phone models.PHONE RECOGNIT ION EXPERIMENTSPhone RecognizerThe baseline phone recognizer uses a set of 35 CI phonemodels.
Each model is a 3-state left-to-right HMM withGaussian mixture observation densities.
The 16 kHz speechwas downsampled by 2 and a 26-dimensional feature vectorwas computed every 10 ms.
The feature vector is composedof 13 cepstrum coefficients and 13 differential cepstrum co-efficients.
Duration is modeled with a gamma distributionper phone model.
As proposed by Rabiner et al\[ll\], theHMM and duration parameters are estimated separately andcombined in the recognition process for the Viterbi search.Maximum likelihood estimators were used for the HMM pa-rameters and moment estimators for the gamma distributions.DataThe training data consists of approximately 50 minutesof speech from 43 training speakers (21m/22f).
There are33,289 phone segments containing 5961 different riphones.Thirty-seven of the sentences are "all-phone" sentences inwhich the text was selected so as to contain all 35 phones\[4\].These sentences are quite long, having on the order of 190phones/sentence.
The remaining sentences are taken fromparagraph texts and have about 65 phones/sentence.
The testdata is comprised of 109 sentences spoken by 21 new speak-ers (10m/1 If).
There are a total of 7635 phone segments (70phones/sentence) and 3270 distinct riphones in the test set.Phonotactic onstraintsPhone, diphone and triphone statistics, computed on the5 million word original text, are used to provide phonotac-tic constraints.
Table 6 gives the information stored in theMarkov sources (1-gram to 3-gram) estimated from the oc-currence frequencies on the original text in bits/phone\[4\].For now only the 1-gram and 2-gram constraints have beenincorporated in the model.Unit/modelphones/l-gramdiphones/2-gramtriphones/3-gram#distinct entropyunits (b/ph)35 4.721,160 3.9225,999 3.40modelt(b/ph)0.401.211.72Table 6: N-gram statistics computed on the 5 million word text andthe information stored in Markov source models.346Condition Corr.
Subs.
Del.
Ins.
Ace.0-gram0-gram+duration1-gram1-gram+duration2-gram2-gram+durationTable 7: Phone recognition results for 35 CI models.ResultsTable 7 gives recognition results using 35 CI phone modelswith 16 mixture components.
Silence segments were notincluded in the computation of the phone accuracy.
Resultsare given for different phone language models, both withand without a duration model.
The improvement obtainedby including the duration model is relatively small, on theorder of 0.3% to 0.8 %, probably in part due to the widevariation in phone durations across contexts and speakers.Each additional order in the language model adds about 2%to the phone accuracy.
The best phone accuracy is 63.5%with the 2-gram language model and duration.Condition Corr.
Subs.
DeL Ins.
Ace.O-gramO-gram+duration1-gram1-gram+duration2-gram2-grain+durationTable 8: Phone recognition results for 428 CD models.Table 8 gives recognition results using a set of 428 CDphone models\]12\] with 16 mixture components.
The mod-eled contexts were automatically selected based on their fre-quencies in the training data.
This model set is essentiallycomposed of right-context phone models, with only one-fourth of the models being triphone models.
Less than 2%of the triphones found in the training data can be modeled infull.
In choosing to model right contexts over left contexts,a preference is given to modeling anticipatory coarticulationover perservatory coarticulation.Including the duration models improves performance a lit-tle more than was observed for the CI models.
The durationmodels are probably better estimates of the underlying distri-bution since the data has less variability due to context.
Theduration models give about a 1% improvement in accuracywhen used with a I-gram or 2-gram language model.
Thephonotactic constraints, however, have a larger effect withthe CI models, presumably because the CD models alreadyincorporate some to the phonotactic nformation.The use of CD models reduces the errors by 14% (com-paring the best CI and CD models), which is less than the27% error reduction reported by Lee and Hon\[7\].
There areseveral factors that may account for this difference.
Mostimportantly, Lee and Hon\[7\] compare 1450 right-CD mod-els to 39 CI models, whereas in this study only 428 contextswere modeled.
In addition, the baseline recognition accuracyreported by Lee and Hon is 53.3% with a bigrarn languagemodel, compared to our baseline phone accuracy of 63.5%.Confusion paire ---~ EE~ea~EE~an ---~ my ---~ i#Sub~ %Sub~64 4.258 3.831 4.227 1.827 1.827 1.8Table 9: The most common substitutions with 428 models.The most recognition errors occurred for the phones: /E/8.1%,/a/7.6%, let 7.2%, let 4.9%, It/4.3%,and Ix/4.2%,accounting for almost 40% of the substitution errors.
Ofthese phones only let and/E/have high phone error rates ofabout 40%.
Table 9 shows the most frequent substitutionsmade by the recognizer.
The two most common confusionsare reciprocal confusions between let and/E/and between/E/and/a/.
Together these account for 13% of the confusions.Many speakers do not make a clear distinction between thephones/E/and lee/when they occur word-internally, whichmay account for their high confusability.
The high numberof errors for /a /are  probably due to the large amount ofvariability of/a/observed in different contexts.14% of the insertions are Irl, followed by 11% for IlLThese two phones also are deleted the most: 13% of thedeletions are/1/and 11% Irl.
Although/1/and Irl accountfor many of the insertion and deletion errors, the overallerror rate for these phones are relatively low, 11% and 7%,respectively.
Improved performance on these phones may beachieved by modeling more contexts and by improving theirduration models.\] Condition II Corr.
\]Subs.
\[Del.
\]Ins.
\]Ace.
II CD 132 II 69.1 I 22.0 \] 8.9 \] 3.9 165.2 ITable 10: Recognition results for phone class based CD models.In Table 10 results are given for a set of 132 CD models.The models were selected so as to group phonetically similarcontexts based on manner of articulation classes.
'nais issimilar to the approach taken by Deng et al\[2\].
Taking intoconsideration that French is a syllable-based language, left-context models were defined for vowels and right-contextmodels for consonants.
The phone accuracy of 65.2% lies inbetween the recognition accuracies of the CI and CD models.WORD RECOGNIT ION EXPERIMENTSTwo types of implementation are usually considered torecognize words based on phone models.
In the first solu-347tion, which can be called integretated approach, an HMM isgenerated for each word by concatenating the phone modelsaccording to the phone transcriptions.
The word models areput together to represent the entire lexicon with one largeHMM.
The recognition process is then performed for exam-ple by using the Viterbi decoding algorithm.
The secondsolution uses the output of the phone recognizer as an in-termediary level of coding such that the lexical decoding isderived only from this ouput.
Phonological rules may beincluded in the lexical decoding, or alternatively may be rep-resented directly in the lexical entries.
The phone recognizeroutput is usually a phone weillis including phone hypothesesfor each of the associated speech segments and their corre-sponding likelihoods.
If the fist  approach appears to offer amore optimal solution to the decoding problem by avoidingan intermediary coding, the second approach greatly reducesthe computional requirements of the acoustic level which isindependent of the lexicon size and offers a solution to handleout of lexicon words.Since our goal is to build a system capable of recognizingat least 20,000 words, the second solution is attractive since itallows us to develop and evaluate lexical and language mod-els without interaction with the acoustic level.
In particular,this approach is of interest as it permits us to more easilystudy problems like liaison which are specific to the Frenchlanguage.
However, in order to obtain preliminary resultson word recognition using BREF, we have chosen to use theintegrated approach, primarily because the phone recognizerdoes not at this time provide a phone trellis.
In doing so wehave represented liaison in the lexicon by providing alternatepronunciations.Table 11: VI word recognition results (no grammar).Lexicon1KVocabulary-independent word recognition experimentswere run using four different lexicons.
The smaller lexi-con (1K lexicon) contains 1139 orthographic words, onlythose words found in the test sentences.
The 3K lexicon con-tains all the words found in the training and test sentences, atotal of 2716 words.
The 5K and 10K lexicons include all thewords in the test data complemented with the most commonwords in the original text.
These two lexicons contain re-spectively 4863 and 1051 1 words.
Alternate pronunciationsincrease the number of phonemic forms in the lexicon byabout 10%.
The word recognition results with no grammarare given in Table 11.
Since no grammar is used, single wordhomophone confusions are not counted as errors.Homophones present a large problem for French.
If theCorr:73.4homophone errors are included the phone accuracies drop byabout 10%.
A lexical study with 300,000 words found thatthere can be over 30 words with the same pronunciation[l].In the Le Monde text corpus of 4.2 million words, therewere 92,185 orthographically distinct words, but only 63,981phonemidly distinct words, giving a homophone rate ofabout 30%.
In the 1K and 3K lexicons the homophone rateis lower, on the order of 15%.
The "worst-case" homophonein the 3K lexicon is for the phonemic word /sA/, which maycorrespond to any of the 7 following orthographic words:100, cent, cents, s'en, sang, sans, sent.
For comparison,there are roughly 3% homophones in RM, less than 2% forTIMIT, and less than 5% in the MIT Pocket lexicon.While the large number of word homophones in Frenchpresents its problems, more complicated homophone prob-lems exist, where sequences of words form homophones.The example in Figure 1 shows some of the homophones forthe phonetic sequence /parle/ for the words in the 3K lexicon.These multiple word homophones account for a few percentof the errors in Table 11.
In fluent speech, the problems aremore complicated as illustrated by Figure 2.
While nomi-nally the phonetic transcription of the word "adolescence" is/adclEsAs/, the realized pronunciation is /adxlEsAs/, havingSubs.20.9the given homophones.phonetic transcription: p a r 1 eword candidates: parlerDel.5.8par lespart lesparle esparlent esparle etparlent etFigure 1: An example of a multiple word homophone.Ins.4.2phonetic transcription: a d x 1 E s A sword candidates: adolescencea de les sansa de les sensAcc.69.2Figure 2: An example of a homophone caused by vowel reduction.The examples given in Figures 1 and 2 do not considersyntactic or semantic constraints.
Figure 3 gives an exampleof the possible analyses of the phrase "un murmure de me-contentement".
This example taken from [l] illustrates boththe complexity of the problem and the power of the syntac-tic constraints.
Lexical access using a full-form lexicon withover 300,000 entries yields 340 possible word segmentations.This expands to over 2 million possible phrases when all thecombinations are considered.
Syntactic constraints includ-ing form agreement reduce the set to 6 possibilities, all ofwhich are semantically plausible.text :  un murmure de m6contentementphones: /ImyrmyrdxmekOtAtmA/lexical access: 340 possible word segmentations2,419,620 phrasessyntact i c  ana lys i s :  6 possible phrases- un murmure de m6contentement- un murmure de m6contentes ment- un murmure de rues contentements- un tour mQr de m6contentement- un tour mQr de m6contentes merit- un mr  mQr de mes contentementsFigure 3: Lexical hypotheses from a phonemic transcription.D ISCUSSION AND SUMMARYThese preliminary experiments have set a baseUne perfor-mance for phone recognition using BREF.
The preliminaryresults are somewhat comparable to those obtained for En-glish using the TIMIT corpus.
Lee and Hon\[7\] report 53%accuracy (66% correct) for 39 CI models and 66% accuracy(74% correct) using 1450 right-CD models.
Digalakis etal.\[3\] report 64% (70% correct) accuracy using CI modelswith a 39-phone symbol set.
Levinson et a1.\[8\] report 52%phone recognition with 12% insertions, and do not specifythe number of deletions.
Phone recognition rates reportedfor French by Merialdo\[9\] for speaker-dependent (4 speak-ers) recognition of isolated syllables were 80.6% accuracy(84.4% correct).We have taken a first step at vocabulary-independent wordrecognition using 1K to 10K word lexicons with no grammar.The word accuracy falls from 69% to 49% when the lexiconsize increases from 1K to 10K.
While these experiments arepreliminary, they have given us insight into the problems en-countered in lexical access, particularly the difficulties foundwith single-word and multiple-word homophones, and withliaison.A procedure for automatic segmentation a d labeling ofthe BREF corpus is being developed.
The preliminary inves-tigations indicate that the main problems lie in predicting thephone string, and that while the segmentation is not exact, thevast majority of segment boundaries are located within thesame frame as a hand-segmentation.
However, it is expectedthat more accurate segmentations will be obtained by usingCD models for segmentation.
In addition, a smaller framestep will be used to provide a finer segmentation.Text-to-phone prediction can be improved by includingdifficult items, such as foreign words and acronyums, in theexception dictionary.
This will not, however, eliminate theneed for verification, as it will not handle alternate pronun-ciations.
One option is to have the text-to-phoneme systempropose alternate pronunciations for dates and acronyms,and to allow liaison and the pronunciation of mute-e to beoptional.
In addition, providing a means of flagging poormatches would greatly ease the process of verification.An HMM-based recognizer has been used for a baselineperformance evaluation and verification of the data.
In thefuture better acoustic phone models and duration models willbe used.
The improvement observed using the sets of CDmodels indicates, at least with these preliminary experiments,that the improvement appears to be related to the number ofCD models that can be trained.
We expect to obtain improvedphone recognition performance by using more of the trainingdata as only a small portion of the BREF corpus has beenused.REFERENCES\[1\] G. Adda (1992), personal communication.\[2\] L. Deng, V. Gupta, M. Lennig, P. Kenny, P. Mermelstein(1990), "Acoustic Recognition Componentof an86,000-wordSpeech Recognizer," Proc.
IEEE ICASSP-90, pp.
741-744.\[3\] V. Digalakis, M. Ostendoff, J.R. Rohkicek (1990), "FastSearch Algorithms for Connected Phone Recognition Usingthe Stochastic Segment Model," Proc.
DARPA Speech andNatural Language Workshop, Hidden Valley, June 1990, pp.173-178.\[4\] J.-L. Gauvain, L.F. Lamel, M. Esk6nazi (1990), "Design Con-siderations and Text Selection for BREF, a large French read-speech corpus," Proc.
ICSLP-90, pp.
1097-2000.\[5\] H.-W. Hon and K.-F. Lee (1990),"On Vocabulary-IndependentSpeech Modeling," Proc.
ICASSP-90, pp.
725-728.\[6\] L.F. Lamel, J.-L. Gauvain, M. Esk6nazi (1991), "BREF,a Large Vocabulary Spoken Corpus for French," Proc.EUROSPEECH-91, pp.
505-508.\[7\] K.-F. Lee, H.-W. Hon (1989), "Speaker-Independent PhoneRecognition Using Hidden Markov Models," Proc.
IEEETrans.
ASSP, Vol.
37, No.
11, pp.
1641-1989.\[8\] S.E.
Levinson, M.Y.
Liberman,A.
Ljolje, L.G.
Miller (1989),"Speaker Independent Phonetic Transcription of FluentSpeech for Large Vocabulary Speech Recognition," Proc.IEEE ICASSP-89, pp.
441-444.\[9\] B. Merialdo (1988), "Phonetic Recognition Using HiddenMarkov Models and Maximum Mutual Information Train-ing," Proc.
IEEE ICASSP-88, pp.
111-114.\[10\] B. Prouts (1980),"Contribution/da synth~se de la parole hpar-tit du texte: Transcription graph~me-phon~me en t mps r6elsur microprocesseur", Th~se de docteur-ing6nieur, Universit6Paris XI, Nov. 1980.\[11\] L.R.
Rabiner, B.H.
Juang, S.E.
Levinson, M.M.
Sondhi( 1985 ), "Recognition of Isolated Digits Using Hidden MarkovModels with Continuous Mixture Densities," AT&TTechnicalJournal, 64(6), pp.
1211-1233, July-Aug. 1985.\[12\] R. Schwartz, Y. Chow, O. Kimball, S. Roucos, M. Krasner, J.Makhoul (1985), "Context-dependent modeling for acoustic-phonetic recognition of continuous peech," Proc.
ICASSP-85, pp.
1205-1208.349
