Extract ing Key Paragraph based on Topic and Event Detect ion- -  Towards Mul t i -Document  Summarizat ionFumiyo Fukumoto and Yoshimi Suzuki tDepartment.
ofComputer Science and Media Enginccring,Yamanashi University4-3-11 Takeda, Kofu 400-8511 Japan{j~tkumotoCo)skye.
esb:.
ysuzuki @aIps l. esi~ }.
yamano.~hi, ac.jpAbstractThis paper proposes a method for extracting keyparagraph for multi-document summarization basedon distinction between a topic and a~ event.
A topicemd an event are identified using a simple criterioncalled domain dependency of words.
The methodwas tested on the TDT1 corpus which has been de-veloped by the TDT Pilot Study and the result canbe regarded as promising the idea of domain depen-dency of words effectively employed.1 IntroductionAs the volume of olfline documents has drasticallyincreased, summarization techniques have becomevery importaalt in IR and NLP studies.
Most of thesummarization work has focused on a single docu-ment.
Tiffs paper focuses on multi-document sum-marization: broadcast news documents about thesame topic.
One of the major problems in the multi-document summarization task is how to identify dif-ferences and similza'ities across documents.
This canbe interpreted as a question of how to make a cleardistinction between an e~ent mM a topic in docu=meats.
Here, an event is the subject of a documentitself, i.e.
a writer wants to express, in other words,notions of who, what, where, when.
why and how ina document.
On the other hand, a topic in this paperis some unique thing that happens at some specifictime and place, and the unavoidable consequences.I t 'becomes background among documents.
For ex-ample, in the documents of :Kobe Japan quake', theevent includes early reports of damage, location andnature of quake, rescue efforts, consequences of thequake, a~ld on-site reports, while the topic is KobeJapaa~ quake.
The well-known past experience fromIR ~ that notions of who, what, where, when, whyand how may not make a great contribution to thetopic detection and tracking task (Allan and Papka,1998) causes this fact, i.e.
a topic and an event aredifferent from each other 1 .1 Some topic words can also be an event.
Fbr instance:in the document shown in Figure 1: 'Japan: and =quake' aretopic words and also event words in the document.
However,we regarded these words as a topic, i.e.
not be an event.In this paper: we propose a. method fi)r extract-ing key paragraph for multi-document smnmariza-tion based on distinction between a topic and anevent.
We use a silnple criterion called domain de-pendency of words as a solution and present how thei.dea of domain dependency of words can be utilizedeffectively to identify a topic and an event: and thusallow multi-document summarization.The basic idea of our approach is that whether aword appeared in a document is a topic (an event)or not, depends on the domain to which the docu-ment belongs.
Let us take a look at the followingdocument from the TDT1 corpus.
(1-2) Two Americans known dead in Japan quake1.
The number of \[Americans\] known to have beenkilled in Tuesday's earthquake in Japan has risen totwo, the \[State\] [Department\] said Thursday.2.
The first was named Wednesday as Voni Lynn~Vong~ a teacher from California.
\[State I \[De-partment\] spokswoman Christine Shelly declinedto name the second: saying formalities of notifyingthe family had not been completed.3.
With the death toll still mounting, at least 4:000people were killed in the earthquake which devas-tated the Japanese city of Kobe.4.
\[U.S.\] diplomats were trying to locate the sevcrMthousand-strong \[U.S.\] community in the area: andsome \[Americans\] who had been made homelesswere found shelter in the \[U.S.\] consulate there:which was only lightly damaged in the quake.5.
Shelly said an emergency \[State\] \[Department\]telephone number in Washington to provide infor-mation about private \[American\] citizens in Japanhad received over 6,000 calls, more than half ot'Th-'e'mseeking direct assistance.6.
The Pentagon has agreed to send 57:000 blanketsto Japan and \[U.S.\] ambassador to Tokyo ~ValterMondale has donated a $25,000 discretionary fundfor emergencies to the Japanese Red Cross, Shellysaid.7.
Japan has also agreed to a visit by a team of \[U.S.\]experts headed by Richard Witt, national directorof the Federal Emergency Management Agency.Figure 1: The document titled 'Two Americansl~lown dead in Japan quake'Figure I is the document whose topic is 'Kobe Japanquake', and the subject of the document (event31words) is 'Two Americans known dead in Japanquake'.
Underlined words denote a topic, and thewords marked with '\[ \]' are events.
'1,,,7' of Figure1 is paragraph id.
Like Lulm's technique of keywordextraction, our method assumes that an event asso-ciated with a document appears throughout parmgraphs (Luhn, 1958), but a topic does not.
This isbecause an event is the subject of a document i self.while a topic is an event, along with all directly re-lated events.
In Figure 1, event words 'Americans'and 'U.S.
', for instance, appears across paragraphs,while a topic word, for example, 'Kobe' appears onlythe third paragraph.
Let us consider further a broadcoverage domain which consists of a small number ofsanaple news documents about the same topic, 'KobeJapan quake'.
Figure 2 and 3 are documents with'Kobe Japan quake'.
( l- l)  Quake collapses buildings in central Japan1.
At lea.~t two people died and dozens were injuredwhen a powerful earthquake rolled through centralJapan Tue..~lay morning, collapsing buildings andsetting off fires in the cities of Kobe and Osaka.2.
The Japan Meteorological Agency said theearthquake, which measured 7.2 on the open-endedRichter scale: rmnbled across Honshu Island fromthe Pacific Ocean to the Japan Sea.Figure 2: The document itled 'Quake collapsesbuildings in central Japan'(1-3) Kobe quake leaves questions about medical system1.
The earthquake that devastated Kobe in Januaryraised serious questions about the efficiency ofJapan's emergency medical system, a governmentreport released on Tuesday said.2.
'The earthquake xposed many i~ues in termsof quantity, quality, promptness and efficiency ofJapan's medical care in time of disaster,' the reporton-'ff'h-~alth and welfare said.Figure 3: The document itled 'Kobe quake leavesquestions about medical system'Underlined words in Figure 2 and 3 show the topicof these documents.
In these two documents, :Kobe'which is a topic appears in eveD" document, while'Americans' and 'U.S.'
which are events of the docu-ment shown in Figure 1, does not appear.
Our  tech-nique for making the distinction between a topic andan event explicitly exploits this feature of the domaindependency of words: how strongly a word featuresa given set of data.The rest of the paper is organized as follows.The next section provides domain dependency ofwords which is used to identify a topic and an eventfor broadcast news documents.
We then present amethod for extracting topic and event words: and de-scribe a paragraph-based summarization algorithmusing the result of topic and event extraction.
Fi-nally~ we report some experiments using the TDT1corpus which has been developed by the TDT (TopicDetection and Tracking) Pilot Study (Allan andCarbonell, 1998) with a discussion of evaluation.2 Domain  Dependency  o f  WordsThe domain dependency of words that how stronglya word features a given set of data (documents) con-tributes to event extraction, as we previously re-ported (Fukumoto et al: 1997).
In the study, wehypothesi~d that the articles from the Wall StreetJournal corpus can be structured by three levels, i.e.Domain, Article and Paragraph.
It'a word is nil eventin a given article, it satisfies the two conditions: (1)The dispersion value of the word in the Paragraphlevel is smaller than that of the Art.iele, since the.word appears throughout paragr~q~hs in the Para-graph level rather than articles in the Article level.
(2) The dispersion value of the word in the Arti-cle is smaller than that of the Domain, as the wordappears across articles rather than domains.However, ~here are two problems to adapt it tomultl-document summarization task.
The first isthat the method extracts only events in the docu-ment.
Because the goal of the study is to summarizea single document, and thus there is no answer tothe question of how to identi~' differences and sim-ilarities across documents.
The second is that theperformance of the method greatly depends on thestructure of a given data itself.
Like the Wall StreetJournal corpus, (i) if a given data caal be structuredby three levels, Paragraph, Article and Domain, eachof which consists of several paragraphs, articles anddomains, respectively, aaad (ii) if Domain consists ofdifferent subject domains, such as 'aerospace', 'en-vironment' and 'stock market', the method can bedone with satisfactoD' accuracy.
However, there isno guarantee to make such an appropriate structurefrom a given set of documents in the multi-documentsummarization task.The purpose of this paper is to define domaindependency of words for a number of sample doc-uments about the same topic, and thus for multi-document summarization task.
Figure 4 illustratesthe structure of broadcast news documents whichhave been developed by the TDT (Topic Detectionand Tracking) Pilot Study (Allan and Carbonell,1998).
It consists of two levels, Paragraph and Doc-ument.
In Document level, there is a small numberof sample news documents about the same topic.These documents are arranged in chronological or-der such as, ' ( l - l )  Quake collapses buildings in cen-tral ,Japan (Figure 2)', '(1-2) Two Americans knowndead in Japan quake (Figure 1)' and '(1-3) gobequake leaves questions about medical system (Fig-ure 3)'.
A particular document consists of severalIIIII32IIIiIIiIparagraphs.
We call it Paragraph level.
Let wordswithin a document be an event, a topic, or amongothers (We call it n .qeneraZ word).
(H)0 0i:r~umedlevel(t.2):0 xx:h00; 5 Ai=20.3}oX00 X ~ o o * .
.0 ' -.. Ji=mooi Paragraphleve~ ! '
0 / ', Xi::rX :alk.2i o .
?
l li xi?~ lopic word& event wordx: general word(1.1) 'Qu~e corpses b,.l~s in cen~ Japan'(1.2)'Two Americans known dead b Japan qu~e'(1-3) ~obe quake leaves quests about me&al system'Figure 4: The stnmture of broadcast news documents(event extraction)Given the structure shown in Figure 4, how can weidenti~" every word in document (1-2) with an event,a topic or a general word?
Our method assumes thataal event associated with a document appears acrossparagraphs, but a topic word does not.
Then, we usedomain dependency of words to extract event andtopic words in document (1-2).
Domain dependencyof words is a measure showing how greatly each wordfeatures a given set of data.In Figure 4.. let 'C)', 'A '  and 'x '  denote a topiclan event and a general word in document (1-2), re-spectively.
We recall the example shown in Figure 1.
'A',  for instance, 'U.S.'
appears across paragraphs.However, in the Document level, :A' frequently ap-pears in document, (1-2) itself.
On the basis of thisexample, we hypothesize that if word i is an event,it"satisfies the following condition:\[1\] Word i greatly depends on a particulardocument in the Document level ratherthan a particular paragraph in the Para-graph.Next, we turn to identi~" the remains (words) wit.ha topic, or a general word.
In Figure 5; a topic ofdocuments (1-1) ~ (1-3), for instance, :Kobe' aP-pears in a particular paragraph in each level of Para-graphl, Paragraph2 and Paragraph3.
Here, (1-1), (1-2) and (1-3) corresponds to Paragraph1, Paragraph2and Paragraph3, respectively.
On the other hand,in Document level, a topic frequently appears acros.~documents.
Then: we hypothesize that if word i is a33(H).
x  x I.~e.nt  !e_ve\[.
c.o ' : .
i:~1Paragraph 1: C.level ~!
C' xi xj=l?
o?
- .
?ij=2 ~ .
.
.
.
.
j=nic.ParagraphZi O "level !!
x i(1-2) p-3)?x x \[ 0 t i zL i:i=2 i=3m ~- - iX i , .
, .
oi=rni 0 i !
,  iO:topic word Paragraph 3 0 ; jx: general word i leve iJ !
o x !~ j !C': .......l~igure 5: The structure of broadcast news documents(topic extraction)topic, it satisfies the following condition:\[2\] Word i greatly depends on a particu-lar paragraph in each Paragraph levelrather than a particular document inDocument.3 Topic and Event ExtractionWe hypothesized that  the domain dependency o fwords is a key clue to make a distinction betweena topic and an event.
This can be broken down intotwo observations: (i) whether a word appears acrossparagraphs (documents), (it) whether or not a wordappears frequently.
We represented the former byusing dispersion value, and the latter by deviationvalue.
Topic and event words are extracted by usingthese values.The first step to extract opic and event words isto assign weight to the individual word in a docu-ment.
We applied TF*IDF to each level of the Doc-ument and Paragraph, i.e.
Paragraphl, Paragraph2and Paragraph3.NWdit = TFdit * log Ndt (1)Wdit in formula (1) is TF*IDF of term t in the i-thdocument.
In a similar way, Wpit denotes TF*IDFof the term t in the i-th paragraph.
TFdit in (1)denotes term frequency of t in the i-th document.
Nis the number of documents and Ndt is the numberof do(:uments where t occurs.
The second step is tocalculate domain dependency of words.
We definedit by using formula (2) and (3).DispOt = /I/E'~=l(I4;dit - mean')2 (2)?
TnDe vdi, = (Wdit - meant) ,10+50 (3)DispDtFormula (2) is dispersion value of term t in the levelof Document which consists of m documents, anddenotes how frequently t appears across documents.In a similar way, DispPt denotes dispersion of termt in the level of Paragraph.
Formula (3) is the devia-tion value of t in the i-th document and denotes howfrequently it appears in a particular document, hei-th document.
Devpit is deviation of term t in thei-th paragraph.
In (2) and (3), meant is the meanof the total TF*IDF values of term t in the level ofDocument.The last step is to extract a topic and an ever~tusing fonmfla (2) and (3).
We recall that if t is anevent, it satisfies \[1\] described in section 2.
This isshown by using formula (4) mad (5).DispPt < DispDt (4)for all Pi E di Devpjt < Devdit (5)Formula (4) shows that t frequently appears acrossparagraphs rather than documents.
In formula (5),di is the i-th document and consists of the numberof n paragraphs (see Figure 4).
Pi is an element ofdi.
(5) shows that t frequently appears in the i-thdocument di rather than paragraphs pj ( 1 < j <n).
On the other hand: if t satisfies formula (6) and(7), then propose t as a topic.DispPt > DispDt (6)for all dl E D,Pit exists such that Devpjt >_ Devdlt (7)In formula (7), D consists of the number of rn doc-aments (see Figure 5).
(7) denotes that t frequentlyappears in the particular paragraph pj rather thanthe document di which includes pj.4 Key Paragraph ExtractionThe summarization task in this paper is paragraph-based extraction (Stein et al, 1999).
Basically, para-graphs which include not only event words but alsotopic words are considered to be significant para-graphs.
The basic algorithm works as follows:1.
For each document: extract topic and eventwords.2.
Determine the paragraph weights for all para-graphs in the documents:(a) Compute the sum of topic weights over thetotal number of topic words for each para-graph.
(b) Compute the sum of event weights over thetotal number of event words for each para-graph.A topic and an event weights are calculatedby using Devdlt in formula (3).
Here, t is atopic or an evcnt and i is the i-th documentin the documents.
(c) Compute the sum of (a) and (b) for eachparagraph.3.
Sort the paragraphs t~ccording to their weightsand extract he N highest weighted paragrai~hsin documents in order to yield summarizationof the documents.4.
When their weights are the same, Compute thesum of all the topic and event word weights.Select a paragraph whose weight is higher thanthe others.5 ExperimentsEvaluation of extracting key paragraph based onmulti-document is difficult.
First, we have not foundan existing collection of summaries of multiple doc-uments.
Second, the maamal effort needed to judgesystem output is far more extensive than for singledocument summarization.
Consequently, we focusedon the TDT1 corpus.
This is because (i) events havebeen defined to support the TDT study effort, (ii)it was completely annotated with respect o theseevents (Allan and Carbonell, 1997).
Therefore, wedo not need the manual effort to collect documentswhich discuss about the target event.We report the results of three experiments.
Thefirst experiment, Event Extraction, is concerned withevent extraction technique, ha the second experi-ment, Tracking Task, we applied the extracted top-ics to tracking task (Allan and Carbonell, 1998).The third experiment: Key Paragraph Extraction isconducted to evaluate how the extracted topic andevent words can be used effectively to extract keyparagraph.5.1 DataThe TDT1 corpus comprises a set of documents(.15,863) that includes both newswire (Reuters)7..965 and a manual transcription of the broadcastnews speech (CNN) 7,898 documents.
A set of 25target events were defined 2All documents were tagged by the tagger (Brill,1992).
%Ve used nouns in the documents.h t t p://morph.ldc.upenn.edu/TDTIIIIIiIIIIIIiIi!Ii34IIIIIIIIIiIiI/IIII5.2  Event  Ext ract ionWe collected 300 documents from the TDT1 corpus,each of which is mmolated with respect o one of 25events.'
The result is shown in Table 1.In Table 1, 'Event type' illustrates the target eventsdefined by the TDT Pilot Study.
'Doe' denotes thenumber of documents.
'Rec' (Recall) is the imm-ber of correct events divided by the total mnnberof events which are selected by a human, and 'Prec'(Precision) stands for the number of correcteventsdivided by the number of events which are selectedby our method.
The denominator 'Rec' is made bya hmnan judge.
'Accuracy' in Table 1 is the totalaverage ratio.In Table 1, recall and precision values range from55.0/47.0 to 83.3/84.2, the average being 71.0/72.2.The worst result of recall and precision was whenevent type was 'Serbs violate Bihac' (55.0/59.3).
Wecurrently hypothesize that this drop of accuracy isdue to the fhct that some documents are against ourassumption of an event.
Examining the documentswhose event type is 'Serbs violate Bihac', 3 ( onefrom CNN and two from Reuters).out of 16 docu-ments has discussed the same event, i.e.
'BosnianMuslim enclave hit by heavy shelling'.
As a result,the event appears across these three documents?
Fu-ture research will shed nmre light on that.5.3 T rack ing  TaskTracking task in the TDT project is starting froma few sample documents and finding all subsequentdocuments that discuss the same event (Allan andCarbonell, 1998), (Carbonell et al, 1999).
The cor-pus is divided into two parts: training set and testset.
Each of the documents i flagged as to whetherit discusses the target event, and these flags ('YES',:'NO') axe the only information used for training the.system to correctly classiC" the target event.
We ap-plied the extracted topic to the tracking task under?
these conditions.
The basic algorithm used in theexperiment is as follows:1L Create a single document Sip and represent it asa term vectorFor the results of topic extraction, all the docu-ments that belong to the same topic are lmndledinto a single document Stp and represent i bya term vector as follows:Stp -~ttpltip2ttpns.t.
itpj ={ f(ttpj) i f t t~j isatoplcof Stp0 otherwise.f(w) denotes term frequency of word w.Represent other training and test documents aste rm vectors=.
S= =, 3.Let $1: --', S,, be all the other training docu-ments (where m is the number of training doc-uments which does not belong to the targetevent) and Sx be a test docmnent which shouldbe classified as to whether or not it discusses thetarget event.
81, "" ", Sm mid Sz are represented "by term vectors as follows:I l lti2s.t?
li.i = { f ( t ,A  if t,~ (1 < i < m) appears ill S; and not, be a topic of Sip0 otherwisetzli=2i=.f(t=j) if t.~j appears i,i t;~s.t.
txj = 0 otherwiseCompute the similarity between a training docu-ment and a test documentGiven a vector epresentation f documents SI,?
?
", Sin, Sty and Sx, a similarity between twodocuments Si (1 < i < m, tp) and the test doc-ument S~ would be obtained by using formula(8), i.e.
the inner product of their normalizedvectors.Si.
Sxs~m(s .s~)  - I S~ II S=l (s)The greater the value of Sim(Si, S=) is, themore similar Si and S,  are.
If the similarityvalue between the test document Sx and thedocument Sip is largest among all the otherpairs of documents, i.e.
(&,  S=).---,  (S~, S=),Sx is judged to be a document hat discussesthe target event.We used the standard TDT evaluation measureTable 2 illustrates the result.3.Table 2: The results of tracking task1248?
16Avg%Miss32.523.723.112.013.721.0%F/A F1 %Rec %Prec0.16 0.68 67.5 70.00~06 0.80 76.3 87.80.05 0.81 76.9 90.10.08 0,87 88.0 91.40.06 0.89 86.3 93.60.08 0.76 79.0 86.6In Table 2, 'Nt' denotes the number of positive train-ing documents where A~ takes on values 1, 2, 4, 8.3 h t tp : / /www.n is t .gov /speech/ td t98 .htm35ITable 1: The results of event words extraction ImEvent type Doc Avg Rec/Avg Prec 'Karrigan/Harding 2 .
64.1/55.5 " ' IKobe Japan quake 16 74.5/75.0Lost in Iraq 16 ~5.7/68.8NYC Subway bombing 16 68.0/84.2 'OK-City bombing 16 78.8/47.0 ?
IPentium chip flaw 4 81.1/72.9Quayle lung clot 8 63.6/74.4Serbians down F- 16 16 .78"6/75"0 ISerbs violate Bihac 16 55.1)/59.3Shannon Faulker 4 11.4/82.4USAir 427 crash 16 72.6/86.3WTC Bombing trial 16 62.6/70.1 I71.0/72.2 - - -=  |In Table 3.
'Event' denotes event words in the firstdocument in chronological order from A~ --- 4, and ithe title of the document is 'Emergency Work Con-tinues After Earthquake in Japan'.
Table 3 clearlydemonstrates that the criterion, domain dependencyof-''words effectively employed.Figure 6 illustrates the DET (Detection Evalua-tion Tradeoff) curves for a sample event (event ype.is 'Comet into Jupiter) runs at several values of Nt.
\ ]~'/l!
~"  i~'~-~.~ .1, !
.
: ~ i i .
: i| !
: : " ~ " t ~.'.
"~ ~ " E -v*~ : ?
.
: " : ~ : ~ ~ :~ : : N=4 ....... :?
- : .
"1 : " , .
.
: t ' t "  ~"  : H=8 .
.
.
.
.
.
.
:I t t : : ** ; I g t?
~ t "~ * t -* : ?| ?
* * " ?
~ ?
~ , .
to  1~, ?
t ?
?
* ??
t I t l = i~ = .
"~ t ' ~ t l t =| 2o t - .
- .
.
.
.
.
r .
.
?
- .
.
.
- - - .
.T .
.
.
.~ : : : .
.
.
.
.
.
.
.
:  ; : :~ : .
.
.
.T .~ .
.
.
.
.
.
.
.
.
!
.
.
.
.
.
.
.
.
.
.
!
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
I  ; -  .. : ?
: ?
.
?
~ .
: .
.
.
: : : : I~o ~,..-.,...*...:......*....
!....~ .
.
.. ....
*.--'~.
'~';'i""~-"': ......... ~ .......... ":...... "i.s i , .4 .
- .1 .
.
.~ .
.
.
i .
.
.
.
.4 .
.
.
.
| .
.
.
.
.~  .
.
.
.
.
.
4 .
.
.
.
.
.
4 .
- .
.
:4b  a .~.
.
i .
.
'~  .
.
.
.
.
.
.
.
.
| .
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
4: : : : : : : " : : "'1:~ : ::" : : : IP .4 .
.
.
.
i .
.
.~ .
.
.
.
| .
.
.
.
.~ .
.
.
.
.
i .
.
.
.
.
i  .
.
.
.
.
.
4 .
.
.
.
.
.
.
;  .
.
.
.
.
.
~..i......i....~.
'...i .
.
.
.
.
.
.
.
.
4 .
.
.
.
.
.
.Event type Avg Rec/Avg Prec8 61.7/70.58 60.7/73.376.3/79.165.7/80.075.9/80.065.2._/61.965.2173.983.3/71.478.7/72.962.0/74.078.5/75.0.80.4/70.28 75.9/72.2Di.spPt DispDtDocAldrich AmesCarlos the JackalCarter in BosniaCessna on White HouseClinic Murders' Comet into JupiterCuban riot in PanamaDeath of Kim Jong-DNA in OJ trialHaiti ousts observersHall's copter168161621616816Humble: TX, flooding 16Justice-to-be BreyerAccuracyand 16.
'Miss' means Miss rate, which is the ra-tio of the doounents that were judged as YES butwere not evahmted as YES for the run in question.
'F/A' shows false alarm rate and 'FI' is a measurethat balances recall and precision.
'Rec' denotes theratio of the documents judged YES that were alsoevaluated as YES, and 'Prec' is the percent of thedocuments that were evaluated as YES which corre-spond to documents actually judged as YES.Table 2 shows that more training data helps theperformance, as the best result was when we used:Yt = 16.Table 3 illustrates the extracted topic and eventwords in a sample document.
The topic is 'KobeJapan quake' and the number of positive trainingdocuments is 4.
'Devpzt', 'Devd\]t', DispPt' and'DispDt' denote values calculated by using formula(2) and (3).,Table 3: Topic and event words in :Kobe Japanquake'Topic wordearthquakeJapanKobefireDevplt53,569,856,657.0Devdzt50.050.050.046.412.3 10.313.3 9.88.6 6.42.3 1.5Event wordemergencyareaworkerrescueDevplt50.040.650.043.3Devdzt74.750.066.150.0DispP t0.90.60.42.3DispDt1.51.01.03.4.ol .
(m .o6 o.1 o2.
o.5 1 g s lo '2o 4o $o 8o 90Fatse Atarm p'rotm~Jity (in %)I IEigure 6: DET curve for a sample tracking runs ?Overall, the curves also show that more traininghelps tile performance, while there is no significant Bdifference among -'Yt = 2, 4 and 8.5.4 Key  Paragraph Extract ionro l lWe used 4 different sets as a test data.
Each set con- ?sists of 2, 4.. 8 and 16 documents.
For each set, we36II5.2 Event  Ext ract ionWe collected 300 docmnents from the TDT1 corpus,each of which is annotated with respect o one of 25events.'
The result is shown in Table 1.In Table 1.. 'Event type' illustrates the target eventsdefined by the TDT Pilot Study.
~Doc' denotes thenumber of documents.
'Rec' (Recall) is the nmn-bet of correct events divided by the total numberof events which are selected by a humaa, and :Pree ~(Precision) stands for the number of correct-eventsdivided by the number of events which are selectedby our method.
The denominator 'Rec: is made bya human judge.
'Accuracy' in Table 1 is the totalaverage ratio.In Table 1, recall and precision values range, from55.0/47.0 to 83.3/84.2, the average being 71.0/72.2.The worst result of recall and precision was whenevent ype was 'Serbs violate Bihac' (55.0/59.3).
Wecurrently hypothesize that this drop of accuracy isdue to the fact that some documents are against ourassumption of an event.
Examining the ctocumentswhose event type is 'Serbs violate Bihac', 3 ( onefrom CNN and two from Reuters) out of 16 docu-ments has discussed the same evefit, i.e.
'BosnianMuslim enclave hit by heavy shelling'.
As a result,the event appears across these three documents.
Fu-ture research will shed more light on that.5.3 T rack ing  TaskTracking task in the TDT project is starting froma few sample documents and finding all subsequentdocuments that discuss the same event (Allan andCarbonell, 1998), (Carbonell et al, 1999).
The cor-pus is divided into two parts: training set and test~et.
Each of the documents i flagged as to whetherit discusses the target event, and these flags ('YES','NO') are the only information used tbr training thesystem to correctly classiC" the target event.
We ap-plied the extracted topic to the tracking task underthese conditions.
The basic algorithm used in the?
experiment is as follows:1.
Create a single document Stp and represent it as".a term vectorFor the results of topic extraction, all the docu-ments that belong to the sanae topic are bundledinto a single document S,p and represent i bya term vector as follows:~tp -~ttplttp2?
s .
t .
t tp j  =ttpn{ /(t,pj) ift,pj is atoplcof Stp0 otherwisef (w)  denotes term frequency of word w.2.
Represent other training and test documents asterm vectors35Let $1, ---, S,,, be all the other training docu-ments (where m is the number of training doc-uments which does not belong to the targetevent) and Sx be a test document which shouldbe classified as to whether or not it discusses thetarget event.
$1, "- -, Sm and Sx are represented "by term vectors as follows:~ =' "  {s .
t .
l l j  =f(t~j) ift 0 (1 < i <m)appears in S~ andnot be a topic of ,5"tp(I otherwiseS= =tzlt~2?
S .
t .
t~ j  =f(t.r.j) ift~j ~ppears i , S,0 otherwise3.
Compute the similarity between a training docu-ment and a test documentGiven a vector epresentation f documents SI,?
.., S.,, Stp and S=; a similarity between twodocuments Si (1 < i < m,  tp) mad the test doc-ument S= would be obtained by using formula(8), i.e.
the inner product of their normalizedvectors.Si ?
S=Sim(Si, S~) = I Si II S~ I (S)The greater the value of S im(S i ,S , )  is, themore similar 5"/ and Sz are.
If the similarityvalue between the test document S, and thedocument Stp is largest among all the otherpairs of documents, i,e.
($1, Sx), " ", (Sin, S=),S= is judged to be a document hat discussesthe target event.We used the standard TDT evaluation measure 3Table 2 illustrates the result?Table 2: The results of tracking taskNt %Miss %F/A F1 %Rec %Prec1 32.5 0.16 0.68 67.5 70:02 23.7 0.06 0.80 76.3 87.84 23.1 0.05 0.81 76.9 90.18 12.0 0.08 0.87 88.0 91.416 13.7 0.06 0.89 86.3 93.6"Avg 21.0 0.08 0.76 79.0 86.6In Table 2, 'Nt '  denotes the number of positive train-ing documents where A~ takes on values 1, 2, 4, 8z http://www.nist.gov/speech/tdt98.htrnTable 1: The results of event words extract ionIIEvent type Doe Avg Rec /Avg Prec Event type Doc Avg Rec /Avg  Prec.. Aldrich Ames 8 61.7/70.5 Karr igan/Hard ing 2 64.7/55.5 .
I ICarlos the Jackal 8 60.7/73.3 Kobe Japan  quake 16 74.5/75.0Carter  in Bosnia " 1-6 76.3/79.1 Lost in I raq 16 75.7/68.8 JCessna on White House 8 65.7/80.0 NYC Subway bombing 16 68.0/84.2cl inic Murders 16 75.9/80.0 OK-Ci ty  bombing 16 78.8/47.0 iComet into Jupi ter  16 6~o.2/61.9 Pent ium chip flaw 4 81.1/72.9 IICuban riot in Panama 2 65.2/73.9 Quayle lung clot 8 63.6/74.4Death of K im Jong 16 83.3/71.4 Serbians down F-16 16 78.6/75.0 * lDNA in OJ  trial 16 78.7/72.9 Serbs violate Bihac 16 55.0/59.3 iHaiti  ousts observers 8 62.0/74.0 Shannon Faulker 4 71.4/82.4 lHall 's copter 16 78.5/75.0 USAir 427 crash 16 72.6/86.3Humble,  TX,  f looding 16 ...... 80.21/70.2 WTC Bombing trial 16 62.6/70.1Just ice-to-be Breyer 8 75.9/72.2 I!Accuracy 71.0/72.2!and 16.
'Miss' means Miss rate, which is the ra- In Table 3, 'Event '  denotes event words in the first -rio of the documents that were, judged as YES but  document in chronological order from .,X~ = 4, andnot evaluated as YES for the run in question, the tit le of the document  is 'Emergency Work Con- i were' F /A '  shows false Mann rate mad 'F I '  is a measure tinues After Earthquake in Japan ' .
Table 3 clearly ithat  balances recall and precision.
'Rec'  denotes the demonstrate~ that  the criterion, domain dependencyratio of the documents  judged YES that  were also of words effectively employed, ievaluated as YES, and Tree '  is the percent of the Figure 6 i l lustrates the DET  (Detect ion Evalua- |documents that  were evaluated as YES which corre- tion Tradeoff)  curves for a sample event (event typespond to documents  actually judged as YES.
is 'Comet  into Jupi ter ' )  runs at several values of Art.iTable 2 shows that  more training data helps theperformance, as the best result was when we used 9o , .
, .
.
.
.
.
.
.
.
.
---., , .
.
.
.
.
.
, .
.
.
.  '
I-'Vt = 16.
~" ?
q .." ~, ~ "', .
: ?
.. , ,~m~'~,~' - - - -Table 3 i l lustrates the extracted topic and event E0 ~" ...... " ~"'*'''"=~''~'":"*'"'"'"'''~i .:',..
: wt.
".....: : ................ i e ~  ....... ?
Bwords in a sample  document.
The topic is 'Kobe  i i i i q  i ~a'!4 ~-  i i ~ \ [ ; \ [ :  W Japem quake' and the m~mber of posit ive training e0 ~.4....i..-~...~....s....i-...i~:u...i...~,.4 .... .
... ... e ,~ ,  ?i .
:  ~ " ~ . '
.~'~.
'~ ;.~ " ~ .
~-- ' , ' .
:  documents  is 4.
'Devp\]t', 'Devd\]t', 'DispPt'  and ~ l :.
:.
: : :.
'-q: ;~ ~.~.
: 1 : m~s--  !
'DispDt' denote values calculated by using formula 4o : ~..~..~.~.
: .
.
.
:(2) and (3).
: : : " :: : : : : : ~ : ~.
.
| .a .
: i  .
: " -: : - -20 ~*.. '2.
.
.
, .
t .
.
.
. '
* - - .
: ,* ,* ,?
.
.
.
.
;~: :=~: : ; - -  : * ' * " : ' "~"  .
.
.
.
.
.
.
!
.
.
.
.
.
.
.
.
.
.
!
.
.
.
.
.
.
.
.
.
.
.
9" " ' " *~i i : : " : ~ : : " .
: " .
:- " " !
: " i : .
: : " .
: : .
!~ .
- t .~_  :.
: : : : Table 3: Topic and event words in 'Kobe Japan  ~o| i i i i i i i i i :41 t. ~.
\[ i i i"quake'  ~ i,.L...i.,.;-..i.....;....|.....
; .. 4......~;..... ~j.an..:~ ........ i ......... ,;....... di " "  ": " : " " " " "  " I "  ":~ ": " !
I""  " " ~ " " i " " .,~.
"~- , "  " "Topic word Devp~t Devd~t DispPt DispDt ~ i..4....i...~....i.....~....J.....i .
.. 4 ..... 4 .
.
.
.
.
.
?..i.....-i....~..i .
........ 4...... q : : : ?
!
:" ?
.
: - .
.
: .
.
:  ?
.
,  ~ .
"earthquake 53.5 50.0 12.3 10.3 ~ "=:"= ' .
.
.
.
"- : =:" .
.
.
.
.
.
.
.
.  "
......... "==" ............ : : .01 .
(\]2 .I\]6 0.1 0.2 0.5 1 2 S 10 20 40 60 80 90J apanKobefire69.856.657.050.050.046.413.38.62.39.86.41.5Event wordemergencyaxeaworkerrescueDevplt50.040.650.043.3Devdlt74.750.066.150.0Di s pP t0.90.60.42.3bL~pDt1.51.01.03.4Fat~ ~aan.
Pr0ea~y fm ~)I IFigure 6: DET  curve for a sample tracking runs B'Overal l ,  the curves also show that  more trailfinghelps tile performance,  while there is no significantdifference anaong :Yt = 2, 4 and 8. il5.4 Key  Paragraph  Ext rac t ionWe used 4 different sets as a test data.
Each set con- Isists of 2, 4, 8 and 16 documents.
For each set, we I I36IIextracted 10% and 20% of the full-documents para-"graph length (Jing et al, 1998).
Table 4 illustratesthe result.In Table 4, 'Num ~ denotes the number of documentsin a set.
10 and 20?~ indicate the extraction ratio.
'Para' denotes the number of par~]graphs exr.ractedby a humaa~ judge, and 'Correct' shows the accuracyot" the method.The best result was 77.7% (the extraction ratio is20% and the number of documents i 2).Wc now turn our attention to the main question:how was the contribution of making the distinctionbetween a topic and an event for summarizationtask?
Figure 7 illustrates the results of the methodswhich used (i) the extracted topic artd event words,i.e.
our method, and (ii) only the extracted event"words.75~, 708<17560551 4 8 16NumFigure 7: Accuracy with each methodIn Figure 7, '(10%): and '(20%)' denote the ex-tracted paragraph ratio.
'Event' is the result whenwe used only the extracted event words.
Figure 7shows that our method consistently outperforms themethod which used only the extra,.ted events.
Tosummarize the evaluation:][: Event extraction effectively employed wheneach document discusses different subject aboutthe same topic.
This shows that the method willbe applicable to other genres of corpora whichconsist of different subjects.2.
The result of tracking task (79.0% average recalland 86.6% average precision) is comparable tothe existing tracking techniques which tested onthe TDT1 corpus (Allan and Carbonell, 1998).3.
Distinction between a topic and an event im-proved the results of key paragraph extrac-tion, as our method consistently outperformsthe method which used only the extracted eventwords (see Figure 7).376 Re la ted  WorkThe majority of techniques for summarization fallwithin two broad categories: Those that rely on tem-plate instantiation and those that rely on passageextraction.Work in the former approach is the DARPA-sponsored TIPSTER program and, in particular, themessage understanding conferences hag provided fer-tile groined for such work, by placing the emphasisof docunmnt analysis to the identification and ex-traction of certain core entities and facts in a doc-ument, while work on template-driven, knowledge.based summarization to date is hardly domain orgenre-independent (Boguraev and Kennedy.
1997).The alternative approach largely escapes this con-straint, by viewing the task as one of identi~,ingcertain passages(typically sentences) which, by somemetric, are deemed to be the most representative, ofthe document's content.
A variety of approaches ex-ist for determining the salient sentences in the text:statistical techniques based oll word distribution(Kupiec et al, 1995), (Zechner, 1996), (Salton etal., 1991), (Teufell and Moens, 1997), symbolic tech-niques based on discourse structure (Marcu, 1997)and semantic relations between words (Barzil~v andElhadad, 1997).
All of their results demonstrate hatpassage xtraction techniques are a useful first stepin document summarization, although most of themhave focused on a single document.Some researchers have started to apply asingle-document summarization technique to multi-document.
Stein et.
al.
proposed a method forsummarizing multi-document using single-documentsummarizer (Stralkowsik et al, 1998), (Stralkowskiet al.
1999).
Their method first summarizes eachdocument of multi-document, then groups the sum-maries in clusters and finally, orders these summariesin a logical way (Stein et al, 1999).
Their techniqueseems ensible.
However, as she admits, (i) the orderthe information should not only depend on topic cov-ered, (ii) background information that helps clari~"related information should be placed first.
More seri-ously, as Barzilay and Mani claim, summarization ofmultiple documents requires information about sim-ilarities and differences across  documents.
There-fore it is difficult to identi~" these information usinga single-document summarizer technique (Mani andBloedorn, 1997), (Barzilay et al, 1999).A method proposed by Mani et.
al.
deal withthe problem, i.e.
they tried to detect the similar-ities and differences in information content  amongdocuments (Mani and Bloedorn, 1997).
They useda spreading activation algorithm and graph match-ing in order to identify similarities and differencesacross documents.
The output is presented as a setof paragraphs with similar and unique words high-lighted.
However, if the same information is men-Nun:Table 4: The results of Key Paragraph ExtractionAccuracy%10Paa'a Correct(%) Para2 58 44(75.8) 1174 107 80(74.7) 2148 202 138(68.3) 40416 281 175(62~) 563Total 648 437(67.4) 1,298%20Correct(%) Para91(77.7) 175160(74.7) 321278(68.8) 606361(64.1) 844890(68.5) 1,946TotalCorrect(%)135(77.1)240(74.7)416(68.6)536(63.5)1,327(68.1)"tioned several times in different documents, much ofthe summary will be redundant.Allan et.
al.
also address the problem aald pro-posed a method for event tracking using commonwords and surprising features by supplementing thecorpus statistics (Allan and Papka, 1998) (Papka etal., 1999).
One of the purpose of this study is tomake a distinction between an event aald an eventclass using surprising features.
Here event class fea-tures are broad news areas such as politics, death,destruction and ~,'~fare.
The idea is considered tobe necessary to obtain higti accuracy, while Allanclaims that the surprising words do not provide abroad enough coverage to capture all documents onthe event.A more recent approach dealing with this problemis Barzilav et.
al's approach (Barzilay et al, 1999).They used paraphrasing rules which are maaauallyderived from the result of syntactic analysis to iden-tify theme intersection and used language generationto reformulate them as a coherent, summary.
Whilepromising to obtain high accuracy: the result of sum-marization task has not been reported.Like Mani and Barzil~,'s techniques, our ap-proach focuses on the problem that how to identi~"differences and similarities across documents, ratherthan the problem that how to form the actual sum-mar:,, (Sparck, 1993), (McKeown and Radev, 1995),(Radev and McKeown, 1998).
However, while Barzi-lav's approach used paraphrasing rules to eliminateredmadancy in a summary, we proposed omain de-pendency of words to address robustness of the tech-nique.7 Conc lus ionIn this paper, we proposed a method for extract-ing key paragraph for summarization based on dis-tinction between a topic and an event.
The resultsshowed that the average accuracy was 68.1~ whenwe used the TDT1 corpus.
TIPSTER Text Sum-marization Evaluation (SUMMAC) proposed vari-ous methods for evaluating document summariza-tion and tasks (Mani et al, 1999).
Of these, par-ticipants submitted two summaries: a fixed-lengthsummary limited to 10% of tile length of the source,and a summary which was not limited in length.
Fu-ture work includes quantitative and qualitative val-uation.
In addition, our method used single wordsrather thaaa phrases.
These phrases, however, wouldbe helpful to resolve ambiguity and reduce a lot ofnoise, i.e.
yield much better accuracy.
We plaal toapply our method to phrase-based topic and eventextraction, then turn to focus on the problem thathow to form the actual summary..AcknowledgmentsThe authors would like to thank the reviewersfor their valuable comments.
This work was sup-ported ~'  the Grant-in-aid for the Japan Society forthe Promotion of Science(JSPS, No.11780258) andTateisi Science and Technology Foundation.Re ferencesJ.
Allan and J. Carbonell.
1997.
The tdt pilot studycorpus documentation.
In TDT.Study.
Carpus,V1.3.doc.J.
Allan and J. Carbonell.
1998.
Topic detectionand tracking pilot study: Final report..
In Proc.of the DARPA Broadcast News Transcription andUnderstanding Workshop.J.
Allan and R. Papka.
1998.
On-line new event de-tection and tracking.
In Proc.
of 21st Annual b~-ternational A CM SIGIR Conference on Researchand Development in Information Retrieval, pages37-45.R.
Barzila.v and M. Elhadad.
1997.
Using lexicalchains for text summarization.
In Proc.
of ACLWorkshop on b~telligent Scalable Text Summa-rization, pages 10-17.R.
Barzilay, K. R. McKeown, and M. Elhadad.1999.
Information fusion in the context of multi-document summarization.
In Proc.
of 87th An-nual Meet.ing of Association for ComputationalLinguistics, pages 550-557.38IIiiiIIiIIIliiIIIIIB.
Boguraev mad C. Kennedy.
1997.
Saiience-basedcontent characterization f text documents.
IxiProc.
of A CL Workshop on b,telligent ScalableTezt Summarization: p~ges 2-9.E.
Brill.
1992.
A simple rule-based part of speechtagger.
In Proc.
of the 3rd Conference on AppliedNatural Language Processing, pages 152-155.J.
Carbonell, Y. Yang, mad J. Lafferty.
1999.
CMUreport on TDT-2: Segmentation, detection andtracking.
In Proc.
o/the DARPA Broadcast NewsWorkshop.F.
Fukumoto, Y. Suzuki, and J. Fukumoto.
1997.An automatic extraction of key paragraphs basedoil context dependency.
In Proc.
of the 5th Con-ference on Applied Natural Language Processing,pages 291-298.H.
Jing, R. Barzil~', K. R. McKeown, and M. E1-hadad.
1998.
Summarization evaluation methods:Experiments and analysis, intelligent ext sum- /marization.
In Proc.
o/1998 American Associa-tion/or Artificial h~telligence Sprin 9 Symposium,pages 51-59.J.
Kupiec, 3.
Pedersen, and F..Chen.
1995.
Atrainable document summarizer.
In Proc.
of the18th Annual International ACM SIGIR Confer-ence on Research and Development in h~formationRetrieval, pages 68-73.H.
P. Lutm.
1958.
The automatic creation of litera-ture abstracts.
IBM journal, 2(1):159-165.I.
Mani and E. Bloedorn.
1997.
Multi-documentsummarization bygraph search and matching.
InProc.
o/the 15th National Conference on Artifi-cial h~telligence , pages 622-628.I.
Mani, T. Firmin, and B. Sundheim.
1999.
TheTIPSTER SUMMAC text summarization evalu-ation.
In Proc.
o/Ninth Conference o/the Eu-ropean Chapter o/the Association/or Computa-tional Linguistics, pages 77-85.D.
Marcu.
1997.
From discourse structures to textsummaries.
In Proc.
of A CI, Workshop on Intel-ligent Scalable Text Summarization, pages 82-88.K.
R. McKeown and D. R. Radev.
1995.
Generatingsummaries of multiple news articles.
In Proc.
ofthe 18th Annual h~ternational A CM SIGIR Con-ference on Research and Development in Informa-tion Retrieval, pages 74-82.R.
Papka, J. Allan, and V. Lavrenko.
1999.
UMASSapproaches todetection and tracking at TDT2.
InProc.
of the DARPA Broadcast News Workshop.D.
R. Radev and K. R. McKeown.
1998.
Gen-erating natural anguage summaries from multi-pie on-line sources.
Computational Linguistics.24(3):469-500.G.
Salton, J. Allan, C. Buckle); and A. Singhal.1991.
Automatic aaaalysis, theme generation, andsummarization of machine-readable texts.
Sci-ence, 164:1421-1426.K.
J. Sparck.
1993.
What might be in a summary?In Proc.
of h~forraation Retrieval98, pages 9-26.G.
C. Stein, T. Strzalkowski, aald G. B.
Wise.
1999.Summarizing multiple documents using text ex-traction and interactive clustering.
In Proc.
of.the Pacific Association for Computational Lin-guistics1999, pages 200-208.T.
Stralkowsik, G. C. Stein, aaad G. B.
Wise.
1998.A text-extractlon based summarizer.
In Proc.
ofTipster Workshop.T.
Stralkowski.. G. C. Stein; and G. B.
Wise.
1999.Getracker: A robust, lightweight topic trackingsystem.
In Proe.
o/the DARPA Broadcast NewsWorkshop.S.
Teufell and M. Moens.
1997.
Sentence xtractionas a classification task.
In Proc.
of ACL Workshopon h~telligent Scalable Text Summarization, pages58-65.K.
Zechner.
1996.
Fast generation ofabstracts fromgeneral domain text corpora by extracting rele-vant sentences.
In Proc.
of the 16th InternationalGonference on Gomputational Lin9uistics, pages986-989.!
39
