Correct ing Ob ject -Re la ted  Misconcept ions:How Shou ld  The  System Respond?
tKathleen F. McCoyDepartment of Computer & Inft~rmation ScienceUniversity of PennsylvaniaPhiladelphia, PA 19104AbstractTills paper describes a computational method for correctingusers' miseonceptioas concerning the objects modelled by acompute," s.ystem.
The method involves classifying object-relatedmisc,mce|,tions according to the knowledge-base f ature involvedin the incorrect information.
For each resulting class sub-types areidentified, :.
:cording to the structure of the knowledge base, whichindicate wh:LI i .
formativn may be supporting the misconceptionand therefore what information to include in the response.
Such acharacteriza*i,,n, along with a model of what the user knows,enables the syst.cm to reas,m in a domain-independent way abouthow best to c~rrv,'t \[he user.1.
I n t roduct ionA meier ar,.a of Al research as been the development of"expert sys.tcms" - systems which are able to answer user'sque:~titms concerning a particular domain.
Studies identifyingdesirabl,, iutora,'tive capabilities for such systems \[Pollack et al 82\]have ft,und that.
it is not sufficient simply to allow the user to ,~ka question and Itavo the system answ~.r it.
Users often want toquestion the system's rea-~oning,to make sure certain constraintshave been taken into consideration, anti so on.
Thus we muststrive to provide expert systems with the ability to interact withthe user in the kind of cooperative di:LIogues that we see betweentwo bullish ctmversational partners.Allowing .,uch interactions between the system and a userraises difficulties for a Natural-Language system.
Since the user isinteracting with a system a.s s/he would with a human export, s/hewill nizam likely exp-ct the system to b(have as a human expert.Among other things, the n:.er will expect the systenl to be adheringto the cooperative principles of conversation \[Grice 7,5, .loshi 821.If these principte~ are not followed by the system, the user is bkeiyto become confu~ed.In this paper I.focus on one a,;pect of the cooperativebehavior found between two conversat, ional partners: responding torecognized ifferences in the beliefs of the two participants.
Oftenwhen two people interact, ouc reveals-a belief or assumption thatis incompatible with the b~*liefs held by the other.
Failure tocorrect this disparity may not only implicitly confirm the disparatebcli,'f, but may even make it impos~;ibie to complete tile ongoingtask.
Imagine the following excilange:U.
Give ll|e the ItUI.L NO of all Destroyers whoseMAST_ I IE IG I IT  is above 190.E.
All Destrt,yers that I know al)out |lave aMAbT_HEIG l lT  between 85 and 90.
Were youthinking of the Aircraft-Carriers?in this example, the user (U) ha.s apparently ctmfused a Destroyerwith an Aircraft-Carrier.
This confusion has caused her toattr ibute a property value to Destroyers that they do not have.
Inthis case a correct a/tswer by the expert (E} of *none" is likely toconfuse U'.
In order to continue the conver.-ation with a minimalamount of eoafu.~ion, the user's incorrect belief must first beaddressed.My primary interest is in what an expert system, aspiring tohuman expert performance, should include in such responses.
Inparticular, \[ am concerned with system responses to te~'ognizeddisparate beliefs/assumptions about cbflct.~.
In the past thisproblem has been h, ft to the tutoring or CAI systems \[Stevens etaL 79, Steven~ & ('ollins 80, Brown g:: Burton 78, Sleeman 82\],which attetupt o correct student's misconceptions concerning aparticular domain.
For the most part, their approach ha.~ been tolist a priori :dl mi.-conceptions in a given domain.
Tile futility t,fthis appr,~ach is empha'.,ized in \[gleeman ,~2\].
In contrast,theapproach taken hvre i~ to ,-la:,~iry.
in a dolttnin independent way,obj,'ct-related di.-pariti,~s ;u:c,~rding to the l'~n.wh'dge ~:tse (l(.I~)feature involved.
A nund)er of respon:~e strategies :ire associatedwith each resulting cla,~.
Deciding which strategy to use for agiven miseoncepti,m will be determined by analyzing a user modeland the discourse situation.2.
What  Goes Into a Correct ion?In this work I am making thc btllowing assunlptions:?
\]:or th*, purposes .
f  the initial correct.ion attempt, thesystem is a~umed to have complet,, attd corr~'ctknowledge of the domain.
Th:tt is.
the system willinitiMly perceive a disparity as a mise.neel,tion on thepart of the u~er It wi l l thus attempt o bring tileuser's beli~,fs into line with its own.?
The system's KB i~tclude-: the following fo:t~trce: anobject taxonomy, knowledge of object attributes andtheir possible values, and intornlation about I)O.~iblerelationships between ol)jects.?
Tile user's KB contains imilar features, llowev,'r,mneh of the information (content} in the system's !
'(Bmay he mb-.~ing from the u~or '~ b~ll \[e.g., the us+,r's l'(\[~may I)e ~parser ot coarser than the system's I(B, c,rvarious attributes (,~f c~:nccpts ma~ t;e missi:~g frets theu~,'r's I'(P,}.
In additi~m.
~.me inf,~rmation ia the u.,er'sKB may be wrong, in tiffs work, to say that the user'sKB is u'rong means that it is i.,:'m.
:i.~terJ with the,~g.,t,m) KB  (e.g., things may be c!a.
'~ified ifferently,properties attr ibuted ifferently, and ~'o on).IThiz v, ork is p~rtiMb" supported by the NSF gr~nt #MC~81-07200.444?
Whiw the system t~\]ay n,,t km,w e:;actly what isc(m~ained in the user's l,~b', information about the user?-:tt~ b, ~ d(,riv,'d hum two smtrcrs.
First, the .~ystem canhave ,q tm.h,I of a canoni,:at u,mr.
(Of course thism.,h,\[ m:ty turn o .
t  t,, differ from any given user'smodel.)
~.~,,,'~,ndl)', it ,'an deriw" knowledge about what?
the user kn .ws fr ,nt the ongoing dise.urse.
This l:tt,.rtype of km)~h'dge op,~titutes what the' system discer~sto bt, tits, mutua l  h(.liv.
:s of the system attd user asdefin.d iu \[.h,.hi 82\].
"\['he.-,e t~s,~ s,)ur(',~,s .
f  informati,mtogether '.n~t it ul c the s)stem's model of the user'sKB.
ThN h,,,:t.I itself may be incompi,,te arid/orine,,rrect witlt respect tt, t\]te system's KB.A tt,-'r'~; utterance refh.cls .
i ther the state of his/herKIL - r  ~,,m,) re:~s..i~,g s/he ha~ just done t() fill insome mi.,sing p:;rt of ~.h:,t K,q, or both.
( ; lynn Ilu,~e a~suinptit,ns, we earl consider what  shouhl I)ehtch~d,:d in a rcsp.nse to an object-r, 'htt, 'd disparity.
I f  a personexhiltit~ wh.at h i - /her  conv~ r.-.ationa\] partn~,r perceives as aInisconcellti,,n, I IH' vory least one w~mld expect from that  partneris to deny t | .
.
fal.e inf .
rmat ion ~- for example -U. I th .ugh|  a whale wa~ a fish.g.
It's n .
t .'
l 'ranscript~ of "u:d ura\[ly ~wcurring" expert systems how thatexperts often include more informati,m in their response than asiHIpl,' d,'nial.
Tit(.
,'xp~,rt Inn)' provide all alterhative truest:~tem~.nt (e.g., "\Vha;,.~ :,re marnnt:d'; ').
S/he may offerju~.t i fb'at ion andb, r  supp.
r t  for the rt~rr,wtion (e.g., ?VChales arent:~mln:~l~ J)r,('au~*" t il%V hen:/the through hmgs and h'ed their youngwith milk.'}.
S/he nmy als.
refute the faulty reasoning s/hetho~tght the ns~r had d .ne tt, ~,rrive at the misconception (e.g.," l laving fins and li~ ing in the water is not enough to make a whalea f i sh . '}
.
This behavior can be characterized a.s confirming thecorr4.et inh,rmation which mc\]y have h'd the user to the wrongconclusion, but indi(:ating w.hy the false conclusion does no!
followby bringing in a:lditional, overriding information, sThe ltroblem f,,r a computer sy,-tem is to decide what kind ~?ihformu!itm re:C,' I,e supporting a given misconception.
Whatthings m::y he relevant?
What faulty reasoning may have beendone?1 char:~cterize -bject-relatcd misconeeptious in terms of theK l l  f l ,tturt inwJved.
Misclgssifying an object, ?1 thought a whalewas a f ish',  i .wAw.s the SUlwrordinate KB feature.
Giving anobject a pr -p .
r ty  it doe~ not have, "~Vhat is the interest rate onthis st, ,ck?'
,  lovely,.
: the, attri ltu:e KB feature.
Thischatact?~ri~:di-n i. helpful in d,-termining, in terms of the structureof a K\[L what htform;\]tion may be supporting a part icularmis,'onr,'ption.
Thus, it is helpful in determining what to includein the r-..'ponso.2Throtlghout this work I am as.-~tmlng that tht miseone*ption if impttrt~ntto the tlk~k at hand and should therefore be corrected.
The re.q,~ases I amintcrest(,d in ?eneraVing at( the "full blown" resl, Ot;~es.
if ?
mlsecneeption isdet~,c\]rd which N n,al ilnl,or\].t.
!~t to he task at hand.
it is conceivable thateith,:r th,.
l i l l Sc ' ) l l , ' o l l t iOB tl~ ignored or a It, rlrtlllled I vPr?~on of o/\]e t;\[' thoser,,~l,Oll..,.$ |In givPii.5'l'h~.
:~r~l, ~;b' exhH.ih,.I hy *i~, '..:,r;.,u ..xp,tt~ is v,,cy Anfilar to the "grainof truth" rorr~.,'tion f,~.nd ic tu~erit~g si\]uations a~ i,t, I,';fied in tWo.If &Mcl),*.ald ~3 I.
"FhN .
'trat,'gy first id,.nGSes th,, grai~; t,( truth i\[~ a student'sansw~.r x l ld  lip-it go~.
'< Oil to  give t i t -  eorr?, t  I ;,n,~or.In the foi l .wing sections l will discuss the two classes ofobject trii~.conreptions just mentioned: superordinatemisconceptions and attr ibute misconceptions.
Examples of theseclasses :d.ng with correction strategies will be given.
In addition,indications of how a system might choose a part icular strategy willbe investigated.3.
Superordinate MisconceptionsSi.,.e the information ttmt human experts include in theirrespon~l.
Co a ga l .
, r .
rd inate misc.ncepti,m seems to hinge on theexl.
.r l 's l,ere~.ption <,f ~ tiw misconception occurred or whatinformati(,n may h:tve bt.cn supporting the misconception, I havesub-cat,'g,,rized s,qwrordinate misconct, ptions according to the kindof support they hate.
F .
r  each type (~ub-category) ofsup,,r(udinat(, mis,.
(,m:,,iJtion, 1 have identified information thal.would I."
relevant u, the correction.In this analysis t,f supf.rordinate misconceptieus, I amassulning that the user's knowledge al)out the snperordinateconcept is correct.
The user therefore arrives at the misconceptionbecause of his/her incomplete understanding of the object.
1 amalso, for I he moment,  ignoring misconceptions that  occur becausetwo objects have similar names.Given these restrictions, 1found three major  correctionstrategies used by human experts.
These correspond to threereasons why u user might misclassify an object:TYPE ONE - Object Shares Many Properties with PositedSupe~ordinate - This may cause the user wrongly to c .nc lude thatthese shared attr ibutes are inherited from the superordinate.
Thistype of misconc,.ption is i l lustrated by an example involving astudent and a teacher: 4U.
\] thoughl a whale w.~s a fish.E.
No, it's a mammal .
Ahhou~h it has fins and li~e~ in thewater, it's a mamntal  s~nce it is warm blooded andfeeds its young with milk.Nc, tice the expert not only specif i~ the correct s0perordinate, butalso gives additional inf.rn=ati,~n tt, justify the c~)rre, :i,~n.
Shedo~.s this by acknowledging that there are some pr6per~ies thatwhales .d/are with fish which m:O' lead the student to conclude th8%a whah: is a fish.
At the same time she indicates that thesepc.pectins are not sufficient, h,r inclusion in the cla.~s of fish.
Thewhale, in fact, lia.s other properties which define it to be amamm:d.Thus, the strategy the expert uses when s/he perceives themisc,,J,ct,ption tu be of TYPE ONE may be characterized as: (I)l )e ,y the posited superordinate and iudk:ate the correct one, (2)State at tributes (prol>,'rties) that  the obj+ct has in common withthe posited super<~rdin:tte, (at State defining attr ibutes of the realsuper-r<thmte, thus giviug evidence/justif ication for the correctch,~+:ifi,'~ti.n.
The sy, lem may hdlow this strategy when the usermod~l indicates that the itser thinks the p++sited suFerordinate andthe .h i \ ]e l  are simih\]r bee:ruse they share man)' common properties{n,,t held by the real SUl~.rordinate).TYPE TWO - Objt,ct Shares Properties with Auother Objectwhich is a Member of Pos:ited Superordinate - In this c:rse thelAhho,Jgh the analysis given hero wa~ d~:rived through ,t~,lying xr~uLIhuman interactions, the exarapDs given ire simply illustrative and have notbeen extrs,,-t~d frorn a real interaetiJn.445misclassified object and the "other object" are similar because theyhave some other common superordinate.
The properties that  theyshare arc no_..~t those inherited from the posited superordinate; butthose inherited from this other common superordhlate.
Figure3-1 shows a representation of this situation.
OBJECT andOTIIEIi-LIBJEC'E have many common properties because theyslt:.t.re a CtHltllton superordinate (COMMON-St !I'E|2OI2DINATE).Hence.
if the user knows that  OTI IE I1-OBJECT is a tnember ofthe POSrFED SUPEROl lDINATE,  ~/J|e inay wr~mgly concludethat OBJECT is also a member of POSITED :SUI>ERORD1NATE.F igure  3-1: TYPE TWO Superordinate Misconeeptio.For example, imagine the following exchange taking place i'ta junior high sch.
- I  bioh,gy ela_,~s (here U is a st ,d, .nt ,  E ateacher):U. I thought a tomato was a vegetable.E.
No it's a fruit.
You may think it's a vegetable sinceyou grow tomatoes in your vegetal',\]e garden :?h)ugwith the lettuce and green beans.
However.
it's a fruitbecause it's really the ripened ovary of a seed plant.Here it is intportant for the student o understand about plants.Thus, the teacher denies the posited superordinate, vegetable, andgives the corr,-ct one, fruit.
She backs this up by refuting evidencethat  the student may I)e using to support the misconception.
Inthis ca...e, the st l .h  nt may wrongly believe that  tomatoes arevegetables becau~.e lh~'y are like some other objects which arevegetables, lettuce and green beans, in that  all three share thecommon super.rdln:tte: I,l:mts grown in vegetable garden.
Theteacher acknowledges this similarity but refutes the conclusion thattomatoes are vegetables by giving the property of tomatoes whichdefine them to be fruits.The correction strategy used in this case was: (I) Deny thechk, csification posited by the user attd indicate the correctela:.,.ifieation.
(2) Cite the -t i ler memb~.rs of the positedsup*,rordinale that  the user may be either confusing with theobject being discu.
'.sed (Dr makhtg a b:td an:dogy from.
(,3) Give thefeatures which disling~Jl.h the correct and p~sited superordinatesthus justifying the classlfi(':ttion.
A system may f.l low lt.;sstrategy if a structure like that ht figure ;3-1 is f(~und in the usermodel.TYPE THREE - Wrong Information - The user either hasbeen told wrying informal i .n and h.'~ not done any rea;tming tojustify it, or has tttisclassified the object in response to somecotnpl*.x rea.soniug process that  the system can' t  duplicate.
In thiskind of situation, the system, just like a human expert, can onlyc .
r tect  the wrong information, give the corresponding trueinformation, at .
t  possibly give some defining featuresdistinguishing the posited and actual  superordiuates.
;f thiscnrrection does not satisfy the user.
it is up to h im/her  to continuethe interaction until the underlying misconception is ch.ared up(see \[.J'eff~rson 72\]).The iuformation included in this kind of response is similarto that  which McKeown's TEXT system, which answers questionsabout database structure \[McKeown 82 l, would include if the userhad asked about the diff~.rence between two entities.
In her case,the information included would depend on how' similar the twoobjects were according to the system KB, not on a model of whatthe user knows or why the user might be asking the question.
5U.
Is a debenture a secured bond?S.
No it's an unsecured bond - it has nothing backing itshould the issuing company default.ANDU.
Is the whiskey a missile?S.
No.
it's a submarine which is an underwater  vehicle(not a destructive device).The strategy folh;wed in these ca..,es can be characterized as:(1} Deny posited supev,rdinate and give correct one.
(2) Giveadditional iuformathm as lleeded.
Tills .xtra inform:ttion mayinclude defining features of the correct, superordinate orinformation ab .ut  the highest superordinate that  distinguishes theobject from the posited superordinate.
This strategy may befollowed by the system when there is insufficient evidence in theuser Ioodel for concI.Jding that  either a TYPE  ONE or a TYPETWO mlsconcepti(m has occurred.4.
Attribute MisconceptionsA second class of nlisconception occurs when a personwrongly attr ibutes a properly to an object.
There are at leastthree reasons wl v thi~, kind of ntisc~mception : ay occur.TYPE ()NE - Wren!.
; Object - The user is either confusingthe ob j ,c t  being discussed with :Hmther object that  has thespecified property,  or s /he is making a b~.t analogy using a similarobject.
In either c.'~e the second object should be included in thecorrefti.
:lu SO the problem does not f:,~ulinu?
*.\[u the foll,)wing example the ,'xpert assume.,~ the user isconfusiug the object with as imi lar  object.U.
I have my money in a money market  certif icate so Ican get to it r ight away.E.
But you can't!
Your money is tied up in a eertit'icate- do you mean a money market  fund?The strategy followed in this situation can be characterized~.s: ( l )Deny  the wrong information.
(2) (; ire the corresp.ml ingcorrect information.
(3) Mention the object of confusion or possibleanalogical reas.ning.
This s rategy can I)e followed by a .sy~tenlv.
'hPit there is at}other obj, 'ct which is "cio~e in con, eel = to Iheobject being discussed and zhi,:h ha.- the property involved in theinisconceptiou.
Or course, the perception of h(,w "cl(.
:~e incant'clot =two objects are chan'~.es with conte.\t.
This may bebecause some attr ibutes are highlighted in SOlile contexts andhidden in others.
};'or this reason it is anticipated that a el':sette'~s5McKeown do~* indl.-:~te that this kind of inf'~rm:,tlon wou\],i improve herre-ponsos.
Th- niaior Ihru:~t of her work was ,~n t,,:.i ..trlicture; the tie# of iuser model could hP eL.aily hltegrilil.d into her t'ri, m.w,-,rk.446measure such as that described in \[Tversky 77\], which takes intoaccount he salience of various attributes, will be useful.TYPE TWO - Wrong Attribute - The user has confused theattribute being discussed with another attribute.
In this case thecorrect attribute should be included in the response along withadditional information concerning the confused attributes (e.g.,their similarities and differences).
In the following example thesimilarity of the two attributes, in this case a common function, ismentioned in the response:U.
Where are the gills on the whale?S.
Whales don't have gills, they breathe through lungs.The strategy followed was: (1) Deny attribute given, (2) Givecorrect attrihutc, (3) Bring in similarities/differences of theattributes which may have led to the confusion.
A system mayfollow this strategy when a similar attribute can be found.There may be some difficulty in distinguishing between aTYPE ONE and a TYPE TWO attribute misconception.
In somesituations the user model alone will not be enough to distinguishthe two cases.
The use of past immediate focus (see \[Sidner 83\])looks to be promising in this case.
Heuristics are currently beingworked out for determining the most likely misconception typebased on what kinds of things {e.g., sets of attributes or objects)have been focused on in the recent past.TYPE THREE - The user w~s simply given bad informationor has done some complicated reasoning which can not beduplicated by the system.
Just as in the TYPE TI~IREEsuperordinate misconception, the system can only respond in alimited way.U.
1 am not working now and my husband has opened aspousal IRA for us.
1 understand that if 1 startworking again, and want to contribute to my own IRA,that we will have to pay a penalty on anything thathad been in our spousal account.E.
No - There is no penalty.
You can split that spousalone any way you wish?
You can have 2000 in each.Here the strategy is: (1) Deny attribute given, (2) Give correctattribute.
This strategy can be followed by the system when thereis not enough evidence in the user model to conclude that either aTYPE ONE or a TYPE TWO attribute misconception hasoccurred.5.
Conclusions?
In this paper I have argued that any Natural-Languagesystem that allows the user to engage in extended ialogues mustbe prepared to handle misconceptions.
Through studying varioustranscripts of how people correct misconceptions, I found that theynot only correct he wrong information, but often includeadditional information to convince the user of the correctionand/or refute the reasoning that may have led to themisconception.
This paper describes a framework for allowing acomputer system to mimic this behavior.The approach taken here is first to classify object-relatedmisconceptions according to the KB feature involved.
For eachresulting class, sub-types are identified in terms of the structure ofa KB rather than its content.
The sub-types characterize the kindof information that may support he misconception.
A correctionstrategy is associated with each sub-type that indicates what kindof information to include in the response.
Finally, algorithms arebeing developed for identifying the type of a particularmisconception based on a user model and a model of the discoursesituation.6.
AcknowledgementsI would like to thank Julia tlirschberg, Aravind Joshi,Martha Poll.~ck, and Bonnie Webber for their many helpfulcomments concerning this work.7.
References\[Brown & Burton 78\]Brown, J.S.
and Burton, R.R.
Diagnostic Modelsfor Procedural Bugs in B~ic Mathematical Skills.
CognitiveScience 2(2):155-192, 1978.\[Grice 75\] Grice, H. P. Logic and Conversation.
In P. Coleand J. L. Morgan (editor), Syntax and Semantics 111: SpeechActs, pages 41-58.
Academic Press, N.Y., 1975.\[Jefferson 721 Jefferson, G. Side Sequences.
In David Sudnow(editor), Studies in.
Social Interaction, .
Macmillan, New York,1972.\[Joshi 82\] Joshi, A. K. Mutual Beliefs in Question-AnswerSystems.
in N. Smith \[editor), Mutual Beliefs, .
Academic Press,N.Y., 1982.\[McKeown 82\] McKeown, K. .
Generating Natural LanguageText in Response to Questions About Database Structure.
PhDthesis, University of Pennsylvania, May, 1982.\[Pollack et al 82\]Pollack, M., Hirschberg, J., & Webber, B. UserParticipation in the Reasoning Processes of Expert Systems.
Int'?oceedings of the 198e National Conference on ArtificialIntelligence.
AAAI, Pittsburgh, Pa., August, 1982.\[Sidner 83\] Sidner, C. L. Focusing in the Comprehension fDefinite Anaphora.
In Michael Brady and Robert Berwick (editor),Computational lt4odcl8 of Discourac, pages 267-330.
MIT Press,Cambridge, Ma, 1983.\[Sleeman 82\] Sleeman, D. Inferring (Mal) Rules From Pupil'sProtocols.
In Proceedings of ECAI-8~, pages 160-164.
ECAI-82,Orsay, France, 1982.\[Stevens & Collins 80\]Stevens, A.L.
and Collins, A.
Multiple ConceptualModels of a Complex System.
In Richard E. Snow, Pat-AnthonyFedcrico and William E. Montague (editor), Aptitude, Learning,and Instruction, pages 177-197.
Erlbaum, Hillsdale, N.J., 1980.\[Stevens et al 79\]Stevens, A., Collins, A. and Goldin, S.E.Misconceptions in Student's Understanding.
Intl.
J. Alan-MachineStudic,s 11:145-156, 1979.\[Tversky 77\] Tversky, A.
Features of Similarity.
PsychologicalReview 84:327-352, 1977.\[Woolf & McDonald 83JWoolf, B. and McDonald, D. Human-ComputerDiscourse in the Design of a PASCAL Tutor.
In Ann Jandaleditor}, CItI'88 Conference Proceedings - Human Factors inComputing Systems, pages 230-234.
ACM SIGCHI/HFS, Boston,Ma., December, 1983.447
