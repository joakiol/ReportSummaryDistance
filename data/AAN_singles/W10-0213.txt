Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 107?115,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsExperiments on Summary-based Opinion ClassificationElena LloretDepartment of Softwareand Computing SystemsUniversity of AlicanteApdo.
de Correos 99E-03080, Alicante, Spainelloret@dlsi.ua.esHoracio SaggionDepartment of Infomation andCommunication TechnologiesGrupo TALNUniversitat Pompeu FabraC/Ta?nger, 122-134, 2nd floor08018 Barcelona, Spainhoracio.saggion@upf.eduManuel PalomarDepartment of Softwareand Computing SystemsUniversity of AlicanteApdo.
de Correos 99E-03080, Alicante, Spainmpalomar@dlsi.ua.esAbstractWe investigate the effect of text summarisa-tion in the problem of rating-inference ?
thetask of associating a fine-grained numericalrating to an opinionated document.
We set-upa comparison framework to study the effect ofdifferent summarisation algorithms of variouscompression rates in this task and compare theclassification accuracy of summaries and doc-uments for associating documents to classes.We make use of SVM algorithms to associatenumerical ratings to opinionated documents.The algorithms are informed by linguistic andsentiment-based features computed from fulldocuments and summaries.
Preliminary re-sults show that some types of summaries couldbe as effective or better as full documents inthis problem.1 IntroductionPublic opinion has a great impact on company andgovernment decision making.
In particular, compa-nies have to constantly monitor public perception oftheir products, services, and key company represen-tatives to ensure that good reputation is maintained.Recent cases of public figures making headlines forthe wrong reasons have shown how companies takeinto account public opinion to distance themselvesfrom figures which can damage their public image.The Web has become an important source for find-ing information, in the field of business intelligence,business analysts are turning their eyes to the Webin order to monitor public perception on products,services, policies, and managers.
The field of senti-ment analysis has recently emerged (Pang and Lee,2008) as an important area of research in NaturalLanguage Processing (NLP) which can provide vi-able solutions for monitoring public perception ona number of issues; with evaluation programs suchas the Text REtrieval Conference track on blog min-ing 1, the Text Analysis Conference 2 track on opin-ion summarisation, and the DEfi Fouille de Textesprogram (Grouin et al, 2009) advances in the stateof the art have been produced.
Although sentimentanalysis involves various different problems such asidentifying subjective sentences or identifying posi-tive and negative opinions in text, here we concen-trate on the opinion classification task; and morespecifically on rating-inference, the task of identify-ing the author?s evaluation of an entity with respectto an ordinal-scale based on the author?s textual eval-uation of the entity (Pang and Lee, 2005).
The spe-cific problem we study in this paper is that of as-sociating a fine-grained rating (1=worst,...5=best)to a review.
This is in general considered a dif-ficult problem because of the fuzziness inherent ofmid-range ratings (Mukras et al, 2007).
A consid-erable body of research has recently been producedto tackle this problem (Chakraborti et al, 2007; Fer-rari et al, 2009) and reported figures showing accu-racies ranging from 30% to 50% for such complextask; most approaches derive features for the classi-fication task from the full document.
In this researchwe ask whether extracting features from documentsummaries could help a classification system.
Sincetext summaries are meant to contain the essentialcontent of a document (Mani, 2001), we investigatewhether filtering noise through text summarisationis of any help in the rating-inference task.
In re-1http:trec.nist.gov/2http://www.nist.gov/tac/107cent years, text summarisation has been used to sup-port both manual and automatic tasks; in the SUM-MAC evaluation (Mani et al, 1998), text summarieswere tested in document classification and ques-tion answering tasks where summaries were consid-ered suitable surrogates for full documents; Baggaand Baldwin (1998) studied summarisation in thecontext of a cross-document coreference task andfound that summaries improved the performance ofa clustering-based coreference mechanism; more re-cently Latif and McGee (2009) have proposed textsummarisation as a preprocessing step for studentessay assessment finding that summaries could beused instead of full essays to group ?similar?
qual-ity essays.
Summarisation has been studied in thefield of sentiment analysis with the objective of pro-ducing opinion summaries, however, to the best ofour knowlegde there has been little research on thestudy of document summarisation as a text pro-cessing step for opinion classification.
This paperpresents a framework and extensive experiments ontext summarisation for opinion classification, and inparticular, for the rating-inference problem.
We willpresent results indicating that some types of sum-maries could be as effective or better than the fulldocuments in this task.The remainder of the paper is organised as fol-lows: Section 2 will compile the existing work withrespect to the inference-rating problem; Section 3and Section 4 will describe the corpus and the NLPtools used for all the experimental set-up.
Next, thetext summarisation approaches will be described inSection 5, and then Section 6 will show the exper-iments conducted and the results obtained togetherwith a discussion.
Finally, we will draw some con-clusions and address further work in Section 7.2 Related WorkMost of the literature regarding sentiment analysisaddresses the problem either by detecting and clas-sifying opinions at a sentence level (Wilson et al,2005; Du and Tan, 2009), or by attempting to cap-ture the overall sentiment of a document (McDonaldet al, 2007; Hu et al, 2008).
Traditional approachestackle the task as binary classification, where textunits (e.g.
words, sentences, fragments) are classi-fied into positive vs. negative, or subjective vs. ob-jective, according to their polarity and subjectivitydegree, respectively.
However, sentiment classifica-tion taking into account a finer granularity has beenless considered.
Rating-inference is a particular taskwithin sentiment analysis, which aims at inferringthe author?s numerical rating for a review.
For in-stance, given a review and 5-star-rating scale (rang-ing from 1 -the worst- to 5 -the best), this task shouldcorrectly predict the review?s rating, based on thelanguage and sentiment expressed in its content.In (Pang and Lee, 2005), the rating-inferenceproblem is analysed for the movies domain.
Inparticular, the utility of employing label and itemsimilarity is shown by analysing the performanceof three different methods based on SVM (one vs.all, regression and metric labeling), in order to inferthe author?s implied numerical rating, which rangesfrom 1 up to 4 stars, depending on the degree the au-thor of the review liked or not the film.
The approachdescribed in (Leung et al, 2006) suggests the use ofcollaborative filtering algorithms together with sen-timent analysis techniques to obtain user preferencesexpressed in textual reviews, focusing also on moviereviews.
Once the opinion words from user reviewshave been identified, the polarity of those opinionwords together with their strength need to be com-puted and mapped to the rating scales to be furtherinput to the collaborative input algorithms.Apart from these approaches, this problem isstated from a different point of view in (Shimadaand Endo, 2008).
Here it is approached from theperspective of rating different details of a productunder the same review.
Consequently, they renamethe problem as ?seeing several stars?
instead of onlyone, corresponding to the overall sentiment of thereview.
Also, in (Baccianella et al, 2009) the ratingof different features regarding hotel reviews (cleanli-ness, location, staff, etc.)
is addressed by analysingseveral aspects involved in the generation of prod-uct review?s representations, such as part-of-speechand lexicons.
Other approaches (Devitt and Ahmad,2007), (Turney, 2002) face this problem by group-ing documents with closer stars under the same cat-egory, i.e.
positive or negative, simplifying the taskinto a binary classification problem.Recently, due to the vast amount of on-line infor-mation and the subjectivity appearing in documents,the combination of sentiment analysis and summari-108sation task in tandem can result in great benefitsfor stand-alone applications of sentiment analysis,as well as for the potential uses of sentiment analy-sis as part of other NLP applications (Stoyanov andCardie, 2006).
Whilst there is much literature com-bining sentiment analysis and text summarisationfocusing on generating opinion-oriented summariesfor the new textual genres, such as blogs (Lloretet al, 2009), or reviews (Zhuang et al, 2006), theuse of summaries as substitutes of full documents intasks such as rating-inference has been not yet ex-plored to the best of our knowledge.
In contrast tothe existing literature, this paper uses summaries in-stead of full reviews to tackle the rating-inferencetask in the financial domain, and we carry out a pre-liminary analysis concerning the potential benefitsof text summaries for this task.3 Dataset for the Rating-inference TaskSince there is no standard dataset for carrying outthe rating-inference task, the corpus used for our ex-periments was one associated to a current project onbusiness intelligence we are working on.
These dataconsisted of 89 reviews of several English banks(Abbey, Barcalys, Halifax, HSBC, Lloyds TSB, andNational Westminster) gathered from the Internet.
Inparticular the documents were collected from Ciao3,a Website where users can write reviews about dif-ferent products and services, depending on their ownexperience.Table 1 lists some of the statistical properties ofthe data.
It is worth stressing upon the fact thatthe reviews have on average 2,603 words, whichmeans that we are dealing with long documentsrather than short ones, making the rating-inferencetask even more challenging.
The shortest documentcontains 1,491 words, whereas the longest documenthas more than 5,000 words.# Reviews Avg length Max length Min length89 2,603 5,730 1,491Table 1: Corpus StatisticsSince the aim of the task we are pursuing focuseson classifying correctly the star for a review (rang-ing from 1 to 5 stars), it is necessary to study how3http://www.ciao.co.uk/many reviews we have for each class, in order to seewhether we have a balanced distribution or not.
Ta-ble 2 shows this numbers for each star-rating.
It isworth mentioning that one-third of the reviews be-long to the 4-star class.
In contrast, we have only 9reviews that have been rated as 3-star, consisting ofthe 10% of the corpus, which is a very low number.Star-rating # reviews %1-star 17 192-star 11 123-star 9 104-star 28 325-star 24 27Table 2: Class Distribution4 Natural Language Processing ToolsLinguistic analysis of textual input is carried outusing the General Architecture for Text Engineer-ing (GATE) ?
a framework for the development anddeployment of language processing technology inlarge scale (Cunningham et al, 2002).
We make useof typical GATE components: tokenisation, parts ofspeech tagging, and morphological analysis to pro-duce document annotations.
From the annotationswe produce a number of features for document rep-resentation.
Features produced from the annotationsare: string ?
the original, unmodified text of eachtoken; root ?
the lemmatised, lower-case form ofthe token; category ?
the part-of-speech (POS) tag, asymbol that represents a grammatical category suchas determiner, present-tense verb, past-tense verb,singular noun, etc.
; orth ?
a code representing the to-ken?s combination of upper- and lower-case letters.In addition to these basic features, ?sentiment?
fea-tures based on a lexical resource are computed asexplained below.4.1 Sentiment FeaturesSentiWordNet (Esuli and Sebastiani, 2006) is a lexi-cal resource in which each synset (set of synonyms)of WordNet (Fellbaum, 1998) is associated withthree numerical scores obj (how objective the wordis), pos (how positive the word is), and neg (hownegative the word is).
Each of the scores rangesfrom 0 to 1, and their sum equals 1.
SentiWord-Net word values have been semi-automatically com-puted based on the use of weakly supervised classi-109fication algorithms.
In this work we compute the?general sentiment?
of a word in the following way:given a word w we compute the number of times theword w is more positive than negative (positive >negative), the number of times is more negative thanpositive (positive < negative) and the total numberof entries of word w in SentiWordNet, therefore wecan consider the overall positivity or negativity aparticular word has in SentiWordNet.
We are in-terested in words that are generally ?positive?, gen-erally ?negative?
or generally ?neutral?
(not muchvariation between positive and negative).
For exam-ple a word such as ?good?
has many more entrieswhere the positive score is greater than the nega-tivity score while a word such as ?unhelpful?
hasmore negative occurrences than positive.
We use thisaggregated scores in our classification experiments.Note that we do not apply any word sense disam-biguation procedure here.4.2 Machine Learning ToolFor the experiments reported here, we adopt a Sup-port Vector Machine (SVM) learning paradigm notonly because it has recently been used with suc-cess in different tasks in natural language processing(Isozaki and Kazawa, 2002), but it has been shownparticularly suitable for text categorization (Kumarand Gopal, 2009) where the feature space is huge, asit is in our case.
We rely on the support vector ma-chines implementation distributed with the GATEsystem (Li et al, 2009) which hides from the userthe complexities of feature extraction and conver-sion from documents to the machine learning imple-mentation.
The tool has been applied with successto a number of datasets for opinion classification andrating-inference (Saggion and Funk, 2009).5 Text Summarisation ApproachIn this Section, three approaches for carrying out thesummarisation process are explained in detail.
First,a generic approach is taken as a basis, and then, it isadapted into a query-focused and a opinion-orientedapproach, respectively.5.1 Generic SummarisationA generic text summarisation approach is first takenas a core, in which three main stages can be distin-guished: i) document preprocessing; ii) relevancedetection; and ii) summary generation.
Since wework with Web documents, an initial preprocessingstep is essential to remove all unnecessary tags andnoisy information.
Therefore, in the first stage thebody of the review out of the whole Web page isautomatically delimitated by means of patterns, andonly this text is used as the input for the next sum-marisation stages.
Further on, a sentence relevancedetection process is carried out employing differentcombinations of various techniques.
In particular,the techniques employed are:Term frequency (tf ): this technique has beenwidely used in different summarisation approaches,showing the the most frequent words in a documentcontain relevant information and can be indicative ofthe document?s topic (Nenkova et al, 2006)Textual entailment (te): a te module (Ferra?ndezet al, 2007) is used to detect redundant informationin the document, by computing the entailment be-tween two consecutive sentences and discarding theentailed ones.
The identification of these entailmentrelations helps to avoid incorporating redundant in-formation in summaries.Code quantity principle (cqp): this is a linguis-tic principle which proves the existence of a propor-tional relation between how important the informa-tion is, and the number of coding elements it has(Givo?n, 1990).
In this approach we assume that sen-tences containing longer noun-phrases are more rel-evant.The aforementioned techniques are combinedtogether taking always into account the term-frequency, leading to different summarisation strate-gies (tf, te+tf, cqp+tf, te+cqp+tf ).
Finally, the re-sulting summary is produced by extracting the high-est scored sentences up to the desired length, accord-ing the techniques explained.5.2 Query-focused SummarisationThrough adapting the generic summarisation ap-proach into a query-focused one, we could benefitfrom obtaining more specific sentences with regardto the topic of the review.
As a preliminary work, weare going to assume that a review is about a bank,and as a consequence, the name of the bank is con-sidered to be the topic.
It is worth mentioning that aperson can refer to a specific bank in different ways.For example, in the case of ?The National Westmin-110ster Bank?, it can be referred to as ?National West-minster?
or ?NatWest?.
Such different denomina-tions were manually identified and they were usedto biased the content of the generated summaries,employing the same techniques of tf, te and the cqpcombined together.
One limitation of this approachis that we do not directly deal with the coreferenceproblem, so for example, sentences containing pro-nouns referring also to the bank, will not be takeninto consideration in the summarisation process.
Weare aware of this limitation and for future work itwould be necessary to run a coreference algorithmto identify all occurrences of a bank within a review.However, since the main goal of this paper is to carryout a preliminary analysis of the usefulness of sum-maries in contrast to whole reviews in the rating-inference problem, we did not take this problem intoaccount at this stage of the research.
In addition,when we do query-focused summarisation only werely on the SUMMA toolkit (Saggion, 2008) to pro-duce a query similarity value for each sentence in thereview which in turn is used to rank sentences for anextractive summary (qf ).
This similarity value is thecosine similarity between a sentence vector (termsand weights) and a query vector (terms and weigths)and where the query is the name of the entity beingreviewed (e.g.
National Westminster).5.3 Opinion-oriented SummarisationSince reviews are written by people who want toexpress their opinion and experience with regardto a bank, in this particular case, either generic orquery-focused summaries can miss including someimportant information concerning their sentimentsand feelings towards this particular entity.
There-fore, a sentiment classification system similar to theone used in (Balahur-Dobrescu et al, 2009) is usedtogether with the summarisation approach, in orderto generate opinion-oriented summaries.
First of all,the sentences containing opinions are identified, as-signing each of them a polarity (positive and neg-ative) and a numerical value corresponding to thepolarity strength (the higher the negative score, themore negative the sentence and similarly, the higherthe positive score, the more positive the sentence).Sentences containing a polarity value of 0 are con-sidered neutral and are not taken into account.
Oncethe sentences are classified into positives, negativesand neutrals, they are grouped together accordingto its type.
Further on, the same combination oftechniques as for previously explained summarisa-tion approaches are then used.Additionally, a summary containing only the mostpositive and negative sentences is also generated (wehave called this type of summaries sent) in order tocheck whether the polarity strength on its own couldbe a relevant feature for the summarisation process.6 Evaluation EnvironmentIn this Section we are going to describe in detail allthe experimental set-up.
Firstly, we will explain thecorpus we used together with some figures regard-ing some statistics computed.
Secondly, we will de-scribe in-depth all the experiments we ran and the re-sults obtained.
Finally, an extensive discussion willbe given in order to analyse all the results and drawsome conclusions.6.1 Experiments and ResultsThe main objective of the paper is to investigate theinfluence of summaries in contrast to full reviews forthe rating-inference problem.The purpose of the experiments is to analyse theperformance of the different suggested text sum-marisation approaches and compare them to the per-formance of the full review.
Therefore, the experi-ments conducted were the following: for each pro-posed summarisation approach, we experimentedwith five different types of compression rates forsummaries (ranging from 10% to 50%).
Apart fromthe full review, we dealt with 14 different sum-marisation approaches (4 for generic, 5 for query-focused and 5 for opinion-oriented summarisation),as well as 2 baselines (lead and final, taking the firstor the last sentences according to a specific compres-sion rate, respectively).
Each experiment consistedof predicting the correct star of a review, either withthe review as a whole or with one of the summari-sation approaches.
As we previously said in Sec-tion 4, for predicting the correct star-rating, we usedmachine learning techniques.
In particular, differ-ent features were used to train a SVM classifier with10-fold cross validation4 , using the whole review:4The classifier used was the one integrated within the GATEframework: http://gate.ac.uk/111the root of each word, its category, and the calcu-lated value employing the SentiWordNet lexicon, aswell as their combinations.
As a baseline for the fulldocument we took into account a totally uninformedapproach with respect to the class with higher num-ber of reviews, i.e.
considering all documents as ifthey were scored with 4 stars.
The different resultsaccording different features can be seen in Table 3.Feature F?=1baseline 0.300root 0.378category 0.367sentiWN 0.333root+category 0.356root+sentiWN 0.333category+sentiWN 0.389root+category+sentiWN 0.413Table 3: F-measure results using the full review for clas-sificationRegarding the features for training the summaries,it is worth mentioning that the best performing fea-ture when no sentiment-based features are taken intoaccount is the one using the root of the words.
Con-sequently, this feature was used to train the sum-maries.
Moreover, since the best results using thefull review were obtained using the combination ofthe all the features (root+category+sentiWN), wealso selected this combination to train the SVMclassifier with our summaries.
Conducting bothexperiments, we could analyse to what extent thesentiment-based feature benefit the classificationprocess.The results obtained are shown in Table 4 andTable 5, respectively.
These tables show the F-measure value obtained for the classification task,when features extracted from summaries are usedinstead from the full review.
On the one hand,results using the root feature extracted from sum-maries can be seen in Table 4.
On the other hand,Table 5 shows the results when the combinationof all the linguistic and sentiment-based features(root+category+sentiWN), that has been extractedfrom summaries, are used for training the SVM clas-sifier.We also performed two statistical tests in orderto measure the significance for the results obtained.The tests we performed were the one-way Analy-sis of Variance (ANOVA) and the t-test (Spiegel andCastellan, 1998).
Given a group of experiments, wefirst run ANOVA for analysing the difference be-tween their means.
In case some differences arefound, we run the t-test between those pairs.6.2 DiscussionA first analysis derived from the results obtained inTable 3 makes us be aware of the difficulty associ-ated to the rating-inference task.
As can be seen,a baseline without any information from the docu-ment at all, is performing around 30%, which com-pared to the remaining approaches is not a very badnumber.
However, we assumed that dealing withsome information contained in documents, the clas-sification algorithm will do better in finding the cor-rect star associated to a review.
This was the rea-son why we experimented with different featuresalone or in combination.
From these experiments,we obtained that the combination of linguistic andsemantic-based features leads to the best results, ob-taining a F-measure value of 41%.
If sentiment-based features are not taken into account, the bestfeature is the root of the word on its own.
Further-more, in order to analyse further combinations, weran some experiments with bigrams.
However, theresults obtained did not improve the ones we alreadyhad, so they are not reported in this paper.As far as the results is concerned comparing theuse of summaries to the full document, it is worthmentioning that when using specific summarisationapproaches, such as query-focused summaries com-bined with term-frequency, we get better results thanusing the full document with a 90% confidence in-terval, according to a t-test.
In particular, qf for 10%is significant with respect to the full document, us-ing only root as feature for training.
For the resultsregarding the combination of root, category and Sen-tiWordNet, qf for 10% and qf+tf for 10% and 20%are significant with respect to the full document.Concerning the different summarisation ap-proaches, it cannot be claimed a general tendencyabout which ones may lead to the best results.
Wealso performed some significance tests between dif-ferent strategies, and in most of the cases, the t-test and the ANOVA did not report significanceover 95%.
Only a few approaches were significantat a 95% confidence level, for instance, te+cqp+tfand sent+te+cqp+tf with respect to sent+cqp+tf112Approach Compression RateSummarisation method 10% 20% 30% 40% 50%lead F?=1 0.411 0.378 0.367 0.311 0.322final F?=1 0.322 0.389 0.300 0.467 0.456tf F?=1 0.400 0.344 0.400 0.367 0.367te+tf F?=1 0.367 0.422 0.411 0.389 0.322cqp+tf F?=1 0.300 0.344 0.311 0.300 0.256te+cqp+tf F?=1 0.422 0.356 0.333 0.300 0.322qf F?=1 0.513 0.388 0.375 0.363 0.363qf+tf F?=1 0.567 0.467 0.311 0.367 0.389qf+te+tf F?=1 0.389 0.367 0.411 0.378 0.333qf+cqp+tf F?=1 0.300 0.356 0.378 0.378 0.333qf+te+cqp+tf F?=1 0.322 0.322 0.367 0.367 0.356sent F?=1 0.344 0.380 0.391 0.290 0.336sent+tf F?=1 0.378 0.425 0.446 0.303 0.337sent+te+tf F?=1 0.278 0.424 0.313 0.369 0.347sent+cqp+tf F?=1 0.333 0.300 0.358 0.358 0.324sent+te+cqp+tf F?=1 0.446 0.334 0.358 0.292 0.369Table 4: Classification results (F-measure) for summaries using root (lead = first sentences; final = last sentences;tf = term frequency; te = textual entailment; cqp = code quantity principle with noun-phrases; qf = query-focusedsummaries; and sent = opinion-oriented summaries)for 10%; sent+tf in comparison to sent+cqp+tffor 20%; or sent with respect to cqp+tf for 40%and 50% compression rates.
Other examples ofthe approaches that were significant at a 90%level of confidence are qf for 10% with respect tosent+te+cqp+tf.
Due to the wide range of summari-sation strategies tested in the experiments, the resultsobtained vary a lot and, due to the space limitations,it is not possible to report all the tables.
What itseems to be clear from the results is that the codequantity principle (see Section 5) is not contributingmuch to the summarisation process, thus obtainingpoor results when it is employed.
Intuitively, thiscan be due to the fact that after the first mention ofthe bank, there is a predominant use of pronouns,and as a consequence, the accuracy of the tool thatidentifies noun-phrases could be affected.
The samereason could be affecting the term-frequency calcu-lus, as it is computed based on the lemmas of thewords, not taking into account the pronouns that re-fer also to them.7 Conclusion and Future WorkThis paper presented a preliminary study ofinference-rating task.
We have proposed here a newframework for comparison and extrinsic evaluationof summaries in a text-based classification task.
Inour research, text summaries generated using differ-ent strategies were used for training a SVM classifierinstead of full reviews.
The aim of this task was tocorrectly predict the category of a review within a 1to 5 star-scale.
For the experiments, we gathered 89bank reviews from the Internet and we generated 16summaries of 5 different compression rates for eachof them (80 different summaries for each review,having generated in total 7,120 summaries).
We alsoexperimented with several linguistic and sentiment-based features for the classifier.
Although the re-sults obtained are not significant enough to statethat summaries really help the rating-inference task,we have shown that in some cases the use of sum-maries (e.g.
query/entity-focused summaries) couldoffer competitive advantage over the use of full doc-uments and we have also shown that some summari-sation techniques do not degrade the performance ofa rating-inference algorithm when compared to theuse of full documents.
We strongly believe that thispreliminary study could serve as a starting point forfuture developments.Although we have carried out extensive experi-mentation with different summarisation techniques,compression rates, and document/summary features,there are many issues that we have not explored.
Inthe future, we plan to investigate whether the re-sults could be affected by the class distribution ofthe reviews, and in this line we would like to see thedistribution of the documents using clustering tech-113Approach Compression RateSummarisation method 10% 20% 30% 40% 50%lead F?=1 0.275 0.422 0.422 0.378 0.322final F?=1 0.275 0.378 0.333 0.344 0.400tf F?=1 0.411 0.422 0.411 0.378 0.378te+tf F?=1 0.411 0.344 0.344 0.344 0.378cqp+tf F?=1 0.358 0.267 0.333 0.222 0.289te+cqp+tf F?=1 0.444 0.411 0.411 0.311 0.322qf F?=1 0.563 0.488 0.400 0.375 0.350qf+tf F?=1 0.444 0.411 0.433 0.367 0.356qf+te+tf F?=1 0.322 0.367 0.356 0.344 0.344qf+cqp+tf F?=1 0.292 0.322 0.367 0.333 0.356qf+te+cqp+tf F?=1 0.356 0.378 0.356 0.367 0.356sent F?=1 0.322 0.370 0.379 0.412 0.414sent+tf F?=1 0.378 0.446 0.359 0.380 0.402sent+te+tf F?=1 0.333 0.414 0.404 0.380 0.381sent+cqp+tf F?=1 0.300 0.333 0.347 0.358 0.296sent+te+cqp+tf F?=1 0.436 0.413 0.425 0.359 0.324Table 5: Classification results (F-measure) for summaries using root, category and SentiWordNet (lead = first sen-tences; final = last sentences; tf = term frequency; te = textual entailment; cqp = code quantity principle withnoun-phrases; qf = query-focused summaries; and sent = opinion-oriented summaries)niques.
Moreover, we would also like to investigatewhat it would happen if we consider the values of thestar-rating scale as ordinal numbers, and not only aslabels for categories.
We will replicate the exper-iments presented here using as evaluation measurethe ?mean square error?
which has been pinpointedas a more appropriate measure for categorisation inan ordinal scale.
Finally, in the medium to long-term we plan to extent the experiments and analy-sis to other available datasets in different domains,such as movie or book reviews, in order to see ifthe results could be influenced by the nature of thecorpus, allowing also further results for comparisonwith other approaches and assessing the difficulty ofthe task from a perspective of different domains.AcknowledgmentsThis research has been supported by the project PROM-ETEO ?Desarrollo de Te?cnicas Inteligentes e Interacti-vas de Miner?
?a de Textos?
(2009/119) from the ValencianGovernment.
Moreover, Elena Lloret is funded by theFPI program (BES-2007-16268) from the Spanish Min-istry of Science and Innovation under the project TEXT-MESS (TIN2006-15265-C06-01), and Horacio Saggionis supported by a Ramo?n y Cajal Fellowship from theMinistry of Science and Innovation, Spain.
The authorswould also like to thank Alexandra Balahur for helping toprocess the dataset with her Opinion Mining approach.ReferencesS.
Baccianella, A. Esuli, and F. Sebastiani.
2009.
Multi-facet Rating of Product Reviews.
In Proceedings ofthe 31th European Conference on IR Research on Ad-vances in Information Retrieval, pages 461?472.A.
Bagga and B. Baldwin.
1998.
Entity-Based Cross-Document Coreferencing Using the Vector SpaceModel.
In Proceedings of the COLING-ACL, pages79?85.A.
Balahur-Dobrescu, M. Kabadjov, J. Steinberger,R.
Steinberger, and A. Montoyo.
2009.
SummarizingOpinions in Blog Threads.
In Proceedings of the Pa-cific Asia Conference on Language, INformation andComputation Conference, pages 606?613.S.
Chakraborti, R. Mukras, R. Lothian, N. Wiratunga,S.
Watt, and D Harper.
2007.
Supervised Latent Se-mantic Indexing using Adaptive Sprinkling.
In Pro-ceedings of IJCAI-07, pages 1582?1587.H.
Cunningham, D. Maynard, K. Bontcheva, andV.
Tablan.
2002.
GATE: A Framework and Graphi-cal Development Environment for Robust NLP Toolsand Applications.
In Proceedings of the ACL.A.
Devitt and K. Ahmad.
2007.
Sentiment Polarity Iden-tification in Financial News: A Cohesion-based Ap-proach.
In Proceedings of the ACL, pages 984?991.W.
Du and S. Tan.
2009.
An Iterative ReinforcementApproach for Fine-Grained Opinion Mining.
In Pro-ceedings of the NAACL, pages 486?493.A.
Esuli and F. Sebastiani.
2006.
SENTIWORDNET: APublicly Available Lexical Resource for Opinion Min-ing.
In Proceedings of LREC, pages 417?422.114C.
Fellbaum.
1998.
WordNet: An Electronical LexicalDatabase.
The MIT Press, Cambridge, MA.O.
Ferra?ndez, D. Micol, R. Mun?oz, and M. Palomar.2007.
A Perspective-Based Approach for Solving Tex-tual Entailment Recognition.
In Proceedings of theACL-PASCAL Workshop on Textual Entailment andParaphrasing, pages 66?71, June.S.
Ferrari, T. Charnois, Y. Mathet, F. Rioult, andD.
Legallois.
2009.
Analyse de Discours ?Evaluatif,Mode`le Linguistique et Applications.
In Fouille dedonne?es d?opinion, volume E-17, pages 71?93.T.
Givo?n, 1990.
Syntax: A functional-typological intro-duction, II.
John Benjamins.C.
Grouin, M. Hurault-Plantet, P. Paroubek, and J. B.Berthelin.
2009.
DEFT?07 : Une Campagned?Avaluation en Fouille d?Opinion.
In Fouille dedonne?es d?opinion, volume E-17, pages 1?24.Y.
Hu, W. Li, and Q. Lu.
2008.
Developing Evalua-tion Model of Topical Term for Document-Level Sen-timent Classification.
In Proceedings of the 10th Pa-cific Rim International Conference on Artificial Intel-ligence, pages 175?186.H.
Isozaki and H. Kazawa.
2002.
Efficient SupportVector Classifiers for Named Entity Recognition.
InProceedings of the 19th International Conference onComputational Linguistics, pages 390?396.M.
A. Kumar and M. Gopal.
2009.
Text CategorizationUsing Fuzzy Proximal SVM and Distributional Clus-tering of Words.
In Proceedings of the 13th Pacific-Asia Conference on Advances in Knowledge Discoveryand Data Mining, pages 52?61.S.
Latif and M. McGee Wood.
2009.
A Novel Techniquefor Automated Linguistic Quality Assessment of Stu-dents?
Essays Using Automatic Summarizers.
Com-puter Science and Information Engineering, WorldCongress on, 5:144?148.C.
W. K. Leung, S. C. F. Chan, and F. L. Chung.2006.
Integrating Collaborative Filtering and Sen-timent Analysis: A Rating Inference Approach.
InProceedings of The ECAI 2006 Workshop on Recom-mender Systems, pages 62?66.Y.
Li, K. Bontcheva, and H. Cunningham.
2009.
Adapt-ing SVM for Data Sparseness and Imbalance: A CaseStudy in Information Extraction.
Natural LanguageEngineering, 15(2):241?271.E.
Lloret, A. Balahur, M. Palomar, and A. Montoyo.2009.
Towards Building a Competitive Opinion Sum-marization System: Challenges and Keys.
In Proceed-ings of the NAACL.
Student Research Workshop andDoctoral Consortium, pages 72?77.I.
Mani, D. House, G. Klein, L. Hirshman, L. Obrst,T.
Firmin, M. Chrzanowski, and B. Sundheim.
1998.The TIPSTER SUMMAC Text Summarization Evalu-ation.
Technical report, The Mitre Corporation.I.
Mani.
2001.
Automatic Text Summarization.
JohnBenjamins Publishing Company.R.
McDonald, K. Hannan, T. Neylon, M. Wells, andJ.
Reynar.
2007.
Structured Models for Fine-to-Coarse Sentiment Analysis.
In Proceedings of theACL, pages 432?439.R.
Mukras, N. Wiratunga, R. Lothian, S. Chakraborti, andD.
Harper.
2007.
Information Gain Feature Selectionfor Ordinal Text Classification using Probability Re-distribution.
In Proceedings of the Textlink workshopat IJCAI-07.A.
Nenkova, L. Vanderwende, and K. McKeown.
2006.A Compositional Context Sensitive Multi-documentSummarizer: Exploring the Factors that InfluenceSummarization.
In Proceedings of the ACM SIGIRconference on Research and development in informa-tion retrieval, pages 573?580.B.
Pang and L. Lee.
2005.
Seeing Stars: ExploitingClass Relationships for Sentiment Categorization withRespect to Rating Scales.
In Proceedings of the ACL,pages 115?124.B.
Pang and L. Lee.
2008.
Opinion Mining and Senti-ment Analysis.
Foundations and Trends in Informa-tion Retrieval, 2(1-2):1?135.H.
Saggion and A. Funk.
2009.
Extracting Opinions andFacts for Business Intelligence.
RNTI, E-17:119?146.H.
Saggion.
2008.
SUMMA: A Robust and Adapt-able Summarization Tool.
Traitement Automatiquedes Languages, 49:103?125.K.
Shimada and T. Endo.
2008.
Seeing Several Stars: ARating Inference Task for a Document Containing Sev-eral Evaluation Criteria.
In Proceedings of the 12thPacific-Asia Conference on Advances in KnowledgeDiscovery and Data Mining, pages 1006?1014.S.
Spiegel and N. J. Castellan, Jr. 1998.
NonparametricStatistics for the Behavioral Sciences.
McGraw-HillInternational.V.
Stoyanov and C. Cardie.
2006.
Toward Opinion Sum-marization: Linking the Sources.
In Proceedings ofthe Workshop on Sentiment and Subjectivity in Text,pages 9?14.P.
D. Turney.
2002.
Thumbs Up or Thumbs Down?
: Se-mantic Orientation Applied to Unsupervised Classifi-cation of Reviews.
In Proceedings of the ACL, pages417?424.T.
Wilson, J. Wiebe, and P. Hoffmann.
2005.
Recog-nizing Contextual Polarity in Phrase-level SentimentAnalysis.
In Proceedings of the EMNLP, pages 347?354.L.
Zhuang, F. Jing, and X. Y. Zhu.
2006.
Movie Re-view Mining and Summarization.
In Proceedings ofthe 15th ACM international conference on Informationand knowledge management, pages 43?50.115
