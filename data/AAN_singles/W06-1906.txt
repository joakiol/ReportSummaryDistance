BRUJA: Question Classification for Spanish.Using Machine Translation and an English Classifier.Miguel ?A.
Garc?
?a CumbrerasSINAI GroupComputer SciencesUniversity of Jae?n.
Spainmagc@ujaen.esL.
Alfonso Uren?a Lo?pezSINAI GroupComputer SciencesUniversity of Jae?n.
Spainlaurena@ujaen.esFernando Mart?
?nez SantiagoSINAI GroupComputer SciencesUniversity of Jae?n .Spaindofer@ujaen.esAbstractQuestion Classification is an importanttask in Question Answering Systems.
Thispaper presents a Spanish Question Classi-fier based on machine learning, automaticonline translators and different languagefeatures.
Our system works with Eng-lish collections and bilingual questions(English/Spanish).
We have tested twoSpanish-English online translators to iden-tify the lost of precision.
We have madeexperiments using lexical, syntactic andsemantic features to test which ones madea better performance.
The obtained resultsshow that our system makes good classifi-cations, over a 80% in terms of accuracyusing the original English questions andover a 65% using Spanish questions andmachine translation systems.
Our conclu-sion about the features is that a lexical,syntactic and semantic features combina-tion obtains the best result.1 IntroductionA Question Answering (QA) system seeks andshows the user an accurate and concise answer,given a free-form question, and using a large textdata collection.The use of Cross Language Information Re-trieval Systems (CLIR) is growing, and also theapplication of these ones into other general sys-tems, such as Question Answering or QuestionClassification.A CLIR system is an Information Retrieval Sys-tem that works with collections in several lan-guages, and extract relevant documents or pas-sages (Grefenstette, 1998).We have proposed a Multilingual Question An-swering System (BRUJA - in Spanish ?Busquedade Respuestas University of Jaen?)
that works withcollections in several languages.
Since there areseveral languages, tasks such as obtaining rele-vant documents and extracting the answer couldbe accomplished in two ways: using NPL toolsand resources for each language or for a pivot lan-guage only (English) and translating to the pivotlanguage the rest of the relevant information whenit is required.
Because of the translation step, thesecond approach is less accurate but more practi-cal since we need only NPL resources for English.The central question is the noise, because of thetranslation process, is too high in order to use thisapproach in spite of their practical advantages.The first step of this system is a Question Clas-sifier (QC).
Given a query, a question classifica-tion module obtains the class of such question.This information is useful for the extraction of theanswer.
For example, given the query ?Whereis Madrid?, the QA system expects a locationentity as answer type.
The proposed QA mod-ule works with questions in several languages,translates them into English using different onlinetranslators, and obtains the type of questions andsome features, such as the focus, the keywordsor the context.
In this work we aim to find outwhether a multilingual QC module is possible byusing translation tools and English as pivot lan-guage or not.2 Question ClassificationQuestion Classification is the task that, given aquestion, classifies it in one of k semantic classes.Some QC systems are based on regular expres-sions and manual grammatical rules (Van Durmeet al, 2003).EACL 2006 Workshop on Multilingual Question Answering - MLQA0639Recent works in QC have studied different ma-chine learning methods.
(Zhang and Lee, 2003)propose a QC system that uses Support Vec-tor Machine (SVM) as the best machine learn-ing algorithm.
They compare the obtained resultswith other algorithms, such as Nearest Neighbors,Naive Bayes, Decision Tree or Sparse Network ofWinnows (SNoW).
(Li and Roth, 2002) propose a system based onSNoW.
They used five main classes and fifty finedclasses.
Other systems have used SVM and modi-fied kernels.QC systems have some restrictions (Haciogluand Ward, 2003), such as:?
Traditional question classification uses a setof rules, for instance ?questions that startwith Who ask about a person?.
These aremanual rules that have to be revised to im-prove the results.?
These rules are very weak, because when newquestions arise, the system has to be updatedto classify them.Most of the QC systems use English as the mainlanguage, and some of the best and standard re-sources are developed for English.It would be possible to build a question classi-fier for every language based on machine learning,using a good training corpus for each language,but is something expensive to produce.
For thisreason we have used Machine Translation Sys-tems.Machine Translation (MT) systems are very ap-preciated in CLIR (McNamee et al, 2000).
Lastyears these systems have improved the results, butthere are not translators for each language pair andthe quality of the result depends on this pair.The reason of using MT and not a Spanish clas-sifier is simple: we have developed a multilingualQA system that works in this moment with threelanguages: English, Spanish and French.
Becauseit is too complex for us to work with resources intothese three languages and also to manage the in-formation into three languages, our kernel systemworks into English, and we use MT to translateinformation when it is necessary.We have developed a QC system that coversthree tasks:?
It uses machine learning algorithms.
We havetested methods based on Support Vector Ma-chine, for instance SVMLight or LibSVM,and TiMBL.
TiMBL 1 is a program thatimplements several Memory-Based Learningtechniques.
It stores a representation of thetraining set explicitly in memory, and classi-fies new cases by extrapolation from the mostsimilar stored cases.?
To classify Spanish questions we havechecked two online machine translators.
Ourproposal is to study how the translation canaffect in the final results, compared to theoriginal English results.?
Finally, we would obtain different results ap-plying different levels of features (lexical,syntactic and semantic).
In the next sectionwell explain them and in results chapter wewill see these differences.Our QC system has three independent modules,so it will be easy to replace each one with other toimprove the final results.
In Figure 1 we can seethem.Figure 1: QC system Modules.The first module translates the question intoother languages, Spanish in this case.
We haveused two machine translation systema that workwell for the language pair Spanish-English: Epalsand Prompt.
This module could work with othermachine translation systems and other languagesif there would be a good translator for the languagepair used.The second module extracts some relevant fea-tures (see next section) using the original or thetranslated English questions.
Some of these fea-tures will be used by the machine learning module(lexical, syntactic and semantic features) and the1ILK Research Group, Tilburg University and CNTS Re-search Group, University of AntwerpEACL 2006 Workshop on Multilingual Question Answering - MLQA0640others will be used later in the answers extractionphase.
Take into account that the second modulealso extracts important features such as the contextof the question, the focus or the keywords that wewould use in next steps of the Question Answeringsystem.The final module applies the machine learn-ing algorithm and returns the question categoryor class.
In our first experiments we used Li-brary for Support Vector Machines (LibSVM) andBayesian Logistic Regression (BBR), but for thisone we have used Tilburg Memory Based Learner(TiMBL).TiMBL (Daelemans et al, 2004) implementsseveral Memory-Based Learning techniques, clas-sic k-NN classification kernel and several metrics.It implements Stanfill modified value differencemetric (MVDM), Jeffrey Divergence and Classvoting in the k-NN kernel according to the dis-tance of the nearest neighbors.
It makes classifi-cation using heuristic approximations, such as theIGTREE decision tree algorithm and the TRIBLand TRIBL2 hybrids.
It also has optimizations forfast classification.2.1 Features in Question ClassificationWe have analyzed each question in order to extractthe following features:?
Lexical Features?
The two first words of the question?
All the words of the question in lower-case?
The stemming words?
Bigrams of the question?
Each word with its position in the ques-tion?
The interrogative pronoun of the ques-tion?
The headwords of the nouns and verbs?
Syntactic Features?
The interrogative pronoun and the Partof Speech (POS) of the rest of the words?
The headword (a word to which an in-dependent meaning can be assigned) ofthe first noun phrase?
All POS?
Chunks?
The first verb chunk?
The length of the question?
Semantic Features?
The question focus (a noun phrase thatis likely to be present in the answer)?
POS with the named entities recognized?
The type of the entity if the focus is oneof them?
Wordnet hypernyms for the nouns andWordnet synonyms for the verbsWe have used some English resources such asthe POS tagger TreeTagger (Schmid, 1994), Ling-pipe 2 to make Named Entity Recognition, and thePorter stemmer (Porter, 1980).
We have also usedWordnet to expand the queries.3 Experiments and Results3.1 Experimental MethodThe experiments are made using some publicdatasets available by USC (Hovy et al, 1999),UIUC and TREC 3 as training and test collections.These datasets have been labeled manually byUIUC group by means of the following generaland detailed categories:ABBR: abbreviation, expansion.DESC: definition, description, manner, reason.ENTY: animal, body, color, creation, cur-rency, disease/medical, event, food, instrument,language, letter, other, plant, product, religion,sport, substance, symbol, technique, term, vehicle,word.HUM: description, group, individual, title.LOC: city, country, mountain, other, state.NUM: code, count, date, distance, money, or-der, other, percent, period, speed, temperature,size, weight.For instance the question ?What does NATOmean??
is an ABBR (abbreviation) category,?What is a receptionist??
is a DESC (definition)category or ?When did George Bush born??
is aNUM (numeric) category.The training data are a set of 5500 questions andthe test data are a set of 500 questions.
All ques-tions were labelled for the 10th conference Cross-Language Evaluation Forum of Question Answer-ing (CLEF-QA).2LingPipe is a suite of Java tools designed to performlinguistic analysis on natural language data, available inhttp://www.alias-i.com/lingpipe3http://trec.nist.govEACL 2006 Workshop on Multilingual Question Answering - MLQA0641The same dataset has been used in other inves-tigations, such as in (Li and Roth, 2002).The distribution of these 5500 training ques-tions, with respect to its interrogative pronoun orthe initial word is showed in Table 1.Likewise, the distribution of categories of these5500 training questions is showed in Table 2.Table 1: Training questions distribution accordingwith its interrogative pronounType NumberWhat 3242Who 577How 764Where 273When 131Which 105Why 103Name 91In 67Define 4Whom 4Others 91Table 2: Training questions distribution accordingwith its general category.Category NumberABBR 86DESC 1162ENTY 1251HUM 1223LOC 835NUM 896The distribution of the 500 test questions, withrespect to its interrogative pronoun or the initialword, is showed in Table 3, and the distribution ofcategories of these 500 test questions is showed inTable 4.Table 3: Test questions distribution according withits interrogative pronoun.Type NumberWhat 343Who 47How 35Where 26When 26Which 6Why 4Name 2In 5Others 6In our experiments we try to identify the generalcategory.
Our proposal is to try a detailed classifi-cation later.Table 4: Test questions distribution according withits general category.Category NumberABBR 9DESC 138ENTY 94HUM 65LOC 81NUM 113We have used the Accuracy as a general mea-sure and the Precision of each category as a de-tailed measure.Accuracy = ]ofcorrectpredictions]ofpredictions(1)precision(c) = ]ofcorrectpredictionsofthecategoryc]ofpredictionsofthecategoryc(2)Other measure used is the F-score, definedas the harmonic mean of precision and recall(Van Rijsbergen, 1979).
It is a commonly usedmetric to summarize precision and recall in onemeasure.F ?
score = 2 ?
precision ?
recallprecision + recall(3)3.2 ResultsWe have made some experiments changing themachine translation systems:?
5500 training questions and 500 test ques-tions, all into English.
This is the basic case.?
5500 training questions into English and 500test questions translated from Spanish usingthe MT Epals.?
5500 training questions into English and 500test questions translated from Spanish usingthe MT Prompt.The MT resources are available in the followingURLs:?
Epalshttp://www.epals.com?
Prompthttp://translation2.paralink.comAccording to the lexical, syntactic and semanticfeatures we have made seven features sets.
Ourproposal here is to check which ones increase thefinal results.
These features sets are the following:EACL 2006 Workshop on Multilingual Question Answering - MLQA06421.
Lexical Features: interrogative pronoun(lex1)2.
Lexical and Syntactic Features: Two firstwords of the question + All the words of thequestion in lowercase + Stemming words +Headwords (lexsyn2)3.
Lexical and Syntactic Features: previous four+ Each word with its position in the ques-tion + interrogative pronoun + The first verbchunk (lexsyn3)4.
Semantic Features: The question focus +POS with the named entities recognized +The type of the entity if the focus is one ofthem (sem4)5.
Syntactic Features: The interrogative pro-noun and the Part of Speech (POS) of therest of the words + All POS + Chunks + Thelength of the question (sin5)6.
All Lexical + all Syntactic + all Semantic(lexsemsin6)7.
Lexical Features: Two first words of thequestion + interrogative pronoun ; SyntacticFeatures: + The headwords of the nouns andverbs + The first verb chunk + the interrog-ative pronoun + the Part of Speech (POS) ofthe rest of the words + The length of the ques-tion; Semantic Features: POS with the namedentities recognized (lexsemsin7)We can see in the Table 5 the obtained results interms of global accuracy.Table 5: Results in terms of Accuracy.Features English original Epals Promptlex1 0,458 0,334 0,414lexsyn2 0,706 0,656 0,632lexsyn3 0,718 0,638 0,612sem4 0,675456 0,59798 0,629555sin5 0,608 0,438 0,518lexsemsin6 0,839757 0,662626 0,722672lexsemsin7 0,8 0,678 0,674Note that the average loss of precision is around17% if we use Epals, and around 12% if we usePrompt.
(Li and Roth, 2002) obtain a better performancefor English, around a 92.5% in terms of accuracy.The best results are obtained when we use acombination of all lexical, syntactic and seman-tic features.
The main reason is that the classifierworks better when the number of features, whichcan be different to each category, is increased.For future work, it will be also necessary tostudy the time consumption for each features set,to decide which ones can be used.Table 6 shows the results in terms of F-score.Table 6: Results in terms of F-score.Features English original Epals Promptlex1 0,476077 0,319793 0,441075lexsyn2 0,708444 0,669692 0,6455lexsyn3 0,721258 0,644813 0,614353sem4 0,649405 0,593019 0,620068sin5 0,576356 0,404038 0,48739lexsemsin6 0,827789 0,664122 0,726667lexsemsin7 0,795897 0,680039 0,68014As an example in Table 7 we show detailed re-sults for the best case, where the result for eachgeneral category is showed.Table 7: Detailed results for each category, usingthe combination lexsemsin6 and the original Eng-lish questions and the translated questions by us-ing PromptClass English original PromptPrecision F-score Precision F-scoreABBR 0.857 0.750 1 0.611DESC 0.8442 0.906 0.695 0.806ENTY 0.731 0.727 0.595 0.737HUM 0.839 0.825 0.898 0.914LOC 0.847 0.867 0.680 0.859NUM 0.935 0.843 0.798 0.856As we have seen there are no important differ-ences between categories.
In addition, this tableshows that the translation results are reliable sincefor every category the lost of precision is similar(about 15%).There are some reasons for the lost of precision.Some of them are the following:1.
Bad translation of synonym words.
For in-stance we can compare an English originalsentence: ?What are the animals that don?thave backbones called?
?, and its Prompttranslation: ?How are they called the animalsthat have no spines??.
The word backbonehas been replaced with spine, so the IR sys-tem cannot find the same lists of relevant doc-uments.2.
Translation of Named Entities.
For instancewe can compare an English original sentence:?Who was Galileo?
?, and its Prompt transla-tion: ?Who was Galilean?
?.EACL 2006 Workshop on Multilingual Question Answering - MLQA06433.
General bad translations.
For instance we cancompare an English original sentence: ?Whodiscovered x-rays?
?, and its Prompt transla-tion: ?Who discovered the beams the Xth?
?.4 ConclusionsMultilingual Question Answering systems haveopened a new investigation task, where the ques-tion classification is an important first phase toknow the type of answer and some relevant infor-mation about this question.Our option is to use some standards resourcesfor English and translate Spanish questions.Of course we could develop a multilingual QCsystem using good training corpus for every lan-guage, but it is expensive to produce.The use of machine translation systems is, then,very important, so the study of different onlinetranslators is a main task.
In our case we haveapplied them to translate questions from Spanishinto English.We have made a complete investigation usingthe two datasets of training and test questions thathave been used by other groups, all labelled man-ually.
Different parameters have been the testfile used (originally in English or translated fromSpanish with the MT Epals or Prompt), the ma-chine learning algorithm, some TiMBL parame-ters and the lexical, syntactic or semantic features.The best results have been obtained using theoriginal English questions and a combination oflexical, syntactic and semantic features.
The bestMT has been Prompt.We have some conclusions:?
Applying machine learning with a completeset of training questions we obtain good re-sults, over 0,8 in terms of accuracy.?
The use of machine translation systems de-creases the results around 15%, but it willbe possible to increase the performance us-ing other models based on machine learningor a voting system for instance.?
A combination of all lexical, syntactic and se-mantic features obtains the best results.As future work we want to check the systemwith other training and test datasets.
We also wantto design a voting system using different QC mod-els; models based on patterns (to detect the classfor some types of questions); models based onrules (filtering non-redundancy types of questions.For instance all questions with ?who?
are relatedto a person).It could be also interested to test the combina-tion between a better QC system, the current oneby Li and Roths for instance (Li and Roth, 2002),and our machine translation method.Finally, we want to study types of questionswith poor results in order to improve them apply-ing other techniques, such as question expansion.Acknowledgement This work has been sup-ported by Spanish Government (MCYT) withgrant TIC2003-07158-C04-04.ReferencesW.
Daelemans, J. Zavrel, K. van der Sloot, andA.
van den Bosch.
2004.
Timbl: Tilburgmemorybased learner, version 5.1, reference guide.
ilk tech-nical report 04-02.G.
Grefenstette, editor.
1998.
Cross-Language Infor-mation Retrieval, volume 1.
Kluwer academic pub-lishers, Boston, USA.K.
Hacioglu and W. Ward.
2003.
Question classifi-cation with support vector machines and error cor-recting codes.
In Proceedings of Human LanguageTechnology conference (HLT-NAACL).E.
Hovy, L. Gerber, U. Hermjakob, C. Lin, andD.
Ravichandran.
1999.
Towards sematics-basedanswer pinpointing.
In Proceedings of the DARPAHuman Language Technology conference (HLT).X.
Li and D. Roth.
2002.
Learning question classifiers.In In COLING?02.P.
McNamee, J. Mayfield, and C. Piatko.
2000.
Thejhu/apl haircut system at trec- 8.
In Proceedings ofthe Eighth Text Retrieval Conference (TREC8).M.
F. Porter.
1980.
An algorithm for suffix stripping.In Program 14.H.
Schmid.
1994.
Probabilistic part-of-speech taggingusing decision trees.
In Proceedings of InternationalConference on New Methods in Language Process-ing.B.
Van Durme, Y. Huang, A. Kupsc, and E. Nyberg.2003.
Towards light semantic processing for ques-tion answering.
In Proceedings of Human LanguageTechnology conference (HLT-NAACL).C.J.
Van Rijsbergen.
1979.
Information retrieval.D.
Zhang and W. Sun Lee.
2003.
Question classifica-tion using support vector machines.
In Proceedingsof the 26th Annual International ACM SIGIR Con-ference on Research and Development in Informa-tion Retrieval.EACL 2006 Workshop on Multilingual Question Answering - MLQA0644
