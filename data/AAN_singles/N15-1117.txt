Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1108?1118,Denver, Colorado, May 31 ?
June 5, 2015. c?2015 Association for Computational LinguisticsRemoving the Training Wheels: A Coreference Datasetthat Entertains Humans and Challenges ComputersAnupam Guha,1Mohit Iyyer,1Danny Bouman,1Jordan Boyd-Graber21University of Maryland, Department of Computer Science and umiacs2University of Colorado, Department of Computer Scienceaguha@cs.umd.edu, miyyer@umiacs.umd.edu, dannybb@gmail.com,Jordan.Boyd.Graber@colorado.eduAbstractCoreference is a core nlp problem.
How-ever, newswire data, the primary sourceof existing coreference data, lack the rich-ness necessary to truly solve coreference.We present a new domain with denserreferences?quiz bowl questions?that ischallenging and enjoyable to humans, andwe use the quiz bowl community to developa new coreference dataset, together with anannotation framework that can tag any textdata with coreferences and named entities.We also successfully integrate active learn-ing into this annotation pipeline to collectdocuments maximally useful to coreferencemodels.
State-of-the-art coreference sys-tems underperform a simple classifier onour new dataset, motivating non-newswiredata for future coreference research.1 IntroductionCoreference resolution?adding annotations toan input text where multiple strings refer to thesame entity?is a fundamental problem in com-putational linguistics.
It is challenging becauseit requires the application of syntactic, semantic,and world knowledge (Ng, 2010).For example, in the sentence Monsieur Poirotassured Hastings that he ought to have faith inhim, the strings Monsieur Poirot and him referto the same person, while Hastings and he referto a different character.There are a panoply of sophisticated corefer-ence systems, both data-driven (Fernandes etal., 2012; Durrett and Klein, 2013; Durrett andKlein, 2014; Bjo?rkelund and Kuhn, 2014) andrule-based (Pradhan et al, 2011; Lee et al, 2011).Recent CoNLL shared tasks provide the oppor-tunity to make a fair comparison between thesesystems.
However, because all of these sharedtasks contain strictly newswire data,1it is unclearhow existing systems perform on more diversedata.We argue in Section 2 that to truly solve coref-erence resolution, the research community needshigh-quality datasets that contain many chal-lenging cases such as nested coreferences andcoreferences that can only be resolved using ex-ternal knowledge.
In contrast, newswire is delib-erately written to contain few coreferences, andthose coreferences should be easy for the readerto resolve.
Thus, systems that are trained onsuch data commonly fail to detect coreferencesin more expressive, non-newswire text.Given newswire?s imperfect range of corefer-ence examples, can we do better?
In Section 3we present a specialized dataset that specificallytests a human?s coreference resolution ability.This dataset comes from a community of triviafans who also serve as enthusiastic annotators(Section 4).
These data have denser coreferencementions than newswire text and present hith-erto unexplored questions of what is coreferentand what is not.
We also incorporate active learn-ing into the annotation process.
The result is asmall but highly dense dataset of 400 documentswith 9,471 mentions.1We use ?newswire?
as an umbrella term that encom-passes all forms of edited news-related data, includingnews articles, blogs, newsgroups, and transcripts of broad-cast news.1108We demonstrate in Section 5 that our datasetis significantly different from newswire based onresults from the effective, widely-used Berkeleysystem (Durrett and Klein, 2013).
These resultsmotivate us to develop a very simple end-to-endcoreference resolution system consisting of a crf-based mention detector and a pairwise classifier.Our system outperforms the Berkeley systemwhen both have been trained on our new dataset.This result motivates further exploration intocomplex coreference types absent in newswiredata, which we discuss at length in Section 7.2 Newswire?s Limitations forCoreferenceNewswire text is widely used as training datafor coreference resolution systems.
The stan-dard datasets used in the muc (MUC-6, 1995;MUC-7, 1997), ace (Doddington et al, 2004),and CoNLL shared tasks (Pradhan et al, 2011)contain only such text.
In this section we ar-gue why this monoculture, despite its many pastsuccesses, offer diminishing results for advancingthe coreference subfield.First, newswire text has sparse references, andthose that it has are mainly identity coreferencesand appositives.
In the CoNLL 2011 sharedtask (Pradhan et al, 2007) based on OntoNotes4.0 (Hovy et al, 2006),2there are 2.1 mentionsper sentence; in the next section we present adataset with 3.7 mentions per sentence.3Innewswire text, most nominal entities (not in-cluding pronouns) are singletons; in other words,they do not corefer to anything.
OntoNotes 4.0development data contains 25.4K singleton nomi-nal entities (Durrett and Klein, 2013), comparedto only 7.6K entities which corefer to something(anaphora).
On the other hand, most pronomi-nals are anaphoric, which makes them easy to re-solve as pronouns are single token entities.
While2As our representative for ?newswire?
data, the En-glish portion of the Ontonotes 4.0 contains professionally-delivered weblogs and newsgroups (15%), newswire (46%),broadcast news (15%), and broadcast conversation (15%).3Neither of these figures include singleton mentions,as OntoNotes does not have gold tagged singletons.
Ourdataset has an even higher density when singletons areincluded.it is easy to obtain a lot of newswire data, theamount of coreferent-heavy mention clusters insuch text is not correspondingly high.Second, coreference resolution in news textis trivial for humans because it rarely requiresworld knowledge or semantic understanding.
Sys-tems trained on news media data for a re-lated problem?entity extraction?falter on non-journalistic texts (Poibeau and Kosseim, 2001).This discrepancy in performance can be at-tributed to the stylistic conventions of journalism.Journalists are instructed to limit the number ofentities mentioned in a sentence, and there arestrict rules for referring to individuals (Boyd etal., 2008).
Furthermore, writers cannot assumethat their readers are familiar with all partici-pants in the story, which requires that each entityis explicitly introduced in the text (Goldstein andPress, 2004).
These constraints make for easyreading and, as a side effect, easy coreferenceresolution.
Unlike this simplified ?journalistic?coreference, everyday coreference relies heavilyon inferring the identities of people and entitiesin language, which requires substantial worldknowledge.While news media contains examples of coref-erence, the primary goal of a journalist is toconvey information, not to challenge the reader?scoreference resolution faculty.
Our goal is toevaluate coreference systems on data that taxeseven human coreference.3 Quiz Bowl: A Game of HumanCoreferenceOne example of such data comes from a gamecalled quiz bowl.
Quiz bowl is a trivia gamewhere questions are structured as a series of sen-tences, all of which indirectly refer to the answer.Each question has multiple clusters of mutually-coreferent terms, and one of those clusters iscoreferent with the answer.
Figure 1 shows anexample of a quiz bowl question where all answercoreferences have been marked.A player?s job is to determine4the entity ref-4In actual competition, it is a race to see which teamcan identify the coreference faster, but we ignore thataspect here.1109NW Later, [they]1all met with [President Jacques Chirac]2.
[Mr. Chirac]2said an importantfirst step had been taken to calm tensions.NW Around the time of the [Macau]1handover, questions that were hot in [the Westernmedia]2were ?what is Macaense??
And what is native [Macau]1culture?NW [MCA]1said that [it]1expects [the proposed transaction]2to be completed no later thanNovember 10th.QB As a child, [this character]1reads [[his]1uncle]2[the column]3[That Body of Yours ]3everySunday.QB At one point, [these characters]1climb into barrels aboard a ship bound for England.Later, [one of [these characters]1]2stabs [the Player]3with a fake knife.QB [One poet from [this country]2]1invented the haiku, while [another]3wrote the [Tale ofGenji ]4.
Identify [this homeland]2of [Basho]1and [Lady Murasaki]3.Table 1: Three newswire sentences and three quiz bowl sentences with annotated coreferences and singletonmentions.
These examples show that quiz bowl sentences contain more complicated types of coreferencesthat may even require world knowledge to resolve.
[The Canadian rock band by [this name]] has releasedsuch albums as Take A Deep Breath, Young Wild andFree, and Love Machine and had a 1986 Top Ten sin-gle with Can?t Wait For the Night.
[The song by [thisname]] is [the first track on Queen?s Sheer Heart At-tack].
[The novel by [this name]] concerns Fred Hale,who returns to town to hand out cards for a newspapercompetition and is murdered by the teenage gang mem-ber Pinkie Brown, who abuses [the title substance].
[Thenovel] was adapted into [a 1947 film starring RichardAttenborough]; [this] was released in the US as YoungScarface.
FTP, identify [the shared name of, most no-tably, [a novel by Graham Greene]].Figure 1: An example quiz bowl question about thenovel Brighton Rock.
Every mention referring to theanswer of the question has been marked; note thevariety of mentions that refer to the same entity.erenced by the question.
Each sentence containsprogressively more informative references andmore well-known clues.
For example, a questionon Sherlock Holmes might refer to him as ?he?,?this character?, ?this housemate of Dr. Watson?,and finally ?this detective and resident of 221BBaker Street?.
While quiz bowl has been viewedas a classification task (Iyyer et al, 2014), pre-vious work has ignored the fundamental task ofcoreference.
Nevertheless, quiz bowl data aredense and diverse in coreference examples.
Forexample, nested mentions, which are difficultfor both humans and machines, are very rarein the newswire text of OntoNotes?0.25 men-02000400060000 10000 20000 30000 40000 50000TokensCountData Newswire Quiz BowlCoref Mentions NestedFigure 2: Density of quiz bowl vs. CoNLL coreferenceboth for raw and nested mentions.tions per sentence?while quiz bowl contains 1.16mentions per sentence (Figure 2).
Examples ofnested mentions can be seen in in Table 1.
Sincequiz bowl is a game, it makes the task of solvingcoreference interesting and challenging for anannotator.
In the next section, we use the intrin-sic fun of this task to create a new annotatedcoreference dataset.4 Intelligent AnnotationHere we describe our annotation process.
Eachdocument is a single quiz bowl question contain-ing an average of 5.2 sentences.
While quiz bowl1110covers all areas of academic knowledge, we focuson questions about literature from Boyd-Graberet al (2012), as annotation standards are morestraightforward.Our webapp (Figure 3) allows users to anno-tate a question by highlighting a phrase usingtheir mouse and then pressing a number corre-sponding to the coreference group to which itbelongs.
Each group is highlighted with a singlecolor in the interface.
The webapp displays asingle question at a time, and for some questions,users can compare their answers against gold an-notations by the authors.
We provide annotatorsthe ability to see if their tags match the goldlabels for a few documents as we need to providea mechanism to help them learn the annotationguidelines as the annotators are crowdsourcedvolunteers.
This improves inter-annotator agree-ment.The webapp was advertised to quiz bowl play-ers before a national tournament and attractedpassionate, competent annotators preparing forthe tournament.
A leaderboard was implementedto encourage competitiveness, and prizes weregiven to the top five annotators.Users are instructed to annotate all authors,characters, works, and the answer to the ques-tion (even if the answer is not one of the previ-ously specified types of entities).
We considera coreference to be the maximal span that canbe replaced by a pronoun.5As an example, inthe phrase this folk sermon by James WeldonJohnson, the entire phrase is marked, not justsermon or this folk sermon.
Users are asked toconsider appositives as separate coreferences tothe same entity.
Thus, The Japanese poet Bashohas two phrases to be marked, The Japanese poetand Basho, which both refer to the same group.6Users annotated prepositional phrases attachedto a noun to capture entire noun phrases.Titular mentions are mentions that refer toentities with similar names or the same name as5We phrased the instruction in this way to allow oureducated but linguistically unsavvy annotators to approx-imate a noun phrase.6The datasets, full annotation guide, and codecan be found at http://www.cs.umd.edu/~aguha/qbcoreference.Number of .
.
.
Quiz bowl OntoNotesdocuments7400 1,667sentences 1,890 44,687tokens 50,347 955,317mentions 9,471 94,155singletons82,461 0anaphora 7,010 94,155nested ment.
2,194 11,454Table 2: Statistics of both our quiz bowl dataset andthe OntoNotes training data from the CoNLL 2011shared task.a title, e.g., ?The titular doctor?
refers to theperson ?Dr.
Zhivago?
while talking about thebook with the same name.
For our purposes, alltitular mentions refer to the same coreferencegroup.
We also encountered a few mentions thatrefer to multiple groups; for example, in thesentence Romeo met Juliet at a fancy ball, andthey get married the next day, the word theyrefers to both Romeo and Juliet.
Currently, ourwebapp cannot handle such mentions.To illustrate how popular the webapp provedto be among the quiz bowl community, we had615 documents tagged by seventy-six users withina month.
The top five annotators, who betweenthem tagged 342 documents out of 651, havean agreement rate of 87% with a set of twentyauthor-annotated questions used to measure tag-ging accuracy.We only consider documents that have eitherbeen tagged by four or more users with a pre-determined degree of similarity and verified byone or more author (150 documents), or docu-ments tagged by the authors in committee (250documents).
Thus, our gold dataset has 400documents.Both our quiz bowl dataset and the OntoNotesdataset are summarized in Table 2.
If corefer-ence resolution is done by pairwise classification,our dataset has a total of 116,125 possible men-tion pairs.
On average it takes about fifteenminutes to tag a document because often theannotator will not know which mentions co-refer7This number is for the OntoNotes training split only.8OntoNotes is not annotated for singletons.1111Figure 3: The webapp to collect annotations.
The user highlights a phrase and then assigns it to a group (bynumber).
Showing a summary list of coreferences on the right significantly speeds up user annotations.to what group without using external knowledge.OntoNotes is 18.97 larger than our dataset interms of tokens but only 13.4 times larger interms of mentions.9Next, we describe a tech-nique that allows our webapp to choose whichdocuments to display for annotation.4.1 Active LearningActive learning is a technique that alternatesbetween training and annotation by selectinginstances or documents that are maximally use-ful for a classifier (Settles, 2010).
Because ofthe large sample space and amount of diversitypresent in the data, active learning helps us buildour coreference dataset.
To be more concrete,the original corpus contains over 7,000 literaturequestions, and we want to tag only the usefulones.
Since it can take a quarter hour to tag asingle document and we want at least four an-notators to agree on every document that weinclude in the final dataset, annotating all 7,000questions is infeasible.We follow Miller et al (2012), who use activelearning for document-level coreference ratherthan at the mention level.
Starting from a seedset of a hundred documents and an evaluationset of fifty documents10we sample 250 more9These numbers do not include singletons asOntoNotes does not have them tagged, while ours does.10These were documents tagged by the quiz bowl com-documents from our set of 7,000 quiz bowl ques-tions.
We use the Berkeley coreference system(described in the next section) for the trainingphase.
In Figure 4 we show the effectivenessof our iteration procedure.
Unlike the resultshown by Miller et al (2012), we find that forour dataset voting sampling beats random sam-pling, which supports the findings of Laws et al(2012).Voting sampling works by dividing the seedset into multiple parts and using each to traina model.
Then, from the rest of the dataset weselect the document that has the most variancein results after predicting using all of the models.Once that document gets tagged, we add it tothe seed set, retrain, and repeat the procedure.This process is impractical with instance-levelactive learning methods, as there are 116,125mention pairs (instances) for just 400 documents.Even with document-level sampling, the proce-dure of training on all documents in the seedset and then testing every document in the sam-ple space is a slow task.
Batch learning canspeed up this process at the cost of increaseddocument redundancy; we choose not to use itbecause we want a diverse collection of annotateddocuments.
Active learning?s advantage is thatnew documents are more likely to contain diversemunity, so we didn?t have to make them wait for theactive learning process to retrain candidate models.1112505254560 20 40 60IterationPrecisionMethod Active RandomFigure 4: Voting sampling active learning works bet-ter than randomly sampling for annotation.
(and thus interesting) combinations of entitiesand references, which annotators noticed dur-ing the annotation process.
Documents selectedby the active learning process were dissimilarto previously-selected questions in both contentand structure.5 Experimental Comparison ofCoreference SystemsWe evaluate the widely used Berkeley corefer-ence system (Durrett and Klein, 2013) on ourdataset to show that models trained on newswiredata cannot effectively resolve coreference in quizbowl data.
Training and evaluating the Berkeleysystem on quiz bowl data also results in poorperformance.11This result motivates us to buildan end-to-end coreference resolution system thatincludes a data-driven mention detector (as op-posed to Berkeley?s rule-based one) and a simplepairwise classifier.
Using our mentions and onlysix feature types, we are able to outperform theBerkeley system on our data.
Finally, we ex-plore the linguistic phenomena that make quizbowl coreference so hard and draw insights fromour analysis that may help to guide the nextgeneration of coreference systems.11We use default options, including hyperparameterstuned on OntoNotes5.1 Evaluating the Berkeley System onQuiz Bowl DataWe use two publicly-available pretrained modelssupplied with the Berkeley coreference system,Surface and Final, which are trained on the en-tire OntoNotes dataset.
The difference betweenthe two models is that Final includes semanticfeatures.
We report results with both models tosee if the extra semantic features in Final are ex-pressive enough to capture quiz bowl?s inherentlydifficult coreferences.
We also train the Berke-ley system on quiz bowl data and compare theperformance of these models to the pretrainednewswire ones in Table 3.
Our results are ob-tained by running a five-fold cross-validation onour dataset.
The results show that newswire isa poor source of data for learning how to resolvequiz bowl coreferences and prompted us to seehow well a pairwise classifier does in comparison.To build an end-to-end coreference system usingthis classifier, we first need to know which partsof the text are ?mentions?, or spans of a text thatrefer to real world entities.
In the next sectionwe talk about our mention detection system.5.2 A Simple Mention DetectorDetecting mentions is done differently by differ-ent coreference systems.
The Berkeley systemdoes rule-based mention detection to detect everyNP span, every pronoun, and every named entity,which leads to many spurious mentions.
This pro-cess is based on an earlier work of Kummerfeldet al (2011), which assumes that every maximalprojection of a noun or a pronoun is a mentionand uses rules to weed out spurious mentions.
In-stead of using such a rule-based mention detector,our system detects mentions via sequence label-ing, as detecting mentions is essentially a prob-lem of detecting start and stop points in spansof text.
We solve this sequence tagging problemusing the mallet (McCallum, 2002) implementa-tion of conditional random fields (Lafferty et al,2001).
Since our data contain nested mentions,the sequence labels are bio markers (Ratinovand Roth, 2009).
The features we use, whichare similar to those used in Kummerfeld et al(2011), are:1113mucSystem Train P R F1Surface OntoN 47.22 27.97 35.13Final OntoN 50.79 30.77 38.32Surface QB 60.44 31.31 41.2Final QB 60.21 33.41 42.35Table 3: The top half of the table represents Berkeleymodels trained on OntoNotes 4.0 data, while the bot-tom half shows models trained on quiz bowl data.
Themuc F1-score of the Berkeley system on OntoNotestext is 66.4, which when compared to these resultsprove that quiz bowl coreference is significantly dif-ferent than OntoNotes coreference.?
the token itself?
the part of speech?
the named entity type?
a dependency relation concatenated withthe parent token12Using these simple features, we obtain sur-prisingly good results.
When comparing ourdetected mentions to gold standard mentions onthe quiz bowl dataset using exact matches, weobtain 76.1% precision, 69.6% recall, and 72.7%F1measure.
Now that we have high-quality men-tions, we can feed each pair of mentions into apairwise mention classifier.5.3 A Simple Coref ClassifierWe follow previous pairwise coreference sys-tems (Ng and Cardie, 2002; Uryupina, 2006;Versley et al, 2008) in extracting a set of lexical,syntactic, and semantic features from two men-tions to determine whether they are coreferent.For example, if Sylvia Plath, he, and she are all ofthe mentions that occur in a document, our clas-sifier gives predictions for the pairs he?SylviaPlath, she?Sylvia Plath, and he?she.Given two mentions in a document, m1andm2, we generate the following features and feedthem to a logistic regression classifier:?
binary indicators for all tokens contained in12These features were obtained using the Stanford de-pendency parser (De Marneffe et al, 2006).m1and m2concatenated with their parts-of-speech?
same as above except for an n-word windowbefore and after m1and m2?
how many tokens separate m1and m2?
how many sentences separate m1and m2?
the cosine similarity of word2vec (Mikolovet al, 2013) vector representations of m1and m2; we obtain these vectors by averag-ing the word embeddings for all words ineach mention.
We use publicly-available 300-dimensional embeddings that have been pre-trained on 100B tokens from Google News.?
same as above except with publicly-available300-dimensional GloVe (Pennington et al,2014) vector embeddings trained on 840Btokens from the Common CrawlThe first four features are standard in corefer-ence literature and similar to some of the surfacefeatures used by the Berkeley system, while theword embedding similarity scores increase ourF-measure by about 5 points on the quiz bowldata.
Since they have been trained on huge cor-pora, the word embeddings allow us to infuseworld knowledge into our model; for instance, thevector for Russian is more similar to Dostoevskythan Hemingway.Figure 5 shows that our logistic regressionmodel (lr) outperforms the Berkeley system onnumerous metrics when trained and evaluatedon the quiz bowl dataset.
We use precision, re-call, and F1, metrics applied to muc, bcub, andceafe measures used for comparing coreferencesystems.13We find that our lr model outper-forms Berkeley by a wide margin when both aretrained on the mentions found by our mentiondetector (crf).
For four metrics, the crf men-tions actually improve over training on the goldmentions.Why does the lr model outperform Berkeley13The muc (Vilain et al, 1995) score is the minimumnumber of links between mentions to be inserted or deletedwhen mapping the output to a gold standard key set.bcub (Bagga and Baldwin, 1998) computes the precisionand recall for all mentions separately and then combinesthem to get the final precision and recall of the output.ceafe (Luo, 2005) is an improvement on bcub and doesnot use entities multiple times to compute scores.1114Berkeley Mentions CRF Mentions Gold Mentions025507502550750255075BCUBCEAFEMUCF1 P R F1 P R F1 P RScoreCoreference LR QB Final (Berkeley trained on QB)Figure 5: All models are trained and evaluated on quiz bowl data via five fold cross validation on F1, precision,and recall.
Berkeley/crf/Gold refers to the mention detection used, lr refers to our logistic regression modeland QB Final refers to the Berkeley model trained on quiz bowl data.
Our model outperforms the Berkeleymodel on every metric when using our detected crf mentions.
When given gold mentions, lr outperformsBerkeley QB Final in five of nine metrics.when both are trained on our quiz bowl dataset?We hypothesize that some of Berkeley?s features,while helpful for sparse OntoNotes coreferences,do not offer the same utility in the denser quizbowl domain.
Compared to newswire text, ourdataset contains a much larger percentage ofcomplex coreference types that require worldknowledge to resolve.
Since the Berkeley systemlacks semantic features, it is unlikely to correctlyresolve these instances, whereas the pretrainedword embedding features give our lr model abetter chance of handling them correctly.
An-other difference between the two models is thatthe Berkeley system ranks mentions as opposedto doing pairwise classification like our lr model,and the mention ranking features may be opti-mized for newswire text.5.4 Why Quiz Bowl Coreference isChallengingWhile models trained on newswire falter on thesedata, is this simply a domain adaptation issueor something deeper?
In the rest of this section,we examine specific examples to understand whyquiz bowl coreference is so difficult.
We beginwith examples that Final gets wrong.This writer depicted a group of samu-rai?s battle against an imperial.
For tenpoints, name this Japanese writer of APersonal Matter and The Silent Cry.While Final identifies most of pronouns associ-ated with Kenzaburo Oe (the answer), it cannotrecognize that the theme of the entire paragraphis building to the final reference, ?this Japanesewriter?, despite the many Japanese-related ideasin the text of the question (e.g., Samurai andemperor).
Final also cannot reason effectivelyabout coreferences that are tied together by sim-ilar modifiers as in the below example:That title character plots to secure a?beautiful death?
for Lovberg by burn-ing his manuscript and giving him apistol.
For 10 points, name this play inwhich the titular wife of George Tesmancommits suicide.While a reader can connect ?titular?
and ?title?to the same character, Hedda Gabler, the Berke-ley system fails to make this inference.
Thesedata are a challenge for all systems, as they re-quire extensive world knowledge.
For example,in the following sentence, a model must knowthat the story referenced in the first sentence isabout a dragon and that dragons can fly.1115The protagonist of one of this man?sworks erects a sign claiming that thatstory?s title figure will fly to heavenfrom a pond.
Identify this author ofDragon: the Old Potter?s TaleHumans solve cases like these using a vastamount of external knowledge, but existing mod-els lack information about worlds (both real andimaginary) and thus cannot confidently markthese coreferences.
We discuss coreference workthat incorporates external resources such asWikipedia in the next section; our aim is toprovide a dataset that benefits more from thistype of information than newswire does.6 Related WorkWe describe relevant data-driven coreference re-search in this section, all of which train andevaluate on only newswire text.
Despite effortsto build better rule-based (Luo et al, 2004) orhybrid statistical systems (Haghighi and Klein,2010), data-driven systems currently dominatethe field.
The 2012 CoNLL shared task ledto improved data-driven systems for coreferenceresolution that finally outperformed both theStanford system (Lee et al, 2011) and the imssystem (Bjo?rkelund and Farkas, 2012), the lat-ter of which was the best available publicly-available English coreference system at the time.The recently-released Berkeley coreference sys-tem (Durrett and Klein, 2013) is especially strik-ing: it performs well with only a sparse set ofcarefully-chosen features.
Semantic knowledgesources?especially WordNet (Miller, 1995) andWikipedia?have been used in coreference en-gines (Ponzetto and Strube, 2006).
A systemby Ratinov and Roth (2012) demonstrates goodperformance by using Wikipedia knowledge tostrengthen a multi-pass rule based system.
Ina more recent work, Durrett and Klein (2014)outperform previous systems by building a jointmodel that matches mentions to Wikipedia en-tities while doing named entity resolution andcoreference resolution simultaneously.
We takea different approach by approximating semanticand world knowledge through our word embed-ding features.
Our simple classifier yields a bi-nary decision for each mention pair, a methodthat had been very popular before the last fiveyears (Soon et al, 2001; Bengtson and Roth,2008; Stoyanov et al, 2010).
Recently, betterresults have been obtained with mention-rankingsystems (Luo et al, 2004; Haghighi and Klein,2010; Durrett and Klein, 2013; Bjo?rkelund andKuhn, 2014).
However, on quiz bowl data, ourexperiments show that binary classifiers can out-perform mention-ranking approaches.7 Embracing Harder CoreferenceThis paper introduces a new, naturally-occuringcoreference dataset that is easy to annotate butdifficult for computers to solve.
We show that ac-tive learning allows us to create a dataset that isrich in different types of coreference.
We developan end-to-end coreference system using very sim-ple mention detection and pairwise classificationmodels that outperforms traditional systems onour dataset.
The next challenge is to incorporatethe necessary world knowledge to solve theseharder coreference problems.
Systems should beable to distinguish who is likely to marry whom,identify the titles of books from roundabout de-scriptions, and intuit family relationships fromraw text.
These are coreference challenges notfound in newswire but that do exist in the realworld.
Unlike other ai-complete problems likemachine translation, coreference in challengingdatasets is easy to both annotate and evaluate.This paper provides the necessary building blocksto create and evaluate those systems.8 AcknowledgmentsWe thank the anonymous reviewers for theirinsightful comments.
We also thank Dr. HalDaume?
III and the members of the ?feetthinking?research group for their advice and assistance.We also thank Dr. Yuening Hu and Mossaab Bag-douri for their help in reviewing the draft of thispaper.
This work was supported by nsf GrantIIS-1320538.
Boyd-Graber is also supported bynsf Grants CCF-1018625 and NCSE-1422492.Any opinions, findings, results, or recommenda-tions expressed here are of the authors and donot necessarily reflect the view of the sponsor.1116ReferencesAmit Bagga and Breck Baldwin.
1998.
Algorithmsfor scoring coreference chains.
In InternationalLanguage Resources and Evaluation.
Citeseer.Eric Bengtson and Dan Roth.
2008.
Understandingthe value of features for coreference resolution.
InProceedings of Emperical Methods in Natural Lan-guage Processing.
Association for ComputationalLinguistics.Anders Bjo?rkelund and Richa?rd Farkas.
2012.
Data-driven multilingual coreference resolution usingresolver stacking.
In Conference on ComputationalNatural Language Learning.Anders Bjo?rkelund and Jonas Kuhn.
2014.
Learningstructured perceptrons for coreference resolutionwith latent antecedents and non-local features.
InProceedings of the Association for ComputationalLinguistics.A.
Boyd, P. Stewart, and R. Alexander.
2008.
Broad-cast Journalism: Techniques of Radio and Televi-sion News.
Taylor & Francis.Jordan Boyd-Graber, Brianna Satinoff, He He, andHal Daume III.
2012.
Besting the quiz master:Crowdsourcing incremental classification games.In Proceedings of Emperical Methods in NaturalLanguage Processing.Marie-Catherine De Marneffe, Bill MacCartney,Christopher D Manning, et al 2006.
Generatingtyped dependency parses from phrase structureparses.
In International Language Resources andEvaluation.George R Doddington, Alexis Mitchell, Mark A Przy-bocki, Lance A Ramshaw, Stephanie Strassel, andRalph M Weischedel.
2004.
The automatic con-tent extraction (ACE) program-tasks, data, andevaluation.
In International Language Resourcesand Evaluation.Greg Durrett and Dan Klein.
2013.
Easy victoriesand uphill battles in coreference resolution.
In Pro-ceedings of Emperical Methods in Natural LanguageProcessing.Greg Durrett and Dan Klein.
2014.
A joint model forentity analysis: Coreference, typing, and linking.Transactions of the Association for ComputationalLinguistics.Eraldo Rezende Fernandes, C?
?cero Nogueira Dos San-tos, and Ruy Luiz Milidiu?.
2012.
Latent structureperceptron with feature induction for unrestrictedcoreference resolution.
In Proceedings of EmpericalMethods in Natural Language Processing.N.
Goldstein and A.
Press.
2004.
The AssociatedPress Stylebook and Briefing on Media Law.
Asso-ciated Press Stylebook and Briefing on Media Law.Basic Books.Aria Haghighi and Dan Klein.
2010.
Coreferenceresolution in a modular, entity-centered model.
InConference of the North American Chapter of theAssociation for Computational Linguistics.Eduard Hovy, Mitchell Marcus, Martha Palmer,Lance Ramshaw, and Ralph Weischedel.
2006.Ontonotes: the 90% solution.
In Conference ofthe North American Chapter of the Association forComputational Linguistics.Mohit Iyyer, Jordan Boyd-Graber, LeonardoClaudino, Richard Socher, and Hal Daume?
III.2014.
A neural network for factoid question answer-ing over paragraphs.
In Proceedings of EmpericalMethods in Natural Language Processing.Jonathan K Kummerfeld, Mohit Bansal, David Bur-kett, and Dan Klein.
2011.
Mention detection:heuristics for the ontonotes annotations.
In Con-ference on Computational Natural Language Learn-ing.John Lafferty, Andrew McCallum, and Fernando CNPereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.Florian Laws, Florian Heimerl, and Hinrich Schu?tze.2012.
Active learning for coreference resolution.
InConference of the North American Chapter of theAssociation for Computational Linguistics.Heeyoung Lee, Yves Peirsman, Angel Chang,Nathanael Chambers, Mihai Surdeanu, and DanJurafsky.
2011.
Stanford?s multi-pass sieve coref-erence resolution system at the conll-2011 sharedtask.
In Conference on Computational NaturalLanguage Learning.Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing,Nanda Kambhatla, and Salim Roukos.
2004.
Amention-synchronous coreference resolution algo-rithm based on the bell tree.
In Proceedings of theAssociation for Computational Linguistics.Xiaoqiang Luo.
2005.
On coreference resolutionperformance metrics.
In Proceedings of EmpericalMethods in Natural Language Processing.Andrew Kachites McCallum.
2002.
Mal-let: A machine learning for language toolkit.http://mallet.cs.umass.edu.Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-frey Dean.
2013.
Efficient estimation of wordrepresentations in vector space.
arXiv preprintarXiv:1301.3781.Timothy A Miller, Dmitriy Dligach, and Guergana KSavova.
2012.
Active learning for coreference res-olution.
In Proceedings of the 2012 Workshop on1117Biomedical Natural Language Processing.
Proceed-ings of the Association for Computational Linguis-tics.George A Miller.
1995.
Wordnet: a lexicaldatabase for english.
Communications of the ACM,38(11):39?41.MUC-6.
1995.
Coreference task definition (v2.3,8 sep 95).
In Proceedings of the Sixth MessageUnderstanding Conference (MUC-6).MUC-7.
1997.
Coreference task definition (v3.0, 13jun 97).
In Proceedings of the Seventh MessageUnderstanding Conference (MUC-7).Vincent Ng and Claire Cardie.
2002.
Improving ma-chine learning approaches to coreference resolution.In Proceedings of the Association for Computa-tional Linguistics.Vincent Ng.
2010.
Supervised noun phrase corefer-ence research: The first fifteen years.
In Proceed-ings of the Association for Computational Linguis-tics.Jeffrey Pennington, Richard Socher, and ChristopherManning.
2014.
Glove: Global vectors for wordrepresentation.
In Proceedings of Emperical Meth-ods in Natural Language Processing.Thierry Poibeau and Leila Kosseim.
2001.
Propername extraction from non-journalistic texts.
Lan-guage and computers, 37(1):144?157.Simone Paolo Ponzetto and Michael Strube.
2006.Exploiting semantic role labeling, WordNet andWikipedia for coreference resolution.
In Conferenceof the North American Chapter of the Associationfor Computational Linguistics.Sameer S Pradhan, Eduard Hovy, Mitch Marcus,Martha Palmer, Lance Ramshaw, and RalphWeischedel.
2007.
Ontonotes: A unified relationalsemantic representation.
International Journal ofSemantic Computing, 1(04).Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,Martha Palmer, Ralph Weischedel, and NianwenXue.
2011.
CoNLL-2011 shared task: Modeling un-restricted coreference in Ontonotes.
In Conferenceon Computational Natural Language Learning.Lev Ratinov and Dan Roth.
2009.
Design challengesand misconceptions in named entity recognition.In Conference on Computational Natural LanguageLearning.Lev Ratinov and Dan Roth.
2012.
Learning-basedmulti-sieve co-reference resolution with knowledge.In Proceedings of Emperical Methods in NaturalLanguage Processing.Burr Settles.
2010.
Active learning literature survey.University of Wisconsin, Madison, 52:55?66.Wee Meng Soon, Hwee Tou Ng, and DanielChung Yong Lim.
2001.
A machine learning ap-proach to coreference resolution of noun phrases.Computational linguistics, 27(4).Veselin Stoyanov, Claire Cardie, Nathan Gilbert,Ellen Riloff, David Buttler, and David Hysom.2010.
Coreference resolution with reconcile.
InProceedings of the Association for ComputationalLinguistics.Olga Uryupina.
2006.
Coreference resolution withand without linguistic knowledge.
In InternationalLanguage Resources and Evaluation.Yannick Versley, Simone Paolo Ponzetto, MassimoPoesio, Vladimir Eidelman, Alan Jern, JasonSmith, Xiaofeng Yang, and Alessandro Moschitti.2008.
Bart: A modular toolkit for coreferenceresolution.
In Proceedings of the Association forComputational Linguistics.Marc Vilain, John Burger, John Aberdeen, DennisConnolly, and Lynette Hirschman.
1995.
A model-theoretic coreference scoring scheme.
In Proceed-ings of the conference on Message understanding,pages 45?52.1118
