Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 721?728Manchester, August 2008Unsupervised Induction of Labeled Parse Treesby Clustering with Syntactic FeaturesRoi ReichartICNCHebrew University of Jerusalemroiri@cs.huji.ac.ilAri RappoportInstitute of computer scienceHebrew University of Jerusalemarir@cs.huji.ac.ilAbstractWe present an algorithm for unsupervisedinduction of labeled parse trees.
The al-gorithm has three stages: bracketing, ini-tial labeling, and label clustering.
Brack-eting is done from raw text using an un-supervised incremental parser.
Initial la-beling is done using a merging model thataims at minimizing the grammar descrip-tion length.
Finally, labels are clusteredto a desired number of labels using syn-tactic features extracted from the initiallylabeled trees.
The algorithm obtains 59%labeled f-score on the WSJ10 corpus, ascompared to 35% in previous work, andsubstantial error reduction over a randombaseline.
We report results for English,German and Chinese corpora, using twolabel mapping methods and two label setsizes.1 IntroductionUnsupervised learning of grammar from text(?grammar induction?)
is of great theoretical andpractical importance.
It can shed light on languageacquisition by humans and on the general structureof language, and it can potentially assist NLP ap-plications that utilize parser output.
The problemhas attracted researchers for decades, and interesthas greatly increased recently, in part due to theavailability of huge corpora, computation power,and new learning algorithms (see Section 2).A fundamental issue in this research direction isthe representation of the resulting induced gram-c?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.mar.
Most recent work (e.g., (Klein and Manning,2004; Dennis, 2005; Bod, 2006a; Smith and Eis-ner, 2006; Seginer, 2007)) annotates text sentencesusing a hierarchical bracketing (constituents) or adependency structure, and thus represents the in-duced grammar through its behavior in a parsingtask.
Solan et al (2005) uses a graph representa-tion, while (Nakamura, 2006) simply uses a gram-mar formalism such as PCFG.
When the bracket-ing approach is taken, some algorithms label theresulting constituents, while most do not.Each of these approaches can be justified or crit-icized; a detailed discussion of this issue is be-yond the scope of this paper.
The algorithm pre-sented here belongs to the first group, annotatinggiven sentences with labeled bracketing structures.The main theoretical justification for this approachis that many linguistic and psycho-linguistic theo-ries posit some kind of a hierarchical labeled con-stituent (or constructional) structure, arguing that ithas a measurable psychological (cognitive) reality(e.g., (Goldberg, 2006)).
The main practical argu-ments in favor of this approach are that it enablesa detailed and large-scale evaluation using anno-tated corpora, as is done in this paper, and that theoutput format is suitable for many applications.When an algorithm generates labeled structures,the number of labels is an important issue.
From atheoretical point of view, the algorithm should alsodiscover the appropriate number of labels.
How-ever, for evaluation and application purposes it isuseful to base the number of labels on a specifictarget grammar.
In previous work, the number wasset to be equal to that in the target grammar.
Thisis a reasonable approach that we experiment within this paper.
In addition, to reduce the possiblearbitrariness in this approach, we also experimentwith the number of prominent labels in the target721grammar, determined according to their coverageof corpus constituents.Another issue relates to the nature of the in-put.
In most cases (e.g., in the Klein, Smith, Den-nis and Bod papers above), the input consists ofpart-of-speech (POS) sequences, derived from textcorpora by manual or automatic POS tagging.
Insome cases (e.g., in the Seginer and Solan papersabove) it can consist of plain text.
Again, eachapproach has its pros and cons.
The algorithmwe present here requires POS tags for its labelingstages.
Parts-of-speech are widely considered tohave a psychological reality (at least in English,including when they are viewed as low-level con-structions as in (Croft, 2001)), so this kind of inputis reasonable for theoretical research.
Moreover, asPOS induction is of medium quality (Clark, 2003),using a manually POS tagged corpus enables us tomeasure the performance of other induction stagesin a controlled manner.
Since supervised POS tag-ging is of very high quality and very efficient com-putationally (Brants, 2000), this requirement doesnot seriously limit the practical applicability of agrammar induction algorithm.Our labeled bracketings induction algorithmconsists of three stages.
We first induce unla-beled bracketing trees using the algorithm given in(Seginer, 2007)1.
We then induce initial labels us-ing a Bayesian Model Merging (BMM) labeling al-gorithm (Borensztajn and Zuidema, 2007), whichaims at minimizing the description length of theinput data and the induced grammar.
Finally, theinitial labels are clustered to a desired number oflabels using syntactic features extracted from theinitially labeled trees.
Previous work on labeledbrackets induction (Section 2) did not differentiatethe unlabeled structure induction phase from thelabeling phase, applying a single phase approach.To evaluate labeled bracketings, we need a map-ping between the label symbols of the induced andtarget grammars.
Previous work used a ?greedy?,many to one, mapping.
We used both the greedymapping and a label-to-label (LL) mapping, sincegreedy mapping is highly forgiving to structuralproblems in the induced labeling.
We report resultsfor two cases: one in which the number of labels inthe induced and target grammars is the same, andone in which the former is the number of promi-nent labels in the target grammar.
We discuss howthis number can be defined and determined.
We1The algorithm uses raw (not POS tagged) sentences.experimented with English (WSJ10, Brown10),German (NEGRA10) and Chinese (CTB10) cor-pora.When comparing to previous work that usedmanually annotated corpora in its evaluation(Haghighi and Klein, 2006)2, we obtained 59.5%labeled f-score on the WSJ10 setup vs. their 35.3%(Section 5).
We also show substantial improve-ment over a random baseline, and that the cluster-ing stage of our algorithm improves the results ofthe second merging stage.Section 2 discusses previous work.
In Section 3we detail our algorithm.
The experimental setupand results are presented in Sections 4 and 5.2 Previous WorkUnsupervised parsing has attracted researchers fordecades (see (Clark, 2001; Klein, 2005) for recentreviews).
Many types of input, syntax formalisms,search procedures, and success criteria were used.Among the theoretical and practical motivations tothis problem are the study of human language ac-quisition (in particular, an empirical study of thepoverty of stimulus hypothesis), preprocessing forconstructing large treebanks (Van Zaanen, 2001),and improving language models (Chen, 1995).In recent years efforts have been made to eval-uate the algorithms on manually annotated cor-pora such as the WSJ PennTreebank.
Recently,works along this line have for the first time out-performed the right branching heuristic baselinefor English.
These include the constituent?contextmodel (CCM) (Klein and Manning, 2002), itsextension using a dependency model (Klein andManning, 2004), (U)DOP based models (Bod,2006a; Bod, 2006b; Bod, 2007), an exemplar?based approach (Dennis, 2005), guiding EM usingcontrastive estimation (Smith and Eisner, 2006),and the incremental parser of (Seginer, 2007).
Allof these use as input POS tag sequences, exceptof Seginer?s algorithm, which uses plain text.
Allof these papers induce unlabeled bracketing or de-pendencies.There are other algorithmic approaches to theproblem (e.g., (Adriaans, 1992; Daelemans, 1995;Van Zaanen, 2001)).
None of these had evaluatedlabeled bracketing on annotated corpora.In this paper we focus on the induction oflabeled bracketing.
Bayesian Model Merging2Using, as they did, a greedy mapping with an equal num-ber of labels in the induced and target grammars.722(BMM) (Stolcke, 1994; Stolcke and Omohundro,1994) is a framework for inducing PCFG contain-ing both a bracketing and a labeling.
The charac-teristics of this framework (separating prior prob-ability, data likelihood and heuristic search proce-dures) can also be found in the grammar inductionmodels of (Wolf, 1982; Langley and Stromsten,2000; Petasis et al, 2004; Solan et al, 2005).
TheBMM model used here (Borensztajn and Zuidema,2007) combines features of (Petasis et al, 2004)and Stolcke?s algorithm, applying the minimumdescription length (MDL) principle.
We use it hereonly for initial labeling of existing bracketings.The MDL principle was also used in (Grunwald,1994; de Marcken, 1995; Clark, 2001).There are only two previous papers we areaware of that induce labeled bracketing and eval-uate on corpora annotated with a similar repre-sentation (Haghighi and Klein, 2006; Borensztajnand Zuidema, 2007).
We utilize and extend thelatter?s labeling algorithm.
However, the evalu-ation done by the latter dealt only with labeling,using gold-standard (manually annotated) bracket-ings.
Thus, we can directly compare our resultsonly to (Haghighi and Klein, 2006), where twomodels (PCFG ?
NONE and PCFG ?
CCM) are fully un-supervised.
These models use the inside-outsideand EM algorithms to induce bracketing and label-ing simultaneously, as opposed to our three stepmethod3.3 AlgorithmOur model consists of three stages: bracketing, ini-tial labeling, and label clustering.3.1 Induction of Unlabeled BracketingIn this step, we apply the algorithm of (Seginer,2007) to induce bracketing from plain text4.
Wehave chosen that algorithm because it is very fast(both learning and parsing) and its code is publiclyavailable.
We could have chosen any of the algo-rithms mentioned above producing a similar outputformat.3.2 Initial Constituent LabelingOur label clustering stage uses syntactic fea-tures.
To obtain these, we need an initial label-ing on the bracketings computed in the previous3Their other models, which were the core of their paper,are semi-supervised.4http://www.seggu.net/cclstage.
To do that we modify the Bayesian ModelMerging (BMM) algorithm of (Borensztajn andZuidema, 2007), which induces context-free gram-mars (bracketing and labeling) from POS tags,combining features of the models of (Stolcke andOmohundro, 1994) and (Petasis et al, 2004).The BMM algorithm (Borensztajn andZuidema, 2007) uses an iterative heuristicgreedy search for an optimal PCFG according tothe Bayesian criterion of maximum posterior prob-ability.
Two operators define possible transitionsbetween grammars: MERGE creates generaliza-tions by replacing two existing non-terminalsX1and X2that occur in the same contexts by asingle new non-terminal Y ; CHUNK concatenatesrepeating patterns by taking a sequence of twonon-terminals X1and X2and creating a newnon-terminal Y that expands to X1X2.We have used the algorithm to deal only withlabeling.
It reads the initial rules of the grammarfrom all productions implicit in the bracketed cor-pus induced in the previous step.
Every constituent(except of the start symbol) is given a unique label.Since only labeling is required, only MERGE oper-ations are performed.The objective function the algorithm tries to op-timize at each step is the posterior probability cal-culated according to Bayes?
Law:MMAP= argmaxMP (M|X) = argmaxMP (X|M) ?
P (M)(1)where P (X|M) is the likelihood of the data Xgiven the grammar M and P (M) is the prior prob-ability of the grammar.
This is equivalent to mini-mizing the function?log(P (X|M)) ?
logP (m) := DDL+ GDL := DL.
(2)Using a Minimal Description Length (MDL)principle, BMM interprets this function as total de-scription length (DL): The Grammar DescriptionLength GDL = ?logP (M) is the space neededto encode the model, and the Data DescriptionLength DDL = ?logP (X|M) is the space re-quired to describe the data given the model.
Therationale for MDL is to prefer smaller grammarsthat describe the data well.
DDL and GDL arecomputed as in (Stolcke, 1994; Stolcke and Omo-hundro, 1994).
In order to reduce the number ofgrammars considered at each step, which naivelyis quadratic in the number of non-terminals, amethod based on (Petasis et al, 2004) for effi-ciently predicting DL gain is applied.
The process723is iterated until no additional merge operation im-proves the objective function.
Full details are givenin (Borensztajn and Zuidema, 2007).3.3 Label ClusteringLabel set size.
BMM produces quite a large num-ber of labels (4944 for WSJ105).
In the third stepof our algorithm we reduce that number.
We firstdiscuss the issue of the number of labels in inducedgrammars, which is an important issue.In many situations, it is reasonable to use a num-ber T identical to the number of labels in a giventarget grammar, for example when that grammaris used for applications or evaluation.
This is theapproach in (Haghighi and Klein, 2006) for theirunsupervised models6, and we use it in part of ourevaluation.
However, it is also reasonable to arguethat the granularity of syntactic categories (labels)in the gold standard annotation of the corpora weexperiment with is somewhat arbitrary.
For exam-ple, in the WSJ Penn Treebank noun phrases areannotated with the symbol NP, but there is no dis-tinction between subject and object NPs.
Incorpo-rating such a distinction into the WSJ10 grammarwould result in a 27 labels grammar instead of 26.To examine this issue, consider Figure 1, whichshows the amount of constituent coverage obtainedby a certain number of labels in the four corporawe use (see Section 4).
In all of them, about 95%of the constituents are covered by 23% ?
37% ofthe labels, and the curve rises very sharply untilthat 95% value.
Motivated by this observation,given a corpus annotated using a certain hierarchi-cal labeled grammar, we refer to the set of P labelsthat cover at least 95% of the constituents in thecorpus as the grammar?s prominent labels.The prominent labels are not only the mostfrequent in the corpus; each of them substan-tially contributes to constituent labeling, while thesaliency of other labels is much smaller.
It isthus reasonable to assume that by addressing onlyprominent labels, we address a level of granularitythat is uniform and basic (to the annotation schemeused).
As a result, by asking the induced grammarto produce P labels, we reduce arbitrariness andenable our testing to focus on our success in iden-tifying the basic phenomena in the target grammar.5For completeness, in Section 5 we provide results for thisgrammar using greedy mapping evaluation.
LL mapping eval-uation cannot be performed when the numbers of induced andtarget labels differ.6Personal communication with the authors.0 5 10 15 20 25 302030405060708090100K most frequent labels%of constituentsNEGRA10BROWN10WSJ10CTB10Figure 1: For each k, the fraction of constituentslabeled with the k most frequent labels, for WSJ10(solid), Brown10 (triangles), NEGRA10 (dashed)and CTB10 (dotted).
In all corpora, more than95% of the constituents are labeled using less than10 prominent labels.As a result, we generated two grammars for eachcorpus we experimented with, one having T labelsand the other having P labels.Clustering.
we stop BMM when no improvementto its objective function is possible, and cluster thelabels to conform to the size constraint.
7Denote the number of labels in the inducedgrammar with M , the set of D most frequent in-duced labels with A, and the set consisting of theother induced labels with B (|B| = M ?
D).
IfM 6> D, there is nothing to do since the con-straint holds.
Otherwise, we map each label inB to the label in A that exhibits the most simi-lar syntactic behavior, as follows.
We constructa feature vector representation of each of the la-bels, using 3M + |K| features, where K is the setof POS tags in the corpus.
The first M featurescorrespond to parent-child relationships betweeneach of the induced labels and the represented la-bel.
The i-th feature (i ?
[1,M ]) is the number oftimes the i-th label is the parent of the representedlabel.
Similarly, the next M features correspondto child-parent relationships, the next M featurescorrespond to sibling relationships and the last |K|features correspond to the number of times eachPOS tag is the leftmost POS tag in a constituentlabeled by the represented label.
Note that in orderto compute the values of the first 3M features, weneeded an initial labeling on the induced bracket-ings; this is the main reason for using the BMMstage.For each label bi?
B, we compute the cosine7It is possible to force BMM to iterate until a desired num-ber of induced labels (T or P ) is achieved.
However, the in-duced grammars are of very low quality (see Section 5).724metric between its vector bviand that of every aj?A, mapping bito the label ajwith which it obtainsthe highest score:Map(bi) = argmaxjbvi?
avj|bvi||avj|(3)The cosine metric grows when the same coordi-nates (features) in both vectors have higher values.As a result, vectors with high values of the samefeatures (corresponding to similar syntactic behav-ior) get high scores.4 Experimental SetupWe evaluated our algorithm on English, Germanand Chinese corpora: the WSJ Penn Treebank,containing economic English newspaper articles,the Brown corpus, containing various English gen-res, the Negra corpus (Brants, 1997) of Germannewspaper text, and version 5.0 of the ChinesePenn Treebank (Xue et al, 2002).
In each cor-pus, we used the sentences of length at most 108,numbering 7422 (WSJ10), 9117 (Brown10), 7542(NEGRA10) and 4626 (CTB10).For each corpus the following T and P valueswere used: WSJ10: 26, 8; Brown10: 28, 7; NE-GRA10: 22, 6; CTB10: 24, 9.
Each number pro-duces a different grammar.For labeled f-score evaluation, the induced la-bels should be mapped to the target labels9.
Weevaluated with two different mapping schemes.For each pair (Xi, Yj) of induced and target labels,let CXi,Yjbe the number of times they label a con-stituent having the same span in the same sentence.Following (Haghighi and Klein, 2006) we applieda greedy (many to one) mapping where the map-ping is given by Map(Xi) = argmaxYjCXi,Yj.This greedy mapping tends to map many inducedlabels to the same target label, and is thereforehighly forgiving of large mismatches between thestructures of the induced and target grammars.Hence, we also applied a label-to-label (LL) map-ping, computed by reducing this problem to op-timal assignment in a weighted complete bipar-tite graph, formally defined as follows.
Given aweighted complete bipartite graph G = (X ?Y ;X ?
Y ) where edge (Xi, Yj) has weight wij,8Excluding punctuation and null elements, according tothe scheme of (Klein, 2005).9There are many possible methods for evaluating cluster-ing quality (Rosenberg and Hirschberg, 2007).
For our task,overall f-score is a very natural one.
We will address othermethods in future papers.find a (one-to-one) matching M from X to Y hav-ing a maximal weight.
In our case, X is the set ofmodel symbols, Y is the set of T or P most fre-quent target symbols (depending on the desired la-bel set size used), and wij:= CXi,Yj, computed asin greedy mapping (the number of times xiand yjshare a constituent).
To make the graph complete,we add zero weight edges between induced andtarget labels that do not share any constituent.
TheKuhn-Munkres algorithm (Kuhn, 1955; Munkres,1957) solves this problem, and we used it to per-form the LL mapping (see also (Luo, 2005)).We assessed the overall quality of our algorithm,the quality of its labeling stage and the quality ofthe syntactic clustering (SC) stage.
For the over-all quality of the induced grammar (both brack-eting and labeling) we compare our results with(Haghighi and Klein, 2006), using their setup10.That setup was used for all numbers reported inthis paper.
Note that a random baseline wouldyield very poor results, so there is nothing to begained from comparing to it.We assessed the quality of the labeling (MDLand SC) stages alone, using only the correct brack-etings produced by the first stage of the algorithm.We compare to a random baseline on these correctconstituents that randomly selects (using a uniformdistribution) a label for each constituent among theset of labels allowed to the algorithm.To asses the quality of the third stage (SC)we compare the f-score performance of our threestages labeled trees induction algorithm (bracket-ing, MDL, SC) to an algorithm consisting of thefirst two stages only (bracketing and MDL) andthe accuracy of the two stages labeling algorithm(MDL, SC) to an algorithm where the syntacticclustering stage is replaced by a simpler method(MDL, random clustering).5 ResultsWe start with comparing our algorithm with(Haghighi and Klein, 2006), the only previouswork that produces labeled bracketing and wastested on large manually annotated corpora.
Theirrelevant models are PCFG ?
NONE and PCFG ?
CCM11.10Brackets covering a single word are not counted, multi-ple labels and the sentence level constituent are counted.
Twosentence level constituents are usually used: one for the rootsymbol at the top (which was not counted), and one real sym-bol (in WSJ10 it is usually, but not always, S), which wascounted.
We had verified the setup with the authors.11They focused on a different, semi-supervised, setting.725This Paper PCFG?
CCM PCFG ?
NONEWSJ10 59.5 35.3 26.3Table 1: F-scores of our algorithm and of the unsu-pervised models in (Haghighi and Klein, 2006) onWSJ10 (they did not test these models on the othercorpora we experimented with).The number of labels in their induced grammarequals the number of labels in the target grammar(26 for WSJ10), and they had used a greedy map-ping.
Table 1 shows that our algorithm achievesa superior f-score of 59.5% over their 35.3%.Haghighi and Klein (2006) did not experimentwith the NEGRA10 and Brown10 corpora, and hadused version 3.0 of CTB10 while we have used thesubstantially different version 5.0, so we can onlycompare our results on WSJ10.Table 2 shows the labeled recall, precision and f-score of our algorithm on the various corpora andmappings we use.
On Brown10, NEGRA10 andCTB10 (version 5.0) these are the first reportedresults for this task.
For reference, the table alsoshows the unlabeled f-score results of Seginer?sbracketing algorithm (our first stage)12.We can see that greedy mapping is indeed moreforgiving than LL mapping, for both T labels andP labels.
WSJ results are generally higher than forthe other corpora, probably because WSJ bracket-ing results are higher than for the other corpora.Comparing the left and right columns in eachof the table sections reveals that for greedy map-ping, mapping to a higher number of labels resultsin higher scores than mapping to a lower number.LL mapping behaves in exactly the opposite way.The explanation for this is that when we force themapping to cover all of the target labels (as doneby LL mapping for T labels), we move probabil-ity mass from the correct, heavy labels to smallerones, thereby magnifying errors.Table 4 addresses the quality of the whole la-beling stage (MDL and SC) and of the SC stage.We report the quality of our labels (top line foreach corpus in the table) the random baseline la-bels (third line) and the labels of an algorithmwhere MDL is performed and the syntactic clus-tering is replaced by a random clustering (RC) al-gorithm that, given a label L that is not one of theT or P most frequent labels, randomly selects oneof the most frequent labels and adds L to its clus-12The numbers slightly differ from those in Seginer?s paper,since we use the (Haghighi and Klein, 2006) setup.Greedy LLT P T PWSJ10MDL,SC 80 67 47 59MDL,RC 67 61 37 42Rand.
Base.
30 30 5 14Error Reduction 39%,71% 15%,53% 16%, 44% 29%, 52%Brown10MDL,SC 73 61 48 60MDL,RC 68 59 46 51Rand.
Base.
27 27 4 14Error Reduction 16%,63% 5%, 47% 4%, 46% 18%, 53%NEGRA10MDL,SC 79 72 65 72MDL,RC 73 69 54 58Rand.
Base.
39 39 5 17Error Reduction 22%,66% 10%,34% 24%,63% 33%,66%CTB10MDL,SC 70 67 44 55MDL,RC 36 32 40 45Rand.
Base.
29 29 5 12Error Reduction 53%,58% 51%, 54% 7%,41% 18%,49%Table 4: Pure labeling results (taking into accountonly the correct bracketings produced at stage 1),compared to the random and (MDL,RC) baselines.The left number in the Error Reduction lines slotscompares (MDL,SC) to (MDL,RC) and the rightnumber compares (MDL,SC) to random labeling.
(MDL,SC) algorithm is substantially superior.ter (second line).13 All three labeling algorithmsused Seginer?s bracketing and results are reportedonly for labels of correctly bracketed constituents.Reported are the algorithm and baselines accuracy(percentage of correctly labeled constituents afterthe mapping has been performed) and the error re-duction of the algorithm over the baselines (bottomline).
(MDL,SC) substantially outperforms boththe random baseline, demonstrating the power ofthe whole labeling stage, and the (MDL,RC) algo-rithm, demonstrating the power of the SC stage.We compared our grammars to the grammars in-duced by the first two stages (bracketing and thenMDL that stops when no DL improvement is pos-sible) alone.
Since the number of labels in thesegrammars is much larger than in the target gram-mar, only the evaluation with the greedy, many toone, mapping is performed.
Using greedy map-ping, the F-score of these grammars constitutes anupper bound on the F-score after the subsequentSC stage.
For WSJ10 (4944 labels), NEGRA10(5557 labels), CTB10 (2298 labels) and Brown10(3314 labels) F-score values are 64.6, 49.9, 38.7and 52.5 compared to F-score values of 59.5(50.2),45.6(42), 36.4(34.7) and 49.4(41.3) after mappingall induced labels to the T (P ) most frequent la-bels with SC (Table 2, ?greedy?
section).
The frac-13Our algorithm?s numbers can be deduced from Table 2.Results for all random baselines are averaged over 10 runs.726Greedy Mapping LL Mapping SeginerT labels P labels T labels P labels (unlabeled)Corpus R P F R P F R P F R P F FWSJ10 58 61 59.5 48.9 51.5 50.2 34.2 36.1 35.2 42.7 44.9 43.8 74.6NEGRA10 54.2 39.3 45.6 50 36.2 42 44.7 32.4 37.6 49.5 35.9 41.7 58.1CTB10 35.1 37.8 36.4 33.4 36 34.7 21.9 23.6 22.7 27.4 29.5 28.4 51.8Brown10 47.6 51.3 49.4 39.9 43 41.3 31.3 33.7 32.4 38.9 41.9 40.3 67.8Table 2: Labeled recall, precision and f-score of our algorithm, mapping model labels into target labelsgreedily (left) and using LL mapping (right).
The number of induced labels was set to be the totalnumber T of target labels or the number P of prominent labels in the target grammar (WSJ10: 26, 8;Brown10: 28, 7; NEGRA10: 22, 6; CTB10: 24, 9).
Also shown are Seginer?s unlabeled bracketingresults (rightmost column), which constitute an upper bound on the quality of subsequent labeling steps.WSJ10 Brown10Label T labels P labels T labels P labelsR P F R P F R P F R P FS 77.1 77.6 77.3 75.4 67.9 71.5 72.3 60.9 66.1 69.3 63.2 66.1NP 8.5 79.5 15.4 19.8 61.6 30 10.7 79.3 18.9 15.6 78 26VP 20.4 67.6 31.3 64.2 36.7 46.7 9.8 72.5 17.3 14.1 59 22.8PP 40.8 63.5 49.7 8 8.9 8.4 17.4 59.2 26.9 75.5 14.4 24.2Table 3: Recall, Precision and F-score for constituents labeled with the 4 most frequent labels in theWSJ10 and Brown10 test sets.
LL mapping is used for evaluation.tion of constituents covered by the T (P ) most fre-quent labels before mapping with SC is 0.42(0.29),0.33(0.23), 0.58(0.45) and 0.66(0.42), emphasiz-ing the effect of SC on the final result.MDL finds the best merge at each iteration.
In-stead of stopping it when no DL gains are possi-ble, we can keep merging after the deltas becomeworse than the total DL, stopping only when thedesired number of labels (T or P ) is achieved.
Wetried this version of a (bracketing and MDL) algo-rithm and obtained grammars of very low quality.This further demonstrates the importance of the SCstage.Table 3 shows results for the four most frequentlabels of WSJ10 and Brown10 .6 ConclusionUnsupervised grammar induction is a central re-search problem, possessing both theoretical andpractical significance.
There is great value in pro-ducing an output format consistent with and evalu-ated against formats used in large human annotatedcorpora.
Most previous work of that kind producesunlabeled bracketing or dependencies.
In this pa-per we presented an algorithm that induces labeledbracketing.
The labeling stages of the algorithmuse the MDL principle to induce an initial, rela-tively large, set of labels, which are then clusteredusing syntactic features.
We discussed the issue ofthe desired number of labels, and introduced theconcept of prominent labels, which allows us cov-erage of the basic and most salient level of a targetgrammar.
Labels are clearly an important aspect ofgrammar induction.
Future work will explore theirsignificance for applications.Evaluating induced labels is a complex issue.We applied greedy mapping as in previous work,and showed that our algorithm significantly out-performs it.
In addition, we introduced LL map-ping, which overcomes some of the shortcomingsof greedy mapping.
There are several other possi-ble methods for evaluating labeled induced gram-mars, and we plan to explore them in future work.We evaluated on large human annotated corporaof different English domains and three languages,and showed that our labeling stages, and specif-ically the SC stage, outperform several baselinesfor all corpora and mapping methods.Acknowledgments.
We thank Gideon Borensz-tajn and Yoav Seginer for their help.ReferencesPieter Adriaans, 1992.
Learning Language from aCategorical Perspective.
Ph.D. thesis, University ofAmsterdam.Rens Bod, 2006a.
An All-Subtrees Approach to Un-supervised Parsing.
Proc.
of the 44th Meeting of theACL.Rens Bod, 2006b.
Unsupervised Parsing with U-DOP.Proc.
of CoNLL X.727Rens Bod, 2007.
Is the End of Supervised Parsing inSight?
Proc.
of the 45th Meeting of the ACL.Gideon Borensztajn and Willem Zuidema, 2007.Bayesian Model Merging for Unsupervised Con-stituent Labeling and Grammar Induction.
TechnicalReport, ILLC .
http: //staff.science.uva.nl/?gideon/Thorsten Brants, 1997.
The NEGRA Export Format.CLAUS Report, Saarland University.Thorsten Brants, 2000.
TnT: A Statistical Part-Of-Speech Tagger.
Proc.
of the 6th Applied NaturalLanguage Processing Conference.Stanley F. Chen, 1995.
Bayesian grammar inductionfor language modeling.
Proc.
of the 33th Meeting ofthe ACL.Alexander Clark, 2001.
Unsupervised Language Ac-quisition: Theory and Practice.
Ph.D. thesis, Uni-versity of Sussex.Alexander Clark, 2003.
Combining Distributional andMorphological Information for Part of Speech In-duction.
Proc.
of the 10th Meeting of the EuropeanChapter of the ACL.Willliam A. Croft, 2001.
Radical Construction Gram-mar.
Cambridge University Press.Carl G. de Marcken, 1995.
Unsupervised LanguageAcquisition.
Ph.D. thesis, MIT.Walter Daelemans, 1995.
Memory-based lexical ac-quisition and processing.
Lecture Notes In ArtificialIntelligence, 898:85?98.Simon Dennis, 2005.
An exemplar-based approach tounsupervised parsing.
Proceedings of the 27th Con-ference of the Cognitive Science Society.Adele E. Goldberg, 2006.
Constructions at Work.
Ox-ford University Press.Peter Grunwald, 1994.
A minimum description lengthapproach to grammar inference.
Lecture Notes InArtificial Intelligence, 1004 : 203-216.Aria Haghighi and Dan Klein, 2006.
Prototype-drivengrammar induction.
Proc.
of the 44th Meeting of theACL.Jin?Dong Kim, Tomoko Ohta, Yuka Teteisi andJun?ichi Tsujii, 2003.
GENIA corpus ?
a seman-tically annotated corpus for bio-textmining.
Bioin-formatics, 19:i180?i182, Oxford University Press,2003.Dan Klein and Christopher Manning, 2002.
A gener-ative constituent-context model for improved gram-mar induction.
Proc.
of the 40th Meeting of the ACL.Dan Klein and Christopher Manning, 2004.
Corpus-based induction of syntactic structure: Models of de-pendency and constituency.
Proc.
of the 42nd Meet-ing of the ACL.Dan Klein, 2005.
The unsupervised learning of natu-ral language structure.
Ph.D. thesis, Stanford Uni-versity.Harold W. Kuhn, 1955.
The Hungarian method forthe assignment problem.
Naval Research LogisticsQuarterly, 2:83-97.Pat Langley and Sean Stromsten, 2000.
Learningcontext-free grammars with a simplicity bias.
Proc.of the 11th European Conference on Machine Learn-ing.Xiaoqiang Luo, 2005.
On coreference resolution per-formance metrics.
Proc.
of the 2005 Conference onEmpirical Methods in Natural Language Processing.James Munkres, 1957.
Algorithms for the Assignmentand Transportation Problems.
Journal of the SIAM,5(1):32?38.Katsuhiko Nakamura, 2006.
Incremental learning ofcontext free grammars by bridging rule generationand search for semi-optimum rule sets.
Proc.
of the8th ICGI.Georgios Petasis, Georgios Paliouras and VangelisKarkaletsis, 2004.
E-grids: Computationally effi-cient grammatical inference from positive examples.Grammars, 7:69?110.Andrew Rosenberg and Julia Hirschberg, 2007.Entropy-based external cluster evaluation measure.Proc.
of the 2007 Conference on Empirical Methodsin Natural Language Processing.Yoav Seginer, 2007.
Fast Unsupervised IncrementalParsing.
Proc.
of the 45th Meeting of the ACL.Noah A. Smith and Jason Eisner, 2006.
AnnealingStructural Bias in Multilingual Weighted GrammarInduction .
Proc.
of the 44th Meeting of the ACL.Zach Solan, David Horn, Eytan Ruppin, and ShimonEdelman, 2005.
Unsupervised learning of naturallanguages.
Proceedings of the National Academy ofSciences, 102 : 11629?11634.Andreas Stolcke.
1994.
Bayesian Learning of Proba-bilistic Language Models.
Ph.D. thesis, Universityof of California at Berkeley.Andreas Stolcke and Stephen M. Omohundro, 1994.Inducing probabilistic grammars by Bayesian modelmerging .
Proc.
of the 2nd ICGI.Menno van Zaanen, 2001.
Bootstrapping Structureinto Language: Alignment-Based Learning.
Ph.D.thesis, University of Leeds.J.
Gerard Wolff, 1982.
Language acquisition, datacompression and generalization.
Language andCommunication, 2(1): 57?89.Nianwen Xue, Fu-Dong Chiou and Martha Palmer,2002.
Building a large?scale annotated Chinese cor-pus.
Proc.
of the 40th Meeting of the ACL.728
