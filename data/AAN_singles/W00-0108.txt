Example-based Complexity--Syntax and Semantics as theProduction of Ad-hoc Arrangements of ExamplesRobert John FREEMANrj freeman@email.comAbstractComputational linguists have traditionallysought to model language by findingunderlying parameters which governnumerous examples.
I describe a differentapproach which argues that numerousexamples themselves, by virtue of theirmany possible arrangements, provide theonly way to specify a sufficiently rich set of"parameters".Essentially I argue for a differentrelationship between example andparameter.
With examples primary, andparameterizafions of them secondary, thereal "productions".
Rather than representinga redundant complexity, examples houldactually be seen as a simplification, a basisfor the numerous arrangements of their"parameterizations".Another way of looking at it is to say I arguearrangements of examples, rather thansimply revealing underlying parameters,represent in themselves an ignored resourcefor the modelling of syntactic, and semantic,complexity.I have implemented a small, working,"shallow parser" based on these ideas.Introduction--Machine Learning, Data,and Parameterizations!
contrast my work with Machine Learning.There are similarities in the emphasis on theanalysis of relationships among data, but thereare also differences in the assumptions about henature of the system.
I think there has been atacit assumption in Machine Learning thatlanguage system consists of underlyingparameters which generate a variety ofexamples.
My argument is that you can turn thatrelationship around and get a great deal moredescriptive power in the form of varyingparameterizations of the order in a set ofexamples.Under the umbrella of Machine Learning Iinclude a wide variety of data based analyses oflanguage which have become popular in recentyears.
Both distributed and statistical data basedmodels fit in that category: back-propagationnetworks, Hidden Markov Models, maximumentropy parametefizafions.
Apart from theiremphasis on data, however, they have one thingin common, and in common with earliersymbolic attempts to codify language system.They all hypothesize parameters for distributionsof data.
I say it is worth considering that theessence of language is not in such underlyingparameters but the collections of examples weseek them through.
That there are no underlyingparameters, only the chaos of example, much asis the case in a population of people (see alsoKenneth Pike "analogies between linguisticstructure and the structure of society", in deBeaugrande ( 1991)).One way to describe this is to say that languagemight be "irreducibly distributed".
A systemwhere a collection of examples is the smallestset which describes all its structure.
Althoughthere might be different levels of thisindependence (along with differing abilities toparameterize: viz.
phonology, morphology,syntax).
We might contrast irreduciblydistributed systems with those which areparametrically distributed, like a letterrecognition system.
Certainly, however, wecould contrast them with statistical, systems,where only the likelihood of the outcomes isvariable.47R from N and the Descriptive Power ofSetsThe best thing about such "irreduciblydistributed" systems i  their power.The number of combinations of R objects takenfrom N is C(N,R) = N!/(N-R)!R!.
This is thenumber of "word association classes" N wordassociations can model, for instance.The idea that we can model syntactic lasses as"word association classes" is not new.
There arenumerous tudies dating from the early 1990'sand before which take this approach e.g.Schuetze (1993), Finch (1993); and Powers(1996) lists references back to Pike'sTagmemics.
What is different in my approach isthe assumed relationship between these classesand the data which reveal them.
If the variety ofexample can be generated by a small number ofabstract parameters then we expect one set ofrelationships among that data to be moreimportant than the others.
If on the other handwe consider the full range of relationshipspossible among all the examples then we havean enormous range of structure at our disposal.Given the problems we have had describinglanguage according to parameters, it issurprising that we have not more widelyconsidered the attraction of this power.Consider the evidence that we need this power:a) StructureCollocation, phraseology.
The data basedanalysis of language has bought home more andmore strongly that some structure is beyond anylogic we can enumerate.
Face to face with thereality of use this realization has been mostwidely accepted in areas of linguistics whichdeal with language acquisition and teaching.Examples of relevant discussions are Pawleyand Syder (1983), Nattinger (1980), Weinert(1995).
We are talking about explaining whyyou might say "strong tea" but not "powerfultea".In practical terms a processor basedfundamentally ondistributions should be able totell that "strong tea" is idiomatic and "powerfultea" less so because the "word associationdistributions", say, of "strong" and "powerful"are different in detail, though not in generalities.A system based on labels, an assumption ofunderlying parameters, will not be able to dothat (for a set of labels smaller than the set of allsuch distinct utterances).An irreducibly distributed representation givesus the power to model collocation.
We wouldneed a different syntactic class for everycollocational restriction otherwise.b) MeaningN!/(N-R)!R!
groupings give you an essentiallyinfinite set of configurations.
We have the powerto associate a different configuration witheverything we might ever want to say, if we like.In fact, by default we will do so.
This means wehave the power to represent not only syntacticidiosyncrasy, but the complexity of meaning,directly.The idea of meaning implied by the associationis interesting in itself, h is an organization ofdata.
But this is reasonable.
And if we accept itthen we have a fundamental definition ofmeaning in terms we can quantify.
Meaning issynonymous with an organization of data:events, observations.
New organization equalsnew meaning.There is an interesting topical analogy to bemade here: a Web search engine.
In a sense anycollection of documents found "represent" hemeaning of a set of search keys.
There are manymore subtleties of collection possible than canever be labeled in an index.In a way my argument isjust.that if we want tomodel the full complexity of syntacticrestriction, or semantic subjectivity, we have nochoice but to demote categories from beingcentral, make them a product, and base them onthe reorganization f content much the way theyare treated in most Web search engines.Such an irreducibly distributed definitionexplains many puzzling properties of thought.
Itprovides anatural mechanism for how:48?
new concepts can be created (novelreorganization ofold examples--"Aha\[")?
new meaning can be communicated (I forceyou to reorganize your examples in the wayI've just reorganized mine)?
language (and conceptual) drift can occur(slow shift in balance of examples).As well as the usual useful properties ofdistributed representations:?
flexibility (the group can vary)?
robustness (it does not matter of a fewelements are missing)?
ambiguity (intersection sets)?
subjectivity (sub-sets etc.
)There is also an interesting tie in between this(meaning, and the primacy of data overparameter) and the vigorous "rebel" linguisticschool of Systemic Functional Grammar.
Mostimportantly in SFG the only irreducibledefinition of meaning, or structure, is a set ofcontrasts between events, or observations.Unfortunately in SFG an overemphasis onabstract parameters (function/meaning) meansthat in practice the flail power of contrastsamong sets to model complexity is not applied.Nevertheless, there are strong parallels betweenmy model and the core tenets of SystemicFunctional Grammar.
I find that a naturalanalysis according to the principles I haveoutlined above results in structure along lines offunctional category.
In fact the associationgroupings on which I base my analysis lead meto propose an "inverse" relationship (in a sensethat can be precisely defined) betweenfunctional category, about which SFG isdescribed, and categories based on syntacticregularities of the type which have traditionallybeen seen as important.A Simple "Association Parser"I have implemented a small "association parser"based on these principles and the initial resultshave been interesting.
I provide a list of typical"parses" in the appendix.
Essentially it scoresthe grammaticality and provides a structuralbreakdown of each string of words it ispresented with.
Among more interestingobservations, as I mentioned above, is the factthat my parser seems to naturally identifystructure along lines of functional equivalence.Rather like the kind of analysis a SystemicFunctional Grammarian might favor.Since processing is essentially a search over adatabase for similar examples the mainbottleneck is the inefficiency of a serialprocessor for nearest neighbor search.
There aretwo key complexities.
The search over one Ihave managed to reduce to linear time.
The otherremains to be resolved.ReferencesBeaugrande, Robert de (1991) Linguistic Theory: TheDiscourse of Fundamental Works, section 5.84,Harlow: Longman.Finch, Steven (1993) Finding Structure in Language.Ph.D.
Thesis, University of Edinburgh.Nattinger, James R: (1980) A lexical PhraseGrammar for ESL, TESOL Quarterly Vol.
XIV.,No.
3, pp.
33%334.Pawley, A.
& Syder F. (1983) Two puzzles forlinguistic theory: nativelike selection and nativelikefluency, in L Richards and IL Schmidt (eds.)
1983:Language and Communication, pp.
191-226.London: Longman.Powers, D. M. W. (1996) Unsupervised learning oflinguistic structure: An empirical evaluation,International Journal of Corpus Linguistics 1#2.Schuetze, H. (1993) Distributed SyntacticRepresentations with an Application to Part-of-Speech Tagging, 1993 IEEE InternationalConference on Neural Networks, p1504-9 vol.
3.Weinert, Regina.
(1995) The Role of FormulaicLanguage in Second Language Acquisition: AReview, Applied Linguistics, Vol.
16, No.
2, pp.181-205.Appendix--Examples of Parses Producedby my "Association Parser" Prototypemake some productsParsed: (make (some products)), score: 1.329954Parsed: ((make some) products), score: 0.023665make some moneyParsed: (make (some money)), score: 1.555408Parsed: ((make some) money), score: 0.04205949make a carParsed: (make (a car)), score: 5.689303Parsed: ((make a) car), score: 2.120204make another carParsed: (make (another car)), score: 1.642482Parsed: ((make another) car), score: O.
189554make another tryParsed: ((make another) try), score: 0.051537Parsed: (make (another try)), score: 0.039471go with the presidentParsed: ((go with) (the president)), score:7.983729Parsed: (go (with (the president))), score:4.620297Parsed: (go ((with the) president)), score:0.771305Parsed: (((go with) the) president), score:0.318181Parsed: ((go (with the)) president), score:0.065606I try to goParsed: (i ((try to) go)), score: 4.343059Parsed: (((i try) to) go), score: 1.297454Parsed: ((i (try to)) go), score: 1.174891Parsed: (i (try (to go))), score: 0.553270Parsed: ((i try) (to go)), score: 0.474397the election resultsParsed: (the (election results)), score: 89.247596Parsed: ((the election) results), score: 15.212562they held an electionParsed: (they (held (an election))), score:0.000238Parsed: ((they held) (an election)), score:0.000007Parsed: (((they held) an) election), score:0.000000go with herParsed: ((go with) her), score: 9.073902Parsed: (go (with her)), score: O.
10743550
