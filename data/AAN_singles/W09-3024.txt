Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 138?141,Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLPUnsupervised Detection of Annotation InconsistenciesUsing Apriori AlgorithmVa?clav Nova?k Magda Raz?
?mova?Institute of Formal and Applied LinguisticsCharles University in PragueCzech Republic{novak,razimova}@ufal.mff.cuni.czAbstractWe present a new method for automateddiscovery of inconsistencies in a complexmanually annotated corpora.
The pro-posed technique is based on Apriori al-gorithm for mining association rules fromdatasets.
By setting appropriate parame-ters to the algorithm, we were able to au-tomatically infer highly reliable rules ofannotation and subsequently we searchedfor records for which the inferred ruleswere violated.
We show that the viola-tions found by this simple technique areoften caused by an annotation error.
Wepresent an evaluation of this technique ona hand-annotated corpus PDT 2.0, presentthe error analysis and show that in the first100 detected nodes 20 of them containedan annotation error.1 IntroductionComplex annotation schemes pose a serious chal-lenge to annotators caused by the number of at-tributes they are asked to fill.
The annotationtool can help them in ensuring that the values ofall attributes are from the appropriate domain butthe interplay of individual values and their mutualcompatibility are at best described in annotationinstructions and often implicit.
Another source oferrors are idiomatic expressions where it is diffi-cult for the annotator to think about the categoriesof a word which often exists only as a part of theidiom at hand.In our approach, detection of annotation in-consistencies is an instance of anomaly detection,which is mainly used in the field of intrusion de-tection.
Traditionally, the anomaly detection isbased on distances between feature vectors of indi-vidual instances.
These methods are described inSection 2.
Our new method presented in Section 3uses the data-mining technique Apriori (Borgeltand Kruse, 2002) for inferring high-quality rules,whose violation indicates a possible annotator?smistake or another source of inconsistency.
Wetested the proposed method on a manually anno-tated corpus and described both the data and theexperimental results in Section 4.
We conclude bySection 5.2 Related WorkUnsupervised anomaly detection has been shownto be viable for intrusion detection (Eskin et al,2002).
The unsupervised techniques rely on fea-ture vectors generated by individual instances andtry to find outliers in the vector space.
Thiscan be done using clustering (Chimphlee et al,2005), Principle Component Analysis (Hawkins,1974), geometric methods (Eskin et al, 2002) andmore (Lazarevic et al, 2003).The difference between our method and previ-ous work lies mainly in the fact that instead us-ing vector space of features, we directly infer an-notation rules.
The manual annotation is alwaysbased on some rules, some of which are containedin the annotation manual but many others are moreor less implied.
These rules will have their confi-dence measured in the annotated corpus equal to1 or at least very close (see Section 3 for defi-nition of confidence).
In our approach we learnsuch rules and detect exceptions to the most cred-ible rules.
The rules are learned using the com-mon Apriori algorithm (Borgelt and Kruse, 2002).Previously, rules have been also mined by GUHAalgorithm (Ha?jek and Havra?nek, 1978), but not inthe anomaly detection context.3 Method DescriptionOur process of anomaly detection comprises twosteps: rules mining and anomaly search.1383.1 Rules MiningThe association rules mining was originally de-signed for market basket analysis to automaticallyderive rules such as ?if the customer buys a tooth-paste and a soap, he is also likely to buy a tooth-brush?.
Every check-out x = (x1, x2, .
.
.
, xN )is modeled as a draw from an unknown probabil-ity distribution ?, where N is the total number ofitems available at the store and xi is the numberof items of type i contained in the shopping cart.Further, we define event Ej = {x|xj > 0}, i.e.,the event that the shopping cart contains the itemj.In this model, we define a rule A = (L,R)as a tuple where the left side L and the rightside R are sets of events Ej .
For instance sup-pose that the toothpaste, toothbrush and soap haveindices 1, 2 and 3, respectively.
Then the ex-ample rule mentioned above can be written asAexample = ({E1, E3}, {E2}), or alternatively{E1, E3} ?
{E2}.
For every rule A = (L,R)we define two important measures: the supports(A) and the confidence c(A):s ((L,R)) = P??
?l?L(l) ?
?r?R(r)??
(1)c ((L,R)) = P???r?R(r)????l?L(l)??
(2)In our example the support is the probabilitythat a cart contains a toothpaste, a toothbrush and asoap.
The confidence is the probability that a cartcontains a toothbrush given the cart contains botha toothpaste and a soap.The input of the Apriori algorithm (Borgelt andKruse, 2002) consists of a sample from the proba-bility distribution?, the threshold of the estimatedconfidence, the threshold of the estimated supportand the maximum size of rules.
Using this datathe Apriori algorithm lists all rules satisfying therequired constraints.In the context of market basket analysis the con-fidence is rarely anywhere close to one, but in thecase of linguistic annotation, there are rules thatare always or almost always followed.
The confi-dence of these rules is very close or equal to one.The Apriori algorithm allows us to gather rulesthat have the confidence close to one and a suf-ficient support.3.2 Anomaly SearchAfter extracting the highly confident rules we se-lect the rules with the highest support and find theannotations where these rules are violated.
Thisprovides us with the list of anomalies.
The searchis linear with the size of the data set and the sizeof the list of extracted rules.4 Experiments4.1 Data and ToolsThe experiments were carried out using the R sta-tistical analysis software (R Development CoreTeam, 2006) using the arules library (Borgelt andKruse, 2002).
The dataset used was full manu-ally annotated data of Prague Dependency Tree-bank 2.0 (PDT 2.0).
PDT 2.0 data were annotatedat three layers, namely morphological, analyti-cal (shallow dependency syntax) and tectogram-matical (deep dependency syntax; (Hajic?
et al,2006)).
The units of each annotation layer werelinked with corresponding units of the precedinglayer.
The morphological units were linked di-rectly with the original text.
The annotation atthe tectogrammatical layer was checked automat-ically for consistency with the annotation instruc-tions (S?te?pa?nek, 2006), however, using our tech-nique, we were still able to automatically find er-rors.
The experimental dataset (full PDT 2.0 dataannotated at all three layers) contained 49,431 sen-tences or 833,195 tokens.4.2 Experimental Setup and Error AnalysisIn our experimental setup, every check-out (i.e.,every draw from the probability distribution ?
)contains all attributes of one tectogrammaticalnode and its governor.
The attributes extractedfrom the nodes are listed in Table 1.
Thus everycheck-out has exactly 52 items, 26 coming fromthe node in question and 26 coming from its gov-ernor.This being input to the Apriori algorithm, weset the maximal size of rules to 3, minimal supportto 0.001 and minimal confidence to 0.995.
Whenthe rules were extracted, we sorted them accord-ing to the descending confidence and stripped allrules with confidence equal to 1.
Using the re-maining rules, we searched the corpus for the vio-lations of the rules (starting from the top one) untilwe found first 100 suspicious nodes.
We manuallyanalyzed these 100 positions and found out that 20139Attribute Descriptionfunctor semantic values of deep-syntactic dependency relationsis dsp root root node of the sub-tree representing direct speechtfa contextual boundnessis generated element not expressed in the surface form of the sentenceis member member of a coordination or an appositionis name of person proper name of a personis parenthesis node is part of a parenthesisis state modification with the meaning of a statesentmod sentential modalitysubfunctor semantic variation within a particular functoraspect aspect of verbsdegcmp degree of comparisondeontmod an event is necessary, possible, permitted etc.dispmod relation (attitude) of the agent to the eventgender masculine animate, masculine inanimate, feminine or neuterindeftype types of pronouns (indefinite, negative etc.
)iterativeness multiple/iterated eventsnegation a negated or an affirmative formnumber singular or pluralnumertype types of numerals (cardinal, ordinal etc.
)person reference to the speaker/hearer/something elsepoliteness polite formresultative event is presented as the resulting statesempos semantic part of speechtense verbal tense (simultaneous, preceding or subsequent events)verbmod verbal mood (indicative, conditional or imperative)Table 1: Attributes of tectogrammatical nodes used as the input to the rule mining algorithm.
Theircomplex interplay can hardly be fully prescribed in an annotation manual.of them constitute an annotation error.
Examplesof extracted rules follow.is parenthesis:1& governor:functor:PAR?
governor:is parenthesis:1(3)Rule 3 states that if a tectogrammatical node hasthe attribute is parenthesis set to 1 (i.e., the nodeis part of a parenthesis) and at the same time thegovernor of this node in the tectogrammatical treehas its functor set to PAR (it is the root node ofnodes which are parenthesis in a sentence), thegovernor?s is parenthesis attribute is also set to 1.Using this rule we detected 6 nodes in the corpuswhere the annotator forgot to fill the value 1 in theis parenthesis attribute.
There were no false posi-tives and this automatically extracted rule is likelyto be added to the consistency checking routinesin the future.functor:RSTR& gender:nr?
number:nr(4)Rule 4 states that RSTR nodes (mostly attributesof nouns) with nr gender (indeterminable gender)also have indeterminable number.
Our procedurelocated a node where the annotator correctly de-termined the number as sg but failed to recognizethe gender (namely, masculine inanimate) of thenode.is member:1& dispmod:nil?
tense:nil(5)Rule 5, stating that for nodes with is member setto 1 the nil value (which means that none of thedefined basic values is suitable) of the dispmodattribute implicates the nil value of the tense, isan example of a rule producing false positives.140Due to the data sparsity problem, there are not somany nodes satisfying the premises and in mostof them the nil value were simply filled in theirtense attribute.
However, there are (rather rare)transgressive verb forms in the corpus for whichthe correct annotation violates this rule.
Many ofthem were found by this procedure but they aremore anomalies in the underlying text rather thananomalies in the annotation.
An interesting pointto note is that there were several rules exhibitingthis behavior with different first premises (e.g.,gender:anim & governor:dispmod:nil ?
gover-nor:tense:nil ).
The more general rule (dispmod:nil?
tense:nil ) would not get enough confidence, butby combining it with other unrelated attributes, theprocedure was able to find rules with enough con-fidence, although not very useful ones.resultative:res0& governor:degcmp:pos?
governor:sempos:adj.denot(6)Rule 6 is an example of a successful rule.
Itstates that nodes that govern a non-resultative nodeand have the positive degree of comparison are al-ways denominating semantic adjectives (i.e., com-mon adjectives such as black or good ).
Usingthis rule we detected a node where the annotatorscorrectly determined the semantic part of speechas adj.quant.grad (quantificational semantic adjec-tive) but failed to indicate degcmp:comp.5 Conclusion and Future WorkWe have described a fast method for automatic de-tection of inconsistencies in a hand-annotated cor-pus using easily available software tools and eval-uated it showing that in top 100 suspicious nodesthere were an error in 20 cases.
This method seemto work best for high-quality annotation where theerrors are rare: in our experiments the rules had toachieve at least 99.5% confidence to be includedin the search for violations.
However, it can alsopoint out inconsistencies in the annotation instruc-tions by revealing the suspicious data points.
Wehave shown the typical rules and errors revealedby our procedure.The method can be generalized for any manu-ally entered categorical datasets.
The rules cantake values from multiple data entries (nodes,words, etc.)
into account to capture the de-pendency in the annotation.
Other rule-miningtechniques such as GUHA (Ha?jek and Havra?nek,1978) can be used instead of Apriori.AcknowledgementThis work was supported by Czech Academyof Science grants 1ET201120505 and1ET101120503; by Ministry of Education, Youthand Sports projects LC536 and MSM0021620838.ReferencesChristian Borgelt and Rudolf Kruse.
2002.
Inductionof Association Rules: Apriori Implementation.
InProceedings of 15th Conference on ComputationalStatistics (Compstat), pages 395?400, Heidelberg,Germany.
Physica Verlag.W.
Chimphlee, Abdul Hanan Abdullah, MohdNoor Md Sap, S. Chimphlee, and S. Srinoy.
2005.Unsupervised Clustering methods for IdentifyingRare Events in Anomaly Detection.
In Proceed-ings of the 6th International Enformatika Confer-ence (IEC2005), Budapest, Hungary, October 26-28.E.
Eskin, A. Arnold, M. Prerau, L. Portnoy, andS.
Stolfo.
2002.
A geometric framework for un-supervised anomaly detection: Detecting intrusionsin unlabeled data.
In Data Mining for Security Ap-plications.
Kluwer.Petr Ha?jek and Toma?s?
Havra?nek.
1978.
Mechaniz-ing Hypothesis Formation; Mathematical Founda-tions for a General Theory.
Springer-Verlag, Berlin,Heidelberg, New York.Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, PetrSgall, Petr Pajas, Jan S?te?pa?nek, Jir???
Havelka,Marie Mikulova?, Zdene?k Z?abokrtsky?, and MagdaS?evc???kova?-Raz??mova?.
2006.
Prague DependencyTreebank 2.0.
CD-ROM, Linguistic Data Consor-tium, LDC Catalog No.
: LDC2006T01, Philadel-phia, Pennsylvania.D.
M. Hawkins.
1974.
The Detection of Errorsin Multivariate Data Using Principal Components.Journal of the American Statistical Association,69(346):340?344.A.
Lazarevic, A. Ozgur, L. Ertoz, J. Srivastava, andV.
Kumar.
2003.
A comparative study of anomalydetection schemes in network intrusion detection.
InProceedings of SIAM International Conference onData Mining.R Development Core Team, 2006.
R: A Language andEnvironment for Statistical Computing.
R Foun-dation for Statistical Computing, Vienna, Austria.ISBN 3-900051-07-0.Jan S?te?pa?nek.
2006.
Post-annotation Checking ofPrague Dependency Treebank 2.0 Data.
In Proceed-ings of the 9th International Conference, TSD 2006,number 4188 in Lecture Notes in Computer Science,pages 277?284.
Springer-Verlag Berlin Heidelberg.141
