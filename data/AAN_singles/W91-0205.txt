Logical Structures in the LexiconJohn F. SowaIBM Systems Research Institute500 Columbus AvenueThornwood, NY 10594emaU: sowa@watson.ibm.comAbstractThe lexical entry for a word must contain all the information eeded to construct a se-mantic representation for sentences that contain the word.
Because of that requirement,the formats for lexical representations must be as detailed as the semantic forms.
Simplerepresentations, such as features and frames, are adequate for resolving many syntacticambiguities.
But since those notations cannot represent all of logic, they are incapableof supporting all the function needed for semantics.
Richer semantic-based approacheshave been developed in both the model-theoretic tradition and the more computationalAI tradition.
Although superficially in conflict, these two traditions have a great deal incommon at a deeper level.
Both of them have developed semantic structures that are ca-pable of representing a wide range of linguistic phenomena.
This paper compares theseapproaches and evaluates their adequacy for various kinds of semantic information thatmust be stored in the lexicon.
It presents conceptual graphs as a synthesis of the logicistand AI representations designed to support he requirements of both.1 Semantics from the Point of View of the LexiconTo understand a semantic theory, start by looking at what goes into the lexicon.
In oneof the early semantic theories in the Chomskyan tradition, Katz and Fodor (1963) did infact start with the lexicon.
More recent heories, however, almost treat the lexicon as anafterthought.
Yet the essence of the theory is still in the lexicon: every element of thesemantic representation f a sentence ultimately derives from something in the lexicon.That principle is just as true for Richard Montague's highly formalized grammar as forRoger Schank's "scruffy" conceptual dependencies, scripts, and MOPs.Context and background knowledge are also important, since most sentences cannotbe understood in isolation.
This fact contradicts Frege's principle of compositionality,which says that the meaning of a sentence is derived from the meanings of the words itcontains.
Yet context can also be stated in words and sentences.
Even when nonlinguisticsurroundings are necessary for understanding a sentence, every significant feature couldbe encoded in a sentence; people normally encode such information when they tell a story.More general encyclopedic knowledge is contextual information that was learned at anearlier time; it too could be stated in sentences.
An extended Fregean principle shouldtherefore say that the meaning of a sentence must be derivable from the meanings of thewords in the sentence together with the meanings of the words in the sentences that de-scribe the relevant context and background knowledge.38Besides the meanings of words, grammar and logic are necessary to combine thosemeanings into a complete semantic representation.
But there are competing theoriesabout how much grammar and logic is necessary, how much is expressed in the lexicon,and how much is expressed in the linguistic system outside the lexicon.
Lexically basedtheories uggest that the grammar rules should be simple and that most of the syntacticcomplexity should be encoded in the lexicon.
One might even go further and say thatmost of the syntactic omplexity isn't syntactic at M1.
It is the result of interactions amongthe logical structures of the underlying concepts.
In his work on semanticMly based syn-tax, Dixon (1991) maintained that syntactic irregularities and idiosyncrasies are not acci-dental.
Instead, he showed that many of them can be predicted from the semantics of thewords.
Such theories imply that a system would only need a simple grammar to representa language if it had sufficiently rich semantic structures.A rich theory of semantics in the lexicon must also explain how the semantics got intothe lexicon.
A child could learn an initial stock of meanings by associating prelinguisticstructures with words.
But even those prelinguistic structures are shaped, polished, andrefmed by long usage in the context of sentences.
They are combined with the structureslearned from other words, and they are molded into patterns that are traditional in thelanguage and culture.
More complex, abstract, and sophisticated concepts are eitherlearned exclusively through language or through experiences that are highly colored andshaped by language.
For these reasons, the meaning representations i  the lexicon shouldbe derivable from the semantic representations for sentences.
As a working hypothesis,the two should be identical: the same knowledge representation language should be usedfor representing meanings in the lexicon and for representing the semantics of sentencesand extended iscourse structures.This paper explores the implications of that hypothesis.
Section 2 reviews severaldifferent lexical representations and their implications.
Section 3 compares the underlyingassumptions of the model-theoretic and AI traditions and shows that they are computa-tionaUy more compatible than their metaphysics would suggest.
Section 4 illustrates theuse of conceptual graphs for representing the semantic ontent of lexical entries.
Section5 shows how such representations can be used to handle aspects of language that requireboth logic and background knowledge.2 Review of Lexical RepresentationsFeatures are one of the oldest and simplest semantic representations.
In his UniversalCharacteristic, Leibniz (1679) assigned prime numbers to semantic primitives and repres-ented compound concepts by products of the primes.
If RATIONAL were representedby 2 and ANIMAL by 3, then their product 6 would represent RATIONAL ANIMALor HUMAN.
That representation generates a lattice: concept A is a supertype of B if thenumber for A divides the number for B; the minimal common supertype of A and Bcorresponds to their greatest common divisor; and the maximal common subtype of Aand B corresponds to their least common multiple.
Leibniz tried to use his system tomechanize Aristotle's syllogisms, but a feature-based representation is too limited.
Itcannot distinguish different quantitiers or show how primitive concepts are related to oneanother in compounds.
Modem notations use bit strings instead of products of primes,but their logical power is just as limited as Leibniz's system of 1679.39In their feature-based system, Katz and Fodor (1963) factored the meaning of a wordinto a string of features and an undigested lump called a distinguisher.
Following is theirrepresentation forone sense of bachelor:bachelor ?
noun ?
(Animal) + (Male} ~ (Young}?
\[ fur seal when without a mate during the breeding time\].In this definition, noun is the syntactic marker; the features (Animal), (Male), and(Young} are the semantic markers that contain the theoretically significant information;and the phrase in brackets is the unanalyzed distinguisher.
Shortly after it appeared, theKatz-Fodor theory was subjected to devastating criticisms.
Although no one today usesthe theory in its original form, those criticisms are worth mentioning because many of themore modern approaches suffer from the same limitations:?
The sharp distinction between semantic features and the distinguisher is so funda-mental to the theory that it should have an enormous impact on the structures oflanguage and the normal use of language.
Yet there is no linguistic evidence fromsyntax or cooccurrence patterns to indicate that it has any effect whatever.?
The distinguisher is made up of words, each of which has its own meaning.
A com-plete semantic theory should explain how the meanings of the words in thedistinguisher contribute to the meaning of the whole.
But such an analysis wouldimply a deeper epresentation that underlies both the features and the distinguisher.?
The Katz-Fodor theory treats different word senses as if they were purely accidentalgroupings that have no relationship to one another.
Yet the four senses of bachelordo have something in common.
They all represent an immature or transitional stagethat leads to some further goal: a student who has completed an academic step onthe way to becoming a master or doctor; a young knight who is still an apprentice toanother; a seal on its way to full maturity as a patriarch of the herd; or art unmarriedman who has not yet started to form his own family.
The feature-distinguisher t orydoes not show the commonality; it cannot explain how these meanings developedfrom a common root or why they remain associated with the same word form.?
If the features had no deeper structure, there would be nothing to constrain theirpossible combinations.
Yet certain combinations, uch as (Abstract)+ (Color) or(Action) + (Weight), never occur.
More structure is needed in the theory to explainwhy such combinations are impossible.?
Finally, many features cannot be named with a single word.
Certain Mexican dialects,for example, make a distinction between a difunto, a deceased person who was marriedat the time of death, and an angelito, a deceased person who was not married at thetime of death (El Guindi 1986).
A feature such as (Married-at-the-time-of-death) isso blatantly nonprirnitive that it cries out for a theory that represents deeper struc-tures.These criticisms do not imply that features are useless.
But they indicate that features arederived from some deeper, more fundamental representation.Despite their limitations, features are attractive because they are easy to program, andthey generate convenient computational structures, uch as Leibniz's lattice.
Yet a latticegenerated by features permits too many impossible combinations.
A well-structureddefinitional mechanism is necessary to generate lattices without hose undesirable nodes.40The prototype for all definitions is Aristotle's method of genus plus differentiae: a newterm is defined by specifying its genus or supertype plus the differentiae that distinguishit from other subtypes of the same genus.
The result of such definitions is a tree.
The treebecomes a lattice when the differentiae are specified in a system of logic that can determinewhen one definition is implied by another.
Such a lattice, variously known as a type,taxonomic, or subsumption hierarchy, forms the basis for many systems of frames andsemantic networks.
These lattices have several advantages over feature lattices: only thedesired nodes are ever defined; most, if not all, of the impossible combinations can beproved to be contradictory in the underlying logic; and the logic can specify relationshipsthat a simple concatenation of features can never represent.A serious computational problem arises with definitional systems that are based onlogic.
If the logic is rich enough to express anything in ftrst-order predicate calculus, theproof procedures can become intractable.
To simplify the computations, many framesystems adopt a restricted logic, usually without negations or disjunctions.
Yet those re-stricted logics cannot express all the definitions used in language.
The definition of pen-niless, for example, requires a negation to express "not having a penny."
The wordbachelor in Katz and Fodor's example requires temporal logic to express "without a mateduring the breeding time."
In general, every logical quantifier, Boolean operator, andmodal operator occurs in dictionary definitions.
Although tractability is an importantfeature in a computational system, a logic that is suitable for natural anguage semanticsmust be able to represent anything that people might say.
A problem solver or reasonermight have to simplify the statements in order to improve efficiency, but those simplifi-cations are likely to be highly domain-dependent.
A general language handier must rep-resent every statement in as rich a form as it was originally expressed.In several articles written shortly before his death, Richard Montague (1974) beganthe tradition of applying model-theoretic techniques to natural anguage semantics.
Hestarted with Carnap's intuition (1947) that the intension of a sentence is a function frompossible worlds to truth values.
He combined that notion with Frege's principle ofcompositionality o develop a systematic way of deriving the function that represents hemeaning of a sentence.
The intension of each word in each lexical category would corre-spond to a functional form of some sort.
The intension of the noun unicorn, for example,would be a function that applies to entities in the world and generates the value true foreach unicorn and false for each nonunicom.
Other lexical categories would be representedby 2-expressions that would combine with the functional forms that happened to occurto their fight or left.
As an example, Montague's lexical entry for the word be is a functionthat checks whether the predicate is true of the subject.
The idea is straightforward, buthis implementation uses a rather heavy notation with many subtle distinctions:2~ 2x (~)  where ext(x)= ext(y)).This 2-expression defines the intension of be as a function with two arguments: ~' is theintension of the phrase that follows the word be, and x is the intension of the subject ofbe.
The body of the expression appfies the thnction ~ to some y whose extension is equalto the extension of x. Montague's lexicon consists of such constructions that applyfunctions of functions to generate other functions of functions of functions, aTo improve readability, this example modifies Montague's notation by adding the keyword "where" andusing the function ext(x) for extension instead of Montague's cryptic marks.41Besides constructing functions, Montague used them to solve certain logical puzzles.One of them is Barbara Partee's example: The temperature is ninety, and it is rising.Therefore, ninety is rising.
To avoid the conclusion that a constant like ninety couldchange, Montague drew some subtle distinctions.
He treated temperature as an "extraor-dinary noun" that denoted "an individual concept, not an individual".
He also gave spe-cial treatment to rise, which "unlike most verbs, depends for its applicability on the fullbehavior of individual concepts, not just on their extensions."
As a result, he claimed thatThe temperature is ninety asserted the equality of extensions, but that The temperature isrising applied the verb rise to the intension.
Consequently; the conclusion that ninety isrising would be blocked, since rise would not be applied to the extension.
To linguists,Montague's distinction between words whose semantics depend on intensions and thosewhose semantics depend on extensions seemed like an ad hoc contrivance; he never gaveany linguistic evidence to support it.
To psychologists, the complex manipulations re-quired for processing the 2-expressions seemed unlikely to have any psychological reality.And to programmers, the infinities of possible worlds seemed computationally intractable.Yet for all its unnaturalness, Montague's ystem was an impressive achievement: itshowed that formal methods of logic could be applied to natural anguages, that theycould define the semantics of a significant subset of English, and that they could representlogical aspects of natural anguage with the depth and precision usually attained only inartificial systems of logic.At the opposite xtreme from Montague's logical rigor are Roger Schank's informaldiagrams and quasi-psychological theories that were never tested in controlled psycho-logical experiments.
Yet they led his students to build impressive demos that exhibitedinteresting language behavior.
As an example, the Integrated Partial Parser (Schank,Lebowitz, & Bimbaum 1980) represents a fairly mature stage of Schank's theories.
IPPwould analyze newspaper stories about international terrorism, search for words thatrepresent concepts in that domain, and apply scripts that relate those concepts to oneanother.
In one example, IPP processed the sentence, About 20 persons occupied the officeof Amnesty International seeking better jail conditions for three alleged terrorists.
To in-terpret hat sentence, it used the following dictionary entry for the word occupied:(WORD-DEF OCCUPIEDINTEREST 5TYPE EBSUBCLASS SEBTEMPLATE (SCRIPT $DEMONSTRATEACTOR NILOBJECT NILDEMANDS NILMETHOD (SCENE $OCCUPYACTOR NILLOCATION NIL))FILL (((ACTOR) (TOP-OF *ACTOR-STACK*))((METHOD ACTOR) (TOP-OF *ACTOR-STACK*)))REQS (FIND-DEMON-OBJECTFIND-OCCUPY-LOCRECOGNIZE-DEMANDS) )This entry says that occupied has interest level 5 (on a scale from 0 to 10) and it is an eventbuilder (EB) of subclass scene event builder (SEB).
The template is a script of type42$DEMONSTRATE with an unknown actor, object, and demands.
As its method, thedemonstration has a scene of type $OCCUPY with an unknown actor and location.
Atthe end of the entry are fall and request slots that give procedural hints for finding the ac-tor, object, location, and demands.
In analyzing the sample sentence, IPP identified the20 persons as the actors, the office as the location, and the better jail conditions as thedemand.The flU and request slots implement the Schanldan "expectations."
A fill slot is filledwith something previously found in the sentence, and a request slot waits for somethingstill to come.
They serve the same purpose as Montague's left and right cancellation rulesfor categorial grammar.
The act of Idling the slots corresponds to the 2 rules for ex-panding a function that is applied to a fist of arguments.
Their differences in style aremore significant han their differences in computational mechanisms:?
Schank's antiformalist stance is irrelevant, since anything that can be programmed ona digital computer could be formalized.
One Prolog programmer, in fact, showed thatmost of the slot filling in Schank's parsers and script handlers could be done directlyby Prolog's unification algorithm.
Techniques uch as unification and graph gram-mars could be used to formalize Schank's ystems while making major improvementsin clarity, robustness, and generality.?
Montague's appearance of rigor results from his use of Greek letters and logical sym-bols.
Yet some constructions, uch as his solution to Partee's puzzle, are contrivancesthat programmers would call "hacks" (if they ever took the trouble to work their waythrough his notation).?
Schank and Montague had different attitudes about what aspects of language weremost important.
Schank befieved that the ability to represent and use world know-ledge is the essence of language understanding, and Montague befieved that the abilityto handle scope of quantifiers and modalities was the most significant.
They wereboth right in believing that their favorite aspects were important, but they were bothwrong in ignoring the others.Schank and Montague represented different aspects of language with different methodol-ogies, but they are complementary ather than conflicting.
Yorick Wilks (1991) observedthat Montague's lexical entries are most complex for words like the, for which Schank'sentries are trivial.
Conversely, Schank's entries are richest for content words, whichMontague simply put in one of his categories, while ignoring their connotations.
Bothlogic and background knowledge are important, and the lexicon must include both kindsof information.3 Metaphysical Baggage and Observable ResultsLinguistic theories are usually packaged in metaphysical terms that go far beyond theavailable vidence.
Chomsky's metaphysics may be summarized in a single sentence fromSyntactic Structures: "Grammar is best formulated as a serf-contained study independentof semantics."
For Montague, the title and opening sentence of "English as a Formal?
Language" express his point of view: "I reject the contention that an important heore-tical difference xists between formal and natural anguages."
Schank's outlook is sum-marized in the following sentence from Conceptual Information Processing: "ConceptualDependency Theory was always intended to be a theory of how humans process natural43language that was explicit enough to allow for programming it on a computer."
Thesecharacteristic sentences provide a key to understanding their authors' motivation.
Yettheir actual achievements are easier to understand when the metaphysics is ignored.
Lookat what they do, not at what they say.In their basic attitudes and metaphysics, Schank and Montague are irreconcilable.Montague is the epitome of the kind of logician that Schank has always denounced asmisguided or at least irrelevant.
Montague stated every detail of his theory in a preciseformalism, while Schank made sweeping eneralizations and left the detailed programmingto his students.
For Montague, the meaning of a sentence is a function from possibleworlds to truth values; for Schank, it is a diagram that represents human conceptuali-zations.
On the surface, their only point of agreement is their implacable opposition toChomsky and "the developments emanating from the Massachusetts Institute of Tech-nology" (Montague 1970).
Yet in their reaction against Chomsky, both Montague andSchank evolved positions that are remarkably similar, although their terminology hides theresemblance.
What Chomsky called a noun, Schank called a picture producer, andMontague called a function from entities to truth values.
But those terms are irrelevantto anything that they ever did: Schank never produced a single picture or even stated aplausible hypothesis about how one might be produced from his diagrams; Montaguenever applied any of his functions to the real world, let alne the infinity of possible worldshe so freely assumed.
In neutral terms, what Montague and Schank did could be de-scribed in a way that makes the logicist and AI points of view nearly indistinguishable:1.
Semantics, not syntax, is the key to understanding sentence structure.
The traditionalgrammatical categories are surface manifestations of the fundamental semantic ate-gories.2.
Associated with each word is a characteristic semantic structure that determines howit combines with other words in a sentence.3.
The grammar of a language can be reduced to relatively simple rules that show whatcategories of words may occur on the right or the left of a given word (the cancellationrules of categorial grammar or the Schankian expectations).
The variety of sentencepatterns is not the result of a complex grammar, but of the complex interactions be-tween a simple grammar and the underlying semantic structures.4.
The meaning of a sentence is derived by combining the semantic structures for eachof the words it contains.
The combining operations are primarily semantic, althoughthey are guided by word order and inflections.5.
The truth of a sentence in a possible world is computed by evaluating its meaningrepresentation in terms of a model of that world.
Although Schank never used logicalterms like denotation, his question-answering systems embodied effective proceduresfor computing denotations, while Montague's infinities were computationaUyintractable.Terms like picture producer or function from entities to truth values engender heated argu-ments, but they have no effect on the application of the theory to language or its imple-mentation in a computer program.
Without the metaphysical baggage, both theoriesincorporate a semantic-based approach that is widely accepted in AI and computationallinguistics.44At the level of data structures and operations, there are significant differences betweenMontague and Schank.
Montague's representations were A-expression s, which have theassociated operations of functional application, A-expansion, and A-contraction.
Hismetaphysics gave him a rigorous methodology tbr assigning each word to one of his cat-egories of functions (even though he never actually applied those functions to the realworld or any possible world).
And his concerns about logic led him to a careful treatmentof quantifiers, modalities, and their scope.
Schank's representations are graphs on paperand LISP structures of various kinds in his students' programs.
The permissible oper-ations include any manipulations of those structures that could be performed in LISP.Schank's lack of a precise formalism gave his students the freedom and flexibility to inventnovel solutions to problems that Montague's followers never attempted to address, suchas the use of world knowledge in language understanding.
Yet that lack of formalism ledto ad hoc accretions in the programs that made them unrnaintainable.
Many of Schank'sstudents found it easier to start from scratch and write a new parser than to modify onethat was written by an earlier generation of students.
Montague and Schank have com-plementary strengths: rigor vs. flexibility; logical precision vs. open-ended access tobackground knowledge; exhaustive analysis of a tiny fragment of English vs. a broad-brushsketch of a wide range of language use.Montague and Schank represent two extremes on the semantic-based spectrum, whichis broad enough to encompass most AI work on language.
Since the extremes are morecomplementary than conflicting, it is possible to formulate approaches that combine thestrengths of both: a precise formalism, the expressive power of intensional logic, and theability to use background knowledge in language understanding.
To allow greater flexi-bility, some of Montague's rigid constraints must be relaxed: his requirement of a strictone-to-one mapping between syntactic rules and semantic rules; his use of A-expressionsas the meaning representation; and his inability to handle ellipses, metaphor, metonymy,anaphora, and anything requiring background knowledge.
With a well-designedformalism, these constraints could be relaxed while still allowing formal definitions of thepermissible operations.4 Lexical Representations in Conceptual GraphsWithout a formal system of logic, the issues that a lexical theory must address can onlybe discussed at an anecdotal level.
A formal system can clarify the issues, make themprecise, and lead the discussion to a deeper level of detail.
This paper uses the theory ofconceptual graphs: a system of logic with a graph notation designed for a direct mappingto and from the semantic structures of natural language.
They have a graph structurebased on the semantic networks of AI and C. S. Peirce's existential graphs, which form acomplete system of logic.
They are as formal and expressive as Montague's intensionallogic, but they permit a broader ange of operations on the formalism.
The theory hasbeen presented in book tbrm (Sowa 1984) and in a recent summary (Sowa 1991); see thosesources for more detail.
An earlier paper on lexical issues with conceptual graphs (Sowa1988) did not discuss logic explicitly.
This section will give some examples to illustratethe kind of logical structures that are needed in the lexicon.A basic feature of conceptual graph theory is the use of canonical graphs to representthe expected roles associated with each concept ype.
Canonical graphs are similar to thecase frames used in many systems, but they are richer in the kinds of structures and logical45operators they support.
As an example, Figure 1 shows a canonical graph that representsthe lexical pattern associated with the verb support, It shows that every instance of theconcept ype SUPPORT has four expected participants: an animate agent, some entityas patient, some entity as instrument, and a purpose, which is represented by a nestedcontext.
That context, which might represent something at a different ime and place fromthe outer context, shows that the entity is in some state.l ENTITY JJJJPJ@#J#STATE ~-~Figure 1.
Canonical graph for the lexical type SUPPORTWhereas case frames merely show the thematic roles for a verb and the expectedconcept ypes that can fill those roles, conceptual graphs can grow arbitrarily large: theycan show long-range dependencies far removed from the central concept; and they maycontain nested contexts that show situations at different imes and in different modalities.The dotted line in Figure 1 is a coreference link that crosses context boundaries; it showsthat the entity that is the patient of SUPPORT is coreferent with the thing in the nestedcontext.Conceptual graphs are a complete system of logic with their own model-theoretic se-mantics, but there is also a formula operator ck that maps conceptual graphs into predicatecalculus.
Following is the result of applying Ob to Figure 1:(Vx)(3y)(3z)(~u)(3v)(support(x)(animate(y) ^  entity(z)  ^  entity(u) A situation(v) Aagnt(x,y) A ptnt(x,z) A inst(x,u)  ^  purp(x,v) ^description(v, (3w)(state(w) A stat(z ,w))) ) ) .This formula and the graph in Figure 1 express exactly the same irrforrnation with identicalontological presuppositions.
The first three lines of the formula represent the standard46information that is typical of case frames.
The fourth line, however, represents structurestypical of situation semantics.
It says that the situation v is described by a formula, whichsays that there exists a state w and that the entity z is in state w; i.e.
the purpose of sup-porting is to maintain an entity in some state.
The predicate description(s,p) means thata situation s is described by a proposition p. This is one of the predicates that result fromthe context-creating boxes in conceptual graphs, which are necessary for representing avariety of structures in language.Besides being more readable, the graph contains structures that are not Supported inpredicate calculus, such as the context box that encloses the purpose of the supporting.That box itself represents a concept o which relations can be attached to express purpose,causality, time sequence, and other intersentential connectives.
The description predicatein the formula requires a version of higher order logic with nested propositions andquantification over the situations described by those propositions.
Although the de-scription predicate allows some contexts to be translated into predicate calculus by ~b,there are other features associated with contexts that cannot be translated.
One such fea-ture is the indexical referent #, which is used to represent unresolved anaphoric references,tenses, and other context-dependent phenomena.
The formula operator qb is undefinedfor graphs containing # until those references have been resolved.As another example, Figure 2 shows the canonical graphs for the concepts EASY andEAGER, which are used for the adjectives easy and eager as well as the adverbs easily andeagerly.+ACT i,o,  /Figure 2.
Canonical graphs for EASY and EAGERThe first graph says that every instance of EASY is an attribute of some ENTITY, andit is also the manner of some ACT.
That ENTITY also happens to be the patient of thesame ACT.
The graph for EAGER has the same shape as the graph for EASY, butEAGER is an attribute of some ANIMATE being that is the agent of some ACT.
Thesegraphs illustrate the point that many syntactic features result from deeper logical proper-ties.
In this case, the AGNT and PTNT relations have different preferences for expressionas subject or object.
As a result, the canomcal graphs permit sentences of the followingform:John easily does the homework.John eagerly does the homework.47The homework is easy for  John to do.But they rule out the foUowing sentences:* The homework is eager for John to do.
* John is easy to do the homework.Sowa and Way (1986) showed how such graphs could be used in a semantic interpreter.The grammar would only require general rules for adjectives and adverbs; no featureswould be required to distinguish special properties of easy and eager or their adverbialforms.
Instead, the correct options would be selected and the incorrect ones would beblocked by the graph unification operations (the maximal joins of conceptual graphs).Besides the display form for conceptual graphs (Figures 1 and 2), there is also a morecompact linear form.
Following are the linear representations for the graphs in Figure 2:\[EASY: V\]-(ATTR) ?
\[ENTITY\] ?(PTNT)?
\[ACT : *x\](MANR)?\[*x\].\[EAGER: V\]-(ATTR) ?\[ANIMATE\] ?
(AGNT) ?\[ACT : *x\](MANR)?\[*x\].In the linear notation, concepts are enclosed in square brackets, and conceptual relationsare enclosed in parentheses.
The hyphens show that additional relations attached to anode are continued on separate lines.
The variable *x indicates that the concept \[*x\] re-presents the same node as the concept \[ACT: *x\].
Converting from the box and circlenotation to the linear notation causes cycles be broken, and variables like *x are neededto show cross-references.
The point at which the cycle'is broken is purely arbitrary; dif-ferent linea.rizations represent exactly the same graph.The examples cited in Section 2 --penniless, difunto, and angelito -- can also be de-fined in conceptual graphs.
The definition of PENNILESS, for example, requires a ne-gation:PENNILESS : (2x) \[STATE: *x\]?
(STAT)?\[PERSON: *y\]9\[ \ [*y\]?
(POSS)?
\[PENNY\]\].This definition says that PENNILESS is a state x of a person y, where it is false that yhas possession (POSS) of a penny.
Systems that do not allow negations in definitionsmay make the reasoning process faster, but they make it impossible to define many kindsof concepts.
The definition for the concept ype DIFUNTO would require a relationWHEN linking two situations:DIFUNTO : (x) \[PERSON: *x\]?
(STAT)?\[DEAO\]\[SITUATION: \[*x\] ?
(STAT) --~ \[MARRIED\] \] ?
(WHEN) ~ \[SITUATION: \[*x\] ?
(PTNT) ?
\[DI E\] \] .By this definition, a difunto is a person x in state dead, where x was in a situation of beingmarried when a situation occurred in which x died.
The definition of angefito is similar,but with a negation inside the first situation.
In reasoning by inheritance, bothDIFUNTO and ANGELITO could be treated as simple subtypes of DEAD-PERSON,and the details inside the definition could be ignored.
But to determine whether a partic-48ular individual was a difunto or an angelito, the details could be recovered by expandingthe 2-expression.New types of conceptual relations can also be defined by 2-abstraction.
The relationWHEN in the previous examples could be defined by the following graph:WHEN = (2x,y)\[SITUATION: *x\]+(PTIM)?\[TIME\]?
(PTIM)?\[SITUATION: *y\].In this definition, WHEN has two formal parameters x and y; each of them refers to asituation that occurs at the same point in time (PTIM).
Any occurrence of the relationWHEN in a graph could be replaced by the sequence of concepts and relations betweenl*xl and \[*y\].
The graph in the definition of DIFUNTO could be expanded to the fol-lowing:\[SITUATION: \ [*x\]*  (STAT) ?
\[MARRIED\] \?
(PTIM)?
\[TIME\]-(PTIM) ?\[SITUATION: \[*x\]?
(PTNT) ?\[DIE\]\].This graph says that x is in the state married at some point in time, which is the same timethat x dies.
Since the graph is too long to fit on one line, the hyphen shows that the re-lation attached to \]TIME\] is continued on the next line.In Section 2, one criticism of Katz and Fodor's theory was its inability to show thecommonality among different senses of the same word.
?
Some linguists, such as Ruhl(1989), even maintain a principle of monosemy: each word has a single very abstractmeaning, and the multiple senses that appear in dictionary definitions are the result ofapplying the word in different domains.
For Katz and Fodor's example of bachelor, thereis a such a unifying meaning: "an animate being x preparing for a situation in which x isin a mature state."
That definition could also be expressed in a conceptual graph:BACHELOR = (2x) \[ANIMATE: *x\]?(AGNT)+\[PREPARE\]-(PURP)?
\[SITUATI ON: \[*x\]-~ (STAT) ?
\[STATE\] ?(ATTR)?
\[MATURE\] \ .This central meaning for the concept ype BACHELOR explains why Pope John PaulII, who is an unmarried man, would not be called a bachelor: he has already achieved theultimate stage in his profession, which has celibacy as a precondition.Partee's puzzle about the temperature may be represented in conceptual graphs bydistinguishing temperature as a state from its measure as a number.
The sentence Thetemperature is 90 would therefore be treated as an abbreviation for the sentence Themeasure of  the temperature is 90.
Such abbreviations are common in ordinary language,and words such as measure and amount are evidence for a distinction that is familiar tomost speakers.
The following graph shows that the temperature is in the state of RISEand its current measure happens to be 90?F.\[TEMP-MEASURE: gOF\] ?
(MEAS) ?
\[TEMPERATURE: #\] ?
( STAr ) -~- \[RISE\].In this graph, the temperature is in a state of rising.
Since its measure of 90?F is not di-rectly attached to \]RISE\], the value of 90 will not change.
Instead, the temperature atalater time will have a different measure.
Unlike Montague's ubtle distinctions, this sol-ution to Partee's puzzle is based on concepts derived from the words and phrases usedby people who talk about temperature.As an example of a complex sentence containing nested contexts and quantification,Figure 3 shows the graph for a sentence that defines Prix Goncourt as "an institution49comprising a panel of judges who each year award a prize of money to an author whopublished an outstanding literary work.
"I'NST'TUT'ON: P"" Ooooo'.
'r'F< '  (" illI YEAR: VSITUATION:llIllllll lll# l MoNEY.
.IAUTHO"Figure 3.
Conceptual graph that defines the Prix GoncourtIn Figure 3, the quantifier V permits the concept \[YEAR\] to be instantiated with dif-ferent years, in each of which a separate author is awarded a separate instance of the prize.The relation (PAST) shows the past tense of published; that relation is not a primitive,since it may be expanded according to the following definition:PAST : (Rx)\[SITUATION: *x\]+(PTIM)+\[TIME\]+-(SUCC)?\[TIME: #\].This defines (PAST) as a monadic relation with a formal parameter x.
It applies to asituation whose point in time (PTIM) is a successor to some contextually defined time.The marker # indicates a reference to be resolved to the point in time of the containingcontext, in this case the year of the award -- i.e.
the publishing occurred before theawarding.505 Operations on Knowledge in the LexiconThe purpose of a rich semantic representation is to support a rich set of operations on therepresentation.
Of the various reasoning systems that have been implemented for con-ceptual graphs, some are theorem provers that are based on the logical structure of thegraphs; others emphasize heuristic techniques based on semantic distance measures; andothers combine logic and heuristics to speed up the proofs.
Fargues et al (1986) imple-mented a Prolog-Like theorem prover with conceptual graphs as the replacement for theusual predicates.
It incorporated several advances over standard Prolog: arbitrarily largegraphs as the unit of inference instead of single predicates; emantic unification that derivesmaximal common supertypes when joining graphs; and the ability to do 2-expansion andcontraction of types.
Garner and Tsui (1988) implemented an inference ngine that usedheuristics and semantic distance measures to guide the proofs.
They demonstrated thatit could handle the kinds of scripts processed by the Schankian systems, but with a morerobust, formally defined representation.
Hartley and Coombs (1991) showed how con-ceptual graphs could be used in abductive reasoning for generating models that satisfiedgiven constraints.Interpreting noniiteral language is an appLication where background knowledge is es-sential.
Way (1991) presented a book-length study of metaphor using conceptual graphs.According to her hypothesis, the purpose of a good metaphor is to refine the concept hi-erarchy by creating a new type.
As a result of interpreting a metaphor, a word is gener-aLized to a concept ype that includes the original meaning plus a more abstract meaningthat can be transferred to a new domain.
Her approach takes advantage of the operationsfor joining and projecting raphs and the 2-e?pressions for defining new concept ypes.Metonymy is another kind of nonliteral anguage that requires detailed semanticstructures in the lexicon and detailed operators for processing them.
As an example ofmetonymy, consider the sentence The White House announced the budget.
Syntactically,White House is the subject of announce, but semantic onstraints rule out the building asa possible agent of the concept ANNOUNCE.
Therefore, a semantic interpreter mightconstruct a graph with "subj" as a syntactic annotation on the relation, but with the typeof relation unspecified:\[BUILDING: White House\]?
(; subj)?\[ANNOUNCE\]+(PTNT)+\[BUDGET: #\].After constructing this graph in the parsing stage, the semantic interpreter must determinethe unknown relation type and insert it in front of the semicolon.
It could search forbackground knowledge about the White House, discovering that people work there whomake announcements.
From the graphs that state that knowledge, it could abstract thefollowing 2-expression to define a possible relation between an act and a building:(~x,y) \[ACT: *x\]?
(AGNT)+\[PERSON\]+(LOC)+\[BUILOING: *y\].This definition relates an act x whose agent is a person located in a building y.
The entire2-expression could then be inserted just before the semicolon of the undefined relation:\[BUILDING: White House\]+((~x,y) \[ACT: *x\]+(AGNT)-~\[PERSON\]+(LOC)?\[BUILDING: *y\] ;su bj ) ?
\[ANNOUNCE\] +(PTNT) + \[BUDGET: #\].When the 2-expression is expanded, the concept marked by x (the first parameter) isjoined to the concept with the arrow pointing towards the relation; and the concept51marked by y (the second parameter) is joined to the concept with the arrow pointing awayfrom the relation.
The syntactic annotation "subj" must be dropped, since no single re-lation in the expansion exactly corresponds to the original subject of the sentence.\[BUILDING: White House\]?(LOC)~-\[PERSON\]?
(AGNT)?\[ANNOUNCE\]-(PTNT)?\[BUDGET: #\].This graph represents he expanded sentence A person at the White House announced thebudget.
The option of omitting the conceptual relation in cases of metonymy or ellipsisallows certain decisions to be deferred until additional i.rfformation is obtained from thecontext or from background knowledge.As another example of metonymy, consider the problem of multiple meanings of theterm Prix Goncourt, discussed by Kayser (1988).
He found seven metonyms for that term:a literary prize, the money awarded as the prize, the person who received the prize, thepanel that awards the prize, the book that won the prize, the time that the prize was won,or the institution that grants a new instance of the prize each year.
Following are hissample sentences and their English translations:?
Prize: Le PG a 6t6 attribu6 d X.
\[The PG was awarded to X.\]?
Money: X a vers~ son PG d la Croix Rouge.
\[X turned over his PG to the Red Cross.1?
Person: Le PG a 6t6 f~licitd par le Pre'sident.
\[The PG was congratulated by thePresident.l?
Panel: Le PG a admis un nouveaujur~.
\[The PG admitted a new judge.\]?
Book: Peux-tu aller m'acheter le PG d la librairie X?
\[Could you go buy the PG forme at bookstore X?\]?
Time: Depuis son PG, il est devenu arrogant.
ISince his PG, he has become arrogant.\]e Institution: Le PG pervertit la vie litt6raire.
\[The PG perverts the literary life.\]Each of these metonyms could be interpreted by the method of constructing a2-expression for the unknown relation.
The graph in Figure 3 provides the basic back-ground knowledge for constructing those relations.With Figure 3 as background knowledge, each metonym of Prix Goncourt can bedetermined by finding a suitable path through the graph and mapping it into a2-expression that defines the unknown relation.
The verbs in the input sentences imposeselectional constraints that determine the direction the path may take.
When the con-straints imposed by the verb are not strong enough, additional background knowledgederived from other words in the input sentence may be needed; that knowledge could berepresented in other conceptual graphs that would also be joined to the input graph.Following is a sketch of how such a system could interpret each metonym:?
Prize: The verb attribu6 in the input maps to the concept \[AWARD\].
The corre-sponding concept in the background graph is linked to \]PRIZEI by the relation(PTNT).
A maximal join of the input graph to the background graph starting withthe two concepts of type AWARD would automatically associate PG with the nodeIPRIZEi.?
Money: X could present either the prize itself or the money of the prize to the RedCross.
Background knowledge that people give money to charitable organizations52would lead to a preference for \[MONEY\].
That knowledge could be represented ina separate conceptual graph triggered by the term Red Cross.?
Person: The verb f3licit3 maps to \]CONGRATULATE\], with its selectional con-straints for PERSON or a subtype such as AUTHOR.
Judges are also persons, butthe node \[JUDGE: {,}\] is marked as plural by the symbol {,} and is therefore unlikelyto be indicated as the Pfix Goncourt.
Information about salience should also bemarked on the graph: \[AUTHOR\], \]PRIZE\], and \[LITERARY-WORK\] are moresalient and hence more likely to be selected.?
Panel: The verb admis maps to the concept \]ADMIT\], which would select an agentof type PERSON or a collection of persons, such as a panel.
But that constraintwould permit either \[AUTHOR\[, \[JUDGE\], or \]PANEL\] as the agent.
The remainderof the sentence un nouveau jur~ introduces the concept \]JUDGE\], which would unifywith the set of judges linked by the member elation to \[PANEL\].
The preference rulefor increased connectivity would select the concept \[PANEL\], especially since onesense of ADMIT would include the admission of a member to a set.?
Book: The verb acheter maps to \[BUY\[, which would prefer a nonhuman physicalentity as patient.
Buying a prize is possible, but that might suggest bribing the panel.The background knowledge that bookstores sell books would give a strong preferencefor BOOK, which would unify with \[LITERARY-WORK\] (although this is anotherexample of metonymy, since book could refer to the fiterary work or to a physicalobject in which the work is printed).?
Time: The preposition depuis requires a point in time as its object.
The concept\[YEAR: V\] indicates an entire series of possible times.
The possessive pronoun son,coreferent with il, indicates a particular person, which would most likely select thenode \[AUTHOR\], which would occur in one instance of a year, which would thenbe the correct ime.?
Institution: The verb pervertit maps to \[PERVERT\], which could have a human asagent or almost anything as -instrument, either of which might occur in subject posi-tion.
But the present ense of the verb suggests a continuing influence; therefore, thesubject must be something outside the scope of the quantifier on \[YEAR: V\].
Since\[AUTHOR\], \[PRIZE\], and \[MONEY\] are all inside that scope, there would be aseparate instance of them for each year.
A continuing perversion could only be ex-erted by something outside that scope, such as the institution or the panel; when ei-ther is permissible, salience might prefer the node \[INSTITUTION\].Once a concept node has been selected by one of these mechanisms, the correct metonymcould then be defined by a 2-abstraction over the graph with that node marked as theformal parameter.
As these examples illustrate, the process of interpretation is complex:it requires a great deal of domain-dependent knowledge; and it must be sensitive to manysyntactic and semantic features, including verb tenses, definite and indefinite articles, andquantifier scopes.
Yet the kind of analysis required, although complex, is within the realmof what is computable -- but only if the background knowledge and lexical entries areencoded in a suitably rich knowledge representation language.536 Towards a Synthesis of the Logicist and AI TraditionsLinguists who work in the tradition of Noarn Chomsky are fond of saying that semantictheory is not as well developed as syntax.
That may be true of their work, but it is nottrue of the model-theoretic radition that follows from the work of Richard Montague.Nor is it true of the computational work in the AI tradition.
These two approaches,which are often considered iametrically opposed, have complementary strengths andweaknesses.
With a suitable knowledge representation, it is possible to have the best ofboth: a formal system of logic that can accommodate background knowledge as used inAI systems.
Features and frames are too weak to serve as a complete system of logic.Graphs are potentially much more powerful, but they must be formalized in order tosupport all logical operators.
The inventor of the modern linear notation for predicatecalculus was C. S. Pierce, who later abandoned the linear form in favor of his existentialgraphs, which he called "the logic of the future."
Remarkably, Peirce's graphs have acontext structure that is isomorphic to Karnp's Discourse Representation Structures(1981).
As a synthesis of Peirce's graphs with the AI work on semantic networks, con-ceptual graphs benefitted from a stroke of serendipity: their contexts can directly supportKamp's rules for resolving anaphora, even though that was not one of their original designcriteria.
A great deal of research is undoubtedly necessary to support all the semanticstructures of language, but these felicitous convergences give hope that such a synthesisof the logicist and AI traditions is proceeding in the fight direction.ReferencesDixon, R. M. W. (1991) A New Approach to English Grammar on Semantic Principles,Oxford University Press, New York.Fargues, Jean, Marie Claude Landau, Anne Dugourd, & Laurent Catach, (1986) "Con-ceptual graphs for semantics and knowledge processing," IBM Journal of Research andDevelopment 30:1, 70-79.Garner, B.J., & Tsui, E. (1988) "General purpose inference ngine for canonical graphmodels," Knowledge Based Systems 1:5, pp.
266-278.Hartley, Roger T., & Michael J. Coombs (1991) "Reasoning with graph operations," inJ.
F. Sowa, ed., Principles of Semantic Networks: Explorations in the Representation ofKnowledge, Morgan Kaufmann Publishers, San Mateo, CA, pp.
487-505.Karnp, Hans (1981) "Events, discourse representations, and temporal references,"Langages 64, 39-64.Katz, Jerrold J., & Jerry A. Fodor (1963) "The structure of a semantic theory," Language39, 170-210.
Reprinted in J.
A. Fodor & J. J. Katz, eds.
(1964) The Structure ofLanguage, Prentice-Hall, Englewood Cliffs, N J, pp.
479-518.Kayser, Daniel (1988) "What kind of thing is a concept?"
Computational Intelligence 4:2,158-165.Montague, Richard (1970) "English as a formal anguage," reprinted in Montague (1974),pp.
188-221.54Montague, Richard (1974) Formal Philosophy, Yale University Press, New Haven.Ruhl, Charles (1989) On Monosemy."
A Study in Linguistic Semantics, State Universityof New York Press, Albany.Schank, Roger C., ed.
(1975) Conceptual Information Processing, North-Holland Pub-lishing Co., Amsterdam.Schank, Roger C., Michael Lebowitz, & Lawrence Bimbaum (1980) "An integratedunderstander," American Journal of Computational Linguistics 6, 13-30.Sowa, John F. (1976) "Conceptual graphs for a database interface," IBM Journal of Re-search and Development 20:4, pp.
336-357.Sowa, John F. (1984) Conceptual Structures: Information Processing in Mind and Ma-chine, Addison-Wesley, Reading, MA.Sowa, John F. (1988) "Using a lexicon of canonical graphs in a semantic interpreter," inM.
Evens, ed., Relational Models of the Lexicon, Cambridge University Press, pp.
73-97.Sowa, John F. (1991) "Towards the expressive power of natural language," in J. F.
Sowa,ed., Principles of Semantic Networks: Explorations in the Representation f Knowledge,Morgan Kaufmann Publishers, San Mateo, CA, pp.
157-189.Way, Eileen C. (1991) Dynamic Type Hierarchies, Kluwer Academic Publishers.Wilks, Yorick A.
(1991) Personal communication.55
