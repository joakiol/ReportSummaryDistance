Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 101?106,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsExtracting Opinion Expressions and Their Polarities ?
Exploration ofPipelines and Joint ModelsRichard Johansson and Alessandro MoschittiDISI, University of TrentoVia Sommarive 14, 38123 Trento (TN), Italy{johansson, moschitti}@disi.unitn.itAbstractWe investigate systems that identify opinionexpressions and assigns polarities to the ex-tracted expressions.
In particular, we demon-strate the benefit of integrating opinion ex-traction and polarity classification into a jointmodel using features reflecting the global po-larity structure.
The model is trained usinglarge-margin structured prediction methods.The system is evaluated on the MPQA opinioncorpus, where we compare it to the only previ-ously published end-to-end system for opinionexpression extraction and polarity classifica-tion.
The results show an improvement of be-tween 10 and 15 absolute points in F-measure.1 IntroductionAutomatic systems for the analysis of opinions ex-pressed in text on the web have been studied exten-sively.
Initially, this was formulated as a coarse-grained task ?
locating opinionated documents ?and tackled using methods derived from standard re-trieval or categorization.
However, in recent yearsthere has been a shift towards a more detailed task:not only finding the text expressing the opinion, butalso analysing it: who holds the opinion and to whatis addressed; it is positive or negative (polarity);what its intensity is.
This more complex formula-tion leads us deep into NLP territory; the methodsemployed here have been inspired by informationextraction and semantic role labeling, combinatorialoptimization and structured machine learning.A crucial step in the automatic analysis of opinionis to mark up the opinion expressions: the pieces oftext allowing us to infer that someone has a partic-ular feeling about some topic.
Then, opinions canbe assigned a polarity describing whether the feel-ing is positive, neutral or negative.
These two taskshave generally been tackled in isolation.
Breck et al(2007) introduced a sequence model to extract opin-ions and we took this one step further by adding areranker on top of the sequence labeler to take theglobal sentence structure into account in (Johanssonand Moschitti, 2010b); later we also added holderextraction (Johansson and Moschitti, 2010a).
Forthe task of classifiying the polarity of a given expres-sion, there has been fairly extensive work on suitableclassification features (Wilson et al, 2009).While the tasks of expression detection and polar-ity classification have mostly been studied in isola-tion, Choi and Cardie (2010) developed a sequencelabeler that simultaneously extracted opinion ex-pressions and assigned polarities.
This is so farthe only published result on joint opinion segmenta-tion and polarity classification.
However, their ex-periment lacked the obvious baseline: a standardpipeline consisting of an expression identifier fol-lowed by a polarity classifier.In addition, while theirs is the first end-to-end sys-tem for expression extraction with polarities, it isstill a sequence labeler, which, by construction, isrestricted to use simple local features.
In contrast, in(Johansson and Moschitti, 2010b), we showed thatglobal structure matters: opinions interact to a largeextent, and we can learn about their interactions onthe opinion level by means of their interactions onthe syntactic and semantic levels.
It is intuitive thatthis should also be valid when polarities enter the101picture ?
this was also noted by Choi and Cardie(2008).
Evaluative adjectives referring to the sameevaluee may cluster together in the same clause orbe dominated by a verb of categorization; opinionswith opposite polarities may be conjoined through acontrastive discourse connective such as but.In this paper, we first implement two strong base-lines consisting of pipelines of opinion expressionsegmentation and polarity labeling and comparethem to the joint opinion extractor and polarity clas-sifier by Choi and Cardie (2010).
Secondly, we ex-tend the global structure approach and add featuresreflecting the polarity structure of the sentence.
Oursystems were superior by between 8 and 14 absoluteF-measure points.2 The MPQA Opinion CorpusOur system was developed using version 2.0 of theMPQA corpus (Wiebe et al, 2005).
The centralbuilding block in the MPQA annotation is the opin-ion expression.
Opinion expressions belong to twocategories: Direct subjective expressions (DSEs)are explicit mentions of opinion whereas expressivesubjective elements (ESEs) signal the attitude of thespeaker by the choice of words.
Opinions have twofeatures: polarity and intensity, and most expres-sions are also associated with a holder, also calledsource.
In this work, we only consider polarities,not intensities or holders.
The polarity takes the val-ues POSITIVE, NEUTRAL, NEGATIVE, and BOTH;for compatibility with Choi and Cardie (2010), wemapped BOTH to NEUTRAL.3 The BaselinesIn order to test our hypothesis against strong base-lines, we developed two pipeline systems.
The firstpart of each pipeline extracts opinion expressions,and this is followed by a multiclass classifier assign-ing a polarity to a given opinion expression, similarto that described by Wilson et al (2009).The first of the two baselines extracts opinion ex-pressions using a sequence labeler similar to that byBreck et al (2007) and Choi et al (2006).
Sequencelabeling techniques such as HMMs and CRFs arewidely used for segmentation problems such asnamed entity recognition and noun chunk extraction.We trained a first-order labeler with the discrimi-native training method by Collins (2002) and usedcommon features: words, POS, lemmas in a slidingwindow.
In addition, we used subjectivity clues ex-tracted from the lexicon by Wilson et al (2005).For the second baseline, we added our opinion ex-pression reranker (Johansson and Moschitti, 2010b)on top of the expression sequence labeler.Given an expression, we use a classifier to assigna polarity value: positive, neutral, or negative.
Wetrained linear support vector machines to carry outthis classification.
The problem of polarity classi-fication has been studied in detail by Wilson et al(2009), who used a set of carefully devised linguis-tic features.
Our classifier is simpler and is basedon fairly shallow features: words, POS, subjectivityclues, and bigrams inside and around the expression.4 The Joint ModelWe formulate the opinion extraction task as a struc-tured prediction problem y?
= arg maxy w ??
(x, y).where w is a weight vector and ?
a feature extractorrepresenting a sentence x and a set y of polarity-labeled opinions.
This is a high-level formulation ?we still need an inference procedure for the arg maxand a learner to estimate w on a training set.4.1 Approximate InferenceSince there is a combinatorial number of ways tosegment a sentence and label the segments with po-larities, the tractability of the arg max operation willobviously depend on whether we can factorize theproblem for a particular ?.Choi and Cardie (2010) used a Markov factor-ization and could thus apply standard sequence la-beling with a Viterbi arg max.
However, in (Jo-hansson and Moschitti, 2010b), we showed that alarge improvement can be achieved if relations be-tween possible expressions are considered; these re-lations can be syntactic or semantic in nature, forinstance.
This representation breaks the Markov as-sumption and the arg max becomes intractable.
Weinstead used a reranking approximation: a Viterbi-based sequence tagger following Breck et al (2007)generated a manageable hypothesis set of completesegmentations, from which the reranking classifierpicked one hypothesis as its final output.
Since theset is small, no particular structure assumption (such102as Markovization) needs to be made, so the rerankercan in principle use features of arbitrary complexity.We now adapt that approach to the problem ofjoint opinion expression segmentation and polarityclassification.
In that case, we not only need hy-potheses generated by a sequence labeler, but alsothe polarity labelings output by a polarity classifier.The hypothesis generation thus proceeds as follows:?
For a given sentence, let the base sequence la-beler generate up to ks sequences of unlabeledopinion expressions;?
for every sequence, apply the base polarityclassifier to generate up to kp polarity labelings.Thus, the hypothesis set size is at most ks ?
kp.
Weused a ks of 64 and a kp of 4 in all experiments.To illustrate this process we give a hypotheticalexample, assuming ks = kp = 2 and the sentenceThe appeasement emboldened the terrorists.
Wefirst generate the opinion expression sequencecandidates:The [appeasement] emboldened the [terrorists]The [appeasement] [emboldened] the [terrorists]and in the second step we add polarity values:The [appeasement]?
emboldened the [terrorists]?The [appeasement]?
[emboldened]+ the [terrorists]?The [appeasement]0 emboldened the [terrorists]?The [appeasement]?
[emboldened]0 the [terrorists]?4.2 Features of the Joint ModelThe features used by the joint opinion segmenter andpolarity classifier are based on pairs of opinions: ba-sic features extracted from each expression such aspolarities and words, and relational features describ-ing their interaction.
To extract relations we used theparser by Johansson and Nugues (2008) to annotatesentences with dependencies and shallow semanticsin the PropBank (Palmer et al, 2005) and NomBank(Meyers et al, 2004) frameworks.Figure 1 shows the sentence the appeasement em-boldened the terrorists, where appeasement and ter-rorists are opinions with negative polarity, with de-pendency syntax (above the text) and a predicate?argument structure (below).
The predicate em-boldened, an instance of the PropBank frameembolden.01, has two semantic arguments: theAgent (A0) and the Theme (A1), realized syntacti-cally as a subject and a direct object, respectively.
[appeasement] emboldened terroriststhe [embolden.01]TheNMOD SBJ OBJNMODA1A0Figure 1: Syntactic and shallow semantic structure.The model used the following novel features thattake the polarities of the expressions into account.The examples are given with respect to the two ex-pressions (appeasement and terrorists) in Figure 1.Base polarity classifier score.
Sum of the scoresfrom the polarity classifier for every opinion.Polarity pair.
For every pair of opinions in thesentence, we add the pair of polarities: NEG-ATIVE+NEGATIVE.Polarity pair and syntactic path.
For a pairof opinions, we use the polarities and arepresentation of the path through the syn-tax tree between the expressions, follow-ing standard practice from dependency-basedSRL (Johansson and Nugues, 2008): NEGA-TIVE+SBJ?OBJ?+NEGATIVE.Polarity pair and syntactic dominance.
In additionto the detailed syntactic path, we use a simplerfeature based on dominance, i.e.
that one ex-pression is above the other in the syntax tree.
Inthe example, no such feature is extracted sinceneither of the expressions dominates the other.Polarity pair and word pair.
The polarity pairconcatenated with the words of the clos-est nodes of the two expressions: NEGA-TIVE+NEGATIVE+appeasement+terrorists.Polarity pair and types and syntactic path.
Fromthe opinion sequence labeler, we get the expres-sion type as in MPQA (DSE or ESE): ESE-NEGATIVE:+SBJ?OBJ?+ESE-NEGATIVE.Polarity pair and semantic relation.
When twoopinions are directly connected through a linkin the semantic structure, we add the role labelas a feature.103Polarity pair and words along syntactic path.
Wefollow the path between the expressions andadd a feature for every word we pass: NEG-ATIVE:+emboldened+NEGATIVE.We also used the features we developed in (Jo-hansson and Moschitti, 2010b) to represent relationsbetween expressions without taking polarity into ac-count.4.3 Training the ModelTo train the model ?
find w ?
we applied max-marginestimation for structured outputs, a generalization ofthe well-known support vector machine from binaryclassification to prediction of structured objects.Formally, for a training set T = {?xi, yi?
}, wherethe output space for the input xi is Yi, we state thelearning problem as a quadratic program:minimize w ?w?2subject to w(?
(xi, yi)?
?
(xi, yij)) ?
?
(yi, yij),?
?xi, yi?
?
T , yij ?
YiSince real-world data tends to be noisy, we mayregularize to reduce overfitting and introduce a pa-rameter C as in regular SVMs (Taskar et al, 2004).The quadratic program is usually not solved directlysince the number of constraints precludes a directsolution.
Instead, an approximation is needed inpractice; we used SVMstruct (Tsochantaridis et al,2005; Joachims et al, 2009), which finds a solu-tion by successively finding the most violated con-straints and adding them to a working set.
Theloss ?
was defined as 1 minus a weighted combi-nation of polarity-labeled and unlabeled intersectionF-measure as described in Section 5.5 ExperimentsOpinion expression boundaries are hard to definerigorously (Wiebe et al, 2005), so evaluations oftheir quality typically use soft metrics.
The MPQAannotators used the overlap metric: an expressionis counted as correct if it overlaps with one in thegold standard.
This has also been used to evaluateopinion extractors (Choi et al, 2006; Breck et al,2007).
However, this metric has a number of prob-lems: 1) it is possible to ?fool?
the metric by creat-ing expressions that cover the whole sentence; 2) itdoes not give higher credit to output that is ?almostperfect?
rather than ?almost incorrect?.
Therefore,in (Johansson and Moschitti, 2010b), we measuredthe intersection between the system output and thegold standard: every compared segment is assigneda score between 0 and 1, as opposed to strict or over-lap scoring that only assigns 0 or 1.
For compatibil-ity we present results in both metrics.5.1 Evaluation of Segmentation with PolarityWe first compared the two baselines to the newintegrated segmentation/polarity system.
Table 1shows the performance according to the intersec-tion metric.
Our first baseline consists of an expres-sion segmenter and a polarity classifier (ES+PC),while in the second baseline we also add the ex-pression reranker (ER) as we did in (Johansson andMoschitti, 2010b).
The new reranker described inthis paper is referred to as the expression/polarityreranker (EPR).
We carried out the evaluation usingthe same partition of the MPQA dataset as in ourprevious work (Johansson and Moschitti, 2010b),with 541 documents in the training set and 150 inthe test set.System P R FES+PC 56.5 38.4 45.7ES+ER+PC 53.8 44.5 48.8ES+PC+EPR 54.7 45.6 49.7Table 1: Results with intersection metric.The result shows that the reranking-based mod-els give us significant boosts in recall, followingour previous results in (Johansson and Moschitti,2010b), which also mainly improved the recall.
Theprecision shows a slight drop but much lower thanthe recall improvement.In addition, we see the benefit of the new rerankerwith polarity interaction features.
The system usingthis reranker (ES+PC+EPR) outperforms the expres-sion reranker (ES+ER+PC).
The performance dif-ferences are statistically significant according to apermutation test: precision p < 0.02, recall and F-measure p < 0.005.5.2 Comparison with Previous ResultsSince the results by Choi and Cardie (2010) are theonly ones that we are aware of, we carried out an104evaluation in their setting.1 Table 2 shows our fig-ures (for the two baselines and the new reranker)along with theirs, referred to as C & C (2010).The table shows the scores for every polarity value.For compatibility with their evaluation, we used theoverlap metric and carried out the evaluation us-ing a 10-fold cross-validation procedure on a 400-document subset of the MPQA corpus.POSITIVE P R FES+PC 59.3 46.2 51.8ES+ER+PC 53.1 50.9 52.0ES+PC+EPR 58.2 49.3 53.4C & C (2010) 67.1 31.8 43.1NEUTRAL P R FES+PC 61.0 49.3 54.3ES+ER+PC 55.1 57.7 56.4ES+PC+EPR 60.3 55.8 58.0C & C (2010) 66.6 31.9 43.1NEGATIVE P R FES+PC 71.6 52.2 60.3ES+ER+PC 65.4 58.2 61.6ES+PC+EPR 67.6 59.9 63.5C & C (2010) 76.2 40.4 52.8Table 2: Results with overlap metric.The C & C system shows a large precisionbias despite being optimized with respect to therecall-promoting overlap metric.
In recall and F-measure, their system scores much lower than oursimplest baseline, which is in turn clearly outper-formed by the stronger baseline and the polarity-based reranker.
The precision is lower than for C& C overall, but this is offset by recall boosts forall polarities that are much larger than the precisiondrops.
The polarity-based reranker (ES+PC+EPR)soundly outperforms all other systems.6 ConclusionWe have studied the implementation of end-to-endsystems for opinion expression extraction and po-larity labeling.
We first showed that it was easy to1In addition to polarity, their system also assigned opinionintensity which we do not consider here.improve over previous results simply by combiningan opinion extractor and a polarity classifier; the im-provements were between 7.5 and 11 points in over-lap F-measure.However, our most interesting result is that a jointmodel of expression extraction and polarity label-ing significantly improves over the sequential ap-proach.
This model uses features describing the in-teraction of opinions through linguistic structures.This precludes exact inference, but we resorted toa reranker.
The model was trained using approx-imate max-margin learning.
The final system im-proved over the baseline by 4 points in intersectionF-measure and 7 points in recall.
The improvementsover Choi and Cardie (2010) ranged between 10 and15 in overlap F-measure and between 17 and 24 inrecall.This is not only of practical value but also con-firms our linguistic intuitions that surface phenom-ena such as syntax and semantic roles are used inencoding the rhetorical organization of the sentence,and that we can thus extract useful information fromthose structures.
This would also suggest that weshould leave the surface and instead process the dis-course structure, and this has indeed been proposed(Somasundaran et al, 2009).
However, automaticdiscourse structure analysis is still in its infancywhile syntactic and shallow semantic parsing are rel-atively mature.Interesting future work should be devoted to ad-dress the use of structural kernels for the proposedreranker.
This would allow to better exploit syn-tactic and shallow semantic structures, e.g.
as in(Moschitti, 2008), also applying lexical similarityand syntactic kernels (Bloehdorn et al, 2006; Bloe-hdorn and Moschitti, 2007a; Bloehdorn and Mos-chitti, 2007b; Moschitti, 2009).AcknowledgementsThe research described in this paper has receivedfunding from the European Community?s Sev-enth Framework Programme (FP7/2007-2013) un-der grant 231126: LivingKnowledge ?
Facts, Opin-ions and Bias in Time, and under grant 247758:Trustworthy Eternal Systems via Evolving Software,Data and Knowledge (EternalS).105ReferencesStephan Bloehdorn and Alessandro Moschitti.
2007a.Combined syntactic and semantic kernels for text clas-sification.
In Proceedings of ECIR 2007, Rome, Italy.Stephan Bloehdorn and Alessandro Moschitti.
2007b.Structure and semantics for expressive text kernels.
InIn Proceedings of CIKM ?07.Stephan Bloehdorn, Roberto Basili, Marco Cammisa, andAlessandro Moschitti.
2006.
Semantic kernels for textclassification based on topological measures of featuresimilarity.
In Proceedings of ICDM 06, Hong Kong,2006.Eric Breck, Yejin Choi, and Claire Cardie.
2007.
Iden-tifying expressions of opinion in context.
In IJCAI2007, Proceedings of the 20th International Joint Con-ference on Artificial Intelligence, pages 2683?2688,Hyderabad, India.Yejin Choi and Claire Cardie.
2008.
Learning with com-positional semantics as structural inference for subsen-tential sentiment analysis.
In Proceedings of the 2008Conference on Empirical Methods in Natural Lan-guage Processing, pages 793?801, Honolulu, UnitedStates.Yejin Choi and Claire Cardie.
2010.
Hierarchical se-quential learning for extracting opinions and their at-tributes.
In Proceedings of the 48th Annual Meeting ofthe Association for Computational Linguistics, pages269?274, Uppsala, Sweden.Yejin Choi, Eric Breck, and Claire Cardie.
2006.
Jointextraction of entities and relations for opinion recog-nition.
In Proceedings of the 2006 Conference onEmpirical Methods in Natural Language Processing,pages 431?439, Sydney, Australia.Michael Collins.
2002.
Discriminative training meth-ods for hidden Markov models: Theory and experi-ments with perceptron algorithms.
In Proceedings ofthe 2002 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP 2002), pages 1?8.Thorsten Joachims, Thomas Finley, and Chun-Nam Yu.2009.
Cutting-plane training of structural SVMs.
Ma-chine Learning, 77(1):27?59.Richard Johansson and Alessandro Moschitti.
2010a.Reranking models in fine-grained opinion analysis.
InProceedings of the 23rd International Conference ofComputational Linguistics (Coling 2010), pages 519?527, Beijing, China.Richard Johansson and Alessandro Moschitti.
2010b.Syntactic and semantic structure for opinion expres-sion detection.
In Proceedings of the Fourteenth Con-ference on Computational Natural Language Learn-ing, pages 67?76, Uppsala, Sweden.Richard Johansson and Pierre Nugues.
2008.Dependency-based syntactic?semantic analysiswith PropBank and NomBank.
In CoNLL 2008:Proceedings of the Twelfth Conference on NaturalLanguage Learning, pages 183?187, Manchester,United Kingdom.Adam Meyers, Ruth Reeves, Catherine Macleod, RachelSzekely, Veronika Zielinska, Brian Young, and RalphGrishman.
2004.
The NomBank project: An interimreport.
In HLT-NAACL 2004 Workshop: Frontiersin Corpus Annotation, pages 24?31, Boston, UnitedStates.Alessandro Moschitti.
2008.
Kernel methods, syntax andsemantics for relational text categorization.
In Pro-ceeding of CIKM ?08, NY, USA.Alessandro Moschitti.
2009.
Syntactic and Seman-tic Kernels for Short Text Pair Categorization.
InProceedings of the 12th Conference of the EuropeanChapter of the ACL (EACL 2009), pages 576?584,Athens, Greece, March.
Association for Computa-tional Linguistics.Martha Palmer, Dan Gildea, and Paul Kingsbury.
2005.The proposition bank: An annotated corpus of seman-tic roles.
Computational Linguistics, 31(1):71?105.Swapna Somasundaran, Galileo Namata, Janyce Wiebe,and Lise Getoor.
2009.
Supervised and unsupervisedmethods in employing discourse relations for improv-ing opinion polarity classification.
In Proceedings ofEMNLP 2009: conference on Empirical Methods inNatural Language Processing.Ben Taskar, Carlos Guestrin, and Daphne Koller.
2004.Max-margin Markov networks.
In Advances in Neu-ral Information Processing Systems 16, Vancouver,Canada.Iannis Tsochantaridis, Thorsten Joachims, Thomas Hof-mann, and Yasemin Altun.
2005.
Large margin meth-ods for structured and interdependent output variables.Journal of Machine Learning Research, 6(Sep):1453?1484.Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005.Annotating expressions of opinions and emotions inlanguage.
Language Resources and Evaluation, 39(2-3):165?210.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-levelsentiment analysis.
In Proceedings of Human Lan-guage Technology Conference and Conference on Em-pirical Methods in Natural Language Processing.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2009.
Recognizing contextual polarity: An explo-ration of features for phrase-level sentiment analysis.Computational Linguistics, 35(3):399?433.106
