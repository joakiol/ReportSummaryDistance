Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 65?73,New York, June 2006. c?2006 Association for Computational LinguisticsRole of Local Context in Automatic Deidentificationof Ungrammatical, Fragmented TextTawanda SibandaCSAILMassachusetts Institute of TechnologyCambridge, MA 02139tawanda@mit.eduOzlem UzunerDepartment of Information StudiesCollege of Computing and InformationUniversity at Albany, SUNYAlbany, NY 12222ouzuner@albany.eduAbstractDeidentification of clinical records is acrucial step before these records can bedistributed to non-hospital researchers.Most approaches to deidentification relyheavily on dictionaries and heuristic rules;these approaches fail to remove most per-sonal health information (PHI) that cannotbe found in dictionaries.
They also can failto remove PHI that is ambiguous betweenPHI and non-PHI.Named entity recognition (NER) tech-nologies can be used for deidentification.Some of these technologies exploit bothlocal and global context of a word to iden-tify its entity type.
When documents aregrammatically written, global context canimprove NER.In this paper, we show that we can dei-dentify medical discharge summaries us-ing support vector machines that rely on astatistical representation of local context.We compare our approach with three dif-ferent systems.
Comparison with a rule-based approach shows that a statisticalrepresentation of local context contributesmore to deidentification than dictionariesand hand-tailored heuristics.
Compari-son with two well-known systems, SNoWand IdentiFinder, shows that when the lan-guage of documents is fragmented, localcontext contributes more to deidentifica-tion than global context.1 IntroductionMedical discharge summaries contain informationthat is useful to clinical researchers who study theinteractions between, for example, different med-ications and diseases.
However, these summariesinclude explicit personal health information (PHI)whose release would jeopardize privacy.
In theUnited States, the Health Information Portabilityand Accountability Act (HIPAA) provides guide-lines for protecting the confidentiality of health careinformation.
HIPAA lists seventeen pieces of textualPHI of which the following appear in medical dis-charge summaries: first and last names of patients,their health proxies, and family members; doctors?first and last names; identification numbers; tele-phone, fax, and pager numbers; hospital names; ge-ographic locations; and dates.
Removing PHI frommedical documents is the goal of deidentification.This paper presents a method based on a statis-tical representation of local context for automati-cally removing explicit PHI from medical dischargesummaries, despite the often ungrammatical, frag-mented, and ad hoc language of these documents,even when some words in the documents are am-biguous between PHI and non-PHI (e.g., ?Hunting-ton?
as the name of a person and as the name ofa disease), and even when some of the PHI cannotbe found in dictionaries (e.g., misspelled and/or for-eign names).
This method differs from traditionalapproaches to deidentification in its independencefrom dictionaries and hand-tailored heuristics.
Itapplies statistical named entity recognition (NER)methods to the more challenging task of deidenti-65fication but differs from traditional NER approachesin its heavy reliance on a statistical representation oflocal context.
Finally, this approach targets all PHIthat appear in medical discharge summaries.
Experi-ments reported in this paper show that context playsa more important role in deidentification than dic-tionaries, and that a statistical representation of lo-cal context contributes more to deidentification thanglobal context.2 Related WorkIn the literature, named entities such as people,places, and organizations mentioned in news arti-cles have been successfully identified by various ap-proaches (Bikel et al, 1999; McCallum et al, 2000;Riloff and Jones, 1996; Collins and Singer, 1999;Hobbs et al, 1996).
Most of these approaches aretailored to a particular domain, e.g., understandingdisaster news; they exploit both the characteristicsof the entities they focus on and the contextual cluesrelated to these entities.In the biomedical domain, NER has focused onidentification of biological entities such as genesand proteins (Collier et al, 2000; Yu et al, 2002).Various statistical approaches, e.g., a maximumentropy model (Finkel et al, 2004), HMMs andSVMs (GuoDong et al, 2005), have been used withvarious feature sets including surface and syntac-tic features, word formation patterns, morphologi-cal patterns, part-of-speech tags, head noun triggers,and coreferences.Deidentification refers to the removal of identi-fying information from records.
Some approachesto deidentification have focused on particular cat-egories of PHI, e.g., Taira et al focused on onlypatient names (2002), Thomas et al focused onproper names including doctors?
names (2002).
Forfull deidentification, i.e., removal of all PHI, Guptaet al used ?a complex set of rules, dictionaries,pattern-matching algorithms, and Unified MedicalLanguage System?
(2004).
Sweeney?s Scrub sys-tem employed competing algorithms that used pat-terns and lexicons to find PHI.
Each of the algo-rithms included in her system specialized in onekind of PHI, each calculated the probability that agiven word belonged to the class of PHI that it spe-cialized in, and the algorithm with the highest prece-dence and the highest probability labelled the givenword.
This system identified 99-100% of all PHI inthe test corpus of patient records and letters to physi-cians (1996).We use a variety of features to train a supportvector machine (SVM) that can automatically ex-tract local context cues and can recognize PHI (evenwhen some PHI are ambiguous between PHI andnon-PHI, and even when PHI do not appear in dic-tionaries).
We compare this approach with threeothers: a heuristic rule-based approach (Douglass,2005), the SNoW (Sparse Network of Winnows)system?s NER component (Roth and Yih, 2002), andIdentiFinder (Bikel et al, 1999).
The heuristic rule-based system relies heavily on dictionaries.
SNoWand IdentiFinder consider some representation of thelocal context of words; they also rely on informa-tion about global context.
Local context helps themrecognize stereotypical names and name structures.Global context helps these systems update the prob-ability of observing a particular entity type based onthe other entity types contained in the sentence.
Wehypothesize that, given the mostly fragmented andungrammatical nature of discharge summaries, localcontext will be more important for deidentificationthan global context.
We further hypothesize that lo-cal context will be a more reliable indication of PHIthan dictionaries (which can be incomplete).
The re-sults presented in this paper show that SVMs trainedwith a statistical representation of local context out-perform all baselines.
In other words, a classifierthat relies heavily on local context (very little ondictionaries, and not at all on global context) out-performs classifiers that rely either on global con-text or dictionaries (but make much less use of lo-cal context).
Global context cannot contribute muchto deidentification when the language of documentsis fragmented; dictionaries cannot contribute to dei-dentification when PHI are either missing from dic-tionaries or are ambiguous between PHI and non-PHI.
Local context remains a reliable indication ofPHI under these circumstances.The features used for our SVM-based system canbe enriched in order to automatically acquire moreand varied local context information.
The featuresdiscussed in this paper have been chosen because oftheir simplicity and effectiveness on both grammati-cal and ungrammatical free text.663 CorporaDischarge summaries are the reports generated bymedical personnel at the end of a patient?s hospi-tal stay and contain important information about thepatient?s health.
Linguistic processing of these doc-uments is challenging, mainly because these reportsare full of medical jargon, acronyms, shorthand no-tations, misspellings, ad hoc language, and frag-ments of sentences.
Our goal is to identify the PHIused in discharge summaries even when text is frag-mented and ad hoc, even when many words in thesummaries are ambiguous between PHI and non-PHI, and even when many PHI contain misspelledor foreign words.In this study, we worked with various corporaconsisting of discharge summaries.
One of thesecorpora was obtained already deidentified1; i.e.,(many) PHI (and some non-PHI) found in this cor-pus had been replaced with the generic placeholder[REMOVED].
An excerpt from this corpus is below:HISTORY OF PRESENT ILLNESS: The patientis a 77-year-old-woman with long standing hyper-tension who presented as a Walk-in to me at the[REMOVED] Health Center on [REMOVED].
Re-cently had been started q.o.d.
on Clonidine since[REMOVED] to taper off of the drug.
Was told tostart Zestril 20 mg. q.d.
again.
The patient was sentto the [REMOVED] Unit for direct admission forcardioversion and anticoagulation, with the Cardi-ologist, Dr. [REMOVED] to follow.SOCIAL HISTORY: Lives alone, has one daughterliving in [REMOVED].
Is a non-smoker, and doesnot drink alcohol.HOSPITAL COURSE AND TREATMENT: Dur-ing admission, the patient was seen by Cardiology,Dr.
[REMOVED], was started on IV Heparin, So-talol 40 mg PO b.i.d.
increased to 80 mg b.i.d.,and had an echocardiogram.
By [REMOVED] thepatient had better rate control and blood pressurecontrol but remained in atrial fibrillation.
On [RE-MOVED], the patient was felt to be medically sta-ble....We hand-annotated this corpus and experimentedwith it in several ways: we used it to generatea corpus of discharge summaries in which the[REMOVED] tokens were replaced with appropri-ate, fake PHI obtained from dictionaries2 (Douglass,1Authentic clinical data is very difficult to obtain for privacyreasons; therefore, the initial implementation of our system wastested on previously deidentified data that we reidentified.2e.g., John Smith initiated radiation therapy ...2005); we used it to generate a second corpus inwhich most of the [REMOVED] tokens and someof the remaining text were appropriately replacedwith lexical items that were ambiguous between PHIand non-PHI3; we used it to generate another cor-pus in which all of the [REMOVED] tokens corre-sponding to names were replaced with appropriatelyformatted entries that could not be found in dictio-naries4.
For all of these corpora, we generated real-istic substitutes for the [REMOVED] tokens usingdictionaries (e.g., a dictionary of names from USCensus Bureau) and patterns (e.g., names of peoplecould be of the formats, ?Mr.
F.
Lastname?, ?First-name Lastname?, ?Lastname?, ?F.
M.
Lastname?,etc.
; dates could appear as ?dd/mm/yy?, ?dd Mon-thName, yyyy?, ?ddth of MonthName, yyyy?, etc.
).In addition to these reidentified corpora (i.e., cor-pora generated from previously deidentified data),we also experimented with authentic discharge sum-maries5.
The approximate distributions of PHI in thereidentified corpora and in the authentic corpus areshown in Table 1.Class No.
in reidentified No.
in authenticsummaries summariesNon-PHI 17872 112720Patient 1047 287Doctor 311 730Location 24 84Hospital 592 651Date 735 1933ID 36 477Phone 39 32Table 1: Distribution of different PHI (in terms of number ofwords) in the corpora.4 Baseline Approaches4.1 Rule-Based Baseline: Heuristic+DictionaryTraditional deidentification approaches rely heavilyon dictionaries and hand-tailored heuristics.3e.g., D. Sessions initiated radiation therapy...4e.g., O. Ymfgkstjj initiated radiation therapy ...5We obtained authentic discharge summaries with real PHIin the final stages of this project.67We obtained one such system (Douglass, 2005)that used three kinds of dictionaries:?
PHI lookup tables for female and male firstnames, last names, last name prefixes, hospitalnames, locations, and states.?
A dictionary of ?common words?
that shouldnever be classified as PHI.?
Lookup tables for context clues such as titles,e.g., Mr.; name indicators, e.g., proxy, daugh-ter; location indicators, e.g., lives in.Given these dictionaries, this system identifies key-words that appear in the PHI lookup tables but donot occur in the common words list, finds approx-imate matches for possibly misspelled words, anduses patterns and indicators to find PHI.4.2 SNoWSNoW is a statistical classifier that includes a NERcomponent for recognizing entities and their rela-tions.
To create a hypothesis about the entity type ofa word, SNoW first takes advantage of ?words, tags,conjunctions of words and tags, bigram and trigramof words and tags?, number of words in the entity,bigrams of words in the entity, and some attributessuch as the prefix and suffix, as well as informa-tion about the presence of the word in a dictionaryof people, organization, and location names (Rothand Yih, 2002).
After this initial step, it uses thepossible relations of the entity with other entities inthe sentence to strengthen or weaken its hypothe-sis about the entity?s type.
The constraints imposedon the entities and their relationships constitute theglobal context of inference.
Intuitively, informationabout global context and constraints imposed on therelationships of entities should improve recognitionof both entities and relations.
Roth and Yih (2002)present results that support this hypothesis.SNoW can recognize entities that correspond topeople, locations, and organizations.
For deidenti-fication purposes, all of these entities correspond toPHI; however, they do not constitute a comprehen-sive set.
We evaluated SNoW only on the PHI it isbuilt to recognize.
We trained and tested its NERcomponent using ten-fold cross-validation on eachof our corpora.4.3 IdentiFinderIdentiFinder uses Hidden Markov Models to learnthe characteristics of names of entities, includingpeople, locations, geographic jurisdictions, organi-zations, dates, and contact information (Bikel et al,1999).
For each named entity class, this systemlearns a bigram language model which indicates thelikelihood that a sequence of words belongs to thatclass.
This model takes into consideration featuresof words, such as whether the word is capitalized, allupper case, or all lower case, whether it is the firstword of the sentence, or whether it contains digitsand punctuation.
Thus, it captures the local contextof the target word (i.e., the word to be classified; alsoreferred to as TW).
To find the names of all entities,the system finds the most likely sequence of entitytypes in a sentence given a sequence of words; thus,it captures the global context of the entities in a sen-tence.We obtained this system pre-trained on a newscorpus and applied it to our corpora.
We mappedits entity tags to our PHI and non-PHI labels.
Ad-mittedly, testing IdentiFinder on the discharge sum-maries puts this system at a disadvantage comparedto the other statistical approaches.
However, despitethis shortcoming, IdentiFinder helps us evaluate thecontribution of global context to deidentification.5 SVMs with Local ContextWe hypothesize that systems that rely on dictionar-ies and hand-tailored heuristics face a major chal-lenge when particular PHI can be used in many dif-ferent contexts, when PHI are ambiguous, or whenthe PHI cannot be found in dictionaries.
We furtherhypothesize that given the ungrammatical and adhoc nature of our data, despite being very powerfulsystems, IdentiFinder and SNoW may not provideperfect deidentification.
In addition to being veryfragmented, discharge summaries do not present in-formation in the form of relations between entities,and many sentences contain only one entity.
There-fore, the global context utilized by IdentiFinder andSNoW cannot contribute reliably to deidentification.When run on discharge summaries, the strength ofthese systems comes from their ability to recognizethe structure of the names of different entity typesand the local contexts of these entities.68Discharge summaries contain patterns that canserve as local context.
Therefore, we built an SVM-based system that, given a target word (TW), wouldaccurately predict whether the TW was part of PHI.We used a development corpus to find features thatcaptured as much of the immediate context of theTW as possible, paying particular attention to cueshuman annotators found useful for deidentification.We added to this some surface characteristics for theTW itself and obtained the following features: theTW itself, the word before, and the word after (alllemmatized); the bigram before and the bigram af-ter TW (lemmatized); the part of speech of TW, ofthe word before, and of the word after; capitalizationof TW; length of TW; MeSH ID of the noun phrasecontaining TW (MeSH is a dictionary of MedicalSubject Headings and is a subset of the Unified Med-ical Language System (UMLS) of the National Li-brary of Medicine); presence of TW, of the wordbefore, and of the word after TW in the name, lo-cation, hospital, and month dictionaries; the headingof the section in which TW appears, e.g., ?Historyof Present Illness?
; and, whether TW contains ?-?
or?/?
characters.
Note that some of these features, e.g.,capitalization and punctuation within TW, were alsoused in IdentiFinder.We used the SVM implementation provided byLIBSVM (Chang and Lin, 2001) with a linear ker-nel to classify each word in the summaries as ei-ther PHI or non-PHI based on the above-listed fea-tures.
We evaluated this system using ten-fold cross-validation.6 EvaluationLocal context contributes differently to each of thefour deidentification systems.
Our SVM-based ap-proach uses only local context.
The heuristic, rule-based system relies heavily on dictionaries.
Identi-Finder uses a simplified representation of local con-text and adds to this information about the globalcontext as represented by transition probabilities be-tween entities in the sentence.
SNoW uses local con-text as well, but it also makes an effort to benefitfrom relations between entities.
Given the differencein the strengths of these systems, we compared theirperformance on both the reidentified and authenticcorpora (see Section 3).
We hypothesized that giventhe nature of medical discharge summaries, Iden-tiFinder would not be able to find enough globalcontext and SNoW would not be able to make useof relations (because many sentences in this cor-pus contain only one entity).
We further hypothe-sized that when the data contain words ambiguousbetween PHI and non-PHI, or when the PHI cannotbe found in dictionaries, the heuristic, rule-based ap-proach would perform poorly.
In all of these cases,SVMs trained with local context information wouldbe sufficient for proper deidentification.To compare the SVM approach with Identi-Finder, we evaluated both on PHI consisting ofnames of people (i.e., patient and doctor names),locations (i.e., geographic locations), and organiza-tions (i.e., hospitals), as well as PHI consisting ofdates, and contact information (i.e., phone numbers,pagers).
We omitted PHI representing ID numbersfrom this experiment in order to be fair to Identi-Finder which was not trained on this category.
Tocompare the SVM approach with SNoW, we trainedboth systems with only PHI consisting of names ofpeople, locations, and organizations, i.e., the entitiesthat SNoW was designed to recognize.6.1 Deidentifying Reidentified and AuthenticDischarge SummariesWe first deidentified:?
Previously deidentified discharge summariesinto which we inserted invented but realisticsurrogates for PHI without deliberately intro-ducing ambiguous words or words not found indictionaries, and?
Authentic discharge summaries with real PHI.Our experiments showed that SVMs with localcontext outperformed all other approaches.
On thereidentified corpus, SVMs gave an F-measure of97.2% for PHI.
In comparison, IdentiFinder, hav-ing been trained on the news corpus, gave an F-measure of 67.4% and was outperformed by theheuristic+dictionary approach (see Table 2).66Note that in deidentification, recall is much more importantthan precision.
Low recall indicates that many PHI remain inthe documents and that there is high risk to patient privacy.
Lowprecision means that words that do not correspond to PHI havealso been removed.
This hurts the integrity of the data but doesnot present a risk to privacy.69We evaluated SNoW only on the three kindsof entities it is designed to recognize.
We cross-validated it on our corpora and found that its per-formance in recognizing people, locations, and or-ganizations was 96.2% in terms of F-measure (seeTable 37).
In comparison, our SVM-based system,when retrained to only consider people, locations,and organizations so as to be directly comparable toSNoW, had an F-measure of 98%.8Method Class P R FSVM PHI 96.8% 97.7% 97.2%IFinder PHI 60.2% 76.7% 67.4%H+D PHI 88.9% 67.6% 76.8%SVM Non-PHI 99.6% 99.5% 99.6%IFinder Non-PHI 95.8% 91.4% 93.6%H+D Non-PHI 95.2% 95.2% 95.2%Table 2: Precision, Recall, and F-measure on reidentified dis-charge summaries.
IFinder refers to IdentiFinder, H+D refers toheuristic+dictionary approach.Method Class P R FSVM PHI 97.7% 98.2% 98.0%SNoW PHI 96.1% 96.2% 96.2%SVM Non-PHI 99.8% 99.8% 99.8%SNoW Non-PHI 99.6% 99.6% 99.6%Table 3: Evaluation of SNoW and SVM on recognizing peo-ple, locations, and organizations found in reidentified dischargesummaries.Similarly, on the authentic discharge summaries,the SVM approach outperformed all other ap-proaches in recognizing PHI (see Tables 4 and 5).6.2 Deidentifying Data with Ambiguous PHIIn discharge summaries, the same words can appearboth as PHI and as non-PHI.
For example, in thesame corpus, the word ?Swan?
can appear both asthe name of a medical device (i.e., ?Swan Catheter?
)and as the name of a person, etc.
Ideally, we wouldlike to deidentify data even when many words in the7The best performances are marked in bold in all of the ta-bles in this paper.8For all of the corpora presented in this paper, a performancedifference of 1% or more is statistically significant at ?
= 0.05.Method Class P R FSVM PHI 97.5% 95.0% 96.2%IFinder PHI 25.2% 45.2% 32.3%H+D PHI 81.9% 87.6% 84.7%SVM Non-PHI 99.8% 99.9% 99.9%IFinder Non-PHI 97.1% 93.3% 95.2%H+D Non-PHI 99.6% 99.6% 99.6%Table 4: Evaluation on authentic discharge summaries.Method Class P R FSVM PHI 97.4% 93.8% 95.6%SNoW PHI 93.7% 93.4% 93.6%SVM Non-PHI 99.9% 100% 100%SNoW Non-PHI 99.9% 99.9% 99.9%Table 5: Evaluation of SNoW and SVM on authentic dis-charge summaries.corpus are ambiguous between PHI and non-PHI.We hypothesize that given ambiguities in the data,context will play an important role in determiningwhether the particular instance of the word is PHIand that given the many fragmented sentences in ourcorpus, local context will be particularly useful.
Totest these hypotheses, we generated a corpus by rei-dentifying the previously deidentified corpus withwords that were ambiguous between PHI and non-PHI, making sure to use each ambiguous word bothas PHI and non-PHI, and also making sure to coverall acceptable formats of all PHI (see Section 3).
Theresulting distribution of PHI is shown in Table 6.Class Total # Words # Ambiguous WordsNon-PHI 19296 3781Patient 1047 514Doctor 311 247Location 24 24Hospital 592 82Date 736 201ID 36 0Phone 39 0Table 6: Distribution of PHI when some words are ambiguousbetween PHI and non-PHI.70Our results showed that, on this corpus, the SVM-based system accurately recognized 91.9% of allPHI; its performance, measured in terms of F-measure was also significantly better than all otherapproaches both on the complete corpus containingambiguous entries (see Table 7 and Table 8) and onlyon the ambiguous words in this corpus (see Table 9).Method Class P R FSVM PHI 92.0% 92.1% 92.0%IFinder PHI 45.4% 71.4% 55.5%H+D PHI 70.1% 46.6% 56.0%SVM Non-PHI 98.9% 98.9% 98.9%IFinder Non-PHI 95.0% 86.5% 90.1%H+D Non-PHI 92.7% 92.7% 92.7%Table 7: Evaluation on the corpus containing ambiguousdata.Method Class P R FSVM PHI 92.1% 92.8% 92.5%SNoW PHI 91.6% 77% 83.7%SVM Non-PHI 99.3% 99.2% 99.3%SNoW Non-PHI 97.6% 99.3% 98.4%Table 8: Evaluation of SNoW and SVM on ambiguous data.Method Class P R FSVM PHI 90.2% 87.5% 88.8%IFinder PHI 55.8% 64.0% 59.6%H+D PHI 59.8% 24.3% 34.6%SNoW PHI 91.6% 82.9% 87.1%SVM Non-PHI 90.5% 92.7% 91.6%IFinder Non-PHI 69.0% 61.3% 64.9%H+D Non-PHI 59.9% 87.4% 71.1%SNoW Non-PHI 90.4% 95.5% 92.9%Table 9: Evaluation only on ambiguous people, locations,and organizations found in ambiguous data.6.3 Deidentifying PHI Not Found inDictionariesSome medical documents contain foreign or mis-spelled names that need to be effectively removed.To evaluate the different deidentification approachesunder such circumstances, we generated a corpus inwhich the names of people, locations, and hospitalswere all random permutations of letters.
The result-ing words were not found in any dictionaries but fol-lowed the general format of the entity name categoryto which they belonged.
The distribution of PHI inthis third corpus is in Table 10.Class Total PHI PHI Not in Dict.Non-PHI 17872 0Patient 1045 1045Doctor 302 302Location 24 24Hospital 376 376Date 735 0ID 36 0Phone 39 0Table 10: Distribution of PHI in the corpus where all PHIassociated with names are randomly generated so as not to befound in dictionaries.On this data set, dictionaries cannot contribute todeidentification because none of the PHI appear indictionaries.
Under these conditions, proper deiden-tification relies completely on context.
Our resultsshowed that SVM approach outperformed all otherapproaches on this corpus also (Tables 11 and 12).Method Class P R FSVM PHI 94.0% 96.0% 95.0%IFinder PHI 55.1% 65.5% 59.8%H+D PHI 76.4% 27.8% 40.8%SVM Non-PHI 99.4% 99.1% 99.3%IFinder Non-PHI 94.4% 91.6% 92.9%H+D Non-PHI 90.7% 90.7% 90.7%Table 11: Evaluation on the corpus containing PHI not indictionaries.Of only the PHI not found in dictionaries, 95.5%was accurately identified by the SVM approach.
Incomparison, the heuristic+dictionary approach ac-curately identified those PHI that could not be foundin dictionaries 11.1% of the time, IdentiFinder rec-ognized these entities 76.7% of the time and SNoWgave an accuracy of 79% (see Table 13).71Method Class P R FSVM PHI 93.9% 96.0% 95.0%SNoW PHI 93.7% 79.0% 85.7%SVM Non-PHI 99.6% 99.4% 99.5%SNoW Non-PHI 98.0% 99.5% 98.7%Table 12: Evaluation of SNoW and SVM on the people, loca-tions, and organizations found in the corpus containing PHI notfound in dictionaries.Method SVM IFinder SNoW H+DPrecision 95.5% 76.7% 79.0% 11.1%Table 13: Precision on only the PHI not found in dictionaries.6.4 Feature ImportanceAs hypothesized, in all experiments, the SVM-based approach outperformed all other approaches.SVM?s feature set included a total of 26 features,12 of which were dictionary-related features (ex-cluding MeSH).
Information gain showed that themost informative features for deidentification werethe TW, the bigram before TW, the bigram after TW,the word before TW, and the word after TW.Note that the TW itself is important for classifi-cation; many of the non-PHI correspond to commonwords that appear in the corpus frequently and theSVM learns the fact that some words, e.g., the, ad-mit, etc., are never PHI.
In addition, the context ofTW (captured in the form of unigrams and bigramsof words and part-of-speech tags surrounding TW)contributes significantly to deidentification.There are many ways of automatically capturingcontext.
In our data, unigrams and bigrams of wordsand their part-of-speech tags seem to be sufficientfor a statistical representation of local context.
Theglobal context, as represented within IdentiFinderand SNoW, could not contribute much to deiden-tification on this corpus because of the fragmentednature of the language of these documents, becausemost sentences in this corpus contain only one en-tity, and because many sentences do not include ex-plicit relations between entities.
However, there isenough structure in this data that can be captured bylocal context; lack of relations between entities andthe inability to capture global context do not hold usback from almost perfect deidentification.7 ConclusionWe presented a set of experimental results that showthat local context contributes more to deidentifica-tion than dictionaries and global context when work-ing with medical discharge summaries.
These docu-ments are characterized by incomplete, fragmentedsentences, and ad hoc language.
They use a lotof jargon, many times omit subjects of sentences,use entity names that can be misspelled or foreignwords, can include entity names that are ambigu-ous between PHI and non-PHI, etc.
Similar doc-uments in many domains exist; our experimentshere show that even on such challenging corpora,local context can be exploited to identify entities.Even a rudimentary statistical representation of lo-cal context, as captured by unigrams and bigrams oflemmatized keywords and part-of-speech tags, givesgood results and outperforms more sophisticated ap-proaches that rely on global context.
The simplicityof the representation of local context and the resultsobtained using this simple representation are partic-ularly promising for many tasks that require pro-cessing ungrammatical and fragmented text whereglobal context cannot be counted on.8 AcknowledgementsThis publication was made possible by grant num-ber R01-EB001659 from the National Instituteof Biomedical Imaging and Bioengineering; bygrant number N01-LM-3-3513 on National Multi-Protocol Ensemble for Self-Scaling Systems forHealth from National Library of Medicine; and, bygrant number U54-LM008748 on Informatics for In-tegrating Biology to the Bedside from National Li-brary of Medicine.We are grateful to Professor Peter Szolovits andDr.
Boris Katz for their insights, and to ProfessorCarol Doll, Sue Felshin, Gregory Marton, and TianHe for their feedback on this paper.ReferencesJ.
J. Berman.
2002.
Concept-Match Medical DataScrubbing: How Pathology Text Can Be Used inResearch.
Archives of Pathology and LaboratoryMedicine, 127(6).D.
M. Bikel, R. Schwartz, and R. M. Weischedel.
1999.72An Algorithm That Learns What?s in a Name.
Ma-chine Learning Journal Special Issue on Natural Lan-guage Learning, 34(1/3).C.
Chang and C. Lin.
2001.
LIBSVM: a Library for Sup-port Vector Machines.N.
Collier, C. Nobata, and J. Tsujii.
2000.
Extractingthe Names of Genes and Gene Products with a HiddenMarkov Model.
Proceedings of COLING.M.
Collins and Y.
Singer.
1999.
Unsupervised Mod-els for Named Entity Classification.
Proceedings ofEMNLP.J.
Finkel, S. Dingare, H. Nguyen, M. Nissim, C. Man-ning, and G. Sinclair.
2004.
Exploiting Context forBiomedical Entity Recognition: From Syntax to theWeb.
Proceedings of Joint Workshop on Natural Lan-guage Processing in Biomedicine and its Applicationsat COLING.R.
Gaizauskas, G. Demetriou, P. Artymiuk, and P. Willett.2003.
Protein Structures and Information Extractionfrom Biological Texts: The PASTA System.
Bioinfor-matics, 19(1).Z.
GuoDong, Z. Jie, S. Jian, S. Dan, T. ChewLim.
2005.Recognizing Names in Biomedical Texts: a MachineLearning Approach.
Bioinformatics, 20(7).D.
Gupta, M. Saul, J. Gilbertson.
2004.
Evalua-tion of a Deidentification (De-Id) Software Engine toShare Pathology Reports and Clinical Documents forResearch.
American Journal of Clinical Pathology,121(6).J.
R. Hobbs, D. E. Appelt, J.
Bear, D. Israel, M.Kameyama, M. Stickel, and M. Tyson.
1996.
FAS-TUS: A Cascaded Finite-State Transducer for Extract-ing Information from Natural-Language Text.
In Fi-nite State Devices for Natural Language Processing.MIT Press, Cambridge, MA.M.
Douglass, G. D. Clifford, A. Reisner, G. B. Moody,R.
G. Mark.
2005.
Computer-Assisted De-Identification of Free Text in the MIMIC II Database.Computers in Cardiology.
32:331-334.A.
McCallum, D. Freitag, and F. Pereira.
2000.
Maxi-mum Entropy Markov Models for Information Extrac-tion and Segmentation.
Proceedings of ICML.E.
Riloff and R. Jones.
1996.
Automatically GeneratingExtraction Patterns from Untagged Text.
Proceedingsof AAAI-96.D.
Roth and W. Yih.
2002.
Probabilistic Reasoningfor Entity and Relation Recognition.
Proceedings ofCOLING.P.
Ruch, R. H. Baud, A. Rassinoux, P. Bouillon, G.Robert.
2000.
Medical Document Anonymizationwith a Semantic Lexicon.
Proceedings of AMIA.M.
Surdeanu, S. M. Harabagiu, J. Williams, and P.Aarseth.
2003.
Using Predicate-Argument Structuresfor Information Extraction.
Proceedings of ACL 2003.L.
Sweeney.
1996.
Replacing personally-identifying in-formation in medical records, the scrub system.
Jour-nal of the American Medical Informatics Association.R.
K. Taira, A.
A. T. Bui, H. Kangarloo.
2002.
Identifi-cation of patient name references within medical doc-uments using semantic selectional restrictions.
Pro-ceedings of AMIA.S.
M. Thomas, B. Mamlin, G. Schadow, C. McDonald.2002.
A Successful Technique for Removing Namesin Pathology Reports Using an Augmented Search andReplace Method.
Proceedings of AMIA.H.
Yu, V. Hatzivassiloglou, C. Friedman, W. J. Wilbur.2002.
Automatic Extraction of Gene and Protein Syn-onyms from MEDLINE and Journal Articles.
Pro-ceedings of AMIA.73
