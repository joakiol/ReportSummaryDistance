Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 309?315,Sydney, July 2006. c?2006 Association for Computational LinguisticsReduced n-gram models for English and Chinese corporaLe Q Ha, P Hanna, D W Stewart and F J SmithSchool of Electronics, Electrical Engineering and Computer Science,Queen?s University BelfastBelfast BT7 1NN, Northern Ireland, United Kingdomlequanha@lequanha.comAbstractStatistical language models shouldimprove as the size of the n-gramsincreases from 3 to 5 or higher.
However,the number of parameters andcalculations, and the storage requirementincrease very rapidly if we attempt tostore all possible combinations ofn-grams.
To avoid these problems, thereduced n-grams?
approach previouslydeveloped by O?Boyle (1993) can beapplied.
A reduced n-gram languagemodel can store an entire corpus?sphrase-history length within feasiblestorage limits.
Another theoreticaladvantage of reduced n-grams is that theyare closer to being semantically completethan traditional models, which include alln-grams.
In our experiments, the reducedn-gram Zipf curves are first presented,and compared with previously obtainedconventional n-grams for both Englishand Chinese.
The reduced n-gram modelis then applied to large English andChinese corpora.
For English, we canreduce the model sizes, compared to7-gram traditional model sizes, withfactors of 14.6 for a 40-million-wordcorpus and 11.0 for a 500-million-wordcorpus while obtaining 5.8% and 4.2%improvements in perplexities.
ForChinese, we gain a 16.9% perplexityreductions and we reduce the model sizeby a factor larger than 11.2.
This paper isa step towards the modeling of Englishand Chinese using semantically completephrases in an n-gram model.1 Introduction to the Reduced N-GramApproachShortly after this laboratory first published avariable n-gram algorithm (Smith and O?Boyle,1992), O?Boyle (1993) proposed a statisticalmethod to improve language models based on theremoval of overlapping phrases.A distortion in the use of phrase frequencieshad been observed in the small railway timetableVodis Corpus when the bigram ?RAILENQUIRIES?
and its super-phrase ?BRITISHRAIL ENQUIRIES?
were examined.
Both occur73 times, which is a large number for such asmall corpus.
?ENQUIRIES?
follows ?RAIL?with a very high probability when it is precededby ?BRITISH.?
However, when ?RAIL?
ispreceded by words other than ?BRITISH,??ENQUIRIES?
does not occur, but words like?TICKET?
or ?JOURNEY?
may.
Thus, thebigram ?RAIL ENQUIRIES?
gives a misleadingprobability that ?RAIL?
is followed by?ENQUIRIES?
irrespective of what precedes it.At the time of their research, O?Boyle reducedthe frequencies of ?RAIL ENQUIRIES?
bysubtracting the frequency of the larger trigram,which gave a probability of zero for?ENQUIRIES?
following ?RAIL?
if it was notpreceded by ?BRITISH.?
The phrase with a newreduced frequency is called a reduced phrase.Therefore, a phrase can occur in a corpus as areduced n-gram in some places and as part of alarger reduced n-gram in other places.
In areduced model, the occurrence of an n-gram isnot counted when it is a part of a larger reducedn-gram.
One algorithm to detect/identify/extractreduced n-grams from a corpus is the so-calledreduced n-gram algorithm.
In 1992, O?Boyle wasable to use it to analyse the Brown corpus ofAmerican English (Francis and Kucera, 1964) (ofone million word tokens, whose longest phrase-309length is 30), which was a considerableimprovement at the time.
The results were usedin an n-gram language model by O?Boyle, butwith poor results, due to lack of statistics fromsuch a small corpus.
We have developed andpresent here a modification of his method, andwe discuss its usefulness for reducing n-gramperplexity.2 Similar Approaches and CapabilityRecent progress in variable n-gram languagemodeling has provided an efficientrepresentation of n-gram models and made thetraining of higher order n-grams possible.Compared to variable n-grams, class-basedlanguage models are more often used to reducethe size of a language model, but this typicallyleads to recognition performance degradation.Classes can alternatively be used to smooth alanguage model or provide back-off estimates,which have led to small performance gains.
Forthe LOB corpus, the varigram model obtained11.3% higher perplexity in comparison with theword-trigram model (Niesler and Woodland,1996.
)Kneser (1996) built up variable-context lengthlanguage models based on the North AmericanBusiness News (NAB-240 million words) andthe German Verbmobil (300,000 words with avocabulary of 5,000 types.)
His results show thatthe variable-length model outperformsconventional models of the same size, and if amoderate loss in performance is acceptable, thatthe size of a language model can be reduceddrastically by using his pruning algorithm.Kneser?s results improve with longer contextsand a same number of parameters.
For example,reducing the size of the standard NAB trigrammodel by a factor of 3 results in a loss of only7% in perplexity and 3% in the word error rate.The improvement obtained by Kneser?s methoddepended on the length of the fixed context andon the amount of available training data.
In thecase of the NAB corpus, the improvement was10% in perplexity.Siu and Ostendorf (2000) developed Kneser?sbasic ideas further and applied the variable4-gram, thus improving the perplexity and worderror rate results compared to a fixed trigrammodel.
They obtained word error reductions of0.1 and 0.5% (absolute) in development andevaluation test sets, respectively.
However, thenumber of parameters was reduced by 60%.
Byusing the variable 4-gram, they were able tomodel a longer history while reducing the size ofthe model by more than 50%, compared to aregular trigram model, and at the same timeimprove both the test-set perplexity andrecognition performance.
They also reduced thesize of the model by an additional 8%.Other related work are those of Seymore andRosenfeld (1996); Hu, Turin and Brown (1997);Blasig (1999); and Goodman and Gao (2000.
)In order to obtain an overview of variablen-grams, Table 1 combines all of their results.COMBINATION OF LANGUAGE MODEL TYPESBasicn-gramVariablen-gramsCategory SkippingdistanceClasses #params Perplexity Size SourceTrigram?
987k 474Bigram?
- 603.2Trigram?
- 544.1?
?
- 534.11M LOBTrigram?
743k 81.5Trigram?
379k 78.1Trigram?
?
363k 78.0Trigram?
?
?
338k 77.74-gram?
580k 1084-gram?
?
577k 1084-gram?
?
?
536k 1075-gram?
383k 77.55-gram?
?
381k 77.45-gram?
?
?
359k 77.22M SwitchboardCorpusTable 1.
Comparison of combinations of variable n-grams and other Language Models3103 Reduced N-Gram AlgorithmThe main goal of this algorithm (Ha, 2005) is toproduce three main files from the training text:?
The file that contains all the completen-grams appearing at least m times iscalled the PHR file (m ?
2.)?
The file that contains all the n-gramsappearing as sub-phrases, following theremoval of the first word from any othercomplete n-gram in the PHR file, is calledthe SUB file.?
The file that contains any overlappingn-grams that occur at least m times in theSUB file is called the LOS file.The final list of reduced phrases is called the FINfile, whereSUBLOSPHRFIN ?+=:(1)Before O?Boyle?s work, a student (Craig) in anunpublished project used a loop algorithm thatwas equivalent to FIN:=PHR?SUB.
This yieldsnegative frequencies for some resulting n-gramswith overlapping, hence the need for the LOSfile.There are 2 additional files?
To create the PHR file, a SOR file isneeded that contains all the completen-grams regardless of m (the SOR file isthe PHR file in the special case wherem = 1.)
To create the PHR file, words areremoved from the right-hand side of eachSOR phrase in the SOR file until theresultant phrase appears at least m times (ifthe phrase already occurs more than mtimes, no words will be removed.)?
To create the LOS file, O?Boyle applieda POS file: for any SUB phrase, if oneword can be added back on the right-handside (previously removed when the PHRfile was created from the SOR file), thenone POS phrase will exist as the addedphrase.
Thus, if any POS phrase appears atleast m times, its original SUB phrase willbe an overlapping n-gram in the LOS file.The application scope of O?Boyle?s reducedn-gram algorithm is limited to small corpora,such as the Brown corpus (American English) of1 million words (1992), in which the longestphrase has 30 words.
Now their algorithm,re-checked by us, still works for medium sizeand large corpora.
In order to work well for verylarge corpora, it has been implemented by filedistribution and sort processes.Ha, Seymour, Hanna and Smith (2005) haveinvestigated a reduced n-gram model for theChinese TREC corpus of the Linguistic DataConsortium (LDC) (http://www.ldc.upenn.edu/),catalog no.
LDC2000T52.4 Reduced N-Grams and Zipf?s LawBy re-applying O?Boyle and Smith?s algorithm,we obtained reduced n-grams from two Englishlarge corpora and a Chinese large corpus.The two English corpora used in ourexperiments are the full text of articles appearingin the Wall Street Journal (WSJ) (Paul andBaker, 1992) for 1987, 1988, 1989, with sizesapproximately 19 million, 16 million and 6million tokens respectively; and the NorthAmerican News Text (NANT) corpus from theLDC, sizing 500 million tokens, including LosAngeles Times & Washington Post for May1994-August 1997, New York Times NewsSyndicate for July 1994-December 1996, ReutersNews Service (General & Financial) for April1994-December 1996 and Wall Street Journal forJuly 1994-December 1996.
Therefore, the WSJparts from the two English corpora are notoverlapping together.The Mandarin News corpus from the LDC,catalog no.
LDC95T13 was obtained from thePeople?s Daily Newspaper from 1991 to 1996(125 million syllables); from the Xinhua NewsAgency from 1994 to 1996 (25 millionsyllables); and from transcripts of China RadioInternational broadcast from 1994 to 1996 (100million syllables), altogether over 250 millionsyllables.
The number of syllable types (i.e.unigrams) in the Mandarin News corpus is 6,800.Ha, Sicilia-Garcia, Ming and Smith (2003)produced a compound word version of theMandarin News corpus with 50,000 types; thisversion was employed in our study for reducedn-grams.We next present the Zipf curves (Zipf, 1949)for the English and Chinese reduced n-grams.4.1 Wall Street JournalThe WSJ reduced n-grams can be created by theoriginal O?Boyle-Smith algorithm implementedon a Pentium II 586 of 512MByte RAM for over40 hours, the disk storage requirement beingonly 5GBytes.311The conventional 10-highest frequency WSJwords have been published by Ha et al (2002)and the most common WSJ reduced unigrams,bigrams and trigrams are shown in Table 2.
Itillustrates that the most common reduced word isnot THE; even OF is not in the top ten.
Thesewords are now mainly part of longer n-gramswith large n.The Zipf curves are plotted for reducedunigrams and n-grams in Figure 1 showing allthe curves have slopes within [-0.6, -0.5].
TheWSJ reduced bigram, trigram, 4-gram and5-gram curves become almost parallel andstraight, with a small observed noise between thereduced 4-gram and 5-gram curves when they cuteach other at the beginning.
Note thatinformation theory tells us that an idealinformation channel would be made of symbolswith the same probability.
So having a slope of?0.5 is closer than ?1 to this ideal.Unigrams Bigrams Trigrams RankFreq Token Freq Token Freq Token123456789104,2732,4692,4222,1441,9181,6601,2491,1011,007997Mr.butandthesaysorsaidhoweverwhilemeanwhile2,2682,0521,9451,5031,332950856855832754he saidhe saysbut thebut Mr.and thesays Mr.in additionand Mr.last yearfor example1,231709664538524523488484469466terms weren?t disclosedthe company saidas previously reportedhe said thea spokesman forthe spokesman saidas a resultearlier this yearin addition toaccording to Mr.Table 2.
Most common WSJ reduced n-gramslog ranklogfrequency1-gram2-gram3-gram4-gram5-gram012342  510  4 3 6  7slope -1Figure 1.
The WSJ reduced n-gram Zipf curves4.2 North American News Text corpusThe NANT reduced n-grams are created by theimproved algorithm after over 300 hoursprocessing, needing a storage requirement of100GBytes on a Pentium II 586 of 512MByteRAM.Their Zipf curves are plotted for reducedunigrams and n-grams in Figure 2 showing allthe curves are just sloped around [-0.54, -0.5].The reduced unigrams of NANT still show the2-slope behavior when it starts with slope ?0.54and then drop with slope nearly ?2 at the end ofthe curve.
We have found that the traditionaln-grams also show this behaviour, with an initialslope of ?1 changing to ?2 for large ranks (Haand Smith, 2004.
)log ranklogfrequency1-gram2-gram3-gram4-gram5-gramslope -12 510 4 3 6 7 80123456Figure 2.
The NANT reduced n-gram Zipf curves4.3 Mandarin News compound wordsThe Zipf curves are plotted for the smallerChinese TREC reduced unigrams and n-gramswere shown by Ha et al (2005.
)312log ranklogfrequency1-gram2-gram3-gram4-gram5-gramslope -12  510  43 6 70123456Figure 3.
Mandarin reduced n-gram Zipf curvesThe Mandarin News reduced word n-grams werecreated in 120 hours, using 20GB of disk space.The Zipf curves are plotted in Figure 3 showingthat the unigram curve now has a larger slopethan ?1, it is around ?1.2.
All the n-gram curvesare now straighter and more parallel than thetraditional n-gram curves, have slopes within[-0.67, -0.5].Usually, Zipf?s rank-frequency law with aslope ?1 is confirmed by empirical data, but thereduced n-grams for English and Chinese shownin Figure 1, Figure 2 and Figure 3 do not confirmit.
In fact, various more sophisticated models forfrequency distributions have been proposed byBaayen (2001) and Evert (2004.
)5 Perplexity for Reduced N-GramsThe reduced n-gram approach was used to builda statistical language model based on theweighted average model of O?Boyle, Owens andSmith (1994.)
We rewrite this model in formulae(2) and (3)( ) ( )( ) 11 2log +??
?= jiijij wfwwgt(2)( ) ( ) ( ) ( ) ( )( )???=??=????+?
?+?= 1011111 NliliNliliiiliiiiNiiWAwwgtwwPwwgtwPwwgtwwP(3)This averages the probabilities of a word wifollowing the previous one word, two words,three words, etc.
(i.e.
making the last word of ann-gram.)
The averaging uses weights thatincrease slowly with their frequency and rapidlywith the length of n-gram.
This weighted averagemodel is a variable length model that givesresults comparable to the Katz back-off method(1987), but is quicker to use.The probabilities of all of the sentences mw1 ina test text are then calculated by the weightedaverage model( ) ( ) ( ) ( )111211 ... ?= mmWAWAWAm wwPwwPwPwP(4)and an average perplexity of each sentence isevaluated using Equation (5)( ) ( )( )??????
?= ?=?LiiiWAm wwwwPLnLwPP11211 ...1exp(5)Ha et al (2005) already investigated andanalysed the main difficulties arising fromperplexity calculations for our reduced model: astatistical model problem, an unseen wordproblem and an unknown word problem.
Theirsolutions are applied in this paper also.
Similarproblems have been found by other authors, e.g.Martin, Liermann and Ney (1997); Kneser andNey (1995.
)The perplexity calculations for both theEnglish and Chinese reduced n-grams includesstatistics on phrase lengths starting withunigrams, bigrams, trigrams?and on up to thelongest phrase which occur in the reduced model.The perplexities of the WSJ reduced model areshown in Table 3, North American News Textcorpus in Table 4 and Mandarin News words inTable 5.The nature of the reduced model makes thereporting of results for limited sizes of n-gramsto be inappropriate, although these are valid for atraditional n-gram model.
Therefore we showresults for several n-gram sizes for the traditionalmodel, but only one perplexity for the reducedmodel.313Tokens 0 UnknownsTypes 0Unigrams 762.69Bigrams 144.33Trigrams 75.36Traditional Model 4-grams 60.735-grams 56.856-grams 55.667-grams 55.29Reduced Model 70.98%Improvement of ReducedModel on baseline Trigrams5.81%Model size reduction 14.56Table 3.
Reduced perplexities for English WSJTokens 24 UnknownsTypes 23Unigrams 1,442.99Bigrams 399.61Trigrams 240.52Traditional Model 4-grams 202.595-grams 194.066-grams 191.917-grams 191.23Reduced Model 230.46%Improvement of ReducedModel on baseline Trigrams4.18%Model size reduction 11.01Table 4.
Reduced perplexities for English NANTTokens 84 UnknownsTypes 26Unigrams 1,620.56Bigrams 377.43Trigrams 179.07Traditional Model 4-grams 135.695-grams 121.536-grams 114.967-grams 111.69Reduced Model 148.71%Improvement of ReducedModel on baseline Trigrams16.95%Model size reduction 11.28Table 5.
Reduced perplexities for MandarinNews wordsIn all three cases the reduced model produces amodest improvement over the traditional3-gram model, but is not as good as thetraditional 4-gram or higher models.
However inall three cases the result is obtained with asignificant reduction in model size, from a factorof 11 to almost 15 compared to the traditional7-gram model size.We did expect a greater improvement inperplexity than we obtained and we believe that afurther look at the methods used to solve thedifficult problems listed by Ha et al (2005)(mentioned above) and others mentioned by Ha(2005) might lead to an improvement.
Missingword tests are also needed.6 ConclusionsThe conventional n-gram language model islimited in terms of its ability to representextended phrase histories because of theexponential growth in the number of parameters.To overcome this limitation, we havere-investigated the approach of O?Boyle (1993)and created reduced n-gram models.
Our aimwas to try to create an n-gram model that usedsemantically more complete n-grams thantraditional n-grams in the expectation that thismight lead to an improvement in languagemodeling.
The improvement in perplexity ismodest, but there is a large decrease in modelsize.
So this represents an encouraging stepforward, although still very far from the finalstep in language modelling.AcknowledgementsThe authors would like to thank Dr Ji Ming forhis support and the reviewers for their valuablecomments.ReferencesDouglas B. Paul and Janet B. Baker.
1992.
TheDesign for the Wall Street Journal based CSRCorpus.
In Proc.
of the DARPA SLS Workshop,pages 357-361.Francis J. Smith and Peter O?Boyle.
1992.
TheN-Gram Language Model.
The Cognitive Scienceof Natural Language Processing Workshop, pages51-58.
Dublin City University.George K. Zipf.
1949.
Human Behaviour and thePrinciple of Least Effort.
Reading, MA: Addison-Wesley Publishing Co.Harald R. Baayen.
2001.
Word FrequencyDistributions.
Kluwer Academic Publishers.Jianying Hu, William Turin and Michael K. Brown.1997.
Language Modeling using StochasticAutomata with Variable Length Contexts.Computer Speech and Language, volume 11, pages1-16.314Joshua Goodman and Jianfeng Gao.
2000.
LanguageModel Size Reduction By Pruning And Clustering.ICSLP?00.
Beijing, China.Kristie Seymore and Ronald Rosenfeld.
1996.Scalable Backoff Language Models.
ICSLP?96,pages 232-235.Le Q. Ha and Francis.
J. Smith.
2004.
Zipf and Type-Token rules for the English and Irish languages.MIDL workshop.
Paris.Le Q. Ha, Elvira I. Sicilia-Garcia, Ji Ming and FrancisJ.
Smith.
2002.
Extension of Zipf?s Law to Wordsand Phrases.
COLING?02, volume 1, pages 315-320.Le Q. Ha, Elvira I. Sicilia-Garcia, Ji Ming and FrancisJ.
Smith.
2003.
Extension of Zipf?s Law to Wordand Character N-Grams for English and Chinese.CLCLP, 8(1):77-102.Le Q. Ha, Rowan Seymour, Philip Hanna and FrancisJ.
Smith.
2005.
Reduced N-Grams for ChineseEvaluation.
CLCLP, 10(1):19-34.Manhung Siu and Mari Ostendorf.
2000.
Integrating aContext-Dependent Phrase Grammar in theVariable N-Gram framework.
ICASSP?00, volume3, pages 1643-1646.Manhung Siu and Mari Ostendorf.
2000.
VariableN-Grams and Extensions for ConversationalSpeech Language Modelling.
IEEE Transactionson Speech and Audio Processing, 8(1):63-75.Nelson Francis and Henry Kucera.
1964.
Manual ofInformation to Accompany A Standard Corpus ofPresent-Day Edited American English, for use withDigital Computers.
Department of Linguistics,Brown University, Providence, Rhode Island.Peter  L.  O?Boyle.
1993.
A  study  of  an  N-GramLanguage Model for Speech Recognition.
PhDthesis.
Queen?s University Belfast.Peter O?Boyle, John McMahon and Francis J. Smith.1995.
Combining a Multi-Level Class Hierarchywith Weighted-Average Function-BasedSmoothing.
IEEE Automatic Speech RecognitionWorkshop.
Snowbird, Utah.Peter O?Boyle, Marie Owens and Francis J. Smith.1994.
A weighted average N-Gram model ofnatural language.
Computer Speech and Language,volume 8, pages 337-349.Ramon Ferrer I. Cancho and Ricard V. Sol?.
2002.Two Regimes in the Frequency of Words and theOrigin of Complex Lexicons.
Journal ofQuantitative Linguistics, 8(3):165-173.Reinhard Blasig.
1999.
Combination of Words andWord Categories in Varigram Histories.ICASSP?99, volume 1, pages 529-532.Reinhard Kneser and Hermann Ney.
1995.
ImprovedBacking-off for M-Gram Language Modeling.ICASSP?95, volume 1, pages 181-184.
Detroit.Reinhard Kneser.
1996.
Statistical LanguageModeling Using a Variable Context Length.ICSLP?96, volume 1, pages 494-497.Slava M. Katz.
1987.
Estimation of Probabilities fromSparse Data for the Language Model Componentof a Speech Recognizer.
In IEEE Transactions onAcoustics, Speech and Signal Processing, volumeASSP-35, pages 400-401.Stefan Evert.
2004.
A Simple LNRE Model forRandom Character Sequences.
In Proc.
of the7?mes Journ?es Internationales d'AnalyseStatistique des Donn?es Textuelles, pages 411-422.Sven C. Martin, J?rg Liermann and Hermann Ney.1997.
Adaptive Topic-Dependent LanguageModelling Using Word-Based Varigrams.EuroSpeech?97, volume 3, pages 1447-1450.Rhodes.Thomas R. Niesler and Phil C. Woodland.
1996.
AVariable-Length Category-Based N-GramLanguage Model.
ICASSP?96, volume 1, pages164-167.Thomas R. Niesler.
1997.
Category-based statisticallanguage models.
St. John?s College, University ofCambridge.315
