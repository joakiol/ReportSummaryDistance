Proceedings of the 2nd Workshop on EVENTS: Definition, Detection, Coreference, and Representation, pages 45?53,Baltimore, Maryland, USA, June 22-27, 2014.c?2014 Association for Computational LinguisticsA Comparison of the Events and Relations Across ACE, ERE, TAC-KBP,and FrameNet Annotation StandardsJacqueline Aguilar and Charley Beller and Paul McNamee and Benjamin Van DurmeHuman Language Technology Center of ExcellenceJohns Hopkins UniversityBaltimore, MD, USAStephanie Strassel and Zhiyi Song and Joe EllisUniversity of PennsylvaniaLinguistic Data Consortium (LDC)Philadelphia, PA, USAAbstractThe resurgence of effort within computa-tional semantics has led to increased in-terest in various types of relation extrac-tion and semantic parsing.
While var-ious manually annotated resources existfor enabling this work, these materialshave been developed with different stan-dards and goals in mind.
In an effortto develop better general understandingacross these resources, we provide a sum-mary overview of the standards underly-ing ACE, ERE, TAC-KBP Slot-filling, andFrameNet.1 OverviewACE and ERE are comprehensive annotation stan-dards that aim to consistently annotate Entities,Events, and Relations within a variety of doc-uments.
The ACE (Automatic Content Extrac-tion) standard was developed by NIST in 1999 andhas evolved over time to support different evalua-tion cycles, the last evaluation having occurred in2008.
The ERE (Entities, Relations, Events) stan-dard was created under the DARPA DEFT pro-gram as a lighter-weight version of ACE with thegoal of making annotation easier, and more con-sistent across annotators.
ERE attempts to achievethis goal by consolidating some of the annotationtype distinctions that were found to be the mostproblematic in ACE, as well as removing somemore complex annotation features.This paper provides an overview of the relation-ship between these two standards and comparesthem to the more restricted standard of the TAC-KBP slot-filling task and the more expansive stan-dard of FrameNet.
Sections 3 and 4 examine Rela-tions and Events in the ACE/ERE standards, sec-tion 5 looks at TAC-KBP slot-filling, and section6 compares FrameNet to the other standards.2 ACE and ERE Entity TaggingMany of the differences in Relations and Eventsannotation across the ACE and ERE standardsstem from differences in entity mention tagging.This is simply because Relation and Event taggingrelies on the distinctions established in the entitytagging portion of the annotation process.
For ex-ample, since ERE collapses the ACE Facility andLocation Types, any ACE Relation or Event thatrelied on that distinction is revised in ERE.
Thesetop-level differences are worth keeping in mindwhen considering how Events and Relations tag-ging is approached in ACE and ERE:?
Type Inventory: ACE and ERE share the Per-son, Organization, Geo-Political Entity, andLocation Types.
ACE has two additionalTypes: Vehicle and Weapon.
ERE does notaccount for these Types and collapses the Fa-cility and Location Types into Location.
EREalso includes a Title Type to address titles,honorifics, roles, and professions (Linguis-tic Data Consortium, 2006; Linguistic DataConsortium, 2013a).?
Subtype Annotation: ACE further classifiesentity mentions by including Subtypes foreach determined Type; if the entity does notfit into any Subtype, it is not annotated.
EREannotation does not include any Subtypes.?
Entity Classes: In addition to Subtype, ACEalso classifies each entity mention according451996 1998 2000 2002 2004 2006 2008 2010 2012FrameNetprojectcreatedACEdevelopedmostcomprehensiveACEcorpuslastACEevalfirstTAC-KBPEREcreatedFigure 1: Important Dates for the ACE, ERE, TAC-KBP, and FrameNet Standardsto entity class (Specific, Generic, Attributive,and Underspecified).?
Taggability: ACE tags Attributive, Generic,Specific, and Underspecified entity mentions.ERE only tags Specific entity mentions.?
Extents and Heads: ACE marks the full nounphrase of an entity mention and tags a headword.
ERE handles tagging based on themention level of an entity; in Name mentions(NAM) the name is the extent, in Nominalmentions (NOM) the full noun phrase is theextent, in Pronoun mentions (PRO) the pro-noun is the extent.?
Tags: ERE only specifies Type and Men-tion level (NAM, NOM, PRO).
ACE speci-fies Type, Subtype, Entity Class (Attributive,Generic, Specific, Underspecified), and Men-tion Level (NAM, NOM, PRO, Headless).3 Relations in ACE and EREIn the ACE and ERE annotation models, the goalof the Relations task is to detect and character-ize relations of the targeted types between enti-ties (Linguistic Data Consortium, 2008; LinguisticData Consortium, 2013c).
The purpose of this taskis to extract a representation of the meaning of thetext, not necessarily tied to underlying syntacticor lexical semantic representations.
Both modelsshare similar overarching guidelines for determin-ing what is taggable.
For relations the differenceslie in the absence or presence of additional fea-tures, syntactic classes, as well as differences inassertion, trigger words, and minor subtype varia-tions.3.1 Similarities in Relations AnnotationIn addition to comprising similar Types (bothmodels include Physical and Part.Whole Types aswell as slightly different Types to address Affilia-tion and Social relations) used to characterize eachrelation, ACE and ERE share important similar-ities concerning their relation-tagging guidelines.These include:?
Limiting relations to only those expressed ina single sentence?
Tagging only for explicit mention?
No ?promoting?
or ?nesting?
of taggable en-tities.
In the sentence, Smith went to a hotelin Brazil, (Smith, hotel) is a taggable Phys-ical.Located relation, but (Smith, Brazil) isnot.
This is because in order to tag this assuch, one would have to promote ?Brazil?.?
Tagging for past and former relations?
Two different Argument slots (Arg1 andArg2) are provided for each relation to cap-ture the importance of Argument ordering.?
Arguments can be more than one token (al-though ACE marks the head as well)?
Using ?templates?
for each relationType/Subtype (e.g., in a Physical.Locatedrelation, the Person that is located some-where will always be assigned to Arg1 andthe place in which the person is located willalways be assigned to Arg2).?
Neither model tags for negative relations?
Both methods contain argument span bound-aries.
That is, the relations should only in-clude tagged entities within the extent of asentence.3.2 Differences in Assertion, Modality, andTenseA primary difference between these two annota-tion models is a result of ERE only annotating as-serted events while ACE also includes hypothet-icals.
ACE accounts for these cases by includingtwo Modality attributes: ASSERTED and OTHER46(Linguistic Data Consortium, 2008).
For exam-ple, in the sentence, We are afraid that Al-Qaedaterrorists will be in Baghdad, ACE would tag thisas an OTHER attribute, where OTHER pertains tosituations in ?some other world defined by coun-terfactual constraints elsewhere in the context?,whereas ERE would simply not tag a relation inthis sentence.
Additionally, while both ACE andERE tag past and former relations, ACE goes fur-ther to mark the Tense of each relation by meansof four attributes: Past, Future, Present and Un-specified.3.3 Syntactic ClassesACE further justifies the tagging of each Relationthrough Syntactic Classes.
The primary functionof these classes is to serve as a sanity check ontaggability and as an additional constraint for tag-ging.
These classes include: Possessive, Prepo-sition, PreMod, Coordination, Formulaic, Partic-ipal, Verbal, Relations Expressed by Verbs, andOther.
Syntactic classes are not present in ERErelations annotation.3.4 TriggersExplicit trigger words do not exist in ACE relationannotation; instead, the model annotates the fullsyntactic clause that serves as the ?trigger?
for therelation.
ERE attempts to minimize the annotatedspan by allowing for the tagging of an optionaltrigger word, defined as ?the smallest extent of textthat indicates a relation Type and Subtype?
(Lin-guistic Data Consortium, 2013c).
These triggersare not limited to a single word, but can also becomposed of a phrase or any extent of the text thatindicates a Type/Subtype relation, left to the dis-cretion of the annotator.
It is common for preposi-tions to be triggers, as in John is in Chicago.
How-ever, sometimes no trigger is needed because thesyntax of the sentence is such that it indicates aparticular relation Type/Subtype without a word toexplicitly signal the relation.3.5 Types and Subtypes of RelationsThere are three types of relations that contain var-ied Subtypes between ERE and ACE.
These arethe Physical, Part-Whole, Social and AffiliationTypes.
The differences are a result of ERE collaps-ing ACE Types and Subtypes into more concise, ifless specific, Type groups.Physical Relation Type Differences The maindifferences in the handling of the physical rela-tions between ACE and ERE are shown in Table1.
ACE only marks Location for PERSON enti-ties (for Arg1).
ERE uses Location for PERSONentities being located somewhere as well as fora geographical location being part of another ge-ographical location.
Additionally, ACE includes?Near?
as a Subtype.
This is used for when an en-tity is explicitly near another entity, but neither en-tity is a part of the other or located in/at the other.ERE does not have an equivalent Subtype to ac-count for this physical relation.
Instead, ERE in-cludes ?Origin?
as a Subtype.
This is used to de-scribe the relation between a PER and an ORG.ACE does not have a Physical Type equivalent,but it does account for this type of relation withina separate General Affiliation Type and ?Citizen-Resident-Religion-Ethnicity?
Subtype.Part-Whole Relation Differences In Table 2,note that ACE has a ?Geographical?
Subtypewhich captures the location of a FAC, LOC, orGPE in or at, or as part of another FAC, LOC,or GPE.
Examples of this would be India con-trolled the region or a phrase such as the Atlantaarea.
ERE does not include this type of annota-tion option.
Instead, ERE tags these regional re-lations as Physical.Located.
ACE and ERE doshare a ?Subsidiary?
Subtype which is defined inboth models as a ?category to capture the own-ership, administrative and other hierarchical rela-tionships between ORGs and/or GPEs?
(Linguis-tic Data Consortium, 2008; Linguistic Data Con-sortium, 2013c).Social and Affiliation Relation DifferencesThe most evident discrepancy in relation anno-tation between the two models lies in the So-cial and Affiliation Relation Types and Subtypes.For social relations, ACE and ERE have threeSubtypes with similar goals (Business, Family,Unspecified/Lasting-Personal) but ERE has an ad-ditional ?Membership?
Subtype, as shown in Ta-ble 3.
ACE addresses all ?Membership?
relationsin its Affiliation Type.
ERE also includes the ?So-cial.Role?
Subtype in order to address the TITLEentity type, which only applies to ERE.
How-ever, both models agree that the arguments foreach relation must be PERSON entities and thatthey should not include relationships implied frominteraction between two entities (e.g., President47Relation Type Relation Subtype ARG1 Type ARG2 TypeEREPhysical Located PER, GPE, LOC GPE, LOCPhysical Origin PER, ORG GPE, LOCACEPhysical Located PER FAC, LOC, GPEPhysical Near PER, FAC, GPE, LOC FAC, GPE, LOCTable 1: Comparison of Permitted Relation Arguments for the Physical Type Distinction in the ERE andACE GuidelinesRelation Type Relation Subtype ARG1 Type ARG2 TypeEREPart-Whole Subsidiary ORG ORG, GPEACEPart-Whole Geographical FAC, LOC, GPE FAC, LOC, GPEPart-Whole Subsidiary ORG ORG, GPETable 2: Comparison of Permitted Relation Arguments for the Part-Whole Type and Subtype Distinctionsin the ERE and ACE GuidelinesRelation Type Relation Subtype ARG1 Type ARG2 TypeERESocial Business PER PERSocial Family PER PERSocial Membership PER PERSocial Role TTL PERSocial Unspecified PER PERACEPersonal-Social Business PER PERPersonal-Social Family PER PERPersonal-Social Lasting-Personal PER PERTable 3: Comparison of Permitted Relation Arguments for the Social Type and Subtype Distinctions inthe ERE and ACE GuidelinesRelation Type Relation Subtype ARG1 Type ARG2 TypeEREAffiliation Employment/Membership PER, ORG,GPEORG, GPEAffiliation Leadership PER ORG, GPEACEORG-Affiliation Employment PER ORG, GPEORG-Affiliation Ownership PER ORGORG-Affiliation Founder PER, ORG ORG, GPEORG-Affiliation Student-Alum PER ORG.EducationalORG-Affiliation Sports-Affiliation PER ORGORG-Affiliation Investor-Shareholder PER, ORG,GPEORG, GPEORG-Affiliation Membership PER, ORG,GPEORGAgent-Artifact User-Owner-Inventor-ManufacturerPER, ORG,GPEFACGen-Affiliation Citizen-Resident-Religion-EthnicityPER PER.Group,LOC, GPE,ORGGen-Affiliation Org-Location-Origin ORG LOC, GPETable 4: Comparison of Permitted Relation Arguments for the Affiliation Type and Subtype Distinctionsin the ERE and ACE Guidelines48Clinton met with Yasser Arafat last week wouldnot be considered a social relation).As for the differences in affiliation relations,ACE includes many Subtype possibilities whichcan more accurately represent affiliation, whereasERE only observes two Affiliation Subtype op-tions (Table 4).4 Events in ACE and EREEvents in both annotation methods are defined as?specific occurrences?, involving ?specific partic-ipants?
(Linguistic Data Consortium, 2005; Lin-guistic Data Consortium, 2013b).
The primarygoal of Event tagging is to detect and character-ize events that include tagged entities.
The centralEvent tagging difference between ACE and EREis the level of specificity present in ACE, whereasERE tends to collapse tags for a more simplifiedapproach.4.1 Event Tagging SimilaritiesBoth annotation schemas annotate the same ex-act Event Types: LIFE, MOVEMENT, TRANS-ACTION, BUSINESS, CONFLICT, CONTACT,PERSONNEL, and JUSTICE events.
Both anno-tation ontologies also include 33 Subtypes for eachType.
Furthermore, both rely on the expressionof an occurrence through the use of a ?Trigger?.ACE, however, restricts the trigger to be a singleword that most clearly expresses the event occur-rence (usually a main verb), while ERE allows forthe trigger to be a word or a phrase that instanti-ates the event (Linguistic Data Consortium, 2005;Linguistic Data Consortium, 2013b).
Both meth-ods annotate modifiers when they trigger eventsas well as anaphors, when they refer to previouslymentioned events.
Furthermore, when there isany ambiguity about which trigger to select, bothmethods have similar rules established, such asthe Stand-Alone Noun Rule (In cases where morethan one trigger is possible, the noun that can beused by itself to refer to the event will be selected)and the Stand-Alone Adjective Rule (Whenever averb and an adjective are used together to expressthe occurrence of an Event, the adjective will bechosen as the trigger whenever it can stand-aloneto express the resulting state brought about by theEvent).
Additionally, both annotation guidelinesagree on the following:?
Tagging of Resultative Events (states that re-sult from taggable Events)?
Nominalized Events are tagged as regularevents?
Reported Events are not tagged?
Implicit events are not tagged?
Light verbs are not tagged?
Coreferential Events are tagged?
Tagging of multi-part triggers (both parts aretagged only if they are contiguous)4.2 Event Tagging DifferencesOne of the more general differences between EREand ACE Event tagging is the way in which eachmodel addresses Event Extent.
ACE defines theextent as always being the ?entire sentence withinwhich the Event is described?
(Linguistic DataConsortium, 2005).
In ERE, the extent is theentire document unless an event is coreferenced(in which case, the extent is defined as the ?spanof a document from the first trigger for a par-ticular event to the next trigger for a particularevent.?
This signifies that the span can crosssentence boundaries).
Unlike ACE, ERE doesnot delve into indicating Polarity, Tense, Gener-icity, and Modality.
ERE simplifies any anno-tator confusion engendered by these features bysimply not tagging negative, future, hypotheti-cal, conditional, uncertain or generic events (al-though it does tag for past events).
While EREonly tags attested Events, ACE allows for irrealisevents, and includes attributes for marking themas such: Believed Events; Hypothetical Events;Commanded and Requested Events; Threatened,Proposed and Discussed Events; Desired Events;Promised Events; and Otherwise Unclear Con-structions.
Additionally both ERE and ACE tagEvent arguments as long as the arguments occurwithin the event mention extent (another way ofsaying that a taggable Event argument will occurin the same sentence as the trigger word for itsEvent).
However, ERE and ACE have a divergingapproach to argument tagging:?
ERE is limited to pre-specified arguments foreach event and relation subtype.
The pos-sible arguments for ACE are: Event partici-pants (limited to pre-specified roles for eachevent type); Event-specific attributes that areassociated with a particular event type (e.g.,the victim of an attack); and General eventattributes that can apply to most or all eventtypes (e.g., time, place).49?
ACE tags arguments regardless of modal cer-tainty of their involvement in the event.
EREonly tags asserted participants in the event.?
The full noun phrase is marked in both EREand ACE arguments, but the head is onlyspecified in ACE.
This is because ACE han-dles entity annotation slightly differently thanERE does; ACE marks the full noun phrasewith a head word for entity mention, and EREtreats mentions differently based on their syn-tactic features (for named or pronominal en-tity mentions the name or pronominal itselfis marked, whereas for nominal mentions thefull noun phrase is marked).Event Type and Subtype Differences Both an-notation methods have almost identical EventType and Subtype categories.
The only differencesbetween both are present in the Contact and Move-ment Event Types.A minor distinction in Subtype exists as a re-sult of the types of entities that can be trans-ported within the Movement Type category.
InACE, ARTIFACT entities (WEAPON or VEHI-CLE) as well as PERSON entities can be trans-ported, whereas in ERE, only PERSON entitiescan be transported.
The difference between thePhone-Write and Communicate Subtypes merelylies in the definition.
Both Subtypes are the de-fault Subtype to cover all Contact events wherea ?face-to-face?
meeting between sender and re-ceiver is not explicitly stated.
In ACE, this contactis limited to written or telephone communicationwhere at least two parties are specified to makethis event subtype less open-ended.
In ERE, thisrequirement is simply widened to comprise elec-tronic communication as well, explicitly includingthose via internet channels (e.g., Skype).5 TAC-KBPAfter the final ACE evaluation in 2008 there wasinterest in the community to form an evaluationexplicitly focused on knowledge bases (KBs) cre-ated from the output of extraction systems.
NISThad recently started the Text Analysis Conferenceseries for related NLP tasks such as RecognizingTextual Entailment, Summarization, and QuestionAnswering.
In 2009 the first Knowledge BasePopulation track (TAC-KBP) was held featuringtwo initial tasks: (a) Entity Linking ?
linking en-tities to KB entities, and (b) Slot Filling ?
addinginformation to entity profiles that is missing fromthe KB (McNamee et al., 2010).
Due to its gener-ous license and large scale, a snapshot of EnglishWikipedia from late 2008 has been used as the ref-erence KB in the TAC-KBP evaluations.5.1 Slot Filling OverviewUnlike ACE and ERE, Slot Filling does not haveas its primary goal the annotation of text.
Rather,the aim is to identify knowledge nuggets about afocal named entity using a fixed inventory of re-lations and attributes.
For example, given a fo-cal entity such as former Ukrainian prime ministerYulia Tymoshenko, the task is to identify attributessuch as schools she attended, occupations, and im-mediate family members.
This is the same sortof information commonly listed about prominentpeople in Wikipedia Infoboxes and in derivativedatabases such as FreeBase and DBpedia.Consequently, Slot Filling is somewhat of a hy-brid between relation extraction and question an-swering ?
slot fills can be considered as the cor-rect responses to a fixed set of questions.
The rela-tions and attributes used in the 2013 task are pre-sented in Table 5.5.2 Differences with ACE-style relationextractionSlot Filling in TAC-KBP differs from extraction inACE and ERE in several significant ways:?
information is sought for named entities,chiefly PERs and ORGs;?
the focus is on values not mentions;?
assessment is more like QA; and,?
events are handled as uncorrelated slotsIn traditional IE evaluation, there was animplicit skew towards highly attested in-formation such as leader(Bush, US), orcapital(Paris, France).
In contrast, TAC-KBPgives full credit for finding a single instance of acorrect fill instead of every attestation of that fact.Slot Filling assessment is somewhat simplerthan IE annotation.
The assessor must decideif provenance text is supportive of a posited factabout the focal entity instead of annotating a doc-ument with all evidenced relations and events forany entity.
For clarity and to increase assessoragreement, guidelines have been developed to jus-tify when a posited relation is deemed adequatelysupported from text.
Additionally, the problem of50Relations Attributesper:children org:shareholders per:alternate names org:alternate namesper:other family org:founded by per:date of birth org:political religious affiliationper:parents org:top members employees per:age org:number of employees membersper:siblings org:member of per:origin org:date foundedper:spouse org:members per:date of death org:date dissolvedper:employee or member of org:parents per:cause of death org:websiteper:schools attended org:subsidiaries per:titleper:city of birth org:city of headquarters per:religionper:stateorprovince of birth org:stateorprovince of headquarters per:chargesper:country of birth org:country of headquartersper:cities of residenceper:statesorprovinces of residenceper:countries of residenceper:city of deathper:stateorprovince of deathper:country of deathTable 5: Relation and attributes for PERs and ORGs.slot value equivalence becomes an issue - a sys-tem should be penalized for redundantly assertingthat a person has four children named Tim, Beth,Timothy, and Elizabeth, or that a person is both acardiologist and a doctor.Rather than explicitly modeling events, TAC-KBP created relations that capture events, morein line with the notion of Infobox filling or ques-tion answering (McNamee et al., 2010).
For exam-ple, instead of a criminal event, there is a slot fillfor charges brought against an entity.
Instead of afounding event, there are slots like org:founded by(who) and org:date founded (when).
Thus a state-ment that ?Jobs is the founder and CEO of Apple?is every bit as useful for the org:founded by rela-tion as ?Jobs founded Apple in 1976.?
even thoughthe date is not included in the former sentence.5.3 Additional tasksStarting in 2012 TAC-KBP introduced the ?ColdStart?
task, which is to literally produce a KBbased on the Slot Filling schema.
To date, ColdStart KBs have been built from collections ofO(50,000) documents, and due to their large size,they are assessed by sampling.
There is alsoan event argument detection evaluation in KBPplanned for 2014.Other TAC-KBP tasks have been introduced in-cluding determining the timeline when dynamicslot fills are valid (e.g., CEO of Microsoft), andtargeted sentiment.6 FrameNetThe FrameNet project has rather different moti-vations than either ACE/ERE or TAC-KBP, butshares with them a goal of capturing informa-tion about events and relations in text.
FrameNetstems from Charles Fillmore?s linguistic and lex-icographic theory of Frame Semantics (Fillmore,1976; Fillmore, 1982).
Frames are descriptionsof event (or state) types and contain informationabout event participants (frame elements), infor-mation as to how event types relate to each other(frame relations), and information about whichwords or multi-word expressions can trigger agiven frame (lexical units).FrameNet is designed with text annotation inmind, but unlike ACE/ERE it prioritizes lexico-graphic and linguistic completeness over ease ofannotation.
As a result Frames tend to be muchfiner grained than ACE/ERE events, and are morenumerous by an order of magnitude.
The BerkeleyFrameNet Project (Baker et al., 1998) was devel-oped as a machine readable database of distinctframes and lexical units (words and multi-wordconstructions) that were known to trigger specificframes.1FrameNet 1.5 includes 1020 identifiedframes and 11830 lexical units.One of the most widespread uses of FrameNethas been as a resource for Semantic Role Label-ing (SRL) (Gildea and Jurafsky, 2002).
FrameNetrelated SRL was promoted as a task by theSENSEVAL-3 workshop (Litkowski, 2004), andthe SemEval-2007 workshop (Baker et al., 2007).
(Das et al., 2010) is a current system for automaticFrameNet annotation.The relation and attribute types of TAC-KBPand the relation and event types in the ACE/EREstandards can be mapped to FrameNet frames.The mapping is complicated by two factors.The first is that FrameNet frames are gener-ally more fine-grained than the ACE/ERE cate-gories.
As a result the mapping is sometimesone-to-many.
For example, the ERE relation Af-1This database is accessible via webpage (https://framenet.icsi.berkeley.edu/fndrupal/)and as a collection of XML files by request.51RelationsFrameNet ACE ERE TAC-KBPKinship Personal-Social.Family Social.Family per:childrenper:other familyper:parentsper:siblingsper:spouseBeing Employed ORG-Affiliation.Employment Affiliation.Employment/Membership per:employee or member ofMembership org:member ofBeing Located Physical.Located Physical.Located org:city of headquartersorg:stateorprovince of headquartersorg:country of headquartersEventsFrameNet ACE EREContacting Phone-Write CommunicateExtradition Justice-Extradition Justice-ExtraditionAttack Conflict-Attack Conflict-AttackBeing Born Life-Be Born Life-Be BornAttributesFrameNet TAC-KBPBeing Named per:alternate namesAge per:ageTable 6: Rough mappings between subsets of FrameNet, ACE, ERE, and TAC-KBPfiliation.Employment/Membership covers boththe Being Employed frame and the Member-ship frame.
At the same time, while TAC-KBP has only a handful of relations relative toFrameNet, some of these relations are more fine-grained than the analogous frames or ACE/ERErelations.
For example, the frame Kinship, whichmaps to the single ERE relation Social.Family,maps to five TAC-KBP relations, and the Be-ing Located, which maps to the ACE/ERE rela-tion Being.Located, maps to three TAC-KBP re-lations.
Rough mappings from a selection of rela-tions, events, and attributes are given in Table 6.The second complication arises from the factthat FrameNet frames are more complex objectsthan ERE/ACE events, and considerably morecomplex than TAC-KBP relations.
Rather than thetwo entities related via a TAC-KBP or ACE/ERErelation, some frames have upwards of 20 frameelements.
Table 7 shows in detail the mapping be-tween frame elements in the Extradition frame andACE?s and ERE?s Justice-Extradition events.
The?core?
frame elements map exactly to the EREevent, the remaining two arguments in the ACEevent map to two non-core frame elements, andthe frame includes several more non-core elementswith no analogue in either ACE or ERE standards.7 ConclusionThe ACE and ERE annotation schemas haveclosely related goals of identifying similar in-formation across various possible types of docu-ments, though their approaches differ due to sepa-rate goals regarding scope and replicability.
EREdiffers from ACE in collapsing different Type dis-tinctions and in removing annotation features inorder to eliminate annotator confusion and to im-FrameNet ACE EREAuthorities Agent-Arg Agent-ArgCrime jursidiction Destination-Arg Destination-ArgCurrent jursidiction Origin-Arg Origin-ArgSuspect Person-Arg Person-ArgReason Crime-ArgTime Time-ArgLegal BasisMannerMeansPlacePurposeDepictiveTable 7: Mapping between frame elements of Ex-tradition (FrameNet), and arguments of Justice-Extradition (ACE/ERE): A line divides core frameelements (above) from non-core (below).prove consistency, efficiency, and higher inter-annotator agreement.
TAC-KPB slot-filling sharessome goals with ACE/ERE, but is wholly fo-cused on a set collection of questions (slots tobe filled) concerning entities to the extent thatthere is no explicit modeling of events.
At theother extreme, FrameNet seeks to capture thefull range of linguistic and lexicographic varia-tion in event representations in text.
In general, allevents, relations, and attributes that can be repre-sented by ACE/ERE and TAC-KBP standards canbe mapped to FrameNet representations, thoughadjustments need to be made for granularity ofevent/relation types and granularity of arguments.AcknowledgementsThis material is partially based on research spon-sored by the NSF under grant IIS-1249516 andDARPA under agreement number FA8750-13-2-0017 (the DEFT program).52ReferencesCollin F Baker, Charles J Fillmore, and John B Lowe.1998.
The berkeley framenet project.
In Proceed-ings of the 17th international conference on Compu-tational linguistics-Volume 1, pages 86?90.
Associ-ation for Computational Linguistics.Collin Baker, Michael Ellsworth, and Katrin Erk.2007.
Semeval-2007 task 19: Frame semanticstructure extraction.
In Proceedings of the FourthInternational Workshop on Semantic Evaluations(SemEval-2007), pages 99?104, Prague, Czech Re-public, June.
Association for Computational Lin-guistics.Dipanjan Das, Nathan Schneider, Desai Chen, andNoah A Smith.
2010.
Probabilistic frame-semanticparsing.
In Proceedings of NAACL-HLT, pages 948?956.
Association for Computational Linguistics.George Doddington, Alexis Mitchell, Mark Przybocki,Lancec Ramshaw, Stephanie Strassel, and RalphWeischedel.
2004.
The automatic content extrac-tion (ace) program- tasks, data, and evaluation.
InProceedings of LREC 2004: Fourth InternationalConference on Language Resources and Evaluation,Lisbon, May 24-30.Charles J Fillmore.
1976.
Frame semantics and the na-ture of language.
Annals of the New York Academyof Sciences, 280(1):20?32.Charles Fillmore.
1982.
Frame semantics.
In Linguis-tics in the morning calm, pages 111?137.
HanshinPublishing Co.Daniel Gildea and Daniel Jurafsky.
2002.
Automaticlabeling of semantic roles.
Computational linguis-tics, 28(3):245?288.Linguistic Data Consortium.
2005.
ACE (automaticcontent extraction) English annotation guidelinesfor events.
https://www.ldc.upenn.edu/collaborations/past-projects/ace.Version 5.4.3 2005.07.01.Linguistic Data Consortium.
2006.
ACE (automaticcontent extraction) English annotation guidelinesfor entities.
https://www.ldc.upenn.edu/collaborations/past-projects/ace,Version 5.6.6 2006.08.01.Linguistic Data Consortium.
2008.
ACE (automaticcontent extraction) English annotation guidelines forrelations.
https://www.ldc.upenn.edu/collaborations/past-projects/ace.Version 6.0 2008.01.07.Linguistic Data Consortium.
2013a.
DEFT ERE anno-tation guidelines: Entities v1.1, 05.17.2013.Linguistic Data Consortium.
2013b.
DEFT ERE anno-tation guidelines: Events v1.1.
05.17.2013.Linguistic Data Consortium.
2013c.
DEFT ERE anno-tation guidelines: Relations v1.1.
05.17.2013.Ken Litkowski.
2004.
Senseval-3 task: Automaticlabeling of semantic roles.
In Rada Mihalcea andPhil Edmonds, editors, Senseval-3: Third Interna-tional Workshop on the Evaluation of Systems for theSemantic Analysis of Text, pages 9?12, Barcelona,Spain, July.
Association for Computational Linguis-tics.Paul McNamee, Hoa Trang Dang, Heather Simpson,Patrick Schone, and Stephanie Strassel.
2010.
Anevaluation of technologies for knowledge base pop-ulation.
In Proceedings of LREC.53
