Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, ACL 2010, pages 27?32,Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational LinguisticsRelatedness Curves for Acquiring ParaphrasesGeorgiana DinuSaarland UniversitySaarbruecken, Germanydinu@coli.uni-sb.deGrzegorz Chrupa?aSaarland UniversitySaarbruecken, Germanygchrupala@lsv.uni-saarland.deAbstractIn this paper we investigate methodsfor computing similarity of two phrasesbased on their relatedness scores acrossall ranks k in a SVD approximation ofa phrase/term co-occurrence matrix.
Weconfirm the major observations made inprevious work and our preliminary experi-ments indicate that these methods can leadto reliable similarity scores which in turncan be used for the task of paraphrasing.1 IntroductionDistributional methods for word similarity uselarge amounts of text to acquire similarity judg-ments based solely on co-occurrence statistics.Typically each word is assigned a representationas a point in a high dimensional space, where thedimensions represent contextual features; follow-ing this, vector similarity measures are used tojudge the meaning relatedness of words.
One wayto make these computations more reliable is to useSingular Value Decomposition (SVD) in order toobtain a lower rank approximation of an originalco-occurrence matrix.SVD is a matrix factorization method whichhas applications in a large number of fields suchas signal processing or statistics.
In natural lan-guage processing methods such as Latent Seman-tic Analysis (LSA) (Deerwester et al, 1990)use SVD to obtain a factorization of a (typically)word/document co-occurrence matrix.
The under-lying idea in these models is that the dimension-ality reduction will produce meaningful dimen-sions which represent concepts rather than justterms, rendering similarity measures on these vec-tors more accurate.
Over the years, it has beenshown that these methods can closely match hu-man similarity judgments and that they can beused in various applications such as informationretrieval, document classification, essay gradingetc.
However it has been noted that the successof these methods is drastically determined by thechoice of dimension k to which the original spaceis reduced.
(Bast and Majumdar, 2005) investigates exactlythis aspect and proves that no fixed choice of di-mension is appropriate.
The authors show that twoterms can be reliably compared only by investigat-ing the curve of their relatedness scores over alldimensions k. The authors use a term/documentmatrix and analyze relatedness curves for inducinga hard related/not-related decision and show thattheir algorithms significantly improve over previ-ous methods for information retrieval.In this paper we investigate: 1) how the findingsof (Bast and Majumdar, 2005) carry over to ac-quiring paraphrases using SVD on a phrase/termco-occurrence matrix and 2) if reliable similarityscores can be obtained from the analysis of relat-edness curves.2 Background2.1 Singular Value DecompositionModels such as LSA use Singular Value Decom-position, in order to obtain term representationsover a space of concepts.Given a co-occurrence matrix X of size (t, d),we can compute the singular value decomposition:U?V T of rank r. Matrices U and V T of sizes(t, r) and (r, d) are the left and right singular vec-tors; ?
is the (r, r) diagonal matrix of singularvalues (ordered in descending order)1.
Similaritybetween terms i and j is computed as the scalarproduct between the two vectors associated to thewords in the U matrix:sim(ui, uj) = ?kl=1uilujl1Any approximation of rank k < r can simply be ob-tained from an approximation or rank r by deleting rows andcolumns.272.2 Relatedness curvesFinding the optimal dimensionality k has provento be an extremely important and not trivial step.
(Bast and Majumdar, 2005) show that no single cutdimension is appropriate to compute the similarityof two terms but this should be deduced from thecurve of similarity scores over all dimensions k.The curve of relatedness for two terms ui and uj isgiven by their scalar product across all dimensionsk, k smaller than a rank r:k ?
?kl=1uilujl, for k = 1, ..., rThey show that a smooth curve indicates closelyrelated terms, while a curve exhibiting many direc-tion changes indicates unrelated terms; the actualvalues of the similarity scores are often mislead-ing, which explains why a good cut dimension kis so difficult to find.2.3 Vector space representation of phrasesWe choose to apply this to acquiring paraphrases(or inference rules, i.e.
entailments which hold injust one direction) in the sense of DIRT (Lin andPantel, 2001).In the DIRT algorithm a phrase is a noun-ending path in a dependency graph and the goalis to acquire inference rules such as (X solve Y,X find solution to Y).
We will call dependencypaths patterns.
The input data consists of largeamounts of parsed text, from which patterns to-gether with X-filler and Y-filler frequency countsare extracted.In this setting, a pattern receives two vector rep-resentation, one in a X-filler space and one in theY-filler space.
In order to compute the similaritybetween two patterns, these are compared in theX space and in the Y space, and the two result-ing scores are multiplied.
(The DIRT algorithmuses Lin measure for computing similarity, whichis given in Section 4).
Obtaining these vectorsfrom the frequency counts is straightforward andit is exemplified in Table 1 which shows a frag-ment of a Y-filler DIRT-like vector space... case problem ..(X solve Y, Y) .. 6.1 4.4 ..(X settle Y, Y) .. 5.2 5.9 ..Table 1: DIRT-like vector representation in the Y-fillerspace.
The values represent mutual information.3 Relatedness curves for acquiringparaphrases3.1 SetupWe parsed the XIE fragment of GigaWord (ap-prox.
100 mil.
tokens) with Stanford dependencyparser.
From this we built a pattern/word matrix ofsize (85000, 3000) containing co-occurrence dataof the most frequent patterns with the most fre-quent words2.
We perform SVD factorization onthis matrix of rank k = 800.
For each pair of pat-terns, we can associate two relatedness curves: aX curve and Y curve given by the scalar productsof their vectors in the U matrix, across dimensionsk : 1, ..., 800.3.2 Evaluating smoothness of the relatednesscurvesIn Figure 1 we plotted the X and Y curves of com-paring the pattern X subj????
win dobj????
Y with itself.Figure 1: X-filler and Y-filler relatedness curvesfor the identity pair (X subj????
win dobj????
Y,X subj????windobj????
Y )Figure 2: X-filler and Y-filler relatedness curvesfor (X subj????
leader prp???
of pobj????
Y,X pobj????
by prp???leadsubj????
Y )Normally, the X and Y curves for the identicalpair are monotonically increasing.
However whatcan be noticed is that the actual values of thesefunctions differ by one order of magnitude in theX and in the Y curves of identical patterns, show-ing that in themselves they are not a good indica-2Even if conceptually we have two semantic spaces (givenby X-fillers and Y-fillers), in reality we can work with a sin-gle matrix, containing for each pattern also its reverse, bothrepresented solely in a X-filler space28Figure 3: X-filler and Y-filler relatedness curvesfor (X subj????
win dobj????
Y,X subj????
murder dobj????
Y )tor of similarity.
In Figure 2 we investigate a pairof closely related patterns: (X subj????
leader prp???ofpobj????
Y,Xpobj????
byprp???
leadsubj????
Y ).
It can benoticed that while still not comparable to those ofthe identical pair, these curves are much smootherthan the ones associated to the pair of unrelatedpatterns in Figure 33.However, unlike in the information retrievalscenario in (Bast and Majumdar, 2005), for whicha hard related/not-related assignment works best,for acquiring paraphrases we need to quantify thesmoothness of the curves.
We describe two func-tions for evaluating curve smoothness which wewill use to compute scores in X-filler and Y-fillersemantic spaces.Smooth function 1 This function simply com-putes the number of changes in the direction of thecurve, as the percentage of times the scalar prod-uct increases or remains equal from step l to stepl + 1:CurveS1(ui, uj) =?uilujl?01k, l = 1, ..., kAn increasing curve will be assigned the maximalvalue 1, while for a curve that is monotonicallydecreasing the score will be 0.Smooth function 2 (Bast and Majumdar, 2005)The second smooth function is given by:CurveS2(ui, uj) =max?min?kl=1abs(uilujl)where max and min are the largest and smallestvalues in the curves.
A curve which is always in-creasing or always decreasing will get a score of 1.Unlike the previous method this function is sensi-tive to the absolute values in the drops of a curve.3The drop out dimension discussed in (Bast and Majum-dar, 2005) Section 3, does not seem to exist for our data.
Thisis to be expected since this result stems from a definition ofperfectly related terms which is adapted to the particularitiesof term/document matrices, and not of term/term matrices.A curve with large drops, irrelevant of their cardi-nality, will be penalized by being assigned a lowscore.4 Experimental resultsIn order to compute the similarity score betweentwo phrases, we follow (Lin and Pantel, 2001)and compute two similarity scores, correspondingto the X-fillers and Y-fillers, and multiply them.Given a similarity function, any pattern encoun-tered in the corpus can be paraphrased by return-ing its most similar patterns.We implement five similarity functions on thedata we have described in the previous section.The first one is the DIRT algorithm and it is theonly method using the original co-occurrence ma-trix in which raw counts are replaced by point-wise mutual information scores.DIRT method The similarity function for twovectors pi and pj is:simLin(pi, pj) =?l?I(pi)?I(pj)(pil + pjl)?l?I(pi)pil +?l?I(pj)pjlwhere values in pi and pj are point-wise mu-tual information, and I(?)
gives the indices of non-negative values in a vector.Methods on SVD factorization All these meth-ods perform computations the (85000, 800) U ma-trix in the SVD factorization.
On this we imple-ment two methods which do an arbitrary dimen-sion cut of k = 600: 1) SP-600 (scalar product)and 2) COS-600 (cosine similarity).
The othertwo algorithms: CurveS1 and CurveS2 use thetwo curve smoothness functions in Section 3.2; thecurves plot the scalar product corresponding to thetwo patterns, from dimension 1 to 800.Data In these preliminary experiments we limitourselves to paraphrasing a set of patterns ex-tracted from a subset of the TREC02-TREC06question answering tracks.
From these questionswe extracted and paraphrased the most frequentlyoccurring 20 patterns.
Since judging the cor-rectness of these paraphrases ?out-of-context?
israther difficult we limit ourselves to giving exam-ples and analyzing errors made on this data; im-portant observations can be clearly made this way,however in future work we plan to build a properevaluation setting (e.g.
task-based or instance-based in the sense of (Szpektor et al, 2007)) for29a more detailed analysis of the performance on themethods discussed.4.1 ResultsWe list the paraphrases obtained with the differentmethods for the pattern X subj????
show dobj????
Y .
Thispattern has been chosen out of the total set dueto its medium difficulty in terms of paraphrasing;some of the patterns in our list are relatively ac-curately paraphrased by all methods, such as win,while others such as marry are almost impossibleto paraphrase, for all methods.
In Table 2 we listthe top 10 expansions returned by the four meth-ods using the SVD factorization.
In bold we markcorrect patterns, which we consider to be patternsfor which there is a context in which the entail-ment holds in at least one direction.As it is clearly reflected in this example the SP-600 is much worse than any of the curve analy-sis methods; however using cosine as similaritymeasure at the same arbitrarily chosen dimension(COS-600) brings major improvements.The two curve smoothness methods exhibit asystematic difference between them.
In this ex-ample, and also across all 20 instances we haveconsidered, CurveS1 ranks as most similar, a largevariety of patterns with the same lexical root (inwhich, of course, syntax is often incorrect).
Onlyfollowing this we can find patterns expressing lex-ical variations; these again will be present in manysyntactic variations.
This sets CurveS1 apart fromboth CurveS2 and from COS-600 methods.
Theselatter two methods, although conceptually differ-ent seem to exhibit surprisingly similar behavior.The behavior of CurveS1 smoothing method isdifficult to judge without a proper evaluation; itcan be the case that the errors (mostly in syntac-tic relations) are indeed errors of the algorithm orthat the parser introduces them already in our inputdata.Table 3 shows the top 10 paraphrases returnedby the DIRT algorithm.
The DIRT paraphrases arerather accurate, however it is interesting to observethat DIRT and SVD methods can extract differ-ent paraphrases.
Table 4 gives examples of correctparaphrases which are identified by DIRT but notCurveS2 and the other way around.
This seems toindicate that these algorithms do capture differentaspects of the data and can be combined for bet-ter results.
An important aspect here is the factthat obtaining highly accurate paraphrases at theDIRTsubj????
reflectdobj????subj????
indicatedobj????subj????
demonstratedobj????pobj????
inprp???
showdobj????pobj????
toprp???
showdobj????subj????
representdobj????subj????
showprp???
inpobj????subj????
displaydobj????subj????
bringdobj????pobj????
withprp???
showdobj???
?Table 3: Top 10 paraphrases for X subj????
show dobj???
?Ycost of losing coverage is not particularly difficult4however not very useful.
Previous work such as(Dinu and Wang, 2009) has shown that for theseresources, the coverage is a rather important as-pect, since they have to capture the great varietyof ways in which a meaning can be expressed indifferent contexts.CurveS2 DIRTsubj????
showdobj????pobj????
inprp???
indicatedobj????subj????
displaydobj????pobj????
inprp???
reflectdobj????subj????
confirmdobj????dobj????
interpretprp???
aspobj????subj????
pointprp???
topobj????subj????
windobj????subj????
vieprp???
forpobj????pos???
victoryprp???
inpobj????subj????
competeprp???
forpobj????subj????
windobj????
titlenn???subj????
securedobj????appos?????
winnernn???subj????
enterdobj????subj????
marchprp???
intopobj????subj????
startprp???
inpobj????subj????
advanceprp???
intopobj????subj????
playprp???
inpobj????pos???
entryprp???
topobj????subj????
joinprp???
inpobj???
?Table 4: Example of paraphrases (i.e.
ranked inthe top 30) identified by one method and not theother4.2 DiscussionIn this section we attempt to get more insight intothe way the relatedness curves relate to the intu-itive notion of similarity, by examining curves ofincorrect paraphrases extracted by our methods.The first error we consider, is the pattern X pos???confidencepobj????
ofprp???
Y which is judged as be-ing very similar to show by SP-600, COS-600 aswell as CurveS2.
Figure 4 shows the relatednesscurves.
As it can be noticed, both the X and Ysimilarities grow dramatically around dimension4High precision can be very easily achieved simply by in-tersecting the sets of paraphrases returned by two or more ofthe methods implemented30SP-600 COS-600 CurveS1 CurveS2pos???
confidencepobj????
ofprp???subj????
indicatedobj????subj????
showprp???
inpobj????subj????
indicatedobj????subj????
boostdobj????
ratenn???subj????
showprp???
ofpobj????subj????
indicatedobj????subj????
reflectdobj????subj????
showprp???
ofpobj????subj????
representdobj????subj????
showprp???
withpobj????subj????
representdobj????prp???
topobj????
percentnn???pobj????
byprp???
showpartmod???????pobj????
withprp???
showdobj????subj????
bringdobj????
ratenn???subj????
totaldobj????
yuanappos?????pobj????
inprp???
reflectdobj????subj????
showtmod?????subj????
showprp???
ofpobj????subj????
hitdobj????
dollarappos?????pos???
confidencepobj????
ofprp???subj????
showprp???
despitepobj????dobj????
interpretprp???
aspobj????subj????
reachdobj????
dollarappos?????pobj????
byprp???
reflectdobj????pobj????
duringprp???
showdobj????pos???
confidencepobj????
ofprp???subj????
slashdobj????
ratenn???pobj????
inprp???
indicatedobj????pobj????
inprp???
showdobj????subj????
showdobj????
ratenn???nn???
confidencepobj????
ofprp???subj????
reflectdobj????pobj????
byprp???
showpartmod???????subj????
putdobj????
ratenn???subj????
raisedobj????
ratenn???subj????
interpretprp???
aspobj????pobj????
onprp???
showdobj????pobj????
byprp???
showpartmod??????
?Table 2: Top 10 paraphrases for X subj????
show dobj????
Y500.
Therefore the scalar product will be very highat cut point 600, leading to methods?
SP-600 andCOS-600 error.
However the two curve methodsare sensitive to the shape of the relatedness curves.Since CurveS2 is sensitive to actual drop values inthese curves, this pair will still be ranked very sim-ilar.
The curves do decrease by small amounts inmany points which is why method CurveS1 doesscore these two patterns as very similar.An interesting point to be made here is that, thispair is ranked similar by three methods out of fourbecause of the dramatic increase in relatedness ataround dimension 500.
However, intuitively, suchan increase should be more relevant at earlier di-mensions, which correspond to the larger eigen-values, and therefore to the most relevant con-cepts.
Indeed, in the data we have analyzed, highlysimilar patterns exhibit large increases at earlier(first 100-200) dimensions, similarly to the exam-ples given in Figure 1 and Figure 2.
This leadsus to a particular aspect that we would like to in-vestigate in future work, which is to analyze thebehavior of a relatedness curve in relation to rel-evance weights obtained from the eigenvalues ofthe matrix factorization.In Figure 5 we plot a second error, the relat-edness curves of show with X subj????
boost dobj????ratenn???
Y which is as error made only by the SP-600 method.
The similarity reflected in curve Yis relatively high (given by the large overlap of Y-filler interest), however we obtain a very high Xsimilarity only due to the peak of the scalar prod-uct exactly around the cut dimension 600.5 ConclusionIn this paper we have investigated the relevance ofjudging similarity of two phrases across all ranksk in a SVD approximation of a phrase/term co-Figure 4: X-filler and Y-filler relatedness curvesfor (X subj????
show dobj????
Y,X pos???
confidence pobj????ofprp???
Y )Figure 5: X-filler and Y-filler relatedness curvesfor (X subj????
show dobj????
Y,X subj????
boost dobj????ratenn???
Y )occurrence matrix.
We confirm the major observa-tions made in previous work and our preliminaryexperiments indicate that reliable similarity scoresfor paraphrasing can be obtained from the analysisof relatedness scores across all dimensions.In the future we plan to 1) use the observationswe have made in Section 4.2 to focus on iden-tifying good curve-smoothness functions and 2)build an appropriate evaluation setting in order tobe able to accurately judge the performance of themethods we propose.Finally, in this paper we have investigated theseaspects for the task of paraphrasing in a particularsetting, however our findings can be applied to anyvector space method for semantic similarity.31ReferencesScott C. Deerwester and Susan T. Dumais and ThomasK.
Landauer and George W. Furnas and Richard A.Harshman 1990.
Indexing by Latent Semantic Anal-ysis In JASIS.Bast, Holger and Majumdar, Debapriyo.
2005.
Whyspectral retrieval works.
SIGIR ?05: Proceedings ofthe 28th annual international ACM SIGIR confer-ence on Research and development in informationretrieval.Dekang Lin and Patrick Pantel.
2001.
DIRT - Discov-ery of Inference Rules from Text.
In Proceedings ofthe ACM SIGKDD Conference on Knowledge Dis-covery and Data Mining.Georgiana Dinu and Rui Wang.
2009.
Inference rulesand their application to recognizing textual entail-ment.
In Proceedings of the 12th Conference of theEuropean Chapter of the ACL (EACL 2009).Idan Szpektor and Eyal Shnarch and Ido Dagan 2007.Instance-based Evaluation of Entailment Rule Ac-quisition.
In Proceedings of the 45th Annual Meet-ing of the Association for Computational Linguis-tics.32
