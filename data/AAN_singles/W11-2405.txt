Proceedings of the TextInfer 2011 Workshop on Textual Entailment, EMNLP 2011, pages 35?39,Edinburgh, Scotland, UK, July 30, 2011. c?2011 Association for Computational LinguisticsDiversity-aware Evaluation for Paraphrase PatternsHideki Shima Teruko MitamuraLanguage Technologies Institute Language Technologies InstituteCarnegie Mellon University Carnegie Mellon UniversityPittsburgh, PA, USA Pittsburgh, PA, USAhideki@cs.cmu.edu teruko@cs.cmu.eduAbstractCommon evaluation metrics for paraphrasepatterns do not necessarily correlate withextrinsic recognition task performance.
Wepropose a metric which gives weight to lex-ical variety in paraphrase patterns; our pro-posed metric has a positive correlation withparaphrase recognition task performance,with a Pearson correlation of 0.5~0.7 (k=10,with ?strict?
judgment) in a statistically sig-nificant level (p-value<0.01).1 IntroductionWe propose a diversity-aware paraphrase evaluation me-tric called DIMPLE1, which boosts the scores of lexicallydiverse paraphrase pairs.
Paraphrase pairs or patterns areuseful in various NLP related research domains, sincethere is a common need to automatically identify meaningequivalence between two or more texts.Consider a paraphrase pair resource that links ?killed?to ?assassinated?
(in the rest of this paper we denote sucha rule as ?
?killed?2, ?assassinated?3?).
In automatic evalu-ation for Machine Translation (MT) (Zhou et al, 2006;Kauchak and Barzilay, 2006; Pad?
et al, 2009), this rulemay enable a metric to identify phrase-level semanticsimilarity between a system response containing ?killed?,and a reference translation containing ?assassinated?.Similarly in query expansion for information retrieval(IR) (Riezler et al, 2007), this rule may enable a system to1DIversity-aware Metric for Pattern Learning Experiments2Source term/phrase that contains ?killed?3Paraphrase that contains ?assassinated?expand the query term ?killed?
with the paraphrase ?as-sassinated?, in order to match a potentially relevant doc-ument containing the expanded term.To evaluate paraphrase patterns during pattern dis-covery, ideally we should use an evaluation metric thatstrongly predicts performance on the extrinsic task (e.g.fluency and adequacy scores in MT, mean averageprecision in IR) where the paraphrase patterns are used.Many existing approaches use a paraphrase evaluationmethodology where human assessors judge each paraph-rase pair as to whether they have the same meaning.
Overa set of paraphrase rules for one source term, ExpectedPrecision (EP) is calculated by taking the mean of preci-sion, or the ratio of positive labels annotated by assessors(Bannard and Callison-Burch, 2005; Callison-Burch,2008; Kok and Brockett, 2010; Metzler et al, 2011).The weakness of this approach is that EP is an in-trinsic measure that does not necessarily predict howwell a paraphrase-embedded system will perform inpractice.
For example, a set of paraphrase pairs?
?killed?, ?shot and killed?
?, ?
?killed?, ?reportedkilled??
?
?
?killed?, ?killed in??
may receive a perfectscore of 1.0 in EP; however, these patterns do notprovide lexical diversity (e.g.
?
?killed?, ?assassi-nated??)
and therefore may not perform well in anapplication where lexical diversity is important.The goal of this paper is to provide empirical evidenceto support the assumption that the proposed paraphraseevaluation metric DIMPLE correlates better with pa-raphrase recognition task metric scores than previousmetrics do, by rewarding lexical diverse patterns.2 DIMPLE MetricPatterns or rules for capturing equivalence in meaningare used in various NLP applications.
In a broad sense,35the terms ?paraphrase?
will be used to denote pairs ora set of patterns that represent semantically equivalentor close texts with different surface forms.Given paraphrase patterns P, or the ranked list of dis-tinct paraphrase pairs sorted by confidence in descendingorder, DIMPLEk evaluates the top k patterns, and pro-duces a real number between 0 and 1 (higher the better).2.1 Cumulative GainDIMPLE is inspired by the Cumulative Gain (CG)metric (J?rvelin and Kek?l?inen, 2002; Kek?l?inen,2005) used in IR.
CG for the top k retrieved docu-ments is calculated as ?==ki ikgain1CGwhere thegain function is human-judged relevance grade of thei-th document with respect to information need (e.g.
0through 3 for irrelevant, marginally relevant, fairlyrelevant and highly relevant respectively).
We take analternative well-known formula for CG calculation,which puts stronger emphasis at higher gain:.)1^2(CG1?
= ?= ki ik gain2.2 DIMPLE AlgorithmDIMPLE is a normalized CG calculated on each pa-raphrase.
The gain function of DIMPLE isrepresented as a product of pattern quality Q and lex-ical diversity D: .iii DQgain ?=DIMPLE at rank k isa normalized CGk which is defined as:ZDQZki iikk?=?
?==1}1)(^2{CGDIMPLEwhere Z is a normalization factor such that the perfectCG score is given.
Since Q takes a real value between0 and 1, and D takes an integer between 1 and 3,.}13^2{1?
= ?= kiZBeing able to design Q and D independently is one ofcharacteristics in DIMPLE.
In theory, Q can be anyquality measure on paraphrase patterns, such as the in-stance-based evaluation score (Szpektor et al, 2007), oralignment-based evaluation score (Callison-Burch et al,2008).
Similarly, D can be implemented depending onthe domain task; for example, if we are interested inlearning paraphrases that are out-of-vocabulary or do-main-specific, D could consult a dictionary, and return ahigh score if the lexical entry could not be found.The DIMPLE framework is implemented in thefollowing way4.
Let Q be the ratio of positive labels4Implementation used for this experiment is available athttp://code.google.com/p/dimple/averaged over pairs by human assessors given pi as towhether a paraphrase has the same meaning as thesource term or not.
Let D be the degree of lexical di-versity of a pattern calculated using Algorithm 1 below.Algorithm 1.
D score calculationInput: paraphrases {w1, ?, wk} for a source term s1: Set history1 = extractContentWords(s)2: Set history2 = stemWords(history1)3: for i=1 to k do4:     Set W1 = extractContentWords(wi)5:     Set W2 = stemWords(W1) // Porter stemming6:     if W1==?
|| W1 ?
history1 != ?7:         D[i] = 1 // word already seen8:     else9:         if W2 ?
history2 != ?10:             D[i] = 2 // root already seen11:         else12:             D[i] = 3 // unseen word13:         end if14:         history1 = W1 ?
history115:         history2 = W2 ?
history216:     end if17: end for3 ExperimentWe use the Pearson product-moment correlation coeffi-cient to measure correlation between two vectors con-sisting of intrinsic and extrinsic scores on paraphrasepatterns, following previous meta-evaluation research(Callison-Burch et al, 2007; Callison-Burch et al, 2008;Tratz and Hovy, 2009; Przybocki et al, 2009).
By in-trinsic score, we mean a theory-based direct assessmentresult on the paraphrase patterns.
By extrinsic score, wemean to measure how much the paraphrase recognitioncomponent helps the entire system to achieve a task.
Thecorrelation score is 1 if there is a perfect positive corre-lation, 0 if there is no correlation and -1 if there is a per-fect negative correlation.Using a task performance score to evaluate a pa-raphrase generation algorithm has been studied pre-viously (Bhagat and Ravichandran, 2008; Szpektorand Dagan, 2007; Szpektor and Dagan, 2008).
Acommon issue in extrinsic evaluations is that it is hardto separate out errors, or contributions from otherpossibly complex modules.
This paper presents anapproach which can predict task performance in moresimple experimental settings.3.1 Annotated Paraphrase ResourceWe used the paraphrase pattern dataset ?paraph-rase-eval?
(Metzler et al, 2011; Metzler and Hovy,2011) which contains paraphrase patterns acquired bymultiple algorithms: 1) PD (Pasca and Dienes, 2005),36which is based on the left and right n-gram contexts ofthe source term, with scoring based on overlap; 2) BR(Bhagat and Ravichandran, 2008), based on NounPhrase chunks as contexts; 3) BCB (Bannard andCallison-Burch, 2005) and 4) BCB-S (Callison-Burch,2008), which are based on monolingual phrasealignment from a bilingual corpus using a pivot.
In thedataset, each paraphrase pair is assigned with an an-notation as to whether a pair is a correct paraphrase ornot by 2 or 3 human annotators.The source terms are 100 verbs extracted fromnewswire about terrorism and American football.
Weselected 10 verbs according to their frequency in ex-trinsic task datasets (details follow in Section 3.3).Following the methodology used in previous pa-raphrase evaluations (Bannard and Callison-Burch,2005; Callison-Burch, 2008; Kok and Brockett, 2010),the labels were annotated on a pair of two sentences: anoriginal sentence containing the source term, and thesame sentence with the source term replaced with theparaphrase pattern, so that contextual informationcould help annotators to make consistent judgments.The judgment is based on whether the ?same meaning?is present between the source term and its paraphrase.There is a lenient and a strict distinction on the ?samemeaning?
judgments.
The strict label is given when thereplaced sentence is grammatically correct whereas thelenient label is given even when the sentence is gram-matically incorrect.In total, we have 10 (source terms listed in Table 1)?
4 (paraphrase generation algorithms introducedabove) = 40 sets of paraphrase patterns.
In each set ofparaphrase patterns, there are up to 10 unique ?sourceterm, paraphrase?
pairs.3.2 Intrinsic Paraphrase MetricsWe will discuss the common metric EP, and its variantEPR as baselines to be compared with DIMPLE.
Foreach metric, we used a cutoff value of k=1, 5 and 10.EP: Our baseline is the Expected Precision at k, which isthe expected number of correct paraphrases among thetop k returned, and is computed as:?==ki ikQk 11EP whereQ is the ratio of positive labels.
For instance, if 2 out of 3human annotators judged that pi = ?
?killed?, ?fatallyshot??
has the same meaning, Qi = 2/3.EPR: Metzler et al, (2011) extended EP with a Re-dundancy judgment, which we shall call EPR wherelexically redundant paraphrases did not receive a cre-dit.
Unlike Metzler et al, (2011) where humansjudged redundancies, we do the judgment automati-cally with a Porter Stemmer (Porter, 1980) to extractand compare stemmed forms.
In that way EPR?soutput become comparable to DIMPLE?s, remainingredundancy scoring different (i.e.
binary filtering inEPR and 3-level weighting in DIMPLE).3.3 Extrinsic Evaluation DatasetsIdeally, paraphrase metric scores should correlate wellwith task performance metrics.
To insulate the expe-riment from external, uncontrollable factors (e.g.
er-rors from other task components), we created threedatasets with slightly different characteristics, wherethe essential task of recognizing meaning equivalencebetween different surface texts can be conducted.The numbers of positive-labeled pairs that we ex-tracted for the three corpus, MSRPC, RTE and CQAEare 3900, 2805 and 27397 respectively.
Table 1 showsthe number of text pairs selected in which at least oneof each pair contains a frequently occurring verb.Src verb MSRPC RTE CQAEfound 89 62 319called 59 61 379told 125 34 189killed 48 109 277accused 30 44 143to take 21 23 63reached 22 18 107returned 14 20 57turned 22 10 94broke 10 10 35Table 1.
10 most frequently occurring source verbsin three datasets.
Numbers are positive-labeled pairswhere the verb appears in at least one side of a pair.MSRPC: The Microsoft Research Paraphrase Corpus(Dollan et al, 2005) contains 5800 pairs of sentencesalong with human annotations where positive labelsmean semantic equivalence of pairs.RTE: (Quasi-)paraphrase patterns are useful for theclosely related task, Recognizing Textual Entailment.This dataset has been taken from the 2-way/3-way trackat PASCAL/TAC RTE1-4.
Positive examples are pre-mise-hypothesis pairs where human annotators assignedthe entailment label.
The original dataset has been gen-erated from actual applications such as Text Summari-zation, Information Extraction, IR, Question Answering.CQAE: Complex Question Answering Evaluation(CQAE) dataset has been built from 6 past TREC QAtracks, i.e., ?Other?
QA data from TREC 2005 through2007,  relation QA data from TREC 2005 and ciQAfrom TREC 2006 and 2007 (Voorhees and Dang, 2005;Dang et al, 2006; Dang et al, 2007).
We created uniquepairs consisting of a system response (often sen-37tence-length) and an answer nugget as positive examples,where the system response is judged by human as con-taining or expressing the meaning of the nugget.3.4 Extrinsic Performance MetricUsing the dataset described in Section 3.3, perfor-mance measures for each of the 40 paraphrase sets (10verbs times 4 generators) are calculated as the ratio ofpairs correctly identified as paraphrases.In order to make the experimental settings close to anactual system with an embedded paraphrase engine, wefirst apply simple unigram matching with stemmingenabled.
At this stage, a text with the source verb ?killed?and another text with the inflectional variant ?killing?would match.
As an alternative approach, we consult theparaphrase pattern set trying to find a match between thetexts.
This identification judgment is automated, wherewe assume a meaning equivalence is identified betweentexts when the source verb matches5 one text and one ofup to 10 paraphrases in the set matches the other.
Giventhese evaluation settings, a noisy paraphrase pair such as?
?killed?, ?to??
can easily match many pairs and falselyboost the performance score.
We filter such exceptionalcases when the paraphrase text contains only functionalwords.3.5 ResultsWe conducted experiments to provide evidence thatthe Pearson correlation coefficient of DIMPLE ishigher than that of the other two baselines.
Table 2and 3 below present the result where each number isthe correlation calculated on the 40 data points.EPk EPRk DIMPLEkk=1 5 10 1 5 10 1 5 10MSRPC -0.02 -0.24 -0.11 0.33 0.27 -0.12 0.32 0.20 0.25RTE 0.13 -0.05 0.11 0.33 0.12 0.09 0.46 0.25 0.37CQAE 0.08 -0.09 0.00 -0.02 -0.08 -0.13 0.35 0.25 0.40Table 2.
Correlation between intrinsic paraphrasemetrics and extrinsic paraphrase recognition task me-trics where DIMPLE?s Q score is based on lenientjudgment.
Bold figures indicate statistical significanceof the correlation statistics (null-hypothesis tested:?there is no correlation?, p-value<0.01).EPk EPRk DIMPLEkk=1 5 10 1 5 10 1 5 10MSRPC 0.12 0.13 0.19 0.26 0.36 0.37 0.26 0.35 0.52RTE 0.34 0.34 0.29 0.43 0.41 0.38 0.49 0.55 0.58CQAE 0.44 0.51 0.47 0.37 0.60 0.55 0.37 0.70 0.70Table 3.
Same as the Table 2, except that the Qscore is based on strict judgment.5We consider word boundaries when matching texts, e.g.?skilled?
and ?killed?
do not match.Table 2 shows that correlations are almost alwaysclose to 0, indicating that EP does not correlate withthe extrinsic measures when the Q score is calculatedin lenient judgment mode.
On the other hand, whenthe Q function is based on strict judgments, EP scoressometimes show a medium positive correlation withthe extrinsic task performance, such as on the CQAEdataset.In both tables, there is a general trend where thecorrelation scores fall in the same relative order (giventhe same cut-off value): EP < EPR < DIMPLE.
Thissuggests that DIMPLE has a higher correlation than theother two baselines, given the task performance meas-ure we experimented with.
As we can see from Table 2,DIMPLE correlates well with paraphrase task perfor-mance, especially when the cutoff value k is 5 or 10.The higher values in Table 3 (compared to Table 2)show that the strict judgment used for intrinsic metriccalculation is preferable over the lenient one.4 Conclusion and Future WorksWe proposed a novel paraphrase evaluation metriccalled DIMPLE, which gives weight to lexical variety.We built large scale datasets from three sources andconducted extrinsic evaluations where paraphraserecognition is involved.
Experimental results showedthat Pearson correlation statistics for DIMPLE areapproximately 0.5 to 0.7 (when k=10 and ?strict?annotations are used to calculate the score), which ishigher than scores for the commonly used EP andEPR metrics.Future works include applying DIMPLE on pat-terns for other tasks where lexical diversity matters(e.g.
Relation Extraction) with a customized Q and Dfunctions.
If Q function can be also calculated fullyautomatically, DIMPLE may be useful for learninglexically diverse pattern learning when it is incorpo-rated into optimization criteria.AcknowledgmentsWe gratefully acknowledges the support of DefenseAdvanced Research Projects Agency (DARPA) Ma-chine Reading Program under Air Force ResearchLaboratory (AFRL) prime contract no.FA8750-09-C-0172.
Any opinions, findings, andconclusion or recommendations expressed in thismaterial are those of the author(s) and do not neces-sarily reflect the view of the DARPA, AFRL, or theUS government.
We also thank Donald Metzler et alfor sharing their data, and Eric Nyberg and anonym-ous reviewers for their helpful comments.38ReferencesBannard, Colin and Chris Callison-Burch.
2005.
Pa-raphrasing with Bilingual Parallel Corpora.
In Pro-ceedings of ACL 2005.Bhagat, Rahul, Patrick Pantel, Eduard Hovy, and MarinaRey.
2007.
LEDIR: An Unsupervised Algorithm forLearning Directionality of Inference Rules.
In Pro-ceedings of EMNLP-CoNLL 2007.Bhagat, Rahul and Deepak Ravichandran.
2008.
LargeScale Acquisition of Paraphrases for Learning Sur-face Patterns.
In Proceedings of ACL-08: HLT.Callison-Burch, Chris.
2008.
Syntactic Constraints onParaphrases Extracted from Parallel Corpora.
InProceedings of EMNLP 2008.Callison-Burch, Chris, Cameron Fordyce, Philipp Koehn,Christof Monz, and Josh Schroeder.
2007.
(Meta-)Evaluation of Machine Translation.
In Proceedings ofthe Second Workshop on Statistical Machine Trans-lation - StatMT ?07.Callison-Burch, Chris, Cameron Fordyce, Philipp Koehn,Christof Monz, and Josh Schroeder.
2008.
FurtherMeta-Evaluation of Machine Translation.
In Pro-ceedings of the Third Workshop on Statistical Ma-chine Translation - StatMT ?08.Dang, Hoa Trang, Jimmy Lin, and Diane Kelly.
2006.Overview of the TREC 2006 Question AnsweringTrack.
In Proceedings of TREC 2006.Dang, Hoa Trang, Diane Kelly, and Jimmy Lin.
2007.Overview of the TREC 2007 Question AnsweringTrack.
In Proceedings of TREC 2007.Dolan, William B., and Chris Brockett.
2005.
Automat-ically Constructing a Corpus of Sentential Paraph-rases.
In Proceedings of the Third InternationalWorkshop on Paraphrasing (IWP2005).J?rvelin, Kalervo, Jaana Kek?l?inen.
2002.
CumulatedGain-based Evaluation of IR Techniques.
ACMTrans.
Inf.
Syst., Vol.
20, No.
4.
(October 2002), pp.422-446.Kauchak, David, and Regina Barzilay.
2006.
Paraph-rasing for Automatic Evaluation.
In Proceedings ofHLT-NAACL 2006.Kek?l?inen, Jaana.
2005.
Binary and Graded Relevancein IR Evaluations ?
Comparison of the Effects onRanking of IR Systems.
Information Processing &Management, 41, 1019-1033.Kok, Stanley and Chris Brockett.
2010.
Hitting the RightParaphrases in Good Time.
In Proceedings ofHLT-NAACL 2010.Lin, Dekang, and Patrick Pantel.
2001.
DIRT - Discoveryof Inference Rules from Text.
In Proceedings of theseventh ACM SIGKDD international conference onKnowledge discovery and data mining - KDD ?01323-328.Metzler, Donald, Eduard Hovy, and Chunliang Zhang.2011.
An Empirical Evaluation of Data-Driven Pa-raphrase Generation Techniques.
In Proceedings ofACL-HLT 2011.Metzler, Donald and Eduard Hovy.
2011.
Mavuno: AScalable and Effective Hadoop-Based ParaphraseHarvesting System.
To appear in Proceedings of theKDD Workshop on Large-scale Data Mining: Theoryand Applications (LDMTA 2011).Miller, Geroge A.
1995.
Wordnet: A Lexical Databasefor English.
CACM, 38(11):39-41.Pad?, Sebastian, Michel Galley, Dan Jurafsky, andChristopher D. Manning.
2009.
Robust MachineTranslation Evaluation with Entailment Features.
InProceedings of  ACL-IJCNLP ?09.Pasca, Marius and Pter Dienes.
2005.
Aligning Needlesin a Haystack: Paraphrase Acquisition Across theWeb.
In Processing of IJCNLP 2005.Porter, Martin F. 1980.
An Algorithm for Suffix Strip-ping, Program, 14(3): 130?137.Przybocki, Mark, Kay Peterson, S?bastien Bronsart, andGregory Sanders.
2009.
The NIST 2008 Metrics forMachine Translation Challenge?Overview, Me-thodology, Metrics, and Results.
Machine Translation,Volume 23 Issue 2-3.Riezler, Stefan, Alexander Vasserman, Ioannis Tso-chantaridis, Vibhu Mittal, and Yi Liu.
2007.
Statis-tical Machine Translation for Query Expansion inAnswer Retrieval.
In Proceedings of ACL 2007.Szpektor, Idan and Ido Dagan.
2007.
Learning CanonicalForms of Entailment Rules.
In Proceedings ofRANLP 2007.Szpektor, Idan, Eyal Shnarch and Ido Dagan.
2007.
In-stance-based Evaluation of Entailment Rule Acqui-sition.
In Proceedings of ACL 2007.Szpektor, Idan and Ido Dagan.
2008.
Learning Entail-ment Rules for Unary Templates.
In Proceedings ofCOLING 2008.Tratz, Stephen and Eduard Hovy.
2009.
BEwT-E forTAC 2009's AESOP Task.
In Proceedings of TAC-09.Gaithersburg, Maryland.Voorhees, Ellen M., and Hoa Trang Dang.
2005.
Over-view of the TREC 2005 Question Answering Track.In Proceedings of TREC 2005.Zhou, Liang, Chin-Yew Lin, and Eduard Hovy.
2006.Re-evaluating Machine Translation Results with Pa-raphrase Support.
In Proceedings of EMNLP 2006.39
