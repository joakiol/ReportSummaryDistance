Introduction to the CoNLL-2002 Shared Task:Language-Independent Named Entity RecognitionErik F. Tjong Kim SangCNTS - Language Technology GroupUniversity of Antwerperikt@uia.ua.ac.beAbstractWe describe the CoNLL-2002 shared task:language-independent named entity recogni-tion.
We give background information on thedata sets and the evaluation method, present ageneral overview of the systems that have takenpart in the task and discuss their performance.1 IntroductionNamed entities are phrases that contain thenames of persons, organizations, locations,times and quantities.
Example:[PER Wol ] , currently a journalist in[LOC Argentina ] , played with [PERDel Bosque ] in the nal years of theseventies in [ORG Real Madrid ] .This sentence contains four named entities:Wol and Del Bosque are persons, Argentinais a location and Real Madrid is a organiza-tion.
The shared task of CoNLL-2002 concernslanguage-independent named entity recogni-tion.
We will concentrate on four types ofnamed entities: persons, locations, organiza-tions and names of miscellaneous entities thatdo not belong to the previous three groups.
Theparticipants of the shared task have been oeredtraining and test data for two European lan-guages: Spanish and Dutch.
They have used thedata for developing a named-entity recognitionsystem that includes a machine learning compo-nent.
The organizers of the shared task were es-pecially interested in approaches that make useof additional nonannotated data for improvingtheir performance.2 Data and EvaluationThe CoNLL-2002 named entity data consists ofsix les covering two languages: Spanish andDutch1.
Each of the languages has a trainingle, a development le and a test le.
Thelearning methods will be trained with the train-ing data.
The development data can be usedfor tuning the parameters of the learning meth-ods.
When the best parameters are found, themethod can be trained on the training data andtested on the test data.
The results of the dif-ferent learning methods on the test sets will becompared in the evaluation of the shared task.The split between development data and testdata has been chosen to avoid that systems arebeing tuned to the test data.All data les contain one word per line withempty lines representing sentence boundaries.Additionally each line contains a tag whichstates whether the word is inside a named entityor not.
The tag also encodes the type of namedentity.
Here is a part of the example sentence:Wol B-PER, Ocurrently Oa Ojournalist Oin OArgentina B-LOC, Oplayed Owith ODel B-PERBosque I-PERWords tagged with O are outside of namedentities.
The B-XXX tag is used for the rstword in a named entity of type XXX and I-XXX is used for all other words in named en-tities of type XXX.
The data contains enti-1The data les are available from http://lcg-www.uia.ac.be/conll2002/ner/ties of four types: persons (PER), organiza-tions (ORG), locations (LOC) and miscella-neous names (MISC).
The tagging scheme is avariant of the IOB scheme originally put for-ward by Ramshaw and Marcus (1995).
We as-sume that named entities are non-recursive andnon-overlapping.
In case a named entity is em-bedded in another named entity usually onlythe top level entity will be marked.The Spanish data is a collection of news wirearticles made available by the Spanish EFENews Agency.
The articles are from May 2000.The annotation was carried out by the TALPResearch Center2of the Technical University ofCatalonia (UPC) and the Center of Languageand Computation (CLiC3) of the University ofBarcelona (UB), and funded by the EuropeanCommission through the NAMIC project (IST-1999-12392).
The data contains words and en-tity tags only.
The training, development andtest data les contain 273037, 54837 and 53049lines respectively.The Dutch data consist of four editions of theBelgian newspaper "De Morgen" of 2000 (June2, July 1, August 1 and September 1).
The datawas annotated as a part of the Atranos project4at the University of Antwerp in Belgium, Eu-rope.
The annotator has followed the MITREand SAIC guidelines for named entity recogni-tion (Chinchor et al, 1999) as well as possible.The data consists of words, entity tags and part-of-speech tags which have been derived by aDutch part-of-speech tagger (Daelemans et al,1996).
Additionally the article boundaries inthe text have been marked explicitly with linescontaining the tag -DOCSTART-.
The training,development and test data les contain 218737,40656 and 74189 lines respectively.The performance in this task is mea-sured with F=1rate which is equal to(2+1)*precision*recall / (2*precision+recall)with =1 (van Rijsbergen, 1975).
Precision isthe percentage of named entities found by thelearning system that are correct.
Recall is thepercentage of named entities present in the cor-pus that are found by the system.
A namedentity is correct only if it is an exact match ofthe corresponding entity in the data le.2http://www.talp.upc.es/3http://clic.
l.ub.es/4http://atranos.esat.kuleuven.ac.be/3 ResultsTwelve systems have participated in this sharedtask.
The results for the test sets for Spanishand Dutch can be found in Table 1.
A baselinerate was computed for both sets.
It was pro-duced by a system which only identied entitieswhich had a unique class in the training data.
Ifa phrase was part of more than one entity, thesystem would select the longest one.
All sys-tems that participated in the shared task haveoutperformed the baseline system.McNamee and Mayeld (2002) have appliedsupport vector machines to the data of theshared task.
Their system used many binaryfeatures for representing words (almost 9000).They have evaluated dierent parameter set-tings of the system and have selected a cascadedapproach in which rst entity boundaries werepredicted and then entity classes (Spanish testset: F=1=60.97; Dutch test set: F=1=59.52).Black and Vasilakopoulos (2002) have evalu-ated two approaches to the shared task.
Therst was a transformation-based method whichgenerated in rules in a single pass rather thanin many passes.
The second method was adecision tree method.
They found that thetransformation-based method consistently out-performed the decision trees (Spanish test set:F=1=67.49; Dutch test set: F=1=56.43)Tsukamoto, Mitsuishi and Sassano (2002)used a stacked AdaBoost classier for ndingnamed entities.
They found that cascading clas-siers helped improved performance.
Their -nal system consisted of a cascade of ve learnerseach of which performed 10,000 boosting rounds(Spanish test set: F=1=71.49; Dutch test set:F=1=60.93)Malouf (2002) tested dierent models withthe shared task data: a statistical baselinemodel, a Hidden Markov Model and maximumentropy models with dierent features.
The lat-ter proved to perform best.
The maximum en-tropy models beneted from extra feature whichencoded capitalization information, positionalinformation and information about the currentword being part of a person name earlier inthe text.
However, incorporating a list of per-son names in the training process did not help(Spanish test set: F=1=73.66; Dutch test set:F=1=68.08)Jansche (2002) employed a rst-order Markovmodel as a named entity recognizer.
His systemused two separate passes, one for extracting en-tity boundaries and one for classifying entities.He evaluated dierent features in both subpro-cesses.
The categorization process was trainedseparately from the extraction process but thatdid not seem to have harmed overall perfor-mance (Spanish test set: F=1=73.89; Dutchtest set: F=1=69.68)Patrick, Whitelaw and Munro (2002) presentSLINERC, a language-independent named en-tity recognizer.
The system uses tries as wellas character n-grams for encoding word-internaland contextual information.
Additionally, it re-lies on lists of entities which have been com-piled from the training data.
The overall sys-tem consists of six stages, three regarding entityrecognition and three for entity categorization.Stages use the output of previous stages for ob-taining an improved performance (Spanish testset: F=1=73.92; Dutch test set: F=1=71.36)Tjong Kim Sang (2002) has applied amemory-based learner to the data of the sharedtask.
He used a two-stage processing strategyas well: rst identifying entities and then clas-sifying them.
Apart from the base classier,his system made use of three extra techniquesfor boosting performance: cascading classiers(stacking), feature selection and system combi-nation.
Each of these techniques were shown tobe useful (Spanish test set: F=1=75.78; Dutchtest set: F=1=70.67).Burger, Henderson and Morgan (2002) haveevaluated three approaches to nding namedentities.
They started with a baseline systemwhich consisted of an HMM-based phrase tag-ger.
They gave the tagger access to a listof approximately 250,000 named entities andthe performance improved.
After this severalsmoothed word classes derived from the avail-able data were incorporated into the trainingprocess.
The system performed better with thederived word lists than with the external namedentity lists (Spanish test set: F=1=75.78;Dutch test set: F=1=72.57).Cucerzan and Yarowsky (2002) approachedthe shared task by using word-internal and con-textual information stored in character-basedtries.
Their system obtained good results by us-ing part-of-speech tag information and employ-ing the one sense per discourse principle.
Theauthors expect a performance increase when thesystem has access to external entity lists buthave not presented the results of this in detail(Spanish test set: F=1=77.15; Dutch test set:F=1=72.31).Wu, Ngai, Carpuat, Larsen and Yang (2002)have applied AdaBoost.MH to the sharedtask data and compared the performance withthat of a maximum entropy-based namedentity tagger.
Their system used lexicaland part-of-speech information, contextual andword-internal clues, capitalization information,knowledge about entity classes of previous oc-currences of words and a small external listof named entity words.
The boosting tech-niques operated on decision stumps, decisiontrees of depth one.
They outperformed themaximum entropy-based named entity tagger(Spanish test set: F=1=76.61; Dutch test set:F=1=75.36).Florian (2002) employed three stackedlearners for named entity recognition:transformation-based learning for obtain-ing base-level non-typed named entities, Snowfor improving the quality of these entitiesand the forward-backward algorithm for nd-ing categories for the named entities.
Thecombination of the three algorithms showeda substantially improved performance whencompared with a single algorithm and analgorithm pair (Spanish test set: F=1=79.05;Dutch test set: F=1=74.99).Carreras, Marquez and Padro (2002) have ap-proached the shared task by using AdaBoostapplied to xed-depth decision trees.
Their sys-tem used many dierent input features contex-tual information, word-internal clues, previousentity classes, part-of-speech tags (Dutch only)and external word lists (Spanish only).
It pro-cessed the data in two stages: rst entity recog-nition and then classication.
Their systemobtained the best results in this shared taskfor both the Spanish and Dutch test data sets(Spanish test set: F=1=81.39; Dutch test set:F=1=77.05).4 Concluding RemarksWe have described the CoNLL-2002 sharedtask: language-independent named entityrecognition.
Twelve dierent systems have beenapplied to data covering two Western Europeanlanguages: Spanish and Dutch.
A boosted deci-sion tree method obtained the best performanceon both data sets (Carreras et al, 2002).AcknowledgementsTjong Kim Sang is supported by IWT STWWas a researcher in the ATRANOS project.ReferencesWilliam J.
Black and Argyrios Vasilakopoulos.
2002.Language-independent named entity classicationby modied transformation-based learning and bydecision tree induction.
In Proceedings of CoNLL-2002.
Taipei, Taiwan.John D. Burger, John C. Henderson, and William T.Morgan.
2002.
Statistical named entity recog-nizer adaptation.
In Proceedings of CoNLL-2002.Taipei, Taiwan.Xavier Carreras, Llus Marques, and Llus Padro.2002.
Named entity extraction using adaboost.In Proceedings of CoNLL-2002.
Taipei, Taiwan.Nancy Chinchor, Erica Brown, Lisa Ferro, and PattyRobinson.
1999.
1999 Named Entity RecognitionTask Denition.
MITRE and SAIC.Silviu Cucerzan and David Yarowsky.
2002.
Lan-guage independent ner using a unied model ofinternal and contextual evidence.
In Proceedingsof CoNLL-2002.
Taipei, Taiwan.Walter Daelemans, Jakub Zavrel, Peter Berck, andSteven Gillis.
1996.
Mbt: A memory-based partof speech tagger-generator.
In Proceedings of theFourth Workshop on Very Large Corpora, pages14{27.
Copenhagen, Denmark.Radu Florian.
2002.
Named entity recognition as ahouse of cards: Classier stacking.
In Proceedingsof CoNLL-2002.
Taipei, Taiwan.Martin Jansche.
2002.
Named entity extractionwith conditional markov models and classiers.In Proceedings of CoNLL-2002.
Taipei, Taiwan.Robert Malouf.
2002.
Markov models for language-independent named entity recognition.
In Pro-ceedings of CoNLL-2002.
Taipei, Taiwan.Paul McNamee and James Mayeld.
2002.
Entityextraction without language-specic resources.
InProceedings of CoNLL-2002.
Taipei, Taiwan.Jon Patrick, Casey Whitelaw, and Robert Munro.2002.
Slinerc: The sydney language-independentnamed entity recogniser and classier.
In Proceed-ings of CoNLL-2002.
Taipei, Taiwan.Lance A. Ramshaw and Mitchell P. Marcus.
1995.Text chunking using transformation-based learn-ing.
In Proceedings of the Third ACL Workshopon Very Large Corpora, pages 82{94.
Cambridge,MA, USA.Erik F. Tjong Kim Sang.
2002.
Memory-basedSpanish test precision recall F=1Carreras et.al.
81.38% 81.40% 81.39Florian 78.70% 79.40% 79.05Cucerzan et.al.
78.19% 76.14% 77.15Wu et.al.
75.85% 77.38% 76.61Burger et.al.
74.19% 77.44% 75.78Tjong Kim Sang 76.00% 75.55% 75.78Patrick et.al.
74.32% 73.52% 73.92Jansche 74.03% 73.76% 73.89Malouf 73.93% 73.39% 73.66Tsukamoto 69.04% 74.12% 71.49Black et.al.
68.78% 66.24% 67.49McNamee et.al.
56.28% 66.51% 60.97baseline526.27% 56.48% 35.86Dutch test precision recall F=1Carreras et.al.
77.83% 76.29% 77.05Wu et.al.
76.95% 73.83% 75.36Florian 75.10% 74.89% 74.99Burger et.al.
72.69% 72.45% 72.57Cucerzan et.al.
73.03% 71.62% 72.31Patrick et.al.
74.01% 68.90% 71.36Tjong Kim Sang 72.56% 68.88% 70.67Jansche 70.11% 69.26% 69.68Malouf 70.88% 65.50% 68.08Tsukamoto 57.33% 65.02% 60.93McNamee et.al.
56.22% 63.24% 59.52Black et.al.
62.12% 51.69% 56.43baseline564.38% 45.19% 53.10Table 1: Overall precision, recall and F=1ratesobtained by the twelve participating systems onthe test data sets for the two languages in theCoNLL-2002 shared task.named entity recognition.
In Proceedings ofCoNLL-2002.
Taipei, Taiwan.Koji Tsukamoto, Yutaka Mitsuishi, and ManabuSassano.
2002.
Learning with multiple stackingfor named entity recognition.
In Proceedings ofCoNLL-2002.
Taipei, Taiwan.C.J.
van Rijsbergen.
1975.
Information Retrieval.Buttersworth.Dekai Wu, Grace Ngai, Marine Carpuat, JeppeLarsen, and Yongsheng Yang.
2002.
Boostingfor named entity recognition.
In Proceedings ofCoNLL-2002.
Taipei, Taiwan.5Due to some harmful annotation errors in the train-ing data, the baseline system performs less well thanexpected.
Without the errors, the baseline F=1rateswould have been 62.49 for Spanish and 57.59 for Dutch.
