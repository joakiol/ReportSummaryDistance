N-th Order Ergodie Multigram HMM for Modeling ofLanguages without Marked Word BoundariesHubert Hin-Cheung LAWDept .
of Computer  Sc ienceThe  Un ivers i ty  of I Iong KonghhcJ_aw?c s .
hku .
hkChork in  CHANDept .
of Computer  Sc ienceThe  Un ivers i ty  of I tong KongcchanOcs ,  hku .
hkAbstractI,;rgodie IIMMs have been successfullyused for modeling sentence production.llowever for some oriental languagessuch as Chinese, a word can consist ofmultiple characters without word bound-ary markers between adjacent wordsin a sentence.
This makes word-segmentation  the training and testingdata necessary before ergodic ItMM canbe applied as the langnage model.
Thispaper introduces the N-th order ErgodicMnltigram HMM for language modelingof such languages.
Each state of theIIMM can generate a variable numberof characters corresponding to one word.The model can be trained without word-segmented and tagged corpus, and bothsegmentation and tagging are trained inone single model.
Results on its applicwLion on a Chinese corpus are reported.1 MotivationStatistical language modeling offers advantagesincluding minimal domain specific knowledge andhand-written rules, trainability and scalabilitygiven a language corpus.
Language models, suchas N-gram class models (Brown et al, 1992) andErgodic Hidden Markov Models (Kuhn el, al.,1994) were proposed and used in applications suchas syntactic lass (POS) tagging for English (Cut-ting et al, 1992), clustering and scoring of recog-nizer sentence hypotheses.IIowever, in Chinese and many other orientallanguages, there are no boundary markers, suchas space, between words.
Therefore preprocessorshave to be used to perform word segmentation iorder to identify individual words before applyingthese word-based language models.
As a resultcurrent approaches to modeling these languagesare separated into two seperated processes.Word segmentation is by no means a trivial pro-cess, since ambiguity often exists.
Pot proper seg-mentation of a sentence, some linguistic informa-tion of the sentence should be used.
iIowever,commonly used heuristics or statistical based ap-proaches, such as maximal matching, fl'equencycounts or mutual information statistics, have toperform the segmentation without knowledge suchas the resulting word categories.To reduce the impact of erroneous egmenta-tion on the subsequent language model, (Changand Chan, 1993) used an N-best segmentation i -terface between them.
llowever, since this is stilla two stage model, the parameters of the wholemodel cannot be optimized together, and an N-best interface is inadequate for processing outputsfrom recognizers which can be highly ambiguous.A better approach :is to keep all possible seg-mentations in a lattice form, score the latticewith a language model, and finally retrieve thebest candidate by dynamic programming or somesearching algorithms.
N-gram models arc usu-ally used for scoring (Gu et al, 1991) (Nagata,1994), but their training requires the sentences ofthe corpus to be manuMly segmented, and evenclass-tagged if class-based N-gram is used, as in(Nagata, 1994).A language model which considers segmenta-tion ambiguities and integrates this with a N-gram model, and able to be trained and testedon a raw, unsegmented and untagged corpus, ishighly desirable for processing languages withoutmarked word boundaries.2 The Ergodie Mult igram HMMModel2.1 OverviewBased on the Hidden Markov Model, the Er-godic Multigram llidden Markov Model (l,aw andChan, 1996), when applied as a language model,can process directly on unsegmented input corpus204as it a l lows  a var iab le  mmf l )e r  o f  characters  in eachword class.
Other than that  its prol)erties are sin>liar to l';rgodic t l idden Markov Models (Kuhn ctal., 1994), that  both training and scoring can bedone directly on a raw, unCagged corpus, given alexicon with word classes.Specifically, the N-Oh order F, rgodic Mul t igramIt M M, as in conventional class-based (N+I ) -g rammodel,  assumes a (loubly stochastic process in sen-tence product ion.
The word-class sequence in ascalene(: follows Che N-Oh order Markov assulnl>tion, i.e.
tile identity of  a (:lass in the s('.lite\[Ic(~delmn(Is only on tim previous N classes, and theword observed depelads only on the class it l)e-longs to.
The difference is thai, this is a mult i -gram model  (Doligne and Bimbot,  1995) in thesense Chat each state (i.e.
node in the I IMM) (:a,tgenera.re a wu-iable number of ot)served charactersequences.
Sentence boundaries are inodelcd as asl)ecial class.This model can be apl/l ied to a.ll input sent(raceor a characCer latt ice as a language model.
'Fhemaxinnun l ikel ihood scat(: sequence through l,hemodel, obtaine(t using the ViCerl)i or Stack I)(>coding AlgoriChln, ret)resenCs the 1)est part icularsegmentat ion and class-tagging for the input sen-tence or lattice, since transit ion of states denotesa wor(t boundary and state identity denotes tileClU'rent word class.2.2  Le.xi( 'onA lexicon (CK\] P, 1993) of 78,322 words, each con~tainiug up to 10 characters, is awdlabh~ for use illthis work.
l ' ract ica l ly  all characters have an cnCl:yill the lexicon, so Chat out-of -vocalmlary words aremodeled as indivi(hlal eharacters.
There is a totalof 192 syntact ic  classes, arranged in a hierarchicalway.
For example,  the month names arc deuotedby the class Ndabc, where lg denotes Nouu, Nd de-notes ' lbmpora l  Nouns, Igda \['or 'l'im(~ lmmes andNdab for reusabh' ti lne names.
'\['here~ is a total of8 major  categories.Each word ill the dict ionary is aullol,al.cd withone or nlore syntact ic tags, tel)resenting dilferentsyntact ic  classes Che word cnn possibly belong to.Also, a frequ(mcy count tbr each word, base(l on acertain corpus, is given, bi l l  without inforniationon its distr ibut ion over different syntact ic classes.2.3  T( : rminoh)gyI,el, )42 be the set of all Chinese words in l, hc lex-icon.
A word "wk C W is made up of one or morecharacters, l,et ,s~ r = (.~;I, .';'21....
";T) denote, a sen-tence as a T-character  sequence.
A funcCion (5~,,is defined such Chat (Sw (~Vk, sit +r- I ) is \] if w,.
is ar -character  word st .
.
.
st+,,-1, and 0 otherwise.
11,et /2 be the Ul)per bound of r, i.e.
t,11o maxinntmuumber of characters ill a word (10 ill this paper).I,et (2/ = {c l .
.
.
cL}  be the set, of syntact icclasses, where L is the nmnber of syntact ic (:lassesin the lexicon (192 in our case).
Lot t?
C W ?
(/denote Che relaCion for all syntact ic classiticationsof the.
lexicon, such ChaC ('tot:, el) @ C ill' cl is one ofthe syntact ic classes tbr 'wk.
Each word wk llltlStbelong to one or more of the classes.A path Chrough the model represents a partic-ular segnmnCation and (:lass Lagging for the Sell--I,(~IIC('.. I,et ?7 = ( 'wt ,  ( : I t  ; ?
.
.
; "Wig, Cl K ) t)e a part icu-lar segmentat ion and (;lass tagging for the sentences~', where Wk is the kth word and elk dCllOtCS tllc(;lass assigned to w,:, as i l lustrated below.11) I ~cl 1 lt Jk ,e lk  ~I)K ~Cll,(( .S l  ?
?
.S t l - I  .
?
.
,S tk_  1 .
.
.S tk -1  ?
?
.8 IK_  l ?
.
.S ' I ' )l"(,r C Co be proper, I1' 2, .,~_, ) 1 aml(wk,cl~) C l' must be saCistied, where t0 = 1, tic =7 '+ 1 and tk - j  < l,, for 1 < k < K.2.4  I tMM S|:a|;es for  l;.he N- th  o rdermode lIn Che tirst order IIMM (class 1)it(am) lnodel, eachI1MM state corresl)onds directly to the word-classof a word.
lh l t  in general, for an N-Oh order I IMMmodel,  siuce each class depends on N previousclasses, each state has to rel)lJesellt C\]I(t COlil\])illa-t, ion of the classes of the most recelfl; N words,iuctlading the current, word.I,et Qi represent a stal,(~ of the N- th  order Er-go(lit Mult igraul  I IMM.
Thus Qi = ( (%.
.
.
c i~_ , )where tie iS the current word (:lass, ci, is the previ-()us word class, etc.
'\['here is a CeCal of L N states,which may nleall too many l)aranl('ters ( l /v+l  pos-sible state transit ions, each state can transit  to Lother states) for the model  if N is anything greaterth an ont.
'1'o solve this l)rol)lem, a reasonal)le aSSlllllli-l ion can })c luade that  the d('taih'xl (;lass ideatitles of a mor(~ (listanl, word have, in general,less influence than the closer ones Co the currentword class.
Thus instead of using C as tim clas-sit ication relation for all l)revious words, a set ofI~I'he ;algorithm to bc described ;tSSUlnCs tlt~Lt, th(,.
(:ha.r;tctcr identities arc known for the S(!lltCttC(~ 8; ?, })It(,it can *also be al)plicd when ca.ch charttctcr positionsL becomes a. set of possible (:h~u'a(:ter (:~Lndida.t, es bysimply letting &,,(wk,sl  +' ' - I )  -- i for all words wkwhich can be constructed from the c\]mr~t(:ter positionss t .
.
.
s t+,  1 of the input c\]mractcr lattice.
This en-al)les the mo(M to 1)e used as the languzLgc modelcomponent for r(!
(:ognizcrs and for decoding phoncti(:input.205classification relations {C(?
), C(1),...C (N-l) } canbe used, where C(?)
= C represents the origi-nal, most detailed classification relation for thecurrent word, and C (n) is the less detailed clas-sification scheme for the nth previous word ateach state.
Thus the number of states reducesto LQ ---- L(?
)L (1) .
.
.
L  (N-l) in which L('0 _ < L.Each state is represented as Qi = (c~?o)...elN-_~ O)where C (n) = {cln)}, 1 < I < L (n) is the class tagset for the nth previous word.However, if no constraints are imposed on theseries of classification relations C Oo , the numberof possible transitions may increase despite a de-crease in the number of states, since state tran-sitions may become possible between every twostate, resulting in a total of L(?
)2L (02 ... L (N- 1)2possible transitions.A constraint is imposed that, given that a wordbelongs to the class cl n) in the classification C(n),we can determine the corresponding word classc}, ~+0 the given word will belong to in C(~+1),and for every word there is no extra classifica-tions in C (n+l) not corresponding to one in C (n).Formally, there exist mapping functions 5 c('0 :COO ~ C("+0,  0 _< n _< N-2 ,  such that ifC(n)  ~(n+l ) \ ]  ~ .~'(n) then ((wk, cl n)) 6 C (n)) =>I ' '~1 ~ ), (n+l), C(n+l)) (wk,c v ) 6 for all wk 6 W, and thaty(n) is surjective.
In particular, to model sentenceboundaries, we allow $ to be a valid class tag forall C(n), and define 5e('~)($) = 2.The above constraint ensures that given a stateQ, : ,(c!?
),o .
cl :, 1))it can only transit toQi = (c5~),br(?
)(c~))' ' '  J-(N-2)(c~N--~u)))where c~? )
is any state in C (?).
Thus reducing tothe maximum number of possible transitions toL(?
)2L0) .
.
.
L(N- 1).This constraint is easily satisfied by using a hi-erarchical word-class cheme, such as the one inthe CKIP lexicon or one generated by hierarchi-cal word-clustering, so that the classification formore distant words (higher n in C (n)) uses a higherlevel, less detail tag set in the scheme.2.5 Sentence  L ike l ihood  Formulat ionLet {?}
be the set of all possible segmentationsand class taggings of a sentence.
Under the N-th order model (.
)N, the likelihood of each validsegmentation and tagging 12 of the sentence sT,/~(8T ,  ~\ [oN) ,  can  be derived as follows.P(w,, c** ; w=, c~= ;.
.
.
; Wg, e~,,.
IO N)= P(W 1 \]Cll )P(c l  1 I$N)P($MK... el.~_,,,+, ) ?K ( \[ Ik:= P(W~\]Clk )P(clk IC~*-1 " " " elk_N))= P(w~lc , , )P (O , , lSN)p($ lO ,K)  ?K (\[Ik=u P(w~lclk)P(Ql~ IQ,k-~))using Nth order Markov assumption and repre-senting the class history as HMM states.
$ de-notes the sentence boundary, elk is $ for k _< 0, andQ~k re(?)
c!
N-l) \] Note that Qlk can be de-I lk  * " ' ~k- -N+l  ""termined from clk and Qlk-~ due to the constrainton the classification, and thus P(Qzk\]Qlk_~) =P(ct~ IQl~-~).The likelihood of the sentence s T under themodel is given by the sum of the likelihoods ofits possible segmentations.v(s lo ) = v (sLno3 The Algorithms3.1 The  ParametersAs in conventional HMM, the Ergodic MultigramHMM consists of parameters E) N ~-- {A, B}, inwhich A = {aij\], 0 < i , j  <_ LQ (Total num-ber of states), denotes the set of state transitionprobabilities from Qi to Qi, i.e.
P(Q31Qi).
Inparticular, a0i = P(Qi\[$ N) and ai0 = P($\]Qi)denote the probabilities that the state Qi is theinitial and final state in traversing the HMM, re-spectively, a00 is left undefined.
H = {bj(w~)\],where 1 < j < L Q, denotes the set of word ob-servation probabilities of wk at the state Qj, i.e.P(wk\]Qj).The B matrix, as shown above, models theprobabilities that wk is observed given N mostrecent classes, and contains LQ\[W\] parameters(recall that LQ = L(?)L(1)...
L(N-1)).
Our ~as-sumption that wk only depends on the currentclass reduces the number of parameters to L(?
)\]W\[for the /3 matrix.
Thus in the model, bj(wk)representing P(Wk\[Qj) are tied together for allstates Qj with the same current word-class, i.e.P(wklOj) = P(welc,) if 03 = (c, .
.
. )
.
Also, aij is0 if Qi cannot transit to Qj.
As a resul~ the num-ber of parameters in the A matrix is only L(?
)LQ.Given the segmentation and class sequence ?of a sentence, the state sequence (Qz~ .. .
QI~) canbe derived from the class sequence (eh...ci~.
).Thus the observation probability of the sentence?
P~d '  ?
/ON) ,  can s~ ~ given ?
and the model O N ,  1,be reformulated asb ll (wl)ao l I(206Given this tbrmulation the training procedure ismost ly  similar to that of the first order ErgodicMnlt igram HMM.3.2 Forward  and  Backward  ProcedureThe forward variable is defined asO't(i) = P(S1 .
- .
St, Q I ( t ) -  " Qi\[ ~)N)where Q~(t) is the state of the \[IMM when the wordcontaining the character st as the last character isproduced.The recursive equations for c~t(i) are~t(j) =~t(j) =0{br t< 1w ~.1~ LQ~ \[~c~t-,'(i)aljbj(w~)l~w (Wk, stt -r+l  )\['or l <t  <7 'Similarly, the backward variable is defin('d asl i t ( i )  7- \ [ ' (S t -b1 .
.
.
s t  Iq+(,) = Qi ,  O N)'l'he recursive equations for fit(i) aref i t ( i )  - -9 (i) =fit(i)0 for t > TaioIt LQr= l  wkEla2 j==l~~o (wk, t+,.~ St+l )for I <t  <T- -1As A, H arrays and the 5~, fimction are mostly 0s,considerable simplification can be done in irnph'.-mentation.The likelihood of the sentence given the modelcan be evaluated asLQP(s'(' lO N) = ~f_~.,r(i)aioi=1The Viterbi algo,'ithm \[br this model can be obtained by replacing the summations of the forwardalgorithm with maximizations.3.3 Re-es t imat ion  A lgor i thm&(i, j) is detined as the probabil ity that given asentence .s~' and the model (_)N, a word ends atthe character st in the state Qi an(l tile next wordstarts at the character st+l in the state Qj.
Thus~t(i, j) can be expressed asRs,+, (j)r= l  wkCWP(sY'leN)\['or l < t < fl '-- I 1 < i , j  < LQ.
turthermoredellne %(/) to be the probahil ity that, given Sl rand O N , a word ends at the character st in thestate Qi.
Thusctt(i)/3,(i) for 1 <t  <7 ' ,1  < i< LQ.
7, ( i ) -  p(sy.l?N)Sulnlnation of (t (i, j )  ()vet" t gives tile expectednumber of times that state Qi transits to slateQj in the sentence, aml stunmation of 7t(i) overt gives the expected number of state Qi occurringin it.
Thtts the quotient of their summation overt gives aij, the new estimation for aij.
"1'- 1 (l'aij -- ~_\[, ~'t(,,Y)/~_~ 7,(i) for 1 _< i , j  .
::( LQt=l tin1The initial and fi,,a\[ class probability estimates,a0j and ai0 can be re-estimated as follows.Itr=l wkE'VV= t (si"leN)Paio -- c~.r(i)aio /~ 'T t ( i )To derive bj (w~:), first define ctt ~ (i) as the prob-ability of the sentence prefix (sl ?
?
.
st) with 'wa, instate Qi as the last coml)lete word.
ThusIt 1,~r= l  i=l( (): t-- ; ( i )a i j  bj ( w k )~w ('u)k , S tt--r + l ))This represents the contribution of wk, occurringas the last word in sl, to ,~,(j).
Also define 7't ?~ (j)to be the I)robability that, given the sente.nce ,s'~"and the model, we is observed to end at characterst in the state Qj.
(,~\[~(j)fJt(j)7~"~(J) - p(8~'lO N)Let Qj o Qj, denot(;s the relation that both Qjand Qj, represent the s~me current word class.Thus summation of 71~k(j) ow:r t gives the e.x-petted munber of times that wk is observed in207state Qj, and summation of 7t(J) over t givesthe total expected number of occurrence of stateQj.
Since states with the same current word classare tied together by our assumption, the requiredvalue of bj(wk) is given byE J' E~I  ,./~ok (j,)-Dj (Wk ) = Q.ioQj,E ; ET1  7t(J')QjoQj,4 Exper imenta l  Resu l ts4.1 SetupA corpus of daily newspaper articles is dividedinto training and testing sets for the experiments,which is 21M and 4M in size respectively.
Th(' firstorder (N=I )  algorithms are applied to the train-ing sets, and parameters obtained after differentiterations are used for testing.The initial parameters of the HMM are setbased on the frequency counts from the lexicon.The class-transition probability aij is initializedas the a priori probability of the state P(Qj), es-timated fl'om the relative frequency counts of thelexicon, bj(wk) is initialized as the relative countof the word wk within the class corresponding tothe current word class in Qj.
Words belongingto multiple classes have their counts distributedequally among them.
Smoothing is then appliedby adding each word count by 0.5 and normaliz-ing.After training, the Viterbi algorithm is used toretrieve the best segmentation and tagging ?
* ofeach sentence of the test corpus, by tracing thebest state sequence traversed.4.2 PerplexityThe test-set perplexity, calculated asm'= exp(- M \]-- log(J'(Z',iwhere the summation is taken over all sentencess~ '~ in the testing corpus, and M represents thenumber of characters in it, is used to measure theperformance of the model.The results for models trained on training cor-pus subsets of various sizes, and after various it-erations are shown (Table 1).
It is obvious thatwith small training corpus, over-training occurswith more iterations.
With more training data,the performance improves and over-training is notevident.4.3 Phonet ic  Input  Decod ingA further experiment is performed to use the mod-els to decode phonetic inputs (Gu et el., 1991).
'Daining Size 2 d 6 898K 194.009 214.096 246.613 286.7211.3M 126.084 122.304 121.606 121.7766.3M 118.531 113.600 111.745 110.78321M 116.376 11.1.275 109.282 108.1/12Table 1: Test Set Perplexities of testing set afterdifferent iterations on subsets of training setThis is not trivial since each Chinese syllablecan correspond to up to 80 different characters.Sentences from the testing corpus are first ex-panded into a lattice, formed by generating allthe common homophones of each Chinese charac-ter.
Tested on 360K characters, a character ecog-nition rate of 91.24:% is obtained for the modeltrained after 8 iterations with 21M of trainingtext.
The results are satisfactory given that thetest corpus contains many personal names and ()titof vocabulary words, and the highly ambiguousnature of (;he problem.5 Discuss ion and Conc lus ionIn this paper the N-th order Ergodic MultigramIIMM is introduced, whose application enables in-tegrated, iterative language model training on nn-tagged and unsegmented corpus in languages suchas Chinese.The pertbrmanee on higher order models are ex-pected to be better as the size of training corpus isrelatively large.
Itowever some form of smoothingmay have to be applied when the training corpussize is small.With some moditication this algorithm wouldwork on phoneme candidate input instead of char-acter candidate input.
This is useful in decod-ing phonetic strings without character boundaries,such as in continuous Chinese~Japanese~Koreanphonetic inpnt, or speech recognizers which out-put phonemes.This model also makes a wealth of techniqnesdeveloped for HMM in the speech recognitionfield available for language modeling in these lan-guages.ReferencesBrown, P.F., deSouza, P.V., Mercer, 11..L., DellaPietra, V.J., Lai, J.C. 1992.
Class-Based n-gram Models of Natural Language.
In Compu-lalional Linguistics, 18:467-479.Chang, C.II., Chart, C.1).
1993.
A Study on Inte-grating Chinese Word Segmentation and l)~rt -208of-Speech Tagging.
In Comm.
of COLIP,5', Vol3, No.
I, pp.69-77.Chinese Knowledge lntbrmation Group 1!)!)3.
InTechnical Report No.
93-05.
\[nstitul.e of lnt'ofmation Science, Academia Sinica, 'l'aiwan.Cutting, K., Kupic(', J., l)cdcrs(:n, J., Sibun, P.1992.
A PracticM I)ar|,-of-Sl)Cech Tagger.
InProceeding,s of the Third Confercu.cc on Appli(:dNatural Language Procc.s,sin9, pp.
133-140.I)clignc, S., Bimbot, F. t9i)5, l,~mgu;~g('.
Model-ing by Vt~ritd)le Length S(;quenccs: Thcor('.ticalFormul~ttion ~md Evahmtion of Multigrams.
In1CAb'5'P 95, Pl).
169-172.Gu, II.Y., Tscng, C.Y., l,cc, I,.S.
1991.
MarkovModeling of Mmldarin C'hincsc for decoding thephonc~ic sequence into Chinese ch;~r~cl.(ws.
InUompuler ,5'pooch and Language, Vol 5, pl).363-377.Kuhn, T., Nicmann, H., Schukat ?
TM~tmazz-ini, E.G.
1994.
Ergodic t/iddcn Markov Mod-els trod Polygr~ms for I,anguage Modeling.
InICA,gSP 94, pp.357-360.L~tw, t\[.II.C., Chan, (3.
1996.
Ergodi(" Multi-grotto IIMM Integrating Word Segmc'ntal, iou;rod Class Tagging for (Jhinesc I,mlguagc Mod-e\]ing.
'Fo appear in 1CAHS'I ~ 95.Na.gata, M. 1994.
A Stochastic ,\]ap~mcs(~Morphok)gical AnMyzcr Using ~ l,'orwa.rd-l)PB~L<;kwa.rd-A* N-Best Sear<:h Algorithm.
InCOL1NG 94, I)1).201-207.209
