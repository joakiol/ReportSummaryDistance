Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 1?12,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsStatistical Parsing of Morphologically Rich Languages (SPMRL)What, How and WhitherReut TsarfatyUppsala UniversitetDjame?
SeddahAlpage (Inria/Univ.
Paris-Sorbonne)Yoav GoldbergBen Gurion UniversitySandra Ku?blerIndiana UniversityMarie CanditoAlpage (Inria/Univ.
Paris 7)Jennifer FosterNCLT, Dublin City UniversityYannick VersleyUniversita?t Tu?bingenInes RehbeinUniversita?t Saarbru?ckenLamia TounsiNCLT, Dublin City UniversityAbstractThe term Morphologically Rich Languages(MRLs) refers to languages in which signif-icant information concerning syntactic unitsand relations is expressed at word-level.
Thereis ample evidence that the application of read-ily available statistical parsing models to suchlanguages is susceptible to serious perfor-mance degradation.
The first workshop on sta-tistical parsing of MRLs hosts a variety of con-tributions which show that despite language-specific idiosyncrasies, the problems associ-ated with parsing MRLs cut across languagesand parsing frameworks.
In this paper we re-view the current state-of-affairs with respectto parsing MRLs and point out central chal-lenges.
We synthesize the contributions of re-searchers working on parsing Arabic, Basque,French, German, Hebrew, Hindi and Koreanto point out shared solutions across languages.The overarching analysis suggests itself as asource of directions for future investigations.1 IntroductionThe availability of large syntactically annotated cor-pora led to an explosion of interest in automati-cally inducing models for syntactic analysis and dis-ambiguation called statistical parsers.
The devel-opment of successful statistical parsing models forEnglish focused on the Wall Street Journal PennTreebank (PTB, (Marcus et al, 1993)) as the pri-mary, and sometimes only, resource.
Since the ini-tial release of the Penn Treebank (PTB Marcus etal.
(1993)), many different constituent-based parsingmodels have been developed in the context of pars-ing English (e.g.
(Magerman, 1995; Collins, 1997;Charniak, 2000; Chiang, 2000; Bod, 2003; Char-niak and Johnson, 2005; Petrov et al, 2006; Huang,2008; Finkel et al, 2008; Carreras et al, 2008)).At their time, each of these models improved thestate-of-the-art, bringing parsing performance on thestandard test set of the Wall-Street-Journal to a per-formance ceiling of 92% F1-score using the PARS-EVAL evaluation metrics (Black et al, 1991).
Someof these parsers have been adapted to other lan-guage/treebank pairs, but many of these adaptationshave been shown to be considerably less successful.Among the arguments that have been proposedto explain this performance gap are the impact ofsmall data sets, differences in treebanks?
annotationschemes, and inadequacy of the widely used PARS-EVAL evaluation metrics.
None of these aspects inisolation can account for the systematic performancedeterioration, but observed from a wider, cross-linguistic perspective, a picture begins to emerge ?that the morphologically rich nature of some of thelanguages makes them inherently more susceptibleto such performance degradation.
Linguistic factorsassociated with MRLs, such as a large inventory ofword-forms, higher degrees of word order freedom,and the use of morphological information in indi-cating syntactic relations, makes them substantiallyharder to parse with models and techniques that havebeen developed with English data in mind.1In addition to these technical and linguistic fac-tors, the prominence of English parsing in the litera-ture reduces the visibility of research aiming to solveproblems particular to MRLs.
The lack of stream-lined communication among researchers workingon different MRLs often leads to a reinventing thewheel syndrome.
To circumvent this, the first work-shop on Statistical Parsing of Morphologically RichLanguages (SPMRL 2010) offers a platform forthis growing community to share their views of thedifferent problems and oftentimes similar solutions.We identify three main types of challenges, eachof which raises many questions.
Many of the ques-tions are yet to be conclusively answered.
The firsttype of challenges has to do with the architecturalsetup of parsing MRLs: What is the nature of the in-put?
Can words be represented abstractly to reflectshared morphological aspects?
How can we copewith morphological segmentation errors propagatedthrough the pipeline?
The second type concerns therepresentation of morphological information insidethe articulated syntactic model: Should morpholog-ical information be encoded at the level of PoS tags?On dependency relations?
On top of non-terminalssymbols?
How should the integrated representationsbe learned and used?
A final genuine challengehas to do with sound estimation for lexical probabil-ities: Given the finite, and often rather small, set ofdata, and the large number of morphological analy-ses licensed by rich inflectional systems, how can weanalyze words unseen in the training data?Many of the challenges reported here are mostlyirrelevant when parsing Section 23 of the PTB butthey are of primordial importance in other tasks, in-cluding out-of-domain parsing, statistical machinetranslation, and parsing resource-poor languages.By synthesizing the contributions to the workshopand bringing it to the forefront, we hope to advancethe state of the art of statistical parsing in general.In this paper we therefore take the opportunityto analyze the knowledge that has been acquired inthe different investigations for the purpose of iden-tifying main bottlenecks and pointing out promisingresearch directions.
In section 2, we define MRLsand identify syntactic characteristics associated withthem.
We then discuss work on parsing MRLs inboth the dependency-based and constituency-basedsetup.
In section 3, we review the types of chal-lenges associated with parsing MRLs across frame-works.
In section 4, we focus on the contributions tothe SPMRL workshop and identify recurring trendsin the empirical results and conceptual solutions.
Insection 5, we analyze the emerging picture from abird?s eye view, and conclude that many challengescould be more faithfully addressed in the context ofparsing morphologically ambiguous input.2 Background2.1 What are MRLs?The term Morphologically Rich Languages (MRLs)is used in the CL/NLP literature to refer to languagesin which substantial grammatical information, i.e.,information concerning the arrangement of wordsinto syntactic units or cues to syntactic relations, isexpressed at word level.The common linguistic and typological wisdom isthat ?morphology competes with syntax?
(Bresnan,2001).
In effect, this means that rich morphologygoes hand in hand with a host of nonconfigurationalsyntactic phenomena of the kind discussed by Hale(1983).
Because information about the relations be-tween syntactic elements is indicated in the form ofwords, these words can freely change their positionsin the sentence.
This is referred to as free word or-der (Mithun, 1992).
Information about the group-ing of elements together can further be expressed byreference to their morphological form.
Such logicalgroupings of disparate elements are often called dis-continuous constituents.
In dependency structures,such discontinuities impose nonprojectivity.
Finally,rich morphological information is found in abun-dance in conjunction with so-called pro-drop or zeroanaphora.
In such cases, rich morphological infor-mation in the head (or co-head) of the clause of-ten makes it possible to omit an overt subject whichwould be semantically impoverished.English, the most heavily studied language withinthe CL/NLP community, is not an MRL.
Eventhough a handful of syntactic features (such as per-son and number) are reflected in the form of words,morphological information is often secondary toother syntactic factors, such as the position of wordsand their arrangement into phrases.
German, anIndo-European language closely related to English,already exhibits some of the properties that make2parsing MRLs problematic.
The Semitic languagesArabic and Hebrew show an even more extreme casein terms of the richness of their morphological formsand the flexibility in their syntactic ordering.2.2 Parsing MRLsPushing the envelope of constituency parsing:The Head-Driven models of the type proposedby Collins (1997) have been ported to parsingmany MRLs, often via the implementation of Bikel(2002).
For Czech, the adaptation by Collins et al(1999) culminated in an 80 F1-score.German has become almost an archetype of theproblems caused by MRLs; even though Germanhas a moderately rich morphology and a moder-ately free word order, parsing results are far fromthose for English (see (Ku?bler, 2008) and referencestherein).
Dubey (2005) showed that, for Germanparsing, adding case and morphology informationtogether with smoothed markovization and an ade-quate unknown-word model is more important thanlexicalization (Dubey and Keller, 2003).For Modern Hebrew, Tsarfaty and Sima?an (2007)show that a simple treebank PCFG augmented withparent annotation and morphological information asstate-splits significantly outperforms Head-Drivenmarkovized models of the kind made popular byKlein and Manning (2003).
Results for parsingModern Standard Arabic using Bikel?s implemen-tation on gold-standard tagging and segmentationhave not improved substantially since the initial re-lease of the treebank (Maamouri et al, 2004; Kulicket al, 2006; Maamouri et al, 2008).For Italian, Corazza et al (2004) used the Stan-ford parser and Bikel?s parser emulation of Collins?model 2 (Collins, 1997) on the ISST treebank, andobtained significantly lower results compared to En-glish.
It is notable that these models were ap-plied without adding morphological signatures, us-ing gold lemmas instead.
Corazza et al (2004) fur-ther tried different refinements including parent an-notation and horizontal markovization, but none ofthem obtained the desired improvement.For French, Crabbe?
and Candito (2008) and Sed-dah et al (2010) show that, given a corpus compara-ble in size and properties (i.e.
the number of tokensand grammar size), the performance level, both forCharniak?s parser (Charniak, 2000) and the Berke-ley parser (Petrov et al, 2006) was higher for pars-ing the PTB than it was for French.
The split-merge-smooth implementation of (Petrov et al, 2006) con-sistently outperform various lexicalized and unlexi-calized models for French (Seddah et al, 2009) andfor many other languages (Petrov and Klein, 2007).In this respect, (Petrov et al, 2006) is consideredMRL-friendly, due to its language agnostic design.The rise of dependency parsing: It is commonlyassumed that dependency structures are better suitedfor representing the syntactic structures of free wordorder, morphologically rich, languages, because thisrepresentation format does not rely crucially on theposition of words and the internal grouping of sur-face chunks (Mel?c?uk, 1988).
It is an entirely differ-ent question, however, whether dependency parsersare in fact better suited for parsing such languages.The CoNLL shared tasks on multilingual depen-dency parsing in 2006 and 2007 (Buchholz andMarsi, 2006; Nivre et al, 2007a) demonstrated thatdependency parsing for MRLs is quite challenging.While dependency parsers are adaptable to manylanguages, as reflected in the multiplicity of the lan-guages covered,1 the analysis by Nivre et al (2007b)shows that the best result was obtained for English,followed by Catalan, and that the most difficult lan-guages to parse were Arabic, Basque, and Greek.Nivre et al (2007a) drew a somewhat typologicalconclusion, that languages with rich morphologyand free word order are the hardest to parse.
Thiswas shown to be the case for both MaltParser (Nivreet al, 2007c) and MST (McDonald et al, 2005), twoof the best performing parsers on the whole.Annotation and evaluation matter: An emerg-ing question is therefore whether models that havebeen so successful in parsing English are necessar-ily appropriate for parsing MRLs ?
but associatedwith this question are important questions concern-ing the annotation scheme of the related treebanks.Obviously, when annotating structures for languageswith characteristics different than English one has toface different annotation decisions, and it comes asno surprise that the annotated structures for MRLsoften differ from those employed in the PTB.1The shared tasks involved 18 languages, including manyMRLs such as Arabic, Basque, Czech, Hungarian, and Turkish.3For Spanish and French, it was shown by Cowanand Collins (2005) and in (Arun and Keller, 2005;Schluter and van Genabith, 2007), that restructuringthe treebanks?
native annotation scheme to matchthe PTB annotation style led to a significant gain inparsing performance of Head-Driven models of thekind proposed in (Collins, 1997).
For German, alanguage with four different treebanks and two sub-stantially different annotation schemes, it has beenshown that a PCFG parser is sensitive to the kind ofrepresentation employed in the treebank.Dubey and Keller (2003), for example, showedthat a simple PCFG parser outperformed an emula-tion of Collins?
model 1 on NEGRA.
They showedthat using sister-head dependencies instead of head-head dependencies improved parsing performance,and hypothesized that it is due to the flatness ofphrasal annotation.
Ku?bler et al (2006) showed con-siderably lower PARSEVAL scores on NEGRA (Skutet al, 1998) relative to the more hierarchically struc-tured Tu?Ba-D/Z (Hinrichs et al, 2005), again, hy-pothesizing that this is due to annotation differences.Related to such comparisons is the question of therelevance of the PARSEVAL metrics for evaluatingparsing results across languages and treebanks.
Re-hbein and van Genabith (2007) showed that PARS-EVAL measures are sensitive to annotation schemeparticularities (e.g.
the internal node ratio).
It wasfurther shown that different metrics (i.e.
the Leaf-ancestor path (Sampson and Babarczy, 2003) anddependency based ones in (Lin, 1995)) can lead todifferent performance ranking.
This was confirmedalso for French by Seddah et al (2009).The questions of how to annotate treebanks forMRLs and how to evaluate the performance of thedifferent parsers on these different treebanks is cru-cial.
For the MRL parsing community to be able toassess the difficulty of improving parsing results forFrench, German, Arabic, Korean, Basque, Hindi orHebrew, we ought to first address fundamental ques-tions including: Is the treebank sufficiently largeto allow for proper grammar induction?
Does theannotation scheme fit the language characteristics?Does the use of PTB annotation variants for otherlanguages influence parsing results?
Does the space-delimited tokenization allow for phrase boundarydetection?
Do the results for a specific approachgeneralize to more than one language?3 Primary Research QuestionsIt is firmly established in theoretical linguistics thatmorphology and syntax closely interact through pat-terns of case marking, agreement, clitics and varioustypes of compounds.
Because of such close interac-tions, we expect morphological cues to help parsingperformance.
But in practice, when trying to incor-porate morphological information into parsing mod-els, three types of challenges present themselves:Architecture and Setup: When attempting toparse complex word-forms that encapsulate bothlexical and functional information, important archi-tectural questions emerge, namely, what is the na-ture of the input that is given to the parsing system?Does the system attempt to parse sequences of wordsor does it aim to assign structures to sequences ofmorphological segments?
If the former is the case,how can we represent words abstractly so as to re-flect shared morphological aspects between them?If the latter is the case, how can we arrive at a goodenough morphological segmentation for the purposeof statistical parsing, given raw input texts?When working with morphologically rich lan-guages such as Hebrew or Arabic, affixes may havesyntactically independent functions.
Many parsingmodels assume segmentation of the syntactically in-dependent parts, such as prepositions or pronominalclitics, prior to parsing.
But morphological segmen-tation requires disambiguation which is non-trivial,due to case syncretism and high morphological am-biguity exhibited by rich inflectional systems.
Thequestion is then when should we disambiguate themorphological analyses of input forms?
Should wedo that prior to parsing or perhaps jointly with it?2Representation and Modeling: Assuming thatthe input to our system reflects morphological infor-mation, one way or another, which types of morpho-2Most studies on parsing MRLs nowadays assume the goldstandard segmentation and disambiguated morphological infor-mation as input.
This is the case, for instance, for the Arabicparsing at CoNLL 2007 (Nivre et al, 2007a).
This practice de-ludes the community as to the validity of the parsing resultsreported for MRLs in shared tasks.
Goldberg et al (2009), forinstance, show a gap of up to 6pt F1-score between performanceon gold standard segmentation vs. raw text.
One way to over-come this is to devise joint morphological and syntactic disam-biguation frameworks (cf.
(Goldberg and Tsarfaty, 2008)).4logical information should we include in the parsingmodel?
Inflectional and/or derivational?
Case infor-mation and/or agreement features?
How can valencyrequirements reflected in derivational morphologyaffect the overall syntactic structure?
In tandem withthe decision concerning the morphological informa-tion to include, we face genuine challenges concern-ing how to represent such information in the syntac-tic model, be it constituency-based or dependency-based.
Should we encode morphological informa-tion at the level of PoS tags and/or on top of syn-tactic elements?
Should we decorate non-terminalsnodes and/or dependency arcs or both?Incorporating morphology in the statistical modelis often even more challenging than the sum ofthese bare decisions, because of the nonconfigu-rational structures (free word order, discontinuousconstituents) for rich markings are crucial (Hale,1983).
The parsing models designed for English of-ten focus on learning rigid word order, and they donot take morphological information into account (cf.developing parsers for German (Dubey and Keller,2003; Ku?bler et al, 2006)).
The more complex ques-tion is therefore: what type of parsing model shouldwe use for parsing MRLs?
shall we use a generalpurpose implementation and attempt to amend it?how?
or perhaps we should devise a new model fromfirst principles, to address nonconfigurational phe-nomena effectively?
using what form of representa-tion?
is it possible to find a single model that caneffectively cope with different kinds of languages?Estimation and Smoothing: Compared to En-glish, MRLs tend to have a greater number of wordforms and higher out-of-vocabulary (OOV) rates,due to the many feature combinations licensed bythe inflectional system.
A typical problem associ-ated with parsing MRLs is substantial lexical datasparseness due to high morphological variation insurface forms.
The question is therefore, given ourfinite, and often fairly small, annotated sets of data,how can we guess the morphological analyses, in-cluding the PoS tag assignment and various features,of an OOV word?
How can we learn the probabil-ities of such assignments?
In a more general setup,this problem is akin to handling out-of-vocabularyor rare words for robust statistical parsing, and tech-niques for domain adaptation via lexicon enhance-Constituency-Based Dependency-BasedArabic (Attia et al, 2010) (Marton et al, 2010)?Basque - (Bengoetxea and Gojenola, 2010)English (Attia et al, 2010) -French (Attia et al, 2010)(Seddah et al, 2010)(Candito and Seddah, 2010)?
-German (Maier, 2010) -Hebrew (Tsarfaty and Sima?an, 2010) (Goldberg and Elhadad, 2010)?Hindi - (Ambati et al, 2010a)?
(Ambati et al, 2010b)Korean (Chung et al, 2010) -Table 1: An overview of SPMRL contributions.
(?
reportresults also for non-gold standard input)ment (also explored for English and other morpho-logically impoverished languages).So, in fact, incorporating morphological informa-tion inside the syntactic model for the purpose ofstatistical parsing is anything but trivial.
In the nextsection we review the various approaches taken inthe individual contributions of the SPMRL work-shop for addressing such challenges.4 Parsing MRLs: Recurring TrendsThe first workshop on parsing MRLs features 11contributions for a variety of languages with arange of different parsing frameworks.
Table 1 liststhe individual contributions within a cross-languagecross-framework grid.
In this section, we focus ontrends that occur among the different contributions.This may be a biased view since some of the prob-lems that exist for parsing MRLs may have not beenat all present, but it is a synopsis of where we standwith respect to problems that are being addressed.4.1 Architecture and Setup: Gold vs. PredictedMorphological InformationWhile morphological information can be very infor-mative for syntactic analysis, morphological anal-ysis of surface forms is ambiguous in many ways.In German, for instance, case syncretism (i.e.
a sin-gle surface form corresponding to different cases) ispervasive, and in Hebrew and Arabic, the lack of vo-calization patterns in written texts leads to multiplemorphological analyses for each space-delimited to-ken.
In real world situations, gold morphological in-formation is not available prior to parsing.
Can pars-ing systems make effective use of morphology evenwhen gold morphological information is absent?5Several papers address this challenge by present-ing results for both the gold and the automaticallypredicted PoS and morphological information (Am-bati et al, 2010a; Marton et al, 2010; Goldberg andElhadad, 2010; Seddah et al, 2010).
Not very sur-prisingly, all evaluated systems show a drop in pars-ing accuracy in the non-gold settings.An interesting trend is that in many cases, us-ing noisy morphological information is worse thannot using any at all.
For Arabic Dependency pars-ing, using predicted CASE causes a substantial dropin accuracy while it greatly improves performancein the gold setting (Marton et al, 2010).
ForHindi Dependency Parsing, using chunk-internalcues (i.e.
marking non-recursive phrases) is benefi-cial when gold chunk-boundaries are available, butsuboptimal when they are automatically predicted(Ambati et al, 2010a).
For Hebrew DependencyParsing with the MST parser, using gold morpholog-ical features shows no benefit over not using them,while using automatically predicted morphologicalfeatures causes a big drop in accuracy compared tonot using them (Goldberg and Elhadad, 2010).
ForFrench Constituency Parsing, Seddah et al (2010)and Candito and Seddah (2010) show that whilegold information for the part-of-speech and lemmaof each word form results in a significant improve-ment, the gain is low when switching to predictedinformation.
Reassuringly, Ambati et al (2010a),Marton et al (2010), and Goldberg and Elhadad(2010) demonstrate that some morphological infor-mation can indeed be beneficial for parsing even inthe automatic setting.
Ensuring that this is indeedso, appears to be in turn linked to the question ofhow morphology is represented and incorporated inthe parsing model.The same effect in a different guise appears inthe contribution of Chung et al (2010) concerningparsing Korean.
Chung et al (2010) show a sig-nificant improvement in parsing accuracy when in-cluding traces of null anaphors (a.k.a.
pro-drop) inthe input to the parser.
Just like overt morphology,traces and null elements encapsulate functional in-formation about relational entities in the sentence(the subject, the object, etc.
), and including them atthe input level provides helpful disambiguating cuesfor the overall structure that represents such rela-tions.
However, assuming that such traces are givenprior to parsing is, for all practical purposes, infeasi-ble.
This leads to an interesting question: will iden-tifying such functional elements (marked as traces,overt morphology, etc) during parsing, while com-plicating that task itself, be on the whole justified?Closely linked to the inclusion of morphologicalinformation in the input is the choice of PoS tag setto use.
The generally accepted view is that fine-grained PoS tags are morphologically more informa-tive but may be harder to statistically learn and parsewith, in particular in the non-gold scenario.
Mar-ton et al (2010) demonstrate that a fine-grained tagset provides the best results for Arabic dependencyparsing when gold tags are known, while a muchsmaller tag set is preferred in the automatic setting.4.2 Representation and Modeling:Incorporating Morphological InformationMany of the studies presented here explore the useof feature representation of morphological informa-tion for the purpose of syntactic parsing (Ambati etal., 2010a; Ambati et al, 2010b; Bengoetxea andGojenola, 2010; Goldberg and Elhadad, 2010; Mar-ton et al, 2010; Tsarfaty and Sima?an, 2010).
Cleartrends among the contributions emerge concerningthe kind of morphological information that helps sta-tistical parsing.
Morphological CASE is shown to bebeneficial across the board.
It is shown to help forparsing Basque, Hebrew, Hindi and to some extentArabic.3 Morphological DEFINITENESS and STATEare beneficial for Hebrew and Arabic when explic-itly represented in the model.
STATE, ASPECT andMOOD are beneficial for Hindi, but only marginallybeneficial for Arabic.
CASE and SUBORDINATION-TYPE are the most beneficial features for Basquetransition-based dependency parsing.A closer view into the results mentioned in theprevious paragraph suggests that, beyond the kindof information that is being used, the way in whichmorphological information is represented and usedby the model has substantial ramification as towhether or not it leads to performance improve-ments.
The so-called ?agreement features?
GEN-DER, NUMBER, PERSON, provide for an interestingcase study in this respect.
When included directly as3For Arabic, CASE is useful when gold morphology infor-mation is available, but substantially hurt results when it is not.6machine learning features, agreement features ben-efit dependency parsing for Arabic (Marton et al,2010), but not Hindi (dependency) (Ambati et al,2010a; Ambati et al, 2010b) or Hebrew (Goldbergand Elhadad, 2010).
When represented as simplesplits of non-terminal symbols, agreement informa-tion does not help constituency-based parsing per-formance for Hebrew (Tsarfaty and Sima?an, 2010).However, when agreement patterns are directly rep-resented on dependency arcs, they contribute an im-provement for Hebrew dependency parsing (Gold-berg and Elhadad, 2010).
When agreement is en-coded at the realization level inside a Relational-Realizational model (Tsarfaty and Sima?an, 2008),agreement features improve the state-of-the-art forHebrew parsing (Tsarfaty and Sima?an, 2010).One of the advantages of the latter study is thatmorphological information which is expressed at thelevel of words gets interpreted elsewhere, on func-tional elements higher up the constituency tree.
Independency parsing, similar cases may arise, thatis, morphological information might not be as use-ful on the form on which it is expressed, but wouldbe more useful at a different position where it couldinfluence the correct attachment of the main verbto other elements.
Interesting patterns of that sortoccur in Basque, where the SUBORDINATIONTYPEmorpheme attaches to the auxiliary verb, though itmainly influences attachments to the main verb.Bengoetxea and Gojenola (2010) attempted twodifferent ways to address this, one using a trans-formation segmenting the relevant morpheme andattaching it to the main verb instead, and anotherby propagating the morpheme along arcs, througha ?stacking?
process, to where it is relevant.
Bothways led to performance improvements.
The idea ofa segmentation transformation imposes non-trivialpre-processing, but it may be that automaticallylearning the propagation of morphological featuresis a promising direction for future investigation.Another, albeit indirect, way to include morpho-logical information in the parsing model is usingso-called latent information or some mechanismof clustering.
The general idea is the following:when morphological information is added to stan-dard terminal or non-terminal symbols, it imposesrestrictions on the distribution of these no-longer-equivalent elements.
Learning latent informa-tion does not represent morphological informationdirectly, but presumably, the distributional restric-tions can be automatically learned along with thesplits of labels symbols in models such as (Petrovet al, 2006).
For Korean (Chung et al, 2010),latent information contributes significant improve-ments.
One can further do the opposite, namely,merging terminals symbols for the purpose of ob-taining an abstraction over morphological features.When such clustering uses a morphological signa-ture of some sort, it is shown to significantly im-prove constituency-based parsing for French (Can-dito and Seddah, 2010).4.3 Representation and Modeling: Free WordOrder and Flexible Constituency StructureOff-the-shelf parsing tools are found in abundancefor English.
One problematic aspect of using themto parse MRLs lies in the fact that these tools fo-cus on the statistical modeling of configurationalinformation.
These models often condition on theposition of words relative to one another (e.g.
intransition-based dependency parsing) or on the dis-tance between words inside constituents (e.g.
inHead-Driven parsing).
Many of the contributions tothe workshop show that working around existing im-plementations may be insufficient, and we may haveto come up with more radical solutions.Several studies present results that support theconjecture that when free word-order is explicitlytaken into account, morphological information ismore likely to contribute to parsing accuracy.
TheRelational-Realizational model used in (Tsarfatyand Sima?an, 2010) allows for reordering of con-stituents at a configuration layer, which is indepen-dent of the realization patterns learned from the data(vis-a`-vis case marking and agreement).
The easy-first algorithm of (Goldberg and Elhadad, 2010)which allows for significant flexibility in the order ofattachment, allows the model to benefit from agree-ment patterns over dependency arcs that are easierto detect and attach first.
The use of larger subtreesin (Chung et al, 2010) for parsing Korean, within aBayesian framework, allows the model to learn dis-tributions that take more elements into account, andthus learn the different distributions associated withmorphologically marked elements in constituencystructures, to improve performance.7In addition to free word order, MRLs show higherdegree of freedom in extraposition.
Both of thesephenomena can result in discontinuous structures.In constituency-based treebanks, this is either an-notated as additional information which has to berecovered somehow (traces in the case of the PTB,complex edge labels in the German Tu?Ba-D/Z), oras discontinuous phrase structures, which cannot behandled with current PCFG models.
Maier (2010)suggests the use of Linear Context-Free RewritingSystems (LCFRSs) in order to make discontinuousstructure transparent to the parsing process and yetpreserve familiar notions from constituency.Dependency representation uses non-projectivedependencies to reflect discontinuities, which isproblematic to parse with models that assume pro-jectivity.
Different ways have been proposed to dealwith non-projectivity (Nivre and Nilsson, 2005; Mc-Donald et al, 2005; McDonald and Pereira, 2006;Nivre, 2009).
Bengoetxea and Gojenola (2010)discuss non-projective dependencies in Basque andshow that the pseudo-projective transformation of(Nivre and Nilsson, 2005) improves accuracy for de-pendency parsing of Basque.
Moreover, they showthat in combination with other transformations, itimproves the utility of these other ones, too.4.4 Estimation and Smoothing: Coping withLexical SparsityMorphological word form variation augments thevocabulary size and thus worsens the problem of lex-ical data sparseness.
Words occurring with medium-frequency receive less reliable estimates, and thenumber of rare/unknown words is increased.
Oneway to cope with the one of both aspects of thisproblem is through clustering, that is, providing anabstract representation over word forms that reflectstheir shared morphological and morphosyntactic as-pects.
This was done, for instance, in previous workon parsing German.
Versley and Rehbein (2009)cluster words according to linear context features.These clusters include valency information added toverbs and morphological features such as case andnumber added to pre-terminal nodes.
The clustersare then integrated as features in a discriminativeparsing model to cope with unknown words.
Theirdiscriminative model thus obtains state-of-the-art re-sults on parsing German.Several contribution address similar challenges.For constituency-based generative parsers, the sim-ple technique of replacing word forms with moreabstract symbols is investigated by (Seddah et al,2010; Candito and Seddah, 2010).
For French, re-placing each word form by its predicted part-of-speech and lemma pair results in a slight perfor-mance improvement (Seddah et al, 2010).
Whenwords are clustered, even according to a very locallinear-context similarity measure, measured over alarge raw corpus, and when word clusters are used inplace of word forms, the gain in performance is evenhigher (Candito and Seddah, 2010).
In both cases,the technique provides more reliable estimates forin-vocabulary words, since a given lemma or clusterappear more frequently.
It also increases the knownvocabulary.
For instance, if a plural form is un-seen in the training set but the corresponding singu-lar form is known, then in a setting of using lemmasin terminal symbols, both forms are known.For dependency parsing, Marton et al (2010) in-vestigates the use of morphological features that in-volve some semantic abstraction over Arabic forms.The use of undiacritized lemmas is shown to im-prove performance.
Attia et al (2010) specificallyaddress the handling of unknown words in the latent-variable parsing model.
Here again, the techniquethat is investigated is to project unknown words tomore general symbols using morphological clues.
Astudy on three languages, English, French and Ara-bic, shows that this method helps in all cases, butthat the greatest improvement is obtained for Arabic,which has the richest morphology among three.5 Where we?re atIt is clear from the present overview that we areyet to obtain a complete understanding concerningwhich models effectively parse MRLs, how to an-notate treebanks for MRLs and, importantly, howto evaluate parsing performance across types of lan-guages and treebanks.
These foundational issues arecrucial for deriving more conclusive recommenda-tions as to the kind of models and morphologicalfeatures that can lead to advancing the state-of-the-art for parsing MRLs.
One way to target such anunderstanding would be to encourage the investiga-tion of particular tasks, individually or in the context8of shared tasks, that are tailored to treat those prob-lematic aspects of MRLs that we surveyed here.So far, constituency-based parsers have been as-sessed based on their performance on the PTB (andto some extent, across German treebanks (Ku?bler,2008)) whereas comparison across languages wasrendered opaque due to data set differences andrepresentation idiosyncrasies.
It would be interest-ing to investigate such a cross-linguistic compari-son of parsers in the context of a shared task onconstituency-based statistical parsing, in additionalto dependency-based ones as reported in (Nivre etal., 2007a).
Standardizing data sets for a largenumber of languages with different characteristics,would require us, as a community, to aim forconstituency-representation guidelines that can rep-resent the shared aspects of structures in differentlanguages, while at the same time allowing differ-ences between them to be reflected in the model.Furthermore, it would be a good idea to intro-duce parsing tasks, for either constituent-based ordependency-based setups, which consider raw textas input, rather than morphologically segmentedand analyzed text.
Addressing the parsing prob-lem while facing the morphological disambiguationchallenge in its full-blown complexity would be il-luminating and educating for at least two reasons:firstly, it would give us a better idea of what is thestate-of-the-art for parsing MRLs in realistic scenar-ios.
Secondly, it might lead to profound insightsabout the potentially successful ways to use mor-phology inside a parser, which may differ from theinsights concerning the use of morphology in theless realistic parsing scenarios, where gold morpho-logical information is given.Finally, to be able to perceive where we standwith respect to parsing MRLs and how models fareagainst one another across languages, it would becrucial to arrive at evaluation metrics that captureinformation that is shared among the different repre-sentations, for instance, functional information con-cerning predicate-argument relations.
Using the dif-ferent kinds of measures in the context of cross-framework tasks will help us understand the util-ity of the different evaluation metrics that have beenproposed and to arrive at a clearer picture of what itis that we wish to compare, and how we can faith-fully do so across models, languages and treebanks.6 ConclusionThis paper presents the synthesis of 11 contributionsto the first workshop on statistical parsing for mor-phologically rich languages.
We have shown thatarchitectural, representational, and estimation issuesassociated with parsing MRLs are found to be chal-lenging across languages and parsing frameworks.The use of morphological information in the nongold-tagged input scenario is found to cause sub-stantial differences in parsing performance, and inthe kind of morphological features that lead to per-formance improvements.Whether or not morphological features help pars-ing also depends on the kind of model in whichthey are embedded, and the different ways they aretreated within.
Furthermore, sound statistical esti-mation methods for morphologically rich, complexlexica, turn out to be crucial for obtaining good pars-ing accuracy when using general-purpose modelsand algorithms.
In the future we hope to gain betterunderstanding of the common pitfalls in, and novelsolutions for, parsing morphologically ambiguousinput, and to arrive at principled guidelines for se-lecting the model and features to include when pars-ing different kinds of languages.
Such insights maybe gained, among other things, in the context ofmore morphologically-aware shared parsing tasks.AcknowledgementsThe program committee would like to thankNAACL for hosting the workshop and SIGPARSEfor their sponsorship.
We further thank INRIA Al-page team for their generous sponsorship.
We arefinally grateful to our reviewers and authors for theirdedicated work and individual contributions.ReferencesBharat Ram Ambati, Samar Husain, Sambhav Jain,Dipti Misra Sharma, and Rajeev Sangal.
2010a.
Twomethods to incorporate local morphosyntactic featuresin Hindi dependency parsing.
In Proceedings of theNAACL/HLT Workshop on Statistical Parsing of Mor-phologically Rich Languages (SPMRL 2010), Los An-geles, CA.Bharat Ram Ambati, Samar Husain, Joakim Nivre, andRajeev Sangal.
2010b.
On the role of morphosyntacticfeatures in Hindi dependency parsing.
In Proceedings9of the NAACL/HLT Workshop on Statistical Parsing ofMorphologically Rich Languages (SPMRL 2010), LosAngeles, CA.Abhishek Arun and Frank Keller.
2005.
Lexicalizationin crosslinguistic probabilistic parsing: The case ofFrench.
In Proceedings of the 43rd Annual Meeting ofthe Association for Computational Linguistics, pages306?313, Ann Arbor, MI.Mohammed Attia, Jennifer Foster, Deirdre Hogan,Joseph Le Roux, Lamia Tounsi, and Josef van Gen-abith.
2010.
Handling unknown words in statisticallatent-variable parsing models for Arabic, English andFrench.
In Proceedings of the NAACL/HLT Workshopon Statistical Parsing of Morphologically Rich Lan-guages (SPMRL 2010), Los Angeles, CA.Kepa Bengoetxea and Koldo Gojenola.
2010.
Applica-tion of different techniques to dependency parsing ofBasque.
In Proceedings of the NAACL/HLT Workshopon Statistical Parsing of Morphologically Rich Lan-guages (SPMRL 2010), Los Angeles, CA.Daniel M. Bikel.
2002.
Design of a multi-lingual,parallel-processing statistical parsing engine.
In Pro-ceedings of the Second International Conference onHuman Language Technology Research, pages 178?182.
Morgan Kaufmann Publishers Inc. San Francisco,CA, USA.E.
Black, S. Abney, D. Flickinger, C. Gdaniec, R. Gr-ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,J.
Klavans, M. Liberman, M. Marcus, S. Roukos,B.
Santorini, and T. Strzalkowski.
1991.
A procedurefor quantitatively comparing the syntactic coverageof English grammars.
In Proceedings of the DARPASpeech and Natural Language Workshop, pages 306?311, San Mateo (CA).
Morgan Kaufman.Rens Bod.
2003.
An efficient implementation of a newDOP model.
In Proceedings of the tenth conferenceon European chapter of the Association for Computa-tional Linguistics, pages 19?26, Budapest, Hungary.Joan Bresnan.
2001.
Lexical-Functional Syntax.
Black-well, Oxford.Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-Xshared task on multilingual dependency parsing.
InProceedings of the Tenth Conference on Computa-tional Language Learning (CoNLL), pages 149?164,New York, NY.Marie Candito and Djame?
Seddah.
2010.
Parsing wordclusters.
In Proceedings of the NAACL/HLT Workshopon Statistical Parsing of Morphologically Rich Lan-guages (SPMRL 2010), Los Angeles, CA.Xavier Carreras, Michael Collins, and Terry Koo.
2008.TAG, dynamic programming, and the perceptron forefficient, feature-rich parsing.
In Proceedings of theTwelfth Conference on Computational Natural Lan-guage Learning (CoNLL), pages 9?16, Manchester,UK.Eugene Charniak and Mark Johnson.
2005.
Course-to-fine n-best-parsing and maxent discriminative rerank-ing.
In Proceedings of the 43rd Annual Meeting of theACL, pages 173?180, Barcelona, Spain, June.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proceedings of the 1st Annual Meeting of theNorth American Chapter of the ACL (NAACL), Seattle.David Chiang.
2000.
Statistical parsing with anautomatically-extracted Tree Adjoining Grammar.
InProceedings of the 38th Annual Meeting on Associ-ation for Computational Linguistics, pages 456?463,Hong Kong.
Association for Computational Linguis-tics Morristown, NJ, USA.Tagyoung Chung, Matt Post, and Daniel Gildea.
2010.Factors affecting the accuracy of Korean parsing.
InProceedings of the NAACL/HLT Workshop on Sta-tistical Parsing of Morphologically Rich Languages(SPMRL 2010), Los Angeles, CA.Michael Collins, Jan Hajic?, Lance Ramshaw, andChristoph Tillmann.
1999.
A statistical parser forCzech.
In Proceedings of the 37th Annual Meetingof the ACL, volume 37, pages 505?512, College Park,MD.Michael Collins.
1997.
Three Generative, LexicalizedModels for Statistical Parsing.
In Proceedings of the35th Annual Meeting of the Association for Computa-tional Linguistics, pages 16?23, Madrid, Spain.Anna Corazza, Alberto Lavelli, Giogio Satta, andRoberto Zanoli.
2004.
Analyzing an Italian treebankwith state-of-the-art statistical parsers.
In Proceedingsof the Third Third Workshop on Treebanks and Lin-guistic Theories (TLT 2004), Tu?bingen, Germany.Brooke Cowan and Michael Collins.
2005.
Morphologyand reranking for the statistical parsing of Spanish.
Inin Proceedins of EMNLP.Benoit Crabbe?
and Marie Candito.
2008.
Expe?riencesd?analyse syntaxique statistique du franc?ais.
In Actesde la 15e`me Confe?rence sur le Traitement Automatiquedes Langues Naturelles (TALN?08), pages 45?54, Avi-gnon, France.Amit Dubey and Frank Keller.
2003.
Probabilistic pars-ing for German using sister-head dependencies.
In InProceedings of the 41st Annual Meeting of the Asso-ciation for Computational Linguistics, pages 96?103,Ann Arbor, MI.Amit Dubey.
2005.
What to do when lexicalization fails:parsing German with suffix analysis and smoothing.In 43rd Annual Meeting of the Association for Compu-tational Linguistics.10Jenny Rose Finkel, Alex Kleeman, and Christopher D.Manning.
2008.
Efficient, feature-based, conditionalrandom field parsing.
In Proceedings of ACL.Yoav Goldberg and Michael Elhadad.
2010.
Easy-first dependency parsing of Modern Hebrew.
In Pro-ceedings of the NAACL/HLT Workshop on StatisticalParsing of Morphologically Rich Languages (SPMRL2010), Los Angeles, CA.Yoav Goldberg and Reut Tsarfaty.
2008.
A single frame-work for joint morphological segmentation and syntac-tic parsing.
In Proceedings of the 46nd Annual Meet-ing of the Association for Computational Linguistics.Yoav Goldberg, Reut Tsarfaty, Meni Adler, and MichaelElhadad.
2009.
Enhancing unlexicalized parsing per-formance using a wide coverage lexicon, fuzzy tag-setmapping, and em-hmm-based lexical probabilities.
InProceedings of the 12th Conference of the EuropeanChapter of the Association for Computational Linguis-tics, pages 327?335.Kenneth L. Hale.
1983.
Warlpiri and the grammar ofnon-configurational languages.
Natural Language andLinguistic Theory, 1(1).Erhard W. Hinrichs, Sandra Ku?bler, and Karin Naumann.2005.
A unified representation for morphological,syntactic, semantic, and referential annotations.
InProceedings of the ACL Workshop on Frontiers in Cor-pus Annotation II: Pie in the Sky, pages 13?20, AnnArbor, MI.Liang Huang.
2008.
Forest reranking: Discriminativeparsing with non-local features.
In Proceedings ofACL.Dan Klein and Christopher D. Manning.
2003.
Accurateunlexicalized parsing.
In Proceedings of ACL, pages423?430.Sandra Ku?bler, Erhard W. Hinrichs, and Wolfgang Maier.2006.
Is it really that difficult to parse German?In Proceedings of the 2006 Conference on EmpiricalMethods in Natural Language Processing, pages 111?119, Sydney, Australia, July.
Association for Compu-tational Linguistics.Sandra Ku?bler.
2008.
The PaGe 2008 shared task onparsing German.
In Proceedings of the Workshop onParsing German, pages 55?63.
Association for Com-putational Linguistics.Seth Kulick, Ryan Gabbard, and Mitchell Marcus.
2006.Parsing the Arabic treebank: Analysis and improve-ments.
In Proceedings of TLT.Dekang Lin.
1995.
A dependency-based method forevaluating broad-coverage parsers.
In InternationalJoint Conference on Artificial Intelligence, pages1420?1425, Montreal.Mohamed Maamouri, Ann Bies, Tim Buckwalter, andWigdan Mekki.
2004.
The Penn Arabic treebank:Building a large-scale annotated Arabic corpus.
InProceedings of NEMLAR International Conference onArabic Language Resources and Tools.Mohamed Maamouri, Ann Bies, and Seth Kulick.
2008.Enhanced annotation and parsing of the Arabic tree-bank.
In Proceedings of INFOS.David M. Magerman.
1995.
Statistical decision-treemodels for parsing.
In Proceedings of the 33rd An-nual Meeting on Association for Computational Lin-guistics, pages 276?283, Cambridge, MA.Wolfgang Maier.
2010.
Direct parsing of discontin-uous constituents in german.
In Proceedings of theNAACL/HLT Workshop on Statistical Parsing of Mor-phologically Rich Languages (SPMRL 2010), Los An-geles, CA.Mitchell Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: The Penn Treebank.
ComputationalLinguistics, 19(2):313?330.Yuval Marton, Nizar Habash, and Owen Rambow.
2010.Improving Arabic dependency parsing with lexical andinflectional morphological features.
In Proceedings ofthe NAACL/HLT Workshop on Statistical Parsing ofMorphologically Rich Languages (SPMRL 2010), LosAngeles, CA.Ryan T. McDonald and Fernando C. N. Pereira.
2006.Online learning of approximate dependency parsingalgorithms.
In Proc.
of EACL?06.Ryan T. McDonald, Koby Crammer, and Fernando C. N.Pereira.
2005.
Online large-margin training of depen-dency parsers.
In Proc.
of ACL?05, Ann Arbor, USA.Igor Mel?c?uk.
1988.
Dependency Syntax: Theory andPractice.
State University of New York Press.Marianne Mithun.
1992.
Is basic word order universal?In Doris L. Payne, editor, Pragmatics of Word OrderFlexibility.
John Benjamins, Amsterdam.Joakim Nivre and Jens Nilsson.
2005.
Pseudo-projectivedependency parsing.
In Proceedings of the 43rd An-nual Meeting of the Association for ComputationalLinguistics (ACL), Ann Arbor, MI.Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-Donald, Jens Nilsson, Sebastian Riedel, and DenizYuret.
2007a.
The CoNLL 2007 shared task on depen-dency parsing.
In Proceedings of the CoNLL SharedTask Session of EMNLP-CoNLL 2007, pages 915?932,Prague, Czech Republic, June.
Association for Com-putational Linguistics.Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.2007b.
The CoNLL 2007 shared task on dependencyparsing.
In Proceedings of the CoNLL Shared TaskSession of EMNLP-CoNLL 2007, pages 915?932.11Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,Gu?ls?en Eryig?it, Sandra Ku?bler, Svetoslav Marinov,and Erwin Marsi.
2007c.
MaltParser: A language-independent system for data-driven dependency pars-ing.
Natural Language Engineering, 13(2):95?135.Joakim Nivre.
2009.
Non-projective dependency parsingin expected linear time.
In Proceedings of the JointConference of the 47th Annual Meeting of the ACL andthe 4th International Joint Conference on Natural Lan-guage Processing of the AFNLP.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In Human Language Tech-nologies 2007: The Conference of the North AmericanChapter of the Association for Computational Linguis-tics; Proceedings of the Main Conference.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In Proceedings of the 21st In-ternational Conference on Computational Linguisticsand 44th Annual Meeting of the Association for Com-putational Linguistics, Sydney, Australia, July.
Asso-ciation for Computational Linguistics.Ines Rehbein and Josef van Genabith.
2007.
Treebankannotation schemes and parser evaluation for German.In Proceedings of the 2007 Joint Conference on Em-pirical Methods in Natural Language Processing andComputational Natural Language Learning (EMNLP-CoNLL), Prague, Czech Republic.Geoffrey Sampson and Anna Babarczy.
2003.
A test ofthe leaf-ancestor metric for parse accuracy.
NaturalLanguage Engineering, 9(04):365?380.Natalie Schluter and Josef van Genabith.
2007.
Prepar-ing, restructuring, and augmenting a French Treebank:Lexicalised parsers or coherent treebanks?
In Proc.
ofPACLING 07, Melbourne, Australia.Djame?
Seddah, Marie Candito, and Benoit Crabbe?.
2009.Cross parser evaluation and tagset variation: A FrenchTreebank study.
In Proceedings of the 11th Interna-tion Conference on Parsing Technologies (IWPT?09),pages 150?161, Paris, France, October.
Associationfor Computational Linguistics.Djame?
Seddah, Grzegorz Chrupa?a, Ozlem Cetinoglu,Josef van Genabith, and Marie Candito.
2010.Lemmatization and statistical lexicalized parsing ofmorphologically-rich languages.
In Proceedings of theNAACL/HLT Workshop on Statistical Parsing of Mor-phologically Rich Languages (SPMRL 2010), Los An-geles, CA.Wojciech Skut, Thorsten Brants, Brigitte Krenn, andHans Uszkoreit.
1998.
A linguistically interpretedcorpus of German newspaper texts.
In ESSLLIWorkshop on Recent Advances in Corpus Annotation,Saarbru?cken, Germany.Reut Tsarfaty and Khalil Sima?an.
2007.
Three-dimensional parametrization for parsing morphologi-cally rich languages.
In Proceedings of the 10th Inter-national Conference on Parsing Technologies (IWPT),pages 156?167.Reut Tsarfaty and Khalil Sima?an.
2008.
Relational-Realizational parsing.
In Proceedings of the 22nd In-ternational Conference on Computational Linguistics,pages 889?896.Reut Tsarfaty and Khalil Sima?an.
2010.
Model-ing morphosyntactic agreement in constituency-basedparsing of Modern Hebrew.
In Proceedings of theNAACL/HLT Workshop on Statistical Parsing of Mor-phologically Rich Languages (SPMRL 2010), Los An-geles, CA.Yannick Versley and Ines Rehbein.
2009.
Scalable dis-criminative parsing for german.
In Proceedings of the11th International Conference on Parsing Technolo-gies (IWPT?09), pages 134?137, Paris, France, Octo-ber.
Association for Computational Linguistics.12
