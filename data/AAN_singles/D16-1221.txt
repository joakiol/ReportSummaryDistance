Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2066?2070,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsAn Embedding Model for Predicting Roll-Call VotesPeter E. Kraft Hirsh Jain Alexander M. RushSchool of Engineering and Applied Science, Harvard University{pkraft, hirshjain}@college.harvard.edu, srush@seas.harvard.eduAbstractWe develop a novel embedding-based modelfor predicting legislative roll-call votes frombill text.
The model introduces multidimen-sional ideal vectors for legislators as an alter-native to single dimensional ideal point mod-els for quantitatively analyzing roll-call data.These vectors are learned to correspond withpre-trained word embeddings which allows usto analyze which features in a bill text are mostpredictive of political support.
Our model isquite simple, while at the same time allowingus to successfully predict legislator votes onspecific bills with higher accuracy than pastmethods.1 IntroductionQuantitative analysis of political data can contributeto our understanding of governments.
One impor-tant source of such data is roll-call votes, recordsof how legislators vote on bills.
Analysis of roll-calldata can reveal interesting information about legisla-tors (such as political leanings and ideological clus-ters) and can also allow prediction of future votes(Clinton, 2012).Previous work on analyzing roll-call votes haschiefly involved positioning congresspeople on idealpoint models.
Ideal point models assume all legisla-tors and bills can be plotted as single points in one-dimensional ?political space.?
The closer a particu-lar bill?s position is to a particular congressperson?s,the more utility the congressperson is expected toderive from the bill.
Initial work on ideal pointmodels focused on using them to test theories aboutlegislative behavior, such as predicting that the rel-ative differences between ideal points of congress-people of different parties, and thus party polar-ization, would increase over time (McCarty, 2001).Ideal point models are often created using Bayesiantechniques over large amounts of roll-call data (Clin-ton et al, 2004; Jackman, 2001).
However, thesemodels are not used to make predictions.
They aretrained using the complete vote matrix for the bill,which indicates how each congressperson voted oneach bill.
Therefore, they cannot say anything abouthow congresspeople will vote on a new bill, as untilsome congresspeople have voted on the bill its idealpoint is not known.We target this vote prediction problem: given thetext of a bill and a congressperson, can we indepen-dently predict how each congressperson will vote onthe bill?
The first prior attempt at this task was madeby Gerrish and Blei (2011) who create an ideal pointtopic model which integrates a topic model similar toLDA for the bill text with an ideal point model forthe congresspeople.
They use variational inferenceto approximate the posterior distribution of the top-ics and ideal points, predicting with a linear model.Gerrish and Blei (2012) further extend this workwith an issue-adjusted model, a similar model thatmodifies congressperson ideal points based on top-ics identified with labeled LDA, but which cannotbe used for predictions.
Further work in a similarvein includes Wang et al (2013), who introducedtemporal information to a graphical model for pre-dicting Congressional votes, and Kim et al (2014),who used sparse factor analysis to estimate Senato-rial ideal points from bill text and the votes of partyleadership.In this work we revisit this task with a simplebilinear model that learns multidimensional embed-dings for both legislators and bills, combining them2066to make vote predictions.
We represent a bill asthe average of its word embeddings.
We representlegislators as ideal vectors, trained end-to-end forvote prediction.
These ideal vectors serve as a use-ful, easy-to-train, multidimensional representationof legislator ideology that does not rely on elaboratestatistical models or any further assumptions aboutlegislator behavior.
Finally, we train our model byoptimizing a cross-entropy objective instead of theposterior of a topic model.
The final model achieveshigh accuracy at predicting roll-call votes.2 ModelOur goal is to predict roll-call votes by learning fromthe texts of bills and from past votes.
Our input con-sists of a congressperson c and the set B of uniquewords in a bill.
Our output y is whether that the con-gressperson voted yea or nay on the bill.
We trainon the full set of congressional votes on a number ofbills.
At test time, we supply entirely new bills andpredict how each congressperson will vote on eachnew bill.We propose a simple bilinear model that useslow-dimensional embeddings to model each word inour dictionary and each congressperson.
We rep-resent each bill using its word embeddings in or-der to capture the multivariate relationships betweenwords and their meanings (Collobert et al, 2011;Mikolov et al, 2013).
The model is trained tosynthesize information about each congressperson?svoting record into a multidimensional ideal vector.At test time, the model combines the embedding rep-resentation of a new bill with the trained ideal vectorof a congressperson and generates a prediction forhow the congressperson will vote on the bill.Let ew ?
Rdword be the pretrained embedding fora word w. We initialize to the GloVe embeddingswith dword = 50 (Pennington et al, 2014), thenjointly train them with the model.
To represent abill, we average over the embeddings of the set B ofwords in the bill.To represent a congressperson, we introduce an-other set of embeddings vc ?
Rdemb for each con-gressperson c. The embeddings act as the ideal vec-tor for each legislator.
Unlike the word embeddings,we initialize these randomly.The full model takes in a bill and a congressper-Congress # Bills House Senate Pres106 557 R R Clinton107 505 R D2 Bush108 607 R R Bush109 579 R R Bush110 854 D D Bush111 965 D D ObamaTable 1: Dataset details for 106-111th Congress.son.
It applies an affine transformation, representedby a matrix W ?
Rdemb?dword and bias b ?
Rdemb ,to map the bill representation into the space of theideal vectors, and then uses a dot-product to providea yea/nay score.p(y = yea|B, c) = ?
((W(?w?Bew/|B|)+b) ?vc)The full model is simply trained to minimize thenegative log-likelihood of the training set, and re-quires no additional meta-information (such as partyaffiliation) or additional preprocessing of the billsduring training- or test-time.3 Experimental SetupData Following past work, our dataset is derivedfrom the Govtrack database.1 Specifically, ourdataset consists of all votes on the full-text (notamendments) of bills or resolutions from the 106th-111th Congress, six of the most recent Congressesfor which bill texts are readily available.
Details ofeach these congresses are shown in Table 1.To create our dataset, we first find a list of all voteson the full text of bills, and create a matrix of howeach congressperson voted on each bill, which willbe used in training and in testing.
In accordance withprevious work, we only consider yes-or-no votes andomit abstentions and ?present?
votes (Gerrish andBlei, 2011).
We then simply collect the set of wordsused in each bill.
Overall, our dataset consists of4067 bills and over a million unique yes-or-no votes.1https://www.govtrack.us/2Mostly.
Republicans controlled the 107th Senate for fivemonths between the inauguration of Dick Cheney as vice-president in January of 2001 and the defection of Jim Jeffordsin June.2067Congress YEA GB IDP EMB106 83.0 - 79.5 84.9107 85.9 - 85.8 89.7108 87.1 - 85.0 91.9109 83.5 - 81.5 88.4110 82.7 - 80.8 92.1111 86.0 - 85.7 93.4Avg 84.5 89.0 83.1 90.6Table 2: Main results comparing predictive accu-racy of our model EMB with a several baselines (de-scribed in the text) on the 106th-111th Congress.Model We tested prediction accuracy of theaverage-of-embeddings model, EMB, by running itfor ten epochs at a learning rate of ?
= 0.1 and dembset to 10.
Hyperparameters were tuned on a held-out section of the 107th Congress.
We ran on eachof the 106th to 111th Congresses individually usingfive-fold cross-validation.Baselines We compare our results to three differ-ent baselines.
The first, YEA, is a majority classbaseline which assumes all legislators vote yea.The second, IDP, is our model with demb set to 1to simulate a simple ideal point model.
The third,GB, is Gerrish and Blei?s reported predictive accu-racy of 89 % on average from the 106th to 111thCongresses, which is to the extent of our knowledgethe best predictive accuracy on roll-call votes yetachieved in the literature.
Gerrish and Blei reporton the same data set using cross-validation and likeus train and test on each congress individually, butdo not split out results into individual congresses.4 Experiments and AnalysisPredictive Results The main predictive experi-mental results are shown in Table 2.
We see thatEMB performs substantially better than YEA on allsix Congresses.
It has a weighted average of 90.6%on an 84.5% baseline, compared to Gerrish andBlei?s 89% on an identical dataset.
IDP, however,actually does worse than the baseline, demonstrat-ing that the bulk of our gain in prediction accu-racy comes from using ideal vectors instead of idealpoints.
To further test this hypothesis, we experi-mented with replacing word embeddings with LDACongress EMB106 0.524107 0.546108 0.595109 0.628110 0.728111 0.737Avg 0.645Table 3: Minority class F1 Scores of our model EMBon the 106th-111th Congress.and obtained an accuracy of 89.5%, in between GBand EMB.
This indicates that the word embeddingsare also responsible for part, but not all, of the accu-racy improvement.
We also report minority class F1scores for EMB in Table 3, finding an overall averageF1 score of 0.645.Ideal Vectors Beyond predictive accuracy, one ofthe most interesting features of the model is that itproduces ideal vectors as its complete representa-tion of congresspeople.
These vectors are much eas-ier to compute than standard ideal points, which re-quire relatively complex and computationally inten-sive statistical models (Jackman, 2001).
Addition-ally unlike ideal point models, which tend to containmany assumptions about legislative behavior, idealvectors arise naturally from raw data and bill text(Clinton et al, 2004).In Figure 1, we show the ideal vectors for the111th Congress.
We use PCA to project the vec-tors down to two dimensions.
This graph displaysseveral interesting patterns in agreement with theo-ries of legislative behavior.
For example, politicalscientists theorize that the majority party in a legis-lature will display more unity in roll-call votes be-cause they decide what gets voted on and only allowa vote on a bill if they can unify behind it and passit, while that bill may divide the other party (Car-rubba et al, 2006; Carrubba et al, 2008).
On thisgraph, in accordance with that prediction, the ma-jority Democrats are more clustered than the minor-ity Republicans.
We observe similar trends in theideal vectors of the other Congresses.
Moreover,the model lets us examine the positions of individualcongresspeople.
In the figure, the 34 Democrats who2068Figure 1: PCA projection of the ideal vectors for111th Congress, both House and Senate.
Republi-cans shown in red, Democrats who voted for Afford-able Care Act (ACA) in blue, Democrats who votedagainst ACA in yellow, and independents in green.voted against the Affordable Care Act (ACA, bet-ter known as Obamacare) are shown in yellow.
TheACA was a major Democratic priority and point ofdifference between the two parties.
The Democratswho voted against it tended to be relatively conser-vative and closer to the Republicans.
The modelpicks up on this distinction.Furthermore, since our model maps individualwords and congresspeople to the same vector space,we can use it to determine how words (and by proxyissues) unite or divide congresspeople and parties.In Figure 2, we show the scaled probabilities thatcongresspeople will vote for a bill containing onlythe word ?enterprise?
versus one containing onlythe word ?science?
in the 110th Congress.
Theword ?enterprise,?
denoting pro-business legislation,neatly divides the parties.
Both are for it, but Repub-licans favor it more.
More interestingly, the word?science?
creates division within the parties, as nei-ther was at the time more for science funding thanthe other but both contained congresspeople withvarying levels of support for it.
An ideal point modelwould likely capture the ?enterprise?
dimension, butnot the ?science?
one, and would not be able to dis-tinguish between libertarians like Ron Paul (R-TX)who are against both ?corporate welfare?
and gov-ernment science funding, conservative budget hawkslike Jeff Flake (R-AZ) who favor business but areskeptical of government funding of science, and es-Figure 2: Relative likelihood of congresspeople inthe 110th Congress voting for a bill containing onlythe word ?Enterprise?
versus only the word ?Sci-ence.?
Coordinates are sigmoids of dot products ofcongressperson vectors with normalized word vec-tors.tablishment Republicans like Kevin McCarthy (R-CA) who support both.
Indeed, ideal point modelsare known to perform poorly at describing ideolog-ically idiosyncratic figures like Ron Paul (Gerrishand Blei, 2011).
Providing the ability to exploremultiple dimensions of difference between legisla-tors will be extremely helpful for political scientistsanalyzing the dynamics of legislatures.Lexical Properties Finally, as with topic model-ing approaches, we can use our model to analyzethe relationships between congresspeople or partiesand individual words in bills.
For example, Ta-ble 4 shows the ten words closest by cosine simi-larity to each party?s average congressperson (stopwords omitted) for the 110th Congress.
The Demo-cratic list mostly contains words relating to govern-ing and regulating, such as ?consumer,?
?state,?
and?educational,?
likely because the Democrats wereat the time the majority party with the responsibil-ity for passing large governmental and regulatorybills like budgets.
The Republican list is largelyconcerned with the military, with words like ?vet-erans,?
?service,?
and ?executive,?
probably becauseof the importance at the time of the wars in Iraq andAfghanistan, started by a Republican president.2069Democrats Republicanseconomic veteransexchange headstate opportunitiescarrying providedgovernment promotehigher servicecongress identifiedconsumer informationeducational recordspecial executiveTable 4: Top ten words by cosine similarity for eachparty in the 110th Congress with stop words re-moved.5 ConclusionWe have developed a novel model for predictingCongressional roll-call votes.
This new modelprovides a new and interesting way of analyz-ing the behavior of parties and legislatures.
Itachieves predictive accuracies around 90.6% on av-erage and outperforms any prior model of roll-callvoting.
We also introduce the idea of ideal vec-tors as a fast, simple, and multidimensional al-ternative to ideal point models for analyzing theactions of individual legislators and testing theo-ries about their behavior.
Our code and datasetsare available online at https://github.com/kraftp/roll_call_predictor.ReferencesClifford J Carrubba, Matthew Gabel, Lacey Murrah,Ryan Clough, Elizabeth Montgomery, and RebeccaSchambach.
2006.
Off the record: Unrecorded leg-islative votes, selection bias and roll-call vote analysis.British Journal of Political Science, 36(04):691?704.Clifford Carrubba, Matthew Gabel, and Simon Hug.2008.
Legislative voting behavior, seen and unseen:A theory of roll-call vote selection.
Legislative Stud-ies Quarterly, 33(4):543?572.Joshua Clinton, Simon Jackman, and Douglas Rivers.2004.
The statistical analysis of roll call data.
Ameri-can Political Science Review, 98(02):355?370.Joshua D Clinton.
2012.
Using roll call estimates to testmodels of politics.
Annual Review of Political Science,15:79?99.Ronan Collobert, Jason Weston, Le?on Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011.Natural language processing (almost) from scratch.The Journal of Machine Learning Research, 12:2493?2537.Sean Gerrish and David M Blei.
2011.
Predicting leg-islative roll calls from text.
In Proceedings of the 28thinternational conference on machine learning (icml-11), pages 489?496.Sean Gerrish and David M Blei.
2012.
How theyvote: Issue-adjusted models of legislative behavior.
InAdvances in Neural Information Processing Systems,pages 2753?2761.Simon Jackman.
2001.
Multidimensional analysis ofroll call data via bayesian simulation: Identification,estimation, inference, and model checking.
PoliticalAnalysis, 9(3):227?241.In Song Kim, John Londregan, and Marc Ratkovic.
2014.Voting, speechmaking, and the dimensions of conflictin the us senate.
In Annual Meeting of the MidwestPolitical Science Association.Nolan McCarty.
2001.
The hunt for party discipline incongress.
In American Political Science Association,volume 95, pages 673?687.
Cambridge Univ Press.Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.2013.
Linguistic regularities in continuous space wordrepresentations.
In HLT-NAACL, pages 746?751.Jeffrey Pennington, Richard Socher, and Christopher DManning.
2014.
Glove: Global vectors for word rep-resentation.
In EMNLP, volume 14, pages 1532?1543.Eric Wang, Esther Salazar, David Dunson, LawrenceCarin, et al 2013.
Spatio-temporal modeling of legis-lation and votes.
Bayesian Analysis, 8(1):233?268.2070
