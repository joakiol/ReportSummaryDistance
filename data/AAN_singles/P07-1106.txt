Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 840?847,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsChinese Segmentation with a Word-Based Perceptron AlgorithmYue Zhang and Stephen ClarkOxford University Computing LaboratoryWolfson Building, Parks RoadOxford OX1 3QD, UK{yue.zhang,stephen.clark}@comlab.ox.ac.ukAbstractStandard approaches to Chinese word seg-mentation treat the problem as a taggingtask, assigning labels to the characters inthe sequence indicating whether the char-acter marks a word boundary.
Discrimina-tively trained models based on local char-acter features are used to make the taggingdecisions, with Viterbi decoding finding thehighest scoring segmentation.
In this paperwe propose an alternative, word-based seg-mentor, which uses features based on com-plete words and word sequences.
The gener-alized perceptron algorithm is used for dis-criminative training, and we use a beam-search decoder.
Closed tests on the first andsecond SIGHAN bakeoffs show that our sys-tem is competitive with the best in the litera-ture, achieving the highest reported F-scoresfor a number of corpora.1 IntroductionWords are the basic units to process for most NLPtasks.
The problem of Chinese word segmentation(CWS) is to find these basic units for a given sen-tence, which is written as a continuous sequence ofcharacters.
It is the initial step for most Chinese pro-cessing applications.Chinese character sequences are ambiguous, of-ten requiring knowledge from a variety of sourcesfor disambiguation.
Out-of-vocabulary (OOV) wordsare a major source of ambiguity.
For example, adifficult case occurs when an OOV word consistsof characters which have themselves been seen aswords; here an automatic segmentor may split theOOV word into individual single-character words.Typical examples of unseen words include Chinesenames, translated foreign names and idioms.The segmentation of known words can also beambiguous.
For example, ???b?
should be ???
(here)b (flour)?
in the sentence ???b?s?5?
(flour and rice are expensive here) or ??
(here)?b (inside)?
in the sentence ???b???
(it?scold inside here).
The ambiguity can be resolvedwith information about the neighboring words.
Incomparison, for the sentences ?=??
?,possible segmentations include ?= (the discus-sion)  (will) ?
(very) ?
(be successful)?
and?= (the discussion meeting)?
(very)?
(besuccessful)?.
The ambiguity can only be resolvedwith contextual information outside the sentence.Human readers often use semantics, contextual in-formation about the document and world knowledgeto resolve segmentation ambiguities.There is no fixed standard for Chinese word seg-mentation.
Experiments have shown that there isonly about 75% agreement among native speakersregarding the correct word segmentation (Sproat etal., 1996).
Also, specific NLP tasks may require dif-ferent segmentation criteria.
For example, ???L?
could be treated as a single word (Bank of Bei-jing) for machine translation, while it is more natu-rally segmented into ??
(Beijing) ?L (bank)?for tasks such as text-to-speech synthesis.
There-fore, supervised learning with specifically definedtraining data has become the dominant approach.Following Xue (2003), the standard approach to840supervised learning for CWS is to treat it as a taggingtask.
Tags are assigned to each character in the sen-tence, indicating whether the character is a single-character word or the start, middle or end of a multi-character word.
The features are usually confined toa five-character window with the current characterin the middle.
In this way, dynamic programmingalgorithms such as the Viterbi algorithm can be usedfor decoding.Several discriminatively trained models have re-cently been applied to the CWS problem.
Exam-ples include Xue (2003), Peng et al (2004) and Shiand Wang (2007); these use maximum entropy (ME)and conditional random field (CRF) models (Ratna-parkhi, 1998; Lafferty et al, 2001).
An advantageof these models is their flexibility in allowing knowl-edge from various sources to be encoded as features.Contextual information plays an important role inword segmentation decisions; especially useful is in-formation about surrounding words.
Consider thesentence ?-??, which can be from ?v-(among which) ? (foreign)  (companies)?,or ?-?
(in China)  (foreign companies) ?
(business)?.
Note that the five-character windowsurrounding ??
is the same in both cases, makingthe tagging decision for that character difficult giventhe local window.
However, the correct decision canbe made by comparison of the two three-word win-dows containing this character.In order to explore the potential of word-basedmodels, we adapt the perceptron discriminativelearning algorithm to the CWS problem.
Collins(2002) proposed the perceptron as an alternative tothe CRF method for HMM-style taggers.
However,our model does not map the segmentation problemto a tag sequence learning problem, but defines fea-tures on segmented sentences directly.
Hence weuse a beam-search decoder during training and test-ing; our idea is similar to that of Collins and Roark(2004) who used a beam-search decoder as part ofa perceptron parsing model.
Our work can also beseen as part of the recent move towards search-basedlearning methods which do not rely on dynamic pro-gramming and are thus able to exploit larger parts ofthe context for making decisions (Daume III, 2006).We study several factors that influence the per-formance of the perceptron word segmentor, includ-ing the averaged perceptron method, the size of thebeam and the importance of word-based features.We compare the accuracy of our final system to thestate-of-the-art CWS systems in the literature usingthe first and second SIGHAN bakeoff data.
Our sys-tem is competitive with the best systems, obtainingthe highest reported F-scores on a number of thebakeoff corpora.
These results demonstrate the im-portance of word-based features for CWS.
Further-more, our approach provides an example of the po-tential of search-based discriminative training meth-ods for NLP tasks.2 The Perceptron Training AlgorithmWe formulate the CWS problem as finding a mappingfrom an input sentence x ?
X to an output sentencey ?
Y , where X is the set of possible raw sentencesand Y is the set of possible segmented sentences.Given an input sentence x, the correct output seg-mentation F (x) satisfies:F (x) = argmaxy?GEN(x)Score(y)where GEN(x) denotes the set of possible segmen-tations for an input sentence x, consistent with nota-tion from Collins (2002).The score for a segmented sentence is computedby first mapping it into a set of features.
A featureis an indicator of the occurrence of a certain patternin a segmented sentence.
For example, it can be theoccurrence of ??b?
as a single word, or the occur-rence of ???
separated from ?b?
in two adjacentwords.
By defining features, a segmented sentenceis mapped into a global feature vector, in which eachdimension represents the count of a particular fea-ture in the sentence.
The term ?global?
feature vec-tor is used by Collins (2002) to distinguish betweenfeature count vectors for whole sequences and the?local?
feature vectors in ME tagging models, whichare Boolean valued vectors containing the indicatorfeatures for one element in the sequence.Denote the global feature vector for segmentedsentence y with ?
(y) ?
Rd, where d is the totalnumber of features in the model; then Score(y) iscomputed by the dot product of vector ?
(y) and aparameter vector ?
?
Rd, where ?i is the weight forthe ith feature:Score(y) = ?
(y) ?
?841Inputs: training examples (xi, yi)Initialization: set ?
= 0Algorithm:for t = 1..T , i = 1..Ncalculate zi = argmaxy?GEN(xi) ?
(y) ?
?if zi 6= yi?
= ?
+ ?(yi)?
?
(zi)Outputs: ?Figure 1: the perceptron learning algorithm, adaptedfrom Collins (2002)The perceptron training algorithm is used to deter-mine the weight values ?.The training algorithm initializes the parametervector as all zeros, and updates the vector by decod-ing the training examples.
Each training sentenceis turned into the raw input form, and then decodedwith the current parameter vector.
The output seg-mented sentence is compared with the original train-ing example.
If the output is incorrect, the parametervector is updated by adding the global feature vectorof the training example and subtracting the globalfeature vector of the decoder output.
The algorithmcan perform multiple passes over the same trainingsentences.
Figure 1 gives the algorithm, where N isthe number of training sentences and T is the num-ber of passes over the data.Note that the algorithm from Collins (2002) wasdesigned for discriminatively training an HMM-styletagger.
Features are extracted from an input se-quence x and its corresponding tag sequence y:Score(x, y) = ?
(x, y) ?
?Our algorithm is not based on an HMM.
For a giveninput sequence x, even the length of different candi-dates y (the number of words) is not fixed.
Becausethe output sequence y (the segmented sentence) con-tains all the information from the input sequence x(the raw sentence), the global feature vector ?
(x, y)is replaced with ?
(y), which is extracted from thecandidate segmented sentences directly.Despite the above differences, since the theoremsof convergence and their proof (Collins, 2002) areonly dependent on the feature vectors, and not onthe source of the feature definitions, the perceptronalgorithm is applicable to the training of our CWSmodel.2.1 The averaged perceptronThe averaged perceptron algorithm (Collins, 2002)was proposed as a way of reducing overfitting onthe training data.
It was motivated by the voted-perceptron algorithm (Freund and Schapire, 1999)and has been shown to give improved accuracy overthe non-averaged perceptron on a number of tasks.Let N be the number of training sentences, T thenumber of training iterations, and ?n,t the parame-ter vector immediately after the nth sentence in thetth iteration.
The averaged parameter vector ?
?
Rdis defined as:?
= 1NT?n=1..N,t=1..T?n,tTo compute the averaged parameters ?, the train-ing algorithm in Figure 1 can be modified by keep-ing a total parameter vector ?n,t = ?
?n,t, which isupdated using ?
after each training example.
Afterthe final iteration, ?
is computed as ?n,t/NT .
In theaveraged perceptron algorithm, ?
is used instead of?
as the final parameter vector.With a large number of features, calculating thetotal parameter vector ?n,t after each training exam-ple is expensive.
Since the number of changed di-mensions in the parameter vector ?
after each train-ing example is a small proportion of the total vec-tor, we use a lazy update optimization for the train-ing process.1 Define an update vector ?
to recordthe number of the training sentence n and iterationt when each dimension of the averaged parametervector was last updated.
Then after each trainingsentence is processed, only update the dimensionsof the total parameter vector corresponding to thefeatures in the sentence.
(Except for the last exam-ple in the last iteration, when each dimension of ?is updated, no matter whether the decoder output iscorrect or not).Denote the sth dimension in each vector beforeprocessing the nth example in the tth iteration as?n?1,ts , ?n?1,ts and ?n?1,ts = (n?,s, t?,s).
Supposethat the decoder output zn,t is different from thetraining example yn.
Now ?n,ts , ?n,ts and ?n,ts can1Daume III (2006) describes a similar algorithm.842be updated in the following way:?n,ts = ?n?1,ts + ?n?1,ts ?
(tN+n?t?,sN?
n?,s)?n,ts = ?n?1,ts + ?(yn)?
?
(zn,t)?n,ts = ?n,ts + ?(yn)?
?
(zn,t)?n,ts = (n, t)We found that this lazy update method was signif-icantly faster than the naive method.3 The Beam-Search DecoderThe decoder reads characters from the input sen-tence one at a time, and generates candidate seg-mentations incrementally.
At each stage, the next in-coming character is combined with an existing can-didate in two different ways to generate new candi-dates: it is either appended to the last word in thecandidate, or taken as the start of a new word.
Thismethod guarantees exhaustive generation of possiblesegmentations for any input sentence.Two agendas are used: the source agenda and thetarget agenda.
Initially the source agenda containsan empty sentence and the target agenda is empty.At each processing stage, the decoder reads in acharacter from the input sentence, combines it witheach candidate in the source agenda and puts thegenerated candidates onto the target agenda.
Aftereach character is processed, the items in the targetagenda are copied to the source agenda, and then thetarget agenda is cleaned, so that the newly generatedcandidates can be combined with the next incom-ing character to generate new candidates.
After thelast character is processed, the decoder returns thecandidate with the best score in the source agenda.Figure 2 gives the decoding algorithm.For a sentence with length l, there are 2l?1 differ-ent possible segmentations.
To guarantee reasonablerunning speed, the size of the target agenda is lim-ited, keeping only the B best candidates.4 Feature templatesThe feature templates are shown in Table 1.
Features1 and 2 contain only word information, 3 to 5 con-tain character and length information, 6 and 7 con-tain only character information, 8 to 12 contain wordand character information, while 13 and 14 containInput: raw sentence sent ?
a list of charactersInitialization: set agendas src = [[]], tgt = []Variables: candidate sentence item ?
a list of wordsAlgorithm:for index = 0..sent.length?1:var char = sent[index]foreach item in src:// append as a new word to the candidatevar item1 = itemitem1.append(char.toWord())tgt.insert(item1)// append the character to the last wordif item.length > 1:var item2 = itemitem2[item2.length?1].append(char)tgt.insert(item2)src = tgttgt = []Outputs: src.best itemFigure 2: The decoding algorithmword and length information.
Any segmented sen-tence is mapped to a global feature vector accordingto these templates.
There are 356, 337 features withnon-zero values after 6 training iterations using thedevelopment data.For this particular feature set, the longest rangefeatures are word bigrams.
Therefore, among partialcandidates ending with the same bigram, the bestone will also be in the best final candidate.
Thedecoder can be optimized accordingly: when an in-coming character is combined with candidate itemsas a new word, only the best candidate is kept amongthose having the same last word.5 Comparison with Previous WorkAmong the character-tagging CWS models, Li et al(2005) uses an uneven margin alteration of the tradi-tional perceptron classifier (Li et al, 2002).
Eachcharacter is classified independently, using infor-mation in the neighboring five-character window.Liang (2005) uses the discriminative perceptron al-gorithm (Collins, 2002) to score whole character tagsequences, finding the best candidate by the globalscore.
It can be seen as an alternative to the ME andCRF models (Xue, 2003; Peng et al, 2004), which8431 word w2 word bigram w1w23 single-character word w4 a word starting with character c and havinglength l5 a word ending with character c and havinglength l6 space-separated characters c1 and c27 character bigram c1c2 in any word8 the first and last characters c1 and c2 of anyword9 word w immediately before character c10 character c immediately before word w11 the starting characters c1 and c2 of two con-secutive words12 the ending characters c1 and c2 of two con-secutive words13 a word of length l and the previous word w14 a word of length l and the next word wTable 1: feature templatesdo not involve word information.
Wang et al (2006)incorporates an N-gram language model in ME tag-ging, making use of word information to improvethe character tagging model.
The key difference be-tween our model and the above models is the word-based nature of our system.One existing method that is based on sub-word in-formation, Zhang et al (2006), combines a CRF anda rule-based model.
Unlike the character-taggingmodels, the CRF submodel assigns tags to sub-words, which include single-character words andthe most frequent multiple-character words from thetraining corpus.
Thus it can be seen as a step towardsa word-based model.
However, sub-words do notnecessarily contain full word information.
More-over, sub-word extraction is performed separatelyfrom feature extraction.
Another difference fromour model is the rule-based submodel, which uses adictionary-based forward maximum match methoddescribed by Sproat et al (1996).6 ExperimentsTwo sets of experiments were conducted.
The first,used for development, was based on the part of Chi-nese Treebank 4 that is not in Chinese Treebank3 (since CTB3 was used as part of the first bake-off).
This corpus contains 240K characters (150Kwords and 4798 sentences).
80% of the sentences(3813) were randomly chosen for training and therest (985 sentences) were used as development test-ing data.
The accuracies and learning curves for thenon-averaged and averaged perceptron were com-pared.
The influence of particular features and theagenda size were also studied.The second set of experiments used training andtesting sets from the first and second internationalChinese word segmentation bakeoffs (Sproat andEmerson, 2003; Emerson, 2005).
The accuracies arecompared to other models in the literature.F-measure is used as the accuracy measure.
De-fine precision p as the percentage of words in the de-coder output that are segmented correctly, and recallr as the percentage of gold standard output wordsthat are correctly segmented by the decoder.
The(balanced) F-measure is 2pr/(p + r).CWS systems are evaluated by two types of tests.The closed tests require that the system is trainedonly with a designated training corpus.
Any extraknowledge is not allowed, including common sur-names, Chinese and Arabic numbers, European let-ters, lexicons, part-of-speech, semantics and so on.The open tests do not impose such restrictions.Open tests measure a model?s capability to utilizeextra information and domain knowledge, which canlead to improved performance, but since this extrainformation is not standardized, direct comparisonbetween open test results is less informative.In this paper, we focus only on the closed test.However, the perceptron model allows a wide rangeof features, and so future work will consider how tointegrate open resources into our system.6.1 Learning curveIn this experiment, the agenda size was set to 16, forboth training and testing.
Table 2 shows the preci-sion, recall and F-measure for the development setafter 1 to 10 training iterations, as well as the num-ber of mistakes made in each iteration.
The corre-sponding learning curves for both the non-averagedand averaged perceptron are given in Figure 3.The table shows that the number of mistakes madein each iteration decreases, reflecting the conver-gence of the learning algorithm.
The averaged per-844Iteration 1 2 3 4 5 6 7 8 9 10P (non-avg) 89.0 91.6 92.0 92.3 92.5 92.5 92.5 92.7 92.6 92.6R (non-avg) 88.3 91.4 92.2 92.6 92.7 92.8 93.0 93.0 93.1 93.2F (non-avg) 88.6 91.5 92.1 92.5 92.6 92.6 92.7 92.8 92.8 92.9P (avg) 91.7 92.8 93.1 92.2 93.1 93.2 93.2 93.2 93.2 93.2R (avg) 91.6 92.9 93.3 93.4 93.4 93.5 93.5 93.5 93.6 93.6F (avg) 91.6 92.9 93.2 93.3 93.3 93.4 93.3 93.3 93.4 93.4#Wrong sentences 3401 1652 945 621 463 288 217 176 151 139Table 2: accuracy using non-averaged and averaged perceptron.P - precision (%), R - recall (%), F - F-measure.B 2 4 8 16 32 64 128 256 512 1024Tr 660 610 683 830 1111 1645 2545 4922 9104 15598Seg 18.65 18.18 28.85 26.52 36.58 56.45 95.45 173.38 325.99 559.87F 86.90 92.95 93.33 93.38 93.25 93.29 93.19 93.07 93.24 93.34Table 3: the influence of agenda size.B - agenda size, Tr - training time (seconds), Seg - testing time (seconds), F - F-measure.0.860.870.880.890.90.910.920.930.941 2 3 4 5 6 7 8 9 10number of training iterationsF-measurenon-averagedaveragedFigure 3: learning curves of the averaged and non-averaged perceptron algorithmsceptron algorithm improves the segmentation ac-curacy at each iteration, compared with the non-averaged perceptron.
The learning curve was usedto fix the number of training iterations at 6 for theremaining experiments.6.2 The influence of agenda sizeReducing the agenda size increases the decodingspeed, but it could cause loss of accuracy by elimi-nating potentially good candidates.
The agenda sizealso affects the training time, and resulting model,since the perceptron training algorithm uses the de-coder output to adjust the model parameters.
Table 3shows the accuracies with ten different agenda sizes,each used for both training and testing.Accuracy does not increase beyond B = 16.Moreover, the accuracy is quite competitive evenwith B as low as 4.
This reflects the fact that the bestsegmentation is often within the current top few can-didates in the agenda.2 Since the training and testingtime generally increases as N increases, the agendasize is fixed to 16 for the remaining experiments.6.3 The influence of particular featuresOur CWS model is highly dependent upon word in-formation.
Most of the features in Table 1 are relatedto words.
Table 4 shows the accuracy with variousfeatures from the model removed.Among the features, vocabulary words (feature 1)and length prediction by characters (features 3 to 5)showed strong influence on the accuracy, while wordbigrams (feature 2) and special characters in them(features 11 and 12) showed comparatively weak in-fluence.2The optimization in Section 4, which has a pruning effect,was applied to this experiment.
Similar observations were madein separate experiments without such optimization.845Features F Features FAll 93.38 w/o 1 92.88w/o 2 93.36 w/o 3, 4, 5 92.72w/o 6 93.13 w/o 7 93.13w/o 8 93.14 w/o 9, 10 93.31w/o 11, 12 93.38 w/o 13, 14 93.23Table 4: the influence of features.
(F: F-measure.Feature numbers are from Table 1)6.4 Closed test on the SIGHAN bakeoffsFour training and testing corpora were used in thefirst bakeoff (Sproat and Emerson, 2003), includingthe Academia Sinica Corpus (AS), the Penn ChineseTreebank Corpus (CTB), the Hong Kong City Uni-versity Corpus (CU) and the Peking University Cor-pus (PU).
However, because the testing data fromthe Penn Chinese Treebank Corpus is currently un-available, we excluded this corpus.
The corpora areencoded in GB (PU, CTB) and BIG5 (AS, CU).
Inorder to test them consistently in our system, theyare all converted to UTF8 without loss of informa-tion.The results are shown in Table 5.
We follow theformat from Peng et al (2004).
Each row repre-sents a CWS model.
The first eight rows representmodels from Sproat and Emerson (2003) that partic-ipated in at least one closed test from the table, row?Peng?
represents the CRF model from Peng et al(2004), and the last row represents our model.
Thefirst three columns represent tests with the AS, CUand PU corpora, respectively.
The best score in eachcolumn is shown in bold.
The last two columns rep-resent the average accuracy of each model over thetests it participated in (SAV), and our average overthe same tests (OAV), respectively.
For each row thebest average is shown in bold.We achieved the best accuracy in two of the threecorpora, and better overall accuracy than the major-ity of the other models.
The average score of S10is 0.7% higher than our model, but S10 only partici-pated in the HK test.Four training and testing corpora were used inthe second bakeoff (Emerson, 2005), including theAcademia Sinica corpus (AS), the Hong Kong CityUniversity Corpus (CU), the Peking University Cor-pus (PK) and the Microsoft Research Corpus (MR).AS CU PU SAV OAVS01 93.8 90.1 95.1 93.0 95.0S04 93.9 93.9 94.0S05 94.2 89.4 91.8 95.3S06 94.5 92.4 92.4 93.1 95.0S08 90.4 93.6 92.0 94.3S09 96.1 94.6 95.4 95.3S10 94.7 94.7 94.0S12 95.9 91.6 93.8 95.6Peng 95.6 92.8 94.1 94.2 95.096.5 94.6 94.0Table 5: the accuracies over the first SIGHAN bake-off data.AS CU PK MR SAV OAVS14 94.7 94.3 95.0 96.4 95.1 95.4S15b 95.2 94.1 94.1 95.8 94.8 95.4S27 94.5 94.0 95.0 96.0 94.9 95.4Zh-a 94.7 94.6 94.5 96.4 95.1 95.4Zh-b 95.1 95.1 95.1 97.1 95.6 95.494.6 95.1 94.5 97.2Table 6: the accuracies over the second SIGHANbakeoff data.Different encodings were provided, and the UTF8data for all four corpora were used in this experi-ment.Following the format of Table 5, the results forthis bakeoff are shown in Table 6.
We chose thethree models that achieved at least one best scorein the closed tests from Emerson (2005), as well asthe sub-word-based model of Zhang et al (2006) forcomparison.
Row ?Zh-a?
and ?Zh-b?
represent thepure sub-word CRF model and the confidence-basedcombination of the CRF and rule-based models, re-spectively.Again, our model achieved better overall accu-racy than the majority of the other models.
One sys-tem to achieve comparable accuracy with our sys-tem is Zh-b, which improves upon the sub-word CRFmodel (Zh-a) by combining it with an independentdictionary-based submodel and improving the accu-racy of known words.
In comparison, our system isbased on a single perceptron model.In summary, closed tests for both the first and thesecond bakeoff showed competitive results for our846system compared with the best results in the litera-ture.
Our word-based system achieved the best F-measures over the AS (96.5%) and CU (94.6%) cor-pora in the first bakeoff, and the CU (95.1%) andMR (97.2%) corpora in the second bakeoff.7 Conclusions and Future WorkWe proposed a word-based CWS model using thediscriminative perceptron learning algorithm.
Thismodel is an alternative to the existing character-based tagging models, and allows word informationto be used as features.
One attractive feature of theperceptron training algorithm is its simplicity, con-sisting of only a decoder and a trivial update process.We use a beam-search decoder, which places ourwork in the context of recent proposals for search-based discriminative learning algorithms.
Closedtests using the first and second SIGHAN CWS bake-off data demonstrated our system to be competitivewith the best in the literature.Open features, such as knowledge of numbers andEuropean letters, and relationships from semanticnetworks (Shi and Wang, 2007), have been reportedto improve accuracy.
Therefore, given the flexibilityof the feature-based perceptron model, an obviousnext step is the study of open features in the seg-mentor.Also, we wish to explore the possibility of in-corporating POS tagging and parsing features intothe discriminative model, leading to joint decod-ing.
The advantage is two-fold: higher level syn-tactic information can be used in word segmenta-tion, while joint decoding helps to prevent bottom-up error propagation among the different processingsteps.AcknowledgementsThis work is supported by the ORS and ClarendonFund.
We thank the anonymous reviewers for theirinsightful comments.ReferencesMichael Collins and Brian Roark.
2004.
Incremental parsingwith the perceptron algorithm.
In Proceedings of ACL?04,pages 111?118, Barcelona, Spain, July.Michael Collins.
2002.
Discriminative training methods forhidden markov models: Theory and experiments with per-ceptron algorithms.
In Proceedings of EMNLP, pages 1?8,Philadelphia, USA, July.Hal Daume III.
2006.
Practical Structured Learning for Natu-ral Language Processing.
Ph.D. thesis, USC.Thomas Emerson.
2005.
The second international Chineseword segmentation bakeoff.
In Proceedings of The FourthSIGHAN Workshop, Jeju, Korea.Y.
Freund and R. Schapire.
1999.
Large margin classificationusing the perceptron algorithm.
In Machine Learning, pages277?296.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Conditionalrandom fields: Probabilistic models for segmenting and la-beling sequence data.
In Proceedings of the 18th ICML,pages 282?289, Massachusetts, USA.Y.
Li, Zaragoza, R. H., Herbrich, J. Shawe-Taylor, and J. Kan-dola.
2002.
The perceptron algorithm with uneven margins.In Proceedings of the 9th ICML, pages 379?386, Sydney,Australia.Yaoyong Li, Chuanjiang Miao, Kalina Bontcheva, and HamishCunningham.
2005.
Perceptron learning for Chinese wordsegmentation.
In Proceedings of the Fourth SIGHAN Work-shop, Jeju, Korea.Percy Liang.
2005.
Semi-supervised learning for natural lan-guage.
Master?s thesis, MIT.F.
Peng, F. Feng, , and A. McCallum.
2004.
Chinese segmenta-tion and new word detection using conditional random fields.In Proceedings of COLING, Geneva, Switzerland.Adwait Ratnaparkhi.
1998.
Maximum Entropy Models for Nat-ural Language Ambiguity Resolution.
Ph.D. thesis, UPenn.Yanxin Shi and Mengqiu Wang.
2007.
A dual-layer CRFbased joint decoding method for cascade segmentation andlabelling tasks.
In Proceedings of IJCAI, Hyderabad, India.Richard Sproat and Thomas Emerson.
2003.
The first interna-tional Chinese word segmentation bakeoff.
In Proceedingsof The Second SIGHAN Workshop, pages 282?289, Sapporo,Japan, July.R.
Sproat, C. Shih, W. Gail, and N. Chang.
1996.
A stochas-tic finite-state word-segmentation algorithm for Chinese.
InComputational Linguistics, volume 22(3), pages 377?404.Xinhao Wang, Xiaojun Lin, Dianhai Yu, Hao Tian, and XihongWu.
2006.
Chinese word segmentation with maximum en-tropy and n-gram language model.
In Proceedings of theFifth SIGHAN Workshop, pages 138?141, Sydney, Australia,July.N.
Xue.
2003.
Chinese word segmentation as character tag-ging.
In International Journal of Computational Linguisticsand Chinese Language Processing, volume 8(1).Ruiqiang Zhang, Genichiro Kikui, and Eiichiro Sumita.
2006.Subword-based tagging by conditional random fields forChinese word segmentation.
In Proceedings of the HumanLanguage Technology Conference of the NAACL, Compan-ion, volume Short Papers, pages 193?196, New York City,USA, June.847
