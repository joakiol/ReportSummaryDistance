Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1373?1383,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsHierarchical Phrase-Based Translation RepresentationsGonzalo Iglesias?
Cyril Allauzen?
William Byrne?Adria` de Gispert?
Michael Riley?
?Department of Engineering, University of Cambridge, Cambridge, CB2 1PZ, U.K.{gi212,wjb31,ad465}@eng.cam.ac.uk?
Google Research, 76 Ninth Avenue, New York, NY 10011{allauzen,riley}@google.comAbstractThis paper compares several translation rep-resentations for a synchronous context-freegrammar parse including CFGs/hypergraphs,finite-state automata (FSA), and pushdownautomata (PDA).
The representation choice isshown to determine the form and complex-ity of target LM intersection and shortest-pathalgorithms that follow.
Intersection, shortestpath, FSA expansion and RTN replacement al-gorithms are presented for PDAs.
Chinese-to-English translation experiments using HiFSTand HiPDT, FSA and PDA-based decoders,are presented using admissible (or exact)search, possible for HiFST with compactSCFG rulesets and HiPDT with compact LMs.For large rulesets with large LMs, we intro-duce a two-pass search strategy which we thenanalyze in terms of search errors and transla-tion performance.1 IntroductionHierarchical phrase-based translation, using a syn-chronous context-free translation grammar (SCFG)together with an n-gram target language model(LM), is a popular approach in machine transla-tion (Chiang, 2007).
Given a SCFG G and an n-gram language model M , this paper focuses on howto decode with them, i.e.
how to apply them to thesource text to generate a target translation.
Decod-ing has three basic steps, which we first describein terms of the formal languages and relations in-volved, with data representations and algorithms tofollow.1.
Translating the source sentence s with Gto give target translations: T = {s} ?
G,a (weighted) context-free language resultingfrom the composition of a finite language andthe algebraic relation G for SCFG G.2.
Applying the language model to these targettranslations: L=T ?M, a (weighted) context-free language resulting from the intersectionof a context-free language and the regular lan-guage M for M .3.
Searching for the translation and languagemodel combination with the highest-probablitypath: L?=argmaxl?LLOf course, decoding requires explicit data represen-tations and algorithms for combining and searchingthem.
In common to the approaches we will con-sider here, s is applied to G by using the CYK algo-rithm in Step 1 and M is represented by a finite au-tomaton in Step 2.
The choice of the representationof T in many ways determines the remaining de-coder representations and algorithms needed.
Since{s} is a finite language and we assume through-out that G does not allow unbounded insertions,T and L are, in fact, regular languages.
As such,T and L have finite automaton representations Tfand Lf .
In this case, weighted finite-state intersec-tion and single-source shortest path algorithms (us-ing negative log probabilities) can be used to solveSteps 2 and 3 (Mohri, 2009).
This is the approachtaken in (Iglesias et al, 2009a; de Gispert et al,2010).
Instead T and L can be represented by hy-pergraphs Th and Lh (or very similarly context-freerules, and-or trees, or deductive systems).
In thiscase, hypergraph intersection with a finite automa-ton and hypergraph shortest path algorithms can beused to solve Steps 2 and 3 (Huang, 2008).
Thisis the approach taken by Chiang (2007).
In thispaper, we will consider another representation forcontext-free languages T and L as well, pushdownautomata (PDA) Tp and Lp, familiar from formal1373language theory (Aho and Ullman, 1972).
We willdescribe PDA intersection with a finite automatonand PDA shortest-path algorithms in Section 2 thatcan be used to solve Steps 2 and 3.
It cannot beover-emphasized that the CFG, hypergraph and PDArepresentations of T are used for their compactnessrather than for expressing non-regular languages.As presented so far, the search performed in Step3 is admissible (or exact) ?
the true shortest pathis found.
However, the search space in MT can bequite large.
Many systems employ aggressive prun-ing during the shortest-path computation with littletheoretical or empirical guarantees of correctness.Further, such pruning can greatly complicate anycomplexity analysis of the underlying representa-tions and algorithms.
In this paper, we will excludeany inadmissible pruning in the shortest-path algo-rithm itself.
This allows us in Section 3 to comparethe computational complexity of using these differ-ent representations.
We show that the PDA represen-tation is particularly suited for decoding with largeSCFGs and compact LMs.We present Chinese-English translation resultsunder the FSA and PDA translation representations.We describe a two-pass translation strategy whichwe have developed to allow use of the PDA repre-sentation in large-scale translation.
In the first pass,translation is done using a lattice-generating versionof the shortest path algorithm.
The full translationgrammar is used but with a compact, entropy-prunedversion (Stolcke, 1998) of the full language model.This first-step uses admissible pruning and latticegeneration under the compact language model.
Inthe second pass, the original, unpruned LM is simplyapplied to the lattices produced in the first pass.
Wefind that entropy-pruning and first-pass translationcan be done so as to introduce very few search errorsin the overall process; we can identify search errorsin this experiment by comparison to exact transla-tion under the full translation grammar and languagemodel using the FSA representation.
We then inves-tigate a translation grammar which is large enoughthat exact translation under the FSA representationis not possible.
We find that translation is possibleusing the two-pass strategy with the PDA translationrepresentation and that gains in BLEU score resultfrom using the larger translation grammar.1.1 Related WorkThere is extensive prior work on computational ef-ficiency and algorithmic complexity in hierarchicalphrase-based translation.
The challenge is to find al-gorithms that can be made to work with large trans-lation grammars and large language models.Following the original algorithms and analysis ofChiang (2007), Huang and Chiang (2007) devel-oped the cube-growing algorithm, and more recentlyHuang and Mi (2010) developed an incremental de-coding approach that exploits left-to-right nature ofthe language models.Search errors in hierarchical translation, and intranslation more generally, have not been as exten-sively studied; this is undoubtedly due to the diffi-culties inherent in finding exact translations for usein comparison.
Using a relatively simple phrase-based translation grammar, Iglesias et al (2009b)compared search via cube-pruning to an exact FSTimplementation (Kumar et al, 2006) and found thatcube-pruning suffered significant search errors.
ForHiero translation, an extensive comparison of searcherrors between the cube pruning and FSA imple-mentation was presented by Iglesias et al (2009a)and de Gispert et al (2010).
Relaxation techniqueshave also recently been shown to finding exact so-lutions in parsing (Koo et al, 2010) and in SMTwith tree-to-string translation grammars and trigramlanguage models (Rush and Collins, 2011), muchsmaller models compared to the work presented inthis paper.Although entropy-pruned language models havebeen used to produce real-time translation sys-tems (Prasad et al, 2007), we believe our use ofentropy-pruned language models in two-pass trans-lation to be novel.
This is an approach that is widely-used in automatic speech recognition (Ljolje et al,1999) and we note that it relies on efficient represen-tation of very large search spaces T for subsequentrescoring, as is possible with FSAs and PDAs.2 Pushdown AutomataIn this section, we formally define pushdown au-tomata and give intersection, shortest-path and re-lated algorithms that will be needed later.Informally, pushdown automata are finite au-tomata that have been augmented with a stack.
Typ-137401a2?(3)b01a2?
(?3)?b(a) (b)01(3?2a4()5b)0,?1,(?3,??2,(a4,(??5,(b?
(c) (d)Figure 1: PDA Examples: (a) Non-regular PDA accept-ing {anbn|n ?
N}.
(b) Regular (but not bounded-stack)PDA accepting a?b?.
(c) Bounded-stack PDA acceptinga?b?
and (d) its expansion as an FSA.ically this is done by adding a stack alphabet and la-beling each transition with a stack operation (a stacksymbol to be pushed onto, popped or read from thestack) in additon to the usual input label (Aho andUllman, 1972; Berstel, 1979) and weight (Kuichand Salomaa, 1986; Petre and Salomaa, 2009).
Ourequivalent representation allows a transition to be la-beled by a stack operation or a regular input symbolbut not both.
Stack operations are represented bypairs of open and close parentheses (pushing a sym-bol on and popping it from the stack).
The advantageof this representation is that is identical to the finiteautomaton representation except that certain sym-bols (the parentheses) have special semantics.
Assuch, several finite-state algorithms either immedi-ately generalize to this PDA representation or do sowith minimal changes.
The algorithms described inthis section have been implemented in the PDT ex-tension (Allauzen and Riley, 2011) of the OpenFstlibrary (Allauzen et al, 2007).2.1 DefinitionsA (restricted) Dyck language consist of ?well-formed?
or ?balanced?
strings over a finite num-ber of pairs of parentheses.
Thus the string( [ ( ) ( ) ] { } [ ] ) ( ) is in the Dyck language over 3pairs of parentheses.More formally, let A and A be two finite alpha-bets such that there exists a bijection f from A toA.
Intuitively, f maps an open parenthesis to its cor-responding close parenthesis.
Let a?
denote f(a) ifa ?
A and f?1(a) if a ?
A.
The Dyck languageDA over the alphabet A?
= A ?
A is then the lan-guage defined by the following context-free gram-mar: S ?
, S ?
SS and S ?
aSa?
for all a?A.We define the mapping cA : A??
?
A??
as follow.cA(x) is the string obtained by iteratively deletingfrom x all factors of the form aa?
with a ?
A. Ob-serve that DA=c?1A ().Let A and B be two finite alphabets such thatB ?
A, we define the mapping rB : A?
?
B?by rB(x1 .
.
.
xn) = y1 .
.
.
yn with yi = xi if xi ?Band yi= otherwise.A weighted pushdown automaton (PDA) T overthe tropical semiring (R ?
{?
},min,+,?, 0) isa 9-tuple (?,?,?, Q,E, I, F, ?)
where ?
is the fi-nite input alphabet, ?
and ?
are the finite open andclose parenthesis alphabets, Q is a finite set of states,I ?Q the initial state, F ?
Q the set of final states,E ?
Q ?
(?
?
??
?
{}) ?
(R ?
{?})
?
Q a fi-nite set of transitions, and ?
: F ?
R ?
{?}
thefinal weight function.
Let e= (p[e], i[e], w[e], n[e])denote a transition in E.A path pi is a sequence of transitions pi=e1 .
.
.
ensuch that n[ei]=p[ei+1] for 1 ?
i < n. We then de-fine p[pi] = p[e1], n[pi] = n[en], i[pi] = i[e1] ?
?
?
i[en],and w[pi]=w[e1] + .
.
.
+ w[en].A path pi is accepting if p[pi] = I and n[pi] ?
F .A path pi is balanced if r??
(i[pi]) ?D?.
A balancedpath pi accepts the string x ?
??
if it is a balancedaccepting path such that r?
(i[pi])=x.The weight associated by T to a string x ?
?
?is T (x) = minpi?P (x) w[pi] + ?
(n[pi]) where P (x)denotes the set of balanced paths accepting x. Aweighted language is recognizable by a weightedpushdown automaton iff it is context-free.
We de-fine the size of T as |T |= |Q|+|E|.A PDA T has a bounded stack if there exists K ?N such that for any sub-path pi of any balanced pathin T : |c?(r??
(i[pi]))| ?
K .
If T has a bounded stack,then it represents a regular language.
Figure 1 showsnon-regular, regular and bounded-stack PDAs.A weighted finite automaton (FSA) can be viewedas a PDA where the open and close parentheses al-phabets are empty, see (Mohri, 2009) for a stand-alone definition.13752.2 Expansion AlgorithmGiven a bounded-stack PDA T , the expansion of Tis the FSA T ?
equivalent to T defined as follows.A state in T ?
is a pair (q, z) where q is a state in Tand z ???.
A transition (q, a, w, q?)
in T results ina transition ((q, z), a?, w, (q?, z?))
in T ?
only when:(a) a??
?
{}, z?
=z and a?
=a, (b) a?
?, z?
=zaand a?
= , or (c) a ?
?, z?
is such that z = z?aand a?
= .
The initial state of T ?
is I ?
= (I, ).
Astate (q, z) in T ?
is final if q is final in T and z = (??
((q, ))=?(q)).
The set of states of T ?
is the set ofpairs (q, z) that can be reached from an initial stateby transitions defined as above.
The condition thatT has a bounded stack ensures that this set is finite(since it implies that for any (q, z), |z| ?
K).The complexity of the algorithm is linear inO(|T ?|)=O(e|T |).
Figure 1d show the result of thealgorithm when applied to the PDA of Figure 1c.2.3 Intersection AlgorithmThe class of weighted pushdown automata is closedunder intersection with weighted finite automata(Bar-Hillel et al, 1964; Nederhof and Satta, 2003).Considering a pair (T1, T2) where one element is anFSA and the other element a PDA, then there existsa PDA T1?T2, the intersection of T1 and T2, suchthat for all x ?
??
: (T1?T2)(x) = T1(x)+T2(x).We assume in the following that T2 is an FSA.
Wealso assume that T2 has no input- transitions.
WhenT2 has input- transitions, an epsilon filter (Mohri,2009; Allauzen et al, 2011) generalized to handleparentheses can be used.A state in T =T1?T2 is a pair (q1, q2) where q1 isa state of T1 and q2 a state of T2.
The initial state isI=(I1, I2).
Given a transition e1=(q1, a, w1, q?1) inT1, transitions out of (q1, q2) in T are obtained usingthe following rules.If a ?
?, then e1 can be matched with a tran-sition (q2, a, w2, q?2) in T2 resulting a transition((q1, q2), a, w1+w2, (q?1, q?2)) in T .If a = , then e1 is matched with staying in q2resulting in a transition ((q1, q2), , w1, (q?1, q2)).Finally, if a ?
?
?, e1 is also matchedwith staying in q2, resulting in a transition((q1, q2), a, w1, (q?1, q2)) in T .A state (q1, q2) in T is final when both q1 and q2are final, and then ?
((q1, q2))=?1(q1)+?2(q2).SHORTESTDISTANCE(T )1 for each q ?
Q and a ?
?
do2 B[q, a]?
?3 GETDISTANCE(T, I)4 return d[f, I ]RELAX(q, s, w,S)1 if d[q, s] > w then2 d[q, s]?
w3 if q 6?
S then4 ENQUEUE(S , q)GETDISTANCE(T,s)1 for each q ?
Q do2 d[q, s]?
?3 d[s, s]?
04 Ss ?
s5 while Ss 6=?
do6 q ?
HEAD(Ss)7 DEQUEUE(Ss)8 for each e ?
E[q] do9 if i[e] ?
?
?
{} then10 RELAX(n[e], s, d[q, s] + w[e],Ss)11 elseif i[e] ?
?
then12 B[s, i[e]]?
B[s, i[e]] ?
{e}13 elseif i[e] ?
?
then14 if d[n[e], n[e]] is undefined then15 GETDISTANCE(T, n[e])16 for each e?
?
B[n[e], i[e]] do17 w?
d[q, s] +w[e] + d[p[e?
], n[e]] + w[e?
]18 RELAX(n[e?
], s, w,Ss)Figure 2: PDA shortest distance algorithm.
We assumethat F ={f} and ?
(f)=0 to simplify the presentation.The complexity of the algorithm is in O(|T1||T2|).2.4 Shortest Distance and Path AlgorithmsA shortest path in a PDA T is a balanced acceptingpath with minimal weight and the shortest distancein T is the weight of such a path.
We show that whenT has a bounded stack, shortest distance and short-est path can be computed in O(|T |3 log |T |) time(assuming T has no negative weights) and O(|T |2)space.Given a state s in T with at least one incomingopen parenthesis transition, we denote by Cs the setof states that can be reached from s by a balancedpath.
If s has several incoming open parenthesistransitions, a naive implementation might lead to thestates in Cs to be visited up to exponentially manytimes.
The basic idea of the algorithm is to memo-ize the shortest distance from s to states in Cs.
The1376pseudo-code is given in Figure 2.GETDISTANCE(T, s) starts a new instance of theshortest-distance algorithm from s using the queueSs, initially containing s. While the queue is notempty, a state is dequeued and its outgoing transi-tions examined (line 5-9).
Transitions labeled bynon-parenthesis are treated as in Mohri (2009) (line9-10).
When the considered transition e is labeled bya close parenthesis, it is remembered that it balancesall incoming open parentheses in s labeled by i[e]by adding e to B[s, i[e]] (line 11-12).
Finally, whene is labeled with an open parenthesis, if its destina-tion has not already been visited, a new instance isstarted from n[e] (line 14-15).
The destination statesof all transitions balancing e are then relaxed (line16-18).The space complexity of the algorithm isquadratic for two reasons.
First, the number ofnon-infinity d[q, s] is |Q|2.
Second, the space re-quired for storing B is at most in O(|E|2) sincefor each open parenthesis transition e, the size of|B[n[e], i[e]]| is O(|E|) in the worst case.
Thislast observation also implies that the cumulatednumber of transitions examined at line 16 is inO(N |Q| |E|2) in the worst case, where N denotesthe maximal number of times a state is inserted inthe queue for a given call of GETDISTANCE.
As-suming the cost of a queue operation is ?
(n) for aqueue containing n elements, the worst-case timecomplexity of the algorithm can then be expressedas O(N |T |3 ?
(|T |)).
When T contains no negativeweights, using a shortest-first queue discipline leadsto a time complexity in O(|T |3 log |T |).
When allthe Cs?s are acyclic, using a topological order queuediscipline leads to a O(|T |3) time complexity.In effect, we are solving a k-sources shortest-path problem with k single-source solutions.
A po-tentially better approach might be to solve the k-sources or k-pairs problem directly (Hershberger etal., 2003).When T has been obtained by converting an RTNor an hypergraph into a PDA (Section 2.5), the poly-nomial dependency in |T | becomes a linear depen-dency both for the time and space complexities.
In-deed, for each q in T , there exists a unique s suchthat d[q, s] is non-infinity.
Moreover, for each closeparenthesis transistion e, there exists a unique openparenthesis transition e?
such that e?B[n[e?
], i[e?
]].When each component of the RTN is acyclic, thecomplexity of the algorithm is hence in O(|T |) intime and space.The algorithm can be modified to compute theshortest path by keeping track of parent pointers.2.5 Replacement AlgorithmA recursive transition network (RTN) can be speci-fied by (N,?, (T?)?
?N , S) where N is an alphabetof nonterminals, ?
is the input alphabet, (T?)?
?N isa family of FSAs with input alphabet ?
?
N , andS?N is the root nonterminal.A string x ?
??
is accepted by R if there existsan accepting path pi in TS such that recursively re-placing any transition with input label ?
?N by anaccepting path in T?
leads to a path pi?
with input x.The weight associated by R is the minimum over allsuch pi?
of w[pi?]+?S(n[pi?
]).Given an RTN R, the replacement of R is thePDA T equivalent to R defined by the 9-tuple(?,?,?, Q,E, I, F, ?, ?)
with ?=Q=??
?N Q?
,I = IS , F = FS , ?= ?S , and E =???N?e?E?
Eewhere Ee = {e} if i[e] 6?
N and Ee ={(p[e], n[e], w[e], I?
), (f, n[e], ??
(f), n[e])|f ?F?
}with ?= i[e]?N otherwise.The complexity of the construction is in O(|T |).If |F?
| = 1, then |T | = O(??
?N |T?
|) = O(|R|).Creating a superfinal state for each T?
would lead toa T whose size is always linear in the size of R.3 Hierarchical Phrase-Based TranslationRepresentationIn this section, we compare several different repre-sentations for the target translations T of the sourcesentence s by synchronous CFG G prior to languagemodel M application.
As discussed in the introduc-tion, T is a context-free language.
For example, sup-pose it corresponds to:S?abXdg, S?acXfg, and X?bc.Figure 3 shows several alternative representations ofT : Figure 3a shows the hypergraph representation ofthis grammar; there is a 1:1 correspondence betweeneach production in the CFG and each hyperedge inthe hypergraph.
Figure 3b shows the RTN represen-tation of this grammar with a 1:1 correspondence be-tween each production in the CFG and each path inthe RTN; this is the translation representation pro-1377SX3 3a1 1b21c22d4f4g5 5(a) Hypergraph01a6a2b7c3X 4d 5g8X 9f 10g 11 12b 13cS X(b) RTN01a6a2b7c11(12b3 4d 5g[8 9f 10g13c)](c) PDA0,?1,?a6,?a2,?b7,?c11,(?11,[?12,(b12,[b13,(c13,[c3,??8,?
?4,?d9,?f5,?g10,?g(d) FSAFigure 3: Alternative translation representationsduced by the HiFST decoder (Iglesias et al, 2009a;de Gispert et al, 2010).
Figure 3c shows the push-down automaton representation generated from theRTN with the replacement algorithm of Section 2.5.Since s is a finite language and G does not allowunbounded insertion, Tp has a bounded stack andT is, in fact, a regular language.
Figure 3d showsthe finite-state automaton representation of T gen-erated by the PDA using the expansion algorithmof Section 2.2.
The HiFST decoder converts itsRTN translation representation immediately into thefinite-state representation using an algorithm equiv-alent to converting the RTN into a PDA followed byPDA expansion.As shown in Figure 4, an advantage of the RTN,PDA, and FSA representations is that they can bene-fit from FSA epsilon removal, determinization andminimization algorithms applied to their compo-nents (for RTNs and PDAs) or their entirety (forFSAs).
For the complexity discussion below, how-ever, we disregard these optimizations.
Instead wefocus on the complexity of each MT step describedin the introduction:1.
SCFG Translation: Assuming that the parsingof the input is performed by a CYK parse, thenthe CFG, hypergraph, RTN and PDA represen-0 1a2b3c4X5X6df 7g0 1b 2cS X(a) RTN0 1a2b3c 8([ 9b46d7g5f1 0c)](b) PDA0 1a2b3c4b5b6c7c8df 9g(c) FSAFigure 4: Optimized translation representationstations can be generated in O(|s|3|G|) time andspace (Aho and Ullman, 1972).
The FSA rep-resentation can require an additional O(e|s|3|G|)time and space since the PDA expansion can beexponential.2.
Intersection: The intersection of a CFG Thwith a finite automaton M can be performed bythe classical Bar-Hillel algorithm (Bar-Hillelet al, 1964) with time and space complex-ity O(|Th||M |3).1 The PDA intersection algo-rithm from Section 2.3 has time and space com-plexity O(|Tp||M |).
Finally, the FSA intersec-tion algorithm has time and space complexityO(|Tf ||M |) (Mohri, 2009).3.
Shortest Path: The shortest path algorithm onthe hypergraph, RTN, and FSA representationsrequires linear time and space (given the under-lying acyclicity) (Huang, 2008; Mohri, 2009).As presented in Section 2.4, the PDA rep-resentation can require time cubic and spacequadratic in |M |.2Table 1 summarizes the complexity results.
Notethe PDA representation is equivalent in time and su-perior in space to the CFG/hypergraph representa-tion, in general, and it can be superior in both space1The modified Bar-Hillel construction described by Chi-ang (2007) has time and space complexity O(|Th||M |4); themodifications were introduced presumably to benefit the subse-quent pruning method employed (but see Huang et al (2005)).2The time (resp.
space) complexity is not cubic (resp.quadratic) in |Tp||M |.
Given a state q in Tp, there exists aunique sq such that q belongs to Csq .
Given a state (q1, q2)in Tp ?M , (q1, q2) ?
C(s1,s2) only if s1 = sq1 , and hence(q1, q2) belongs to at most |M | components.1378Representation Time Complexity Space ComplexityCFG/hypergraph O(|s|3 |G| |M |3) O(|s|3 |G| |M |3)PDA O(|s|3 |G| |M |3) O(|s|3 |G| |M |2)FSA O(e|s|3|G| |M |) O(e|s|3|G| |M |)Table 1: Complexity using various target translation rep-resentations.and time to the FSA representation depending on therelative SCFG and LM sizes.
The FSA representa-tion favors smaller target translation sets and largerlanguage models.
Should a better complexity PDAshortest path algorithm be found, this conclusioncould change.
In practice, the PDA and FSA rep-resentations benefit hugely from the optimizationsmentioned above, these optimizations improve thetime and space usage by one order of magnitude.4 Experimental FrameworkWe use two hierarchical phrase-based SMT de-coders.
The first one is a lattice-based decoder im-plemented with weighted finite-state transducers (deGispert et al, 2010) and described in Section 3.
Thesecond decoder is a modified version using PDAsas described in Section 2.
In order to distinguishboth decoders we call them HiFST and HiPDT, re-spectively.
The principal difference between the twodecoders is where the finite-state expansion step isdone.
In HiFST, the RTN representation is immedi-ately expanded to an FSA.
In HiPDT, this expansionis delayed as late as possible - in the output of theshortest path algorithm.
Another possible configu-ration is to expand after the LM intersection step butbefore the shortest path algorithm; in practice this isquite similar to HiFST.In the following sections we report experimentsin Chinese-to-English translation.
For translationmodel training, we use a subset of the GALE 2008evaluation parallel text;3 this is 2.1M sentences andapproximately 45M words per language.
We re-port translation results on a development set tune-nw(1,755 sentences) and a test set test-nw (1,671 sen-tences).
These contain translations produced by theGALE program and portions of the newswire sec-tions of MT02 through MT06.
In tuning the sys-3See http://projects.ldc.upenn.edu/gale/data/catalog.html.We excluded the UN material and the LDC2002E18,LDC2004T08, LDC2007E08 and CUDonga collections.0 7.5?
10?9 7.5?
10?8 7.5?
10?7207.5 20.2 4.1 0.9Table 2: Number of ngrams (in millions) in the 1st pass 4-gramlanguage models obtained with different ?
values (top row).tems, standard MERT (Och, 2003) iterative param-eter estimation under IBM BLEU4 is performed onthe development set.The parallel corpus is aligned using MTTK (Dengand Byrne, 2008) in both source-to-target andtarget-to-source directions.
We then follow stan-dard heuristics (Chiang, 2007) and filtering strate-gies (Iglesias et al, 2009b) to extract hierarchicalphrases from the union of the directional word align-ments.
We call a translation grammar the set of rulesextracted from this process.
We extract two transla-tion grammars:?
A restricted grammar where we apply the fol-lowing additional constraint: rules are onlyconsidered if they have a forward translationprobability p > 0.01.
We call this G1.
As willbe discussed later, the interest of this grammaris that decoding under it can be exact, that is,without any pruning in search.?
An unrestricted one without the previous con-straint.
We call this G2.
This is a superset ofthe previous grammar, and exact search underit is not feasible for HiFST: pruning is requiredin search.The initial English language model is a Kneser-Ney 4-gram estimated over the target side of the par-allel text and the AFP and Xinhua portions of mono-lingual data from the English Gigaword Fourth Edi-tion (LDC2009T13).
This is a total of 1.3B words.We will call this language model M1.
For large lan-guage model rescoring we also use the LM M2 ob-tained by interpolating M1 with a zero-cutoff stupid-backoff (Brants et al, 2007) 5-gram estimated using6.6B words of English newswire text.We next describe how we build translation sys-tems using entropy-pruned language models.1.
We build a baseline HiFST system that uses M1and a hierarchical grammar G, parameters be-ing optimized with MERT under BLEU.4See ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13.pl13792.
We then use entropy-based pruning of the lan-guage model (Stolcke, 1998) under a relativeperplexity threshold of ?
to reduce the size ofM1.
We will call the resulting language modelas M?1 .
Table 2 shows the number of n-grams(in millions) obtained for different ?
values.3.
We translate with M?1 using the same param-eters obtained in MERT in step 1, except forthe word penalty, tuned over the lattices underBLEU performance.
This produces a transla-tion lattice in the topmost cell that contains hy-potheses with exact scores under the translationgrammar and M?1 .4.
Translation lattices in the topmost cell arepruned with a likelihood-based beam width ?.5.
We remove the M?1 scores from the prunedtranslation lattices and reapply M1, moving theword penalty back to the original value ob-tained in MERT.
These operations can be car-ried out efficiently via standard FSA opera-tions.6.
Additionally, we can rescore the translation lat-tices obtained in steps 1 or 5 with the largerlanguage model M2.
Again, this can be donevia standard FSA operations.Note that if ?=?
or if ?=0, the translation latticesobtained in step 1 should be identical to the ones ofstep 5.
While the goal is to increase ?
to reduce thesize of the language model used at Step 3, ?
willhave to increase accordingly so as to avoid pruningaway desirable hypotheses in Step 4.
If ?
definesa sufficiently wide beam to contain the hypotheseswhich would be favoured by M1, faster decodingwith M?1 would be possible without incurring searcherrors M1.
This is investigated next.5 Entropy-Pruned LM in RescoringIn Table 3 we show translation performance undergrammar G1 for different values of ?.
Performanceis reported after first-pass decoding with M?1 (seestep 3 in Section 4), after rescoring with M1 (seestep 5) and after rescoring with M2 (see step 6).
Thebaseline (experiment number 1) uses ?
= 0 (that is,M1) for decoding.Under translation grammar G1, HiFST is able togenerate an FSA with the entire space of possiblecandidate hypotheses.
Therefore, any degradationin performance is only due to the M?1 involved indecoding and the ?
applied prior to rescoring.As shown in row number 2, for ?
?
10?9 thesystem provides the same performance to the base-line when ?
> 8, while decoding time is reducedby roughly 40%.
This is because M?1 is 10% of thesize of the original language model M1, as shownin Table 2.
As M?1 is further reduced by increas-ing ?
(see rows number 3 and 4), decoding time isalso reduced.
However, the beam width ?
requiredin order to recover the good hypotheses in rescoringincreases, reaching 12 for experiment 3 and 15 forexperiment 4.Regarding rescoring with the larger M2 (step 6in Section 4), the system is also able to match thebaseline performance as long as ?
is wide enough,given the particular M?1 used in first-pass decoding.Interestingly, results show that a similar ?
value isneeded when rescoring either with M1 or M2.The usage of entropy-pruned language models in-crements speed at the risk of search errors.
For in-stance, comparing the outputs of systems 1 and 2with ?=10 in Table 3 we find 45 different 1-best hy-potheses, even though the BLEU score is identical.In other words, we have 45 cases in which system 2is not able to recover the baseline output because the1st-pass likelihood beam ?
is not wide enough.
Sim-ilarly, system 3 fails in 101 cases (?
=12) and sys-tem 4 fails in 95 cases.
Interestingly, some of thesesentences would require impractically huge beams.This might be due to the Kneser-Ney smoothing,which interacts badly with entropy pruning (Chelbaet al, 2010).6 Hiero with PDAs and FSAsIn this section we contrast HiFST with HiPDT underthe same translation grammar and entropy-prunedlanguage models.
Under the constrained grammarG1 their performance is identical as both decoderscan generate the entire search space which can thenbe rescored with M1 or M2 as shown in the previoussection.Therefore, we now focus on the unconstrainedgrammar G2, where exact search is not feasible forHiFST.
In order to evaluate this problem, we runboth decoders over tune-nw, restricting memory us-age to 10 gigabytes.
If this limit is reached in decod-1380HiFST (G1 + M?1 ) +M1 +M2# ?
tune-nw test-nw time ?
tune-nw test-nw tune-nw test-nw1 0 (M1) 34.3 34.5 0.68 - - - 34.8 35.62 7.5?
10?9 32.0 32.8 0.38 10 34.8 35.69 34.3 34.5 34.9 35.583 7.5?
10?8 29.5 30.0 0.28 12 34.2 34.5 34.7 35.69 34.3 34.4 34.8 35.28 34.2 35.14 7.5?
10?7 26.0 26.4 0.20 15 34.2 34.5 34.7 35.612 34.4 35.5Table 3: Results (lowercase IBM BLEU scores) under G1 with various M?1 as obtained with several values of ?.Performance in subsequent rescoring with M1 and M2 after likelihood-based pruning of the translation lattices forvarious ?
is also reported.
Decoding time, in seconds/word over test-nw, refers strictly to first-pass decoding.Exact search for G2 + M?1 with memory usage under 10 GB# ?
HiFST HiPDTSuccess Failure Success FailureExpand Compose Compose Expand2 7.5?
10?9 12 51 37 40 8 523 7.5?
10?8 16 53 31 76 1 234 7.5?
10?7 18 53 29 99.8 0 0.2Table 4: Percentage of success in producing the 1-best translation under G2 with various M?1 when applying a hardmemory limitation of 10 GB, as measured over tune-nw (1755 sentences).
If decoder fails, we report what step wasbeing done when the limit was reached.
HiFST could be expanding into an FSA or composing the FSA with M?1 ;HiPDT could be PDA composing with M?1 or PDA expanding into an FSA.HiPDT (G2 + M?1 ) +M1 +M2?
tune-nw test-nw ?
tune-nw test-nw tune-nw test-nw7.5?
10?7 25.7 26.3 15 34.6 34.8 35.2 36.1Table 5: HiPDT performance on grammar G2 with ?
= 7.5 ?
10?7.
Exact search with HiFST is not possible underthese conditions: pruning during search would be required.ing, the process is killed5.
We report what internaldecoding operation caused the system to crash.
ForHiFST, these include expansion into an FSA (Ex-pand) and subsequent intersection with the languagemodel (Compose).
For HiPDT, these include PDAintersection with the language model (Compose) andsubsequent expansion into an FSA (Expand), usingalgorithms described in Section 2.Table 4 shows the number of times each decodersucceeds in finding a hypothesis given the memorylimit, and the operations being carried out when theyfail to do so, when decoding with various M?1 .
With?=7.5?
10?9 (row 2), HiFST can only decode 218sentences, while HiPDT succeeds in 703 cases.
The5We used ulimit command.
The experiment was carried outover machines with different configurations and load.
There-fore, these numbers must be considered as approximate values.differences between both decoders increase as theM?1 is more reduced, and for ?=7.5?10?7 (row 4),HiPDT is able to perform exact search over all butthree sentences.Table 5 shows performance using the latter con-figuration (Table 4, row 4).
After large languagemodel rescoring, HiPDT improves 0.5 BLEU overbaseline with G1 (Table 3, row 1).7 Discussion and ConclusionHiFST fails to decode mainly because the expansioninto an FST leads to far too big search spaces (e.g.fails 938 times under ?
= 7.5 ?
10?8).
If it suc-ceeds in expanding the search space into an FST,the decoder still has to compose with the languagemodel, which is also critical in terms of memory us-1381age (fails 536 times).
In contrast, HiPDT creates aPDA, which is a more compact representation of thesearch space and allows efficient intersection withthe language model before expansion into an FST.Therefore, the memory usage is considerably lower.Nevertheless, the complexity of the language modelis critical for the PDA intersection and very speciallythe PDA expansion into an FST (fails 403 times for?=7.5?
10?8).With the algorithms presented in this paper, de-coding with PDAs is possible for any translationgrammar as long as an entropy pruned LM is used.While this allows exact decoding, it comes at thecost of making decisions based on less complexLMs, although this has been shown to be an ad-equate strategy when applying compact CFG rule-sets.
On the other hand, HiFST cannot decode underlarge translation grammars, thus requiring pruningduring lattice construction, but it can apply an un-pruned LM in this process.
We find that with care-fully designed pruning strategies, HiFST can matchthe performance of HiPDT reported in Table 5.
Butwithout pruning in search, expansion directly into anFST would lead to an explosion in terms of memoryusage.
Of course, without memory constraints bothstrategies would reach the same performance.Overall, these results suggest that HiPDT is morerobust than HiFST when using complex hierarchi-cal grammars.
Conversely, FSTs might be moreefficient for search spaces described by more con-strained hierarchical grammars.
This suggests thata hybrid solution could be effective: we could usePDAs or FSTs e.g.
depending on the number ofstates of the FST representing the expanded searchspace, or other conditions.8 AcknowledgmentsThe research leading to these results has receivedfunding from the European Union Seventh Frame-work Programme (FP7-ICT-2009-4) under grantagreement number 247762, and was supported inpart by the GALE program of the Defense AdvancedResearch Projects Agency, Contract No.HR0011-06-C-0022, and a Google Faculty Research Award,May 2010.ReferencesAlfred V. Aho and Jeffrey D. Ullman.
1972.
The Theoryof Parsing, Translation and Compiling, volume 1-2.Prentice-Hall.Cyril Allauzen and Michael Riley, 2011.
PushdownTransducers.
http://pdt.openfst.org.Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-jciech Skut, and Mehryar Mohri.
2007.
OpenFst:A general and efficient weighted finite-state trans-ducer library.
In Proceedings of CIAA, pages 11?23.http://www.openfst.org.Cyril Allauzen, Michael Riley, and Johan Schalkwyk.2011.
Filters for efficient composition of weightedfinite-state transducers.
In Proceedings of CIAA, vol-ume 6482 of LNCS, pages 28?38.
Springer.Y.
Bar-Hillel, M. Perles, and E. Shamir.
1964.
On formalproperties of simple phrase structure grammars.
InY.
Bar-Hillel, editor, Language and Information: Se-lected Essays on their Theory and Application, pages116?150.
Addison-Wesley.Jean Berstel.
1979.
Transductions and Context-FreeLanguages.
Teubner.Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,and Jeffrey Dean.
2007.
Large language models inmachine translation.
In Proceedings of EMNLP-ACL,pages 858?867.Ciprian Chelba, Thorsten Brants, Will Neveitt, and PengXu.
2010.
Study on interaction between entropy prun-ing and kneser-ney smoothing.
In Proceedings of In-terspeech, pages 2242?2245.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201?228.Adria` de Gispert, Gonzalo Iglesias, Graeme Blackwood,Eduardo R. Banga, and William Byrne.
2010.
Hier-archical phrase-based translation with weighted finitestate transducers and shallow-n grammars.
Computa-tional Linguistics, 36(3).Yonggang Deng and William Byrne.
2008.
HMM wordand phrase alignment for statistical machine transla-tion.
IEEE Transactions on Audio, Speech, and Lan-guage Processing, 16(3):494?507.Manfred Drosde, Werner Kuick, and Heiko Vogler, ed-itors.
2009.
Handbook of Weighted Automata.Springer.John Hershberger, Subhash Suri, and Amit Bhosle.
2003.On the difficulty of some shortest path problems.
InProceedings of STACS, volume 2607 of LNCS, pages343?354.
Springer.Liang Huang and David Chiang.
2007.
Forest rescoring:Faster decoding with integrated language models.
InProceedings of ACL, pages 144?151.1382Liang Huang and Haitao Mi.
2010.
Efficient incrementaldecoding for tree-to-string translation.
In Proceedingsof EMNLP, pages 273?283.Liang Huang, Hao Zhang, and Daniel Gildea.
2005.Machine translation as lexicalized parsing with hooks.In Proceedings of the Ninth International Workshopon Parsing Technology, Parsing ?05, pages 65?73,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Liang Huang.
2008.
Advanced dynamic programming insemiring and hypergraph frameworks.
In Proceedingsof COLING, pages 1?18.Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,and William Byrne.
2009a.
Hierarchical phrase-basedtranslation with weighted finite state transducers.
InProceedings of NAACL-HLT, pages 433?441.Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,and William Byrne.
2009b.
Rule filtering by patternfor efficient hierarchical translation.
In Proceedings ofEACL, pages 380?388.Terry Koo, Alexander M. Rush, Michael Collins, TommiJaakkola, and David Sontag.
2010.
Dual decomposi-tion for parsing with non-projective head automata.
InProceedings of EMNLP, pages 1288?1298.Werner Kuich and Arto Salomaa.
1986.
Semirings, au-tomata, languages.
Springer.Shankar Kumar, Yonggang Deng, and William Byrne.2006.
A weighted finite state transducer transla-tion template model for statistical machine translation.Natural Language Engineering, 12(1):35?75.Andrej Ljolje, Fernando Pereira, and Michael Riley.1999.
Efficient general lattice generation and rescor-ing.
In Proceedings of Eurospeech, pages 1251?1254.Mehryar Mohri.
2009.
Weighted automata algorithms.In Drosde et al (Drosde et al, 2009), chapter 6, pages213?254.Mark-Jan Nederhof and Giorgio Satta.
2003.
Probabilis-tic parsing as intersection.
In Proceedings of 8th In-ternational Workshop on Parsing Technologies, pages137?148.Franz J. Och.
2003.
Minimum error rate training in sta-tistical machine translation.
In Proceedings of ACL,pages 160?167.Ion Petre and Arto Salomaa.
2009.
Algebraic systemsand pushdown automata.
In Drosde et al (Drosde etal., 2009), chapter 7, pages 257?289.R.
Prasad, K. Krstovski, F. Choi, S. Saleem, P. Natarajan,M.
Decerbo, and D. Stallard.
2007.
Real-time speech-to-speech translation for pdas.
In Proceedings of IEEEInternational Conference on Portable Information De-vices, pages 1 ?5.Alexander M. Rush and Michael Collins.
2011.
Ex-act decoding of syntactic translation models throughlagrangian relaxation.
In Proceedings of ACL-HLT,pages 72?82.Andreas Stolcke.
1998.
Entropy-based pruning ofbackoff language models.
In Proceedings of DARPABroadcast News Transcription and UnderstandingWorkshop, pages 270?274.1383
