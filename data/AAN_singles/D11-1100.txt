Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1081?1091,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsHarnessing WordNet Senses for Supervised Sentiment ClassificationBalamurali A R1,2 Aditya Joshi2 Pushpak Bhattacharyya21 IITB-Monash Research Academy, IIT Bombay2Dept.
of Computer Science and Engineering, IIT BombayMumbai, India - 400076{balamurali,adityaj,pb}@cse.iitb.ac.inAbstractTraditional approaches to sentiment classifica-tion rely on lexical features, syntax-based fea-tures or a combination of the two.
We pro-pose semantic features using word senses fora supervised document-level sentiment classi-fier.
To highlight the benefit of sense-basedfeatures, we compare word-based representa-tion of documents with a sense-based repre-sentation where WordNet senses of the wordsare used as features.
In addition, we highlightthe benefit of senses by presenting a part-of-speech-wise effect on sentiment classification.Finally, we show that even if a WSD enginedisambiguates between a limited set of wordsin a document, a sentiment classifier still per-forms better than what it does in absence ofsense annotation.
Since word senses used asfeatures show promise, we also examine thepossibility of using similarity metrics definedon WordNet to address the problem of notfinding a sense in the training corpus.
We per-form experiments using three popular similar-ity metrics to mitigate the effect of unknownsynsets in a test corpus by replacing them withsimilar synsets from the training corpus.
Theresults show promising improvement with re-spect to the baseline.1 IntroductionSentiment Analysis (SA) is the task of prediction ofopinion in text.
Sentiment classification deals withtagging text as positive, negative or neutral from theperspective of the speaker/writer with respect to atopic.
In this work, we follow the definition of Panget al (2002) & Turney (2002) and consider a binaryclassification task for output labels as positive andnegative.Traditional supervised approaches for SA haveexplored lexeme and syntax-level units as features.Approaches using lexeme-based features use bag-of-words (Pang and Lee, 2008) or identify theroles of different parts-of-speech (POS) like adjec-tives (Pang et al, 2002; Whitelaw et al, 2005).Approaches using syntax-based features constructparse trees (Matsumoto et al, 2005) or use textparsers to model valence shifters (Kennedy andInkpen, 2006).Our work explores incorporation of semanticsin a supervised sentiment classifier.
We use thesynsets in Wordnet as the feature space to representword senses.
Thus, a document consisting ofwords gets mapped to a document consisting ofcorresponding word senses.
Harnessing WordNetsenses as features helps us address two issues:1.
Impact of WordNet sense-based features on theperformance of supervised SA2.
Use of WordNet similarity metrics to solve theproblem of features unseen in the training cor-pusThe first points deals with evaluating sense-basedfeatures against word-based features.
The second is-sue that we address is in fact an opportunity to im-prove the performance of SA that opens up becauseof the choice of sense space.
Since sense-basedfeatures prove to generate superior sentiment clas-sifiers, we get an opportunity to mitigate unknown1081synsets in the test corpus by replacing them withknown synsets in the training corpus.
Note that suchreplacement is not possible if word-based represen-tation were used as it is not feasible to make such alarge number of similarity comparisons.We use the corpus by Ye et al (2009) that con-sists of travel domain reviews marked as positive ornegative at the document level.
Our experiments onstudying the impact of Wordnet sense-based featuresdeal with variants of this corpus manually or auto-matically annotated with senses.
Besides showingthe overall impact, we perform a POS-wise analysisof the benefit to SA.
In addition, we compare the ef-fect of varying training samples on a sentiment clas-sifier developed using word based features and sensebased features.
Through empirical evidence, we alsoshow that disambiguating some words in a docu-ment also provides a better accuracy as comparedto not disambiguating any words.
These four sets ofexperiments highlight our hypothesis that WordNetsenses are better features as compared to words.Wordnet sense-based space allows us to mitigateunknown features in the test corpus.
Our synset re-placement algorithm uses Wordnet similarity-basedmetrics which replace an unknown synset in the testcorpus with the closest approximation in the trainingcorpus.
Our results show that such a replacementbenefits the performance of SA.The roadmap for the rest of the paper is as fol-lows: Existing related work in SA and the differ-entiating aspects of our work are explained in sec-tion 2 Section 3 describes the sense-based featuresthat we use for this work.
We explain the similarity-based replacement technique using WordNet synsetsin section 4.
Our experiments have been describedin section 5.
In section 6, we present our resultsand related discussions.
Section 7 analyzes some ofthe causes for erroneous classification.
Finally, sec-tion 8 concludes the paper and points to future work.2 Related WorkThis work studies the benefit of a word sense-basedfeature space to supervised sentiment classification.However, a word sense-based feature space is feasi-ble subject to verification of the hypothesis that sen-timent and word senses are related.
Towards this,Wiebe and Mihalcea (2006) conduct a study on hu-man annotation of 354 words senses with polarityand report a high inter-annotator agreement.
Thework in sentiment analysis using sense-based fea-tures, including ours, assumes this hypothesis thatsense decides the sentiment.The novelty of our work lies in the following.Firstly our approach is distinctly.
Akkaya et al(2009) and Martn-Wanton et al (2010) report per-formance of rule-based sentiment classification us-ing word senses.
Instead of a rule-based implemen-tation, We used supervised learning.
The supervisednature of our approach renders lexical resources un-necessary as used in Martn-Wanton et al (2010).Rentoumi et al (2009) suggest using word sensesto detect sentence level polarity of news headlines.The authors use graph similarity to detect polarity ofsenses.
To predict sentence level polarity, a HMMis trained on word sense and POS as the observa-tion.
The authors report that word senses partic-ularly help understanding metaphors in these sen-tences.
Our work differs in terms of the corpus anddocument sizes in addition to generating a generalpurpose classifier.Another supervised approach of creating an emo-tional intensity classifier using concepts as featureshas been reported by Carrillo de Albornoz et al(2010).
This work is different based on the featurespace used.
The concepts used for the purpose arelimited to affective classes.
This restricts the size ofthe feature space to a limited set of labels.
As op-posed to this, we construct feature vectors that mapto a larger sense-based space.
In order to do so, weuse synset offsets as representation of sense-basedfeatures.Akkaya et al (2009), Martn-Wanton et al (2010)and Carrillo de Albornoz et al (2010) perform sen-timent classification of individual sentences.
How-ever, we consider a document as a unit of sentimentclassification i.e.
our goal is to predict a documenton the whole as positive or negative.
This is differentfrom Pang and Lee (2004) which suggests that sen-timent is associated only with subjective content.
Adocument in its entirety is represented using sense-based features in our experiments.
Carrillo de Al-bornoz et al (2010) suggests expansion using Word-Net relations which we also follow.
This is a benefitthat can be achieved only in a sense-based space.10823 Features based on WordNet SensesIn their original form, documents are said to be inlexical space since they consist of words.
When thewords are replaced by their corresponding senses,the resultant document is said to be in semanticspace.WordNet 2.1 (Fellbaum, 1998) has been used asthe sense repository.
Each word/lexeme is mappedto an appropriate synset in WordNet based onits sense and represented using the correspondingsynset id of WordNet.
Thus, the word love is dis-ambiguated and replaced by the identifier 21758160which consists of a POS category identifier 2 fol-lowed by synset offset identifier 1758160.
This pa-per refers to synset offset as synset identifiers or sim-ply, senses.This section first gives the motivation for usingword senses and then, describes the approaches thatwe use for our experiments.3.1 MotivationConsider the following sentences as the first sce-nario.1.
?Her face fell when she heard that she hadbeen fired.?2.
?The fruit fell from the tree.
?The word ?fell?
occurs in different senses in thetwo sentences.
In the first sentence, ?fell?
has themeaning of ?assume a disappointed or sad expres-sion, whereas in the second sentence, it has themeaning of ?descend in free fall under the influenceof gravity?.
A user will infer the negative polarity ofthe first sentence from the negative sense of ?fell?
init while the user will state that the second sentencedoes not carry any sentiment.
This implies that thereis at least one sense of the word ?fell?
that carriessentiment and at least one that does not.In the second scenario, consider the following ex-amples.1.
?The snake bite proved to be deadly for theyoung boy.?2.
?Shane Warne is a deadly spinner.
?The word deadly has senses which carry oppositepolarity in the two sentences and these senses as-sign the polarity to the corresponding sentence.
Thefirst sentence is negative while the second sentenceis positive.Finally in the third scenario, consider the follow-ing pair of sentences.1.
?He speaks a vulgar language.?2.
?Now that?s real crude behavior!
?The words vulgar and crude occur as synonymsin the synset that corresponds to the sense ?conspic-uously and tastelessly indecent?.
The synonymousnature of words can be identified only if they arelooked at as senses and not just words.As one may observe, the first scenario shows thata word may have some sentiment-bearing and somenon-sentiment-bearing senses.
In the second sce-nario, we show that there may be different sensesof a word that bear sentiments of opposite polarity.Finally, in the third scenario, we show how a sensecan be manifested using different words, i.e., wordsin a synset.
The three scenarios motivate the use ofsemantic space for sentiment prediction.3.2 Sense versus Lexeme-based FeatureRepresentationWe annotate the words in the corpus with theirsenses using two sense disambiguation approaches.As the first approach, manual sense annotationof documents is carried out by two annotators on twosubsets of the corpus, the details of which are givenin Section 5.1.
This is done to determine the idealcase scenario- the skyline performance.As the second approach, a state-of-art algorithmfor domain-specific WSD proposed by Khapra etal.
(2010) is used to obtain an automatically sense-tagged corpus.
This algorithm called iterative WSDor IWSD iteratively disambiguates words by rank-ing the candidate senses based on a scoring function.The two types of sense-annotated corpus lead usto four feature representations for a document:1.
Word senses that have been manually annotated(M)2.
Word senses that have been annotated by an au-tomatic WSD (I)10833.
Manually annotated word senses and words(both separately as features) (Words +Sense(M))4.
Automatically annotated word senses andwords (both separately as features) (Words +Sense(I))Our first set of experiments compares the fourfeature representations to find the feature represen-tation with which sentiment classification gives thebest performance.
W+S(M) and W+S(I) are used toovercome non-coverage of WordNet for some nounsynsets.
In addition to this, we also present a part-of-speech-wise analysis of benefit to SA as well aseffect of varying the training samples on sentimentclassification accuracy.3.3 Partial disambiguation as opposed to nodisambiguationThe state-of-the-art automatic WSD engine that weuse performs (approximately) with 70% accuracy ontourism domain (Khapra et al, 2010).
This meansthat the performance of SA depends on the perfor-mance of WSD which is not very high in case of theengine we use.A partially disambiguated document is a docu-ment which does not contain senses of all words.Our hypothesis is that disambiguation of even fewwords in a document can give better results thanno disambiguation.
To verify this, we create differ-ent variants of the corpus by disambiguating wordswhich have candidate senses within a threshold.
Forexample, a partially disambiguated variant of thecorpus with threshold 3 for candidate senses is cre-ated by disambiguating words which have a maxi-mum of three candidate senses.
These synsets arethen used as features for classification along withlexeme based features.
We conduct multiple experi-ments using this approach by varying the number ofcandidate senses.4 Advantage of senses: Similarity Metricsand Unknown Synsets4.1 Synset Replacement AlgorithmUsing WordNet senses provides an opportunity touse similarity-based metrics for WordNet to reducethe effect of unknown features.
If a synset encoun-tered in a test document is not found in the trainingcorpus, it is replaced by one of the synsets presentin the training corpus.
The substitute synset is deter-mined on the basis of its similarity with the synsetin the test document.
The synset that is replaced isreferred to as an unseen synset as it is not known tothe trained model.For example, consider excerpts of two reviews,the first of which occurs in the training corpus whilethe second occurs in the test corpus.1.
?
In the night, it is a lovely city and... ?2.
?
The city has many beautiful hot spots for hon-eymooners.
?The synset of ?beautiful?
is not present in the train-ing corpus.
We evaluate a similarity metric for allsynsets in the training corpus with respect to thesense of beautiful and find that the sense of lovely isclosest to it.
Hence, the sense of beautiful in the testdocument is replaced by the sense of lovely which ispresent in the training corpus.The replacement algorithm is described in Algo-rithm 1.
The algorithm follows from the fact that thesimilarity value for a synset with itself is maximum.4.2 Similarity metrics usedWe conduct different runs of the replacementalgorithm using three similarity metrics, namelyLIN?s similarity metric, Lesk similarity metric andLeacock and Chodorow (LCH) similarity metric.These runs generate three variants of the corpus.We compare the benefit of each of these metrics bystudying their sentiment classification performance.The metrics can be described as follows:LIN: The metric by Lin (1998) uses the infor-mation content individually possessed by two con-cepts in addition to that shared by them.
The infor-mation content shared by two concepts A and B isgiven by their most specific subsumer (lowest super-ordinate(lso).
Thus, this metric defines the similaritybetween two concepts assimLIN (A,B) =2?
logPr(lso(A,B))logPr(A) + logPr(B) (1)1084Input: Training Corpus, Test Corpus,Similarity MetricOutput: New Test CorpusT:= Training Corpus;X:= Test Corpus;S:= Similarity metric;train concept list = get list concept(T) ;test concept list = get list concept(X);for each concept C in test concept list dotemp max similarity = 0 ;temp concept = C ;for each concept D in train concept list dosimilarity value = get similarity value(C,D,S);if (similarity value > temp max similarity) thentemp max similarity= similarity value;temp concept = D ;endendC = temp concept ;replace synset corpus(C,X);endReturn X ;Algorithm 1: Synset replacement using similaritymetricLesk: Each concept in WordNet is definedthrough gloss.
To compute the Lesk similar-ity (Banerjee and Pedersen, 2002) between A andB, a scoring function based on the overlap of wordsin their individual glosses is used.Leacock and Chodorow (LCH): To measuresimilarity between two concepts A and B, Leacockand Chodorow (1998) compute the shortest paththrough hypernymy relation between them under theconstraint that there exists such a path.
The finalvalue is computed by scaling the path length by theoverall taxonomy depth (D).simLCH(A,B) = ?
log( len(A,B)2D)(2)5 ExperimentationWe describe the variants of the corpus generated andthe experiments in this section.5.1 Data PreparationWe create different variants of the dataset by Ye etal.
(2009).
This dataset contains 600 positive and591 negative reviews about seven travel destinations.Each review contains approximately 4-5 sentenceswith an average number of words per review being80-85.To create the manually annotated corpus, two hu-man annotators annotate words in the corpus withsenses for two disjoint subsets of the original cor-pus by Ye et al (2009).
The inter-annotation agree-ment for a subset of the corpus showed 91% senseoverlap.
The manually annotated corpus consists of34508 words with 6004 synsets.POS #Words P(%) R(%) F-Score(%)Noun 12693 75.54 75.12 75.33Adverb 4114 71.16 70.90 71.03Adjective 6194 67.26 66.31 66.78Verb 11507 68.28 67.97 68.12Overall 34508 71.12 70.65 70.88Table 1: Annotation Statistics for IWSD; P- Precision,R-RecallThe second variant of the corpus contains wordsenses obtained from automatic disambiguation us-ing IWSD.
The evaluation statistics of the IWSD isshown in Table 1.
Table 1 shows that the F-score fornoun synsets is high while that for adjective synsetsis the lowest among all.
The low recall for adjectivePOS based synsets can be detrimental to classifica-tion since adjectives are known to express direct sen-timent (Pang et al, 2002).
Hence, in the context ofsentiment classification, disambiguation of adjectivesynsets is more critical as compared to disambigua-tion of noun synsets.5.2 Experimental setupThe experiments are performed using C-SVM (lin-ear kernel with default parameters1) available as apart of LibSVM2 package.
We choose to use SVMsince it performs the best for sentiment classification(Pang et al, 2002).
All results reported are averageof five-fold cross-validation accuracies.To conduct experiments on words as features, wefirst perform stop-word removal.
The words are notstemmed since stemming is known to be detrimen-tal to sentiment classification (Leopold and Kinder-mann, 2002).
To conduct the experiments based on1C=0.0,=0.00102http://www.csie.ntu.edu.tw/ cjlin/libsvm1085Feature Representations Accuracy(%) PF NF PP NP PR NRWords (Baseline) 84.90 85.07 84.76 84.95 84.92 85.19 84.60Sense (M) 89.10 88.22 89.11 91.50 87.07 85.18 91.24Words + Sense (M) 90.20 89.81 90.43 92.02 88.55 87.71 92.39Sense (I) 85.48 85.31 85.65 87.17 83.93 83.53 87.46Words + Sense (I) 86.08 86.28 85.92 85.87 86.38 86.69 85.46Table 2: Classification Results; PF-Positive F-score(%), NF-Negative F-score (%), PP-Positive Precision (%), NP-Negative Precision (%), PR-Positive Recall (%), NR-Negative Recall (%)the synset representation, words in the corpus are an-notated with synset identifiers along with POS cat-egory identifiers.
For automatic sense disambigua-tion, we used the trained IWSD engine from Khapraet al (2010).
These synset identifiers along withPOS category identifiers are then used as features.For replacement using semantic similarity measures,we used WordNet::Similarity 2.05 package by Ped-ersen et al (2004).To evaluate the result, we use accuracy, F-score,recall and precision as the metrics.
Classificationaccuracy defines the ratio of the number of true in-stances to the total number of instances.
Recall iscalculated as a ratio of the true instances found tothe total number of false positives and true posi-tives.
Precision is defined as the number of trueinstances divided by number of true positives andfalse negatives.
Positive Precision (PP) and Posi-tive Recall (PR) are precision and recall for positivedocuments while Negative Precision (NP) and Nega-tive Recall (NR) are precision and recall for negativedocuments.
F-score is the weighted precision-recallscore.6 Results and Discussions6.1 Comparison of various featurerepresentationsTable 2 shows results of classification for differentfeature representations.
The baseline for our resultsis the unigram bag-of-words model (Baseline).An improvement of 4.2% is observed in the ac-curacy of sentiment prediction when manually an-notated sense-based features (M) are used in placeof word-based features (Words).
The precision ofboth the classes using features based on semanticspace is also better than one based on lexeme space.While reported results suggest that it is more diffi-cult to detect negative sentiment than positive senti-ment (Gindl and Liegl, 2008), our results show thatnegative recall increases by around 8% in case ofsense-based representation of documents.The combined model of words and manually an-notated senses (Words + Senses (M)) gives the bestperformance with an accuracy of 90.2%.
This leadsto an improvement of 5.3% over the baseline accu-racy 3.One of the reasons for improved performance isthe feature abstraction achieved due to the synset-based features.
The dimension of feature vector isreduced by a factor of 82% when the document isrepresented in synset space.
The reduction in dimen-sionality may also lead to reduction in noise (Cun-ningham, 2008).A comparison of accuracy of different sense rep-resentations in Table 2 shows that manual disam-biguation performs better than using automatic al-gorithms like IWSD.
Although overall classificationaccuracy improvement of IWSD over baseline ismarginal, negative recall also improves.
This bene-fit is despite the fact that evaluation of IWSD engineover manually annotated corpus gave an overall F-score of 71% (refer Table 1).
For a WSD enginewith a better accuracy, the performance of sense-based SA can be boosted further.Thus, in terms of feature representation of docu-ments, sense-based features provide a better overallperformance as compared to word-based features.1086Sense81.2478.3066.1473 .70.0080.0090.0050.0060.00racy(%)20.0030.0040.00Accu0.0010.0020.00Adverb?Verb?
POWords 74.9966.83.7871.8180.03Noun?AdjectiveOS?categoryFigure 1: POS-wise statistics of manually annotated se-mantic space6.2 POS-wise analysisFor each POS, we compare the performance of twomodels:?
Model trained on words of only that POS?
Model trained on word senses of only that POSFigure 1 shows the parts-of-speech-wise classifica-tion accuracy of sentiment classification for senses(manual) and words.
In the lexeme space, adjectivesdirectly impact the classification performance.
But itcan be seen that disambiguation of adverb and verbsynsets impact the performance of SA higher thandisambiguation of nouns and adjectives.While it is believed that adjectives carry directsentiments, our results suggest that using adjectivesalone as features may not improve the accuracy.
Theresults prove that sentiment may be subtle at timesand not expressed directly through adjectives.As manual sense annotation is an effort and costintensive process, the parts-of-speech-wise resultssuggest improvements expected from an automaticWSD engine so that it can aid sentiment classifica-tion.
Table 1 suggests that the WSD engine worksbetter for noun synsets compared to adjective andadverb synsets.
While this is expected in a typicalWSD setup, it is the adverbs and verbs that are moreimportant for detecting sentiment in semantics space3The improvement in results of semantic space is found tobe statistically significant over the baseline at 95% confidencelevel when tested using a paired t-test.than nouns.
The future WSD systems will have toshow an improvement in their accuracy with respectto adverb and verb synsets.Sense WordsPOS Category PF NF PF NFAdverb 79.65 80.45 70.25 73.68Verb 75.50 79.28 62.23 63.12Noun 73.39 75.40 69.77 72.55Adjective 63.11 65.03 78.29 79.20Table 3: POS-wise F-score for sense (M) and Words;PF-Positive F-score(%), NF- Negative F-score (%)Table 3 shows the positive and negative F-scorestatistics with respect to different POS.
Detectionof negative reviews using lexeme space is difficult.POS-wise statistics also suggest the same.
It shouldbe noted that adverb and verb synsets play an im-portant role in negative class detection.
Thus, an au-tomatic WSD engine should give importance to thecorrect disambiguation of these POS categories.6.3 Effect of size of training corpus#TrainingDocumentsW M I W+S(M) W+S(I)100 76.5 87 79.5 82.5 79.5200 81.5 88.5 82 90 84300 79.5 92 81 89.5 82400 82 90.5 81 94 85.5500 83.5 91 85 96 82.5Table 4: Accuracy (%) with respect to number of trainingdocuments; W: Words, M: Manual Annotation, I: IWSD-based sense annotation, W+S(M): Word+Senses (Manualannotation), W+S(I): Word+Senses(IWSD-based senseannotation)From table 2, the benefit of sense disambigua-tion to sentiment prediction is evident.
In addition,Table 4 shows variation of classification accuracywith respect to different number of training sam-ples based on different approaches of annotation ex-plained in previous sections.
The results are basedon a blind set of 90 test samples from both the po-larity labels 4.4No cross validation is performed for this experiment1087Compared to lexeme-based features, manually an-notated sense based features give better performancewith lower number of training samples.
IWSD isalso better than lexeme-based features.
A SA sys-tem trained on 100 training samples using manuallyannotated senses gives an accuracy of 87%.
Word-based features never achieve this accuracy.
AnIWSD-based system requires lesser samples whencompared to lexeme space for an equivalent accu-racy.
Note that model based on words + senses(M)features achieve an accuracy of 96% on this test set.This implies that the synset space, in additionto benefit to sentiment prediction in general, re-quires lesser number of training samples in order toachieve the accuracy that lexeme space can achievewith a larger number of samples.6.4 Effect of Partial disambiguationFigure 2 shows the accuracy, positive F-score andnegative F-score with respect to different thresholdsof candidate senses for partially disambiguated doc-uments as described in Section 3.3.
We compare theperformance of these documents with word-basedfeatures (B) and sense-based features based on man-ually (M) or automatically obtained senses (I).
Notethat Sense (I) and Sense (M) correspond to com-pletely disambiguated documents.In case of partial disambiguation using manualannotation, disambiguating words with less thanthree candidate senses performs better than others.For partial disambiguation that relies on an auto-matic WSD engine, a comparable performance tofull disambiguation can be obtained by disambiguat-ing words which have a maximum of four candidatesenses.As expected, completely disambiguated docu-ments provide the best F-score and accuracy fig-ures5.
However, a performance comparable to com-plete disambiguation can be attained by disam-biguating selective words.Our results show that even if highly ambiguous(in terms of senses) words are not disambiguated bya WSD engine, the performance of sentiment classi-fication improves.5All results are statistically significant with respect to base-line6senses(M)6?senses?(I)Negative?FscorePos5?senses?(M)5?senses?(I)6?senses?(M)3?senses?(I)4?senses?(M)4?senses?(I)2?senses?(M)2?senses?(I)3?senses?(M)Words?
(B)Sense(M)Sense(I) 81.0082.0083.0084.0085.00Fscore/itive?FscoreAccuracy86.0087.0088.0089.0090.0091.00/Accuracy?
(%)Figure 2: Partial disambiguation statistics: Accu-racy,Positive F-score, Negative F-score variation with re-spect to sense disambiguation difficult level is shown.Words(B): baseline system6.5 Synset replacement using similarity metricsTable 5 shows the results of synset replacement ex-periments performed using similarity metrics de-fined in section 4.
The similarity metric value NAshown in the table indicates that synset replacementis not performed for the specific run of experiment.For this set of experiments, we use the combina-tion of sense and words as features (indicated bySenses+Words (M)).Synset replacement using a similarity metricshows an improvement over using words alone.However, the improvement in classification accu-racy is marginal compared to sense-based represen-tation without synset replacement (Similarity Met-ric=NA).Replacement using LIN and LCH metrics givesmarginally better results compared to the vanilla set-ting in a manually annotated corpus.
The same phe-nomenon is seen in the case of IWSD based ap-proach6.
The limited improvement can be due tothe fact that since LCH and LIN consider only IS-A6Results based on LCH and LIN similarity metric for auto-matic sense disambiguation is not statistically significant with?=0.051088Feature Representation SimilarityMetricAccuracy PF NF PP NP PR NRWords (Baseline) NA 84.90 85.07 84.76 84.95 84.92 85.19 84.60Words + Sense(M) NA 90.20 89.81 90.43 92.02 88.55 87.71 92.39Words + Sense(I) NA 86.08 86.28 85.92 85.87 86.38 86.69 85.46Words + Sense (M) LCH 90.60 90.20 90.85 92.85 88.61 87.70 93.21Words + Sense(M) LIN 90.70 90.26 90.97 93.17 88.50 87.53 93.57Words + Sense (M) Lesk 91.12 90.70 91.38 93.55 88.97 88.03 93.92Words + Sense (I) LCH 85.66 85.85 85.52 85.67 85.76 86.02 85.28Words + Sense(I) LIN 86.16 86.37 86.00 86.06 86.40 86.69 85.61Words + Sense (I) Lesk 86.25 86.41 86.10 86.31 86.26 86.52 85.95Table 5: Similarity Metric Analysis using different similarity metrics with synsets and a combinations of synset andwords;PF-Positive F-score(%), NF-Negative F-score (%), PP-Positive Precision (%), NP-Negative Precision (%), PR-Positive Recall (%), NR-Negative Recall (%)Top informationcontent features(in %)IWSDsynset #Manualsynsets #Matchsynset #MatchSynsets (%)UnmatchedSynset(%)10 601 722 288 39.89 60.1120 1199 1443 650 45.05 54.9530 1795 2165 1005 46.42 53.5840 2396 2889 1375 47.59 52.4150 2997 3613 1730 47.88 52.12Table 6: Comparison of top information gain-based features of manually annotated corpora and automatically anno-tated corporarelationship in WordNet, the replacement happensonly for verbs and nouns.
This excludes adverbsynsets which we have shown to be the best featuresfor a sense-based SA system.Among all similarity metrics, the best classifica-tion accuracy is achieved using Lesk.
The systemperforms with an overall classification accuracy of91.12%, which is a substantial improvement of 6.2%over baseline.
Again, it is only 1% over the vanillasetting that uses combination of synset and words.However, the similarity metric is not sophisticatedas LIN or LCH.Thus, we observe a marginal improvement by us-ing similarity-based metrics for WordNet.
A goodmetric which covers all POS categories can providesubstantial improvement in the classification accu-racy.7 Error AnalysisFor sentiment classification based on semanticspace, we classify the errors into four categories.The examples quoted are from manual evaluation ofthe results.1.
Effect of low disambiguation accuracy of IWSDengine: SA using automatic sense annotationdepends on the annotation system used.
To as-sess the impact of IWSD system on sentimentclassification, we compare the feature set basedon manually annotated senses with the featureset based on automatically annotated senses.We compare the most informative features ofthe two classifiers.
Table 6 shows the numberof top informative features (synset) selected asthe percentage of total synset features presentwhen the semantic representation of documen-tation is used.
The matched synset column rep-resents the number of IWSD synsets that match1089with manually annotated synsets.The number of top performing features is morein case of manually annotated synsets.
Thiscan be attributed to the total number of synsetstagged in the two variant of the corpus.
The re-duction in the performance of SA for automati-cally annotated senses is because of the numberof unmatched synsets.Thus, although the accuracy of IWSD is cur-rently 70%, the table indicates that IWSD canmatch the performance of manually annotatedsenses for SA if IWSD is able to tag correctlythose top information content synsets.
This as-pect needs to be investigated further.2.
Negation Handling: For the purpose of thiswork, we concentrate on words as units for sen-timent determination.
Syntax and its contri-bution in understanding sentiment is neglectedand hence, positive documents which con-tain negations are wrongly classified as nega-tive.
Negation may be direct as in the excerpt?....what is there not to like about Vegas.?
ormay be double as in the excerpt?...that aren?tinsecure?.3.
Interjections and WordNet coverage: Recentinformal words are not covered in WordNet andhence, do not get disambiguated.
The sameis the case for interjections like ?wow?,?duh?which sometimes carry direct sentiment.
Lex-ical resources which include them can be usedto incorporate information about these lexicalunits.4.
Document Specificity: The assumption under-lying our analysis is that a document containsdescription of only one topic.
However, re-views are generic in nature and tend to expresscontrasting sentiment about sub-topics .
Forexample, a travel review about Paris can talkabout restaurants in Paris, traffic in Paris, pub-lic behaviour, etc.
with opposing sentiments.Assigning an overall sentiment to a documentis subjective in such cases.8 Conclusion & Future WorkThis work presents an empirical benefit of WSD tosentiment analysis.
The study shows that supervisedsentiment classifier modeled on wordNet senses per-form better than word-based features.
We show howthe performance impact differs for different auto-matic and manual techniques, parts-of-speech, dif-ferent training sample size and different levels ofdisambiguation.
In addition, we also show the bene-fit of using WordNet based similarity metrics for re-placing unknown features in the test set.
Our resultssupport the fact that not only does sense space im-prove the performance of a sentiment classificationsystem, but also opens opportunities for improve-ment using better similarity metrics.Incorporation of syntactical information alongwith semantics can be an interesting area of work.More sophisticated features which include the twoneed to be explored.
Another line of work is in thecontext of cross-lingual sentiment analysis.
Currentsolutions are based on machine translation which isvery resource-intensive.
Using a bi-lingual dictio-nary which maps WordNet across languages, such amachine translation sub-system can be avoided.AcknowledgmentWe thank Jaya Saraswati and Rajita Shukla fromCFILT Lab, IIT Bombay for annotating the datasetused for this work.
We also thank Mitesh Khapraand Salil Joshi, IIT Bombay for providing us withthe IWSD engine for the required experiments.ReferencesCem Akkaya, Janyce Wiebe, and Rada Mihalcea.
2009.Subjectivity word sense disambiguation.
In Proc.
ofEMNLP ?09, pages 190?199, Singapore.Satanjeev Banerjee and Ted Pedersen.
2002.
An adaptedlesk algorithm for word sense disambiguation usingwordnet.
In Proc.
of CICLing?02, pages 136?145,London, UK.Jorge Carrillo de Albornoz, Laura Plaza, and PabloGervs.
2010.
Improving emotional intensity clas-sification using word sense disambiguation.
Specialissue: Natural Language Processing and its Appli-cations.
Journal on Research in Computing Science,46:131?142.1090Pdraig Cunningham.
2008.
Dimension reduction.
InMachine Learning Techniques for Multimedia, Cogni-tive Technologies, pages 91?112.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
Bradford Books.Stefan Gindl and Johannes Liegl, 2008.
Evaluation ofdifferent sentiment detection methods for polarity clas-sification on web-based reviews, pages 35?43.Alistair Kennedy and Diana Inkpen.
2006.
Sentimentclassification of movie reviews using contextual va-lence shifters.
Computational Intelligence, 22(2):110?125.Mitesh Khapra, Sapan Shah, Piyush Kedia, and PushpakBhattacharyya.
2010.
Domain-specific word sensedisambiguation combining corpus basedand wordnetbased parameters.
In Proc.
of GWC?10, Mumbai, In-dia.Claudia Leacock and Martin Chodorow.
1998.
Com-bining local context with wordnet similarity for wordsense identification.
In WordNet: A Lexical ReferenceSystem and its Application.Edda Leopold and Jo?rg Kindermann.
2002.
Text catego-rization with support vector machines.
how to repre-sent texts in input space?
Machine Learning, 46:423?444.Dekang Lin.
1998.
An information-theoretic definitionof similarity.
In In Proc.
of the 15th International Con-ference on Machine Learning, pages 296?304.Tamara Martn-Wanton, Alexandra Balahur-Dobrescu,Andres Montoyo-Guijarro, and Aurora Pons-Porrata.2010.
Word sense disambiguation in opinion mining:Pros and cons.
In Proc.
of CICLing?10, Madrid,Spain.Shotaro Matsumoto, Hiroya Takamura, and ManabuOkumura.
2005.
Sentiment classification using wordsub-sequences and dependency sub-trees.
In Proc.of PAKDD?05,, Lecture Notes in Computer Science,pages 301?311.Bo Pang and Lillian Lee.
2004.
A sentimental education:Sentiment analysis using subjectivity summarizationbased on minimum cuts.
In Proc.
of ACL?04, pages271?278, Barcelona, Spain.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Found.
Trends Inf.
Retr., 2:1?135.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
sentiment classification using ma-chine learning techniques.
volume 10, pages 79?86.Ted Pedersen, Siddharth Patwardhan, and Jason Miche-lizzi.
2004.
Wordnet::similarity: measuring the relat-edness of concepts.
In Demonstration Papers at HLT-NAACL?04, pages 38?41.Vassiliki Rentoumi, George Giannakopoulos, VangelisKarkaletsis, and George A. Vouros.
2009.
Sen-timent analysis of figurative language using a wordsense disambiguation approach.
In Proc.
of the In-ternational Conference RANLP?09, pages 370?375,Borovets, Bulgaria.Peter Turney.
2002.
Thumbs up or thumbs down?
Se-mantic orientation applied to unsupervised classifica-tion of reviews.
In Proc.
of ACL?02, pages 417?424,Philadelphia, US.Casey Whitelaw, Navendu Garg, and Shlomo Argamon.2005.
Using appraisal groups for sentiment analysis.In Proc.
of CIKM ?05, pages 625?631, New York, NY,USA.Janyce Wiebe and Rada Mihalcea.
2006.
Word senseand subjectivity.
In Proc.
of COLING-ACL?06, pages1065?1072.Qiang Ye, Ziqiong Zhang, and Rob Law.
2009.
Senti-ment classification of online reviews to travel destina-tions by supervised machine learning approaches.
Ex-pert Systems with Applications, 36(3):6527 ?
6535.1091
