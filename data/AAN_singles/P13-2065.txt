Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 364?369,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsStem Translation with Affix-Based Rule Selectionfor Agglutinative LanguagesZhiyang Wang?, Yajuan Lu?
?, Meng Sun?, Qun Liu??
?Key Laboratory of Intelligent Information ProcessingInstitute of Computing Technology, Chinese Academy of SciencesP.O.
Box 2704, Beijing 100190, China{wangzhiyang,lvyajuan,sunmeng,liuqun}@ict.ac.cn?Centre for Next Generation LocalisationFaculty of Engineering and Computing, Dublin City Universityqliu@computing.dcu.ieAbstractCurrent translation models are mainly de-signed for languages with limited mor-phology, which are not readily applicableto agglutinative languages as the differ-ence in the way lexical forms are gener-ated.
In this paper, we propose a nov-el approach for translating agglutinativelanguages by treating stems and affixesdifferently.
We employ stem as the atomictranslation unit to alleviate data spare-ness.
In addition, we associate each stem-granularity translation rule with a distri-bution of related affixes, and select desir-able rules according to the similarity oftheir affix distributions with given spans tobe translated.
Experimental results showthat our approach significantly improvesthe translation performance on tasks oftranslating from three Turkic languages toChinese.1 IntroductionCurrently, most methods on statistical machinetranslation (SMT) are developed for translationof languages with limited morphology (e.g., En-glish, Chinese).
They assumed that word was theatomic translation unit (ATU), always ignoring theinternal morphological structure of word.
Thisassumption can be traced back to the originalIBM word-based models (Brown et al, 1993) andseveral significantly improved models, includingphrase-based (Och and Ney, 2004; Koehn et al,2003), hierarchical (Chiang, 2005) and syntac-tic (Quirk et al, 2005; Galley et al, 2006; Liu etal., 2006) models.
These improved models workedwell for translating languages like English withlarge scale parallel corpora available.Different from languages with limited morphol-ogy, words of agglutinative languages are formedmainly by concatenation of stems and affixes.Generally, a stem can attach with several affixes,thus leading to tens of hundreds of possible inflect-ed variants of lexicons for a single stem.
Modelingeach lexical form as a separate word will generatehigh out-of-vocabulary rate for SMT.
Theoretical-ly, ways like morphological analysis and increas-ing bilingual corpora could alleviate the problemof data sparsity, but most agglutinative languagesare less-studied and suffer from the problem ofresource-scarceness.
Therefore, previous researchmainly focused on the different inflected variantsof the same stem and made various transformationof input by morphological analysis, such as (Lee,2004; Goldwater and McClosky, 2005; Yang andKirchhoff, 2006; Habash and Sadat, 2006; Bisazzaand Federico, 2009; Wang et al, 2011).
Thesework still assume that the atomic translation unitis word, stem or morpheme, without consideringthe difference between stems and affixes.In agglutinative languages, stem is the basepart of word not including inflectional affixes.Affix, especially inflectional affix, indicates dif-ferent grammatical categories such as tense, per-son, number and case, etc., which is useful fortranslation rule disambiguation.
Therefore, weemploy stem as the atomic translation unit anduse affix information to guide translation ruleselection.
Stem-granularity translation rules havemuch larger coverage and can lower the OOVrate.
Affix based rule selection takes advantageof auxiliary syntactic roles of affixes to make abetter rule selection.
In this way, we can achievea balance between rule coverage and matchingaccuracy, and ultimately improve the translationperformance.364zunyi/STMi/SUFyighin/STMgha/SUFzunyi yighin ||| ??
???
||| i ghaOriginal:zunyi yighin+i+ghaMeaning:of zunyi conference(B)Translation rules with affix distributionzunyi yighin ||| ?????
||| i:0 gha:0.09 zunyi yighin ||| ??????
||| i:0 da:0.24zunyi/STMi/SUFyighin/STMda/SUFzunyi yighin ||| ??????
||| i da(A) Instances of translation rule(1) (2)zunyi/STMi/SUFyighin/STMgha/SUFzunyi yighin ||| ??
???
||| i gha(3)Original:zunyi yighin+i+daMeaning:on zunyi conferenceOriginal:zunyi yighin+i+ghaMeaning:of zunyi conferenceFigure 1: Translation rule extraction from Uyghur to Chinese.
Here tag ?/STM?
represents stem and?/SUF?
means suffix.2 Affix Based Rule Selection ModelFigure 1 (B) shows two translation rules alongwith affix distributions.
Here a translation rulecontains three parts: the source part (on stem lev-el), the target part, and the related affix distribution(represented as a vector).
We can see that, al-though the source part of the two translation rulesare identical, their affix distributions are quitedifferent.
Affix ?gha?
in the first rule indicatesthat something is affiliated to a subject, similar to?of?
in English.
And ?da?
in second rule implieslocation information.
Therefore, given a span?zunyi/STM yighin/STM+i/SUF+da/SUF+...?
tobe translated, we hope to encourage our model toselect the second translation rule.
We can achievethis by calculating similarity between the affixdistributions of the translation rule and the span.The affix distribution can be obtained by keep-ing the related affixes for each rule instance duringtranslation rule extraction ((A) in Figure 1).
Afterextracting and scoring stem-granularity rules in atraditional way, we extract stem-granularity rulesagain by keeping affix information and computethe affix distribution with tf-idf (Salton and Buck-ley, 1987).
Finally, the affix distribution will beadded to the previous stem-granularity rules.2.1 Affix Distribution EstimationFormally, translation rule instances with the samesource part can be treated as a document collec-tion1, so each rule instance in the collection is1We employ concepts from text classification to illustratehow to estimate affix distribution.some kind of document.
Our goal is to classify thesource parts into the target parts on the documentcollection level with the help of affix distribu-tion.
Accordingly, we employ vector space model(VSM) to represent affix distribution of each ruleinstance.
In this model, the feature weights arerepresented by the classic tf-idf (Salton and Buck-ley, 1987):tf i,j =ni,j?k nk,jidf i,j = log|D||j : ai ?
rj|tfidf i,j = tf i,j ?
idf i,j(1)where tfidf i,j is the weight of affix ai in transla-tion rule instance rj .
ni,j indicates the number ofoccurrence of affix ai in rj .
|D| is the numberof rule instance with the same source part, and|j : ai ?
rj| is the number of rule instance whichcontains affix ai within |D|.Let?s take the suffix ?gha?
from (A1) in Figure1 as an example.
We assume that there are onlythree instances of translation rules extracted fromparallel corpus ((A) in Figure 1).
We can see that?gha?
only appear once in (A1) and also appearonce in whole instances.
Therefore, tfgha,(A1) is0.5 and idfgha,(A1) is log(3/2).
tfidfgha,(A1) isthe product of tfgha,(A1) and idfgha,(A1) whichis 0.09.Given a set of N translation rule instances withthe same source and target part, we define thecentroid vector dr according to the centroid-basedclassification algorithm (Han and Karypis, 2000),dr =1N?i?Ndi (2)365Data set #Sent.
#Type #Tokenword stem morph word stem morphUY-CH-Train.
50K 69K 39K 42K 1.2M 1.2M 1.6MUY-CH-Dev.
0.7K*4 5.9K 4.1K 4.6K 18K 18K 23.5KUY-CH-Test.
0.7K*1 4.7K 3.3K 3.8K 14K 14K 17.8KKA-CH-Train.
50K 62K 40K 42K 1.1M 1.1M 1.3MKA-CH-Dev.
0.7K*4 5.3K 4.2K 4.5K 15K 15K 18KKA-CH-Test.
0.2K*1 2.6K 2.0K 2.3K 8.6K 8.6K 10.8KKI-CH-Train.
50K 53K 27K 31K 1.2M 1.2M 1.5MKI-CH-Dev.
0.5K*4 4.1K 3.1K 3.5K 12K 12K 15KKI-CH-Test.
0.2K*4 2.2K 1.8K 2.1K 4.7K 4.7K 5.8KTable 1: Statistics of data sets.
?N means the number of reference, morph is short to morpheme.
UY,KA, KI, CH represent Uyghur, Kazakh, Kirghiz and Chinese respectively.dr is the final affix distribution.By comparing the similarity of affix distribu-tions, we are able to decide whether a translationrule is suitable for a span to be translated.
Inthis work, similarity is measured using the cosinedistance similarity metric, given bysim(d1,d2) =d1 ?
d2?d1?
?
?d2?
(3)where di corresponds to a vector indicating affixdistribution, and ???
denotes the inner product ofthe two vectors.Therefore, for a specific span to be translated,we first analyze it to get the corresponding stemsequence and related affix distribution representedas a vector.
Then the stem sequence is used tosearch the translation rule table.
If the source partis matched, the similarity will be calculated foreach candidate translation rule by cosine similarity(as in equation 3).
Therefore, in addition to thetraditional translation features on stem level, ourmodel also adds the affix similarity score as adynamic feature into the log-linear model (Ochand Ney, 2002).3 Related WorkMost previous work on agglutinative languagetranslation mainly focus on Turkish and Finnish.Bisazza and Federico (2009) and Mermer andSaraclar (2011) optimized morphological analysisas a pre-processing step to improve the translationbetween Turkish and English.
Yeniterzi and Oflaz-er (2010) mapped the syntax of the English sideto the morphology of the Turkish side with thefactored model (Koehn and Hoang, 2007).
Yangand Kirchhoff (2006) backed off surface form tostem when translating OOV words of Finnish.Luong and Kan (2010) and Luong et al (2010)focused on Finnish-English translation throughimproving word alignment and enhancing phrasetable.
These works still assumed that the atomictranslation unit is word, stem or morpheme, with-out considering the difference between stems andaffixes.There are also some work that employed thecontext information to make a better choice oftranslation rules (Carpuat and Wu, 2007; Chan etal., 2007; He et al, 2008; Cui et al, 2010).
all thework employed rich context information, such asPOS, syntactic, etc., and experiments were mostlydone on less inflectional languages (i.e.
Chinese,English) and resourceful languages (i.e.
Arabic).4 ExperimentsIn this work, we conduct our experiments onthree different agglutinative languages, includingUyghur, Kazakh and Kirghiz.
All of them arederived from Altaic language family, belonging toTurkic languages, and mostly spoken by people inCentral Asia.
There are about 24 million peopletake these languages as mother tongue.
All ofthe tasks are derived from the evaluation of Chi-na Workshop of Machine Translation (CWMT)2.Table 1 shows the statistics of data sets.For the language model, we use the SRI Lan-guage Modeling Toolkit (Stolcke, 2002) to traina 5-gram model with the target side of trainingcorpus.
And phrase-based Moses3 is used as our2http://mt.xmu.edu.cn/cwmt2011/en/index.html.3http://www.statmt.org/moses/366UY-CH KA-CH KI-CHword 31.74+0.0 28.64+0.0 35.05+0.0stem 33.74+2.0 30.14+1.5 35.52+0.47morph 32.69+0.95 29.21+0.57 34.97?0.08affix 34.34+2.6 30.19+2.27 35.96+0.91Table 2: Translation results from Turkic languagesto Chinese.
word: ATU is surface form,stem: ATU is represented stem, morph: ATUdenotes morpheme, affix: stem translation withaffix distribution similarity.
BLEU scores inbold means significantly better than the baselineaccording to (Koehn, 2004) for p-value less than0.01.baseline SMT system.
The decoding weights areoptimized with MERT (Och, 2003) to maximumword-level BLEU scores (Papineni et al, 2002).4.1 Using Unsupervised MorphologicalAnalyzerAs most agglutinative languages are resource-poor, we employ unsupervised learning methodto obtain the morphological structure.
Follow-ing the approach in (Virpioja et al, 2007), weemploy the Morfessor4 Categories-MAP algorith-m (Creutz and Lagus, 2005).
It applies a hierar-chical model with three categories (prefix, stem,and suffix) in an unsupervised way.
From Table 1we can see that vocabulary sizes of the three lan-guages are reduced obviously after unsupervisedmorphological analysis.Table 2 shows the translation results.
All thethree translation tasks achieve obvious improve-ments with the proposed model, which always per-forms better than only employ word, stem andmorph.
For the Uyghur to Chinese translation(UY-CH) task in Table 2, performances after unsu-pervised morphological analysis are always betterthan the baseline.
And we gain up to +2.6 BLEUpoints improvements with affix compared to thebaseline.
For the Kazakh to Chinese translation(KA-CH) task, the improvements are also signifi-cant.
We achieve +2.27 and +0.77 improvementscompared to the baseline and stem, respectively.As for the Kirghiz to Chinese translation (KI-CH)task, improvements seem relative small comparedto the other two language pairs.
However, it alsogains +0.91 BLEU points over the baseline.4http://www.cis.hut.fi/projects/morpho/UY Unsup Supstem #Type 39K 21K#Token 1.2M 1.2Maffix #Type 3.0K 0.3K#Token 0.4M 0.7MTable 3: Statistics of training corpus after unsuper-vised(Unsup) and supervised(Sup) morphologicalanalysis.31.5 3232.5 3333.5 3434.5 3535.5 3636.5word morph stem affixBLEUscore(%)UnsupervisedSupervisedFigure 2: Uyghur to Chinese translation resultsafter unsupervised and supervised analysis.4.2 Using Supervised MorphologicalAnalyzerTaking it further, we also want to see the effect ofsupervised analysis on our model.
A generativestatistical model of morphological analysis forUyghur was developed according to (Mairehabaet al, 2012).
Table 3 shows the difference ofstatistics of training corpus after supervised andunsupervised analysis.
Supervised method gen-erates fewer type of stems and affixes than theunsupervised approach.
As we can see fromFigure 2, except for the morph method, stemand affix based approaches perform better aftersupervised analysis.
The results show that ourapproach can obtain even better translation per-formance if better morphological analyzers areavailable.
Supervised morphological analysis gen-erates more meaningful morphemes, which lead tobetter disambiguation of translation rules.5 Conclusions and Future WorkIn this paper we propose a novel framework foragglutinative language translation by treating stemand affix differently.
We employ the stem se-quence as the main part for training and decod-ing.
Besides, we associate each stem-granularitytranslation rule with an affix distribution, whichcould be used to make better translation decisionsby calculating the affix distribution similarity be-367tween the rule and the instance to be translated.We conduct our model on three different languagepairs, all of which substantially improved thetranslation performance.
The procedure is totallylanguage-independent, and we expect that otherlanguage pairs could benefit from our approach.AcknowledgmentsThe authors were supported by 863 StateKey Project (No.
2011AA01A207), andNational Key Technology R&D Program (No.2012BAH39B03), Key Project of KnowledgeInnovation Program of Chinese Academy ofSciences (No.
KGZD-EW-501).
Qun Liu?s workis partially supported by Science FoundationIreland (Grant No.07/CE/I1142) as part of theCNGL at Dublin City University.
We wouldlike to thank the anonymous reviewers for theirinsightful comments and those who helped tomodify the paper.ReferencesArianna Bisazza and Marcello Federico.
2009.
Mor-phological pre-processing for Turkish to Englishstatistical machine translation.
In Proceedings ofIWSLT, pages 129?135.Peter F. Brown, Vincent J. Della Pietra, StephenA.
Della Pietra, and Robert L. Mercer.
1993.
Themathematics of statistical machine translation: pa-rameter estimation.
Comput.
Linguist., 19(2):263?311.Marine Carpuat and Dekai Wu.
2007.
Improvingstatistical machine translation using word sensedisambiguation.
In Proceedings of EMNLP-CoNLL,pages 61?72.Yee Seng Chan, Hwee Tou Ng, and David Chiang.2007.
Word sense disambiguation improvesstatistical machine translation.
In Proceedings ofACL, pages 33?40.David Chiang.
2005.
A hierarchical phrase-based model for statistical machine translation.
InProceedings of ACL, pages 263?270.Mathias Creutz and Krista Lagus.
2005.
Inducing themorphological lexicon of a natural language fromunannotated text.
In Proceedings of AKRR, pages106?113.Lei Cui, Dongdong Zhang, Mu Li, Ming Zhou,and Tiejun Zhao.
2010.
A joint rule selectionmodel for hierarchical phrase-based translation.
InProceedings of ACL, Short Papers, pages 6?11.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and trainingof context-rich syntactic translation models.
InProceedings of COLING/ACL, pages 961?968.Sharon Goldwater and David McClosky.
2005.Improving statistical MT through morphologicalanalysis.
In Proceedings of HLT-EMNLP, pages676?683.Nizar Habash and Fatiha Sadat.
2006.
Arabic prepro-cessing schemes for statistical machine translation.In Proceedings of NAACL, Short Papers, pages 49?52.Eui-Hong Sam Han and George Karypis.
2000.Centroid-based document classification: analysisexperimental results.
In Proceedings of PKDD,pages 424?431.Zhongjun He, Qun Liu, and Shouxun Lin.
2008.Improving statistical machine translation usinglexicalized rule selection.
In Proceedings ofCOLING, pages 321?328.Philipp Koehn and Hieu Hoang.
2007.
Factoredtranslation models.
In Proceedings of EMNLP-CoNLL, pages 868?876.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
InProceedings of NAACL, pages 48?54.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proceedings ofEMNLP, pages 388?395.Young-Suk Lee.
2004.
Morphological analysis forstatistical machine translation.
In Proceedings ofHLT-NAACL, Short Papers, pages 57?60.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machinetranslation.
In Proceedings of COLING-ACL, pages609?616.Minh-Thang Luong and Min-Yen Kan. 2010.Enhancing morphological alignment for translatinghighly inflected languages.
In Proceedings ofCOLING, pages 743?751.Minh-Thang Luong, Preslav Nakov, and Min-Yen Kan.2010.
A hybrid morpheme-word representationfor machine translation of morphologically richlanguages.
In Proceedings of EMNLP, pages 148?157.Aili Mairehaba, Wenbin Jiang, Zhiyang Wang, Yibu-layin Tuergen, and Qun Liu.
2012.
Directed graphmodel of Uyghur morphological analysis.
Journalof Software, 23(12):3115?3129.Coskun Mermer and Murat Saraclar.
2011.
Un-supervised Turkish morphological segmentation forstatistical machine translation.
In Workshop of MTand Morphologically-rich Languages.368Franz Josef Och and Hermann Ney.
2002.
Discrim-inative training and maximum entropy models forstatistical machine translation.
In Proceedings ofACL, pages 295?302.Franz Josef Och and Hermann Ney.
2004.
Thealignment template approach to statistical machinetranslation.
Comput.
Linguist., pages 417?449.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofACL, pages 160?167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedingsof ACL, pages 311?318.Chris Quirk, Arul Menezes, and Colin Cherry.2005.
Dependency treelet translation: syntacticallyinformed phrasal SMT.
In Proceedings of ACL,pages 271?279.Gerard Salton and Chris Buckley.
1987.
Termweighting approaches in automatic text retrieval.Technical report.Andreas Stolcke.
2002.
SRILM - an extensiblelanguage modeling toolkit.
In Proceedings ofICSLP, pages 311?318.Sami Virpioja, Jaakko J. Va?yrynen, Mathias Creutz,and Markus Sadeniemi.
2007.
Morphology-awarestatistical machine translation based on morphsinduced in an unsupervised manner.
In Proceedingsof MT SUMMIT, pages 491?498.Zhiyang Wang, Yajuan Lu?, and Qun Liu.
2011.Multi-granularity word alignment and decoding foragglutinative language translation.
In Proceedingsof MT SUMMIT, pages 360?367.Mei Yang and Katrin Kirchhoff.
2006.
Phrase-basedbackoff models for machine translation of highlyinflected languages.
In Proceedings of EACL, pages1017?1020.Reyyan Yeniterzi and Kemal Oflazer.
2010.
Syntax-to-morphology mapping in factored phrase-basedstatistical machine translation from English toTurkish.
In Proceedings of ACL, pages 454?464.369
