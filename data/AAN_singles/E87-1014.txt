AN AUTOMATIC SPEECH RECOGNITION SYSTEM FOR T!
IE ITALIAN LANGUAGEPaolo D'Orta, Marco Ferretti, Alessandro Martelli, Stefano SeareiIBM Rome Scientific Centervia Giorgione 159, ROME (Italy)ABSTRACT 4.An automatic speech recognition system for Italianlanguage has been developed at IBM Italy Scientific Centerin Rome.
It is able to recognize in real time naturallanguage sentences, composed with words from a dictionaryof 6500 items, dictated by a speaker with short pausesamong them.
The system is speaker dependent, beforeusing it the speaker has to perform the training stagereading a predefined text 15-20 minutes long.
It runs on anarchitecture composed by an IBM 3090 mainframe and aPC/AT based workstation with signal processingequipments.PROBABIL IST IC  APPROACHThe problem of recognizing human voice is approachedin a probabilistic manner.
I_~t W = w 1, w 2 .
.
.
.
.
w n be asequence of n words, and let A be the acoustic informationextracted from the speech signal, from which the system willtry to identify the pronounced words.
P (WI~)  i...ndieates theprobability that the the sequence of words W has beenspoken, once we observe the acoustic string A" produced atthe end of the signal processing stage.
The most probablesequence of word, given A- , is that maximizing /'(W'IA') ?Through Bayes' formul~~ felX)= maxff,r'(w IA ) -  max w e(~)P_.~.A J W) denotes the probability that the se__quence of wordsW will produce the acoustic s t r ing j ' ,  P(W) is the a prioriprobability of word string W, P(A) is the probability ofacoustic string A.
To find the word sequence whichmaximizes the third term in the preceding equation, it issufficient to find the sequence which maximizes thenumerator; P(A) is, in fact, clearly not dependent on anyW.
Then, the recognition task can be decomposed in theseproblems:1. perform an acoustic processing able to extract from thespeech signal an information A representative of itsacoustic features, and, at the same time, adequate for astatistical analysis;3.create an acoustic model which makes it possible toevaluate P(A'\[ W-~_), that is the probability that theacoustic string A will be produced when the speakerpronounces the word string W;create a language model giving the prob._ability P(IV)that the speaker will wish to pronounce W;find, among all possible sequences of words, the mostprobable or,e.
F, ven with small vocabularies it is notfeasible to ~.onduct an exhaustive search; so, we need toidentify an efficient search strategy.ACOUSTIC  PROCESSINGAcoustic processing is performed in the acousticfront-end forrned by an acquisition stage (microphone, filter,amplifier, A~I) converter) and a processing stage.
Theanalog to digital converter gives a numeric representation fthe signal picked up by the microphone, constituted by20000 samples/see., each of 12 bits.
Every 10 millisecondsan acoustic vector of 20 parameters i computed escribing,through its spectral features, the behavior of speech signalfor that interval.
This operation takes into account recentstudies on physiology of the human ear and on psychologyof sound,, perccption.
The signal energy in several frequencybands is determined through a Fourier analysis 161.
Widthof bands is n~}l uniform; it grows with frequency.
This is inaccordance with the behavior of the cochlea that has abetter resolution power at low frequencies.
Furthermore,compulation of parameters considers other features ofauditory system, as dynamic adaptation to signal level.Each acoustic vector is then compared with a set of 200prototype vectors and the closest prototype is chosen torepresent i ; the label of this prototype (a number from I to200), will then be substituted to th__e original vector.Therefore, the acoustic information ,4 is formed by asequence of labels a l ,  a2,"" , with a considerable reductionin the amount of data needed to represent the speech signal.ACOUSI" IC  MODEl.
,The acoustic model must compute the probabilityP(A IW) that lhe pronunciation of word string I,V willproduce the hd~el string A.
To design the acoustic model itis esscnlial t-  understand the relationship between wordsand sounds of a language.
With sounds of a language wemean those particular sounds usually generated duringspeaking.
Ph(~netics is helpful in this task.
Experts inlinguistics usually classify sounds in classes, called phonemex\[2\].
The same phoneme can be representative of manydifferent sounds, but they are completely equivalent from alinguistic point of view.
The Italian language is usuallydescribed with 31 phonemes; in our system we use anextended set composed of 56 phonetic elements, to take intoaccount particular aspects of the process of pronunciationnot considered by the usual classification: coarticulation,different behavior in stressed and non-stressed vowels,pronunciation of w~wels and fricatives by people fromdifferent regi(ms. F, ach word in the language can bephonetically described by a sequence of phonemes,80representing the sequence of basic sounds that compose it.So, it is very useful to build up the acoustic model startingfrom phonemes.For each phoneme, a Markov source i"51 is defined,which is a model representing the phenomenon of producingacoustic labels during pronunciation of the phoneme itself.Markov sources can be represented by a set of states and aset of transitions among them.
Every 10 milliseconds atransition takes place and an acoustic label is generated bythe source.
Transitions and labels are not predetermined,but are chosen randomly on the basis of a probabilitydistribution.
Starting from phoneme models, we can buildmodels for words, or for word strings, simply byconcatenating the Markov sources of the correspondingphonemes.
Figure 1 shows a typical structure for Markovmodel of a phonetic unit and figure 2 the structure of theMarkov model for a word.The structure of Markov models is completely definedby the number of states and by interconneetions amongthem.
It is unique for all the phonemes and for all thespeakers and has been determined on the basis of intuitiveconsiderations and experimental results, because noalgorithm is known to find the best structure to describesuch a phenomenon.
The different behavior in differentphonemes and in the voice of different speakers is taken intoaccount in the evaluation of the model parameters:probability of transition between pair of states andprobability of emission of labels.
This evaluation, executedinthe training stage, is performed, given the word sequenc~ ~_If: of training text and collected the acoustic label string Afrom the front-end, accordingly to the maximum likelihoodcriterion \[l-I, maximizing the probability P(A't W).
Aspeaker, during training, does not have to pronounce all thewords in the dictionary; on the other hand, it is necessarythat the text to be read contains all the phonemes of thelanguage, each of them well represented in a great variety ofphonetic ontexts.in the recognition stage the term P(A'I W) is computedon the basis of statistical parameters determined uring thetraining; then it is necessary to evaluate the probability thatthe Mar._kov source for the word string W will emit the labelstring A, going from its initial state to its final one.
Thismust be done summing the probability of all the paths ofthis kind, but it could be eomputationally very heavy andimpractical to count them all because._their number dependsexponentially on the length of A.
Using dynamicprogramming techniques, it is possible to reach this goallimiting the amount of calculation to be done.
The forwardpass algorithm 1"5\], is, in._ fact, computationally linearlydependent on the length of A.LANGUAGE MODEL__The language model is used to evaluate the probabilityP(W) of the word sequence m. Let I/l = Wl, w2, ... , w n ;P(W) can be computed as:tlef t ) - -  1--\[p(w, I w~_t ..... wl)k=lFigure I.
Typical s t ructure  for Markov  model of  aphonetic unit.So, tbe task of the language model is to calculateP(WklWk.
I .
.
.
.
.
Wl) , that is, given the beginning of asentence Wl, w 2 .... , Wk_ 1 , to evaluate the probability ofwords in the vocabulary to be at place k in the sentence, or,in other terms, to estimate the probability of the word toappear in that context.if we ignore the language model (that meansconsidering words as equiprobable), it would be impossibleto distinguish omophones, (acoustically equivalent words),and it would be very hard to recognize correctly very similarwords on the basis of the acoustic information only.
Theestimation of probabilities could be based on grammaticaland semantic information, but a practical and easy way touse this approach has not been found yet.
For this reason,in our approach the language model is built up from theanalysis of statistical data.
They have been collected from ahuge set (corpus) of Italian sentences (in all, about 6 millionsof words).
Even using a small dictionary, no corpus cancontain all the possible contexts Wi_l, w i .2 , .
.
.
,  w 1 .
Theevaluation of the termP(W)= 1--\[P(w~ I w,._ t .
.
.
.
.
wt)is then based on the intuitive consideration that recentlyspoken words in a sentence have more influence than oldones on the continuation of the sentence itself.
Inparticular, we consider the probability of a word in acontext depending only on the two preceding words in thesentence:l ' (wk l  w~ I, "'t, ,_ .
.
.
.
.
wO= P(wklwk-t, Wk-2)Such a model it called trigram language model.
It is basedon a very simple idea and, for this reason, its statistics canbe built very easily only counting all the sequences of threeconsecutive words present in the corpus.
On the otherband, its predictive power is very high.
i f  the informationgiven by the language model were not available, in everycontext there would be uncertainty about the next wordamong all the 6500 words in the dictionary.
Using thetrigram model, uncertainty is, on the average, reduced to theFigure 2.
Typical s t ruc ture  for Markov  model of  aword81choice of a word among 100-110.
In the procedure ofestimating the language model statistics, a problem comesout: the probability of trigrams never observed in thecorpus must be evaluated.
For a 6500-word dictionary thenumber of different rigrams is about 270 billions; but froma corpus of 6 millions of words, only 6 millions of trigramscan be extracted, and not all of them are different.
It isclearly evident that, even with the availability of a biggercorpus, it is not possible to estimate probabilities oftrigrams by their relative frequencies.
Trigrams never seenin the corpus must be considered allowable, although notvery probable, otherwise it could be impossible to recognizea sentence containing one of them.
To overcome thisproblem, some techniques have been developed, giving agood estimate of probability of never observed events \[3\].Sentences in the corpus are taken from economy andfinance magazines, and, as a consequence, the model iscapable to work well on phrases about this topic, worse onother subjects.
Clearly, the availability of corpus on differenttopics could be very useful in order to use the languagemodel in different contexts.
Nevertheless, some studiesdemonstrate that language model could be still fruitfullyused for a matter different o the main one, if the collecteddata are enriched with a small corpus (about 1-2% thedimension of the main one) related to the new subject.
Thistechnique is used to allow the recognition of sentences noton finance and economy.Figure 3 shows the coverage of the corpus on texts ofeconomy and finance as a function of the vocabulary size.SEARCH STRATEGYTo find the word sequence W which maximizes theterm P(WIA)  , it is not feasible to consider all thesequences that can be built with words in the dictionary.For this reason an efficient search strategy is used that limitsthe investigation to a small fraction of the allowed wordstrings.
The sequences generable with the N words in thedictionary can be represented by a tree.
N branches,corresponding to the first word in the sentence, go out Fromthe root, one For each word in the dictionary.
Each branchends in a new node, From which other N branches aregenerated for the second word in the sentence, and so on.A node in the tree defines univocally a sequence of words,constituted by words corresponding to branches in the pathfrom the root to tile node itself.
During the recognitionprocess, tree nodes are explored, and, for each of them, theprobability (ac(~ustical nd linguistical) that the sentence willstart with the corresponding words is computed.
Nodeswith a low probability are discarded; among the remainingnodes, the path that seems, so Far, the more probable isextended.
This choice can be modified during the process,selecting at any time the best current path.
This strategy,usually called slack sequential decoding, leads, in general, tothe requested solution: the most probable sentence \[4\].The extension of a path from a node is done analyzingall the branches going out From it, that means all the wordsin the vocabulary.
It is computationally not practical todetermine the acoustic likelihood of each word through theforward pass algorithm.
The problem of a Fast access to agreat dictionary is one of the most important topics in100~ \[9s I90858O7570.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
$ .
- .
!i .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
/ .
.
.
.
.
.
.
.
.
.
.
.
.
!
-4.000 8000 12000Figure 3.
Coverage of  the corpus as a function of  vocabulary size.16000 2000082speech recognition.
Studies are conducted to find goodstrategies.
In our system, first a rough match is rapidlyconducted on the whole dictionary to select a subset ofwords.
Then, a more precise search is performed on thissubset with forward pass.
It has been seen that thisprocedure assures most of the times the identification of themost acoustically ikely word.The stack decoding algorithm conducts a left to rightsearch from the beginning to the end of the sentence,examining labels in the order they are produced by theacoustic front-end and it does not require in advance theknowledge of the whole label string.
Therefore, it is wellsuited to operate in real time.The search in the tree of all the possible solutions, alongwith the computation of acoustical and linguisticalprobabilities, is performed in the IBM 3090 mainframe.
Thisdictionarysize average best worst1000 92.2 95.1 89.53000 86.1 89.6 83.36500 82.0 86.4 78.0Table 1.
Recognition accuracy without language model.dictionarysize average best worst1000 97.9 98.5 96.43000 97.1 97.9 95.96500 96.3 94.9 97.4Table 2.
Recognition accuracy with language model\ [ i \ ]\[2\]REFERENCESBahl L.R., Jelinek F., Mercer R.L.
A MaximumLikelihood Approach to Continuous SpeechRecognition, IEEE Trans.
on Pattern Analysis andMachine Intelligence, vol.
PAMI-5, no.
2, 1983,pp.
179-190.Flanagan, J.L., Speech Analysis, Synthesis andPerception, Springer, New York, 1972.task is,~ in Fact, computationally so heavy that only thispowerful syslem can avoid the use of specialized processors.RESULTSSeveral experiments were conducted on the recognitionsystem with ten different speakers who had previouslytrained the system.
Each speaker dictated a text composedby natural anguage sentences about finance and economy.Recognition accuracy is always over 94%, and, on theaverage is 96%.
It has been seen that the language model iscapable to avoid about 10% of the errors made using onlythe acoustic model.
This shows the importance of using oflinguistic information.
"Fable t shows the recognition accuracy obtainedconsidering all tile words equiprobable for three dictionariesof different size, table 2 shows the results obtained for thesame test with Ihe language model.\[3\]\[4\]\[5\]\[6\]Nadas A. ,Estimation of Probabilities in tileLanguage Model of the IBM Speech RecognitionSystem, IEEE Trans.
on Acoustics, Speech, andSignal Processing, no.
4, ASSP-32 (1984), pp.859-861.Nilsson N.J. Problem-Solving Methods inArtificial Intelligence McGraw-ltill, New York,1971, pp.
43-79.Rabiner I..R., Juang B.I-I.
An Introduction toIlldden Markov Models, IEEE ASSP Magazine,no.
l, vol.
3, January 1986, pp.
4-16.Rabiner, I,.R., R.W.
Schafer, Digital Processing ofSpeech Signals, Prentice Hall, Englewood Cliffs,1978.83
