Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 12?13,Vancouver, October 2005.Pattern Visualization for Machine Translation OutputAdam LopezInstitute for Advanced Computer StudiesDepartment of Computer ScienceUniversity of MarylandCollege Park, MD 20742alopez@cs.umd.eduPhilip ResnikInstitute for Advanced Computer StudiesDepartment of LinguisticsUniversity of MarylandCollege Park, MD 20742resnik@umd.eduAbstractWe describe a method for identifying system-atic patterns in translation data using part-of-speech tag sequences.
We incorporate thisanalysis into a diagnostic tool intended for de-velopers of machine translation systems, anddemonstrate how our application can be usedby developers to explore patterns in machinetranslation output.1 IntroductionOver the last few years, several automatic metrics for ma-chine translation (MT) evaluation have been introduced,largely to reduce the human cost of iterative system evalu-ation during the development cycle (Papineni et al, 2002;Melamed et al, 2003).
All are predicated on the con-cept of n-gram matching between the sentence hypoth-esized by the translation system and one or more ref-erence translations?that is, human translations for thetest sentence.
Although the formulae underlying thesemetrics vary, each produces a single number represent-ing the ?goodness?
of the MT system output over a setof reference documents.
We can compare the numbers ofcompeting systems to get a coarse estimate of their rela-tive performance.
However, this comparison is holistic.It provides no insight into the specific competencies orweaknesses of either system.Ideally, we would like to use automatic methods to pro-vide immediate diagnostic information about the transla-tion output?what the system does well, and what it doespoorly.
At the most general level, we want to know howour system performs on the two most basic problems intranslation ?
word translation and reordering.
Holisticmetrics are at odds with day-to-day hypothesis testing onthese two problems.
For instance, during the develop-ment of a new MT system we may may wish to comparecompeting reordering models.
We can incorporate eachmodel into the system in turn, and rank the results on atest corpus using BLEU (Papineni et al, 2002).
We mightthen conclude that the model used in the highest-scoringsystem is best.
However, this is merely an implicit testof the hypothesis; it does not tell us anything about thespecific strengths and weaknesses of each method, whichmay be different from our expectations.
Furthermore, ifwe understand the relative strengths of each method, wemay be able to devise good ways to combine them, ratherthan simply using the best one, or combining strictly bytrial and error.
In order to fine-tune MT systems, we needfine-grained error analysis.What we would really like to know is how well thesystem is able to capture systematic reordering patternsin the input, which ones it is successful with, and whichones it has difficulty with.
Word n-grams are little helphere: they are too many, too sparse, and it is difficult todiscern general patterns from them.2 Part-of-Speech Sequence RecallIn developing a new analysis method, we are motivatedin part by recent studies suggesting that word reorder-ings follow general patterns with respect to syntax, al-though there remains a high degree of flexibility (Fox,2002; Hwa et al, 2002).
This suggests that in a com-parative analysis of two MT systems (or two versions ofthe same system), it may be useful to look for syntacticpatterns that one system (or version) captures well in thetarget language and the other does not, using a syntax-based, recall-oriented metric.As an initial step, we would like to summarize reorder-ing patterns using part-of-speech sequences.
Unfortu-nately, recent work has confirmed the intuition that ap-plying statistical analyzers trained on well-formed text tothe noisy output of MT systems produces unuseable re-sults (e.g.
(Och et al, 2004)).
Therefore, we make theconservative choice to apply annotation only to the refer-ence corpus.
Word n-gram correspondences with a refer-ence translation are used to infer the part-of-speech tagsfor words in the system output.The method:1.
Part-of-speech tag the reference corpus.
We used12Figure 1: Comparing two systems that differ significantly in their recall for POS n-gram JJ NN IN DT NN.
Theinterface uses color to make examples easy to find.MXPOST (Ratnaparkhi, 1996), and in order to dis-cover more general patterns, we map the tag setdown after tagging, e.g.
NN, NNP, NNPS and NNSall map to NN.2.
Compute the frequency freq(ti .
.
.
t j) of every possi-ble tag sequence ti .
.
.
t j in the reference corpus.3.
Compute the correspondence between each hypoth-esis sentence and each of its corresponding refer-ence sentences using an approximation to maximummatching (Melamed et al, 2003).
This algorithmprovides a list of runs or contiguous sequences ofwords ei .
.
.e j in the reference that are also present inthe hypothesis.
(Note that runs are order-sensitive.)4.
For each recalled n-gram ei .
.
.e j, look up the asso-ciated tag sequence ti .
.
.
t j and increment a counterrecalled(ti .
.
.
t j)Using this method, we compute the recall of tag pat-terns, R(ti .
.
.
t j) = recalled(ti .
.
.
t j)/freq(ti .
.
.
t j), for allpatterns in the corpus.To compare two systems (which could include two ver-sions of the same system), we identify POS n-grams thatare recalled significantly more frequently by one systemthan the other, using a difference-of-proportions test toassess statistical significance.
We have used this methodto analyze the output of two different statistical machinetranslation models (Chiang et al, 2005).3 VisualizationOur demonstration system uses an HTML interface tosummarize the observed pattern recall.
Based on frequentor significantly-different recall, the user can select andvisually inspect color-coded examples of each pattern ofinterest in context with both source and reference sen-tences.
An example visualization is shown in Figure 1.4 AcknowledgementsThe authors would like to thank David Chiang, ChristofMonz, and Michael Subotin for helpful commentary onthis work.
This research was supported in part by ONRMURI Contract FCPO.810548265 and Department ofDefense contract RD-02-5700.ReferencesDavid Chiang, Adam Lopez, Nitin Madnani, Christof Monz,Philip Resnik, and Michael Subotin.
2005.
The hiero ma-chine translation system: Extensions, evaluation, and analy-sis.
In Proceedings of HLT/EMNLP 2005, Oct.Heidi J.
Fox.
2002.
Phrasal cohesion and statistical ma-chine translation.
In Proceedings of the 2002 Conferenceon EMNLP, pages 304?311, Jul.Rebecca Hwa, Philip Resnik, Amy Weinberg, and Okan Kolak.2002.
Evaluating translational correspondence using annota-tion projection.
In Proceedings of the 40th Annual Meetingof the ACL, pages 392?399, Jul.I.
Dan Melamed, Ryan Green, and Joseph P. Turian.
2003.Precision and recall of machine translation.
In HLT-NAACL2003 Companion Volume, pages 61?63, May.Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, AnoopSarkar, Kenji Yamada, Alex Fraser, Shankar Kumar, LibinShen, David Smith, Katherine Eng, Viren Jain, Zhen Jin, andDragomir Radev.
2004.
A smorgasbord of features for sta-tistical machine translation.
In Proceedings of HLT-NAACL2004, pages 161?168, May.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-JingZhu.
2002.
BLEU: a method for automatic evaluation of ma-chine translation.
In Proceedings of the 40th Annual Meetingof the ACL, pages 311?318, Jul.Adwait Ratnaparkhi.
1996.
A maximum entropy model forpart-of-speech tagging.
In Proceedings of the Conference onEMNLP, pages 133?142, May.13
