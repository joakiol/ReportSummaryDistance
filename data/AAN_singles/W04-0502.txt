Evaluation of Restricted Domain Question-Answering SystemsAnne R. Diekema, Ozgur Yilmazel, and Elizabeth D. LiddyCenter for Natural Language ProcessingSchool of Information StudiesSyracuse University4-206 Center for Science and TechnologySyracuse, NY 13244{diekemar,liddy,oyilmaz}@syr.eduAbstractQuestion-Answering (QA) evaluation effortshave largely been tailored to open-domainsystems.
The TREC QA test collections containnewswire articles and the accompanying queriescover a wide variety of topics.
While someapprehension about the limitations of restricted-domain systems is no doubt justified, the strictpromotion of unlimited domain QA evaluationsmay have some unintended consequences.Simply applying the open domain QA evaluationparadigm to a restricted-domain system posesproblems in the areas of test questiondevelopment, answer key creation, and testcollection construction.
This paper examines theevaluation requirements of restricted domainsystems.
It incorporates evaluation criteriaidentified by users of an operational QA systemin the aerospace engineering domain.
While thepaper demonstrates that user-centered task-basedevaluations are required for restricted domainsystems, these evaluations are found to beequally applicable to open domain systems.1 IntroductionThe Text REtrieval Conference (TREC)organized the first QA evaluation (QA track) in1999 (Voorhees, 2000) and annual evaluations ofthis nature are ongoing (Voorhees, to appear).While the tasks and answer requirements havevaried slightly from year to year, the purposebehind QA evaluations remains the same: tomove from the traditional document retrieval toactual information retrieval by providing ananswer to a question rather than a ranked list ofrelevant documents.
The track was originallyintended to bring together the fields ofInformation Extraction (IE) and InformationRetrieval (IR).
This legacy still continues in thefactoid questions that require an IE type answersnippet in response, e.g.
: ?What country is theAswan High Dam located in??
This style of QAevaluation is spreading with very similarevaluations in Asia (Fukumoto, Kato, Masui,2003) and Europe (Magnini et al, 2003).Although these evaluations have a multilingualslant, they are strongly modeled after the TRECQA track.Typical QA systems that participate in theseevaluations classify the questions into typeswhich determine what kind of answer is required.After an initial retrieval of documents pertainingto the question, some form of text processing isthen applied to identify possible answersentences in the documents.
Sentences that arenear or contain keywords from the originalquestion and contain the desired answer patternare selected for answer extraction.
Since it isdifficult for systems to determine which part ofthe sentence is the correct answer, especially if itcontains multiple extractions of the desired type,many systems have resorted to redundancytactics (Banko et al, 2002; Buchholz, 2002).These systems use the Web as an answerverification tool by choosing the answer thatappears most often together with the questionkeywords.
While this technique is verysuccessful in open domain evaluations,restricted-domain systems do not have the luxuryof using redundancy, making these evaluationsinappropriate for systems such as these.Our QA system participated in the threeearlier TREC evaluations, e.g.
(Diekema et al,2002).
However, after starting work in therestricted-domain of re-usable launch vehicles,we found that the TREC evaluation no longersuited our system development needs andmaintaining two different QA systems was toocostly.2 Restricted-domain systemcharacteristicsThe restricted-domain systems of today aredifferent from the toy systems from the earlyyears of QA (Voorhees and Tice, 2000), whichmight be what first comes to mind when readingthe term ?restricted-domain?.
Early systems likeLUNAR (with a domain somewhat tangentiallyrelated to ours, namely lunar archeology) weredeveloped by researchers in the field of naturallanguage understanding.
These early systemsencoded large amounts of domain knowledge indatabases.
The restricted-domain systems oftoday are far less dependent on large knowledgebases and do not aim for language understandingper se.
Rather, they use specialized extractionrules on a domain specific collection.
The onething that both types of restricted-domainsystems have in common is that they are oftendeveloped with a certain goal or task in mind.As we will see later, this task orientationbecomes equally important in the evaluation ofthese QA systems.An example of a modern-day restricted-domain system is our Knowledge Acquisitionand Access System (KAAS) QA system.
TheKAAS was developed for use in a collaborativelearning environment (Advanced InteractiveDiscovery Environment for EngineeringEducation or AIDE) for undergraduate studentsfrom two universities majoring in aeronauticalengineering.
While students are working withinthe AIDE they can ask questions and quickly getanswers.
The collection against which thequestions are searched consists of textbooks,technical papers, and websites that have beenpre-selected for relevance and pedagogical value.The KAAS system uses a two-stage retrievalmodel to find answers in relevant passages.Relevant passages are processed by the Centerfor Natural Language Processing?s eQueryinformation extraction system using additionalrules in the domain of reusable launch vehicles.Users are aided in their question formulationsthrough domain specific query expansions.3 Initiating a restricted domainevaluationWhen it came time to evaluate the KAASsystem, we initially defaulted to the TREC styleQA evaluation with short, fact-based questions,adjudicated answers to these questions, and a testcollection in which to find those answers.
Thischoice of evaluation was not surprising sinceearly versions of our system grew out of thatenvironment.
However, it quickly becameapparent that this evaluation style posedproblems for our restricted-domain, specificpurpose system.Developing a set of test questions was easiersaid than done.
Unlike the open domainevaluations, where test questions can be minedfrom question logs (Encarta, Excite, AskJeeves),no question sets are at the disposal of restricted-domain evaluators.
To build a set of testquestions, we hired two sophomore aerospaceengineering students.
Based on class projectpapers of the previous semester and examples ofTREC questions, the students were asked tocreate as many short factoid questions as theycould, i.e ?What is APAS??
However, the realuser questions that we collected later did not lookanything like the short test questions in thisinitial evaluation set.
The user questions weremuch more complex, e.g.
?How difficult is it tomold and shape graphite-epoxies compared withalloys or ceramics that may be used for thermalprotective applications??
A more in depthanalysis of KAAS question types can be found inDiekema et al (to appear).Establishing answers for the initial testquestions proved difficult as well.
The studentsdid fine at collecting the questions that they hadwhile reading the papers, but lacked sufficientdomain expertise to establish answer correctness.Another issue was determining recall because itwasn?t always clear whether the (small) corpussimply did not contain the answer or whether thesystem was not able to find it.
A third student, adoctoral student in aerospace engineering, washired to help with these issues.
To facilitateautomatic evaluation we wanted to represent theanswers in simple patterns but found thatcomplex answers are not necessarily suitable forsuch a representation, even though patterns haveproven feasible for TREC systems.While a newswire document collection forgeneral domain evaluation is easy to find, acollection in our specialized domain needed to becreated from scratch.
Not only did the collectionof documents take time, the conversion of mostof these documents to text proved to be quite anunexpected hurdle as well.As is evident, the TREC style QA evaluationdid not suit our restricted domain system.
It alsoleaves out the user entirely.
While information-based evaluations are necessary to establish theability of the system to answer questionscorrectly, we felt that they were not sufficient forevaluating a system with real users.4 User-based evaluation dimensionsRestricted domain systems tend to be situatednot only within a specific domain, but also withina certain user community and within a specifictask domain.
A generic evaluation is neithersufficient nor suitable for a restricted domainsystem.
The environment in which KAAS issituated should drive the evaluation.
Unlikemany of the systems that participate in a TRECQA evaluation, the KAAS system has to functionin real time with real users, not in batch modewith surrogate relevance assessors.
This bringswith it additional evaluation criteria such asutility and system speed (Nyberg and Mitamura,2003).KAAS users were asked in two separatesurveys about their use and experiences with thesystem.
The surveys were part of larger scale,cross-university course evaluations which lookedat the students?
perceptions of distance learning,collaboration at a distance, the collaborativesoftware package, the KAAS, and eachparticipating faculty member.
While there wassome structure and guidance in the user survey ofthe QA system, it was minimal and the survey ismainly characterized by the open nature of theresponses.
There were 25 to 30 studentsparticipating in each full course survey, but sincewe do not have the actual surveys that wereturned in, we are not certain as to exactly howmany students completed the survey section onthe KAAS.
However, it appears that most, if notall of the students provided feedback.Given the free text nature of the responses, itwas decided that the three researchers would do acontent analysis of the responses andindependently derive a set of evaluationdimensions that they detected in the students?responses.
Through content analysis of the userresponses and follow-up discussion, weidentified 5 main areas of importance to KAASusers when using the system: systemperformance, answers, database content, display,and expectations (see Figure 1).
Each of thecategories will be described in more detailbelow.4.1 System PerformanceSystem Performance is the category that dealswith system speed and system availability.
Usersindicated that the speed with which answers werereturned to them mattered.
While they did notnecessarily expect an immediate answer, theyalso did not want to wait, e.g.
?took so long, so Igave up?.
Whenever users have a question, theywant to find an answer immediately.
If thesystem is down or not available to them at thatmoment, they will not come back later and tryagain.Possible system performance metrics are the?answer return rate?, and ?up time?.
The answerreturn rate measures how long it takes (onaverage) to return an answer after the user hassubmitted a question.
?Up-time?
measures for acertain time period how often the system isavailable (system available time divided by thelength of up-time time period).Figure 1: User-based evaluation dimensions.1 System Performance1.1 Speed1.2 Availability / reliability / upness2 Answers2.1 Completeness2.2 Accuracy2.3 Relevance2.4 Applicability to task / utility / usefulness3 Database Content3.1 Authority / provenance / Source quality3.2 Scope /extensiveness / coverage3.3 Size3.4 Updatedness4 Display (UI)4.1 Input4.1.1 Question understanding / info needunderstanding4.1.2 Querying style4.1.2.1 Question4.1.2.1.1 NL query4.1.2.2 Keywords4.1.2.3 Browsing4.1.3 Question formulation assistance4.1.3.1 Spell Checker4.1.3.2 Abbreviation recognition4.2 Output4.2.1 Organization4.2.2 Feedback Solicitation5 Expectations5.1 Googleness4.2 AnswersWhat users find important in an answer iscaptured in the Answers category.
The users notonly wanted answers to be accurate, they alsowanted them to be complete and, something thatis not tested at all in a regular evaluation,applicable to their task.
e.g.
?in general what Ireceived was helpful and accurate?, ?it [thesystem] was useful for the Columbia incidentexercise?
?.Possible metrics concerning answers are?accuracy or correctness?, ?completeness?,?relevance?, or ?task suitability?.
While the firstthree metrics are used in some shape or form inthe TREC evaluations, ?task suitability?
is not.Perhaps this measure requires a certain taskdescription with a question to test whether theanswer provided by the system allowed the userto complete the task.4.3 Database ContentUsers also shared thoughts about the DatabaseContent or source documents that are searchedfor answers.
They find it important that thesedocuments are reputable.
They also sharedconcerns about the size of the database, fearingthat a limit in size would restrict the number ofanswerable questions, e.g.
?it needs moredocuments?.
The same is true for the scope of thecollection.
Users desired extended coverage toensure that a wide range of questions could befielded by the collection, e.g.
?I found the datatoo limited in scope?.Possible database content metrics are?authority?, ?coverage?, ?size?, and ?up-to-dateness?.
To measure ?authority?
one wouldfirst have to identify the core authors for adomain through citation analysis.
Once that isestablished, one could measure the percentage ofdatabase content created by these coreresearchers.
?Coverage?
could be measured in asimilar way after the main research areas withina domain are identified.
?Size?
could simply bemeasured in megabytes or gigabytes.
?Up-to-dateness?
could be measured by calculating thenumber of articles per year or simply noting thedate of the most recent article.4.4 User InterfaceThe User Interface of a system was also foundof importance.
Users were critical about the waythey were asked to input their questions.
Theydid not always want to phrase their question as aquestion but sometimes preferred to usekeywords, e.g.
?a keyword search would be moreuseful?.
They also expected the system to promptthem with assistance in case they misspelledterms, or when the system did not understand thequestion, e.g ?sometimes very good at correctingyou to what you need, other times not verygood?.
Users also care about the way in whichthe results are presented to them and whether thesystem desires any additional responses fromthem.
They did not like being prompted forfeedback on a document?s relevance for example,e.g.
?
?the ?was this useful?
window wasdisruptive?.Measuring UI related aspects can be donethrough observation, questionnaires andinterviews and does not typically result in actualmetrics but rather a set of recommendations thatcan be implemented in the next version of thesystem.4.5 ExpectationsAnother interesting aspect of user criteria isExpectations , e.g.
?the documents in the e-Querydatabase were useful, but Google is muchfaster?.
All users are familiar with Google andtend to have little patience with systems thathave a different look and feel.Expectations can be captured by survey sothat it can be established whether theseexpectations are reasonable and whether they canbe met.5 Restricted domain QA EvaluationIf we consider a restricted domain QA systemas a system developed for a certain application, itis clear that these systems require a situatedevaluation.
The evaluation has to be situated inthe task, domain, and user community for whichthe system is developed.How then can a restricted domain system bestbe evaluated?
We believe that the evaluationshould be driven by the dimensions identified bythe users as important: system performance,answers, database content, display, andexpectations.The system should be evaluated on itsperformance.
How many seconds does it take toanswer a question?
Once the speed is known, onecan determine how long users are willing to waitfor an answer.
It may very well be that theanswer-finding capability of a system will needto be simplified in order to speed up the systemand satisfy its users.
Similarly, tests to determinerobustness need to be part of the systemperformance evaluation.
Users tend to shy awayfrom systems that are periodically unavailable orslow to a crawl during peak usage hours.Systems should also be evaluated on theiranswer providing ability.
This evaluation shouldinclude measures for answer completeness,accuracy, and relevancy.
Test questions shouldbe within the domain of the QA system in orderto test the answer quality for that domain.Answers to certain questions require a more fine-grained scoring procedure: answers that areexplanations or summaries or biographies orcomparative evaluations cannot be meaningfullyrated as simply right or wrong.
The answerproviding capability should be evaluated in lightof the task or purpose of the system.
Forexample, users of the KAAS are learners in thefield and are not well served with exact answersnippets.
For their task, they need answer contextinformation to be able to learn from the answertext.The evaluation should also include measuresof the Database Content.
Rather than assumingrelevancy of a collection, it should be evaluatedwhether the content is regularly updated, whetherthe contents are of acceptable quality to theusers, and whether the coverage of the restricteddomain is extensive enough.Another system component that should beevaluated is the User Interface.
Is the systemeasy to use?
Does the interface provide clearguidance and/or assistance to the user?
Does itallow users to search in multiple ways?Finally, it may be pertinent to evaluate howfar the system goes in living up to userexpectations.
Although it is impossible to satisfyeverybody, the system developers need to knowwhether there is a large discrepancy between userexpectations and the actual system, since thismay influence the use of the system.6 Cross-fertilization between evaluationsHow different are restricted-domainevaluations from open-domain evaluations?
Arethey so diametrically opposed that restricted-domain systems require separate evaluationsfrom open-domain systems and vice versa?
Aspointed out in Section 1, we stoppedparticipating in the TREC QA evaluationsbecause that evaluation was not well suited toour restricted-domain system.
However, weregretted this as we believe we could,nevertheless, have gained valuable insights.Clearly, open-domain systems would benefitfrom the evaluation dimensions discussed inSection 4.
The difference would be that the testquestions used for evaluation would be generalrather than tailored to a specific domain.Additionally, it may be harder to evaluate thedatabase content (i.e.
the collection) for a generaldomain system than would be the case forrestricted-domain systems.To make open-domain evaluations moreapplicable to restricted-domain systems, theycould be extended to include metrics aboutanswer speed, and the ability of answering withina certain task.
For example, the evaluation couldinclude system performance to get an indicationas to how much processing time, given certainhardware, is required in getting the answers.
Asfor answer correctness itself, it may beinteresting to require extensive use of taskscenarios that would determine aspects such asanswer length and level of detail.
It may also bedesirable to evaluate runs without redundancytechniques separately.
Ideally, users would beincorporated into the evaluation to assess the userinterface and the ability of the system to assistthem in completion of a certain task.7 SummaryRestricted-domain systems require a moresituated evaluation than is generally provided inopen-domain evaluations.
A restricted-domainevaluation extends beyond domain specific testquestions and collections to include the user andtheir task.
Users of the restricted-domain KAASsystem identified five areas that should beincluded in an evaluation: System Performance,Answers, Database Content, Display, andExpectations.
Most of these evaluationdimensions could be applied to open-domainevaluations as well.
Adding system performancemetrics (such as answer speed) and specific taskrequirements may allow a convergence betweenopen domain and restricted domain QAevaluations.AcknowledgementsFunding for this research has been jointly provided byNASA,  NY State, and AT&T.ReferencesBanko, M., Brill, E., Dumais, S. and Lin, J.
2002.AskMSR: Question answering using the worldwideWeb.
In Proceedings of the 2002 AAAI SpringSymposium on Mining Answers from Texts andKnowledge Bases, March 2002, Palo Alto,California.Buchholz, S. 2002.
Using Grammatical Relations,Answer Frequencies and the World Wide Web forTREC Question Answering.
In: E. M. Voorheesand D. K. Harman (Eds.
), The Tenth Text REtrievalConference (TREC 2001), volume 500-250 ofNIST Special Publication, Gaithersburg, MD.National Institute of Standards and Technology,2002, pp.
502-509.Diekema, A.R., Chen, J., McCracken, N, Ozgencil,N.E., Taffet, M.D., Yilmazel, O. and Liddy, E.D.2002.
Question Answering: CNLP at the TREC-2002 Question Answering Track.
In: Proceedingsof the Eleventh Text Retrieval Conference (TREC-2002).
E.M. Voorhees and D.K.
Harman (Eds.
).Gaithersburg, MD: Department of Commerce,National Institute of Standards and Technology,2002.Fukumoto, J., Kato, T., and Masui, F. 2003.
QuestionAnswering Challenge (QAC-1): An Evaluation ofQuestion Answering Tasks at the NTCIRWorkshop 3.
In Proceedings of the AAAI SpringSymposium: New Directions in QuestionAnswering, p.122-133, 2003.Diekema, A.R., Yilmazel, O., Chen, J., Harwell, S.,He, L., and Liddy, E.D.
Finding Answers toComplex Questions.
To appaer.
In Maybury, M.(Ed.)
New Directions in Question Answering.AAAI-MIT Press.Magnini, B., Romagnoli, S., Vallin, A., Herrera, J.Pe?as, A., Peinado, V., Verdejo, F., M. de Rijke,The Multiple Language Question Answering Trackat CLEF 2003.
In Carol Peters (Ed.
), WorkingNotes for the CLEF 2003 Workshop, 21-22 August,Trondheim, Norway, 2003.Nyberg E. and T. Mitamura.
2002.
Evaluating QASystems on Multiple Dimensions.
In Proceedingsof LREC 2002 Workshop on QA Strategy andResources, May 28th, Las Palmas, Gran Canaria.Voorhees, E.M. 2003.
DRAFT Overview of theTREC 2003 Question Answering Track.
To appearin Proceedings of TREC 2003.
Gaithersburg, MD,NIST, to appear.Voorhees, E.M. Overview of the TREC-8 QuestionAnswering Track Report.
In Proceedings of TREC-8, 77-82.
Gaithersburg, MD, NIST, 2000.Voorhees, E.M. & Tice, D.M.
Implementing aQuestion Answering Evaluation.
In Proceedings ofLREC?2000 Workshop on Using Evaluation withinHLT Programs: Results and Trends.
2000.
