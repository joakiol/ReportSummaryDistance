Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 494?498,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsLABR: A Large Scale Arabic Book Reviews DatasetMohamed AlyComputer Engineering DepartmentCairo UniversityGiza, Egyptmohamed@mohamedaly.infoAmir AtiyaComputer Engineering DepartmentCairo UniversityGiza, Egyptamir@alumni.caltech.eduAbstractWe introduce LABR, the largest sentimentanalysis dataset to-date for the Arabic lan-guage.
It consists of over 63,000 bookreviews, each rated on a scale of 1 to 5stars.
We investigate the properties of thethe dataset, and present its statistics.
Weexplore using the dataset for two tasks:sentiment polarity classification and rat-ing classification.
We provide standardsplits of the dataset into training and test-ing, for both polarity and rating classifica-tion, in both balanced and unbalanced set-tings.
We run baseline experiments on thedataset to establish a benchmark.1 IntroductionThe internet is full of platforms where users canexpress their opinions about different subjects,from movies and commercial products to booksand restaurants.
With the explosion of social me-dia, this has become easier and more prevalentthan ever.
Mining these troves of unstructured texthas become a very active area of research withlots of applications.
Sentiment Classification isamong the most studied tasks for processing opin-ions (Pang and Lee, 2008; Liu, 2010).
In its ba-sic form, it involves classifying a piece of opinion,e.g.
a movie or book review, into either having apositive or negative sentiment.
Another form in-volves predicting the actual rating of a review, e.g.predicting the number of stars on a scale from 1 to5 stars.Most of the current research has focused onbuilding sentiment analysis applications for theEnglish language (Pang and Lee, 2008; Liu, 2010;Korayem et al, 2012), with much less work onother languages.
In particular, there has beenlittle work on sentiment analysis in Arabic (Ab-basi et al, 2008; Abdul-Mageed et al, 2011;Abdul-Mageed et al, 2012; Abdul-Mageed andDiab, 2012b; Korayem et al, 2012), and veryfew, considerably small-sized, datasets to workwith (Rushdi-Saleh et al, 2011b; Rushdi-Saleh etal., 2011a; Abdul-Mageed and Diab, 2012a; Elar-naoty et al, 2012).
In this work, we try to addressthe lack of large-scale Arabic sentiment analysisdatasets in this field, in the hope of sparking moreinterest in research in Arabic sentiment analysisand related tasks.
Towards this end, we intro-duce LABR, the Large-scale Arabic Book Reviewdataset.
It is a set of over 63K book reviews, eachwith a rating of 1 to 5 stars.We make the following contributions: (1)We present the largest Arabic sentiment analy-sis dataset to-date (up to our knowledge); (2)We provide standard splits for the dataset intotraining and testing sets.
This will makecomparing different results much easier.
Thedataset and the splits are publicly available atwww.mohamedaly.info/datasets; (3) We explorethe structure and properties of the dataset, and per-form baseline experiments for two tasks: senti-ment polarity classification and rating classifica-tion.2 Related WorkA few Arabic sentiment analysis datasets havebeen collected in the past couple of years, we men-tion the relevant two sets:OCA Opinion Corpus for Arabic (Rushdi-Salehet al, 2011b) contains 500 movie reviews in Ara-bic, collected from forums and websites.
It is di-vided into 250 positive and 250 negative reviews,although the division is not standard in that there isno rating for neutral reviews i.e.
for 10-star ratingsystems, ratings above and including 5 are con-sidered positive and those below 5 are considerednegative.AWATIF is a multi-genre corpus for Mod-ern Standard Arabic sentiment analysis (Abdul-494Number of reviews 63,257Number of users 16,486Avg.
reviews per user 3.84Median reviews per user 2Number of books 2,131Avg.
reviews per book 29.68Median reviews per book 6Median tokens per review 33Max tokens per review 3,736Avg.
tokens per review 65Number of tokens 4,134,853Number of sentences 342,199Table 1: Important Dataset Statistics.1 2 3 4 5Rating0500010000150002000025000Numberof reviews2,939 5,28512,20119,05423,778Figure 1: Reviews Histogram.
The plot showsthe number of reviews for each rating.Mageed and Diab, 2012a).
It consists of about2855 sentences of news wire stories, 5342 sen-tences from Wikipedia talk pages, and 2532threaded conversations from web forums.3 Dataset CollectionWe downloaded over 220,000 reviews from thebook readers social network www.goodreads.comduring the month of March 2013.
These reviewswere from the first 2143 books in the list of BestArabic Books.
After harvesting the reviews, wefound out that over 70% of them were not in Ara-bic, either because some non-Arabic books exist inthe list, or because of existing translations of someof the books in other languages.
After filtering outthe non-Arabic reviews, and performing severalpre-processing steps to clean up HTML tags andother unwanted content, we ended up with 63,257Arabic reviews.4 Dataset PropertiesThe dataset contains 63,257 reviews that were sub-mitted by 16,486 users for 2,131 different books.Task Training Set Test Set1.
Polarity Classification B 13,160 3,288U 40,845 10,2112.
Rating Classification B 11,760 2,935U 50,606 12,651Table 2: Training and Test sets.
B stands for bal-anced, and U stands for Unbalanced.Table 1 contains some important facts about thedataset and Fig.
1 shows the number of reviewsfor each rating.
We consider as positive reviewsthose with ratings 4 or 5, and negative reviewsthose with ratings 1 or 2.
Reviews with rating 3are considered neutral and not included in the po-larity classification.
The number of positive re-views is much larger than that of negative reviews.We believe this is because the books we got re-views for were the most popular books, and thetop rated ones had many more reviews than the theleast popular books.The average user provided 3.84 reviews with themedian being 2.
The average book got almost 30reviews with the median being 6.
Fig.
2 showsthe number of reviews per user and book.
Asshown in the Fig.
2c, most books and users havefew reviews, and vice versa.
Figures 2a-b showa box plot of the number of reviews per user andbook.
We notice that books (and users) tend tohave (give) positive reviews than negative reviews,where the median number of positive reviews perbook is 5 while that for negative reviews is only 2(and similarly for reviews per user).Fig.
3 shows the statistics of tokens and sen-tences.
The reviews were tokenized and ?rough?sentence counts were computed (by looking forpunctuation characters).
The average number oftokens per review is 65.4, the average number ofsentences per review is 5.4, and the average num-ber of tokens per sentence is 12.
Figures 3a-bshow that the distribution is similar for positiveand negative reviews.
Fig.
3c shows a plot of thefrequency of the tokens in the vocabulary in a log-log scale, which conforms to Zipf?s law (Manningand Sch?tze, 2000).5 ExperimentsWe explored using the dataset for two tasks: (a)Sentiment polarity classification: where the goalis to predict if the review is positive i.e.
with rating4 or 5, or is negative i.e.
with rating 1 or 2; and (b)495All Pos Neg110100#reviews / user(a) UsersAll Pos Neg1101001000#reviews / book(b) Books100 101 102 103 104# reviews100101102103104# of users/books(c) Number of users/booksUsersBooksFigure 2: Users and Books Statistics.
(a) Box plot of the number of reviews per user for all, positive,and negative reviews.
The red line denotes the median, and the edges of the box the quartiles.
(b) thenumber of reviews per book for all, positive, and negative reviews.
(c) the number of books/users with agiven number of reviews.All Pos Neg50100150200#tokens / review(a) TokensAll Pos Neg5101520#sentences / review(b) Sentences100 101 102 103 104 105 106vocabulary token100101102103104105106frequency(c) VocabularyFigure 3: Tokens and Sentences Statistics.
(a) the number of tokens per review for all, positive, andnegative reviews.
(b) the number of sentences per review.
(c) the frequency distribution of the vocabularytokens.Rating classification: where the goal is to predictthe rating of the review on a scale of 1 to 5.To this end, we divided the dataset into separatetraining and test sets, with a ratio of 8:2.
We dothis because we already have enough training data,so there is no need to resort to cross-validation(Pang et al, 2002).
To avoid the bias of havingmore positive than negative reviews, we exploredtwo settings: (a) a balanced split where the num-ber of reviews from every class is the same, andis taken to be the size of the smallest class (wherelarger classes are down-sampled); (b) an unbal-anced split where the number of reviews from ev-ery class is unrestricted, and follows the distribu-tion shown in Fig.
1.
Table 2 shows the number ofreviews in the training and test sets for each of thetwo tasks for the balanced and unbalanced splits,while Fig.
4 shows the breakdown of these num-bers per class.Tables 3-4 show results of the experiments forboth tasks in both balanced/unbalanced settings.We tried different features: unigrams, bigrams,and trigrams with/without tf-idf weighting.
Forclassifiers, we used Multinomial Naive Bayes,Bernoulli Naive Bayes (for binary counts), andSupport Vector Machines.
We report two mea-sures: the total classification accuracy (percentageof correctly classified test examples) and weightedF1 measure (Manning and Sch?tze, 2000).
Allexperiments were implemented in Python usingscikit-learn (Pedregosa et al, 2011) and Qalsadi(available at pypi.python.org/pypi/qalsadi).We notice that: (a) The total accuracy andweighted F1 are quite correlated and go hand-in-hand.
(b) Task 1 is much easier than task 2, whichis expected.
(c) The unbalanced setting seems eas-496Features Tf-Idf Balanced UnbalancedMNB BNB SVM MNB BNB SVM1g No 0.801 / 0.801 0.807 / 0.807 0.766 / 0.766 0.887 / 0.879 0.889 / 0.876 0.880 / 0.877Yes 0.809 / 0.808 0.529 / 0.417 0.801 / 0.801 0.838 / 0.765 0.838 / 0.766 0.903 / 0.8951g+2g No 0.821 / 0.821 0.821 / 0.821 0.789 / 0.789 0.893 / 0.877 0.891 / 0.873 0.892 / 0.888Yes 0.822 / 0.822 0.513 / 0.368 0.818 / 0.818 0.838 / 0.765 0.837 / 0.763 0.910 / 0.9011g+2g+3g No 0.821 / 0.821 0.823 / 0.823 0.786 / 0.786 0.889 / 0.869 0.886 / 0.863 0.893 / 0.888Yes 0.827 / 0.827 0.511 / 0.363 0.821 / 0.820 0.838 / 0.765 0.837 / 0.763 0.910 / 0.901Table 3: Task 1: Polarity Classification Experimental Results.
1g means using the unigram model,1g+2g is using unigrams + bigrams, and 1g+2g+3g is using trigrams.
Tf-Idf indicates whether tf-idfweighting was used or not.
MNB is Multinomial Naive Bayes, BNB is Bernoulli Naive Bayes, and SVMis the Support Vector Machine.
The numbers represent total accuracy / weighted F1 measure.
See Sec.5.Features Tf-Idf Balanced UnbalancedMNB BNB SVM MNB BNB SVM1g No 0.393 / 0.392 0.395 / 0.396 0.367 / 0.365 0.465 / 0.445 0.464 / 0.438 0.460 / 0.454Yes 0.402 / 0.405 0.222 / 0.128 0.387 / 0.384 0.430 / 0.330 0.379 / 0.229 0.482 / 0.4721g+2g No 0.407 / 0.408 0.418 / 0.421 0.383 / 0.379 0.487 / 0.460 0.487 / 0.458 0.472 / 0.466Yes 0.419 / 0.423 0.212 / 0.098 0.411 / 0.407 0.432 / 0.325 0.379 / 0.217 0.501 / 0.4901g+2g+3g No 0.405 / 0.408 0.417 / 0.420 0.384 / 0.381 0.487 / 0.457 0.484 / 0.452 0.474 / 0.467Yes 0.426 / 0.431 0.211 / 0.093 0.410 / 0.407 0.431 / 0.322 0.379 / 0.216 0.503 / 0.491Table 4: Task 2: Rating Classification Experimental Results.
See Table 3 and Sec.
5.Negative Positive01000020000300004000050000# reviews1,6446,5801,6706,5541,6446,5808,54134,291(a) Polarity ClassificationTraining - balancedTesting - balancedTraining - unbalancedTesting - unbalanced1 2 3 4 5Rating0500010000150002000025000# reviews5872,352 6022,337 5872,3521,0884,197 5872,3522,3609,841 5872,3523,83815,2165872,3524,76319,015(b) Rating ClassificationTraining - balancedTesting - balancedTraining - unbalancedTesting - unbalancedFigure 4: Training-Test Splits.
(a) Histogram ofthe number of training and test reviews for the po-larity classification task for balanced (solid) andunbalanced (hatched) cases.
(b) The same for therating classification task.
In the balanced set, allclasses have the same number of reviews as thesmallest class, which is done by down-samplingthe larger classes.ier than the balanced one.
This might be becausethe unbalanced sets contain more training exam-ples to make use of.
(d) SVM does much betterin the unbalanced setting, while MNB is slightlybetter than SVM in the balanced setting.
(e) Usingmore ngrams helps, and especially combined withtf-idf weighting, as all the best scores are with tf-idf.6 Conclusion and Future WorkIn this work we presented the largest Arabic sen-timent analysis dataset to-date.
We explored itsproperties and statistics, provided standard splits,and performed several baseline experiments to es-tablish a benchmark.
Although we used very sim-ple features and classifiers, task 1 achieved quitegood results (~90% accuracy) but there is muchroom for improvement in task 2 (~50% accuracy).We plan next to work more on the dataset toget sentence-level polarity labels, and to extractArabic sentiment lexicon and explore its poten-tial.
Furthermore, we also plan to explore usingArabic-specific and more powerful features.497ReferencesAhmed Abbasi, Hsinchun Chen, and Arab Salem.2008.
Sentiment analysis in multiple languages:Feature selection for opinion classification in webforums.
ACM Transactions on Information Systems(TOIS).Muhammad Abdul-Mageed and Mona Diab.
2012a.Awatif: A multi-genre corpus for modern standardarabic subjectivity and sentiment analysis.
In Pro-ceedings of the Eight International Conference onLanguage Resources and Evaluation.Muhammad Abdul-Mageed and Mona Diab.
2012b.Toward building a large-scale arabic sentiment lexi-con.
In Proceedings of the 6th International GlobalWord-Net Conference.Muhammad Abdul-Mageed, Mona Diab, and Mo-hammed Korayem.
2011.
Subjectivity and senti-ment analysis of modern standard arabic.
In 49thAnnual Meeting of the Association for Computa-tional Linguistics: Human Language Technologies.Muhammad Abdul-Mageed, Sandra K?bler, and MonaDiab.
2012.
Samar: A system for subjectivity andsentiment analysis of arabic social media.
In Pro-ceedings of the 3rd Workshop in Computational Ap-proaches to Subjectivity and Sentiment Analysis.Mohamed Elarnaoty, Samir AbdelRahman, and AlyFahmy.
2012.
A machine learning approach foropinion holder extraction in arabic language.
arXivpreprint arXiv:1206.1011.Mohammed Korayem, David Crandall, and Muham-mad Abdul-Mageed.
2012.
Subjectivity and sen-timent analysis of arabic: A survey.
In AdvancedMachine Learning Technologies and Applications.Bing Liu.
2010.
Sentiment analysis and subjectivity.Handbook of Natural Language Processing.Christopher D. Manning and Hinrich Sch?tze.
2000.Foundations of Statistical Natural Language Pro-cessing.
MIT Press.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and Trends in In-formation Retrieval, 2:1?135.B.
Pang, L. Lee, and S. Vaithyanathan.
2002.
Thumbsup?
: Sentiment classification using machine learn-ing techniques.
In EMNLP.F.
Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,B.
Thirion, O. Grisel, M. Blondel, P. Prettenhofer,R.
Weiss, V. Dubourg, J. Vanderplas, A. Passos,D.
Cournapeau, M. Brucher, M. Perrot, and E. Duch-esnay.
2011.
Scikit-learn: Machine Learning inPython .
Journal of Machine Learning Research,12:2825?2830.M.
Rushdi-Saleh, M. Mart?n-Valdivia, L. Ure?a-L?pez,and J. Perea-Ortega.
2011a.
Bilingual experimentswith an arabic-english corpus for opinion mining.In Proceedings of Recent Advances in Natural Lan-guage Processing (RANLP).M.
Rushdi-Saleh, M. Mart?n-Valdivia, L. Ure?a-L?pez,and J. Perea-Ortega.
2011b.
Oca: Opinion corpusfor arabic.
Journal of the American Society for In-formation Science and Technology.498
