Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 418?427,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsDivide and Translate: Improving Long Distance Reordering in StatisticalMachine TranslationKatsuhito Sudoh, Kevin Duh, Hajime Tsukada, Tsutomu Hirao, Masaaki NagataNTT Communication Science Laboratories2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0237, Japansudoh@cslab.kecl.ntt.co.jpAbstractThis paper proposes a novel methodfor long distance, clause-level reorderingin statistical machine translation (SMT).The proposed method separately translatesclauses in the source sentence and recon-structs the target sentence using the clausetranslations with non-terminals.
The non-terminals are placeholders of embeddedclauses, by which we reduce complicatedclause-level reordering into simple word-level reordering.
Its translation modelis trained using a bilingual corpus withclause-level alignment, which can be au-tomatically annotated by our alignmentalgorithm with a syntactic parser in thesource language.
We achieved signifi-cant improvements of 1.4% in BLEU and1.3% in TER by using Moses, and 2.2%in BLEU and 3.5% in TER by usingour hierarchical phrase-based SMT, forthe English-to-Japanese translation of re-search paper abstracts in the medical do-main.1 IntroductionOne of the common problems of statistical ma-chine translation (SMT) is to overcome the differ-ences in word order between the source and targetlanguages.
This reordering problem is especiallyserious for language pairs with very different wordorders, such as English-Japanese.
Many previousstudies on SMT have addressed the problem byincorporating probabilistic models into SMT re-ordering.
This approach faces the very large com-putational cost of searching over many possibili-ties, especially for long sentences.
In practice thesearch can be made tractable by limiting its re-ordering distance, but this also renders long dis-tance movements impossible.
Some recent stud-ies avoid the problem by reordering source wordsprior to decoding.
This approach faces difficul-ties when the input phrases are long and requiresignificant word reordering, mainly because theirreordering model is not very accurate.In this paper, we propose a novel method fortranslating long sentences that is different fromthe above approaches.
Problematic long sentencesoften include embedded clauses1 such as rela-tive clauses.
Such an embedded (subordinate)clause can usually be translated almost indepen-dently of words outside the clause.
From thisviewpoint, we propose a divide-and-conquer ap-proach: we aim to translate the clauses sepa-rately and reconstruct the target sentence using theclause translations.
We first segment a source sen-tence into clauses using a syntactic parser.
Theclauses can include non-terminals as placeholdersfor nested clauses.
Then we translate the clauseswith a standard SMT method, in which the non-terminals are reordered as words.
Finally we re-construct the target sentence by replacing the non-terminals with their corresponding clause transla-tions.
With this method, clause-level reordering isreduced to word-level reordering and can be dealtwith efficiently.
The models for clause translationare trained using a bilingual corpus with clause-level alignment.
We also present an automaticclause alignment algorithm that can be applied tosentence-aligned bilingual corpora.In our experiment on the English-to-Japanesetranslation of multi-clause sentences, the proposedmethod improved the translation performance by1.4% in BLEU and 1.3% in TER by using Moses,and by 2.2% in BLEU and 3.5% in TER by usingour hierarchical phrase-based SMT.The main contribution of this paper is two-fold:1Although various definitions of a clause can beconsidered, this paper follows the definition of ?S?
(sentence) in Enju.
It basically follows the Penn Tree-bank II scheme but also includes SINV, SQ, SBAR.
Seehttp://www-tsujii.is.s.u-tokyo.ac.jp/enju/enju-manual/enju-output-spec.html#correspondence for details.4181.
We introduce the idea of explicit separa-tion of in-clause and outside-clause reorder-ing and reduction of outside-clause reorder-ing into common word-level reordering.2.
We propose an automatic clause alignmentalgorithm, by which our approach can beused without manual clause-level alignment.This paper is organized as follows.
The nextsection reviews related studies on reordering.
Sec-tion 3 describes the proposed method in detail.Section 4 presents and discusses our experimen-tal results.
Finally, we conclude this paper withour thoughts on future studies.2 Related WorkReordering in SMT can be roughly classified intotwo approaches, namely a search in SMT decod-ing and preprocessing.The former approach is a straightforward waythat models reordering in noisy channel transla-tion, and has been studied from the early periodof SMT research.
Distance-based reordering is atypical approach used in many previous studies re-lated to word-based SMT (Brown et al, 1993) andphrase-based SMT (Koehn et al, 2003).
Alongwith the advances in phrase-based SMT, lexical-ized reordering with a block orientation model wasproposed (Tillmann, 2004; Koehn et al, 2005).This kind of reordering is suitable and commonlyused in phrase-based SMT.
On the other hand,a syntax-based SMT naturally includes reorder-ing in its translation model.
A lot of researchwork undertaken in this decade has used syntac-tic parsing for linguistically-motivated translation.
(Yamada and Knight, 2001; Graehl and Knight,2004; Galley et al, 2004; Liu et al, 2006).
Wu(1997) and Chiang (2007) focus on formal struc-tures that can be extracted from parallel corpora,instead of a syntactic parser trained using tree-banks.
These syntactic approaches can theoret-ically model reordering over an arbitrary length,however, long distance reordering still faces thedifficulty of searching over an extremely largesearch space.The preprocessing approach employs deter-ministic reordering so that the following trans-lation process requires only short distance re-ordering (or even a monotone).
Several previ-ous studies have proposed syntax-driven reorder-ing based on source-side parse trees.
Xia andMcCord (2004) extracted reordering rules auto-matically from bilingual corpora for English-to-French translation; Collins et al (2005) usedlinguistically-motivated clause restructuring rulesfor German-to-English translation; Li et al (2007)modeled reordering on parse tree nodes by us-ing a maximum entropy model with surface andsyntactic features for Chinese-to-English trans-lation; Katz-Brown and Collins (2008) applieda very simple reverse ordering to Japanese-to-English translation, which reversed the word orderin Japanese segments separated by a few simplecues; Xu et al (2009) utilized a dependency parserwith several hand-labeled precedence rules for re-ordering English to subject-object-verb order likeKorean and Japanese.
Tromble and Eisner (2009)proposed another reordering approach based on alinear ordering problem over source words with-out a linguistically syntactic structure.
These pre-processing methods reorder source words closeto the target-side order by employing language-dependent rules or statistical reordering modelsbased on automatic word alignment.
Althoughthe use of language-dependent rules is a naturaland promising way of bridging gaps between lan-guages with large syntactic differences, the rulesare usually unsuitable for other language groups.On the other hand, statistical methods can be ap-plied to any language pairs.
However, it is verydifficult to reorder all source words so that they aremonotonic with the target words.
This is becauseautomatic word alignment is not usually reliableowing to data sparseness and the weak modelingof many-to-many word alignments.
Since sucha reordering is not complete or may even harmword ordering consistency in the source language,these previous methods further applied reorderingin their decoding.
Li et al (2007) used N-bestreordering hypotheses to overcome the reorderingambiguity.Our approach is different from those of previousstudies that aim to perform both short and long dis-tance reordering at the same time.
The proposedmethod distinguishes the reordering of embeddedclauses from others and efficiently accomplishes itby using a divide-and-conquer framework.
The re-maining (relatively short distance) reordering canbe realized in decoding and preprocessing by themethods described above.
The proposed frame-work itself does not depend on a certain languagepair.
It is based on the assumption that a source419language clause is translated to the correspondingtarget language clause as a continuous segment.The only language-dependent resource we need isa syntactic parser of the source language.
Notethat clause translation in the proposed method is astandardMT problem and therefore any reorderingmethod can be employed for further improvement.This work is inspired by syntax-based meth-ods with respect to the use of non-terminals.
Ourmethod can be seen as a variant of tree-to-stringtranslation that focuses only on the clause struc-ture in parse trees and independently translates theclauses.
Although previous syntax-based methodscan theoretically model this kind of derivation, itis practically difficult to decode long multi-clausesentences as described above.Our approach is also related to sentence sim-plification and is intended to obtain simple andshort source sentences for better translation.
Kimand Ehara (1994) proposed a rule-based methodfor splitting long Japanese sentences for Japanese-to-English translation; Furuse et al (1998) useda syntactic structure to split ill-formed inputs inspeech translation.
Their splitting approach splitsa sentence sequentially to obtain short segments,and does not undertake their reordering.Another related field is clause identification(Tjong et al, 2001).
The proposed method is notlimited to a specific clause identification methodand any method can be employed, if their clausedefinition matches the proposed method whereclauses are independently translated.3 Proposed MethodThe proposed method consists of the followingsteps illustrated in Figure 1.During training:1) clause segmentation of source sentences witha syntactic parser (section 3.1)2) alignment of target words with source clausesto develop a clause-level aligned corpus (section3.2)3) training the clause translation models usingthe corpus (section 3.3)During testing:1) clause translation with the clause translationmodels (section 3.4)2) sentence reconstruction based on non-terminals (section 3.5)BilingualCorpus(Training)sourcetargetparse & clausesegmentationparse &clausesegmen-tationSource Sentences(clause-segmented)Word AlignmentModelTarget Word BigramLanguage ModelLM trainingwordalignmentBilingual Corpus(clause-aligned)automatic clause alignmentClauseTranslation Models(Phrase Table, N-gram LMs, ...)training from scratchBilingualCorpus(Development)(clause-segmented)MERTTest SentenceSentenceTranslationclauseclauseclauseclausetranslationclausetranslationclausetranslationsentence reconstructionbased on non-terminalstranslationOriginal (sentence-aligned)corpus can also be usedFigure 1: Overview of proposed method.3.1 Clause Segmentation of Source SentencesClauses in source sentences are identified by asyntactic parser.
Figure 2 shows a parse tree forthe example sentence below.
The example sen-tence has a relative clause modifying the nounbook.
Figure 3 shows the word alignment of thisexample.English: John lost the book that was borrowedlast week from Mary.Japanese: john wa (topic marker) senshu (lastweek) mary kara (from) kari (borrow) ta(past tense marker) hon (book) o (direct ob-ject marker) nakushi (lose) ta (past tensemarker) .We segment the source sentence at the clause leveland the example is rewritten with two clauses asfollows.?
John lost the book s0 .?
that was borrowed last week from Marys0 is a non-terminal symbol the serves as a place-holder of the relative clause.
We allow an arbitrary420SSJohnlostthebookthatwasborrowedfrom Marylast weekFigure 2: Parse tree for example English sentence.Node labels are omitted except S.JohnlostthebookthatwasborrowedfromMarylastweekjohnwatanakushiohontakarikaramarysenshuFigure 3: Word alignment for example bilingualsentence.number of non-terminals in each clause2.
A nestedclause structure can be represented in the samemanner using such non-terminals recursively.3.2 Alignment of Target Words with SourceClausesTo translate source clauses with non-terminal sym-bols, we need models trained using a clause-levelaligned bilingual corpus.
A clause-level alignedcorpus is defined as a set of parallel, bilingualclause pairs including non-terminals that representembedded clauses.We assume that a sentence-aligned bilingualcorpus is available and consider the alignment oftarget words with source clauses.
We can manu-ally align these Japanese words with the Englishclauses as follows.?
john wa s0 hon o nakushi ta .2In practice not so many clauses are embedded in a singlesentence but we found some examples with nine embeddedclauses for coordination in our corpora.John lost the book s0 .?
senshu mary kara kari tathat was borrowed last week from MarySince the cost of manual clause alignment ishigh especially for a large-scale corpus, a natu-ral question to ask is whether this resource can beobtained from a sentence-aligned bilingual corpusautomatically with no human input.
To answerthis, we now describe a simple method for deal-ing with clause alignment data from scratch, us-ing only the word alignment and language modelprobabilities inferred from bilingual and monolin-gual corpora.Our method is based on the idea that automaticclause alignment can be viewed as a classificationproblem: for an English sentence with N words (e= (e1, e2, .
.
.
, eN )) andK clauses (e?1,e?2,.
.
.
,e?K),and its Japanese translation with M words (f= (f1, f2, .
.
.
, fM )), the goal is to classify eachJapanese word into one of {1, .
.
.
,K} classes.
In-tuitively, the probability that a Japanese word fmis assigned to class k ?
{1, .
.
.
,K} depends ontwo factors:1.
The probability of translating fm into the En-glish words of clause k (i.e.
?e?e?k p(e|fm)).We expect fm to be assigned to a clausewhere this value is high.2.
The language model probability(i.e.
p(fm|fm?1)).
If this value is high,we expect fm and fm?1 to be assigned to thesame clause.We implement this intuition using a graph-based method.
For each English-Japanese sen-tence pair, we construct a graph with K clausenodes (representing English clauses) and M wordnodes (representing Japanese words).
The edgeweights between word and clause nodes are de-fined as the sum of lexical translation probabilities?e?e?k p(e|fm).
The edge weights between wordsare defined as the bigram probability p(fm|fm?1).Each clause node is labeled with a class ID k ?
{1, .
.
.
,K}.
We then propagate these K labelsalong the graph to label the M word nodes.
Fig-ure 4 shows the graph for the example sentence.Many label propagation algorithms are avail-able.
The important thing is to use an algo-rithm that encourages node pairs with strong edgeweights to receive the same label.
We use the labelpropagation algorithm of (Zhu et al, 2003).
If we421John  lost  the  book  that  was  borrowed ...clause(1) clause(2)John Mary fromlast weektopicmarkerp(John |           )+ p(lost |           )+ ...p(that |        )+ p(was |        )+ ...p(     |         ) p(         |            ) p(        |         )p(            |     )john karakarajohnjohn wa senshu mary karawa  john senshu  wa mary  senshu kara maryFigure 4: Graph-based representation of the ex-ample sentence.
We propagate the clause labels tothe Japanese word nodes on this graph to form theclause alignments.assume the labels are binary, the following objec-tive is minimized:argminl?RK+M?i,jwij(li ?
lj)2 (1)where wij is the edge weight between nodes iand j (1 ?
i ?
K + M , 1 ?
j ?
K +M ), and l (li ?
{0, 1}) is a vector of labelson the nodes.
The first K elements of l, lc =(l1, l2, ..., lK)T , are constant because the clausenodes are pre-labeled.
The remaining M ele-ments, lf = (lK+1, lK+2, ..., lK+M )T , are un-known and to be determined.
Here, we considerthe decomposition of the weight matrixW = [wij ]into four blocks after the K-th row and column asfollows:W =[W cc W cfW fc W ff](2)The solution of eqn.
(1), namely lf , is given by thefollowing equation:lf = (Dff ?W ff )?1W fc lc (3)where D is the diagonal matrix with di =?j wijand is decomposed similarly to W .
Each elementof lf is in the interval (0, 1) and can be regardedas the label propagation probability.
A detailed ex-planation of this solution can be found in Section 2of (Zhu et al, 2003).
For our multi-label problemwith K labels, we slightly modified the algorithmby expanding the vector l to an (M + K) ?
Kbinary matrix L = [ l1 l2 ... lK ].After the optimization, we can normalize Lfto obtain the clause alignment scores t(lm =k|fm) between each Japanese word fm and En-glish clause k. Theoretically, we can simply out-put the clause id k?
for each fm by finding k?
=argmaxk t(lm = k|fm).
In practice, this maysometimes lead to Japanese clauses that have toomany gaps, so we employ a two-stage procedureto extract clauses that are more contiguous.First, we segment the Japanese sentence into Kclauses based on a dynamic programming algo-rithm proposed by Malioutov and Barzilay (2006).We define an M ?
M similarity matrix S = [sij ]with sij = exp(?||li?lj ||) where li is (K + i)-throw vector in the label matrix L. sij representsthe similarity between the i-th and j-th Japanesewords with respect to their clause alignment scoredistributions; if the score distributions are sim-ilar then sij is large.
The details of this algo-rithm can be found in (Malioutov and Barzilay,2006).
The clause segmentation gives us contigu-ous Japanese clauses f?1, f?2, ..., f?K , thus min-imizing inter-segment similarity and maximizingintra-segment similarity.
Second, we determinethe clause labels of the segmented clauses, basedon clause alignment scores T = [Tkk? ]
for Englishand automatically-segmented Japanese clauses:Tkk?
=?fm?f?
k?t(lm = k|fm) (4)where f?k?
is the j?-th Japanese clause.
In descend-ing order of the clause alignment score, we greed-ily determine the clause label 3.3.3 Training Clause Translation ModelsWe train clause translation models using theclause-level aligned corpus.
In addition we canalso include the original sentence-aligned corpus.We emphasize that we can use standard techniquesfor heuristically extracted phrase tables, word n-gram language models, and so on.3.4 Clause TranslationBy using the source language parser, a multi-clause source sentence is reduced to a set ofclauses.
We translate these clauses with a commonSMT method using the clause translation models.Here we present another English example Ibought the magazine which Tom recommendedyesterday.
This sentence is segmented into clausesas follows.3Although a full search is available when the number ofclauses is small, we employ a greedy search in this paper.422?
I bought the magazine s0 .?
which Tom recommended yersterdayThese clauses are translated into Japanese:?
watashi (I) wa (topic marker) s0zasshi (magazine) o (direct object marker)kat (buy) ta (past tense marker).?
tom ga (subject marker) kino (yesterday)susume (recommend) ta (past tense marker)3.5 Sentence ReconstructionWe reconstruct the target sentence from the clausetranslations, based on non-terminals.
Startingfrom the clause translation of the top clause, we re-cursively replace non-terminal symbols with theircorresponding clause translations.
Here, if a non-terminal is eventually deleted in SMT decoding,we simply concatenate the translation behind itsparent clause.Using the example above, we replace the non-terminal symbol s0 with the second clause andobtain the Japanese sentence:watashi wa tom ga kino susume ta zasshi o kat ta .4 ExperimentWe conducted the following experiments on theEnglish-to-Japanese translation of research paperabstracts in the medical domain.
Such techni-cal documents are logically and formally writ-ten, and sentences are often so long and syntac-tically complex that their translation needs longdistance reordering.
We believe that the medicaldomain is suitable as regards evaluating the pro-posed method.4.1 ResourcesOur bilingual resources were taken from the med-ical domain.
The parallel corpus consisted ofresearch paper abstracts in English taken fromPubMed4 and the corresponding Japanese transla-tions.The training portion consisted of 25,500 sen-tences (no-clause-seg.
; original sentences with-out clause segmentation).
4,132 English sen-tences in the corpus were composed of multi-ple clauses and were separated at the clause level4http://www.ncbi.nlm.nih.gov/pubmed/by the procedure in section 3.1.
As the syntac-tic parser, we used the Enju5 (Miyao and Tsu-jii, 2008) English HPSG parser.
For these train-ing sentences, we automatically aligned Japanesewords with each English clause as described insection 3.2 and developed a clause-level alignedcorpus, called auto-aligned corpus.
We preparedmanually-aligned (oracle) clauses for reference,called oracle-aligned clauses.
The clause align-ment error rate of the auto-aligned corpus was14% (number of wrong clause assignments di-vided by total number of words).
The develop-ment and test portions each consisted of 1,032multi-clause sentences.
because this paper focusesonly on multi-clause sentences.
Their English-side was segmented into clauses in the same man-ner as the training sentences, and the developmentsentences had oracle clause alignment for MERT.We also used the Life Science Dictionary6 fortraining.
We extracted 100,606 unique Englishentries from the dictionary including entries withmultiple translation options, which we expandedto one-to-one entries, and finally we obtained155,692 entries.English-side tokenization was obtained usingEnju, and we applied a simple preprocessing thatremoved articles (a, an, the) and normalized plu-ral forms to singular ones.
Japanese-side tokeniza-tion was obtained using MeCab7 with ComeJisyo8(dictionary for Japanese medical document tok-enization).
Our resource statistics are summarizedin Table 1.4.2 Model and DecoderWe used two decoders in the experiments,Moses9 (Koehn et al, 2007) and our in-house hierarchical phrase-based SMT (almostequivalent to Hiero (Chiang, 2007)).
Mosesused a phrase table with a maximum phraselength of 7, a lexicalized reordering model withmsd-bidirectional-fe, and a distortionlimit of 1210.
Our hierarchical phrase-based SMTused a phrase table with a maximum rule length of7 and a window size (Hiero?s ?)
of 12 11.
Both5http://www-tsujii.is.s.u-tokyo.ac.jp/enju/index.html6http://lsd.pharm.kyoto-u.ac.jp/en/index.html7http://mecab.sourceforge.net/8http://sourceforge.jp/projects/comedic/ (in Japanese)9http://www.statmt.org/moses/10Unlimited distortion was also tested but the results wereworse.11A larger window size could not be used due to its mem-ory requirements.423Table 1: Data statistics on training, development,and test sets.
All development and test sentencesare multi-clause sentences.TrainingCorpus Type #words #sentencesParallel E 690,536(no-clause-seg.)
J 942,91325,550Parallel E 135,698(auto-aligned) J 183,0434,132(oracle-aligned) J 183,147(10,766 clauses)E 263,175 155.692DictionaryJ 291,455 (entries)DevelopmentCorpus Type #words #sentencesParallel E 34,417 1,032(oracle-aligned) J 46,480 (2,683 clauses)TestCorpus Type #words #sentencesParallel E 34,433 1,032(clause-seg.)
J 45,975 (2,737 clauses)decoders employed two language models: a word5-gram language model from the Japanese sen-tences in the parallel corpus and a word 4-gramlanguage model from the Japanese entries in thedictionary.
The feature weights were optimizedfor BLEU (Papineni et al, 2002) by MERT, usingthe development sentences.4.3 Compared MethodsWe compared four different training and test con-ditions with respect to the use of clauses in trainingand testing.
The development (i.e., MERT) condi-tions followed the test conditions.
Two additionalconditions with oracle clause alignment were alsotested for reference.Table 2 lists the compared methods.
First,the proposed method (proposed) used the auto-aligned corpus in training and clause segmen-tation in testing.
Second, the baseline method(baseline) did not use clause segmentation in ei-ther training or testing.
Using this standard base-line method, we focused on the advantages of thedivide-and-conquer translation itself.
Third, wetested the same translation models as used withthe proposed method for test sentences withoutclause segmentation, (comp.(1)).
Although thiscomparison method cannot employ the proposedclause-level reordering, it was expected to be bet-ter than the baseline method because its transla-tion model can be trained more precisely using thefinely aligned clause-level corpus.
Finally, the sec-ond comparison method (comp.
(2)) translated seg-mented clauses with the baseline (without clausesegmentation) model, as if each of them was a sin-gle sentence.
Its translation of each clause wasexpected to be better than that of the baseline be-cause of the efficient search over shortened inputs,while its reordering of clauses (non-terminals) wasunreliable due to the lack of clause informationin training.
Its sentence reconstruction based onnon-terminals was the same as with the proposedmethod.
Although non-terminals in the secondcomparison method were out-of-vocabulary wordsand may be deleted in decoding, all of them sur-vived and we could reconstruct sentences fromtranslated clauses throughout the experiments.
Inaddition, two other conditions were tested: us-ing oracle-aligned clauses in training: the pro-posed method trained using oracle-aligned (ora-cle) clauses and the first comparison method usingoracle-aligned (oracle-comp.)
clauses.4.4 ResultsTable 3 shows the results in BLEU, Transla-tion Edit Rate (TER) (Snover et al, 2006),and Position-independent Word-error Rate (PER)(Och et al, 2001), obtained with Moses and ourhierarchical phrase-based SMT, respectively.
Boldface results indicate the best scores obtained withthe compared methods (excluding oracles).The proposed method consistently outper-formed the baseline.
The BLEU improve-ments with the proposed method over the base-line and comparison methods were statisticallysignificant according to the bootstrap samplingtest (p < 0.05, 1,000 samples) (Zhang et al,2004).
With Moses, the improvement when us-ing the proposed method was 1.4% (33.19% to34.60%) in BLEU and 1.3% (57.83% to 56.50%)in TER, with a slight improvement in PER(35.84% to 35.61%).
We observed: oracle ?proposed ?
comp.
(1) ?
baseline ?
comp.
(2)by the Bonferroni method, where the symbolA ?
B means ?A?s improvement over B isstatistically significant.?
With the hierarchicalphrase-based SMT, the improvement was 2.2%(32.39% to 34.55%) in BLEU, 3.5% (58.36% to54.87%) in TER, and 1.5% in PER (36.42% to34.79%).
We observed: oracle ?
proposed ?424Table 2: Compared methods.PPPPPPPPTestTraining w/ auto-aligned w/o aligned w/ oracle-alignedclause-seg.
proposed comp.
(2) oracleno-clause-seg.
comp.
(1) baseline oracle-comp.{comp.
(1), comp.
(2)} ?
baseline by the Bon-ferroni method.
The oracle results were better thanthese obtained with the proposed method but thedifferences were not very large.4.5 DiscussionWe think the advantage of the proposed methodarises from three possibilities: 1) better translationmodel training using the fine-aligned corpus, 2) anefficient decoder search over shortened inputs, and3) an effective clause-level reordering model real-ized by using non-terminals.First, the results of the first comparison method(comp.
(1)) indicate an advantage of the transla-tion models trained using the auto-aligned corpus.The training of the translation models, namelyword alignment and phrase extraction, is difficultfor long sentences due to their large ambiguity.This result suggests that the use of clause-levelalignment provides fine-grained word alignmentsand precise translation models.
We can also ex-pect that the model of the proposed method willwork better for the translation of single-clause sen-tences.Second, the average and median lengths (in-cluding non-terminals) of the clause-seg.
test setwere 13.2 and 10 words, respectively.
They weremuch smaller than those of no-clause-seg.
at 33.4and 30 words and are expected to help realizean efficient SMT search.
Another observation isthe relationship between the number of clausesand translation performance, as shown in Fig-ure 5.
The proposed method achieved a greater im-provement in sentences with a greater number ofclauses.
This suggests that our divide-and-conquerapproach works effectively for multi-clause sen-tences.
Here, the results of the second comparisonmethod (comp.
(2)) with Moses were worse thanthe baseline results, while there was an improve-ment with our hierarchical phrase-based SMT.This probably arose from the difference betweenthe decoders when translating out-of-vocabularywords.
The non-terminals were handled as out-of-vocabulary words under the comp.
(2) condition.52545658606264662 4 53TER(%)The number of clausesbaselineproposedcomp.
(2)Figure 5: Relationship between TER and numberof clauses for proposed, baseline, and comp.
(2)when using our hierarchical phrase-based SMT.Moses generated erroneous translations aroundsuch non-terminals that can be identified at aglance, while our hierarchical phrase-based SMTgenerated relatively good translations.
This maybe a decoder-dependent issue and is not an essen-tial problem.Third, the results obtained with the proposedmethod reveal an advantage in reordering in ad-dition to the previous two advantages.
The differ-ence between the PERs with the proposed methodand the baseline with Moses was small (0.2%)in spite of the large differences in BLEU andTER (about 1.5%).
This suggests that the pro-posed method is better in word ordering and im-plies our method is also effective in reordering.With the hierarchical phrase-based SMT, the pro-posed method showed a large improvement fromthe baseline and comparison methods, especiallyin TER which was better than the best Mosesconfiguration (proposed).
This suggests that thedecoding of long sentences with long-distancereordering is not easy even for the hierarchicalphrase-based SMT due to its limited window size,while the hierarchical framework itself can natu-rally model a long-distance reordering.
If we try tofind a derivation with such long-distance reorder-ing, we will probably be faced with an intractablesearch space and computation time.
Therefore,we can conclude that the proposed divide-and-425Table 3: Experimental results obtained with Moses and our hierarchical phrase-based SMT, in BLEU,TER, and PER.Moses : BLEU (%) / TER (%) / PER (%)PPPPPPPPTestTraining w/ auto-aligned w/o aligned w/ oracle-alignedclause-seg.
34.60 / 56.50 / 35.61 32.14 / 58.78 / 36.08 35.31 / 55.12 / 34.42no-clause-seg.
34.22 / 56.90 / 35.20 33.19 / 57.83 / 35.84 34.24 / 56.67 / 35.03Hierarchical : BLEU (%) / TER (%) / PER (%)PPPPPPPPTestTraining w/ auto-aligned w/o aligned w/ oracle-alignedclause-seg.
34.55 / 54.87 / 34.79 33.03 / 56.70 / 36.03 35.08 / 54.22 / 34.77no-clause-seg.
33.41 / 57.02 / 35.86 32.39 / 58.36 / 36.42 33.83 / 56.26 / 34.96conquer approach provides more practical long-distance reordering at the clause level.We also analyzed the difference between auto-matic and manual clause alignment.
Since auto-aligned corpus had many obvious alignment er-rors, we suspected these noisy clauses hurt theclause translation model.
However, they were notserious in terms of final translation performance.So we can conclude that our proposed divide-and-conquer approach is promising for long sentencetranslation.
Although we aimed to see whether wecould bootstrap using existing bilingual corpora inthis paper, we imagine better clause alignment canbe obtained with some supervised classifiers.One problem with the divide-and-conquer ap-proach is that its independently-translated clausespotentially cause disfluencies in final sentencetranslations, mainly due to wrong inflections.
Apromising solution is to optimize a whole sentencetranslation by integrating search of each clausetranslation but this may require a much largersearch space for decoding.
More simply, we maybe able to approximate it using n-best clause trans-lations.
This problem should be addressed for fur-ther improvement in future studies.5 ConclusionIn this paper we proposed a clause-based divide-and-conquer approach for SMT that can re-duce complicated clause-level reordering to sim-ple word-level reordering.
The proposed methodseparately translates clauses with non-terminals byusing a well-known SMT method and reconstructsa sentence based on the non-terminals, to reorderlong clauses.
The clause translation models aretrained using a bilingual corpus with clause-levelalignment, which can be obtained with an un-supervised graph-based method using sentence-aligned corpora.
The proposed method improvesthe translation of long, multi-clause sentences andis especially effective for language pairs withlarge word order differences, such as English-to-Japanese.This paper focused only on clauses as segmentsfor division.
However, other long segments suchas prepositional phrases are similarly difficult toreorder correctly.
The divide-and-conquer ap-proach itself can be applied to long phrases, andit is worth pursuing such an extension.
As anotherfuture direction, we must develop a more sophis-ticated method for automatic clause alignment ifwe are to use the proposed method for various lan-guage pairs and domains.AcknowledgmentsWe thank the U. S. National Library of Medicinefor the use of PubMed abstracts and Prof. ShujiKaneko of Kyoto University for the use of LifeScience Dictionary.
We also thank the anonymousreviewers for their valuable comments.ReferencesPeter F. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra, and Robert L. Mercer.
1993.
Themathematics of statistical machine translation: Pa-rameter estimation.
Computational Linguistics,19(2):263?311.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2):201?228.Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.2005.
Clause restructuring for statistical machinetranslation.
In Proc.
ACL, pages 531?540.426Osamu Furuse, Setsuo Yamada, and Kazuhide Ya-mamoto.
1998.
Splitting long or ill-formed in-put for robust spoken-language translation.
In Proc.COLING-ACL, pages 421?427.Michel Galley, Mark Hopkins, Kevin Knight, andDaniel Marcu.
2004.
What?s in a translation rule?In Proc.
NAACL, pages 273?280.Jonathan Graehl and Kevin Knight.
2004.
Trainingtree transducers.
In Proc.
HLT-NAACL, pages 105?112.Jason Katz-Brown and Michael Collins.
2008.
Syntac-tic reordering in preprocessing for Japanese-Englishtranslation: MIT system description for NTCIR-7patent translation task.
In Proc.
NTCIR-7, pages409?414.Yeun-Bae Kim and Terumasa Ehara.
1994.
A methodfor partitioning of long Japanese sentences with sub-ject resolution in J/E machine translation.
In Proc.International Conference on Computer Processingof Oriental Languages, pages 467?473.Phillip Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proc.HLT-NAACL, pages 263?270.Philipp Koehn, Amittai Axelrod, Alexandra BirchMayne, Chris Callison-Burch, Miles Osborne, andDavid Talbot.
2005.
Edinburgh system descriptionfor the 2005 IWSLT speech translation evaluation.In Proc.
IWSLT.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InProc.
ACL Companion Volume Proceedings of theDemo and Poster Sessions, pages 177?180.Chi-Ho Li, Dongdong Zhang, Mu Li, Ming Zhou,Minghui Li, and Yi Guan.
2007.
A probabilistic ap-proach to syntax-based reordering for statistical ma-chine translation.
In Proc.
ACL, pages 720?727.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-String alignment template for statistical machinetranslation.
In Proc.
Coling-ACL, pages 609?616.Igor Malioutov and Regina Barzilay.
2006.
Minimumcut model for spoken lecture segmentation.
In Proc.Coling-ACL, pages 25?32.Yusuke Miyao and Jun?ichi Tsujii.
2008.
Feature for-est models for probabilistic HPSG parsing.
Compu-tational Linguistics, 34(1):35?80.Franz Josef Och, Nicola Ueffing, and Hermann Ney.2001.
An efficient A* search algorithm for statis-tical machine translation.
In Proc.
the ACL Work-shop on Data-Driven Methods in Machine Transla-tion, pages 55?62.Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proc.
ACL,pages 311?318.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proc.
AMTA, pages 223?231.Christoph Tillmann.
2004.
A unigram orientationmodel for statistical machine translation.
In Proc.HLT-NAACL, pages 101?104.Erik F. Tjong, Kim Sang, and Herve?
De?jean.
2001.
In-troduction to the CoNLL-2001 shared task: Clauseidentification.
In Proc.
CoNLL, pages 53?57.Roy Tromble and Jason Eisner.
2009.
Learning linearordering problems for better translation.
In Proc.EMNLP, pages 1007?1016.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?404.Fei Xia and Michael McCord.
2004.
Improvinga statistical MT system with automatically learnedrewrite patterns.
In Proc.
COLING, pages 508?514.Peng Xu, Jaeho Kang, Michael Ringgaard, and FranzOch.
2009.
Using a dependency parser to improveSMT for Subject-Object-Verb languages.
In Proc.HLT-NAACL, pages 245?253.Kenji Yamada and Kevin Knight.
2001.
A syntax-based statistical translation model.
In Proc.
ACL,pages 523?530.Ying Zhang, Stephan Vogel, and Alex Weibel.
2004.Interpreting BLEU/NIST scores: How much im-provement do we need to have a better system?
InProc.
LREC, pages 2051?2054.Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty.2003.
Semi-supervised learning using gaussianfields and harmonic functions.
In Proc.
ICML, pages912?919.427
