MODULARITY  IN  A CONNECTIONIST  MODELMORPHOLOGY ACQUIS IT IONMichae l  GasserDepartments of Computer Science and LinguisticsIndiana UniversityOFAbst ractThis paper describes a modular connection,stmodel of the acquisition of receptive inflectionalmorphology.
The model takes inputs in the formof phones one at a time and outputs the associ-ated roots and infections.
In its simplest version,the network consists of separate simple recurrentsubnetworks for root and inflection identification;both networks take the phone sequence as inputs.It is shown that  the performance of the two separatemodular networks is superior to a single network re-sponsible for both root and inflection identification.In a more elaborate version of the model, the net-work learns to use separate hidden-layer modulesto solve the separate tasks of root and inilectionidentification.INTRODUCTIONFor many natural  languages, the complexity ofbound morphology makes it a potentially challeng-ing problem for a learning system, wl, ether hu-man or machine.
A language learner must ac-quire both the ability to map polymorphemlc wordsonto the sets of semantic elements they tel)resentand to map meanings onto polymorphemic words.Unlike previous work on connection,st morphology(e.g., MacWhinney ~5 Leinbaeh (1991), Plunker,& Marehman (1991) and Rumelhart  & MeClelland(1986)), the focus of this paper is receptive nmr-phology, which represents the more fundamental,or at least the earlier, process, one which produc-tive nmrphology presumably buihls on.The task of learning receptive morphology isviewed here ,as follows.
The learner is "trained" onpairs of forms, consisting of sequcnces of phones,and "meanings", consisting of sets of roots and in-flections.
I will refer to the task as root and inflec-tion identification.
Generalization is tested by pre-senting the learner with words consisting of novelcombinations of familiar morphemes.
If the rule inquestion has been acquired, the learner is able toidentify the root and inflections in the test word.Of interest is whether a model is capable of ac-quiring rules of all of the types known for naturallanguages.
This paper describes a psychologicallymotivated connection,st model (Modular Connec-tion,st Network for the Acquisition of Morphology,MCNAM) which approaches this level of perfor-mance.
The emphasis here is on the role of mod-ularity at the level of root and inflection in themodel.
I show how this sort of modularity improvesperformance (lramatically and consider how a net-work might learn to use modules it is provided with.A sel)arate paper (Gasser, 1994) looks in detail atthe model's performance for particular categoriesof morI)hology, in particular, template morphologyand reduplication.The paper is organized as folh)ws.
I first providea brief overview of the categories of morphologicalrules found in the werhl's languages.
I then presenta simple version of the model and discuss simula-tions which demonstrate hat it generalizes for mostkinds of morphoh)gical rules.
I then describe a ver-sion of the model augmented with modularity atthe level of root and inflection which generalizessignificantly better and show why this appears tobe the case.
Finally, I describe some tentative at-tempts to develop a model which is provided withmodules and learns  how to use them to solve themorphology identification tasks it is faced with.CATEGORIES  OFMORPHOLOGICAL  PROCESSESI will be discussing morphology in terms of the tra-ditional categories of "root" and "intlection" andmorphological processes in terms of "rules", thoughit should I)e emphasized that  a language learnerdoes not have direct access to these notions, andit is an open question whether they need to be anexplicit part of the system which the learner devel-ops, let ah)ne the device which the learner starts outwith.
I will not make a distinct,m, between inflec-tional and deriwttional morl)hoh)gy (using "inth,c-tion" for both) and will not consider compmmding.AJIixation ,revolves the addition of the inflectionto the root (or st,,,,,), either I,efore ('~,,,'efixatio,O,after (su/fizatlon), within (infixation), or both be-fore and after (circun~Ji:r.ation) the root.
A furthertype of morphological rule, which I will refer to asmutation, consists in modification to the root seg-ments themselves.
A third type of rule, familiar inSemitic languages, is known as template morphol-ogy.
tIere a word (er stem) consists of a root and apattern of segments which are intercalated 1)ctweenthe root segments in a way which is specified withinthe pattern.
A fourth type, the rarest of all, con-sists in the deletion of one or ,nor(; segments.
Afifth type, like aflixation, involves the addition ofsomething to the root form.
But the form of whatis added in this case is a copy, or a systematically214altered copy, of stone l)ortlon of the root.
This pro-cess, reduplication, is it, one way the most cmnplextype of morphology (though it may not necessarilybe the most difficult for a child to learn) because itseems to require a variable.
It is not handled by themodel discussed in this paper.
G;Lsser (1994) dis-cusses modification of the model which is requiredto accommodate r duplication.THE MODELThe al)l)roach to hmguage acquisition exempliliedin this paper differs from traditional symbolic al)-proaches in that  the focus is on specifying tile sortof cognitive architecture and the sort of general pro-ce.ssing and learning mcchani.~ms which h;we thecapacity to learn some ;~speet of language, ratherthan the innate knowledge which this might require.If successflfl, such ~t model would provide a sim-pler account of the acquisition of morphology thavone which begins with symbolic knowledge and con-straints.
Connectionlst models are.
interesting inthis regard because of their powerfi,l sul)-symbolielearning algorithms.
But in the past, there has beenrelatively little interest in investigating the effecton tile language acquisitio,t capacity of structuringnetworks in particular ways.
The concern in thisl)aper will I)e with what is gained 1)y adding mod-ularity to a network.Given tile I)iusic l)rol)lem of what it means tolearn receptive morphology, I will begin witl, oneof the siml)lest networks that  could have that ca-pacity and then augment he device as necessary.In this paper, two versions of the model are de-scribed.
Version 1 successfidly learns simple exam-pies of all of tile morl)hological rules except redu-plication and circumfixation, but its l)erformanceis far from the level that  might be exl)ected frmna human language learner.
Version 2 (MCNAMproper) incorporates a form of built-in modularitywhich separates portions of tile network resl)onsi-l)le.
for tile i(lentificatimt of the root and the in-flections; this improves the nctwork's 1)erformancesigniticantly on all of the rule types except redupli-cation, which cavnot be learned even by a networkoutfitted with this form of modularity.Word recognition is an incremental process.Words are often recognized hmg before they fin-ish; hearers eem to be continuously coml)ariug thecontents of at linguistic short-term memory withthe phonological representations ill their mentallexicons (Marsk, n-Wilson & Tyler, 1980).
Thustile task at hand requires a short-term memory ofsome sort.
There are several ways of represent-ing short-term memory in cmmectionist networks(Port, 1990), in particular, through the use of time-delay connections out of input units and throughthe use of recurrent time-delay cmmections on someof the network units.
The most ttexible apl)roachmakes use of recurrent connections on hidden units,though the arguments ill favor of this opthm arebeyond the scope of this l)aper.
The model to bedescribed here is a network of this type, a version ofthe simple recurrent network due to Elman (1990).Vers ion  1The Version 1 network is shown in Figure 1.
Eachbox represents a layer of connectionist processingunits and each arrow a cmnplete set of weightedconnections between two layers.
The network op-erates as follows.
A sequence of l)hanes is presentedto the input layer one at a time.
Tl,at is, each tickof the network's chick represents the presentationof ~t single phone.
Each l)hone unit represents allhonetic fi~ature, and each word consists of a se-quence of i)hones l)reeede(l by a boundary "phone"made.
up of 0.0 actiwttlons.
"1 root ~) in f lec t ion:: ======================================I"il~ure 1: Network for Acquisition of Morphology(Version 1)An input phone 1)attern sm,ds actiwttion to tilenetwork's hidden layer.
The 1,idden layer also re-ceives activation from the pattern that apl)earedthere on the l)revious time stel).
Thus each hiddenunit is joined by a time-deh W connection to eachother hidden unit.
It is the previous hidde,>layerpattern which represents the system's hort-termmemory.
Because the hi(ldcn layer has access tothis previous state, which in turn del)ended on itsstate al.
the time step before that,  there is no ab-solute limit to the.
length of the context stare(1 inthe sho,'t-term memory.
At the 1)eginnlng of eachword sequence, the.
hidden layer is reinitialized toa pattern consisting of 0.0 activations.l:'inally the output re,its are activated l)y the hid-den layer.
There are three output layers.
One rep-rese.nts imply a copy of the current input l)hone.Training the network to auto-associate its currentinput aids in learning the root and inflection identi-fication task because it forces the network to learnto distinguish the individual phones at the hiddenlayer, a prerequisite to using the short-term mem-ory effectively.
The second layer of output uvitsrel)resents the root ",neavlng".
For each root thereis a single outlmt unit.
Thus while there is no realsemantics, the association between the inl)ut phonesequence and the "meaning" is at least an arl)itrary215one.
The third group of output units represents theinflection "meaning".
Again there is a unit for eachseparate inflection.For each input phone, the network receives it tar-get consisting of the correct phone, root, and inflec-tion outputs for the current word.
The phone targetis identicM to the input phone.
The root and in-flection targets, which are constant hroughout thepresentation of a word, are the patterns associatedwith the root and inflection for the input word.The network is trained using the backpropa-gation learning algorithm (Rumelhart, IIinton, &Williams, 1986), which adjusts the weights on allof the network's connections in such a way as tominimize the error, that is, the difference betweenthe network's outputs and the targets.
For eachmorphological rule, a separate network is trainedon a subset of the possible combinations of rootand inflection.
At various points during training,the network is tested on unfamiliar words, that is,novel combinations of roots and inflections.
Theperformance of the network is the percentage of thetest roots and inflections for which its output is cor-rect at the end of each word sequence when it hasenough information to identify both root and in-flection.
A "correct" output is one which is ch>serto the appropriate target than to any of the others.In all of the experiments reported on here, thestimuli presented to the network consisted of wordsin an artificial language.
The phoneme inventoryof the language was made up 19 phones (24 h)r themutation rule, which nasalizes vowels).
For eachmorphological rule, there were 30 roots, 15 eachof CVC and CVCVC patterns of phones.
Eachword consisted of two morphemes, a root and asingle "tense" inflection, marking the "l)resent ''or "past".
Examples of each rule: (1) suffix:present-vibuni, pmst-vibuna; (2) prefix: present-ivibun, past-avibun; (3) infix: prescnt-vikbun,past-vinbun; (4) circumfix: 1)rescnt-ivibuni, pmst-avibuna; (5) mutation: prcsent-vibun, past-vib.Sn;(6) deletion: prescnt-vibun, l)ast-vibu; (7) tem-plate: present-vaban, past-vbaan.For each morphological rule there were 60 (30roots x 2 inflections) dilferent words.
From these40 were selected randomly as training words, andthe remaining 20 were set a.side as test words.
Foreach rule, ten separate networks, with different ran-dom initial weights, were trained for 150 epochs(repetitions of all training patterns).
Every 95epochs, the performance of the network on the testpatterns was assessed.Figure 2 shows the performance of the VersionI network on each rule (,as well as perfor,nance onVersiou 2, to be described below).
Note that chanceperforrnance for the roots was .033 and for the.
iu-fiections .5 sluce there were 30 roots and '2.
inflec-tions.
There are several things to notice in these re-sults.
Except for root identification for the eircum-fix rule, the network performs well above cllance.IIowever, the results are still disappointing in manycases.
In particular, note the poor performance ouroot identification for the prefix rule and inflectionidentification for the sufHx rule.
The 1)chavior ismuch poorer than we might expect from a childlearning these relatively simple rules.The problem, it turns out, is interference betweenthe two tasks which the network is faced with.
Onthe one hand, it must pay attention to infornmtionwhich is relcwtnt o root identification, on the other,to information relevant o inflection identification.This means making use of the network's hort-termmemory in very different ways.
Consider the pre-fixing case, fl)r example.
Here for inflection identifi-cation, the network need only pay" attention to thefirst phone and then remelnber it until the end ofthe sequence is reached, ignoring all of the phoneswhich appear in between.
For root identification,however, the network does best if it ignores the ini-tial phone in the sequence anti then pays carefulattention to each of the following phones.hleally the network's lfidden layer would divideinto modules, one dedicated to root identification,the other to inflection identificatlon.
This couldhappen if some of the recurrent hidden-unit weightsand some of the weights on hidden-to-output con-nections went to 0. tIowcver, ordinary backpropa-gation tends to implement sharing among hidden-layer units: each hidden-layer unit participates tosome extent in activating all output units.
Whenthere arc conflicting output tasks, as in this ease,there are two sorts of possible consequences: ei-ther performance on both t~sks is mediocre, or thesimpler task comes to dominate the hidden layer,yielding good performaacc on that task and poorperformance on the other.
In the Version 1 resultsshown in Figure 2, we see both sorts of outcomes.What is apparently needed is modularity at thehidden-layer level.
One sort of modularity is hard-wired into the network's architecture in Version 2of the model, described in the next section.Vers ion 2\]\]ccause root and inflection i<lentitication make con-tlicting demands on the network's hort-term mem-ory, it is predicted that performance will improvewith scparate hid<len layers for the two tasks.
Var-ious degrees of modolarity are possible in connec-tionist networks; the form implemcllted in Version2 of the model is total modularity, coml)letely sep-arate networks h>r the two tasks.
This is shownin Figure 3.
There are now two hidden-layer mo<l-ules, each with recurrent connections only to miltswithin the same module and with connections toone of the two output identification layers of units.
(Both hidden layers connect o the auto-associativephone output layer.
)The same stimuli were used in training and test-216il 1 .................................................... ~ .................................. : .......... ~; .............................. 7: ....................................................................................... ii-~  0 .6  .
.
.
.
.o .z 0 .4  - ii m 0.2 .
.
.
.
.
.
.
.suffix Prefix Infix circumfix Delete Mutate Template~ ~ Root, V.1 ~N,,,",~ Root, V.2 Chance .
.
.
.I E~ I n f lec t ion ,  V.1 ~x~'-~ In f lec t ion ,  V .2  Chance .
.
.
.
.
.
.
.Figure 2: Performance on Test Words I"ollowing Training (Network Versions 1 and 2)\[\[t , root I inflection I,.!
'\[ phonei .
.
!ZZZZZZZZZZ .t :~Figure 3: Network for Acquisitiou of Merl>hology(Version 2)ing the Version 2 network as the Version 1 network.Each Version 2 network had the same number oftotal hidden units as each Version 1 network, 30.Each hidden-layer module contained 15 units.
Notethat this means there are fewer connections in theVersion 2 than the Version 1 networks, hwestiga-tions with networks with hidden layers of differentsizes indicate that, if anything, this should faw)rthe Version 1 networks.Figure 2 comp~tres results from the two versionsfollowing 150 epo('hs of training.
For all of therule typcs, modularity improves pcrfornlance forboth root and inilection identification.
Olwiously,hidden-layer modularity results in diminished inter-feren<:c between tile two output tasks.
Performance.is still far from perfect for some of the rule types,but further iml>rovcment is l>ossible with optin|iza-tion of the learning parameters.TOWARDS ADAPT IVEMODULARITYIt is important o 1)c clear on tile nature of the mod-ularity being prol)osed here.
As discussed al)ove, Ihave (lefitLe(l the task of word recognition in such~t way that there is a built-in distinction betweenlexical :tad grammatical "meanings" because theseare localized iu separate ~)utl)ut layers.
Tit(.'
modu-lar architecture of Figure 3 extends this distin(:tiouinto the domai|| of phonology.
That is, the shapeof words ix rel)resente(l iuternally (on the hiddenlayer) in terms of two distinct patterns, one for theroot and one for the inflection, and the network"knows" this even before it is trained, though ofcourse it does not know how the root and intlec-tiens will 1)e realized in the language.A fitrther concern arises when we consider whathapl)ens whcx~ more than one grammatical c~ttegoryis represented in tile words Acing recognized, forexample, aspect in addition to tense on verbs.
As-suming the hidden-layer modules are a lmrt of tileinnate makeul) of tile learning device, this nteansthat it fixed number of given modtdes must be di-vided up among the separate outl)ut "tasks" which217the target language presents.
Ideally, the networkwould have the capacity to figure out for itself howto distril)ute the modules it starts with among thevarious output tasks; I return to this possibility be-low.
But it is ,also informative to investigate whatsort of a sharing arrangement achieves the best per-formance.
For example, given two modules andthree output tasks, root identification and the iden-tification of two separate inflections, which of thethree possible ways of sharing the modules achievesthe best performance?Two sets of experiments were conducted to in-vestigate the optimM use of fixed modules by anetwork, one designed to determine the best wayof distr ibuting raodnles among output tasks whenthe number of modules does not match the num-ber of output tasks and one dcsigne<l to determinewhether a network could assign the modules to thetasks itself.
In both sets of experiments, the stim-uli were words composed of a stem an<l two affixes,either two suffixes, two prefixes, or one prefix andone suffix.
(All of these possibilities occur in natu:ral languages.)
The roots were the same ones usedin the afl\]xation and deletion experiments alreadyreported.
In the two-suffix ease, the first suffix was/a /  o r / i / ,  the second suffix / s /  or /k / .
Thus theh>ur forms for the root migon were migonik, migo-nis, migonak, and migonas.
In the two-prefix casetit(': l,retixcs were /s /o r /k /a l id  /a /  or / i / .
In theprefix---sufflx case, the prefix , , ' as /u /or /e /and  thesuffix la l  or Ill. ' there  ,,,ere in all  case~ two hidden-layer modules.
The size of the nlodules was sltchthat  the root identil ieation task had potentially 20units and each of the inilection identification taskspotentially 3 units at its disposal; the sum of theunits in the two modules was always 26.The results are only summarized here.
The con-tiguration in which a single nm(tule is shared by thetwo affix-identification tasks is consistently superiorfor petbrmance on root identification but only su-perior for affix identification in the two-sufflx case.For the l)refix-sullix case, the configuration i whichone module is shared by root identification and suf-fix identification is clearly inferior to the other twoconfigurations for performance on snflix identifica:tion.
For the two-preflx ciLsc, the configurationsmake little diffcrcnce for performance on identifica-tion of either of the prefixes.
Note that the resultsfor file two-prefix and two-suffix cases agree withthose for the single-prefix and single-suffix cases re-spectively (Figure 2).What  the results for root identification makeclear is that,  even though the affix identificationtasks arc easily learned with only 3 units, when theyare provided with more units (23 in these experi-mcnts), they will tend to "distribute" themseh,esover the available units.
If this were not the case,performance on the competing, and more difficnlt,task, root identification, wouhl be no better whenit has 20 units to itself than when it shares 23 unitswith one of the other two tasks.We conclude that  the division of labor into sep-arate root and inflection identification modulesworks best, I)rimarily because it reduces interfer-ence with root identification, but also for the two-suffix ease, and to a lesser extent for the prefix-suffix case, because it improves performance on af-fix identification.
If one distribution of the awtil-able modules is more efficient than the others, wewouhl like the network to be able to find this dis-tr ibution on its own.
Otherwise it wouhl have tobe wired into the system from the start, and thiswouhl require knowing that  the different inflectiontasks belong to the same category.
Somc form ofadaptive use of the awtilable modules seems cMledfor.Given a system with a fixed set of modules but nowired-in constraints on how they are used to solvethe wtrious output t~sks, can a network organizeitself in such a way that  it uses the modules effi-ciently?
There has been considerable interest in thelast few years in architectures which are endowedwith modularity and learn to use the modularityto solve tasks which call for it.
The architecturedescribed by Jaeobs, Jordan, & 13arto (1991) is anexample.
In this approach there are connectionsfrom each modular hidden layer to all of the out-put units.
In addition there are one or more gatingnetworks whose function is to modulate the inputto the ontpnt units from the hidden-layer modules.In the version of the architecture which is appropri-ate for domains uch as the current one, there is asingle gating unit responsible for the set of connec-tions from each hidden nmdule to each output taskgronl).
The outl)uts of the modules are weightedby the outl)uts of the corresponding gating unitsto give the output of the entire system.
The wholenetwork is trained using backl)ropagation.
For eachof the niodules, the error is weighted by the vahle ofthe gating input as it is l>assed back to the modules.Thus each niodule adjusl;s its weights in such a waythat the difference, between the system's output andthe desired target is mininlized, and the extent towhich a nio<htle's weights are change<l <leiden<Is onits contribution to the outl)ut.
For the gating net-works, the error function implcments coml)etitionamong the modules for each output task group.For our purposes, two further augmentations arerequired.
First, we are dealing with recurrent net-works, so we permit each of the modular hiddenlayers to see its own previous values in ad(lition tothe current input, but not the l)revious values ofthe hidden layers of the other modules.
Second, weare interested not only in competit ion among themodules for the output groups, but also in coml)e-tition among the outpnt groups for the modules.In particular, we would like to prevent he networkfrom assigning a single module to all output tasks.218To achieve this, the error function is modified sothat  error is mi,fimized, all else l)eing equal, whenthe total of the outputs of all gating units dedicatedto a single module is neither close to 0.0 nor closeto the total number of output groups.Figure 4 shows the arctfitccture for the situa-tion in wlfich there is only one intlection to belcarne(l. (The auto-associative phone output layeris not shown.)
The connections ending in circlessymbolize the emnpcfit ion between sets of gatingunits which is built into the error function for thenetwork.
Note that  the gating units have no in-put connections.
These units have only to learna bias, which, once tile system is stable, h.'ads toa relatively constant outlmt.
The ~ussumption isthat,  since we are dealing with a spatial crosstalkl)roblem, the way in which l)articular modules areassigned to particular tausks shonld not wny withthe inl)ut to the nctwm'k.~-  0~.
.
.
hidden2/ ~ - - ~  ~ hidden t"6 o 'EFigure 4: Adal)tive Modular Architecture fro" Mor-phology AcquisitionAn initial experiment demonstrated that theadaptive modular network consistently assigne(1separate modules to the output tasks when the,'ewere two modules and two tasks (identification ofthe root and a single intlection).Next a set of experiments tested whcthe.r theadaptive modular architecture wonhl assign twomodules to three tasks (root and two intlections)in the most efficient way for the two-suffix, two-prefix, and prefix-suffix cases.
Recall that tile mostefficient patteru of connectivity in all cases was theone in which one of the two modules was sl,ared bythe two affix identification tasks.Adaptive mmlular networks with two modules of15 units each were trained on tile two-sufflx, two-prefix, and prefix-suffix tasks described in the lastsection.
Following 120 epochs, the outputs of thesix gating units fl)r the different modules were ex-anfined to determine how the modules were shared.The results were completely negative; the threepossible ways of assigning the modules to the threeidentilication tasks occurred with approximatelyequal frequency.
The prolfiem was that the inflec-tion identilication tasks were so nmch easier thanthe root identilicatlon task that  they claimed thetwo modules for themsclves early on, while neithermodule was strongly prefc,'red by the root task.q_'hus as often as not, the two inflections ended upassigned to dil\['erent modules.
To compensate forthis, then, is it reasonable to give root identifica-tion some sort of advantage over i,dlection identiti-cation?
It is well-known that children begin to ac-quire lexlcal morphemes before they acquire gram-matical morphemes.
Among the reasons for thisis llrobably the more abstract nature of the lncan-ings of the grammatical morphemes.
In terms ofthe network's tasks, this relative difficulty wouhltranslate into an inability to know what the inllee-tion targets would I)e fl>r particular inlmt patterns.Thus we couh\[ m<>del it by (lelayi,lg training on theinltection identification task.The exl)eriment with the adaptive modular net-works was repeated, this tixne witl, the fl)llowingtraining regimen.
Entire words (consisting of rootand two at\[ixes) were i)resented throughout train-ing, lint for the first 80 epochs, the network sawtargets for only the root identification task.
Thatis, the connections into the output units for the twoinilcctions were not altered during this plume.
I-'oflowing the 80th epoch, by which time the networkwas well on its way to learning the roots, train-ing on the inllections was introduced.
This pro-cedure was followed for the.
two-sulfix, twoq)retix,and prelix-sul\[ix tasks; 20 sel):trate networks weretrained for each type.
For the two-sutlix task, in allcases the network organized itself in the p,'cdictedway.
That  is, for all 20 networks one of the mod-nh.
's was associated mainly with the two intlectio,,output units and the other associatcd with the rootoutput units.
In the preilx-suflix case, however, theresults were more equivocal.
Only 12 out of 20 ofthe networks organized themselves in such a waythat tile two intlecti(m tasks were shared by onemodule, while in the 8 other cases, one module w~sshared by the root and pretix identitication t~sks.Finally, in the two-pretlx case, all of the networksorganized themselves ill Sllch a v,'ay that the rootand the first pretix shared a module rather than inthe apllarently more eillcient contlguration.The ditt'erence is not surprising when we considerthe nature of the advantage of the configuratioit219in which the two inflection identification tasks areshared 1)3, one module.
For all three types of af-fixes, roots are identified better with this configu-ration.
But this will have little effect on the way thenetwork organizes itself becanse, following the 80thepoch when competition among tile three outputtasks is introduced, one or the other of tile mod-ules will already be firmly linked to the root out-put layer.
At this point, the outcome will dependmainly on the competition between tlle two inflec-tion identification t,'~sks for the two modules, theone already claimed for root identification and theone which is still unused.
Thus we can expect thistraining regimen to settle on tl,e best configurationonly when it makes a significant ditference for in-flection, as opposed to root, identification.
Sincethis difference was greater for tile two-suflix wordsthan for the prefix-sufl\]x words and virtually non-existent for the two-prefix words, there is the.
great-est preference in the two-suffix case for tile config-uration in which the two inflection tasks are sharedby a single module.
It is also of interest hat for tileprefix-suffix cruse, tile network never chose to shareone module between the root and the suffix; this iseasily the least efficient of the three configurationsfrom the perspeetlve of inflection klentlficatlon.Thus we are left with only a partial sohttion totlle problem of how the modular architecture mightarise in the first place.
For circumstances in whichthe different sorts of modularity impinge on inflec-tion identification, the adaptive api)roach can findthe right configuration.
When it is performance onroot identification that makes the difference, how-ever, this api)roach has nothing to offer.
Futurework will also have to address what happens whenthere are more than two modules and/or more thantwo intlections in a word.CONCLUSIONSEarly work applying connectimfist networks tohigh-level cognitive tasks often seemed based on theassumption that a single network wouhl l)e al)le tohandle a wide range of phenomena.
Increasingly,however, the emphasis is moving in the directionof special-l)urpose modules for subtasks which mayeontlict with each other if handled by the samehardware (aacobs et al, 1991).
These apl)roachesbring eonnectionist models somewhat more in linewith tile symbolic models which they seek to re-place.
In this paper I have shown how tile ability ofsimple recurrent networks to extract "structure intime" (Ehnan, 1990) is enhanced by built-in modu-larity which I)ermits the recurrent hidden-unit con-nections to develop in ways which are suitable forthe root and inflection identification tasks.
Not(.,that this modularity does not amount o endowingthe network with the distlnctiml 1)etween root andaffix because both modules take the entire sequenceof phones as input, and the modularity is the samewhen tile rule being learned is one for which thereare 11o affixes at all (mutation, for examph!
).Modular approaches, whether symbolic or con-nectionist, inevitably raise fllrther questions, how-ever.
The modularity in the pre-wired version ofMCNAM, which is reminiscent of the traditionalseparation of lexical and grammatical knowledge inlinguistic models, assumes that the division of "se-mantic" outlmt units into lexical and grammaticalcategories has already l)een made.
The adaptiveversion partially addresses tills shortcoming, lint itis only etfective in cases where modularity 1)cue-fits inflection identification.
Furthermore, it is stillbased on the assumption that the output is dividedinitially into groups rel)resenting separate compet-ing tasks.
I am currently experimenting with re-lated a(lal)tive approaches, as well as inethods in-volving weigl,t decay and weight pruning, whichtreat each output unit as a separate task.Re ferencesEl,nan, J.
(1990).
Fiudi,,g structure in time.
Cog-nitive Science, 14, 179-211.Gasser, M. (199,1).
Acquiring receptive morphol-ogy: a counectionist model.
Annual Meetingof the Association for Computational Linguis-tics, 32.Jaeobs, R. A., Jordan, M. I., & Barto, A. G. (1991).Task decomposition through competition in atmodular conneetionist architecture: the whatand where vision tasks.
Cognitive Science, 15,219-250.MacWhinney, B.
~ Leinbach, J.
(1991).
hnplemen-tatkms are not conceptualization: revising theverb learning model.
Cognition, 40, 121-157.Marslen-Wilson, W. D. & Tyler, L. K. (1980).
Thetemporal structure of Sl)oken language under-standing.
Cognition, 8, 1--71.Plunkett, K. & Marchman, V. (1991).
U-shapedlearning and frequency effects in at multi-layered 1)erc('ptron: implications for chihl lan-guage acquisition.
Cognition, 38, 1-60.Port, R. (1990).
Representation and recognitionof teml)oral 1)atterns.
Connection Science, 2,151-176.Rumelhart, D. E. & McCMland, a. L. (198G).
Onlearning the past tense of English verbs.
IuMcClelhmd, .l.L.
~ Rumelhart, D. E.
(Eds.
),Parallel Distributed Processing, VohLme 2, pp.216 -271.
MIT Press, Cambridge, MA.Rumelhart, D. E., IIinton, G., ,~ Williams, R.(1986).
Learning internal representations byerror propagation.
In Rmnelhart, D. E. & Me-Clelland, a. L.
(Eds.
), Parallel Distributed Pro-cessing, Volume 1, pp.
318-304.
MIT Press,Cambridge, MA.220
