Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 825?833,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsImproving Statistical Machine Translation withMonolingual CollocationZhanyi Liu1, Haifeng Wang2, Hua Wu2, Sheng Li11Harbin Institute of Technology, Harbin, China2Baidu.com Inc., Beijing, Chinazhanyiliu@gmail.com{wanghaifeng, wu_hua}@baidu.comlisheng@hit.edu.cnAbstract?This paper proposes to use monolingualcollocations to improve Statistical Ma-chine Translation (SMT).
We make useof the collocation probabilities, which areestimated from monolingual corpora, intwo aspects, namely improving wordalignment for various kinds of SMT sys-tems and improving phrase table forphrase-based SMT.
The experimental re-sults show that our method improves theperformance of both word alignment andtranslation quality significantly.
As com-pared to baseline systems, we achieve ab-solute improvements of 2.40 BLEU scoreon a phrase-based SMT system and 1.76BLEU score on a parsing-based SMTsystem.1 IntroductionStatistical bilingual word alignment (Brown et al1993) is the base of most SMT systems.
As com-pared to single-word alignment, multi-wordalignment is more difficult to be identified.
Al-though many methods were proposed to improvethe quality of word alignments (Wu, 1997; Ochand Ney, 2000; Marcu and Wong, 2002; Cherryand Lin, 2003; Liu et al, 2005; Huang, 2009),the correlation of the words in multi-wordalignments is not fully considered.In phrase-based SMT (Koehn et al, 2003), thephrase boundary is usually determined based onthe bi-directional word alignments.
But as far aswe know, few previous studies exploit the collo-cation relations of the words in a phrase.
SomeThis work was partially done at Toshiba (China) Researchand Development Center.researches used soft syntactic constraints to pre-dict whether source phrase can be translated to-gether (Marton and Resnik, 2008; Xiong et al,2009).
However, the constraints were learnedfrom the parsed corpus, which is not availablefor many languages.In this paper, we propose to use monolingualcollocations to improve SMT.
We first identifypotentially collocated words and estimate collo-cation probabilities from monolingual corporausing a Monolingual Word Alignment (MWA)method (Liu et al, 2009), which does not needany additional resource or linguistic preprocess-ing, and which outperforms previous methods onthe same experimental data.
Then the collocationinformation is employed to improve BilingualWord Alignment (BWA) for various kinds ofSMT systems and to improve phrase table forphrase-based SMT.To improve BWA, we re-estimate the align-ment probabilities by using the collocation prob-abilities of words in the same cept.
A cept is theset of source words that are connected to thesame target word (Brown et al, 1993).
Analignment between a source multi-word cept anda target word is a many-to-one multi-wordalignment.To improve phrase table, we calculate phrasecollocation probabilities based on word colloca-tion probabilities.
Then the phrase collocationprobabilities are used as additional features inphrase-based SMT systems.The evaluation results show that the proposedmethod in this paper significantly improves mul-ti-word alignment, achieving an absolute errorrate reduction of 29%.
The alignment improve-ment results in an improvement of 2.16 BLEUscore on phrase-based SMT system and an im-provement of 1.76 BLEU score on parsing-basedSMT system.
If we use phrase collocation proba-bilities as additional features, the phrase-based825SMT performance is further improved by 0.24BLEU score.The paper is organized as follows: In section 2,we introduce the collocation model based on theMWA method.
In section 3 and 4, we show howto improve the BWA method and the phrase ta-ble using collocation models respectively.
Wedescribe the experimental results in section 5, 6and 7.
Lastly, we conclude in section 8.2 Collocation ModelCollocation is generally defined as a group ofwords that occur together more often than bychance (McKeown and Radev, 2000).
A colloca-tion is composed of two words occurring as ei-ther a consecutive word sequence or an inter-rupted word sequence in sentences, such as "byaccident" or "take ... advice".
In this paper, weuse the MWA method (Liu et al, 2009) for col-location extraction.
This method adapts the bi-lingual word alignment algorithm to monolingualscenario to extract collocations only from mono-lingual corpora.
And the experimental results in(Liu et al, 2009) showed that this methodachieved higher precision and recall than pre-vious methods on the same experimental data.2.1 Monolingual word alignmentThe monolingual corpus is first replicated togenerate a parallel corpus, where each sentencepair consists of two identical sentences in thesame language.
Then the monolingual wordalignment algorithm is employed to align thepotentially collocated words in the monolingualsentences.According to Liu et al (2009), we employ theMWA Model 3 (corresponding to IBM Model 3)to calculate the probability of the monolingualword alignment sequence, as shown in Eq.
(1).?
?????
?ljjajliiilajdwwtwnSASpj113 ModelMWA),|()|()|()|,( ?
(1)Where lwS 1?
is a monolingual sentence, i?denotes the number of words that are alignedwithiw .
Since a word never collocates with itself,the alignment set is denoted as}&],1[|),{( ialiaiA ii ???
.
Three kinds of prob-abilities are involved in this model: word collo-cation probability)|( jaj wwt, position colloca-tion probability ),|( lajd j  and fertility probabili-ty )|( ii wn ?
.In the MWA method, the similar algorithm tobilingual word alignment is used to estimate theparameters of the models, except that a wordcannot be aligned to itself.Figure 1 shows an example of the potentiallycollocated word pairs aligned by the MWA me-thod.Figure 1.
MWA Example2.2 Collocation probabilityGiven the monolingual word aligned corpus, wecalculate the frequency of two words aligned inthe corpus, denoted as ),( ji wwfreq .
We filteredthe aligned words occurring only once.
Then theprobability for each aligned word pair is esti-mated as follows:?
??
?w jjiji wwfreqwwfreqwwp ),(),()|((2)?
??
?w ijiij wwfreqwwfreqwwp ),(),()|((3)In this paper, the words of collocation aresymmetric and we do not determine which wordis the head and which word is the modifier.
Thus,the collocation probability of two words is de-fined as the average of both probabilities, as inEq.
(4).2)|()|(),( ijjiji wwpwwpwwr ??
(4)If we have multiple monolingual corpora toestimate the collocation probabilities, we interpo-late the probabilities as shown in Eq.
(5).
),(),( jik kkji wwrwwr ??
?(5)k?
denotes the interpolation coefficient forthe probabilities estimated on the kth corpus.3 Improving Statistical Bilingual WordAlignmentWe use the collocation information to improveboth one-directional and bi-directional bilingualword alignments.
The alignment probabilities arere-estimated by using the collocation probabili-ties of words in the same cept.The team leader plays a key role in the project undertaking.The team leader plays a key role in the project undertaking.8263.1 Improving one-directional bilingualword alignmentAccording to the BWA method, given a bilingualsentence pair leE 1?
and mfF 1?
, the optimalalignment sequence A  between E and F can beobtained as in Eq.
(6).
)|,(maxarg* EAFpA A?
(6)The method is implemented in a series of fivemodels (IBM Models).
IBM Model 1 only em-ploys the word translation model to calculate theprobabilities of alignments.
In IBM Model 2,both the word translation model and position dis-tribution model are used.
IBM Model 3, 4 and 5consider the fertility model in addition to theword translation model and position distributionmodel.
And these three models are similar, ex-cept for the word distortion models.One-to-one and many-to-one alignments couldbe produced by using IBM models.
Although thefertility model is used to restrict the number ofsource words in a cept and the position distortionmodel is used to describe the correlation of thepositions of the source words, the quality ofmany-to-one alignments is lower than that ofone-to-one alignments.Intuitively, the probability of the source wordsaligned to a target word is not only related to thefertility ability and their relative positions, butalso related to lexical tokens of words, such ascommon phrase or idiom.
In this paper, we usethe collocation probability of the source words ina cept to measure their correlation strength.
Giv-en source words }|{ iaf jj ?
aligned to ie , theircollocation probability is calculated as in Eq.
(7).
)1(*),(2})|({11 1][][??
?????
?
?iik kggikijji i ffriafr ???
?
(7)Here,kif ][andgif ][denote the thk  word andthg  word in }|{ iaf jj ?
; ),( ][][ giki ffrdenotesthe collocation probability ofkif ][andgif ][, asshown in Eq.
(4).Thus, the collocation probability of the align-ment sequence of a sentence pair can be calcu-lated according to Eq.
(8).?
??
?li jj iafrEAFr 1 })|({)|,((8)Based on maximum entropy framework, wecombine the collocation model and the BWAmodel to calculate the word alignment probabili-ty of a sentence pair, as shown in Eq.
(9).?
?
???
')),,(exp()),,(exp()|,(A iiiiiir AEFhAEFhEAFp ??
(9)Here, ),,( AEFhi and i?
denote features andfeature weights, respectively.
We use two fea-tures in this paper, namely alignment probabili-ties and collocation probabilities.Thus, we obtain the decision rule:}),,({maxarg* ??
i iiA AEFhA ?
(10)Based on the GIZA++ package 1 , we imple-mented a tool for the improved BWA method.We first train IBM Model 4 and collocationmodel on bilingual corpus and monolingual cor-pus respectively.
Then we employ the hill-climbing algorithm (Al-Onaizan et al, 1999) tosearch for the optimal alignment sequence of agiven sentence pair, where the score of an align-ment sequence is calculated as in Eq.
(10).We note that Eq.
(8) only deals with many-to-one alignments, but the alignment sequence of asentence pair also includes one-to-one align-ments.
To calculate the collocation probability ofthe alignment sequence, we should also considerthe collocation probabilities of such one-to-onealignments.
To solve this problem, we use thecollocation probability of the whole source sen-tence, )(Fr , as the collocation probability ofone-word cept.3.2 Improving bi-directional bilingual wordalignmentsIn word alignment models implemented in GI-ZA++, only one-to-one and many-to-one wordalignment links can be found.
Thus, some multi-word units cannot be correctly aligned.
Thesymmetrization method is used to effectivelyovercome this deficiency (Och and Ney, 2003).Bi-directional alignments are generally obtainedfrom source-to-target algnmentstsA 2  and target-to-source alignmentsstA 2 , using some heuristicrules (Koehn et al, 2005).
This method ignoresthe correlation of the words in the same align-ment unit, so an alignment may include manyunrelated words2 , which influences the perfor-mances of SMT systems.1 http://www.fjoch.com/GIZA++.html2 In our experiments, a multi-word unit may include up to40 words.827In order to solve the above problem, we incor-porate the collocation probabilities into the bi-directional word alignment process.Given alignment setstsA 2  and stA 2 .
We canobtain the unionsttsts AAA 22 ???
.
The sourcesentence mf1  can be segmented into m?
ceptsmf ?1 .
The target sentence le1  can also be seg-mented into l ?
cepts le ?1 .
The words in the samecept can be a consecutive word sequence or aninterrupted word sequence.Finally, the optimal alignments A  betweenmf ?1  and le ?1  can be obtained from tsA ?
using thefollowing decision rule.})()(),({maxarg),,(321),(*'1'1??????????
AfejijiAAmljitsfrerfepAfe     (11)Here, )( jfr  and )( ier  denote the collocationprobabilities of the words in the source languageand target language respectively, which are cal-culated by using Eq.
(7).
),( ji fep  denotes theword translation probability that is calculatedaccording to Eq.
(12).i?
denotes the weights ofthese probabilities.||*||2/))|()|((),(jiee ffji feefpfepfep i j?
?
??
?
?
(12))|( fep  and )|( efp  are the source-to-targetand target-to-source translation probabilitiestrained from the word aligned bilingual corpus.4 Improving Phrase TablePhrase-based SMT system automatically extractsbilingual phrase pairs from the word aligned bi-lingual corpus.
In such a system, an idiomaticexpression may be split into several fragments,and the phrases may include irrelevant words.
Inthis paper, we use the collocation probability tomeasure the possibility of words composing aphrase.For each bilingual phrase pair automaticallyextracted from word aligned corpus, we calculatethe collocation probabilities of source phrase andtarget phrase respectively, according to Eq.
(13).
)1(*),(2)(11 11 ??
????
?
?nnwwrwrninijjin                  (13)Here, nw1  denotes a phrase with n words;),( ji wwrdenotes the collocation probability of aCorporaChinesewordsEnglishwordsBilingual corpus 6.3M 8.5MAdditional monolingualcorpora312M 203MTable 1.
Statistics of training dataword pair calculated according to Eq.
(4).
For thephrase only including one word, we set a fixedcollocation probability that is the average of thecollocation probabilities of the sentences on adevelopment set.
These collocation probabilitiesare incorporated into the phrase-based SMT sys-tem as features.5 Experiments on Word Alignment5.1 Experimental settingsWe use a bilingual corpus, FBIS (LDC2003E14),to train the IBM models.
To train the collocationmodels, besides the monolingual parts of FBIS,we also employ some other larger Chinese andEnglish monolingual corpora, namely, ChineseGigaword (LDC2007T38), English Gigaword(LDC2007T07), UN corpus (LDC2004E12), Si-norama corpus (LDC2005T10), as shown in Ta-ble 1.Using these corpora, we got three kinds of col-location models:CM-1: the training data is the additional mo-nolingual corpora;CM-2: the training data is either side of the bi-lingual corpus;CM-3: the interpolation of CM-1 and CM-2.To investigate the quality of the generatedword alignments, we randomly selected a subsetfrom the bilingual corpus as test set, including500 sentence pairs.
Then word alignments in thesubset were manually labeled, referring to theguideline of the Chinese-to-English alignment(LDC2006E93), but we made some modifica-tions for the guideline.
For example, if a preposi-tion appears after a verb as a phrase aligned toone single word in the corresponding sentence,then they are glued together.There are several different evaluation metricsfor word alignment (Ahrenberg et al, 2000).
Weuse precision (P), recall (R) and alignment errorratio (AER), which are similar to those in Ochand Ney (2000), except that we consider eachalignment as a sure link.828ExperimentsSingle word alignments Multi-word alignmentsP R AER P R AERBaseline 0.77 0.45 0.43 0.23 0.71 0.65Improved BWA methodsCM-1 0.70 0.50 0.42 0.35 0.86 0.50CM-2 0.73 0.48 0.42 0.36 0.89 0.49CM-3 0.73 0.48 0.41 0.39 0.78 0.47Table 2.
English-to-Chinese word alignment resultsFigure 2.
Example of the English-to-Chinese word alignments generated by the BWA method andthe improved BWA method using CM-3. "
" denotes the alignments of our method; " " denotesthe alignments of the baseline method.||||grgSSSP ??
(14)||||rrgSSSR ??
(15)||||||*21rgrgSSSSAER ????
(16)Where,gSandrS  denote the automaticallygenerated alignments and the reference align-ments.In order to tune the interpolation coefficientsin Eq.
(5) and the weights of the probabilities inEq.
(11), we also manually labeled a develop-ment set including 100 sentence pairs, in thesame manner as the test set.
By minimizing theAER on the development set, the interpolationcoefficients of the collocation probabilities onCM-1 and CM-2 were set to 0.1 and 0.9.
And theweights of probabilities were set as 6.01 ??
,2.02 ??
and 2.03 ??
.5.2 Evaluation resultsOne-directional alignment resultsTo train a Chinese-to-English SMT system,we need to perform both Chinese-to-English andEnglish-to-Chinese word alignment.
We onlyevaluate the English-to-Chinese word alignmenthere.
GIZA++ with the default settings is used asthe baseline method.
The evaluation results inTable 2 indicate that the performances of ourmethods on single word alignments are close tothat of the baseline method.
For multi-wordalignments, our methods significantly outper-form the baseline method in terms of both preci-sion and recall, achieving up to 18% absoluteerror rate reduction.Although the size of the bilingual corpus ismuch smaller than that of additional monolingualcorpora, our methods using CM-1 and CM-2achieve comparable performances.
It is becauseCM-2 and the BWA model are derived from thesame resource.
By interpolating CM1 and CM2,i.e.
CM-3, the error rate of multi-word alignmentresults is further reduced.Figure 2 shows an example of word alignmentresults generated by the baseline method and theimproved method using CM-3.
In this example,our method successfully identifies many-to-onealignments such as "the people of the world??".
In our collocation model, the collocationprobability of "the people of the world" is muchhigher than that of "people world".
And our me-thod is also effective to prevent the unrelated??
?
????
??
??
?
??
?
??
??
?
??
?China's science and technology research has made achievements which have gained the attention of the people of the world .??
?
????
??
??
?
??
?
??
??
?
??
?zhong-guo  de     ke-xue-ji-shu      yan-jiu      qu-de       le      xu-duo   ling   shi-ren     zhu-mu     de     cheng-jiu .china        DE    science and         research   obtain      LE      many     let    common    attract     DE  achievement .technology                                                                            people    attention829ExperimentsSingle word alignments Multi-word alignments All alignmentsP R AER P R AER P R AERBaseline 0.84 0.43 0.42 0.18 0.74 0.70 0.52 0.45 0.51Our methodsWA-1 0.80 0.51 0.37 0.30 0.89 0.55 0.58 0.51 0.45WA-2 0.81 0.50 0.37 0.33 0.81 0.52 0.62 0.50 0.44WA-3 0.78 0.56 0.34 0.44 0.88 0.41 0.63 0.54 0.40Table 3.
Bi-directional word alignment resultswords from being aligned.
For example, in thebaseline alignment "has made ... have ??
","have" and "has" are unrelated to the target word,while our method only generated "made  ??
", this is because that the collocation probabili-ties of "has/have" and "made" are much lowerthan that of the whole source sentence.Bi-directional alignment resultsWe build a bi-directional alignment baselinein two steps: (1) GIZA++ is used to obtain thesource-to-target and target-to-source alignments;(2) the bi-directional alignments are generated byusing "grow-diag-final".
We use the methodsproposed in section 3 to replace the correspond-ing steps in the baseline method.
We evaluatethree methods:WA-1: one-directional alignment method pro-posed in section 3.1 and grow-diag-final;WA-2: GIZA++ and the bi-directional bilin-gual word alignments method proposed insection 3.2;WA-3: both methods proposed in section 3.Here, CM-3 is used in our methods.
The re-sults are shown in Table 3.We can see that WA-1 achieves lower align-ment error rate as compared to the baseline me-thod, since the performance of the improved one-directional alignment method is better than thatof GIZA++.
This result indicates that improvingone-directional word alignment results in bi-directional word alignment improvement.The results also show that the AER of WA-2is lower than that of the baseline.
This is becausethe proposed bi-directional alignment methodcan effectively recognize the correct alignmentsfrom the alignment union, by leveraging colloca-tion probabilities of the words in the same cept.Our method using both methods proposed insection 3 produces the best alignment perfor-mance, achieving 11% absolute error rate reduc-tion.Experiments BLEU (%)Baseline 29.62Our methodsWA-1CM-1 30.85CM-2 31.28CM-3 31.48WA-2CM-1 31.00CM-2 31.33CM-3 31.51WA-3CM-1 31.43CM-2 31.62CM-3 31.78Table 4.
Performances of Moses using the dif-ferent bi-directional word alignments (Signifi-cantly better than baseline with p < 0.01)6 Experiments on Phrase-Based SMT6.1 Experimental settingsWe use FBIS corpus to train the Chinese-to-English SMT systems.
Moses (Koehn et al, 2007)is used as the baseline phrase-based SMT system.We use SRI language modeling toolkit (Stolcke,2002) to train a 5-gram language model on theEnglish sentences of FBIS corpus.
We used theNIST MT-2002 set as the development set andthe NIST MT-2004 test set as the test set.
AndKoehn's implementation of minimum error ratetraining (Och, 2003) is used to tune the featureweights on the development set.We use BLEU (Papineni et al, 2002) as eval-uation metrics.
We also calculate the statisticalsignificance differences between our methodsand the baseline method by using paired boot-strap re-sample method (Koehn, 2004).6.2 Effect of improved word alignment onphrase-based SMTWe investigate the effectiveness of the improvedword alignments on the phrase-based SMT sys-tem.
The bi-directional alignments are obtained830Figure 3.
Example of the translations generated by the baseline system and the system where thephrase collocation probabilities are addedExperiments BLEU (%)Moses 29.62+ Phrase collocation probability 30.47+ Improved word alignments+ Phrase collocation probability32.02Table 5.
Performances of Moses employingour proposed methods (Significantly better thanbaseline with p < 0.01)using the same methods as those shown in Table3.
Here, we investigate three different collocationmodels for translation quality improvement.
Theresults are shown in Table 4.From the results of Table 4, it can be seen thatthe systems using the improved bi-directionalalignments achieve higher quality of translationthan the baseline system.
If the same alignmentmethod is used, the systems using CM-3 got thehighest BLEU scores.
And if the same colloca-tion model is used, the systems using WA-3achieved the higher scores.
These results areconsistent with the evaluations of word align-ments as shown in Tables 2 and 3.6.3 Effect of phrase collocation probabili-tiesTo investigate the effectiveness of the methodproposed in section 4, we only use the colloca-tion model CM-3 as described in section 5.1.
Theresults are shown in Table 5.
When the phrasecollocation probabilities are incorporated into theSMT system, the translation quality is improved,achieving an absolute improvement of 0.85BLEU score.
This result indicates that the collo-cation probabilities of phrases are useful in de-termining the boundary of phrase and predictingwhether phrases should be translated together,which helps to improve the phrase-based SMTperformance.Figure 3 shows an example: T1 is generatedby the system where the phrase collocation prob-abilities are used and T2 is generated by thebaseline system.
In this example, since the collo-cation probability of "?
??"
is much higherthan that of "??
?
", our method tends to split"?
??
?"
into "(?
??)
(?
)", rather than"(?)
(??
?)".
For the phrase "??
??"
inthe source sentence, the collocation probabilityof the translation "in order to avoid" is higherthan that of the translation "can we avoid".
Thus,our method selects the former as the translation.Although the phrase "??
??
??
??
??"
in the source sentence has the same transla-tion "We must adopt effective measures", ourmethod splits this phrase into two parts "??
??"
and "??
??
??
", because two partshave higher collocation probabilities than thewhole phrase.We also investigate the performance of thesystem employing both the word alignment im-provement and phrase table improvement me-thods.
From the results in Table 5, it can be seenthat the quality of translation is future improved.As compared with the baseline system, an abso-lute improvement of 2.40 BLEU score isachieved.
And this result is also better than  theresults shown in Table 4.7 Experiments on Parsing-Based SMTWe also investigate the effectiveness of the im-proved word alignments on the parsing-basedSMT system, Joshua (Li et al, 2009).
In this sys-tem, the Hiero-style SCFG model is used(Chiang, 2007), without syntactic information.The rules are extracted only based on the FBIScorpus, where words are aligned by "MW-3 &CM-3".
And the language model is the same asthat in Moses.
The feature weights are tuned onthe development set using the minimum error??
??
??
??
??
??
??
?
??
?wo-men bi-xu      cai-qu   you-xiao  cuo-shi   cai-neng  bi-mian  chu      wen-ti      .we          must        use      effective   measure    can        avoid    out      problem  .We must  adopt effective measures  in order to avoid  problems  .We must adopt effective measures  can we avoid  out of the  question .T1:T2:831Experiments BLEU (%)Joshua 30.05+ Improved word alignments 31.81Table 6.
Performances of Joshua using the dif-ferent word alignments (Significantly better thanbaseline with p < 0.01)rate training method.
We use the same evaluationmeasure as described in section 6.1.The translation results on Joshua are shown inTable 6.
The system using the improved wordalignments achieves an absolute improvement of1.76 BLEU score, which indicates that the im-provements of word alignments are also effectiveto improve the performance of the parsing-basedSMT systems.8 ConclusionWe presented a novel method to use monolingualcollocations to improve SMT.
We first used theMWA method to identify potentially collocatedwords and estimate collocation probabilities onlyfrom monolingual corpora, no additional re-source or linguistic preprocessing is needed.Then the collocation information was employedto improve BWA for various kinds of SMT sys-tems and to improve phrase table for phrase-based SMT.To improve BWA, we re-estimate the align-ment probabilities by using the collocation prob-abilities of words in the same cept.
To improvephrase table, we calculate phrase collocationprobabilities based on word collocation probabil-ities.
Then the phrase collocation probabilitiesare used as additional features in phrase-basedSMT systems.The evaluation results showed that the pro-posed method significantly improved wordalignment, achieving an absolute error rate re-duction of 29% on multi-word alignment.
Theimproved word alignment results in an improve-ment of 2.16 BLEU score on a phrase-basedSMT system and an improvement of 1.76 BLEUscore on a parsing-based SMT system.
When wealso used phrase collocation probabilities as ad-ditional features, the phrase-based SMT perfor-mance is finally improved by 2.40 BLEU scoreas compared with the baseline system.ReferenceLars Ahrenberg, Magnus Merkel, Anna Sagvall Hein,and Jorg Tiedemann.
2000.
Evaluation of WordAlignment Systems.
In Proceedings of the SecondInternational Conference on Language Resourcesand Evaluation, pp.
1255-1261.Yaser Al-Onaizan, Jan Curin, Michael Jahr, KevinKnight, John Lafferty, Dan Melamed, Franz-JosefOch, David Purdy, Noah A. Smith, and David Ya-rowsky.
1999.
Statistical Machine Translation.
Fi-nal Report.
In Johns Hopkins University Workshop.Peter F. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra, and Robert.
L. Mercer.
1993.
The Ma-thematics of Statistical Machine Translation: Pa-rameter estimation.
Computational Linguistics,19(2): 263-311.Colin Cherry and Dekang Lin.
2003.
A ProbabilityModel to Improve Word Alignment.
In Proceed-ings of the 41st Annual Meeting of the Associationfor Computational Linguistics, pp.
88-95.David Chiang.
2007.
Hierarchical Phrase-BasedTranslation.
Computational Linguistics, 33(2):201-228.Fei Huang.
2009.
Confidence Measure for WordAlignment.
In Proceedings of the 47th AnnualMeeting of the ACL and the 4th IJCNLP, pp.
932-940.Philipp Koehn.
2004.
Statistical Significance Tests forMachine Translation Evaluation.
In Proceedings ofthe 2004 Conference on Empirical Methods inNatural Language Processing, pp.
388-395.Philipp Koehn, Amittai Axelrod, Alexandra BirchMayne, Chris Callison-Burch, Miles Osborne, andDavid Talbot.
2005.
Edinburgh System Descriptionfor the 2005 IWSLT Speech Translation Evalua-tion.
In Processings of the International Workshopon Spoken Language Translation 2005.Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003.Statistical Phrase-based Translation.
In Proceed-ings of the Human Language Technology Confe-rence and the North American Association forComputational Linguistics, pp.
127-133.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran Ri-chard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: OpenSource Toolkit for Statistical Machine Translation.In Proceedings of the 45th Annual Meeting of theACL, Poster and Demonstration Sessions, pp.
177-180.Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Ga-nitkevitch, Sanjeev Khudanpur, Lane Schwartz,Wren Thornton, Jonathan Weese, and Omar Zaidan.2009.
Demonstration of Joshua: An Open SourceToolkit for Parsing-based Machine Translation.
InProceedings of the 47th Annual Meeting of the As-832sociation for Computational Linguistics, SoftwareDemonstrations, pp.
25-28.Yang Liu, Qun Liu, and Shouxun Lin.
Log-linearModels for Word Alignment.
2005.
In Proceedingsof the 43rd Annual Meeting of the Association forComputational Linguistics, pp.
459-466.Zhanyi Liu, Haifeng Wang, Hua Wu, and Sheng Li.2009.
Collocation Extraction Using MonolingualWord Alignment Method.
In Proceedings of the2009 Conference on Empirical Methods in NaturalLanguage Processing, pp.
487-495.Daniel Marcu and William Wong.
2002.
A Phrase-Based, Joint Probability Model for Statistical Ma-chine Translation.
In Proceedings of the 2002 Con-ference on Empirical Methods in Natural Lan-guage Processing,  pp.
133-139.Yuval Marton and Philip Resnik.
2008.
Soft SyntacticConstraints for Hierarchical Phrase-Based Transla-tion.
In Proceedings of the 46st Annual Meeting ofthe Association for Computational Linguistics, pp.1003-1011.Kathleen R. McKeown and Dragomir R. Radev.
2000.Collocations.
In Robert Dale, Hermann Moisl, andHarold Somers (Ed.
), A Handbook of Natural Lan-guage Processing, pp.
507-523.Franz Josef Och and Hermann Ney.
2000.
ImprovedStatistical Alignment Models.
In Proceedings ofthe 38th Annual Meeting of the Association forComputational Linguistics, pp.
440-447.Franz Josef Och.
2003.
Minimum Error Rate Trainingin Statistical Machine Translation.
In Proceedingsof the 41st Annual Meeting of the Association forComputational Linguistics, pp.
160-167.Franz Josef Och and Hermann Ney.
2003.
A Syste-matic Comparison of Various Statistical AlignmentModels.
Computational Linguistics, 29(1): 19-52.Kishore Papineni, Salim Roukos, Todd Ward, andWeijing Zhu.
2002.
BLEU: A Method for Auto-matic Evaluation of Machine Translation.
In Pro-ceedings of 40th annual meeting of the Associationfor Computational Linguistics, pp.
311-318.Andreas Stolcke.
2002.
SRILM - An Extensible Lan-guage Modeling Toolkit.
In Proceedings for the In-ternational Conference on Spoken LanguageProcessing, pp.
901-904.Dekai Wu.
1997.
Stochastic Inversion TransductionGrammars and Bilingual Parsing of Parallel Cor-pora.
Computational Linguistics, 23(3): 377-403.Deyi Xiong, Min Zhang, Aiti Aw, and Haizhou Li.2009.
A Syntax-Driven Bracketing Model forPhrase-Based Translation.
In Proceedings of the47th Annual Meeting of the ACL and the 4thIJCNLP, pp.
315-323.833
