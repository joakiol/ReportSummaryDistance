Generating and Validating Abstracts of Meeting Conversations: a UserStudyGabriel Murraygabrielm@cs.ubc.caGiuseppe Careninicarenini@cs.ubc.caDepartment of Computer Science, University of British ColumbiaVancouver, CanadaRaymond Ngrng@cs.ubc.caAbstractIn this paper we present a complete sys-tem for automatically generating naturallanguage abstracts of meeting conversa-tions.
This system is comprised of com-ponents relating to interpretation of themeeting documents according to a meet-ing ontology, transformation or contentselection from that source representationto a summary representation, and gener-ation of new summary text.
In a forma-tive user study, we compare this approachto gold-standard human abstracts and ex-tracts to gauge the usefulness of the dif-ferent summary types for browsing meet-ing conversations.
We find that our auto-matically generated summaries are rankedsignificantly higher than human-selectedextracts on coherence and usability crite-ria.
More generally, users demonstrate astrong preference for abstract-style sum-maries over extracts.1 IntroductionThe most common solution to the task of summa-rizing spoken and written data is sentence (or ut-terance) extraction, where binary sentence classi-fication yields a cut-and-paste summary compris-ing informative sentences from the document con-catenated in a new, condensed document.
Suchextractive approaches have dominated the field ofautomatic summarization for decades, in large partbecause extractive systems do not require a natu-ral language generation (NLG) component sincethe summary sentences are simply lifted from thesource document.Extrinsic evaluations have shown that, while ex-tractive summaries may be less coherent than hu-man abstracts, users still find them to be valuabletools for browsing documents (He et al, 1999;McKeown et al, 2005; Murray et al, 2009).
How-ever, these previous evaluations also illustrate thatconcise abstracts are generally preferred by usersand lead to higher objective task scores.
A weak-ness of typical extractive summaries is that the enduser does not know why the extracted sentencesare important; exploring the original sentence con-text may be the only way to resolve this uncer-tainty.
And if the input source document consistsof noisy, unstructured text such as ungrammatical,disfluent multi-party speech, then the resultant ex-tract is likely to be noisy and unstructured as well.Herein we describe a complete and fully auto-matic system for generating abstract summariesof meeting conversations.
Our abstractor mapsinput sentences to a meeting ontology, generatesmessages that abstract over multiple sentences,selects the most informative messages, and ulti-mately generates new text to describe these rele-vant messages at a high level.
We conduct a userstudy where participants must browse a meetingconversation within a very constrained timeframe,having a summary at their disposal.
We compareour automatic abstracts with human abstracts andextracts and find that our abstract summaries sig-nificantly outperform extracts in terms of coher-ence and usability according to human ratings.
Ingeneral, users rate abstract-style summaries muchmore highly than extracts for these conversations.2 Related ResearchAutomatic summarizaton has been described asconsisting of interpretation, transformation andgeneration (Jones, 1999).
Popular approaches totext extraction essentially collapse interpretationand transformation into one step, with genera-tion either being ignored or consisting of post-processing techniques such as sentence compres-sion (Knight and Marcu, 2000; Clarke and Lapata,2006) or sentence merging (Barzilay and McKe-own, 2005).
In contrast, in this work we clearlyseparate interpretation from transformation and in-corporate an NLG component to generate new textto describe meeting conversations.While extraction remains the most common ap-proach to text summarization, one application inwhich abstractive summarization is widely used isdata-to-text generation.
Summarization is criticalfor data-to-text generation because the amount ofcollected data may be massive.
Examples of suchapplications include the summarization of inten-sive care unit data in the medical domain (Portetet al, 2009) and data from gas turbine sensors (Yuet al, 2007).
Our approach is similar except thatour input is text data in the form of conversations.We otherwise utilize a very similar architecture ofpattern recognition, pattern abstraction, patternselection and summary generation.Kleinbauer et al (2007) carry out topic-basedmeeting abstraction.
Our systems differ in twomajor respects: their summarization process useshuman gold-standard annotations of topic seg-ments, topic labels and content items from the on-tology, while our summarizer is fully automatic;secondly, the ontology they used is specific notjust to meetings but to the AMI scenario meetings(Carletta et al, 2005), while our ontology appliesto conversations in general, allowing our approachto be extended to emails, blogs, etc.In this work we conduct a user study where par-ticipants use summaries to browse meeting tran-scripts.
Some previous work has compared ex-tracts and abstracts for the task of a decision au-dit (Murray et al, 2009) , finding that human ab-stracts are a challenging gold-standard in termsof enabling participants to work quickly and cor-rectly identify the relevant information.
For thattask, automatic extracts and the semi-automaticabstracts of Kleinbauer et al (2007) were foundto be competitive with one another in terms ofuser satisfaction and resultant task scores.
Otherresearch on comparing extracts and abstracts hasfound that an automatic abstractor outperforms ageneric extractor in the domains of technical ar-ticles (Saggion and Lapalme, 2002) and evalua-tive reviews (Carenini and Cheung, 2008), and thathuman-written abstracts were rated best overall.3 Interpretation - Ontology MappingSource document interpretation in our system re-lies on a general conversation ontology.
The on-tology is written in OWL/RDF and contains upper-level classes such as Participant, Entity, Utterance,and DialogueAct.
When additional information isavailable about participant roles in a given domain,Participant subclasses such as ProjectManager canbe utilized.
Object properties connect instances ofontology classes; for example, the following entryin the ontology states that the object property has-Speaker has an instance of Utterance as its domainand an instance of Participant as its range.<owl:ObjectProperty rdf:about="#hasSpeaker"><rdfs:range rdf:resource="#Participant"/><rdfs:domain rdf:resource="#Utterance"/></owl:ObjectProperty>The DialogueAct class has subclasses cor-responding to a variety of sentence-level phe-nomena: decisions, actions, problems, positive-subjective sentences, negative-subjective sen-tences and general extractive sentences (importantsentences that may not match the other categories).Utterance instances are connected to DialogueActsubclasses through an object property hasDAType.A single utterance may correspond to more thanone DialogueAct; for example, it may representboth a positive-subjective sentence and a decision.Our current definition of Entity instances issimple.
The entities in a conversation are nounphrases with mid-range document frequency.
Thisis similar to the definition of concept proposed byXie et al (2009), where n-grams are weightedby tf.idf scores, except that we use noun phrasesrather than any n-grams because we want to referto the entities in the generated text.
We use mid-range document frequency instead of idf (Churchand Gale, 1995), where the entities occur in be-tween 10% and 90% of the documents in the col-lection.
We do not currently attempt coreferenceresolution for entities; recent work has investi-gated coreference resolution for multi-party dia-logues (Muller, 2007; Gupta et al, 2007), but thechallenge of resolution on such noisy data is high-lighted by low accuracy (e.g.
F-measure of 21.21)compared with using well-formed text.We map sentences to our ontology classes bybuilding numerous supervised classifiers trainedon labeled decision sentences, action sentences,etc.
A general extractive classifier is also trainedon sentences simply labeled as important.
We givea specific example of the ontology mapping usingthe following excerpt from the AMI corpus, withentities italicized and resulting sentence classifica-tions shown in bold:?
A: And you two are going to work togetheron a prototype using modelling clay.
[action]?
A: You?ll get specific instructions from yourpersonal coach.
[action]?
C: Cool.
[positive-subjective]?
A: Um did we decide on a chip?
[decision]?
A: Let?s go with a simple chip.
[decision,positive-subjective]The ontology is populated by adding all ofthe sentence entities as instances of the Entityclass, all of the participants as instances of theParticipant class (or its subclasses such as Pro-jectManager when these are represented), and allof the utterances as instances of Utterance withtheir associated hasDAType properties indicatingthe utterance-level phenomena of interest.
Herewe show a sample Utterance instance:<Utterance rdf:about="#ES2014a.B.dact.37"><hasSpeaker rdf:resource="#IndustrialDesigner"/><hasDAType rdf:resource="#PositiveSubjective"/><begTime>456.58</begTime><endTime>458.832</endTime></Utterance>3.1 Feature SetThe interpretation component as just described re-lies on supervised classifiers for the detection ofitems such as decisions, actions, and problems.This component uses general features that are ap-plicable to any conversation domain.
The first setof features we use for this ontology mapping arefeatures relating to conversational structure.
Theyinclude sentence length, sentence position in theconversation and in the current turn, pause-stylefeatures, lexical cohesion, centroid scores, andfeatures that measure how terms cluster betweenconversation participants and conversation turns.While these features have been found to workwell for generic extractive summarization (Murrayand Carenini, 2008), we use additional featuresfor capturing the more specific sentence-level phe-nomena of this research.
These include charactertrigrams, word bigrams, part-of-speech bigrams,word pairs, part-of-speech pairs, and varying in-stantiation n-grams, described in more detail in(Murray et al, 2010).
After removing featuresthat occur fewer than five times, we end up with218,957 total features.3.2 Message GenerationRather than merely classifying individual sen-tences as decisions, action items, and so on, wealso aim to detect larger patterns ?
or messages?
within the meeting.
For example, a given par-ticipant may repeatedly make positive commentsabout an entity throughout the meeting, or maygive contrasting opinions of an entity.
In or-der to determine which messages are essential forsummarizing meetings, three human judges con-ducted a detailed analysis of four developmentset meetings.
They first independently examinedpreviously-written human abstracts for the meet-ings to identify which messages were present inthe summaries.
In the second step, the judges mettogether to decide on a final message set.
Thisresulted in a set of messages common to all themeetings and agreed upon by all the judges.
Themessages that our summarizer will automaticallygenerate are defined as follows:?
OpeningMessage and ClosingMessage: Briefly de-scribes opening/closing of the meeting?
RepeatedPositiveMessage and RepeatedNegativeMes-sage: Describes a participant making positive/negativestatements about a giv en entity?
ActionItemsMessage: Indicates that a participant hasaction items relating to some entity?
DecisionMessage: Indicates that a participant was in-volved in a decision-making process regarding someentity?
ProblemMessage: Indicates that a participant repeat-edly discussed problems or issues about some entity?
GeneralDiscussionMessage: Indicates that a partici-pant repeatedly discussed a given entityMessage generation takes as input the ontologymapping described in the previous section, andoutputs a set of messages for a particular meeting.This is done by identifying pairs of Participantsand Entities that repeatedly co-occur with the var-ious sentence-level predictions.
For example, ifthe project manager repeatedly discusses the inter-face using utterances that are classified as positive-subjective, a RepeatedPositiveMessage is gener-ated for that Participant-Entity pair.
Messages aregenerated in a similar fashion for all other mes-sage types except for the opening and closing mes-sages.
These latter two messages are created sim-ply by identifying which participants were mostactive in the introductory and concluding portionsof the meeting and generating messages that de-scribe that participant opening or closing the meet-ing.Messages types are defined within the OWL on-tology, and the ontology is populated with mes-sage instances for each meeting.
The followingmessage describes the Marketing Expert makinga decision concerning the television, and lists therelevant sentences contained by that decision mes-sage.<DecisionMessage rdf:about="#dec9"><messageSource rdf:resource="#MarketingExpert"/><messageTarget rdf:resource="#television"/><containsUtterance rdf:resource="#ES2014a.D.dact.55"/><containsUtterance rdf:resource="#ES2014a.D.dact.63"/></DecisionMessage>4 Transformation - ILP ContentSelection for MessagesHaving detected all the messages for a given meet-ing conversation, we now turn to the task oftransforming the source representation to a sum-mary representation, which involves identifyingthe most informative messages for which we willgenerate text.
We choose an integer linear pro-gramming (ILP) approach to message selection.ILP has previously been used for sentence selec-tion in an extractive framework.
Xie et al (2009)used ILP to create a summary by maximizing aglobal objective function combining sentence andentity weights.
Our method is similar except thatwe are selecting messages based on optimizingan objective function combining message and sen-tence weights:maximize (1??)?
?iwisi +??
?jujmj (1)subject to?ilisi < L (2)where wi is the score for sentence i, uj is thescore for message j, si is a binary variable in-dicating whether sentence i is selected, mj is abinary variable indicating whether message j isselected, li is the length of sentence i and L isthe desired summary length.
The ?
term is usedto balance sentence and message weights.
Oursentence weight wi is the sum of all the poste-rior probabilities for sentence i derived from thevarious sentence-level classifiers.
In other words,sentences are weighted highly if they correspondto multiple object properties in the ontology.
Tocontinue the example from Section 3, the sen-tence Let?s go with the simple chip will be highlyweighted because it represents both a decision anda positive-subjective opinion.
The message scoreuj is the number of sentences contained by themessage j.
For instance, the DecisionMessageat the end of Section 3.2 contains two sentences.We can create a higher level of abstraction in oursummaries if we select messages which containnumerous utterances.
Similar to how sentencesand concepts are combined in the previous ILP ex-traction approach (Xie et al, 2009; Gillick et al,2009), messages and sentences are tied together bytwo additional constraints:?jmjoij ?
si ?i (3)mjoij ?
si ?ij (4)where oij is the occurence of sentence i in mes-sage j.
These constraints state that a sentence canonly be selected if it occurs in a message that isselected, and that a message can only be selectedif all of its sentences have also been selected.For these initial experiments, ?
is set to 0.5.
Thesummary length L is set to 15% of the conver-sation word count.
Note that this is a constrainton the length of the selected utterances; we ad-ditionally place a length constraint on the gener-ated summary described in the following section.The reason for both types of length constraint is toavoid creating an abstract that is linked to a greatmany conversation utterances but is very brief andlikely to be vague and uninformative.5 Summary GenerationThe generation component of our system fol-lows the standard pipeline architecture (Reiter andDale, 2000), comprised of a text planner, a micro-planner and a realizer.
We describe each of thesein turn.5.1 Text PlanningThe input to the document planner is an ontol-ogy which contains the selected messages fromthe content selection stage.
We take a top-down, schema-based approach to document plan-ning (Reiter and Dale, 2000).
This method is ef-fective for summaries with a canonical structure,as is the case with meetings.
There are three high-level schemas invoked in order: opening mes-sages, body messages, and closing messages.
Forthe body of the summary, messages are retrievedfrom the ontology using SPARQL, an SQL-stylequery language for ontologies, and are clusteredaccording to entities.
Entities are temporally or-dered according to their average timestamp in themeeting.
In the overall document plan tree struc-ture, the body plan is comprised of document sub-plans for each entity, and the document sub-planfor each entity is comprised of document sub-plans for each message type.
The output of thedocument planner is a tree structure with messagesas its leaves and document plans for its internalnodes.
Our text planner is implemented within theJena semantic web programming framework1.5.2 MicroplanningThe microplanner takes the document plan as in-put and performs two operations: aggregation andgeneration of referring expressions.5.2.1 AggregationThere are several possibilities for aggregation inthis domain, such as aggregating over participants,entities and message types.
The analysis of ourfour development set meetings revealed that ag-gregation over meeting participants is quite com-mon in human abstracts, so our system supportssuch aggregation.
This involves combining mes-sages that differ in participants but share a com-mon entity and message type; for example, if thereare two RepeatedPositiveMessage instances aboutthe user interface, one with the project manageras the source and one with the industrial designeras the source, a single RepeatedPositiveMessageinstance is created that contains two sources.
Wedo not aggregate over entities for the sole reasonthat the text planner already clustered messagesaccording to entity.
The entity clustering is in-tended to give the summary a more coherent struc-ture but has the effect of prohibiting aggregationover entities.5.2.2 Referring ExpressionsTo reduce redundancy in our generated abstracts,we generate alternative referring expressions whena participant or an entity is mentioned multipletimes in sequence.
For participants, this meansthe generation of a personal pronoun.
For entities,rather than referring repeatedly to, e.g., the remotecontrol, we generate expressions such as that issueor this matter.5.3 RealizationThe text realizer takes the output of the microplan-ner and generates a textual summary of a meet-ing.
This is accomplished by first associating ele-ments of the ontology with linguistic annotations.For example, participants are associated with anoun phrase denoting their role, such as the projectmanager.
Since entities were defined simply asnoun phrases with mid-frequency IDF scores, anentity instance is associated with that noun phrase.Messages themselves are associated with verbs,1to be made publicly available upon publicatonsubject templates and object templates.
For exam-ple, instances of DecisionMessage are associatedwith the verb make, have a subject template set tothe noun phrase of the message source, and havean object template [NP a decision PP [concern-ing ]] where the object of the prepositionalphrase is the noun phrase associated with the mes-sage target.To give a concrete example, consider the fol-lowing decision message:<DecisionMessage rdf:about="#dec9"><rdf:type rdf:resource="&owl;Thing"/><hasVerb>make</hasVerb><hasCompl>a decision</hasCompl><messageSource rdf:resource="#MarketingExpert"/><messageSource rdf:resource="#ProjectManager"/><messageTarget rdf:resource="#television"/><containsUtterance rdf:resource="#ES2014a.D.dact.55"/><containsUtterance rdf:resource="#ES2014a.D.dact.63"/></DecisionMessage>There are two message sources,ProjectManager and MarketingExpert,and one message target, television.
Thesubjects of the message are set to be the nounphrases associated with the marketing expert andthe project manager, while the object template isfilled with the noun phrase the television.
Thismessage is realized as The project manager andthe marketing expert made a decision about thetelevision.For our realizer we use simpleNLG2.
We tra-verse the document plan output by the microplan-ner and generate a sentence for each message leaf.A new paragraph is created when both the messagetype and target of the current message are differentthan the message type and target for the previousmessage.6 Task-Based User StudyWe carried out a formative user study in order toinform this early work on automatic conversationabstraction.
This task required participants to re-view meeting conversations within a short time-frame, having a summary at their disposal.
Wecompared human abstracts and extracts with ourautomatically generated abstracts.
The interpre-tation component and a preliminary version ofthe transformation component have already beentested in previous work (Murray et al, 2010).
Thesentence-level classifiers were found to performwell according to the area under the receiver op-erator characteristic (AUROC) metric, which eva-lutes the true-positive/false-positive ratio as the2http://www.csd.abdn.ac.uk/?ereiter/simplenlg/posterior threshold is varied, with scores rangingfrom 0.76 for subjective sentences to 0.92 for ac-tion item sentences.
In the following, we focuson the formative evaluation of the complete sys-tem.
We first describe the corpus we used, thenthe materials, participants and procedure.
Finallywe discuss the study results.6.1 AMI Meeting CorpusFor our meeting summarization experiments, weuse the scenario portion of the AMI corpus (Car-letta et al, 2005), where groups of four partici-pants take part in a series of four meetings andplay roles within a fictitious company.
There are140 of these meetings in total.
For the sum-mary annotation, annotators wrote abstract sum-maries of each meeting and extracted sentencesthat best conveyed or supported the informationin the abstracts.
The human-authored abstractseach contain a general abstract summary and threesubsections for ?decisions,?
?actions?
and ?prob-lems?
from the meeting.
A many-to-many map-ping between transcript sentences and sentencesfrom the human abstract was obtained for each an-notator.
Approximately 13% of the total transcriptsentences are ultimately labeled as extracted sen-tences.
A sentence is considered a decision itemif it is linked to the decision portion of the ab-stract, and action and problem sentences are de-rived similarly.
We additionally use subjectivityand polarity annotations for the AMI corpus (Wil-son, 2008).6.2 Materials, Participants and ProceduresWe selected five AMI meetings for this user study,with each stage of the four-stage AMI scenariorepresented.
The meetings average approximately500 sentences each.
We included the follow-ing three types of summaries for each meeting:(EH) gold-standard human extracts, (AH) gold-standard human abstracts described in Section6.1, and (AA) the automatic abstracts output byour abstractor.
All three conditions feature man-ual transcriptions of the conversation.
Each sum-mary contains links to the sentences in the meet-ing transcript.
For extracts, this is a one-to-onemapping.
For the two abstract conditions, this canbe a many-to-many mapping between abstract sen-tences and transcript sentences.Participants were given instructions to browseeach meeting in order to understand the gist ofthe meeting, taking no longer than 15 minutes permeeting.
They were asked to consider the sce-nario in which they were a company employeewho wanted to quickly review a previous meet-ing by using a browsing interface designed for thistask.
Figure 1 shows the browsing interface formeeting IS1001d with an automatically generatedabstract on the left-hand side and the transcript onthe right.
In the screenshot, the user has clickedthe abstract sentence The industrial designer madea decision on the cost and has been linked to atranscript utterance, highlighted in yellow, whichreads Also for the cost, we should only put one bat-tery in it.
Notice that this output is not entirely cor-rect, as the decision pertained to the battery, whichimpacted the cost.
This sentence was generatedbecause the entity cost appeared in several deci-sion sentences.The time constraint meant that it was not fea-sible to simply read the entire transcript straightthrough.
Participants were free to adopt whateverbrowsing strategy suited them, including skim-ming the transcript and using the summary as theysaw fit.
Upon finishing their review of each meet-ing, participants were asked to rate their level ofagreement or disagreement on several Likert-stylestatements relating to the difficulty of the task andthe usefulness of the summary.
There were sixstatements to be evaluated on a 1-5 scale, with1 indicating strong disagreement and 5 indicatingstrong agreement:?
Q1: I understood the overall content of the discussion.?
Q2: It required a lot of effort to review the meeting inthe allotted time.?
Q3: The summary was coherent and readable.?
Q4: The information in the summary was relevant.?
Q5: The summary was useful for navigating the dis-cussion.?
Q6: The summary was missing relevant information.Participants were also asked if there was any-thing they would have liked to have seen in thesummary, and whether they had any general com-ments on the summary.We recruited 19 participants in total, with eachreceiving financial reimbursement for their partic-ipation.
Each participant saw one summary permeeting and rated every summary condition dur-ing the experiment.
We varied the order of themeetings and summary conditions.
With 19 sub-jects, three summary conditions and six Likertstatements, we collected a total of 342 user judg-ments.
To ensure fair comparison between thethree summary types, we limit summary length toFigure 1: Summary Interfacebe equal to the length of the human abstract foreach meeting.
This ranges from approximately190 to 350 words per meeting summary.6.2.1 Results and DiscussionParticipants took approximately 12 minutes on av-erage to review each meeting, slightly shorter thanthe maximum allotted fifteen minutes.Figure 2 shows the average ratings for eachsummary condition on each Likert statement.
ForQ1, which concerns general comprehension ofthe meeting discussion, condition AH (humanabstracts) is rated significantly higher than EH(human extracts) and AA (automatic abstracts)(p=0.0016 and p=0.0119 according to t-test, re-spectively).
However, for the other statement thataddresses the overall task, Q2, AA is rated bestoverall.
Note that for Q2 a lower score is better.While there are no significantly differences on thiscriterion, it is a compelling finding that automaticabstracts can greatly reduce the effort required forreviewing the meeting, at a level comparable tohuman abstracts.Q3 concerns coherence and readability.
Condi-tion AH is significantly better than both EH andAA (p<0.0001 and p=0.0321).
Our condition AAis also significantly better than the extractive con-dition EH (p=0.0196).
In the introduction we men-tioned that a potential weakness of extractive sum-maries is that coherence and readability decreasewhen sentences are removed from their originalcontexts, and that extracts of noisy, unstructuredsource documents will tend to be noisy and un-structured as well.
These ratings confirm that ex-tracts are not rated well on coherence and readabil-ity.Q4 concerns the perceived relevance of thesummary.
Condition AH is again significantly bet-ter than EH and AH (both p<0.0001).
AA is ratedsubstantially higher than EH on summary rele-vance, but not at a significant level.Q5 is a key question because it directly ad-dresses the issue of summary usability for such atask.
Condition AH is significantly better than EHand AA (both p<0.0001), but we also find that AAis significantly better than EH (p=0.0476).
Ex-tracts have an average score of only 2.37 out of5, compared with 3.21 and 4.63 for automatic andhuman abstracts, respectively.
For quickly review-ing a meeting conversation, abstracts are muchmore useful than extracts.Q6 indicates whether the summaries were miss-ing any relevant information.
As with Q2, a lowerscore is better.
Condition AH is significantly bet-ter than EH and AA (p<0.0001 and p=0.0179),while AA is better than EH with marginal signif-icance (p=0.0778).
This indicates that our auto-matic abstracts were better at containing all therelevant information than were human-selectedextracts.All participants gave written answers to theopen-ended questions, yielding insights into thestrengths and weaknesses of the different sum-mary types.
Regarding the automatic abstracts(AA), the most common criticisms were that the012345Q1 - Understood MeetingQ2 - Required Effort**Q3 - Summary CoherentQ4 - Summary RelevantQ5 - Summary UsefulQ6 - Summary Missing Info**AverageUserRatingsHuman AbstractsAuto AbstractsHuman ExtractsFigure 2: User Ratings (** indicates lower scoreis better)summaries are too vague (e.g.
?more concretewould help?)
and that the phrasing can be repet-itive.
There is a potential many-to-many map-ping between abstract sentences and transcriptsentences, and some participants felt that it wasunnecessarily redundant to be linked to the sametranscript sentence more than once (e.g.
?quite afew repetitive citations?).
Several participants feltthat the sentences regarding positive-subjectiveand negative-subjective opinions were overstatedand that the actual opinions were either more sub-tle or neutral.
One participant wrote that these sen-tences constituted ?a lot of bias in the summary.
?On the positive side, several participants consid-ered the links between abstract sentences and tran-script sentences to be very helpful, e.g.
?it re-ally linked to the transcript well?
and ?I like howthe summary has links connected to the transcript.Easier to follow-up on the meeting w/ the aid ofthe summary.?
One participant particularly likedthe subjectivity-oriented sentences: ?Lifting someof the positive/negative from the discussion intothe summary can mean the discussion does noteven need to be included to get understanding.
?The written comments on the extractive condi-tion (EH) were almost wholly negative.
Many par-ticipants felt that the extracts did not even con-stitute a summary or that a cut-and-paste fromthe transcript does not make a sufficient summary(e.g.
?The summary was not helpful @ all be-cause it?s just cut from the transcript?, ?All copyand paste not a summary?, ?Not very clear sum-mary - looked like the transcript?, and ?No ef-fort was made in the summary to put things intocontext?).
Interestingly, several participants criti-cized the extracts for not containing the most im-portant sentences from the transcript despite thesebeing human-selected extracts, demonstrating thata good summary is a subjective matter.The comments on human abstracts (AH) weregenerally very positive, e.g.
?easy to follow?, ?itwas good, clear?, and ?I could?ve just read thesummary and still understood the bulk of the meet-ing?s content.?
The most frequent negative criti-cisms were that the abstract sentences sometimescontained too many links to the transcript (?mas-sive amount of links look daunting?
), and that thesummaries were sometimes too vague (?perhapssome points from the discussion can be included,instead of just having topic outlines?, ?
[want] spe-cific details?).
It is interesting to observe that thislatter criticism is shared between human abstractsand our automatic abstracts.
When generalizingover the source document, details are sometimessacrificed.7 ConclusionWe have presented a system for automatically gen-erating abstracts of meeting conversations.
Thissummarizer relies on first mapping sentences toa conversation ontology representing phenomenasuch as decisions, action items and sentiment, thenidentifying message patterns that abstract overmultiple sentences.
We select the most informa-tive messages through an ILP optimization ap-proach, aggregate messages, and finally generatetext describing all of the selected messages.
Aformative user study shows that, overall, our auto-matic abstractive summaries rate very well in com-parison with human extracts, particularly regard-ing readability, coherence and usefulness.
Theautomatic abstracts are also significantly better interms of containing all of the relevant information(Q6), and it is impressive that an automatic ab-stractor substantially outperforms human-selectedcontent on such a metric.
In future work we aimto bridge the performance gap between automaticand human abstracts by identifying more specificmessages and reducing redundancy in the sentencemapping.
We plan to improve the NLG output byintroducing more linguistic variety and better textstructuring.
We are also investigating the impactof ASR transcripts on abstracts and extracts, withencouraging early results.Acknowledgments Thanks to Nicholas Fitzgerald forwork on implementing the top-down planner.ReferencesR.
Barzilay and K. McKeown.
2005.
Sentence fusionfor multidocument news summarization.
Computa-tional Linguistics, 31(3):297?328.G.
Carenini and JCK Cheung.
2008.
Extractive vs.nlg-based abstractive summarization of evaluativetext: The effect of corpus controveriality.
In Proc.of the 5th International Natural Generation Confer-ence.J.
Carletta, S. Ashby, S. Bourban, M. Flynn,M.
Guillemot, T. Hain, J. Kadlec, V. Karaiskos,W.
Kraaij, M. Kronenthal, G. Lathoud, M. Lincoln,A.
Lisowska, I. McCowan, W. Post, D. Reidsma, andP.
Wellner.
2005.
The AMI meeting corpus: A pre-announcement.
In Proc.
of MLMI 2005, Edinburgh,UK, pages 28?39.K.
Church and W. Gale.
1995.
Inverse document fre-quency IDF: A measure of deviation from poisson.In Proc.
of the Third Workshop on Very Large Cor-pora, pages 121?130.J.
Clarke and M. Lapata.
2006.
Constraint-basedsentence compression: An integer programming ap-proach.
In Proc.
of COLING/ACL 2006, pages 144?151.D.
Gillick, K. Riedhammer, B. Favre, and D. Hakkani-Tu?r.
2009.
A global optimization framework formeeting summarization.
In Proc.
of ICASSP 2009,Taipei, Taiwan.S.
Gupta, J. Niekrasz, M. Purver, and D. Jurafsky.2007.
Resolving ?You?
in multi-party dialog.
InProc.
of SIGdial 2007, Antwerp, Belgium.L.
He, E. Sanocki, A. Gupta, and J. Grudin.
1999.Auto-summarization of audio-video presentations.In Proc.
of ACM MULTIMEDIA ?99, Orlando, FL,USA, pages 489?498.K.
Spa?rck Jones.
1999.
Automatic summarizing: Fac-tors and directions.
In I. Mani and M. Maybury,editors, Advances in Automatic Text Summarization,pages 1?12.
MITP.T.
Kleinbauer, S. Becker, and T. Becker.
2007.
Com-bining multiple information layers for the automaticgeneration of indicative meeting abstracts.
In Proc.of ENLG 2007, Dagstuhl, Germany.K.
Knight and D. Marcu.
2000.
Statistics-based sum-marization - step one: Sentence compression.
InProc.
of AAAI 2000, Austin, Texas, USA, pages 703?710.K.
McKeown, J. Hirschberg, M. Galley, and S. Maskey.2005.
From text to speech summarization.
In Proc.of ICASSP 2005, Philadelphia, USA, pages 997?1000.C.
Muller.
2007.
Resolving It, This and That in un-restricted multi-party dialog.
In Proc.
of ACL 2007,Prague, Czech Republic.G.
Murray and G. Carenini.
2008.
Summarizing spo-ken and written conversations.
In Proc.
of EMNLP2008, Honolulu, HI, USA.G.
Murray, T. Kleinbauer, P. Poller, S. Renals,T.
Becker, and J. Kilgour.
2009.
Extrinsic sum-marization evaluation: A decision audit task.
ACMTransactions on SLP, 6(2).G.
Murray, G. Carenini, and R. Ng.
2010.
Interpre-tation and transformation for abstracting conversa-tions.
In Proc.
of NAACL 2010, Los Angeles, USA.F.
Portet, E. Reiter, A. Gatt, J.
Hunter, S. Sripada,Y.
Freer, and C. Sykes.
2009.
Automatic gener-ation of textual summaries from neonatal intensivecare data.
Artificial Intelligence, 173:789?816.E.
Reiter and R. Dale.
2000.
Building Natural Lan-guage Generation Systems.
Cambridge UniversityPress, Cambridge, GB.H.
Saggion and G. Lapalme.
2002.
Generat-ing indicative-informative summaries with sumum.Computational Linguistics, 28(4):497?526.T.
Wilson.
2008.
Annotating subjective content inmeetings.
In Proc.
of LREC 2008, Marrakech, Mo-rocco.S.
Xie, B. Favre, D. Hakkani-Tu?r, and Y. Liu.
2009.Leveraging sentence weights in a concept-based op-timization framework for extractive meeting sum-marization.
In Proc.
of Interspeech 2009, Brighton,England.J.
Yu, E. Reiter, J.
Hunter, and C. Mellish.
2007.Choosing the content of textual summaries of largetime-series data sets.
Journal of Natural LanguageEngineering, 13:25?49.
