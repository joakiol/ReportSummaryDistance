Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1099?1109,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsConstraints based Taxonomic Relation ClassificationQuang Xuan Do Dan RothDepartment of Computer ScienceUniversity of Illinois at Urbana-ChampaignUrbana, IL 61801, USA{quangdo2,danr}@illinois.eduAbstractDetermining whether two terms in text havean ancestor relation (e.g.
Toyota and car) ora sibling relation (e.g.
Toyota and Honda) isan essential component of textual inference inNLP applications such as Question Answer-ing, Summarization, and Recognizing TextualEntailment.
Significant work has been doneon developing stationary knowledge sourcesthat could potentially support these tasks, butthese resources often suffer from low cover-age, noise, and are inflexible when needed tosupport terms that are not identical to thoseplaced in them, making their use as generalpurpose background knowledge resources dif-ficult.
In this paper, rather than building a sta-tionary hierarchical structure of terms and re-lations, we describe a system that, given twoterms, determines the taxonomic relation be-tween them using a machine learning-basedapproach that makes use of existing resources.Moreover, we develop a global constraint opti-mization inference process and use it to lever-age an existing knowledge base also to enforcerelational constraints among terms and thusimprove the classifier predictions.
Our exper-imental evaluation shows that our approachsignificantly outperforms other systems builtupon existing well-known knowledge sources.1 IntroductionTaxonomic relations that are read off of structuredontological knowledge bases have been shown toplay important roles in many computational linguis-tics tasks, such as document clustering (Hotho etal., 2003), navigating text databases (Chakrabarti etal., 1997), Question Answering (QA) (Saxena et al,2007) and summarization (Vikas et al, 2008).
Itis clear that the recognition of taxonomic relationbetween terms in sentences is essential to supporttextual inference tasks such as Recognizing TextualEntailment (RTE) (Dagan et al, 2006).
For exam-ple, it may be important to know that a blue Toy-ota is neither a red Toyota nor a blue Honda, butthat all are cars, and even Japanese cars.
Work inTextual Entailment has argued quite convincingly(MacCartney and Manning, 2008; MacCartney andManning, 2009) that many such textual inferencesare largely compositional and depend on the abilityto recognize some basic taxonomic relations suchas the ancestor or sibling relations between terms.To date, these taxonomic relations can be read offmanually generated ontologies such as Wordnet thatexplicitly represent these, and there has also beensome work trying to extend the manually built re-sources using automatic acquisition methods result-ing in structured knowledge bases such as the Ex-tended WordNet (Snow et al, 2006) and the YAGOontology (Suchanek et al, 2007).However, identifying when these relations holdusing fixed stationary hierarchical structures maybe impaired by noise in the resource and by uncer-tainty in mapping targeted terms to concepts in thestructures.
In addition, for knowledge sources de-rived using bootstrapping algorithms and distribu-tional semantic models such as (Pantel and Pen-nacchiotti, 2006; Kozareva et al, 2008; Baroni andLenci, 2010), there is typically a trade-off betweenprecision and recall, resulting either in a relativelyaccurate resource with low coverage or a noisy re-1099source with broader coverage.
In the current work,we take a different approach, identifying directlywhether a pair of terms hold a taxonomic relation.Fixed resources, as we observe, are inflexiblewhen dealing with targeted terms not being cov-ered.
This often happens when targeted terms havethe same meaning, but different surface forms, thanthe terms used in the resources (e.g.
Toyota Camryand Camry).
We argue that it is essential to have aclassifier that, given two terms, can build a semanticrepresentation of the terms and determines the tax-onomic relations between them.
This classifier willmake use of existing knowledge bases in multipleways, but will provide significantly larger coverageand more precise results.
We make use of a dynamicresource such as Wikipedia to guarantee increasedcoverage without changing our model and also per-form normalization-to-Wikipedia to find appropri-ate Wikipedia replacements for outside-Wikipediaterms.
Moreover, stationary resources are usuallybrittle because of the way most of them are built:using local relational patterns (e.g.
(Hearst, 1992;Snow et al, 2005)).
Infrequent terms are less likelyto be covered, and some relations may not be sup-ported well by these methods because their cor-responding terms rarely appear in close proximity(e.g., an Israeli tennis player Dudi Sela and RogerFederrer).
Our approach uses search techniques togather relevant Wikipedia pages of input terms andperforms a learning-based classification w.r.t.
to thefeatures extracted from these pages as a way to getaround this brittleness.Motivated by the needs of NLP applications suchas RTE, QA, Summarization, and the composition-ality argument alluded to above, we focus on identi-fying two fundamental types of taxonomic relations- ancestor and sibling.
An ancestor relation and itsdirectionality can help us infer that a statement withrespect to the child (e.g.
cannabis) holds for anancestor (e.g.
drugs) as in the following example,taken from a textual entailment challenge dataset:T: Nigeria?s NDLEA has seized 80 metrictonnes of cannabis in one of its largest everhauls, officials say.H: Nigeria seizes 80 tonnes of drugs.Similarly, it is important to know of a sibling re-lation to infer that a statement about Taiwan may(without additional information) contradict a simi-lar statement with respect to Japan since these aredifferent countries, as in the following:T: A strong earthquake struck off the southerntip of Taiwan at 12:26 UTC, triggering a warn-ing from Japan?s Meteorological Agency thata 3.3 foot tsunami could be heading towardsBasco, in the Philippines.H: An earthquake strikes Japan.Several recent TE studies (Abad et al, 2010; Sam-mons et al, 2010) suggest to isolate TE phenomena,such as recognizing taxonomic relations, and studythem separately; they discuss some of characteristicsof phenomena such as contradiction from a similarperspective to ours, but do not provide a solution.In this paper, we present TAxonomic RElationClassifier (TAREC), a system that classifies taxo-nomic relations between a given pair of terms us-ing a machine learning based classifier.
An inte-gral part of TAREC is also our inference model thatmakes use of relational constraints to enforce co-herency among several related predictions.
TARECdoes not aim at building or extracting a hierarchi-cal structure of concepts and relations, but rather todirectly recognize taxonomic relations given a pairof terms.
Target terms are represented using vectorof features that are extracted from retrieved corre-sponding Wikipedia pages.
In addition, we makeuse of existing stationary ontologies to find relatedterms to the target terms, and classify those too.
Thisallows us to make use of a constraint-based infer-ence model (following (Roth and Yih, 2004; Rothand Yih, 2007) that enforces coherency of decisionsacross related pairs (e.g., if x is-a y and y is-a z, itcannot be that x is a sibling of z).In the rest of the paper, after discussing re-lated work in Section 2, we present an overview ofTAREC in Section 3.
The learning component andthe inference model of TAREC are described in Sec-tions 4 and 5.
We experimentally evaluate TARECin Section 6 and conclude our paper in Section 7.2 Related WorkThere are several works that aim at building tax-onomies and ontologies which organize conceptsand their taxonomic relations into hierarchical struc-tures.
(Snow et al, 2005; Snow et al, 2006) con-1100structed classifiers to identify hypernym relation-ship between terms from dependency trees of largecorpora.
Terms with recognized hypernym rela-tion are extracted and incorporated into a man-madelexical database, WordNet (Fellbaum, 1998), re-sulting in the extended WordNet, which has beenaugmented with over 400, 000 synsets.
(Ponzettoand Strube, 2007) and (Suchanek et al, 2007) bothmined Wikipedia to construct hierarchical structuresof concepts and relations.
While the former ex-ploited Wikipedia category system as a conceptualnetwork and extracted a taxonomy consisting of sub-sumption relations, the latter presented the YAGOontology, which was automatically constructed bymining and combining Wikipedia and WordNet.
Anatural way to use these hierarchical structures tosupport taxonomic relation classification is to maptargeted terms onto the hierarchies and check ifthey subsume each other or share a common sub-sumer.
However, this approach is limited becauseconstructed hierarchies may suffer from noise andrequire exact mapping (Section 6).
TAREC over-comes these limitations by searching and selectingthe top relevant articles in Wikipedia for each inputterm; taxonomic relations are then recognized basedon the features extracted from these articles.On the other hand, information extraction boot-strapping algorithms, such as (Pantel and Pennac-chiotti, 2006; Kozareva et al, 2008), automaticallyharvest related terms on large corpora by startingwith a few seeds of pre-specified relations (e.g.
is-a, part-of).
Bootstrapping algorithms rely on somescoring function to assess the quality of terms andadditional patterns extracted during bootstrapping it-erations.
Similarly, but with a different focus, OpenIE, (Banko and Etzioni, 2008; Davidov and Rap-poport, 2008), deals with a large number of relationswhich are not pre-specified.
Either way, the out-put of these algorithms is usually limited to a smallnumber of high-quality terms while sacrificing cov-erage (or vice versa).
Moreover, an Open IE sys-tem cannot control the extracted relations and this isessential when identifying taxonomic relations.
Re-cently, (Baroni and Lenci, 2010) described a gen-eral framework of distributional semantic modelsthat extracts significant contexts of given terms fromlarge corpora.
Consequently, a term can be repre-sented by a vector of contexts in which it frequentlyappears.
Any vector space model could then use theterms?
vectors to cluster terms into categories.
Sib-ling terms (e.g.
Honda, Toyota), therefore, have veryhigh chance to be clustered together.
Nevertheless,this approach cannot recognize ancestor relations.In this paper, we compare TAREC with this frame-work only on recognizing sibling vs. no relation, ina strict experimental setting which pre-specifies thecategories to which the terms belong.3 An Overview of the TAREC Algorithm3.1 PreliminariesIn the TAREC algorithm, a term refers to any men-tion in text, such as mountain, George W. Bush, bat-tle of Normandy.
TAREC does not aim at extractingterms and building a stationary hierarchical structureof terms, but rather recognize the taxonomic relationbetween any two given terms.
TAREC focuses onclassifying two fundamental types of taxonomic re-lations: ancestor and sibling.
Determining whethertwo terms hold a taxonomic relation depends on apragmatic decision of how far one wants to climb upa taxonomy to find a common subsumer.
For exam-ple, George W. Bush is a child of Presidents of theUnited States as well as people, even more, that termcould also be considered as a child of mammals ororganisms w.r.t.
the Wikipedia category system; inthat sense, George W. Bush may be considered as asibling of oak because they have organisms as a leastcommon subsumer.
TAREC makes use of a hierar-chical structure as background knowledge and con-siders two terms to hold a taxonomic relation onlyif the relation can be recognized from informationacquired by climbing up at most K levels from therepresentation of the target terms in the structure.
Itis also possible that the sibling relation can be rec-ognized by clustering terms together by using vectorspace models.
If so, two terms are siblings if theybelong to the same cluster.To cast the problem of identifying taxonomic rela-tions between two terms x and y in a machine learn-ing perspective, we model it as a multi-class classi-fication problem.
Table 1 defines four relations withsome examples in our experiment data sets.This paper focuses on studying a fundamentalproblem of recognizing taxonomic relations (givenwell-segmented terms) and leaves the orthogonal is-1101ExamplesRelation Meaning Term x Term yx?
y x is an ancestor actor Mel Gibsonof y food ricex?
y x is a child Makalu mountainof y Monopoly gamex?
y x and y are Paris Londonsiblings copper oxygenx= y x and y have Roja C++no relation egg VegaTable 1: Taxonomic relations and some examples in ourdata sets.sues of how to take contexts into account and how itshould be used in applications to a future work.3.2 The Overview of TARECAssume that we already have a learned local clas-sifier that can classify taxonomic relations betweenany two terms.
Given two terms, TAREC usesWikipedia and the local classifier in an inferencemodel to make a final prediction on the taxonomicrelation between these two.
To motivate the need foran inference model, beyond the local classifier itself,we observe that the presence of other terms in addi-tion to the two input terms, can provide some naturalconstraints on the possible taxonomic relations andthus can be used to make the final prediction (whichwe also refer as global prediction) more coherent.
Inpractice, we first train a local classifier (Section 4),then incorporate it into an inference model (Section5) to classify taxonomic relations between terms.The TAREC algorithm consists of three steps andis summarized in Figure 1 and explained below.1.
Normalizing input terms to Wikipedia: Al-though most commonly used terms have corre-sponding Wikipedia articles, there are still a lot ofterms with no corresponding Wikipedia articles.
Fora non-Wikipedia term, we make an attempt to finda replacement by using Web search.
We wish tofind a replacement such that the taxonomic relationis unchanged.
For example, for input pair (LojzeKovac?ic?, Rudi S?eligo), there is no English Wikipediapage for Lojze Kovac?ic?, but if we can find MarjanRoz?anc and use it as a replacement of Lojze Kovac?ic?
(two terms are siblings and refer to two writers), wecan continue classifying the taxonomic relation ofthe pair (Marjan Roz?anc, Rudi S?eligo).
This partof the algorithm was motivated by (Sarmento et al,TAxonomic RElation Classifier (TAREC)INPUT: A pair of terms (x, y)A learned local classifierR (Sec.
4)WikipediaWOUTPUT: Taxonomic relation r?
between x and y1.
(x, y)?
NormalizeToWikipedia(x, y,W)2.
Z ?
GetAddionalTerms(x, y) (Sec.
5.2)3. r?
= ClassifyAndInference(x, y,Z,R,W) (Sec.
5.1)RETURN: r?
;Figure 1: The TAREC algorithm.2007).
We first make a query with the two inputterms (e.g.
?Lojze Kovac?ic??
AND ?Rudi S?eligo?
)to search for list-structure snippets in Web docu-ments1 such as ?...
?delimiter?
ca ?delimiter?
cb?delimiter?
cc ?delimiter?
...?
(the two input termsshould be among ca, cb, cc, ...).
The delimiter couldbe commas, periods, or asterisks2.
For snippets thatcontain the patterns of interest, we extract ca, cb, ccetc.
as replacement candidates.
To reduce noise,we empirically constrain the list to contain at least4 terms that are no longer than 20 characters each.The candidates are ranked based on their occurrencefrequency.
The top candidate with Wikipedia pagesis used as a replacement.2.
Getting additional terms (Section 5.2): TARECleverage an existing knowledge base to extract addi-tional terms related to the input terms, to be used inthe inference model in step 3.3.
Making global prediction with relational con-straints (Section 5.1): TAREC performs several lo-cal predictions using the local classifier R (Section4) on the two input terms and these terms with theadditional ones.
The global prediction is then in-ferred by enforcing relational constraints among theterms?
relations.4 Learning Taxonomic RelationsThe local classifier of TAREC is trained on thepairs of terms with correct taxonomic relation labels(some examples are showed in Table 1).
The trainedclassifier when applied on a new input pair of termswill return a real valued number which can be inter-preted as the probability of the predicted label.
Inthis section, we describe the learning features used1We use http://developer.yahoo.com/search/web/2Periods and asterisks capture enumerations.1102Title/Term Text CategoriesPresident ofthe UnitedStatesThe President of the United States is the head of state and head of government of the United States and is thehighest political official in the United States by influence and recognition.
The President leads the executivebranch of the federal government and is one of only two elected members of the executive branch...Presidents of the United States, Presidency ofthe United StatesGeorge W.BushGeorge Walker Bush; born July 6, 1946) served as the 43rd President of the United States from 2001 to 2009.He was the 46th Governor of Texas from 1995 to 2000 before being sworn in as President on January 20, 2001...Children of Presidents of the United States, Gov-ernors of Texas, Presidents of the United States,Texas Republicans...Gerald Ford Gerald Rudolff Ford (born Leslie Lynch King, Jr.) (July 14, 1913 December 26, 2006) was the 38th Presidentof the United States, serving from 1974 to 1977, and the 40th Vice President of the United States serving from1973 to 1974.Presidents of the United States, Vice Presidentsof the United States, Republican Party (UnitedStates) presidential nominees...Table 2: Examples of texts and categories of Wikipedia articles.by our local taxonomic relation classifier.Given two input terms, we first build a semanticrepresentation for each term by using a local searchengine3 to retrieve a list of top articles in Wikipediathat are relevant to the term.
To do this, we use thefollowing procedure: (1) Using both terms to make aquery (e.g.
?George W. Bush?
AND ?Bill Clinton?
)to search in Wikipedia ; (2) Extracting importantkeywords in the titles and categories of the retrievedarticles using TF-IDF (e.g.
president, politician); (3)Combining each input term with the extracted key-words (e.g.
?George W. Bush?
AND ?president?AND ?politician?)
to create a final query used tosearch for the term?s relevant articles in Wikipedia.This is motivated by the assumption that the realworld applications calling TAREC typically does sowith two terms that are related in some sense, so ourprocedure is designed to exploit that.
For example,it?s more likely that term Ford in the pair (GeorgeW.
Bush, Ford) refers to the former president of theUnited States, Gerald Ford, than the founder of FordMotor Company, Henry Ford.Once we have a semantic representation of eachterm, in the form of the extracted articles, we extractfrom it features that we use as the representation ofthe two input terms in our learning algorithm.
It isworth noting that a Wikipedia page usually consistsof a title (i.e.
the term), a body text, and a list ofcategories to which the page belongs.
Table 2 showssome Wikipedia articles.
From now on, we use thetitles of x, the texts of x, and the categories of x torefer to the titles, texts, and categories of the asso-ciated articles in the representation of x.
Below arethe learning features extracted for input pair (x,y).Bags-of-words Similarity: We use cosine simi-larity metric to measure the degree of similarity be-tween bags of words.
We define four bags-of-wordsfeatures as the degree of similarity between the texts3E.g.
http://lucene.apache.org/Degree of similaritytexts(x) vs. categories(y)categories(x) vs. texts(y)texts(x) vs. texts(y)categories(x) vs. categories(y)Table 3: Bag-of-word features of the pair of terms (x,y);texts(.)
and categories(.)
are two functions that extractassociated texts and categories from the semantic repre-sentation of x and y.and categories associated with two input terms x andy in Table 3.
To collect categories of a term, we takethe categories of its associated articles and go up Klevels in the Wikipedia category system.
In our ex-periments, we use abstracts of Wikipedia articles in-stead of whole texts.Association Information: This features repre-sents a measure of association between the termsby considering their information overlap.
We cap-ture this feature by the pointwise mutual informa-tion (pmi) which quantifies the discrepancy betweenthe probability of two terms appearing together ver-sus the probability of each term appearing indepen-dently4.
The pmi of two terms x and y is estimatedas follows:pmi(x, y) = logp(x, y)p(x)p(y)= logNf(x, y)f(x)f(y),where N is the total number of Wikipedia articles,and f(.)
is the function which counts the number ofappearances of its argument.Overlap Ratios: The overlap ratio features cap-ture the fact that the titles of a term usually overlapwith the categories of its descendants.
We measurethis overlap as the ratio of the number of commonphrases used in the titles of one term and the cate-gories of the other term.
In our context, a phrase is4pmi is different than mutual information.
The former ap-plies to specific outcomes, while the latter is to measure themutual dependence of two random variables.1103considered to be a common phrase if it appears in thetitles of one term and the categories of the other termand it is also of the following types: (1) the wholestring of a category, or (2) the head in the root formof a category, or (3) the post-modifier of a category.We use the Noun Group Parser from (Suchanek etal., 2007) to extract the head and post-modifier froma category.
For example, one of the categories of anarticle about Chicago is Cities in Illinois.
This cate-gory can be parsed into a head in its root form City,and a post-modifier Illinois.
Given term pair (City,Chicago), we observe that City matches the head ofthe category Cities in Illinois of term Chicago.
Thisis a strong indication that Chicago is a child of City.We also use a feature that captures the overlapratio of common phrases between the categories oftwo input terms.
For this feature, we do not use thepost-modifier of the categories.
We use Jaccard sim-ilarity coefficient to measure these overlaps ratios.5 Inference with Relational ConstraintsOnce we have a local multi-class classifier that mapsa given pair of terms to one of the four possible rela-tions, we use a constraint-based optimization algo-rithm to improve this prediction.
The key insightbehind the way we model the inference model isthat if we consider more than two terms, there arelogical constraints that restrict the possible relationsamong them.
For instance, George W. Bush can-not be an ancestor or sibling of president if we areconfident that president is an ancestor of Bill Clin-ton, and Bill Clinton is a sibling of George W. Bush.We call the combination of terms and their relationsa term network.
Figure 2 shows some n-term net-works consisting of two input terms (x, y), and ad-ditional terms z, w, v.The aforementioned observations show that if wecan obtain additional terms that are related to thetwo target terms, we can enforce such coherencyrelational constraints and make a global predictionthat would improve the prediction of the taxonomicrelation between the two given terms.
Our infer-ence model follows constraint-based formulationsthat were introduced in the NLP community andwere shown to be very effective in exploiting declar-ative background knowledge (Roth and Yih, 2004;Denis and Baldridge, 2007; Punyakanok et al, 2008;Chang et al, 2008).George W.BushPresidentBill ClintonxyzRed GreenBluexyz(a) (b)Honda ToyotacarmanufacturerxyzwBMWCelcius metertemperaturexyzwlength(d)(c)vphysicalquantitiesFigure 2: Examples of n-term networks with two inputterm x and y.
(a) and (c) show valid combinations ofedges, whereas (b) and (d) are two relational constraints.For simplicity, we do not draw no relation edges in (d).5.1 Enforcing Coherency through InferenceLet x, y be two input terms, and Z ={z1, z2, ..., zm} be a set of additional terms.
For asubset Z ?
Z , we construct a set of term networkswhose nodes are x, y and all elements in Z, and theedge, e, between every two nodes is one of four tax-onomic relations whose weight, w(e), is given bya local classifier (Section 4).
If l = |Z|, there aren = 2 + l nodes in each network, and 4[12n(n?1)]term networks can be constructed.
In our experi-ments we only use 3-term networks (i.e.
l = 1).For example, for the input pair (red, green) andZ = {blue, yellow}, we can construct 64 networksfor the triple ?red, green, Z = {blue}?
and 64 net-works for ?red, green, Z = {yellow}?
by trying allpossible relations between the terms.A relational constraint is defined as a term net-work consisting of only its ?illegitimate?
edge set-tings, those that belongs to a pre-defined list of in-valid edge combinations.
For example, Figure 2bshows an invalid network where red is a sibling ofboth green and blue, and green is an ancestor of blue.In Figure 2d, Celcius and meter cannot be siblingsbecause they are children of two sibling terms tem-perature and length.
The relational constraints usedin our experiments are manually constructed.Let C be a list of relational constraints.
Equation(1) defines the network scoring function, which is alinear combination of the edge weights, w(e), andthe penalties, ?k, of term networks matching con-straint Ck ?
C.score(t) =?e?tw(e)?|C|?k=1?kdCk(t) (1)function dCk(t) indicates if t matches Ck.
In ourwork, we use relational constraints as hard con-1104YAGO Query PatternsINPUT: term ?x?OUTPUT: lists of ancestors, siblings, and children of ?x?Pattern 1 Pattern 2 Pattern 3?x?
MEANS ?A ?x?
MEANS ?A ?x?
MEANS ?D?A SUBCLASSOF ?B ?A TYPE ?B ?E TYPE ?D?C SUBCLASSOF ?B ?C TYPE ?BRETURN: ?B, ?C, ?E aslists of ancestors, siblings, and children, respectively.Figure 3: Our YAGO query patterns used to obtain relatedterms for ?x?.straints and set their penalty ?k to ?.
For a set ofterm networks formed by ?x, y, Z?
and all possiblerelations between the terms, we select the best net-work, t?
= argmaxtscore(t).After picking the best term network t?
for everyZ ?
Z , we make the final decision on the taxonomicrelation between x and y.
Let r denote the relationbetween x and y in a particular t?
(e.g.
r = x?
y.
)The set of all t?
is divided into 4 groups with respectto r (e.g.
a group of all t?
having r = x ?
y, agroup of all t?
having r = x ?
y.)
We denote agroup with term networks holding r as the relationbetween x and y by Tr.
To choose the best taxo-nomic relation, r?, of x and y, we solve the objectivefunction defined in Equation 2.r?
= argmaxr1|Tr|?t??Tr?t?score(t?)
(2)where ?t is the weight of term network t, definedas the occurrence probability of t (regarding only itsedges?
setting) in the training data, which is aug-mented with additional terms.
Equation (2) finds thebest taxonomic relation of two input terms by com-puting the average score of every group of the bestterm networks representing a particular relation oftwo input terms.5.2 Extracting Related TermsIn the inference model, we need to obtain otherterms that are related to the two input terms.
Here-after, we refer to additional terms as related terms.The related term space is a space of direct ancestors,siblings and children in a particular resource.We propose an approach that uses the YAGO on-tology (Suchanek et al, 2007) to provide relatedterms.
It is worth noting that YAGO is chosen overthe Wikipedia category system used in our work be-cause YAGO is a clean ontology built by carefullycombining Wikipedia and WordNet.5In YAGO model, all objects (e.g.
cities, people,etc.)
are represented as entities.
To map our inputterms to entities in YAGO, we use the MEANS re-lation defined in the YAGO ontology.
Furthermore,similar entities are grouped into classes.
This allowsus to obtain direct ancestors of an entity by usingthe TYPE relation which gives the entity?s classes.Furthermore, we can get ancestors of a class withthe SUBCLASSOF relation6.
By using three relationsMEANS, TYPE and SUBCLASSOF in YAGO model,we can obtain Proposals for direct ancestors, sib-lings, and children, if any, for any input term.
Wethen evaluate our classifier on all pairs, and run theinference to improve the prediction using the co-herency constraints.
Figure 3 presents three patternsthat we used to query related terms from YAGO.6 Experimental StudyIn this section, we evaluate TAREC against severalsystems built upon existing well-known knowledgesources.
The resources are either hierarchical struc-tures or extracted by using distributional semanticmodels.
We also perform several experimental anal-yses to understand TAREC?s behavior in details.6.1 Comparison to Hierarchical StructuresWe create and use two main data sets in our ex-periments.
Dataset-I is generated from 40 seman-tic classes of about 11,000 instances.
The orig-inal semantic classes and instances were manu-ally constructed with a limited amount of manualpost-filtering and were used to evaluate informa-tion extraction tasks in (Pas?ca, 2007; Pas?ca andVan Durme, 2008) (we refer to this original data asOrgData-I).
This dataset contains both terms withWikipedia pages (e.g.
George W. Bush) and non-Wikipedia terms (e.g.
hindu mysticism).
Pairs ofterms are generated by randomly pairing seman-tic class names and instances.
We generate dis-joint training and test sets of 8,000 and 12,000 pairsof terms, respectively.
We call the test set of this5However, YAGO by itself is weaker than our approach inidentifying taxonomic relations (see Section 6.
)6These relations are defined in the YAGO ontology.1105dataset Test-I.
Dataset-II is generated from 44 se-mantic classes of more than 10,000 instances usedin (Vyas and Pantel, 2009)7.
The original semanticclasses and instances were extracted from Wikipedialists.
This data, therefore, only contains terms withcorresponding Wikipedia pages.
We also generatedisjoint training and test sets of 8,000 and 12,000pairs of terms, respectively, and call the test set ofthis dataset Test-II.8Several semantic class names in the original dataare written in short forms (e.g.
chemicalelem,proglanguage).
We expand these names to somemeaningful names which are used by all systems inour experiments.
For example, terroristgroup is ex-panded to terrorist group, terrorism.
Table 1 showssome pairs of terms which are generated.
Four typesof taxonomic relations are covered with balancednumbers of examples in all data sets.
To evaluate oursystems, we use a snapshot of Wikipedia from July,2008.
After cleaning and removing articles withoutcategories (except redirect pages), 5,503,763 articlesremain.
We index these articles using Lucene9.
Asa learning algorithm, we use a regularized averagedPerceptron (Freund and Schapire, 1999).We compare TAREC with three systems that webuilt using recently developed large-scale hierarchi-cal structures.
Strube07 is built on the latest ver-sion of a taxonomy, TStrube, which was derived fromWikipedia (Ponzetto and Strube, 2007).
It is worthnoting that the structure of TStrube is similar to thepage structure of Wikipedia.
For a fair comparison,we first generate a semantic representation for eachinput term by following the same procedure used inTAREC described in Section 4.
The titles and cat-egories of the articles in the representation of eachinput term are then extracted.
Only titles and theircorresponding categories that are in TStrube are con-sidered.
A term is an ancestor of the other if atleast one of its titles is in the categories of the otherterm.
If two terms share a common category, theyare considered siblings; and no relation, otherwise.The ancestor relation is checked first, then sibling,and finally no relation.
Snow06 uses the extended7There were 50 semantic classes in the original dataset.
Wegrouped some semantically similar classes for the purpose ofclassifying taxonomic relations.8Published at http://cogcomp.cs.illinois.edu/page/software9http://lucene.apache.org, version 2.3.2Test-I Test-IIStrube07 24.32 25.63Snow06 41.97 36.26Yago07 65.93 70.63TAREC (local) 81.89 84.7TAREC 85.34 86.98Table 4: Evaluating and comparing performances, in ac-curacy, of the systems on Test-I and Test-II.
TAREC (lo-cal) uses only our local classifier to identify taxonomic re-lations by choosing the relation with highest confidence.WordNet (Snow et al, 2006).
Words in the extendedWordNet can be common nouns or proper nouns.Given two input terms, we first map them onto thehierarchical structure of the extended WordNet byexact string matching.
A term is an ancestor of theother if it can be found as an hypernym after goingup K levels in the hierarchy from the other term.
Iftwo terms share a common subsumer within somelevels, then they are considered as siblings.
Oth-erwise, there is no relation between the two inputterms.
Similar to Strube07, we first check ancestor,then sibling, and finally no relation.
Yago07 usesthe YAGO ontology (Suchanek et al, 2007) as itsmain source of background knowledge.
Because theYAGO ontology is a combination of Wikipedia andWordNet, this system is expected to perform well atrecognizing taxonomic relations.
To access a term?sancestors and siblings, we use patterns 1 and 2 inFigure 3 to map a term to the ontology and move upon the ontology.
The relation identification processis then similar to those of Snow06 and Strube07.
Ifan input term is not recognized by these systems,they return no relation.Our overall algorithm, TAREC, is described inFigure 1.
We manually construct a pre-defined listof 35 relational constraints to use in the inferencemodel.
We also evaluate our local classifier (Section4), which is referred as TAREC (local).
To makeclassification decision with TAREC (local), for apair of terms, we choose the predicted relation withhighest confidence returned by the classifier.In all systems compared, we vary the value ofK10from 1 to 4.
The best result of each system is re-ported.
Table 4 shows the comparison of all sys-tems evaluated on both Test-I and Test-II.
Our sys-tems, as shown, significantly outperform the other10See Section 3.1 for the meaning of K.1106systems.
In Table 4, the improvement of TARECover TAREC (local) on Test-I shows the contribu-tion of both the normalization procedure (that is, go-ing outside Wikipedia terms) and the global infer-ence model to the classification decisions, whereasthe improvement on Test-II shows only the contribu-tion of the inference model, because Test-II containsonly terms with corresponding Wikipedia articles.Observing the results we see that our algorithmsis doing significantly better that fixed taxonomiesbased algorithms.
This is true both for TAREC (lo-cal) and for TAREC.
We believe that our machinelearning based classifier is very flexible in extract-ing features of the two input terms and thus in pre-dicting their taxonomic Relation.
On the other hand,other system rely heavily on string matching tech-niques to map input terms to their respective ontolo-gies, and these are very inflexible and brittle.
Thisclearly shows one limitation of using existing struc-tured resources to classify taxonomic relations.We do not use special tactics to handle polyse-mous terms.
However, our procedure of building se-mantic representations for input terms described inSection 4 ties the senses of the two input terms andthus, implicitly, may get some sense information.We do not use this procedure in Snow06 becauseWordNet and Wikipedia are two different knowl-edge bases.
We also do not use this procedure inYago07 because in YAGO, a term is mapped onto theontology by using the MEANS operator (in Pattern 1,Figure 3).
This cannot follow our procedure.6.2 Comparison to Harvested KnowledgeAs we discussed in Section 2, the output ofbootstrapping-based algorithms is usually limited toa small number of high-quality terms while sacri-ficing coverage (or vice versa).
For example, thefull Espresso algorithm in (Pantel and Pennacchiotti,2006) extracted 69,156 instances of is-a relationwith 36.2% precision.
Similarly, (Kozareva et al,2008) evaluated only a small number (a few hun-dreds) of harvested instances.
Recently, (Baroniand Lenci, 2010) proposed a general framework toextract properties of input terms.
Their TypeDMmodel harvested 5,000 significant properties foreach term out of 20,410 noun terms.
For exam-ple, the properties of marine include ?own, bomb?,?use, gun?.
Using vector space models we couldmeasure the similarity between terms using theirproperty vectors.
However, since the informationavailable in TypeDM does not support predicting theancestor relation between terms, we only evaluateTypeDM in classifying sibling vs. no relation.
Wedo this by giving a list of semantic classes using thefollowing procedure: (1) For each semantic class,use some seeds to compute a centroid vector fromthe seeds?
vectors in TypeDM, (2) each term in aninput pair is classified into its best semantic classbased on the cosine similarity between its vector andthe centroid vector of the category, (3) two terms aresiblings if they are classified into the same category;and have no relation, otherwise.
Out of 20,410 nounterms in TypeDM, there are only 345 terms overlap-ping with the instances in OrgData-I and belongingto 10 significant semantic classes.
For each seman-tic class, we randomly pick 5 instances as its seeds tomake a centroid vector.
The rest of the overlappinginstances are randomly paired to make a dataset of4,000 pairs of terms balanced in the number of sib-ling and no relation pairs.
On this dataset, TypeDMachieves the accuracy of 79.75%.
TAREC (local),with the local classifier trained on the training set(with 4 relation classes) of Dataset-I, gives 78.35%of accuracy.
The full TAREC system with relationalconstraints achieves 82.65%.
We also re-train andevaluate the local classifier of TAREC on the sametraining set but without ancestor relation pairs.
Thislocal classifier has an accuracy of 81.08%.These results show that although the full TARECsystem gives better performance, TypeDM is verycompetitive in recognizing sibling vs. no relation.However, TypeDM can only work in a limited set-ting where semantic classes are given in advance,which is not practical in real-world applications; andof course, TypeDM does not help to recognize an-cestor relations between two terms.6.3 Experimental AnalysisIn this section, we discuss some experimental anal-yses to better understand our systems.Precision and Recall: We want to study TARECon individual taxonomic relations using Precisionand Recall.
Table 5 shows that TAREC performsvery well on ancestor relation.
Sibling and no rela-tion are the most difficult relations to classify.
Inthe same experimental setting on Test-I, Yago071107TARECTest-I Test-IIPrec Rec Prec Recx?
y 95.82 88.01 96.46 88.48x?
y 94.61 89.29 96.15 88.86x?
y 79.23 84.01 83.15 81.87x= y 73.94 79.9 75.54 88.27Average 85.9 85.3 87.83 86.87Table 5: Performance of TAREC on individual taxo-nomic relation.Wiki WordNet non-WikiStrube07 24.59 24.13 21.18Snow06 41.23 46.91 34.46Yago07 69.95 70.42 34.26TAREC (local) 89.37 89.72 31.22TAREC 91.03 91.2 45.21Table 6: Performance of the systems on special data sets,in accuracy.
On the non-Wikipedia test set, TAREC (lo-cal) simply returns sibling relation.achieves 79.34% and 66.03% of average Precisionand Recall, respectively.
These numbers on Test-IIare 81.33% and 70.44%.Special Data Sets: We evaluate all systems thatuse hierarchical structures as background knowl-edge on three special data sets derived from Test-I.From 12,000 pairs in Test-I, we created a test set,Wiki, consisting of 10, 456 pairs with all terms inWikipedia.
We use the rest of 1, 544 pairs with atleast one non-Wikipedia term to build a non-Wikitest set.
The third dataset, WordNet, contains 8, 625pairs with all terms in WordNet and Wikipedia.
Ta-ble 6 shows the performance of the systems on thesedata sets.
Unsurprisingly, Yago07 gets better resultson Wiki than on Test-I.
Snow06, as expected, givesbetter performance on the WordNet test set.
TARECstill significantly outperforms these systems.
Theimprovement of TAREC over TAREC (local) on theWiki and WordNet test sets shows the contributionof the inference model, whereas the improvement onthe non-Wikipedia test set shows the contribution ofnormalizing input terms to Wikipedia.Contribution of Related Terms in Inference:We evaluate TAREC when the inference procedureis fed by related terms that are generated using a?gold standard?
source instead of YAGO.
To do this,we use the original data which was used to generateTest-I.
For each term in the examples of Test-I, weget its ancestors, siblings, and children, if any, fromK=1 K=2 K=3 K=4TAREC 82.93 85.34 85.23 83.95TAREC (Gold Infer.)
83.46 86.18 85.9 84.93Table 7: Evaluating TAREC with different sources pro-viding related terms to do inference.the original data and use them as related terms in theinference model.
This system is referred as TAREC(Gold Infer.).
Table 7 shows the results of the twosystems on different K as the number of levels togo up on the Wikipedia category system.
We seethat TAREC gets better results when doing inferencewith better related terms.
In this experiment, the twosystems use the same number of related terms.7 ConclusionsWe studied an important component of many com-putational linguistics tasks: given two target terms,determine that taxonomic relation between them.We have argued that static structured knowledgebases cannot support this task well enough, and pro-vided empirical support for this claim.
We have de-veloped TAREC, a novel algorithm that leverages in-formation from existing knowledge sources and usesmachine learning and a constraint-based inferencemodel to mitigate the noise and the level of uncer-tainty inherent in these resources.
Our evaluationsshow that TAREC significantly outperforms othersystems built upon existing well-known knowledgesources.
Our approach generalizes and handles non-Wikipedia term well across semantic classes.
Ourfuture work will include an evaluation of TAREC inthe context of textual inference applications.AcknowledgmentsThe authors thank Mark Sammons, Vivek Srikumar, JamesClarke and the anonymous reviewers for their insightful com-ments and suggestions.
University of Illinois at Urbana-Champaign gratefully acknowledges the support of DefenseAdvanced Research Projects Agency (DARPA) Machine Read-ing Program under Air Force Research Laboratory (AFRL)prime contract No.
FA8750-09-C-0181.
The first author alsothanks the Vietnam Education Foundation (VEF) for its spon-sorship.
Any opinions, findings, and conclusion or recommen-dations expressed in this material are those of the authors anddo not necessarily reflect the view of the VEF, DARPA, AFRL,or the US government.1108ReferencesA.
Abad, L. Bentivogli, I. Dagan, D. Giampiccolo,S.
Mirkin, E. Pianta, and A. Stern.
2010.
A resourcefor investigating the impact of anaphora and corefer-ence on inference.
In LREC.M.
Banko and O. Etzioni.
2008.
The tradeoffs betweenopen and traditional relation extraction.
In ACL-HLT.M.
Baroni and A. Lenci.
2010.
Distributional mem-ory: A general framework for corpus-based semantics.Computational Linguistics, 36.S.
Chakrabarti, B. Dom, R. Agrawal, and P. Raghavan.1997.
Using taxonomy, discriminants, and signaturesfor navigating in text databases.
In VLDB.M.
Chang, L. Ratinov, and D. Roth.
2008.
Constraints asprior knowledge.
In ICML Workshop on Prior Knowl-edge for Text and Language Processing.D.
Davidov and A. Rappoport.
2008.
Unsupervised dis-covery of generic relationships using pattern clustersand its evaluation by automatically generated sat anal-ogy questions.
In ACL.P.
Denis and J. Baldridge.
2007.
Joint determination ofanaphoricity and coreference resolution using integerprogramming.
In NAACL.C.
Fellbaum.
1998.
WordNet: An Electronic LexicalDatabase.
MIT Press.Y.
Freund and R. E. Schapire.
1999.
Large margin clas-sification using the perceptron algorithm.
MachineLearning.M.
A. Hearst.
1992.
Acquisition of hyponyms from largetext corpora.
In COLING.A.
Hotho, S. Staab, and G. Stumme.
2003.
Ontologiesimprove text document clustering.
In ICDM.Z.
Kozareva, E. Riloff, and E. Hovy.
2008.
Seman-tic class learning from the web with hyponym patternlinkage graphs.
In ACL-HLT.B.
MacCartney and C. D. Manning.
2008.
Modeling se-mantic containment and exclusion in natural languageinference.
In COLING.B.
MacCartney and C. D. Manning.
2009.
An extendedmodel of natural logic.
In IWCS-8.M.
Pas?ca and B.
Van Durme.
2008.
Weakly-supervisedacquisition of open-domain classes and class attributesfrom web documents and query logs.
In ACL-HLT.M.
Pas?ca.
2007.
Organizing and searching the worldwide web of facts step two: Harnessing the wisdomof the crowds.
In WWW.P.
Pantel and M. Pennacchiotti.
2006.
Espresso: Lever-aging generic patterns for automatically harvesting se-mantic relations.
In ACL, pages 113?120.S.
P. Ponzetto and M. Strube.
2007.
Deriving a largescale taxonomy from wikipedia.
AAAI.V.
Punyakanok, D. Roth, and W. Yih.
2008.
The impor-tance of syntactic parsing and inference in semanticrole labeling.
Computational Linguistics, 34(2).D.
Roth and W. Yih.
2004.
A linear programming formu-lation for global inference in natural language tasks.
InCoNLL.D.
Roth and W. Yih.
2007.
Global inference for en-tity and relation identification via a linear program-ming formulation.
In Lise Getoor and Ben Taskar, ed-itors, Introduction to Statistical Relational Learning.MIT Press.M.
Sammons, V.G.
Vydiswaran, and D. Roth.
2010.
Asknot what textual entailment can do for you...
In ACL.L.
Sarmento, V. Jijkuon, M. de Rijke, and E. Oliveira.2007.
?more like these?
: growing entity classes fromseeds.
In CIKM.A.
K. Saxena, G. V. Sambhu, S. Kaushik, and L. V. Sub-ramaniam.
2007.
Iitd-ibmirl system for question an-swering using pattern matching, semantic type and se-mantic category recognition.
In TREC.R.
Snow, D. Jurafsky, and A. Y. Ng.
2005.
Learningsyntactic patterns for automatic hypernym discovery.In NIPS.R.
Snow, D. Jurafsky, and A. Y. Ng.
2006.
Semantictaxonomy induction from heterogenous evidence.
InACL.F.
M. Suchanek, G. Kasneci, and G. Weikum.
2007.Yago: A Core of Semantic Knowledge.
In WWW.O.
Vikas, A. K. Meshram, G. Meena, and A. Gupta.2008.
Multiple document summarization using princi-pal component analysis incorporating semantic vectorspace model.
In Computational Linguistics and Chi-nese Language Processing.V.
Vyas and P. Pantel.
2009.
Semi-automatic entity setrefinement.
In NAACL-HLT.D.
Yarowsky.
1995.
Unsupervised woed sense disam-biguation rivaling supervied methods.
In Proceedingsof ACL-95.1109
