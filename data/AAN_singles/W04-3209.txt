Comparing and Combining Generative and Posterior Probability Models:Some Advances in Sentence Boundary Detection in SpeechYang LiuICSI and Purdue Universityyangl@icsi.berkeley.eduAndreas Stolcke Elizabeth ShribergSRI and ICSIstolcke,ees@speech.sri.comMary HarperPurdue Universityharper@ecn.purdue.eduAbstractWe compare and contrast two different models fordetecting sentence-like units in continuous speech.The first approach uses hidden Markov sequencemodels based on N-grams and maximum likeli-hood estimation, and employs model interpolationto combine different representations of the data.The second approach models the posterior proba-bilities of the target classes; it is discriminative andintegrates multiple knowledge sources in the max-imum entropy (maxent) framework.
Both modelscombine lexical, syntactic, and prosodic informa-tion.
We develop a technique for integrating pre-trained probability models into the maxent frame-work, and show that this approach can improveon an HMM-based state-of-the-art system for thesentence-boundary detection task.
An even moresubstantial improvement is obtained by combiningthe posterior probabilities of the two systems.1 IntroductionSentence boundary detection is a problem that hasreceived limited attention in the text-based com-putational linguistics community (Schmid, 2000;Palmer and Hearst, 1994; Reynar and Ratnaparkhi,1997), but which has recently acquired renewed im-portance through an effort by the DARPA EARSprogram (DARPA Information Processing Technol-ogy Office, 2003) to improve automatic speech tran-scription technology.
Since standard speech recog-nizers output an unstructured stream of words, im-proving transcription means not only that word ac-curacy must be improved, but also that commonlyused structural features such as sentence boundariesneed to be recognized.
The task is thus fundamen-tally based on both acoustic and textual (via auto-matic word recognition) information.
From a com-putational linguistics point of view, sentence unitsare crucial and assumed in most of the further pro-cessing steps that one would want to apply to suchoutput: tagging and parsing, information extraction,and summarization, among others.Sentence segmentation from speech is a difficultproblem.
The best systems benchmarked in a re-cent government-administered evaluation yield er-ror rates between 30% and 50%, depending on thegenre of speech processed (measured as the num-ber of missed and inserted sentence boundaries asa percentage of true sentence boundaries).
Becauseof the difficulty of the task, which leaves plenty ofroom for improvement, its relevance to real-worldapplications, and the range of potential knowledgesources to be modeled (acoustics and text-based,lower- and higher-level), this is an interesting chal-lenge problem for statistical and computational ap-proaches.All of the systems participating in the recentDARPA RT-03F Metadata Extraction evaluation(National Institute of Standards and Technology,2003) were based on a hidden Markov model frame-work, in which word/tag sequences are modeled byN-gram language models (LMs).
Additional fea-tures (mostly reflecting speech prosody) are mod-eled as observation likelihoods attached to the N-gram states of the HMM (Shriberg et al, 2000).
TheHMM is a generative modeling approach, since itdescribes a stochastic process with hidden variables(the locations of sentence boundaries) that producesthe observable data.
The segmentation is inferredby comparing the likelihoods of different boundaryhypotheses.While the HMM approach is computationally ef-ficient and (as described later) provides a convenientway for modularizing the knowledge sources, it hastwo main drawbacks: First, the standard trainingmethods for HMMs maximize the joint probabilityof observed and hidden events, as opposed to theposterior probability of the correct hidden variableassignment given the observations.
The latter is acriterion more closely related to classification error.Second, the N-gram LM underlying the HMM tran-sition model makes it difficult to use features thatare highly correlated (such as word and POS labels)without greatly increasing the number of model pa-rameters; this in turn would make robust estimationchannelword stringprosodyideasyntax, semantics,word selection,puntuationimpose prosodysignalprosodic featureextractionprosodicfeaturestextual featurespeechrecognizerword stringword, POS,classesfusion ofknowledgesoucessentence boundaryhypothesisFigure 1: Diagram of the sentence segmentation task.difficult.In this paper, we describe our effort to overcomethese shortcomings by 1) replacing the generativemodel with one that estimates the posterior proba-bilities directly, and 2) using the maximum entropy(maxent) framework to estimate conditional distri-butions, giving us a more principled way to com-bine a large number of overlapping features.
Bothtechniques have been used previously for traditionalNLP tasks, but they are not straightforward to ap-ply in our case because of the diverse nature of theknowledge sources used in sentence segmentation.We describe the techniques we developed to workaround these difficulties, and compare classificationaccuracy of the old and new approach on differentgenres of speech.
We also investigate how wordrecognition error affects that comparison.
Finally,we show that a simple combination of the two ap-proaches turns out to be highly effective in improv-ing the best previous results obtained on a bench-mark task.2 The Sentence Segmentation TaskThe sentence boundary detection problem is de-picted in Figure 1 in the source-channel framework.The speaker intends to say something, chooses theword string, and imposes prosodic cues (duration,emphasis, intonation, etc).
This signal goes throughthe speech production channel to generate an acous-tic signal.
A speech recognizer determines the mostlikely word string given this signal.
To detect pos-sible sentence boundaries in the recognized wordstring, prosodic features are extracted from the sig-nal, and combined with textual cues obtained fromthe word string.
At issue in this paper is the finalbox in the diagram: how to model and combine theavailable knowledge sources to find the most accu-rate hypotheses.Note that this problem differs from the sen-tence boundary detection problem for written text inthe natural language processing literature (Schmid,2000; Palmer and Hearst, 1994; Reynar and Rat-naparkhi, 1997).
Here we are dealing with spo-ken language, therefore there is no punctuation in-formation, the words are not capitalized, and thetranscripts from the recognition output are errorful.This lack of textual cues is partly compensated byprosodic information (timing, pitch, and energy pat-terns) conveyed by speech.
Also note that in spon-taneous conversational speech ?sentence?
is not al-ways a straightforward notion.
For our purposes weuse the definition of a ?sentence-like unit?, or SU,as defined by the LDC for labeling and evaluationpurposes (Strassel, 2003).The training data has SU boundaries marked byannotators, based on both the recorded speech andits transcription.
In testing, a system has to recoverboth the words and the locations of sentence bound-aries, denoted by (W;E) = w1e1w2: : : wiei: : : wnwhere W represents the strings of word tokens andE the inter-word boundary events (sentence bound-ary or no boundary).The system output is scored by first finding a min-imum edit distance alignment between the hypothe-sized word string and the reference, and then com-paring the aligned event labels.
The SU error rate isdefined as the total number of deleted or inserted SUboundary events, divided by the number of true SUboundaries.1 For diagnostic purposes a secondaryevaluation condition allows use of the correct wordtranscripts.
This condition allows us to study thesegmentation task without the confounding effect ofspeech recognition errors, using perfect lexical in-formation.3 Features and Knowledge SourcesWords and sentence boundaries are mutually con-strained via syntactic structure.
Therefore, the wordidentities themselves (from automatic recognitionor human transcripts) constitute a primary knowl-edge source for the sentence segmentation task.
Wealso make use of various automatic taggers that mapthe word sequence to other representations.
TheTnT tagger (Brants, 2000) is used to obtain part-of-speech (POS) tags.
A TBL chunker trained on WallStreet Journal corpus (Ngai and Florian, 2001) mapseach word to an associated chunk tag, encodingchunk type and relative word position (beginning ofan NP, inside a VP, etc.).
The tagged versions ofthe word stream are provided to allow generaliza-tions based on syntactic structure and to smooth outpossibly undertrained word-based probability esti-1This is the same as simple per-event classification accu-racy, except that the denominator counts only the ?marked?events, thereby yielding error rates that are much higher thanif one uses all potential boundary locations.mates.
For the same reasons we also generate wordclass labels that are automatically induced from bi-gram word distributions (Brown et al, 1992).To model the prosodic structure of sentenceboundaries, we extract several hundred featuresaround each word boundary.
These are based on theacoustic alignments produced by a speech recog-nizer (or forced alignments of the true words whengiven).
The features capture duration, pitch, andenergy patterns associated with the word bound-aries.
Informative features include the pause du-ration at the boundary, the difference in pitch be-fore and after the boundary, and so on.
A cru-cial aspect of many of these features is that theyare highly correlated (e.g., by being derived fromthe same raw measurements via different normaliza-tions), real-valued (not discrete), and possibly unde-fined (e.g., unvoiced speech regions have no pitch).These properties make prosodic features difficult tomodel directly in either of the approaches we are ex-amining in the paper.
Hence, we have resorted to amodular approach: the information from prosodicfeatures is modeled separately by a decision treeclassifier that outputs posterior probability estimatesP (eijfi), where eiis the boundary event after wi,and fiis the prosodic feature vector associated withthe word boundary.
Conveniently, this approachalso permits us to include some non-prosodic fea-tures that are highly relevant for the task, but nototherwise represented, such as whether a speaker(turn) change occurred at the location in question.2A practical issue that greatly influences model de-sign is that not all information sources are avail-able uniformly for all training data.
For example,prosodic modeling assumes acoustic data; whereas,word-based models can be trained on text-only data,which is usually available in much larger quantities.This poses a problem for approaches that model allrelevant information jointly and is another strongmotivation for modular approaches.4 The Models4.1 Hidden Markov Model for SegmentationOur baseline model, and the one that forms the ba-sis of much of the prior work on acoustic sentencesegmentation (Shriberg et al, 2000; Gotoh and Re-nals, 2000; Christensen, 2001; Kim and Woodland,2001), is a hidden Markov model.
The states ofthe model correspond to words wiand following2Here we are glossing over some details on prosodic mod-eling that are orthogonal to the discussion in this paper.
Forexample, instead of simple decision trees we actually use en-semble bagging to reduce the variance of the classifier (Liu etal., 2004).WiEiFiOiWi+1Ei+1Oi+1WiFi+1Wi+1Figure 2: The graphical model for the SU detectionproblem.
Only one word+event is depicted in each state,but in a model based on N-grams the previous N   1tokens would condition the transition to the next state.event labels ei.
The observations associated withthe states are the words, as well as other (mainlyprosodic) features fi.
Figure 2 shows a graphi-cal model representation of the variables involved.Note that the words appear in both the states and theobservations, such that the word stream constrainsthe possible hidden states to matching words; theambiguity in the task stems entirely from the choiceof events.4.1.1 ClassificationStandard algorithms are available to extract the mostprobable state (and thus event) sequence given a setof observations.
The error metric is based on clas-sification of individual word boundaries.
Therefore,rather than finding the highest probability sequenceof events, we identify the events with highest poste-rior individually at each boundary i:e^i= arg maxeiP (eijW;F ) (1)where W and F are the words and features forthe entire test sequence, respectively.
The individ-ual event posteriors are obtained by applying theforward-backward algorithm for HMMs (Rabinerand Juang, 1986).4.1.2 Model EstimationTraining of the HMM is supervised since event-labeled data is available.
There are two sets of pa-rameters to estimate.
The state transition proba-bilities are estimated using a hidden event N-gramLM (Stolcke and Shriberg, 1996).
The LM isobtained with standard N-gram estimation meth-ods from data that contains the word+event tags insequence: w1; e1; w2; : : : en 1; wn.
The resultingLM can then compute the required HMM transitionprobabilities as3P (wieijw1e1: : : wi 1ei 1) =P (wijw1e1: : : wi 1ei 1) P (eijw1e1: : : wi 1ei 1wi)The N-gram estimator maximizes the jointword+event sequence likelihood P (W;E) on thetraining data (modulo smoothing), and does notguarantee that the correct event posteriors neededfor classification according to Equation (1) aremaximized.The second set of HMM parameters are the ob-servation likelihoods P (fijei; wi).
Instead of train-ing a likelihood model we make use of the prosodicclassifiers described in Section 3.
We have at ourdisposal decision trees that estimate P (eijfi).
Ifwe further assume that prosodic features are inde-pendent of words given the event type (a reasonablesimplification if features are chosen appropriately),observation likelihoods may be obtained byP (fijwi; ei) =P (eijfi)P (ei)P (fi) (2)Since P (fi) is constant we can ignore it when car-rying out the maximization (1).4.1.3 Knowledge CombinationThe HMM structure makes strong independence as-sumptions: (1) that features depend only on the cur-rent state (and in practice, as we saw, only on theevent label) and (2) that each word+event label de-pends only on the last N   1 tokens.
In return, weget a computationally efficient structure that allowsinformation from the entire sequence W;F to in-form the posterior probabilities needed for classifi-cation, via the forward-backward algorithm.More problematic in practice is the integrationof multiple word-level features, such as POS tagsand chunker output.
Theoretically, all tags couldsimply be included in the hidden state representa-tion to allow joint modeling of words, tags, andevents.
However, this would drastically increase thesize of the state space, making robust model estima-tion with standard N-gram techniques difficult.
Amethod that works well in practice is linear inter-polation, whereby the conditional probability esti-mates of various models are simply averaged, thusreducing variance.
In our case, we obtain good re-sults by interpolating a word-N-gram model with3To utilize word+event contexts of length greater than onewe have to employ HMMs of order 2 or greater, or equivalently,make the entire word+event N-gram be the state.one based on automatically induced word classes(Brown et al, 1992).Similarly, we can interpolate LMs trained fromdifferent corpora.
This is usually more effectivethan pooling the training data because it allows con-trol over the contributions of the different sources.For example, we have a small corpus of training datalabeled precisely to the LDC?s SU specifications,but a much larger (130M word) corpus of standardbroadcast new transcripts with punctuation, fromwhich an approximate version of SUs could be in-ferred.
The larger corpus should get a larger weighton account of its size, but a lower weight given themismatch of the SU labels.
By tuning the interpola-tion weight of the two LMs empirically (using held-out data) the right compromise was found.4.2 Maxent Posterior Probability ModelAs observed, HMM training does not maximize theposterior probabilities of the correct labels.
Thismismatch between training and use of the modelas a classifier would not arise if the model directlyestimated the posterior boundary label probabilitiesP (eijW;F ).
A second problem with HMMs is thatthe underlying N-gram sequence model does notcope well with multiple representations (features) ofthe word sequence (words, POS, etc.)
short of build-ing a joint model of all variables.
This type of sit-uation is well-suited to a maximum entropy formu-lation (Berger et al, 1996), which allows condition-ing features to apply simultaneously, and thereforegives greater freedom in choosing representations.Another desirable characteristic of maxent modelsis that they do not split the data recursively to condi-tion their probability estimates, which makes themmore robust than decision trees when training datais limited.4.2.1 Model Formulation and TrainingWe built a posterior probability model for sentenceboundary classification in the maxent framework.Such a model takes the familiar exponential form4P (ejW;F ) =1Z(W;F )ePkkgk(e;W;F ) (3)where Z(W;F ) is the normalization term:Z(W;F ) =Xe0ePkkgk(e0;W;F ) (4)The functions gk(e;W;F ) are indicator functionscorresponding to (complex) features defined over4We omit the index i from e here since the ?current?
eventis meant in all cases.events, words, and prosodic features.
For example,one such feature function might be:g(e;W;F ) =1 : if wi= uhhuh and e = SU0 : otherwiseThe maxent model is estimated by finding the pa-rameters ksuch that the expected values of the var-ious feature functions EP[gk(e0;W; F )] match theempirical averages in the training data.
It can beshown that the resulting model has maximal entropyamong all the distributions satisfying these expec-tation constraints.
At the same time, the parame-ters so chosen maximize the conditional likelihoodQiP (eijW;F ) over the training data, subject to theconstraints of the exponential form given by Equa-tion (3).5 The conditional likelihood is closely re-lated to the individual event posteriors used for clas-sification, meaning that this type of model explicitlyoptimizes discrimination of correct from incorrectlabels.4.2.2 Choice of FeaturesEven though the mathematical formulation gives usthe freedom to use features that are overlapping orotherwise dependent, we still have to choose a sub-set that is informative and parsimonious, so as togive good generalization and robust parameter es-timates.
Various feature selection algorithms formaxent models have been proposed, e.g., (Berger etal., 1996).
However, since computational efficiencywas not an issue in our experiments, we included allfeatures that corresponded to information availableto our baseline approach, as listed below.
We dideliminate features that were triggered only once inthe training set to improve robustness and to avoidoverconstraining the model. Word N-grams.
We use combinationsof preceding and following words to en-code the word context of the event, e.g.,<wi>, <wi+1>, <wi; wi+1>, <wi 1; wi>,<wi 2; wi 1; wi>, and <wi; wi+1; wi+2>,where wirefers to the word before the bound-ary of interest. POS N-grams.
POS tags are the same as usedfor the HMM approach.
The features capturingPOS context are similar to those based on wordtokens. Chunker tags.
These are used similarly to POSand word features, except we use tags encoding5In our experiments we used the L-BFGS parameter estima-tion method, with Gaussian-prior smoothing (Chen and Rosen-feld, 1999) to avoid overfitting.chunk type (NP, VP, etc.)
and word positionwithin the chunk (beginning versus inside).6 Word classes.
These are similar to N-gram pat-terns but over automatically induced classes. Turn flags.
Since speaker change often marksan SU boundary, we use this binary feature.Note that in the HMM approach this featurehad to be grouped with the prosodic featuresand handled by the decision tree.
In the max-ent approach we can use it separately. Prosody.
As we described earlier, decision treeclassifiers are used to generate the posteriorprobabilities p(eijfi).
Since the maxent classi-fier is most conveniently used with binary fea-tures, we encode the prosodic posteriors intoseveral binary features via thresholding.
Equa-tion (3) shows that the presence of each fea-ture in a maxent model has a monotonic effecton the final probability (raising or lowering itby a constant factor ekgk).
This suggests en-coding the decision tree posteriors in a cumu-lative fashion through a series of binary fea-tures, for example, p > 0:1, p > 0:3, p > 0:5,p > 0:7, p > 0:9.
This representation is alsomore robust to mismatch between the posteriorprobability in training and test set, since smallchanges in the posterior value affect at mostone feature.Note that the maxent framework does allow theuse of real-valued feature functions, but pre-liminary experiments have shown no gain com-pared to the binary features constructed as de-scribed above.
Still, this is a topic for futureresearch. Auxiliary LM.
As mentioned earlier, additionaltext-only language model training data is of-ten available.
In the HMM model we incor-porated auxiliary LMs by interpolation, whichis not possible here since there is no LM perse, but rather N-gram features.
However, wecan use the same trick as we used for prosodicfeatures.
A word-only HMM is used to esti-mate posterior event probabilities according tothe auxiliary LM, and these posteriors are thenthresholded to yield binary features. Combined features.
To date we have not fullyinvestigated compound features that combinedifferent knowledge sources and are able tomodel the interaction between them explicitly.6Chunker features were only used for broadcast newsdata, due to the poor chunking performance on conversationalspeech.We only included a limited set of such features,for example, a combination of the decision treehypothesis and POS contexts.4.3 Differences Between HMM and MaxentWe have already discussed the differences betweenthe two approaches regarding the training objectivefunction (joint likelihood versus conditional likeli-hood) and with respect to the handling of depen-dent word features (model interpolation versus in-tegrated modeling via maxent).
On both counts themaxent classifier should be superior to the HMM.However, the maxent approach also has some the-oretical disadvantages compared to the HMM bydesign.
One obvious shortcoming is that some in-formation gets lost in the thresholding that convertsposterior probabilities from the prosodic model andthe auxiliary LM into binary features.A more qualitative limitation of the maxentmodel is that it only uses local evidence (the sur-rounding word context and the local prosodic fea-tures).
In that respect, the maxent model resem-bles the conditional probability model at the in-dividual HMM states.
The HMM as a whole,however, through the forward-backward procedure,propagates evidence from all parts of the observa-tion sequence to any given decision point.
Variantssuch as the conditional Markov model (CMM) com-bine sequence modeling with posterior probability(e.g., maxent) modeling, but it has been shown thatCMM?s are still structurally inferior to HMMs be-cause they only propagate evidence forward in time,not backwards (Klein and Manning, 2002).5 Results and Discussion5.1 Experimental SetupExperiments comparing the two modeling ap-proaches were conducted on two corpora: broad-cast news (BN) and conversational telephone speech(CTS).
BN and CTS differ in genre and speakingstyle.
These differences are reflected in the fre-quency of SU boundaries: about 14% of inter-wordboundaries are SUs in CTS, compared to roughly8% in BN.The corpora are annotated by LDC according tothe guidelines of (Strassel, 2003).
Training and testdata are those used in the DARPA Rich Transcrip-tion Fall 2003 evaluation.7 For CTS, there is about40 hours of conversational data from the Switch-board corpus for training and 6 hours (72 conversa-tions) for testing.
The BN data has about 20 hours7We used both the development set and the evaluation setas the test set in this paper, in order to have a larger test set tomake the results more meaningful.HMM Maxent CombinedBN REF 48.72 48.61 46.79STT 55.37 56.51 54.35CTS REF 31.51 30.66 29.30STT 42.97 43.02 41.88Table 1: SU detection results (error rate in %) usingmaxent and HMM individually and in combination onBN and CTS.of broadcast news shows in the training set and 3hours (6 shows) in the test set.
The SU detectiontask is evaluated on both the reference transcriptions(REF) and speech recognition outputs (STT).
Thespeech recognition output is obtained from the SRIrecognizer (Stolcke et al, 2003).System performance is evaluated using the offi-cial NIST evaluation tools,8 which implement themetric described earlier.
In our experiments, wecompare how the two approaches perform individ-ually and in combination.
The combined classifieris obtained by simply averaging the posterior esti-mates from the two models, and then picking theevent type with the highest probability at each posi-tion.We also investigate other experimental factors,such as the impact of the speech recognition errors,the impact of genre, and the contribution of text ver-sus prosodic information in each model.5.2 Experimental ResultsTable 1 shows SU detection results for BN andCTS, using both reference transcriptions and speechrecognition output, using the HMM and the max-ent approach individually and in combination.
Themaxent approach slightly outperforms the HMM ap-proach when evaluating on the reference transcripts,and the combination of the two approaches achievesthe best performance for all tasks (significant atp < 0:05 using the sign test on the reference tran-scription condition, mixed results on using recogni-tion output).5.2.1 BN vs. CTSThe detection error rate on CTS is lower than onBN.
This may be due to the metric used for per-formance.
Detection error rate is measured as thepercentage of errors per reference SU.
The numberof SUs in CTS is much larger than for BN, makingthe relative error rate lower for the conversationalspeech task.
Notice also from Table 1 that maxentyields more gain on CTS than on BN (for the refer-ence transcription condition on both corpora).
Onepossible reason for this is that we have more train-8http://www.nist.gov/speech/tests/rt/rt2003/fall/Del Ins TotalBN HMM 28.48 20.24 48.72Maxent 32.06 16.54 48.61CTS HMM 17.19 14.32 31.51Maxent 19.97 10.69 30.66Table 2: Error rates for the two approaches on referencetranscriptions.
Performance is shown in deletion, inser-tion, and total error rate (%).BN CTSHMM Textual 67.48 38.92Textual + prosody 48.72 31.51Maxent Textual 63.56 36.32Textual + prosody 48.61 30.66Table 3: SU detection error rate (%) using differentknowledge sources, for BN and CTS, evaluated on thereference transcription.ing data and thus less of a sparse data problem forCTS.5.2.2 Error Type AnalysisTable 2 shows error rates for the HMM and the max-ent approaches in the reference condition.
Due tothe reduced dependence on the prosody model, theerrors made in the maxent approach are differentfrom the HMM approach.
There are more deletionerrors and fewer insertion errors, since the prosodymodel tends to overgenerate SU hypotheses.
Thedifferent error patterns suggest that we can effec-tively combine the system output from the two ap-proaches.
As shown in the Table 1, the combinationof maxent and HMM consistently yields the bestperformance.5.2.3 Contribution of Knowledge SourcesTable 3 shows SU detection results for the two ap-proaches, using textual information only, as well asin combination with the prosody model (which arethe same results as shown in Table 1).
We only re-port the results on the reference transcription con-dition, in order to not confound the comparison byword recognition errors.The superior results for text-only classificationare consistent with the maxent model?s ability tocombine overlapping word-level features in a prin-cipled way.
However, the HMM largely catchesup once prosodic information is added.
This canbe attributed to the loss-less integration of prosodicposteriors in the HMM, as well as the fact that inthe HMM, each boundary decision is affected byprosodic information throughout the data; whereas,the maxent model only uses the prosodic features atthe boundary to be classified.5.2.4 Effect of Recognition ErrorsWe observe in Table 1 that there is a large increase inerror rate when evaluating on the speech recognitionoutput.
This happens in part because word informa-tion is inaccurate in the recognition output, thus im-pacting the LMs and lexical features.
The prosodymodel is also affected, since the alignment of incor-rect words to the speech is imperfect, thereby affect-ing the prosodic feature extraction.
However, theprosody model is more robust to recognition errorsthan the LMs, due to its lesser dependence on wordidentity.
The degradation on CTS is larger than onBN.
This can easily be explained by the differencein word error rates, 22.9% on CTS and 12.1% onBN.The maxent system degrades more than thenHMM system when errorful recognition output isused.
In light of the previous section, this makessense: most of the improvement of the maxentmodel comes from better lexical feature modeling.But these are exactly the features that are most de-teriorated by faulty recognition output.6 Conclusions and Future WorkWe have described two different approaches formodeling and integration of diverse knowledgesources for automatic sentence segmentation fromspeech: a state-of-the-art approach based onHMMs, and an alternative approach based on pos-terior probability estimation via maximum entropy.To achieve competitive performance with the max-ent model we devised a cumulative binary codingscheme to map posterior estimates from auxiliarysubmodels into features for the maxent model.The two approaches have complementarystrengths and weaknesses that were reflected in theresults, consistent with the findings for text-basedNLP tasks (Klein and Manning, 2002).
The maxentmodel showed much better accuracy than the HMMwith lexical information, and a smaller win aftercombination with prosodic features.
The HMMmade more effective use of prosodic informationand degraded less with errorful word recognition.A interpolation of posterior probabilities from thetwo systems achieved 2-7% relative error reductioncompared to the baseline (significant at p < 0:05for the reference transcription condition).
Theresults were consistent for two different genres ofspeech.In future work we hope to determine how the in-dividual qualitative differences of the two models(estimation methods, model structure, etc.)
con-tribute to the observed differences in results.
Toimprove results overall, we plan to explore featuresthat combine multiple knowledge sources, as wellas approaches that model recognition uncertainty inorder to mitigate the effects of word errors.
We alsoplan to investigate using a conditional random field(CRF) model.
CRFs combine the advantages ofboth the HMM and the maxent approaches, beinga discriminatively trained model that can incorpo-rate overlapping features (the maxent advantages),while also modeling sequence dependencies (an ad-vantage of HMMs) (Lafferty et al, 2001).7 AcknowledgmentsThe authors gratefully thank Le Zhang for his guid-ance in applying the maximum entropy approachto this task.
This research has been supportedby DARPA under contract MDA972-02-C-0038,NSF-STIMULATE under IRI-9619921, NSF BCS-9980054, and NASA under NCC 2-1256.
Distri-bution is unlimited.
Any opinions expressed in thispaper are those of the authors and do not necessarilyreflect the views of DARPA, NSF, or NASA.
Part ofthis work was carried out while the last author wason leave from Purdue University and at NSF.ReferencesA.
L. Berger, S. A. Della Pietra, and V. J. DellaPietra.
1996.
A maximum entropy approach tonatural language processing.
Computational Lin-guistics, 22:39?72.T.
Brants.
2000.
TnT?a statistical part-of-speechtagger.
In Proc.
of the Sixth Applied NLP, pages224?231.P.
F. Brown, V. J. Della Pietra, P. V. DeSouza, J. C.Lai, and R. L. Mercer.
1992.
Class-based n-grammodels of natural language.
Computational Lin-guistics, 18:467?479.S.
Chen and R. Rosenfeld.
1999.
A Gaussian priorfor smoothing maximum entropy models.
Tech-nical report, Carnegie Mellon University.H.
Christensen.
2001.
Punctuation annotation us-ing statistical prosody models.
In ISCA Work-shop on Prosody in Speech Recognition and Un-derstanding.DARPA Information Processing Technol-ogy Office.
2003.
Effective, afford-able, reusable speech-to-text (EARS).http://www.darpa.mil/ipto/programs/ears/.Y.
Gotoh and S. Renals.
2000.
Sentence bound-ary detection in broadcast speech transcripts.
InISCA Workshop: Automatic Speech Recognition:Challenges for the new Millennium ASR-2000,pages 228?235.J.
Kim and P. C. Woodland.
2001.
The use ofprosody in a combined system for punctuationgeneration and speech recognition.
In Proc.
ofEurospeech 2001, pages 2757?2760.D.
Klein and C. Manning.
2002.
Conditional struc-ture versus conditional estimation in NLP mod-els.
In Proc.
of EMNLP 2002, pages 9?16.J.
Lafferty, A. McCallum, and F. Pereira.
2001.Conditional random field: Probabilistic modelsfor segmenting and labeling sequence data.
InProf.
of ICML 2001, pages 282?289.Y.
Liu, E. Shriberg, A. Stolcke, and M. Harper.2004.
Using machine learning to cope with im-balanced classes in natural speech: Evidencefrom sentence boundary and disfluency detection.In Proc.
of ICSLP 2004 (To Appear).National Institute of Standards and Technol-ogy.
2003.
RT-03F workshop agenda andpresentations.
http://www.nist.gov/speech/tests/rt/rt2003/fall/presentations/, November.G.
Ngai and R. Florian.
2001.
Transformation-based learning in the fast lane.
In Proc.
of NAACL2001, pages 40?47, June.D.
D. Palmer and M. A. Hearst.
1994.
Adaptivesentence boundary disambiguation.
In Proc.
ofthe Fourth Applied NLP, pages 78?83.L.
R. Rabiner and B. H. Juang.
1986.
An introduc-tion to hidden Markov models.
IEEE ASSP Mag-azine, 3(1):4?16, January.J.
Reynar and A. Ratnaparkhi.
1997.
A maximumentropy approach to identifying sentence bound-aries.
In Proc.
of the Fifth Applied NLP, pages16?19.H.
Schmid.
2000.
Unsupervised learning of pe-riod disambiguation for tokenisation.
Universityof Stuttgart, Internal Report.E.
Shriberg, A. Stolcke, D. H. Tur, and G. Tur.2000.
Prosody-based automatic segmentation ofspeech into sentences and topics.
Speech Com-munication, 32(1-2):127?154.A.
Stolcke and E. Shriberg.
1996.
Automatic lin-guistic segmentation of conversational speech.
InProc.
of ICSLP 1996, pages 1005?1008.A.
Stolcke, H. Franco, and R. Gadde et al2003.
Speech-to-text research at SRI-ICSI-UW.http://www.nist.gov/speech/tests/rt/rt2003/spring/presentations/index.htm.S.
Strassel, 2003.
Simple Metadata AnnotationSpecification V5.0.
Linguistic Data Consortium.
