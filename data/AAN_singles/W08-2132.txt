CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 228?232Manchester, August 2008A Pipeline Approach for Syntactic and Semantic Dependency ParsingYotaro Watanabe and Masakazu Iwatate and Masayuki Asahara and Yuji MatsumotoNara Institute of Science and Technology, Japan8916-5, Takayama, Ikoma, Nara, Japan, 630-0192{yotaro-w, masakazu-i, masayu-a, matsu}@is.naist.jpAbstractThis paper describes our system for syn-tactic and semantic dependency parsingto participate the shared task of CoNLL-2008.
We use a pipeline approach, inwhich syntactic dependency parsing, wordsense disambiguation, and semantic rolelabeling are performed separately: Syn-tactic dependency parsing is performedby a tournament model with a supportvector machine; word sense disambigua-tion is performed by a nearest neighbourmethod in a compressed feature space byprobabilistic latent semantic indexing; andsemantic role labeling is performed bya an online passive-aggressive algorithm.The submitted result was 79.10 macro-average F1 for the joint task, 87.18% syn-tactic dependencies LAS, and 70.84 se-mantic dependencies F1.
After the dead-line, we constructed the other configura-tion, which achieved 80.89 F1 for the jointtask, and 74.53 semantic dependencies F1.The result shows that the configuration ofpipeline is a crucial issue in the task.1 IntroductionThis paper presents the description of our systemin CoNLL-2008 shared task.
We split the sharedtask into five sub-problems ?
syntactic dependencyparsing, syntactic dependency label classification,predicate identification, word sense disambigua-tion, and semantic role labeling.
The overviewof our system is illustrated in Figure 1.
Our de-c?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.Figure 1: Overview of the Systempendency parsing module is based on a tourna-ment model (Iida et al, 2003), in which a depen-dency attachment is estimated in step-ladder tour-nament matches.
The relative preference of the at-tachment is modeled by one-on-one match in thetournament.
Iwatate et al (Iwatate et al, 2008)initially proposed the method for Japanese depen-dency parsing, and we applied it to other languagesby relaxing some constraints (Section 2.1).
Depen-dency label classification is performed by a linear-chain sequential labeling on the dependency sib-lings like McDonald?s schemata (McDonald et al,2006).
We use an online passive-aggressive al-gorithm (Crammer et al, 2006) for linear-chainsequential labeling (Section 2.2).
We also usethe other linear-chain sequential labeling methodto annotate whether each word is a predicate ornot (Section 2.3).
If an identified predicate hasmore than one sense, a nearest neighbour classifierdisambiguates the word sense candidates (Section2.4).
We use an online passive-aggressive algo-rithm again for the semantic role labeling (Section2.5).
The machine learning algorithms used in sep-arated modules are diverse due to role sharing.11Unlabeled dependency parsing was done by Iwatate, de-pendency label classification and semantic role labeling wasdone by Watanabe, predicate identification and word sense228We attempt to construct a framework in whicheach module passes k-best solutions and the lastsemantic role labeling module performs rerank-ing of the k-best solutions using the overall infor-mation.
Unfortunately, we couldn?t complete theframework before the deadline of the test run.
Ourmethod is not a ?joint learning?
approach but apipeline approach.2 Methods2.1 Unlabeled Dependency ParsingThe detailed description of the tournament model-based Japanese dependency parsing is found in(Iwatate et al, 2008).
The original Iwatate?s pars-ing algorithm was for Japanese, which is for astrictly head-final language.
We adapt the algo-rithm to English in this shared task.
The tour-nament model chooses the most likely candidatehead of each of the focused words in a step-ladder tournament.
For a given word, the al-gorithm repeats to compare two candidate headsand finds the most plausible head in the seriesof a tournament.
On each comparison, the win-ner is chosen by an SVM binary classifier witha quadratic polynomial kernel2.
The model usesdifferent algorithms for training example gener-ation and parsing.
Figures 2 and 3 show train-ing example generation and parsing algorithm, re-spectively.
Time complexity of both algorithms isO(n2) for the number of words in an input sen-tence.
Below, we present the features for SVM// N: # of tokens in input sentence// true_head[j]: token j?s head at// training data// gen(j,i1,i2,LEFT): generate an example// where token j is dependent of i1// gen(j,i1,i2,RIGHT): generate an example// where token j is dependent of i2// Token 0 is the virtual ROOT.for j = 1 to N-1 doh = true_head[j];for i = 0 to h-1 doif i!=j then gen(j,i,h,RIGHT);for i = h+1 to N doif i!=j then gen(j,h,i,LEFT);end-for;Figure 2: Pseudo Code of Training Example Gen-erationdisambiguation was done by Asahara, and all tasks were su-pervised by Matsumoto.2We use TinySVM as an SVM classifier.
chasen.org/?taku/software/TinySVM/// N: # of tokens in input sentence// head[]: (analyzed-) head of tokens// classify(j,i1,i2): ask SVM// which candidate (i1 or i2) is// more likely for head of j.// return LEFT if i1 wins.// return RIGHT if i2 wins.// cands.push_back(k): add token index k// to the end of cands.// cands.erase(i): remove i-th element// from cands.for j = 1 to N docands = [];for i = 0 to N doif i!=j then cands.push_back(i);end-for;while cands.size() > 1 doif classify(j,cands[0],cands[1]) = LEFT thencands.erase(1);elsecands.erase(0);end-if;end-while;head[j] = cands[0];end-for;Figure 3: Pseudo Code of Parsing Algorithmin our tournament model.
The FORM, LEMMA,GPOS(for training), PPOS(for testing, instead ofGPOS), SPLIT FORM, SPLIT LEMMA, PPOSSin the following tokens were used as the features:?
Dependent, candidate1, candidate2?
Immediately-adjacent tokens of dependent, candidate1,candidate2, respectively?
All tokens between dependent-candidate1, dependent-candidate2, candidate1-candidate2, respectivelyWe also used the distance feature: distance (1 or2-5 or 6+ tokens) between dependent-candidate1,dependent-candidate2, and candidate1-candidate2.Features corresponding to the candidates, includ-ing the distance feature, have a prefix that indicatesits side: ?L-?
(the candidate appears on left-hand-side of the dependent) or ?R-?
(appears on right-hand-side of the dependent).
Training an SVMmodel with all examples is time-consuming, andsplit the examples by the dependent GPOS fortraining (PPOS for testing, instead of GPOS3) torun SVM training in parallel.
Since the number ofexamples with the dependent PPOS:IN, NN, NNP3We cannot use GPOS for testing due to the shared taskregulation.229is still large, we used only first 1.5 million exam-ples for the dependent GPOS.
Note that, the algo-rithm does not check the well-formedness of de-pendency trees4.2.2 Dependency Label ClassificationThis phase labels a dependency relation label toeach word in a parse tree produced in the preced-ing phase.
(McDonald et al, 2006) suggests thatedges of head xiand its dependents xj1, ..., xjMare highly correlated, and capturing these corre-lation improves classification accuracy.
In theirapproach, edges of a head and its dependentsei,j1, ..., ei,jMare classified sequentially, and thenViterbi algorithm is performed to find the highestscoring label sequence.
We take a similar approachwith some simplification.
In our system, each edgeis classified deterministically, and the previous de-cision is used as a feature for the subsequent clas-sification.We use an online passive aggressive algorithm(Crammer et al, 2006)5for dependency label clas-sification since it converges fast, gives good per-formance and can be implemented easily.
The fea-tures used in this phase are primarily similar to thatof (McDonald et al, 2006).Word features: SPLIT LEMMA, PPOS, affix (lengths 2and 3) of the head and the dependent.Position: Position relation between the head and the depen-dent (Is the head anterior to dependent?).
Is the wordtop of the sentence?
Is the word last of the sentence?Context features: SPLIT LEMMA, PPOS, affix (lengths 2and 3) of the nearest left/right word.
SPLIT LEMMAand PPOS bigram (ww, wp, pw, pp) of the head and thedependent (window size 5).Sibling features: SPLIT LEMMA, PPOS, affix (lengths 2and 3) of the dependent?s nearest left and right siblingsin the dependency tree.Other features: The number of dependent?s children.Whether the dependent and the dependent?s grandparent SPLIT LEMMA/PPOS are the same.
Theprevious classification result (previous label).2.3 Predicate IdentificationThis phase solves which word can be a predi-cate.
In the predicate spotting, the linear-chain4We tried to make a k-best cascaded model among themodules.
The latter module can check the well-formednessof the tree.
The current implementation skips this well-formedness checking.5We use PA algorithm among PA, PA-I and PA-II in(Crammer et al, 2006).CRF (Lafferty et al, 2001) annotates whether theword is a predicate or not.
The FORM, LEMMA(itself, and whether the LEMMA is registered inthe PropBank/NomBank frames), SPLIT FORM,SPLIT LEMMA, PPOSS within 5 token windowsize are used as the features.
We also use bigramfeatures within 3 token window size and trigramfeatures within 5 token window size for FORM,LEMMA, SPLIT FORM, SPLIT LEMMA, andPPOSS.
The main reason why we use a sequencelabeling method for predicate identification wasto relax the effect of the tagging error of PPOSand PPOSS.
However, we will show later that thismodule aggravates the total performance.2.4 Word Sense DisambiguationFor the word sense disambiguation, we use 1-nearest neighbour method in a compressed fea-ture space by probabilistic latent semantic index-ing (PLSI).
We trained the word sense disambigua-tion model from the example sentences in the train-ing/development data and PropBank/NomBankframes.
The metric in the nearest neighbourmethod is based on the occurrence of LEMMAin the example sentences.
However, the exam-ples in the PropBank/NomBank do not contain thelemma information.
To lemmatize the words inthe PropBank/NomBank, we compose a lemma-tizer from the FORM-LEMMA table in the train-ing and development.6Since the metric spaceis very sparse, PLSI (Hofmann, 1999) is used toreduce the metric space dimensions.
We usedKL-divergence between two examples of P (di|zk)of P (di, wj) =?kP (di|zk)P (wj|zk)P (zk) ashemi-metric for the nearest neighbour method7,in which di?
D is an example sentence in thetraining/devel/test data and PropBank/NomBankframes; wj?
W is LEMMA; and zk?
Z is alatent class.
We use |Z| = 100, which gave thebest performance in the development data.
Note,we transductively used the test data for the PLSImodeling within the test run period.2.5 Semantic Role LabelingWhile semantic role labeling task is generally per-formed by two phases: argument identification andargument classification, we did not divide the task6We are not violating the closed track regulation to buildthe lemmatizer.
If a word in the PropBank/NomBank is not inthe training/development data, we give up lemmatization.7We useDKL=?kP (dinput data|zk)logP (dinput data|zk)P (d1-nearest data|zk)as hemi-metric.
It is a non-commutative measure.230into the two phases.
That is, argument candidatesare directly assigned a particular semantic role la-bel.
We did not employ any candidate filtering pro-cedure, so argument candidates consist of words inany predicate-word pair.
The argument candidatesthat have no roles are assigned ?NONE?
label.
Forthe reason that described in Section 2.2 (fast con-vergence and good performance), we use an on-line passive aggressive algorithm for learning thesemantic role classifiers.Useful features for argument classification ofverb and noun predicates are different.
For exam-ple, voice (active or passive) is essential for verbpredicate?s argument classification.
On the otherhand, presence of a genitive word is useful fornoun predicate?s argument classification.
For thisreason, we created twomodels: argument classifierfor verb predicates and that for noun predicates.Semantic frames are useful information for se-mantic role classification.
Generally, obligatoryarguments not included in semantic frames do notappear in actual texts.
For this reason, we usePropBank/NomBank semantic frames for seman-tic role pruning.
Suppose semantic roles in the se-mantic frame are Fi= {A0, A1, A2, A3}.
Sinceobligatory arguments are {A0...AA}, the remain-ing arguments {A4, A5, AA} are removed fromlabel candidates.For verb predicates, the features used in our sys-tem are based on (Hacioglu, 2004).
We also em-ployed some other features proposed in (Gildeaand Jurafsky, 2002; Pradhan et al, 2004b).
Fornoun predicates, the features are primarily basedon (Pradhan et al, 2004a).
The features that wedefined for semantic role labeling are as follows:Word features: SPLIT LEMMA and PPOS of the predicate,dependent and dependent?s head, and its conjunctions.Dependency label: The dependency label between the argu-ment candidate and the its head.Family: The position of the argument candidate with respectto the predicate position over the dependency tree (e.g.,child, sibling).Position: The position of the head of the dependency relationwith respect to the predicate position in the sentence.Pattern: The left-to-right chain of the PPOS/dependency la-bels of the predicate?s children.Context features: PPOS of the nearest left/right word.Path features: SPLIT LEMMA, PPOS and dependency la-bel paths between the predicate and the argument can-didate, and its path bi-gram.Distance: The number of paths between the predicate andthe argument candidate.Voice: Voice of the predicate (active or passive) and voice-position conjunction (for verb predicates).Is predicate plural: Whether the predicate is singular orplural (for noun predicates).Genitives between the predicate and the argument: Isthere a genitive word between the predicate and theargument?
(for noun predicates)3 ResultsTable 1 shows the result of our system.
The pro-posed method was effective in dependency pars-ing (rank 3rd), but was not good in semantic rolelabeling (rank 9th).
One reason of the result ofsemantic role labeling could be usages of Prop-Bank/NomBank frames.
We did not achieve themaximum use of the resources, hence the design offeatures and the choice of learning algorithm maynot be optimal.Figure 4: Overview of the Modified SystemThe other reason is the design of the pipeline.We changed the design of the pipeline after thetest run.
The overview of the modified systemis illustrated in Figure 4.
After the syntactic de-pendency parsing, we limited the predicate can-didates as verbs and nouns by PPOSS, and fil-tered the argument candidates by Xue?s method(Xue and Palmer, 2004).
Next, the candidate pairof predicate-argument was classified by an onlinepassive-aggressive algorithm as shown in Section2.5.
Finally, the word sense of the predicate is de-termined by the module in Section 2.4.
The newresult is scores with ?
in Table 1.
The result meansthat the first design was not the best for the task.AcknowledgementsWe would like to thank the CoNLL-2008 sharedtask organizers and the data providers (Surdeanuet al, 2008).231Problem All WSJ Brown RankComplete Problem 79.10 (80.89?)
80.30 (82.06?)
69.29 (71.32?)
9thSemantic Dependency 70.84 (74.53?)
72.37 (76.01?)
58.21 (62.41?)
9thSemantic Role Labeling 67.92 (72.31?)
69.31 (73.62?)
56.42 (61.64?)
-Predicate Identification & Word Sense Disambiguation 77.20 (79.17?)
79.02 (80.99?)
62.10 (64.03?)
-Syntactic Dependency (Labeled) 87.18 88.06 80.17 3rdSyntactic Label Accuracy 91.63 92.31 86.26 -Unlabeled Syntactic Dependency Unlabeled 90.20 90.73 85.94 -The scores with ?
mark are our post-evaluation results.Table 1: The Results ?
Closed ChallengeReferencesBuchholz, Sabine and Erwin Marsi.
2006.
CoNLL-X Shared Task on Multilingual Dependency Parsing.In CoNLL-2006: Proceedings of the Tenth Confer-ence on Computational Natural Language Learning,pages 149?164.Crammer, Koby, Ofer Dekel, Joseph Keshet, ShaiShalev-Schwarz, and Yoram Singer.
2006.
OnlinePassive-Agressive Algorithms.
Journal of MachineLearning Research, 7:551?585.Gildea, Daniel and Daniel Jurafsky.
2002.
AutomaticLabeling of Semantic Roles.
Computational Lin-guistics, 28(3):245?288.Hacioglu, Kadri.
2004.
Semantic role labeling usingdependency trees.
In COLING-2004: Proceedingsof the 20th International Conference on Computa-tional Linguistics, pages 1273?1276.Hofmann, Thomas.
1999.
Probabilistic Latent Seman-tic Indexing.
In SIGIR-1999: Proceedings of the22nd Annual International ACM SIGIR Conferenceon Research and Development in Informatino Re-trieval, pages 50?57.Iida, Ryu, Kentaro Inui, Hiroya Takamura, and YujiMatsumoto.
2003.
Incorporating Contextual Cuesin Trainable Models for Coreference Resolution.
InEACL Workshop ?The Computational Treatment ofAnaphora?, pages 23?30.Iwatate, Masakazu, Masayuki Asahara, and Yuji Mat-sumoto.
2008.
Japanese Dependency Parsing Usinga Tournament Model.
In COLING-2008: Proceed-ings of the 22nd International Conference on Com-putational Linguistics (To Appear).Lafferty, John D., Andrew McCallum, and FernandoC.
N. Pereira.
2001.
Conditional Random Fields:Probabilistic Models for Segmenting and LabelingSequence Data.
In ICML-1001: Proceedings ofthe Eighteenth International Conference on MachineLearning, pages 282?289.McDonald, Ryan, Kevin Lerman, and FernandoPereira.
2006.
Multilingual Dependency Analysiswith a Two-Stage Discriminative Parser.
In CoNLL-2006: Proceedings of the Tenth Conference on Com-putational Natural Language Learning, pages 216?220.Nivre, Joakim, Johan Hall, Sandra K?ubler, Ryan Mc-Donald, Jens Nilsson, Sebastian Riedel, and DenizYuret.
2007.
The CoNLL 2007 Shared Task on De-pendency Parsing.
In CoNLL-2007: Proceedings ofthe CoNLL Shared Task Session of EMNLP-CoNLL-2007, pages 915?932.Pradhan, Sameer, Honglin Sun, Wayne Ward, James H.Martin, and Dan Jurafsky.
2004a.
Parsing Argu-ments of Nominalizations in English and Chinese.
InHLT-NAACL-2004: Proceedings of the Human Lan-guage Technology Conference of the North AmericanChapter of the Association for Computational Lin-guistics, pages 141?144.Pradhan, Sameer, Wayne Ward, Kadri Hacioglu,James H. Martin, and Dan Jurafsky.
2004b.
Shal-low Semantic Parsing Using Support Vector Ma-chines.
In HLT-NAACL-2004: Proceedings of theHuman Language Technology Conference of theNorth American Chapter of the Association for Com-putational Linguistics, pages 233?240.Surdeanu, Mihai, Richard Johansson, Adam Meyers,Llu?
?s M`arquez, and Joakim Nivre.
2008.
TheCoNLL-2008 Shared Task on Joint Parsing of Syn-tactic and Semantic Dependencies.
In CoNLL-2008:Proceedings of the 12th Conference on Computa-tional Natural Language Learning.Xue, Nianwen and Martha Palmer.
2004.
CalibratingFeatures for Semantic Role Labeling.
In EMNLP-2004: Proceedings of 2004 Conference on EmpiricalMethods in Natural Language Processing, pages 88?94.232
