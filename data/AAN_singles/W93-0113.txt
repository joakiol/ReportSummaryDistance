Evaluation Techniques for Automatic SemanticExtraction: Comparing Syntactic and Window BasedApproachesGregory GrefenstetteDepartment of Computer ScienceUniversity of PittsburghPittsburgh, PA 15260grefen@cs.pitt, ednAbst ractAs large on-line corpora become more prevalent, a number of attempts have beenmade to automatically extract hesaurus-like r lations directly from text using knowl-edge poor methods.
In the absence of any specific application, comparing the resultsof these attempts i difficult.
Here we propose an evaluation method using gold stan-dards, i.e., pre-existing hand-compiled resources, as a means of comparing extractiontechniques.
Using this evaluation method, we compare two semantic extraction tech-niques which produce similar word lists, one using syntactic ontext of words , andthe other using windows of heuristically tagged words.
The two techniques are verysimilar except hat in one case selective natural anguage processing, a partial syn-tactic analysis, is performed.
On a 4 megabyte corpus, syntactic ontexts producesignificantly better results against the gold standards for the most characteristk:words in the corpus, while windows produce better results for rare words.1 IntroductionAs more text becomes available lectronically, it is tempting to imagine the developmentof automatic  filters able to screen these tremendous flows of text extracting usefill bits ofinformation.
In order to properly filter, it is useful to know when two words are similarin a corpus.
Knowing this would allcviate part of the term variability problem of naturallanguage discussed in Furnas et al (1987) .
Individuals will choose a variety of wordsto name the same object or operation, with little overlap between people's choices.
Thisvariabil ity in naming was cited as the principal reason for large numbers of missed citationsin a large-scale valuation of an information retrieval system \[Blair and Maron, 1985\].
Aproper filter must be able to access information in the text using any word of a set ofsimilar words.
A number of knowledge-rich \[Jacobs and Rau, 1990, Calzolari and Bindi,1990, Mauldin, 1991\] and knowledge-poor \[Brown et al, 1992, Hindle, 1990, Ruge, 1991,Grefenstette, 1992\] methods have been proposed for recognizing when words are similar.The knowledge-rich approaches require either a conceptual dependency representation, orsemantic tagging of the words, while the knowledge-poor approaches require no previouslyencoded semantic information, and depend on frequency of co-occurrence of word contextsto determine similarity.
Evaluations of results produced by the above systems are oftenbeen l imited to visual verification by a human subject or left to the human reader.In this paper, we propose gold standard evaluation techniques, allowing us to ob-jectively evaluate and to compare two knowledge-poor approaches for extracting wordsimilarity relations from large text corpora.
In order to evaluate the relations extracted,we measure the overlap of the results of each technique against existing hand-created143repositories of semantic information such as thesauri and dictionaries.
We describe below}low such resources can be used as evaluation tools, and apply them to two knowledge-poorapproaches.One of the tested semantic extraction approaches uses selective natural anguage pro-cessing, in this case the lexical-syntactic relations that can be extracted for each word ina corpus by robust parsers \[Hindle, 1983, Grefenstette, 1993\].
The other approach uses avariation on a classic windowing technique around each word such as was used in \[Phillips,1985\].
Both techniques are applied to the same 4 megabyte corpus.
We evaluate the re-sults of both techniques using our gold standard evaluations over thesauri and dictionariesand compare the results obtained by the syntactic based method to those obtained by thewindowing method.
The syntax-based method provides a better overlap with the manu-ally defined thesaurus classes for the 600 most frequently appearing words in the corpus,while for rare words the windowing method performs lightly better for rare words.2 Go ld  Standards  Eva luat ion2.1 Thesaur iRoger's Thesaurus i readily available via anonymous ftp 1.
In it are collected more than30,000 unique words arranged in a shallow hierarchy under 1000 topic numbers uch asExistence (Topic Number 1), Inexistence (2), Substantiality (3), Unsubstantiality (4),.
.
.
,  Rite (998), Canonicals (999), and Temple (1000).
Although this is far from thetotal number of semantic axes of which one could think, it does provide a wide swath ofcommonly accepted associations of English language words.
We would expect that anysystem claiming to extract semantics from text should find some of the relations containedin this resource.By transforming the online source of such a thesaurus, we use it as a gold standard bywhich to measure the results of different similarity extraction techniques.
This measure-ment is done by checking whether the 'similar words' discovered by each technique areplaced under the same heading in this thesaurus.In order to create this evaluation tool, we extracted a list consisting of all single-wordentries from our thesauri with their topic number or numbers.
A portion of the extractedRoger list in Figure 1 shows that abatement appears under two topics: Nonincrease (36)and Discount (813).
Abbe and abbess both belong under the same topic heading 996(Clergy).
The extracted Roger's list has 60,071 words (an average of 60 words for eachof the 1000 topics).
Of these 32,000 are unique (an average of two occurrence for eachword).
If we assume for simplicity that each word appears under exactly 2 of the 1000topics, and that the words are uniformly distributed, the chance that two words wl andw2 occur under the same topic isPnoaa = 2 ,  (2/1000),since wl is under 2 topic headings and since the chance that w2 is under any specific topicheading is 2/1000.
The probability of finding two randomly chosen words together underthe same heading, then, is 0.4%.Our measurement of a similarity extraction technique using this gold standard is per-formed as follows.1 For example, inMarch 1993 it was available via anonymous ftp at the Internet site world.std.com inthe directory/obi/obi2/Gutenberg/etext91, as well  at over  30 other  s ites.144Roget ' sentry Topic, , ?abaCement 36abatement 813abatis 717abatjour 260abattis 717abattoir 361abba 166abbacy 995abbat ia l  995abbatical 995abbatis 717~bbe 996abbess 996Macquarieentry subheading, o ?disesteem 036406disesteem 063701diseur 022701disfavour 003901disfavour 056601disfavour 063701disfeature 018212disfeaturement 018201disfigure 006804disfigure 018212disfigure 020103disfigured 006803disfigured 020102. ?
.Figure 1: Samples from One Word Entries in Both ThesauriGiven a corpus, use the similarity extraction method to derive similarity judge-ments between the words appearing in the corpus.
For each word, take theword appearing as most similar.
Examine the human compiled thesaurus tosee if that pair of words appears under the same topic number.
If it does,count this as a hit.This procedure was followed on the 4 megabyte corpus described below to test two seman-tic extraction techniques, one using syntactically derived contexts to judge similarity andone using window-based contexts.
The results of these evaluations are also given below.2 .2  D ic t ionaryWe also use an online dictionary as a gold standard following a slightly different procedure.Many researchers have drawn on online dictionaries in attempts to do semantic discovery\[Sparck Jones, 1986, Vossen et aL, 1989, Wilks et ai., 1989\], whereas we use it here onlyas a tool for evaluating extraction techniques from unstructured text.
We have an onlineversion of Webster's 7th available, and we use it in evaluating discovered similarity pairs.This evaluation is based on the assumption that similar words will share some overlap intheir dictionary definitions.
In order to determine overlap, each the entire literal definitionis broken into a list of individual words.
This list of tokens contains all the words in thedictionary entry, including dictionary-related markings and abbreviations.
In order toclean this list of non-information-bearing words, we automatically removed any word ortoken1.
of fewer than 4 characters,2.
among the most common 50 words of 4 or more letters in the Brown corpus,3.
among the most common 50 words of 4 or more letters appearing in the definitionsof Webster's 7th,145ad-min - i s - t ra - t lon  n. 1. the act or process of  admin is ter ing 2. per formance of execut ivedut ies  :: c<MANAGEMENT> 3. the  execut ion of public affairs as d is t ingu ished frompolicy mak ing  4. a) a body  of persons who admin is ter  b) i<cap> :: a group const i tu t ingthe polit ical execut ive in a presidential  government  c) a governmenta l  agency or board 5.the  te rm of office of  an admin is t ra t ive  officer, or body.administer, administering, administrative, affairs, agency, board,constituting, distinguished, duties, execution, executive, government,governmental, making, management, office, officer, performance,persons, policy, political, presidential, public, termFigure 2: Webster definition of "administration," and resulting definition list after filteringthrough stoplist.4.
listed as a preposition, quantifier, or determiner in our lexicon,5.
of 4 or more letters from a common information retrieval stoplist,6.
among the dictionary-related set: slang, attrib, kind, word, brit, heSS, lion, ment.These conditions generated a list of 434 stopwords of 4 or more characters which areretracted from any dictionary definition, The remaining words are sorted into a list.
Forexample, the list produced for the definition of the word administration is given in Figure 2.For simplicity no morphological analysis or any other modifications were performed onthe tokens in these lists.To compare two words using these lists, the intersection of each word's filtered defi-nition list is performed.
For example, the intersection between the lists derived from thedictionary entries of diamond and ruby is (precious, stone); between right and freedom itis (acting, condition, political, power, privilege, right).
In order to use these dictionary-derived lists as an evaluation tool, we perform the following experiment on a corpus.Given a corpus, take the similarity pairs derived by the semanticextraction technique in order of decreasing frequency of the firstterm.
Perform the intersection of their respective two dictionarydefinitions as described above.
If this intersection containstwo or more elements, count this as a hit.This evahlation method was also performed on the results of both semantic extractiontechniques applied to the corpus described in the next section.3 CorpusThe corpus used for the evaluating the two techniques was extracted from Grolier's En-cyclopedia for other experiments in semantic extraction.
In order to generate a relativelycoherent corpus, the corpus was created by extracting only those those sentences whichcontained the word Harvard or one of the thirty hyponyms found under the word inst i -tu t ion  in WordNet 2 \[Miller et al, 1990\], viz.
institution, establishment, charity, religion,?.., settlement?
This produced a corpus of 3.9 megabytes of text.2 WordNet was not used itself as a gold standard since its hierarchy is very deep and its inherent notionof semantic lasses is not as clearly defined as in Roger.1464 Semantic Extraction TechniquesWe will use these gold standard evaluation techniques to compare two techniques forextracting similarity lists from raw text.The first technique \[Grefenstette, 1992\] extracts the syntactic ontext of each wordthroughout the corpus.
The corpus is divided into lexical units via a regular grammar,each lexical unit is assigned a list of context-free syntactic ategories, and a normalizedform.
Then a time linear stochastic grammar similar to the one described in \[de Marcken,1990\] selects a most probable category for each word.
A syntactic analyzer described in\[Grefenstette, 1993\] chunks nouns and verb phrases and create relations within chunksand between chunks.
A noun's context becomes all the other adjectives, nouns, and verbsthat enter into syntactic relations with it.As a second technique, more similar to classical knowledge-poor techniques \[Phillips,1985\] for judging word similarity, we do not perform syntactic disambiguation a d analysis,but simply consider some window of words around a given word as forming the contextof that word.
We suppose that we have a lexicon, which we do, that gives all the possibleparts of speech for a word.
Each word in the corpus is looked up in this lexicon as inthe first technique, in order to normalize the word and know its possible parts of speech\[Evans et al, 1991\].
A noun's context will be all the words that can be nouns, adjectives,or verbs within a certain window around the noun.
The window that was used was allnouns, adjectives, or verbs on either side of the noun within ten and within the samesentence.In both cases we will compare nouns to each other, using their contexts.
In the firstcase, the disambiguator determines whether a given ambiguous word is a noun or not.
Inthe second case, we will simply decide that if a word can be at once a noun or verb, or anoun or adjective, that it is a noun.
This distinction between the two techniques of usinga cursory syntactic analysis or not allows us to evaluate what is gained by the addition ofthis processing step.Figure 3 below shows the types of contexts extracted by the selective syntactic tech-nique and by the windowing technique for a sentence from the corpus.Once context is extracted for each noun, the contexts are compared for similarityusing a weighted Jaccard measure \[Grefenstette, 1993\].
In order to reduce run time forthe similarity comparison, only those nouns appearing more than 10 times in tile corpuswere retained.
2661 unique nouns appear 10 times or more.
For the windowing technique33,283 unique attributes with which to judge the words are extracted.
The similarityjudging run takes 4 full days on a DEC 5000, compared to 3 and 1/2 hours for thesimilarity calculation using data from the syntactic technique, due to greatly increasednumber of attributes for each word.
For each noun, we retain the noun rated as mostsimilar by the Jaccard similarity measure.
Figure 4 shows some examples of words foundmost similar by both techniques.5 ResultsThe first table, in Figure 5, compares the hits produced by the two techniques over Rogel'sand over another online thesaurus, Macquarie's, that we had available in the Laboratoryfor Computational Linguistics at Carnegie Mellon University.
This table compares the re-sults obtained from the windowing technique described in preceding paragraphs to those147With the arrival of Europeans in 1788 , many Aboriginal societies, caught vithin the coils of expanding white settlement , weregradually destroyed .Contexts o/nouns extracted after syntactic analysisar r iva l  europeansociety catch-SUBJsettlement expand-DOBJSome contex~ar r iva l  abor ig ina la r r iva l  co i la r r iva l  sett lementeuropean abor ig ina leuropean co i leuropean set t lementsociety europeansociety coilsociety settlementsociety aboriginal society destroy-DOBJcoil catch-IOBJ settlement whiteextracted with 10 full-word windowarrival societyarrival expandarrival destroyeuropean societyeuropean expandeuropean destroysociety aboriginalsociety expandsociety destroyarrival catcharrival uhiteeuropean arrivaleuropean catcheuropean ehitesociety arrivalsociety catchsociety whiteFigure 3: Comparison of Extracted Contexts using Syntactic and Non-Syntactic Tech-niquesCorpus wordformationworkfoundationgovernmenteducationreligiousuniversitygroupestablishmentpowercreationstateprogramlawyearcenterartformcenturymemberpartTechnique usedSyntaxcreationschoolinstitutionconstitutiontrainingreligioninstitutioninstitutioncreationauthorityestablishmentlawinstitutionconstitutioncenturydevelopmentarchitecturegroupyeargroupcentersystemreligioussystemstatepubliccenturyinstitutionmembergovernmentgovernmentstategovernmenteducationpublicgovernmentcitysciencelifereligiousgroupgovernmentFigure 4: Sample of words found to be most similar, by the syntactic based technique,and by the window technique, to some frequently occurring words in the corpus1481-2021-4041-6061-8081-100101-200201-300301-400401-500501-600601-700701-800801-900901-10001001-20002001-300025%10%25%15%15%14%21%13%15%13%8%11%17%8%10.2%7.9%50%30%30%30%40%31%29%17%16%11%11%9%6%10%4.9%2.4%15%20%30%20%15%19%20%12%12%10%11%9%13%9%11.8%7.9%ROGET hitsSYNTAX WINDOW40% 55%45% 40%35% 55%30% 45%35% 35%34% 34%30% 29%18% 25%13% 24%15% 19%14% 20%9% 17%7%; 25%9% 29%5.3% 19.2%2.1% 15.2%50%60%70%05%55%55%34%29%26%16%14%17%12%12%6.9%5.2%Figure 5: Windowing vs Syntactic Percentage of Hits for words from most frequent oleast.c'3&2results over corpus using Window vs Syntactic ContextsR.OGET MACQUARIE  WEBSTERRANK WINDOW SYNTAX WINDOW SYNTAX1-20 21-40 41-60 61-80 81-100 100 200 300 400 500 600 700 800 900 1000 >2000Figure 6: Comparison of hit percentage in Roger's using simple 10-word windowing tech-nique(clear) vs syntactic technique(black).
The y-axis gives the percentage of hits for eachgroup of frequency-ranked terms.149WEBSTER hits?
:\] '3 & ?1-20 21-40 41-60 61-80 81-100 100 200 300 400 500 600 700 800 900 1000 >2000Figure 7: Comparison of hits in Macquarie's using simple 10-word windowing tech-nique(clear) vs syntactic technique(black).
The y-axis gives the percentage of hits foreach group of frequency-ranked terms.
"3MACQUARIE  hits%20 21-40 41'60 61-80 81-100 100 200 300 400 500 600 700 800 9(X) 1000 >2000Figure 8: Comparison of hit percentage in Webster's using simple 10-word windowingtechnique (hashed bars) vs syntactic technique (solid bars).
The y-axis gives the percent-age of hits for each group of frequency-ranked terms.150RogerFirst 600WINDOWHITSMISSSYNTACTICHITS MISS48 6091 401Macquarie SYNTACTICFirst 600 HITS MISSI WINDOWHITSMISS42 54103 401X 2=6.4  X 2= 15.3p < .025 p < .005RogerLast 600WINDOWHITSMISSSYNTACTICHITS MISS2 2814 556MacquarieLast 600WINDOWHITSMISSSYNTACTICHITS MISS4 4014 542X 2=4.6  X 2= 12.5p < .05 p < .0005Figure 9: X 2 results comparing Syntactic and windowing hits in man-made thesauriobtained from the syntactic technique, retaining only words for which similarity judge-ments were made by both techniques.It can be seen in Figure 5 that simple technique of moving a window over a largecorpus, counting co-occurrences of words, and eliminating empty words, provides a goodhit ratio for frequently appearing words, since about 1 out of 5 of the 100 most frequentwords are found similar to words appearing in the same heading in a hand-built hesaurus.It can also be seen that the performance of the partial syntactic analysis based tech-nique is better for the 600 most frequently appearing nouns, which may be considered asthe characteristic vocabulary of the corpus.
The difference in performance between thetwo techniques is statistically significant (p i 0.05).
The results of a X 2 test are given inFigure 9.
Figures 6 and 7 show the same results as histograms.
In these histograms itbecomes more evident that the window co-occurrence t chniques give more hits for lessfrequently occurring words, after the 600th most frequent word.
One reason for this canbe seen by examining the 900th most frequent word, employment.
Since the windowingtechnique xtracts up to 20 non-stopwords from either side, there are still 537 contextwords attached to this word, while the syntactically-based technique, which examinesfiner-grained contexts, only provides 32 attributes.Figure 8 shows the results of applying the less focused dictionary gold standard exper-iment to the similarities obtained from the corpus by each technique.
For this experiment,both techniques provide about the same overlap for frequent words, and the same signifi-cantly stronger showing for the rare words for the windowing technique.1516 Conc lus ionIn this paper wc presented a general method for comparing tile results of two similarityextraction techniques via gold standards.
'Fhis method can be used when no application-specific evaluation technique xists and provides a relative measurement of techniquesagainst human-generated standard semantic resources.
We showed how these gold stan-dards could be processed to produce a tool for measuring overlap between their contentsand the results of a semantic extraction method.
We applied these gold standard evalu-ations to two different semantic extraction techniques passed over the same 4 megabytecorpus.
The syntactic-based technique produced greater overlap with the gold standardsderived from thesauri for the characteristic vocabulary of the corpus, while the window-based technique provided relatively better results for rare words.This dichotomous result suggests that no one statistical technique is adapted to allranges of frequencies of words from a corpus.
Everyday experience suggests that frequentlyoccurring events can be more finely analyzed than rarer ones.
In the domain of corpuslinguistics, the same reasoning can be applied.
For frequent words, finer grained contextsuch as that provided by even rough syntactic analysis, is rich enough to judge similarity.For less frequent words, reaping more though less exact information such as that givenby windows of N words provides more information about each word.
For rare words, thecontext may have to be extended beyond a window, to the paragraph, or section, or entiredocument level, as Crouch (1990) did for rarely appearing words.Acknowledgements .
This research was performed under the auspices of the Labora-tory for Computational Linguistics (Carnegie Mellon University) directed by Professor David A.Evans.Re ferences\[Blair and Maron, 1985\] D.C. Blair and M.E.
Maron.
An evaluation of retrieval effective-ness.
Communications of the ACM, 28:289-299, 1985.\[Brown et al, 1992\] Peter F. Brown, Vincent J. Della Pietra, Petere V. deSouza,Jenifer C. Lai, and Robert L. Mercer.
Class-based n-gram models of natural anguage.Computational Linguistics, 18(4):467-479, 1992.\[Calzolari and Bindi, 1990\] Nicoletta Calzolari and Remo Bindi.
Acquisition of lexicalinformation from a large textual italian corpus.
In Proceedings of the Thirteenth Inter-national Conference on Computational Linguistics, Helsinki, 1990.\[Crouch, 1990\] C. J. Crouch.
An approach to the automatic onstruction of global the-sauri.
Information Processing and Management, 26(5):629-640, 1990.\[de Marcken, 1990\] Carl G. de Marcken.
Parsing the LOB corpus.
In 28th Annual Meetingof the Association for Computational Linguistics, pages 243-251, Pittsburgh, PA, June6-9 1990.
ACL.\[Evans et al, 1991\] David A. Evans, Steve K. Handerson, Robert G. Lefferts, and Ira A.Monarch.
A summary of the CLARIT project.
Technical Report CMU-LCL-91-2,Laboratory for Computational Linguistics, Carnegie-Mellon University, November 1991.152I\[Furnas et aL, 1987\] George W. Fumas, Tomas K. Landauer, L.M.
Gomez, and Susan T.Dumais.
The vocabulary problem in human-system communication.
Communicationsof the ACM, 30(11):964-971, November 1987.\[Grefenstette, 1992\] G. Grefenstette.
Sextant: Exploring unexplored contexts for semanticextraction from syntactic analysis.
In 30th Annual Meeting of the Association forComputational Linguistics, Newark, Delaware, 28 June - 2 July 1992.
ACL'92.\[Grefenstette, 1993\] Gregory Grefenstette.
Extracting semantics from raw text, imple-mentation details.
Heuristics: the Journal of Knowledge Engineering, 1993.
To Appearin the Special Issue on Knowledge Extraction from Text, Available as TR CS92-05,from the University of Pittsburgh, CS Dept.\[Hindle, 1983\] Donald Hindle.
User manual for Fidditeh.
Technical Report 7590-142,Navall Research Laboratory, 1983.\[Hindle, 1990\] D. Hindle.
Noun classification from predicate-argument structures.
In Pro-ceedings of the 28th Annual Meeting of the Association for Computational Linguistics,pages 268-275, Pittsburgh, 1990.
ACL.\[Jacobs and Rau, 1990\] Paul Jacobs and Lisa Rau.
SCISOR.
: Extracting information fromon-line news.
Communications of the ACM, 33(11):88-97, 1990.\[Mauldin, 1991\] M. L. Mauldin.
Conceptual Information Retrieval: A case study in adap-tive parsing.
Kluwer, Norwell, MA, 1991.\[Miller et al, 1990\] George A. Miller, R. Beckwith, C. Fellbaum, D. Gross, and K. J.Miller.
Introduction to WordNet: An on-line lexical database.
Journal of Lexicography,3(4):235-244, 1990.\[Phillips, 1985\] Martin Phillips.
Aspects of Text Structure: An investigation of the lexicalorganization of text.
Elsevier, Amsterdam, 1985.\[Ruge, 1991\] Gerda Ruge.
Experiments on linguistically based term associations.
InRIAO'91, pages 528-545, Barcelona, April 2-5 1991.
CID, Paris.\[Sparck Jones, 1986\] Karen Sparck Jones.
Synonymy and Semantic Classification.
Ed-inburgh University Press, Edinburgh, 1986.
PhD thesis delivered by University ofCambridge in 1964.\[Vossen et ai., 1989\] P. Vossen, W. Meijs, and M. den Broeder.
Meaning and structurein dictionary definitions.
In Bran Boguraev and Ted Briscoe, editors, ComputationalLexicography for Natural Language Processing, pages 171-190.
Longman Group UKLimited, London, 1989.\[Wilks et al, 1989\] Yorick Wilks, D. Fass, C. Guo, J. McDonald, T. Plate, and B. Slator.A tractable machine dictionary as a resource for computational semantics.
In BranBoguraev and Ted Briscoe, editors, Computational Lexicography for Natural LanguageProcessing, pages 193-228.
Longman Group UK Limited, London, 1989.153
