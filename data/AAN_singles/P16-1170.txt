Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1802?1813,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsGenerating Natural Questions About an ImageNasrin Mostafazadeh1, Ishan Misra2, Jacob Devlin3, Margaret Mitchell3Xiaodong He3, Lucy Vanderwende31 University of Rochester, 2 Carnegie Mellon University,3 Microsoft Researchnasrinm@cs.rochester.edu, lucyv@microsoft.comAbstractThere has been an explosion of work in thevision & language community during thepast few years from image captioning tovideo transcription, and answering ques-tions about images.
These tasks have fo-cused on literal descriptions of the image.To move beyond the literal, we choose toexplore how questions about an image areoften directed at commonsense inferenceand the abstract events evoked by objectsin the image.
In this paper, we introducethe novel task of Visual Question Gener-ation (VQG), where the system is taskedwith asking a natural and engaging ques-tion when shown an image.
We providethree datasets which cover a variety of im-ages from object-centric to event-centric,with considerably more abstract trainingdata than provided to state-of-the-art cap-tioning systems thus far.
We train andtest several generative and retrieval mod-els to tackle the task of VQG.
Evaluationresults show that while such models askreasonable questions for a variety of im-ages, there is still a wide gap with humanperformance which motivates further workon connecting images with commonsenseknowledge and pragmatics.
Our proposedtask offers a new challenge to the commu-nity which we hope furthers interest in ex-ploring deeper connections between vision& language.1 IntroductionWe are witnessing a renewed interest in interdis-ciplinary AI research in vision & language, fromdescriptions of the visual input such as image cap-tioning (Chen et al, 2015; Fang et al, 2014; Don-ahue et al, 2014; Chen et al, 2015) and videoNatural Questions:- Was anyone injured in thecrash?- Is the motorcyclist alive?- What caused this accident?Generated Caption:- A man standing next to amotorcycle.Figure 1: Example image along with its naturalquestions and automatically generated caption.transcription (Rohrbach et al, 2012; Venugopalanet al, 2015), to testing computer understandingof an image through question answering (Antol etal., 2015; Malinowski and Fritz, 2014).
The mostestablished work in the vision & language com-munity is ?image captioning?, where the task is toproduce a literal description of the image.
It hasbeen shown (Devlin et al, 2015; Fang et al, 2014;Donahue et al, 2014) that a reasonable languagemodeling paired with deep visual features trainedon large enough datasets promise a good perfor-mance on image captioning, making it a less chal-lenging task from language learning perspective.Furthermore, although this task has a great valuefor communities of people who are low-sighted orcannot see in all or some environments, for oth-ers, the description does not add anything to whata person has already perceived.The popularity of the image sharing applica-tions in social media and user engagement aroundimages is evidence that commenting on picturesis a very natural task.
A person might respondto an image with a short comment such as ?cool?,?nice pic?
or ask a question.
Imagine someone hasshared the image in Figure 1.
What is the veryfirst question that comes to mind?
Your question ismost probably very similar to the questions listednext to the image, expressing concern about themotorcyclist (who is not even present in the im-age).
As you can tell, natural questions are not1802about what is seen, the policemen or the motorcy-cle, but rather about what is inferred given theseobjects, e.g., an accident or injury.
As such, ques-tions are often about abstract concepts, i.e., eventsor states, in contrast to the concrete terms1used inimage captioning.
It is clear that the correspond-ing automatically generated caption2for Figure 1presents only a literal description of objects.To move beyond the literal description of im-age content, we introduce the novel task of VisualQuestion Generation (VQG), where given an im-age, the system should ?ask a natural and engag-ing question?.
Asking a question that can be an-swered simply by looking at the image would beof interest to the Computer Vision community, butsuch questions are neither natural nor engaging fora person to answer and so are not of interest for thetask of VQG.Learning to ask questions is an important task inNLP and is more than a syntactic transformationof a declarative sentence (Vanderwende, 2008).Deciding what to ask about demonstrates under-standing and as such, question generation providesan indication of machine understanding, just assome educational methods assess students?
under-standing by their ability to ask relevant questions3.Furthermore, training a system to ask a good ques-tion (not only answer a question) may imbue thesystem with what appears to be a cognitive abilityunique to humans among other primates (Jordania,2006).
Developing the ability to ask relevant andto-the-point questions can be an essential compo-nent of any dynamic learner which seeks informa-tion.
Such an ability can be an integral compo-nent of any conversational agent, either to engagethe user in starting a conversation or to elicit task-specific information.The contributions of this paper can be sum-marized as follows: (1) in order to enable theVQG research, we carefully created three datasetswith a total of 75,000 questions, which rangefrom object- to event-centric images, where weshow that VQG covers a wide range of abstractterms including events and states (Section 3).
(2)we collected 25,000 gold captions for our event-centric dataset and show that this dataset presents1Concrete terms are the ones that can be experienced withfive senses.
Abstract terms refer to intangible things, such asfeelings, concepts, and qualities2Throughout this paper we use the state-of-the-art cap-tioning system (Fang et al, 2014), henceforth MSR caption-ing system https://www.captionbot.ai/, to gener-ate captions.3http://rightquestion.org/challenges to the state-of-the-art image caption-ing models (Section 3.3).
(3) we perform analysisof various generative and retrieval approaches andconclude that end-to-end deep neural models out-perform other approaches on our most-challengingdataset (Section 4).
(4) we provide a systematicevaluation methodology for this task, where weshow that the automatic metric ?BLEU stronglycorrelates with human judgments (Section 5.3).The results show that while our models learn togenerate promising questions, there is still a largegap to match human performance, making thegeneration of relevant and natural questions an in-teresting and promising new challenge to the com-munity.2 Related WorkFor the task of image captioning, datasets have pri-marily focused on objects, e.g.
Pascal VOC (Ever-ingham et al, 2010) and Microsoft Common Ob-jects in Context (MS COCO) (Lin et al, 2014).MS COCO, for example, includes complex every-day scenes with 91 basic objects in 328k images,each paired with 5 captions.
Event detection is thefocus in video processing and action detection, butthese do not include a textual description of theevent (Yao et al, 2011b; Andriluka et al, 2014;Chao et al, 2015; Xiong et al, 2015).
The num-ber of actions in each of these datasets is still rel-atively small, ranging from 40 (Yao et al, 2011a)to 600 (Chao et al, 2015) and all involve human-oriented activity (e.g.
?cooking?, ?gardening?, ?rid-ing a bike?).
In our work, we are focused on gen-erating questions for static images of events, suchas ?fire?, ?explosion?
or ?snowing?, which have notyet been investigated in any of the above datasets.Visual Question Answering is a relatively newtask where the system provides an answer to aquestion about the image content.
The most no-table, Visual Question Answering (VQA) (Antolet al, 2015), is an open-ended (free-form) dataset,in which both the questions and the answers arecrowd-sourced, with workers prompted to ask avisually verifiable question which will ?stump asmart robot?.
Gao et al (2015) used similarmethodology to create a visual question answeringdataset in Chinese.
COCO-QA (CQA) (Ren et al,2015), in contrast, does not use human-authoredquestions, but generates questions automaticallyfrom image captions of the MS COCO dataset byapplying a set of transformation rules to generatethe wh-question.
The expected answers in CQA1803Figure 2: Example right and wrong questions forthe task of VQG.are by design limited to objects, numbers, colors,or locations.
A more in- depth analysis of VQAand CQA datasets will be presented in Section 3.1.In this work, we focus on questions whichare interesting for a person to answer, not ques-tions designed to evaluate computer vision.
A re-cently published work on VQA, Visual7W (Zhuet al, 2016), establishes a grounding link on theobject regions corresponding to the textual an-swer.
This setup enables a system to answer aquestion with visual answers (in addition to tex-tual answers).
They collect a set of 327,939 7Wmultiple-choice QA pairs, where they point outthat ?where?, ?when?
and ?why?
questions oftenrequire high-level commonsense reasoning, goingbeyond spatial reasoning required for ?which?
or?who?
questions.
This is more in line with the typeof questions that VQG captures, however, the ma-jority of the questions in Visual7w are designedto be answerable by only the image, making themunnatural for asking a human.
Thus, learning togenerate the questions in VQA task is not a use-ful sub-task, as the intersection between VQG andany VQA questions is by definition minimal.Previous work on question generation from tex-tual input has focused on two aspects: the gram-maticality (Wolfe, 1976; Mitkov and Ha, 2003;Heilman and Smith, 2010) and the content focusof question generation, i.e., ?what to ask about?.For the latter, several methods have been explored:(Becker et al, 2012) create fill-in-the-blank ques-tions, (Mazidi and Nielsen, 2014) and (Lindberget al, 2013) use manually constructed questiontemplates, while (Labutov et al, 2015) use crowd-sourcing to collect a set of templates and then rankthe potentially relevant templates for the selectedcontent.
To our knowledge, neither a retrievalmodel nor a deep representation of textual input,presented in our work, have yet been used to gen-erate questions.3 Data Collection MethodologyTask Definition: Given an image, the task is togenerate a natural question which can potentiallyengage a human in starting a conversation.
Ques-tions that are visually verifiable, i.e., that can beanswered by looking at only the image, are out-side the scope of this task.
For instance, in Figure2, a question about the number of horses (appear-ing in the VQA dataset) or the color of the fieldis not of interest.
Although in this paper we focuson asking a question about an image in isolation,adding prior context or history of conversation isthe natural next step in this project.We collected the VQG questions by crowd-sourcing the task on Amazon Mechanical Turk(AMT).
We provide details on the prompt andthe specific instructions for all the crowdsourcingtasks in this paper in the supplementary material.Our prompt was very successful at capturing non-literal questions, as the good question in Figure 2demonstrates.
In the following Sections, we de-scribe our process for selecting the images to beincluded in the VQG dataset.
We start with imagesfrom MS COCO, which enables meaningful com-parison with VQA and CQA questions.
Given thatit is more natural for people to ask questions aboutevent-centric images, we explore sourcing event-ful images from Flickr and from querying an im-age search engine.
Each data source is representedby 5,000 images, with 5 questions per image.3.1 V QGcoco?5000 and V QGF lickr?5000As our first dataset, we collected VQG ques-tions for a sample of images from the MS COCOdataset4.
In order to enable comparisons with re-lated datasets, we sampled 5,000 images of MSCOCO which were also annotated by the CQAdataset (Ren et al, 2015) and by VQA (Antol etal., 2015).
We name this dataset V QGcoco?5000.Table 1 shows a sample MS COCO image alongwith annotations in the various datasets.
As theCQA questions are generated by rule applicationfrom captions, they are not always coherent.
TheVQA questions are written to evaluate the detailedvisual understanding of a robot, so their questionsare mainly visually grounded and literal.
The ta-ble demonstrates how different VQG questions arefrom VQA and CQA questions.In Figure 3 we provide statistics for the variousannotations on that portion of the MS COCO im-ages which are represented in the V QGcoco?50004http://mscoco.org/1804Dataset AnnotationsCOCO- A man holding a box with a largechocolate covered donut.CQA- What is the man holding with alarge chocolate-covered doughnut in it?VQA- Is this a large doughnut?VQG- Why is the donut so large?- Is that for a specific celebration?- Have you ever eaten a donut that largebefore?- Is that a big donut or a cake?- Where did you get that?Table 1: Dataset annotations on the above image.dataset.
In Figure 3(a) we compare the percent-age of object-mentions in each of the annota-tions.
Object-mentions are words associated withthe gold-annotated object boundary boxes5as pro-vided with the MS COCO dataset.
Naturally,COCO captions (green bars) have the highest per-centage of these literal objects.
Since object-mentions are often the answer to VQA and CQAquestions, those questions naturally contain ob-jects less frequently.
Hence, we see that VQGquestions include the mention of more of those lit-eral objects.
Figure 3(b) shows that COCO cap-tions have a larger vocabulary size, which reflectstheir longer and more descriptive sentences.
VQGshows a relatively large vocabulary size as well,indicating greater diversity in question formula-tion than VQA and CQA.
Moreover, Figure 3(c)shows that the verb part of speech is representedwith high frequency in our dataset.Figure 3(d) depicts the percentage of abstractterms such as ?think?
or ?win?
in the vocabulary.Following Ferraro et al (2015), we use a listof most common abstract terms in English (Van-derwende et al, 2015), and count all the otherwords except a set of function words as concrete.This figure supports our expectation that VQGcovers more abstract concepts.
Furthermore, Fig-ure 3(e) shows inter-annotation textual similarityaccording to the BLEU metric (Papineni et al,2002).
Interestingly, VQG shows the highest inter-annotator textual similarity, which reflects on theexistence of consensus among human for asking5Note that MS COCO annotates only 91 object categories.a natural question, even for object-centric imageslike the ones in MS COCO.Figure 3: Comparison of various annotations onthe MS COCO dataset.
(a) Percentage of gold ob-jects used in annotations.
(b) Vocabulary size (c)Percentage of verb POS (d) Percentage of abstractterms (e) Inter-annotation textual similarity score.The MS COCO dataset is limited in terms ofthe concepts it covers, due to its pre-specifiedset of object categories.
Word frequency inV QGcoco?5000dataset, as demonstrated in Figure4, bears this out, with the words ?cat?
and ?dog?the fourth and fifth most frequent words in thedataset.
Not shown in the frequency graph is thatwords such as ?wedding?, ?injured?, or ?accident?are at the very bottom of frequency ranking list.This observation motivated the collection of theV QGFlickr?5000dataset, with images appearingas the middle photo in a story-full photo album(Huang et al, 2016) on Flickr6.
The details aboutthis dataset can be found in the supplementary ma-terial.3.2 V QGBing?5000To obtain a more representative visualization ofspecific event types, we queried a search engine7with 1,200 event-centric query terms which wereobtained as follows: we aggregated all ?event?
and?process?
hyponyms in WordNet (Miller, 1995),1,000 most frequent TimeBank events (Puste-jovsky et al, 2003) and a set of manually curated30 stereotypical events, from which we selectedthe top 1,200 queries based on Project Gutenbergword frequencies.
For each query, we collectedthe first four to five images retrieved, for a total6http://www.flickr.com7https://datamarket.azure.com/dataset/bing/search1805Figure 4: Frequency graph of top 40 words inV QGcoco?5000dataset.Figure 5: Average annotation length of the threeVQG datasets.of 5,000 images, having first used crowdsourcingto filter out images depicting graphics and car-toons.
A similar word frequency analysis showsthat the V QGBing?5000dataset indeed containsmore words asking about events: happen, work,cause appear in top 40 words, which was our aimin creating the Bing dataset.Statistics: Our three datasets together cover awide range of visual concepts and events, total-ing 15,000 images with 75,000 questions.
Fig-ure 5 draws the histogram of number of tokensin VQG questions, where the average questionlength is 6 tokens.
Figure 6 visualizes the n-gramdistribution (with n=6) of questions in the threeVQG datasets8.
Table 2 shows the statistics of thecrowdsourcing task.3.3 CaptionsBing?5000The word frequencies of questions about theV QGBing?5000 dataset indicate that this dataset8Please refer to our web page on http://research.microsoft.com/en-us/downloads to get a link to adynamic visualization and statistics of all n-gram sequences.# all images 15,000# questions per image 5# all workers participated 308Max # questions written by one worker 6,368Average work time per worker (sec) 106.5Median work time per worker (sec) 23.0Average payment per question (cents) 6.0Table 2: Statistics of crowdsourcing task, aggre-gating all three datasets.is substantially different from the MS COCOdataset.
Human evaluation results of a re-cent work (Tran et al, 2016) further confirmsthe significant image captioning quality degra-dation on out-of-domain data.
To further ex-plore this difference, we crowdsourced 5 cap-tions for each image in the V QGBing?5000datasetusing the same prompt as used to source theMS COCO captions.
We call this new datasetCaptionsBing?5000.
Table 3 shows the resultsof testing the state-of-the-art MSR captioning sys-tem on the CaptionsBing?5000 dataset as com-pared to the MS COCO dataset, measured by thestandard BLEU (Papineni et al, 2002) and ME-TEOR (Denkowski and Lavie, 2014) metrics.
Thewide gap in the results further confirms that in-deed the V QGBing?5000 dataset covers a newclass of images; we hope the availability of thisnew dataset will encourage including more diversedomains for image captioning.BLEU METEORBing MS COCO Bing MS COCO0.101 0.291 0.151 0.247Table 3: Image captioning resultsTogether with this paper we are releasing an ex-tended set of VQG dataset to the community.
Wehope that the availability of this dataset will en-courage the research community to tackle moreend-goal oriented vision & language tasks.4 ModelsIn this Section we present several generative andretrieval models for tackling the task of VQG.
Forall the forthcoming models we use the VGGNet(Simonyan and Zisserman, 2014) architecture forcomputing deep convolutional image features.
Weprimarily use the 4096-dimensional output the lastfully connected layer (fc7) as the input to the gen-erative models.1806Figure 6: VQG N-gram sequences.
?End?
token distinguishes natural ending with n-gram cut-off.Figure 7: Three different generative models for tackling the task of VQG.4.1 Generative ModelsFigure 7 represents an overview of our three gen-erative models.
The MELM model (Fang et al,2014) is a pipeline starting from a set of candi-date word probabilities which are directly trainedon images, which then goes through a maximumentropy (ME) language model.
The MT model isa Sequence2Sequence translation model (Cho etal., 2014; Sutskever et al, 2014) which directlytranslates a description of an image into a ques-tion, where we used the MS COCO captions andCaptionsBing?5000as the source of translation.These two models tended to generate less coher-ent sentences, details of which can be found in thesupplementary material.
We obtained the best re-sults by using an end-to-end neural model, GRNN,as follows.Gated Recurrent Neural Network (GRNN):This generation model is based on the state-of-the-art multimodal Recurrent Neural Network modelused for image captioning (Devlin et al, 2015;Vinyals et al, 2015).
First, we transform the fc7vector to a 500-dimensional vector which servesas the initial recurrent state to a 500-dimensionalGated Recurrent Unit (GRU).
We produce the out-put question one word at a time using the GRU,until we hit the end-of-sentence token.
We trainthe GRU and the transformation matrix jointly, butwe do not back-propagate the CNN due to the sizeof the training data.
The neural network is trainedusing Stochastic Gradient Descent with early stop-ping, and decoded using a beam search of size8.
The vocabulary consists of all words seen 3 ormore times in the training, which amounts to 1942unique tokens in the full training set.
Unknownwords are mapped to to an <unk> token duringtraining, but we do not allow the decoder to pro-duce this token at test time.4.2 Retrieval MethodsRetrieval models use the caption of a nearestneighbor training image to label the test image(Hodosh et al, 2013; Devlin et al, 2015; Farhadiet al, 2010; Ordonez et al, 2011).
For the taskof image captioning, it has been shown that up to80% of the captions generated at test time by anear state-of-the-art generation approach (Vinyalset al, 2015) were exactly identical to the trainingset captions, which suggests that reusing trainingannotations can achieve good results.
Moreover,basic nearest neighbor approaches to image cap-tioning on the MS COCO dataset are shown to out-perform generation models according to automaticmetrics (Devlin et al, 2015).
The performance ofretrieval models of course depends on the diversityof the dataset.We implemented several retrieval models cus-tomized for the task of VQG.
As the first step, wecompute K nearest neighbor images for each testimage using the fc7 features to get a candidatepool.
We obtained the most competitive results bysetting K dynamically, as opposed to the earlier1807Q.
Explosion Hurricane Rain Cloud Car AccidentHuman- What caused this explosion?- Was this explosion anaccident?- What caused thedamage to this city?- What happened tothis place?- Are those rain clouds?- Did it rain?- Did the drivers of this accidentlive through it?- How fast were they going?GRNN- How much did the fire cost?- What is being burned here?- What happened tothe city?- What caused the fall?- What kind of clouds arethese?- Was there a bad storm?- How did the car crash?- What happened to the trailer?KNN- What caused this fire?- What state was thisearthquake in?- Did it rain?- Was anybody hurtin this accident?Caption- A train with smoke comingfrom it.- A pile of dirt.- Some clouds in acloudy day.- A man standing next toa motorcycle.Table 4: Sample generations by different systems on V QGbing?5000, in order: HumanconsensusandHumanrandom, GRNNbingand GRNNall, KNN+minbleu?all, MSR captions.
Q is the query-term.works which fix K throughout the testing.
We ob-served that candidate images beyond a certain dis-tance made the pool noisy, hence, we establish aparameter calledmax-distancewhich is an upperbound for including a neighbor image in the pool.Moreover, our experiments showed that if thereexists a very similar image to the test image, thecandidate pool can be ignored and that test imageshould become the only candidate9.
For address-ing this, we set a min-distance parameter.
Allthese parameters were tuned on the correspond-ing validation sets using the Smoothed-BLEU (Linand Och, 2004) metric against the human refer-ence questions.Given that each image in the pool has five ques-tions, we define the one-best question to be thequestion with the highest semantic similarity10tothe other four questions.
This results in a pool of Kcandidate questions.
The following settings wereused for our final retrieval models:?
1-NN: Set K=1, which retrieves the closest im-age and emits its one-best.?
K-NN+min: Set K=30 with max-distance =0.35, and min-distance = 0.1.
Among the 309At test time, the frequency of finding a train set imagewith distance ?
0.1 is 7.68%, 8.4% and 3.0% in COCO,Flickr and Bing datasets respectively.10We use BLEU to compute textual similarity.
This pro-cess eliminates outlier questions per image.candidate questions (one-best of each image), findthe question with the highest similarity to the restof the pool and emit that: we compute the textualsimilarity according the two metrics, Smoothed-BLEU and Average-Word2Vec (gensim)11.Table 4 shows a few example images along withthe generations of our best performing systems.For more examples please refer to the web pageof the project.5 EvaluationWhile in VQG the set of possible questions isnot limited, there is consensus among the naturalquestions (discussed in Section 3.1) which enablesmeaningful evaluation.
Although human evalua-tion is the ideal form of evaluation, it is impor-tant to find an automatic metric that strongly corre-lates with human judgment in order to benchmarkprogress on the task.5.1 Human EvaluationThe quality of the evaluation is in part determinedby how the evaluation is presented.
For instance,11Average-Word2Vec refers to the sentence-level textualsimilarity metric where we compute the cosine similarity be-tween two sentences by averaging their word-level Word2Vec(Mikolov et al, 2013) vector representations.
Here we use theGenSim software framework (?Reh?u?rek and Sojka, 2010).1808it is important for the human judges to see var-ious system hypotheses at the same time in or-der to give a calibrated rating.
We crowdsourcedour human evaluation on AMT, asking three crowdworkers to each rate the quality of candidate ques-tions on a three-point semantic scale.5.2 Automatic EvaluationThe goal of automatic evaluation is to measure thesimilarity of system-generated question hypothe-ses and the crowdsourced question references.
Tocapture n-gram overlap and textual similarity be-tween hypotheses and references, we use standardMachine Translation metrics, BLEU (Papineni etal., 2002) and METEOR (Denkowski and Lavie,2014).
We use BLEU with equal weights up to4-grams and default setting of METEOR version1.5.
Additionally we use ?BLEU (Galley et al,2015) which is specifically tailored towards gen-eration tasks with diverse references, such as con-versations.
?BLEU requires rating per reference,distinguishing between the quality of the refer-ences.
For this purpose, we crowd-sourced threehuman ratings (on a scale of 1-3) per reference andused the majority rating.The pairwise correlational analysis of humanand automatic metrics is presented in Table 6,where we report on Pearson?s r, Spearman?s ?
andKendall?s ?
correlation coefficients.
As this tablereveals, ?BLEU strongly correlates with humanjudgment and we suggest it as the main evaluationmetric for testing a VQG system.
It is importantto note that BLEU is also very competitive with?BLEU, showing strong correlations with humanjudgment.
Hence, we recommend using BLEU forany further benchmarking and optimization pur-poses.
BLEU can also be used as a proxy for?BLEU for evaluation purposes whenever ratingper reference are not available.5.3 ResultsIn this section, we present the human and auto-matic metric evaluation results of the models in-troduced earlier.
We randomly divided each VQG-5000 dataset into train (50%), val (25%) and test(25%) sets.
In order to shed some light on differ-ences between our three datasets, we present theevaluation results separately on each dataset in Ta-ble 5.
Each model (Section 4.2) is once trainedon all train sets, and once trained only on its cor-responding train set (represented as X in the re-sults table).
For quality control and further insighton the task, we include two human annotationsamong our models: ?Humanconsensus?
(the sameas one-best) which indicates the consensus humanannotation on the test image and ?Humanrandom?which is a randomly chosen annotation among thefive human annotations.It is quite interesting to see that among the hu-man annotations, Humanconsensusachieves con-sistently higher scores than Humanrandom.
Thisfurther verifies that there is indeed a commonintuition about what is the most natural ques-tion to ask about a given image.
As the re-sults of human evaluation in Table 5 shows,GRNNallperforms the best as compared with allthe other models in 2/3 of runs.
All the mod-els achieve their best score on V QGCOCO?5000,which was expected given the less diverse set ofimages.
Using automatic metrics, the GRNNXmodel outperforms other models according toall three metrics on the V QGBing?5000dataset.Among retrieval models, the most competitive isK-NN+min bleu all, which performs the best onV QGCOCO?5000and V QGFlickr?5000datasetsaccording to BLEU and ?BLEU score.
This fur-ther confirms our effective retrieval methodologyfor including min-distance and n-gram overlapsimilarity measures.
Furthermore, the boost from1-NN to K-NN models is considerable accordingto both human and automatic metrics.
It is impor-tant to note that none of the retrieval models beatthe GRNN model on the Bing dataset.
This addi-tionally shows that our Bing dataset is in fact moredemanding, making it a meaningful challenge forthe community.6 DiscussionWe introduced the novel task of ?Visual Ques-tion Generation?, where given an image, the sys-tem is tasked with asking a natural question.
Weprovide three distinct datasets, each covering avariety of images.
The most challenging is theBing dataset, requiring systems to generate ques-tions with event-centric concepts such as ?cause?,?event?, ?happen?, etc., from the visual input.
Fur-thermore, we show that our Bing dataset presentschallenging images to the state-of-the-art caption-ing systems.
We encourage the community to re-port their system results on the Bing test datasetand according to the ?BLEU automatic metric.All the datasets will be released to the public12.This work focuses on developing the capabil-12Please find Visual Question Generation under http://research.microsoft.com/en-us/downloads.1809HumanconsensusHumanrandomGRNNXGRNNall1-NNbleu?X1-NNgensim?XK-NN+minbleu?XK-NN+mingensim?X1-NNbleu?all1-NNgensim?allK-NN+minbleu?allK-NN+mingensim?allHuman EvaluationBing 2.49 2.38 1.35 1.76 1.72 1.72 1.69 1.57 1.72 1.73 1.75 1.58COCO 2.49 2.38 1.66 1.94 1.81 1.82 1.88 1.64 1.82 1.82 1.96 1.74Flickr 2.34 2.26 1.24 1.57 1.44 1.44 1.54 1.28 1.46 1.46 1.52 1.30Automatic EvaluationBLEUBing 87.1 83.7 12.3 11.1 9.0 9.0 11.2 7.9 9.0 9.0 11.8 7.9COCO 86.0 83.5 13.9 14.2 11.0 11.0 19.1 11.5 10.7 10.7 19.2 11.2Flickr 84.4 83.6 9.9 9.9 7.4 7.4 10.9 5.9 7.6 7.6 11.7 5.8MET.Bing 62.2 58.8 16.2 15.8 14.7 14.7 15.4 14.7 14.7 14.7 15.5 14.7COCO 60.8 58.3 18.5 18.5 16.2 16.2 19.7 17,4 15.9 15.9 19.5 17.5Flickr 59.9 58.6 14.3 14.9 12.3 12.3 13.6 12.6 12.6 12.6 14.6 13.0?BLEUBing 63.38 57.25 11.6 10.8 8.28 8.28 10.24 7.11 8.43 8.43 11.01 7.59COCO 60.81 56.79 12.45 12.46 9.85 9.85 16.14 9.96 9.78 9.78 16.29 9.96Flickr 62.37 57.34 9.36 9.55 6.47 6.47 9.49 5.37 6.73 6.73 9.8 5.26Table 5: Results of evaluating various models according to different metrics.
X represents training on thecorresponding dataset in the row.
Human score per model is computed by averaging human score acrossmultiple images, where human score per image is the median rating across the three raters.METEOR BLEU ?BLEUr 0.916 (4.8e-27) 0.915 (4.6e-27) 0.915 (5.8e-27)?
0.628 (1.5e-08) 0.67 (7.0e-10) 0.702 (5.0e-11)?
0.476 (1.6e-08) 0.51 (7.9e-10) 0.557 (3.5e-11)Table 6: Correlations of automatic metrics againsthuman judgments, with p-values in parentheses.ity to ask relevant and to-the-point questions, akey intelligent behavior that an AI system shoulddemonstrate.
We believe that VQG is one steptowards building such a system, where an engag-ing question can naturally start a conversation.
Tocontinue progress on this task, it is possible to in-crease the size of the training data, but we also ex-pect to develop models that will learn to generalizeto unseen concepts.
For instance, consider the ex-amples of system errors in Table 7, where visualfeatures can be enough for detecting the specificset of objects in each image, but the system cannotmake sense of the combination of previously un-seen concepts.
Another natural future extension ofthis work is to include question generation withina conversational system (Sordoni et al, 2015; Liet al, 2016), where the context and conversationhistory affect the types of questions being asked.Human- How long did it take tomake that ice sculpture?- Is the dog lookingto take a shower?GRNN- How long has he beenhiking?- Is this in a hotel room?KNN- How deep was thesnow?- Do you enjoy the lightin this bathroom?Table 7: Examples of errors in generation.The rows are Humanconsensus, GRNNall, andKNN+minbleu?all.AcknowledgmentWe would like to thank the anonymous reviewersfor their invaluable comments.
We thank LarryZitnick and Devi Parikh for their helpful discus-sions regarding this work, Rebecca Hanson for hergreat help in data collection, Michel Galley for hisguidelines on evaluation, and Bill Dolan for hisvaluable feedback throughout this work.1810ReferencesMykhaylo Andriluka, Leonid Pishchulin, Peter Gehler,and Bernt Schiele.
2014.
2d human pose estima-tion: New benchmark and state of the art analysis.In IEEE Conference on Computer Vision and Pat-tern Recognition (CVPR), June.Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-garet Mitchell, Dhruv Batra, C. Lawrence Zitnick,and Devi Parikh.
2015.
VQA: Visual question an-swering.
In International Conference on ComputerVision (ICCV).Lee Becker, Sumit Basu, and Lucy Vanderwende.2012.
Mind the gap: Learning to choose gaps forquestion generation.
In Proceedings of the 2012Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies, pages 742?751, Montr?eal,Canada, June.
Association for Computational Lin-guistics.Yu-Wei Chao, Zhan Wang, Yugeng He, Jiaxuan Wang,and Jia Deng.
2015.
HICO: A benchmark for recog-nizing human-object interactions in images.
In Pro-ceedings of the IEEE International Conference onComputer Vision.Jianfu Chen, Polina Kuznetsova, David Warren, andYejin Choi.
2015.
D?ej`a image-captions: A cor-pus of expressive descriptions in repetition.
In Pro-ceedings of the 2015 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pages504?514, Denver, Colorado, May?June.
Associationfor Computational Linguistics.Kyunghyun Cho, Bart Van Merri?enboer, Caglar Gul-cehre, Dzmitry Bahdanau, Fethi Bougares, HolgerSchwenk, and Yoshua Bengio.
2014.
Learningphrase representations using rnn encoder-decoderfor statistical machine translation.
arXiv preprintarXiv:1406.1078.Michael Denkowski and Alon Lavie.
2014.
Meteoruniversal: Language specific translation evaluationfor any target language.
In Proceedings of the EACL2014 Workshop on Statistical Machine Translation.Jacob Devlin, Hao Cheng, Hao Fang, Saurabh Gupta,Li Deng, Xiaodong He, Geoffrey Zweig, and Mar-garet Mitchell.
2015.
Language models for imagecaptioning: The quirks and what works.
In Proceed-ings of the 53rd Annual Meeting of the Associationfor Computational Linguistics and the 7th Interna-tional Joint Conference on Natural Language Pro-cessing (Volume 2: Short Papers), pages 100?105,Beijing, China, July.
Association for ComputationalLinguistics.Jeff Donahue, Lisa Anne Hendricks, Sergio Guadar-rama, Marcus Rohrbach, Subhashini Venugopalan,Kate Saenko, and Trevor Darrell.
2014.
Long-termrecurrent convolutional networks for visual recogni-tion and description.
CoRR, abs/1411.4389.Mark Everingham, Luc Gool, Christopher K. Williams,John Winn, and Andrew Zisserman.
2010.
Thepascal visual object classes (voc) challenge.
Int.
J.Comput.
Vision, 88(2):303?338, June.Hao Fang, Saurabh Gupta, Forrest N. Iandola, Ru-pesh Srivastava, Li Deng, Piotr Doll?ar, JianfengGao, Xiaodong He, Margaret Mitchell, John C. Platt,C.
Lawrence Zitnick, and Geoffrey Zweig.
2014.From captions to visual concepts and back.
CoRR,abs/1411.4952.Ali Farhadi, Mohsen Hejrati, Mohammad AminSadeghi, Peter Young, Cyrus Rashtchian, JuliaHockenmaier, and David Forsyth.
2010.
Every pic-ture tells a story: Generating sentences from images.In Proceedings of the 11th European Conference onComputer Vision: Part IV, ECCV?10, pages 15?29,Berlin, Heidelberg.
Springer-Verlag.Francis Ferraro, Nasrin Mostafazadeh, Ting-HaoHuang, Lucy Vanderwende, Jacob Devlin, MichelGalley, and Margaret Mitchell.
2015.
A survey ofcurrent datasets for vision and language research.In Proceedings of the 2015 Conference on Empiri-cal Methods in Natural Language Processing, pages207?213, Lisbon, Portugal, September.
Associationfor Computational Linguistics.Michel Galley, Chris Brockett, Alessandro Sordoni,Yangfeng Ji, Michael Auli, Chris Quirk, MargaretMitchell, Jianfeng Gao, and Bill Dolan.
2015.deltaBLEU: A discriminative metric for generationtasks with intrinsically diverse targets.
In Proceed-ings of the 53rd Annual Meeting of the Associationfor Computational Linguistics and the 7th Interna-tional Joint Conference on Natural Language Pro-cessing (Volume 2: Short Papers), pages 445?450,Beijing, China, July.
Association for ComputationalLinguistics.Haoyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang,Lei Wang, and Wei Xu.
2015.
Are you talking to amachine?
dataset and methods for multilingual im-age question answering.
CoRR, abs/1505.05612.Michael Heilman and Noah A. Smith.
2010.
Goodquestion!
statistical ranking for question genera-tion.
In Human Language Technologies: The 2010Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 609?617, Los Angeles, California, June.
As-sociation for Computational Linguistics.Micah Hodosh, Peter Young, and Julia Hockenmaier.2013.
Framing image description as a ranking task:Data, models and evaluation metrics.
J. Artif.
Int.Res., 47(1):853?899, May.Ting-Hao Huang, Francis Ferraro, NasrinMostafazadeh, Ishan Misra, Aishwarya Agrawal,Jacob Devlin, Ross B. Girshick, Xiaodong He,Pushmeet Kohli, Dhruv Batra, C. Lawrence Zitnick,Devi Parikh, Lucy Vanderwende, Michel Galley,and Margaret Mitchell.
2016.
Visual storytelling.1811In Proceedings of NAACL 2016.
Association forComputational Linguistics.Joseph Jordania.
2006. Who Asked the First Question?The Origins of Human Choral Singing, Intelligence,Language and Speech.
Logos.Igor Labutov, Sumit Basu, and Lucy Vanderwende.2015.
Deep questions without deep understanding.In Proceedings of the 53rd Annual Meeting of theAssociation for Computational Linguistics and the7th International Joint Conference on Natural Lan-guage Processing (Volume 1: Long Papers).Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,and Bill Dolan.
2016.
A persona-based neural con-versation model.
In Proceedings of the 2015 Con-ference of the North American Chapter of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies.
Association for ComputationalLinguistics.Chin-Yew Lin and Franz Josef Och.
2004.
Auto-matic evaluation of machine translation quality us-ing longest common subsequence and skip-bigramstatistics.
In Proceedings of the 42Nd Annual Meet-ing on Association for Computational Linguistics,ACL ?04, Stroudsburg, PA, USA.
Association forComputational Linguistics.Tsung-Yi Lin, Michael Maire, Serge Belongie, JamesHays, Pietro Perona, Deva Ramanan, Piotr Dollar,and C. Lawrence Zitnick.
2014.
Microsoft COCO:common objects in context.
CoRR, abs/1405.0312.David Lindberg, Fred Popowich, John Nesbit, and PhilWinne.
2013.
Generating natural language ques-tions to support learning on-line.
In Proceedings ofthe 14th European Workshop on Natural LanguageGeneration, pages 105?114, Sofia, Bulgaria, Au-gust.
Association for Computational Linguistics.Mateusz Malinowski and Mario Fritz.
2014.
A multi-world approach to question answering about real-world scenes based on uncertain input.
In Advancesin Neural Information Processing Systems 27, pages1682?1690.Karen Mazidi and Rodney D. Nielsen.
2014.
Linguis-tic considerations in automatic question generation.In Proceedings of the 52nd Annual Meeting of theAssociation for Computational Linguistics (Volume2: Short Papers), pages 321?326, Baltimore, Mary-land, June.
Association for Computational Linguis-tics.Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.Corrado, and Jeffrey Dean.
2013.
Distributed rep-resentations of words and phrases and their com-positionality.
In Advances in Neural InformationProcessing Systems 26: 27th Annual Conference onNeural Information Processing Systems 2013.
Pro-ceedings of a meeting held December 5-8, 2013,Lake Tahoe, Nevada, United States., pages 3111?3119.George A. Miller.
1995.
Wordnet: A lexical databasefor english.
Commun.
ACM, 38(11):39?41, Novem-ber.Ruslan Mitkov and Le An Ha.
2003.
Computer-aidedgeneration of multiple-choice tests.
In Jill Bursteinand Claudia Leacock, editors, Proceedings of theHLT-NAACL 03 Workshop on Building EducationalApplications Using Natural Language Processing,pages 17?22.Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.2011.
Im2text: Describing images using 1 millioncaptioned photographs.
In Neural Information Pro-cessing Systems (NIPS).Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A method for automaticevaluation of machine translation.
In Proceedingsof the 40th Annual Meeting on Association for Com-putational Linguistics, ACL ?02, pages 311?318,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.J.
Pustejovsky, P. Hanks, R. Sauri, A. See,R.
Gaizauskas, A. Setzer, D. Radev, B. Sund-heim, D. Day, L. Ferro, and M. Lazo.
2003.
TheTIMEBANK corpus.
In Proceedings of CorpusLinguistics 2003, pages 647?656, Lancaster, March.Radim?Reh?u?rek and Petr Sojka.
2010.
SoftwareFramework for Topic Modelling with Large Cor-pora.
In Proceedings of the LREC 2010 Workshopon New Challenges for NLP Frameworks, pages 45?50, Valletta, Malta, May.
ELRA.
http://is.muni.cz/publication/884893/en.Mengye Ren, Ryan Kiros, and Richard Zemel.
2015.Question answering about images using visual se-mantic embeddings.
In Deep Learning Workshop,ICML 2015.Marcus Rohrbach, Sikandar Amin, Mykhaylo An-driluka, and Bernt Schiele.
2012.
A database forfine grained activity detection of cooking activities.In IEEE Conference on Computer Vision and PatternRecognition (CVPR).
IEEE, IEEE, June.K.
Simonyan and A. Zisserman.
2014.
Very deep con-volutional networks for large-scale image recogni-tion.
CoRR, abs/1409.1556.Alessandro Sordoni, Michel Galley, Michael Auli,Chris Brockett, Yangfeng Ji, Margaret Mitchell,Jian-Yun Nie, Jianfeng Gao, and Bill Dolan.
2015.A neural network approach to context-sensitive gen-eration of conversational responses.
In Proceed-ings of the 2015 Conference of the North Ameri-can Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pages196?205, Denver, Colorado, May?June.
Associationfor Computational Linguistics.1812Ilya Sutskever, Oriol Vinyals, and Quoc V. Le.
2014.Sequence to sequence learning with neural net-works.
In Advances in Neural Information Process-ing Systems 27: Annual Conference on Neural In-formation Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pages 3104?3112.Kenneth Tran, Xiaodong He, Lei Zhang, Jian Sun, Cor-nelia Carapcea, Chris Thrasher, Chris Buehler, andChris Sienkiewicz.
2016.
Rich image captioning inthe wild.
In Proceedings of Deep Vision Workshopat CVPR 2016.
IEEE, June.Lucy Vanderwende, Arul Menezes, and Chris Quirk.2015.
An amr parser for english, french, german,spanish and japanese and a new amr-annotated cor-pus.
Proceedings of NAACL 2015, June.Lucy Vanderwende.
2008.
The importance of beingimportant: Question generation.
In In Workshop onthe Question Generation Shared Task and Evalua-tion Challenge, Arlington, VA.Subhashini Venugopalan, Huijuan Xu, Jeff Donahue,Marcus Rohrbach, Raymond Mooney, and KateSaenko.
2015.
Translating videos to natural lan-guage using deep recurrent neural networks.
InProceedings the 2015 Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics ?
Human Language Technolo-gies (NAACL HLT 2015), pages 1494?1504, Denver,Colorado, June.Oriol Vinyals, Alexander Toshev, Samy Bengio, andDumitru Erhan.
2015.
Show and tell: A neural im-age caption generator.
In Computer Vision and Pat-tern Recognition.John H Wolfe.
1976.
Automatic question gener-ation from text-an aid to independent study.
InACM SIGCUE Outlook, volume 10, pages 104?112.ACM.Yuanjun Xiong, Kai Zhu, Dahua Lin, and Xiaoou Tang.2015.
Recognize complex events from static im-ages by fusing deep channels.
In The IEEE Confer-ence on Computer Vision and Pattern Recognition(CVPR), June.Bangpeng Yao, Xiaoye Jiang, Aditya Khosla, Andy LaiLin, Leonidas J. Guibas, and Li Fei-Fei.
2011a.Action recognition by learning bases of action at-tributes and parts.
In International Conference onComputer Vision (ICCV), Barcelona, Spain, Novem-ber.Bangpeng Yao, Xiaoye Jiang, Aditya Khosla, Andy LaiLin, Leonidas J. Guibas, and Li Fei-Fei.
2011b.
Hu-man action recognition by learning bases of actionattributes and parts.
In International Conference onComputer Vision (ICCV), Barcelona, Spain, Novem-ber.Yuke Zhu, Oliver Groth, Michael S. Bernstein, andLi Fei-Fei.
2016.
Visual7w: Grounded questionanswering in images.
In IEEE Conference on Com-puter Vision and Pattern Recognition (CVPR).
IEEE,IEEE.1813
