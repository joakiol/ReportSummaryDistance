Proceedings of the Workshop on Lexical and Grammatical Resources for Language Processing, pages 30?38,Coling 2014, Dublin, Ireland, August 24 2014.SentiMerge: Combining Sentiment Lexicons in a Bayesian FrameworkGuy Emerson and Thierry DeclerckDFKI GmbHUniversit?at Campus66123 Saarbr?ucken{guy.emerson, thierry.declerck}@dfki.deAbstractMany approaches to sentiment analysis rely on a lexicon that labels words with a prior polarity.This is particularly true for languages other than English, where labelled training data is noteasily available.
Existing efforts to produce such lexicons exist, and to avoid duplicated effort, aprincipled way to combine multiple resources is required.
In this paper, we introduce a Bayesianprobabilistic model, which can simultaneously combine polarity scores from several data sourcesand estimate the quality of each source.
We apply this algorithm to a set of four German sentimentlexicons, to produce the SentiMerge lexicon, which we make publically available.
In a simpleclassification task, we show that this lexicon outperforms each of the underlying resources, aswell as a majority vote model.1 IntroductionWiegand (2011) describes sentiment analysis as the task of identifying and classifying opinionated con-tent in natural language text.
There are a number of subtasks within this field, such as identifying theholder of the opinion, and the target of the opinion.In this paper, however, we are concerned with the more specific task of identifying polar language -that is, expressing either positive or negative opinions.
Throughout the rest of this paper, we will use theterms sentiment and polarity more or less interchangeably.As Pang and Lee (2008) explain, sentiment analysis has become a major area of research within naturallanguage processing (NLP), with many established techniques, and a range of potential applications.Indeed, in recent years there has been increasing interest in sentiment analysis for commercial purposes.Despite the rapid growth of this area, there is a lack of gold-standard corpora which can be used totrain supervised models, particularly for languages other than English.
Consequently, many algorithmsrely on sentiment lexicons, which provide prior knowledge about which lexical items might indicateopinionated language.
Such lexicons can be used directly to define features in a classifier, or can becombined with a bootstrapping approach.However, when presented with a number of overlapping and potentially contradictory sentiment lex-icons, many machine learning techniques break down, and we therefore require a way to merge theminto a single resource - or else a researcher must choose between resources, and we are left with a leakypipeline between resource creation and application.
We review methods for combining sources of infor-mation in section 2, and then describe four German sentiment lexicons in section 3.To merge these resources, we first want to make them match as closely as possible, and then deal withthe differences that remain.
We deal with the first step in section 4, describing how to align the polarityscores in different lexicons so that they can be directly compared.
Then in section 5, we describe how tocombine these scores together.We report results in section 6, including evaluation against a small annotated corpus, where our mergedresource outperforms both the original resources and also a majority vote baseline.
Finally, we discussdistribution of our resource in section 7, future work in section 8, and conclude in section 9.This work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedings footerare added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/30Lexicon # EntriesC&K 8714PolarityClues 9228SentiWS 1896SentiSpin 95572SentiMerge 96918Table 1: Comparison of lexicon sizes2 Related WorkA general problem is how to deal with missing data - in our case, we cannot expect every word toappear in every lexicon.
Schafer and Graham (2002) review techniques to deal with missing data, andrecommend two approaches: maximum likelihood estimation and Bayesian multiple imputation.
Thelatter is a Monte Carlo method, helpful when the marginal probability distribution cannot be calculatedanalytically.
The probabilistic model presented in section 5.1 is straightforward enough for marginalprobabilities to be calculated directly, and we employ maximum likelihood estimation for this reason.A second problem is how to combine multiple sources of information, which possibly conflict, andwhere some sources are more reliable than others.
This becomes particularly challenging in the casewhen no gold-standard data exists, and so the sources can not be evaluated directly.
Raykar et al.
(2010)discusses this problem from the point of view of crowdsourcing, where there are multiple expert viewsand no certain ground truth - but we can equally apply this in the context of sentiment analysis, viewingeach source as an expert.
However, unlike their approach, our algorithm does not directly produce aclassifier, but rather a newly labelled resource.Confronted with a multiplicity of data sources, some researchers have opted to link resources together(Eckle-Kohler and Gurevych, 2013).
Indeed, the lexicons we consider in section 3 have already beencompiled into a common format by Declerck and Krieger (2014).
However, while linking resourcesmakes it easier to access a larger amount of data, it does not solve the problem of how best to process it.To the best of our knowledge, there has not been a previous attempt to use a probabilistic model tomerge a number of sentiment lexicons into a single resource.3 Data SourcesIn the following subsections, we first describe four existing sentiment lexicons for German.
These fourlexicons represent the data we have merged into a single resource, with a size comparison given in table 1,where we count the number of distinct lemmas, not considering parts of speech.
Finally, in section 3.5,we describe the manually annotated MLSA corpus, which we use for evaluation.3.1 Clematide and KlennerClematide and Klenner (2010) manually curated a lexicon1of around 8000 words, based on the synsetsin GermaNet, a WordNet-like database (Hamp and Feldweg, 1997).
A semi-automatic approach was usedto extend the lexicon, first generating candidate polar words by searching in a corpus for coordinationwith known polar words, and then presenting these words to human annotators.
We will refer to thisresource as the C&K lexicon.3.2 SentimentWortschatzRemus et al.
(2010) compiled a sentiment lexicon2from three data sources: a German translation ofStone et al.
(1966)?s General Inquirer lexicon, a set of rated product reviews, and a German collocationdictionary.
At this stage, words have binary polarity: positive or negative.
To assign polarity weights,they use a corpus to calculate the mutual information of a target word with a small set of seed words.1http://bics.sentimental.li/index.php/downloads2http://asv.informatik.uni-leipzig.de/download/sentiws.html313.3 GermanSentiSpinTakamura et al.
(2005) produced SentiSpin, a sentiment lexicon for English.
It is so named becausedit applies the Ising Model of electron spins.
The lexicon is modelled as an undirected graph, with eachword type represented by a single node.
A dictionary is used to define edges: two nodes are connected ifone word appears in the other?s definition.
Each word is modelled as having either positive or negativesentiment, analogous to electrons being spin up or spin down.
An energy function is defined across thewhole graph, which prefers words to have the same sentiment if they are linked together.
By using asmall seed set of words which are manually assigned positive or negative sentiment, this energy functionallows us to propagate sentiment across the entire graph, assigning each word a real-valued sentimentscore in the interval [?1, 1].Waltinger (2010b) translated the SentiSpin resource into German3using an online dictionary, takingat most three translations of each English word.3.4 GermanPolarityCluesWaltinger (2010a) utilised automatic translations of two English resources: the SentiSpin lexicon, de-scribed in section 3.3 above; and the Subjectivity Clues lexicon, a manually annotated lexicon producedby Wilson et al.
(2005).
The sentiment orientations of the German translations were then manuallyassessed and corrected where necessary, to produce a new resource.43.5 MLSATo evaluate a sentiment lexicon, separately from the general task of judging the sentiment of an entiresentence, we relied on the MLSA (Multi-Layered reference corpus for German Sentiment Analysis).This corpus was produced by Clematide et al.
(2012), independently of the above four lexicons, andconsists of 270 sentences annotated at three levels of granularity.
In the first layer, annotators judgedthe sentiment of whole sentences; in the second layer, the sentiment of words and phrases; and finally inthe third layer, they produced a FrameNet-like analysis of each sentence.
The third layer also includeslemmas, parts of speech, and a syntactic parse.We extracted the sentiment judgements of individual words from the second layer, using the majorityjudgement of the three annotators.
Each token was mapped to its lemmatised form and part of speech,using the information in the third layer.
In some cases, the lemma was listed as ambiguous or unknown,and in these cases, we manually added the correct lemma.
Additionally, we changed the annotation ofnominalised verbs from nouns to verbs, to match the lexical entries.
Finally, we kept all content words(nouns, verbs, and adjectives) to form a set of test data.
In total, there were 1001 distinct lemma types,and 1424 tokens.
Of these, 378 tokens were annotated as having positive polarity, and 399 as negative.4 Normalising ScoresBy considering positive polarity as a positive real number, and negative polarity as a negative real number,all of the four data sources give polarity scores between ?1 and 1.
However, we cannot assume that thevalues directly correspond to one another.
For example, does a 0.5 in one source mean the same thingin another?
An example of the kind of data we are trying to combine is given in table 2, and we can seethat the polarity strengths vary wildly between the sources.The simplest model is to rescale scores linearly, i.e.
for each source, we multiply all of its scores bya constant factor.
Intuitively, the factors should be chosen to harmonise the values - a source with largescores should have them made smaller, and a source with small scores should have them made larger.4.1 Linear Rescaling for Two SourcesTo exemplify our method, we first restrict ourselves to the simpler case of only dealing with two lexicons.Note that when trying to determine the normalisation factors, we only consider words in the overlapbetween the two; otherwise, we would introduce a bias according to what words are considered in each3http://www.ulliwaltinger.de/sentiment4http://www.ulliwaltinger.de/sentiment32Lemma, POS verg?ottern, VC&K 1.000PolarityClues 0.333SentiWS 0.004SentiSpin 0.245Table 2: An example lemma, labelled with polarity strengths from each data sourcesource - it is only in the overlap that we can compare them.
However, once these factors have beendetermined, we can use them to rescale the scores across the entire lexicon, including items that onlyappear in one source.We consider lemmas with their parts of speech, so that the same orthographic word with two possibleparts of speech is treated as two independent lexical entries, in all of the following calculations.
However,we do not distinguish homophonous or polysemous lemmas within the same part of speech, since noneof our data sources provided different sentiment scores for distinct senses.For each word i, let uiand vibe the polarity scores for the two sources.
We would like to findpositive real values ?
and ?
to rescale these to ?uiand ?virespectively, minimising the loss function?i(?ui?
?vi)2.
Intuitively, we are trying to rescale the sources so that the scores are as similar aspossible.
The loss function is trivially minimised when ?
= ?
= 0, since reducing the sizes of the scoresalso reduces their difference.
Hence, we can introduce the constraint that ??
= 1, so that we cannotsimultaneously make the values smaller in both sources.
We would then like to minimise:?i(?ui?1?vi)2= |u|2?2?
2u.v + |v|2?
?2Note that we use vector notation, so that |u|2= ?iu2i.
Differentiating this with respect to ?, we get:2?
|u|2?
2 |v|2?
?3= 0 ?
?
=?|v|?|u|However, observe that we are free to multiply both ?
and ?
by a constant factor, since this doesn?taffect the relationship between the two sources, only the overall size of the polarity values.
By dividingby?|u| |v|, we derive the simpler expressions ?
= |u|?1and ?
= |v|?1, i.e.
we should divide by theroot mean square.
In other words, after normalising, the average squared polarity value is 1 for bothsources.54.2 Rescaling for Multiple SourcesFor multiple sources, the above method needs tweaking.
Although we could use the overlap between allsources, this could potentially be much smaller than the overlap between any two sources, introducingdata sparsity and making the method susceptible to noise.
In the given data, 10749 lexical items appearin at least two sources, but only 1205 appear in all four.
We would like to exploit this extra information,but the missing data means that methods such as linear regression cannot be applied.A simple solution is to calculate the root mean square values for each pair of sources, and then averagethese values for each source.
These averaged values define normalisation factors, as a compromisebetween the various sources.4.3 Unspecified scoresSome lexical items in the PolarityClues dataset were not assigned a numerical score, only a polaritydirection.
In these cases, the task is not to normalise the score, but to assign one.
To do this, we can firstnormalise the scores of all other words, as described above.
Then, we can consider the words withoutscores, and calculate the root mean square polarity of these words in the other sources, and assign themthis value, either positive or negative.5In most sentiment lexicons, polarity strengths are at most 1.
This will no longer be true after this normalisation.335 Combining ScoresNow that we have normalised scores, we need to calculate a combined value.
Here, we take a Bayesianapproach, where we assume that there is a latent ?true?
polarity value, and each source is an observationof this value, plus some noise.5.1 Gaussian ModelA simple model is to assume that we have a prior distribution of polarity values across the vocabulary,distributed normally.
If we further assume that a language is on average neither positive nor negative,then this distribution has mean 0.
We denote the variance as ?2.
Each source independently introducesa linear error term, which we also model with a normal distribution: errors from source a are distributedwith mean 0 and standard deviation ?2a, which varies according to the source.65.2 Hyperparameter SelectionIf we observe a subset S = {a1, .
.
.
, an} of the sources, the marginal distribution of the observationswill be normally distributed, with mean 0 and covariance matrix as shown below.
If the error variances?2aare small compared to the background variance ?2, then this implies a strong correlation between theobservations.??????
?2+ ?2a1?2.
.
.
?2?2?2+ ?2a2?
?
?
?2............?2?2?
?
?
?2+ ?2an?????
?To choose the values for ?2and ?2a, we can aim to maximise the likelihood of the observations, i.e.maximise the value of the above marginal distributions at the observed points.
This is in line with Schaferand Graham (2002)?s recommendations.
Such an optimisation problem can be dealt with using existingsoftware, such as included in the SciPy7package for Python.5.3 InferenceGiven a model as above (whether or not the hyperparameters have been optimised), we can calculate theposterior distribution of polarity values, given the observations xai.
This again turns out to be normallydistributed, with mean ??
and variance ?
?2given by:??
=???2aixai??2+???2ai??
?2= ??2+??
?2aiThe mean is almost a weighted average of the observed polarity values, where each source has weight??2a.
However, there is an additional term ?
?2in the denominator - this means we can interpret thisas a weighted average if we add an additional polarity value 0, with weight ??2.
This additional termcorresponds to the prior.The weights for each source intuitively mean that we trust sources more if they have less noise.
Theextra 0 term from the prior means that we interpret the observations conservatively, skewing valuestowards 0 when there are fewer observations.
For example, if all sources give a large positive polarityvalue, we can be reasonably certain that the true value is also large and positive, but if we only have datafrom one source, then we are less certain if this is true - our estimate ??
is correspondingly smaller, andthe posterior variance ?
?2correspondingly larger.6Because of the independence assumptions, this model can alternatively be viewed as a Markov Network, where we haveone node to represent the latent true polarity strengths, four nodes to represent observations from each source, and five nodesto represent the hyperparameters (variances)7http://www.scipy.org34Figure 1: Gaussian kernel density estimate6 Experiments and Results6.1 Parameter ValuesThe root mean square sentiment values for the sources were: C&K 0.845; PolarityClues 0.608; SentiWS0.267; and SentiSpin 0.560.
We can see that there is a large discrepancy between the sizes of the scoresused, with SentiWS having the smallest of all.
It is precisely for this reason that we need to normalisethe scores.The optimal variances calculated during hyperparameter selection (section 5.2) were: prior 0.528;C&K 0.328; PolarityClues 0.317; SentiWS 0.446; and SentiSpin 0.609.
These values correlate with ourintuition: C&K and PolarityClues have been hand-crafted, and have smaller error variances; SentiWSwas manually finalised, and has a larger error; while finally SentiSpin was automatically generated, andhas the largest error of all, larger in fact than the variance in the prior.
We would expect the polarityvalues from a hand-crafted source to be more accurate, and this appears to be justified by our analysis.6.2 Experimental SetupThe MLSA data (see section 3.5) consists of discrete polarity judgements - a word is positive, negative, orneutral, but nothing in between.8To allow direct evaluation against such a resource, we need to discretisethe continuous range of polarity values; i.e.
if the polarity value is above some positive threshold, wejudge it to be positive; if it is below a negative threshold, negative; and if it is between the two thresholds,neutral.
To choose this threshold before evaluation, we calculated a Gaussian kernel density estimate ofthe polarity values in the entire lexicon, as shown in figure 1.
There is a large density near 0, reflectingthat the bulk of the vocabulary is not strongly polar; indeed, so that the density of polar items is clearlyvisible, we have chosen a scale that forces this bulk to go off the top of the chart.
The high density stopsat around ?0.23, and we have accordingly set this as our threshold.We compared the merged resource to each of the original lexicons, as well as a ?majority vote?
baselinewhich represents an alternative method to combine lexicons.
This baseline involves considering thepolarity judgements of each lexicon (positive, negative, or neutral), and taking the most common answer.To break ties, we took the first answer when consulting the lexicons in the following order, reflecting theirreliability: C&K, PolarityClues, SentiWS, SentiSpin.For the automatically derived resources, we can introduce a threshold as we did for SentiMerge.
How-ever, to make these baselines as competitive as possible, we optimised them on the test data, rather thanchoosing them in advance.
They were chosen to maximise the macro-averaged f-score.
For SentiWS,the threshold was 0, and for SentiSpin, 0.02.Note that a perfect score would be impossible to achieve, since 31 lemmas were annotated with morethan polarity type.
These cases generally involve polysemous words which could be interpreted withdifferent polarities depending on the context.
Indeed, two words appeared with all three labels: Span-nung (tension) and Widerstand (resistance).
In a political context, interpreting Widerstand as positive or8The annotation scheme also allows a further three labels: intensifier, diminisher, and shifter.
While this information isuseful, we treat these values as neutral in our evaluation, since we are only concerned with words that have an inherent positiveor negative polarity.35Lexicon Precision Recall F-scoreC&K 0.754 0.733 0.743PolarityClues 0.705 0.564 0.626SentiWS 0.803 0.513 0.621SentiSpin 0.557 0.668 0.607majority vote 0.548 0.898 0.679SentiMerge 0.708 0.815 0.757Table 3: Performance on MLSA, macro-averagednegative depends very much on whose side you support.
In such cases, a greater context is necessary todecide on polarity, and a lexicon simply cannot suffice.6.3 Evaluation on MLSAWe calculated precision, recall, and f-score (the harmonic mean of precision and recall) for both positiveand negative polarity.
We report the average of these two scores in 3.
We can see that in terms of f-score, SentiMerge outperforms all four data sources, as well as the majority vote.
In applications whereeither precision or recall is deemed to be more important, it would be possible to adjust the thresholdaccordingly.
Indeed, by dropping the threshold to zero, we achieve recall of 0.894, competitive with themajority vote method; and by increasing the threshold to 0.4, we achieve precision of 0.755, competitivewith the C&K lexicon.
Furthermore, in this latter case, the f-score also increases to 0.760.
We do notreport this figure in the table above because it would not be possible to predict such a judicious choiceof threshold without peeking at the test data.
Nonetheless, this demonstrates that our method is robust tochanges in parameter settings.The majority vote method performs considerably worse than SentiMerge, at least in terms of f-score.Indeed, it actually performs worse than the C&K lexicon, with noticeably lower precision.
This findingis consistent with the results of Raykar et al.
(2010), who argue against using majority voting, and whoalso find that it performs poorly.The C&K lexicon achieves almost the same level of performance as SentiMerge, so it is reasonableto ask if there is any point in building a merged lexicon at all.
We believe there are two good reasonsfor doing this.
Firstly, although the C&K lexicon may be the most accurate, it is also small, especiallycompared to SentiSpin.
SentiMerge thus manages to exploit the complementary nature of the differentlexicons, achieving the broad coverage of SentiSpin, but maintaining the precision of the C&K lexiconfor the most important lexical items.Secondly, SentiMerge can provide much more accurate values for polarity strength than any human-annotated resource can.
As Clematide and Klenner (2010) show, inter-annotator agreement for polaritystrength is low, even when agreement for polarity direction is high.
Nonetheless, some notion of po-larity strength can still be helpful in computational applications.
To demonstrate this, we calculated theprecision, recall, and f-scores again, but weighting each answer as a function of the distance from theestimated polarity strength to the threshold.
With this weighted approach, we get a macro-averaged f-score of 0.852.
This is considerably higher than the results given in table 3, which demonstrates that thepolarity scores in SentiMerge are useful as a measure of classification certainty.6.4 Manual InspectionIn cases where all sources agree on whether a word is positive or negative, our algorithm simply servesto assign a more accurate polarity strength.
So, it is more interesting to consider those cases where thesources disagree on polarity direction.
Out of the 1205 lexemes for which we have data from all foursources, only 22 differ between SentiMerge and the C&K lexicon, and only 16 differ between SentiMergeand PolarityClues.
One example is Beschwichtigung (appeasement).
Here we can see the problem withtrying to assign a single numeric value to polarity - in a political context, Beschwichtugung could beinterpreted either as positive, since it implies an attempt to ease tension; or as negative, since it could be36viewed as a sign of weakness.
Another example is unantastbar, which again can be interpreted positivelyor negatively.The controversial words generally denote abstract notions, or have established metaphorical senses.In the authors?
view, their polarity is heavily context-dependent, and a one-dimensional score is notsufficient to model their contibution to sentiment.In fact, most of these words have been assigned very small polarity values in the combined lexicon,which reflects the conflicting evidence present in the various sources.
Of the 22 items which differ inC&K, the one with the largest value in the combined lexicon is dominieren, which has been assigned afairly negative combined score, but was rated positive (0.5) in C&K.7 DistributionWe are making SentiMerge freely available for download.
However, with the expanding number oflanguage resources, it is becoming increasingly important to link resources together, as mentioned insection 2.
For this reason, we are publishing our resource as part of the Linguistic Linked Open Data9initiative.
In particular, we have decided to follow the specifications set forth by Buitelaar et al.
(2013),who propose a representation for sentiment resources based on Lemon (McCrae et al., 2011) and Marl(Westerski et al., 2011).
Lemon10is a model for resource description which builds on LMF (LexicalMarkup Framework),11and facilitates combination of lexicons with ontologies.
Marl is an an ontologylanguage designed for sentiment analysis, which has been fully implemented.128 Future WorkTo align the disparate sources, a simple linear rescaling was used.
However, in principle any monotonicfunction could be considered.
A more general function that would still be tractable could be ui7?
?u?i.Furthermore, the probabilistic model described in section 5.1 makes several simplifying assumptions,which could be weaked or modified.
For instance, we have assumed a normal distribution, with zeromean, both for the prior distribution and for the error terms.
The data is not perfectly modelled by anormal distribution, since there are very clear bounds on the polarity scores, and some of the data takesdiscrete values.
Indeed, we can see in figure 1 that the data is not normally distributed.
An alternativechoice of distribution might yield better results.More generally, our method can be applied to any context where there are multiple resources to bemerged, as long as there is some real-valued property to be aligned.9 ConclusionWe have described the merging of four sentiment lexicons into a single resource, which we have namedSentiMerge.
To demonstrate the utility of the combined lexicon, we set up a word-level sentiment clas-sification task using the MLSA corpus, in which SentiMerge outperformed all four of the underlyingresources, as well as a majority vote baseline.
As a natural by-product of the merging process, we arealso able to indirectly evaluate the quality of each resource, and the results match both intuition and theperformance in the aformentioned classification task.
The approach we have taken requires no parametersetting on the part of the researcher, so we believe that the same method can be applied to other settingswhere different language resources present conflicting information.
This work helps to bridge the gap be-tween resource creation efforts, which may overlap in scope, and NLP research, where researchers oftenwant to use all available data.
Furthermore, by grounding our work in a well-defined Bayesian frame-work, we leave scope for future improvements using more sophisticated probabilistic models.
To allowthe community at large to use and build on this work, we are making SentiMerge publically available fordownload, and are incorporating it into the Linguistic Linked Open Data initiative.9http://linguistics.okfn.org/resources/llod10http://lemon-model.net11http://www.lexicalmarkupframework.org12http://www.gsi.dit.upm.es/ontologies/marl37AcknowledgementsThis work was co-financed by the European Commission, within the FP7 ICT project TrendMiner,13under contract number 287863.
We would like to thank the authors of the all the resources mentionedin this paper for permission to use their data.
We also thank the anonymous reviewers for their helpfulcomments.ReferencesPaul Buitelaar, Mihael Arcan, Carlos A Iglesias, J Fernando S?anchez-Rada, and Carlo Strapparava.
2013.
Lin-guistic linked data for sentiment analysis.
In Proceedings of the 2nd Workshop on Linked Data in Linguistics.Simon Clematide and Manfred Klenner.
2010.
Evaluation and extension of a polarity lexicon for German.
InProceedings of the First Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, page 7.Simon Clematide, Stefan Gindl, Manfred Klenner, Stefanos Petrakis, Robert Remus, Josef Ruppenhofer, UlliWaltinger, and Michael Wiegand.
2012.
MLSA ?
a multi-layered reference corpus for German sentimentanalysis.
pages 3551?3556.
European Language Resources Association (ELRA).Thierry Declerck and Hans-Ulrich Krieger.
2014.
TMO ?
the federated ontology of the TrendMiner project.
InProceedings of the 9th International Language Resources and Evaluation Conference (LREC 2014).Judith Eckle-Kohler and Iryna Gurevych.
2013.
The practitioner?s cookbook for linked lexical resources.Birgit Hamp and Helmut Feldweg.
1997.
GermaNet ?
a lexical-semantic net for German.
In Proceedings ofthe ACL workshop Automatic Information Extraction and Building of Lexical Semantic Resources for NLPApplications, pages 9?15.
Association for Computational Linguistics.John McCrae, Dennis Spohr, and Phillip Cimiano.
2011.
Linking lexical resources and ontologies on the semanticweb with lemon.
In Proceedings of the 8th Extended Semantic Web Conference.Bo Pang and Lillian Lee.
2008.
Opinion mining and sentiment analysis.
Foundations and trends in informationretrieval.Vikas C Raykar, Shipeng Yu, Linda H Zhao, Gerardo Hermosillo Valadez, Charles Florin, Luca Bogoni, and LindaMoy.
2010.
Learning from crowds.
The Journal of Machine Learning Research, 11:1297?1322.Robert Remus, Uwe Quasthoff, and Gerhard Heyer.
2010.
SentiWS ?
a publicly available German-languageresource for sentiment analysis.
In Proceedings of the 7th International Language Resources and EvaluationConference (LREC 2010).Joseph L Schafer and John W Graham.
2002.
Missing data: our view of the state of the art.
Psychologicalmethods, 7(2):147.Philip J Stone, Dexter C Dunphy, and Marshall S Smith.
1966.
The general inquirer: A computer approach tocontent analysis.Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005.
Extracting semantic orientations of words usingspin model.
In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics.Association for Computational Linguistics.Ulli Waltinger.
2010a.
GermanPolarityClues: A lexical resource for German sentiment analysis.
In Proceedingsof the 7th International Language Resources and Evaluation Conference (LREC 2010).Ulli Waltinger.
2010b.
Sentiment analysis reloaded - a comparative study on sentiment polarity identificationcombining machine learning and subjectivity features.
In Proceedings of the 6th International ConferenceonWeb Information Systems and Technologies (WEBIST 2010).
INSTICC Press.Adam Westerski, Carlos A. Iglesias, and Fernando Tapia.
2011.
Linked opinions: Describing sentiments on thestructured web of data.
In Proceedings of the 4th International Workshop Social Data on the Web.Michael Wiegand.
2011.
Hybrid approaches for sentiment analysis.
PhD dissertation, Universit?at des Saarlandes.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005.
Recognizing contextual polarity in phrase-level sen-timent analysis.
In Proceedings of the conference on human language technology and empirical methods innatural language processing, pages 347?354.
Association for Computational Linguistics.13http://www.trendminer-project.eu38
