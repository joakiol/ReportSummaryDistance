Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),pages 191?195, New York City, June 2006. c?2006 Association for Computational LinguisticsMulti-lingual Dependency Parsing at NAISTYuchang CHENG, Masayuki ASAHARA and Yuji MATSUMOTONara Institute of Science and Technology8916-5 Takayama, Ikoma, Nara 630-0192, Japan{yuchan-c, masayu-a, matsu}@is.naist.jpAbstractIn this paper, we present a framework formulti-lingual dependency parsing.
Ourbottom-up deterministic parser adoptsNivre?s algorithm (Nivre, 2004) with apreprocessor.
Support Vector Machines(SVMs) are utilized to determine the worddependency attachments.
Then, a maxi-mum entropy method (MaxEnt) is usedfor determining the label of the depend-ency relation.
To improve the perform-ance of the parser, we construct a taggerbased on SVMs to find neighboring at-tachment as a preprocessor.
Experimentalevaluation shows that the proposed exten-sion improves the parsing accuracy of ourbase parser in 9 languages.
(Haji?
et al,2004; Simov et al, 2005; Simov andOsenova, 2003; Chen   et al, 2003; B?h-mov?
et al, 2003; Kromann, 2003;    vander Beek et al, 2002; Brants et al,2002;   Kawata and Bartels, 2000; Afonsoet al, 2002;   D?eroski et al, 2006; Civitand Mart?, 2002; Nilsson   et al, 2005;Oflazer et al, 2003; Atalay et al, 2003).1 IntroductionThe presented dependency parser is based on ourpreceding work (Cheng, 2005a) for Chinese.
Theparser is a bottom-up deterministic dependencyparser based on the algorithm proposed by (Nivre,2004).
A dependency attachment matrix is con-structed, in which each element corresponds to apair of tokens.
Each dependency attachment is in-crementally constructed, with no crossing con-straint.
In the parser, SVMs (Vapnik, 1998)deterministically estimate whether a pair of wordshas either of four relations: right, left, shift andreduce.
While dependency attachment is estimatedby SVMs, we use a MaxEnt (Ratnaparkhi, 1999)based tagger with the output of the parser to esti-mate the label of dependency relations.
This taggeruses the same features as for the word dependencyanalysis.In our preceding work (Cheng, 2005a), we notonly adopted the Nivre algorithm with SVMs, butalso tried some preprocessing methods.
We inves-tigated several preprocessing methods on a Chi-nese Treebank.
In this shared task (Buchholz et.
al,2006), we also investigate which preprocessingmethod is effective on other languages.
We foundthat only the method that uses a tagger to extractthe word dependency attachment between twoneighboring words works effectively in most of thelanguages.2 System DescriptionThe main part of our dependency parser is basedon Nivre?s algorithm (Nivre, 2004), in which thedependency relations are constructed by a bottom-up deterministic schema.
While Nivre?s methoduses memory-based learning to estimate the de-pendency attachment and the label, we use SVMsto estimate the attachment and MaxEnt to estimateFig.
1 The architecture of our parser(i)Preprocessor (neighboringrelation tagger)(ii)Get contextual features(iii)Estimate dependencyattachment by SVM(iv)Tag label by MaxEntConstruct SubtreeNo more constructionDependency treeFalseTrueLeft or Right attachmentNoneInput sentence (word tokens)191Fig.
2.
The features for dependency analysisBOS-BOSBOS-??-VCV-??-NbN-?-DEDE-??-VHV-??-NacN-??
?-NaN-S Iposition t-1position t-2The child of the position t-1position n position n+1position n+2position tA feature: the distance between the position t and nFORMLEMMACPOSTAGPOSTAGFEATSKey: The features for machinelearning of each tokenthe label.
The architecture of the parser consists offour major procedures and as in Fig.1:(i) Decide the neighboring dependency at-tachment between all adjacent words in theinput sentence by SVM-based tagger (as apreprocessing)(ii) Extract the surrounding features for thefocused pair of nodes.
(iii) Estimate the dependency attachment op-eration of the focused pair of nodes bySVMs.
(iv) If there is a left or right attachment, esti-mate the label of dependency relation byMaxEnt.We will explain the main procedures (steps (ii)-(iv)) in sections 2.1 and 2.2, and the preprocessingin section 2.3.2.1   Word dependency analysisIn the algorithm, the state of the parser is repre-sented by a triple AIS ,, .
S and I are stacks, Skeeps the words being in consideration, and Ikeeps the words to be processed.
A is a list of de-pendency attachments decided in the algorithm.Given an input word sequence W, the parser is ini-tialized by the triple ?,,Wnil .
The parser esti-mates the dependency attachment between twowords (the top elements of stacks S and I).
Thealgorithm iterates until the list I becomes empty.There are four possible operations (Right, Left,Shift and Reduce) for the configuration at hand.Right or Left: If there is a dependency relationthat the word t or n attaches to word n or t, add thenew dependency relation ( )nt ?
or ( )tn ?
into A,remove t or n from S or I.If there is no dependency relation between n andt, check the following conditions.Reduce: If there is no word 'n  ( In ?'
) which maydepend on t, and t has a parent on its left side, theparser removes t from the stack S.Shift: If there is no dependency between n and t,and the triple does not satisfy the conditions forReduce, then push n onto the stack S.In this work, we adopt SVMs for estimating theword dependency attachments.
SVMs are binaryclassifiers based on the maximal margin strategy.We use the polynomial kernel: dK )1()( zxzx, ?+=with d =2.
The performance of SVMs is better thanthat of the maximum entropy method in our pre-ceding work for Chinese dependency analysis(Cheng, 2005b).
This is because that SVMs cancombine features automatically (using the polyno-mial kernel), whereas the maximum entropymethod cannot.
To extend binary classifiers tomulti-class classifiers, we use the pair-wise method,in which we make 2Cn1  binary classifiers betweenall pairs of the classes (Kre?el, 1998).
We useLibsvm (Lin et al, 2001) in our experiments.In our method, the parser considers the depend-ency attachment of two nodes (n,t).
The features ofa node are the word itself, the POS-tag and the in-formation of its child node(s).
The context featuresare 2 preceding nodes of node t (and t itself), 2 suc-ceeding nodes of node n (and n itself), and theirchild nodes.
The distance between nodes n and t isalso used as a feature.
The features are shown inFig.2.2.2   Label taggingWe adopt MaxEnt to estimate the label of depend-ency relations.
We have tried to use linear-chainconditional random fields (CRFs) for estimatingthe labels after the dependency relation analysis.This means that the parser first analyzes the worddependency (head-modifier relation) of the inputsentence, then the CRFs model analyzes the mostsuitable label set with the basic information of in-put sentence (FORM, LEMMA, POSTAG?
?etc)and the head information (FORM and POSTAG)of each word.
However, as the number of possiblelabels in some languages is large, training a CRFmodel with these corpora (we use CRF++ (Kudo,2005)) cost huge memory and time.Instead, we combine the maximum entropymethod in the word dependency analysis to tag thelabel of dependency relation.
As shown in Fig.
1,the parser first gets the contextual features to esti-mate the word dependency.
If the parsing operation1  To estimate the current operation (Left, Right, Shift andReduce) by SVMs, we need to build 6 classifiers(Left-Right,Left-Shift, Left-Reduce, Right-Shift, Right-Reduce and Shift-Reduce).192is ?Left?
or ?Right?, the parser then use MaxEntwith the same features to tag the label of relation.This strategy can tag the label according to the cur-rent states of the focused word pair.
We divide thetraining instances according to the CPOSTAG ofthe focused word n, so that a classifier is con-structed for each of distinct POS-tag of the word n.2.3 Preprocessing2.3.1   Preceding workIn our preceding work (Cheng, 2005a), we dis-cussed three problems of our basic methods (adoptNivre?s algorithm with SVMs) and proposed threepreprocessing methods to resolve these problems.The methods include: (1) using global features anda two-steps process to resolve the ambiguity be-tween the parsing operations ?Shift?
and ?Reduce?.
(2) using a root node finder and dividing the sen-tence at the root node to make use of the top-downinformation.
(3) extracting the prepositional phrase(PP) to resolve the problem of identifying theboundary of PP.We incorporated Nivre?s method with thesepreprocessing methods for Chinese dependencyanalysis with Penn Chinese Treebank and SinicaTreebank (Chen   et al, 2003).
This was effectivebecause of the properties of Chinese: First, there isno multi-root in Chinese Treebank.
Second, theboundary of prepositional phrases is ambiguous.We found that these methods do not always im-prove the accuracy of all the languages in theshared task.We have tried the method (1) in some lan-guages to see if there is any improvement in theparser.
We attempted to use global features andtwo-step analysis to resolve the ambiguity of theoperations.
In Chinese (Chen   et al, 2003) andDanish (Kromann, 2003), this method can improvethe parser performance.
However, in other lan-guages, such as Arabic (Haji?
et al, 2004), thismethod decreased the performance.
The reason isthat the sentence in some languages is too long touse global features.
In our preceding work, theglobal features include the information of all theun-analyzed words.
However, for analyzing longsentences, the global features usually include someuseless information and will confuse the two-stepprocess.
Therefore, we do not use this method inthis shared task.In the method (2), we construct an SVM-basedroot node finder to identify the root node and di-vided the sentence at the root node in the ChineseTreebank.
This method is based on the propertiesof dependency structures ?One and only one ele-ment is independent?
and ?An element cannot havemodifiers lying on the other side of its own head?.However, there are some languages that includemulti-root sentences, such as Arabic, Czech, andSpanish (Civit and Mart?, 2002), and it is difficultto divide the sentence at the roots.
In multi-rootsentences, deciding the head of the words betweenroots is difficult.
Therefore, we do not use themethod (2) in the share task.The method (3) ?namely PP chunker?
can iden-tify the boundary of PP in Chinese and resolve theambiguity of PP boundary, but we cannot guaran-tee that to identify the boundary of PP can improvethe parser in other languages.
Even we do not un-derstand construction of PP in all languages.Therefore, for the robustness in analyzing differentlanguages, we do not use this method.2.3.2   Neighboring dependency attachmenttaggerIn the bottom-up dependency parsing approach, thefeatures and the strategies for parsing in early stage(the dependency between adjacent2 words) is dif-ferent from parsing in upper stage (the dependencybetween phrases).
Parsing in upper stage needs theinformation at the phrases not at the words alone.The features and the strategies for parsing in earlyand upper stages should be separated into distinct.Therefore, we divide the neighboring dependencyattachment (for early stage) and normal depend-ency attachment (for upper stage), and set theneighboring dependency attachment tagger as apreprocessor.When the parser analyzes an input sentence, itextracts the neighboring dependency attachmentsfirst, then analyzes the sentence as described be-fore.
The results show that tagging the neighboringdependency word-pairs can improve 9 languagesout of 12 scoring languages, although in some lan-guages it degrades the performance a little.
Poten-tially, there may be a number of ways fordecomposing the parsing process, and the currentmethod is just the simplest decomposition of theprocess.
The best method of decomposition or dy-namic changing of parsing models should be inves-tigated as the future research.2 We extract all words that depend on the adjacent word (rightor left).1933 Experiment3.1 Experimental settingOur system consists of three parts; first, the SVM-based tagger extracts the neighboring attachmentrelations of the input sentence.
Second, the parseranalyzes further dependency attachments.
If a newdependency attachment is generated, the MaxEntbased tagger estimates the label of the relation.
Thethree parts of our parser are trained on the avail-able data of the languages.In our experiment, we used the full informationof each token (FORM, LEMMA, CPOSTAG,POSTAG, FEATS) when we train and test themodel.
Fig.
2 describes the features of each token.Some languages do not include all columns; suchthat the Chinese data does not include LEMMAand FEATURES, these empty columns are shownby the symbol ?-?
in Fig.
2.
The features for theneighboring dependency tagging are the informa-tion of the focused word, two preceding words andtwo succeeding words.
Fig.
2 shows the windowsize of our features for estimating the word de-pendency in the main procedures.
These featuresinclude the focused words (n, t), two precedingwords and two succeeding words and their children.The features for estimating the relation label arethe same as the features used for word dependencyanalysis.
For example, if the machine learner esti-mates the operation of this situation as ?Left?
or?Right?
by using the features in Fig.
2, the parseruses the same features in Fig.
2 and the depend-ency relation to estimate the label of this relation.For training the models efficiently, we dividedthe training instances of all languages at theCPOSTAG of the focused word n in Fig .2.
In ourpreceding work, we found this procedure can getbetter performance than training with all the in-stances at once.
However, only the instances inCzech are divided at the CPOSTAG of the focusedword-pair t-n3.
The performance of this procedureis worse than using the CPOSTAG of the focusedword n, because the training instances of eachCPOSTAG-pair will become scarce.
However, thedata size of Czech is much larger than other lan-guages; we couldn?t finish the training of Czechusing the CPOSTAG of the focused word n, beforethe deadline for submitting.
Therefore we used thisprocedure only for the experiment of Czech.3 For example, we have 15 SVM-models for Arabic accordingto the CPOSTAG of Arabic (A, C, D, F, G?etc.).
However,we have 139 SVM-models for Czech according theCPOSTAG pair of focused words (A-A, A-C, A-D?etc.
)All our experiments were run on a Linux ma-chine with XEON 2.4GHz and 4.0GB memory.The program is implemented in JAVA.3.2   ResultsTable 1 shows the results of our parser.
We do nottake into consideration the problem of cross rela-tion.
Although these cross relations are few intraining data, they would make our performanceworse in some languages.
We expect that this isone reason that the result of Dutch is not good.
Theaverage length of sentences and the size of trainingdata may have affected the performance of ourparser.
Sentences of Arabic are longer and trainingdata size of Arabic is smaller than other languages;therefore our parser is worse in Arabic.
Similarly,our result in Turkish is also not good because thedata size is small.We compare the result of Chinese with our pre-ceding work.
The score of this shared task is betterthan our preceding work.
It is expected that weselected the FORM and CPOSTAG of each nodesas features in the preceding work.
However, thePOSTAG is also a useful feature for Chinese, andwe grouped the original POS tags of Sinica Tree-bank from 303 to 54 in our preceding work.
Thenumber of CPOSTAG(54) in our preceding workis more than the number of CPOSTAG(22) in thisshared task, the training data of each CPOSTAG inour preceding work is smaller than in this work.Therefore the performance of our preceding workin Sinica Treebank is worse than this task.The last column of the Table 1 shows the unla-beled scores of our parser without the preprocess-ing.
Because our parser estimates the label after thedependency relation is generated.
We only con-sider whether the preprocessing can improve theunlabeled scores.
Although the preprocessing cannot improve some languages (such as Chinese,Spanish and Swedish), the average score showsthat using preprocessing is better than parsingwithout preprocessing.Comparing the gold standard data and the sys-tem output of Chinese, we find the CPOSTAGwith lowest accuracy is ?P (preposition)?, the accu-racy that both dependency and head are correct is71%.
As we described in our preceding work andSection 2.3, we found that boundaries of preposi-tional phrases are ambiguous for Chinese.
The bot-tom-up algorithm usually wrongly parses theprepositional phrase short.
The parser does notcapture the correct information of the children ofthe preposition.
According to the results, this prob-lem does not cause the accuracy of head of194CPOSTAG ?P?
decrease.
Actually, the head accu-racy of ?P?
is better than the CPOSTAG ?C?
or?V?.
However, the dep.
accuracy of ?P?
is worse.We should consider the properties of prepositionsin Chinese to resolve this question.
In Chinese,prepositions are derived from verbs; thereforesome prepositions can be used as a verb.
Naturally,the dependency relation of a preposition is differ-ent from that of a verb.
Important information fordistinguishing whether the preposition is a verb ora preposition is the information of the children ofthe preposition.
The real POS tag of a prepositionwhich includes few children is usually a verb; onthe other hand, the real POS tag of a preposition isusually a preposition.If our parser considers the preposition whichleads a short phrase, the parser will estimate therelation of the preposition as a verb.
At the sametime, if the boundary of prepositional phrase isanalyzed incorrectly, other succeeding words willbe wrongly analyzed, too.Error analysis of Japanese data (Kawata andBartels, 2000) shows that CNJ (Conjunction) is adifficult POS tag.
The parser does not have anymodule to detect coordinate structures.
(Kurohashi,1995) proposed a method in which coordinatestructure with punctuation is detected by a coeffi-cient of similarity.
Similar framework is necessaryfor solving the problem.Another characteristic error in Japanese is seenat adnominal dependency attachment for a com-pound noun.
In such dependency relations, adjec-tives and nouns with "no" (genitive marker) can bea dependent and compound nouns which consist ofmore than one consecutive nouns can be a head.The constituent of compound nouns have samePOSTAG, CPOSTAG and FEATS.
So, the ma-chine learner has to disambiguate the dependencyattachment with sparce feature LEMMA andFORM.
Compound noun analysis by semantic fea-ture is necessary for addressing the issue.4 ConclusionThis paper reported on multi-lingual dependencyparsing on combining SVMs and MaxEnt.
Thesystem uses SVMs for word dependency attach-ment analysis and MaxEnt for the label taggingwhen the new dependency attachment is generated.We discussed some preprocessing methods that areuseful in our preceding work for Chinese depend-ency analysis, but these methods, except one, can-not be used in multi-lingual dependency parsing.Only using the SVM-based tagger to extract theneighbor relation could improve many languagesin our experiment, therefore we use the tagger inthe parser as its preprocessing.ReferencesS.
Buchholz, E. Marsi, A. Dubey and Y. Krymolowski.
2006.CoNLL-X: Shared Task on Multilingual Dependency Pars-ing, CoNLL 2006.Yuchang Cheng, Masayuki Asahara and Yuji Matsumoto.2005a.
Chinese Deterministic Dependency Parser: Exam-ining Effects of Global Features and Root Node Finder,Fourth SIGHAN Workshop, pp.17-24.Yuchang Cheng, Masayuki Asahara and Yuji Matsumoto.2005b.
Machine Learning-based Dependency Parser forChinese, the International Conference on Chinese Comput-ing, pp.66-73.Ulrich.
H.-G. Kre?el, 1998.
Pairwise classification and sup-port vector machines.
In Advances in Kernel Methods, pp.255-268.
The MIT Press.Taku Kudo.
CRF++: Yet Another CRF toolkit,http://www.chasen.org/~taku/software/CRF++/.Sadao Kurohashi.
1995.
Analyzing Coordinate StructuresIncluding Punctuation in English, In IWPT-95, pp.
136-147.Chih Jen Lin, 2001.
A practical guide to support vector classi-fication, http://www.csie.ntu.edu.tw/~cjlin/libsvm/.Joakim Nivre, 2004.
Incrementality in Deterministic Depend-ency Parsing, In Incremental Parsing: Bringing Engineer-ing and Cognition Together.
Workshop at ACL-2004, pp.50-57.Adwait Ratnaparkhi, 1999.
Learning to parse natural lan-guage with maximum entropy models.
Machine Learning,34(1-3):151-175.Vladimir N. Vapnik, 1998.
Statistical Learning Theory.
AWiley-Interscience Publication.Language: LAS: UAS: LAcc.
UAS with out preprocessing:Arabic 65.19 77.74 79.02 76.74Chinese 84.27 89.46 86.42 90.03Czech 76.24 83.4 83.52 82.88Danish 81.72 88.64 86.11 88.45Dutch 71.77 75.49 75.83 74.97German 84.11 87.66 90.67 87.53Japanese 89.91 93.12 92.40 92.99Portugese 85.07 90.3 88.00 90.21Slovene 71.42 81.14 80.96 80.43Spanish 80.46 85.15 88.90 85.19Swedish 81.08 88.57 83.99 88.83Turkish 61.22 74.49 73.91 74.3AV: 77.7 84.6 84.1 84.38SD: 8.67 6.15 5.78 6.42Bulgarian 86.34 91.3 89.27 91.44Table 1: Results195
