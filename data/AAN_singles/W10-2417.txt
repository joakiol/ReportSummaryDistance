Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 110?115,Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational LinguisticsSimplified Feature Set for Arabic Named Entity RecognitionAhmed Abdul-Hamid, Kareem DarwishCairo Microsoft Innovation CenterCairo, Egypt{ahmedab,kareemd}@microsoft.comAbstractThis paper introduces simplified yet effectivefeatures that can robustly identify named enti-ties in Arabic text without the need for mor-phological or syntactic analysis or gazetteers.A CRF sequence labeling model is trained onfeatures that primarily use character n-gram ofleading and trailing letters in words and wordn-grams.
The proposed features help over-come some of the morphological and ortho-graphic complexities of Arabic.
In comparingto results in the literature using Arabic specificfeatures such POS tags on the same datasetand same CRF implementation, the results inthis paper are lower by 2 F-measure points forlocations, but are better by 8 points for organi-zations and 9 points for persons.1 IntroductionNamed entity recognition (NER) continues to bean important part of many NLP applications suchas information extraction, machine translation,and question answering (Benajiba et al, 2008).NER is concerned with identifying sequences ofwords referring to named entities (NE?s) such aspersons, locations, and organizations.
For exam-ple, in the word sequence ?Alan Mulally, CEO ofDetroit based Ford Motor Company,?
Alan Mu-lally, Detroit, and Ford Motor Company wouldbe identified as a person, a location, and an or-ganization respectively.Arabic is a Semitic language that present inter-esting morphological and orthographic challeng-es that may complicate NER.
Some of thesechallenges include:?
Coordinating conjunctions, prepositions,possessive pronouns, and determiners aretypically attached to words as prefixes orsuffixes.?
Proper names are often common languagewords.
For example, the proper name?Iman?
also means faith.?
Lack capitalization of proper nouns.The paper introduces a simplified set of featuresthat can robustly identify NER for Arabic with-out the need for morphological or syntactic anal-ysis.
The proposed features include: word lead-ing and trailing character n-gram features thathelp handle prefix and suffix attachment; wordn-gram probability based features that attempt tocapture the distribution of NE?s in text; wordsequence features; and word length.The contributions of this paper are as follows:1.
Identifying simplified features that work wellfor Arabic without gazetteers and withoutmorphological and syntactic features, leadingto improvements over previously reported re-sults.2.
Using leading and trailing character n-gramsin words, which help capture valuable mor-phological and orthographic clues that wouldindicate or counter-indicate the presence ofNE?s.3.
Incorporating word language modeling basedfeatures to capture word associations and rela-tive distribution of named entities in text.Conditional Random Fields (CRF) sequence la-beling was used in identifying NE?s, and the ex-periments were performed on two standard Ara-bic NER datasets.The rest of the paper is organized as follows:Section 2 surveys prior work on Arabic NER;Section 3 introduces the proposed features andmotivates their use; Section 4 describes experi-mental setup and evaluation sets; Section 5 re-ports on experimental results; and Section 6 con-cludes the paper.2 BackgroundMuch work has been done on NER with multipleevaluation forums dedicated to information ex-traction in general and to NER in specific.Nadeau and Sekine (2009) surveyed lots of workon NER for a variety of languages and using amyriad of techniques.
Significant work has beenconducted by Benajiba and colleagues on ArabicNER (Benajiba and Rosso, 2008; Benajiba et al,2008; Benajiba and Rosso, 2007; Benajiba et al,1102007).
Benajiba et al (2007) used a maximumentropy based classification trained on a featureset that include the use of gazetteers and a stop-word list, appearance of a NE in the training set,leading and trailing word bigrams, and the tag ofthe previous word.
They reported 80%, 37%,and 47% F-measure for locations, organizations,and persons respectively.
Benajiba and Rosso(2007) improved their system by incorporatingPOS tags to improve NE boundary detection.They reported 87%, 46%, and 52% F-measurefor locations, organizations, and persons respec-tively.
Benajiba and Rosso (2008) used CRFsequence labeling and incorporated many lan-guage specific features, namely POS tagging,base-phrase chunking, Arabic tokenization, andadjectives indicating nationality.
They reportedthat tokenization generally improved recall.
Us-ing POS tagging generally improved recall at theexpense of precision, leading to overall im-provement in F-measure.
Using all their sug-gested features they reported 90%, 66%, and73% F-measure for location, organization, andpersons respectively.
In Benajiba et al (2008),they examined the same feature set on the Auto-matic Content Extraction (ACE) datasets usingCRF sequence labeling and Support Vector Ma-chine (SVM) classifier.
They did not report percategory F-measure, but they reported overall81%, 75%, and 78% macro-average F-measurefor broadcast news and newswire on the ACE2003, 2004, and 2005 datasets respectively.Huang (2005) used an HMM based NE recog-nizer for Arabic and reported 77% F-measure onthe ACE 2003 dataset.
Farber et al (2008) usedPOS tags obtained from an Arabic morphologicalanalyzer to enhance NER.
They reported 70% F-measure on the ACE 2005 dataset.
Shaalan andRaza (2007) reported on a rule-based system thatuses hand crafted grammars and regular expres-sions in conjunction with gazetteers.
They re-ported upwards of 93% F-measure, but they con-ducted their experiments on non-standard da-tasets, making comparison difficult.McNamee and Mayfield (2002) explored thetraining of an SVM classifier using many lan-guage independent binary features such as lead-ing and trailing letters in a word, word length,presence of digits in a word, and capitalization.They reported promising results for Spanish andDutch.
In follow on work, Mayfield et al (2003)used thousands of language independent featuressuch character n-grams, capitalization, wordlength, and position in a sentence, along withlanguage dependent features such as POS tagsand BP chunking.
For English, they reported89%, 79%, and 91% F-measure for location, or-ganization, and persons respectively.The use of CRF sequence labeling has beenincreasing over the past few years (McCallumand Li, 2003; Nadeau and Sekine, 2009) withgood success (Benajiba and Rosso, 2008).Though, CRF?s are not guaranteed to be betterthan SVM?s (Benajiba et al, 2008).3 NER FeaturesFor this work, a CRF sequence labeling wasused.
The advantage of using CRF is that theycombine HMM-like generative power with clas-sifier-like discrimination (Lafferty et al, 2001;Sha and Pereira, 2003).
When a CRF makes adecision on the label to assign to a word, it alsoaccounts for the previous and succeeding words.The CRF was trained on a large set of surfacefeatures to minimize the use of Arabic morpho-logical and syntactic features.
Apart from stem-ming two coordinating conjunctions, no otherArabic specific features were used.The features used were as follows:?
Leading and trailing character bigrams (6bi).For a given word composed of the letter se-quence, where    and    are a start andend word markers respectively, the first threebigrams (,, and) and last three bi-grams (,, and) were used asfeatures.
Using leading and trailing charac-ter bigrams of a word was an attempt to ac-count for morphological and orthographiccomplexities of Arabic and to capture sur-face clues that would indicate the presence ofa NE or not.
For example, plural forms ofcommon words in Arabic are often obtainedby attaching the suffixes wn1 (??)
or yn (??
)for masculine nouns and At (??)
for femininenouns.
Presence of such plural form markerswould generally indicate a plural noun, butwould counter-indicate a NE.
Also, verbs inpresent tense start with the letters A (?
), t (?
),y (?
), and n (?).
These would contribute toconcluding that a word may not be a NE.Further, coordinate conjunctions, such as f(?)
and w (?
), and prepositions, such as b(?
), k (?
), and l (?
), composed of single let-ters are often attached as prefixes to words.Accounting for them may help overcomesome of the problems associated with not1 Arabic letters are presented using the Buckwaltertransliteration scheme111stemming.
Further, the determiner Al (??
)may be a good indicator for proper nounsparticularly in the case of organizations.This would be captured by the second bi-gram from the head of the word.
If the de-terminer is preceded by a coordinating con-junction, the third bigram from the head ofthe word would be able to capture this fea-ture.?
Leading and trailing character trigrams(6tri).
For a given word composed of theletter sequence, where    and    are a startand end word markers respectively, the firstthree trigrams (,, and) and last threetrigrams (,, and) were used asfeatures.
The rationale for using these fea-tures is very similar to that of using characterbigrams.
The added value of using charactertrigrams, is that they would allow for thecapture of combinations of prefixes and suf-fixes.
For example, a word may begin withthe prefixes w+Al (???
), which are a coordi-nating conjunction and determiner respec-tively.?
Leading and trailing character 4-grams(6quad).
For a given word composed of theletter sequence, where    and    are a startand end word markers respectively, the firstthree 4 grams (,, and) and last three 4grams (,, and) were used asfeatures.
Similar to leading and trailing tri-grams, these features can capture combina-tions of prefixes and suffixes.?
Word position (WP).
The feature capturesthe relative position of a word in a sentenceas follows:Typically, Arabic is a VSO language.
Thus,NE?s in specific and nouns in general do notstart sentences.?
Word length (WL).
The feature captures thelength of named entities, as some NE?s, par-ticularly transliterated NE?s, may be longerthan regular words.?
Word unigram probability (1gP).
This issimply the unigram probability of word.
Ac-counting for unigram probability would helpexclude common words.
Also, named enti-ties are often out-of-vocabulary words.?
Word with previous and word with succeed-ing word-unigram ratio (1gPr).
Given aword wi, these two features are computed as:(  )(    )(    )(  )This feature would potentially capture majorshifts between word probabilities.
For ex-ample, a named entity is likely to have muchlower probability compared to the word be-fore it and the word after it.?
Features that account for dependence be-tween words in a named entity.
PopularNE?s are likely collocations, and words thatmake up named entities don?t occur next toeach other by chance.
These features are asfollows:o Word with previous and word with succeed-ing word bigram (2gP).
For a given word wi,the two bigram probabilities are p(wi-1wi) andp(wiwi+1).
Words composing named entitiesare likely conditionally dependent.o t-test between a word and the word that pre-cedes and succeeds it (T).
Given a word se-quence wi and wi+1:?
?Wher ?
(      ),   (  )   (    ) ,?, and N is the number of words in thecorpus (Manning and Schutze, 1999).o Mutual information between a word and theword that precedes and succeeds it (MI).Given a word sequence wi and wi+1:[?]
, where  ?
and   are identicalto those in the t-test.?
Character n-gram probability (3gCLM).Given character trigram language models forlocations, persons, organizations, and non-NE?s, the four features are just the characterlanguage model probabilities using the fourdifferent language models.
The motivationfor these features stem from the likelihoodthat NE?s may have a different distributionof characters particularly for person names.This stems from the fact that many NE?s aretransliterated names.4 Experimental Setup4.1 DatasetsFor this work, the NE?s of interest were persons,locations, and organizations only.
Two datasetswere used for the work in this paper.
The first112was a NE tagged dataset developed by Binajibaet al (2007).
The Binajiba dataset is composedof newswire articles totaling more than 150,000words.
The number of different NE?s in the col-lection are:Locations (LOC)  878Organizations (ORG)  342Persons (PER)   689The second was the Arabic Automatic ContentExtraction (ACE) 2005 dataset.
The ACE da-taset is composed of newswire, broadcast news,and weblogs.
For experiments in this work, theweblogs portion of the ACE collection was ex-cluded, because weblogs often include colloquialArabic that does not conform to modern standardArabic.
Also, ACE tags contain many sub-categories.
For example, locations are tagged asregions, bodies of water, states, etc.
All sub-tagswere ignored and were conflated to the base tags(LOC, ORG, PER).
Further, out of the 40 sub-entity types, entities belonging to the following13 ACE sub-entity types were excluded becausethey require anaphora resolution or they refer tonon-specific NE?s: nominal, pronominal, kind ofentity (as opposed to a specific entity), negative-ly quantified entity, underspecified entity, ad-dress, boundary (eg.
border), celestial object(comet), entertainment venue (eg.
movie theater),sport (eg.
football), indeterminate (eg.
human),vehicle, and weapon.
The total number of wordsin the collection is 98,530 words (66,590 fromnewswire and 31,940 from broadcast news).
Thenumber of NE?s is as follows:Locations (LOC)  867Organizations (ORG)  269Persons (PER)   524Since both collections do not follow the sametagging conventions, training and testing wereconducted separately for each collection.
Eachcollection was 80/20 split for training and test-ing.4.2 Data Processing and Sequence LabelingTraining and testing were done using CRF++which is a CRF sequence label toolkit.
The fol-lowing processing steps of Arabic were per-formed:?
The coordinating conjunctions w (?)
and f(?
), which always appear as the first prefix-es in a word, were optionally stemmed.
wand f were stemmed using an in-house Ara-bic stemmer that is a reimplementation of thestemmer proposed by Lee et al (2003).However, stemming w or f could have beendone by stemming the w or f and searchingfor the stemmed word in a large Arabic cor-pus.
If the stemmed word appears more thana certain count, then stemming was appropri-ate.?
The different forms of alef (A (?
), | (?
), > (?
),and < (?))
were normalized to A (?
), y (?)
andY (?)
were normalized to y (?
), and p (?)
wasmapped to h (??
).4.3 EvaluationThe figures of merit for evaluation were preci-sion, recall, and F-measure (?
= 1), with evalua-tion being conducted at the phrase level.
Report-ing experiments with all the different combina-tions of features would adversely affect the read-ability of the paper.
Thus, to ascertain the con-tribution of the different features, a set of 15 ex-periments are being reported for both datasets.The experiments were conducted using raw Ara-bic words (3w) and stems (3s).
Using the shortnames of features (bolded after feature names insection 3), the experiments were as follows:?
3w?
3w_6bi?
3w_6bi_6tri?
3w_6bi_6tri_6quad?
3w_6bi_6tri_6quad_WL?
3w_6bi_6tri_6quad_WP?
3s?
3s_6bi_6tri_6quad?
3s_6bi_6tri_6quad_1gP?
3s_6bi_6tri_6quad_1gPr_1gP?
3s_6bi_6tri_6quad_2gP?
3s_6bi_6tri_6quad_3gCLM?
3s_6bi_6tri_6quad_MI?
3s_6bi_6tri_6quad_T?
3s_6bi_6tri_6quad_T_MI5 Experimental ResultsTable 1 lists the results for the Benajiba andACE datasets respectively.
Tables 2 and 3 reportthe best obtained results for both datasets.
Theresults include precision (P), recall (R), and F-measure (F) for NE?s of types location (LOC),organization (ORG), and person (PER).
The bestresults for P, R, and F are bolded in the tables.In comparing the base experiments 3w and 3s inwhich the only the surface forms and the stemswere used respectively, both produced the high-est precision.
However, 3s improved recall over3w by 7, 13, and 14 points for LOC, ORG, andPER respectively on the Benajiba dataset.Though using 3s led to a drop in P for ORG113compared to 3w, it actually led to improvementin P for PER.
Similar results were observed forthe ACE dataset, but the differences were lesspronounced with 1% to 2% improvements in re-call.
However, when including the 6bi, 6tri, and6quad features the difference between usingwords or stems dropped to about 1 point in recalland nearly no difference in precision.
Thiswould indicate the effectiveness of using leadingand trailing character n-grams in overcomingmorphological and orthographic complexities.Benajiba ACERun Name Type P R F P R F3wLOC 96 59 73 88 59 71ORG 92 36 51 87 50 63PER 90 32 48 94 47 633w_6biLOC 92 75 82 85 72 78ORG 83 57 67 76 54 63PER 87 68 76 89 70 783w_6bi_6triLOC 93 79 86 87 77 82ORG 82 61 70 77 56 65PER 89 72 80 89 73 803w_6bi_6tri_6quadLOC 93 83 87 87 77 81ORG 84 64 72 77 55 65PER 90 73 81 92 71 803w_6bi_6tri_6quad_WLLOC 93 82 87 87 78 82ORG 83 64 73 79 56 65PER 89 73 80 93 71 813w_6bi_6tri_6quad_WPLOC 91 82 86 88 77 82ORG 83 62 71 77 59 67PER 89 74 81 91 70 793sLOC 96 66 78 89 60 72ORG 88 49 63 86 52 65PER 93 46 61 92 49 643s_6bi_6tri_6quadLOC 93 83 88 87 77 82ORG 84 63 72 78 58 67PER 90 74 81 91 70 803s_6bi_6tri_6quad_1gPLOC 93 83 88 87 77 82ORG 84 64 73 79 57 66PER 90 75 82 93 70 803s_6bi_6tri_6quad_1gPr_1gPLOC 93 81 87 87 77 81ORG 85 60 70 82 55 66PER 91 72 81 93 69 793s_6bi_6tri_6quad_2gPLOC 93 81 87 88 77 82ORG 85 61 71 82 56 67PER 89 74 81 90 69 783s_6bi_6tri_6quad_3gCLMLOC 93 82 87 87 76 81ORG 84 65 74 78 56 66PER 90 74 81 93 71 813s_6bi_6tri_6quad_MILOC 93 81 86 87 77 82ORG 84 59 69 82 56 66PER 90 72 80 93 70 803s_6bi_6tri_6quad_TLOC 93 81 87 87 76 81ORG 85 61 71 82 55 66PER 90 72 80 93 69 793s_6bi_6tri_6quad_T_MILOC 93 80 86 87 76 81ORG 85 57 68 82 54 65PER 91 71 80 93 67 78Table 1: NER results for the Benajiba andACE datasetsP R FLOC 93 83 88ORG 84 64 73PERS 90 75 82Avg.
89 74 81Table 2:  Best results on Benajiba dataset(Run name: 3s_6bi_6tri_6quad_1gP)P R FLOC 87 77 82ORG 79 56 65PERS 93 71 81Avg.
88 70 76Table 3:  Best results on ACE dataset(Run name: 3w_6bi_6tri_6quad_WL)P R FLOC 93 87 90ORG 84 54 66PERS 80 67 73Avg.
86 69 76Table 4:  The results in (Benajiba and Rosso,2008) on Benajiba datasetThe 3s_6bi_6tri_6quad run produced nearly thebest F-measure for both datasets, with extra fea-tures improving overall F-measure by at most 1point.Using t-test T and mutual information MI didnot yield any improvement in either recall orprecision, and often hurt overall F-measure.
Ashighlighted in the results, the 1gP, 2gP, WL, WP,and 3gCLM typically improved recall slightly,often leading to 1 point improvement in overallF-measure.To compare to results in the literature, Table 4reports the results obtained by Benajiba and Ros-so (2008) on the Benajiba dataset using theCRF++ implementation of CRF sequence label-ing trained on a variety of Arabic language spe-cific features.
The comparison was not done ontheir results on the ACE 2005 dataset due to po-tential difference in tags.
The averages in Tables2, 3, and 4 are macro-averages as opposed to mi-cro-averages reported by Benajiba and Rosso(2008).
In comparing Tables 2 and 4, the fea-tures suggested in this paper reduced F-measurefor locations by 2 points, but improved F-measure for organizations and persons by 8points and 9 points respectively, due to im-provements in both precision and recall.114The notable part of this work is that using a sim-plified feature set outperforms linguistic features.As explained in Section 3, using leading andtrailing character n-grams implicitly capturemorphological and syntactic features that typical-ly used for Arabic lemmatization and POS tag-ging (Diab, 2009).
The improvement over usinglinguistic features could possibly be attributed tothe following reasons:  not all prefixes and suf-fixes types equally help in identifying namedentities (ex.
appearance of a definite article ornot); not all prefixes and suffix surface formsequally help (ex.
appearance of the coordinatingconjunction w ???
vs. f ???
); and mistakes instemming and POS tagging.
The lag in recall forlocations behind the work of Benajiba and Rosso(2008) could be due to the absence of locationgazetteers.6 Conclusion and Future WorkThis paper presented a set of simplified yet effec-tive features for named entity recognition in Ar-abic.
The features helped overcome some of themorphological and orthographic complexities ofArabic.
The features included the leading andtrailing character n-grams in words, word associ-ation features such as t-test, mutual information,and word n-grams, and surface features suchword length and relative word position in a sen-tence.
The most important features were leadingand trailing character n-grams in words.
Theproposed feature set yielded improved resultsover those in the literature with as much as 9point F-measure improvement for recognizingpersons.For future work, the authors would like to exam-ine the effectiveness of the proposed feature seton other morphologically complex languages,particularly Semitic languages.
Also, it is worthexamining the combination of the proposed fea-tures with morphological features.ReferencesY.
Benajiba, M. Diab, and P. Rosso.
2008.
ArabicNamed Entity Recognition using Optimized Fea-ture Sets.
Proceedings of the 2008 Conference onEmpirical Methods in Natural Language Pro-cessing, pages 284?293, Honolulu, October 2008.Y.
Benajiba and P. Rosso.
2008.
Arabic Named EntityRecognition using Conditional Random Fields.
InProc.
of Workshop on HLT & NLP within the Ar-abic World, LREC?08.Y.
Benajiba, P. Rosso and J. M. Bened?.
2007.
AN-ERsys: An Arabic Named Entity Recognition sys-tem based on Maximum Entropy.
In Proc.
of CI-CLing-2007, Springer-Verlag, LNCS(4394), pp.143-153.Y.
Benajiba and P. Rosso.
2007.
ANERsys 2.0: Con-quering the NER task for the Arabic language bycombining the Maximum Entropy with POS-tag in-formation.
In Proc.
of Workshop on Natural Lan-guage-Independent Engineering, IICAI-2007.M.
Diab.
2009.
Second Generation Tools (AMIRA2.0): Fast and Robust Tokenization, POS tagging,and Base Phrase Chunking.
Proceedings of the Se-cond International Conference on Arabic LanguageResources and Tools, 2009.B.
Farber, D. Freitag, N. Habash, and O. Rambow.2008.
Improving NER in Arabic Using a Morpho-logical Tagger.
In Proc.
of LREC?08.F.
Huang.
2005.
Multilingual Named Entity Extrac-tion and Translation from Text and Speech.
Ph.D.Thesis.
Pittsburgh: Carnegie Mellon University.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models forsegmenting and labeling sequence data, In Proc.
ofICML, pp.282-289, 2001.Young-Suk Lee, Kishore Papineni, Salim Roukos,Ossama Emam, Hany Hassan.
2003.
LanguageModel Based Arabic Word Segmentation.
ACL2003: 399-406C.
Manning and H. Schutze.
1999.
Foundations ofStatistical Natural Language Processing.
Cam-bridge, Massachusetts: The MIT Press.J.
Mayfield, P. McNamee, and C. Piatko.
2003.Named Entity Recognition using Hundreds ofThousands of Features.
HLT-NAACL 2003-Volume 4, 2003.A.
McCallum and W. Li.
2003.
Early Results forNamed Entity Recognition with Conditional Ran-dom Fields, Features Induction and Web-Enhanced Lexicons.
In Proc.
Conference on Com-putational Natural Language Learning.P.
McNamee and J. Mayfield.
2002.
Entity extractionwithout language-specific.
Proceedings of CoNLL,2002.D.
Nadeau and S. Sekine.
2009.
A survey of namedentity recognition and classification.
Named enti-ties: recognition, classification and use, ed.
S.Sekine and E. Ranchhod, John Benjamins Publish-ing Company.F.
Sha and F. Pereira.
2003.
Shallow parsing withconditional random fields, In Proc.
ofHLT/NAACL-2003.K.
Shaalan and H. Raza.
2007.
Person Name EntityRecognition for Arabic.
Proceedings of the 5thWorkshop on Important Unresolved Matters, pages17?24, Prague, Czech Republic, June 2007.115
