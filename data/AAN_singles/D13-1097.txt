Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 946?957,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsDiscourse Level Explanatory Relation Extraction from Product ReviewsUsing First-order LogicQi Zhang, Jin Qian, Huan Chen, Jihua Kang, Xuanjing HuangSchool of Computer ScienceFudan UniversityShanghai, P.R.
China{qz, 12110240030, 12210240054, 12210240059, xjhuang}@fudan.edu.cnAbstractExplanatory sentences are employed to clarifyreasons, details, facts, and so on.
High qualityonline product reviews usually include notonly positive or negative opinions, but also avariety of explanations of why these opinionswere given.
These explanations can helpreaders get easily comprehensible informa-tion of the discussed products and aspect-s.
Moreover, explanatory relations can alsobenefit sentiment analysis applications.
Inthis work, we focus on the task of identi-fying subjective text segments and extractingtheir corresponding explanations from prod-uct reviews in discourse level.
We proposea novel joint extraction method using first-order logic to model rich linguistic featuresand long distance constraints.
Experimentalresults demonstrate the effectiveness of theproposed method.1 IntroductionThrough analyzing product reviews with high help-fulness ratings assigned by readers, we find that alarge number of explanatory sentences are used toclarify the causes, details, or consequences of opin-ions.
According to the statistic based on the datasetwe crawled from a popular product review website,more than 56.1% opinion expressions are furtherexplained by other sentences.
Since most consumersare not experts, these explanations would bring lotsof helpful and easy comprehension information forthem.
Suggestions about writing a product reviewalso advise authors to include not only whether theylike or dislike a product, but also why.11http://www.reviewpips.com/http://www.amazon.com/gp/community-help/customer-For example, let us consider the following snip-pets extracted from online reviews:Example 1: TVs with lower refresh rates maysuffer from motion blur.
If you?re watchinga fast-paced football game, for example, youmay notice a bit of blurring as the players runaround the field.Example 2: The LED screen is highly reflec-tive.
The reflection of my own face makes it veryhard to see the subject I am trying to shoot.The first sentence of example 1 expresses negativeopinion about refresh rate, which is one of the mostimportant attributes of TV.
The second sentencedescribes the consequence of it through an example.In example 2, detail descriptions are used to explainthe reflection problem of the camera screen.Although, explanations provide valuable infor-mation, to the best of our knowledge, there is noexisting work that deals with explanation extractionfor opinions in discourse level.
We think that ifexplanatory relations can be automatically identifiedfrom reviews, sentiment analysis applications maybenefit from it.
Existing opinion mining approachesmainly focus on subjective text.
They try to de-termine the subjectivity and polarity of fragmentsof documents (e.g.
a paragraph, a sentence, aphrase and a word) (Pang et al 2002; Riloff etal., 2003; Takamura et al 2005; Mihalcea et al2007; Dasgupta and Ng, ; Hassan and Radev, 2010;Meng et al 2012; Dragut et al 2012).
Fine-grainedmethods were also introduced to extract opinionholder, opinion expression, opinion target, and otheropinion elements (Kobayashi et al 2007; Wu et alreviews-guidelines9462011; Xu et al 2013; Yang and Cardie, 2013).
Ma-jor research directions and challenges of sentimentanalysis can also be found in surveys (Pang and Lee,2008; Liu, 2012).In this work, we aim to identify subjective tex-t segments and extract their corresponding expla-nations from product reviews in discourse level.We propose to use Markov Logic Networks (ML-N) (Richardson and Domingos, 2006) to learn thejoint model for subjective classification and explana-tory relation extraction.
MLN has been applied inseveral natural language processing tasks (Singlaand Domingos, 2006; Poon and Domingos, 2008;Yoshikawa et al 2009; Andrzejewski et al 2011;Song et al 2012) and demonstrated its advantages.It can easily incorporate rich linguistic features andglobal constraints by designing various logic for-mulas, which can also be viewed as templates orrules.
Logic formulas are combined in a proba-bilistic framework to model soft constraints.
Hence,the proposed approach can benefit a lot from thisframework.To evaluate the proposed method, we crawled alarge number of product reviews and constructed alabeled corpus through Amazon?s Mechanical Turk.Two tasks were deployed for labeling the corpus.We compared the proposed method with state-of-the-art methods on the dataset.
Experimental resultsdemonstrate that the proposed approach can achievebetter performance than state-of-the-art methods.The remaining part of this paper is organized asfollows: In Section 2, we define the problem andgive some examples to show the challenges of thistask.
Section 3 describes the proposed MLN basedmethod.
Dataset construction, experimental resultsand analyses are given in Section 4.
In Section 5, wepresent the related work and Section 6 concludes thepaper.2 Problem StatementMotivated by the argument structure of discourserelations used in Penn Discourse Treebank (Rash-mi Prasad and Webber, 2008), in this work, weadopt the clause unit-based definition.
It means thatclauses are treated as the basic units of opinion ex-pressions and explanations.
Let d = {c1, c2, ...cn}be the clauses of document d. Directed graphG = (V,E) is used to represent the subjectivityof clauses and explanatory relationships betweenthem.
In the graph, vertices represent clauses,whose categories are specified by the vertex at-tributes.
Directed edges describe the explanatoryrelationships between them, of which the heads areexplanatory clauses.
If clause ca describes a set offacts which clarify the causes, context, situation, orconsequences of another clause cb, ca ??
cb is usedto indicate that clause ca explains cb.Adopting clause unit-based definition is basedon the following reasons: 1) clause is normallyconsidered as the smallest grammatical unit whichcan express a complete proposition (Kroeger, 2005);2) from analyzing online reviews, we observe thata clause can express a complete opinion about oneaspect in most of cases; 3) in Penn DiscourseTreebank, the basic unit of discourse relations (witha few exceptions) is also taken to be a clause (Rash-mi Prasad and Webber, 2008).Figure 1(a) illustrates a sample document.
Figure1(b) is the corresponding output of the given docu-ment.
In the graph, vertices whose color are blackstand for subjective clauses.
The other clauses arerepresented by white vertices.
Edges describe theexplanatory relationships between them, of whichthe heads are explanatory clauses.Although the explanatory relation extraction taskhas been studied from the view of linguistic anddiscourse representation by existing works (Carston,1993; Lascarides and Asher, 1993), the automaticextraction task is still an open question.
Consider thefollowing examples extracting from online reviews:Example 3: It takes great pictures.
Color ren-ditions, skin tones, exposure levels are all first rate.From the example, we can observe that the secondsentence explains the first one.
However, the secondsentence itself also expresses opinion on variousopinion targets.
In other words, both subjective andobjective sentences can be used as explanations.Example 4: When we called their service centerthey made us wait for them the whole day and noone turned up.
This level of service is simply notacceptable.
The first sentence in example 4 explainsthe second one.
Hence, the feature of relativelocation between two sentences does not alwayswork well in all cases.Example 5: This backpack is great!
its very big947(c1) I have both the Panasonic LX3 and the CanonS90.
(c2) Both cameras are quite different but trulyexcellent.
(c3) The S90 is a true pocket camera.
(c4) It is very compact.
(c5) The build quality isalso top notch.
(c6) It feels solid and it is easy togrip.
(c7) It is so small and convenient, (c8) youwill find that you will always carry it with you.C5C3C2C4C6C7C1(a) Example Review (b) Directed Graph RepresentationC8Figure 1: Directed graph representation of a sample document.and fits more than enough stuff.
Many sentences,which express explanatory relation, do not containany connectives (e.g.
?because?, ?the reason is?,and so on).
Lin et al2009) generalized four chal-lenges (include ambiguity, inference, context, andworld knowledge) to automated implicit discourserelation recognition.
In this task, we also need toaddress those challenges.From the these examples, we can observe that ex-tracting explanatory relations from product reviewsis a challenging task.
Both linguistic and globalconstraints should be carefully studied.3 The Proposed ApproachIn this section, we present our method for jointlyclassifying the subjectivity of text segments andextracting explanatory relations.
Firstly, we brieflydescribe the framework of Markov Logic Networks.Then, we introduce the clause extraction methodbased on the definition described in the Section 2.Finally, we present the first-order logic formulasincluding local formulas and global formulas usedfor joint modeling in this work.3.1 Markov Logic NetworksA MLN consists of a set of logic formulas thatdescribe first-order knowledge base.
Each formulaconsists of a set of first-order predicates, logicalconnectors and variables.
Different with first-orderlogic, these hard logic formulas are softened andcan be violated with some penalty (the weight offormula) in MLN.We use M to represent a MLN and {(?i, wi)}to represent formula ?i and its weight wi.
Theseweighted formulas define a probability distributionover sets of possible worlds.
Let y denote a possibleworld, the p(y) is defined as follows (Richardsonand Domingos, 2006):p(y) = 1Zexp???
(?i,wi)?Mwi?c?Cn?if?ic (y)??
,where each c is a binding of free variable in ?i toconstraints; f?ic (y) is a binary feature function thatreturns 1 if the true value is obtained in the groundformula we get by replacing the free variables in?i with the constants in c under the given possibleworld y, and 0 otherwise; Cn?i is all possiblebindings of variables to constants, and Z is a nor-malization constant.Many methods have been proposed to learn theweights of MLN using both generative and dis-criminative approaches (Richardson and Domingos,2006; Singla and Domingos, 2006).
There arealso several MLN learning packages available onlinesuch as thebeast2, Tuffy3, PyMLNs4, Alchemy5, andso on.2http://code.google.com/p/thebeast3http://hazy.cs.wisc.edu/hazy/tuffy/4http://www9-old.in.tum.de/people/jain/mlns/5http://alchemy.cs.washington.edu/948Describing the attributes of wordssubjLexicon(w) The word w belongs to the subjective lexicon (Baccianella et al2010).relationLexicon(w) The word w belongs to the lexicon of explanation relationconnectives (Pitler and Nenkova, 2009).Describing the attributes of the clause ciword(i, w) The clause ci has word w.firstWord(i, w) The first word of clause ci is word w.pos(i, w, t) The POS tag of word w is t in clause ci.dep(i, h,m) Word m and h are governor and dependent of a dependencyrelation in clause ci.Describing the attributes of relations between clause ci and clause cjclauseDistance(i, j,m) Distance between clause ci and clause cj in clauses is m.sentenceDistance(i, j, n) Distance between clause ci and clause cj in sentences is n.Table 1: Descriptions of observed predicates.3.2 Clause IdentificationWe model the clause boundary identification prob-lem through sequence labeling and use ConditionalRandom Fields (CRFs) to identify clause bound-aries.
Words and part-of-speech (POS) tags are usedas feature sets.
Since we do not allow embeddedsegments, the performance of our method is promis-ing, which achieves the F1 score of 92.8%.
Theresult is comparable with the best results obtainedduring the CoNLL-2001 campaign (Tjong et al2001).3.3 FormulasIn this work, we propose to use predicate subj(i)to indicate that the ith clause is subjective andexplain(i, j) to indicate that the jth clause explainsthe ith clause.
Both subj and explain are hiddenpredicates and jointly modeled by MLN.
We uselocal and global formulas to model rich linguisticfeatures and long distance constraints.3.3.1 Local FormulasThe local formulas relate one or more observedpredicates to exactly one hidden predicate.
In thiswork, we define a list of observed predicates todescribe the properties of individual clauses andattributes of relations between two clauses.
Theobserved predicates and descriptions are shown inTable 1.
The observed predicates can be categorizedinto 3 groups: words, clauses, and relations betweenclauses.
We use two lexicons to capture backgroundknowledge of words.
Lexical, part-of-speech tag,and dependency relation are used to describe a singleclause.
We also propose two predicates to modeldistance between clauses.Table 2 lists the local formulas used in this work.The ?+?
notation in the formulas indicates that eachconstant of the logic variable should be weightedseparately.
For subjective classification and relationextraction, we construct a number of formulas re-spectively.For subjective classification, the first two formu-las model the influence of lexical and POS tag.
Itis similar as the bag-of-words model, which is asimplifying representation and has been successfullyused for various natural language processing tasks.Since words which provide positive or negativeopinions may provide important information forsubjectivity classification, we combine predicates ofwords and lexicon of opinion words.
Bigrams arealso proved to be useful for textual classification inseveral NLP tasks.
Hence, we also combine predi-cates about individual word and POS tag to capturethis kind of information.
Word-level relations areexplicitly presented at the dependency trees, we949Formulas for subjective classificationword(i,w+)?
subj(i)pos(i,w+,t+)?
subj(i)word(i,w+) ?
subjLexicon(w)?
subj(i)pos(i,w+,t+) ?
subjLexicon(w)?
subj(i)word(i,w1+) ?
word(i,w2+)?
subj(i)pos(i,w1+,t+) ?
pos(i,w2+,t+)?
subj(i)word(i,w1+) ?
word(i,w2+) ?
subjLexicon(w1)?
subj(i)word(i,w1+) ?
word(i,w2+) ?
subjLexicon(w2)?
subj(i)dep(i,w1+,w2+)?
subj(i)dep(i,w1+,w2+) ?
subjLexicon(w1)?
subj(i)dep(i,w1+,w2+) ?
subjLexicon(w2)?
subj(i)Formulas for explanatory relation extractionword(i,w1+) ?
word(j,w2+) ?
j ?=i?
explain(i,j)pos(i,w1+,t+) ?
pos(j,w2+,t+) ?
j?=i?
explain(i,j)dep(i,h1+,m1+) ?
dep(j,h2+,m2+)?
j?=i?
explain(i,j)word(i,w1+) ?
word(j,w2+) ?
clauseDistance(i,j,m+) ?
j?=i?
explain(i,j)pos(i,w1+,t+) ?
pos(j,w2+,t+) ?
clauseDistance(i,j,m+) ?
j?=i?
explain(i,j)dep(i,h1+,m1+) ?
dep(j,h2+,m2+) ?
clauseDistance(i,j,m+) ?
j?=i?
explain(i,j)word(i,w1+) ?
word(j,w2+) ?
sentenceDistance(i,j,n+) ?
j ?=i?
explain(i,j)pos(i,w1+,t+) ?
pos(j,w2+,t+) ?
sentenceDistance(i,j,n+) ?
j ?=i?
explain(i,j)dep(i,h1+,m1+) ?
dep(j,h2+,m2+) ?
sentenceDistance(i,j,n+) ?
j?=i?
explain(i,j)word(i,w1+) ?
word(j,w2+) ?
firstWord(j,w+) ?
j?=i?
explain(i,j)pos(i,w1+,t+) ?
pos(j,w2+,t+) ?
firstWord(j,w+) ?
j?=i?
explain(i,j)dep(i,h1+,m1+) ?
dep(j,h2+,m2+) ?
firstWord(j,w+) ?
j?=i?
explain(i,j)word(i,w1+) ?
word(j,w2+) ?
subjLexicon(w1) ?
j?=i?
explain(i,j)pos(i,w1+,t+) ?
pos(j,w2+,t+) ?
subjLexicon(w1) ?
j?=i?
explain(i,j)dep(i,h1+,m1+) ?
dep(j,h2+,m2+) ?
subjLexicon(m1) ?
j?=i?
explain(i,j)firstWord(j,w+) ?
relationLexicon(w) ?
clauseDistance(i,j,m+) ?
j?=i?
explain(i,j)firstWord(j,w+) ?
relationLexicon(w) ?
sentenceDistance(i,j,n+) ?
j ?=i?
explain(i,j)firstWord(j,w+) ?
relationLexicon(w) ?
pos(j,w,t+) ?
clauseDistance(i,j,m+) ?
j ?=i?
explain(i,j)firstWord(j,w+) ?
relationLexicon(w) ?
pos(j,w,t+) ?
sentenceDistance(i,j,n+) ?
j?=i?
explain(i,j)firstWord(j,w+) ?
relationLexicon(w) ?
word(i,w1+) ?
clauseDistance(i,j,m+) ?
j ?=i?
explain(i,j)firstWord(j,w+) ?
relationLexicon(w) ?
word(i,w1+) ?
sentenceDistance(i,j,n+) ?
j?=i?
explain(i,j)firstWord(j,w+) ?
relationLexicon(w) ?
word(j,w1+) ?
clauseDistance(i,j,m+) ?
j ?=i?
explain(i,j)firstWord(j,w+) ?
relationLexicon(w) ?
word(j,w1+) ?
sentenceDistance(i,j,n+) ?
j?=i?
explain(i,j)Table 2: Descriptions of local formulas.950also construct local formulas based on predicatesextracted from dependency trees of clauses.For explanatory relation extraction, we firstly useformulas to capture lexical and syntactic informationfrom both of the clauses.
Since distances betweenclauses are helpful in determining the relation, weincorporate two kinds of distance features with lex-ical and syntactic predicates.
Connective wordssuch as for example, since, explicitly signal thepresence of the explanation relation.
Although someconnective words are ambiguous in terms of relationthey mark (Pitler and Nenkova, 2009), they may stillbe useful for explanation relation extraction.
Hence,we construct local formulas with relation lexiconand other predicates.3.3.2 Global FormulasLocal formulas are designed to deal with sub-jective classification of a single clause or relationdetermination of a single pair of clauses.
Globalformulas are designed to handle global constraintsof multiple clauses.
From the definition of explana-tory relation and corpus statistics, we observe thefollowing properties:Property 1: One clause can only serve as theexplanation of one subjective clause.Property 2: Explanatory clauses occur immedi-ately before or after their corresponding subjectiveclauses.Property 3: The positions of explanatory clausesare consecutive.
In other words, if clause ck andck+2 explain clause cj , the clause ck+1 would alsobe explanatory clause of cj .For property 1, we use the following global for-mula to make sure that one clause only explains atmost one another clause.explain(i, j) ?
?explain(k, j) ?k ?= i, j (1)Based on the property 2 and 3, explanatory claus-es are consecutive and immediately before or aftertheir corresponding subjective clauses.
We use thefollowing formulas to guarantee the property:explain(i, i+ k) ?
explain(i, i+m),1 ?
m ?
k ?
1 (2)explain(i, i?
k) ?
explain(i, i?m),1 ?
m ?
k ?
1 (3)Since our aim is to extract explanatory for subjec-tive clauses, we also use the following formulas tomake sure that the clauses which are explained aresubjective ones.explain(i, j) ?
subj(i) (4)4 Experiments4.1 Data SetWe crawled a number of reviews about digital cam-eras from Buzzillions6, which is a product reviewsite and contains more than 16 million reviews.We randomly select 100 reviews whose usefulnessratings are 5 on a 5-point scale.
They contain 1137sentences, which are composed by 1665 clauses.Amazon?s Mechanical Turk is used to deploy twotasks for labeling the corpus.
694 clauses are labeledsubjective and 478 clauses explain other ones.
Morethan 56.1% opinion expressions are explained bytheir corresponding explanatory sentences.The two projects we deployed on Amazon?s Me-chanical Turk are: 1) Determine whether a clausecontains opinion expressions or not; 2) Determinewhether a clause clarifies causes, reasons, or conse-quences of another given clause.
In order to controlthe labeling quality, we configured parameters ofthe project to make sure that all the tasks shouldbe judged by at least 20 annotators.
Most of theannotators can complete a task within 25 seconds.Figure 2 shows the screenshots of the two projects.Over all, 127 workers participated in the project.About 72% of them submitted more than 5 tasks.Although we listed several examples on the projectdescriptions, different people may have their ownunderstanding and criteria for those tasks.
In orderto measure the quality of the labeling task, we useperplexity to evaluate each task.
If the perplexityof a task is below 0.51, which means that more than80% of the workers submitted the same decision, theresult of the task will be used as training or testingdata.
From the statistic of the corpus, we observethat only 6.2% of the clauses?
subjectiveness and15.6% of explanation relations can not be certainlydecided.
For the first project, we treated those claus-es as objective one.
And, those clause pairs in thesecond project were not considered as explanationrelations.6www.buzzillions.com951Task2: Help us check whether a sentence is anexplanation of the opinion sentence.The opinion sentence (red one) is extracted from product reviews andexpress opinion towards some attributes/parts of a product.
Pleasehelp us check whether the following blue sentences describe a set offacts which clarifies the causes, reason, and consequences of theopinion given in the opinion sentence.click "yes" if there is an explanation relation between them, "no"otherwise.The battery life is something I come to expect from this line of camera.I can leave the camera on for better than 8 hours shooting()YES()NOThe battery life is something I come to expect from this line of camera.and I have the camera set to shut off the sensor after about 30 seconds()YES()NOTask1: Help us determine whether a sentence issubjective or objective.The following sentences are extracted from product reviews.
Pleasehelp us check whether the following sentences  expressing opiniontowards some attributes/parts of a product.The battery life is something I come to expect from this line of camera.
()Subjective()ObjectiveI have the camera set to shut off the sensor after about 30 seconds()Subjective()ObjectiveFigure 2: Screenshots of the two tasks on AmazonMechanical Turk.4.2 Experiments ConfigurationsStanford parser (Klein and Manning, 2003) is usedfor extracting features from dependency parse trees.For resolving Markov logic network, we use thetoolkit thebeast 7.
The detailed setting of thebeastengine is as follows: The inference algorithm isthe MAP inference with a cutting plane approach.For parameter learning, the weights for formulas areupdated by an online learning algorithm with MIRAupdate rule.
All the initial weights are set to zeros.The number of iterations is set to 10 epochs.Evaluation metrics used for subjectivity classifi-cation and relation extraction throughout the experi-ments include: Precision, Recall, and F1-score.
Werandomly select 80% reviews as training set and theothers as testing set.Since the dataset is newly created for this task, tocompare the performance of the proposed method toother models, we also reimplemented several state-7http://code.google.com/p/thebeastof-the-art methods for comparison.?
CRF-Subj: We follow the method proposed byZhao et al(2008), which regard the subjec-tivity of all clauses throughout a paragraph asa sequential flow of sentiments and use CRFsto model it.
The feature sets are similar asthe local formulas for MLN including words,POS tags, dependency relations, and opinionlexicon.?
RAE-Subj: Socher et al(2011) proposed touse recursive autoencoders for sentence-levelpredication of sentiment label distributions.
Tocompare with it, we also reimplement theirmethod without any hand designed lexicon.?
PDTB-Rel: For discourse relation extraction,we use ?PDTB-Styled End-to-End DiscourseParser?
(Lin et al 2010) to extract discourselevel relations as baseline.
Since it is a gener-al discourse relations identification algorithms,?Cause?, ?Pragmatic Cause?, ?Instantiation?,and ?Restatement?
relation types are treated asexplanatory relation in this work.?
SVM-Rel: We also use LibSVM (Chang andLin, 2011) to classify the relations betweenclauses.
Following the configurations reportedby Feng and Hirst (2012), we use linear kerneland probability estimation to model it.4.3 ResultsTable 3 shows the comparisons of the proposedmethod with the state-of-the-art systems on subjec-tivity classification and explanatory relation extrac-tion.
From the results, we can observe that recur-sive autoencoders based subjectivity classificationmethod achieves slightly better performance thanour method and conditional random fields basedmethod.
The performances of the proposed methodare similar as CRFs?.
We think that the main reasonis that only lexical features are used in MLN modelsfor subjective classification.
However, conditionalrandom fields consider not only lexical informationbut also inference of the contexts of sentences.RAE method learns vector space representations formulti-word phrases and uses compositional seman-tics to understand sentiment.952MethodsSubjective ClassificationP R F1CRF-Subj 83.5% 76.9% 80.1%RAE-Subj 85.3% 79.1% 82.1%MLN 79.2% 80.6% 79.9%MethodsRelation ExtractionP R F1RAE-Subj + PDTB-Rel 28.5% 38.6% 32.8%RAE-Subj + SVM-Rel 32.4% 89.7% 47.6%MLN 56.2% 72.9% 63.5%Table 3: Performance comparisons between the proposedmethod and state-of-the-art methods.
?MLN?
representsthe method proposed in this work.For evaluating the performance of relation extrac-tion, we combine the results of RAE with PDTB-Rel and SVM-Rel.
For all the subjective clausesidentified by RAE, PDTB-Rel and SVM-Rel areused to extract corresponding explanatory clauses.The results are shown in the last three rows inthe Table 3.
From the results, we can observethat the proposed joint model achieves best F1score and precision among all methods.
Althoughthe proposed method achieve slightly worse resultin processing subjectivity classification.
We thinkthat the error propagation is the main reason forworse results of cascaded methods.
The relativeimprovement of MLN over SVM-Rel is more than33.4%.To show the effectiveness of different observedpredicates, we evaluate the performances of theproposed method with different predicate sets.
Wesubtract one observed predicate and its correspond-ing local formulas from the original sets at a time.The results of both subjectivity classification andrelation extraction are shown in Table 4.
The firstrow shows the result of the MLN based method withall observed predicates and local formulas.
From theresults we can observe that the observed predicateswhich are not used in the local formulas for sub-jectivity classification also impact the performanceof subjectivity classification.
We think that the per-formance is effected by the global formulas, whichcombine the procedure of subjectivity classificationand relation extraction.
Among all predicates, weobserve that words and dependency relations playthe most important roles.
Without word predicate,the F1 score of subjectivity classification and re-lation extraction significantly drop to 51.2% and42.9% respectively.
For subjectivity classification,subjective lexicon contributes a lot for recall.
Forrelation extraction, the impacts of clause distanceand sentence distance are not as significant as theother features.5 Related WorkOur work relates to three research areas: sentimentanalysis/opinion mining, discourse-level relation ex-traction, andMarkov logic networks.
Along with theincreasing requirement, subjectivity classificationhas recently received considerable attention fromboth the industry and researchers.
A variety ofapproaches and methods have been proposed forthis task from different aspects.
Among them, anumber of approaches focus on classifying senti-ments of text in different levels (e.g.
words (Kimand Hovy, 2004), phrases (Wilson et al 2005),sentences (Zhao et al 2008), documents (Pang etal., 2002) and so on.
), and detecting the overallpolarity of them.Another research direction tries to convert thesentiment analysis task into entity identification andrelation extraction.
Hu and Liu (2004) proposedto use a set of methods to produce feature-basedsummary of a large number of customer reviews.Kobayashi et al(2007) assumed that evaluativeopinions could be structured as a frame which iscomposed by opinion holder, subject, aspect, andevaluation.
They converted the task to two kindsof relation extraction tasks and proposed a machinelearning-based method which used both contextualand statistical clues.Analysis of some special types of sentences werealso introduced in recent years.
Jindal and Li-u (2006) studied the problem of identifying com-parative sentences.
They analyzed different typesof comparative sentences and proposed learningapproaches to identify them.
Conditional sentenceswere studied by Narayanan et al2009).
Theyanalyzed the conditional sentences in both linguisticand computitional perspectives and used learning953Subjective Classification Relation ExtractionP R F1 P R F1MLN 79.2% 80.6% 79.9% 56.2% 72.9% 63.5%?subjLexicon(w) 76.6% 70.4% 73.4 % 52.3% 68.6% 59.4%?relationLexicon(w) 78.2% 79.4% 78.8% 53.6% 70.8% 61.0%?word(i, w) 52.8% 49.6% 51.2 % 36.4% 52.1% 42.9%?firstWord(i, w) 76.3% 80.1% 78.2% 56.9% 69.8% 62.7%?pos(i, w, t) 72.6% 76.8% 74.6 % 52.4% 60.2% 56.0%?dep(i, h,m) 57.6% 70.6% 63.4% 41.2% 56.8% 47.8%?clauseDistance(i, j,m) 78.9% 80.2% 79.5% 52.6% 70.6% 60.3%?sentenceDistance(i, j, n) 78.6% 80.3% 79.4% 52.4% 70.8% 60.2%Table 4: Performance comparisons of different observed predicatesmethod to do it.
They followed the feature-basedsentiment analysis model (Hu and Liu, 2004), whichalso use flat frames to represent evaluations.Since the cross sentences relations are consideredin this work, the discourse-level relation extrac-tion methods are also related to ours.
Marcu andEchihabi (2002) proposed to use an unsupervisedapproach to recognizing discourse relations.
Lin etal.
(2009) analyzed the impacts of features extractedfrom contextual information, constituent parse trees,dependency parse trees, and word pairs.
Asheret al2009) studied discourse segments containingopinion expressions from the perspective of linguis-tics.
Chen et al(2010) introduced a multi-labelmodel to detect emotion causes.
They developedtwo sets of linguistic features for this task base onlinguistic cues.
Zirn et al(2011) proposed to useMLN framework to capture the context informationin analysing (sub-)sentences.The most similar work to ours was proposed bySomasundaran et al2009).
They proposed to use it-erative classification algorithm to capture discourse-level associations.
However different to us, theyfocused on pairwise relationships between opinionexpressions.
In this paper, we used MLN frameworkto capture another different discourse-level relation,which exists between subject clauses or subjectclause and objective clause.Richardson and Domingos (2006) proposedMarkov Logic Networks, which combines first-order logic and probabilistic graphical models.
Inrecent years, MLN has been adopted for severalnatural language processing tasks and achieveda certain level of success (Singla and Domingos,2006; Riedel and Meza-Ruiz, 2008; Yoshikawaet al 2009; Andrzejewski et al 2011; Jianget al 2012; Huang et al 2012).
Singla andDomingos (2006) modeled the entity resolutionproblem with MLN.
They demonstrated thecapability of MLN to seamlessly combine a numberof previous approaches.
Poon and Domingos (2008)proposed to use MLN for joint unsupervisedcoreference resolution.
Yoshikawa et al(2009)proposed to use Markov logic to incorporate bothlocal features and global constraints that holdbetween temporal relations.
Andrzejewski etal.
(2011) introduced a framework for incorporatinggeneral domain knowledge, which is represented byFirst-Order Logic (FOL) rules, into LDA inferenceto produce topics shaped by both the data and therules.6 ConclusionsIn this paper, we propose to use Markov logicnetworks to identify subjective text segments and ex-tract their corresponding explanations in discourselevel.
We use MLN to jointly model subjectivityclassification and explanatory relation extraction.Rich linguistic features and global constraints areincorporated by various logic formulas and globalformulas.
To evaluate the proposed method, wecollected a large number of product reviews and954constructed a labeled corpus through Amazon?s Me-chanical Turk.
Experimental results demonstratethat the proposed approach achieve better perfor-mance than state-of-the-art methods.7 AcknowledgementThe authors wish to thank the anonymous reviewersfor their helpful comments and Kang Han forpreparing the corpus.
This work was partiallyfunded by National Natural Science Foundationof China (61003092, 61073069), Key Projectsin the National Science & Technology PillarProgram(2012BAH18B01), National MajorScience and Technology Special Project of China(2014ZX03006005), Shanghai Municipal Scienceand Technology Commission (12511504502) and?Chen Guang?
project supported by ShanghaiMunicipal Education Commission and ShanghaiEducation Development Foundation(11CG05).ReferencesDavid Andrzejewski, Xiaojin Zhu, Mark Craven, andBenjamin Recht.
2011.
A framework for incorpo-rating general domain knowledge into latent dirichletallocation using first-order logic.
In Proceedings ofthe Twenty-Second international joint conference onArtificial Intelligence - Volume Volume Two, IJCAI?11,pages 1171?1177.
AAAI Press.Nicholas Asher, Farah Benamara, and Yannick Mathieu.2009.
Appraisal of opinion expressions in discourse.Lingvistic?
Investigationes.Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-tiani.
2010.
Sentiwordnet 3.0: An enhanced lexicalresource for sentiment analysis and opinion mining.In Nicoletta Calzolari (Conference Chair), KhalidChoukri, Bente Maegaard, Joseph Mariani, Jan Odijk,Stelios Piperidis, Mike Rosner, and Daniel Tapias,editors, Proceedings of the Seventh InternationalConference on Language Resources and Evaluation(LREC?10), Valletta, Malta, may.
European LanguageResources Association (ELRA).R.
Carston.
1993.
Conjunction, explanation andrelevance.
Lingua 90, pages 27?48.Chih-Chung Chang and Chih-Jen Lin.
2011.
Libsvm:A library for support vector machines.
ACM Trans.Intell.
Syst.
Technol., 2(3):27:1?27:27, May.Ying Chen, Sophia Yat Mei Lee, Shoushan Li, andChu-Ren Huang.
2010.
Emotion cause detectionwith linguistic constructions.
In Proceedings ofColing2010.Sajib Dasgupta and Vincent Ng.
Mine the easy, classifythe hard: A semi-supervised approach to automaticsentiment classification.
In Proceedings of ACL-IJCNLP 2009.Eduard Dragut, Hong Wang, Clement Yu, Prasad Sistla,and Weiyi Meng.
2012.
Polarity consistencychecking for sentiment dictionaries.
In Proceedingsof the 50th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 997?1005, Jeju Island, Korea, July.
Associationfor Computational Linguistics.Vanessa Wei Feng and Graeme Hirst.
2012.
Text-level discourse parsing with rich linguistic features.In Proceedings of the 50th Annual Meeting of theAssociation for Computational Linguistics (Volume 1:Long Papers), pages 60?68, Jeju Island, Korea, July.Association for Computational Linguistics.Ahmed Hassan and Dragomir R. Radev.
2010.Identifying text polarity using random walks.
InProceedings of ACL 2010, Uppsala, Sweden, July.Minqing Hu and Bing Liu.
2004.
Mining andsummarizing customer reviews.
In Proceedings ofSIGKDD 2004.Minlie Huang, Xing Shi, Feng Jin, and Xiaoyan Zhu.2012.
Using first-order logic to compress sentences.In Proceedings of the Twenty-Sixth AAAI Conferenceon Artificial Intelligence.Shangpu Jiang, D. Lowd, and Dejing Dou.
2012.
Learn-ing to refine an automatically extracted knowledgebase using markov logic.
In Data Mining (ICDM),2012 IEEE 12th International Conference on, pages912?917.Nitin Jindal and Bing Liu.
2006.
Identifying comparativesentences in text documents.
In Proceedings of SIGIR2006.Soo-Min Kim and Eduard Hovy.
2004.
Determiningthe sentiment of opinions.
In Proceedings of COLING2004.Dan Klein and Christopher D.Manning.
2003.
Fast exactinference with a factored model for natural languageparsing.
In Proceedings of NIPS 2003.Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.2007.
Extracting aspect-evaluation and aspect-ofrelations in opinion mining.
In Proceedings ofEMNLP-CoNLL 2007.Paul Kroeger.
2005.
Analyzing Grammar: AnIntroduction.
Cambridge.Alex Lascarides and Nicholas Asher.
1993.
Temporalinterpretation, discourse relations, and common senseentailment.
Linguistics and Philosophy, 16(5):437?493.Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng.
2009.Recognizing implicit discourse relations in the penndiscourse treebank.
In Proceedings of EMNLP 2009.955Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2010.A pdtb-styled end-to-end discourse parser.
CoRR,abs/1011.0835.Bing Liu.
2012.
Sentiment Analysis and OpinionMining.
Morgan & Claypool Publishers.Daniel Marcu and Abdessamad Echihabi.
2002.An unsupervised approach to recognizing discourserelations.
In Proceedings of the 40th Annual Meetingon Association for Computational Linguistics, ACL?02, pages 368?375.Xinfan Meng, Furu Wei, Xiaohua Liu, Ming Zhou,Ge Xu, and Houfeng Wang.
2012.
Cross-lingual mixture model for sentiment classification.In Proceedings of the 50th Annual Meeting of theAssociation for Computational Linguistics (Volume 1:Long Papers), pages 572?581, Jeju Island, Korea, July.Association for Computational Linguistics.RadaMihalcea, Carmen Banea, and JanyceWiebe.
2007.Learning multilingual subjective language via cross-lingual projections.
In Proceedings of ACL 2007.Ramanathan Narayanan, Bing Liu, and Alok Choudhary.2009.
Sentiment analysis of conditional sentences.
InProceedings of EMNLP 2009.Bo Pang and Lillian Lee.
2008.
Opinion miningand sentiment analysis.
Foundations and Trends inInformation Retrieval, 2:1?135, January.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
Sentiment classification usingmachine learning techniques.
In Proceedings ofEMNLP 2002.Emily Pitler and Ani Nenkova.
2009.
Using syntax todisambiguate explicit discourse connectives in text.
InProceedings of the ACL-IJCNLP 2009.Hoifung Poon and Pedro Domingos.
2008.
Joint unsu-pervised coreference resolution with markov logic.
InProceedings of the Conference on Empirical Methodsin Natural Language Processing, EMNLP ?08, pages650?659, Stroudsburg, PA, USA.
Association forComputational Linguistics.Alan Lee Eleni Miltsakaki Livio Robaldo Aravind JoshiRashmi Prasad, Nikhil Dinesh and Bonnie Webber.2008.
The penn discourse treebank 2.0.
InBente Maegaard Joseph Mariani Jan Odjik SteliosPiperidis Daniel Tapias Nicoletta Calzolari (Confer-ence Chair), Khalid Choukri, editor, Proceedings of L-REC?08, Marrakech, Morocco, may.
http://www.lrec-conf.org/proceedings/lrec2008/.Matthew Richardson and Pedro Domingos.
2006.Markov logic networks.
Machine Learning, 62(1-2):107?136.Sebastian Riedel and Ivan Meza-Ruiz.
2008.
Collectivesemantic role labelling with markov logic.
InProceedings of the Twelfth Conference on Compu-tational Natural Language Learning, CoNLL ?08,pages 193?197, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Ellen Riloff, Janyce Wiebe, and Theresa Wilson.
2003.Learning subjective nouns using extraction patternbootstrapping.
In Proceedings of HLT-NAACL 2003.P.
Singla and P. Domingos.
2006.
Entity resolution withmarkov logic.
In Data Mining, 2006.
ICDM ?06.
SixthInternational Conference on, pages 572?582.Richard Socher, Jeffrey Pennington, Eric H. Huang,Andrew Y. Ng, and Christopher D. Manning.
2011.Semi-supervised recursive autoencoders for predict-ing sentiment distributions.
In Proceedings ofthe Conference on Empirical Methods in NaturalLanguage Processing, EMNLP ?11, pages 151?161,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Swapna Somasundaran, Galileo Namata, Lise Getoor,and Janyce Wiebe.
2009.
Opinion graphs forpolarity and discourse classification.
In Proceedingsof TextGraphs-4.Yang Song, Jing Jiang, Wayne Xin Zhao, Sujian Li, andHoufeng Wang.
2012.
Joint learning for coreferenceresolution with markov logic.
In Proceedings ofthe 2012 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning, pages 1245?1254, JejuIsland, Korea, July.
Association for ComputationalLinguistics.Hiroya Takamura, Takashi Inui, and Manabu Okumura.2005.
Extracting semantic orientations of words usingspin model.
In Proceedings of ACL 2005.Erik F. Tjong, Kim Sang, and Herve?
De?jean.
2001.Introduction to the conll-2001 shared task: clauseidentification.
In Proceedings of the 2001 workshopon Computational Natural Language Learning -Volume 7, ConLL ?01, Stroudsburg, PA, USA.Association for Computational Linguistics.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-levelsentiment analysis.
In Proceedings of HLT-EMNLP2005.Yuanbin Wu, Qi Zhang, Xuangjing Huang, and LideWu.
2011.
Structural opinion mining for graph-basedsentiment representation.
In Proceedings of EMNLP2011.Liheng Xu, Kang Liu, Siwei Lai, Yubo Chen, andJun Zhao.
2013.
Walk and learn: a two-stageapproach for opinion words and opinion targets co-extraction.
In Proceedings of the 22nd internationalconference on World Wide Web companion, WWW?13 Companion, pages 95?96, Republic and Canton ofGeneva, Switzerland.
International World Wide WebConferences Steering Committee.956Bishan Yang and Claire Cardie.
2013.
Joint inferencefor fine-grained opinion extraction.
In Proceedings ofACL 2013.Katsumasa Yoshikawa, Sebastian Riedel, MasayukiAsahara, and Yuji Matsumoto.
2009.
Jointlyidentifying temporal relations with markov logic.
InProceedings of the Joint Conference of the 47thAnnual Meeting of the ACL and the 4th InternationalJoint Conference on Natural Language Processingof the AFNLP: Volume 1 - Volume 1, ACL ?09,pages 405?413, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Jun Zhao, Kang Liu, and Gen Wang.
2008.
Addingredundant features for crfs-based sentence sentimentclassification.
In Proceedings of EMNLP 2008.Ca?cilia Zirn, Mathias Niepert, Heiner Stuckenschmidt,and Michael Strube.
2011.
Fine-grained sentimentanalysis with structural features.
In Proceedings of 5thInternational Joint Conference on Natural LanguageProcessing, pages 336?344, Chiang Mai, Thailand,November.
Asian Federation of Natural LanguageProcessing.957
