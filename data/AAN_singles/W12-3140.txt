Proceedings of the 7th Workshop on Statistical Machine Translation, pages 322?329,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsJoint WMT 2012 Submission of the QUAERO Project?Markus Freitag, ?Stephan Peitz, ?Matthias Huck, ?Hermann Ney,?Jan Niehues, ?Teresa Herrmann, ?Alex Waibel,?Le Hai-son, ?Thomas Lavergne, ?Alexandre Allauzen,?Bianka Buschbeck, ?Josep Maria Crego, ?Jean Senellart?RWTH Aachen University, Aachen, Germany?Karlsruhe Institute of Technology, Karlsruhe, Germany?LIMSI-CNRS, Orsay, France?SYSTRAN Software, Inc.?surname@cs.rwth-aachen.de?firstname.surname@kit.edu?firstname.lastname@limsi.fr ?surname@systran.frAbstractThis paper describes the joint QUAERO sub-mission to the WMT 2012 machine transla-tion evaluation.
Four groups (RWTH AachenUniversity, Karlsruhe Institute of Technol-ogy, LIMSI-CNRS, and SYSTRAN) of theQUAERO project submitted a joint translationfor the WMT German?English task.
Eachgroup translated the data sets with their ownsystems and finally the RWTH system combi-nation combined these translations in our finalsubmission.
Experimental results show im-provements of up to 1.7 points in BLEU and3.4 points in TER compared to the best singlesystem.1 IntroductionQUAERO is a European research and develop-ment program with the goal of developing multi-media and multilingual indexing and managementtools for professional and general public applica-tions (http://www.quaero.org).
Research in machinetranslation is mainly assigned to the four groupsparticipating in this joint submission.
The aim ofthis WMT submission was to show the quality of ajoint translation by combining the knowledge of thefour project partners.
Each group develop and main-tain their own different machine translation system.These single systems differ not only in their generalapproach, but also in the preprocessing of trainingand test data.
To take the advantage of these dif-ferences of each translation system, we combinedall hypotheses of the different systems, using theRWTH system combination approach.This paper is structured as follows.
In Section2, the different engines of all four groups are in-troduced.
In Section 3, the RWTH Aachen systemcombination approach is presented.
Experimentswith different system selections for system combi-nation are described in Section 4.
Finally in Section5, we discuss the results.2 Translation SystemsFor WMT 2012 each QUAERO partner trained theirsystems on the parallel Europarl and News Com-mentary corpora.
All single systems were tunedon the newstest2009 or newstest2010 developmentset.
The newstest2011 dev set was used to trainthe system combination parameters.
Finally, thenewstest2008-newstest2010 dev sets were used tocompare the results of the different system combina-tion settings.
In this Section all four different systemengines are presented.2.1 RWTH Aachen Single SystemsFor the WMT 2012 evaluation the RWTH utilizedRWTH?s state-of-the-art phrase-based and hierar-chical translation systems.
GIZA++ (Och and Ney,2003) was employed to train word alignments, lan-guage models have been created with the SRILMtoolkit (Stolcke, 2002).2.1.1 Phrase-Based SystemThe phrase-based translation (PBT) system issimilar to the one described in Zens and Ney (2008).After phrase pair extraction from the word-alignedparallel corpus, the translation probabilities are esti-mated by relative frequencies.
The standard feature322set alo includes an n-gram language model, phrase-level IBM-1 and word-, phrase- and distortion-penalties, which are combined in log-linear fash-ion.
The model weights are optimized with standardMert (Och, 2003) on 200-best lists.
The optimiza-tion criterium is BLEU.2.1.2 Hierarchical SystemFor the hierarchical setups (HPBT) described inthis paper, the open source Jane toolkit (Vilar etal., 2010) is employed.
Jane has been developed atRWTH and implements the hierarchical approach asintroduced by Chiang (2007) with some state-of-the-art extensions.
In hierarchical phrase-based transla-tion, a weighted synchronous context-free grammaris induced from parallel text.
In addition to contigu-ous lexical phrases, hierarchical phrases with up totwo gaps are extracted.
The search is typically car-ried out using the cube pruning algorithm (Huangand Chiang, 2007).
The model weights are opti-mized with standard Mert (Och, 2003) on 100-bestlists.
The optimization criterium is 4BLEU ?TER.2.1.3 PreprocessingIn order to reduce the source vocabulary sizetranslation, the German text was preprocessedby splitting German compound words with thefrequency-based method described in (Koehn andKnight, 2003a).
To further reduce translation com-plexity for the phrase-based approach, we performedthe long-range part-of-speech based reordering rulesproposed by (Popovic?
et al, 2006).2.1.4 Language ModelFor both decoders a 4-gram language model is ap-plied.
The language model is trained on the par-allel data as well as the provided News crawl, the109 French-English, UN and LDC Gigaword FourthEdition corpora.
For the 109 French-English, UNand LDC Gigaword corpora RWTH applied the dataselection technique described in (Moore and Lewis,2010).2.2 Karlsruhe Institute of Technology SingleSystem2.2.1 PreprocessingWe preprocess the training data prior to trainingthe system, first by normalizing symbols such asquotes, dashes and apostrophes.
Then smart-casingof the first words of each sentence is performed.
Forthe German part of the training corpus we use thehunspell1 lexicon to learn a mapping from old Ger-man spelling to new German spelling to obtain a cor-pus with homogenous spelling.
In addition, we per-form compound splitting as described in (Koehn andKnight, 2003b).
Finally, we remove very long sen-tences, empty lines, and sentences that probably arenot parallel due to length mismatch.2.2.2 System OverviewThe KIT system uses an in-house phrase-baseddecoder (Vogel, 2003) to perform translation and op-timization with regard to the BLEU score is done us-ing Minimum Error Rate Training as described inVenugopal et al (2005).2.2.3 Translation ModelsThe translation model is trained on the Europarland News Commentary Corpus and the phrase ta-ble is based on a discriminative word alignment(Niehues and Vogel, 2008).In addition, the system applies a bilingual lan-guage model (Niehues et al, 2011) to extend thecontext of source language words available for trans-lation.Furthermore, we use a discriminative word lexi-con as introduced in (Mauser et al, 2009).
The lex-icon was trained and integrated into our system asdescribed in (Mediani et al, 2011).At last, we tried to find translations forout-of-vocabulary (OOV) words by using quasi-morphological operations as described in Niehuesand Waibel (2011).
For each OOV word, we try tofind a related word that we can translate.
We modifythe ending letters of the OOV word and learn quasi-morphological operations to be performed on theknown translation of the related word to synthesizea translation for the OOV word.
By this approachwe were for example able to translate Kaminen intochimneys using the known translation Kamin # chim-ney.2.2.4 Language ModelsWe use two 4-gram SRI language models, onetrained on the News Shuffle corpus and one trained1http://hunspell.sourceforge.net/323on the Gigaword corpus.
Furthermore, we use a 5-gram cluster-based language model trained on theNews Shuffle corpus.
The word clusters were cre-ated using the MKCLS algorithm.
We used 100word clusters.2.2.5 Reordering ModelReordering is performed based on part-of-speechtags obtained using the TreeTagger (Schmid, 1994).Based on these tags we learn probabilistic continu-ous (Rottmann and Vogel, 2007) and discontinuous(Niehues and Kolss, 2009) rules to cover short andlong-range reorderings.
The rules are learned fromthe training corpus and the alignment.
In addition,we learned tree-based reordering rules.
Therefore,the training corpus was parsed by the Stanford parser(Rafferty and Manning, 2008).
The tree-based rulesconsist of the head node of a subtree and all itschildren as well as the new order and a probability.These rules were applied recursively.
The reorderingrules are applied to the source sentences and the re-ordered sentence variants as well as the original se-quence are encoded in a word lattice which is usedas input to the decoder.
For the test sentences, thereordering based on parts-of-speech and trees allowsus to change the word order in the source sentenceso that the sentence can be translated more easily.In addition, we build reordering lattices for all train-ing sentences and then extract phrase pairs from themonotone source path as well as from the reorderedpaths.2.3 LIMSI-CNRS Single SystemLIMSI?s system is built with n-code (Crego et al,2011), an open source statistical machine translationsystem based on bilingual n-gram2.
In this approach,the translation model relies on a specific decomposi-tion of the joint probability of a sentence pair P(s, t)using the n-gram assumption: a sentence pair is de-composed into a sequence of bilingual units calledtuples, defining a joint segmentation of the sourceand target.
In the approach of (Marin?o et al, 2006),this segmentation is a by-product of source reorder-ing which ultimately derives from initial word andphrase alignments.2http://ncode.limsi.fr/2.3.1 An Overview of n-codeThe baseline translation model is implemented asa stochastic finite-state transducer trained using an-gram model of (source,target) pairs (Casacubertaand Vidal, 2004).
Training this model requires toreorder source sentences so as to match the targetword order.
This is performed by a stochastic finite-state reordering model, which uses part-of-speechinformation3 to generalize reordering patterns be-yond lexical regularities.In addition to the translation model, eleven fea-ture functions are combined: a target-languagemodel; four lexicon models; two lexicalized reorder-ing models (Tillmann, 2004) aiming at predictingthe orientation of the next translation unit; a ?weak?distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which com-pensate for the system preference for short transla-tions.
The four lexicon models are similar to the onesused in a standard phrase based system: two scorescorrespond to the relative frequencies of the tuplesand two lexical weights estimated from the automat-ically generated word alignments.
The weights asso-ciated to feature functions are optimally combinedusing a discriminative training framework (Och,2003), using the newstest2009 development set.The overall search is based on a beam-searchstrategy on top of a dynamic programming algo-rithm.
Reordering hypotheses are computed in apreprocessing step, making use of reordering rulesbuilt from the word reorderings introduced in the tu-ple extraction process.
The resulting reordering hy-potheses are passed to the decoder in the form ofword lattices (Crego and Marin?o, 2007).2.3.2 Continuous Space Translation ModelsOne critical issue with standard n-gram transla-tion models is that the elementary units are bilingualpairs, which means that the underlying vocabularycan be quite large.
Unfortunately, the parallel dataavailable to train these models are typically smallerthan the corresponding monolingual corpora used totrain target language models.
It is very likely then,that such models should face severe estimation prob-lems.
In such setting, using neural network language3Part-of-speech labels for English and German are com-puted using the TreeTagger (Schmid, 1995).324model techniques seem all the more appropriate.
Forthis study, we follow the recommendations of Le etal.
(2012), who propose to factor the joint proba-bility of a sentence pair by decomposing tuples intwo (source and target) parts, and further each partin words.
This yields a word factored translationmodel that can be estimated in a continuous spaceusing the SOUL architecture (Le et al, 2011).The design and integration of a SOUL model forlarge SMT tasks is far from easy, given the computa-tional cost of computing n-gram probabilities.
Thesolution used here was to resort to a two pass ap-proach: the first pass uses a conventional back-offn-gram model to produce a k-best list; in the secondpass, the k-best list is reordered using the probabil-ities of m-gram SOUL translation models.
In thefollowing experiments, we used a fixed context sizefor SOUL of m = 10, and used k = 300.2.3.3 Corpora and Data PreprocessingThe parallel data is word-aligned usingMGIZA++4 with default settings.
For the En-glish monolingual training data, we used the samesetup as last year5 and thus the same target languagemodel as detailed in (Allauzen et al, 2011).For English, we took advantage of our in-housetext processing tools for tokenization and detok-enization steps (De?chelotte et al, 2008) and our sys-tem was built in ?true-case?.
As German is mor-phologically more complex than English, the defaultpolicy which consists in treating each word formindependently is plagued with data sparsity, whichis detrimental both at training and decoding time.Thus, the German side was normalized using a spe-cific pre-processing scheme (Allauzen et al, 2010;Durgar El-Kahlout and Yvon, 2010), which notablyaims at reducing the lexical redundancy by (i) nor-malizing the orthography, (ii) neutralizing most in-flections and (iii) splitting complex compounds.2.4 SYSTRAN Software, Inc.
Single SystemThe data submitted by SYSTRAN were obtained bya system composed of the standard SYSTRAN MTengine in combination with a statistical post editing(SPE) component.4http://geek.kyloo.net/software5The fifth edition of the English Gigaword (LDC2011T07)was not used.The SYSTRAN system is traditionally classi-fied as a rule-based system.
However, over thedecades, its development has always been driven bypragmatic considerations, progressively integratingmany of the most efficient MT approaches and tech-niques.
Nowadays, the baseline engine can be con-sidered as a linguistic-oriented system making use ofdependency analysis, general transfer rules as wellas of large manually encoded dictionaries (100k -800k entries per language pair).The SYSTRAN phrase-based SPE componentviews the output of the rule-based system as thesource language, and the (human) reference trans-lation as the target language, see (L. Dugast andKoehn, 2007).
It performs corrections and adaptionslearned from the 5-gram language model trained onthe parallel target-to-target corpus.
Moreover, thefollowing measures - limiting unwanted statisticaleffects - were applied:?
Named entities, time and numeric expressionsare replaced by special tokens on both sides.This usually improves word alignment, sincethe vocabulary size is significantly reduced.
Inaddition, entity translation is handled more re-liably by the rule-based engine.?
The intersection of both vocabularies (i.e.
vo-cabularies of the rule-based output and the ref-erence translation) is used to produce an addi-tional parallel corpus to help to improve wordalignment.?
Singleton phrase pairs are deleted from thephrase table to avoid overfitting.?
Phrase pairs not containing the same numberof entities on the source and the target side arealso discarded.The SPE language model was trained on 2M bilin-gual phrases from the news/Europarl corpora, pro-vided as training data for WMT 2012.
An addi-tional language model built from 15M phrases ofthe English LDC Gigaword corpus using Kneser-Ney (Kneser and Ney, 1995) smoothing was added.Weights for these separate models were tuned bythe Mert algorithm provided in the Moses toolkit(P. Koehn et al, 2007), using the provided news de-velopment set.3253 RWTH Aachen System CombinationSystem combination is used to produce consensustranslations from multiple hypotheses produced withdifferent translation engines that are better in termsof translation quality than any of the individual hy-potheses.
The basic concept of RWTH?s approachto machine translation system combination has beendescribed by Matusov et al (2006; 2008).
This ap-proach includes an enhanced alignment and reorder-ing framework.
A lattice is built from the input hy-potheses.
The translation with the best score withinthe lattice according to a couple of statistical modelsis selected as consensus translation.4 ExperimentsThis year, we tried different sets of single systemsfor system combination.
As RWTH has two dif-ferent translation systems, we put the output ofboth systems into system combination.
Althoughboth systems have the same preprocessing and lan-guage model, their hypotheses differ because oftheir different decoding approach.
Compared tothe other systems, the system by SYSTRAN has acompletely different approach (see section 2.4).
Itis mainly based on a rule-based system.
For theGerman?English pair, SYSTRAN achieves a lowerBLEU score in each test set compared to the othergroups.
However, since the SYSTRAN system isvery different to the others, we still obtain an im-provement when we add it also to system combina-tion.We did experiments with different optimizationcriteria for the system combination optimization.All results are listed in Table 1 (unoptimized), Table2 (optimized on BLEU) and Table 3 (optimized onTER-BLEU).
Further, we investigated, whether wewill loose performance, if a single system is droppedfrom the system combination.
The results show thatfor each optimization criteria we need all systems toachieve the best results.For the BLEU optimized system combination, weobtain an improvement compared to the best sin-gle systems for all dev sets.
For newstest2008, weget an improvement of 1.5 points in BLEU and 1.5points in TER compared to the best single system ofKarlsruhe Institute of Technology.
For newstest2009we get an improvement of 1.9 points in BLEU and1.5 points in TER compared to the best single sys-tem.
The system combination of all systems outper-forms the best single system with 1.9 points in BLEUand 1.9 points in TER for newstest2010.
For new-stest2011 the improvement is 1.3 points in BLEUand 2.9 points in TER.For the TER-BLEU optimized system combina-tion, we achieved more improvement in TER com-pared to the BLEU optimized system combination.For newstest2008, we get an improvement of 0.8points in BLEU and 3.0 points in TER compared tothe best single system of Karlsruhe Institute of Tech-nology.
The system combinations performs betteron newstest2009 with 1.3 points in BLEU and 2.7points in TER.
For newstest2010, we get an im-provement of 1.7 points in BLEU and 3.4 points inTER and for newstest2011 we get an improvementof 0.7 points in BLEU and 2.5 points in TER.5 ConclusionThe four statistical machine translation systems ofKarlsruhe Institute of Technology, RWTH Aachenand LIMSI and the very structural approach of SYS-TRAN produce hypotheses with a huge variabilitycompared to the others.
Finally, the RWTH Aachensystem combination combined all single system hy-potheses to one hypothesis with a higher BLEU anda lower TER score compared to each single sys-tem.
For each optimization criteria the system com-binations using all single systems outperforms thesystem combinations using one less single system.Although the single system of SYSTRAN has theworst error scores and the RWTH single systems aresimilar, we achieved the best result in using all singlesystems.
For the WMT 12 evaluation, we submittedthe system combination of all systems optimized onBLEU.AcknowledgmentsThis work was achieved as part of the Quaero Pro-gramme, funded by OSEO, French State agency forinnovation.ReferencesAlexandre Allauzen, Josep M. Crego, I?lknur Durgar El-Kahlout, and Francois Yvon.
2010.
LIMSI?s statis-tical translation systems for WMT?10.
In Proc.
of the326Table 1: All systems for the WMT 2012 German?English translation task (truecase).
BLEU and TER results are inpercentage.
sc denotes system combination.
All system combinations are unoptimized.system newstest2008 newstest2009 newstest2010 newstest2011BLEU TER BLEU TER BLEU TER BLEU TER TER-BLEUKIT 22.2 61.8 21.3 61.0 24.1 59.0 22.4 60.2 37.9RWTH.PBT 21.4 62.0 21.3 61.1 23.9 59.1 21.4 61.2 39.7Limsi 22.2 63.0 22.0 61.8 23.9 59.9 21.8 62.0 40.2RWTH.HPBT 21.5 62.6 21.5 61.6 23.6 60.2 21.5 61.8 40.4SYSTRAN 18.3 64.6 17.9 63.4 21.1 60.5 18.3 63.1 44.8sc-withAllSystems 23.4 59.7 22.9 59.0 26.2 56.5 23.3 58.8 35.5sc-without-RWTH.PBT 23.2 59.8 22.8 59.0 25.9 56.6 23.1 58.7 35.6sc-without-RWTH.HPBT 23.2 59.6 22.7 58.9 26.1 56.2 23.1 58.7 35.6sc-without-Limsi 22.7 60.1 22.4 59.2 25.5 56.7 22.8 58.8 36.0sc-without-SYSTRAN 23.0 60.3 22.5 59.5 25.7 57.2 23.1 59.2 36.1sc-without-KIT 23.0 59.9 22.5 59.1 25.9 56.6 22.9 59.1 36.3Table 2: All systems for the WMT 2012 German?English translation task (truecase).
BLEU and TER results are inpercentage.
sc denotes system combination.
All system combinations are optimized on BLEU .system newstest2008 newstest2009 newstest2010 newstest2011BLEU TER BLEU TER BLEU TER BLEU TER TER-BLEUsc-withAllSystems 23.7 60.3 23.2 59.5 26.0 57.1 23.7 59.2 35.6sc-without-RWTH.PBT 23.4 61.1 23.1 59.8 25.5 57.6 23.5 59.5 36.1sc-without-SYSTRAN 23.3 61.1 22.6 60.5 25.3 58.1 23.5 60.0 36.5sc-without-Limsi 23.1 60.7 22.6 59.7 25.4 57.5 23.3 59.4 36.2sc-without-KIT 23.4 60.7 23.0 59.7 25.6 57.7 23.3 59.8 36.5sc-without-RWTH.HPBT 23.3 59.4 22.8 58.6 26.1 56.0 23.1 58.4 35.2Table 3: All systems for the WMT 2012 German?English translation task (truecase).
BLEU and TER results are inpercentage.
sc denotes system combination.
All system combinations are optimized on TER-BLEU .system newstest2008 newstest2009 newstest2010 newstest2011BLEU TER BLEU TER BLEU TER BLEU TER TER-BLEUsc-withAllSystems 23.0 58.8 22.4 58.3 25.8 55.6 23.1 57.7 34.6sc-without-RWTH.PBT 23.0 59.3 22.5 58.5 25.6 56.0 23.1 58.0 34.9sc-without-RWTH.HPBT 23.1 59.0 22.6 58.3 25.8 55.6 23.0 58.0 35.0sc-without-SYSTRAN 22.9 59.7 22.4 59.1 25.6 56.7 23.2 58.5 35.3sc-without-Limsi 22.7 59.4 22.2 58.7 25.3 56.1 22.7 58.1 35.5sc-without-KIT 22.9 59.3 22.4 58.5 25.7 55.8 22.7 58.1 35.4327Joint Workshop on Statistical Machine Translation andMetricsMATR, pages 54?59, Uppsala, Sweden.Alexandre Allauzen, Gilles Adda, He?le`ne Bonneau-Maynard, Josep M. Crego, Hai-Son Le, Aure?lien Max,Adrien Lardilleux, Thomas Lavergne, Artem Sokolov,Guillaume Wisniewski, and Franc?ois Yvon.
2011.LIMSI @ WMT11.
In Proceedings of the Sixth Work-shop on Statistical Machine Translation, pages 309?315, Edinburgh, Scotland, July.
Association for Com-putational Linguistics.F.
Casacuberta and E. Vidal.
2004.
Machine translationwith inferred stochastic finite-state transducers.
Com-putational Linguistics, 30(3):205?225.D.
Chiang.
2007.
Hierarchical Phrase-Based Transla-tion.
Computational Linguistics, 33(2):201?228.J.M.
Crego and J.B. Marin?o.
2007.
Improving statisticalMT by coupling reordering and decoding.
MachineTranslation, 20(3):199?215.Josep M. Crego, Franois Yvon, and Jos B. Mario.2011.
N-code: an open-source Bilingual N-gram SMTToolkit.
Prague Bulletin of Mathematical Linguistics,96:49?58.D.
De?chelotte, O. Galibert G. Adda, A. Allauzen, J. Gau-vain, H. Meynard, and F. Yvon.
2008.
LIMSI?s statis-tical translation systems for WMT?08.
In Proc.
of theNAACL-HTL Statistical Machine Translation Work-shop, Columbus, Ohio.Ilknur Durgar El-Kahlout and Franois Yvon.
2010.
Thepay-offs of preprocessing for German-English Statis-tical Machine Translation.
In Marcello Federico, IanLane, Michael Paul, and Franois Yvon, editors, Pro-ceedings of the seventh International Workshop onSpoken Language Translation (IWSLT), pages 251?258.L.
Huang and D. Chiang.
2007.
Forest Rescoring: FasterDecoding with Integrated Language Models.
In Proc.Annual Meeting of the Association for ComputationalLinguistics, pages 144?151, Prague, Czech Republic,June.R.
Kneser and H. Ney.
1995.
Improved backing-off form-gram language modeling.
In Proceedings of the In-ternational Conference on Acoustics, Speech, and Sig-nal Processing, ICASSP?95, pages 181?184, Detroit,MI.P.
Koehn and K. Knight.
2003a.
Empirical Methods forCompound Splitting.
In EACL, Budapest, Hungary.P.
Koehn and K. Knight.
2003b.
Empirical Methodsfor Compound Splitting.
In Proceedings of EuropeanChapter of the ACL (EACL 2009), pages 187?194.J.
Senellart L. Dugast and P. Koehn.
2007.
Statisticalpost-editing on systran?s rule-based translation system.In Proceedings of the Second Workshop on Statisti-cal Machine Translation, StatMT ?07, pages 220?223,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-LucGauvain, and Franc?ois Yvon.
2011.
Structured outputlayer neural network language model.
In Proceedingsof ICASSP?11, pages 5524?5527.Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.2012.
Continuous space translation models with neu-ral networks.
In NAACL ?12: Proceedings of the2012 Conference of the North American Chapter of theAssociation for Computational Linguistics on HumanLanguage Technology.Jose?
B. Marin?o, R. Banchs, J.M.
Crego, A. de Gispert,P.
Lambert, J.A.R.
Fonollosa, and M.R.
Costa-jussa`.2006.
N-gram-based machine translation.
Computa-tional Linguistics, 32(4).E.
Matusov, N. Ueffing, and H. Ney.
2006.
ComputingConsensus Translation from Multiple Machine Trans-lation Systems Using Enhanced Hypotheses Align-ment.
In Conference of the European Chapter of theAssociation for Computational Linguistics (EACL),pages 33?40.E.
Matusov, G. Leusch, R.E.
Banchs, N. Bertoldi,D.
Dechelotte, M. Federico, M. Kolss, Y.-S. Lee,J.B.
Mari no, M. Paulik, S. Roukos, H. Schwenk, andH.
Ney.
2008.
System Combination for MachineTranslation of Spoken and Written Language.
IEEETransactions on Audio, Speech and Language Pro-cessing, 16(7):1222?1237.Arne Mauser, Sas?a Hasan, and Hermann Ney.
2009.
Ex-tending Statistical Machine Translation with Discrim-inative and Trigger-based Lexicon Models.
In Pro-ceedings of the 2009 Conference on Empirical Meth-ods in Natural Language Processing: Volume 1 - Vol-ume 1, EMNLP ?09, Singapore.Mohammed Mediani, Eunah Cho, Jan Niehues, TeresaHerrmann, and Alex Waibel.
2011.
The KIT English-French Translation Systems for IWSLT 2011.
In Pro-ceedings of the Eighth International Workshop on Spo-ken Language Translation (IWSLT).R.C.
Moore and W. Lewis.
2010.
Intelligent Selectionof Language Model Training Data.
In ACL (Short Pa-pers), pages 220?224, Uppsala, Sweden, July.J.
Niehues and M. Kolss.
2009.
A POS-Based Model forLong-Range Reorderings in SMT.
In Fourth Work-shop on Statistical Machine Translation (WMT 2009),Athens, Greece.J.
Niehues and S. Vogel.
2008.
Discriminative WordAlignment via Alignment Matrix Modeling.
In Proc.of Third ACL Workshop on Statistical Machine Trans-lation, Columbus, USA.Jan Niehues and Alex Waibel.
2011.
Using Wikipediato Translate Domain-specific Terms in SMT.
In Pro-328ceedings of the Eighth International Workshop on Spo-ken Language Translation (IWSLT), San Francisco,CA.Jan Niehues, Teresa Herrmann, Stephan Vogel, and AlexWaibel.
2011.
Wider Context by Using Bilingual Lan-guage Models in Machine Translation.
In Sixth Work-shop on Statistical Machine Translation (WMT 2011),Edinburgh, UK.F.J.
Och and H. Ney.
2003.
A Systematic Comparison ofVarious Statistical Alignment Models.
ComputationalLinguistics, 29(1):19?51.F.J.
Och.
2003.
Minimum Error Rate Training for Statis-tical Machine Translation.
In Proc.
Annual Meeting ofthe Association for Computational Linguistics, pages160?167, Sapporo, Japan, July.A.
Birch P. Koehn, H. Hoang, C. Callison-Burch, M. Fed-erico, N. Bertoldi, B. Cowan, W. Shen, C. Moran,R.
Zens, C. Dyer, O. Bojar, A. Constantin, andE.
Herbst.
2007.
Moses: open source toolkit forstatistical machine translation.
In Proceedings of the45th Annual Meeting of the ACL on Interactive Posterand Demonstration Sessions, ACL ?07, pages 177?180, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.M.
Popovic?, D. Stein, and H. Ney.
2006.
StatisticalMachine Translation of German Compound Words.In FinTAL - 5th International Conference on NaturalLanguage Processing, Springer Verlag, LNCS, pages616?624.Anna N. Rafferty and Christopher D. Manning.
2008.Parsing three German treebanks: lexicalized and un-lexicalized baselines.
In Proceedings of the Workshopon Parsing German.K.
Rottmann and S. Vogel.
2007.
Word Reordering inStatistical Machine Translation with a POS-Based Dis-tortion Model.
In TMI, Sko?vde, Sweden.H.
Schmid.
1994.
Probabilistic Part-of-Speech TaggingUsing Decision Trees.
In International Conferenceon NewMethods in Language Processing, Manchester,UK.Helmut Schmid.
1995.
Improvements in part-of-speechtagging with an application to German.
In EvelyneTzoukermann and SusanEditors Armstrong, editors,Proceedings of the ACL SIGDATWorkshop, pages 47?50.
Kluwer Academic Publishers.A.
Stolcke.
2002.
SRILM - an extensible language mod-eling toolkit.
In Proc.
Int.
Conf.
on Spoken LanguageProcessing, volume 2, pages 901?904, Denver, Col-orado, USA, September.C.
Tillmann.
2004.
A unigram orientation model for sta-tistical machine translation.
In Proceedings of HLT-NAACL 2004, pages 101?104.
Association for Com-putational Linguistics.A.
Venugopal, A. Zollman, and A. Waibel.
2005.
Train-ing and Evaluation Error Minimization Rules for Sta-tistical Machine Translation.
In Workshop on Data-drive Machine Translation and Beyond (WPT-05), AnnArbor, MI.D.
Vilar, S. Stein, M. Huck, and H. Ney.
2010.
Jane:Open Source Hierarchical Translation, Extended withReordering and Lexicon Models.
In ACL 2010 JointFifth Workshop on Statistical Machine Translation andMetrics MATR, pages 262?270, Uppsala, Sweden,July.S.
Vogel.
2003.
SMT Decoder Dissected: Word Re-ordering.
In Int.
Conf.
on Natural Language Process-ing and Knowledge Engineering, Beijing, China.R.
Zens and H. Ney.
2008.
Improvements in DynamicProgramming Beam Search for Phrase-based Statisti-cal Machine Translation.
In Proc.
of the Int.
Workshopon Spoken Language Translation (IWSLT), Honolulu,Hawaii, October.329
