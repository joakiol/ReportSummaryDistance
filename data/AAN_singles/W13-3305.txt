Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 33?42,Sofia, Bulgaria, August 9, 2013. c?2013 Association for Computational LinguisticsDetecting Narrativity to Improve English to FrenchTranslation of Simple Past VerbsThomas MeyerIdiap Research Institute and EPFLMartigny and Lausanne, Switzerlandthomas.meyer@idiap.chCristina GrisotUniversity of GenevaSwitzerlandcristina.grisot@unige.chAndrei Popescu-BelisIdiap Research InstituteMartigny, Switzerlandandrei.popescu-belis@idiap.chAbstractThe correct translation of verb tenses en-sures that the temporal ordering of eventsin the source text is maintained in the tar-get text.
This paper assesses the utilityof automatically labeling English SimplePast verbs with a binary discursive fea-ture, narrative vs. non-narrative, for sta-tistical machine translation (SMT) intoFrench.
The narrativity feature, whichhelps deciding which of the French pasttenses is a correct translation of the En-glish Simple Past, can be assigned withabout 70% accuracy (F1).
The narrativityfeature improves SMT by about 0.2 BLEUpoints when a factored SMT system istrained and tested on automatically labeledEnglish-French data.
More importantly,manual evaluation shows that verb tensetranslation and verb choice are improvedby respectively 9.7% and 3.4% (absolute),leading to an overall improvement of verbtranslation of 17% (relative).1 IntroductionThe correct rendering of verbal tenses is an im-portant aspect of translation.
Translating to awrong verbal tense in the target language does notconvey the same meaning as the source text, forinstance by distorting the temporal order of theevents described in a text.
Current statistical ma-chine translation (SMT) systems may have diffi-culties in choosing the correct verb tense transla-tions, in some language pairs, because these de-pend on a wider-range context than SMT systemsconsider.
Indeed, decoding for SMT is still atthe phrase or sentence level only, thus missinginformation from previously translated sentences(which is also detrimental to lexical cohesion andco-reference).In this paper, we explore the merits of a dis-course feature called narrativity in helping SMTsystems to improve their translation choices forEnglish verbs in the Simple Past tense (hence-forth, SP) into one of the three possible Frenchpast tenses.
The narrativity feature characterizeseach occurrence of an SP verb, either as narrative(for ordered events that happened in the past) ornon-narrative (for past states of affairs).
Narra-tivity is potentially relevant to EN/FR translationbecause three French past tenses can potentiallytranslate an English Simple Past (SP), namely thePasse?
Compose?
(PC), Passe?
Simple (PS) or Impar-fait (IMP).
All of them can be correct translationsof an EN SP verb, depending on its narrative ornon-narrative role.The narrativity feature can be of use to SMTonly if it can be assigned with sufficient preci-sion over a source text by entirely automatic meth-ods.
Moreover, a narrativity-aware SMT model islikely to make a difference with respect to base-line SMT only if it is based on additional featuresthat are not captured by, e.g., a phrase-based SMTmodel.
In this study, we use a small amount ofmanually labeled instances to train a narrativityclassifier for English texts.
The (imperfect) out-put of this classifier over the English side of alarge parallel corpus will then be used to train anarrativity-aware SMT system.
In testing mode,the narrativity classifier provides input to the SMTsystem, resulting (as we will show below) in im-proved tense and lexical choices for verbs, anda modest but statistically significant increase inBLEU and TER scores.
Overall, the method issimilar in substance to our previous work on the33combination of a classifier for discourse connec-tives with an SMT system (Meyer and Popescu-Belis, 2012; Meyer et al 2012).The paper is organized as follows.
Section 2 ex-emplifies the hypothesized relation between nar-rativity and the translations of the English Sim-ple Past into French, along with related work onmodeling tense for MT.
The automatic labelingexperiments are presented in Section 3.
Exper-iments with SMT systems are presented in Sec-tion 4, with results from both automatic (4.3) andmanual translation scoring (4.4), followed by adiscussion of results and suggestions on improv-ing them (Section 5).2 English Simple Past in Translation2.1 Role of Narrativity: an ExampleThe text in Figure 1 is an example taken from the?newstest 2010?
data described in Section 4 below.In this four-sentence discourse, the English verbs,all in Simple Past, express a series of events hav-ing occurred in the past, which no longer affectthe present.
As shown in the French translation bya baseline SMT system (not aware of narrativity),the English SP verbs are translated into the mostfrequent tense in French, as learned from the par-allel data the SMT was trained on.When looking more closely, however, it ap-pears that the Simple Past actually conveys dif-ferent temporal and aspectual information.
Theverbs offered and found describe actual events thatwere ordered in time and took place subsequently,whereas were and was describe states of generalnature, not indicating any temporal ordering.The difference between narrative and non-narrative uses of the English Simple Past is not al-ways captured correctly by the baseline SMT out-put in this example.
The verbs in the first and thirdsentences are correctly translated into the FrenchPC (one of the two tenses for past narratives inFrench along with the PS).
The verb in the sec-ond sentence is also correctly rendered as IMP,in a non-narrative use.
However, the verb was inthe fourth sentence should also have been trans-lated as an IMP, but from lack of sufficient infor-mation, it was incorrectly translated as a PC.
Anon-narrative label could have helped to find thecorrect verb tense, if it would have been annotatedprior to translation.EN: (1) After a party, I offered [Narrative] tothrow out a few glass and plastic bottles.
(2) But,on Kounicova Ulice, there were [Non-narrative]no colored bins to be seen.
(3) Luckily, on theway to the tram, I found [Narrative] the rightplace.
(4) But it was [Non-narrative] overflow-ing with garbage.FR from BASELINE MT system: (1) Apre`sun parti, j?ai propose?
pour rejeter un peu deverre et les bouteilles en plastique.
(2) Mais, surKounicova Ulice, il n?y avait pas de colored binsa` voir.
(3) Heureusement, sur la manie`re de letramway, j?ai trouve?
la bonne place.
(4) Mais il*a e?te?
de?borde?s avec des ramasseurs.Figure 1: Example English text from ?newstest2010?
data with narrativity labels and a translationinto French from a baseline SMT.
The tenses gen-erated in French are, respectively: (1) PC, (2) IMP,(3) PC, (4) PC.
The mistake on the fourth one isexplained in the text.2.2 Modeling Past TensesThe classical view on verb tenses that express pasttense in French (PC, PS and IMP) is that both thePC and PS are perfective, indicating that the eventthey refer to is completed and finished (Martin,1971).
Such events are thus single points in timewithout internal structure.
However, on the onehand, the PC signals an accomplished event (fromthe aspectual point of view) and thus conveys asits meaning the possible consequence of the event.The PS on the other hand is considered as aspectu-ally unaccomplished and is used in contexts wheretime progresses and events are temporally ordered,such as narratives.The IMP is imperfective (as its name suggests),i.e.
it indicates that the event is in its preparatoryphrase and is thus incomplete.
In terms of aspect,the IMP is unaccomplished and provides back-ground information, for instance ongoing state ofaffairs, or situations that are repeated in time, withan internal structure.Conversely, in English, the SP is described ashaving as its main meaning the reference to pasttense, and as specific meanings the reference topresent or future tenses identified under certaincontextual conditions (Quirk et al 1986).
Cor-blin and de Swart (2004) argue that the SP is as-pectually ?transparent?, meaning that it applies to34all types of events and it preserves their aspectualclass.The difficulty for the MT systems is thusto choose correctly among the three above-mentioned tenses in French, which are all validpossibilities of translating the English SP.
WhenMT systems fail to generate the correct tense inFrench, several levels of incorrectness may oc-cur, exemplified in Figure 2 with sentences takenfrom the data used in this paper (see Section 3 andGrisot and Cartoni (2012)).1.
In certain contexts, tenses may be quite inter-changeable, which is the unproblematic casefor machine translation, depending also onthe evaluation measure.
In Example 1 fromFigure 2, the verb e?taient conside?re?es (wereseen) in IMP has a focus on temporal lengthwhich is preserved even if the translated tenseis a PC (ont e?te?
conside?re?es, i.e.
have beenseen) thanks to the adverb toujours (always)).2.
In other contexts, the tense proposed by theMT system can sound strange but remains ac-ceptable.
For instance, in Example 2, thereis a focus on temporal length with the IMPtranslation (voyait, viewed) but this meaningis not preserved if a PC is used (a vu, hasviewed) though it can be recovered by thereader.3.
The tense output by an MT system may begrammatically wrong.
In Example 3, the PCa renouvele?
(has renewed) cannot replace theIMP renouvelaient (renewed) because of theconflict with the imperfective meaning con-veyed by the adverbial sans cesse (again andagain).4.
Finally, a wrong tense in the MT output canbe misleading, if it does not convey the mean-ing of the source text but remains unnoticedby the reader.
In Example 4, using the PCa e?te?
leads to the interpretation that the per-son was no longer involved when he died,whereas using IMP e?tait implies that he wasstill involved, which may trigger very differ-ent expectations in the mind of the reader(e.g.
on the possible cause of the death, or itsimportance to the peace process).1.
EN: Although the US viewed Musharraf as an agentof change, he has never achieved domestic political legiti-macy, and his policies were seen as rife with contradictions.FR: Si les Etats-Unis voient Moucharraf comme un agentde changement, ce dernier n?est jamais parvenu a` avoirune le?gitimite?
dans son propre pays, ou` ses politiques onttoujours e?te?
conside?re?es (PC) / e?taient conside?re?es (IMP)comme un tissu de contradictions.2.
EN: Indeed, she even persuaded other importantpolitical leaders to participate in the planned January 8election, which she viewed as an opportunity to challengereligious extremist forces in the public square.FR: Benazir Bhutto a me?me convaincu d?autres dirigeantsde participer aux e?lections pre?vues le 8 janvier, qu?elle voy-ait (IMP) / ?a vu (PC) comme une occasion de s?opposeraux extre?mistes religieux sur la place publique.3.
EN: The agony of grief which overpowered themat first, was voluntarily renewed, was sought for, wascreated again and again...FR: Elles s?encourage`rent l?une l?autre dans leur affliction,la renouvelaient (IMP) / l?
*a renouvele?
(PC) volontaire-ment, et sans cesse...4.
EN: Last week a person who was at the heart ofthe peace process passed away.FR: La semaine passe?e une personne qui e?tait (IMP) / ae?te?
(PC) au c?ur du processus de paix est de?ce?de?e.Figure 2: Examples of translations of the EnglishSP by an MT system, differing from the refer-ence translation: (1) unproblematic, (2) strangebut acceptable, (3) grammatically wrong (*), and(4) misleading.2.3 Verb Tenses in SMTModeling verb tenses for SMT has only recentlybeen addressed.
For Chinese/English translation,Gong et al(2012) built an n-gram-like sequencemodel that passes information from previouslytranslated main verbs onto the next verb so thatits tense can be more correctly rendered.
Tense ismorphologically not marked in Chinese, unlike inEnglish, where the verbs forms are modified ac-cording to tense (among other factors).
With sucha model, the authors improved translation by up to0.8 BLEU points.Conversely, in view of English/Chinese trans-lation but without implementing an actual trans-lation system, Ye et al(2007) used a classifierto generate and insert appropriate Chinese aspectmarkers that in certain contexts have to follow theChinese verbs but are not present in the Englishsource texts.For translation from English to German, Gojunand Fraser (2012) reordered verbs in the Englishsource to positions where they normally occur in35German, which usually amounts to a long-distancemovement towards the end of clauses.
Reorderingwas implemented as rules on syntax trees and im-proved the translation by up to 0.61 BLEU points.In this paper, as SMT training needs a largeamount of data, we use an automatic classifier totag instances of English SP verbs with narrativitylabels.
The labels output by this classifier are thenmodeled when training the SMT system.3 Automatic Labeling of Narrativity3.1 DataA training set of 458 and a test set of 118 EnglishSP verbs that were manually annotated with narra-tivity labels (narrative or non-narrative) was pro-vided by Grisot and Cartoni (2012) (see their ar-ticle for more details about the data).
The train-ing set consists of 230 narrative and 228 non-narrative instances, the test set has 75 narrative in-stances and 43 non-narrative ones.
The sentencescome from parallel EN/FR corpora of four dif-ferent genres: literature, news, parliamentary de-bates and legislation.
For each instance, the En-glish sentence with the SP verb that must be clas-sified, as well as the previous and following sen-tences, had been given to two human annotators,who assigned a narrative or non-narrative label.
Toavoid interference with the translation into French,which could have provided clues about the label,the translations were not shown to annotators1.Annotators agreed over only 71% of the in-stances, corresponding to a kappa value of only0.44.
As this is at the lower end of the accept-able spectrum for discourse annotation (Carletta,1996), one of the important questions we ask inthis paper is: what can be achieved with this qual-ity of human annotation, in terms of an automaticnarrativity classifier (intrinsic performance) and ofits use for improving verb translation by SMT (ex-trinsic evaluation)?
It must be noted that instanceson which the two annotators had disagreed wereresolved (to either narrative or non-narrative) bylooking at the French human translation (an ac-ceptable method given that our purpose here istranslation into French), thus increasing the qual-ity of the annotation.1The goal was to focus on the narrativity property, regard-less of its translation.
However, annotations were adjudicatedalso by looking at the FR translation.
For a different ap-proach, considering exclusively the tense in translation, seethe discussion in Section 5.Model Recall Prec.
F1 ?MaxEnt 0.71 0.72 0.71 +0.43CRF 0.30 0.44 0.36 ?0.44Table 1: Performance of MaxEnt and CRF clas-sifiers on narrativity.
We report recall, precision,their mean (F1), and the kappa value for classagreement.3.2 Features for NarrativityThe manually annotated instances were used fortraining and testing a Maximum Entropy classi-fier using the Stanford Classifier package (Man-ning and Klein, 2003).
We extracted the followingfeatures from the sentence containing the verb toclassify and the preceding sentence as well, thusmodeling a wider context than the one modeled byphrase-based SMT systems.
For each verb form,we considered its POS tag and syntactical cate-gory, including parents up to the first verbal phrase(VP) parent node, as generated by Charniak andJohnson?s constituent parser (2005).
This parseralso assigns special tags to auxiliary (AUX) andmodal verbs (MD), which we include in the fea-tures.We further used a TimeML parser, the TarsqiToolkit (Verhagen et al 2005; Verhagen andPustejovsky, 2008), which automatically outputsan XML-like structure of the sentence, with a hy-pothesis on the temporal ordering of the eventsmentioned.
From this structure we extract eventmarkers such as PAST-OCCURRENCE and aspec-tual information such as STATE.Temporal ordering is often also signaled byother markers such as adverbials (e.g., three weeksbefore).
We manually gathered a list of 66 suchtemporal markers and assigned them, as an addi-tional feature, a label indicating whether they sig-nal synchrony (e.g., meanwhile, at the same time)or asynchrony (e.g., before, after).3.3 Results of Narrativity LabelingWith the above features, we obtained the classi-fication performance indicated in Table 1.
TheMaxEnt classifier reached 0.71 F1 score, which issimilar to the human annotator?s agreement level.Moreover, the kappa value for inter-class agree-ment was 0.43 between the classifier and the hu-man annotation, a value which is also close to thekappa value for the two human annotators.
In asense, the classifier thus reaches the highest scores36that are still meaningful, i.e.
those of inter-coderagreement.
As a baseline for comparison, the ma-jority class in the test set (the ?narrative?
label)would account for 63.56% of correctly classifiedinstances, whereas the classifier correctly labeled72.88% of all test instances.For further comparison we built a CRFmodel (Lafferty et al 2001) in order to label nar-rativity in sequence of other tags, such as POS.The CRF uses as features the two preceding POStags to label the next POS tag in a sequence ofwords.
The same training set of 458 sentencesas used above was POS-tagged using the Stan-ford POS tagger (Toutanova et al 2003), with theleft3words-distsim model.
We replacedthe instances of ?VBD?
(the POS tag for SP verbs)with the narrativity labels from the manual annota-tion.
The same procedure was then applied to the118 sentences of the test set on which CRF wasevaluated.Overall, the CRF model only labeled narrativitycorrectly at an F1 score of 0.36, while kappa hada negative value signaling a weak inverse correla-tion.
Therefore, it appears that the temporal andsemantic features used for the MaxEnt classifierare useful and account for the much higher per-formance of MaxEnt, which is used in the SMTexperiments described below.We further evaluate the MaxEnt classifier byproviding in Table 2 the confusion matrix of theautomatically obtained narrativity labels over thetest set.
Labeling non-narrative uses is slightlymore prone to errors (32.6% error rate) than nar-rative ones (24% errors), likely due to the largernumber of narratives vs. non-narratives in thetraining and the test data.SystemReference Narr.
Non-narr.
TotalNarrative 57 18 75Non-narr.
14 29 43Total 71 47 118Table 2: Confusion matrix for the labels outputby the MaxEnt classifier (System) versus the goldstandard labels (Reference).4 SMT with Narrativity Labels4.1 MethodTwo methods to use labels conveying to SMT in-formation about narrativity were explored (thoughmore exist).
First, as in our initial studies ap-plied to discourse connectives, the narrativity la-bels were simply concatenated with the SP verbform in EN (Meyer and Popescu-Belis, 2012) ?see Example 2 in Figure 3.
Second, we usedfactored translation models (Koehn and Hoang,2007), which allow for any linguistic annotationto be considered as additional weighted featurevectors, as in our later studies with connectives(Meyer et al 2012).
These factors are log-linearlycombined with the basic features of phrase-basedSMT models (phrase translation, lexical and lan-guage model probabilities).To assess the performance gain of narrativity-augmented systems, we built three different SMTsystems, with the following names and configura-tions:?
BASELINE: plain text, no verbal labels.?
TAGGED: plain text, all SP verb forms con-catenated with a narrativity label.?
FACTORED: all SP verbs have narrativitylabels as source-side translation factors (allother words labeled ?null?).1.
BASELINE SMT: on wednesday the c?ssd de-clared the approval of next year?s budget to be asuccess.
the people?s party was also satisfied.2.
TAGGED SMT: on wednesday the c?ssddeclared-Narrative the approval of next year?sbudget to be a success.
the people?s party was-Non-narrative also satisfied.3.
FACTORED SMT: on wednesday the c?ssddeclared|Narrative the approval of next year?sbudget to be a success.
the people?s partywas|Non-narrative also satisfied.Figure 3: Example input sentence from ?newstest2010?
data for three translation models: (1) plaintext; (2) concatenated narrativity labels; (3) narra-tivity as translation factors (the ?|null?
factors onother words were omitted for readability).Figure 3 shows an example input sentence forthese configurations.
For the FACTORED SMTmodel, both the EN source word and the factor37information are used to generate the FR surfacetarget word forms.
The tagged or factored annota-tions are respectively used for the training, tuningand test data as well.For labeling the SMT data, no manual annota-tion is used.
In a first step, the actual EN SP verbsto be labeled are identified using the Stanford POStagger, which assigns a ?VBD?
tag to each SP verb.These tags are replaced, after feature extractionand execution of the MaxEnt classifier, by the nar-rativity labels output by the latter.
Of course, thePOS tagger and (especially) our narrativity clas-sifier may generate erroneous labels which in theend lead to translation errors.
The challenge isthus to test the improvement of SMT with respectto the baseline, in spite of the noisy training andtest data.4.2 DataIn all experiments, we made use of parallel En-glish/French training, tuning and testing data fromthe translation task of the Workshop on MachineTranslation (www.statmt.org/wmt12/).?
For training, we used Europarl v6 (Koehn,2005), original EN2 to translated FR(321,577 sentences), with 66,143 instancesof SP verbs labeled automatically: 30,452are narrative and 35,691 are non-narrative.?
For tuning, we used the ?newstest 2011?
tun-ing set (3,003 sentences), with 1,401 auto-matically labeled SP verbs, of which 807 arenarrative and 594 non-narrative.?
For testing, we used the ?newstest 2010?
data(2,489 sentences), with 1,156 automaticallylabeled SP verbs (621 narrative and 535 non-narrative).We built a 5-gram language model with SRILM(Stolcke et al 2011) over the entire FR part of Eu-roparl.
Tuning was performed by Minimum ErrorRate Training (MERT) (Och, 2003).
All transla-tion models were phrase-based using either plaintext (possibly with concatenated labels) or fac-tored training as implemented in the Moses SMTtoolkit (Koehn et al 2007).2We only considered texts that were originally authoredin English, not translated into it from French or a third-partylanguage, to ensure only proper tenses uses are observed.
Therelevance of this constraint is discussed for connectives byCartoni et al(2011).4.3 Results: Automatic EvaluationIn order to obtain reliable automatic evaluationscores, we executed three runs of MERT tuning foreach type of translation model.
With MERT beinga randomized, non-deterministic optimization pro-cess, each run leads to different feature weightsand, as a consequence, to different BLEU scoreswhen translating unseen data.Table 3 shows the average BLEU and TERscores on the ?newstest 2010?
data for the threesystems.
The scores are averages over the threetuning runs, with resampling of the test set,both provided in the evaluation tool by Clarket al(2011) (www.github.com/jhclark/multeval).
BLEU is computed using jBLEUV0.1.1 (an exact reimplementation of NIST?s?mteval-v13.pl?
script without tokenization).
TheTranslation Error Rate (TER) is computed withversion 0.8.0 of the software (Snover et al 2006).A t-test was used to compute p values that indicatethe significance of differences in scores.Translation model BLEU TERBASELINE 21.4 61.9TAGGED 21.3 61.8FACTORED 21.6* 61.7*Table 3: Average values of BLEU (the higher thebetter) and TER (the lower the better) over threetuning runs for each model on ?newstest 2010?.The starred values are significantly better (p <0.05) than the baseline.In terms of overall BLEU and TER scores, theFACTORED model improves performance over theBASELINE by +0.2 BLEU and -0.2 TER (as loweris better), and these differences are statisticallysignificant at the 95% level.
On the contrary,the concatenated-label model (noted TAGGED)slightly decreases the global translation perfor-mance compared to the BASELINE.
A similarbehavior was observed when using labeled con-nectives in combination with SMT (Meyer et al2012).The lower scores of the TAGGED model maybe due to the scarcity of data (by a factor of 0.5)when verb word-forms are altered by concatenat-ing them with the narrativity labels.
The smallimprovement by the FACTORED model of over-all scores (such as BLEU) is also related to thescarcity of SP verbs: although their translation is38improved, as we will now show, the translation ofall other words is not changed by our method, soonly a small fraction of the words in the test dataare changed.4.4 Results: Human EvaluationTo assess the improvement specifically due to thenarrativity labels, we manually evaluated the FRtranslations by the FACTORED model for the 207first SP verbs in the test set against the transla-tions from the BASELINE model.
As the TAGGEDmodel did not result in good scores, we did not fur-ther consider it for evaluation.
Manual scoring wasperformed along the following criteria for each oc-currence of an SP verb, by bilingual judges look-ing both at the source sentence and its referencetranslation.?
Is the narrativity label correct?
(?correct?
or?incorrect?)
?
this is a direct evaluation of thenarrativity classifier from Section 3?
Is the verb tense of the FACTORED modelmore accurate than the BASELINE one?
(noted ?+?
if improved, ?=?
if similar, ???
ifdegraded)?
Is the lexical choice of the FACTORED modelmore accurate than the BASELINE one, re-gardless of the tense?
(again noted ?+?
or ?=?or ???)?
Is the BASELINE translation of the verbphrase globally correct?
(?correct?
or ?incor-rect?)?
Is the FACTORED translation of the verbphrase globally correct?
(?correct?
or ?incor-rect?
)Tables 4 and 5 summarize the counts and per-centages of improvements and/or degradations oftranslation quality with the systems FACTOREDand BASELINE.
The correctness of the labels, asevaluated by the human judges on SMT test data,is similar to the values given in Section 3 whenevaluated against the test sentences of the narra-tivity classifier.
As shown in Table 4, the narrativ-ity information clearly helps the FACTORED sys-tem to generate more accurate French verb tensesin almost 10% of the cases, and also helps to findmore accurate vocabulary for verbs in 3.4% of thecases.
Overall, as shown in Table 5, the FAC-TORED model yields more correct translations ofthe verb phrases than the BASELINE in 9% of thecases ?
a small but non-negligible improvement.Criterion Rating N. % ?Labeling correct 147 71.0incorrect 60 29.0Verb + 35 17.0tense = 157 75.8 +9.7?
15 7.2Lexical + 19 9.2choice = 176 85.0 +3.4?
12 5.8Table 4: Human evaluation of verb translationsinto French, comparing the FACTORED modelagainst the BASELINE.
The ?
values show theclear improvement of the narrativity-aware fac-tored translation model.System Rating Number %BASELINE correct 94 45.5incorrect 113 54.5FACTORED correct 113 54.5incorrect 94 45.5Table 5: Human evaluation of the global cor-rectness of 207 translations of EN SP verbs intoFrench.
The FACTORED model yields 9% morecorrect translations than the BASELINE one.An example from the test data shown in Fig-ure 4 illustrates the improved verb translation.
TheBASELINE system translates the SP verb lookedincorrectly into the verb conside?rer (consider), inwrong number and its past participle only (con-side?re?s, plural).
The FACTORED model generatesthe correct tense and number (IMP, semblait, sin-gular) and the better verb sembler (look, appear).This example is scored as follows: the labeling iscorrect (?yes?
), the tense was improved (?+?
), thelexical choice was improved too (?+?
), the BASE-LINE was incorrect while the FACTORED modelwas correct.5 Discussion and Future WorkWhen looking in detail through the translationsthat were degraded by the FACTORED model,some were due to the POS tagging used to findthe EN SP verbs to label.
For verb phrases madeof an auxiliary verb in SP and a past participle(e.g.
was born), the POS tagger outputs was/VBDborn/VBN.
As a consequence, our classifier onlyconsiders was, as non-narrative, although was39EN: tawa hallae looked|Non-narrative likemany other carnivorous dinosaurs.FR BASELINE: tawa hallae *conside?re?scomme de nombreuses autres carnivores di-nosaures.FR FACTORED: tawa hallae semblait commede nombreux autres carnivores dinosaures.Figure 4: Example comparison of a baseline andimproved factored translation.
The ?|null?
factorsin EN were omitted for readability.
See the textfor a discussion.born as a whole is a narrative event.
This canthen result in wrong FR tense translations.
Forinstance, the fragment nelson mandela was|Non-narrative born on .
.
.
is translated as: nelson man-dela *e?tait ne?
en .
.
.
, which in FR is pluperfecttense instead of the correct Passe?
Compose?
est ne?as in the reference translation.
A method to con-catenate such verb phrases to avoid such errors isunder work.A further reason for the small improvements intranslation quality might be that factored transla-tion models still operate on rather local context,even when the narrativity information is present.To widen the context captured by the translationmodel, labeling entire verbal phrase nodes in hi-erarchical or tree-based syntactical models will beconsidered in the future.
Moreover, it has beenshown that it is difficult to choose the optimal pa-rameters for a factored translation model (Tam-chyna and Bojar, 2013).In an alternative approach currently under work,a more direct way to label verb tense is imple-mented, where a classifier can make use of thesame features as those extracted here (in Sec-tion 3.2), but its classes are those that directlyindicate which target verb tense should be out-put by the SMT.
Thus, not only SP verbs canbe considered and no intermediate category suchas narrativity (that is more difficult to learn) isneeded.
The classifier will predict which FR tenseshould be used depending on the context of theEN verbs, for which the FR tense label can beannotated as above, within a factored translationmodel.
Through word alignment and POS tag-ging, this method has the additional advantage ofproviding much more training data, extracted fromword alignment of the verb phrases, and can beapplied to all tenses, not only SP.
Moreover, theapproach is likely to learn which verbs are prefer-ably translated with which tense: for instance, theverb started is much more likely to become a com-mence?
(PC) in FR than to commenc?ait (IMP), dueto its meaning of a punctual event in time, ratherthan a continuous or repetitive one.6 ConclusionThe paper presented a method to automatically la-bel English verbs in Simple Past tense with a bi-nary pragmatic feature, narrativity, which helpsto distinguish temporally ordered events that hap-pened in the past (?narrative?)
from past states ofaffairs (?non-narrative?).
A small amount of man-ually annotated data, combined with the extractionof temporal semantic features, allowed us to train aclassifier that reached 70% correctly classified in-stances.
The classifier was used to automaticallylabel the English SP verbs in a large parallel train-ing corpus for SMT systems.
When implement-ing the labels in a factored SMT model, translationinto French of the English SP verbs was improvedby about 10%, accompanied by a statistically sig-nificant gain of +0.2 BLEU points for the overallquality score.
In the future, we will improve theprocessing of verb phrases, and study a classifierwith labels that are directly based on the target lan-guage tenses.AcknowledgmentsWe are grateful for the funding of this work to theSwiss National Science Foundation (SNSF) underthe COMTIS Sinergia Project, n. CRSI22 127510(see www.idiap.ch/comtis/).
We wouldalso like to thank the anonymous reviewers fortheir helpful suggestions.ReferencesJean Carletta.
1996.
Assessing Agreement on Classi-fication Tasks: The Kappa Statistic.
ComputationalLinguistics, 22:249?254.Bruno Cartoni, Sandrine Zufferey, Thomas Meyer, andAndrei Popescu-Belis.
2011.
How Comparableare Parallel Corpora?
Measuring the Distribution ofGeneral Vocabulary and Connectives.
In Proceed-ings of 4th Workshop on Building and Using Compa-rable Corpora (BUCC), pages 78?86, Portland, OR.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best Parsing and MaxEnt Discriminative40Reranking.
In Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguistics(ACL), pages 173?180, Ann Arbor, MI.Jonathan Clark, Chris Dyer, Alon Lavie, and NoahSmith.
2011.
Better Hypothesis Testing for Statisti-cal Machine Translation: Controlling for OptimizerInstability.
In Proceedings of ACL-HLT 2011 (46thAnnual Meeting of the ACL: Human Language Tech-nologies), Portland, OR.Francis Corblin and Henrie?tte de Swart.
2004.
Hand-book of French Semantics.
CSLI Publications, Stan-ford, CA.Anita Gojun and Alexander Fraser.
2012.
Determin-ing the Placement of German Verbs in English-to-German SMT.
In Proceedings of the 13th Confer-ence of the European Chapter of the Association forComputational Linguistics (EACL), pages 726?735,Avignon, France.Zhengxian Gong, Min Zhang, Chew Lim Tan, andGuodong Zhou.
2012.
N-Gram-Based TenseModels for Statistical Machine Translation.
InProceedings of the Joint Conference on EmpiricalMethods in Natural Language Processing (EMNLP)and Computational Natural Language Learning(CoNLL), pages 276?285, Jeju Island, Korea.Cristina Grisot and Bruno Cartoni.
2012.
Une de-scription bilingue des temps verbaux: e?tude con-trastive en corpus.
Nouveaux cahiers de linguistiquefranc?aise, 30:101?117.Philipp Koehn and Hieu Hoang.
2007.
FactoredTranslation Models.
In Proceedings of the JointConference on Empirical Methods in Natural Lan-guage Processing (EMNLP) and ComputationalNatural Language Learning (CoNLL), pages 868?876, Prague, Czech Republic.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-dra Constantin, and Evan Herbs.
2007.
Moses:Open Source Toolkit for Statistical Machine Trans-lation.
In Proceedings of 45th Annual Meeting of theAssociation for Computational Linguistics (ACL),Demonstration Session, pages 177?180, Prague,Czech Republic.Philipp Koehn.
2005.
Europarl: A Parallel Corpus forStatistical Machine Translation.
In Proceedings ofMT Summit X, pages 79?86, Phuket, Thailand.John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional Random Fields: Prob-abilistic Models for Segmenting and Labeling Se-quence Data.
The Journal of Machine Learning Re-search, 8:693?723.Christopher Manning and Dan Klein.
2003.
Optimiza-tion, MaxEnt Models, and Conditional Estimationwithout Magic.
In Tutorial at HLT-NAACL and 41stACL conferences, Edmonton, Canada and Sapporo,Japan.Robert Martin.
1971.
Temps et aspect: essai surl?emploi des temps narratifs en moyen franc?ais.Klincksieck, Paris, France.Thomas Meyer and Andrei Popescu-Belis.
2012.
Us-ing Sense-labeled Discourse Connectives for Statis-tical Machine Translation.
In Proceedings of theEACL 2012 Joint Workshop on Exploiting Synergiesbetween IR and MT, and Hybrid Approaches to MT(ESIRMT-HyTra), pages 129?138, Avignon, FR.Thomas Meyer, Andrei Popescu-Belis, Najeh Hajlaoui,and Andrea Gesmundo.
2012.
Machine Translationof Labeled Discourse Connectives.
In Proceedingsof the Tenth Biennial Conference of the Associationfor Machine Translation in the Americas (AMTA),San Diego, CA.Franz Josef Och.
2003.
Minimum Error Rate Trainingin Statistical Machine Translation.
In Proceedingsof the 41st Annual Meeting of the Association forComputational Linguistics (ACL), pages 160?167,Sapporo, Japan.Randolph Quirk, Sidney Greenbaum, Geoffrey Leech,and Jan Svartvik.
1986.
A Comprehensive Gram-mar of the English Language.
Pearson Longman,Harlow, UK.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A Studyof Translation Edit Rate with Targeted Human An-notation.
In Proceedings of the Tenth Biennial Con-ference of the Association for Machine Translationin the Americas (AMTA), Cambridge, MA.Andreas Stolcke, Jing Zheng, Wen Wang, and VictorAbrash.
2011.
SRILM at Sixteen: Update andOutlook.
In Proceedings of the IEEE AutomaticSpeech Recognition and Understanding Workshop,Waikoloa, Hawaii.Ales?
Tamchyna and Ondr?ej Bojar.
2013.
No FreeLunch in Factored Phrase-Based Machine Transla-tion.
In Proceedings of the 14th International Con-ference on Computational Linguistics and Intelli-gent Text Processing (CICLING), Samos, Greece.Kristina Toutanova, Dan Klein, Christopher Manning,and Yoram Singer.
2003.
Feature-rich Part-of-Speech Tagging with a Cyclic Dependency Net-work.
In Proceedings of Human Language Tech-nology Conference and the North American Chap-ter of the Association for Computational Linguistics(HLT-NAACL), pages 252?259, Edmonton, CA.Marc Verhagen and James Pustejovsky.
2008.
Tempo-ral Processing with the TARSQI Toolkit.
In Pro-ceedings of the 22nd International Conference onComputational Linguistics (COLING), Companionvolume: Demonstrations, pages 189?192, Manch-ester, UK.41Marc Verhagen, Inderjeet Mani, Roser Sauri, Jes-sica Littman, Robert Knippen, Seok Bae Jang,Anna Rumshisky, John Phillips, and James Puste-jovsky.
2005.
Automating Temporal Annotationwith TARSQI.
In Proceedings of the 43th AnnualMeeting of the Association for Computational Lin-guistics (ACL), Demo Session, pages 81?84, AnnArbor, USA.Yang Ye, Karl-Michael Schneider, and Steven Abney.2007.
Aspect Marker Generation for English-to-Chinese Machine Translation.
In Proceedings of MTSummit XI, pages 521?527, Copenhagen, Danmark.42
