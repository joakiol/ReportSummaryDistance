2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 142?151,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsStructured Perceptron with Inexact SearchLiang HuangInformation Sciences InstituteUniversity of Southern Californialiang.huang.sh@gmail.comSuphan FayongDept.
of Computer ScienceUniversity of Southern Californiasuphan.ying@gmail.comYang GuoBloomberg L.P.New York, NYyangg86@gmail.comAbstractMost existing theory of structured predictionassumes exact inference, which is often in-tractable in many practical problems.
Thisleads to the routine use of approximate infer-ence such as beam search but there is not muchtheory behind it.
Based on the structuredperceptron, we propose a general frameworkof ?violation-fixing?
perceptrons for inexactsearch with a theoretical guarantee for conver-gence under new separability conditions.
Thisframework subsumes and justifies the pop-ular heuristic ?early-update?
for perceptronwith beam search (Collins and Roark, 2004).We also propose several new update meth-ods within this framework, among which the?max-violation?
method dramatically reducestraining time (by 3 fold as compared to early-update) on state-of-the-art part-of-speech tag-ging and incremental parsing systems.1 IntroductionDiscriminative structured prediction algorithmssuch as conditional random fields (Lafferty et al,2001), structured perceptron (Collins, 2002), max-margin markov networks (Taskar et al, 2003), andstructural SVMs (Tsochantaridis et al, 2005) leadto state-of-the-art performance on many structuredprediction problems such as part-of-speech tagging,sequence labeling, and parsing.
But despite theirsuccess, there remains a major problem: these learn-ing algorithms all assume exact inference (over anexponentially-large search space), which is neededto ensure their theoretical properties such as conver-gence.
This exactness assumption, however, rarelyholds in practice since exact inference is often in-tractable in many important problems such as ma-chine translation (Liang et al, 2006), incremen-tal parsing (Collins and Roark, 2004; Huang andSagae, 2010), and bottom-up parsing (McDonaldand Pereira, 2006; Huang, 2008).
This leads toroutine use of approximate inference such as beamsearch as evidenced in the above-cited papers, butthe inexactness unfortunately abandons existing the-oretical guarantees of the learning algorithms, andbesides notable exceptions discussed below and inSection 7, little is known for theoretical propertiesof structured prediction under inexact search.Among these notable exceptions, many exam-ine how and which approximations break theoreticalguarantees of existing learning algorithms (Kuleszaand Pereira, 2007; Finley and Joachims, 2008), butwe ask a deeper and practically more useful ques-tion: can we modify existing learning algorithms toaccommodate the inexactness in inference, so thatthe theoretical properties are still maintained?For the structured perceptron, Collins and Roark(2004) provides a partial answer: they suggest vari-ant called ?early update?
for beam search, which up-dates on partial hypotheses when the correct solutionfalls out of the beam.
This method works signif-icantly better than standard perceptron, and is fol-lowed by later incremental parsers, for instance in(Zhang and Clark, 2008; Huang and Sagae, 2010).However, two problems remain: first, up till nowthere has been no theoretical justification for earlyupdate; and secondly, it makes learning extremelyslow as witnessed by the above-cited papers becauseit only learns on partial examples and often requires15?40 iterations to converge while normal percep-tron converges in 5?10 iterations (Collins, 2002).We develop a theoretical framework of ?violation-fixing?
perceptron that addresses these challenges.In particular, we make the following contributions:?
We show that, somewhat surprisingly, exact142search is not required by perceptron conver-gence.
All we need is that each update involvesa ?violation?, i.e., the 1-best sequence has ahigher model score than the correct sequence.Such an update is considered a ?valid update?,and any perceptron variant that maintains thisis bound to converge.
We call these variants?violation-fixing perceptrons?
(Section 3.1).?
This theory explains why standard perceptronupdate may fail to work with inexact search,because violation is no longer guaranteed: thecorrect structure might indeed be preferred bythe model, but was pruned during the searchprocess (Sec.
3.2).
Such an update is thus con-sidered invalid, and experiments show that in-valid updates lead to bad learning (Sec.
6.2).?
We show that the early update is always validand is thus a special case in our framework; thisis the first theoretical justification for early up-date (Section 4).
We also show that (a variantof) LaSO (Daume?
and Marcu, 2005) is anotherspecial case (Section 7).?
We then propose several other update meth-ods within this framework (Section 5).
Experi-ments in Section 6 confirm that among them,the max-violation method can learn equal orbetter models with dramatically reduced learn-ing times (by 3 fold as compared to earlyupdate) on state-of-the-art part-of-speech tag-ging (Collins, 2002)1 and incremental parsing(Huang and Sagae, 2010) systems.
We alsofound strong correlation between search errorand invalid updates, suggesting that the ad-vantage of valid update methods is more pro-nounced with harder inference problems.Our techniques are widely applicable to other str-cutured prediction problems which require inexactsearch like machine translation and protein folding.2 Structured PerceptronWe review the convergence properties of the stan-dard structured perceptron (Collins, 2002) in our1Incidentally, we achieve the best POS tagging accuracy todate (97.35%) on English Treebank by early update (Sec.
6.1).Algorithm 1 Structured Perceptron (Collins, 2002).Input: data D = {(x(t), y(t))}nt=1 and feature map ?Output: weight vector wLet: EXACT(x,w) ?= argmaxs?Y(x) w ??
(x, s)Let: ??
(x, y, z) ?= ?
(x, y)??
(x, z)1: repeat2: for each example (x, y) in D do3: z ?
EXACT(x,w)4: if z 6= y then5: w?
w + ??
(x, y, z)6: until convergedown notations that will be reused in later sectionsfor non-exact search.
We first define a new concept:Definition 1.
The standard confusion set Cs(D)for training data D = {(x(t), y(t))}nt=1 is the set oftriples (x, y, z) where z is a wrong label for input x:Cs(D) ?= {(x, y, z) | (x, y) ?
D, z ?
Y(x)?
{y}}.The rest of the theory, including separation andviolation, all builds upon this concept.
We call sucha triple S = ?D,?, C?
a training scenario, andin the remainder of this section, we assume C =Cs(D), though later we will define other confusionsets to accommodate other update methods.Definition 2.
The training scenario S = ?D,?, C?is said to be linearly separable (i.e., dataset D islinearly separable in C by representation ?)
withmargin ?
> 0 if there exists an oracle vector uwith ?u?
= 1 such that it can correctly classifyall examples in D (with a gap of at least ?
), i.e.,?
(x, y, z) ?
C,u ?
??
(x, y, z) ?
?.
We definethe maximal margin ?
(S) to be the maximal suchmargin over all unit oracle vectors:?
(S) ?= max?u?=1min(x,y,z)?Cu ???
(x, y, z).Definition 3.
A triple (x, y, z) is said to be a vi-olation in training scenario S = ?D,?, C?
withrespect to weight vector w if (x, y, z) ?
C andw ???
(x, y, z) ?
0.Intuitively, this means model w is possible to mis-label example x (though not necessarily to z) sincey is not its single highest scoring label under w.Lemma 1.
Each update triple (x, y, z) in Algo-rithm 1 (line 5) is a violation in S = ?D,?, Cs(D)?.143Proof.
z = EXACT(x,w), thus for all z?
?
Y(x),w ??
(x, z) ?
w ??
(x, z?
), i.e., w ???
(x, y, z) ?
0.On the other hand, z ?
Y(x) and z 6= y, so(x, y, z) ?
Cs(D).This lemma basically says exact search guaran-tees violation in each update, but as we will see inthe convergence proof, violation itself is more fun-damental than search exactness.Definition 4.
The diameter R(S) for scenario S =?D,?, C?
is max(x,y,z)?C???
(x, y, z)?.Theorem 1 (convergence, Collins).
For a separabletraining scenario S = ?D,?, Cs(D)?
with ?
(S) >0, the perceptron algorithm in Algorithm 1 will makefinite number of updates (before convergence):err(S) ?
R2(S)/?2(S).Proof.
Let w(k) be the weight vector before the kthupdate; w(0) = 0.
Suppose the kth update happenson the triple (x, y, z).
We will bound ?w(k+1)?
fromtwo directions:1. w(k+1) = w(k) + ??
(x, y, z).
Since scenarioS is separable with max margin ?
(S), there ex-ists a unit oracle vector u that achieves thismargin.
Dot product both sides with u, we haveu ?w(k+1) = u ?w(k) + u ???
(x, y, z)?
u ?w(k) + ?
(S)by Lemma 1 that (x, y, z) ?
Cs(D) and by thedefinition of margin.
By induction, we haveu ?
w(k+1) ?
k?(S).
Since for any two vec-tors a and b we have ?a??b?
?
a ?
b, thus?u??w(k+1)?
?
u ?w(k+1) ?
k?(S).
As u isa unit vector, we have ?w(k+1)?
?
k?(S).2.
On the other hand, since ?a + b?2 = ?a?2 +?b?2 +2 a ?b for any vectors a and b,we have?w(k+1)?2 = ?w(k) + ??
(x, y, z)?2= ?w(k)?2 + ???
(x, y, z)?2+ 2 w(k) ???
(x, y, z)?
?w(k)?2 + R2(S) + 0 .By Lemma 1, the update triple is a violation sothat w(k) ???
(x, y, z) ?
0, and that (x, y, z) ?Cs(D) thus ???
(x, y, z)?2 ?
R2(S) by thedefinition of diameter.
By induction, we have?w(k+1)?2 ?
kR2(S).Algorithm 2 Local Violation-Fixing Perceptron.Input: training scenario S = ?D,?, C?1: repeat2: for each example (x, y) in D do3: (x, y?, z) = FINDVIOLATION(x, y,w)4: if z 6= y then ?
(x, y?, z) is a violation5: w?
w + ??
(x, y?, z)6: until convergedCombining the two bounds, we have k2?2(S) ?
?w(k+1)?2 ?
kR2(S), thus k ?
R2(S)/?2(S).3 Violation is All We NeedWe now draw the central observation of this workfrom part 2 of the above proof: note that exact search(argmax) is not required in the proof, instead, itjust needs a violation, which is a much weaker con-dition.2 Exact search is simply used to ensure viola-tion.
In other words, if we can guarantee violation ineach update (which we call ?valid update?
), it doesnot matter whether or how exact the search is.3.1 Violation-Fixing PerceptronThis observation leads us to two generalized vari-ants of perceptron which we call ?violation-fixingperceptrons?.
The local version, Algorithm 2 stillworks on one example at a time, and searches forone violation (if any) in that example to update with.The global version, Algorithm 3, can update on anyviolation in the dataset at any time.
We state the fol-lowing generalized theorem:Theorem 2.
For a separable training scenario Sthe perceptrons in Algorithms 2 and 3 both con-verge with the same update bounds of R2(S)/?2(S)as long as the FINDVIOLATION and FINDVIO-LATIONINDATA functions always return violationtriples if there are any.Proof.
Same as the proof to Theorem 1, except forreplacing Lemma 1 in part 2 by the fact that the up-date triples are guaranteed to be violations.
(Note aviolation triple is by definition in the confusion set,thus we can still use separation and diameter).These generic violation-fixing perceptron algo-rithms can been seen as ?interfaces?
(or APIs),2Crammer and Singer (2003) further demonstrates that aconvex combination of violations can also be used for update.144Algorithm 3 Global Violation-Fixing Perceptron.Input: training scenario S = ?D,?, C?1: repeat2: (x, y, z)?
FINDVIOLATIONINDATA(C,w)3: if x = ?
then break ?
no more violation?4: w?
w + ??
(x, y, z)5: until convergeddata D = {(x, y)}:x fruit flies fly .y N N V .search space: Y(x) = {N} ?
{N, V} ?
{N, V} ?
{.
}.feature map: ?
(x, y) = (#N?N(y), #V?.
(y)).iter label z ??
(x, y, z) w???
new w0 (0, 0)1 N N N .
(?1,+1) 0 ?
(?1, 1)2 N V N .
(+1,+1) 0 ?
(0, 2)3 N N N .
(?1,+1) 2 ?
(?1, 3)4 N V N .
(+1,+1) 2 ?
(0, 4)... infinite loop ...Figure 1: Example that standard perceptron does notconverge with greedy search on a separable scenario(e.g.
u = (1, 2) can separate D with exact search).where later sections will supply different implemen-tations of the FINDVIOLATION and FINDVIOLA-TIONINDATA subroutines, thus establishing alterna-tive update methods for inexact search as specialcases in this general framework.3.2 Non-Convergence with Inexact SearchWhat if we can not guarantee valid updates?
Well,the convergence proof in Theorems 1 and 2 wouldbreak in part 2.
This is exactly why standard struc-tured perceptron may not work well with inexactsearch: with search errors it is no longer possibleto guarantee violation in each update.
For example,an inexact search method explores a (proper) subsetof the search space Y ?w(x) ( Y(x), and finds a labelz = argmaxs?Y ?w(x)w ??
(x, s).It is possible that the correct label y is outside ofthe explored subspace, and yet has a higher score:??
(x, y, z) > 0 but y /?
Y ?w(x).
In this case,(x, y, z) is not a violation, which breaks the proof.We show below that this situation actually exists.Algorithm 4 Greedy Search.Let: NEXT(x, z) ?= {z ?
a | a ?
Y|z|+1(x)} ?
set ofpossible one-step extensions (successors)BEST(x, z,w) ?= argmaxz?
?NEXT(x,z) w ?
?
(x, z?)?
best one-step extension based on history1: function GREEDYSEARCH(x,w)2: z ?
?
?
empty sequence3: for i ?
1 .
.
.
|x| do4: z ?
BEST(x, z,w)5: return zTheorem 3.
For a separable training scenario S =?D,?, Cs(D)?, if the argmax in Algorithm 1 is notexact, the perceptron might not converge.Proof.
See the example in Figure 1.This situation happens very often in NLP: of-ten the search space Y(x) is too big either becauseit does not factor locally, or because it is still toobig after factorization, which requires some approxi-mate search.
In either case, updating the model w ona non-violation (i.e., ?invalid?)
triple (x, y, z) doesnot make sense: it is not the model?s problem: wdoes score the correct label y higher than the incor-rect z; rather, it is a problem of the search, or itsinteraction with the model that prunes away the cor-rect (sub)sequence during the search.What shall we do in this case?
Collins andRoark (2004) suggest that instead of the standardfull update, we should only update on the prefix(x, y[1:i], z[1:i]) up to the point i where the correctsequence falls off the beam.
This intuitively makesa lot of sense, since up to i we can still guaranteeviolation, but after i we may not.
The next sectionformalizes this intuition.4 Early Update is Violation-FixingWe now proceed to show that early update is alwaysvalid and it is thus a special case of the violation-fixing perceptron framework.
First, let us study thesimplest special case, greedy search (beam=1).4.1 Greedy Search and Early UpdateGreedy search is the simplest form of inexact search.Shown in Algorithm 4, at each position, we com-mit to the single best action (e.g., tag for the currentword) given the previous actions and continue to the145?
?
?
?
?
?
???
update ??
skip ?
?Figure 2: Early update at the first error in greedy search.Algorithm 5 Early update for greedy search adaptedfrom Collins and Roark (2004).Input: training scenario S = ?D,?, Cg(D)?1: repeat2: for each example (x, y) in D do3: z ?
?
?
empty sequence4: for i ?
1 .
.
.
|x| do5: z ?
BEST(x, z,w)6: if zi 6= yi then ?
first wrong action7: w?
w + ??
(x, y[1:i], z) ?
early update8: break ?
skip the rest of this example9: until convergednext position.
The notation Yi(x) denotes the set ofpossible actions at position i for example x (for in-stance, the set of possible tags for a word).The early update heuristic, originally proposed forbeam search (Collins and Roark, 2004), now simpli-fies into ?update at the first wrong action?, since thisis exactly the place where the correct sequence fallsoff the singleton beam (see Algorithm 5 for pseu-docode and Fig.
2).
Informally, it is easy to show(below) that this kind of update is always a valid vi-olation, but we need to redefine confusion set.Definition 5.
The greedy confusion set Cg(D) fortraining data D = {(x(t), y(t))}nt=1 is the set oftriples (x, y[1:i], z[1:i]) where y[1:i] is a i-prefix of thecorrect label y, and z[1:i] is an incorrect i-prefix thatagrees with the correct prefix on all decisions exceptthe last one:Cg(D) ?= {(x, y[1:i], z[1:i]) | (x, y, z) ?
Cs(D),1 ?
i ?
|y|, z[1:i?1] = y[1:i?1], zi 6= yi}.We can see intuitively that this new defintionis specially tailored to the early updates in greedysearch.
The concepts of separation/margin, viola-tion, and diameter all change accordingly with thisnew confusion set.
In particular, we say that adataset D is greedily separable in representation?
if and only if ?D,?, Cg(D)?
is linearly separa-ble, and we say (x, y?, z?)
is a greedy violation if(x, y?, z?)
?
Cg(D) and w ???
(x, y?, z?)
?
0.Algorithm 6 Alternative presentation of Alg.
5 as aLocal Violation-Fixing Perceptron (Alg.
2).1: function FINDVIOLATION(x, y,w)2: z ?
?
?
empty sequence3: for i ?
1 .
.
.
|x| do4: z ?
BEST(x, z,w)5: if zi 6= yi then ?
first wrong action6: return (x, y[1:i], z) ?
return for early update7: return (x, y, y) ?
success (z = y), no violationWe now express early update for greedy search(Algorithm 5) in terms of violation-fixing percep-tron.
Algorithm 6 implements the FINDVIOLATIONfunction for the generic Local Violation-Fixing Per-ceptron in Algorithm 2.
Thus Algorithm 5 is equiv-alent to Algorithm 6 plugged into Algorithm 2.Lemma 2.
Each triple (x, y[1:i], z) returned at line 6in Algorithm 6 is a greedy violation.Proof.
Let y?
= y[1:i].
Clearly at line 6, |y?| = i =|z| and y?i 6= zi.
But y?j = zj for all j < i otherwise itwould have returned before position i, so (x, y?, z) ?Cg(D).
Also z = BEST(x, z,w), so w ??
(x, z) ?w ??
(x, y?
), thus w ???
(x, y?, z) ?
0.Theorem 4 (convergence of greedy search withearly update).
For a separable training scenarioS = ?D,?, Cg(D)?, the early update perceptronby plugging Algorithm 6 into Algorithm 2 will makefinite number of updates (before convergence):err(S) < R2(S)/?2(S).Proof.
By Lemma 2 and Theorem 2.4.2 Beam Search and Early UpdateTo formalize beam search, we need some notations:Definition 6 (k-best).
We denote argtopkz?Z f(z)to return (a sorted list of) the top k unique z in termsof f(z), i.e., it returns a list B = [z(1), z(2), .
.
.
, z(k)]where z(i) ?
Z and f(z(1)) ?
f(z(2)) ?
.
.
.
?f(z(k)) ?
f(z?)
for all z?
?
Z ?
B.By unique we mean that no two elements areequivalent with respect to some equivalence relation,i.e., z(i) ?
z(j) ?
i = j.
This equivalence rela-tion is useful for dynamic programming (when usedwith beam search).
For example, in trigram tagging,two tag sequences are equivalent if they are of the146Algorithm 7 Beam-Search.BESTk(x,B,w) ?= argtopkz??
?z?BNEXT(z) w ??
(x, z?)?
top k (unique) extensions from the beam1: function BEAMSEARCH(x,w, k) ?
k is beam width2: B0 ?
[?]
?
initial beam3: for i ?
1 .
.
.
|x| do4: Bi ?
BESTk(x,Bi?1,w)5: return B|x|[0] ?
best sequence in the final beamAlgorithm 8 Early update for beam search (Collinsand Roark 04) as Local Violation-Fixing Perceptron.1: function FINDVIOLATION(x, y,w)2: B0 ?
[?
]3: for i ?
1 .
.
.
|x| do4: Bi ?
BESTk(x,Bi?1,w)5: if y[1:i] /?
Bi then ?
correct seq.
falls off beam6: return (x, y[1:i],Bi[0]) ?
update on prefix7: return (x, y,B|x|[0]) ?
full update if wrong finalsame length and if they agree on the last two tags,i.e.
z ?
z?
iff.
|z| = |z?| and z|z|?1:|z| = z?|z|?1:|z|.
Inincremental parsing this equivalence relation couldbe relevant bits of information on the last few treeson the stack (depending on feature templates), assuggested in (Huang and Sagae, 2010).
3Algorithm 7 shows the pseudocode for beamsearch.
It is trivial to verify that greedy search isa special case of beam search with k = 1.
However,the definition of confusion set changes considerably:Definition 7.
The beam confusion set Cb(D) fortraining data D = {(x(t), y(t))}nt=1 is the set oftriples (x, y[1:i], z[1:i]) where y[1:i] is a i-prefix of thecorrect label y, and z[1:i] is an incorrect i-prefix thatdiffers from the correct prefix (in at least one place):Cb(D) ?= {(x, y[1:i], z[1:i]) | (x, y, z) ?
Cs(D),1 ?
i ?
|y|, z[1:i] 6= y[1:i]}.Similarly, we say that a dataset D is beamseparable in representation ?
if and only if3Note that when checking whether the correct sequencefalls off the beam (line 5), we could either store the whole(sub)sequence for each candidate in the beam (which is whatwe do for non-DP anyway), or check if the equivalence class ofthe correct sequence is in the beam, i.e.
Jy[1:i]K?
?
Bi, and ifits backpointer points to Jy[1:i?1]K?.
For example, in trigramtagging, we just check if ?yi?1, yi?
?
Bi and if its backpointerpoints to ?yi?2, yi?1?.earlymax-violationlatestfull(standard)best in the beamworst in the beamfalls offthe beam biggestviolationlast validupdatecorrect sequenceinvalidupdate!Figure 3: Illustration of various update methods: early,max-violation, latest, and standard (full) update, in thecase when standard update is invalid (shown in red).
Therectangle denotes the beam and the blue line segmentsdenote the trajectory of the correct sequence.
?D,?, Cb(D)?
is linearly separable, and we say(x, y?, z?)
is a beam violation if (x, y?, z?)
?
Cb(D)and w ???
(x, y?, z?)
?
0.It is easy to verify that beam confusion set is su-perset of both greedy and standard confusion sets:for all dataset D, Cg(D) ( Cb(D), and Cs(D) (Cb(D).
This means that beam separability is thestrongest condition among the three separabilities:Theorem 5.
If a dataset D is beam separable, thenit is also greedily and (standard) linear separable.We now present early update for beam search asa Local Violation Fixing Perceptron in Algorithm 8.See Figure 3 for an illustration.Lemma 3.
Each update (lines 6 or 7 in Algorithm 8)involves a beam violation.Proof.
Case 1: early update (line 6): Let z?
= Bi[0]and y?
= y[1:i].
Case 2: full update (line 8): Let z?
=B|x|[0] and y?
= y.
In both cases we have z?
6= y?and |z?| = |y?|, thus (x, y?, z?)
?
Cb(D).
Also wehave w ?
?
(x, z?)
?
w ?
?
(x, y?)
by defintion ofargtop, so w ???
(x, y?, z?)
?
0.Theorem 6 (convergence of beam search with earlyupdate).
For a separable training scenario S =?D,?, Cb(D)?, the early update perceptron byplugging Algorithm 8 into Algorithm 2 will make fi-nite number of updates (before convergence):err(S) < R2(S)/?2(S).Proof.
By Lemma 3 and Theorem 2.1475 New Update Methods for Inexact SearchWe now propose three novel update methods forinexact search within the framework of violation-fixing perceptron.
These methods are inspired byearly update but addresses its very limitation of slowlearning.
See Figure 3 for an illustration.1.
?hybrid?
update.
When the standard updateis valid (i.e., a violation), we perform it, other-wise we perform the early update.2.
?max-violation?
update.
While there aremore than one possible violations on one exam-ple x, we choose the triple that is most violated:(x, y?, z?)
= argmin(x,y?,z?)?C,z??
?i{Bi[0]}w ???
(x, y?, z?).3.
?latest?
update.
Contrary to early update, wecan also choose the latest point where the up-date is still a violation:(x, y?, z?)
= argmax(x,y?,z?)?C,z???i{Bi[0]},w???(x,y?,z?
)>0|z?|.All these three methods go beyond early updatebut can be represented in the Local Violation FixingPerceptron (Algorithm 2), and are thus all guaran-teed to converge.
As we will see in the experiments,these new methods are motivated to address the ma-jor limitation of early update, that is, it learns tooslowly since it only updates on prefixes and neglectthe rest of the examples.
Hybrid update is tryingto do as much standard (?full?)
updates as possible,and latest update further addresses the case whenstandard update is invalid: instead of backing-off toearly update, it uses the longest possible update.6 ExperimentsWe conduct experiments on two typical structuredlearning tasks: part-of-speech tagging with a trigrammodel where exact search is possible, and incremen-tal dependency parsing with arbitrary non-local fea-tures where exact search is intractable.
We run bothexperiments on state-of-the-art implementations.6.1 Part-of-Speech TaggingFollowing the standard split for part-of-speech tag-ging introduced by Collins (2002), we use sec-tions 00?18 of the Penn Treebank (Marcus et al,96.496.696.89797.21  2  3  4  5  6  7  8  9  10best taggingaccuracyonheld-outbeam sizemax-violationearlystandardFigure 4: POS tagging using beam search with variousupdate methods (hybrid/latest similar to early; omitted).b = 1 b = 2 b = 7method it dev it dev it devstandard 12 96.27 6 97.07 4 97.17early 13 96.97 6 97.15 7 97.19max-viol.
7 96.97 3 97.20 4 97.20Table 1: Convergence rate of part-of-speech tagging.
Ingeneral, max-violation converges faster and better thanearly and standard updates, esp.
in smallest beams.1993) for training, sections 19?21 as a held-outdevelopment set, and sections 22?24 for testing.Our baseline system is a faithful implementation ofthe perceptron tagger in Collins (2002), i.e., a tri-gram model with spelling features from Ratnaparkhi(1996), except that we replace one-count words as<unk>.
With standard perceptron and exact search,our baseline system performs slightly better thanCollins (2002) with a beam of 20 (M. Collins, p.c.
).We then implemented beam search on top of dy-namic programming and experimented with stan-dard, early, hybrid, and max-violation update meth-ods with various beam settings (b = 1, 2, 4, 7, 10).Figure 4(a) summarizes these experiments.
We ob-serve that, first of all, the standard update performspoorly with the smallest beams, esp.
at b = 1(greedy search), when search error is the most se-vere causing lots of invalid updates (see Figure 5).Secondly,max-violation is almost consistently thebest-performing method (except for b = 4).
Table 1shows convergence rates, where max-violation up-date also converges faster than early and standardmethods.
In particular, at b = 1, it achieves a 19%error reduction over standard update, while converg-14802550751002  4  6  8  10  12  14  16%of invalidupdatesbeam sizeparsingtaggingFigure 5: Percentages of invalid updates for standard up-date.
In tagging it quickly drops to 0% while in parsing itconverges to ?
50%.
This means search-wise, parsing ismuch harder than tagging, which explains why standardupdate does OK in tagging but terribly in parsing.
Theharder the search is, the more needed valid updates are.method b it time dev teststandard* ?
6 162 m 97.17 97.28early 4 6 37 m 97.22 97.35hybrid 5 5 30 m 97.18 97.19latest 7 5 45 m 97.17 97.13max-viol.
2 3 26 m 97.20 97.33standard 20 Collins (2002) 97.11guided 3 Shen et al (2007) 97.33Table 2: Final test results on POS tagging.
*:baseline.ing twice as fast as early update.4 This agrees withour intuition that by choosing the ?most-violated?triple for update, the perceptron should learn faster.Table 2 presents the final tagging results on thetest set.
For each of the five update methods, wechoose the beam size at which it achieves the high-est accuracy on the held-out.
For standard update, itsbest held-out accuracy 97.17 is indeed achieved byexact search (i.e., b = +?)
since it does not workwell with beam search, but it costs 2.7 hours (162minutes) to train.
By contrast, the four valid up-date methods handle beam search better.
The max-violation method achieves its highest held-out/testaccuracies of 97.20 / 97.33 with a beam size ofonly 2, and only 26 minutes to train.
Early up-date achieves the highest held-out/test accuracies of97.22 / 97.35 across all update methods at the beamsize of 4.
This test accuracy is even better than Shen4for tagging (but not parsing) the difference in per-iterationspeed between early update and max-violation update is small.method b it time dev testearly*838 15.4 h 92.24 92.09standard 1 0.4 h 78.99 79.14hybrid 11 5.6 h 92.26 91.95latest 9 4.5 h 92.06 91.96max-viol.
12 5.5 h 92.32 92.18early 8 Huang & Sagae 2010 92.1Table 3: Final results on incremental parsing.
*: baseline.9191.2591.591.759292.250  2  4  6  8  10  12  14  16  18parsingaccuracyonheld-outtraining time (hours)max-violationearlyFigure 6: Training progress curves for incremental pars-ing (b = 8).
Max-violation learns faster and better: ittakes 4.6 hours (10 iterations) to reach 92.25 on held-out,compared with early update?s 15.4 hours (38 iterations),even though the latter is faster in each iteration due toearly stopping (esp.
at the first few iterations).et al (2007), the best tagging accuracy reported onthe Penn Treebank to date.5,6 To conclude, withvalid update methods, we can learn a better taggingmodel with 5 times faster training than exact search.6.2 Incremental ParsingWhile part-of-speech tagging is mainly a proof ofconcept, incremental parsing is much harder sincenon-local features rules out exact inference.We use the standard split for parsing: secs 02?21 for training, 22 as held-out, and 23 for testing.Our baseline system is a faithful reimplementationof the beam-search dynamic programming parser ofHuang and Sagae (2010).
Like most incrementalparsers, it used early update as search error is severe.5according to ACL Wiki: http://aclweb.org/aclwiki/.6Note that Shen et al (2007) employ contextual featuresup to 5-gram which go beyond our local trigram window.
Wesuspect that adding genuinely non-local features would demon-strate even better the advantages of valid update methods withbeam search, since exact inference will no longer be tractable.149We first confirm that, as reported by Huang andSagae, early update learns very slowly, reaching92.24 on held-out with 38 iterations (15.4 hours).We then experimented with the other updatemethods: standard, hybrid, latest, and max-violation, with beam size b = 1, 2, 4, 8.
We foundthat, first of all, the standard update performs horri-bly on this task: at b = 1 it only achieves 60.04%on held-out, while at b = 8 it improves to 78.99%but is still vastly below all other methods.
This isbecause search error is much more severe in incre-mental parsing (than in part-of-speech tagging), thusstandard update produces an enormous amount ofinvalid updates even at b = 8 (see Figure 5).
Thissuggests that the advantage of valid update meth-ods is more pronounced with tougher search prob-lems.
Secondly, max-violation learns much faster(and better) than early update: it takes only 10 it-erations (4.6 hours) to reach 92.25, compared withearly update?s 15.4 hours (see Fig.
6).
At its peak,max-violation achieves 92.18 on test which is bet-ter than (Huang and Sagae, 2010).
To conclude, wecan train a parser with only 1/3 of training time withmax-violation update, and the harder the search is,the more needed the valid update methods are.7 Related Work and DiscussionsBesides the early update method of Collins andRoark (2004) which inspired us, this work is alsorelated to the LaSO method of Daume?
and Marcu(2005).
LaSO is similar to early update, except thatafter each update, instead of skipping the rest of theexample, LaSO continues on the same example withthe correct hypothesis.
For example, in the greedycase LaSO is just replacing the break statement inAlgorithm 5 by8?
: zi = yiand in beam search it is replacing it with8?
: Bi = [y[1:i]].This is beyond our Local Violation-Fixing Per-ceptron since it makes more than one updates on oneexample, but can be easily represented as a GlobalViolation-Fixing Perceptron (Algorithm 3), since wecan prove any further updates on this example is a vi-olation (under the new weights).
We thus establishLaSO as a special case within our framework.7More interestingly, it is easy to verify that thegreedy case of LaSO update is equivalent to traininga local unstructured perceptron which indepen-dently classifies at each position based on history,which is related to SEARN (Daume?
et al, 2009).Kulesza and Pereira (2007) study perceptronlearning with approximate inference that overgen-erates instead of undergenerates as in our work,but the underlying idea is similar: by learning in aharder setting (LP-relaxed version in their case andprefix-augmented version in our case) we can learnthe simpler original setting.
Our ?beam separabil-ity?
can be viewed as an instance of their ?algorith-mic separability?.
Finley and Joachims (2008) studysimilar approximate inference for structural SVMs.Our max-violation update is also related to othertraining methods for large-margin structured predic-tion, in particular the cutting-plane (Joachims et al,2009) and subgradient (Ratliff et al, 2007) methods,but detailed exploration is left to future work.8 ConclusionsWe have presented a unifying framework of?violation-fixing?
perceptron which guarantees con-vergence with inexact search.
This theory satisfin-gly explained why standard perceptron might notwork well with inexact search, and why the earlyupdate works.
We also proposed some new vari-ants within this framework, among which the max-violation method performs the best on state-of-the-art tagging and parsing systems, leading to bettermodels with greatly reduced training times.
Lastly,the advantage of valid update methods is more pro-nounced when search error is severe.AcknowledgementsWe are grateful to the four anonymous reviewers, es-pecially the one who wrote the comprehensive re-view.
We also thank David Chiang, Kevin Knight,Ben Taskar, Alex Kulesza, Joseph Keshet, DavidMcAllester, Mike Collins, Sasha Rush, and Fei Shafor discussions.
This work is supported in part by aGoogle Faculty Research Award to the first author.7It turns out the original theorem in the LaSO paper (Daume?and Marcu, 2005) contains a bug; see (Xu et al, 2009) for cor-rections.
Thanks to a reviewer for pointing it out.150ReferencesCollins, Michael.
2002.
Discriminative training meth-ods for hidden markov models: Theory and experi-ments with perceptron algorithms.
In Proceedings ofEMNLP.Collins, Michael and Brian Roark.
2004.
Incrementalparsing with the perceptron algorithm.
In Proceedingsof ACL.Crammer, Koby and Yoram Singer.
2003.
Ultra-conservative online algorithms for multiclass prob-lems.
Journal of Machine Learning Research (JMLR),3:951?991.Daume?, Hal, John Langford, and Daniel Marcu.
2009.Search-based structured prediction.Daume?, Hal and Daniel Marcu.
2005.
Learning as searchoptimization: Approximate large margin methods forstructured prediction.
In Proceedings of ICML.Finley, Thomas and Thorsten Joachims.
2008.
Trainingstructural SVMs when exact inference is intractable.In Proceedings of ICML.Huang, Liang.
2008.
Forest reranking: Discriminativeparsing with non-local features.
In Proceedings of theACL: HLT, Columbus, OH, June.Huang, Liang and Kenji Sagae.
2010.
Dynamic pro-gramming for linear-time incremental parsing.
In Pro-ceedings of ACL 2010.Joachims, T., T. Finley, and Chun-Nam Yu.
2009.Cutting-plane training of structural svms.
MachineLearning, 77(1):27?59.Kulesza, Alex and Fernando Pereira.
2007.
Structuredlearning with approximate inference.
In NIPS.Lafferty, John, Andrew McCallum, and FernandoPereira.
2001.
Conditional random fields: Probabilis-tic models for segmenting and labeling sequence data.In Proceedings of ICML.Liang, Percy, Alexandre Bouchard-Co?te?, Dan Klein, andBen Taskar.
2006.
An end-to-end discriminativeapproach to machine translation.
In Proceedings ofCOLING-ACL, Sydney, Australia, July.Marcus, Mitchell P., Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: the Penn Treebank.
ComputationalLinguistics, 19:313?330.McDonald, Ryan and Fernando Pereira.
2006.
On-line learning of approximate dependency parsing al-gorithms.
In Proceedings of EACL.Ratliff, Nathan, J. Andrew Bagnell, and Martin Zinke-vich.
2007.
(online) subgradient methods for struc-tured prediction.
In Proceedings of AIStats.Ratnaparkhi, Adwait.
1996.
A maximum entropy modelfor part-of-speech tagging.
In Proceedings of EMNLP.Shen, Libin, Giorgio Satta, and Aravind Joshi.
2007.Guided learning for bidirectional sequence classifica-tion.
In Proceedings of ACL.Taskar, Ben, Carlos Guestrin, and Daphne Koller.
2003.Max-margin markov networks.
In Proceedings ofNIPS.
MIT Press.Tsochantaridis, I., T. Joachims, T. Hofmann, and Y. Al-tun.
2005.
Large margin methods for structured andinterdependent output variables.
Journal of MachineLearning Research, 6:1453?1484.Xu, Yuehua, Alan Fern, and Sungwook Yoon.
2009.Learning linear ranking functions for beam search withapplication to planning.
Journal of Machine LearningResearch (JMLR), 10:1349?1388.Zhang, Yue and Stephen Clark.
2008.
A tale oftwo parsers: investigating and combining graph-basedand transition-based dependency parsing using beam-search.
In Proceedings of EMNLP.151
