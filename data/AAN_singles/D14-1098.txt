Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 907?916,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsLanguage Modeling with Functional Head Constraint for Code SwitchingSpeech RecognitionYing Li and Pascale FungHuman Language Technology CenterDepartment of Electronic and Computer EngineeringThe Hong Kong University of Science and Technologyeewing@ee.ust.hk, pascale@ece.ust.hkAbstractIn this paper, we propose novel struc-tured language modeling methods for codemixing speech recognition by incorporat-ing a well-known syntactic constraint forswitching code, namely the FunctionalHead Constraint (FHC).
Code mixing datais not abundantly available for traininglanguage models.
Our proposed meth-ods successfully alleviate this core prob-lem for code mixing speech recognitionby using bilingual data to train a struc-tured language model with syntactic con-straint.
Linguists and bilingual speakersfound that code switch do not happen be-tween the functional head and its comple-ments.
We propose to learn the code mix-ing language model from bilingual datawith this constraint in a weighted finitestate transducer (WFST) framework.
Theconstrained code switch language model isobtained by first expanding the search net-work with a translation model, and thenusing parsing to restrict paths to those per-missible under the constraint.
We im-plement and compare two approaches -lattice parsing enables a sequential cou-pling whereas partial parsing enables atight coupling between parsing and fil-tering.
We tested our system on a lec-ture speech dataset with 16% embeddedsecond language, and on a lunch conver-sation dataset with 20% embedded lan-guage.
Our language models with latticeparsing and partial parsing reduce worderror rates from a baseline mixed lan-guage model by 3.8% and 3.9% in termsof word error rate relatively on the aver-age on the first and second tasks respec-tively.
It outperforms the interpolated lan-guage model by 3.7% and 5.6% in terms ofword error rate relatively, and outperformsthe adapted language model by 2.6% and4.6% relatively.
Our proposed approachavoids making early decisions on code-switch boundaries and is therefore morerobust.
We address the code switch datascarcity challenge by using bilingual datawith syntactic structure.1 IntroductionIn multilingual communities, it is common forpeople to mix two or more languages in theirspeech.
A single sentence spoken by bilingualspeakers often contains the main, matrix languageand an embedded second language.
This typeof linguistic phenomenon is called ?code switch-ing?
by linguists.
It is increasingly important forautomatic speech recognition (ASR) systems torecognize code switching speech as they exist inscenarios such as meeting and interview speech,lecture speech, and conversational speech.
Codeswitching is common among bilingual speakers ofSpanish-English, Hindi-English, Chinese-English,and Arabic-English, among others.
In China,lectures, meetings and conversations with techni-cal contents are frequently peppered with Englishterms even though the general population is notconsidered bilingual in Chinese and English.
Un-like the thousands and tens of thousands of hoursof monolingual data available to train, for exam-ple, voice search engines, transcribed code switchdata necessary for training language models ishard to come by.
Code switch language modelingis therefore an even harder problem than acousticmodeling.One approach for code switch speech recogni-tion is to explicitly recognizing the code switchpoints by language identification first using pho-netic or acoustic information, before applyingspeech recognizers for the matrix and embed-ded languages (Chan et.
al, 2004; Shia et.
al,9072004; Lyu and Lyu, 2008).
This approach is ex-tremely error-prone as language identification ateach frame of the speech is necessary and any er-ror will be propagated in the second speech recog-nition stage leading to fatal and irrecoverable er-rors.Meanwhile, there are two general approaches tosolve the problem of lack of training data for lan-guage modeling.
In a first approach, two languagemodels are trained from both the matrix and em-bedded language separately and then interpolatedtogether (Vu et.
al, 2012; Chan et.
al, 2006).
How-ever, an interpolated language model effectivelyallows code switch at all word boundaries withoutmuch of a constraint.
Another approach is to adaptthe matrix language language model with a smallamount of code switch data (Tsai et.
al, 2010; Yehet.
al, 2010; Bhuvanagiri and Kopparapu, 2010;Cao et.
al, 2010).
The effectiveness of adapta-tion is also limited as positions of code switch-ing points are not generalizable from the limiteddata.
Significant progress in speech recognitionhas been made by using deep neural networks foracoustic modeling and language model.
However,improvement thus gained on code switch speechrecognition remains very small.
Again, we pro-pose that syntactic constraints of the code switch-ing phenomenon can help improve performanceand model accuracy.
Previous work of using part-of-speech tags (Zhang et.
al, 2008; Vu et al 2012)and our previous work using syntactic constraints(Li and Fung, 2012, 2013) have made progressin this area.
Part-of-speech is relatively weak inpredicting code switching points.
It is generallyaccepted by linguists that code switching followsthe so-called Functional Head Constraint, wherewords on the nodes of a syntactic sub tree mustfollow the language of that of the headword.
If theheadword is in the matrix language then none ofits complements can switch to the embedded lan-guage.In this work, we propose two ways to incorpo-rate the Functional Head Constraint into speechrecognition and compare them.
We suggest twoapproaches of introducing syntactic constraintsinto the speech recognition system.
One is to ap-ply the knowledge sources in a sequential order.The acoustic model and a monolingual languagemodel are used first to produce an intermediatelattice, then a second pass choose the best resultusing the syntactic constraints.
Another approachuses tight coupling.
We propose using structuredlanguage model (Chelba and Jelinek, 2000) tobuild the syntactic structure incrementally.Following our previous work, we suggest in-corporating the acoustic model, the monolinguallanguage model and a translation model into aWFST framework.
Using a translation model al-lows us to learn what happens when a languageswitches to another with context information.
Wewill motivate and describe this WFST frameworkfor code switching speech recognition in the nextsection.
The Functional Head Constraint is de-scribed in Section 3.
The proposed code switchlanguage models and speech recognition couplingis described in Section 4.
Experimental setup andresults are presented in Section 5.
Finally we con-clude in Section 6.2 Code Switch Language Modeling in aWFST FrameworkAs code switch text data is scarce, we do not haveenough data to train the language model for codeswitch speech recognition.
We propose instead toincorporate language model trained in the matrixlanguage with a translation model to obtain a codeswitch language model.
We propose to integrate abilingual acoustic model (Li et.
al, 2011) and thecode switch language model in a weighted finitestate transducer framework as follows.Suppose X denotes the observed code switchspeech vector, wJ1denotes a word sequence in thematrix language, the hypothesis transcript vI1is asfollows:v?I1= argmaxvI1P (vI1|X)= argmaxvI1P (X|vI1)P (vI1)= argmaxvI1P (X|vI1)?wJ1P (vI1|wJ1)P (wJ1)?=argmaxvI1P (X|vI1)P (vI1|wJ1)P (wJ1) (1)where P (X|vI1) is the acoustic model and P (vI1)is the language model in the mixed language.Our code switch language model is obtainedfrom a translation model P (vI1|wJ1) from the ma-trix language to the mixed language, and the lan-guage model in the matrix language P (wJ1).Instead of word-to-word translation, the trans-duction of the context dependent lexicon trans-fer is constrained by previous words.
Assume thetransduction depends on the previous n words:908P (vI1|wJ1) =I?i=1P (vi|vi?11, wi1)?=I?i=1P (vi?1i?n+1|wii?n+1)=I?i=1P (vi, wi|vi?1i?n+1, wi?1i?n+1)P (wi|vi?1i?n+1, wi?1i?n+1)=I?i=1P (vi, wi|vi?1i?n+1, wi?1i?n+1)P (wi|?vivi?1i?n+1, wi?1i?n+1)(2)There are C-level and H-level search networksin the WFST framework.
The C-level search net-work is composed of the universal phone modelP , the context model C, the lexicon L, and thegrammar GN = P ?
C ?
L ?G (3)The H-level search network is composed of thestate model H , the phoneme model P , the contextmodel C, the lexicon L, and the grammar GN = H ?
P ?
C ?
L ?G (4)The C-level requires less memory then the H-levelsearch network.
We propose to use a weighted fi-nite state transducer framework incorporating thebilingual acoustic model P , the context model C,the lexicon L, and the code switching languagemodels GCSinto a C-level search network formixed language speech recognition.
The outputof the recognition result is in the mixed languageafter projection pi(GCS).N = P ?
C ?
L ?
pi(GCS) (5)The WFST implementation to obtain the codeswitch language model GCSis as follows:Gcs= T ?G(6)where T is the translation modelP (v?L1|wJ1) =L?l=1Pl(v?l|wl) (7)Pl(v?l|wl) is the probability ofwltranslated into v?l.In order to make use of the text data in the ma-trix language to recognize speech in the mixed lan-guage, the translation model P (vI1|wJ1) transducethe language model in the matrix language to themixed language.P (vI1|wJ1) =?v?L1,cL1,rK1,w?K1P (w?K1|wJ1)?P (rK1|w?K1, wJ1)?P (cL1, rK1, w?K1, wJ1)?P (v?K1|cL1, rK1, w?K1, wJ1)?P (vI1|v?K1, rK1, w?K1, wJ1) (8)where P (w?K1|wJ1) is the word-to-phrase segmen-tation model, P (rK1|w?K1, wJ1) is the phrasal re-ordering model, P (cL1, rK1, w?K1, wJ1) is the chunksegmentation model, P (v?K1|cL1, rK1, w?K1, wJ1)is the chunk-to-chunk transduction model,P (vI1|v?K1, rK1, w?K1, wJ1) is the chunk-to-wordreconstruction model.The word-to-phrase segmentation model ex-tracts a table of phrases {v?1, v?2, ..., v?K} forthe transcript in the embedded language and{w?1, w?2, ..., w?K} for the transcript in the ma-trix language based on word-to-word alignmentstrained in both directions with GIZA++ (Och andNey, 2003).
The chunk segmentation model per-forms the segmentation of a phrase sequence w?K1into L phrases {c1, c2, ..., cL} using a segmenta-tion weighted finite-state transducer.
Assumes thata chunk clis code-switched to the embedded lan-guage independently by each chunk, the chunk-to-chunk transduction model is the probability ofa chunk to be code switched to the embedded lan-guage trained on parallel data.
The reconstructionmodel generates word sequence from chunk se-quences and operates in the opposite direction tothe segmentation model.3 Functional Head ConstraintMany linguistics (Abney 1986; Belazi et.
al, 1994;Bhatt 1994) have discovered the so-called Func-tional Head Constraint in code switching.
Theyhave found that code switches between a func-tional head (a complementizer, a determiner, aninflection, etc.)
and its complement (sentence,noun-phrase, verb-phrase) do not happen in natu-ral speech.
In addition, the Functional Head Con-straint is language independent.In this work, we propose to investigate andincorporate the Functional Head Constraint intocode switching language modeling in a WFSTframework.
Figure 1 shows one of the FunctionalHead Constraint examples.
Functional heads are909the roots of the sub trees and complements are partof the sub trees.
Actual words are the leaf nodes.According to the Functional Head Constraint, theleave nodes of a sub tree must be in either thematrix language or embedded language, followingthe language of the functional head.
For instance,the third word ???/something?
is the head ofthe constituents ??
?/very ??
?/important ??/something?.
These three constituent wordscannot be switched.
Thus, it is not permissibleto code switch in the constituent.
More precisely,the language of the constituent is constrained to bethe same as the language of the headword.
In thefollowing sections, we describe the integration ofthe Functional Head Constraint and the languagemodel.We have found this constraint to be empiricallysound as we look into our collected code mixingspeech and language data.
The only violation ofthe constraint comes from rare cases of borrowedwords such as brand names with no translation inthe local, matrix language.
Borrowed words areused even by monolingual speakers so they are ingeneral part of the matrix language lexicon andrequire little, if any, special treatment in speechrecognition.In the following sections, we describe the inte-gration of Functional Head Constraint and the lan-guage model.4 Code Switching Language Modelingwith Functional Head ConstraintWe propose two approaches of language model-ing with Functional Head Constraint: 1) lattice-parsing and sequential-coupling (Chapplerler et.al, 1999); 2) partial-parsing and tight-coupling(Chapplerler et.
al, 1999).
The two approacheswill be described in the followed sections.4.1 Sequential-coupling by Lattice-basedParsingIn this first approach, the acoustic models, thecode switch language model and the syntactic con-straint are incorporated in a sequential order toprogressively constrain the search.
The acousticmodels and the matrix language model are usedfirst to produce an intermediate output.
The in-termediate output is a lattice in which word se-quences are compactly presented.
Lattice-basedparsing is used to expand the word lattice gener-ated from the first decoding step according to theFunctional Head Constraint.We have reasons to use word lattice insteadof N-best hypothesis.
The number of hypothesisof word lattice is larger than N-best hypothesis.Moreover, different kinds of errors correspond tothe language model would be observed if N-bestlist is extracted after the first decoding step.
Thesecond pass run over the N-best list will preventthe language model with Functional Head Con-straint from correcting the errors.
In order to ob-tain a computational feasible number of hypothe-ses without bias to the language model in the firstdecoding step, word lattice is used as the interme-diate output of the first decoding step.A Probabilistic Context-Free Grammar (PCFG)parser is trained on Penn Treebank data.
ThePCFG parser is generalized to take the lattice gen-erated by the recognizer as the input.
Figure 2 il-lustrates a word lattice which is a compact repre-sentation of the hypothesis transcriptions of a aninput sentence.
All the nodes of the word-latticeare ordered by increasing depth.A CYK table is obtained by associating the arcswith their start and end states in the lattice insteadof their sentence position and initialized all thecells in the table corresponding to the arcs (Chap-plerler et.
al, 1999).
Each cell Ck,jof the ta-ble is filled by a n-tuple of the non-terminal A,the length k and the starting position of the wordsequence wj...wj+kif there exists a PCFG ruleA?
wj...wj+k, where A is a non-terminal whichparse sequences of words wj...wj+k.
In order toallow all hypothesis transcriptions of word latticeto be taken into account, multiple word sequencesof the same length and starting point are initializedin the same cell.
Figure 2 mapped the word latticeof the example to the table, where the starting nodelabel of the arc is the column index and the lengthof the arc is the row index.The sequential-coupling by lattice-parsing con-sists of the standard cell-filling and the self-fillingsteps.
First, the cells Ck,jand Ci?k,j+kare com-bined to produce a new interpretation for cell Ci,j.
In order to handle the unary context-free produc-tion A ?
B and update the cells after the stan-dard cell-filling, a n-tuple of A, i and j is addedfor each n-tuple of the non-terminal B, the lengthi and the start j in the cell Ci,j.
The parse treesextracted are associated with the input lattice fromthe table starting from the non-terminal label ofthe top cell.
After the parse tree is obtained, we re-910???DT?this???NN?theory?????NN?EM??VC?is????JJ?important??AD?very???NP?theory???NP?this?
?VP?is ?
?VP?isHypotheses:**EM*	.
* EM*theory.
* this*EM*theory.
* is*this*EM*theory.
* something*is*this*EM*theory.*(not*permissible)*.*.*.*???NN?something????JJ?important??
?NN?somethingFigure 1: A Functional Head Constraint example.0?
1?
2?
6?
7?
8???/very??/very?
???/important????/?important???/?something?
?/is??/is???/?conclude?
??/?this?????/?EM???/?theory?4?3?
5?Figure 2: An example word lattice in the matrix language.2?
3?
5?4?
6?
7?1?!
!
!
!
!
!
!
!!
???"#$%&'()*(!!
?"#+!
!
!
!
!??",-'.!?!???"#$%&'()*(!??"+&$-(/#*0!?"#+!
??"#*1234-!??"(/#+!????"56!??"(/-&'.!
!Figure 3: The mapping of the example word lattice to the table.911cursively enumerate all its subtrees.
Each subtreeis able to code-switch to the embedded languagewith a translation probability Pl(v?l|wl).The lattice parsing operation consists of the anencoding of a given word sequence along witha parse tree (W,T ) and a sequence of elemen-tary model actions.
In order to obtain a correctprobability assignment P (W,T ) one simply as-sign proper conditional probabilities to each tran-sition in the weighted finite states.The probability of a parse T of a word sequenceWP (W,T ) can be calculated as the product of theprobabilities of the subtrees.P (W,T ) =n+1?k=1[P (wk|Wk?1Tk?1) (9)Where Wk= w0...wkis the first k words in thesentence, and (Wk, Tk) is the word-and-parse k-prefix.
The probability of the n-tuple of the non-terminal A, the length i and the starting position jis the probability of the subtree corresponding toA parsing throughout the sequence wj...wj+i?1.The probability of the partial parsing is the productof probabilities of the subtree parses it is made of.The probability of an n-tuple is the maximum overthe probabilities of probable parsing path.The N most probable parses are obtained duringthe lattice-parsing.The probability of a sentence is computed byadding on the probability of each new context-freerule in the sentences.4.2 Tight-coupling by Incremental ParsingTo integrate the acoustic models, language modeland the syntactic constraint in time synchronousdecoding, an incremental operation is used in thisapproach.
The final word-level probability as-signed by our model is calculated using the acous-tic models, the matrix language model, the struc-tured language model and the translation model.The structured language model uses probabilisticparameterization of a shift-reduce parse (Chelbaand Jelinek, 2000).
The tight-coupled languagemodel consists of three transducers, the word pre-dictor, the tagger and the constructor.
As shownin Figure 3, Wk= w0...wk is the first k words ofthe sentence, Tkcontains only those binary sub-trees whose leaves are completely included inWk,excluding w0=<s>.
Single words along withtheir POS tag can be regarded as root-only trees.The exposed head hkis a pair of the headwordof the constituent Wkand the non-terminal label.The exposed head of single words are pairs of thewords and their POS tags.Given the word-and-parse (k-1)-prefixWk?1Tk?1, the new word wkis predicted bythe word-predictor P (wk|Wk?1Tk?1).
Takingthe word-and-parse k ?
1-prefix and the nextword as input, the tagger P (tk|wk,Wk?1Tk?1)gives the POS tag tkof the word wk.
ConstructorP (pki|WkTk) assigns a non-terminal label to theconstituent Wk+1.
The headword of the newlybuilt constituent is inherited from either theheadword of the constituent Wkor the next wordwk+1.P (wk|Wk?1Tk?1)= P (wk|[Wk?1Tk?1])= P (wk|h0, h?1) (10)P (tk|wk,Wk?1Tk?1)= P (tk|wk, [Wk?1Tk?1])= P (tk|wk, h0.tag, h?1.tag) (11)P (pki|WkTk)= P (pki|[WkTk])= P (pki|h0, h1) (12)The probability of a parse tree T P (W,T ) of aword sequence W and a complete parse T can becalculated as:P (W,T ) =n+1?k=1[P (wk|Wk?1Tk?1)P (tk|Wk?1Tk?1, wk)P (Tk|Wk?1Tk?1, wk, tk)](13)P (Tkk?1|Wk?1Tk?1, wk, tk)=Nk?i=1P (pk|Wk?1Tk?1, wk, tk, pk1...pki?1)(14)Where wkis the word predicted by the word-predictor, tkis the POS tag of the word wkpre-dicted by the tagger, Wk?1Tk?1is the word-parse(k - 1)-prefix, Tkk?1is the incremental parse struc-ture that generates Tk= Tk?1||Tkk?1when at-tached to Tk?1; it is the parse structure built ontop of Tk?1and the newly predicted word wk; the|| notation stands for concatenation; Nk?1is thenumber of operations the constructor executes at912Figure 4: A word-and-parse example.position k of the input string before passing con-trol to the word-predictor (the Nkth operation atposition k is the null transition); Nkis a functionof T ; pkidenotes the i th constructor action carriedout at position k in the word string.The probability models of word-predictor, tag-ger and constructor are initialized from the UpennTreebank with headword percolation and bina-rization.
The headwords are percolated using acontext-free approach based on rules of predict-ing the position of the headword of the constituent.The approach consists of three steps.
First a parsetree is decomposed to phrase constituents.
Thenthe headword position is identified and filled inwith the actual word percolated up from the leavesof the tree recursively.Instead of the UPenn Treebank-style, we use amore convenient binary branching tree.
The parsetrees are binarized using a rule-based approach.The probability models of the word-predictor,tagger and constructor are trained in a maximiza-tion likelihood manner.
The possible POS tag as-signments, binary branching parse, non-terminallabels and the head-word annotation for a givensentence are hidden.
We re-estimate them usingEM algorithm.Instead of generating only the complete parse,all parses for all the subsequences of the sen-tence are produced.
The headwords of the subtreesare code switched to the embedded language witha translation probability Pl(v?l|wl) as well as theleaves.4.3 Decoding by TranslationUsing either lattice parsing or partial parsing, atwo-pass decoding is needed to recognize codeswitch speech.
A computationally feasible firstpass generates an intermediate result so that thelanguage model with Functional Head constraintcan be used in the second pass.
The first decodingpass composes of the transducer of the universalphoneme model P , the transducerC from context-dependent phones to context-independent phones,the lexicon transducer L which maps context-independent phone sequences to word strings andthe transducer of the language model G. A T3 de-coder is used in the first pass.ASR1= P ?
C ?
L ?G (15)Instead of N-best list, word lattice is used as theintermediate output of the first decoding step.The language model GCSof the transducer inthe second pass is improved from G by compos-ing with the translation model Pl(v?l|wl).
Finally,the recognition transducer is optimized by deter-mination and minimization operations.ASR2= P?C?min(det(L?min(det(pi(GCS)))))(16)5 Experiments5.1 Experimental SetupThe bilingual acoustic model used for our mixedlanguage ASR is trained from 160 hours of speechfrom GALE Phase 1 Chinese broadcast conver-sation, 40 hours of speech from GALE Phase 1English broadcast conversation, and 3 hours ofin-house nonnative English data.
The acousticfeatures used in our experiments consist of 39components (13MFCC, 13MFCC, 13MFCC us-ing cepstral mean normalization), which are an-alyzed at a 10msec frame rate with a 25msec win-dow size.
The acoustic models used throughoutour paper are state-clustered crossword tri-phoneHMMs with 16 Gaussian mixture output densi-ties per state.
We use the phone set consists of21 Mandarin standard initials, 37 Mandarin finals,6 zero initials and 6 extended English phones.
Thepronunciation dictionary is obtained by modify-ing Mandarin and English dictionaries using thephone set.
The acoustic models are reconstructed913Table 1: Code switching point detection evaluation (Precision/Recall/F-measure)Lecture speech Lunch conversationMixedLM 0.61/0.64/0.64 0.54/0.63/0.58InterpolatedLM 0.62/0.66/0.64 0.55/0.63/0.58AdaptedLM 0.63/0.71/0.67 0.54/0.63/0.58Sequential coupling 0.66/0.71/0.68 0.55/0.70/0.61Tight coupling 0.68/0.71/0.70 0.56/0.70/0.62by decision tree tying.
We also collected twospeech databases with Chinese to English codeswitching - namely, 20 hours of lecture speech cor-pus (Data 1) and 3 hours of lunch conversationcorpus (Data 2).
18 hours of Data 1 is used foracoustic model adaptation and 1 hour of data areused as the test set (Test 1).
2 hours of Data 2 con-taining 2389 utterances is used to adapt the acous-tic model and 280 utterances are used as the testset (Test 2).
To train the parser, we use ChineseTreebank Version 5.0 which consists of 500 thou-sand words and use the standard data split (Petrovand Klein, 2007).For the language models, transcriptions of 18hours of Data 1 are trained as a baseline mixedlanguage model for the lecture speech domain.250,000 sentences from Chinese speech confer-ence papers, power point slides and web dataare used for training a baseline Chinese matrixlanguage model for the lecture speech domain(LM 1).
Transcriptions of 2 hours of Data 2 areused as the baseline mixed language model in thelunch conversation domain.
250,000 sentences ofthe GALE Phase 1 Chinese conversational speechtranscriptions are used to train a Chinese ma-trix language model (LM 2).
250,000 of GALEPhase 1 English conversational speech transcrip-tion are used to train the English embedded lan-guage model (LM 3).
To train the bilingual trans-lation model, the Chinese Gale Phase 1 conversa-tional speech transcriptions are used to generatea bilingual corpus using machine translation.
Forcomparison, an interpolated language model forthe lunch conversation domain is trained from in-terpolating LM 2 with LM 3.
Also for comparison,an adapted language model for lecture speech istrained from LM 1 and transcriptions of 18 hoursof Data 1.
An adapted language mode l for conver-sation is trained from LM 2 and 2 hours of Data 2.The size of the vocabulary for recognition is 20kwords.
The perplexity of the baseline languagemodel trained on the code switching speech tran-scription is 236 on the lecture speech and 279 onthe conversation speech test sets.5.2 Experimental ResultsTable 1 reports precision, recall and F-measureof code switching point in the recognition resultsof the baseline and our proposed language mod-els.
Our proposed code switching language mod-els with functional head constraint improve bothprecision and recall of the code switching pointdetection on the code switching lecture speech andlunch conversation 4.48%.
Our method by tight-coupling increases the F-measure by 9.38% rela-tively on the lecture speech and by 6.90% rela-tively on the lunch conversation compared to thebaseline adapted language model.The Table 2 shows the word error rates (WERs)of experiments on the code switching lecturespeech and Table 3 shows the WERs on the codeswitching lunch conversations.
Our proposed codeswitching language model with Functional HeadConstraints by sequential-coupling reduces theWERs in the baseline mixed language model by3.72% relative on Test 1, and 5.85% on Test 2.
Ourmethod by tight-coupling also reduces WER by2.51% relative compared to the baseline languagemodel on Test 1, and by 4.57% on Test 2.
Weuse the speech recognition scoring toolkit (SCTK)developed by the National Institute of Standardsand Technology to compute the significance lev-els, which is based on two-proportion z-test com-paring the difference between the recognition re-sults of our proposed approach and the baseline.All the WER reductions are statistically signifi-cant.
For our reference, we also compare the per-formance of using Functional Head Constraint tothat of using inversion constraint in (Li and Fung,2012, 2013) and found that the present model re-duces WER by 0.85% on Test 2 but gives no im-provement on Test 1.
We hypothesize that since914Table 2: Our proposed system outperforms the baselines in terms of WER on the lecture speechMatrix Embedded OverallMixedLM 34.41% 39.16% 35.17%InterpolatedLM 34.11% 40.28% 35.10%AdaptedLM 35.11% 38.41% 34.73%Sequential coupling 33.17% 36.84% 33.76%Tight coupling 33.14% 36.65% 33.70%Table 3: Our proposed system outperforms the baselines in terms of WER on the lunch conversationMatrix Embedded OverallMixedLM 46.4% 48.55% 46.83%InterpolatedLM 46.04% 49.04% 46.64%AdaptedLM 46.64% 48.39% 46.20%Sequential coupling 43.24% 46.27% 43.89%Tight coupling 42.97% 46.03% 43.58%Test 1 has mostly Chinese words, the proposedmethod is not as advantageous compared to ourprevious work.
Another future direction is for usto improve the lattice parser as we believe it willlead to further improvement on the final result ofour proposed method.6 ConclusionIn this paper, we propose using lattice parsing andpartial parsing to incorporate a well-known syn-tactic constraint for code mixing speech, namelythe Functional Head Constraint, into a continu-ous speech recognition system.
Under the Func-tional Head Constraint, code switch cannot occurbetween the functional head and its complements.Since code mixing speech data is scarce, we pro-pose to instead learn the code mixing languagemodel from bilingual data with this constraint.The constrained code switching language modelis obtained by first expanding the search networkwith a translation model, and then using parsing torestrict paths to those permissible under the con-straint.
Lattice parsing enables a sequential cou-pling of parsing then constraint filtering whereaspartial parsing enables a tight coupling betweenparsing and filtering.
A WFST-based decoderthen combines a bilingual acoustic model and theproposed code-switch language model in an inte-grated approach.
Lattice-based parsing and partialparsing are used to provide the syntactic structureof the matrix language.
Matrix words at the leavenodes of the syntax tree are permitted to switch tothe embedded language if the switch does not vio-late the Functional Head Constraint.
This reducesthe permissible search paths from those expandedby the bilingual language model.
We tested oursystem on a lecture speech dataset with 16% em-bedded second language, and on a lunch conversa-tion dataset with 20% embedded second language.Our language models with lattice parsing and par-tial parsing reduce word error rates from a baselinemixed language model by 3.72% to 3.89% rela-tive in the first task, and by 5.85% to 5.97% inthe second task.
They are reduced from an inter-polated language model by 3.69% to 3.74%, andby 5.46% to 5.77% in the first and second task re-spectively.
WER reductions from an adapted lan-guage model are 2.51% to 2.63%, and by 4.47%to 4.74% in the two tasks.
The F-measure for codeswitch point detection is improved from 0.64 bythe interpolated model to 0.68, and from 0.67 bythe adapted model to 0.70 by our method.
Ourproposed approach avoids making early decisionson code-switch boundaries and is therefore morerobust.
Our approach also avoids the bottleneck ofcode switch data scarcity by using bilingual datawith syntactic structure.
Moreover, our method re-duces word error rates for both the matrix and theembedded language.AcknowledgmentsThis work is partially supported by grant numberRGF 612211 of the Hong Kong Research GrantsCouncil, by 1314159-0PAFT20F003 of the PingAn Research Institute and by 13140910 of theHuawei Noah?s Ark Lab.915ReferencesJ.J.
Gumperz, ?Discourse strategies?, Cambridge Uni-versity Press, 1, 1982.Coulmas, F., ?The handbook of sociolinguistics?,Wiley-Blackwell, 1998.Vu, N.T.
and Lyu, D.C. and Weiner, J. and Telaar, D.and Schlippe, T. and Blaicher, F. and Chng, E.S.
andSchultz, T. and Li, H. A first speech recognitionsystem for Mandarin-English code-switch conversa-tional speech?, ICASSP, 2012J.Y.C.
Chan and PC Ching and T. Lee and H.M. Meng?Detection of language boundary in code-switchingutterances by bi-phone probabilities?
Chinese Spo-ken Language Processing, 2004 International Sym-posium on, 293?296.C.J.
Shia and Y.H.
Chiu and J.H.
Hsieh and C.H.Wu ?Language boundary detection and identifica-tion of mixed-language speech based on MAP es-timation?
?, ICASSP 2004.D.C.
Lyu and R.Y.
Lyu ?Language identificationon code-switching utterances using multiple cues?Ninth Annual Conference of the InternationalSpeech Communication Association, 2008.Tsai, T.L.
and Chiang, C.Y.
and Yu, H.M. and Lo,L.S.
and Wang, Y.R.
and Chen, S.H.
?A study onHakka and mixed Hakka-Mandarin speech recogni-tion?
Chinese Spoken Language Processing (ISC-SLP), 2010 7th International Symposium on, 199?204Yeh, C.F.
and Huang, C.Y.
and Sun, L.C.
andLee, L.S.
?An integrated framework for transcrib-ing Mandarin-English code-mixed lectures with im-proved acoustic and language modeling?
ChineseSpoken Language Processing (ISCSLP), 2010 7thInternational Symposium on, 214?219K.
Bhuvanagiri and S. Kopparapu, ?An Approach toMixed Language Automatic Speech Recognition?,Oriental COCOSDA, Kathmandu, Nepal, 2010Cao, H. and Ching, PC and Lee, T. and Ye-ung, Y.T.
?Semantics-based language modeling forCantonese-English code-mixing speech recognitionChinese Spoken Language Processing (ISCSLP),2010 7th International Symposium on,246?250Chelba, Ciprian, and Frederick Jelinek.
?Structuredlanguage modeling.?
Computer Speech & Language14, no.
4 (2000): 283-332.Imseng, D. and Bourlard, H. and Magimai-Doss,M.
and Dines, J., ?Language dependent universalphoneme posterior estimation for mixed languagespeech recognition?, ICASSP, 2011.Q.
Zhang and J. Pan and Y. Yan, ?Mandarin-Englishbilingual speech recognition for real world music re-trieval?, ICASSP, 2008.Bouselmi, G. and Fohr, D. and Illina, I., ?Combinedacoustic and pronunciation modelling for non-nativespeech recognition?, Eighth Annual Conference ofthe International Speech Communication Associa-tion, 2007.Woolford, E., ?Bilingual code-switching and syntactictheory?, in Linguistic Inquiry, 14(3):520?536, JS-TOR, 1983.MacSwan, J., ?13 Code-switching and grammaticaltheory?, in The Handbook of Bilingualism and Mul-tilingualism, 323 Wiley-Blackwell, 2012.Poplack, S. and Sankoff, D., ?A formal grammar forcode-switching?, in Papers in Linguistics: Inter-national Journal of Human Communication, 3?45,1980.Moore, Robert C and Lewis, William, ?Intelligent se-lection of language model training data?
Proceed-ings of the ACL 2010 Conference Short Papers,220?224.Belazi, Heidi; Edward Rubin; Almeida JacquelineToribio ?Code switching and X-Bar theory: Thefunctional head constraint?.
Linguistic Inquiry 25(2): 221-37, 1994.Bhatt, Rakesh M., ?Code-switching and the functionalhead constraint?
In Janet Fuller et al.
Proceedings ofthe Eleventh Eastern States Conference on Linguis-tics.
Ithaca, NY: Department of Modern Languagesand Linguistics.
pp.
1-12, 1995Chappelier, Jean-C?dric, et al., ?Lattice parsing forspeech recognition.?
TALN 1999.916
