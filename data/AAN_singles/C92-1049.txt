USING LINGUISTIC, WORLD, AND CONTEXTUAL KNOWLEDGEIN A PLAN RECOGNITION MODEL OF DIALOGUE ~LYNN LAMBERT and SANDRA CARBERRYDepar tment  of Computer  and  In format ion  Sc iencesUn ivers i ty  of  De lawareNewark ,  De laware  19716, USAemail : lambert@cia, udel, edu, earberry@cis, udel, eduAbst rac tThis paper presents a plan-based model of dialoguethat combines world, linguistic, and contextual knowl-edge in order to recognize complex communicative ac-tions such as expressing doubt.
Linguistic knowledgesuggests certain discourse acts, a speaker's beliefs, audthe strength of those beliefs; contextual knowledge sug-gests the most coherent continuation of the dialogue;and world knowledge provides evidence that the appli-cability conditions hold for those discourse acts thatcapture the relationship of the current utterance to thediscourse as a whole.1 In t roduct ionRecognizing the roles that utterances play in adialogue and how the utterances should be interpretedin the context of preceding dialogue is a crucial part ofa robust model of understanding.
In order to performthis recognition, our tripartite plan-based model of di-alogue identifies not only domain and problem-solvingactions but also discourse or communicative actionsthat determine how utterances relate to each other.For this communicative action recognition, we com-bine information gleaned from a variety of knowledgesources: contextual, linguistic, and world knowledge.The combination of these different knowledge sourcesenables the recognition of complex communicative ac-tions such as expressing donbt.
Although our tripar-tite model recognizes three different kinds of actions(domain, problem-solving, and discourse), the focus ofthis paper will be the recognition of discourse actionsand how a combination of knowledge sonrces enablesus to perform this recognition.2 Our Tr ipart i te ModelA number of researchers have contended that acoherent discourse consists of segments that are relatedto one another through some type of structuring rela-tion \[Gri75, MT83\] or have modeled iscourse based onthe semantic relationship of individual clauses \[Pol86\]or groups of clauses \[Rei78\].
But all of these fail tocapture the goal-oriented natnre of discoursc.
Groszand Sidner \[GS86\] argue that recognizing the struc-tural relationships among the intentions underlying a1 This work is being supported by the National Science Foun-dation trader Graait No.
IR1-9122026.
The ~overrlnlellt h&s cer-tMn rights in tiffs material.discourse is necessary to identify discourse structure;although they do not provide the details of a compu-tational mechanism for recognizing these relationships,they do argue convincingly that it requires multipleknowledge sources.
We have developed a plan-basedmodel of dialogue and have incorporated into our modelGrosz and Sidner's claim that linguistic, contextual,and world knowledge should be combined in recogniz-ing the role of an utterance in a discourse.
.We contend that at least three kinds of ac-tions, domain, problem-solving, and discourse, shouldbe captured by a model of task-oriented ialogue.Many researchers \[A1179, Car87, LAB7, Sid85, GS86\]have demonstrated that recognition of domain ac-tions endows a system with the ability to success-fnlly address many important and difficult problemsin understanding.
Several researchers have also in-vestigated the recognition of problem-solving actions\[LA87, Ram9I, Wil81\].
For example, if a user wants toearn a degree, the user might perform problem-solvingactions of 1) evaluating alternative degrees (i.e., theuser might decide whether a BS or a BA is more desir-able), 2) instantiating the type of degree to be earned,and 3) building a plan for performing the domain ac-tion of earning the selected egree.Carberry \[Car89\] points out the importance ofrecognizing discourse actions, the communicative ac-tions that speakers perform iu making an utterance(e.g., asking a question, providing baekgrouml infor-mation, or expressing surprise).
Discourse actions pro-vide expectations for subsequent utterances (e.g., whena question is asked, one expects the question to beaccepted and eventually answered).
Recognition ofsome discourse actions such ms Give-Background alsoexplains the purpose of an utterance and bow it shouldbe interpreted; rather than just a statement of fact,the utterance providing background information shouldbe used by the system to fill in necessary backgroundknowledge in order to fully understand related utter-antes .We capture these three types of actions in sep-arate levels of our disconrse model, which we refer toas a DM \[LC91\].
Within each of these levels, actionsmay contribute to other actions on the same level; forexample, on the discourse level, providing backgrounddata, asking a question, and answering a question allcan be part of obtaining information.
Thus, actionsat each level form a tree structure in which each noderepresents an action that a participant is performingand the children of a node represcnt actions pursuedin order to perform the parent action, ttowever, dis-course, problem-solving, and domain actions are notACTES DE COLING-92, NAMES, 23-28 AOI~'P 1992 3 1 0 PROC.
OV COLING-92, NAI, ZrES.
AUG. 23-28, 1992Domain  Leve lr .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
,I * r I '~+'Co~S I ' CS+60) I :.
.
.
.
.
.
.
.
.
.
-~- .
.
.
.
.
.
.
.
.
.
.
.
.
,P t ?- b- (e-m- -':s- -~Y-I 9tJ- -~Y-q.
.
.
.
.
.
.
.
~ t .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
,I """d'P~"~s_!
: :S 2,Ta~-Co'"o~S 1, CS3~?!)
\]IlmNlliale-V,ars(S l r $21Lea~lI-Mateflal(S \[ tC 3601 ..fac)l \[IDiscourse  Leve l  l. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
- .
.
.
.
.
.
.
.
.
.
.
.
.
.
i .
.
.
.
.
.
.
.
.
, .
.
.
.
.
2",'.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.I Ob ,min+tnfo-RefCSI, $2, _fat, Teaches( fac, CS300)) \]ZA\[.~:uot(sl, ss, :.~,'r+~h?~ t.?, css6o))Reque~l(Sl.
2, lnform+Ref(S2.\[st, f~, Teachex(_ fac, CS360)) \[Sur face-Wn-Question(S 1, S2, I \[ fac, Teach~(_fac, C8360)) \[\[ Answer-Ref(S2, Sl, jac,  - '+I TeachesL.fa~, CS36.O), Teaches~D6 Smith~ CS360 ) |\[ hlform(S2, Sl, Teaches(Dr. Snlim.
CS360)) I?\[*T'?II(S;,'SI, Teaches(Dr. Smlth, lLq360))" }lI Surface-Inform(S2, Sl, Teaches(Ill. Smith, CS360)) IFigure 1: Dialogue Model for two utterances- -  ~" = Emfi)le Arc= Subactlon Arccompletely independent of one another; discourse ac-tions may be executed in order to obtain the informa-tion necessary for performing a problem-solving actionand problem-solving actions may bc executed in orderto construct a domain plan.
Our model captures thisinteraction by allowing links between the actions at ad-jacent levels.
Figure I contains a sample DM derivedfrom two utterances, and section 3 describes how theDM in Figure 1 is constructed.3 D i f fe rent  K inds  o f  KnowledgeThe following dialogue will be used to demon-strate why our system needs world, contextual, andlinguistic knowledge, and to show }low the combina-tion of these different knowledge sources enables thesystem to recognize implicit acceptance of previouslycommuuicated propositions and to identify the role ofutterances that cannot be determined from one or twoknowledge sources alone.
The system is playing therole of $2,(1) SI: Who is teaching CS3607(2) $2: Dr, Smith is teaching CS360.
(3) SI: Bul isn't CS360 an undergrad course?
(d) $2: Yes.
(5) Dr. Smith leaches gradnateand undergrad courses.
(6) Sl: Who handles the CS'360 lab?3.1 Wor ld  KnowledgeIn order to recognize how actions are intendedto contribute to otlmr actions, our system needs knowl-edge abont how to perform actions.
This world knowl-edge is I)rovided in the form of a library of discourse,problem-solving, and domain recipes \[Pol90\].
Our rep-resentation of a recipe includes a header giving thenatoe of tile recipe and the action that it accomplishes,preconditions, applicability conditions, constraints, abody, effects, and a goal.
Applicability conditions rep-resent conditions that nmst be satisfied ill order forthe recipe to be reasonable to apply ill tile given situa-tion whereas constraints limit the allowable instantia-lion of variables in each of the components of a recipe\[LA87, Car87\].
Figure 2 contains a sample discourserecipe.Given the senlalltic representation f a new ut-terance, the system mnst assimilate tim utterance andproduce an updated dialogue model (DM).
Plan in-ference rules \[A1179\] and constraint satisfaction \[LA87,Car87\] suggest chains of higher level actions that anutterance may in!
part of, and foetlsing heuristics\[Car87, SidSl\] order these inference paths according tocoherence.
For exanlple, the semantic rel)resentationof (1) is:Surface WH-Question(fil, $2, _fac, Teaches(_fac,csa@))From this surface question, t)lan inference rules suggest that (1) is executing a Rcquest action and thatthis Request action is part of an Ask-Re\] action whichin turn is part of an Obtain-lnfl~-Re\] action since eachof these actions is part of the body of a recipe thatperforms the higher level action As the system infersthese actions, tile system also tentatively ascribes cer-tain beliet~ that must hold in order fl~r the agent tobe pursuing these discourse actions l"or example, morder for (1) to lw part of ;m Obtaln-ln\]o-Rcf action,,ql must not know the answer to thv questh)n; if SIknew who was teaching CS360, this utterance mightbe part of a 7'est-L+slcncr action instead.
These requi-site beliefs are captured ill tile applicability conditionsof discourse recipes.
As tile system inDrs actions, itmust be plausibh~ that the applicability conditions areACRES DE COLING-92, NANTES.
23-28 AO~r 1992 3 1 1 PRec.
OF COLING-92, NANTES, AUG. 23-28.
1992Discourse  Recipe-Cl:  {_agent1 expresses doubt o _agent2 about _propl because _agent1 believes _prop2 to be true}Action: Express-Doubt(_agentl, _agent2, _propl, _prop2, _rule)App Cond:  believe(_agentl, _prop2)believe(_agentl, befieve(_agent2, _propl))believe(_agent 1, _rule)believe(_agentl, ((_prop2 A _rule) =?, ~_propl))in-focus(_propl)Body:  Convey-Uncertain-Belief(_agentl, _agent2, .prop2)Address-Q-Acceptance(_agent2, _agent1, -prop2)Effects:  believe(_agent2, ~beheve(_agentl, _propl))beheve(_agent2, want(_agentl, Resolve-Conflict(_agent2, _agentl, _prop1, _prop2)))Goal: want(_agent2, Resolve-Conflict(_agent2, _agentl, _propl, _prop2))Figure 2: Sample Discourse Recipesatisfied; otherwise, the inference is rejected.So, another part of our system's world knowl-edge is a model of the speaker's beliefs.
Since our in-vestigation of naturally occurring dialogues indicatesthat people express hades of belief in propositions andexpect others to recognize these beliefs, we maintain amulti-strength model of beliefs to represent an agent'svarying degrees of belief in a proposition, ranging fromhaving no idea whether a proposition is true (or false),to being certain that a proposition is true (or false).
Wealso maintain a model of a stereotypical user whose be-liefs may be attributed to the speaker as appropriateduring tim course of the conversation.After the system has inferred actions on thediscourse level, it must identify how these relate toproblem-solving and domain actions.
This is accom-plished by chaining between actions on adjacent lev-els of the DM.
For example, once the system infersthat (1) contributes to an Obtain-lnfo-Ref action onthe discourse level, plan inference rules suggest that S1wants the goal of the Obtain-lnfo-Ref action, namelyknowref(S1, _fac, teaches(_fac, CS360)).
Since knowingpossible fillers for a parameter is a precondition to in-stantiating that parameter, the system infers that S1wants to know who is teaching CS360 in order to in-stantiate the instructor parameter in an action to learnthe material for that course; that is, the system in-fers that S1 wants lnstantiate-Single-Var(S1, $2, _fac,Learn-Material(S1, CS360, _fac)).
Since instautiatingone paranteter m an action is part of a plan to instan-tiate all of the parameters in that action, the systeminfers that St wants lnslanlialc-Vars(S1, $2, Learn-Matenal(Sl, CS360, _fac)), and since this latter actionis part of  a recipe for building a plan, the system theninfers the problem-solving action, Build-Plan(S1, $2,Take-Course(S1, CS360)).
These instantiate-Single-Var, lnstantiate-Vars, and Build-Plan actions are en-tered into the problem-solving level of the DM.
Build-ing a plan to perform some domain action is a precon-dition to doing that action (assuming agents are actingintentionally), so the system infers that Sl wants Take-Course(Sl, CS360), and this domain action is enteredinto the domain level of the DM.Once the system has assimilated an utteranceinto the DM, it nmst update its belief nmdel for thespeaker to reflect the beliefs that were tentatively as-cribed to the speaker during the plan inference process.These beliefs can then be used in understanding sub-sequent utterances.3.2  Contextua l  KnowledgeEach new utterance must be interpreted withrespect o the existing dialogue context \[GS86, Car87\].This process requires contextual knowledge, which oursystem captures with the use of the DM, a focus ofattention which designates the most salient .action oneach level of the DM, and focusing heuristics which sug-gest the most coherent continuation of the dialogue.For example, on the discourse level, utterances thatcontribute to the currently focused action are more ex-pected, and thus more coherent, than utterances thatcontribute to an ancestor that is further removed fromthe focus of attention.
This contextual knowledge cre-ates expectations that help determine how to interpretnew utterances.
For example, after a qnestion is asked,the context suggests that acceptance of the questionwill be pursued (i.e., the listener will ensure that thequestion is understood, justified, and answerable); thenit is expected that the question will be answered.Because our system does not yet include a plan-ner, we incorporate S2's utterances into the DM in thesame way as Sl 's  have been.
So, continuing with ourexample dialogue, we express the semantic representa-tion of (2) as:Surface-lnform(S2, $1~ Teacites(Dr. Smith, CS360))Plan inference rules suggest hat the surface informmight be part of a Tell action which might be part of anInform action 2 which might he part of an Answer-Refaction which might in turn be part of an Obtain-lnfo-Rcf action since each of these actions is part of thebody of a recipe that performs the higher le4el action.Contextual knowledge is then used to determine howto relate (2) to the previous dialogue.
Focusing heuris-tics suggest hat the best interpretation of (2) is thatit is part of a plan for performing the Obtain-lnfo-Refaction that was an ancestor of the Request action of ut-terance (1).
No new problem-solving or domain actiousare inferred.Figure 1 gives the DM that our system buildsfrom utterances (l) and (2) with the current focus of2We differentiate between telling a listener some string ofwords said informing a listener of a proposition, hi order toinform a listener of some proposition, the listener must first un-derstaald the content of the proposition; tiffs is the goal of theTell action.
The goal of the ln\]orm action is that the listenerbelieve the COlmnunicated proposition.AcrEs DE COL1NG-92, NANTES, 23-28 AOI~T 1992 3 1 2 PROC.
OF COLING-92, NANTES, AUG. 23-28, 1992attention on each level marked with an asterisk.
Thus,we have seen that both world knowledge (consisting ofa plan library and beliefs about the speaker's beliefs)and contextual knowledge (consisting of the existingDM, the current focus of attention, and focusing heuris-tics) are required in order to determine what actions aspeaker is performing and how these actions relate tothe l)revious dialogue.3 .3  L ingu is t i c  KnowledgeLinguistic knowledge must also be taken intoaccount in recognizing the actions that a speaker isperforming.
This knowledge inehldes the surface formof an utterance and clue words.
The surface form ofan utterance is one way that a speaker communicatesvarying degrees of belief in a proposition.
Consider, forexample, the following two utterances:(7) Is Dr. Smith teaching CS3107(8) Isn't Dr. Smith teaching CS3107The form of the utterance in (8) indicates an uncertainbelief in tim proposition that Dr. Smith is teachingCS310; utterance (7), however, conveys only a lack ofknowledge about the proposition.
Similarly (3) is notmerely a Yes/No question; instead, this surface formconveys that S1 thinks that CS360 is an undergradcourse, but is not certain of it.
Our system uses theform of the utterance to recognize the strength of aspeaker's beliefs; these beliefs are then used to deter-mine whether the applicability conditions for the sug-gested discourse actions are satisfied.Tile second type of linguistic information thatwe use is clue words.
These lingtfistic clues often sng-gest what type of discourse action the speaker might bepursuing\[LAB7, Hin89\].
a We use these linguistic cluesas evidence for discourse actions.
For example, utter-ance (3) contains the clue word "but," which suggestsa non-acceptance discourse action.
Thus, the linguisticinformation that our system captures includes knowl-edge about the surface form of an utterance and aboutclue words.
44 Combining Knowledge forDiscourse Act RecognitionBecause there are nlany ways that an utterancecan continue a dialogue and because the correct inter-pretation is not always the one most strongly suggestedby plan chaining and focusing heuristics, evidence fromother knowledge sources is needed to identify the in-tended relationship between an utterance and the ex-isting dialogue context.
For example, the interpreta-tion of (3) most strongly suggested by focusing heuris-tics is that of requesting clarification of (2) in order tonnderstand it.
Intuitively, however, (3) seems to be e?-pressing doubt at S2's answer, not trying to understand3The surface form of some utterances may also serve thispurpose.4The utter~ame itself in fact contains more ilfformation thanjust clue words, surface fornl, and propositional content, but oursystem uses only these three.
Part of our future work includesincorporating other linguistic information such as tense.it.
Plan inference rules and focasing heuristics then arenot sufficient o determine what role (3) is serving inthe discourse.
More knowledge is needed from othersources.Intuitively, for each new utterance that relatesto the previous dialogue on the discourse level, 5 thereis some discourse action that captures this relationshipand serves to describe the role of the new utterancewith respect to the preceding dialogue.
Since manysuch relationships are plausible (e.g., (3) could be in-terpreted as expressing doubt or as indicating a lack ofunderstanding), we contend that evidence is requiredfor recognizing the discourse action that identifies thecorrect relationship.In our model, this discourse action will he an el-ement of an inference path from the utterance to someaction in the current tree structure on the discourselevel.
Furthermore, it will introduce new parameterswhich must be instantiated based on values from theDM in order for the path t ?
terminate with an actionalready in the DM.
By replacing these new parameterswith values from the DM that are not present in thesemantic representation of the utterance, wc are hy-pothesizing a relationship between the new utteranceand the existing discourse level of tile DM.
Thus, thisaction serves the aforementioned role of capturing howthe new utterance relates to the current dialogue con-text.
We will refer to such actions as e-actions sincewe contend that there must be evidence to support heinference of these actions.For example, suppose that the semantic repre-sentation of an utterance such as (8) isSurface-Neg-YN-Question(Sl, $2 PropA)and that chaining suggests that the utterance is partof a Convey-Uncertain.Belief action which is part ofa recipe for the Express-Doubt action shown in Fig-ure 2.
The parameters _agent;I, _agent2, and _p~rop2in the Express-Doubt action will be instant iatedwiththe values $1, $2, and PropA that appear in the se-mantic representation of the utterance and propagateduring chaining to the Convey Uncertain-Belief actionand in turn to the Express-Doubt action, tlowever, theparameters _propl and Aru\].e are introduced for thefirst time in the Express-Doubt action and have manyplausible instantiations.
Continued chaining from theExpress-Doubt discourse action could eventually leadto an Inform action, and we might equate this Informwith an Inform that already exists in the DM, therebyinterpreting tile new utterance as related to this previ-ously identified action.
Unifying the Inform action ontim inference path with an Inform action in the DM,and propagating the resultant substitutions back downthe inference chain, will result in _propt anti _vu:l.e be-ing instantiated based on information from the DM.
Inparticular, _propl will be instantiated with the propo-sition from the Inform action and _vu:l_e will be con-strained to a rule that SI might ttfink suggests that_prop2 and _prt~pl are inconsistent.llowever, it is not enough that these instantia-tions plausibly satisfy the applicability conditions forthe Express-Doubt action.
For example, consider tilefollowing dialogue:5 Solne utterances, llch a~ (1) and (6), do Ilot relate to previ-ous dialogue on the discourse l vel.ACRES DE COLING-92, NANq~S, 23-28 AO'/Zr 1992 3 1 3 PROC.
OF COLING-92, NANTES, AUG. 23-28, 1992t9t SI: Who is teaching CS3607( $2: Dr.
Smith.
(11) SI: Isn't Dr. Smith lhe professor who wonthe teaching award last year?While it is at least plausible that, in the mind of thespeaker, there is a rule which makes winning a teachingaward inconsistent with teaching CS360, interpreting(11) as expressing doubt at (10) seems incorrect.
Thus,to prevent such erroneous interpretations, we contendthat evidence is needed to recognize discourse actionsthat capture the relationship between a new utteranceand the existing dialogue context.In our recognition algorithm, evidence may taketwo forms: 1) world knowledge indicating that tile ap-plicability conditions for an e-action are satisfied, and2) linguistic evidence from clue words suggesting a par-t itular discourse action.
If the system has evidence thatthe applicability conditions of an e-action are satisfied,tilen the system will use the knowledge as evidencethat this may be the discourse action that the speakeris pursuing.
On the other hand, if there is sufficientlinguistic knowledge suggesting a particular discourseaction, then these applicability conditions hould beattributed to the speaker, as long as they are plausi-ble (i.e., if there is nothing in the system's model ofthe speaker's beliefs to suggest hat the applicabilityconditions are not satisfied).
So, if the clue word butis used, then a non-acceptance discourse action suchas expressing doubt should be easier to recognize (i.e.,silould require less evidence that the applicability con-ditions hold) than if the clue word is not present.For example, consider the following, in whichthere are no clue words in (14a) and (14b), but (14~)contains the clue word but.
(12) Sl: Who is teaching CS3607(13) $2: Dr.
Smith.
(14,) SI: Isn't Dr. Smith on sabbatical next year?
(14b) SI: Isn't Dr. Smith the professor who wonthe teaching award last year?
(14c) SI: But isn't Dr. Smith the professor whowon the teaching award last year?Chaining from (14~) could produce an infer-ence path containing an Express-Doubt discourse ac-tion.
The surface form of (14,) establishes that S1 hasan uncertain belief that Dr. Smith is on sabbatical, thefirst applicability condition for an Express-Doubt ac-tion (see Figure 2).s The effect of the question/answerpair in (12) and (13) is that SI believes that $2 thinksthat Dr. Smith teaches CS360, tile second applicabil-ity condition in all Express-Doubt action.
Tile stereo-type mmhl\[e contains the belief that professors on sab-batical do not teach, so the system can ascribe to SIthe following belief: Vx,y (course(y) A professor(x)A on-sabbatical(x)) ~ ~teachcs(x, y).
This belief sat-isfies tbe third applicability condition (the belief abouta rule), and this belief, along with the belief that Dr.Smith is on sabbatical, implies that Dr. Smith is notteacbmg CS360, the fourth applicability condition.
Fi-nally, a check of the DM indicates that the propositioneThe first applicability condition is actually an uncertain be-lief.
\[LC92\] describes our fortnal belief model and how beliefstrengths are represented in recipes and in our belief model.that Dr. Smith teaches CS360 is in focus, the last ap-plicability condition.
Therefore, since there is evidencefor the applicability conditions of the Express-Doubtaction, and since focusing heuristics uggest hat thisis a coherent discourse action (although not the mostpreferred), (14a) is recognized as expressing doubt atS2's answer, that Dr. Smith is teaching CS360.If the dialogue included (14b) or (14c) insteadof (14a), some of the same evidence would exist.
Illboth (14b) and (14c), the system believes l) that S1believes that Dr. Smitll won the teaching award lastyear (though S1 is not sure of this), 2) that S1 believestlmt $2 thinks that Dr. Smith is teaching CS360, and3) that the proposition that Dr. Smith teaches CS360 isin focus.
Thus, some of the applicability conditions forexpressing doubt at Dr. Smith teaching CS360 hold.However, tim system has no knowledge for tile crucialimplication that determines how this utterance relatesto tile preceding dialogue; the system has no evidencethat S1 believes that winning a teaching award impliesthat Dr. Smith is not teaching CS360.
Therefore, (14b)is not interpreted as an expression of doubt at the re-sponse in (13), and other discourse acts are consid-ered.
The presence of the clue word in (14e), however,strongly suggests an Express-Doubt discourse act andthus less evidence is needed to recognize it; that is, thesystem does not need explicit evidence that S1 holdsthe requisite beliefs but only needs to be able to plan-sibly ascribe them to SI.
Therefore, since mlr modelcall plausibly ascribe to S1 belief in the implicationthat Dr. Smith winning a teaching award implies thatDr.
Smith is not teaching CS360, the system will rec-ognize (14~) and (14c) as expressions of doubt, usingevidence from linguistic knowledge for (14~) aml fromworld knowledge for (14o).Thus, we want our system to use lin-guistic knowledge when present, world knowledge whenpresent, and both when possible.
Onr algorithm forprocessing is tim following: from the semantic repre-sentation of the utterance, infer sequences of actions(inference paths) using plan inference roles.
If the ap-plicability conditions for any of these actions are im-plausible, reject the inference.
For actions which arenot e-actions, tentatively ascribe the beliefs in the ap-plicability comlitions.
For actions that are e-actions,determine how much evidence is available for tile ac-tion.
If there is more than one e-action for which thereis evidence from both linguistic and world knowledge,then choose the inference path closest to the focus ofattention for which there is multiple evidence.
If lin-guistic or world evidence is available for nmre than onee-action, then choose the inference path closest to thefocus of attention with this single supporting piece ofevidence.
If there is no evidence for any e-action, thenchoose tim inference path which contains no e-actionsand is closest o the focus of attention.5 Completing the ExampleWe return briefly to the dialogue given in Sec-tion 3 to ilhlstrate how our process model uses theabove algorithm to recognize complex communicativeactions uch as expressing doubt as well as implicit ac-ceptance of previous utterances.Utterances (1) and (2) were discussed earlier,ACTES DE COLING-92, NANTES, 23-28 Ao(rr 1992 3 1 4 PROC.
OF COLING-92, NANTES, AUO.
23-28, 1992Dluoourse LevelDom~dn Leveli .
.~.
.
ip r ol?l.em -..,~ly} n~ .1: .re(el .
.
.
.
\] .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.I l n s t a n t i a t e - V ~?
.1 l:llnstantiate-Sin~le-Var \] I~, .
,~s i .N0v~ I ,.
.
.
.
.
- :~-  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.-~- ":- ~ ~ .~ ,..---~-- = Enable Arc= ,~tlbactloll ArcObtain-hffo-Ref(1) (2)(3) (4)(6)(5)Figure 3: Dialogue Model for Exampleand the I}M for these utterances is given in Figure 1.Tile semantic representation of(3)  is: Surface-Neg-YN-Quest ion(Sl ,  $2, undergrad-eonrse(CS360)).The surface form of the utterance suggests thatSl thinks that CS360 is all undergrad course, hut is notcertain of it.
This  belief is tentatively entered into thesystem's  model of S l ' s  beliefs.
Plan inference rules sug-gest that the surface request is an action in a Convey-Uncertain-Belief action.
The linguistic clue but sug-gests that S1 is objecting to sornc previous utteranceof S2's.
One of the possible infereuees from the Convey-Uncertain-Belief action is an Ezpress-Doubt action.tlowever, since this is an e-action, evidence is neces-sary to infer this action.
The linguistic clue providesone piece of evidence to support  this inference, but thesystem also looks for evidence from world knowledge.It therefore cheeks to see if it is plausible that S1 's beliefabout CS360 being an undergrad course might  mq)tyt, hat Dr. Smith  is not teaching CS360.
Although tbesystem has no such belief explicitly represented in itsmodel of S 1, there is also no evidence to suggest hat S 1does not helieve that this implication might  hold.
Sincethere is no other e-action for which there is evidenceand since tim applicabil ity conditions for an Express-Doubt action are plausible, the system infers that theConvey-Uncertain-Belief action may be an action inan Express-Doubt action which may be an action in anAddress-Unacceptance discourse action which may bean action in an Address-Believability discourse actionwhich may be an action in an lnJorm action.
Focusingheuristics suggest hat this hfform is the same Informthat (2) is pursuing.
Tiros, (3) is interpreted ms notaccet)ting (2) by expressing doubt at it.
vlnferencing for the rcmainder of the dialogue iss imilar to the first three utterances.
The int~rencepaths which result from utterances (4) and (5) areshown in Fig~lre 3 above the appropriate numbers.Finally, although the system initially a~tentptsto closely relate utterance (6) to other utterances atthe discourse level, there is no evidence for any e-actionthat might  link this utterancc to the exist ing contexton tile discourse level.
There arc no clue words tosuggest a relationship, and there arc no e-actions forwhich there is evidence that the applicabil ity conditionshoht.
Therefore, a completely new discourse action ofobtaining information is inferred, s Ttfis init iation ofa new discourse action indicates implicit  acceptanceof tile previous discourse actions since if S1 did notaccept $2'.~ answer, $1 would be required to indicatenon-acceptance \[LC92\].
The DM for the entire dia-logue is shown in 1,'igure 3 (for space rea.sons, only tileaction names are shown).
Thus our model recognizesboth acceptance and non-acceptance of communicated7Although not discussed, there must also be m~ c-actlon thatrelates (2) to tile DM.
This action is tile Answer-J~e\]; vidence forthe An.~wer-Re\] action is fi'om world knowledge, whic:h indicatesthat the applicat)ility conditions for this Answer- Re\] action hold.htferencing of (2) is then similar to that of (3).8Further discussion may determine that SI is trying to con-tinue the negotiation dialogue; however, that has not been com-municated by this uttera~lce.
We a~'e ittvestigatlng how our sys-tern might modify the I)M that it has built if it discovers laterthat the structure built previously is incorrect.ACTES DE COLING-92, NANTES, 23-28 AOt~Tr 1992 3 1 5 PROC.
OF COLING-92, NANTES, AUG. 23-28, 1992propositions, including acceptance after negotiation of \[GS86\]conflicting beliefs.This example has illustrated: 1) how the struc-ture of a discourse isidentified in our three level model,how our model recognizes the relationship of the cur- \[Rin89\]rent utterance to the existing context and to other ut-terances, and how the tripartite structure produces aricher model of discourse structure than previous mod-els; 2) how beliefs are communicated, recognized, andused in the identification of discourse actions and dis-course structure; and 3) how our process model uses \[LA87\]linguistic, world, and contextual knowledge together inorder to recognize acceptance and non-acceptance ofcommunicated propositions.6 Conc lus ions  and  Future  Work  \[LC91\]Our plan-based model of dialogue incorporatesworld, linguistic, and contextual knowledge sourcesinto the recognition of communicative actions.
L in -  \[LC92\]guistic knowledge suggests certain discourse acts, aspeaker's beliefs, and the strength of those beliefs; con-textual knowledge suggests the most coherent contin-uations of the dialogue; and world knowledge providesevidence that the applicability conditions hold for those \[MT83\]discourse acts that identify the relationship of the cur-rent utterance to the discourse as a whole.
By com-bining these different knowledge sources, we are ableto recognize complex discourse acts such as express- \[Po186\]ing doubt, to identify the relationship of utterances toone another, and to capture the rich structure of task-oriented ialogue.Grosz and Sidner \[GS86\] claim that a robustmodel of understanding must use constraint satisfac-tion to interpret utterances; that is, when evidence is \[Pol90\]available from one source, less evidence isneeded fromother sources.
We have partially included their sug-gestion by using world and linguistic knowledge whencontextual knowledge is not sufficient o infer actionsfor which there must he some evidence.
However, wewould like to expand our notion of partial evidence \[Ram91\]to allow evidence from the three knowledge sources tobe represented in terms of degree: thus, when worldknowledge is overwhelmingly strong, no other knowl-edge is needed, but when it is very weak, knowledgefrom other sources will be needed to support he infer-ences not allowed by the weak world knowledge alone.References\[A1179\]\[Car87\]\[Car89\]\[~ri75\]James F. Allen.
A Plan-Based Approach toSpeech Act Recognition.
PhD thesis, Univer-sity of qbronto, Toronto, Ontario, Canada,1979.Sandra Carberry.
Pragmatic Modeling: To-ward a Robust Natural Language Interface.Computational Intelligence, 3:117-136, 1987.Sandra Carberry.
A Pragmatics-Based Ap-proach to Ellipsis Resolution.
ComputationalLinguistics, 15(2):75-96, 1989.Joseph E. Grimes.
The Thread of Discourse.Mouton, The Hague, Paris, 1975.\[Rei78\]\[Sid81\]\[Sid851\[Wil81\]Barbara Grosz and Candaee Sidner.
Atten-tion, Intention, and theStructure of Discourse.
Computational Lin-guistics, 12(3):175-204, 1986.Elizabeth Hinkelman.
Two Constraints onSpeech Act Ambiguity.
In Proceedings ofthe 27th Annual Meeting of the Associationfor Computational Linguistics, pages 212-219, Vancouver, Canada, 1989.Diane Litmus and James Alien.
A PlanRecognition Model for Subdialogues in Con-versation.
Cognitive Science, 11:163-200,1987.Lynn Lambert and Sandra Carberry.
A Tri-partite Planobased Model of Dialogue.
In Pro-ceedings of the 29th Annual Meeting of theACL, pages 47-54, Berkeley, CA, June 1991.Lynn Lambert and Sandra Carberry.
Model-ing Negotiation Dialogues.
In Proceedings ofthe 30th Annual Meeting of the ACL, Newark,DE, June 1992.
To appear.William C. Mann and Sandra A. Thompson.Relational Propositions in Discourse.
Techni-cal Report ISI/RR-83-115, ISI/USC, Novem-ber 1983.Livia Polanyi.
The Linguistics DiscourseModel: Towards a Formal Theory of Dis-course Structure.
Technical Report 6409,Bolt Beranek and Newman Laboratories Inc.,Cambridge, Massachusetts, 1986.Martha Pollack.
Plans as Complex MentalAttitudes.
In Philip R. Cohen, Jerry Mor-gan, and Martha E. Pollack, editors, Inten-tions in Communication, pages 77-104.
MITPress, 1990.Lance A. Ramshaw.
A Three-Level Model forPlan Exploration.
In Proceedings of the 29thAnnual Meeting of the Association for Com-putational Linguistics, pages 36-46, Berkeley,California, 1991.Rachel Reichman.
Conversational Coherency.Cognitive Science, 2:283-327, 1978.Candace L. Sidner.
Focusing for Interpreta-tion of Pronouns.
American Journal of Com-putational Linuistics, 7(4):217-231, 981.Candace L. Sidner.
Plan Parsing for IntendedResponse Recognition in Discourse.
Compu-tational Intelligence, 1:1-10, 1985.Robert Wilensky.
Meta-Ptanning: Represent-ing and Using Knowledge About Planning inProblem Solving and Natural Language Un-derstanding.
Cognitive Science, 5:197-233,1981.AcrEs DE COLING-92, NANTES, 23-28 ^ot~r 1992 3 1 6 PROC.
OF COLING-92, NANTES, AUG. 23-28, 1992
