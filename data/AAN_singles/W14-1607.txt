Proceedings of the Eighteenth Conference on Computational Language Learning, pages 58?67,Baltimore, Maryland USA, June 26-27 2014.c?2014 Association for Computational LinguisticsGrounding Language with Points and Paths in Continuous SpacesJacob Andreas and Dan KleinComputer Science DivisionUniversity of California, Berkeley{jda,klein}@cs.berkeley.eduAbstractWe present a model for generating path-valued interpretations of natural languagetext.
Our model encodes a map fromnatural language descriptions to paths,mediated by segmentation variables whichbreak the language into a discrete set ofevents, and alignment variables whichreorder those events.
Within an event,lexical weights capture the contribution ofeach word to the aligned path segment.We demonstrate the applicability of ourmodel on three diverse tasks: a new colordescription task, a new financial news taskand an established direction-followingtask.
On all three, the model outperformsstrong baselines, and on a hard variant ofthe direction-following task it achievesresults close to the state-of-the-art systemdescribed in Vogel and Jurafsky (2010).1 IntroductionThis paper introduces a probabilistic model forpredicting grounded, real-valued trajectories fromnatural language text.
A long tradition of re-search in compositional semantics has focused ondiscrete representations of meaning.
The origi-nal focus of such work was on logical translation:mapping statements of natural language to a for-mal language like first-order logic (Zettlemoyerand Collins, 2005) or database queries (Zelle andMooney, 1996).
Subsequent work has integratedthis logical translation with interpretation againsta symbolic database (Liang et al., 2013).There has been a recent increase in interestin perceptual grounding, where lexical semanticsanchor in perceptual variables (points, distances,etc.)
derived from images or video.
Bruni et al.
(2014) describe a procedure for constructing wordrepresentations using text- and image-based dis-% Change0.980.991.001.011.02Hour of day10 12 14 10 12 14U.S.
stocks rebound after bruising two-day swoonFigure 1: Example stock data.
The chart displaysindex value over a two-day period (divided by thedotted line), while the accompanying headline de-scribes the observed behavior.tributional information.
Yu and Siskind (2013)describe a model for identifying scenes given de-scriptions, and Golland et al.
(2010), Kollar et al.
(2010), and Krishnamurthy and Kollar (2013) de-scribe models for identifying individual compo-nents of scenes described by text.
These all havethe form of matching problems between text andobserved groundings?what has been missing sofar is the ability to generate grounded interpreta-tions from scratch, given only text.Our work continues in the tradition of this per-ceptual grounding work, but makes two contribu-tions.
First, our approach is able to predict simpleworld states (and their evolution): for a generalclass of continuous domains, we produce a repre-sentation of p(world | text) that admits easy sam-pling and maximization.
This makes it possible toproduce grounded interpretations of text withoutreference to a pre-existing scene.
Simultaneously,we extend the range of temporal phenomena thatcan be modeled?unlike the aforementioned spa-tial semantics work, we consider language that de-58scribes time-evolving trajectories, and unlike Yuand Siskind (2013), we allow these trajectories tohave event substructure, and model temporal or-dering.
Our class of models generalizes to a vari-ety of different domains: a new color-picking task,a new financial news task, and a more challengingvariant of the direction-following task establishedby Vogel and Jurafsky (2010).As an example of the kinds of phenomena wewant to model, consider Figure 1, which showsthe value of the Dow Jones Industrial Averageover June 3rd and 4th 2008, along with a finan-cial news headline from June 4th.
There are sev-eral effects of interest here.
One phenomenon wewant to capture is that the lexical semantics of in-dividual words must be combined: swoon roughlydescribes a drop while bruising indicates that thedrop was severe.
We isolate this lexical combi-nation in Section 4, where we consider a limitedmodel of color descriptions (Figure 2).
A secondphenomenon is that the description is composedof two separate events, a swoon and a rebound;moreover, those events do not occur in their tex-tual order, as revealed by after.
In Section 5, weextend the model to include segmentation and or-dering variables and apply it to this stock data.The situation where language describes apath through some continuous space?literal ormetaphorical?is more general than stock head-lines.
Our claim is that a variety of problemsin language share these same characteristics.
Todemonstrate generality of the model, we also ap-ply it in Section 6 to a challenging variant of thedirection-following task described by Vogel andJurafsky (2010) (Figure 3), where we achieve re-sults close to a state-of-the-art system that makesstronger assumptions about the task.2 Three tasks in grounded semanticsThe problem of inferring a structured state repre-sentation from sensory input is a hard one, but wecan begin to tackle grounded semantics by restrict-ing ourselves to cases where we have sequencesof real-valued observations directly described bytext.
In this paper we?ll consider the problemsof recognizing colors, describing time series, andfollowing navigational instructions.
While thesetasks have been independently studied, we believethat this is the first work which presents them ina unified framework, and carries them out with asingle family of models..dark pastel blue(a) (b)Figure 2: Example color data: (a) a named color;(b) its coordinates in color space.Colors Figure 2 shows a color called dark pas-tel blue.
English speakers, even if unfamiliar withthe specific color, can identify roughly what thename signifies because of prior knowledge of themeanings of the individual words.Because the color domain exhibits lexical com-positionality but not event structure, we present ithere to isolate the non-temporal compositional ef-fects in our model.
Any color visible to the humaneye can be identified with three coordinates, whichwe?ll take to be hue, saturation and value (HSV).As can be seen in Figure 2 the ?hue?
axis corre-sponds to the differentiation made by basic colornames in most languages.
Other modifiers act onthe saturation and value axes: either simple oneslike dark (which decreases value), or more compli-cated ones like pastel (which increases value anddecreases saturation).
Given a set of named colorsand their HSV coordinates, a learning algorithmshould be able to identify the effects of each wordin the vocabulary and predict the appearance ofnew colors with previously-unseen combinationsof modifiers.Compositional interpretations of color have re-ceived attention in linguistics and philosophy oflanguage (Kennedy andMcNally, 2010), but whilework in grounded computational semantics likethat of Krishnamurthy and Kollar (2013) has suc-ceeded in learning simple color predicates, ourmodel is the first to capture the machine learningof color in a fine-grained, compositional way.Time series As a first step into temporal struc-ture, we?ll consider language describing the be-havior of stock market indices.
Here, again, thereis a simple parameterization?in this case just asingle number describing the total value of theindex?but as shown by the headline example inFigure 1, the language used to describe changesin the stock market can be quite complex.
Head-59right round the white water [.
.
. ]
but stay quite close ?causeyou don?t otherwise you?re going to be in that stone creekFigure 3: Example map data: a portion of a map,and a single line from a dialog which describesnavigation relative to the two visible landmarks.lines may describe multiple events, or multi-partevents like rebound or extend; stocks do not sim-ply rise or fall, but stagger, stumble, swoon, andso on.
There are compositional effects here aswell: distinction is made between falling andfalling sharply; gradual trends are distinguishedfrom those which occur suddenly, at the beginningor end of the trading day.
Along with temporalstructure, the problem requires a more sophisti-cated treatment of syntax than the colors case?now we have to identify which subspans of thesentence are associated with each event observed,and determine the correspondence between sur-face order and actual order in time.The learning of correspondences between textand time series has attracted more interest in nat-ural language generation than in semantics (Yu etal., 2007).
Research on natural language process-ing and stock data, meanwhile, has largely focusedon prediction of future events (Kogan et al., 2009).Direction following We?ll conclude by apply-ing our model to the well-studied problem offollowing navigational directions.
A variety ofreinforcement-learning approaches for followingdirections on a map were previously investigatedby Vogel and Jurafsky (2010) using a corpus as-sembled by Anderson et al.
(1991).
An exampleportion of a path and its accompanying instructionis shown in Figure 3.
While also representable asa set of real valued coordinates, here 2-d, this dataset looks very different?a typical example con-sists of more than a hundred sentences of the kindshown in Figure 3, accompanying a long path.
Thelanguage, a transcript of a spoken dialog, is alsoconsiderably less formal than the language foundin the Wall Street Journal examples, involving dis-fluency, redundancy and occasionally errors.
Nev-ertheless the underlying structure of this problemand the stock problem are fundamentally similar.In addition to Vogel and Jurafsky, Tellex et al.
(2011) give a weakly-supervised model for map-ping single sentences to commands, and Brana-van et al.
(2009) give an alternative reinforcement-learning approach for following long command se-quences.
An intermediate between this approachand ours is the work of Chen and Mooney (2011)and Artzi and Zettlemoyer (2013), which boot-strap a semantic parser to generate logical formsspecifying the output path, rather than predictingthe path directly.Between them, these tasks span a wide range oflinguistic phenomena relevant to grounded seman-tics, and provide a demonstration of the useful-ness and general applicability of our model.
Whiledevelopment of the perceptual groundwork neces-sary to generalize these results to more complexstate spaces remains a major problem, our threeexamples provide a starting point for studying therelationship between perception, time and the se-mantics of natural language.3 PreliminariesIn the experiments that follow, each training ex-ample will consist of:?
Natural language text, consisting of a con-stituency parse tree or trees.
For a given ex-ample, we will denote the associated trees(T1, T2, .
.
.).
These are also observed at testtime, and used to predict new groundings.?
A vector-valued, grounded observation, ora sequence of observations (a path), whichwe will denote V for a given example.
Wewill further assume that each of these pathshas been pre-segmented (discussed in detailin Section 5) into a sequence (V1,V2, .
.
.
).These are only observed during training.The probabilistic backbone of our model is acollection of linear and log-linear predictors.
Thusit will be useful to work with vector-valued rep-resentations of both the language and the path,which we accomplish with a pair of feature func-tions ?tand ?v.
As the model is defined onlyin terms of these linear representations, we can60?t(T )?Label at root of T?Lemmatized leaves of T?v(V )?Last element of V?Curvature of quadraticapprox.
to V (stocks only)?a(T,Ai, Ai?1)Cartesian prod.
of ?t(T ) with:?I[Aiis aligned]?I[Ai?1is aligned]?A1?Ai?1(if both aligned)Table 1: Features used for linear parameterizationof the grounding model.simplify notation by writing Ti= ?t(Ti) andVi= ?v(Vi).
As the ultimate prediction task is toproduce paths, and not their featurized representa-tions, we will assume that it is also straightforwardto compute ?
?1v, which projects path features backinto the original grounding domain.All parse trees are predicted from input text us-ing the Berkeley Parser (Petrov and Klein, 2007).Feature representations for both trees and paths aresimple and largely domain-independent; they areexplicitly enumerated in Table 1.The general framework presented here leavesone significant problem unaddressed: given a largestate vector encoding properties of multiple ob-jects, how do we resolve an utterance about a sin-gle object to the correct subset of indices in thevector?
While none of the tasks considered in thispaper require an argument resolution step of thiskind, interpretation of noun phrases is one of thebetter-studied problems in compositional seman-tics (Zelle and Mooney (1996), inter alia), andwe expect generalization of this approach to bestraightforward using these tools.We will consider the color, stock, and naviga-tion tasks in turn.
It is possible to view the modelswe give for all three as instantiations of the samegraphical model, but for ease of presentation wewill introduce this model incrementally.4 Predicting vectorsPrediction of a color variable from text has theform of a regression problem: given a vector oflexical features extracted from the name, we wishto predict the entries of a vector in color space.
Itseems linguistically plausible that this regressionis sparse and linear: that most words, if they pro-vide any constraints at all, tend to express prefer-ences about a subset of the available dimensions;and that composition within the domain of a sin-gle event largely consists of words additively pre-dicting that event?s parameters, without complexnonlinear interactions.
This is motivated by theobservation that pragmatic concerns force linguis-tic descriptors to orient themselves along a smallset of perceptual bases: once we have words fornorth and east, we tend to describe intermediatesas northeast rather than inventing an additionalword which means ?a little of both?.As discussed above, we can represent a color asa point in a three-dimensional HSV space.
Let Tdenote features on the parse tree of the color name,and V its representation in color space (consistentwith the definition of ?vgiven in Table 1).
Linear-ity suggests the following model:p(T, V ) ?
e???
?tT?V?22(1)The learning problem is then:argmin?t?T,V????
?tT ?
V??
?22(2)which, with a sparse prior on ?t, is the proba-bilistic formulation of Lasso regression (Tibshi-rani, 1996), for which standard tools are availablein the optimization literature.To predict color space values from a new (fea-turized) name T , we output:argmaxVp(T, V ) = ?
?tT4.1 EvaluationWe collect a set of color names and theircorresponding HSV triples from the EnglishWikipedia?s List of Colors, retaining only thosecolor names in which every word appears at leastthree times in the training corpus.
This leaves aset of 419 colors, which we randomly divide intoa 377-item training set and 42-item test set.
Themodel?s goal will be to learn to identify new col-ors given only their names.We consider two evaluations: one which mea-sures the model?s ability to distinguish the namedcolor from a random alternative?analogous to theevaluation in Yu and Siskind (2013)?and onewhich measures the absolute difference betweenpredicted and true color values.
In particular, inthe first evaluation the model is presented withthe name of a color and a pair of candidates, one61Method Sel.
?
H ?
S ?
V ?Random 0.50 0.30 0.38 0.39Last word 0.78 0.05 0.26 0.17Full model 0.81 0.07 0.21 0.13Human 0.86 - - -Table 2: Results for the color selection task.Sel(ection accuracy) is frequency with which thesystem was able to correctly identify the color de-scribed when paired with a random alternative.Other columns are the magnitude of the averageprediction error along the axes of the color space.Full model selection accuracy is a statistically sig-nificant (p < 0.05) improvement over the baselineusing a paired sign test.the color corresponding to the name and anotherdrawn randomly from the test set, and report thefraction of times the true color is assigned a higherprobability than the random alternative.
In the sec-ond, we report the absolute value of the differencebetween true and predicted hue, saturation, and lu-minosity.We compare against two baselines: one whichlooks only at the last word in the color name (al-most always a hue category), and so captures nocompositional effects, and another which outputsrandom values for all three coordinates.
Resultsare shown in Table 2.
The model with all lexicalfeatures outperforms both baselines on selectionand all but one absolute error metric.4.2 Error analysisAn informal experiment in which the color selec-tion task was repeated on one of the authors?
col-leagues (the ?Human?
row in Table 2) yielded anaccuracy of 86%, only 5% better than the system.While not intended as a rigorous upper bound onperformance, this suggests that the model capac-ity and training data are sufficient to capture mostinteresting color behavior.
The errors that do oc-cur appear to mostly be of two kinds.
In one case,a base color is seen only with a small (or related)set of modifiers, from which the system is unableto infer the meaning of the base color (e.g.
fromJapanese indigo, lavender indigo, and electric in-digo, the learning algorithm infers that indigo isbright purple).
In the other, no part of the colorword is seen in training, and the system outputs anunrelated ?default?
color (teal is predicted to bebright red).5 Predicting pathsThe idea that a sentence?s meaning is fundamen-tally described by a set of events, each associatedwith a set of predicates, is well-developed in neo-Davidsonian formal semantics (Parsons, 1990).We adopt the skeleton of this formal approach bytying our model to (latent) partitions of the in-put sentence into disjoint events.
Rather than at-tempting to pass through a symbolic meaning rep-resentation, however, this event structure will beused to map text directly into the grounding do-main.
We assume that this domain has pre-existingstructure?in particular, that in our input paths V ,the boundaries of events have already been iden-tified, and that the problem of aligning text toportions of the segment only requires aligning tosegment indices rather than fine-grained time in-dices.
This is a strong assumption, and one thatfuture work may wish to revisit, but there existboth computational tools from the changepoint de-tection literature (Basseville and Nikiforov, 1995)and pieces of evidence from cognitive science (Za-cks and Swallow, 2007) which suggest that assum-ing a pre-linguistic structuring of events is a rea-sonable starting point.In the text domain, we make the correspondingassumption that each of these events is syntacti-cally local?that a given span of the input sentenceprovides information about at most one of thesesegmented events.The main structural difference between thecolor example in Figure 2 and the stock market ex-ample in Figure 1 is the introduction of a time di-mension orthogonal to the dimensions of the statespace.
To accommodate this change, we extendthe model described in the previous subsection inthe following way: Instead of a single vector, eachtree representation T is paired with a sequence ofpath featuresV = (V1, V2, .
.
.
, VM).
For the timebeing we continue to assume that there is onlyone input tree per training example.
As before,we wish to model the probability p(T,V), but theproblem becomes harder: a single sentence mightdescribe multiple events, but we don?t know whatthe correspondence is between regions of the sen-tence and segmentsV.Though the ultimate goal is still prediction of Vvectors from novel T instances, we cannot do thiswithout also inferring a set of latent alignments be-tween portions of the path and input sentence dur-ing training.
To allow a sentence to explain mul-62?
?
?A1 A2C1 C2V1 V2T1T2[Stocks rose] [Stocks rose, then fell]acva?ta  tcFigure 4: Factor graph for stocks groundingmodel.
Only a subset of the alignment candidatesare shown.
?tcmaps text to constraints, ?acvmapsconstraints to grounded segments, and ?tadeter-mines which constraints act on which segments.tiple events, we?ll break each T apart into a set ofalignment candidates Ti.
We?ll allow as an align-ment candidate any subtree of T , and additionallyany subtree from which a single constituent hasbeen deleted.We then introduce two groups of latent vari-ables: alignment variables A = (A1, A2, .
.
.
),which together describe a mapping from piecesof the input sentence to segments of the ob-served path, and what we?ll call ?constraint?
vari-ables C = (C1, C2, .
.
.
), which express eachaligned tree segment?s prediction about what itscorresponding path should look like (so that thepossibly-numerous parts of the tree aligned to asingle segment can independently express prefer-ences about the segment?s path features).In addition to ensuring that the alignment isconsistent with the bracketing of the tree, it mightbe desirable to impose additional global con-straints on the alignment.
There are various waysto do this in a graphical modeling framework; themost straightforward is to add a combinatorial fac-tor touching all alignment variables which checksfor satisfaction of the global constraint.
In gen-eral this makes alignment intractable.
If the totalnumber of alignments licensed by this combina-torial factor is small (i.e.
if acceptable alignmentsare sparse within the exponentially-large set of allpossible assignments to A), it is possible to di-rectly sum them out during inference.
Otherwiseapproximate techniques (as discussed in the fol-lowing section) will be necessary.As discussed in Section 2, our financial time-lines cover two-day periods, and it seems naturalto treat each day as a separate event.
Thenthe simple regression model described in thepreceding section, extended to include alignmentand constraint variables, has the form of the factorgraph shown in Figure 4.
In particular, the jointdistribution p(T,V) is the product of four groupsof factors:Alignment factors ?ta, which use a log-linearmodel to score neighboring pairs of factors witha feature function ?a:?ta(Ti, Ai, Ai?1) =e??a?a(Ti,Ai,Ai?1)?A?i,A?i?1e?
?a?a(Ti,A?i,A?i?1)(3)Constraint factors ?tc, which map text featuresonto constraint values:?tc(Ti, Ci) = e?||?
?tTi?Ci||22(4)Prediction factors ?acvwhich encourage pre-dicted constraints and path features to agree:?acv(Ai, Ci, Vj) ={1 if Ai?= je?||Ci?Vj||22o.w.
(5)A global factor ?a?
(A1, A2, ?
?
? )
which placesan arbitrary combinatorial constraint on thealignment.Note the essential similarity between Equations 1and 4?in general, it can be shown that this factormodel reduces to the regression model we gave forcolors when there is only one of each Tiand Vj.5.1 LearningIn order to make learning in the stocks domaintractable, we introduce the following globalconstraints on alignment: every terminal must bealigned, and two constituents cannot be alignedto the same segment.
Together, these simplifylearning by ensuring that the number of termsin the sum over A and C is polynomial (in factO(n2)) in the length of the input sentence.
Wewish to find the maximum a posteriori estimatep(?t, ?a|T,V) for ?tand ?a, which we can do63using the Expectation?Maximization algorithm.To find regression scoring weights ?t, we have:E step:M = E[?iTi(Ti)?
]; N = E[?iTiV?Ai](6)M step:?t= M?1N (7)To find alignment scoring weights ?a, we mustmaximize:?iE??log??e??a?a(Ai,Ai?1,Ti)?A?i,A?i?1e??a?a(A?i,A?i?1,Ti)????
(8)which can be done using a variety of convex op-timization tools; we used L-BFGS (Liu and No-cedal, 1989).The predictive distribution p(V|T ) can also bestraightforwardly computed using the standard in-ference procedures for graphical models.5.2 EvaluationOur stocks dataset consists of a set of headlinesfrom the ?Market Snapshot?
column of the WallStreet Journal?s MarketWatch website,1pairedwith hourly stock charts for each day describedin a headline.
Data is collected over a roughlydecade-long period between 2001 and 2012; af-ter removing weekends and days with incompletestock data, we have a total of 2218 headline/timeseries pairs.
As headlines most often discuss asingle day or a short multi-day period, each train-ing example consists of two days?
worth of stockdata concatenated together.
We use a 90%/10%train/test split, with all test examples following alltraining examples chronologically.We compare against two baselines: one whichuses no text (and so learns only the overall mar-ket trend during the training period), and anotherwhich uses a fixed alignment instead of summing,aligning the entire tree to the second day?s time se-ries.
Prediction error is the sum of squared errorsbetween the predicted and gold time series.We report both the magnitude of the predictionerror, and the model?s ability to distinguish be-tween the described path and a randomly-selectedalternative.
The system scores poorly on squared1http://www.marketwatch.com/Search?m=Column&mp=Market%20Snapshot% Change0.980.991.001.011.02Hour of day10 12 14 10 12 14[U.S. stocks end lower]2[as economic worries persist]1Figure 5: Example output from the stocks task.The model prediction is given in blue (solid), andthe reference time series in green (dashed).
Brack-ets indicate the predicted boundaries of event-introducing spans, and subscripts their order in thesentence.
The model correctly identifies that endlower refers to the current day, and persist pro-vides information about the previous day.Method Sel.
acc.
?
Pred.
err.
?No text 0.51 0.0012Fixed alignment 0.59 0.0011Full model 0.61 0.0018Human 0.72 ?Table 3: Results for the stocks task.
Sel(ectionaccuracy) measures the frequency with which thesystem correctly identifies the stock described inthe headline when paired with a random alterna-tive.
Pred(iction error) is the mean sum of squarederrors between the real and predicted paths.
Fullmodel selection accuracy is a statistically signif-icant improvement (p < 0.05) over the baselineusing a paired sign test.error (which disproportionately penalizes large de-viations from the correct answer, preferring con-servative models), but outperforms both base-lines on the task of choosing the described stockhistory?when it is wrong, its errors are oftenlarge in magnitude, but its predictions more fre-quently resemble the correct time series than theother systems.Figure 5 shows example system output for anexample sentence.
The model correctly identifiesthe two events, orders them in time and gets theirapproximate trend correct.
Table 4 shows some64% Change0.980.991.001.011.02Hour of day10 12 14 10 12 14[U.S. stocks extend losing stretch]1Figure 6: Example error from the stocks task.
Thesystem?s prediction, in blue (solid), fails to seg-ment the input into two events, and thus incor-rectly extends the losing trend to the entire outputtime span.features learned by the model?as desired, it cor-rectly interprets a variety of different expressionsused to describe stock behavior.5.3 Error analysisAs suggested by Table 4, learned weights for thetrajectory-grounded features ?tare largely correct.Thus, most incorrect outputs from the system in-volve alignment to time.
Many multipart events(like rebound) can be reasonably explained usingthe curvature feature without splitting the text intotwo segments; as a result, the system tends to befairly conservative about segmentation and oftenunder-segments.
This results in examples like Fig-ure 6, in which the downward trend suggested bylosing is incorrectly extended to the entire out-put curve.
Here, another informal experiment us-ing humans as the predictors indicates that pre-dictions are farther from human-level performanceWord Sign Magnitude ?103rise 0.27 ?0.78swoon ?0.57 0sharply ?0.22 0.28slammed ?0.36 0lifted 0.66 0Table 4: Learned parameter settings for overalldaily change, which the path featurization decom-poses into a sign and a magnitude.than they are on the colors task.6 Generalizing the modelLast we consider the problem of following navi-gational directions.
The difference between thisand the previous task is largely one of scale: ratherthan attempting to predict the values of only twosegments, we have a long string of them.
The text,rather than a single tree, consists of a sequence oftens or hundreds of pre-segmented utterances.There is one additional complication?ratherthan being defined in an absolute space, as they arein the case of stocks, constraints in the maps do-main are provided relative to a set of known land-marks (like the white water and stone creek in Fig-ure 3).
We resolve landmarks automatically basedon string matching, in a manner similar to Vogeland Jurafsky (2010), and assign each sentence inthe discourse with a single referred-to landmark li.If no landmark is explicitly named, it inherits fromthe previous utterance.
We continue to score con-straints as before, but update the prediction factor:?acv(Ai, Ci, Vj) ={1 if Ai?= je?||li+Ci?Vj||22o.w.
(9)The factor graph is shown in Figure 7; ob-serve that this is simply an unrolled version ofFigure 4?the basic structure of the model is un-changed.
While pre-segmentation of the discoursemeans we can avoid aligning internal constituentsof trees, we still need to treat every utterance as analignment candidate, without a sparse combinato-rial constraint.
As a result, the sum over A andC is no longer tractable to compute explicitly, andapproximate inference will be necessary.For the experiments described in this paper, wedo this with a sequence of Monte Carlo approxi-mations.
We run a Gibbs sampler, iteratively re-sampling each Aiand Cias well as the parametervectors ?tand ?ato obtain estimates of E?tandE?a.
The resampling steps for ?tand ?aare them-selves difficult to perform exactly, so we performan internal Metropolis-Hastings run (with a Gaus-sian proposal distribution) to obtain samples fromthe marginal distributions over ?tand ?a.We approximate the mode of the posterior dis-tribution by its mean.
To follow a new set of direc-tions in the prediction phase, we fix the parametervectors and instead sample overA, C andV, andoutput EV.
To complete the prediction process65?
?
??
?
??
?
?CNANA1 A2 A3C1 C2 C3V1 V2 VMT 1 T2 T 3 TNta  tca?acvFigure 7: Factor graph for the general grounding model.
Note that Figure 4 is a subgraph.we must invert ?v, which we do by producing theshortest path licensed by the features.6.1 EvaluationThe Map Task Corpus consists of 128 dia-logues describing paths on 16 maps, accompa-nied by transcriptions of spoken instructions, pre-segmented using prosodic cues.
See Vogel and Ju-rafsky (2010) for a more detailed description of thecorpus in a language learning setting.
For com-parability, we?ll use the same evaluation as Vogeland Jurafsky, which rewards the system for mov-ing between pairs of landmarks that also appear inthe reference path, and penalizes it for additionalsuperfluous movement.
Note that we are solv-ing a significantly harder problem: the version ad-dressed by Vogel and Jurafsky is a discrete searchproblem, and the system has hard-coded knowl-edge that all paths pass along one of the four sidesof each landmark.
Our system, by contrast, cannavigate to any point in R2, and must learn thatmost paths stay close to a named landmark.At test time, the system is given a new sequenceof text instructions, and must output the corre-sponding path.
It is scored on the fraction ofcorrect transitions in its output path (precision),and the fraction of transitions in the gold pathrecovered (recall).
Vogel and Jurafsky comparetheir system to a policy-gradient algorithm for us-ing language to follow natural language instruc-tions described by Branavan et al.
(2009), and wepresent both systems for comparison.Results are shown in Table 5.
Our system sub-stantially outperforms the policy gradient baselineof Branavan et al., and performs close (particularlywith respect to transition recall) to the system ofVogel and Jurafsky, with fewer assumptions.System Prec.
Recall F1Branavan et al.
(09) 0.31 0.44 0.36Vogel & Jurafsky (10) 0.46 0.51 0.48This work 0.43 0.51 0.45Table 5: Results for the navigation task.
Higher isbetter for all of precision, recall and F1.6.2 Error analysisAs in the case of stocks, most of the predictionerrors on this task are a result of misalignment.In particular, many of the dialogues make passingreference to already-visited landmarks, or definedestinations in empty regions of the map in termsof multiple landmarks simultaneously.
In each ofthese cases, the system is prone to directly visit-ing the named landmark or landmarks instead ofignoring or interpolating as necessary.7 ConclusionWe have presented a probabilistic model forgrounding natural language text in vector-valuedstate sequences.
The model is capable of seg-menting text into a series of events, ordering theseevents in time, and compositionally determiningtheir internal structure.
We have evaluated on a va-riety of new and established applications involvingcolors, time series and navigation, demonstratingimprovements over strong baselines in all cases.AcknowledgmentsThis work was partially supported by BBN underDARPA contract HR0011-12-C-0014.
The firstauthor is supported by a National Science Foun-dation Graduate Research Fellowship.66ReferencesAnne H Anderson, Miles Bader, Ellen Gurman Bard,Elizabeth Boyle, Gwyneth Doherty, Simon Garrod,Stephen Isard, Jacqueline Kowtko, Jan McAllister,JimMiller, et al.
1991.
The HCRCmap task corpus.Language and speech, 34(4):351?366.Yoav Artzi and Luke Zettlemoyer.
2013.
Weakly su-pervised learning of semantic parsers for mappinginstructions to actions.
Transactions of the Associa-tion for Computational Linguistics, 1(1):49?62.Michele Basseville and Igor V Nikiforov.
1995.
De-tection of abrupt changes: theory and applications.Journal of the Royal Statistical Society-Series AStatistics in Society, 158(1):185.SRK Branavan, Harr Chen, Luke S Zettlemoyer, andRegina Barzilay.
2009.
Reinforcement learning formapping instructions to actions.
In Proceedings ofthe Joint Conference of the 47th Annual Meeting ofthe ACL and the 4th International Joint Conferenceon Natural Language Processing of the AFNLP: Vol-ume 1-Volume 1, pages 82?90.
Association for Com-putational Linguistics.Elia Bruni, NamKhanh Tran, andMarco Baroni.
2014.Multimodal distributional semantics.
Journal of Ar-tificial Intelligence Research, 49:1?47.David L Chen and Raymond J Mooney.
2011.
Learn-ing to interpret natural language navigation instruc-tions from observations.
In AAAI, volume 2.Dave Golland, Percy Liang, and Dan Klein.
2010.A game-theoretic approach to generating spatial de-scriptions.
In Proceedings of the 2010 conference onEmpirical Methods in Natural Language Process-ing, pages 410?419.
Association for ComputationalLinguistics.Christopher Kennedy and Louise McNally.
2010.Color, context, and compositionality.
Synthese,174(1):79?98.Shimon Kogan, Dimitry Levin, Bryan R Routledge,Jacob S Sagi, and Noah A Smith.
2009.
Pre-dicting risk from financial reports with regression.In Proceedings of Human Language Technologies:The 2009 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, pages 272?280.
Association for Computa-tional Linguistics.Thomas Kollar, Stefanie Tellex, Deb Roy, and NicholasRoy.
2010.
Grounding verbs of motion in natu-ral language commands to robots.
In InternationalSymposium on Experimental Robotics.Jayant Krishnamurthy and Thomas Kollar.
2013.Jointly learning to parse and perceive: connectingnatural language to the physical world.
Transactionsof the Association for Computational Linguistics.Percy Liang, Michael I Jordan, and Dan Klein.
2013.Learning dependency-based compositional seman-tics.
Computational Linguistics, 39(2):389?446.Dong C Liu and Jorge Nocedal.
1989.
On the limitedmemory BFGS method for large scale optimization.Mathematical programming, 45(1-3):503?528.Terence Parsons.
1990.
Events in the semantics of En-glish.
MIT Press.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In Proceedings of HumanLanguage Technologies: The 2007 Annual Confer-ence of the North American Chapter of the Associ-ation for Computational Linguistics.
Assocation forComputational Linguistics.Stefanie Tellex, Thomas Kollar, Steven Dickerson,Matthew R. Walter, Ashis Gopal Banerjee, SethTeller, and Nicholas Roy.
2011.
Understanding nat-ural language commands for robotic navigation andmobile manipulation.
In In Proceedings of the Na-tional Conference on Artificial Intelligence.Robert Tibshirani.
1996.
Regression shrinkage and se-lection via the lasso.
Journal of the Royal StatisticalSociety.
Series B (Methodological), pages 267?288.Adam Vogel and Dan Jurafsky.
2010.
Learning to fol-low navigational directions.
In Proceedings of the48th Annual Meeting of the Association for Compu-tational Linguistics, pages 806?814.
Association forComputational Linguistics.Haonan Yu and Jeffrey Mark Siskind.
2013.
Groundedlanguage learning from videos described with sen-tences.
In Proceedings of the 51st Annual Meet-ing of the Association for Computational Linguis-tics.
Association for Computational Linguistics.Jin Yu, Ehud Reiter, Jim Hunter, and Chris Mellish.2007.
Choosing the content of textual summaries oflarge time-series data sets.
Natural Language Engi-neering, 13(1):25?49.Jeffrey M Zacks and Khena M Swallow.
2007.
Eventsegmentation.
Current Directions in PsychologicalScience, 16(2):80?84.John M Zelle and Raymond J Mooney.
1996.
Learn-ing to parse database queries using inductive logicprogramming.
In Proceedings of the National Con-ference on Artificial Intelligence, pages 1050?1055.Luke S. Zettlemoyer and Michael Collins.
2005.Learning to map sentences to logical form: Struc-tured classification with probabilistic categorialgrammars.
In Proceedings of the 21st Conferenceon Uncertainty in Artificial Intelligence, pages 658?666.67
