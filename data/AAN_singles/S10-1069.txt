Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 313?316,Uppsala, Sweden, 15-16 July 2010.c?2010 Association for Computational LinguisticsSCHWA: PETE using CCG Dependencies with the C&C ParserDominick Ng, James W. D. Constable, Matthew Honnibal and James R. Currane-lab, School of Information TechnologiesUniversity of SydneyNSW 2006, Australia{dong7223,jcon6353,mhonn,james}@it.usyd.edu.auAbstractThis paper describes the SCHWA systementered by the University of Sydney in Se-mEval 2010 Task 12 ?
Parser Evaluationusing Textual Entailments (Yuret et al,2010).
Our system achieved an overall ac-curacy of 70% in the task evaluation.We used the C&C parser to build CCG de-pendency parses of the truth and hypothe-sis sentences.
We then used partial matchheuristics to determine whether the sys-tem should predict entailment.
Heuristicswere used because the dependencies gen-erated by the parser are construction spe-cific, making full compatibility unlikely.We also manually annotated the develop-ment set with CCG analyses, establishingan upper bound for our entailment systemof 87%.1 IntroductionThe SemEval 2010 Parser Evaluation using Tex-tual Entailments (PETE) task attempts to addressthe long-standing problems in parser evaluationcaused by the diversity of syntactic formalismsand analyses in use.
The task investigates thefeasibility of a minimalist extrinsic evaluation ?that of detecting textual entailment between a truthsentence and a hypothesis sentence.
It is extrin-sic in the sense that it evaluates parsers on a task,rather than a direct comparison of their outputagainst some gold standard.
However, it requiresonly minimal task-specific logic, and the proposedentailments are designed to be inferrable based onsyntactic information alone.Our system used the C&C parser (Clark andCurran, 2007a), which uses the Combinatory Cat-egorial Grammar formalism (CCG, Steedman,2000).
We used the CCGbank-style dependencyoutput of the parser (Hockenmaier and Steedman,2007), which is a directed graph of head-child re-lations labelled with the head?s lexical categoryand the argument slot filled by the child.We divided the dependency graphs of the truthand hypothesis sentences into predicates that con-sisted of a head word and its immediate children.For instance, the parser?s analysis of the sentenceTotals include only vehicle sales reported in pe-riod might produce predicates like include(Totals,sales), only(include), and reported(sales).
If atleast one such predicate matches in the two parses,we predict entailment.
We consider a single pred-icate match sufficient for entailment because thelexical categories and slots that constitute our de-pendency labels are often different in the hypothe-sis sentence due to the generation process used inthe task.The single predicate heuristic gives us an over-all accuracy of 70% on the test set.
Our precisionand recall over the test set was 68% and 80% re-spectively giving an F-score of 74%.To investigate how many of the errors were dueto parse failures, and how many were failures ofour entailment recognition process, we manuallyannotated the 66 development truth sentences withgold standard CCG derivations.
This establishedan upper bound of 87% F-score for our approach.This upper bound suggests that there is stillwork to be done before the system allows trans-parent evaluation of the parser.
However, cross-framework parser evaluation is a difficult problem:previous attempts to evaluate the C&C parser ongrammatical relations (Clark and Curran, 2007b)and Penn Treebank-trees (Clark and Curran, 2009)have also produced upper bounds between 80 and90% F-score.
Our PETE system was much easierto produce than either of these previous attemptsat cross-framework parser evaluation, suggestingthat this may be a promising approach to a diffi-cult problem.313Totals include only vehicle sales reported in period.NP (S\NP )/NP (S\NP )\(S\NP ) N/N N S\NP ((S\NP )\(S\NP ))/NP NP<B?> >(S\NP )/NP N ?
NP (S\NP )\(S\NP )<S\NP ?
NP\NP<NP>S\NP<SFigure 1: An example CCG derivation, showing how the categories assigned to words are combined toform a sentence.
The arrows indicate the direction of application.2 BackgroundCombinatory Categorial Grammar (CCG, Steed-man, 2000) is a lexicalised grammar formalismbased on combinatory logic.
The grammar is di-rectly encoded in the lexicon in the form of combi-natory categories that govern how each word com-bines with its neighbours.
The parsing process de-termines the most likely assignment of categoriesto words, and finds a sequence of combinators thatallows them to form a sentence.A sample CCG derivation for a sentence fromthe test set is shown in Figure 1.
The category foreach word is indicated beneath it.
It can be seenthat some categories take other categories as ar-guments; each argument slot in a category is num-bered based on the order of application, from latestto earliest.
For example:((S/NP1)/(S/NP )2)\NP3Figure 2 shows how the argument slots aremapped to dependencies.
The first two columnslist the predicate words and their categories, whilethe second two show how each argument slot isfilled.
For example, in the first row, only has thecategory (S\NP )\(S\NP ), with argument slot1 filled by include).
It is these dependencies thatform the basis for our predicates in this task.only (S\NP )\(S\NP ) 1 includevehicle N/N 1 salesin ((S\NP )\(S\NP ))/NP 2 periodin ((S\NP )\(S\NP ))/NP 1 reportedreported S\NP 1 salesinclude (S\NP )/NP 2 salesinclude (S\NP )/NP 1 TotalsFigure 2: The dependencies represented by thederivation in Figure 1.Recent work has seen the development of high-performance parsers built on the CCG formalism.Clark and Curran (2007a) demonstrate the use oftechniques like adaptive supertagging, parallelisa-tion and a dynamic-programming chart parsing al-gorithm to implement the C&C parser, a highlyefficient CCG parser that performs well againstparsers built on different formalisms (Rimell et al,2009).
We use this parser for the PETE task.The performance of statistical parsers is largelya function of the quality of the corpora they aretrained on.
For this task, we used models derivedfrom the CCGbank corpus ?
a transformation ofthe Penn Treebank (Marcus et al, 1993) includingCCG derivations and dependencies (Hockenmaier,2003a).
It was created to further CCG researchby providing a large corpus of appropriately anno-tated data, and has been shown to be suitable forthe training of high-performance parsers (Hocken-maier, 2003b; Clark and Curran, 2004).3 MethodOur system used the C&C parser to parse the truthand hypothesis sentences.
We took the dependen-cies generated by the parser and processed these togenerate predicates encoding the canonical formof the head word, its required arguments, and theirorder.
We then attempted to unify the predicatesfrom the hypothesis sentence with the predicatesin the truth sentence.
A successful unification ofpredicates a and b occurs when the head words ofa and b are identical and their argument slots arealso identical.
If any predicate from the hypothe-sis sentence unified with a predicate from the truthsentence, our system returned YES, otherwise thesystem returned NO.We used the 66 sentence development set totune our approach.
While analysing the hypoth-esis sentences, we noticed that many examples re-314YES entailment NO entailment OverallSystem correct incorrect A (%) correct incorrect A (%) accuracy (%) F-scoreSCHWA 125 31 80 87 58 60 70 74median 71 85 46 88 57 61 53 50baseline 156 0 100 0 145 0 52 68low 68 88 44 76 69 52 48 46Table 1: Final results over the test setYES entailment NO entailment OverallSystem correct incorrect A (%) correct incorrect A (%) accuracy (%) F-scoreGold deps 34 6 85 22 4 90 87 87Parsed deps 32 8 80 20 6 77 79 82Table 2: Results over the development setplaced nouns from the truth sentence with indefi-nite pronouns such as someone or something (e.g.Someone bought something).
In most of these casesthe indefinite would not be present in the truth sen-tence at all, so to deal with this we converted in-definite pronouns into wildcard markers that couldbe matched to any argument.
We also incorporatedsensitivity to passive sentences by adjusting the ar-gument numbers of dependents.In its most naive form our system is heavilybiased towards excellent recall but poor preci-sion.
We evaluated a number of heuristics to prunethe predicate space and selected those which im-proved the performance over the development set.Our final system used the part-of-speech tags gen-erated by the parser to remove predicates headedby determiners, prepositions and adjectives.
Wenote that even after predicate pruning our systemis still likely to return better recall performancethan precision, but this discrepancy was masked inpart by the nature of the development set: most hy-potheses are short and so the potential number ofpredicates after pruning is likely to be small.
Thefinal predicates generated by the system for the ex-ample derivation given in Figure 1 after heuristicpruning are:only(include)reported(sales)include(totals, sales)4 ResultsWe report results over the 301 sentence test set inTable 1.
Our overall accuracy was 70%, and per-formance over YES entailments was roughly 20%higher than accuracy over NO entailments.
Thisbias towards YES entailments is a reflection of oursingle match heuristic that only required one pred-icate match before answering YES.
Our systemperformed nearly 20% better than the baseline sys-tem (all YES responses) and placed second overallin the task evaluation.Table 2 shows our results over the developmentcorpus.
The 17% drop in accuracy and 8% drop inF-score between the development data and the testdata suggests that our heuristics may have over-fitted to the limited development data.
More so-phisticated heuristics over a larger corpus wouldbe useful for further fine-tuning our system.4.1 Results with Gold Standard ParsesOur entailment system?s errors could be broadlydivided into two classes: those due to incorrectparses, and those due to incorrect comparison ofthe parses.
To investigate the relative contribu-tions of these two classes of errors, we manuallyannotated the 66 development sentences with CCGderivations.
This allowed us to evaluate our sys-tem using gold standard parses.
Only one anno-tator was available, so we were unable to calcu-late inter-annotator agreement scores to examinethe quality of our annotations.The annotation was prepared with the annota-tion tool used by Honnibal et al (2009).
The toolpresents the user with a CCG derivation producedby the C&C parser.
The user can then correct thelexical categories, or add bracket constraints to theparser using the algorithm described by Djordjevicand Curran (2006), and reparse the sentence untilthe derivation desired is produced.Our results with gold standard dependencies are315shown in Table 2.
The accuracy is 87%, establish-ing a fairly low upper bound for our approach tothe task.
Manual inspection of the remaining er-rors showed that some were due to incorrect parsesfor the hypothesis sentence, and some were due toentailments which the parser?s dependency anal-yses could not resolve, such as They ate wholesteamed grains ?
The grains were steamed.
Thelargest source of errors was our matching heuris-tics, suggesting that our approach to the task mustbe improved before it can be considered a trans-parent evaluation of the parser.5 ConclusionWe constructed a system to evaluate the C&Cparser using textual entailments.
We converted theparser output into a set of predicate structures andused these to establish the presence of entailment.Our system achieved an overall accuracy of 79%on the development set and 70% over the test set.The gap between our development and test accu-racies suggests our heuristics may have been over-fitted to the development data.Our investigation using gold-standard depen-dencies established an upper bound of 87% onthe development set for our approach to the task.While this is not ideal, we note that previous ef-forts at cross-parser evaluation have shown that itis a difficult problem (Clark and Curran (2007b)and Clark and Curran (2009)).
We conclue thatthe concept of a minimal extrinsic evaluation putforward in this task is a promising avenue forformalism-independent parser comparison.ReferencesStephen Clark and James R. Curran.
Parsing theWSJ using CCG and log-linear models.
In Pro-ceedings of the 42nd Annual Meeting of the As-sociation for Computational Linguistics, pages104?111, 2004.Stephen Clark and James R. Curran.
Wide-Coverage Efficient Statistical Parsing with CCGand Log-Linear Models.
Computational Lin-guistics, 33(4):493?552, 2007a.Stephen Clark and James R. Curran.
Formalism-independent parser evaluation with CCG andDepBank.
In Proceedings of the 45th AnnualMeeting of the Association for ComputationalLinguistics, pages 248?255, Prague, Czech Re-public, 25?27 June 2007b.Stephen Clark and James R. Curran.
Compar-ing the accuracy of CCG and Penn TreebankParsers.
In Proceedings of the ACL-IJCNLP2009 Conference Short Papers, pages 53?56,Suntec, Singapore, August 2009.Bojan Djordjevic and James R. Curran.
Fasterwide-coverage CCG parsing.
In Proceedings ofthe Australasian Language Technology Work-shop 2006, pages 3?10, Sydney, Australia, De-cember 2006.Julia Hockenmaier.
Data and models for sta-tistical parsing with Combinatory CategorialGrammar.
PhD thesis, 2003a.Julia Hockenmaier.
Parsing with generative mod-els of predicate-argument structure.
In Proceed-ings of the 41st Annual Meeting of the Associa-tion for Computational Linguistics, pages 359?366.
Association for Computational LinguisticsMorristown, NJ, USA, 2003b.Julia Hockenmaier and Mark Steedman.
CCG-bank: a corpus of CCG derivations and depen-dency structures extracted from the Penn Tree-bank.
Computational Linguistics, 33(3):355?396, 2007.Matthew Honnibal, Joel Nothman, and James R.Curran.
Evaluating a Statistical CCG Parser onWikipedia.
In Proceedings of the 2009 Work-shop on The People?s Web Meets NLP: Collabo-ratively Constructed Semantic Resources, pages38?41, Singapore, August 2009.Mitchell P. Marcus, Mary Ann Marcinkiewicz,and Beatrice Santorini.
Building a large an-notated corpus of English: The Penn Tree-bank.
Computational Linguistics, 19(2):313?330, 1993.Laura Rimell, Stephen Clark, and Mark Steedman.Unbounded Dependency Recovery for ParserEvaluation.
In Proceedings of the 2009 Con-ference on Empirical Methods in Natural Lan-guage Processing, volume 2, pages 813?821,2009.Mark Steedman.
The Syntactic Process.
MITPress, Massachusetts Institute of Technology,USA, 2000.Deniz Yuret, Ayd?n Han, and Zehra Turgut.SemEval-2010 Task 12: Parser Evaluation us-ing Textual Entailments.
In Proceedings of theSemEval-2010 Evaluation Exercises on Seman-tic Evaluation, 2010.316
