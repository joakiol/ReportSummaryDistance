Identify Temporal Websites Based on User Behavior AnalysisYong Wang, Yiqun Liu,Min Zhang, Shaoping MaState Key Laboratory of IntelligentTechnology and Systems,Tsinghua National Laboratory forInformation Science andTechnology,Department of Computer Scienceand Technology, TsinghuaUniversityBeijing 100084, Chinawang-yong05@mails.thu.edu.cnLiyun RuSohu Inc. R&D centerBeijing, 100084, Chinaruliyun@sohu-rd.comAbstractThe web is growing at a rapid speed and it isalmost impossible for a web crawler todownload all new pages.
Pages reportingbreaking news should be stored into searchengine index as soon as they are published,while others whose content is nottime-related can be left for later crawls.
Wecollected and analyzed into users?
page-viewdata of 75,112,357 pages for 60 days.
Usingthis data, we found that a large proportion oftemporal pages are published by a smallnumber of web sites providing news services,which should be crawled repeatedly withsmall intervals.
Such temporal web sites ofhigh freshness requirements can beidentified by our algorithm based on userbehavior analysis in page view data.
51.6%of all temporal pages can be picked up witha small overhead of untemporal pages.
Withthis method, web crawlers can focus onthese web sites and download pages fromthem with high priority.1 IntroductionMany web users prefer accessing news reportsfrom search engines.
They type a few key wordsabout a recent event and navigate to detailedreports about this event from the result list.
Userswill be frustrated if a search engine fails to performsuch service and turn to other search engines to getaccess to news reports.
In order to satisfy the users?needs, many search engines, including Google andYahoo!, provide special channels for news retrievaland their web crawlers have to download newlyappeared pages as soon as possible.
However, theweb is growing exponentially.
The amount of newpages emerging every week is 8% of the wholeweb[Ntoulas et al, 2004].
It is almost impossibleto download all novel pages in time.Only a small proportion of novel pages aretemporal.
They report recent events and should bedownloaded immediately, others which areuntemporal can be downloaded later when it isconvenient.
So many search engines have differenttypes of web crawlers to download the web withdifferent policies.
A common crawler checksupdates of existing pages and crawls untemporalnovel pages of all kinds of web sites with arelatively low frequency, usually once a month.Common crawlers are widely adopted by mostsearch engines, but they are not suitable for newsweb sites which produce a great amount of pagesevery day.
To news pages, there will be a large gapbetween their publication time and downloadingtime.
Users can not get access to news pages intime.
Thus another kind of crawler called instantcrawler is developed.
This crawler only focuses ontemporal novel pages and checks updates of newsweb sites with much smaller intervals.
Mostnewly-arrived content which is of high news valuecan be discovered by the instant crawler.
Taskdistribution is shown in Figure 1.173A relatively small set of web sites which providenews reporting services collectively generate manytemporal web pages.
These sites are valuable forinstant crawlers and can be identified with webpages they previously generated.
If a largeproportion of web pages in a site are temporal, it isprobable that pages published later from this thesite will be temporal.
Instant crawlers can focus ona list of such web sites.Currently, the list of web sites for instant crawlersis usually generated manually, which is inevitablysubjective and easily influenced by crawleradministrators?
preference.
It includes many websites which are actually untemporal.
Also there aremany mixed web sites which have both types ofweb pages.
It is difficult for administrators to makeaccurate judgments about whether such sitesshould be included in the seed list.
So instantcrawlers have to spend precious and limitedbandwidth to download untemporal pages whilemiss many temporal ones.
What is more, thismanually generated list is not sensitive to emergingand disappearing news sites.In this paper, we propose a method to separatetemporal pages from untemporal ones based onuser behavior analysis in page-view data.
Temporalweb page identification is the prerequisite fortemporal web site identification.
A web site istemporal if most pages it publishes are temporaland most of its page-views are received fromtemporal pages.
Then all web sites are rankedaccording to how temporal they are.
Web sitesranked at a high position are included in the seedlist for instant crawlers.
Instant crawlers can focuson web sites in the list and only download pagesfrom these web sites.
Such a list covers a largeproportion of temporal pages with only a smalloverhead of untemporal pages.
The result is minedfrom web user behavior log, which reflects users?preference and avoids subjectivity of crawleradministrators.
Additionally, there are web sitesassociated with special events, such as OlympicGames.
These web sites are temporal only whenOlympic Games are being held.
User behavior datacan reflect the appearance and disappearance oftemporal web sites.An outline for the rest of the paper is as follows:Section 2 introduces earlier research in theevolution and discoverability of the web; Section 3presents the user interest modal to describe webpage lifetime from web users?
perspective, thengives the definition of temporal web pages basedon this model; Section 4 provides a method togenerate a seed list for instant crawlers, and itsresult is also evaluated in the section; Section 5discusses some alternatives in the experiment?Section 6 is the conclusion of this paper andsuggests some possible directions in our futurework.2 Related WorkEarlier researchers performed intensive study onproperties of images of the web graph[Barabasiand Albert, 1999; Broder et al, 2000; Kumar et al,1999; Mitzenmacher, 2004].
Recently, researchersturned their attention to how the web evolves,including the rates of updates of existingpages[Brewington and Cybenko, 2000; Cho andGarcia-Molina, 2000; Fetterly et al, 2004; Pitkowand Pirolli, 1997] and the rates of new pageemergence [Brewington and Cybenko, 2000].
Theysent out a crawler to download web pagesperiodically, compared local images of the weband found characteristics of web page lifetime.Some researchers studied the frequency of webpage update, predicted the lifetime of web pagesand recrawl the already downloaded pages whennecessary to keep the local repository fresh so thatusers are less bothered by stale information.
Theyassumed that pages are modified or deletedrandomly and independently with a fixed rate overtime, so lifetimes of web pages are independentand identically distributed and a sequence ofmodifications and deletions can be modeled by aPoisson process.
Other researchers focused on thediscoverability of new pages[Dasgupta et al,2007].
They tried to discover as many new pagesas possible at the cost of only recrawling a fewknown pages.But the web is growing explosively.
It isimpossible to download all the new pages.
Acrawler faces a frontier of the web, which isconsisted of a set of discovered but notFigure 1.
Job assigned to different crawlerstemporal untemporalnovel pages existing pagesinstant crawler common crawler174downloaded URLs (see Figure 2).
The crawler hasto make a decision about which URLs should bedownloaded first, which URLs should bedownloaded later and which URLs are not worthydownloading at all.
Thus there is some work inordering the frontier for a crawl according to thepredicted quality of the unknown pages[Cho,Garcia-Molina and Page, 1998; Eiron et al, 2004].They predicted quality of pages which have notbeen downloaded yet based on the link structure ofthe web.This job is similar with ours.
We also make anorder of the frontier, in the perspective of freshnessrequirements, not in the perspective of page quality.Freshness requirements differ from pages to pages.Temporal web pages whose freshness requirementtimescale is minute, hour or day are assigned to theinstant crawler with high priority.
Other pages oflower freshness requirements can be crawled later.This study is conducted with user behavior datainstead of link structure.
The link structure of theweb is controlled by web site administrators.
Itreflects the preference of web site administrators,not that of the web users.
Although in many cases,the two kinds of preference are alike, they are notidentical.
User behavior data reveals the real needsof web users.
What is more, link structure can beeasily misled by spammers.
But spammers can dolittle to influence user behavior data contributed bymass web users.Figure 2.
Web from a crawler?s perspective3 Definition of Temporal Web PageBased on Web Page Lifetime Model3.1  Web page lifetime modelA page is born when it is published on a server,and it dies when it is deleted.
But from users?
view,its lifetime should not be defined by whether it isstored on a server but by whether it is accessed byusers, because a web page is useful only when itcan provide information for users.
To web users, itslife really starts at the ?activation day?
when thefirst user visits it.
The page begins to be dormanton its ?dormancy day?
when users no longer visit itany more.
After that, whether it is stored on theserver does not make many differences.
So thevalid lifetime of a web page is the period betweenits activation day and its dormancy day, asubinterval of the period when it is accessible.Users?
access is the only indication that the page isalive.The state of a web page could be recorded withtwo values: alive and dead[Dhyani, 2002; Fetterlyet al, 2003; Cho and Garcia-molina, 2003].
Itsstate during its valid lifetime is more complex andits liveness could be described with a continuousvalue: user interest.
The number of page views itreceives differs every day.
It is more active when itis accessed by many users and it is not so activewhen it is accessed by fewer users.
The amount ofpage views it receives reflects how many users areinterested in it.User interest in a web page is an amount indicatingto what extent web users as a whole are interestedin the page.
A user visits a web page becausehe/she is interested in its content.
The amount ofpage views a page receives is determined by howmuch user interest it can attracts.
User interest in apage is a continuous variable evolving over time.User interest increases if more and more users getto know the page, and it decreases if the content isno longer fresh to users and the page becomesobsolete.
User interest in a page whose content isnot time related typically does not fluctuate greatlyover time.Web page lifetime could be described with userinterest model, and then temporal web pages canbe separated from untemporal ones according todifferent characteristics of their lifetime.3.2  Definition of temporal web pagesThere are two types of new pages: temporal pagesand untemporal ones.
Temporal pages are thosereporting recent events.
Users are interested in atemporal page and visit it only during a few hoursor a few days after it is published.
For example, apage reporting the president election result istemporal.
Untemporal pages are the pages whosecontent is not related with recent events.
There arealways users visiting such pages.
For example, apage introducing swimming skills is untemporal.The two kinds of new pages should be treated withdifferent policies.
The instant crawler has todownload temporal pages as soon as they arediscovered because users are interested in themunknown spacediscovered but not downloaded pagesthe ?frontier?downloaded pages175only in a short time span after they are born.Temporal pages are about new events and cannotbe replaced by earlier pages.
If the instant crawlerfails to download temporal pages in time, searchengine users cannot get the latest informationbecause there are no earlier pages reporting theevent which has just happened.
One week after theevent, even if temporal pages are downloaded, theyare no longer attractive to users, just like a piece ofold newspaper.
In contrast, untemporal pages arenot of exigencies.
There is no need to downloadthem immediately after they are published.
Even ifthey are not downloaded in time, users can still besatisfied by other existing pages with similarcontent, since untemporal pages concern withproblems which have already existed for a longtime and have been discussed in many pages.
Itdoes not make many differences to download themearly or a month later.
So untemporal pages can beleft to common crawlers to be downloaded later.4 Temporal Web Sites IdentificationAlgorithmA seed list for an instant crawler contains temporalweb sites.
There are three steps to generate theseed list: search user?s interest curves to describeweb page lifetime; identify temporal web pagesbased on user interest curves; identify temporalweb sites according to the proportion of temporalpages in each site.4.1 Search User?s Interest CurvesGenerally speaking, few users know a newly bornweb page and pay attention to it.
Later, more andmore users get to know it, become interested in itand visit it.
As time goes by, some pages becomeoutdated and attract less user attention, while otherpages never suffer from obsolescence.
Users?interest in them is relatively constant.
So thetypical trend of user interest evolution is toincrease at first then decrease in the shape of arainbow, or to keep static.
It is true that userinterest in some pages experiences multi-climaxes.But it is very unlikely that those climaxes appear inour observing window of two months.
Since weare studying short term web page lifetime, we donot consider user interest with multi-climaxes.
Thecurve y = f(x) that is used to describe the evolutionof user interest should satisfy 4 conditionsbelow(assuming the page is activated at time 0):1) its field of definition is [0, +?
)2) f(0) = 03) f(x) ?
0 in its field of definition4) it has only one maximumThe probability density function (PDF) oflogarithmic normal distribution is one of thefunctions that satisfy the conditions, so a modifiededition of it, which will be addressed later, is usedto describe the evolution of user interest duringwhole web page lifetime.Anonymous user access log for consecutive 60days is collected by a proxy server.
Multiplerequests to a single page in one day are merged asone request to avoid automatically generated largenumbers of requests by spammers.
Daily pageview data of 75,112,357 pages from November13th 2006 to January 11th, 2007 is recorded.
Pageswhose total page views during the 60 days are lessthan 60 (one page view each day on average) arefiltered out because of lack of reliability, leaving975,151 reliable ones.
In order to retrieve userinterest curves, we build a coordinate first, wherethe x-axis denotes time and y-axis denotes thenumber of page views.
Given the daily page viewdata of a page, there are a sequence of discrete dotsin the coordination (xi, yi), i = 1, 2, ..., 60, wherexi=i is the ith day, yi is the number of page viewson the ith day.
After that, the dots can be fitted withthe formulafx  A  ?	x  A ?
?
?  e??where A, b, ?, ?
are parameters and ?ln(x) is theprobability density function of logarithmic normaldistribution.
Given a page p and its page viewhistory (xi, yi) (i = 1, 2, ..., 60), the four parameterscan be determined and the user interest curve canbe defined as y  fx.
One of the retrieved userinterest curves is shown in Figure 3.Figure 3.
A User Interest Curve1764.2 Identifying Temporal Web Pages?ln(x) is the probability density function of arandom variable.
The integral of ?ln(x) in its fieldof definition is 1.
The total user interest in a webpage accumulated during its whole lifetime is fx?dx  A  ?	x? dx  AThe parameter A of a popular web page is largerthan that of an unpopular one.
In order to avoiddiscriminating popular pages and unpopular ones,parameter A for all pages is set to 1, so the area ofthe region enclosed by user interest curve andx-axis is 1.
After this normalization, each pagereceives one unit user interest during their wholelifetime.Parameter b indicates the birth time of a page.
Wedo not care about the absolute birth time of a page,so parameter b for all pages are set to 0, whichmeans all pages are activated at time 0.The other two parameters ?
and ?
do not change,so the shape of user interest curves is reserved.After the parameter adjusting, the user interestcurve is redefined asy  	  1?2 !
 "#$This simpler definition of user interest curve isused in the rest of this paper.Let?x   ?t&dtbe the cumulative density function of logarithmicnormal distribution.
Given the user interest curveof page p, ?
(x) is the amount of user interestaccumulated x days after its birth (see the grey areain Figure 4).Figure 4.
Accumulated user interestA temporal web page accumulates most of its userinterest during the first few days after its birth.Given a specific x, the larger ?
(x) is, the moretemporal the page is, because it can accumulatemore user interest during the time span.news.sohu.com is a major portal web siteproviding news services.
Most of its pages arenews reportings, which are temporal.
There are6,464 web pages from news.sohu.com and theiruser interest curves are retrieved.
Figure 5 showsthe distribution of ?
(1) of these pages.
As is shownin Figure 5, on the first day of their birth, mostpages have accumulated more than 80% of its totaluser interest of their whole lifetime.
So theproportion of user interest accumulated during thebeginning period of web page lifetime is a usefulfeature to identify temporal web pages.Figure 5.
Distribution of accumulated userinterest on the first day of birthIn order to discern temporal pages fromuntemporal ones, two parameters should bedetermined: n and q. n is the integrating range, q isthe integral quantity and is also the grey area inFigure 4.
Given a web page p, it is temporal if theproportion of user interest accumulated during thefirst n days of its lifetime is more than q (denotedin the inequality ?
(n) > q), vice versa.
focus.cn is aweb site about real estate.
It publishes bothtemporal pages (such as those reporting pricefluctuation information) and untemporal ones(such as those providing house decorationsuggestions).
We annotate 3,040 web pages in theweb site of focus.cn manually, of which 2,337 arelabeled temporal, 703 are labeled untemporal.After parameter adjusting, n is set to 3 and q is setto 0.7 in order to achieve the best performance thatthe maximized hit(the number of correctclassification) is 2,829, miss(the number oftemporal pages which are classified as untemporal)is 141, false alarm(the number of untemporalpages which are classified as temporal) is 70.
Itmeans that a web page will be classified astemporal if in the first three days after its birth, itcan accumulate more than 70% of the total userinterest it attracts during its whole lifetime.020040060080010000 20 40 60 80 100percentage of accumulated user interestnumber of pages177After the classification, 135,939 web pages arelabeled temporal and the other 839,212 pages arelabeled untemporal.4.3 Identifying Temporal Web SitesA web site has many pages.
There are hardly anyweb sites that publish temporal pages oruntemporal pages exclusively.
Instead, an actualweb site usually contains both temporal pages anduntemporal ones.
For example, a web site aboutautomobiles publishes temporal pages reportingthat a new style of cars appears on the market, andit also publishes untemporal pages about how totake good care of cars.
In order to classify websites with mixed types of pages, we presentdefinitions of temporal web sites.From web sites administrators?
view, a web site istemporal if most of its pages are temporal.
So if theproportion of temporal pages of a web site in all itspages is large enough, the web site will beclassified as temporal.
According to this definition,the type of a web site can be controlled by itsadministrator.
If he/she wants to make the web sitetemporal, he/she can publish more temporal pages.But how are these pages received by web users?Even most pages in a web site are temporal, ifusers pay little attention to them and are attractedmainly by untemporal ones, this web site cannot beclassified as temporal.
So a temporal web siteshould also be defined from web users?
view.From web users?
view, a web site is temporal ifmost of its page views are received from temporalpages.
Given a web site which contains bothtemporal pages and untemporal ones, if users aremore interested in its temporal pages, the site ismore likely to be classified as temporal.Both of the two definitions above make sense.
So aweb site has two scores about how temporal it isbased on the two definitions.
The two scores arecalculated in the following formulasScores  the number of temporal pages in sthe number of total pages in sScores ?
the number of page views of tp:;<:=>;?
@A ;AB=C D	 C?
the number of page views of p;<;AB=C D	 Cwhere s is a web site.
Score1 is the proportion oftemporal web pages in all pages from the web site.Score2 is the proportion of page views receivedfrom temporal web pages in all page views to thesite.
Then the two scores are combined withdifferent weights into the final score for the website.Scores  ?
 Scores F ?
 ScoresWeb sites are ranked according to the final score indescending order.
Search engines can pick the websites ranked at high positions in the list as seeds forinstant crawlers.
They can pick as many seeds astheir instant crawler is capable to monitor.
In ourexperiment, we choose the top 100 web sites in theranked list as temporal sites.
Since there are135,939 temporal pages and 839,212 untemporalones in the data set, precision is defined as theproportion of temporal pages of the top 100 websites in all pages of those sites, and recall isdefined as the proportion of temporal pages of thetop 100 web sites in all temporal ones in the dataset.
The ranked list is evaluated with the traditionalIR evaluation criterion: F-Measure [Baeza-Yatesand Ribeiro-Neto, 1999], which is calculated asF IMeasure  2Precision  RecallPrecision F RecallParameter ?
and ?
are adjusted to improveF-Measure.
When the ratio of ?
and ?
is 3:2, themaximized F-Measure is achieved at 0.615, wherethere are 70,110 temporal pages and 21,886untemporal ones in the top 100 web sites in theranked list.4.4 Evaluation of the Temporal Web Site ListHuman annotated results are always consideredoptimal in general experiment result evaluation ininformation retrieval.
However, in our task, humanannotator cannot make a perfect seed list forinstant crawlers, because it is very difficult todecide whether a web site containing appropriateamount of temporal pages and untemporal ones.
Incontrast, the method we  propose make decisionnot only by the proportion of temporal pages in asite, but also by how well each kind of pages arereceived based on the amount of page views theyget.
So the seed list generated from user behaviordata can outperform that generated by humans.Sohu Inc. has a manually generated list containing100 seed web sites for its instant crawler.
This listis evaluated with the method above.
The 100 websites in the list cover 59,113 temporal pages and49,124 untemporal ones.
The performance ofautomatic generated seed list for instant crawlersusing our method is compared with that ofmanually generated list as the base line.
The resultis shown in Table 1.Compared with the base line, the top 100 web sitesin our seed list contain 18.6% more temporal pagesthan those in the manually generated list.
The totalburden of the instant crawler is also reduced by17815.0% since it downloads 16,241 less pages.Base line Our methodTemporal Pages 59,113 70,110Total Pages 108,237 91,996Precision 54.6% 76.2%Recall 43.5% 51.6%F-Measure 0.484 0.615Table 1.
Evaluation of the two seed list forinstant crawlers5 Discussion5.1  Advantages of using user interest curvesThere are three advantages of using user interestcurves instead of raw page-view data.
First, thenumber of page views is determined by the amountof user interest a page receives, but they do notstrictly equal.
Page view data is affected by manyrandom factors, such as whether it is weekday orweekend.
These random factors are called ?noise?in general.
Such noise can influence the number ofpage views, but it is not the determinant factor.
Thenumber of page views is centered on the amount ofuser interest and fluctuate around it, because pageview data is a combination of user interest and thenoise.
User interest curves are less bothered bysuch noise since the noise is effectively eliminatedafter data fitting.
Second, although the observingwindow is two months wide, which is wide enoughto cover lifetime of most temporal web pages,there are still many temporal ones whose lifetimeis across the observing window boundaries.
Suchfragmented page view records will bring inmistakes in identifying temporal web pages.
But ifmost part of the lifetime of a web page lies in theobserving window, the data fitting process is ableto estimate the absent page-view data and make upthe missing part of user interest curve of its wholelifetime.
So the effects brought by cross-boundaryweb pages can be reduced.
Third, user interestcurve is continuous and can be integrated to showthe accumulated user interest to the page in aperiod of time.5.2  Effects of using different parameter valuesIn our experiment, we used a single threshold nand q (see Section 5) and a page is classified as atemporal one if the user interest it accumulatesduring n days after its birth is greater than q. Butweb users receive different types of news atdifferent speed.
We notice that financial,entertainment, political and military news getsthrough users rapidly.
These kinds of news becomeobsolescent to users quickly, usually only a fewminutes or hours after they are published.
So webpages reporting such news are ephemeral and theycan draw users?
attention only in a short periodafter their birth.
In contrast, it takes much moretime for users to know other kinds of news.
Forexample, a web page reporting a volcano eruptionfar away from users may not be so attractive andhas to spend much more time to accumulate thespecific proportion of user interest.
So maybe it isnecessary to give different thresholds for differenttypes of news.In our experiment, we choose values of n and p inorder to get the maximized hit (see Section 5).Some web crawlers may have abundant networkbandwidth and want lower miss.
Other crawlerswhose network bandwidth is very limited areuntolerant with false alarm.
So the result oftemporal page classification can be evaluated bylinear combination with different weightsPerformance  = A?hit - B?miss - C?falsealarmValues of A, B and C can be determined accordingto the capacity of s crawlers.Whether a web site is temporal is determined bythe proportion of its temporal pages and theproportion of its page views received fromtemporal pages.
The two proportions are combinedwith different weights ?
and ?
in order to getmaximized F-Measure (see Section 6).
However,to some extent, the measure of page-viewproportion is misleading, because a hot eventwhich receives a great deal of user attention isusually reported by several news agencies.
It is oflittle value to download redundant reports fromdifferent web sites although they get many pageviews.
Page-view data discriminates against pagesreporting events which receive little attention.Most of these pages cannot be replaced by othersbecause they are usually the only page reportingsuch events.
Whether these pages can be correctlyretrieved influence user experience greatly.
Usersoften judge a search engine by whether the pagesreceiving low attention can be recalled.
So thetemporal page proportion should be assigned withadditional weight to avoid such bias.6 Conclusion and Future WorkThe web is growing rapidly.
It is impossible todownload all new pages.
Web crawlers have tomake a decision about which pages should bedownloaded with high priority.
Previous179researchers made decisions according to pagequality and suggested downloading pages of highquality first.
They ignored the fact that temporalpages should be downloaded first.
Otherwise, theywill become outdated soon.
It is better to downloadthese temporal pages immediately in theperspective of freshness requirement.Only a few web sites collectively publish a largeproportion of temporal pages.
In this paper, analgorithm is introduced to score each web siteabout how temporal it is based on page-view datawhich records user behavior.
Web sites scored highare judged as temporal.
An instant crawler canfocus on temporal sites only.
It can download moretemporal pages and less untemporal ones in orderto improve its efficiency.Temporal web site identification can be done infiner granularity.
There are several possibledirections.
Firstly, many web site administratorsprefer distributing temporal web pages anduntemporal ones in different folder.
For example,pages stored under ?/news/?
are more likely to betemporal.
Secondly, dynamic URLs (URLs thatcontain the character ???
and pairs of parameterand value) generated from the same web page,which are treated as different pages in the currentwork, are very likely to share the same timeliness.For example, if ?/a.asp?p=1?
is a temporal page, itis probable that ?/a.asp?p=2?
is temporal.
In thefuture, we plan to study timeliness of web sites atfolder level instead of site level.ReferencesAlbert-L?szl?
Barab?si, R?ka Albert.
1999.
Emergenceof Scaling in Random Networks, Science, Vol.
286.no.
5439, Pages 509 - 512.Alexandros Ntoulas, Junghoo Cho, Christopher Olston.2004.
What's new on the Web?
The evolution of theWeb from a search engine perspective.
Proceedingsof the 13th conference on World Wide Web, Pages 1- 12.Andrei Broder, Ravi Kumar, Farzin Maghoul, PrabhakarRaghavan, Sridhar Rajagopalan, Raymie Stata,Andrew Tomkins and Janet Wiener.
2000.
GraphStructure in the Web.
Computer Networks, Volume33, Issues 1-6, Pages 309-320.Brian E. Brewington and George Cybenko.
2000.
HowDynamic is the Web?
Computer Networks, Volume33, Issues 1-6, Pages 257-276.Dennis Fetterly, Mark Manasse, Marc Najork and JanetWiener.
2003.
A Large-Scale Study of the Evolutionof Web Pages.
Proceedings of WWW03, Pages669-678.Devanshu Dhyani, Wee Keong NG, and Sourav S.Bhowmick.
2002.
A Survey of Web Metrics.
ACMComputing Surveys, Volume 34, Issue 4, Pages 469?
503.James Pitkow and Peter Pirolli.
1997.
Life, Death, andLawfulness on the Electronic Frontier.
Proceedingsof the SIGCHI conference on Human factors incomputing systems, Pages 383?390, 1997.Junghoo Cho , Hector Garcia-Molina and LawrencePage.
1998.
Efficient Crawling Through URLOrdering.
Computer Networks, Volume 30, Number1, Pages 161-172(12).Junghoo Cho and Hector Garcia-Molina.
2000.
Theevolution of the web and implications for anincremental crawler.
In Proc.
26th VLDB, Pages200?209.Junghoo Cho and Hector Garcia-Molina.
2003.
EffectivePage Refresh Policies for Web Crawlers.
ACMTransactions on Database Systems, Volume 28 ,Issue 4, Pages 390 ?
426.Michael Mitzenmacher.
2004.
A Brief History ofLognormal and Power Law Distributions.Proceedings of the 39th Annual Allerton Conferenceon Communication, Control, and Computing.Nadav Eiron, Kevin S. McCurley and John A. Tomlin.2004.
Ranking the Web Frontier.
Proceedings of the13th international conference on World Wide Web,pages 309?318.Ravi Kumar , Prabhakar Raghavan , SridharRajagopalan and Andrew Tomkins.
1999.
Trawlingthe Web for Emerging Cyber-communities.Proceeding of the eighth international conference onWorld Wide Web, Pages 1481-1493.Ricardo A. Baeza-Yates and Berthier Ribeiro-Neto.1999.
Modern Information Retrieval.Addison-Wesley Longman Publishing Co., Inc.,Boston, MA.180
