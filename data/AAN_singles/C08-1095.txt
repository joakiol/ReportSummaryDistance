Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 753?760Manchester, August 2008Shift-Reduce Dependency DAG ParsingKenji Sagae?Institute for Creative TechnologiesUniversity of Southern California13274 Fiji WayMarina del Rey, CA 90292sagae@ict.usc.eduJun?ichi TsujiiDepartment of Computer ScienceUniversity of TokyoSchool of Computer ScienceUniversity of ManchesterNational Center for Text Miningtsujii@is.s.u-tokyo.ac.jpAbstract?Most data-driven dependency parsingapproaches assume that sentence struc-ture is represented as trees.
Althoughtrees have several desirable propertiesfrom both computational and linguisticperspectives, the structure of linguisticphenomena that goes beyond shallowsyntax often cannot be fully captured bytree representations.
We present a pars-ing approach that is nearly as simple ascurrent data-driven transition-based de-pendency parsing frameworks, but out-puts directed acyclic graphs (DAGs).
Wedemonstrate the benefits of DAG parsingin two experiments where its advantagesover dependency tree parsing can beclearly observed: predicate-argumentanalysis of English and syntactic analysisof Danish with a representation that in-cludes long-distance dependencies andanaphoric reference links.1 IntroductionNatural language parsing with data-driven de-pendency-based frameworks has received an in-creasing amount of attention in recent years(McDonald et al, 2005; Buchholz and Marsi,2006; Nivre et al, 2006).
Dependency represen-tations directly reflect word-to-word relation-?
?
This work was conducted while the author was atthe Computer Science Department of the Universityof Tokyo.?
2008.
Licensed under the Creative Commons At-tribution-Noncommercial-Share Alike 3.0 Unportedlicense (http://creativecommons.org/licenses/by-nc-sa/3.0).
Some rights reserved.ships in a dependency graph, where the words ina sentence are the nodes, and labeled edges cor-respond to head-dependent syntactic relations.
Inaddition to being inherently lexicalized, depen-dency analyses can be generated efficiently andhave been show to be useful in a variety of prac-tical tasks, such as question answering (Wang etal., 2007), information extraction in biomedicaltext (Erkan et al, 2007; Saetre et al 2007) andmachine translation (Quirk and Corston-Oliver,2006).However, despite rapid progress in the devel-opment of parsers for several languages (Nivre etal., 2007) and algorithms for more linguisticallyadequate non-projective structures (McDonald etal., 2005; Nivre and Nilsson, 2006), most of thecurrent data-driven dependency parsing ap-proaches are limited to producing only depen-dency trees, where each word has exactly onehead.
Although trees have desirable propertiesfrom both computational and linguistic perspec-tives, the structure of linguistic phenomena thatgoes beyond shallow syntax often cannot be fullycaptured by tree representations.
Well-knownlinguistically-motivated dependency-based syn-tactic frameworks, such as Hudson?s WordGrammar (Hudson, 1984), recognize that torepresent phenomena such as relative clauses,control relations and other long-distance depen-dencies, more general graphs are needed.
Hud-son (2005) illustrates his syntactic frameworkwith the analysis shown in figure 1.
In this ex-ample, the arcs above the sentence correspond toa typical dependency tree commonly used in de-pendency parsing.
It is clear, however, that theentire dependency structure is not a tree, but adirected acyclic graph (DAG), where words mayhave one or more heads.
The arcs below the sen-tence represent additional syntactic dependenciescommonly ignored in current dependency pars-ing approaches that are limited to producing tree753structures.
There are several other linguisticphenomena that cannot be represented naturallywith dependency trees, but can easily berepresented with dependency DAGs, includinganaphoric reference and semantically motivatedpredicate-argument relations.
Although there areparsing approaches (often referred to as deepparsing approaches) that compute DAG depen-dency structures, this is usually done throughmore complex lexicalized grammar formalisms(such as HPSG, CCG and LFG) and unificationoperations with tree-based parsing algorithms.We introduce a new data-driven frameworkfor dependency parsing that produces dependen-cy DAGs directly from input strings, in a mannernearly as simple as other current transition-baseddependency parsers (Nivre et al, 2007) producedependency trees.
By moving from tree struc-tures to DAGs, it is possible to use dependencyparsing techniques to address a wider range oflinguistic phenomena beyond surface syntax.We show that this framework is effective andefficient in analysis of predicate-argument de-pendencies represented as DAGs, and in syntac-tic parsing using DAGs that include long-distance dependencies, gapping dependents andanaphoric reference information, in addition tosurface syntactic dependents.Our parsing framework, based on shift-reducedependency parsing, is presented in section 2.Experiments and results are presented and dis-cussed in section 3.
We review related work insection 4, and conclude in section 5.2 A shift-reduce parsing framework fordependency DAGsOne of the key assumptions in both graph-based(McDonald et al, 2005) and transition-based(Nivre, 2004; Nivre and Nilsson, 2006) ap-proaches to data-driven dependency parsing isthat the dependency structure produced by theparser is a tree, where each word has exactly onehead (except for a single root word, which has nohead in the sentence).
This assumption, ofcourse, has to be abandoned in dependency DAGparsing.
McDonald et al (2006) point out that,while exact inference is intractable if the treeconstraints are abandoned in their graph-basedparsing framework, it is possible to computemore general graphs (such as DAGs) using ap-proximate inference, finding a tree first, and add-ing extra edges that increase the graph?s overallscore.
Our approach, in contrast, extends shift-reduce (transition-based) approaches, finding aDAG directly.
Because data-driven shift-reducedependency parsing is based on local decisions(informed by rich a rich feature set), the addi-tional computational cost of computing DAGsinstead of trees is small in practice, as we willshow.We first describe how the basic shift-reducebottom-up dependency parsing algorithm de-scribed by Nivre (2004) can be modified to allowmultiple heads per word.
We then explore thesame type of modification to Nivre?s arc-eageralgorithm, which is a variant of the basic shift-reduce algorithm where arcs can be created at thefirst opportunity.
Like their tree counterparts,our algorithms for dependency DAGs produceonly projective structures, assuming that projec-tivity for DAGs is defined in much the same wayas for trees.
Informally, we define a projectiveDAG to be a DAG where all arcs can be drawnabove the sentence (written sequentially in itsoriginal order) in a way such that no arcs crossand there are no covered roots (although a root isnot a concept associated with DAGs, we borrowthe term from trees to denote words with noheads in the sentence).
However, non-projectivity is predictably more wide-spread inDAG representations, since there are at least asmany arcs as in a tree representation, and oftenmore, including arcs that represent non-local re-lationships.
We then discuss the application ofpseudo-projective transformations (Nivre andNilsson, 2005) and an additional arc-reversingtransform to dependency DAGs.
Using a shift-reduce algorithm that allows multiple heads perword and pseudo-projective transformations to-What do you   think we should wait for ?rxxxxx,csss sor rFigure 1: Word Grammar dependency graph(Hudson, 2005).
Key for edge types: com-plement (c), object (o), sharer/xcomp (r),subject (s), and extractee (x).754gether forms a complete dependency DAG pars-ing framework.2.1 Basic shift-reduce parsing with multipleheadsThe basic bottom-up left-to-right dependencyparsing algorithm described by Nivre (2004)keeps a list of tokens (initialized to contain theinput string) and a stack (initialized to be empty),and allows three types of actions: (1) shift, whichremoves the next item from the input list andpushes it onto the top of the stack; (2) left-reduce, which pops the top two items from thestack, creates a left-arc between the words theyrepresent in the sentence, and push the top item(which is now the head of the item previouslybelow it) back onto the stack; and (3) right-reduce, which works in the same way as left-reduce, but creates a right-arc instead, and push-es back onto the stack the item that was belowthe top item on the stack (which is now the headof the item previously on top of the stack)1.
Newdependency edges (or arcs) are only created byreduce actions, which are constrained so thatthey can only be applied to create a head-dependent pair where the dependent has alreadyfound all of its own dependents (if any).
This isnecessary because once a word is assigned ahead it is popped from the stack and never visitedagain, since each word has only one head.
Thisconstraint, responsible for the parser?s bottom-upbehavior, should be kept in mind, as it is relevantin the design of the multiple-head parsing algo-rithm below.To allow words to have multiple heads, wefirst need to create two new parser actions thatcreate dependency arcs without removing thedependent from further consideration for being adependent of additional heads.
The first newaction is left-attach, which creates a left depen-dency arc attaching the top two items on thestack, making the top item the head of the itemimmediately below, as long as a right arc be-tween the two items does not already exist.
Thisaction is similar to left-reduce, except that nei-ther item is removed from the stack (no reductionoccurs).
The second new action, right-attach,includes one additional final step: first, it createsa right dependency arc between the top two itemson the stack (as long as a left arc between the twoitems does not already exist), making the topitem a dependent of the item immediately below;1 Like Nivre (2004), we consider the direction of thedependency arc to be from the head to the dependent.X Y ZInitial state:(a) Desired output:X Y ZInput tokens StackAction: SHIFTY ZInput tokens StackXCurrent arcs:   X    Y    ZCurrent arcs:   X    Y    ZAction: SHIFTZYInput tokens StackXCurrent arcs:   X    Y    ZAction: LEFT-ATTACHZYInput tokens StackXCurrent arcs:   X    Y    ZAction: SHIFTZYInput tokens StackXCurrent arcs:   X    Y    ZAction: LEFT-REDUCEZInput tokens StackXCurrent arcs: X      Y     ZAction: LEFT-REDUCEZInput tokens StackCurrent arcs: X      Y     ZX Y ZInitial state:(b) Desired output:X Y ZInput tokens StackAction: SHIFTY ZInput tokens StackXCurrent arcs:   X    Y    ZCurrent arcs:   X    Y    ZAction: SHIFTZYInput tokens StackXCurrent arcs:   X    Y    ZAction: SHIFTZYInput tokens StackXCurrent arcs:   X    Y    ZAction: RIGHT-ATTACHYInput tokens StackXCurrent arcs: X      Y     ZAction: RIGHT-REDUCEXInput StackCurrent arcs: X      Y     ZZZAction:  SHIFTXInput tokens StackCurrent arcs: X      Y     ZZAction: RIGHT-REDUCEZInput tokens StackCurrent arcs: X      Y     ZFigure 2: Example of how the basic algorithmbuilds dependencies with multiple heads.755and, as a second step, it pops the top item on thestack (newly made a dependent), and places itback on the list of input words.
This second stepis necessary because of the constraint that wordscan only be made dependents once all of its owndependents have been found.
The behavior ofthe algorithm is illustrated in figure 2, where (a)shows an application of left-attach, and (b)shows an application of right-attach.
In (b), wenote that without placing the dependent in theright-attach action (Z) back on the input list, thedependency between X and Y could not becreated.
If we abandon the algorithm?s bottom-up behavior, it is possible to modify the parseractions so that it is not necessary to place itemsback in the input list.
This is discussed in section2.2.In summary, the algorithm has each of thethree actions from the tree-based algorithm (shift,right-reduce, and left-reduce), and two additionalactions that allow words to be dependents ofmore than one head (right-attach and left-attach).Although the algorithm as described so far buildsunlabeled structures, the extension to labeledstructures is straightforward: any action that re-sults in a new arc being created must also choosea label for the arc.
Another way to accomplishthe same goal is to have a copy of each arc-producing action for each possible arc label.This is the same labeling extension as in the al-gorithm for trees.
Finally, we note that the algo-rithm does not explicitly prevent multiple arcs(with the same direction) from being created be-tween the same two words.
In the unlabeledcase, such a constraint can be easily placed onarc-producing actions.
In the labeled case, how-ever, it is useful to allow arcs with different la-bels to link the same two words2.2.2 Arc-eager shift-reduce parsing withmultiple headsNivre?s arc-eager algorithm was designed tobuild dependencies at the first opportunity,avoiding situations where items that form a chainof right arcs all have to be placed on the stackbefore any structure is built, as in figure 2(b) forexample.
This is done by creating dependenciesnot between the top two items on the stack, butbetween the single top item on the stack and thenext word on the input list, resulting in a hybrid2 This means that the structures produced by the algo-rithm are technically not limited to projective DAGs,since they can also be projective labeled multi-digraphs.bottom-up/top-down strategy.
A similar idea canresult in an algorithm for dependencies that allowmultiple heads per word, but in this case the re-sulting algorithm is not as similar to the arc-eager algorithm for trees as the algorithm in sec-tion 2.1 is to its tree-based counterpart.The projective DAG arc-eager algorithm hasfour actions, each corresponding to one action ofthe tree-based algorithm, but only the shift actionis the same as in the tree based algorithm.
Thefour actions in the new algorithm are: (1) shift,which removes the next token from the inputstring and pushes it onto the top of the stack; (2)reduce, which pops the stack, removing only itstop item, as long as that item has at least onehead (unlike in the tree-based algorithm, howev-er, the algorithm may not reduce immediatelywhen an item that has a head is on the top of thestack); (3) left-arc, which creates a left depen-dency arc between the word on top of the stackand the next token in the input string, where thetoken in the input string is the head and the itemon the stack is the dependent (the stack and inputlist are left untouched), as long as a right arc doesnot already exist between the two words; and (4)right-arc, which creates a right dependency arcbetween the word on top of the stack and thenext token in the input list, where the item on thestack is the head and the token in the input list isthe dependent (again, the stack and input list areleft untouched), as long as a left arc does not al-ready exist between the two words.Like the algorithm in section 2.1, this algo-rithm can easily be extended to produce labeledstructures, and it also allows multiple edges (withthe same direction) between the same two words.2.3 Graph transformations for DAGsAlthough the algorithms presented in sections 2.1and 2.2 can produce dependency structureswhere a word may have more than one head,they are of limited interest on their own, sincethey can only produce projective structures, andmany of the interesting linguistic phenomena thatcan be represented with DAGs cannot berepresented with projective DAGs.
Fortunately,the pseudo-projective transformations (Nivre andNilsson, 2006) used in tree-based dependencyparsing can easily be applied to DAGs.
Thesetransformations consist of identifying specificnon-projective arcs, and moving their heads uptowards the root, making them projective.
Theprocess also involves creating markings on thelabels of the edges involved, so that the trans-formations are (mostly) reversible.
Because non-756projectivity is more common in linguisticallyinteresting DAGs, however, the trans-form/detransform process may be more lossythan it is when applied to trees.
This, of course,varies according to specific DAGs used forrepresenting specific phenomena.
For pseudo-transformations to work well, we must allowmultiple differently labeled arcs between thesame two words (which, as mentioned before, thealgorithms do).
Combining the algorithm in sec-tions 2.1 or 2.2 with pseudo-projective parsing,we can use DAG training data and produce DAGoutput in the overall parsing framework.An alternative to using pseudo-projectivetransformations is to develop an algorithm forDAG parsing based on the family of algorithmsdescribed by Covington (2001), in the same waythe algorithms in sections 2.1 and 2.2 were de-veloped based on the algorithms described byNivre (2004).
Although this may be straightfor-ward, a potential drawback of such an approachis that the number of parse actions taken in a Co-vington-style algorithm is always quadratic onthe length of the input sentence, resulting inparsers that are more costly to train and to run(Nivre, 2007).
The algorithms presented here,however, behave identically to their linear run-time tree counterparts when they are trained withgraphs that are limited to tree structures.
Addi-tional actions are necessary only when wordswith more than one head are encountered.
Fordata sets where most words have only one head,the performance the algorithms described in sec-tions 2.1 and 2.2 should be close to that of shift-reduce projective parsing for dependency trees.In data sets where most words have multipleheads (resulting in higher arc density), the use ofa Covington-style algorithm may be advanta-geous, but this is left as an area of future investi-gation.In addition to pseudo-projective transforma-tions, an additional transformation that is usefulin DAG parsing is arc reversal.
This consists ofsimply reversing the direction of an edge, addinga special mark to its label to indicate that its di-rection has been reversed.
Detransformation istrivial and can be done with perfect accuracy,since it can be accomplished by simply reversingthe arcs marked as reversed.
This transformationis useful in cases where structures are mostly inDAG form, but may sometimes contain cycles.Arc reversal can be used to change the directionof an arc in the cycle, making the previously cyc-lic structure a DAG, which can be handled in theframework presented here.3 ExperimentsTo investigate the efficacy of our DAG parsingframework on natural language data annotatedwith dependency DAGs, we conducted two expe-riments.
The first uses predicate-argument de-pendencies taken from the HPSG Treebank builtby Miyao et al (2004) from the WSJ portion ofthe Penn Treebank.
These predicate-argumentstructures are, in general, dependency graphs thatdo contain cycles (although infrequently), andalso contain a large number of words with mul-tiple heads.
Since the predicate-argument de-pendencies are annotated explicitly in the HPSGTreebank, extracting a corpus of gold-standarddependency graphs is trivial.
The second expe-riment uses the Danish Dependency Treebank,developed by Kromann (2003).
This treebankfollows a dependency scheme that includes, inaddition to standard grammatical relations com-monly used in dependency parsing, long-distancedependencies, gapping dependents, and anaphor-ic reference links.
As with the HPSG predicateargument data, a few structures in the data con-tain cycles, but most of the structures in the tree-bank are DAGs.
In the experiments presentedbelow, the algorithm described in section 2.1 wasused.
We believe the use of the arc-eager algo-rithm described in section 2.2 would producesimilar results, but this is left as future work.3.1 Learning componentThe DAG parsing framework, as described sofar, must decide when to apply each appropriateparser action.
As with other data-driven depen-dency parsing approaches with shift-reduce algo-rithms, we use a classifier to make these deci-sions.
Following the work of Sagae and Tsujii(2007), we use maximum entropy models forclassification.
During training, the DAGs arefirst projectivized with pseudo-projective trans-formations.
They are then processed by the pars-ing algorithm, which records each action neces-sary to build the correct structure in the trainingdata, along with their corresponding parser con-figurations (stack and input list contents).
Fromeach of these parser configurations, a set of fea-tures is extracted and used with the correct pars-ing action as a training example for the maxi-mum entropy classifier.
The specific features weused in both experiments are the same featuresdescribed by Sagae and Tsujii, with the follow-ing two changes: (1) the addition of a feature thatindicates whether an arc already exists betweenthe top two items on the stack, or the top item on757the stack and the next item on the input list, andif so, what type of arc (direction and label); and(2) we did not use lemmas, morphological in-formation or coarse grained part-of-speech tags.For the complete list of features used, please see(Sagae and Tsujii, 2007).During run-time, the classifier is used to de-termine the parser action according to the currentparser configuration.
Like Sagae and Tsujii, weuse a beam search instead of running the algo-rithm in deterministic mode, although we alsoreport deterministic parsing results.3.2 Predicate-argument analysisThe predicate-argument dependencies extractedfrom the HPSG Treebank include informationsuch as extraction, raising, control, and otherlong-distance dependencies.
Unlike in structuresfrom PropBank, predicate-argument informationis provided for nearly all words in the data.
Fol-lowing previous experiments with Penn Tree-bank WSJ data, or data derived from it, we usedsections 02-21 as training material, section 22 fordevelopment, and section 23 for testing.
Onlythe predicate-argument dependencies were used,not the phrase structures or other informationfrom the HPSG analyses.
Part-of-speech taggingwas done separately using a maximum entropytagger (Tsuruoka and Tsujii, 2005) with accuracyof 97.1%.Cycles were eliminated from the dependencystructures using the arc reversal transform in thefollowing way: for each cycle detected in thedata, the shortest arc in the cycle was reverseduntil no cycles remained.
We applied pseudo-projective transformation and detransformationto determine how much information is lost in thisprocess.
By detransforming the projectivegraphs generated from gold-standard dependen-cies, we obtain labeled precision of 98.1% andlabeled recall of 97.7%, which is below the accu-racy expected for detransformation of syntacticdependency trees, but still within a range weconsidered acceptable.
This represents an upper-bound for the accuracy of the DAG parser (in-cluding the arc-reversal and pseudo-projectivetransformations, and the algorithm described insection 2.1).Table 1 shows the results obtained with ourDAG parsing framework in terms of labeled pre-cision, recall and F-score (89.0, 88.5 and 88.7,respectively).
For comparison, we also showpreviously published results obtained by Miyaoand Tsujii (2005), and Sagae et al (2007), whichused the same data, but obtained the predicate-argument analyses using an HPSG parser.
Ourresults are very competitive, at roughly the samelevel as the best previously published results onthis data set, but obtained with significantlyhigher speed.
The parser took less than four mi-nutes to process the test set, and pseudo-projective and arc-reversal detransformation tookless than one minute in standard hardware (a Li-nux workstation with a Pentium 4 processor and4Gb of RAM).
Sagae et al (2007) reported thatan HPSG parser took about 20 minutes to parsethe same data.
Our results were obtained with abeam width of 150 parser states.
Running theparser with a beam width of 1 (a single parserstate), emulating the deterministic search used byNivre (2004), resulted in numerous parse failures(the end of the input string is reached, and nofurther dependency arcs are created) in the de-velopment set, and therefore very low dependen-cy recall (90.1 precision and 36.2 recall on de-velopment data).
Finally, in table 1 we alsoshow results obtained with standard bottom-upshift-reduce dependency parsing for trees, usingthe parser described in (Sagae and Tsujii, 2007).To train the dependency tree parser, we trans-formed the DAG predicate-argument structuresinto trees by removing arcs.
Arcs were selectedfor removal as follows: for each word that hadmore than one head, only the arc between theword and its closest head (in linear distance inthe sentence) was kept.
Although this strategystill produces dependency analyses with relative-ly high F-score (87.0), recall is far lower thanwhen DAG parsing is used, and the tree parserhas no mechanism for capturing some of thestructures captured by the DAG parser.Parser Precision Recall F-scoreDAG-beam 89.0 88.5 88.7Tree only 89.8 84.3 87.0Sagae et al 88.5 88.0 88.2Miyao & Tsujii 85.0 84.3 84.6Table 1: Results from experiments with HPSGpredicate-argument dependencies (labeled preci-sion, recall and F-score).
Our results are denotedby DAG-beam and tree only, and others are pre-viously published results using the same data.3.3 Danish Dependency Treebank experi-mentsOur experiments with the Danish DependencyTreebank followed the same setup as describedfor the HPSG predicate-argument structures.The accuracy of pseudo-projective transforma-758tion and detransformation was higher, at 99.4%precision and 98.8% recall.
To divide the datainto training, development and test sections, wefollowed the same procedure as McDonald et al(2006), who used the same data, so our resultscould be compared directly (a small number ofgraphs that contained cycles was discarded, asdone by McDonald et al).Our results are shown in table 2 (unlabeledprecision and recall are used, for comparisonwith previous work, in addition to labeled preci-sion and recall), along with the results obtainedby McDonald et al, who used an approximateinference strategy in a graph-based dependencyparsing framework, where a dependency tree iscomputed first, and arcs that improve on theoverall graph score are added one by one.
As inthe previous section, we also include results ob-tained with tree-only parsing.
Obtaining treestructures from the Danish Dependency Tree-bank is straightforward, since anaphoric refer-ence and long-distance dependency arcs aremarked as such explicitly and can be easily re-moved.In addition to overall results, we also meas-ured the parser?s precision and recall on long-distance dependencies and anaphoric reference.On long-distance dependencies the parser had83.2 precision and 82.0 recall.
On anaphoricreference links the parser has 84.9 precision and84.4 recall.
Although these are below the pars-er?s overall accuracy figures, they are encourag-ing results.
Finally, unlike with the HPSG predi-cate-argument structures, using a beam width of1 reduces precision and recall by only about 1.5.Parser Precision Recall F-scoreDAG-beam 87.3 87.1 87.2Tree only 87.5 82.7 85.0McDonald et al 86.2 84.9 85.6DAG-labeled 82.7 82.2 82.4Table 2: Results from experiments with theDanish Dependency Treebank.
Precision, recalland F-score for the first three rows are for unla-beled dependencies.
The last row, DAG-labeled,shows our results in labeled precision, recall andF-score (not directly comparable to other rows).4 Related workThe work presented here builds on the dependen-cy parsing work of Nivre (2004), as discussed insection 2, on the work of Nivre and Nilsson(2006) on pseudo-projective transformations, andon the work of Sagae and Tsujii (2007) in using abeam search in shift-reduce dependency parsingusing maximum entropy classifiers.
As men-tioned before, McDonald et al (2006) presentedan approach to DAG parsing (that could also eas-ily be applied to cyclic structures) using approx-imate inference in an edge-factored dependencymodel starting from dependency trees.
In theirmodel, the addition of extra arcs to the tree waslearned with the parameters to build the initialtree itself, which shows the power and flexibilityof approximate inference in graph-based depen-dency models.Other parsing approaches that produce depen-dency graphs that are not limited to tree struc-tures include those based on linguistically-motivated lexicalized grammar formalisms, suchas HPSG, CCG and LFG.
In particular, Clark etal.
(2002) use a probabilistic model of dependen-cy DAGs extracted from the CCGBank (Hock-enmeier and Steedman, 2007) in a CCG parserthat builds the CCG predicate-argument depen-dency structures following the CCG derivation,not directly through DAG parsing.
Similarly, theHPSG parser of Miyao and Tsujii (2005) buildsthe HPSG predicate-argument dependency struc-ture following unification operations duringHPSG parsing.
Sagae et al (2007) use a depen-dency parsing combined with an HPSG parser toproduce predicate-argument dependencies.However, the dependency parser is used only toproduce a dependency tree backbone, which theHPSG parser then uses to produce the more gen-eral dependency graph.
A similar strategy isused in the RASP parser (Briscoe et al, 2006),which builds a dependency graph through unifi-cation operations performed during a phrasestructure tree parsing process.5 ConclusionWe have presented a framework for dependencyDAG parsing, using a novel algorithm for projec-tive DAGs that extends existing shift-reduce al-gorithms for parsing with dependency trees, andpseudo-projective transformations applied toDAG structures.We have demonstrated that the parsing ap-proach is effective in analysis of predicate-argument structure in English using data from theHPSG Treebank (Miyao et al, 2004), and inparsing of Danish using a rich dependency repre-sentation (Kromann, 2003).759AcknowledgementsWe thank Yusuke Miyao and Takuya Matsuzakifor insightful discussions.
This work was partial-ly supported by Grant-in-Aid for Specially Pro-moted Research (MEXT, Japan).ReferencesBriscoe, T., Carroll, J. and Watson, R. 2006.
Thesecond release of the RASP system.
In Proceed-ings of the COLING/ACL-06 Demo Session.Buchholz, Sabine and Erwin Marsi.
2006.
CoNLL-XShared Task on Multilingual Dependency Parsing.In Proceedings of the 10th Conference on Compu-tational Natural Language Learning (CoNLL-X)Shared Task session.Clark, Stephen, Julia Hockenmaier, and Mark Steed-man.
2002.
Building Deep Dependency Structuresusing a Wide-Coverage CCG Parser.
In Proceed-ings of the 40th Annual Meeting of the Associationfor Computational Linguistics (ACL).Covington, Michael A.
2001.
A fundamental algo-rithm for dependency parsing.
In Proceedings ofthe Annual ACM Southeast Conference, 95-102.Erkan, Gunes, Arzucan Ozgur, and Dragomir R. Ra-dev.
2007.
Semisupervised classification for ex-tracting protein interaction sentences using depen-dency parsing.
In Proceedings of  CoNLL-EMNLP.Hudson, Richard.
1984.
Word Grammar.
Oxford:Blackwell.Hudson, Richard.
2005.
Word Grammar.
In K.
Brown(Ed.
), Encyclopedia of Language and Linguistics(second ed., pp 633-642).
Elsevier.Hockenmaier, Julia and Mark Steedman.
2007.CCGbank: a corpus of CCG derivations and de-pendency structures extracted from the Penn Tree-bank.
In Computational Linguistics 33(3), pp 355-396, MIT press.Kromann, Matthias T. 2003.
The Danish dependencytreebank and the underlying linguistic theory.
InProceedings of the Second Workshop on Treebanksand Linguistic Theories (TLT).McDonald, Ryan and Fernando Pereira.
2006.
Onlinelearning of approximate dependency parsing algo-rithms.
In Proceedings of the 11th Conference ofthe European Chapter of the Association for Com-putational Linguistics (EACL).McDonald, Ryan, Fernando Pereira, Kiril Ribarov andJan Hajic.
2005.
Non-projective Dependency Pars-ing using Spanning Tree Algorithms.
In Proceed-ings of the Human Language Technology Confe-rence and Conference on Empirical Methods inNatural Language Processing (HLT-EMNLP).Miyao, Yusuke, Takashi Ninomiya, and Jun?ichi Tsu-jii.
2004.
Corpus-oriented grammar developmentfor acquiring a Head-driven Phrase StructureGrammar from the Penn Treebank.
In Proceedingsof the International Joint Conference on NaturalLanguage Processing (IJCNLP).Miyao Yusuke and Jun'ichi Tsujii.
2005.
Probabilisticdisambiguation models for wide-coverage HPSGparsing.
In Proceedings of the 43rd Annual Meet-ing on Association for Computational Linguistics.Nivre, Joakim.
2004.
Incrementality in DeterministicDependency Parsing.
In Incremental Parsing:Bringing Engineering and Cognition Together(Workshop at ACL-2004).Nivre, Joakim, Johan Hall, Sandra K?bler, RyanMcDonald, Jens Nilsson, Sebastian Riedel, DenizYuret.
2007.
In Proceedings of the CoNLL 2007Shared Task in the Joint Conference on EmpiricalMethods in Natural Language Processing andComputational Natural Language Learning.Nivre, Joakim.
and Jens Nilsson.
2005.
Pseudo-Projective Dependency Parsing.
In Proceedings ofthe 43rd Annual Meeting of the Association forComputational Linguistics (ACL), pp.
99-106.Nivre, Joakim.
2007.
Incremental non-projective de-pendency parsing.
In Proceedings of Human Lan-guage Technologies: The Annual Conference of theNorth American Chapter of the Association forComputational Linguistics (NAACL-HLT?07).Saetre, R., Sagae, K., and Tsujii, J.
2007.
Syntacticfeatures for protein-protein interaction extraction.In Proceedings of the International Symposium onLanguages in Biology and Medicine (LBM shortoral presentations).Sagae, Kenji., Yusuke Miyao Jun?ichi and Tsujii.2007.
HPSG Parsing with shallow dependencyconstraints.
In Proceedings of the 44th Meeting ofthe Association for Computational Linguistics.Sagae, K., Tsujii, J.
2007.
Dependency parsing anddomain adaptation with LR models and parser en-sembles.
In Proceedings of the CoNLL 2007Shared Task.
in EMNLP-CoNLL.Tsuruoka, Yoshimasa and Tsujii, Jun?ichi.
2005.
Bidi-rectional inference with the easiest-first strategy fortagging sequence data.
In Proceedings of the Hu-man Language Technology Conference and Confe-rence on Empirical Methods in Natural LanguageProcessing (HLT-EMNLP), pp.
523-530.Wang, Mengqiu, Noah A. Smith, and Teruko Mita-mura.
2007.
What is the Jeopardy Model?
A Quasi-Synchronous Grammar for QA.
In Proceedings ofthe Joint Conference on Empirical Methods inNatural Language Processing and ComputationalNatural Language Learning (EMNLP-CoNLL).760
