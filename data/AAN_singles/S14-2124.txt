Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 694?698,Dublin, Ireland, August 23-24, 2014.?UFAL: Using Hand-crafted Rules in Aspect Based Sentiment Analysison Parsed DataKate?rina Veselovsk?a, Ale?s TamchynaCharles University in Prague, Faculty of Mathematics and PhysicsInstitute of Formal and Applied LinguisticsMalostransk?e n?am?est??
25, Prague, Czech Republic{veselovska,tamchyna}@ufal.mff.cuni.czAbstractThis paper describes our submission to Se-mEval 2014 Task 41(aspect based senti-ment analysis).
The current work is basedon the assumption that it could be advan-tageous to connect the subtasks into oneworkflow, not necessarily following theirgiven order.
We took part in all four sub-tasks (aspect term extraction, aspect termpolarity, aspect category detection, aspectcategory polarity), using polarity items de-tection via various subjectivity lexiconsand employing a rule-based system ap-plied on dependency data.
To determineaspect categories, we simply look up theirWordNet hypernyms.
For such a basicmethod using no machine learning tech-niques, we consider the results rather sat-isfactory.1 IntroductionIn a real-life scenario, we usually do not have anygolden aspects at our disposal.
Therefore, it couldbe practical to be able to extract both aspects andtheir polarities at once.
So we first parse the data,bearing in mind that it is very difficult to detectboth sources/targets and their aspects on plain textcorpora.
This holds especially for pro-drop lan-guages, e.g.
Czech (Veselovsk?a et al., 2014) butthe proposed method is still language independentto some extent.
Secondly, we detect the polar-ity items in the parsed text using a union of twodifferent existing subjectivity lexicons (see Sec-tion 2).
Afterwards, we extract the aspect terms inthe dependency structures containing polarity ex-1http://alt.qcri.org/semeval2014/task4/This work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/pressions.
In this task, we employ several hand-crafted rules detecting aspects based on syntacticfeatures of the evaluative sentences, inspired bythe method by Qiu et al.
(2011).
Finally, we iden-tify aspect term categories with the help of the En-glish WordNet and derive their polarities based onthe polarities of individual aspects.
The obtainedresults are discussed in Section 4.2 Related WorkThis work is related to polarity detection based ona list of evaluative items, i.e.
subjectivity lexi-cons, generally described e.g.
in Taboada et al.(2011).
The English ones we use are minutely de-scribed in Wiebe et al.
(2005) and several papersby Bing Liu, starting with Hu and Liu (2004).
In-spired by Kobayashi et al.
(2007), who make useof evaluative expressions when learning syntac-tic patterns obtained via pattern mining to extractaspect-evaluation pairs, we use the opinion wordsto detect evaluative structures in parsed data.
Theissue of target extraction in sentiment analysis isdiscussed in articles proposing different methods,mainly tested on product review datasets (Popescuand Etzioni, 2005; Mei et al., 2007; Scaffidi et al.,2007).
Some of the authors take into considerationalso product aspects (features), defined as prod-uct components or product attributes (Liu, 2006).Hu and Liu (2004) take as the feature candidatesall noun phrases found in the text.
Stoyanov andCardie (2008) see the problem of target extractionas part of a topic modelling problem, similarly toMei et al.
(2007).
In this contribution, we followthe work of Qiu et al.
(2011) who learn syntacticrelations from dependency trees.3 PipelineOur workflow is illustrated in Figure 1.
We firstpre-process the data, then mark all aspects seen inthe training data (still on plain text).
The rest ofthe pipeline is implemented in Treex (Popel and694Pattern Example sentenceSubjaspectPredcopulaPAdj The food was great.SubjaspectPredcopulaPNoun The coconut juice is the MUST!SubjaspectPred AdvevalThe pizza tastes so good.AttrevalNounaspectNice value.SubjaspectPredevalTheir wine sucks.SubjsourcePredevalObjaspectI liked the beer selection.Table 1: Syntactic rules.Pre-process & spellcheckMark known aspectsMark aspect categoriesPlain textTreexMark evaluative wordsRun tagger & parserApply syntactic rulesFigure 1: Overall schema of our approach.
?Zabokrtsk?y, 2010) and consists of linguistic anal-ysis (tagging, dependency parsing), identificationof evaluative words, and application of syntacticrules to find the evaluated aspects.
Finally, forrestaurants, we also identify aspect categories andtheir polarity.3.1 DataWe used the training and trial data provided by theorganizers.
During system development, we usedthe trial section as a held-out set.
In the final sub-mission, both datasets are utilized in training.3.2 Pre-processingThe main phase of pre-processing (apart fromparsing the input files and other simple tasks) isrunning a spell-checker.
As data for this taskcomes from real-world reviews, it contains varioustypos and other small errors.
We therefore imple-mented a statistical spell-checker which works intwo stages:1.
Run Aspell2to detect typos and obtain sug-gestions for them.2.
Select the appropriate suggestions using alanguage model (LM).We trained a trigram LM from the English sideof CzEng 1.0 (Bojar et al., 2012) using SRILM(Stolcke, 2002).
We binarized the LM and usethe Lazy decoder (Heafield et al., 2013) for select-ing the suggestions that best fit the current context.Our script is freely available for download.3We created a list of exceptions (domain-specificwords, such as ?netbook?, are unknown to As-pell?s dictionary) which should not be correctedand also skip named entities in spell-checking.3.3 Marking Known AspectsBefore any linguistic processing, we mark allwords (and multiword expressions) which aremarked as aspects in the training data.
For our fi-nal submission, the list also includes aspects fromthe provided development sets.3.4 Morphological Analysis and ParsingFurther, we lemmatize the data and parse it usingTreex (Popel and?Zabokrtsk?y, 2010), a modularframework for natural language processing (NLP).Treex is focused primarily on dependency syntaxand includes blocks (wrappers) for taggers, parsersand other NLP tools.
Within Treex, we used theMor?ce tagger (Haji?c et al., 2007) and the MST de-pendency parser (McDonald et al., 2005).3.5 Finding Evaluative WordsIn the obtained dependency data, we detect polar-ity items using MPQA subjectivity lexicon (Wiebeet al., 2005) and Bing Liu?s subjectivity clues.42http://aspell.net/3https://redmine.ms.mff.cuni.cz/projects/staspell4http://www.cs.uic.edu/?liub/FBS/sentiment-analysis.html#lexicon695Task 1: aspect extraction Task 2: aspect polarity Task 3: category detection Task 4: category polarityprec recall F-measure accuracy prec recall F-measure accuracyUFAL 0.50 0.72 0.59 0.67 0.57 0.74 0.65 0.63best 0.91 0.82 0.84 0.81 0.91 0.86 0.88 0.83Table 2: Results of our system on the Restaurants dataset as evaluated by the task organizers.Task 1: aspect extraction Task 2: aspect polarityprec recall F-measure accuracyUFAL 0.39 0.66 0.49 0.57best 0.85 0.67 0.75 0.70Table 3: Results of our system on the Laptops dataset as evaluated by the task organizers.We lemmatize both lexicons and look first formatching surface forms, then for matching lem-mas.
(English lemmas as output by Mor?ce aresometimes too coarse, eliminating e.g.
negation?
we can mostly avoid their matching by lookingat surface forms first.
)3.6 Syntactic RulesFurther, we created six basic rules for findingaspects in sentences containing evaluative itemsfrom the lexicons, e.g.
?If you find an adjectivewhich is a part of a verbonominal predicate, thesubject of its governing verb should be an aspect.
?,see Table 1.
Situational functions are marked withsubscript, PAdj and PNoun stand for adjectival andnominal predicative expressions.Moreover, we applied three more rules con-cerning coordinations.
We suppose that if we findan aspect, every member of a given coordinationmust be an aspect too.The excellent mussels, puff pastry, goat cheeseand salad.Concerning but-clauses, we expect that ifthere is no other aspect in the second part ofthe sentence, we assign the conflict value to theidentified aspect.The food was pretty good, but a little flavorless.If there are two aspects identified in thebut-coordination, they should be marked withopposite polarity.The place is cramped, but the food is fantastic!3.7 Aspect CategoriesWe collect a list of aspects from the training dataand find all their hypernyms in WordNet (Fell-baum, 1998).
We hand-craft a list of typical hy-pernyms for each category (such as ?cooking?
or?consumption?
for the category ?food?).
More-over, we look at the most frequent aspects in thetraining data and add as exceptions those for whichour list would fail.We rely on the output of aspect identificationfor this subtask.
For each aspect marked in thesentence, we look up all its hypernyms in Word-Net and compare them to our list.
When we finda known hypernym, we assign its category to theaspect.
Otherwise, we put the aspect in the ?anec-dotes/miscellaneous?
category.
For category po-larity assignment, we combine the polarities of allaspects in that category in the following way:?
all positive?
positive?
all negative?
negative?
all neutral?
neutral?
otherwise?
conflict4 Results and DiscussionTable 2 and Table 3 summarize the results of oursubmission.
We do not achieve the best perfor-mance in any particular task, our system overallranked in the middle.We tend to do better in terms of recall than pre-cision.
This effect is mainly caused by our deci-sion to also automatically mark all aspects seen inthe training data.4.1 Effect of the Spell-checkerWe evaluated the performance of our system withand without the spell-checker.
Overall, the impact696is very small (f-measure stays within 2-decimalrounding error).
In some cases its corrections areuseful (?convienent?
?
?convenient parking?
),sometimes its limited vocabulary harms our sys-tem (?fettucino alfredo??
?fitting Alfred?).
Thisissue could be mitigated by providing a customlexicon to Aspell.4.2 Sources of ErrorsAs we always extract aspects that were observed inthe training data, our system often marks them innon-evaluative contexts, leading to a considerablenumber of false positives.
However, using this ap-proach improves our f-measure score due to thelimited recall of the syntactic rules.The usefulness of our rules is mainly limited bythe (i) sentiment lexicons and (ii) parsing errors.
(i) Since we used the lexicons directly withoutdomain adaptation, many domain-specific termsare missed (?flavorless?, ?crowded?)
and some arematched incorrectly.
(ii) Parsing errors often confuse the rules andnegatively impact both recall and precision.
Of-ten, they prevented the system from taking nega-tion into account, so some of the negated polarityitems were assigned incorrectly.The ?conflict?
polarity value was rarely correct?
all aspects and their polarity values need to becorrectly discovered to assign this value.
How-ever, this type of polarity is infrequent in the data,so the overall impact is small.Having participated in all four tasks, our sys-tem can be readily deployed as a complete solutionwhich covers the whole process from plain text toaspects and aspect categories annotated with po-larity.
Considering the number of tasks coveredand the fact that our system is entirely rule-based,the achieved results seem satisfactory.5 Conclusion and Future WorkIn our work, we developed a purely rule-based sys-tem for aspect based sentiment analysis which canboth detect aspect terms (and categories) and as-sign polarity values to them.
We have shown thateven such a simple approach can achieve relativelygood results.In the future, our main plan is to involve ma-chine learning in our system.
We expect that out-puts of our rules can serve as useful indicator fea-tures for a discriminative learning model, alongwith standard features such as bag-of-words (lem-mas) or n-grams.6 AcknowledgementsThe research described herein has been supportedby the by SVV project number 260 140 and by theLINDAT/CLARIN project funded by the Ministryof Education, Youth and Sports of the Czech Re-public, project No.
LM2010013.This work has been using language resourcesdeveloped and/or stored and/or distributed by theLINDAT/CLARIN project of the Ministry of Ed-ucation, Youth and Sports of the Czech Republic(project LM2010013).ReferencesOnd?rej Bojar, Zden?ek?Zabokrtsk?y, Ond?rej Du?sek, Pe-tra Galu?s?c?akov?a, Martin Majli?s, David Mare?cek, Ji?r??Mar?s?
?k, Michal Nov?ak, Martin Popel, and Ale?s Tam-chyna.
2012.
The Joy of Parallelism with CzEng1.0.
In Proc.
of LREC, pages 3921?3928.
ELRA.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
Bradford Books.Jan Haji?c, Jan Votrubec, Pavel Krbec, Pavel Kv?eto?n,et al.
2007.
The best of two worlds: Cooperation ofstatistical and rule-based taggers for czech.
In Pro-ceedings of the Workshop on Balto-Slavonic NaturalLanguage Processing: Information Extraction andEnabling Technologies, pages 67?74.Kenneth Heafield, Philipp Koehn, and Alon Lavie.2013.
Grouping language model boundary words tospeed k-best extraction from hypergraphs.
In Pro-ceedings of the 2013 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pages958?968, Atlanta, Georgia, USA, June.Minqing Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In Proceedings of the TenthACM SIGKDD International Conference on Knowl-edge Discovery and Data Mining, KDD ?04, pages168?177, New York, NY, USA.
ACM.Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.2007.
Extracting aspect-evaluation and aspect-of re-lations in opinion mining.
In Proceedings of the2007 Joint Conference on Empirical Methods inNatural Language Processing and ComputationalNatural Language Learning (EMNLP-CoNLL).Bing Liu.
2006.
Web Data Mining: Exploring Hyper-links, Contents, and Usage Data (Data-Centric Sys-tems and Applications).
Springer-Verlag New York,Inc., Secaucus, NJ, USA.697Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Haji?c.
2005.
Non-projective dependency pars-ing using spanning tree algorithms.
In Proceedingsof the conference on Human Language Technologyand Empirical Methods in Natural Language Pro-cessing, pages 523?530.Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su,and ChengXiang Zhai.
2007.
Topic sentiment mix-ture: Modeling facets and opinions in weblogs.
InProceedings of the 16th International Conference onWorld Wide Web, WWW ?07, pages 171?180, NewYork, NY, USA.
ACM.Martin Popel and Zden?ek?Zabokrtsk?y.
2010.
Tec-toMT: Modular NLP Framework.
In Hrafn Lofts-son, Eirikur R?ognvaldsson, and Sigrun Helgadottir,editors, IceTAL 2010, volume 6233 of Lecture Notesin Computer Science, pages 293?304.
Iceland Cen-tre for Language Technology (ICLT), Springer.Ana-Maria Popescu and Oren Etzioni.
2005.
Ex-tracting product features and opinions from reviews.In Proceedings of the Conference on Human Lan-guage Technology and Empirical Methods in Natu-ral Language Processing, HLT ?05, pages 339?346,Stroudsburg, PA, USA.Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.2011.
Opinion word expansion and target extrac-tion through double propagation.
Comput.
Linguist.,37(1):9?27, March.Christopher Scaffidi, Kevin Bierhoff, Eric Chang,Mikhael Felker, Herman Ng, and Chun Jin.
2007.Red opal: Product-feature scoring from reviews.
InProceedings of the 8th ACM Conference on Elec-tronic Commerce, EC ?07, pages 182?191, NewYork, NY, USA.
ACM.Andreas Stolcke.
2002.
SRILM ?
An Extensible Lan-guage Modeling Toolkit.
In Proc.
Intl.
Conf.
onSpoken Language Processing, volume 2, pages 901?904.Veselin Stoyanov and Claire Cardie.
2008.
Topicidentification for fine-grained opinion analysis.
InProceedings of the 22Nd International Conferenceon Computational Linguistics - Volume 1, COLING?08, pages 817?824, Stroudsburg, PA, USA.Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-berly Voll, and Manfred Stede.
2011.
Lexicon-based methods for sentiment analysis.
Comput.
Lin-guist., 37(2):267?307, June.Kate?rina Veselovsk?a, Jan Ma?sek, and Vladislav Kubo?n.2014.
Sentiment detection and annotation in a tree-bank.Janyce Wiebe, Theresa Wilson, and Claire Cardie.2005.
Annotating expressions of opinions and emo-tions in language.
Language resources and evalua-tion, 39(2-3):165?210.698
