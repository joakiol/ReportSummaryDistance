Is N-Best Dead?Long Nguyen, Richard Schwartz, Ying Zhao, George Zavaliagkos tBBN Systems and TechnologiesCambridge, MA 02138tNortheastern U iversityABSTRACTWe developed a faster search algorithm that avoids theuse of the N-Best paradigm until after more powerfulknowledge sources have been used.
We found, however,that there was little or no decrease in word errors.
Wethen showed that the use of the N-Best paradigm is stillessential for the use of still more powerful knowledgesources, and for several other purposes that are outlinedin the paper.1.
INTRODUCTIONThe N-Best Paradigm \[1\] was introduced originally as ameans for integrating the speech recognition and languageunderstanding components of a spoken language system.Since then, we have generalized its use for integrating intothe recognition search other expensive knowledge sources(such as higher-order n-gram language models, between-word co-articulation models, and segmental models) with-out increasing the search space \[2\].
The basic idea is thatwe use inexpensive knowledge sources to find N alternativesentence hypotheses.
Then we rescore each of these hy-potheses with the more expensive and more accurate knowl-edge sources in order to determine the most likely utterance.The N-Best Paradigm specifically, and multi-pass earch al-gofithms in general, are now used widely by the speechrecognition research community.Besides its use as an efficient search strategy, the N-BestParadigm has been used extensively in several other ways\[2\].
Its simplicity has made it ideal as a means for coop-eration between research sites.
For example, we regularlysend the N-Best lists of alternatives to research sites thatdo not have an advanced speech recognition capability (e.g.,Paramax and NYU) in order that they can apply their ownlinguistic components for understanding or for research intoalternative language modeling techniques.Another related use of the N-Best lists is for evaluation ofalternative knowledge sources.
New knowledge sources canbe evaluated without having to integrate them into the searchstrategy.
For example, we can determine whether a newprosodic module or linguistic knowledge source reduces theerror rate when used to reorder the N-Best list.
This isparticularly important for knowledge sources that are noteasily formulated in a left-to-ddght incremental manner.Finally, we have presented techniques for optimizing theweights for different knowledge sources, and for discrimi-native training \[2\].In this paper we attempt to determine whether the N-BestParadigm results in substantial search errors.
If it does, thenits use for the other purposes mentioned above would alsobe questionable.
First we describe briefly how we used theN-Best paradigm in previous versions of BYBLOS.
Then,we descfibe our attempts to avoid the errors that might be aresult of using the N-Best paradigm.
Finally, we argue thatthere will always be cases where the N-Best paradigm willmake it possible to use some knowledge sources that wouldlikely never be used otherwise.2.
3 -PASS N-BEST SEARCH STRATEGYThe BYBLOS system has been described previously (e.g.,\[3\]).
We reiterate here the use of the N-Best Paradigm inthat system.The decoder used a 3-pass earch strategy.
The strategy useda forward pass followed by a backward Word-Dependent N-Best search algorithm \[4\] using a bigram language model,within-word triphone models, and top-1 (discrete VQ) densi-ties.
The N-Best hypotheses were then rescored using cross-word triphone context models, top-5 mixture densities, andtrigram language model.Typically, the backward Word-Dependent N-Best pass re-quires about half the time required by the forward pass.Rescoring each alternative sentence hypothesis individuallywith cross-word triphone models only requires about 0.2 sec-onds per hypothesis.
And rescofing the text of the hypothe-ses with a high-order n-gram language model \[5\] requiresessentially no time.3.
ADMISS IB IL ITYIt has often been asserted that the N-Best paradigm is inad-missible because when the initial N-Best list is created usingweaker knowledge sources, then the answer that would havehad the highest score using the stronger knowledge sourcesmight not be within the list of alternatives, and thereforenever have a chance to be rescored.
This would be espe-cially likely when the error rate is high and the utterances411are long, since the number of alternative sentences neededto include the correct answer would grow exponentially withthe length of the utterance.The knowledge sources (e.g., cross-word triphones and tri-gram language models) used for rescoring in the 3-Pass N-Best strategy described above were much more powerfulthan the original knowledge sources (e.g, within-word tri-phones and bigram language models) in that they frequentlyreduced the error rate by half.
However, we had assured our-selves that, at least for moderate-size problems (like ATISwith 2,000 words or WSJ with 5,000 words), there werefew if' any additional errors caused by the correct answernot being included in the N-Best list.However, after the November 1992 DARPA ContinuousSpeech Recognition (CSR) evaluations, we were concernedthat we might be losing some performance as a result ofour use of the 3-Pass N-Best strategy (rescoring with cross-word triphones, top-5 mixture densities, and trigrarn lan-guage models) on the 20,000 words WSJ test.
This wasbecause there were many sentences for which the correctanswer was not in the N-Best hypotheses although it bada higher total score (when including the trigram languagemodel and cross-word triphones) than any sentence hypoth-esis in the N-Best list.
We felt that this was due to the higherword error rate that resulted from recognition with a largevocabulary of 20,000 words, and the long utterances foundin the Wall Street Journal (WSJ) corpus.Therefore, this year we implemented a more complicatedsearch strategy similar to the Progressive-Search strategysuggested by Murveit \[6\] in which we use the initial passesto create a lattice of alternative hypotheses, which can thenbe rescored.
The advantage of this approach is that a latticewith a small number of alternatives at each point can repre-sent a very large number of alternative sentence hypotheses.In addition, rescoring the lattice of alternatives i computa-tionally less expensive than rescoring a large explicit list ofsentence alternatives.
This also avoids the rather large in-termediate storage required to store the N-Best hypotheses.3.1.
4-Pass Lattice Search AlgorithmIn this section we describe a4-Pass Lattice Search algorithmthat avoids the early use of the N-Best.1.
The time-synchronous beam search algorithm with a vo-cabulary of 20,000 words and a bigram language model typ-ically requires substantial computation on today's worksta-tions.
Therefore, we make extensive use of the NormalizedForward Backward Search aigonthm \[8\] to reduce compu-tation.
We perform a first pass using a fast match techniquewhose sole purpose is to find and save high scoring wordends.
Because this model is approximate, it can run consid-erably faster than the usual beam search.
And because thelater passes will be more accurate, the first pass need not beas accurate.2.
A second pass, time-synchronous beam search, using abigram language model, within-word triphones, and (top-1VQ) discrete models runs backward.
This pass is sped up byseveral orders of magnitude by using the Normalized For-ward Backward pruning on the word-ending scores producedby the first pass.
We save the beginning times and scores(fl~,) of all words found.
This pass requires much less timethan the first pass.3.
A third pass identical to the second pass runs forward, us-ing the Normalized Forward Backward pruning on the word-beginning scores produced by the second pass.
Similar tot the second pass, we save the ending times and scores (t~,~)of all words found (constrained by the second pass).4.
We use the beginning (fit .)
and ending (a~ .)
scores frompasses 2 and 3 to determine possible word-j~ncture times.Specifically, if the forward-backward score for a particularpair of words is within a threshold of the total score for theutterance, then this word-pair is used.
That is if' Pr(wilw./) ft., > A ?~tu~jwhere Pr(wilw~) is the probability of wj followed by wi,and A is the threshold (which can be a function of either c~or  j~).Adjacent word-junctures are merged.
Having found a word-pair, we look for the next word-juncture where this secondword is the first word of the next pair.
The result is a latticeof word hypotheses.
If the range of beginning and endingtimes for a single word overlap, then we create a loop forthat word.The word lattice (which is really just a small finite-statelanguage model) is then expanded to allow for maintainingseparate scores for trigrarn language models and cross-wordtriphones.
This entails copying each word in the context ofeach preceding word, and replacing the triphones on eitherside of the word junctures with the appropriate cross-wordtriphones.
Thus, each word in the lattice represents a partic-ular instance of that word in the context of some particularother word.
The transition probabilities in the lattice are theprobability of the next word given the previous two words- trigram probabilities.We perform a fourth pass in the backward direction us-ing this expanded language model.
The result is the mostlikely hypothesis including cross-word and trigram knowl-edge sources.However, we are not done at this point, because we maywant to apply more powerful, but more expensive, knowl-edge sources.
We generate the N-Best alternative hypothe-ses out of the search on this lattice.
The Word-DependentN-Best algorithm \[4\] requires that we keep separate scores412at each state depending on the previous word, because theboundary between two words clearly depends on those twowords.
But the words in the lattice are only defined inthe context of the neighboring word.
Thus, by keeping thescores of all of the ending word hypotheses, we can recoverthe N-Best alternatives.
However, in contrast to its previoususe, these N-Best answers have been computed includingthe more powerful knowledge sources of cross-word coar-ticulation models and trigram language models.3.2.
Experimental Resu l tsWe performed an experiment in which we compared therecognition accuracy of this 4-Pass Lattice approach withthe previous 3-Pass N-Best approach.
In both cases, theinitial search (in order to create the lattice or to find theN-Best sentence hypotheses) used only a bigram languagemodel and within-word coarticulation models with topl-VQdiscrete densities, while the final search (on the lattice) orrescoring (the N-Best) used a trigram language model andbetween-word coarticulation models with top-5 mixture den-sities.Initially, we were surprised to find that the accuracy usingthe lattice was actually slightly worse than that of the orig-inal N-Best method.
Then, we realized that this was due tothe larger number of alternatives.
A lattice with an averagedepth (the average number of branches out of a word-endnode) of 10 for a sentence of 20 words can be thought of asan N-Best list with 1020 hypotheses.
When we had previ-ously found that, in the 3-Pass N-Best approach, the correctutterance might have a higher score than the answers in thetop 100 best hypotheses, there were also countless other in-correct hypotheses, in the 4-Pass Lattice approach, that alsohad higher scores than the answers in the original N-Best.The search on the lattice often found one of these otherincorrect answers.We alleviated this problem by optimizing (automatically) theweights (for trigram language model, word insertion penaltyand phone insertion penalty) using the N-Best alternativehypotheses found after the lattice search.
These new weightswere then used to search the lattice again.
Finally, we wereable to obtain 5% fewer word errors using the 4-Pass Latticestrategy than when using the 3-Pass N-Best approach.
Thiswas a much smaller eduction in error than we had hopedfor.
Apparently the reduced search errors were largely offsetby the larger search space on the lattice.It would appear, therefore, that the doom and gloom predic-tions for N-Best are unfounded so far, at least for the 20,000WSJ task.
In fact, the N-Best paradigm continues to offeradvantages not available otherwise, as mentioned below.4.
CURRENT USES FOR N-BESTWhile it is possible to expand a lattice of alternatives forresconng with trigram language models, there are still manyknowledge sources that are too expensive to use this way.For example, for the November of 1993 evaluations, we in-cluded a model of whole segments (Segmental Neural Net-work \[10\]).
And Boston University also rescored our N-Besthypotheses with a similarly motivated Stochastic SegmentModel \[9\].
Both of these models are much more expensivethan HMM models due to their constrained slope featuresand global dependence.
Either of these models reduce theword error rate by about 10% in combination with the HMMscores.
We also experimented with a more complex HMMtopology for a phoneme that includes thirteen states insteadof the usual three or five states.
While this model couldhave been integrated irectly, it was much easier and fasterto simply restore the N-Best hypotheses with this largermodel.
The resulting small reduction in error rate wouldnot have been worth the larger computation and storage as-sociated with using it in the original search, if not to mentionthe time of implementation to integrate these models into thesearch.Also for the 1993 evaluations on the ATIS domain, we foundthat we could reduce the word error rate by 8% by rescor-ing the N-Best hypotheses with a four-gram class languagemodel.
Again, expanding the word lattice for a four-gramlanguage model would have been possible, but would haveresulted in a huge lattice with the same word replicated manytimes.
But resconng the N-Best hypotheses with four-gramsrequired almost no computation and did not require rerun-ning the recognition.There is a tremendous advantage in being able to defineany scoring function without having to get involved withthe details of a general search strategy since only one linearsequence of words need be scored at one time.In combining these various experinaental knowledge sources,it is important that they be weighted appropriately, or elsethere may be no gain, or even a loss.
Optimizing the weightsfor several knowledge sources on a development test set ofseveral hundred sentences can be accomplished in secondsor minutes on the N-Best hypotheses rather than days byexplicit experimentation.And of course, we still use the N-Best paradigm to com-bine the speech recognition with the language understand-ing component.
It would be infeasible to use the entireconstrained space defined by the understanding model in thespeech recognition search.
But it is a trivial matter to provideseveral (5 to 10) alternatives to the understanding compo-nent for its choice.
Again, in this year as in the past, wealso provided the N-Best alternatives output from our speechrecognition system to the language understanding group atParamax.
This simple text-based interface makes arbitraryintegration simple.
The integration between two sites acrossthe ARPA network was quite straightforward.4135.
CONCLUSIONSWe developed a search strategy similar in spirit to the Pro-gressive Search technique \[6\] that allows us to incorporatecross-word triphones and trigrarn language models directlywithin the search.
The resulting search, although usingmany passes is considerably faster than our previous trat-egy.
However, we found only marginal improvements in theaccuracy, indicating that there were not really many searcherrors incurred using the original 3-Pass N-Best strategy.Despite; the ability to integrate some knowledge sources di-rectly into the search, we still use the N-Best Paradigm inall the ways that we used it previously, including integratingmore expensive knowledge sources, cooperation with otherresearclh sites, rapid testing of new knowledge sources, andautomatic optimization of recognition parameters.6.
ACKKNOWLEDGEMENTThis work was supported by the Advanced Research ProjectsAgency and monitored by the Office of Naval Research un-der contract No.
N00014-92-C-0035.References1.
Schwartz, R., and Y. Chow, "The N-Best Algorithm:An Efficient Procedure for Finding the Top N Sen-tence Hypotheses", Proc.
of DARPA Speech and Nat-ural Language Workshop, Cape Cod, MA, Oct. 1989,pp.
199-2022.
Also Proc.
of lEEE ICASSP-90, Albu-querque, NM, Apr.
1990, $2.12, pp.
81-84.2.
Schwartz, R., S. Austin, F. Kubala, and J. Makhoul,"New Uses for the N-Best Sentence Hypotheses Withinthe BYBLOS Speech Recognition System", Proc.
ofIEEE ICASSP-92, San Francisco, CA, March 1992, pp.1.1-I.4.3.
Bates, M., R. Bobrow, P. Fung, R. Ingria, F. Kubala,J.
Makhoul, L. Nguyen, R. Schwartz, D. StaUard,"The BBN/I-IARC Spoken Language UnderstandingSystem", Proc.
of lEEE ICASSP-93, Minneapolis, MN,April 1993, pp.
111-114, Vol.
H.4.
Schwartz, R. and S. Austin, "A Comparison Of Sev-eral Approximate Algorithms for Finding Multiple (N-Best) Sentence Hypotheses", Proc.
of IEEE ICASSP-91, Toronto, Canada, pp.
701-704.5.
Placeway, P., R. Schwartz, P. Fung, and L. Nguyen,"The Estimation of Powerful Language Models fromSmall and Large Corpora", Proc.
of IEEE ICASSP-93,Minneapolis, MN, Apr.
1993, Vol.
II, pp.
33-36.6.
Murveit, H., J. Butzberger, V. Digalakis, M. Wein-traub, "Progressive-Search Algorithms for Large Vo-cabulary Speech Recognition", Proc.
of ARPA HumanLanguage Technology Workshop, Princeton, NJ, Mar.1993, pp.
87-90.7.
Austin, S., Schwartz, R., and P. Placeway, "TheForward-Backward Search Algorithm", Proc.
of IEEEICASSP-91, Toronto, Canada, pp.
697-700.8.
Nguyen, L., R. Schwartz, F. Kubala, and P. Place-way, "Search Algorithms for Software-Only Real-TimeRecognition with Very Large Vocabularies", Proc.
ofARPA Human Language Technology Workshop, Prince-ton, NJ, Mar.
1993, pp.
91-95.9.
Ostendorf, M., A. Kannan, S. Austin, O. Kimball,R.
Schwartz, and J. R. Rohlicek, "Integration of Di-verse Recognition Methodologies Through Reevalu-ation of N-Best Sentence Hypotheses", Proc.
of theDARPA Speech and Natural Language Workshop, Mor-gan Kaufmann Publishers, Feb. 1991, pp.
83-87.10.
Zavaliagkos, G., S. Austin, J. Makhoul, R. Schwartz,"A Hybrid Continuous Speech Recognition SystemUsing Segmental Neural Nets With Hidden MarkovModels", International Journal of Pattern Recognitionand Artificial Intelligence, World Scientific PublishingCompany 1993, Vol.
7, No.
4, pp.
949-963.414
