Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conferenceand the Shared Task, pages 80?84, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational LinguisticsDistinguishing Common and Proper NounsJudita Preiss and Mark Stevenson{j.preiss, r.m.stevenson}@sheffield.ac.ukDepartment of Computer Science,University of Sheffield211 Portobello, Sheffield S1 4DPUnited KingdomAbstractWe describe a number of techniques for auto-matically deriving lists of common and propernouns, and show that the distinction betweenthe two can be made automatically using avector space model learning algorithm.
Wepresent a direct evaluation on the British Na-tional Corpus, and application based evalua-tions on Twitter messages and on automaticspeech recognition (where the system could beemployed to restore case).1 IntroductionSome nouns are homographs (they have the samewritten form, but different meaning) which can beused to denote either a common or proper noun, forexample the word apple in the following examples:(1) Apple designs and creates iPod (2) The Apple IIseries is a set of 8-bit home computers (3) The appleis the pomaceous fruit of the apple tree (4) For appleenthusiasts ?
tasting notes and apple identification.The common and proper uses are not always asclearly distinct as in this example; for example, aspecific instance of a common noun, e.g., DistrictCourt turns court into a proper noun.While heuristically, proper nouns often start witha capital letter in English, capitalization can be in-consistent, incorrect or omitted, and the presence orabsence of an article cannot be relied on.The problem of distinguishing between commonand proper usages of nouns has not received muchattention within language processing, despite beingan important component for many tasks includingmachine translation (Lopez, 2008; Hermjakob et al2008), sentiment analysis (Pang and Lee, 2008; Wil-son et al 2009) and topic tracking (Petrovic?
et al2010).
Approaches to the problem also have appli-cations to tasks such as web search (Chen et al1998; Baeza-Yates and Ribeiro-Neto, 2011), andcase restoration (e.g., in automatic speech recogni-tion output) (Baldwin et al 2009), but frequentlyinvolve the manual creation of a gazeteer (a list ofproper nouns), which suffer not only from omissionsbut also often do not allow the listed words to as-sume their common role in text.This paper presents methods for generating listsof nouns that have both common and proper usages(Section 2) and methods for identifying the type ofusage (Section 3) which are evaluated using data de-rived automatically from the BNC (Section 4) andon two applications (Section 5).
It shows that it isdifficult to automatically construct lists of ambigu-ous nouns but also that they can be distinguished ef-fectively using standard features from Word SenseDisambiguation.2 Generating Lists of NounsTo our knowledge, no comprehensive list of com-mon nouns with proper noun usage is available.
Wedevelop a number of heuristics to generate such listsautomatically.Part of speech tags A number of part of speech(PoS) taggers assign different tags to common andproper nouns.
Ambiguous nouns are identified bytagging a corpus and extracting those that havehad both tags assigned, together with the frequencyof occurrence of the common/proper usage.
TheCLAWS (Garside, 1987) and the RASP taggers80(Briscoe et al 2006) were applied to the British Na-tional Corpus (BNC) (Leech, 1992) to generate thelists BNCclaws and BNCrasp respectively.
In addi-tion the RASP tagger was also run over the 1.75 bil-lion word Gigaword corpus (Graff, 2003) to extractthe list Gigaword.Capitalization Nouns appearing intra-sententially with both lower and upper casefirst letters are assumed to be ambiguous.
Thistechnique is applied to the 5-grams from the Googlecorpus (Brants and Franz, 2006) and the BNC(creating the lists 5-grams and BNCcaps).Wikipedia includes disambiguation pages forambiguous words which provide information abouttheir potential usage.
Wikipedia pages for nounswith senses (according to the disambiguation page)in a set of predefined categories were identified toform the list Wikipedia.Named entity recognition The Stanford NamedEntity Recogniser (Finkel et al 2005) was run overthe BNC and any nouns that occur in the corpus withboth named entity and non-named entity tags are ex-tracted to form the list Stanford.WordNet The final heuristic makes use of Word-Net (Fellbaum, 1998) which lists nouns that are of-ten used as proper nouns with capitalisation.
Nounswhich appeared in both a capitalized and lowercasedform were extracted to create the list WordNet.Table 1 shows the number of nouns identified byeach technique in the column labeled words whichdemonstrates that the number of nouns identifiedvaries significantly depending upon which heuris-tic is used.
A pairwise score is also shown to in-dicate the consistency between each list and two ex-ample lists, BNCclaws and Gigaword.
It can be seenthat the level of overlap is quite low and the variousheuristics generate quite different lists of nouns.
Inparticular the recall is low, in almost all cases lessthan a third of nouns in one list appear in the other.One possible reason for the low overlap betweenthe noun lists is mistakes by the heuristics used toextract them.
For example, if a PoS tagger mistak-enly tags just one instance of a common noun asproper then that noun will be added to the list ex-tracted by the part of speech heuristic.
Two filter-ing schemes were applied to improve the accuracy ofthe lists: (1) minimum frequency of occurrence, thenoun must appear more than a set number of timeswords BNCclaws GigawordP R P RBNCclaws 41,110 100 100 31 2BNCrasp 20,901 52 27 45 17BNCcaps 18,524 56 26 66 215-grams 27,170 45 29 59 28Gigaword 57,196 22 31 100 100Wikipedia 7,351 49 9 59 8WordNet 798 75 1 68 1Stanford 64,875 43 67 26 29Table 1: Pairwise comparison of lists.
The nouns in eachlist are compared against the BNCclaws and Gigawordlists.
Results are computed for P(recision) and R(ecall).in the corpus and (2) bias, the least common type ofnoun usage (i.e., common or proper) must accountfor more than a set percentage of all usages.We experimented with various values for these fil-ters and a selection of results is shown in Table 2,where freq is the minimum frequency of occurrencefilter and bias indicates the percentage of the lessfrequent noun type.bias freq words BNCclaws GigawordP R P RBNCclaws 40 100 274 100 1 53 1BNCrasp 30 100 253 94 1 85 05-grams 40 150 305 80 1 67 0Stanford 40 200 260 87 1 47 0Table 2: Pairwise comparison of lists with filteringPrecision (against BNCclaws) increased as the fil-ters become more aggressive.
However comparisonwith Gigaword does not show such high precisionand recall is extremely low in all cases.These experiments demonstrate that it is difficultto automatically generate a list of nouns that exhibitboth common and proper usages.
Manual analy-sis of the lists generated suggest that the heuristicscan identify ambiguous nouns but intersecting thelists results in the loss of some obviously ambigu-ous nouns (however, their union introduces a largeamount of noise).
We select nouns from the listscreated by these heuristics (such that the distribu-tion of either the common or proper noun sense inthe data was not less than 45%) for experiments inthe following sections.11The 100 words selected for our evaluation are available athttp://pastehtml.com/view/cjsbs4xvl.txt813 Identifying Noun TypesWe cast the problem of distinguishing between com-mon and proper usages of nouns as a classificationtask and develop the following approaches.3.1 Most frequent usageA naive baseline is supplied by assigning each wordits most frequent usage form (common or propernoun).
The most frequent usage is derived from thetraining portion of labeled data.3.2 n-gram systemA system based on n-grams was implemented usingNLTK (Bird et al 2009).
Five-grams, four-grams,trigrams and bigrams from the training corpus arematched against a test corpus sentence, and resultsof each match are summed to yield a preferred use inthe given context with a higher weight (experimen-tally determined) being assigned to longer n-grams.The system backs off to the most frequent usage (asderived from the training data).3.3 Vector Space Model (VSM)Distinguishing between common and proper nounscan be viewed as a classification problem.
Treatingthe problem in this manner is reminiscent of tech-niques commonly employed in Word Sense Disam-biguation (WSD).
Our supervised approach is basedon an existing WSD system (Agirre and Martinez,2004) that uses a wide range of features:?
Word form, lemma or PoS bigrams and tri-grams containing the target word.?
Preceding or following lemma (or word form)content word appearing in the same sentence asthe target word.?
High-likelihood, salient, bigrams.?
Lemmas of all content words in the same sen-tence as the target word.?
Lemmas of all content words within a?4 wordwindow of the target word.?
Non stopword lemmas which appear more thantwice throughout the corpus.Each occurrence of a common / proper noun isrepresented as a binary vector in which each positionindicates the presence or absence of a feature.
Acentroid vector is created during the training phasefor the common noun and the proper noun instancesof a word.
During the test phase, the centroids arecompared to the vector of each test instance usingthe cosine metric, and the word is assigned the typeof the closest centroid.4 EvaluationThe approaches described in the previous section areevaluated on two data sets extracted automaticallyfrom the BNC.
The BNC-PoS data set is createdusing the output from the CLAWS tagger.
Nounsassigned the tag NP0 are treated as proper nounsand those assigned any other nominal tag as com-mon nouns.
(According to the BNC manual theNP0 tag has a precision 83.99% and recall 97.76%.2)This data set consists of all sentences in the BNC inwhich the target word appears.
The second data set,BNC-Capital, is created using capitalisation infor-mation and consists of instances of the target nounthat do not appear sentence-initially.
Any instancesthat are capitalised are treated as proper nouns andthose which are non-capitalised as common nouns.Experiments were carried out using capitalisedand decapitalized versions of the two test corpora.The decapitalised versions by lowercasing each cor-pus and using it for training and testing.
Results arepresented in Table 3.
Ten fold cross validation isused for all experiments: i.e.
9/10th of the corpuswere used to acquire the training data centroids and1/10th was used for evaluation.
The average perfor-mance over the 10 experiments is reported.The vector space model (VSM) outperforms otherapproaches on both corpora.
Performance is partic-ularly high when capitalisation is included (VSM wcaps).
However, this approach still outperforms thebaseline without case information (VSM w/o caps),demonstrating that using this simple approach is lesseffective than making use of local context.2No manual annotation of common and proper nouns in thiscorpus exists and thus an exact accuracy figure for this corpuscannot be obtained.82Gold standardBNC-PoS BNC-CapitalMost frequent 79% 67%n-gram w caps 80% 77%n-gram w/o caps 68% 56%VSM w caps 90% 100%VSM w/o caps 86% 80%Table 3: BNC evaluation results5 ApplicationsWe also carried out experiments on two types oftext in which capitalization information may not beavailable: social media and ASR output.5.1 TwitterAs demonstrated in the BNC based evaluations, thesystem can be applied to text which does not containcapitalization information to identify proper nouns(and, as a side effect, enable the correction of capi-talization).
An example of such a dataset are the (upto) 140 character messages posted on Twitter.There are some interesting observations to bemade on messages downloaded from Twitter.
Al-though some users choose to always tweet in lowercase, the overall distribution of capitalization intweets is high for the 100 words selected in Section 2and only 3.7% of the downloaded tweets are entirelylower case.
It also appeared that users who capital-ize, do so fairly consistently.This allows the creation of a dataset based ondownloaded Twitter data3:1.
Identify purely lower case tweets containingthe target word.
These will form the test data(and are manually assigned usage).2.
Any non-sentence initial occurrences of the tar-get word are used as training instances: lowercase indicating a common instance, upper caseindicating a proper instance.14 words4 were randomly selected from the listused in Section 4 and their lowercase tweet instanceswere manually annotated by a single annotator.
The3http://search.twitter.com/api4abbot, bull, cathedral, dawn, herald, justice, knight, lily,lodge, manor, park, president, raven and windowsTraining corpus MF n-grams VSMTwitter 59% 40% 60%BNCclaw decap 59% 44% 79%Table 4: Results on the Twitter dataaverage proportion of proper nouns in the test datawas 59%.The results for the three systems are presented inTable 4.
As the length of the average sentence in theTwitter data is only 15 words (compared to 27 wordsin the BNCclaws data for the same target words),the Twitter data is likely to be suffering sparsenessissues.
This hypothesis is partly supported by the in-crease in performance when the BNCclaws decapi-talized data is added to the training data, however,the performance of the n-gram system remains be-low the most frequent use.
On closer examination,this is likely due to the skew in the data ?
there aremany more examples for the common use of eachnoun, and thus each context is much more likely tohave been seen in this setting.5.2 Automatic speech recognitionMost automatic speech recognition (ASR) systemsdo not provide capitalization.
However, our sys-tem does not rely on capitalization information, andtherefore can identify proper / common nouns evenif capitalization is absent.
Also, once proper nounsare identified, the system can be used to restore case?
a feature which allows an evaluation to take placeon this dataset.
We use the TDT2 Test and Speechcorpus (Cieri et al 1999), which contains ASR anda manually transcribed version of news texts fromsix different sources, to demonstrate the usefulnessof this system for this task.The ASR corpus is restricted to those segmentswhich contain an equal number of target word oc-currences in the ASR text and the manually tran-scribed version, and all such segments are extracted.The gold standard, and the most frequent usage, aredrawn from the manually transcribed data.Again, results are based on an average perfor-mance obtained using a ten fold cross validation.Three versions of training data are used: the 9/10 ofASR data (with labels provided by the manual tran-scription), the equivalent 9/10 of lowercased manu-83Training corpus MF n-grams VSMManual 66% 42% 73%ASR 63% 41% 79%Table 5: Results on the ASR dataally transcribed data, and a combination of the two.The results can be seen in Table 5.
The perfor-mance rise obtained with the VSM model when theASR data is used is likely due to the repeated errorswithin this, which will not be appearing in the man-ually transcribed texts.
The n-gram performance isgreatly affected by the low volume of training dataavailable, and again, a large skew within this.6 ConclusionWe automatically generate lists of common andproper nouns using a number of different techniques.A vector space model technique for distinguish-ing common and proper nouns is found to achievehigh performance when evaluated on the BNC.
Thisgreatly outperforms a simple n-gram based system,due to its better adaptability to sparse training data.Two application based evaluations also demonstratethe system?s performance and as a side effect thesystem could serve as a technique for automatic caserestoration.AcknowledgmentsThe authors are grateful to the funding for thisresearch received from Google (Google ResearchAward) and the UK Engineering and Physical Sci-ences Research Council (EP/J008427/1).ReferencesAgirre, E. and Martinez, D. (2004).
The Basque Coun-try University system: English and Basque tasks.In Senseval-3: Third International Workshop on theEvaluation of Systems for the Semantic Analysis ofText, pages 44?48.Baeza-Yates, R. and Ribeiro-Neto, B.
(2011).
ModernInformation Retrieval: The Concepts and TechnologyBehind Search.
Addison Wesley Longman Limited,Essex.Baldwin, T., Paul, M., and Joseph, A.
(2009).
Restoringpunctuation and casing in English text.
In Proceedingsof the 22nd Australian Joint Conference on ArtificialIntelligence (AI09), pages 547?556.Bird, S., Klein, E., and Loper, E. (2009).
Natural Lan-guage Processing with Python ?
Analyzing Text withthe Natural Language Toolkit.
O?Reilly.Brants, T. and Franz, A.
(2006).
Web 1T 5-gram v1.Briscoe, T., Carroll, J., and Watson, R. (2006).
The sec-ond release of the RASP system.
In Proceedings of theCOLING/ACL 2006 Interactive Presentation Sessions.Chen, H., Huang, S., Ding, Y., and Tsai, S. (1998).Proper name translation in cross-language informationretrieval.
In Proceedings of the 36th Annual Meetingof the Association for Computational Linguistics and17th International Conference on Computational Lin-guistics, Volume 1, pages 232?236, Montreal, Canada.Cieri, C., Graff, D., Liberman, M., Martey, N., andStrassel, S. (1999).
The TDT-2 text and speech cor-pus.
In Proceedings of DARPA Broadcast News Work-shop, pages 57?60.Fellbaum, C., editor (1998).
WordNet: An ElectronicLexical Database and some of its Applications.
MITPress, Cambridge, MA.Finkel, J. R., Grenager, T., and Manning, C. (2005).
In-corporating non-local information into information ex-traction systems by Gibbs sampling.
In Proceedings ofthe 43nd Annual Meeting of the Association for Com-putational Linguistics, pages 363?370.Garside, R. (1987).
The CLAWS word-tagging system.In Garside, R., Leech, G., and Sampson, G., editors,The Computational Analysis of English: A Corpus-based Approach.
London: Longman.Graff, D. (2003).
English Gigaword.
Technical report,Linguistic Data Consortium.Hermjakob, U., Knight, K., and Daume?
III, H. (2008).Name translation in statistical machine translation -learning when to transliterate.
In Proceedings of ACL-08: HLT, pages 389?397, Columbus, Ohio.Leech, G. (1992).
100 million words of English:the British National Corpus.
Language Research,28(1):1?13.Lopez, A.
(2008).
Statistical machine translation.
ACMComputing Surveys, 40(3):1?49.Pang, B. and Lee, L. (2008).
Opinion mining and senti-ment analysis.
Foundations and Trends in InformationRetrieval, Vol.
2(1-2):pp.
1?135.Petrovic?, S., Osborne, M., and Lavrenko, V. (2010).Streaming first story detection with application to twit-ter.
In Human Language Technologies: The 2010 An-nual Conference of the North American Chapter ofthe Association for Computational Linguistics, pages181?189, Los Angeles, California.Wilson, T., Wiebe, J., and Hoffman, P. (2009).
Recogniz-ing contextual polarity: an exploration of features forphrase-level sentiment analysis.
Computational Lin-guistics, 35(5).84
