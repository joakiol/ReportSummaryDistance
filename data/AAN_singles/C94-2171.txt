N-GRAM CLUSTER IDENTIF ICAT ION DURING EMPIR ICAL  KNOWLEDGEREPRESENTAT ION GENERATIONRobin CollierDepartment of Computer Science, University of SheffieldRegent Court, 211 Portobello Street, Sheffield, S1 4DP, Englandr.collier@dcs.shef.ac.ukAbstract:This paper presents an overview of currentresearch concerning knowledge xtraction fromtechnical texts.
In particular, the use of empiri-cal techniques during the identification and gen-eration of a semantic representation isconsidered.
A key step is the discovery of use-ful n-grams and correlations between clusters ofthese n-grams.keywords: knowledge representation, large textcorpora, language understanding.1.
BACKGROUNDThe primary knowledge extraction and text retrievalconferences (MUC-4, 1992; TREC-1, 1993; TIPSTER,forthcoming) utilise domain-specific queries and tem-plates to identify relevant concepts from within a corpusand extract applicable documents or information.The structures generated by the system discussed inthis paper are similar to these domain-specific templates,they could be used for compact representation f infor-mation contained in documents for text retrieval pur-poses.
The automatic generation of templates would be asignificant development.The motivation for generating a domain specific rep-resentation is similar to that of Riloff (1993), althoughthe approach is quite different.
The conceptual sentenceanalyser developed at the University of Massachusetts,CIRCUS (Lehnert, 1990), contains apart-of-speech lexiocon and a manually constructed concept dictionary,Riloff's AutoSlog will autonmtically generate a domain-specific oncept dictionary.Case frames are used to represent concepts.
Each con-cept contains a range of information.
The trigger is aspecific word or phrase identifying a potential match.
Aset of enabling conditions defines constraints that requiresatisfaction.
Relevant information, extracted from thesurrounding context, is placed into variable slots whichdefine information such as objects and actors.
Each wwi-able slot has a syntactic expectation associated with itwhich defines the expected linguistic context.
Slot con-straints define seleetional restrictions on the slot filler.Finally, information that is common to all instantiationsof the concept is defined in constant slots.Autoslog utilises a set of heuristics to determinewhich words and phrases are likely to activate usefulconcept nodes.
For example, the conceptual anchorpoint heuristics define typical linguistic contexts sur-rounding prospective triggers.A variety of other systems which generate domain-specific representations ,are discussed in a survey paperby Collier (forthcoming).
Some of these systems gener-ate structures that are similar to templates, for exaanpleGENESIS (Mooney, 1985), and others acquire domainspecific semantic representations, for exampleMAIMRA (Siskind, 1990).2.
SYSTEM OVERVIEWThe approach acquires adomain specific semantic repre-sentation by carrying out stochastic analysis on a largecorpus from a technical domain, ltigh frequency phrasesare identified and used to recognise groups of paragraphscontaining similar subsets of these phrases.
It is assumedthat, in general, the similarities between paragraphswithin each group will define stereotypical concepts.Tools will enable a domain expert o view and manipu-late these sets of paragraphs and generate a hierarchicalsemantic representation f concepts.The corpus ,and semantic representation are used togenerate schematic structures within the technicaldomain.
Each structure consists of a list of semantic on-cepts.
Sets of structures which have a high level of corre-spondence are generated.
It is assumed that stereotypicalstructures are represented by similarities between themembers of sets containing a sufficient number of struc-tures, and sufficient correspondence.
These are stored ina structure knowledge base.The structures represent stereotypical situations uchas lists of actions (e.g.
scientific experiments), and com-mon textual information (e.g.
the definition of applica-tion areas).
They are used to translate the existing textsinto a semantic/pragmatic representation and store theknowledge in a concise and structured format in a tech-nical knowledge base.New texts are processed immediately after publica-tion, dynamically updating the technical knowledgebase.
If segments of new texts cannot be processed bythe existing structures, then they are analysed and anovel structure is appended to the structure base.Collier (1993) presents a more comprehensive outlineof the system's architecture ,and some preliminary sto-chastic analysis.3.
PARAGRAPH CLUSTERINGThe fundamental stage in the process described above isthe generation of a domain specific semantic representa-tion.
The approach identifies clusters of useful n-grams1054within paragraphs which correlate with other paragraphs.The term useful defines n-grams that have certain quali-ties, such as a high frequency of occnrrence, and a widedistribution over texts within the domain.There are two principal steps in the identification ofthese chtsters: to recognise useful n-grams of varyinglengths within a corpus, and to recoguise sets of para-graphs which contain similar clusters, and therefore cor-relate.3.1 StructuresFive fundamental strllctures are used during tile identifi-cation of correlating paragraphs.3.1.1 Un ique  word / in teger  a r rayTbe tirst structure is an associative array containing anentry for each unique word ill the corpus.
Each entry isindexed by the word, and holds a unique integer epre-senting that word.This array is nsed to translate the textual corpus into alist of integers.
All subsequent processing is carried outon this list of integers, this increases efficiency.The renmining four structures have the same format.Rather than being in word order, as the original text is,identical words ,are grouped together in the anay.
Theseword groups are ordered according to their size.
For thisreason, the word with the highest frequency of occur-reuce within the text will exist at the beginning of thearray.
Figure 1 gives an example of the typical array for?matI~q~ 3__12 I~YU_YX_ \ ]  ..- E-LINFig.
1: array formatThe highest frequency word that occurs within the textis the, therefore its group is at the beginning of the array.The second highest liequency word is and, then of., etc.The lowest frequency word is set, its group is positionedat the end of the array.The information contained in each of the rc,nainingfour arrays is explained below.3.1.2 Word  order  a r rayDue to the grouping of words, the word order will havebeen lost.
The second structure defines this, it containspointers to the next word ill the text.
Figure 2 shows thepositions of the pointers representing the phrase "... theset of ...".<:- Cho ---><--- .
.
.
.
a----><---- or .-.
,~t - -+Fig.
2: word order array3.1.3 Next word arrayThe third structure contains tile unique integer epresent-ing the next word pointed to in the text.
The value of thiswill be the integer that represents he word group whichthe word ordering array element points to.It is clear that the grouping o1' the words in the arraysmakes it necessary to create additional arrays and com-plicates the existing ones.
The advantage of this group-ing is increased computational elficiency.An example of the enhanced efficiency can he demon-strated by considering the identification of similar n-grams.
The next word array groups together next wordvalues which are present alter identical words in the text.For example, if the two word phrases the book, the car,the book and the explosion were present in the text, thenintegers representing book, ear, book and explosionwould be grouped together in the next word array.
Whentesting for silnilar n-grams it is only necess,'u'y to lookthrough one section of the array to identify sets of identi-cal n+ l-grams, rather than it being necessary to jump tomany different positions within an extremely large array.This increases the efficiency of memory access due to theenormous reduction in memory paging.3.1.4 Phrase  length ar rayThe lburth structure contains a phrase length associatedwith each word.
For example, a 1 represents an individ-ual word, 2 represents athi-gram (the word attd the cmethat is pointed to as the next one), etc.After the process is complete this array will associatethe useful n-grants with their initial word and also definetheir length.3.1.5 Next phrase  ar rayThe final structure is related to the fourth.
Each corre-sponding entry is a pointer to the next identical phrase.For example, if there were three occurrences of the set ofnumbers in the corpus, then there would be three entriesin the phrase length array containing a 4.
Each of the cor-responding entries in the next phrase array would pointto the next identical phrase (figure 3).Fig.
3: phrase length and next phrase arrays3.2 A lgor i thmThe two lnincipal steps of the process described in sec-tion 3 can be divided into six snbsteps.
The first four sub-steps represent the identification of usefnl n-gnuns ofvarying lengths within a corpus, and the last two rcpte-sent the identification of sets of p~agraphs which con-lain similar clusters.Each of the substeps, which create and manipulate thestructures defined in section 3.
I, is explained below.70553.2.1 Word/ integer  generat ionThis procedure produces three arrays.
The first associateseach tmique word with a unique integer, the seconddefines the frequency of occurrence of each word, andthe third contains pointers to the first position of eachword group in the array format defined in figure 1.Initially, each word in the corpus is read sequentially.If an entry associating the word with a unique integerisn't already present, then one is created.
If an entry ispresent, another array containing the frequency of occur-rence of each word is incremented.The array containing the words and their associatedunique integer is sorted into descending order by consid-ering each word's frequency.
Therefore the highest fre-quency word is associated with 1, the second highestwith 2, etc.
An array is also created which contains theinitial index positions of each unique word in the wordgrouping format (figure 1).
For example, the highest fie-quency word would have an initial index of zero.
If it hada frequency of 10, then the second highest frequencyword index would be 10.
If the second word had a fre-quency of eight, then the third highest frequency word'sindex would be 18.
This indexing array is required ur-ing the creation of the word order and next word arrays.3.2.2 Integer translat ionThis stage creates three arrays.
The first and second arethe word order and next word ,'m'ays, defined in sections3.1.2 and 3.1.3.
The third is an ,array associating eachdocument in the corpus with the position, in the wordorder array, of its first word.This procedure sequentially processes each word ofeach document.
As each new document commences, thedocument name and the pointer value associated with thefirst word are stored in an array.
This enables the begin-ning of any document to be accessed.For each word, the associated index position from thearray generated in the previous step is looked up.
Thisindex value is stored in the position in the word orderingarray of the previous word that was read.
Therefore,defining that this is the index of the next word after theprevious one.
it also stores the current word's uniqueinteger in the position in the next word array of the previ-ous word that was read.
Therelbre, defining that this isthe unique integer of the next word after the previousone.
Fin,-dly, it increments the index pointer of the word,as this position has now been filled.At the end of each paragraph a special integer epre-senting the carriage return is placed in the next wordarray, this enables identification of paragraph boundaries.3.2.3 Generate phrase lengthsThis step generates three arrays.
The first is the phraselength array defined in section 3.1.4.
The second is thenext phrase ,array defined in section 3.1.5.
The third issimilar to the previous one, but it points to the previousidentical phrase rather than the next.
The algorithmbecomes rather complicated when overwriting existingentries in the second and third arrays.
This is due to themanipulation of the pointers to the next and previousidentical phrases.Each of the groups of similar words are processed inturn (e.g.
in figure 1 all of the the's, then the and's, etc.
).The next word array is used to identify the word follow-ing the first the in the group.
Then all of the other the'sare checked to identify those with the same next word,creating a set of those that match.
This set represents allof the phrases within the corpus that are the same as thefirst bi-gram.The phrase length is incremented to two, and thismatching process is repeated for the next word of theoriginal phrase (i.e.
the third word of the n-gram), butonly on the reduced set of previously matching words.This process continues until the longest phrase whichoccurs a multiple number of times is generated, or a car-riage return is encountered.If the final phrase length is greater than one, then eachof the words in the nmtching set is processed in turn.
Ifthe position pointed to by the word does not already havea phrase associated with it, then the phrase length isstored in the associated position in the phrase lengtharray.
The position in the next phrase array of the previ-ous phrase in the matching set is updated with the currentphrase's position, and therefore defines that this is thenext identical phrase after the previous one.
Also, anarray which defines the previous phrase's position isupdated by storing the pointer value of the previousphrase in the current phrase's lot, and therefore pointingto the previous identical phrase.If the position does already contain a phrase lengththat is longer, then the current phrase is missed out andthe next one processed.
In this case the position alreadyhas a longer n-gram associated with it.If the new phrase length is longer than the current one,then the phrase is overwritten, but the pointers to the pre-vious and next phrase require updating.
For example, ifboth a previous phrase and a next phrase pointer existthen the current position should be removed from thelinking up of the existing set of identical phrases (figure4).
It is necessary to alter the next phrase value of theprevious phrase (which is currently set to the position tobe overwritten) to the current positions next phrase.
Alsothe next phrase's previons phrase position (which holdsthe ct, rrent position to be overwritten) requires updatingto the current phrases previous phrase position.old pointer --~>new pointerphrasepreviousphrase ~~\]~Fig.
4: identical phrase removal1056This process is repeated for all of the other the's inturn, attd then for each of the other groups, generatingthe longest n-grams which have at least wo occurrences.3.2,4 Identi fy useful n-gramsThe fourth step is the identification of the n-grams thatprovide effective correlations between phrases and para-graphs.
The phrase length and next phrase arrays arerevised so that they only contain these u-grams.The previous process will have identified the longestphrase that occurs a mt, ltiple number of times in the cor-pns.
The phrase length array ix traversed and each phrasewith this longest length is stored in a set.
At the sametime, the next phrase array is nsed to identify the fre-quency of occurrence of each phrase.
This can beobtained by counting while traversing through the point-ers to the next identical phrase.This set of longest phrases is arranged in ascendingorder by frequency of occurrence.
The n-best remain inthe phrase length and next phrase arrays.
The value of nwill depend on the domain being analysed.
A domainwith considerable correlation will have a greater u than adomain with little correlation.
This is an ,area for fi~rtherinvestigation after development of the entire system.All of the subphrases that exist within these n-best aredeleted flom the arrays.
For example in the phrase the setof numbers, subphrases set of numbers and of numberswill be deleted and so that they are not considered uringfi,rther analysis.Those that do not exist within the u-best have theirassociated phrase lengths reduced by one.
This shorterphrase is compared with all other phrases of the samelength in the group to identify whether it is identical toan existing phrase.
If this is the case, then the next phrasepointer of the last phrase in the set will be altered to pointto the first phrase in the identical phrase set, and vice-versa for the previous phrase array.This entire process is repeated, reducing the length ofthe phrases to be considered by one each time.
There-fore, the second iteration will consider phrases with alength equal to the longest phrase minus one, the thirditeration considers phrases with a length equal to thelongest phrase minus two, etc.When this process is complete the phrase length andnext phrase arrays will contain all of the useful phrases.The final two processes identify clusters of phraseswithin individual p,'u'agraphs which correlate with clus-ters of phrases in other paragraphs.3.2.5 Paragraph  weight parseThis procedme associates each paragraph with a weightrepresenting its probability of correlating with otherparagraphs.
The weight considers factors uch as the sizeof the paragraph, the size and frequency of n-gramsexisting within that paragraph, and the distribution of then-grams throughout the corpus.The actual process is relatively straightforward.
Thecorpus is parsed, beginning at the first word and nsiugtile pointers in the next word array.
This will traverse thewords in the order of the original text, enabling identifi-cation of all n-grams in each paragraph and using themin an equation to assign the correlation weight.The current equation to generate paragraph weights is:(n0nl b i -g , .
s*2 ,~)+(n l :m~~*(n+( fn~ 1)*0.5)~total no words ill paragraphThis equation is simple but accounts for ,'Ill the impor-tmtt factors listed above, apart from the distribution ofthe n-granls within the corpus.These weights are nsed to sort the paragraphs intoascending order.3.2.6 Identify useful paragraph clustersThe final process identifies ,all of the sets of correlatingparagraphs within tile corpus, mid extracts tile highestquality correlations.Each paragraph produced in the previous tep is pro-cessed in tt, rn.
Using tile next phrase array, all para-graphs which correlate with at least one n-gram are iden-tified.Groups of paragraphs containing identical subsets of'n-grants are identified and placed into sets.
Each of thesesets can then be assigned aweight representing the quan-tity, i.e.
number of paragraphs, and quality, i.e.
numberattd size of n-grams.The final step is to sort the correlation weights intoascending order.The system has now produced a list of n-gram clustersrepresenting paragraph correlations.
These are orderedby considering the quality of n-grams within the cluster,and the quantity of con'elation occurring with other para-graphs.
From the assumptions outlined in section 2, "thesimilarities between paragraphs within each gronp willdefine stereotypical concepts", these clusters will beextremely useful in the generation of a domain specificsemantic representation.4.
PRELIMINARY RESULTSThe entire system, which is discussed in section 2, is cur-rently trader development.
The stage concerning theidentification of cot~'elating paragraphs, which is dis-cussed in section 3, has only recently been implemented.For this reason there are a limited number of results toreport upon.The corpt, s currently being cousidered consists of 82chemical patents containing over half a million words.The progr~,nls are beiug rt,n on a Sun TM SparcstationClassic with 32 megabytes of RAM.qhble 1 presents an elementary example which isintended to demonstrate he systems cope for improve-meat as larger corpora are considered.
It shows that it ispossible to identify paragraphs which sufficiently corre-late to provide a strong indication of fundamental con-cepts within the domain.
In this example, a commonstage of an expert,neat is being indicated.1057The results in table 1 were gained from analysis of asingle patent containing approximately 14000 words.
Inthe patent, 15 paragraphs contained the 4 gram (this isdefined by a **4** after the first word of the n-gram)This was prepared from, and two of these contained the9-gram oxime and 3-methoxycarbonyl-l-vinylo?
'-carbo-nyl-l,2,5,6-tetrahydropyridine and recrystallised frommethanol/diethyl ether; mp.This **4** was prepared from isopropyl car-boxamide oxime **9** and 3-methoxycarbo-nyl-l-vinyloxy-earbonyl- 1,2,5,6-tetrahydropyridiue and recrystallised frommethanol/dlethyl ether, mp 112~C, Rf = 0.28in dichloromethane/methanol (20:1) on silica.This **4** was prepared from phenylaceta-mide oxime **9** and 3-methoxycarbonyl-1-vinyloxy-earbonyl- 1,2,5,6-tetrahydropyri-dine and reerystallised from metlmnol/diethyl ether, nrp 154-158~C, Rf = 0.63 indichloro-methanc/methanol (20:1) on ahnnina.Table 1 : examples of paragraph correlationFurther correlations exists between the two para-graphs which have not been identified by the system dueto the n-grams either being small or containing minortextual differences (e.g.
OC, Rf =, and dichloro-methane/methanol (20:1) on).Many more examples can be drawn from the analysisof this single patent which contain a large number of cor-relating n-grams but are too large and complicated toreport on in this paper.Finally, an interesting result was that a 69-gram wasidentified which occurred twice within the single patent.It concerned the exph'mation of a diagram presenting thestructure of a compound.5.
CONCLUSIONSI am not aware of any techniques, within knowledge rep-resentation generation research, which are significantlysimilar to this clustering approach.
The novelty is due tothe use of n-gram correspondences during the identifica-tion of sets of paragraphs containing similar conceptualinlbrmation, and the employment of these examples toemphasise the fundamental concepts within the domain.For this reason, it could prove to be a rewarding area forfurther esearch.Due to the nature of technical documents and techni-cal language, a large quantity of the phrases used ,'u'ehighly structured and standardised.
This formalismimplies that the n-gram clustering approach will produceeffective resnlts during the identification of conceptuallysimilar paragraphs.An essential test will be the assessment of a domain-specific semantic representation created using the corre-lating paragraphs generated by the system and the toolsmentioned in section 2.
It will be necessary to evaluatethe scope and quality of the representation.
One possibil-ity is to compare, using an identical corpus, a representa-tion created by a group of experts with that of the system.The fundamental point to convey is that as larger cor-pora are analysed the quantity of examples and quality ofcorrelations will improve.
The results of filrther experi-mentation and analysis will be reported in fi~ture publica-tions.Although this knowledge representation generation isthe flmdamental stage of the process outlined in section2, it is only a fragment of the entire system.
An applica-tion developed using this process has the potential to beinvaluable for domain specialists who wish to identifydocuments contailbing simih'u" conceptual informationwithin extremely large knowledge bases.6.
REFERENCESCollier, R. (1993).
Knowledge acquisition from technicaltexts using natural anguage processing techniques.Proceedings of the 2nd Workshop on the CognitiveScience of Natural Language Processing, pp.
11.1 to11.15.
Dublin, Eire: Dublin City University.Collier, R. (forthcoming).
An historical overview of nat-ural language processing systems that learn.
Artifi-cial Intelligence Review.
Kluwer AcademicPublisher: Dordrecht, Germany.Lehnert, W. (1990).
Symbolic/subsymbolic sentenceanalysis: exploiting the best of two worlds.
InBarnden, J. and J. Pollack (Eds.
), Advances in Con-nectionist and Neural Computation Theory, volume1, pp.
135 to 164.
Ablex Publishers: Norwood, NJ.Mooney, R. (1985) Generalising expl,'mations of narra-tives into schemata.
Technical Report T-147.
Co-ordinated Science Laboratory, University of Illinois,Urbana.MUC-4 (1992).
Proceedings of the Fourth MessageUnderstanding Conference.
Morgan Kaufmann: SanMarco, CA.Riloff, E. (1993).
Automatically constructing a dictio-nary for information extraction tasks.
Proceedingsof the Eleventh National Conference of ArtificialIntelligence.
Washington, D.C.: MrF Press, Cam-bridge, MA.Siskind, J.M.
(1990) Acquiring core meanings of words,represented as Jackendoff-style conceptual struc-tures, from correlated streams of lingnistic and non-linguistic input.
Proceedings of the Twenty-eighthAnnual Meeting of the Association for Computa-tional Linguistics, pp.
143 to 156.
University ofPittsburgh, Pennsylvania: Association for Computa-tional Linguistics.TIPSTER (forthcoming).
Proceedings of TIPSTER TextPhase L 24 Month Conference.
Morgan Kaufinann:Fredericksburg, Virginia.TREC-1 (1993).
Proceedings of The First 7~'xt RetrievalConference, Iiarman, D.K.
(Ed.).
National Instituteof Standards and q~chnology: Gaithersburg, Mary-land.7058
