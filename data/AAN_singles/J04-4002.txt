c?
2004 Association for Computational LinguisticsThe Alignment Template Approach toStatistical Machine TranslationFranz Josef Och?
Hermann Ney?Google RWTH AachenA phrase-based statistical machine translation approach ?
the alignment template approach ?
isdescribed.
This translation approach allows for general many-to-many relations between words.Thereby, the context of words is taken into account in the translation model, and local changesin word order from source to target language can be learned explicitly.
The model is describedusing a log-linear modeling approach, which is a generalization of the often used source?channelapproach.
Thereby, the model is easier to extend than classical statistical machine translationsystems.
We describe in detail the process for learning phrasal translations, the feature functionsused, and the search algorithm.
The evaluation of this approach is performed on three differenttasks.
For the German?English speech Verbmobil task, we analyze the effect of various sys-tem components.
On the French?English Canadian Hansards task, the alignment templatesystem obtains significantly better results than a single-word-based translation model.
In theChinese?English 2002 National Institute of Standards and Technology (NIST) machine transla-tion evaluation it yields statistically significantly better NIST scores than all competing researchand commercial translation systems.1.
IntroductionMachine translation (MT) is a hard problem, because natural languages are highlycomplex, many words have various meanings and different possible translations, sen-tences might have various readings, and the relationships between linguistic entitiesare often vague.
In addition, it is sometimes necessary to take world knowledge intoaccount.
The number of relevant dependencies is much too large and those depen-dencies are too complex to take them all into account in a machine translation system.Given these boundary conditions, a machine translation system has to make decisions(produce translations) given incomplete knowledge.
In such a case, a principled ap-proach to solving that problem is to use the concepts of statistical decision theory to tryto make optimal decisions given incomplete knowledge.
This is the goal of statisticalmachine translation.The use of statistical techniques in machine translation has led to dramatic im-provements in the quality of research systems in recent years.
For example, the statis-tical approaches of the Verbmobil evaluations (Wahlster 2000) or the U.S. National?
1600 Amphitheatre Parkway, Mountain View, CA 94043.
E-mail: och@google.com.?
Lehrstuhl fu?r Informatik VI, Computer Science Department, RWTH Aachen?University of Technology,Ahornstr.
55, 52056 Aachen, Germany.
E-mail: ney@cs.rwth-aachen.de.Submission received: 19 November 2002; Revised submission received: 7 October 2003; Accepted forpublication: 1 June 2004418Computational Linguistics Volume 30, Number 4Institute of Standards and Technology (NIST)/TIDES MT evaluations 2001 through20031 obtain the best results.
In addition, the field of statistical machine translation israpidly progressing, and the quality of systems is getting better and better.
An im-portant factor in these improvements is definitely the availability of large amounts ofdata for training statistical models.
Yet the modeling, training, and search methodshave also improved since the field of statistical machine translation was pioneered byIBM in the late 1980s and early 1990s (Brown et al 1990; Brown et al 1993; Berger etal.
1994).
This article focuses on an important improvement, namely, the use of (gen-eralized) phrases instead of just single words as the core elements of the statisticaltranslation model.We describe in Section 2 the basics of our statistical translation model.
We suggestthe use of a log-linear model to incorporate the various knowledge sources into anoverall translation system and to perform discriminative training of the free modelparameters.
This approach can be seen as a generalization of the originally suggestedsource?channel modeling framework for statistical machine translation.In Section 3, we describe the statistical alignment models used to obtain a wordalignment and techniques for learning phrase translations from word alignments.
Here,the term phrase just refers to a consecutive sequence of words occurring in text andhas to be distinguished from the use of the term in a linguistic sense.
The learnedbilingual phrases are not constrained by linguistic phrase boundaries.
Compared tothe word-based statistical translation models in Brown et al (1993), this model is basedon a (statistical) phrase lexicon instead of a single-word-based lexicon.
Looking at theresults of the recent machine translation evaluations, this approach seems currently togive the best results, and an increasing number of researchers are working on differentmethods for learning phrase translation lexica for machine translation purposes (Marcuand Wong 2002; Venugopal, Vogel, and Waibel 2003; Tillmann 2003; Koehn, Och, andMarcu 2003).
Our approach to learning a phrase translation lexicon works in twostages: In the first stage, we compute an alignment between words, and in the secondstage, we extract the aligned phrase pairs.
In our machine translation system, we thenuse generalized versions of these phrases, called alignment templates, that also includethe word alignment and use word classes instead of the words themselves.In Section 4, we describe the various components of the statistical translationmodel.
The backbone of the translation model is the alignment template feature func-tion, which requires that a translation of a new sentence be composed of a set of align-ment templates that covers the source sentence and the produced translation.
Otherfeature functions score the well-formedness of the produced target language sentence(i.e., language model feature functions), the number of produced words, or the or-der of the alignment templates.
Note that all components of our statistical machinetranslation model are purely data-driven and that there is no need for linguisticallyannotated corpora.
This is an important advantage compared to syntax-based trans-lation models (Yamada and Knight 2001; Gildea 2003; Charniak, Knight, and Yamada2003) that require a parser for source or target language.In Section 5, we describe in detail our search algorithm and discuss an efficientimplementation.
We use a dynamic-programming-based beam search algorithm thatallows a trade-off between efficiency and quality.
We also discuss the use of heuristicfunctions to reduce the number of search errors for a fixed beam size.In Section 6, we describe various results obtained on different tasks.
For theGerman?English Verbmobil task, we analyze the effect of various system compo-1 http://www.nist.gov/speech/tests/mt/.419Och and Ney The Alignment Template Approach to Statistical Machine TranslationFigure 1Architecture of the translation approach based on a log-linear modeling approach.nents.
On the French?English Canadian Hansards task, the alignment template sys-tem obtains significantly better results than a single-word-based translation model.
Inthe Chinese?English 2002 NIST machine translation evaluation it yields results that aresignificantly better statistically than all competing research and commercial translationsystems.2.
Log-Linear Models for Statistical Machine TranslationWe are given a source (French) sentence f = f J1 = f1, .
.
.
, fj, .
.
.
, fJ, which is to be trans-lated into a target (English) sentence e = eI1 = e1, .
.
.
, ei, .
.
.
, eI.
Among all possibletarget sentences, we will choose the sentence with the highest probability:2e?I1 = argmaxeI1{Pr(eI1 | fJ1)} (1)The argmax operation denotes the search problem, that is, the generation of the outputsentence in the target language.As an alternative to the often used source?channel approach (Brown et al 1993),we directly model the posterior probability Pr(eI1 | fJ1) (Och and Ney 2002).
An es-pecially well-founded framework for doing this is the maximum-entropy framework(Berger, Della Pietra, and Della Pietra 1996).
In this framework, we have a set of M fea-ture functions hm(eI1, fJ1), m = 1, .
.
.
, M. For each feature function, there exists a model2 The notational convention employed in this article is as follows.
We use the symbol Pr(?)
to denotegeneral probability distributions with (nearly) no specific assumptions.
In contrast, for model-basedprobability distributions, we use the generic symbol p(?
).420Computational Linguistics Volume 30, Number 4parameter ?m, m = 1, .
.
.
, M. The direct translation probability is given byPr(eI1 | fJ1) = p?M1 (eI1 | fJ1) (2)=exp[?Mm=1 ?mhm(eI1, fJ1)]?e?
I1exp[?Mm=1 ?mhm(e?I1, fJ1)](3)This approach has been suggested by Papineni, Roukos, and Ward (1997, 1998) for anatural language understanding task.We obtain the following decision rule:e?I1 = argmaxeI1{Pr(eI1 | fJ1)}= argmaxeI1{M?m=1?mhm(eI1, fJ1)}Hence, the time-consuming renormalization in equation (3) is not needed in search.The overall architecture of the log-linear modeling approach is summarized in Figure 1.A standard criterion on a parallel training corpus consisting of S sentence pairs{(fs, es): s = 1, .
.
.
, S} for log-linear models is the maximum class posterior probabilitycriterion, which can be derived from the maximum-entropy principle:?
?M1 = argmax?M1{S?s=1log p?M1 (es | fs)}(4)This corresponds to maximizing the equivocation or maximizing the likelihood of thedirect-translation model.
This direct optimization of the posterior probability in Bayes?decision rule is referred to as discriminative training (Ney 1995) because we directlytake into account the overlap in the probability distributions.
The optimization prob-lem under this criterion has very nice properties: There is one unique global optimum,and there are algorithms (e.g.
gradient descent) that are guaranteed to converge to theglobal optimum.
Yet the ultimate goal is to obtain good translation quality on un-seen test data.
An alternative training criterion therefore directly optimizes translationquality as measured by an automatic evaluation criterion (Och 2003).Typically, the translation probability Pr(eI1 | fJ1) is decomposed via additional hid-den variables.
To include these dependencies in our log-linear model, we extend thefeature functions to include the dependence on the additional hidden variable.
Usingfor example the alignment aJ1 as hidden variable, we obtain M feature functions of theform hm(eI1, fJ1, aJ1), m = 1, .
.
.
, M and the following model:Pr(eI1, aJ1 | fJ1) =exp(?Mm=1 ?mhm(eI1, fJ1, aJ1))?e?
I1,a?J1exp(?Mm=1 ?mhm(e?I1, fJ1, a?J1))Obviously, we can perform the same step for translation models with an even richerset of hidden variables than only the alignment aJ1.3.
Learning Translation LexicaIn this section, we describe methods for learning the single-word and phrase-basedtranslation lexica that are the basis of the machine translation system described in421Och and Ney The Alignment Template Approach to Statistical Machine TranslationSection 4.
First, we introduce the basic concepts of statistical alignment models, whichare used to learn word alignment.
Then, we describe how these alignments can beused to learn bilingual phrasal translations.3.1 Statistical Alignment ModelsIn (statistical) alignment models Pr(f J1, aJ1 | eI1), a ?hidden?
alignment a = aJ1 is intro-duced that describes a mapping from a source position j to a target position aj.
Therelationship between the translation model and the alignment model is given byPr(f J1 | eI1) =?aJ1Pr(f J1, aJ1 | eI1) (5)The alignment aJ1 may contain alignments aj = 0 with the ?empty?
word e0 to accountfor source words that are not aligned with any target word.In general, the statistical model depends on a set of unknown parameters ?
that islearned from training data.
To express the dependence of the model on the parameterset, we use the following notation:Pr(f J1, aJ1 | eI1) = p?
(fJ1, aJ1 | eI1) (6)A detailed description of different specific statistical alignment models can be found inBrown et al (1993) and Och and Ney (2003).
Here, we use the hidden Markov model(HMM) alignment model (Vogel, Ney, and Tillmann 1996) and Model 4 of Brown etal.
(1993) to compute the word alignment for the parallel training corpus.To train the unknown parameters ?, we are given a parallel training corpus con-sisting of S sentence pairs {(fs, es): s = 1, .
.
.
, S}.
For each sentence pair (fs, es), thealignment variable is denoted by a = aJ1.
The unknown parameters ?
are determinedby maximizing the likelihood on the parallel training corpus:??
= argmax?{S?s=1[?ap?
(fs, a | es)]}(7)This optimization can be performed using the expectation maximization (EM) algo-rithm (Dempster, Laird, and Rubin 1977).
For a given sentence pair there are a largenumber of alignments.
The alignment a?J1 that has the highest probability (under acertain model) is also called the Viterbi alignment (of that model):a?J1 = argmaxaJ1p??
(fJ1, aJ1 | eI1) (8)A detailed comparison of the quality of these Viterbi alignments for various statisticalalignment models compared to human-made word alignments can be found in Ochand Ney (2003).3.2 SymmetrizationThe baseline alignment model does not allow a source word to be aligned with twoor more target words.
Therefore, lexical correspondences like the German compoundword Zahnarzttermin for dentist?s appointment cause problems because a single sourceword must be mapped onto two or more target words.
Therefore, the resulting Viterbialignment of the standard alignment models has a systematic loss in recall.
Here, we422Computational Linguistics Volume 30, Number 4Figure 2Example of a (symmetrized) word alignment (Verbmobil task).describe various methods for performing a symmetrization of our directed statisticalalignment models by applying a heuristic postprocessing step that combines the align-ments in both translation directions (source to target, target to source).
Figure 2 showsan example of a symmetrized alignment.To solve this problem, we train in both translation directions.
For each sentencepair, we compute two Viterbi alignments aJ1 and bI1.
Let A1 = {(aj, j) | aj > 0} andA2 = {(i, bi) | bi > 0} denote the sets of alignments in the two Viterbi alignments.
Toincrease the quality of the alignments, we can combine (symmetrize) A1 and A2 intoone alignment matrix A using one of the following combination methods:?
Intersection: A = A1 ?
A2.?
Union: A = A1 ?
A2.?
Refined method: In a first step, the intersection A = A1 ?
A2 isdetermined.
The elements of this intersection result from both Viterbialignments and are therefore very reliable.
Then, we extend thealignment A iteratively by adding alignments (i, j) occurring only in the423Och and Ney The Alignment Template Approach to Statistical Machine Translationalignment A1 or in the alignment A2 if neither fj nor ei have an alignmentin A, or if the following conditions both hold:?
The alignment (i, j) has a horizontal neighbor (i ?
1, j), (i + 1, j)or a vertical neighbor (i, j ?
1), (i, j + 1) that is already in A.?
The set A ?
{(i, j)} does not contain alignments with bothhorizontal and vertical neighbors.Obviously, the intersection yields an alignment consisting of only one-to-one align-ments with a higher precision and a lower recall.
The union yields a higher recalland a lower precision of the combined alignment.
The refined alignment method isoften able to improve precision and recall compared to the nonsymmetrized align-ments.
Whether a higher precision or a higher recall is preferred depends on the finalapplication of the word alignment.
For the purpose of statistical MT, it seems that ahigher recall is more important.
Therefore, we use the union or the refined combinationmethod to obtain a symmetrized alignment matrix.The resulting symmetrized alignments are then used to train single-word-basedtranslation lexica p(e | f ) by computing relative frequencies using the count N(e, f ) ofhow many times e and f are aligned divided by the count N(f ) of how many timesthe word f occurs:p(e | f ) = N(e, f )N(f )3.3 Bilingual Contiguous PhrasesIn this section, we present a method for learning relationships between whole phrasesof m source language words and n target language words.
This algorithm, whichwill be called phrase-extract, takes as input a general word alignment matrix (Sec-tion 3.2).
The output is a set of bilingual phrases.In the following, we describe the criterion that defines the set of phrases that isconsistent with the word alignment matrix:BP(f J1, eI1, A) ={(f j+mj , ei+ni): ?
(i?, j?)
?
A : j ?
j?
?
j + m ?
i ?
i?
?
i + n (9)??
(i?, j?)
?
A : j ?
j?
?
j + m ?
i ?
i?
?
i + n}Hence, the set of all bilingual phrases that are consistent with the alignment is con-stituted by all bilingual phrase pairs in which all words within the source languagephrase are aligned only with the words of the target language phrase and the wordsof the target language phrase are aligned only with the words of the source languagephrase.
Note that we require that at least one word in the source language phrase bealigned with at least one word of the target language phrase.
As a result there are noempty source or target language phrases that would correspond to the ?empty word?of the word-based statistical alignment models.These phrases can be computed straightforwardly by enumerating all possiblephrases in one language and checking whether the aligned words in the other lan-guage are consecutive, with the possible exception of words that are not aligned atall.
Figure 3 gives the algorithm phrase-extract that computes the phrases.
The algo-rithm takes into account possibly unaligned words at the boundaries of the source ortarget language phrases.
Table 1 shows the bilingual phrases containing between twoand seven words that result from the application of this algorithm to the alignmentof Figure 2.424Computational Linguistics Volume 30, Number 4Table 1Examples of two- to seven-word bilingual phrases obtained by applying the algorithmphrase-extract to the alignment of Figure 2.ja , yes ,ja , ich yes , Ija , ich denke mal yes , I thinkja , ich denke mal , yes , I think ,ja , ich denke mal , also yes , I think , well, ich , I, ich denke mal , I think, ich denke mal , , I think ,, ich denke mal , also , I think , well, ich denke mal , also wir , I think , well weich denke mal I thinkich denke mal , I think ,ich denke mal , also I think , wellich denke mal , also wir I think , well weich denke mal , also wir wollten I think , well we plan todenke mal , think ,denke mal , also think , welldenke mal , also wir think , well wedenke mal , also wir wollten think , well we plan to, also , well, also wir , well we, also wir wollten , well we plan toalso wir well wealso wir wollten well we plan towir wollten we plan toin unserer in ourin unserer Abteilung in our departmentin unserer Abteilung ein neues Netzwerk a new network in our departmentin unserer Abteilung ein neues Netzwerk set up a new network in our departmentaufbauenunserer Abteilung our departmentein neues a newein neues Netzwerk a new networkein neues Netzwerk aufbauen set up a new networkneues Netzwerk new networkIt should be emphasized that this constraint to consecutive phrases limits the ex-pressive power.
If a consecutive phrase in one language is translated into two or threenonconsecutive phrases in the other language, there is no corresponding bilingualphrase pair learned by this approach.
In principle, this approach to learning phrasesfrom a word-aligned corpus could be extended straightforwardly to handle noncon-secutive phrases in source and target language as well.
Informal experiments haveshown that allowing for nonconsecutive phrases significantly increases the number ofextracted phrases and especially increases the percentage of wrong phrases.
Therefore,we consider only consecutive phrases.3.4 Alignment TemplatesIn the following, we add generalization capability to the bilingual phrase lexicon byreplacing words with word classes and also by storing the alignment informationfor each phrase pair.
These generalized and alignment-annotated phrase pairs arecalled alignment templates.
Formally, an alignment template z is a triple (FJ?1 , EI?1 , A?
)425Och and Ney The Alignment Template Approach to Statistical Machine TranslationINPUT: eI1, fJ1, Ai1 := 1WHILE i1 ?
Ii2 := i1WHILE i2 ?
ITP := {j|?i : i1 ?
i ?
i2 ?
A(i, j)}IF quasi-consecutive(TP)THEN j1 := min(TP)j2 := max(TP)SP := {i|?j : j1 ?
j ?
j2 ?
A(i, j)}IF SP ?
{i1, i1 + 1, .
.
.
, i2}THEN BP := BP ?
{(ei2i1 , fj2j1)}WHILE j1 > 0 ?
?i : A(i, j1) = 0j??
:= j2WHILE j??
?
J ?
?i : A(i, j??)
= 0BP := BP ?
{(ei2i1 , fj??j1)}j??
:= j??
+ 1j1 := j1 ?
1OUTPUT: BPFigure 3Algorithm phrase-extract for extracting phrases from a word-aligned sentence pair.
Herequasi-consecutive(TP) is a predicate that tests whether the set of words TP is consecutive,with the possible exception of words that are not aligned.that describes the alignment A?
between a source class sequence FJ?1 and a target classsequence EI?1 .
If each word corresponds to one class, an alignment template correspondsto a bilingual phrase together with an alignment within this phrase.
Figure 4 showsexamples of alignment templates.The alignment A?
is represented as a matrix with J?
?
(I?
+ 1) binary elements.
Amatrix element with value 1 means that the words at the corresponding positions arealigned, and the value 0 means that the words are not aligned.
If a source word is notaligned with a target word, then it is aligned with the empty word e0, which is at theimaginary position i = 0.The classes used in FJ?1 and EI?1 are automatically trained bilingual classes usingthe method described in Och (1999) and constitute a partition of the vocabulary ofsource and target language.
In general, we are not limited to disjoint classes as longas each specific instance of a word is disambiguated, that is, uniquely belongs to aspecific class.
In the following, we use the class function C to map words to theirclasses.
Hence, it would be possible to employ parts-of-speech or semantic categoriesinstead of the automatically trained word classes used here.The use of classes instead of the words themselves has the advantage of bettergeneralization.
For example, if there exist classes in source and target language thatcontain town names, it is possible that an alignment template learned using a specifictown name can be generalized to other town names.In the following, e?
and f?
denote target and source phrases, respectively.
To trainthe probability of applying an alignment template p(z = (FJ?1 , EI?1 , A?)
| f?
), we use anextended version of the algorithm phrase-extract from Section 3.3.
All bilingualphrases that are consistent with the alignment are extracted together with the align-426Computational Linguistics Volume 30, Number 4Figure 4Examples of alignment templates obtained in training.ment within this bilingual phrase.
Thus, we obtain a count N(z) of how often analignment template occurred in the aligned training corpus.
The probability of usingan alignment template to translate a specific source language phrase f?
is estimated bymeans of relative frequency:p(z = (FJ?1 , EI?1 , A?)
| f? )
=N(z) ?
?
(FJ?1 , C(f?
))N(C(f?
))(10)To reduce the memory requirement of the alignment templates, we compute theseprobabilities only for phrases up to a certain maximal length in the source language.427Och and Ney The Alignment Template Approach to Statistical Machine TranslationDepending on the size of the corpus, the maximal length in the experiments is be-tween four and seven words.
In addition, we remove alignment templates that havea probability lower than a certain threshold.
In the experiments, we use a thresholdof 0.01.It should be emphasized that this algorithm for computing aligned phrase pairsand their associated probabilities is very easy to implement.
The joint translation modelsuggested by Marcu and Wong (2002) tries to learn phrases as part of a full EMalgorithm, which leads to very large memory requirements and a rather complicatedtraining algorithm.
A comparison of the two approaches can be found in Koehn, Och,and Marcu (2003).4.
Translation ModelTo describe our translation model based on the alignment templates described in theprevious section in a formal way, we first decompose both the source sentence f J1 andthe target sentence eI1 into a sequence of phrases (k = 1, .
.
.
, K):f J1 = f?K1 , f?k = fjk?1+1, .
.
.
, fjk (11)eI1 = e?K1 , e?k = eik?1+1, .
.
.
, eik (12)Note that there are a large number of possible segmentations of a sentence pair into Kphrase pairs.
In the following, we will describe the model for a specific segmentation.Eventually, however, a model can be described in which the specific segmentation isnot known when new text is translated.
Hence, as part of the overall search process(Section 5), we also search for the optimal segmentation.To allow possible reordering of phrases, we introduce an alignment on the phraselevel ?K1 between the source phrases f?K1 and the target phrases e?K1 .
Hence, ?K1 is apermutation of the phrase positions 1, .
.
.
, K and indicates that the phrases e?k andf?
?k are translations of one another.
We assume that for the translation between thesephrases a specific alignment template zk is used:e?kzk??
f?
?kHence, our model has the following hidden variables:?K1 , zK1Figure 5 gives an example of the word alignment and phrase alignment of aGerman?English sentence pair.We describe our model using a log-linear modeling approach.
Hence, all knowl-edge sources are described as feature functions that include the given source languagestring f J1, the target language string eI1, and the above-stated hidden variables.
Hence,we have the following functional form of all feature functions:h(eI1, fJ1, ?K1 , zK1 )Figure 6 gives an overview of the decisions made in the alignment template model.First, the source sentence words f J1 are grouped into phrases f?K1 .
For each phrase f?
analignment template z is chosen and the sequence of chosen alignment templates isreordered (according to ?K1 ).
Then, every phrase f?
produces its translation e?
(using thecorresponding alignment template z).
Finally, the sequence of phrases e?K1 constitutesthe sequence of words eI1.428Computational Linguistics Volume 30, Number 4Figure 5Example of segmentation of German sentence and its English translation into alignmenttemplates.Figure 6Dependencies in the alignment template model.429Och and Ney The Alignment Template Approach to Statistical Machine Translation4.1 Feature Functions4.1.1 Alignment Template Selection.
To score the use of an alignment template, weuse the probability p(z | f? )
defined in Section 3.
We establish a corresponding featurefunction by multiplying the probability of all used alignment templates and taking thelogarithm:hAT(eI1, fJ1, ?K1 , zK1 ) = logK?k=1p(zk | fj?kj?k?1+1) (13)Here, j?k?1 + 1 is the position of the first word of alignment template zk in the sourcelanguage sentence and j?k is the position of the last word of that alignment template.Note that this feature function requires that a translation of a new sentence becomposed of a set of alignment templates that covers both the source sentence andthe produced translation.
There is no notion of ?empty phrase?
that corresponds tothe ?empty word?
in word-based statistical alignment models.
The alignment on thephrase level is actually a permutation, and no insertions or deletions are allowed.4.1.2 Word Selection.
For scoring the use of target language words, we use a lexiconprobability p(e | f ), which is estimated using relative frequencies as described in Sec-tion 3.2.
The target word e depends on the aligned source words.
If we denote theresulting word alignment matrix by A := A?K1 ,zK1 and the predicted word class for wordei by Ei, then the feature function hWRD is defined as follows:hWRD(eI1, fJ1, ?K1 , zK1 ) = logI?i=1p(ei | {fj | (i, j) ?
A}, Ei) (14)For p(ei | {fj | (i, j) ?
A}) we use a uniform mixture of a single-word model p(e | f ),which is constrained to predict only words that are in the predicted word class Ei:p(ei | {fj | (i, j) ?
A}, Ei) =?
{j|(i,j)?A} p(ei | fj)|{j | (i, j) ?
A}| ?
?
(C(ei), Ei)A disadvantage of this model is that the word order is ignored in the translationmodel.
The translations the day after tomorrow or after the day tomorrow for the Germanword u?bermorgen receive an identical probability.
Yet the first one should obtain asignificantly higher probability.
Hence, we also include a dependence on the wordpositions in the lexicon model p(e | f , i, j):p(ei | fj,i?1?i?=1[(i?, j) ?
A],j?1?j?=1[(i, j?)
?
A]) (15)Here, [(i?, j) ?
A] is 1 if (i?, j) ?
A and 0 otherwise.
As a result, the word ei dependsnot only on the aligned French word fj, but also on the number of preceding Frenchwords aligned with ei and on the number of the preceding English words aligned withfj.
This model distinguishes the positions within a phrasal translation.
The number ofparameters of p(e | f , i, j) is significantly higher than that of p(e | f ) alone.
Hence,there is a data estimation problem especially for words that rarely occur.
Therefore,we linearly interpolate the models p(e | f ) and p(e | f , i, j).4.1.3 Phrase Alignment.
The phrase alignment feature simply takes into account thatvery often a monotone alignment is a correct alignment.
Hence, the feature functionhAL measures the ?amount of nonmonotonicity?
by summing over the distance (in the430Computational Linguistics Volume 30, Number 4source language) of alignment templates that are consecutive in the target language:hAL(eI1, fJ1, ?K1 , zK1 ) =K+1?k=1|j?k?1 ?
j?k?1 | (16)Here, j?0 is defined to equal 0 and j?K+1?1 is defined to equal J.
The above-stated sumincludes k = K + 1 to include the distance from the end position of the last phrase tothe end of sentence.The sequence of K = 6 alignment templates in Figure 5 corresponds to the follow-ing sum of seven jump distances: 0 + 0 + 1 + 3 + 2 + 0 + 0 = 6.4.1.4 Language Model Features.
As a default language model feature, we use a stan-dard backing-off word-based trigram language model (Ney, Generet, and Wessel 1995):hLM(eI1, fJ1, ?K1 , zK1 ) = logI+1?i=1p(ei | ei?2, ei?1) (17)In addition, we use a 5-gram class-based language model:hCLM(eI1, fJ1, ?K1 , zK1 ) = logI+1?i=1p(C(ei) | C(ei?4), .
.
.
, C(ei?1)) (18)The use of the language model feature in equation (18) helps take long-range depen-dencies better into account.4.1.5 Word Penalty.
To improve the scoring for different target sentence lengths, wealso use as a feature the number of produced target language words (i.e., the lengthof the produced target language sentence):hWP(eI1, fJ1, ?K1 , zK1 ) = I (19)Without this feature, we typically observe that the produced sentences tend to be tooshort.4.1.6 Conventional Lexicon.
We also use a feature that counts how many entries of aconventional lexicon co-occur in the given sentence pair.
Therefore, the weight for theprovided conventional dictionary can be learned:hLEX(eI1, fJ1, ?K1 , zK1 ) = #CO-OCCURRENCES(LEX, eI1, fJ1) (20)The intuition is that the conventional dictionary LEX is more reliable than the auto-matically trained lexicon and therefore should get a larger weight.4.1.7 Additional Features.
A major advantage of the log-linear modeling approachused is that we can add numerous features that deal with specific problems of thebaseline statistical MT system.
Here, we will restrict ourselves to the described setof features.
Yet we could use grammatical features that relate certain grammaticaldependencies of source and target language.
For example, using a function k(?)
thatcounts how many arguments the main verb of a sentence has in the source or targetsentence, we can define the following feature, which has a nonzero value if the verbin each of the two sentences has the same number of arguments:h(f J1, eI1, ?K1 , zK1 ) = ?
(k(fJ1), k(eI1)) (21)In the same way, we can introduce semantic features or pragmatic features such asthe dialogue act classification.431Och and Ney The Alignment Template Approach to Statistical Machine Translation4.2 TrainingFor the three different tasks on which we report results, we use two different trainingapproaches.
For the Verbmobil task, we train the model parameters ?M1 accordingto the maximum class posterior probability criterion (equation (4)).
For the French?English Hansards task and the Chinese?English NIST task, we simply tune the modelparameters by coordinate descent on held-out data with respect to the automatic eval-uation metric employed, using as a starting point the model parameters obtained onthe Verbmobil task.
Note that this tuning depends on the starting point of the modelparameters and is not guaranteed to converge to the global optimum on the trainingdata.
As a result, this approach is limited to a very small number of model parame-ters.
An efficient algorithm for performing this tuning for a larger number of modelparameters can be found in Och (2003).A standard approach to training the log-linear model parameters of the maximumclass posterior probability criterion is the GIS (Generalized Iterative Scaling) algorithm(Darroch and Ratcliff 1972).
To apply this algorithm, we have to solve various practi-cal problems.
The renormalization needed in equation (3) requires a sum over manypossible sentences, for which we do not know of an efficient algorithm.
Hence, we ap-proximate this sum by extracting a large set of highly probable sentences as a samplefrom the space of all possible sentences (n-best approximation).
The set of consideredsentences is computed by means of an appropriately extended version of the searchalgorithm described in Section 5.Using an n-best approximation, we might face the problem that the parameterstrained with the GIS algorithm yield worse translation results even on the trainingcorpus.
This can happen because with the modified model scaling factors, the n-bestlist can change significantly and can include sentences that have not been taken intoaccount in training.
Using these sentences, the new model parameters might performworse than the old model parameters.
To avoid this problem, we proceed as follows.In a first step, we perform a search, compute an n-best list, and use this n-best list totrain the model parameters.
Second, we use the new model parameters in a new searchand compute a new n-best list, which is combined with the existing n-best list.
Third,using this extended n-best list, new model parameters are computed.
This process isiterated until the resulting n-best list does not change.
In this algorithm, convergenceis guaranteed, as in the limit the n-best list will contain all possible translations.
Inpractice, the algorithm converges after five to seven iterations.
In our experiments thisfinal n-best list contains about 500?1000 alternative translations.We might have the problem that none of the given reference translations is partof the n-best list because the n-best list is too small or because the search algorithmperforms pruning which in principle limits the possible translations that can be pro-duced given a certain input sentence.
To solve this problem, we define as referencetranslation for maximum-entropy training each sentence that has the minimal numberof word errors with respect to any of the reference translations in the n-best list.
Moredetails of the training procedure can be found in Och and Ney (2002).5.
SearchIn this section, we describe an efficient search architecture for the alignment templatemodel.5.1 General ConceptIn general, the search problem for statistical MT even using only Model 1 of Brownet al (1993) is NP-complete (Knight 1999).
Therefore, we cannot expect to develop432Computational Linguistics Volume 30, Number 4efficient search algorithms that are guaranteed to solve the problem without searcherrors.
Yet for practical applications it is acceptable to commit some search errors(Section 6.1.2).
Hence, the art of developing a search algorithm lies in finding suitableapproximations and heuristics that allow an efficient search without committing toomany search errors.In the development of the search algorithm described in this section, our mainaim is that the search algorithm should be efficient.
It should be possible to translatea sentence of reasonable length within a few seconds of computing time.
We acceptthat the search algorithm sometimes results in search errors, as long as the impacton translation quality is minor.
Yet it should be possible to reduce the number ofsearch errors by increasing computing time.
In the limit, it should be possible tosearch without search errors.
The search algorithm should not impose any principallimitations.
We also expect that the search algorithm be able to scale up to very longsentences with an acceptable computing time.To meet these aims, it is necessary to have a mechanism that restricts the searcheffort.
We accomplish such a restriction by searching in a breadth-first manner withpruning: beam search.
In pruning, we constrain the set of considered translation can-didates (the ?beam?)
only to the promising ones.
We compare in beam search thosehypotheses that cover different parts of the input sentence.
This makes the comparisonof the probabilities problematic.
Therefore, we integrate an admissible estimation ofthe remaining probabilities to arrive at a complete translation (Section 5.6)Many of the other search approaches suggested in the literature do not meet thedescribed aims:?
Neither optimal A* search (Och, Ueffing, and Ney 2001) nor optimalinteger programming (Germann et al 2001) for statistical MT allowsefficient search for long sentences.?
Greedy search algorithms (Wang 1998; Germann et al 2001) typicallycommit severe search errors (Germann et al 2001).?
Other approaches to solving the search problem obtain polynomial timealgorithms by assuming monotone alignments (Tillmann et al 1997) orimposing a simplified recombination structure (Nie?en et al 1998).Others make simplifying assumptions about the search space(Garc?
?a-Varea, Casacuberta, and Ney 1998; Garc?
?a-Varea et al 2001), asdoes the original IBM stack search decoder (Berger et al 1994).
All thesesimplifications ultimately make the search problem simpler butintroduce fundamental search errors.In the following, we describe our search algorithm based on the concept of beamsearch, which allows a trade-off between efficiency and quality by adjusting the size ofthe beam.
The search algorithm can be easily adapted to other phrase-based translationmodels.
For single-word-based search in MT, a similar algorithm has been describedin Tillmann and Ney (2003).5.2 Search ProblemPutting everything together and performing search in maximum approximation, weobtain the following decision rule:e?I1 = argmaxeI1,?K1 ,zK1{M?m=1?m ?
hm(eI1, fJ1, ?K1 , zK1 )}(22)433Och and Ney The Alignment Template Approach to Statistical Machine TranslationUsing the four feature functions AT, AL, WRD, and LM, we obtain the followingdecision rule:3e?I1 = argmaxeI1,?K1 ,zK1{(23)I?i=1(?LM log p(ei | ei?2, ei?1) + ?WRD log p(ei | {fj | (i, j) ?
A}, Ei))(24)+K?k=1(?AT log p(zk | fj?kj?k?1+1) + ?AL ?
|j?k ?
j?k?1+1|)(25)+?AL ?
(J ?
j?K) + ?LM log p(EOS | eI?1, eI)}(26)Here, we have grouped the contributions of the various feature functions into those foreach word (from LM and WRD, expression (24)), those for every alignment template(from AT and AL, expression (25)), and those for the end of sentence (expression (26)),which includes a term log p(EOS | eI?1, eI) for the end-of-sentence language modelprobability.To extend this decision rule for the word penalty (WP) feature function, we sim-ply obtain an additional term ?WP for each word.
The class-based 5-gram languagemodel (CLM) can be included like the trigram language model.
Note that all these fea-ture functions decompose nicely into contributions for each produced target languageword or for each covered source language word.
This makes it possible to developan efficient dynamic programming search algorithm.
Not all feature functions havethis nice property: For the conventional lexicon feature function (LEX), we obtain anadditional term in our decision rule which depends on the full sentence.
Therefore,this feature function will not be integrated in the dynamic programming search butinstead will be used to rerank the set of candidate translations produced by the search.5.3 Structure of Search SpaceWe have to structure the search space in a suitable way to search efficiently.
In oursearch algorithm, we generate search hypotheses that correspond to prefixes of targetlanguage sentences.
Each hypothesis is the translation of a part of the source languagesentence.
A hypothesis is extended by appending one target word.
The set of all hy-potheses can be structured as a graph with a source node representing the sentencestart, goal nodes representing complete translations, and intermediate nodes repre-senting partial translations.
There is a directed edge between hypotheses n1 and n2 ifthe hypothesis n2 is obtained by appending one word to hypothesis n1.
Each edge hasassociated costs resulting from the contributions of all feature functions.
Finally, oursearch problem can be reformulated as finding the optimal path through this graph.In the first step, we determine the set of all source phrases in f?
for which an appli-cable alignment template exists.
Every possible application of an alignment templatez = (FJ?1 , EI?1 , A?)
to a subsequence fj+J?
?1j of the source sentence is called an alignmenttemplate instantiation Z = (z, j).
Hence, the set of all alignment template instantiationsfor the source sentence f J1 is{Z = (z, j) | z = (FJ?1 , EI?1 , A?)
?
?j : p(z | fj+J?
?1j ) > 0}(27)3 Note that here some of the simplifying notation of Section 4 has been used.434Computational Linguistics Volume 30, Number 4If the source sentence contains words that have not been seen in the training data, weintroduce a new alignment template that performs a one-to-one translation of each ofthese words by itself.In the second step, we determine a set of probable target language words foreach target word position in the alignment template instantiation.
Only these wordsare then hypothesized in the search.
We call this selection of highly probable wordsobservation pruning (Tillmann and Ney 2000).
As a criterion for a word e at positioni in the alignment template instantiation, we use?
(Ei, C(e)) ?J??j=0A?
(i, j)?i?
A?
(i?, j)?
p(e | fj) (28)In our experiments, we hypothesize only the five best-scoring words.A decision is a triple d = (Z, e, l) consisting of an alignment template instantiationZ, the generated word e, and the index l of the generated word in Z.
A hypothesis ncorresponds to a valid sequence of decisions di1.
The possible decisions are as follows:1.
Start a new alignment template: di = (Zi, ei, 1).
In this case, the indexl = 1.
This decision can be made only if the previous decision di?1finished an alignment template and if the newly chosen alignmenttemplate instantiation does not overlap with any previously chosenalignment template instantiation.
The resulting decision scorecorresponds to the contribution of the LM and the WRD features(expression (24)) for the produced word and the contribution of AL andAT features (expression (25)) for the started alignment template.2.
Extend an alignment template: di = (Zi, ei, l).
This decision can be madeonly if the previous decision uses the same alignment templateinstantiation and has as index l ?
1: di?1 = (Zi, ei?1, l ?
1).
The resultingdecision score corresponds to the contribution of the LM and the WRDfeatures (expression (24)).3.
Finish the translation of a sentence: di = (EOS, EOS, 0).
In this case, thehypothesis is marked as a goal hypothesis.
This decision is possible onlyif the previous decision di?1 finished an alignment template and if thealignment template instantiations completely cover the input sentence.The resulting decision score corresponds to the contribution ofexpression (26).Any valid and complete sequence of decisions dI+11 uniquely corresponds to a certaintranslation eI1, a segmentation into K phrases, a phrase alignment ?K1 , and a sequenceof alignment template instantiations zK1 .
The sum of the decision scores is equal to thecorresponding score described in expressions (24)?
(26).A straightforward representation of all hypotheses would be the prefix tree of allpossible sequences of decisions.
Obviously, there would be a large redundancy in thissearch space representation, because there are many search nodes that are indistin-guishable in the sense that the subtrees following these search nodes are identical.
Wecan recombine these identical search nodes; that is, we have to maintain only the mostprobable hypothesis (Bellman 1957).In general, the criterion for recombining a set of nodes is that the hypotheses can bedistinguished by neither language nor translation model.
In performing recombination,435Och and Ney The Alignment Template Approach to Statistical Machine TranslationINPUT: implicitly defined search space (functions Recombine, Extend)H = {initial-hypothesis}WHILE H = ?Hext := ?FOR n ?
HIF hypothesis n is finalTHEN Hfin := Hfin ?
{n}ELSE Hext := Hext ?
Extend(n)H := Recombine(Hext)Q?
= maxn?H Q(n)H := {n ?
H : Q(n) > log(tp) + Q?
}H := HistogramPruning(H, Np)n?
= argmaxn?HfinQ(n)OUTPUT: n?Figure 7Algorithm for breadth-first search with pruning.we obtain a search graph instead of a search tree.
The exact criterion for performingrecombination for the alignment templates is described in Section 5.5.5.4 Search AlgorithmTheoretically, we could use any graph search algorithm to search the optimal path inthe search space.
We use a breadth-first search algorithm with pruning.
This approachoffers very good possibilities for adjusting the trade-off between quality and efficiency.In pruning, we always compare hypotheses that have produced the same number oftarget words.Figure 7 shows a structogram of the algorithm.
As the search space increases expo-nentially, it is not possible to explicitly represent it.
Therefore, we represent the searchspace implicitly, using the functions Extend and Recombine.
The function Extend pro-duces new hypotheses extending the current hypothesis by one word.
Some hypothe-ses might be identical or indistinguishable by the language and translation models.These are recombined by the function Recombine.
We expand the search space suchthat only hypotheses with the same number of target language words are recombined.In the pruning step, we use two different types of pruning.
First, we perform prun-ing relative to the score Q?
of the current best hypothesis.
We ignore all hypotheses thathave a probability lower than log(tp)+Q?, where tp is an adjustable pruning parameter.This type of pruning can be performed when the hypothesis extensions are computed.Second, in histogram pruning (Steinbiss, Tran, and Ney 1994), we maintain only thebest Np hypotheses.
The two pruning parameters tp and Np have to be optimized withrespect to the trade-off between efficiency and quality.5.5 ImplementationIn this section, we describe various issues involved in performing an efficient imple-mentation of a search algorithm for the alignment template approach.A very important design decision in the implementation is the representation ofa hypothesis.
Theoretically, it would be possible to represent search hypotheses onlyby the associated decision and a back-pointer to the previous hypothesis.
Yet thiswould be a very inefficient representation for the implementation of the operations436Computational Linguistics Volume 30, Number 4that have to be performed in the search.
The hypothesis representation should containall information required to perform efficiently the computations needed in the searchbut should contain no more information than that, to keep the memory consumptionsmall.In search, we produce hypotheses n, each of which contains the following infor-mation:1. e: the final target word produced2.
h: the state of the language model (to predict the following word)3. c = cJ1: the coverage vector representing the already covered positions ofthe source sentence (cj = 1 means the position j is covered, cj = 0 meansthe position j is not covered)4.
Z: a reference to the alignment template instantiation that produced thefinal target word5.
l: the position of the final target word in the alignment templateinstantiation6.
Q(n): the accumulated score of all previous decisions7.
n?
: a reference to the previous hypothesisUsing this representation, we can perform the following operations very efficiently:?
Determining whether a specific alignment template instantiation can beused to extend a hypothesis.
To do this, we check whether the positionsof the alignment template instantiation are still free in the hypothesiscoverage vector.?
Checking whether a hypothesis is final.
To do this, we determinewhether the coverage vector contains no uncovered position.
Using a bitvector as representation, the operation to check whether a hypothesis isfinal can be implemented very efficiently.?
Checking whether two hypotheses can be recombined.
The criterion forrecombining two hypotheses n1 = (e1, h1, c1, Z1, l1) andn2 = (e2, h2, c2, Z2, l2) ish1 = h2?
identical language model statec1 = c2?
identical coverage vector( (Z1 = Z2 ?
l1 = l2)?
alignment template instantiation is identical(J(Z1) = l1 ?
J(Z2) = l2) ) alignment template instantiation finishedWe compare in beam search those hypotheses that cover different parts of the inputsentence.
This makes the comparison of the probabilities problematic.
Therefore, weintegrate an admissible estimation of the remaining probabilities to arrive at a completetranslation.
Details of the heuristic function for the alignment templates are providedin the next section.5.6 Heuristic FunctionTo improve the comparability of search hypotheses, we introduce heuristic functions.A heuristic function estimates the probabilities of reaching the goal node from a certain437Och and Ney The Alignment Template Approach to Statistical Machine Translationsearch node.
An admissible heuristic function is always an optimistic estimate; thatis, for each search node, the product of edge probabilities of reaching a goal nodeis always equal to or smaller than the estimated probability.
For an A*-based searchalgorithm, a good heuristic function is crucial to being able to translate long sentences.For a beam search algorithm, the heuristic function has a different motivation.
It is usedto improve the scoring of search hypotheses.
The goal is to make the probabilities ofall hypotheses more comparable, in order to minimize the chance that the hypothesisleading to the optimal translation is pruned away.Heuristic functions for search in statistical MT have been used in Wang and Waibel(1997) and Och, Ueffing, and Ney (2001).
Wang and Waibel (1997) have described asimple heuristic function for Model 2 of Brown et al (1993) that was not admissible.Och, Ueffing, and Ney (2001) have described an admissible heuristic function forModel 4 of Brown et al (1993) and an almost-admissible heuristic function that isempirically obtained.We have to keep in mind that a heuristic function is helpful only if the overheadintroduced in computing the heuristic function is more than compensated for by thegain obtained through a better pruning of search hypotheses.
The heuristic functionsdescribed in the following are designed such that their computation can be performedefficiently.The basic idea for developing a heuristic function for an alignment model is that allsource sentence positions that have not been covered so far still have to be translatedto complete the sentence.
If we have an estimation rX(j) of the optimal score fortranslating position j, then the value of the heuristic function RX(n) for a node ncan be inferred by summing over the contribution for every position j that is not inthe coverage vector c(n) (here X denotes different possibilities to choose the heuristicfunction):RX(n) =?j?c(n)rX(j) (29)The situation in the case of the alignment template approach is more complicated, asnot every word is translated alone, but typically the words are translated in context.Therefore, the basic quantity for the heuristic function in the case of the alignmenttemplate approach is a function r(Z) that assigns to every alignment template in-stantiation Z a maximal probability.
Using r(Z), we can induce a position-dependentheuristic function r(j):r(j) := maxZ:j(Z)?j?j(Z)+J(Z)?1r(Z)/J(Z) (30)Here, J(Z) denotes the number of source language words produced by the alignmenttemplate instantiation Z and j(Z) denotes the position of the first source languageword.
It can be easily shown that if r(Z) is admissible, then r(j) is also admissible.
Wehave to show that for all nonoverlapping sequences ZK1 the following holds:K?k=1r(Zk) ?
?j?c(ZK1 )r(j) (31)Here, c(ZK1 ) denotes the set of all positions covered by the sequence of alignmenttemplates ZK1 .
This can be shown easily:K?k=1r(Zk) =K?k=1J(Zk)?j=1r(Zk)/J(Zk) (32)438Computational Linguistics Volume 30, Number 4INPUT: coverage vector cJ1, previously covered position jff = min({j?
| cj?
= 0})mj = |j ?
ff|WHILE ff = (J + 1)fo := min({j?
| j?
> ff ?
cj?
= 1})ff := min({j?
| j?
> fo ?
cj?
= 0 ?
j?
= J + 1})mj := mj + |ff ?
fo|OUTPUT: mjFigure 8Algorithm min-jumps to compute the minimum number of needed jumps D(cJ1, j) to completethe translation.=?j?c(ZK1 )r(Zk(j))/J(Zk(j)) (33)?
?j?c(ZK1 )maxZ:j(Z)?j?j(Z)+J(Z)?1r(Z)/J(Z) (34)Here, k(j) denotes the phrase index k that includes the target language word position j.In the following, we develop various heuristic functions r(Z) of increasing complexity.The simplest realization of a heuristic function r(Z) takes into account only the priorprobability of an alignment template instantiation:RAT(Z = (z, j)) = ?AT ?
log p(z | fj,j+J(z)?1) (35)The lexicon model can be integrated as follows:RWRD(Z) = ?WRD ?j(Z)+J(Z)?1?j?=j(Z)maxelog p(e | fj?)
(36)The language model can be incorporated by considering that for each target wordthere exists an optimal language model probability:pL(e) = maxe?,e?
?p(e | e?, e??)
(37)Here, we assume a trigram language model.
In general, it is necessary to maximizeover all possible different language model histories.
We can also combine the languagemodel and the lexicon model into one heuristic function:RWRD+LM(Z) =j(Z)+J(Z)?1?j?=j(Z)maxe?WRD log(p(e | fj?))
+ ?LM log(pL(e)) (38)To include the phrase alignment probability in the heuristic function, we computethe minimum sum of all jump widths that is needed to complete the translation.
Thissum can be computed efficiently using the algorithm shown in Figure 8.
Then, anadmissible heuristic function for the jump width is obtained byRAL(c, j) = ?AL ?
D(c, j) (39)439Och and Ney The Alignment Template Approach to Statistical Machine TranslationTable 2Statistics for Verbmobil task: training corpus (Train), conventional dictionary (Lex),development corpus (Dev), test corpus (Test) (Words*: words without punctuation marks).No Preprocessing With PreprocessingGerman English German EnglishTrain Sentences 58,073Words 519,523 549,921 522,933 548,874Words* 418,974 453,612 420,919 450,297Singletons 3,453 1,698 3,570 1,763Vocabulary 7,940 4,673 8,102 4,780Lex Entries 12,779Extended vocabulary 11,501 6,867 11,904 7,089Dev Sentences 276Words 3,159 3,438 3,172 3,445Trigram perplexity ?
28.1 ?
26.3Test Sentences 251Words 2,628 2,871 2,640 2,862Trigram perplexity ?
30.5 ?
29.9Combining all the heuristic functions for the various models, we obtain as final heuris-tic function for a search hypothesis nR(n) = RAL(c(n), j(n)) +?j?c(n)(RAT(j) + RWRD+LM(j))(40)6.
Results6.1 Results on the Verbmobil TaskWe present results on the Verbmobil task, which is a speech translation task in thedomain of appointment scheduling, travel planning, and hotel reservation (Wahlster2000).
Table 2 shows the corpus statistics for this task.
We use a training corpus,which is used to train the alignment template model and the language models, adevelopment corpus, which is used to estimate the model scaling factors, and a testcorpus.
On average, 3.32 reference translations for the development corpus and 5.14reference translations for the test corpus are used.A standard vocabulary had been defined for the various speech recognizers usedin Verbmobil.
However, not all words of this vocabulary were observed in the train-ing corpus.
Therefore, the translation vocabulary was extended semiautomatically byadding about 13,000 German?English entries from an online bilingual lexicon avail-able on the Web.
The resulting lexicon contained not only word-word entries, butalso multi-word translations, especially for the large number of German compoundwords.
To counteract the sparseness of the training data, a couple of straightforwardrule-based preprocessing steps were applied before any other type of processing:?
normalization of?
numbers?
time and date phrases?
spelling (e.g., don?t ?
do not)?
splitting of German compound words.440Computational Linguistics Volume 30, Number 4So far, in machine translation research there is no generally accepted criterionfor the evaluation of experimental results.
Therefore, we use various criteria.
In thefollowing experiments, we use:?
WER (word error rate)/mWER (multireference word error rate): TheWER is computed as the minimum number of substitution, insertion,and deletion operations that have to be performed to convert thegenerated sentence into the target sentence.
In the case of themultireference word error rate for each test sentence, not just a singlereference translation is used, as for the WER, but a whole set of referencetranslations.
For each translation hypothesis, the edit distance to themost similar sentence is calculated (Nie?en et al 2000).?
PER (position-independent WER): A shortcoming of the WER is the factthat it requires a perfect word order.
An acceptable sentence can have aword order that is different from that of the target sentence, so the WERmeasure alone could be misleading.
To overcome this problem, weintroduce as an additional measure the position-independent word errorrate.
This measure compares the words in the two sentences, ignoringthe word order.?
BLEU (bilingual evalutation understudy) score: This score measures theprecision of unigrams, bigrams, trigrams, and 4-grams with respect to awhole set of reference translations, with a penalty for too-short sentences(Papineni et al 2001).
Unlike all other evaluation criteria used here,BLEU measures accuracy, that is, the opposite of error rate.
Hence, thelarger BLEU scores, the better.In the following, we analyze the effect of various system components: alignment tem-plate length, search pruning, and language model n-gram size.
A systematic evaluationof the alignment template system comparing it with other translation approaches (e.g.,rule-based) has been performed in the Verbmobil project and is described in Tessioreand von Hahn (2000).
There, the alignment-template-based system achieved a sig-nificantly larger number of ?approximately correct?
translations than the competingtranslation systems (Ney, Och, and Vogel 2001).6.1.1 Effect of Alignment Template Length.
Table 3 shows the effect of constrainingthe maximum length of the alignment templates in the source language.
Typically, itis necessary to restrict the alignment template length to keep memory requirementslow.
We see that using alignment templates with only one or two words in the sourcelanguages results in very bad translation quality.
Yet using alignment templates withlengths as small as three words yields optimal results.6.1.2 Effect of Pruning and Heuristic Function.
In the following, we analyze the effectof beam search pruning and of the heuristic function.
We use the following criteria:?
Number of search errors: A search error occurs when the searchalgorithm misses the most probable translation and produces atranslation which is less probable.
As we typically cannot efficientlycompute the probability of the optimal translation, we cannot efficientlycompute the number of search errors.
Yet we can compute a lowerbound on the number of search errors by comparing the translation441Och and Ney The Alignment Template Approach to Statistical Machine TranslationTable 3Effect of alignment template length on translation quality.AT length PER [%] mWER [%] BLEU [%]1 29.8 39.9 44.62 27.0 33.0 53.63 26.5 30.7 56.14 26.9 31.4 55.75 26.8 31.4 55.76 26.5 30.9 56.07 26.5 30.9 56.1Table 4Effect of pruning parameter tp and heuristic function on search efficiency for direct-translationmodel (Np = 50,000).no heuristic function AT+WRD +LM +ALtime search time search time search time searchtp [s] errors [s] errors [s] errors [s] errors10?2 0.0 194 0.0 174 0.0 150 0.0 9110?4 0.2 97 0.2 57 0.3 40 0.2 1310?6 2.0 61 2.8 21 4.1 11 1.8 310?8 11.9 41 15.0 7 19.9 5 9.5 110?10 45.6 38 50.9 6 65.2 3 32.0 110?12 114.6 34 119.2 5 146.2 2 75.2 0found under specific pruning thresholds with the best translation thatwe have found using very conservative pruning thresholds.?
Average translation time per sentence: Pruning is used to adjust thetrade-off between efficiency and quality.
Hence, we present the averagetime needed to translate one sentence of the test corpus.?
Translation quality (mWER, BLEU): Typically, a sentence can have manydifferent correct translations.
Therefore, a search error does notnecessarily result in poorer translation quality.
It is even possible that asearch error can improve translation quality.
Hence, we analyze the effectof search on translation quality, using the automatic evaluation criteriamWER and BLEU.Tables 4 and 5 show the effect of the pruning parameter tp with the histogrampruning parameter Np = 50,000.
Tables 6 and 7 show the effect of the pruning pa-rameter Np with the pruning parameter tp = 10?12.
In all four tables, we provide theresults for using no heuristic functions and three variants of an increasingly infor-mative heuristic function.
The first is an estimate of the alignment template and thelexicon probability (AT+WRD), the second adds an estimate of the language model(+LM) probability, and the third also adds the alignment probability (+AL).
Theseheuristic functions are described in Section 5.6.Without a heuristic function, even more than a hundred seconds per sentencecannot guarantee search-error-free translation.
We draw the conclusion that a goodheuristic function is very important to obtaining an efficient search algorithm.442Computational Linguistics Volume 30, Number 4Table 5Effect of pruning parameter tp and heuristic function on error rate for direct-translation model(Np = 50,000).error rates [%]no heuristic function AT+WRD +LM +ALtp mWER BLEU mWER BLEU mWER BLEU mWER BLEU10?2 48.9 46.8 44.3 49.4 40.9 51.3 33.6 53.410?4 39.8 50.9 35.0 53.8 32.3 55.0 30.7 55.910?6 37.1 51.3 31.8 55.0 30.9 55.6 30.8 56.010?8 35.7 53.0 31.4 55.7 31.2 55.7 30.9 56.010?10 36.1 52.9 31.3 55.8 31.0 55.9 30.8 56.010?12 35.7 52.9 31.2 55.9 31.0 55.9 30.8 56.0Table 6Effect of pruning parameter Np and heuristic function on search efficiency fordirect-translation model (tp = 10?12).no heuristic function AT+WRD +LM +ALtime search time search time search time searchNp [s] errors [s] errors [s] errors [s] errors1 0.0 237 0.0 238 0.0 238 0.0 23210 0.0 169 0.0 154 0.0 148 0.0 98100 0.3 101 0.3 69 0.3 60 0.2 211,000 2.2 65 2.3 33 2.4 27 2.0 510,000 18.3 40 18.3 10 21.1 5 14.3 150,000 114.6 34 119.2 5 146.2 2 75.2 0Table 7Effect of pruning parameter Np and heuristic function on error rate for direct-translationmodel (tp = 10?12).error rates [%]no heuristic function AT+WRD +LM +ALNp mWER BLEU mWER BLEU mWER BLEU mWER BLEU1 64.4 29.7 61.9 31.7 59.8 32.4 49.4 38.210 46.6 46.9 43.0 49.2 42.0 49.1 34.6 52.3100 41.0 49.8 36.7 52.6 34.8 53.8 31.3 55.61,000 37.8 51.5 33.0 54.5 32.3 55.3 30.6 56.010,000 35.5 53.1 31.4 55.6 30.9 55.6 30.8 56.050,000 35.7 52.9 31.2 55.9 31.0 55.9 30.8 56.0443Och and Ney The Alignment Template Approach to Statistical Machine TranslationTable 8Effect of the length of the language model history (Unigram/Bigram/Trigram: word-based;CLM: class-based 5-gram).Language model type PP PER [%] mWER [%] BLEU [%]Zerogram 4781.0 38.1 45.9 29.0Unigram 203.1 30.2 40.9 37.7Bigram 38.3 26.9 32.9 53.0Trigram 29.9 26.8 31.8 55.2Trigram + CLM ?
26.5 30.9 56.1In addition, the search errors have a more severe effect on the error rates if wedo not use a heuristic function.
If we compare the error rates in Table 7, which corre-spond to about 55 search errors in Table 6, we obtain an mWER of 36.7% (53 searcherrors) using no heuristic function and an mWER of 32.6% (57 search errors) usingthe combined heuristic function.
The reason is that without a heuristic function, oftenthe ?easy?
part of the input sentence is translated first.
This yields severe reorderingerrors.6.1.3 Effect of the Length of the Language Model History.
In this work, we use onlyn-gram-based language models.
Ideally, we would like to take into account long-rangedependencies.
Yet long n-grams are seen rarely and are therefore rarely used on unseendata.
Therefore, we expect that extending the history length will at some point notimprove further translation quality.Table 8 shows the effect of the length of the language model history on translationquality.
We see that the language model perplexity improves from 4,781 for a unigrammodel to 29.9 for a trigram model.
The corresponding translation quality improvesfrom an mWER of 45.9% to an mWER of 31.8%.
The largest effect seems to come fromtaking into account the bigram dependence, which achieves an mWER of 32.9%.
If weperform log-linear interpolation of a trigram model with a class-based 5-gram model,we observe an additional small improvement in translation quality to an mWER of30.9%.6.2 Results on the Hansards taskThe Hansards task involves the proceedings of the Canadian parliament, which arekept by law in both French and English.
About three million parallel sentences of thisbilingual data have been made available by the Linguistic Data Consortium (LDC).Here, we use a subset of the data containing only sentences of up to 30 words.
Table 9shows the training and test corpus statistics.The results for French to English and for English to French are shown in Table 10.Because of memory limitations, the maximum alignment template length has beenrestricted to four words.
We compare here against the single-word-based search forModel 4 described in Tillmann (2001).
We see that the alignment template approachobtains significantly better results than the single-word-based search.6.3 Results on Chinese?EnglishVarious statistical, example-based, and rule-based MT systems for a Chinese?Englishnews domain were evaluated in the NIST 2002 MT evaluation.4 Using the alignment4 Evaluation home page: http://www.nist.gov/speech/tests/mt/mt2001/index.htm.444Computational Linguistics Volume 30, Number 4Table 9Corpus statistics for Hansards task (Words*: words without punctuation marks).French EnglishTraining Sentences 1,470,473Words 24,338,195 22,163,092Words* 22,175,069 20,063,378Vocabulary 100,269 78,332Singletons 40,199 31,319Test Sentences 5,432Words 97,646 88,773Trigram perplexity ?
179.8Table 10Translation results on the Hansards task.French?English English?FrenchTranslation approach WER [%] PER [%] WER [%] PER [%]Alignment templates 61.5 49.2 60.9 47.9Single-word-based: monotone search 65.5 53.0 66.6 56.3Single-word-based: reordering search 64.9 51.4 66.0 54.4Table 11Corpus statistics for Chinese?English corpora?large data track (Words*: words withoutpunctuation marks).No preprocessing With preprocessingChinese English Chinese EnglishTrain Sentences 1,645,631Unique sentences 1,289,890Words 31,175,023 33,044,374 30,849,149 32,511,418Words* 27,091,283 29,212,384 26,828,721 28,806,735Singletons 15,324 24,933 5,336 26,344Vocabulary 67,103 92,488 45,111 85,116Lex Entries 80,977Extended vocabulary 76,182 100,704 54,190 93,350Dev Sentences 993Words 26,361 32,267 25,852 31,607Trigram perplexity ?
237,154 ?
171,922Test Sentences 878Words 24,540 ?
24,144 ?template approach described in this article, we participated in these evaluations.
Theproblem domain is the translation of Chinese news text into English.
Table 11 givesan overview on the training and test data.
The English vocabulary consists of full-form words that have been converted to lowercase letters.
The number of sentenceshas been artificially increased by adding certain parts of the original training materialmore than once to the training corpus, in order to give larger weight to those partsof the training corpus that consist of high-quality aligned Chinese news text and aretherefore expected to be especially helpful for the translation of the test data.445Och and Ney The Alignment Template Approach to Statistical Machine TranslationTable 12Results of Chinese?English NIST MT evaluation, June 2002, large data track (NIST-09 score:larger values are better).System NIST-09 scoreAlignment template approach 7.65Competing research systems 5.03?7.34Best of six commercial off-the-shelf systems 6.08The Chinese language poses special problems because the boundaries of Chinesewords are not marked.
Chinese text is provided as a sequence of characters, and it isunclear which characters have to be grouped together to obtain entities that can beinterpreted as words.
For statistical MT, it would be possible to ignore this fact andtreat the Chinese characters as elementary units and translate them into English.
Yetpreliminary experiments showed that the existing alignment models produce betterresults if the Chinese characters are segmented in a preprocessing step into singlewords.
We use the LDC segmentation tool.5For the English corpus, the following preprocessing steps are applied.
First, thecorpus is tokenized; it is then segmented into sentences, and all uppercase charactersare converted to lowercase.
As the final evaluation criterion does not distinguish case,it is not necessary to deal with the case information.Then, the preprocessed Chinese and English corpora are sentence aligned in whichthe lengths of the source and target sentences are significantly different.
From theresulting corpus, we automatically replace translations.
In addition, only sentenceswith less than 60 words in English and Chinese are used.To improve the translation of Chinese numbers, we use a categorization of Chi-nese number and date expressions.
For the statistical learning, all number and dateexpressions are replaced with one of two generic symbols, $number or $date.
The num-ber and date expressions are subjected to a rule-based translation by simple lexiconlookup.
The translation of the number and date expressions is inserted into the out-put using the alignment information.
For Chinese and English, this categorization isimplemented independently of the other language.To evaluate MT quality on this task, NIST made available the NIST-09 evaluationtool.
This tool provides a modified BLEU score by computing a weighted precision ofn-grams modified by a length penalty for very short translations.
Table 12 shows theresults of the official evaluation performed by NIST in June 2002.
With a score of 7.65,the results obtained were statistically significantly better than any other competingapproach.
Differences in the NIST score larger than 0.12 are statistically significant atthe 95% level.
We conclude that the developed alignment template approach is alsoapplicable to unrelated language pairs such as Chinese?English and that the developedstatistical models indeed seem to be largely language-independent.
Table 13 showsvarious example translations.7.
ConclusionsWe have presented a framework for statistical MT for natural languages which ismore general than the widely used source?channel approach.
It allows a baseline MT5 The LDC segmentation tool is available athttp://morph.ldc.upenn.edu/Projects/Chinese/LDC ch.htm#cseg.446Computational Linguistics Volume 30, Number 4Table 13Example translations for Chinese?English MT.Reference Significant Accomplishment Achieved in the Economic Construction ofthe Fourteen Open Border Cities in ChinaTranslation The opening up of the economy of China?s fourteen City madesignificant achievements in constructionReference Xinhua News Agency, Beijing, Feb. 12?Exciting accomplishment hasbeen achieved in 1995 in the economic construction of China?s fourteenborder cities open to foreigners.Translation Xinhua News Agency, Beijing, February 12?China?s opening up to theoutside world of the 1995 in the fourteen border pleasedto obtain the construction of the economy.Reference Foreign Investment in Jiangsu?s Agriculture on the IncreaseTranslation To increase the operation of foreign investment in Jiangsu agricultureReference According to the data provided today by the Ministry of Foreign Tradeand Economic Cooperation, as of November this year, China hasactually utilized 46.959 billion US dollars of foreign capital, including40.007 billion US dollars of direct investment from foreign businessmen.Translation The external economic and trade cooperation Department todayprovided that this year, the foreign capital actually utilized by China onNovember to US $46.959 billion, including of foreign company directinvestment was US $40.007 billion.Reference According to officials from the Provincial Department of Agricultureand Forestry of Jiangsu, the ?Three-Capital?
ventures approved byagencies within the agricultural system of Jiangsu Province since 1994have numbered more than 500 and have utilized over 700 million USdollars worth of foreign capital, respectively three times and seventimes more than in 1993.Translation Jiangsu Province for the Secretaries said that, from the 1994 years,Jiangsu Province system the approval of the ?three-funded?enterprises, there are more than 500, foreign investment utilizationrate of more than US $700 million, 1993 years before three and seven.Reference The actual amount of foreign capital has also increased more than 30%as compared with the same period last year.Translation The actual amount of foreign investment has increased by morethan 30% compared with the same period last year.Reference Import and Export in Pudong New District Exceeding 9 billion USdollars This YearTranslation Foreign trade imports and exports of this year to the Pudongnew Region exceeds US $9 billionsystem to be extended easily by adding new feature functions.
We have describedthe alignment template approach for statistical machine translation, which uses twodifferent alignment levels: a phrase-level alignment between phrases and a word-level alignment between single words.
As a result the context of words has a greaterinfluence, and the changes in word order from source to target language can be learnedexplicitly.
An advantage of this method is that machine translation is learned fullyautomatically through the use of a bilingual training corpus.
We have shown that thepresented approach is capable of achieving better translation results on various taskscompared to other statistical, example-based, or rule-based translation systems.
Thisis especially interesting, as our system is structured simpler than many competingsystems.447Och and Ney The Alignment Template Approach to Statistical Machine TranslationWe expect that better translation can be achieved by using models that go beyondthe flat phrase segmentation that we perform in our model.
A promising avenue is togradually extend the model to take into account to some extent the recursive structureof natural languages using ideas from Wu and Wong (1998) or Alshawi, Bangalore, andDouglas (2000).
We expect other improvements as well from learning nonconsecutivephrases in source or target language and from better generalization methods for thelearned-phrase pairs.AcknowledgmentsThe work reported here was carried outwhile the first author was with theLehrstuhl fu?r Informatik VI, ComputerScience Department, RWTHAachen?University of Technology.ReferencesAlshawi, Hiyan, Srinivas Bangalore, andShona Douglas.
2000.
Learningdependency translation models ascollections of finite state head transducers.Computational Linguistics, 26(1):45?60.Bellman, Richard.
1957.
DynamicProgramming.
Princeton University Press,Princeton.Berger, Adam L., Peter F. Brown, Stephen A.Della Pietra, Vincent J. Della Pietra,John R. Gillett, John D. Lafferty, HarryPrintz, and Lubos Ures?.
1994.
TheCandide system for machine translation.In Proceedings of the ARPA Workshop onHuman Language Technology, pages157?162, Plainsboro, NJ, March.Berger, Adam L., Stephen A. Della Pietra,and Vincent J. Della Pietra.
1996.
Amaximum entropy approach to naturallanguage processing.
ComputationalLinguistics, 22(1):39?72.Brown, Peter F., J. Cocke, Stephen A. DellaPietra, Vincent J. Della Pietra, FrederickJelinek, John D. Lafferty, Robert L. Mercer,and Paul S. Roossin.
1990.
A statisticalapproach to machine translation.Computational Linguistics, 16(2):79?85.Brown, Peter F., Stephen A. Della Pietra,Vincent J. Della Pietra, and R. L. Mercer.1993.
The mathematics of statisticalmachine translation: Parameterestimation.
Computational Linguistics,19(2):263?311.Charniak, Eugene, Kevin Knight, and KenjiYamada.
2003.
Syntax-based languagemodels for machine translation.
In MTSummit IX, pages 40?46, New Orleans,September.Darroch, J. N. and D. Ratcliff.
1972.Generalized iterative scaling for log-linearmodels.
Annals of Mathematical Statistics,43:1470?1480.Dempster, A. P., N. M. Laird, and D. B.Rubin.
1977.
Maximum likelihood fromincomplete data via the EM algorithm.Journal of the Royal Statistical Society,Series B, 39(1):1?22.Garc?
?a-Varea, Ismael, Francisco Casacuberta,and Hermann Ney.
1998.
An iterative,DP-based search algorithm for statisticalmachine translation.
In Proceedings of theInternational Conference on Spoken LanguageProcessing (ICSLP?98), pages 1235?1238,Sydney, November.Garc?
?a-Varea, Ismael, Franz Josef Och,Hermann Ney, and FranciscoCasacuberta.
2001.
Refined lexicon modelsfor statistical machine translation using amaximum entropy approach.
InProceedings of the 39th Annual Meeting of theAssociation for Computational Linguistics(ACL), pages 204?211, Toulouse, France,July.Germann, Ulrich, Michael Jahr, KevinKnight, Daniel Marcu, and Kenji Yamada.2001.
Fast decoding and optimal decodingfor machine translation.
In Proceedings ofthe 39th Annual Meeting of the Association forComputational Linguistics (ACL), pages228?235, Toulouse, France, July.Gildea, Daniel.
2003.
Loosely tree-basedalignment for machine translation.
InProceedings of the 41st Annual Meeting of theAssociation for Computational Linguistics(ACL), pages 80?87, Sapporo, Japan, July.Knight, Kevin.
1999.
Decoding complexityin word-replacement translation models.Computational Linguistics, 25(4):607?615.Koehn, Philipp, Franz Josef Och, and DanielMarcu.
2003.
Statistical phrase-basedtranslation.
In Proceedings of the HumanLanguage Technology and North AmericanAssociation for Computational LinguisticsConference (HLT/NAACL), pages 127?133,Edmonton, Alberta.Marcu, Daniel and William Wong.
2002.
Aphrase-based, joint probability model forstatistical machine translation.
InProceedings of the Conference on EmpiricalMethods in Natural Language Processing(EMNLP-2002), pages 133?139,Philadelphia, July.448Computational Linguistics Volume 30, Number 4Ney, Hermann.
1995.
On theprobabilistic-interpretation ofneural-network classifiers anddiscriminative training criteria.
IEEETransactions on Pattern Analysis and MachineIntelligence, 17(2):107?119.Ney, Hermann, Margit Generet, and FrankWessel.
1995.
Extensions of absolutediscounting for language modeling.
InProceedings of the Fourth European Conferenceon Speech Communication and Technology,pages 1245?1248, Madrid, September.Ney, Hermann, Franz Josef Och, andStephan Vogel.
2001.
The RWTH systemfor statistical translation of spokendialogues.
In Proceedings of the ARPAWorkshop on Human Language Technology,San Diego, March.Nie?en, Sonja, Franz Josef Och, GregorLeusch, and Hermann Ney.
2000.
Anevaluation tool for machine translation:Fast evaluation for machine translationresearch.
In Proceedings of the SecondInternational Conference on LanguageResources and Evaluation (LREC), pages39?45, Athens, May.Nie?en, Sonja, Stephan Vogel, HermannNey, and Christoph Tillmann.
1998.
ADP-based search algorithm for statisticalmachine translation.
In COLING-ACL ?98:36th Annual Meeting of the Association forComputational Linguistics and 17thInternational Conference on ComputationalLinguistics, pages 960?967, Montreal,August.Och, Franz Josef.
1999.
An efficient methodfor determining bilingual word classes.
InEACL ?99: Ninth Conference of the EuropeanChapter of the Association for ComputationalLinguistics, pages 71?76, Bergen, Norway,June.Och, Franz Josef.
2003.
Minimum error ratetraining in statistical machine translation.In Proceedings of the 41st Annual Meeting ofthe Association for Computational Linguistics(ACL), pages 160?167, Sapporo, Japan,July.Och, Franz Josef and Hermann Ney.
2002.Discriminative training and maximumentropy models for statistical machinetranslation.
In Proceedings of the 40thAnnual Meeting of the Association forComputational Linguistics (ACL), pages295?302, Philadelphia, July.Och, Franz Josef and Hermann Ney.
2003.
Asystematic comparison of variousstatistical alignment models.Computational Linguistics, 29(1):19?51.Och, Franz Josef, Nicola Ueffing, andHermann Ney.
2001.
An efficient A*search algorithm for statistical machinetranslation.
In Data-Driven MachineTranslation Workshop, pages 55?62,Toulouse, France, July.Papineni, Kishore A., Salim Roukos, andR.
Todd Ward.
1997.
Feature-basedlanguage understanding.
In EuropeanConference on Speech Communication andTechnology, pages 1435?1438, Rhodes,Greece, September.Papineni, Kishore A., Salim Roukos, andR.
Todd Ward.
1998.
Maximum likelihoodand discriminative training of directtranslation models.
In Proceedings of theInternational Conference on Acoustics, Speech,and Signal Processing, pages 189?192,Seattle, May.Papineni, Kishore A., Salim Roukos, ToddWard, and Wei-Jing Zhu.
2001.
Bleu:A method for automatic evaluation ofmachine translation.
Technical ReportRC22176 (W0109-022), IBM ResearchDivision, Thomas J. Watson ResearchCenter, Yorktown Heights, NY.Steinbiss, Volker, Bach-Hiep Tran, andHermann Ney.
1994.
Improvements inbeam search.
In Proceedings of theInternational Conference on Spoken LanguageProcessing (ICSLP?94), pages 2143?2146,Yokohama, Japan, September.Tessiore, Lorenzo and Walther von Hahn.2000.
Functional validation of a machineinterpretation system: Verbmobil.
InWolfgang Wahlster, editor, Verbmobil:Foundations of Speech-to-Speech Translations,pages 611?631.
Springer, Berlin.Tillmann, Christoph.
2001.
Word Re-orderingand Dynamic Programming Based SearchAlgorithms for Statistical MachineTranslation.
Ph.D. thesis, ComputerScience Department, RWTH Aachen,Germany.Tillmann, Christoph.
2003.
A projectionextension algorithm for statisticalmachine translation.
In Michael Collinsand Mark Steedman, editors, Proceedingsof the 2003 Conference on Empirical Methodsin Natural Language Processing, pages 1?8,Sapporo, Japan.Tillmann, Christoph and Hermann Ney.2000.
Word re-ordering and DP-basedsearch in statistical machine translation.In COLING ?00: The 18th InternationalConference on Computational Linguistics,pages 850?856, Saarbru?cken, Germany,July.Tillmann, Christoph and Hermann Ney.2003.
Word reordering and a dynamicprogramming beam search algorithm forstatistical machine translation.Computational Linguistics, 29(1):97?133.449Och and Ney The Alignment Template Approach to Statistical Machine TranslationTillmann, Christoph, Stephan Vogel,Hermann Ney, and Alex Zubiaga.
1997.
ADP-based search using monotonealignments in statistical translation.
InProceedings of the 35th Annual Conference ofthe Association for Computational Linguistics,pages 289?296, Madrid, July.Venugopal, Ashish, Stephan Vogel, andAlex Waibel.
2003.
Effective phrasetranslation extraction from alignmentmodels.
In Proceedings of the 41st AnnualMeeting of the Association for ComputationalLinguistics (ACL), pages 319?326, Sapporo,Japan, July.Vogel, Stephan, Hermann Ney, andChristoph Tillmann.
1996.
HMM-basedword alignment in statistical translation.In COLING ?96: The 16th InternationalConference on Computational Linguistics,pages 836?841, Copenhagen, August.Wahlster, Wolfgang, editor.
2000.
Verbmobil:Foundations of Speech-to-Speech Translations.Springer, Berlin.Wang, Ye-Yi.
1998.
Grammar Inference andStatistical Machine Translation.
Ph.D. thesis,School of Computer Science, LanguageTechnologies Institute, Carnegie MellonUniversity, Pittsburgh.Wang, Ye-Yi and Alex Waibel.
1997.Decoding algorithm in statisticaltranslation.
In Proceedings of the 35thAnnual Conference of the Association forComputational Linguistics, pages 366?372,Madrid, July.Wu, Dekai and William Wong.
1998.Machine translation with a stochasticgrammatical channel.
In COLING-ACL?98: 36th Annual Meeting of the Associationfor Computational Linguistics and 17thInternational Conference on ComputationalLinguistics, pages 1408?1414, Montreal,August.Yamada, Kenji and Kevin Knight.
2001.
Asyntax-based statistical translation model.In Proceedings of the 39th Annual Meeting ofthe Association for Computational Linguistics(ACL), pages 523?530, Toulouse, France,July.
