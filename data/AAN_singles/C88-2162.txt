Language Acquisition:Coping with Lexical GapsUri ZERN1KArtificial Intelligence ProgramGE, Research and Development CenterPO Box 8Schenectady, NY 12301USAAbstractComputer programs so far have not fared well in modelinglanguage acquisition.
For one thing, learning methodology appli-cable in general domains does not readily lend itself in thelinguistic domain.
For another, linguistic representation used bylanguage processing systems is not geared to learning.
We intro-duced a new linguistic representation, the Dynamic HierarchicalPhrasal Lexicon (DHPL) \[Zernik88\], to facilitate language ac-quisition.
From this, a language learning model was implement-ed in the program RINA, which enhances its own lexical hierar-chy by processing examples in context.
We identified two tasks:First, how linguistic concepts are acquired from training exam-ples and organized in a hierarchy; this task was discussed inprevious papers \[Zernik87\].
Second, we show in this paper howa lexical hierarchy is used in predicting new linguistic concepts.Thus, a program does not stall even in the presence of a lexicalunknown, and a hypothesis can be produced for covering thatlexical gap.1.
~TRODUCTIONCoping with unkowns is an integral part of human communica-tion which has been ingnored by previous linguistic models\[Chomsky81, Bresnan82, Gazdar85\].
Consider the followingsentence produced by a second language speaker:John suggested her to go out, but she refused.This incorrect use of suggest could be viewed as a communicationfailure, since by text-book grammar suggest does not take thisform of the infinitive.
Alternatively, this can be viewed as asurprising success.
In spite of missing lexical information, a per-son managed to convey a concept, rather than give up the com-muni.cation task altogether.
Our aim here is to explain suchrobust human performance in computational terms, and conse-quently to describe the principles underlying the program RINA\[Zernik85, Zernik86\] which models language acquisition.1.1 The Model led BehaviorThe problems arising from incomplete lexical knowledge are il-lustrated through the following scenario.
In this scenario RINAencounters two unknown words plead*, and doove, and uses theword suggest whose lexical definition is incomplete.796User:Input text:Corinne ne, eded help with her homework.Her friend Frank called and plended her to come over.But she dooved to stay home.Paraphrased text:RINA: Frank suggested her to come over.Corinne turned own the suggestion.RINA reads a paragraph provided by a user, and then generatestext which conveys the state of her knowledge to the user.
Thefirst problem is presented by the word plead which does not existin RINA's lexicon.
RINA is able to extract partial information:Frank communicated a concept to Corinne regarding comingover.
It is not clear, however, who comes over.
Did Frankpromise Corinne to come over to her, or did Frank ask Corinneto come over to him?The word doove is also unknown.
Here too, RINA can guess themain concept: Corinne decided not to come over.
This hy-pothesis is not necessarily correct.
However, it fits well the con-text and the structure of the sentence.At this point, RINA must respond to the input text by generatinga paraphrase which conveys its current hypothesis.
Also in gen-eration, RINA faces the problem of incomplete lexiealknowledge.
In absence of specific knowledge regarding the useof suggest, RINA produced an incorrect sentence: he suggested her tocome over, which albeit incorrect, is well understood by a humanlistener.1.2 The IssuesThe basic problem is this: how can any program parse a sen-tence when a lexical entry such as doove or plena is missing?And equivalently, how can a program use a lexicalentry-suggest-which is not precisely specified?
Throe knowledgesources must be negotiated in resolving this problem.
* The dummy words vacua and doove are used here to bring home, even tonative English speakers, the problems faced by language learners.Syntax and Control: In Frank asked Corime to come over, the worda~k actually controls the analysis of the entire sentence\[Bx'esnan82all, nd detemfines the answer to the elementary ques-tion,who comes to whom?~l~e mbedd~ phrase to come over, which does not have an expli-c i t  subject obtains its subject fi'om the control matrix\[Bresnan82a\] of ask.
Accordingly, Corinne is file subject of%oming over".
On the other hand, in he pleaded her to come oyez,the controlliHg word plead, is yet unknown.
In absence of a con-trol matrix it is not clear how to interpret to come over.
Itow can aprogram then, extract even partial information from text in suchcinmmstances?Lex~cal Clues: Although plend itself is unknown, ThE form of ritesentence X piended Y to come over, suggests that "X communicatedto Y a concept regarding coming over".
Three assumptions areimplied: (a) #end is a communication act, (b) Y is the actor of"coming over", (c) "coming over" is only a hypothetical, futureact (and not an act which took place in the past).
How is thisintuition, which facilitates the initial hypothesis for plead, encod-ed in the lexicon?Contextual Clues: The hypothesis elected for doove above is adirect consequence of the context, which brings in a structure ofplans and goals: (1) Corrine has an outstanding goal; (2) Franksuggests help.
Given this background, the selected hypothesis :(3) Corinne rejects the offer.
This selection is problematic sincedoove could stand for other acts, e.g., she wanted to stay, she tried tostay, and she ~orgot to stay, etc.
Thus, how does the context impactthe selection of a hypothesis?Some of the: issues above can be handled by specific heuristicrules, custom tailored for each case.
However, the challenge ofthis entire enterprise is to show how a unified mode!
can employits "normal" parsing mechanism in handling "exceptions".1.3 The Hierarchical LexiconHumans pelceive objects in conceptual hierarchies \[Rosch78,Fahlman79, Shapiro79, Schank82\].
This is best illustrated by anexample from peoples's communication.
Consider the question:what is Jack?
The answer Jack is a cat is satisfactory, provided thelistener knows that a cat is a mammal and a mammal is an ani-mate.
The listener need not be provided with more general factsabout Jack (e.g., Jack has four logs and a tail), since such informationcan be accessed by inheritance from the general classes ubsum-ing a cat.
In fact, for a person who dees not know that cats aremammals, an adequate description of Jack should be more exten-sive.Hierarchical organization is essential in dynamic representationsystems for three reasons:o Economy:  A feature shared by multiple instances houldnot be repetitively acquired per each instance.
Suchredundancy should be avoided by inheriting sharedfeatures from general classes.o Learnability: As shown by \[Winston72, MitcheU82, Ko-lodner84\], through a hierarchy learning can be reduced toa search process.
When one acquires a new zoologicalterm, for example feline, one can traverse the conceptualhierarchy, by generalizing and specializing, until the ap-propriate location is found for feline in thehierarchy-above a number of specific species, and belowthe general mammal.o Prediction: Hierarchy accounts for predictive power,which allows learning models to form intelligent hy-potheses.
When first observing a leopard and by assumingit is a feline, a learner, who has not been exposed to priorinfomaation about leopards, may hypothesize that thisnew animal feeds, breeds, and hunts in certain ways,based on his/her knowledge of felines in general.While it is clear how hierarchy should be used in representingzoological concepts, it is not clear how it applies in representinglinguistic concepts.
Can linguistic systems too benefit from ahierarchical organization?
Following \[Langacker86\] and\[Jacobs85\] we have shown in DHPL (Dynamic HierarchicalPhrasal Lexicon) \[Zemik88\] how elements in a lexicon can beorganized in a hierarchy and thus facilitate a dynamic linguisticbehavior.2.
TIlE LEXICAL HIERARCHY FOR COMMUNICAT IONACTSConsider DHPL's lexical hierarchy for communication acts \[Ki-parskyT1\].
This is a hierarchy by generality where specific in-stances reside at file bottom, and general grammar rules reside atthe top.
Given this hierarchy, which turns out to be incomplete,RINA is capable of coping with a missing specific phrases by in-heriting form general categories.FO.
~ .
~ .
.
~  \]P~ \ ~ subject-equiI~11 / I sense Ii n l t la te /~ P6 communicate 1~ object-equiP3ask / ~ ~PIO suggestS/  threaten promiseask1 \]P=ask,?.Figure 1: The Hierarchy for Complement-Taking Verbs7~7Each node in this hierarchy, denoted for reference purposes by amnemonic word, is actually a full-fledged lexical phrase-an as-sociation of a syntactic pattern with its conceptual meaning.2.1 Specific Phrasal Entries: Two entries for ASK (PI andP2)Consider the representation f the word ask as it appears in thesentence below:(1) The meeting was long and tedious.So Frank asked to leave arly.pattern: X:person ask:verb Z:actconcept:X communicated hat act Z by Xcan achieve agoal G of X.The word ask is given in the lexicon as an entire phrase, or apattern-concept pair \[Wilensky81\].
The abbreviated.notation forP1 above stands for a full-fledged frame \[Mueller84\] as shownbelow:(pattern (subject (instance X))(verb (root ask)(comp (concept Z))(concept (mtrans (actor X)(object (plan Z)(achieve (goal-of X)))))The pattern of the phrase has three constituents: a subject X(Frank), the verb itself, and a complement Z (to leave early).
Inparticular, the semantics of the phrase specify that X is the sub-ject of the embedded act Z, a fact which is not explicit in thetext.
However, this specification fails in capturing further sen-tences, such as the following one.
(2) Frank asked the chairman to adjourn the meeting.There are two discrepancies: (a) this sentence includes a directobject (the chairman), and (b) Frank is not the subject of the com-plement as prescribed in phrase Pl.
Thus, a second phrase P2 isadded on to account for sentences of this kind.pattern: X:person ask:verb Y:person Z:actconcept:X communicated toY that act Z by Y can achieve goal G of XHowever, in order to cope with lexical unknowns, common pro-perties hared by such phrases must be extracted and generalized.2.2 Generalized FeaturesThe phrases PI and P2 above can be abstracted in three ways:(a) along semantics of general equi rules, (b) along the semanticsof the word ask, and (e), along semantics of general eommuniea-?
t ion verbs.
When an unknown word is encountered, its behavioris derived from these general categories.
(a) The general entry  for ASK (P3): The semantic propertiesof ask itself can be generalized through the follwing phrase:pattern: X:person ask!verb Z:aetconcept: X communicate that act Z can achieve a goal G of XThis generalized phrase simply states the meaning of ask, namely"X communicates that act Z can achieve a goal of X", regardlessof ~)  whoJi~ the object of the communication act, and (b) whoegeeuteS the act Z.
(b) The general EQUI-rule (P4 and PS): Semantic propertiescan be generalized across complement-taking verbs:pattern: X:person V:verb Z:actconcept: X is the subject of the embedded act Zpattern: X:person V:verb Y:person Z:aetconcept: Y is the subject of the embedded act ZThese phrases dictate the default identity of the implicit subjectin complement-taking verbs: it is either the object, or the subject(if an object does not exist) of the embedding phrase.
(c) The general COMMUNICATION act (P6): Semanticfeatures of communication acts can be further abstracted.pattern: X:person V:verb Y:person Z:infinitive-aetconcept: Y communicated Z to YPhrase P6 implies that (1) X communicated an act Z to Y, and(2) Z is a hypothetical ct.
When a new word is encountered,for which no specific phrase can be indexed in the lexicon, a hy-pothesis is constructed by inheriting eneral features from thesegeneral phrases.3.
PHRASE INTERACTIONHow does the lexicon become operational in processing text?Consider the following three sentences, ordered according totheir complexity.
(1) Frank came over.
(2) Frank asked Corinne to come over.
(3) Frank plended Corinne to come over.
(1) Sentence (1) is analyzed by simple table lookup.
A phrase(PT-come over)is found in the lexicon, and its concept is instan-tiated.
(2) No single lexical phrase matches entence (2).
Therefore,the analysis o f  (2) involves interaction of two lexical phrases(P2-ask and P7-come over).
(3) No specific lexical phrase matches (3), since it includesan unknown word.
Therefore the analysis of (3) requires the useof generalized phrases, as elaborated below.7983.1 Uni ih~tion with a General  PhraseNo specific phrase in the lexicon matches the word plcod, but ahypothesis regarding the new word can be inherited from generalphrases.
What general phrase should be used?
In our algorithm\[Zernik88\], properties are inherited from the #nost pecific phrasewhich matches file input clause.
In the case of plend above, pro-perties are inherited from two generalized phrases P5-communicateand P6-,objeet-equi, as shown in the figure below:ward (to a native speaker), they certainly convey the main con-cepts, and a user becomes acknowledged of the model's tate ofknowledge.
The general principle of operation is summarizedbelow:P0"~.~.
,~n fin It IveIn i t ia te /  P8 communicate P5, o,o,.ooo, .
i /  \ ,~ ome overFigure 2:While, a single concept was constructed for the word ask in theprevious example, for plend there are multiple possiblities to con-sider.
Steps (2) and (3) are carried out for each.
(1) Select in the hierarchy all possible categories (generalphrases) which match the unknown word.
The communica-tion act (P6) is one possible category for plead.
(2) Unify the appropriate phrases.
The general phrase P6-communicate leaves some parameters unsp~ified.
In par~ticular, the identity of the subject of the embedded phraseis yet unknown-who is supposed to come over to whom?This missing argument is derived by unification withphrase P5, which dictates the default object-equi: thelistener is supposed to come over to the speaker.
(3) lnstantiate the constructed hypotheses:F.13 communicated to C.17 that C.17 will come over to F.13,where coming over achieves a goal of C.17.Several such hypotheses are instantiated.
(4) Discriminate among the mnltiple hypotheses by their se-mautic ingredients.
For example the preceding contextsuggests that Corinne's goal (and not Frank's goal) is ac-tive.
This feature discriminates between two acts such aspromise and plead.5.
ConclusionsUnification with a Generalized PhraseSpecific phrases are preferred to generalphrases.
However, in absence of a precisespecific phrase, inherit properties of generalphrases.\[Bresnan82\]\[Bresnan82a\]\[Chomsky81\]\[Fahlman79\]\[Gazdar85\]While paraing in general presents problems of ambiguity, in thepresence of a lexical gap a situation becomes even furtherunder-cot~vtrained.
So in the ease above there are many legiti-mate hypotheses.
In our method we pick one hypothesis whichmatches the context, and present it to user who may continue theineraetion by providing additional examples.Our model explains a range of generation and comprehension er-rors made by language learners who are foreed to utilize generalapproximations, Although the resulting hypetheses sounds awk-\[Jacobs85\]\[Kiparsky71 \]ReferencesBresnan, J. and R. Kaplan, "Lexical-Functional Grammar," in The MentalRepresentation f Grammatical Relations, ed.J.
Bresnan, M1T Press, MA (1982).Bresnan, J., "Control and Complementation,"in The Mental Representation f GrammaticalRelations, ed.
J. Bresnan, The MIT Press,Cambridge MA (1982).Chomsky, N., Lectures on Government andBinding, Fox-is, Dordrecht (1981).Fahlman, S. E., NETL: A System forRepresenting and Using Real-WorldKnowledge, MIT Press, Cambridge, MA(1979).Gazdar, G., E. Klein, G. Pullum, and I. Sag,Generalized Phrase Structure Grammar, Har-vard University Press, Cambridge MA (1985).Jacobs, Paul S., "PHRED: A Generator forNatural Language Interfaces," UCB/CSD85/198, Computer Science Division, Univer-sity of California Berkeley, Berkeley, Cali-fornia (January 1985).Kiparsky, P. and C. Klparsky, "Fact," in Seomantics, an Interdisciplinary Reader, ed.
D.Steinberg L. Jakobovits, Cambridge Universi-ty Press.
Cambridge.
England (1971).799\[Kolodner84\]\[Langacker86\]\[Mitchell82\]\[Mueller84\]\[Rosch78\]\[Schank82\]\[Shapiro79\]Kolodner, J. L., Retrieval and OrganizationalStrategies in Conceptual Memory: A Comput-er Model, Lawrence Erlbaum Associates,Hillsdale NJ (1984).Langacker, R. W., "An Introduction to Cog-nitive Grammar," Cognitive Science 10(1)(1986).Mitchell, T. M., "Generalization asSearch,"Artificial Intelligence 18, pp.203-226 (1982).Mueller, E. and U. Zernik, "GATE ReferenceManual," UCLA-AI-84-5, Computer Sci-ence, AI Lab (1984).Rosch, E., "Principles of Categorization," inCognition and Categorization, ed.
B. Lloyd,Lawrence Erlbaum Associates (1978).Schank, R. C., Dynamic Memory, CambridgeUniversity Press, Cambridge Britain (1982).Shapiro, S.C., "The SNePS Semantic Net-work Processing System," in Associative Net-works, ed.
N.V. Findler, Academic Press,New York (1979).\[Wilensky81\]\[Winston72\]\[Zemik85\]\[Zernik86\]\[Zernik87\]\[Zernik88\]Wilensky, R., "A Knowledge-Based Ap-proach to Natural Language Processing: Aprogress Report," in Proceedings SeventhInternational Joint Conference on ArtificialIntelligence, Vancouver, Canada (1981).Winston, P. H., "Learning Structural Descrip-tions from Examples," in The Psychology ofComputer Vision, etL P. H. Winston,McGraw-Hill, New York, NY (1972).Zemik, U. and M. G. Dyer, "Towards aSelf-Extending Phrasal Lexicon," in Proceed-ings 23rd Annual Meeting of the Associationfor Computational Linguistics, Chicago IL(July 1985).Zemik, U. and M. G. Dyer, "Disambiguationand Acquisition through the Phrasal Lexi-con," in Proceedings llth InternationalConference on Computational Linguistics,Bonn Germany (1986).Zernik, U., "Learning Phrases in a Hierar-chy," in Proceedings lOth International JointConference on Artificial Intelligence, MilanoItaly (August 1987).Zernik, U. and M. G. Dyer, "The Self-Extending Phrasal Lexicon," The Journal ofComputational Linguistics: Special Issue onthe Lexicon (1988).
to appear.800
