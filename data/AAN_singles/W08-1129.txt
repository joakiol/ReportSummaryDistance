CNTS: Memory-Based Learning of Generating Repeated ReferencesIris Hendrickx, Walter Daelemans, Kim Luyckx, Roser Morante, Vincent Van AschCNTS, Department of LinguisticsUniversity of AntwerpPrinsstraat 13, 2000, Antwerp, Belgiumfirstname.lastname@ua.ac.beAbstractIn this paper we describe our machine learningapproach to the generation of referring expres-sions.
As our algorithm we use memory-basedlearning.
Our results show that in case of pre-dicting the TYPE of the expression, having onegeneral classifier gives the best results.
On thecontrary, when predicting the full set of prop-erties of an expression, a combined set of spe-cialized classifiers for each subdomain givesthe best performance.1 IntroductionIn this paper we describe the systems with whichwe participated in the GREC task of the REG 2008challenge (Belz and Varges, 2007).
The GREC taskconcerns predicting which expression is appropriateto refer to a particular discourse referent in a certainposition in a text, given a set of alternative referringexpressions for selection.
The organizers providedthe GREC corpus that consists of 2000 texts col-lected from Wikipedia, from 5 different subdomains(people, cities, countries, mountains and rivers) .One of the main goals of the task is to discoverwhat kind of information is useful in the input tomake the decision between candidate referring ex-pressions.
We experimented with a pool of featuresand several machine learning algorithms in order toachieve this goal.2 MethodWe apply a standard machine learning approachto the task.
We train a classifier to predict thecorrect label for each mention.
As our machinelearning algorithm we use memory-based learn-ing as implemented in the Timbl package (Daele-mans et al, 2007).
To select the optimal algorith-mic parameter setting for each classifier we useda heuristic optimization method called paramsearch(Van den Bosch, 2004).
We also tried several othermachine learning algorithms implemented in theWeka package (Witten and Frank, 2005), but theseexperiments did not lead to better results and are notfurther discussed here.We developed four systems: a system that onlypredicts the TYPE of each expression (Type), so itpredicts four class labels; and a system that pre-dicts the four properties (TYPE, EMPATHIC, HEAD,CASE) of each expression simultaneously (Prop).The class labels predicted by this system are con-catenated strings: ?common no nominal plain?, andthese concatenations lead to 14 classes, whichmeans that not all combinations appear in the train-ing set.
For both Type an Prop we created two vari-ants: one general classifer (g) that is trained on allsubdomains, and a set of combined specialized clas-sifiers (s) that are optimized for each domain sepa-rately.3 System descriptionTo build the feature representations, we first prepro-cessed the texts performing the following actions:rule-based tokenization, memory-based part-of-speech tagging, NP-chunking, Named entity recog-nition, and grammatical relation finding (Daelemansand van den Bosch, 2005).
We create an instance foreach mention, using the following features to repre-194sent each instance:?
Positional features: the sentence number, theNP number, a boolean feature that indicates ifthe mention appears in the first sentence.?
Syntactic and semantic category given of theentity (SEMCAT, SYNCAT).?
Local context of 3 words and POS tags left andright of the entity.?
Distance to the previous mention measured insentences and in NPs.?
Trigram pattern of the given syntactic cate-gories of 3 previous mentions.?
Boolean feature indicating if the previous sen-tence contains another named entity than theentity in focus.?
the main verb of the sentence.We do not use any information about the given setof alternative expressions except for post process-ing.
In a few cases our classifier predicts a label thatis not present in the set of alternatives.
For thosecases we choose the most frequent class label (as es-timated on the training set).We experimented with predicting all subdomainswith the same classifier and with creating separateclassifiers for each subdomains.
We expected thatsemantically different domains would have differentpreferences for expressions.4 ResultsWe provide results for the four systems Type-g,Type-s, Prop-g and Prop-s in Table 1.
The evalua-tion script was provided by the organisers.
The vari-ant Type-g performs best with a score of 76.52% onthe development set.5 ConclusionsIn this paper we described our machine learning ap-proach to the generation of referring expressions.We reported results of four memory-based systems.Predicting all subdomains with the same classifieris more efficient when predicting the coarse-grainedTYPE class.
On the contrary, training specializedclassifiers for each subdomain works better for theData Type-g Type-sCities 64.65 60.61Countries 75.00 71.74Mountains 75.42 77.07People 85.37 72.50Rivers 65.00 80.00All 76.52 72.26Data Prop-g Prop-sCities 63.64 65.66Countries 72.83 69.57Mountains 72.08 74.58People 79.51 79.51Rivers 65.00 70.00All 73.02 73.93Table 1: Accuracy on GREC development set.more fine-grained prediction of all properties simu-laneously.
For the test set we will present results thetwo best systems: CNTS-Type-g and CNTS-Prop-s.AcknowledgmentsThis research is funded by FWO, IWT, GOA BOF UA,and the STEVIN programme funded by the Dutch andFlemish Governments.ReferencesA.
Belz and S. Varges.
2007.
Generation of repeated ref-erences to discourse entities.
In In Proceedings of the11th European Workshop on Natural Language Gen-eration (ENLG?07), pages 9?16.W.
Daelemans and A. van den Bosch.
2005.
Memory-based language processing.
Cambridge UniversityPress, Cambridge, UK.W.
Daelemans, J. Zavrel, K. Van der Sloot, andA.
Van den Bosch.
2007.
TiMBL: Tilburg MemoryBased Learner, version 6.1, reference manual.
Techni-cal Report 07-07, ILK, Tilburg University.A.
Van den Bosch.
2004.
Wrapped progressive samplingsearch for optimizing learning algorithm parameters.In Proceedings of the 16th Belgian-Dutch Conferenceon Artificial Intelligence, pages 219?226.I.
H. Witten and E. Frank.
2005.
Data Mining: Prac-tical machine learning tools and techniques, secondedition.
Morgan Kaufmann, San Francisco.195
