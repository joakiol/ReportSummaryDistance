Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 450?458,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPJoint Learning of Preposition Senses andSemantic Roles of Prepositional PhrasesDaniel Dahlmeier1, Hwee Tou Ng1,2, Tanja Schultz31NUS Graduate School for Integrative Sciences and Engineering2Department of Computer Science, National University of Singapore3Cognitive Systems Lab, University of Karlsruhe{danielhe,nght}@comp.nus.edu.sgtanja@ira.uka.deAbstractThe sense of a preposition is related to thesemantics of its dominating prepositionalphrase.
Knowing the sense of a prepo-sition could help to correctly classify thesemantic role of the dominating preposi-tional phrase and vice versa.
In this pa-per, we propose a joint probabilistic modelfor word sense disambiguation of preposi-tions and semantic role labeling of prepo-sitional phrases.
Our experiments on thePropBank corpus show that jointly learn-ing the word sense and the semantic roleleads to an improvement over state-of-the-art individual classifier models on the twotasks.1 IntroductionWord sense disambiguation (WSD) and seman-tic role labeling (SRL) are two key componentsin natural language processing to find a semanticrepresentation for a sentence.
Semantic role la-beling is the task of determining the constituentsof a sentence that represent semantic argumentswith respect to a predicate and labeling each witha semantic role.
Word sense disambiguation triesto determine the correct meaning of a word in agiven context.
Ambiguous words occur frequentlyin normal English text.One word class which is both frequent andhighly ambiguous is preposition.
The differentsenses of a preposition express different relationsbetween the preposition complement and the restof the sentence.
Semantic roles and word sensesoffer two different inventories of ?meaning?
forprepositional phrases (PP): semantic roles distin-guish between different verb complements whileword senses intend to fully capture the prepositionsemantics at a more fine-grained level.
In this pa-per, we use the semantic roles from the PropBankcorpus and the preposition senses from the Prepo-sition Project (TPP).
Both corpora are explainedin more detail in the following section.
The re-lationship between the two inventories (PropBanksemantic roles and TPP preposition senses) is nota simple one-to-one mapping, as we can see fromthe following examples:?
She now lives with relatives [insense1Alabama.]ARGM-LOC?
The envelope arrives [insense1the mail.]ARG4?
[Insense5separate statements]ARGM-LOCthe twosides said they want to have ?further discus-sions.
?In the first two examples, the sense of the preposi-tion in is annotated as sense 1 (?surrounded by orenclosed in?
), following the definitions of the TPP,but the semantic roles are different.
In the firstexample the semantic role is a locative adjunctiveargument (ARGM-LOC), while in the second ex-ample it is ARG4 which denotes the ?end point ordestination?
of the arriving action1.
In the first andthird example, the semantic roles are the same, butthe preposition senses are different, i.e., sense 1and sense 5 (?inclusion or involvement?
).Preposition senses and semantic roles providetwo different views on the semantics of PPs.Knowing the semantic role of the PP could behelpful to successfully disambiguate the sense ofthe preposition.
Likewise, the preposition sensecould provide valuable information to classify thesemantic role of the PP.
This is especially so forthe semantic roles ARGM-LOC and ARGM-TMP,where we expect a strong correlation with spatialand temporal preposition senses respectively.In this paper, we propose a probabilistic modelfor joint inference on preposition senses and se-mantic roles.
For each prepositional phrase that1http://verbs.colorado.edu/framesets/arrive-v.html450has been identified as an argument of the pred-icate, we jointly infer its semantic role and thesense of the preposition that is the lexical head ofthe prepositional phrase.
That is, our model maxi-mizes the joint probability of the semantic role andthe preposition sense.Previous research has shown the benefit ofjointly learning semantic roles of multiple con-stituents (Toutanova et al, 2008; Koomen et al,2005).
In contrast, our joint model makes pre-dictions for a single constituent, but multiple tasks(WSD and SRL) .Our experiments show that adding the SRLinformation leads to statistically significant im-provements over an independent, state-of-the-artWSD classifier.
For the SRL task, we show statis-tically significant improvements of our joint modelover an independent, state-of-the-art SRL clas-sifier for locative and temporal adjunctive argu-ments, even though the overall improvement overall semantic roles is small.
To the best of ourknowledge, no previous research has attempted toperform preposition WSD and SRL of preposi-tional phrases in a joint learning approach.The remainder of this paper is structured as fol-lows: First, we give an introduction to the WSDand SRL task.
Then, in Section 3, we describe theindividual and joint classifier models.
The detailsof the data set used in our experiments are givenin Section 4.
In Section 5, we present experimentsand results.
Section 6 summarizes related work,before we conclude in the final section.2 Task DescriptionThis section gives an introduction to prepositionsense disambiguation and semantic role labelingof prepositional phrases.2.1 Preposition Sense DisambiguationThe task of word sense disambiguation is to findthe correct meaning of a word, given its context.Most prior research on word sense disambigua-tion has focused on disambiguating the senses ofnouns, verbs, and adjectives, but not on preposi-tions.
Word sense disambiguation can be framedas a classification task.
For each preposition, aclassifier is trained on a corpus of training exam-ples annotated with preposition senses, and testedon a set of unseen test examples.To perform WSD for prepositions, it is neces-sary to first find a set of suitable sense classes.We adopt the sense inventory from the PrepositionProject (TPP) (Litkowski and Hargraves, 2005)that was also used in the SemEval 2007 preposi-tion WSD task (Litkowski and Hargraves, 2007).TPP is an attempt to create a comprehensive lex-ical database of English prepositions that is suit-able for use in computational linguistics research.For each of the over 300 prepositions and phrasalprepositions, the database contains a set of sensedefinitions, which are based on the Oxford Dic-tionary of English.
Every preposition has a setof fine-grained senses, which are grouped togetherinto a smaller number of coarse-grained senses.
Inour experiments, we only focus on coarse-grainedsenses since better inter-annotator agreement canbe achieved on coarse-grained senses, which alsoresults in higher accuracy of the trainedWSD clas-sifier.2.2 Semantic Role LabelingThe task of semantic role labeling in the contextof PropBank (Palmer et al, 2005) is to label treenodes with semantic roles in a syntactic parse tree.The PropBank corpus adds a semantic layer toparse trees from the Wall Street Journal section ofthe Penn Treebank II corpus (Marcus et al, 1993).There are two classes of semantic roles: core argu-ments and adjunctive arguments.
Core argumentsare verb sense specific, i.e., their meaning is de-fined relative to a specific verb sense.
They arelabeled with consecutive numbers ARG0, ARG1,etc.
ARG0 usually denotes the AGENT and ARG1the THEME of the event.
Besides the core ar-guments, a verb can have a number of adjunc-tive arguments that express more general proper-ties like time, location, or manner.
They are la-beled as ARGM plus a functional tag, e.g., LOC forlocative or TMP for temporal modifiers.
Preposi-tional phrases can appear as adjunctive argumentsor core arguments.The standard approach to semantic role labelingis to divide the task into two sequential sub-tasks:identification and classification.
During the identi-fication phase, the system separates the nodes thatfill some semantic roles from the rest.
During theclassification phase, the system assigns the exactsemantic roles for all nodes that are identified asarguments.
In this paper, we focus on the classi-fication phase.
That is, we assume that preposi-tional phrases that are semantic arguments havebeen identified correctly and concentrate on the451task of determining the semantic role of preposi-tional phrases.
The reason is that argument identi-fication mostly relies on syntactic features, like thepath from the constituent to the predicate (Pradhanet al, 2005).
Consider, for example, the phrase inthe dark in the sentence: ?We are in the dark?, hesaid.
The phrase is clearly not an argument to theverb say.
But if we alter the syntactic structureof the sentence appropriately (while the sense ofthe preposition in remains unchanged), the samephrase suddenly becomes an adjunctive argument:In the dark, he said ?We are?.
On the other hand,we can easily find examples, where in has a differ-ent sense, but the phrase always fills some seman-tic role:?
In a separate manner, he said .
.
.?
In 1998, he said .
.
.?
In Washington, he said .
.
.This illustrates that the preposition sense is inde-pendent of whether the PP is an argument or not.Thus, a joint learning model for argument identifi-cation and preposition sense is unlikely to performbetter than the independent models.3 ModelsThis section describes the models for prepositionsense disambiguation and semantic role labeling.We compare three different models for eachtask: First, we implement an independent modelthat only uses task specific features from the liter-ature.
This serves as the baseline model.
Second,we extend the baseline model by adding the mostlikely prediction of the other task as an additionalfeature.
This is equivalent to a pipeline model ofclassifiers that feeds the prediction of one classifi-cation step into the next stage.
Finally, we presenta joint model to determine the preposition senseand semantic role that maximize the joint proba-bility.3.1 WSD modelOur approach to building a preposition WSD clas-sifier follows that of Lee and Ng (2002), who eval-uated a set of different knowledge sources andlearning algorithms for WSD.
However, in this pa-per we use maximum entropy models2(instead ofsupport vector machines (SVM) reported in (Lee2Zhang Le?s Maximum Entropy Modeling Toolkit,http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.htmland Ng, 2002)), because maximum entropy mod-els output probability distributions, unlike SVM.This property is useful in the joint model, as wewill see later.
Maxent models have been success-fully applied to various NLP tasks and achievestate-of-the-art performance.
There are two train-ing parameters that have to be adjusted for maxentmodels: the number of training iterations and theGaussian smoothing parameter.
We find optimalvalues for both parameters through 10-fold cross-validation on the training set.For every preposition, a baseline maxent modelis trained using a set of features reported inthe state-of-the-art WSD system of Lee andNg (2002).
These features encode three knowl-edge sources:?
Part-of-speech (POS) of surrounding words?
Single words in the surrounding context?
Local collocationsFor part-of-speech features, we include the POStags of surrounding tokens from the same sentencewithin a window of seven tokens around the targetprepositions.
All tokens (i.e., all words and punc-tuation symbols) are considered.
We use the PennTreebank II POS tag set.For the knowledge source single words in thesurrounding context, we consider all words fromthe same sentence.
The input sentence is tokenizedand all tokens that do not contain at least one al-phabetical character (such as punctuation symbolsand numbers) and all words that appear on a stop-word list are removed.
The remaining words areconverted to lower case and replaced by their mor-phological root form.
Every unique morphologi-cal root word contributes one binary feature, in-dicating whether or not the word is present in thecontext.
The position of a word in the sentence isignored in this knowledge source.The third knowledge source, local collocations,encodes position-specific information of wordswithin a small window around the target prepo-sition.
For this knowledge source, we considerunigrams, bigrams, and trigrams from a windowof seven tokens.
The position of the target prepo-sition inside the n-gram is marked with a specialcharacter ?
?.
Words are converted to lower case,but no stemming or removal of stopwords is per-formed.
If a token falls outside the sentence, it isreplaced by the empty token symbol nil.During testing, the maxent model computes the452conditional probability of the sense, given the fea-ture representation of the surrounding context c.The classifier outputs the sense that receives thehighest probability:s?
= argmaxsP (s|?
(c)) (1)where ?(?)
is a feature map from the surroundingcontext to the feature representation.To ensure that our model is competitive, wetested our system on the data set from the SemEval2007 preposition WSD task (Litkowski and Har-graves, 2007).
Our baseline classifier achieved acoarse-grained accuracy of 70.7% (micro-average)on the official test set.
This would have made oursystem the second best system in the competition,behind the MELB-YB system (Ye and Baldwin,2007).We also investigate the effect of the semanticrole label by adding it as a feature to the base-line model.
This pipeline model is inspired by thework of Dang and Palmer (2005) who investigatedthe role of SRL features in verb WSD.
We addthe semantic role of the prepositional phrase dom-inating the preposition as a feature to the WSDmodel.
During training, the PropBank gold SRLlabel is used.
During testing, we rely on the base-line SRL model (to be introduced in the next sub-section) to predict the semantic role of the prepo-sitional phrase.
This is equivalent to first per-forming semantic role labeling and adding the out-put as a feature to the WSD classifier.
In ear-lier experiments, we found that training on goldSRL labels gave better results than training onautomatically predicted SRL labels (using cross-validation).
Note that our approach uses automati-cally assigned SRL labels during testing, while thesystem of Dang and Palmer (2005) only uses goldSRL labels.3.2 SRL modelOur semantic role labeling classifier is also basedon maxent models.
It has been shown that max-imum entropy models achieve state-of-the-art re-sults on SRL (Xue and Palmer, 2004; Toutanovaet al, 2008).
Again, we find optimal valuesfor the training parameters through 10-fold cross-validation on the training set.By treating SRL as a classification problem, thechoice of appropriate features becomes a key is-sue.
Features are encoded as binary-valued func-tions.
During testing, the maxent model computesBaseline Features (Gildea and Jurafsky, 2002)pred predicate lemmapath path from constituent to predicateptype syntactic category (NP, PP, etc.
)pos relative position to the predicatevoice active or passive voicehw syntactic head word of the phrasesub-cat rule expanding the predicate?s parentAdvanced Features (Pradhan et al, 2005)hw POS POS of the syntactic head wordPP hw/POS head word and POS of the rightmostNP child if the phrase is a PPfirst/last word first/last word and POS in the con-stituentparent ptype syntactic category of the parent nodeparent hw/POS head word and POS of the parentsister ptype phrase type of left and right sistersister hw/POS head word and POS of left and rightsistertemporal temporal key words presentpartPath partial path predicateproPath projected path without directionsFeature Combinations (Xue and Palmer, 2004)pred & ptype predicate and phrase typepred & hw predicate and head wordpred & path predicate and pathpred & pos predicate and relative positionTable 1: SRL features for the baseline modelthe conditional probability P (a|t, p, v) of the ar-gument label a, given the parse tree t, predicate p,and constituent node v. The classifier outputs thesemantic role with the highest probability:a?
= argmaxaP (a|t, p, v) (2)= argmaxaP (a|?
(t, p, v)) (3)where ?
(?, ?, ?)
is a feature map to an appropriatefeature representation.For our baseline SRL model, we adopt the fea-tures used in other state-of-the-art SRL systems,which include the seven baseline features from theoriginal work of Gildea and Jurafsky (2002), addi-tional features taken from Pradhan et al (2005),and feature combinations which are inspired bythe system in Xue and Palmer (2004).
Table 1 liststhe features we use for easy reference.In the pipeline model, we investigate the use-fulness of the preposition sense as a feature forSRL by adding the preposition lemma concate-nated with the sense number (e.g., on 1) as a fea-ture.
During training, the gold annotated prepo-sition sense is used.
During testing, the sense isautomatically tagged by the baseline WSD model.This is equivalent to first running the WSD clas-sifier for all prepositions, and adding the outputpreposition sense as a feature to our baseline SRL453system.3.3 Joint Inference ModelThe two previous models seek to maximize theprobability of the semantic role and the preposi-tion sense individually, thus ignoring possible de-pendencies between the two.
Instead of maximiz-ing the individual probabilities, we would like tomaximize the joint probability of the semantic roleand the preposition sense, given the parse tree,predicate, constituent node, and surrounding con-text.?
(a, s) = argmax(a,s)P (a, s|t, p, v, c) (4)We assume that the probability of the semanticrole is already determined by the syntactic parsetree t, the predicate p, and the constituent node v,and is conditionally independent of the remainingsurrounding context c given t, p, and v. Likewise,we assume that the probability of the prepositionsense is conditionally independent of the parse treet, predicate p, and constituent v, given the sur-rounding context c and the semantic role a. Thisassumption allows us to factor the joint probabilityinto an SRL and a WSD component:?
(a, s) = argmax(a,s)P (a|t, p, v)?P (s|c, a) (5)= argmax(a,s)P (a|?
(t, p, v))?P (s|?
(c, a))(6)We observe that the first component in our jointmodel corresponds to the baseline SRL modeland the second component corresponds to theWSD pipeline model.
Because our maxent mod-els output a complete probability distribution, wecan combine both components by multiplying theprobabilities.
Theoretically, the joint probabilitycould be factored in the other way, by first com-puting the probability of the preposition sense andthen conditioning the SRL model on the predictedpreposition sense.
However, in our early exper-iments, we found that this approach gave lowerclassification accuracy.During testing, the classifier seeks to find thetuple of semantic role and preposition sense thatmaximizes the joint probability.
For every se-mantic role, the classifier computes its probabilitygiven the SRL features, and multiplies it by theprobability of the most likely preposition sense,given the context and the semantic role.
The tu-ple that receives the highest joint probability is thefinal output of the joint classifier.Semantic Role Total Training TestARG0 28 15 13ARG1 374 208 166ARG2 649 352 297ARG3 111 67 44ARG4 177 91 86ARGM-ADV 141 101 40ARGM-CAU 31 23 8ARGM-DIR 28 19 9ARGM-DIS 29 9 20ARGM-EXT 61 42 19ARGM-LOC 954 668 286ARGM-MNR 316 225 91ARGM-PNC 115 78 37ARGM-PRD 1 1 0ARGM-REC 1 0 1ARGM-TMP 838 563 275Total 3854 2462 1392Table 2: Number of annotated prepositionalphrases for each semantic role4 Data SetThe joint model uses the probability of a prepo-sition sense, given the semantic role of the dom-inating prepositional phrase.
To estimate thisprobability, we need a corpus which is annotatedwith both preposition senses and semantic roles.Unfortunately, PropBank is not annotated withpreposition senses.
Instead, we manually anno-tated the seven most frequent prepositions in foursections of the PropBank corpus with their sensesfrom the TPP dictionary.
According to Juraf-sky and Martin (2008), the most frequent Englishprepositions are: of, in, for, to, with, on and at (inorder of frequency).
Our counts on Sections 2 to21 of PropBank revealed that these top 7 prepo-sitions account for about 65% of all prepositionalphrases that are labeled with semantic roles.The annotation proceeds in the following way.First, we automatically extract all sentences whichhave one of the prepositions as the lexical head ofa prepositional phrase.
The position of the prepo-sition is marked in the sentence.
By only consid-ering prepositional phrases, we automatically ex-clude occurrences of the word to before infinitivesand instances of particle usage of prepositions,such as phrasal verbs.
The extracted prepositionsare manually tagged with their senses from theTPP dictionary.
Idiomatic usage of prepositionslike for example or in fact, and complex preposi-tion constructions that involve more than one word(e.g., because of, instead of, etc.)
are excluded bythe annotators and compiled into a stoplist.We annotated 3854 instances of the top 7 prepo-454Preposition Total Training Testat 404 260 144for 478 307 171in 1590 1083 507of 97 51 46on 408 246 162to 532 304 228with 345 211 134Total 3854 2462 1392Table 3: Number of annotated prepositionalphrases for each prepositionsitions in Sections 2 to 4 and 23 of the PropBankcorpus.
The data shows a strong correlation be-tween semantic roles and preposition senses thatexpress a spatial or temporal meaning.
For thepreposition in, 90.8% of the instances that ap-pear inside an ARGM-LOC are tagged with sense 1(?surrounded by or enclosed in?)
or sense 5 (?in-clusion or involvement?).
94.6% of the instancesthat appear inside an ARGM-TMP role are taggedwith sense 2 (?period of time?).
Our counts fur-thermore show that about one third of the anno-tated prepositional phrases fill core roles and thatARGM-LOC and ARGM-TMP are the most fre-quent roles.
The detailed breakdown of semanticroles is shown in Table 2.To see how consistent humans can perform theannotation task, we computed the inter-annotatoragreement between two annotators on Section 4 ofthe PropBank corpus.
We found that the two anno-tators assigned the same sense in 86% of the cases.Although not directly comparable, it is interestingto note that this figure is similar to inter-annotatoragreement for open-class words reported in previ-ous work (Palmer et al, 2000).
In our final dataset, all labels were tagged by the same annotator,which we believe makes our annotation reason-ably consistent across different instances.
Becausewe annotate running text, not all prepositions havethe same number of annotated instances.
Thenumbers for all seven prepositions are shown inTable 3.
In our experiments, we use Sections 2 to 4to train the models, and Section 23 is kept for test-ing.
Although our experiments are limited to threesections of training data, it still allows us to traincompetitive SRL models.
Pradhan et al (2005)have shown that the benefit of using more trainingdata diminishes after a few thousand training in-stances.
We found that the accuracy of our SRLbaseline model, which is trained on the 5275 sen-tences of these three sections, is only an absoluteBaselinePipelineJoint30%40%50%60%70%80%90%at for in of on to with totalAccuracyFigure 1: Classification accuracy of the WSDmodels for the seven most frequent prepositionsin test section 233.89% lower than the accuracy of the same modelwhen it is trained on twenty sections (71.71% ac-curacy compared to 75.60% accuracy).5 Experiments and ResultsWe evaluate the performance of the joint model onthe annotated prepositional phrases in test section23 and compare the results with the performanceof the baseline models and the pipeline models.Figure 1 shows the classification accuracy of theWSD models for each of the seven prepositions inthe test section.
The results show that the pipelinemodel and the joint model perform almost equally,with the joint model performing marginally betterin the overall score.
The detailed scores are givenin Table 4.
Both models outperform the baselineclassifier for three of the seven prepositions: at,for, and to.
For the prepositions in, of, and on, theSRL feature did not affect the WSD classificationaccuracy significantly.
For the preposition with,the classification accuracy even dropped by about6%.Performing the student?s t-test, we found thatthe improvement for the prepositions at, for, andto is statistical significant (p < 0.05), as is theoverall improvement.
This confirms our hypoth-esis that the semantic role of the prepositionalphrase is a strong hint for the preposition sense.However, our results also show that it is theSRL feature that brings the improvement, not thejoint model, because the pipeline and joint modelachieve about the same performance.For the SRL task, we report the classificationaccuracy over all annotated prepositional phrasesin the test section and the F1measure for the se-mantic roles ARGM-LOC and ARGM-TMP.
Fig-455Preposition Baseline Pipeline Jointat 70.83 78.47?78.47?for 41.52 49.12?49.12?in 62.33 61.74 61.93of 43.48 43.48 43.48on 51.85 51.85 52.47to 58.77 67.11?66.67?with 44.78 38.06 38.06Total 56.54 58.76?58.84?Table 4: Classification accuracy of the baseline,pipeline, and joint model on the WSD task in testsection 23, statistically significant improvementsover the baseline are marked with an (*)BaselinePipelineJoint65%70%75%80%85%90%Argm?LOC Argm?TMP Overallf1?measureFigure 2: F1measure of the SRL models forARGM-LOC and ARGM-TMP, and overall accu-racy on prepositional phrases in test section 23ure 2 shows the results.
The joint model showsa small performance increase of 0.43% over thebaseline in the overall accuracy.
Adding thepreposition sense as a feature, on the other hand,significantly lowers the accuracy by over 2%.
ForARGM-LOC and ARGM-TMP, the joint model im-proves the F1measure by about 1.3% each.
Theimprovement of the joint model for these rolesis statistically significant (p ?
0.05, student?s t-test).
Simply adding the preposition sense in thepipeline model again lowers the F1measure.
Thedetailed results are listed in Table 5.Semantic Role Baseline Pipeline JointARGM-LOC(F1) 72.88 71.54 74.27*ARGM-TMP(F1) 81.87 79.43 83.24*Overall(A) 71.71 69.47 72.14Table 5: F1measure and accuracy of the baseline,pipeline, and joint model on the SRL task in testsection 23, statistically significant improvementsover the baseline are marked with an (*)Our SRL experiments show that a pipelinemodel degrades the performance.
The reason isthe relatively high degree of noise in the WSDclassification and that the pipeline model does notdiscriminate whether the previous classifier pre-dicts the extra feature with high or low confi-dence.
Instead, the model only passes on the 1-best WSD prediction, which can cause the nextclassifier to make a wrong classification based onthe erroneous prediction of the previous step.
Inprinciple, this problem can be mitigated by train-ing the pipeline model on automatically predictedlabels using cross-validation, but in our case wefound that automatically predicted WSD labelsdecreased the performance of the pipeline modeleven more.
In contrast, the joint model computesthe full probability distribution over the semanticroles and preposition senses.
If the noise level inthe first classification step is low, the joint modeland the pipeline model perform almost identically,as we have seen in the previousWSD experiments.But if the noise level is high, the joint model canstill improve while the pipeline model drops inperformance.
Our experiments show that the jointmodel is more robust in the presence of noisy fea-tures than the pipeline model.6 Related WorkThere is relatively less prior research on preposi-tions and prepositional phrases in the NLP com-munity.
O?Hara and Wiebe (2003) proposed aWSD system to disambiguate function tags ofprepositional phrases.
An extended version oftheir work was recently presented in (O?Hara andWiebe, 2009).
Ye and Baldwin (2006) extendedtheir work to a semantic role tagger specificallyfor prepositional phrases.
Their system first classi-fies the semantic roles of all prepositional phrasesand later merges the output with a general SRLsystem.
Ye and Baldwin (2007) used semanticrole tags from surrounding tokens as part of theMELB-YB preposition WSD system.
They foundthat the SRL features did not significantly helptheir classifier, which is different from our find-ings.
Dang and Palmer (2005) showed that se-mantic role features are helpful to disambiguateverb senses.
Their approach is similar to ourpipeline WSD model, but they do not present re-sults with automatically predicted semantic roles.Toutanova et al (2008) presented a re-rankingmodel to jointly learn the semantic roles of mul-tiple constituents in the SRL task.
Their workdealt with joint learning in SRL, but it is not di-rectly comparable to ours.
The difference is that456Toutanova et al attempt to jointly learn semanticrole assignment of different constituents for onetask (SRL), while we attempt to jointly learn twotasks (WSD and SRL) for one constituent.
Be-cause we only look at one constituent at a time,we do not have to restrict ourselves to a re-rankingapproach like Toutanova et al, but can calculatethe full joint probability distribution of both tasks.Andrew et al (2004) propose a method to learn ajoint generative inference model from partially la-beled data and apply their method to the problemsof word sense disambiguation for verbs and deter-mination of verb subcategorization frames.
Theirmotivation is similar to ours, but they focus onlearning from partially labeled data and they in-vestigate different tasks.None of these systems attempted to jointly learnthe semantics of the prepositional phrase and thepreposition in a single model, which is the maincontribution of our work reported in this paper.7 ConclusionWe propose a probabilistic model to jointly clas-sify the semantic role of a prepositional phraseand the sense of the associated preposition.
Weshow that learning both tasks together leads to animprovement over competitive, individual modelsfor both subtasks.
For the WSD task, we showthat the SRL information improves the classifi-cation accuracy, although joint learning does notsignificantly outperform a simpler pipeline modelhere.
For the SRL task, we show that the jointmodel improves over both the baseline model andthe pipeline model, especially for temporal and lo-cation arguments.
As we only disambiguate theseven most frequent prepositions, potentially moreimprovement could be gained by including moreprepositions into our data set.AcknowledgementsThis research was supported by a research grantR-252-000-225-112 from National University ofSingapore Academic Research Fund.ReferencesGalen Andrew, Trond Grenager, and Christopher D.Manning.
2004.
Verb Sense and Subcategorization:Using Joint Inference to Improve Performance onComplementary Tasks.
In Proceedings of the 2004Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP 2004), pages 150?157.Hoa Trang Dang and Martha Palmer.
2005.
TheRole of Semantic Roles in Disambiguating VerbSenses.
In Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguistics(ACL-05), pages 42?49.Daniel Gildea and Daniel Jurafsky.
2002.
AutomaticLabeling of Semantic Roles.
Computational Lin-guistics, 28(3):245?288.Daniel Jurafsky and James H. Martin.
2008.
Speechand Language Processing.
Prentice-Hall, Inc. Up-per Saddle River, NJ, USA.Peter Koomen, Vasin Punyakanok, Dan Roth, andWen-tau Yih.
2005.
Generalized Inference withMultiple Semantic Role Labeling Systems.
In Pro-ceedings of the 9th Conference on ComputationalNatural Language Learning (CoNLL 2005), pages181?184.Yoong Keok Lee and Hwee Tou Ng.
2002.
An Empir-ical Evaluation of Knowledge Sources and Learn-ing Algorithms for Word Sense Disambiguation.
InProceedings of the 2002 Conference on EmpiricalMethods in Natural Language Processing (EMNLP2002), pages 41?48.Kenneth C. Litkowski and Orin Hargraves.
2005.
ThePreposition Project.
In Proceedings of the 2nd ACL-SIGSEM Workshop on The Linguistic Dimensions ofPrepositions and Their Use in Computational Lin-guistic Formalisms and Applications, pages 171?179.Kenneth C. Litkowski and Orin Hargraves.
2007.SemEval-2007 Task 06: Word-Sense Disambigua-tion of Prepositions.
In Proceedings of the 4th In-ternational Workshop on Semantic Evaluations (Se-mEval 2007), pages 24?29.Mitchell P. Marcus, Mary A. Marcinkiewicz, and Beat-rice Santorini.
1993.
Building a Large AnnotatedCorpus of English: The Penn Treebank.
Computa-tional Linguistics, 19(2):313?330.Tom O?Hara and Janyce Wiebe.
2003.
Preposi-tion Semantic Classification via Penn Treebank andFrameNet.
In Proceedings of the 7th Conference onComputational Natural Language Learning (CoNLL2003), pages 79?86.Tom O?Hara and Janyce Wiebe.
2009.
Exploiting Se-mantic Role Resources for Preposition Disambigua-tion.
Computational Linguistics, 35(2):151?184.Martha Palmer, Hoa Trang Dang, and Joseph Rosen-zweig.
2000.
Sense Tagging the Penn Treebank.
InProceedings of the 2nd International Conference onLanguage Resources and Evaluation (LREC 2000).Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The Proposition Bank: An Annotated Cor-pus of Semantic Roles.
Computational Linguistics,31(1):71?105.457Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,Wayne Ward, James H. Martin, and Daniel Juraf-sky.
2005.
Support Vector Learning for SemanticArgument Classification.
Machine Learning, 60(1?3):11?39.Kristina Toutanova, Aria Haghighi, and Christopher D.Manning.
2008.
A Global Joint Model for Se-mantic Role Labeling.
Computational Linguistics,34(2):161?191.Nianwen Xue and Martha Palmer.
2004.
CalibratingFeatures for Semantic Role Labeling.
In Proceed-ings of the 2004 Conference on Empirical Methodsin Natural Language Processing (EMNLP 2004),pages 88?94.Patrick Ye and Timothy Baldwin.
2006.
Seman-tic Role Labeling of Prepositional Phrases.
ACMTransactions on Asian Language Information Pro-cessing (TALIP), 5(3):228?244.Patrick Ye and Timothy Baldwin.
2007.
MELB-YB:Preposition Sense Disambiguation Using Rich Se-mantic Features.
In Proceedings of the 4th Interna-tional Workshop on Semantic Evaluations (SemEval2007), pages 241?244.458
