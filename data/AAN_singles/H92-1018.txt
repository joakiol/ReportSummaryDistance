SRI  INTERNATIONAL RESULTSFEBRUARY 1992 ATIS BENCHMARK TESTDouglas E. Appelt, Eric JacksonSRI InternationalMenlo Park, CA 94025ABSTRACTWe describe the results that SRI International chieved onthe February 1992 ATIS Speech and Natural Language Sys-tem Test.
The basic architecture of the system is described,including a set of parameters capable of altering the system'sbehavior and processing strategy.
We report on several ex-periments that were run on the February test set to evaluateseveral processing strategies for both natural-language onlyand full spoken-language system tests.1.
INTRODUCTIONThis paper reports on the results of running SRI In-ternational's poken-language system on the DARPA-sponsored February 1992 test.
The system's natural-language processing has been parameterized in severalways to achieve different behaviors.
In addition to run-ning our system with what we believed at the time of thetest to be the optimal parameter settings to produce ourofficial results, we have conducted some experiments byrunning the system with a variety of parameter settings.The results of these experiments shed some light on thetrade-offs among various SLS and natural-language pro-cessing strategies, and provide some interesting data forevaluating the evaluation methodology itself.2.
SYSTEM DESCRIPT IONThe SLS system used for the February evaluation is anintegration of the SRI DECIPHER speech recognition\[1,4,5\] system with the SRI TRAVELOGUE natural-language processing system.
The integration betweenthese two systems is currently accomplished by a sim-ple serial interface: the best accoustic hypothesis i pro-cessed by the NL system to produce the answer to thequery.The DECIPHER SystemDECIPHER is a speaker-independent continuous-speechspeech recognition system based on tied-mixture Hid-den Markov Model (HMM) models.
It uses six features,three being vectors (cepstra, delta-cepstra, and delta-delta-cepstra) and three scalars (energy, delta-energy,and delta-delta-energy).
These features are computed95from a filter bank that is derived via an FFT and high-pass filtered (RASTA filtered) in the log-spectral-energydomain.
DECIPHER models pronunciation variabilitythrough word networks generated by linguistic rules thenpruned probabilistically.
There are cross-word acousticand phonological models.
Parallel recognizers were im-plemented and trained separately on male and femalespeech.
The DECIPHER-ATIS system uses a backed-offbigram language model to reduce the perplexity of theinput speech.The acoustic models were trained on all available ATISspontaneous and read data (excluding 809 sentences usedfor system development that include 362 October 1991dry run sentences and 447 MADCOW sentences).
Thebacked-off bigram language model was trained on theavailable ATIS spontaneous speech data.
This included14,779 sentences (approximately 150,000 words).
Therecognition lexicon consisted of all words spoken in allavailable spontaneous ATIS data.
There are also lexi-cal entries for breaths and silence.
No catch-all rejectionmodel was used for out-of-vocabulary items.
The vocab-ulary size is 1385 words.The TRAVELOGUE SystemThe TRAVELOGUE system consists of a template-matching sentence-analysis mechanism \[3\] coupled witha context-handling mechanism and a database querygeneration component.The template matcher operates by producing templatesfrom the input sentence which then get translated intodatabase queries.
The two main components of a tem-plate are the template type, which generally correspondsto a relation in the underlying database, and a set offilled slots, which represent constraints present in thequery.
A template for the sentence "Show me the non-stop flights from Boston" might be of the type "flight"and have an origin slot filled with "Boston" and a stopsslot filled with "0."
In addition to these components,a template contains an illocutionary force marker (e.g.,"show," "how many," "yes/no"), and a list of explicitlyrequested fields from the relation associated with thetemplate type.
There are 20 different template typesand 110 distinct slots.The template matcher determines the type of templateby looking for certain key nouns or key phrases in thesentence.
It incorporates a simple noun phrase grammarthat allows it to identify phrases containing key nouns.The presence of a key noun in certain contexts (e.g., in anoun phrase preceded by a word like "show") will morestrongly trigger the associated template type than anisolated occurrence of that key noun.
Conjunctions ofnoun phrases containing key nouns produce templateswith multiple template types.Slots are filled by matching regular-expression patternsagainst the input string.
For example, "from" followedby an airport or city name may fill the origin slot ofthe flight template.
To find fillers for slots, the templatematcher makes use of a lexicon of names and codes, eachassociated with the appropriate sort, and special gram-mars tier recognizing numbers, dates, and times.
Foreach template type with some key noun or key phrasepresent in the sentence, the system tries to find the best"slot covering" of the sentence it can.
That is, it tries tofind the sequence of slot-filling patterns that matches thesentence and consumes as many words as possible.
Twoconstraints are (1) slot filling phrases may not overlap,and (2) no slot may be filled twice with different Val-ues.
The system incorporates a schematic mapping ofthe domain, which contains the information as to howentities are related, and allows the system to determinewhat slots are possible for each template.In the :next stage, the system chooses a single templatefrom the set of candidate templates that have been con-structed.
I t  chooses on the basis of several factors, in-cluding the type of key that triggered the template andthe number of words consumed in filling slots.
A tem-plate score is then computed for the chosen template,reflecting the proportion of words in the sentence thatare considered to be consumed.
Words that fill slotsor help slots get filled count, as well as function wordsand certain other words (such as "please") that are ig-nored for the purposes of scoring.
If the template doesnot score above a threshold, the system chooses not torisk answering the query.
The threshold can be varieddepending on how much risk of a wrong answer can betolerated.
For evaluation we have found a threshold ofabout 0.85 to be optimal, while for data collection weuse a lower threshold, typically 0.5.The template matcher incorporates special mechanismsto handle certain types of false starts and complex con-junctions.
These phenomena cannot be handled well ina straightforward, unaugmented, template-matching ap-96proach.The template matcher was developed on all the anno-tated MADCOW data available as of January 1, 1992.In addition, a 3,000-sentence subset of the MADCOWdata was annotated with the correct template for eachutterance.
The template production of the system couldbe quickly evaluated on these sentences.
As of January1992, the systenfs performance on this corpus was above90%.When a template is produced, the context-handlingmechanism of TRAVELOGUE is invoked to determinewhether the template for the current sentence should hemodified or expanded based on the current state of thedialogue.
The system emnploys a variety of context han-dling rules, each of which is justified by a plan-basedmodel of dialogue structure similar to that of Grosz andSidner \[2\].
The basic model tracks the context of a di-alogue by assuming the user is following a plan that in-volves knowing which database ntities satisfy a set ofconstraints that he or she has in fnind when the sessioncommences, because the user has the goal of formulat-ing a travel plan (as opposed to other purposes for whichsuch a database would he useful).The context mechanism inherits constraints expressedby previous queries in a scenario as long as accumulatingthese constraints i consistent with knowing a single setof constraints applicable to a single travel plan.
Knowingwhether this set of constraints i consistent with the over-all plan is accomplished by comparing the new slots to acontext priority-lattice that establishes a partial order ofdependencies among various template slots.
Changes inhigher-level constraints cause lower-level constraints tobe discarded.
This general mechanism is supplementedwith a mechanism for handling deicti?
references andreferences to particular database ntities that have ap-peared in answers to previous questions.When a template including contextually inherited slots isproduced, the TRAVELOGUE produces, optimizes, andruns a PROLOG database query, generating the finalanswer.3.
OFF IC IAL  RESULTSIn the February 1992 DARPA ATIS benchmark tests,SRI achieved the following results: In the ATIS speechrecognition evaluation, SRI achieved a word recogni-tion error rate of 11.0% and a sentence recognition er-ror rate of 48.7% over all sentences on the test corpus.In the ATIS natural-language-only test, SRI achieved aweighted error rate of 31.1%, with 533 queries answeredcorrectly, 60 incorrectly, and 94 given no answer.
In theATIS spoken-language systems evaluation, SRI achieveda weighted error rate of 45.4%, with 444 queries answeredcorrectly, 69 incorrectly, and  174 queries given no an-swer.We performed an error analysis on the NL-only evalu-ation results.
We examined all the queries that we didnot answer or for which we were scored wrong, and triedto ascertain the cause.Of the sentences that were either incorrect or unan-swered, 46% can be attributed to the failure of thetemplate matcher to generate a correct template.
Ofthese failures, 80% could be remedied within the currentframework while 20% would require a substantially dif-ferent approach, such as a parser and grammar that to-gether could provide more structural information abouta sentence.
We estimate that 12% of the errors weredue to the database query generation component, and18% were due to failures of the context mechanism toidentify the correct context.
The remaining errors areattributed to the system declining to answer questionswhen it determined that its uncertainty about the con-text was too great.These figures were derived in a highly subjective fash-ion, but, nevertheless, we feel they give a roughly ac-curate picture.
For a majority of the utterances thatcaused trouble for the template-generating component,it is clear that adding a new phrase or new slot couldsolve the problem.
The conclusion we draw from this isthat a template-matching approach can be highly suc-cessful on a domain of about the same complexity asATIS.
How well this type of approach would scale up toa significantly larger domain remains uncertain.4.
ADDIT IONAL EXPERIMENTSWe have implemented several parameters that controlthe behavior of the system.
One parameter is the 1.template-matcher score cutoff.We recognized that if a system failed to respond correctlyto a query, it might give incorrect answers to a numberof subseqent context-dependent queries, even though thesubseqent sentences were processed correctly, given ev-erything the system can determine about the state of the 2.dialogue.
Therefore, we have included several parame-ters that regulate the generation of responses in situa-tions in which, for one reason or another, the state ofthe context is in doubt.One such parameter is a cumulative template-score cut-off.
We reasoned that if the system answers a series ofquestions, each of which receives an acceptable, although 3.less than perfect, template score, eventually a point isreached in which the system is so uncertain about the97correctness of the accumulated contextual information,that it should, for evaluation purposes, stop answeringquestions until a query is encountered that definitely setsa new top-level context.
This point is detected by mul-tiplying template scores until the cumulative productdrops below the level indicated by the cumulative cutoffparameter.
Our official results were produced by usingvalues of 0.85 and 0.82 for the template score cutoff andcumulative score cutoff, respectively.Another parameter controls the choice of one of threepossible ways of dealing with the failure to produce ananswer for a query.
When the system fails to answer aquery, it could refuse to answer any further queries untilone is found that sets a new top-level context.
Althoughthis would be a ridiculous way for a system to behavewhen interacting with a real user, some preliminary in-vestigation led us to believe that such a strategy was in-deed optimal for the evaluation; this is the strategy usedto generate our official results.
Another possible strat-egy, which we dub "always answer," is to have the systemanswer every question in the last previously known con-text, regardless of how many intermediate queries fail toproduce answers.
Finally, we have a "usually answer"mode, in which queries are always evaluated in the mostrecently determined context, unless there is some fea-ture of the query that indicates explicit dependency ona question that was not answered (such as a pronounor demonstrative r ference that could rely on an unan-swered query for its resolution).We ran experiments on our system for the following con-figurations of parameters on both NL and SLS data.These runs were made by changing only the parametersdiscussed above, without attempting to influence the be-havior of the system in any other way:Re laxed  Cutoff .
We set the template score cutoffto 0.82, and the cumulative cutoff to 0.70.
Some ofour earlier experiments suggested that these valueswere optimal for processing speech recognizer out-put.
(Because of an oversight, they were not usedin the official test).Low Scor ing Template  St rategy.
This strategysets the template score cutoff and cumulative cutoffto be 0.01.
This allows very low scoring templatesto be considered as analyses for a sentence.
Theconservative strategy of not answering questions af-ter failure to produce any template at all until thenext context-resetting sentence was still followed.Max imum Recal l  S t rategy.
This strategy com-bines the Low-Scoring Template Strategy with theAlways Answer strategy.
It seeks to maximize recallby always answering a query whenever any analy-sis at all is possible.
Naturally, precision suffers,because of the increased chance that some of thepoorly rated analyses will be wrong.4.
Max imum Prec is ion  S t ra tegy .
We attempted tomaximize the system's precision score by setting thetemplate score cutoff and the cumulative cutoff tobe 0.99.
This strategy causes the system to respondonly to templates with perfect scores and to stop an-swering in context whenever any uncertainty abouta template xists.
Naturally, because some correcttemplates will be discarded, recall suffers.5.
A lways Answer  S t ra tegy .
The "always answer"context-handling strategy was adopted, keeping thetemplate score cutoff the same as in the official run.6.
Usua l ly  Answer  S t ra tegy .
The "usually answer"context-handling strategy was adopted, keeping thetemplate score cutoff the same as in the official run.5.
RESULTS OF  EXPERIMENTSThe results we observed for the experiments describedin the previous section (as well as our official results onthe evaluation) were as follows, ordered by increasingweighted error:For NL only:the desired recall-precision tradeoff, although neither ofthese strategies produced the best results as measuredby weighted error.
It is also interesting to note that,with the exception of the tests for Relaxed Cutoff andUsually Answer configurations (which were in any casevery close), the ordering of the results as measured byweighted error was the same for both NL and SLS tests.6.
SLS  EVALUATION WITH BBNRECOGNIZER OUTPUTBecause the preliminary results of the February 1992ATIS benchmark tests suggested that the SRI TRAV-ELOGUE NL system and the BBN BYBLOS speech-recognition system had both performed particularly well,SRI and BBN collaborated on an experiment to see howwell a combined system would have performed on thebenchmark test, using the output of BYBLOS as the in-put to TRAVELOGUE.
We took the BYBLOS outputfrom the official February 1992 ATIS SPREC test andran it through TRAVELOGUE, configured exactly as itwas for the official February 1992 ATIS SLS test.
So, al-though this was not submitted as official February 1992ATIS SLS test output, it is comparable in every respectto the official results obtained by BBN and SRI.
Theresulting combination produced 482 correct answers, 69wrong answers, and 136 without answers, for a weightederror of 39.88%.Parameter No Wtd.Settings Right Wrong Ans ErrorAlways Answer 554 72 61 29.84Usually Answer 538 60 89 30.42Relaxed Cutoff 537 62 88 30.86Official Results 533 60 94 31.05Low-Score Template 558 90 39 31.88Maximum Recall 565 98 24 32.02Maximum Precision 480 38 169 35.66This experiment may shed some light on the impactof speech-recognition accuracy for SLS performance, ifwe compare SLS performance with the SRI and BBNrecognizers, holding NL processing constant.
The im-provement of the SLS weighted error from 45.4% to39.9% represents a error reduction by a factor of 0.12,and was obtained was obtained by running the NL sys-tem on input data for which the word error rate on classA and D sentences was improved from 8.4% to 6.2%, anerror reduction factor of 0.26.
The corresponding sen-tence error rates were 44.5% and 34.6%, for an errorreduction factor of 0.22.
For SLS:Parameter No Wtd.Settings Right Wrong Ans ErrorAlways Answer 457 75 155 44.40I Relaxed Cutoff 447 69 171 44.98Usually Answer 445 69 173 45.27Official Results 444 69 174 45.40Low-Score Template 455 86 146 46.29Maximum Recall 460 93 134 46.58Maximum Precision 423 62 202 47.45As can be seen, the predicted parameter settings forMaximum Recall and Maximum Precision did result inAlthough the NL processing in TRAVELOGUE is de-signed to be robust in the face of recognition errors, it isclear that the point of diminishing return on recognitionaccuracy has not yet been reached, and significant im-provements can be obtained if these error rates can bereduced still further.We did one other experiment with the combination ofBYBLOS and TRAVELOGUE, in which we took theBYBLOS SPREC test output and ran it through TRAV-ELOGUE using the parameter settings that we now be-lieve to be optimal as a result of the experiments re-ported in the preceding section.
This was a combination98of the "always answer" context-handling strategy withthe "relaxed cutoff" parameter settings.
We felt thatthis would represent he best performance the systemwas currently capable of without increasing the basicunderlying competence.
In this experiment we obtained495 correct answers, 77 wrong answers, and 117 withoutanswers, for a weighted error of 39.16%.7.
EL IMINAT ING CLASS XSENTENCESIn addition to the above tests, we ran a test to evalu-ate the impact of a proposed change to the evaluationprocedures to eliminate class-X sentences from the evalu-ation.
Queries are classified as X for a variety of reasons,the most common being that the query lies outside thescope of the database.
Although class-X utterances arenot counted when computing the scores for NL and SLSevalutions, it may be the case that class-X queries thatare clearly outside the scope of the system's processingcapabilities could adversely impact the system's abilityto track the context, and thus indirectly affect the sys-tem's test results.If the inclusion of class-X sentences in the test were tomake a large difference in the scores, it would call intoquestion the success of the effort to eliminate the impactof processing class-X queries from the evaluation results.To test the impact of class-X sentences on our system,we ran the system configured exactly as it was for theofficial test, except that all class-X sentences were ex-cluded from consideration.
We found that the weightederror decreased by 0.58 for the NL-only test and by 1.0for the SLS test.
While there is an observable "class-Xeffect," it seems to be relatively small with our system,and would only be noticeable with a processing strategythat based answering decisions on context uncertainty.8.
SUMMARY AND CONCLUSIONSIt is difficult to draw conclusions from these experimentsabout the efficacy of various parameter settings and pro-cessing strategies for improving performance on the eval-uation.
The results are in fact very similar, and couldwell be different with a different est set.
It is possible toconclude with confidence only that the Maximum Pre-cision strategy is unlikely to yield the lowest weightederror.The results of these experiments were rather surprisingin that we had originally believed that the parameterchoices would have a more significant impact on theweighted error than what we observed.
Indeed, the re-sults show a surprising insensitivity to parameter choice.99It seems to be the case that the weighted error metricdisguises differences in system behavior.
For example,the Maximum Precision and Maximum Recall strategiesproduce vastly different behavior on the SLS test: theMaximum Recall strategy answers almost 70 queries towhich the Maximum Precision strategy gives no answer.Yet the difference in weighted error for the two strategiesis less than one point.For comparing performance across systems, it is desire-able to have a metric for comparing performance acrosssystems that is relatively insensitive to different answer-ing strategies, and therefore has a better chance of trulyreflecting the comprehensiveness of a system's coverageof the domain.
These experiments demonstrate that theweighted error metric at least comes close to having thatproperty - -  a fortunate consequence, because it was cho-sen primarily on the basis of its inuitive appeal.
On theother hand, systems with specific characteristics are pre-ferred for particular purposes.
For example, when SRIuses its system for MADCOW data collection, it runs ina mode more closely approximating the Maximum Recallstrategy, on the theory that producing some affswer, eventhough not perfectly correct, will hold the user's interestand lead to a smoother flowing dialogue than would fre-quent "I don't understand" responses, even though theexperiments indicate that such a strategy is suboptimalfor evaluation.
These experiments underscore the needto examine multiple properties of a system to arrive atconclusions regarding that system's overall effectivenessat solving user problems, as effectiveness can depend onfactors other than the system's ability to obtain a lowweighted error.An important observation is that the five systems withthe best scores in the NL evaluation differed by only3.8 points.
We have shown that our system can demon-strate a variation of more than 3 points in weighted errorthrough the selection of different answering strategiesholding the basic competence of the system constant.We would therefore be reluctant to conclude that thescores achieved on this benchmark test indicate a cleardifference among these five systems in basic competence.We found it interesting that the Always Answer contextstrategy would have produced the best results on thisevaluation, because this is the most reasonable strategyto employ in a system intended to interact with a user,rather than merely scoring high on the evaluation.
Ifthe goal is to evaluate systems under conditions that ap-proximate as much as possible their conditions of use inthe real world, it is reassuring that behavior appropri-ate to the real world would not be inappropriate for theevaluation.REFERENCES1.
Butzberger, J. et al, "Modeling Spontaneous Speech Ef-fects in Large Vocabulary Speech Applications", Pro-ceed~ngs of the 1992 DARPA Speech and Natural Lan-guage Workshop.2.
Grosz, B. and Sidner, C., "Attentions, Intentions, andthe Structure of Discourse," Computational Linguistics,Vol.
12, No.
3, 1966.3.
Jackson, E. et al, "A Template Matcher for Robust NLInterpretation," Proceedings ofthe 1991 DARPA Speechand Natural Language Workshop, pp.
190-194.4.
Murveit, H. et al, "Performance of SRI's DecipherSpeech Recognition System on DARPA's ATIS Task,"Proceedings of the 1992 DARPA Speech and NaturalLanguage Workshop.5.
Murveit, H. et al, "Reduced Channel-Dependence forSpeech Recognition," Proceedings of the 1992 DARPASpeech and Natural Language Workshop.i00
