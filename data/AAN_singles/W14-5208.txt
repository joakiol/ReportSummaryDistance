Proceedings of the Workshop on Open Infrastructures and Analysis Frameworks for HLT, pages 66?76,Dublin, Ireland, August 23rd 2014.SSF: A Common Representation Scheme for Language Analysis forLanguage Technology Infrastructure DevelopmentAkshar BharatiAkshar Bharati GroupHyderabadsangal@iiit.ac.inRajeev SangalIIT (BHU), Varanasisangal@iiit.ac.inDipti SharmaIIIT, Hyderabaddipti@iiit.ac.inAnil Kumar SinghIIT (BHU), Varanasinlprnd@gmail.comAbstractWe describe a representation scheme and an analysis engine using that scheme, both of whichhave been used to develop infrastructure for HLT.
The Shakti Standard Format is a readable androbust representation scheme for analysis frameworks and other purposes.
The representationis highly extensible.
This representation scheme, based on the blackboard architectural model,allows a very wide variety of linguistic and non-linguistic information to be stored in one placeand operated upon by any number of processing modules.
We show how it has been successfullyused for building machine translation systems for several language pairs using the same architec-ture.
It has also been used for creation of language resources such as treebanks and for differentkinds of annotation interfaces.
There is even a query language designed for this representation.Easily wrappable into XML, it can be used equally well for distributed computing.1 IntroductionBuilding infrastructures for human language technology is a non-trivial task.
There can be numerousissues that have to be addressed, whether linguistic or non-linguistic.
Unless carefully managed, theoverall complexity can easily get out of control and seriously threaten the sustainability of the system.This may apply to all large software systems, but the complexities associated with humans languages(both within and across languages) only add to the problem.
To make it possible to build various compo-nents of an infrastructure that scales within and across languages for a wide variety of purposes, and tobe able to do it by re-using the representation(s) and the code, deserves to be considered an achievement.GATE1(Cunningham et al., 2011; Li et al., 2009), UIMA2(Ferrucci and Lally, 2004; Bari et al., 2013;Noh and Pad?o, 2013) and NLTK3(Bird, 2002) are well known achievements of this kind.
This paper isabout one other such effort that has proved to be successful over the last decade or more.2 Related WorkGATE is designed to be an architecture, a framework and a development environment, quite like UIMA,although the two differ in their realization of this goal.
It enables users to develop and deploy robustlanguage engineering components and resources.
It also comes bundled with several commonly usedbaseline Natural Language Processing (NLP) applications.
It makes strict distinction between data, al-gorithms, and ways of visualising them, such that algorithms + data + GUI = applications.
Consequently,it has three types of components: language resources, processing resources and visual resources.GATEuses an annotation format with stand-off markup.UIMA is a middleware architecture for processing unstructured information (UIM) (Ferrucci andLally, 2004), with special focus on NLP.
Its development originated in the realization that the abilityto quickly discover each other?s results and rapidly combine different technologies and approaches ac-celerates scientific advance.
It has powerful search capabilities and a data-driven framework for theThis work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedingsfooter are added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/1http://gate.ac.uk/2https://uima.apache.org/3http://www.nltk.org/66development, composition and distributed deployment of analysis engines.
More than the developmentof independent UIM applications, UIMA aims to enable accelerated development of integrated and ro-bust applications, combining independent applications in diverse sub-areas of NLP, so as to acceleratethe research cycle as well as the production time.
In UIMA, The original document and its analysis arerepresented in a structure called the Common Analysis Structure, or CAS.
Annotations in the CAS aremaintained separately from the document itself to allow greater flexibility than inline markup.
Thereis an XML specification for the CAS and it is possible to develop analysis engines that operate on andoutput data in this XML format, which also (like GATE and NLTK) uses stand-off markup.The Natural Language Toolkit (NLTK) is a suite of modules, data sets and tutorials (Bird, 2002).
Itsupports many NLP data types and can process many NLP tasks.
It has a rich collection of educationalmaterial (such as animated algorithms) for those who are learning NLP.
It can also be used as a platformfor prototyping of research systems.SSF and the Shakti Analyzer are similar to the above three but have a major difference when com-pared with them.
SSF is a ?powerful?
notation for representing the NLP analysis, at all stages, whethermorphological, part-of-speech level, chunk level, or sentence level parse.
The notation is so designedthat it is flexible, as well as readable.
The notation can be read by human beings and can also be loadedin memory, so that it can be used efficiently.
It also allows the architecture to consist of modules whichcan be configured easily under different settings.
The power of the notation and the flexibility of theresulting architecture gives enormous power to the system framework.The readability of the format allows it to be used directly with any plain text editors, without requiringthe use of any special tools or editors.
Many users prefer the data in plain text format as it allows them touse the editors they are familiar with.
Such readability and simplicity has turned out, in our experience,to be an advantage even for experts like software developers and (computer savvy) linguists.It would be an interesting exercise to marry SSF notation and the Shakti way of doing things with theGATE and UIMA architecture.
Our own feeling is that the resulting system/framework with a powerfulnotation like SSF and the comprehensive framework like UIMA/GATE would lead to a new even morepowerful framework with a principled notation.3 Shakti Standard FormatShakti Standard Format (SSF) is a representation scheme (along with a corresponding format) that can beused for most kinds of linguistically analyzed data.
It allows information in a sentence to be representedin the form of one or more trees together with a set of attribute-value pairs with nodes of the trees.
Theattribute-value pairs allow features or properties to be specified with every node.
Relations of differenttypes across nodes can also be specified using an attribute-value like representation.
The representation isspecially designed to allow different levels and kinds of linguistic analyses to be stored.
The developersuse APIs to store or access information regarding structure of trees and attribute-value pairs.If a module is successful in its task, it adds a new analysis using trees and attribute values to therepresentation.
Thus, even though the format is fixed, it is extensible in terms of attributes or analyses.This approach allows ready-made packages (such as, POS tagger, chunker, and parser) to be incorporatedeasily using a wrapper (or a pair of converters).
In order to interface such pre-existing packages to thesystem, all that is required is to convert from (input) SSF to the input format required by that package and,the output of the package to SSF.
The rest of the modules of the system continue to operate seamlessly.The format allows both in-memory representation as well as stream (or text) representation.
They areinter-convertible using a reader (stream to memory) and printer (memory to stream).
The in-memoryrepresentation is good in speed of processing, while the stream is good for portability, heterogenousmachines, and flexibility, in general.SSF promotes the dictum: ?Simplify globally, and if unavoidable, complicate only locally.?
Even ifthe number of modules is large and each module does a small job, the local complexity (of individualmodules) remains under tight control for most of the modules.
At worst, complexity is introduced onlylocally, without affecting the global simplicity.673.1 Text Level SSFIn SSF, a text or a document has a sequence of sentences with some structure such as paragraphs andheadings.
It also includes meta information related to title, author, publisher, year and other informationrelated to the origin of the text or the document.
Usually, there is also the information related to encoding,and version number of the tagging scheme, etc.
The text level SSF has two parts, header and body:Figure 1: Document Structure in SSF<document docid="..." docnumber="..."><header>...</header><body>...</body>The header contains meta information about the title, author, publisher, etc.
as contained in the CML(Corpus Markup Language) input4.
The body contains sentences, each in SSF.
The body of a text in SSFcontains text blocks given by the tag tb.<body encode= ... ><tb>...</tb>...</body>A text block (tb) contains a sequence of sentences.
Each sentence can be marked as a segment (toindicate a heading, a partial sentence, etc.)
or not a segment (to indicate a normal sentence).3.2 Sentence Level SSFSeveral formalisms have been developed for such descriptions, but the two main ones in the field ofNLP are Phrase Structure Grammar (PSG) (Chomsky, 1957) and Dependency Grammar (DG) (Tesniere,1959).
In PSG, a set of phrase structure rules are given for the grammar of a language.
It is constituencybased and order of elements are a part of the grammar, and the resulting tree.
DG, on the other hand, isrelational and shows relations between words or elements of a sentence.
It, usually, tries to capture thesyntactico-semantic relations of the elements in a sentence.
The resulting dependency tree is a tree withnodes and edges being labelled.The difference in the two approaches are shown below with the help of the following English example:Ram ate the banana.The phrase structure tree is drawn in Fig.
2 using a set of phrase structure rules.
Fig.
3 shows thedependency tree representation for this sentence.
SSF can represent both these formats.4Thus SSF becomes a part of CML.68Figure 2: Phrase structure tree Figure 3: Dependency treeSentence level SSF is used to store the analysis of a sentence.
It occurs as part of text level SSF.
Theanalysis of a sentence may mark any or all of the following kinds of information as appropriate: partof speech of the words in the sentence; morphological analysis of the words including properties suchas root, gender, number, person, tense, aspect, modality; phrase-structure or dependency structure of thesentence; and properties of units such as chunks, phrases, local word groups, bags, etc.
Note that SSFis theory neutral and allows both phrase structure as well as dependency structure to be coded, and evenmixed in well defined ways.Though the format in SSF is fixed, it is extensible to handle new features.
It also has a text represen-tation, which makes it easy to read the output.
The following example illustrates the SSF.
For example,the following English sentence,Children are watching some programmes on television in the house.
-- (1)The representation for the above sentence is shown in SSF in Fig.
4.
As shown in this figure, each linerepresents a word/token or a group (except for lines with ?))?
which only indicate the end of a group).For each group, the symbol used is ?((?.
Each word or group has 3 parts.
The first part stores the treeaddress of each word or group, and is for human readability only.
The word or group is in the secondpart, with part of speech tag or group/phrase category in the third part.Address Token Category Attribute-value pairs-----------------------------------------------1 (( NP1.1 children NNS <fs af=child,n,m,p,3,0,,>))2 (( VG2.1 are VBP <fs af=be,v,m,p,3,0,,>2.2 watching VBG <fs af=?watch,v,m,s,3,0,,?
aspect=PROG>))3 (( NP3.1 some DT <fs af=some,det,m,s,3,0,,>3.2 programmes NNS <fs af=programme,n,m,p,3,0,,>))4 (( PP4.1 on IN <fs af=on,p,m,s,3,0,,>4.1.1 (( NP4.1.2 television NN <fs af=television,n,m,s,3,0,,>))))5 (( PP5.1 in IN <fs af=in,p,m,s,3,0,,>5.2 (( NP5.2.1 the DT <fs af=the,det,m,s,3,0,,>5.2.2 house NN <fs af=house,n,m,s,3,0,,>))))-----------------------------------------------Figure 4: Shakti Standard Format69The example below shows the SSF for the first noun phrase where feature information is also shown, asthe fourth part on each line.
Some frequently occurring attributes (such as root, cat, gend, etc.)
may beabbreviated using a special attribute called ?af?
or abbreviated attributes, as follows:1 (( NP1.1 children NNS <fs af=?child,n,m,p,3,0,,?
>| | | | | || | | | | \root | | |pers || | | casecategory | number|genderThe field for each attribute is at a fixed position, and a comma is used as a separater.
Thus, in case novalue is given for a particular attribute, the field is left blank, e.g.
last two fields in the above example.Corresponding to the above SSF text stream, an in-memory data structure may be created using theAPIs.
(However, note that value of the property Address is not stored in the in-memory data structureexplicitly.
It is for human reference and readability only, and is computed when needed.
A unique name,however can be assigned to a node and saved in the memory, as mentioned later.
)There are two types of attributes: user defined or system defined.
The convention that is used is thata user defined attribute should not have an underscore at the end.
System attribute may have a singleunderscore at its end.Values are of two types: simple and structured.
Simple values are represented by alphanumeric strings,with a possible underscore.
Structured values have progressively more refined values separated by doubleunderscores.
For example, if a value is:vmod__varg__k1it shows the value as ?vmod?
(modifier of a verb), which is further refined as ?varg?
(argument of theverb) of type ?k1?
(karta karaka).3.3 Interlinking of NodesNodes might be interlinked with each other through directed edges.
Usually, these edges have nothingto do with phrase structure tree, and are concerned with dependency structure, thematic structure, etc.These are specified using the attribute value syntax, however, they do not specify a property for a node,rather a relation between two nodes.For example, if a node is karta karaka of another node named ?play1?
in the dependency structure (inother words, if there is a directed edge from the latter to the former) it can be represented as follows:1 children NN < fs drel =?k1 : play1?>2 played VB < fs name = play1 >The above says that there is an edge labelled with ?k1?
from ?played?
to ?children?
in the ?drel?
tree(dependency relation tree).
The node with token ?played?
is named as ?play1?
using a special attributecalled ?name?.So the syntax is as follows: if you associate an arc with a node C as follows:<treename>=<edgelabel>:<nodename>it means that there is an edge from < nodename > to C, and the edge is labelled with < edgelabel >.Name of a node may be declared with the attribute ?name?
:name=<nodename>3.4 Cross Linking across SentencesThere is a need to relate elements across sentences.
A common case is that of co-reference of pronouns.For example, in the following sentences:Sita saw Ram in the house.
He had come all by himself.
-- (2)70the pronoun ?he?
in the second sentence refers to the same person as referred to by ?Ram?.
Similarly?himself?
refers to same person as ?he?
refers to.
This is show by means of a co-reference link from ?he?to ?Ram?, and from ?himself?
to ?he?.
SSF allows such cross-links to be marked.The above text of two sentences is shown in SSF below.<document docid="gandhi-324" docnumber="2"><header> ... </header><body><tb><sentence num=1>...2 Ram <fs name=R>...</sentence><sentence num=2>1 He <fs coref="..%R" name=he>...6 himself <fs coref=he>7 .</sentence></tb>Note that ?himself?
in sentence 2 co-refers to ?he?
in the same sentence.
This is shown using attribute?coref?
and value ?he?.
To show co-reference across sentences, a notation is used with ?%?.
It is explainednext.Name labels are defined at the level of a sentence: Scope of any name label is a sentence.
It should beunique within a sentence, and can be referred to within the sentence by using it directly.To refer to a name label in another sentence in the same text block (paragraph), path has to be specified:..%RTo refer to a name label R in a sentence in another text block numbered 3, refer to it as:..%..%3%1%R4 Shakti Natural Language AnalyzerShakti Analyzer has been designed for analyzing natural languages.
Originally, it was available foranalyzing English as part of the Shakti5English-Hindi machine translation system.
It has now beenextended for analyzing a number of Indian languages as mentioned later (Section-6.1).The Shakti Analyzer can incorporate new modules as black boxes or as open-source software.
Thesimplicity of the overall architecture makes it easy to do so.
Different available English parsers havebeen extensively adapted, and the version used by Shakti system runs using Collins parser.Shakti analyzer combines rule-based approach with statistical approach.
The SSF representation isdesigned to keep both kinds of information.
The rules are mostly linguistic in nature, and the statisticalapproach tries to infer or use linguistic information.
For example, statistical POS tagger tries to inferlinguistic (part-of-speech) tags, whereas WSD module uses grammatical relations together with statisticsto disambiguate the word sense.The system has a number of innovative design principles which are described below.4.1 System Organization PrinciplesA number of system organization principles have been used which have led to the rapid development ofthe system.
While the principles by themselves might not appear to be new, their application is perhapsnew.4.1.1 ModularityThe system consists of a large number of modules, each one of which typically performs a small logicaltask.
This allows the overall machine translation task to be broken up into a large number of small sub-tasks, each of which can be accomplished separately.
Currently the system (as used in the Shakti system)5http:/shakti.iiit.ac.in71has 69 different modules.
About 9 modules are used for analyzing the source language (English), 24modules are used for performing bilingual tasks such as substituting target language roots and reorderingetc., and the remaining modules are used for generating target language.4.1.2 Simplicity of OrganizationThe overall system architecture is kept extremely simple.
All modules operate on data in SSF .
Theycommunicate with each other via SSF.The attribute value pairs allow features or properties to be specified with every node.
Relations ofdifferent types across nodes can also be specified using an attribute-value like representation.
The repre-sentation is specially designed to allow different levels and kinds of linguistic analyses to be stored.
Thedeveloper uses APIs to store or access information regarding structure of trees and attribute value pairs.4.1.3 Designed to Deal with FailureNLP analysis modules are known to have limited coverage.
They are not always able to produce an out-put.
They fail to produce output either because of limits of the best known algorithms or incompletenessof data or rules.
For example, a sentential parser might fail to parse either because it does not know howto deal with a construction or because a dictionary entry is missing.
Similarly, a chunker or part of speechtagger might fail, at times, to produce an analysis.
The system is designed to deal with failure at everystep in the pipeline.
This is facilitated by a common representation for the outputs of the POS tagger,chunker and parser (all in SSF).
The downstream modules continue to operate on the data stream, albeitless effectively, when a more detailed analysis is not available.
(If all modules were to fail, a default ruleof no-reordering and dictionary lookup would still be applied.
)As another example, if the word sense disambiguation (WSD) module fails to identify the sense of aword in the input sentence, it does not put in the sense feature for the word.
This only means that themodule which substitutes the target language root from the available equivalents from dictionary, willuse a default rule for selecting the sense because the detailed WSD was not successful (say, due to lackof training data).The SSF is designed to represent partial information, routinely.
Appropriate modules know what todo when their desired information is available and use defaults when it is not available.
In fact, for manymodules, there are not just two but several levels at which they operate, depending on availability ofinformation corresponding to that level.
Each level represents a graceful degradation of output quality.The above flexibility is achieved by using two kinds of representation: constituent level representationand feature-structure level representation.
The former is used to store phrase level analysis (and partialparse etc.)
and the latter for outputs of many kinds of other tasks such as WSD, TAM computation, casecomputation, dependency relations, etc.4.1.4 Transparency for DevelopersAn extremely important characteristic for the successful development of complex software such as a ma-chine translation system is to expose the input and output produced by every module.
This transparencybecomes even more important in a research environment where new ideas are constantly being tried witha high turnover of student developers.In the Shakti system, unprecedented transparency is achieved by using a highly readable textual nota-tion for the SSF, and requiring every module to produce output in this format.
In fact, the textual SSFoutput of a module is not only for the human consumption, but is used by the subsequent module inthe data stream as its input.
This ensures that no part of the resulting analysis is left hidden in someglobal variables; all analysis is represented in readable SSF (otherwise it is not processed at all by thesubsequent modules).Experience has shown that this methodology has made debugging as well as the development of thesystem convenient for programmers and linguists alike.
In case an output is not as expected, one canquickly find out which module went wrong (that is, which module did not function as expected).
In fact,linguists are using the system quite effectively to debug their linguistic data with ease.725 ImplementationsA considerable repository of implementations (in code) has evolved around SSF and the analyzer.
In thissection we consider two of the kinds of implementations that have accumulated so far.5.1 SSF APIApplication Programming Interfaces (APIs) have been implemented in multiple programming languagesto allow programmers to transparently operate on any data stored in SSF.
Of these, the better designedAPIs, such as those in Perl and Java, allow all kinds of operations to be performed on the SSF data.These operation include basic operations such as reading, writing and modifying the data, as well asfor advanced operations such as search and bulk transformation of the data.
The Java API is a part ofSanchay6, which is a collection of tools and APIs for language processing, specially tailored for theneeds of Indian languages which were not (till very recently) well supported on computers and operatingsystems.The availability of decently designed APIs for SSF allow programmers to use SSF for arbitrary pur-poses.
And they have used it successfully to build natural language systems and tools as described below.5.2 Sanchay Corpus Query LanguageTrees have a quite constrained structure, whereas graphs have somewhat anarchic structure.
Threadedtrees (Ait-Mokhtar et al., 2002; Larchevelque, 2002) provided a middle ground between the two.
Theystart with trees as the core structure, but they allow constrained links between the nodes of a tree that apure tree would not allow.
This overlaying of constrained links over the core trees allows multiple layersand/or types of annotation to be stored in the same structure.
With a little more improvisation, we caneven have links across sentences, i.e., at the discourse level (see section-3.3).
It is possible, for example,to have a phrase structure tree (the core tree) overlaid with a dependency tree (via constrained links or?threads?
), just as it is possible to have POS tagged and chunked data to be overlaid with named entitiesand discourse relations.The Sanchay Corpus Query Language (SCQL) (Singh, 2012) is a query language designed forthreaded trees.
It so turns out that SSF is also a representation that can be viewed as threaded trees.Thus, the SCQL can work over data in SSF.
This language has a simple, intuitive and concise syntax andhigh expressive power.
It allows not only to search for complicated patterns with short queries but alsoallows data manipulation and specification of arbitrary return values.
Many of the commonly used tasksthat otherwise require writing programs, can be performed with one or more queries.6 Applications6.1 Sampark Machine Translation ArchitectureOvercoming the language barrier in the Indian sub-continent is a very challenging task7.
Sampark8isan effort in this direction.
Sampark has been developed as part of the consortium project called IndianLanguage to India Language Machine translation (ILMT) funded by TDIL program of Department of In-formation Technology, Government of India.
Work on this project is contributed to by 11 major researchcentres across India working on Natural Language Processing.Sampark, or the ILMT project, has developed language technology for 9 Indian languages resultingin MT for 18 language pairs.
These are: 14 bi-directional systems between Hindi and Urdu / Punjabi /Telugu / Bengali / Tamil / Marathi / Kannada and 4 bi-directional systems between Tamil and Malayalam/ Telugu.
Out of these, 8 pairs have been exposed via a web interface.
A REST API is also available toacess the machine translation system over the Internet.6http://sanchay.co.in7There are 22 constitutionally recognized languages in India, and many more which are not recognized.
Hindi, Bengali,Telugu, Marathi, Tamil and Urdu are among the major languages of the world in terms of number of speakers, summing up toa total of 850 million.8http://sampark.org.in73The Sampark system uses Computational Paninian Grammar (CPG) (Bharati et al., 1995), in combina-tion with machine learning.
Thus, it is a hybrid system using both rule-based and statistical approaches.There are 13 major modules that together form a hybrid system.
The machine translation system is basedon the analyze-transfer-generate paradigm.
It starts with an analysis of the source language sentence.Then a transfer of structure and vocabulary to target language is carried out.
Finally the target languageis generated.
One of the benefits of this approach is that the language analyzer for a particular languagecan be developed once and then be combined with generators for other languages, making it easier tobuild a machine translation system for new pairs of languages.Indian languages have a lot of similarities in grammatical structures, so only shallow parsing was foundto be adequate for the purposes of building a machine translation system.
Transfer grammar componenthas also been kept simple.
Domain dictionaries are used to cover domain specific aspects.At the core of the Sampark architecture is an enhanced version of the Shakti Natural Language Ana-lyzer.
The individual modules may, of course, be different for different language pairs, but the pipelinedarchitecture bears close resemblance to the Shakti machine translation system.
And it uses the ShaktiStandard Format as the blackboard (Erman et al., 1980) on which the different modules (POS taggers,chunkers, named entity recognzier, transfer grammar module etc.)
operate, that is, read from and writeto.
SSF thus becomes the glue that ties together all the modules in all the MT systems for the variouslanguage pairs.
The modules are not only written in different programming languages, some of them arerule-based, whereas others are statistical.The use of SSF as the underlying default representation helps to control the complexity of the overallsystem.
It also helps to achieve unprecedented transparency for input and output for every module.Readability of SSF helps in development and debugging because the input and output of any modulecan be easily seen and read by humans, whether linguists or programmers.
Even if a module fails, SSFhelps to run the modules without any effect on normal operation of system.
In such a case, the outputSSF would have unfilled value of an attribute and downstream modules continue to operate on the datastream.6.2 Annotation Interfaces and Other ToolsSanchay, mentioned above, has a syntactic annotation interface that has been used for development oftreebanks for Indian languages (Begum et al., 2008).
These treebanks have been one of the primarysources of information for the development the Sampark machine translation systems, among otherthings.
This syntactic annotation interface provides facilities for everything that is required to be doneto transform the selected data in the raw text format to the final annotated treebank.
The usual stages ofannotation include POS tagging, morphological annotation, chunking and dependency annotation.
Thisinterface has evolved over a period of several years based on the feedback received from the annotatorsand other users.
There are plans to use the interface for similar annotation for even more languages.The underlying default format used in the above interface is SSF.
The advantages of using SSF for thispurpose are similar to those mentioned earlier for purposes such as building machine translation systems.The complete process of annotation required to build a full-fledged treebank is complicated and there arenumerous issues that have to be taken care of.
The blackboard-like nature of SSF allows for a smoothshifts between different stages of annotation, even going back to an earlier stage, if necessary, to correctmistakes.
It allows all the annotation information to be situated in one contiguous place.The interface uses the Java API for SSF, which is perhaps the most developed among the differentAPIs for SSF.
The API (a part of Sanchay) again allows transparency for the programmer as far asmanipulating the data is concerned.
It also ensures that there are fewer bugs when new programmerswork on any part of the system where SSF data is being used.
One recent addition to the interface was aGUI to correct mistakes in treebanks (Agarwal et al., 2012).The syntactic annotation interface is not the only interface in Sanchay that uses SSF.
Some otherinterfaces do that too.
For example, there are sentence alignment and word alignment interfaces, whichalso use the same format for similar reasons.
Thus, it is even possible to build parallel treebanks in SSFusing the Sanchay interfaces.74Then there are other tools in Sanchay such as the integrated tool for accessing language re-sources (Singh and Ambati, 2010).
This tool allows various kinds of language resources, including thosein SSF, to be accessed, searched and manipulated through the inter-connected annotation interfaces andthe SSF API.
There is also a text editor in Sanchay that is specially tailored for Indian languages and itcan validate SSF (Singh, 2008).The availability of a corpus query language (section-5.2) that is implemented in Sanchay and that canbe used for data in SSF is another big facilitator for anyone who wants to build new tools for languageprocessing and wants to operate on linguistic data.Apart from these, a number of research projects have used SSF (the representation or the analyzer)directly or indirectly, that is, either for theoretical frameworks or as part of the implementation (Bharatiet al., 2009; Gadde et al., 2010; Husain et al., 2011).7 ConclusionWe described a readable representation scheme called Shakti Standard Format (SSF).
We showed howthis scheme (an instance of the blackboard architectural model), which is based on certain organizationalprinciples such as modularity, simplicity, robustness and transparency, can be used to create not onlya linguistic analysis engine (Shakti Natural Language Analyzer), but can be used for arbitrary otherpurposes wherever linguistic analysis is one of the tasks.
We briefly described the machine translationsystems (Shakti and Sampark) which use this scheme at their core level.
Similarly, we described howit can be used for creation of language resources (such as treebanks) and the annotation interfaces usedto create these resources.
It has also figured in several research projects so far.
We mentioned onequery language (Sanchay Corpus Query Language) that operates on this representation scheme and hasbeen integrated with the annotation interfaces.
Overall, the representation scheme has been successful atbuilding infrastructure for language technology over the last more than a decade.
The scheme is theoryneutral and can be used for both phrase structure grammar and for dependency grammar.ReferencesRahul Agarwal, Bharat Ram Ambati, and Anil Kumar Singh.
2012.
A GUI to Detect and Correct Errors in HindiDependency Treebank.
In Proceedings of the Eighth International Conference on Language Resources andEvaluation (LREC), Instanbul, Turkey.
ELRA.S.
Ait-Mokhtar, J.P. Chanod, and C. Roux.
2002.
Robustness beyond shallowness: incremental deep parsing.Natural Language Engineering, 8(2-3):121144, January.Alessandro Di Bari, Alessandro Faraotti, Carmela Gambardella, and Guido Vetere.
2013.
A Model-driven ap-proach to NLP programming with UIMA.
In UIMA@GSCL, pages 2?9.Rafiya Begum, Samar Husain, Arun Dhwaj, Dipti Misra Sharma, Lakshmi Bai, and Rajeev Sangal.
2008.
Depen-dency Annotation Scheme for Indian Languages.
In Proceedings of The Third International Joint Conferenceon Natural Language Processing (IJCNLP), Hyderabad, India.Ashkar Bharati, Vineet Chaitanya, and Rajeev Sangal.
1995.
Natural Language Processing: A Paninian Perspec-tive.
Prentice-Hall of India Pvt.
Ltd.Akshar Bharati, Samar Husain, Phani Gadde, Bharat Ambati, Dipti M Sharma, and Rajeev Sangal.
2009.
AModular Cascaded Approach to Complete Parsing.
In Proceedings of the COLIPS International Conference onAsian Language Processing 2009 (IALP), Singapore.Steven Bird.
2002.
NLTK: The Natural Language Toolkit.
In In Proceedings of the ACL Workshop on EffectiveTools and Methodologies for Teaching Natural Language Processing and Computational Linguistics.
Philadel-phia: Association for Computational Linguistics.Noam Chomsky.
1957.
Syntactic Structures.
The Hague/Paris: Mouton.Hamish Cunningham, Diana Maynard, Kalina Bontcheva, Valentin Tablan, Niraj Aswani, Ian Roberts, GenevieveGorrell, Adam Funk, Angus Roberts, Danica Damljanovic, Thomas Heitz, Mark A. Greenwood, Horacio Sag-gion, Johann Petrak, Yaoyong Li, and Wim Peters.
2011.
Text Processing with GATE (Version 6).75Lee D. Erman, Frederick Hayes-Roth, Victor R. Lesser, and D. Raj Reddy.
1980.
The Hearsay-II Speech-Understanding System: Integrating Knowledge to Resolve Uncertainty.
ACM Comput.
Surv., 12(2):213?253,June.D.
Ferrucci and A. Lally.
2004.
UIMA: an architectural approach to unstructured information processing in thecorporate research environment.
Natural Language Engineering, 10(3-4):327?348.Phani Gadde, Karan Jindal, Samar Husain, Dipti Misra Sharma, and Rajeev Sangal.
2010.
Improving DataDriven Dependency Parsing using Clausal Information.
In Proceedings of 11th Annual Conference of the NorthAmerican Chapter of the Association for Computational Linguistics (NAACL-HLT), Los Angeles.Samar Husain, Phani Gadde, Joakim Nivre, and Rajeev Sangal.
2011.
Clausal Parsing Helps Data-driven De-pendency Parsing: Experiments with Hindi.
In Proceedings of Fifth International Joint Conference on NaturalLanguage Processing (IJCNLP), Thailand.J.M.
Larchevelque.
2002.
Optimal Incremental Parsing.
ACM Transactions on Programing Languages andSystems, 17(1):115, January.Yaoyong Li, Kalina Bontcheva, and Hamish Cunningham.
2009.
Adapting SVM for Data Sparseness and Imbal-ance: A Case Study on Information Extraction.
Natural Language Engineering, 15(2):241?271.Tae-Gil Noh and Sebastian Pad?o.
2013.
Using UIMA to Structure An Open Platform for Textual Entailment.
InUIMA@GSCL, pages 26?33.Anil Kumar Singh and Bharat Ambati.
2010.
An Integrated Digital Tool for Accessing Language Resources.
InProceedings of the Seventh International Conference on Language Resources and Evaluation (LREC), Malta.ELRA.Anil Kumar Singh.
2008.
A Mechanism to Provide Language-Encoding Support and an NLP Friendly Editor.
InProceedings of the Third International Joint Conference on Natural Language Processing (IJCNLP), Hyder-abad, India.
AFNLP.Anil Kumar Singh.
2012.
A Concise Query Language with Search and Transform Operations for Corpora withMultiple Levels of Annotation.
In Proceedings of the Eighth International Conference on Language Resourcesand Evaluation (LREC), Instanbul, Turkey.
ELRA.L.
Tesniere.
1959.
Elements de syntaxe structurale.
Paris: Klincksieck.76
