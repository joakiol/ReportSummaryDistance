Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 465?472,Sydney, July 2006. c?2006 Association for Computational LinguisticsA Hybrid Markov/Semi-Markov Conditional Random Fieldfor Sequence SegmentationGalen AndrewMicrosoft ResearchOne Microsoft WayRedmond, WA 98052galena@microsoft.comAbstractMarkov order-1 conditional random fields(CRFs) and semi-Markov CRFs are twopopular models for sequence segmenta-tion and labeling.
Both models have ad-vantages in terms of the type of featuresthey most naturally represent.
We pro-pose a hybrid model that is capable of rep-resenting both types of features, and de-scribe efficient algorithms for its trainingand inference.
We demonstrate that ourhybrid model achieves error reductions of18% and 25% over a standard order-1 CRFand a semi-Markov CRF (resp.)
on thetask of Chinese word segmentation.
Wealso propose the use of a powerful fea-ture for the semi-Markov CRF: the logconditional odds that a given token se-quence constitutes a chunk according toa generative model, which reduces errorby an additional 13%.
Our best systemachieves 96.8% F-measure, the highest re-ported score on this test set.1 IntroductionThe problem of segmenting sequence data intochunks arises in many natural language applica-tions, such as named-entity recognition, shallowparsing, and word segmentation in East Asian lan-guages.
Two popular discriminative models thathave been proposed for these tasks are the condi-tional random field (CRFs) (Lafferty et al, 2001)and the semi-Markov conditional random field(semi-CRF) (Sarawagi and Cohen, 2004).A CRF in its basic form is a model for label-ing tokens in a sequence; however it can easilybe adapted to perform segmentation via labelingeach token as BEGIN or CONTINUATION, or accord-ing to some similar scheme.
CRFs using this tech-nique have been shown to be very successful at thetask of Chinese word segmentation (CWS), start-ing with the model of Peng et al (2004).
In theSecond International Chinese Word SegmentationBakeoff (Emerson, 2005), two of the highest scor-ing systems in the closed track competition werebased on a CRF model.
(Tseng et al, 2005; Asa-hara et al, 2005)While the CRF is quite effective compared withother models designed for CWS, one wonderswhether it may be limited by its restrictive inde-pendence assumptions on non-adjacent labels: anorder-M CRF satisfies the order-M Markov as-sumption that, globally conditioned on the inputsequence, each label is independent of all otherlabels given the M labels to its left and right.Consequently, the model only ?sees?
word bound-aries within a moving window of M + 1 charac-ters, which prohibits it from explicitly modelingthe tendency of strings longer than that windowto form words, or from modeling the lengths ofthe words.
Although the window can in principlebe widened by increasing M , this is not a practi-cal solution as the complexity of training and de-coding a linear sequence CRF grows exponentiallywith the Markov order.The semi-CRF is a sequence model that is de-signed to address this difficulty via careful relax-ation of the Markov assumption.
Rather than re-casting the segmentation problem as a labelingproblem, the semi-CRF directly models the dis-tribution of chunk boundaries.1 In terms of inde-1As it was originally described, the semi-CRF also as-signs labels to each chunk, effectively performing joint seg-mentation and labeling, but in a pure segmentation problemsuch as CWS, the use of labels is unnecessary.465pendence, using an order-M semi-CRF entails theassumption that, globally conditioned on the inputsequence, the position of each chunk boundary isindependent of all other boundaries given the po-sitions of the M boundaries to its left and rightregardless of how far away they are.
Even with anorder-1 model, this enables several classes of fea-tures that one would expect to be of great utilityto the word segmentation task, in particular wordlength and word identity.Despite this, the only work of which we areaware exploring the use of a semi-Markov CRFfor Chinese word segmentation did not find signif-icant gains over the standard CRF (Liang, 2005).This is surprising, not only because the additionalfeatures a semi-CRF enables are intuitively veryuseful, but because as we will show, an order-Msemi-CRF is strictly more powerful than an or-der-M CRF, in the sense that any feature that canbe used in the latter can also be used in the for-mer, or equivalently, the semi-CRF makes strictlyweaker independence assumptions.
Given a judi-cious choice of features (or simply enough trainingdata) the semi-CRF should be superior.We propose that the reason for this discrepancymay be that despite the greater representationalpower of the semi-CRF, there are some valuablefeatures that are more naturally expressed in aCRF segmentation model, and so they are not typ-ically included in semi-CRFs (indeed, they havenot to date been used in any semi-CRF model forany task, to our knowledge).
In this paper, weshow that semi-CRFs are strictly more expressive,and also demonstrate how CRF-type features canbe used in a semi-CRF model for Chinese wordsegmentation.
Our experiments show that a modelincorporating both types of features can outper-form models using only one or the other type.Orthogonally, we explore in this paper the useof a very powerful feature for the semi-CRF de-rived from a generative model.It is common in statistical NLP to use as fea-tures in a discriminative model the (logarithm ofthe) estimated probability of some event accord-ing to a generative model.
For example, Collins(2000) uses a discriminative classifier for choosingamong the top N parse trees output by a generativebaseline model, and uses the log-probability of aparse according to the baseline model as a featurein the reranker.
Similarly, the machine translationsystem of Och and Ney uses log-probabilities ofphrasal translations and other events as features ina log-linear model (Och and Ney, 2002; Och andNey, 2004).
There are many reasons for incorpo-rating these types of features, including the desireto combine the higher accuracy of a discriminativemodel with the simple parameter estimation andinference of a generative one, and also the fact thatgenerative models are more robust in data sparsescenarios (Ng and Jordan, 2001).For word segmentation, one might want to useas a local feature the log-probability that a segmentis a word, given the character sequence it spans.
Acurious property of this feature is that it inducesa counterintuitive asymmetry between the is-wordand is-not-word cases: the component generativemodel can effectively dictate that a certain chunkis not a word, by assigning it a very low probability(driving the feature value to negative infinity), butit cannot dictate that a chunk is a word, becausethe log-probability is bounded above.2 If insteadthe log conditional odds log Pi(y|x)Pi(?y|x) is used, theasymmetry disappears.
We show that such a log-odds feature provides much greater benefit thanthe log-probability, and that it is useful to includesuch a feature even when the model also includesindicator function features for every word in thetraining corpus.2 Hybrid Markov/Semi-Markov CRFThe model we describe is formally a type of semi-Markov CRF, distinguished only in that it also in-volves CRF-style features.
So we first describe thesemi-Markov model in its general form.2.1 Semi-Markov CRFAn (unlabeled) semi-Markov conditional randomfield is a log-linear model defining the conditionalprobability of a segmentation given an observationsequence.
The general form of a log-linear modelis as follows: given an input x ?
X , an outputy ?
Y , a feature mapping ?
: X ?
Y 7?
Rn, anda weight vector w, the conditional probability ofy given x is estimated as:P (y | x) =exp (w ?
?
(x,y))Z(x)where Z : x 7?
R is a normalizing factor.
wis typically chosen to maximize the conditionallikelihood of a labeled training set.
In the word2We assume the weight assigned to the log-probabilityfeature is positive.466segmentation task, x is an ordered sequence ofcharacters (x1, x2, .
.
.
, xn), and y is a set of in-dices corresponding to the start of each word:{y1, y2, .
.
.
, ym} such that y1 = 1, ym ?
n, andfor all j, yj < yj+1.
A log-linear model in thisspace is an order-1 semi-CRF if its feature map ?decomposes according to?
(x,y) =m?j=1?S(yj , yj+1,x) (1)where ?S is a local feature map that only considersone chunk at a time (defining ym+1 = n+1).
Thisdecomposition is responsible for the characteristicindependence assumptions of the semi-CRF.Hand-in-hand with the feature decompositionand independence assumptions comes the capac-ity for exact decoding using the Viterbi algorithm,and exact computation of the objective gradientusing the forward-backward algorithm, both intime quadratic in the lengths of the sentences.Furthermore, if the model is constrained to pro-pose only chunkings with maximum word lengthk, then the time for inference and training be-comes linear in the sentence length (and in k).
ForChinese word segmentation, choosing a moderatevalue of k does not pose any significant risk, sincethe vast majority of Chinese words are only a fewcharacters long: in our training set, 91% of wordtokens were one or two characters, and 99% werefive characters or less.Using a semi-CRF as opposed to a traditionalMarkov CRF allows us to model some aspectsof word segmentation that one would expect tobe very informative.
In particular, it makes pos-sible the use of local indicator function featuresof the type ?the chunk consists of character se-quence ?1, .
.
.
, ?`,?
or ?the chunk is of length `.
?It also enables ?pseudo-bigram language model?features, firing when a given word occurs in thecontext of a given character unigram or bigram.3And crucially, although it is slightly less naturalto do so, any feature used in an order-1 MarkovCRF can also be represented in a semi-CRF.
AsMarkov CRFs are used in the most competitiveChinese word segmentation models to date, onemight expect that incorporating both types of fea-tures could yield a superior model.3We did not experiment with this type of feature.2.2 CRF vs. Semi-CRFIn order to compare the two types of linear CRFs,it is convenient to define a representation of thesegmentation problem in terms of character labelsas opposed to sets of whole words.
Denote byL(y) ?
{B,C}n (for BEGIN vs.
CONTINUATION)the sequence {L1, L2, .
.
.
Ln} of labels such thatLi = B if and only if yi ?
y.
It is clear that if weconstrain L1 = B, the two representations y andL(y) are equivalent.
An order-1 Markov CRF is alog-linear model in which the global feature vector?
decomposes into a sum over local feature vec-tors that consider bigrams of the label sequence:?
(x,y) =n?i=1?M (Li, Li+1, i,x) (2)(where Ln+1 is defined as B).
The local featuresthat are most naturally expressed in this contextare indicators of some joint event of the label bi-gram (Li, Li+1) and nearby characters in x. Forexample, one might use the feature ?the currentcharacter xi is ?
and Li = C?, or ?the current andnext characters are identical and Li = Li+1 = B.?Although we have heretofore disparaged theCRF as being incapable of representing such pow-erful features as word identity, the type of featuresthat it most naturally represents should be help-ful in CWS for generalizing to unseen words.
Forexample, the first feature mentioned above couldbe valuable to rule out certain word boundaries if?
were a character that typically occurs only as asuffix but that combines freely with a variety ofroot forms to create new words.
This type of fea-ture (specifically, a feature indicating the absenceas opposed to the presence of a chunk boundary)is a bit less natural in a semi-CRF, since in thatcase local features ?S(yj , yj+1,x) are defined onpairs of adjacent boundaries.
Information aboutwhich tokens are not on boundaries is only im-plicit, making it a bit more difficult to incorporatethat information into the features.
Indeed, neitherLiang (2005) nor Sarawagi and Cohen (2004) norany other system using a semi-Markov CRF onany task has included this type of feature to ourknowledge.
We hypothesize (and our experimentsconfirm) that the lack of this feature explains thefailure of the semi-CRF to outperform the CRF forword segmentation in the past.Before showing how CRF-type features can beused in a semi-CRF, we first demonstrate that thesemi-CRF is indeed strictly more expressive than467the CRF, meaning that any global feature map ?that decomposes according to (2) also decomposesaccording to (1).
It is sufficient to show that forany feature map ?M of a Markov CRF, there existsa semi-Markov-type feature map ?S such that forany x,y,?M (x,y) =n?i=1?M (Li, Li+1, i,x) (3)=m?j=1?S(yj , yj+1,x) = ?S(x,y)To this end, note that there are only four possiblelabel bigrams: BB, BC, CB, and CC.
As a di-rect result of the definition of L(y), we have that(Li, Li+1) = (B,B) if and only if some word oflength one begins at i, or equivalently, there existsa word j such that yj = i and yj+1?yj = 1.
Sim-ilarly, (Li, Li+1) = (B,C) if and only if someword of length > 1 begins at i, etc.
Using theseconditions, we can define ?S to satisfy equation 3as follows:?S(yj , yj+1,x) = ?M (B,B, yj ,x)if yj+1 ?
yj = 1, and?S(yj , yj+1,x) = ?M (B,C, yj ,x)+yj+1?2?k=yj+1?M (C,C, k,x) (4)+ ?M (C,B, yj+1 ?
1,x)otherwise.
Defined thus,?mj=1 ?S will contain ex-actly n ?M terms, corresponding to the n label bi-grams.42.3 Order-1 Markov Features in a Semi-CRFWhile it is fairly intuitive that any feature used in a1-CRF can also be used in a semi-CRF, the aboveargument reveals an algorithmic difficulty that islikely another reason that such features are not typ-ically used.
The problem is essentially an effect ofthe sum for CC label bigrams in (4): quadratictime training and decoding assumes that the fea-tures of each chunk ?S(yj , yj+1,x) can be multi-plied with the weight vector w in a number of op-erations that is roughly constant over all chunks,4We have discussed the case of Markov order-1, but theargument can be generalized to show that an order-M CRFhas an equivalent representation as an order-M semi-CRF,for any M .procedure ComputeScores(x,w)for i = 2 .
.
.
(n?
1) do?CCi ?
?M (C,C, i,x) ?wend forfor a = 1 .
.
.
n doCCsum?
0for b = (a+ 1) .
.
.
(n + 1) doif b?
a = 1 then?ab ?
?M (B,B, a,x) ?welse?ab ?
?M (B,C, a,x) ?w + CCsum+?M (C,B, b?
1,x) ?wCCsum?
CCsum+ ?CCb?1end ifend forend forFigure 1: Dynamic program for computing chunkscores ?ab with 1-CRF-type features.but if one na?
?vely distributes the product over thesum, longer chunks will take proportionally longerto score, resulting in cubic time algorithms.5In fact, it is possible to use these featureswithout any asymptotic decrease in efficiency bymeans of a dynamic program.
Both Viterbi andforward-backward involve the scores ?ab = w ?
?S(a, b,x).
Suppose that before starting those al-gorithms, we compute and cache the score ?ab ofeach chunk, so that remainder the algorithm runsin quadratic time, as usual.
This pre-computationcan be done quickly if we first compute the values?CCi = w ?
?M (C,C, i,x), and use them to fill inthe values of ?ab as shown in Figure 1.In addition, computing the gradient of the semi-CRF objective requires that we compute the ex-pected value of each feature.
For CRF-type fea-tures, this is tantamount to being able to computethe probability that each label bigram (Li, Li+1)takes any value.
Assume that we have already runstandard forward-backward inference so that wehave for any (a, b) the probability that the subse-quence (xa,xa+1, .
.
.
,xb?1) segments as a chunk,P (chunk(a, b)).
Computing the probability that(Li, Li+1) takes the values BB, BC or CB issimple to compute:P (Li, Li+1 = BB) = P (chunk(i, i+ 1))5Note that the problem would arise even if only zero-orderMarkov (label unigram) features were used, only in that casethe troublesome features would be those that involved the la-bel unigram C.468and, e.g.,P (Li, Li+1 = BC) =?j>i+1P (chunk(i, j)),but the same method of summing over chunks can-not be used for the value CC since for each labelbigram there are quadratically many chunks cor-responding to that value.
In this case, the solutionis deceptively simple: using the fact that for anygiven label bigram, the sum of the probabilities ofthe four labels must be one, we can deduce thatP (Li, Li+1 = CC) = 1.0?
P (Li, Li+1 = BB)?
P (Li, Li+1 = BC)?
P (Li, Li+1 = CB).One might object that features of the C and CClabels (the ones presenting algorithmic difficulty)are unnecessary, since under certain conditions,their removal would not in fact change the expres-sivity of the model or the distribution that maxi-mizes training likelihood.
This will indeed be thecase when the following conditions are fulfilled:1.
All label bigram features are of the form?M (Li,Li+1, i,x) =1{(Li, Li+1) = ?
& pred(i,x)}for some label bigram ?
and predicate pred,and any such feature with a given predicatehas variants for all four label bigrams ?.2.
No regularization is used during training.A proof of this claim would require too muchspace for this paper, but the key is that, given amodel satisfying the above conditions, one canobtain an equivalent model via adding, for eachfeature type over pred, some constant to the fourweights corresponding to the four label bigrams,such that the CC bigram features all have weightzero.In practice, however, one or both of these con-ditions is always broken.
It is common knowl-edge that regularization of log-linear models witha large number of features is necessary to achievehigh performance, and typically in NLP one de-fines feature templates and chooses only those fea-tures that occur in some positive example in thetraining set.
In fact, if both of these conditions arefulfilled, it is very likely that the optimal modelwill have some weights with infinite values.
Weconclude that it is not a practical alternative to omitthe C and CC label features.2.4 Generative Features in a DiscriminativeModelWhen using the output of a generative model asa feature in a discriminative model, Raina et al(2004) provide a justification for the use of logconditional odds as opposed to log-probability:they show that using log conditional odds as fea-tures in a logistic regression model is equivalentto discriminatively training weights for the fea-tures of a Na?
?ve Bayes classifier to maximizeconditional likelihood.6 They demonstrate thatthe resulting classifier, termed a ?hybrid genera-tive/discriminative classifier?, achieves lower testerror than either pure Na?
?ve Bayes or pure logisticregression on a text classification task, regardlessof training set size.The hybrid generative/discriminative classifieralso uses a unique method for using the same dataused to estimate the parameters of the compo-nent generative models for training the discrimina-tive model parameters w without introducing bias.A ?leave-one-out?
strategy is used to choose w,whereby the feature values of the i-th training ex-ample are computed using probabilities estimatedwith the i-th example held out.
The beauty of thisapproach is that since the probabilities are esti-mated according to (smoothed) relative frequency,it is only necessary during feature computation tomaintain sufficient statistics and adjust them asnecessary for each example.In this paper, we experiment with the use ofa single ?hybrid?
local semi-CRF feature, thesmoothed log conditional odds that a given sub-sequence xab = (xa, .
.
.
,xb?1) forms a word:logwordcount(xab) + 1nonwordcount(xab) + 1,where wordcount(xab) is the number of timesxab forms a word in the training set, andnonwordcount(xab) is the number of times xaboccurs, not segmented into a single word.
Themodels we test are not strictly speaking hybridgenerative/discriminative models, since we alsouse indicator features not derived from a genera-tive model.
We did however use the leave-one-outapproach for computing the log conditional oddsfeature during training.6In fact, one more step beyond what is shown in that paperis required to reach the stated conclusion, since their featuresare not actually log conditional odds, but log P (x|y)P (x|?y) .
It issimple to show that in the given context this feature is equiv-alent to log conditional odds.4693 ExperimentsTo test the ideas discussed in this paper, we com-pared the performance of semi-CRFs using vari-ous feature sets on a Chinese word segmentationtask.
The data used was the Microsoft ResearchBeijing corpus from the Second InternationalChinese Word Segmentation Bakeoff (Emerson,2005), and we used the same train/test split used inthe competition.
The training set consists of 87Ksentences of Beijing dialect Chinese, hand seg-mented into 2.37M words.
The test set contains107K words comprising roughly 4K sentences.We used a maximum word length k of 15 in ourexperiments, which accounted for 99.99% of theword tokens in our training set.
The 249 train-ing sentences that contained words longer than 15characters were discarded.
We did not discard anytest sentences.In order to be directly comparable to the Bake-off results, we also worked under the very strict?closed test?
conditions of the Bakeoff, which re-quire that no information or data outside of thetraining set be used, not even prior knowledge ofwhich characters represent Arabic numerals, Latincharacters or punctuation marks.3.1 Features UsedWe divide our main features into two types accord-ing to whether they are most naturally used in aCRF or a semi-CRF.The CRF-type features are indicator functionsthat fire when the character label (or label bigram)takes some value and some predicate of the inputat a certain position relative to the label is satis-fied.
For each character label unigram L at posi-tion i, we use the same set of predicate templateschecking:?
The identity of xi?1 and xi?
The identity of the character bigram startingat positions i?
2, i?
1 and i?
Whether xj and xj+1 are identical, for j =(i?
2) .
.
.
i?
Whether xj and xj+2 are identical, for j =(i?
3) .
.
.
i?
Whether the sequence xj .
.
.xj+3 forms anAABB sequence for j = (i?
4) .
.
.
i?
Whether the sequence xj .
.
.xj+3 forms anABAB sequence for j = (i?
4) .
.
.
iThe latter four feature templates are designed todetect character or word reduplication, a morpho-logical phenomenon that can influence word seg-mentation in Chinese.
The first two of these werealso used by Tseng et al (2005).For label bigrams (Li, Li+1), we use the sametemplates, but extending the range of positionsby one to the right.7 Each label uni- or bigramalso has a ?prior?
feature that always fires forthat label configuration.
All configurations con-tain the above features for the label unigram B,since these are easily used in either a CRF or semi-CRF model.
To determine the influence of CRF-type features on performance, we also test config-urations in which both B and C label features areused, and configurations using all label uni- andbigrams.In the semi-Markov conditions, we also use asfeature templates indicators of the length of a word`, for ` = 1 .
.
.
k, and indicators of the identity ofthe corresponding character sequence.All feature templates were instantiated with val-ues that occur in positive training examples.
Wefound that excluding CRF-type features that occuronly once in the training set consistently improvedperformance on the development set, so we use acount threshold of two for the experiments.
We donot do any thresholding of the semi-CRF features,however.Finally, we use the single generative feature,log conditional odds that the given string formsa word.
We also present results using the moretypical log conditional probability instead of theodds, for comparison.
In fact, these are both semi-Markov-type features, but we single them out todetermine what they contribute over and above theother semi-Markov features.3.2 ResultsThe results of test set runs are summarized in ta-ble 3.2.
The columns indicate which CRF-typefeatures were used: features of only the label B,features of label unigrams B and C, or featuresof all label unigrams and bigrams.
The rows indi-cate which semi-Markov-type features were used:7For both label unigram and label bigram features, the in-dices are chosen so that the feature set exhibits no asymmetrywith respect to direction: for each feature considering someboundary and some property of the character(s) at a givenoffset to the left, there is a corresponding feature consideringthat boundary and the same property of the character(s) at thesame offset to the right, and vice-versa.470Features B only uni uni+binone 92.33 94.71 95.69semi 95.28 96.05 96.46prob 93.86 95.40 96.04semi+prob 95.51 96.24 96.55odds 95.10 96.06 96.40semi+odds 96.27 96.77 96.84Table 1: Test F-measure for different model con-figurations.?semi?
means length and word identity featureswere used, ?prob?
means the log-probability fea-ture was used, and ?odds?
means the log-odds fea-ture was used.To establish the impact of each type of feature(C label unigrams, label bigrams, semi-CRF-typefeatures, and the log-odds feature), we look at thereduction in error brought about by adding eachtype of feature.
First consider the effect of theCRF-type features.
Adding the C label featuresreduces error by 31% if no semi-CRF features areused, by 16% when semi-CRF indicator featuresare turned on, and by 13% when all semi-CRF fea-tures (including log-odds) are used.
Using all labelbigrams reduces error by 44%, 25%, and 15% inthese three conditions, respectively.Contrary to previous conclusions, our resultsshow a significant impact due to the use of semi-CRF-type features, when CRF-type features areheld constant.
Adding semi-CRF indicator fea-tures results in a 38% error reduction withoutCRF-type features, and 18% with them.
Addingsemi-CRF indicator features plus the log-odds fea-ture gives 52% and 27% in these two conditions,respectively.Finally, across configurations, the log condi-tional odds does much better than log condi-tional probability.
When the log-odds feature isadded to the complete CRF model (uni+bi) asthe only semi-CRF-type feature, errors are re-duced by 24%, compared to only 7.6% for the log-probability.
Even when the other semi-CRF-typefeatures are present as well, log-odds reduces errorby 13% compared to 2.5% for log-probability.Our best model, combining all features, resultedin an error reduction of 12% over the highest scoreon this dataset from the 2005 Sighan closed testcompetition (96.4%), achieved by the pure CRFsystem of Tseng et al (2005).3.3 DiscussionOur results indicate that both Markov-type andsemi-Markov-type features are useful for generali-zation to unseen data.
This may be because thetwo types of features are in a sense complemen-tary: semi-Markov-type features such as word-identity are valuable for modeling the tendencyof known strings to segment as words, while la-bel based features are valuable for modeling prop-erties of sub-lexical components such as affixes,helping to generalize to words that have not previ-ously been encountered.
We did not explicitly testthe utility of CRF-type features for improving re-call on out-of-vocabulary items, but we note thatin the Bakeoff, the model of Tseng et al (2005),which was very similar to our CRF-only system(only containing a few more feature templates),was consistently among the best performing sys-tems in terms of test OOV recall (Emerson, 2005).We also found that for this sequence segmenta-tion task, the use of log conditional odds as a fea-ture results in much better performance than theuse of the more typical log conditional probabil-ity.
It would be interesting to see the log-oddsapplied in more contexts where log-probabilitiesare typically used as features.
We have presentedthe intuitive argument that the log-odds may beadvantageous because it does not exhibit the 0-1asymmetry of the log-probability, but it would besatisfying to justify the choice on more theoreticalgrounds.4 Relation to Previous WorkThere is a significant volume of work explor-ing the use of CRFs for a variety of chunkingtasks, including named-entity recognition, geneprediction, shallow parsing and others (Finkel etal., 2005; Culotta et al, 2005; Sha and Pereira,2003).
The current work indicates that these sys-tems might be improved by moving to a semi-CRFmodel.There have not been a large number of studiesusing the semi-CRF, but the few that have beendone found only marginal improvements over pureCRF systems (Sarawagi and Cohen, 2004; Liang,2005; Daume?
III and Marcu, 2005).
Notably,none of those studies experimented with featuresof chunk non-boundaries, as is achieved by the useof CRF-type features involving the label C, andwe take this to be the reason for their not obtain-ing higher results.471Although it has become fairly common in NLPto use the log conditional probabilities of eventsas features in a discriminative model, we are notaware of any work using the log conditional odds.5 ConclusionWe have shown that order-1 semi-Markov condi-tional random fields are strictly more expressivethan order-1 Markov CRFs, and that the addedexpressivity enables the use of features that leadto improvements on a segmentation task.
On theother hand, Markov CRFs can more naturally in-corporate certain features that may be useful formodeling sub-chunk phenomena and generaliza-tion to unseen chunks.
To achieve the best per-formance for segmentation, we propose that bothtypes of features be used, and we show how thiscan be done efficiently.Additionally, we have shown that a log condi-tional odds feature estimated from a generativemodel can be superior to the more common logconditional probability.6 AcknowledgementsMany thanks to Kristina Toutanova for herthoughtful discussion and feedback, and also tothe anonymous reviewers for their suggestions.ReferencesMasayuki Asahara, Kenta Fukuoka, Ai Azuma, Chooi-Ling Goh, Yotaro Watanabe, Yuji Matsumoto, andTakahashi Tsuzuki.
2005.
Combination of machinelearning methods for optimum chinese word seg-mentation.
In Proc.
Fourth SIGHAN Workshop onChinese Language Processing, pages 134?137.Michael Collins.
2000.
Discriminative reranking fornatural language parsing.
In Proc.
14th Interna-tional Conf.
on Machine Learning.Aron Culotta, David Kulp, and Andrew McCallum.2005.
Gene prediction with conditional randomfields.
Technical report, University of Massa-chusetts Dept.
of Computer Science, April.Hal Daume?
III and Daniel Marcu.
2005.
Learningas search optimization: Approximate large marginmethods for structured prediction.
In Proc.
19th In-ternational Conf.
on Machine Learning.Thomas Emerson.
2005.
The second international chi-nese word segmentation bakeoff.
In Proc.
FourthSIGHAN Workshop on Chinese Language Process-ing, pages 123?133.Jenny Finkel, Trond Grenager, and Christopher D.Manning.
2005.
Incorporating non-local informa-tion into information extraction systems by gibbssampling.
Proc.
41th Annual Meeting of the Assi-ciation of Computation Linguistics.John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.
In Proc.
18th International Conf.
onMachine Learning, pages 282?289.
Morgan Kauf-mann, San Francisco, CA.Percy Liang.
2005.
Semi-supervised learning for nat-ural language.
Master?s thesis, Massachusetts Insti-tute of Technology.Andrew Y. Ng and Michael I. Jordan.
2001.
On dis-criminative vs. generative classifiers: A comparisonof logistic regression and Na?
?ve Bayes.
In Proc.
Ad-vances in Neural Information Processing 14.Franz Josef Och and Hermann Ney.
2002.
Discrim-inative training and maximum entropy models forstatistical machine translation.
Proc.
38th AnnualMeeting of the Assiciation of Computation Linguis-tics.Franz Josef Och and Hermann Ney.
2004.
The align-ment template approach to statistical machine trans-lation.
Computational Linguistics, 30(4):417?449,December.Fuchun Peng, Fangfang Feng, and Andrew McCallum.2004.
Chinese segmentation and new word detec-tion using conditional random fields.
In Proc.
20thInternational Conf.
on Computational Linguistics.Rajat Raina, Yirong Shen, Andrew Y. Ng, and AndrewMcCallum.
2004.
Classification with hybrid gen-erative/discriminative models.
In Proc.
Advances inNeural Information Processing 17.Brian Roark and Seeger Fisher.
2005.
OGI/OHSUbaseline multilingual multi-document sumarizationsystem.
In Proc.
Multilingual Summarization Eval-uation in ACL Workshop: Intrinsic and ExtrinsicEvaluation Measures for MT and/or Summarization.Sunita Sarawagi and William Cohen.
2004.
Semi-markov conditional random fields for informationextraction.
In Proc.
18th International Conf.
on Ma-chine Learning.Fei Sha and Fernando Pereira.
2003.
Shallow parsingwith conditional random fields.
Proc.
HLT-NAACL.Huihsin Tseng, Pichuan Chang, Galen Andrew, DanielJurafsky, and Christopher Manning.
2005.
A condi-tional random field word segmenter for sighan bake-off 2005.
In Proc.
Fourth SIGHAN Workshop onChinese Language Processing, pages 168?171.472
