Last WordsAmazon Mechanical Turk:Gold Mine or Coal Mine?Kare?n Fort?INIST-CNRS/LIPNGilles Adda??LIMSI/CNRSK.
Bretonnel Cohen?University of Colorado School of Medicineand University of Colorado at BoulderRecently heard at a tutorial in our field: ?It cost me less than one hundred bucks toannotate this using Amazon Mechanical Turk!?
Assertions like this are increasinglycommon, but we believe they should not be stated so proudly; they ignore the ethicalconsequences of using MTurk (Amazon Mechanical Turk) as a source of labor.Manually annotating corpora or manually developing any other linguistic resource,such as a set of judgments about system outputs, represents such a high cost that manyresearchers are looking for alternative solutions to the standard approach.
MTurk isbecoming a popular one.
However, as in any scientific endeavor involving humans,there is an unspoken ethical dimension involved in resource construction and systemevaluation, and this is especially true of MTurk.We would like here to raise some questions about the use of MTurk.
To do so, wewill define precisely what MTurk is and what it is not, highlighting the issues raisedby the system.
We hope that this will point out opportunities for our community todeliberately value ethics above cost savings.What Is MTurk?
What Is It Not?MTurk is an on-line crowdsourcing, microworking1 system which enables elementarytasks to be performed by a huge number of people (typically called ?Turkers?)
on-line.Ideally, these tasks are meant to be solved by computers, but they still remain out ofcomputational reach (for instance, the translation of an English sentence into Urdu).?
INIST-CNRS/LIPN, 2 alle?e de Brabois, F-54500 Vandoeuvre-le`s-Nancy, France.E-mail: karen.fort@inist.fr.??
LIMSI/CNRS, Rue John von Neumann, Universite?
Paris-Sud F-91403 ORSAY, France.E-mail: gilles.adda@limsi.fr.?
Center for Computational Pharmacology, University of Colorado School of Medicine, University ofColorado at Boulder.
E-mail: kevin.cohen@gmail.com.1 Microworking refers to situations where tasks are cut into small pieces and their execution is paid for.Crowdsourcing refers to situations where the job is outsourced on the Web and done by many people(paid or not).?
2011 Association for Computational LinguisticsComputational Linguistics Volume 37, Number 2Figure 1Evolution of MTurk usage in NLP publications.MTurk is composed of two populations: the Requesters, who launch the tasks to becompleted, and the Turkers, who complete these tasks.
Requesters create the so-called?HITs?
(Human Intelligence Tasks), which are elementary components of complextasks.
The art of the Requesters is to split complex tasks into basic steps and to fix areward, usually very low (for instance US$0.05 to translate a sentence).
Using the MTurkparadigm, language resources can be produced at a fraction (1/10th at least) of the usualcost (Callison-Burch and Dredze 2010).MTurk should therefore not be considered to be a game.
Although it is superficiallysimilar to Phrase Detectives, in that case the gain is not emphasized (only the bestcontributors gain a prize, which consists of Amazon vouchers).
The same applies tothe French-language JeuxDeMots (?Play on Words?
), which does not offer any prize(Lafourcade 2007), and to Phrase Detectives (Chamberlain, Poesio, and Kruschwitz2008), in which the gain is not emphasized (only the best contributors gain a prize).MTurk is not a game or a social network, it is an unregulated labor marketplace:a system which deliberately does not pay fair wages, does not pay due taxes, andprovides no protections for workers.Why Are We Concerned?Since its introduction in 2005, there has been a steadily growing use of MTurk inbuilding or validating NLP resources, and most of the main scientific conferences in ourfield include papers involving MTurk.
Figure 1 was created by automatically searchingthe proceedings of some of the main speech and language processing conferences, aswell as some smaller events specializing in linguistic resources, using the quoted phrase?Mechanical Turk.?
We then manually checked the retrieved articles, source by source,to identify those which really make use of MTurk, ignoring those which simply talkabout it.
(For example, in the LREC 2010 proceedings, eight articles talk about MTurk,but only five used it, and in 2008, out of two papers citing MTurk, only one used it.
)The present journal, Computational Linguistics (CL), appears in the bar chart with a414Fort, Adda, and Cohen Amazon Mechanical Turk: Gold Mine or Coal Mine?zero count, as none of the articles published in it so far mention MTurk.2 All of theother sources contained at least one article per year using MTurk.
The total numberof publications varies from year to year, since, for example, conferences may acceptdifferent numbers of papers each year, and some conferences, such as LREC, occur onlyevery two years.We performed another, less detailed, search, this time in the whole ACL Anthology(not source by source), using the same quoted phrase ?Mechanical Turk?
on 5 Novem-ber 2010.
We examined the hits manually, and out of the 124 resulting hits, 86 werepapers in which the authors actually used MTurk as part of their research methodology.Interestingly, we noticed that at least one paper that we know to have used MTurk (i.e.,Biadsy, Hirschberg, and Filatova 2008), was not returned by the search.
The publishedversion of this paper does not explicitly mention MTurk, but the corresponding pre-sentation at the conference indicated that MTurk was used.
This is some evidence thatuse of MTurk may be under-reported.
It should be noted that these results include aspecialized workshop?the NAACL-HLT 2010 Workshop on Amazon Mechanical Turk(35 papers)?the existence of which is, in itself, strong evidence of the importance of theuse of MTurk in the domain.A vast majority of papers present small to medium size experiments where theauthors have been able to produce linguistic resources or perform evaluations at a verylow cost; at least for transcription and translation, the quality is sufficient to train andevaluate statistical translation/transcription systems (Callison-Burch and Dredze 2010;Marge, Banerjee, and Rudnicky 2010).
Some of these papers, however, bring to lightlanguage resource quality problems.
For example, Tratz and Hovy (2010, page 684) notethat the user interface limitations constitute ?
[t]he first and most significant drawback?of MTurk, as, in their context of annotating noun compound relations using a largetaxonomy, ?it is impossible to force each Turker to label every data point withoutputting all the terms onto a single Web page, which is highly impractical for a largetaxonomy.
Some Turkers may label every compound, but most do not.?
They alsonote that ?while we requested that Turkers only work on our task if English was theirfirst language, we had no method of enforcing this.?
Finally, they note that ?Turkerannotation quality varies considerably.?
Another important point is made in Bhardwajet al (2010), where it is shown that, for their task of word sense disambiguation, a smallnumber of trained annotators are superior to a larger number of untrained Turkers.On that point, their results contradict that of Snow et al (2008), whose task was muchsimpler (the number of senses per word was 3 for the latter, versus 9.5 for the former).The difficulty of having Turkers perform complex tasks also appears in Gillick and Liu(2010, page 148), an article from the proceedings of the NAACL-HLT 2010 Workshop onAmazon Mechanical Turk, in which non-expert evaluation of summarization systemsis proved to be ?not able to recover system rankings derived from experts.?
Evenmore interestingly, Wais et al (2010) show that standard machine learning techniques(in their case, a naive Bayes classifier) can outperfom the Turkers on a categorizationtask (classifying businesses into Automotive, Health, Real Estate, etc.).
Therefore, insome cases, NLP tools already do better than MTurk.
Finally, as we said earlier, thevast majority of papers present only small or medium size experiments.
This can beexplained by the fact that, at least according to Ipeirotis (2010a), submitting large jobsin MTurk results in low quality and unpredictable completion time.2 Note that one article in this particular issue of CL uses MTurk (Taboada et al, this issue).415Computational Linguistics Volume 37, Number 2Who Are the Turkers?Many people conceive of MTurk as a transposition of Grid Computing to humans, thusmaking it possible to benefit from humans?
?spare cycles?
to develop a virtual computerof unlimited power.
The assumption is that there is no inconvenience for humans (as itis not real work), and the power comes from the myriad.
This a fiction.Let us look first at how many Turkers are performing the HITs.
This is a quitedifficult task, because Amazon does not give access to many figures about them.
Weknow that over 500k people are registered as Turkers in the MTurk system.
But howmany Turkers are really performing HITs?
To evaluate this, we combined two differentsources of information.
First, we have access to some surveys about the demographicsof the Turkers (Ross et al 2009, 2010; Ipeirotis 2010b).
These surveys may have a biasover the real population of Turkers, as some Turkers may be reluctant to respond tosurveys.
Because the results of these surveys are quite consistent, and the surveysare usually easy to complete, not particularly boring, and paid above the usual rate,we may assume that this bias is minor, and accept what they say as a good pictureof the population of Turkers.
In these surveys we see many interesting things.
Forinstance, there is a growing number of people from India: There were below 10%in 2008, above 33% in early 2010, and they represented about 50% of the Turkers inMay 2010.3 Even if these surveys show that the populations from India and the U.S.are quite different, we may take as an approximation that they have about the samereasons to perform HITs in MTurk, and produce about the same activity.
We lookedat how many HITs the 1,000 Turkers who completed the survey in Ipeirotis (2010b)claim to perform: between 138,654 and 395,106 HITs per week.4 The second source ofinformation comes from the Mechanical Turk Tracker:5 According to this, 700,000 HITsare performed each week.
But the tracker system neither keeps track of the HITs whichare completed in less than one hour, nor is able to quantify the fact that the same HITcan be completed by multiple workers and in fact should be, according to regular userslike Callison-Burch and Dredze (2010).
Asking the authors of Ipeirotis (2010b), and thecreator of the Mechanical Turk Tracker (who are in fact the same person), he suggestedthat we should multiply the number given by the tracker by 1.7 ?
5 to take intoaccount these two factors,6 resulting in the (conjectural) total number of 5,950,000 HITs.Taking those two data points,7 we are able to hypothesize that the real number ofTurkers is between 15,059 and 42,912.
However, from the surveys, we have access toanother figure: Eighty percent (80%) of the HITs are performed by the 20% most activeTurkers (Deneme 2009), who spend more than 15 hours per week in the MTurk system(Adda and Mariani 2010)?consistent with the Pareto principle which says that 80%of the effects come from 20% of the causes.
We may therefore say that 80% of theHITs are performed by 3,011 to 8,582 Turkers.
These figures represent 0.6?1.7% of the3 http://blog.crowdflower.com/2010/05/amazon-mechanical-turk-survey/.4 The two figures come from the fact that each Turker gave a range of activity rather than an averagenumber of HITs.5 This system keeps track of all the HITs posted on MTurk, each hour.
http://mturk-tracker.com.6 Personal communication in the comments of http://behind-the-enemy-lines.blogspot.com/2010/03/new-demographics-of-mechanical-turk.html, reporting that the tracker is missing ?70of the posted HITs, which are posted and completed within less than one hour, and a 5x factor for theunobserved HIT redundancy.7 That is, 1,000 Turkers perform between 138,654 and 395,106 HITs per week, and the total number of HITsin the MTurk system is about 5.95M HITs per week.416Fort, Adda, and Cohen Amazon Mechanical Turk: Gold Mine or Coal Mine?registered Turkers, which in turn is in accord with the ?90-9-1?
rule8 valid in theInternet culture.Another important question is whether activity in MTurk should be consideredas labor or something else (hobby, volunteer work, etc.).
The observed mean hourlywages for performing jobs in the MTurk system is below US$2 (US$1.25 according toRoss et al [2009]).
Because they accept such low rewards, a common assumption isthat Turkers are U.S. students or stay-at-home mothers who have plenty of leisure timeand are happy to fill their recreation time by making some extra money.
According torecent studies in the social sciences (Ipeirotis 2010b; Ross et al 2010), it is quite truethat a majority (60%) of Turkers think that MTurk is a fruitful way to spend free timegetting some cash; but it is only 20% (5% of the India Turkers) who say that they use it tokill time.
And these studies also show that 20% (30% of the India Turkers) declare thatthey use MTurk ?to make basic ends meet.?
From these answers, we find that money isan important motivation for a majority of the Turkers (20% use MTurk as their primarysource of income, and 50% as their secondary source of income), and leisure is importantfor only a minority (30%).
We cannot conclude from these studies that the activity inMTurk should be considered as labor for all the Turkers, but we can at least for theminority (20%) for whom MTurk represents a primary source of income.9 Moreover,using the survey in Ipeirotis (2010b), we find that this minority is performing more thatone third of all the HITs.What Are the Issues with MTurk?The very low wages (below US$2 an hour) are a first issue, but the use of MechanicalTurk raises other ethical issues as well.
The position of many prototypical Turkers wouldbe considered ethically unacceptable in major developed countries.
Denied even thebasic workplace right of collective bargaining (unionization), this community has norecourse to any channels for redress of employer wrongdoing, let alne the normal onesavailable to any typical worker in the United States and many other developed nations(e.g., class action lawsuits, other lawsuits, and complaints to government agencies),while simultaneously being subjected to egregious vulnerabilities, including the factthat they have no guarantee of payment for work properly performed.Legal issues surrounding the use of MTurk have also been encountered.
At leastone university legal department was sufficiently concerned that Turkers working forseveral months would claim employee status and demand health and other benefits thatthey refused to allow grant funds to be expended on MTurk (personal communication,E.
Hovy).
A small number of universities have insisted on institutional review boardapproval for MTurk experiments (personal communication, K. Cohen).
(Institutionalreview boards in U.S. universities are independent bodies that review proposed experi-ments for legal and ethical issues.
)Is MTurk the Future of Linguistic Resource Development?The implicit belief that the very low cost of MTurk derives from the fact that incentiviz-ing casual hobbyists requires only minimal payment is a mirage: Once you admit that amajority of Turkers do not consider MTurk as a hobby, but as a primary or a secondary8 http://en.wikipedia.org/wiki/1\%25 rule (Internet culture).9 And even for the 50% who are utilizing MTurk as a secondary source of income.417Computational Linguistics Volume 37, Number 2source of income, and that one third of the HITs are performed by Turkers who needMTurk to make basic ends meet, you then have to admit that MTurk is, at least forthem, a labor marketplace.
Moreover, the frequent assumption that the low rewards area result of the classical law of supply-and-demand (large numbers of Turkers meansmore supply of labor and therefore lower acceptable salaries) is false.
Firstly, we donot observe that there are too many Turkers.
In fact, there are not enough Turkers.
Thiscan be observed through the difficulty in finding Turkers with certain abilities (e.g.,understanding a specific language [Novotney and Callison-Burch 2010]), and in thedifficulty in performing very large HIT groups (Ipeirotis 2010a).
This is not surprising,as we have seen that the number of active Turkers is not that large.
Secondly, the lowcost is a result of the Requesters?
view of the relation between quality and reward: Manyarticles (e.g., Marge, Banerjee, and Rudnicky 2010) relate that there is no correlationbetween the reward and the final quality.
The reason is that increasing the price isbelieved to attract spammers (i.e., Turkers who cheat, not really performing the job,but using robots or answering randomly), and these are numerous in the MTurk systembecause of an inadequate worker reputation system.10 We obtain here a schema whichis very close to what the 2001 economics Nobel prize winner George Akerlof calls ?themarket for lemons,?
where asymmetric information in a market results in ?the baddriving out the good.?
He takes the market for used cars as an example (Akerlof 1970),where owners of good cars (here, good workers) will not place their cars on the usedcar market, because of the existence of many cars in bad shape (here, the spammers),which encourage the buyer (here, the Requester) to offer a low price (here, the reward)because he does not know the exact value of the car.
After some time, the good workersleave the market because they are not able to earn enough money given the work done(and sometimes they are not even paid), which in turn decreases the quality.
At themoment, the system is stable in terms of the number of Turkers, because good workersare replaced by naive workers.Amazon?s attitude towards reputational issues has been passive.
It maintains that itis a neutral clearinghouse for labor, in which all else is the responsibility of the two con-senting parties.
This attitude has led to an explosion of micro-crowdsourcing start-ups,which observed the MTurk flaws and tried to overcome them.11 Some of these start-upscould become serious alternatives to MTurk (TheQuill 2010), like Samasource,12 whichoffers at least a fair wage to workers, who in turn are clearly identified on the Web site,with their resumes.
But others are even worse than MTurk, ethically speaking.
MTurkis ethically questionable enough; as a scientific community with ethical responsibilitieswe should seek to minimize the existence of even less-ethical alternatives to it.What?s Next?If we persist in claiming that with MTurk we are now able to produce any linguisticresource or perform any manual evaluation of output at a very low cost, fundingagencies will come to expect it.
It is predictable that in assessing projects involvinglinguistic resource production or manual evaluation of output, funding agencies will10 For more details, see http://behind-the-enemy-lines.blogspot.com/2010/10/be-top-mechanical-turk-worker-you-need.html.11 For instance, Agent Anything, Clickworker, CloudCrowd, CrowdFlower, DoMyWork, JobBoy, LiveWork,Microtask, microWorkers, MiniFreelance, MiniJobz, MinuteWorkers, MyEasyTask, MyMicroJob, OpTask,RapidWorkers, Samasource, ShortTask, SimpleWorkers, SmartSheet, and so forth.12 http://www.samasource.org.418Fort, Adda, and Cohen Amazon Mechanical Turk: Gold Mine or Coal Mine?prefer projects which propose to produce 10 or 100 times more data for the sameamount of money.
MTurk costs will then become the standard costs, and it will be verydifficult to obtain funding for a project involving linguistic resource production at anylevel that would allow for more traditional, non-crowdsourced resource constructionmethodologies.
Therefore, our community?s use of MTurk not only supports a work-place model that is unfair and open to abuses of a variety of sorts, but also creates ade facto standard for the development of linguistic resources that may have long-termfunding consequences.Non-exploitative methods for decreasing the cost of linguistic resource devel-opment exist.
They include semi-automatic processing, better methodologies andtools, and games with a purpose, as well as microworking Web sites (like Samasource)that guarantee workers minimum payment levels.
We encourage the computationallinguistics and NLP communities to keep these alternatives in mind when planningexperiments.
If a microworking system is considered desirable by the ACL andISCA communities, then we also suggest that they explore the creation and use of alinguistically specialized special-purpose microworking alternative to MTurk thatboth ensures linguistic quality and holds itself to the highest ethical standards ofemployer/employee relationships.
Through our work as grant evaluators and recipi-ents, we should also encourage funding bodies to require institutional review boardapproval for crowdsourced experiments and to insist on adherence to fair labor prac-tices in such work.AcknowledgmentsWe would like to thank Sophie Rosset,Joseph Mariani, Panos Ipeirotis, EduardHovy, and Robert Dale for their suggestionsand encouragements.
Any remaining errorsare our own.ReferencesAdda, Gilles and Joseph Mariani.
2010.Language resources and AmazonMechanical Turk: Legal, ethical andother issues.
In LISLR2010, ?Legal Issuesfor Sharing Language Resources workshop,?LREC2010, Malta.Akerlof, George A.
1970.
The market for?lemons?
: Quality uncertainty and themarket mechanism.
Quarterly Journal ofEconomics, 84(3):488?500.Bhardwaj, Vikas, Rebecca Passonneau,Ansaf Salleb-Aouissi, and Nancy Ide.2010.
Anveshan: A tool for analysis ofmultiple annotators?
labeling behavior.In Proceedings of The Fourth LinguisticAnnotation Workshop (LAW IV),pages 47?55, Uppsala.Biadsy, Fadi, Julia Hirschberg, and ElenaFilatova.
2008.
An unsupervised approachto biography production using Wikipedia.In Proceedings of ACL 2008, pages 807?815,Columbus.Callison-Burch, Chris and Mark Dredze.2010.
Creating speech and language datawith Amazon?s Mechanical Turk.
InCSLDAMT ?10: Proceedings of the NAACLHLT 2010 Workshop on Creating Speech andLanguage Data with Amazon?s MechanicalTurk, pages 1?12, Morristown, NJ.Chamberlain, J., M. Poesio, andU.
Kruschwitz.
2008.
Phrase detectives:a Web-based collaborative annotationgame.
In Proceedings of the InternationalConference on Semantic Systems(I-Semantics?08), pages 42?49, Graz.Deneme.
2009.
How many Turkers are there?Available at http://groups.csail.mit.edu/uid/deneme/?p=502.
AccessedDecember 2009.Gillick, Dan and Yang Liu.
2010.
Non-expertevaluation of summarization systems isrisky.
In Proceedings of the NAACL HLT2010 Workshop on Creating Speech andLanguage Data with Amazon?s MechanicalTurk, CSLDAMT ?10, pages 148?151,Stroudsburg, PA.Ipeirotis, Panos.
2010a.
Analyzing theAmazon Mechanical Turk marketplace.CeDER Working Papers No.
CeDER-10-04.Available at http://hdl.handle.net/2451/29801.Ipeirotis, Panos.
2010b.
Demographicsof Mechanical Turk.
CeDER WorkingPapers No.
CeDER-10-01.
Available athttp://hdl.handle.net/2451/29585.Lafourcade, Mathieu.
2007.
Making peopleplay for lexical acquisition.
In Proceedings419Computational Linguistics Volume 37, Number 2of SNLP 2007, 7th Symposium on NaturalLanguage Processing, Pattaya.Marge, Matthew, Satanjeev Banerjee, andAlexander I. Rudnicky.
2010.
Using theAmazon Mechanical Turk for transcriptionof spoken language.
In IEEE InternationalConference on Acoustics Speech and SignalProcessing (ICASSP), pages 5270?5273,Dallas, TX.Novotney, Scott and Chris Callison-Burch.2010.
Cheap, fast and good enough:Automatic speech recognition withnon-expert transcription.
In HumanLanguage Technologies: The 2010 AnnualConference of the North American Chapter ofthe Association for Computational Linguistics,HLT ?10, pages 207?215, Morristown, NJ.Ross, Joel, Lilly Irani, M. Six Silberman,Andrew Zaldivar, and Bill Tomlinson.2010.
Who are the crowdworkers?
Shiftingdemographics in Mechanical Turk.
InProceedings of the 28th of the InternationalConference Extended Abstracts on HumanFactors in Computing Systems, CHI EA ?10,pages 2863?2872, New York, NY.Ross, Joel, Andrew Zaldivar, Lilly Irani,and Bill Tomlinson.
2009. Who are theTurkers?
Worker demographics inAmazon Mechanical Turk.
SocialCode Report 2009-01.
Available athttp://www.ics.uci.edu/?jwross/pubs/SocialCode-2009-01.pdf.Snow, Rion, Brendan O?Connor, DanielJurafsky, and Andrew Y. Ng.
2008.
Cheapand fast?but is it good?
Evaluatingnon-expert annotations for naturallanguage tasks.
In Proceedings of EMNLP2008, pages 254?263, Waikiki.TheQuill.
2010.
Making money on-line,part 2: Microworking.
Available athttp://thequill.org/personal-finance/9-making-money-on-line-part-2-microworking-.html.Accessed 21 June 2010.Tratz, Stephen and Eduard Hovy.
2010.A taxonomy, dataset, and classifier forautomatic noun compound interpretation.In Proceedings of the 48th Annual Meeting ofthe Association for Computational Linguistics,pages 678?687, Uppsala.Wais, Paul, Shivaram Lingamneni,Duncan Cook, Jason Fennell, BenjaminGoldenberg, Daniel Lubarov, David Marin,and Hari Simons.
2010.
Towards building ahigh-quality workforce with MechanicalTurk.
In Proceedings of Computational SocialScience and the Wisdom of Crowds (NIPS),pages 1?5, Whister.420
