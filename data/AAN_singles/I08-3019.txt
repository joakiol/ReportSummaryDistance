Proceedings of the IJCNLP-08 Workshop on NLP for Less Privileged Languages, pages 117?122,Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language ProcessingPart-Of-Speech Tagging for Gujarati Using Conditional RandomFieldsChirag Patel and Karthik GaliLanguage Technologies Research CentreInternational Institute of Information TechnologyHyderabad, Indiachirag_p,karthikg@students.iiit.ac.inAbstractThis paper describes a machine learningalgorithm for Gujarati Part of Speech Tag-ging.
The machine learning part is per-formed using a CRF model.
The featuresgiven to CRF are properly chosen keepingthe linguistic aspect of Gujarati in mind.
AsGujarati is currently a less privileged lan-guage in the sense of being resource poor,manually tagged data is only around 600sentences.
The tagset contains 26 differenttags which is the standard Indian Language(IL) tagset.
Both tagged (600 sentences)and untagged (5000 sentences) are used forlearning.
The algorithm has achieved anaccuracy of 92% for Gujarati texts wherethe training corpus is of 10,000 words andthe test corpus is of 5,000 words.1 IntroductionParts of Speech tagging is the process of taggingthe words of a running text with their categoriesthat best suits the definition of the word as well asthe context of the sentence in which it is used.
Thisprocess is often the first step for many NLP appli-cations.
Work in this field is usually either statisti-cal or machine learning based, or rule based.
Someof the models that use the first approach are Hid-den Markov Models (HMMs), Conditional Ran-dom Fields (CRFs), Maximum Entropy MarkovModels (MEMMs), etc.The other method is the rule based approachwhere by we formulate rules based on the study ofthe linguistic aspect of the language.
These rulesare directly applied on the test corpus.
The statisti-cal learning based tools attack the problem mostlyas a classification problem.
They are not languagespecific and hence they fail when semantic knowl-edge is needed while tagging a word with morethan one sense.
Even for unknown words, i.e.,those words which have not appeared in the train-ing corpus, these tools go by the probabilities butare not guaranteed to give the correct tag as theylack the semantic knowledge of the language.Also, they need a large annotated corpus.
But thebright side of these tools is they can tag any word(known or unknown) with a high accuracy basedon the probabilities of similar tags occurring in aparticular context and some features provided forlearning from the training data.On the other hand, purely rule based systems failwhen the word is unknown or does not satisfy anyof the rules.
These systems just crash if the word isunknown.
They cannot predict the plausible orlikely tag.
Hence an exhaustive set of rules areneeded to achieve a high accuracy using this ap-proach.There is another class of tools which are the hy-brid ones.
These may perform better than plainstatistical or rule based approaches.
The hybridtools first use the probabilistic features of the sta-tistical tools and then apply the language specificrules on the results as post processing.
The bestapproach which seems intuitive is to generalize thelanguage specific rules and convert them into fea-tures.
Then incorporate these features into the sta-tistical tools.
The problem here is the lack of con-trol and flexibility on the statistical tools.
So theperfect selection of features is what actually mat-ters with respect to the accuracy.
The more lan-117guage specific features that can be designed thehigher accuracy can be achieved.2 Previous WorkDifferent approaches have been used for part-of-speech tagging previously.
Some have focused onrule based linguistically motivated part-of-speechtagging such as by Brill (Brill, 1992 and Brill,1994).
On the machine learning side, most of theprevious work uses two main machine learningapproaches for sequence labeling.
The first ap-proach relies on k-order generative probabilisticmodels of paired input sequences, for instanceHMM (Frieda and McCallum, 2000) or multilevelMarkov Models (Bikel et al 1999).CRFs bring together the best of generative andclassification models.
Like classification models,they can accommodate many statistically corre-lated features of the input, and they are trained dis-criminatively.
And like generative models they canalso tradeoff decisions at different sequence posi-tions to obtain a globally optimal labeling.
Condi-tional Random Fields were first used for the task ofshallow parsing by Lafferty et al (Lafferty et al,2000), where CRFs were applied for NP chunkingfor English on WSJ corpus and reported a per-formance of 94.38%.
For Hindi, CRFs were firstapplied to shallow parsing by Ravindran et al(Ravindran et.
al., 2006) and Himanshu et al (Hi-manshu et.
al., 2006) for POS tagging and chunk-ing, where they reported a performance of 89.69%and 90.89% respectively.
Lafferty also showed thatCRFs beat related classification models as well asHMMs on synthetic data and on POS-tagging task.Several POS taggers using supervised learning,both over word instances and tagging rules, reportprecision greater than 96% for English.
For Hindiand other South Asian languages, the tagged cor-pora is limited and together with higher morpho-logical complexity of these languages it poses adifficulty in achieving results as good as thoseachieved for English in the past.3 Conditional Random FieldsCharles Sutton et al (Sutton et al, 2005) formu-lated CRFs as follows.
Let G be a factor graphover Y.
Then p(y|x) is a conditional random field iffor any fixed x, the distribution p(y|x) factorizesaccording to G. Thus, every conditional distribu-tion p(y|x) is a CRF for some, perhaps trivial, fac-tor graph.
If F = {A} is the set of factors in G, andeach factor takes the exponential family form, thenthe conditional distribution can be written asX here is a random variable over data sequencesto be labeled, and Y is a random variable over cor-responding label sequences.
All components Yi ofY are assumed to range over a finite label alphabetY.
For example, X might range over natural lan-guage sentences and Y range over part-of-speechtagging of those sentences, with Y the set of possi-ble part-of-speech tags.
The random variables Xand Y are jointly distributed, but in a discrimina-tive framework we construct a conditional modelp(Y|X) from paired observation and label se-quences, and do not explicitly model the marginalp(X).CRFs define conditional probability distribu-tions P(Y|X) of label sequences given input se-quences.
Lafferty et al defines the probability of aparticular label sequence Y given observation se-quence X to be a normalized product of potentialfunctions each of the form:exp(??jtj(Yi-1,Yi,X,i)+?
?ksk (Yi,X,i))where tj(Yi-1,Yi,X,i) is a transition feature func-tion of the entire observation sequence and the la-bels at positions i and i-1 in the label sequence; sk(Yi,X,i) is a state feature function of the label atposition I and the observation sequence; and ?j and?k are parameters to be estimated from trainingdata.Fj(Y,X)= ?
fj (Yi-1,Yi,X,i)where each fj (Yi-1,Yi,X,i) is either a state func-tion s(Yi-1,Yi,X,i) or a transition function t(Yi-1,Yi,X,i).
This allows the probability of a labelsequence Y given an observation sequence X to bewritten as:P(Y|X, ?)
= (1/Z(X)) exp(?
?j Fj(Y,X))where Z(X) is a normalization factor.4 IL TagsetThe currently used tagset for this project and whichis a standard for Indian Languages is the IL (Indian118Languages) tagset.
The tagset consists of 26 tags.These have been specially designed for IndianLanguages.
The tagset contains the minimum tagsnecessary at the Parts of Speech tagging level.
Itcopes with the phenomena of fineness versuscoarseness.
The tags are broadly categorized into 5main groups, with the nouns consisting of the gen-eral nouns, space or time related nouns or propernouns, and the verbs consisting of the main and theauxiliary verbs.
Another category is of the nounand verb modifiers like adjectives, quantifiers andadverbs.
Finally, there are numbers, cardinals etc.5 ApproachApproach presented in this paper is a machinelearning model.
It uses supervised as well as unsu-pervised techniques.
It uses a CRF to statisticallytag the test corpus.
The CRF is trained using fea-tures over a tagged and untagged data.
A CRFwhen provided with good features gives accuracymuch better than other models.
The intuition hereis that if we convert the linguistic rules specific toGujarati in to features provided to CRF, then wemake use of advantages of both statistical and rulebased approach.
But due to lack of control andflexibility not all features can be incorporated inthe CRF.
So after the CRF is done we do the erroranalysis.
From the errors we formulate rules,which are general and language specific, and thenconvert them to new features and apply them backto CRF.
This increases the accuracy.Gujarati when viewed linguistically is a freeword order language.
It is partially agglutinative,in the sense maximum 4 suffixes can attach to themain root.
Words in Gujarati can have more thanone sense where the tags are different in differentsenses.
For e.g.
?paNa?
can be a particle meaning ?
?also?, and also can be a connective meaning ??but?.
?pUrI?
can be a noun meaning ?
?an eat-able?, can be an adjective meaning ?
?finished?,and can also be a verb meaning ?
?to fill?.Also, in Gujarati, postpositions can be or can notbe attached to the head word.
For e.g.
One maywrite ?rAme?
or ?rAma e?
literally meaning?rAma (ergative)?.Most of all, this language can drop words fromthe sentences.
For example:Sent:     baXA  loko     GaramAM   gayA.Literal:    all    people   house + in   went.Tags:   QF  NN    NN          VMHere, we can drop the noun (NN) ?loko?
and inwhich case the quantifier (QF) ?baXA?
now be-comes the noun (NN).Features used in CRF are suffixes, prefixes,numbers etc.
For e.g.
Words having suffix ?ne?,like ?grAhakone?
are tagged as NN.
CRF learnsfrom the tags given to words with same suffixes inthe training data.
This suffix window is 4.
Thisway the vibhakti information is explored.
Similarlyif words like ?KAine?
and ?KAwo?
come in thetraining corpus the CRF learns the preffix and tagsother words with that prefix.
This way the steminformation is explored.
Also if the token is anumber then it must be QC, and if it has a numberin it then it must be a NNP.6 ExperimentsInitially we just ran a rule based tagging code onthe test data.
This code used both machine learningand rule based features for tagging.
It gave an ac-curacy of 86.43%.
The error analysis revealed that,as the training corpus being less, the unknownwords are many and also well distributed over thetags.
Hence the heuristics were not effective.Then we ran a CRF tool on the test data.
Wefound it giving an accuracy of 89.90%.
Then dur-ing the error analysis we observed that the featureswere not up to the mark.
Then we selected particu-lar features which were generalization of rulebased, used in the previous code, and more specificto Gujarati.
This increased the accuracy to 91.74%.Then after adding more heuristics the accuracy wasin fact reducing.
Heuristics like converting allNNPs to NNs, removing some tags as optionswhile tagging the unknown words likeCC,QW,PRP etc.
as these in a language are verylimited and are expected that they must have cameonce in the training corpus.
We also tried taggingthe word on the basis of possible tags between thetwo surrounding words.
But that too reduced theaccuracy.
Also heuristics like previous and currentword vibhakti combination failed.Training data Test data Results (%)11185 5895 91.74Table-1.
POS Tagging Results and Data Size1197 Error AnalysisHere the above table confirms that the errorshave occurred across all the tags.
This is mainlydue to lack of training data.
The numbers of un-known words in the corpus were around 40%.
TheCRF while using the features and the probabilitiesto tag a particular unknown word made mistakesdue to the flexible nature of the language.
For e.g.the maximum errors occurred because of taggingan adjective by a noun.
An example:motA`QFC BAganA`QF viSeRa`NN SEk-SaNika`JJ jaruriyAwo`NN GarAvawA`VMbAlYako`NN sAmAnya`JJ skUlamAM`NNjaSe`VM .`SYMActual Tag Assigned Tag CountsJJ NN 58NNP NN 35NN JJ 26NN VM 22NNC NN 21PSP NN 19VM VAUX 19NNPC NN 18NNC JJ 17NST NN 14VM NN 13Table-2.
Errors Made by the Tagger.In the above example the word ?viSeRa`NN?
iswrongly tagged.
This being an adjective is taggedas NN, firstly because it is an unknown word.
Alsoin this language adjectives may or may not occurbefore the nouns.
Hence the probability of this un-known word to be a NN or a JJ is equal or will de-pend on the number of instances of both in thetraining corpus.
Further more there is more prob-ability of it being tagged as a noun as the nextword is an adjective.
There are very less instanceswhere two adjectives come together in the trainingcorpus.
Again the chances of it being a noun in-crease as the QF mostly precede nouns instead ofadjectives.
Here we also have a QF before the un-known word.
The same reason also is responsiblefor the third class of errors ?
NN being wronglytagged as JJ.
These errors can only be corrected ifthe word is some how known.
Again the next classof errors is the Named Entity Recognition problemwhich is an open problem in itself.8 ConclusionWe have trained a CRF on Gujarati which gives anaccuracy of around 92%.
From the experiments weobserved that if the language specific rules can beformulated in to features for CRF then the accu-racy can be reached to very high extents.
The CRFlearns from both tagged that is 600 sentences andalso untagged data, which is 5,000 sentences.From the errors we conclude that as the trainingdata increases, the less number of unknown wordswill be encountered in the test corpus, which willincrease the accuracy.
We can also use some ma-chine readable resources like dictionaries, morphsetc.
when ever they are built.9 IntuitionWe noticed that on a less amount of training dataalso we have a good accuracy.
The reason we feltintuitive was Gujarati uses the best part of the vib-hakti feature linguistically.
It, being more aggluti-native than Hindi has more word forms, hencemore word coverage, and being some less aggluti-native than Telugu, has less ambiguity and also ispractical to hard code the vibhaktis, uses the bestpart of advantages of the vibhakti feature in POStagging.
Based only on the hard coded vibhaktiinformation we could tag around 1500 unknownwords out of 5000.10 Future workWe are looking forward to manually tag moretraining data in the future.
We will also be trying tobuild language resources for Gujarati that will helpin the Tagger.
By increasing the amount of trainingdata we expect an appreciable increase in the accu-racy.ReferencesHimanshu Agarwal and Anirudh Mani.
2006.
Part ofSpeech Tagging and Chunk-ing with ConditionalRandom Fields.
In the Proceedings of NWAI work-shop.Pranjal Awasthi, Delip Rao, Balaraman Ravindran.2006.
Part Of Speech Tagging and Chunking withHMM and CRF.
Proceedings of the NLPAI contestworkshop during  NWAI ?06, SIGAI Mumbai.Karthik Kumar G, Sudheer K, Avinesh PVS.
2006.Comparative study of various Machine Learningmethods For Telugu Part of Speech tagging.
Pro-120ceedings of the NLPAI contest workshop duringNWAI ?06, SIGAI Mumbai.John Lafferty, Andrew McCallum and FernandoPereira.
2001.
Conditional Random Fields: Probabil-istic Models for Segment-ing and Labeling SequenceData.
In proceedings of ICML?01.Avinesh PVS and Karthik G. 2007.
Part-Of-SpeechTagging and Chunking Using Conditional RandomFields and Transformation Based Learning.
Proceed-ings of the SPSAL workshop during IJCAI?07.Fei Sha and Fernando Pereira.
2003.
Shallow Parsingwith Conditional Random Fields.
In the Proceedingsof HLT-NAACL.Charles Sutton.
2007.
An Introduction to ConditionalRandom Fields for Relational Learning.
In proceed-ings of ICML?07.CRF++: Yet Another Toolkit.http://chasen.org/~taku/software/CRF++121122
