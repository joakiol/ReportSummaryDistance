Proceedings of the 8th International Natural Language Generation Conference, pages 108?112,Philadelphia, Pennsylvania, 19-21 June 2014. c?2014 Association for Computational LinguisticsClassifiers for data-driven deep sentence generationMiguel Ballesteros1, Simon Mille1 and Leo Wanner2,11NLP Group, Department of Information and Communication TechnologiesPompeu Fabra University, Barcelona2Catalan Institute for Research and Advanced Studies (ICREA)<fname>.<lname>@upf.eduAbstractState-of-the-art statistical sentence gener-ators deal with isomorphic structures only.Therefore, given that semantic and syntac-tic structures tend to differ in their topol-ogy and number of nodes, i.e., are not iso-morphic, statistical generation saw so faritself confined to shallow, syntactic gener-ation.
In this paper, we present a seriesof fine-grained classifiers that are essen-tial for data-driven deep sentence genera-tion in that they handle the problem of theprojection of non-isomorphic structures.1 IntroductionDeep data-driven (or stochastic) sentence gener-ation needs to be able to map abstract seman-tic structures onto syntactic structures.
This hasbeen a problem so far since both types of struc-tures differ in their topology and number of nodes(i.e., are non-isomorphic).
For instance, a trulysemantic structure will not contain any functionalnodes,1 while a surface-syntactic structure or achain of tokens in a linearized tree will con-tain all of them.
Some state-of-the-art propos-als use a rule-based module to handle the projec-tion between non-isomorphic semantic and syn-tactic structures/chains of tokens, e.g., (Varges andMellish, 2001; Belz, 2008; Bohnet et al., 2011),and some adapt the semantic structures to be iso-morphic with syntactic structures (Bohnet et al.,2010).
In this paper, we present two alternativestochastic approaches to the projection betweennon-isomorphic structures, both based on a cas-cade of Support Vector Machine (SVM) classi-fiers.2 The first approach addresses the projectionas a generic non-isomorphic graph transduction1See, for instance, (Bouayad-Agha et al., 2012).2Obviously, other machine learning techniques could alsobe used.problem in terms of four classifiers for 1. identi-fication of the (non-isomorphic) correspondencesbetween fragments of the source and target struc-ture, 2. generation of the nodes of the target struc-ture, 3. generation of the dependencies betweencorresponding fragments of the source and targetstructure, and 4. generation of the internal depen-dencies in all fragments of the target structure.The second approach takes advantage of the lin-guistic knowledge about the projection of the in-dividual linguistic token types.
It replaces eachof the above four classifiers by a set of classifiers,with each single classifier dealing with only oneindividual linguistic token type (verb, noun, ad-verb, etc.)
or with a configuration thereof.
As willbe seen, the linguistic knowledge pays off: the sec-ond approach achieves considerably better results.Since our goal is to address the challenge of theprojection of non-isomorphic structures, we focus,in what follows, on this task.
That is, we do notbuild a complete generation pipeline until the sur-face.
This could be done, for instance, by feed-ing the output obtained from the projection of asemantic onto a syntactic structure to the surfacerealizer described in (Bohnet et al., 2010).2 The TaskThe difference in the linguistic abstraction of se-mantic and syntactic structures leads to diver-gences that impede the isomorphy between thetwo and make the mapping between them a chal-lenge for statistical generation.
Let us, before wecome to the implementation, give some theoreticaldetails on these structures as we picture them andon the possible approaches to the projection of asemantic structure to a syntactic one.2.1 The Notion of semantic and syntacticstructuresAs semantic structure, we assume a shallow se-mantic representation that is very similar to the108PropBank (Babko-Malaya, 2005) and deep anno-tations as used in the Surface Realisation SharedTask (Belz et al., 2011): the deep-syntactic layerof the AnCora-UPF corpus (Mille et al., 2013).Deep-syntactic structures (DSyntSs) do notcontain any punctuation and functional nodes, i.e.,governed prepositions and conjunctions, auxil-iaries and determiners.3As syntactic structure (in the terminologyof Ancora-UPF: surface-syntactic structures,SSyntSs), we assume dependency trees in whichthe nodes are labeled by open or closed classlexemes and the edges by grammatical functionrelations of the type subject, oblique object,adverbial, modifier, etc.
; cf.4 See Figure 1 for acontrastive illustration of DSyntS and SSyntS.rumor want new song be successfulIIIATTR I IIa rumor wants that the new song will be successfuldet subj dobjconjdetmodif subj analyt fut copulFigure 1: DSyntS (above) and SSyntS (below) ofan English Sentence.Note, however, that the proposal outlined be-low for the projection of non-isomorphic struc-tures is trainable on any multi-layered treebankswhere different layers are not isomorphic.2.2 Projection of DSyntSs onto SSyntSsIn order to project a DSyntS onto its correspond-ing SSyntS in the course of sentence generation,the following types of actions need to be per-formed:1.
Project each node in the DSyntS onto its SSynS-correspondence.
This correspondence can be asingle node, as, e.g., successful?
successful, or asubtree (hypernode, known as syntagm in linguis-tics), as, e.g., song ?
the song ?DT NN?
(where?DT?
is a determiner and ?NN?
a noun) or be?
that will be ?IN VAUX VB?
(where ?IN?
is apreposition, ?VAUX?
an auxiliary and ?VB?
a fullverb).
In formal terms, we assume any SSyntS-correspondence to be a hypernode with a cardinal-ity ?
1.2.
Generate the correct lemma for the nodes in3For more details on the SSyntS, see (Mille et al., 2013).4DSyntSs and their corresponding SSyntSs are stored inthe 14-column CoNLL?08 format.SSyntS that do not have a 1:1 correspondence inthe SSyntS (as ?DT?, ?IN?
and ?VAUX?
above).3.
Establish the dependencies within the individ-ual SSyntS-hypernodes.4.
Establish the dependencies between theSSyntS-hypernodes (more precisely, between thenodes of different SSyntS-hypernodes) to obtain aconnected SSyntS-tree.3 ClassifiersAs mentioned in the Introduction, the realizationof the actions 1.?
4. can be approached either interms of 4 generic classifiers (Section 3.1) or interms of 4 sets of fine-grained (micro) classifiers(Section 3.2) that map one representation onto an-other.
As also mentioned above, we realize bothapproaches as Support Vector Machines (SVMs).3.1 Generic classifier approachEach of the generic classifiers deals with one ofthe following tasks.a.
Hypernode Identification: Given a deepsyntactic node nd from the DSyntS, the systemmust find the shape of the surface hypernode (=syntagm) that corresponds to nd in the SSyntS.The hypernode identification SVM uses the fol-lowing features:POS of nd, POS of nd?s head, voice,temp.
constituency, finiteness, tense, lemma ofnd, and nd?s dependencies.In order to simplify the task, we define the shapeof a surface hypernode as a list of surface PoS-tags.
This list contains the PoS of each of the lem-mas within the hypernode and a tag that signals theoriginal deep node; for instance:[ VB(deep), VAUX, IN]b. Lemma Generation.
Once the hypernodesof the SSyntS under construction have been pro-duced, the functional nodes that have been newlyintroduced in the hypernodes must be assigned alemma.
The lemma generation SVM uses the fol-lowing features of the deep nodes nd in the hyper-nodes:?
finiteness, ?
definiteness, ?
PoS of nd, ?
lemmaof nd, ?
PoS of the head of ndto select the most likely lemma.c.
Intra-hypernode Dependency Generation.Given a hypernode and its lemmas provided bythe two previous stages, the dependencies (i.e., thedependency attachments and dependency labels)between the elements of the hypernode must be109determined (and thus also the governor of the hy-pernode).
For this task, the intra-hypernode de-pendency generation SVM uses the following fea-tures:?
lemmas included in the hypernode, ?
PoS-tagsof the lemmas in the hypernode, ?
voice of thehead h of the hypernode, ?
deep dependency re-lation to h.[ VB(deep), VAUX, IN]analyt fut preposFigure 2: Internal dependency within a hypernode.d.
Inter-hypernode Dependency Generation.Once the individual hypernodes have been con-verted into connected dependency subtrees, thehypernodes must be connected between eachother, such that we obtain a complete SSyntS.
Theinter-hypernode dependency generation SVM usesthe following features of a hypernode ss:?
the internal dependencies of ss, ?
the head ofss, ?
the lemmas of ss, ?
the PoS of the depen-dent of the head of ss in DSyntSto determine for each hypernode its governor.
[ VB(deep), VAUX, IN] [ NN(deep), DT]subjFigure 3: Surface dependencies between two hy-pernodes.3.2 Implementation of sets of microclassifiersIn this alternative approach, a single classifier isforeseen for each kind of input.
Thus, for thehypernode identification module, for each deepPoS tag (which can be one of the following four:?N?
(noun), ?V?
(verb), ?Adv?
(adverb), ?A?
(ad-jective)), a separate multi-class classifier is de-fined.
For instance, in the case of ?N?, the N-classifier will use the above features to assignto the a DSynt-node with PoS ?N?
the most ap-propriate (most likely) hypernode?in this case,[NN(deep), DT].
In a similar way, in the case ofthe lemma generation module, for each surfacePoS tag, a separate classifier is defined.
Thus,the DT-classifier would pick for the hypernode[NN(deep), DT] the most likely lemma for the DT-node (optimally, a determiner).For the intra-hypernode attachments module,for each kind of hypernode, dynamically a sepa-rate classifier is generated.5 In the case of the hy-5This implies that the number of classifiers varies depend-ing on the training set, in the intra-hypernode dependencygeneration there are 108 SVMs.pernode [ VB(deep), VAUX, IN], the correspond-ing classifier will create a link between the prepo-sition and the auxiliary, and between the auxiliaryand the verb, with respectively the preposition andthe auxiliary as heads because it is the best linkthat it can find; cf.
Figure 2 for illustration.Finally, for the inter-hypernode attachmentsmodule, for each hypernode with a distinct in-ternal dependency pattern, a separate classifier isdynamically derived (for our treebank, we ob-tained 114 different SVM classifiers because italso takes into account hypernodes with just onetoken).
For instance, the classifier for the hypern-ode [ NN(deep), DT] is most likely to identify asits governor VAUX in the hypernode [ VB(deep),VAUX, IN]; cf.
Figure 3.4 Experiments and ResultsIn this section, we present the performance of thetwo approaches to DSyntS?SSyntS projection onthe DSyntS- and SSynt-layers of the AnCora-UPFtreebank (Mille et al., 2013).6 Table 1 displaysthe results for the generic classifier for all taskson the development and the test set, while Table2 displays the results obtained through the sets ofmicro classifiers.Dev.set # %Hypernode identification 3131/3441 90.99Lemma generation 818/936 87.39Intra-hypernode dep.
generation 545/798 68.30Inter-hypernode dep.
generation 2588/3055 84.71Test set # %Hypernode identification 5166/5887 87.75Lemma generation 1822/2084 87.43Intra-hypernode dep.
generation 1093/1699 64.33Inter-hypernode dep.
generation 4679/5385 86.89Table 1: Results of the evaluation of the genericclassifiers for the non-isomorphic transduction.The results show that for hypernode identifica-tion and inter-hypernode dependency generation,the results of both types of classifiers are compara-ble, be it on the development set or on the test set.However, thanks to the micro classifiers, with thesame features, the lemma generation model basedon micro classifiers improves by 4 points and theintra-hypernode dependency generation by nearly6Following a classical machine learning set-up, we di-vided the treebank into: (i) a development set (219 sen-tences, 3271 tokens in the DSyntS treebank and 4953 tokensin the SSyntS treebank); (ii) a training set (3036 sentences,57665 tokens in the DSyntS treebank and 86984 tokens inthe SSyntS treebank); and a (iii) a held-out test for evalua-tion (258 sentences, 5641 tokens in the DSyntS treebank and8955 tokens in the SSyntS treebank).110Dev.set # %Hypernode identification 3133/3441 91.05Lemma generation 851/936 90.92Intra-hypernode dep.
generation 767/798 96.12Inter-hypernode dep.
generation 2574/3055 84.26Test set # %Hypernode identification 5169/5886 87.82Lemma generation 1913/2084 91.79Intra-hypernode dep.
generation 1630/1699 95.94Inter-hypernode dep.
generation 4648/5385 86.31Table 2: Results of the evaluation of the microclassifiers for the non-isomorphic transduction.30 points.
This means that the intra-hypernode de-pendency generation task is too sparse to be real-ized as a single classifier.
The micro classifiersare in this case binary, i.e., 2:1, or unary, i.e., 1:1classifiers, which implies a tremendous reductionof the search space (and thus higher accuracy).
Incontrast, the single classifier is a multi-class clas-sifier that must decide among more than 60 pos-sible classes.
Although most of these 60 classesare diferentiated by features, the differentiationis not perfect.
In the case of lemma generation,we observe a similar phenomenon.
In this case,the micro-classifiers are multi-class classifiers thatnormally have to cope with 5 different classes(lemmas in this case), while the unique classi-fier has to cope with around 60 different classes(or lemmas).
Hypernode identification and inter-hypernode dependency generation are completelyguided by the input; thus, it seems that they do noterr in the same way.Although the micro classifier approach leadsto significantly better results, we believe that itcan still be improved.
First, the introduction ofprepositions causes most errors in hypernode de-tection and lemma generation: when a prepositionshould be introduced or not and which preposi-tion should be introduced depends exclusively onthe sub-categorization frame of the governor ofthe deep node.
A treebank of a limited size asused in our experiments simply does not containsubcategorization patterns of all predicative lexi-cal items (especially of nouns)?which would becrucial.
Thus, in the test set evaluation, out of the171 lemma errors 147 are prepositions and out ofthe 717 errors on hypernode identification, morethan 500 are due to nouns and preposition.
The in-crease of the size of the treebank would thereforebe an advantage.Second, in the case of inter-hypernode depen-dency, errors are due to the labels of the dependen-cies more than to the attachements, and are quitedistributed over the different types of configura-tions.
The generation of these dependencies suf-fers from the fact that the SSyntS tag-set is veryfine-grained.
For instance, there are 9 differenttypes of verbal objects in SSyntS,7 which capturevery specific syntactic properties of Spanish, suchas ?can the dependent can be replaced by a cliticpronoun?
Can the dependent be moved away fromits governor?
Etc.
This kind of information is notof a high relevance for generation of well-formedtext.
Using a more reduced (more coarse-grained)SSyntS tag set would definitely improve the qual-ity of the projection.5 Related workThere is an increasing amount of work on sta-tistical sentence generation; see, e.g., (Bangaloreand Rambow, 2000; Langkilde-Geary, 2002; Fil-ippova and Strube, 2008).
However, hardly anyaddresses the problem of the projection betweennon-isomorphic semantic and syntactic structures.In general, structure prediction approaches usea single classifier model (Smith, 2011).
Butsee, e.g., (Carreras et al., 2008), who use dif-ferent models to predict each part of the tripletfor spinal model pruning, and (Bjo?rkelund et al.,2010; Johansson and Nugues, 2008), who usea set of classifiers for predicate identification inthe context of semantic role labelling.
Amalgam(Corston-Oliver et al., 2002), which maps a logi-cal input onto sentences with intermediate syntac-tic (phrase-based) representation, uses language-specific decision trees in order to predict when tointroduce auxiliaries, determiners, cases, etc.6 ConclusionsWe presented two alternative classifier approachesto deep generation that cope with the projectionof non-isomorphic semantic and syntactic struc-tures and argued that the micro classifier approachis more adequate.
In spite of possible improve-ments presented in Section 4, each set of microclassifiers achieves results above 86% on the testset.
For intra-hypernode dependency generation,it even reaches 95.94% .AcknowledgmentsThis work has been partially funded by the Euro-pean Commission under the contract number FP7-ICT-610411.7There are 47 SSynt dependencies in total, to compare tothe 7 dependencies in the DSyntS.111ReferencesOlga Babko-Malaya, 2005.
Propbank AnnotationGuidelines.Srinivas Bangalore and Owen Rambow.
2000.
Ex-ploiting a probabilistic hierarchical model for gener-ation.
In Proceedings of the 18th International Con-ference on Computational Linguistics (COLING),pages 42?48, Saarbru?cken, Germany.Anja Belz, Michael White, Dominic Espinosa, EricKow, Deirdre Hogan, and Amanda Stent.
2011.
Thefirst Surface Realisation Shared Task: Overview andevaluation results.
In Proceedings of the GenerationChallenges Session at the 13th European Workshopon Natural Language Generation (ENLG), pages217?226, Nancy, France.Anja Belz.
2008.
Automatic generation of weatherforecast texts using comprehensive probabilisticgeneration-space models.
Journal of Natural Lan-guage Engineering, 14(4):431?455.A.
Bjo?rkelund, B. Bohnet, L. Hafdell, and P. Nugues.2010.
A high-performance syntactic and semanticdependency parser.
In Proceedings of the 23rd In-ternational Conference on Computational Linguis-tics : Demonstration Volume (COLING), pages 33?36, Beijing, China.Bernd Bohnet, Leo Wanner, Simon Mille, and Ali-cia Burga.
2010.
Broad coverage multilingualdeep sentence generation with a stochastic multi-level realizer.
In Proceedings of the 23rd Inter-national Conference on Computational Linguistics(COLING), pages 98?106, Beijing, China.Bernd Bohnet, Simon Mille, Beno?
?t Favre, and LeoWanner.
2011.
StuMaBa: From deep representationto surface.
In Proceedings of the Generation Chal-lenges Session at the 13th European Workshop onNatural Language Generation (ENLG), pages 232?235, Nancy, France.Nadjet Bouayad-Agha, Gerard Casamayor, SimonMille, and Leo Wanner.
2012.
Perspective-orientedgeneration of football match summaries: Old tasks,new challenges.
ACM Transactions on Speech andLanguage Processing, 9(2):3:1?3:31.Xavier Carreras, Michael Collins, and Terry Koo.2008.
TAG, dynamic programming, and the per-ceptron for efficient, feature-rich parsing.
In Pro-ceedings of the 12th Conference on ComputationalNatural Language Learning (CoNLL), pages 9?16,Manchester, UK.Simon Corston-Oliver, Michael Gamon, Eric Ringger,and Robert Moore.
2002.
An overview of Amal-gam: A machine-learned generation module.
InProceedings of the 2nd International Natural Lan-guage Generation Conference (INLG), pages 33?40,New-York, NY, USA.Katja Filippova and Michael Strube.
2008.
Sen-tence fusion via dependency graph compression.In Proceedings of the 2008 Conference on Em-pirical Methods in Natural Language Processing(EMNLP), pages 177?185, Honolulu, Hawaii.Richard Johansson and Pierre Nugues.
2008.Dependency-based Semantic Role Labeling of Prop-Bank.
In Proceedings of the 2008 Conference onEmpirical Methods in Natural Language Processing(EMNLP), pages 69?78, Honolulu, Hawaii.Irene Langkilde-Geary.
2002.
An empirical verifi-cation of coverage and correctness for a general-purpose sentence generator.
In Proceedings ofthe 2nd International Natural Language GenerationConference (INLG), pages 17?24, New-York, NY,USA.
Citeseer.Simon Mille, Alicia Burga, and Leo Wanner.
2013.AnCora-UPF: A multi-level annotation of Spanish.In Proceedings of the 2nd International Conferenceon Dependency Linguistics (DepLing), pages 217?226, Prague, Czech Republic.Noah A. Smith.
2011.
Linguistic Structure Prediction.Synthesis Lectures on Human Language Technolo-gies.
Morgan and Claypool.Sebastian Varges and Chris Mellish.
2001.
Instance-based Natural Language Generation.
In Proceed-ings of the 2nd Meeting of the North AmericanChapter of the Association for Computational Lin-guistics (NAACL), pages 1?8, Pittsburgh, PA, USA.112
