Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 70?74,Prague, June 2007. c?2007 Association for Computational LinguisticsSemEval-2007 Task 14: Affective TextCarlo StrapparavaFBK ?
irstIstituto per la Ricerca Scientifica e TecnologicaI-38050, Povo, Trento, Italystrappa@itc.itRada MihalceaDepartment of Computer ScienceUniversity of North TexasDenton, TX, 76203, USArada@cs.unt.eduAbstractThe ?Affective Text?
task focuses on theclassification of emotions and valence (pos-itive/negative polarity) in news headlines,and is meant as an exploration of the connec-tion between emotions and lexical seman-tics.
In this paper, we describe the data setused in the evaluation and the results ob-tained by the participating systems.1 IntroductionAll words can potentially convey affective mean-ing.
Every word, even those apparently neutral, canevoke pleasant or painful experiences due to theirsemantic relation with emotional concepts or cate-gories.
Some words have emotional meaning withrespect to an individual story, while for many othersthe affective power is part of the collective imagina-tion (e.g., words such as ?mum?, ?ghost?, ?war?
).The automatic detection of emotion in texts isbecoming increasingly important from an applica-tive point of view.
Consider for example the tasksof opinion mining and market analysis, affectivecomputing, or natural language interfaces such ase-learning environments or educational/edutainmentgames.
Possible beneficial effects of emotions onmemory and attention of the users, and in general onfostering their creativity are also well-known in thefield of psychology.For instance, the following represent examplesof applicative scenarios in which affective analysiswould give valuable and interesting contributions:Sentiment Analysis.
Text categorization accordingto affective relevance, opinion exploration formarket analysis, etc.
are just some exam-ples of application of these techniques.
Whilepositive/negative valence annotation is an ac-tive field of sentiment analysis, we believe thata fine-grained emotion annotation would in-crease the effectiveness of these applications.Computer Assisted Creativity.
The automatedgeneration of evaluative expressions witha bias on some polarity orientation are akey component for automatic personalizedadvertisement and persuasive communication.Verbal Expressivity in Human Computer Interaction.Future human-computer interaction, accord-ing to a widespread view, will emphasizenaturalness and effectiveness and hence theincorporation of models of possibly many hu-man cognitive capabilities, including affectiveanalysis and generation.
For example, emo-tion expression by synthetic characters (e.g.,embodied conversational agents) is considerednow a key element for their believability.Affective words selection and understanding iscrucial for realizing appropriate and expressiveconversations.The ?Affective Text?
task was intended as an ex-ploration of the connection between lexical seman-tics and emotions, and an evaluation of various au-tomatic approaches to emotion recognition.The task is not easy.
Indeed, as (Ortony etal., 1987) indicates, besides words directly refer-ring to emotional states (e.g., ?fear?, ?cheerful?)
andfor which an appropriate lexicon would help, thereare words that act only as an indirect reference to70emotions depending on the context (e.g.
?monster?,?ghost?).
We can call the former direct affectivewords and the latter indirect affective words (Strap-parava et al, 2006).2 Task DefinitionWe proposed to focus on the emotion classificationof news headlines extracted from news web sites.Headlines typically consist of a few words and areoften written by creative people with the intentionto ?provoke?
emotions, and consequently to attractthe readers?
attention.
These characteristics makethis type of text particularly suitable for use in anautomatic emotion recognition setting, as the affec-tive/emotional features (if present) are guaranteed toappear in these short sentences.The structure of the task was as follows:Corpus: News titles, extracted from news web sites(such as Google news, CNN) and/or newspa-pers.
In the case of web sites, we can easilycollect a few thousand titles in a short amountof time.Objective: Provided a set of predefined six emotionlabels (i.e., Anger, Disgust, Fear, Joy, Sadness,Surprise), classify the titles with the appropri-ate emotion label and/or with a valence indica-tion (positive/negative).The emotion labeling and valence classificationwere seen as independent tasks, and thus a team wasable to participate in one or both tasks.
The taskwas carried out in an unsupervised setting, and con-sequently no training was provided.
The reason be-hind this decision is that we wanted to emphasize thestudy of emotion lexical semantics, and avoid bias-ing the participants toward simple ?text categoriza-tion?
approaches.
Nonetheless supervised systemswere not precluded from participation, and in suchcases the teams were allowed to create their own su-pervised training sets.Participants were free to use any resources theywanted.
We provided a set words extracted fromWordNet Affect (Strapparava and Valitutti, 2004),relevant to the six emotions of interest.
However,the use of this list was entirely optional.2.1 Data SetThe data set consisted of news headlines drawn frommajor newspapers such as New York Times, CNN,and BBC News, as well as from the Google Newssearch engine.
We decided to focus our attention onheadlines for two main reasons.
First, news havetypically a high load of emotional content, as theydescribe major national or worldwide events, and arewritten in a style meant to attract the attention of thereaders.
Second, the structure of headlines was ap-propriate for our goal of conducting sentence-levelannotations of emotions.Two data sets were made available: a develop-ment data set consisting of 250 annotated headlines,and a test data set with 1,000 annotated headlines.2.2 Data AnnotationTo perform the annotations, we developed a Web-based annotation interface that displayed one head-line at a time, together with six slide bars for emo-tions and one slide bar for valence.
The interval forthe emotion annotations was set to [0, 100], where 0means the emotion is missing from the given head-line, and 100 represents maximum emotional load.The interval for the valence annotations was set to[?100, 100], where 0 represents a neutral headline,?100 represents a highly negative headline, and 100corresponds to a highly positive headline.Unlike previous annotations of sentiment or sub-jectivity (Wiebe et al, 2005; Pang and Lee, 2004),which typically relied on binary 0/1 annotations, wedecided to use a finer-grained scale, hence allow-ing the annotators to select different degrees of emo-tional load.The test data set was independently labeled by sixannotators.
The annotators were instructed to selectthe appropriate emotions for each headline based onthe presence of words or phrases with emotionalcontent, as well as the overall feeling invoked bythe headline.
Annotation examples were also pro-vided, including examples of headlines bearing twoor more emotions to illustrate the case where sev-eral emotions were jointly applicable.
Finally, theannotators were encouraged to follow their ?first in-tuition,?
and to use the full-range of the annotationscale bars.712.3 Inter-Annotator AgreementWe conducted inter-tagger agreement studies foreach of the six emotions and for the valence an-notations.
The agreement evaluations were carriedout using the Pearson correlation measure, and areshown in Table 1.
To measure the agreement amongthe six annotators, we first measured the agreementbetween each annotator and the average of the re-maining five annotators, followed by an averageover the six resulting agreement figures.EMOTIONSAnger 49.55Disgust 44.51Fear 63.81Joy 59.91Sadness 68.19Surprise 36.07VALENCEValence 78.01Table 1: Pearson correlation for inter-annotatoragreement2.4 Fine-grained and Coarse-grainedEvaluationsFine-grained evaluations were conducted using thePearson measure of correlation between the systemscores and the gold standard scores, averaged overall the headlines in the data set.We have also run a coarse-grained evaluation,where each emotion was mapped to a 0/1 classifica-tion (0 = [0,50), 1 = [50,100]), and each valence wasmapped to a -1/0/1 classification (-1 = [-100,-50],0 = (-50,50), 1 = [50,100]).
For the coarse-grainedevaluations, we calculated accuracy, precision, andrecall.
Note that the accuracy is calculated with re-spect to all the possible classes, and thus it can beartificially high in the case of unbalanced datasets(as some of the emotions are, due to the high num-ber of neutral headlines).
Instead, the precision andrecall figures exclude the neutral annotations.3 Participating SystemsFive teams have participated in the task, with fivesystems for valence classification and three systemsfor emotion labeling.
The following represents ashort description of the systems.UPAR7: This is a rule-based system using a lin-guistic approach.
A first pass through the data ?un-capitalizes?
common words in the news title.
Thesystem then used the Stanford syntactic parser onthe modified title, and tried to identify what is beingsaid about the main subject by exploiting the depen-dency graph obtained from the parser.Each word was first rated separately for each emo-tion (the six emotions plus Compassion) and for va-lence.
Next, the main subject rating was boosted.Contrasts and accentuations between ?good?
or?bad?
were detected, making it possible to identifysurprising good or bad news.
The system also takesinto account: human will (as opposed to illness ornatural disasters); negation and modals; high-techcontext; celebrities.The lexical resource used was a combinationof SentiWordNet (Esuli and Sebastiani, 2006) andWordNetAffect (Strapparava and Valitutti, 2004),which were semi-automatically enriched on the ba-sis of the original trial data.SICS: The SICS team used a very simple ap-proach for valence annotation based on a word-spacemodel and a set of seed words.
The idea was to cre-ate two points in a high-dimensional word space -one representing positive valence, the other repre-senting negative valence - and then projecting eachheadline into this space, choosing the valence whosepoint was closer to the headline.The word space was produced from a lemmatizedand stop list filtered version of the LA times cor-pus (consisting of documents from 1994, releasedfor experimentation in the Cross Language Eval-uation Forum (CLEF)) using documents as con-texts and standard TFIDF weighting of frequencies.No dimensionality reduction was used, resulting ina 220,220-dimensional word space containing pre-dominantly syntagmatic relations between words.Valence vectors were created in this space by sum-ming the context vectors of a set of manually se-lected seed words (8 positive and 8 negative words).For each headline in the test data, stop words andwords with frequency above 10,000 in the LA timescorpus were removed.
The context vectors of the re-maining words were then summed, and the cosine ofthe angles between the summed vector and each ofthe valence vectors were computed, and the head-line was ascribed the valence value (computed as72[cosine * 100 + 50]) of the closest valence vector(headlines that were closer to the negative valencevector were assigned a negative valence value).
In11 cases, a value of -0.0 was ascribed either becauseno words were left in the headline after frequencyand stop word filtering, or because none of the re-maining words occurred in the LA times corpus andthus did not have any context vector.CLaC: This team submitted two systems to thecompetition: an unsupervised knowledge-based sys-tem (ClaC) and a supervised corpus-based system(CLaC-NB).
Both systems were used for assigningpositive/negative and neutral valence to headlines onthe scale [-100,100].CLaC: The CLaC system relies on a knowledge-based domain-independent unsupervised approachto headline valence detection and scoring.
Thesystem uses three main kinds of knowledge: alist of sentiment-bearing words, a list of valenceshifters and a set of rules that define the scope andthe result of the combination of sentiment-bearingwords and valence shifters.
The unigrams used forsentence/headline classification were learned fromWordNet dictionary entries.
In order to take advan-tage of the special properties of WordNet glossesand relations, we developed a system that used thelist of human-annotated adjectives from (Hatzivas-siloglou and McKeown, 1997) as a seed list andlearned additional unigrams from WordNet synsetsand glosses.
The list was then expanded by addingto it all the words annotated with Positive or Neg-ative tags in the General Inquirer.
Each unigram inthe resulting list had the degree of membership in thecategory of positive or negative sentiment assignedto it using the fuzzy Net Overlap Score method de-scribed in the team?s earlier work (Andreevskaia andBergler, 2006).
Only words with fuzzy member-ship score not equal to zero were retained in thelist.
The resulting list contained 10,809 sentiment-bearing words of different parts of speech.The fuzzy Net Overlap Score counts were com-plemented with the capability to discern and takeinto account some relevant elements of syntacticstructure of the sentences.
Two components wereadded to the system to enable this capability: (1)valence shifter handling rules and (2) parse treeanalysis.
The list of valence shifters was a com-bination of a list of common English negationsand a subset of the list of automatically obtainedwords with increase/decrease semantics, comple-mented with manual annotation.
The full list con-sists of 450 words and expressions.
Each entry inthe list of valence shifters has an action and scopeassociated with it, which are used by special han-dling rules that enable the system to identify suchwords and phrases in the text and take them into ac-count in sentence sentiment determination.
In orderto correctly determine the scope of valence shiftersin a sentence, the system used a parse tree analysisusing MiniPar.As a result of this processing, every headline re-ceived a system score assigned based on the com-bined fuzzy Net Overlap Score of its constituents.This score was then mapped into the [-100 to 100]scale as required by the task.CLaC-NB: In order to assess the performance ofbasic Machine Learning techniques on headlines,a second system ClaC-NB was also implemented.This system used a Na?
?ve Bayes classifier in order toassign valence to headlines.
It was trained on a smallcorpus composed of the development corpus of 250headlines provided for this competition, plus an ad-ditional 200 headlines manually annotated and 400positive and negative news sentences.
The probabil-ities assigned by the classifier were mapped to the [-100, 100] scale as follows: all negative headlines re-ceived the score of -100, all positive headlines wereassigned the score of +100, and the neutral headlinesobtained the score of 0.UA: In order to determine the kind and the amountof emotions in a headline, statistics were gatheredfrom three different web Search Engines: MyWay,AlltheWeb and Yahoo.
This information was used toobserve the distribution of the nouns, the verbs, theadverbs and the adjectives extracted from the head-line and the different emotions.The emotion scores were obtained through Point-wise Mutual Information (PMI).
First, the numberof documents obtained from the three web searchengines using a query that contains all the headlinewords and an emotion (the words occur in an inde-pendent proximity across the web documents) wasdivided by the number of documents containing onlyan emotion and the number of documents containingall the headline words.
Second, an associative scorebetween each content word and an emotion was es-73timated and used to weight the final PMI score.
Theobtained results were normalized in the 0-100 range.SWAT: SWAT is a supervised system using an u-nigram model trained to annotate emotional content.Synonym expansion on the emotion label words wasalso performed, using the Roget Thesaurus.
In addi-tion to the development data provided by the taskorganizers, the SWAT team annotated an additionalset of 1000 headlines, which was used for training.Fine Coarser Acc.
Prec.
Rec.
F1CLaC 47.70 55.10 61.42 9.20 16.00UPAR7 36.96 55.00 57.54 8.78 15.24SWAT 35.25 53.20 45.71 3.42 6.36CLaC-NB 25.41 31.20 31.18 66.38 42.43SICS 20.68 29.00 28.41 60.17 38.60Table 2: System results for valence annotationsFine Coarser Acc.
Prec.
Rec.
F1AngerSWAT 24.51 92.10 12.00 5.00 7.06UA 23.20 86.40 12.74 21.6 16.03UPAR7 32.33 93.60 16.67 1.66 3.02DisgustSWAT 18.55 97.20 0.00 0.00 -UA 16.21 97.30 0.00 0.00 -UPAR7 12.85 95.30 0.00 0.00 -FearSWAT 32.52 84.80 25.00 14.40 18.27UA 23.15 75.30 16.23 26.27 20.06UPAR7 44.92 87.90 33.33 2.54 4.72JoySWAT 26.11 80.60 35.41 9.44 14.91UA 2.35 81.80 40.00 2.22 4.21UPAR7 22.49 82.20 54.54 6.66 11.87SadnessSWAT 38.98 87.70 32.50 11.92 17.44UA 12.28 88.90 25.00 0.91 1.76UPAR7 40.98 89.00 48.97 22.02 30.38SurpriseSWAT 11.82 89.10 11.86 10.93 11.78UA 7.75 84.60 13.70 16.56 15.00UPAR7 16.71 88.60 12.12 1.25 2.27Table 3: System results for emotion annotations4 ResultsTables 2 and 3 show the results obtained by the par-ticipating systems.
The tables show both the fine-grained Pearson correlation measure and the coarse-grained accuracy, precision and recall figures.While further analysis is still needed, the resultsindicate that the task of emotion annotation is diffi-cult.
Although the Pearson correlation for the inter-tagger agreement is not particularly high, the gapbetween the results obtained by the systems and theupper bound represented by the annotator agreementsuggests that there is room for future improvements.AcknowledgmentsCarlo Strapparava was partially supported by theHUMAINE Network of Excellence.ReferencesA.
Andreevskaia and S. Bergler.
2006.
Senses and senti-ments: Sentiment tagging of adjectives at the meaninglevel.
In Proceedings of the 19th Canadian Confer-ence on Artificial Intelligence, AI?06, Quebec, Canada.A.
Esuli and F. Sebastiani.
2006.
SentiWordNet: Apublicly available lexical resource for opinion mining.In Proceedings of the Fifth International Conferenceon Language Resources and Evaluation (LREC 2006),Genoa, Italy, May.V.
Hatzivassiloglou and K. McKeown.
1997.
Predictingthe semantic orientation of adjectives.
In Proceedingsof the 35th Annual Meeting of the ACL, Madrid, Spain,July.A.
Ortony, G. L. Clore, and M. A. Foss.
1987.
The ref-erential structure of the affective lexicon.
CognitiveScience, (11).B.
Pang and L. Lee.
2004.
A sentimental education:Sentiment analysis using subjectivity summarizationbased on minimum cuts.
In Proceedings of the 42ndMeeting of the Association for Computational Linguis-tics, Barcelona, Spain, July.C.
Strapparava and A. Valitutti.
2004.
Wordnet-affect:an affective extension of wordnet.
In Proceedingsof the 4th International Conference on Language Re-sources and Evaluation, Lisbon.C.
Strapparava, A. Valitutti, and O.
Stock.
2006.
Theaffective weight of lexicon.
In Proceedings of the FifthInternational Conference on Language Resources andEvaluation.J.
Wiebe, T. Wilson, and C. Cardie.
2005.
Annotating ex-pressions of opinions and emotions in language.
Lan-guage Resources and Evaluation, 39(2-3):165?210.74
