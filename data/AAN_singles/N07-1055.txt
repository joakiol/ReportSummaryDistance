Proceedings of NAACL HLT 2007, pages 436?443,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsA Unified Local and Global Model for Discourse CoherenceMicha Elsner, Joseph Austerweil, and Eugene CharniakBrown Laboratory for Linguistic Information Processing (BLLIP)Brown UniversityProvidence, RI 02912{melsner,ec}@cs.brown.edu, joseph.austerweil@gmail.comAbstractWe present a model for discourse co-herence which combines the local entity-based approach of (Barzilay and Lapata,2005) and the HMM-based content modelof (Barzilay and Lee, 2004).
Unlike themixture model of (Soricut and Marcu,2006), we learn local and global featuresjointly, providing a better theoretical ex-planation of how they are useful.
As thelocal component of our model we adapt(Barzilay and Lapata, 2005) by relaxingindependence assumptions so that it is ef-fective when estimated generatively.
Ourmodel performs the ordering task compet-itively with (Soricut and Marcu, 2006),and significantly better than either of themodels it is based on.1 IntroductionModels of coherent discourse are central to severaltasks in natural language processing: such mod-els have been used in text generation (Kibble andPower, 2004) and evaluation of human-producedtext in educational applications (Miltsakaki and Ku-kich, 2004; Higgins et al, 2004).
Moreover, an ac-curate model can reveal information about documentstructure, aiding in such tasks as supervised summa-rization (Barzilay and Lapata, 2005).Models of coherence tend to fall into two classes.Local models (Lapata, 2003; Barzilay and Lapata,2005; Foltz et al, 1998) attempt to capture the gen-eralization that adjacent sentences often have similarcontent, and therefore tend to contain related words.Models of this type are good at finding sentencesthat belong near one another in the document.
How-ever, they have trouble finding the beginning or endof the document, or recovering from sudden shifts intopic (such as occur at paragraph boundaries).
Somelocal models also have trouble deciding which of apair of related sentences ought to come first.In contrast, the global HMM model of Barzilayand Lee (2004) tries to track the predictable changesin topic between sentences.
This gives it a pro-nounced advantage in ordering sentences, since itcan learn to represent beginnings, ends and bound-aries as separate states.
However, it has no localfeatures; the particular words in each sentence aregenerated based only on the current state of the doc-ument.
Since information can pass from sentenceto sentence only in this restricted manner, the modelsometimes fails to place sentences next to the correctneighbors.We attempt here to unify the two approaches byconstructing a model with both sentence-to-sentencedependencies providing local cues, and a hiddentopic variable for global structure.
Our local fea-tures are based on the entity grid model of (Barzilayand Lapata, 2005; Lapata and Barzilay, 2005).
Thismodel has previously been most successful in a con-ditional setting; to integrate it into our model, wefirst relax its independence assumptions to improveits performance when used generatively.
Our globalmodel is an HMM like that of Barzilay and Lee(2004), but with emission probabilities drawn fromthe entity grid.
We present results for two tasks,the ordering task, on which global models usuallydo well, and the discrimination task, on which lo-cal models tend to outperform them.
Our model im-proves on purely global or local approaches on both436tasks.Previous work by Soricut and Marcu (2006) hasalso attempted to integrate local and global fea-tures using a mixture model, with promising results.However, mixture models lack explanatory power;since each of the individual component models isknown to be flawed, it is difficult to say that the com-bination is theoretically more sound than the parts,even if it usually works better.
Moreover, since themodel we describe uses a strict subset of the fea-tures used in the component models of (Soricut andMarcu, 2006), we suspect that adding it to the mix-ture would lead to still further improved results.2 Naive Entity GridsEntity grids, first described in (Lapata and Barzilay,2005), are designed to capture some ideas of Cen-tering Theory (Grosz et al, 1995), namely that ad-jacent utterances in a locally coherent discourses arelikely to contain the same nouns, and that importantnouns often appear in syntactically important rolessuch as subject or object.
An entity grid representsa document as a matrix with a column for each en-tity, and a row for each sentence.
The entry ri,j de-scribes the syntactic role of entity j in sentence i:these roles are subject (S), object (O), or some otherrole (X)1.
In addition there is a special marker (-)for nouns which do not appear at all in a given sen-tence.
Each noun appears only once in a given rowof the grid; if a noun appears multiple times, its gridsymbol describes the most important of its syntac-tic roles: subject if possible, then object, or finallyother.
An example text is figure 1, whose grid is fig-ure 2.Nouns are also treated as salient or non-salient,another important concern of Centering Theory.
Wecondition events involving a noun on the frequencyof that noun.
Unfortunately, this way of representingsalience makes our model slightly deficient, sincethe model conditions on a particular noun occurringe.g.
2 times, but assigns nonzero probabilities todocuments where it occurs 3 times.
This is theo-1Roles are determined heuristically using trees produced bythe parser of (Charniak and Johnson, 2005).
Following previouswork, we slightly conflate thematic and syntactic roles, markingthe subject of a passive verb as O.2The numeric token ?1300?
is removed in preprocessing,and ?Nuevo Laredo?
is marked as ?PROPER?.0 [The commercial pilot]O , [sole occupant of [the airplane]X]X, was not injured .1 [The airplane]O was owned and operated by [a privateowner]X .2 [Visual meteorological conditions]S prevailed for [the per-sonal cross country flight for which [a VFR flight plan]O wasfiled]X .3 [The flight]S originated at [Nuevo Laredo , Mexico]X , at[approximately 1300]X .Figure 1: A section of a document, with syntacticroles of noun phrases marked.0 1 2 3PLAN - - O -AIRPLANE X O - -CONDITION - - S -FLIGHT - - X SPILOT O - - -PROPER - - - XOWNER - X - -OCCUPANT X - - -Figure 2: The entity grid for figure 12.retically quite unpleasant but in comparing differentorderings of the same document, it seems not to dotoo much damage.Properly speaking entities may be referents ofmany different nouns and pronouns throughout thediscourse, and both (Lapata and Barzilay, 2005) and(Barzilay and Lapata, 2005) present models whichuse coreference resolution systems to group nouns.We follow (Soricut and Marcu, 2006) in droppingthis component of the system, and treat each headnoun as having an individual single referent.To model transitions in this entity grid model,Lapata and Barzilay (2005) takes a generative ap-proach.
First, the probability of a document is de-fined as P (D) = P (Si..Sn), the joint probability ofall the sentences.
Sentences are generated in orderconditioned on all previous sentences:P (D) =?iP (Si|S0..(i?1)).
(1)We make a Markov assumption of order h (in ourexperiments h = 2) to shorten the history.
We repre-sent the truncated history as ~Shi?1 = S(i?h)..S(i?1).Each sentence Si can be split up into a set ofnouns representing entities, Ei, and their corre-sponding syntactic roles Ri, plus a set of wordswhich are not entities, Wi.
The model treats Wi asindependent of the previous sentences.
For any fixed437set of sentences Si,?i P (Wi) is always constant,and so cannot help in finding a coherent ordering.The probability of a sentence is therefore dependentonly on the entities:P (Si|~Sh(i?1)) = P (Ei, Ri|~Sh(i?1)).
(2)Next, the model assumes that each entity ej ap-pears in sentences and takes on syntactic roles in-dependent of all the other entities.
As we showin section 3, this assumption can be problem-atic.
Once we assume this, however, we can sim-plify P (Ei, Ri|~Sh(i?1)) by calculating for each en-tity whether it occurs in sentence i and if so, whichrole it takes.
This is equivalent to predicting ri,j .We represent the history of the specific entity ej as~r h(i?1),j = r(i?h),j ..r(i?1),j , and write:P (Ei, Ri|~Sh(i?1)) ?
?jP (ri,j|~r h(i?1),j).
(3)For instance, in figure 2, the probability of S3 withhorizon 1 is the product of P (S|X) (for FLIGHT),P (X|?)
(for PROPER), and likewise for each otherentity, P (?|O), P (?|S), P (?|?
)3.Although this generative approach outperformsseveral models in correlation with coherence ratingsassigned by human judges, it suffers in comparisonwith later systems.
Barzilay and Lapata (2005) usesthe same grid representation, but treats the transi-tion probabilities P (ri,j |~ri,j) for each document asfeatures for input to an SVM classifier.
Soricut andMarcu (2006)?s implementation of the entity-basedmodel also uses discriminative training.The generative model?s main weakness in com-parison to these conditional models is its assump-tion of independence between entities.
In real doc-uments, each sentence tends to contain only a fewnouns, and even fewer of them can fill roles likesubject and object.
In other words, nouns competewith each other for the available syntactic positionsin sentences; once one noun is chosen as the sub-ject, the probability that any other will also becomea subject (of a different subclause of the same sen-tence) is drastically lowered.
Since the generativeentity grid does not take this into account, it learnsthat in general, the probability of any given entityappearing in a specific sentence is low.
Thus it gen-erates blank sentences (those without any nouns atall) with overwhelmingly high probability.It may not be obvious that this misallocation ofprobability mass also reduces the effectiveness ofthe generative entity grid in ordering fixed sets ofsentences.
However, consider the case where an en-tity has a history ~r h, and then does not appear inthe next sentence.
The model treats this as evidencethat entities generally do not occur immediately af-ter ~r h?
but it may also happen that the entity wasoutcompeted by some other word with even moresignificance.3 Relaxed Entity GridIn this section, we relax the troublesome assump-tion of independence between entities, thus mov-ing the probability distribution over documents awayfrom blank sentences.
We begin at the same point asabove: sequential generation of sentences: P (D) =?i P (Si|S0..(i?1)).
We similarly separate the wordsinto entities and non-entities, treat the non-entities asindependent of the history ~S and omit them.
We alsodistinguish two types of entities.
Let the known setKi = ej : ej ?
~S(i?1), the set of all entities whichhave appeared before sentence i.
Of the entities ap-pearing in Si, those in Ki are known entities, andthose which are not are new entities.
Since each en-tity in the document is new precisely once, we treatthese as independent and omit them from our calcu-lations as we did the non-entities.
We return to bothgroups of omitted words in section 4 below whendiscussing our topic-based models.To model a sentence, then, we generate the set ofknown entities it contains along with their syntac-tic roles, given the history and the known set Ki.We truncate the history, as above, with horizon h;note that this does not make the model Markovian,since the known set has no horizon.
Finally, we con-sider only the portion of the history which relates toknown nouns (since all non-known nouns have thesame history - -).
In all the equations below, we re-strict Ei to known entities which actually appear insentence i, and Ri to roles filled by known entities.The probability of a sentence is now:P (Si|~Sh(i?1)) = P (Ei, Ri|~Rh(i?1)).
(4)We make one further simplification before begin-ning to approximate: we first generate the set of syn-tactic slots Ri which we intend to fill with known en-tities, and then decide which entities from the known438set to select.
Again, we assume independence fromthe history, so that the contribution of P (Ri) for anyordering of a fixed set of sentences is constant andwe omit it:P (Ei, Ri|~Rh(i?1),j) = P (Ei|Ri, ~Rh(i?1),j).
(5)Estimating P (Ei|Ri, ~Rh(i?1),j) proves to be dif-ficult, since the contexts are very sparse.
To con-tinue, we make a series of approximations.
First leteach role be filled individually (where r ?
e is theboolean indicator function ?noun e fills role r?
):P (Ei|Ri, ~Rh(i?1),j) ?
?r?RiP (r ?
ej |r, ~Rh(i?1),j).
(6)Notice that this process can select the same noun ejto fill multiple roles r, while the entity grid cannotrepresent such an occurrence.
The resulting distri-bution is therefore slightly deficient.Unfortunately, we are still faced with the sparsecontext ~Rh(i?1),j , the set of histories of all currentlyknown nouns.
It is much easier to estimate P (r ?ej |r,~r h(i?1),j), where we condition only on the his-tory of the particular noun which is chosen to fillslot r. However, in this case we do not have a properprobability distribution: i.e.
the probabilities do notsum to 1.
To overcome this difficulty we simply nor-malize by force3:P (r ?
ej|r, ~Rh(i?1),j) ?
(7)P (r ?
ej |r,~r h(i?1),j)?j?Ki P (r ?
ej|r,~r h(i?1),j)The individual probabilities P (r ?
ej |r,~r h(i?1),j)are calculated by counting situations in the train-ing documents in which a known noun has his-tory ~r h(i?1),j and fills slot r in the next sentence,versus situations where the slot r exists but isfilled by some other noun.
Some rare contexts arestill sparse, and so we smooth by adding a pseu-docount of 1 for all events.
Our model is ex-pressed by equations (1),(4),(5),(6) and (7).
In3Unfortunately this estimator is not consistent (that is, giveninfinite training data produced by the model, the estimated pa-rameters do not converge to the true parameters).
We are in-vestigating maximum entropy estimation as a solution to thisproblem.figure 2, the probability of S3 with horizon 1 isnow calculated as follows: the known set con-tains PLAN, AIRPLANE, CONDITION, FLIGHT,PILOT, OWNER and OCCUPANT.
There is one syn-tactic role filled by a known noun, S. The proba-bility is then calculated as P (+|S,X) (the proba-bility of selecting a noun with history X to fill therole of S) normalized by P (+|S,O)+P (+|S,S)+P (+|S,X) + 4?
P (+|S,?
).Like Lapata and Barzilay (2005), our relaxedmodel assigns low probability to sentences wherenouns with important-seeming histories do not ap-pear.
However, in our model, the penalty is lesssevere if there are many competitor nouns.
On theother hand, if the sentence contains many slots, giv-ing the noun more opportunity to fill one of them,the penalty is proportionally greater if it does notappear.4 Topic-Based ModelThe model we describe above is a purely local one,and moreover it relies on a particular set of local fea-tures which capture the way adjacent sentences tendto share lexical choices.
Its lack of any global struc-ture makes it impossible for the model to recover ata paragraph boundary, or to accurately guess whichsentence should begin a document.
Its lack of lexi-calization, meanwhile, renders it incapable of learn-ing dependences between pairs of words: for in-stance, that a sentence discussing a crash is oftenfollowed by a casualty report.We remedy both these problems by extending ourmodel of document generation.
Like Barzilay andLee (2004), we learn an HMM in which each sen-tence has a hidden topic qi, which is chosen con-ditioned on the previous state qi?1.
The emissionmodel of each state is an instance of the relaxed en-tity grid model as described above, but in additionto conditioning on the role and history, we condi-tion also on the state and on the particular set oflexical items lex(Ki) which may be selected to fillthe role: P (r ?
ej |r, ~Rh(i?1),j , qi, lex(Ki)).
Thisdistribution is approximated as above by the nor-malized value of P (r ?
ej |r,~r h(i?1),j , qi, lex(ej)).However, due to our use of lexical information,even this may be too sparse for accurate estima-tion, so we back off by interpolating with the pre-439Figure 3: A single time-slice of our HMM.Wi ?
PY (?|qi; ?LM , discountLM )Ni ?
PY (?|qi; ?NN , discountNN )Ei ?
EGrid(?|R, ~R2i?1, qi, lex(Ki); ?EG)qi ?
DP (?|qi?1)In the equations above, only the manually set inter-polation hyperparameters are indicated.vious model.
In each context, we introduce ?EGpseudo-observations, split fractionally according tothe backoff distribution: if we abbreviate the contextin the relaxed entity grid as C and the event as e, thissmoothing corresponds to:P (e|C, qi, ej) =#(e,C, qi, ej) + ?EGP (e|C)#(e,C, qi, ej) + ?EG.This is equivalent to defining the topic-based entitygrid as a Dirichlet process with parameter ?EG sam-pling from the relaxed entity grid.In addition, we are now in a position to gener-ate the non-entity words Wi and new entities Ni inan informative way, by conditioning on the sentencetopic qi.
Since they are interrupted by the knownentities, they do not form contiguous sequences ofwords, so we make a bag-of-words assumption.
Tomodel these sets of words, we use unigram ver-sions of the hierarchical Pitman-Yor processes of(Teh, 2006), which implement a Bayesian versionof Kneser-Ney smoothing.To represent the HMM itself, we adapt the non-parametric HMM of (Beal et al, 2001).
This isa Bayesian alternative to the conventional HMMmodel learned using EM, chosen mostly for conve-nience.
Our variant of it, unlike (Beal et al, 2001),has no parameter ?
to control self-transitions; ouremission model is complex enough to make it un-necessary.The actual number of states found by the modeldepends mostly on the backoff constants, the ?s(and, for Pitman-Yor processes, discounts) chosenfor the emission models (the entity grid, non-entityword model and new noun model), and is relativelyinsensitive to particular choices of prior for the otherhyperparameters.
As the backoff constants decrease,the emission models become more dependent on thestate variable q, which leads to more states (andeventually to memorization of the training data).
Ifinstead the backoff rate increases, the emission mod-els all become close to the general distribution andthe model prefers relatively few states.
We train withinterpolations which generally result in around 40states.Once the interpolation constants are set, themodel can be trained by Gibbs sampling.
We alsodo inference over the remaining hyperparameters ofthe model by Metropolis sampling from uninforma-tive priors.
Convergence is generally very rapid; weobtain good results after about 10 iterations.
UnlikeBarzilay and Lee (2004), we do not initialize withan informative starting distribution.When finding the probability of a test document,we do not do inference over the full Bayesian model,because the number of states, and the probability ofdifferent transitions, can change with every new ob-servation, making dynamic programming impossi-ble.
Beal et al (2001) proposes an inference algo-rithm based on particle filters, but we feel that inthis case, the effects are relatively minor, so we ap-proximate by treating the model as a standard HMM,using a fixed transition function based only on thetraining data.
This allows us to use the conventionalViterbi algorithm.
The backoff rates we choose attraining time are typically too small for optimal in-ference in the ordering task.
Before doing tests, weset them to higher values (determined to optimizeordering performance on held-out data) so that ouremission distributions are properly smoothed.5 ExperimentsOur experiments use the popular AIRPLANE cor-pus, a collection of documents describing airplanecrashes taken from the database of the National440Transportation Safety Board, used in (Barzilay andLee, 2004; Barzilay and Lapata, 2005; Soricut andMarcu, 2006).
We use the standard division ofthe corpus into 100 training and 100 test docu-ments; for development purposes we did 10-foldcross-validation on the training data.
The AIRPLANEdocuments have some advantages for coherence re-search: they are short (11.5 sentences on average)and quite formulaic, which makes it easy to find lex-ical and structural patterns.
On the other hand, theydo have some oddities.
46 of the training documentsbegin with a standard preamble: ?This is prelimi-nary information, subject to change, and may con-tain errors.
Any errors in this report will be correctedwhen the final report has been completed,?
whichessentially gives coherence models the first two sen-tences for free.
Others, however, begin abruptly withno introductory material whatsoever, and sometimeswithout even providing references for their definitenoun phrases; one document begins: ?At V1, theDC-10-30?s number 1 engine, a General ElectricCF6-50C2, experienced a casing breach when the2nd-stage low pressure turbine (LPT) anti-rotationnozzle locks failed.?
Even humans might have trou-ble identifying this sentence as the beginning of adocument.5.1 Sentence OrderingIn the sentence ordering task, (Lapata, 2003; Barzi-lay and Lee, 2004; Barzilay and Lapata, 2005; Sori-cut and Marcu, 2006), we view a document as anunordered bag of sentences and try to find the or-dering of the sentences which maximizes coherenceaccording to our model.
This type of ordering pro-cess has applications in natural language generationand multi-document summarization.
Unfortunately,finding the optimal ordering according to a prob-abilistic model with local features is NP-completeand non-approximable (Althaus et al, 2004).
More-over, since our model is not Markovian, the relax-ation used as a heuristic for A?
search by Soricutand Marcu (2006) is ineffective.
We therefore usesimulated annealing to find a high-probability order-ing, starting from a random permutation of the sen-tences.
Our search system has few Estimated SearchErrors as defined by Soricut and Marcu (2006); itrarely proposes an ordering which has lower proba-?
Discr.
(%)(Barzilay and Lapata, 2005) - 90(Barzilay and Lee, 2004) .44 745(Soricut and Marcu, 2006) .50 -6Topic-based (relaxed) .50 94Table 1: Results for AIRPLANE test data.bility than the original ordering4 .To evaluate the quality of the orderings we predictas optimal, we use Kendall?s ?
, a measurement ofthe number of pairwise swaps needed to transformour proposed ordering into the original document,normalized to lie between ?1 (reverse order) and 1(original order).
Lapata (2006) shows that it corre-sponds well with human judgements of coherenceand reading times.
A slight problem with ?
is thatit does not always distinguish between proposed or-derings of a document which disrupt local relation-ships at random, and orderings in which paragraph-like units move as a whole.
In longer documents, itmay be worth taking this problem into account whenselecting a metric; however, the documents in theAIRPLANE corpus are mostly short and have littleparagraph structure, so ?
is an effective metric.5.2 DiscriminationOur second task is the discriminative test used by(Barzilay and Lapata, 2005).
In this task we gen-erate random permutations of a test document, andmeasure how often the probability of a permutationis higher than that of the original document.
Thistask bears some resemblance to the task of discrim-inating coherent from incoherent essays in (Milt-sakaki and Kukich, 2004), and is also equivalentin the limit to the ranking metric of (Barzilay andLee, 2004), which we cannot calculate because ourmodel does not produce k-best output.
As opposedto the ordering task, which tries to measure howclose the model?s preferred orderings are to the orig-inal, this measurement assesses how many orderingsthe model prefers.
We use 20 random permutationsper document, for 2000 total tests.441?
Discr.
(%)Naive Entity Grid .17 81Relaxed Entity Grid .02 87Topic-based (naive) .39 85Topic-based (relaxed) .54 96Table 2: Results for 10-fold cross-validation on AIR-PLANE training data.6 ResultsSince the ordering task requires a model to proposethe complete structure for a set of sentences, it isvery dependent on global features.
To perform ad-equately, a model must be able to locate the begin-ning and end of the document, and place intermedi-ate sentences relative to these two points.
Withoutany way of doing this, our relaxed entity grid modelhas ?
of approximately 0, meaning its optimal or-derings are essentially uncorrelated with the correctorderings7 .
The HMM content model of (Barzilayand Lee, 2004), which does have global structure,performs much better on ordering, at ?
of .44.
How-ever, local features can help substantially for thistask, since models which use them are better at plac-ing related sentences next to one another.
Using bothsets of features, our topic-based model achieves stateof the art performance (?
= .5) on the ordering task,comparable with the mixture model of (Soricut andMarcu, 2006).The need for good local coherence features is es-pecially clear from the results on the discriminationtask (table 1).
Permuting a document may leave ob-vious ?signposts?
like the introduction and conclu-sion in place, but it almost always splits up manypairs of neighboring sentences, reducing local co-herence.
(Barzilay and Lee, 2004), which lacks lo-cal features, does quite poorly on this task (74%),while our model performs extremely well (94%).It is also clear from the results that our relaxed en-tity grid model (87%) improves substantially on thegenerative naive entity grid (81%).
When used on40 times on test data, 3 times in cross-validation.5Calculated on our test permutations using the code athttp://people.csail.mit.edu/regina/code.html.6Soricut and Marcu (2006) do not report results on this task,except to say that their implementation of the entity grid per-forms comparably to (Barzilay and Lapata, 2005).7Barzilay and Lapata (2005) do not report ?
scores.its own, it performs much better on the discrimina-tion task, which is the one for which it was designed.
(The naive entity grid has a higher ?
score, .17, es-sentially by accident.
It slightly prefers to generateinfrequent nouns from the start context rather thanthe context - -, which happens to produce the correctplacement for the ?preliminary information?
pream-ble.)
When used as the emission model for knownentities in our topic-based system, the relaxed en-tity grid shows its improved performance even morestrongly (table 2); its results are about 10% higherthan the naive version under both metrics.Our combined model uses only entity-grid fea-tures and unigram language models,a strict subset ofthe feature set of (Soricut and Marcu, 2006).
Theirmixture includes an entity grid model and a versionof the HMM of (Barzilay and Lee, 2004), whichuses n-gram language modeling.
It also uses a modelof lexical generation based on the IBM-1 model formachine translation, which produces all words in thedocument conditioned on words from previous sen-tences.
In contrast, we generate only entities con-ditioned on words from previous sentences; otherwords are conditionally independent given the topicvariable.
It seems likely therefore that using ourmodel as a component of a mixture might improveon the state of the art result.7 Future WorkOrdering in the AIRPLANE corpus and similar con-strained sets of short documents is by no means asolved problem, but the results so far show a gooddeal of promise.
Unfortunately, in longer and lessformulaic corpora, the models, inference algorithmsand even evaluation metrics used thus far may proveextremely difficult to scale up.
Domains with morenatural writing styles will make lexical prediction amuch more difficult problem.
On the other hand,the wider variety of grammatical constructions usedmay motivate more complex syntactic features, forinstance as proposed by (Siddharthan et al, 2004) insentence clustering.Finding optimal orderings is a difficult task evenfor short documents, and will become exponen-tially more challenging in longer ones.
For multi-paragraph documents, it is probably impractical touse full-scale coherence models to find optimal or-442derings directly.
A better approach may be a coarse-to-fine or hierarchical system which cuts up longerdocuments into more manageable chunks that can beordered as a unit.Multi-paragraph documents also pose a problemfor the ?
metric itself.
In documents with clear the-matic divisions between their different sections, agood ordering metric should treat transposed para-graphs differently than transposed sentences.8 AcknowledgementsWe are extremely grateful to Regina Barzilay, for hercode, data and extensive support, Mirella Lapata forcode and advice, and the BLLIP group, especiallyTom Griffiths, Sharon Goldwater and Mark Johnson,for comments and criticism.
We were supported byDARPA GALE contract HR0011-06-2-0001 and theKaren T. Romer Foundation.
Finally we thank threeanonymous reviewers for their comments.ReferencesErnst Althaus, Nikiforos Karamanis, and AlexanderKoller.
2004.
Computing locally coherent discourses.In Proceedings of the 42nd ACL, Barcelona.Regina Barzilay and Mirella Lapata.
2005.
Modeling lo-cal coherence: an entity-based approach.
In Proceed-ings of the 43rd Annual Meeting of the Association forComputational Linguistics (ACL?05).Regina Barzilay and Lillian Lee.
2004.
Catching thedrift: Probabilistic content models, with applicationsto generation and summarization.
In HLT-NAACL2004: Proceedings of the Main Conference, pages113?120.Matthew J. Beal, Zoubin Ghahramani, and Carl Ed-ward Rasmussen.
2001.
The infinite Hidden MarkovModel.
In NIPS, pages 577?584.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and MaxEnt discriminative rerank-ing.
In Proc.
of the 2005 Meeting of the Assoc.
forComputational Linguistics (ACL), pages 173?180.Peter Foltz, Walter Kintsch, and Thomas Landauer.1998.
The measurement of textual coherence withlatent semantic analysis.
Discourse Processes,25(2&3):285?307.Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.1995.
Centering: A framework for modeling the lo-cal coherence of discourse.
Computational Linguis-tics, 21(2):203?225.Derrick Higgins, Jill Burstein, Daniel Marcu, and Clau-dia Gentile.
2004.
Evaluating multiple aspects of co-herence in student essays.
In HLT-NAACL, pages 185?192.Roger Kibble and Richard Power.
2004.
Optimising ref-erential coherence in text generation.
ComputationalLinguistics, 30(4):401?416.Mirella Lapata and Regina Barzilay.
2005.
Automaticevaluation of text coherence: Models and representa-tions.
In IJCAI, pages 1085?1090.Mirella Lapata.
2003.
Probabilistic text structuring: Ex-periments with sentence ordering.
In Proceedings ofthe annual meeting of ACL, 2003.Mirella Lapata.
2006.
Automatic evaluation of informa-tion ordering: Kendall?s tau.
Computational Linguis-tics, 32(4):1?14.E.
Miltsakaki and K. Kukich.
2004.
Evaluation of textcoherence for electronic essay scoring systems.
Nat.Lang.
Eng., 10(1):25?55.Advaith Siddharthan, Ani Nenkova, and Kathleen McK-eown.
2004.
Syntactic simplification for improvingcontent selection in multi-document summarization.In COLING04, pages 896?902.Radu Soricut and Daniel Marcu.
2006.
Discourse gener-ation using utility-trained coherence models.
In Pro-ceedings of the Association for Computational Lin-guistics Conference (ACL-2006).Y.W.
Teh.
2006.
A Bayesian interpretation of interpo-lated Kneser-Ney.
Technical Report TRA2/06, Na-tional University of Singapore.443
