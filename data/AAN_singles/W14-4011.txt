Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 94?102,October 25, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsA CYK+ Variant for SCFG Decoding Without a Dot ChartRico SennrichSchool of InformaticsUniversity of Edinburgh10 Crichton StreetEdinburgh EH8 9ABScotland, UKv1rsennr@staffmail.ed.ac.ukAbstractWhile CYK+ and Earley-style variants arepopular algorithms for decoding unbina-rized SCFGs, in particular for syntax-based Statistical Machine Translation, thealgorithms rely on a so-called dot chartwhich suffers from a high memory con-sumption.
We propose a recursive vari-ant of the CYK+ algorithm that elimi-nates the dot chart, without incurring anincrease in time complexity for SCFG de-coding.
In an evaluation on a string-to-tree SMT scenario, we empirically demon-strate substantial improvements in mem-ory consumption and translation speed.1 IntroductionSCFG decoding can be performed with monolin-gual parsing algorithms, and various SMT sys-tems implement the CYK+ algorithm or a closeEarley-style variant (Zhang et al., 2006; Koehn etal., 2007; Venugopal and Zollmann, 2009; Dyeret al., 2010; Vilar et al., 2012).
The CYK+ algo-rithm (Chappelier and Rajman, 1998) generalizesthe CYK algorithm to n-ary rules by performing adynamic binarization of the grammar during pars-ing through a so-called dot chart.
The constructionof the dot chart is a major cause of space ineffi-ciency in SCFG decoding with CYK+, and mem-ory consumption makes the algorithm impracticalfor long sentences without artificial limits on thespan of chart cells.We demonstrate that, by changing the traver-sal through the main parse chart, we can elimi-nate the dot chart from the CYK+ algorithm at nocomputational cost for SCFG decoding.
Our algo-rithm improves space complexity, and an empiri-cal evaluation confirms substantial improvementsin memory consumption over the standard CYK+algorithm, along with remarkable gains in speed.This paper is structured as follows.
As mo-tivation, we discuss some implementation needsand complexity characteristics of SCFG decodingWe then describe our algorithm as a variant ofCYK+, and finally perform an empirical evalua-tion of memory consumption and translation speedof several parsing algorithms.2 SCFG DecodingTo motivate our algorithm, we want to highlightsome important differences between (monolin-gual) CFG parsing and SCFG decoding.Grammars in SMT are typically several ordersof magnitude larger than for monolingual parsing,partially because of the large amounts of trainingdata employed to learn SCFGs, partially becauseSMT systems benefit from using contextually richrules rather than only minimal rules (Galley et al.,2006).
Also, the same right-hand-side rule on thesource side can be associated with many trans-lations, and different (source and/or target) left-hand-side symbols.
Consequently, a compact rep-resentation of the grammar is of paramount impor-tance.We follow the implementation in the MosesSMT toolkit (Koehn et al., 2007) which encodesan SCFG as a trie in which each node representsa (partial or completed) rule, and a node has out-going edges for each possible continuation of therule in the grammar, either a source-side termi-nal symbol or pair of non-terminal-symbols.
If anode represents a completed rule, it is also asso-ciated with a collection of left-hand-side symbolsand the associated target-side rules and probabil-ities.
A trie data structure allows for an efficientgrammar lookup, since all rules with the same pre-94fix are compactly represented by a single node.Rules are matched to the input in a bottom-up-fashion as described in the next section.
A singlerule or rule prefix can match the input many times,either by matching different spans of the input, orby matching the same span, but with different sub-spans for its non-terminal symbols.
Each produc-tion is uniquely identified by a span, a grammartrie node, and back-pointers to its subderivations.The same is true for a partial production (dotteditem).A key difference between monolingual parsingand SCFG decoding, whose implications on timecomplexity are discussed by Hopkins and Lang-mead (2010), is that SCFG decoders need to con-sider language model costs when searching for thebest derivation of an input sentence.
This criticallyaffects the parser?s ability to discard dotted itemsearly.
For CFG parsing, we only need to keep onepartial production per rule prefix and span, or kfor k-best parsing, selecting the one(s) whose sub-derivations have the lower cost in case of ambigu-ity.
For SCFG decoding, the subderivation withthe higher local cost may be the globally betterchoice after taking language model costs into ac-count.
Consequently, SCFG decoders need to con-sider multiple possible productions for the samerule and span.Hopkins and Langmead (2010) provide a run-time analysis of SCFG decoding, showing thattime complexity depends on the number of choicepoints in a rule, i.e.
rule-initial, consecutive, orrule-final non-terminal symbols.1The number ofchoice points (or scope) gives an upper bound tothe number of productions that exist for a rule andspan.
If we define the scope of a grammar G tobe the maximal scope of all rules in the grammar,decoding can be performed in O(nscope(G)) time.If we retain all partial productions of the same ruleprefix, this also raises the space complexity of thedot chart from O(n2) to O(nscope(G)).2Crucially, the inclusion of language model costsboth increases the space complexity of the dotchart, and removes one of its benefits, namely theability to discard partial productions early withoutrisking search errors.
Still, there is a second way1Assuming that there is a constant upper bound on thefrequency of each symbol in the input sentence, and on thelength of rules.2In a left-to-right construction of productions, a rule pre-fix of a scope-x rule may actually have scope x + 1, namelyif the rule prefix ends in a non-terminal, but the rule does not.it is a trap12345678910it is a trap12345678910Figure 1: Traditional CYK/CYK+ chart traversalorder (left) and proposed order (right).in which a dot chart saves computational cost inthe CYK+ algorithm.
The exact chart traversal or-der is underspecified in CYK parsing, the only re-quirement being that all subspans of a given spanneed to be visited before the span itself.
CYK+or Earley-style parsers typically traverse the chartbottom-up left-to-right, as in Figure 1 (left).
Thesame partial productions are visited throughouttime during chart parsing, and storing them in adot chart saves us the cost of recomputing them.For example, step 10 in Figure 1 (left) re-uses par-tial productions that were found in steps 1, 5 and8.We propose to specify the chart traversal orderto be right-to-left, depth-first, as illustrated on theright-hand-side in Figure 1.
This traversal ordergroups all cells with the same start position to-gether, and offers a useful guarantee.
For eachspan, all spans that start at a later position havebeen visited before.
Thus, whenever we generatea partial production, we can immediately exploreall of its continuations, and then discard the par-tial production.
This eliminates the need for a dotchart, without incurring any computational cost.We could also say that the dot chart exists in aminimal form with at most one item at a time, anda space complexity of O(1).
We proceed with adescription of the proposed algorithm, contrastedwith the closely related CYK+ algorithm.3 Algorithm3.1 The CYK+ algorithmWe here summarize the CYK+ algorithm, orig-inally described by Chappelier and Rajman(1998).33Chappelier and Rajman (1998) add the restriction thatrules may not be partially lexicalized; our description ofCYK+, and our own algorithm, do not place this restriction.95The main data structure during decoding is achart with one cell for each span of words in aninput string w1...wnof length n. Each cell Ti,jcorresponding to the span from wito wjcontainstwo lists of items:4?
a list of type-1 items, which are non-terminals (representing productions).?
a list of type-2 items (dotted items), whichare strings of symbols ?
that parse the sub-string wi...wjand for which there is a rule inthe grammar of the form A ?
?
?, with ?being a non-empty string of symbols.
Suchan item may be completed into a type-1 itemat a future point, and is denoted ?
?.For each cell (i, j) of the chart, we perform thefollowing steps:1. if i = j, search for all rules A?
wi?.
If ?
isempty, add A to the type-1 list of cell (i, j);otherwise, add wi?
to the type-2 list of cell(i, j).2. if j > i, search for all combinations of a type-2 item ??
in a cell (i, k) and a type-1 item Bin a cell (k+1, j) for which a rule of the formA?
?B?
exists.5If ?
is empty, add the ruleto the type-1 list of cell (i, j); otherwise, add?B?
to the type-2 list of cell (i, j).3. for each item B in the type-1 list of the cell(i, j), if there is a rule of the form A ?
B?,and ?
is non-empty, add B?
to the type-2 listof cell (i, j).3.2 Our algorithmThe main idea behind our algorithm is that we canavoid the need to store type-2 lists if we processthe individual cells in a right-to-left, depth-first or-der, as illustrated in Figure 1.
Rules are still com-pleted left-to-right, but processing the rightmostcells first allows us to immediately extend partialproductions into full productions instead of storingthem in memory.We perform the following steps for each cell.1.
if i = j, if there is a rule A ?
wi, add A tothe type-1 list of cell (i, j).However, our description excludes non-lexical unary rules,and epsilon rules.4For simplicity, we describe a monolingual acceptor.5To allow mixed-terminal rules, we also search for B =wjif j = k + 1.2. if j > i, search for all combinations of a type-2 item ??
and a type-1 itemB in a cell (j, k),with j ?
k ?
n for which a rule of the formC ?
?B?
exists.
In the initial call, we allow??
= A?
for any type-1 item A in cell (i, j?1).6If ?
is empty, add C to the type-1 list ofcell (i, k); otherwise, recursively repeat thisstep, using ?B?
as ??
and k + 1 as j.To illustrate the difference between the two al-gorithms, let us consider the chart cell (1, 2), i.e.the chart cell spanning the substring it is, in Fig-ure 1, and let us assume the following grammar:S ?
NP V NPNP ?
ART NNNP ?
itV ?
isART ?
aNN ?
trapIn both algorithms, we can combine the sym-bols NP from cell (1, 1) and V from cell (2, 2) topartially parse the rule S ?
NP V NP.
How-ever, in CYK+, we cannot yet know if the rule canbe completed with a cell (3, x) containing symbolNP, since the cell (3, 4) may be processed after cell(1, 2).
Thus, the partial production is stored in atype-2 list for later processing.In our algorithm, we require all cells (3, x) tobe processed before cell (1, 2), so we can imme-diately perform a recursion with ?
= NP V andj = 3.
In this recursive step, we search for a sym-bol NP in any cell (3, x), and upon finding it incell (3, 4), add S as type-1 item to cell (1, 4).We provide side-by-side pseudocode of the twoalgorithms in Figure 2.7The algorithms arealigned to highlight their similarity, the main dif-ference between them being that type-2 items areadded to the dot chart in CYK+, and recursivelyconsumed in our variant.
An attractive propertyof the dynamic binarization in CYK+ is that eachpartial production is constructed exactly once, andcan be re-used to find parses for cells that covera larger span.
Our algorithm retains this property.Note that the chart traversal order is different be-tween the algorithms, as illustrated earlier in Fig-ure 1.
While the original CYK+ algorithm workswith either chart traversal order, our recursive vari-6To allow mixed-terminal rules, we also allow ??
= wi?if j = i+ 1, and B = wjif k = j.7Some implementation details are left out for simplicity.For instance, note that terminal and non-terminal grammartrie edges can be kept separate to avoid iterating over all ter-minal edges.96Algorithm 1: CYK+Input: array w of length Ninitialize chart[N,N ], collections[N,N ],dotchart[N ]root?
root node of grammar triefor span in [1..N]:for i in [1..(N-span+1)]:j?
i+span-1if i = j: #step 1if (w[i], X) in arc[root]:addToChart(X, i, j)else:for B in chart[i, j-1]: #step 3if (B, X) in arc[root]:if arc[X] is not empty:add (X, j-1) to dotchart[i]for (a, k) in dotchart[i]: #step 2if k+1 = j:if (w[j], X) in arc[a]:addToChart(X, i, j)for (B, X) in arc[a]:if B in chart[k+1, j]:addToChart(X, i, j)chart[i, j] = cube_prune(collections[i, j])def addToChart(trie node X, int i, int j):if X has target collection:add X to collections[i, j]if arc[X] is not empty:add (X, j) to dotchart[i]Algorithm 2: recursive CYK+Input: array w of length Ninitialize chart[N,N ], collections[N,N ]root?
root node of grammar triefor i in [N..1]:for j in [i..N]:if i = j: #step 1if (w[i], X) in arc[root]:addToChart(X, i, j, false)else: #step 2consume(root, i, i, j-1)chart[i, j] = cube_prune(collections[i, j])def consume(trie node a, int i, int j, int k):unary ?
i = jif j = k:if (w[j], X) in arc[a]:addToChart(X, i, k, unary)for (B, X) in arc[a]:if B in chart[j, k]:addToChart(X, i, k, unary)def addToChart(trie node X, int i, int j, bool u):if X has target collection and u is false:add X to collections[i, j]if arc[X] is not empty:for k in [(j+1)..N]:consume(X, i, j+1, k)Figure 2: side-by-side pseudocode of CYK+ (left) and our algorithm (right).
Our algorithm uses a newchart traversal order and recursive consume function instead of a dot chart.97ant requires a right-to-left, depth-first chart traver-sal.With our implementation of the SCFG as a trie,a type-2 is identified by a trie node, an array ofback-pointers to antecedent cells, and a span.
Wedistinguish between type-1 items before and aftercube pruning.
Productions, or specifically the tar-get collections and back-pointers associated withthem, are first added to a collections object, eithersynchronously or asynchronously.
Cube pruningis always performed synchronously after all pro-duction of a cell have been found.
Thus, the choiceof algorithm does not change the search space incube pruning, or the decoder output.
After cubepruning, the chart cell is filled with a mappingfrom a non-terminal symbol to an object that com-pactly represents a collection of translation hy-potheses and associated scores.3.3 Chart CompressionGiven a partial production for span (i, j), the num-ber of chart cells in which the production can becontinued is linear to sentence length.
The recur-sive variant explicitly loops through all cells start-ing at position j + 1, but this search also exists inthe original CYK+ in the form of the same type-2item being re-used over time.The guarantee that all cells (j+1, k) are visitedbefore cell (i, j) in the recursive algorithm allowsfor a further optimization.
We construct a com-pressed matrix representation of the chart, whichcan be incrementally updated in O(|V | ?n2), V be-ing the vocabulary of non-terminal symbols.
Foreach start position and non-terminal symbol, wemaintain an array of possible end positions andthe corresponding chart entry, as illustrated in Ta-ble 1.
The array is compressed in that it does notrepresent empty chart cells.
Using the previousexample, instead of searching all cells (3, x) fora symbol NP, we only need to retrieve the arraycorresponding to start position 3 and symbol NPto obtain the array of cells which can continue thepartial production.While not affecting the time complexity ofthe algorithm, this compression technique reducescomputational cost in two ways.
If the chart issparsely populated, i.e.
if the size of the arrays issmaller than n ?
j, the algorithm iterates throughfewer elements.
Even if the chart is dense, we onlyperform one chart look-up per non-terminal andpartial production, instead of n?
j.cell S NP V ART NN(3,3) 0x81(3,4) 0x86start symbol compressed column3 ART [(3, 0x81)]3 NP [(4, 0x86)]3 S,V,NN []Table 1: Matrix representation of all chart en-tries starting at position 3 (top), and equivalentcompressed representation (bottom).
Chart entriesare pointers to objects that represent collection oftranslation hypotheses and their scores.4 Related WorkOur proposed algorithm is similar to the workby Leermakers (1992), who describe a recursivevariant of Earley?s algorithm.
While they discussfunction memoization, which takes the place ofcharts in their work, as a space-time trade-off, akey insight of our work is that we can order thechart traversal in SCFG decoding so that partialproductions need not be tabulated or memoized,without incurring any trade-off in time complex-ity.Dunlop et al.
(2010) employ a similar matrixcompression strategy for CYK parsing, but theirmethod is different to ours in that they employ ma-trix compression on the grammar, which they as-sume to be in Chomsky Normal Form, whereas werepresent n-ary grammars as tries, and use matrixcompression for the chart.An obvious alternative to n-ary parsing is theuse of binary grammars, and early SCFG mod-els for SMT allowed only binary rules, as in thehierarchical models by Chiang (2007)8, or bina-rizable ones as in inversion-transduction grammar(ITG) (Wu, 1997).
Whether an n-ary rule can bebinarized depends on the rule-internal reorderingsbetween non-terminals; Zhang et al.
(2006) de-scribe a synchronous binarization algorithm.Hopkins and Langmead (2010) show that thecomplexity of parsing n-ary rules is determinedby the number of choice points, i.e.
non-terminalsthat are initial, consecutive, or final, since terminalsymbols in the rule constrain which cells are pos-sible application contexts of a non-terminal sym-bol.
They propose pruning of the SCFG to rules8Specifically, Chiang (2007) allows at most two non-terminals per rule, and no adjacent non-terminals on thesource side.98with at most 3 decision points, or scope 3, as analternative to binarization that allows parsing incubic time.
In a runtime evaluation, SMT withtheir pruned, unbinarized grammar offers a bet-ter speed-quality trade-off than synchronous bi-narization because, even though both have thesame complexity characteristics, synchronous bi-narization increases both the overall number ofrules, and the number of non-terminals, which in-creases the grammar constant.
In contrast, Chunget al.
(2011) compare binarization and Earley-styleparsing with scope-pruned grammars, and findEarley-style parsing to be slower.
They attributethe comparative slowness of Earley-style parsingto the cost of building and storing the dot chartduring decoding, which is exactly the problem thatour paper addresses.Williams and Koehn (2012) describe a parsingalgorithm motivated by Hopkins and Langmead(2010) in which they store the grammar in a com-pact trie with source terminal symbols or a genericgap symbol as edge labels.
Each path through thistrie corresponds to a rule pattern, and is associatedwith the set of grammar rules that share the samerule pattern.
Their algorithm initially constructs asecondary trie that records all rule patterns that ap-ply to the input sentence, and stores the position ofmatching terminal symbols.
Then, chart cells arepopulated by constructing a lattice for each rulepattern identified in the initial step, and traversingall paths through this lattice.
Their algorithm issimilar to ours in that they also avoid the construc-tion of a dot chart, but they construct two otherauxiliary structures instead: a secondary trie anda lattice for each rule pattern.
In comparison, ouralgorithm is simpler, and we perform an empiricalcomparison of the two in the next section.5 Empirical ResultsWe empirically compare our algorithm to theCYK+ algorithm, and the Scope-3 algorithm asdescribed by Williams and Koehn (2012), in astring-to-tree SMT task.
All parsing algorithmsare equivalent in terms of translation output, andour evaluation focuses on memory consumptionand speed.5.1 DataFor SMT decoding, we use the Moses toolkit(Koehn et al., 2007) with KenLM for languagemodel queries (Heafield, 2011).
We use trainingalgorithm n = 20 n = 40 n = 80Scope-3 0.02 0.04 0.34CYK+ 0.32 2.63 51.64+ recursive 0.02 0.04 0.15+ compression 0.02 0.04 0.15Table 2: Peak memory consumption (in GB) ofstring-to-tree SMT decoder for sentences of dif-ferent length n with different parsing algorithms.data from the ACL 2014 Ninth Workshop on Sta-tistical Machine Translation (WMT) shared trans-lation task, consisting of 4.5 million sentence pairsof parallel data and a total of 120 million sen-tences of monolingual data.
We build a string-to-tree translation system English?German, us-ing target-side syntactic parses obtained with thedependency parser ParZu (Sennrich et al., 2013).A synchronous grammar is extracted with GHKMrule extraction (Galley et al., 2004; Galley et al.,2006), and the grammar is pruned to scope 3.The synchronous grammar contains 38 millionrule pairs with 23 million distinct source-siderules.
We report decoding time for a random sam-ple of 1000 sentences from the newstest2013/4sets (average sentence length: 21.9 tokens), andpeak memory consumption for sentences of 20,40, and 80 tokens.
We do not report the timeand space required for loading the SMT models,which is stable for all experiments.9The parsingalgorithm only accounts for part of the cost duringdecoding, and the relative gains from optimizingthe parsing algorithm are highest if the rest of thedecoder is fast.
For best speed, we use cube prun-ing with language model boundary word grouping(Heafield et al., 2013) in all experiments.
We setno limit to the maximal span of SCFG rules, butonly keep the best 100 productions per span forcube pruning.
The cube pruning limit itself is setto 1000.5.2 Memory consumptionPeak memory consumption for different sentencelengths is shown in Table 2.
For sentences oflength 80, we observe more than 50 GB in peakmemory consumption for CYK+, which makesit impractical for long sentences, especially formulti-threaded decoding.
Our recursive variantskeep memory consumption small, as does the9The language model consumes 13 GB of memory, andthe SCFG 37 GB.
We leave the task of compacting the gram-mar to future research.990 20 40 60 800100200300400sentence lengthdecodingtime(seconds)Scope-3 parserCYK++ recursive+ compressionFigure 3: Decoding time per sentence as a func-tion of sentence length for four parsing variants.Regression curves use least squares fitting on cu-bic function.algorithmlength 80 randomparse total parse totalScope-3 74.5 81.1 1.9 2.6CYK+ 358.0 365.4 8.4 9.1+ recursive 33.7 40.1 1.5 2.2+ compression 15.0 21.2 1.0 1.7Table 3: Parse time and total decoding time persentence (in seconds) of string-to-tree SMT de-coder with different parsing algorithms.Scope-3 algorithm.
This is in line with our theoret-ical expectation, since both algorithms eliminatethe dot chart, which is the costliest data structurein the original CYK+ algorithm.5.3 SpeedWhile the main motivation for eliminating the dotchart was to reduce memory consumption, we alsofind that our parsing variants are markedly fasterthan the original CYK+ algorithm.
Figure 3 showsdecoding time for sentences of different lengthwith the four parsing variants.
Table 3 shows se-lected results numerically, and also distinguishesbetween total decoding time and time spent in theparsing block, the latter ignoring the cost of cubepruning and language model scoring.
If we con-sider parse time for sentences of length 80, we ob-serve a speed-up by a factor of 24 between ourfastest variant (with recursion and chart compres-sion), and the original CYK+.The gains from chart compression over the re-cursive variant ?
a factor 2 reduction in parse timefor sentences of length 80 ?
are attributable to areduction in the number of computational steps.The large speed difference between CYK+ andthe recursive variant is somewhat more surpris-ing, given the similarity of the two algorithms.Profiling results show that the recursive variant isnot only faster because it saves the computationaloverhead of creating and destroying the dot chart,but that it also has a better locality of reference,with markedly fewer CPU cache misses.Time differences are smaller for shorter sen-tences, both in terms of time spent parsing, and be-cause the time spent outside of parsing is a higherproportion of the total.
Still, we observe a factor5 speed-up in total decoding time on our randomtranslation sample from CYK+ to our fastest vari-ant.
We also observe speed-ups over the Scope-3parser, ranging from a factor 5 speed-up (parsingtime on sentences of length 80) to a 50% speed-up(total time on random translation sample).
It is un-clear to what extent these speed differences reflectthe cost of building the auxiliary data structures inthe Scope-3 parser, and how far they are due toimplementation details.5.4 Rule prefix scopeFor the CYK+ parser, the growth of both memoryconsumption and decoding time exceeds our cubicgrowth expectation.
We earlier remarked that therule prefix of a scope-3 rule may actually be scope-4 if the prefix ends in a non-terminal, but the ruleitself does not.
Since this could increase space andtime complexity of CYK+ to O(n4), we did addi-tional experiments in which we prune all scope-3rules with a scope-4 prefix.
This affected 1% ofall source-side rules in our model, and only hada small effect on translation quality (19.76 BLEU?
19.73 BLEU on newstest2013).
With this addi-tional pruning, memory consumption with CYK+is closer to our theoretical expectation, with a peakmemory consumption of 23 GB for sentences oflength 80 (?
23times more than for length 40).We also observe reductions in parse time as shownin Table 4.
While we do see marked reductionsin parse time for all CYK+ variants, our recursivevariants maintain their efficiency advantage overthe original algorithm.
Rule prefix scope is irrel-evant for the Scope-3 parsing algorithm10, and its10Despite its name, the Scope-3 parsing algorithm al-lows grammars of any scope, with a time complexity ofO(nscope(G)).100algorithmlength 80 randomfull pruned full prunedScope-3 74.5 70.1 1.9 1.8CYK+ 358.0 245.5 8.4 6.4+ recursive 33.7 24.5 1.5 1.2+ compression 15.0 10.5 1.0 0.8Table 4: Average parse time (in seconds) of string-to-tree SMT decoder with different parsing algo-rithms, before and after scope-3 rules with scope-4prefix have been pruned from grammar.speed is only marginally affected by this pruningprocedure.6 ConclusionWhile SCFG decoders with dot charts are stillwide-spread, we argue that dot charts are only oflimited use for SCFG decoding.
The core contri-butions of this paper are the insight that a right-to-left, depth-first chart traversal order allows forthe removal of the dot chart from the popularCYK+ algorithm without incurring any computa-tional cost for SCFG decoding, and the presen-tation of a recursive CYK+ variant that is basedon this insight.
Apart from substantial savingsin space complexity, we empirically demonstrategains in decoding speed.
The new chart traversalorder also allows for a chart compression strategythat yields further speed gains.Our parsing algorithm does not affect the searchspace or cause any loss in translation quality,and its speed improvements are orthogonal to im-provements in cube pruning (Gesmundo et al.,2012; Heafield et al., 2013).
The algorithmicmodifications to CYK+ that we propose are sim-ple, but we believe that the efficiency gains ofour algorithm are of high practical importance forsyntax-based SMT.
An implementation of the al-gorithm has been released as part of the MosesSMT toolkit.AcknowledgementsI thank Matt Post, Philip Williams, MarcinJunczys-Dowmunt and the anonymous reviewersfor their helpful suggestions and feedback.
Thisresearch was funded by the Swiss National Sci-ence Foundation under grant P2ZHP1_148717.ReferencesJean-C?dric Chappelier and Martin Rajman.
1998.
AGeneralized CYK Algorithm for Parsing StochasticCFG.
In TAPD, pages 133?137.David Chiang.
2007.
Hierarchical Phrase-BasedTranslation.
Comput.
Linguist., 33(2):201?228.Tagyoung Chung, Licheng Fang, and Daniel Gildea.2011.
Issues Concerning Decoding with Syn-chronous Context-free Grammar.
In ACL (ShortPapers), pages 413?417.
The Association for Com-puter Linguistics.Aaron Dunlop, Nathan Bodenstab, and Brian Roark.2010.
Reducing the grammar constant: an analysisof CYK parsing efficiency.
Technical report CSLU-2010-02, OHSU.Chris Dyer, Adam Lopez, Juri Ganitkevitch, JohnathanWeese, Ferhan Ture, Phil Blunsom, Hendra Seti-awan, Vladimir Eidelman, and Philip Resnik.
2010.cdec: A Decoder, Alignment, and Learning frame-work for finite-state and context-free translationmodels.
In Proceedings of the Association for Com-putational Linguistics (ACL).Michel Galley, Mark Hopkins, Kevin Knight, andDaniel Marcu.
2004.
What?s in a Translation Rule?In HLT-NAACL ?04.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In ACL-44: Proceedings of the 21st International Confer-ence on Computational Linguistics and the 44th an-nual meeting of the Association for ComputationalLinguistics, pages 961?968, Sydney, Australia.
As-sociation for Computational Linguistics.Andrea Gesmundo, Giorgio Satta, and James Hender-son.
2012.
Heuristic Cube Pruning in Linear Time.In Proceedings of the 50th Annual Meeting of theAssociation for Computational Linguistics: ShortPapers - Volume 2, ACL ?12, pages 296?300, JejuIsland, Korea.
Association for Computational Lin-guistics.Kenneth Heafield, Philipp Koehn, and Alon Lavie.2013.
Grouping Language Model Boundary Wordsto Speed K-Best Extraction from Hypergraphs.
InProceedings of the 2013 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 958?968, Atlanta, Georgia, USA.Kenneth Heafield.
2011.
KenLM: Faster and SmallerLanguage Model Queries.
In Proceedings of theSixth Workshop on Statistical Machine Translation,Edinburgh, UK.
Association for Computational Lin-guistics.Mark Hopkins and Greg Langmead.
2010.
SCFGDecoding Without Binarization.
In EMNLP, pages646?655.101Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ond?rej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: OpenSource Toolkit for Statistical Machine Translation.In Proceedings of the ACL-2007 Demo and PosterSessions, pages 177?180, Prague, Czech Republic.Association for Computational Linguistics.Ren?
Leermakers.
1992.
A recursive ascent Earleyparser.
Information Processing Letters, 41(2):87?91, February.Rico Sennrich, Martin Volk, and Gerold Schneider.2013.
Exploiting Synergies Between Open Re-sources for German Dependency Parsing, POS-tagging, and Morphological Analysis.
In Proceed-ings of the International Conference Recent Ad-vances in Natural Language Processing 2013, pages601?609, Hissar, Bulgaria.Ashish Venugopal and Andreas Zollmann.
2009.Grammar based statistical MT on Hadoop: An end-to-end toolkit for large scale PSCFG based MT.
ThePrague Bulletin of Mathematical Linguistics, 91:67?78.David Vilar, Daniel Stein, Matthias Huck, and Her-mann Ney.
2012.
Jane: an advanced freely avail-able hierarchical machine translation toolkit.
Ma-chine Translation, 26(3):197?216.Philip Williams and Philipp Koehn.
2012.
GHKMRule Extraction and Scope-3 Parsing in Moses.
InProceedings of the Seventh Workshop on Statisti-cal Machine Translation, pages 388?394, Montr?al,Canada, June.
Association for Computational Lin-guistics.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?403.Hao Zhang, Liang Huang, Daniel Gildea, and KevinKnight.
2006.
Synchronous Binarization for Ma-chine Translation.
In HLT-NAACL.
The Associationfor Computational Linguistics.102
