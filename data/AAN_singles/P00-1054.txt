Lexical transfer using a vector-space modelEiichiro SUMITAATR Spoken Language Translation Research Laboratories2-2 Hikaridai, Seika, SorakuKyoto 619-0288, Japansumita@slt.atr.co.jpAbstractBuilding a bilingual dictionary fortransfer in a machine translation system isconventionally done by hand and is verytime-consuming.
In order to overcomethis bottleneck, we propose a newmechanism for lexical transfer, which issimple and suitable for learning frombilingual corpora.
It exploits avector-space model developed ininformation retrieval research.
We presenta preliminary result from ourcomputational experiment.IntroductionMany machine translation systems havebeen developed and commercialized.
Whenthese systems are faced with unknown domains,however, their performance degrades.
Althoughthere are several reasons behind this poorperformance, in this paper, we concentrate onone of the major problems, i.e., building abilingual dictionary for transfer.A bilingual dictionary consists of rules thatmap a part of the representation of a sourcesentence to a target representation by takinggrammatical differences (such as the word orderbetween the source and target languages) intoconsideration.
These rules usually usecase-frames as their base and accompanysyntactic and/or semantic constraints onmapping from a source word to a target word.For many machine translation systems,experienced experts on individual systemscompile the bilingual dictionary, because this isa complicated and difficult task.
In other words,this task is knowledge-intensive andlabor-intensive, and therefore, time-consuming.Typically, the developer of a machinetranslation system has to spend several yearsbuilding a general-purpose bilingual dictionary.Unfortunately, such a general-purposedictionary is not almighty, in that (1) whenfaced with a new domain, unknown sourcewords may emerge and/or some domain-specificusages of known words may appear and (2) theaccuracy of the target word selection may beinsufficient due to the handling of many targetwords simultaneously.Recently, to overcome these bottlenecks inknowledge building and/or tuning, theautomation of lexicography has been studied bymany researchers: (1) approaches using adecision tree: the ID3 learning algorithm isapplied to obtain transfer rules from case-framerepresentations of simple sentences with athesaurus for generalization (Akiba et.
al., 1996and Tanaka, 1995); (2) approaches usingstructural matching: to obtain transfer rules,several search methods have been proposed formaximal structural matching between treesobtained by parsing bilingual sentences(Kitamura and Matsumoto, 1996; Meyers et.
al.,1998; and Kaji et.
al.,1992).1 Our proposal1.1 Our problem and approachIn this paper, we concentrate on lexicaltransfer, i.e., target word selection.
In otherwords, the mapping of structures betweensource and target expressions is not dealt withhere.
We assume that this structural transfer canbe solved on top of lexical transfer.We propose an approach that differs fromthe studies mentioned in the introduction sectionin that:I) It use not structural representationslike case frames but vector-spacerepresentations.II) The weight of each element forconstraining the ambiguity of targetwords is determined automatically byfollowing the term frequency andinverse document frequency ininformation retrieval research.III) A word alignment that does notrely on parsing is utilized.IV) Bilingual corpora are clustered interms of target equivalence.1.2 BackgroundThe background for the decisions made inour approach is as follows:A) We would like to reduce humaninteraction to prepare the datanecessary for building lexical transferrules.B) We do not expect that mature parsingsystems for multi-languages and/orspoken languages will be available inthe near future.C) We would like the determination ofthe importance of each feature in thetarget selection to be automated.D) We would like the problem caused byerrors in the corpora and datasparseness to be reduced.2 Vector-space modelThis section explains our trial for applyinga vector-space model to lexical transfer startingfrom a basic idea.2.1 Basic ideaWe can select an appropriate target wordfor a given source word by observing theenvironment including the context, worldknowledge, and target words in theneighborhood.
The most influential elements inthe environment are of course the other words inthe source sentence surrounding the concernedsource word.Suppose that we have translation examplesincluding the concerned source word and weknow in advance which target word correspondsto the source word.By measuring the similarity between (1) anunknown sentence that includes the concernedsource word and (2) known sentences thatinclude the concerned source word, we canselect the target word which is included in themost similar sentence.This is the same idea as example-basedmachine translation (Sato and Nagao, 1990 andFuruse et.
al., 1994).Group1: ??
(not sweet)source sentence 1: This beer is drier and full-bodied.target sentence 1: ?????????????????
?source sentence 2: Would you like dry or sweet sherry?target sentence 2: ?????????????????????????
?source sentence 3: A dry red wine would go well with it.target sentence 3: ?????????????????
?Group2: ??
(not wet)source sentence 4: Your skin feels so dry.target sentence 4: ???????????
?source sentence 5: You might want to use some cream to protect your skin against the dry air.target sentence 5: ?????????????????????????????
?Table 1 Portions of English ?dry?
into Japanese for an aligned corpusListed in Table 1 are samples ofEnglish-Japanese sentence pairs of our corpusincluding the source word ?dry.?
The upperthree samples of group 1 are translated with thetarget word ???
(not sweet)?
and the lowertwo samples of group 2 are translated with thetarget word ???
(not wet).?
The remainingportions of target sentences are hidden herebecause they do not relate to the discussion inthe paper.
The underlined words are some of thecues used to select the target words.
They aredistributed in the source sentence with severaldifferent grammatical relations such as subject,parallel adjective, modified noun, and so on, forthe concerned word ?dry.
?2.2 Sentence vectorWe propose representing the sentence as asentence vector, i.e., a vector that lists all of thewords in the sentence.
The sentence vector ofthe first sentence of Table 1 is as follows:<this, beer, is, dry, and, full-body>Figure 1 System ConfigurationFigure 1 outlines our proposal.
Supposethat we have the sentence vector of an inputsentence I and the sentence vector of anexample sentence E from a bilingual corpus.We measure the similarity by computingthe cosine of the angle between I and E.We output the target word of the examplesentence whose cosine is maximal.2.3 Modification of sentence vectorThe na?ve implementation of a sentencevector that uses the occurrence of wordsthemselves suffers from data sparseness andunawareness of relevance.2.3.1 Semantic category incorporationTo reduce the adverse influence of datasparseness, we count occurrences by not onlythe words themselves but also by the semanticcategories of the words given by a thesaurus.
Forexample, the ???
(not sweet)?
sentences ofVector generatorBilingual corpusCorpus vector, {E}ThesaurusInput sentenceInput vector, ICosine calculationThe most similar vectorTable 1 have the different cue words of ?beer,??sherry,?
and ?wine,?
and the cues are mergedinto a single semantic category alcohol in thesentence vectors.2.3.2 Grouping sentences and weightingdimensionsThe previous subsection does not considerthe relevance to the target selection of eachelement of the vectors; therefore, the selectionmay fail due to non-relevant elements.We exploit the term frequency and inversedocument frequency in information retrievalresearch.
Here, we regard a group of sentencesthat share the same target word as a document.
?Vectors are made not sentence-wise butgroup-wise.
The relevance of each dimension isthe term frequency multiplied by the inversedocument frequency.
The term frequency is thefrequency in the document (group).
A repetitiveoccurrence may indicate the importance of theword.
The inverse document frequencycorresponds to the discriminative power of thetarget selection.
It is usually calculated as alogarithm of N divided by df where N is thenumber of the documents (groups) and df is thefrequency of documents (groups) that includethe word.Cluster 1: a piece of paper money, C(??
)source sentence 1: May I have change for a ten dollar bill?target sentence 1: ????????????????
?source sentence 2: Could you change a fifty dollar bill?target sentence 2: ??????????????
?Cluster 2: an account, C(??
)source sentence 3: I've already paid the bill.target sentence 3: ??????????
?source sentence 4: Isn't my bill too high?target sentence 4: ?????????????
?source sentence 5: I'm checking out.
May I have the bill, please?target sentence 5: ????????????????
?Table 2 Samples of groups clustered by target equivalence3 Pre-processing of corpusBefore generating vectors, the givenbilingual corpus is pre-processed in two ways(1) words are aligned in terms of translation; (2)sentences are clustered in terms of targetequivalence to reduce problems caused by datasparseness.3.1 Word alignmentWe need to have source words and targetwords aligned in parallel corpora.
We use aword alignment program that does not rely onparsing (Sumita, 2000).
This is not the focus ofthis paper, and therefore, we will only describe itbriefly here.First, all possible alignments arehypothesized as a matrix filled with occurrencesimilarities between source words and targetwords.Second, using the occurrence similaritiesand other constraints, the most plausiblealignment is selected from the matrix.3.2 Clustering by target wordsWe adopt a clustering method to avoid thesparseness that comes from variations in targetwords.The translation of a word can vary morethan the meaning of the target word.
Forexample, the English word ?bill?
has two mainmeanings: (1) a piece of paper money, and (2)an account.
In Japanese, there is more than oneword for each meaning.
For (1), ???
and ????
can correspond, and for (2), ???,?
???,?
and ????
can correspond.The most frequent target word canrepresent the cluster, e.g., ????
for (1) a pieceof paper money; ????
for (2) an account.
Weassume that selecting a cluster is equal toselecting the target word.If we can merge such equivalent translationvariations of target words into clusters, we canimprove the accuracy of lexical transfer for tworeasons: (1) doing so makes the mark larger byneglecting accidental differences among targetwords; (2) doing so collects scattered pieces ofevidence and strengthens the effect.Furthermore, word alignment as anautomated process is incomplete.
We thereforeneed to filter out erroneous target words thatcome from alignment errors.
Erroneous targetwords are considered to be low in frequency andare expected to be semantically dissimilar fromcorrect target words based on correct alignment.Clustering example corpora can help filter outerroneous target words.By calculating the semantic similaritybetween the semantic codes of target words, weperform clustering according to the simplealgorithm in subsection 3.2.2.3.2.1 Semantic similaritySuppose each target word has semanticcodes for all of its possible meanings.
In ourthesaurus, for example, the target word ???
hasthree decimal codes, 974 (label/tag), 829(counter) and 975 (money) and the target word????
has a single code 975 (money).
Werepresent this as a code vector and define thesimilarity between the two target words bycomputing the cosine of the angle between theircode vectors.3.2.2 Clustering algorithmWe adopt a simple procedure to cluster aset of n target words X = {X1, X2,?, Xn}.
X issorted in the descending order of the frequencyof Xn in a sub-corpus including the concernedsource word.We repeat (1) and (2) until the set X isempty.
(1) We move the leftmost Xl from X tothe new cluster C(Xl).
(2) For all m (m>l) , we move Xm fromX to C(Xl) if the cosine of Xl andXm is larger than the threshold T.As a result, we obtain a set of clusters{C(Xl)} for each meaning as exemplified inTable 2.The threshold of semantic similarity T isdetermined empirically.
T in the experiment was1/2.4 ExperimentTo demonstrate the feasibility of ourproposal, we conducted a pilot experiment asexplained in this section.Number of sentence pairs (English-Japanese) 19,402Number of source words (English) 156,128Number of target words (Japanese) 178,247Number of source content words (English) 58,633Number of target content words (Japanese) 64,682Number of source different content words (English) 4,643Number of target different content words (Japanese) 6,686Table 3 Corpus statistics4.1 Experimental conditionsFor our sentence vectors and code vectors,we used hand-made thesauri of Japanese andEnglish covering our corpus (for a travelarrangement task), whose hierarchy is based onthat of the Japanese commercial thesaurusKadokawa Ruigo Jiten (Ohno and Hamanishi,1984).We used our English-Japanese phrase book(a collection of pairs of typical sentences andtheir translations) for foreign tourists.
Thestatistics of the corpus are summarized in Table3.
We word-aligned the corpus beforegenerating the sentence vectors.We focused on the transfer of contentwords such as nouns, verbs, and adjectives.
Wepicked out six polysemous words for apreliminary evaluation: ?bill,?
?dry,?
?call?in English and ??
,?
???
,?
???
?
inJapanese.We confined ourselves to a selectionbetween two major clusters of each source wordusing the method in subsection 3.2#1&2 #1 baseline #correct vsmbill [noun] 47 30 64% 40 85%call [verb] 179 93 52% 118 66%dry [adjective] 6 3 50% 4 67%?
[noun] 19 13 68% 14 73%??
[verb] 60 42 70% 49 82%??
[adjective] 26 15 57% 16 62%Table 4 Accuracy of the baseline and the VSM systems4.2 Selection accuracyWe compared the accuracy of our proposalusing the vector-space model  (vsm system)with that of a decision-by-majority model(baseline system).
The results are shown inTable 4.Here, the accuracy of the baseline system is#1 (the number of target sentences of the mostmajor cluster) divided by #1&2 (the number oftarget sentences of clusters 1 & 2).
The accuracyof the vsm system is #correct (the number ofvsm answers that match the target sentence)divided by #1&2.#all #1&2 Coveragebill [noun] 63 47 74%call [verb] 226 179 79%dry [adjective] 8 6 75%?
[noun] 22 19 86%??
[verb] 77 60 78%??
[adjective] 38 26 68%Table 5 Coverage of the top two clustersJudging was done mechanically byassuming that the aligned data was 100%correct.1 Our vsm system achieved an accuracyfrom about 60% to about 80% and outperformedthe baseline system by about 5% to about 20%.1  This does not necessarily hold, therefore,performance degrades in a certain degree.4.3 Coverage of major clustersOne reason why we clustered the exampledatabase was to filter out noise, i.e., wronglyaligned words.
We skimmed the clusters and wesaw that many instances of noise were filteredout.
At the same time, however, a portion ofcorrectly aligned data was unfortunatelydiscarded.
We think that such discarding is notfatal because the coverage of clusters 1&2 wasrelatively high, around 70% or 80% as shown inTable 5.
Here, the coverage is #1&2 (the numberof data not filtered) divided by #all (the numberof data before discarding).5 Discussion5.1 AccuracyAn experiment was done for a restrictedproblem, i.e., select the appropriate one cluster(target word) from two major clusters (targetwords), and the result was encouraging for theautomation of the lexicography for transfer.We plan to improve the accuracy obtainedso far by exploring elementary techniques: (1)Adding new features including extra linguisticinformation such as the role of the speaker of thesentence (Yamada et al, 2000) (also, the topicthat sentences are referring to) may be effective;and (2) Considering the physical distance fromthe concerned input word, which may improvethe accuracy.
A kind of window function mightalso be useful; (3) Improving the wordalignment, which may contribute to the overallaccuracy.5.2 Data sparsenessIn our proposal, deficiencies in the na?veimplementation of vsm are compensated inseveral ways by using a thesaurus, grouping, andclustering, as explained in subsections 2.3 and3.2.5.3 Future workWe showed only the translation of contentwords.
Next, we will explore the translation offunction words, the word order, and fullsentences.Our proposal depends on a handcraftedthesaurus.
If we manage to do withoutcraftsmanship, we will achieve broaderapplicability.
Therefore, automatic thesaurusconstruction is an important research goal for thefuture.ConclusionIn order to overcome a bottleneck inbuilding a bilingual dictionary, we proposed asimple mechanism for lexical transfer using avector space.A preliminary computational experimentshowed that our basic proposal is promising.Further development, however, is required: touse a window function or to use a betteralignment program; to compare other statisticalmethods such as decision trees, maximal entropy,and so on.Furthermore, an important future work is tocreate a full translation mechanism based on thislexical transfer.AcknowledgementsOur thanks go to Kadokawa-Shoten forproviding us with the Ruigo-Shin-Jiten.ReferencesAkiba, O., Ishii, M., ALMUALLIM, H., andKaneda, S. (1996) A Revision Learner to AcquireEnglish Verb Selection Rules, Journal of NLP, 3/3,pp.
53-68, (in Japanese).Furuse, O., Sumita, E. and Iida, H. (1994)Transfer-Driven Machine Translation UtilizingEmpirical Knowledge, Transactions of IPSJ, 35/3,pp.
414-425, (in Japanese).Kaji, H., Kida, Y. and Morimoto, Y.
(1992)Learning translation templates from bilingual text,Proc.
of Coling-92, pp.
672-678.Kitamura, M. and Matsumoto, Y.
(1996)Automatic Acquisition of Translation Rules fromParallel Corpora, Transactions of IPSJ, 37/6, pp.1030-1040, (in Japanese).Meyers, A., Yangarber, R., Grishman, R.,Macleod, C., and Sandoval, A.
(1998) DerivingTransfer rules from dominance-preservingalignments, Coling-ACL98, pp.
843-847.Ohno, S. and Hamanishi, M. (1984)Ruigo-Shin-Jiten, Kadokawa, p. 932, (in Japanese).Sato, S. and Nagao, M. (1990) Towardmemory-based translation, Coling-90, pp.
247-252.Sumita, E. (2000) Word alignment usingmatrix PRICAI-00, 2000, (to appear).Tanaka H. (1995) Statistical Learning of?Case Frame Tree?
for Translating English Verbs,Journal of NLP, 2/3, pp.
49-72, (in Japanese).Yamada, S., Sumita, E. and Kashioka, H.(2000) Translation using Information on DialogueParticipants, ANLP-00, pp.
37-43.
