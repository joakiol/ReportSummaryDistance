Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 27?35,Dublin, Ireland, August 23-24, 2014.SemEval-2014 Task 4: Aspect Based Sentiment AnalysisMaria PontikiInstitute for Languageand Speech Processing,?Athena?
Research Centermpontiki@ilsp.grHaris PapageorgiouInstitute for Languageand Speech Processing,?Athena?
Research Centerxaris@ilsp.grDimitrios GalanisInstitute for Languageand Speech Processing,?Athena?
Research Centergalanisd@ilsp.grIon AndroutsopoulosDept.
of InformaticsAthens University ofEconomics and Businession@aueb.grJohn PavlopoulosDept.
of Informatics,Athens University ofEconomics and Businessannis@aueb.grSuresh ManandharDept.
of Computer Science,University of Yorksuresh@cs.york.ac.ukAbstractSentiment analysis is increasingly viewedas a vital task both from an academic anda commercial standpoint.
The majority ofcurrent approaches, however, attempt todetect the overall polarity of a sentence,paragraph, or text span, irrespective of theentities mentioned (e.g., laptops) and theiraspects (e.g., battery, screen).
SemEval-2014 Task 4 aimed to foster research in thefield of aspect-based sentiment analysis,where the goal is to identify the aspectsof given target entities and the sentimentexpressed for each aspect.
The task pro-vided datasets containing manually anno-tated reviews of restaurants and laptops, aswell as a common evaluation procedure.
Itattracted 163 submissions from 32 teams.1 IntroductionWith the proliferation of user-generated content onthe web, interest in mining sentiment and opinionsin text has grown rapidly, both in academia andbusiness.
Early work in sentiment analysis mainlyaimed to detect the overall polarity (e.g., positiveor negative) of a given text or text span (Pang etal., 2002; Turney, 2002).
However, the need for amore fine-grained approach, such as aspect-based(or ?feature-based?)
sentiment analysis (ABSA),soon became apparent (Liu, 2012).
For example,laptop reviews not only express the overall senti-ment about a specific model (e.g., ?This is a greatThis work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/laptop?
), but also sentiments relating to its spe-cific aspects, such as the hardware, software, price,etc.
Subsequently, a review may convey opposingsentiments (e.g., ?Its performance is ideal, I wishI could say the same about the price?)
or objectiveinformation (e.g., ?This one still has the CD slot?
)for different aspects of an entity.ABSA is critical in mining and summarizingopinions from on-line reviews (Gamon et al.,2005; Titov and McDonald, 2008; Hu and Liu,2004a; Popescu and Etzioni, 2005).
In this set-ting, ABSA aims to identify the aspects of the en-tities being reviewed and to determine the senti-ment the reviewers express for each aspect.
Withinthe last decade, several ABSA systems of this kindhave been developed for movie reviews (Thet etal., 2010), customer reviews of electronic productslike digital cameras (Hu and Liu, 2004a) or net-book computers (Brody and Elhadad, 2010), ser-vices (Long et al., 2010), and restaurants (Ganu etal., 2009; Brody and Elhadad, 2010).Previous publicly available ABSA benchmarkdatasets adopt different annotation schemes withindifferent tasks.
The restaurant reviews dataset ofGanu et al.
(2009) uses six coarse-grained aspects(e.g., FOOD, PRICE, SERVICE) and four overallsentence polarity labels (positive, negative, con-flict, neutral).
Each sentence is assigned one ormore aspects together with a polarity label foreach aspect; for example, ?The restaurant was ex-pensive, but the menu was great.?
would be as-signed the aspect PRICE with negative polarity andFOOD with positive polarity.
In the product re-views dataset of Hu and Liu (2004a; 2004b), as-pect terms, i.e., terms naming aspects (e.g., ?ra-dio?, ?voice dialing?)
together with strength scores(e.g., ?radio?
: +2, ?voice dialing?
: ?3) are pro-27vided.
No predefined inventory of aspects is pro-vided, unlike the dataset of Ganu et al.The SemEval-2014 ABSA Task is based on lap-top and restaurant reviews and consists of foursubtasks (see Section 2).
Participants were free toparticipate in a subset of subtasks and the domains(laptops or restaurants) of their choice.2 Task DescriptionFor the first two subtasks (SB1, SB2), datasets onboth domains (restaurants, laptops) were provided.For the last two subtasks (SB3, SB4), datasets onlyfor the restaurant reviews were provided.Aspect term extraction (SB1): Given a set ofreview sentences, the task is to identify all as-pect terms present in each sentence (e.g., ?wine?,?waiter?, ?appetizer?, ?price?, ?food?).
We requireall the aspect terms to be identified, including as-pect terms for which no sentiment is expressed(neutral polarity).
These will be useful for con-structing an ontology of aspect terms and to iden-tify frequently discussed aspects.Aspect term polarity (SB2): In this subtask,we assume that the aspect terms are given (as de-scribed in SB1) and the task is to determine the po-larity of each aspect term (positive, negative, con-flict, or neutral).
The conflict label applies whenboth positive and negative sentiment is expressedabout an aspect term (e.g., ?Certainly not the bestsushi in New York, however, it is always fresh?
).An alternative would have been to tag the aspectterm in these cases with the dominant polarity, butthis in turn would be difficult to agree on.Aspect category detection (SB3): Given apredefined set of aspect categories (e.g., PRICE,FOOD) and a set of review sentences (but withoutany annotations of aspect terms and their polari-ties), the task is to identify the aspect categoriesdiscussed in each sentence.
Aspect categories aretypically coarser than the aspect terms as definedin SB1, and they do not necessarily occur as termsin the sentences.
For example, in ?Delicious butexpensive?, the aspect categories FOOD and PRICEare not instantiated through specific aspect terms,but are only inferred through the adjectives ?deli-cious?
and ?expensive?.
SB1 and SB3 were treatedas separate subtasks, thus no information linkingaspect terms to aspect categories was provided.Aspect category polarity (SB4): For this sub-task, aspect categories for each review sentenceare provided.
The goal is to determine the polar-ity (positive, negative, conflict, or neutral) of eachaspect category discussed in each sentence.Subtasks SB1 and SB2 are useful in cases whereno predefined inventory of aspect categories isavailable.
In these cases, frequently discussed as-pect terms of the entity can be identified togetherwith their overall sentiment polarities.
We hope toinclude an additional aspect term aggregation sub-task in future (Pavlopoulos and Androutsopoulos,2014b) to cluster near-synonymous (e.g., ?money?,?price?, ?cost?)
or related aspect terms (e.g., ?de-sign?, ?color?, ?feeling?)
together with their aver-aged sentiment scores as shown in Fig.
1.Figure 1: Aggregated aspect terms and averagesentiment polarities for a target entity.Subtasks SB3 and SB4 are useful when a pre-defined inventory of (coarse) aspect categories isavailable.
A table like the one of Fig.
1 can thenalso be generated, but this time using the mostfrequent aspect categories to label the rows, withstars showing the proportion of reviews express-ing positive vs. negative opinions for each aspectcategory.3 Datasets3.1 Data CollectionThe training and test data sizes are provided in Ta-ble 1.
The restaurants training data, consisting of3041 English sentences, is a subset of the datasetfrom Ganu et al.
(2009), which included annota-tions for coarse aspect categories (as in SB3) andoverall sentence polarities.
We added annotationsfor aspect terms occurring in the sentences (SB1),aspect term polarities (SB2), and aspect categorypolarities (SB4).
Additional restaurant reviewswere collected and annotated (from scratch) inthe same manner and used as test data (800 sen-tences).
The laptops dataset contains 3845 English28sentences extracted from laptop custumer reviews.Human annotators tagged the aspect terms (SB1)and their polarities (SB2); 3045 sentences wereused for training and 800 for testing (evaluation).Domain Train Test TotalRestaurants 3041 800 3841Laptops 3045 800 3845Total 6086 1600 7686Table 1: Sizes (sentences) of the datasets.3.2 Annotation ProcessFor a given target entity (a restaurant or a lap-top) being reviewed, the annotators were asked toprovide two types of information: aspect terms(SB1) and aspect term polarities (SB2).
For therestaurants dataset, two additional annotation lay-ers were added: aspect category (SB3) and aspectcategory polarity (SB4).The annotators used BRAT (Stenetorp et al.,2012), a web-based annotation tool, which wasconfigured appropriately for the needs of theABSA task.1Figure 2 shows an annotated sen-tence in BRAT, as viewed by the annotators.Stage 1: Aspect terms and polarities.
Duringa first annotation stage, the annotators tagged allthe single or multiword terms that named par-ticular aspects of the target entity (e.g., ?I likedthe service and the staff, but not the food?
?
{?service?, ?staff?, ?food?
}, ?The hard disk is verynoisy??
{?hard disk?}).
They were asked to tagonly aspect terms explicitly naming particular as-pects (e.g., ?everything about it?
or ?it?s expen-sive?
do not name particular aspects).
The as-pect terms were annotated as they appeared, evenif misspelled (e.g., ?warrenty?
instead of ?war-ranty?).
Each identified aspect term also had to beassigned a polarity label (positive, negative, neu-tral, conflict).
For example, ?I hated their fajitas,but their salads were great?
?
{?fajitas?
: nega-tive, ?salads?
: positive}, ?The hard disk is verynoisy??
{?hard disk?
: negative}.Each sentence of the two datasets was anno-tated by two annotators, a graduate student (an-notator A) and an expert linguist (annotator B).Initially, two subsets of sentences (300 from eachdataset) were tagged by annotator A and the anno-tations were inspected and validated by annotator1Consult http://brat.nlplab.org/ for more in-formation about BRAT.B.
The disagreements between the two annotatorswere confined to borderline cases.
Taking into ac-count the types of these disagreements (discussedbelow), annotator A was provided with additionalguidelines and tagged the remainder of the sen-tences in both datasets.2When A was not confi-dent, a decision was made collaboratively with B.When A and B disagreed, a decision was madecollaboratively by them and a third expert annota-tor.
Most disagreements fall into one of the fol-lowing three types:Polarity ambiguity: In several sentences, it wasunclear if the reviewer expressed positive or neg-ative opinion, or no opinion at all (just reportinga fact), due to lack of context.
For example, in?12.44 seconds boot time?
it is unclear if the re-viewer expresses a positive, negative, or no opin-ion about the aspect term ?boot time?.
In futurechallenges, it would be better to allow the annota-tors (and the participating systems) to consider theentire review instead of each sentence in isolation.Multi-word aspect term boundaries: In sev-eral cases, the annotators disagreed on the exactboundaries of multi-word aspect terms when theyappeared in conjunctions or disjunctions (e.g.,?selection of meats and seafoods?, ?noodle andrices dishes?, ?school or office use?).
In suchcases, we asked the annotators to tag as a sin-gle aspect term the maximal noun phrase (the en-tire conjunction or disjunction).
Other disagree-ments concerned the extent of the aspect termswhen adjectives that may or may not have a sub-jective meaning were also present.
For example,if ?large?
in ?large whole shrimp?
is part of thedish name, then the guidelines require the adjec-tive to be included in the aspect term; otherwise(e.g., in ?large portions?)
?large?
is a subjectivityindicator not to be included in the aspect term.
De-spite the guidelines, in some cases it was difficultto isolate and tag the exact aspect term, because ofintervening words, punctuation, or long-term de-pendencies.Aspect term vs. reference to target entity: Insome cases, it was unclear if a noun or noun phrasewas used as the aspect term or if it referred to theentity being reviewed as whole.
In ?This placeis awesome?, for example, ?place?
most probablyrefers to the restaurant as a whole (hence, it shouldnot be tagged as an aspect term), but in ?Cozy2The guidelines are available at: http://alt.qcri.org/semeval2014/task4/data/uploads/.29Figure 2: A sentence in the BRAT tool, annotated with four aspect terms (?appetizers?, ?salads?, ?steak?,?pasta?)
and one aspect category (FOOD).
For aspect categories, the whole sentence is tagged.place and good pizza?
it probably refers to the am-bience of the restaurant.
A broader context wouldagain help in some of these cases.We note that laptop reviews often evaluate eachlaptop as a whole, rather than expressing opinionsabout particular aspects.
Furthermore, when theyexpress opinions about particular aspects, they of-ten do so by using adjectives that refer implicitlyto aspects (e.g., ?expensive?, ?heavy?
), rather thanusing explicit aspect terms (e.g., ?cost?, ?weight?
);the annotators were instructed to tag only explicitaspect terms, not adjectives implicitly referring toaspects.
By contrast, restaurant reviews containmany more aspect terms (Table 2, last column).3Dataset Pos.
Neg.
Con.
Neu.
Tot.LPT-TR 987 866 45 460 2358LPT-TE 341 128 16 169 654RST-TR 2164 805 91 633 3693RST-TE 728 196 14 196 1134Table 2: Aspect terms and their polarities per do-main.
LPT and RST indicate laptop and restau-rant reviews, respectively.
TR and TE indicate thetraining and test set.Another difference between the two datasetsis that the neutral class is much more frequentin (the aspect terms of) laptops, since laptop re-views often mention features without expressingany (clear) sentiment (e.g., ?the latest version doesnot have a disc drive?).
Nevertheless, the positiveclass is the majority in both datasets, but it is muchmore frequent in restaurants (Table 2).
The ma-jority of the aspect terms are single-words in bothdatasets (2148 in laptops, 4827 in restaurants, outof 3012 and 4827 total aspect terms, respectively).Stage 2: Aspect categories and polarities.
Inthis task, each sentence needs to be tagged withthe aspect categories discussed in the sentence.The aspect categories are FOOD, SERVICE, PRICE,AMBIENCE (the atmosphere and environment of3We count aspect term occurrences, not distinct terms.a restaurant), and ANECDOTES/MISCELLANEOUS(sentences not belonging in any of the previousaspect categories).4For example, ?The restau-rant was expensive, but the menu was great?
isassigned the aspect categories PRICE and FOOD.Additionally, a polarity (positive, negative, con-flict, neutral) for each aspect category should beprovided (e.g., ?The restaurant was expensive, butthe menu was great??
{PRICE: negative, FOOD:positive}.One annotator validated the existing aspect cat-egory annotations of the corpus of Ganu et al.(2009).
The agreement with the existing anno-tations was 92% measured as average F1.
Mostdisagreements concerned additions of missing as-pect category annotations.
Furthermore, the sameannotator validated and corrected (if needed) theexisting polarity labels per aspect category anno-tation.
The agreement for the polarity labels was87% in terms of accuracy and it was measuredonly on the common aspect category annotations.The additional 800 sentences (not present in Ganuet al.
?s dataset) were used for testing and were an-notated from scratch in the same manner.
The dis-tribution of the polarity classes per category is pre-sented in Table 3.
Again, ?positive?
is the majoritypolarity class while the dominant aspect categoryis FOOD in both the training and test restaurantsentences.Determining the aspect categories of the sen-tences and their polarities (Stage 2) was an easiertask compared to detecting aspect terms and theirpolarities (Stage 1).
The annotators needed lesstime in Stage 2 and it was easier to reach agree-ment.
Exceptions were some sentences where itwas difficult to decide if the categories AMBIENCEor ANECDOTES/MISCELLANEOUS applied (e.g.,?One of my Fav spots in the city?).
We instructedthe annotators to classify those sentences only inANECDOTES/MISCELLANEOUS, if they conveyed4In the original dataset of Ganu et al.
(2009), ANECDOTESand MISCELLANEOUS were separate categories, but in prac-tice they were difficult to distinguish and we merged them.30Positive Negative Conflict Neutral TotalCategory Train Test Train Test Train Test Train Test Train TestFOOD 867 302 209 69 66 16 90 31 1232 418PRICE 179 51 115 28 17 3 10 1 321 83SERVICE 324 101 218 63 35 5 20 3 597 172AMBIENCE 263 76 98 21 47 13 23 8 431 118ANECD./MISC.
546 127 199 41 30 15 357 51 1132 234Total 2179 657 839 159 163 52 500 94 3713 1025Table 3: Aspect categories distribution per sentiment class.general views about a restaurant, without explic-itly referring to its atmosphere or environment.3.3 Format and Availability of the DatasetsThe datasets of the ABSA task were provided inan XML format (see Fig.
3).
They are avail-able with a non commercial, no redistribution li-cense through META-SHARE, a repository de-voted to the sharing and dissemination of languageresources (Piperidis, 2012).54 Evaluation Measures and BaselinesThe evaluation of the ABSA task ran in twophases.
In Phase A, the participants were askedto return the aspect terms (SB1) and aspect cate-gories (SB3) for the provided test datasets.
Subse-quently, in Phase B, the participants were giventhe gold aspect terms and aspect categories (asin Fig.
3) for the sentences of Phase A and theywere asked to return the polarities of the aspectterms (SB2) and the polarities of the aspect cate-gories of each sentence (SB4).6Each participat-ing team was allowed to submit up to two runsper subtask and domain (restaurants, laptops) ineach phase; one constrained (C), where only theprovided training data and other resources (e.g.,publicly available lexica) excluding additional an-notated sentences could be used, and one uncon-strained (U), where additional data of any kindcould be used for training.
In the latter case, theteams had to report the resources they used.To evaluate aspect term extraction (SB1) and as-pect category detection (SB3) in Phase A, we used5The datasets can be downloaded from http://metashare.ilsp.gr:8080/.
META-SHARE (http://www.meta-share.org/) was implemented in theframework of the META-NET Network of Excellence(http://www.meta-net.eu/).6Phase A ran from 9:00 GMT, March 24 to 21:00 GMT,March 25, 2014.
Phase B ran from 9:00 GMT, March 27 to17:00 GMT, March 29, 2014.the F1measure, defined as usually:F1=2 ?
P ?RP + R(1)where precision (P ) and recall (R) are defined as:P =|S ?G||S|, R =|S ?G||G|(2)Here S is the set of aspect term or aspect categoryannotations (in SB1 and SB3, respectively) that asystem returned for all the test sentences (of a do-main), and G is the set of the gold (correct) aspectterm or aspect category annotations.To evaluate aspect term polarity (SB2) and as-pect category polarity (SB4) detection in Phase B,we calculated the accuracy of each system, definedas the number of correctly predicted aspect termor aspect category polarity labels, respectively, di-vided by the total number of aspect term or aspectcategory annotations.
Recall that we used the goldaspect term and category annotations in Phase B.We provided four baselines, one per subtask:7Aspect term extraction (SB1) baseline: A se-quence of tokens is tagged as an aspect term ina test sentence (of a domain), if it is listed in adictionary that contains all the aspect terms of thetraining sentences (of the same domain).Aspect term polarity (SB2) baseline: For eachaspect term t in a test sentence s (of a particu-lar domain), this baseline checks if t had beenencountered in the training sentences (of the do-main).
If so, it retrieves the k most similar to straining sentences (of the domain), and assigns tothe aspect term t the most frequent polarity it hadin the k sentences.
Otherwise, if t had not been en-countered in the training sentences, it is assignedthe most frequent aspect term polarity label of the7Implementations of the baselines and further informationabout the baselines are available at: http://alt.qcri.org/semeval2014/task4/data/uploads/.31<sentence id="11351725#582163#9"><text>Our waiter was friendly and it is a shame that he didnt have a supportivestaff to work with.</text><aspectTerms><aspectTerm term="waiter" polarity="positive" from="4" to="10"/><aspectTerm term="staff" polarity="negative" from="74" to="79"/></aspectTerms><aspectCategories><aspectCategory category="service" polarity="conflict"/></aspectCategories></sentence>Figure 3: An XML snippet that corresponds to the annotated sentence of Fig.
2.training set.
The similarity between two sentencesis measured as the Dice coefficient of the sets of(distinct) words of the two sentences.
For exam-ple, the similarity between ?this is a demo?
and?that is yet another demo?
is2?24+5= 0.44.Aspect category extraction (SB3) baseline: Forevery test sentence s, the k most similar to s train-ing sentences are retrieved (as in the SB2 base-line).
Then, s is assigned the m most frequent as-pect category labels of the k retrieved sentences;m is the most frequent number of aspect categorylabels per sentence among the k sentences.Aspect category polarity (SB4): This baselineassigns to each aspect category c of a test sentences the most frequent polarity label that c had in thek most similar to s training sentences (of the samedomain), considering only training sentences thathave the aspect category label c. Sentence similar-ity is computed as in the SB2 baseline.For subtasks SB2 and SB4, we also use a major-ity baseline that assigns the most frequent polarity(in the training data) to all the aspect terms and as-pect categories.
The scores of all the baselines andsystems are presented in Tables 4?6.5 Evaluation ResultsThe ABSA task attracted 32 teams in total and 165submissions (systems), 76 for phase A and 89 forphase B.
Based on the human-annotation experi-ence, the expectations were that systems wouldperform better in Phase B (SB3, SB4, involvingaspect categories) than in Phase A (SB1, SB2, in-volving aspect terms).
The evaluation results con-firmed our expectations (Tables 4?6).5.1 Results of Phase AThe aspect term extraction subtask (SB1) attracted24 teams for the laptops dataset and 24 teams forthe restaurants dataset; consult Table 4.Laptops RestaurantsTeam F1Team F1IHS RD.
74.55?
DLIREC 84.01*DLIREC 73.78* XRCE 83.98DLIREC 70.4 NRC-Can.
80.18NRC-Can.
68.56 UNITOR 80.09UNITOR 67.95* UNITOR 79.96*XRCE 67.24 IHS RD.
79.62?SAP RI 66.6 UWB 79.35*IITP 66.55 SeemGo 78.61UNITOR 66.08 DLIREC 78.34SeemGo 65.99 ECNU 78.24ECNU 65.88 SAP RI 77.88SNAP 62.4 UWB 76.23DMIS 60.59 IITP 74.94UWB 60.39 DMIS 72.73JU CSE.
59.37 JU CSE.
72.34lsis lif 56.97 Blinov 71.21*USF 52.58 lsis lif 71.09Blinov 52.07* USF 70.69UFAL 48.98 EBDG 69.28*UBham 47.49 UBham 68.63*UBham 47.26* UBham 68.51SINAI 45.28 SINAI 65.41EBDG 41.52* V3 60.43*V3 36.62* UFAL 58.88COMMIT.
25.19 COMMIT.
54.38NILCUSP 25.19 NILCUSP 49.04iTac 23.92 SNAP 46.46iTac 38.29Baseline 35.64 Baseline 47.15Table 4: Results for aspect term extraction (SB1).Stars indicate unconstrained systems.
The ?
indi-cates a constrained system that was not trained onthe in-domain training dataset (unlike the rest ofthe constrained systems), but on the union of thetwo training datasets (laptops, restaurants).32Restaurants RestaurantsTeam F1Team Acc.NRC-Can.
88.57 NRC-Can.
82.92UNITOR 85.26* XRCE 78.14XRCE 82.28 UNITOR 76.29*UWB 81.55* SAP RI 75.6UWB 81.04 SeemGo 74.63UNITOR 80.76 SA-UZH 73.07SAP RI 79.04 UNITOR 73.07SNAP 78.22 UWB 72.78Blinov 75.27* UWB 72.78*UBham 74.79* lsis lif 72.09UBham 74.24 UBham 71.9EBDG 73.98* EBDG 69.75SeemGo 73.75 SNAP 69.56SINAI 73.67 COMMIT.
67.7JU CSE.
70.46 Blinov 65.65*lsis lif 68.27 Ualberta.
65.46ECNU 67.29 JU CSE.
64.09UFAL 64.51 ECNU 63.41V3 60.20* UFAL 63.21COMMIT.
59.3 iTac 62.73*iTac 56.95 ECNU 60.39*SINAI 60.29V3 47.21Baseline 65.65Baseline 63.89 Majority 64.09Table 5: Results for aspect category detection(SB3) and aspect category polarity (SB4).
Starsindicate unconstrained systems.Overall, the systems achieved significantlyhigher scores (+10%) in the restaurants domain,compared to laptops.
The best F1score (74.55%)for laptops was achieved by the IHS RD.
team,which relied on Conditional Random Fields (CRF)with features extracted using named entity recog-nition, POS tagging, parsing, and semantic anal-ysis.
The IHS RD.
team used additional reviewsfrom Amazon and Epinions (without annotatedterms) to learn the sentiment orientation of wordsand they trained their CRF on the union of therestaurant and laptop training data that we pro-vided; the same trained CRF classifier was thenused in both domains.The second system, the unconstrained system ofDLIREC, also uses a CRF, along with POS anddependency tree based features.
It also uses fea-tures derived from the aspect terms of the train-ing data and clusters created from additional re-views from YELP and Amazon.
In the restaurantsdomain, the unconstrained system of DLIRECranked first with an F1of 84.01%, but the bestunconstrained system, that of XRCE, was veryclose (83.98%).
The XRCE system relies on aparser to extract syntactic/semantic dependencies(e.g., ?dissapointed???food?).
For aspect term ex-traction, the parser?s vocabulary was enriched withthe aspect terms of the training data and a termlist extracted from Wikipedia and Wordnet.
A setof grammar rules was also added to detect multi-word terms and associate them with the corre-sponding aspect category (e.g., FOOD, PRICE).The aspect category extraction subtask (SB3)attracted 18 teams.
As shown in Table 5, the bestscore was achieved by the system of NRC-Canada(88.57%), which relied on five binary (one-vs-all)SVMs, one for each aspect category.
The SVMsused features based on various types of n-grams(e.g., stemmed) and information from a lexiconlearnt from YELP data, which associates aspectterms with aspect categories.
The latter lexiconsignificantly improved F1.
The constrained UN-ITOR system uses five SVMs with bag-of-words(BoW) features, which in the unconstrained sub-mission are generalized using distributional vec-tors learnt from Opinosis and TripAdvisor data.Similarly, UWB uses a binary MaxEnt classifierfor each aspect category with BoW and TF-IDFfeatures.
The unconstrained submission of UWBalso uses word clusters learnt using various meth-ods (e.g., LDA); additional features indicate whichclusters the words of the sentence being classi-fied come from.
XRCE uses information identi-fied by its syntactic parser as well as BoW featuresto train a logistic regression model that assigns tothe sentence probabilities of belonging to each as-pect category.
A probability threshold, tuned onthe training data, is then used to determine whichcategories will be assigned to the sentence.5.2 Results of Phase BThe aspect term polarity detection subtask (SB2)attracted 26 teams for the laptops dataset and 26teams for the restaurants dataset.
DCU and NRC-Canada had the best systems in both domains (Ta-ble 6).
Their scores on the laptops dataset wereidentical (70.48%).
On the laptops dataset, theDCU system performed slightly better (80.95%vs.
80.15%).
For SB2, both NRC-Canada andDCU relied on an SVM classifier with features33mainly based on n-grams, parse trees, and sev-eral out-of-domain, publicly available sentimentlexica (e.g., MPQA, SentiWordnet and Bing Liu?sOpinion Lexicon).
NRC-Canada also used twoautomatically compiled polarity lexica for restau-rants and laptops, obtained from YELP and Ama-zon data, respectively.
Furthermore, NRC-Canadashowed by ablation experiments that the most use-ful features are those derived from the sentimentlexica.
On the other hand, DCU used only publiclyavailable lexica, which were manually adapted byfiltering words that do not express sentiment inlaptop and restaurant reviews (e.g., ?really?)
andby adding others that were missing and do expresssentiment (e.g., ?mouthwatering?
).The aspect category polarity detection subtask(SB4) attracted 20 teams.
NRC-Canada again hadthe best score (82.92%) using an SVM classifier.The same feature set as in SB2 was used, but itwas further enriched to capture information re-lated to each specific aspect category.
The secondteam, XRCE, used information from its syntacticparser, BoW features, and an out-of-domain senti-ment lexicon to train an SVM model that predictsthe polarity of each given aspect category.6 Conclusions and Future WorkWe provided an overview of Task 4 of SemEval-2014.
The task aimed to foster research in aspect-based sentiment analysis (ABSA).
We constructedand released ABSA benchmark datasets contain-ing manually annotated reviews from two domains(restaurants, laptops).
The task attracted 163 sub-missions from 32 teams that were evaluated in foursubtasks centered around aspect terms (detectingaspect terms and their polarities) and coarser as-pect categories (assigning aspect categories andaspect category polarities to sentences).
The taskwill be repeated in SemEval-2015 with additionaldatasets and a domain-adaptation subtask.8In thefuture, we hope to add an aspect term aggrega-tion subtask (Pavlopoulos and Androutsopoulos,2014a).AcknowledgementsWe thank Ioanna Lazari, who provided an ini-tial version of the laptops dataset, Konstantina Pa-panikolaou, who carried out a critical part of the8Consult http://alt.qcri.org/semeval2015/task12/.Laptops RestaurantsTeam Acc.
Team Acc.DCU 70.48 DCU 80.95NRC-Can.
70.48 NRC-Can.
80.15?SZTE-NLP 66.97 UWB 77.68*UBham 66.66 XRCE 77.68UWB 66.66* SZTE-NLP 75.22lsis lif 64.52 UNITOR 74.95*USF 64.52 UBham 74.6SNAP 64.06 USF 73.19UNITOR 62.99 UNITOR 72.48UWB 62.53 SeemGo 72.31IHS RD.
61.62 lsis lif 72.13SeemGo 61.31 UWB 71.95ECNU 61.16 SA-UZH 70.98ECNU 61.16* IHS RD.
70.81SINAI 58.71 SNAP 70.81SAP RI 58.56 ECNU 70.72UNITOR 58.56* ECNU 70.72*SA-UZH 58.25 INSIGHT.
70.72COMMIT 57.03 SAP RI 69.92INSIGHT.
57.03 EBDG 68.6UMCC.
57.03* UMCC.
66.84*UFAL 56.88 UFAL 66.57UMCC.
56.11 UMCC.
66.57EBDG 55.96 COMMIT 65.96JU CSE.
55.65 JU CSE.
65.52UO UA 55.19* Blinov 63.58*V3 53.82 iTac 62.25*Blinov 52.29* V3 59.78iTac 51.83* SINAI 58.73DLIREC 36.54 DLIREC 42.32*DLIREC 36.54* DLIREC 41.71IITP 66.97 IITP 67.37Baseline 51.37 Baseline 64.28Majority 52.14 Majority 64.19Table 6: Results for the aspect term polarity sub-task (SB2).
Stars indicate unconstrained systems.The ?
indicates a constrained system that was nottrained on the in-domain training dataset (unlikethe rest of the constrained systems), but on theunion of the two training datasets.
IITP?s originalsubmission files were corrupted; they were resentand scored after the end of the evaluation period.annotation process, and Juli Bakagianni, who sup-ported our use of the META-SHARE platform.We are also very grateful to the participants fortheir feedback.
Maria Pontiki and Haris Papageor-giou were supported by the IS-HELLEANA (09-3472-922) and the POLYTROPON (KRIPIS-GSRT,MIS: 448306) projects.ReferencesSamuel Brody and Noemie Elhadad.
2010.
An unsu-pervised aspect-sentiment model for online reviews.In Proceedings of NAACL, pages 804?812, Los An-geles, California.Michael Gamon, Anthony Aue, Simon Corston-Oliver,and Eric K. Ringger.
2005.
Pulse: Mining customeropinions from free text.
In IDA, pages 121?132,Madrid, Spain.Gayatree Ganu, Noemie Elhadad, and Am?elie Marian.2009.
Beyond the stars: Improving rating predic-tions using review text content.
In Proceedings ofWebDB, Providence, Rhode Island, USA.Minqing Hu and Bing Liu.
2004a.
Mining and sum-marizing customer reviews.
In Proceedings of KDD,pages 168?177, Seattle, WA, USA.Minqing Hu and Bing Liu.
2004b.
Mining opinion fea-tures in customer reviews.
In Proceedings of AAAI,pages 755?760, San Jose, California.Bing Liu.
2012.
Sentiment Analysis and Opinion Min-ing.
Synthesis Lectures on Human Language Tech-nologies.
Morgan & Claypool Publishers.Chong Long, Jie Zhang, and Xiaoyan Zhu.
2010.
Areview selection approach for accurate feature ratingestimation.
In Proceedings of COLING (Posters),pages 766?774, Beijing, China.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
sentiment classification us-ing machine learning techniques.
In Proceedings ofEMNLP, pages 79?86, Philadelphia, Pennsylvania,USA.John Pavlopoulos and Ion Androutsopoulos.
2014a.Aspect term extraction for sentiment analysis: Newdatasets, new evaluation measures and an improvedunsupervised method.
In Proceedings of LASM-EACL, pages 44?52, Gothenburg, Sweden.John Pavlopoulos and Ion Androutsopoulos.
2014b.Multi-granular aspect aggregation in aspect-basedsentiment analysis.
In Proceedings of EACL, pages78?87, Gothenburg, Sweden.Stelios Piperidis.
2012.
The META-SHARE languageresources sharing infrastructure: Principles, chal-lenges, solutions.
In Proceedings of LREC-2012,pages 36?42, Istanbul, Turkey.Ana-Maria Popescu and Oren Etzioni.
2005.
Extract-ing product features and opinions from reviews.
InProceedings of HLT/EMNLP, pages 339?346, Van-couver, British Columbia, Canada.Pontus Stenetorp, Sampo Pyysalo, Goran Topi?c,Tomoko Ohta, Sophia Ananiadou, and Jun?ichi Tsu-jii.
2012.
BRAT: a web-based tool for NLP-assistedtext annotation.
In Proceedings of EACL, pages102?107, Avignon, France.Tun Thura Thet, Jin-Cheon Na, and Christopher S. G.Khoo.
2010.
Aspect-based sentiment analysis ofmovie reviews on discussion boards.
J. InformationScience, 36(6):823?848.Ivan Titov and Ryan T. McDonald.
2008.
A jointmodel of text and aspect ratings for sentiment sum-marization.
In Proceedings of ACL, pages 308?316,Columbus, Ohio, USA.Peter Turney.
2002.
Thumbs up or thumbs down?
se-mantic orientation applied to unsupervised classifi-cation of reviews.
In Proceedings of ACL, pages417?424, Philadelphia, Pennsylvania, USA.35
