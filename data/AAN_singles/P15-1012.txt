Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 116?125,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsJoint Models of Disagreement and Stance in Online DebateDhanya Sridhar,1James Foulds,1Bert Huang,2Lise Getoor,1Marilyn Walker11Department of Computer Science, University of California Santa Cruz{dsridhar, jfoulds, getoor, mawalker}@ucsc.edu2Department of Computer Science, Virginia Techbhuang@vt.eduAbstractOnline debate forums present a valu-able opportunity for the understanding andmodeling of dialogue.
To understand thesedebates, a key challenge is inferring thestances of the participants, all of whichare interrelated and dependent.
Whilecollectively modeling users?
stances hasbeen shown to be effective (Walker et al,2012c; Hasan and Ng, 2013), there aremany modeling decisions whose ramifi-cations are not well understood.
To in-vestigate these choices and their effects,we introduce a scalable unified probabilis-tic modeling framework for stance clas-sification models that 1) are collective,2) reason about disagreement, and 3) canmodel stance at either the author level orat the post level.
We comprehensivelyevaluate the possible modeling choices oneight topics across two online debate cor-pora, finding accuracy improvements ofup to 11.5 percentage points over a localclassifier.
Our results highlight the im-portance of making the correct modelingchoices for online dialogues, and having aunified probabilistic modeling frameworkthat makes this possible.1 IntroductionUnderstanding stance and opinion in dialoguescan provide critical insight into the theoretical un-derpinnings of discourse, argumentation, and sen-timent.
Systems for predicting the stances of indi-viduals can potentially have positive social impactand are of practical interest to non-profits, govern-mental organizations, and companies.
For exam-Dialogue Turns StanceUser 1: 18.
That?s the smoking age thats the shooting age.Why do you think they call it ATF?ANTIUser 2: Shooting age?
I know 7 year old shooters.
18 shouldbe the gun purchasing age, but there is really no ?shooting?age.ANTIUser 1: I know.
I was just pointing out that the logic used topropose a 21 year ?shooting age?
was inconsistent.ANTIUser 2: I see.
I dont think its really fair that you can join thearmy at 18 and use handguns and military weapons, but youcant purchase a handgun until 21.ANTIFigure 1: Example of a debate dialogue turn be-tween two users on the gun control topic, from4FORUMS.COM.ple, stance predictions may be used to target pub-lic awareness and advocacy campaigns, direct po-litical fundraising and get-out-the vote efforts, andimprove personalized recommendations.Online debate websites are a particularly richsource of argumentative dialogic data (Fig.
1).
Onthese websites, users debate and share their opin-ions on a variety of social and political issues.Previous work (Somasundaran and Wiebe, 2010;Walker et al, 2012c) has shown that stance clas-sification in online debates is a challenging prob-lem.
While collective approaches that jointly pre-dict user stance seem promising (Walker et al,2012c; Hasan and Ng, 2013), the rich structure ofonline debate forums necessitates many modelingchoices.
For example, users publish opinions andreply and respond to each others?
posts.
In so do-ing, they may agree or disagree with either all ora portion of another user?s post, suggesting thatcollective classifiers for stance may benefit fromtext-based disagreement modeling.
Furthermore,one can model stance either at the author level?assuming that an author?s stance is based on all oftheir posts on a topic (Burfoot et al, 2011)?or at116the post level?assuming that an author?s stanceis post-specific and may vary across posts (Hasanand Ng, 2013).
These decisions can drasticallychange the nature of stance models, so understand-ing their implications is critical.In this paper, we develop a flexible modelingframework for stance classification using proba-bilistic soft logic (PSL) (Bach et al, 2013; Bachet al, 2015), a recently introduced probabilis-tic modeling framework.1PSL is a probabilis-tic programming system that allows models to bespecified using a declarative, rule-like language.The resulting models are a special form of con-ditional random field, called a hinge-loss Markovrandom field, which admits highly scalable exactinference (Bach et al, 2013).
Modeling stancein large, richly connected online debate forumsrequires a careful exploration of many modelingchoices.
This complex domain especially benefitsfrom PSL?s flexibility and scalability.
PSL makesit easy to develop model variations and extensions,as one can readily incorporate new factors captur-ing additional intuitions about dependencies in adomain.We evaluate our models on data from twodebate sites, 4FORUMS and CREATEDEBATE(Walker et al, 2012b; Hasan and Ng, 2013), whichwe describe in detail in Section 2.
Our experi-mental results show that there are important rami-fications of several modeling decisions, includingwhether to use collective or non-collective mod-els, to represent stance at the post level or the au-thor level, and how to model disagreement.
Wefind that with appropriate modeling choices, ourapproach leads to improvements of up to 11.5 per-centage points of accuracy over simple classifica-tion approaches.Our contributions include (1) a flexible, unifiedframework for modeling online debates, (2) ex-tensive experimental study of many possible mod-els on eight forum datasets, collected across twodifferent debate websites, and (3) general model-ing recommendations resulting from our empiricalstudies.2 Online Debate ForumsOnline debate forums represent richly structuredargumentative dialogues.
On these forums, usersdebate with each other in discussion threads on a1PSL is an open-source Java toolkit, available here:http://psl.cs.umd.edu.variety of topics or issues, such as gun control, gaymarriage, and marijuana legalization.
Each dis-cussion consists of a number of posts, which areshort text documents authored by users of the fo-rum.
A post is either a reply to a previous post,or it is the start (root) of a thread.
As users en-gage with each other, a thread branches out intoa tree of argumentative interactions between theusers.
Forum users often post numerous timesand across multiple discussions and topics, whichcreates a richly structured interaction graph.
On-line debates present different challenges than morecontrolled dialogic settings such as congressionaldebates.
Posts are short and informal, there is lim-ited external information about authors, and de-bate topics admit many modes of argumentationranging from serious, to tangential, to sarcastic.The reply graph in online debates also has sub-stantially different semantics to networks in otherdebate settings, such as the graph of speaker men-tions in congressional debates.
To illustrate thissetting, Fig.
1 shows an example dialogue betweentwo users who are debating their opinions on thetopic of gun control.In the context of online debate forums, stanceclassification (Thomas et al, 2006; Somasundaranand Wiebe, 2009) is the task of assigning stancelabels with respect to a discussion topic, either atthe level of the user or the level of the post.
Stanceis typically treated as a binary classification prob-lem, with labels PRO and ANTI.
In Fig.
1, bothusers?
stances toward gun control are ANTI.Previous work on stance in online debates hasshown that contextual information given by replylinks is important for stance classification (Walkeret al, 2012a), and that collective classification of-ten outperforms methods which treat each postindependently.
Hasan and Ng (2013) use condi-tional random fields (CRFs) to encourage oppositestances between sequences of posts, and Walker etal.
(2012c) use MaxCut over explicitly given re-buttal links between posts to separate them intoPRO and ANTI clusters.
Sridhar et al (2014) usehinge-loss Markov random fields (HL-MRFs) toencourage consistency between post level stancelabels and observed post-level textual agreementsand disagreements.While the first two approaches leverage rebuttalor reply links, they model reply links as being in-dicative of opposite stances.
However, as shown inFig.
1, responses?even rebuttals?can occur be-117tween users with the same stance, which suggeststhe benefit of a more nuanced treatment of replylinks.
The approach of Sridhar et al (2014) con-siders text-based agreement annotations betweenposts, though it requires that reply links are la-beled.
Accurate reply polarity labels are likely tobe as expensive to obtain as the stance labels thatwe aim to predict.
Noisy or sparse reply labels arecheaper, though likely to reduce performance.
Inthis work, we show how to reason over uncertainreply label predictions to improve stance classifi-cation.Also in the online debate setting, Hasan and Ng(2014) show the benefits of joint modeling to clas-sify post-level stance and the authors?
reasons fortheir stances.
In contrast, in this work we focus onthe dependencies between stance and polarity ofreplies.In the context of opinion subgroup discov-ery, Abu-Jbara and Radev (2013) demonstratethe effectiveness of clustering users by opinion-target similarity.
In contrast, Murakami and Ray-mond (2010) use simple recurring patterns suchas ?that?s a good idea?
to categorize reply linksas agree, disagree or neutral, prior to using Max-Cut for subgroup clustering of comment streamson government websites.
This approach improvesover a MaxCut approach that casts all reply linksas disagreements.
Building on this work, Lu et al(2012) model unsupervised discovery of support-ing and opposing groups of users for topics in on-line military forums.
They improve upon a Max-Cut baseline by formulating a linear program (LP)to combine multiple textual and reply-link signals,suggesting the benefits of jointly modeling textualand reply-link features.In a different line of work, while Somasundaranand Wiebe (2010) do not use relational informa-tion between users or posts, their approach showsthe benefit of modeling opinions and their targetsat a fine-grained level using relational sentimentanalysis techniques.
Similarly, Wang and Cardie(2014) demonstrate the effectiveness of using sen-timent analysis to identify disputes on WikipediaTalk pages.
Boltu?zi?c and?Snajder (2014) andGhosh et al (2014) study various linguistic fea-tures to model stance and agreement interactionsrespectively.In the congressional debate setting, approachesusing CRFs and similar collective techniques suchas minimum-cut have also leveraged reply link4FORUMS CREATEDEBATEUsers per topic 336 311Posts per user, per topic 19 4Words per user, per topic 2511 476Words per post 134 124Distinct reply links 6 3per user, per topicStance labels given for Users Posts%Post-level reply links 71.6 73.9have opposite-stance users%Author-level reply links 52.0 68.9have opposite-stance usersTable 1: Structural statistics averages for 4FO-RUMS and CREATEDEBATE.polarity for improvements in stance classification(Thomas et al, 2006; Bansal et al, 2008; Bal-ahur et al, 2009; Burfoot et al, 2011).
How-ever, these methods rely heavily on features spe-cific to the congressional setting in order to pre-dict link polarity, and make little use of textualfeatures.
In contrast, Abbott et al (2011) use arange of linguistic features from the text of postsand their parents to classify agreement or disagree-ment between posts on the online debate website4FORUMS.COM, without the goal of classifyingstance.In this work, we study datasets from two on-line debate websites: 4FORUMS.COM, from theInternet Argument Corpus (Walker et al, 2012b),and CREATEDEBATE.COM (Hasan and Ng, 2013).Table 1 shows statistics about these datasets in-cluding the average number of users per dis-cussion topic and average number of posts au-thored.
The best stance classification accuracy todate for online debate forums ranges from 70.1%on CONVINCEME.NET to 75.4% on CREATEDE-BATE.COM (Walker et al, 2012c; Hasan and Ng,2013).
The web interface for CONVINCEME.NETenforces opposite stances for reply posts, makingthis dataset inapplicable for text-based disagree-ment modeling, and so we do not consider it inour experiments.
In the more typical online debateforum corpora that we study, the presence of a re-ply, or even a textual disagreement between posts,does not necessarily indicate opposite stance (e.g.in gun control debates on 4Forums, 23% of dis-agreements correspond with same stance).For our unified framework, we specify a hinge-loss Markov random field to reason jointly aboutstance and reply-link polarity labels.
A closelyrelated line of work focuses on improving struc-118tured prediction with domain knowledge modeledas constraints in the objective function (Chang etal., 2012; Ganchev et al, 2010; Mann and Mc-Callum, 2010).
Though more often used in semi-supervised settings, constraint-based learning canbe especially appropriate for supervised learningwhen commonly used feature functions for linearmodels do not capture the richness of the data.Our HL-MRF formulation admits highly expres-sive features while maintaining a convex objec-tive, thereby enjoying both tractability and a fullyprobabilistic interpretation.3 Modeling ChoicesWe face multiple modeling decisions that mayimpact predictive performance when classifyingstance in online debates.
A key contribution ofthis work is the exploration of the ramifications ofthese choices.
We consider the following varia-tions on modeling: collective (C) versus local (L)classifiers, whether to explicitly model disagree-ment (D), and author-level (A) versus post-level(P) models.Collective versus Local.
Both collective andnon-collective methods for stance prediction re-quire a strong local text classifier.
The methodsproposed in this paper build upon the state-of-the-art local classification approach of Walker et al(2012a), which trains a supervised classifier us-ing features including n-grams, lexical categorycounts, and text lengths.
We use logistic regres-sion for the local classifier.
These models will bereferred to as local (L).
In collective (C) classifi-cation approaches for stance prediction, the stancelabels are all predicted jointly, leveraging relation-ships along the graph of replies.
The simplestway to make use of reply links is to encode thatthe stance of posts (or authors) that reply to eachother is likely to be opposite (Walker et al, 2012c;Hasan and Ng, 2013).
Collective approaches at-tempt to find the most likely joint stance labelingthat is consistent with both the local classifier?spredictions and the alternation of stance along re-sponse threads.
The alternating stance assumptionis not necessarily a hard constraint, and may po-tentially be overridden by the local predictions.
Cand L models can be constructed with A or P-levelgranularity as described below, resulting in fourmodeling combinations.Modeling Disagreement.
As seen in Fig.
1 andTable 1, the assumption that reply links corre-spond to opposite stance is not always correct.This suggests the potential benefit of more nu-anced models of agreement and disagreement.
Anatural disagreement modeling approach is to pre-dict the polarity of reply links jointly with stance.There are two variants of reply link polarity toconsider.
In textual disagreement, replying postsare coded as expressing agreement or disagree-ment with the text of the parent post.
This maynot correspond to a disagreement in stance rela-tive to the thread topic.
Some forum interfacessupport user self-labeling of post reply links as re-buttals or agreements, thereby explicitly provid-ing textual disagreement labels for posts.
Alter-natively, in the stance disagreement variant, replylinks denote either same or opposite stance be-tween users (posts).
In Fig.
1, User 1 and User2 disagree in text but have the same stance.
Forcollective modeling of stance and disagreement, itis useful to consider the stance disagreement vari-ant which identifies opposite and same-stance re-ply links, and jointly encourage stance predictionsto be consistent with the disagreement predictions.As with the local classification of stance, we canconstruct local classifiers for stance disagreement.In this work, for each reply link instance, we use acopy of the local stance classification features foreach author/post at the ends of the reply link.
Thelinguistic features further include discourse mark-ers such as ?actually?
and ?because?
from the dis-agreement classifier of Abbott et al (2011).
Addi-tionally, we use textual disagreement as a featurefor stance disagreementwhen available.
When re-ply links are not explicitly labeled as rebuttals oragreements, or only rebuttals are known, we in-stead predict textual disagreement using the fea-tures given above, trained on a separate data setwith textual-disagreement labels.Finally, with a stance disagreement classifier inhand, we can build collective models that predictstance based on predicted stance disagreement po-larity.
We denote these models as disagreement(D).
When applied at one of A or P-level model-ing, this yields two more possible modeling con-figurations.
These models are certainly more com-plex than others we consider, but their design isconsistent with intuition about the nature of dis-course, so the added complexity may yield betteraccuracy.119All models: Collective models only: Disagreement models only:localPro(X1) ?
pro(X1) disagree(X1, X2) ?
pro(X1) ?
?
pro(X2) localDisagree(X1, X2) ?
disagree(X1, X2)?
localPro(X1) ?
?
pro(X1) disagree(X1, X2) ?
?
pro(X1) ?
pro(X2) ?
localDisagree(X1, X2) ?
?
disagree(X1, X2)?
disagree(X1, X2) ?
pro(X1) ?
pro(X2) pro(X1) ?
?
pro(X2) ?
disagree(X1, X2)?
disagree(X1, X2) ?
?
pro(X1) ?
?
pro(X2) pro(X1) ?
pro(X2) ?
?
disagree(X1, X2)disagree(X1, X2) = 1 ?
pro(X1) ?
?
pro(X2) ?
?
disagree(X1, X2)Figure 2: PSL rules to define the collective classification models, both for post-level and author-levelmodels.
Each X is an author or a post, depending on the level of granularity that the model is appliedat.
The disagree(X1, X2) predicates apply to post reply links, and to pairs of authors connected by replylinks.Author-Level versus Post-Level.
When model-ing debates, stance classifiers can predict eitherthe stance of a debate participant (i.e.
an author(A)) (Burfoot et al, 2011), or the stance expressedby a specific dialogue act (i.e.
a post (P)) (Hasanand Ng, 2013).
The choice of prediction targetmay depend on the downstream goal, such as usermodeling or the study of the dialogic expressionof disagreement.
From a philosophical perspec-tive, authors are individuals who hold opinions,while posts are not.
A post is simply a piece oftext which may or may not express the opinions ofits author.Nevertheless, given a prediction target, eitherauthor or post, it may be beneficial to considermodeling at a different level of granularity.
Forexample, Hasan and Ng (2013) find that post-levelprediction accuracy can be improved by ?clamp-ing?
all posts by a given author to the samestance in order to smooth their labels.
Alterna-tively, author-level predictions may potentially beimproved by first treating each post separately,thereby effectively giving a classifier more train-ing examples, i.e.
the number of posts instead ofthe number of authors.
With this procedure, a fi-nal author-level prediction can be obtained by av-eraging the predictions over the posts for the au-thor, trading the noisiness of post-level instancesagainst the smoothing afforded by the final ag-gregation.
When designing a stance classifier,the modeler must decide the level of granularityfor the prediction target and find the best modeltherein.4 A Collective Classification FrameworkTo study these choices, we build a flexiblestance classification framework that implementsthe above variations using probabilistic soft logic(PSL) (Bach et al, 2015; Bach et al, 2013), a re-cently introduced probabilistic programming sys-tem.
Like other probabilistic modeling frame-works, notably Markov logic (Richardson andDomingos, 2006), PSL uses a logic-like languagefor defining the potential functions for a condi-tional random field.
However, unlike Markovlogic, PSL makes inference tractable, even in theloopy author-level networks and the very largepost-level networks of online debates.PSL?s tractability arises from the use of a specialclass of conditional random field models referredto as hinge-loss MRFs (HL-MRFs), which admitefficient, scalable and exact maximum a posteriori(MAP) inference (Bach et al, 2013).
These mod-els are defined over continuous random variables,and MAP inference is a convex optimization prob-lem over these variables.
Formally, a hinge-lossMRF defines a probability density function of theformP (Y|X) =1Zexp(?M?r=1?r?r(Y,X)), (1)where the entries of Y and X are in [0, 1], ?
is avector of weight parameters, Z is a normalizationconstant, and?r(Y,X) = (max{lr(Y,X), 0})?r(2)is a hinge-loss potential specified by a linear func-tion lrand optional exponent ?r?
{1, 2}.
Givena collection of first-order PSL rules, each instan-tiation of the rules maps to a hinge-loss poten-tial function as in Equation 2, and the potentialfunctions define an HL-MRF model.
For exam-ple, a ?
b , max(a ?
b, 0), where a and b areground variables, and max(a ?
b, 0) is a convexrelaxation of logical implication, and which canbe understood as its distance to satisfaction.
For afull description of PSL, see (Bach et al, 2015).The models we introduce are specified by thePSL rules in Fig.
2, with both post-level andauthor-level models following the same design.We denote the different modeling choices with the120letters defined in Section 3.
First, local logisticregression classifiers output stance probabilitiesbased on textual features of posts or authors.
Allof the models begin with these real-valued stancepredictions, encoded by the observed predicate lo-calPro(Xi).
The rules listed for all models en-courage the inferred global predictions pro(Xi) tomatch these local predictions.This defines the local classification models L,which are HL-MRFs with node potentials and noedge potentials, and which are equivalent to thelocal classifiers.
The collective models extend theL models by adding edge potentials which en-courage the stance labels to respect disagreementrelationships along reply links.
Specifically, ev-ery reply link between authors (for author-levelmodels) or between posts (for post-level mod-els) x1and x2is associated with a latent vari-able disagree(x1, x2).
The rules encourage theglobal stance variables to respect the polarity ofthe disagreement variables (same stance, or op-posite stance) and while also trying to match thestance classifiers.
For the models that do not ex-plicitly model disagreement, it is assumed that ev-ery reply edge constitutes a disagreement, i.e.
dis-agree(x1, x2) = 1.
These models are denoted C.Otherwise, the disagreement variables are en-couraged to match binary-valued predictions fromthe local disagreement classifiers.
We binarizethe predictions of the disagreement classifiers toencourage propagation.
The disagreement vari-ables are modeled jointly with the stance variables,and label information propagates in both direc-tions between stance and disagreement variables.The full joint stance/disagreement collective mod-els are denoted D. In the following, the models aredenoted by pairs of letters according to their col-lectivity level and modeling granularity.
For ex-ample, AC denotes collective classification per-formed at the author level, without joint model-ing of disagreement.
To train these models anduse them for prediction, weight learning and MAPinference are performed using the structured per-ceptron algorithm and ADMM algorithm of Bachet al (2013).5 Experimental EvaluationThe goals of our experiments were to validate theproposed collective modeling framework, and tomake substantive conclusions about the merits ofthe different possible modeling options describedin Section 3.
To this end, we evaluated the mod-els on eight topics from 4FORUMS.COM (Walkeret al, 2012b) and CREATEDEBATE.COM (Hasanand Ng, 2013), for classification tasks at both theauthor level and the post level.
With comparisonto Hasan and Ng (2013), our collective models (C)are essentially equivalent to their CRF, up to theform of the CRF potential function, which is notexplicitly specified in the paper.
A further goalof our experiments was to determine whether themodeling options in our more general CRF couldimprove performance over models with this struc-ture.On average, each topic-wise data set containshundreds of authors and thousands of posts.
The4FORUMS data sets are annotated for stance at theauthor level, while CREATEDEBATE has stance la-bels at the post level.
To perform post-level evalu-ations on 4FORUMS we apply author labels to theposts of each author, and on CREATEDEBATE wecomputed author labels by selecting the majoritylabel of their posts.
For 4FORUMS, since post-level stance labels correspond directly to author-level stance labels, we use averages of post-levelpredictions as the local classifier output for au-thors.
Section 2 includes an overview of these de-bate forum data sets.In the experiments, classification accuracywas estimated via five repeats of 5-fold cross-validation.
In each fold, we ran logistic regres-sion using the scikit-learn software package,2us-ing the default settings, except for the L1 regu-larization trade-off parameter C which was tunedon a within-fold hold-out set consisting of 20%of the discussions within the fold.
For the collec-tive models, weight learning was performed on thesame in-fold tuning sets.
We trained via 700 itera-tions of structured perceptron, and ran the ADMMMAP inference algorithm to convergence at testtime.
On average, weight learning and inferencetook around 1 minute per fold.The full results for author-level and post-levelpredictions are given in Table 2 and Table 3, re-spectively.
In the tables, entries in bold identifystatistically significant differences from the localclassifier baseline under a paired t-test with sig-nificance level ?
= 0.05.
These results are sum-marized in Fig.
3, which shows box plots for thesix possible models, computed over the final cross-validated accuracy scores of each of the four data2Available at http://scikit-learn.org/.121PL PC PD AL AC AD50556065707580859095AccuracyAuthor Stance: CreateDebate.comPL PC PD AL AC AD50556065707580859095AccuracyPost Stance: CreateDebate.comPL PC PD AL AC AD50556065707580859095AccuracyAuthor Stance: 4Forums.comPL PC PD AL AC AD50556065707580859095AccuracyPost Stance: 4Forums.comFigure 3: Overall accuracies per model for the author stance prediction task, computed over the finalresults for each of the four data sets per forum.
Note that we expect significant variation in these plots,as the data sets are of varying degrees of difficulty.sets from each forum.
The overall trends can beseen by reading the box plots in each figure fromleft to right.
In general, collective models out-perform local models, and modeling disagreementfurther improves accuracy.
Author-level model-ing is typically better than post-level, even forthe post-level prediction task.
The improvementsshown by collective models and author-level mod-els are consistent with Hasan and Ng (2013)?s con-clusion about the benefits of user-level constraints.This may suggest that posts only provide relativelynoisy observations of the underlying author-levelstance.
Modeling at the author level results inmore stable predictions, as noisy posts are pooledtogether.
But here we also show that the full jointdisagreement model at the author level, AD, per-forms the best overall, for both prediction tasksand for both forums, gaining up to 11.5 percentagepoints of post-level accuracy over the local post-level classifier.A closer analysis reveals some subtleties.
Whencomparing D models with C models in Fig.
3, dis-agreement modeling makes a much bigger differ-ence at the author level than at the post level.
Thisis likely impacted by the level of class imbalancefor disagreement classification in the different lev-els of modeling.
Disagreement, rather than agree-ment, between authors prompts many responses.Thus, reply links are more likely disagreementswhen measured at the post level, as seen in Ta-1224FORUMS CREATEDEBATEModels Abortion Evolution Gay Gun Abortion Gay Marijuana ObamaMarriage Control RightsPL 61.9 ?
4.3 76.6 ?
3.9 72.0 ?
3.6 66.4 ?
4.6 66.4 ?
5.2 70.2 ?
5.0 74.1 ?
6.5 63.8 ?
8.7PC 63.4 ?
5.9 74.6 ?
4.1 73.7 ?
4.3 68.3 ?
5.5 68.7 ?
5.7 72.6 ?
5.6 75.4 ?
7.4 66.1 ?
8.5PD 63.0 ?
5.4 76.7 ?
4.2 73.7 ?
4.6 67.9 ?
5.0 69.5 ?
5.7 73.2 ?
5.9 74.7 ?
7.0 66.1 ?
8.5AL 64.9 ?
4.2 77.3 ?
2.9 74.5 ?
2.9 67.1 ?
4.5 65.2 ?
6.5 69.5 ?
4.4 74.0 ?
6.6 59.0 ?
7.5AC 66.0 ?
5.0 74.4 ?
4.2 75.7 ?
5.1 61.5 ?
5.6 65.8 ?
7.0 73.6 ?
3.5 73.9 ?
7.6 62.5 ?
8.3AD 65.8 ?
4.4 78.7 ?
3.3 77.1 ?
4.4 67.1 ?
5.4 67.4 ?
7.5 74.0 ?
5.3 74.8 ?
7.5 63.0 ?
8.3Table 2: Author stance classification accuracy and standard deviation for 4FORUMS (left) and CREAT-EDEBATE (right), estimated via 5 repeats of 5-fold cross-validation.
Bolded figures indicate statisticallysignificant (?
= 0.05) improvement over AL, the baseline model for the author stance classification task.4FORUMS CREATEDEBATEModels Abortion Evolution Gay Gun Abortion Gay Marijuana ObamaMarriage Control RightsPL 66.1 ?
2.5 72.4 ?
4.2 69.0 ?
2.7 67.8 ?
3.5 60.2 ?
3.2 62.7 ?
4.4 68.1 ?
6.1 59.4 ?
6.0PC 70.5 ?
2.5 74.1 ?
3.8 73.2 ?
3.1 69.1 ?
3.0 62.8 ?
3.8 66.1 ?
4.9 68.7 ?
7.9 61.1 ?
6.6PD 69.7 ?
2.5 73.9 ?
4.0 72.5 ?
3.0 68.8 ?
3.0 62.6 ?
4.1 66.2 ?
5.4 69.1 ?
7.4 61.0 ?
6.6AL 74.7 ?
7.1 73.0 ?
5.7 70.3 ?
6.0 68.7 ?
5.3 61.6 ?
9.8 63.7 ?
5.3 66.7 ?
6.7 59.7 ?
13.6AC 76.8 ?
8.1 68.3 ?
5.3 72.7 ?
11.1 46.9 ?
8.0 63.4 ?
12.4 71.2 ?
8.4 66.9 ?
9.0 63.7 ?
15.6AD 77.0 ?
8.9 80.3 ?
5.5 80.5 ?
8.5 65.4 ?
8.3 66.8 ?
12.2 72.7 ?
8.9 69.0 ?
8.3 63.5 ?
16.3Table 3: Post stance classification accuracy and standard deviations for 4FORUMS (left) and CREAT-EDEBATE (right), estimated via 5 repeats of 5-fold cross-validation.
Bolded figures indicate statisticallysignificant (?
= 0.05) improvement over PL, the baseline model for the post stance classification task.ble 1.
Therefore, enforcing disagreement may bea better assumption at the post level, and the nu-anced disagreement model is not necessary in thiscase.
The overall improvements in accuracy fromdisagreement modeling for post-level models weresmall.On the other hand, the assumption that re-ply edges constitute disagreement is less accuratewhen modeling at the author level (see Table 1).In this case, the full joint disagreement model isnecessary to obtain good performance.
In an ex-treme example, the two datasets with the lowestdisagreement rates at the author level are evolution(44.4%) and gun control (50.7%) from 4FORUMS.The AC classifier performed very poorly for thesedata sets, dropping to 46.9% accuracy in one in-stance, as the ?opposite stance?
assumption didnot hold (Tables 2 and 3).
The full joint disagree-ment model AD performed much better, in factachieving an outstanding accuracy rates of 80.3%and 80.5% for posts on evolution and gay marriagerespectively.
To illustrate the benefits of author-level disagreement modeling, Fig.
4 shows a postfor an author whose stance towards gun control iscorrectly predicted by AD but not the AC model,Text StancePost: I agree with everything except the last part.
Safe gunstorage is very important, and sensible storage requirementshave two important factors.ANTIReply: I can agree with this.
And in case it seemed otherwise,I know full well how to store guns safely, and why it?s nec-essary.
My point was that I don?t like the idea of such a law,especially when you consider the problem of enforcement.ANTIFigure 4: A post-reply pair by 4FORUMS.COM au-thors whose gun control stance is correctly pre-dicted by AD, but not by AC.along with a subsequent reply.
The authors largelyagree with each other?s views, which the joint dis-agreement model leverages, while the simpler col-lective model encourages opposite stance due tothe presence of reply links between them.To summarize our conclusions from these ex-periments, the results suggest that author-levelmodeling is the preferred strategy, regardless ofthe prediction task.
In this scenario, it is essen-tial to explicitly model disagreement in the collec-tive classifier.
Our top performing AD model sta-tistically significantly outperforms the respectiveprediction task baseline on 6 out of 8 topics forboth tasks with p-values less than 0.001.
Based onour experimental results, we recommend the full123author-disagreement model AD as the classifier ofchoice.6 Discussion and Future WorkThe prediction of user stance in online debate fo-rums is a valuable task, and modeling debate di-alogue is complex and requires many decisionssuch collective or non-collective reasoning, nu-anced or naive use of disagreement information,and post versus author-level modeling granularity.We systematically explore each choice, and in do-ing so build a unified joint framework that incor-porates each salient decision.
Our method uses ahinge-loss Markov random field to encourage con-sistency between local classifier predictions forstance and disagreement information.
We find thatmodeling at the author level gives better predic-tive performance regardless of the granularity ofthe prediction task, and that nuanced disagreementmodeling is of particular importance for author-level collective modeling.
The resulting collectiveclassifier gives improved predictive performanceover both the simple non-collective and standardcollective approaches, with a running time over-head of only a few minutes, thanks to the efficientnature of hinge-loss MRFs.There are many directions for future work.
Ourresults have found that collective reasoning canalso be beneficial at the post level, as previouslyreported by Hasan and Ng (2013).
It is likely thata multi-level model for a combination of post- andauthor-level collective modeling of both stanceand disagreement could bring further improve-ments in performance.
It would also be informa-tive to explore dynamic models which elucidatetrends of opinions over time.
Another direction isto model influence between users in online debateforums, and to identify the most influential userswho are able to convince other users to changetheir opinions.
Finally, we note that stance anddisagreement classification are both challengingand important problems, and going forward, thereis likely to be much room for improvement in theseprediction tasks.AcknowledgmentsThis work was supported by NSF grantIIS1218488, and IARPA via DoI/NBC contractnumber D12PC00337.
The U.S. Government isauthorized to reproduce and distribute reprintsfor governmental purposes notwithstanding anycopyright annotation thereon.
Disclaimer: Theviews and conclusions contained herein are thoseof the authors and should not be interpreted asnecessarily representing the official policies orendorsements, either expressed or implied, ofIARPA, DoI/NBC, or the U.S. Government.ReferencesRob Abbott, Marilyn Walker, Jean E. Fox Tree, PranavAnand, Robeson Bowmani, and Joseph King.
2011.How can you say such things?!?
: Recognizing dis-agreement in informal political argument.
In ACLWorkshop on Language and Social Media.Amjad Abu-Jbara and Dragomir R Radev.
2013.
Iden-tifying opinion subgroups in Arabic online discus-sions.
In ACL.Stephen H. Bach, Bert Huang, Ben London, and LiseGetoor.
2013.
Hinge-loss Markov random fields:Convex inference for structured prediction.
In Un-certainty in Artificial Intelligence (UAI).S.
H. Bach, M. Broecheler, B. Huang, and L. Getoor.2015.
Hinge-loss Markov random fields and proba-bilistic soft logic.
arXiv:1505.04406 [cs.LG].Alexandra Balahur, Zornitsa Kozareva, and AndresMontoyo.
2009.
Determining the polarity andsource of opinions expressed in political debates.Computational Linguistics and Intelligent Text Pro-cessing.Mohit Bansal, Claire Cardie, and Lillian Lee.
2008.The power of negative thinking: Exploiting labeldisagreement in the min-cut classification frame-work.
COLING.Filip Boltu?zi?c and Jan?Snajder.
2014.
Back up yourstance: recognizing arguments in online discussions.In ACL Workshop on Argumentation Mining.Clinton Burfoot, Steven Bird, and Timothy Baldwin.2011.
Collective classification of congressionalfloor-debate transcripts.
In ACL.Ming-Wei Chang, Lev Ratinov, and Dan Roth.
2012.Structured learning with constrained conditionalmodels.
Machine learning, 88(3):399?431.Kuzman Ganchev, Joao Grac?a, Jennifer Gillenwater,and Ben Taskar.
2010.
Posterior regularization forstructured latent variable models.
Machine Learn-ing, 11:2001?2049.Debanjan Ghosh, Smaranda Muresan, Nina Wacholder,Mark Aakhus, and Matthew Mitsui.
2014.
Analyz-ing argumentative discourse units in online interac-tions.
In ACL Workshop on Argumentation Mining.Kazi Saidul Hasan and Vincent Ng.
2013.
Stance clas-sification of ideological debates: Data, models, fea-tures, and constraints.
International Joint Confer-ence on Natural Language Processing.124Kazi Saidul Hasan and Vincent Ng.
2014.
Why areyou taking this stance?
Identifying and classifyingreasons in ideological debates.
In EMNLP.Y.
Lu, H. Wang, C. Zhai, and D. Roth.
2012.
Unsuper-vised discovery of opposing opinion networks fromforum discussions.
In CIKM.Gideon S Mann and Andrew McCallum.
2010.
Gener-alized expectation criteria for semi-supervised learn-ing with weakly labeled data.
Machine Learning,11:955?984.Akiko Murakami and Rudy Raymond.
2010.
Supportor Oppose?
Classifying positions in online debatesfrom reply activities and opinion expressions.
InACL.Matthew Richardson and Pedro Domingos.
2006.Markov logic networks.
Machine learning, 62(1-2).Swapna Somasundaran and Janyce Wiebe.
2009.
Rec-ognizing stances in online debates.
In ACL andAFNLP.Swapna Somasundaran and Janyce Wiebe.
2010.
Rec-ognizing stances in ideological on-line debates.
InNAACL HLT 2010 Workshop on Computational Ap-proaches to Analysis and Generation of Emotion inText.Dhanya Sridhar, Lise Getoor, and Marilyn Walker.2014.
Collective stance classification of posts inonline debate forums.
In ACL Joint Workshop onSocial Dynamics and Personal Attributes in SocialMedia.Matt Thomas, Bo Pang, and Lillian Lee.
2006.
Get outthe vote: Determining support or opposition fromCongressional floor-debate transcripts.
In EMNLP.Marilyn Walker, Pranav Anand, Rob Abbott, Jean E.Fox Tree, Craig Martell, and Joseph King.
2012a.That?s your evidence?
: Classifying stance in onlinepolitical debate.
Decision Support Sciences.Marilyn Walker, Pranav Anand, Robert Abbott, andJean E. Fox Tree.
2012b.
A corpus for researchon deliberation and debate.
In LREC.Marilyn Walker, Pranav Anand, Robert Abbott, andRichard Grant.
2012c.
Stance classification usingdialogic properties of persuasion.
In NAACL.Lu Wang and Claire Cardie.
2014.
A piece of mymind: A sentiment analysis approach for online dis-pute detection.
In ACL.125
