Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 196?206,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsSyntactic and Semantic Factors in Processing Difficulty:An Integrated MeasureJeff Mitchell, Mirella Lapata, Vera Demberg and Frank KellerUniversity of EdinburghEdinburgh, United Kingdomjeff.mitchell@ed.ac.uk, mlap@inf.ed.ac.uk,v.demberg@ed.ac.uk, keller@inf.ed.ac.ukAbstractThe analysis of reading times can pro-vide insights into the processes that under-lie language comprehension, with longerreading times indicating greater cognitiveload.
There is evidence that the languageprocessor is highly predictive, such thatprior context allows upcoming linguisticmaterial to be anticipated.
Previous workhas investigated the contributions of se-mantic and syntactic contexts in isolation,essentially treating them as independentfactors.
In this paper we analyze readingtimes in terms of a single predictive mea-sure which integrates a model of seman-tic composition with an incremental parserand a language model.1 IntroductionPsycholinguists have long realized that languagecomprehension is highly incremental, with readersand listeners continuously extracting the meaningof utterances on a word-by-word basis.
As soonas they encounter a word in a sentence, they inte-grate it as fully as possible into a representationof the sentence thus far (Marslen-Wilson 1973;Konieczny 2000; Tanenhaus et al 1995; Sturt andLombardo 2005).
Recent research suggests thatlanguage comprehension can also be highly pre-dictive, i.e., comprehenders are able to anticipateupcoming linguistic material.
This is beneficial asit gives them more time to keep up with the in-put, and predictions can be used to compensate forproblems with noise or ambiguity.Two types of prediction have been observed inthe literature.
The first type is semantic predic-tion, as evidenced in semantic priming: a wordthat is preceded by a semantically related primeor a semantically congruous sentence fragment isprocessed faster (Stanovich and West 1981; vanBerkum et al 1999; Clifton et al 2007).
Anotherexample is argument prediction: listeners are ableto launch eye-movements to the predicted argu-ment of a verb before having encountered it, e.g.,they will fixate an edible object as soon as theyhear the word eat (Altmann and Kamide 1999).The second type of prediction is syntactic predic-tion.
Comprehenders are faster at naming wordsthat are syntactically compatible with prior con-text, even when they bear no semantic relationshipto the context (Wright and Garrett 1984).
Anotherinstance of syntactic prediction has been reportedby Staub and Clifton (2006): following the wordeither, readers predict or and the complement thatfollows it, and process it faster compared to a con-trol condition without either.Thus, human language processing takes advan-tage of the constraints imposed by the precedingsemantic and syntactic context to derive expecta-tions about the upcoming input.
Much recent workhas focused on developing computational mea-sures of these constraints and expectations.
Again,the literature is split into syntactic and semanticmodels.
Probably the best known measure of syn-tactic expectation is surprisal (Hale 2001) whichcan be coarsely defined as the negative log proba-bility of word wt given the preceding words, typ-ically computed using a probabilistic context-freegrammar.Modeling work on semantic constraint focuseson the degree to which a word is related to itspreceding context.
Pynte et al (2008) use La-tent Semantic Analysis (LSA, Landauer and Du-mais 1997) to assess the degree of contextual con-straint exerted on a word by its context.
In thisframework, word meanings are represented as vec-tors in a high dimensional space and distance inthis space is interpreted as an index of process-ing difficulty.
Other work (McDonald and Brew2004) models contextual constraint in informationtheoretic terms.
The assumption is that wordscarry prior semantic expectations which are up-dated upon seeing the next word.
Expectations arerepresented by a vector of probabilities which re-flects the likely location in semantic space of theupcoming word.The measures discussed above are typicallycomputed automatically on real-language corporausing data-driven methods and their predictionsare verified through analysis of eye-movementsthat people make while reading.
Ample evidence196(Rayner 1998) demonstrates that eye-movementsare related to the moment-to-moment cognitive ac-tivities of readers.
They also provide an accuratetemporal record of the on-line processing of nat-ural language, and through the analysis of eye-movement measurements (e.g., the amount of timespent looking at a word) can give insight into theprocessing difficulty involved in reading.In this paper, we investigate a model of predic-tion that is incremental and takes into account syn-tactic as well as semantic constraint.
The modelessentially integrates the predictions of an incre-mental parser (Roark 2001) together with thoseof a semantic space model (Mitchell and Lap-ata 2009).
The latter creates meaning representa-tions compositionally, and therefore builds seman-tic expectations for word sequences (e.g., phrases,sentences, even documents) rather than isolatedwords.
Some existing models of sentence process-ing integrate semantic information into a prob-abilistic parser (Narayanan and Jurafsky 2002;Pado?
et al 2009); however, the semantic compo-nent of these models is limited to semantic role in-formation, rather than attempting to build a full se-mantic representation for a sentence.
Furthermore,the models of Narayanan and Jurafsky (2002) andPado?
et al (2009) do not explicitly model pre-diction, but rather focus on accounting for gardenpath effects.
The proposed model simultaneouslycaptures semantic and syntactic effects in a sin-gle measure which we empirically show is predic-tive of processing difficulty as manifested in eye-movements.2 Models of Processing DifficultyAs described in Section 1, reading times providean insight into the various cognitive activities thatcontribute to the overall processing difficulty in-volved in comprehending a written text.
To quan-tify and understand the overall cognitive load asso-ciated with processing a word in context, we willbreak that load down into a sum of terms repre-senting distinct computational costs (semantic andsyntactic).
For example, surprisal can be thoughtof as measuring the cost of dealing with unex-pected input.
When a word conforms to the lan-guage processor?s expectations, surprisal is low,and the cognitive load associated with processingthat input will also be low.
In contrast, unexpectedwords will have a high surprisal and a high cogni-tive cost.However, high-level syntactic and semantic fac-tors are only one source of cognitive costs.
A siz-able proportion of the variance in reading times isaccounted for by costs associated with low-levelfeatures of the stimuli, e.g.. relating to orthographyand eye-movement control (Rayner 1998).
In ad-dition, there may also be costs associated with theintegration of new input into an incremental rep-resentation.
Dependency Locality Theory (DLT,Gibson 2000) is essentially a distance-based mea-sure of the amount of processing effort requiredwhen the head of a phrase is integrated with itssyntactic dependents.
We do not consider integra-tion costs here (as they have not been shown tocorrelate reliably with reading times; see Dembergand Keller 2008 for details) and instead focus onthe costs associated with semantic and syntacticconstraint and low-level features, which appear tomake the most substantial contributions.In the following subsections we describe thevarious features which contribute to the process-ing costs of a word in context.
We begin by look-ing at the low-level costs and move on to con-sider the costs associated with syntactic and se-mantic constraint.
For readers unfamiliar with themethodology involved in modeling eye-trackingdata, we note that regression analysis (or the moregeneral mixed effects models) is typically used tostudy the relationship between dependent and in-dependent variables.
The independent variablesare the various costs of processing effort andthe dependent variables are measurements of eye-movements, three of which are routinely used inthe literature: first fixation duration (the durationof the first fixation on a word regardless of whetherit is the first fixation on a word or the first of mul-tiple fixations on the same word), first pass dura-tion, also known as gaze duration, (the sum of allfixations made on a word prior to looking at an-other word), and total reading time (the sum ofall fixations on a word including refixations aftermoving on to other words).2.1 Low-level CostsLow-level features include word frequency (morefrequent words are read faster), word length(shorter words are read faster), and the positionof the word in the sentence (later words are readfaster).
Oculomotor variables have also beenfound to influence reading times.
These includeprevious fixation (indicating whether or not theprevious word has been fixated), launch distance(how many characters intervene between the cur-rent fixation and the previous fixation), and land-ing position (which letter in the word the fixationlanded on).Information about the sequential context of aword can also influence reading times.
Mc-197Donald and Shillcock (2003) show that forwardand backward transitional probabilities are pre-dictive of first fixation and first pass durations:the higher the transitional probability, the shorterthe fixation time.
Backward transitional prob-ability is essentially the conditional probabil-ity of a word given its immediately precedingword, P(wk|wk?1).
Analogously, forward proba-bility is the conditional probability of the currentword given the next word, P(wk|wk+1).2.2 Syntactic ConstraintAs mentioned earlier, surprisal (Hale 2001; Levy2008) is one of the best known models of process-ing difficulty associated with syntactic constraint,and has been previously applied to the modeling ofreading times (Demberg and Keller 2008; FerraraBoston et al 2008; Roark et al 2009; Frank 2009).The basic idea is that the processing costs relatingto the expectations of the language processor canbe expressed in terms of the probabilities assignedby some form of language model to the input.These processing costs are assumed to arise fromthe change in the expectations of the language pro-cessor as new input arrives.
If we express these ex-pectations in terms of a distribution over all possi-ble continuations of the input seen so far, then wecan measure the magnitude of this change in termsof the Kullback-Leibler divergence of the old dis-tribution to the updated distribution.
This measureof processing cost for an input word, wk+1, giventhe previous context, w1 .
.
.wk, can be expressedstraightforwardly in terms of its conditional prob-ability as:S =?
logP(wk+1|w1 .
.
.wk) (1)That is, the processing cost for a word decreases asits probability increases, with zero processing costincurred for words which must appear in a givencontext, as these do not result in any change in theexpectations of the language processor.The original formulation of surprisal (Hale2001) used a probabilistic parser to calculate theseprobabilities, as the emphasis was on the process-ing costs incurred when parsing structurally am-biguous garden path sentences.1 Several variantsof calculating surprisal have been developed in theliterature since using different parsing strategies1While hearing a sentence like The horse raced past thebarn fell (Bever 1970), English speakers are inclined to in-terpreted horse as the subject of raced expecting the sentenceto end at the word barn.
So upon hearing the word fell theyare forced to revise their analysis of the sentence thus far andadopt a reduced relative reading.
(e.g., left-to-right vs. top-down, PCFGs vs de-pendency parsing) and different degrees of lexi-calization (see Roark et al 2009 for an overview) .For instance, unlexicalized surprisal can be easilyderived by substituting the words in Equation (1)with parts of speech (Demberg and Keller 2008).Surprisal could be also defined using a vanillalanguage model that does not take any structuralor grammatical information into account (Frank2009).2.3 Semantic ConstraintDistributional models of meaning have been com-monly used to quantify the semantic relation be-tween a word and its context in computationalstudies of lexical processing.
These models arebased on the idea that words with similar mean-ings will be found in similar contexts.
In puttingthis idea into practice, the meaning of a word isthen represented as a vector in a high dimensionalspace, with the vector components relating to thestrength on occurrence of that word in varioustypes of context.
Semantic similarities are thenmodeled in terms of geometric similarities withinthe space.To give a concrete example, Latent SemanticAnalysis (LSA, Landauer and Dumais 1997) cre-ates a meaning representation for words by con-structing a word-document co-occurrence matrixfrom a large collection of documents.
Each row inthe matrix represents a word, each column a doc-ument, and each entry the frequency with whichthe word appeared within that document.
Becausethis matrix tends to be quite large it is often trans-formed via a singular value decomposition (Berryet al 1995) into three component matrices: a ma-trix of word vectors, a matrix of document vectors,and a diagonal matrix containing singular values.Re-multiplying these matrices together using onlythe initial portions of each (corresponding to theuse of a lower dimensional spatial representation)produces a tractable approximation to the originalmatrix.
In this framework, the similarity betweentwo words can be easily quantified, e.g., by mea-suring the cosine of the angle of the vectors repre-senting them.As LSA is one the best known semantic spacemodels it comes as no surprise that it has beenused to analyze semantic constraint.
Pynte et al(2008) measure the similarity between the nextword and its preceding context under the assump-tion that high similarity indicates high semanticconstraint (i.e., the word was expected) and analo-gously low similarity indicates low semantic con-straint (i.e., the word was unexpected).
They oper-198ationalize preceding contexts in two ways, eitheras the word immediately preceding the next wordas the sentence fragment preceding it.
Sentencefragments are represented as the average of thewords they contain independently of their order.The model takes into account only content words,function words are of little interest here as they canbe found in any context.Pynte et al (2008) analyze reading times on theFrench part of the Dundee corpus (Kennedy andPynte 2005) and find that word-level LSA similar-ities are predictive of first fixation and first passdurations, whereas sentence-level LSA is onlypredictive of first pass duration (i.e., for a mea-sure that includes refixation).
This latter findingis somewhat counterintuitive, one would expectlonger contexts to have an immediate effect asthey are presumably more constraining.
One rea-son why sentence-level influences are only visibleon first pass duration may be due to LSA itself,which is syntax-blind.
Another reason relates tothe way sentential context was modeled as vec-tor addition (or averaging).
The idea of averag-ing is not very attractive from a linguistic perspec-tive as it blends the meanings of individual wordstogether.
Ideally, the combination of simple el-ements onto more complex ones must allow theconstruction of novel meanings which go beyondthose of the individual elements (Pinker 1994).The only other model of semantic constraint weare aware of is Incremental Contextual Distinc-tiveness (ICD, McDonald 2000; McDonald andBrew 2004).
ICD assumes that words carry priorsemantic expectations which are updated uponseeing the next word.
Context is represented bya vector of probabilities which reflects the likelylocation in semantic space of the upcoming word.When the latter is observed, the prior expecta-tion is updated using a Bayesian inference mecha-nism to reflect the newly arrived information.
LikeLSA, ICD is based on word co-occurrence vectors,however it does not employ singular value decom-position, and constructs a word-word rather than aword-document co-occurrence matrix.
Althoughthis model has been shown to successfully simu-late single- and multiple-word priming (McDon-ald and Brew 2004), it failed to predict processingcosts in the Embra eye-tracking corpus (McDon-ald and Shillcock 2003).In this work we model semantic constraint us-ing the representational framework put forward inMitchell and Lapata (2008).
Their aim is not somuch to model processing difficulty, but to con-struct vector-based meaning representations thatgo beyond individual words.
They introduce ageneral framework for studying vector composi-tion, which they formulate as a function f of twovectors u and v:h = f (u,v) (2)where h denotes the composition of u and v. Dif-ferent composition models arise, depending onhow f is chosen.
Assuming that h is a linear func-tion of the Cartesian product of u and v allows tospecify additive models which are by far the mostcommon method of vector combination in the lit-erature:hi = ui + vi (3)Alternatively, we can assume that h is a linearfunction of the tensor product of u and v, and thusderive models based on multiplication:hi = ui ?
vi (4)Mitchell and Lapata (2008) show that several ad-ditive and multiplicative models can be formu-lated under this framework, including the well-known tensor products (Smolensky 1990) and cir-cular convolution (Plate 1995).
Importantly, com-position models are not defined with a specific se-mantic space in mind, they could easily be adaptedto LSA, or simple co-occurrence vectors, or moresophisticated semantic representations (e.g., Grif-fiths et al 2007), although admittedly some com-position functions may be better suited for partic-ular semantic spaces.Composition models can be straightforwardlyused as predictors of processing difficulty, againvia measuring the cosine of the angle between avector w representing the upcoming word and avector h representing the words preceding it:sim(w,h) =w ?h|w||h|(5)where h is created compositionally, via some (ad-ditive or multiplicative) function f .In this paper we evaluate additive and compo-sitional models in their ability to capture seman-tic prediction.
We also examine the influence ofthe underlying meaning representations by com-paring a simple semantic space similar to Mc-Donald (2000) against Latent Dirichlet Allocation(Blei et al 2003; Griffiths et al 2007).
Specif-ically, the simpler space is based on word co-occurrence counts; it constructs the vector repre-senting a given target word, t, by identifying all thetokens of t in a corpus and recording the counts ofcontext words, ci (within a specific window).
Thecontext words, ci, are limited to a set of the n most199common content words and each vector compo-nent is given by the ratio of the probability of a cigiven t to the overall probability of ci.vi =p(ci|t)p(ci)(6)Despite its simplicity, the above semantic space(and variants thereof) has been used to success-fully simulate lexical priming (e.g., McDonald2000), human judgments of semantic similarity(Bullinaria and Levy 2007), and synonymy tests(Pado?
and Lapata 2007) such as those included inthe Test of English as Foreign Language (TOEFL).LDA is a probabilistic topic model offering analternative to spatial semantic representations.
Itis similar in spirit to LSA, it also operates on aword-document co-occurrence matrix and derivesa reduced dimensionality description of words anddocuments.
Whereas in LSA words are repre-sented as points in a multi-dimensional space,LDA represents words using topics.
Specifically,each document in a corpus is modeled as a distri-bution over K topics, which are themselves char-acterized as distribution over words.
The individ-ual words in a document are generated by repeat-edly sampling a topic according to the topic distri-bution and then sampling a single word from thechosen topic.
Under this framework, word mean-ing is represented as a probability distribution overa set of latent topics, essentially a vector whosedimensions correspond to topics and values to theprobability of the word given these topics.
Topicmodels have been recently gaining ground as amore structured representation of word meaning(Griffiths et al 2007; Steyvers and Griffiths 2007).In contrast to more standard semantic space mod-els where word senses are conflated into a singlerepresentation, topics have an intuitive correspon-dence to coarse-grained sense distinctions.3 Integrating Semantic Constraint intoSurprisalThe treatment of semantic and syntactic constraintin models of processing difficulty has been some-what inconsistent.
While surprisal is a theo-retically well-motivated measure, formalizing theidea of linguistic processing being highly predic-tive in terms of probabilistic language models, themeasurement of semantic constraint in terms ofvector similarities lacks a clear motivation.
More-over, the two approaches, surprisal and similarity,produce mathematically different types of mea-sures.
Formally, it would be preferable to havea single approach to capturing constraint and theobvious solution is to derive some form of seman-tic surprisal rather than sticking with similarity.This can be achieved by turning a vector modelof semantic similarity into a probabilistic languagemodel.There are in fact a number of approaches to de-riving language models from distributional mod-els of semantics (e.g., Bellegarda 2000; Coccaroand Jurafsky 1998; Gildea and Hofmann 1999).We focus here on the model of Mitchell and La-pata (2009) which tackles the issue of the compo-sition of semantic vectors and also integrates theoutput of an incremental parser.
The core of theirmodel is based on the product of a trigram modelp(wn|wn?1n?2) and a semantic component ?
(wn,h)which determines the factor by which this proba-bility should be scaled up or down given the priorsemantic context h:p(wn) = p(wn|wn?1n?2) ??
(wn,h) (7)The factor ?
(wn,h) is essentially based on a com-parison between the vector representing the cur-rent word wn and the vector representing the priorhistory h. Varying the method for constructingword vectors (e.g., using LDA or a simpler seman-tic space model) and for combining them into arepresentation of the prior context h (e.g., usingadditive or multiplicative functions) produces dis-tinct models of semantic composition.The calculation of ?
is then based on a weighteddot product of the vector representing the upcom-ing word w, with the vector representing the priorcontext h:?
(w,h) =?iwihi p(ci) (8)As shown in Equation (7) this semantic factor thenmodulates the trigram probabilities, to take ac-count of the effect of the semantic content outsidethe n-gram window.Mitchell and Lapata (2009) show that a com-bined semantic-trigram language model derivedfrom this approach and trained on the Wall StreetJournal outperforms a baseline trigram model interms of perplexity on a held out set.
They alsolinearly interpolate this semantic language modelwith the output of an incremental parser, whichcomputes the following probability:p(w|h) = ?p1(w|h)+(1??
)p2(w|h) (9)where p1(w|h) is computed as in Equation (7)and p2(w|h) is computed by the parser.
Their im-plementation uses Roark?s (2001) top-down incre-mental parser which estimates the probability of200the next word based upon the previous words ofthe sentence.
These prefix probabilities are calcu-lated from a grammar, by considering the likeli-hood of seeing the next word given the possiblegrammatical relations representing the prior con-text.Equation (9) essentially defines a languagemodel which combines semantic, syntactic andn-gram structure, and Mitchell and Lapata (2009)demonstrate that it improves further upon a se-mantic language model in terms of perplexity.
Weargue that the probabilities from this model giveus a means to model the incrementally and predic-tivity of the language processor in a manner thatintegrates both syntactic and semantic constraints.Converting these probabilities to surprisal shouldresult in a single measure of the processing cost as-sociated with semantic and syntactic expectations.4 MethodData The models discussed in the previous sec-tion were evaluated against an eye-tracking cor-pus.
Specifically, we used the English portionof the Dundee Corpus (Kennedy and Pynte 2005)which contains 20 texts taken from The Indepen-dent newspaper.
The corpus consists of 51,502tokens and 9,776 types in total.
It is annotatedwith the eye-movement records of 10 English na-tive speakers, who each read the whole corpus.The eye-tracking data was preprocessed followingthe methodology described in Demberg and Keller(2008).
From this data, we computed total readingtime for each word in the corpus.
Our statisticalanalyses were based on actual reading times, andso we only included words that were not skipped.We also excluded words for which the previousword had been skipped, and words on which thenormal left-to-right movement of gaze had beeninterrupted, i.e., by blinks, regressions, etc.
Fi-nally, because our focus is the influence of seman-tic context, we selected only content words whoseprior sentential context contained at least two fur-ther content words.
The resulting data set con-sisted of 53,704 data points, which is about 10%of the theoretically possible total.22The total of all words read by all subjects is 515,020.The pre-processing recommended by Demberg and Keller?s(2008) results in a data sets containing 436,000 data points.Removing non-content words leaves 205,922 data points.
Itonly makes sense to consider words that were actually fixated(the eye-tracking measures used are not defined on skippedwords), which leaves 162,129 data points.
Following Pynteet al (2008), we require that the previous word was fixated,with 70,051 data points remaining.
We exclude words onwhich the normal left to right movement of gaze had beeninterrupted, e.g., by blinks and regressions, which results inthe final total to 53,704 data points.Model Implementation All elements of ourmodel were trained on the BLLIP corpus, a col-lection of texts from the Wall Street Journal(years 1987?89).
The training corpus consisted of38,521,346 words.
We used a development cor-pus of 50,006 words and a test corpus of similarsize.
All words were converted to lowercase andnumbers were replaced with the symbol ?num?.
Avocabulary of 20,000 words was chosen and theremaining tokens were replaced with ?unk?.Following Mitchell and Lapata (2009), we con-structed a simple semantic space based on co-occurrence statistics from the BLLIP training set.We used the 2,000 most frequent word types ascontexts and a symmetric five word window.
Vec-tor components were defined as in Equation (6).We also trained the LDA model on BLLIP, usingthe Gibb?s sampling procedure discussed in Grif-fiths et al (2007).
We experimented with differentnumbers of topics on the development set (from 10to 1,000) and report results on the test set with 100topics.
In our experiments, the hyperparameter ?was initialized to .5, and the ?
word probabilitieswere initialized randomly.We integrated our compositional models with atrigram model which we also trained on BLLIP.The model was built using the SRILM toolkit(Stolcke 2002) with backoff and Kneser-Neysmoothing.
As our incremental parser we usedRoark?s (2001) parser trained on sections 2?21 ofthe Penn Treebank containing 936,017 words.
Theparser produces prefix probabilities for each wordof a sentence which we converted to conditionalprobabilities by dividing each current probabilityby the previous one.Statistical Analysis The statistical analyses inthis paper were carried out using linear mixedeffects models (LME, Pinheiro and Bates 2000).The latter can be thought of as generalization oflinear regression that allows the inclusion of ran-dom factors (such as participants or items) as wellas fixed factors (e.g., word frequency).
In ouranalyses, we treat participant as a random factor,which means that our models contain an interceptterm for each participant, representing the individ-ual differences in the rates at which they read.3We evaluated the effect of adding a factor to amodel by comparing the likelihoods of the mod-els with and without that factor.
If a ?2 test on the3Other random factors that are appropriate for our anal-yses are word and sentence; however, due to the large num-ber of instances for these factors (given that the Dundee cor-pus contains 51,502 tokens), we were not able to includethem: the model fitting algorithm we used (implemented inthe R package lme4) does not converge for such large models.201Factor CoefficientIntercept ?.011Word Length .264Launch Distance .109Landing Position .612Word Frequency ?.010Reading Time of Last Word .151Table 1: Coefficients of the baseline LME modelfor total reading timelikelihood ratio is significant, then this indicatesthat the new factor significantly improves modelfit.
We also experimented with adding randomslopes for participant to the model (in addition tothe random intercept); however, this either led tonon-convergence of the model fitting procedure, orfailed to result in an increase in model fit accord-ing to the likelihood ratio test.
Therefore, all mod-els reported in the rest of this paper contain ran-dom intercept of participants as the sole randomfactor.Rather than model raw reading times, we modeltimes on the log scale.
This is desirable for anumber of reasons.
Firstly, the raw reading timestend to have a skew distribution and taking logsproduces something closer to normal, which ispreferable for modeling.
Secondly, the regres-sion equation makes more sense on the log scaleas the contribution of each term to raw readingtime is multiplicative rather than additive.
That is,log(t) = ?i?ixi implies t = ?i e?ixi .
In particular,the intercept term for each participant now repre-sents a multiplicative factor by which that partici-pant is slower or faster.5 ResultsWe computed separate mixed effects models forthree dependent variables, namely first fixation du-ration, first pass duration, and total reading time.We report results for total times throughout, asthe results of the other two dependent variablesare broadly similar.
Our strategy was to first con-struct a baseline model of low-level factors influ-encing reading time, and then to take the resid-uals from that model as the dependent variablein subsequent analyses.
In this way we removedthe effects of low-level factors before investigatingthe factors associated with syntactic and semanticconstraint.
This avoids problems with collinear-ity between low-level factors and the factors weare interested in (e.g., trigram probability is highlycorrelated with word frequency).
The baselinemodel contained the factors word length, word fre-Model Composition CoefficientSSSAdditive ?.03820??
?Multiplicative ?.00895??
?LDAAdditive ?.02500??
?Multiplicative ?.00262??
?Table 2: Coefficients of LME models includingsimple semantic space (SSS) or Latent DirichletAllocation (LDA) as factors; ??
?p < .001quency, launch distance, landing position, and thereading time for the last fixated word, and its pa-rameter estimates are given in Table 1.
To furtherreduce collinearity, we also centered all fixed fac-tors, both in the baseline model, and in the modelsfitted on the residuals that we report in the follow-ing.
Note that some intercorrelations remain be-tween the factors, which we will discuss at the endof Section 5.Before investigating whether an integratedmodel of semantic and syntactic constraint im-proves the goodness of fit over the baseline, we ex-amined the influence of semantic constraint alone.This was necessary as compositional models havenot been previously used to model processingdifficulty.
Besides, replicating Pynte et al?s(2008) finding, we were also interested in assess-ing whether the underlying semantic representa-tion (simple semantic space or LDA) and com-position function (additive versus multiplicative)modulate reading times differentially.We built an LME model that predicted the resid-ual reading times of the baseline model using thesimilarity scores from our composition models asfactors.
We then carried out a ?2 test on the like-lihood ratio of a model only containing the ran-dom factor and the intercept, and a model alsocontaining the semantic factor (cosine similarity).The addition of the semantic factor significantlyimproves model fit for both the simple semanticspace and LDA.
This result is observed for bothadditive and multiplicative composition functions.Our results are summarized in Table 2 which re-ports the coefficients of the four LME models fit-ted against the residuals of the baseline model, to-gether with the p-values of the ?2 test.Before evaluating our integrated surprisal mea-sure, we evaluated its components individually inorder to tease their contributions apart.
For ex-ample, it may be the case that syntactic surprisalis an overwhelmingly better predictor of readingtime than semantic surprisal, however we wouldnot be able to detect this by simply adding a factorbased on Equation (9) to the baseline model.
The202Factor SSS Coef LDA Coef?
log(p) .00760???
.00760???Add?
log(?)
.03810???
.00622???log(?+(1??)
p2p1 ) .00953???
.00943??
?Mult ?
log(?)
.01110???
?.00033log(?+(1??)
p2p1 ) .00882???
.00133Table 3: Coefficients of nested LME models withthe components of SSS or LDA surprisal as fac-tors; only the coefficient of the additional factor ateach step are shownintegrated surprisal measure can be written as:S =?
log(?p1 +(1??
)p2) (10)Where p2 is the incremental parser probability andp1 is the product of the semantic component, ?,and the trigram probability, p. This can be brokendown into the sum of two terms:S =?
log(p1)?
log(?+(1??
)p2p1) (11)Since the first term, ?
log(p1) is itself a product itcan also be broken down further:S =?
log(p)?
log(?)?
log(?+(1??
)p2p1) (12)Thus, to evaluate the contribution of the threecomponents to the integrated surprisal measure wefitted nested LME models, i.e., we entered theseterms one at a time into a mixed effects modeland tested the significance of the improvement inmodel fit for each additional term.We again start with an LME model that onlycontains the random factor and the intercept, withthe residuals of the baseline models as the depen-dent variable.
Considering the trigram model first,we find that adding this factor to the model gives asignificant improvement in fit.
Also adding the se-mantic component (?
log(?))
improves fit further,both for additive and multiplicative compositionfunctions using a simple semantic space.
Finally,the addition of the parser probabilities (log(?+(1??)
p2p1 )) again improves model fit significantly.As far as LDA is concerned, the additive modelsignificantly improves model fit, whereas the mul-tiplicative one does not.
These results mirrorthe findings of Mitchell and Lapata (2009), whoreport that a multiplicative composition functionproduced the lowest perplexity for the simple se-mantic space model, whereas an additive functiongave the best perplexity for the LDA space.
Ta-ble 3 lists the coefficients for the nested models forModel Composition CoefficientSSSAdditive .00804??
?Multiplicative .00819??
?LDAAdditive .00817??
?Multiplicative .00640??
?Table 4: Coefficients of LME models with inte-grated surprisal measure (based on SSS or LDA)as factorall four variants of our semantic constraint mea-sure.Finally, we built a separate LME model wherewe added the integrated surprisal measure (seeEquation (9)) to the model only containing the ran-dom factor and the intercept (see Table 4).
Wedid this separately for all four versions of the in-tegrated surprisal measure (SSS, LDA; additive,multiplicative).
We find that model fit improvedsignificantly all versions of integrated surprisal.One technical issue that remains to be discussedis collinearity, i.e., intercorrelations between thefactors in a model.
The presence of collinearityis problematic, as it can render the model fittingprocedure unstable; it can also affect the signifi-cance of individual factors.
As mentioned in Sec-tion 4 we used two techniques to reduce collinear-ity: residualizing and centering.
Table 5 givesan overview of the correlation coefficients for allpairs of factors.
It becomes clear that collinear-ity has mostly been removed; there is a remainingrelationship between word length and word fre-quency, which is expected as shorter words tend tobe more frequent.
This correlation is not a prob-lem for our analysis, as it is confined to the base-line model.
Furthermore, word frequency and tri-gram probability are highly correlated.
Again thisis expected, given that the frequencies of unigramsand higher-level n-grams tend to be related.
Thiscorrelation is taken care of by residualizing, whichisolates the two factors: word frequency is partof the baseline model, while trigram probability ispart of the separate models that we fit on the resid-uals.
All other correlations are small (with coeffi-cients of .27 or less), with one exception: there isa high correlation between the ?
log(?)
term andthe log(?+ (1?
?)
p2p1 ) term in the multiplicativeLDA model.
This collinearity issue may explainthe absence of a significant improvement in modelfit when these two terms are added to the baseline(see Table 3).203Factor Len Freq ?l(p)?l(?
)Frequency ?.310?
log(p) .230?.700SSSAdd?
log(?)
.016?.120 .025log(?+(1??)
p2p1 ) .024 .036?.270 .065SSSMult ?
log(?)
?.015?.110 .035log(?+(1??)
p2p1 ) .020 .028?.260 .160LDAAdd?
log(?)
?.024?.130 .046log(?+(1??)
p2p1 ) .005 .014?.250 .030LDAMult ?
log(?)
?.120 .006?.046log(?+(1??)
p2p1 )?.089?.005?.180 .740Table 5: Intercorrelations between model factors6 DiscussionIn this paper we investigated the contributions ofsyntactic and semantic constraint in modeling pro-cessing difficulty.
Our work departs from previ-ous approaches in that we propose a single mea-sure which integrates syntactic and semantic fac-tors.
Evaluation on an eye-tracking corpus showsthat our measure predicts reading time better thana baseline model that captures low-level factorsin reading (word length, landing position, etc.
).Crucially, we were able to show that the semanticcomponent of our measure improves reading timepredictions over and above a model that includessyntactic measures (based on a trigram model andincremental parser).
This means that semanticcosts are a significant predictor of reading time inaddition to the well-known syntactic surprisal.An open issue is whether a single, integratedmeasure (as evaluated in Table 4) fits the eye-movement data significantly better than separatemeasures for trigram, syntactic, and semantic sur-prisal (as evaluated in Table 3.
However, we arenot able to investigate this hypothesis: our ap-proach to testing the significance of factors re-quires nested models; the log-likelihood test (seeSection 4) is only able to establish whether addinga factor to a model improves its fit; it cannot com-pare models with disjunct sets of factors (such asa model containing the integrated surprisal mea-sure and one containing the three separate ones).However, we would argue that a single, integratedmeasure that captures human predictive process-ing is preferable over a collection of separate mea-sures.
It is conceptually simpler (as it is more par-simonious), and is also easier to use in applica-tions (such as readability prediction).
Finally, anintegrated measure requires less parameters; ourdefinition of surprisal in 12 is simply the sum ofthe trigram, syntactic, and semantic components.An LME model containing separate factors, on theother hand, requires a coefficient for each of them,and thus has more parameters.In evaluating our model, we adopted a broadcoverage approach using the reading time datafrom a naturalistic corpus rather than artificiallyconstructed experimental materials.
In doing so,we were able to compare different syntactic andsemantic costs on the same footing.
Previousanalyses of semantic constraint have been con-ducted on different eye-tracking corpora (Dundeeand Embra Corpus) and on different languages(English, French).
Moreover, comparisons of theindividual contributions of syntactic and semanticfactors were generally absent from the literature.Our analysis showed that both of these factors canbe captured by our integrated surprisal measurewhich is uniformly probabilistic and thus prefer-able to modeling semantic and syntactic costs dis-jointly using a mixture of probabilistic and non-probabilistic measures.An interesting question is which aspects of se-mantics our model is able to capture, i.e., whydoes the combination of LSA or LDA representa-tions with an incremental parser yield a better fit ofthe behavioral data.
In the psycholinguistic liter-ature, various types of semantic information havebeen investigated: lexical semantics (word senses,selectional restrictions, thematic roles), senten-tial semantics (scope, binding), and discourse se-mantics (coreference and coherence); see Keller(2010) of a detailed discussion.
We conjecture thatour model is mainly capturing lexical semantics(through the vector space representation of words)and sentential semantics (through the multiplica-tion or addition of words).
However, discoursecoreference effects (such as the ones reported byAltmann and Steedman (1988) and much subse-quent work) are probably not amenable to a treat-ment in terms of vector space semantics; an ex-plicit representation of discourse entities and co-reference relations is required (see Dubey 2010for a model of human sentence processing that canhandle coreference).A key objective for future work will be to in-vestigate models that integrate semantic constraintwith syntactic predictions more tightly.
For ex-ample, we could envisage a parser that uses se-mantic representations to guide its search, e.g., bypruning syntactic analyses that have a low seman-tic probability.
At the same time, the semanticmodel should have access to syntactic informa-tion, i.e., the composition of word representationsshould take their syntactic relationships into ac-count, rather than just linear order.204ReferencesACL.
2010.
Proceedings of the 48th Annual Meet-ing of the Association for Computational Lin-guistics.
Uppsala.Altmann, Gerry T. M. and Yuki Kamide.
1999.Incremental interpretation at verbs: Restrictingthe domain of subsequent reference.
Cognition73:247?264.Altmann, Gerry T. M. and Mark J. Steedman.1988.
Interaction with context during humansentence processing.
Cognition 30(3):191?238.Bellegarda, Jerome R. 2000.
Exploiting latent se-mantic information in statistical language mod-eling.
Proceedings of the IEEE 88(8):1279?1296.Berry, Michael W., Susan T. Dumais, andGavin W. O?Brien.
1995.
Using linear algebrafor intelligent information retrieval.
SIAM re-view 37(4):573?595.Bever, Thomas G. 1970.
The cognitive basis forlinguistic strutures.
In J. R. Hayes, editor, Cog-nition and the Development of Language, Wi-ley, New York, pages 279?362.Blei, David M., Andrew Y. Ng, and Michael I. Jor-dan.
2003.
Latent Dirichlet alocation.
Journalof Machine Learning Research 3:993?1022.Bullinaria, John A. and Joseph P. Levy.
2007.
Ex-tracting semantic representations from word co-occurrence statistics: A computational study.Behavior Research Methods 39:510?526.Clifton, Charles, Adrian Staub, and Keith Rayner.2007.
Eye movement in reading words and sen-tences.
In R V Gompel, M Fisher, W Murray,and R L Hill, editors, Eye Movements: A Win-dow in Mind and Brain, Elsevier, pages 341?372.Coccaro, Noah and Daniel Jurafsky.
1998.
To-wards better integration of semantic predictorsin satistical language modeling.
In Proceedingsof the 5th International Conference on SpokenLanguage Processing.
Sydney, Australia, pages2403?2406.Demberg, Vera and Frank Keller.
2008.
Data fromeye-tracking corpora as evidence for theoriesof syntactic processing complexity.
Cognition101(2):193?210.Dubey, Amit.
2010.
The influence of discourse onsyntax: A psycholinguistic model of sentenceprocessing.
In ACL.Ferrara Boston, Marisa, John Hale, ReinholdKliegl, Umesh Patil, and Shravan Vasishth.2008.
Parsing costs as predictors of reading dif-ficulty: An evaluation using the Potsdam Sen-tence Corpus.
Journal of Eye Movement Re-search 2(1):1?12.Frank, Stefan L. 2009.
Surprisal-based compar-ison between a symbolic and a connectionistmodel of sentence processing.
In Proceedingsof the 31st Annual Conference of the CognitiveScience Society.
Austin, TX, pages 139?1144.Gibson, Edward.
2000.
Dependency locality the-ory: A distance-dased theory of linguistic com-plexity.
In Alec Marantz, Yasushi Miyashita,and Wayne O?Neil, editors, Image, Language,Brain: Papers from the First Mind ArticulationProject Symposium, MIT Press, Cambridge,MA, pages 95?126.Gildea, Daniel and Thomas Hofmann.
1999.Topic-based language models using EM.
InProceedings of the 6th European Conferenceon Speech Communiation and Technology.
Bu-dapest, Hungary, pages 2167?2170.Griffiths, Thomas L., Mark Steyvers, andJoshua B. Tenenbaum.
2007.
Topics in se-mantic representation.
Psychological Review114(2):211?244.Hale, John.
2001.
A probabilistic Earley parser asa psycholinguistic model.
In Proceedings of the2nd Conference of the North American Chap-ter of the Association.
Association for Compu-tational Linguistics, Pittsburgh, PA, volume 2,pages 159?166.Keller, Frank.
2010.
Cognitively plausible modelsof human language processing.
In ACL.Kennedy, Alan and Joel Pynte.
2005.
Parafoveal-on-foveal effects in normal reading.
Vision Re-search 45:153?168.Konieczny, Lars.
2000.
Locality and parsing com-plexity.
Journal of Psycholinguistic Research29(6):627?645.Landauer, Thomas K. and Susan T. Dumais.
1997.A solution to Plato?s problem: the latent seman-tic analysis theory of acquisition, induction andrepresentation of knowledge.
Psychological Re-view 104(2):211?240.Levy, Roger.
2008.
Expectation-based syntacticcomprehension.
Cognition 106(3):1126?1177.Marslen-Wilson, William D. 1973.
Linguisticstructure and speech shadowing at very short la-tencies.
Nature 244:522?523.McDonald, Scott.
2000.
Environmental Determi-nants of Lexical Processing Effort.
Ph.D. thesis,University of Edinburgh.205McDonald, Scott and Chris Brew.
2004.
A dis-tributional model of semantic context effects inlexical processing.
In Proceedings of the 42thAnnual Meeting of the Association for Com-putational Linguistics.
Barcelona, Spain, pages17?24.McDonald, Scott A. and Richard C. Shillcock.2003.
Low-level predictive inference in read-ing: The influence of transitional probabilitieson eye movements.
Vision Research 43:1735?1751.Mitchell, Jeff and Mirella Lapata.
2008.
Vector-based models of semantic composition.
In Pro-ceedings of ACL-08: HLT .
Columbus, OH,pages 236?244.Mitchell, Jeff and Mirella Lapata.
2009.
Languagemodels based on semantic composition.
In Pro-ceedings of the 2009 Conference on EmpiricalMethods in Natural Language Processing.
Sin-gapore, pages 430?439.Narayanan, Srini and Daniel Jurafsky.
2002.
ABayesian model predicts human parse prefer-ence and reading time in sentence processing.
InThomas G. Dietterich, Sue Becker, and ZoubinGhahramani, editors, Advances in Neural In-formation Processing Systems 14.
MIT Press,Cambridge, MA, pages 59?65.Pado?, Sebastian and Mirella Lapata.
2007.Dependency-based construction of semanticspace models.
Computational Linguistics33(2):161?199.Pado?, Ulrike, Matthew W. Crocker, and FrankKeller.
2009.
A probabilistic model of semanticplausibility in sentence processing.
CognitiveScience 33(5):794?838.Pinheiro, Jose C. and Douglas M. Bates.2000.
Mixed-effects Models in S and S-PLUS.Springer, New York.Pinker, Steven.
1994.
The Language Instinct: Howthe Mind Creates Language.
HarperCollins,New York.Plate, Tony A.
1995.
Holographic reduced repre-sentations.
IEEE Transactions on Neural Net-works 6(3):623?641.Pynte, Joel, Boris New, and Alan Kennedy.
2008.On-line contextual influences during readingnormal text: A multiple-regression analysis.
Vi-sion Research 48:2172?2183.Rayner, Keith.
1998.
Eye movements in read-ing and information processing: 20 years of re-search.
Psychological Bulletin 124(3):372?422.Roark, Brian.
2001.
Probabilistic top-down pars-ing and language modeling.
ComputationalLinguistics 27(2):249?276.Roark, Brian, Asaf Bachrach, Carlos Cardenas,and Christophe Pallier.
2009.
Deriving lex-ical and syntactic expectation-based measuresfor psycholinguistic modeling via incrementaltop-down parsing.
In Proceedings of the 2009Conference on Empirical Methods in NaturalLanguage Processing.
Association for Compu-tational Linguistics, Singapore, pages 324?333.Smolensky, Paul.
1990.
Tensor product vari-able binding and the representation of symbolicstructures in connectionist systems.
ArtificialIntelligence 46:159?216.Stanovich, Kieth E. and Richard F. West.
1981.The effect of sentence context on ongoing wordrecognition: Tests of a two-pricess theory.
Jour-nal of Experimental Psychology: Human Per-ception and Performance 7:658?672.Staub, Adrian and Charles Clifton.
2006.
Syntac-tic prediction in language comprehension: Evi-dence from either .
.
.or.
Journal of Experimen-tal Psychology: Learning, Memory, and Cogni-tion 32:425?436.Steyvers, Mark and Tom Griffiths.
2007.
Proba-bilistic topic models.
In T. Landauer, D. Mc-Namara, S Dennis, and W Kintsch, editors, AHandbook of Latent Semantic Analysis, Psy-chology Press.Stolcke, Andreas.
2002.
Srilm - an extensible lan-guage modeling toolkit.
In Proceedings of theInternatinal Conference on Spoken LanguageProcessing.
Denver, Colorado.Sturt, Patrick and Vincenzo Lombardo.
2005.Processing coordinated structures: Incremen-tality and connectedness.
Cognitive Science29(2):291?305.Tanenhaus, Michael K., Michael J. Spivey-Knowlton, Kathleen M. Eberhard, and Julie C.Sedivy.
1995.
Integration of visual and linguis-tic information in spoken language comprehen-sion.
Science 268:1632?1634.van Berkum, Jos J.
A., Colin M. Brown, and PeterHagoort.
1999.
Early referential context effectsin sentence processing: Evidence from event-related brain potentials.
Journal of Memory andLanguage 41:147?182.Wright, Barton and Merrill F. Garrett.
1984.
Lex-ical decision in sentences: Effects of syntacticstructure.
Memory and Cognition 12:31?45.206
