Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 678?682,Dublin, Ireland, August 23-24, 2014.Turku: Broad-Coverage Semantic Parsing with Rich FeaturesJenna Kanerva?Department of InformationTechnologyUniversity of TurkuFinlandjmnybl@utu.fiJuhani Luotolahti?Department of InformationTechnologyUniversity of TurkuFinlandmjluot@utu.fiFilip GinterDepartment of InformationTechnologyUniversity of TurkuFinlandfigint@utu.fiAbstractIn this paper we introduce our system ca-pable of producing semantic parses of sen-tences using three different annotation for-mats.
The system was used to partic-ipate in the SemEval-2014 Shared Taskon broad-coverage semantic dependencyparsing and it was ranked third with anoverall F1-score of 80.49%.
The sys-tem has a pipeline architecture, consistingof three separate supervised classificationsteps.1 IntroductionIn the SemEval-2014 Task 8 on semantic parsing,the objective is to extract for each sentence a richset of typed semantic dependencies in three differ-ent formats: DM, PAS and PCEDT.
These formatsdiffer substantially both in the assignment of se-mantic heads as well as in the lexicon of seman-tic dependency types.
In the open track of theshared task, participants were encouraged to useall resources and tools also beyond the providedtraining data.
To improve the comparability of thesystems, the organizers provided ready-to-use de-pendency parses produced using the state-of-the-art parser of Bohnet and Nivre (2012).In this paper we describe our entry in the opentrack of the shared task.
Our system is a pipelineof three support vector machine classifiers trainedseparately for detecting semantic dependencies,assigning their roles, and selecting the top nodesof semantic graphs.
In this, we loosely followthe architecture of e.g.
the TEES (Bj?orne et al.,2012) and EventMine (Miwa et al., 2012) systems,which were found to be effective in the structurally?These authors contributed equally.This work is licenced under a Creative Commons Attribution4.0 International License.
Page numbers and proceedingsfooter are added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/related task of biomedical event extraction.
Sim-ilar classification approach is shown to be effec-tive also in semantic parsing by e.g.
Zhao et al.
(2009), the winner of the CoNLL?09 Shared Taskon Syntactic and Semantic Dependencies in Mul-tiple Languages (SRL-only subtask) (Haji?c et al.,2009), where semantic parsing is approached as aword-pair classification problem and semantic ar-guments and their roles are predicted simultane-ously.
In preliminary experiments, we also de-veloped a joint approach to simultaneously iden-tify semantic dependencies and assign their roles,but found that the performance of the joint predic-tion was substantially worse than for the currentpipeline approach.
As the source of features, werely heavily on the syntactic parses as well as otherexternal resources such as vector space represen-tations of words and large-scale syntactic n-gramstatistics.In the following sections, we describe the threeindividual classification steps of our semanticparsing pipeline.2 Detecting Semantic DependenciesThe first step of our semantic parsing pipelineis to detect semantic dependencies, i.e.
governor-dependent pairs which has a semantic relation be-tween them.
The first stage covers only the identi-fication of such dependencies; the labels describ-ing the semantic roles of the dependents are as-signed in a later stage.The semantic dependencies are identified usinga binary support vector machine classifier from theLIBSVM package (Chang and Lin, 2011).
Eachpossible combination of two tokens in the sen-tence is considered to be a candidate for a seman-tic dependency in both directions, and thus alsoincluded as a training example.
No beforehandpruning of possible candidates is performed dur-ing training.
However, we correct for the over-whelming number of negative training examples678by setting the weights of positive and negative ex-amples used during training, so as to maximize theunlabeled F1-score on the development set.Increasing the recall of semantic dependencydetection can be beneficial for the overall perfor-mance of the pipeline system, since a candidatelost in the dependency detection stage cannot berecovered later.
We therefore tested the approachapplied, among others by Bj?orne et al.
(2012),whereby the dependency detection stage heavilyovergenerates candidates and the next stage in thepipeline is given the option to predict a nega-tive label, thus removing a candidate dependency.In preliminary experiments we tried to explicitlyovergenerate the dependency candidates by alter-ing the classifier threshold, but noticed that heavyovergeneration of positive examples leads to a de-creased performance in the role assigning stage.Instead, the above-mentioned optimization of theexample weights during training results in a clas-sifier which overgenerates positive examples by4.4%, achieving the same objective and improvingthe overall performance of the system.Features used during the dependency identifi-cation are derived from tokens and the syntacticparse trees provided by the organizers.
Our pri-mary source of features are the syntactic trees,since 73.2% of semantic dependencies have a cor-responding undirected syntactic dependency in theparse tree.
Further, the syntactic dependency pathbetween the governor and the dependent is shorterthan their linear distance in 48.8% of cases (in43.4% of cases the distance is the same).
The finalfeature set used in the identification is optimizedby training models with different combinations offeatures and selecting the best combination basedon performance on the held-out development set.Interestingly, the highest performance is achievedwith a rather small set of features, whose full list-ing is shown in Table 1.
The feature vectors arenormalized to unit length prior to classificationand the SVM regularization parameter c is opti-mized separately for each annotation format.3 Role AssignmentAfter the semantic governor-dependent pairs areidentified, the next step is to assign a role foreach pair to constitute a full semantic dependency.This is done by training a multiclass support vec-tor machine classifier implemented in the SVM-multiclass package by Joachims (1999).
We it-Feature D R Targ.pos X Xarg.deptype X Xarg.lemma X Xpred.pos X X Xpred.deptype X X Xpred.lemma X X Xpred.is predicate X Xarg.issyntaxdep Xarg.issyntaxgov Xarg.issyntaxsibling Xpath.length X Xundirected path.deptype X Xdirected path.deptype X Xundirected path.pos X Xextended path.deptype X Xsimplified path.deptype with len Xsimplified path.deptype wo len Xsplitted undirected path.deptype Xarg.prev.pos X Xarg.next.pos X Xarg.prev+arg.pos X Xarg.next+arg.pos X Xarg.next+arg+arg.prev.pos X Xpred.prev.pos X Xpred.next.pos X Xpred.prev+pred.pos X Xpred.next+pred.pos X Xpred.next+pred+pred.prev.pos X Xlinear route.pos Xarg.child.pos Xarg.child.deptype Xarg.child.lemma Xpred.child.pos Xpred.child.deptype X Xpred.child.lemma Xsyntaxgov.child.deptype Xvector similarities Xn-gram frequencies Xpred.sem role Xpred.child.sem role Xpred.syntaxsibling.deptype Xpred.semanticsibling.sem role XTable 1: Features used in the detection of semanticdependencies (D), assigning their roles (R) and topnode detection (T).
path refers to syntactic depen-dencies between the argument and the predicate,and linear route refers to all tokens between theargument and the predicate.
In top node detection,where only one token is considered at a time, thepred is used to represent that token.679erate through all identified dependencies, and foreach assign a role, or alternatively classify it as anegative example.
This is to account for the 4.4%of overgenerated dependencies.
However, the pro-portion of negative classifications should stay rel-atively low and to ensure this, we downsample thenumber of negative examples used in training tocontain only 5% of all negative examples.
Thedownsampling ratio is optimized on the develop-ment set using grid search and downsampled train-ing instances are chosen randomly.The basic features, shown in Table 1, follow thesame style as in dependency identification.
Wealso combine some of the basic features by creat-ing all possible feature pairs in a given set, but donot perform this with the full set of features.
In theopen track, participants are also allowed to use ad-ditional data and tools beyond the official trainingdata.
In addition to the parse trees, we include alsofeatures utilizing syntactic n-gram frequencies andvector space similarities.Google has recently released a large corpusof syntactic n-grams, a collection of depen-dency subtrees with frequency counts (Goldbergand Orwant, 2013).
The syntactic n-grams areinduced from the Google Books collection, a350B token corpus of syntactically parsed text.In this work we are interested in arcs, whichare (governor, dependent, syntactic relation)triplets associated with their count.For each governor-dependent pair, we generatea set of n-gram features by iterating through allknown dependency types and searching from thesyntactic n-grams how many times (if any) thegovernor-dependent pair with the particular de-pendency type is seen.
A separate feature is thencreated for each dependency type and the countsare encoded in feature weights compressed usingw = log10(count).
This approach gives us an op-portunity to include statistical information aboutword relations induced from a very large corpus.Information is captured also outside the particularsyntactic context, as we iterate through all knowndependency types during the process.Another source of additional data used in roleclassification is a publicly available Google Newsvector space model1representing word similari-ties.
The vector space model is induced from theGoogle News corpus with the word2vec software(Mikolov et al., 2013) and negative sampling ar-1https://code.google.com/p/word2vec/chitecture, and each vector have 300 dimensions.The vector space representation gives us an oppor-tunity to measure word similarities using the stan-dard cosine similarity function.The approach to transforming the vector repre-sentations into features varies with the three dif-ferent annotation formats.
On DM and PAS, wefollow the method of Kanerva and Ginter (2014),where for each role an average argument vectoris calculated.
This is done by averaging all wordvectors seen in the training data as arguments forthe given predicate with a particular role.
For eachcandidate argument, we can then establish a set ofsimilarity values to each possible role by takingthe cosine similarity of the argument vector to therole-wise average vectors.
These similarities arethen turned into separate features, where the simi-larity values are encoded as feature weights.On PCEDT, preliminary experiments showedthat the best strategy to include word vectors intoclassification is by turning them directly into fea-tures, so that each dimension of the word vectoris represented as a separate feature.
Thus, we it-erate through all 300 vector dimensions and cre-ate a separate feature representing the position andvalue of a particular dimension.
Values are againencoded in feature weights.
These features are cre-ated separately for both the argument and the pred-icate.
The word vectors are pre-normalized to unitlength, so no additional normalization of featureweights is needed.Both the n-gram?
and vector similarities?basedfeatures give a modest improvement to the classi-fication performance.4 Detecting Top NodesThe last step in the pipeline is the detection oftop nodes.
A top node is the semantic head orthe structural root of the sentence.
Typically eachsentence annotated in the DM and PAS formatscontains one top node, whereas PCEDT sentenceshave on average 1.12 top nodes per sentence.As in the two previous stages, we predict topnodes by training a support vector machine clas-sifier, with each token being considered a candi-date.
Because the top node prediction is the laststep performed, in addition to the basic informa-tion available in the two previous steps, we areable to use also predicted arguments as features.Otherwise, the feature set used in top node detec-tion follows the same style as in the two previous680LP LR LF UFDM 80.94 82.14 81.53 83.48PAS 87.33 87.76 87.54 88.97PCEDT 72.42 72.37 72.40 85.86Overall 80.23 80.76 80.49 86.10Table 2: Overall scores of whole task as well asseparately for each annotation format in terms oflabeled precision (LP), recall (LR) and F1-score(LF) as well as unlabeled F1-score (UF).tasks, but is substantially smaller (see Table 1).
Wealso create all possible feature pairs prior to clas-sification to simulate the use of a second-degreepolynomial kernel.For each token in the sentence, we predictwhether it is a top node or not.
However, in DMand PAS, where typically only one top node is al-lowed, we choose only the token with the maxi-mum positive value to be the final top node.
InPCEDT, we simply let all positive predictions actas top nodes.5 ResultsThe primary evaluation measure is the labeled F1-score of the predicted dependencies, where theidentification of top nodes is incorporated as anadditional dummy dependency.
The overall se-mantic F1-score of our system is 80.49%.
Theprediction performance in DM is 81.53%, in PAS87.54% and in PCEDT 72.40%.
The top nodesare identified with an overall F1-score of 87.05%.The unlabeled F1-score reflects the performanceof the dependency detection in isolation from la-beling task and by comparing the labeled and un-labeled F1-scores from Table 2 we can see that themost common mistake relates to the identificationof correct governor-dependent pairs.
This is espe-cially true with the DM and PAS formats where thedifference between labeled and unlabeled scoresis very small (1.9pp and 1.4pp), reflecting highperformance in assigning the roles.
Instead, inPCEDT the role assignment accuracy is substan-tially below the other two and the difference be-tween unlabeled and labeled F1-score is as muchas 13.5pp.
One likely reason is the higher numberof possible roles defined in the PCEDT format.5.1 DiscussionNaturally, our system generally performs betterwith frequently seen semantic roles than roles thatare seen rarely.
In the case of DM, the 4 mostcommon semantic roles cover over 87% of thegold standard dependencies and are predicted witha macro F1-score of 85.3%, while the remaining35 dependency labels found in the gold standardare predicted at an average rate of 49.4%.
Togive this a perspective, the most common 4 roleshave on average 121K training instances, while theremaining 35 roles have on average about 2000training instances.
For PAS, the 9 most commonlabels, which comprise over 80% of all depen-dencies in the gold standard data and have on av-erage about 66K training instances per role, arepredicted with an F1-score of 87.6%, while theremaining 32 labels have on average 4200 train-ing instance and are predicted with an F1-score of57.8%.
The PCEDT format has the highest num-ber of possible semantic roles and also lowest cor-relation between the frequency in training data andF1-score.
For PCEDT, the 11 most common la-bels, which cover over 80% of all dependencies inthe gold standard, are predicted with an F1-scoreof 69.6%, while the remaining 53 roles are pre-dicted at an average rate of 46.6%.
The highernumber of roles also naturally affects the numberof training instances and the 11 most common la-bels in PCEDT have on average 35K training in-stances, while the remaining 53 roles have on av-erage 1600 instances per role.Similarly, the system performs better with se-mantic arguments which are nearby the governor.This is true for both linear distance between thetwo tokens and especially for distance measuredby syntactic dependency steps.
For example in thecase of DM, semantic dependencies shorter than3 steps in the syntactic tree cover more than 95%of the semantic dependencies in the gold standardand have an F1-score of 75.1%, while the rest haveonly 32.6%.
The same general pattern is also evi-dent in the other formats.6 ConclusionIn this paper we presented our system used toparticipate in the SemEval-2014 Shared Task onbroad-coverage semantic dependency parsing.
Webuilt a pipeline of three supervised classifiers toidentify semantic dependencies, assign a role foreach dependency and finally, detect the top nodes.In addition to basic features used in classifica-tion we have shown that additional information,such as frequencies of syntactic n-grams and word681similarities derived from vector space representa-tions, can also positively contribute to the classifi-cation performance.The overall F1-score of our system is 80.49%and it was ranked third in the open track of theshared task.AcknowledgmentsThis work was supported by the Emil AaltonenFoundation and the Kone Foundation.
Computa-tional resources were provided by CSC ?
IT Cen-ter for Science.ReferencesJari Bj?orne, Filip Ginter, and Tapio Salakoski.
2012.University of Turku in the BioNLP?11 shared task.BMC Bioinformatics, 13(Suppl 11):S4.Bernd Bohnet and Joakim Nivre.
2012.
A transition-based system for joint part-of-speech tagging and la-beled non-projective dependency parsing.
In Pro-ceedings of the 2012 Joint Conference on Empiri-cal Methods in Natural Language Processing andComputational Natural Language Learning, pages1455?1465.Chih-Chung Chang and Chih-Jen Lin.
2011.
LIB-SVM: A library for support vector machines.
ACMTransactions on Intelligent Systems and Technology,2:27:1?27:27.Yoav Goldberg and Jon Orwant.
2013.
A dataset ofSyntactic-Ngrams over time from a very large cor-pus of English books.
In Second Joint Conferenceon Lexical and Computational Semantics (*SEM),Volume 1: Proceedings of the Main Conference andthe Shared Task: Semantic Textual Similarity, pages241?247.Jan Haji?c, Massimiliano Ciaramita, Richard Johans-son, Daisuke Kawahara, Maria Ant`onia Mart?
?, Llu?
?sM`arquez, Adam Meyers, Joakim Nivre, SebastianPad?o, Jan?St?ep?anek, et al.
2009.
The CoNLL-2009shared task: Syntactic and semantic dependenciesin multiple languages.
In Proceedings of the Thir-teenth Conference on Computational Natural Lan-guage Learning: Shared Task, pages 1?18.Thorsten Joachims.
1999.
Making large-scale SVMlearning practical.
In Advances in Kernel Meth-ods - Support Vector Learning, pages 169?184.
MITPress.Jenna Kanerva and Filip Ginter.
2014.
Post-hoc ma-nipulations of vector space models with applicationto semantic role labeling.
In Proceedings of the 2ndWorkshop on Continuous Vector Space Models andtheir Compositionality (CVSC)@ EACL, pages 1?10.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013.
Efficient estimation of word represen-tations in vector space.
In Workshop Proceedings ofInternational Conference on Learning Representa-tions.Makoto Miwa, Paul Thompson, John McNaught, Dou-glas Kell, and Sophia Ananiadou.
2012.
Extractingsemantically enriched events from biomedical liter-ature.
BMC Bioinformatics, 13(1):108.Hai Zhao, Wenliang Chen, Chunyu Kit, and GuodongZhou.
2009.
Multilingual dependency learning:a huge feature engineering method to semantic de-pendency parsing.
In Proceedings of the Thir-teenth Conference on Computational Natural Lan-guage Learning: Shared Task, pages 55?60.682
