Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 421?431,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsTransition-Based Neural Word SegmentationMeishan Zhang1and Yue Zhang2and Guohong Fu11.
School of Computer Science and Technology, Heilongjiang University, Harbin, China2.
Singapore University of Technology and Designmason.zms@gmail.com,yue zhang@sutd.edu.sg,ghfu@hotmail.comAbstractCharacter-based and word-based methodsare two main types of statistical modelsfor Chinese word segmentation, the for-mer exploiting sequence labeling modelsover characters and the latter typically ex-ploiting a transition-based model, with theadvantages that word-level features canbe easily utilized.
Neural models havebeen exploited for character-based Chi-nese word segmentation, giving high accu-racies by making use of external characterembeddings, yet requiring less feature en-gineering.
In this paper, we study a neu-ral model for word-based Chinese wordsegmentation, by replacing the manually-designed discrete features with neural fea-tures in a word-based segmentation frame-work.
Experimental results demonstratethat word features lead to comparable per-formances to the best systems in the litera-ture, and a further combination of discreteand neural features gives top accuracies.1 IntroductionStatistical word segmentation methods can be cat-egorized character-based (Xue, 2003; Tseng et al,2005) and word-based (Andrew, 2006; Zhang andClark, 2007) approaches.
The former casts wordsegmentation as a sequence labeling problem, us-ing segmentation tags on characters to mark theirrelative positions inside words.
The latter, in con-trast, ranks candidate segmented outputs directly,extracting both character and full-word features.An influential character-based word segmenta-tion model (Peng et al, 2004; Tseng et al, 2005)uses B/I/E/S labels to mark a character as the be-ginning, internal (neither beginning nor end), endand only-character (both beginning and end) of acharacter-based word-baseddiscretePeng et al (2004) Andrew (2006)Tseng et al (2005) Zhang and Clark (2007)neuralZheng et al (2013)this workChen et al (2015b)Figure 1: Word segmentation methods.word, respectively, employing conditional randomfield (CRF) to model the correspondence betweenthe input character sequence and output label se-quence.
For each character, features are extractedfrom a five-character context window and a two-label history window.
Subsequent work exploresdifferent label sets (Zhao et al, 2006), feature sets(Shi and Wang, 2007) and semi-supervised learn-ing (Sun and Xu, 2011), reporting state-of-the-artaccuracies.Recently, neural network models have been in-vestigated for the character tagging approach.
Themain idea is to replace manual discrete featureswith automatic real-valued features, which are de-rived automatically from distributed character rep-resentations using neural networks.
In particular,convolution neural network1(Zheng et al, 2013),tensor neural network (Pei et al, 2014), recur-sive neural network (Chen et al, 2015a) and long-short-term-memory (LSTM) (Chen et al, 2015b)have been used to derive neural feature represen-tations from input word sequences, which are fedinto a CRF inference layer.In this paper, we investigate the effectiveness ofword embedding features for neural network seg-mentation using transition-based models.
Sinceit is challenging to integrate word features tothe CRF inference framework of the existing1The term in this paper is used to denote the neural net-work structure with convolutional layers, which is differentfrom the typical convolution neural network that has a pool-ing layer upon convolutional layers (Krizhevsky et al, 2012).421step action buffer(?
?
?w?1w0) queue(c0c1?
?
?
)0 - ?
?
?
?
?
?1 SEP ?
?
?
?
?
?2 APP ??
?
?
?
?
?3 SEP ??
?
?
?
?
?
?4 APP ??
??
?
?
?
?
?5 SEP ??
??
?
?
?
?
?
?6 APP ??
??
??
?
?
?
?
?7 SEP ?
?
?
??
?
?
?
?8 APP ?
?
?
??
??
?
?9 SEP ?
?
?
??
?
?10 APP ?
?
?
??
??
?Figure 2: Segmentation process of ???
(Chi-nese) ??
(foreign company) ??
(busi-ness) ??
(develop) ??
(quickly)?.character-based methods, we take inspiration fromword-based discrete segmentation instead.
In par-ticular, we follow Zhang and Clark (2007), usingthe transition-based framework to decode a sen-tence from left-to-right incrementally, scoring par-tially segmented results using both character-leveland word-level features.
Beam-search is applied toreduce error propagation and large-margin train-ing with early-update (Collins and Roark, 2004) isused for learning from inexact search.We replace the discrete word and character fea-tures of Zhang and Clark (2007) with word andcharacter embeddings, respectively, and changetheir linear model into a deep neural network.Following Zheng et al (2013) and Chen et al(2015b), we use convolution neural networks toachieve local feature combination and LSTM tolearn global sentence-level features, respectively.The resulting model is a word-based neural seg-menter that can leverage rich embedding features.Its correlation with existing work on Chinese seg-mentation is shown in Figure 1.Results on standard benchmark datasets showthe effectiveness of word embedding features forneural segmentation.
Our method achieves state-of-the-art results without any preprocess based onexternal knowledge such as Chinese idioms ofChen et al (2015a) and Chen et al (2015b).
We re-lease our code under GPL for research reference.22 Baseline Discrete ModelWe exploit the word-based segmentor of Zhangand Clark (2011) as the baseline system.
It in-crementally segments a sentence using a transitionsystem, with a state holding a partially-segmented2https://github.com/SUTDNLP/NNTransitionSegmentor.sentence in a buffer s and ordering the next incom-ing characters in a queue q.
Given an input Chi-nese sentence, the buffer is initially empty and thequeue contains all characters of the sentence, a se-quence of transition actions are used to consumecharacters in the queue and build the output sen-tence in the buffer.
The actions include:?
Append (APP), which removes the firstcharacter from the queue, and appends it tothe last word in the buffer;?
Separate (SEP), which moves the firstcharacter of the queue onto the buffer as anew (sub) word.Given the input sequence of characters ????????????
(The business of foreigncompany in China develops quickly), the correctoutput can be derived using action sequence ?SEPAPP SEP APP SEP APP SEP APP SEP APP?, asshown in Figure 2.Search.
Based on the transition system, the de-coder searches for an optimal action sequence fora given sentence.
Denote an action sequence asA = a1?
?
?
an.
We define the score of A as thetotal score of all actions in the sequence, which iscomputed by:score(A) =?a?Ascore(a) =?a?Aw ?
f(s, q, a),wherew is the model parameters, f is a feature ex-traction function, s and q are the buffer and queueof a certain state before the action a is applied.The feature templates are shown in Table 1,which are the same as Zhang and Clark (2011).These base features include three main source ofinformation.
First, characters in the front of thequeue and the end of the buffer are used for scor-ing both separate and append actions (e.g.
c0).Second, words that are identified are used to guideseparate actions (e.g.
w0).
Third, relevant infor-mation of identified words, such as their lengthsand first/last characters are utilized for additionalfeatures (e.g.
len(w?1)).We follow Zhang and Clark (2011) in usingbeam-search for decoding, shown in Algorith 1,where ?
is the set of model parameters.
Initiallythe beam contains only the initial state.
At eachstep, each state in the beam is extended by apply-ing both SEP and APP, resulting in a set of newstates, which are scored and ranked.
The top B are422Feature templates Actionc?1c0APP, SEPw?1, w?1w?2, w?1c0, w?2len(w?1)SEPstart(w?1)c0, end(w?1)c0start(w?1)end(w?1), end(w?2)end(w?1)w?2len(w?1), len(w?2)w?1w?1, where len(w?1) = 1Table 1: Feature templates for the baseline model,where widenotes the word in the buffer, cide-notes the character in the queue, as shown in Fig-ure 2, start(.
), end(.)
and len(.)
denote the first,last character and length of a word, respectively.Algorithm 1 Beam-search decoding.function DECODE(c1?
?
?
cn, ?)agenda?
{ (?, c1?
?
?
cn, score=0.0) }for i in 1 ?
?
?nbeam?
{ }for cand in agendanew?
SEP(cand, ci, ?
)ADDITEM(beam, new)new?
APP(cand, ci, ?
)ADDITEM(beam, new)agenda?
TOP-B(beam, B)best?
BESTITEM(agenda)w1?
?
?wm?
EXTRACTWORDS(best)used as the beam states for the next step.
The sameprocess replaces until all input character are pro-cessed, and the highest-scored state in the beam istaken for output.
Online leaning with max-marginis used, which is given in section 4.3 Transition-Based Neural ModelWe use a neural network model to replace thediscrete linear model for scoring transition actionsequences.
For better comparison between dis-crete and neural features, the overall segmentationframework of the baseline is used, which includesthe incremental segmentation process, the beam-search decoder and the training process integratedwith beam-search (Zhang and Clark, 2011).
In ad-dition, the neural network scorer takes the simi-lar feature sources as the baseline, which includescharacter information over the input, word infor-mation of the partially constructed output, and thehistory sequence of the actions that have been ap-plied so far.The overall architecture of the neural scoreris shown in Figure 3.
Given a certain statescore(SEP) score(APP)?
?
?hsep?
?
?happ?
?
?rc?
?
?rw?
?
?raword sequence character sequence action sequenceRNN RNN RNN?
?
?w?1w0?
?
?
c?1c0c1?
?
?
?
?
?
a?1a0Figure 3: Scorer for the neural transition-basedChinese word segmentation model.
We denote thelast word in the buffer as w0, the next incomingcharacter as c0in the queue in consistent with Fig-ure 2, and the last applied action as a0.configuration (s, q), we use three separate re-current neural networks (RNN) to model theword sequence ?
?
?w?1w0, the character se-quence ?
?
?
c?1c0c1?
?
?
, and the action sequence?
?
?
a?1a0, respectively, resulting in three densereal-valued vectors {rw, rcand ra}, respectively.All the three feature vectors are used scoring theSEP action.
For APP, on the other hand, we useonly the character and action features rcand rabecause the last word w0in the buffer is a partialword.
Formally, given rw, rc, ra, the action scoresare computed by:score(SEP) = wsephsepscore(APP) = wapphappwherehsep= tanh(Wsep[rw, rc, ra] + bsep)happ= tanh(Wapp[rc, ra] + bapp)Wsep,Wapp,bsep,bapp,wsep,wappare model pa-rameters.The neural networks take the embedding formsof words, characters and actions as input, for ex-tracting rw, rcand ra, respectively.
We exploit theLSTM-RNN structure (Hochreiter and Schmidhu-ber, 1997), which can better capture non-local syn-tactic and semantic information from a sequentialinput, yet reducing gradient explosion or dimin-ishing during training.In general, given a sequence of input vectorsx0?
?
?xn, the LSTM-RNN computes a sequenceof hidden vectors h0?
?
?hn, respectively, witheach hibeing determined by the input xiand theprevious hidden vector hi?1.
A cell structure ce is423...wi...wi?1.........xwi......(a) word representation...ai...ai?1.........xai......(b) action representation...?...ci, ci?1ci...?...ci?1, ci?2ci?1...?...ci+1, ci+1ci...... .........xci...... ......(c) character representationFigure 4: Input representations of LSTMS for ra(actions) rw(words) and rc(characters).used to carry long-term memory information overthe history h0?
?
?hifor calculating hi, and infor-mation flow is controlled by an input gate ig, anoutput gate og and a forget gate fg.
Formally, thecalculation of hiusing hi?1and xiis:igi= ?
(Wigxi+ Uighi?1+ Vigcei?1+ big)fgi= ?
(Wfgxi+ Ufghi?1+ Vfgcei?1+ bfg)cei= fgicei?1+igitanh(Wcexi+ Ucehi?1+ bce)ogi= ?
(Wogxi+ Uoghi?1+ Vogcei+ bog)hi= ogitanh(cei),where U, V,W,b are model parameters, and de-notes Hadamard product.When used to calculate rw, rcand ra, the gen-eral LSTM structure above is given different inputsequences x0?
?
?xn, according to the word, char-acter and action sequences, respectively.3.1 Input representationWords.
Given a word w, we use a looking-up ma-trix Ewto obtain its embedding ew(w).
The ma-trix can be obtained through pre-training on largesize of auto segmented corpus.
As shown in Fig-ure 4(a), we use a convolutional neural layer upona two-word window to obtain ?
?
?xw?1xw0for theLSTM for rw, with the following formula:xwi= tanh(Ww[ew(wi?1), ew(wi)] + bw)Actions.
We represent an action a with an em-bedding ea(a) from a looking-up table Ea, andapply the similar convolutional neural network toobtain ?
?
?xa?1xa0for ra, as shown in Figure 4(b).Given the input action sequence ?
?
?
a?1a0, the xaiis computed by:xai= tanh(Wa[ea(ai?1), ea(ai)] + ba)Characters.
We make embeddings for both char-acter unigrams and bigrams by looking-up ma-trixes Ecand Ebc, respectively, the latter beingshown to be useful by Pei et al (2014).
Foreach character ci, the unigram embedding ec(ci)and the bigram embedding ebc(ci?1ci) are con-catenated, before being given to a CNN with aconvolution size of 5.
For the character sequence?
?
?
c?1c0c1?
?
?
of a given state (s, q), we computeits input vectors ?
?
?xc?1xc0xc1?
?
?
for the LSTM forrcby:xci= tanh(Wc[ec(ci?2)?
ebc(ci?3ci?2),?
?
?
, ec(ci)?
ebc(ci?1ci), ?
?
?
,ec(ci+2)?
ebc(ci+1ci+2)] + bc)For all the above input representations, thelooking-up tables Ew, Ea, Ec, Ebcand theweights Ww, Wa, Wc, bw, ba, bcare model pa-rameters.
For calculating rwand ra, we apply theLSTMs directly over the sequences ?
?
?xw?1xw0and?
?
?xa?1xa0for words and actions, and use the out-puts hw0and ha0for rwand ra, respectively.
Forcalculating rc, we further use a bi-directional ex-tension of the original LSTM structure.
In partic-ular, the base LSTM is applied to the input char-acter sequence both from left to right and fromright to left, leading to two hidden node sequences?
?
?hcL?1hcL0hcL1?
?
?
and ?
?
?hcR?1hcR0hcR1?
?
?
, re-spectively.
For the current character c0, hcL0andhcR0are concatenated to form the final vector rc.This is feasible because the character sequence isinput and static, and previous work has demon-strated better capability of bi-directional LSTMfor modeling sequences (Yao and Zweig, 2015).3.2 Integrating discrete featuresOur model can be extended by integrating thebaseline discrete features into the feature layer.
Inparticular,score(SEP) = w?sep(hsep?
fsep)score(APP) = w?app(happ?
fapp),where fsepand fapprepresent the baseline sparsevector for SEP and APP features, respectively, and?
denotes the vector concatenation operation.424Algorithm 2 Max-margin training with early-update.function TRAIN(c1?
?
?
cn, ag1?
?
?
agn, ?)agenda?
{ (?, c1?
?
?
cn, score=0.0) }for i in 1 ?
?
?nbeam?
{ }for cand in agendanew?
SEP(cand, ci, ?
)if {agi6= SEP} new.score += ?ADDITEM(beam, new)new?
APP(cand, ci, ?
)if {agi6= APP} new.score += ?ADDITEM(beam, new)agenda?
TOP-B(beam, B)if {ITEM(ag1?
?
?
agi) /?
agenda}?
= ??
f(BESTITEM(agenda))?
= ?
+ f(ITEM((ag1?
?
?
agi))returnif {ITEM(ag1?
?
?
agn) 6= BESTITEM(agenda)}?
= ??
f(BESTITEM(agenda))?
= ?
+ f(ITEM((ag1?
?
?
agn))4 TrainingTo train model parameters for both the discreteand neural models, we exploit online learning withearly-update as shown in Algorithm 2.
A max-margin objective is exploited,3which is defined as:L(?)
=1KK?k=1l(Agk,?)
+?2?
?
?2l(Agk,?)
= maxA(score(Ak,?)
+ ?
?
?
(Ak, Agk))?
score(Agk,?
),where ?
is the set of all parameters, {Agk}Kn=1aregold action sequences to segment the training cor-pus,Akis the model output action sequence, ?
is aregularization parameter and ?
is used to tune theloss margins.For the discrete models, f(?)
denotes the fea-tures extracted according to the feature templatesin Table 1.
For the neural models, f(?)
denotesthe corresponding hsepand happ.
Thus only theoutput layer is updated, and we further use back-propagation to learn the parameters of the otherlayers (LeCun et al, 2012).
We use online Ada-3Zhou et al (2015) find that max-margin training did notyield reasonable results for neural transition-based parsing,which is different from our findings.
One likely reason is thatwhen the number of labels is small max-margin is effective.CTB60 PKU MSRTraining#sent 23k 17k 78k#word 641k 1,010k 2,122kDevelopment#sent 2.1k 1.9k 8.7k#word 60k 100k 246kTest#sent 2.8k 1.9k 4.0k#word 82k 104k 106kTable 2: Statistics of datasets.Type hyper-parametersNetwork d(hsep) = 100, d(happ) = 80structure d(hai) = 20, d(xai) = 20d(hwi) = 50, d(xwi) = 50d(hcLi) = d(hcRi) = 50, d(xci) = 50d(ew(wi)) = 50, d(ea(ai)) = 20d(ec(ci)) = 50, d(ebc(ci?1ci)) = 50Training ?
= 10?8, ?
= 0.01, ?
= 0.2Table 3: Hyper-parameter values in our model.Grad (Duchi et al, 2011) to minimize the objec-tive function for both the discrete and neural mod-els.
All the matrix and vector parameters are ini-tialized by uniform sampling in (?0.01, 0.01).5 Experiments5.1 Experimental SettingsData.
We use three datasets for evaluation,namely CTB6, PKU and MSR.
The CTB6 corpusis taken from Chinese Treebank 6.0, and the PKUand MSR corpora can be obtained from Bake-Off 2005 (Emerson, 2005).
We follow Zhang etal.
(2014), splitting the CTB6 corpus into train-ing, development and testing sections.
For thePKU and MSR corpora, only the training and testdatasets are specified and we randomly split 10%of the training sections for development.
Table 1shows the overall statistics of the three datasets.Embeddings.
We use word2vec4to pre-trainword, character and bi-character embeddings onChinese Gigaword corpus (LDC2011T13).
In or-der to train full word embeddings, the corpus issegmented automatically by our baseline model.Hyper-parameters.
The hyper-parameter valuesare tuned according to development performances.We list their final values in Table 3.5.2 Development ResultsTo better understand the word-based neural mod-els, we perform several development experiments.All the experiments in this section are conductedon the CTB6 development dataset.4http://word2vec.googlecode.com/4255 10 15 20868890929496(a) discrete5 10 15 20b16 b8 b4 b2 b1(b) neural(-tune)5 10 15 20(c) neural(+tune)Figure 5: Accuracies against the training epochusing beam sizes 1, 2, 4, 8 and 16, respectively.5.2.1 Embeddings and beam sizeWe study the influence of beam size on the base-line and neural models.
Our neural model has twochoices of using pre-trained word embeddings.We can either fine-tune or fix the embeddings dur-ing training.
In case of fine-tuning, only words inthe training data can be learned, while embeddingsof out-of-vocabulary (OOV) words could not beused effectively.5In addition, following Dyer etal.
(2015) we randomly set words with frequency1 in the training data as the OOV words in orderto learn the OOV embedding, while avoiding over-fitting.
If the pretrained word embeddings are notfine-tuned, we can utilize all word embeddings.Figure 5 shows the development results, wherethe training curve of the discrete baseline is shownin Figure 5(a) and the curve of the neural modelwithout and with fine tuning are shown in 5(b) and5(c), respectively.
The performance increases witha larger beam size in all settings.
When the beamincreases into 16, the gains levels out.
The resultsof the discrete model and the neural model withoutfine-tuning are highly similar, showing the useful-ness of beam-search.On the other hand, with fine-tuning, the resultsare different.
The model with beam size 1 givesbetter accuracies compared to the other modelswith the same beam size.
However, as the beamsize increases, the performance increases very lit-tle.
The results are consistent with Dyer et al(2015), who find that beam-search improves theresults only slightly on dependency parsing.
Whena beam size of 16 is used, this model performs the5We perform experiments using random initialized wordembeddings as well when fine-tune is used, which is a fullysupervised model.
The performance is slightly lower.Model P R Fneural 95.21 95.69 95.45-word 91.81 92.00 91.90-character unigram 94.89 95.56 95.22-character bigram 94.93 95.53 95.23-action 95.00 95.31 95.17+discrete features96.38 96.22 96.30(combined)Table 4: Feature experiments.0.8 0.84 0.88 0.92 0.96 10.80.840.880.920.961discreteneuralFigure 6: Sentence accuracy comparisons for thediscrete and the neural models.worst compared with the discrete model and theneural model without fine-tuning.
This is likelybecause the fine-tuning of embeddings leads tooverfitting of in-vocabulary words, and underfit-ting over OOV words.
Based on the observation,we exploit fixed word embeddings in our finalmodels.5.2.2 Feature ablationWe conduct feature ablation experiments to studythe effects of the word, character unigram, charac-ter bigram and action features to the neural model.The results are shown in Table 4.
Word featuresare particularly important to the model, withoutwhich the performance decreases by 4.5%.
Theeffects of the character unigram, bigram and ac-tion features are relatively much weaker.6Thisdemonstrates that in the word-based incrementalsearch framework, words are the most crucial in-formation to the neural model.5.2.3 Integrating discrete featuresPrior work has shown the effectiveness of integrat-ing discrete and neural features for several NLPtasks (Turian et al, 2010; Wang and Manning,6In all our experiments, we fix the character unigram andbigram embeddings, because fine-tuning of these embeddingsresults in little changes.426Models P R Fword-based modelsdiscrete 95.29 95.26 95.28neural 95.34 94.69 95.01combined 96.11 95.79 95.95character-based modelsdiscrete 95.38 95.12 95.25neural 94.59 94.92 94.76combined 95.63 95.60 95.61other modelsZhang et al (2014) N/A N/A 95.71Wang et al (2011) 95.83 95.75 95.79Zhang and Clark (2011) 95.46 94.78 95.13Table 5: Main results on CTB60 test dataset.2013; Durrett and Klein, 2015; Zhang and Zhang,2015).
We investigate the usefulness of such inte-gration to our word-based segmentor on the devel-opment dataset.
We study it by two ways.
First,we compare the error distributions between thediscrete and the neural models.
Intuitively, differ-ent error distributions are necessary for improve-ments by integration.
We draw a scatter graph toshow their differences, with the (x, y) values ofeach point denoting the F-measure scores of thetwo models with respect to sentences, respectively.As shown in Figure 6, the points are rather disper-sive, showing the differences of the two models.Further, we directly look at the results after in-tegration of both discrete and neural features.
Asshown in Table 4, the integrated model improvesthe accuracies from 95.45% to 96.30%, demon-strating that the automatically-induced neural fea-tures contain highly complementary informationto the manual discrete features.5.3 Final ResultsTable 6 shows the final results on CTB6 testdataset.
For thorough comparison, we implementdiscrete, neural and combined character-basedmodels as well.7In particular, the character-baseddiscrete model is a CRF tagging model using char-acter unigrams, bigrams, trigrams and tag transi-tions (Tseng et al, 2005), and the character-basedneural model exploits a bi-directional LSTM layerto model character sequences8and a CRF layer for7The code is released for research reference under GPL athttps://github.com/SUTDNLP/NNSegmentation.8We use a concatenation of character unigram and bigramembeddings at each position as the input to LSTM, becauseour experiments show that the character bigram embeddingsare useful, without which character-based neural models aresignificantly lower than their discrete counterparts.Models PKU MSRour word-based modelsdiscrete 95.1 97.3neural 95.1 97.0combined 95.7 97.7character-based modelsdiscrete 94.9 96.8neural 94.4 97.2combined 95.4 97.2other modelsCai and Zhao (2016) 95.5 96.5Ma and Hinrichs (2015) 95.1 96.6Pei et al (2014) 95.2 97.2Zhang et al (2013a) 96.1 97.5Sun et al (2012) 95.4 97.4Zhang and Clark (2011) 95.1 97.1Sun (2010) 95.2 96.9Sun et al (2009) 95.2 97.3Table 6: Main results on PKU and MSR testdatasets.output (Chen et al, 2015b).9The combined modeluses the same method for integrating discrete andneural features as our word-based model.The word-based models achieve better perfor-mances than character-based models, since ourmodel can exploit additional word informationlearnt from large auto-segmented corpus.
We alsocompare the results with other models.
Wang etal.
(2011) is a semi-supervised model that exploitsword statistics from auto-segmented raw corpus,which is similar with our combined model in usingsemi-supervised word information.
We achieveslightly better accuracies.
Zhang et al (2014) is ajoint segmentation, POS-tagging and dependencyparsing model, which can exploit syntactic infor-mation.To compare our models with other state-of-the-art models in the literature, we report the perfor-mance on the PKU and MSR datasets also.10Ourcombined model gives the best result on the MSRdataset, and the second best on PKU.
The methodof Zhang et al (2013a) gives the best performanceon PKU by co-training on large-scale data.5.4 Error AnalysisTo study the differences between word-based andcharacter-based neural models, we conduct erroranalysis on the test dataset of CTB60.
First,9Bi-directional LSTM is slightly better than a single left-right LSTM used in Chen et al (2015b).10The results of Chen et al (2015a) and Chen et al (2015b)are not listed, because they take a preprocessing step by re-placing Chinese idioms with a uniform symbol in their testdata.4270.8 0.84 0.88 0.92 0.96 10.80.840.880.920.961wordcharacterFigure 7: Sentence accuracy comparisons forword- and character-based neural models.5 10 15 20 25 30 35 40 45 50+92949698F-measures(%)word characterFigure 8: F-measure against character length.we examine the error distribution on individualsentences.
Figure 7 shows the F-measure val-ues of each test sentence by word- and character-based neural models, respectively, where the x-axis value denotes the F-measure value of theword-based neural model, and the y-axis value de-notes its performance of the character-based neu-ral model.
We can see that the majority scat-ter points are off the diagonal line, demonstratingstrong differences between the two models.
Thisresults from the differences in feature sources.Second, we study the F-measure distributionof the two neural models with respect to sen-tence lengths.
We divide the test sentences intoten bins, with bin i denoting sentence lengths in[5 ?
(i?
1), 5 ?
i].
Figure 8 shows the results.
Ac-cording to the figure, we observe that word-basedneural model is relatively weaker for sentenceswith length in [5, 10], while can better tackle longsentences.Third, we compare the two neural models bytheir capabilities of modeling words with differentlengths.
Figure 9 shows the results.
The perfor-1 2 3 4+7580859095Figure 9: F-measure against word length, wherethe boxes with red dots denote the performances ofword-based neural model, and the boxes with blueslant lines denote character-based neural model.mances are lower for words with lengths beyond 2,and the performance drops significantly for wordswith lengths over 3.
Overall, the word-based neu-ral model achieves comparable performances withthe character-based model, but gives significantlybetter performances for long words, in particularwhen the word length is over 3.
This demonstratesthe advantage of word-level features.6 Related WorkXue (2003) was the first to propose a character-tagging method to Chinese word segmentation, us-ing a maximum entropy model to assign B/I/E/Stags to each character in the input sentence sepa-rately.
Peng et al (2004) showed that better resultscan be achieved by global learning using a CRFmodel.
This method has been followed by mostsubsequent models in the literature (Tseng et al,2005; Zhao, 2009; Sun et al, 2012).
The mosteffective features have been character unigrams,bigrams and trigrams within a five-character win-dow, and a bigram tag window.
Special characterssuch as alphabets, numbers and date/time charac-ters are also differentiated for extracting features.Zheng et al (2013) built a neural network seg-mentor, which essentially substitutes the manualdiscrete features of Peng et al (2004), with densereal-valued features induced automatically fromcharacter embeddings, using a deep neural net-work structure (Collobert et al, 2011).
A tag tran-sition matrix is used for inference, which makesthe model effectively.
Most subsequent work onneural segmentation followed this method, im-proving the extraction of emission features by us-ing more complex neural network structures.Mansur et al (2013) experimented with embed-dings of richer features, and in particular charac-428ter bigrams.
Pei et al (2014) used a tensor neu-ral network to achieve extensive feature combi-nations, capturing the interaction between charac-ters and tags.
Chen et al (2015a) used a recur-sive network structure to the same end, extract-ing more combined features to model complicatedcharacter combinations in a five-character win-dow.
Chen et al (2015b) used a LSTM model tocapture long-range dependencies between charac-ters in a sentence.
Xu and Sun (2016) proposed adependency-based gated recursive neural networkto efficiently integrate local and long-distance fea-tures.
The above methods are all character-basedmodels, making no use of full word information.In contrast, we leverage both character embed-dings and word embeddings for better accuracies.For word-based segmentation, Andrew (2006)used a semi-CRF model to integrate word fea-tures, Zhang and Clark (2007) used a percep-tron algorithm with inexact search, and Sun etal.
(2009) used a discriminative latent variablemodel to make use of word features.
Recently,there have been several neural-based models us-ing word-level embedding features (Morita et al,2015; Liu et al, 2016; Cai and Zhao, 2016), whichare different from our work in the basic frame-work.
For instance, Liu et al (2016) follow An-drew (2006) using a semi-CRF for structured in-ference.We followed the global learning and beam-search framework of Zhang and Clark (2011) inbuilding a word-based neural segmentor.
Themain difference between our model and that ofZhang and Clark (2011) is that we use a neuralnetwork to induce feature combinations directlyfrom character and word embeddings.
In addi-tion, the use of a bi-directional LSTM allows us toleverage non-local information from the word se-quence, and look-ahead information from the in-coming character sequence.
The automatic neu-ral features are complementary to the manual dis-crete features of Zhang and Clark (2011).
Weshow that our model can accommodate the inte-gration of both types of features.
This is similar inspirit to the work of Sun (2010) and Wang et al(2014), who integrated features of character-basedand word-based segmentors.Transition-based framework with beam searchhas been widely exploited in a number of otherNLP tasks, including syntactic parsing (Zhang andNivre, 2011; Zhu et al, 2013), information ex-traction (Li and Ji, 2014) and the work of jointmodels (Zhang et al, 2013b; Zhang et al, 2014).Recently, the effectiveness of neural features hasbeen studied for this framework.
In the naturallanguage parsing community, it has achieved greatsuccess.
Representative work includes Zhou et al(2015), Weiss et al (2015), Watanabe and Sumita(2015) and Andor et al (2016).
In this work,we apply the transition-based neural framework toChinese segmentation, in order to exploit word-level neural features such as word embeddings.7 ConclusionWe proposed a word-based neural model for Chi-nese segmentation, which exploits not only char-acter embeddings as previous work does, but alsoword embeddings pre-trained from large scalecorpus.
The model achieved comparable per-formances compared with a discrete word-basedbaseline, and also the state-of-the-art character-based neural models in the literature.
We fur-ther demonstrated that the model can utilize dis-crete features conveniently, resulting in a com-bined model that achieved top performances com-pared with previous work.
Finally, we conductedseveral comparisons to study the differences be-tween our word-based model with character-basedneural models, showing that they have different er-ror characteristics.AcknowledgmentsWe thank the anonymous reviewers, Yijia Liu andHai Zhao for their constructive comments, whichhelp to improve the final paper.
This work is sup-ported by National Natural Science Foundationof China (NSFC) under grant 61170148, Natu-ral Science Foundation of Heilongjiang Province(China) under grant No.F2016036, the SingaporeMinistry of Education (MOE) AcRF Tier 2 grantT2MOE201301 and SRG ISTD 2012 038 fromSingapore University of Technology and Design.Yue Zhang is the corresponding author.ReferencesDaniel Andor, Chris Alberti, David Weiss, AliakseiSeveryn, Alessandro Presta, Kuzman Ganchev, SlavPetrov, and Michael Collins.
2016.
Globally nor-malized transition-based neural networks.
In Pro-ceedings of the ACL 2016.Galen Andrew.
2006.
A hybrid markov/semi-markovconditional random field for sequence segmentation.429In Proceedings of the 2006 Conference on EMNLP,pages 465?472, Sydney, Australia, July.Deng Cai and Hai Zhao.
2016.
Neural word segmen-tation learning for Chinese.
In Proceedings of ACL2016.Xinchi Chen, Xipeng Qiu, Chenxi Zhu, and XuanjingHuang.
2015a.
Gated recursive neural network forchinese word segmentation.
In Proceedings of the53nd ACL, pages 1744?1753, July.Xinchi Chen, Xipeng Qiu, Chenxi Zhu, Pengfei Liu,and Xuanjing Huang.
2015b.
Long short-termmemory neural networks for chinese word segmen-tation.
In Proceedings of the 2015 EMNLP, pages1197?1206, September.Michael Collins and Brian Roark.
2004.
Incremen-tal parsing with the perceptron algorithm.
In Pro-ceedings of the 42nd Meeting of the Association forComputational Linguistics (ACL?04), Main Volume,pages 111?118, Barcelona, Spain, July.R.
Collobert, J. Weston, L. Bottou, M. Karlen,K.
Kavukcuoglu, and P. Kuksa.
2011.
Natural lan-guage processing (almost) from scratch.
Journal ofMachine Learning Research, 12:2493?2537.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learningand stochastic optimization.
The Journal of Ma-chine Learning Research, 12:2121?2159.Greg Durrett and Dan Klein.
2015.
Neural crf pars-ing.
In Proceedings of the 53nd ACL, pages 302?312, July.Chris Dyer, Miguel Ballesteros, Wang Ling, AustinMatthews, and Noah A. Smith.
2015.
Transition-based dependency parsing with stack long short-term memory.
In Proceedings of the 53nd ACL,pages 334?343, July.Thomas Emerson.
2005.
The second international chi-nese word segmentation bakeoff.
In Proceedingsof the Second SIGHAN Workshop on Chinese Lan-guage Processing, pages 123?133.Sepp Hochreiter and J?urgen Schmidhuber.
1997.Long short-term memory.
Neural computation,9(8):1735?1780.Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin-ton.
2012.
Imagenet classification with deep con-volutional neural networks.
In Advances in neuralinformation processing systems, pages 1097?1105.Yann A LeCun, L?eon Bottou, Genevieve B Orr, andKlaus-Robert M?uller.
2012.
Efficient backprop.
InNeural networks: Tricks of the trade, pages 9?48.Springer.Qi Li and Heng Ji.
2014.
Incremental joint extractionof entity mentions and relations.
In Proceedings ofthe ACL 2014.Yijia Liu, Wanxiang Che, Jiang Guo, Bing Qin, andTing Liu.
2016.
Exploring segment representationsfor neural segmentation models.
In Proceedings ofIJCAI 2016.Jianqiang Ma and Erhard Hinrichs.
2015.
Accuratelinear-time chinese word segmentation via embed-ding matching.
In Proceedings of the 53nd ACL,pages 1733?1743, July.Mairgup Mansur, Wenzhe Pei, and Baobao Chang.2013.
Feature-based neural language model andchinese word segmentation.
In Proceedings ofthe Sixth International Joint Conference on NaturalLanguage Processing, pages 1271?1277, Nagoya,Japan, October.
Asian Federation of Natural Lan-guage Processing.Hajime Morita, Daisuke Kawahara, and Sadao Kuro-hashi.
2015.
Morphological analysis for unseg-mented languages using recurrent neural networklanguage model.
In Proceedings of the 2015 Con-ference on EMNLP, pages 2292?2297.Wenzhe Pei, Tao Ge, and Baobao Chang.
2014.
Max-margin tensor neural network for chinese word seg-mentation.
In Proceedings of the 52nd ACL, pages293?303, Baltimore, Maryland, June.Fuchun Peng, Fangfang Feng, and Andrew McCallum.2004.
Chinese segmentation and new word detec-tion using conditional random fields.
In Proceedingsof Coling 2004, pages 562?568, Geneva, Switzer-land, Aug 23?Aug 27.Yanxin Shi and Mengqiu Wang.
2007.
A dual-layercrfs based joint decoding method for cascaded seg-mentation and labeling tasks.
In IJCAI, pages 1707?1712.Weiwei Sun and Jia Xu.
2011.
Enhancing chineseword segmentation using unlabeled data.
In Pro-ceedings of the 2011 Conference on EMNLP, pages970?979, July.Xu Sun, Yaozhong Zhang, Takuya Matsuzaki, Yoshi-masa Tsuruoka, and Jun?ichi Tsujii.
2009.
A dis-criminative latent variable chinese segmenter withhybrid word/character information.
In Proceedingsof NAACL 2009, pages 56?64, June.Xu Sun, Houfeng Wang, and Wenjie Li.
2012.
Fast on-line training with frequency-adaptive learning ratesfor chinese word segmentation and new word detec-tion.
In Proceedings of the 50th ACL, pages 253?262, July.Weiwei Sun.
2010.
Word-based and character-basedword segmentation models: Comparison and combi-nation.
In Coling 2010: Posters, pages 1211?1219,August.Huihsin Tseng, Pichuan Chang, Galen Andrew, DanielJurafsky, and Christopher Manning.
2005.
A condi-tional random field word segmenter for sighan bake-off 2005.
In Proceedings of the fourth SIGHANworkshop, pages 168?171.430Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.2010.
Word representations: A simple and generalmethod for semi-supervised learning.
In Proceed-ings of the 48th Annual Meeting of the Associationfor Computational Linguistics, pages 384?394, July.Mengqiu Wang and Christopher D. Manning.
2013.Effect of non-linear deep architecture in sequencelabeling.
In Proceedings of the Sixth InternationalJoint Conference on Natural Language Processing,pages 1285?1291, Nagoya, Japan, October.
AsianFederation of Natural Language Processing.Yiou Wang, Jun?ichi Kazama, Yoshimasa Tsuruoka,Wenliang Chen, Yujie Zhang, and Kentaro Tori-sawa.
2011.
Improving chinese word segmenta-tion and pos tagging with semi-supervised methodsusing large auto-analyzed data.
In Proceedings of5th IJCNLP, pages 309?317, Chiang Mai, Thailand,November.Mengqiu Wang, Rob Voigt, and Christopher D. Man-ning.
2014.
Two knives cut better than one: Chi-nese word segmentation with dual decomposition.In Proceedings of the 52nd ACL, pages 193?198,Baltimore, Maryland, June.Taro Watanabe and Eiichiro Sumita.
2015.
Transition-based neural constituent parsing.
In Proceedings ofthe 53rd ACL, pages 1169?1179, July.David Weiss, Chris Alberti, Michael Collins, and SlavPetrov.
2015.
Structured training for neural networktransition-based parsing.
In Proceedings of the 53rdACL, pages 323?333, July.Jingjing Xu and Xu Sun.
2016.
Dependency-basedgated recursive neural network for chinese word seg-mentation.
In Proceedings of ACL 2016.Nianwen Xue.
2003.
Chinese word segmentation ascharacter tagging.
International Journal of Compu-tational Linguistics and Chinese Language Process-ing, 8(1).Kaisheng Yao and Geoffrey Zweig.
2015.Sequence-to-sequence neural net models forgrapheme-to-phoneme conversion.
arXiv preprintarXiv:1506.00196.Yue Zhang and Stephen Clark.
2007.
Chinese seg-mentation with a word-based perceptron algorithm.In Proceedings of the 45th ACL, pages 840?847,Prague, Czech Republic, June.Yue Zhang and Stephen Clark.
2011.
Syntactic pro-cessing using the generalized perceptron and beamsearch.
Computational Linguistics, 37(1):105?151.Yue Zhang and Joakim Nivre.
2011.
Transition-baseddependency parsing with rich non-local features.
InProceedings of the 49th ACL, pages 188?193, June.Meishan Zhang and Yue Zhang.
2015.
Combin-ing discrete and continuous features for determin-istic transition-based dependency parsing.
In Pro-ceedings of the 2015 EMNLP, pages 1316?1321,September.Longkai Zhang, Houfeng Wang, Xu Sun, and MairgupMansur.
2013a.
Exploring representations from un-labeled data with co-training for Chinese word seg-mentation.
In Proceedings of the EMNLP 2013,pages 311?321, Seattle, Washington, USA, October.Meishan Zhang, Yue Zhang, Wanxiang Che, and TingLiu.
2013b.
Chinese parsing exploiting characters.In Proceedings of the 51st ACL, pages 125?134, Au-gust.Meishan Zhang, Yue Zhang, Wanxiang Che, and TingLiu.
2014.
Character-level chinese dependencyparsing.
In Proceedings of the 52nd ACL, pages1326?1336, Baltimore, Maryland, June.Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-LiangLu.
2006.
Effective tag set selection in chinese wordsegmentation via conditional random field model-ing.
In Proceedings of PACLIC, volume 20, pages87?94.
Citeseer.Hai Zhao.
2009.
Character-level dependencies in chi-nese: Usefulness and learning.
In Proceedings ofthe EACL, pages 879?887, Athens, Greece, March.Xiaoqing Zheng, Hanyang Chen, and Tianyu Xu.2013.
Deep learning for Chinese word segmentationand POS tagging.
In Proceedings of the 2013 Con-ference on EMNLP, pages 647?657, Seattle, Wash-ington, USA, October.Hao Zhou, Yue Zhang, Shujian Huang, and JiajunChen.
2015.
A neural probabilistic structured-prediction model for transition-based dependencyparsing.
In Proceedings of the 53rd ACL, pages1213?1222, July.Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang,and Jingbo Zhu.
2013.
Fast and accurate shift-reduce constituent parsing.
In Proceedings of the51st ACL, pages 434?443, August.431
