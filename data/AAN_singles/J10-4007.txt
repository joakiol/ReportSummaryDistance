A Flexible, Corpus-Driven Model of Regularand Inverse Selectional PreferencesKatrin Erk?University of Texas at AustinSebastian Pad??
?Heidelberg UniversityUlrike Pad?
?Vico Research and Consulting GmbHWe present a vector space?based model for selectional preferences that predicts plausibilityscores for argument headwords.
It does not require any lexical resources (such as WordNet).
Itcan be trained either on one corpus with syntactic annotation, or on a combination of a smallsemantically annotated primary corpus and a large, syntactically analyzed generalization cor-pus.
Our model is able to predict inverse selectional preferences, that is, plausibility scores forpredicates given argument heads.We evaluate our model on one NLP task (pseudo-disambiguation) and one cognitive task(prediction of human plausibility judgments), gauging the influence of different parameters andcomparing our model against other model classes.
We obtain consistent benefits from using thedisambiguation and semantic role information provided by a semantically tagged primary cor-pus.
As for parameters, we identify settings that yield good performance across a range of experi-mental conditions.
However, frequency remains a major influence of prediction quality, andwe also identify more robust parameter settings suitable for applications with many infrequentitems.1.
IntroductionSelectional preferences or selectional constraints describe knowledge about possibleand plausible fillers for a predicate?s argument positions.
They model the fact that thereis often a semantically coherent set of concepts that can fill a given argument posi-tion.
Selectional preferences can help for many text analysis tasks which involve com-paring different attachment decisions.
Examples include syntactic disambiguation(Hindle and Rooth 1993; Toutanova et al 2005), word sense disambiguation (WSD,?
Department of Linguistics, Calhoun Hall 512, 1 University Station B 5100, Austin, TX 78712.E-mail: katrin.erk@mail.utexas.edu.??
E-mail: pado@cl.uni-heidelberg.de.?
E-mail: ulrike.pado@vico-research.com.Submission received: 26 November 2009; revised submission received: 6 May 2010; accepted for publication:29 June 2010.
Part of the work reported in this article was done while S. P. was a postdoctoral scholar and U. P.a visiting scholar at Stanford University.?
2010 Association for Computational LinguisticsComputational Linguistics Volume 36, Number 4McCarthy and Carroll 2003), semantic role labeling (SRL, Gildea and Jurafsky 2002), andcharacterizing the conditions under which entailment holds between two predicates(Zanzotto, Pennacchiotti, and Pazienza 2006; Pantel et al 2007).
Furthermore, selec-tional preferences are also helpful for determining linguistic properties of predicatesand predicate?argument combinations, for example in compositionality assessment(McCarthy, Venkatapathy, and Joshi 2007) or the detection of diathesis alternations(McCarthy 2000).
In psycholinguistics, selectional preferences predict human plausibil-ity judgments for predicate?argument combinations (Resnik 1996) and effects in humansentence reading times (Pad?, Crocker, and Keller 2009).All these applications rely on the availability of broad-coverage, reliable selectionalpreferences for predicates and their argument positions.
Given the immense effort nec-essary for manual semantic lexicon building and its associated reliability problems (see,e.g., Briscoe and Boguraev 1989), all contemporary models of selectional preferencesacquire selectional preferences automatically from large corpora.The simplest strategy is to extract triples (v, r, a) of a predicate, role, and argumentheadword (or filler) from a corpus, and then to compute selectional preference asrelative frequencies.
However, due to the Zipfian nature of word frequencies, the firststep on its own results in a very sparse list of headwords, in particular for less frequentpredicates.
As an example, the verb anglicize only appears with nine direct objects inthe 100-million word British National Corpus (BNC, Burnard 1995).
Only one of them,name, appears more than once.
Many highly plausible fillers are missing from the list,such as word or spelling.In order to make sensible predictions for triples that are unseen at training time,it is crucial to add a generalization step that infers a degree of preference for new,unseen headwords for a given predicate and role.1 The result is, in the ideal case, anassignment to every possible headword of some degree of compatibility (or plausibil-ity) with the predicate?s preferences.
In the case of anglicize, the desired result would bea high plausibility for words like the (previously seen) wordlist and surname as well asthe (unseen) word and spelling, and a low plausibility for (likewise unseen) words likecow and machine.The predominant approach to generalizing over headwords, first introduced byResnik (1996), is based on semantic hierarchies such as WordNet (Miller et al 1990).
Theidea is to map all observed headwords onto synsets, and then generalize to a characteri-zation of the selectional preference in terms of the WordNet noun hierarchy.
This can beachieved in many different ways (Abe and Li 1996; Resnik 1996; Ciaramita and Johnson2000; Clark and Weir 2001).
The performance of these models relies on the coverageof the lexical resources, which can be a problem even for English (Gildea and Jurafsky2002).
An alternative approach to generalization uses co-occurrence information, eitherin the form of distributional models or through a clustering approach.
These models,which avoid dependence on lexical resources, use corpus data for generalization(Dagan, Lee, and Pereira 1999; Rooth et al 1999; Bergsma, Lin, and Goebel 2008).In this article, we present a lightweight model for the acquisition and representa-tion of selectional preferences.
Our model is fully distributional and does not requireany knowledge sources beyond a large corpus where subjects and objects can be iden-tified with reasonable accuracy.
Its key point is to use vector space similarity (Lundand Burgess 1996; Laundauer and Dumais 1997) to generalize from seen to unseen1 Some approaches also fix a role and headword list and generalize from seen predicates to other, similarpredicates.724Erk, Pad?, and Pad?
A Flexible, Corpus-Driven Model of Selectional Preferencesheadwords.
The vector space representations which serve as a basis for computingsimilarity can in principle be computed from any arbitrary corpus, given that it is largeenough.
In particular, this need not be the same corpus as the one on which we observepredicate?headword co-occurrences.
Our model thus distinguishes between a primarycorpus, from which the predicate?role?headword triples are extracted, and a generali-zation corpus for computing the vector space representations.
This distinction makesit possible to apply our model to primary corpora with rich information that are toosmall for efficient generalization, such as domain-specific corpora or corpora withdeeper linguistic analysis, as long as a larger, even if potentially noisier, generalizationcorpus is available.
We empirically demonstrate the benefit of this distinction.
We useFrameNet (Fillmore, Johnson, and Petruck 2003) as primary corpus and the BNC asgeneralization corpus, modeling selectional preferences for semantic roles with near-perfect coverage and low error rate.2We evaluate our model on two tasks.
The first task is pseudo-disambiguation(Yarowsky 1993), where the model decides which of two randomly chosen words is abetter filler for the given argument position.
This task tests model properties that areneeded for concrete semantic analysis tasks, most notably word sense disambiguation,but also for semantic role labeling.
The second task is the prediction of humanplausibility ratings, which is a standard task-independent benchmark for the qualityof selectional preferences.
We test our model across a range of parameter settings toidentify best-practice values and show that it robustly outperforms both WordNet-based and other distributional models on both tasks.Finally, we investigate inverse preferences, that is, preferences that argumentshave for their predicates.
Although there is ample cognitive evidence for the existenceof such preferences (e.g., McRae et al 2005), to our knowledge, they have not been in-vestigated systematically in linguistics.
However, statistics about inverse preferenceshave been used implicitly in computational linguistics (e.g., Hindle 1990; Rooth et al1999).
We investigate the properties of inverse selectional preferences in comparison toregular selectional preferences, and show that it is possible to predict inverse prefer-ences with our selectional preference model as well.The model that we discuss in this article, EPP, was first introduced in Erk (2007)(using a pseudo-disambiguation task for evaluation) and further studied by Pad?, Pad?,and Erk (2007) (evaluating against human plausibility judgments).
In the current text,we perform a more extensive evaluation and analysis, including the new evaluation oninverse preferences, and we introduce a new similarity measure, nGCM, which achievesexcellent performance in many settings.2.
Computational Models of Selectional PreferencesIn this section, we provide an overview of corpus-based models of selectional prefer-ences.
See Table 1 for a summary of the notation that we use.2 As descriptions of semantic classes of participants in events, selectional preferences are most naturallyapplied to semantic argument positions, that is, semantic roles (such as agent or patient).
In contrast,syntactic argument positions (like subject and object) can comprise several semantic argument positions,due to the presence of diathesis alternations, and thus show less consistent selectional preferences.Nevertheless, work in computational linguistics also makes use of selectional preferences for syntacticargument positions, considering them noisy approximations of semantic argument positions.725Computational Linguistics Volume 36, Number 4Table 1Notation used throughout the article.w ?
Lemmas Word.
We assume lemmatization throughout.v ?
Preds Predicate.
Preds may be a subset of Lemmas, or a set ofsemantic classes.r ?
Roles Role/Argument slot.
Roles may be a set of grammaticalfunctions, or of semantic roles.a ?
Args ?
Lemmas (Potential) argument headword.c ?
C Semantic class on which selectional preferences areconditioned, for example, WordNet sense, FrameNet frame,or latent semantic class.VS = (DTrans,Basis, sim, STrans)Vector space.
Basis is a set of basis elements, sima similaritymeasure, DTrans a transformation of raw counts, and STransa transformation of the space.We write w = ?wb1 , .
.
.
, wbn?
for the representation of w ?Lemmas in a vector space with Basis = {b1, .
.
.
, bn}.wtr,v(a) Weight of argument headword a for predicate v and role r.2.1 Historical ModelsIn formal linguistics, selectional restrictions were employed as strict Boolean restrictionsby Katz and Postal (Katz and Fodor 1963; Katz and Postal 1964) as input to a mutual dis-ambiguation process between predicates and their modifiers.
Sentences are semanticallyanomalous if there are no mutually consistent readings for the two words.
Semanticallyanomalous sentences would receive no reading, whereas ambiguous sentences wouldreceive several readings.The strict dismissal as meaningless of sentences that violate selectional restrictionswas later criticized.
A case in point is metaphors, which often combine predicates andarguments from different domains (Lakoff and Johnson 1980).
Wilks (1975:329) statedthat ?rejecting utterances is just what humans do not.
They try to understand them.
?He proposes to reconceptualize selectional restrictions as preferences whose violationis dispreferred, but not fatal.
His proposal for a semantic interpretation mechanism stilluses semantic primitives, but always produces a single most plausible interpretation bychoosing the senses of each word that maximize the compatibility between selectionalpreferences and semantic types.
In this manner, he is able to compute semantic repre-sentations for sentences that violate selectional restrictions, including metaphors suchas ?my car drinks gasoline.
?2.2 Semantic Hierarchy?Based ModelsThe first broad-coverage computational model of selectional preferences, and still oneof the best-known ones, namely that of Resnik (1996), belongs to the class of semantichierarchy?based models.
These models generalize over observed headwords using asemantic hierarchy or ontology for nouns.
The two main advantages of such models arethat (a) they can make predictions for all words covered by the hierarchy, even for veryinfrequent ones for which distributional representations tend to be unreliable; and (b)the hierarchy robustly guides generalization even for few observed headwords.Resnik?s model instantiates the set of relations Roles with grammatical functionswhich can be observed in syntactically analyzed corpora.
More specifically, it concen-726Erk, Pad?, and Pad?
A Flexible, Corpus-Driven Model of Selectional Preferencestrates on selectional preferences for subjects and objects.
For the generalization step,Resnik?s model maps all headwords onto WordNet synsets (or classes) c. Resnik firstcomputes the overall selectional preference strength for each verb?relation pair (v, r),that is, the degree to which the pair constrains possible fillers.
To estimate this quantity,the distribution of WordNet synsets for this particular verb?relation pair is comparedto the distribution of synsets over all verbs, given the relation r. Technically, this isachieved using Kullback?Leibler divergence:SelStr(v, r) = D(P(c|v, r)||P(c|r)) =?c?CP(c|v, r)log(P(c|v, r)P(c|r) ) (1)The parameters P(c|v, r) and P(c|r) are estimated from the corpus frequencies of tuples(v, r, a) and the membership of nouns a in WordNet classes c: The observed frequencyof (v, r, a) is split equally among all WordNet classes for a.
This avoids word sensedisambiguration, but incurs a certain share of wrong attributions.
The intuition ofSelStr(v, r) is that a verb?relation pair that only allows a limited range of argument headswill have a posterior distribution over classes that strongly diverges from the prior.Next, the selectional association of the triple, SelAssoc(v, r, c), is computed as theratio of the selectional preference strength for this particular class c to the overall selec-tional preference strength of the verb?relation pair (v, r).
This is shown in Equation (2).SelAssoc(v, r, c) =P(c|v, r)logP(c|v,r)P(c|r)SelStr(v, r)(2)Finally, the selectional preference between a verb, a relation, and an argument headis defined as the maximal selectional association of the verb, the relation, and anyWordNet class c that the argument can instantiate.
We will refer to this model asRESNIK herein.In subsequent years, a number of WordNet-based models were developed thatdiffer from Resnik?s model in the details of how the generalization in the WordNethierarchy is performed.
Abe and Li (1996) characterize selectional preferences by atree cut through the WordNet noun hierarchy that minimizes tree cut length whilemaximizing accuracy of prediction.
Clark and Weir (2001) perform generalization byascending the WordNet noun hierarchy as long as the degree of selectional preferenceamong siblings is not significantly different.
Ciaramita and Johnson (2000) encodeWordNet in a Bayesian Network to take advantage of the Bayes nets?
ability to ?ex-plain away?
ambiguity.
Grishman and Sterling (1992) perform generalization on thebasis of a manually constructed semantic hierarchy specifically developed on the samecorpus.2.3 Distributional ModelsDistributional models do not make use of any lexicon resource for the generalizationstep.
Instead, they use word co-occurrence?typically obtained from the same corpusas the observed headwords?for generalization.
This independence from manuallyconstructed resources gives distributional models a good cost?benefit ratio and makesthem especially attractive for domain-specific applications.
These models, like the727Computational Linguistics Volume 36, Number 4semantic hierarchy?based models, usually use grammatical functions as the set Rolesfor which selectional preferences are predicted.Pereira, Tishby, and Lee (1993) and Rooth et al (1999) generalize by discoveringlatent classes of noun?verb pairs with soft clustering.
They model the probability ofa word a as the argument of a predicate v as the probability of generating v and aindependently from the latent classes c:P(v, a) =?c?CP(c, v, a) =?c?CP(c)P(v|c)P(a|c) (3)Pereira, Tishby, and Lee (1993) develop a task-specific procedure to optimize P(c),P(v|c), and P(a|c).
Their procedure supports hierarchical clustering and can optimizethe number of clusters.
Rooth et al (1999) present a simpler Expectation Maximization?based estimation procedure which takes the number of clusters as input parameter.
Werefer to this model as ROOTH ET AL.
herein.Dagan, Lee, and Pereira (1999) introduce a general model for computing co-occurrence probabilities with similarity-based smoothing.
Although not intended as amodel of selectional preferences, it can also be interpreted as such.
Given a similaritymeasure sim defined on word pairs, they compute the smoothed occurrence probabilityof a word w2 given w1 asPsim(w2|w1) =?w?Simset(w1)sim(w1, w)Z(w1)P(w2|w) (4)where Simset(w) is the set of words most similar to w according to sim, and Z(w1) =?w?Simset(w1) sim(w1, w) is a normalizing factor.
This model predicts w2 given w1 bybacking off from w1 to words w similar to w1.
The contribution of each w in predictingP(w2|w1) is weighted by sim(w1, w).
The similarity sim(w1, w) is computed on vectorspace representations.Recently, Bergsma, Lin, and Goebel (2008) have adopted a discriminative ap-proach to the prediction of selectional preferences.
The features they use are mainly co-occurrence statistics, enriched with morphological context features to alleviate sparsedata problems for low-frequency argument heads.
They train one SVM per verb?argument position pair, using unobserved verb?argument combinations as negativeexamples, which makes their approach independent of manually annotated trainingdata.
Schulte im Walde et al (2008) present a model that combines features of thesemantic hierarchy?based and the distributional approaches by integrating WordNetinto an EM-based clustering model; Schulte im Walde (2010) shows that integratingnoun?modifier relations improves the prediction of human plausibility judgments.2.4 Semantic Role?Based ModelsThe third class of models takes advantage of semantic resources beyond simple seman-tic hierarchies, notably of corpora with semantic role annotation.
Such corpora allow theprediction of selectional preferences for semantic roles rather than grammatical func-tions.
From a linguistic perspective, semantic roles represent a more appropriate levelfor defining selectional preferences.
For that reason, the role annotation provides cleanerand more specific training data than even a manually syntactically annotated corpus728Erk, Pad?, and Pad?
A Flexible, Corpus-Driven Model of Selectional Preferenceswould.
These advantages, however, come at the cost of considerably greater sparsityissues.Pad?, Crocker, and Keller (2009) present a model based on FrameNet (Fillmore,Johnson, and Petruck 2003).
This model estimates selectional preferences with a gen-erative probability model that equates the plausibility of a (v, r, a) triple with the jointprobability of observing the thematic role r, the verb v, and the argument a, plus theverb?s FrameNet sense c and the grammatical function gf of the argument.
This jointprobability can be decomposed using the chain rule:P(v, c, r, gf , a) = P(v)P(c|v)P(r|v, c)P(gf |r, v, c)P(a|gf , r, v, c) (5)The model does not make any independence assumptions.
To counteract sparse dataissues for the more complex terms, the model applies WordNet-based generalization(for nouns), distributional clustering (for verbs), and Good?Turing smoothing.
We referto this model as PADO ET AL.
Another semantic role?based model was proposed byVandekerckhove, Sandra, and Daelemans (2009).
It acquires selectional preferences forPropBank roles from a PropBank-labeled corpus, generalizing to unseen headwordswith memory-based learning.3.
A Distributional Exemplar-Based Model of Selectional Preferences: EPPWe now present the EPP model of selectional preferences.
It falls into the category ofdistributional models.
More specifically, it is an exemplar model that remembers allseen headwords for a given argument position and computes the degree of plausibilityfor a new headword candidate through its similarity to the stored exemplars.
Exemplarsare modeled as vectors in a semantic space.Exemplar models are a well-known modeling framework that is used in psychol-ogy (Nosofsky 1986), in computational linguistics (under the name of memory-basedlearning [Daelemans and van der Bosch 2005]), and in linguistics, particularly phonet-ics (Hay, Nolan, and Drager 2006).
The appeal of exemplar models is that they providea cognitively plausible process of learning as storing exemplars, and categorization assimilarity computation that is grounded in features of the exemplars (e.g., formants inphonetics, and contexts in lexical semantics).The representation of selectional preferences through feature vectors also fits in wellwith work in psycholinguistics by McRae, Ferretti, and Amyote (1997), who studied thecharacterization of verb selectional preferences through features elicited from humansubjects.
They found high overlap between features used to characterize the selectionalpreferences on the one hand, and features listed for typical role fillers on the other hand.For example, features generated for the agent role of frighten include mean, scary, andugly, features that were also highly relevant for the typical filler noun monster.As briefly mentioned in Section 1, we consider selectional preferences to be charac-terizations of typical fillers for the semantic roles of a predicate.
Still, we keep our modelmodular to different notions of argumenthood, such that it is also applicable to thecomputation of selectional preferences for syntactic dependents of a predicate, as this isan important case for computational applications.
When we compute selectional prefer-ences for syntactic dependents rather than semantic roles, we view syntactic argumentpositions as noisy approximations of semantic roles.729Computational Linguistics Volume 36, Number 43.1 The ModelAs stated previously, we assume that we have two corpora which assume different func-tions in the model: the primary corpus, which provides information about predicate?argument co-occurrences but may be too sparse for generalization; and the large, butpotentially noisy, generalization corpus, from which we obtain reliable semantic simi-larity estimates.Thus, the first step is the extraction of triples (v, r, a) of a predicate v ?
Preds, arelation r ?
Roles, and a headword a ?
Args from the primary corpus.
Let Seenargs(r, v)be the set of argument headwords seen with an argument position r of a predicate vin the primary corpus.
Given these triples, we predict the plausibility for an arbitrarynoun a0 in position (v, r) through the semantic similarity of a0 to all the membersof Seenargs(r, v).
We obtain these similarity ratings by first computing vector spacerepresentations for both and the members of seen(r, v) from the generalization corpus,and then using a standard vector space similarity measure.
We compute the plausibilityfor a0 asSelprefEPPr,v(a0) =?a?Seenargs(r,v)wtr,v(a)Zr,v?
sim(a0, a) (6)where sim(a0, a) is the similarity between the vector space representations of a0 anda, wtr,v(a) a weight for the seen headword a, and Zr,v a normalization constant, Zr,v =?a?Seenargs(r,v) wtr,v(a), so that the number of observed exemplars for each (v, r) pair doesnot matter.
Because SelprefEPP is basically a weighted average over similarity values, therange of SelprefEPP is identical to the range of the employed similarity function sim.
Forexample, the range is [?1, 1] for cosine similarity, or [0, 1] for the Jaccard coefficient (cf.Section 3.3).
We discuss possible choices of both the similarity sim and the weight wtr,vin Section 3.3.3.2 Vector Space RepresentationsWe use vector space representations for generalization.
In a vector space model, eachtarget word is represented as a vector, typically constructed from co-occurrence countswith context words in a large corpus (the so-called basis elements).
The underlyingassumption, which goes back to Firth (1957) and Harris (1968), is that words with similarmeanings occur in similar contexts and will be assigned similar vectors.
Thus, thedistance between the vectors of two target words, as given by some distance measure(e.g., Cosine or Jaccard), reflects their semantic similarity.Vector space models are simple to construct, and the semantic similarity they pro-vide has found a wide range of applications.
Examples in NLP include informationretrieval (Salton, Wong, and Yang 1975), automatic thesaurus extraction (Grefenstette1994), and predominant sense identification (McCarthy et al 2004).
Lexical resourcesbased on distributional similarity (e.g., Lin [1998]?s thesaurus) are used in a wide rangeof applications that profit from knowledge about word similarity.
In cognitive science,they have been used, for example, to account for the influence of context on humanlexical processing (McDonald and Brew 2004) and lexical priming (Lowe and McDonald2000).An idealized example for a semantic space representation of selectional preferencesis shown in Figure 1(a).
The two ellipses represent the exemplar clouds formed by the730Erk, Pad?, and Pad?
A Flexible, Corpus-Driven Model of Selectional PreferencesFigure 1An idealized vector space for the plausibilities of (shoot, agent, hunter) and (shoot, patient, hunter).fillers of the agent and patient position of shoot, respectively.
In order to judge whether ahunter is a plausible agent of shoot, the vector space representation of hunter is comparedto the members of the exemplar cloud for the agent position?namely, poacher, policeman,and director.
Due to the high average similarity of the hunter vector to these vectors,hunter will be judged a fairly good agent of shoot.
Compare this with the result for thepatient role: hunter is rather distant from roe, deer, and buck, and is therefore predictedto be a bad patient of shoot.
However, note that hunter is still more plausible as a patientof shoot than, for example, director.3.3 Formalization and Parameter ChoiceVector space models have been formalized by Lowe (2001) as tuples VS = (DTrans,Basis, sim, STrans), where Basis is a set of basis elements or dimensions, DTrans is atransformation of raw co-occurrence counts, sim is a similarity measure, and STrans isa transformation of the whole space, typically dimensionality reduction.
An additionalparameter that becomes relevant for our use of vector spaces (cf.
Equation [6]) is theweighting function wt that determines the contribution of each exemplar to the overallsimilarity.
We discuss the parameters in turn and discuss our reasons for either explor-ing them or fixing them.Basis elements Basis.
Traditionally, context words are used as basis elements, and co-occurrence is defined in terms of a surface window.
Such bag-of-words spaces tend togroup words by topics.
They ignore the syntactic relation between context items andthe target, which is a problem for selectional preference modeling.
The top table inFigure 1(b) illustrates the problem: deer and hunter receive identical vectors, even thoughthey show complementary plausibility ratings.
The reason is that deer and hunter oftenco-occur in similar lexical bag-of-words contexts (namely, hunting-related activities).The bottom table in Figure 1(b) indicates a way out of this problem, namely the useof word-relation pairs as basis elements (Grefenstette 1994; Pad?
and Lapata 2007).This space splits the co-occurrences with context words such as shoot based on thegrammatical relation between target and context word, and this split looks differentfor different words: whereas deer occurs exclusively as the object of shoot, hunter pre-dominantly occurs as the subject.
We find the reverse pattern for escape.
In consequence,731Computational Linguistics Volume 36, Number 4Table 2Similarity measures explored in this article.
Notation: We assume Basis = {b1, .
.
.
, bn}.
We write Ifor mutual information, and BE(a) for the set of basis elements that co-occur at least once with a.simLin(a, a?)
=?(r,v)?BE(a)?BE(a? )
I(a,r,v)+I(a?,r,v)?
(r,v)?BE(a) I(a,r,v)?(r,v)?BE(a? )
I(a?
,r,v) simcosine(a, a?)
=?ni=1 abi?a?bi||a||?||a?||simDice(a, a?)
=2?|BE(a)?BE(a?
)||BE(a)|+|BE(a?
)| simJaccard(a, a?)
= |BE(a)?BE(a?
)||BE(a)?BE(a?
)|simnGCM(a, a?)
= exp(??
?ni=1 (abi||a|| ?a?bi||a?||)2)where ||a|| =?
?ni=1 a2bisimHindle(a, a?)
=?ni=1 simHindle(a, a?, i) wheresimHindle(a, a?, i) ={min(I(a,bi ),I(a?,bi )) if I(a, bi) > 0 and I(a?, bi) > 0abs(max(I(a,bi ),I(a?
,bi ))) if I(a, bi) < 0 and I(a?, bi) < 00 elsethe resulting spaces gain the ability to distinguish between words like hunter and deer,based on differences in typical occurrences in argument positions.On the downside, dependency-based spaces are more expensive to compute thanword-based spaces because they require a corpus with syntactic analysis.
Thus, weexplore both options.
The word-based space records co-occurrences within a surfacewindow of 10 (lemmatized) words.3 We refer to it as WORDSPACE.
The dependency-based space, called DEPSPACE, has basis elements consisting of a grammatical functionconcatenated with a word, as in the bottom example in Figure 1(b) (Pad?
and Lapata2007).
Following earlier experiments on the representation of selectional preferencesin word-dependency-relation spaces (Pad?, Pad?, and Erk 2007), we use a subject?object context specification that only considers co-occurrences between verbs and theirsubjects and direct objects.4 In each case, we adopt the 2,000 most frequent context itemsas basis elements.Similarity measure sim.
In principle, any similarity measure for vectors can be pluggedinto our model.
Previous studies that compared similarity measures came to variousconclusions about the usefulness of different measures.
Cosine similarity is very popu-lar in Information Retrieval.
Lee (1999) obtains good results for the Jaccard coefficientin pseudo-disambiguation.
In the synonymy prediction task of Curran (2004), Diceemerged in first place.
Pad?
and Lapata (2007) found good results with Lin?s measurefor predominant word sense identification.Because it is unclear whether the findings about best similarity measures general-ize to new tasks, we will investigate a range of similarity measures shown in Table 2:Cosine, the Dice and Jaccard coefficients, Hindle?s (1990) and Lin?s (1998) mutualinformation-based metrics, and an adaptation of Nosofsky?s (1986) Generalized ContextModel (GCM), a model for exemplar-based similarity from psychology.
The originalGCM includes normalization by summed similarity over all classes of exemplars, whichintroduces competition between categories.
Our version, which we call nGCM, insteadnormalizes by vector length to alleviate the influence of overall target frequency, but3 We do not remove stop words for reasons of simplicity, as there is no unequivocal definition of this set,and we do not wish to remove potentially informative contexts.4 This context specification is available as soonly in the DependencyVectors software package(http://www.nlpado.de/?sebastian/dv.html) starting from Release 2.5.732Erk, Pad?, and Pad?
A Flexible, Corpus-Driven Model of Selectional Preferencespreserves the central idea that similarity decreases exponentially with distance (Shepard1987).All similarity measures from Table 2 are applicable to semantic spaces with arbitrarybasis elements, with the exception of the Lin measure, whose definition applies onlyto dependency-based spaces.
The reason is that it decomposes the basis elements intorelation?word pairs (r, v).
For semantic spaces with words as basis elements, the Linmeasure can be adapted by omitting the random variable r (cf.
Pad?
and Lapata 2007).Transformations DTrans and STrans.
Next, we come to transformations on counts and vec-tor spaces.
Concerning the count transformations DTrans, all counts are log-likelihoodtransformed (Dunning 1993), a standard procedure for word-based semantic spacemodels which alleviates the problematic effects of the Zipfian distribution of lexicalitems, as proposed by Lowe (2001).
As for transformations on the complete space STrans,many studies do not perform dimensionality reduction at all.
Others, like the LSA fam-ily of vector spaces (Landauer and Dumais 1997), regard it as a crucial ingredient.
Togauge the impact of STrans, we compare unreduced spaces (2,000 dimensions) to 500-dimensional spaces created using Principal Component Analysis (PCA), a standardmethod for dimensionality reduction that identifies the directions of highest variancein a high-dimensional space.Weight functions wt.
Exemplar-based models are usually applied in conjunction with afunction that can assign each exemplar an individual weight, which can be interpretedcognitively as degree of activation (Nosofsky 1986).
We assess a small number ofweight functions to investigate their importance within the EPP model.
The first one,UNI, assumes a uniform distribution, wtr,v(a) = 1.
The second one, FREQ, uses the co-occurrence frequency as weight, wtr,v(a) = freq(a, r, v), with the intuition that more fre-quent exemplars should be both more activated and more reliable.
Finally, we considera weight function that is an analogue of inverse document frequency in InformationRetrieval.
It weights words higher that occur with a smaller number of verb?role pairs:wtr,v(a) = log|?a?
Seenrv (a?
)||Seenrv (a)| , where we write Seenrv(a) for the set of verb?role pairs (r, v)for which a occurs as a headword.5 We abbreviate this weight function by DISCR for?discrimination?.3.4 DiscussionOur EPP model can be seen as a straightforward implementation of the intuition tomodel selectional preference by generalizing from seen headwords to other, similar,words.
We use vector space representations to judge the similarity of words, obtaininga completely corpus-driven model that does not require any additional resources and isvery flexible.
A complementary view on this model is as a generalization of traditionalvector space models that represent semantic similarities between pairs of words.
TheEPP model goes beyond this by computing similarity between a vector and a set of othervectors.
By instantiating the set with the vectors for seen headwords of some relation r,the similarity turns into a plausibility prediction that is specific to this relation.Like other distributional models, the EPP model is applicable whenever corpusdata are available; no lexical resource is required.
Additionally, it does not require theheadword observation step and the generalization step (cf.
Section 1) to use the same5 By keeping the constant |?a?
Seenrv(a?
)|, we guarantee that the fraction remains larger than one, andwtr,v(a) remains positive.
This is to ensure that the weighted average in Equation (6) yields correct results.733Computational Linguistics Volume 36, Number 4corpus.6 This allows us to work with a relatively small and deeply linguistically ana-lyzed corpus of seen headwords, the FrameNet corpus, while using a much bigger dataset to generalize over seen headwords.
It also allows us to make predictions for thepotentially deeper relations annotated in the primary corpus, for example, semanticroles.
We will investigate the potential of this setup in our Experiments 1 and 2.As a distributional model, EPP avoids the two pitfalls of resource-based models.One is a coverage problem due to the limited size of the resource (see the task-basedevaluation in Gildea and Jurafsky [2002]).
For example, the semantic role?based PADOET AL.
model resorts to class-based smoothing methods to improve coverage, whichEPP does not need.
The other problem of resource-based models is that the shape of theWordNet hierarchy determines the generalizations that the models make.
These are notalways intuitive.
For example, Resnik (1996) observes that (answer, obj, tragedy) receivesa high preference because tragedy in WordNet is a type of written communication, whichis a preferred argument class of answer.The ROOTH ET AL.
model (Rooth et al 1999) shares the resource independence ofEPP, but has complementary benefits and problems.
Querying the probabilistic ROOTHET AL.
model takes only constant time, whereas querying the exemplar-based EPPmodel takes time linear in the number of seen arguments for the argument position.However, the ROOTH ET AL.
model requires a dedicated training phase with a spacecomplexity linear in the total number of verbs and nouns, which can lead to practicalproblems for large corpora (cf.
Section 5.1).
The separation of similarity computationand headword observation in EPP also gives the experimenter more fine-grained controlover the types and sources of information in the model.The EPP model looks superficially similar to the model of Dagan, Lee, and Pereira(1999).
However, they differ in the role of the similarity measure: The Dagan, Lee, andPereira model computes a co-occurrence probability, and it uses similarity as a weight-ing scheme.
The EPP model computes similarity (of a word to the typical fillers of anargument position), and its weighting schemes are separate from the similarity measure.The two models also differ in the kinds of items they consider as a basis for generaliza-tion (or smoothing): In computing the probability of seeing a word w2 after w1, the sumin the Dagan, Lee, and Pereira model runs over all words that are similar to w1, whereasthe sum in the EPP model runs over all words that have been seen as headwords in theargument position in question.
Given that occurrence in an argument position is a formof co-occurrence, and similarity (in both models) is computed on the basis of vectorsderived from co-occurrence counts, one could say that the sum in the EPP model runsover words determined by first-order co-occurrence, whereas the sum in Dagan, Lee,and Pereira runs over words chosen through second-order co-occurrence (where w1 andw2 are second-order co-occurring if they both tend to occur with the same words w3).4.
Design of the Experimental EvaluationIn this section, we give a high-level overview over the experiments and experimentalsettings we will use subsequently.
Details will be provided in the following sections.We evaluate the EPP model in three ways: We test the prediction of verbalselectional preference models with a pseudo-disambiguation task (Experiment 1).Then, we address the task of predicting human verb?argument plausibility ratings(Experiment 2).
Finally, we investigate inverse selectional preferences?preferences of6 Dagan, Lee, and Pereira (1999) could in principle do the same, but do not explore this option.734Erk, Pad?, and Pad?
A Flexible, Corpus-Driven Model of Selectional Preferencesnouns for the predicates that they co-occur with?again using pseudo-disambiguation(Experiment 3).We compare the EPP model to models from the three model categories presented inSection 2: RESNIK as a hierarchical model; ROOTH ET AL.
as a distributional model; andPADO ET AL.
as a semantic role?based model.
As both Brockmann and Lapata (2003)and Pad?
(2007) have argued, no WordNet-based model systematically outperformsthe others, and the RESNIK model shows the most consistent behavior across differentscenarios.
Among the distributional models, we choose ROOTH ET AL.
as a model thatperforms soft clustering and thus shows a marked difference to the EPP model.
To ourknowledge, this is the first comparison of all three generalization paradigms: semantichierarchy?based, distributional, and semantic role?based.7As mentioned earlier, we employ two tasks to evaluate the four models: pseudo-disambiguation and the prediction of human plausibility ratings.
The pseudo-disambiguation task (Yarowsky 1993) has become a standard evaluation measure forselectional preference models (Dagan, Lee, and Pereira 1999; Rooth et al 1999).
Givena choice of two potential headwords, the task of a selectional preference model is topick the more plausible one to fill a particular argument position of a given predicate.Pseudo-disambiguation can be viewed as a word sense disambiguation task in whichthe two potential headwords together form a ?pseudo-word,?
for example herb/strugglefrom the original words herb and struggle.
The task is to ?disambiguate?
the pseudo-word to the word that fits better in the given context.
It can also be viewed as an in vitroversion of semantic role labeling and dependency parsing (depending on whether therelations are semantic roles or grammatical functions) (Zapirain, Agirre, and M?rquez2009).
In this case, the scenario is that of a sentence containing a predicate and twowords that could potentially fill an argument position of that predicate, for example, thepredicate recommend with the potential headwords herb and struggle for the grammaticalrelation of direct object.
The task is to decide which of the two potential headwords isbetter suited to fill the argument position.Human plausibility ratings, on the other hand, make considerably more fine-grained distinctions than those occurring in pseudo-disambiguation tasks.
Here, mod-els predict the exact human ratings for verb?argument?role triples.
Ratings are collectedto further control carefully selected experimental items for psycholinguistic studies(Trueswell, Tanenhaus, and Garnsey 1994; McRae, Spivey-Knowlton, and Tanenhaus1998), or are solicited for corpus-derived triples specifically to create evaluation data forplausibility models (Brockmann and Lapata 2003; Pad?
2007).We contrast two different levels of semantic analysis for the predicates and argu-ment positions.
In the SEM PRIMARY setting, the predicates are FrameNet frames, eachof them potentially instantiated by multiple different verbs.
The argument positions inthese settings are frame-semantic roles.
This setting most closely matches the notionof selectional preferences as characterizations of semantic arguments of an event.
Inaddition, we study the SYN PRIMARY setting, where predicates are verbs, and argumentpositions are grammatical functions (subject and direct object).
Viewing grammaticalfunctions as shallow approximations of semantic roles, we can expect the selectionalpreference models for this setting to yield noisier estimates than in the SEM PRIMARYsetting.
The two settings will differ only in the choice of primary corpus, but will usethe same generalization corpus.7 Erk (2007) has a comparison between hierarchy-based and distributional models, but does not include asemantic role?based model.735Computational Linguistics Volume 36, Number 4Table 3 illustrates the difference between the SEM PRIMARY setting and the SYNPRIMARY setting on an example from a pseudo-disambiguation task: The SEM PRIMARYsetting has predicates like the FrameNet frame (predicate sense) ADORNING, with thesemantic role THEME as argument position.
In contrast, the SYN PRIMARY setting haspredicates that are verb lemmas, such as cause, and argument positions that are gram-matical functions (subj).
In both settings, the two potential headwords (here calledheadword and confounder, to be explained in more detail in the next section) to bedistinguished in the pseudo-disambiguation task are noun lemmas.The verb?dependency?headword tuples of the SYN PRIMARY setting yield muchmore coarse-grained and noisy characterizations of selectional preferences; however,they can be extracted from corpora with only syntactic annotation.
We are thereforeable to use the 100-million word BNC (Burnard 1995) as the primary corpus for thissetting by parsing it with the Minipar dependency parser (Lin 1993).
Minipar couldparse almost all of the corpus, resulting in 6,005,130 parsed sentences.For the SEM PRIMARY setting, we require a primary corpus with role-semanticannotation.
We use the much smaller FrameNet corpus (Fillmore, Johnson, and Petruck2003).
FrameNet is a semantic lexicon for English that groups words in semantic classescalled frames and lists fine-grained semantic argument roles for each frame.
Ambiguityis expressed by membership of a word in multiple frames.
Each frame is exemplifiedwith annotated example sentences extracted from the BNC.
The FrameNet release 1.2comprises 131,582 annotated sentences (roughly three million words).
To determineheadwords of the semantic roles, the corpus was parsed using the Collins (1997) parser.As generalization corpus, we use the Minipar-parsed BNC in both settings.
The ex-perimentation with two different primary corpora allows us to directly study the influ-ence of the disambiguation of predicates and the semantic characterization of argumentpositions on the performance of selectional preference models.
Note, however, that thecomparison is complicated by differences between the two corpora: The primary corpusfor the SYN PRIMARY setting is parsed automatically, which can introduce noise in thedetermination of predicates, grammatical functions, and headwords.
The primary cor-pus for the SEM PRIMARY setting is manually annotated for semantics but is parsedautomatically to determine headwords.
This can introduce noise in the headwords, butnot in the determination of predicates and semantic roles.
Also, the primary corpus forthe SYN PRIMARY setting is much larger than the one used in the SEM PRIMARY setting.5.
Experiment 1: Pseudo-DisambiguationThe first experiment uses a pseudo-disambiguation task to evaluate the models?
perfor-mance on modeling the plausibility of nouns as headwords of argument positions ofverbal predicates.Table 3Pseudo-disambiguation items for the SYN PRIMARY setting and the SEM PRIMARY setting.Setting Predicate (v) Arg.
pos.
(r) Headword (a) Confounder (a?
)SYN cause subj succession islandappear subj feasibility desireSEM ADORNING THEME illustration axeROPE_MANIPULATION ROPE cord literature736Erk, Pad?, and Pad?
A Flexible, Corpus-Driven Model of Selectional PreferencesRequire: Some corpus T: a list of triples (v, r, a) of seen predicates, roles, and arguments.Require: Some corpus N: a list of noun lemmas, along with a function freqN : N ?
 that associates each noun n ?
N with its corpus frequency.1: Nmid = {n ?
N | freqN(n) ?
30 and freqN(n) ?
3, 000}2: We define a probability distribution pN over the n ?
Nmid by pN(n) = freqN (n)?m freqN (m)3: conf = { } # set of headword/confounder mappings, starts empty4: AT = {a | (v, r, a) ?
T} # set of seen headwords5: for every a in AT do6: choose a confounder a?
?
Nmid according to pN7: conf = conf ?
{ a ?
a?
}8: end for9: Return: confFigure 2Algorithm for choosing confounders.5.1 SetupTask and data.
In a data set of tuples (v, r, a) of a predicate v, argument position r, andheadword a, each tuple is paired with a confounder a?.
The task is to pick the originalheadword by comparing the tuples (v, r, a) and (v, r, a?).
Table 3 shows some examples.We begin by collecting all triples (v, r, a) observed in the respective primary corpus.In the SYN PRIMARY setting, this corresponds to all headwords observed in subject ordirect object position of a verbal predicate in the BNC, and in the SEM PRIMARY setting,to all nouns observed as headword of some semantic role in a frame introduced by averb.
From this set of triples (v, r, a) for a given primary corpus, we draw an evaluationsample that is balanced by the corpus frequency of predicates and argument position.As test set, we choose 100 (v, r) pairs at random, drawing 20 pairs each from five fre-quency bands: 50?100 occurrences; 100?200 occurrences; 200?500; 500?1,000; and morethan 1,000 occurrences.
For any chosen predicate?relation pair, we sample triples (v, r, a)equally from six frequency bands of arguments a: 1?50 occurrences; 50?100; 100?200;200?500; 500-1,000; and more than 1,000 occurrences.
These evaluation samples containa total of 213,929 (SYN) and 65,902 (SEM) tuples.Next, we pair each headword with a confounder sampled from the primary corpusas described in Figure 2.8 In the literature, there have been two different approaches tochoosing confounders for pseudo-disambiguation tasks: The first approach, used byDagan, Lee, and Pereira (1999), chooses confounders to match the headword a infrequency.
The second approach, used in Rooth et al (1999), sets the probability thata word is drawn as a confounder to its relative frequency.
The advantage and dis-advantage of the first approach is that it largely eliminates the frequency bias that isa general problem of vector space-based approaches.
This is an advantage in that itallows the generalization achieved by the model to be evaluated without any distortionfrom frequency bias.
It is a disadvantage in that in any practical application makinguse of selectional preferences, the data will not be frequency-balanced.
For example,selectional preferences could be used by a dependency parser to decide which word inthe sentence to link to a given verb via a subject edge, or selectional preferences could8 The confounder is the same for all instances of the headword a in the evaluation sample, regardless of thevalues for r and v. As confounder candidates, we only use words with between 30 and 3,000 occurrencesin the BNC, following Rooth et al (1999).737Computational Linguistics Volume 36, Number 4be used by a semantic role labeler to decide which constituent is the overall best fillerfor the AGENT role for a given predicate.
In such cases, it does not appear warranted toassume that the frequencies of different headword candidates are balanced.
We choosethe second option for our experiments, using relative corpus frequency to approximatethe probability of encountering different headword candidates.Training of models.
As stated earlier, we evaluate all models in the SYN PRIMARY settingand the SEM PRIMARY setting.
In all experiments herein, we perform two 2-fold cross-validations runs.
In each run, we randomly split the respective (SYN or SEM) evaluationsample into a training and a test set at the token level.
Figure 3 describes the experimen-tal procedure in pseudo-code.The EPP, RESNIK, and PADO ET AL.
models are trained on the training split of theevaluation sample.
The EPP model additionally uses the BNC as generalization corpusin both the SYN PRIMARY setting and the SEM PRIMARY setting.
This generalizationcorpus is used to compute either a WORDSPACE or a DEPSPACE vector space, asdiscussed in Section 3.3.
For the ROOTH ET AL.
model, we had to employ a frequencyRequire: A set Formalisms of formalisms to testRequire: A primary corpus T: a list of triples (v, r, a) of seen predicates, argumentpositions, and arguments, along with a function freqT : T ?
  that associates eachtriple (v, r, a) ?
T with its corpus frequencyRequire: A mapping conf : Lemmas ?
Lemmas of headwords to confounders such that{a | (v, r, a) ?
T} ?
Domain(conf )1: eval_results = { }2: for splitno in 1:2 do3: # prepare two independent splits4: half1 = { }, half2 = { } # mappings from headwords to counts5: for each tuple t in T do6: # decide how many occurrences of t to put in half1, half2 by drawing from the binomialdistribution7: Sample k ?
B( freqT(t), 0.5)8: half1 = half1 ?
{ t ?
k }, half2 = half2 ?
{ t ?
freqT(t) ?
k }9: end for10: splits = { (half1, half2), (half2, half1) }11: for ( ftrain, ftest) in splits do12: for each formalism F in Formalisms do13: train a model mF according to formalism F using the training set defined bythe frequency function ftrain.14: for each tuple (v, r, a) in T do15: for i in 1:ftest(v, r, a) do16: Evaluate the performance of mF on the tuple (v, r, a, conf (a)) and add theresult to eval_results17: end for18: end for19: end for20: end for21: end for22: Return: eval_resultsFigure 3Algorithm for running a pseudo-disambiguation experiment738Erk, Pad?, and Pad?
A Flexible, Corpus-Driven Model of Selectional Preferencescutoff of five in the SYN PRIMARY setting to reduce the amount of training data due tomemory limitations.
The PADO ET AL.
model is only used in the SEM PRIMARY setting:FrameNet is an integral part of this model, and it cannot be used in a syntax-onlysetting without major changes.
For details on training, see Section 2.4.
Note that noverb classes had to be induced from the data, because the predicates v are alreadyinstantiated by verb classes, namely, FrameNet frames (see Table 3).Finally, we report three baselines.
The first baseline, headword frequency (HW), isvery simple.
It decides between the headword a and the confounder a?
by comparingthe frequencies f (a) and f (a?).
The second, more informed, baseline is triple frequency(TRIPLE).
It votes for a if f (v, r, a) > f (v, r, a?
), and vice versa.
The third baseline, a bigramlanguage model (LM), was constructed by training a 2-gram language model from thelarge English ukWAC Web corpus (Baroni et al 2009) using the SRILM toolkit (Stolcke2002) with default Good?Turing smoothing.
We retained only verbs, nouns, adjectives,and adverbs in order to maximize the proximity between verbs and their subjects andobjects.
We defined the preference score for verb?subject triples as the probability of thesequence av, that is, Pref (v, subj, a) = P(v|a).
Conversely, the preference score for verb?object triples was defined as the probability of the sequence va, that is, Pref (v, obj, a) =P(a|v).
Again, the model compares Pref (v, r, a) and Pref (v, r, a?)
to make its decision.Evaluation.
For all models, we report two evaluation figures.
One is coverage: A tupleis covered if the model assigns some preference to both a and a?, and the preferences arenot equal.
The second is error rate, which is the relative frequency, among all coveredtuples, of instances where the confounder was at least equally preferred.
Both coverageand error rate are averages over the 2 x 2 cross-validation runs in each setting.We determine the statistical significance of differences between error rates usingbootstrap resampling (Efron and Tibshirani 1994).
This procedure samples correspond-ing model predictions with replacement from the set of predictions made by the modelsto be compared and computes the difference in error rates.
On the basis of n suchsamples (n = 1,000), the empirical 95% confidence interval for the difference in strengthon the basis of all observed differences is computed.
If the interval includes 0, thedifference is not statistically significant.5.2 SYN PRIMARY Setting: ResultsTable 4 shows the results for the SYN PRIMARY setting.
The overall best error rate isachieved by a variant of the EPP model, with the RESNIK model coming in second(the performance difference is significant at the 0.05 level).
The EPP variants also shownear-perfect coverage, whereas the RESNIK model delivers results only for 63% of thedata points.
We found a very high error rate and a comparatively low coverage forROOTH ET AL., which most likely stems from the data pruning necessary to reduce thetraining data (compare the subsequent results in the SEM PRIMARY setting).
The PADOET AL.
model was not tested in the SEM PRIMARY setting, because it requires semanticrole annotation.
The HW baseline is somewhat below chance (50%), which is an effect ofour by-token sampling procedure, according to which confounders often have highercorpus frequencies than the real arguments.
The TRIPLE baseline has a better error ratethan the LM baseline, but has very low coverage.
Both the RESNIK and the EPP modelsoutperform the baselines in terms of error rate.
That they outperform the TRIPLEbaseline in terms of error rate indicates that we sometimes have confounders that haveactually been seen more often with the verb?argument pair than the headword, but that739Computational Linguistics Volume 36, Number 4Table 4SYN PRIMARY setting: Pseudo-disambiguation results for different weighting schemes.Model Similarity Error rate (%) Coverage (%)UNI FREQ DISCREPP:DEPSPACECosine 32.8 30.3 31.2 98.5Dice 49.4 48.2 47.5 97.1nGCM 27.6 27.5 25.7 98.5Hindle 53.7 52.3 52.8 96.6Jaccard 49.5 48.2 47.6 97.1Lin 35.5 34.3 33.2 98.8EPP:DEPSPACE, PCACosine 30.2 28.7 28.8 98.1Dice 29.9 30.8 28.6 98.2nGCM 26.4 26.4 25.6 98.1Hindle 45.0 44.4 44.2 95.7Jaccard 29.7 30.7 28.5 98.2Lin 28.7 29.1 26.7 97.7EPP:WORDSPACECosine 35.3 35.8 34.0 97.4Dice 51.0 50.7 50.3 96.0nGCM 33.2 34.7 31.8 97.4Hindle 52.7 52.8 52.4 96.0Jaccard 51.8 52.0 51.3 96.0Lin 32.0 31.8 31.4 98.2EPP:WORDSPACE, PCACosine 30.3 31.3 29.4 97.1Dice 31.3 32.4 30.5 97.8nGCM 30.0 30.9 29.0 97.1Hindle 40.2 41.0 40.4 95.3Jaccard 31.0 32.1 30.2 97.8Lin 27.8 29.8 26.9 97.3RESNIK 28.1 63.4ROOTH ET AL.
58.1 61.5PADO ET AL.
?
?HW 60.0 100.0TRIPLE 32.0 4.0LM 37.0 86.0are dissimilar from other seen headwords, which allows RESNIK and EPP to identifythem as confounders in spite of their higher co-occurrence frequency.We now turn to a comparison of the EPP variants.
The coverage of all EPP models isvery high (0.95 or higher), independent of space, similarity measure, and dimensionalityreduction.
We generally observe that error rates are lower when word meaning isrepresented in DEPSPACE, and when discrimination weighting is used.
In DEPSPACE,nGCM works best, yielding the overall best result with an error rate of 25.6?25.7%.In WORDSPACE, the Lin measure shows the best error rates with an error rate of justbelow 27%.
These results hold both for the unreduced and the reduced spaces and arehighly significant (p ?
0.01).
Hindle is clearly the worst measure at around randomperformance.740Erk, Pad?, and Pad?
A Flexible, Corpus-Driven Model of Selectional PreferencesThe difference between UNI and DISCR is significant throughout; the differencebetween FREQ and DISCR is less uniform.
In DEPSPACE, the difference between the bestmeasure with and without PCA (nGCM in both cases) is not significant; in WORDSPACE,the difference between the best measure with and without PCA (Lin in both cases) issignificant (p ?
0.01).For both WORDSPACEs and DEPSPACEs without PCA, the similarity measures divideinto two distinct groups: Lin, nGCM, and Cosine on the one hand and Jaccard, Dice, andHindle on the other, with a significant difference in performance between the groups(p ?
0.01).
The use of dimensionality reduction through PCA improves performancefor all similarity measures, in WORDSPACE as well as DEPSPACE.
The improvement isespecially marked for the Dice and Jaccard measures, which perform at the level ofa random baseline for unreduced spaces.
We assume that these set intersection-basedmeasures benefit from the independent dimensions that PCA produces.
For the simi-larity measures with best performance, the improvement through PCA is less marked.Thus, PCA-reduced spaces show more similar error rates across similarity measures.After PCA, only nGCM and Lin still significantly (p ?
0.01) outperform the othersin DEPSPACE, and in WORDSPACE, Lin is the only measure that performs significantlydifferently from the rest (p ?
0.01).As arguments are sampled from six frequency bins, we can inspect the effect ofargument frequency on error rate.
Figure 4 examines the performance of the EPP modelwith different similarity measures and weighting schemes by argument frequency bins(cf.
the subsection Task and Data in Section 5).
We find that the overall best weightingscheme, DISCR, also works best for all except the highest argument frequency bin.
Inthe DEPSPACE setting (upper row), all similarity measures show a frequency bias in thatFigure 4SYN PRIMARY setting: Error rate by argument frequency bin.
Bins: 1 = 1?50; 2 = 50?100;3 = 100?200; 4 = 200?500; 5 = 500?1,000; 6 > 1,000.741Computational Linguistics Volume 36, Number 4Figure 5SYN PRIMARY setting: Error rate by predicate frequency bin: DISCR weighting.
Bins: 1 = 50?100;2 = 100?200; 3 = 200?500; 4 = 500?1,000; 5 > 1,000.error rate is lower for more frequent arguments, but this bias is much less pronouncedin Cosine and nGCM than in the other measures, with error rates varying between 45%and 25% rather than 80% and 20%.
(Dice and Hindle, not shown here, exhibit similarbehavior to Jaccard.)
In PCA-transformed DEPSPACE (middle row), this frequency biaslargely disappears for all similarity measures.
In WORDSPACE (bottom row), althoughthere is again a frequency bias in all similarity measures, Lin now joins Cosine andnGCM in being much less biased than Jaccard, Dice, and Hindle.
For WORDSPACEwith PCA-transformation, not shown here, the curves resemble those of DEPSPACE withPCA-transformation.Figure 5 examines the effect of (predicate, argument position) pair frequencyon error rate.
Predicate?argument position pairs were sampled from five frequencybins.
The figure shows DISCR weighting only.
In the spaces without dimensionalityreduction, there is a clear division between Cosine, nGCM, and Lin on the one hand,and Jaccard, Dice, and Hindle on the other.
In PCA spaces, all measures except forHindle are similar in their performance.
In both DEPSPACE conditions, error ratedecreases towards the higher frequency predicate bins, although this is not so in742Erk, Pad?, and Pad?
A Flexible, Corpus-Driven Model of Selectional PreferencesWORDSPACE.
It seems that in the sparser DEPSPACE, models can still profit from theadditional seen headwords in the highest predicate frequency bins, whereas in the lesssparse but noisier WORDSPACE, the added noise is stronger than the added signal inthe highest predicate frequency bins.
For the lowest predicate frequency bins, the bestresults in WORDSPACE are better than those in DEPSPACE.5.3 SEM PRIMARY Setting: ResultsTable 5 shows the results for the SEM PRIMARY setting, where we predict head words forpairs of a frame (predicate sense) and semantic role.
In comparison to the SYN PRIMARYsetting (Table 4), error rates are lower across the board.
The difference for the EPP modelsis on average around 10%.Table 5SEM PRIMARY setting: Pseudo-disambiguation results for different weighting schemes.Model Similarity Error rate (%) Coverage (%)UNI FREQ DISCREPP:DEPSPACECosine 19.8 16.9 19.0 97.1Dice 42.3 32.4 39.5 96.3nGCM 20.2 16.3 20.8 97.1Hindle 48.3 46.8 47.8 93.4Jaccard 41.5 31.5 38.5 96.3Lin 31.1 20.2 29.0 97.7EPP:DEPSPACE, PCACosine 18.5 17.0 17.8 96.9Dice 19.3 19.2 18.0 97.6nGCM 16.9 15.7 16.4 96.9Hindle 44.9 45.6 44.7 89.3Jaccard 18.2 18.5 17.5 97.6Lin 18.8 19.5 18.3 98.9EPP:WORDSPACECosine 24.1 20.4 23.4 93.1Dice 23.7 24.5 22.5 89.6nGCM 21.1 17.8 19.4 93.1Hindle 31.8 33.1 31.8 83.1Jaccard 24.8 26.5 24.2 89.6Lin 22.3 18.4 21.9 92.8EPP:WORDSPACE, PCACosine 21.0 17.6 20.5 93.1Dice 18.5 16.4 17.8 96.8nGCM 19.7 16.4 19.3 93.1Hindle 41.0 39.8 40.7 90.6Jaccard 18.1 16.2 17.6 96.8Lin 21.3 17.1 20.7 98.3RESNIK 16.5 62.8ROOTH ET AL.
24.9 100.0PADO ET AL.
7.1 59.0HW 65.0 100.0TRIPLE 44.0 2.0LM NA NA743Computational Linguistics Volume 36, Number 4The error rate of the PADO ET AL.
model, at 7%, is the best by a large margin.
Weattribute this to the extensive generalization mechanisms that the model uses, whichdraw on an array of lexical?semantic resources.
However, with a coverage of 59%,the model is still unable to make predictions for many of the test items.
Error ratesfor the RESNIK and the EPP models are comparable, at 16.5% for RESNIK and 15.7% forthe best EPP variant.
The two models differ sharply in coverage, however: 62.8% forRESNIK, consistent with the findings of Gildea and Jurafsky (2002), and between 90%and 98% for EPP variants.
The RESNIK model also profits from the presence of semanticdisambiguation in the SEM PRIMARY setting (in the SYN PRIMARY setting its errorrate was 28%), which underlines the substantial impact that properties of the trainingdata have on semantic hierarchy?based models of selectional preferences.
ROOTHET AL.
now has perfect coverage, affirming our assumption that the very bad resultsof the ROOTH ET AL.
model in the SYN PRIMARY setting were an artifact of the datasampling necessary for that data set.
Although its error rate of 24.9% is a substantialimprovement over all baselines, the EPP model achieves error rates that are up to9 points lower at a comparable coverage.
Among the baselines, HW shows that here, asin the SYN PRIMARY setting, arguments have some tendency of having lower frequencythan the confounders.
The TRIPLE baseline shows near-random performance, at verylow coverage, a result of the very small size of the corpus.
Because there is no largecorpus with frame-semantic roles, nor is the annotation easily linearizable, we couldnot compute a LM baseline in the SEM PRIMARY setting.Among EPP models, the DEPSPACEs and WORDSPACEs perform comparably, with anon-significant advantage for DEPSPACE among the best models.
Overall error ratesshow the same clear divide between the three high-performing similarity measures(Cosine, nGCM, and Lin) and the three weaker ones (Dice, Jaccard, and Hindle).
Di-mensionality reduction again dramatically improves the weaker models, with Jaccardyielding the best result for the PCA-reduced WORDSPACE.9 Whereas all best parame-trizations in the SYN PRIMARY setting used DISCR weighting, it is now FREQ weightingthat yields the best results.Figure 6 again analyzes the influence of argument frequency on performance byshowing the performance of different variants of the EPP model over six argumentfrequency bins.
The upper row shows DEPSPACE without dimensionality reduction.Note that FREQ weighting now works especially well for the lowest argument frequencybin, much better than DISCR and PLAIN.
This is the opposite of what we saw for theSYN PRIMARY setting in Figure 4.
With DISCR and PLAIN weighting, Jaccard and Linagain have noticeable problems with the lowest argument frequency bins?as in the SYNPRIMARY setting?but not with FREQ weighting.
With DEPSPACE and dimensionalityreduction (middle row), we get error rates of ?
26% for all settings and all frequencybins.
On the lowest frequency bin, we again see a large advantage of FREQ weightingover the two other weighting schemes.
The bottom row shows WORDSPACE withoutdimensionality reduction.
Note that there is much less variation in error rates acrossfrequency bins here than in unreduced DEPSPACE.Figure 7 charts error rate by predicate frequency bin, showing FREQ weightingonly, as this showed the best results on this data set.
The figure clearly illustrates thedivide between the top and the bottom three similarity measures in DEPSPACE, as wellas the disappearance of this divide for both PCA settings.
In unreduced WORDSPACE,9 The differences to other similarity metrics in the FREQ setting are insignificant, with the exceptionof Hindle.744Erk, Pad?, and Pad?
A Flexible, Corpus-Driven Model of Selectional Preferencesthe divide is not as clearly visible.
The figure also indicates a slight tendency for errorrates to rise for the lowest-frequency as well as the highest-frequency predicates, acrossall spaces.5.4 DiscussionThe resource-based approaches that we tested, RESNIK and PADO ET AL., show superiorperformance when they have coverage (which coincides with findings in other lexicalsemantics tasks that supervised data, when available, always increases performance),but showed low coverage, at most 63% (RESNIK, SYN PRIMARY setting).
The EPP modelachieves near-perfect coverage at good error rates: In the SYN PRIMARY setting, theRESNIK model achieved an error rate of 28%, and the best EPP variant was at 26%.
Inthe SEM PRIMARY setting, error rates were 7% for the PADO ET AL.
model, 16.5% forthe RESNIK model, and 16% for the best EPP variant.
Comparing the EPP and ROOTHET AL.
models in the SEM PRIMARY setting, we find that the use of an additional gen-eralization corpus in the EPP model seems to offset any advantages introduced by thejoint clustering of predicates and arguments.The difference in model performance on the two primary corpora (SYN and SEM)is striking.
Even though the FrameNet corpus is smaller and a sparse data prob-lem might be expected, models perform at considerably lower error rates in the SEMPRIMARY setting than when the primary corpus is the larger BNC.
This underscoresthe point that selectional preferences belong to a predicate sense rather than a predicatelemma, and that they describe the semantics of fillers of semantic roles rather than ofFigure 6SEM PRIMARY setting: Error rate by argument frequency bin.
Bins: 1 = 1?50; 2 = 50?100;3 = 100?200; 4 = 200?500; 5 = 500?1,000; 6 > 1,000.745Computational Linguistics Volume 36, Number 4Figure 7SEM PRIMARY setting: Error rate by predicate frequency bin: FREQ weighting.
Bins: 1 = 50?100;2 = 100?200; 3 = 200?500; 4 = 500?1,000; 5 > 1,000.syntactic dependents (recall that in this setting, we predict head words for pairs of apredicate sense and semantic role).
In the SEM PRIMARY setting, the data is cleaner, soit is expected that seen headwords of an argument position will be more semanticallyuniform.
This has a strong influence on model performance.
Another factor contributingto the difference in performance between the two data sets may be that the primarycorpus in the SYN PRIMARY setting is parsed automatically, whereas manual annotationis available in the FrameNet corpus.
However, although this manual annotation iden-tifies predicate senses, role headwords are still determined through automatic parsing.The division of the training data into a primary and a secondary corpus allows us tosuccessfully use FrameNet as the basis for semantic space?based similarity estimatesdespite the fact that this corpus alone would be too small to sustain the construction of arobust space.In terms of model parameters for EPP, the following patterns stand out.
Cosine,Lin, and nGCM show good performance across all spaces and parameter settings; Diceand Jaccard work comparably only on spaces that use dimensionality reduction.
TheHindle measure is an underperformer in all conditions.
With Lin, Jaccard, Dice, andHindle, error rates rise sharply for less frequent arguments in many spaces.
AlthoughCosine and nGCM also have some frequency bias, it is much less pronounced.
nGCMseems to work well with sparse data sets that are not too noisy, as evidenced by thefact that it has the best performance among all EPP variants on both DEPSPACEs in746Erk, Pad?, and Pad?
A Flexible, Corpus-Driven Model of Selectional PreferencesTable 6Verb?argument position?noun triples with plausibility judgments on a 7-point scale (McRaeet al, 1998).Verb Argument position Noun Plausibilityshoot agent hunter 6.9shoot patient hunter 2.8shoot agent deer 1.0shoot patient deer 6.4the SYN PRIMARY setting, as well as in all SEM conditions except reduced WORDSPACE.The Lin measure seems to work well with noisier data: It is the best EPP model whenusing WORDSPACE in the SYN PRIMARY setting.
Cosine, although never showing thetop performance, is among the best models in any setting.
Although dimensionalityreduction only improves the overall error rates of the best models by a few points, ithas two important consequences: First, dimensionality reduction reduces dependenceof the results on the exact similarity measure chosen, as all measures except Hindleshow nearly indistinguishable error rates on reduced spaces (Figures 5 and 7).
Second,low-frequency arguments profit by a huge margin when PCA is used (Figures 4 and 6).Among weighting schemes, DISCR weighting seems to be most useful when the datais sparse but somewhat noisy (as is the case in the lower argument frequency bins inthe SYN PRIMARY setting).
Frequency weighting seems to work best when the data iseither not sparse (as in the highest argument frequency bin in the SYN PRIMARY setting)or very clean but sparse (as in the lowest argument frequency bin in the SEM PRIMARYsetting).
A comparison of the two vector spaces, DEPSPACE and WORDSPACE, showsno clear winner.
When the collections of seen headwords are noisier, as they are in theSYN PRIMARY setting, DEPSPACE, with its more aggressive filtering, yields the betterresults.
Sets of headwords collected by predicate sense, as in the SEM PRIMARY setting,are sparser but cleaner, and WORDSPACE shows lower error rates.6.
Experiment 2: Human Plausibility JudgmentsExperimental psycholinguistics affords a second perspective on selectional preferences:The plausibility of verb?argument pairs has been shown to have an important effecton human sentence processing (e.g., Trueswell, Tanenhaus, and Gransey 1994; Garnseyet al 1997; McRae, Spivey-Knowlton, and Tanenhaus 1998).
In these studies, plausibilitywas operationalized as the thematic fit or selectional preference between a verb and itsargument in a specific argument position.
Models of human sentence processing there-fore need selectional preference models (Pad?, Crocker, and Keller 2009).
Conversely,psycholinguistic plausibility judgments can be used to evaluate computational modelsof selectional preferences.6.1 Experimental MaterialsWe present evaluations on two plausibility judgment data sets used in recent studies.747Computational Linguistics Volume 36, Number 4The first data set consists of 100 data points10 from McRae, Spivey-Knowlton, andTanenhaus (1998).
Our example in Table 6, which is taken from this data set, waselicited by asking study participants to rate the plausibility of, for example, a huntershooting (AGENT) or being shot (PATIENT).
The data point demonstrates the McRae set?sbalanced structure: 25 verbs are paired with two argument headwords in two argumentpositions each, such that each argument is highly plausible in one argument positionbut implausible in the other (hunters shoot, but are seldom shot, and vice versa for deer).The resulting distribution of ratings is thus highly bimodal.
Models can only reliablypredict the human ratings in this data set if they can capture the difference betweenverb argument positions as well as between individual fillers.
However, because theverb?argument pairs were created by hand and with strict requirements, many of thearguments are infrequent in standard corpora (e.g., wimp, bellboy, or knight).
WhenFrameNet is used to annotate senses for the verbs, no appropriate senses are availablefor 28 of the 100 verb?argument pairs, reducing the test set to 72 data points.The second, larger data set addresses this sparseness issue.
Its triples are con-structed on the basis of corpus co-occurrences (Pad?
2007).
Eighteen verbs are combinedwith their three most frequent subjects and objects found in the Penn Treebank andFrameNet corpora, respectively, up to a total of 12 arguments.
Each verb?argument pairwas rated both as an agent and as a patient (i.e., both in the observed and an unobservedargument position), which leads to a total of 24 rated triples per verb.
The data setcontains ratings for 414 triples.
The resulting judgments show a more even distributionof data.
With FrameNet annotation for the verbs, appropriate senses are not attested forsix verb?argument pairs, reducing the test set to 408 data points.6.2 SetupWe evaluate the same four models as in Experiment 1: EPP, the WordNet-based RESNIKmodel, the distributional ROOTH ET AL.
model, and the semantic role?based PADOET AL.
model.
We again compare a SYN PRIMARY setting, where the models make pre-dictions for pairs of a verb and a grammatical function, with a SEM PRIMARY setting,for which the two test data sets were annotated with verb sense and semantic roles inthe FrameNet paradigm (Pad?
2007) and where models make predictions for pairs of aframe and a semantic role.
As before, the PADO ET AL.
model is only tested in the SEMPRIMARY setting.11 For the EPP model, we focus on parsed, dimensionally unreducedspaces and DISCR weighting, following earlier results (Pad?, Pad?, and Erk 2007).
Weprovide results for the best WORDSPACE models from Experiment 1 for comparison.The primary corpora for training selectional preference models were prepared as inExperiment 1 (cf.
Section 5.1).
The generalization corpus for EPP was again the BNC.For the ROOTH ET AL.
model in the SYN PRIMARY setting, we again used a frequencycutoff.
We found the RESNIK model to perform better when using just a subset of theBNC (namely, all the triples for verbs present in the test set).6.3 Evaluation ProcedureWe evaluate our models by correlating the predicted plausibility values with the humanjudgments, which range between 1 and 7.
Because we do not assume a priori that there10 The original data set has 60 data points more, which were used as the development set for the PADOET AL.
model.11 The PADO ET AL.
model now uses automatically induced verb clusters instead of FrameNet frames.748Erk, Pad?, and Pad?
A Flexible, Corpus-Driven Model of Selectional PreferencesTable 7Comparison of EPP DEPSPACE models on McRae data.
Unreduced spaces, DISCR weighting.
***p < 0.001.SEM SYNSim Coverage Spearman?s ?
Coverage Spearman?s ?Dice 100% 0.038 ns 98% 0.148 nsJaccard 100% 0.045 ns 98% 0.153 nsCosine 100% 0.162 ns 98% 0.197 nsHindle 100% 0.060 ns 98% 0.108 nsLin 100% 0.085 ns 98% 0.094 nsnGCM 100% 0.154 ns 98% 0.325 ***is a linear correlation between the two variables, we do not use Pearson?s product-moment correlation, but instead Spearman?s ?, a non-parametric rank-order correlationcoefficient.12 Note that significance is harder to reach the smaller the number of datapoints is.In line with Experiment 1, we include a simple frequency baseline FREQ, whichpredicts the plausibility of each item as its frequency in the BNC (SYN) and in FrameNet(SEM), respectively.
With regard to an upper bound, we assume that automatic modelsof plausibility should not be expected to surpass the typical human agreement on theplausibility judgment.
This is roughly ?
?
0.7 for the Pado data set.6.4 McRae Data Set: Results and DiscussionTable 7 focuses on EPP variants with unreduced DEPSPACE for the McRae data set.
Wesee that this data set is rather difficult to model.
None of the models trained in the SEMPRIMARY setting achieves a significant correlation.13 Apparently, the FrameNet corpusis too small to acquire selectional preferences that generalize well to the infrequentitems that make up the McRae data set.
In the SYN PRIMARY setting, the nGCM model?spredictions reach significance.Table 8 shows results on the McRae data set for all selectional preference modelsthat we are considering.
For EPP, we only show nGCM as the best-performing similaritymeasure from the pseudo-disambiguation task, and Cosine as a widely used vanillameasure.
The results for the SEM PRIMARY setting (left-hand side) mirror the resultsfor the SEM PRIMARY setting in Experiment 1: The deep PADO ET AL.
model shows thebest correlation (it is the only model to predict human judgments significantly).
Itovercomes the sparseness in the FrameNet corpus by using semantic verb classes thatare particularly geared towards grouping the existing verb occurrences in the waythat is most meaningful for this task.
It covers about 80% of the test data.
EPP hasfull coverage, and although it does not make statistically significant predictions, itshows substantially higher correlation coefficients than ROOTH ET AL.
and RESNIK.12 A second concern is the computation of significance values: The methods most widely used for thePearson coefficient (Student?s t-distribution, Fisher transformation) assume that the variables arenormally distributed, which is not the case in our data set.
For Spearman?s ?, we use the algorithm byBest and Roberts (1975), which does not make this assumption.13 Significance here refers to significance of correlation with the human data, not significance of differencesbetween models.749Computational Linguistics Volume 36, Number 4Table 8Comparison across models on McRae data.
**p < 0.01, ***p < 0.001.SEM SYNModel Coverage Spearman?s ?
Coverage Spearman?s ?EPP (DEPSPACE nGCM) 100% 0.154 ns 98% 0.325 ***EPP (DEPSPACE Cosine) 100% 0.162 ns 98% 0.197 nsRESNIK 100% ?0.041 ns 100% 0.123 nsROOTH ET AL.
67% 0.078 ns 48% 0.465 ***PADO ET AL.
78% 0.415 ** ?
?EPP (WORDSPACE Lin) 100% 0.138 ns 98% 0.062 nsEPP (WORDSPACE nGCM) 100% 0.167 ns 98% 0.110 nsFREQ 18% 0.087 ns 36% 0.103 nsThe DEPSPACE and WORDSPACE variants of EPP perform similarly here, and the simplefrequency baseline has very low coverage and correlation.As the right-hand side of Table 8 shows, both ROOTH ET AL.
and EPP achievebetter results in the SYN PRIMARY setting than in the SEM PRIMARY setting.
The ROOTHET AL.
model obtains a highly significant correlation.
The combination of infrequentheadwords in the McRae data set and the large primary corpus brings out the benefitsthat the ROOTH ET AL.
model can derive from generalizing from verbs and nouns tothe latent classes via soft clustering.
Unfortunately, its coverage is still quite low (48%),and for this reason, the difference from the best EPP model is not significant.14 In theSYN PRIMARY setting, the EPP DEPSPACE models clearly outperform the WORDSPACEbecause of the DEPSPACE models?
more aggressive filtering.
Interestingly, RESNIKstill performs poorly in the SYN PRIMARY setting: WordNet does not make the rightgeneralizations to capture the selectional preferences at play in the McRae data, nomatter how much training data is available.
This is underscored by an analysis of whichWordNet classes were most frequently determined as the strongest association withthe target verbs: The classes entity, person, and physical object are assigned in 60 out of100 test cases for the McRae data (SYN PRIMARY setting), a data set where plausibilityis determined by factors much more fine-grained than animacy.
(In the SEM PRIMARYsetting, the picture is similar with classes person, organism, and entity assigned in 48 outof 72 test cases.)
The frequency baseline again performs badly.6.5 Pado Data Set: Results and DiscussionWe now turn to the Pado data set.
Again, we first focus on the performance of differ-ent similarity measures in EPP using unreduced DEPSPACE (Table 9).
Correlation withhuman judgments is much better than for the McRae data set, and highly significantfor all SEM PRIMARY setting models and three of the SYN PRIMARY setting models.
Inboth settings, Cosine and Lin are the best measures (difference not significant), followedby nGCM.
Hindle comes out worst once more.
The difference between the strong and14 As in Experiment 1, we apply bootstrap resampling to determine the significance of differences betweenmodels.
This procedure also takes differences in coverage into account?specifically, a significantdifference becomes harder to achieve as the number of data points shared between the models shrinks.750Erk, Pad?, and Pad?
A Flexible, Corpus-Driven Model of Selectional PreferencesTable 9Comparison of EPP DEPSPACE parametrizations on Pad?
data.
Unreduced spaces, DISCRweighting.
**p < 0.01; ***p < 0.001.SEM SYNSim Coverage Spearman?s ?
Coverage Spearman?s ?Dice 100% 0.289 *** 100% 0.026 nsJaccard 100% 0.285 *** 100% 0.023 nsCosine 100% 0.508 *** 100% 0.403 ***Hindle 100% 0.160 ** 100% ?0.004 nsLin 100% 0.498 *** 100% 0.229 ***nGCM 100% 0.384 *** 100% 0.156 **weak measures is more pronounced for the SYN PRIMARY setting, compared with theSEM PRIMARY setting.
Coverage is at or close to 100% throughout.Table 10 shows results on the Pado data set for all selectional preference models thatwe consider.
In the SEM PRIMARY setting (where both the data and the primary corpushave FrameNet annotation), EPP and the deep PADO ET AL.
model predict the humanjudgments similarly well (difference not significant).
Because all verbs in this data setare covered by FrameNet, the PADO ET AL.
model also shows a nearly perfect cover-age.
EPP and PADO ET AL.
do much better than ROOTH ET AL.
(differences significantat p ?
0.01).
ROOTH ET AL.
has the lowest coverage at 88%, but this is still higher thanits coverage of the McRae data.
As with the McRae data, ROOTH ET AL.
achieves bettercorrelation in the SYN PRIMARY setting than the SEM PRIMARY setting, indicating thatthe frequency cutoff does not harm performance as much in Experiment 2 as it did inExperiment 1.
However, the coverage of ROOTH ET AL.
is lower in the SYN PRIMARYsetting, perhaps because the SEM PRIMARY setting smoothes rare verbs by groupingthem in frames with other verbs.
RESNIK also achieves better correlation in the SYNPRIMARY setting, but recall that it was trained on a subset of the BNC only to reducenoise in the training data?when trained on the whole BNC set, performance degradesto ?
= 0.060.
The difference from the best EPP model remains numerically large.
As forTable 10Comparison across models on Pad?
data.
***p < 0.001.SEM SYNModel Coverage Spearman?s ?
Coverage Spearman?s ?EPP (DEPSPACE Cosine) 100% 0.489 *** 98% 0.470 ***EPP (DEPSPACE nGCM) 100% 0.393 *** 98% 0.328 ***RESNIK 98% 0.230 *** 97% 0.317 ***ROOTH ET AL.
88% 0.060 ns 58% 0.200 ***PADO ET AL.
97% 0.515 *** ?
?EPP (WORDSPACE Lin) 100% 0.254 *** 100% 0.056 nsEPP (WORDSPACE nGCM) 100% 0.192 *** 100% 0.078 nsFREQ 32% ?0.041 ns 69% 0.090 ns751Computational Linguistics Volume 36, Number 4the McRae data set, the EPP WORDSPACE models show much worse performance thanthe DEPSPACE models, and do not significantly predict the human plausibility ratings.The frequency baseline shows a considerably better coverage for this data set, butits correlations hover around zero, which underlines our intuition that verb?argumentcombinations can be plausible without being frequent in corpora.
An example is thecombination (to) embarrass (an) official, which is rated as highly plausible, but occursonly once each in the BNC and FrameNet.6.6 DiscussionThe McRae data set seems in general more difficult to account for than the Pado dataset, as noted by Pad?, Pad?, and Erk (2007).
They explain it by a general frequency effectin the BNC data (which are a superset of the FrameNet data): The median frequency ofthe hand-selected McRae nouns in the BNC is 1,356, as opposed to 8,184 for the corpus-derived Pado nouns.Comparing all selectional preference models, we find that the RESNIK and theROOTH ET AL.
models generally do worse than EPP both in terms of coverage andquality of predictions.
One notable exception is the excellent performance of the ROOTHET AL.
model on the McRae data in the SYN PRIMARY setting, which comes, however,with a low coverage of less than 50%.
A closer inspection of the predictions showedthat ROOTH ET AL.
makes many predictions for verb?object pairs but abstains fromsubjects, thus reducing the complexity of the task.
For only 20% of verbs, predictionsare made for subjects and objects.
As noted in Pad?, Pad?, and Erk (2007), the relativelypoor performance of the RESNIK model may be explained by the fact that its ability togeneralize is limited to the structure of WordNet, where some semantic distinctions areeasier to make than others.
For example, a fairly easy distinction to make for WordNet-based models is animate vs. inanimate.
Because the Pado set contains a portion ofinanimate arguments with animate counterparts, the RESNIK model does well on those.In contrast, in the McRae test set, all arguments are animates, and thus similar to oneanother in terms of WordNet.The deep PADO ET AL.
model achieves the best correlation with the human judg-ments on both data sets, but it is limited to the SEM PRIMARY setting.
Although thebest model is not always among the EPP DEPSPACE models, they consistently show acoverage of close to 100%, and are generally statistically indistinguishable from the bestmodel.
Unlike ROOTH ET AL.
and RESNIK, whose performance varies widely betweenthe SEM PRIMARY setting and the SYN PRIMARY setting, the correlation coefficients forthe EPP models are generally similar across settings.
We take this as evidence that EPPmodels can extract relevant information from deeper annotation on small corpora aswell as from large, but noisy and shallow, training data.Finally, we consider the different similarity measures for the EPP model evaluatedon unreduced DEPSPACE.
The picture differs somewhat between the two data sets, butthe Cosine measure performs well overall, with Lin and nGCM generally in secondand third place.
So, the group of the three best similarity measures is the same as inExperiment 1, but Cosine shows better performance.
One possible reason for this liesin the verb frequency, which is relatively high in both data sets: 68% of the McRaeverbs and 83% of the Pado verbs have BNC frequencies of 1,000 and more, whereasExperiment 1 used an equal number of predicates from five frequency bins, the highestbeing 1,000 and more occurrences.
In that highest predicate frequency bin, Cosineconsistently performed as well as Lin or better in Experiment 1 (Figures 5 and 7).752Erk, Pad?, and Pad?
A Flexible, Corpus-Driven Model of Selectional Preferences7.
Experiment 3: Inverse Selectional PreferencesThe term selectional preference is typically used to describe the semantic constraintsthat predicates place on their arguments.
In this section, we will investigate how nominalarguments place semantic constraints or expectations on the predicates with which theyoccur.
Such expectations can be thought of as typical events that involve the givenobject.
For example, a noun like apple could be said to have preferences about its inversesubject position, that is, the verbs that can take it as a plausible subject.
Examples mightbe verbs like grow or fall; for its inverse object position, apple probably prefers verbslike eat, cut, or plant.
We will use the term inverse selectional preference to refer topreferences of nouns for their predicates, distinguishing them from regular selectionalpreferences.It is clear that not all verbs will be equally likely to occur with a given noun?role pair.
Still, inverse selectional preferences warrant a closer look: To what extent doinverse selectional preferences differ from regular ones?
And are the tasks of predictingregular and inverse selectional preferences equally difficult?
We start in Section 7.2 withan exploratory data analysis of inverse selectional preferences, which shows that inverseselectional preferences show semantically coherent patterns like regular selectionalpreferences, but that, in contrast to most verbs, nouns tend to occur with multiplesemantic groups of verbs.
In Sections 7.3?7.5, we test the EPP model on a pseudo-disambiguation task for inverse selectional preferences.7.1 Related WorkIn computational linguistics, some approaches to characterizing selectional preferenceshave used the symmetric nature of their models to characterize nouns in terms of theverbs that they use (Hindle 1990; Rooth et al 1999).
However, they do not explicitlycompare the two types of preferences.
Also, there are approaches using selectionalpreference information, in particular for word sense disambiguation and related tasks,that could be characterized as using regular along with inverse selectional preferences(Dligach and Palmer 2008; Erk and Pad?
2008; Nastase 2008).
By comparing selectionalpreference model performance on the tasks of predicting inverse and regular selectionalpreferences in Sections 7.3?7.5, we hope to contribute to an understanding of what canbe achieved by using inverse preferences in word sense analysis tasks.At the same time, inverse selectional preferences have been the object of fruitfulresearch in both psycholinguistics and theoretical linguistics.
In psycholinguistics, aparticularly plausible argument for the existence of expectations of nouns for theirpredicates in human language processing is head-final word order (as in Japanese orin German subordinate clauses), where hearers may encounter all objects before thehead.
It is likely that these objects are immediately integrated into a preliminary eventstructure with an assumed predicate instead of being stored in short-term memory untilthe predicate is encountered (Konieczny and D?ring 2003; Nakatani and Gibson 2009).Another strand of work is McRae et al (2001, 2005), who have studied priming of verbsfrom nouns.
They found that a noun engenders priming of verbs for which it is a typicalagent, patient, instrument, or location.In theoretical linguistics, the idea of event knowledge being encoded in the lex-ical entries of nouns has been formulated in the context of Pustejovsky?s generativelexicon (Pustejovsky 1995), where the qualia roles TELIC and AGENTIVE provide infor-mation about the typical use of an object (book: read) and its construction (book: write),753Computational Linguistics Volume 36, Number 4respectively.
Pustejovsky uses this knowledge to account, for example, for the interpre-tation of logical metonymy (begin a book).
Although qualia roles are instantiated withindividual predicates rather than characterizations of all possible events, constructionand use are arguably two very salient events for an object.
Through the data explorationin Section 7.2, we hope to contribute to a linguistic characterization of inverse selectionalpreferences.7.2 Empirical Analysis of Inverse Selectional PreferencesThe first question we ask concerns the selectional preference strength of regular andinverse selectional preferences, using the measure introduced by Resnik (1996) to de-termine the degree to which verbs select for nouns, and vice versa.
As verb?role pairs,we re-use the same 100 pairs that were used for the pseudo-disambiguation task inExperiment 1.
For the comparison, we randomly sample a total of 100 noun/inverse-role pairs from the BNC, using the same five frequency bands as for the verbs (50?100, 100?200, 200?500, 500?1,000, >1,000).
The sample contains approximately the samenumber of (inverse) subject and object roles.We adapt the selectional preference strength measure from Equation (1) to ourcase: Unlike Resnik, we compute KL divergence not on a distribution across WordNetsynsets, but on a distribution across lemmas.SelStr(w1, r) = D(P(w2|w1, r)||P(w2|r)) (7)For regular selectional preferences, w1 is a verb lemma, w2 a noun lemma, and r a role.For inverse preferences, w1 is a noun lemma, w2 a verb lemma, and r an inverse role.SelStr(w1, r) can be interpreted as a measure of the degree to which w1 has selectionalpreferences concerning the role r. We induce the probability distributions throughmaximum likelihood estimation on the BNC.We can expect to see the same overall tendency in regular and inverse selec-tional preference strength.
It is not possible that inverse selectional preference strengthwould be uniform throughout if regular selectional preference strength varied betweenverbs.
After all, if we fix the relation r for the time being, P(v|n) and P(n|v) are re-lated through Bayes?
formula.
Instead, the questions we will ask are more specific.Are regular and inverse preference strengths similar in size?
Are regular and inversepreference strengths similar by frequency band?that is, do frequent nouns behavesimilarly to frequent verbs?
And what effects do we see of the prior distributions P(n|r)and P(v|r)?Table 11 shows the range of selectional preference strengths found in each frequencyband for verbs and nouns.
As expected, we see substantial strengths in both regularand inverse preferences.
Both parts of speech show the same pattern of decreasing KLdivergences for higher-frequency words, presumably because frequent words tend to bepolysemous, and can combine with many different words.
However, the strengths forinverse selectional preferences are in general lower than those for regular preferences.One possible reason for this is that the number of nouns seen with a verb?rolepair might differ, in general, from the number of verbs seen with each noun?rolepair.
However, we find that verbs and nouns occur with roughly the same number ofassociates in the frequency bands up to the 200?500 band.
In the band 500?1,000, verbsappear with roughly one third more nouns than nouns appear with verbs, and in theband of 1,000 occurrences or more, verbs appear with twice as many nouns (on average)754Erk, Pad?, and Pad?
A Flexible, Corpus-Driven Model of Selectional PreferencesTable 11Minimal, median, and maximal selectional preference strength (measured in terms of KLdivergence) in a sample of 100 verbs and 100 nouns (20 lemmas each per frequency band).Band Verbs Nounsmin median max min median max50?100 4.5 7.4 8.8 3.7 4.8 6.3100?200 3.9 5.8 7.6 2.7 3.8 5.0200?500 3.3 5.2 6.9 2.4 3.3 4.7500?1,000 2.4 4.4 5.9 1.9 2.9 4.11,000?
1.8 3.6 6.2 1.4 2.3 3.7as nouns appear with verbs in this band (1,189 vs. 636).
Incidentally, the fact that thehighest-frequency verbs (which also tend to be the most ambiguous) appear in a muchlarger number of contexts than the highest-frequency nouns could be a contributingfactor to the well-known problem that verbs are harder to disambiguate than nouns.For the lower frequency bands, number of associates is unlikely to be the reason forthe weaker inverse preferences.
Instead, a more likely reason for the overall weakerinverse preferences lies in the overall distributions of nouns and verbs in the BNC.Both show a Zipfian distribution, but there are 15,570 verbs as opposed to 455,173nouns.
Recall that KL divergence will be high whenever the individual terms p(w2|w1,r)p(w2|r)to be summed are large.
This, in turn, is the case when p(w2|r) is small.
And p(w2|r) maybe small when the distribution p(w2|r) ranges over a larger number of words w2.
Forregular selectional preferences, the w2 are nouns, and for inverse preferences the w2 areverbs.
Because there are many more nouns than verbs, the denominator p(w2|r) tends tobe smaller for regular preferences.To get a clearer understanding of how inverse selectional preferences compare toregular selectional preferences, we next do a qualitative analysis, looking at associationstrength SelAssoc for individual triples verb?role?noun and noun?inverse-role?verb.We adapt Equation (2) to the lexicon-free case and obtainSelAssoc(w1, r, w2) =1SelStr(w1, r)P(w2|w1, r) logP(w2|w1, r)P(w2|r)(8)Table 12 shows the five strongest associates for one verb?role pair and one noun?rolepair from each frequency band.
The associates on both sides of the table generallyare semantically coherent and make intuitive sense.
However, there is an interestingdifference between the verbs and nouns: We find that the nouns?
preferred verbs canoften be grouped loosely into several meaning clusters, whereas the verbs?
associatestend to group into one cluster per grammatical function.
For example, predicates takingwheat as objects fall into those describing production (grow, sow) and those describingprocessing (shred, grind, mill).
Similarly, the predicates found for pill either concerningestion (take, swallow, pop), prescription, or idiomatic usage.
In contrast, the objects ofrebut describe different kinds of statements, and the objects of celebrate are anniversariesand other special events.
Another observation that we can make in Table 12 is thatthe nouns?
most preferred associates have a similarly large share in the nouns?
overallselectional preference strength as the verbs?
most preferred associates have in the verbs?selectional preference strength.
This indicates that the distribution of selectional prefer-ences is similarly skewed towards the most preferred associate for verbs and nouns.755Computational Linguistics Volume 36, Number 4Table 12Examples of regular and inverse selectional preferences from different frequency bands forargument positions of nouns and verbs: overall selectional preference strength SelStr and mosthighly associated fillers with association strengths SelAssoc.Band Verbs Nouns50?100rebut?obj, SelStr(w)= 7.43 wreckage?obj?1, SelStr(w)= 5.91presumption 0.283 survey 0.126allegation 0.088 examine 0.089charge 0.082 sift 0.075criticism 0.049 clear 0.056claim 0.041 sight 0.051100?200enunciate?obj, SelStr(w)= 6.89 wheat?obj?1, SelStr(w)= 5.00principle 0.242 grow 0.184word 0.085 shred 0.049theory 0.034 grind 0.049philosophy 0.034 mill 0.042policy 0.029 sow 0.040200?500break_with?obj, SelStr(w)= 6.92 pill?obj?1, SelStr(w)= 4.15tradition 0.237 take 0.290past 0.054 swallow 0.165precedent 0.035 sweeten 0.070convention 0.035 prescribe 0.049Rome 0.022 pop 0.028500?1,000commence?obj, SelStr(w)= 5.92 dividend?obj?1, SelStr(w)= 4.10proceedings 0.185 pay 0.508action 0.051 declare 0.064work 0.043 receive 0.064proceeding 0.041 recommend 0.054operation 0.033 raise 0.0231,000?celebrate?obj, SelStr(w)= 6.23 requirement?obj?1, SelStr(w)= 3.24anniversary 0.177 meet 0.332birthday 0.170 satisfy 0.015centenary 0.046 comply_with 0.093victory 0.033 fulfill 0.061mass 0.028 impose 0.028In sum, we find that inverse selectional preferences have weaker overall selectionalpreference strength than regular preferences, but that may be due more to specifics ofthe formula used rather than the skewness towards preferred role fillers.
Two differ-ences do emerge, though.
First, noun selectional preferences show more semantic fillersets than verb preferences.
Second, the highest frequency verbs appear with many moredifferent associates than the highest frequency nouns.7.3 Modeling Inverse Selectional PreferencesIn the rest of this section, we test selectional preference models on the task of pre-dicting inverse selectional preferences in a pseudo-disambiguation task, and comparethe results to the performance on predicting regular preferences in Experiment 1.
Wedo not repeat Experiment 2 even though it would have been technically possible to756Erk, Pad?, and Pad?
A Flexible, Corpus-Driven Model of Selectional Preferencesre-use the McRae and Pado data sets and predict plausibility judgments through inversepreferences.
However, the data sets combine each verb with both plausible and im-plausible nouns, but they do not combine each noun with different verbs in a balancedfashion, so a repetition of Experiment 2 with inverse preferences would not be veryinformative.For the pseudo-disambiguation experiment, we focus on the EPP model.
Distri-butional models can, in general, be used straightforwardly to model both regularand inverse selectional preferences.
This is different for models like RESNIK that usethe WordNet noun hierarchy to represent regular selectional preferences.
To modelinverse preferences, it would be necessary to use the WordNet verb hierarchy.
However,WordNet organizes verbs in a comparatively flat, unconnected hierarchy with a highbranching factor formed by the hypernymy/troponymy (?type of?)
relation.
This makeseffective generalization difficult, in particular in conjunction with the marked variationin the set of preferred predicates that we observed for inverse selectional preferences inSection 7.2.We adapt the formulation of the EPP model to the inverse selectional preferencecase as follows.
Let a stand for a noun, r for an inverse argument position of thisnoun, and Seenpreds(r, a) for the set of predicates seen with noun a and role r. Then theselectional preference SelprefEPP of (r, a) for a verb v0 is defined in parallel to Equation (6)as weighted average similarity to seen verbs:SelprefEPPr,a(v0) =?v?Seenpreds(r,a)wtr,a(v)Zr,asim(v, v0) (9)with Zr,a =?v?Seenpreds(r,a) wtr,a(v) as the normalization constant.7.4 Pseudo-Disambiguation: Experimental SetupWe evaluate inverse selectional preferences on a pseudo-disambiguation task that isset up completely analogously to our experiments on regular preferences in Section 5:given a noun, an inverse argument position, one verb observed in this position, and aconfounder verb, distinguish between the two verbs.
We use the 100 nouns sampledacross five frequency bands that we already used in Section 7.2.
We experiment withboth WORDSPACE and DEPSPACE models, but restrict our attention to DISCR weighting,which showed good results in Experiment 1.In Section 5, we experimented on two different primary corpora, the BNC (SYNPRIMARY setting) and FrameNet (SEM PRIMARY setting).
Subsequently, we will use theSYN PRIMARY setting again, but not the SEM PRIMARY setting.
In the SEM PRIMARYsetting, the roles are FrameNet frame elements (semantic roles).
However, frame ele-ments are specific to a single frame, for example, the frame element ROPE belongs tothe frame ROPE_MANIPULATION.15 It would thus be pointless to predict a verb framegiven a noun and a frame element name, as the frame element already gives away theframe.15 It is possible for multiple frame elements to share a name, for example there are multiple frames with aframe element named THEME.
However, conceptually, this is only a shared name, not a shared role acrossframes.757Computational Linguistics Volume 36, Number 47.5 Pseudo-Disambiguation: Results and DiscussionTable 13 shows the results of testing the EPP model for inverse selectional preferenceson pseudo-disambiguation.
Coverage is very good for all model variants, similarly toExperiment 1.
The error rates, as well, are close to those for the regular preferences inthe SYN PRIMARY setting (cf.
Table 4).
The best model there (DEPSPACE, PCA, nGCMwith DISCR weighting) achieved an error rate of 25.6%, and the best model for inversepreferences (WORDSPACE, Lin with DISCR weighting) reaches an error rate of 27.2%here.
Lin shows the best error rates in all conditions, closely followed by nGCM (thedifference is significant in WORDSPACE and the reduced DEPSPACE, but not significantin the unreduced DEPSPACE).
The Hindle similarity measure again brings up the rear.In PCA-transformed spaces, the error rates are similar across all similarity measuresexcept for Hindle, as in Experiment 1.WORDSPACEs yield better results than DEPSPACEs here, in contrast to Experiment 1.The best WORDSPACE model (Lin without PCA) reaches significantly better error rates(p ?
0.01) than the best DEPSPACE model (Lin with PCA).
We think that the reason forthis lies in the fact that for inverse selectional preferences, the true associate and theconfounder that need to be distinguished in the pseudo-disambiguation task are verbsrather than nouns.
A noun will probably have more other nouns in a bag-of-wordscontext window than a verb would other verbs, which will make it easier to distinguishverbs in a WORDSPACE than to distinguish nouns.
A DEPSPACE, in contrast, will bringout differences in the immediate syntactic neighborhood of nouns even if they occur inthe same sentence.8.
ConclusionIn this article, we have presented a similarity-based model of selectional preferences,EPP.
It computes the selectional fit of a candidate role filler as a weighted sum of seman-tic similarities to headwords observed in a corpus, in a straightforward implementationTable 13Pseudo-disambiguation results for inverse selectional preferences (BNC as primary andsecondary corpus, DISCR weighting).
ER = Error rate; Cov = Coverage.Dimensions Similarity DEPSPACE WORDSPACEER (%) Cov (%) ER (%) Cov (%)Original2,000 dimensionsCosine 37.4 99.0 34.0 99.1Dice 42.4 98.8 43.4 98.7nGCM 33.7 99.3 31.5 99.3Hindle 48.8 96.0 52.2 94.6Jaccard 36.7 99.4 44.9 98.7Lin 32.8 98.9 27.2 98.9PCA500 dimensionsCosine 35.2 99.0 31.3 99.4Dice 35.0 99.6 32.9 99.8nGCM 32.4 99.2 30.3 99.6Hindle 44.2 99.0 48.7 99.1Jaccard 34.8 99.6 32.6 99.8Lin 30.6 99.8 28.8 99.8758Erk, Pad?, and Pad?
A Flexible, Corpus-Driven Model of Selectional Preferencesof the intuition that plausibility judgments should generalize to fillers with similarmeaning.
Our model is simple and easy to compute.
In common with other distri-butional models like Rooth et al (1999), it does not depend on lexical resources.
Ourmodel derives additional flexibility from distinguishing between a primary corpus (forobserving headwords) and a generalization corpus (for inducing semantic similarities).This allows it to use primary corpora with deeper semantic annotation that are too smallas a basis for computing vector space representations.We have evaluated the EPP model on two tasks, a pseudo-disambiguation taskthat can be viewed as an abstraction of both word sense disambiguation and semanticrole labeling, as well as on the prediction of human plausibility judgments.
The modelachieves similar error rates to the semantic hierarchy?based RESNIK model, at consid-erably higher coverage, and it achieves lower error rates than the ROOTH ET AL.
softclustering model.
The semantic role?based PADO ET AL.
model, although highly accu-rate in its predictions, has much lower coverage and needs a semantically annotatedcorpus as a basis.
We have also demonstrated that our model is able to meaningfullymodel inverse selectional preferences, that is, expectations of nouns about verbs forwhich they appear as arguments.With respect to parameter settings of the EPP model, we find consistent patternsacross the three tasks we have considered.
nGCM, Lin, and Cosine are the best-performing similarity measures throughout.
The good performance of the nGCM mea-sure, an exponential similarity measure, is particularly noteworthy.
We found it to workwell on data sets that are sparse and not too noisy, whereas the Lin similarity measureachieved better performance when the data was noisy (see Section 5.4 for details).
Di-mensionality reduction (PCA) on the vector space raises the performance of the Jaccardand Dice similarity measures to a similar level as the best three.
More importantly, PCAneutralizes a strong frequency bias that otherwise leads to a large performance dropon rare arguments.
Concerning weighting schemes, we found that frequency-basedweighting works well when the data is either clean or not too sparse.
In the face of sparsenoisy data, DISCR weighting (a variant of tf/idf) is helpful.
Comparing bag-of-words?based and dependency-based vector spaces, DEPSPACEs are sparser but cleaner thanWORDSPACEs.
Accordingly, DEPSPACEs are at an advantage when many headwords areavailable, making efficient use of this information, whereas WORDSPACEs work betterfor predicates with few seen headwords because they are less affected by sparseness.We conclude with two open questions.
The first question concerns the appropriaterepresentation of selectional preferences for polysemous verbs such as address, whosedirect object can either be a person, or a problem.
Polysemy leads to headwords withlower similarity among them than for non-polysemous verbs, which in turn can leadto artificially low plausibilities for all fillers.
In the SEM PRIMARY setting, occurrencesof polysemous verbs are separated into different frames.
In future work, we hope toimprove our SYN PRIMARY setting models by clustering the seen headwords, and thencomputing plausibility of new headwords relative to the nearest cluster.A second question is the usefulness of inverse selectional preferences for the ac-quisition of fine-grained information about nouns.
As we discussed in Section 7, thepreferred verbs for a noun can often be grouped into meaning clusters.
In future work,we plan to investigate whether there are groups of predicates that recur across similarnouns, and how they can be characterized.
We expect some groups to correspond toPustejovsky?s qualia (Pustejovsky 1995), which constitute particularly salient eventsfor an object, namely, their creation and typical use.
However, we expect corpus datato yield a more complex picture of the events connected to a noun, which manifestthemselves in the form of additional, more specific meaning clusters.759Computational Linguistics Volume 36, Number 4AcknowledgmentsWe would like to thank Detlef Prescherand Carsten Brockmann for theirimplementations of the ROOTH ET AL.and RESNIK models, respectively.
Weare also grateful to Nate Chambers andYves Peirsman, as well as to theanonymous reviewers, for theircomments and suggestions.ReferencesAbe, Naoki and Hang Li.
1996.
Learningword association norms using tree cut pairmodels.
In Proceedings of the 10thInternational Conference on MachineLearning, pages 3?11, Bari.Baroni, Marco, Silvia Bernardini, AdrianoFerraresi, and Eros Zanchetta.
2009.
Thewacky wide Web: A collection of verylarge linguistically processed Web-crawledcorpora.
Language Resources and Evaluation,43(3):209?222.Bergsma, Shane, Dekang Lin, and RandyGoebel.
2008.
Discriminative learning ofselectional preference from unlabeled text.In Proceedings of the 13th Conference onEmpirical Methods in Natural LanguageProcessing, pages 59?68, Honolulu, HI.Best, John and D. E. Roberts.
1975.
AlgorithmAS 89: The upper tail probabilities ofSpearman?s Rho.
Applied Statistics,24:377?379.Briscoe, Ted and Bran Boguraev, editors.1989.
Computational Lexicography forNatural Language Processing.
LongmanPublishing Group, New York.Brockmann, Carsten and Mirella Lapata.2003.
Evaluating and combiningapproaches to selectional preferenceacquisition.
In Proceedings of the 16thMeeting of the European Chapter of theAssociation for Computational Linguistics,pages 27?34, Budapest.Burnard, Lou, 1995.
User?s Guide for theBritish National Corpus.
British NationalCorpus Consortium, Oxford UniversityComputing Services.Ciaramita, Massimiliano and Mark Johnson.2000.
Explaining away ambiguity:Learning verb selectional preferencewith Bayesian networks.
In Proceedingsof the 18th International Conference onComputational Linguistics, pages 187?193,Saarbr?cken.Clark, Stephen and David Weir.
2001.Class-based probability estimation using asemantic hierarchy.
In Proceedings of the2nd Annual Meeting of the North AmericanChapter of the Association for ComputationalLinguistics, pages 95?102, Pittsburgh, PA.Collins, Michael.
1997.
Three generative,lexicalised models for statistical parsing.In Proceedings of the 35th Annual Meeting ofthe Association for Computational Linguistics,pages 16?23, Madrid.Curran, James.
2004.
From Distributional toSemantic Similarity.
Ph.D. thesis, Universityof Edinburgh.Daelemans, Walter and Antal van denBosch.
2005.
Memory-Based LanguageProcessing.
Cambridge University Press,Cambridge, UK.Dagan, Ido, Lillian Lee, and Fernando C. N.Pereira.
1999.
Similarity-based modelsof word cooccurrence probabilities.Machine Learning, 34(1):34?69.Dligach, Dmitriy and Martha Palmer.
2008.Novel semantic features for verb sensedisambiguation.
In Proceedings of the46th Annual Meeting of the Association forComputational Linguistics:Human LanguageTechnologies, Short Papers, pages 29?32,Columbus, OH.Dunning, Ted.
1993.
Accurate methods forthe statistics of surprise and coincidence.Computational Linguistics, 19(1):61?74.Efron, Bradley and Robert Tibshirani.
1994.An Introduction to the Bootstrap.Monographs on Statistics and AppliedProbability 57.
Chapman & Hall, London.Erk, Katrin.
2007.
A simple, similarity-basedmodel for selectional preferences.
InProceedings of the Annual Meeting of theAssociation for Computational Linguistics,pages 216?223, Prague.Erk, Katrin and Sebastian Pad?.
2008.A structured vector space model forword meaning in context.
In Proceedingsof the 13th Conference on EmpiricalMethods in Natural Language Processing,pages 897?906, Honolulu, HI.Fillmore, Charles J., Christopher R. Johnson,and Miriam R. L. Petruck.
2003.Background to FrameNet.
InternationalJournal of Lexicography, 16:235?250.Firth, John Rupert.
1957.
A synopsis oflinguistic theory, 1930?1955.
In PhilologicalSociety, editor, Studies in LinguisticAnalysis.
Blackwell, Oxford, pages 1?32.Garnsey, Susan, Neal Pearlmutter, ElizabethMyers, and Melanie Lotocky.
1997.
Thecontributions of verb bias and plausibilityto the comprehension of temporarilyambiguous sentences.
Journal of Memoryand Language, 37:58?93.Gildea, Daniel and Daniel Jurafsky.
2002.Automatic labeling of semantic roles.Computational Linguistics, 28(3):245?288.760Erk, Pad?, and Pad?
A Flexible, Corpus-Driven Model of Selectional PreferencesGrefenstette, Gregory.
1994.
Explorations inAutomatic Thesaurus Discovery.
KluwerAcademic Publishers, Dordrecht.Grishman, Ralph and John Sterling.
1992.Acquisition of selectional patterns.
InProceedings of the 14th InternationalConference on Computational Linguistics,pages 658?664, Nantes.Harris, Zellig.
1968.
Mathematical Structure ofLanguage.
Wiley, New York.Hay, Jennifer, Aaron Nolan, and KatieDrager.
2006.
From fush to feesh: Exemplarpriming in speech perception.
TheLinguistic Review, 23(3):351?379.Hindle, Donald.
1990.
Noun classificationfrom predicate-argument structures.
InProceedings of the 28th Annual Meeting of theAssociation for Computational Linguistics,pages 268?275, Pittsburgh, PA.Hindle, Donald and Mats Rooth.
1993.Structural ambiguity and lexical relations.Computational Linguistics, 19(1):103?120.Katz, Jerrold J. and Jerry A. Fodor.
1963.
Thestructure of a semantic theory.
Language,39(2):170?210.Katz, Jerrold J. and Paul M. Postal.
1964.
AnIntegrated Theory of Linguistic Descriptions.Research Monograph No.
26.
MIT Press,Cambridge, MA.Konieczny, Lars and Philipp D?ring.
2003.Anticipation of clause-final heads.Evidence from eye-tracking and SRNs.In Proceedings of the 4th InternationalConference on Cognitive Science,pages 330?335, Sydney.Lakoff, George and Mark Johnson.
1980.Metaphors We Live By.
University ofChicago Press, Chicago, IL.Landauer, Thomas and Susan Dumais.
1997.A solution to Plato?s problem: The latentsemantic analysis theory of acquisition,induction, and representation ofknowledge.
Psychological Review,104(2):211?240.Lee, Lillian.
1999.
Measures of distributionalsimilarity.
In Proceedings of the 31st AnnualMeeting of the Association for ComputationalLinguistics, pages 25?32, College Park, MA.Lin, Dekang.
1993.
Principle-based parsingwithout overgeneration.
In Proceedings ofthe 31st Annual Meeting of the Association forComputational Linguistics, pages 112?120,Columbus, OH.Lin, Dekang.
1998.
Automatic retrieval andclustering of similar words.
In Proceedingsof the Joint Annual Meeting of the Associationfor Computational Linguistics andInternational Conference on ComputationalLinguistics, pages 768?774, Montreal.Lowe, Will.
2001.
Towards a theory ofsemantic space.
In Proceedings of the 23rdAnnual Conference of the Cognitive ScienceSociety, pages 576?581, Edinburgh.Lowe, Will and Scott McDonald.
2000.
Thedirect route: Mediated priming in semanticspace.
In Proceedings of the 22nd AnnualConference of the Cognitive Science Society,pages 675?680, Philadelphia, PA.Lund, Kevin and Curt Burgess.
1996.Producing high-dimensional semanticspaces from lexical co-occurrence.
BehaviorResearch Methods, Instruments, andComputers, 28:203?208.McCarthy, Diana.
2000.
Using semanticpreferences to identify verbal participationin role switching alternations.
InProceedings of the 1st Annual Meeting of theNorth American Chapter of the Association forComputational Linguistics, pages 256?263,Seattle, WA.McCarthy, Diana and John Carroll.
2003.Disambiguating nouns, verbs, andadjectives using automatically acquiredselectional preferences.
ComputationalLinguistics, 29(4):639?654.McCarthy, Diana, Rob Koeling, Julie Weeds,and John Carroll.
2004.
Findingpredominant word senses in untaggedtext.
In Proceedings of the 42th AnnualMeeting of the Association for ComputationalLinguistics, pages 279?286, Barcelona.McCarthy, Diana, Sriram Venkatapathy, andAravind K. Joshi.
2007.
Detectingcompositionality of verb-objectcombinations using selectionalpreferences.
In Proceedings of the 12th JointConference on Empirical Methods in NaturalLanguage Processing and Conference onNatural Language Learning, pages 369?379,Prague.McDonald, Scott and Chris Brew.
2004.
Adistributional model of semantic contexteffects in lexical processing.
In Proceedingsof the 42th Annual Meeting of the Associationfor Computational Linguistics, pages 17?24,Barcelona.McRae, Ken, Todd Ferretti, and LianeAmyote.
1997.
Thematic roles asverb-specific concepts.
Language andCognitive Processes, 12(2/3):137?176.McRae, Ken, Mary Hare, Jeffrey Elman, andTodd Ferretti.
2005.
A basis for generatingexpectancies for verbs from nouns.Memory and Cognition, 33(7):1174?1184.McRae, Ken, Mary Hare, Todd Ferretti, andJeffrey Elman.
2001.
Activating verbstypical agents, patients, instruments, andlocations via event schemas.
In Proceedings761Computational Linguistics Volume 36, Number 4of the Twenty-Third Annual Conference of theCognitive Science Society, pages 617?622,Mahwah, NJ.McRae, Ken, Michael Spivey-Knowlton, andMichael Tanenhaus.
1998.
Modeling theinfluence of thematic fit (and otherconstraints) in on-line sentencecomprehension.
Journal of Memory andLanguage, 38:283?312.Miller, George, Richard Beckwith, ChristianeFellbaum, Derek Gross, and KatherineMiller.
1990.
Five papers on WordNet.International Journal of Lexicography,3(4):235?312.Nakatani, Kentaro and Edward Gibson.
2009.An on-line study of Japanese nestingcomplexity.
Cognitive Science, 1(34):94?112.Nastase, Vivi.
2008.
Unsupervised all-wordsword sense disambiguation withgrammatical dependencies.
In Proceedingsof the 3rd International Joint Conferenceon Natural Language Processing,pages 757?762, Honolulu, HI.Nosofsky, Robert.
1986.
Attention, similarity,and the identification-categorizationrelationship.
Journal of ExperimentalPsychology: General, 115(1):39?57.Pad?, Sebastian and Mirella Lapata.
2007.Dependency-based construction ofsemantic space models.
ComputationalLinguistics, 33(2):161?199.Pad?, Sebastian, Ulrike Pad?, and Katrin Erk.2007.
Flexible, corpus-based modelling ofhuman plausibility judgements.
InProceedings of the 12th Joint Conference onEmpirical Methods in Natural LanguageProcessing and Conference on NaturalLanguage Learning, pages 400?409, Prague.Pad?, Ulrike.
2007.
The Integration of Syntaxand Semantic Plausibility in a Wide-CoverageModel of Human Sentence Processing.
Ph.D.thesis, Saarland University, Saarbr?cken,Germany.Pad?, Ulrike, Matthew W. Crocker, andFrank Keller.
2009.
A probabilistic modelof semantic plausibility in sentenceprocessing.
Cognitive Science, 33:794?838.Pantel, Patrick, Rahul Bhagat, BonaventuraCoppola, Timothy Chklovski, andEduard Hovy.
2007.
ISP: Learninginferential selectional preferences.
InProceedings of the Joint Human LanguageTechnology Conference and Annual Meetingof the North American Chapter of theAssociation for Computational Linguistics,pages 564?571, Rochester, NY.Pereira, Fernando, Naftali Tishby, and LillianLee.
1993.
Distributional clustering ofEnglish words.
In Proceedings of the 31stAnnual Meeting of the Association forComputational Linguistics, pages 183?190,Columbus, OH.Pustejovsky, James.
1995.
The GenerativeLexicon.
MIT Press, Cambridge, MA.Resnik, Philip.
1996.
Selectional constraints:An information-theoretic model and itscomputational realization.
Cognition,61:127?159.Rooth, Mats, Stefan Riezler, Detlef Prescher,Glenn Carroll, and Franz Beil.
1999.Inducing a semantically annotated lexiconvia EM-based clustering.
In Proceedings ofthe 37th Annual Meeting of the Association forComputational Linguistics, pages 104?111,College Park, MA.Salton, Gerard, Anita Wong, and Chung-ShuYang.
1975.
A vector-space model forinformation retrieval.
Journal of theAmerican Society for Information Science,18:613?620.Schulte im Walde, Sabine.
2010.
Comparingcomputational approaches to selectionalpreferences: Second-order co-occurrencevs.
latent semantic clusters.
In Proceedingsof the 7th International Conference onLanguage Resources and Evaluation,pages 1381?1388, Valleta.Schulte im Walde, Sabine, Christian Hying,Christian Scheible, and Helmut Schmid.2008.
Combining EM training and theMDL principle for an automatic verbclassification incorporating selectionalpreferences.
In Proceedings of the 46thAnnual Meeting of the Association forComputational Linguistics, pages 496?504,Columbus, OH.Shepard, Roger.
1987.
Towards a universallaw of generalization for psychologicalscience.
Science, 237(4820):1317?1323.Stolcke, Andreas.
2002.
SRILM?anextensible language modeling toolkit.
InProceedings of the International Conference onSpoken Language Processing, pages 901?904,Denver, CO.Toutanova, Kristina, Christoper D. Manning,Dan Flickinger, and Stephan Oepen.
2005.Stochastic HPSG parse selection using theRedwoods corpus.
Journal of Research onLanguage and Computation, 3(1):83?105.Trueswell, John, Michael Tanenhaus, andSusan Garnsey.
1994.
Semantic influenceson parsing: Use of thematic roleinformation in syntactic ambiguityresolution.
Journal of Memory and Language,33:285?318.Vandekerckhove, Bram, Dominiek Sandra,and Walter Daelemans.
2009.
A robust andextensible exemplar-based model of762Erk, Pad?, and Pad?
A Flexible, Corpus-Driven Model of Selectional Preferencesthematic fit.
In Proceedings of the 12thMeeting of the European Chapter of theAssociation for Computational Linguistics,pages 826?834, Athens.Wilks, Yorick.
1975.
Preference semantics.In E. Keenan, editor, Formal Semantics ofNatural Language.
Cambridge UniversityPress, Cambridge, UK, pages 329?350.Yarowsky, David.
1993.
One sense percollocation.
In Proceedings of the ARPAHuman Language Technology Workshop,pages 266?271, Princeton, NJ.Zanzotto, Fabio Massimo, MarcoPennacchiotti, and Maria Teresa Pazienza.2006.
Discovering asymmetric entailmentrelations between verbs using selectionalpreferences.
In Proceedings of the JointAnnual Meeting of the Association forComputational Linguistics and InternationalConference on Computational Linguistics,pages 849?856, Sydney.Zapirain, Be?at, Eneko Agirre, and Llu?sM?rquez.
2009.
Generalizing over lexicalfeatures: Selectional preferences forsemantic role classification.
In Proceedingsof the 47th Annual Meeting of the Associationfor Computational Linguistics, pages 73?76,Singapore.763
