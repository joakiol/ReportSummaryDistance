Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 116?125,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsModelling the Lexicon inUnsupervised Part of Speech InductionGreg DubbinDepartment of Computer ScienceUniversity of OxfordUnited KingdomGregory.Dubbin@wolfson.ox.ac.ukPhil BlunsomDepartment of Computer ScienceUniversity of OxfordUnited KingdomPhil.Blunsom@cs.ox.ac.ukAbstractAutomatically inducing the syntactic part-of-speech categories for words in textis a fundamental task in ComputationalLinguistics.
While the performance ofunsupervised tagging models has beenslowly improving, current state-of-the-artsystems make the obviously incorrect as-sumption that all tokens of a given wordtype must share a single part-of-speechtag.
This one-tag-per-type heuristic coun-ters the tendency of Hidden MarkovModel based taggers to over generate tagsfor a given word type.
However, it isclearly incompatible with basic syntactictheory.
In this paper we extend a state-of-the-art Pitman-Yor Hidden Markov Modeltagger with an explicit model of the lexi-con.
In doing so we are able to incorpo-rate a soft bias towards inducing few tagsper type.
We develop a particle filter fordrawing samples from the posterior of ourmodel and present empirical results thatshow that our model is competitive withand faster than the state-of-the-art withoutmaking any unrealistic restrictions.1 IntroductionResearch on the unsupervised induction of part-of-speech (PoS) tags has the potential to im-prove both our understanding of the plausibil-ity of theories of first language acquisition, andNatural Language Processing applications suchas Speech Recognition and Machine Transla-tion.
While there has been much prior workon this task (Brown et al., 1992; Clark, 2003;Christodoulopoulos et al., 2010; Toutanova andJohnson, 2008; Goldwater and Griffiths, 2007;Blunsom and Cohn, 2011), a common thread inmany of these works is that models based on aHidden Markov Model (HMM) graphical struc-ture suffer from a tendency to assign too manydifferent tags to the tokens of a given word type.Models which restrict word types to only occurwith a single tag show a significant increase inperformance, even though this restriction is clearlyat odds with the gold standard labeling (Brown etal., 1992; Clark, 2003; Blunsom and Cohn, 2011).While the empirically observed expectation for thenumber of tags per word type is close to one, thereare many exceptions, e.g.
words that occur as bothnouns and verbs (opening, increase, related etc.
).In this paper we extend the Pitman-Yor HMMtagger (Blunsom and Cohn, 2011) to explicitly in-clude a model of the lexicon that encodes fromwhich tags a word type may be generated.
Foreach word type we draw an ambiguity class whichis the set of tags that it may occur with, captur-ing the fact that words are often ambiguous be-tween certain tags (e.g.
Noun and Verb), whilerarely between others (e.g.
Determiner and Verb).We extend the type based Sequential Monte Carlo(SMC) inference algorithm of Dubbin and Blun-som (2012) to incorporate our model of the lexi-con, removing the need for the heuristic inferencetechnique of Blunsom and Cohn (2011).We start in Section 3 by introducing the origi-nal PYP-HMM model and our extended model ofthe lexicon.
Section 4 introduces a Particle Gibbssampler for this model, a basic SMC method thatgenerates samples from the model?s posterior.
Weevaluate these algorithms in Section 5, analyzingtheir behavior in comparisons to previously pro-posed state-of-the-art approaches.1162 BackgroundFrom the early work in the 1990?s, much of thefocus on unsupervised PoS induction has beenon hidden Markov Models (HMM) (Brown et al.,1992; Kupiec, 1992; Merialdo, 1993).
The HMMhas proven to be a powerful model of PoS tag as-signment.
Successful approaches generally buildupon the HMM model by expanding its contextand smoothing the sparse data.
Constraints suchas tag dictionaries simplify inference by restrictingthe number of tags to explore for each word (Gold-water and Griffiths, 2007).
Ganchev et al.
(2010)used posterior regularization to ensure that wordtypes have a sparse posterior distribution over tags.A similar approach constrains inference to onlyexplore tag assignments such that all tokens of thesame word type are assigned the same tag.
Theseconstraints reduce tag assignment ambiguity whilealso providing a bias towards the natural spar-sity of tag distributions in language (Clark, 2003).However they do not provide a model based solu-tion to tag ambiguity.Recent work encodes similar sparsity infor-mation with non-parametric priors, relying onBayesian inference to achieve strong results with-out any tag dictionaries or constraints (Goldwaterand Griffiths, 2007; Johnson, 2007; Gao and John-son, 2008).
Liang et al.
(2010) propose a type-based approach to this Bayesian inference similarto Brown et al.
(1992), suggesting that there arestrong dependencies between tokens of the sameword-type.
Lee et al.
(2010) demonstrate strongresults with a similar model and the introductionof a one-tag-per-type constraint on inference.Blunsom and Cohn (2011) extend the Bayesianinference approach with a hierarchical non-parametric prior that expands the HMM con-text to trigrams.
However, the hierarchical non-parametric model adds too many long-range de-pendencies for the type-based inference proposedearlier.
The model produces state-of-the art re-sults with a one-tag-per-type constraint, but evenwith this constraint the tag assignments must beroughly inferred from an approximation of the ex-pectations.Ambiguity classes representing the set of tagseach word-type can take aid inference by mak-ing the sparsity between tags and words explicit.Toutanova and Johnson (2008) showed that mod-elling ambiguity classes can lead to positive re-sults with a small tag-dictionary extracted from thedata.
By including ambiguity classes in the model,this approach is able to infer ambiguity classes ofunknown words.Many improvements in part-of-speech induc-tion over the last few years have come from theuse of semi-supervised approaches in the form ofprojecting PoS constraints across languages withparallel corpora (Das and Petrov, 2011) or extract-ing them from the wiktionary (Li et al., 2012).These semi-supervised methods ultimately rely ona strong unsupervised model of PoS as their base.Thus, further improvements in unsupervised mod-els, especially in modelling tag constrains, shouldlead to improvements in semi-supervised part-of-speech induction.We find that modelling the lexicon in part-of-speech inference can lead to more efficient algo-rithms that match the state-of-the-art unsupervisedperformance.
We also note that the lexicon modelrelies heavily on morphological information, andsuffers without it on languages with flexible wordordering.
These results promise further improve-ments with more advanced lexicon models.3 The Pitman-Yor Lexicon HiddenMarkov ModelThis article proposes enhancing the standard Hid-den Markov Model (HMM) by explicitly incorpo-rating a model of the lexicon that consists of wordtypes and their associated tag ambiguity classes.The ambiguity class of a word type is the set ofpossible lexical categories to which tokens of thattype can be assigned.
In this work we aim tolearn the ambiguity classes unsupervised ratherthan have them specified in a tag dictionary.The Lexicon HMM (Lex-HMM) extends thePitman-Yor HMM (PYP-HMM) described byBlunsom and Cohn (2011).
When the ambiguityclass of all of the word types in the lexicon is thecomplete tagset, the two models are the same.3.1 PYP-HMMThe base of the model applies a hierarchicalPitman-Yor process (PYP) prior to a trigram hid-den Markov model to jointly model the distribu-tion of a sequence of latent word tags, t, andword tokens, w. The joint probability definedby the transition, P?
(tl|tn?1, tn?2), and emission,P?
(wn|tn), distributions of a trigram HMM isP?
(t,w) =N+1?n=1P?
(tl|tn?1, tn?2)P?
(wn|tn)117where N = |t| = |w| and the special tag $is added to denote the sentence boundaries.
Themodel defines a generative process in which thetags are selected from a transition distribution,tl|tl?1, tl?2, T , determined by the two previoustags in their history, and the word tokens are se-lected from the emission distribution, wl|tl, E, ofthe latest tag.tn|tn?1, tn?2, T ?
Ttn?1,tn?2wn|tn, E ?
EtnThe PYP-HMM draws the above multinomial dis-tributions from a hierarchical Pitman-Yor Processprior.
The Pitman-Yor prior defines a smooth backoff probability from more complex to less com-plex transition and emission distributions.
In thePYP-HMM trigram model, the transition distri-butions form a hierarchy with trigram transitiondistributions drawn from a PYP with the bigramtransitions as their base distribution, and the bi-gram transitions similarly backing off to the uni-gram transitions.
The hierarchical prior can be in-tuitively understood to smooth the trigram transi-tion distributions with bigram and unigram distri-butions in a similar manner to an ngram languagemodel (Teh, 2006).
This back-off structure greatlyreduces sparsity in the trigram distributions and isachieved by chaining together the PYPs throughtheir base distributions:Tij|aT, bT, Bi?
PYP(aT, bT, Bi)Bi|aB, bB, U ?
PYP(aB, bB, U)U |aU, bU?
PYP(aU, bU,Uniform).Ei|aE, bE, C ?
PYP(aE, bE, Ci),where Tij, Bi, and U are trigram, bigram, and un-igram transition distributions respectively, and Ciis either a uniform distribution (PYP-HMM) or abigram character language model distribution tomodel word morphology (PYP-HMM+LM).Sampling from the posterior of the hierarchi-cal PYP is calculated with a variant of the Chi-nese Restaurant Process (CRP) called the ChineseRestaurant Franchise (CRF) (Teh, 2006; Goldwa-ter et al., 2006).
In the CRP analogy, each latentvariable (tag) in a sequence is represented by acustomer entering a restaurant and sitting at one ofan infinite number of tables.
A customer choosesto sit at a table in a restaurant according to theprobabilityP (zn= k|z1:n?1) ={c?k?an?1+b1 ?
k ?
K?K?a+bn?1+bk = K?+ 1(1)where znis the index of the table chosen by thenth customer to the restaurant, z1:n?1is the seat-ing arrangement of the previous n ?
1 customersto enter, c?kis the count of the customers at tablek, and K?is the total number of tables chosen bythe previous n ?
1 customers.
All customers at atable share the same dish, representing the valueassigned to the latent variables.
When customerssit at an empty table, a new dish is assigned to thattable according to the base distribution of the PYP.To expand the CRP analogy to the CRF for hierar-chical PYPs, when a customer sits at a new table,a new customer enters the restaurant of the PYP ofthe base distribution.Blunsom and Cohn (2011) explored two Gibbssampling methods for inference with the PYP-HMM model.
The first individually samples tagassignments for each token.
The second employsa tactic shown to be effective by earlier works byconstraining inference to only one tag per wordtype (PYP-1HMM).
However marginalizing overall possible table assignments for more than a sin-gle tag is intractable.
Blunsom and Cohn (2011)approximates the PYP-1HMM tag posteriors for aparticular sample according to heuristic fractionaltable counts.
This approximation is shown to beparticularly inaccurate for values of a close to one.3.2 The Lexicon HMMWe define the lexicon to be the set of all wordtypes (W ) and a function (L) which maps eachword type (Wi?
W ) to an element in the powerset of possible tags T ,L : W ?
P(T ).The Lexicon HMM (Lex-HMM) generates thelexicon with all of the word types and their ambi-guity classes before generating the standard HMMparameters.
The set of tags associated with eachword type is referred to as its ambiguity classsi?
T .
The ambiguity classes are generated froma multinomial distribution with a sparse, Pitman-Yor Process prior,si|S ?
SS|aS, bS?
PY P (aS, bS, G)118UBjTijEjw1t1w2t2w3t3...WisiSFigure 1: Lex-HMM Structure: The graphicalstructure of the Lex-HMM model.
In addition tothe trigram transition (Tij) and emission (Ej), themodel includes an ambiguity class (si) for eachword type (Wi) drawn from a distribution S witha PYP prior.where S is the multinomial distribution over allpossible ambiguity classes.
The base distributionof the PYP, G, chooses the size of the ambiguityclass according to a geometric distribution (nor-malized so that the size of the class is at most thenumber of tags |T |).
G assigns uniform probabil-ity to all classes of the same size.
A plate diagramfor this model is shown in Figure 1.This model represents the observation that thereare relatively few distinct ambiguity classes overall of the word types in a corpus.
For example, thefull Penn-Treebank Wall Street Journal (WSJ) cor-pus with 45 possible tags and 49,206 word typeshas only 343 ambiguity classes.
Figure 2 showsthat ambiguity classes in the WSJ have a power-law distribution.
Furthermore, these classes aregenerally small; the average ambiguity class in theWSJ corpus has 2.94 tags.
The PYP prior favorspower-law distributions and the modified geomet-ric base distribution favors smaller class sizes.Once the lexicon is generated, the standardHMM parameters can be generated as describedin section 3.1.
The base emission probabilities Care constrained to fit the generated lexicon.
Thestandard Lex-HMM model emission probabilitiesfor tag tiare uniform over all word types with tiin their ambiguity class.
The character languagemodel presents a challenge because it is non-trivialto renormalise over words with tiin their ambigu-ity class.
In this case word types without tiin their100 101 102 103Rank100101102103104105FrequencyGold LexiconPredicted LexiconLog-Log Ambiguity Class Frequency vs. RankFigure 2: Ambiguity Class Distribution: Log-log plot of ambiguity class frequency over rankfor the Penn-Treebank WSJ Gold Standard lexiconhighlighting a Zipfian distribution and the ambigu-ity of classes extracted from the predicted tags.ambiguity class are assigned an emission probabil-ity of 0 and the model is left deficient.Neither of the samplers proposed by Blunsomand Cohn (2011) and briefly described in section3.1 are well suited to inference with the lexicon.Local Gibbs sampling of individual token-tag as-signments would be very unlikely to explore arange of confusion classes, while the type basedapproximate sample relies on a one-tag-per-typerestriction.
Thus in the next section we extend theParticle Filtering solution presented in Dubbin andBlunsom (2012) to the problem of simultaneousresampling the ambiguity class as well as the tagsfor all tokens of a given type.
This sampler pro-vides both a more attractive inference algorithmfor the original PYP-HMM and one adaptable toour Lex-HMM.4 InferenceTo perform inference with both the lexicon andthe tag assignments, we block sample the ambi-guity class assignment as well as all tag assign-ments for tokens of the same word type.
It wouldbe intractable to exactly calculate the probabili-ties to sample these blocks.
Particle filters are anexample of a Sequential Monte Carlo techniquewhich generates unbiased samples from a distribu-tion without summing over the intractable numberof possibilities.The particle filter samples multiple independentsequences of ambiguity classes and tag assign-ments.
Each sequence of samples, called a parti-119cle, is generated incrementally.
For each particle,the particle filter first samples an ambiguity class,and then samples each tag assignment in sequencebased only on the previous samples in the parti-cle.
The value of the next variable in a sequenceis sampled from a proposal distribution based onlyon the earlier values in the sequence.
Each particleis assigned an importance weight such that a par-ticle sampled proportional to its weight representsan unbiased sample of the true distribution.Each particle represents a specific sampling ofan ambiguity class, tag sequence, tW,p1:n, and thecount deltas, zW,p1:n. The term tW,p1:ndenotes the se-quence of n tags generated for word-type W andstored as part of particle p ?
[1, P ].
The countdeltas store the differences in the seating arrange-ment neccessary to calculate the posterior proba-bilities according to the Chinese restaurant fran-chise described in section 3.1.
The table countsfrom each particle are the only data necessary tocalculate the probabilities described in equation(1).The ambiguity class for a particle is proposedby uniformly sampling one tag from the tagset toadd to or remove from the previous iteration?s am-biguity class with the additional possibility of us-ing the same ambiguity class.
The particle weightsare then set toP (sW,p|S?W)?t?sW,p(et+ 1)#(Et)?t?T?sW,p(et)#(Et)where P (sW,p|S?W) is the probability of the am-biguity class proposed for particle p for word typeW given the ambiguity classes for the rest of thevocabulary, etis the number of word types with tin their ambiguity class, and #(Et) is the numberof tables in the CRP for the emission distributionof tag t. The last two terms of the equation cor-rect for the difference in the base probabilities ofthe words that have already been sampled with adifferent lexicon.At each token occurrence n, the next tag assign-ment, tW,pnfor each particle p ?
[1, P ] is deter-mined by the seating decisions zW,pn, which aremade according the proposal distribution:qW,pn(zW,pn|zW,p1:n?1, z?W) ?P (zW,pn|c?2, c?1, zW,p1:n?1, z?W)?P (c+1n|c?1n, zW,pn, zW,p1:n?1, z?W)?P (c+2n|zW,pn, c+1n, zW,p1:n?1, z?W)?P (wWn|zW,pn, zW,p1:n?1, z?W).In this case, c?knrepresents a tag in the context ofsite tWnoffset by k, while zW,p1:n?1and z?Wrep-resent the table counts from the seating decisionspreviously chosen by particle p and the values atall of the sites where a word token of type Wdoes not appear, respectively.
This proposal dis-tribution ignores changes to the seating arrange-ment between the three transitions involving thesite n. The specific tag assignement, tW, pn, iscompletely determined by the seating decisionssampled according to this proposal distribution.Once all of the particles have been sampled, oneof them is sampled with probability proportionalto its weight.
This final sample is a sample fromthe target distribution.As the Particle Filter is embedded in a Gibbssampler which cycles over all word types this al-gorithm is an instance of Particle Gibbs.
Andrieuet al.
(2010) shows that to ensure the samples gen-erated by SMC for a Gibbs sampler have the tar-get distribution as the invariant density, the par-ticle filter must be modified to perform a condi-tional SMC update.
This means that the particlefilter guarantees that one of the final particles is as-signed the same values as the previous Gibbs iter-ation.
Therefore, a special 0thparticle is automati-cally assigned the value from the prior iteration ofthe Gibbs sampler at each site n, though the pro-posal probability qWn(tW,0n|tW,p1:n?1, zW,p1:n?1) still hasto be calculated to update the weight ?W,pnprop-erly.
This ensures that the sampler has a chance ofreverting to the prior iteration?s sequence.5 Experiments and ResultsWe provide an empirical evaluation of our pro-posed Lex-HMM in terms of the accuracy ofthe taggings learned according to the most pop-ular metric, and the distributions over ambiguityclasses.
Our experimental evaluation considers theimpact of our improved Particle Gibbs inferencealgorithm both for the original PYP-HMM andwhen used for inference in our extended model.We intend to learn whether the lexicon modelcan match or exceed the performance of the othermodels despite focusing on only a subset of thepossible tags each iteration.
We hypothesize thatan accurate lexicon model and the sparsity it in-duces over the number of tags per word-type willimprove the performance over the standard PYP-HMM model while also decreasing training time.Furthermore, our lexicon model is novel, and its120Sampler M-1 Accuracy Time (h)Meta-Model (CGS10) 76.1 ?MEMM (BBDK10) 75.5?40*Lex-HMM 71.1 7.9Type PYP-HMM 70.1 401.2Local PYP-HMM 70.2 8.6PYP-1HMM 75.6 20.6Lex-HMM+LM 77.5 16.9Type PYP-HMM+LM 73.5 446.0PYP-1HMM+LM 77.5 34.9Table 1: M-1 Accuracy on the WSJ Corpus:Comparison of the accuracy of each of the sam-plers with and without the language model emis-sion prior on the English WSJ Corpus.
The secondcolumn reports run time in hours where available*.Note the Lex-HMM+LM model matches the PYP-1HMM+LM approximation despite finishing inhalf the time.
The abbreviations in parenthesesindicate that the results were reported in CGS10(Christodoulopoulos et al., 2010) and BBDK10(Berg-Kirkpatrick et al., 2010) *CGS10 reportsthat the MEMM model takes approximately 40hours on 16 cores.accuracy in representing ambiguity classes is animportant aspect of its performance.
The modelfocuses inference on the most likely tag choices,represented by ambiguity classes.5.1 Unsupervised Part-of-Speech TaggingThe most popular evaluation for unsupervisedpart-of-speech taggers is to induce a tagging fora corpus and compare the induced tags to thoseannotated by a linguist.
As the induced tags aresimply integer labels, we must employ a map-ping between these and the more meaningful syn-tactic categories of the gold standard.
We re-port results using the many-to-one (M-1) met-ric considered most intuitive by the evaluation ofChristodoulopoulos et al.
(2010).
M-1 measuresthe accuracy of the model after mapping each pre-dicted class to its most frequent corresponding tag.While Christodoulopoulos et al.
(2010) found V-measure to be more stable over the number ofparts-of-speech, this effect doesn?t appear whenthe number of tags is constant, as in our case.
Forexperiments on English, we report results on theentire Penn.
Treebank (Marcus et al., 1993).
Forother languages we use the corpora made avail-able for the CoNLL-X Shared Task (Buchholz andMarsi, 2006).
All Lex-HMM results are reportedwith 10 particles as no significant improvementwas found with 50 particles.Table 1 compares the M-1 accuracies of boththe PYP-HMM and the Lex-HMM models on thePenn.
Treebank Wall Street Journal corpus.
Blun-som and Cohn (2011) found that the Local PYP-HMM+LM sampler is unable to mix, achievingaccuracy below 50%, therefore it has been leftout of this analysis.
The Lex-HMM+LM modelachieves the same accuracy as the state-of-the-art PYP-1HMM+LM approximation.
The Lex-HMM+LM?s focus on only the most likely tags foreach word type allows it to finish training in halfthe time as the PYP-1HMM+LM approximationwithout any artificial restrictions on the number oftags per type.
This contrasts with other approachesthat eliminate the constraint at a much greater cost,e.g.
the Type PYP-HMM, the MEMM, and theMeta-Model1The left side of table 2 compares the M-1 accu-racies of the Lex-HMM model to the PYP-HMMmodel.
These models both ignore word morphol-ogy and rely on word order.
The 1HMM approxi-mation achieves the highest average accuracy.
TheLex-HMM model matches or surpasses the type-based PYP-HMM approach in six languages whilerunning much faster due to the particle filter con-sidering a smaller set of parts-of-speech for eachparticle.
However, in the absence of morpho-logical information, the Lex-HMM model has asimilar average accuracy to the local and type-based PYP-HMM samplers.
The especially lowperformance on Hungarian, a language with freeword ordering and strong morphology, suggeststhat the Lex-HMM model struggles to find ambi-guity classes without morphology.
The Lex-HMMmodel has a higher average accuracy than the type-based or local PYP-HMM samplers when Hungar-ian is ignored.The right side of table 2 compares the M-1 ac-curacies of the Lex-HMM+LM model to the PYP-HMM+LM.
The language model leads to consis-tently improved performance for each of the sam-plers excepting the token sampler, which is un-able to mix properly with the additional complex-ity.
The accuracies achieved by the 1HMM+LM1While were unable to get an estimate on the runtime ofthe Meta-Model, it uses a system similar to the feature-basedsystem of the MEMM with an additional feature derived fromthe proposed class from the brown model.
Therefore, it islikely that this model has a similar runtime.121Language Lex-HMM PYP-HMM Local 1HMM Lex-HMM+LM PYP-HMM+LM 1HMM+LMWSJ 71.1 70.1 70.2 75.6 77.5 73.5 77.5Arabic 57.2 57.6 56.2 61.9 62.1 62.7 62.0Bulgarian 67.2 67.8 67.6 71.4 72.7 72.1 76.2Czech 61.3 61.6 64.5 65.4 68.2 67.4 67.9Danish 68.6 70.3 69.1 70.6 74.7 73.1 74.6Dutch 70.3 71.6 64.1 73.2 71.7 71.8 72.9Hungarian 57.9 61.8 64.8 69.6 64.4 69.9 73.2Portuguese 69.5 71.1 68.1 72.0 76.3 73.9 77.1Spanish 73.2 69.1 68.5 74.7 80.0 75.2 78.8Swedish 66.3 63.5 67.6 67.2 70.4 67.6 68.6Average 66.3 (67.2) 66.5 (67.0) 66.1 (66.2) 70.2 (70.3) 71.8 (72.6) 70.7 (70.8) 72.9 (72.9)Table 2: M-1 Accuracy of Lex-HMM and PYP-HMM models: Comparison of M-1 accuracy for thelexicon based model (Lex-HMM) and the PYP-HMM model on several languages.
The Lex-HMM andPYP-HMM columns indicate the results of word type based particle filtering with 10 and 100 particles,respectively, while the Local and 1HMM columns use the token based sampler and the 1HMM approxi-mation described by Blunsom and Cohn (2011).
The token based sampler was run for 500 iterations andthe other samplers for 200.
The percentages in brakets represent the average accuracy over all languagesexcept for Hungarian.sampler represent the previous state-of-the-art.These results show that the Lex-HMM+LM modelachieves state-of-the-art M-1 accuracies on sev-eral datasets, including the English WSJ.
The Lex-HMM+LM model performs nearly as well as, andoften better than, the 1HMM+LM sampler withoutany restrictions on tag assignments.The drastic improvement in the performanceof the Lex-HMM model reinforces our hypothe-sis that morphology is critical to the inference ofambiguity classes.
Without the language modelrepresenting word morphology, the distinction be-tween ambiguity classes is too ambiguous.
Thisleads the sampler to infer an excess of poor am-biguity classes.
For example, the tag assignmentsfrom the Lex-PYP model on the WSJ dataset con-sist of 660 distinct ambiguity classes, while theLex-PYP+LM tag assignments only have 182 dis-tinct ambiguity classes.Note that while the Lex-HMM and Lex-HMM+LM samplers do not have any restrictionson inference, they do not sacrifice time.
The ad-ditional samples generated by the particle filterare mitigated by limiting the number of tags eachparticle must consider.
In practice, this results inthe Lex-HMM samplers with 10 particles runningin half time as the 1HMM samplers.
The Lex-HMM+LM sampler with 10 particles took 16.9hours, while the 1HMM+LM sampler required34.9 hours.
Furthermore, the run time evaluationdoes not take advantage of the inherent distributednature of particle filters.
Each of the particles canbe sampled completely independentally from theothers, making it trivial to run each on a seperatecore.5.2 Lexicon AnalysisWhile section 5.1 demonstrates that the Lex-HMM+LM sampler performs similarly to themore restricted 1HMM+LM, we also seek to eval-uate the accuracy of the lexicon model itself.
Wecompare the ambiguity classes extracted from thegold standard and predicted tag assignments of theWSJ corpus.
We also explore the relationship be-tween the actual and sampled ambiguity classes.The solid curve in figure 2 shows the distribu-tion of the number of word types assigned to eachambiguity set extracted from the gold standard tagassignments from the Penn Treebank Wall StreetJournal corpus.
The straight line strongly indi-cates that ambiguity classes follow a Zipfian dis-tribution.
Figure 2 also graphs the distribution ofthe ambiguity classes extracted from the best tag-assignment prediction from the model.
The pre-dicted graph has a similar shape to the gold stan-dard but represents half as many distinct ambigu-ity classes - 182 versus 343.For a qualitative analysis of the generated lex-icon, table 3 lists frequent ambiguity classes andthe most common words assigned to them.
The 14most frequent ambiguity classes contain only onetag each, the top half of table 3 shows the 5 mostfrequent.
One third of the word-types in the firstfive rows of the table are exactly matched with theambiguity classes from the gold standard.
Most ofthe remaining words in those rows are assigned to122Rank Gold Rank Tags Top Word Types1 1 NNP Mr., Corp. (1), Inc. (.99), Co. (1), Exchange (.99)2 2 NN % (1), company, stock (.99), -RRB- (0), years (0)3 3 JJ new, other, first (.9), most (0), major (1)4 5 NNS companies, prices (1), quarter (0), week (0), investors5 4 CD $ (0), million (1), billion, 31, # (0)15 303 NN, CD yen (.47, 0), dollar (1, 0), 150 (0, 1), 29 (0, 1), 33 (0, 1)16 17 VB, NN plan (.03, .9), offer (.2, .74), issues (0, 0), increase (.34, .66), end (.18, .81)17 115 DT, NNP As (0, 0), One (0, .01), First (0, .82), Big (0, .91), On (0, .01)18 11 NN, JJ market (.99, 0), U.S. (0, 0), bank (1, 0), cash (.98, 0), high (.06, .9)20 22 VBN, JJ estimated (.58, .15), lost (.43, .03), failed (.35, .04), related (.74, .23), re-duced (.57, .12)Table 3: Selection of Predicted Ambiguity Classes: Common ambiguity classes from the predictedpart-of-speech assignments from the WSJ data set, and the five most common word types associatedwith each ambiguity class.
The sets are ranked according to the number of word types associated tothem.
Words in bold are matched to exactly the same ambiguity set in the gold standard.
The lowerfive ambiguity classes are the most common with more than one part-of-speech.
Numbers in parenthesesrepresent the proportion of tokens of that type assigned to each tag in the gold standard for that ambiguityclass.a class representing almost all of the words?
occur-rences in the gold standard, e.g., ?Corp.?
is an NNPin 1514 out of 1521 occurrences.
Some words areassigned to classes with similar parts of speech,e.g.
{NNS} rather than {NN} for week.The lower half of table 3 shows the most fre-quent ambiguity classes with more than a sin-gle tag.
The words assigned to the {NN,CD},{DT,NNP}, and {NN,JJ} classes are not them-selves ambiguous.
Rather words that are unam-biguously one of the two tags are often assignedto an ambiguity class with both.
The most com-mon types in the {NN, CD} set are unambiguouslyeither NN or CD.
In many cases the words aremerged into broader ambiguity classes because theLex-HMM+LM uses the language model to modelthe morphology of words over individual parts-of-speech, rather than entire ambiguity classes.Therefore, a word-type is likely to be assigneda given ambiguity class as long as at least onepart-of-speech in that ambiguity class is associ-ated with morphologically similar words.
Theseresults suggest modifying the Lex-HMM+LM tomodel word morphology over ambiguity classesrather than parts-of-speech.The {VB,NN} and {VBN,JJ} are representativeof true ambiguity classes.
Occurrences of words inthese classes are likely to be either of the possibleparts-of-speech.
These results show that the Lex-HMM is modelling ambiguity classes as intended.6 ConclusionThis paper described an extension to the PYP-HMM part-of-speech model that incorporates asparse prior on the lexicon and an SMC based in-ference algorithm.
These contributions provide amore plausible model of part-of-speech inductionwhich models the true ambiguity of tag to type as-signments without the loss of performance of ear-lier HMM models.
Our empirical evaluation indi-cates that this model is able to meet or exceed theperformance of the previous state-of-the-art acrossa range of language families.In addition to the promising empirical results,our analysis indicates that the model learns ambi-guity classes that are often quite similar to thosein the gold standard.
We believe that further im-provements in both the structure of the lexiconprior and the inference algorithm will lead to addi-tional performance gains.
For example, the modelcould be improved by better modelling the rela-tionship between a word?s morphology and its am-biguity class.
We intend to apply our model torecent semi-supervised approaches which inducepartial tag dictionaries from parallel language data(Das and Petrov, 2011) or the Wiktionary (Li etal., 2012).
We hypothesize that the additional datashould improve the modelled lexicon and conse-quently improve tag assignments.The Lex-HMM models ambiguity classes to fo-cus the sampler on the most likely parts-of-speechfor a given word-type.
In doing so, it matches orimproves on the accuracy of other models whilerunning much faster.123ReferencesChristophe Andrieu, Arnaud Doucet, and RomanHolenstein.
2010.
Particle markov chain montecarlo methods.
Journal Of The Royal Statistical So-ciety Series B, 72(3):269?342.Taylor Berg-Kirkpatrick, Alexandre Bouchard-C?ot?e,John DeNero, and Dan Klein.
2010.
Painless un-supervised learning with features.
In Human Lan-guage Technologies: The 2010 Annual Conferenceof the North American Chapter of the Associationfor Computational Linguistics, pages 582?590, LosAngeles, California, June.
Association for Compu-tational Linguistics.Phil Blunsom and Trevor Cohn.
2011.
A hierarchi-cal Pitman-Yor process hmm for unsupervised partof speech induction.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies, pages865?874, Portland, Oregon, USA, June.
Associationfor Computational Linguistics.Peter F. Brown, Peter V. deSouza, Robert L. Mer-cer, Vincent J. Della Pietra, and Jenifer C. Lai.1992.
Class-based n-gram models of natural lan-guage.
Comput.
Linguist., 18:467?479, December.Sabine Buchholz and Erwin Marsi.
2006.
Conll-xshared task on multilingual dependency parsing.
InProceedings of the Tenth Conference on Computa-tional Natural Language Learning, CoNLL-X ?06,pages 149?164, Morristown, NJ, USA.
Associationfor Computational Linguistics.Christos Christodoulopoulos, Sharon Goldwater, andMark Steedman.
2010.
Two decades of unsu-pervised POS induction: How far have we come?In Proceedings of the 2010 Conference on Empiri-cal Methods in Natural Language Processing, pages575?584, Cambridge, MA, October.
Association forComputational Linguistics.Alexander Clark.
2003.
Combining distributional andmorphological information for part of speech induc-tion.
In Proceedings of the tenth Annual Meetingof the European Association for Computational Lin-guistics (EACL), pages 59?66.Dipanjan Das and Slav Petrov.
2011.
Unsupervisedpart-of-speech tagging with bilingual graph-basedprojections.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Lin-guistics: Human Language Technologies - Volume1, HLT ?11, pages 600?609, Stroudsburg, PA, USA.Association for Computational Linguistics.Gregory Dubbin and Phil Blunsom.
2012.
Unsuper-vised bayesian part of speech inference with particlegibbs.
In Peter A. Flach, Tijl De Bie, and Nello Cris-tianini, editors, ECML/PKDD (1), volume 7523 ofLecture Notes in Computer Science, pages 760?773.Springer.Kuzman Ganchev, Jo?ao Grac?a, Jennifer Gillenwater,and Ben Taskar.
2010.
Posterior regularization forstructured latent variable models.
Journal of Ma-chine Learning Research, 99:2001?2049, August.Jianfeng Gao and Mark Johnson.
2008.
A compar-ison of bayesian estimators for unsupervised hid-den markov model pos taggers.
In Proceedings ofthe Conference on Empirical Methods in NaturalLanguage Processing, EMNLP ?08, pages 344?352,Morristown, NJ, USA.
Association for Computa-tional Linguistics.Sharon Goldwater and Tom Griffiths.
2007.
A fullybayesian approach to unsupervised part-of-speechtagging.
In Proc.
of the 45th Annual Meeting ofthe ACL (ACL-2007), pages 744?751, Prague, CzechRepublic, June.Sharon Goldwater, Tom Griffiths, and Mark John-son.
2006.
Interpolating between types and tokensby estimating power-law generators.
In Y. Weiss,B.
Sch?olkopf, and J. Platt, editors, Advances in Neu-ral Information Processing Systems 18, pages 459?466.
MIT Press, Cambridge, MA.Mark Johnson.
2007.
Why doesnt EM find goodHMM POS-taggers?
In Proc.
of the 2007 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP-2007), pages 296?305, Prague,Czech Republic.Julian Kupiec.
1992.
Robust part-of-speech taggingusing a hidden Markov model.
Computer Speechand Language, 6:225?242.Yoong Keok Lee, Aria Haghighi, and Regina Barzilay.2010.
Simple type-level unsupervised pos tagging.In Proceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, EMNLP?10, pages 853?861, Morristown, NJ, USA.
Associ-ation for Computational Linguistics.Shen Li, Jo?ao V. Grac?a, and Ben Taskar.
2012.
Wiki-lysupervised part-of-speech tagging.
In Proceedingsof the 2012 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning, EMNLP-CoNLL ?12,pages 1389?1398, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.P.
Liang, M. I. Jordan, and D. Klein.
2010.
Type-based MCMC.
In North American Association forComputational Linguistics (NAACL).Mitchell P. Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a large anno-tated corpus of English: the Penn treebank.
Compu-tational Linguistics, 19(2):313?330.Bernard Merialdo.
1993.
Tagging english text witha probabilistic model.
Computational Linguistics,20:155?171.124Yee Whye Teh.
2006.
A hierarchical bayesian lan-guage model based on Pitman-Yor processes.
InProceedings of the 21st International Conferenceon Computational Linguistics and the 44th annualmeeting of the Association for Computational Lin-guistics, ACL-44, pages 985?992, Morristown, NJ,USA.
Association for Computational Linguistics.Kristina Toutanova and Mark Johnson.
2008.
ABayesian LDA-based model for semi-supervisedpart-of-speech tagging.
In J.C. Platt, D. Koller,Y.
Singer, and S. Roweis, editors, Advances in Neu-ral Information Processing Systems 20, pages 1521?1528.
MIT Press, Cambridge, MA.125
