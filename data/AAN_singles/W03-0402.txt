An SVM Based Voting Algorithmwith Application to Parse RerankingLibin Shen and Aravind K. JoshiDepartment of Computer and Information ScienceUniversity of PennsylvaniaPhiladelphia, PA 19104, USA{libin,joshi}@linc.cis.upenn.eduAbstractThis paper introduces a novel Support Vec-tor Machines (SVMs) based voting algorithmfor reranking, which provides a way to solvethe sequential models indirectly.
We havepresented a risk formulation under the PACframework for this voting algorithm.
We haveapplied this algorithm to the parse rerankingproblem, and achieved labeled recall and pre-cision of 89.4%/89.8% on WSJ section 23 ofPenn Treebank.1 IntroductionSupport Vector Machines (SVMs) have been successfullyused in many machine learning tasks.
Unlike the error-driven algorithms, SVMs search for the hyperplane thatseparates a set of training samples that contain two dis-tinct classes and maximizes the margin between thesetwo classes.
The ability to maximize the margin is be-lieved to be the reason for SVMs?
superiority over otherclassifiers.
In addition, SVMs can achieve high perfor-mance even with input data of high dimensional featurespace, especially because of the use of the ?kernel trick?.However, the incorporation of SVMs into sequentialmodels remains a problem.
An obvious reason is thatthe output of an SVM is the distance to the separatinghyperplane, but not a probability.
A possible solutionto this problem is to map SVMs?
results into probabili-ties through a Sigmoid function, and use Viterbi searchto combine those probabilities (Platt, 1999).
However,this approach conflicts with SVMs?
purpose of achievingthe so-called global optimization1.
First, this approachmay constrain SVMs to local features because of the left-to-right scanning strategy.
Furthermore, like other non-generative Markov models, it suffers from the so-called1By global we mean the use of quadratic optimization inmargin maximization.label bias problem, which means that the transitions leav-ing a given state compete only against each other, ratherthan against all other transitions in the model (Lafferty etal., 2001).
Intuitively, it is the local normalization thatresults in the label bias problem.One way of using discriminative machine learning al-gorithms in sequential models is to rerank the n-best out-puts of a generative system.
Reranking uses global fea-tures as well as local features, and does not make lo-cal normalization.
If the output set is large enough, thereranking approach may help to alleviate the impact ofthe label bias problem, because the victim parses (i.e.those parses which get penalized due to the label biasproblem) will have a chance to take part in the rerank-ing.In recent years, reranking techniques have been suc-cessfully applied to the so-called history-based models(Black et al, 1993), especially to parsing (Collins, 2000;Collins and Duffy, 2002).
In a history-based model, thecurrent decision depends on the decisions made previ-ously.
Therefore, we may regard parsing as a special formof sequential model without losing generality.Collins (2000) has proposed two reranking algorithmsto rerank the output of an existing parser (Collins, 1999,Model 2).
One is based on Markov Random Fields, andthe other is based on a boosting approach.
In (Collins andDuffy, 2002), the use of Voted Perceptron (VP) (Freundand Schapire, 1999) for the parse reranking problem hasbeen described.
In that paper, the tree kernel (Collinsand Duffy, 2001) has been used to efficiently count thenumber of common subtrees as described in (Bod, 1998).In this paper we will follow the reranking approach.We describe a novel SVM-based voting algorithm forreranking.
It provides an alternative way of using a largemargin classifier for sequential models.
Instead of usingthe parse tree itself as a training sample, we use a pair ofparse trees as a sample, which is analogous to the pref-erence relation used in the context of ordinal regression(Herbrich et al, 2000).
Furthermore, we justify the al-gorithm through a modification of the proof of the largemargin rank boundaries for ordinal regression.
We thenapply this algorithm to the parse reranking problem.1.1 A Short Introduction of SVMsIn this section, we give a short introduction of SupportVector Machines.
We follow (Vapnik, 1998)?s definitionof SVMs.
For each training sample (yi,xi), yi repre-sents its class, and xi represents its input vector definedon a d-dimensional space.
Suppose the training samples{(y1,x1), ..., (yn,xn)} (xi ?
Rd, yi ?
{?1, 1}) can beseparated by a hyperplane H: (x ?
w) + b = 0, whichmeansyi((xi ?w) + b) ?
1, (1)where w is normal to the hyperplane.
To train an SVM isequivalent to searching for the optimal separating hyper-plane that separates the training data without error andmaximizes the margin between two classes of samples.
Itcan be shown that maximizing the margin is equivalent tominimizing ||w||2.In order to handle linearly non-separable cases, weintroduce a positive slack variable ?i for each sample(yi,xi).
Then training can be reduced to the followingQuadratic Programming (QP) problem.Maximize:LD(?)
?l?i=1?i ?12l?i,j=1yiyj?i?j(xi ?
xj) (2)subject to: 0 ?
?i ?
C and?i ?iyi = 0,where ?i(i = 1...l) are the Lagrange multipliers, l is thetotal number of training samples, and C is a weightingparameter for mis-classification.Since linearly non-separable samples may become sep-arable in a high-dimensional space, SVMs employ the?kernel trick?
to implicitly separate training samples ina high-dimensional feature space.
Let ?
: Rd 7?
Rhbe a function that maps a d-dimensional input vectorx to an h-dimensional feature vector ?(x).
In orderto search for the optimal separating hyperplane in thehigher-dimensional feature space, we only need to sub-stitute ?
(xi) ?
?
(xj) with xi ?
xj in formula (2).If there is a function K, such that K(xi,xj) = ?(xi)??
(xj), we don?t need to compute ?
(xi) explicitly.
K iscalled a kernel function.
Thus during the training phasewe need to solve the following QP problem.Maximize:LD(?)
?l?i=1?i ?12l?i,j=1yiyj?i?jK(xi,xj).
(3)subject to: 0 ?
?i ?
C, and?i ?iyi = 0.Let x be a test vector, the decision function isf(x) = sgn(Ns?j=1?jyjK(sj,x) + b) (4)where sj is a training vector whose corresponding La-grange multiplier ?j > 0. sj is called a support vector.Ns is the total number of the support vectors.
Accordingto (4), the decision function only depends on the supportvectors.It is worth noting that not any function K can be usedas a kernel.
We call function K : Rd ?
Rd 7?
Ra well-defined kernel if and only if there is a mappingfunction ?
: Rd 7?
Rh such that, for any xi,xj ?
Rd,K(xi,xj) = ?
(xi) ?
?(xj).
One way of testing whethera function is a well-defined kernel is to use the Mer-cer?s theorem (Vapnik, 1998) by utilizing the positivesemidefinteness property.
However, as far as a discretekernel is concerned, there is a more convenient way toshow that a function is a well-defined kernel.
This isachieved by showing that a function K is a kernel by find-ing the corresponding mapping function ?.
This methodwas used in the proof of the string subsequence kernel(Cristianini and Shawe-Tayor, 2000) and the tree kernel(Collins and Duffy, 2001).1.2 Large Margin ClassifiersSVMs are called large margin classifiers because theysearch for the hyperplane that maximizes the margin.
Thevalidity of the large margin method is guaranteed by thetheorems of Structural Risk Minimization (SRM) underProbably Approximately Correct (PAC) framework2; testerror is related to training data error, number of trainingsamples and the capacity of the learning machine (Smolaet al, 2000).Vapnik-Chervonenkis (VC) dimension (Vapnik, 1999),as well as some other measures, is used to estimate thecomplexity of the hypothesis space, or the capacity of thelearning machine.
The drawback of VC dimension is thatit ignores the structure of the mapping from training sam-ples to hypotheses, and concentrates solely on the rangeof the possible outputs of the learning machine (Smolaet al, 2000).
In this paper we will use another measure,the so-called Fat Shattering Dimension (Shawe-Taylor etal., 1998), which is shown to be more accurate than VCdimension (Smola et al, 2000), to justify our voting al-gorithm,Let F be a family of hypothesis functions.
The fatshattering dimension of F is a function from margin ?
tothe maximum number of samples such that any subset of2SVM?s theoretical accuracy is much lower than their actualperformance.
The ability to maximize the margin is believed tobe the reason for SVMs?
superiority over other classifiers.these samples can be classified with margin ?
by a func-tion in F .
An upper bound of the expected error is givenin Theorem 1 below (Shawe-Taylor et al, 1998).
We willuse this theorem to justify the new voting algorithm.Theorem 1 Consider a real-valued function class Fhaving fat-shattering function bounded above by thefunction afat : R ?
N which is continuous from theright.
Fix ?
?
R. If a learner correctly classifies mindependently generated examples z with h = T?
(f) ?T?
(F ) such that erz(h) = 0 and ?
= min |f(xi) ?
?|,then with confidence 1 ?
?
the expected error of h isbounded from above by2m(k log(8emk) log(32m) + log(8m?))
(5)where k = afat(?/8).2 A New SVM-based Voting AlgorithmLet xij be the jth candidate parse for the ith sentence intraining data.
Let xi1 is the parse with the highest f -scoreamong all the parses for the ith sentence.We may take xi1 as positive samples, and xij(j>1) asnegative samples.
However, experiments have shown thatthis is not the best way to utilize SVMs in reranking (Di-jkstra, 2001).
A trick to be used here is to take a pair ofparses as a sample: for any i and j > 1, (xi1, xij) is apositive sample, and (xij , xi1) is a negative sample.Similar idea was employed in the early works of parsereranking.
In the boosting algorithm of (Collins, 2000),for each sample (parse) xij , its margin is defined asF (xi1, ??)
?
F (xij , ??
), where F is a score function and??
is the parameter vector.
In (Collins and Duffy, 2002),for each offending parse, the parameter vector updatingfunction is in the form of w = w + h(xi1) ?
h(xij),where w is the parameter vector and h returns the featurevector of a parse.
But neither of these two papers used apair of parses as a sample and defined functions on pairsof parses.
Furthermore, the advantage of using differencebetween parses was not theoretically clarified, which wewill describe in the next section.As far as SVMs are concerned, the use of parses orpairs of parses both maximize the margin between xi1and xij , but the one using a single parse as a sampleneeds to satisfy some extra constraints on the selectionof decision function.
However these constraints are notnecessary (see section 3.3).
Therefore the use of pairs ofparses has both theoretic and practical advantages.Now we need to define the kernel on pairs of parses.Let (t1, t2), (v1, v2) are two pairs of parses.
Let K isany kernel function on the space of single parses.
Thepreference kernel PK is defined on K as follows.PK((t1, t2), (v1, v2)) ?
K(t1, v1)?K(t1, v2)?K(t2, v1) +K(t2, v2) (6)The preference kernel of this form was previously usedin the context of ordinal regression in (Herbrich et al,2000).
Then the decision function isf((xj , xk)) =Ns?i=1?iyiPK((si1, si2), (xj , xk)) + b= b + (Ns?i=1?iyi(K(si1, xj)?K(si2, xj)))?
(Ns?i=1?iyi(K(si1, xk)?K(si2, xk))),where xj and xk are two distinct parses of a sentence,(si1, si2) is the ith support vector, and Ns is the totalnumber of support vectors.As we have defined them, the training samples aresymmetric with respect to the origin in the space.
There-fore, for any hyperplane that does not pass through theorigin, we can always find a parallel hyperplane thatcrosses the origin and makes the margin larger.
Hence,the outcome separating hyperplane has to pass throughthe origin, which means that b = 0.Therefore, for each test parse x, we only need to com-pute its score as follows.score(x) =Ns?i=1?iyi(K(si1, x)?K(si2, x)), (7)becausef((xj , xk)) = score(xj)?
score(xk).
(8)2.1 KernelsIn (6), the preference kernel PK is defined on kernel K.K can be any possible kernel.
We will show that PK iswell-defined in the next section.
In this paper, we con-sider two kernels for K, the linear kernel and the treekernel.In (Collins, 2000), each parse is associated with a setof features.
Linear combination of the features is usedin the decision function.
As far as SVM is concerned,we may encode the features of each parse with a vec-tor.
Dot product is used as the kernel K. Let u and vare two parses.
The computational complexity of linearkernel O(|fu| ?
|fv|), where |fu| and |fv| are the lengthof the vectors associated with parse u and v respectively.The goodness of the linear kernel is that it runs very fastin the test phase, because coefficients of the support vec-tors can be combined in advance.
For a test parse x, thecomputational complexity of test is only O(|fx|), whichis independent with the number of the support vectors.In (Collins and Duffy, 2002), the tree kernel Tr is usedto count the total number of common sub-trees of twoparse trees.
Let u and v be two trees.
Because Tr can becomputed by dynamic programming, the computationalcomplexity of Tr(u, v) is O(|u| ?
|v|), where |u| and |v|are the tree sizes of u and v respectively.
For a test parsex, the computational complexity of the test is O(S ?
|x|),where S is the number of support vectors.3 Justifying the Algorithm3.1 Justifying the KernelFirstly, we show that the preference kernel PK definedabove is well-defined.
Suppose kernel K is defined onT ?
T. So there exists ?
: T 7?
H, such thatK(x1, x2) = ?
(x1) ?
?
(x2) for any x1, x2 ?
T.It suffices to show that there exist space H?
andmapping function ??
: T?T ?
H?
such thatPK((t1, t2), (v1, v2)) = ??
(t1, t2) ?
??
(t1, t2), wheret1, t2, v1, v2 ?
T.According to the definition of PK, we havePK((t1, t2), (v1, v2))= K(t1, v1)?
K(t1, v2)?
K(t2, v1) + K(t2, v2)= ?
(t1) ?
?(v1)?
?
(t1) ?
?(v2)??
(t2) ?
?
(v1) + ?
(t2) ?
?
(v2)= (?(t1)?
?
(t2)) ?
(?(v1)?
?
(v2)), (9)Let H?
= H and ??
(x1, x2) = ?(x1)??(x2).
Hencekernel PK is well-defined.3.2 Margin Bound for SVM-based VotingWe will show that the expected error of voting is boundedfrom above in the PAC framework.
The approach usedhere is analogous to the proof of ordinal regression (Her-brich et al, 2000).
The key idea is to show the equiva-lence of the voting risk and the classification risk.Let X be the set of all parse trees.
For each x ?
X ,let x?
be the best parse for the sentence related to x. Thusthe appropriate loss function for the voting problem is asfollows.lvote(x, f) ?
{1 if f(x?)
< f(x)0 otherwise (10)where f is a parse scoring function.Let E = {(x, x?
)|x ?
X} ?
{(x?, x)|x ?
X}.
Eis the space of event of the classification problem, andPrE((x, x?))
= PrE((x?, x)) = 12PrX(x).
For any parsescoring function f , let gf (x1, x2) ?
sgn(f(x1)?f(x2)).For classifier gf on space E , its loss function islclass(x1, x2, gf ) ???????????
?1 if x1 = x?2 andgf (x1, x2) = ?11 if x2 = x?1 andgf (x1, x2) = +10 otherwiseTherefore the expected risk Rvote(f) for the votingproblem is equivalent to the expected risk Rclass(gf ) forthe classification problem.Rvote(f)= Ex?X(lvote(x, f))= E(x,x?
)?E(lvote(x, f)) +E(x?,x)?E(lvote(x, f))= E(x1,x2)?E(lclass(x1, x2, gf ))= Rclass(gf ) (11)However, the definition of space E violates the inde-pendently and identically distributed (iid) assumption.Parses for the same sentence are not independent.
If wesuppose that no two pairs of parses come from the samesentence, then the idd assumption holds.
In practice, thenumber of sentences is very large, i.e.
more than 30000.So we may use more than one pair of parses of the samesentence and still assume the idd property roughly, be-cause for any two arbitrary pairs of parses, 29999 out of30000, these two samples are independent.Let ?
?
mini=1..n,j=2..mi |f(xi1) ?
f(xij)| =mini=1..n,j=2..mi |g(xi1, xij)?0|.
According to (11) andTheorem 1 in section 1.2 we get the following theorem.Theorem 2 If gf makes no error on the training data,with confidence 1?
?Rvote(f) = Rclass(gf )?2m(k log(8emk) log(32m)+ log(8m?
)), (12)where k = afat(?/8),m =?i=1..n(mi ?
1).3.3 Justifying Pairwise SamplesAn obvious way to use SVM is to use each single parse,instead of a pair of parse trees, as a training sample.
Onlythe best parse of each sentence is regarded as a positivesample, and all the rest are regarded as negative samples.Similar to the pairwise system, it also maximizes the mar-gin between the best parse of a sentence and all incorrectparses of this sentence.
Suppose f is the function result-ing from the SVM.
It requires yijf(xij) > 0 for eachsample (xij , yij).
However this constraint is not neces-sary.
We only need to guarantee that f(xi1) > f(xij).This is the reason for using pairs of parses as trainingsamples instead of single parses.We may rewrite the score function (7) as follows.score(x) =?i,jci,jK(si,j , x), (13)where i is the index for sentence, j is the index for parse,and ?i?j ci,j = 0.The format of score in (13) is the same as the deci-sion function generated by an SVM trained on the singleparses as samples.
However, there is a constraint thatthe sum of the coefficients related to parses of the samesentence is 0.
So in this way we decrease the size of hy-pothesis space based on the prior knowledge that only thedifferent segments of two distinct parses determine whichparse is better.4 Related WorkThe use of pairs of parse trees in our model is analogousto the preference relation used in the ordinal regressionalgorithm (Herbrich et al, 2000).
In that paper, pairs ofobjects have been used as training samples.
For exam-ple, let (r1, r2, ...rm) be a list of objects in the trainingdata, where ri ranks ith.
Then pairs of objects (ri?1, ri)are training samples.
Preference kernel PK in our paperis the same as the preference kernel in (Herbrich et al,2000) in format.However, the purpose of our model is different fromthat of the ordinal regression algorithm.
Ordinal regres-sion searches for a regression function for ordinal values,while our algorithm is designed to solve a voting prob-lem.
As a result, the two algorithms differ on the def-inition of the margin.
In ordinal regression, the marginis min |f(ri) ?
f(ri?1)|, where f is the regression func-tion for ordinal values.
In our algorithm, the margin ismin |score(xi1)?
score(xij)|.In (Kudo and Matsumoto, 2001), SVMs have been em-ployed in the NP chunking task, a typical labeling prob-lem.
However, they have used a deterministic algorithmfor decoding.In (Collins, 2000), two reranking algorithms were pro-posed.
In both of these two models, the loss functions arecomputed directly on the feature space.
All the featuresare manually defined.In (Collins and Duffy, 2002), the Voted Perceptron al-gorithm was used to in parse reranking.
It was shownin (Freund and Schapire, 1999; Graepel et al, 2001) thaterror bound of (voted) Perceptron is related to marginsexisting in the training data, but these algorithm are notsupposed to maximize margins.
Variants of the Percep-tron algorithm, which are known as Approximate Maxi-mal Margin classifier, such as PAM (Krauth and Mezard,1987), ALMA (Gentile, 2001) and PAUM (Li et al,2002), produce decision hyperplanes within ratio of themaximal margin.
However, almost all these algorithmsare reported to be inferior to SVMs in accuracy, whilemore efficient in training.Furthermore, these variants of the Perceptron algo-rithm take advantage of the large margin existing in thetraining data.
However, in NLP applications, samples areusually inseparable even if the kernel trick is used.
SVMscan still be trained to maximize the margin through themethod of soft margin.5 Experiments and AnalysisWe use SVM light (Joachims, 1998) as the SVM clas-sifier.
The soft margin parameter C is set to its defaultvalue in SVM light.We use the same data set as described in (Collins,2000; Collins and Duffy, 2002).
Section 2-21 of the PennWSJ Treebank (Marcus et al, 1994) are used as train-ing data, and section 23 is used for final test.
The train-ing data contains around 40,000 sentences, each of whichhas 27 distinct parses on average.
Of the 40,000 trainingsentences, the first 36,000 are used to train SVMs.
Theremaining 4,000 sentences are used as development data.The training complexity for SVM light is roughlyO(n2.1) (Joachims, 1998), where n is the number of thetraining samples.
One solution to the scaling difficultiesis to use the Kernel Fisher Discriminant as described in(Salomon et al, 2002).
In this paper, we divide train-ing data into slices to speed up training.
Each slice con-tains two pairs of parses from each sentence.
Specifically,slice i contains positive samples ((p?k, pki),+1) and neg-ative samples ((pki, p?k),?1), where p?k is the best parsefor sentence k, pki is the parse with the ith highest log-likelihood in all the parses for sentence k and it is not thebest parse.
There are about 60000 parses in each slice inaverage.
For each slice, we train an SVM.
Then resultsof SVMs are put together with a simple combination.
Ittakes about 2 days to train a slice on a P3 1.13GHz pro-cessor.As a result of this subdivision of the training data intoslices, we cannot take advantage of SVM?s global opti-mization ability.
This seems to nullify our effort to cre-ate this new algorithm.
However, our new algorithm isstill useful for the following reasons.
Firstly, with the im-provement in the computing resources, we will be able touse larger slices so as to utilize more global optimization.SVMs are superior to other linear classifiers in theory.
Onthe other hand, the current size of the slice is large enoughfor other NLP applications like text chunking, although itis not large enough for parse reranking.
The last reasonis that we have achieved state-of-the-art results even withthe sliced data.We have used both a linear kernel and a tree kernel.For the linear kernel test, we have used the same datasetas that in (Collins, 2000).
In this experiment, we first train22 SVMs on 22 distinct slices.
In order to combine thoseSVMs results, we have tried mapping SVMs?
results toprobabilities via a Sigmoid as described in (Platt, 1999).We use the development data to estimate parameter Aand B in the SigmoidPi(y = 1|fi) =11 + Ae?fiB, (14)Table 1: Results on section 23 of the WSJ Treebank.LR/LP = labeled recall/precision.
CBs = average numberof crossing brackets per sentence.
0 CBs, 2 CBs are thepercentage of sentences with 0 or ?
2 crossing bracketsrespectively.
CO99 = Model 2 of (Collins, 1999).
CH00= (Charniak, 2000).
CO00 = (Collins, 2000).
?40 Words (2245 sentences)Model LR LP CBs 0 CBs 2 CBsCO99 88.5% 88.7% 0.92 66.7% 87.1%CH00 90.1% 90.1% 0.74 70.1% 89.6%CO00 90.1% 90.4% 0.73 70.7% 89.6%SVM 89.9% 90.3% 0.75 71.7% 89.4%?100 Words (2416 sentences)Model LR LP CBs 0 CBs 2 CBsCO99 88.1% 88.3% 1.06 64.0% 85.1%CH00 89.6% 89.5% 0.88 67.6% 87.7%CO00 89.6% 89.9% 0.87 68.3% 87.7%SVM 89.4% 89.8% 0.89 69.2% 87.6%where fi is the result of the ith SVM.
The parse withmaximal value of?i Pi(y = 1|fi) is chosen as the top-most parse.
Experiments on the development data showsthat the result is better if Ae?fiB is much larger than 1.Thereforen?i=1Pi(y = 1|fi) =n?i=111 + Ae?fiB?n?i1Ae?fiB= A?ne(B?ni=1fi) (15)Therefore, we may use?i fi directly, and there is noneed to estimate A and B in (14).
Then we combineSVMs?
result with the log-likelihood generated by theparser (Collins, 1999).
Parameter ?
is used as the weightof the log-likelihood.
In addition, we find that our SVMhas greater labeled precision than labeled recall, whichmeans that the system prefer parses with less brackets.So we take the number of brackets as another feature tobe considered, with weight ?.
?
and ?
are estimated onthe development data.The result is shown in Table 1.
The performance ofour system matches the results of (Charniak, 2000), butis a little lower than the results of the Boosting systemin (Collins, 2000), except that the percentage of sen-tences with no crossing brackets is 1% higher than that of(Collins, 2000).
Since we have to divide data into slices,we cannot take full advantage of the margin maximiza-tion.Figure 1 shows the learning curves.
?
is used to con-Figure 1: Learning curves on the development dataset of(Collins, 2000).
X-axis stands for the number of slices tobe combined.
Y-axis stands for the F -score.0.890.8920.8940.8960.8980.90.9020.9040.9060.9080.910 5 10 15 20number of slicesbaseline?=033333333333333333333333?=200+++++++++++++++++++++++?=50022222222222222222222222?=1000??????????????????????
?Table 2: Results on section 23 of the WSJ Treebank.LR/LP = labeled recall/precision.
CBs = average num-ber of crossing brackets per sentence.
CO99 = Model 2of (Collins, 1999).
CD02 = (Collins and Duffy, 2002)?100 Words (2416 sentences)Model LR LP CBsCO99 88.1% 88.3% 1.06CD02 88.6% 88.9% 0.99SVM(tree) 88.7% 88.8% 0.99trol the weight of log-likelihood given by the parser.
Theproper value of ?
depends on the size of training data.The best result does not improve much after combining 7slices of training data.
We think this is due the limitationof local optimization.Our next experiment is on the tree kernel as it is usedin (Collins and Duffy, 2002).
We have only trained 5slices, since each slice takes about 2 weeks to train ona P3 1.13GHz processor.
In addition, the speed of testfor the tree kernel is much slower than that for the lin-ear kernel.
The experimental result is shown in Table 2The results of our SVM system match the results of theVoted Perceptron algorithm in (Collins and Duffy, 2002),although only 5 slices, amounting to less than one fourthof the whole training dataset, have been used.6 Conclusions and Future WorkWe have introduced a new approach for applying SVMsto sequential models indirectly, and described a novelSVM based voting algorithm inspired by the parsereranking problem.
We have presented a risk formula-tion under the PAC framework for this voting algorithm,and applied this algorithm to the parse reranking prob-lem, and achieved LR/LP of 89.4%/89.8% on WSJ sec-tion 23.Experimental results show that the SVM with a linearkernel is superior to the SVM with tree kernel in bothaccuracy and speed.
The SVM with tree kernel onlyachieves a rather low f -score because it takes too manyunrelated features into account.
The linear kernel is de-fined on the features which are manually selected from alarge set of possible features.As far as context-free grammars are concerned, it willbe hard to include more features into the current featureset.
If we simply use n-grams on context-free grammars,it is very possible that we will introduce many useless fea-tures, which may be harmful as they are in tree kernel sys-tems.
One way to include more useful features is to takeadvantage of the derivation tree and the elementary treesin Lexicalized Tree Adjoining Grammar (LTAG) (Joshiand Schabes, 1997).
The basic idea is that each elemen-tary tree and every segment in a derivation tree is linguis-tically meaningful.We also plan to apply this algorithm to other sequen-tial models, especially to the Supertagging problem.
Webelieve it will also be very useful to problems of POStagging and NP chunking.
Compared to parse rerank-ing, they have a much smaller training dataset and featuresize, which is more suitable for our SVM-based votingproblem.AcknowledgmentsWe thank Michael Collins for help concerning the dataset, and Anoop Sarkar for his comments.
We also thankthree anonymous reviewers for helpful comments.ReferencesE.
Black, F. Jelinek, J. Lafferty, Magerman D. M.,R.
Mercer, and S. Roukos.
1993.
Towards history-based grammars: Using richer models for probabilisticparsing.
In Proceedings of the ACL 1993.Rens Bod.
1998.
Beyond Grammar: An Experience-Based Theory of Language.
CSLI Publica-tions/Cambridge University Press.E.
Charniak.
2000.
A maximum-entropy-inspired parser.In Proceedings of NAACL 2000.Michael Collins and Nigel Duffy.
2001.
Convolutionkernels for natural language.
In Proceedings of NeuralInformation Processing Systems (NIPS 14).Michael Collins and Nigel Duffy.
2002.
New ranking al-gorithms for parsing and tagging: Kernels over discretestructures, and the voted perceptron.
In Proceedings ofACL 2002.Michael Collins.
1999.
Head-Driven Statistical Modelsfor Natural Language Parsing.
Ph.D. thesis, Univer-sity of Pennsylvania.Michael Collins.
2000.
Discriminative reranking for nat-ural language parsing.
In Proceedings of the 7th Inter-national Conference on Machine Learning.N.
Cristianini and J. Shawe-Tayor.
2000.
An introductionto Support Vector Machines and other kernel-basedlearning methods.
Cambridge University Press.Emma Dijkstra.
2001.
Support vector machines for parseselection.
Master?s thesis, Univ.
of Edinburgh.Yoav Freund and Robert E. Schapire.
1999.
Large mar-gin classification using the perceptron algorithm.
Ma-chine Learning, 37(3):277?296.Claudio Gentile.
2001.
A new approximate maximalmargin classification algorithm.
Journal of MachineLearning Research, 2:213?242.Thore Graepel, Ralf Herbrich, and Robert C. Williamson.2001.
From margin to sparsity.
In Advances in NeuralInformation Processing Systems 13.Ralf Herbrich, Thore Graepel, and Klaus Obermayer.2000.
Large margin rank boundaries for ordinal re-gression.
In Advances in Large Margin Classifiers,pages 115?132.
MIT Press.Thorsten Joachims.
1998.
Making large-scale supportvector machine learning practical.
In Advances in Ker-nel Methods: Support Vector Machine.
MIT Press.A.
Joshi and Y. Schabes.
1997.
Tree-adjoining gram-mars.
In G. Rozenberg and A. Salomaa, editors, Hand-book of Formal Languages, volume 3, pages 69 ?
124.Springer.W.
Krauth and M. Mezard.
1987.
Learning algorithmswith optimal stability in neural networks.
Journal ofPhysics A, 20:745?752.Taku Kudo and Yuji Matsumoto.
2001.
Chunking withsupport vector machines.
In Proceedings of NAACL2001.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Condi-tional random fields: Probabilistic models for segmen-tation and labeling sequence data.
In Proceedings ofICML.Yaoyong Li, Hugo Zaragoza, Ralf Herbrich, John Shawe-Taylor, and Jaz Kandola.
2002.
The perceptron al-gorithm with uneven margins.
In Proceedings of theInternational Conference of Machine Learning.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1994.
Building a large annotated cor-pus of English: The Penn Treebank.
ComputationalLinguistics, 19(2):313?330.John Platt.
1999.
Probabilistic outputs for support vectormachines and comparisons to regularized likelihoodmethods.
In Advances in Large Margin Classifiers.MIT Press.Jesper Salomon, Simon King, and Miles Osborne.
2002.Framewise phone classification using support vectormachines.
In Proceedings of ICSLP 2002.John Shawe-Taylor, Peter L. Bartlett, Robert C.Williamson, and Martin Anthony.
1998.
Structuralrisk minimization over data-dependent hierarchies.IEEE Trans.
on Information Theory, 44(5):1926?1940.A.J.
Smola, P. Bartlett, B. Scho?lkopf, and C. Schuurmans.2000.
Introduction to large margin classifiers.
In A.J.Smola, P. Bartlett, B. Scho?lkopf, and C. Schuurmans,editors, Advances in Large Margin Classifiers, pages1?26.
MIT Press.Vladimir N. Vapnik.
1998.
Statistical Learning Theory.John Wiley and Sons, Inc.Vladimir N. Vapnik.
1999.
The Nature of StatisticalLearning Theory.
Springer, 2 edition.
