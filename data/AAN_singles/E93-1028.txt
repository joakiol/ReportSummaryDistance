Similarity between WordsComputed by Spreading Activation on an English DictionaryHideki KozimaCourse in Computer Science.
.
.
.
and Information Mathematics,Graduate School,University of Electro-Communications1-5-1, Chofugaoka, Chofu,Tokyo 182, Japan(xkozima@phaeton.
cs.
uec.
ac.
j p)Teiji FurugoriDepartment ofComputer Scienceand Information Mathematics,University of Electro-Communications1-5-1, Chofugaoka, Chofu,Tokyo 182, JapanTel.
+81-424-83-2161 (ex.4461)( fu rugor i@phaet  on.
cs.
uec.
ac.
j p )AbstractThis paper proposes a method for measur-ing semantic similarity between words asa new tool for text analysis.
The simi-larity is measured on a semantic networkconstructed systematically from a subsetof the English dictionary, LDOCE (Long-man Dictionary of Contemporary English).Spreading activation on the network can di-rectly compute the similarity between anytwo words in the Longman Defining Vocab-ulary, and indirectly the similarity of all theother words in LDOCE.
The similarity rep-resents the strength of lexical cohesion orsemantic relation, and also provides valu-able information about similarity and co-herence of texts.1 Introduct ionA text is not just a sequence of words, but it also hascoherent structure.
The meaning of each word in atext depends on the structure of the text.
Recogniz-ing the structure of text is an essential task in textunderstanding.\[Grosz andSidner, 1986\]One of the valuable indicators of the structure oftext is lexical cohesion.\[Halliday nd Hasan, 1976\]Lexical cohesion is the relationship between words,classified as follows:1.
Reiteration:Molly likes cats.
She keeps a cat.2.
Semantic relation:a. Desmond saw a cat.
It was Molly's pet.b.
Molly goes to the north.
Not east.c.
Desmond goes to a theatre.
He likes films.Reiteration of words is easy to capture by morpho-logical analysis.
Semantic relation between words,which is the focus of this paper, is hard to recognizeby computers.We consider lexical cohesion as semantic similaritybetween words.
Similarity is Computed by spread-ing activation (or association) \[Waltz and Pollack,1985\] on a semantic network constructed systemati-cally from an English dictionary.
Whereas it is editedby some lexicographers, a dictionary is a set of asso-ciative relation shared by the people in a linguisticcommunity.The similarity between words is a mapping a: LxL ---* \[0, 1\], where L is a set of words (or lexicon).The following examples uggest he feature of thesimilarity:a(cat ,  pet)  = 0.133722 (similar),a(cat ,  mat) = 0.002692 (dissimilar).The value of a(w, w') increases with strength of se-mantic relation between w and w'.The following section examines related work in or-der to clarify the nature of the semantic similarity.Section 3 describes how the semantic network is sys-tematically constructed from the English dictionary.Section 4 explains how to measure the similarity byspreading activation on the semantic network.
Sec-tion 5 shows applications of the similarity measure - -computing similarity between texts, and measuringcoherence of a text.
Section 6 discusses the theoret-ical aspects of the similarity.2 Related Work on Measur ingSimilarityWords in a language are organized by two kinds ofrelationship.
One is a syntagmatic relation: how thewords are arranged in sequential texts.
The other is a232"polite"azlgulax I I I '.
~.~ I I roundedveak  : : : : ; ~1  I strongrough : : : : : ' ~: : Boothact ive  ' ' ' ~ '  .
.
.
.
.
.
.
: pass ivesmall  I I l~  : l I I largecold .
.
.
.
I , , , : hotgood " '  ~ ' I '~ '  " ; ; I badfresh .
.
.
.
.
.
I I staleFigure 1.
A psycholinguistic measurement(semantic differential \[Osgood, 1952\]).paradigmatic relation: how the words are associatedwith each other.
Similarity between words can bedefined by either a syntagmatic or a paradigmaticrelation.Syntagmatic similarity is based on co-occurrencedata extracted from corpora \[Church and Hanks,1990\], definitions in dictionaries \[Wilks etal., 1989\],and so on.
Paradigmatic similarity is based onassociation data extracted from thesauri \[Morrisand Hirst, 1991\], psychological experiments \[Osgood,1952\], and so on.This paper concentrates on paradigmatic similar-ity, because a paradigmatic relation can be estab-lished both inside a sentence and across sentenceboundaries, while syntagmatic relations can be seenmainly inside a sentence -- like syntax deals withsentence structure.
The rest of this section fo-cuses on two related works on measuring paradig-matic similarity -- a psycholinguistic approach anda thesaurus-based approach.2.1 A Psycholinguistic ApproachPsycholinguists have been proposed methods formeasuring similarity.
One of the pioneering worksis 'semantic differential' \[Osgood, 1952\] which anal-yses meaning of words into a range of different di-mensions with the opposed adjectives at both ends(see Figure 1), and locates the words in the semanticspace.Recent works on knowledge representation aresomewhat related to Osgood's emantic differential.Most of them describe meaning of words using specialsymbols like microfeatures \[Waltz and Pollack, 1985;Hendler, 1989\] that correspond to the semantic di-mensions.However, the following problems arise from thesemantic differential procedure as measurement ofmeaning.
The procedure is not based on the deno-tative meaning of a word, but only on the connota-tive emotions attached to the word; it is difficult tochoose the relevant dimensions, i.e.
the dimensionsrequired for the sufficient semantic space.2.2 A Thesaurus -based  ApproachMorris and Hirst \[1991\] used Roget's thesaurus asknowledge base for determining whether or not twowords are semantically related.
For example, thesemantic relation of t ruck/car  and dr ive/car  arecaptured in the following way:1. t ruck  E vehicle B car(both are included in the vehicle class),2. dr ive E journey ~ vehicle B carOourney refersto vehicle).This method can capture Mmost all types of se-mantic relations (except emotional and situationalrelation), such as paraphrasing by superordinate ( x.cat/pet) ,  systematic relation (ex.
north/east) ,  andnon-systematic relation (ex.
theatre / f i \ ]~) .However, thesauri provide neither informationabout semantic difference between words juxtaposedin a category, nor about strength of the semantic re-lation between words - -  both are to be dealt in thispaper.
The reason is that thesauri axe designed tohelp writers find relevant words, not to provide themeaning of words.3 Parad igme:  A F ie ld  fo r  Measur ingS imi la r i tyWe analyse word meaning in terms of the seman-tic space defined by a semantic network, calledParadigme.
Paradigme is systematically constructedfrom Gloss~me, a subset of an English dictionary.3.1 Gloss~me - -  A Closed Subsystem ofEnglishA dictionary is a closed paraphrasing system of nat-ural language.
Each of its headwords i  defined bya phrase which is composed of the headwords andtheir derivations.
A dictionary, viewed as a whole,looks like a tangled network of words.We adopted Longman Dictionary of ContemporaryEnglish (LDOCE) \[1987\] assuch a closed system ofEnglish.
LDOCE has a unique feature that each ofits 56,000 headwords i defined by using the words inLongman Defining Vocabulary (hereafter, LDV) andtheir derivations.
LDV consists of 2,851 words (asthe headwords in LDOCE) based on the survey ofrestricted vocabulary \[West, 1953\].We made a reduced version of LDOCE, calledGlossdme.
Gloss~me has every entry of LDOCEwhose headword is included in LDV.
Thus, LDVisdefined by Gloss~me, and Glossdme is composed of .
.
.
.
.
.LDV.
Gloss~me is a closed subsystem of English.GIoss~me has 2,851 entries that consist of 101,861words (35.73 words/entry on the average).
An itemof Gloss~me has a headword, a word-class, and oneor more units corresponding to numbered efinitionsin the entry of LDOCE.
Each unit has one head-part and several det-parts.
The head-part is the firstphrase in the definition, which describes the broader233red t /red/ adj -dd- 1 of the colour of bloodor fire: a red rose~dress \[ We painted the doorred.
- -  see also l ike a red  rag  to a bul l(RAG 1) 2 (of human hair) of a bright brownishorange or copper colour 3 (of the human skin)pink, usa.
for a short time: I turned red withembarrassment~anger.
I The child's eye (= theskin round the eyes) were red from crying.
4(of wine) of a dark pink to dark purple colour- ~ n ~ .
\ [U \ ](red adj((of the colour)(of blood or fire) )((of a bright brownish(of human hair) )(pink(usu for a short time)(of the human akin) ); headeord ,  eord-c lass; unit 1 -- head-par t; det -par torange or copper colour); unit 3 -- head-part; det-part 1; det-part 2( (of  a dark pink to  dark purp le  co lour)(of wine) ))F igure  2.
A sample entry of LDOCE and a corresponding entry of Glosseme (in S-expression).
(red_l (adj) 0.000000 ;;;; referent(+ ;;  eubreferant  1(0.333333 ;; weight of(* (0.001594 of_l)(0.042108 colour_2)(0.185058 fire_l);; subreferant 2(0.277778(* (0.000278 of_l)(0.466411 orange_l)(0.007330 colour_2)(0.016372 hair_l); ;  aubreferant 3(0.222222(* (0.410692 pink_l)(0.028846 short_l)(0.000595 the_2);;  subre ferant  4(0.166667(* (0.000328 of_l)(0.123290 pink_l)(0.000273 to_3)(0.141273 purple_2)(0.338512 wine_l);; refereheadeord, word-class, and activity-valuesubreferant 1(0.001733 the_l) (0.001733 the_2) (0.042108 colour_l)(0.000797 of_l) (0.539281 blood_l) (0.000529 or_l)(0.185058 fire_2) ))(0.000196 a_l) (0.030997 bright_l) (0.065587 broen_l)(0.000184 or_l) (0.385443 copper_l) (0.007330 colour_l)(0.000139 of_l) (0.009868 human_l) (0.009868 human_2)))(0.410692 pink_2) (0.003210 for_l) (0.000386 a_l)(0.006263 time_l) (0.000547 of_l) (0.000595 the_l)(0.038896 human_l) (0.038896 human_2) (0.060383 akin_l) ))(0.000232 a_l) (0.028368 daxk_l) (0.028368 dark_2)(0.123290 pink_2) (0.000273 to_1) (0.000273 to_2)(0.028368 dark_l) (0.028368 dark_2) (0.141273 purple_l)(0.008673 colour_l) (0.008673 colour_2) (0.000164 of_l))))(* (0.031058 apple_l) (0.029261 blood_l) (0.008678(0.029140 copper_l) (0.009537 diamond_l) (0.003015(0.006464 fox_l) (0.006152 heart_l) (0.098349(0.029140 orange_l) (0.007714 pepper_l) (0.196698(0.098349 pink_2) (0.018733 purple_2) (0.028100(0.196698 red_2) (0.004230 signal_l) ))co lour_ l )  (0.009256f i re_l)  (0.073762lake_2) (0.007025pink_l) (0.012294purple,2) (0.
098349F igure  3.
A sample node of Paradigme (in S-expression).comb_l)flame_l)l ip_i)pink_2)red_2)meaning of the headword.
The det-parts restrict hemeaning of the head-part.
(See Figure 2.
)3.2 Parad lgme - -  A Semant ic  NetworkWe then translated Gloss~me into a semantic net-work Paradigme.
Each entry in Gloss~me is mappedonto a node in Paradigme.
Paradigme has 2,851nodes and 295,914 unnamed links between the nodes(103.79 links/node on the average).
Figure 3 showsa sample node red_l.
Each node consists of a head-word, a word-class, an activity-value, and two setsof links: a rdf4rant and a rdfdrd.A r~f~rant of a node consists of several subrdfdrantscorrespond to the units of Giossdme.
As shown inFigure 2 and 3, a morphological analysis maps theword broml ish in the second unit onto a link to thenode broom_l, and the word co lour  onto two linksto colour_ l  (adjective) and co lour .2  (noun).A rdfdrd of a node p records the nodes referring top.
For example, the rdf6rd of red_l is a set of links tonodes (ex.
apple_l)  that have a link to red_t in theirrdf~rants.
The rdf6rd provides information about theextension of red_l, not the intension shown in therdf6rant.Each link has thickness tk, which is computedfrom the frequency of the word wk in Gloss~me andother information, and normalized as )-~tk = 1 ineach subrdf6rant or r6f~rd.
Each subrdf~rant alsohas thickness (for example, 0.333333 in the firstsubrdf6rant of red_l), which is computed by the or-der of the units which represents significance of thedefinitions.
Appendix A describes the structure ofParadigme in detail.234w w w ''(?)
I I lFigure 4.
Process of measuring the similarity a(w, w') on Paradigme.
(1) Start activating w. (2) Produce an activated pattern.
(3) Observe activity of w'.20.8 :6.4' ~-~ - -red_2recLl ~orange_1~ ~pxnk  .D -M'blood_Jcopper_l~- purpk~-~purpAe_~rose-~1.04 6 8 I0T (steps)Figure 5.
An activated pattern produced from red(changing of activity values of 10 nodesholding highest activity at T= 10).4 Computing Similarity betweenWordsSimilarity between words is computed by spreadingactivation on Paradigme.
Each of its nodes can holdactivity, and it moves through the links.
Each nodecomputes its activity value vi(T+ 1) at time T+ 1 asfollows:v(T+l) = ?
(Ri(T), R~(T), e,(T)),where Rd(T) and R~(T) are the sum of weighted ac-tivity (at time T) of the nodes referred in the r6f6rantand r~f6r6 respectively.
And, ei(T) is activity givenfrom outside (at time T); to 'activate a node' is tolet ei(T) > 0.
The output function ?
sums up threeactivity values in appropriate proportion and limitsthe output value to \[0,1\].
Appendix B gives the de-tails of the spreading activation.4.1 Measuring SimilarityActivating a node for a certain period of time causesthe activity to spread over Paradigme and producean activated pattern on it.
The activated pattern ap-proximately gets equilibrium after 10 steps, whereasit will never reach the actual equilibrium.
The pat-tern thus produced represents the meaning of thenode or of the words related to the node by morpho-logical analysis 1.The activated pattern, produced from a word w,suggests similarity between w and any headword inLDV.
The similarity a(w, w') E \[0, 1\] is computed inthe following way.
(See also Figure 4.)1.
Reset activity of all nodes in Paradigme.2.
Activate w with strength s(w) for 10 steps,where s(w) is significance of the word w.Then, an activated pattern P(w) is producedon Paradigmc.3.
Observe a(P(w), w') -- an activity value of thenode w' in P(w).Then, a(w, w') is s(w').a(P(w), w').The word significance s(w) E \[0, 1\] is defined asthe normalized information of the word w in the cor-pus \[West, 1953\].
For example, the word red ap-pears 2,308 times in the 5,487,056-word corpus, andthe word and appears 106,064 times.
So, s(red) ands(and) are computed as follows:- log(230S/5487056)s(red) = -- 1og(1/5487056) -- 0.500955,- 1og(106064/5487056)s(and) = -- 1og(1/5487056) = 0.254294.We estimated the significance of the words excludedfrom the word list \[West, 1953\] at the average sig-nificance of their word classes.
This interpolationvirtually enlarged West's 5,000,000-word corpus.For example, let us consider the similarity betweenred  and orange.
First, we produce an activated pat-tern P(red) on Paradigrae.
(See Figure 5.)
Inthis case, both of the nodes red..1 (adjective) andred_,?.
(noun) are activated with strength s(red)=0.500955.
Next, we compute s(oraage)= 0.676253,and observe a(P(red),orange) = 0.390774.
Then,the similarity between red and orange is obtainedas follows:a( red ,  o range)  = 0.676253 ?
0.390774= 0.264262 .XThe morphological analysis maps all words derivedby 48 affixes in LDV onto their root forms (i.e.
headwotdsof  LDOCE).2354.2 Examples  o f  S imi lar i ty  between WordsThe procedure described above can compute the sim-ilarity a(w, w I) between any two words w, w I in LDVand their derivations.
Computer programs of thisp rocedure -  spreading activation (in C), morpho-logical analysis and others (in Common Lisp) - -  cancompute a(w, w') within 2.5 seconds on a worksta-tion (SPARCstation 2).The similarity ?r between words works as an indi-cator of the lexical cohesion.
The following exam-ples illustrate that a increases with the strength ofsemantic relation:o(wine, alcohol) = 0.118078 ,~(wine, line) = 0.002040 ,or(big, la rge)  = 0.120587 ,a(clean, la rge)  = 0.004943 ,a(buy, sell) = 0.135686 ,o'(buy, walk) = 0.007993.The similarity ~r also increases with theoccurrence tendency of words, for example:a(wai ter ,  restaurant) = 0.175699,a(computer, restaurant) = 0.003268,a(red, blood) = 0.111443 ,o(green, blood) = 0.002268 ,~(dig, spade) = 0.116200,~r(fly, spade) = 0.003431.CO-Note that a(w, w') has direction (from w to w'), sothat a(w, w') may not be equal to a(w', w):a(films, theatre) = 0.178988 ,o ( theat re ,  films) ---- 0.068927.Meaningful words should have higher similar-ity; meaningless words (especially, function words)should have lower similarity.
The similarity a(w, w')increases with the significance s(w) and s(w') thatrepresent meaningfulness of w and w':a(north,  east) : 0.100482 ,o'(to, theatre) : 0.007259 ,a(films, of) = 0.005914 ,o'(t o, the) = 0.002240.Note that the reflective similarity a(w,w) also de-pends on the significance s(w), so that cr(w,w) < 1:a(waiter, waiter) = 0.596803 ,er(of, of) = 0.045256.4.3 S imi lar i ty  of  Ext ra  WordsThe similarity of words in LDV and their derivationsis measured irectly on Paradigme; the similarityof extra words is measured indirectly on Paradigmeby treating an extra word as a word l i s t  W ={Wl,..., wn} of its definition in LDOCE.
(Note thateach wi E W is included in LDV or their derivations.
)The similarity between the word lists W, W ~ is de-fined as follows.
(See aiso Figure 6.
)or(W, W') = ?
(~t0'ew'  s(w').a(P(W),w')),W W'1MJ1, " " " ,ff3n tO1 ,  " " " , lOrn\\\ fit,Figure 6.
Measuring similarity of entra wordsas the similarity between word fists.o.2"l__lF=:~, ~.
i - - k  \ \ \ \bot t !e - l~h  ~_ "~ ~ \ ~po ison_ l~ ~ \[swal!ow_l~ \[ i \[ I spixit _I~"--~ \[ \[ I2 4 6 8 I0T (steps)Figure 7.
An activated pattern produced fromthe word list: {red, alcoholic, drink}.where P(W) is the activated pattern producedfrom W by activating each wi E W with strengths(wl)2/~ s(wk) for 10 steps.
And, ?
is an outputfunction which limits the value to \[0,1\].As shown in Figure 7, bottle_l and wine_l havehigh activity in the pattern produced from the phrase"red alcoholic drink".
So, we may say that the over-lapped pattern implies % bottle of wine".For example, the similarity between l ingu is t i csand s ty l i s t i cs ,  both are the extra words, is com-puted as follows:~(linguistics, stylistics)= o({the, study, of, language, in,general, and, of, particular,languages, and, their, structure,and, grammar, and, history},{the, study, of, style, in,written, or, spoken, language} )= 0.140089.Obviously, both ~r(W,w) and a(w, W), where Wis an extra word and w is not, are also computable.Therefore, we can compute the similarity betweenany two headwords in LDOCE and their derivations.236text: Xxl x; x~?
JepisodesFigure 8.
Episode association on Paradigrae(recalling the most similar episode in memory).5 Applications of the SimilarityThis section shows the application of the similaritybetween words to text analysis - -  measuring similar-ity between texts, and measuring text coherence.5.1 Measur ing  S imi lar i ty  between TextsSuppose a text is a word list without syntactic struc-ture.
Then, the similarity ~r(X,X') between twotexts X, X '  can be computed as the similarity of ex-tra words described above.The following examples uggest hat the similar-ity between texts indicates the strength of coherencerelation between them:~("I have a bummer.
","Take  some nai ls ."
) = 0.100611 ,a("I have a bummer.
","Take some apples."
) = 0.005295 ,~("I have a pen.
","Where is ink?"
) = 0.113140 ,a("I  have a pen.
","Where do you l i ve?"
) = 0.007676 .It is worth noting that meaningless iteration ofwords (especially, of function words) has less influ-ence on the text similarity:a("It is a dog.
","That must be your dog."
)= 0.252536,ff("It is a doE.
","It i s  a log."
) = 0.053261 .The text similarity provides a semantic space fortext retrieval - -  to recall the most similar text inX' { 1 , ""  X '}  to the given text X.
Once the ac-tivated pattern P(X) of the text X is producedon Paradigms, we can compute and compare thesimilarity a(X, XI), .
- .
,  a(X, X') immediately.
(SeeFigure 8.
)5.2 Measur ing  Text  CoherenceLet us consider the reflective similarity a(X, X) ofa text X, and use the notation c(X) for a(X, X).Then, c(X) can be computed as follows:= ?
(E. x ,(,O,(P(X).,,,)).The activated pattern P(X), as shown in Figure 7,represents the average meaning of wl @ X.
So, c(X)represents cohesiveness of X - -  or semantic losenessof w 6 X, or semantic ompactness of X.
(It is alsoclosely related to distortion i clustering.
)The following examples uggest that  c(X) indi-cates the strength of coherence of X:c ("She opened the world with hertypewr i ter .
Her work was typ ing.But She did not type qu ick ly . "
)= 0.502510 (coherent),c ("Put on your c lo thes  at  once.I can not walk ten mi les .There i s  no one here but me."
)= 0.250840 (incoherent).However, a cohesive text can be incoherent; thefollowing example shows cohesiveness of the incoher-ent text - -  three sentences randomly selected fromLDOCE:c ("I saw a l i on .A l ion  be longs to the cat fami ly .My fami ly  keeps a pet . "
)= 0.560172 (incoherent, but cohesive).Thus, c(X) can not capture all the aspects of textcoherence.
This is because c(X) is based only on thelexical cohesion of the words in X.6 DiscussionThe structure of Paradigme represents the knowl-edge system of English, and an activated state pro-duced on it represents word meaning.
This sectiondiscusses the nature of the structure and states ofParadigms, and also the nature of the similarity com-puted on it.6.1 Parad igms and  Semant ic  SpaceThe set of all the possible activated patterns pro-duced on Paradigms can be considered as a seman-tic space where each state is represented as a point.The semantic space is a 2,851-dimensional hyper-cube; each of its edges corresponds to a word inLDV.LDV is selected according to the following infor-mation: the word frequency in written English, andthe range of contexts in which each word appears.So, LDV has a potential for covering all the conceptscommonly found in the world.This implies the completeness of LDV as dimen-sions of the semantic space.
Osgood's emantic dif-ferential procedure \[1952\] used 50 adjective dimen-sions; our semantic measurement uses 2,851 dimen-sions with completeness and objectivity.Our method can be applied to construct a se-mantic network from an ordinary dictionary whose237defining vocabulary is not restricted.
Such a net-work, however, is too large to spread activity overit.
Paradigme is the small and complete network formeasuring the similarity.6.2 Connotat ion  and  Extens ion  o f  WordsThe proposed similarity is based only on the deno-tational and intensional definitions in the dictionaryLDOCE.
Lack of the connotational nd extensionalknowledge causes some unexpected results of mea-suring the similarity.
For example, consider the fol-lowing similarity:~(t ree,  leaf) = 0.008693.This is due to the nature of the dictionary defi-n i t ions -  they only indicate sufficient conditions ofthe headword.
For example, the definition of t reein LDOCE tells nothing about leaves:t ree  n 1 a tall plant with a wooden trunk andbranches, that lives for many years 2 a bushor other plant with a treelike form 3 a drawingwith a branching form, esp.
as used for showingfamily relationshipsHowever, the definition is followed by pictures ofleafy trees providing readers with connotational ndextensional stereotypes of trees.6.3 Parad igmat ic  and Syntagmat icS imi lar i tyIn the proposed method, the definitions in LDOCEare treated as word lists, though they are phraseswith syntactic structures.
Let us consider the fol-lowing definition of l i f t :llft v 1 to bring from a lower to a higher level;raise 2 (of movable parts) to be able to belifted 3 ---Anyone can imagine that something is moving up-ward.
But, such a movement can not be representedin the activated pattern produced from the phrase.The meaning of a phrase, sentence, or text shouldbe represented as pattern changing in time, thoughwhat we need is static and paradigmatic relation.This paradox also arises in measuring the similar-ity between texts and the text coherence.
As we haveseen in Section 5, there is a difference between thesimilarity of texts and the similarity of word lists,and also between the coherence of a text and cohe-siveness of a word list.However, so far as the similarity between wordsis concerned, we assume that activated patterns onParadigme will approximate the meaning of words,like a still picture can express a story.7 ConclusionWe described measurement of semantic similarity be-tween words.
The similarity between words is com-puted by spreading activation on the semantic net-work Paradigme which is systematically constructedfrom a subset of the English dictionary LDOCE.Paradigme can directly compute the similarity be-tween any two words in LDV, and indirectly the sim-ilarity of all the other words in LDOCE.The similarity between words provides a newmethod for analysing the structure of text.
It can beapplied to computing the similarity between texts,and measuring the cohesiveness of a text which sug-gests coherence of the text, as we have seen in Sec-tion 5.
And, we are now applying it to text seg-mentation \[Grosz and Sidner, 1986; Youmans, 1991\],i.e.
to capture the shifts of coherent scenes in a story.In future research, we intend to deal with syntag-mat ic  relations between words.
Meaning of a text liesin the texture of paradigmatic and syntagmatic re-lations between words \[Hjelmslev, 1943\].
Paradigmeprovides the former dimension - -  an associative sys-tem of words - -  as a screen onto which the meaningof a word is projected like a still picture.
The latterdimension - -  syntactic process - -  will be treated asa film projected ynamically onto Paradigme.
Thisenables us to measure the similarity between textsas a syntactic process, not as word lists.We regard Paradigme as a field for the interac-tion between text and episodes in memory - -  theinteraction between what one is hearing or readingand what one knows \[Schank, 1990\].
The meaningof words, sentences, or even texts can be projectedin a uniform way on Paradigme, as we have seen inSection 4 and 5.
Similarly, we can project text andepisodes, and recall the most relevant episode for in-terpretation of the text.Appendix A.
Structure of Paradigmew Mapping Gloss~me onto ParadigmeThe semantic network Paradigme is systematicallyconstructed from the small and closed English dictio-nary Glossdme.
Each entry of Gloss~me is mappedonto a node of Paradigme in the following way.
(Seealso Figure 2 and 3.
)Step 1.
For each entry Gi in Glossdme, mapeach unit uij in Gi onto a subr6f~rant sij of thecorresponding node Pi in Paradigme.
Each wordwij,, E uij is mapped onto a link or links in sij, inthe following way:1.
Let t ,  be the reciprocal of the number of ap-pearance of wij, (as its root form) in GIoss~me.2.
If wij, is in a head-part, let t ,  be doubled.3.
Find nodes {Pnl ,P ,~," '}  corresponds to wlj,(ex.
red ~ {red_l, red_2}).
Then, divide t ,into {t ,x , t ,2 , .
.
.}
in proportion to their fre-quency.4.
Add links l , l, l,2,.., to sij, where Into is a linkto the node Pn,n with thickness t,,n.Thus, sij becomes a set of links: {lijl,lij2,...},where iijk is a link with thickness tijk.
Then, nor-238malise thickness of the links as ~"~k tlp, = 1, in eachStep 2.
For each node P/, compute thickness hijof each subr~f&ant sij in the following way:1.
Let m/be the number of subr~f~rants of P/.2.
Let hij be 2ml -1 - j .
(Note that hll : h/,n = 2 : 1.)3.
Normalize thickness hij as ~"~j h/j = 1, in eachP,.Step 3.
Generate r~f~r6 of each node inParadigme, in the following way:1.
For each node P/ in Paradigme, let its r~f~r~ ribe an empty set.2.
For each P~, for each subr~f~rant sij of Pi, foreach link lijk in sij:a.
Let Pii~ be the node referred by i/i~, and lett~i~ be thickness of Ilia.b.
Add a new link !
~ to r~f~r~ of Pi~, where !~ isa link to P/with thickness t' = h~i .t~j~.3.
Thus, each r~ becomes a set of links:{l'x, its,..-}, where 11i is a link with thicknesst~-.
Then, normalize thickness of the links ast i j -  1, in each ri.Appendix B.
Function of ParadigmeSpreading Activation RulesEach node Pi of the semantic network Paradigmecomputes its activity value vi(T+ 1) at time T+I  asfollows:v'(T+ l) = ?
( R~(T) + R~(T) ) 2 + e~(T) ,where R/(T) and R~(T) are activity (at time T) col-lected from the nodes referred in the r~f6rant andr~f~r~ respectively; q(T) E \[0, 1\] is activity givenfrom outside (at time T); the output function ?limits the value to \[0,1\].R/(T) is activity of the most plausible subr~fdrantin Pi, defined as follows:re(T) = S{m(T),m = argmaxj {hij .Sii(T)},where hii is thickness of the j-th subr~f~rant of P{.Sii(T) is the sum of weighted activity of the nodesreferred in the j-th subr~f~rant of P{, defined as fol-lows:S, i (T) = ~ tijk .a,jk (T),kwhere tljk is thickness of the k-th link of so.
, anda~j~(T) is activity (at time T) of the node referredby the k-th link of sij.R\[(T) is weighted activity of the nodes referred inthe r6f~r~ rl of P/:R~(T) = ~ t~t .a~k(T),where t~k is thickness of the/~-th link ofri, and a~k isactivity (at time T) of the node referred by the k-thlink of ri.References\[Church and Hanks, 1990\] K. Church and P. Hanks.Word association norms, mutual information, andlexicography.
Computational Linguistics, 16:22-29, 1990.\[Grosz and Sidner, 1986\] B. J. Grosz and C. L. Sid-ner.
Attention, intentions, and the structure ofdiscourse.
Computational Linguistics, 12:175-204,1986.\[Halliday and Hasan, 1976\] M. A. K. Halliday andR.
Hasan.
Cohesion in English.
Longrnan, Harlow,Essex, 1976.\[Hendler, 1989\] J.
A. Hendler.
Marker-passing overmicrofeatures: Towards a hybrid symbolic / con-nectionist model.
Cognitive Science, 13:79-106,1989.\[Hjelmslev, 1943\] L. Hjelmslev.
Omkring Sprogteo-riens Grundl~eggelse.
Akademisk Forlag, Kcben-havn, 1943.\[LDO, 1987\] Longman Dictionary of ContemporaryEnglish.
Longman, Harlow, Essex, new edition,1987.\[Morris and Hirst, 1991\] J. Morris and G. Hirst.Lexical cohesion computed by thesaural relationsas an indicator of the structure of text.
Computa-tional Linguistics, 17:21-48, 1991.\[Osgood, 1952\] C. E. Osgood.
The nature andmeasurement of meaning.
Psychological Bulletin,49:197-237, 1952.\[Schank, 1990\] R. C. Schank.
Tell Me a Story: ANew Look at Real and Artificial Memory.
Scribner,New York, 1990.\[Waltz and Pollack, 1985\] D. L. Waltz and J.
B. Pol-lack.
Massively parallel parsing: A strongly inter-active model of natural anguage interpretation.Cognitive Science, 9:51-74, 1985.\[West, 1953\] M. West.
A General Service List of En-glish Words.
Longman, Harlow, Essex, 1953.\[Wilks et al, 1989\] Y. Wilks, D. Fass, C. M. Guo,J.
McDonald, T. Plate, and B. Slator.
A tractablemachine dictionary as a resource for computa-tional semantics.
In B. Boguraev and E. J. Briscoe,editors, Computational Lexicography for NaturalLanguage Processing.
Longman, Harlow, Essex,1989.\[Youmans, 1991\] G. Youmans.
A new tool for dis-course analysis: The vocabulary-management pro-file.
Language, 67:763-789, 1991.239
