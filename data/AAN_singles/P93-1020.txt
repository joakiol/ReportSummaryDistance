INTENTION-BASED SEGMENTATION:HUMAN REL IAB IL ITY  AND CORRELAT ION WITH L INGUIST IC  CUESRebecca  J .
PassonneauDepar tment  of Computer  ScienceCo lumbia  Univers i tyNew York, NY 10027becky@cs.columbia.eduAbst rac tCertain spans of utterances in a discourse, referredto here as segments, are widely assumedto formcoherent units.
Further, the segmental structureof discourse has been claimed to constrain and beconstrained by many phenomena.
However, thereis weak consensus on the nature of segments andthe criteria for recognizing or generating them.
Wepresent quantitative r sults of a two part study us-ing a corpus of spontaneous, narrative monologues.The first part evaluates the statistical reliability ofhuman segmentation f our corpus, where speakerintention is the segmentation criterion.
We then usethe subjects' segmentations to evaluate the corre-lation of discourse segmentation with three linguis-tic cues (referential noun phrases, cue words, andpauses), using information retrieval metrics.INTRODUCTIONA discourse consists not simply of a linear se-quence of utterances, 1 hut of meaningful relationsamong the utterances.
As in much of the litera-ture on discourse processing, we assume that cer-tain spans of utterances, referred to here as dis-course segments, form coherent units.
The seg-mental structure of discourse has been claimed toconstrain and be constrained by disparate phe-nomena: cue phrases (Hirschberg and Litman,1993; Gross and Sidner, 1986; Reichman, 1985; Co-hen, 1984); lexical cohesion (Morris and Hirst,1991); plans and intentions (Carberry, 1990; Lit-man and Allen, 1990; Gross and Sidner, 1986);prosody (Grosz and Hirschberg, 1992; Hirschbergand Gross, 1992; Hirschberg and Pierrehumbert,1986); reference (Webber, 1991; Gross and Sidner,1986; Linde, 1979); and tense (Webber, 1988; Hwangand Schubert, 1992; Song and Cohen, 1991).
How-ever, there is weak consensus on the nature of seg-ments and the criteria for recognizing or generat-ing them in a natural anguage processing system.Until recently, little empirical work has been di-rected at establishing obje'~ively verifiable segmentboundaries, even though this is a precondition for1We use  the  te rm ut terance  to  mean a use  of  a sen-tence  or  o ther  l ingu is t i c  un i t ,  whether  in  text  or  spokenlanguage.Diane  J .
L i tmanAT&T Bell Laborator ies600 Mounta in  AvenueMurray  Hill, NJ  07974d iane@research.att .comSEGMENT 1Okay.tsk There's  ~ ,he looks like ay uh Chicano American,he is picking pears.A-nd u-m he's just  picking them,he comes off of the ladder,a-nd he- u-h puts his pears into the basket.SEGMENT 2U-h a number  of people are going by,and one is um /you  know/  I don' t  know,I can ' t  remember  the first .
.
.
the first person that  goes by.Oh.A u-m a man with a goat comes by.It see it seems to be a busy place.You know,fairly busy,i t 's  out in the country,maybe.
in  u-m u-h the valley or something.um \ [ -~  goes up the ladder, A-ndand picks some more pears.Figure 1: Discourse Segment Structureavoiding circularity in relating segments to linguis-tic phenomena.
We present he results of a twopart study on the reliability of human segmenta-tion, and correlation with linguistic ues.
We showthat human subjects can reliably perform discoursesegmentation using speaker intention as a criterion.We use the segmentations produced by our subjectsto quantify and evaluate the correlation of discoursesegmentation with three linguistic cues: referentialnoun phrases, cue words, and pauses.Figure 1 illustrates how discourse structure in-teracts with reference resolution in an excerpt akenfrom our corpus.
The utterances of this discourseare grouped into two hierarchically structured seg-ments, with segment 2 embedded in segment 1.
Thissegmental structure is crucial for determining thatthe boxed pronoun he corefers with the boxed nounphrase a farmer.
Without he segmentation, the ref-erent of the underlined noun phrase a man with agoat is a potential referent of the pronoun becauseit is the most recent noun phrase consistent withthe number and gender estrictions of the pronoun.With the segmentation a alysis, a man with a goatis ruled out on structural grounds; this noun phraseoccurs in segment 2, while the pronoun occurs afterresumption of segment 1.
A fa rmer  is thus the mostrecent noun phrase that is both consistent with, and148in the relevant interpretation context of, the pro-noun in question.One problem in trying to model such dis-course structure effects is that segmentation hasbeen observed to be rather subjective (Mann et al,1992; Johnson, 1985).
Several researchers have be-gun to investigate the ability of humans to agreewith one another on segmentation.
Grosz andHirschberg (Grosz and Hirschberg, 1992; Hirschbergand Grosz, 1992) asked subjects to structure threeAP news stories (averaging 450 words in length) ac-cording to the model of Grosz and Sidner (1986).Subjects identified hierarchical structures of dis-course segments, as well as local structural features,using text alone as well as text and professionallyrecorded speech.
Agreement ranged from 74%-95%,depending upon discourse feature.
Hearst (1993)asked subjects to place boundaries between para-graphs of three expository texts (length 77 to 160sentences), to indicate topic changes.
She foundagreement greater than 80%.
We present resultsof an empirical study of a large corpus of sponta-neous oral narratives, with a large number of poten-tial boundaries per narrative.
Subjects were askedto segment transcripts using an informal notion ofspeaker intention.
As we will see, we found agree-ment ranging from 82%-92%, with very high levelsof statistical significance (from p = .114 x 10 -6 top < .6 x 10-9).One of the goals of such empirical work is touse the results to correlate linguistic ues with dis-course structure.
By asking subjects to segmentdiscourse using a non-linguistic criterion, the corre-lation of linguistic devices with independently de-rived segments can be investigated.
Grosz andHirschberg (Grosz and Hirschberg, 1992; Hirschbergand Grosz, 1992) derived a discourse structure foreach text in their study, by incorporating the struc-tural features agreed upon by all of their subjects.They then used statistical measures to character-ize these discourse structures in terms of acoustic-prosodic features.
Morris and Hirst (1991) struc-tured a set of magazine texts using the theoryof Grosz and Sidner (1986).
They developed alexical cohesion algorithm that used the informa-tion in a thesaurus to segment text, then qualita-tively compared their segmentations with the re-suits.
Hearst (1993) derived adiscourse structure foreach text in her study, by incorporating the bound-aries agreed upon by the majority of her subjects.Hearst developed a lexical algorithm based on in-formation retrieval measurements to segment text,then qualitatively compared the results with thestructures derived from her subjects, as well as withthose produced by Morris and Hirst.
Iwanska (1993)compares her segmentations of factual reports withsegmentations produced using syntactic, semantic,and pragmatic information.
We derive segmenta-tions from our empirical data based on the statisti-cM significance of the agreement among subjects, orboundary strength.
We develop three segmentationalgorithms, based on results in the discourse litera-ture.
We use measures from information retrievalto quantify and evaluate the correlation betweenthe segmentations produced by our algorithms andthose derived from our subjects.REL IAB IL ITYThe correspondence between discourse segmentsand more abstract units of meaning is poorly under-stood (see (Moore and Pollack, 1992)).
A numberof alternative proposals have been presented whichdirectly or indirectly relate segments to intentions(Grosz and Sidner, 1986), RST relations (Mannet al, 1992) or other semantic relations (Polanyi,1988).
We present initial results of an investigationof whether naive subjects can reliably segment dis-course using speaker intention as a criterion.Our corpus consists of 20 narrative monologuesabout the same movie, taken from Chafe (1980)(N~14,000 words).
The subjects were introductorypsychology students at the University of Connecti-cut and volunteers solicited from electronic bulletinboards.
Each narrative was segmented by 7 sub-jects.
Subjects were instructed to identify each pointin a narrative where the speaker had completed onecommunicative task, and began a new one.
Theywere also instructed to briefly identify the speaker'sintention associated with each segment.
Intentionwas explained in common sense terms and by ex-ample (details in (Litman and Passonneau, 1993)).To simplify data collection, we did not ask sub-jects to identify the type of hierarchical relationsamong segments illustrated in Figure 1.
In a pilotstudy we conducted, subjects found it difficult andtime-consuming to identify non-sequential relations.Given that the average length of our narratives i700 words, this is consistent with previous findings(Rotondo, 1984) that non-linear segmentation is im-practical for naive subjects in discourses longer than200 words.
"Since prosodic phrases were already marked inthe transcripts, we restricted subjects to placingboundaries between prosodic phrases.
In principle,this makes it more likely that subjects will agreeon a given boundary than if subjects were com-pletely unrestricted.
However, previous tudies haveshown that the smallest unit subjects use in sim-ilar tasks corresponds roughly to a breath group,prosodic phrase, or clause (Chafe, 1980; Rotondo,1984; Hirschberg and Grosz, 1992).
Using smallerunits would have artificially lowered the probabilityfor agreement on boundaries.Figure 2 shows the responses of subjects at eachpotential boundary site for a portion of the excerptfrom Figure 1.
Prosodic phrases are numbered se-quentially, with the first field indicating prosodicphrases with sentence-final contours, and the second1493.3 \[.35+ \[.35\] a-nd\] he- u-h \[.3\] puts his pears into the basket.l 6 SUBJECTS I NP, PAUSE4.1 \[I.0 \[.5\] U-hi a number of people are going by,CUE, PAUSE4.2 \[.35+ and \[.35\]\] one is \[1.15 urn/ /you know/ I  don't know,4.3 I can't  remember the first .
.
.
the first person that goes by.\[ 1 SUBJECTS \[ PAUSE5.1 OhSUBJECTS I NP tl6.1 A u-m.. a man with a goat \[.2\] comes by.I\[2 SUBJECTS I NP, PAUSE7.1 \[.25\] It see it seems to be a busy place.PAUSE8.1 \[.1\] You know,8.2 fairly busy,I ,  suBJeCTS I8.3 it's out in the country,PAUSE8.4 \[.4\] maybe in u-m \[.8\] u-h the valley or something.\[7 SUBJECTS\[ NP, CUE, PAUSE9.1 \[2.95 \[.9\] A-nd um \[.25\] [.35\]\] he goes up the ladder,Figure 2: Excerpt from 9, with Boundariesfield indicating phrase-final contours.
2 Line spacesbetween prosodic phrases represent potential bound-ary sites.
Note that a majority of subjects agreedon only 2 of the 11 possible boundary sites: after 3.3(n=6) and after 8.4 (n=7).
(The symbols NP, CUEand PAUSE will be explained later.
)Figure 2 typifies our results.
Agreement amongsubjects was far from perfect, as shown by the pres-ence here of 4 boundary sites identified by only 1 or 2subjects.
Nevertheless, as we show in the followingsections, the degree of agreement among subjectsis high enough to demonstrate hat segments canbe reliably identified.
In the next section we dis-cuss the percent agreement among subjects.
In thesubsequent section we show that the frequency ofboundary sites where a majority of subjects assigna boundary is highly significant.AGREEMENT AMONG SUBJECTSWe measure the ability of subjects to agree with oneanother, using a figure called percent agreement.Percent agreement, defined in (Gale et al, 1992),is the ratio of observed agreements with the ma-jority opinion to possible agreements with the ma-jority opinion.
Here, agreement among four, five,six, or seven subjects on whether or not there is asegment boundary between two adjacent prosodicphrases constitutes a majority opinion.
Given atranscript of length n prosodic phrases, there aren-1 possible boundaries.
The total possible agree-ments with the majority corresponds to the numberof subjects times n-1.
Teral observed agreementsequals the number of times that subjects' bound-ary decisions agree with the majority opinion.
As2The transcripts presented to subjects did not con-tain line numbering or pause information (pauses indi-cated here by bracketed numbers.
)noted above, only 2 of the 11 possible boundariesin Figure 2 are boundaries using the majority opin-ion criterion.
There are 77 possible agreements withthe majority opinion, and 71 observed agreements.Thus, percent agreement for the excerpt as a wholeis 71/77, or 92%.
The breakdown of agreement onboundary and non-boundary majority opinions is13/14 (93%) and 58/63 (92%), respectively.The figures for percent agreement with the ma-jority opinion for all 20 narratives are shown in Ta-ble 1.
The columns represent the narratives in ourcorpus.
The first two rows give the absolute numberof potential boundary sites in each narrative (i.e., n-1) followed by the corresponding percent agreementfigure for the narrative as a whole.
Percent agree-ment in this case averages 89% (variance ~r=.0006;max.=92%; min.=82%).
The next two pairs of rowsgive the figures when the majority opinions are bro-ken down into boundary and non-boundary opin-ions, respectively.
Non-boundaries, with an averagepercent agreement of 91% (tr=.0006; max.=95%;min.=84%), show greater agreement among subjectsthan boundaries, where average percent agreementis 73% (or= .003; max.=80%; min.=60%).
Thispartly reflects the fact that non-boundaries greatlyoutnumber boundaries, an average of 89 versus 11majority opinions per transcript.
The low variances,or spread around the average, show that subjects arealso consistent with one another.Defining a task so as to maximize percent agree-ment can be difficult.
The high and consistent lev-els of agreement for our task suggest hat we havefound a useful experimental formulation of the taskof discourse segmentation.
Furthermore, our per-cent agreement figures are comparable with the re-sults of other segmentation studies discussed above.While studies of other tasks have achieved strongerresults (e.g., 96.8% in a word-sense disambiguationstudy (Gale et al, 1992)), the meaning of percentagreement in isolation is unclear.
For example, apercent agreement figure of less than 90% could stillbe very meaningful if the probability of obtainingsuch a figure is low.
In the next section we demon-strate the significance of our findings.STATISTICAL SIGNIFICANCEWe represent the segmentation data for each narra-tive as an { x j matrix of height i=7 subjects andwidth j=n-1.
The value in each cell ci,j is a one if theith subject assigned a boundary at site j, and a zeroif they did not.
We use Cochran's test (Cochran,1950) to evaluate significance of differences acrosscolumns in the matrix.
3Cochran's test assumes that the number of Iswithin a single row of the matrix is fixed by ob-servation, and that the totals across rows can vary.Here a row total corresponds to the total number3We thank Julia Hirschberg for suggesting this test.150Narrat ive  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20All Opinions 138 121 55 63 69 83 90 50 96 195 110 160 108 113 112 46 151 85 94 56Al~reement 87 82 91 89 89 90 90 90 90 88 92 90 91 89 85 89 92 91 91 86Boundary  21 16 7 10 6 5 11 5 8 22 13 17 9 11 8 7 15 11 10 6Agreement  74 70 76 77 60 80 79 69 75 70 74 75 73 71 68 73 77 71 80 74Non-Boundary% Agreement117 105 48 53 63 78 79 45 88 173 97 143 99 102 104 39 136 74 84 5089 84 93 91 92 91 92 92 92 90 95 91 93 91 87 92 93 94 93 88Table 1: Percent Agreement with the Majority Opinionof boundaries assigned by subject i.
In the case ofnarrative 9 (j=96), one of the subjects assigned 8boundaries.
The probability of a 1 in any of the jcells of the row is thus 8/96, with (9s6) ways for the8 boundaries to be distributed.
Taking this into ac-count for each row, Cochran's test evaluates the nullhypothesis that the number of ls in a column, herethe total number of subjects assigning a boundaryat the jth site, is randomly distributed.
Where therow totals are ui, the column totals are Tj,  and theaverage column total is T, the statistic is given by:Q approximates the X 2 distribution with j-1 de-grees of freedom (Cochran, 1950).
Our results indi-cate that the agreement among subjects is extremelyhighly significant.
That is, the number of 0s or ls incertain columns is much greater than would be ex-pected by chance.
For the 20 narratives, the prob-abilities of the observed distributions range fromp=.
l l4x  10 -6 top<,6x  10 -9 .The percent agreement analysis classified all thepotential boundary sites into two classes, boundariesversus non-boundaries, depending on how the ma-jority of subjects responded.
This is justified byfurther analysis of Q.
As noted in the preceding sec-tion, the proportion of non-boundaries agreed uponby most subjects (i.e., where 0 <Tj  < 3) is higherthan the proportion of boundaries they agree on(4 < Tj < 7).
That agreement on non-boundariesis more probable suggests that the significance of Qowes most to the cases where columns have a ma-jority of l's.
This assumption is borne out when Qis partitioned into distinct components for each pos-sible value of Tj (0 to 7), based on partioning thesum of squares in the numerator of Q into distinctsamples (Cochran, 1950).
We find that Qj is signif-icant for each distinct Tj > 4 across all narratives.For T j=4,  .0002 < p < .30 x 10-s; probabilitiesbecome more signfficant for higher levels of Tj ,  andthe converse.
At T j=3,  p is sometimes above oursignificance level of .01, depending on the narrative.DISCUSSION OF RESULTSWe have shown that an atheoretical notion ofspeaker intention is understood sufficiently uni-formly by naive subjects to yield significant agree-ment across subjects on segment boundaries in acorpus of oral narratives.
We obtained high levels ofpercent agreement on boundaries as well as on non-boundaries.
Because the average narrative length is100 prosodic phrases and boundaries are relativelyinfrequent (average boundary frequency=16%), per-cent agreement among ?
subjects (row one in Ta-ble 1) is largely determined by percent agreementon non-boundaries (row three).
Thus, total percentagreement could be very high, even if subjects didnot agree on any boundaries.
However, our resultsshow that percent agreement on boundaries is notonly high (row two), but also statistically significant.We have shown that boundaries agreed on by atleast 4 subjects are very unlikely to be the result ofchance.
Rather, they most likely reflect the validityof the notion of segment as defined here.
In Figure2, 6 of the 11 possible boundary sites were identi-fied by at least 1 subject.
Of these, only two wereidentified by a majority of subjects.
If we take thesetwo boundaries, appearing after prosodic phrases 3.3and 8.4, to be statistically validated, we arrive at alinear version of the segmentation used in Figure 1.In the next section we evaluate how well statisticallyvalidated boundaries correlate with the distributionof linguistic cues.CORRELAT IONIn this section we present and evaluate three dis-course segmentation algorithms, each based on theuse of a single linguistic cue: referential nounphrases (NPs), cue words, and pauses.
4 Whilethe discourse effects of these and other linguisticphenomena have been discussed in the literature,there has been little work on examining the use ofsuch effects for recognizing or generating segmentboundaries,  or on evaluating the comparative util-ity of different phenomena for these tasks.
The algo-rithms reported here were developed based on ideasin the literature, then evaluated on a representativeset of 10 narratives.
Our results allow us to directlycompare the performance of the three algorithms, tounderstand the utility of the individual knowledgesources.We have not yet attempted to create compre-hensive algorithms that would incorporate all pos-sible relevant features.
In subsequent phases of ourwork, we will tune the algorithms by adding and4The input to each algorithm is a discourse tran-scription labeled with prosodic phrases.
In addition,for the NP algorithm, noun phrases need to be labeledwith anaphoric relations.
The pause algorithm requirespauses to be noted.SA notable exception is the literature on pauses.151SubjectsAl~orithm Boundary Non-BoundaryBoundary a bNon-Boundary c dRecall Precision Fallout Errora/(a+c) a/(a+b) b/(b+d) (b+c)/(a+b+c+d)Table 2: Evaluation Metricsrefining features, using the initial 10 narratives asa training set.
Final evaluation will be on a testset corresponding to the 10 remaining narratives.The initial results reported here will provide us witha baseline for quantifying improvements resultingfrom distinct modifications to the algorithms.We use metrics from the area of informationretrieval to evaluate the performance of our algo-rithms.
The correlation between the boundariesproduced by an algorithm and those independentlyderived from our subjects can be represented as amatrix, as shown in Table 2.
The value a (in cellcz,1) represents the number .
of potential boundariesidentified by both the algorithm and the subjects, bthe number identified by the algorithm but not thesubjects, c the number identified by the subjects butnot the algorithm, and d the number neither the al-gorithm nor the subjects identified.
Table 2 alsoshows the definition of the four evaluation metricsin terms of these values.
Recall errors represent thefalse rejection of a boundary, while precision errorsrepresent he false acceptance of a boundary.
Analgorithm with perfect performance segments a dis-course by placing a boundary at all and only thoselocations with a subject boundary.
Such an algo-rithm has 100% recall and precision, and 0% falloutand error.For each narrative, our human segmentationdata provides us with a set of boundaries classifiedby 7 levels of subject strength: (1 < T/ < 7).That is, boundaries of strength 7 are the set of pos-sible boundaries identified by all 7 subjects.
As abaseline for examining the performance of our algo-rithms, we compare the boundaries produced by thealgorithms to boundaries of strength ~ >_ 4.
Theseare the statistically validated boundaries discussedabove, i.e., those boundari.~,,~ identified by 4 or moresubjects.
Note that recall for ~ > 4 correspondsto percent agreement for boundaries.
We also ex-amine the evaluation metrics for each algorithm,cross-classified by the individual evels of boundarystrength.REFERENTIAL  NOUN PHRASESOur procedure for encoding the input to the re-ferring expression algorithm takes 4 factors intoaccount, as documented in (Passonneau, 1993a).Briefly, we construct a 4-tuple for each referentialNP: <FIC, NP, i, I>.
FIC is clause location, NPis surface form, i is referential identity, and I is aset of inferential relations.
Clause location is de-25 16.1 You could hear the bicycler2,16.2 wheelsls going round.CODING <25, wheels, 13, (13 rl 12)>Figure 3: Sample Coding (from Narrative 4)termined by sequentially assigning distinct indicesto each functionally independent clause (FIC); anFIC is roughly equivalent to a tensed clause that isneither a verb argument nor a restrictive relative.Figure 3 illustrates the coding of an NP, wheels.It's location is FIC number 25.
The surface form isthe string wheels.
The wheels are new to the dis-course, so the referential index 13 is new.
The infer-ential relation (13 r l  12) indicates that the wheelsentity is related to the bicycle entity (index 12) bya part/whole relation.
6The input to the segmentation algorithm is alist of 4-tuples representing all the referential NPsin a narrative.
The output is a set of boundariesB, represented as ordered pairs of adjacent clauses:(F IC, ,F IC,+I) .
Before describing how boundariesare assigned, we explain that the potential bound-ary locations for the algorithm, between each FIC,differ from the potential boundary locations for thehuman study, between each prosodic phrase.
Caseswhere multiple prosodic phrases map to one FIC,as in Figure 3, simply reflect the use of additionallinguistic features to reject certain boundary sites,e.g., (16.1,16.2).
However, the algorithm has thepotential to assign multiple boundaries between ad-jacent prosodic phrases.
The example shown in Fig-ure 4 has one boundary site available to the humansubjects, between 3.1 and 3.2.
Because 3.1 consistsof multiple FICs (6 and 7) the algorithm can anddoes assign 2 boundaries here: (6,7) and (7,8).
Tonormalize the algorithm output, we reduce multipleboundaries at a boundary site to one, here (7,8).
Atotal of 5 boundaries are eliminated in 3 of the 10test narratives (out of 213 in all 10).
All the re-maining boundaries (here (3.1,3.2)) fall into class bof Table 2.The algorithm operates on the principle that ifan NP in the current FIC provides a referential linkto the current segment, the current segment contin-ues.
However, NPs and pronouns are treated differ-ently based on the notion of focus (cf.
(Passonneau,1993a).
A third person definite pronoun provides areferential link if its index occurs anywhere in thecurrent segment.
Any other NP type provides a ref-erential ink if its index occurs in the immediatelypreceding FIC.The symbol NP in Figure 2 indicates bound-aries assigned by the algorithm.
Boundary (3.3,4.1)is assigned because the sole NP in 4.1, a number ofpeople, refers to a new entity, one that cannot be in-ferred from any entity mentioned in 3.3.
Boundary6We use 5 inferrability relations (Passonneau, 1993a).Since there is a phrase boundary between the bicycle andwheels, we do not take bicycle to modify wheels.1526 3.1 A-nd he's not ... paying all that much attentionNP BOUNDARY7 because  you know the  pears  fall,NP BOUNDARY (no subjects)8 3.2 and he doesn't really notice,Figure 4: Multiple FICs in One Prosodic PhraseFORALL  F IC ,` , I  < n < lastIF CD,`  n CD,`_I ?
STHENCDs = CDs t9 CD,~% (COREFERENTIAL LINK TO NP IN F IC , , _  1)ELSE IFF , ,  n CD, , _  1 ~ ~THEN CDs  = CDs  U CD,`% (INFERENTIAL LINK TO NP IN F IC ,`_ I )ELSE IF PRO, ,  n CDs  ~ STHEN CDs  = CDs  U CD,`% (DEFINITE PRONOUN LINK TO SEGMENT)ELSE B = B t9 { (F IC ,`_ \ ] ,F IC , ` )}% (IF NO LINK, ADD A BOUNDARY)Figure 5: Referential NP Algorithm(8.4,9.1) results from the following facts about theNPs in 9.1: 1) the full NP the ladder is not referredto implicitly or explicitly in 8.4, 2) the third personpronoun he refers to an entity, the farmer, that waslast mentioned in 3.3, and 3 NP boundaries havebeen assigned since then.
If the farmer had been re-ferred to anywhere in 7.1 through 8.4, no boundarywould be assigned at (8.4,9.1).Figure 5 illustrates the three decision points ofthe algorithm.
FIC,* is the current clause (at lo-cation n); CD, is the set of all indices for NPs inFIC,;  F ,  is the set of entities that are inferrentiallylinked to entities in CDn; PRO,, is the subset of CD,where NP is a third person definite pronoun; CDn-1is the contextual domain for the previous FIC, andCDs is the contextual domain for the current seg-ment.
FIC,* continues the current segment if it isanaphorically inked to the preceding clause 1) by acoreferential NP, or 2) by an inferential relation, or3) if a third person definite pronoun in FIC,* refersto an entity in the current segment.
If no boundaryis added, CDs is updated with CDn.
If all 3 testsfail, FICn is determined to begin a new segment,and (FICn_I,FICn) is added to B.Table 3 shows the average performance ofthe referring expression algorithm (row labelledNP) on the 4 measures we use here.
Recallis .66 (a=.068; max=l;  min=.25), precision is.25 (a=.013; max=.44; min=.09), fallout is .16(~r=.004) and error rate is 0.17 (or=.005).
Notethat the error rate and fallout, which in a senseare more sensitive measures of inaccuracy, are bothmuch lower than the precision and have very lowvariance.
Both recall and precision have a relativelyhigh variance.CUE WORDSCue words (e.g., "now") are words that are some-times used to explicitly signal the structure of adiscourse.
We develop a b,'~eline segmentation al-gorithm based on cue words, using a simplificationof one of the features hown by Hirschberg and Lit-man (1993) to identify discourse usages of cue words.Hirschberg and Litman (1993) examine a large setof cue words proposed in the literature and showthat certain prosodic and structural features, in-cluding a position of first in prosodic phrase, arehighly correlated with the discourse uses of thesewords.
The input to our lower bound cue word al-gorithm is a sequential list of the prosodic phrasesconstituting a given narrative, the same input oursubjects received.
The output is a set of bound-aries B, represented as ordered pairs of adjacentphrases (P,,P,*+I), such that the first item in P,*+Iis a member of the set of cue words summarized inHirschberg and Litman (1993).
That is, if a cueword occurs at the beginning of a prosodic phrase,the usage is assumed to be discourse and thus thephrase is taken to be the beginning of a new seg-ment.
Figure 2 shows 2 boundaries (CUE) assignedby the algorithm, both due to and.Table 3 shows the average performance of thecue word algorithm for statistically validated bound-aries.
Recall is 72% (cr=.027; max=.88; min=.40),precision is 15% (or=.003; max=.23; min=.04), fall-out is 53% (a=.006) and error is 50% (~=.005).While recall is quite comparable to human perfor-mance (row 4 of the table), the precision is low whilefallout and error are quite high.
Precision, falloutand error have much lower variance, however.PAUSESGrosz and Hirschberg (Grosz and Hirschberg, 1992;Hirschberg and Grosz, 1992) found that in a cor-pus of recordings of AP news texts, phrases be-ginning discourse segments are correlated with du-ration of preceding pauses, while phrases endingdiscourse segments are correlated with subsequentpauses.
We use a simplification of these results todevelop a baseline algorithm for identifying bound-aries in our corpus using pauses.
The input to ourpause segmentation algorithm is a sequential list ofall prosodic phrases constituting a given narrative,with pauses (and their durations) noted.
The out-put is a set of boundaries B, represented as orderedpairs of adjacent phrases (P,*,Pn+I), such that thereis a pause between Pn and Pn+l- Unlike Grosz andHirschberg, we do not currently take phrase dura-tion into account.
In addition, since our segmenta-tion task is not hierarchical, we do not note whetherphrases begin, end, suspend, or resume segments.Figure 2 shows boundaries (PAUSE) assigned by thealgorithm.Table 3 shows the average performance of thepause algorithm for statistically validated bound-aries.
Recall is 92% (~=.008; max=l ;  min=.73),precision is 18% (~=.002; max=.25; min=.09), fall-out is 54% (a=.004), and error is 49% (a=.004).Our algorithm thus performs with recall higher thanhuman performance.
However, precision is low,153Recall Precision FalloutNP .66 .25 .16Cue .72 .15 .53Pause .92 .18 .54Humans .74 .55 .09Table 3: Evaluation for Tj > 4Error.17.50.49.11Tj 1 2 3 4 5 6 7NPsf Precision .18 .26 .15 .02 .15 .07 .06CuesI "1 ?
?1 Precision .17 .09 .08 .07 .04 .03 .02PausesPrecision .18 .10 .08 .06 .06 .04 .03Humanst "1 Precision .14 .14 .17 .15 .15 .13 .14Table 4: Variation with Boundary Strengthwhile both fallout and error are quite high.D ISCUSSION OF RESULTSIn order to evaluate the performance measures forthe algorithms, it is important o understand howindividual humans perform on all 4 measures.
Row4 of Table 3 reports the average individual perfor-mance for the 70 subjects on the 10 narratives.
Theaverage recall for humans is .74 (~=.038), ~ and theaverage precision is .55 (a=.027), much lower thanthe ideal scores of 1.
The fallout and error rates of.09 (~=.004) and .11 (a=.003) more closely approx-imate the ideal scores of 0.
The low recall and preci-sion reflect the considerable variation in the numberof boundaries ubjects assign, as well as the imper-fect percent agreement (Table 1).To compare algorithms, we must take into ac-count the dimensions along which they differ apartfrom the different cues.
For example, the referringexpression algorithm (RA) differs markedly from thepause and cue algorithms (PA, CA) in using moreknowledge.
CA and PA depend only on the abilityto identify boundary sites, potential cue words andpause locations while RA relies on 4 features of NPsto make 3 different ests (Figure 5).
Unsurprisingly,RA performs most like humans.
For both CA andPA, the recall is relatively high, but the precisionis very low, and the fallout and error rate are bothvery high.
For lZA, recall and precision are not asdifferent, precision is higher than CA and PA, andfallout and error rate are both relatively low.A second dimension to consider in comparing7Human recall is equivalent to percent agreement forboundaries.
However, the average shown here representsonly 10 narratives, while the average from Table 1 rep-resents all 20.performance is that humans and RA assign bound-aries based on a global criterion, in contrast o CAand PA.
Subjects typically use a relatively grosslevel of speaker intention.
By default, RA assumesthat the current segment continues, and assigns aboundary under relatively narrow criteria.
However,CA and PA rely on cues that are relevant at the localas well as the global level, and consequently assignboundaries more often.
This leads to a preponder-ance of cases where PA and CA propose a boundarybut where a majority of humans did not, categoryb from Table 2.
High b lowers precision, reflected inthe low precision for CA and PA.We are optimistic that all three algorithms canbe improved, for example, by discriminating amongtypes of pauses, types of cue words, and features ofreferential NPs.
We have enhanced RA with cer-tain grammatical role features following (Passon-neau, 1993b).
In a preliminary experiment usingboundaries from our first set of subjects (4 per nar-rative instead of 7), this increased both recall andprecision by ,~ 10%.The statistical results validate boundariesagreed on by a majority of subjects, but do notthereby invalidate boundaries proposed by only 1-3subjects.
We evaluate how performance varies withboundary strength (1 _< 7) _< 7).
Table 4 showsrecall and precision of RA, PA, CA and humanswhen boundaries are broken down into those identi-fied by exactly 1 subject, exactly 2, and so on up to7.
8 There is a strong tendency for recall to increaseand precision to decrease as boundary strength in-creases.
We take this as evidence that the presenceof a boundary is not a binary decision; rather, thatboundaries vary in perceptual salience.CONCLUSIONWe have shown that human subjects can reliablyperform linear discourse segmentation i a corpusof transcripts of spoken narratives, using an infor-mal notion of speaker intention.
We found that per-cent agreement with the segmentations produced bythe majority of subjects ranged from 82%-92%, withan average across all narratives of 89% (~=.0006).We found that these agreement results were highlysignificant, with probabilities of randomly achiev-ing our findings ranging from p = .114 x 10 -6 top < .6 x 10 -9.We have investigated the correlation of ourintention-based discourse segmentations with refer-ential noun phrases, cue words, and pauses.
We de-veloped segmentation algorithms based on the use ofeach of these linguistic cues, and quantitatively eval-uated their performance in identifying the statisti-cally validated boundaries independently producedby our subjects.
We found that compared to hu-man performance, the recall of the three algorithmsSFallout and error rate do not vary much across T i .154was comparable, the precision was much lower, andthe fallout and error of only the noun phrase algo-rithm was comparable.
We also found a tendencyfor recall to increase and precision to decrease withexact boundary strength, suggesting that the cogni-tive salience of boundaries i  graded.While our initial results are promising, there iscertainly room for improvement.
In future work onour data, we will attempt to maximize the corre-lation of our segmentations with linguistic cues byimproving the performance of our individual algo-rithms, and by investigating ways to combine ouralgorithms (cf.
Grosz and Hirschberg (1992)).
Wewill also explore the use of alternative valuationmetrics (e.g.
string matching) to support close aswell as exact correlation.ACKNOWLEDGMENTSThe authors wish to thank W. Chafe, K. Church, J.DuBois, B. Gale, V. Hatzivassiloglou, M. Hearst, J.Hirschberg, J. Klavans, D. Lewis, E. Levy, K. McK-eown, E. Siegel, and anonymous reviewers for helpfulcomments, references and resources.
Both authors' workwas partially supported by DARPA and ONR undercontract N00014-89-J-1782; Passonneau was also partlysupported by NSF  grant IRI-91-13064.REFERENCESS.
Carberry.
1990.
Plan Recognition in Natural Lan-guage Dialogue.
MIT Press, Cambridge, MA.W.
L. Chafe.
1980.
The Pear Stories: Cognitive, Cul-tural and Linguistic Aspects of Narrative Produc-tion.
Ablex Publishing Corporation, Norwood, NJ.W.
G. Cochran.
1950.
The comparison of percentagesin matched samples.
Biometrika, 37:256-266.R.
Cohen.
1984.
A computational theory of the functionof clue words in argument understanding.
In Proc.of COLINGS4, pages 251-258, Stanford.W.
Gale, K. W. Church, and D. Yarowsky.
1992.
Esti-mating upper and lower bounds on the performanceof word-sense disambiguation programs.
In Proc.
ofA CL, pages 249-256, Newark, Delaware.B.
Grosz and J. Hirschberg.
1992.
Some intonationalcharacteristics of discourse structure.
In Proc.
ofthe International Conference on Spoken LanguageProcessing.B.
J. Grosz and C. L. Sidner.
1986.
Attention, inten-tions and the structure of discourse.
ComputationalLinguistics, 12:175-204.M.
A. Hearst.
1993.
TextTiling: A quantitative ap-proach to discourse segmentation.
Technical Report93/24, Sequoia 2000 Technical Report, University ofCalifornia, Berkeley.J.
Hirschberg and B. Grosz.
1992.
Intonational featuresof local and global discourse structure.
In Proc.
ofDarpa Workshop on Speech and Natural Language.J.
Hirschberg and D. Litman.
1993.
Empirical studieson the disambiguation of cue phrases.
Computa-tional Linguistics, 19.J.
Hirschberg and J. Pierrehumbert.
1986.
The intona-tional structuring of discourse.
In Proc.
of ACL.C.
H. Hwang and L. K. Schubert.
1992.
Tense trees asthe 'fine structure' of discourse.
In Proc.
of the 30thAnnual Meeting of the ACL, pages 232-240.L.
Iwafiska.
1993.
Discourse structure in factual report-ing (in preparation).N.
S. Johnson.
1985.
Coding and analyzing experimen-tal protocols.
In T. A.
Van Dijk, editor, Handbookof Discourse Analysis, Vol.
~: Dimensions of Dis-course.
Academic Press, London.C.
Linde.
1979.
Focus of attention and the choice ofpronouns in discourse.
In T. Givon, editor, Syntaxand Semantics: Discourse and Syntax, pages 337-354.
Academic Press, New York.D.
Litman and J. Allen.
1990.
Discourse processing andcommonsense plans.
In P. R. Cohen, J. Morgan,and M. E. Pollack, editors, Intentions in Commu-nication.
MIT Press, Cambridge, MA.D.
Litman and R. Passonneau.
1993.
Empirical ev-idence for intention-based discourse segmentation.In Proc.
of the A CL Workshop on lntentionality andStructure in Discourse Relations.W.
C. Mann, C. M. Matthiessen, and S. A. Thompson.1992.
Rhetorical structure theory and text analy-sis.
In W. C. Mann and S. A. Thompson, editors,Discourse Description.
J Benjamins Pub.
Co., Am-sterdam.J.
D. Moore and M. E. Pollack.
1992.
A problem forRST: The need for multi-level discourse analysis.Computational Linguistics, 18:537-544.J.
Morris and G. Hirst.
1991.
Lexical cohesion computedby thesaural relations as an indicator of the struc-ture of text.
Computational Linguistics, 17:21-48.R.
J. Passonneau.
1993a.
Coding scheme and algorithmfor identification of discourse segment boundarieson the basis of the distribution of referential nounphrases.
Technical report, Columbia University.R.
J. Passonneau.
1993b.
Getting and keeping the cen-ter of attention.
In R. Weischedel and M. Bates,editors, Challenges in Natural Language Processing.Cambridge University Press.L.
Polanyi.
1988.
A formal model of the structure ofdiscourse.
Journal of Pragmatics, 12:601-638.R.
Reichman.
1985.
Getting Computers to TalkLike You and Me.
MIT Press, Cambridge, Mas-sachusetts.J.
A. Rotondo.
1984.
Clustering analysis of subjectpartitions of text.
Discourse Processes, 7:69-88.F.
Song and R. Cohen.
1991.
Tense interpretation ithe context of narrative.
In Proc.
of AAA1, pages131-136.B.
L. Webber.
1988.
Tense as discourse anaphor.
Com-putational Linguistics, 14:113-122.B.
L. Webber.
1991.
Structure and ostension in the in-terpretation ofdiscourse deixis.
Language and Cog-nitive Processes, pages 107-135.155
