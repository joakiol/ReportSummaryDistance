Proceedings of the 43rd Annual Meeting of the ACL, pages 215?222,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsQuestion Answering as Question-Biased Term Extraction:A New Approach toward Multilingual QAYutaka SasakiDepartment of Natural Language ProcessingATR Spoken Language Communication Research Laboratories2-2-2 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0288 Japanyutaka.sasaki@atr.jpAbstractThis paper regards Question Answering(QA) as Question-Biased Term Extraction(QBTE).
This new QBTE approach lib-erates QA systems from the heavy bur-den imposed by question types (or answertypes).
In conventional approaches, a QAsystem analyzes a given question and de-termines the question type, and then it se-lects answers from among answer candi-dates that match the question type.
Con-sequently, the output of a QA system isrestricted by the design of the questiontypes.
The QBTE directly extracts an-swers as terms biased by the question.
Toconfirm the feasibility of our QBTE ap-proach, we conducted experiments on theCRL QA Data based on 10-fold cross val-idation, using Maximum Entropy Models(MEMs) as an ML technique.
Experimen-tal results showed that the trained systemachieved 0.36 in MRR and 0.47 in Top5accuracy.1 IntroductionThe conventional Question Answering (QA) archi-tecture is a cascade of the following building blocks:Question Analyzer analyzes a question sentenceand identifies the question types (or answertypes).Document Retriever retrieves documents relatedto the question from a large-scale document set.Answer Candidate Extractor extracts answercandidates that match the question types fromthe retrieved documents.Answer Selector ranks the answer candidates ac-cording to the syntactic and semantic confor-mity of each answer with the question and itscontext in the document.Typically, question types consist of named en-tities, e.g., PERSON, DATE, and ORGANIZATION,numerical expressions, e.g., LENGTH, WEIGHT,SPEED, and class names, e.g., FLOWER, BIRD, andFOOD.
The question type is also used for selectinganswer candidates.
For example, if the question typeof a given question is PERSON, the answer candidateextractor lists only person names that are tagged asthe named entity PERSON.The conventional QA architecture has a drawbackin that the question-type system restricts the range ofquestions that can be answered by the system.
It isthus problematic for QA system developers to care-fully design and build an answer candidate extrac-tor that works well in conjunction with the question-type system.
This problem is particularly difficultwhen the task is to develop a multilingual QA sys-tem to handle languages that are unfamiliar to thedeveloper.
Developing high-quality tools that canextract named entities, numerical expressions, andclass names for each foreign language is very costlyand time-consuming.Recently, some pioneering studies have inves-tigated approaches to automatically construct QAcomponents from scratch by applying machinelearning techniques to training data (Ittycheriah etal., 2001a)(Ittycheriah et al, 2001b)(Ng et al, 2001)(Pasca and Harabagiu)(Suzuki et al, 2002)(Suzuki215Table 1: Number of Questions in Question Types of CRL QA Data# of Questions # of Question Types Example1-9 74 AWARD, CRIME, OFFENSE10-50 32 PERCENT, N PRODUCT, YEAR PERIOD51-100 6 COUNTRY, COMPANY, GROUP100-300 3 PERSON, DATE, MONEYTotal 115et al, 2003) (Zukerman and Horvitz, 2001)(Sasakiet al, 2004).
These approaches still suffer from theproblem of preparing an adequate amount of trainingdata specifically designed for a particular QA sys-tem because each QA system uses its own question-type system.
It is very typical in the course of sys-tem development to redesign the question-type sys-tem in order to improve system performance.
Thisinevitably leads to revision of a large-scale trainingdataset, which requires a heavy workload.For example, assume that you have to develop aChinese or Greek QA system and have 10,000 pairsof question and answers.
You have to manually clas-sify the questions according to your own question-type system.
In addition, you have to annotate thetags of the question types to large-scale Chinese orGreek documents.
If you wanted to redesign thequestion type ORGANIZATION to three categories,COMPANY, SCHOOL, and OTHER ORGANIZATION,then the ORGANIZATION tags in the annotated doc-ument set would need to be manually revisited andrevised.To solve this problem, this paper regards Ques-tion Answering as Question-Biased Term Extraction(QBTE).
This new QBTE approach liberates QAsystems from the heavy burden imposed by questiontypes.Since it is a challenging as well as a very com-plex and sensitive problem to directly extract an-swers without using question types and only usingfeatures of questions, correct answers, and contextsin documents, we have to investigate the feasibilityof this approach: how well can answer candidatesbe extracted, and how well are answer candidatesranked?In response, this paper employs the ma-chine learning technique Maximum Entropy Models(MEMs) to extract answers to a question from doc-uments based on question features, document fea-tures, and the combined features.
Experimental re-sults show the performance of a QA system that ap-plies MEMs.2 Preparation2.1 Training DataDocument Set Japanese newspaper articles of TheMainichi Newspaper published in 1995.Question/Answer Set We used the CRL1 QAData (Sekine et al, 2002).
This dataset com-prises 2,000 Japanese questions with correctanswers as well as question types and IDs ofarticles that contain the answers.
Each ques-tion is categorized as one of 115 hierarchicallyclassified question types.The document set is used not only in the trainingphase but also in the execution phrase.Although the CRL QA Data contains questiontypes, the information of question types are not usedfor the training.
This is because more than the 60%of question types have fewer than 10 questions asexamples (Table 1).
This means it is very unlikelythat we can train a QA system that can handle this60% due to data sparseness.
2 Only for the purposeof analyzing experimental results in this paper do werefer to the question types of the dataset.2.2 Learning with Maximum Entropy ModelsThis section briefly introduces the machine learningtechnique Maximum Entropy Models and describeshow to apply MEMs to QA tasks.2.2.1 Maximum Entropy ModelsLet X be a set of input symbols and Y be a setof class labels.
A sample (x, y) is a pair of inputx={x1,.
.
.
, xm} (xi ?
X ) and output y ?
Y .1Presently, National Institute of Information and Communi-cations Technology (NICT), Japan2A machine learning approach to hierarchical question anal-ysis was reported in (Suzuki et al, 2003), but training and main-taining an answer extractor for question types of fine granularityis not an easy task.216The Maximum Entropy Principle (Berger et al,1996) is to find a model p?
= argmaxp?CH(p), whichmeans a probability model p(y|x) that maximizesentropy H(p).Given data (x(1), y(1)),.
.
.,(x(n), y(n)), let?k(x(k) ?
{y(k)}) = {?x?1, y?1?, ..., ?x?i, y?i?, ...,?x?m, y?m?}.
This means that we enumerate all pairsof an input symbol and label and represent them as?x?i, y?i?
using index i (1 ?
i ?
m).In this paper, feature function fi is defined as fol-lows.fi(x, y) ={1 if x?i ?
x and y = y?i0 otherwiseWe use all combinations of input symbols in x andclass labels for features (or the feature function) ofMEMs.With Lagrangian ?
= ?1, ..., ?m, the dual func-tion of H is:?(?)
= ??xp?
(x) log Z?
(x) +??ip?
(fi),where Z?
(x) =?yexp(?i?ifi(x, y)) and p?
(x)and p?
(fi) indicate the empirical distribution of x andfi in the training data.The dual optimization problem ??
=argmax??(?)
can be efficiently solved as anoptimization problem without constraints.
As aresult, probabilistic model p?
= p??
is obtained as:p??
(y|x) =1Z?
(x)exp(?i?ifi(x, y)).2.2.2 Applying MEMs to QAQuestion analysis is a classification problem thatclassifies questions into different question types.Answer candidate extraction is also a classifica-tion problem that classifies words into answer types(i.e., question types), such as PERSON, DATE, andAWARD.
Answer selection is an exactly classifica-tion that classifies answer candidates as positive ornegative.
Therefore, we can apply machine learningtechniques to generate classifiers that work as com-ponents of a QA system.In the QBTE approach, these three components,i.e., question analysis, answer candidate extraction,and answer selection, are integrated into one classi-fier.To successfully carry out this goal, we have toextract features that reflect properties of correct an-swers of a question in the context of articles.3 QBTE Model 1This section presents a framework, QBTE Model1, to construct a QA system from question-answerpairs based on the QBTE Approach.
When a usergives a question, the framework finds answers to thequestion in the following two steps.Document Retrieval retrieves the top N articles orparagraphs from a large-scale corpus.QBTE creates input data by combining the questionfeatures and documents features, evaluates theinput data, and outputs the top M answers.3Since this paper focuses on QBTE, this paper usesa simple idf method in document retrieval.Let wi be words and w1,w2,.
.
.wm be a docu-ment.
Question Answering in the QBTE Model 1involves directly classifying words wi in the docu-ment into answer words or non-answer words.
Thatis, given input x(i) for wi, its class label is selectedfrom among {I, O, B} as follows:I: if the word is in the middle of the answer wordsequence;O: if the word is not in the answer word sequence;B: if the word is the start word of the answer wordsequence.The class labeling system in our experiment isIOB2 (Sang, 2000), which is a variation ofIOB (Ramshaw and Marcus, 1995).Input x(i) of each word is defined as described be-low.3.1 Feature ExtractionThis paper employs three groups of features as fea-tures of input data:?
Question Feature Set (QF);?
Document Feature Set (DF);?
Combined Feature Set (CF), i.e., combinationsof question and document features.3In this paper, M is set to 5.2173.1.1 Question Feature Set (QF)A Question Feature Set (QF) is a set of featuresextracted only from a question sentence.
This fea-ture set is defined as belonging to a question sen-tence.The following are elements of a Question FeatureSet:qw: an enumeration of the word n-grams (1 ?n ?
N ), e.g., given question ?What is CNN?
?,the features are {qw:What, qw:is, qw:CNN,qw:What-is, qw:is-CNN } if N = 2,qq: interrogative words (e.g., who, where, what,how many),qm1: POS1 of words in the question, e.g., given?What is CNN?
?, { qm1:wh-adv, qm1:verb,qm1:noun } are features,qm2: POS2 of words in the question,qm3: POS3 of words in the question,qm4: POS4 of words in the question.POS1-POS4 indicate part-of-speech (POS) of theIPA POS tag set generated by the Japanese mor-phological analyzer ChaSen.
For example, ?Tokyo?is analyzed as POS1 = noun, POS2 = propernoun,POS3 = location, and POS4 = general.
This paperused up to 4-grams for qw.3.1.2 Document Feature Set (DF)Document Feature Set (DF) is a feature set ex-tracted only from a document.
Using only DF corre-sponds to unbiased Term Extraction (TE).For each word wi, the following features are ex-tracted:dw?k,.
.
.,dw+0,.
.
.,dw+k: k preceding and follow-ing words of the word wi, e.g., { dw?1:wi?1,dw+0:wi, dw+1:wi+1} if k = 1,dm1?k,.
.
.,dm1+0,.
.
.,dm1+k: POS1 of k preced-ing and following words of the word wi,dm2?k,.
.
.,dm2+0,.
.
.,dm2+k: POS2 of k preced-ing and following words of the word wi,dm3?k,.
.
.,dm3+0,.
.
.,dm3+k: POS3 of k preced-ing and following words of the word wi,dm4?k,.
.
.,dm4+0,.
.
.,dm4+k: POS4 of k preced-ing and following words of the word wi.In this paper, k is set to 3 so that the window size is7.3.1.3 Combined Feature Set (CF)Combined Feature Set (CF) contains features cre-ated by combining question features and documentfeatures.
QBTE Model 1 employs CF.
For each wordwi, the following features are created.cw?k,.
.
.,cw+0,.
.
.,cw+k: matching results(true/false) between each of dw?k,...,dw+kfeatures and any qw feature, e.g., cw?1:true ifdw?1:President and qw: President,cm1?k,.
.
.,cm1+0,.
.
.,cm1+k: matching results(true/false) between each of dm1?k,...,dm1+kfeatures and any POS1 in qm1 features,cm2?k,.
.
.,cm2+0,.
.
.,cm2+k: matching results(true/false) between each of dm2?k,...,dm2+kfeatures and any POS2 in qm2 features,cm3?k,.
.
.,cm3+0,.
.
.,cm3+k: matching results(true/false) between each of dm3?k,...,dm3+kfeatures and any POS3 in qm3 features,cm4?k,.
.
.,cm4+0,.
.
.,cm4+k: matching results(true/false) between each of dm4?k,...,dm4+kfeatures and any POS4 in qm4 features,cq?k,.
.
.,cq+0,.
.
.,cq+k: combinations of each ofdw?k,...,dw+k features and qw features, e.g.,cq?1:President&Who is a combination of dw?1:President and qw:Who.3.2 Training and ExecutionThe training phase estimates a probabilistic modelfrom training data (x(1),y(1)),...,(x(n),y(n)) gener-ated from the CRL QA Data.
The execution phaseevaluates the probability of y?
(i) given inputx?
(i) us-ing the the probabilistic model.Training Phase1.
Given question q, correct answer a, and docu-ment d.2.
Annotate ?A?
and ?/A?
right before and afteranswer a in d.3.
Morphologically analyze d.4.
For d = w1, ..., ?A?, wj , ..., wk, ?/A?, ..., wm,extract features as x(1),...,x(m).5.
Class label y(i) = B if wi follows ?A?, y(i) = Iif wi is inside of ?A?
and ?/A?, and y(i) = Ootherwise.218Table 2: Main Results with 10-fold Cross ValidationCorrect Answer Rank MRR Top51 2 3 4 5Exact match 453 139 68 35 19 0.28 0.36Partial match 684 222 126 80 48 0.43 0.58Ave.
0.355 0.47Manual evaluation 578 188 86 55 34 0.36 0.476.
Estimate p??
from (x(1),y(1)),...,(x(n),y(n)) us-ing Maximum Entropy Models.The execution phase extracts answers from re-trieved documents as Term Extraction, biased by thequestion.Execution Phase1.
Given question q and paragraph d.2.
Morphologically analyze d.3.
For wi of d = w1, ..., wm, create input datax?
(i) by extracting features.4.
For each y?
(j) ?
Y , compute p?
?
(y?(j)|x?
(i)),which is a probability of y?
(j) given x?(i).5.
For each x?
(i), y?
(j) with the highest probabilityis selected as the label of wi.6.
Extract word sequences that start with the wordlabeled B and are followed by words labeled Ifrom the labeled word sequence of d.7.
Rank the top M answers according to the prob-ability of the first word.This approach is designed to extract only the mosthighly probable answers.
However, pin-pointingonly answers is not an easy task.
To select the topfive answers, it is necessary to loosen the conditionfor extracting answers.
Therefore, in the executionphase, we only give label O to a word if its probabil-ity exceeds 99%, otherwise we give the second mostprobable label.As a further relaxation, word sequences that in-clude B inside the sequences are extracted for an-swers.
This is because our preliminary experimentsindicated that it is very rare for two answer candi-dates to be adjacent in Question-Biased Term Ex-traction, unlike an ordinary Term Extraction task.4 Experimental ResultsWe conducted 10-fold cross validation using theCRL QA Data.
The output is evaluated using theTop5 score and MRR.Top5 Score shows the rate at which at least onecorrect answer is included in the top 5 answers.MRR (Mean Reciprocal Rank) is the average re-ciprocal rank (1/n) of the highest rank n of acorrect answer for each question.Judgment of whether an answer is correct is doneby both automatic and manual evaluation.
Auto-matic evaluation consists of exact matching and par-tial matching.
Partial matching is useful for ab-sorbing the variation in extraction range.
A partialmatch is judged correct if a system?s answer com-pletely includes the correct answer or the correct an-swer completely includes a system?s answer.
Table 2presents the experimental results.
The results showthat a QA system can be built by using our QBTE ap-proach.
The manually evaluated performance scoredMRR=0.36 and Top5=0.47.
However, manual eval-uation is costly and time-consuming, so we use au-tomatic evaluation results, i.e., exact matching re-sults and partial matching results, as a pseudo lower-bound and upper-bound of the performances.
Inter-estingly, the manual evaluation results of MRR andTop5 are nearly equal to the average between exactand partial evaluation.To confirm that the QBTE ranks potential answersto the higher rank, we changed the number of para-graphs retrieved from a large corpus from N =1, 3, 5 to 10.
Table 3 shows the results.
Whereasthe performances of Term Extraction (TE) and TermExtraction with question features (TE+QF) signifi-cantly degraded, the performance of the QBTE (CF)did not severely degrade with the larger number ofretrieved paragraphs.219Table 3: Answer Extraction from Top N documentsFeature set Top N paragraphs Match Correct Answer Rank MRR Top51 2 3 4 51 Exact 102 109 80 71 62 0.11 0.21Partial 207 186 155 153 121 0.21 0.413 Exact 65 63 55 53 43 0.07 0.14TE (DF) Partial 120 131 112 108 94 0.13 0.285 Exact 51 38 38 36 36 0.05 0.10Partial 99 80 89 81 75 0.10 0.2110 Exact 29 17 19 22 18 0.03 0.07Partial 59 38 35 49 46 0.07 0.141 Exact 120 105 94 63 80 0.12 0.
23Partial 207 198 175 126 140 0.21 0 .42TE (DF) 3 Exact 65 68 52 58 57 0.07 0.15+ Partial 119 117 111 122 106 0.13 0.29QF 5 Exact 44 57 41 35 31 0.05 0.10Partial 91 104 71 82 63 0.10 0.2110 Exact 28 42 30 28 26 0.04 0.08Partial 57 68 57 56 45 0.07 0.141 Exact 453 139 68 35 19 0.28 0.36Partial 684 222 126 80 48 0.43 0.583 Exact 403 156 92 52 43 0.27 0.37QBTE (CF) Partial 539 296 145 105 92 0.42 0.625 Exact 381 153 92 59 50 0.26 0.37Partial 542 291 164 122 102 0.40 0.6110 Exact 348 128 92 65 57 0.24 0.35Partial 481 257 173 124 102 0.36 0.575 DiscussionOur approach needs no question type system, and itstill achieved 0.36 in MRR and 0.47 in Top5.
Thisperformance is comparable to the results of SAIQA-II (Sasaki et al, 2004) (MRR=0.4, Top5=0.55)whose question analysis, answer candidate extrac-tion, and answer selection modules were indepen-dently built from a QA dataset and an NE dataset,which is limited to eight named entities, such asPERSON and LOCATION.
Since the QA dataset isnot publicly available, it is not possible to directlycompare the experimental results; however we be-lieve that the performance of the QBTE Model 1 iscomparable to that of the conventional approaches,even though it does not depend on question types,named entities, or class names.Most of the partial answers were judged correctin manual evaluation.
For example, for ?How manytimes bigger ...?
?, ?two times?
is a correct answerbut ?two?
was judged correct.
Suppose that ?JohnKerry?
is a prepared correct answer in the CRL QAData.
In this case, ?Senator John Kerry?
would alsobe correct.
Such additions and omissions occur be-cause our approach is not restricted to particular ex-traction units, such as named entities or class names.The performance of QBTE was affected little bythe larger number of retrieved paragraphs, whereasthe performances of TE and TE + QF significantlydegraded.
This indicates that QBTE Model 1 is notmere Term Extraction with document retrieval butTerm Extraction appropriately biased by questions.Our experiments used no information about ques-tion types given in the CRL QA Data because we areseeking a universal method that can be used for anyQA dataset.
Beyond this main goal, as a reference,The Appendix shows our experimental results clas-sified into question types without using them in thetraining phase.
The results of automatic evaluationof complete matching are in Top5 (T5), and MRRand partial matching are in Top5 (T5?)
and MRR?.It is interesting that minor question types were cor-rectly answered, e.g., SEA and WEAPON, for whichthere was only one training question.We also conducted an additional experiment, as areference, on the training data that included questiontypes defined in the CRL QA Data; the question-type of each question is added to the qw feature.
Theperformance of QBTE from the first-ranked para-graph showed no difference from that of experi-ments shown in Table 2.2206 Related WorkThere are two previous studies on integratingQA components into one using machine learn-ing/statistical NLP techniques.
Echihabi et al (Echi-habi et al, 2003) used Noisy-Channel Models toconstruct a QA system.
In this approach, the rangeof Term Extraction is not trained by a data set but se-lected from answer candidates, e.g., named entitiesand noun phrases, generated by a decoder.
Lita etal.
(Lita and Carbonell, 2004) share our motivationto build a QA system only from question-answerpairs without depending on the question types.
Theirmethod finds clusters of questions and defines howto answer questions in each cluster.
However, theirapproach is to find snippets, i.e., short passagesincluding answers, not exact answers extracted byTerm Extraction.7 ConclusionThis paper described a novel approach to extract-ing answers to a question using probabilistic mod-els constructed from only question-answer pairs.This approach requires no question type system, nonamed entity extractor, and no class name extractor.To the best of our knowledge, no previous study hasregarded Question Answering as Question-BiasedTerm Extraction.
As a feasibility study, we builta QA system using Maximum Entropy Models ona 2000-question/answer dataset.
The results wereevaluated by 10-fold cross validation, which showedthat the performance is 0.36 in MRR and 0.47 inTop5.
Since this approach relies on a morphologicalanalyzer, applying the QBTE Model 1 to QA tasksof other languages is our future work.AcknowledgmentThis research was supported by a contract with theNational Institute of Information and Communica-tions Technology (NICT) of Japan entitled, ?A studyof speech dialogue translation technology based ona large corpus?.ReferencesAdam L. Berger, Stephen A. Della Pietra, and Vincent J.Della Pietra: A Maximum Entropy Approach to Nat-ural Language Processing, Computational Linguistics,Vol.
22, No.
1, pp.
39?71 (1996).Abdessamad Echihabi and Daniel Marcu: A Noisy-Channel Approach to Question Answering, Proc.
ofACL-2003, pp.
16-23 (2003).Abraham Ittycheriah, Martin Franz, Wei-Jing Zhu, andAdwait Ratnaparkhi: Question Answering UsingMaximum-Entropy Components, Proc.
of NAACL-2001 (2001).Abraham Ittycheriah, Martin Franz, Wei-Jing Zhu, andAdwait Ratnaparkhi: IBM?s Statistical Question An-swering System ?
TREC-10, Proc.
of TREC-10(2001).Lucian Vlad Lita and Jaime Carbonell: Instance-BasedQuestion Answering: A Data-Driven Approach: Proc.of EMNLP-2004, pp.
396?403 (2004).Hwee T. Ng, Jennifer L. P. Kwan, and Yiyuan Xia: Ques-tion Answering Using a Large Text Database: A Ma-chine Learning Approach: Proc.
of EMNLP-2001, pp.67?73 (2001).Marisu A. Pasca and Sanda M. Harabagiu: High Perfor-mance Question/Answering, Proc.
of SIGIR-2001, pp.366?374 (2001).Lance A. Ramshaw and Mitchell P. Marcus: Text Chunk-ing using Transformation-Based Learning, Proc.
ofWVLC-95, pp.
82?94 (1995).Erik F. Tjong Kim Sang: Noun Phrase Recognition bySystem Combination, Proc.
of NAACL-2000, pp.
55?55 (2000).Yutaka Sasaki, Hideki Isozaki, Jun Suzuki, KoujiKokuryou, Tsutomu Hirao, Hideto Kazawa, andEisaku Maeda, SAIQA-II: A Trainable Japanese QASystem with SVM, IPSJ Journal, Vol.
45, NO.
2, pp.635-646, 2004.
(in Japanese)Satoshi Sekine, Kiyoshi Sudo, Yusuke Shinyama,Chikashi Nobata, Kiyotaka Uchimoto, and Hitoshi Isa-hara, NYU/CRL QA system, QAC question analysisand CRL QA data, in Working Notes of NTCIR Work-shop 3 (2002).Jun Suzuki, Yutaka Sasaki, and Eisaku Maeda: SVM An-swer Selection for Open-Domain Question Answer-ing, Proc.
of Coling-2002, pp.
974?980 (2002).Jun Suzuki, Hirotoshi Taira, Yutaka Sasaki, and EisakuMaeda: Directed Acyclic Graph Kernel, Proc.
of ACL2003 Workshop on Multilingual Summarization andQuestion Answering - Machine Learning and Beyond,pp.
61?68, Sapporo (2003).Ingrid Zukerman and Eric Horvitz: Using MachineLearning Techniques to Interpret WH-Questions,Proc.
of ACL-2001, Toulouse, France, pp.
547?554(2001).221Appendix: Analysis of Evaluation Results w.r.t.Question Type ?
Results of QBTE from the first-ranked paragraph (NB: No information about thesequestion types was used in the training phrase.
)Question Type #Qs MRR T5 MRR?
T5?GOE 36 0.30 0.36 0.41 0.53GPE 4 0.50 0.50 1.00 1.00N EVENT 7 0.76 0.86 0.76 0.86EVENT 19 0.17 0.21 0.41 0.53GROUP 74 0.28 0.35 0.45 0.62SPORTS TEAM 15 0.28 0.40 0.45 0.73BROADCAST 1 0.00 0.00 0.00 0.00POINT 2 0.00 0.00 0.00 0.00DRUG 2 0.00 0.00 0.00 0.00SPACESHIP 4 0.88 1.00 0.88 1.00ACTION 18 0.22 0.22 0.30 0.44MOVIE 6 0.50 0.50 0.56 0.67MUSIC 8 0.19 0.25 0.56 0.62WATER FORM 3 0.50 0.67 0.50 0.67CONFERENCE 17 0.14 0.24 0.46 0.65SEA 1 1.00 1.00 1.00 1.00PICTURE 1 0.00 0.00 0.00 0.00SCHOOL 21 0.10 0.10 0.33 0.43ACADEMIC 5 0.20 0.20 0.37 0.60PERCENT 47 0.35 0.43 0.43 0.55COMPANY 77 0.45 0.55 0.57 0.70PERIODX 1 1.00 1.00 1.00 1.00RULE 35 0.30 0.43 0.49 0.69MONUMENT 2 0.00 0.00 0.25 0.50SPORTS 9 0.17 0.22 0.40 0.67INSTITUTE 26 0.38 0.46 0.53 0.69MONEY 110 0.33 0.40 0.48 0.63AIRPORT 4 0.38 0.50 0.44 0.75MILITARY 4 0.00 0.00 0.25 0.25ART 4 0.25 0.50 0.25 0.50MONTH PERIOD 6 0.06 0.17 0.06 0.17LANGUAGE 3 1.00 1.00 1.00 1.00COUNTX 10 0.33 0.40 0.38 0.60AMUSEMENT 2 0.00 0.00 0.00 0.00PARK 1 0.00 0.00 0.00 0.00SHOW 3 0.78 1.00 1.11 1.33PUBLIC INST 19 0.18 0.26 0.34 0.53PORT 3 0.17 0.33 0.33 0.67N COUNTRY 8 0.28 0.38 0.32 0.50NATIONALITY 4 0.50 0.50 1.00 1.00COUNTRY 84 0.45 0.60 0.51 0.67OFFENSE 9 0.23 0.44 0.23 0.44CITY 72 0.41 0.50 0.53 0.65N FACILITY 4 0.25 0.25 0.38 0.50FACILITY 11 0.20 0.36 0.25 0.55TIMEX 3 0.00 0.00 0.00 0.00TIME TOP 2 0.00 0.00 0.50 0.50TIME PERIOD 8 0.12 0.12 0.48 0.75TIME 13 0.22 0.31 0.29 0.38ERA 3 0.00 0.00 0.33 0.33PHENOMENA 5 0.50 0.60 0.60 0.80DISASTER 4 0.50 0.75 0.50 0.75OBJECT 5 0.47 0.60 0.47 0.60CAR 1 1.00 1.00 1.00 1.00RELIGION 5 0.30 0.40 0.30 0.40WEEK PERIOD 4 0.05 0.25 0.55 0.75WEIGHT 12 0.21 0.25 0.31 0.42PRINTING 6 0.17 0.17 0.38 0.50Question Type #Q MRR T5 MRR?
T5?RANK 7 0.18 0.29 0.54 0.71BOOK 6 0.31 0.50 0.47 0.67AWARD 9 0.17 0.33 0.34 0.56N LOCATION 2 0.10 0.50 0.10 0.50VEGETABLE 10 0.31 0.50 0.34 0.60COLOR 5 0.20 0.20 0.20 0.20NEWSPAPER 7 0.61 0.71 0.61 0.71WORSHIP 8 0.47 0.62 0.62 0.88SEISMIC 1 0.00 0.00 1.00 1.00N PERSON 72 0.30 0.39 0.43 0.60PERSON 282 0.18 0.21 0.46 0.55NUMEX 19 0.32 0.32 0.35 0.47MEASUREMENT 1 0.00 0.00 0.00 0.00P ORGANIZATION 3 0.33 0.33 0.67 0.67P PARTY 37 0.30 0.41 0.43 0.57GOVERNMENT 37 0.50 0.54 0.53 0.57N PRODUCT 41 0.25 0.37 0.37 0.56PRODUCT 58 0.24 0.34 0.44 0.69WAR 2 0.75 1.00 0.75 1.00SHIP 7 0.26 0.43 0.40 0.57N ORGANIZATION 20 0.14 0.25 0.28 0.55ORGANIZATION 23 0.08 0.13 0.20 0.30SPEED 1 0.00 0.00 1.00 1.00VOLUME 5 0.00 0.00 0.18 0.60GAMES 8 0.28 0.38 0.34 0.50POSITION TITLE 39 0.20 0.28 0.30 0.44REGION 22 0.17 0.23 0.46 0.64GEOLOGICAL 3 0.42 0.67 0.42 0.67LOCATION 2 0.00 0.00 0.50 0.50EXTENT 22 0.04 0.09 0.13 0.18CURRENCY 1 0.00 0.00 0.00 0.00STATION 3 0.50 0.67 0.50 0.67RAILROAD 1 0.00 0.00 0.25 1.00PHONE 1 0.00 0.00 0.00 0.00PROVINCE 36 0.30 0.33 0.45 0.50N ANIMAL 3 0.11 0.33 0.22 0.67ANIMAL 10 0.26 0.50 0.31 0.60ROAD 1 0.00 0.00 0.50 1.00DATE PERIOD 9 0.11 0.11 0.33 0.33DATE 130 0.24 0.32 0.41 0.58YEAR PERIOD 34 0.22 0.29 0.38 0.59AGE 22 0.34 0.45 0.44 0.59MULTIPLICATION 9 0.39 0.44 0.56 0.67CRIME 4 0.75 0.75 0.75 0.75AIRCRAFT 2 0.00 0.00 0.25 0.50MUSEUM 3 0.33 0.33 0.33 0.33DISEASE 18 0.29 0.50 0.43 0.72FREQUENCY 13 0.18 0.31 0.19 0.38WEAPON 1 1.00 1.00 1.00 1.00MINERAL 18 0.16 0.22 0.25 0.39METHOD 29 0.39 0.48 0.48 0.62ETHNIC 3 0.42 0.67 0.75 1.00NAME 5 0.20 0.20 0.40 0.40SPACE 4 0.50 0.50 0.50 0.50THEORY 1 0.00 0.00 0.00 0.00LANDFORM 5 0.13 0.40 0.13 0.40TRAIN 2 0.17 0.50 0.17 0.502000 0.28 0.36 0.43 0.58222
