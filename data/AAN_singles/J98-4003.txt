Machine TransliterationKevin Knight*University of Southern CaliforniaJonathan Graehl*University of Southern CaliforniaIt is challenging to translate names and technical terms across languages with different alphabetsand sound inventories.
These items are commonly transliterated, i.e., replaced with approxi-mate phonetic equivalents.
For example, "computer" in English comes out as "konpyuutaa" inJapanese.
Translating such items from Japanese back to English is even more challenging, andof practical interest, as transliterated items make up the bulk of text phrases not found in bilin-gual dictionaries.
We describe and evaluate amethod for performing backwards transliterationsby machine.
This method uses a generative model, incorporating several distinct stages in thetransliteration process.1.
IntroductionOne of the most frequent problems translators must deal with is translating propernames and technical terms.
For language pairs like Spanish/English, this presents nogreat challenge: a phrase like Antonio Gil usually gets translated as Antonio Gil.
How-ever, the situation is more complicated for language pairs that employ very differentalphabets and sound systems, such as Japanese/English and Arabic/English.
Phonetictranslation across these pairs is called transliteration.
We will look at Japanese/Englishtransliteration i this article.Japanese frequently imports vocabulary from other languages, primarily (but notexclusively) from English.
It has a special phonetic alphabet called katakana, which isused primarily (but not exclusively) to write down foreign names and loanwords.
Thekatakana symbols are shown in Figure 1, with their Japanese pronunciations.
The twosymbols hown in the lower right corner ( --, 7 ) are used to lengthen any Japanesevowel or consonant.To write a word like golfbag in katakana, some compromises must be made.
Forexample, Japanese has no distinct L and R sounds: the two English sounds collapseonto the same Japanese sound.
A similar compromise must be struck for English Hand F. Also, Japanese generally uses an alternating consonant-vowel structure, makingit impossible to pronounce LFB without intervening vowels.
Katakana writing is asyllabary rather than an alphabet--there is one symbol for ga (~"), another for gi( 4 ~" ), another for gu ( Y" ), etc.
So the way to write golfbag in katakana is ~',,t, 7~,< 7 Y',roughly pronounced go-ru-hu-ba-ggu.
Here are a few more examples:* USC/Inforrnation Sciences Institute, Marina del Rey, CA 90292 and USC/Computer ScienceDepartment, Los Angeles, CA 90089t USC/Computer Science Department, Los Angeles, CA 90089(~) 1998 Association for Computational LinguisticsComputational Linguistics Volume 24, Number 4T (a) ~ (ka) ~-(sa) ~ (ta) ~(na)  ?"
(ha) ~(ma)  ~ (ra)(i) ~ (k?)
~ (shi) Y-(ch?)
~ (ni) a (hi) ~ (mi) ~ (ri)(u) ~ (ku) X (su) 7 (tsu) % (nu) 7 (hu) ~ (mu) 2~ (ru):n(e) ~(ke)  ~ (se) ~ (te) ~ (he) ~-(he) fl (me) , ~ (re)M- (o) = (ko) Y (so) b (to) \] (no) ?
(ho) ~ (mo) ~ (ro)-~ (ba) 2"(ga) -< (pa) -Y(za) ~(da) T (a) -V (ya) ~ (ya)(bi) @'(gi) ff (pi) ~ (ji) Y(de)  4 (i) ~ (yo) ~ (yo)Y (bu) ~(gu)  ~ (pu) X'(zu) F (do) ~ (u) :~(yu) ~ (yu)-<(be) ~(ge)  ~ (pe) ~'(ze) ~ (n) ~ (e) ~ (v)(bo) ~(go)  ~:(po) / (zo) ~'(chi) ~ (o) V (wa) --Figure 1Katakana symbols and their Japanese pronunciations.Angela Johnson(a n j ira jyo n son)New York Times(nyu uyo oku ta imuzu)ice cream(a i sukur f imu)Omaha Beach(omahabiit chi)pro soccer(purosakkaa)Tonya Harding(toonya haadingu)ramp lamp casual fashion team leader~yT"  ?y~ ~J=T J~7~y ~- -~- -~ ' - -(ranpu) (ranpu) (kaj yuaruhas shyon) (chifmuriidaa)Notice how the transliteration is more phonetic than orthographic; the letter h inJohnson does not produce any katakana.
Also, a dot-separator (,) is used to sepa-rate words, but not consistently.
And transliteration is clearly an information-losingoperation: ranpu could come from either lamp or ramp, while aisukuriimu loses thedistinction between ice cream and I scream.Transliteration is not trivial to automate, but we will be concerned with an evenmore challenging problem--going from katakana back to English, i.e., back-translit-eration.
Human translators can often "sound out" a katakana phrase to guess anappropriate translation.
Automating this process has great practical importance inJapanese/English machine translation.
Katakana phrases are the largest source of textphrases that do not appear in bilingual dictionaries or training corpora (a.k.a.
"not-found words"), but very little computational work has been done in this area.
Yamronet al (1994) briefly mention a pattern-matching approach, while Arbabi et al (1994)discuss a hybrid neural-net/expert-system approach to (forward) transliteration.The information-losing aspect of transliteration makes it hard to invert.
Here aresome problem instances, taken from actual newspaper articles:?
?
?
(aasudee) (robaato shyoon renaado) (masutaazutoonamento)600Knight and Graehl Machine TransliterationEnglish translations appear later in this article.Here are a few observations about back-transliteration that give an idea of thedifficulty of the task:?
Back-transliteration s less forgiving than transliteration.
There are manyways to write an English word like switch in katakana, all equally valid,but we do not have this flexibility in the reverse direction.
For example,we cannot drop the t in switch, nor can we write arture when we meanarcher.
Forward-direction flexibility wreaks havoc with dictionary-basedsolutions, because no dictionary will contain all katakana variants.?
Back-transliteration s harder than romanizat ion.
A romanization schemesimply sets down a method for writing a foreign script in roman letters.For example, to romanize T y "Y ~,  we look up each symbol in Figure 1and substitute characters.
This substitution gives us (romanized) anj ira,but not (translated) angela.
Romanization schemes are usuallydeterministic and invertible, although small ambiguities can arise.
Wediscuss ome wrinkles in Section 3.4.?
Finally, not all katakana phrases can be "sounded out" bv back-transliteration.
Some phrases are shorthand, e.g., V -  7 ?
~ (waapuro)should be translated as word processing.
Others are onomatopoetic anddifficult o translate.
These cases must be solved by techniques otherthan those described here.The most desirable feature of an automatic back-transliterator is accuracy.
If pos-sible, our techniques should also be:?
portable to new language pairs like Arabic/English with minimal effort,possibly reusing resources.?
robust against errors introduced by optical character recognition.?
relevant to speech recognition situations in which the speaker has aheavy foreign accent.?
able to take textual (topical/syntactic) context into account, or at least beable to return a ranked list of possible English translations.Like most problems in computational linguistics, this one requires full worldknowledge for a 100% solution.
Choosing between Katarina and Catalina (both goodguesses for ~ ~ ~J ~- ) might even require detailed knowledge of geography and figureskating.
At that level, human translators find the problem quite difficult as well, sowe only aim to match or possibly exceed their performance.2.
A Modular Learning ApproachBilingual glossaries contain many entries mapping katakana phrases onto Englishphrases, e.g., (aircraft carrier ~ sT  ~ ~ ~ 1- ~ -~- ~J T ).
It is possible to automaticallyanalyze such pairs to gain enough knowledge to accurately map new katakana phrasesthat come along, and this learning approach travels well to other language pairs.
Anaive approach to finding direct correspondences between English letters and katakana601Computational Linguistics Volume 24, Number 4symbols, however, suffers from a number of problems.
One can easily wind up witha system that proposes iskrym as a back-transliteration f aisukuriimu.
Taking letterfrequencies into account improves this to a more plausible-looking isclim.
Moving toreal words may give is crime: the i corresponds to ai, the s corresponds to su, etc.Unfortunately, the correct answer here is ice cream.After initial experiments along these lines, we stepped back and built a generativemodel of the transliteration process, which goes like this:.2.3.4.5.An English phrase is written.A translator pronounces it in English.The pronunciation is modified to fit the Japanese sound inventory.The sounds are converted into katakana.Katakana is written.This divides our problem into five subproblems.
Fortunately, there are techniquesfor coordinating solutions to such subproblems, and for using generative models in thereverse direction.
These techniques rely on probabilities and Bayes' theorem.
Supposewe build an English phrase generator that produces word sequences according tosome probability distribution P(w).
And suppose we build an English pronouncer thattakes a word sequence and assigns it a set of pronunciations, again probabilistically,according to some P(plw).
Given a pronunciation p, we may want to search for theword sequence w that maximizes P(wlp ).
Bayes' theorem lets us equivalently maximizeP(w) ?
P(plw), exactly the two distributions we have modeled.Extending this notion, we settled down to build five probability distributions:.2.3.4.5.P(w) - -  generates written English word sequences.P(elw) - -  pronounces English word sequences.P(jle) - -  converts English sounds into Japanese sounds.P(klj ) - -  converts Japanese sounds to katakana writing.P(o\]k) - -  introduces misspellings caused by optical character recognition(OCR).Given a katakana string o observed by OCR, we want to find the English wordsequence w that maximizes the sum, over all e, j, and k, ofP(w).
P(elw).
P(jle).
P(klj) ?
P(olk)Following Pereira and Riley (1997), we implement P(w) in a weighted finite-state ac-ceptor (WFSA) and we implement the other distributions in weighted finite-state trans-ducers (WFSTs).
A WFSA is a state/transition diagram with weights and symbols onthe transitions, making some output sequences more likely than others.
A WFST is aWFSA with a pair of symbols on each transition, one input and one output.
Inputsand outputs may include the empty symbol ?.
Also following Pereira and Riley (1997),we have implemented a general composition algorithm for constructing an integratedmodel P(xlz) from models P(xly ) and P(y\[z), treating WFSAs as WFSTs with identicalinputs and outputs.
We use this to combine an observed katakana string with each602Knight and Graehl Machine Transliterationof the models in turn.
The result is a large WFSA containing all possible Englishtranslations.We have implemented two algorithms for extracting the best translations.
The firstis Dijkstra's shortest-path graph algorithm (Dijkstra 1959).
The second is a recentlydiscovered k-shortest-paths algorithm (Eppstein 1994) that makes it possible for us toidentify the top k translations in efficient O(m + n log n + kn) time, where the WFSAcontains n states and m arcs.The approach is modular.
We can test each engine independently and be confidentthat their results are combined correctly.
We do no pruning, so the final WFSA containsevery solution, however unlikely.
The only approximation is the Viterbi one, whichsearches for the best path through a WFSA instead of the best sequence (i.e., the samesequence does not receive bonus points for appearing more than once).3.
Probabilistic ModelsThis section describes how we designed and built each of our five models.
For consis-tency, we continue to print written English word sequences in italics (golf ball), Englishsound sequences in all capitals (G AA L F B A0 L), Japanese sound sequences in lowercase (g o r u h u b o o r u)and katakana sequences naturally (~,,~7,~--)t,).3.1 Word SequencesThe first model generates scored word sequences, the idea being that ice cream shouldscore higher than ice creme, which should score higher than aice kreem.
We adopted asimple unigram scoring method that multiplies the scores of the known words andphrases in a sequence.
Our 262,000-entry frequency list draws its Words and phrasesfrom the Wall Street Journal corpus, an on-line English name list, and an on-linegazetteer of place names, l A portion of the WFSA looks like this:los / 0.000087federal / 0.001~ angeleP Dmonth / 0.000992An ideal word sequence model would look a bit different.
It would prefer exactlythose strings which are actually grist for Japanese transliterators.
For example, peoplerarely transliterate auxiliary verbs, but surnames are often transliterated.
We haveapproximated such a model by removing high-frequency words like has, an, are, am,were, their, and does, plus unlikely words corresponding to Japanese sound bites, likecoup and oh.We also built a separate word sequence model containing only English first andlast names.
If we know (from context) that the transliterated phrase is a personal name,this model is more precise.3.2 Words to English SoundsThe next WFST converts English word sequences into English sound sequences.
Weuse the English phoneme inventory from the on-line CMU Pronunciation Dictio-1 Available from the ACL Data Collection I itiative.603Computational Linguistics Volume 24, Number 4nary, minus the stress marks.
2 This gives a total of 40 sounds, including 14 vowelsounds (e.g., AA, AE, UN), 25 consonant sounds (e.g., K, HH, R), plus one special symbol(PAUSE).
The dictionary has pronunciations for 110,000 words, and we organized atree-based WFST from it:E:Eff:zNote that we insert an optional PAUSE between word pronunciations.We originally thought o build a general etter-to-sound WFST (Divay and Vitale1997), on the theory that while wrong (overgeneralized) pronunciations might occa-sionally be generated, Japanese transliterators also mispronounce words.
However,our letter-to-sound WFST did not match the performance of Japanese transliterators,and it turns out that mispronunciations are modeled adequately in the next stage ofthe cascade.3.3 English Sounds to Japanese SoundsNext, we map English sound sequences onto Japanese sound sequences.
This is an in-herently information-losing process, as English R and L sounds collapse onto Japaneser, the 14 English vowel sounds collapse onto the 5 Japanese vowel sounds, etc.
Weface two immediate problems:1.
What is the target Japanese sound inventory?2.
How can we build a WFST to perform the sequence mapping?An obvious target inventory is the Japanese syllabary itself, written down inkatakana (e.g., = ) or a roman equivalent (e.g., ni).
With this approach, the Englishsound K corresponds to one of ~ (ka), ~ (ki), ~ (ku), ~r (ke), or = (ko), depend-ing on its context.
Unfortunately, because katakana is a syllabary, we would be un-able to express an obvious and useful generalization, amely that English K usuallycorresponds to Japanese k, independent of context.
Moreover, the correspondence ofJapanese katakana writing to Japanese sound sequences i  not perfectly one-to-one (seeSection 3.4), so an independent sound inventory is well-motivated in any case.
OurJapanese sound inventory includes 39 symbols: 5 vowel sounds, 33 consonant sounds(including doubled consonants like kk), and one special symbol (pause).
An Englishsound sequence like (P R 0W PAUSE S AA K ER) might map onto a Japanese soundsequence like (p u r o pause s a kk a a).
Note that long Japanese vowel sounds2 The CMU Pronunciation Dictionary can be found on-line athttp ://www.
speech,  cs.
cmu.
edu/cgi-bin/cmudict.604Knight and Graehl Machine Transliterationare written with two symbols (a a) instead of just one (aa).
This scheme is attractivebecause Japanese sequences are almost always longer than English sequences.Our WFST is learned automatically from 8,000 pairs of English/Japanese soundsequences, e.g., ((S AA K ER) ~ (s a kk a a) ).
We were able to produce these pairsby manipulating a small English-katakana glossary.
For each glossary entry, we con-verted English words into English sounds using the model described in the previoussection, and we converted katakana words into Japanese sounds using the model wedescribe in the next section.
We then applied the estimation-maximization (EM) al-gorithm (Baum 1972; Dempster, Laird, and Rubin 1977) to generate symbol-mappingprobabilities, hown in Figure 2.
Our EM training goes like this:1.
For each English/Japanese sequence pair, compute all possiblealignments between their elements.
In our case, an alignment is adrawing that connects each English sound with one or more Japanese "sounds, such that all Japanese sounds are covered and no lines cross.
Forexample, there are two ways to align the pair ((L 0W) <-> (r o o)):L OW L OW/ / k  \r o o r o oIn this case, the alignment on the left is intuitively preferable.
Thealgorithm learns such preferences.2.
For each pair, assign an equal weight to each of its alignments, such thatthose weights sum to 1.
In the case above, each alignment gets a weightof 0.5.3.
For each of the 40 English sounds, count up instances of its differentmappings, as observed in all alignments of all pairs.
Each alignmentcontributes counts in proportion to its own weight.4.
For each of the 40 English sounds, normalize the scores of the Japanesesequences it maps to, so that the scores sum to 1.
These are thesymbol-mapping probabilities hown in Figure 2.5.
Recompute the alignment scores.
Each alignment is scored with theproduct of the scores of the symbol mappings it contains.
Figure 3 showssample alignments found automatically through EM training.6.
Normalize the alignment scores.
Scores for each pair's alignments houldsum to 1.7.
Repeat 3--6 until the symbol-mapping probabilities converge.We then build a WFST directly from the symbol-mapping probabilities:PAUSE:pausef~  I~ .
:o  / 0.018 AA:a / 0.024C?
e :a ~/~ e:Ov AA:a  / 0 .382Our WFST has 99 states and 283 arcs.605Computational Linguistics Volume 24, Number 4e j P(j\]e)AA o 0 .566a 0 .382a a 0 .024o o 0 .018AE a 0.942y a 0.046AH a 0.486o 0.169e 0 .134i 0.111u 0 .076A0 o 0 .671o o 0 .257a 0 .047AN a u 0 .830a w "0.095o o 0.027a o 0 .020a 0 .014AY a i 0.864i 0.073a 0 .018a i y 0 .018B '  b 0 .802b u 0 .185CH ch y 0.277ch  0.240tab  i 0.199ch  i 0 .159tch  0 .038ch  y u 0 .021tch  y 0 .020DHEHERd 0 .535d o 0 .329dd  o 0 .053j 0.032z 0 .670z u 0 .125j 0.125a z 0 .0800 .9010 .069a a 0 .719a 0 .081a r 0 .063e r 0 .042o r 0 .029e j P ( j Ie )EY e e 0 .641a 0 .122e 0 .114e i 0 .080a i 0 .014F h 0 .623h u 0 .331hh  0 .019a h u 0 .010G g 0 .598g u 0 ,304gg  u 0.059gg 0 .010HH h 0 .959w 0.014IH i 0.908e 0 .071IY i i 0.573i 0.317e 0 .074e e 0 .016JH j 0 .329j y 0 .328j i 0 .129jj i 0.066e j i 0 .057z 0 .032g 0 .018j j  0 .012e 0 .012K k 0 .528k u 0 .238kk  u 0 .150kk  0 .043k i 0 .015k y 0 .012\[L r 0 .621I r u 0 .362M m 0.653 im u 0 .207n 0 .123n m 0.011N n 0 .978NG n g u 0 .743n 0 .220n g 0 .023e j P ( j le )OW o 0 .516o o 0 .456o u 0.0110Y o i 0.828o o i 0.057i 0.029o i y 0.029o 0.027o o y 0.014o o 0.014P p 0 .649p u 0 .218pp  u 0 .085pp  0 .045PAUSE pause  1 .000R r 0 .661a 0 .170o 0 .076r u 0 .042u r 0 .016a r 0 .012S s u 0 .539s 0 .269sh  0 .109u 0 .028ss  0 .014SH shy  0 .475sh  0 .175ssh  y u 0 .166ssh  y 0 .088sh  i 0 .029ssh  0 .027shy  u 0 .015T t 0 .463t o 0 .305t t  o 0 .103ch  0 .043t t  0 .021ts  0 .020ts  u 0 .011TH s u 0 .418s 0 .303sh  0 .130ch  0 .038t 0 .029e j PUle)i UH u 0 .794u u 0 .098dd  0 .034a 0 .030o 0 .026UW u u 0 .550u 0 .302y u u 0 .109y u 0.021V b 0 .810b u 0 .150w 0 .015W w 0 .693u 0 .194o 0 .039i 0 .027a 0 .015e 0 .012Y y 0 .652i 0 .220y u 0 .050u 0 .048b 0 .016Z z 0 .296z u 0 .283j 0 .107s u 0 .103u 0 .073a 0 .036o 0 .018s 0 .015n 0 .013i 0 .011sh  0 .011ZH j y 0 .324sh  ?
0 .270j i 0 .173j 0 .135a j y u 0 .027shy  0 .027s 0 .027a j ?
0 .016Figure 2Eng l i sh  sounds  ( in  cap i ta l s )  w i th  probab i l i s t i c  mapp ings  to  Japanese  sound sequences  ( inlower  case) ,  as  learned  by  es t imat ion-max imizat ion .
On ly  mapp ings  w i th  cond i t iona lp robab i l i t ies  greater  than  1% are  shown,  so  the  f igures  may not  sum to  1.We have also built models that allow individual English sounds to be "swallowed"(i.e., produce zero Japanese sounds).
However, these models are expensive to compute(many more alignments) and lead to a vast number of hypotheses during WFST com-position.
Furthermore, in disallowing "swallowing," we were able to automaticallyremove hundreds of potentially harmful pairs from our training set, e.g., ((B AA RB ER SH AA P) ~ (b a a b a a)) .
Because no alignments are possible, such pairsare skipped by the learning algorithm; cases like these must be solved by dictionary606Knight and Graehl Machine Transliterationb&cuitEng l i sh  sound sequence:Japanese  sound sequence:~viderEng l i sh  sound sequence:Japanesef i l~rEng l i shJapaneseFigure 3B IH S K AH TB I S U K E TT OD IH V AY DI / / \sound sequence  : D I B A I DERsound sequence  : F IH L T ERsound sequence  : H I R U T A Ap-....A AAlignments between English and Japanese sound sequences, asdetermined by EM training.Best alignments are shown for the English words biscuit, divider, and filter.lookup anyway.
Only two pairs failed to align when we wished they had--both in-volved turning English Y UW into Japanese u, as in ((Y uw K AH L EY L IY) ~ (u kur  e r e)).Note also that our model  translates each English sound without regard to context.We have also built context-based models, using decision trees recoded as WFSTs.
Forexample, at the end of a word, English T is likely to come out as (t o) rather than (t).However, context-based models proved unnecessary for back-transliteration.
They aremore useful for English-to-Japanese forward transliteration.3.4 Japanese Sounds to KatakanaTo map Japanese sound sequences like (m o o t a a) onto katakana sequences like(~--  ~-  ), we manually constructed two WFSTs.
Composed together, they yield anintegrated WFST with 53 states and 303 arcs, producing a katakana inventory contain-ing 81 symbols, including the dot-separator (-).
The first WFST simply merges longJapanese vowel sounds into new symbols aa, i i ,  uu, ee, and oo.
The second WFSTmaps Japanese sounds onto katakana symbols.
The basic idea is to consume a wholesyllable worth of sounds before producing any katakana.
For example:o:pause: ?
/ 0.
7607Computational Linguistics Volume 24, Number 4This fragment shows one kind of spelling variation in Japanese: long vowel sounds(oo) are usually written with a long vowel mark (~- )  but are sometimes writtenwith repeated katakana ( ~-  ).
We combined corpus analysis with guidelines from aJapanese textbook (Jorden and Chaplin 1976) to turn up many spelling variations andunusual katakana symbols:?
the sound sequence (j i) is usually written "Y, but occasionally Y .?
(g u a) is usually P'T , but occasionally p" 7.?
(w o o) is variously ~ ~---,  9 ~- - ,  or with a special old-style katakanafor wo.?
(y e) maybe an, 4an ,or  4?
(w ?)
is either 9 4 or ~ 4?
(n y e) is a rare sound sequence, but is written = ~ when it occurs.?
(t y u) is rarer than (ch y u), but is written ~ =- when it occurs.and so on.Spelling variation is clearest in cases where an English word like switch showsup transliterated variously ( :z 4 7 ~-, :z 4 ~, ~ ,  :z ~ 4 ~ ~ ) in different dictionaries.Treating these variations as an equivalence class enables us to learn general soundmappings even if our bilingual glossary adheres to a single narrow spelling conven-tion.
We do not, however, generate all katakana sequences with this model; for exam-ple, we do not output strings that begin with a subscripted vowel katakana.
So thismodel also serves to ter out some ill-formed katakana sequences, possibly proposedby optical character recognition.3.5 Katakana to OCRPerhaps uncharitably, we can view optical character recognition (OCR) as a device thatgarbles perfectly good katakana sequences.
Typical confusions made by our commer-cial OCR system include '~ for ~-', ~ for ~ ,  T for 7 ,  and 7 for 7".
To generatepre-OCR text, we collected 19,500 characters worth of katakana words, stored them ina e, and printed them out.
To generate post-OCR text, we OCR'd the printouts.
Wethen ran the EM algorithm to determine symbol-mapping ("garbling") probabilities.Here is part of that table:k o P(olk)~" 0.492t:" 0.4340.0427 0.011~" ~ 1.000z* ~,- 0.964/ ?
0.036608Knight and Graehl Machine TransliterationThis model outputs a superset of the 81 katakana symbols, including spuriousquote marks, alphabetic symbols, and the numeral 7.
34.
A Sample  Back-transl i terat ionWe can now use the models to do a sample back-transliteration.
We start with akatakana phrase as observed by OCR.
We then serially compose it with the models, inreverse order.
Each intermediate stage is a WFSA that encodes many possibilities.
Thefinal stage contains all back-transliterations suggested by the models, and we finallyextract he best one.We start with the masutaazutoonamento pr blem from Section 1.
Our OCR ob-serves:vx~-x 'F -T}  w FThis string has two recognition errors: ~ (ku) for 9 (ta), and ~ (chi) for 9- (na).We turn the string into a chained 12-state/11-arc WFSA and compose it with the P(klo )model.
This yields a fatter 12-state/15-arc WFSA, which accepts the correct spellingat a lower probability.
Next comes the P(jlk) model, which produces a 28-state/31-arcWFSA whose highest-scoring sequence is:m a s u t a a z u t  ooch iment  oNext comes P(elj ), yielding a 62-state/241-arc WFSA whose best sequence is:M AE S T AE AE DH UH T A0 A0 CH IH M EH N T A0Next to last comes P(wle ), which results in a 2982-state/4601-arc WFSA whose bestsequence (out of roughly three hundred million) is:masters tone am ent aweThis English string is closest phonetically to the Japanese, but we are willing to tradephonetic proximity for more sensical English; we rescore this WFSA by composing itwith P(w) and extract he best translation:masters tournamentOther Section I examples (aasudee and robaato  shyoon renaado) are translated cor-rectly as earth day and robert sean leonard.We may also be interested in the k best translations.
In fact, after any composition,we can inspect several high-scoring sequences using the algorithm of Eppstein (1994).Given the following katakana input phrase:3 A more thorough OCR model would train on a wide variety of fonts and photocopy distortions.
Inpractice, such degradations can easily overwhelm even the better OCR systems.609Computational Linguistics Volume 24, Number 4(pronounced anj i rahoorasuteruna i to ) ,  the top five English sound sequences areAE N JH IH K AE HH A0 A0 K AE S T EH g UH N AE IH T A0AE N JH IH R AE HH A0 A0 K AE S T EH K UH N AY T A0AE N JH IH L AE HH A0 A0 K AE S T EH R UH N AE IH T A0AE N JH IH R AE HH A0 A0 R AE S T EH L UH N AE IH T A0AE N JH IH i~ AE HH A0 A0 L AE S T EH R UH N AE IH T A0Notice that different K and L combinations are visible in this list.
The top five finaltranslations are:angela forrestal knightangela forrester knightangela forest el knightangela forester knightangela forest air knightP(w) * P(klw)3.6e-208.5e-212.7e-212.5e-211.7e-21P(k le)0.007530.007420.007350.007350.00735Inspecting the k-best list is useful for diagnosing problems with the models.
If theright answer appears low in the list, then some numbers are probably off somewhere.If the right answer does not appear at all, then one of the models may be missing aword or suffer from some kind of brittleness.
A k-best list can also be used as inputto a later context-based disambiguator, or as an aid to a human translator.5.
ExperimentsWe have performed two large-scale xperiments, one using a full-language P(w) model,and one using a personal name language model.In the first experiment, we extracted 1,449 unique katakana phrases from a corpusof 100 short news articles.
Of these, 222 were missing from an on-line 100,000-entrybilingual dictionary.
We back-transliterated these 222 phrases.
Many of the translationsare perfect: technical program, sex scandal, omaha beach, new york times, ramon diaz.
Othersare close: tanya harding, nickel simpson, danger washington, world cap.
Some miss themark: nancy care again, plus occur, patriot miss real.
4 While it is difficult to judge overallaccuracy--some of the phrases are onomatopoetic, and others are simply too hard evenfor good human translators--it s easier to identify system weaknesses, and most ofthese lie in the P(w) model.
For example, nancy kerrigan should be preferred over nancycare again.In a second experiment, we took (non-OCR) katakana versions of the names of 100U.S.
politicians, e.g.
: -Y~ y ?
7"~-  ( jyon.buroo),  T~I , ,~yx  ?
:9"-v.;, }- (aruhonsu.damatto), and -v 4 ~ ?
Y V 4 Y (maiku.
dewain).
We back-transliterated these by ma-chine and asked four human subjects to do the same.
These subjects were nativeEnglish speakers and news-aware; we gave them brief instructions.
The results wereas in Table 1.There is room for improvement on both sides.
Being English speakers, the humansubjects were good at English name spelling and U.S. politics, but not at Japanesephonetics.
A native Japanese speaker might be expert at the latter but not the former.People who are expert in all of these areas, however, are rare.4 Correct ranslations are tonya harding, nicole simpson, denzel washington, world cup, nancy kerrigan, prosoccer, and patriot missile.610Knight and Graehl Machine TransliterationTable 1Accuracy of back-transliteration byhuman subjects and machine.Human Machinecorrect 27% 64%(e.g., spencer abraham / spencer abraham)phonetically equivalent, but misspelled 7% 12%(e.g., richard brian / richard bryan)incorrect 66% 24%(e.g., olin hatch / orren hatch)On the automatic side, many errors can be corrected.
A first-name/last-namemodel would rank richard bryan more highly than richard brian.
A bigram model wouldprefer orren hatch over olin hatch.
Other errors are due to unigram training problems, ormore rarely, incorrect or brittle phonetic models.
For example, Long occurs much moreoften than Ron in newspaper text, and our word selection does not exclude phraseslike Long Island.
So we get long wyden instead of ron wyden.
One way to fix these prob-lems is by manually changing unigram probabilities.
Reducing P(long) by a factor often solves the problem while maintaining a high score for P(long I rongu).Despite these problems, the machine's performance is impressive.
When wordseparators (o) are removed from the katakana phrases, rendering the task exceedinglydifficult for people, the machine's performance is unchanged.
In other words, it offersthe same top-scoring translations whether or not the separators are present; how-ever, their presence significantly cuts down on the number of alternatives considered,improving efficiency.
When we use OCR, 7% of katakana tokens are misrecognized,affecting 50% of test strings, but translation accuracy only drops from 64% to 52%.6.
D iscuss ionIn a 1947 memorandum, Weaver (1955) wrote:One naturally wonders if the problem of translation could conceivablybe treated as a problem of cryptography.
When I look at an article inRussian, I say: "'This is really written in English, but it has been codedin some strange symbols.
I will now proceed to decode."
(p. 18)Whether this is a useful perspective for machine translation is debatable (Brown etal.
1993; Knoblock 1996)--however, it is a dead-on description of transliteration.
Mostkatakana phrases really are English, ready to be decoded.We have presented a method for automatic back-transliteration which, while farfrom perfect, is highly competitive.
It also achieves the objectives outlined in Section 1.It ports easily to new language pairs; the P(w) and P(elw ) models are entirely reusable,while other models are learned automatically.
It is robust against OCR noise, in a rareexample of high-level language processing being useful (necessary, even) in improvinglow-level OCR.There are several directions for improving accuracy.
The biggest problem is thatraw English frequency counts are not the best indication of whether a word is a possi-ble source for transliteration.
Alternative data collection methods must be considered.611Computational Linguistics Volume 24, Number 4We may also consider changes to the model sequence itself.
As we have pre-sented it, our hypothetical human transliterator produces Japanese sounds from En-glish sounds only, without regard for the original English spelling.
This means thatEnglish homonyms will produce exactly the same katakana strings.
In reality, though,transliterators will sometimes key off spelling, so that tonya and tanya produce toonyaand taanya.
It might pay to carry along some spelling information in the Englishpronunciation lattices.Sentential context should be useful for determining correct ranslations.
It is oftenclear from a Japanese sentence whether a katakana phrase is a person, an institution,or a place.
In many cases it is possible to narrow things further--given the phrase"such-and-such, Arizona," we can restrict our P(w) model to include only those citiesand towns in Arizona.It is also interesting to consider transliteration for other languages.
In Arabic, forexample, it is more difficult to identify candidates for transliteration because there isno distinct, explicit alphabet hat marks them.
Furthermore, Arabic is usually writtenwithout vowels, so we must generate vowel sounds from scratch in order to producecorrect English.Finally, it may be possible to embed phonetic-shift models inside speech recogniz-ers, to explicitly adjust for heavy foreign accents.AcknowledgmentsWe would like to thank Alton Earl Ingrain,Yolanda Gil, Bonnie Glover Stalls, RichardWhitney, Kenji Yamada, and the anonymousreviewers for their helpful comments.
Wewould also like to thank our sponsors at theDepartment of Defense.ReferencesArbabi, Mansur, Scott M. Fischthal,Vincent C. Cheng, and Elizabeth Bart.1994.
Algorithms for Arabic nametransliteration.
IBM Journal of Research andDevelopment, 38(2):183-193.Baum, Leonard E. 1972.
An inequality andassociated maximization technique instatistical estimation of probabilisticfunctions of a Markov process.Inequalities, 3:1-8.Brown, Peter E, Stephen A. Della Pietra,Vincent J. Della Pietra, and Robert L.Mercer.
1993.
The mathematics ofstatistical machine translation: Parameterestimation.
Computational Linguistics,19(2):263-311.Dempster, A. P., N. M. Laird, and D. B.Rubin.
1977.
Maximum likelihood fromincomplete via the EM algorithm.
Journalof the Royal Statistical Society, 39(B):1-38.Dijkstra, Edsgar W. 1959.
A note on twoproblems in connexion with graphs.Numerische Mathematik, 1:269-271.Divay, Michel and Anthony J. Vitale.
1997.Algorithms for grapheme-phonemetranslation for English andFrench:Applications.
ComputationalLinguistics, 23(4):495-524.Eppstein, David.
1994.
Finding the kshortest paths.
In Proceedings ofthe35thSymposium on the Foundations ofComputer Science, pages 154-165.Jorden, Eleanor H. and Hamako I. Chaplin.1976.
Reading Japanese.
Yale UniversityPress, New Haven.Knoblock, Craig.
1996.
Trends andcontroversies: Statistical versusknowledge-based machine translation.IEEE Expert, 11(2):12-18.Pereira, Fernando C. N. and Michael Riley.1997.
Speech recognition by compositionof weighted finite automata.
In E. Rocheand Y. Schabes, editors, Finite-StateLanguage Processing, pages 431-453.
MITPress.Weaver, Warren.
1955.
Translation.
InWilliam N. Locke and A. Donald Booth,editors, Machine Translation of Languages.Technology Press of MIT and John Wiley& Sons, New York (1949 memorandum,reprinted, quoting a 1947 letter fromWeaver to Norbert Wiener).Yamron, Jonathan, James Cant, AnneDemedts, Taiko Dietzel, and Yoshiko Ito.1994.
The automatic omponent of theLINGSTAT machine-aided translationsystem.
In Proceedings ofthe ARPAWorkshop on Human Language Technology,pages 163-168.
Morgan Kaufmann.612
