Learning Word Meanings and Descriptive Parameter Spaces from MusicBrian WhitmanMIT Media LabMusic, Mind and MachineCambridge, MA USAbwhitman@media.mit.eduDeb RoyMIT Media LabCognitive MachinesCambridge, MA USAdkroy@media.mit.eduBarry VercoeMIT Media LabMusic, Mind and MachineCambridge, MA USAbv@media.mit.eduAbstractThe audio bitstream in music encodes a highamount of statistical, acoustic, emotional andcultural information.
But music also has animportant linguistic accessory; most musicalartists are described in great detail in recordreviews, fan sites and news items.
We high-light current and ongoing research into extract-ing relevant features from audio and simulta-neously learning language features linked tothe music.
We show results in a ?query-by-description?
task in which we learn the per-ceptual meaning of automatically-discoveredsingle-term descriptive components, as well asa method of automatically uncovering ?seman-tically attached?
terms (terms that have percep-tual grounding.)
We then show recent work in?semantic basis functions?
?
parameter spacesof description (such as fast ... slow or male... female) that encode the highest descriptivevariance in a semantic space.1 IntroductionWhat can you learn by listening to the radio all day?
Ifthe DJ was wordy enough, we argue that you can gainenough knowledge of the language of perception, as wellas the grammar of description and the grammar of music.Here we develop a system that uncovers descriptive pa-rameters of perception completely autonomously.
Rela-tions between English adjectives and audio features arelearned using a new ?severe multi-class?
algorithm basedon the support vector machine.
Training data consists ofmusic reviews from the Internet correlated echnology andEntertainment Media: Rights and Responsibilities withacoustic recordings of the reviewed music.
Once trained,we obtain a perceptually-grounded lexicon of adjectivesthat may be used to automatically label new music.
The1000 2000 3000 4000 500000.511.5 quiet1000 2000 3000 4000 500000.511.5 loud1000 2000 3000 4000 500000.511.5 funky1000 2000 3000 4000 500000.511.5 lonesomeFigure 1: Mean spectral characteristics of four differentterms uncovered by the spectral frame-based single termattachment system.
Magnitude of frequency on the y-axis, frequency in Hz on the x-axis.predictive accuracy of the perceptual models are evalu-ated on unseen test music-review data samples.
We con-sider terms with high predictive accuracy (i.e., that agreewith word usage of musical reviews not used during train-ing) to be well grounded.
We extend our prior work byintroducing a ?linguistic expert,?
in the form of a lexicalknowledge base that provides human-encoded symbolicknowledge about lexical relations.
We apply lexical re-lations to well grounded adjectives to determine the per-ceptual correlates of opposition.
This enables us to movefrom isolated word groundings to a gradation system bydiscovering the perceptual basis underlying lexical oppo-sition of adjective pairs (fast ... slow, hard ... soft, etc.
).Once we have uncovered these gradations, we effectivelyobtain a set of ?semantic basis functions?
which can beused to characterize music samples based on their per-ceptual projections onto these lexically determined basisfunctions.Term Precision Term Precisionacoustic 23.2% annoying 0.0%classical 27.4% dangerous 0.0%clean 38.9% gorgeous 0.0%dark 17.1% hilarious 0.0%electronic 11.7% lyrical 0.0%female 32.9% sexy 1.5%happy 13.8% troubled 0.0%romantic 23.1% typical 0.0%upbeat 21.0% wicked 0.0%vocal 18.6% worldwide 2.8%Table 1: Selected adjective terms and their weighted pre-cision in predicting a description of as-yet ?unheard?
mu-sic in the frame-based single term attachment system.The very low baseline and noisy ground truth contributeto low overall scores, but the difference between ?un-groundable?
and high-scoring terms are significant?
forexample, the system cannot find a spectral definition of?sexy.
?2 BackgroundIn the general audio domain, work has recently been done(Slaney, 2002) that links sound samples to description us-ing the labeled descriptions on the sample sets.
In the vi-sual domain, some work has been undertaken attemptingto learn a link between language and multimedia.
Thelexicon-learning aspects in (Duygulu et al, 2002) studya set of fixed words applied to an image database anduse a method similar to EM (expectation-maximization)to discover where in the image the terms (nouns) ap-pear.
(Barnard and Forsyth, 2000) outlines similar work.Regier has studied the visual grounding of spatial termsacross languages, finding subtle effects that depend onthe relative shape, size, and orientation of objects (Regier,1996).
Work on motion verb semantics include both pro-cedural (action) based representations building on PetriNet formalisms (Bailey, 1997; Narayanan, 1997) and en-codings of salient perceptual features (Siskind, 2001).
In(Roy, 1999), we explored aspects of learning shape andcolor terms, and took first steps in perceptually-groundedgrammar acquisition.We refer to a word as ?grounded?
if we are able to de-termine reliable perceptual or procedural associations ofthe word that agree with normal usage.
However, encod-ing single terms in isolation is only a first step in sensory-motor grounding.
Lexicographers have traditionally stud-ied lexical semantics in terms of lexical relations suchas opposition, hyponymy, and meronymy (Cruse, 1986).We have made initial investigations into the perceptualgrounding of lexical relations.
We argue that gradationsor linguistic parameter spaces (such as fast ... slow orbig ... small) are necessary to describe high-dimensionalperceptual input.Figure 2: The ?Radio, Radio?
platform for autonomouslylearning language and music models.
A bank of systems(with a distributed computing back-end connecting them)listens to multiple genres of radio streams and hones anacoustic model.
When a new artist is detected from themetadata, our cultural representation crawler extracts lan-guage used to describe the artist and adds to our languagemodel.
Concurrently, we learn relations between the mu-sic and language models to ground language terms in per-ception.Our first approach to this problem was in (Whitmanand Rifkin, 2002), in which we learned the descriptionsof music by a combination of automated web crawls forartist description and analysis of the spectral content oftheir music.
The results for that work, which appearin Figure 1 and Table 1, show that we can accuratelypredict (well above an impossibly low baseline) a labelon a held-out test set of music.
We also see encour-aging results in the set of terms that were accuratelypredicted.
In effect we can draw an imaginary line inthe form of a confidence threshold around our resultsand assign certain types of terms ?grounded?
while oth-ers are ?ungroundable.?
In Table 1 above, we note thatterms like ?electronic?
and ?vocal?
that would appear inthe underlying perceptual feature space get high scoreswhile more culturally-influenced terms like ?gorgeous?and ?sexy?
do not do as well.
We have recently extendedthis work (Whitman et al, 2003) by learning parametersin the same manner.
Just because we know the spectralshape of ?quiet?
and ?loud?
(as in Figure 1) we cannotinfer any sort of connecting space between them unlesswe know that they are antonyms.
In this work, we infersuch gradation spaces through the use of a lexical knowl-edge base, ?grounding?
such parameters through percep-tion.
As well, to capture important time-aware gradationssuch as ?fast...slow?
we introduce a new machine listeningnp Term Scorebeth gibbons 0.1648trip hop 0.1581dummy 0.1153goosebumps 0.0756soulful melodies 0.0608rounder records 0.0499dante 0.0499may 1997 0.0499sbk 0.0499grace 0.0499adj Term Scorecynical 0.2997produced 0.1143smooth 0.0792dark 0.0583particular 0.0571loud 0.0558amazing 0.0457vocal 0.0391unique 0.0362simple 0.0354Table 2: Top 10 terms (noun phrase and adjective sets) forthe musical group ?Portishead?
from community meta-data.representation that allows for far more perceptual gener-ality in the time domain than our previous work?s singleframe-based power spectral density.
Our current platformfor retrieving audio and description is shown in Figure 2.We acknowledge previous work on the computationalstudy of adjectival scales as in (Hatzivassiloglou andMcKeown, 1993), where a system could group gradationscales using a clustering algorithm.
The polar represen-tation of adjectives discussed in (Miller, 1990) also influ-enced our system.3 Automatically Uncovering DescriptionWe propose an unsupervised model of language featurecollection that is based on description by observation,that is, learning target classifications by reading about themusical artists in reviews and discussions.3.1 Community MetadataOur model is called community metadata (Whitman andLawrence, 2002) and has been successfully used in styledetection (Whitman and Smaragdis, 2002) and artist sim-ilarity prediction (Ellis et al, 2002).
It creates a ma-chine understandable representation of artist descriptionby searching the Internet for the artist name and perform-ing light natural language processing on the retrievedpages.
We split the returned documents into classes en-compassing n-grams (terms of word length n), adjectives(using a part-of-speech tagger (Brill, 1992)) and nounphrases (using a lexical chunker (Ramshaw and Mar-cus, 1995).)
Each pair {artist, term} retrieved is givenan associated salience weight, which indicates the rela-tive importance of term as associated to artist.
Thesesaliences are computed using a variant of the popular TF-IDF measure gaussian weighted to avoid highly specificand highly general terms.
(See Table 2 for an example.
)One important feature of community metadata is its time-sensitivity; terms can be crawled once a week and we cantake into account trajectories of community-level opinionabout certain artists.Although tempting, we are reticent to make the claimthat the community metadata vectors computationally ap-proach the ?linguistic division of labor?
proposed in (Put-nam, 1987) as each (albeit unaware) member of the net-worked community is providing a small bit of informa-tion and description about the artist in question.
We feelthat the heavily biased opinion extracted from the Inter-net is best treated as an approximation of a ?ground truthdescription.?
Factorizing the Internet community into rel-atively coherent smaller communities to obtain sharpenedlexical groundings is part of future work.
However, wedo in fact find that the huge amount of information weretrieve from these crawls average out to a good generalidea of the artists.4 Time-Aware Machine ListeningWe aim for a representation of audio content that cap-tures as much perceptual content as possible and ask thesystem to find patterns on its own.
Our representation isbased on the MPEG-7 (Casey, 2001) standard for con-tent understanding and metadata organization.1 The re-sult of an MPEG-7 encoding is a discrete state numberl (l = [1...n]) for each 1100 th of a second of input au-dio.
We histogram the state visits into counts for eachn-second piece of audio.5 Relating Audio to DescriptionGiven an audio and text model, we next discuss how todiscover relationships between them.
The approach weuse is the same as our previous work, where we placethe problem as a multi-class classification problem.
Ourinput observations are the audio-derived features, andin training, each audio feature is associated with somesalience weight of each of the 200,000 possible terms thatour community metadata crawler discovered.
In a recenttest, training 703 separate SVMs on a small adjective setin the frame-based single term system took over 10 days.In most machine learning classifiers, time is dependent onthe number of classes.
As well, due to the unsupervisedand automatic nature of the description classes, many areincorrect (such as when an artist is wrongly described)or unimportant (as in the case of terms such as ?talented?or ?cool??
meaningless to the audio domain.)
Lastly, be-cause the decision space over the entire artist space is solarge, most class outputs are negative.
This creates a biasproblem for most machine learning algorithms.
We nextshow our attempt at solving these sorts of problems us-ing a new classifier technique based on the support vectormachine.1Our audio representation is fully described in (Whitman etal., 2003).5.1 Regularized Least-Squares ClassificationRegularized Least-Squares Classification (Rifkin, 2002)allows us to solve ?severe multi-class?
problems wherethere are a great number of target classes and a fixed setof source observations.
It is related to the Support VectorMachine (Vapnik, 1998) in that they are both instancesof Tikhonov regularization (Evgeniou et al, 2000), butwhereas training a Support Vector Machine requires thesolution of a constrained quadratic programming prob-lem, training RLSC only requires solving a single systemof linear equations.
Recent work (Fung and Mangasar-ian, 2001), (Rifkin, 2002) has shown that the accuracy ofRLSC is essentially identical to that of SVMs.We arrange our observations in a Gram matrix K,where Kij ?
Kf (xi, xj) using the kernel function Kf .Kf (x1, x2) is a generalized dot product (in a Reproduc-ing Kernel Hilbert Space (Aronszajn, 1950)) between xiand xj.
We use the Gaussian kernelKf (x1, x2) = e?
(|x1?x2|)2?2 (1)where ?
is a parameter we keep at 0.5.Then, training an RLSC system consists of solving thesystem of linear equations(K +IC)c = y, (2)where C is a user-supplied regularization constant.
Theresulting real-valued classification function f isf(x) =?`i=1ciK(x, xi).
(3)The crucial property of RLSC is that if we store the in-verse matrix (K+ IC )?1, then for a new right-hand side y,we can compute the new c via a simple matrix multipli-cation.
This allows us to compute new classifiers (afterarranging the data and storing it in memory) on the flywith simple matrix multiplications.5.2 Evaluation for a ?Query-by-Description?
TaskTo evaluate our connection-finding system, we computethe weighted precision P (at) of predicting the label t foraudio derived features of artist a.
We train a new ct foreach term t against the training set.
ft(x) for the test setis computed over each audio-derived observation framex and term t. If the sign of ft(x) is the same as oursupposed ?ground truth?
for that {artist, t}, (i.e.
did theaudio frame for an artist correctly resolve to a known de-scriptive term?)
we consider the prediction successful.Due to the bias problem mentioned earlier, the evaluationis then computed on the test set by computing a ?weightedprecision?
: where P (ap) indicates overall positive accu-racy (given an audio-derived observation, the probabil-ity that a positive association to a term is predicted) andPerceptionLexicalKnowledge BaseDescription by ObservationFigure 3: Overview of our parameter grounding method.Semantically attached terms are discovered by findingstrong connections to perception.
We then ask a ?pro-fessional?
in the form of a lexical knowledge base aboutantonymial relations.
We use those relations to infer gra-dations in perception.P (an) indicates overall negative accuracy, P (a) is de-fined as P (ap)P (an), which should remain significanteven in the face of extreme negative output class bias.Now we sort the list of P (at) and set an arbitrarythreshold ?.
In our implementation, we use ?
= 0.1.
AnyP (at) greater than ?
is considered ?grounded.?
In thismanner we can use training accuracy to throw away badlyscoring classes and then figure out which were incorrector unimportant.6 Linguistic Experts for ParameterDiscoveryGiven a set of ?grounded?
single terms, we now discussour method for uncovering parameter spaces among thoseterms and learning the knobs to vary their gradation.
Ourmodel states that certain knowledge is not inferred fromsensory input or intrinsic knowledge but rather by query-ing a ?linguistic expert.?
If we hear ?loud?
audio and wehear ?quiet?
audio, we would need to know that thoseterms are antonymially related before inferring the gra-dation space between them.6.1 WordNetWordNet (Miller, 1990) is a lexical database hand-developed by lexicographers.
Its main organization isthe ?synset?, a group of synonymous words that may re-place each other in some linguistic context.
The mean-ing of a synset is captured by its lexical relations, suchas hyponymy, meronymy, or antonymy, to other synsets.WordNet has a large community of users and variousAPIs for accessing the information automatically.
Ad-jectives in WordNet are organized in two polar clusters ofsynsets, which each focal synset (the head adjective) link-ing to some antonym adjective.
The intended belief is thatnorthern - southern playful - seriousunlimited - limited naive - sophisticatedforeign - native consistent - inconsistentoutdoor - indoor foreign - domesticdissonant - musical physical - mentalopposite - alternate censored - uncensoredunforgettable - forgettable comfortable - uncomfortableconcrete - abstract untamed - tamepartial - fair empirical - theoreticalatomic - conventional curved - straightlean - rich lean - fatTable 3: Example synant relations.descriptive relations are stored as polar gradation spaces,implying that we can?t fully understand ?loud?
withoutalso understanding ?quiet.?
We use these antonymial re-lations to build up a new relation that encodes as muchantonymial expressivity as possible, which we describebelow.6.2 Synant SetsWe define a set of lexical relations called synants, whichconsist of every antonym of a source term along with ev-ery antonym of each synonym and every synonym of eachantonym.
In effect, we recurse through WordNet?s treeone extra level to uncover as many antonymial relationsas possible.
For example, ?quiet?
?s anchor antonym is?noisy,?
but ?noisy?
has other synonyms such as ?clan-gorous?
and ?thundering.?
By uncovering these second-order antonyms in the synant set, we hope to uncover asmuch gradation expressivity as possible.
Some examplesynants are shown in Table 3.The obvious downside of computing the synant set isthat they can quickly lose synonymy?
following from theexample above, we can go from ?quiet?
to its synonym?untroubled,?
which leads to an synantonymial relationof ?infested.?
We also expect problems due to our lack ofsense tagging: ?quiet?
to its fourth sense synonym ?re-strained?
to its antonym ?demonstrative,?
for example,probably has little to do with sound.
But in both caseswe rely again on the sheer size of our example space;with so many possible adjective descriptors and the largepotential size of the synant set, we expect our connection-finding machines to do the hard work of throwing awaythe mistakes.7 Innate Dimensionality of ParametersNow that we have a set of grounded antonymial adjec-tives pairs, we would like to investigate the mapping inperceptual space between each pair.
We can do this witha multidimensional scaling (MDS) algorithm.
Let us callall acoustically derived data associated with one adjec-tive as X1 and all data associated with the syn-antonymX2.
An MDS algorithm can be used to find a multidi-mensional embedding of the data based on pairwise sim-ilarity distances between data points.
The similarity dis-tances between music samples is based on the represen-tations described in the previous section.
Consider firstonly the data from X1.
The perceptual diversity of thisdata will reflect the fact that it represents numerous artistsand songs.
Overall, however, we would predict that a lowdimensional space can embed X1 with low stress (i.e.,good fit to the data) since all samples of X1 share a de-scriptive label that is well grounded.
Now consider theembedding of the combined data set of X1 and X2.
Inthis case, the additional dimensions needed to accomo-date the joint data will reflect the relation between thetwo datasets.
Our hypothesis was that the additional per-ceptual variance of datasets formed by combining pairsof datasets on the basis of adjective pairs which are (1)well grounded, and (2) synants, would small compared tocombinations in which either of these two combinationsdid not hold.
Following are intial results supporting thishypothesis.7.1 Nonlinear Dimensionality ReductionClassical dimensional scaling systems such as MDS orPCA can efficiently learn a low-dimensional weightingbut can only use euclidean or tangent distances betweenobservations to do so.
In complex data sets, the distancesmight be better represented as a nonlinear function tocapture inherent structure in the dimensions.
Especiallyin the case of music, time variances among adjacent ob-servations could be encoded as distances and used in thescaling.
We use the Isomap algorithm from (Tenenbaumet al, 2000) to capture this inherent nonlinearity andstructure of the audio features.
Isomap scales dimensionsgiven a NxN matrix of distances between every observa-tion in N .
It roughly computes global geodesic distanceby adding up a number of short ?neighbor hops?
(wherethe number of neighbors is a tunable parameter, here weuse k = 20) to get between two arbitrarily far points in in-put space.
Schemes like PCA or MDS would simply usethe euclidean distance to do this, where Isomap operateson prior knowledge of the structure within the data.
Forour purposes, we use the same gaussian kernel functionas we do for RLSC (Equation 1) for a distance metric,which has proved to work well for most music classifica-tion tasks.Isomap can embed in a set of dimensions beyond thetarget dimension to find the best fit.
By studying theresidual variance of each embedding, we can look for the?elbow?
(the point at which the variance falls off to theminimum)?
and treat that embedding as the innate one.We use this variance to show that our highly-groundedparameter spaces can be embedded in less dimensionsthan ungrounded ones.8 Experiments and ResultsIn the following section we describe our experiments us-ing the aforementioned models and show how we can au-tomatically uncover the perceptual parameter spaces un-derlying adjective oppositions.8.1 Audio datasetWe use audio from the NECI Minnowmatch testbed(Whitman et al, 2001).
The testbed includes on averageten songs from each of 1,000 albums from roughly 500artists.
The album list was chosen from the most popularsongs on OpenNap, a popular peer-to-peer music sharingservice, in August of 2001.
We do not separate audio-derived features among separate songs since our connec-tions in language are at the artist level (community meta-data refers to an artist, not an album or song.)
Therefore,each artist a is represented as a concatenated matrix ofFa computed from each song performed by that artist.Fa contains N rows of 40-dimensional data.
Each ob-servation represents 10 seconds of audio data.
We choosea random sampling of artists for both training and testing(25 artists each, 5 songs for a total of N observations fortesting and training) from the Minnowmatch testbed.8.2 RLSC for Audio to Term RelationEach artist in the testbed has previously been crawled forcommunity metadata vectors, which we associate withthe audio vectors as a yt truth vector.
In this experiment,we limit our results to adjective terms only.
The entirecommunity metadata space of 500 artists ended up withroughly 2,000 unique adjectives, which provide a goodsense of musical description.
The other term types (n-grams and noun phrases) are more useful in text retrievaltasks, as they contain more specific information such asband members, equipment or song titles.
Each audio ob-servation in N is associated with an artist a, which inturn is related to the set of adjectives with pre-definedsalience.
(Salience is zero if the term is not related, un-bounded if related.)
We are treating this problem as clas-sification, not regression, so we assign not-related termsa value of -1 and positively related terms are regularizedto 1.We compute a ct for each adjective term t on the train-ing set after computing the stored kernel.
We use a Cof 10.
After all the cts are stored to disk we then bringout the held-out test set and compute relative adjectiveweighted prediction accuracy P (a) for each term.
Theresults (in Table 4) are similar to our previous work butwe note that our new representation allows us to capturemore time- and structure-oriented terms.
We see that thetime-aware MPEG-7 representation creates a far bettersense of perceptual salience than our prior frame-basedpower spectral density estimation, which threw away allshort- and mid-time features.Term Precision Term Precisionbusy 42.2% artistic 0.0%steady 41.5% homeless 0.0%funky 39.2% hungry 0.0%intense 38.4% great 0.0%acoustic 36.6% awful 0.0%african 35.3% warped 0.0%melodic 27.8% illegal 0.0%romantic 23.1% cruel 0.0%slow 21.6% notorious 0.0%wild 25.5% good 0.0%young 17.5% okay 0.0%Table 4: Select adjective terms discovered by the time-aware adjective grounding system.
Overall, the attachedterm list is more musical due to the increased time-awareinformation in the representation.Parameter Precisionbig - little 30.3%present - past 29.3%unusual - familiar 28.7%low - high 27.0%male - female 22.3%hard - soft 21.9%loud - soft 19.8%smooth - rough 14.6%clean - dirty 14.0%vocal - instrumental 10.5%minor - major 10.2%Table 5: Select automatically discovered parameterspaces and their weighted precision.
The top are the mostsemantically significant description spaces for music un-derstanding uncovered autonomously by our system.8.3 Finding Parameter Spaces using WordNetLexical RelationsWe now take our new single-term results and ask our pro-fessional for help in finding parameters.
For all adjec-tives over our predefined ?
we retrieve a restricted synantset.
This restricted set only retrieves synants that arein our community metadata space: i.e.
we would notreturn ?soft?
as a synant to ?loud?
if we did not havecommunity-derived ?soft?
audio.
The point here is to onlyfind synantonymial relations that we have perceptual datato ?ground?
with.
We rank our synant space by the meanof the P (a) of each polar term.
For example, P (asoft)was 0.12 and we found a synant ?loud?
in our space witha P (aloud) of 0.26, so our P (aloud...soft) would be 0.19.This allows us to sort our parameter spaces by the maxi-mum semantic attachment.
We see results of this processin Table 5.We consider this result our major finding: from lis-tening to a set of albums and reading about the artists, acomputational system has automatically derived the opti-0 5 10 15 200.80.850.90.95 male ?
female0 5 10 15 200.40.60.81 major ?
minor0 5 10 15 200.80.850.90.951 loud ?
soft0 5 10 15 200.70.80.91 low ?
high0 5 10 15 200.920.940.960.981 alive ?
dead0 5 10 15 200.80.850.90.951 quiet ?
softResidual VarianceDimensionFigure 4: Residual variance elbows (marked by arrows)for different parameter spaces.
Note the clear elbowsfor grounded parameter spaces, while less audio-derivedspaces such as ?alive - dead?
maintain a high variancethroughout.
Bad antonym relations such as ?quiet - soft?also have no inherent dimensionality.mal (strongest connection to perception) semantic grada-tion spaces to describe the incoming observation.
Theseare not the most statistically significant bases but ratherthe most semantically significant bases for understandingand retrieval.8.4 Making Knobs and Uncovering DimensionalityWe would like to show the results of such understandingat work in a classification or retrieval interface, so we thenhave another algorithm learn the d-dimensional mappingof the two polar adjectives in each of the top n parameterspaces.
We also use this algorithm to uncover the naturaldimensionality of the parameter space.For each parameter space a1 ... a2, we take all obser-vations automatically labeled by the test pass of RLSC asa1 and all as a2 and separate them from the rest of theobservations.
The observations Fa1 are concatenated to-gether with Fa2 serially, and we choose an equal numberof observations from both to eliminate bias.
We take thissubset of observation Fa12 and embed it into a distancematrix D with the gaussian kernel in Equation 1.
We feedD to Isomap and ask for a one-dimensional embeddingof the space.
The result is a weighting that we can feedcompletely new unlabeled audio into and retrieve scalarvalues for each of these parameters.
We would like topropose that the set of responses from each of our new?semantic experts?
(weight matrices to determine param-eter values) define the most expressive semantic repre-sentation possible for music.By studying the residual variances of Isomap as in Fig-ure 4, we can see that Isomap finds inherent dimension-ality for our top grounded parameter spaces.
But for ?un-grounded?
parameters or non-antonymial spaces, there isless of a clear ?elbow?
in the variances indicating a naturalembedding.
For example, we see from Figure 4 that the?male - female?
parameter (which we construe as genderof artist or vocalist) has a lower inherent dimensional-ity than the more complex ?low - high?
parameter and islower yet than the ungroundable (in audio) ?alive - dead.
?These results allow us to evaluate our parameter discov-ery system (in which we show that groundable terms haveclearer elbows) but also provide an interesting windowinto the nature of descriptions of perception.9 ConclusionsWe show that we can derive the most semantically sig-nificant description spaces automatically, and also formthem into a knob for future classification, retrieval andeven synthesis.
Our next steps involve user studies ofmusic description, an attempt to discover if the meaningderived by community metadata matches up with individ-ual description, and a way to extract a user model fromlanguage to specify results based on prior experience.We are also currently working on new automatic lexi-cal relation discovery techniques.
For example, from theset of audio observations, we can infer antonymial rela-tions without the use of an expert by finding optimallystatistically separable observations.
As well, meronymy,hyponymy and synonymy can be inferred by studying ar-tificial combinations of observation (the mixture of ?loud?and ?peaceful?
might not resolve but the mixture of ?sexy?and ?romantic?
might.
)From the perspective of computational linguistics, wesee a rich area of future exploration at the boundary ofperceptual computing and lexical semantics.
We havedrawn upon WordNet to strengthen our perceptual repre-sentations, but we believe the converse is also true.
Theseexperiments are a step towards grounding WordNet inmachine perception.ReferencesN.
Aronszajn.
1950.
Theory of reproducing kernels.Transactions of the American Mathematical Society,68:337?404.D.
Bailey.
1997.
When push comes to shove: A compu-tational model of the role of motor control in the ac-quisition of action verbs.
Ph.D. thesis, University ofCalifornia at Berkeley.K.
Barnard and D. Forsyth.
2000.
Learning the seman-tics of words and pictures.Eric Brill.
1992.
A simple rule-based part-of-speech tag-ger.
In Proc.
ANLP-92, 3rd Conference on AppliedNatural Language Processing, pages 152?155, Trento,IT.Michael Casey.
2001.
General sound recognition andsimilarity tools.
In MPEG-7 Audio Workshop W-6 atthe AES 110th Convention, May.D.A.
Cruse.
1986.
Lexical Semantics.
Cambridge Uni-versity Press.P.
Duygulu, K. Barnard, J.F.G.
De Freitas, and D.A.Forsyth.
2002.
Object recognition as machine transla-tion: Learning a lexicon for a fixed image vocabulary.Dan Ellis, Brian Whitman, Adam Berezweig, and SteveLawrence.
2002.
The quest for ground truth in musicalartist similarity.
In Proc.
International Symposium onMusic Information Retrieval ISMIR-2002.Theodoros Evgeniou, Massimiliano Pontil, and TomasoPoggio.
2000.
Regularization networks and supportvector machines.
Advanced In Computational Mathe-matics, 13(1):1?50.Glenn Fung and O. L. Mangasarian.
2001.
Proximalsupport vector classifiers.
In Provost and Srikant, edi-tors, Proc.
Seventh ACM SIGKDD International Con-ference on Knowledge Discovery and Data Mining,pages 77?86.
ACM.Vasileios Hatzivassiloglou and Kathleen R. McKeown.1993.
Towards the automatic identification of adjecti-val scales: Clustering adjectives according to meaning.In Proceedings of the 31st Annual Meeting of the ACL.G.A.
Miller.
1990.
Wordnet: An on-line lexical database.International Journal of Lexicography, 3(4):235?312.S.
Narayanan.
1997.
Knowledge-based Action Repre-sentations for Metaphor and Aspect (KARMA).
Ph.D.thesis, University of California at Berkeley.H.
Putnam.
1987.
Representation and Reality.
MITPress.Lance Ramshaw and Mitch Marcus.
1995.
Text chunk-ing using transformation-based learning.
In DavidYarovsky and Kenneth Church, editors, Proc.
ThirdWorkshop on Very Large Corpora, pages 82?94, Som-erset, New Jersey.
Association for Computational Lin-guistics.T.
Regier.
1996.
The human semantic potential.
MITPress, Cambridge, MA.Ryan M. Rifkin.
2002.
Everything Old Is New Again:A Fresh Look at Historical Approaches to MachineLearning.
Ph.D. thesis, Massachusetts Institute ofTechnology.D.
Roy.
1999.
Learning Words from Sights and Sounds:A Computational Model.
Ph.D. thesis, MassachusettsInstitute of Technology.J.
Siskind.
2001.
Grounding the Lexical Semantics ofVerbs in Visual Perception using Force Dynamics andEvent Logic.
Journal of Artificial Intelligence Re-search, 15:31?90.Malcolm Slaney.
2002.
Semantic-audio retrieval.
InProc.
2002 IEEE International Conference on Acous-tics, Speech and Signal Processing, May.J.B.
Tenenbaum, V. de Silva, and J.C. Langford.
2000.
Aglobal geometric framework for nonlinear dimension-ality reduction.
Science, 290:2319?2323.Vladimir N. Vapnik.
1998.
Statistical Learning Theory.John Wiley & Sons.Brian Whitman and S. Lawrence.
2002.
Inferring de-scriptions and similarity for music from communitymetadata.
In Proc.
Int.
Computer Music Conference2002 (ICMC), pages 591?598, September.Brian Whitman and Ryan Rifkin.
2002.
Musical query-by-description as a multi-class learning problem.
InProc.
IEEE Multimedia Signal Processing Conference(MMSP), December.Brian Whitman and Paris Smaragdis.
2002.
Combin-ing musical and cultural features for intelligent styledetection.
In Proc.
Int.
Symposium on Music Inform.Retriev.
(ISMIR), pages 47?52, October.Brian Whitman, Gary Flake, and Steve Lawrence.
2001.Artist detection in music with minnowmatch.
In Proc.2001 IEEE Workshop on Neural Networks for SignalProcessing, pages 559?568.
Falmouth, Massachusetts,September 10?12.Brian Whitman, Deb Roy, and Barry Vercoe.
2003.Grounding a lexicon and lexical relations from ma-chine perception of music.
submitted.
