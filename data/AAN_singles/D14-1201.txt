Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1870?1880,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsZORE: A Syntax-based System for Chinese Open Relation ExtractionLikun Qiu and Yue ZhangSingapore University of Technology and Design, Singaporeqiulikun@gmail.com, yue zhang@sutd.edu.sgAbstractOpen Relation Extraction (ORE) over-comes the limitations of traditional IEtechniques, which train individual extrac-tors for every single relation type.
Sys-tems such as ReVerb, PATTY, OLLIE, andExemplar have attracted much attentionon English ORE.
However, few studieshave been reported on ORE for languagesbeyond English.
This paper presents asyntax-based Chinese (Zh) ORE system,ZORE, for extracting relations and seman-tic patterns from Chinese text.
ZOREidentifies relation candidates from auto-matically parsed dependency trees, andthen extracts relations with their semanticpatterns iteratively through a novel doublepropagation algorithm.
Empirical resultson two data sets show the effectiveness ofthe proposed system.1 IntroductionTraditional Information Extraction (IE) system-s train extractors for pre-specified relations (Kimand Moldovan, 1993).
This approach cannot scaleto the web, where target relations are not definedin advance.
Open Relation Extraction (ORE) at-tempts to solve this problem by shallow-parsing-based, syntax-based or semantic-role-based pat-tern matching without pre-defined relation types,and has achieved great success on open-domaincorpora ranging from news to Wikipedia (Bankoet al., 2007; Wu and Weld, 2010; Nakashole etal., 2012; Etzioni et al., 2011; Moro and Nav-igli, 2013).
Many NLP and IR applications, in-cluding selectional preference learning, common-sense knowledge and entailment rule mining, havebenefited from ORE (Ritter et al., 2010).
Howev-er, most existing ORE systems focus on English,and little research has been reported on other lan-guages.
In addition, existing ORE techniques aremainly concerned with the extraction of textual re-lations, without trying to give semantic analysis,which is the advantage of traditional IE.Our goal in this paper is to present a syntax-based Chinese (Zh) ORE system, ZORE, whichextracts relations by using syntactic dependen-cy patterns, while associating them with explic-it semantic information.
An example is shownis Figure 1, where the relation (cn?
(Oba-ma)o?
(President) , Pred[.?
(graduate)],M?
(Harvard) {? (Law School)) is extract-ed from the given sentence ?cn?
(Obama) o?
(President) .?
(graduate) u (from) M?
(Harvard) {? (Law School)?, and general-ized into the syntactic-semantic pattern {nsubj-NR(Af) Pred[.?
(graduate)] prep-u (from)pobj-NN(Di)}.
Here, Af and Di stand for humanand institution, respectively, according to a Chi-nese taxonomy Extended Cilin (Che et al., 2010).Rather than extracting binary relations and thengeneralizing them into semantic patterns, whichmost previous work does (Mausam et al., 2012;Nakashole et al., 2012; Moro and Navigli, 2012;Moro and Navigli, 2013), we develop a novelmethod that extracts relations and patterns simul-taneously.
A double propagation algorithm is usedto make relation and pattern information reinforceeach other, so that negative effects from automaticsyntactic and semantic analysis errors can be miti-gated.
In this way, semantic pattern information isleveraged to improve relation extraction.We manually annotate two sets of data, fromnews text and Wikipedia, respectively.
Experi-ments on both data sets show that the double prop-agation algorithm gives better precision and recallcompared to the baseline.
To our knowledge, weare one of the first to report empirical results onChinese ORE.
The ZORE system, together withthe two sets of test data we annotated, and the setsof 5 million relations and 344K semantic patternsextracted from news and Wikipedia, is freely re-1870Figure 1: A sample sentence analyzed by ZORE.leased1.2 Basic Definitions for Open InformationExtractionZORE is applied to web text to extract general re-lations and their semantic types.
Our definitionof relations follow previous work on ORE (Moroand Navigli, 2013), but with language-specific ad-justments.
In this section, we use the sentence inFigure 1 as an instance to describe the basic defi-nitions for ZORE.Definition 1 (predicate phrase) A predicatephrase is a sequence of words that contains at leastone verb or copula, and governs one or more nounphrases syntactically.
For instance, a predicatephrase for the sentence in Figure 1 is ?.?
(grad-uate)?.
Following Fader et al.
(2011), Mausam etal.
(2012) and Nakashole et al.
(2012), in case oflight verb constructions, the verb and its direct ob-ject jointly serve as predicate phrase.
We do notinclude prepositions into the predicate phrases.Definition 2 (argument) An argument is a basenoun phrase governed by a predicate phrase direct-ly or indirectly with a preposition.
For instance,?cn?
(Obama) o?
(President)?
and ?M?
(Harvard) {? (Law School)?
are two argu-ments of the predicate phrase ?.?
(graduate)?.Definition 3 (relation) A binary relation is atriple that consists of the predicate phrase Predand its two arguments x and y.
Accordingly, ann-ary relation contains n arguments.
For instance,the sentence in Figure 1 contains the binary rela-tion (cn?
(Obama) o?
(President), Pred[.?
(graduate)], M?
(Harvard) {? (LawSchool)).
In English, the two arguments of a bi-nary relation are usually positioned on the left andright of Pred, respectively.
Hence, shallow pat-terns are highly useful for English relation extrac-1https://sourceforge.net/projects/zore/tion (Banko et al., 2007).
In Chinese, however, thetwo arguments can be both on the left, both on theright or one on the left and one on the right of thepredicate, and the resulting binary relation can beeither (x, y, Pred), (Pred, x, y) and (x, Pred, y), de-pending on the sentence.
This makes the detectionof relation phrases more complicated.Definition 4 (syntactic pattern) A syntactic pat-tern is the syntactic abstraction of a relation.
A re-lation can be generalized into the combination ofwords, POS-tags and syntactic dependency labels(Nakashole et al., 2012).
For instance, the syntac-tic pattern of the sentence in Figure 1 is {nsubj-NR(A) Pred[.?]
prep-u pobj-NN(A)}.
It con-sists of four sub-patterns.
The first, nsubj-NR(A),denotes that the current phrase acts as the sub-ject of the predicate phrase with the POS-tag NR(proper nouns).
Here, ?(A)?
means that the phraseis an argument of the extracted relation.
The sec-ond sub-pattern denotes that the predicate phraseof the example is ?.?
(graduate)?.
Note thatthe words between the predicate and arguments(e.g., prep-u) are included into the pattern direct-ly (Nakashole et al., 2012; Mausam et al., 2012).Definition 5 (semantic signature) The seman-tic signature of a relation consists of the semanticcategories of the arguments.
The semantic signa-ture of Figure 1 is (Af, Di), where Af and Di de-notes human and institute, respectively.Definition 6 (semantic pattern) A semantic pat-tern is the semantic abstraction of a relation.
Itis the combination of a syntactic pattern and asemantic signature.
For instance, the syntacticpattern {nsubj-NR(A) Pred[.?]
prep-u pobj-NN(A)}, combined with the semantic signature(Af, Di), results in the semantic pattern {nsubj-NR(Af) Pred[.?]
prep-u pobj-NN(Di)}.1871Figure 2: Architecture of ZORE.Figure 3: Parsing result of the example sentencein Figure 1, in Stanford dependencies.3 ZOREThe architecture of ZORE is shown in Figure 2.
Itconsists of three components.
The first is a relationcandidate extractor, which consumes input tex-t and performs sentence segmentation, word seg-mentation, POS tagging, syntactic parsing, baseNP extraction, light verb structure (LVC) detectionand relation candidate extraction.
The output is aset of relation candidates.
The second componenttags relations and extracts semantic patterns by adouble propagation algorithm.
In the third compo-nent, extracted patterns are grouped into synsets,and relations are filtered by confidence scores.3.1 Extracting Relation Candidates3.1.1 Parsing and Base NP ExtractionZORE analyzes the syntactic structures of inputtexts by applying a pipeline of NLP tools.
Eachsentence is segmented into a list of words by usingthe Stanford segmenter (Chang et al., 2008), andparsed by using ZPar (Zhang and Clark, 2011),with POS tags and constituent structures by theCTB standard (Xue et al., 2005).
The result-ing constituent trees are transformed into projec-tive trees with Stanford dependencies by using theStanford parser (Chang et al., 2009).
Figure 4shows the parse tree of the sentence in Figure 1.Next, base noun phrases (NPs) are extractedfrom the dependency tree.
Here a base NP is amaximum phrase whose words can only have POSfrom the first row of Table 1.
The head word of abase NP can be either a noun, a pronoun, a num-ber or a measure word (the second row of Table1).
The dependency labels within a base NP canonly be from the third row of Table 1.
Obviously,a base NP does not contain other base NPs, and isalso not contained by any other base NP.3.1.2 Detecting Light Verb ConstructionsIn linguistics, a light verb is a verb that has littlesemantic content of its own, and typically form-s a predicate with a noun (Butt, 2003).
Exam-ple predicates by light verb constructions (LVC)include ?is a capital of?
and ?claim responsibil-ity for?, where ?is?
and ?claim?
are light verbs.Improper handling of LVC can cause a significan-t problem by uninformative extractions (Etzioni etal., 2011).
For example, if ?is?
and ?claim?
are ex-tracted as predicates, the resulting relations (suchas (Hamas, claimed, responsibility) from the sen-tence ?Hamas claimed responsibility for the Gazaattack?)
might not bare useful information.
Re-Verb (Etzioni et al., 2011) handles this problem byhard syntactic constraints, taking the noun phrase(e.g., responsibility) between a verb phrase (e.g.,?claim?)
and a preposition (e.g., ?for?)
as a part ofthe predicate phrase rather than an argument, lead-ing to the relation (Hamas, claimed responsibilityfor, the Gaza attack).In Chinese, LVCs are highly frequent andshould be handled properly in order to ensure thatthe extracted relations are informative.
Howev-er, the syntactic constraints in ReVerb can not betransferred to Chinese directly, because the wordorders of English and Chinese are quite different.1872LabelsBase NP modifier NN (common noun), M (measure word), CD (cardinal number), OD (ordinal number), PN (pronoun), NR(proper noun), NT (temporal noun), JJ (other noun-modifier), or PU (punctuation)Base NP head NN (common noun), M (measure word), CD (cardinal number), OD (ordinal number), PN (pronoun), NR(proper noun), NT (temporal noun)Labels in base NPs nn (noun compound modifier), conj (conjunct), nummod (number modifier), cc (coordinating conjunction),clf (classifier modifier), det (determiner), ordmod (ordinal number modifier), punct (punctuation), dep (otherdependencies), or amod (adjectival modifier)Labels from baseNPs to predicatephrasensubj (nominal subject), conj (conjunct), dobj (direct object), advmod (adverbial modifier), prep (preposi-tional modifier), pobj (prepositional object), lobj (localizer object), range (dative object that is a quantifierphrase), tmod (temporal modifier), plmod (localizer modifier of a preposition), attr (attributive), loc (local-izer), top (topic), xsubj (controlling subject), ba (?ba?
construction), nsubjpass (nominal passive subject)Table 1: Constraints on POS-tags and dependency labels.
Labels in the top three rows are used for baseNP extraction, while labels in the last row for traversing from a base NP to the predicate phrase.In Chinese, prepositions acting as the modifier ofa verb can be on both the left and right of the ver-b.
For instance, the sentence ?cn?
(Obama)o?
(President) u (from) M?
(Harvard) {?(Law School).?
(graduate)?
is a paraphrase ofthe sentence in Figure 1, with the preposition u(from) on the left of the predicate phrase.Chinese LVCs can be classified into two types,which we refer to as dummy-LVCs and commonLVCs, respectively.
For the first type, the predi-cate is a dummy verb such as ?
?1 (do)?
and ???
(give)?, which has a noun phrase as its object.Since dummy verbs in Chinese are a closed set, wedetect this type of LVCs (such as ?
?1 (do)?!(talk)?)
by finding the dummy verb from a lexi-con.
For the second type of LVCs, the predicate isa common verb, which has a nominalized structureor a common noun as its object.
For instance, ?
?m (launch) N (investigation)?
belongs to thistype of construction.Common LVCs are more difficult to detect thandummy-LVCs.
We detect common LVCs by thecontext.
Besides the NPs in the LVC itself, a com-mon LVC typically governs two NPs, with the lat-ter being connected to the predicate phrase by anLVC-related preposition such as ??
(for), ?u(for), ?
(for), ?
(to), ?
(with), ?
(with), ?
(with) ?.
Based on the observation, a basic idea ofidentifying common LVCs is to find verb-objectstructures that frequently co-occur with a LVC-related preposition in a large-scale corpus parsedautomatically.
For a given verb-object v, let fvandfpdenote the frequency of v and the frequency of vco-occurring with an LVC-related preposition, re-spectively.
We define the statistical strength of vto be an LVC as the ratio fp/fv.
If the statisticalstrength of v exceeds a threshold tlvc, we identify vas a LVC.
Table 2 illustrates some high-frequencyLVCs extracted by the method automatically.3.1.3 Extracting Relation CandidatesZORE tries to extract relation candidates fromsentences that contain two or more base NPs.
Giv-en two base NPs, we traverse the dependency treeto obtain the shortest path that connects them.
Thepath can contain only dependency labels in thefourth row of Table 1, and should contain at leastone of the labels from ?nsubj?
and ?dobj?
to en-sure that a predicate phrase is included in the path.If such a path is acquired, other base NPs governedby the same predicate phrase are included into thetarget relation, resulting in a n-ary relation candi-dates with each base NP being an argument.
Ac-cording to the predicate phrase, relation candidatescan be classified into the following classes.Common and dummy LVC relations.
In thistype of relations, the predicate phrase of the pathis an LVC (e.g., a light verb and a nominal ob-ject).
The two base NPs can be the subject orprepositional object of the light verb.
For instance,in the sentence ?
?&Z (Houdini) ?
(to) ?(my)??
(career)k (have)??
(big)K?
(in-fluence)?, ?k (have)?and ?K?
(influence)?
arecombined into a common LVC and taken as thepredicate phrase, resulting the relation (?&Z(Houdini), Pred[k (have)K?
(influence)],?(my)??
(career)).
In the corresponding Englishsentence, the predicate phrase ?be a big influencein?
is also an LVC structure.Verb relations.
In this type of relations, a verbacts as the predicate phrase.
For instance, the rela-tion (cn?
(Obama) o?
(President), Pred[.?
(graduate)], M?
(Harvard) {? (LawSchool)) extracted from the sentence in Figure 1is a typical verb relation.Relative-clause relations.
In an relative-clause1873Verb Noun?1 (do) (*) u1 (distribution),??
(analysis),?8 (collection),?U (modification),??
(visit),?v (punishment)k (have) (*) K?
(effect), z (contribution),, (interest),??
(help),@?
(understanding),?"
(expectation)) (generate) (**) K?
(effect),, (interest),~?(doubt),??
(shock),?a (good feeling),??
(fear)E?
(cause) (**) K?
(effect),??(destruction),??
(harm),% (threat),??
(pressure),Z6 (distraction)L?
(express) (**) ??
(satisfaction),?H (welcome),??
(respect),?b (worry),H (mourning),a (gratitude)?m (launch) (**) N (investigation),??
(attack),??
(offensive),1?
(criticism),1 (negotiation),?z (lawsuit)Table 2: Instances of dummy-LVCs (*) and common LVCs (**).
A verb in the left column is combinedwith a noun in the right column to form an LVC, which serves as the predicate phrase.relation, the head word is a noun, modified byan relative clause, but acting as an argument ofthe predicate of the relative clause semantically.The sentence ?.?
(graduate) u (from) M?
(Harvard) {? (Law School)  (de, an aux-iliary word) cn?
(Obama) o?
(president)?is a paraphrase of the sentence in Figure 1, withthe same predicate phrase and arguments.
How-ever, the relation extracted from this phrase is anrelative-clause relation (Pred[.?
(graduate)],M?
(Harvard) {? (Law School), cn?
(Obama) o?
(president)), which belongs to thesame pattern synset as the relation of Figure 1.3.2 Semantic Tagging by Double PropagationThe basic idea of our approach is to identify rela-tions and patterns iteratively through semantical-ly tagging the head words of arguments in rela-tion candidates.
Given a set of relation candidatesand a semantic taxonomy, the propagation consist-s of three steps.
In Step 1, monosemic argumentsin candidate relations are tagged with a seman-tic category, such as Af and Di, to obtain seman-tic patterns.
In Step 2 and Step 3, untagged am-biguous and unknown words are tagged by perfectmatching and partial matching, respectively.
In theend of each step, semantic patterns are generalizedfrom extracted and tagged relations, and then usedto help relation tagging in the next step.
Becauseof the two-way information exchange, we call thismethod double propagation.
The method can alsobe treated as similar to bootstrapping (Yangarberet al., 2000; Qiu et al., 2009).3.2.1 Step 1: Tagging Monosemic ArgumentsEach argument in a relation candidate is a base N-P.
Since base NPs are endocentric, we can take thesemantic category of the head word of a base NPas the semantic category of the base NP.
In a tax-onomy, each word is associated with one or moresemantic categories.
In this step, however, onlymonosemic words are tagged, while both ambigu-ous words and unknown words are left untagged.Most named entities are not included in thetaxonomy.
However, after POS-tagging, most ofthem are detected as NR (proper noun).
As a re-sult, they are taken as ambiguous words that canbe person names, organization names or locationnames.
The named entities that are not included inthe taxonomy are tagged in Steps 2 and 3.After this step, all the arguments in some re-lation candidates have been tagged with semanticcategories.
We refer to these relation candidates astagged relation candidates, and the remaining re-lation candidates as untagged relation candidates.Tagged relation candidate are generalized into se-mantic patterns, consisting of syntactic patternsand semantic signatures, as illustrated in Figure 1and Section 2.
We call the set of resulting seman-tic patterns SetSemPat.3.2.2 Step 2: Tagging by Perfect PatternMatchingIn this step, the arguments in the untagged relationcandidates are tagged by semantic pattern match-ing.
Given an untagged relation candidate r, weacquire a set of possible semantic categories foreach argument with an ambiguous head word.
Forthe arguments with unknown head words, we ac-quire a set of possible semantic categories accord-ing to their characters.
Qiu et al.
(2011) demon-strate that 98% Chinese words have at least onesynonym, which shares at least one character.
ForChinese nouns, the set of synonyms usually sharesthe last one or two characters.
According to this,our strategy for acquiring possible semantic cate-gories for an unknown word is as follows.Given an unknown word wu, if we find a knownword wkthat shares the last two character with wu,the semantic categories of wkwill be used as thepossible semantic categories of wu.
Otherwise, ifwe find a known word wkthat share the last onecharacter with wu, the semantic categories of wkwill be used as the possible categories of wu.1874We then acquire possible semantic signatures ofuntagged relation candidates, of which all the ar-guments are tagged with possible semantic cate-gories.
As in Step 1, we generalize relation r in-to a syntactic pattern patsyn, and then combinepatsynwith each possible semantic signature ofr to generate possible semantic patterns.
In caseone or more possible semantic patterns of r ex-ist in SetSemPat, if the highest frequency of thesepatterns is above a threshold tsem, the correspond-ing pattern will be taken as the semantic patternof r, from which we infer the semantic signaturefor r and then the semantic category for the headword of each argument of r. After this step, thefrequency of each semantic pattern in SetSemPatis updated according to the newly tagged relationcandidates.3.2.3 Step 3: Tagging by Partial PatternMatchingIn this step, we tag the ambiguous and unknownwords by partial matching rather than perfec-t matching of the whole semantic pattern.
Thiscan be treated as a back-off of the last step.We first split n-ary semantic patterns inSetSemPatinto binary semantic patterns, and cal-culate their frequencies.
Second, we split eachuntagged relation candidate r into several binarysub-relations and then search for the correspond-ing semantic patterns as in Step 2 ?
for each bi-nary sub-relation, we obtain a binary semantic sig-nature with the highest frequency.
By combiningthe binary semantic signatures, we obtain one n-ary semantic signature for r, based on which allthe unknown and ambiguous words can be taggedwith a semantic categories.
If all the argumentsof a relation candidate r are tagged, r is treated astagged.
Finally, according to the newly tagged re-lations, statistics in SetSemPatare updated.3.3 Grouping Patterns into SynsetsIn this step, we group semantic patterns fromSetSemPatinto pattern synsets, based on asingle-pass clustering process (Papka and Allan,1998).
Given two semantic patterns SemPatiandSemPatj, we refer to their corresponding syntac-tic pattern, semantic signature and predicate phras-es as SynPatiand SynPatj, SemSigiand SemSigj,Prediand Predj, respectively.
Not taking the pred-icate phrase into account, SynPatiand SynPatjareidentical, and we call them loosely identical (?
).The algorithm in Figure 4 is used to groupFigure 4: Algorithm for pattern synset grouping.Type Feature WeightBase r covers all words in c 0.96Base There are commas within r -0.47Base LENGTH(r)<10 words 0.35Base 10 words[LENGTH(r)<20 words 0.11Base 20 words[LENGTH(r) -1.06Base COUNT(arguments)=2 0.14Base COUNT(arguments)=3 0.33Base COUNT(arguments)=4 -0.60Base COUNT(arguments)> 4 -0.46SemPat Being tagged in Step 3 0.87SemPat Being tagged before Step 3 0.75SemPat 50[SIZE(SemPat) and untagged -0.05SemPat 50[SIZE(SemPat) and tagged 0.65SemPat 10[SIZE(SemPat)<50 and untagged -0.16SemPat 10[SIZE(SemPat)<50 and tagged 0.39SemPat 5[SIZE(SemPat)<10 and untagged -0.22SemPat 5[SIZE(SemPat)<10 and tagged 0.36SemPat SIZE(SemPat) <5 and untagged -0.92SemPat SIZE(SemPat) <5 and tagged -0.64Table 3: Features of the logistic regression classi-fier with weights trained on Wiki-500 dataset.patterns, where ARGCOUNT(SynPatj) denotes thenumber of arguments in SemPati, SEMCAT(arg1)indicates the semantic category of the first ar-gument, and ISSYNONYM (Predi, Predj) return-s whether two predicates are synonyms.
Insimilarity-based single-pass clustering, the topicexcursion problem is common (Papka and Allan,1998).
But since our similarity measure is sym-metric, we do not suffer from this problem.3.4 Computing the Confidence for RelationsWithout filtering, the extraction algorithm in theprevious sections may yield false relations.
Fol-lowing previous ORE systems, we make a balancebetween recall and precision by using a confidencethreshold (Fader et al., 2011).
A logistic regres-sion classifier is used to give a confidence scoreto each relation, with features shown in Table 3.In the table, c, r, arguments and SemPat denote1875Dataset Source #Sen #RelWiki-500 Chinese Wikipedia 500 561Sina-500 Sina News 500 707Table 4: Annotated relation datasets.clause, relation, arguments in a relation, and se-mantic pattern, respectively.
LENGTH(r), COUN-T(arguments) and SIZE(SemPat) indicate the num-ber of words in r, the number of arguments inr, and the number of relations that belong to thesame semantic pattern SemPat as r. Because se-mantic patterns from the double propagation al-gorithm are used as features in the classifier, theyparticipate in relation extraction also.
Their effecton relation extraction can directly demonstrate theeffectiveness of double propagation.4 Experiments4.1 Experimental SetupWe run ZORE on two difference corpora: the Chi-nese edition of Wikipedia (Wiki), which contain-s 4.3 million sentences (as of March 29, 2014),and a corpus from the Sina News archive (SinaNews), which includes 6.1 million sentences fromJanuary 2013 to May 2013.
The sentences that donot end with punctuations are filtered.
The Chi-nese taxonomy Extended Cilin2(Cilin) (Che et al.,2010) is used to give semantic categories for eachword.
Cilin contains 77,492 Chinese words, or-ganized into a five-level hierarchy.
There are 12categories in the top level, 94 in the second and1492 in the third.
In this paper, the second levelis used for semantic categories.
We create two testsets, containing 500 sentences from Wiki and 500sentences from Sina News, respectively (see Table4), annotated by two independent annotators us-ing the annotation strategy of Fader et al.
(2011).The thresholds tlvcand tsemfor pattern matchingare set as 0.4 and 5, tuned on 100 sentences fromWiki-500 dataset, respectively.4.2 Evaluation of Relation ExtractionFirst, we compare ZORE with a baseline systemto illustrate the effectiveness of the double prop-agation algorithm.
The baseline system does nothave the double propagation tagging componentin Figure 2, using the logistic regression classifierin Section 3.4.1 with the 9 base features to filterextracted relation candidates.
It is similar to thearchitecture of ReVerb (Fader et al., 2011).
We2http://ir.hit.edu.cn/demo/ltp/Sharing Plan.htmFigure 5: Performance on Wiki.Figure 6: Performance on Sina News.measure the precision and recall of the extractedrelations.
An extracted relation is considered cor-rect only when the predicate phrase and all the ar-guments match the the gold set.
On each data set,we perform 5-fold cross-validation test and takethe average as the final precision and recall.Figures 5 and 6 show the comparison of the twosystems on Wiki and Sina News, respectively.
OnWiki, ZORE has higher precision than the baselineat all levels of recall.
When the recall is 0.3, theprecision of ZORE is 0.77, 0.11 higher than thebaseline.
The result on Sina News is similar.
Thesecond column of Table 3 shows the weights of allfeatures trained on the Wiki data set, which indi-cates that the semantic pattern features can give apositive effect on relation filtering.Second, we compare the intermediate results atSteps 1, 2, and 3 in Section 3.2, respectively.
Theprecision, recall and F1 of the three steps with d-ifferent numbers of Wiki sentences (from 10K to5M sentences) are shown in Table 5.
This figureshows that Step 2 achieves higher precision thanStep 1 at all levels of recall, indicating that theword sense tagging method in step 2 is useful for1876Sentences Step 1 Step 2 Step 3P R F1 P R F1 P R F110K 0.947 0.032 0.062 0.960 0.043 0.082 0.933 0.075 0.13950K 0.894 0.075 0.138 0.922 0.105 0.189 0.907 0.139 0.241100K 0.897 0.093 0.169 0.924 0.130 0.228 0.909 0.160 0.272200K 0.901 0.114 0.202 0.926 0.157 0.268 0.892 0.191 0.315500K 0.891 0.146 0.251 0.909 0.196 0.322 0.860 0.230 0.3631M 0.860 0.164 0.275 0.885 0.219 0.351 0.842 0.248 0.3832M 0.797 0.182 0.296 0.819 0.250 0.383 0.788 0.278 0.4113M 0.784 0.187 0.302 0.802 0.253 0.385 0.778 0.282 0.4144M 0.739 0.178 0.287 0.801 0.258 0.390 0.778 0.287 0.4195M 0.779 0.189 0.304 0.798 0.260 0.392 0.768 0.289 0.420Table 5: Accuracies on different numbers Wiki sentences.a significant boost of recall, together with a lit-tle improvement in precision.
In particular, Step2 can extract about 20% relations with relativelyhigh precision (about 90%).
The result of Step 3is better to that of Step 2 in terms of F1-measure,with the highest F1-measure achieved by this step.4.3 Evaluation of PatternsZORE acquires 122K and 222K patterns from Wi-ki and Sina News, clustered into 59K and 118Kpattern synsets, respectively.
The frequency distri-bution of the Wiki patterns is shown in Figure 7,which conforms to Zipf?s law.To assess the accuracy of pattern extraction, werank the extracted patterns by the size, and eval-uated the precision of the top 100 and a randomset of 100 pattern synsets.
Two annotators wereshown a pattern synset with its semantic signatureand a few example relations, and then asked tojudge whether it indicates a valid semantic rela-tion or not.
The results are shown in Table 6.
Theaveraged precision is 92% for the top 100 set, and85% for the random 100 set.The patterns in a pattern synset can be takenas paraphrases (Barzilay and Lee, 2003).
We ob-serve that two synonymous patterns might differin three aspects.
First, two patterns can differby the predicates, which are synonyms.
For in-stance, the verbs ??
?, , ?, ?
?, ?, ?
?are synonyms, meaning ?to hold the appointmentof?.
Second, two patterns in the same synset canbelong to different syntactic patterns, and there-fore are paraphrases in the syntactic level.
For in-stance, the semantic patterns of the two sentences?.?
(graduate) u (from) M?
(Harvard) {? (Law School) (de, an auxiliary word) cn?
(Obama)o?
(president)?
and ?cn?
(Oba-ma)o?
(president)l(from)M?
(Harvard){? (Law School).?
(graduate)?
are both syn-onymous to that of the sentence in Figure 1; al-l the three patterns are found in the same synsetobtained by ZORE.
Third, two patterns can dif-fer only by the POS-tag.
For instance, ?cn?
(Obama) l(from) M?
(Harvard) {? (LawSchool).?
(graduate)?
and ?@?
(That) ??
(attorney) l(from) M?
(Harvard) {? (LawSchool) .?
(graduate)?
are synonyms with d-ifferent POS-tags for the first argument (i.e.
N-R and NN).
According to the grouping algorithmin Section 3.3, all the three types of paraphrasesare grouped in a pattern synset, which makes somesynsets very large.
The largest synset contains 110patterns, while the top 100 synsets contain morethan 20 patterns.4.4 Error analysisWe analyze the incorrect extractions (precisionloss) and missed correct relations (recall loss) re-turned by Step 2, running on 500K sentences.
Ta-ble 7 summarizes the types of correct relations thatare missed by ZORE.
40% missed relations aredue to the minimum frequency constraint on se-mantic patterns, which is used for a balance be-tween precision and recall.
Another main sourceof failure is the incorrect identification of the pred-icate phrase due to parsing errors, which accountfor 37% of the total errors.
Other sources of fail-ures include redundant arguments and segmenta-tion errors.
Most redundant arguments are relatedto prepositions such as ?U?
(according to)?
and???
(on the basis of)?.
For instance, in the sen-tence ?U?
(according to)??
(the)*: (pointof view) ?
(,) ?
(fundamental) ?K (prob-lem) ?
(is)?, an incorrect binary relation (??
(the)*: (point of view),?
(fundamental)?K (problem), Pred[?
(is)]) is extracted, becausethe prepositional object ???
(the) *: (pointof view)?
is tagged as an argument of the predi-cate phrase ??
(is)?.Table 8 summarizes the major types of incor-1877Corpus Patterns Synsets Top100 Random100Wiki 122,723 59,298 0.93 0.87Sina 222,773 118,923 0.91 0.83Table 6: Precision of pattern synsets.40% Relations filtered by semantic pattern constraint37% Could not identify correct predicates because ofpreprocessing errors12% Too many arguments because of parsing errors11% Segmentation and POS tagging errorsTable 7: Relations missed by ZORE.rect relations, 56% of which were caused by pars-ing errors, and 34% of which were due to wordsegmentation and POS tagging errors.
Althoughmany errors have been filtered by ZORE, thebiggest source of errors is still syntactic analysis,which is very important for high quality of ORE.5 Related WorkEnglish has been the major language on whichORE research has been conducted.
Previouswork on English ORE has evolved from shallow-syntactic (Banko et al., 2007; Fader et al., 2011;Merhav et al., 2012) to full-syntactic (Nakasholeet al., 2012; Mausam et al., 2012; Moro and Nav-igli, 2013; Xu et al., 2013) and semantic (Johans-son and Nugues, 2008) systems.It has been shown that a full-syntactic systembased on dependency grammar can give signif-icantly better results than shallow syntactic sys-tems based on surface POS-patterns, yet enjoyhigher efficiency compared with semantic system-s (Mesquita et al., 2013).
Our investigation onChinese ORE takes root in full dependency syntaxand is hence able to identify patterns that involvelong-range dependencies.
Considering the charac-teristics of the Chinese language, such as the lackof morphology and function words, and the highsegmentation and word sense ambiguities, we in-corporate semantic ontology information into thedesign of the system to improve the output qualitywithout sacrificing efficiency.The state-of-the-art systems most closely relat-ed to our approach are PATTY (Nakashole et al.,2012) and the system of Moro and Navigli (2013).Both, however, extract relations first, and then de-fines patterns based on extracted relations.
Thispaper differs in that patterns and relations are ex-tracted in a simultaneous process and so they canimprove each other.
Previous studies show thatpattern generalization benefit from relation extrac-56% Parsing errors17% Segmentation errors17% POS tagging errors6% Redundant arguments6% Other, including base NP extraction errorsTable 8: Incorrect extractions by ZORE.Figure 7: The frequency distribution of patternsextracted from Wiki.
Size and Count denote thenumber of relations that belong to a semantic pat-tern and the logarithmic number of semantic pat-terns that have the same size, respectively.tion (Nakashole et al., 2012; Moro and Navigli,2013), and relation extraction can benefit frompattern generalization (Mausam et al., 2012).
Byusing double propagation, not only can we makerelation and pattern extraction benefit from eachother, but we can also tag relations and patternswith semantic categories in a joint process.There has been a line of research on Chinese re-lation extraction, where both feature-based (Zhouet al., 2005; Li et al., 2008) and kernel-based(Zhang et al., 2006; Che et al., 2005) methods havebeen applied.
In addition, semantic ontologiessuch as Extended Cilin have been shown usefulfor Chinese relation extraction (Liu et al., 2013).However, these studies have focused on tradition-al IE, with pre-defined relations.
In contrast, weinvestigate ORE for Chinese, finding that seman-tic ontologies useful for this task also.
Tseng et al.
(2014) is the only previous research focusing onChinese ORE. Their system can be considered asa pipeline of word segmentation, POS-tagging andparsing, while our work gives semantic interpreta-tion and explicitly deals with statistical errors inparsing by a novel double propagation algorithmbetween patterns and relations.18786 Conclusion and Future WorkWe presented a Chinese ORE system that inte-grates relation extraction with semantic patterngeneralization by double propagation.
Experimen-tal results on two datasets demonstrated the ef-fectiveness of the proposed algorithm.
We makethe ZORE system, together with the large scalerelations and pattern synsets extracted by ZORE,freely available at (https://sourceforge.net/projects/zore/).
Another version ofZORE (ZORE-PMT), which is based on the de-pendency tagset from PMT1.0 (Qiu et al., 2014),is also provided.Our error analysis demonstrates that the quali-ty of syntactic parsing is crucial to the accuracy ofsyntax-based Chinese ORE.
Improvements to syn-tactic analysis is likely to lead to improved ORE.In addition, the idea of double propagation can begeneralized into information propagation betweenrelation extraction and syntactic analysis.
We planto investigate the use of ORE in improving syntac-tic analysis in future work.AcknowledgmentsWe gratefully acknowledge the invaluable assis-tance of Ji Ma, Wanxiang Che and Yijia Liu.
Wealso thank the anonymous reviewers for their con-structive comments, and gratefully acknowledgethe support of the Singapore Ministry of Educa-tion (MOE) AcRF Tier 2 grant T2MOE201301,the start-up grant SRG ISTD 2012 038 fromSingapore University of Technology and Design,the National Natural Science Foundation of Chi-na (No.
61103089), National High Technolo-gy Research and Development Program of Chi-na (863 Program) (No.
2012AA011101), Ma-jor National Social Science Fund of China (No.12&ZD227), Scientific Research Foundation ofShandong Province Outstanding Young ScientistAward (No.
BS2013DX020) and Humanities andSocial Science Projects of Ludong University (No.WY2013003).ReferencesMichele Banko, Michael J Cafarella, Stephen Soder-land, Matthew Broadhead, and Oren Etzioni.
2007.Open information extraction for the web.
In IJCAI,volume 7, pages 2670?2676.Regina Barzilay and Lillian Lee.
2003.
Learn-ing to paraphrase: an unsupervised approach usingmultiple-sequence alignment.
In Proceedings of the2003 Conference of the North American Chapterof the Association for Computational Linguistics onHuman Language Technology-Volume 1, pages 16?23.
Association for Computational Linguistics.Miriam Butt.
2003.
The light verb jungle.
In Work-shop on Multi-Verb Constructions, pages 1?28.Pi-Chuan Chang, Michel Galley, and Christopher DManning.
2008.
Optimizing Chinese word segmen-tation for machine translation performance.
In Pro-ceedings of the Third Workshop on Statistical Ma-chine Translation, pages 224?232.
Association forComputational Linguistics.Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, andChristopher D Manning.
2009.
Discriminative re-ordering with Chinese grammatical relations fea-tures.
In Proceedings of the Third Workshop on Syn-tax and Structure in Statistical Translation, pages51?59.
Association for Computational Linguistics.WX Che, Jianmin Jiang, Zhong Su, Yue Pan, and T-ing Liu.
2005.
Improved-edit-distance kernel forChinese relation extraction.
In IJCNLP, pages 132?137.Wanxiang Che, Zhenghua Li, and Ting Liu.
2010.
Ltp:A Chinese language technology platform.
In Pro-ceedings of the 23rd International Conference onComputational Linguistics: Demonstrations, pages13?16.
Association for Computational Linguistics.Oren Etzioni, Anthony Fader, Janara Christensen,Stephen Soderland, and Mausam Mausam.
2011.Open information extraction: The second genera-tion.
In Proceedings of the Twenty-Second inter-national joint conference on Artificial Intelligence-Volume Volume One, pages 3?10.
AAAI Press.Anthony Fader, Stephen Soderland, and Oren Etzion-i.
2011.
Identifying relations for open informa-tion extraction.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Pro-cessing, pages 1535?1545.
Association for Compu-tational Linguistics.Richard Johansson and Pierre Nugues.
2008.Dependency-based semantic role labeling of prop-bank.
In Proceedings of the Conference on Empiri-cal Methods in Natural Language Processing, pages69?78.
Association for Computational Linguistics.Jun-Tae Kim and Dan I Moldovan.
1993.
Acquisitionof semantic patterns for information extraction fromcorpora.
In Artificial Intelligence for Application-s, 1993.
Proceedings., Ninth Conference on, pages171?176.
IEEE.Wenjie Li, Peng Zhang, Furu Wei, Yuexian Hou, andQin Lu.
2008.
A novel feature-based approachto Chinese entity relation extraction.
In Proceed-ings of the 46th Annual Meeting of the Associationfor Computational Linguistics on Human LanguageTechnologies: Short Papers, pages 89?92.
Associa-tion for Computational Linguistics.1879Dandan Liu, Zhiwei Zhao, Yanan Hu, and LonghuaQian.
2013.
Incorporating lexical semantic simi-larity to tree kernel-based Chinese relation extrac-tion.
In Chinese Lexical Semantics, pages 11?21.Springer.Michael Schmitz Mausam, Robert Bart, StephenSoderland, and Oren Etzioni.
2012.
Open languagelearning for information extraction.
pages 523?534.Yuval Merhav, Filipe Mesquita, Denilson Barbosa,Wai Gen Yee, and Ophir Frieder.
2012.
Extractinginformation networks from the blogosphere.
ACMTransactions on the Web (TWEB), 6(3):11.Filipe Mesquita, Jordan Schmidek, and Denilson Bar-bosa.
2013.
Effectiveness and efficiency of openrelation extraction.
New York Times, 500:150.Andrea Moro and Roberto Navigli.
2012.
Wisenet:Building a wikipedia-based semantic network withontologized relations.
In Proceedings of the 21stACM international conference on Information andknowledge management, pages 1672?1676.
ACM.Andrea Moro and Roberto Navigli.
2013.
Integratingsyntactic and semantic analysis into the open infor-mation extraction paradigm.
In Proceedings of theTwenty-Third international joint conference on Arti-ficial Intelligence, pages 2148?2154.
AAAI Press.Ndapandula Nakashole, Gerhard Weikum, and Fabi-an Suchanek.
2012.
PATTY: a taxonomy of rela-tional patterns with semantic types.
In Proceedingsof the 2012 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning, pages 1135?1145.
As-sociation for Computational Linguistics.Ron Papka and James Allan.
1998.
On-line new eventdetection using single pass clustering.
University ofMassachusetts, Amherst.Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.2009.
Expanding domain sentiment lexicon throughdouble propagation.
In IJCAI, volume 9, pages1199?1204.Likun Qiu, Yunfang Wu, and Yanqiu Shao.
2011.Combining contextual and structural information forsupersense tagging of Chinese unknown words.
InComputational Linguistics and Intelligent Text Pro-cessing, pages 15?28.
Springer.Likun Qiu, Yue Zhang, Peng Jin, and Houfeng Wang.2014.
Multi-view Chinese treebanking.
In Proceed-ings of COLING 2014, pages 257?268.Alan Ritter, Oren Etzioni, et al.
2010.
A latent dirich-let allocation method for selectional preferences.
InProceedings of the 48th Annual Meeting of the As-sociation for Computational Linguistics, pages 424?434.
Association for Computational Linguistics.Yuen-Hsien Tseng, Lung-Hao Lee, Shu-Yen Lin, Bo-Shun Liao, Mei-Jun Liu, Hsin-Hsi Chen, Oren Et-zioni, and Anthony Fader.
2014.
Chinese open rela-tion extraction for knowledge acquisition.
In EACL2014, pages 12?16.Fei Wu and Daniel S Weld.
2010.
Open informationextraction using wikipedia.
In Proceedings of the48th Annual Meeting of the Association for Compu-tational Linguistics, pages 118?127.
Association forComputational Linguistics.Ying Xu, Mi-Young Kim, Kevin Quinn, Randy Goebel,and Denilson Barbosa.
2013.
Open informationextraction with tree kernels.
In Proceedings ofNAACL-HLT, pages 868?877.Naiwen Xue, Fei Xia, Fu-Dong Chiou, and MarthaPalmer.
2005.
The penn Chinese treebank: Phrasestructure annotation of a large corpus.
Natural lan-guage engineering, 11(2):207?238.Roman Yangarber, Ralph Grishman, Pasi Tapanainen,and Silja Huttunen.
2000.
Automatic acquisitionof domain knowledge for information extraction.
InProceedings of the 18th conference on Computation-al linguistics-Volume 2, pages 940?946.
Associationfor Computational Linguistics.Yue Zhang and Stephen Clark.
2011.
Syntactic pro-cessing using the generalized perceptron and beamsearch.
Computational Linguistics, 37(1):105?151.Min Zhang, Jie Zhang, Jian Su, and Guodong Zhou.2006.
A composite kernel to extract relations be-tween entities with both flat and structured features.In Proceedings of the 21st International Conferenceon Computational Linguistics and the 44th annualmeeting of the Association for Computational Lin-guistics, pages 825?832.
Association for Computa-tional Linguistics.GuoDong Zhou, Su Jian, Zhang Jie, and Zhang Min.2005.
Exploring various knowledge in relation ex-traction.
In Proceedings of the 43rd annual meetingon association for computational linguistics, pages427?434.
Association for Computational Linguistic-s.1880
