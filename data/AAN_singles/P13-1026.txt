Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 260?269,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsModeling Thesis Clarity in Student EssaysIsaac Persing and Vincent NgHuman Language Technology Research InstituteUniversity of Texas at DallasRichardson, TX 75083-0688{persingq,vince}@hlt.utdallas.eduAbstractRecently, researchers have begun explor-ing methods of scoring student essays withrespect to particular dimensions of qual-ity such as coherence, technical errors,and relevance to prompt, but there is rel-atively little work on modeling thesis clar-ity.
We present a new annotated corpusand propose a learning-based approach toscoring essays along the thesis clarity di-mension.
Additionally, in order to pro-vide more valuable feedback on why anessay is scored as it is, we propose a sec-ond learning-based approach to identify-ing what kinds of errors an essay has thatmay lower its thesis clarity score.1 IntroductionAutomated essay scoring, the task of employingcomputer technology to evaluate and score writ-ten text, is one of the most important educationalapplications of natural language processing (NLP)(see Shermis and Burstein (2003) and Shermis etal.
(2010) for an overview of the state of the artin this task).
A major weakness of many ex-isting scoring engines such as the Intelligent Es-say AssessorTM(Landauer et al, 2003) is that theyadopt a holistic scoring scheme, which summa-rizes the quality of an essay with a single score andthus provides very limited feedback to the writer.In particular, it is not clear which dimension ofan essay (e.g., style, coherence, relevance) a scoreshould be attributed to.
Recent work addresses thisproblem by scoring a particular dimension of es-say quality such as coherence (Miltsakaki and Ku-kich, 2004), technical errors, Relevance to Prompt(Higgins et al, 2004), and organization (Persinget al, 2010).
Essay grading software that providesfeedback along multiple dimensions of essay qual-ity such as E-rater/Criterion (Attali and Burstein,2006) has also begun to emerge.Nevertheless, there is an essay scoring dimen-sion for which few computational models havebeen developed ?
thesis clarity.
Thesis clarityrefers to how clearly an author explains the thesisof her essay, i.e., the position she argues for withrespect to the topic on which the essay is written.1An essay with a high thesis clarity score presentsits thesis in a way that is easy for the reader tounderstand, preferably but not necessarily directly,as in essays with explicit thesis sentences.
It addi-tionally contains no errors such as excessive mis-spellings that make it more difficult for the readerto understand the writer?s purpose.Our goals in this paper are two-fold.
First, weaim to develop a computational model for scoringthe thesis clarity of student essays.
Because thereare many reasons why an essay may receive a lowthesis clarity score, our second goal is to build asystem for determining why an essay receives itsscore.
We believe the feedback provided by thissystem will be more informative to a student thanwould a thesis clarity score alone, as it will helpher understand which aspects of her writing needto be improved in order to better convey her the-sis.
To this end, we identify five common errorsthat impact thesis clarity, and our system?s pur-pose is to determine which of these errors occurin a given essay.
We evaluate our thesis clarityscoring model and error identification system on adata set of 830 essays annotated with both thesisclarity scores and errors.In sum, our contributions in this paper are three-fold.
First, we develop a scoring model and erroridentification system for the thesis clarity dimen-sion on student essays.
Second, we use featuresexplicitly designed for each of the identified error1An essay?s thesis is the overall message of the entire es-say.
This concept is unbound from the the concept of thesissentences, as even an essay that never explicitly states its the-sis in any of its sentences may still have an overall messagethat can be inferred from the arguments it makes.260Topic Languages EssaysMost university degrees are the-oretical and do not prepare stu-dents for the real world.
They aretherefore of very little value.13 131The prison system is outdated.No civilized society should pun-ish its criminals: it should reha-bilitate them.11 80In his novel Animal Farm,George Orwell wrote ?All menare equal but some are moreequal than others.?
How true isthis today?10 64Table 1: Some examples of writing topics.types in order to train our scoring model, in con-trast to many existing systems for other scoring di-mensions, which use more general features devel-oped without the concept of error classes.
Third,we make our data set consisting of thesis clarityannotations of 830 essays publicly available in or-der to stimulate further research on this task.
Sinceprogress in thesis clarity modeling is hindered inpart by the lack of a publicly annotated corpus, webelieve that our data set will be a valuable resourceto the NLP community.2 Corpus InformationWe use as our corpus the 4.5 million word Interna-tional Corpus of Learner English (ICLE) (Grangeret al, 2009), which consists of more than 6000 es-says written by university undergraduates from 16countries and 16 native languages who are learn-ers of English as a Foreign Language.
91% of theICLE texts are argumentative.
We select a sub-set consisting of 830 argumentative essays fromthe ICLE to annotate and use for training and test-ing of our models of essay thesis clarity.
Table 1shows three of the thirteen topics selected for an-notation.
Fifteen native languages are representedin the set of essays selected for annotation.3 Corpus AnnotationFor each of the 830 argumentative essays, we asktwo native English speakers to (1) score it alongthe thesis clarity dimension and (2) determine thesubset of the five pre-defined errors that detractsfrom the clarity of its thesis.Scoring.
Annotators evaluate the clarity of eachessay?s thesis using a numerical score from 1 to4 at half-point increments (see Table 2 for a de-scription of each score).
This contrasts with pre-vious work on essay scoring, where the corpus isScore Description of Thesis Clarity4 essay presents a very clear thesis and requireslittle or no clarification3 essay presents a moderately clear thesis butcould benefit from some clarification2 essay presents an unclear thesis and wouldgreatly benefit from further clarification1 essay presents no thesis of any kind and it isdifficult to see what the thesis could beTable 2: Descriptions of the meaning of scores.annotated with a binary decision (i.e., good or bad)for a given scoring dimension (e.g., Higgins et al(2004)).
Hence, our annotation scheme not onlyprovides a finer-grained distinction of thesis clar-ity (which can be important in practice), but alsomakes the prediction task more challenging.To ensure consistency in annotation, we ran-domly select 100 essays to have graded by bothannotators.
Analysis of these essays reveals that,though annotators only exactly agree on the the-sis clarity score of an essay 36% of the time, thescores they apply are within 0.5 points in 62% ofessays and within 1.0 point in 85% of essays.
Ta-ble 3 shows the number of essays that receive eachof the seven scores for thesis clarity.score 1.0 1.5 2.0 2.5 3.0 3.5 4.0essays 4 9 52 78 168 202 317Table 3: Distribution of thesis clarity scores.Error identification.
To identify what kinds oferrors make an essay?s thesis unclear, we ask oneof our annotators to write 1?4 sentence critiquesof thesis clarity on 527 essays, and obtain our listof five common error classes by categorizing thethings he found to criticize.
We present our anno-tators with descriptions of these five error classes(see Table 4), and ask them to assign zero or moreof the error types to each essay.It is important to note that we ask our anno-tators to mark an essay with one of these errorsonly when the error makes the thesis less clear.
Sofor example, an essay whose thesis is irrelevant tothe prompt but is explicitly and otherwise clearlystated would not be marked as having a Relevanceto Prompt error.
If the irrelevant thesis is statedin such a way that its inapplicability to the promptcauses the reader to be confused about what theessay?s purpose is, however, then the essay wouldbe assigned a Relevance to Prompt error.To measure inter-annotator agreement on erroridentification, we ask both annotators to identify261Id Error DescriptionCP Confusing Phrasing The thesis is phrased oddly, making it hard to understand the writer?s point.IPR Incomplete Prompt Response The thesis seems to leave some part of a multi-part prompt unaddressed.R Relevance to Prompt The apparent thesis?s weak relation to the prompt causes confusion.MD Missing Details The thesis leaves out important detail needed to understand the writer?s point.WP Writer Position The thesis describes a position on the topic without making it clear that this isthe position the writer supports.Table 4: Descriptions of thesis clarity errors.the errors in the same 100 essays that were doubly-annotated with thesis clarity scores.
We then com-pute Cohen?s Kappa (Carletta, 1996) on each er-ror from the two sets of annotations, obtaining anaverage Kappa value of 0.75, which indicates fairagreement.
Table 5 shows the number of essaysassigned to each of the five thesis clarity errors.As we can see, Confusing Phrasing, IncompletePrompt Response, and Relevance to Prompt arethe major error types.error CP IPR R MD WPessays 152 123 142 47 39Table 5: Distribution of thesis clarity errors.Relationship between clarity scores and errorclasses.
To determine the relationship betweenthesis clarity scores and the five error classes, wetrain a linear SVM regressor using the SVMlightsoftware package (Joachims, 1999) with the fiveerror types as independent variables and the re-duction in thesis clarity score due to errors as thedependent variable.
More specifically, each train-ing example consists of a target, which we set tothe essay?s thesis clarity score minus 4.0, and sixbinary features, each of the first five representingthe presence or absence of one of the five errors inthe essay, and the sixth being a bias feature whichwe always set to 1.
Representing the reduction inan essay?s thesis clarity score with its thesis clarityscore minus 4.0 allows us to more easily interpretthe error and bias weights of the trained system,as under this setup, each error?s weight should be anegative number reflecting how many points an es-say loses due to the presence of that error.
The biasfeature allows for the possibility that an essay maylose points from its thesis clarity score for prob-lems not accounted for in our five error classes.By setting this bias feature to 1, we tell our learnerthat an essay?s default score may be less than 4.0because these other problems may lower the aver-age score of otherwise perfect essays.After training, we examined the weight param-eters of the learned regressor and found that theywere all negative: ?0.6 for CP, ?0.5998 for IPR,?0.8992 for R, ?0.6 for MD, ?0.8 for WP, and?0.1 for the bias.
These results are consistentwith our intuition that each of the enumerated er-ror classes has a negative impact on thesis clarityscore.
In particular, each has a demonstrable neg-ative impact, costing essays an average of morethan 0.59 points when it occurs.
Moreover, this setof errors accounts for a large majority of all errorsimpacting thesis clarity because unenumerated er-rors cost essays an average of only one-tenth ofone point on the four-point thesis clarity scale.4 Error ClassificationIn this section, we describe in detail our system foridentifying thesis clarity errors.4.1 Model Training and ApplicationWe recast the problem of identifying which the-sis clarity errors apply to an essay as a multi-labelclassification problem, wherein each essay may beassigned zero or more of the five pre-defined er-ror types.
To solve this problem, we train five bi-nary classifiers, one for each error type, using aone-versus-all scheme.
So in the binary classifi-cation problem for identifying error ei, we createone training instance from each essay in the train-ing set, labeling the instance as positive if the es-say has ei as one of its labels, and negative other-wise.
Each instance is represented by seven typesof features, including two types of baseline fea-tures (Section 4.2) and five types of features weintroduce for error identification (Section 4.3).After creating training instances for error ei, wetrain a binary classifier, bi, for identifying whichtest essays contain error ei.
We use SVMlight forclassifier training with the regularization param-eter, C , set to ci.
To improve classifier perfor-mance, we perform feature selection.
While weemploy seven types of features (see Sections 4.2and 4.3), only the word n-gram features are sub-ject to feature selection.2 Specifically, we employ2We do not apply feature selection to the remaining fea-262the top ni n-gram features as selected according toinformation gain computed over the training data(see Yang and Pedersen (1997) for details).
Fi-nally, since each classifier assigns a real value toeach test essay presented to it indicating its con-fidence that the essay should be assigned error ei,we employ a classification threshold ti to decidehow high this real value must be in order for oursystem to conclude that an essay contains error ei.Using held-out validation data, we jointly tunethe three parameters in the previous paragraph, ci,ni, and ti, to optimize the F-score achieved by bifor error ei.3 However, an exact solution to this op-timization problem is computationally expensive.Consequently, we find a local maximum by em-ploying the simulated annealing algorithm (Kirk-patrick et al, 1983), altering one parameter at atime to optimize F-score by holding the remainingparameters fixed.After training the classifiers, we use them toclassify the test set essays.
The test instances arecreated in the same way as the training instances.4.2 Baseline FeaturesOur Baseline system for error classification em-ploys two types of features.
First, since labelingessays with thesis clarity errors can be viewed asa text categorization task, we employ lemmatizedword unigram, bigram, and trigram features thatoccur in the essay that have not been removed bythe feature selection parameter ni.
Because theessays vary greatly in length, we normalize eachessay?s set of word features to unit length.The second type of baseline features is based onrandom indexing (Kanerva et al, 2000).
Randomindexing is ?an efficient, scalable and incremen-tal alternative?
(Sahlgren, 2005) to Latent Seman-tic Indexing (Deerwester et al, 1990; Landauerture types since each of them includes only a small numberof overall features that are expected to be useful.3For parameter tuning, we employ the following values.ci may be assigned any of the values 102, 103, 104, 105, or106.
ni may be assigned any of the values 3000, 4000, 5000,or ALL, where ALL means all features are used.
For ti, wesplit the range of classification values bi returns for the test setinto tenths.
ti may take the values 0.0, 0.1, 0.2, .
.
., 1.0, andX, where 0.0 classifies all instances as negative, 0.1 classifiesonly instances bi assigned values in the top tenth of the rangeas positive, and so on, and X is the default threshold, labelingessays as positive instances of ei only if bi returns for them avalue greater than 0.
It was necessary to assign ti in this waybecause the range of values classifiers return varies greatlydepending on which error type we are classifying and whichother parameters we use.
This method gives us reasonablyfine-grained thresholds without having to try an unreasonablylarge number of values for ti.and Dutnais, 1997) which allows us to automat-ically generate a semantic similarity measure be-tween any two words.
We train our random in-dexing model on over 30 million words of the En-glish Gigaword corpus (Parker et al, 2009) usingthe S-Space package (Jurgens and Stevens, 2010).We expect that features based on random index-ing may be particularly useful for the IncompletePrompt Response and Relevance to Prompt errorsbecause they may help us find text related to theprompt even if some of its components have beenrephrased (e.g., an essay may talk about ?jail?rather than ?prison?, which is mentioned in oneof the prompts).
For each essay, we therefore gen-erate four random indexing features, one encodingthe entire essay?s similarity to the prompt, anotherencoding the essay?s highest individual sentence?ssimilarity to the prompt, a third encoding the high-est entire essay similarity to one of the prompt sen-tences, and finally one encoding the highest indi-vidual sentence similarity to an individual promptsentence.
Since random indexing does not pro-vide a straightforward way to measure similar-ity between groups of words such as sentencesor essays, we use Higgins and Burstein?s (2007)method to generate these features.4.3 Novel FeaturesNext, we introduce five types of novel features.Spelling.
One problem we note when examiningthe information gain top-ranked features for theConfusing Phrasing error is that there are very fewcommon confusing phrases that can contribute tothis error.
Errors of this type tend to be unique, andhence are not very useful for error classification(because we are not likely to see the same errorin the training and test sets).
We notice, however,that there are a few misspelled words at the top ofthe list.
This makes sense because a thesis sen-tence containing excessive misspellings may beless clear to the reader.
Even the most commonspelling errors, however, tend to be rare.
Further-more, we ask our annotators to only annotate anerror if it makes the thesis less clear.
The merepresence of an awkward phrase or misspelling isnot enough to justify the Confusing Phrasing label.Hence, we introduce a misspelling feature whosevalue is the number of spelling errors in an essay?smost-misspelled sentence.44We employ SCOWL (http://wordlist.sourceforge.net/) as our dictionary, assuming that a263Keywords.
Improving the prediction of major-ity classes can greatly enhance our system?s over-all performance.
Hence, since we have introducedthe misspelling feature to enhance our system?sperformance on one of the more frequently occur-ring errors (Confusing Phrasing), it makes senseto introduce another type of feature to improveperformance on the other two most frequent er-rors, Incomplete Prompt Response and Relevanceto Prompt.
For this reason, we introduce keywordfeatures.
To use this feature, we first examine eachof the 13 essay prompts, splitting it into its com-ponent pieces.
For our purposes, a component ofa prompt is a prompt substring such that, if an es-say does not address it, it may be assigned the In-complete Prompt Response label.
Then, for eachcomponent, we manually select the most impor-tant (primary) and second most important (sec-ondary) words that it would be good for a writerto use to address the component.
To give an ex-ample, the lemmatized version of the third com-ponent of the second essay in Table 1 is ?it shouldrehabilitate they?.
For this component we selected?rehabilitate?
as a primary keyword and ?society?as a secondary keyword.
To compute one of ourkeyword features, we compute the random index-ing similarity between the essay and each group ofprimary keywords taken from components of theessay?s prompt and assign the feature the lowestof these values.
If this feature has a low value, thatsuggests that the essay may have an IncompletePrompt Response error because the essay proba-bly did not respond to the part of the prompt fromwhich this value came.
To compute another of thekeyword features, we count the numbers of com-bined primary and secondary keywords the essaycontains from each component of its prompt, anddivide each number by the total number of primaryand secondary features for that component.
If thegreatest of these fractions has a low value, that in-dicates the essay?s thesis might not be very Rele-vant to the Prompt.5Aggregated word n-gram features.
Otherways we could measure our system?s performance(such as macro F-score) would consider oursystem?s performance on the less frequent errorsno less important than its performance on theword that does not appear in the dictionary is misspelled.5Space limitations preclude a complete listing of the key-word features.
See our website at http://www.hlt.utdallas.edu/?persingq/ICLE/ for the completelist.most frequent errors.
For this reason, it nowmakes sense for us to introduce a feature tailoredto help our system do better at identifying theleast-frequent error types, Missing Details andWriter Position, each of which occurs in fewerthan 50 essays.
To help with identification ofthese error classes, we introduce aggregatedword n-gram features.
While we mention in theprevious section one of the reasons regular wordn-gram features can be expected to help withthese error classes, one of the problems withregular word n-gram features is that it is fairlyinfrequent for the exact same useful phrase tooccur too frequently.
Additionally, since there arenumerous word n-grams, some infrequent onesmay just by chance only occur in positive trainingset instances, causing the learner to think theyindicate the positive class when they do not.
Toaddress these problems, for each of the five errorclasses ei, we construct two Aggregated wordfeatures Aw+i and Aw?i.
For each essay, Aw+icounts the number of word n-grams we believeindicate that an essay is a positive example of ei,and Aw?i counts the number of word n-gramswe believe indicate an essay is not an example ofei.
Aw+ n-grams for the Missing Details errortend to include phrases like ?there is something?or ?this statement?, while Aw?
ngrams are oftenwords taken directly from an essay?s prompt.N-grams used for Writer Position?s Aw+ tendto suggest the writer is distancing herself fromwhatever statement is being made such as ?everyperson?, but n-grams for this error?s Aw?
featureare difficult to find.
Since Aw+i and Aw?i areso error specific, they are only included in anessay?s feature representation when it is presentedto learner bi.
So while aggregated word n-gramsintroduce ten new features, each learner bi onlysees two of these (Aw+i and Aw?i).We construct the lists of word n-grams that areaggregated for use as the Aw+ and Aw?
fea-ture values in the following way.
For each errorclass ei, we sort the list of all features occurringat least ten times in the training set by informationgain.
A human annotator then manually inspectsthe top thousand features in each of the five listsand sorts each list?s features into three categories.The first category for ei?s list consists of featuresthat indicate an essay may be a positive instance.Each word n-gram from this list that occurs in anessay increases the essay?s Aw+i value by one.264Similarly, any word n-gram sorted into the secondcategory, which consists of features the annotatorthinks indicate a negative instance of ei, increasesthe essay?s Aw?
value by one.
The third categoryjust contains all the features the annotator did notbelieve were useful enough to either class, and wemake no further use of those features.
For most er-ror types, only about 12% of the top 1000 featuresget sorted into one of the first two categories.POS n-grams.
We might further improve oursystem?s performance on the Missing Details er-ror type by introducing a feature that aggregatespart-of-speech (POS) tag n-grams in the same waythat the Aw features aggregate word n-gram fea-tures.
For this reason, we include POS tag 1, 2,3, and 4-grams in the set of features we sort inthe previous paragraph.
For each error ei, we se-lect POS tag n-grams from the top thousand fea-tures of the information gain sorted list to counttoward the Ap+i and Ap?i aggregation features.We believe this kind of feature may help improveperformance on Missing Details because the listof features aggregated to generate the Ap+i fea-ture?s value includes POS n-gram features like CC?
NN ?
(scare quotes).
This feature type may alsohelp with Confusing Phrasing because the list ofPOS tag n-grams our annotator generated for itsAp+i contains useful features like DT NNS VBZVBN (e.g., ?these signals has been?
), which cap-tures noun-verb disagreement.Semantic roles.
Our last aggregated feature isgenerated using FrameNet-style semantic role la-bels obtained using SEMAFOR (Das et al, 2010).For each sentence in our data set, SEMAFORidentifies each semantic frame occurring in thesentence as well as each frame element that par-ticipates in it.
For example, a semantic framemay describe an event that occurs in a sentence,and the event?s frame elements may be the peo-ple or objects that participate in the event.
Fora more concrete example, consider the sentence?They said they do not believe that the prison sys-tem is outdated?.
This sentence contains a State-ment frame because a statement is made in it.
Oneof the frame elements participating in the frame isthe Speaker ?they?.
From this frame, we wouldextract a feature pairing the frame together withits frame element to get the feature ?Statement-Speaker-they?.
This feature indicates that the es-say it occurs in might be a positive instance of theWriter Position error since it tells us the writer isattributing some statement being made to someoneelse.
Hence, this feature along with several oth-ers like ?Awareness-Cognizer-we all?
are usefulwhen constructing the lists of frame features forWriter Position?s aggregated frame features Af+iand Af?i.
Like every other aggregated feature,Af+i and Af?i are generated for every error ei.5 Score PredictionBecause essays containing thesis clarity errorstend to have lower thesis clarity scores than essayswith fewer errors, we believe that thesis clarityscores can be predicted for essays by utilizing thesame features we use for identifying thesis clarityerrors.
Because our score prediction system usesthe same feature types we use for thesis error iden-tification, each essay?s vector space representationremains unchanged.
Only its label changes to oneof the values in Table 2 in order to reflect its thesisclarity score.
To make use of the fact that somepairs of scores are more similar than others (e.g.,an essay with a score of 3.5 is more similar to anessay with a score of 4.0 than it is to one with ascore of 1.0), we cast thesis clarity score predic-tion as a regression rather than classification task.Treating thesis clarity score prediction as a re-gression problem removes our need for a classi-fication threshold parameter like the one we usein the error identification problem, but if we useSVMlight?s regression option, it does not removethe need for tuning a regularization parameter, C ,or a feature selection parameter, n.6 We jointlytune these two parameters to optimize perfor-mance on held-out validation data by performingan exhaustive search in the parameter space.7After we select the features, construct the essayinstances, train a regressor on training set essays,and tune parameters on validation set essays, wecan use the regressor to obtain thesis clarity scoreson test set essays.6Before tuning the feature selection parameter, we have tosort the list of n-gram features occurring the training set.
Toenable the use of information gain as the sorting criterion, wetreat each distinct score as its own class.7The absence of the classification threshold parameter andthe fact that we do not need to train multiple learners, one foreach score, make it feasible for us to do two things.
First, weexplore a wider range of values for the two parameters: weallow C to take any value from 100, 101, 102, 103, 104, 105,106, or 107, and we allow n to take any value from 1000,2000, 3000, 4000, 5000, or ALL.
Second, we exhaustivelyexplore the space defined by these parameters in order to ob-tain an exact solution to the parameter optimization problem.2656 EvaluationIn this section, we evaluate our systems for erroridentification and scoring.
All the results we re-port are obtained via five-fold cross-validation ex-periments.
In each experiment, we use 3/5 of ourlabeled essays for model training, another 1/5 forparameter tuning, and the final 1/5 for testing.6.1 Error IdentificationEvaluation metrics.
To evaluate our thesis clar-ity error type identification system, we computeprecision, recall, micro F-score, and macro F-score, which are calculated as follows.
Let tpi bethe number of test essays correctly labeled as posi-tive by error ei?s binary classifier bi; pi be the totalnumber of test essays labeled as positive by bi; andgi be the total number of test essays that belong toei according to the gold standard.
Then, the preci-sion (Pi), recall (Ri), and F-score (Fi) for bi andthe macro F-score (F?)
of the combined system forone test fold are calculated byPi =tpipi,Ri =tpigi, Fi =2PiRiPi +Ri, F?
=?i Fi5 .However, the macro F-score calculation can beseen as giving too much weight to the less frequenterrors.
To avoid this problem, we also calculatefor each system the micro precision, recall, and F-score (P, R, and F), whereP =?i tpi?i pi,R =?i tpi?i gi,F = 2PRP + R .Since we perform five-fold cross-validation,each value we report for each of these measuresis an average over its values for the five folds.8Results and discussion.
Results on error iden-tification, expressed in terms of precision, recall,micro F-score, and macro F-score are shown inthe first four columns of Table 6.
Our Baselinesystem, which only uses word n-gram and randomindexing features, seems to perform uniformlypoorly across both micro and macro F-scores (Fand F?
; see row 1).
The per-class results9 showthat, since micro F-score places more weight onthe correct identification of the most frequent er-rors, the system?s micro F-score (31.1%) is fairlyclose to the average of the scores obtained on thethree most frequent error classes, CP, IPR, and R,8This averaging explains why the formula for F does notexactly hold in the Table 6 results.9Per-class results are not shown due to space limitations.Error Identification ScoringSystem P R F F?
S1 S2 S31 B 24.8 44.7 31.1 24.0 .658 .517 .4032 Bm 24.2 44.2 31.2 25.3 .654 .515 .4023 Bmk 29.2 44.2 34.9 26.7 .663 .490 .3694 Bmkw 28.5 49.6 35.5 31.4 .651 .484 .3745 Bmkwp 34.2 49.6 40.4 34.6 .671 .483 .3776 Bmkwpf 33.6 54.4 41.4 37.3 .672 .486 .382Table 6: Five-fold cross-validation results for the-sis clarity error identification and scoring.and remains unaffected by very low F-scores onthe two remaining infrequent classes.10When we add the misspelling feature to thebaseline, resulting in the system called Bm(row 2), the micro F-score sees a very small, in-significant improvement.11 What is pleasantly sur-prising, however, is that, even though the mis-spelling features were developed for the Confus-ing Phrasing error type, they actually have moreof a positive impact on Missing Details and WriterPosition, bumping their individual error F-scoresup by about 5 and 3 percent respectively.
This sug-gests that spelling difficulties may be correlatedwith these other essay-writing difficulties, despitetheir apparent unrelatedness.
This effect is strongenough to generate the small, though insignificant,gain in macro F-score shown in the table.When we add keyword features to the system,micro F-score increases significantly by 3.7 points(row 3).
The micro per-class results reveal that,as intended, keyword features improve IncompletePrompt Response and Relevance to Prompt?s F-scores reveals that they do by 6.4 and 9.2 percent-age points respectively.
The macro F-scores revealthis too, though the macro F-score gains are 3.2points and 11.5 points respectively.
The macro F-score of the overall system would likely have im-proved more than shown in the table if the additionof keyword features did not simultaneously reduceMissing Details?s score by several points.While we hoped that adding aggregated wordn-gram features to the system (row 4) would beable to improve performance on Confusing Phras-ing due to the presence of phrases such as ?in uni-versity be?
in the error?s Aw+i list, there turnedout to be few such common phrases in the data set,10Since parameters for optimizing micro F-score andmacro F-score are selected independently, the per-class F-scores associated with micro F-score are different than thoseused for calculating macro F-score.
Hence, when we discussper-class changes influencing micro F-score, we refer to theformer set, and otherwise we refer to the latter set.11All significance tests are paired t-tests, with p < 0.05.266so performance on this class remains mostly un-changed.
This feature type does, however, resultin major improvements to micro and macro perfor-mance on Missing Details and Writer Position, theother two classes this feature was designed to help.Indeed, the micro F-score versions of Missing De-tails and Writer Position improve by 15.3 and 10.8percentage points respectively.
Since these are mi-nority classes, however, the large improvementsresult in only a small, insignificant improvementin the overall system?s micro F-score.
The macroF-score results for these classes, however, improveby 6.5% and 17.6% respectively, giving us a nearly5-point, statistically significant bump in macro F-score after we add this feature.Confusing Phrasing has up to now stubbornlyresisted any improvement, even when we addedfeatures explicitly designed to help our system dobetter on this error type.
When we add aggregatedpart of speech n-gram features on top of the pre-vious system, that changes dramatically.
Addingthese features makes both our system?s F-scoreson Confusing Phrasing shoot up almost 8%, re-sulting in a significant, nearly 4.9% improvementin overall micro F-score and a more modest butinsignificant 3.2% improvement in macro F-score(row 5).
The micro F-score improvement canalso be partly attributed to a four point improve-ment in Incomplete Prompt Response?s micro F-score.
The 13.7% macro F-score improvement ofthe Missing Details error plays a larger role in theoverall system?s macro F-score improvement thanConfusing Phrasing?s improvement, however.The improvement we see in micro F-score whenwe add aggregated frame features (row 6) can beattributed almost solely to improvements in classi-fication of the minority classes.
This is surprisingbecause, as we mentioned before, minority classestend to have a much smaller impact on overallmicro F-score.
Furthermore, the overall microF-score improvement occurrs despite declines inthe performances on two of the majority class er-rors.
Missing Details and Writer Position?s mi-cro F-score performances increase by 19.1% and13.4%.
The latter is surprising only because ofthe magnitude of its improvement, as this featuretype was explicitly intended to improve its perfor-mance.
We did not expect this aggregated featuretype to be especially useful for Missing Details er-ror identification because very few of these typesof features occur in its Af+i list, and there arenone in its Af?i list.
The few that are in the for-mer list, however, occur fairly often and look likefairly good indicators of this error (both the exam-ples ?Event-Event-it?
and ?Categorization-Item-that?
occur in the positive list, and both do seemvague, indicating more details are to be desired).Overall, this system improves our base-line?s macro F-score performance significantly by13.3% and its micro F-score performance signifi-cantly by 10.3%.
As we progressed, adding eachnew feature type to the baseline system, there wasno definite and consistent pattern to how the pre-cisions and recalls changed in order to producethe universal increases in the F-scores that we ob-served for each new system.
Both just tended tojerkily progress upward as new feature types wereadded.
This confirms our intuition about these fea-tures ?
namely that they do not all uniformly im-prove our performance in the same way.
Some aimto improve precision by telling us when essays areless likely to be positive instances of an error class,such as any of the Aw?i, Ap?i, or Af?i features,and others aim to tell us when an essay is morelikely to be a positive instance of an error.6.2 ScoringScoring metrics.
We design three evaluationmetrics to measure the error of our thesis clarityscoring system.
The S1 metric measures the fre-quency at which a system predicts the wrong scoreout of the seven possible scores.
Hence, a systemthat predicts the right score only 25% of the timewould receive an S1 score of 0.75.The S2 metric measures the average distancebetween the system?s score and the actual score.This metric reflects the idea that a system thatestimates scores close to the annotator-assignedscores should be preferred over a system whoseestimations are further off, even if both systemsestimate the correct score at the same frequency.Finally, the S3 metric measures the averagesquare of the distance between a system?s the-sis clarity score estimations and the annotator-assigned scores.
The intuition behind this metricis that not only should we prefer a system whoseestimations are close to the annotator scores, butwe should also prefer one whose estimations arenot too frequently very far away from the annota-tor scores.
These three scores are given by:1N?Aj 6=E?j1, 1NN?i=1|Aj ?
Ej|,1NN?i=1(Aj ?
Ej)2267where Aj , Ej , and E?j are the annotator assigned,system estimated, and rounded system estimatedscores12 respectively for essay j, and N is thenumber of essays.Results and discussion.
Results on scoring areshown in the last three columns of Table 6.
Wesee that the thesis clarity score predicting variationof the Baseline system, which employs as featuresonly word n-grams and random indexing features,predicts the wrong score 65.8% of the time.
Itspredicted score is on average 0.517 points off ofthe actual score, and the average squared distancebetween the predicted and actual scores is 0.403.We observed earlier that a high number of mis-spellings may be positively correlated with oneor more unrelated errors.
Adding the misspellingfeature to the scoring systems, however, onlyyields minor, insignificant improvements to theirperformances under the three scoring metrics.While adding keyword features on top of thissystem does not improve the frequency with whichthe right score is predicted, it both tends to movethe predictions closer to the actual thesis clar-ity score value (as evidenced by the significantimprovement in S2) and ensures that predictedscores will not too often stray too far from theactual value (as evidenced by the significant im-provement in S3).
Overall, the scoring model em-ploying the Bmk feature set performs significantlybetter than the Baseline scoring model with re-spect to two out of three scoring metrics.The only remaining feature type whose additionyields a significant performance improvement isthe aggregated word feature type, which improvessystem Bmk?s S2 score significantly while havingan insignificant impact on the other S metrics.Neither of the remaining aggregative featuresyields any significant improvements in perfor-mance.
This is a surprising finding since, up un-til we introduced aggregated part-of-speech tag n-gram features into our regressor, each additionalfeature that helped with error classification madeat least a small but positive contribution to at leasttwo out of the three S scores.
These aggregativefeatures, which proved to be very powerful whenassigning error labels, are not as useful for thesis12Since our regressor assigns each essay a real value ratherthan an actual valid thesis clarity score, it would be difficultto obtain a reasonable S1 score without rounding the systemestimated score to one of the possible values.
For that rea-son, we round the estimated score to the nearest of the sevenscores the human annotators were permitted to assign (1.0,1.5, 2.0, 2.5, 3.0, 3.5, 4.0) only when calculating S1.S1 (Bmkw) S2 (Bmkwp) S3 (Bmk)Gold .25 .50 .75 .25 .50 .75 .25 .50 .751.0 3.5 3.5 3.5 3.0 3.2 3.5 3.1 3.2 3.31.5 2.5 3.0 3.0 2.8 3.1 3.2 2.6 3.0 3.22.0 3.0 3.0 3.5 3.0 3.2 3.5 3.0 3.1 3.42.5 3.0 3.5 3.5 3.0 3.3 3.6 3.0 3.3 3.53.0 3.0 3.5 3.5 3.1 3.4 3.5 3.1 3.3 3.53.5 3.5 3.5 4.0 3.2 3.4 3.6 3.2 3.4 3.54.0 3.5 3.5 4.0 3.4 3.6 3.8 3.4 3.5 3.7Table 7: Regressor scores for top three systems.clarity scoring.To more closely examine the behavior of thebest scoring systems, in Table 7 we chart the dis-tributions of scores they predict for each gold stan-dard score.
As an example of how to read this ta-ble, consider the number 2.8 appearing in row 1.5in the .25 column of the S2 (Bmkwp) region.
Thismeans that 25% of the time, when system Bmkwp(which obtains the best S2 score) is presented witha test essay having a gold standard score of 1.5,it predicts that the essay has a score less than orequal to 2.8 for the S2 metric.From this table, we see that each of the best sys-tems has a strong bias toward predicting more fre-quent scores as there are no numbers less than 3.0in the 50% columns, and about 82.8% of all essayshave gold standard scores of 3.0 or above.
Never-theless, no system relies entirely on bias, as evi-denced by the fact that each column in the tablehas a tendency for its scores to ascend as the goldstandard score increases, implying that the sys-tems have some success at predicting lower scoresfor essays with lower gold standard scores.Finally, we note that the difference in errorweighting between the S2 and S3 scoring metricsappears to be having its desired effect, as there is astrong tendency for each entry in the S3 subtableto be less than or equal to its corresponding entryin the S2 subtable due to the greater penalty theS3 metric imposes for predictions that are very faraway from the gold standard scores.7 ConclusionWe examined the problem of modeling thesis clar-ity errors and scoring in student essays.
In additionto developing these models, we proposed novelfeatures for use in our thesis clarity error modeland employed these features, each of which wasexplicitly designed for one or more of the errortypes, to train our scoring model.
We make ourthesis clarity annotations publicly available in or-der to stimulate further research on this task.268AcknowledgmentsWe thank the three anonymous reviewers for theirdetailed and insightful comments on an earlierdraft of the paper.
This work was supported inpart by NSF Grants IIS-1147644 and IIS-1219142.Any opinions, findings, conclusions or recommen-dations expressed in this paper are those of the au-thors and do not necessarily reflect the views or of-ficial policies, either expressed or implied, of NSF.ReferencesYigal Attali and Jill Burstein.
2006.
Automated essayscoring with E-rater v.2.0.
Journal of Technology,Learning, and Assessment, 4(3).Jean Carletta.
1996.
Assessing agreement on classi-fication tasks: The Kappa statistic.
ComputationalLinguistics, 22(2):249?254.Dipanjan Das, Nathan Schneider, Desai Chen, andNoah A. Smith.
2010.
Probabilistic frame-semanticparsing.
In Human Language Technologies: The2010 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, pages 948?956.Scott Deerwester, Susan T. Dumais, George W. Fur-nas, Thomas K. Landauer, and Richard Harshman.1990.
Indexing by Latent Semantic Analysis.
Jour-nal of the American Society for Information Science,41(6):391?407.Sylviane Granger, Estelle Dagneaux, Fanny Meunier,and Magali Paquot.
2009. International Corpus ofLearner English (Version 2).
Presses universitairesde Louvain.Derrick Higgins and Jill Burstein.
2007.
Sentence sim-ilarity measures for essay coherence.
In Proceed-ings of the 7th International Workshop on Computa-tional Semantics.Derrick Higgins, Jill Burstein, Daniel Marcu, and Clau-dia Gentile.
2004.
Evaluating multiple aspectsof coherence in student essays.
In Human Lan-guage Technologies: The 2004 Annual Conferenceof the North American Chapter of the Associationfor Computational Linguistics, pages 185?192.Thorsten Joachims.
1999.
Making large-scale SVMlearning practical.
In B. Scho?lkopf, C. Burges, andA.
Smola, editors, Advances in Kernel Methods -Support Vector Learning, Chapter 11, pages 169?184.
MIT Press, Cambridge, MA.David Jurgens and Keith Stevens.
2010.
The S-Spacepackage: An open source package for word spacemodels.
In Proceedings of the ACL 2010 SystemDemonstrations, pages 30?35.Pentti Kanerva, Jan Kristoferson, and Anders Holst.2000.
Random indexing of text samples for LatentSemantic Analysis.
In Proceedings the 22nd AnnualConference of the Cognitive Science Society, pages103?106.Scott Kirkpatrick, C. D. Gelatt, and Mario P. Vecchi.1983.
Optimization by simulated annealing.
Sci-ence, 220(4598):671?680.Thomas K Landauer and Susan T. Dumais.
1997.
Asolution to Plato?s problem: The Latent SemanticAnalysis theory of acquisition, induction, and rep-resentation of knowledge.
Psychological review,pages 211?240.Thomas K. Landauer, Darrell Laham, and Peter W.Foltz.
2003.
Automated scoring and annotation ofessays with the Intelligent Essay AssessorTM.
In Au-tomated Essay Scoring: A Cross-Disciplinary Per-spective, pages 87?112.
Lawrence Erlbaum Asso-ciates, Inc., Mahwah, NJ.Eleni Miltsakaki and Karen Kukich.
2004.
Evaluationof text coherence for electronic essay scoring sys-tems.
Natural Language Engineering, 10(1):25?55.Robert Parker, David Graff, Junbo Kong, Ke Chen, andKazuaki Maeda.
2009.
English Gigaword FourthEdition.
Linguistic Data Consortium, Philadelphia.Isaac Persing, Alan Davis, and Vincent Ng.
2010.Modeling organization in student essays.
In Pro-ceedings of the 2010 Conference on Empirical Meth-ods in Natural Language Processing, pages 229?239.Magnus Sahlgren.
2005.
An introduction to randomindexing.
In Proceedings of the Methods and Appli-cations of Semantic Indexing Workshop at the 7th In-ternational Conference on Terminology and Knowl-edge Engineering.Mark D. Shermis and Jill C. Burstein.
2003.
Au-tomated Essay Scoring: A Cross-Disciplinary Per-spective.
Lawrence Erlbaum Associates, Inc., Mah-wah, NJ.Mark D. Shermis, Jill Burstein, Derrick Higgins, andKlaus Zechner.
2010.
Automated essay scoring:Writing assessment and instruction.
In InternationalEncyclopedia of Education (3rd edition).
Elsevier,Oxford, UK.Yiming Yang and Jan O. Pedersen.
1997.
A compara-tive study on feature selection in text categorization.In Proceedings of the 14th International Conferenceon Machine Learning, pages 412?420.269
