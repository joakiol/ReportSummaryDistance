Description of the NCU Chinese Word Segmentation and Part-of-SpeechTagging for SIGHAN Bakeoff 2007Yu-Chieh Wu Jie-Chi Yang Yue-Shi LeeDept.
of Computer Science andInformation EngineeringGraduate Institute of Net-work Learning TechnologyDept.
of Computer Science and In-formation EngineeringNational Central University National Central University Ming Chuan UniversityTaoyuan, Taiwan Taoyuan, Taiwan Taoyuan, Taiwanbcbb@db.csie.ncu.edu.tw yang@cl.ncu.edu.tw leeys@mcu.edu.twAbstractIn Chinese, most of the language process-ing starts from word segmentation andpart-of-speech (POS) tagging.
These twosteps tokenize the word from a sequenceof characters and predict the syntactic la-bels for each segmented word.
In this pa-per, we present two distinct sequentialtagging models for the above two tasks.The first word segmentation model wasbasically similar to previous work whichmade use of conditional random fields(CRF) and set of predefined dictionariesto recognize word boundaries.
Second, werevise and modify support vector ma-chine-based chunking model to label thePOS tag in the tagging task.
Our methodin the WS task achieves moderately rankamong all participants, while in the POStagging task, it reaches very competitiveresults.1 IntroductionWith the rapid expansion of online text articlessuch as blog, web news, and research/technicalreports, there is an increasing demand for text min-ing and management.
Different from western-likelanguages, handling oriented languages is far moredifficult since there is no explicit boundary symbolto indicate what a word is in the text.
However themost important preliminary step for natural lan-guage processing is to tokenize words and separatethem from the word sequence.
In Chinese, theword tokenization is also known as word segmen-tation or Chinese word tokenization.
The problemof the Chinese word segmentation is very criticalfor most Chinese linguistics because the error seg-mented words deeply affects the downstream pur-pose, like POS tagging and parsing.
In additiontokenizing the unknown words is also an unavoid-able problem.To support the above targets, it is necessary todetect the boundaries between words in a givensentence.
In tradition, the Chinese word segmenta-tion technologies can be categorized into threetypes, (heuristic) rule-based, machine learning, andhybrid.
Among them, the machine learning-basedtechniques showed excellent performance in manyrecent research studies (Peng et al, 2004; Zhou etal., 2005; Gao et al, 2004).
This method treats theword segmentation problem as a sequence of wordclassification.
The classifier online assigns either?boundary?
or ?non-boundary?
label to each wordby learning from the large annotated corpora.
Ma-chine learning-based word segmentation method isquite similar to the word sequence inference tech-niques, such as part-of-speech (POS) tagging(Clark et al, 2003; Gimenez and Marquez, 2003),phrase chunking (Lee and Wu, 2007) and worddependency parsing (Wu et al, 2006, 2007).In this paper, we present two prototype systemsfor Chinese word segmentation and POS tagging161Sixth SIGHAN Workshop on Chinese Language Processingtasks.
The former was basically an extension ofprevious literatures (Ng and Low, 2004; Zhou et al,2006), while the latter incorporates the unknownword and known word tagging into one step.
Thetwo frameworks were designed based on two vari-ant machine learning algorithms, namely CRF andSVM.
In our pilot study, the SVM showed betterperformance than CRF in the POS tagging task.
Toidentify unknown words, we also encode the suffixand prefix features to represent the training exam-ple.
The strategy was showed very effective forimproving both known and unknown word chunk-ing on both Chinese and English phrase chunking(Lee and Wu, 2007).
In this year, the presentedword segmentation method achieved moderaterank among all participants.
Meanwhile, the pro-posed SVM-based POS tagging model reachedvery competitive accuracy in most POS tasks.
Forexample, our method yields second best result onthe CTB POS tagging track.The rest of this paper is organized as follows.Section 2 describes employed machine learningalgorithms, CRF and SVM.
In section 3, we pre-sent the proposed word segmentation and POStagging framework which used for the SIGHAN-bake-off this year.
Experimental result and evalua-tions are reported in section 4.
Finally, in section 5,we draw conclusion and future remarks.2 Classification Algorithms2.1 Conditional Random FieldsConditional random field (CRF) was an extensionof both Maximum Entropy Model (MEMs) andHidden Markov Models (HMMs) that was firstlyintroduced by (Lafferty et al, 2001).
CRF definedconditional probability distribution P(Y|X) of givensequence given input sentence where Y is the?class label?
sequence and X denotes as the obser-vation word sequence.A CRF on (X,Y) is specified by a feature vectorF of local context and the corresponding featureweight ?.
The F can be treated as the combinationof state transition and observation value in conven-tional HMM.
To determine the optimal label se-quence, the CRF uses the following equation toestimate the most probability.
),(maxarg),|(maxarg xyFxyPyyy??
==The most probable label sequence y can be effi-ciently extracted via the Viterbi algorithm.
How-ever, training a CRF is equivalent to estimate theparameter set?for the feature set.
In this paper, wedirectly use CRF++ (Kudo and Matsumoto, 2003)which included the quasi-Newton L-BFGS 1method (Nocedal and Wright, 1999) to iterativeupdate the parameters.2.2 Support Vector MachinesAssume we have a set of training examples,}1 ,1{ ,  ),,(),...,,(),,( 2211 ?+???
iDinn yxyxyxyxwhere xi is a feature vector in D-dimension spaceof the i-th example, and yi is the label of xi eitherpositive or negative.
The training of SVMs in-volves minimizing the following object function(primal form, soft-margin (Vapnik, 1995)):?=?+?=niii yxWLossCWWW1),(21)( :minimize ?
(1)The loss function indicates the loss of misclassi-fication risk.
Usually, the hinge-loss is used (Vap-nik, 1995; Keerthi and DeCoste, 2005).
The factorC in (1) is a parameter that allows one to trade offtraining error and margin size.
To classify a giventesting example X, the decision rule takes the fol-lowing form:))),((()( ?
?+=SVsxiiiibxXKysignXy ?
(2)?i represents the weight of training example xiwhich lies on the hyperplane, and b denotes as abias threshold.
SVs means the support vectors andobviously has the non-zero weights of ?i.
)()(),( ii xXxXK ??
?=  is a pre-defined kernel func-tion that might transform the original feature spacefrom D?
to 'D?
(usually D<<D?).
In the linearkernel form, the ),( ixXK  simply compute the dotproducts of the two variables.
By introducing ofthe polynomial kernel, we re-write the decisionfunction of (1) as:1 http://www-unix.mcs.anl.gov/tao/162Sixth SIGHAN Workshop on Chinese Language Processing))),(1((())),((()(????++=+=SVsxdiiiSVsxiiiiibxXdotysignbxXKysignXy??
(3)wheredii xXdotxXK )),(1(),( +=                                     (4)and d is the polynomial kernel degree.In many NLP problems, the training and testingexamples are represented as bits of binary vectors.In this section, we focus on this case.
Later, wepresent a general form without considering thisconstraint.3 System DescriptionIn this section, we first describe the problem set-tings for the word segmentation problems.
In sec-tion 3.2, the proposed POS tagging framework isthen presented.3.1 Word Sequence ClassificationSimilar to English text chunking (Ramshaw andMarcus, 1995; Lee and Wu, 2007), the word se-quence classification model aims to classify eachword via encoding its context features.By encoding with BIES (LMR tagging scheme)or IOB2 style, both WS and NER problems can beviewed as a sequence of word classification.
Dur-ing testing, we seek to find the optimal word typefor each Chinese character.
These types stronglyreflect the actual word boundaries for Chinesewords or named entity phrases.As reported by (Zhou et al, 2006), the use ofricher tag set can effectively enhance the perform-ance.
They extend the tag of ?Begin of word?
into?second-begin?
and ?third-begin?
to capture morecharacter types.
However, there are some ambigu-ous problem to the 3-character Chinese words and4-character Chinese words.
For example, to encode?????
with his extended tag set, the first char-acter can be encoded as ?B?
tag.
But for the secondcharacter, we can use ?second-begin?
or ?I?
tag torepresent the middle of word.In order to make the extension clearer, in thispaper, we explicitly extend the B tag and E tagwith ?after begin?
(BI), and ?before end?
(IE) tags.Table 1 lists the difference between the traditionalBIES and the proposed E-BIES encodings methods.Table 2 illustrates an example of how the BIESand E-BIES encode with different number of char-acters.Table 1: BIES and E-BIES encoding strategiesBIES E-BIESBegin of a word B BAfter begin of a word - BIMiddle of a word I IBefore end of a word - IEEnd of a word E ESingle word S STable 2: An example of the BIES and E-BIESencoding strategiesN-character word BIES E-BIES?
S S??
B,E B,E???
B,I,E B,BI,E????
B,I,I,E B,BI,IE,E?????
B,I,I,I,E B,BI,I,IE,ETo effect classify each character, in this paper,we adopted most feature types to train the CRF(Kudo and Matsumoto, 2004).
Table 3 lists theadopted feature templates.
The dictionary flag isvery similar to previous literature (Ng and Low,2004) while we adding up English full-characterinto our dictionary.Table 3: Feature template used for Chineseword segmentation taskFeature Type Context Position DescriptionUnigram C-2,C-1,C0,C1,C2Chinese character featureNearing Bi-gram(C-2,C-1)(C-1,C0)(C1,C0)(C1,C2)Bi-character featureJump Bigram (C-1,C1)Non-continuous character featureDictionaryFlag C0Date, Digital, English letter or punctuationDictionaryFlag N-gram (C-1,C0,C1)N-gram of the dictionary flags3.2 Feature Codification for Chinese POSTaggingAs reported by (Ng, and Low, 2004; Clark et al,2003), the pure POS tagging performance is nomore than 92% in the CTB data and no more than163Sixth SIGHAN Workshop on Chinese Language Processing96.8% in English WSJ.
The learner used in his lit-erature is maximum entropy model.
However themain limitation of his POS tagging strategy is thatthe unknown word classification problem was notresolved.To circumvent this vita, we simply extend theidea of SVM-based chunker (Lee and Wu, 2007)and develop our own SVM-based POS tagger.
Al-though CRF showed excellent performance inword segmentation task, in English POS tagging,the SVM is more effective than CRF.
Also in ourclosed experiment, we had tried transformation-based error-driven learner (TBL), CRF, and SVMclassifiers.
The pilot experiment showed that theSVM outperformed the other two learners andachieved almost 94% accuracy in the CTB data.Meanwhile TBL reached the worst result than theother two classifiers (~88%).Handling unknown word is very important toPOS tagging problem.
As pointed out by (Lee andWu, 2007; Gimenez, and Marquez, 2003), the in-troduction of suffix features can effectively help toguess the unknown words for tagging and chunk-ing.
Different from (Gimenez and Marquez, 2003),we did not derive data for unknown word guessing.Instead, we directly encode all suffix- and prefix-features for each training instance.
In trainingphase, the rich feature types are able to disambigu-ate not only the unknown word guessing, but alsoimprove the known word classification.
As re-ported by (Lee and Wu, 2007), the strategy didimprove the English and Chinese chunking per-formance for both known and unknown words.Table 4: Feature patterns used for Chinese POStagging taskFeatureTypeContext Position DescriptionUnigram W-2,W-1,W0,W1,W2Chinese word featureNearingBigram(W-2,W-1)(W-1,W0)(W1,W0)(W1,W2)Bi-word featureJump Bi-gram(W-2,W0)(W-1,W1)(W2,W0)(W1,W3)Non-continuous character featurePossibletags W0Possible POS tag in the training dataPrefix 3/2/1characters W-1,W0,W1Pre-characters ofwordSuffix 3/2/1characters W-1,W0,W1Post-characters ofwordThe used feature set of our POS tagger is listed inTable 4.
In this paper, we did not conduct the fea-ture selection experiment for each tagging corpus,instead a unified feature set was used due to thetime line.
We trust our POS tagger could be furtherimproved by removing or adding new feature set.The learner used in this paper (SVM) is mainlydeveloped by our own (Wu et al, 2007).
The costfactor C is simply set as 0.15 for all languages.Furthermore, to remove rare words, we eliminatethe words which appear no more than twice in thetraining data.4 Evaluations and Experimental Result4.1 Dataset and EvaluationsIn this year, we mainly focus on the close track forWS and POS tagging tracks.
The CTB, SXU, andNCC corpora were used for evaluated the pre-sented word segmentation method, while all thereleased POS tagging data were tested by ourSVM-based tagger, included CityU, CKIP, CTB,NCC, and PKU.
Both settings of the two modelswere set as previously noted.
The evaluation of thetwo tasks was mainly measured by the three met-rics, namely, recall, precision, and f-measure.However, the evaluation process for the POS tag-ging track is somewhat different from WS.
In WS,participant should reform the testing data into sen-tence level whereas in the POS tagging track theword had been correctly segmented.
Thus themeasurement of the POS tagging track is mainlyaccuracy-based (correct or incorrect).4.2 Experimental Result on Word Segmenta-tion TaskIn this year, we only select the following three datato perform our method for the word segmentationtask.
They are CTB, NCC, and SXU where theNCC and SXU are fresh in this year.
Table5 showsthe experimental results of our model in the closeWS track with except for CKIP and CityU corpora.Table 5: Official results on the word segmenta-tion task (closed-task)Recall Precision F-measureCTB 0.9471 0.9500 0.9486NCC 0.9236 0.9269 0.9252SXU 0.9505 0.9515 0.9510164Sixth SIGHAN Workshop on Chinese Language ProcessingAs shown above, our method in the CTB datashowed 10th best out of 26 submissions.
In theNCC and SXU datasets, our method achieved19/26 and 18/30 rank.
In overall, the presented ex-tend-BIES scheme seems to work well on the CTBdata and results in middle rank in comparison tothe other participants.4.3 Experimental Result on Part-of-SpeechTagging TaskIn the second experiment, we focus on the de-signed POS tagging model.
To measure the effec-tiveness, we apply our method to all the releaseddataset, i.e., CityU, CKIP, CTB, NCC, and PKU.Table 6 lists the experimental result of our methodin this task.Similar to WS task, our method is still very effec-tive to CTB dataset.
It turns out our methodachieved second best in the CTB, while for theother corpora, it achieved 4th best among all theparticipants.
We also found that our method wasvery close to the top 1 score about 1.3% (CKIP) to0.09%.
For the NCC, and PKU, our method wasworse than the best system in 0.8% in overall accu-racy.
We conclude that by selecting suitable fea-tures and cost factor C to SVM, our method can befurther improved.
We left the work as future direc-tion.Table 6: Official results on the part-of-speechtagging task (closed-task)Riv Roov Rmt AccuracyCityU 0.9326 0.4322 0.8707 0.8865CKIP 0.9504 0.5631 0.9065 0.9160CTB 0.9554 0.7135 0.9183 0.9401NCC 0.9658 0.5822 0.9116 0.9456PKU 0.9591 0.5832 0.9173 0.93685 Conclusions and Future WorkChinese word segmentation is the most importantinfrastructure for many Chinese linguistic tech-nologies such as text categorization and informa-tion retrieval.
In this paper, we present simpleChinese word segmentation and part-of-speechtagging models based on the conventional se-quence classification technique.
We treat the twotasks as two different learning framework and ap-plying CRF and SVM as separated learners.
With-out any prior knowledge and rules, such a simpletechnique shows satisfactory results on both wordsegmentation and part-of-speech tagging tasks.
InPOS tagging task, our model shows very competi-tive results which merely spend few hours to train.To reach state-of-the-art, our method still needs tofurther select features and parameter tunings.
In thefuture, one of the main directions is to extend thismodel toward full unsupervised learning fromlarge un-annotated text.
Mining from large unla-beled data have been showed benefits to improvethe original accuracy.
Thus, not only the stochasticfeature analysis, but also adjust the learner fromunlabeled data are important future remarks.ReferencesClark, S., Curran, J. R., and Osborne, M. 2003.Bootstrapping POS Taggers Using Unlabeleddata.
In Proceedings of the 7th Conference onNatural Language Learning (CoNLL), pages 49-55.Gao, J., Wu, A., Li, M., Huang, C. N., Li, H., Xia,X., and Qin, H. 2004.
Adaptive Chinese wordsegmentation.
In Proceedings the 41st AnnualMeeting of the Association for ComputationalLinguistics, pp.
21-26.Gim?nez, J., and M?rquez, L. 2003.
Fast and accu-rate Part-of-Speech tagging: the SVM approachrevisited.
In Proceedings of the InternationalConference on Recent Advances in NaturalLanguage Processing, pages 158-165.Keerthi, S., and DeCoste, D. 2005.
A ModifiedFinite Newton Method for Fast Solution ofLarge Scale Linear SVMs.
Journal of MachineLearning Research, 6: 341-361.Kudo, T., and Matsumoto, Y.
2004.
AppliyingConditional Random Fields to Japanese Mor-phological Analysis.
In Proceedings of the Em-pirical.
Methods in Natural LanguageProcessing (EMNLP), pages 230-237.Lafferty, J., McCallum, A., and Pereira, F. 2001.Conditional Random Field: Probabilistic modelsfor segmenting and labeling sequence data.
InProceedings of the International Conference onMachine Learning.Ramshaw, L. A., and Marcus, M. P. 1995.
TextChunking Using Transformation-based Learning.165Sixth SIGHAN Workshop on Chinese Language ProcessingIn Proceedings of the 3rd Workshop on VeryLarge Corpora, pages 82-94.Lee, Y. S. and Wu, Y. C. 2007.
A Robust Multi-lingual Portable Phrase Chunking System.
Ex-pert Systems with Applications, 33(3): 1-26.Ng, H. T., and Low, J. K. 2004.
Chinese Part-of-Speech Tagging: One-at-a-time or All-at-once?Word-based or Character-based?
In Proceedingsof the Empirical.
Methods in Natural LanguageProcessing (EMNLP).Nocedal, J., and Wright, S. 1999.
Numerical opti-mization.
Springer.Peng, F., Feng, F., and McCallum, A.
2004.
Chi-nese segmentation and new word detection us-ing conditional random fields.
In Porceedings ofthe Computational Linguistics, pp.
562-568.Shi, W. 2005.
Chinese Word Segmentation BasedOn Direct Maximum Entropy Model.
In Pro-ceedings of the Fourth SIGHAN Workshop onChinese Language Processing.Vapnik, V. N. 1995.
The Nature of StatisticalLearning Theory.
Springer.Wu, Y. C., Yang, J. C., and Lee, Y. S. 2007.
AnApproximate Approach for Training PolynomialKernel SVMs in Linear Time.
In Proceedings ofthe Annual Meeting of the Association forComputational Linguistics, pages 65-68.Wu, Y. C., Lee, Y. S., and Yang, J. C. 2007.
Multi-lingual Deterministic Dependency ParsingFramework using Modified Finite NewtonMethod Support Vector Machines.
In Proceed-ings of the Joint Conferences on EmpiricalMethods on Natural Language Processing andConference on Natural Language Learning(EMNLP-CoNLL), pages 1175-1181.Wu, Y. C., Lee, Y. S., and Yang, J. C. 2006.
TheExploration of Deterministic and Efficient De-pendency Parsing.
In Proceedings of the 10thConference on Natural Language Learning(CoNLL).Zhou, H., Huang, C. N., and Li, M. 2006.
An Im-proved Word Segmentation System with Condi-tional Random Fields.
In Proceedings of theSIGHAN Workshop on Chinese LanguageProcessing Workshop, pages 162-165.Zhou, J., Dai, X., Ni, R., Chen, J.
2005.
.A HybridApproach to Chinese Word Segmentationaround CRFs.
In Proceedings of the FourthSIGHAN Workshop on Chinese LanguageProcessing.166Sixth SIGHAN Workshop on Chinese Language Processing
