Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 872?881,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsSemantic parsing of speech using grammars learned with weak supervisionJudith GaspersSemantic Computing GroupCITECBielefeld UniversityPhilipp CimianoSemantic Computing GroupCITECBielefeld University{jgaspers|cimiano|bwrede}@cit-ec.uni-bielefeld.deBritta WredeApplied InformaticsFaculty of TechnologyBielefeld UniversityAbstractSemantic grammars can be applied both as alanguage model for a speech recognizer andfor semantic parsing, e.g.
in order to mapthe output of a speech recognizer into formalmeaning representations.
Semantic speechrecognition grammars are, however, typicallycreated manually or learned in a supervisedfashion, requiring extensive manual effort inboth cases.
Aiming to reduce this effort, in thispaper we investigate the induction of semanticspeech recognition grammars under weak su-pervision.
We present empirical results, indi-cating that the induced grammars support se-mantic parsing of speech with a rather lowloss in performance when compared to pars-ing of input without recognition errors.
Fur-ther, we show improved parsing performancecompared to applying n-gram models as lan-guage models and demonstrate how our se-mantic speech recognition grammars can beenhanced by weights based on occurrence fre-quencies, yielding an improvement in parsingperformance over applying unweighted gram-mars.1 MotivationSemantic parsers map natural language utterances(NL) into formal meaning representations (MR), andare applied for both parsing of textual input andin Spoken Language Understanding (SLU).
In data-driven SLU research, typically pipeline-based sys-tems are applied in which first an automatic speechrecognizer (ASR) is applied to transcribe speech in-put, and subsequently a semantic parser is appliedto map the transcriptions into some semantic form(Deoras et al, 2013).
Such systems typically usedifferent models for recognition and understanding.Since ASR yields recognition errors, parsing per-formance can degrade rapidly compared to pars-ing performance on written text.
While the per-formance of ASR and parsing components are of-ten optimized independently of each other, in par-ticular in case of the ASR to minimize recognitionserrors, research has shown that ASR transcriptionswith a lower error rate can in fact yield worse un-derstanding performance (Wang et al, 2003; Bayerand Riccardi, 2012) and that joint approaches torecognition and understanding can yield improvedperformance (Wang and Acero, 2006b; Deoras etal., 2013).
In particular, Wang and Acero (2006b)have shown that applying the same grammar forspeech recognition and understanding can yield im-proved understanding performance compared to ap-plying a standard n-gram model with the ASR,since dependencies between acoustics and seman-tics can be captured.
Their grammars are, however,learned in a supervised setting.
In fact, while se-mantic grammars are often applied for speech recog-nition and/or understanding, they are often createdmanually or ?
as mentioned previously ?
learnedfrom data containing semantic annotations, whichare time-consuming to produce.In the field of Natural Language Processing (NLP),the development of semantic parsers has receivedconsiderable attention.
While some researchershave considered fully supervised settings (Wong andMooney, 2006; Zettlemoyer and Collins, 2007),requiring accurate and complete semantic annota-872tions, others have developed weakly supervised ap-proaches exploiting ambiguous representations ofthe context in which an utterance is produced in-stead of accurate and complete annotations (Chenet al, 2010; B?orschinger et al, 2011; Chen andMooney, 2008).
In this line, in this paper we ex-plore how an approach that induces semantic parsersin the form of a (semantic) grammar from ambigu-ous training data can be applied to acquire a lan-guage model (LM) for speech recognition as well asa semantic parser for the understanding task at thesame time.
Making use of a semantic parser as alanguage model for speech recognition also comeswith the advantage that no separate language modelmust be trained.
In our experiments, we compareperformance of the induced grammars to the per-formance of different language models, in particu-lar n-gram models, and we investigate the impact ofenhancing induced semantic grammars with weightsbased on the training data.
We present empirical re-sults showing that it is possible to induce semanticgrammars with weak supervision that can be appliedsuccessfully both as an LM for a speech recognizerand for semantic parsing.
We show that with re-spect to parsing performance, our joint approach inwhich the same grammar is used for parsing and asan LM yields a higher F1(84.46%) compared to anapproach in which a standard n-gram based modelis used as an LM (78.36%).
In addition, our resultsindicate that enhancing speech recognition grammarrules with weights based on occurrence frequenciescan yield improved performance over unweightedgrammars (84.46% vs 82.37% for weighted vs un-weighted grammars, respectively).2 Background & related workIn principle, two different types of language mod-els can be applied with an ASR: stochastic LMs ?typically n-gram models ?
and speech recognitiongrammars.
While n-gram models estimate probabil-ities of word sequences, speech recognition gram-mars explicitly specify rules defining which wordsand patterns a user may utter.
Further, seman-tic information can be directly included within therules.
Thus, when applied with an ASR, spokenutterances can be directly transformed into a corre-sponding semantic representation without producinga sequence of words as intermediate step.
This ap-proach is typically taken when building commercialsystems (Wang et al, 2011).
Such grammars are,however, typically created manually, which is time-consuming and error-prone.
Hence, data-drivenapproaches to automatic grammar induction havebeen explored (Wang and Acero, 2006b; Wang andAcero, 2005; Wang and Acero, 2003).
However,they often rely on fully supervised settings, requir-ing training data which is annotated at the utterance-or word level, which is costly and time-consuming toproduce.
In contrast, aiming to reduce the requiredmanual effort, in this paper we explore the utility ofweak supervision in the form of ambiguous contextinformation for the induction of grammars applica-ble for both speech recognition and understanding.The utility of this kind of weak supervision has beenexplored previously in the field of semantic parsing(Chen et al, 2010; B?orschinger et al, 2011; Chenand Mooney, 2008), and unsupervised approaches tosemantic parsing have been proposed as well (Poonand Domingos, 2009; Goldwasser et al, 2011).While such approaches may be applied as parsingcomponents for SLU systems ?
notice though thatthe SLU task differs from parsing of written textin that recognition errors and phenomena of spo-ken language must be handled, and that not all SLUmodels can be applied as an LM (Wang et al, 2011)?
we are not aware of work aiming to transformthese parsers into speech recognition grammars orinvestigating their performance with respect to dif-ferent LMs applied with an ASR.Semantic parsers applied in pipeline-based SLU sys-tems are in general usually learned in a supervisedfashion.
Other than semantic grammar-based ap-proaches, probabilistic models and machine learn-ing techniques have been applied in SLU for con-ceptual tagging due to their robustness to noise,e.g.
Conditional Random Fields (Lafferty et al,2001) have been applied (e.g.
Wang and Acero(2006a; Dinarelli et al (2012)); He and Young(2005) present an approach based on Hidden MarkosModels.
However, evaluations have shown that evenin case of applying machine learning techniques orprobabilistic models, semantic parsing of ASR tran-scriptions is affected by much more errors com-pared to parsing of correct transcriptions (De Mori,2011).
In order to reduce annotation costs, work has,873for instance, focused on providing annotation tools(Wang and Acero, 2006b; Wang and Acero, 2005),exploring supervised learning in combination withactive learning (Wu et al, 2010) and gaining addi-tional training data, for instance, from the Web us-ing queries generated from a (small) existing gram-mar (Klasinas et al, 2013).
These approaches, how-ever, still assume manual effort and may be some-what complementary to the one investigated here.Further, data-driven SLU parsers are often based onrather local features, e.g.
n-grams, while we exploretemplate-based grammars which can capture long-distance linguistic dependencies.Several approaches have addressed unsupervised(Solan et al, 2005; van Zaanen and Adriaans, 2001)and semi-supervised (Wong and Meng, 2001; Siuand Meng, 1999; Meng and Siu, 2002) inductionof grammars, where the latter may comprise man-ual post-processing of automatically induced rules.In particular, in order to be applicable as an SLUmodel, semantic information must be added manu-ally, since only syntactic structures can be inducedautomatically in this case.While in data-driven SLU research typicallypipeline-based systems are applied, a few joint ap-proaches have been proposed (Deoras et al, 2013;Wang and Acero, 2006b; Bayer and Riccardi, 2012).Specifically, the work presented here is most sim-ilar to the approach presented by Wang and Acero(2006b).
In particular, we also attempt to learngrammars applicable for both speech recognitionand understanding.
However, Wang and Acero(2006b) explore a supervised setting based on word-level annotations for slots and induce rather localrules, i.e.
based on preambles and postambles forslots, while we explore a template-based approach,capturing long-distance linguistic dependencies.3 MethodologyIn this paper, we explore the induction of semanticgrammars under weak supervision provided in theform of ambiguous representations of the semanticcontext as explored in the NLP field of SemanticParsing (Chen et al, 2010).
In particular, the train-ing data comprises of a set of textual utterances cou-pled with symbolic context information from whichwe induce semantic parsers and derive different LMsfor application with an ASR.
LMs are then appliedto transcribe speech data, and the resulting transcrip-tions are in turn mapped into meaning representa-tions by the learned semantic parsers.
In the follow-ing, we will first describe the input data and learningscenario and subsequently the semantic parsing ap-proach as well as the creation of language models.3.1 Learning scenario and input dataOur experiments were performed on the RoboCupsoccer corpus (Chen and Mooney, 2008), which is astandard dataset used for the evaluation of seman-tic parsing algorithms taking written natural lan-guage utterances as input.
The corpus comprisesfour RoboCup games.
Game events are repre-sented by predicate logic formulas, which repre-sent the ambiguous contextual representations fromwhich semantic parsers are trained in a weakly su-pervised fashion.
The games were commented byhumans, yielding examples for written natural lan-guage utterances (NL).
In the corpus, each NL ispaired with a set of possible meaning representa-tions mri?
MR, each expressing a game action, andNL corresponds to at most one them.
For example,pass(purple10,purple7) represents an mr for a pass-ing event which might be commented as ?purple10kicks to purple7?.
However, there is no direct corre-spondence between the NL comments and their cor-responding mrs; thus, these correspondences have tobe learned.The corpus also contains a gold standard compris-ing NLs annotated with their correct mrs. Sev-eral semantic parsers have been evaluated using thisdataset by applying the evaluation schema intro-duced by Chen et al (2010).
The authors performed4-fold cross-validation on the four games.
Trainingwas done on the ambiguous training data, while thegold standard for a fourth game was used for testing.Results were presented by means of the F1score.Precision and recall were computed as the percent-age of mrs produced by the system that were correctand the percentage of mrs that the system producedcorrectly, respectively.
A parse was considered ascorrect if it matched the gold standard exactly (Chenet al, 2010).
Recently, this task has been extended toconsider speech data, both in learning and applyinga parser.
In particular, the approach of Gaspers andCimiano (2014) relied on transcriptions made by a874task-independent phoneme recognizer as input.
Forthis purpose, NL comments contained in the datasetwere read by a speaker.
By contrast, in this paperwe explore how grammars for speech recognitionand understanding can be built from textual inputin a weakly supervised setting, and subsequently beapplied for recognition and parsing of speech input.Hence, we explore a 4-fold cross-validation scenarioin which for each fold learning is performed usingthe written ambiguous training data for three games,while the spoken gold standard of the forth gameis used for testing, i.e.
for performing both speechrecognition and subsequent parsing of the resultingASR transcriptions; spoken data are the same as inGaspers and Cimiano (2014).
For application withthe ASR we normalized training data which mainlycomprised lowercasing and replacement of numbersin player names, e.g.
?pink4?
?
?pink four?.
Somestatistics for the normalized dataset are presented inTable 1.1Table 1: Dataset statistics.Total number of comments 1,872Comments having correct mr 1,539Average number of events per comment 2.5Maximum number of events per comment 12SD in number of events per comment 1.8Mean utterance length 7.39# Types 355# Tokens 13,8383.2 The applied semantic parsing algorithmFor semantic parser induction we applied the al-gorithm presented in Gaspers and Cimiano (2014),which is mainly designed to work with the outputof a phoneme recognizer.
The algorithm is alsoapplicable to textual input and has been shown toachieve state-of-the-art performance on written in-put (cf.
Gaspers and Cimiano (2014)).
The in-duced parser is represented in the form of a lexiconand an inventory containing syntactic constructionsand thus well-suited to be transformed into a rule-based speech recognition grammar.
The learned lex-1Numbers for mean utterance length and number of tokensand types are computed only for comments included in thetraining dataset.
Regarding the total number of comments weuse one more per game than Chen et al (2010) in line withB?orschinger et al (2011).icon comprises lexical units, i.e.
words or short se-quences of words, along with their mapping to se-mantic referents, e.g.
?pink goalie?
?
pink1.
Eachsyntactic construction consists of a syntactic pat-tern, e.g.
?X1kicks to X2?, along with an asso-ciated semantic frame, e.g.
pass(ARG1, ARG2),and a mapping which maps slots in the syntactic pat-tern to argument slots in the semantic frame, e.g.X1?
ARG1, X2?
ARG2.
Slots in syntacticpatterns represent positions in which a lexical unitfrom the parser?s lexicon can be inserted.
For in-stance, in the previous pattern ?pink goalie?
can beinserted at position X1or X2.
When applied to writ-ten text, parser induction is performed by applyingthe following learning steps:1. acquisition of an initial lexicon2.
computation of alignments between NLs andambiguous context representations, and3.
estimation of co-occurrence frequencies at dif-ferent levels.This work flow is illustrated in Fig 1.Figure 1: The algorithm?s work flow.In step 1, initial lexical knowledge is learned bycomputing co-occurrence frequencies between allbi- and unigrams appearing in the NL data and allsemantic referents appearing in the MR data.
In step2, this knowledge is used to compute alignments foreach example between its NL and all mri?
MR ob-served with it, i.e.
lexical knowledge is used to seg-ment NL such that all semantic referents observedin an mr are expressed by individual sequences, andhypotheses concerning a mapping between NL and875semantics are created.
For instance, given an inputexample(1)NL: purple eight kicks to purple sevenmr1: playmode(play on)mr2: pass(purple8, purple7)mr3: pass(purple2, purple5)the following alignment might be created:(2)NL X1kicks to X2mrpass(ARG1, ARG2)Mapping: X1?
ARG1, X2?
ARG2nl?
refpurple eight?
purple8purple seven?
purple7That is, assuming that the algorithm has learnedin step 1 that ?purple eight?
and ?purple seven?
re-fer to purpl8 and purple7, respectively, it may usethis knowledge to hypothesize that NL is an instan-tiation of a pattern ?X1kicks to X2?, which in turnrefers to the predicate pass(ARG1, ARG2) with amapping X1?
ARG1, X2?
ARG2.Alignments are rated, and for each example onlythose having maximal scores are used for parser in-duction.
By this, lexical knowledge is used to pre-disambiguate the training data.
Given the align-ments induced in step 3, a parser is estimated bycomputing co-occurrence frequencies at differentlevels.
In particular, association scores are computedat three different levels, i.e.1.
nl ?
ref : between all lexical units, e.g.
?pur-ple eight?, and semantic referents, e.g.
purple8,appearing in alignments,2.
NL ?
mr: between all syntactic patterns,e.g.
?X1kicks to X2?, and semantic frames,e.g.
pass(ARG1, ARG2), appearing in align-ments, and3.
mapping: between all slots in a syntactic pat-tern, e.g.
X1, and argument slots, e.g.
ARG1,specific for each pattern and semantic frame.Then, the parser?s lexicon consists of rules of theform nl ?
ref , while the syntactic constructionshave the form NL ?mr, each coupled with its indi-vidual mapping.Parsing is performed by searching for an appropri-ate syntactic construction given an input NL, andthe arguments matching the elements at the slots inthe syntactic pattern are inserted into the appropri-ate argument slots in the associated semantic frame.Approximate matching can be applied during pars-ing of NLs for which no pattern can be found other-wise.
In this paper, we always perform approximatematching by searching for a matching syntactic pat-tern with a Levenshtein distance of 1 if no match-ing pattern can be found directly, which allows usto parse utterances containing a recognition error.Even though ?
of course ?
more than one recognitionerror might be contained in a given utterance, we donot use greater distance values because this wouldlikely yield parsing errors, as utterances are rathershort and most of the words are important for detect-ing the meaning; leaving out too many words will ingeneral increase the likelihood of matching wrongpatterns, thus yielding spurious interpretations.
Us-ing the algorithm, for each fold of the RoboCup dataset we created a semantic parser using the writtentraining data of three games.3.3 Creation of language modelsBased on the written training data we created dif-ferent LMs.
In particular, we created rule-basedrecognition grammars using the algorithm and fur-ther LMs, such as trigram models, for comparison.3.3.1 Recognition grammarsWe built semantic speech recognition grammarsgiven a semantic parser by transforming all ruleswith an occurrence greater than one into JSpeechGrammar Format (JSGF)2.
The resulting grammarsconsisted of rules representing the parser?s inven-tory of syntactic constructions as well as its lexicon.In case of the inventory of syntactic constructions,alternative expansions of learned syntactic patternswere defined, and in case of the lexicon, alterna-tive expansions of learned lexical units were defined.In particular, with respect to the lexicon we defineda rule <ref> which comprises the learned lexicalunits.
With respect to syntactic constructions we de-fined a rule <utterance> which comprises the pat-terns.
Further, syntactic slots in patterns were re-placed by <ref>, allowing lexical units to appearat those positions.
In grammar creation, we also in-vestigated the influence of occurrence frequencies of2http://www.w3.org/TR/jsgf/876syntactic patterns and lexical units to enhance gram-matical rules with weights.
In particular, we createdboth weighted and unweighted grammars.
When us-ing weights, rules were weighted by using occur-rence frequencies, i.e.
the frequency which was ob-served for pattern or lexical unit as aligned by thealgorithm during training.
Hence, weights for pat-terns and lexical units aligned less frequently in thetraining data were smaller, indicating that they wereless likely to be spoken.
An example illustrating asubset of two (weighted) rules is illustrated in Fig.2.Notice that resulting JSGF grammars do not ex-plicitly contain semantic information, but their in-duction was driven by semantic information.
Thisis the case because a mapping to semantics was notneeded during recognition as we explore a two-stageapproach where parsing is performed after recog-nition, allowing the inclusion of further LMs dur-ing recognition.
However, because both parsing andunderstanding are performed using the same gram-mar ?
where semantic information is ignored by theLM ?
it would also be possible to induce a semanticgrammar that directly maps ASR output into seman-tic representations.3.3.2 Baseline language modelsWe computed different language models for com-parison; these were mainly stochastic LMs.
In par-ticular, we created standard trigram language mod-els from the written training data without makinguse of concurrent perceptual context information us-ing SRILM (Stolcke, 2002).
Since the RoboCupcorpus is rather small and n-gram models are typi-cally learned from large amounts of data, in additionwe interpolated the trigram models trained solely onthe in-domain RoboCup corpus each with a largebackground language model trained on a broadcastnews corpus, i.e.
the HUB4 dataset (Fiscus et al,1998).
We also experimented with class-based mod-els, but automatic induction of classes in an unsu-pervised fashion did not appear promising and werefrained from manually creating classes since thefocus of this paper is on the automatic creation ofASR resources without requiring extensive manualeffort.
However, an interesting experiment would beto utilize the semantic classes induced by our algo-rithm in order to create class-based language mod-els.Moreover, in order to evaluate the utility of ambigu-ous perceptual context for speech recognition gram-mar induction, as a fully unsupervised grammar-based baseline, we induced syntactic grammars re-lying on the ADIOS algorithm (Solan et al, 2005).Notice, however, that it is not common to applygrammars learned in an unsupervised fashion di-rectly for SLU.
In particular, with respect to seman-tic parsing, automatically induced grammars are typ-ically post-processed manually, which we refrainedfrom doing, since the focus of this paper is on theautomatic creation of speech recognition and under-standing components.4 Experiments & ResultsWe evaluated the word error rate (WER) as well asparsing accuracy for different language models andcombinations thereof.
In particular, in case of apply-ing recognition grammars we applied these also incombination with an n-gram back off LM.
In partic-ular, the n-gram model was applied in case of utter-ances which were rejected by the recognizer as outof grammar (OOG), as these might still be parsedsubsequently by applying approximate matching.Notice, however, that for our experiments we didnot apply both LMs at a time but combined the out-put of two recognizers for further processing.
No-tice further that most speech recognizers can onlybe applied using either a recognition grammar or ann-gram model at a time, but one can assume thattwo recognizers might be configured to run in paral-lel.
As mentioned previously, we performed 4-foldcross-validation on the four RoboCup games.
Foreach fold, learning semantic parsers and creation oflanguage models was performed using the ambigu-ous written training data for three games and thespoken gold standard for the forth game for testing.In the following, we will discuss results for applyingour induced grammars as an LM compared to usingstandard trigrams models (solely trained on the in-domain data) as a baseline, since these yielded thebest results.
In particular, we do not discuss the re-sults achieved by the grammars induced in a purelysyntactic manner as they performed worse than se-mantic grammars in all experiments, and we do notdiscuss the experiments for the interpolated/adapted877Figure 2: A subset of weighted speech recognition grammar rulespublic <utterance> = /6/ <ref> again passes to <ref> | /199/ <ref> kicks to <ref> | ...<ref> = /15/ pink goalie | /132/ pink nine | /10/ pink one | ...trigram models as they performed worse than thein-domain trigram models with the exception of avery slight improvement when applied as a back offmodel for SLU.34.1 Speech recognitionSpeech recognition was performed using different(combinations of) LMs individually; lexicon andacoustic models were the same in all cases.4Speechrecognition results with respect to the word error rateaveraged over all folds are presented in Table 2.Table 2: speech recognition resultsApplied language model(s) WER (%)Semantic grammar w/o weights 15.55Semantic grammar w/o weights12.63+ trigram back offSemantic grammar inc. weights 17.15Semantic grammar inc. weights10.88+ trigram back offTrigram (baseline) 7.1As can be seen, with a rather low error rate of7.1%, applying trigram language models yields thebest results.
While in case of applying semanti-cally motivated recognition grammars the WER in-creases, it must be noted that in cases in whichno back off models were applied this is to someextent due to OOG utterances (as these yield sev-eral deletions compared to the reference data).
Yet,the OOG-rate is rather low, i.e.
averaged over allfolds 8.6% and 4.1% when using grammars withand without weights, respectively.
However, even in3The results were: interpolated/adapted LM: WER: 13.43%,F1: 71.22%, semantic grammar + interpolated/adapted LMbackoff: WER: 13.85, F1: 84.6%, syntactic recognition gram-mar: WER: 18.98%, F1: 70.86%, syntactic recognition gram-mar + trigram back off: WER: 13.98%, F1: 71.27%.4We applied Sphinx4 (Walker et al, 2004) using lexiconand acoustic models trained on the HUB4 dataset (Fiscus etal., 1998), which contains broadcast news speech matching ourRoboCup data with respect to acoustics in that in both casesread speech is addressed; these resources are available online.We added phonetic transcriptions for out of vocabulary (OOV)to the vocabulary; only two were OOV along with some typos.cases where OOG utterances are recognized by ap-plying trigram language models, the WER is highercompared to applying trigram language models only.Notably, these results were not consistent acrossfolds.
For two folds, the WER actually decreasedwhen combining a semantically motivated grammarincluding weights with a trigram language modelcompared to applying the trigram language modelonly, thus indicating that combining semanticallymotivated grammars learned with weak supervisionwith trigram models can also yield improved recog-nition performance over applying trigram modelsonly in some cases.4.2 Semantic parsingFor each fold, ASR transcriptions were parsed usingthe semantic parser learned on the training data forthat fold.
For comparison, as an upper baseline wecomputed parsing performance on normalized goldstandard data, since typically performance degrades?
and often to a large extent ?
when a semanticparser is applied to ASR transcriptions of speech;recall that the applied algorithm achieves state-of-the-art performance on the dataset.5Results are pre-sented in Table 3.Table 3: Semantic parsing results on written text and onspeech transcribed using different language modelsWritten text (reference)F1Prec.
RecallNormalized text 87.26 94.28 81.42SpeechApplied language model(s) F1Prec.
RecallSemantic grammar inc. weights 84.18 88.7 80.18Semantic grammar inc. weights84.46 87.53 81.64+ trigram back offSemantic grammar w/o weights 82.24 84.83 79.84Semantic grammar w/o weights82.37 84.67 80.21+ trigram back offTrigram (baseline) 78.36 90.34 69.45While comparison with a manually created gold standardgrammar would be interesting as well, a manually created gram-mar for the utilized dataset is unfortunately not available.878The results reveal that in case of applying trigramLMs F1degrades about 9% absolute compared toparsing written text (reference), yielding 78.36% inF1, even though the WER is rather low with a valueof 7.1%.
Thus, the trigram seems to ?destroy?
se-mantically meaningful sequences while restoring se-quences that contain no meaning.
By contrast, incase of applying a semantically motivated recogni-tion grammar including weights, performance im-proves by 6% absolute over the trigram model, eventhough the WER is higher in this case.
More-over, including weights in the recognition grammarsyields improved performance compared to using un-weighted grammars.Notably, the decrease in performance in case of ap-plying a weighted semantically motivated recogni-tion grammar (+ trigram back off) compared to per-formance on the reference data is mainly due to adecrease in precision.
Here it must be noted thatthe high values in F1are achieved without perform-ing any optimization of (recognition) parameters.
Inthe performed experiments, the probability for OOGutterances was rather low, and thus utterances werematched incorrectly by the ASR which were actuallynot covered by the grammar, yielding both recogni-tion and subsequent parsing errors.
However, theseparameters can be tuned, likely increasing precisionand F1even further (and probably also the WER).Applying a back off trigram model yields only littleimprovement in parsing performance, although thismay to some extent be due to not tuning recognitionparameters.
That is, if the ASR would be tuned to re-ject more OOG utterances correctly, these utterancesmight instead be recognized by a trigram model andprobably parsed correctly by applying approximatematching.5 DiscussionWhen applying trigram models, even with a ratherlow error rate of 7.1%, semantic parsing perfor-mance degraded about 9% absolute in F1.
Here itmust be noted that due to the evaluation schema asingle recognition error can yield a completely in-correct parse.
Recall that evaluation is performedon the basis of fully correct mrs, i.e.
all referentsand the predicate must be determined correctly inorder to yield a correct parse.
For instance, if anyof the words ?purple?, ?pink?, ?two?
or ?five?
isdeleted or substituted in an utterance ?purple twopasses to pink five?, one of the referents may notbe identified (correctly).
Similarly, deleting or sub-stituting ?passes?
may yield an incorrect predicateor no parse at all.
Hence, parsing performance candegrade rapidly even on ASR transcriptions contain-ing only few recognition errors.The results show that, in line with previous research(Wang et al, 2003; Bayer and Riccardi, 2012), alower WER may not yield better understanding re-sults, i.e.
in our case parsing performance is not di-rectly dependent on the WER but rather on the typeof errors made.
In particular, with respect to seman-tic parsing it is important that words carrying im-portant meaning are recognized correctly.
For in-stance, a spoken utterance ?pink nine passes the ballto pink seven?
in which ?seven?
is incorrectly recog-nized as ?eleven?
likely yields a parsing error whilea recognition error which substitutes ?backward?
by?forward?
may not prevent correct parsing.
This isthe case because ?forward?
and ?backward?
do notcarry any semantics in the data set at hand, whilecorrect identification of numbers is in most cases es-sential for detecting the correct semantic referents.Applying the semantically motivated grammars mayhave been beneficial in recognizing the semantic ref-erents correctly because the system can explicitlylearn them and their appearances in certain patternsin contrast to the trigram model.
In particular, if anutterance ?pink nine passes the ball to pink seven?appears during recognition and ?pink seven?
has notbeen observed in the context of the preceding wordsduring training, then the n-gram model would assigna low probability, likely leading to a recognition er-ror such as ?pink eleven?.
By contrast, in case ofsemantic grammars the system can learn that the ut-terance is an instantiation of a pattern ?player passesthe ball to player?
and that all players can appear atthe contained slots, thus making the appearance ofthe example utterance more likely.
Notably, seman-tic classes such as player can in principle also bemodeled in stochastic language models, in particu-lar by applying class-based models, or in syntacti-cally motivated grammars.
Recall that we also ex-perimented with syntactically motivated grammarsand with class-based models and that neither classeswhich were auto-induced on the raw text data nor879syntactically motivated grammars yielded promisingresults.
Thus, using weak supervision in the formof perceptual context information appears to be ben-eficial for detecting semantic classes compared toworking with raw text.
An interesting point for fu-ture work might be to explore whether using seman-tic groupings induced by our algorithm in a class-based model yields reasonable results, in particularwhen applied as a back-off model in combinationwith a semantically motivated recognition grammar.Further, we have also investigated weighting rulesfor semantically meaningful lexical units, i.e.
inthis example the probability for the occurrence ofplayers like ?pink nine?
and ?pink seven?
can be in-creased according to their occurrence frequencies,thus making recognizing them more likely.
Our re-sults indicate that by weighting semantically mean-ingful sequences, performance is improved, possi-bly because more words carrying semantics are rec-ognized correctly, even though words carrying nosemantics like ?forward?
or ?backward?
might beconfused, which, however, may not prevent correctparsing.
In general, while in SLU research mainlycascading systems are explored, in line with previ-ous work (Wang et al, 2003; Bayer and Riccardi,2012), our results indicate that joint models yieldimproved parsing performance, even though wordrecognition performance may decrease.
Yet, our re-sults indicate that a combination of a semantic gram-mar with a standard trigram model during speechrecognition can also reduce the word error rate insome cases compared to applying the trigram modelonly.
Furthermore, the results emphasize that cap-turing semantic information in a language model ap-plied during ASR is beneficial for subsequent se-mantic parsing, since the ASR can be tuned to-wards recognizing words carrying semantics moreprecisely, which is important with respect to parsingperformance.6 ConclusionThis work investigated the induction of semanticgrammars applicable for both speech recognitionand understanding in a weakly supervised setting,i.e.
using ambiguous context information.
In doingso, we compared parsing the output of speech recog-nizers applied with different language models.
Ourresults indicate that by applying the same semanti-cally motivated grammar learned with weak super-vision for both recognition and parsing, speech canbe parsed into formal meaning representations witha rather low loss in performance compared to pars-ing of data without recognition errors, that is, tex-tual data or manual transcriptions of speech.
Animprovement in parsing performance was obtainedover a cascading approach in which a standard n-gram model is used, and we have shown how learn-ing weights for grammatical rules applied in speechrecognition can yield improved subsequent parsingresults compared to applying unweighted grammars.AcknowledgmentsThis work has been funded by the DFG within theCRC 673 and the Cognitive Interaction TechnologyExcellence Center.ReferencesAli Orkan Bayer and Giuseppe Riccardi.
2012.
Jointlanguage models for automatic speech recognitionand understanding.
In Proceedings of the IEEE/ACLWorkshop on Spoken Language Technology (SLT).Benjamin B?orschinger, Bevan K. Jones, and Mark John-son.
2011.
Reducing grounded learning tasks to gram-matical inference.
In Proceedings of the 2011 Confer-ence on Empirical Methods in Natural Language Pro-cessing (EMNLP).David L. Chen and Raymond J. Mooney.
2008.
Learningto sportscast: A test of grounded language acquisition.In Proceedings of the 25th International Conferenceon Machine Learning (ICML).David L. Chen, Joohyun Kim, and Raymond J. Mooney.2010.
Training a multilingual sportscaster: Using per-ceptual context to learn language.
Journal of ArtificialIntelligence Research, 37(1):397?435.Renato De Mori, 2011.
History of Knowledge and Pro-cesses for Spoken Language Understanding, pages11?40.
John Wiley & Sons.Anoop Deoras, Gokhan Tur, Ruhi Sarikaya, and DilekHakkani-Tur.
2013.
Joint discriminative decoding ofwords and semantic tags for spoken language under-standing.
IEEE Transactions on Audio, Speech andLanguage Processing.Marco Dinarelli, Alessandro Moschitti, and GiuseppeRiccardi.
2012.
Discriminative reranking for spokenlanguage understanding.
IEEE Transactions on AudioSpeech and Language Processing, 20(2):526?539.880Jonathan Fiscus, John Garofolo, Mark Przybocki,William Fisher, and David Pallett.
1998.
1997 en-glish broadcast news speech (hub4).
Linguistic DataConsortium.Judith Gaspers and Philipp Cimiano.
2014.
Learning asemantic parser from spoken utterances.
In Proceed-ings of the IEEE International Conference on Acous-tics, Speech and Signal Processing (ICASSP).Dan Goldwasser, Roi Reichart, James Clarke, and DanRoth.
2011.
Confidence driven unsupervised semanticparsing.
In Proceedings of the Annual Meeting of theAssociation for Computational Linguistics (ACL).Yulan He and Steve Young.
2005.
Semantic processingusing the hidden vector state model.
Computer Speechand Language, 19:85?106.Ioannis Klasinas, Alexandros Potamianos, Elias Iosif,Spiros Georgiladakis, and Gianluca Mameli.
2013.Web data harvesting for speech understanding gram-mar induction.
In Proceedings INTERSPEECH.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In Proceedingsof the International Conference on Machine Learning(ICML).Helen M. Meng and Kai-Chung Siu.
2002.
Semi-automatic acquisition of semantic structures for un-derstanding domain-specific natural language queries.IEEE Trans.
Knowl.
Data Eng., 14(1):172?181.Hoifung Poon and Pedro Domingos.
2009.
Unsuper-vised semantic parsing.
In Proceedings of the Confer-ence on Empirical Methods in Natural Language Pro-cessing (EMNLP).Kai-chung Siu and Helen M. Meng.
1999.
Semi-automatic acquisition of domain-specific semanticstructures.
In Proceedings EUROSPEECH.Zach Solan, David Horn, Eytan Ruppin, and ShimonEdelman.
2005.
Unsupervised learning of natural lan-guages.
Proceedings of the National Academy of Sci-ences, 102(33):11629?11634.Andreas Stolcke.
2002.
SRILM - An Extensible Lan-guage Modeling Toolkit.
In Proceedings of the Inter-national Conference on Spoken Language Processing,pages 901?904.Menno van Zaanen and Pieter Adriaans.
2001.Alignment-Based Learning versus EMILE: A Com-parison.
In Proceedings of the Belgian-Dutch Confer-ence on Artificial Intelligence.Willie Walker, Paul Lamere, Philip Kwok, Bhiksha Raj,Rita Singh, Evandro Gouvea, Peter Wolf, and JoeWoelfel.
2004.
Sphinx-4: A flexible open sourceframework for speech recognition.
Technical report,Sun Microsystems.Ye-Yi Wang and Alex Acero.
2003.
Combination ofCFG and N-gram Modeling in Semantic GrammarLearning.
In Proceedings EUROSPEECH.Ye-Yi Wang and Alex Acero.
2005.
Sgstudio: Rapidsemantic grammar development for spoken languageunderstanding.
In Proceedings of the European Con-ference on Speech Communication and Technology.Ye-Yi Wang and Alex Acero.
2006a.
Discriminativemodels for spoken language understanding.
In Pro-ceedings of the International Conference on SpokenLanguage Processing.Ye-Yi Wang and Alex Acero.
2006b.
Rapid developmentof spoken language understanding grammars.
SpeechCommunication, 48 (3-4):390?416.Ye-Yi Wang, Alex Acero, and Ciprian Chelba.
2003.
Isword error rate a good indicator for spoken languageunderstanding accuracy.
In IEEE Workshop on Auto-matic Speech Recognition and Understanding.Ye-Yi Wang, Li Deng, and Alex Acero, 2011.
SemanticFrame-based Spoken Language Understanding, pages41?92.
John Wiley & Sons.Chin-Chung Wong and Helen Meng.
2001.
Improve-ments on a semi-automatic grammar induction frame-work.
In Proceedings of the IEEE Automatic SpeechRecognition and Understanding Workshop.Yuk Wah Wong and Raymond J. Mooney.
2006.
Learn-ing for semantic parsing with statistical machine trans-lation.
In Proceedings of the Human Language Tech-nology Conference of the North American Chapter ofthe Association for Computational Linguistics.Wei-Lin Wu, Ru-Zhan Lu, Jian-Yong Duan, Hui Liu,Feng Gao, and Yu-Quan Chen.
2010.
Spoken lan-guage understanding using weakly supervised learn-ing.
Computer, 24:358?382.Luke S. Zettlemoyer and Michael Collins.
2007.
On-line Learning of Relaxed CCG Grammars for Pars-ing to Logical Form.
In Empirical Methods in Natu-ral Language Processing and Computational NaturalLanguage Learning (EMNLP).881
