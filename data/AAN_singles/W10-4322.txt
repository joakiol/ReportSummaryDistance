Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 107?115,The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational LinguisticsSparse Approximate Dynamic Programming for Dialog ManagementSenthilkumar Chandramohan, Matthieu Geist, Olivier PietquinSUPELEC - IMS Research Group, Metz - France.
{senthilkumar.chandramohan, matthieu.geist, olivier.pietquin}@supelec.frAbstractSpoken dialogue management strategy op-timization by means of ReinforcementLearning (RL) is now part of the state ofthe art.
Yet, there is still a clear mis-match between the complexity implied bythe required naturalness of dialogue sys-tems and the inability of standard RL al-gorithms to scale up.
Another issue is thesparsity of the data available for training inthe dialogue domain which can not ensureconvergence of most of RL algorithms.In this paper, we propose to combine asample-efficient generalization frameworkfor RL with a feature selection algorithmfor the learning of an optimal spoken dia-logue management strategy.1 IntroductionOptimization of dialogue management strategiesby means of Reinforcement Learning (RL) (Sut-ton and Barto, 1998) is now part of the state ofthe art in the research area of Spoken DialogueSystems (SDS) (Levin and Pieraccini, 1998; Singhet al, 1999; Pietquin and Dutoit, 2006; Williamsand Young, 2007).
It consists in casting the dia-logue management problem into the Markov Deci-sion Processes (MDP) paradigm (Bellman, 1957)and solving the associated optimization problem.Yet, there is still a clear mismatch between thecomplexity implied by the required naturalness ofthe dialogue systems and the inability of standardRL algorithms to scale up.
Another issue is thesparsity of the data available for training in thedialogue domain because collecting and annotat-ing data is very time consuming.
Yet, RL algo-rithms are very data demanding and low amountsof data can not ensure convergence of most ofRL algorithms.
This latter problem has been ex-tensively studied in the recent years and is ad-dressed by simulating new dialogues thanks toa statistical model of human-machine interaction(Pietquin, 2005) and user modeling (Eckert et al,1997; Pietquin and Dutoit, 2006; Schatzmann etal., 2006).
However, this results in a variability ofthe learned strategy depending on the user model-ing method (Schatzmann et al, 2005) and no com-mon agreement exists on the best user model.The former problem, that is dealing with com-plex dialogue systems within the RL framework,has received much less attention.
Although someworks can be found in the SDS literature it is farfrom taking advantage of the large amount of ma-chine learning literature devoted to this problem.In (Williams and Young, 2005), the authors reducethe complexity of the problem (which is actually aPartially Observable MDP) by automatically con-densing the continuous state space in a so-calledsummary space.
This results in a clustering of thestate space in a discrete set of states on which stan-dard RL algorithms are applied.
In (Henderson etal., 2008), the authors use a linear approximationscheme and apply the SARSA(?)
algorithm (Sut-ton and Barto, 1998) in a batch setting (from dataand not from interactions or simulations).
This al-gorithm was actually designed for online learningand is known to converge very slowly.
It there-fore requires a lot of data and especially in largestate spaces.
Moreover, the choice of the featuresused for the linear approximation is particularlysimple since features are the state variables them-selves.
The approximated function can thereforenot be more complex than an hyper-plane in thestate variables space.
This drawback is shared bythe approach of (Li et al, 2009) where a batch al-gorithm (Least Square Policy Iteration or LSPI) iscombined to a pruning method to only keep themost meaningful features.
In addition the com-plexity of LSPI is O(p3).In the machine learning community, this issueis actually addressed by function approximationaccompanied with dimensionality reduction.
The107data sparsity problem is also widely addressed inthis literature, and sample-efficiency is one maintrend of research in this field.
In this paper, wepropose to combine a sample-efficient batch RLalgorithm (namely the Fitted Value Iteration (FVI)algorithm) with a feature selection method in anovel manner and to apply this original combi-nation to the learning of an optimal spoken dia-logue strategy.
Although the algorithm uses a lin-ear combination of features (or basis functions),these features are much richer in their ability ofrepresenting complex functions.The ultimate goal of this research is to providea way of learning optimal dialogue policies for alarge set of situations from a small and fixed set ofannotated data in a tractable way.The rest of this paper is structured as follows.Section 2 gives a formal insight of MDP andbriefly reminds the casting of the dialogue prob-lem into the MDP framework.
Section 3.2 pro-vides a description of approximate Dynamic Pro-gramming along with LSPI and FVI algorithms.Section 4 provides an overview on how LSPI andFVI can be combined with a feature selectionscheme (which is employed to learn the represen-tation of theQ-function from the dialogue corpus).Our experimental set-up, results and a comparisonwith state-of-the-art methods are presented in Sec-tion 5.
Eventually, Section 6 concludes.2 Markov Decision ProcessesThe MDP (Puterman, 1994) framework is usedto describe and solve sequential decision mak-ing problems or equivalently optimal control prob-lems in the case of stochastic dynamic systems.AnMDP is formally a tuple {S,A, P,R, ?
}whereS is the (finite) state space, A the (finite) actionspace, P ?
P(S)S?A the family of Markoviantransition probabilities1, R ?
RS?A?S the rewardfunction and ?
the discounting factor (0 ?
?
?
1).According to this formalism, a system to be con-trolled steps from state to state (s ?
S) accordingto transition probabilities P as a consequence ofthe controller?s actions (a ?
A).
After each tran-sition, the system generates an immediate reward(r) according to its reward function R. How thesystem is controlled is modeled with a so-calledpolicy pi ?
AS mapping states to actions.
Thequality of a policy is quantified by the so-calledvalue function which maps each state to the ex-1Notation f ?
AB is equivalent to f : B ?
Apected discounted cumulative reward given thatthe agent starts in this state and follows the policypi: V pi(s) = E[?
?i=0 ?iri|s0 = s, pi].
An optimalpolicy pi?
maximizes this function for each state:pi?
= argmaxpi Vpi.
Suppose that we are given theoptimal value function V ?
(that is the value func-tion associated to an optimal policy), deriving theassociated policy would require to know the transi-tion probabilities P .
Yet, this is usually unknown.This is why the state-action value (or Q-) functionis introduced.
It adds a degree of freedom on thechoice of the first action:Qpi(s, a) = E[?
?i=0?iri|s0 = s, a0 = a, pi] (1)The optimal policy is noted pi?
and the relatedQ-function Q?
(s, a).
An action-selection strategythat is greedy according to this function (pi(s) =argmaxa Q?
(s, a)) provides an optimal policy.2.1 Dialogue as an MDPThe casting of the spoken dialogue managementproblem into the MDP framework (MDP-SDS)comes from the equivalence of this problem toa sequential decision making problem.
Indeed,the role of the dialogue manager (or the decisionmaker) is to select and perform dialogue acts (ac-tions in the MDP paradigm) when it reaches agiven dialogue turn (state in the MDP paradigm)while interacting with a human user.
There canbe several types of system dialogue acts.
Forexample, in the case of a restaurant informationsystem, possible acts are request(cuisine type),provide(address), confirm(price range), close etc.The dialogue state is usually represented effi-ciently by the Information State paradigm (Lars-son and Traum, 2000).
In this paradigm, the di-alogue state contains a compact representation ofthe history of the dialogue in terms of system actsand its subsequent user responses (user acts).
Itsummarizes the information exchanged betweenthe user and the system until the considered stateis reached.A dialogue management strategy is thus a map-ping between dialogue states and dialogue acts.Still following the MDP?s definitions, the optimalstrategy is the one that maximizes some cumula-tive function of rewards collected all along the in-teraction.
A common choice for the immediatereward is the contribution of each action to usersatisfaction (Singh et al, 1999).
This subjective108reward is usually approximated by a linear com-bination of objective measures like dialogue dura-tion, number of ASR errors, task completion etc.
(Walker et al, 1997).3 Solving MDPs3.1 Dynamic ProgrammingDynamic programming (DP) (Bellman, 1957)aims at computing the optimal policy pi?
if thetransition probabilities and the reward function areknown.First, the policy iteration algorithm computesthe optimal policy in an iterative way.
The ini-tial policy is arbitrary set to pi0.
At iteration k, thepolicy pik?1 is evaluated, that is the associated Q-function Qpik?1(s, a) is computed.
To do so, theMarkovian property of the transition probabilitiesis used to rewrite Equation (1) as :Qpi(s, a) = Es?|s,a[R(s, a, s?)
+ ?Qpi(s?, pi(s?
))]= T piQpi(s, a) (2)This is the so-called Bellman evaluation equa-tion and T pi is the Bellman evaluation opera-tor.
T pi is linear and therefore this defines a lin-ear system that can be solved by standard meth-ods or by an iterative method using the factthat Qpi is the unique fixed-point of the Bell-man evaluation operator (T pi being a contrac-tion): Q?pii = TpiQ?pii?1, ?Q?pi0 limi??
Q?pii =Qpi.
Then the policy is improved, that ispik is greedy respectively to Qpik?1 : pik(s) =argmaxa?A Qpik?1(s, a).
Evaluation and im-provement steps are iterated until convergence ofpik to pi?
(which can be demonstrated to happen ina finite number of iterations when pik = pik?1).The value iteration algorithm aims at estimat-ing directly the optimal state-action value functionQ?
which is the solution of the Bellman optimalityequation (or equivalently the unique fixed-point ofthe Bellman optimality operator T ?):Q?
(s, a) = Es?|s,a[R(s, a, s?)
+ ?
maxb?AQ?
(s?, b)]= T ?Q?
(s, a) (3)The T ?
operator is not linear, therefore comput-ingQ?
via standard system-solving methods is notpossible.
However, it can be shown that T ?
isalso a contraction (Puterman, 1994).
Therefore,according to Banach fixed-point theorem, Q?
canbe estimated using the following iterative way:Q?
?i = T?Q?
?i?1, ?Q?
?0 limi??Q?
?i = Q?
(4)However, the convergence takes an infinite num-ber of iterations.
Practically speaking, iterationsare stopped when some criterion is met, classi-cally a small difference between two iterations:?Q?
?i ?
Q??i?1?
< ?.
The estimated optimal pol-icy (which is what we are ultimately interested in)is greedy respectively to the estimated optimal Q-function: p?i?
(s) = argmaxa?A Q??
(s, a).3.2 Approximate Dynamic ProgrammingDP-based approaches have two drawbacks.
First,they assume the transition probabilities and the re-ward function to be known.
Practically, it is rarelytrue and especially in the case of spoken dialoguesystems.
Most often, only examples of dialoguesare available which are actually trajectories in thestate-action space.
Second, it assumes that the Q-function can be exactly represented.
However, inreal world dialogue management problems, stateand action spaces are often too large (even contin-uous) for such an assumption to hold.
Approxi-mate Dynamic Programming (ADP) aims at esti-mating the optimal policy from trajectories whenthe state space is too large for a tabular representa-tion.
It assumes that theQ-function can be approx-imated by some parameterized function Q??
(s, a).In this paper, a linear approximation of the Q-function will be assumed: Q??
(s, a) = ?T?
(s, a).where ?
?
Rp is the parameter vector and ?
(s, a)is the set of p basis functions.
All functions ex-pressed in this way define a so-called hypothesisspace H = {Q??|?
?
Rp}.
Any function Q can beprojected onto this hypothesis space by the opera-tor ?
defined as?Q = argminQ???H?Q?
Q???2.
(5)The goal of the ADP algorithms explained in thesubsequent sections is to compute the best set ofparameters ?
given the basis functions.3.2.1 Least-Squares Policy IterationThe least-squares policy iteration (LSPI) algo-rithm has been introduced by Lagoudakis and Parr(2003).
The underlying idea is exactly the sameas for policy iteration: interleaving evaluation andimprovement steps.
The improvement steps aresame as before, but the evaluation step shouldlearn an approximate representation of the Q-function using samples.
In LSPI, this is done usingthe Least-Squares Temporal Differences (LSTD)algorithm of Bradtke and Barto (1996).109LSTD aims at minimizing the distance betweenthe approximated Q-function Q??
and the projec-tion onto the hypothesis space of its image throughthe Bellman evaluation operator ?T piQ??
: ?pi =argmin?
?Rp ?Q??
?
?TpiQ???2.
This can be in-terpreted as trying to minimize the difference be-tween the two sides of the Bellman equation (1)(which should ideally be zero) in the hypothesisspace.
Because of the approximation, this differ-ence is most likely to be non-zero.Practically, T pi is not known, but a set ofN transitions {(sj , aj , rj , s?j)1?j?N} is available.LSTD therefore solves the following optimiza-tion problem: ?pi = argmin?
?Nj=1 CNj (?)
whereCNj (?)
= (rj +?Q?
?pi(s?j , pi(s?j))??Q??
(sj , aj))2.Notice that ?pi appears in both sides of the equa-tion, which renders this problem difficult to solve.However, thanks to the linear parametrization, itadmits an analytical solution, which defines theLSTD algorithm:?pi = (N?j=1?j?
?pij )?1N?j=1?jrj (6)with ?j = ?
(sj , aj) and ?
?pij = ?
(sj , aj) ???
(s?j , pi(s?j)).LSPI is initialized with a policy pi0.
Then, atiteration k, the Q-function of policy pik?1 is esti-mated using LSTD, and pik is greedy respectivelyto this estimated state-action value function.
Itera-tions are stopped when some stopping criterion ismet (e.g., small differences between consecutivepolicies or associated Q-functions).3.2.2 Least-Squares Fitted Value IterationThe Fitted Value Iteration (FVI) class of algo-rithms (Bellman and Dreyfus, 1959; Gordon,1995; Ernst et al, 2005) generalizes value iter-ation to model-free and large state space prob-lems.
The T ?
operator (eq.
(3)) being a con-traction, a straightforward idea would be to applyit iteratively to the approximation similarly to eq.
(4): Q?
?k = T?Q?
?k?1 .
However, T?Q??
does notnecessarily lie in H, it should thus be projectedagain onto the hypothesis space H. By consider-ing the same projection operator ?
as before, thisleads to finding the parameter vector ?
satisfying:Q???
= ?T?Q???.
The fitted-Q algorithm (a spe-cial case of FVI) assumes that the composed ?T ?operator is a contraction and therefore admits anunique fixed point, which is searched for throughthe classic iterative scheme: Q?
?k = ?T?Q?
?k?1 .However, the model (transition probabilities andthe reward function) is usually not known, there-fore a sampled Bellman optimality operator T?
?is considered instead.
For a transition sample(sj , aj , rj , s?j), it is defined as: T?
?Q(sj , aj) =rj + ?
maxa?A Q(s?j , a).
This defines the generalfitted-Q algorithm (?0 being chosen by the user):Q?
?k = ?T??Q?
?k?1 .
Fitted-Q can then be special-ized by choosing how T?
?Q?
?k?1 is projected ontothe hypothesis space, that is the supervised learn-ing algorithm that solves the projection problemof eq.
(5).
The least squares algorithm is chosenhere.The parametrization being linear, and a train-ing base {(sj , aj , rj , s?j)1?j?N} being available,the least-squares fitted-Q (LSFQ for short) is de-rived as follows (we note ?
(sj , aj) = ?j):?k = argmin??RpNXj=1(T?
?Q?
?k?1(sj , aj)?
Q??
(sj , aj))2 (7)= (NXj=1?j?Tj )?1NXj=1?j(rj + ?
maxa?A(?Tk?1?
(s?j , a)))Equation (7) defines an iteration of the proposedlinear least-squares-based fitted-Q algorithm.
Aninitial parameter vector ?0 should be chosen, anditerations are stopped when some criterion is met(maximum number of iterations or small differ-ence between two consecutive parameter vectorestimates).
Assuming that there are M itera-tions, the optimal policy is estimated as p?i?
(s) =argmaxa?A Q?
?M (s, a).4 Learning a sparse parametrizationLSPI and LSFQ (FVI) assume that the basis func-tions are chosen beforehand.
However, this is dif-ficult and problem-dependent.
Thus, we proposeto combine these algorithms with a scheme whichlearns the representation from dialogue corpora.Let?s place ourselves in a general context.
Wewant to learn a parametric representation for anapproximated function f?
(z) = ?T?
(z) fromsamples {z1, .
.
.
, zN}.
A classical choice is tochoose a kernel-based representation (Scholkopfand Smola, 2001).
Formally, a kernel K(z, z?i)is a continuous, positive and semi-definite func-tion (e.g., Gaussian or polynomial kernels) cen-tered on z?i.
The feature vector ?
(z) is thereforeof the form: ?
(z) =(K(z, z?1) .
.
.
K(z, z?p)).The question this section answers is the following:given the training basis {z1, .
.
.
, zN} and a kernel110K, how to choose the number p of basis functionsand the associated kernel centers (z?1, .
.
.
, z?p)?An important result about kernels is the Mer-cer theorem, which states that for each kernelK there exists a mapping ?
: z ?
Z ??
(z) ?
F such that ?z1, z2 ?
Z, K(z1, z2) =??
(z1), ?(z2)?
(in short, K defines a dot prod-uct in F).
The space F is called the featurespace, and it can be of infinite dimension (e.g.,Gaussian kernel), therefore ?
cannot always beexplicitly built.
Given this result and from thebilinearity of the dot product, f?
can be rewrit-ten as follows: f?
(z) =?pi=1 ?iK(z, z?i) =??
(z),?pi=1 ?i?(z?i)?.
Therefore, a kernel-basedparametrization corresponds to a linear approx-imation in the feature space, the weight vectorbeing?pi=1 ?i?(z?i).
This is called the kerneltrick.
Consequently, kernel centers (z?1, .
.
.
, z?p)should be chosen such that (?
(z?1), .
.
.
, ?
(z?p)) arelinearly independent in order to avoid using re-dundant basis functions.
Moreover, kernel cen-ters should be chosen among the training samples.To sum up, learning such a parametrization re-duces to finding a dictionary D = (z?1, .
.
.
, z?p) ?
{z1, .
.
.
, zN} such that (?
(z?1), .
.
.
, ?
(z?p)) are lin-early independent and such that they span thesame subspace as (?
(z1), .
.
.
, ?
(zN )).
Engel etal.
(2004) provides a dictionary method to solvethis problem, briefly sketched here.The training base is sequentially processed, andthe dictionary is initiated with the first sample:D1 = {z1}.
At iteration k, a dictionary Dk?1computed from {z1, .
.
.
, zk?1} is available and thekth sample zk is considered.
If ?
(zk) is linearlyindependent of ?
(Dk?1), then it is added to thedictionary: Dk = Dk?1 ?
{zk}.
Otherwise, thedictionary remains unchanged: Dk = Dk?1.
Lin-ear dependency can be checked by solving thefollowing optimization problem (pk?1 being thesize of Dk?1): ?
= argminw?Rpk?1 ??
(zk) ?
?pk?1i=1 wi?(z?i)?2.
Thanks to the kernel trick (thatis the fact that ??
(zk), ?(z?i)?
= K(zk, z?i)) and tothe bilinearity of the dot product, this optimizationproblem can be solved analytically and withoutcomputing explicitly ?.
Formally, linear depen-dency is satisfied if ?
= 0.
However, an approxi-mate linear dependency is allowed, and ?
(zk) willbe considered as linearly dependent of ?
(Dk?1) if?
< ?, where ?
is the so-called sparsification fac-tor.
This allows controlling the trade-off betweenquality of the representation and its sparsity.
SeeEngel et al (2004) for details as well as an efficientimplementation of this dictionary approach.4.1 Resulting algorithmsWe propose to combine LSPI and LSFQ with thesparsification approach exposed in the previoussection: a kernel is chosen, the dictionary is com-puted and then LSPI or LSFQ is applied using thelearnt basis functions.
For LSPI, this scheme hasbeen proposed before by Xu et al (2007) (withthe difference that they generate new trajectoriesat each iteration whereas we use the same for alliterations).
The proposed sparse LSFQ algorithmis a novel contribution of this paper.We start with the sparse LSFQ algorithm.
In or-der to train the dictionary, the inputs are needed(state-action couples in this case), but not the out-puts (reward are not used).
For LSFQ, the inputspace remains the same over iterations, thereforethe dictionary can be computed in a preprocessingstep from {(sj , aj)1?j?N}.
Notice that the matrix(?Nj=1 ?j?Tj )?1 remains also the same over itera-tions, therefore it can be computed in a preprocess-ing step too.
The proposed sparse LSFQ algorithmis summarized in appendix Algorithm 1.For the sparse LSPI algorithm, things aredifferent.
This time, the inputs depend onthe iteration.
More precisely, at iteration k,the input is composed of state-action couples(sj , aj) but also of transiting state-action cou-ples (s?j , pik?1(s?j)).
Therefore the dictionaryhas to be computed at each iteration from{(sj , aj)1?j?N , (s?j , pik?1(s?j))1?j?N}.
This de-fines the parametrization which is considered forthe Q-function evaluation.
The rest of the algo-rithm is as for the classic LSPI and it is summa-rized in appendix Algorithm 2.Notice that sparse LSFQ has a lower computa-tional complexity than the sparse LSPI.
For sparseLSFQ, dictionary and the matrix P?1 are com-puted in a preprocessing step, therefore the com-plexity per iteration is in O(p2), with p beingthe number of basis functions computed using thedictionary method.
For LSPI, the inverse matrixdepends on the iteration, as well as the dictio-nary, therefore the computational complexity is inO(p3k) per iteration, where pk is the size of the dic-tionary computed at the kth iteration.1115 Experimental set-up and results5.1 Dialogue task and RL parametersThe experimental setup is a form-filling dialoguesystem in the tourist information domain similar tothe one studied in (Lemon et al, 2006).
The sys-tem aims to give information about restaurants inthe city based on specific user preferences.
Threeslots are considered: (i) location, (ii) cuisine and(iii) price-range of the restaurant.
The dialoguestate has three continuous components rangingfrom 0 to 1, each representing the average of fillingand confirmation confidence of the correspondingslots.
The MDP SDS has 13 actions: Ask-slot(3 actions), Explicit-confirm (3 actions), Implicit-confirm and Ask-slot value (6 actions) and Close-dialogue (1 action).
The ?
parameter was set to0.95 in order to encourage delayed rewards andalso to induce an implicit penalty for the length ofthe dialogue episode.
The reward function R ispresented as follows: every correct slot filling isawarded 25, every incorrect slot filling is awarded-75 and every empty slot filling is awarded -300.The reward is awarded at the end of the dialogue.5.2 Dialogue corpora for policy optimizationSo as to perform sparse LSFQ or sparse LSPI, a di-alogue corpus which represents the problem spaceis needed.
As for any batch learning method, thesamples used for learning should be chosen (ifthey can be chosen) to span across the problemspace.
In this experiment, a user simulation tech-nique was used to generate the data corpora.
Thisway, the sensibility of the method to the size ofthe training data-set could be analyzed (availablehuman-dialogue corpora are limited in size).
Theuser simulator was plugged to the DIPPER (Lemonet al, 2006) dialogue management system to gen-erate dialogue samples.
To generate data, the dia-logue manager strategy was jointly based on a sim-ple hand-coded policy (which aims only to fill allthe slots before closing the dialogue episode irre-spective of slot confidence score i.e.,) and randomaction selection.Randomly selected system acts are used withprobability ?
and hand-coded policy selected sys-tem acts are used with probability (1-?).
Duringour data generation process the ?
value was set to0.9.
Rather than using a fully random policy weused an ?-greedy policy to ensure that the prob-lem space is well sampled and in the same time atleast few episodes have successful completion oftask compared to a totally random policy.
We ran56,485 episodes between the policy learner andan unigram user simulation, using the ?-greedypolicy (of which 65% are successful task com-pletion episodes) and collected 393,896 dialogueturns (state transitions).
The maximum episodelength is set as 100 dialogue turns.
The dialogueturns (samples) are then divided into eight differ-ent training sets each with 5.104 samples.5.3 Linear representation of Q-functionTwo different linear representations of the Q-function were used.
First, a set of basis functionscomputed using the dictionary method outlined inSection 4 is used.
A Gaussian kernel is used forthe dictionary computation (?
= 0.25).
The num-ber of elements present in the dictionary variedbased on the number of samples used for computa-tion and the sparsification factor.
It was observedduring the experiments that including a constantterm to the Q-function representation (value setto 1) in addition to features selected by the dic-tionary method avoided weight divergence.
Oursecond representation of Q-function used a set ofhand-picked features presented as a set of Gaus-sian functions, centered in ?i and with the samestandard deviation ?i = ?).
Our RBF networkhad 3 Gaussians for each dimension in the statevector and considering that we have 13 actions, intotal we used 351 (i.e, 33 ?
13) features for ap-proximating theQ-function.
This allows consider-ing that each state variable contributes to the valuefunction differently according to its value contrar-ily to similar work (Li et al, 2009; Henderson etal., 2008) that considers linear contribution of eachstate variable.
Gaussians were centered at ?i = 0.0,0.5, 1.0 in every dimension with a standard devi-ation ?i = ?
= 0.25.
Our stopping criteria wasbased on comparison between L1 norm of suc-ceeding weights and a threshold ?
which was set to10?2 i.e, convergence if?i(|?ni ?
?n?1i |1)< ?,where n is the iteration number.
For sparse LSPIsince the dictionary is computed during each iter-ation, stopping criteria based on ?
is not feasiblethus the learning was stopped after 30 iterations.5.4 Evaluation of learned policyWe ran a set of learning iterations using two differ-ent representations of Q-function and with differ-ent numbers of training samples (one sample is adialogue turn, that is a state transition {s, a, r, s?
}).The number of samples used for training ranged11201020304050607050001000020000300004000050000Average discounted sum of rewardsNumber of samples used fortrainingHand crafted FittedQ LSPISparseFittedQ (Nu=0.7)SparseFittedQ (Nu=0.8)Figure 1: FittedQ policy evaluation statisticsfrom 1.103 to 50.103 samples (no convergence ofweights was observed with fewer samples than1.103).
The training is repeated for each of the 8training data sets.
Dictionary computed using dif-ferent number of training samples and with ?=0.7and 0.8 had a maximum of 367 and 306 elementsrespectively (with lower values of ?
the numberof features is higher than the hand-selected ver-sion).
The policies learned were then tested us-ing a unigram user simulation and the DIPPER di-alogue management framework.
Figures 1 and 2show the average discounted sum of rewards ofpolicies tested over 8?25 dialogue episodes.5.5 Analysis of evaluation resultsOur experimental results show that the dialoguepolicies learned using sparse SLFQ and LSPI withthe two different Q-function representations per-form significantly better than the hand-coded pol-icy.
Most importantly it can be observed fromFigure 1 and 2 that the performance of sparseLSFQ and sparse LSPI (which uses the dictionarymethod for feature selection) are nearly as goodas LSFQ and LSPI (which employs more numer-ous hand-selected basis functions).
This shows theeffectiveness of using the dictionary method forlearning the representation of the Q-function fromthe dialogue corpora.
For this specific problemthe set of hand selected features seem to performbetter than sparse LSPI and sparse LSFQ, but thismay not be always the case.
For complex dialoguemanagement problems feature selection methodssuch as the one studied here will be handy sincethe option of manually selecting a good set of fea-tures will cease to exist.Secondly it can be concluded that, similar toLSFQ and LSPI, the sparse LSFQ and sparse LSPIbased dialogue management are also sample effi-01020304050607050001000020000300004000050000Average discounted sum of rewardsNumber of samples used fortrainingHand crafted LSPISparse-LSPI(Nu=0.7)Sparse-LSPI(Nu=0.8)Figure 2: LSPI policy evaluation statisticscient and needs only few thousand samples (recallthat a sample is a dialogue turn and not a dialogueepisode) to learn fairly good policies, thus exhibit-ing a possibility to learn a good policy directlyfrom very limited amount of dialogue examples.We believe this is a significant improvement whencompared to the corpora requirement for dialoguemanagement using other RL algorithms such asSARSA.
However, sparse LSPI seems to result inpoorer performance compared to sparse LSFQ.One key advantage of using the dictionarymethod is that only mandatory basis functions areselected to be part of the dictionary.
This resultsin fewer feature weights ensuring faster conver-gence during training.
From Figure 1 it can alsobe observed that the performance of both LSFQand LSPI (using hand selected features) are nearlyidentical.
From a computational complexity pointof view, LSFQ and LSPI roughly need the samenumber of iterations before the stopping criterionis met.
However, reminding that the proposedLSFQ complexity is O(p)2 per iteration whereasLSPI complexity is O(p3) per iteration, LSFQ iscomputationally less intensive.6 Discussion and ConclusionIn this paper, we proposed two sample-efficientgeneralization techniques to learn optimal dia-logue policies from limited amounts of dialogueexamples (namely sparse LSFQ and LSPI).
Par-ticularly, a novel sparse LSFQ method has beenproposed and was demonstrated to out-performhandcrafted and LSPI-based policies while usinga limited number of features.
By using a kernel-based approximation scheme, the power of repre-sentation of the state-action value function (or Q-function) is increased with comparison to state-of-113the-art algorithms (such as (Li et al, 2009; Hen-derson et al, 2008)).
Yet the number of features isalso increased.
Using a sparsification algorithm,this number is reduced while policy performancesare kept.
In the future, more compact representa-tion of the state-action value function will be in-vestigated such as neural networks.AcknowledgmentsThe work presented here is part of an ongoing re-search for CLASSiC project (Grant No.
216594,www.classic-project.org) funded by the EuropeanCommission?s 7th Framework Programme (FP7).ReferencesRichard Bellman and Stuart Dreyfus.
1959.
Functionalapproximation and dynamic programming.
Math-ematical Tables and Other Aids to Computation,13:247?251.Richard Bellman.
1957.
Dynamic Programming.Dover Publications, sixth edition.Steven J. Bradtke and Andrew G. Barto.
1996.
Lin-ear Least-Squares algorithms for temporal differ-ence learning.
Machine Learning, 22(1-3):33?57.Wieland Eckert, Esther Levin, and Roberto Pieraccini.1997.
User Modeling for Spoken Dialogue SystemEvaluation.
In ASRU?97, pages 80?87.Yaakov Engel, Shie Mannor, and Ron Meir.
2004.
TheKernel Recursive Least Squares Algorithm.
IEEETransactions on Signal Processing, 52:2275?2285.Damien Ernst, Pierre Geurts, and Louis Wehenkel.2005.
Tree-Based Batch Mode ReinforcementLearning.
Journal of Machine Learning Research,6:503?556.Geoffrey Gordon.
1995.
Stable Function Approxima-tion in Dynamic Programming.
In ICML?95.James Henderson, Oliver Lemon, and KallirroiGeorgila.
2008.
Hybrid reinforcement/supervisedlearning of dialogue policies from fixed data sets.Computational Linguistics, vol.
34(4), pp 487-511.Michail G. Lagoudakis and Ronald Parr.
2003.
Least-squares policy iteration.
Journal of Machine Learn-ing Research, 4:1107?1149.Staffan Larsson and David R. Traum.
2000.
Informa-tion state and dialogue management in the TRINDIdialogue move engine toolkit.
Natural LanguageEngineering, vol.
6, pp 323?340.Oliver Lemon, Kallirroi Georgila, James Henderson,and Matthew Stuttle.
2006.
An ISU dialogue sys-tem exhibiting reinforcement learning of dialoguepolicies: generic slot-filling in the TALK in-car sys-tem.
In EACL?06, Morristown, NJ, USA.Esther Levin and Roberto Pieraccini.
1998.
Us-ing markov decision process for learning dialoguestrategies.
In ICASSP?98.Lihong Li, Suhrid Balakrishnan, and Jason Williams.2009.
Reinforcement Learning for Dialog Man-agement using Least-Squares Policy Iteration andFast Feature Selection.
In InterSpeech?09, Brighton(UK).Olivier Pietquin and Thierry Dutoit.
2006.
A prob-abilistic framework for dialog simulation and opti-mal strategy learning.
IEEE Transactions on Audio,Speech & Language Processing, 14(2): 589-599.Olivier Pietquin.
2005.
A probabilistic descrip-tion of man-machine spoken communication.
InICME?05, pages 410?413, Amsterdam (The Nether-lands), July.Martin L. Puterman.
1994.
Markov Decision Pro-cesses: Discrete Stochastic Dynamic Programming.Wiley-Interscience, April.Jost Schatzmann, Matthew N. Stuttle, Karl Weilham-mer, and Steve Young.
2005.
Effects of theuser model on simulation-based learning of dialoguestrategies.
In ASRU?05, December.Jost Schatzmann, Karl Weilhammer, Matt Stuttle, andSteve Young.
2006.
A survey of statistical user sim-ulation techniques for reinforcement-learning of dia-logue management strategies.
Knowledge Engineer-ing Review, vol.
21(2), pp.
97?126.Bernhard Scholkopf and Alexander J. Smola.
2001.Learning with Kernels: Support Vector Machines,Regularization, Optimization, and Beyond.
MITPress, Cambridge, MA, USA.Satinder Singh, Michael Kearns, Diane Litman, andMarilyn Walker.
1999.
Reinforcement learning forspoken dialogue systems.
In NIPS?99.
Springer.Richard S. Sutton and Andrew G. Barto.
1998.
Re-inforcement Learning: An Introduction (AdaptiveComputation and Machine Learning).
The MITPress, 3rd edition, March.Marilyn A. Walker, Diane J. Litman, Candace A.Kamm, and Alicia Abella.
1997.
PARADISE: Aframework for evaluating spoken dialogue agents.In ACL?97, pages 271?280, Madrid (Spain).Jason Williams and Steve Young.
2005.
Scaling uppomdps for dialogue management: the summarypomdp method.
In ASRU?05.Jason D. Williams and Steve Young.
2007.
Partiallyobservable markov decision processes for spoken di-alog systems.
Computer Speech and Language, vol.21(2), pp.
393?422.Xin Xu, Dewen Hu, and Xicheng Lu.
2007.
Kernel-based least squares policy iteration for reinforce-ment learning.
IEEE Transactions on Neural Net-works, 18(4):973?992, July.114AppendixThis appendix provides pseudo code for the algo-rithms described in the paper.Algorithm 1: Sparse LSFQ.Initialization;Initialize vector ?0, choose a kernel K and asparsification factor ?
;Compute the dictionary;D = {(s?j , a?j)1?j?p} from {(sj , aj)1?j?N};Define the parametrization;Q?
(s, a) = ?T?
(s, a) with ?
(s, a) =(K((s, a), (s?1, a?1)), .
.
.
,K((s, a), (s?p, a?p)))T ;Compute P?1;P?1 = (?Nj=1 ?j?Tj )?1;for k = 1, 2, .
.
.
,M doCompute ?k, see Eq.
(7);endp?i?M (s) = argmaxa?A Q?
?M (s, a);Algorithm 2: Sparse LSPI.Initialization;Initialize policy pi0, choose a kernel K and asparsification factor ?
;for k = 1, 2, .
.
.
doCompute the dictionary;D = {(s?j , a?j)1?j?pk} from{(sj , aj)1?j?N , (s?j , pik?1(s?j))1?j?N};Define the parametrization;Q?
(s, a) = ?T?
(s, a) with ?
(s, a) =(K((s, a), (s?1, a?1)), .
.
.
,K((s, a), (s?pk , a?pk)))T ;Compute ?k?1, see Eq.
(6);Compute pik;pik(s) = argmaxa?A Q?
?k?1(s, a);end115
