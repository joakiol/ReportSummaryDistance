Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 10?20,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsReport of NEWS 2012 Machine Transliteration Shared TaskMin Zhang?, Haizhou Li?, A Kumaran?
and Ming Liu ?
?Institute for Infocomm Research, A*STAR, Singapore 138632{mzhang,hli,mliu}@i2r.a-star.edu.sg?Multilingual Systems Research, Microsoft Research IndiaA.Kumaran@microsoft.comAbstractThis report documents the MachineTransliteration Shared Task conducted asa part of the Named Entities Workshop(NEWS 2012), an ACL 2012 workshop.The shared task features machine translit-eration of proper names from English to11 languages and from 3 languages toEnglish.
In total, 14 tasks are provided.7 teams participated in the evaluations.Finally, 57 standard and 1 non-standardruns are submitted, where diverse translit-eration methodologies are explored andreported on the evaluation data.
We reportthe results with 4 performance metrics.We believe that the shared task hassuccessfully achieved its objective by pro-viding a common benchmarking platformfor the research community to evaluate thestate-of-the-art technologies that benefitthe future research and development.1 IntroductionNames play a significant role in many NaturalLanguage Processing (NLP) and Information Re-trieval (IR) systems.
They are important in CrossLingual Information Retrieval (CLIR) and Ma-chine Translation (MT) as the system performancehas been shown to positively correlate with thecorrect conversion of names between the lan-guages in several studies (Demner-Fushman andOard, 2002; Mandl and Womser-Hacker, 2005;Hermjakob et al, 2008; Udupa et al, 2009).
Thetraditional source for name equivalence, the bilin-gual dictionaries ?
whether handcrafted or sta-tistical ?
offer only limited support because newnames always emerge.All of the above point to the critical need for ro-bust Machine Transliteration technology and sys-tems.
Much research effort has been made to ad-dress the transliteration issue in the research com-munity (Knight and Graehl, 1998; Meng et al,2001; Li et al, 2004; Zelenko and Aone, 2006;Sproat et al, 2006; Sherif and Kondrak, 2007;Hermjakob et al, 2008; Al-Onaizan and Knight,2002; Goldwasser and Roth, 2008; Goldberg andElhadad, 2008; Klementiev and Roth, 2006; Ohand Choi, 2002; Virga and Khudanpur, 2003; Wanand Verspoor, 1998; Kang and Choi, 2000; Gaoet al, 2004; Zelenko and Aone, 2006; Li et al,2009b; Li et al, 2009a).
These previous workfall into three categories, i.e., grapheme-based,phoneme-based and hybrid methods.
Grapheme-based method (Li et al, 2004) treats translitera-tion as a direct orthographic mapping and onlyuses orthography-related features while phoneme-based method (Knight and Graehl, 1998) makesuse of phonetic correspondence to generate thetransliteration.
Hybrid method refers to the com-bination of several different models or knowledgesources to support the transliteration generation.The first machine transliteration shared task (Liet al, 2009b; Li et al, 2009a) was held in NEWS2009 at ACL-IJCNLP 2009.
It was the first timeto provide common benchmarking data in diverselanguage pairs for evaluation of state-of-the-arttechniques.
While the focus of the 2009 sharedtask was on establishing the quality metrics andon baselining the transliteration quality based onthose metrics, the 2010 shared task (Li et al,2010a; Li et al, 2010b) expanded the scope ofthe transliteration generation task to about a dozenlanguages, and explored the quality depending onthe direction of transliteration, between the lan-guages.
In NEWS 2011 (Zhang et al, 2011a;Zhang et al, 2011b), we significantly increasedthe hand-crafted parallel named entities corpora toinclude 14 different language pairs from 11 lan-guage families, and made them available as thecommon dataset for the shared task.
NEWS 2012was a continued effort of NEWS 2011, NEWS102010 and NEWS 2009.The rest of the report is organised as follows.Section 2 outlines the machine transliteration taskand the corpora used and Section 3 discusses themetrics chosen for evaluation, along with the ratio-nale for choosing them.
Sections 4 and 5 presentthe participation in the shared task and the resultswith their analysis, respectively.
Section 6 con-cludes the report.2 Transliteration Shared TaskIn this section, we outline the definition and thedescription of the shared task.2.1 ?Transliteration?
: A definitionThere exists several terms that are used inter-changeably in the contemporary research litera-ture for the conversion of names between twolanguages, such as, transliteration, transcription,and sometimes Romanisation, especially if Latinscripts are used for target strings (Halpern, 2007).Our aim is not only at capturing the name con-version process from a source to a target lan-guage, but also at its practical utility for down-stream applications, such as CLIR and MT.
There-fore, we adopted the same definition of translit-eration as during the NEWS 2009 workshop (Liet al, 2009a) to narrow down ?transliteration?
tothree specific requirements for the task, as fol-lows:?Transliteration is the conversion of a givenname in the source language (a text string in thesource writing system or orthography) to a namein the target language (another text string in thetarget writing system or orthography), such thatthe target language name is: (i) phonemicallyequivalent to the source name (ii) conforms to thephonology of the target language and (iii) matchesthe user intuition of the equivalent of the sourcelanguage name in the target language, consider-ing the culture and orthographic character usagein the target language.
?Following NEWS 2011, in NEWS 2012, westill keep the three back-transliteration tasks.
Wedefine back-transliteration as a process of restor-ing transliterated words to their original lan-guages.
For example, NEWS 2012 offers the tasksto convert western names written in Chinese andThai into their original English spellings, and ro-manized Japanese names into their original Kanjiwritings.2.2 Shared Task DescriptionFollowing the tradition of NEWS workshop se-ries, the shared task at NEWS 2012 is specifiedas development of machine transliteration systemsin one or more of the specified language pairs.Each language pair of the shared task consists of asource and a target language, implicitly specifyingthe transliteration direction.
Training and develop-ment data in each of the language pairs have beenmade available to all registered participants for de-veloping a transliteration system for that specificlanguage pair using any approach that they findappropriate.At the evaluation time, a standard hand-craftedtest set consisting of between 500 and 3,000source names (approximately 5-10% of the train-ing data size) have been released, on which theparticipants are required to produce a ranked listof transliteration candidates in the target languagefor each source name.
The system output istested against a reference set (which may includemultiple correct transliterations for some sourcenames), and the performance of a system is cap-tured in multiple metrics (defined in Section 3),each designed to capture a specific performancedimension.For every language pair each participant is re-quired to submit at least one run (designated as a?standard?
run) that uses only the data provided bythe NEWS workshop organisers in that languagepair, and no other data or linguistic resources.
Thisstandard run ensures parity between systems andenables meaningful comparison of performanceof various algorithmic approaches in a given lan-guage pair.
Participants are allowed to submitmore ?standard?
runs, up to 4 in total.
If more thanone ?standard?
runs is submitted, it is required toname one of them as a ?primary?
run, which isused to compare results across different systems.In addition, up to 4 ?non-standard?
runs could besubmitted for every language pair using either databeyond that provided by the shared task organisersor linguistic resources in a specific language, orboth.
This essentially may enable any participantto demonstrate the limits of performance of theirsystem in a given language pair.The shared task timelines provide adequate timefor development, testing (more than 1 month afterthe release of the training data) and the final re-sult submission (4 days after the release of the testdata).112.3 Shared Task CorporaWe considered two specific constraints in select-ing languages for the shared task: language diver-sity and data availability.
To make the shared taskinteresting and to attract wider participation, it isimportant to ensure a reasonable variety amongthe languages in terms of linguistic diversity, or-thography and geography.
Clearly, the ability ofprocuring and distributing a reasonably large (ap-proximately 10K paired names for training andtesting together) hand-crafted corpora consistingprimarily of paired names is critical for this pro-cess.
At the end of the planning stage and afterdiscussion with the data providers, we have cho-sen the set of 14 tasks shown in Table 1 (Li et al,2004; Kumaran and Kellner, 2007; MSRI, 2009;CJKI, 2010).NEWS 2012 leverages on the success of NEWS2011 by utilizing the training set of NEWS 2011 asthe training data of NEWS 2012 and the dev dataof NEWS 2011 as the dev data of NEWS 2012.NEWS 2012 provides entirely new test data acrossall 14 tasks for evaluation.The names given in the training sets for Chi-nese, Japanese, Korean, Thai, Persian and Hebrewlanguages are Western names and their respectivetransliterations; the Japanese Name (in English)?
Japanese Kanji data set consists only of nativeJapanese names; the Arabic data set consists onlyof native Arabic names.
The Indic data set (Hindi,Tamil, Kannada, Bangla) consists of a mix of In-dian and Western names.For all of the tasks chosen, we have beenable to procure paired names data between thesource and the target scripts and were able tomake them available to the participants.
Forsome language pairs, such as English-Chinese andEnglish-Thai, there are both transliteration andback-transliteration tasks.
Most of the task are justone-way transliteration, although Indian data setscontained mixture of names of both Indian andWestern origins.
The language of origin of thenames for each task is indicated in the first columnof Table 1.Finally, it should be noted here that the corporaprocured and released for NEWS 2012 representperhaps the most diverse and largest corpora to beused for any common transliteration tasks today.3 Evaluation Metrics and RationaleThe participants have been asked to submit resultsof up to four standard and four non-standard runs.One standard run must be named as the primarysubmission and is used for the performance sum-mary.
Each run contains a ranked list of up to10 candidate transliterations for each source name.The submitted results are compared to the groundtruth (reference transliterations) using 4 evalua-tion metrics capturing different aspects of translit-eration performance.
The same as the NEWS2011, we have dropped two MAP metrics usedin NEWS 2009 because they don?t offer additionalinformation to MAPref .
Since a name may havemultiple correct transliterations, all these alterna-tives are treated equally in the evaluation, that is,any of these alternatives is considered as a correcttransliteration, and all candidates matching any ofthe reference transliterations are accepted as cor-rect ones.The following notation is further assumed:N : Total number of names (sourcewords) in the test setni : Number of reference transliterationsfor i-th name in the test set (ni ?
1)ri,j : j-th reference transliteration for i-thname in the test setci,k : k-th candidate transliteration (systemoutput) for i-th name in the test set(1 ?
k ?
10)Ki : Number of candidate transliterationsproduced by a transliteration system3.1 Word Accuracy in Top-1 (ACC)Also known as Word Error Rate, it measures cor-rectness of the first transliteration candidate in thecandidate list produced by a transliteration system.ACC = 1 means that all top candidates are cor-rect transliterations i.e.
they match one of the ref-erences, and ACC = 0 means that none of the topcandidates are correct.ACC =1NN?i=1{1 if ?ri,j : ri,j = ci,1;0 otherwise}(1)3.2 Fuzziness in Top-1 (Mean F-score)The mean F-score measures how different, on av-erage, the top transliteration candidate is from itsclosest reference.
F-score for each source word12Name origin Source script Target script Data Owner Data Size Task IDTrain Dev TestWestern English Chinese Institute for Infocomm Research 37K 2.8K 2K 1K EnChWestern Chinese English Institute for Infocomm Research 28K 2.7K 2.2K 1K ChEnWestern English Korean Hangul CJK Institute 7K 1K 609 1K EnKoWestern English Japanese Katakana CJK Institute 26K 2K 1.8K 1K EnJaJapanese English Japanese Kanji CJK Institute 10K 2K 571 1K JnJkArabic Arabic English CJK Institute 27K 2.5K 2.6K 1K ArEnMixed English Hindi Microsoft Research India 12K 1K 1K 1K EnHiMixed English Tamil Microsoft Research India 10K 1K 1K 1K EnTaMixed English Kannada Microsoft Research India 10K 1K 1K 1K EnKaMixed English Bangla Microsoft Research India 13K 1K 1K 1K EnBaWestern English Thai NECTEC 27K 2K 2K 1K EnThWestern Thai English NECTEC 25K 2K 1.9K 1K ThEnWestern English Persian Sarvnaz Karimi / RMIT 10K 2K 2K 1K EnPeWestern English Hebrew Microsoft Research India 9.5K 1K 1K 1K EnHeTable 1: Source and target languages for the shared task on transliteration.is a function of Precision and Recall and equals 1when the top candidate matches one of the refer-ences, and 0 when there are no common charactersbetween the candidate and any of the references.Precision and Recall are calculated based onthe length of the Longest Common Subsequence(LCS) between a candidate and a reference:LCS(c, r) =12(|c|+ |r| ?
ED(c, r)) (2)where ED is the edit distance and |x| is the lengthof x.
For example, the longest common subse-quence between ?abcd?
and ?afcde?
is ?acd?
andits length is 3.
The best matching reference, thatis, the reference for which the edit distance hasthe minimum, is taken for calculation.
If the bestmatching reference is given byri,m = argminj(ED(ci,1, ri,j)) (3)then Recall, Precision and F-score for i-th wordare calculated asRi =LCS(ci,1, ri,m)|ri,m|(4)Pi =LCS(ci,1, ri,m)|ci,1|(5)Fi = 2Ri ?
PiRi + Pi(6)?
The length is computed in distinct Unicodecharacters.?
No distinction is made on different charactertypes of a language (e.g., vowel vs. conso-nants vs. combining diereses etc.
)3.3 Mean Reciprocal Rank (MRR)Measures traditional MRR for any right answerproduced by the system, from among the candi-dates.
1/MRR tells approximately the averagerank of the correct transliteration.
MRR closer to 1implies that the correct answer is mostly producedclose to the top of the n-best lists.RRi ={minj 1j if ?ri,j , ci,k : ri,j = ci,k;0 otherwise}(7)MRR =1NN?i=1RRi (8)3.4 MAPrefMeasures tightly the precision in the n-best can-didates for i-th source name, for which referencetransliterations are available.
If all of the refer-ences are produced, then the MAP is 1.
Let?s de-note the number of correct candidates for the i-thsource word in k-best list as num(i, k).
MAPrefis then given byMAPref =1NN?i1ni(ni?k=1num(i, k))(9)4 Participation in Shared Task7 teams submitted their transliteration results.
Ta-ble 3 shows the details of registration tasks.
Teamsare required to submit at least one standard run forevery task they participated in.
In total, we re-ceive 57 standard and 1 non-standard runs.
Table 2shows the number of standard and non-standardruns submitted for each task.
It is clear that themost ?popular?
task is the transliteration from En-glish to Chinese being attempted by 7 participants.13English toChineseChinese toEnglishEnglish toThaiThai to En-glishEnglish toHindiEnglish toTamilEnglish toKannadaLanguage pair code EnCh ChEn EnTh ThEn EnHi EnTa EnKaStandard runs 14 5 2 2 2 2 2Non-standard runs 0 0 0 0 0 0 0English toJapaneseKatakanaEnglishto KoreanHangulEnglish toJapaneseKanjiArabic toEnglishEnglish toBengali(Bangla)English toPersianEnglish toHebrewLanguage pair code EnJa EnKo JnJk ArEn EnBa EnPe EnHeStandard runs 3 4 4 5 4 4 4Non-standard runs 0 1 0 0 0 0 0Table 2: Number of runs submitted for each task.
Number of participants coincides with the number ofstandard runs submitted.TeamIDOrganisation EnCh ChEn EnTh ThEn EnHi EnTa EnKa EnJa EnKo JnJk ArEn EnBa EnPe EnHe1 University of Alberta x2 NICT x x x x x x x x x x x x x x3 MIT@Lab of HIT x4 IASL, AcademiaSinicax5 Yahoo Japan Corpora-tionx x x x x x x x6 Yuan Ze University x7 CMU x x x x x x x x x x x x x xTable 3: Participation of teams in different tasks.5 Task Results and Analysis5.1 Standard runsAll the results are presented numerically in Ta-bles 4?17, for all evaluation metrics.
These are theofficial evaluation results published for this editionof the transliteration shared task.The methodologies used in the ten submittedsystem papers are summarized as follows.
Similarto their NEWS 2011 system, Finch et al (2012)employ non-Parametric Bayesian method to co-segment bilingual named entities for model train-ing and report very good performance.
This sys-tem is based on phrase-based statistical machinetransliteration (SMT) (Finch and Sumita, 2008),an approach initially developed for machine trans-lation (Koehn et al, 2003), where the SMT sys-tem?s log-linear model is augmented with a set offeatures specifically suited to the task of translit-eration.
In particular, the model utilizes a fea-ture based on a joint source-channel model, anda feature based on a maximum entropy model thatpredicts target grapheme sequences using the localcontext of graphemes and grapheme sequences inboth source and target languages.
Different fromtheir NEWS 2011 system, in order to solve thedata sparseness issue, they use two RNN-basedLM to project the grapheme set onto a smaller hid-den representation: one for the target grapheme se-quence and the other for the sequence of graphemesequence pair used to generate the target.Zhang et al (2012) also use the statisticalphrase-based SMT framework.
They propose thefine-grained English segmentation algorithm andother new features and achieve very good perfor-mance.
Wu et al (2012) uses m2m-aligner andDirecTL-p decoder and two re-ranking methods:co-occurrence at web corpus and JLIS-Rerankingmethod based on the features from alignment re-sults.
They report very good performance atEnglish-Korean tasks.
Okuno (2012) studies thempaligner (an improvement of m2m-aligner) and14shows that mpaligner is more effective than m2m-aligner.
They also find that de-romanization is cru-cial to JnJk task and mora is the best alignmentunit for EnJa task.
Ammar et al (2012) use CRFas the basic model but with two innovations: atraining objective that optimizes toward any of aset of possible correct labels (i.e., multiple refer-ences) and a k-best reranking with non-local fea-tures.
Their results on ArEn show that the twofeatures are very effective in accuracy improve-ment.
Kondrak et al (2012) study the language-specific adaptations in the context of two languagepairs: English to Chinese (Pinyin representation)and Arabic to English (letter mapping).
They con-clude that Pinyin representation is useful while let-ter mapping is less effective.
Kuo et al (2012) ex-plore two-stage CRF for Enligsh-to-Chinese taskand show that the two-stage CRF outperform tra-ditional one-stage CRF.5.2 Non-standard runsFor the non-standard runs, we pose no restrictionson the use of data or other linguistic resources.The purpose of non-standard runs is to see howbest personal name transliteration can be, for agiven language pair.
In NEWS 2012, only onenon-standard run (Wu et al, 2012) was submitted.Their reported web-based re-validation method isvery effective.6 Conclusions and Future PlansThe Machine Transliteration Shared Task inNEWS 2012 shows that the community has a con-tinued interest in this area.
This report summa-rizes the results of the shared task.
Again, weare pleased to report a comprehensive calibra-tion and baselining of machine transliteration ap-proaches as most state-of-the-art machine translit-eration techniques are represented in the sharedtask.In addition to the most popular techniques suchas Phrase-Based Machine Transliteration (Koehnet al, 2003), CRF, re-ranking, DirecTL-p de-coder, Non-Parametric Bayesian Co-segmentation(Finch et al, 2011), and Multi-to-Multi JointSource Channel Model (Chen et al, 2011) in theNEWS 2011, we are delighted to see that sev-eral new techniques have been proposed and ex-plored with promising results reported, includingRNN-based LM (Finch et al, 2012), English Seg-mentation algorithm (Zhang et al, 2012), JLIS-reranking method (Wu et al, 2012), improvedm2m-aligner (Okuno, 2012), multiple reference-optimized CRF (Ammar et al, 2012), languagedependent adaptation (Kondrak et al, 2012) andtwo-stage CRF (Kuo et al, 2012).
As the stan-dard runs are limited by the use of corpus, most ofthe systems are implemented under the direct or-thographic mapping (DOM) framework (Li et al,2004).
While the standard runs allow us to con-duct meaningful comparison across different al-gorithms, we recognise that the non-standard runsopen up more opportunities for exploiting a vari-ety of additional linguistic corpora.Encouraged by the success of the NEWS work-shop series, we would like to continue this eventin the future conference to promote the machinetransliteration research and development.AcknowledgementsThe organisers of the NEWS 2012 Shared Taskwould like to thank the Institute for InfocommResearch (Singapore), Microsoft Research In-dia, CJK Institute (Japan), National Electronicsand Computer Technology Center (Thailand) andSarvnaz Karim / RMIT for providing the corporaand technical support.
Without those, the SharedTask would not be possible.
We thank those par-ticipants who identified errors in the data and sentus the errata.
We also want to thank the membersof programme committee for their invaluable com-ments that improve the quality of the shared taskpapers.
Finally, we wish to thank all the partici-pants for their active participation that have madethis first machine transliteration shared task a com-prehensive one.15ReferencesYaser Al-Onaizan and Kevin Knight.
2002.
Machinetransliteration of names in arabic text.
In Proc.ACL-2002Workshop: Computational Apporaches toSemitic Languages, Philadelphia, PA, USA.Waleed Ammar, Chris Dyer, and Noah Smith.
2012.Transliteration by sequence labeling with lattice en-codings and reranking.
In Proc.
Named EntitiesWorkshop at ACL 2012.Yu Chen, Rui Wang, and Yi Zhang.
2011.
Statisti-cal machine transliteration with multi-to-multi jointsource channel model.
In Proc.
Named EntitiesWorkshop at IJCNLP 2011.CJKI.
2010.
CJK Institute.
http://www.cjk.org/.D.
Demner-Fushman and D. W. Oard.
2002.
The ef-fect of bilingual term list size on dictionary-basedcross-language information retrieval.
In Proc.
36-thHawaii Int?l.
Conf.
System Sciences, volume 4, page108.2.Andrew Finch and Eiichiro Sumita.
2008.
Phrase-based machine transliteration.
In Proc.
3rd Int?l.Joint Conf NLP, volume 1, Hyderabad, India, Jan-uary.Andrew Finch, Paul Dixon, and Eiichiro Sumita.
2011.Integrating models derived from non-parametricbayesian co-segmentation into a statistical machinetransliteration system.
In Proc.
Named EntitiesWorkshop at IJCNLP 2011.Andrew Finch, Paul Dixon, and Eiichiro Sumita.
2012.Rescoring a phrase-based machine transliterationsystemwith recurrent neural network language mod-els.
In Proc.
Named Entities Workshop at ACL 2012.Wei Gao, Kam-Fai Wong, and Wai Lam.
2004.Phoneme-based transliteration of foreign names forOOV problem.
In Proc.
IJCNLP, pages 374?381,Sanya, Hainan, China.Yoav Goldberg andMichael Elhadad.
2008.
Identifica-tion of transliterated foreign words in Hebrew script.In Proc.
CICLing, volume LNCS 4919, pages 466?477.Dan Goldwasser and Dan Roth.
2008.
Translitera-tion as constrained optimization.
In Proc.
EMNLP,pages 353?362.Jack Halpern.
2007.
The challenges and pitfallsof Arabic romanization and arabization.
In Proc.Workshop on Comp.
Approaches to Arabic Script-based Lang.Ulf Hermjakob, Kevin Knight, and Hal Daume?.
2008.Name translation in statistical machine translation:Learning when to transliterate.
In Proc.
ACL,Columbus, OH, USA, June.Byung-Ju Kang and Key-Sun Choi.
2000.English-Korean automatic transliteration/back-transliteration system and character alignment.
InProc.
ACL, pages 17?18, Hong Kong.Alexandre Klementiev and Dan Roth.
2006.
Weaklysupervised named entity transliteration and discov-ery from multilingual comparable corpora.
In Proc.21st Int?l Conf Computational Linguistics and 44thAnnual Meeting of ACL, pages 817?824, Sydney,Australia, July.Kevin Knight and Jonathan Graehl.
1998.
Machinetransliteration.
Computational Linguistics, 24(4).P.
Koehn, F. J. Och, and D. Marcu.
2003.
Statisticalphrase-based translation.
In Proc.
HLT-NAACL.Grzegorz Kondrak, Xingkai Li, and MohammadSalameh.
2012.
Transliteration experiments on chi-nese and arabic.
In Proc.
Named Entities Workshopat ACL 2012.A Kumaran and T. Kellner.
2007.
A generic frame-work for machine transliteration.
In Proc.
SIGIR,pages 721?722.Chan-Hung Kuo, Shih-Hung Liu, Mike Tian-JianJiang, Cheng-Wei Lee, and Wen-Lian Hsu.
2012.Cost-benefit analysis of two-stage conditionalrandom fields based english-to-chinese machinetransliteration.
In Proc.
Named Entities Workshopat ACL 2012.Haizhou Li, Min Zhang, and Jian Su.
2004.
A jointsource-channel model for machine transliteration.In Proc.
42nd ACL Annual Meeting, pages 159?166,Barcelona, Spain.Haizhou Li, A Kumaran, Vladimir Pervouchine, andMin Zhang.
2009a.
Report of NEWS 2009 machinetransliteration shared task.
In Proc.
Named EntitiesWorkshop at ACL 2009.Haizhou Li, A Kumaran, Min Zhang, and VladimirPervouchine.
2009b.
ACL-IJCNLP 2009 NamedEntities Workshop ?
Shared Task on Translitera-tion.
In Proc.
Named Entities Workshop at ACL2009.Haizhou Li, A Kumaran, Min Zhang, and VladimirPervouchine.
2010a.
Report of news 2010 translit-eration generation shared task.
In Proc.
Named En-tities Workshop at ACL 2010.Haizhou Li, A Kumaran, Min Zhang, and VladimirPervouchine.
2010b.
Whitepaper of news 2010shared task on transliteration generation.
In Proc.Named Entities Workshop at ACL 2010.T.
Mandl and C. Womser-Hacker.
2005.
The effect ofnamed entities on effectiveness in cross-language in-formation retrieval evaluation.
In Proc.
ACM Symp.Applied Comp., pages 1059?1064.16Helen M. Meng, Wai-Kit Lo, Berlin Chen, and KarenTang.
2001.
Generate phonetic cognates to han-dle name entities in English-Chinese cross-languagespoken document retrieval.
In Proc.
ASRU.MSRI.
2009.
Microsoft Research India.http://research.microsoft.com/india.Jong-Hoon Oh and Key-Sun Choi.
2002.
An English-Korean transliteration model using pronunciationand contextual rules.
In Proc.
COLING 2002,Taipei, Taiwan.Yoh Okuno.
2012.
Applying mpaligner to machinetransliteration with japanese-specific heuristics.
InProc.
Named Entities Workshop at ACL 2012.Tarek Sherif and Grzegorz Kondrak.
2007.
Substring-based transliteration.
In Proc.
45th Annual Meetingof the ACL, pages 944?951, Prague, Czech Repub-lic, June.Richard Sproat, Tao Tao, and ChengXiang Zhai.
2006.Named entity transliteration with comparable cor-pora.
In Proc.
21st Int?l Conf Computational Lin-guistics and 44th Annual Meeting of ACL, pages 73?80, Sydney, Australia.Raghavendra Udupa, K. Saravanan, Anton Bakalov,and Abhijit Bhole.
2009.
?They are out there, ifyou know where to look?
: Mining transliterationsof OOV query terms for cross-language informa-tion retrieval.
In LNCS: Advances in InformationRetrieval, volume 5478, pages 437?448.
SpringerBerlin / Heidelberg.Paola Virga and Sanjeev Khudanpur.
2003.
Translit-eration of proper names in cross-lingual informationretrieval.
In Proc.
ACL MLNER, Sapporo, Japan.Stephen Wan and Cornelia Maria Verspoor.
1998.
Au-tomatic English-Chinese name transliteration for de-velopment of multilingual resources.
In Proc.
COL-ING, pages 1352?1356.Chun-Kai Wu, Yu-Chun Wang, and Richard Tzong-Han Tsai.
2012.
English-korean named entitytransliteration using substring alignment and re-ranking methods.
In Proc.
Named Entities Work-shop at ACL 2012.Dmitry Zelenko and Chinatsu Aone.
2006.
Discrimi-native methods for transliteration.
In Proc.
EMNLP,pages 612?617, Sydney, Australia, July.Min Zhang, A Kumaran, and Haizhou Li.
2011a.Whitepaper of news 2011 shared task on machinetransliteration.
In Proc.
Named Entities Workshopat IJCNLP 2011.Min Zhang, Haizhou Li, A Kumaran, and Ming Liu.2011b.
Report of news 2011 machine transliterationshared task.
In Proc.
Named Entities Workshop atIJCNLP 2011.Chunyue Zhang, Tingting Li, and Tiejun Zhao.
2012.Syllable-based machine transliteration with extraphrase features.
In Proc.
Named Entities Workshopat ACL 2012.17Team ID ACC F -score MRR MAPref OrganisationPrimary runs3 0.330357 0.66898 0.413062 0.320285 MIT@Lab of HIT1 0.325397 0.67228 0.418079 0.316296 University of Alberta2 0.310516 0.66585 0.44664 0.307788 NICT4 0.310516 0.662467 0.37696 0.299266 IASL, Academia Sinica5 0.300595 0.655091 0.376025 0.292252 Yahoo Japan Corporation7 0.031746 0.430698 0.055574 0.030265 CMUNon-primary standard runs3 0.330357 0.676232 0.407755 0.3191 MIT@Lab of HIT1 0.325397 0.673053 0.409452 0.316055 University of Alberta1 0.324405 0.668165 0.424517 0.316248 University of Alberta3 0.31746 0.666551 0.399476 0.308187 MIT@Lab of HIT4 0.298611 0.658836 0.362263 0.288725 IASL, Academia Sinica5 0.298611 0.656974 0.357481 0.289373 Yahoo Japan Corporation4 0.294643 0.651988 0.357495 0.284274 IASL, Academia Sinica4 0.290675 0.653565 0.370733 0.282545 IASL, Academia SinicaTable 4: Runs submitted for English to Chinese task.Team ID ACC F -score MRR MAPref OrganisationPrimary runs2 0.20314 0.736058 0.308801 0.199569 NICT3 0.176644 0.701791 0.257324 0.172991 MIT@Lab of HIT7 0.030422 0.489705 0.048211 0.03004 CMU5 0.012758 0.258962 0.017354 0.012758 Yahoo Japan CorporationNon-primary standard runs5 0.007851 0.258013 0.012163 0.007851 Yahoo Japan CorporationTable 5: Runs submitted for Chinese to English back-transliteration task.Team ID ACC F -score MRR MAPref OrganisationPrimary runs2 0.122168 0.746824 0.183318 0.122168 NICT7 0.000809 0.288585 0.001883 0.000809 CMUTable 6: Runs submitted for English to Thai task.Team ID ACC F -score MRR MAPref OrganisationPrimary runs2 0.139968 0.765534 0.21551 0.139968 NICT7 0 0.417451 0.000566 0 CMUTable 7: Runs submitted for Thai to English back-transliteration task.18Team ID ACC F -score MRR MAPref OrganisationPrimary runs2 0.668 0.923347 0.73795 0.661278 NICT7 0.048 0.645666 0.087842 0.048528 CMUTable 8: Runs submitted for English to Hindi task.Team ID ACC F -score MRR MAPref OrganisationPrimary runs2 0.592 0.908444 0.67881 0.5915 NICT7 0.052 0.638029 0.083728 0.052 CMUTable 9: Runs submitted for English to Tamil task.Team ID ACC F -score MRR MAPref OrganisationPrimary runs2 0.546 0.900557 0.640534 0.545361 NICT7 0.116 0.737857 0.180234 0.11625 CMUTable 10: Runs submitted for English to Kannada task.Team ID ACC F -score MRR MAPref OrganisationPrimary runs2 0.400774 0.810109 0.522758 0.397386 NICT5 0.362052 0.802701 0.468973 0.35939 Yahoo Japan Corporation7 0 0.147441 0.00038 0 CMUTable 11: Runs submitted for English to Japanese Katakana task.Team ID ACC F -score MRR MAPref OrganisationPrimary runs6 0.398095 0.731212 0.398095 0.396905 Yuan Ze University2 0.38381 0.721247 0.464553 0.383095 NICT5 0.334286 0.687794 0.411264 0.334048 Yahoo Japan Corporation7 0 0 0.00019 0 CMUNon-standard runs6 0.458095 0.756755 0.484048 0.458095 Yuan Ze UniversityTable 12: Runs submitted for English to Korean task.Team ID ACC F -score MRR MAPref OrganisationPrimary runs2 0.513242 0.693184 0.598304 0.418708 NICT5 0.512329 0.693029 0.581803 0.400505 Yahoo Japan Corporation7 0 0 0 0 CMUNon-primary standard runs5 0.511416 0.691131 0.580485 0.402127 Yahoo Japan CorporationTable 13: Runs submitted for English to Japanese Kanji back-transliteration task.19Team ID ACC F -score MRR MAPref OrganisationPrimary runs2 0.588235 0.929787 0.709003 0.506991 NICT7 0.58391 0.925292 0.694338 0.367162 CMU1 0.583045 0.932959 0.670457 0.42041 University of AlbertaNon-primary standard runs7 0.57699 0.93025 0.678898 0.330353 CMU7 0.573529 0.925306 0.675125 0.328782 CMUTable 14: Runs submitted for Arabic to English task.Team ID ACC F -score MRR MAPref OrganisationPrimary runs2 0.46 0.891476 0.582944 0.458417 NICT5 0.404 0.882395 0.514541 0.402917 Yahoo Japan Corporation7 0.178 0.783893 0.248674 0.177139 CMUNon-primary standard runs5 0.398 0.880286 0.510148 0.396528 Yahoo Japan CorporationTable 15: Runs submitted for English to Bengali (Bangla) task.Team ID ACC F -score MRR MAPref OrganisationPrimary runs5 0.658349 0.940642 0.761223 0.639873 Yahoo Japan Corporation2 0.65547 0.941044 0.773843 0.642663 NICT7 0.18618 0.803002 0.311881 0.184961 CMUNon-primary standard runs5 0.054702 0.627335 0.082754 0.054367 Yahoo Japan CorporationTable 16: Runs submitted for English to Persian task.Team ID ACC F -score MRR MAPref OrganisationPrimary runs5 0.190909 0.808491 0.253575 0.19 Yahoo Japan Corporation2 0.153636 0.787254 0.228649 0.152727 NICT7 0.097273 0.759444 0.130955 0.096818 CMUNon-primary standard runs5 0.165455 0.803019 0.241948 0.164545 Yahoo Japan CorporationTable 17: Runs submitted for English to Hebrew task.20
