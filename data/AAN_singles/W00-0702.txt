In: Proceedings of CoNLL-2000 and LLL-2000, pages 7-12, Lisbon, Portugal, 2000.Corpus-Based Grammar SpecializationNico la  Cancedda and Chr i s te r  SamuelssonXerox Research Centre Europe6, chemin de Maupertuis38240, Meylan, France{Nicola.
Cancedda, Christer.
Samuelsson}@xrce.
xerox, comAbst rac tBroad-coverage rammars tend to be highly am-biguous.
When such grammars are used in arestricted omain, it may be desirable to spe-cialize them, in effect trading some coverage fora reduction in ambiguity.
Grammar specializa-tion is here given a novel formulation as an opti-mization problem, in which the search is guidedby a global measure combining coverage, ambi-guity and grammar size.
The method, applica-ble to any unification grammar with a phrase-structure backbone, is shown to be effective inspecializing a broad-coverage LFG for French.1 In t roduct ionExpressive grammar formalisms allow grammardevelopers to capture complex linguistic gener-alizations concisely and elegantly, thus greatlyfacilitating grammar development and main-tenance.
Broad-coverage grammars, however,tend to overgenerate considerably, thus allowinglarge amounts of spurious ambiguity.
If the ben-efits resulting from more concise grammaticaldescriptions are to outweigh the costs of spuri-ous ambiguity, the latter must be brought down.We here investigate a corpus-based compilationtechnique that reduces overgeneration a d spu-rious ambiguity without jeopardizing coverageor burdening the grammar developer.The current work extends previous workon corpus-based grammar specialization, whichapplies variants of explanation-based learning(EBL) to grammars of natural anguages.
Theearliest work (Rayner, 1988; Samuelsson andRayner, 1991) builds a specialized grammar bychunking together grammar ule combinationswhile parsing training examples.
What rules tocombine is specified by hand-coded criteria.Subsequent work (Rayner and Carter, 1996;Samuelsson, 1994) views the problem as thatof cutting up each tree in a treebank of cor-rect parse trees into subtrees, after which therule combinations corresponding to the subtreesdetermine the rules of the specialized gram-mar.
This approach reports experimental re-sults, using the SRI Core Language Engine,(Alshawi, 1992), in the ATIS domain, of morethan a 3-fold speedup at a cost of 5% in gram-matical coverage, the latter which is compen-sated by an increase in parsing accuracy.
Laterwork (Samuelsson, 1994; Sima'an, 1999) at-tempts to automatically determine appropriatetree-cutting criteria, the former using local mea-sures, the latter using global ones.The current work reverts to the view of EBLas chunking grammar rules.
It extends thelatter work by formulating rammar special-ization as a global optimization problem overthe space of all possible specialized grammarswith an objective function based on the cover-age, ambiguity and size of the resulting gram-mar.
The method was evaluated on the LFGgrammar for French developed within the PAR-GRAM project (Butt et al, 1999), but it isapplicable to any unification grammar with aphrase-structure backbone where the referencetreebank contains all possible analyses for eachtraining example, along with an indication ofwhich one is the correct one.To explore the space of possible grammars, aspecial treebank representation was developed,called a \]folded treebank, which allows the ob-jective function to be computed very efficientlyfor each candidate grammar.
This representa-tion relies on the fact that all possible parsesreturned by the original grammar for each train-ing sentence axe available and the fact that thegrammar specialization ever introduces newparses; it only removes existing ones.The rest of this paper is organized as follows:Section 2 describes the initial candidate gram-mar and the operators used to generate newcandidate grammars from any given one.
Thefunction to be maximized is introduced and mo-tivated in Section 3.
The folded treebank repre-sentation is described in Section 4, while Sec-tion 5 presents the experimental results.2 Unfo ld ing  and  Spec ia l i za t ionThe initial grammar is the grammar underly-ing the subset of correct parses in the trainingset.
This is in itself a specialization of the gram-mar which was used to parse the treebank, sincesome rules may not show up in any correct parsein the training set; experimental results for thisfirst-order specialization are reported in (Can-cedda and Samuelsson, 2000).
This grammaris further specialized by inhibiting rule combi-nations that show up in incorrect parses muchmore often than in correct parses.In more detail, we considered ownward un-folding of grammar ules (see Fig.
l)3 A gram-mar rule is unfolded downwards on one of thesymbols in its right-hand side if it is replacedby a set of rules, each corresponding to the ex-pansion of the chosen symbol by means of an-other grammar ule.
More formally, let G =(E, EN, S, R) be a context-free grammar, andlet r , r '  C R, k E .M + such that rhs(r) = aAfl,lal = k - 1, lhs(r') = A, rhs(r') = V. The ruleadjunction of r I in the k th position of r is definedas a new rule RA(r, k, r ~) = r ' ,  such that:lhs(r") = lhs(r)rhs(r") = aVflFor unification grammars, we instead requirelhs(r') U rhs(r)(k)lhs(r 1') = O(lhs(r))rhs(r") = O(oLTfl )where rhs(r)(k) is the kth symbol of rhs(r),where X t3 Y indicates that X and Y unify, andwhere 0 is the most general unifier of lhs(r ~) andrhs(r)(k).The downward rule unfolding of rule r on itsk th position is then defined as:DRU(r, k) =1The converse operation, upward unfolding, was notused in the current experiments.
{r'\[3r"\[r' = RA(r, k, r")\]} if ?
0= {r} otherwiseIt is easy to see that if all r I E DRU(r, k) areretained then the new grammar has exactly thesame coverage as the old one.
Once the rulehas been unfolded, however, the grammar canbe specialized.
This involves inhibiting somerule combinations by simply removing the cor-responding newly created rules.
Any subsetX C_ DRU(r,k) is called a downward special-ization of rule r on the k th element of its rhs.Given a grammar, all possible (downward)unfoldings of its rules are considered and, foreach unfolding, the specialization leading to thebest increase in the objective function is deter-mined.
The set of all such best specializationsdefines the set of candidate successor grammars.In the experiments, a simple hill-climbing algo-r ithm was adopted.
Other iterative-refinementschemes, such as simulated annealing, could eas-ily be implemented.3 The  Ob ject ive  Funct ionPrevious research approached the task of de-termining which rule combinations to allow ei-ther by a process of manual trial and error orby statistical measures based on a collection ofpositive examples only: if the original grammarproduces more than a single parse of a sentence,only the "correct" parse was stored in the tree-bank.
However, we here also have access to allincorrect parses assigned by the original gram-mar.
This in turn means that we do not needto estimate ambiguity through some correlatedstatistical indicator, since we can measure it di-rectly simply by checking which parse trees arelicensed by every new candidate grammar G.There are many possible ways of combining thecounts of correct and incorrect parses in a suit-able objective function.
For the sake of sire-plicity we opted for a linear combination.
How-ever, simply maximizing correct parses and min-imizing incorrect ones would most likely lead tooverfitting.
In fact, a grammar with one largeflat rule for each correct parse in the treebankwould achieve a very high score during training,but most likely perform poorly on unseen data.A way to avoid overfitting consists in penalizinglarge grammars by introducing an appropriateterm in the linear combination.
The objective8A C  A-oc  A_ocB "-'~" D A "-'P" E F C A ---~" E F CB-----P- E F A ~B ~ G SpecializationDownward unfoldingof A -> B C on "B"A  C  jB -----~ A D B " '~"B  C C " 'D"E  B CB ' ' '~  A C " " -~E B CC ' ' '~  E A SpecializationUpward unfoldingof  A -> B CFigure 1: Schematic examples of upward and downward unfolding of rules.function Score was thus formulated as follows:Scorea = Acorr Corra -- Aine InCa - ~size S i zeawhere Corr and Inc are the number of correctand incorrect parses allowed by the grammar,and Size is the size of the grammar measuredas the total number of symbol occurrences in theright-hand sides of its rules.
Acorr and Ainc areweights controlling the pruning aggressiveness:their ratio Acorr/Ainc intuitively corresponds tothe number of incorrect trees a specializationmust disallow for each disallowed correct tree,if the specialization is to lead to an improve-ment over the current grammar.
The lower thisratio is, the more aggressive the pruning is.
Therelative value of ;~size with respect o the otherAs also controls the depth to which the search isconducted: most specializations result in an in-crease in grammar size, which tends to be moreand more significant as the number and the sizeof rules grows; a larger Asize thus has the effectof stopping the search earlier.
Note that onlytwo of the three weights are independent.4 T reebank  Representat ionA folded treebank is a representation of a setof parse trees which allows an immediate as-sessment of the effects of inhibiting specific rulecombinations.
It is based on the idea of "fold-ing" each tree onto a representation f the gram-mar itself.
Any phrase-structure grammar canbe represented as a concatenation/or g aph - -a directed bipartite multigraph with an or-nodefor each symbol and a concatenation-node foreach rule in the grammar.
The present de-scription covers context-free grammars, but thescheme can easily be extended to any unifica-tion grammar with a context-free backbone byreplacing symbol eqality with unification.Given a grammar G = (E, EN, S,R), we candefine a relation ~ and a (partial) function ~?n:?
~/~ C EN ?
R s.t.
(A , r  / E r/~ iffA = lhs(r)?
r~R : R ?
Af + ~ E s.t.
r lR ( r  , i )  = X iffrhs(r) = f iX% \]/3\[ = i - 1Figure 2 shows the correspondence between asimple grammar fragment and its concatena-t ion/or graph.Each tree can be represented by folding itonto the concatenation/or graph representingthe grammar it was derived with, or, in otherwords, by appropriately annotating the graphitself.
If N is the set of nodes of a parse treeobtained using grammar G, the correspondingfolded tree is a partial function ff :NxN- - -~RxAf  +such that f (n ,n ' )  = (r, k) implies that node nwas expanded using rule r, and that node n' isits k th daughter in the tree (Fig.3).
In the fol-lowing, we will use the inverse image of (r, k) un-der f ,  which we denote ?
(r, k) : f - l ( ( r ,  k)) :{(n ,n ' ) l f (n ,n '  ) : ( r ,k)} C g ?
N. This can inturn be seen as a partial function?
: R ?
Jkf+ ~ 2 N?NDisallowing the expansion of the k th elementin the right-hand side of rule r by means of ruler' (assuming symbols match, i.e., (r/R(r, k), r') Erl2 ) results in suppressing a tree where:3n, n', n" E N, k' E .hf +\[<n, e ?
(r, k) A <n', n"> ?
?
(r', k')\]This check can be performed very efficientlyonce the tree is represented by the ?
function,i.e., once it is folded, as all this requires is tocompare the entries for (r, k) and (r', k') 2 with aprocedure linear in the size of the entries them-selves.
If we used a more traditional represen-tation, the same check would require traversing2In fact, it suffices to check the entries for (r', 1).A?
nOnl ~ B?
n2 ?c ~ n4?f n3c On5?
n7 ?
n8 f e?
(rl ,  1) = {<no,n1), <rt4,rt5>}?
(r l ,  2) = {<no, n2), (n4, n6)}?
(r2, 1) = 0?
(r2, 2) = 0?
(r3, 1) = {(n2, n3)}?
(r3,  2) = (<n2, n4>}?
(r4, I) = {<n6, nT>}?
(r4,  2) -~ {(n6, n8) }Ar 1 --% ~ r2O^'~ " OC x \e .
~ ~;, .er ~ ~o"~ ; !
'~r4Figure 3: A tree and its folded representation.the whole tree.
The worst-case complexity isstill linear in the size of the tree, but in prac-tice, the number of nodes expanded using anygiven rule is much smaller than the total num-ber of nodes.Whenever a specialization is performed, allfolded trees that are no longer licensed areremoved; the concatenation/or graph for thegrammar is updated to reflect the introductionand the elimination of rules; and the annota-tions on the affected edges are appropriately re-combined and distributed.
If the performed spe-cialization is X C_ DRU(r, k) ~ {r}, 3 then theconcatenation/or g aph is updated as follows= nux\{~)~r = ~r U {(lhs(r),~)l~ E X} \ {(lhs(r),r)}~(r" ,  i) ={ VR(r",i), r" ?
X~\]R(r,i), r u E X , i  < k= VR(r' , i  -- k + 1), r" = aA(r ,  k, r') E X,k<i<k+m-1,~R(r, i -- m + 1), r" = RA(r, k, r') E X,i>k+m-1,where m = arity(r') = Irhs(r')l is the numberof right-hand-side symbols of rule r ~.
For eachtree that is not eliminated as a consequence of3If X = DRU(r, k) = {r}, then no update is needed.the specialization we have~(~",~) =?
(r",i),if r" ?~ X ,  r" ?
r'?
(~",/) k(<n',n">13n\[<n,n'> e ?
(~, k)\]),if r" = r ~{(n, n">13n', n", k'\[<n, n'> ~ ?
(~, k)A(n ' ,n" )  E ?
(r ' ,k ' )  A (n,n'")  E ?
(r,i)\]}if r" = RA( r ,k , r  ~) E X , i  < k{(n, n")13n'\[(n, n') E ?
(r, k)A(n', n") E ?
(r', i -- k + 1)\]}i f r "=RA(r ,k , r  ~) EX ,  k<i<k+m-1,{ (n ,n" )13n ' ,n" ,k ' \ [ (n ,n '  ) E ?
(r,k)A<n', n"> e ?
(~', k')A(n, n'") E ?
(r, i - m + 1)\]}if r" = RA(r ,  k, r ~) E X ,  i > k + m - 1,where again m = arity(r').
These updates canbe implemented efficiently, requiring neither atraversal of the tree nor of the grammar.5 Exper imenta l  Resu l t sWe specialized a broad-coverage LFG grammarfor French on a corpus of technical documen-tation using the method described above.
Thetreebank consisted of 960 sentences which wereall known to be covered by the original gram-mar.
For each sentence, all the trees returned by10R --  { r l ,  r2, r3, r4} s.t.rl: A -~cBr2: A --+ e Ara: B -+fAr4: B -arE~2 = {(A, r l) ,  (A, r2}, {B, r3), (B, r4)}CfiR: (rl, 1) ~ c(rl, 2) ~ B(r2,1) -+ e(r2, 2) ~ A(r3, 1) ~ f(r3, 2) ~ A(r4,1) I@4, 2) --~ eAr l  r 2?
??
?
/ ?
e /r 3 \ / r4fFigure 2: A tiny grammar and the correspond-ing concatenation/or graph.the original grammar were available, togetherwith a manually assigned indication of whichwas the correct one.
The environment used,the Xerox Linguistic Environment (Kaplan andMaxwell, 1996) implements a version of opti-mality theory where parses are assigned "opti-mality marks" based on a number of criteria,and are ranked according to these marks.
Theset of parses with the best marks are called theoptimal parses for a sentence.
The correct parsewas also an optimal parse for 913 out of 960 sen-tences.
Given this, the specialization was aimedat reducing the number of optimal parses persentence.We ran a series of ten-fold cross-validation ex-periments; the results are summarized in the ta-ble in Fig.4.
The first line contains values forthe original grammar.
The second line containsmeasures for the first-order pruning grammar,i.e., the grammar with all and only those rulesactually used in correct parses in the trainingset, with no combination inhibited.
Lines 3 and4 list results for fully specialized grammars.
Re-sults in the third line were obtained with a valuefor ~corr equal to 15 times the value of Ainc inthe objective function: in other words, duringtraining we were willing to lose a correct parseonly if at least 15 incorrect parses were canceledas well.
Results in the fourth line were obtainedwhen this ratio was reduced to 10.
The averagenumber of parses per sentence is reported in thefirst column, whereas the second lists the av-erage number of optimal parses.
Coverage wasmeasured as the fraction of sentences which stillreceive the correct parse with the specializedgrammar.
To assess the trade off between cover-age and ambiguity reduction, we computed theF-score 4 considering only optimal parses whencomputing precision.
This measure should notbe confused with the F-score on labelled brack-eting reported for many stochastic parsers; hereprecision and recall concern perfect matching ofwhole trees.
Recall is the same as coverage: theratio between the number of correct parses pro-duced by the specialized grammar and the to-tal number of correct parses (equalling the totalnumber of sentences in the test set).
Precision isthe ratio between the number of correct parsesproduced by the specialized grammar and thetotal number of parses produced by the samegrammar.
The fourth column lists values for theF-score when equal weight is given to precisionand recall.
Intuitively, however, in many casesmissing the correct parse is more of a problemthan returning spurious parses, so we also com-puted the F-score with a much larger emphasison recall, i.e., with a = 0.1.
The correspondingvalues are listed in the last column.The average number of parses per sentence,both optimal and non-optimal, decreases signif-icantly as more and more aggressive specializa-tion.is carried out, and consequently, more cov-erage is lost.
The most aggressive form of spe-4The F-score is the harmonic mean of recall and pre-cision, where precision is weighted a and recall 1 - a.11Avg.p/s Avg.
o.p./s.
Coverage (%) F(a-- 0.5) F(c~-- 0.1)orig.
1941 4.69 100 35.15 73.05f.o.pruning 184 3.38 89 40.64 71.89Acorr=lh)~inc 82 2.23 86 53.25 76.58~corr----lO)kin c 63 2.03 82.5 54.46 74.80Figure 4: Results of thecialization gives the highest F-score for c~ = 0.5,whereas omewhat more conservative parame-ter settings lead to a better F-score when re-call is valued more.
A speedup of a factor 4is achieved already by first-order pruning andremains approximately the same after furtherspecialization.6 ConclusionsBroad-coverage rammars tend to be highly am-biguous, which may constitute a serious prob-lem when using them for natural-language pro-cessing.
Corpus-independent compilation tech-niques, although useful for increasing efficiency,do little in terms of reducing ambiguity.In this paper we proposed a corpus-basedtechnique for specializing a grammar on a do-main for which a treebank exists containing alltrees returned for each sentence.
This tech-nique, which builds extensively on previouswork on explanation-based learning for NLP,consists in casting the problem as an optimiza-tion problem in the space of all possible spe-cializations of the original grammar.
As initialcandidate grammar, the first-order pruning ofthe original grammar is considered.
Candidatesuccessor grammars are obtained through thedownward rule unfolding and specialization op-erator, that has the desirable property of nevercausing previously unseen parses to becomeavailable for sentences in the training set.
Can-didate grammars are then assessed according toan objecting function combining rammar am-biguity and coverage, adapted to avoid overfit-ting.
In order to ensure efficient computabilityof the objective function, the treebank is pre-viously folded onto the grammar itself.
Exper-imental results using a broad-coverage lexical-functional grammar of French show that thetechnique allows effectively trading coverage forambiguity reduction.
Moreover, the parametersof the objective function can be used to controlthe trade off.specialization experiments.AcknowledgementsWe would like to thank the members of theMLTT group at the Xerox Research Centre Eu-rope in Grenoble, France, and the three anony-mous reviewers for valuable discussions andcomments.
This research was funded by the Eu-ropean TMR network Learning ComputationalGrammars.Re ferencesHiyan Alshawi, editor.
1992.
The Core LanguageEngine.
MIT Press.M.
Butt, T.H.
King, M.E.
Nifio, and F. Segond.1999.
A Grammar Writer's Cookbook.
CSLI Pub-lications, Stanford, CA.Nicola Cancedda nd Christer Samuelsson.
2000.Experiments with corpus-based lfgspecialization.In Proceedings o/ the NAACL-ANLP 2000 Con-ference, Seattle, WA.Ronald Kaplan and John T. Maxwell.
1996.LFG grammar writer's workbench.
Technicalreport, Xerox PARC.
Available on-line asftp://ftp.parc.xerox.com/pub/lfg/lfgmanual.ps.Manny Rayner and David Carter.
1996.
Fast pars-ing using pruning and grammar specialization.
InProceedings of the ACL-96, Santa Cruz, CA.Manny Rayner.
1988.
Applying explanation-basedgeneralization to natural-language processing.In Proceedings o/ the International Con/erenceon Fifth Generation Computer Systems, Tokyo,Japan.Christer Samuelsson and Manny Rayner.
1991.Quantitative evaluation of explanation-basedlearning as an optimization tool for a large-scalenatural anguage system.
In Proceedings o/theIJCAI-91, Sydney, Australia.Christer Samuelsson.
1994.
Grammar specializationthrough entropy thresholds.
In Proceedings oftheACL-94, Las Cruces, New Mexico.
Available ascmp-lg/9405022.Khalil Sima'an.
1999.
Learning Efficient Dis-ambiguation.
Ph.D. thesis, Institute for Logic,Language and Computation, Amsterdam, TheNetherlands.12
