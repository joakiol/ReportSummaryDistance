Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1546?1556,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsPhrase-based Machine Translation is State-of-the-Artfor Automatic Grammatical Error CorrectionMarcin Junczys-Dowmunt and Roman GrundkiewiczAdam Mickiewicz University in Poznan?ul.
Umultowska 87, 61-614 Poznan?, Poland{junczys,romang}@amu.edu.plAbstractIn this work, we study parameter tuning to-wards the M2 metric, the standard metric forautomatic grammar error correction (GEC)tasks.
After implementing M2 as a scorerin the Moses tuning framework, we investi-gate interactions of dense and sparse features,different optimizers, and tuning strategies forthe CoNLL-2014 shared task.
We notice er-ratic behavior when optimizing sparse featureweights with M2 and offer partial solutions.We find that a bare-bones phrase-based SMTsetup with task-specific parameter-tuning out-performs all previously published results forthe CoNLL-2014 test set by a large margin(46.37% M2 over previously 41.75%, by anSMT system with neural features) while be-ing trained on the same, publicly availabledata.
Our newly introduced dense and sparsefeatures widen that gap, and we improve thestate-of-the-art to 49.49% M2.1 IntroductionStatistical machine translation (SMT), especially thephrase-based variant, is well established in the fieldof automatic grammatical error correction (GEC)and systems that are either pure SMT or incorporateSMT as system components occupied top positionsin GEC shared tasks for different languages.With the recent paradigm shift in machine trans-lation towards neural translation models, neuralencoder-decoder models are expected to appear inthe field of GEC as well, and first published results(Xie et al, 2016) already look promising.
As it isthe case in classical bilingual machine translationresearch, these models should be compared againststrong SMT baselines.
Similarly, system combina-tions of SMT with classifier-based approaches (Ro-zovskaya and Roth, 2016) suffer from unnecessarilyweak MT base systems which make it hard to assesshow large the contribution of the classifier pipelinesreally is.
In this work we provide these baselines.During our experiments, we find that a bare-bonesphrase-based system outperforms the best publishedresults on the CoNLL-2014 test set by a significantmargin only due to a task-specific parameter tun-ing when being trained on the same data as previoussystems.
When we further investigate the influenceof well-known SMT-specific features and introducenew features adapted to the problem of GEC, our fi-nal systems outperform the best reported results by8% M2, moving the state-of-the-art results for theCoNLL-2014 test set from 41.75% M2 to 49.49%.The paper is organized as follows: section 2describes previous work, the CoNLL-2014 sharedtasks on GEC and follow-up papers.
Our main con-tributions are presented in sections 3 and 4 wherewe investigate the interaction of parameter tuningtowards the M2 metric with task-specific dense andsparse features.
Especially tuning for sparse fea-tures is more challenging than initially expected, butwe describe optimizer hyper-parameters that makesparse feature tuning with M2 feasible.
Section 5reports on the effects of adding a web-scale n-gramlanguage model to our models.Scripts and models used in this paper are availablefrom https://github.com/grammatical/baselines-emnlp2016 to facilitate repro-ducibility of our results.1546CoNLL2014 (u)Susantoet al.
(2014) (r)Xieet al.
(2016) (u)Chollampattet al.
(2016) (r)Hoang et al.
(2016)(r)dense (r)sparse(r)dense (u)sparse(u)0.010.020.030.040.050.0baseline (r)baseline (u)M2this workFigure 1: Comparison with previous work on theCoNLL-2014 task, trained on publicly availabledata.
Dashed lines mark results for our baseline sys-tems with restricted (r) and unrestricted (u) data.2 Previous WorkWhile machine translation has been used for GECin works as early as Brockett et al (2006), we startour discussion with the CoNLL-2014 shared task(Ng et al, 2014) where for the first time an unre-stricted set of errors had to be fully corrected.
Previ-ous work, most notably during the CoNLL shared-task 2013 (Ng et al, 2013), concentrated only onfive selected errors types, but machine translationapproaches (Yoshimoto et al, 2013; Yuan and Fe-lice, 2013) were used as well.The goal of the CoNLL-2014 shared task was toevaluate algorithms and systems for automaticallycorrecting grammatical errors in essays written bysecond language learners of English.
Grammaticalerrors of 28 types were targeted.
Participating teamswere given training data with manually annotatedcorrections of grammatical errors and were allowedto use additional publicly available data.The corrected system outputs were evalu-ated blindly using the MaxMatch (M2) metric(Dahlmeier and Ng, 2012).
Thirteen system sub-missions took part in the shared task.
Among thetop-three positioned systems, two submissions ?CAMB (Felice et al, 2014) and AMU (Junczys-Dowmunt and Grundkiewicz, 2014)1 ?
were par-tially or fully based on SMT.
The second system,CUUI (Rozovskaya et al, 2014), was a classifier-based approach, another popular paradigm in GEC.After the shared task, Susanto et al (2014) pub-lished work on GEC systems combinations.
Theycombined the output from a classification-basedsystem and a SMT-based system using MEMT(Heafield and Lavie, 2010), reporting new state-of-the-art results for the CoNLL-2014 test set.Xie et al (2016) presented a neural network-based approach to GEC.
Their method relies ona character-level encoder-decoder recurrent neuralnetwork with an attention mechanism.
They use datafrom the public Lang-8 corpus and combine theirmodel with an n-gram language model trained onweb-scale Common Crawl data.More recent results are Chollampatt et al (2016)and Hoang et al (2016) which also rely on MTsystems with new features (a feed-forward neuraltranslation model) and n-best list re-ranking meth-ods.
However, most of the improvement over theCoNLL-2014 shared task of these works stems fromusing the parameter tuning tools we introduced inJunczys-Dowmunt and Grundkiewicz (2014).In Figure 1 we give a graphical overview of thepublished results for the CoNLL-2014 test set incomparison to the results we will discuss in thiswork.
Positions marked with (r) use only restricteddata which corresponds to the data set used by Su-santo et al (2014).
Positions with (u) make useof web-scale data, this corresponds to the resourcesused in Xie et al (2016).
We marked the participantsof the CoNLL-2014 shared task as unrestricted assome participants made use of Common Crawl dataor Google n-grams.
The visible plateau for results1Junczys-Dowmunt and Grundkiewicz (2014) is our owncontribution and introduced many of the concepts discussed inthis work, but seemingly to little effect during the task.
Lateranalysis revealed that our submission had an incorrectly filteredlanguage model that was missing many possible entries.
Ouroriginal system without this deficiency would have achieved re-sults around 44% M2 already in 2014.
This discovery triggeredan intensive reanalysis of our shared task system with signifi-cantly new conclusions presented in this work.
We apologizefor supplying these results so late, as this seems to have haltedprogress in the field for nearly two years.1547MT(r)classifier (u)pipline(u)dense (r)sparse(r)dense (u)sparse(u)0.010.020.030.040.050.0M2this workRozovskaya and Roth (2016)Figure 2: Comparison with Rozovskaya and Roth(2016) using the non-public Lang-8 data set.
Here(r) means no web-scale monolingual resources, (u)includes Google 1T n-grams or CommonCrawl.prior to this work seem to confirm our claims aboutmissing strong baselines.Rozovskaya and Roth (2016) introduce a SMT-classifier pipeline with state-of-the-art results.
Un-fortunately, these results are reported for a trainingset that is not publicly available (data crawled fromthe Lang-8 website)2.
Figure 2 compares our resultsfor this resource to Rozovskaya and Roth (2016).See Section 6 for details.3 Dense feature optimizationMoses comes with tools that can tune parameter vec-tors according to different MT tuning metrics.
Priorwork used Moses with default settings: minimumerror rate training (Och, 2003) towards BLEU (Pa-pineni et al, 2002).
BLEU was never designed forgrammatical error correction; we find that directlyoptimizing for M2 works far better.2We shared this resource that has been crawled by us for usein Junczys-Dowmunt and Grundkiewicz (2014) privately withRozovskaya and Roth (2016), but originally were not planningto report results for this resource in the future.
We now provide acomparison to Rozovskaya and Roth (2016), but discourage anyfurther use of this unofficial data due to reproducibility issues.3.1 Tuning towards M2The M2 metric (Dahlmeier and Ng, 2012) is an F-Score, based on the edits extracted from a Leven-shtein distance matrix.
For the CoNLL-2014 sharedtask, the ?-parameter was set to 0.5, putting twotimes more weight on precision than on recall.In Junczys-Dowmunt and Grundkiewicz (2014)we have shown that tuning with BLEU is counter-productive in a setting where M2 is the evaluationmetric.
For inherently weak systems this can resultin all correction attempts to be disabled, MERT thenlearns to disallow all changes since they lower thesimilarity to the reference as determined by BLEU.Systems with better training data, can be tuned withBLEU without suffering this ?disabling?
effect, butwill reach non-optimal performance.
However, Su-santo et al (2014) tune the feature weights of theirtwo SMT-based systems with BLEU on the CoNLL-2013 test set and report state-of-the-art results.Despite tuning with M2, in Junczys-Dowmunt andGrundkiewicz (2014) we were not able to beat sys-tems that did not tune for the task metric.
We re-investigated these ideas with radically better results,re-implemented the M2 metric in C++ and addedit as a scorer to the Moses parameter optimizationframework.
Due to this integration we can now tuneparameter weights with MERT, PRO or Batch Mira.The inclusion of the latter two enables us to experi-ment with sparse features.Based on Clark et al (2011) concerning the ef-fects of optimizer instability, we report results aver-aged over five tuning runs.
Additionally, we com-pute parameter weight vector centroids as suggestedby Cettolo et al (2011).
They showed that param-eter vector centroids averaged over several tuningruns yield similar to or better than average resultsand reduce variance.
We generally confirm this forM2-based tuning.3.2 Dense featuresThe standard features in SMT have been chosen tohelp guiding the translation process.
In a GEC set-ting the most natural units seem to be minimal editoperations that can be either counted or modeled incontext with varying degrees of generalization.
Thatway, the decoder can be informed on several levels1548source phrase target phrase LD D I Sa short time .
short term only .
3 1 1 1a situation into a situation 1 0 1 0a supermarket .
a supermarket .
0 0 0 0a supermarket .
at a supermarket 2 1 1 0able unable 1 0 0 1Table 1: Word-based Levenshtein distance (LD) fea-ture and separated edit operations (D = deletions, I= insertions, S = substitutions)of abstraction how the output differs from the input.3In this section we implement several features that tryto capture these operation in isolation and in context.3.2.1 Stateless featuresOur stateless features are computed during trans-lation option generation before decoding, model-ing relations between source and target phrases.They are meant to extend the standard SMT-specificMLE-based phrase and word translation probabili-ties with meaningful phrase-level information aboutthe correction process.Levenshtein distance.
In Junczys-Dowmunt andGrundkiewicz (2014) we use word-based Leven-shtein distance between source and target as a trans-lation model feature, Felice et al (2014) indepen-dently experiment with a character-based version.Edit operation counts.
We further refine Leven-shtein distance feature with edit operation counts.Based on the Levenshtein distance matrix, the num-bers of deletions, insertions, and substitutions thattransform the source phrase into the target phraseare computed, the sum of these counts is equal tothe original Levenshtein distance (see Table 1).3.2.2 Stateful featuresContrary to stateless features, stateful features canlook at translation hypotheses outside their own spanand take advantage of the constructed target context.The most typical stateful features are language mod-els.
In this section, we discuss LM-like features overedit operations.3We believe this is important information that currently hasnot yet been mastered in neural encoder-decoder approaches.Corpus Sentences TokensNUCLE 57.15 K 1.15 MCoNLL-2013 Test Set 1.38 K 29.07 KCoNLL-2014 Test Set 1.31 K 30.11 KLang-8 2.23 M 30.03 MLang-8 (non-public) 3.72 M 51.07 MWikipedia 213.08 M 3.37 GCommonCrawl (u) 59.13 G 975.63 GTable 2: Parallel (above line) and monolingual train-ing data.Operation SequenceModel.
Durrani et al (2013)introduce Operation Sequence Models in Moses.These models are Markov translation models thatin our setting can be interpreted as Markov editionmodels.
Translations between identical words arematches, translations that have different words onsource and target sides are substitutions; insertionsand deletions are interpreted in the same way as forSMT.
Gaps, jumps, and other operations typical forOSMs do not appear as we disabled reordering.Word-class language model.
The monolingualWikipedia data has been used create a 9-gram word-class language model with 200 word-classes pro-duced by word2vec (Mikolov et al, 2013).
This fea-tures allows to capture possible long distance depen-dencies and semantical aspects.3.3 Training and Test DataThe training data provided in both shared tasksis the NUS Corpus of Learner English (NUCLE)(Dahlmeier et al, 2013).
NUCLE consists of 1,414essays written by Singaporean students who are non-native speakers of English.
The essays cover top-ics, such as environmental pollution, health care, etc.The grammatical errors in these essays have beenhand-corrected by professional English teachers andannotated with one of the 28 predefined error type.Another 50 essays, collected and annotated sim-ilarly as NUCLE, were used in both CoNLL GECshared tasks as blind test data.
The CoNLL-2014test set has been annotated by two human annota-tors, the CoNLL-2013 by one annotator.
Many par-ticipants of CoNLL-2014 shared task used the testset from 2013 as development set for their systems.As mentioned before, we report main results us-1549BaselineLevenshteinEdit ops.Alldense36.038.040.042.0M2(a) Optimized using BLEU on theCoNLL-2013 test setBaselineLevenshteinEdit ops.Alldense(b) Optimized using M2 on theCoNLL-2013 test setBaselineLevenshteinEdit ops.AlldenseAverage M2Centroid M2(c) Optimized using M2 on 4 foldsof error-rate-adapted NUCLEFigure 3: Results on the CoNLL-2014 test set for different optimization settings (5 runs for each system)and different feature sets, the ?All dense?
entry includes OSM, the word class language model, and editoperations).
The small circle marks results for averaged weights vectors and is chosen as the final result.ing similar training data as Susanto et al (2014).
Werefer to this setting that as the ?resticted-data set-ting?
(r).
Parallel data for translation model train-ing is adapted from the above mentioned NUCLEcorpus and the publicly available Lang-8 corpus(Mizumoto et al, 2012), this corpus is distinct fromthe non-public web-crawled data described in Sec-tion 6.
Uncorrected sentences serve as source data,corrected counterparts as target data.
For languagemodeling, the target language sentences of both par-allel resources are used, additionally we extract alltext from the English Wikipedia.Phrase-based SMT makes it ease to scale up interms of training data, especially in the case of n-gram language models.
To demonstrate the ease ofdata integration we propose an ?unrestricted setting?
(u) based on the data used in Junczys-Dowmunt andGrundkiewicz (2014), one of the shared task submis-sions, and later in Xie et al (2016).
We use CommonCrawl data made-available by Buck et al (2014).3.4 ExperimentsOur system is based on the phrase-based part of thestatistical machine translation system Moses (Koehnet al, 2007).
Only plain text data is used for lan-guage model and translation model training.
Ex-ternal linguistic knowledge is introduced during pa-rameter tuning as the tuning metric relies on theerror annotation present in NUCLE.
The transla-tion model is built with the standard Moses trainingscript, word-alignment models are produced withMGIZA++ (Gao and Vogel, 2008), we restrict theword alignment training to 5 iterations of Model 1and 5 iterations of the HMM-Model.
No reorder-ing models are used, the distortion limit is set to0, effectively prohibiting any reordering.
All sys-tems use one 5-gram language model that has beenestimated from the target side of the parallel dataavailable for translation model training.
Another 5-gram language model trained on Wikipedia in therestricted setting or on Common Crawl in the unre-stricted case.Systems are retuned when new features of anytype are added.
We first successfully reproduce re-sults from Susanto et al (2014) for BLEU-basedtuning on the CoNLL-2013 test set as the devel-opment set (Fig.
3a) using similar training data.Repeated tuning places the scores reported by Su-santo et al (2014) for their SMT-ML combinations(37.90 ?
39.39) within the range of possible valuesfor a purely Moses-based system without any spe-cific features (35.19 ?
38.38) or with just the Leven-shtein distance features (37.46 ?
40.52).
Since Su-santo et al (2014) do not report results for multipletuning steps, the extend of influence of optimizer1550instability on their experiments remains unclear.Even with BLEU-based tuning, we can see signifi-cant improvements when replacing Levenshtein dis-tance with the finer-grained edit operations, and an-other performance jump with additional stateful fea-tures.
The value range of the different tuning runsfor the last feature set includes the currently best-performing system (Xie et al (2016) with 40.56%),but the result for the averaged centroid are inferior.Tuning directly with M2 (Fig.
3b) and averag-ing weights across five iterations, yields between40.66% M2 for a vanilla Moses system and 42.32%for a system with all described dense features.
Re-sults seen to be more stable.
Averaging weight vec-tors across runs to produce the final vector seemslike a fair bet.
Performance with the averagedweight vectors is either similar to or better than theaverage number for five runs.3.5 Larger development setsNo less important than choosing the correct tun-ing metric is a good choice of the development set.Among MT researches, there is a number of moreor less well known truths about suitable develop-ment sets for translation-focused settings: usuallythey consist of between 2000 and 3000 sentences,they should be a good representation of the testingdata, sparse features require more sentences or morereferences, etc.
Until now, we followed the seem-ingly obvious approach from Susanto et al (2014) totune on the CoNLL-2013 test set.
The CoNLL-2013test set consists of 1380 sentences, which might bebarely enough for a translation-task, and it is unclearhow to quantify it in the context of grammar correc-tion.
Furthermore, calculating the error rate in thisset reveals that only 14.97% of the tokens are part ofan erroneous fragment, for the rest, input and refer-ence data are identical.
Intuitively, this seems to bevery little significant data for tuning an SMT system.We therefore decide to take advantage of the en-tire NUCLE data as a development set which sofar has only been used as translation model train-ing data.
NUCLE consist of more than 57,000 sen-tences, however, the error rate is significantly lowerthan in the previous development set, only 6.23%.We adapt the error rate by greedily removing sen-tences from NUCLE until an error rate of ca.
15%is reached, 23381 sentences and most error annota-tions remain.
We further divide the data into fourfolds.
Each folds serves as development set for pa-rameter tuning, while the three remaining parts aretreated as translation model training data.
The fullLang-8 data is concatenated with is NUCLE train-ing set, and four models are trained.
Tuning is thenperformed four times and the resulting four parame-ter weight vectors are averaged into a single weightvector across folds.
We repeat this procedure againfive times which results in 20 separate tuning steps.Results on the CoNLL-2014 test set are obtained us-ing the full translation model with a parameter vec-tor average across five runs.
The CoNLL-2013 testset is not being used for tuning and can serve as asecond test set.As can be seen in Fig.
3c, this procedure sig-nificantly improves performance, also for the bare-bones set-up (41.63%).
The lower variance betweeniterations is an effect of averaging across folds.It turns out that what was meant to be a strongbaseline, is actually among the strongest systems re-ported for this task, outperformed only by the fur-ther improvements over this baseline presented inthis work.4 Sparse FeaturesWe saw that introducing finer-grained edit opera-tions improved performance.
The natural evolutionof that idea are features that describe specific cor-rection operations with and without context.
Thiscan be accomplished with sparse features, but tun-ing sparse features according to the M2 metric posesunexpected problems.4.1 Optimizing for M2 with PRO and MiraThe MERT tool included in Moses cannot handleparameter tuning with sparse feature weights andone of the other optimizers available in Moses hasto be used.
We first experimented with both, PRO(Hopkins and May, 2011) and Batch Mira (Cherryand Foster, 2012), for the dense features only, andfound PRO and Batch Mira with standard settingsto either severely underperform in comparison toMERT or to suffer from instability with regard todifferent test sets (Table 3).Experiments with Mira hyper-parameters allowedto counter these effects.
We first change the1551Optimizer 2013 2014MERT 33.50 42.85PRO 33.68 40.34Mira 29.19 34.13-model-bg 31.06 43.88-D 0.001 33.86 42.91Table 3: Tuning with different optimizers with densefeatures only, results are given for the CoNLL-2013and CoNLL-2014 test setbackground BLEU approximation method in BatchMira to use model-best hypotheses (--model-bg)which seems to produce more satisfactory results.Inspecting the tuning process, however, revealsproblems with this setting, too.
Figure 4 documentshow instable the tuning process with Mira is acrossiterations.
The best result is reached after only threeiterations.
In a setting with sparse features thiswould result in only a small set of weighted sparsefeatures.After consulting with one of the authors of Batch-Mira, we set the background corpus decay rate to0.001 (-D 0.001), resulting in a sentence-levelapproximation of M2.
Mira?s behavior seems to sta-bilize across iterations.
At this point it is not quiteclear why this is required.
While PRO?s behav-ior is more sane during tuning, results on the testsets are subpar.
It seems that no comparable hyper-parameter settings exist for PRO.4.2 Sparse edit operationsOur sparse edit operations are again based on theLevenshtein distance matrix and count specific editsthat are annotated with the source and target tokensthat took part in the edit.
For the following erro-neous/corrected sentence pairErr: Then a new problem comes out .Cor: Hence , a new problem surfaces .we generate sparse features that model contextlessedits (matches are omitted):subst(Then,Hence)=1insert(,)=1subst(comes, surfaces)=1del(out)=1and sparse features with one-sided left or right ortwo-sided context:<s>_subst(Then,Hence)=11 2 3 4 5 6 7 8 9 10 11 12 13 14 1515.020.025.030.0No.
of iterationsM2MERTPROMiraMira -model-bgMira -model-bg -D 0.001Figure 4: Results per iteration on development set(4-th NUCLE fold)subst(Then,Hence)_a=1Hence_insert(,)=1insert(,)_a=1problem_subst(comes, surfaces)=1subst(comes, surfaces)_out=1comes_del(out)=1del(out)_.=1<s>_subst(Then,Hence)_a=1Hence_insert(,)_a=1problem_subst(comes, surfaces)_out=1comes_del(out)_.=1All sparse feature types are added on-top of ourbest dense-features system.
When using sparse fea-tures with context, the contextless features are in-cluded.
The context annotation comes from the er-roneous source sentence, not from the corrected tar-get sentence.
We further investigate different sourcefactors: elements taking part in the edit operation orappearing in the context can either be word forms(factor 0) or word classes (factor 1).
As before fordense features we average sparse feature weightsacross folds and multiple tuning runs.Figure 5 summarizes the results for our sparsefeature experiments.
On both test sets we cansee significant improvements when including edit-based sparse features, the performance increaseseven more when source context is added.
TheCoNLL-2013 test set contains annotations from onlyone annotator and is strongly biased towards high15522 4 6 832.034.036.0M2(a) CoNLL-2014 test set2 4 6 842.044.046.0M2(b) CoNLL-2014 test setSymbol DescriptionE0 Edit operation on words, no contextE1 Edit operation on word classes, no contextE0C10 Edit operation on words with left/right context of maximum length 1 on wordsE1C11 Edit operation on word classes with left/right context of maximum length 1 on word classesE0C11 Edit operation on words with left/right context of maximum length 1 on word classesFigure 5: Results on the CoNLL-2013 and CoNLL-2014 test set for different sparse features setsprecision which might explain the greater instability.It appears that sparse features with context wheresurface forms and word-classes are mixed allow forthe best fine-tuning.5 Adding a web-scale language modelUntil now we restricted our experiments to data usedby Susanto et al (2014).
However, systems from theCoNLL-2014 were free to use any publicly availabledata, for instance in Junczys-Dowmunt and Grund-kiewicz (2014), we made use of an n-gram lan-guage model trained from Common Crawl.
Xie etal.
(2016) reach the best published result for the task(before this work) by integrating a similar n-gramlanguage model with their neural approach.We filter the English resources made availableby Buck et al (2014) with cross-entropy filtering(Moore and Lewis, 2010) using the corrected NU-CLE corpus as seed data.
We keep all sentencewith a negative cross-entropy score and compute a 5-gram KenLM (Heafield, 2011) language model withheavy pruning.
This step produces roughly 300Gof compressed text and a manageable 21G binarymodel (available for download).Table 4 summarizes the best results reported inthis paper for the CoNLL-2014 test set (column2014) before and after adding the Common Crawln-gram language model.
The vanilla Moses base-line with the Common Crawl model can be seen as anew simple baseline for unrestricted settings and isahead of any previously published result.
The com-bination of sparse features and web-scale monolin-gual data marks our best result, outperforming pre-viously published results by 8% M2 using similartraining data.
While our sparse features cause a re-spectable gain when used with the smaller languagemodel, the web-scale language model seems to can-cel out part of the effect.Bryant and Ng (2015) extended the CoNLL-2014test set with additional annotations from two to tenannotators.
We report results for this valuable re-source (column 2014-10) as well.4 According toBryant and Ng (2015), human annotators seem toreach on average 72.58% M2 which can be seen asan upper-bound for the task.
In this work, we madea large step towards this upper-bound.4See Bryant and Ng (2015) for a re-assessment of theCoNLL-2014 systems with this extended test set.15532014 2014-10System Prec.
Recall M2 Prec.
Recall M2Baseline 48.97 26.03 41.63 69.29 31.35 55.78+CCLM 58.91 25.05 46.37 77.17 29.38 58.23Best dense 50.94 26.21 42.85 71.21 31.70 57.00+CCLM 59.98 28.17 48.93 79.98 32.76 62.08Best sparse 57.99 25.11 45.95 76.61 29.74 58.25+CCLM 61.27 27.98 49.49 80.93 32.47 62.33Table 4: Best results in restricted setting with added unrestricted language model for original (2014) andextended (2014-10) CoNLL test set (trained with public data only).System Prec.
Recall M2R&R (np) 60.17 25.64 47.40Best dense (np) 53.56 29.59 46.09+CCLM 61.74 30.51 51.25Best sparse (np) 58.57 27.11 47.54+CCLM 63.52 30.49 52.21Table 5: Previous best systems trained with non-public (np) error-corrected data for comparison withRozovskaya and Roth (2016) denoted as R&R.6 More error-corrected dataAs mentioned before, Rozovskaya and Roth (2016)trained their systems on crawled data from the Lang-8 website that has been collect by us for our submis-sion to the CoNLL-2014 shared task.
Since this datahas not been made officially available, we treat it asnon-public.
This makes it difficult to put their resultsin relation with previously published work, but wecan at least provide a comparison for our systems.As our strongest MT-only systems trained on pub-lic data already outperform the pipelined approachesfrom Rozovskaya and Roth (2016), it is unsurprisingthat adding more error-corrected parallel data resultsin an even wider gap (Table 5).
We can assume thatthis gap would persist if only public data had beenused.
Although these are the highest reported resultsfor the CoNLL-2014 shared task so far, we think ofthem as unofficial results and refer to Table 4 as ourfinal results in this work.7 ConclusionsDespite the fact that statistical machine translationapproaches are among the most popular methods inautomatic grammatical error correction, few papersthat report results for the CoNLL-2014 test set seemto have exploited its full potential.
An important as-pect when training SMT systems that one needs totune parameters towards the task evaluation metricseems to have been under-explored.We have shown that a pure SMT system actu-ally outperforms the best reported results for anyparadigm in GEC if correct parameter tuning is per-formed.
With this tuning mechanism available, task-specific features have been explored that bring fur-ther significant improvements, putting phrase-basedSMT ahead of other approaches by a large margin.None of the explored features require complicatedpipelines or re-ranking mechanisms.
Instead theyare a natural part of the log-linear model in phrase-based SMT.
It is therefore quite easy to reproduceour results and the presented systems may serve asnew baselines for automatic grammatical error cor-rection.
Our systems and scripts have been madeavailable for better reproducibility.AcknowledgmentsThe authors would like to thank Colin Cherry forhis help with Batch Mira hyper-parameters andKenneth Heafield for many helpful comments anddiscussions.
This work was partially funded bythe Polish National Science Centre (Grant No.2014/15/N/ST6/02330) and by Facebook.
Theviews and conclusions contained herein are those ofthe authors and should not be interpreted as neces-sarily representing the official policies or endorse-ments, either expressed or implied, of Facebook.1554ReferencesChris Brockett, William B. Dolan, and Michael Gamon.2006.
Correcting ESL errors using phrasal SMT tech-niques.
In Proceedings of the 21st International Con-ference on Computational Linguistics and the 44th An-nual Meeting of the Association for ComputationalLinguistics, pages 249?256, Stroudsburg, USA.
Asso-ciation for Computational Linguistics.Christopher Bryant and Hwee Tou Ng.
2015.
How farare we from fully automatic high quality grammaticalerror correction?
In Proceedings of the 53rd AnnualMeeting of the Association for Computational Linguis-tics and the 7th International Joint Conference on Nat-ural Language Processing of the Asian Federation ofNatural Language Processing, pages 697?707.
Asso-ciation for Computational Linguistics.Christian Buck, Kenneth Heafield, and Bas van Ooyen.2014.
N-gram counts and language models from theCommon Crawl.
In Proceedings of the LanguageResources and Evaluation Conference, pages 3579?3584, Reykjav?
?k, Iceland.Mauro Cettolo, Nicola Bertoldi, and Marcello Federico.2011.
Methods for smoothing the optimizer instabilityin SMT.
In MT Summit XIII: the Thirteenth MachineTranslation Summit, pages 32?39.Colin Cherry and George Foster.
2012.
Batch tuningstrategies for statistical machine translation.
In Pro-ceedings of the 2012 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pages427?436, Stroudsburg, USA.
Association for Compu-tational Linguistics.Shamil Chollampatt, Kaveh Taghipour, and Hwee TouNg.
2016.
Neural network translation models forgrammatical error correction.Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.Smith.
2011.
Better hypothesis testing for statisticalmachine translation: Controlling for optimizer insta-bility.
In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics: Hu-man Language Technologies, HLT ?11, pages 176?181, Stroudsburg, USA.
Association for Computa-tional Linguistics.Daniel Dahlmeier and Hwee Tou Ng.
2012.
Better evalu-ation for grammatical error correction.
In Proceedingsof the 2012 Conference of the North American Chapterof the Association for Computational Linguistics: Hu-man Language Technologies, pages 568?572, Strouds-burg, USA.
Association for Computational Linguis-tics.Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.2013.
Building a large annotated corpus of learner en-glish: The NUS Corpus of Learner English.
In Pro-ceedings of the Eighth Workshop on Innovative Use ofNLP for Building Educational Applications, pages 22?31, Atlanta, Georgia.
Association for ComputationalLinguistics.Nadir Durrani, Alexander Fraser, Helmut Schmid, HieuHoang, and Philipp Koehn.
2013.
Can MarkovModels Over Minimal Translation Units Help Phrase-Based SMT?
In ACL (2), pages 399?405.
The Associ-ation for Computer Linguistics.Mariano Felice, Zheng Yuan, ?istein E. Andersen, He-len Yannakoudakis, and Ekaterina Kochmar.
2014.Grammatical error correction using hybrid systemsand type filtering.
In Proceedings of the Eigh-teenth Conference on Computational Natural Lan-guage Learning: Shared Task, pages 15?24, Balti-more, Maryland.
Association for Computational Lin-guistics.Qin Gao and Stephan Vogel.
2008.
Parallel implemen-tations of word alignment tool.
In Software Engineer-ing, Testing, and Quality Assurance for Natural Lan-guage Processing, pages 49?57.
ACL.Kenneth Heafield and Alon Lavie.
2010.
Combin-ing machine translation output with open source:The Carnegie Mellon multi-engine machine transla-tion scheme.
The Prague Bulletin of MathematicalLinguistics, 93:27?36.Kenneth Heafield.
2011.
KenLM: Faster and smallerlanguage model queries.
In Proceedings of the SixthWorkshop on Statistical Machine Translation, WMT?11, pages 187?197, Stroudsburg, USA.
Associationfor Computational Linguistics.Duc Tam Hoang, Shamil Chollampatt, and Hwee Tou Ng.2016.
Exploiting n-best hypotheses to improve an smtapproach to grammatical error correction.Mark Hopkins and Jonathan May.
2011.
Tuning as rank-ing.
In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing, EMNLP?11, pages 1352?1362, Stroudsburg, USA.
Associationfor Computational Linguistics.Marcin Junczys-Dowmunt and Roman Grundkiewicz.2014.
The AMU system in the CoNLL-2014 sharedtask: Grammatical error correction by data-intensiveand feature-rich statistical machine translation.
InProceedings of the Eighteenth Conference on Com-putational Natural Language Learning: Shared Task(CoNLL-2014 Shared Task), pages 25?33, Baltimore,USA.
Association for Computational Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Annual1555Meeting of the Association for Computational Linguis-tics.
The Association for Computer Linguistics.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013.
Efficient estimation of word represen-tations in vector space.
CoRR, abs/1301.3781.Tomoya Mizumoto, Yuta Hayashibe, Mamoru Komachi,Masaaki Nagata, and Yu Matsumoto.
2012.
The effectof learner corpus size in grammatical error correctionof ESL writings.
In Proceedings of COLING 2012,pages 863?872.Robert C. Moore and William Lewis.
2010.
Intelli-gent selection of language model training data.
InProceedings of the ACL 2010 Conference Short Pa-pers, ACLShort ?10, pages 220?224, Stroudsburg, PA,USA.
Association for Computational Linguistics.Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, ChristianHadiwinoto, and Joel Tetreault.
2013.
The CoNLL-2013 shared task on grammatical error correction.
InProceedings of the 17th Conference on ComputationalNatural Language Learning: Shared Task, pages 1?12, Sofia, Bulgaria.
Association for ComputationalLinguistics.Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, ChristianHadiwinoto, Raymond Hendy Susanto, , and Christo-pher Bryant.
2014.
The CoNLL-2014 shared taskon grammatical error correction.
In Proceedings ofthe Eighteenth Conference on Computational Natu-ral Language Learning: Shared Task (CoNLL-2014Shared Task), pages 1?14, Baltimore, USA.
Associ-ation for Computational Linguistics.Franz Josef Och.
2003.
Minimum error rate train-ing in statistical machine translation.
In Proceedingsof the 41st Annual Meeting on Association for Com-putational Linguistics - Volume 1, ACL ?03, pages160?167, Stroudsburg, USA.
Association for Compu-tational Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A method for automaticevaluation of machine translation.
In Proceedings ofthe 40th Annual Meeting on Association for Computa-tional Linguistics, pages 311?318, Stroudsburg, USA.Association for Computational Linguistics.Alla Rozovskaya and Dan Roth.
2016.
Grammatical er-ror correction: Machine translation and classifiers.
InProceedings of the 54th Annual Meeting of the Asso-ciation for Computational Linguistics, ACL 2016, Au-gust 7-12, 2016, Berlin, Germany, Volume 1: Long Pa-pers.
The Association for Computer Linguistics.Alla Rozovskaya, Kai-Wei Chang, Mark Sammons, DanRoth, and Nizar Habash.
2014.
The Illinois-Columbiasystem in the CoNLL-2014 shared task.
In CoNLL-2014, pages 34?42.Hendy Raymond Susanto, Peter Phandi, and Tou HweeNg.
2014.
System combination for grammatical errorcorrection.
In Proceedings of the 2014 Conference onEmpirical Methods in Natural Language Processing(EMNLP), pages 951?962.
Association for Computa-tional Linguistics.Ziang Xie, Anand Avati, Naveen Arivazhagan, Dan Ju-rafsky, and Andrew Y. Ng.
2016.
Neural languagecorrection with character-based attention.
CoRR,abs/1603.09727.Ippei Yoshimoto, Tomoya Kose, Kensuke Mitsuzawa,Keisuke Sakaguchi, Tomoya Mizumoto, YutaHayashibe, Mamoru Komachi, and Yuji Matsumoto.2013.
NAIST at 2013 CoNLL grammatical errorcorrection shared task.
In Proceedings of the 17thConference on Computational Natural LanguageLearning: Shared Task, pages 26?33, Sofia, Bulgaria.Association for Computational Linguistics.Zheng Yuan and Mariano Felice.
2013.
Constrainedgrammatical error correction using statistical machinetranslation.
In Proceedings of the 17th Conference onComputational Natural Language Learning: SharedTask, pages 52?61, Sofia, Bulgaria.
Association forComputational Linguistics.1556
