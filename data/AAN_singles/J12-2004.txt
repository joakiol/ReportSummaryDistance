Cross-Genre and Cross-Domain Detection ofSemantic UncertaintyGyo?rgy Szarvas?Technische Universita?t DarmstadtVeronika Vincze?
?Hungarian Academy of SciencesRicha?rd Farkas?Universita?t StuttgartGyo?rgy Mo?ra?University of SzegedIryna Gurevych?Technische Universita?t DarmstadtUncertainty is an important linguistic phenomenon that is relevant in various NaturalLanguage Processing applications, in diverse genres from medical to community generated,newswire or scientific discourse, and domains from science to humanities.
The semantic un-certainty of a proposition can be identified in most cases by using a finite dictionary (i.e., lexicalcues) and the key steps of uncertainty detection in an application include the steps of locatingthe (genre- and domain-specific) lexical cues, disambiguating them, and linking them with theunits of interest for the particular application (e.g., identified events in information extraction).In this study, we focus on the genre and domain differences of the context-dependent semanticuncertainty cue recognition task.We introduce a unified subcategorization of semantic uncertainty as different domain appli-cations can apply different uncertainty categories.
Based on this categorization, we normalizedthe annotation of three corpora and present results with a state-of-the-art uncertainty cuerecognition model for four fine-grained categories of semantic uncertainty.?
Technische Universita?t Darmstadt, Ubiquitous Knowledge Processing (UKP) Lab, TU Darmstadt - FB 20Hochschulstrasse 10, D-64289 Darmstadt, Germany.
E-mail: {szarvas,gurevych}@tk.informatik.tu-darmstadt.de.??
Hungarian Academy of Sciences, Research Group on Artificial Intelligence, Tisza Lajos krt.
103,6720 Szeged, Hungary.
E-mail: vinczev@inf.u-szeged.hu.?
Universita?t Stuttgart, Institut fu?r Maschinelle Sprachverarbeitung.
Azenbergstrasse 12, D-70174 Stuttgart,Germany.
E-mail: farkas@ims.uni-stuttgart.de.?
University of Szeged, Department of Informatics, A?rpa?d te?r 2, 6720 Szeged, Hungary.E-mail: gymora@inf.u-szeged.hu.Submission received: 6 April 2011; revised submission received: 1 October 2011; accepted for publication:30 November 2011.?
2012 Association for Computational LinguisticsComputational Linguistics Volume 38, Number 2Our results reveal the domain and genre dependence of the problem; nevertheless, wealso show that even a distant source domain data set can contribute to the recognitionand disambiguation of uncertainty cues, efficiently reducing the annotation costs needed tocover a new domain.
Thus, the unified subcategorization and domain adaptation for trainingthe models offer an efficient solution for cross-domain and cross-genre semantic uncertaintyrecognition.1.
IntroductionIn computational linguistics, especially in information extraction and retrieval, it isof the utmost importance to distinguish between uncertain statements and factualinformation.
In most cases, what the user needs is factual information, hence uncertainpropositions should be treated in a special way: Depending on the exact task, thesystem should either ignore such texts or separate them from factual information.
Inmachine translation, it is also necessary to identify linguistic cues of uncertainty becausethe source and the target language may differ in their toolkit to express uncertainty(one language uses an auxiliary, the other uses just a morpheme).
To cite anotherexample, in clinical document classification, medical reports can be grouped accordingto whether the patient definitely suffers, probably suffers, or does not suffer from anillness.There are several linguistic phenomena that are referred to as uncertainty in theliterature.
We consider propositions to which no truth value can be attributed, giventhe speaker?s mental state, as instances of semantic uncertainty.
In contrast, uncertaintymay also arise at the discourse level, when the speaker intentionally omits some infor-mation from the statement, making it vague, ambiguous, or misleading.
Determiningwhether a given proposition is uncertain or not may involve using a finite dictionary oflinguistic devices (i.e., cues).
Lexical cues (such as modal verbs or adverbs) are respon-sible for semantic uncertainty whereas discourse-level uncertainty may be expressed bylexical cues and syntactic cues (such as passive constructions) as well.
We focus on fourtypes of semantic uncertainty in this study and henceforth the term cuewill be taken tomean lexical cue.The key steps of recognizing semantically uncertain propositions in a naturallanguage processing (NLP) application include the steps of locating lexical cues foruncertainty, disambiguating them (as not all occurrences of the cues indicate uncer-tainty), and finally linking them with the textual representation of the propositionsin question.
The linking of a cue to the textual representation of the proposition canbe performed on the basis of syntactic rules that depend on the word class of thelexical cue, but they are independent of the actual application domain or text typewhere the cue is observed.
The set of cues used and the frequency of their certainand uncertain usages are domain and genre dependent, however, and this has to beaddressed if we seek to craft automatic uncertainty detectors.
Here we interpret genreas the basic style and formal characteristics of the writing that is independent of its topic(e.g., scientific papers, newswire texts, or business letters), and domain as a particularfield of knowledge and is related to the topic of the text (e.g., medicine, archeology, orpolitics).Uncertainty cue candidates do not display uncertainty in all of their occurrences.For instance, the mathematical sense of probable is dominant in mathematical textswhereas its ratio can be relatively low in papers in the humanities.
The frequency ofthe two distinct meanings of the verb evaluate (which can be a synonym of judge [an336Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertaintyuncertain meaning] and calculate) is also different in the bioinformatics and cell biologydomains.
Compare:(1) To evaluateCUE the PML/RARalpha role in myelopoiesis, transgenic miceexpressing PML/RARalpha were engineered.
(2) Our method was evaluated on the Lindahl benchmark for fold recognition.In this article we focus on the domain-dependent aspects of uncertainty detectionand we examine the recognition of uncertainty cues in context.
We do not address theproblem of linking cues to propositions in detail (see, e.g., Chapman, Chu, and Dowling[2007] and Kilicoglu and Bergler [2009] for the information extraction case).For the empirical investigation of the domain dependent aspects, data sets arerequired from various domains.
To date, several corpora annotated for uncertainty havebeen constructed for different genres and domains (BioScope, FactBank, WikiWeasel,and MPQA, to name but a few).
These corpora cover different aspects of uncertainty,however, being grounded on different linguistic models, which makes it hard to exploitcross-domain knowledge in applications.
These differences in part stem from the variedapplication needs across application domains.
Different types of uncertainty and classesof linguistic expressions are relevant for different domains.
Although hypotheses andinvestigations form a crucial part of the relevant cases in scientific applications, theyare less prominent in newswire texts, where beliefs and rumors play a major role.
Thisfinding motivates a more fine-grained treatment of uncertainty.
In order to bridge theexisting gaps between application goals, these typical cases need to be differentiated.
Afine-grained categorization enables the individual treatment of each subclass, whichis less dependent on domain differences than using one coarse-grained uncertaintyclass.
Moreover, this approach enables each particular application to identify and selectfrom a pool of models only those aspects of uncertainty that are relevant in the specificdomain.As one of the main contributions of this study, we propose a uniform subcategoriza-tion of semantic uncertainty in which all the previous corpus annotation works can beplaced, and which reveals the fundamental differences between the currently existingresources.
In addition, we manually harmonized the annotations of three corpora andperformed the fine-grained labeling according to the suggested subcategorization so asto be able to perform cross-domain experiments.An important factor in training robust cross-domain models is to focus on shallowfeatures that can be reliably obtained for many different domains and text types, andto craft models that exploit the shared knowledge from different sources as muchas possible, making the adaptation to new domains efficient.
The study of learningefficient models across different domains is the subject of transfer learning and domainadaptation research (cf.
Daume?
III and Marcu 2006; Pan and Yang 2010).
The domainadaptation setting assumes a target domain (for which an accurate model should belearned with a limited amount of labeled training data), a source domain (with charac-teristics different from the target and for which a substantial amount of labeled data isavailable), and an arbitrary supervised learning model that exploits both the target andsource domain data in order to learn an improved target domain model.The success of domain adaptation mainly depends on two factors: (i) the similarityof the target and source domains (the two domains should be sufficiently similar toallow knowledge transfer); and (ii) the application of an efficient domain adaptation337Computational Linguistics Volume 38, Number 2method (which permits the learning algorithm to exploit the commonalities of thedomains while preserving the special characteristics of the target domain).As our second main contribution, we study the impact of domain differences onuncertainty detection, how this impact depends on the distance between target andsource domains concerning their domains and genres, and how these differences canbe reduced to produce accurate target domain models with limited annotation effort.Because previously existing resources exhibited fundamental differences that madedomain adaptation difficult,1 to our knowledge this is the first study to analyze domaindifferences and adaptability in the context of uncertainty detection in depth, and alsothe first study to report consistently positive results in cross-training.The main contributions of the current paper can be summarized as follows: We provide a uniform subcategorization of semantic uncertainty (withdefinitions, examples, and test batteries for annotation) and classify allmajor previous studies on uncertainty corpus annotation into the proposedcategorization system, in order to reveal and analyze the differences. We provide a harmonized, fine-grained reannotation of three corpora,according to the suggested subcategorization, to allow an in-depthanalysis of the domain dependent aspects of uncertainty detection. We compare the two state-of-the-art approaches to uncertainty cuedetection (i.e., the one based on token classification and the one onsequence labeling models), using a shared feature set, in the context of theCoNLL-2010 shared task, to understand their strengths and weaknesses.2 We train an accurate semantic uncertainty detector that distinguishes fourfine-grained categories of semantic uncertainty (epistemic, doxastic,investigation, and condition types) and thus is better for futureapplications in various domains than previous models.
Our experimentsreveal that, similar to the best model of the CoNLL-2010 shared task forbiological texts but in a fine-grained context, shallow features providegood results in recognizing semantic uncertainty.
We also show that thisrepresentation is less suited to detecting discourse-level uncertainty(which was part of the CoNLL task for Wikipedia texts). We examine in detail the differences between domains and genres asregards the language used to express semantic uncertainty, and learn howthe domain or genre distance affects uncertainty recognition in texts withunseen characteristics. We apply domain adaptation techniques to fully exploit out-of-domaindata and minimize annotation costs to adapt to a new domain, and wereport successful results for various text domains and genres.The rest of the paper is structured as follows.
In Section 2, our classification ofuncertainty phenomena is presented in detail and it is compared with the concept of1 Only 3 out of the more than 20 participants of the related CoNLL-2010 shared task (Farkas et al 2010)managed to exploit out-of-domain data to improve their results, and only by a negligible margin.2 The most successful CoNLL systems were based on these approaches, but different featurerepresentations make direct comparisons difficult.338Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertaintyuncertainty used in existing corpora.
A framework for detecting semantic uncertaintyis then presented in Section 3.
Relatedwork on cue detection is summarized in Section 4,which is followed by a description of our cue recognition system and a presentation ofour experimental set-up using various source and target genre and domain pairs forcross-domain learning and domain adaptation in Section 5.
Our results are elaboratedon in Section 6 with a focus on the effect of domain similarities and on the annotationeffort needed to cover a new domain.
We then conclude with a summary of our resultsand make some suggestions for future research.2.
The Phenomenon UncertaintyIn order to be able to introduce and discuss our data sets, experiments, and findings,we have to clarify our understanding of the term uncertainty.
Uncertainty?in its mostgeneral sense?can be interpreted as lack of information: The receiver of the information(i.e., the hearer or the reader) cannot be certain about some pieces of information.
In thisrespect, uncertainty differs from both factuality and negation; as regards the former,the hearer/reader is sure that the information is true and as for the latter, he is surethat the information is not true.
From the viewpoint of computer science, uncertaintyemerges due to partial observability, nondeterminism, or both (Russell and Norvig2010).
Linguistic theories usually associate the notion of modality with uncertainty:Epistemic modality encodes how much certainty or evidence a speaker has for theproposition expressed by his utterance (Palmer 1986) or it refers to a possible state ofthe world in which the given proposition holds (Kiefer 2005).
The common point inthese approaches is that in the case of uncertainty, the truth value/reliability of theproposition cannot be decided because some other piece of information is missing.Thus, uncertain propositions are those in our understanding whose truth value orreliability cannot be determined due to lack of information.In the following, we focus on semantic uncertainty and we suggest a tentativeclassification of several types of semantic uncertainty.
Our classification is grounded onthe knowledge of existing corpora and uncertainty recognition tools and our chief goalhere is to provide a computational linguistics-oriented classification.
With this in mind,our subclasses are intended to be well-defined and easily identifiable by automatictools.
Moreover, this classification allows different applications to choose the subset ofphenomena to be recognized in accordance with their main task (i.e., we tried to avoidan overly coarse or fine-grained categorization).2.1 Classification of Uncertainty TypesSeveral corpora annotated for uncertainty have been published in different domainssuch as biology (Medlock and Briscoe 2007; Kim, Ohta, and Tsujii 2008; Settles, Craven,and Friedland 2008; Shatkay et al 2008; Vincze et al 2008; Nawaz, Thompson, andAnaniadou 2010), medicine (Uzuner, Zhang, and Sibanda 2009), news media (Rubin,Liddy, and Kando 2005; Wilson 2008; Saur??
and Pustejovsky 2009; Rubin 2010), andencyclopedia (Farkas et al 2010).
As can be seen from publicly available annotationguidelines, there are many overlaps but differences as well in the understanding ofuncertainty, which is sometimes connected to domain- and genre-specific features ofthe texts.
Here we introduce a domain- and genre-independent classification of severaltypes of semantic uncertainty, which was inspired by both theoretical and computa-tional linguistic considerations.339Computational Linguistics Volume 38, Number 2Figure 1Types of uncertainty.
FB = FactBank; Genia = Genia Event; Rubin = the data set describedin Rubin, Liddy and Noriko (2005); META = the data set described in Nawaz, Thompsonand Ananiadou (2010); Medlock = the data set described in Medlock and Briscoe (2007);Shatkay = the data set described in Shatkay et al (2008); Settles = the data set described inSettles et al (2008).2.1.1 A Tentative Classification.
Based on corpus data and annotation principles, theexpression uncertainty can be used as an umbrella term for covering phenomena at thesemantic and discourse levels.3 Our classification of semantic uncertainty is assumedto be language-independent, but our examples presented here come from the Englishlanguage, to keep matters simple.Semantically uncertain propositions can be defined in terms of truth conditionalsemantics.
They cannot be assigned a truth value (i.e., it cannot be stated for surewhether they are true or false) given the speaker?s current mental state.Semantic level uncertainty can be subcategorized into epistemic and hypothetical(see Figure 1).
The main difference between epistemic and hypothetical uncertainty isthat whereas instances of hypothetical uncertainty can be true, false or uncertain, epis-temically uncertain propositions are definitely uncertain?in terms of possible worlds,hypothetical propositions allow that the proposition can be false in the actual world,but in the case of epistemic uncertainty the factuality of the proposition is not known.In the case of epistemic uncertainty, it is known that the proposition is neither truenor false: It describes a possible world where the proposition holds but this possibleworld does not coincide with the speaker?s actual world.
In other words, it is certainthat the proposition is uncertain.
Epistemic uncertainty is related to epistemic modality:a sentence is epistemically uncertain if on the basis of our world knowledge we cannotdecide at the moment whether it is true or false (hence the name) (Kiefer 2005).
Thesource of an epistemically uncertain proposition cannot claim the uncertain propositionand be sure about its opposite at the same time.
(3) EPISTEMIC: Itmay be raining.3 The entire typology of semantic uncertainty phenomena and a test battery for their classification aredescribed in a supplementary file.
Together with the corpora and the experimental software, they areavailable at http://www.inf.u-szeged.hu/rgai/uncertainty.340Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic UncertaintyAs for hypothetical uncertainty, the truth value of the propositions cannot bedetermined either and nothing can be said about the probability of their happening.Propositions under investigation are an example of such statements: Until further anal-ysis, the truth value of the proposition under question cannot be stated.
Conditionalscan also be classified as instances of hypotheses.
It is also common in these two types ofuncertain propositions that the speaker can utter them while it is certain (for others oreven for him) that its opposite holds hence they can be called instances of paradoxicaluncertainty.Hypothetical uncertainty is connectedwith non-epistemic types ofmodality aswell.Doxastic modality expresses the speaker?s beliefs?which may be known to be true orfalse by others in the current state of the world.
Necessity (duties, obligation, orders)is the main objective of deontic modality; dispositional modality is determined by thedispositions (i.e., physical abilities) of the person involved; and circumstantial modalityis defined by external circumstances.
Buletic modality is related to wishes, intentions,plans, and desires.
An umbrella term for deontic, dispositional, circumstantial, andbuletic modality is dynamic modality (Kiefer 2005).HYPOTHETICAL:(4) DYNAMIC: I have to go.
(5) DOXASTIC: He believes that the Earth is flat.
(6) INVESTIGATION: We examined the role of NF-kappa B in protein activation.
(7) CONDITION: If it rains, we?ll stay in.Conditions and instances of dynamic modality are related to the future: In thefuture, they may happen but at the moment it is not clear whether they will take placeor not / whether they are true, false, or uncertain.2.1.2 Comparison with other Classifications.
The feasibility of the classification proposed inthis study can be justified by mapping the annotation schemes used in other existingcorpora to our subcategorizations of uncertainty.
This systematic comparison also high-lights the major differences between existing works and partly explains why examplesfor successful cross-domain application of existing resources and models are hard tofind in the literature.Most of the annotations found in biomedical corpora (Medlock and Briscoe 2007;Settles, Craven, and Friedland 2008; Shatkay et al 2008; Thompson et al 2008; Nawaz,Thompson, and Ananiadou 2010) fall into the epistemic uncertainty class.
BioScope(Vincze et al 2008) annotations mostly belong to the epistemic uncertainty category,with the exception of clausal hypotheses (i.e., hypotheses that are expressed by a clauseheaded by if or whether), which are instances of the investigation class.
The probableclass of Genia Event (Kim, Ohta, and Tsujii 2008) is of the epistemically uncertain typeand the doubtful class belongs to the investigation class.
Rubin, Liddy, and Kando (2005)consider uncertainty as a phenomenon belonging to epistemicmodality: The high, mod-erate, and low levels of certainty coincide with our epistemic uncertainty category.
Thespeculation annotations of the MPQA corpus also belong to the epistemic uncertaintyclass, with four levels (Wilson 2008).
The probable and possible classes found in FactBank(Saur??
and Pustejovsky 2009) are of the epistemically uncertain type, events with ageneric source belong to discourse-level uncertainty, whereas underspecified events are341Computational Linguistics Volume 38, Number 2classified as hypothetical uncertainty in our system as, by definition, their truth valuecannot be determined.
WikiWeasel (Farkas et al 2010) contains annotation for epistemicuncertainty, but discourse-level uncertainty is also annotated in the corpus (see Figure 1for an overview).
The categories used for themachine reading task described inMoranteand Daelemans (2011) also overlap with our fine-grained classes: Uncertain events intheir system fall into our epistemic uncertainty class.
Their modal events expressingpurpose, need, obligation, or desire are instances of dynamic modality, whereas theirconditions are understood in a similar way to our condition class.
The modality typeslisted in Baker et al (2010) can be classified as types of dynamic modality, except fortheir belief category.
Instances of the latter category are either certain (It is certain that hemet the president) or epistemic or doxastic modality in our system.2.2 Types of Semantic Uncertainty CuesWe assume that the nature of the lexical unit determines the type of uncertainty itrepresents, that is, semantic uncertainty is highly lexical in nature.
The part of speech ofthe uncertainty cue candidates serves as the basis for categorization, similar to the onesfound in Hyland (1994, 1996, 1998) and Rizomilioti (2006).
In English, modality is oftenassociated with modal auxiliaries (Palmer 1979), but, as Table 1 shows, there are manyother parts of speech that can express uncertainty.
It should be added that there arecues where it depends on the context, rather than the given lexical item, what subclassof uncertainty the cue refers to, for example, may can denote epistemic modality (It mayrain.
.
. )
or dynamic modality (Now you may open the door).
These categories are listed inTable 1.3.
A Framework for Detecting Semantic UncertaintyIn our model, uncertainty detection is a standalone task that is largely independent ofthe underlying application.
In this section, we briefly discuss how uncertainty detectionTable 1Uncertainty cues.Adjectives / adverbsprobable, likely, possible, unsure, possibly, perhaps, etc.
epistemicAuxiliariesmay, might, can, would, should, could, etc.
semanticVerbsspeculative: suggest, question, seem, appear, favor, etc.
epistemicpsych: think, believe, etc.
doxasticanalytic: investigate, analyze, examine, etc.
investigationprospective: plan, want, order, allow, etc.
dynamicConjunctionsif, whether, etc.
investigationNounsnouns derived speculation, proposal, consideration, etc.
same as the verbfrom uncertain verb:other rumor, idea, etc.
doxasticuncertain nouns:342Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertaintycan be incorporated into an information extraction task, which is probably the mostrelevant application area (see Kim et al [2009] for more details).
In the informationextraction context, the key steps of recognizing uncertain propositions are locating thecues, disambiguating them (as not all occurrences of the cues indicate uncertainty; recallthe example of evaluate), and finally linking them with the textual representation ofthe propositions in question.
We note here that marking the textual representations ofimportant propositions (often referred to as events in information extraction) is actuallythe main goal of an information extraction system, hence we will not focus on theiridentification and just assume that they are already marked in texts.The following is an example that demonstrates the process of uncertainty detection:(8) In this study we hypothesizedCUE that the phosphorylation of TRAF2inhibitsEVENT binding to the CD40 cytoplasmic domain.Here the EVENT mark-up is produced by the information extraction system, anduncertainty detection consists of i) the recognition of the cue word hypothesized, anddetermining whether it denotes uncertainty in this specific case (producing the CUEmark-up) and ii) determining whether the cue hypothesizedmodifies the event triggeredby inhibits or not (positive example in this case).3.1 Uncertainty Cue Detection and DisambiguationThe cue detection and disambiguation problem can be essentially regarded as a tokenlabeling problem.
Here the task is to assign a label to each of the tokens of a sentencein question according to whether it is the starting token of an uncertainty cue (B-CUE TYPE), an inside token of a cue (I-CUE TYPE), or it is not part of any cue (O).
Mostprevious studies assume a binary classification task, namely, each token is either part ofan uncertainty cue, or it is not a cue.
For fine-grained uncertainty detection, a differentlabel has to be used for each uncertainty type to be distinguished.
This way, the labelsequence of a sentence naturally identifies all uncertainty cues (with their types) in thesentence, and disambiguation is solved jointly with recognition.Because the uncertainty cue vocabulary and the distribution of certain and uncer-tain senses of cues vary in different domains and genres, uncertainty cue detection anddisambiguation are the main focus of the current study.3.2 Linking Uncertainty Cues to PropositionsThe task of linking the detected uncertainty cues to propositions can be formulatedas a binary classification task over uncertainty cue and event marker pairs.
The relationholds and is considered true if the cuemodifies the truth value (confidence) of the event;it does not hold and is considered false if the cue does not have any impact on theinterpretation of the event.
That is, the pair (hypothesized, inhibits) in Example (8) is aninstance of positive relation.The linking of uncertainty cues and event markers can be established by using de-pendency grammar rules (i.e., the problem is mainly syntax driven).
As the grammaticalproperties of the language are similar in various domains and genres, this task is largelydomain-independent, as opposed to the recognition and disambiguation task.
Becauseof this, we sketch the most important matching patterns, but do not address the linkingtask in great detail here.343Computational Linguistics Volume 38, Number 2The following are the characteristic rules that can be used to link uncertainty cuesto event markers.
For practical implementations of heuristic cue/event matching, seeChapman, Chu, and Dowling (2007) and Kilicoglu and Bergler (2009). If the event clue has an uncertain verb, noun, preposition, or auxiliary as a(not necessarily direct) parent in the dependency graph of the sentence,the event is regarded as uncertain. If the event clue has an uncertain adverb or adjective as its child, it istreated as uncertain.4.
Related Work on Uncertainty Cue DetectionHerewe review the publishedworks related to uncertainty cue detection.
Earlier studiesfocused either on in-domain cue recognition for a single domain or on cue lexiconextraction from large corpora.
The latter approach is applicable tomultiple domains, butdoes not address the disambiguation of uncertain and other meanings of the extractedcue words.
We are also aware of several studies that discussed the differences of cuedistributions in various domains, without developing a cue detector.
To the best ofour knowledge, our study is the first to address the genre- and domain-adaptability ofuncertainty cue recognition systems and thus uncertainty detection in a general context.We should add that there are plenty of studies on end-application oriented uncer-tainty detection, that is, how to utilize the recognized cues (see, for instance, Kilicogluand Bergler [2008], Uzuner, Zhang, and Sibanda [2009] and Saur??
[2008] for informationextraction or Farkas and Szarvas [2008] for document labeling applications), and arecent pilot task sought to exploit negation and hedge cue detectors in machine reading(Morante and Daelemans 2011).
As the focus of our paper is cue recognition, however,we omit their detailed description here.4.1 In-Domain Cue DetectionIn-domain uncertainty detectors have been developed since the mid 1990s.
Most ofthese systems use hand-crafted lexicons for cue recognition and they treat each oc-currence of the lexicon items as a cue?that is, they do not address the problem ofdisambiguating cues (Friedman et al 1994; Light, Qiu, and Srinivasan 2004; Farkas andSzarvas 2008; Saur??
2008; Conway, Doan, and Collier 2009; Van Landeghem et al 2009).ConText (Chapman, Chu, and Dowling 2007) uses regular expressions to define cuesand ?pseudo-triggers?.
A pseudo-trigger is a superstring of a cue and it is basicallyused for recognizing contexts where a cue does not imply uncertainty (i.e., it can beregarded as a hand-crafted cue disambiguation module).
MacKinlay, Martinez, andBaldwin (2009) introduced a system which also used non-consecutive tokens as cues(like not+as+yet).Utilizing manually labeled corpora, machine learning?based uncertainty cue de-tectors have also been developed (to the best of our knowledge each of them uses anin-domain training data set).
They use token classification (Morante and Daelemans2009; Clausen 2010; Fernandes, Crestana, and Milidiu?
2010; Sa?nchez, Li, and Vogel2010) or sequence labeling approaches (Li et al 2010; Rei and Briscoe 2010; Tang et al2010; Zhang et al 2010).
In both cases the tokens are labeled according to whetherthey are part of a cue.
The latter assigns a label sequence to a sentence (a sequence of344Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertaintytokens) thus it naturally deals with the context of a particular word.
On the other hand,context information for a token is built into the feature space of the token classificationapproaches.
O?zgu?r and Radev (2009) and Velldal (2010) match cues from a lexicon thenapply a binary classifier based on features describing the context of the cue candidate.Each of these approaches uses a rich feature representation for tokens, which usu-ally includes surface-level, part-of-speech, and chunk-level features.
A few systemshave also used dependency relation types originating at the cue (Rei and Briscoe 2010;Sa?nchez, Li, and Vogel 2010; Velldal, ?vrelid, and Oepen 2010; Zhang et al 2010); theCoNLL-2010 Shared Task final ranking suggests that it has only a limited impact on theperformance of an entire system (Farkas et al 2010), however.
O?zgu?r and Radev (2009)further extended the feature set with the other cues that occur in the same sentence asthe cue, and positional features such as the section header of the article in which the cueoccurs (the latter is only defined for scientific publications).
Velldal (2010) argues thatthe dimensionality of the uncertainty cue detection feature space is too high and reportsimprovements by using the sparse random indexing technique.Ganter and Strube (2009) proposed a rather different approach for (weasel) cuedetection?exploiting weasel tags4 in Wikipedia articles given by editors.
They usedsyntax-based patterns to recognize the internal structure of the cues, which has proveduseful as discourse-level uncertainty cues are usually long and have a complex internalstructure (as opposed to semantic uncertainty cues).As can be seen, uncertainty cue detectors have mostly been developed in the bio-logical and medical domains.
All of these studies, however, focus on only one domain,namely, in-domain cue detection is carried out, which assumes the availability of atraining data set of sufficient size.
The only exceptionwe are aware of is the CoNLL-2010Shared Task (Farkas et al 2010), where participants had the chance to use Wikipediadata on biomedical domain and vice versa.
Probably due to the differences in theannotated uncertainty types and the stylistic and topical characteristics of the texts,very few participants performed cross-domain experiments and reported only limitedsuccess (see Section 5.3.2 for more on this).Overall, the findings of these studies indicate that disambiguating cue candidates isan important aspect of uncertainty detection and that the domain specificity of disam-biguation models and domain adaptation in general are largely unexplored problemsin uncertainty detection.4.2 Weakly Supervised Extraction of Cue LexiconSimilar to our approach, several studies have addressed the problem of developingan uncertainty detector for a new domain using as little annotation effort as possible.The aim of these studies is to identify uncertain sentences; this is carried out by semi-automatic construction of cue lexicons.
The weakly supervised approaches start withvery small seed sets of annotated certain and uncertain sentences, and use bootstrap-ping to induce a suitable training corpus in an automatic way.
Such approaches collectpotentially certain and uncertain sentences from a large unlabeled pool based on theirsimilarity to the instances in the seed sets (Medlock and Briscoe 2007), or based on theknown errors of an information extraction system that is itself sensitive to uncertaintexts (Szarvas 2008).
Further instances are then collected (in an iterative fashion) onthe basis of their similarity to the current training instances.
Based on the observation4 See http://en.wikipedia.org/wiki/Wikipedia:Embrace_weasel_words.345Computational Linguistics Volume 38, Number 2that uncertain sentences tend to contain more than one uncertainty cue, these modelssuccessfully extend the seed sets with automatically labeled sentences, and can producean uncertainty classifier with a sentence-level F-score of 60?80% for the uncertain class,given that the texts of the seed examples, the unlabeled pool, and the actual evaluationdata share very similar properties.Szarvas (2008) showed that these models essentially learn the uncertainty lexicon(set of cues) of the given domain, but are otherwise unable to disambiguate the potentialcue words?that is, to distinguish between the uncertain and certain uses of the previ-ously seen cues.
This deficiency of the derived models is inherent to the bootstrappingprocess, which considers all occurrences of the cue candidates as good candidates forpositive examples (as opposed to unlabeled sentences without any previously seen cuewords).Kilicoglu and Bergler (2008) proposed a semi-automatic method to expand a seedcue lexicon.
Their linguistically motivated approach is also based on the weakly super-vised induction of a corpus of uncertain sentences.
It exploits the syntactic patterns ofuncertain sentences to identify new cue candidates.The previous studies on weakly supervised approaches to uncertainty detectiondid not tackle the problem of disambiguating the certain and uncertain uses of cuecandidates, which is a major drawback from a practical point of view.4.3 Cue Distribution AnalysesBesides automatic uncertainty recognition, several studies investigated the distributionof hedge cues in scientific papers from different domains (Hyland 1998; Falahati 2006;Rizomilioti 2006).
The effect of different domains on the frequency of uncertain expres-sions was examined in Rizomilioti (2006).
Based on a previously defined dictionary ofhedge cues, she analyzed the linguistic tools expressing epistemic modality in researchpapers from three domains, namely, archeology, literary criticism, and biology.
Herresults indicated that archaeological papers tend to contain the most uncertainty cues(which she calls downtoners) and the fewest uncertainty cues can be found in literarycriticism papers.
Different academic disciplines were contrasted in Hyland (1998) fromthe viewpoint of hedging: Papers belonging to the humanities contain significantlymore hedging devices than papers in sciences.
It is interesting to note, however, thatin both studies, biological papers are situated in the middle as far as the percentage rateof uncertainty cues is concerned.
Falahati (2006) examined hedges in research articles inmedicine, chemistry, and psychology and concluded that it is psychology articles thatcontain the most hedges.Overall, these studies demonstrate that there are substantial differences in the waydifferent technical/scientific domains and different genres express uncertainty in gen-eral, and in the use of semantic uncertainty in particular.
Differences are found not justin the use of different vocabulary for expressing uncertainty, but also in the frequencyof certain and uncertain usage of particular uncertainty cues.
These findings underpinthe practical importance of domain portability and domain adaptation of uncertaintydetectors.5.
Uncertainty Cue RecognitionIn this section, we present our uncertainty cue detector and the results of the cross-genreand -domain experiments carried out by us.
Before describing ourmodel and discussingthe results of the experiments, a short overview of the texts used as training and test346Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertaintydata sets will be given along with an empirical analysis of the sense distributions of themost frequent cues.5.1 Data SetsIn our investigations, we selected three corpora (i.e., BioScope, WikiWeasel, and Fact-Bank) from different domains (biomedical, encyclopedia, and newswire, respectively).Genres also vary in the corpora (in the scientific genre, there are papers and abstractswhereas the other corpora contain pieces of news and encyclopedia articles).
We pre-ferred corpora on which earlier experiments had been carried out because this allowedus to compare our results with those of previous studies.
This selectionmakes it possibleto investigate domain and genre differences because each domain has its characteristiclanguage use (which might result in differences in cue distribution) and different genresalso require different writing strategies (e.g., in abstracts, implications of experimentalresults are often emphasized, which usually involves the use of uncertain language).The BioScope corpus (Vincze et al 2008) contains clinical texts as well as biologicaltexts from full papers and scientific abstracts; the texts were manually annotated forhedge cues and their scopes.
In our experiments, 15 other papers annotated for theCoNLL-2010 Shared Task (Farkas et al 2010) were also added to the set of BioScopepapers.
The WikiWeasel corpus (Farkas et al 2010) was also used in the CoNLL-2010Shared Task and it was manually annotated for weasel cues and semantic uncertaintyin randomly selected paragraphs taken from Wikipedia articles.
The FactBank corpuscontains texts from the newswire domain (Saur??
and Pustejovsky 2009).
Events areannotated in the data set and they are evaluated on the basis of their factuality fromthe viewpoint of their sources.Table 2 provides statistical data on the three corpora.
Because in our experimentalset-up, texts belonging to different genres also play an important role, data on abstractsand papers are included separately.5.1.1 Genres and Domains.
Texts found in the three corpora to be investigated can becategorized into three genres, which can be further divided to subgenres at a finer levelof distinction.
Figure 2 depicts this classification.The majority of BioScope texts (papers and abstracts) belong to the scientific dis-course genre.
FactBank texts can be divided into broadcast and written news, andWikipedia texts belong to the encyclopedia genre.As for the domain of the texts, there are three broad domains, namely, biology, news,and encyclopedia.
Once again, these domains can be further divided into narrowerTable 2Data on the corpora.
sent.
= sentence; epist.
= epistemic cue; dox.
= doxastic cue;inv.
= investigation cue; cond.
= condition cue.Data Set #sent.
#epist.
#dox.
#inv.
#cond.
TotalBioScope papers 7676 1373 220 295 187 2075BioScope abstracts 11797 2478 200 784 24 3486BioScope total 19473 3851 420 1079 211 5561WikiWeasel 20756 1171 909 94 491 3265FactBank 3123 305 201 36 178 720Total 43352 5927 1530 1209 880 9546347Computational Linguistics Volume 38, Number 2Figure 2Genres of texts.Figure 3Domains of texts.topics at a fine-grained level, which is shown in Figure 3.
All abstracts and five papersin BioScope are related to the MeSH terms human, blood cell, and transcription factor (hbcin Figure 3).
Nine BMC Bioinformatics papers come from the bioinformatics domain(bmc in Figure 3), and ten papers describe some experimental results on the Drosophilaspecies (fly).
FactBank news can be classified as stock news, political news, andcriminal news.
Encyclopedia articles cover a broad range of topics, hence no detailedclassification is given here.5.1.2 The Normalization of the Corpora.
In order to uniformly evaluate our methods ineach domain and genre (and each corpus), the evaluation data sets were normalized.This meant that cues had to be annotated in each data set and differentiated for typesof semantic uncertainty.
This resulted in the reannotation of BioScope, WikiWeasel, andFactBank.5 In BioScope, the originally annotated cues were separated into epistemiccues and subtypes of hypothetical cues and instances of hypothetical uncertainty notyet marked were also annotated.
In FactBank, epistemic and hypothetical cues wereannotated: Uncertain events were matched with their uncertainty cues and instancesof hypothetical uncertainty that were originally not annotated were also marked inthe corpus.
In the case of WikiWeasel, these two types of cues were separated fromdiscourse-level cues.One class of hypothetical uncertainty (i.e., dynamic modality) was not annotatedin any of the corpora.
Although dynamic modality seems to play a role in the newsdomain, it is less important and less represented in the other two domains we investi-gated here.
The other subclasses are more of general interest for the applications.
Forexample, one of our training corpora comes from the scientific domain, where it is moreimportant to distinguish facts from hypotheses and propositions under investigation5 The corpora are available at http://www.inf.u-szeged.hu/rgai/uncertainty.348Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertainty(which can be later confirmed or rejected, compare the meta-knowledge annotationscheme developed for biological events [Nawaz, Thompson, andAnaniadou 2010]), andfrom propositions that depend on each other (conditions).5.1.3 Uncertainty Cues in the Corpora.
An analysis of the cue distributions reveals someinteresting trends that can be exploited in uncertainty detection across domains andgenres.
The most frequent cue stems in the (sub)corpora used in our study can be seenin Table 3 and they are responsible for about 74% of epistemic cue occurrences, 55% ofdoxastic cue occurrences, 70% of investigation cue occurrences, and 91% of conditioncue occurrences.As can be seen, one of the most frequent epistemic cues in each corpus is may.
If,possible, might, and suggest also occur frequently in our data set.The distribution of the uncertainty cues was also analyzed from the perspective ofuncertainty classes in each corpus, which is presented in Figure 4.
Inmost of the corpora,epistemic cues are the most frequent (except for FactBank) and they vary the most:Out of the 300 cue stems occurring in the corpora, 206 are epistemic cues.
Comparingthe domains, it can readily be seen that in biological texts, doxastic uncertainty is notfrequent, which is especially true for abstracts, whereas in FactBank and WikiWeaselthey cover about 27% of the data.
The most frequent doxastic keywords exhibit somedomain-specific differences, however: In BioScope, the most frequent ones include puta-tive and hypothesis, which rarely occur in FactBank and WikiWeasel.
Nevertheless, cuesbelonging to the investigation class can be found almost exclusively in scientific texts(89% of them are in BioScope), which can be expected because the aim of scientific pub-lications is to examine whether a hypothesized phenomenon occurs.
Among the mostTable 3The most frequent cues in the corpora.
epist.
= epistemic cue; dox.
= doxastic cue; inv.
=investigation cue; cond.
= condition cue.Global Abstracts Full papers BioScope FactBank WikiWeaselEpist.
may 1508 suggest 616 may 228 suggest 810 may 43 may 721suggest 928 may 516 suggest 194 may 744 could 29 probable 112indicate 421 indicate 301 indicate 103 indicate 404 possible 26 suggest 108possible 304 appear 143 possible 84 appear 213 likely 24 possible 93appear 260 or 119 might 83 or 197 might 23 likely 80might 256 possible 101 or 78 possible 185 appear 15 might 78likely 221 might 72 can 73 might 155 seem 11 seem 67or 198 potential 72 appear 70 can 117 potential 10 could 55could 196 likely 60 likely 57 likely 117 probable 10 perhaps 51probable 157 could 56 could 56 could 112 suggest 10 appear 32Dox.
consider 276 putative 43 putative 37 putative 80 expect 75 consider 250believe 222 think 43 hypothesis 33 hypothesis 77 believe 25 believe 173expect 136 hypothesis 43 assume 24 think 66 think 24 allege 81think 131 believe 14 think 24 assume 32 allege 8 think 61putative 83 consider 10 expect 22 predict 26 accuse 7 regard 58Invest.
whether 247 investigate 177 whether 73 investigate 221 whether 26 whether 52investigate 222 examine 160 investigate 44 examine 183 if 3 if 20examine 183 whether 96 test 25 whether 169 remain to be seen 2 whether or not 7study 102 study 88 examine 23 study 101 question 1 assess 3determine 90 determine 67 determine 20 determine 87 determine 1 evaluate 3Cond.
if 418 if 14 if 85 if 99 if 65 if 254would 238 would 6 would 46 would 52 would 50 would 136will 80 until 2 will 20 will 20 will 21 will 39until 40 could 1 should 11 should 11 until 16 until 15could 30 unless 1 could 9 could 10 could 9 unless 14349Computational Linguistics Volume 38, Number 2Figure 4Cue type distributions in the corpora.frequent cues, investigate, examine, and study belong to this group.
These data revealthat the frequency of doxastic and investigation cues is strongly domain-dependent,and this explains the fact that the investigation vocabulary is very limited in Factbankand WikiWeasel.
Only about 10 cue stems belong to this uncertainty class in these cor-pora.
The set of condition cue stems, however, is very small in each corpus; altogether18 condition cue stems can be found in the data, although if and would are responsiblefor almost 75% of condition cue occurrences.
It should also be mentioned that thepercentage of condition cues is higher in FactBank than in the other corpora.Another interesting trend was observed when word forms were considered insteadof stemmed forms: Certain verbs in third person singular (e.g., expects or believes) occurmostly in FactBank and WikiWeasel.
The reason for this may be that when speakingabout someone else?s opinion in scientific discourse, the source of the opinion is usuallyprovided in the form of references or citations?usually at the end of the sentence?anddue to this, the verb is often used in the passive form, as in Example (9).
(9) It is currently believed that both RAG1 and RAG2 proteins were originallyencoded by the same transposon recruited in a common ancestor of jawedvertebrates [3,12,13,16].In contrast, impersonal constructions are hardly used in news media, where the ob-jective is to inform listeners about the source of the news presented as well in orderto enable them to judge the reliability of a piece of news.
Here, a clause including thesource and a communication verb is usually attached to the proposition.A genre-related difference between scientific abstracts and full papers is that con-dition cues can rarely be found in abstracts, although they occur more frequently inpapers (with the non-cue usage still being much more frequent).
Another difference isthe percentage of cues of the investigation type, which may be related to the structure350Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertaintyof abstracts.
Biological abstracts usually present the problem they examine and describemethods they use.
This entails the application of predicates belonging to the investiga-tion class of uncertainty.
It can be argued, however, that scientific papers also have thesecharacteristics but abstracts are much shorter than papers (generally, they contain about10?12 sentences).
Hence, investigation cues are responsible for a greater percentage ofcues.There are some lexical differences among the corpora that are related to domain orgenre specificity.
For instance, due to their semantics, the words charge, accuse, allege,fear, worry, and rumor are highly unlikely to occur in scientific publications, but theyoccur relatively often in news texts and in Wikipedia articles.
As for lexical divergencesbetween abstracts and papers, many of them are related to verbs of investigation andtheir different usage.
In the corpora, verbs of investigations were marked only if it wasnot clear whether the event/phenomenon would take place or not.
If it has alreadyhappened (The police are investigating the crime) or the existence of the thing underinvestigation can be stated with certainty, independently of the investigation (The topten organisms were examined), then they are not instances of hypotheses, so they werenot annotated.
As the data sets make clear, there were some candidates of investigationverbs that occurred in the investigation sense mostly in abstracts but in another sense inpapers, especially in the bmc data set (e.g.
assess or examine).
Evaluate also had a specialmathematical sense in bmc papers, which did not occur in abstracts.It can also be seen that some of the very frequent cues in papers do not occur (oronly relatively rarely) in abstracts.
This is especially true for the bmc data set, where can,if, would, could, and will are among the 15 most frequent cues and represent 23.21% ofcue occurrences, but only 3.85% in abstracts.
It is also apparent that the rate of epistemiccues is lower in bmc papers than in abstracts or other types of papers.Genre-dependent characteristics can be analyzed if BioScope abstracts and hbcpapers are compared because their fine-grained domain is the same.
Thus, it may beassumed that differences between their cues are related to the genre.
The sets of cuesused are similar, but the sense distributions may differ for certain ambiguous cues.
Forinstance, indicate mostly appears in the ?suggest?
sense in abstracts, whereas in papersit is used in the ?signal?
sense.
Another difference is that the percentage rate of doxasticcues is almost twice as high in papers as in abstracts (10.6% and 5.7%, respectively).Besides these differences, the two data sets are quite similar.Domain-related differences can be analyzed when the three subdomains of biolog-ical papers are contrasted.
As stressed earlier, bmc papers contain fewer instances ofepistemic uncertainty, but condition cues occur more frequently in them.
Nevertheless,fly and hbc papers are rather similar in these respects but hbc papers contain moreinvestigation cues than the other two subcorpora.
As regards lexical issues, the non-cueusage of possible in comparative constructions is more frequent in the bmc data set thanin the other papers and many occurrences of if in bmc are related to definitions, whichwere not annotated as uncertain.
On the basis of this information, the fly and the hbcdomains seem to be more similar to each other than to the BMC data set from a linguisticpoint of view.From the perspective of genre and domain adaptation, the following points shouldbe highlighted concerning the distribution of uncertainty cues across corpora.
Doxasticuncertainty is of primary importance in the news and encyclopedia domains whereasthe investigation class is characteristic of the biological domain.
Within the latter, thereis a genre-related difference as well: It is the epistemic and investigation classes thatare mainly present in abstracts whereas in papers cues belonging to other uncertaintyclasses can also be found.
Thus, when applying techniques developed for biological351Computational Linguistics Volume 38, Number 2texts or abstracts to news texts, for example, doxastic uncertainty cues deserve specialattention as it might well be the case that there are insufficient training examples for thisclass of uncertainty cues.
The adaptation of an uncertainty cue detector constructed forencyclopedia texts requires the special treatment of investigation cues, however, if, forinstance, scientific discourse is the target genre since they are underrepresented in thesource genre.5.2 Evaluation MetricsAs evaluation metrics, we used cue-level and sentence-level F?=1 scores for the uncer-tain class (the standard evaluation metrics of Task 1 of the CoNLL-2010 shared task)and denote them by Fcue and Fsent, respectively.
We report cue-level F?=1 scores on theindividual subcategories of uncertainty and the unlabeled (binary) F?=1 scores as well.A sentence is treated as uncertain (in the gold standard and prediction) iff it contains atleast one cue.
Note that the cue-level metric is quite strict as it is based on recognizedphrases?that is, only cues with perfect boundary matches are true positives.
For thesentence-level evaluation we simply labeled those sentences as uncertain that containedat least one recognized cue.5.3 Cross-Domain Cue Recognition ModelIn order to minimize the development cost of a labeled corpus and an uncertaintydetector for a new genre/domain, we need to induce an accurate model from a minimalamount of labeled data, or take advantage of existing corpora for different genresand/or domains and use a domain adaptation approach.
Experiments investigating thevalue and sufficiency of existing corpora (which are usually out-of-domain) and simpledomain adaptation methods were carried out.
For this purpose, we implemented a cuerecognition model, which is described in this section.To train our models, we applied surface level (e.g., capitalization) and shallowsyntactic features (part-of-speech tags and chunks) and avoided the use of lexicon-basedfeatures listing potential cue words, in order to reduce the domain dependence of thelearned models.
Now we will introduce our model, which is competitive with the state-of-the-art systems and focus on its domain adaptability.
We will also describe the im-plementation details of the learning model and the features employed.
We should addthat the optimization of a cue detector was not the main focus of our study, however.5.3.1 Feature Set.We extracted two types of features for each token to describe the tokenitself, together with its local context in a window of limited size (1, 2, or no window,depending on the feature).The first group consists of features describing the surface form of the tokens.
Herewe provide the list of the surface features with the corresponding window sizes: Stems of the tokens by the Porter stemmer in a window of size 2 (currenttoken and two tokens to the left and right). Surface pattern of the tokens in a window of size one (current tokenand 1 token to the left and right).
These patterns are similar to the wordshape feature described in Sun et al (2007).
This feature can describe thecapitalization and other orthographic features as well.
Patterns represent352Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertaintycharacter sequences of the same type with one single character for agiven word.
There are six different pattern types denoting capitalized andlowercased character sequences with the characters ?A?
and ?a?, numbersequences with ?0?, Greek letter sequences with ?G?
and ?g?, Romannumerals with ?R?
and ?r?, and non-alphanumerical characters with ?!
?. Prefixes and suffixes of word forms from three to five characters long.The second group of features describes the syntactic properties of the token and itslocal context.
The list of the syntactic features with the corresponding window sizes isthe following: Part-of-speech (POS) tags of the tokens by the C&C POS-tagger in awindow of size 2. Syntactic chunk of the tokens, as given by the C&C chunker,6 and thechunk code of the tokens in a window of size 2. Concatenated stem, POS, and chunk labels similar to the features usedby Tang et al (2010).
These feature strings were a combination of the stemand the chunk code of the current token, the stem of the current tokencombined with the POS-codes of the token left and right, and the chunkcode of the current token with the stems of the neighboring tokens.5.3.2 CoNLL-2010 Experiments.
The CoNLL-2010 shared task Learning to detect hedges andtheir scope in natural language text focused on uncertainty detection.
Two subtasks weredefined at the shared task: The first task sought to recognize sentences that contain someuncertain language in two different domains and the second task sought to recognizelexical cues together with their linguistic scope in biological texts (i.e., the text span interms of constituency grammar that covers the part of the sentence that is modifiedby the cue).
The lexical cue recognition subproblem of the second task7 is identicalto the problem setting used in this study, with the only major difference being thetypes of uncertainty addressed: In the CoNLL-2010 task biological texts contained onlyepistemic, doxastic, and investigation types of uncertainty.
Apart from these differences,the CoNLL-2010 shared task offers an excellent testbed for comparing our uncertaintydetection model with other state-of-the-art approaches for uncertainty detection and tocompare different classification approaches.
Here we present our detailed experimentsusing the CoNLL data sets, analyze the performance of our models, and select the mostsuitable models for further experiments.CoNLL systems.
The uncertainty detection systems that were submitted to the CoNLLshared task can be classified into three major types.
The first set of systems treats theproblem as a sentence classification task, that is, one to decide whether a sentencecontains any uncertain element or not.
These models operate at the sentence level andare unsuitable for cue detection.
The second group handles the problem as a token6 POS-tagging and chunking were performed on all corpora using the C&C Tools (Curran, Clark, andBos 2007).7 As an intermediate level, participants of the first task could submit the lexical cues found in sentencesfor evaluation, without their scope, which gave some insight into the nature of cue detection on theWikipedia corpus (where scope annotation does not exist) as well.353Computational Linguistics Volume 38, Number 2Table 4Results on the original CoNLL-2010 data sets.
The first three rows correspond to our baseline,token-based, and sequence labeling models.
The BEST/SEQ row shows the results of the bestsequence labeling approach of the CoNLL shared task (for both domains), the BEST/TOK rowsshow the best token-based models, and the BEST/SENT rows show the best sentence-levelclassifiers (these models did not produce cue-level results).BIOLOGICAL WIKIPEDIAFcue Fsent Fcue FsentBASELINE 74.5 81.4 19.5 58.6TOKEN/MAXENT 79.7 85.8 22.3 58.1SEQUENCE/CRF 81.4 87.0 32.7 47.0BEST/SEQ (Tang et al 2010) 81.3 86.4 36.5 55.0BEST/TOK BIO (Velldal, ?vrelid, and Oepen 2010) 78.7 85.2 ?
?BEST/TOKWIKI (Morante, Van Asch, and Daelemans 2010) 76.7 81.7 11.3 57.3BEST/SENT BIO (Ta?ckstro?m et al 2010) ?
85.2 ?
55.4BEST/SENTWIKI (Georgescul 2010) ?
78.5 ?
60.2classification task, and classifies each token independently as uncertain (or not).Contextual information is only included in the form of feature functions.
The thirdgroup of systems handled the task as a sequential token labeling problem, that is, de-termined the most likely label sequence of a sentence in one step, taking the informa-tion about neighboring labels into account.
Sequence labeling and token classificationapproaches performed best for biological texts and sentence-level models and tokenclassification approaches gave the best results for Wikipedia texts (see Table 6 in Farkaset al [2010]).
Here we compare a state-of-the-art token classification and sequencelabeling approach using a shared feature representation to decide which model to usein further experiments.Classifier models.
We used a first-order linear chain conditional random fields (CRF)model as a sequence labeler and a Maximum Entropy (Maxent) classifier model as atoken classifier, implemented in the Mallet (McCallum 2002) package for training theuncertainty cue detectors.
This choicewasmotivated by the fact that thesewere themostpopular classification approaches among the CoNLL-2010 participants, and that CRFmodels are known to provide high accuracy for the detection of phrases with accurateboundaries (e.g., in named entity recognition).
We trained the CRF and Maxent modelswith their default settings in Mallet for 200 iterations or until convergence (CRF), andalso until convergence (Maxent) in each experimental set-up.As a baseline model, we applied a simple dictionary-based approach which clas-sifies every uni- and bigram as uncertain that is tagged as uncertain in over 50% ofthe cases in the training data.
Hence, it is a similar system to that presented by TjongKim Sang (2010), without tuning the decision threshold for predicting uncertainty.CoNLL results.
An overview of the results achieved on the CoNLL-2010 data sets canbe found in Table 4.
A comparison of our models with the CoNLL systems reveals thatour uncertainty detection model is very competitive when applied on the biologicaldata set.
Our CRF model trained on the official training data set of the shared taskachieved a cue-level F-score of 81.4 and sentence-level F-score of 87.0 on the biological354Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertaintyevaluation data set.
These results would have come first in the shared task, with amarginal difference compared to the top performing participant.
In contrast, our modelis less competitive on the Wikipedia data set: The Maxent model achieved a cue-levelF-score of 22.3 and sentence-level F-score of 58.1 on the Wikipedia evaluation dataset, whereas our CRF model was not competitive with the best participating systems.The observation that sequence-labeling models perform worse than token-basedapproaches on Wikipedia, especially for sentence-level evaluation measures, coincideswith the findings of the shared task: The discourse-level uncertainty cues in theWikipedia data set are rather long and heterogeneous and sequence labeling modelsoften revert to not annotating any token in a sentence when the phrase boundaries arehard to detect.
Still, sequence labeling models have an advantage in terms of cue-levelaccuracy.
This is not surprising because CRF is a state-of-the-art model for chunking /sequence labeling tasks.We conclude from Table 4 that our model is competitive with the state-of-the-artsystems for detecting semantic uncertainty (which is closer to the biological subtask),but it is less suited to recognizing discourse-level uncertainty.
In the subsequent exper-iments we used our CRF model, which performed best in detecting uncertainty cues innatural language sentences.5.3.3 Domain Adaptation Model.
In supervised machine learning, the task is to learn howto make predictions on previously unseen, new examples based on a statistical modellearned from a collection of labeled training examples (i.e., a set of examples coupledwith the desired output for them).
The classification setting assumes a set of labels L, aset of features X, and a probability distribution p(X) describing the examples in terms oftheir features.
Then the training examples are assumed to be given in the form of {xi, li}pairs and the goal of classification is to estimate the label distribution p(L|X), which canbe used later on to predict the labels for unseen examples.Domain adaptation focuses on the problem where the same (or a closely related)learning task has to be solved in multiple domains which have different characteristicsin terms of their features: The set of features X may be different or the probability dis-tributions p(X) describing the inputs may be different.
When the target tasks are treatedas different (but related), the label distribution p(L|X) is dependent on the domain.
Thatis, given a domain d, the problem can be formalized as modeling p(L|X)d based on Xd,p(X)d and a set of examples: {xi,d, li}.8 In the context of domain adaptation, there is atarget domain t and a source domain s, with labeled data available for both, and thegoal is to induce a more accurate target domain model p(L|X)t from {xi,t, li} ?
{xi,s, li}than the one learned from {xi,t, li} only.
In practical scenarios, the goal is to exploit thesource data to acquire an accurate model from just limited target data which are aloneinsufficient to train an accurate in-domain model, and thus to port the model to a newdomain with moderate annotation costs.
The problem is difficult because it is nontrivialfor a learning method to account for the different data (and label) distributions betweentarget and source, which causes a remarkable drop in model accuracy when it is appliedto classifying examples taken from the target domain.In our experimental context, both topic- and genre-related differences of texts posean adaptation problem as these factors have an impact on both the vocabulary (p(X))and the sense distributions of the cues (p(L|X)) found in different texts.
There is some8 The literature also describes the case when the set of labels depends on the domain, but we omit this caseto simplify our notation and discussion.
For details, see Pan and Yang (2010).355Computational Linguistics Volume 38, Number 2confusion in the literature regarding the terminology describing the various domainmismatches in the learning problem.
For example, Daume?
III (2007) describes a domainadaptation method where he assumes that the label distribution is unchanged (we notehere that this assumption is not exploited in the method, and that the label distributionchanges in our problem), whereas Pan and Yang (2010) uses the term inductive transferlearning to refer to our scenario (in their paper, domain adaptation refers to a differentsetting).9 In this study we always use the term domain adaptation to refer to our problemsetting, that is, where both p(X) and p(L|X) are assumed to change.In our experiments, we used various data sets taken from multiple genres anddomains (see Section 5.1.1 for an overview) and applied a simple but effective do-main adaptation model (Daume?
III 2007) for training our classifiers.
In this model,domain adaptation is carried out by defining each feature over the target and sourcedata sets twice?just once for target domain instances, and once for both the tar-get and source domain instances.
Formally, having a target domain t and a sourcedomain s and n features {f1, f2, .
.
.
fn}, for each fi we have a target-only version fi,tand a shared version fi,t+s.
Each target domain example is described by 2n features:{ f1,t, f2,t, .
.
.
fn,t, f1,t+s, f2,t+s, .
.
.
fn,t+s} and source domain examples are described by onlythe n shared features: { f1,t+s, f2,t+s, .
.
.
fn,t+s}.
Using the union of the source and targettraining data sets {xi,t, li} ?
{xi,s, li} and this feature representation, any standard super-vised machine learning technique can be used and it becomes possible for the algorithmto learn target-dependent and shared patterns at the same time and handle the changesin the underlying distributions.
This easy domain adaptation technique has been foundto work well in many NLP-oriented tasks.
We used the CRF models introduced hereinand in this way, we were able to exploit feature?label correspondences across domains(for features that behave consistently across domains) and also to learn patterns specificto the target domain.5.4 Cross-Domain and Genre ExperimentsWe defined several settings (target and source pairs) with varied domain and genredistances and target data set sizes.
These experiments allowed us to study the po-tential of transferring knowledge across existing corpora for the accurate detection ofuncertain language in a wide variety of text types.
In our experiments, we used all thecombinations of genres and domains that we found plausible.
News texts (and theirsubdomains) were not used as source data because FactBank is significantly smallerthan the other corpora (WikiWeasel or scientific texts).
As the source data set is typicallylarger than the target data set in practical scenarios, news texts can only be used as targetdata.
Abstracts were only used as source data because information extraction typicallyaddresses full texts whereas abstracts just provide annotated data for development pur-poses.
Besides these restrictions, we experimented with all possible target and sourcepairs.We used four different machine-learning settings for each target?source pair in ourinvestigations.
In the purely cross-domain (CROSS) setting, the model was trained onthe source domain and evaluated on the target (i.e., no labeled target domain datasets were used for training).
In the purely in-domain setting (TARGET), we performed9 More on this can be found in Pan and Yang (2010) and at http://nlpers.blogspot.com/2007/11/domain-adaptation-vs-transfer-learning.html.356Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic UncertaintyTable 5Experimental results on different target and source domain pairs.
The third column contains theratio of the target train and source data sets?
sizes in terms of sentences.
DIST shows the distanceof the source and target domain/genre (?-?
same; ?+?
fine-grade difference; ?++?
coarse-gradedifference; bio = biological; enc = encyclopedia; sci paper = scientific paper; sci abs = scientificabstract; sci paper hbc = scientific papers on human blood cell experiments; sci paper fly =scientific papers on Drosophila; sci paper bmc = scientific papers on bioinformatics).CROSS TARGET DA/ALL DA/CUETARGET SOURCE SOURCETARGETDIST Fcue Fsent Fcue Fsent Fcue Fsent Fcue Fsentenc sci paper+ abs 0.9 ++/++ 68.0 74.2 82.4 87.4 82.6 87.6 82.6 87.6news sci paper+ abs 6.2 ++/++ 64.4 70.5 68.7 77.1 72.7 79.5 73.8 81.0news enc 6.6 ++/++ 68.2 74.8 68.7 77.1 73.7 81.2 73.1 80.0sci paper enc 2.7 ++/++ 67.8 75.1 78.8 84.4 80.0 85.9 79.8 85.4sci paper bmc sci abs hbc 4.3 +/+ 58.2 70.5 64.0 74.5 68.1 76.7 69.3 77.8sci paper fly sci abs hbc 3.4 +/+ 70.5 79.1 80.0 85.1 83.3 88.2 82.9 87.8sci paper hbc sci abs hbc 8.2 ?/+ 76.5 82.9 74.2 80.2 84.2 88.6 83.0 88.9sci paper bmc sci paper fly+ hbc 1.8 +/?
69.8 77.6 64.0 74.5 70.0 78.2 69.4 78.1sci paper fly sci paper bmc+ hbc 1.2 +/?
78.4 83.5 80.0 85.1 82.6 87.0 82.9 87.0sci paper hbc sci paper bmc+ fly 4.4 +/?
81.7 85.9 74.2 80.2 80.7 86.9 80.7 85.9AVERAGE: 70.4 77.4 73.5 80.6 77.8 84.0 77.8 84.010-fold cross-validation on the target data (i.e., no source domain data were used).
Inthe two domain adaptation settings, we again performed 10-fold cross-validation onthe target data but exploited the source data set (as described in Section 5.3).
Here, weeither used each sentence of the source data set (DA/ALL) or only those sentences thatcontained a cue observed in the target train data set (DA/CUE).Table 5 lists the results obtained on various target and source domains in variousmachine learning settings and Table 6 contains the absolute differences between aparticular result and the in-domain (TARGET) results.Fine-grained semantic uncertainty classification results are summarized in Tables 7and 8.
Table 7 contrasts the coarse-grained Fcue with the unlabeled/binary Fcue of fine-grained experiments, therefore it quantifies the difference in accuracy due to the moredifficult classification setting and the increased sparseness of the task.
Table 8 showsthe per class Fcue scores, namely, how accurately our model recognizes the individualuncertainty types.Table 6The absolute difference between the F-scores of Table 5 relative to the baseline TARGET setting.CROSS TARGET DA/ALL DA/CUETARGET SOURCE SOURCETARGETDIST Fcue Fsent Fcue Fsent Fcue Fsent Fcue Fsentenc sci paper+ abs 0.9 ++/++ ?14.4 ?13.2 82.4 87.4 0.2 0.2 0.2 0.2news sci paper+ abs 6.2 ++/++ ?4.3 ?6.6 68.7 77.1 4.0 2.4 5.1 3.9news enc 6.6 ++/++ ?0.5 ?2.3 68.7 77.1 5.0 4.1 4.4 2.9sci paper enc 2.7 ++/++ ?11.0 ?9.3 78.8 84.4 1.2 1.5 1.0 1.0sci paper bmc sci abs hbc 4.3 +/+ ?5.8 ?4.0 64.0 74.5 4.1 2.2 5.3 3.3sci paper fly sci abs hbc 3.4 +/+ ?9.5 ?6.0 80.0 85.1 3.3 3.1 2.9 2.7sci paper hbc sci abs hbc 8.2 ?/+ 2.3 2.7 74.2 80.2 10.0 8.4 8.8 8.7sci paper bmc sci paper fly+ hbc 1.8 +/?
5.8 3.1 64.0 74.5 6.0 3.7 5.4 3.6sci paper fly sci paper bmc+ hbc 1.2 +/?
?1.6 ?1.6 80.0 85.1 2.6 1.9 2.9 1.9sci paper hbc sci paper bmc+ fly 4.4 +/?
7.5 5.7 74.2 80.2 6.5 6.7 6.5 5.7AVERAGE: ?3.1 ?3.2 73.5 80.6 4.3 3.4 4.3 3.4357Computational Linguistics Volume 38, Number 2Table 7Comparison of cue-level binary (Fbin) and unlabeled F-scores (Funl).
Binary F-score correspondsto coarse-grained classification (uncertain vs. certain), and unlabeled F-score is the fine-grainedclassification converted to binary (disregarding the fine-grained category labels).CROSS TARGET DA/ALL DA/CUETARGET SOURCE SOURCETARGETDIST Fbin Funl Fbin Funl Fbin Funl Fbin Funlenc sci paper+ abs 0.9 ++/++ 68.0 67.4 82.4 82.4 82.6 81.9 82.6 81.7news sci paper+ abs 6.2 ++/++ 64.4 59.9 68.7 66.4 72.7 71.5 73.8 71.8news enc 6.6 ++/++ 68.2 67.0 68.7 66.4 73.7 73.6 73.1 73.4sci paper enc 2.7 ++/++ 67.8 67.2 78.8 78.3 80.0 80.2 79.8 79.5sci paper bmc sci abs hbc 4.3 +/+ 58.2 66.3 64.0 61.9 68.1 68.5 69.3 67.9sci paper fly sci abs hbc 3.4 +/+ 70.5 78.7 80.0 79.2 83.3 83.4 82.9 83.2sci paper hbc sci abs hbc 8.2 ?/+ 76.5 83.6 74.2 69.3 84.2 83.1 83.0 83.4sci paper bmc sci paper fly+ hbc 1.8 +/?
69.8 69.7 64.0 61.9 70.0 69.5 69.4 65.9sci paper fly sci paper bmc+ hbc 1.2 +/?
78.4 77.7 80.0 79.2 82.6 82.1 82.9 82.5sci paper hbc sci paper bmc+ fly 4.4 +/?
81.7 81.9 74.2 69.3 80.7 81.3 80.7 81.2AVERAGE: 70.4 71.9 73.5 71.4 77.8 77.5 77.8 77.0Table 8The per class cue-level F-scores in fine-grained classification.
Fcrs, Ftgt, and Fda correspond to theCROSS, TARGET, and DA/CUE settings, respectively (same as previous).
The DA/ALL settingis not shown for space reasons and due to its similarity to the DA/CUE results.EPISTEMIC INVESTIGATION DOXASTIC CONDITIONTARGET SOURCE Fcrs Ftgt Fda Fcrs Ftgt Fda Fcrs Ftgt Fda Fcrs Ftgt Fdaenc sci paper+ abs 75.9 83.4 82.8 67.3 67.5 70.4 48.8 89.2 88.1 54.4 62.6 61.2news sci paper+ abs 70.9 65.4 75.2 79.5 75.9 83.1 39.1 68.9 71.3 47.2 57.1 57.5news enc 65.4 65.4 74.5 74.6 75.9 87.5 76.3 68.9 78.0 50.6 57.1 56.7sci paper enc 72.9 81.2 81.9 36.5 72.9 72.4 63.6 74.9 79.8 57.0 58.9 59.7sci paper bmc sci abs hbc 71.5 68.3 72.6 56.1 37.7 58.1 68.1 61.9 69.4 45.5 45.0 49.5sci paper fly sci abs hbc 82.9 82.1 85.3 69.0 68.6 76.6 75.1 71.7 75.4 28.6 63.4 64.1sci paper hbc sci abs hbc 87.5 77.7 86.4 76.5 53.5 77.5 80.6 39.0 76.7 26.1 10.0 33.3sci paper bmc sci paper fly+ hbc 74.4 68.3 69.2 55.9 37.7 57.4 63.7 61.9 64.7 57.3 45.0 50.7sci paper fly sci paper bmc+ hbc 80.3 82.1 84.3 66.7 68.6 75.8 77.7 71.7 77.3 53.5 63.4 68.0sci paper hbc sci paper bmc+ fly 85.2 77.7 86.0 74.0 53.5 70.3 75.9 39.0 70.2 58.1 10.0 41.4AVERAGE: 76.7 75.2 79.8 65.6 61.2 72.9 66.9 64.7 75.1 47.8 47.3 54.2The size of the target training data sets proved to be an important factor in theseinvestigations.
Hence, we performed experiments with different target data set sizes.We utilized the DA/ALL model (which is more robust for extremely small target datasizes [e.g., 100-400 sentences]) and performed the same 10-fold cross validation on thetarget data set as in Tables 5-8.
For each fold of the cross-validation here, however, wejust used n sentences (x axis of the figures) from the target training data set and a fixedset of 4,000 source sentences to alleviate the effect of varying data set sizes.
Figure 5depicts the learning curves for two target/source data set pairs.6.
DiscussionAs Table 5 shows, incorporating labeled data from different genres and/or domainsconsistently improves the performance.
The successful applicability of domain adap-tation tells us that the problem of detecting uncertainty has similar characteristicsacross genres and domains.
The uncertainty cue lexicons of different domains and358Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic UncertaintyFigure 5Learning curves: Results achieved with different target train sizes.
The left and right figuresshow two selected source/target pairs.
The upper figures depict coarse-grained classificationresults (Fcue); DA, CROSS, and TARGET with the same settings as in Table 5.
The lower figuresshow the per class Fcue of the DA/ALL model in the fine-grained classification.genres indeed share a core vocabulary and despite the differences in sense distributions,labeled data from a different source improves uncertainty classification in a new genreand domain if the different data sets are annotated consistently.
This justifies our aimto create a consistent representation of uncertainty that can be applied to multipledomains.6.1 Domain Adaptation ResultsThe size of the target and source data sets largely influences to what extent externaldata can improve results.
The only case where domain adaptation had only a negligibleeffect (an F-score gain less than 1%) is where the target data set is itself very large.This is expected as the more target data one has, the less crucial it is to incorporateadditional data with some undesirable characteristics (difference in style, domain,certain/uncertain sense distribution, etc.
).The performance scores for the CROSS setting clearly indicate the domain/genredistance of the data sets: Themore distant the domain and genre of the source and targetdata sets are, the more the CROSS performance (where no labeled target data is used)degrades, compared with the TARGET model.
In general, when the distance betweenboth the domain and the genre of texts is substantial (++/++ and +/+ rows in Tables 5359Computational Linguistics Volume 38, Number 2and 6), this accounts for a 6?10% decrease in both the sentence and cue-level F-scores.An exception is the case of encyclopedic source and news target domains.
Here theperformance is very close to the target domain performance.
This indicates that thesesettings are not so different from each other as it might seem at the first glance.
Theencyclopedic and news genres share quite a lot of commonalities (compare cue distribu-tions in Figure 4, for instance).
We verified this observation by using a knowledge-poorquantitative estimator of similarity between domains (Van Asch and Daelemans 2010):Using cosine as the similarity measure, the newswire and encyclopedia texts are foundto be the second most similar domain pair in our experiments, with a score comparableto those obtained for the pairs of scientific article types bmc, hbc, and fly.When there is a domain or genre match between source and target (?/+ and +/?rows in Tables 5 and 6), however, and the distance regarding the other is just moderate,the cross-training performance is close to or even better than the target-only results.That is, the larger amount of source training data balances the differences between thedomains.
These results indicate that the learned uncertainty classifiers can be directlyapplied to slightly different data sets.
This suitability is due to the learned disambigua-tion models, which generalize well in similar settings.
This is contrary to the findingsof earlier studies, which built the uncertainty detectors using seed examples and boot-strapping.
These models were not designed to learn any disambiguation models forthe cue words found, and their performance degraded even for slightly different data(Szarvas 2008).Comparing the two domain adaptation procedures DA/CUE and DA/ALL, adap-tation via transferring only source sentences that contain a target domain cue is, onaverage, comparable to transferring all the data from the source domain.
In other words,when we have a small but sufficient amount of target data available, it is enough toaccount for source data corresponding to the uncertainty cues we saw in the limitedtarget data set.
This observation has several consequences, namely: The source-only cues, or to be more precise, their disambiguation models,are not helpful for the target domains as they cannot be adapted.
This isdue to the differences in the source and target disambiguation models. Similarly, domain adaptation improves the disambiguation models for theobserved target cues, rather than introducing new vocabulary into thetarget domain.
This mechanism coincides with our initial goal of usingdomain adaptation to learn better semantic models.
This effect is theopposite of how bootstrapping-based weakly supervised approachesimprove the performance in an underresourced domain.
This observationsuggests a promising future direction of combining the two approaches tomaximize the gains while minimizing the annotation costs. In a general context, we can effectively extend the data for a given domainif we have robust knowledge of the potential uncertainty vocabulary forthat domain.
Given the wide variety of the domains and genres of our datasets, it is reasonable to suppose that they represent uncertain language ingeneral quite well, and the joint vocabularies provide a good starting pointfor a targeted data development for further domains.As regards the fine-grained classification results, Table 7 demonstrates that thefine-grained distinction results in only a small, or no, loss in performance.
The coarse-grained model is slightly more accurate than the fine-grained model (counting correctly360Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertaintyrecognized but misclassified cues as true positives) in most settings.
The most signif-icant difference is observed for the target-only settings, where no out-of-domain dataare used for the training and thus the data sets are accordingly smaller.
A noticeableexception is when scientific abstracts are used for cross training: In those settings thecoarse-grained model performs poorly, due to its lower recall, which we attribute tooverfitting the special characteristics of abstracts.
The fact that in fine-grained classi-fication the CROSS results consistently outperform the TARGET models (see Table 8)even for distant domain pairs, also underlines that the increased sparseness caused bythe differentiation of the various subtypes of uncertainty is an important factor only forsmaller data sets.
The improvement by domain adaptation is clearly more prominentin fine-grained than in coarse-grained classification, however: The individual cue typesbenefit by 5?10% points in terms of the F-score from out-of-domain data and domainadaptation.
Moreover, as Table 8 shows, for the domain pairs and fine-grained classeswhere a nice amount of positive examples are at hand, the per class Fcue scores arealso around 80% and above.
This means that it is possible to accurately identify theindividual subtypes of semantic uncertainty, and thus it also proves the feasibility ofthe subcategorization and annotation scheme proposed in this study (Section 2).
Otherimportant observations here are that domain adaptation is even more significant in themore difficult fine-grained classification setting, and that the condition class representsa challenge for our model.
The performance for the condition class is lower than thatfor the other classes, which can only in part be attributed to the fact that this is theleast represented subtype in our data sets: as opposed to other cue types, condition cuesare typically used in many different contexts and they may belong to other uncertaintyclasses as well.6.2 The Required Amount of AnnotationBased on our experiments, we may conclude that a manually annotated training dataset consisting of 3,000?5,000 sentences is sufficient for training an accurate cue detectorfor a new genre/domain.
The results of our learning curve experiments (Figure 5)illustrate the situations where only a limited amount of annotated data (fewer than 3,000sentences) is available for the target domain.
The feasibility of decreasing annotationefforts and the real added value of domain adaptation are more prominent in this range.It is easy to see that the TARGET results approach to DA results with more target data.Figure 5 shows that the size of the target training data set where the supervisedTARGET setting outperforms the CROSS model (trained on 4,000 source sentences) isaround 1,000 sentences.
Aswementioned earlier, even distant domain data can improvethe cue recognition model in the absence of a sufficient target data set.
Figure 5 justifiesthis observation, as the CROSS and DA settings outperform the TARGET setting oneach source?target data set pair.
It can also be observed that the doxastic type is moredomain-dependent than the others and its results consistently improve by increasingthe size of the target domain annotation (which coincides with the cue frequencyinvestigations of Section 5.1.3).
In the news target domain, however, the investigationand epistemic classes benefit a lot from a small amount of annotated target data but theirperformance scores increase just slightly after that.
This indicates that most of the im-portant domain-dependent (probably lexical) knowledge could be gathered from 100?400 sentences.
In the biological experiments, we may conclude that the investigationclass is already covered by the source domain (intuitively, the investigation cues are wellrepresented in the abstracts) and its results are not improved significantly by usingmore361Computational Linguistics Volume 38, Number 2target data.
The condition class is underrepresented in both the source and target datasets and hence no reliable observations can bemade regarding this subclass (see Table 2).Overall, if we would like to have an uncertainty cue detector for a newgenre/domain: (i) We can achieve performance around 60?70% by using cross trainingdepending on the difference between the domains (i.e., without any annotation effort);(ii) By annotating around 3,000 sentences, we can have a performance of 70?80%,depending on the level of difficulty of the texts; (iii) We can get the same 70?80% resultswith annotating just 1,000 sentences and using domain adaptation.6.3 Interesting Examples and Error AnalysisAs might be expected, most of the erroneous cue predictions were due to vocabularydifferences, for example, fear or accuse occurred only in news texts, which is why theywere not recognized by models trained on biological or encyclopedia texts.
Anotherexample is the case of or, which is a frequent cue in biological texts.
Still, it is rarelyused as a cue in other domains but without domain adaptation, the model trained onbiological texts marks quite a few occurrences of or as cues in the news or encyclope-dia domains.
Many of these anomalies were eliminated by the application of domainadaptation techniques, however.Many errors were related to multi-class cues.
These cues are especially hard todisambiguate because not only can they refer to several classes of uncertainty, butthey typically have non-cue usage as well.
For instance, the case of would is rathercomplicated because it can fulfill several functions:(10) EPISTEMIC USAGE (?IT IS HIGHLY PROBABLE?
): Further biochemical studieson the mechanism of action of purified kinesin-5 from multiple systemswould obviously be fruitful.
(Corpus: fly)(11) CONDITIONAL: ?If religion was a thing that money could buy,/The richwould live and the poor would die.?
(Corpus: WikiWeasel)(12) FUTURE IN THE PAST: This Aarup can trace its history back to 1500, but itwould be 1860?s before it would become a town.
(Corpus: WikiWeasel)(13) REPEATED ACTION IN THE PAST (?USED TO?
): ?Becker?
was the next T.V.Series for Paramount that Farrell would co-star in.
(Corpus: WikiWeasel)(14) DYNAMIC MODALITY: Individuals would first have a small lesion at thesite of the insect bite, which would eventually leave a small scar.
(Corpus:WikiWeasel)(15) PRAGMATIC USAGE: Although some would dispute the fact, the jokerelated to a peculiar smell that follows his person.
(Corpus: WikiWeasel)The epistemic uses of would are annotated as epistemic cues whereas its occurrences inconditionals are marked as hypothetical cues.
The habitual past meaning is not relatedto uncertainty, hence it is not annotated.
The future in the past meaning (i.e., past tenseof will), however, denotes an event of which it is known that happened later, so it iscertain.
The dynamically modal would is similar to the future will (which is an instanceof dynamic modality as well), but it is not annotated in the corpora.
The pragmatic362Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertaintyuse of would does not refer to semantic uncertainty (the semantic value of the sentencewould be exactly the same without it or if it is replaced with may, might, will, etc., thatis, some will/may/might/?
dispute the fact mean the same).
It is rather a stylistic issueto further express uncertainty at the discourse level (i.e., there are some unidentifiedpeople who dispute the fact, hence the opinion cannot be associated with any definitesource).The last two uses of would are not typically described in grammars of Englishand seem to be characteristic primarily of the news and encyclopedia domains.
Thusit is advisable to explore such cases and treat them with special consideration whenadapting an algorithm trained and tested in a specific domain to another domain.Another interesting example is may in its non-cue usage.
Being (one of) the mostfrequent cues in each subcorpus, its non-cue usage is rather limited but can be foundoccasionally in FactBank and WikiWeasel.
The following instance of may in FactBankwas correctly marked as non-cue by the cue detector when trained on Wikipedia texts.On the other hand, it was marked as a cue when trained on biological texts since in thiscase, there were insufficient training examples of may not being a cue:(16) ?Wellmay we say ?God save the Queen,?
for nothing will save therepublic,?
outraged monarchist delegate David Mitchell said.
(Corpus:FactBank)A final example to be discussed is concern.
This word also has several uses:(17) NOUN MEANING ?COMPANY?
: The insurance concern said all conversionrights on the stock will terminate on Nov. 30.
(Corpus: FactBank)(18) NOUN MEANING ?WORRY?
: Concern about declines in other markets,especially New York, caused selling pressure.
(Corpus: FactBank)(19) PREPOSITION: The company also said it continues to explore all optionsconcerning the possible sale of National Aluminum?s 54.5% stake in analuminum smelter in Hawesville, Ky. (Corpus: FactBank)(20) VERB: Many of the predictions in these two data sets concern protein pairsand proteins that are not present in other data sets.
(Corpus: bmc)Among these examples, only the second one should be annotated as uncertain.
POS-tagging seems to provide enough information for excluding the verbal and preposi-tional uses of the word but in the case of nominal usage, additional information is alsorequired to enable the system to decide whether it is an uncertainty cue or not (in thiscase, the noun in the ?company?
sense cannot have an argument while in the ?worry?sense, it can have [about declines]).
Again, the frequency of the two senses dependsheavily on the domain of the texts, which should also be considered when adaptingthe cue detector to a different domain.
We should mention that the role of POS-taggingis essential in cue detection because many ambiguities can be resolved on the basis ofPOS-tags.
Hence, POS-tagging errors can lead to a serious decline in performance.We think that an analysis of similar examples can further support domain adapta-tion and cue detection across genres and domains.363Computational Linguistics Volume 38, Number 27.
Conclusions and Future WorkIn this article, we introduced an uncertainty cue detection model that can perform wellacross different domains and genres.
Even though several types of uncertainty exist,available corpora and resources focus only on some of the possible types and therebyonly cover particular aspects of the phenomenon.
This means that uncertainty modelsfound in the literature are heterogeneous, and the results of experiments on differentcorpora are hardly comparable.
These facts motivated us to offer a unified model ofsemantic uncertainty enhanced by linguistic and computer science considerations.
Inaccordance with this classification, we reannotated three corpora from several domainsand genres using our uniform annotation guidelines.Our results suggest that simple cross training can be employed and it achieves areasonable performance (60?70% cue-level F-score) when no annotated data is at handfor a new domain.
When some annotated data is available (here somemeans fewer than3,000 annotated sentences for the target domain), domain adaptation techniques arethe best choice: (i) they lead to a significant improvement compared to simple crosstraining, and (ii) they can provide a reasonable performance with significantly lessannotation.
In our experiments, the annotation of 3,000 sentences and training only onthese is roughly equivalent to the annotation of 1,000 sentences using external data anddomain adaptation.
If the size of the training data set is sufficiently large (larger than5,000 sentences) the effect of incorporating additional data?having some undesirablecharacteristics?is not crucial.Comparing different domain adaptation techniques, we found that similar resultscould be attained when the source domain was filtered for sentences that containedcues in the target domain.
This tells us that models learn to better disambiguate thecues seen in the target domain instead of finding new, unseen cues.
In this sense, thisapproach can be regarded as a complementarymethod toweakly supervised techniquesfor lexicon extraction.
A promising way to further minimize annotation costs whilemaximizing performance would be the integration of the two approaches, which weplan to investigate in the near future.In our study, we did not pay attention to dynamic modality (due to the lack of an-notated resources), but the detection of such phenomena is also desirable.
For instance,dynamically modal events cannot be treated as certain?that is, the event of buyingcannot be assigned the same truth value in They agreed to buy the company and Theybought the company.
Whereas the second sentence expresses a fact, the first one informsus about the intention of buying the company, which will be certainly carried out in aworld where moral or business laws are observed but at the moment it cannot be statedwhether the transaction takes place (i.e., that it is certain).
Hence, in the future, we alsointend to integrate the identification of dynamically modal cues into our uncertaintycue detector.AcknowledgmentsThis work was supported in part bythe NIH grants (project codenamesMASZEKER and BELAMI) of theHungarian government, by the GermanMinistry of Education and Researchunder grant SiDiM (grant no.
01IS10054G),and by the Volkswagen Foundation aspart of the Lichtenberg-ProfessorshipProgram (grant no.
I/82806).
Richa?rdFarkas was funded by the DeutscheForschungsgemeinschaft grant SFB 732.ReferencesBaker, Kathy, Michael Bloodgood, MonaDiab, Bonnie Dorr, Ed Hovy, Lori Levin,Marjorie McShane, Teruko Mitamura,364Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic UncertaintySergei Nirenburg, Christine Piatko,Owen Rambow, and Gramm Richardson.2010.
Modality Annotation Guidelines.Technical Report 4, Human LanguageTechnology Center of Excellence,Baltimore, MD.Chapman, Wendy W., David Chu, andJohn N. Dowling.
2007.
ConText: Analgorithm for identifying contextualfeatures from clinical text.
In Proceedingsof the ACL Workshop on BioNLP 2007,pages 81?88, Prague, Czech Republic.Clausen, David.
2010.
HedgeHunter:A system for hedge detection anduncertainty classification.
In Proceedings ofthe Fourteenth Conference on ComputationalNatural Language Learning (CoNLL-2010):Shared Task, pages 120?125, Uppsala.Conway, Mike, Son Doan, and Nigel Collier.2009.
Using hedges to enhance a diseaseoutbreak report text mining system.
InProceedings of the BioNLP 2009 Workshop,pages 142?143, Boulder, CO.Curran, James, Stephen Clark, and JohanBos.
2007.
Linguistically motivatedlarge-scale NLP with C&C and Boxer.
InProceedings of the 45th Annual Meeting of theAssociation for Computational LinguisticsCompanion Volume Proceedings of the Demoand Poster Sessions, pages 33?36, Prague.Daume?
III, Hal.
2007.
Frustratingly easydomain adaptation.
In Proceedings of the45th Annual Meeting of the Association ofComputational Linguistics, pages 256?263,Prague.Daume?
III, Hal and Daniel Marcu.
2006.Domain adaptation for statisticalclassifiers.
Journal of Artificial IntelligenceResearch, 26:101?126.Falahati, Reza.
2006.
The use of hedgingacross different disciplines and rhetoricalsections of research articles.
In Proceedingsof the 22nd NorthWest Linguistics Conference(NWLC22), pages 99?112, Burnaby.Farkas, Richa?rd and Gyo?rgy Szarvas.
2008.Automatic construction of rule-basedICD-9-CM coding systems.
BMCBioinformatics, 9:1?9.Farkas, Richa?rd, Veronika Vincze, Gyo?rgyMo?ra, Ja?nos Csirik, and Gyo?rgy Szarvas.2010.
The CoNLL-2010 Shared Task:Learning to detect hedges and their scopein natural language text.
In Proceedings ofthe Fourteenth Conference on ComputationalNatural Language Learning (CoNLL-2010):Shared Task, pages 1?12, Uppsala.Fernandes, Eraldo R., Carlos E. M. Crestana,and Ruy L. Milidiu?.
2010.
Hedge detectionusing the RelHunter approach.
InProceedings of the Fourteenth Conference onComputational Natural Language Learning(CoNLL-2010): Shared Task, pages 64?69,Uppsala.Friedman, Carol, Philip O. Alderson,John H. M. Austin, James J. Cimino, andStephen B. Johnson.
1994.
A Generalnatural-language text processor for clinicalradiology.
Journal of the American MedicalInformatics Association, 1(2):161?174.Ganter, Viola and Michael Strube.
2009.Finding hedges by chasing weasels: Hedgedetection using Wikipedia tags andshallow linguistic features.
In Proceedingsof the ACL-IJCNLP 2009 Conference ShortPapers, pages 173?176, Suntec.Georgescul, Maria.
2010.
A hedgehop over amax-margin framework using hedge cues.In Proceedings of the Fourteenth Conferenceon Computational Natural Language Learning(CoNLL-2010): Shared Task, pages 26?31,Uppsala.Hyland, Ken.
1994.
Hedging in academicwriting and EAP textbooks.
English forSpecific Purposes, 13(3):239?256.Hyland, Ken.
1996.
Writing withoutconviction?
Hedging in scientificresearch articles.
Applied Linguistics,17(4):433?454.Hyland, Ken.
1998.
Boosters, hedging andthe negotiation of academic knowledge.Text, 18(3):349?382.Kiefer, Ferenc.
2005.
Leheto?se?g e?sszu?kse?gszeru?se?g [Possibility andnecessity].
Tinta Kiado?, Budapest.Kilicoglu, Halil and Sabine Bergler.2008.
Recognizing speculativelanguage in biomedical research articles:A linguistically motivated perspective.In Proceedings of the Workshop on CurrentTrends in Biomedical Natural LanguageProcessing, pages 46?53, Columbus, OH.Kilicoglu, Halil and Sabine Bergler.
2009.Syntactic dependency based heuristics forbiological event extraction.
In Proceedingsof the BioNLP 2009 Workshop CompanionVolume for Shared Task, pages 119?127,Boulder, CO.Kim, Jin-Dong, Tomoko Ohta, SampoPyysalo, Yoshinobu Kano, and Jun?ichiTsujii.
2009.
Overview of BioNLP?09Shared Task on Event Extraction.
InProceedings of the BioNLP 2009 WorkshopCompanion Volume for Shared Task,pages 1?9, Boulder, OH.Kim, Jin-Dong, Tomoko Ohta, and Jun?ichiTsujii.
2008.
Corpus annotation for miningbiomedical events from literature.
BMCBioinformatics, 9(Suppl 10).365Computational Linguistics Volume 38, Number 2Li, Xinxin, Jianping Shen, Xiang Gao, andXuan Wang.
2010.
Exploiting rich featuresfor detecting hedges and their scope.
InProceedings of the Fourteenth Conference onComputational Natural Language Learning(CoNLL-2010): Shared Task, pages 78?83,Uppsala.Light, Marc, Xin Ying Qiu, and PadminiSrinivasan.
2004.
The language ofbioscience: Facts, speculations, andstatements in between.
In Proceedingsof the HLT-NAACL 2004 Workshop:Biolink 2004, Linking Biological Literature,Ontologies and Databases, pages 17?24,Boston, Massachusetts, USA.MacKinlay, Andrew, David Martinez, andTimothy Baldwin.
2009.
Biomedical eventannotation with CRFs and precisiongrammars.
In Proceedings of the Workshopon Current Trends in Biomedical NaturalLanguage Processing: Shared Task,BioNLP ?09, pages 77?85, Uppsala.McCallum, Andrew Kachites.
2002.MALLET: A Machine Learning forLanguage Toolkit.
Available athttp://mallet.cs.umass.edu.Medlock, Ben and Ted Briscoe.
2007.Weakly supervised learning for hedgeclassification in scientific literature.
InProceedings of the ACL, pages 992?999,Prague.Morante, Roser and Walter Daelemans.2009.
Learning the scope of hedge cuesin biomedical texts.
In Proceedings of theBioNLP 2009 Workshop, pages 28?36,Boulder, CO.Morante, Roser and Walter Daelemans.
2011.Annotating modality and negation for amachine reading evaluation.
In Proceedingsof CLEF 2011, Amsterdam, Netherlands.Morante, Roser, Vincent Van Asch, andWalter Daelemans.
2010.
Memory-basedresolution of in-sentence scopes of hedgecues.
In Proceedings of the FourteenthConference on Computational NaturalLanguage Learning (CoNLL-2010): SharedTask, pages 40?47, Uppsala, Sweden.Nawaz, Raheel, Paul Thompson, andSophia Ananiadou.
2010.
Evaluating ameta-knowledge annotation scheme forbio-events.
In Proceedings of the Workshopon Negation and Speculation in NaturalLanguage Processing, pages 69?77, Uppsala.O?zgu?r, Arzucan and Dragomir R. Radev.2009.
Detecting speculations and theirscopes in scientific text.
In Proceedingsof the 2009 Conference on EmpiricalMethods in Natural Language Processing,pages 1398?1407, Singapore.Palmer, Frank Robert.
1979.Modality andthe English Modals.
Longman, London.Palmer, Frank Robert.
1986.Mood andModality.
Cambridge University Press,Cambridge.Pan, Sinno Jialin and Qiang Yang.
2010.A survey on transfer learning.
IEEETransactions on Knowledge and DataEngineering, 22(10):1345?1359.Rei, Marek and Ted Briscoe.
2010.
Combiningmanual rules and supervised learningfor hedge cue and scope detection.
InProceedings of the Fourteenth Conference onComputational Natural Language Learning(CoNLL-2010): Shared Task, pages 56?63,Uppsala.Rizomilioti, Vassiliki.
2006.
Exploringepistemic modality in academic discourseusing corpora.
In Elisabet Arno?
Macia,Antonia Soler Cervera, and Carmen RuedaRamos, editors, Information Technology inLanguages for Specific Purposes, volume 7of Educational Linguistics.
Springer US,New York, pages 53?71.Rubin, Victoria L. 2010.
Epistemic modality:From uncertainty to certainty in thecontext of information seeking asinteractions with texts.
InformationProcessing & Management, 46(5):533?540.Rubin, Victoria L., Elizabeth D. Liddy,and Noriko Kando.
2005.
Certaintyidentification in texts: Categorizationmodel and manual tagging results.
InJames G. Shanahan, Yan Qu, and JanyceWiebe, editors, Computing Attitude andAffect in Text: Theory and Applications (theInformation Retrieval Series), SpringerVerlag, New York, pages 61?76.Russell, Stuart J. and Peter Norvig.
2010.Artificial Intelligence?AModern Approach(3rd international edition).
Upper SaddleRiver, NJ: Pearson Education.Sa?nchez, Liliana Mamani, Baoli Li, andCarl Vogel.
2010.
Exploiting CCGstructures with tree kernels for speculationdetection.
In Proceedings of the FourteenthConference on Computational NaturalLanguage Learning (CoNLL-2010): SharedTask, pages 126?131, Uppsala.Saur?
?, Roser.
2008.
A Factuality Profilerfor Eventualities in Text.
Ph.D. thesis,Brandeis University, Waltham, MA.Saur?
?, Roser and James Pustejovsky.
2009.FactBank: A corpus annotated withevent factuality.
Language Resources andEvaluation, 43:227?268.Settles, Burr, Mark Craven, and LewisFriedland.
2008.
Active learning withreal annotation costs.
In Proceedings of366Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertaintythe NIPS Workshop on Cost-SensitiveLearning, pages 1?10, Vancouver, Canada.Shatkay, Hagit, Fengxia Pan, AndreyRzhetsky, and W. John Wilbur.
2008.Multi-dimensional classification ofbiomedical text: Toward automated,practical provision of high-utilitytext to diverse users.
Bioinformatics,24(18):2086?2093.Sun, Chengjie, Lei Lin, Xiaolong Wang,and Yi Guan.
2007.
Using maximumentropy model to extract protein-proteininteraction information from biomedicalliterature.
In De-Shuang Huang, Donald C.Wunsch, Daniel S. Levine, and Kang-HyunJo, editors, Advanced Intelligent ComputingTheories and Applications.
With Aspectsof Theoretical and Methodological Issues.Springer Verlag, Heidelberg,pages 730?737.Szarvas, Gyo?rgy.
2008.
Hedge classificationin biomedical texts with a weaklysupervised selection of keywords.In Proceedings of ACL-08: HLT,pages 281?289, Columbus, OH.Ta?ckstro?m, Oscar, Sumithra Velupillai,Martin Hassel, Gunnar Eriksson,Hercules Dalianis, and Jussi Karlgren.2010.
Uncertainty detection asapproximate max-margin sequencelabelling.
In Proceedings of the FourteenthConference on Computational NaturalLanguage Learning (CoNLL-2010):Shared Task, pages 84?91, Uppsala.Tang, Buzhou, Xiaolong Wang, Xuan Wang,Bo Yuan, and Shixi Fan.
2010.
A cascademethod for detecting hedges and theirscope in natural language text.
InProceedings of the Fourteenth Conference onComputational Natural Language Learning(CoNLL-2010): Shared Task, pages 13?17,Uppsala.Thompson, Paul, Giulia Venturi, JohnMcNaught, Simonetta Montemagni, andSophia Ananiadou.
2008.
Categorisingmodality in biomedical texts.
In Proceedingsof the LREC 2008 Workshop on Building andEvaluating Resources for Biomedical TextMining, pages 27?34, Marrakech, Morocco.Tjong Kim Sang, Erik.
2010.
A baselineapproach for detecting sentencescontaining uncertainty.
In Proceedingsof the Fourteenth Conference onComputational Natural LanguageLearning (CoNLL-2010): Shared Task,pages 148?150, Uppsala.Uzuner, O?zlem, Xiaoran Zhang, andTawanda Sibanda.
2009.
Machinelearning and rule-based approaches toassertion classification.
Journal of theAmerican Medical Informatics Association,16(1):109?115.Van Asch, Vincent and Walter Daelemans.2010.
Using domain similarity forperformance estimation.
In Proceedings ofthe 2010 Workshop on Domain Adaptation forNatural Language Processing, pages 31?36,Uppsala.Van Landeghem, Sofie, Yvan Saeys,Bernard De Baets, and Yves Van de Peer.2009.
Analyzing text in search ofbio-molecular events: A high-precisionmachine learning framework.
InProceedings of the BioNLP 2009 WorkshopCompanion Volume for Shared Task,pages 128?136, Boulder, CO.Velldal, Erik.
2010.
Detecting uncertaintyin biomedical literature: A simpledisambiguation approach using sparserandom indexing.
In Proceedings of SMBM2010, pages 75?83, Cambridge.Velldal, Erik, Lilja ?vrelid, and StephanOepen.
2010.
Resolving speculation:MaxEnt cue classification anddependency-based scope rules.In Proceedings of the Fourteenth Conferenceon Computational Natural LanguageLearning (CoNLL-2010): Shared Task,pages 48?55, Uppsala.Vincze, Veronika, Gyo?rgy Szarvas, Richa?rdFarkas, Gyo?rgy Mo?ra, and Ja?nos Csirik.2008.
The BioScope Corpus: Biomedicaltexts annotated for uncertainty, negationand their scopes.
BMC Bioinformatics,9(Suppl 11):S9.Wilson, Theresa Ann.
2008.
Fine-grainedSubjectivity and Sentiment Analysis:Recognizing the Intensity, Polarity, andAttitudes of Private States.
Ph.D. thesis,University of Pittsburgh, PA.Zhang, Shaodian, Hai Zhao, Guodong Zhou,and Bao-Liang Lu.
2010.
Hedge detectionand scope finding by sequence labelingwith normalized feature selection.
InProceedings of the Fourteenth Conference onComputational Natural Language Learning(CoNLL-2010): Shared Task, pages 92?99,Uppsala.367
