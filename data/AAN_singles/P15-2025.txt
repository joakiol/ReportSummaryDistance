Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 150?155,Beijing, China, July 26-31, 2015. c?2015 Association for Computational LinguisticsRepresentation Based Translation Evaluation MetricsBoxing Chen and Hongyu GuoNational Research Council Canadafirst.last@nrc-cnrc.gc.caAbstractPrecisely evaluating the quality of a trans-lation against human references is a chal-lenging task due to the flexible word or-dering of a sentence and the existence ofa large number of synonyms for words.This paper proposes to evaluate transla-tions with distributed representations ofwords and sentences.
We study severalmetrics based on word and sentence repre-sentations and their combination.
Experi-ments on the WMT metric task shows thatthe metric based on the combined repre-sentations achieves the best performance,outperforming the state-of-the-art transla-tion metrics by a large margin.
In particu-lar, training the distributed representationsonly needs a reasonable amount of mono-lingual, unlabeled data that is not neces-sary drawn from the test domain.1 IntroductionAutomatic machine translation (MT) evaluationmetrics measure the quality of the translationsagainst human references.
They allow rapid com-parisons between different systems and enable thetuning of parameter values during system train-ing.
Many machine translation metrics have beenproposed in recent years, such as BLEU (Pap-ineni et al, 2002), NIST (Doddington, 2002), TER(Snover et al, 2006), Meteor (Banerjee and Lavie,2005) and its extensions, and the MEANT family(Lo and Wu, 2011), amongst others.Precisely evaluating translation, however, is noteasy.
This is mainly caused by the flexible wordordering and the existence of the large numberof synonyms for words.
One straightforward so-lution to improve the evaluation quality is to in-crease the number of various references.
Never-theless, it is expensive to create multiple refer-ences.
In order to catch synonym matches be-tween the translations and references, synonymdictionaries or paraphrasing tables have been used.For example, Meteor (Banerjee and Lavie, 2005)uses WordNet (Miller, 1995); TER-Plus (Snover etal., 2009) and Meteor Universal (Denkowski andLavie, 2014) deploy paraphrasing tables.
Thesedictionaries have helped to improve the accuracyof the evaluation; however, not all languages havesynonym dictionaries or paraphrasing tables, espe-cially for those low resource languages.This paper leverages recent developments ondistributed representations to address the abovementioned two challenges.
A distributed represen-tation maps each word or sentence to a continu-ous, low dimensional space, where words or sen-tences having similar syntactic and semantic prop-erties are close to one another (Bengio et al, 2003;Socher et al, 2011; Socher et al, 2013; Mikolovet al, 2013).
For example, the words vacationand holiday are close to each other in the vectorspace, but both are far from the word business inthat space.We propose to evaluate the translations with dif-ferent word and sentence representations.
Specif-ically, we investigate the use of three widely de-ployed representations: one-hot representations,distributed word representations learned from aneural network model, and distributed sentencerepresentations computed with recursive auto-encoder.
In particular, to leverage the different ad-vantages and focuses, in terms of benefiting eval-uation, of various representations, we concatenatethe three representations to form one vector rep-resentation for each sentence.
Our experimentson the WMT metric task show that the metricbased on the concatenated representation outper-forms several state-of-the-art machine translationmetrics, by a large margin on both segment andsystem-level.
Furthermore, our results also indi-cate that the representation based metrics are ro-bust to a variety of training conditions, such as thedata volume and domain.1502 RepresentationsA representation, in the context of NLP, is a math-ematical object associated with each word, sen-tence, or document.
This object is typically a vec-tor where each element?s value describes, to somedegree, the semantic or syntactic properties of theassociated word, sentence, or document.
Usingword or phrase representations as extra featureshas been proven to be an effective and simpleway to improve the predictive performance of anNLP system (Turian et al, 2010; Cherry and Guo,2015).
Our evaluation metrics are based on threewidely used representations, as discussed next.2.1 One-hot RepresentationsConventionally, a word is represented by a one-hotvector.
In a one-hot representation, a vocabularyis first defined, and then each word in the vocabu-lary is assigned a symbolic ID.
In this scenario, foreach word, the feature vector has the same lengthas the size of the vocabulary, and only one dimen-sion that corresponds to the word is on, such asa vector with one element set to 1 and all othersset to 0.
This feature representation has been tra-ditionally used for many NLP systems.
On theother hand, recent years have witnessed that sim-ply plugging in distributed word vectors as real-valued features is an effective way to improve aNLP system (Turian et al, 2010).2.2 Distributed Word RepresentationsDistributed word representations, also called wordembeddings, map each word deterministically to areal-valued, dense vector (Bengio et al, 2003).
Awidely used approach for generating useful wordvectors is developed by (Mikolov et al, 2013).This method scales very well to very large trainingcorpora.
Their skip-gram model, which we adopthere, learns word vectors that are good at predict-ing the words in a context window surrounding it.A very promising perspective of such distributedrepresentation is that words that have similar con-texts, and therefore similar syntactic and semanticproperties, will tend to be near one another in thelow-dimensional vector space.2.3 Sentence Vector RepresentationsWord level representation often cannot properlycapture more complex linguistic phenomena in asentence or multi-word phrase.
Therefore, weadopt an effective and efficient method for multi-word phrase distributed representation, namely thegreedy unsupervised recursive auto-encoder strat-egy (RAE) (Socher et al, 2011).
This methodworks under an unsupervised setting.
In particular,it does not rely on a parsing tree structure in orderto generate sentence level vectors.
This character-istic makes it very desirable for applying it to theoutputs of machine translation systems.
This is be-cause the outputs of translation systems are oftennot syntactically correct sentences; parsing themis possible to introduce unexpected noise.For a given sentence, the greedy unsupervisedRAE greedily searches a pair of words that re-sults in minimal reconstruction error by an auto-encoder.
The corresponding hidden vector of theauto-encoder (denoted as the two children?s par-ent vector), which has the same size as that of thetwo child vectors, is then used to replace the twochildren vectors.
This process repeats and treatsthe new parent vector like any other word vectors.In such a recursive manner, the parent vector gen-erated from the word pool with only two vectorsleft will be used as the vector representation forthe whole sentence.
Interested readers are referredto (Socher et al, 2011) for detailed discussions ofthe strategy.2.4 Combined RepresentationsEach of the above mentioned representations hasa different strength in terms of encoding syntacticand semantic contextual information for a givensentence.
Specifically, the one-hot representationis able to reflect the particular words that occurin the sentence.
The word embeddings can rec-ognize synonyms of words appearing in the sen-tence, through the co-occurrence information en-coded in the vector?s representation.
Finally, theRAE vector can encode the composed semanticinformation of the given sentence.
These obser-vations suggest that it is beneficial to take varioustypes of representations into account.The most straightforward way to integrate mul-tiple vectors is using concatenation.
In our studieshere, we first compute the sentence-level one-hot,word embedding, and RAE representations.
Next,we concatenate the three sentence-level represen-tations to form one vector for each sentence.3 Representations Based MetricsOur translation evaluation metrics are built on thefour representations as discussed in Section 2.151Consider we have the sentence representationsfor the translations (t) and references (r), thetranslation quality is measured with a similarityscore computed with Cosine function and a lengthpenalty.
Suppose the size of the vector is N , wecalculate the quality as follows.Score(t, r) = Cos?
(t, r)?
Plen(1)Cos(t, r) =?i=Ni=1vi(t) ?
vi(r)??i=Ni=1v2i(t)??i=Ni=1v2i(r)(2)Plen={exp(1?
lr/lt) if (lt< lr)exp(1?
lt/lr) if (lt?
lr)(3)where ?
is a free parameter, vi(.)
is the value ofthe vector element, Plenis the length penalty, andlr, ltare length of the translation and reference,respectively.In the scenarios of there exist multiple refer-ences, we compute the score with each reference,then choose the highest one.
Also, we treat thedocument-level score as the weighted average ofsentence-level scores, with the weights being thereference lengths, as follows.Scored=?Di=1len(ri)Scorei?Di=1len(ri)(4)where Scoreidenotes the score of sentence i, andD is the size of the document in sentences.
Withthese score equations, we then can formulate ourfive presentations based metrics as follows.For the one-hot representation metric, once wehave the representations of the words and n-grams,we sum all the vectors to obtain the representationof the sentence.
For efficiency, we only keep theentries which are not both zero in the referenceand translation vectors.
After we generate the twovectors for both translation and reference, we thencompute the score using Equation 1.For the word embedding based metric, we firstlearn the word vector representation using thecode provided by (Mikolov et al, 2013) 1.
Next,following (Zou et al, 2013), we average the wordembeddings of all words in the sentence to obtainthe representation of the sentence.As discussed in Section 2.4, the three sentence-level one-hot, word embedding and RAE repre-sentations have different strength when they are1https://code.google.com/p/word2vec/used to compare two sentences.
In our metric here,each of the three vectors is first scaled with a par-ticular weight (learned on dev data) and then thevectors are concatenated.
With these concatena-tion vectors, we then calculate the similarity scoreusing Equation 1.For comparison, we also combine the strengthof the three representations using weighted aver-age of the three metrics computed.
Weights aretuned using development data.4 ExperimentsWe conducted experiments on the WMT met-ric task data.
Development sets include WMT2011 all-to-English, and English-to-all submis-sions.
Test sets contain WMT 2012, and WMT2013 all-to-English, plus 2012, 2013 English-to-all submissions.
The languages ?all?
includeFrench, Spanish, German and Czech.
For trainingthe word embedding and recursive auto-encodermodel, we used WMT 2013 training data 2.We compared our metrics with smoothed BLEU(mteval-v13a), TER 3, Meteor v1.0 4, and MeteorUniversal (i.e.
v1.5) 5.
We used the default set-tings for all these four metrics.When considering the representation based met-rics, we tuned all the parameters to maximize thesystem-level ?
score for all representation basedmetrics on the dev sets.
We tuned the weightsfor combining the three vectors automatically, us-ing the downhill simplex method as described in(Press et al, 2002).
The weights are 1 for theRAE vector, about 0.1 for the word embeddingvector, and around 0.01 for the one-hot vector, re-spectively.
We tuned other parameters manually.Specifically, we set n equal to 2 for the one-hotn-gram representation, the vector size of the re-cursive auto-encoder to 10, and the vector size ofword embeddings to 80.Following WMT 2013?s metric task (Macha?c?ekand Bojar, 2013), to measure the correlation withhuman judgment, we use Kendall?s rank correla-tion coefficient ?
for the segment level, and Pear-son?s correlation coefficient (?
in the below tablesand figures) for the system-level respectively.2http://www.statmt.org/wmt13/translation-task.html3http://www.cs.umd.edu/ snover/tercom/4http://www.cs.cmu.edu/ alavie/METEOR/5Meteor universal package does not include paraphrasingtable for other target language except English, so we did notrun Out-of-English experiments for this metric.152Into-Eng Out-of-Engmetric seg ?
sys ?
seg ?
sys ?BLEU 0.220 0.751 0.179 0.736TER 0.211 0.742 0.175 0.745Meteor 0.228 0.824 0.180 0.778Met.
Uni.
0.249 0.808 ?
?One-hot 0.235 0.795 0.183 0.773Word emb.
0.212 0.818 0.175 0.788RAE vec.
0.203 0.856 0.171 0.780Comb.
rep. 0.259 0.874 0.191 0.832Wghted avg.
0.247 0.863 0.185 0.798Table 1: Correlations with human judgment onWMT data for Into-English and Out-of-Englishtask.
Results are averaged on all test sets.4.1 General PerformanceWe first report the main experimental results con-ducted on the Into-English and Out-of-Englishtasks.
Results in Tables 1 suggest that metricsbased on three single representations all obtainedcomparable or better performance than BLEU,TER and Meteor.
In particular, the metric basedon recursive auto-encoder outperformed the othertesting metrics on system-level.
When combin-ing the strengths of the three representations, ourexperimental results show that the metric basedon the combined representation outperformed allstate-of-the-art metrics by a large margin on bothsegment- and system-level.Regarding the evaluation speed of the represen-tation metrics, it took around 1 minute to scoreabout 2000 sentences with the above settings ona machine with a 2.33GHz Intel CPU.
It is worthnoting that if we increase the vector size of theRAE model and word embeddings, longer execu-tion time is expected for the scoring processes.0.20.40.60.820K 80K 320K 1.28M 5.12Mseg ?sys ?Into?Englishnumber of training sentencesFigure 1: Correlations with human judgment onWMT data for Into-English task for combined rep-resentation based metric when increasing the sizeof the training data.4.2 Effect of the Training Data SizeIn our second experiment, we measure the per-formance on the Into-English task and increasethe training data from 20K sentences to 11 mil-lion sentences.
The sentences are randomly se-lected from the whole training data, which in-clude the English side of WMT 2013 French-to-English parallel data (?Europarl v7?, ?News Com-mentary?
and ?UN Corpus?).
The results are re-ported in Figure 1.
From this figure, one can con-clude that the performance improves with the in-creasing of the training data, however, when morethan 1.28M sentences are used, the performancestabilizes.
This result indicates that training a sta-ble and good model for our metric does not need ahuge amount of training data.4.3 Sensitivity to Data Across DomainsThe last experiment aimed at the following ques-tion: should the test domain be consistent with thetraining domain?
In this experiment, we sampledthree training sets from different domain data setsin equal number (136K) of sentences: Europarl(EP), News Commentary (NC), and United Na-tion proceedings (UN), while the test domain re-mains the same, i.e., the news domain.
The met-ric trained on NC domain data achieved slightlyhigher segment-level ?
score (0.181 vs 0.178 forEP, 0.176 for UN) and system-level Pearson?s cor-relation score ?
(0.821 vs 0.820 for EP, 0.817for UN).
Nevertheless, the results are consistentacross domains.
This is explainable: although thesame test sentence may have different representa-tions w.r.t.
the training domain, the distance be-tween the translation and its reference may stayconsistent.
Practically, the training and test datanot necessary being in the same domain is a veryattractive characteristic for the translation metrics.It means that we do not have to train the word em-beddings and RAE model for each testing domain.4.4 Cope with Word Ordering and SynonymIn order to better understand why metrics based oncombined representations can achieve better cor-relation with human judgment than other metrics,we select, in Table 2, some interesting examplesfor further analysis.Consider, for instance, the first reference (de-noted as ?1 R?
in Table 2) and their translations.
Ifwe replace the word vacation in the reference withwords business and holiday, respectively, then we153id sentence BLEU rep.1 R i had a wonderful vacation in italy ?
?1 H1 i had a wonderful business in italy 0.489 0.5551 H2 i had a wonderful holiday in italy 0.489 0.8651 H3 in italy i had a wonderful vacation 0.707 0.8041 H4 vacation in i had a wonderful italy 0.508 0.3052 R but the decision was not his to make ?
?2 H1 but it is not up to him to decide 0.063 0.6522 H2 but the decision not him to take 0.241 0.6202 H3 but the decision was not the to make 0.595 0.6123 R they were set to go on trial in jan ?
?3 H1 they should appear in court in jan 0.109 0.4983 H2 the trial was scheduled in jan 0.109 0.4543 H3 the procedures were prepared in jan 0.109 0.445Table 2: Examples evaluated with smoothed BLEU and combined representation based metric.
Examples2-3 are picked up from the real test sets; human judgment ranks H1 better than H2, and H2 better than H3for each of these example sentences.
The combined representation based metric better matches humanjudgment than BLEU does.have hypothesis 1 and hypothesis 2, denoted as ?1H1?
and ?1 H2?, respectively, in Table 2 .
In thisscenario, the metric BLEU assigns the same scoreof 0.489 for these two translations.
In contrast, therepresentation based metric associates hypothesis2 with a much higher score than that of hypothesis1, namely 0.865 and 0.555, respectively.
In otherwords, the score for hypothesis 2 is close to one,suggesting that the RAE based metric considersthis translation is almost identical to the reference.The reason here is that the vector representationsfor the two words are very near to one another inthe vector space.
Consequently, the representationbased metric treats the holiday as a synonym ofvacation, which matches human?s judgment per-fectly.Let us continue with this example.
Suppose, inhypothesis 3, we reorder the phrase in italy.
Therepresentation based metric still considers this tobe a good translation with respect to the reference,thus associating a very close score as that of thereference, namely 0.804.
The reason for represen-tation metric?s correct judgment is that H3 and thereference, in the vector space, embed very similarsemantic knowledge, although they have differentword orderings.
Now let us take this example abit further.
We randomly mess up the words in thereference, resulting in hypothesis 4 (denoted as ?1H4?
as shown in Table 2).
In such scenario, therepresentation metric score drops sharply becausethe syntactic and semantic information embeddedin the vector space is very different from the refer-ence.
Interestingly, the BLEU metric still considerthis translation is not a very bad translation.We made up the first example sentence for il-lustrative purpose, however, the examples 2-3 arepicked up from the real test sets.
According tothe human judgment, hypothesis 1 (H1) is betterthan hypothesis 2 (H2); hypothesis 2 is better thanhypothesis 3 (H3) for each of these example sen-tences.
These results indicate that the combinedrepresentation based metric better matches the hu-man judgment than BLEU does.5 ConclusionWe studied a series of translation evaluationmetrics based on three widely used representa-tions.
Experiments on the WMT metric task in-dicate that the representation metrics obtain bet-ter correlations with human judgment on bothsystem-level and segment-level, compared to pop-ular translation evaluation metrics such as BLEU,Meteor, Meteor Universal, and TER.
Also, therepresentation-based metrics use only monolin-gual, unlabeled data for training; such data areeasy to obtain.
Furthermore, the proposed metricsare robust to various training conditions, such asthe data size and domain.AcknowledgementsThe authors would like to thank Colin Cherry andRoland Kuhn for useful discussions.154ReferencesSatanjeev Banerjee and Alon Lavie.
2005.
METEOR:An automatic metric for MT evaluation with im-proved correlation with human judgments.
In Pro-ceedings of the ACL Workshop on Intrinsic and Ex-trinsic Evaluation Measures for Machine Transla-tion and/or Summarization, pages 65?72, Ann Ar-bor, Michigan, June.
Association for ComputationalLinguistics.Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, andChristian Janvin.
2003.
A neural probabilistic lan-guage model.
J. Mach.
Learn.
Res., 3:1137?1155,March.Colin Cherry and Hongyu Guo.
2015.
The unreason-able effectiveness of word representations for twit-ter named entity recognition.
In Proceedings of the2012 Conference of the North American Chapter ofthe Association for Computational Linguistics: Hu-man Language Technologies.Michael Denkowski and Alon Lavie.
2014.
Meteoruniversal: Language specific translation evaluationfor any target language.
In Proceedings of the NinthWorkshop on Statistical Machine Translation, pages376?380, Baltimore, Maryland, USA, June.
Associ-ation for Computational Linguistics.G.
Doddington.
2002.
Authomatic evaluationof machine translation quality using n-gram co-occurrence statistics.
In Proceedings of the HumanLanguage Technology Conference, page 128132,San Diego, CA.Chi-kiu Lo and Dekai Wu.
2011.
Meant: An inexpen-sive, high-accuracy, semi-automatic metric for eval-uating translation utility based on semantic roles.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, pages 220?229, Portland, Ore-gon, USA, June.
Association for Computational Lin-guistics.Matous?
Macha?c?ek and Ondr?ej Bojar.
2013.
Results ofthe WMT13 metrics shared task.
In Proceedings ofthe Eighth Workshop on Statistical Machine Trans-lation, pages 45?51, Sofia, Bulgaria, August.
Asso-ciation for Computational Linguistics.Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.Corrado, and Jeffrey Dean.
2013.
Distributed rep-resentations of words and phrases and their com-positionality.
In Advances in Neural InformationProcessing Systems 26: 27th Annual Conference onNeural Information Processing Systems 2013.
Pro-ceedings of a meeting held December 5-8, 2013,Lake Tahoe, Nevada, United States., pages 3111?3119.George A. Miller.
1995.
Wordnet: A lexical databasefor english.
Comunications of the ACM, 38:39?41.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A method for automaticevaluation of Machine Translation.
In Proceedingsof the 40th Annual Meeting of the Association forComputational Linguistics (ACL), pages 311?318,Philadelphia, July.
ACL.W.
Press, S. Teukolsky, W. Vetterling, and B. Flannery.2002.
Numerical Recipes in C++.
Cambridge Uni-versity Press, Cambridge, UK.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of Association for Machine Transla-tion in the Americas.Matthew G. Snover, Nitin Madnani, Bonnie Dorr, andRichard Schwartz.
2009.
Ter-plus: Paraphrase, se-mantic, and alignment enhancements to translationedit rate.
In Machine Translation, volume 23, pages117?127.Richard Socher, Jeffrey Pennington, Eric H. Huang,Andrew Y. Ng, and Christopher D. Manning.
2011.Semi-supervised recursive autoencoders for predict-ing sentiment distributions.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing, EMNLP ?11, pages 151?161,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Richard Socher, Alex Perelygin, Jean Y Wu, JasonChuang, Christopher D Manning, Andrew Y Ng,and Christopher Potts Potts.
2013.
Recursive deepmodels for semantic compositionality over a senti-ment treebank.
In EMNLP.J.
Turian, L. Ratinov, and Y. Bengio.
2010.
Word rep-resentations: A simple and general method for semi-supervised learning.
In ACL, pages 384?394.Will Y. Zou, Richard Socher, Daniel Cer, and Christo-pher D. Manning.
2013.
Bilingual word embed-dings for phrase-based machine translation.
In Pro-ceedings of the 2013 Conference on Empirical Meth-ods in Natural Language Processing, pages 1393?1398, Seattle, Washington, USA, October.
Associa-tion for Computational Linguistics.155
