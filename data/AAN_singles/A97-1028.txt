A Statistical Profile of the Named Entity TaskDavid D. Pa lmer  and David S. DayThe MITRE Corporation202 Burlington RoadBedford, MA 01730, USA{palmer, day}@mitre, orgAbstractIn this paper we present a statistical profile of theNamed Entity task, a specific information extrac-tion task for which corpora in several languages areavailable.
Using the results of the statistical anal-ysis, we propose an algorithm for lower bound es-timation for Named Entity corpora and discuss thesignificance of the cross-lingual comparisons pro-vided by the analysis.1 The  Named Entity taskThere is currently much interest, in both research and com-mercial arenas, in natural anguage processing systems whichcan perform multilingual in fo rmat ion  ext ract ion  (IE), thetask of automatically identifying the various aspects of a textthat are of interest o specific users.
An example of IE is theNamed Ent i ty  (NE) task, which has become establishedas the important first step in many other IE tasks, provid-ing information useful for coreference and template filling.Named Entity evaluation began as a part of recent MessageUnderstanding Conferences (MUC), whose objective was tostandardize the evaluation of IE tasks (Sundheim, 1995b).Several organized evaluations have been held to determinethe state-of-the-art in NE systems, and there are commercialsystems available.The goal of the NE task is to automatically identify theboundaries of a variety of phrases in a raw text, and then tocategorize the phrases identified.
There are three categories ofnamed-entities defined by the guidelines: TIMEX, NUMEX,and ENAMEX.
TIMEX phrases are temporal expressions,which are subdivided into date expressions (April 7) and timeexpressions (noon EST).
NUMEX phrases are numeric expres-sions, which are subdivided into percent expressions (3.2~)and money expressions ($180 million).
ENAMEX phrasesare proper names, representing references in a text to persons(Jeffrey H. Birnbaum), locations (New York), and organiza-tions (Northwest Airlines).Evaluation of system performance for the NE task is doneusing an automatic scoring program (Chinchor, 1995), withthe scores based on two measures - reca l l  and prec is ion.Recall is the percent of the "correct" named-entities that thesystem identifies; precision is the percent of the phrases thatthe system identifies that are actually correct NE phrases.The component recall and precision scores are then used tocalculate a balanced F-measure (Rijsbergen, 1979), whereF = 2PR/(P + R).Human performance on the NE task has been determinedto be quite high, with F-measures better than 96% (Sund-heim, 1995b).
Despite the fact that some systems in recentevaluations have performance approaching this human perfor-mance, it is important o note that named-entity recognitionis by no means a "solved problem."
The fact that existing sys-tems perform extremely well on mixed-case English newswirecorpora is certainly related to the years of research (and or-ganized evaluations) on this specific task in this language.Although performance by MUC-6 and MET systems is en-couraging, it is not clear what resources are required to adaptsystems to new languages.
It is also unknown how the exist-ing high-scoring systems would perform on less well-behavedtexts, such as single-case texts, non-newswire texts, or textsobtained via optical character recognition (OCR).There has been little discussion of the linguistic signifi-cance of performing NE recognition, or of how much linguisticknowledge is required to perform well on such an evaluation.However, any given language task should be examined care-fully to establish a baseline of performance which should beattainable by any system; only then can we adequately de-termine the significance of the results reported on that task.In this paper we give the results of an analysis of NE cor-pora in six languages from the point of view of a systemwith no knowledge of the languages; that is, we performed ananalysis based purely on the strings of characters composingthe texts and the named-entity phrases.
The performance ofsuch a straw-man system, which did not use language-specificlexicons or word lists or even information about tokeniza-tion/segmentation r part-of-speech, can serve as a baselinescore for comparison of more sophisticated systems.2 The CorporaThe definition of the NE task we discuss in this paper wastaken from the guidelines for the Sixth Message Understand-ing Conferences (MUC-6) (Sundheim, 1995a) and the recentMultilingual Entity Task (MET, May 1996), both sponsoredby the T IPSTER program.
MUC-6 evaluated English NEsystems, and MET evaluated Spanish, Japanese, and ChineseNE systems.
The Spanish, Japanese, and Chinese corpora weanalyzed each consisted of the MET training documents; sim-ilarly, the English corpus contains 60 Wall Street Journal ar-ticles prepared for the MUC-6 dry-run and official evaluation.In addition to the four corpora available from the recent orga-nized NE evaluations, we analyzed similar-sized French and190Portuguese corpora 1 which were prepared according to the language, as well as a breakdown of total phrases into theMET guidelines.
Table 1 shows the sources for the corpora, three individual categories.ChineseEnglishFrenchJapanesePortugueseSpanishXinhuaWall Street JournalLe MondeKyodoRadiobrasAgence France PresseChinaUSAFranceJapanBrazilFranceLanguageChineseEnglishFrenchJapanesePortugueseSpanishNE TIM NUM ENA4454 17.2 0 1.8 0 80.9 02242 10.7% 9.5% 79.8%2321 18.6% 3.0% 78.4%2146 26.4% 4.0% 69.6%3839 17.7% 12.1% 70.3%3579 24.6% 3.0% 72.5%Table 1: Corpora sources.
Table 3: NE phrases, by subcategory.All six corpora consisted of a collection of newswire articles,and none of the articles in any language was a translation ofan article in another language.
There were important differ-ences in the makeup of these individual corpora that affectedthis analysis.
The French corpus, for example, contained awide range of articles from a single issue of Le Monde, sothe topics of the articles ranged from world politics to theParis fashion scene.
The articles in the English and Spanishcorpora were specifically selected (by the MUC-6 and METevaluation organizers) because they contained references topress conferences.
While the content was more homogeneousin the English corpus, the articles were nevertheless drawnfrom a range of several months of the Wall Street Journal,so the specific topics (and constituent Named Entities) werevery diverse.
The Chinese Xinhua corpus was, in contrast, ex-tremely homogeneous.
These differences demonstrate a num-ber of difficulties presented by corpora in different languages.In order to estimate the complexity of the NE task, we firstdetermined the vocabulary size of the corpora involved (i.e.
"count the words"), in terms of individual lexemes of the lan-guage.
For our analysis of the European-language corpora,we considered a token to be any sequence of characters de-limited by white space, and we ignored the case of all letters.The Japanese corpus was segmented using NEWJUMAN, theChinese corpus with a segmenter made available by New Mex-ico State University.
This segmentation i formation was usedonly to estimate the corpora sizes and was not used in any ofthe other portions of our analysis.Since many words occurred frequently within a corpus, thelinguistic type-token distinction was important o our analy-sis.
An example of this distinction would be the sentence apound costs a pound, which has 5 lexeme tokens and 3 lex-eme types.
The ratio of lexeme tokens to types, which can bethought of as the average occurrence of each lexeme, is shownin Table 2 with the vocabulary sizes of the six corpora.LanguageChineseEnglishFrenchJapanesePortugueseSpanishLexeme Lexeme Token/Tokens Types Type34782 4584 7.624797 5764 4.335997 8691 4.121484 3655 5.942621 7756 5.531991 7850 4.1Table 2: Corpora  size by lexeme.Table 3 shows the total number of NE phrases for each1The French corpus was prepared by Marc Vilain; the Por-tuguese corpus was prepared by Sasha Caskey.2.1 NUMEX and T IMEX phrasesFrom Table 3 we see that TIMEX and NUMEX phrases to-gether composed only 20-30% of all NE phrases in each lan-guage.
Furthermore, these phrases were the easiest o recog-nize, because they could be represented by very few simplepatterns.
Upon inspection of the corpora, for example, wewere able to represent nearly all NUMEX phrases in eachof the six corpora with just 5 patterns.
2 Similarly, givena simple list of the basic temporal phrase words for a lan-guage (months, days of the week, seasons, etc.
), it was possi-ble to construct a series of patterns to represent most of theTIMEX phrases.
3 We were able to represent at least 95% ofall TIMEX in each language in similar ways with just a fewpatterns (less than 30 per language), constructed in a fewhours.
Since we found most NUMEX and TIMEX phrasesto be easy to recognize, we therefore restricted our furtheranalysis of the corpora to ENAMEX phrases, which provedto be significantly more complex.2.2 ENAMEX phrasesTable 4 shows the numbers of ENAMEX phrases tokens con-tained by the six corpora.
The average occurrence of eachtoken in each language was quite low (much lower than the av-erage occurrence of each lexeme), which indicated that manyphrases occurred very infrequently in the corpus.LanguageChineseEnglishFrenchJapanesePortugueseSpanishENAMEX ENAMEX Token/Tokens Types Type3605 887 4.11789 840 2.11820 1085 1.71493 614 2.42698 981 2.82593 1177 2.2Table 4: Corpora  size by ENAMEX phrases.Nevertheless, a large number of all phrase tokens couldbe accounted for by a few frequently-occurring phrase types.For example, the Chinese corpus contained 2156 total LOCA-TION phrases, but 449 of these locations (20.8%) could be2An example of a NUMEX pattern representing a SpanishPERCENT would be a sequence of digits followed by eitherthe percent sign (%) or the words "por ciento'.3An example of a NUMEX pattern representing a SpanishDATE would be the name of a month (or its abbreviation)followed by a sequence of digits (the day), optionally followedby a comma and another sequence of digits (the year).191accounted for by the three common Chinese words for China.Figure 1 shows a graph of the cumulative percentage of allphrases of the corresponding category represented by the zmost frequently-occurring phrases of that type in the givenlanguage.10o6o1o..!o0 50 10oFigure 1: Graph of the cumulative % of phrase tokensprovided by % of phrase types.Since high performance on training texts is meaningless i f asystem performs poorly on new, unseen texts, we estimatedthe performance of a simple memorization algorithm on un-seen data.
For our simple system, the answer to the questiondepended on the vocabu lary  t ransfer  rate of the corpus,the percentage of phrases occurring in the training corpuswhich also occurred in the test corpus.
To measure the vocab-ulary transfer ate for the six corpora, we randomly dividedeach corpus into a training set and a test set, with each testset containing about 450 ENAMEX phrases, and each train-ing set containing all remaining phrases.
We then examinedthe ENAMEX phrases in the training set to determine howmany also occurred in the test set.The results of this experiment showed that, to a certainextent, a word list built from the training set provided rea-sonable performance.
Just as some frequent phrase typescomprised a large percentage of the phrase tokens within acorpus, a small number of phrase types from the training setaccounted for many tokens in the test set.
As shown by thetransfer curve for the six languages in Figure 2, the transferrate varied dramatically depending on the language, but thegraph has the same shape for each, even though the six cor-pora contained ifferent amounts of training data (thus thelines of different length).The graph shows a similar shape for all subcategories ofENAMEX phrases in all the languages investigated, althoughthe rate of increase varies slightly.
It is clear from the classicZipfian distribution (cf.
(Zipf, 1932; Zipf, 1949)) shown by thegraph that a significant percentage of the ENAMEX phrasetokens could be represented by a small amount of frequently-occurring phrase types.
However, Zipf's law also tells us thata non-trivial percentage of the phrases (those in the tail ofthe graph) are very infrequent, most likely never occurring inany amount of training data.Unlike the distribution of the overall NE phrases, the rel-ative proportion of constituent ENAMEX phrase subcate-gories (PERSON, LOCATION, and ORGANIZATION) var-ied greatly by language.
The breakdown by ENAMEX phrasesubcategory is shown in Table 5.Language Org Loc PersEnglishFrenchJapanesePortugueseSpanish20.2 o 59.8 o 20.0 o56,2% 14.5% 29.2%33.8% 30.0% 38.1%39.2% 40.8% 20.0%49.9% 19.5% 30.1%28.6% 43.5% 27.9%100.90,riO.60.frO.40,~I0,20,I0 .00f ohlnese,_..* sl~n~h"a ~ tm(jz=h_~- , , - - - -  V,achI I I I I200  400 600 eO0 |000Figure 2: Graph of the cumulative test phrase tokens (%)covered by training phrase types.Table 5: ENAMEX phrases by subcategory.The significance of this result is that each ENAMEX phrasesubcategory had to be treated as equivalent.
It was not pos-sible to focus on a particular subcategory to obtain a con-sistently high score.
In other words, a strategy that focuseson locations would do well on the Chinese corpus where loca-tions comprise 59.8% of the ENAMEX phrases, but would dopoorly on the English corpus, where locations are only 14.5%of the ENAMEX.3 T ra in ing  and  ambigu i tyA logical question to pose is, "How well can our system per-form if it simply memorizes the phrases in the training texts?
"In each language, the transfer rate for the most frequentphrase types (the steep part of the graph) was quite high;however, the graph rapidly peaks and leaves a large per-centage of the phrases uncovered by the training phrases.The remaining "uncovered" phrases can only be recognizedby means other than "memorization," such as by examiningcontextual clues.
Table 6 shows the transfer ates of phrasetokens.The accuracy of the pure memorization can be reduced bytwo forms of ambiguity.
Phrases or parts of phrases can oc-cur within two or more named-entity categories, such as thestring Boston, which by itself is a location but within BostonRed Sox is an organization.
In most cases this ambiguity canbe resolved using a simple longest-match heuristic.
Anothersource of ambiguity occurs when a string can occur both as a192Overa l lLanguage ENAMEX Org Loc PersChineseEnglishFrenchJapanesePortugueseSpanish73.2%21.2%23.6?/059.2%61.3%48.1%46.9% 87.1% 42.6%17.7% 42.7% 13.3%13.4% 45.9% 11.2%56.2% 72.7% 37.5%56.4% 57.4% 47.9%49.8% 71.4% 13.7%Table 6: Vocabulary transfer (tokens).LowerLanguage BoundChinese Xinhua 71.8English WSJ 38.4French Le Monde 34.5Japanese Kyodo 70.1Portuguese Radiobras 71.3Spanish AFP 59.3Table 7: Est imated lower bounds.NE phrase and as a non-phrase, such as Apple, which wouldsometimes refer to the computer company (and thus be taggedan organization) and sometimes refer to the fruit (and thusnot be tagged at all).
Such cases, although infrequent, wouldresult in precision errors which we do not factor into the fol-lowing estimation of a recall lower bound.4 Es t imat ing  a lower boundGiven the above statistical analysis, we estimated a baselinescore for our straw-man algorithm on the NE task, a scorewhich should easily be attainable by any system attemptingto perform the task.
First, we estimated that any systemshould be able to recognize a large percentage of NUMEXand TIMEX phrases; our experience indicates that 95% ispossible due to the small number of patterns which composemost of these phrases.In order to estimate a lower bound for ENAMEX recogni-tion, we relied on the transfer graph in Figure 2.
It is clearfrom the graph that the contribution of the training data hasleveled off in each language by the time the number of trainingtypes is roughly equal to the size of the test data (450 in thiscase).
Selecting this point on the graph allowed us to directlycompare memorization performance for the six languages.
Anideal memorization-based algorithm would be able to recog-nize phrases according to the transfer ate corresponding tothis amount of training data.
Our lower bound formula wouldthus be((N~vMSX + NTIM~X) * ~) + (NENAMEX * TENAM~X)wherea = 0.95 (in our experience)N~at = Percentage of NE phrases representedby category (from Table 3)TENAMEX = ENAMEX transfer ate(from Figure 2)The resulting lower bound scores, shown in Table 7, weresurprisingly high, indicating that a very simple NE systemcould easily achieve a recall above 70 for some languages.The range of lower bound scores can partly be attributed tothe differences in corpus makeup discussed in Section 3, butthe range also illustrates the large score differences which arepossible from one corpus to the next.The upper bounds of memorization algorithms implied bythe preceding analysis do not require that a deeper under-standing of the linguistic phenomena of a target language isnecessary to generalize NE recognition in unseen test data.Contextual clues can improve the expected score of a base-line system without requiring extensive linguistic knowledge.Just as most of the TIMEX and NUMEX phrases in any lan-guage can be recognized upon inspection using simple patternmatching, a large percentage of the ENAMEX phrases couldbe codified given an adequate analysis of the phrasal contextsin the training documents.
Furthermore, lists of titles, ge-ographic units, and corporate designators would assist thiscontextual analysis and improve the expected baseline.
In-deed, such simple strategies drive most current NE systems.5 D iscuss ionThe results of this analysis indicate that it is possible to per-form much of the task of named-entity recognition with a verysimple analysis of the strings composing the NE phrases; evenmore is possible with an additional inspection of the commonphrasal contexts.
The underlying principle is Zipf's Law; dueto the prevalence of very frequent phenomena, a little effortgoes a long way and very high scores can be achieved irectlyfrom the training data.
Yet according to the same Law thatgives us that initial high score, incremental advances abovethe baseline can be arduous and very language specific.
Suchimprovement can most certainly only be achieved with a cer-tain amount of well-placed linguistic intuition.The analysis also demonstrated the large differences in lan-guages for the NE task, suggesting that we need to not onlyexamine the overall score but also the ability to surpass thelimitations of word lists, especially since extensive lists axeavailable in very few languages.
It is particularly importantto evaluate system performance beyond a lower bound, suchas that proposed in Section 4.
Since the baseline scores willdiffer for different languages and corpora, scores for differentcorpora that appear equal may not necessarily be comparable.ReferencesNancy Chinchor.
1995.
MUC-5 evaluation metrics.
In Proceedings of theFifth Message Understanding Conference (MUCS), pages 69-78, Baltimore,Maryland.Steven Maiorano and Terry Wilson.
1996.
Multilingu&l Entity Task (MET):Japanese Results.
In Proceedings of TIPSTER Text Program (Phase 1I),May.Roberta Merchant and Mary Ellen Okurowski.
1996.
The MultilinguM EntityTask (MET) Overview.
In Proceedings o/ TIPSTER Text Program (Phase11), May.MET.
May 1996.
Task definition.
MultilinguM Entity Task.C.
J.
Van P~ijsbergen.
1979.
Information Retrieval.
nutterworths, London.Beth Sundheim.
1995a.
MUG6 named entity task definition, Version 2.1.
InProceedings of the Sizth Message Understanding Conference (MUC6).Beth M. Sundheim.
1995b.
Overview of results of the MUC-6 evaluation.
InProceedings o/ the Si=th Message Understanding Conference (MUC6).G.
Zipf.
1932.
Selected Studies o/ the Principle of Relative Frequency in Lan-guage.
Harvard University Press, Cambridge, MA.G.
Zipf.
1949.
Human Behavior and the principle of least effort.
Hafner, NewYork.193
