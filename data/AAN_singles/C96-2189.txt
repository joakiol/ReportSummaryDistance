Using sentence connectors for evaluating MT outputEric M. Visser ?
and Masaru FujiFu j i tsu  Laborator ies  Ltd .
,  Med ia  In tegrat ion  Laboratory4-1-1 Kamikodanaka ,  Nakahara -ku ,  Kawasak i  211, Japan{ eric Ifuj i } @flab.fuj it su.co.j  pAbst ractThis paper elaborates on the design of amachine translation evaluation methodthat aims to determine to what degreethe meaning of an original text is pre-served in translation, without lookinginto the grammatical correctness of itsconstituent sentences.
The basic ideais to have a human evaluator take thesentences of the translated text and, foreach of these sentences, determine the se-mantic relationship that exists betweenit and the sentence immediately preced-ing it.
In order to minimise evalua-tor dependence, relations between sen-tences are expressed in terms of the eon-juncts  that can connect them, ratherthan through explicit categories.
For ann-sentence text this results in a list ofn - 1 sentence-to-sentence relationships,which we call the text's conneetivltyprofi le.
This can then be compared tothe connectivity profile of the originaltext, and the degree of correspondencebetween the two would be a measure forthe quality of the translation.A set of "essential" conjuncts was ex-tracted for English and Japanese, and acomputer interface was designed to sup-port the task of inserting the most fittingconjuncts between sentence pairs.
Withthese in place, several sets of experimentswere performed.1 BackgroundEvaluation of MT results is generally tackled ona very detailed, linguistic-technical level.
Typi-cally, a test set of sentences i prepared each ofwhich is carefully designed to ascertain whetherthe MT system can handle a certain grammati-cal phenomenon - e.g.
(Isahara, 1995).
Other?The first author is currently at ATR Interpret-ing Telecommunications Research Laboratories; cur-rent e-mail address is (eric@itl.atr.co.jp).methods may concentrate on word choice, consis-tency in terminology, PP attachment, dependencyrelations, or other specific grammatical or lexicalaspects.
While such evaluation methods are cer-tainly necessary and useful for the MT developer,they do not necessarily give us a reliable indica-tion of user satisfaction.Especially now that MT systems are becomingwidely available on the home user market andcoming within the casuM user's reach, MT de-velopers need to pay more attention to this as-pect.
Casual users might just not care all thatmuch about grammatical correctness: as long asthey can understand the output, they might besatisfied with the system.
Moreover, such usersare not likely to judge the system on a sentence-by-sentence basis: rather, they will be interestedin the understandability of the text as a whole.The flurry of integrated WWW-browsers cum MTsystems 1 to hit the (Japanese) market recently hasadded to the plausibility of this scenario.We conclude then that an MT evaluationmethod is called for which concentrates on wholetexts rather than on single sentences, and whichjudges meaning and readability rather than gram-mar.
In addition, we specify that evaluationresults should be reproducible and evaluator-independent ( o a reasonable degree at least), andquantifiable.
These additional requirements arenecessary to ensure that results obtained at dif-ferent imes and/or by different evaluators (prefer-ably using different exts) are comparable.In (Suet  al., 1992) an interesting alternativeevaluation method is proposed, in which the dis-crepancy is measured between raw MT output andthe post-edited result.
This method does work onwhole texts, and could conceivably be adapted tojudge meaning and readability (by adequately in-structing the post-editors); then again in "brows-ing" applications post-editing is not the norm, andit may be difficult to attain a good approximationof "browsable" MT output, in this paper, we trya different approach.1 These allow you to read English WWW-pages inJapanese, preserving the original page layout.10662 Out l ine  o f  the  eva luat ion  method2.1 Compare salient propertiesTo test whether the meaning of a translated texthas come across, one could simply ask the evalu-ators questions about the translated text, or havethem summarise it.
Such methods however areeither costly (for each new text a new set of ques-tions will have to be devised) or hard to quantifyobjectively, or even both.The method we will adopt involves constructinga profile of both ttre original and the translatedtext in terms of some salient semantic or prag-matic property of its constituent sentences.
Theseprofiles can then be compared to give an indica-tion of translation quality: if we assume that theoriginal text's profile is "perfect", then the degreeto which the profile of the translated text resem-bles tile perfect profile will correspond (in theoryat least) to the quality of the translation.
Thisapproach assumes that the number and order ofsentences are invariant in translation; luckily, forMT systems, this is almost always true.As for the salient property to be used in the pro-file, we settled on meaning relations of single sen-tences with previous text: this property seemedto us to be both fairly discriminating and imple-mentable.
In summary, a profile will be an orderedlist of meaning relations xi, i = 2 ... n which de-scribe the relation of sentence i with what camebefore.
Moreover, the target of each relation istaken to be the previous entence, i.e.
sentence i-1(see ?
4 for further discussion).2.2 Avoid (-ontrived def in i t ionsA set of sentence-to-sentence relation categorieswill then have to be designed and defined; butthe wide w~riety of proposed methods and solu-tions (see (Hovy and maier, 1993) for an overview)suggests that this is not an easy task.
Indeed, theproblem with categories and definitions is that theevaluator will always have to depend to a certainextent on his own personal understanding of thesedefinitions; and the more categories there are, thegreater the chance that their definitions will notalways be clear and fixed in his mind.
This natu-rally has a deleterious effect on the reliability anduniversality of evaluation results.We will get back to the design problem later,but with respect o the definition problem, our so-lution was to simply hide the definitions.
We havesought to accomplish this by instructing the eval-uator to link sentences linguistically; more specif-ically, wc have opted to instruct the evaluator tochoose a con junct  2 to be inserted between ev-ery pair of consecutive sentences.
The conjuncts2A subclass of the adverbs, el.
(Quirk et al, 1985)pg.
631-.
For languages that do not recognise thisclass, surrogates can be concocted: for Japanese, amixture of conjunctions and conjoining adverbs.themselves may be divided into categories, butthese can remain hidden from the evaluator.
Thisapproach inges on the hope that straight linguis-tic knowledge comes more naturally to people andis less susceptible to person-to-person differencesthan contrived meaning categories.2.3 Standardise thinking methodsSmall-scale preliminary experiments (on paper)showed that in spite of the above refinements,evaluator differences were still larger than seemedreasonable.
We surmised that this was due to dif-ferences in work methods (or thinking methods),and that therefore these needed to be equalised alittle more.
We decided on two countermeasures.Recoguising that the class of eonjuncts was to()large for the evaluator to encompass at a glance,we decided to implement an interactive Q&A in-terface on the computer in order to graduallyguide the evahlator to the optimal choice of a con-janet.
Obviously this opens a whole new can ofworms, in that the interface has to be designed(the kind and order of questions etc.
); we will getback to that later (in ?
4).The other step was to instruct the evaluatorto extract the top ic  and comment  of the sen-tence under consideration.
Both topic and com-nlent were only loosely defined: in truth the topicand comment are not important as such, rathertheir extraction was intended as a means to forcethe evaluator to get a clearer picture of the mean-ing of the sentence under consideration (thoughwe did not tell them this).3 Bas ic  assumpt ionsAt this point, it is useful to look back at the de-sign considerations outlined above and to clarifyexactly what assumptions on sentences and rela-tions underlie them.
With a little luck, our resultscan provide some support for these assumptions.The first of our assumptions is that it is al-ways possible to make explicit the relationshipof a sentence to what has come before using aconjunct.
The conjunct may be present in thesentence, but even if it is not, it can be addedin a linguistically satisfactory way.
We also as-sume that the assignment of acceptable conjunctsis reader-independent to a large degree.We assume that conjuncts (which form a closedclass) can be divided into a limited number of cat-egories that are meaningful in terms of expressingthe semantic relationship between sentences.Yet another assumption is that the meaning re-lationships between sentences of a text combineto form a characteristic feature (3 profile) of thattext, and that this profile needs to be preservedin translation.
Moreover, the ease with which thisprofile can be discerned in the translated text isassumed to be related to the readability or under-standability of the text as a whole.10674 The implementationA prototype was implemented on a Macintoshcomputer using HyperCard.
The evaluation pro-cess is made up of the following steps, which haveto be executed for every sentence in the text.1.
Extract the topic(s) and comment(s) of thesentence under consideration.2.
If there is more than one topic/comment pair,order the pairs as seems best and determine(using the same method as for sentences)which conjuncts fit best between the pairs.3.
Determine through a dialog with the systemwhich conjnnct fits best at the start of thesentence under consideration.A backtrack function was implemented which al-lowed the subjects to come back on decisions madeearlier in the dialog.
The prototype keeps a verydetailed log of what the evaluator does exactly.Without going into technical details, the follow-ing were the main tasks in the implementation.Categor is ing  the con junctsOur first categorisation of conjuncts was based oninformation concerning conjuncts and rhetoricalstructures that we patched together from author-itative grammars for English (Quirk et al, 1985),Japanese (Martin, 1975) e.a.
We came up with9 categories; in a later redesign we took the con-juncts themselves as our starting point and, bytracing crossreferenees in dictionaries, were ableto reduce the initial number of ?
220 to 32 "ba-sic" conjuncts, divided over 11 categories.Assist ing top ic /comment  ext ract ionFrankly we have been unable to find a foolproofmethod, and have settled for user-requested onlinehelp cued on linguistic aspects of the sentence.Def in ing the  scope of  mean ing  re lat ionsWe have established above that meaning relationshold between consecutive sentences; this is how-ever not self-evident.
A sentence may relate toa more remote sentence @5, for instance), or toa block of sentences; ee (Kurohashi and Nagao,1995) for a more plausible model.
We found how-ever that an online computer interface that wouldallow the user to specify the target of a relationto this extent would become prohibitively compli-cated.
The evaluator's task would involve so muchjuggling with relations and attaining such a deepunderstanding of the text that it would in the endhave a negative ffect on the reproducability andevaluator-independence of the results.Designing the dialogWe believe that this is a trial-and-error processwhich will have to bc guided by the outcome ofexperiments; more about this will follow below.II AI HI c Imean 0.43 0.46 0.37 0.32time (re:s) 13:27 14:16 12:32 41:23backtracks 11,8 9.8 4.4 2.6Table \] : Results of the first experiment5 The  exper imentsWe decided that experiments needed to establishthree qualities of this system.Eva luator - independence  Given a text in onelanguage, different evaluators should producethe same connectivity profile.Language- independence  Given a "perfectly"translated text, its connectivity profile shouldturn out the same as that of the original.Quant i f iab i l i ty  Given translations of varyingquality, the degree of correspondence in theconnectivity profiles must be shown to corre-spond to the quality of the translation.But first we conducted a preliminary experiment.5.1 Exper iments  w i th  the  dialogOur first experiments (Japanese only) concernedthe conjunct-determining dialogs.
We imple-mented 3 interfaces, each comprising the same61 conjuncts spread over 9 categories: one (A)based on categories (the subjects got a list of cat-egories in the first screen, and if they clicked onethey got the conjuncts in that category on thesecond screen); one (B) based on the conjunctsthemselves (the subjects just got the whole list ofconjuncts, spread over a couple of screens, with-out elaboration); and one (C) with questions (3answers to choose from on the first screen, one ofthese leads to a second question with 4 answers,all other links lead to sets of conjuncts).Subjects were assigned an interface, given a 9-sentence text and asked to connect he sentences,without however performing topic/comment ex-traction.
A fourth group was asked to use inter-face C, but also to extract topic and commentbefore connecting the sentences (D).
The resultsare given in table 1.
The mean of the evaluators'choices was computed by transforming the resultsinto numbers (if 7 out of 10 evaluators chose cate-gory X, 2 chose Y, and 1 Z, then this would resultin the values {1 1 1 1 1 1 1 2 2 3}), and inputtingthese numbers into the following formula.n1 ~(x i -  "2) 2i=IWe might add that subjects using interfaces A andB were more likely to choose "safe" (ambiguous,vague) conjuncts uch as 'soshite' (and then), andalso -- for what it's worth --- complained more.1068II 04)\[=B (13)I C (7) ID (7) Imean(cat) 0.52 0.60 0.89 0.69m-can(con) 1.99 2.66 2.20 1.76t-line (re:s) 15:44 19:38 13:40 14:42backtracks 4 .6  9.8 6.7 6.1NB: (C+D) mean(cat) = 1.65, mean(con) = 4.91.Table 2: Experiment results for the various textsF.- I IA+B A-bE A+D A+C+I ) \ ]~(cat )  0.73 0.98 1.02 1.5{)~(con)  4.62 4.50 4.29 7.91Tal)le 3: Combined experiment resultsTo be quite honest his experiment was too smallin scale to allow scientific conclusions (20 peopleparticipated), but we went ahead anyway and con-cluded that a) the projc('t showed pronfisc, b) in-terface C was the way to go, e) topic/cornmeal, ex-traction was important, but d) it was also costly(took three times as long!)
so we'd stick to theqazy' evaluation tbr further exl)erinmnts.5.2 Va l idat ion exi )er i lnentsFor the second set of cxperhnents, we designedidentical interfaces for English and Jai)anese.There was only one.
question, with 6 answers, andall of these led to a screen with conjuncts to choosefl:om, never more than 8 on a screen.
The set ofconjuncts was designed to be minimal (no redun-dancies, no ambiguous conjmlcts); there were 32of them, spread over 11 categories (el.
~ 4).An originM English texl, was chosen (A); l.hen a"perfi;ct" (but aligned) .lapanese translation wasproduced (B); anti finally two "less-than-perfect"translations were contrived (C was raw MT out-put, D was output from a tuned MT system theunderstandability of which had been determinedby independent ext)eriments to be halfway be-tween B and C - level 3 in (Fuji, 1996)).
Thesizes of the subject groups are given in table 2 be-tween parentheses.
Distribution means were com-puted both for categories and for conjuncts.6 DiscussionThe category means basically follow expectations.Those of C and 1) come ont a bit h)w, lint thecombined mean for C+D suggests that this maybe partly due to the size of the sample.
The (:on-junct mean of B is very high; it is not clear why.It must be noted that the evMuators were totallyuntrained; in the context of the intended use ofthis method, requiring a certain level of trainingseems acceptable and this would surely bring re-suits closer to tile goal of evaluator independence.llowever, we also observed several instances wherethe choice of a conjunct was dictated by the evah>ator's prior knowledge (or lack of it) of the subjectarea; this is a discrepancy we cannot resolve.The cross-linguistic category mean for A+B issignificantly lower than that of A+C and A+I).The conjunct mean is rather high: this is proba-bly due to the unexplaine(\[ high conjunct mean forB.
The conjunct means of A+C and A+I) seemto correlate with the number of unintelligible sen-tences in the machine-translated t xts.
Again themeans of" A+C+I)  are fairly enormous, indicatingthat size is still a factor.A rather unsettling result, however, was thatthe most-chosen sentence connector was identicalacross texts fur ahnost each of the sentence pairs.This suggests that redu(:ing evalnator dependencewill lower all means, which would defeat he pur-pose of this research.In conclusion, we feel justified in hoping that,the goals of evalnatt)r-indei)endence, and language-imtependence are reachable through judicious tun-lug of the.
current systenL The project has alsobeen successflll in that it has yielded a wealth ofinteresting data about sentence connections.
1tis (toubtflfl however that the approach will give auseful indication of translation quality.ReferencesMasaru Fuji.
1996.
Experiments to evaluatethe reading comprehension of /,:-a machine-translated texts.
In l)~vceedings of the 2ndAnnual Meeting of the Ass.
for NLP (Japan),pages 21 24.
In Japanese..Eduard It.
Hovy and glisal)eth Maier.
1993.
Par-simonious or prottigate: ltow many and whichdiscourse structure relations?
Technical re-port, Information Sciences Institute, Universityof Southern California.ltitoshi 1sahara.
1995. a EIDA's test-sets for qual-ity evaluation of MT systems: TechnicM eval-nation from the developer's point of view.
InProceedings of lhe M 7' Summit V. Luxembourg.Sadao Kurohashi and Makoto Nagao.
1995.
Auto-marie detection of discourse structure by check-ing surface information in sentences.
Journal ofthe Ass.
for NLP, 1(1):3 20.
In Japanese.Samuel E. Martin.
1975.
A reference grammar ofJapanese.
Yale University Press, New llaven.Randolph Quirk, Sidney Greenbanm, GeoffreyI,eech, and Jan Svartvik.
1985.
A comprehen-sive grammar of the English language.
Long-man, London.Keh-Yih Su, Ming-Wen Wu, and Jing-ShinChang.
1992.
A new quantitative quality mea-sure for machine translation systems.
In Pro-ceedings of COLING-92, pages 433 -439.1069
