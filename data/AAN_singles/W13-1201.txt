Proceedings of the The 1st Workshop on EVENTS: Definition, Detection, Coreference, and Representation, pages 1?10,Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational LinguisticsCoping With Implicit Arguments And Events Coreference Rodolfo Delmonte Department of Language Studies & Department of Computer Science Ca?
Foscari University - 30123, Venezia, Italy delmont@unive.it  Abstract In this paper we present ongoing work for the creation of a linguistically-based system for event coreference.
We assume that this task requires deep understanding of text and that statistically-based methods, both supervised and unsupervised are inadequate.
The reason for this choice is due to the fact that event coreference can only take place whenever argumenthood is properly computed.
It is a fact that in many cases, arguments of predicates are implicit and thus linguistically unexpressed.
This prevents training to produce sensible results.
We also assume that spatiotemporal locations need to be taken into account and this is also very often left implicit.
We used GETARUNS system to develop the coreference system which works on the basis of the discourse model and the automatically annotated markables.
We present data from the analysis, both on unexpressed implicit arguments and the description of the coreference algorithm.
1 Introduction NLP processing is more and more oriented towards semantic processing which in turn requires deep understanding of texts.
We assume that this is only possible if unexpressed implicit linguistic elements and semantically deficient items are taken into consideration (Delmonte 2009a, 2009b).
One of the first problem in the analysis of any text is accounting for implicit or linguistically unexpressed information.
This kind of information is not available in dependecy-based current annotated corpora or is only partially available ?
as in Penn Treebank ?
but it cannot possibly be learnt.
The problem of null and pronominal elements is paramount in the recovery of Predicate-Argument Structures which constitutes the fundamental element onto which propositional semantics is made to work.
However, applying machinelearning techniques on available treebanks is of no help.
State of the art systems are using more and more dependency representations which have lately shown great resiliency, robustness, scalability and great adaptability for semantic enrichment and processing.
However, by far the majority of systems available off the shelf don?t support a fully semantically consistent representation and lack Null Elements or Antecedents for pronominal ones.
If we limit ourselves to Null Elements, and to PennTreebank (hence PT), we may note that Marcus (?94) referred explicitly to Predicate-Argument Structures (hence PASs) and to the need to address this level of annotation.
He mentions explicitly that ?we intend to automatically extract a bank of PASs intended at the very least for parser evaluation from the resulting annotated corpus?
and further on ?the notation should make it easy to automatically recover PAS?
(ibid.
121).
He also mentions the need to allow for a clear and concise distinction between verb ARGUMENTs and ADJUNCTs, which he asserts to be very difficult to make, consistently.
This happens to be true: the final version of PT II does not include coindexing in controversial cases and has coindexing for null SBJ only in a percentage of the cases.
PT contains 36862 cases of null elements (including traces, expletives, gapping and ambiguity) as listed in Johansson(2007), over 93532 simple clauses and 55600 utterances, for a percentage of 66.3%.
Of course this number does not include pronominal arguments which need to be bound ?
and are not bound in PT - to an antecedent in order to become semantically consistent.
As to PT, the difficulty of the task is testified by the presence of non coindexed Null Elements: in particular we see that they are 8416, that is 22.83%.
If we exclude all traces of WH and topicalization and limit ourselves to the category OTHER TRACES which includes all unexpressed SUBJects of infinitivals and gerundives, we come up with 12172 cases of Null non-coindexed1elements, 33% of all cases.
We should note that for how much large this number may seem, this still represents a small percentage when compared to the number of null elements in languages like Chinese or Romance languages like Italian, which allow for free null subjects insertion in tensed clauses.
Current statistically dependency parsers have made improvements in enriching their structural output representation (Gabbard et al2006; Sagae and Tsujii, 2008;?Choi & Palmer, 2010;?Cai et al2011).
However, coindexation is not always performed: when it is, its performance is computed separately because it is lower than accuracy for labeled/unlabeled tasks.
In particular, Schmid reports 84% F-score for empty elements prediction and 77% for coindexation on PT.
However, other parsers have much worse results, with Johnson(2001) being the worst, with 68% F-score.
The presence of additional difficulties to predict empty categories is the cause of a bad drop in performance in Chinese - no more than 50% accuracy reported by Cai et al(2011) compared to 74/77% of the labeled/unlabeled task.
Results reported by Yang & Xue (2010) on recovering labeled empty elements in an experiment carried on a small subset of the Penn Chinese Treebank 6.0 reach an average of 60.5% of F-measure.
As to recovery of specific items, we note that over a total number of 290 little_pro items recall fares around 50%.
Of course the phenomenon is very much language dependent, as discussed above.
If we consider a language like Italian ?
which we described fully from structures annotated in the treebank called VIT (Delmonte 2004) ?
we can see that in addition to untensed sentences also simple clauses with tensed verbs show the same problem.
In fact, over 66.5% (9634 over 15874) of all simple clauses are subjectless, they have an omitted or unexpressed subject which is marked in linguistics with a little_pro and the agreement coming from morphology of the main verb.
Of the remaining lexically expressed subjects, only 64% (6166 over 9634) are in canonical position, that is in preverbal position and adjacent to the inflected verb.
The remaining 36% of lexically expresses subjects are positioned to the right or are separated from the verb by other constituents.
2 Events and Null ElementsWe will now try to describe events in terms of the contribution of Null Elements.
Events are mainly characterized by their meaning which is defined in a gloss or by one or more semantic categories, or even by a synset of synonym concepts.
In addition to that, events may be regarded as being composed of two other elements: - the participants to the event, which are arguments and adjuncts or circumstantials - the spatiotemporal location of the event Both components may be linguistically expressed or be left implicit and thus should be inferred from previous discourse.
In fact the spatiotemporal location of the event is usually indicated explicitly only if needed and is mostly left unexpressed.
Participants on the contrary are mostly explicitly expressed before they can be left implicit.
However, in some case, participants are linguistically unexpressed for structural reasons or else expressed by a pronoun.
Both cases require a deep system or a deep parser together with a pronominal binding algorithm to be in place, in order to find the appropriate antecedent and bind the empty arguments.
There are exceptions to these rules and they are constituted by utterances of generic or arbitrary reference, something intended in utterances such as, (1) Doing regular physical exercise is strongly recommended at a certain age.
where no participant is explicitly indicated, but it is clearly understood by inferences determined by knowledge of the world.
Events may be coreferred or may be queried: in both cases, we are also dealing with semantic relations at discourse structure level.
The need to corefer to a previous event derives from conversational or argumentative strategies.
Generally speaking, it is due to the need of expanding concepts and facts reported in the previous mention.
At a discourse level, this is usually called ELABORATION or EXPLANATION.
Other possible cases of event coreference at discourse structure level can be due to the need of enriching the previous description of additional facts cooccuring with the previously mentioned event: in this case we may have an hypernymic or an hyponymic relation intervening2between the two facts or concepts.
Let?s look at some examples taken from the demo text made available by the organizers.
After the title, we have a first event description, reported by a newspaper, which is a violent event followed by an adjunct clause describing the effects or caused consequences: we capitalize event naming words and then indicate the semantic discourse relation:  ?A Kurdish newspaper said Wednesday that Iraqi members of an Al Qaeda-linked group, a Kurd and an Arab, BLEW themselves up in northern Iraq on February 1, KILLING at least 105 people.?
CAUSE ??
BLOW, KILL The idea in this case is that the two events are linked by a semantic relation rather than simply the first event being coreferred by the second.
The text continues by expanding the event introducing some comment that elaborates on the previous sentence:  ?The twin suicide bombing WAS the deadliest attack in post-war Iraq and WAS SUSPECTED TO HAVE BEEN CARRIED OUT by foreign fighters, possibly linked to Osama bin Laden's Al-Qaeda network.?
ELABORATION ??
BOMBING, ATTACK CAUSE ??
BOMBING, BLOW EXPLANATION ??
SUSPECT(CARRY_OUT), BLOW Discourse relations are triggered by event coreference which in this sentence is achieved by two nominalizations: in fact only definite expressions are taken into consideration, in particular if singular in number.
The first one is TWIN SUICIDE BOMBING which we understand to be a new enriched mention of BLOW at first by a causal relation intervening between BOMB and BLOW.
This semantic relation is not available from WordNet but from Sumo-Milo, where the verbs BOMB, BLAST, ATTACK, KILL, and FIGHT all share one semantic class, VIOLENT_CONTEST and/or DESTRUCTION with BLOW.
The causal relation is derived from commonsense knowledge available in "ConceptNet" by the AI Laboratory of MIT.Searching for relations intervening between BOMB and BLOW_UP, this is what you can find - represented in an appropriate Prolog-like format: cpn(udf,bomb,[blow,something,up]).
cpn(udf,bomb,[blow,things,up]).
cpn(udf,bomb,[blow,up,buildings]).
cpn(udf,bomb,[blow,up,stuff]).
cpn(udf,bomb,blow).
cpn(do,person,[don_t,want,be,blow,up,by,bomb]).
cpn(dof,person,[not,be,blow,up,by,bomb]).
They are also all classified as NEGATIVE polarity items and are part of the same Lexical Field in Roget's Thesaurus.
Then the additional contribution of its arguments, where ?blowing themselves up?
implies a SUICIDE took place.
At the same time, the use of ?twin?
is coreferring with ?members?
a plural noun, better specified as being composed of two individuals ?a Kurd and an Arab?
in an apposition to it.
Thus, the nominalization does not add any new information that could not be understood from previous mention, but certainly clarifies previous information thus respecting Grice?s maxims.
The copulative structure headed by WAS, is used to assign a property to the coreferred event thus contributing new information.
We now know that the newspaper reports the event as being ?the deadliest attack in post-war Iraq?.
We also learn that the two fighters identity was suspected to be not Iraqi but possibly ?foreign?, deemed to belong to bin Laden?s network.
All of this new information can be labeled as ?Explanation?.
The news story continues by elaborating on the two fighters by expanding on their identity, and then explaining the way in which the bombing was organized in the following two sentences.
?The pair were named respectively as Abu Bakr Hawleri and Kazem Al-Juburi, alias Abu Turab, by independent newspaper Hawlani, which said they belonged to the Army of Ansar al-Sunna.
The Kurd blew himself up in the offices of the Patriotic Union of Kurdistan (PUK) and the Arab in the offices of the Kurdistan Democratic Party (KDP), both in the Kurdish city of Arbil, said the newspaper.3Each one carried a belt packed with four kilograms (8.8 pounds) of TNT mixed with phosphorus, a highly flammable material, the newspaper said.?
The use of a definite singular expression is highly indicative of the coreference mechanism being activated.
This applies to THE PAIR, coreferring with "Iraqi members" and also with "twin".
The same can be said of "The Kurd" coreferring with the previous mention and also the use of the same predicate BLOW UP.
In the following stretch of discourse, the story corefers to the ?Army of Ansar al-Sunna?, to explain the role that the organization had in the bombing: ?Ansar al-Sunna last week claimed the twin bombings in a statement posted on an Islamist website.
The newspaper said the motive of the attack was to "punish" the two Kurdish secular groups, which control Iraqi Kurdistan, for their alliance with the US-led coalition.
The newspaper said Ansar al-Sunna broke away from the Ansar al-Islam group last October and was led by an Arab whose alias is Abu Abdullah Hasan bin Mahmud.
Ansar al-Sunna is more extreme, said the newspaper.?
The first coreference link is expressed by the sentence ?Ansar claimed the TWIN BOMBINGs?
which is used to expand on the role of the organization of the original event.
Additional events are the STATEMENT, a nominal event, and the MOTIVE OF THE ATTACK which introduces the MOTIVATION for the event.
This causal link is connected to actual causal event: PUNISHing the Kurdish group controlling Iraqi Kurdistan.
In turn, the action of PUNISHing is explained by another eventive nominalization, the ALLIANCE of the group (the possessive THEIR corefers with it), with the US-led coalition.
Additional explanation is reported in the final sentence (longer than 40 tokens!!)
where the relation intervening between the motive the attack as contained in the statement and previously occurring facts is further clarified:  ?The newspaper added that bin Mahmud is the brother of man whose alias is Abdullah Al-Shami, an Ansar al-Islam leader who was killed last year while fighting a US-backed onslaught by the PUKthat forced the group out of its enclave near the Iranian border at the end of March last year.?
The sentence contains additional coreferring nominalizations like ONSLAUGHT, which reminds of the bombing and of the previous attack.
The overall events description is rich in temporal and spatial locations which contribute to the understanding and the overall discourse structure.
In particular we start out by a spatial location, NORTHERN IRAQ, and a temporal location, FEBRUARY 1st.
Both locations remain the same in the following sentences until we reach a change in topics and locations.
This happens when ?Ansar al-Sunna?
is introduced as SUBJect of CLAIM, an event location in time, LAST WEEK.
Additional information the Ansar al-Islam group takes us back to LAST OCTOBER.
Eventually, in the final sentence, we have been told that the current bombing event may have relation with the killing of another Ansar al-Islam leader, during an ONSLAUGHT that took place LAST YEAR, in a different location, NEAR THE IRANIAN BORDER.
The generic location LAST_YEAR is further specified as being END OF MARCH.
3.
GETARUNS : a system for text understanding  GETARUNS1, the system for text understanding developed at the University of Venice, is organized as a pipeline which includes two versions of the system: what we call the Partial and the Deep GETARUNS and they work in a backoff policy.
There are in fact three parsers interconnected and they are activated in order to prevent failure to take place.
The system has a middle module for semantic interpretation and discourse model construction which is cast into Situation Semantics; and a higher module where reasoning and generation takes place.
The system is based on LFG theoretical???????????????????????????????????????????????????????
?1 The system has been tested in STEP competition (see Delmonte 2008), and can be downloaded in two separate places.
The partial system called VENSES in its stand-alone version is available at http://www.aclweb.org/aclwiki/ index.php?title=Textual_Entailment_Resource_Pool.
The complete deep system is available both at http://www.sigsem.org/wiki/ STEP_2008_shared_task:_comparing_semantic_representations, and at, http://project.cgm.unive.it/html/sharedtask/.4framework and has a highly interconnected modular structure.
The output of grammatical modules is fed then onto the Binding Module which activates an algorithm for anaphoric binding.
Antecedents for pronouns are ranked according to grammatical function, semantic role, inherent features and their position at f-structure.
Eventually, this information is added into the original f-structure graph and then passed on to the Discourse Module (hence DM).
GETARUNS, has a linguistically based semantic module which is used to build up the DM.
Semantic processing is strongly modularized and distributed amongst a number of different submodules which take care of Spatio-Temporal Reasoning, Discourse Level Anaphora Resolution, and other subsidiary processes like Topic Hierarchy which cooperate to find the most probable antecedent of coreferring and cospecifying referential expressions when creating semantic individuals.
These are then asserted in the DM, which is then the sole knowledge representation used to solve nominal coreference.
The system uses two resolution submodules which work in sequence: the first one is fired whenever a free sentence external pronoun is spotted; the second one takes the results of the first submodule and checks for nominal anaphora.
They have access to all data structures contemporarily and pass the resolved pair, anaphor-antecedent to the following modules.
Semantic Mapping is performed in two steps: at first a Logical Form is produced which is a structural mapping from DAGs onto unscoped well-formed formulas.
These are then turned into situational semantics informational units, infons which may become facts or sits.
Each unit has a relation, a list of arguments which in our case receive their semantic roles from lower processing ?
a polarity, a temporal and a spatial location index.
All entities and their properties are asserted in the DM with the relations in which they are involved; in turn the relations may have modifiers - sentence level adjuncts, and entities may also have modifiers and attributes.
Each entity has a polarity and a couple of spatiotemporal indices which are linked to main temporal and spatial locations if any exists; else they are linked to presumed time reference derived from tense and aspect computation.
On second occurrence of the same nominal head the semantic index is recovered fromthe  history list and the system checks whether it is the same referring expression and has non-conflicting attributes or properties.
In all other cases a new entity is asserted in the DM which however is also computed as being included in (a superset of) or by (a subset of) the previous entity.4.
A System For Event Marking And Event Coreference I will now go through the text above indicating places where the system has been able to locate and identify missing arguments.
In order to clarify the working of the system I will use the output of the discourse model, which contains fully coreferred empty or linguistically unexpressed elements which have gone through pronominal binding process as well as coreference analysis.
The first unexpressed element is the subject of the adjunct gerundive headed by KILL in the first sentence:  (1) A Kurdish newspaper said Wednesday that Iraqi members of an Al Qaeda-linked group, a Kurd and an Arab, blew themselves up in northern Iraq on February 1, killing at least 105 people.
The second unexpressed argument is contained in sentence 2, in the infinitival governed by SUSPECT and headed by CARRY_OUT, which is contained in the coordinate structure headed by SUSPECT,  (2) The twin suicide bombing was the deadliest attack in post-war Iraq and was suspected to have been carried out by foreign fighters, possibly linked to Osama bin Laden's Al-Qaeda network.
where the suicide_bombing is predicated as the "deadliest attack" in the previous main sentence.
The main spatial location now becomes Iraq.
Another unexpressed argument is the subject of POSTED, a participial modifying STATEMENT,  (3) Ansar al-Sunna last week claimed the twin bombings in a statement posted on an Islamist website.
where we still want to know who posted the ?statement?, and the information is passed by the5main clause as the subject of CLAIM, i.e.
Ansar al-Sunna.
Then we have another infinitival lacking subject argument information, in the following sentence,  (4) The newspaper said the motive of the attack was to "punish" the two Kurdish secular groups, which control Iraqi Kurdistan, for their alliance with the US-led coalition.
This is a copulative structure where the subject MOTIVE is predicated by the infinitival headed by PUNISH.
In fact, this verb is lacking a referential subject simply because the predication prevents it from having a specific one.
The same GROUP is coreferred in the final sentence that contains the most important sequence of unexpressed but yet essential arguments:  (5) The newspaper added that bin Mahmud is the brother of man whose alias is Abdullah Al-Shami, an Ansar al-Islam leader who was killed last year while fighting a US-backed onslaught by the PUK that forced the group out of its enclave near the Iranian border at the end of March last year.
The gerundive headed by FIGHT has LEADER as SUBJect and as OBJect the GROUP we found in the previous sentence.
It is important to notice that this mention of GROUP is NOT coreferent with the one appearing at the beginning of the text.
This non coreference is clearly apparent from attributes accompanying the head: the first one is expressed as "an Al Qaeda-linked group", whereas the second as "the two Kurdish secular groups".
In the final computation, the system produces a set of entity pools, that is a set of all referents to a given semantic index - be they properties, entities or relations.
In particular, the referent to the LEADER coincides by virtue of a predication, with Abdullah Al-Shami and has the property of being associated to Ansar al-Islam: from the pool, we now know that he was KILLED, being associated to the THEME_AFFected role.
4.1 The Experiment and an Evaluation We tested the coreference module with the sample text and produced the following output that we comment in this section.
For each event we have two vectors of information that we then use toevaluate its relevance and its possible coreference in the previous text.
The categories used are fully explained in Delmonte (2007; 2009) and here we limit ourselves to a short description.
The event may be a verb and be related to a propositional analysis or be a noun.
Nouns classified as activity or events are selected as markables: this classification is partially derived from NomBank associated information about eventive nominals.
Coreference links are activated by synonymity or just similarity, measured by WordNet synset, a Thesaurus or sharing identical semantic classes as indicated in SUMO-MILO or other similar computational lexica.
The certainty value varies accordingly: from more certain, say .9, to less certain .4.
Obviously, copulative predications are marked with certainty equal to 1 being properties predicated in the syntax of the subject.
4.1.1 Coreference links We present here briefly the addition to the system GETARUNS that have been produced for this task.
The annotation of each text is shown in an xml file which has been obtained in the following steps: a. the system GETARUNS produces a deep analysis of each text on a sentence by sentence basis.
At the end of the analysis of each sentence, markables are collected and all semantic information is attached to each word of the sentence.
We collected all verbs and also eventive nominals and possible eventive modifiers.
This is done in two steps.
b. at the end of parsing each word of the sentence is associated to its lemma and general semantic categories are also collected from the analysis.
c. The system produces then the steps required for the Discourse Model which is where entities, relations and properties are asserted with their attributes.
Semantic indices are assigned to each new entity and previous mentions receive previously assigned indices.
At this point the contents of the discourse model are associated to each word of the sentence.
d. At the end of the analysis of the text the system collects all markables, which are internally made of four elements: an markable index, a word, a lemma, a semantic index (from the discourse model) or a generic indicator of eventuality for all verbs.6e.
Then the complete discourse model is searched to produce a list of all entities, relations and properties with their spatiotemporal relations and polarity, as documented in situation semantics.
Additional information is derived in this phase from WordNet, FrameNet or SumoMilo ontology and is made available to the coreference algorithm.
Another component that is activated at the end of the analysis is sentiment analysis that computes an affective label associated to each markable - if possible - and classifies each markable into three different classes: positive, negative and neutral.
d. The coreference algorithm works as follows: for each markable it check all possible coreference links, at first on the basis only of inherent semantic features, which are: wordform and lemma identity; then semantic similarity measured on the basis of a number of similarity criteria which are lexically based (no statistical measure is used).
We search WordNet synsets and assign a score according to whether the markables are directly contained in the same synset or not.
A different score is assigned if their relation can be inferred from the hierarchy.
Other computational lexical resources we use are those documented in our work on Text Entailment Recognition (Delmonte et al2005; 2006; 2007; 2008), and include FrameNet and Frames hierarchy; SumoMilo and its semantic classification.
f. After collecting all possible coreferential relations between semantically validated markables, we then proceed to filter out those links that are inconsistent or incompatible according to three criteria: - first criterion: diverse sentiment polarity - second criterion: different argument structure  - third criterion: non related spatiotemporal relations Both argument structure and spatiotemporal relations are collected in the discourse structure which also contains dependence relations expressed by discourse relations in discourse structures; temporal logical relations as computed from an adaptation of Allen's algorithm; and a point of view computed on the basis of presence of ?reportive?
verbs, or direct speech, reported speech, reported indirect speech.
Another criterion we adopt is the nature of semantic similarity computed by the system.
Values below a certain threshold indicate the coreference has been chosen on the basis of weaksimilarity, as may apply to semantic lexical fields.
These are based on thesauri classification.
Some examples below.
As said above, event coreference links require sentiment match, argument identity or semantic similarity.
In particular consider such cases as   <MARK  ID=m34> claimed  </MARK>.
is semantically computed as a communication verb on a par with SAY, but coreference is prevented by the fact that arguments don't coincide.
SAY in all its various forms is used to report what the newspaper Hawlani said.
Here CLAIM is related to different arguments as shown in the discourse structure entry, ds(to(7-17),7-18,claim([id86:[ansar,sunna,al],id4:suspect,id87:statement],1,id71),during(tes(sn19evs7),tes(sn31evs6)), narration,'ansar_al-sunna') The same applies to the use of KILL in the last sentence (11) whose argument structure prevents a coreference link with the previous occurrence of an identical verb form in sentence (2).
Here below are the two discourse structures containing argument structures for the verb KILL in the two sentences:  ds(down(11-28),11-29,kill([id140:[[abdullah,shami,al],leader],id145:exist],1,id71), after(tes(f562evs11),tes(f772evs11)), narration,narrator), ds(to(2-3),2-4,kill([id16:member,id18:people],1,univ),after(tes(f4_evs_2),tes(f2_evs_1)),result,narrator), Discourse Structures also contain temporal logical relations, Discourse relation and Point of View.
If we consider all computed markables, which are in our system 67, we come up with 47 possible coreference links.
However only 17 have been regarded admissible and consistent and are listed here below.
1.coref-ident m1 m7 hypothetical_certainty 1 2.coref-ident m3 m17 hypothetical_certainty 1 3.coref-simil m2 m20 hypothetical_certainty 0.9 4.coref-simil m4 m14 hypothetical_certainty 0.9 5.coref-ident m7 m25 hypothetical_certainty 1 6.coref-simil m10 m21 hypothetical_certainty 0.9 7.coref-ident m6 m28 hypothetical_certainty 178.coref-ident m7 m29 hypothetical_certainty 1 9.coref-ident m7 m33 hypothetical_certainty 1 10.coref-simil m1 m36 hypothetical_certainty 0.9 11.coref-simil m11 m35 hypothetical_certainty 0.9 12.coref-simil m15 m37 hypothetical_certainty 0.9 13.coref-ident m6 m43 hypothetical_certainty 1 14.coref-ident m7 m38 hypothetical_certainty 1 15.coref-ident m14 m40 hypothetical_certainty 1 16.coref-simil m32 m47 hypothetical_certainty 0.9 17.coref-ident m7 m48 hypothetical_certainty 1  Markables M1, M7, M25, M33, M38, M48 all refer to verb SAY and have as SUBJect the newspaper; in one case M1 is wrongly coreferred to M36, STATEMENT.
M3 is attached to the noun SUSPECT and is made to corefer to M17, the verb SUSPECT which share arguments with the noun.
M2 is ?Al_Qaeda-linked?
and is coreferred to M20, ?linked?.
M4 is BLASTS and is coreferred to M14, ATTACK.
M10 is TWIN and is coreferred to M21, PAIR.
M6 is ?Kurdish?
coreferred to M28 again Kurdish, but also M43.
M11 is ?suicide_bombing?
which corefers to M35, ?bombings?.
M15 is ?post-war?
and is wrongly coreferred to M37 ?posted?.
M14 ATTACK is coreferred to M40 again ATTACK.
M32 MIXED is wrongly coreferred to M47 COALITION.
There are three errors over 17.
Omitted links include the following one coref- (m4- (blasts-blast-id5))- (m8- (blew-blow_up-id26))-5 where coreference between BLAST and BLOW_UP is established and the score assigned is 0.5.
This score is regarded too low and is filtered out, even though a causal link was clearly inferrable.
5.
Conclusion and Future Work  We show here below in Table 1. total counts for the 13 texts distributed with the Event Coreference Task.
The system computed automatically Controllers and Antecedents: the first are referred to syntactically controlled Null Elements of Relative and Interrogative Clauses.
The second are referred to SUBJects of infinitivals, and other  predicative structures both argumental and non-argumental.
The table also includes counts of Markables and Coreferent Links, again computed automatically.
There is no evaluation yet available.
What we wanted to show is the proportion of NEswith respect to sentences, which is 1.6 per sentence, that is there are three NEs every two sentences.
LI/Rounds Round1 Round2 Round3 Total Markables 334 372 325 1031 Corefs 72 79 37 188 Controllers 69 57 55 181 Antecedents 60 57 53 170 Sentences 69 78 72 219 Total Null  Elementss  129  114  108  351 Table 1.
Null Elements, Markables and Coreferents automatically computed by Getaruns on the 13 texts of the Task  In this paper we presented ongoing work to produce a system for event coreference that uses a linguistically-based approach and the output of a deep system for the representation of a text in a situation semantics framework.
The output of the system on the sample text has been fairly consistent in particular for what concerns the computation of implicit information which we regard paramount for a successful performance in the task at hand.
Semantic relations have been built taking into due account all attributes and modifiers of the semantic head.
This process has allowed preventing coreference to take place on the basis of simple concept matching procedures.
Some inferential processes have been fired using commonsense knowledge stored in the publicly available resource, ConceptNet.
Besides, the computation of temporal relations based on a revised version of Allen's algorithm has allowed to control inclusion relations intervening between event structures.
The output of the system includes a discourse structure which shows coordination and subordination links between discourse stretches defined by propositional level analysis.
Structural inclusion is allowed again only in presence of same TOPIC and same spatiotemporal relation checking.
Both NEW topic and NEW spatiotemporal relation will cause the structure to jump up to any possible previous node that may be used to provide a cohesion link in the text.
This notion of coreference has not been explored yet and will be the topic of further study.8REFERENCES Alshawi, H., Pi-Chuan Chang, M. Ringgaard.
(2011).
Deterministic Statistical Mapping of Sentences to Underspecified Semantics, in Johan Bos and Stephen Pulman (editors), Proceedings of the 9th International Conference on Computational Semantics, IWCS,15-24.
Bender, E.M. and D.Flickinger (2005).
Rapid prototyping of scalable grammars: Towards modularity in extensions to a language-independent core.
in Proc.
2nd IJCNLP-05, Jeju Island, Korea.
Bender, E.M., D.Flickinger, and S.Oepen (2002).
The Grammar Matrix: An open-source starter-kit for the rapid development of cross-linguistically consistent broad-coverage precision grammars.
In J.Carroll et alEds.
), Proc.
Workshop Grammar Engineering and Evaluation at COLING19, Taipei, Taiwan, 8-14.
Bresnan, J., 2000.
Lexical-Functional Syntax, Blackwell.
Cai, Shu, David Chiang, Yoav Goldberg, 2011.
Language-Independent Parsing with Empty Elements, in Proceedings of the 49th Annual Meeting of the ACL, 212?216.
Choi, Jinho D., Martha Palmer, 2010.
Robust Constituent-to-Dependency Conversion for English, in Proceedings of the 9th International Workshop on Treebanks and Linguistic Theories (TLT'9), 55-66, Tartu, Estonia.
Clark P., C. Fellbaum, J. Hobbs, P. Harrison, W.R.Murray, J. Thompson, 2008.
Augmenting WordNet for Deep Understanding of Text, in J.Bos & R.Delmonte(eds.
), 2008.
ACL-SigSem, STEP (Semantics in Text Processing), College Publications, London, p.45-58.
Copestake, Ann.
2004/2006.
Robust Minimal Recursion Semantics, Unpublished draft (downloadable from http://www.cl.cam.ac.uk/~aac10/papers.html).
Copestake, Ann, (2009).
Invited Talk: Slacker Semantics: Why Superficiality, Dependency and Avoidance of Commitment can be the Right Way to Go.
In: Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009), pages 1-9.
Athens, Greece, 2009.
Copestake, A., D.Flickinger, C.Pollard, and I.Sag (2005).
Minimal recursion semantics: An introduction.
Research on Language and Computations 3(4), 281-332.
CoreLex:- http://www.cs.brandeis.edu/~paulb/CoreLex/ corelex.html EuroWordNet:- http://www.illc.uva.nl/EuroWordNet/ Delmonte R.(1990), Semantic Parsing with an LFG-based Lexicon and Conceptual Representations, Computers & the Humanities, 5-6, pp.461-488.
Delmonte R., D.Bianchi(1991), Binding Pronominals with an LFG Parser, Proceeding of the Second International Workshop on Parsing Technologies,Cancun(Messico), ACL 1991, pp.59-72.
Delmonte R.(1995), Lexical Representations: Syntax-Semantics interface and World Knowledge, in Rivista dell'AI*IA (Associazione Italiana di Intelligenza Artificiale), Roma, pp.11-16.
Delmonte R.(1996),  Lexical Representations, Event Structure and Quantification, Quaderni Patavini di Linguistica 15, 39-93.
Bianchi D., Delmonte R. (1996),  Temporal Logic in Sentence and Discourse, in Atti SIMAI'96, pp.226-228.
Dario Bianchi, Rodolfo Delmonte(1999), Reasoning with A Discourse Model and Conceptual Representations, Proc.
VEXTAL, Unipress, pp.
401-411.
Delmonte R.(2002),  From Deep to Shallow Anaphora Resolution:, in Proc.
DAARC2002 , 4th Discourse Anaphora and Anaphora Resolution Colloquium, Lisbona, pp.57-62.
Delmonte R.(2002), GETARUN PARSER - A parser equipped with Quantifier Raising and Anaphoric Binding based on LFG, Proc.
LFG2002 Conference, Athens, pp.130-153, at http://cslipublications.
stanford.edu/hand/miscpubsonline.html.
Delmonte R., Sara Tonelli, Marco Aldo Piccolino Boniforti, Antonella Bristot, Emanuele Pianta (2005), VENSES ?
a Linguistically-Based System for Semantic Evaluation, in Joaquin Qui?onero-Candela, Ido Dagan, Bernardo Magnini, Florence d?Alch?-Buc, 2005, Machine Learning Challenges.
Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Textual Entailment.
: First PASCAL Machine Learning Challenges Workshop, MLCW 2005, Southampton, UK, April 11-13, 2005, Revised Selected Papers, 344-371.
Delmonte, R., Antonella Bristot, Marco Aldo Piccolino Boniforti and Sara Tonelli, 2006.
Another Evaluation of Anaphora Resolution Algorithms and a Comparison with GETARUNS' Knowledge Rich Approach, ROMAND 2006, 11th EACL, Trento, Association for Computational Linguistics, 3-10.
Delmonte, R., A. Bristot, M.A.Piccolino Boniforti and S. Tonelli, 2006.
Coping with semantic uncertainty with VENSES, in Bernardo Magnini, Ido Dagan(eds.
), Proceedings of the Challenges Workshop - The 2nd PASCAL Recognizing Textual Entailment Challenge, 86-91, Universit?
Ca' Foscari, Venezia.
Delmonte R., (2007), Computational Linguistic Text Processing ?
Logical Form, Semantic Interpretation, Discourse Relations and Question Answering, Nova Science Publishers, New York.
Delmonte R., A. Bristot, M.A.Piccolino Boniforti, S.Tonelli (2007), Entailment and Anaphora Resolution in RTE3, in Proc.
ACL Workshop on Text Entailment and Paraphrasing, Prague, ACL9Madison, USA, pp.
48-53.
Bos Johan & Rodolfo Delmonte (eds.
), 2008.
Semantics in Text Processing (STEP), Research in Computational Semantics, Vol.1, College Publications, London.
Delmonte R., 2009.
Computational Linguistic Text Processing ?
Lexicon, Grammar, Parsing and Anaphora Resolution, Nova Science Publishers, New York.
Delmonte R., G. Nicolae, S. Harabagiu, C.Nicolae (2007), A Linguistically-based Approach to Discourse Relations Recognition, in B.Sharp & M.Zock(eds.
), Natural Language Processing and Cognitive Science, Proc.
4th NLPCS, Funchal, Portugal, INSTICC PRESS, pp.
81-91.
Delmonte R., G. Nicolae, S. Harabagiu (2007), A Linguistically-based Approach to Detect Causality Relations in Unrestricted Text, in Proc.
MICAI-2007, IEEE Publications, 173-185.
Delmonte R., 2008.
Semantic and Pragmatic Computing with GETARUNS, in Bos & Delmonte (eds.
), Semantics in Text Processing (STEP), Research in Computational Semantics, Vol.1, College Publications, London, pp.
287-298.
Delmonte R., E. Pianta, (2009), Computing Implicit Entities and Events for Story Understanding, in H.Bunt, V.Petukhova and S.Wubben(eds.
), Proc.
Eighth International Conference on Computational Semantics IWCS-8, Tilburg University Press, pp.
277-281.
Delmonte R., (2009), A computational approach to implicit entities and events in text and discourse, in International Journal of Speech Technology (IJST), Springer, pp.
1-14.
Gabbard, Ryan, Mitchell Marcus, Seth Kulick, 2006.
Fully Parsing the Penn Treebank, in Proceedings of the HLT Conference of the North American Chapter of the ACL, 184?191.
Grice, H. P., 1975.
Logic and Conversation.
in P. Cole & J. L. Morgan, Syntax and Semantics, Vol.
3: Speech Acts.
New York : Academic Press, 41-58.
Harabagiu, S.M., Miller, G.A., Moldovan, D.I.
: eXtended WordNet - A Morphologically andSemantically Enhanced Resource (2003), http://xwn.hlt.utdallas.edu, 1-8.
Hobbs, J.
(2005).
Toward a useful notion of causality for lexical semantics.
Journal of Semantics 22, 181?209.
Hobbs, J.
(2008).
Encoding commonsense knowledge.
Technical report, USC/ISI.
http://www.isi.edu/?hobbs/csk.html.
Johansson, R. and P. Nugues.
2007.
Extended Constituent-to-dependency Conversion for English.
In Proceedings of NODALIDA 2007, Tartu, Estonia.
Johnson.
M., 2001.
Joint and conditional estimation of tagging and parsing models.
In ACL 2001, pages 322?329.
Liu, H., Singh, P. (2004).
ConceptNet: A Practical Commonsense Reasoning Toolkit.
Marcus, M., G. Kim, M. Ann Marcinkiewicz, R. Macintyre, A. Bies, M. Ferguson, K. Katz, B. Schasberger, (1994).
The Penn Treebank: Annotating Predicate Argument Structure, In ARPA Human Language Technology Workshop, 114-119.
Sagae, K. and Tsujii, J.
2008.
Shift-Reduce Dependency DAG Parsing.
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008).
Manchester, UK.
Yang, Yaqin and Nianwen Xue.
2010.
Chasing the ghost: recovering empty categories in the Chinese Treebank.
In Proc.
COLING.Schubert, L. and C. Hwang (1993).
Episodic logic: A situational logic for NLP.
In Situation Theory and Its Applications, pp.
303?337.
Schubert, L. and C.Hwang (1993).
Episodic logic: A situational logic for NLP.
In Peter Aczel, David Israel, Yasuhiro Katagiri, and Stanley Peters, (Eds.
), Situation Theory and its Applications, vol.3, 303-337.
Tonelli, S. & R. Delmonte, 2011.
"Desperately seeking Implicit arguments in text", in RELMS'2011, Workshop on Relational Models of Semantics at ACL 2011 Portland, USA.
pp.54-62.
At:http://web.media.mit.edu/~push/ConceptNet.pdf.10
