Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 543?552,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsModeling Argument Strength in Student EssaysIsaac Persing and Vincent NgHuman Language Technology Research InstituteUniversity of Texas at DallasRichardson, TX 75083-0688{persingq,vince}@hlt.utdallas.eduAbstractWhile recent years have seen a surge of in-terest in automated essay grading, includ-ing work on grading essays with respectto particular dimensions such as promptadherence, coherence, and technical qual-ity, there has been relatively little workon grading the essay dimension of argu-ment strength, which is arguably the mostimportant aspect of argumentative essays.We introduce a new corpus of argumen-tative student essays annotated with argu-ment strength scores and propose a su-pervised, feature-rich approach to auto-matically scoring the essays along thisdimension.
Our approach significantlyoutperforms a baseline that relies solelyon heuristically applied sentence argumentfunction labels by up to 16.1%.1 IntroductionAutomated essay scoring, the task of employingcomputer technology to evaluate and score writ-ten text, is one of the most important educationalapplications of natural language processing (NLP)(see Shermis and Burstein (2003) and Shermis etal.
(2010) for an overview of the state of the artin this task).
A major weakness of many ex-isting scoring engines such as the Intelligent Es-say AssessorTM(Landauer et al, 2003) is that theyadopt a holistic scoring scheme, which summa-rizes the quality of an essay with a single score andthus provides very limited feedback to the writer.In particular, it is not clear which dimension ofan essay (e.g., style, coherence, relevance) a scoreshould be attributed to.
Recent work addresses thisproblem by scoring a particular dimension of es-say quality such as coherence (Miltsakaki and Ku-kich, 2004), technical errors, relevance to prompt(Higgins et al, 2004; Persing and Ng, 2014), or-ganization (Persing et al, 2010), and thesis clarity(Persing and Ng, 2013).
Essay grading softwarethat provides feedback along multiple dimensionsof essay quality such as E-rater/Criterion (Attaliand Burstein, 2006) has also begun to emerge.Our goal in this paper is to develop a com-putational model for scoring the essay dimensionof argument strength, which is arguably the mostimportant aspect of argumentative essays.
Argu-ment strength refers to the strength of the argu-ment an essay makes for its thesis.
An essay witha high argument strength score presents a strongargument for its thesis and would convince mostreaders.
While there has been work on design-ing argument schemes (e.g., Burstein et al (2003),Song et al (2014), Stab and Gurevych (2014a))for annotating arguments manually (e.g., Song etal.
(2014), Stab and Gurevych (2014b)) and auto-matically (e.g., Falakmasir et al (2014), Song etal.
(2014)) in student essays, little work has beendone on scoring the argument strength of studentessays.
It is worth mentioning that some work hasinvestigated the use of automatically determinedargument labels for heuristic (Ong et al, 2014)and learning-based (Song et al, 2014) essay scor-ing, but their focus is holistic essay scoring, notargument strength essay scoring.In sum, our contributions in this paper are two-fold.
First, we develop a scoring model for the ar-gument strength dimension on student essays us-ing a feature-rich approach.
Second, in order tostimulate further research on this task, we makeour data set consisting of argument strength anno-tations of 1000 essays publicly available.
Sinceprogress in argument strength modeling is hin-dered in part by the lack of a publicly annotatedcorpus, we believe that our data set will be a valu-able resource to the NLP community.2 Corpus InformationWe use as our corpus the 4.5 million word Interna-tional Corpus of Learner English (ICLE) (Granger543Topic Languages EssaysMost university degrees are the-oretical and do not prepare stu-dents for the real world.
They aretherefore of very little value.13 131The prison system is outdated.No civilized society should pun-ish its criminals: it should reha-bilitate them.11 80In his novel Animal Farm,George Orwell wrote ?All menare equal but some are moreequal than others.?
How true isthis today?10 64Table 1: Some examples of writing topics.et al, 2009), which consists of more than 6000essays on a variety of different topics written byuniversity undergraduates from 16 countries and16 native languages who are learners of Englishas a Foreign Language.
91% of the ICLE textsare written in response to prompts that trigger ar-gumentative essays.
We select 10 such prompts,and from the subset of argumentative essays writ-ten in response to them, we select 1000 essays toannotate for training and testing of our essay ar-gument strength scoring system.
Table 1 showsthree of the 10 topics selected for annotation.
Fif-teen native languages are represented in the set ofannotated essays.3 Corpus AnnotationWe ask human annotators to score each of the1000 argumentative essays along the argumentstrength dimension.
Our annotators were selectedfrom over 30 applicants who were familiarizedwith the scoring rubric and given sample essaysto score.
The six who were most consistent withthe expected scores were given additional essaysto annotate.
Annotators evaluated the argumentstrength of each essay using a numerical scorefrom one to four at half-point increments (see Ta-ble 2 for a description of each score).1This con-trasts with previous work on essay scoring, wherethe corpus is annotated with a binary decision(i.e., good or bad) for a given scoring dimension(e.g., Higgins et al (2004)).
Hence, our annota-tion scheme not only provides a finer-grained dis-tinction of argument strength (which can be im-portant in practice), but also makes the predictiontask more challenging.1See our website at http://www.hlt.utdallas.edu/?persingq/ICLE/ for the complete list of argu-ment strength annotations.Score Description of Argument Strength4 essay makes a strong argument for its thesisand would convince most readers3 essay makes a decent argument for its thesisand could convince some readers2 essay makes a weak argument for its thesis orsometimes even argues against it1 essay does not make an argument or it is oftenunclear what the argument isTable 2: Descriptions of the meaning of scores.To ensure consistency in annotation, we ran-domly select 846 essays to have graded by mul-tiple annotators.
Though annotators exactly agreeon the argument strength score of an essay only26% of the time, the scores they apply fall within0.5 points in 67% of essays and within 1.0 point in89% of essays.
For the sake of our experiments,whenever the two annotators disagree on an es-say?s argument strength score, we assign the es-say the average the two scores rounded down tothe nearest half point.
Table 3 shows the numberof essays that receive each of the seven scores forargument strength.score 1.0 1.5 2.0 2.5 3.0 3.5 4.0essays 2 21 116 342 372 132 15Table 3: Distribution of argument strength scores.4 Score PredictionWe cast the task of predicting an essay?s argumentstrength score as a regression problem.
Using re-gression captures the fact that some pairs of scoresare more similar than others (e.g., an essay withan argument strength score of 2.5 is more similarto an essay with a score of 3.0 than it is to onewith a score of 1.0).
A classification system, bycontrast, may sometimes believe that the scores1.0 and 4.0 are most likely for a particular essay,even though these scores are at opposite ends ofthe score range.
In the rest of this section, we de-scribe how we train and apply our regressor.Training the regressor.
Each essay in the train-ing set is represented as an instance whose labelis the essay?s gold score (one of the values shownin Table 3), with a set of baseline features (Sec-tion 5) and up to seven other feature types we pro-pose (Section 6).
After creating training instances,we train a linear regressor with regularization pa-rameter c for scoring test essays using the linearSVM regressor implemented in the LIBSVM soft-ware package (Chang and Lin, 2001).
All SVM-specific learning parameters are set to their default544values except c, which we tune to maximize per-formance on held-out validation data.2Applying the regressor.
After training the re-gressor, we use it to score the test set essays.
Testinstances are created in the same way as the train-ing instances.
The regressor may assign an essayany score in the range of 1.0?4.0.5 Baseline SystemsIn this section, we describe two baseline systemsfor predicting essays?
argument strength scores.5.1 Baseline 1: Most Frequent BaselineSince there is no existing system specifically forscoring argument strength, we begin by designinga simple baseline.
When examining the score dis-tribution shown in Table 3, we notice that, whilethere exist at least a few essays having each of theseven possible scores, the essays are most denselyclustered around scores 2.5 and 3.0.
A system thatalways predicts one of these two scores will veryfrequently be right.
For this reason, we developa most frequent baseline.
Given a training set,Baseline 1 counts the number of essays assignedto each of the seven scores.
From these counts, itdetermines which score is most frequent and as-signs this most frequent score to each test essay.5.2 Baseline 2: Learning-based Ong et alOur second baseline is a learning-based version ofOng et al?s (2014) system.
Recall from the intro-duction that Ong et al presented a rule-based ap-proach to predict the holistic score of an argumen-tative essay.
Their approach was composed of twosteps.
First, they constructed eight heuristic rulesfor automatically labeling each of the sentencesin their corpus with exactly one of the followingargument labels: OPPOSES, SUPPORTS, CITA-TION, CLAIM, HYPOTHESIS, CURRENT STUDY,or NONE.
After that, they employed these sen-tence labels to construct five heuristic rules toholistically score a student essay.We create Baseline 2 as follows, employing themethods described in Section 4 for training, pa-rameter tuning, and testing.
We employ Ong etal.
?s method to tag each sentence of our essayswith an argument label, but modify their methodto accommodate differences between their and ourcorpus.
In particular, our more informal corpus2For parameter tuning, we employ the following c values:100101, 102, 103, 104, 105, or 106.# Rule1 Sentences that begin with a comparison dis-course connective or contain any string prefixesfrom ?conflict?
or ?oppose?
are tagged OP-POSES.2 Sentences that begin with a contingency con-nective are tagged SUPPORTS.3 Sentences containing any string prefixes from?suggest?, ?evidence?, ?shows?, ?Essentially?,or ?indicate?
are tagged CLAIM.4 Sentences in the first, second, or last paragraphthat contain string prefixes from ?hypothes?,or ?predict?, but do not contain string prefixesfrom ?conflict?
or ?oppose?
are tagged HY-POTHESIS.5 Sentences containing the word ?should?
thatcontain no contingency connectives or stringprefixes from ?conflict?
or ?oppose?
are alsotagged HYPOTHESIS.6 If the previous sentence was tagged hypothesisand this sentence begins with an expansion con-nective, it is also tagged HYPOTHESIS.7 Do not apply a label to this sentence.Table 4: Sentence labeling rules.does not contain CURRENT STUDY or CITATIONsentences, so we removed portions of rules thatattempt to identify these labels (e.g.
portions ofrules that search for a four-digit number, as wouldappear as the year in a citation).
Our resulting ruleset is shown in Table 4.
If more than one of theserules applies to a sentence, we tag it with the labelfrom the earliest rule that applies.After labeling all the sentences in our corpus,we then convert three of their five heuristic scor-ing rules into features for training a regressor.3The resulting three features describe (1) whetheran essay contains at least one sentence labeled HY-POTHESIS, (2) whether it contains at least one sen-tence labeled OPPOSES, and (3) the sum of CLAIMsentences and SUPPORTS sentences divided by thenumber of paragraphs in the essay.
If the value ofthe last feature exceeds 1, we instead assign it avalue of 1.
These features make sense because,for example, we would expect essays containinglots of SUPPORTS sentences to offer stronger ar-guments.6 Our ApproachOur approach augments the feature set available toBaseline 2 with seven types of novel features.1.
POS N-grams (POS) Word n-grams, thoughcommonly used as features for training text clas-sifiers, are typically not used in automated essay3We do not apply the remaining two of their heuristicscoring rules because they deal solely with current studiesand citations.545grading.
The reason is that any list of word n-gramfeatures automatically compiled from a given setof training essays would be contaminated withprompt-specific n-grams that may make the result-ing regressor generalize less well to essays writtenfor new prompts.To generalize our feature set in a way that doesnot risk introducing prompt-dependent features,we introduce POS n-gram features.
Specifically,we construct one feature from each sequence of1?5 part-of-speech tags appearing in our corpus.In order to obtain one of these features?
values fora particular essay, we automatically label each es-say with POS tags using the Stanford CoreNLPsystem (Manning et al, 2014), then count thenumber of times the POS tag sequence occurs inthe essay.
An example of a useful feature of thistype is ?CC NN ,?, as it is able to capture whena student writes either ?for instance,?
or ?for ex-ample,?.
We normalize each essay?s set of POSn-gram features to unit length.2.
Semantic Frames (SFR) While POS n-gramsprovide syntactic generalizations of word n-grams,FrameNet-style semantic role labels provide se-mantic generalizations.
For each essay in our dataset, we employ SEMAFOR (Das et al, 2010) toidentify each semantic frame occurring in the es-say as well as each frame element that participatesin it.
For example, a semantic frame may describean event that occurs in a sentence, and the event?sframe elements may be the people or objects thatparticipate in the event.
For a more concrete exam-ple, consider the sentence ?I said that I do not be-lieve that it is a good idea?.
This sentence containsa Statement frame because a statement is madein it.
One of the frame elements participating inthe frame is the Speaker ?I?.
From this frame,we would extract a feature pairing the frame to-gether with its frame element to get the feature?Statement-Speaker-I?.
We would expect this fea-ture to be useful for argument strength scoring be-cause we noticed that essays that focus excessivelyon the writer?s personal opinions and experiencestended to receive lower argument strength scores.As with POS n-grams, we normalize each es-say?s set of Semantic Frame features to unit length.3.
Transitional Phrases (TRP) We hypothesizethat a more cohesive essay, being easier for areader to follow, is more persuasive, and thusmakes a stronger argument.
For this reason, itwould be worthwhile to introduce features thatmeasure how cohesive an essay is.
Consequently,we create features based on the 149 transitionalphrases compiled by Study Guides and Strate-gies4.
Study Guides and Strategies collected thesetransitions into lists of phrases that are useful fordifferent tasks (e.g.
a list of transitional phrases forrestating points such as ?in essence?
or ?in short?
).There are 14 such lists, which we use to general-ize transitional features.
Particularly, we constructa feature for each of the 14 phrase type lists.
Foreach essay, we assign the feature a value indicat-ing the average number of transitions from the listthat occur in the essay per sentence.
Despite be-ing phrase-based, transitional phrases features aredesigned to capture only prompt-independent in-formation, which as previously mentioned is im-portant in essay grading.4.
Coreference (COR) As mentioned in our dis-cussion of transitional phrases, a strong argumentmust be cohesive so that the reader can under-stand what is being argued.
While the transi-tional phrases already capture one aspect of this,they cannot capture when transitions are made viarepeated mentions of the same entities in differ-ent sentences.
We therefore introduce a set of 19coreference features that capture information suchas the fraction of an essay?s sentences that mentionentities introduced in the prompt, and the averagenumber of total mentions per sentence.5Calculat-ing these feature values, of course, requires thatthe text be annotated with coreference informa-tion.
We automatically coreference-annotate theessays using the Stanford CoreNLP system.5.
Prompt Agreement (PRA) An essay?sprompt is always either a single statement, or canbe split up into multiple statements with which awriter may AGREE STRONGLY, AGREE SOME-WHAT, be NEUTRAL, DISAGREE SOMEWHAT,DISAGREE STRONGLY, NOT ADDRESS, or ex-plicitly have NO OPINION on.
We believe in-formation regarding which of these categories awriter?s opinion falls into has some bearing on thestrength of her argument because, for example, awriter who explicitly mentions having no opinionhas probably not made a persuasive argument.For this reason, we annotate a subset of 830 ofour ICLE essays with these agreement labels.
Wethen train a multiclass maximum entropy classifier4http://www.studygs.net/wrtstr6.htm5See our website at http://www.hlt.utdallas.edu/?persingq/ICLE/ for a complete list of corefer-ence features.546using MALLET (McCallum, 2002) for identifyingwhich one of these seven categories an author?sopinion falls into.
The feature set we use for thistask includes POS n-gram and semantic frame fea-tures as described earlier in this section, lemma-tized word 1-3 grams, the keyword and prompt ad-herence keyword features we described in Persingand Ng (2013) and Persing and Ng (2014), respec-tively, and a feature indicating which statement inthe prompt we are attempting to classify the au-thor?s agreement level with respect to.Our classifier?s training set in this case is thesubset of prompt agreement annotated essays thatfall within the training set of our 1000 essay ar-gument strength annotated data.
We then applythe trained classifier to our entire 1000 essay setin order to obtain predictions from which we canthen construct features for argument strength scor-ing.
For each prediction, we construct a featureindicating which of the seven classes the classifierbelieves is most likely, as well as seven additionalfeatures indicating the probability the classifier as-sociates with each of the seven classes.We produce additional related annotations onthis 830 essay set in cases when the annotatedopinion was neither AGREE STRONGLY nor DIS-AGREE STRONGLY, as the reason the annotatorchose one of the remaining five classes may some-times offer insight into the writer?s argument.
Theclasses of reasons we annotate include cases whenthe writer: (1) offered CONFLICTING OPINIONS,(2) EXPLICITLY STATED an agreement level, (3)gave only a PARTIAL RESPONSE to the prompt,(4) argued a SUBTLER POINT not capturable byextreme opinions, (5) did not make it clear thatthe WRITER?S POSITION matched the one she ar-gued, (6) only BRIEFLY DISCUSSED the topic,(7) CONFUSINGLY PHRASED her argument, or (8)wrote something whose RELEVANCE to the topicwas not clear.
We believe that knowing which rea-son(s) apply to an argument may be useful for ar-gument strength scoring because, for example, theCONFLICTING OPINIONS class indicates that theauthor wrote a confused argument, which proba-bly deserves a lower argument strength score.We train eight binary maximum entropy classi-fiers, one for each of these reasons, using the sametraining data and feature set we use for agreementlevel prediction.
We then use the trained classi-fiers to make predictions for these eight reasons onall 1000 essays.
Finally, we generate features forour argument strength regressor from these predic-tions by constructing two features from each of theeight reasons.
The first binary feature is turned onwhenever the maximum entropy classifier believesthat the reason applies (i.e., when it assigns thereason a probability of over 0.5).
The second fea-ture?s value is the probability the classifier assignsfor this reason.6.
Argument Component Predictions (ACP)Many of our features thus far do not result froman attempt to build a deep understanding of thestructure of the arguments within our essays.
Tointroduce such an understanding into our system,we follow Stab and Gurevych (2014a), who col-lected and annotated a corpus of 90 persuasive es-says (not from the ICLE corpus) with the under-standing that the arguments contained therein con-sist of three types of argument components.
Inone essay, these argument components typicallyinclude a MAJOR CLAIM, several lesser CLAIMswhich usually support or attack the major claim,and PREMISEs which usually underpin the valid-ity of a claim or major claim.Stab and Gurevych (2014b) trained a system toidentify these three types of argument componentswithin their corpus given the components?
bound-aries.
Since our corpus does not contain annotatedargument components, we modify their approachin order to simultaneously identify argument com-ponents and their boundaries.We begin by implementing a maximum entropyversion of their system using MALLET for per-forming the argument component identificationtask.
We feed our system the same structural andlexical features they described.
We then augmentthe system in the following ways.First, since our corpus is not annotated with ar-gument component boundaries, we construct a setof low precision, high recall heuristics for iden-tifying the locations in each sentence where anargument component?s boundaries might occur.The majority of these rules depend primarily ona syntactic parse tree we automatically generatedfor the sentence using the Stanford CoreNLP sys-tem.
Since a large majority of annotated argumentcomponents are substrings of a simple declarativeclause (an S node in the parse tree), we begin byidentifying each S node in the sentence?s tree.Given one of these clauses, we collect a list ofleft and right boundaries where an argument com-ponent may begin or end.
The rules we used to547(a) Potential left boundary locations# Rule1 Exactly where the S node begins.2 After an initial explicit connective, or if the connec-tive is immediately followed by a comma, after thecomma.3 After nth comma that is an immediate child of the Snode.4 After nth comma.
(b) Potential right boundary locations# Rule5 Exactly where the S node ends, or if S ends in apunctuation, immediately before the punctuation.6 If the S node ends in a (possibly nested) SBAR node,immediately before the nth shallowest SBAR.67 If the S node ends in a (possibly nested) PP node,immediately before the nth shallowest PP.Table 5: Rules for extracting candidate argumentcomponent boundary locations.find these boundaries are summarized in Table 5.Given an S node, we use our rules to constructup to l ?
r argument component candidate in-stances to feed into our system by combining eachleft boundary with each right boundary that oc-curs after it, where l is the number of potential leftboundaries our rules found, and r is the number ofright boundaries they found.The second way we augment the system is byadding a boundary rule feature type.
Wheneverwe generate an argument component candidate in-stance, we augment its normal feature set withtwo binary features indicating which heuristic rulewas used to find the candidate?s left boundary, andwhich rule was used to find its right boundary.
Iftwo rules can be used to find the same left or rightboundary position, the first rule listed in the tableis the one used to create the boundary rule feature.This is why, for example, the table contains mul-tiple rules that can find boundaries at comma lo-cations.
We would expect some types of commas(e.g., ones following an explicit connective) to bemore significant than others.A last point that requires additional explanationis that several of the rules contain the word ?nth?.This means that, for example, if a sentence con-tains multiple commas, we will generate multipleleft boundary positions for it using rule 4, and theleft boundary rule feature associated with each po-sition will be different (e.g., there is a unique fea-6The S node may end in an SBAR node which itself has anSBAR node as its last child, and so on.
In this case, the S nodecould be said to end with any of these ?nested?
SBARS, sowe use the position before each (nth) one as a right boundary.ture for the first comma, and for the the secondcomma, etc.
).The last augmentation we make to the systemis that we apply a NONE label to all argumentcomponent candidates whose boundaries do notexactly match those of a gold standard argumentcomponent.
While Stab and Gurevych also didthis, their list of such argument component candi-dates consisted solely of sentences containing noargument components at all.
We could not do this,however, since our corpus is not annotated with ar-gument components and we therefore do not knowwhich sentences these would be.We train our system on all the instances we gen-erated from the 90 essay corpus and apply it to la-bel all the instances we generated in the same wayfrom our 1000 essay ICLE corpus.
As a result, weend up with a set of automatically generated ar-gument component annotations on our 1000 essaycorpus.
We use these annotations to generate fiveadditional features for our argument strength scor-ing SVM regressor.
These features?
values are thenumber of major claims in the essay, the numberof claims in the essay, the number of premises inthe essay, the fraction of paragraphs that containeither a claim or a major claim, and the fractionof paragraphs that contain at least one argumentcomponent of any kind.7.
Argument Errors (ARE) We manually iden-tified three common problems essays might havethat tend to result in weaker arguments, and thuslower argument strength scores.
We heuristicallyconstruct three features, one for each of theseproblems, to indicate to the learner when we be-lieve an essay has one of these problems.It is difficult to make a reasonably strong argu-ment in an essay that is too short.
For this reason,we construct a feature that encodes whether the es-say has 15 or fewer sentences, as only about 7% ofour essays are this short.In the Stab and Gurevych corpus, only about5% of paragraphs have no claims or major claimsin them.
We believe that an essay that containstoo many of these claim or major claim-less para-graphs may have an argument that is badly struc-tured, as it is typical for a paragraph to containone or two (major) claim(s).
For this reason, weconstruct a feature that encodes whether more thanhalf of the essay?s paragraphs contain no claims ormajor claims, as indicated by the previously gen-erated automatic annotations.548Similarly, only 5% of the Stab and Gurevych es-says contain no argument components at all.
Webelieve that an essay that contains too many ofthese component-less paragraphs is likely to havetaken too much space discussing issues that are notrelevant to the main argument of the essay.
Forthis reason, we construct a feature that encodeswhether more than one of the essay?s paragraphscontain no components, as indicated by the previ-ously generated automatic annotations.7 EvaluationIn this section, we evaluate our system for argu-ment strength scoring.
All the results we reportare obtained via five-fold cross-validation experi-ments.
In each experiment, we use 60% of our la-beled essays for model training, another 20% forparameter tuning and feature selection, and the fi-nal 20% for testing.
These correspond to the train-ing set, held-out validation data, and test set men-tioned in Section 4.7.1 Scoring MetricsWe employ four evaluation metrics.
As we willsee below, S1, S2, and S3 are error metrics, solower scores on them imply better performance.In contrast, PC is a correlation metric, so highercorrelation implies better performance.The simplest metric, S1, measures the fre-quency at which a system predicts the wrong scoreout of the seven possible scores.
Hence, a systemthat predicts the right score only 25% of the timewould receive an S1 score of 0.75.The S2 metric measures the average distancebetween a system?s predicted score and the actualscore.
This metric reflects the idea that a systemthat predicts scores close to the annotator-assignedscores should be preferred over a system whosepredictions are further off, even if both systemsestimate the correct score at the same frequency.The S3 metric measures the average square ofthe distance between a system?s score predictionsand the annotator-assigned scores.
The intuitionbehind this metric is that not only should we prefera system whose predictions are close to the anno-tator scores, but we should also prefer one whosepredictions are not too frequently very far awayfrom the annotated scores.
The three error metricscores are given by:1N?Aj6=E?j1,1NN?j=1|Aj?
Ej|,1NN?j=1(Aj?
Ej)2System S1 S2 S3 PCBaseline 1 .668 .428 .321 .000Baseline 2 .652 .418 .267 .061Our System .618 .392 .244 .212Table 6: Five-fold cross-validation results for ar-gument strength scoring.where Aj, Ej, and E?jare the annotator assigned,system predicted, and rounded system predictedscores7respectively for essay j, and N is the num-ber of essays.The last metric, PC , computes Pearson?s cor-relation coefficient between a system?s predictedscores and the annotator-assigned scores.
PCranges from ?1 to 1.
A positive (negative) PCimplies that the two sets of predictions are posi-tively (negatively) correlated.7.2 Results and DiscussionFive-fold cross-validation results on argumentstrength score prediction are shown in Table 6.The first two rows show our baseline systems?
per-formances.
The best baseline system (Baseline 2),which recall is a learning-based version of Onget al?s (2014) system, predicts the wrong score65.2% of the time.
Its predictions are off by anaverage of .418 points, the average squared errorof its predictions is .267, and its average Pear-son correlation coefficient with the gold argumentstrength score across the five folds is .061.Results of our system are shown on the thirdrow of Table 6.
Rather than using all of theavailable features (i.e., Baseline 2?s features andthe novel features described in Section 6), oursystem uses only the feature subset selected bythe backward elimination feature selection algo-rithm (Blum and Langley, 1997) that achieves thebest performance on the validation data (see Sec-tion 7.3 for details).
As we can see, our systempredicts the wrong score only 61.8% of the time,predicts scores that are off by an average of .392points, the average squared error of its predictionsis .244, and its average Pearson correlation coeffi-cient with the gold scores is .212.
These numberscorrespond to relative error reductions8of 5.2%,7We round all predictions to 1.0 or 4.0 if they fall outsidethe 1.0?4.0 range and round S1 predictions to the nearesthalf point.8These numbers are calculatedB?OB?Pwhere B is the base-line system?s score, O is our system?s score, and P is a per-fect score.
Perfect scores for error measures and PC are 0and 1 respectively.5496.2%, 8.6%, and 16.1% over Baseline 2 for S1, S2,S3, and PC, respectively, the last three of whichare significant improvements9.
The magnitudes ofthese improvements suggest that, while our systemyields improvements over the best baseline by allfour measures, its greatest contribution is that itspredicted scores are best-correlated with the goldstandard argument strength scores.7.3 Feature AblationTo gain insight into how much impact each of thefeature types has on our system, we perform fea-ture ablation experiments in which we remove thefeature types from our system one-by-one.We show the results of the ablation experimentson the held-out validation data as measured by thefour scoring metrics in Table 7.
The top line ofeach subtable shows what a system that uses allavailable features?s score would be if we removedjust one of the feature types.
So to see how oursystem performs by the PC metric if we removeonly prompt agreement (PRA) features, we wouldlook at the first row of results of Table 7(d) underthe column headed by PRA.
The number here tellsus that the resulting system?s PC score is .303.Since our system that uses all feature types obtainsS1, S2, S3, and PC scores of .521, .366, .218,and .341 on the validation data respectively, theremoval of PRA features costs the complete sys-tem .038 PC points, and thus we can infer that theinclusion of PRA features has a beneficial effect.From row 1 of Table 7(a), we can see that re-moving the Baseline 2 feature set (BAS) yields asystem with the best S1 score in the presence ofthe remaining feature types in this row.
For thisreason, we permanently remove the BAS featuresfrom the system before we generate the results online 2.
We iteratively remove the feature type thatyields a system with the best performance in thisway until we get to the last line, where only onefeature type is used to generate each result.Since the feature type whose removal yields thebest system is always the rightmost entry in a line,the order of column headings indicates the rela-tive importance of the feature types, with the left-most feature types being most important to per-formance and the rightmost feature types beingleast important in the presence of the other featuretypes.
The score corresponding to the best systemis boldfaced for emphasis, indicating that all fea-9All significance tests are paired t-tests with p < 0.05.
(a) Results using the S1 metricSFR ACP TRP PRA POS COR ARE BAS.534 .594 .530 .524 .522 .532 .529 .521.530 .554 .526 .529 .526 .528 .525.534 .555 .525 .531 .528 .522.543 .558 .536 .530 .527.565 .561 .536 .529.563 .547 .539.592 .550(b) Results using the S2 metricPOS PRA ACP TRP BAS SFR COR ARE.370 .369 .375 .367 .367 .366 .366 .365.369 .369 .375 .366 .366 .365 .365.370 .371 .372 .367 .366 .365.374 .374 .376 .368 .366.377 .375 .374 .368.381 .377 .376.385 .382(c) Results using the S3 metricPOS PRA ACP TRP BAS COR ARE SFR.221 .220 .225 .219 .218 .217 .217 .211.220 .219 .221 .214 .212 .211 .211.218 .218 .220 .212 .211 .209.221 .216 .218 .212 .210.224 .217 .218 .212.228 .220 .219.229 .225(d) Results using the PC metricPOS ACP PRA TRP BAS ARE COR SFR.302 .270 .303 .326 .324 .347 .347 .356.316 .300 .327 .344 .361 .366 .371.346 .331 .341 .356 .367 .378.325 .331 .345 .362 .375.297 .331 .339 .360.280 .320 .321.281 .281Table 7: Feature ablation results.
In each subtable, thefirst row shows how our system would perform on the vali-dation set essays if each feature type was removed.
We thenremove the least important feature type, and show in the nextrow how the adjusted system would perform without each re-maining type.ture types appearing to its left are included in thebest system.10It is interesting to note that while the relativeimportance of different feature types does not re-main exactly the same if we measure performancein different ways, we can see that some featuretypes tend to be more important than others in amajority of the four scoring metrics.From these tables, it is clear that POS n-grams10The reason the performances shown in these tables ap-pear so much better than those shown previously is that inthese tables we tune parameters and display results on thevalidation set in order to make it clearer why we chose to re-move each feature type.
In Table 6, by contrast, we tune pa-rameters on the validation set, but display results using thoseparameters on the test set.550S1 S2 S3 PCGold .25 .50 .75 .25 .50 .75 .25 .50 .75 .25 .50 .751.0 2.90 2.90 2.90 2.74 2.74 2.74 2.74 2.74 2.74 2.74 2.74 2.741.5 2.69 2.78 2.89 2.36 2.67 2.78 2.52 2.63 2.71 2.52 2.63 2.812.0 2.61 2.72 2.85 2.54 2.69 2.79 2.60 2.69 2.78 2.60 2.70 2.802.5 2.64 2.71 2.85 2.65 2.75 2.86 2.66 2.75 2.85 2.69 2.79 2.893.0 2.73 2.84 2.92 2.71 2.81 2.91 2.70 2.80 2.90 2.72 2.83 2.903.5 2.74 2.85 2.97 2.78 2.89 3.02 2.79 2.90 3.00 2.81 2.90 2.984.0 2.75 2.87 3.10 2.76 2.85 3.09 2.76 2.83 3.08 2.81 2.86 3.19Table 8: Distribution of regressor scores for our system.
(POS), prompt agreement features (PRA), and ar-gument component predictions (ACP) are the mostgenerally important feature types in roughly thatorder.
They all appear in the leftmost three po-sitions under the tables for metrics S2, S3, andPC , the three metrics by which our system sig-nificantly outperforms Baseline 2.
Furthermore,removing any of them tends to have a larger neg-ative impact on our system than removing any ofthe other feature types.Transitional phrase features (TRP) and Base-line 2 features (BAS), by contrast, are of moremiddling importance.
While both appear in thebest feature sets for the aforementioned metrics(i.e., they appear to the left of the boldfaced entryin the corresponding ablation tables), the impactof their removal is relatively less than that of POS,PRA, or ACP features.Finally, while the remaining three feature typesmight at first glance seem unimportant to argu-ment strength scoring, it is useful to note thatthey all appear in the best performing feature setas measured by at least one of the four scoringmetrics.
Indeed, semantic frame features (SFR)appear to be the most important feature type asmeasured by the S1 metric, despite being one ofthe least useful feature types as measured by theother performance metrics.
From this we learnthat when designing an argument strength scoringsystem, it is important to understand what the ulti-mate goal is, as the choice of performance metriccan have a large impact on what type of systemwill seem ideal.7.4 Analysis of Predicted ScoresTo more closely examine the behavior of our sys-tem, in Table 8 we chart the distributions of scoresit predicts for essays having each gold standardscore.
As an example of how to read this table,consider the number 2.60 appearing in row 2.0 inthe .25 column of the S3 region.
This means that25% of the time, when our system with param-eters tuned for optimizing S3 (including the S3feature set as selected in Table 7(c)) is presentedwith a test essay having a gold standard score of2.0, it predicts that the essay has a score less thanor equal to 2.60.From this table, we see that our system has abias toward predicting more frequent scores as thesmallest entry in the table is 2.36 and the largestentry is 3.19, and as we saw in Table 3, 71.4% ofessays have gold scores in this range.
Neverthe-less, our system does not rely entirely on bias, asevidenced by the fact that each column in the tablehas a tendency for its scores to ascend as the goldstandard score increases, implying that our systemhas some success at predicting lower scores for es-says with lower gold standard argument strengthscores and higher scores for essays with highergold standard argument strength scores.
The ma-jor exception to this rule is line 1.0, but this is tobe expected since there are only two essays hav-ing this gold score, so the sample from which thenumbers on this line are calculated is very small.8 ConclusionWe proposed a feature-rich approach to the newproblem of predicting argument strength scores onstudent essays.
In an evaluation on 1000 argumen-tative essays selected from the ICLE corpus, oursystem significantly outperformed a baseline sys-tem that relies solely on features built from heuris-tically labeled sentence argument function labelsby up to 16.1%.
To stimulate further research onthis task, we make all of our annotations publiclyavailable.AcknowledgmentsWe thank the three anonymous reviewers for theirdetailed comments.
This work was supported inpart by NSFGrants IIS-1147644 and IIS-1219142.Any opinions, findings, conclusions or recommen-dations expressed in this paper are those of the au-thors and do not necessarily reflect the views or of-ficial policies, either expressed or implied, of NSF.551ReferencesYigal Attali and Jill Burstein.
2006.
Automated essayscoring with E-rater v.2.0.
Journal of Technology,Learning, and Assessment, 4(3).Avrim Blum and Pat Langley.
1997.
Selection of rele-vant features and examples in machine learning.
Ar-tificial Intelligence, 97(1?2):245?271.Jill Burstein, Daniel Marcu, and Kevin Knight.
2003.Finding the WRITE stuff: Automatic identificationof discourse structure in student essays.
IEEE Intel-ligent Systems, 18(1):32?39.Chih-Chung Chang and Chih-Jen Lin, 2001.
LIB-SVM: A library for support vector machines.Software available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.Dipanjan Das, Nathan Schneider, Desai Chen, andNoah A. Smith.
2010.
Probabilistic frame-semanticparsing.
In Human Language Technologies: The2010 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, pages 948?956.Mohammad Hassan Falakmasir, Kevin D. Ashley,Christian D. Schunn, and Diane J. Litman.
2014.Identifying thesis and conclusion statements in stu-dent essays to scaffold peer review.
In IntelligentTutoring Systems, pages 254?259.
Springer Interna-tional Publishing.Sylviane Granger, Estelle Dagneaux, Fanny Meunier,and Magali Paquot.
2009. International Corpus ofLearner English (Version 2).
Presses universitairesde Louvain.Derrick Higgins, Jill Burstein, DanielMarcu, and Clau-dia Gentile.
2004.
Evaluating multiple aspectsof coherence in student essays.
In Human Lan-guage Technologies: The 2004 Annual Conferenceof the North American Chapter of the Associationfor Computational Linguistics, pages 185?192.Thomas K. Landauer, Darrell Laham, and Peter W.Foltz.
2003.
Automated scoring and annotation ofessays with the Intelligent Essay AssessorTM.
In Au-tomated Essay Scoring: A Cross-Disciplinary Per-spective, pages 87?112.
Lawrence Erlbaum Asso-ciates, Inc., Mahwah, NJ.Christopher D. Manning, Mihai Surdeanu, John Bauer,Jenny Finkel, Steven J. Bethard, and David Mc-Closky.
2014.
The Stanford CoreNLP natural lan-guage processing toolkit.
In Proceedings of 52ndAnnual Meeting of the Association for Computa-tional Linguistics: System Demonstrations, pages55?60.Andrew Kachites McCallum.
2002.
MALLET: AMachine Learning for Language Toolkit.
http://mallet.cs.umass.edu.Eleni Miltsakaki and Karen Kukich.
2004.
Evaluationof text coherence for electronic essay scoring sys-tems.
Natural Language Engineering, 10(1):25?55.Nathan Ong, Diane Litman, and AlexandraBrusilovsky.
2014.
Ontology-based argumentmining and automatic essay scoring.
In Pro-ceedings of the First Workshop on ArgumentationMining, pages 24?28.Isaac Persing and Vincent Ng.
2013.
Modeling the-sis clarity in student essays.
In Proceedings of the51st Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages260?269.Isaac Persing and Vincent Ng.
2014.
Modeling promptadherence in student essays.
In Proceedings of the52nd Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages1534?1543.Isaac Persing, Alan Davis, and Vincent Ng.
2010.Modeling organization in student essays.
In Pro-ceedings of the 2010 Conference on Empirical Meth-ods in Natural Language Processing, pages 229?239.Mark D. Shermis and Jill C. Burstein.
2003.
Au-tomated Essay Scoring: A Cross-Disciplinary Per-spective.
Lawrence Erlbaum Associates, Inc., Mah-wah, NJ.Mark D. Shermis, Jill Burstein, Derrick Higgins, andKlaus Zechner.
2010.
Automated essay scoring:Writing assessment and instruction.
In InternationalEncyclopedia of Education (3rd edition).
Elsevier,Oxford, UK.Yi Song, Michael Heilman, Beata Beigman Klebanov,and Paul Deane.
2014.
Applying argumentationschemes for essay scoring.
In Proceedings of theFirst Workshop on Argumentation Mining, pages69?78.Christian Stab and Iryna Gurevych.
2014a.
Annotat-ing argument components and relations in persua-sive essays.
In Proceedings of the 25th InternationalConference on Computational Linguistics: Techni-cal Papers, pages 1501?1510.Christian Stab and Iryna Gurevych.
2014b.
Identify-ing argumentative discourse structures in persuasiveessays.
In Proceedings of the 2014 Conference onEmpirical Methods in Natural Language Process-ing, pages 46?56.552
