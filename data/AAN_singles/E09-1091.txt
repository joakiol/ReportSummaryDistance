Proceedings of the 12th Conference of the European Chapter of the ACL, pages 799?807,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsMINT: A Method for Effective and Scalable Mining ofNamed Entity Transliterations from Large Comparable CorporaRaghavendra Udupa         K Saravanan         A Kumaran        Jagadeesh Jagarlamudi*Microsoft Research IndiaBangalore 560080 INDIA[raghavu,v-sarak,kumarana,jags}@microsoft.comAbstractIn this paper, we address the problem of min-ing transliterations of Named Entities (NEs)from large comparable corpora.
We leveragethe empirical fact that multilingual news ar-ticles with similar news content are rich inNamed Entity Transliteration Equivalents(NETEs).
Our mining algorithm, MINT, usesa cross-language document similarity model toalign multilingual news articles and thenmines NETEs from the aligned articles using atransliteration similarity model.
We show thatour approach is highly effective on 6 differentcomparable corpora between English and 4languages from 3 different language families.Furthermore, it performs substantially betterthan a state-of-the-art competitor.1 IntroductionNamed Entities (NEs) play a critical role in manyNatural Language Processing and InformationRetrieval (IR) tasks.
In Cross-Language Infor-mation Retrieval (CLIR) systems, they play aneven more important role as the accuracy of theirtransliterations is shown to correlate highly withthe performance of the CLIR systems (Mandland Womser-Hacker, 2005, Xu and Weischedel,2005).
Traditional methods for transliterationshave not proven to be very effective in CLIR.Machine Transliteration systems (AbdulJaleeland Larkey, 2003; Al-Onaizan and Knight, 2002;Virga and Khudanpur, 2003) usually produceincorrect transliterations and translation lexcionssuch as hand-crafted or statistical dictionaries aretoo static to have good coverage of NEs1 occur-ring in the current news events.
Hence, there is acritical need for creating and continually updat-* Currently with University of Utah.1 New NEs are introduced to the vocabulary of a lan-guage every day.
On an average, 260 and 452 newNEs appeared daily in the XIE and AFE segments ofthe LDC English Gigaword corpora respectively.ing multilingual Named Entity transliterationlexicons.The ubiquitous availability of comparablenews corpora in multiple languages suggests apromising alternative to Machine Transliteration,namely, the mining of Named Entity Translitera-tion Equivalents (NETEs) from such corpora.News stories are typically rich in NEs and there-fore, comparable news corpora can be expectedto contain NETEs (Klementiev and Roth, 2006;Tao et al, 2006).
The large quantity and the per-petual availability of news corpora in many ofthe world?s languages, make mining of NETEs aviable alternative to traditional approaches.
It isthis opportunity that we address in our work.In this paper, we detail an effective and scala-ble mining method, called MINT (MIningNamed-entity Transliteration equivalents), formining of NETEs from large comparable corpo-ra.
MINT addresses several challenges in miningNETEs from large comparable corpora: exhaus-tiveness (in mining sparse NETEs), computa-tional efficiency (in scaling on corpora size),language independence (in being applicable tomany language pairs) and linguistic frugality (inrequiring minimal external linguistic resources).Our contributions are as follows:?
We give empirical evidence for the hypo-thesis that news articles in different languageswith reasonably similar content are rich sourcesof NETEs (Udupa, et al, 2008).?
We demonstrate that the above insight canbe translated into an effective approach for min-ing NETEs from large comparable corpora evenwhen similar articles are not known a priori.?
We demonstrate MINT?s effectiveness on4 language pairs involving 5 languages (English,Hindi, Kannada, Russian, and Tamil) from 3 dif-ferent language families, and its scalability oncorpora of vastly different sizes (2,000 to200,000 articles).?
We show that MINT?s performance is sig-nificantly better than a state of the art method(Klementiev and Roth, 2006).799We discuss the motivation behind our ap-proach in Section 2 and present the details inSection 3.
In Section 4, we describe the evalua-tion process and in Section 5, we present the re-sults and analysis.
We discuss related work inSection 6.2 MotivationMINT is based on the hypothesis that news ar-ticles in different languages with similar contentcontain highly overlapping set of NEs.
Newsarticles are typically rich in NEs as news is aboutevents involving people, locations, organizations,etc2.
It is reasonable to expect that multilingualnews articles reporting the same news eventmention the same NEs in the respective languag-es.
For instance, consider the English and Hindinews reports from the New York Times and theBBC on the second oath taking of President Ba-rack Obama (Figure 1).
The articles are not pa-rallel but discuss the same event.
Naturally, theymention the same NEs (such as Barack Obama,John Roberts, White House) in the respectivelanguages, and hence, are rich sources of NETEs.Our empirical investigation of comparablecorpora confirmed the above insight.
A study of2 News articles from the BBC corpus had, on anaverage, 12.9 NEs and new articles from the TheNew Indian Express, about 11.8 NEs.200 pairs of similar news articles published byThe New Indian Express in 2007 in English andTamil showed that 87% of the single word NEsin the English articles had at least one translitera-tion equivalent in the conjugate Tamil articles.The MINT method leverages this empiricallybacked insight to mine NETEs from such compa-rable corpora.However, there are several challenges to themining process: firstly, vast majority of the NEsin comparable corpora are very sparse; our anal-ysis showed that 80% of the NEs in The NewIndian Express news corpora appear less than 5times in the entire corpora.
Hence, any miningmethod that depends mainly on repeated occur-rences of the NEs in the corpora is likely to missvast majority of the NETEs.
Secondly, the min-ing method must restrict the candidate NETEsthat need to be examined for match to a reasona-bly small number, not only to minimize falsepositives but also to be computationally efficient.Thirdly, the use of linguistic tools and resourcesmust be kept to a minimum as resources areavailable only in a handful of languages.
Finally,it is important to use as little language-specificknowledge as possible in order to make the min-ing method applicable across a vast majority oflanguages of the world.
The MINT method pro-posed in this paper addresses all the above is-sues.8003 The MINT Mining MethodMINT has two stages.
In the first stage, forevery document in the source language side, theset of documents in the target language side withsimilar news content are found using a cross-language document similarity model.
In thesecond stage, the NEs in the source languageside are extracted using a Named Entity Recog-nizer (NER) and, subsequently, for each NE in asource language document, its transliterations aremined from the corresponding target languagedocuments.
We present the details of the twostages of MINT in the remainder of this section.3.1 Finding Similar Document PairsThe first stage of MINT method (Figure 2) workson the documents from the comparable corpora(CS, CT) in languages S and T and produces a col-lection AS,T  of similar article pairs (DS, DT).
Eacharticle pair (DS, DT) in AS,T consists of an article(DS) in language S and an article (DT) in languageT, that have similar content.
The cross-languagesimilarity between DS and DT, as measured by thecross-language similarity model MD, is at least ?> 0.Cross-language Document Similarity Model:The cross-language document similarity modelmeasures the degree of similarity between a pairof documents in source and target languages.We use the negative KL-divergence betweensource and target document probability distribu-tions as the similarity measure.Given two documents DS, DT in source and tar-get languages respectively, withTS VV , denotingthe vocabulary of source and target languages,the similarity between the two documents is giv-en by the KL-divergence measure, -KL(DS || DT),as:??
TTw STTTSTV DwpDwpDwp )|()|(log)|(where p(w | D) is the likelihood of word w in D.As we are interested in target documents whichare similar to a given source document, we canignore the numerator as it is independent of thetarget document.
Finally, expanding p(wT | Ds)as)|()|( SVw TSS wwpDwpSS?
?we specify thecross-language similarity score as follows:Cross-language similarity =)|(log)|()|( TTSTw w SS DwpwwpDwpTVT SVS?
??
?3.2 Mining NETEs from Document PairsThe second stage of the MINT method works oneach pair of articles (DS, DT) in the collection AS,Tand produces a set PS,T of NETEs.
Each pair (?S,?T) in PS,T  consists of an NE ?S in language S, anda token ?T in language T, that are transliterationequivalents of each other.
Furthermore, thetransliteration similarity between ?S and ?T, asmeasured by the transliteration similarity modelMT, is at least ?
> 0.
Figure 3 outlines this algo-rithm.Discriminative Transliteration SimilarityModel:The transliteration similarity model MT measuresthe degree of transliteration equivalence betweena source language and a target language term.Input: Comparable news corpora (CS, CT) in languages (S,T)Crosslanguage Document Similarity Model MD for (S, T)Threshold score ?.Output: Set AS,T of pairs of similar articles (DS, DT) from (CS, CT).1 AS,T  ?
?
;         // Set of Similar articles (DS, DT)2 for each article DS in CS do3     XS   ?
?
;       // Set of candidates for DS.4      for each article dT  in CT  do5         score = CrossLanguageDocumentSimilarity(DS,dT,MD);6         if (score ?
?)
then XS  ?
XS  ?
(dT , score) ;7      end8     DT  = BestScoringCandidate(XS);9    if (DT  ?
?)
then AS,T  ?
AS,T  ?
(DS, DT) ;10 endCrossLanguageSimilarDocumentPairsFigure 2.
Stage 1 of MINTInput:Set AS,T  of similar documents (DS, DT)  in languages(S,T),Transliteration Similarity Model MT for (S, T),Threshold score ?.Output: Set PS,T  of NETEs (?S, ?T) from  AS,T ;1   PS,T  ?
?
;2   for each pair of articles (DS, DT) in AS,T  do3        for each named entity ?S in DS do4            YS ?
?
; // Set of candidates for ?S.5            for each candidate eT  in DT  do6                 score = TransliterationSimilarity(?S, eT, MT) ;7                 if (score ?
?)
then   YS  ?
YS ?
(eT , score) ;8            end9            ?T  = BestScoringCandidate(YS) ;10          if (?T  ?
null) then PS,T  ?
PS,T  ?
(?S, ?T) ;11      end12 endTransliterationEquivalentsFigure 3.
Stage 2 of MINT801We employ a logistic function as our translitera-tion similarity model MT, as follows:TransliterationSimilarity (?S,eT,MT) =),( TS11ewte ????
?where ?
(?S, eT) is the feature vector for the pair(?S, eT) and w is the weights vector.
Note that thetransliteration similarity takes a value in therange [0..1].
The weights vector w is learnt dis-criminatively over a training corpus of knowntransliteration equivalents in the given pair oflanguages.Features: The features employed by the modelcapture interesting cross-language associationsobserved in (?S, eT):?
All unigrams and bigrams from thesource and target language strings.?
Pairs of source string n-grams and targetstring n-grams such that difference in thestart positions of the source and target n-grams is at most 2.
Here n ?
?2,1?
.?
Difference in the lengths of the twostrings.Generative Transliteration Similarity Model:We also experimented with an extension of He?sW-HMM model (He, 2007).
The transition prob-ability depends on both the jump width and theprevious source character as in the W-HMMmodel.
The emission probability depends on thecurrent source character and the previous targetcharacter unlike the W-HMM model (Udupa etal., 2009).
Instead of using any single alignmentof characters in the pair (wS, wT), we marginalizeover all possible alignments:?
?
?
?
?
?11111 ,|,|| 1 ???????
jajajjAmjnm tstpsaapstPjjHere,jt(and resp.is ) denotes the jth (and resp.ith) character in wT (and resp.
wS) and maA 1?
isthe hidden alignment between wT and wS wherejtis aligned tojas, ,m,j ?1?
.
We estimatethe parameters of the model using the EM algo-rithm.
The transliteration similarity score of apair (wS, wT) is log P(wT  | wS) appropriately trans-formed.4 Experimental SetupOur empirical investigation consists of experi-ments in three data environments, with each en-vironment providing answer to specific set ofquestions, as listed below:1.
Ideal Environment (IDEAL): Given a collec-tion AS,T of oracle-aligned article pairs (DS, DT)in S and T, how effective is Stage 2 of MINT inmining NETE from AS,T?2.
Near Ideal Environment (NEAR-IDEAL):Let AS,T  be a collection of similar article pairs(DS, DT) in S and T. Given comparable corpora(CS, CT) consisting of only articles from AS,T, butwithout the knowledge of pairings between thearticles,a.
How effective is Stage 1 of MINT in re-covering AS,T  from (CS, CT) ?b.
What is the effect of Stage 1 on theoverall effectiveness of MINT?3.
Real Environment (REAL): Given largecomparable corpora (CS, CT), how effective isMINT, end-to-end?The IDEAL environment is indeed ideal forMINT since every article in the comparable cor-pora is paired with exactly one similar article inthe other language and the pairing of articles inthe comparable corpora is known in advance.We want to emphasize here that such corpora areindeed available in many domains such as tech-nical documents and interlinked multilingualWikipedia articles.
In the IDEAL environment,only Stage 2 of MINT is put to test, as articlealignments are given.In the NEAR-IDEAL data environment, everyarticle in the comparable corpora is known tohave exactly one conjugate article in the otherlanguage though the pairing itself is not knownin advance.
In such a setting, MINT needs todiscover the article pairing before mining NETEsand therefore, both stages of MINT are put totest.
The best performance possible in this envi-ronment should ideally be the same as that ofIDEAL, and any degradation points to the short-coming of the Stage 1 of MINT.
These two en-vironments quantify the stage-wise performanceof the MINT method.Finally, in the data environment REAL, wetest MINT on large comparable corpora, whereeven the existence of a conjugate article in thetarget side for a given article in the source side ofthe comparable corpora is not guaranteed, as in802any normal large multilingual news corpora.
Inthis scenario both the stages of MINT are put totest.
This is the toughest, and perhaps the typicalsetting in which MINT would be used.4.1 Comparable CorporaIn our experiments, the source language is Eng-lish whereas the 4 target languages are fromthree different language families (Hindi from theIndo-Aryan family, Russian from the Slavic fam-ily, Kannada and Tamil from the Dravidian fami-ly).
Note that none of the five languages use acommon script and hence identification of cog-nates, spelling variations, suffix transformations,and other techniques commonly used for closelyrelated languages that have a common script arenot applicable for mining NETEs.
Table 1 sum-marizes the 6 different comparable corpora thatwere used for the empirical investigation; 4 forthe IDEAL and NEAR-IDEAL environments (in4 language pairs), and 2 for the REAL environ-ment (in 2 language pairs).Cor-pusSource -TargetDataEnviron-mentArticles (inThousands)Words (inMillions)Src Tgt Src TgtEK-SEnglish-KannadaIDEAL&NEAR-IDEAL2.90 2.90 0.42 0.34ET-SEnglish-TamilIDEAL&NEAR-IDEAL2.90 2.90 0.42 0.32ER-SEnglish-RussianIDEAL&NEAR-IDEAL2.30 2.30 1.03 0.40EH-SEnglish-HindiIDEAL&NEAR-IDEAL11.9 11.9 3.77 3.57EK-LEnglish-KannadaREAL 103.8 111.0 27.5 18.2ET-LEnglish-TamilREAL 103.8 144.3 27.5 19.4Table 1: Comparable CorporaThe corpora can be categorized into two sepa-rate groups, group S (for Small) consisting ofEK-S, ET-S, ER-S, and EH-S and group L (forLarge) consisting of EK-L and ET-L. Corpora ingroup S are relatively small in size, and containpairs of articles that have been judged by humanannotators as similar.
Corpora in group L are twoorders of magnitude larger in size than those ingroup S and contain a large number of articlesthat may not have conjugates in the target side.In addition the pairings are unknown even for thearticles that have conjugates.
All comparablecorpora had publication dates, except EH-S,which is known to have been published over thesame year.The EK-S, ET-S, EK-L and ET-L corpora arefrom The New Indian Express news paper, whe-reas the EH-S corpora are from Web Dunia andthe ER-S corpora are from BBC/Lenta NewsAgency respectively.4.2 Cross-language Similarity ModelThe cross-language document similarity modelrequires a bilingual dictionary in the appropriatelanguage pair.
Therefore, we generated statisticaldictionaries for 3 language pairs (from parallelcorpora of the following sizes: 11K sentencepairs in English-Kannada, 54K in English-Hindi,and 14K in English-Tamil) using the GIZA++statistical alignment tool (Och et al, 2003), with5 iterations each of IBM Model 1 and HMM.We did not have access to an English-Russianparallel corpus and hence could not generate adictionary for this language pair.
Hence, theNEAR-IDEAL experiments were not run for theEnglish-Russian language pair.Although the coverage of the dictionaries waslow, this turned out to be not a serious issue forour cross-language document similarity model asit might have for topic based CLIR (Ballesterosand Croft, 1998).
Unlike CLIR, where the queryis typically smaller in length compared to thedocuments, in our case we are dealing with newsarticles of comparable size in both source andtarget languages.When many translations were available for asource word, we considered only the top-4 trans-lations.
Further, we smoothed the documentprobability distributions with collection frequen-cy as described in (Ponte and Croft, 1998).4.3 Transliteration Similarity ModelThe transliteration similarity models for each ofthe 4 language pairs were produced by learningover a training corpus consisting of about 16,000single word NETEs, in each pair of languages.The training corpus in English-Hindi, English-Kannada and English-Tamil were hand-craftedby professionals, the English-Russian name pairswere culled from Wikipedia interwiki links andwere cleaned heuristically.
Equal number ofnegative samples was used for training the mod-els.
To produce the negative samples, we pairedeach source language NE with a random non-matching target language NE.
No language spe-cific features were used and the same feature setwas used in each of the 4 language pairs makingMINT language neutral.In all the experiments, our source side lan-guage is English, and the Stanford Named EntityRecognizer (Finkel et al 2005) was used to ex-tract NEs from the source side article.
It shouldbe noted here that while the precision of the NER803used was consistently high, its recall was low,(~40%) especially in the New Indian Expresscorpus, perhaps due to the differences in the dataused for training the NER and the data on whichwe used it.4.4 Performance MeasuresOur intention is to measure the effectiveness ofMINT by comparing its performance with theoracular (human annotator) performance.
Astransliteration equivalents must exist in thepaired articles to be found by MINT, we focusonly on those NEs that actually have at least onetransliteration equivalent in the conjugate article.Three performance measures are of interest tous: the fraction of distinct NEs from source lan-guage for which we found at least one translitera-tion in the target side (Recall on distinct NEs),the fraction of distinct NETEs (Recall on distinctNETEs) and the Mean Reciprocal Rank (MRR)of the NETEs mined.
Since we are interested inmining not only the highly frequent but also theinfrequent NETEs, recall metrics measure howeffective our method is in mining NETEs ex-haustively.
The MRR score indicates how effec-tive our method is in preferring the correct onesamong candidates.To measure the performance of MINT, wecreated a test bed for each of the language pairs.The test beds are summarized in Table 2.The test beds consist of pairs of similar ar-ticles in each of the language pairs.
It should benoted here that as transliteration equivalents mustexist in the paired articles to be found by MINT,we focus only on those NEs that actually have atleast one transliteration equivalent in the conju-gate article.5 Results & AnalysisIn this section, we present qualitative and quan-titative performance of the MINT algorithm, inmining NETEs from comparable news corpora.All the results in Sections 5.1 to 5.3 were ob-tained using the discriminative transliterationsimilarity model described in Section 3.2.
Theresults using the generative transliteration simi-larity model are discussed in Section 5.4.5.1 IDEAL EnvironmentOur first set of experiments investigated the ef-fectiveness of Stage 2 of MINT, namely the min-ing of NETEs in an IDEAL environment.
AsMINT is provided with paired articles in this ex-periment, all experiments for this environmentwere run on test beds created from group S cor-pora (Table 2).Results in the IDEAL Environment:The recall measures for distinct NEs and distinctNETEs for the IDEAL environment are reportedin Table 3.TestBedRecall (%)Distinct NEs Distinct NETEsEK-ST 97.30 95.07ET-ST 99.11 98.06EH-ST 98.55 98.66ER-ST 93.33 85.88Table 3: Recall of MINT in IDEALNote that in the first 3 language pairs MINT wasable to mine a transliteration equivalent for al-most all the distinct NEs.
The performance inEnglish-Russian pair was relatively worse, per-haps due to the noisy training data.In order to compare the effectiveness ofMINT with a state-of-the-art NETE mining ap-proach, we implemented the time series basedCo-Ranking algorithm based on (Klementiev andRoth, 2006).Table 4 shows the MRR results in the IDEALenvironment ?
both for MINT and the Co-Ranking baseline: MINT outperformed Co-Ranking on all the language pairs, despite notusing time series similarity in the miningprocess.
The high MRRs (@1 and @5) indicatethat in almost all the cases, the top-ranked candi-date is a correct NETE.
Note that Co-Rankingcould not be run on the EH-ST test bed as thearticles did not have a date stamp.
Co-Ranking iscrucially dependent on time series and hence re-quires date stamps for the articles.Test BedComparableCorporaArticlePairsDistinctNEsDistinctNETEsEK-ST EK-S 200 481 710ET-ST ET-S 200 449 672EH-ST EH-S 200 347 373ER-ST ER-S 100 195 347Table 2: Test Beds for IDEAL & NEAR-IDEALTestBedMRR@1 MRR@5MINT CoRanking MINT CoRankingEK-ST 0.94 0.26 0.95 0.29ET-ST 0.91 0.26 0.94 0.29EH-ST 0.93 - 0.95 -ER-ST 0.80 0.38 0.85 0.43Table 4: MINT & Co-Ranking in IDEAL8045.2 NEAR-IDEAL EnvironmentThe second set of experiments investigated theeffectiveness of Stage 1 of MINT on comparablecorpora that are constituted by pairs of similararticles, where the pairing information betweenthe articles is with-held.
MINT reconstructed thepairings using the cross-language document si-milarity model and subsequently mined NETEs.As in previous experiments, we ran our experi-ments on test beds described in Section 4.4.Results in the NEAR-IDEAL Environment:There are two parts to this set of experiments.
Inthe first part, we investigated the effectiveness ofthe cross-language document similarity modeldescribed in Section 3.1.
Since we know theidentity of the conjugate article for every articlein the test bed, and articles can be ranked accord-ing to the cross-language document similarityscore, we simply computed the MRR for thedocuments identified in each of the test beds,considering only the top-2 results.
Further, whereavailable, we made use of the publication date ofarticles to restrict the number of target articlesthat are considered in lines 4 and 5 of the MINTalgorithm in Figure 2.
Table 5 shows the resultsfor two date windows ?
3 days and 1 year.TestBedMRR@1 MRR@23 days 1 year 3 days 1 yearEK-ST 0.99 0.91 0.99 0.93ET-ST 0.96 0.83 0.97 0.87EH-ST - 0.81 - 0.82Table 5: MRR of Stage 1 in NEAR-IDEALSubsequently, the output of the Stage 1 was giv-en as the input to the Stage 2 of the MINT me-thod.
In Table 6 we report the MRR @1 and @5for the second stage, for both time windows (3days & 1 year).It is interesting to compare the results of MINTin NEAR-IDEAL data environment (Table 6)with MINT?s results in IDEAL environment(Table 4).
The drop in MRR@1 is small: ~2%for EK-ST and ~3% for ET-ST. For EH-ST thedrop is relatively more (~12%) as may be ex-pected since the time window (3 days) could notbe applied for this test bed.5.3 REAL EnvironmentThe third set of experiments investigated the ef-fectiveness of MINT on large comparable corpo-ra.
We ran the experiments on test beds createdfrom group L corpora.Test-beds for the REAL Environment: Thetest beds for the REAL environment (Table 7)consisted of only English articles since we do notknow in advance whether these articles have anysimilar articles in the target languages.Results in the REAL Environment: In realenvironment, we examined the top 2 articles ofreturned by Stage 1 of MINT, and mined NETEsfrom them.
We used a date window of 3 in Stage1.
Table 8 summarizes the results for the REALenvironment.We observe that the performance of MINT isimpressive, considering the fact that the compa-rable corpora used in the REAL environment istwo orders of magnitude larger than those used inIDEAL and NEAR-IDEAL environments.
Thisimplies that MINT is able to effectively mineNETEs whenever the Stage 1 algorithm was ableto find a good conjugate for each of the sourcelanguage articles.5.4 Generative Transliteration SimilarityModelWe employed the extended W-HMM translitera-tion similarity model in MINT and used it in theIDEAL data environment.
Table 9 shows theresults.TestBedMRR@1 MRR@53 days 1 year 3 days 1 yearEK-ST 0.92 0.87 0.94 0.90ET-ST 0.88 0.74 0.91 0.78EH-ST - 0.82 - 0.87Table 6: MRR of Stage 2 in NEAR-IDEALTestBedComparableCorporaArticlesDistinctNEsEK-LT EK-L 100 306ET-LT ET-L 100 228Table 7: Test Beds for REALTest BedMRR@1 @5EK-LT 0.86 0.88ET-LT 0.82 0.85Table 8: MRR of Stage 2 in REALTest BedMRR@1 @5EK-S 0.85 0.86ET-S 0.81 0.82EH-S 0.91 0.93Table 9:  MRR of Stage 2 in IDEAL using genera-tive transliteration similarity model805We see that the results for the generative transli-teration similarity model are good but not asgood as those for the discriminative translitera-tion similarity model.
As we did not stem eitherthe English NEs or the target language words,the generative model made more mistakes oninflected words compared to the discriminativemodel.5.5  Examples of Mined NETEsTable 10 gives some examples of the NETEsmined from the comparable news corpora.6  Related WorkCLIR systems have been studied in severalworks (Ballesteros and Croft, 1998; Kraiij et al2003).
The limited coverage of dictionaries hasbeen recognized as a problem in CLIR and MT(Demner-Fushman & Oard, 2002; Mandl &Womser-hacker, 2005; Xu &Weischedel, 2005).In order to address this problem, differentkinds of approaches have been taken, from learn-ing transformation rules from dictionaries andapplying the rules to find cross-lingual spellingvariants (Pirkola et al, 2003), to  learning trans-lation lexicon from monolingual and/or compa-rable corpora (Fung, 1995; Al-Onaizan andKnight, 2002; Koehn and Knight, 2002; Rapp,1996).
While these works have focused on find-ing translation equivalents of all class of words,we focus specifically on transliteration equiva-lents of NEs.
(Munteanu and Marcu, 2006;Quirk et al, 2007) addresses mining of parallelsentences and fragments from nearly parallelsentences.
In contrast, our approach minesNETEs from article pairs that may not even haveany parallel or nearly parallel sentences.NETE discovery from comparable corporausing time series and transliteration model wasproposed in (Klementiev and Roth, 2006), andextended for NETE mining for several languagesin (Saravanan and Kumaran, 2007).
However,such methods miss vast majority of the NETEsdue to their dependency on frequency signatures.In addition, (Klementiev and Roth, 2006) maynot scale for large corpora, as they examineevery word in the target side as a potential trans-literation equivalent.
NETE mining from compa-rable corpora using phonetic mappings was pro-posed in (Tao et al, 2006), but the need for lan-guage specific knowledge restricts its applicabili-ty across languages.
We proposed the idea ofmining NETEs from multilingual articles withsimilar content in (Udupa, et al, 2008).
In thiswork, we extend the approach and provide a de-tailed description of the empirical studies.7  ConclusionIn this paper, we showed that MINT, a simpleand intuitive technique employing cross-language document similarity and transliterationsimilarity models, is capable of mining NETEseffectively from large comparable news corpora.Our three stage empirical investigation showedthat MINT performed close to optimal on com-parable corpora consisting of pairs of similar ar-ticles when the pairings are known in advance.MINT induced fairly good pairings and performsexceedingly well even when the pairings are notknown in advance.
Further, MINT outperformeda state-of-the-art baseline and scaled to largecomparable corpora.
Finally, we demonstratedthe language neutrality of MINT, by miningNETEs from 4 language pairs (between Englishand one of Russian, Hindi, Kannada or Tamil)from 3 vastly different linguistic families.As a future work, we plan to use the ex-tended W-HMM model to get features for thediscriminative transliteration similarity model.We also want to use a combination of the cross-language document similarity score and thetransliteration similarity score for scoring theNETEs.
Finally, we would like to use the minedNETEs to improve the performance of the firststage of MINT.AcknowledgmentsWe thank Abhijit Bhole for his help and ChrisQuirk for valuable comments.LanguagePairSource NE TransliterationEnglish-KannadaWoolmer ??????
?Kafeel ????
?Baghdad ???????
?English-Tamil Lloyd ?????
?Mumbai ?????????
?Manchester ??????????
?English-Hindi Vanhanen ??????
?Trinidad ??????????
?Ibuprofen ?????????
?English-RussianKreuzberg ?????????
?Gaddafi ??????
?Karadzic ???????
?Table 10: Examples of Mined NETEs806ReferencesAbdulJaleel, N. and Larkey, L.S.
2003.
Statistical translite-ration for English-Arabic cross language information re-trieval.
Proceedings of CIKM 2003.Al-Onaizan, Y. and Knight, K. 2002.
Translating namedentities using monolingual and bilingual resources.
Pro-ceedings of the 40th Annual Meeting of ACL.Ballesteros, L. and Croft, B.
1998.
Dictionary Methods forCross-Lingual Information Retrieval.
Proceedings ofDEXA?96.Chen, H., et al 1998.
Proper Name Translation in Cross-Language Information Retrieval.
Proceedings of the 36thAnnual Meeting of the ACL.Demner-Fushman, D., and Oard, D. W. 2002.
The effect ofbilingual term list size on dictionary-based cross-language information retrieval.
Proceedings of the 36thHawaii International Conference on System Sciences.Finkel, J. Trond Grenager, and Christopher Manning.
2005.Incorporating Non-local Information into InformationExtraction Systems by Gibbs Sampling.
Proceedings ofthe 43nd Annual Meeting of the ACL.Fung, P. 1995.
Compiling bilingual lexicon entries from anon-parallel English-Chinese corpus.
Proceedings of the3rd Workshop on Very Large Corpora.Fung, P. 1995.
A pattern matching method for finding nounand proper noun translations from noisy parallel corpora.Proceedings of ACL 1995.He.
X.
2007: Using word dependent transition models inHMM based word alignment for statistical machinetranslation.
In Proceedings of 2nd ACL Workshop on Sta-tistical Machine Translation .Hermjakob, U., Knight, K., and Daume, H. 2008.
Nametranslation in statistical machine translation: knowingwhen to transliterate.
Proceedings ACL 2008.Klementiev, A. and Roth, D. 2006.
Weakly supervisednamed entity transliteration and discovery from multilin-gual comparable corpora.
Proceedings of the 44th AnnualMeeting of the ACL.Knight, K. and Graehl, J.
1998.
Machine Transliteration.Computational Linguistics.Koehn, P. and Knight, K. 2002.
Learning a translation lex-icon from monolingual corpora.
Proceedings of Unsu-pervised Lexical Acquisition.Kraiij, W., Nie, J-Y.
and  Simard, M. 2003.
EmebddingWeb-based Statistical Translation Models in Cross-Language Information Retrieval.
Computational Linguis-tics., 29(3):381-419.Mandl, T., and Womser-Hacker, C.  2004.
How do namedentities contribute to retrieval effectiveness?
Proceedingsof the 2004 Cross Language Evaluation Forum Cam-paign 2004.Mandl, T., and Womser-Hacker, C.  2005.
The Effect ofnamed entities on effectiveness in cross-language infor-mation retrieval evaluation.
ACM Symposium on AppliedComputing.Munteanu, D. and Marcu D. 2006.
Extracting parallel sub-sentential fragments from non-parallel corpora.
Proceed-ings of the ACL 2006.Och, F. and Ney, H. 2003.
A systematic comparison of var-ious statistical alignment models.
Computational Lin-guistics.Pirkola, A., Toivonen, J., Keskustalo, H., Visala, K. andJarvelin, K. 2003.
Fuzzy translation of cross-lingualspelling variants.
Proceedings of SIGIR 2003.Ponte, J. M. and Croft, B.
1998.
A Language ModelingApproach to Information Retrieval.
Proceedings of ACMSIGIR 1998.Quirk, C., Udupa, R. and Menezes, A.
2007.
Generativemodels of noisy translations with applications to parallelfragments extraction.
Proceedings of the 11th MT Sum-mit.Rapp, R. 1996.
Automatic identification of word transla-tions from unrelated English and German corpora.
Pro-ceedings of ACL?99Saravanan, K. and Kumaran, A.
2007.
Some experiments inmining named entity transliteration pairs from compara-ble corpora.
Proceedings of the 2nd International Work-shop on Cross Lingual Information Access.Tao, T., Yoon, S., Fister, A., Sproat, R. and Zhai, C. 2006.Unsupervised named entity transliteration using temporaland phonetic correlation.
Proceedings of EMNLP 2006.Udupa, R., Saravanan, K., Kumaran, A. and Jagarlamudi, J.2008.
Mining Named Entity Transliteration Equivalentsfrom Comparable Corpora.
Proceedings of the CIKM2008.Udupa, R., Saravanan, K., Bakalov, A. and Bhole, A.
2009.?They are out there if you know where to look?
: Miningtransliterations of OOV terms in cross-language informa-tion retrieval.
Proceedings of the ECIR 2009.Virga, P. and Khudanpur, S. 2003.
Transliteration of propernames in cross-lingual information retrieval.
Proceedingsof the ACL Workshop on Multilingual and Mixed Lan-guage Named Entity Recognition.Xu, J. and Weischedel, R. 2005.
Empirical studies on theimpact of lexical resources on CLIR performance.
In-formation Processing and Management.807
