Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 163?171,Montre?al, Canada, June 8, 2012. c?2012 Association for Computational LinguisticsRankPref: Ranking Sentences Describing Relationsbetween Biomedical Entities with an ApplicationCatalina O Tudor K Vijay-ShankerDepartment of Computer and Information SciencesUniversity of Delaware, Newark, DE, USAtudor@cis.udel.edu vijay@cis.udel.eduAbstractThis paper presents a machine learning ap-proach that selects and, more generally, rankssentences containing clear relations betweengenes and terms that are related to them.
Thisis treated as a binary classification task, wherepreference judgments are used to learn how tochoose a sentence from a pair of sentences.Features to capture how the relationship is de-scribed textually, as well as how central therelationship is in the sentence, are used in thelearning process.
Simplification of complexsentences into simple structures is also appliedfor the extraction of the features.
We show thatsuch simplification improves the results by upto 13%.
We conducted three different evalu-ations and we found that the system signifi-cantly outperforms the baselines.1 IntroductionLife scientists, doctors and clinicians often searchfor information relating biological concepts.
For ex-ample, a doctor might be interested to know the im-pact of a drug on some disease.
One source of infor-mation is the knowledge bases and ontologies thatare manually curated with facts from scientific arti-cles.
However, the curation process is slow and can-not keep up with ongoing publications.
Moreover,not all associations between biological concepts canbe found in these databases.Another source of information is the scientificliterature itself.
However, searching for biologicalfacts and how they might be related is often cumber-some.
The work presented in this paper tries to au-tomate the process of finding sentences that clearlydescribe relationships between biological concepts.We rank all sentences mentioning two concepts andpick the top one to show to the user.
In this paper, wefocused on certain specific types of concepts (i.e.,genes1 and terms believed to be related to them), al-though our approach can be generalized.Systems to facilitate knowledge exploration ofgenes are being built for the biomedical domain.One of them, eGIFT (Tudor et al, 2010), tries toidentify iTerms (informative terms) for a gene basedon frequency of co-occurrence (see Figure 1 for top15 terms selected for gene Groucho).
iTerms areunigrams, bigrams, and exact matches of biomedi-cal terms gathered from various controlled vocabu-laries.
Thus, iTerms can be of any type (e.g., pro-cesses, domains, drugs, other genes, etc.
), the typesbeing determined by what is being described aboutthe gene in the literature.
The iTerms for a geneare ranked based on a score that compares their fre-quencies of occurrence in publications mentioningthe gene in question with their frequencies in a back-ground set of articles about a wide variety of genes.Previous evaluation of eGIFT by life scientistssuggested that there is almost always some kind ofrelationship between a gene and its iTerms.
Theserelationships can be many and varied from one gene-term pair to another.
Sometimes a user might makean erroneous assumption about a gene-term asso-ciation if sentences supporting the association arenot immediately inspected.
For example, upon see-ing ?co-repressor?
in connection to gene Groucho,eGIFT users might correctly assume that Groucho is1Throughout the paper, the word ?gene?
will be used forboth the gene and its products.163Figure 1: Top iTerms for gene Groucho, and sentences picked by RankPref for various iTerms.a co-repressor (i.e., a protein that binds to transcrip-tion factors).
However, upon seeing ?wrpw motif?,a user might assume that this is a motif containedwithin gene Groucho, as this is typically the asso-ciation that we make between genes and informa-tion annotated for them in knowledge bases.
Butthis would be a wrong assumption, since in actualitythe wrpw motif is contained within other genes thatinteract with Groucho, fact which is evident fromreading sentences containing the gene and the mo-tif.
To get a quick overall understanding of a gene?sfunctionalities, users of eGIFT could be presentedwith terms extracted for the gene, as well as sen-tences clearly describing how they are related.Our method selects sentences using a model thatis trained on preference judgments provided by biol-ogists.
Example sentences chosen by our method areshown in Figure 1.
While we evaluate our approachon sentences from eGIFT, this work could haveequally applied on other similar systems (Smal-heiser et al, 2008; Gladki et al, 2008; Kim et al,2008; Kaczanowski et al, 2009).
These systemsalso identify ?important terms?
from a set of doc-uments retrieved for a given search (either a genename or other biomedical concept).The main contributions of this work are: (1) amethod for ranking sentences by employing ma-chine learning; (2) the use of preference judgments;(3) features to capture whether two terms are clearlyrelated and in focus in a sentence; (4) another appli-cation of sentence simplification, showing a signifi-cant gain in performance when utilized.We continue with a description of our approach,which includes the use of preference judgments tolearn the models, how the features are extracted, andhow the sentence simplifier is used for this task.
Theevaluation of the trained model and the system?s re-sults are presented in the following section.
Re-lated work, conclusions, and future directions areprovided at the end of the manuscript.2 MethodsRather than pre-judging what is important for thistask and manually determining a weighting schemato automatically score sentences for a gene-termpair, we approached this task using machine learn-ing.
We asked a group of annotators to rank sen-tences relating genes and iTerms, and we used theirannotations, together with features described in Sec-tion 2.3, to learn how to rank sentences.1642.1 Preference JudgmentsFor the annotation task, we presented biologists withsentences containing a gene-term pair and askedthem to specify which sentence they prefer.
Oneway to do this is by employing the pointwise ap-proach, which requires absolute judgments (i.e.
theannotator scores each sentence in a list or ranks thesentences based on their relevance to the given task).A second approach is the pairwise approach, whichrequires the iteration of preference judgments (i.e.,the annotator is presented with two sentences at atime, and is asked to chose one as more relevant tothe task than the other).In order to simplify the annotator?s task, as wellas construct a more reliable training set, we used thepairwise approach.
Our decision was influenced byCarterette et al (2008), who showed that preferencejudgments are faster and easier to make than abso-lute judgments.
Thus, we can obtain many annotatedinstances in a relatively short amount of time.
More-over, since there are only two possible outcomes inchoosing one sentence, we need at most three judgesfor a majority vote.
This will also ensure consistencyin the annotations.
We discuss the model trained onpreference judgments in Section 2.2.2.2 Learned Models: PrefSVM and RankPrefWe used the preference judgments to learn a model,PrefSVM, that picks one sentence from a pair of sen-tences.
This model was built using SVMLight witha linear kernel.
The examples used in the learningprocess correspond to pairs of sentences.
For eachpair, we constructed a vector of feature values, bysubtracting the feature values corresponding to thefirst sentence from the feature values correspondingto the second sentence.
We assigned a positive valueto a pair vector if the first sentence was preferred anda negative value if the second one was preferred.We can also use PrefSVM to design a system thatcan rank all the sentences containing a gene andan iTerm, by performing comparisons between sen-tences in the list.
We call RankPref the system thatpicks one sentence from a group of sentences, andwhich also ranks the entire set of sentences.
Thismethod recursively applies PrefSVM in the followingmanner: Two sentences are randomly picked froma given list of sentences.
PrefSVM chooses one sen-tence and discards the other.
A third sentence is thenrandomly picked from the list, and PrefSVM makesits choice by comparing it to the sentence kept in theprevious step.
This process of picking, comparingand discarding sentences is continued until there isonly one sentence left.
We keep track of comparisonresults and apply transitivity, in order to speed up theprocess of ranking all the sentences.2.3 FeaturesEach sentence is first chunked into base phrases.
Weused Genia Tagger (Tsuruoka et al, 2005), whichprovides part-of-speech tags for every word in thesentence.
We trained a chunker (i.e., shallow parserthat identifies base NPs) using the Genia corpus.We considered typical features that are used inmachine learning approaches, such as distance be-tween gene and iTerm, length of sentence, etc.Moreover, we included additional groups of featuresthat we felt might be important for this task: onegroup to capture how the relationship is describedtextually, another group to capture how central therelationship is in terms of what is being described inthe sentence, and the last to capture whether the re-lation is stated as a conjecture or a fact.
The weightsfor these features will be determined automaticallyduring the learning process and they will be depen-dent on whether or not the features were effective,given the annotation set.The first type of features is to capture how therelationship is described textually.
As an example,consider the sentence ?Bmp2 stimulates osteoblas-tic differentiation?2, where the gene and the iTermare in subject and object (direct object or otherwise)positions, and the verb is a common biological verb.Thus, we constructed a set of lexico-syntactic pat-terns to capture the different kinds of argument re-lations served by the two concepts.
We grouped 25lexico-syntactic patterns into 8 groups, correspond-ing to different relational constructions that can ex-ist between a gene and an iTerm.
Example patternsare shown in Table 1 for each group, and the sym-bols used in these patterns are explained in Table 2.When a sentence matches a pattern group, the corre-sponding value is set to 1 for that feature.2In our examples, the gene will be marked in italics and theiTerm will be marked in bold.165Group Example PatternG1 G VG+ IG2 G/I via/by/through I/GG3 G VG+ (NP/PP)* by/in VBG IG4 G/I by/in VBG I/GG5 G/I VB I/GG6 G/I of I/GG7 G/I other preposition I/GG8 including/such as/etc.
G/I and I/GTable 1: Examples of lexico-syntactic patternsFor example, the following sentence, in whichthe gene is Lmo2 and the iTerm is ?erythropoiesis?,matches the pattern in G1: [G VG+ I].While Tal1 has been shown to induce ery-throid differentiation , Lmo2 appears to sup-press fetal erythropoiesis.where ?Lmo2?
matches G, ?appears to suppress?matches VG+, and ?fetal erythropoiesis?
matches I.Notice how the verb plays an important role inthe patterns of groups G1, G3, G4, and G5.
We alsohave a verb type feature which differentiates groupsof verbs having the gene and the iTerm as arguments(e.g., ?activates?, ?is involved in?, ?plays a role?,etc.
are treated as different types).The second type of features captures how cen-tral the relationship is in terms of what is being de-scribed in the sentence.
The subject feature recordswhether the gene and iTerm appear in the subjectposition, as this will tell us if they are in focus inthe sentence.
While we do not parse the sentence,we take a simplified sentence (see Section 2.4) andsee if the gene/term appear in a noun phrase pre-ceding the first tensed verb.
Another feature, thegene-iTerm position, measures how close the geneand the term are to each other and to the beginningof the sentence, as this makes it easier for a readerto grasp the relation between them.
For this, we addthe number of words occurring to the left of the seg-ment spanning the gene and iTerm, and half of thenumber of words occurring between them.
Finally,we included a headedness feature.
The idea here isthat if the gene/term are not the head of the noungroup, but rather embedded inside, then this poten-tially makes the relation less straightforward.
TheseSymb DefinitionNP a base noun phrasePP a preposition followed by a base noun phraseVG+ a series of one or more verb groupsVBG a verb group in which the head is a gerund verbVBN a verb group in which the head is a participle verbVB a verb group in which the head is a base verbG, I base noun phrases, with 0 or more prepositionalphrases, containing the gene/iTermTable 2: Symbols used in the pattern notationgroups are denoted by G and I in the patterns shownin Table 1.The third type of features captures informationabout the sentence itself.
The sentence complexityfeature is measured in terms of the number of verbs,conjunctions, commas, and parentheticals that oc-cur in the sentence.
We use a conjecture feature fordetecting whether the sentence involves a hypothe-sis.
We have a simple rule for this feature, by see-ing if words such as ?may?, ?could?, ?probably?,?potentially?, etc., appear in proximity of the geneand iTerm.
Additionally, we have a negation featureto detect whether the relationship is mentioned in anegative way.
We look for words such as ?not?, ?nei-ther?, etc., within proximity of the gene and iTerm.Although the features and lexico-syntactic pat-terns were determined by analyzing a developmentset of sentences containing genes and their iTerms,we believe that these features and patterns can beused to rank sentences involving other biomedicalentities, not just genes.2.4 Sentence SimplificationNotice that the lexico-syntactic patterns are writtenas sequences of chunks and lexical tags.
If a sen-tence matches a pattern, then the sentence expressesa relation between the gene and the iTerm.
However,sometimes it is not possible to match a pattern if thesentence is complex.For example, consider sentence A in Table 3, forgene Cd63.
Let us assume that the iTerm is ?prota-somes?.
Clearly, there is a relationship between thegene and the iTerm, namely that Cd63 was found inpc-3 cell-derived protasomes.
However, none of thelexico-syntactic patterns is able to capture this rela-tion, because of all the extra information between166A Cd63, an integral membrane protein foundin multivesicular lysosomes and secretorygranules, was also found in pc-3 cell-derived protasomes.S1 Cd63 was found in pc-3 cell-derived pro-tasomes.S2 Cd63 is an integral membrane protein.CS1 Cd63 is found in multivesicular lyso-somes.CS2 Cd63 is found in secretory granules.Table 3: Simplified sentences for gene Cd63.
ExampleiTerms: ?protasomes?
and ?secretory granules?.the gene and the term.
While we may have multi-ple patterns in each group, we cannot necessarily ac-count for each lexical variation at this level of gran-ularity.We are using a sentence simplifier, built in-house,to ensure a match where applicable.
The simpli-fier identifies appositions, relative clauses, and con-junctions/lists of different types, using regular ex-pressions to match chunked tags.
In the sentenceof Table 3, the simplifier recognizes the apposition?an integral membrane protein?, the reduced relativeclause ?found in multivesicular bodies/lysosomesand secretory granules?
and the noun conjunction?multivesicular bodies/lysosome and secretory gran-ules?.
It then produces several simplified sentencescontaining the gene.
S1 and S2, shown in Table 3,are simplified sentences obtained from the simpli-fier.
CS1 and CS2 are additional simplified sen-tences, which required the combination of multiplesimplifications: the appositive, the relative clause,and the noun conjunction.Notice how each of the simplified sentencesshown in Table 3 is now matching a pattern group.If we are interested in the relationship between Cd63and ?protasomes?, we can look at S1.
Likewise, ifwe are interested in the relationship between Cd63and ?secretory granules?, we can look at CS2.We have a matching feature that tells whether thepattern was matched in the original sentence, a sim-plified sentence, or a combined sentence, and thisfeature is taken into account in the learning process.3 Results and DiscussionWe evaluated both PrefSVM and RankPref.
Each re-quired a different set of annotated data.
For theevaluation of PrefSVM, we used the preference judg-ments and leave-one-out cross validation.
And forthe evaluation of RankPref, we asked the annota-tors to order a group of sentences mentioning gene-iTerm pairs.
Six life science researchers, with grad-uate degrees, annotated both sets.3.1 Evaluation of PrefSVMFirst, we evaluated the performance of PrefSVM us-ing leave-one-out cross validation.3.1.1 Annotation of Preference JudgementsWe started by selecting a group of pairs of sen-tences.
We randomly picked gene-iTerm combi-nations, and for each combination, we randomlypicked two sentences containing both the gene andthe term.
To alleviate bias, the order of the sentenceswas chosen randomly before displaying them to theannotators.
In our guidelines, we asked the annota-tors to choose sentences that clearly state the rela-tionship between the gene and the iTerm.
Becausethe focus here is on the relationship between the twoterms, we also asked them to refrain from choos-ing sentences that describe additional information orother aspects.
It is conceivable that, for other appli-cations, extra information might be an important de-termining factor, but for our task we wanted to focuson the relationship only.For each pair of sentences, we wanted to havethree opinions so that we can have a majority vote.To alleviate the burden on the annotators, we startedby giving each pair of sentences to two annotators,and asked for an extra opinion only when they didnot agree.
Each biologist was given an initial setof 75 pairs of sentences to annotate, and shared thesame amount of annotations (15) with each of theother biologists.
225 unique pairs of sentences werethus annotated, but six were discarded after the an-notators informed us that they did not contain thegene in question.In 34 out of 219 pairs of sentences, the two biol-ogists disagreed on their annotations.
These casesincluded pairs of similar sentences, or pairs of sen-tences that did not describe any relationship between167System Performance CorrectBaseline 1 65.75% 144Baseline 2 71.69% 157PrefSVM without Simp 72.14% 158PrefSVM with Simp 83.10% 182Table 4: Results for PrefSVMthe gene and the iTerm.
An example of sentences forwhich the annotators could not agree is:1.
The tle proteins are the mammalian ho-mologues of gro, a member of the drosophilanotch signaling pathway.2.
In drosophila, gro is one of the neurogenicgenes that participates in the notch signallingpathway .For these 34 pairs, we randomly selected anotherannotator and considered the majority vote.3.1.2 BaselinesWe chose two baselines against which to com-pare PrefSVM.
The first baseline always choosesthe shortest sentence.
For the second baseline, welooked at the proximity of the gene/term to the be-ginning of the sentence, as well as the proximity ofthe two to each other, and chose the sentence thathad the lowest accumulated proximity.
The reasonfor this second baseline is because the proximity ofthe gene/term to the beginning of the sentence couldmean that the sentence focuses on the gene/term andtheir relation.
Furthermore, the proximity of thegene to the iTerm could mean a clearer relation be-tween them.3.1.3 ResultsWe evaluated PrefSVM by performing leave-one-out cross validation on the set of 219 pairs of sen-tences.
Each pair of sentences was tested by usingthe model trained on the remaining 218 pairs.
Theresults are shown in Table 4.The first baseline performed at 65.75%, correctlychoosing 144 of 219 sentences.
The second base-line performed slightly better, at 71.69%.
PrefSVMoutperformed both baselines, especially when thesentence simplifier was used, as this facilitated thematch of the lexico-syntactic patterns used as fea-tures.
PrefSVM performed at 83.10%, which is17.35% better than the first baseline, and 11.41%better than the second baseline.3.2 Evaluation of RankPrefThe previous evaluation showed how PrefSVM per-forms at picking a sentence from a pair of sentences.But ultimately, for the intended eGIFT application,the system needs to choose one sentence from many.We evaluated RankPref for this task.3.2.1 Annotating Data for Sentence SelectionFor this evaluation, we needed to create a differentset of annotated data that reflects the selection of onesentence from a group of sentences.Since a gene and an iTerm can appear in manysentences, it is too onerous a task for a human anno-tator to choose one out of tens or hundreds of sen-tences.
For this reason, we limited the set of sen-tences mentioning a gene and an iTerm to only 10.We randomly picked 100 gene-term pairs and for thepairs that contained more than ten sentences, we ran-domly chose ten of them.
On average, there were 9.4sentences per set.We asked the same annotators as in the previousevaluation to participate in this annotation task.
Be-cause the task is very time consuming, and becauseit is hard to decide how to combine the results frommultiple annotators, we assigned each set of sen-tences to only one annotator.
We showed the sen-tences in a random order so that biasing them wouldnot be an issue.We initially asked the annotators to order the sen-tences in the set.
However, this task proved to be im-possible, since many sentences were alike.
Instead,we asked the annotators to assign them one of threecategories:(Cat.1) Any sentence in this category could beconsidered the ?best?
among the choices provided;(Cat.2) These sentences are good, but there areother sentences that are slightly better;(Cat.3) These sentences are not good or at leastthere are other sentences in this set that are muchbetter.Classifying the sentences into these categorieswas less cumbersome, fact which was confirmed byour evaluators after a trial annotation.Out of the total of 936 sentences, 322 (34.4%)were placed in the first category, 332 (35.5%) were168System Cat.1 Cat.2 Cat.3Baseline 1 58 30 12Baseline 2 61 24 15RankPref without Simp 67 21 12RankPref with Simp 80 17 3Table 5: Results for RankPrefplaced in the second category, and 282 (30.1%) wereplaced in the third category.
On average, it tookabout 15 minutes for an annotator to group a set?ssentences into these three categories.
So each anno-tator volunteered approximately 5 hours of annota-tion time.3.2.2 ResultsTable 5 shows how the top sentences picked forthe 100 gene-term pairs by the four systems matchedwith the annotations.
80 of 100 sentences thatRankPref picked were placed in Cat.1 by the anno-tators, 17 were placed in Cat.2, and 3 sentenceswere placed in Cat.3.
These results compare favor-ably with results obtained for the two baselines andRankPref without the use of the simplifier.Furthermore, instead of just focussing on the topchoice sentence, we also considered the ranking ofthe entire set of sentences.
We looked at how theranked lists agree with the categories assigned bythe annotators.
We used the normalized discountedcumulative gain (nDCG) (Jarvelin and Kekalainen,2002), a standard metric used in information re-trieval to evaluate the quality of the ranked lists.DCG at rank p is defined as:DCGp = rel1 +p?i=2relilog2iwhere reli is the relevance of the item at position i.We normalize DCG by dividing it by an ideal gain(i.e., DCG of same list, when ordered from highestto lowest relevance).For our task, we took the relevance score to be 1for a sentence placed in Cat.1, a relevance score of0.5 for a sentence placed in Cat.2, and a relevancescore of 0 for a sentence placed in Cat.3.
We reporta normalized discounted cumulative gain of 77.19%.This result compares favorably with results re-ported for the two baselines (68.36% for B1 andFigure 2: Distribution of nDCG for different relevancescores assigned to sentences placed in category Cat.2.68.32% for B2) as well as for when the sentencesimplifier was removed (69.45%).Figure 2 shows different results for nDCG whenthe relevance score for Cat.2 is varied between 0(same as sentences placed in Cat.1) and 1 (same assentences placed in Cat.3).4 Related WorkTo the best of our knowledge, no one has attemptedto rank sentences from the biomedical literature,using machine learning on a set of data markedwith preference judgments.
However, different ap-proaches have been described in the literature thatuse preference judgments to learn ranked lists.
Forexample, Radlinski and Joachims (2005) used pref-erence judgments to learn ranked retrieval functionsfor web search results.
These judgments were gen-erated automatically from search engine logs.
Theirlearned rankings outperformed a static ranking func-tion.
Similar approaches in IR are those of Cohen etal.
(1999) and Freund et al (2003).Ranking of text passages and documents hasbeen done previously in BioNLP for other purposes.Suomela and Andrade (2005) proposed a way torank the entire PubMed database, given a large train-ing set for a specific topic.
Goldberg et al (2008)and Lu et al (2009) describe in detail how they iden-tified and ranked passages for the 2006 Trec Ge-nomics Track (Hersh et al, 2006).
Yeganova etal.
(2011) present a method for ranking positively la-beled data within large sets of data, and this methodwas applied by Neveol et al (2011) to rank sen-tences containing deposition relationships betweenbiological data and public repositories.169Extraction of sentences describing gene functionshas also been applied for creating gene summaries(Ling et al, 2007; Jin et al, 2009; Yang et al, 2009).However, these methods differ in that their goal isnot to look for sentences containing specific termsand their relations with genes, but rather for sen-tences that fall into some predefined categories ofsentences typically observed in gene summaries.Sentence simplification has been used to aid pars-ing (Chandrasekar et al, 1996; Jonnalagadda etal., 2009).
Devlin and Tait (1998) and Carroll etal.
(1998) use it to help people with aphasia.
Sid-dharthan (2004) was concerned with cohesion andsuggested some applications.The idea of using lexico-syntactic patterns toidentify relation candidates has also been applied inthe work of Banko et al (2007), although their pat-terns are not used in the learning process.5 Conclusion and Future DirectionsWe have developed a system which aims to identifysentences that clearly and succinctly describe the re-lation between two entities.
We used a set of prefer-ence judgements, as provided by biologists, to learnan SVM model that could make a choice betweenany two sentences mentioning these entities.The model compares favorably with baselines onboth the task of choosing between two sentences, aswell as ranking a set of sentences.
The performancefor choosing between two sentences was 83.10%, ascompared to 65.75% and 71.69% for the two base-lines, respectively.
For choosing one sentence froma list of sentences, the performance was 80%, ascompared to 58% and 61%.
Furthermore, when theentire list of ranked sentences was evaluated, thesystem reported a nDCG of 77.19%, compared to68.36% and 68.32% for the two baselines.The model?s performance was also shown tobe significantly better when sentence simplificationwas used.
We were able to match relation patternson complex sentences, and observed an increase of10.96%, 13%, and 7.74% for the three evaluationsafore-mentioned, respectively.
It is noteworthy that,without the simplification, the performance is onlyslightly better than the second baseline.
This is be-cause the second baseline uses information that isalso used by our system, although this does not in-clude the lexico-syntactic patterns that identify thetype of relation between the gene and the term.Given that the full system?s performance is muchbetter than both baselines, and that the system?s per-formance without simplification is only slightly bet-ter than the second baseline, we believe that: (1) thepattern and type of relation determination are impor-tant, and (2) sentence simplification is crucial for thedetermination of the relationship type.We are currently pursuing summaries for genes.Since iTerms have been shown in previous evalua-tions to represent important aspects of a gene?s func-tionality and behavior, we are investigating whetherthey are represented in gene summaries found in En-trezGene and UniProtKB.
If so, an extractive sum-mary can be produced by choosing sentences for thegene and its iTerms.
We are also considering de-veloping abstractive summaries.
Our use of lexico-syntactic patterns can be extended to pick the exactrelation between a gene and the iTerm.
For exam-ple, by using the lexico-syntactic patterns, coupledwith simplification, we can extract the following ex-act relations from the four sentences shown in Fig-ure 1: ?Groucho is a corepressor?, ?The wrpw motifrecruits groucho?, ?Groucho is implicated in notchsignaling?, and ?The eh1 repression domain bindsgroucho?.
With these relations extracted, using textgeneration algorithms for textual realization and co-hesion, we can produce abstractive summaries.We would also like to investigate how to general-ize this work to other pairs of entities, as well as howto generalize this work for other applications whichmay or may not require the same features as the oneswe used.AcknowledgmentsThis work has been supported in part by USDAGrant 2008-35205-18734 and the Agriculture andFood Research Initiative Competitive USDA Grant2011-67015-3032.
We thank Cecilia Arighi, KevinBullaughey, Teresia Buza, Fiona McCarthy, Lak-shmi Pillai, Carl Schmidt, Liang Sun, Hui Wang,and Qinghua Wang for participating in the anno-tation task and/or for various discussions.
Wealso thank the anonymous reviewers for their com-ments and suggestions, which helped us improve themanuscript.170ReferencesMichele Banko, Michael J Cafarella, Stephen Soderland,Matt Broadhead, and Oren Etzioni.
2007.
Open In-formation Extraction from the Web.
In Proceedings ofIJCAI.John Carroll, Guido Minnen, Yvonne Canning, SiobhanDevlin, and John Tait.
1998.
Practical simplificationof English newspaper text to assist aphasic readers.Proceedings of the AAAI98 Workshop on IntegratingAI and Assistive Technology, pages 7?10.Ben Carterette, Paul N Bennett, David Maxwell Chicker-ing, and Susan T Dumais.
2008.
Here or there: Pref-erence judgments for relevance.
In Proceedings of theIR research, 30th European conference on Adv.
in IR.R Chandrasekar, Christine Doran, and B Srinivas.
1996.Motivations and methods for text simplification.
InProceedings of the 16th conference on Computationallinguistics, volume 2, pages 1041?1044.
Associationfor Computational Linguistics.Wiliam W Cohen, Robert E Schapire, and Yoram Singer.1999.
Learning to order things.
Journal of ArtificialIntelligence Research, 10:243?270.Siobhan Devlin and John Tait.
1998.
The use of a psy-cholinguistic database in the simplification of text foraphasic readers.
Linguistic Databases, pages 161?173.Yoav Freund, Raj Iyer, Robert E Schapire, and YoramSinger.
2003.
An efficient boosting algorithm forcombining preferences.
Journal of Machine LearningResearch, 4:933?969.Arek Gladki, Pawel Siedlecki, Szymon Kaczanowski,and Piotr Zielenkewicz.
2008. e-LiSe?an online toolfor finding needles in the ?Medline haystack?.
Bioin-formatics, 24(8):1115?1117.Andrew B Goldberg, David Andrzejewski, Jurgen VanGael, Burr Settles, Xiaojin Zhu, and Mark Craven.2008.
Ranking biomedical passages for relevance anddiversity.
In Proceedings of TREC.William Hersh, Aaron M Cohen, Phoebe Roberts, andHari Krishna Rekapalli.
2006.
TREC 2006 GenomicsTrack Overview.Kalervo Jarvelin and Jaana Kekalainen.
2002.
Cumu-lated gain-based evaluation of IR techniques.
ACMTransactions on Information Systems, 20(4):422?446.Feng Jin, Minlie Huang, Zhiyong Lu, and Xiaoyan Zhu.2009.
Towards automatic generation of gene sum-mary.
In Proceedings of the BioNLP 2009 Work-shop, pages 97?105.
Association for ComputationalLinguistics, June.Siddhartha Jonnalagadda, Luis Tari, Jorg Hakenberg,Chitta Baral, and Graciela Gonzalez.
2009.
Towardseffective sentence simplification for automatic pro-cessing of biomedical text.
In Proceedings of NAACLHLT 2009: Short Papers, pages 177?180.Szymon Kaczanowski, Pawel Siedlecki, and Piotr Zie-lenkewicz.
2009.
The high throughput sequenceannotation service (HT-SAS) - the shortcut from se-quence to true medline words.
BMC Bioinformatics,10:148?154, May.Jung-Jae Kim, Piotr Pezik, and Dietrich Rebholz-Schuhmann.
2008.
MedEvi: Retrieving textual evi-dence of relations between biomedical concepts frommedline.
Bioinformatics, 24(11):1410?1412.Xu Ling, Jing Jiang, Xin He, Qiaozhu Mei, Chengxi-ang Zhai, and Bruce Schatz.
2007.
Generating genesummaries from biomedical literature: A study ofsemi-structured summarization.
Information Process-ing and Management, 43:1777?1791, March.Yue Lu, Hui Fang, and Chengxiang Zhai.
2009.
Anempirical study of gene synonym query expansionin biomedical information retrieval.
Information Re-trieval, 12:51?68, February.Aure?lie Ne?ve?ol, W John Wilbur, and Zhiyong Lu.
2011.Extraction of data deposition statements from the lit-erature: a method for automatically tracking researchresults.
Bioinformatics, 27(23):3306?3312.Filip Radlinski and Thorsten Joachims.
2005.
Querychains: Learning to rank from implicit feedback.
InProceedings of KDD?05.Advaith Siddharthan.
2004.
Syntactic Simplification andText Cohesion.
Ph.D. thesis, University of Cambridge.Neil R Smalheiser, Wei Zhou, and Vetle I Torvik.
2008.Anne O?Tate: A tool to support user-driven summa-rization, drill-down and browsing of PubMed searchresults.
Journal of Biomedical Discovery and Collab-oration, 3(1):2?11.Brian P Suomela and Miguel A Andrade.
2005.
Rank-ing the whole MEDLINE database according to a largetraining set using text indexing.
BMC Bioinformatics,6(75), March.Yoshimasa Tsuruoka, Yuka Tateishi, Jing-Dong Kim,Tomoko Ohta, John McNaught, Sophia Ananiadou,and Jun?ichi Tsujii.
2005.
Developing a robust part-of-speech tagger for biomedical text.
In Advances inInformatics ?
10th Panhellenic Conference on Infor-matics, LNCS 3746, pages 382?392.Catalina O Tudor, Carl J Schmidt, and K Vijay-Shanker.2010.
eGIFT: Mining Gene Information from the Lit-erature.
BMC Bioinformatics, 11:418.Jianji Yang, Aaron Cohen, and William Hersh.
2009.Evaluation of a gene information summarization sys-tem by users during the analysis process of microarraydatasets.
BMC Bioinformatics, 10(Suppl 2):S5.Lana Yeganova, Donald C Comeau, Won Kim, andW John Wilbur.
2011.
Text Mining Techniques forLeveraging Positively Labeled Data.
In Proceedingsof ACL Workshop BioNLP, pages 155?163.171
