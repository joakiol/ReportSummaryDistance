Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 49?56, Vancouver, October 2005. c?2005 Association for Computational LinguisticsSemantic Similarity for Detecting Recognition Errors in AutomaticSpeech TranscriptsDiana Inkpen Alain D?siletsSchool of Information Technology and Engineering Institute for Information TechnologyUniversity of Ottawa National Research Council of CanadaOttawa, ON, K1N 6H5, Canada Ottawa, ON, K1AOR6, Canadadiana@site.uottawa.ca alain.desilets@nrc-cnrc.gc.caAbstractBrowsing through large volumes of spokenaudio is known to be a challenging task forend users.
One way to alleviate this prob-lem is to allow users to gist a spoken audiodocument by glancing over a transcriptgenerated through Automatic Speech Rec-ognition.
Unfortunately, such transcriptstypically contain many recognition errorswhich are highly distracting and make gist-ing more difficult.
In this paper we presentan approach that detects recognition errorsby identifying words which are semanticoutliers with respect to other words in thetranscript.
We describe several variants ofthis approach.
We investigate a wide rangeof evaluation measures and we show thatwe can significantly reduce the number oferrors in content words, with the trade-offof losing some good content words.1 IntroductionSpoken audio documents are becoming more andmore common place due to the rising popularity oftechnologies such as: video and audio conferenc-ing, video web-casting and digital cameras for theconsumer market.
Unfortunately, speech docu-ments are inherently hard to browse because oftheir transient nature.
For example, imagine tryingto locate the audio segment in the recording of a60-minute meeting, where John talked about pro-ject X.
Typically, this would require fast forward-ing through the audio by some amount, thenlistening and trying to remember if the current seg-ment was spoken before or after the desired seg-ment, then fast-forwarding or backtracking by asmall amount, and so on.One way to make audio browsing of audio docu-ments more efficient is to allow the user to navi-gate through a textual transcript that is cross-referenced with corresponding time points into theoriginal audio (Nakatani et al 1998; Hirschberg etal.
1999).
Such transcripts can easily be producedwith Automatic Speech Recognition (ASR) sys-tems today.
Unfortunately, such transcripts typi-cally contain recognition errors that make themhard to browse and understand.
Although WordError Rates (WER) of the order of 20% can beachieved for broadcast quality audio, the WER formore common situations (ex: less-than-broadcastquality recordings of meetings) is typically in theorder of 50% or more.The work we present in this paper aims at auto-matically identifying recognition errors and remov-ing them from the transcript, in order to makegisting and browsing of the corresponding audiomore efficient.
For example, consider the follow-ing portion of a transcript that was produced withthe Dragon NaturallySpeaking speech recognitionsystem from the audio of a meeting:?Weenie to decide quickly whether local for largeexpensive plasma screen aura for a bunch ofsmaller and cheaper ones and Holland together?Now consider the following filtered transcriptwhere recognition errors were automatically blot-ted out using our proposed algorithm:?
... to decide quickly whether ... large expensiveplasma screen ... for a bunch of smaller andcheaper ones and ... together?We believe that transcripts like this second onemay be more efficient for gisting and browsing the49content of the original audio whose correct tran-script is:?We need to decide quickly whether we will go fora large expensive plasma screen or for a bunch ofsmaller and cheaper ones and tile them together.
?Our approach to filtering recognition errors is toidentify semantic outliers.
By this, we meanwords that do not cohere well semantically withother words in the transcript.
More often than not,such outliers turn out to be mistranscribed words.We present several variants of an algorithm foridentifying semantic outliers, and evaluate them interms of how well they are able to filter out recog-nition errors.2 Related WorkHirschberg et al (1999), and Nakatani et al (1998)proposed the idea of using automatic transcripts forgisting and navigating audio documents.
Text-based summarization techniques on automaticspeech transcription have also been used.
For ex-ample, the method of D?silets et al (2001) wasfound to produce accurate keyphrases for transcrip-tions with Word Error Rates (WER) in the order of25%, but performance was less than ideal for tran-scripts with WER in the order of 60%.
With suchtranscripts, a large proportion of the extracted key-phrases included serious transcription errors.
Ink-pen and D?silets (2004) presented an experimentthat filters out errors in keywords extracted fromspeech, by identifying the keywords that are notsemantically close to the rest of the keywords.Semantic similarity measures were used formany tasks.
Two examples are: real-word errorcorrection (Budanitsky and Hirst, 2000) and an-swering synonym questions (Turney, 2001),(Jarmasz and Szpakowicz, 2003).There is a lot of research on confidence meas-ures for identifying errors in speech recognitionoutput.
Most papers on this topic use informationthat is internal to the ASR system, generated by thedecoder during the recognition process.
Examplesare likelihood ratios derived by a Viterbi decoder(Gillick et al, 1997), measures of competingwords at a word boundary (Cox and Rose, 1996),word score densities in N-best lists, and variousacoustic and phonetic features.
Machine learningtechniques were used to identify the best combina-tions of features for classification (Chase, 1997)(Schaaf and Kemp, 1997) (Ma et al, 2001)(Skantze and Edlund, 2004) (Zhou and Meng,2004) (Zhou et al, 2005).
Some of these methodsachieve good performance, although they use dif-ferent test sets and report different evaluationmeasures from the set we enumerate in Section 6.In our work, we use information that is externalto the ASR system, because new knowledge seemslikely to help in the detection of semantic outliers.In this respect, the work of Cox and Dasmahapatra(2000) is closest to ours.
They compared the accu-racy of a measure based on Latent SemanticAnalysis (LSA) (Landauer and Dumais, 1997) toan ASR-based confidence measure, and found thatthe ASR-based measure (using N-best lists) outper-formed the LSA approach.
While the N-best listsapproach was better at the high-Recall end of thespectrum, the LSA was better at the high-Precisionend.
They also showed that a hybrid combinationof the two approaches worked best.
Our work issimilar to the LSA-based part of Cox and Dasma-hapatra, except that we use Point-wise MutualInformation (PMI) instead of LSA.
Because PMIscales up to very large corpora, it has been shownto work better than LSA for assessing the semanticsimilarity of words (Turney, 2001).
Another dis-tinguishing feature is that Cox and Dasmahapatraonly looked at transcripts with moderate WER,whereas we additionally evaluate the technique forthe purpose of doing error filtering on transcriptswith high WER, which are more typical of non-broadcast conversational audio.3 The DataWe evaluated our algorithms on a randomly se-lected subset of 100 stories from the TDT2 EnglishAudio corpus.
We conducted experiments with twotypes of automatically-generated speech tran-scripts.
The first ones were generated by theNIST/BBN time-adaptive speech recognizer andhave a moderate WER (27.6%), which is represen-tative of what can be obtained with a speaker-independent ASR system tuned for the BroadcastNews domain.
In the rest of this paper, we refer tothese moderate accuracy transcripts as the BBNdataset.
The second set of transcripts was obtainedusing the Dragon NaturallySpeaking speaker-dependent recognizer.
Their WER (62.3%) wasmuch higher because the voice model was nottrained for speaker-independent broadcast qualityaudio.
These transcripts approximate the type of50high WER seen in more casual less-than-broadcastquality audio.
We refer to these transcripts as theDragon dataset.4 The methodOur algorithm tries to detect recognition errors byidentifying and filtering semantic outliers in thetranscripts.
In other words, it declares as recogni-tion errors all the words with low semantic similar-ity to other words in the transcript.
The algorithmfocuses on content words, i.e., words that do notappear in a list of 779 stopwords (including closed-class words, such as prepositions, articles, etc.
).The reason to ignore stopwords is that they tend toco-occur with most words, and are therefore se-mantically coherent with most words.
The basicalgorithm for determining if a word w is a recogni-tion error is as follows.1.
Compute the neighborhood N(w) of w as theset of content words that occur before and after win a context window (including w itself).2.
Compute pair-wise semantic similarity scoresS(wi, wj) between all pairs of words wi ?
wj (in-cluding w) in the neighborhood N(w), using a se-mantic similarity measure.
Scale up those S(wi, wj)by a constant so that they are all non-negative, andthe smallest one is 0.3.
For each wi in the neighborhood N(w) (includ-ing w), compute its semantic coherence SC(wi).by ?aggregating?
the pair-wise semantic similari-ties S(wi, wj) of wi with all its neighbors (wi ?
wj)into a single number.4.
Let SCavg be the average of SC(wi) over all wi inthe neighborhood N(w).5.
Label w as a recognition error if SC(w) <K?SCavg, where K is a parameter that allows us tocontrol the amount of error filtering (K% of theaverage semantic coherence score).
Low values ofK mean little error filtering and high values of Kmean a lot of error filtering.We tested a number of variants of Steps 1-3.
ForStep 1, we experimented with two ways of com-puting the neighborhood N(w).
The first approachwas to set N(w) to be all the words in the transcript(the All variant).
The second neighborhood ap-proach was to set N(w) to be the set of 10 contentwords before and after w in the transcript (theWindow variant).For Step 2 we experimented with two differentmeasures for evaluating the pair-wise semanticsimilarities S(wi, wj).
The first measure used ahand-crafted dictionary (the Roget variant)whereas the second one used a statistical measurebased on a large corpus (the PMI variant).For Step 3 we experimented with differentschemes for ?aggregating?
the pair-wise semanticsimilarities S(wi, wj) into a single semantic coher-ence number SC(wi) for a given word wi.
The firstaggregation scheme was simply to average theSC(wi) values (the AVG variant).
Note that withthis scheme, we filter words that do not coherewell with all the words in the neighborhood N(w).This might be too aggressive in the case of the Allvariant, especially for longer or multi-topic audiodocuments.
Therefore, we investigated other ag-gregation schemes that only required words to co-here well with a subset of the words in N(w).
Thesecond aggregation scheme was to set SC(wi) tothe value of the most similar neighbor in N(w) (theMAX variant).
The third aggregation scheme wasto set SC(wi) to the average of the 3 most similarneighbors in N(w) (the 3MAX variant).Thus, there are altogether 2x2x3 = 12 possibleconfigurations of the algorithm.
In the rest of thispaper, we will refer to specific configurations us-ing the following naming scheme: Step1Variant-Step2Variant-Step3Variant.
For example, All-PMI-AVG means the configuration that uses theAll variant of Step 1, the PMI variant of Step 2,and the AVG variant of step 3.It is worth noting that all configurations of thisalgorithm are computationally intensive, mainlybecause of Step 2.
However, since our aim is toprovide transcripts for browsing audio recordings,we do not have to correct errors in real time.5 Choosing a semantic similarity measureSemantic similarity refers to the degree with whichtwo words (two concepts) are related.
For example,most human judges would agree that paper andpencil are more closely related than car andtoothbrush.
We use the term semantic similarity inthis paper in a more general sense of semantic re-latedness (two concepts can be related by theircontext of use without necessarily being similar).51There are three types of semantic similaritymeasures: dictionary-based (lexical taxonomystructure), corpus-based, and hybrid.
Most of thedictionary-based measures use path length inWordNet ?
for example (Leacock and Chodorow,1998), (Hirst and St-Onge, 1998).
The corpus-based measures use some form of vector similarity.The cosine measure uses frequency counts in itsvectors and cosine to compute similarity; the sim-pler methods use binary vectors and compute coef-ficients such as: Matching, Dice, Jaccard, andOverlap.
Examples of hybrid measures, based onWordNet and small corpora, are: Resnik (1995),Jiang and Conrath (1997), Lin (1998).
All diction-ary-based measures have the disadvantage of lim-ited coverage: they cannot deal with many propernames and new words that are not in the diction-ary.
For WordNet-based approaches, there is theadditional issue that they tend to work well onlyfor nouns because the noun hierarchy in WordNetis the most developed.
Also, most of the WordNet-based measures do not work for words with differ-ent part-of-speech, with small exceptions such asthe extended Lesk measure (Banerjee and Peder-sen, 2003).We did a pre-screening of the various semanticsimilarity measures in order to choose the onemeasure of each type (dictionary-based and cor-pus-based) that seemed most promising for ourtask of detecting semantic outliers in automaticspeech transcripts.
The dictionary-based ap-proaches that we evaluated were: the WordNet-based measure by Leacock and Chodorow (1987),and one other dictionary-based measure that usesthe Roget thesaurus.
The Roget measure (Jarmaszand Szpakowicz, 2003) has the advantage that itworks across part-of-speech.
The corpus-basedmeasures we evaluated were: (a) the cosine meas-ure based on word co-occurrence vectors (Lesk,1969), (b) a new method that computes the Pearsoncorrelation coefficient of the co-occurrence vectorsinstead of the cosine, and (c) a measure based onpoint-wise mutual information.
We computed thefirst two measures on the 100-million-words Brit-ish National Corpus (BNC)1, and the third one on amuch larger-corpus of Web data (one terabyte)accessed through the Waterloo Multitext system(Clarke and Terra, 2003).
The reason for usingcorpora of different sizes is that PMI is the only1 http://www.natcorp.ox.ac.uk/index.htmlone of the three corpus-based approaches thatscales up to a terabyte corpus.We describe here in detail the PMI corpus-basedmeasure, because it is the most important for thispaper.
The semantic similarity score between twowords w1 and w2 is defined as the probability ofseeing the two words together divided by the prob-ability of each word separately: PMI(w1,w2) = log[P(w1,w2) / (P(w1)?P(w2))] =  log [C(w1,w2)?N /(C(w1)?C(w2))], where C(w1,w2), C(w1), C(w2) arefrequency counts, and N is the total number ofwords in the corpus.
Such counts can easily andefficiently be retrieved for a terabyte corpus usingthe Waterloo Multitext system.In order to assess how well the semantic similar-ity measures correlate with human perception, weuse the set of 30 word pairs of Miller and Charles(1991), and the 65 pairs of Rubenstein and Goode-nough (1965).
Both used humans to judge the simi-larity.
The Miller and Charles pairs were a subsetof the Rubenstein and Goodenough pairs.
Note thatboth of those sets were limited to nouns that ap-peared in the Roget thesaurus, and they are there-fore favorably biased towards dictionary-basedapproaches.
Table 1 shows the correlation of 5similarity measures for the Rubenstein and Goode-nough (R&G) and Miller and Charles (M&C) data-set.
Note that although there are many WordNet-based semantic similarity measures, we only showcorrelations for Leacock and Chodorow (L&C)because it was previously shown to be better corre-lated (Jarmasz and Szpakowicz, 2003).
We do notshow figures for hybrid measures either becausethe same study showed L&C to be better.Table 1: Correlation between human assigned and variousmachine assigned semantic similarity scores.Dictionary-based Corpus-basedL&C Roget Cos. Corr.
PMIM&C 0.821 0.878 0.406 0.438 0.759R&G 0.852 0.818 0.472 0.517 0.746We see that the WordNet-based L&C measurebased (Leacock and Chodorow, 1998 and the Ro-get measure (Jarmasz and Szpakowicz, 2003) bothachieve high correlations but the two vector cor-pus-based measures (Cosine and Pearson Correla-tion) achieve much lower correlation.
The onlycorpus-based measure that does well is PMI,probably because of the much larger corpus.52We decided to experiment with two of the meas-ures (one corpus-based and one thesaurus based)for computing the semantic similarity of wordpairs in Step 2 of the algorithm described in Sec-tion 3.
The two measures are: PMI computed onthe Waterloo terabyte corpus and the Roget-basedmeasure.
These two seem the most promisinggiven the nature of our task and the correlation fig-ures reported above.6 Evaluation MeasuresWe use several evaluation measures to determinehow well our algorithm works for identifying se-mantic outliers.
As summarized in Table 2, the taskof detecting recognition errors can be viewed as aclassification task.
For each word, the algorithmmust predict whether or not that word was tran-scribed correctly.Table 2: Recognition error detection can be seen as a classifi-cation task.Correctlytranscribed(actual)NOT Correctlytranscribed(actual)Correctlytranscribed(predicted)True Positive(TP)False Positive(FP)NOT Correctlytranscribed(predicted)False Negative(FN)True Negative(TN)Note that we decide if a word is actually cor-rectly transcribed or not by using the alignment ofan automatic transcript with the manual transcript.A standard evaluation tool (sclite2) computes WERby counting the number of substitutions, deletions,and insertions needed to align a reference tran-script with a hypothesis file.
It also marks thewords that are correct in automatic transcript (thehypothesis file).
The rest of the words are the ac-tual recognition errors (the insertions or substitu-tions).
The deletions ?
words that are absent fromthe automatic transcript ?
cannot be tagged by theconfidence measure.We define the following performance measuresin order to evaluate the improvement of the filteredtranscripts compared to the initial transcripts:1.
Word error rate in the initial transcript and inthe filtered transcript.
These measures can be com-puted with and without stopwords (for which our2 http://www.nist.gov/speech/tools/algorithm does not apply).
Note that WER withoutstopwords could be slightly lower than traditionalWER mostly because content words tend to be rec-ognized more accurately than stopwords (D?siletset al 2001).
When filtering out semantic outliers,there will be gaps in the filtered transcript, there-fore the general WER might not improve becauseit penalizes heavily the deletions.2.
Content word error rate (cWER).
This is theerror rate in an automatic transcript (initial or fil-tered) from the point of view of the confidencemeasure, for the content words only.
It penalizesthe words in the automatic transcripts that shouldnot be there, but not any missing words (no dele-tions are penalized).
In the case of a transcript fil-tered by our algorithm, it excludes not only thestopwords, but also the filtered words.
We com-puted cWER with sclite without penalizing for thegaps created by the filtered words.3.
The percentage of lost good content words(%Lost).
This is the percentage of correctly rec-ognized content words which are lost in the proc-ess of filtering out recognition errors, defined as:%Lost = 100 * FN / (TP + FN).
We could alsocompute the percent of discarded words, withoutregard if they should have been filtered out or not.D = (TN + FN) / (TP + FP + TN + FN).4.
Precision (P), Recall (R) and F-measure.
Pre-cision is the proportion of truly correct words con-tained in the list of content words which thealgorithm labeled as correct.
Recall is the propor-tion of truly correct content words that the algo-rithm was able to retain.
F-measure is thegeometric mean of P and R and expresses a trade-off between those two measures.
P = TP / (TP +FP); R = TP / (TP + FN); F = 2PR / (P+R).7 ResultsWe ran various configurations of the algorithmdescribed in Section 4 on the 100 story samplefrom the TDT2 corpus.
This section discusses theresults of those experiments.
We studied the Preci-sion-Recall (P-R) curves for various configurationsof our algorithm over the 100 stories, for the twotypes of transcripts: the BBN and Dragon datasets.Figures 1 and 2 show an example for each dataset.Each point on a P-R curve shows the Precision andRecall for one value of K in {0, 20, 40, 60, 80,53100, 120, 140, 160, 180, 200}.
Points on the leftcorrespond to aggressive filtering (high values ofK), whereas points on the right correspond to leni-ent filtering (low values of K).First, we looked at the relative merits of the twosemantic similarity measures (PMI and Roget) forStep 2.
Figures 1 and 2 plot the P-R curves for theAll-PMI-AVG and All-Roget-AVG configurations.The graphs clearly indicate that PMI performs bet-ter, especially for the high WER Dragon dataset.So PMI was used in the rest of the experiments.Next, we looked at the variants for setting up theneighborhood N(w) in Step 1 (All vs.
Window).The three P-R curves for All-PMI-X and Window-PMI-X for all aggregation approaches X in {AVG,MAX, 3MAX} are not shown here because theywere similar to the P-PMI curves from Figures 1and 2, for the BBN dataset and for the Dragondataset, respectively.
The Window variant wasmarginally better for X=MAX on both datasets, aswell as for X=3MAX on the BBN dataset.
In allother cases, the Window and All variants per-formed approximately the same.Next, we looked at the different schemes for ag-gregating the pair-wise similarity scores in Step 3(AVG, MAX, 3MAX).
By plotting the P-R curvesfor All-PMI-AVG, All-PMI-MAX, and All-PMI-3MAX for both datasets we obtained again curvessimilar to the P-PMI curves from Figures 1 and 2.It seemed that AVG performs slightly better forhigh Recall, the difference being more markedwhen there is no windowing or when we are work-ing on the Dragon dataset.
The 3MAX and MAXvariants seemed to be slightly better at high Preci-sion with acceptable Recall values, with 3MAXbeing always equal or very slightly better thanMAX.
In an audio gisting and browsing contextPrecision is more important than Recall, thereforewe can choose 3MAX.Having established Window-PMI-3MAX as oneof the better configurations, we now look moreclosely at its performance.Figures 3 and 4 show how the content word er-ror rate (cWER), the percentage of lost good words(%Lost), and the F-measure vary as we apply moreand more aggressive error filtering (by increasingK) to both datasets.
We see that our semantic out-lier filtering approach is able to significantly re-duce the number of transcription errors, whilelosing some correct words.
For example, with the0.40.50.60.70.80.910 0.2 0.4 0.6 0.8 1RecallPrecisionP-PMIP-RogetFig 1: P-R curves of PMI vs. Roget (with All and AVG) onthe BBN dataset.
Each P-R point corresponds to a differentvalue of the threshold K (high Recall for low values of K, highPrecision for high values of K).0.40.50.60.70.80.910 0.2 0.4 0.6 0.8 1RecallPrecisionP-PMIP-RogetFig 2: P-R curves of PMI vs. Roget (with All and AVG) onthe Dragon dataset0204060801000 20 40 60 80 100 120 140 160 180 200K (threshold)cWER/%lost/FcWER-BBN%Lost-BBNF-measureFig.3.
Content Words Error Rate (cWER), %Lost good key-words (%Lost) and F-measure as a function of the filteringlevel K for the Window-PMI-3MAXconfiguration on the BBNdataset.0204060801000 20 40 60 80 100 120 140 160 180 200K (threshold)cWER/%lost/FcWER-Dragon%Lost-DragonF-measureFig.4.
Content Words Error Rate (cWER), %Lost good key-words (%Lost) and F-measure as a function of the filteringlevel K for the Window-PMI-3MAX configuration on theDragon dataset.54moderately accurate BBN dataset, we can reducecWER by 50%, while losing 45% of the good con-tent words (K=100).
For the low accuracy Dragondataset, we can reduce cWER by 50%, while los-ing 50% of the good content words (K=120).
Wecan choose lower thresholds, for smaller reductionin cWER but smaller percent of lost good contentwords.
Even small reductions in cWER are impor-tant, especially for less-than-broadcast conditionswhere WER is initially very high.In general, we were not able to show an im-provement in WER computed in a standard way(item 1 in Section 6), because of the high penaltydue to deletions for both filtered semantic outliersand lost good content words.
The percent of lostgood words is admittedly too high, but this seemsto be the case for speech error confidence measures(which do not remove the words tagged as incor-rect).
Also, for the purpose of audio browsing andgisting, we believe that fewer errors even with lossof content are preferable for intelligibility.Comparing our results to those reported by Coxand Dasmahapatra (2000) our PMI-based measureseems to performs better than their LSA-basedmeasure, judging by the shape of the Precision-Recall curves.
(For example, at Precision=90%,they obtained Recall=12%, whereas we obtain20%.
At Precision=80%, they obtain Recall=50%,whereas we get Recall=100%.)
Note however thattheir results and ours are not completely compara-ble since the experiments used different audio cor-pora (WSJCAM0 vs. TDT2), but those twocorpora seem to exhibit similar initial WERs (theWER appears to be around 30% for WSJCAM0;the WER is 27.6% for our BBN dataset).
Also, it isworth noting the LSA measure was computedbased on a corpus that was very similar to the au-dio corpus used to evaluate the performance of themeasure (both were Wall Street Journal corpora).If one was to evaluate this measure on audio froma completely different domain (ex: news in the sci-entific or technical domain), one would expect theperformance to drop significantly.
In contrast, ourPMI measure was computed based on a generalsample of the World Wide Web, which was nottailored to the audio corpus used to evaluate itsperformance.
Therefore, our numbers are probablymore representative of what would be experiencedwith audio corpora outside of the Wall Street Jour-nal domain.8 Conclusion and Future WorkWe presented a basic method for filtering recogni-tion errors of content words from automatic speechtranscripts, by identifying semantic outliers.
Wedescribed and evaluated several variants of the ba-sic algorithm.In future work, we plan to run our experimentson other datasets when they become available tous.
In particular, we want to experiment withmulti-topic audio documents where we expectmore marked advantages for windowing and alter-native aggregation schemes like MAX and 3MAX.We plan to explore ways to scale up other corpus-based semantic similarity measures to large tera-byte corpora.
We plan to explore more approachesto detecting semantic outliers, for example cluster-ing or lexical chains (Hirst and St-Onge, 1997).The most promising direction is to combine ourmethod with confidence measures that use internalinformation from the ASR system (although theinternal information is hard to obtain when usingan ASR as a black box, and it could be recognizer-specific).
A combination is likely to improve theperformance, with the PMI-based measure contrib-uting at the high-Precision end and the internalASR measure contributing to the high-Recall endof the spectrum.
To increase Recall we can alsoidentify named entities and not filter them out.Some named entities could have high semanticsimilarity with the text if they are frequently men-tioned in the same contexts in the Web corpus, butsome names could be common to many contexts.Another future direction will be to actually cor-rect the errors instead of just filtering them out.
Forexample, we might look at the top N speech recog-nizer hypotheses (for a fairly large N like 1000)and choose the one that maximizes semantic cohe-sion.
A final direction for research is to conductexperiments with human subjects, to evaluate thedegree to which filtered transcripts are better thanunfiltered ones for tasks like browsing, gisting andsearching audio clips.AcknowledgmentsWe thank the following people: Peter Turney and his col-leagues for useful feedback; Gerald Penn for feedback onearlier versions of this paper; Egidio Terra and Charlie Clarkefor giving us permission to use the Multitext System, the NRCcopy; Mario Jarmasz and Stan Szpakowicz for sharing theircode for the Roget similarity measure; Aminul Islam for the55correlation figures and the correlative measure.
Our research issupported by the Natural Sciences and Engineering ResearchCouncil of Canada, University of Ottawa, IBM Toronto Cen-tre for Advanced Studies, and the National Research Council.ReferencesAlexander Budanitsky and Graeme Hirst.
2001.
Semanticdistance in WordNet: An experimental, application-oriented evaluation of five measures.
Workshop on Word-Net and Other Lexical Resources, NAACL 2001, Pitts-burgh, PA, USA, 29-34.Satanjeev Banerjee, and Ted Pedersen.
2003.
Gloss overlapsas a measure of semantic relatedness.
In Proceedings of theEighteenth International Joint Conference on Artificial In-telligence (IJCAI?03), Acapulco, Mexico.Charlie Clarke and Egidio Terra.
2003.
Passage retrieval vs.document retrieval for factoid question answering.
ACMSIGIR?03, 327-328.Stephen Cox and Srinandan Dasmahapatra.
2000.
A Semanti-cally-Based Confidence Measure for Speech Recognition,Int.
Conf.
on Spoken Language Processing, Beijing, China,vol.
4, 206-209.Stephen Cox and R.C.
Rose.
1996.
Confidence Measures forthe SWITCHBOARD Database.
IEEE Conf.
on Acoustics,Speech, and Signal Processing, 511-515.Lin Chase.
1997.
Word and Acoustic Confidence Annotationfor Large Vocabulary Speech Recognition, Proceedings ofEurospeech'97, Rhodes, Greece, 815-818.Alain D?silets, Berry de Brujin, and Joel Martin.
2001.
Ex-tracting keyphrases from spoken audio documents.SIGIR?01 Workshop on Information Retrieval Techniquesfor Speech Applications, 36-50.Diana Inkpen and Alain D?silets.
2004.
Extracting semanti-cally-coherent keyphrases from speech.
Canadian Acous-tics, 32(3):130-131.L.Gillick, Y.Ito, and J.Young.
1997.
A Probabilistic Approachto Confidence Estimation and Evaluation.
IEEE Conf.
onAcoustics, Speech, and Signal Processing, 266-277.Julia Hirschberg, Steve Whittaker, Donald Hindle, FernandoPereira, Amit Singhal.
1999.
Finding information in audio:a new paradigm for audio browsing and retrieval.
Proceed-ings of the ESCA ETRW Workshop, 26-33.Graeme Hirst and David St-Onge.
1998.
Lexical chains asrepresentations of context for the detection and correctionof malapropisms.
In: C. Fellbaum (editor), WordNet: Anelectronic lexical database and some of its applications,The MIT Press, Cambridge, MA, 305-332.Mario Jarmasz and Stan Szpakowicz.
2003.
Roget's thesaurusand semantic similarity, Proceedings of the InternationalConference RANLP-2003 (Recent Advances in NaturalLanguage Processing), Borovets, Bulgaria, 212-219.Jay J. Jiang and David W. Conrath.
1997.
Semantic similaritybased on corpus statistics and lexical taxonomy.
In Pro-ceedings of the International Conference on Research inComputational Linguistics (ROCLING X), Taiwan.Thomas Landauer and Susan Dumais.
1997.
A solution toPlato?s problem: representation of knowledge.
Psychologi-cal Review 104: 211-240.Claudia Leacock and Martin Chodorow.
1998.
Combininglocal context and WordNet similarity for word sense identi-fication.
In C. Felbaum (editor), WordNet: An ElectronicLexical Database, MIT Press, Cambridge, MA, 264-283.M.E.
Lesk.
1969.
Word-word associations in document re-trieval systems.
American Documentation 20(1): 27-38.Dekang Lin.
1998.
An information-theoretic definition ofsimilarity.
In Proceedings of the 15th International Confer-ence of Machine Learning.Changxue Ma, Mark A. Randolph, and Joe Drish.
2001.
Asupport vector machines-based rejection technique forspeech recognition.
Proceedings of ICASSP'01, Salt LakeCity, USA, vol.
1, 381-384.Lidia Mangu and M. Padmanabhan.
2001.
Error correctivemechanisms for speech recognition.
Proceedings ofICASSP'01, Salt Lake City, USA, vol.
1, 29-32.George A. Miller and W.G.
Charles.
1991.
Contextual corre-lates of semantic similarity, Language and Cognitive Proc-esses, 6(1):1-28.Christine Nakatani, Steve Whittaker, Julia Hirshberg.
1998.Now you hear it, now you don?t: Empirical Studies of Au-dio Browsing Behavior.
Proceedings of the Fifth Interna-tional Conference on Spoken Language Processing,(SLP?98), Sydney, Australia.Philip Resnik.
1995.
Using information content to evaluatesemantic similarity.
In Proceedings of the 14th Joint Inter-national Conference of Artificial Intelligence, Montreal,Canada, 448-453.Herbert Rubenstein and John B. Goodenough.
1965.
Contex-tual correlates of synonymy.
Communications of ACM,8(10): 627-633.Thomas Schaaf and Thomas Kemp.
1997.
Confidence meas-ures for spontaneous speech recognition, in Proceedings ofICASSP?97, Munich, Germany, vol.
II, 875-878.Gabriel Skantze and J. Edlund.
2004.
Error detection on wordlevel.
In Proceedings of Robust 2004, Norwich.Peter D. Turney.
2001.
Mining the Web for synonyms: PMI-IR versus LSA on TOEFL, Proceedings of the TwelfthEuropean Conference on Machine Learning (ECML-2001),Freiburg, Germany, 491-502.Lina Zhou, Jinjuan Feng, Andrew Sears, Yongmei Shi.
2005.Applying the Na?ve Bayes Classifier to Assist Users in De-tecting Speech Recognition Errors.
Procs.
of the 38th An-nual Hawaii International Conference on System Sciences).Z.Y.
Zhou and Helen M. Meng, 2004.
A Two-Level Schemafor Detecting Recognition Errors, Proceedings of the 8thInternational Conference on Spoken Language Processing(ICSLP), Korea.56
