Xiao Qin, Liang Zong, Yuqian Wu, Xiaojun Wan and Jianwu YangInstitute of Computer Science and TechnologyPeking University, China, 100871{qinxiao,zongliang,wuyuqian,wanxiaojun,yangjianwu}@cist.pku.edu.cnAbstractThis paper describes our experiments onthe cross-domain Chinese word segmen-tation task at the first CIPS-SIGHANJoint Conference on Chinese LanguageProcessing.
Our system is based on theConditional Random Fields (CRFs)model.
Considering the particular prop-erties of the out-of-domain data, we pro-pose some novel steps to get some im-provements for the special task.1 IntroductionChinese word segmentation is one of the mostimportant tasks in the field of Chinese informa-tion processing and it is meaningful to intelligentinformation processing technologies.
After a lotof researches, Chinese word segmentation hasachieved a high accuracy.
Many methods havebeen presented, among which the CRFs modelhas attracted more and more attention.
Zhao?sgroup used the CRFs model in the task of Chi-nese word segmentation in Bakeoff-4 and theyranked at the top in all closed tests of word seg-mentation (Zhao and Kit, 2008).
The CRFsmodel has been widely used because of its excel-lent performance.
However, finding a bettersegmentation algorithm for the out-of-domaintext is the focus of CIP-SIGHAN-2010 bakeoff.We still consider word segmentation as a se-quence labeling problem.
What we concern ishow to use the unlabeled corpora to enrich thesupervised CRFs learning.
So we take somestrategies to make use of the information of thetexts in the unlabeled corpora.2 System DescriptionIn this section, we will describe our system indetails.
The system is based on the CRFs modeland we propose some novel steps for some im-provements.
It mainly consists of three steps:preprocessing, CRF-based labeling, and re-labeling.2.1 PreprocessingThis step mainly includes two operations.
First,we should cut the whole text into a series of sen-tences.
We regard ??
?, ??
?, ???
and ???
as thesymbols of the boundary between sentences.Then we do atomic segmentation to all the sen-tences.
Here Atomic segmentation representsthat we should regard the continuous non-Chinese characters as a whole.
Take the word?computer?
as an example, we should regard?computer?
as a whole, but not treat it as 8 sepa-rate letters of ?c?, ?o?, ?m?, ?p?, ?u?, ?t?, ?e?, and?r?.2.2 CRF-based LabelingConditional random field (CRF) is an extensionof both Maximum Entropy Model (MEMs) andHidden Markov Models (HMMs), which wasfirstly introduced by Lafferty (Lafferty et al,2001).
It is an undirected graphical modeltrained to maximize the conditional probabilityof the desired outputs given the correspondinginputs.
This model has achieved great successesin word segmentation.In the CRFs model, the conditional distribu-tion P(y|x) of the labels Y givens observations Xdirectly is defined:CRF-based Experiments for Cross-Domain ChineseWord Segmentation at CIPS-SIGHAN-2010111( / ) exp{ ( , , , )}Tk k t tt kxP y x f y y x tZ ?
???
?
?y is the label sequence, x is observation sequence,Zx is a normalization term that makes the proba-bility of all state sequences sum to one; fk(yt-1, yt,t) is often a binary-valued feature function and?
k is the weight of fk.In our system, we choose six types of tags ac-cording to character position in a word.
Accord-ing to Zhao?s work (Zhao et al, 2006a), the 6-tag set enables our system to generate a betterCRF model than the 4-tag set.
In our experi-ments, we test both the 6-tag set and the 4-tagset, and the 6-tag set truly has a better result.
The6-tag set is defined as below:T = {B, B2, B3, M, E, S}Here B, B2, B3, M, E represent the first,second, third, continuing and end character posi-tions in a multi-character word, and S is the sin-gle-character word tag.We adopt 6 n-gram feature templates as fea-tures.
Some researches have proved that thecombination of 6-tag set and 6 n-gram featuretemplate can achieve a better performance (Zhaoet al, 2006a; Zhao et al, 2006b; Zhao and Kit,2007).The 6 n-gram feature templates used in oursystem are C-1, C0, C1, C-1C0, C0C1, C-1C1.
HereC stands for a character and the subscripts -1, 0and 1 stand for the previous, current and nextcharacter, respectively.Furthermore, we try to take advantage of thetypes for the characters.
For example, in our sys-tem D stands for the date, N stands for the num-ber, L stands for the letter, P stands for the punc-tuation and C stands for the other characters.Introducing these features is beneficial to theCRFs learning.2.3 Re-labeling stepSince the unlabeled corpora belong to differentdomains, traditional methods have some limita-tions.
In this section, we propose an additionalstep to make good use of the unlabelled data forthis special task.
This step is based on the out-puts of the CRFs model in the previous step.After CRFs learning, we get a training mod-el.
With this model, we can label the literature,computer, medicine and finance corpora.
Ac-cording to the outputs of the CRFs model, wechoose some labeled sentences with high confi-dence and add them to the training corpus.
Herethe selection of high confidence must guaranteethat the probability of the sentences selected be-ing correct segmentations is rather high and thenumber of the sentences selected is not too littleor they will make no difference to the generationof the new CRF model.
Since the existing train-ing model does not contain the information inthe out-of-domain data, we treat the labeled sen-tences with high confidence as additional train-ing corpus.
Then we re-train the CRFs modelwith the new training data.
With the training da-ta extracted from different domains, the trainingmodel incorporates more cross-domain informa-tion and it can work better in the correspondingcross-domain prediction task.3 Experiments3.1 Experiment SetupThere are two sources for the corpora: the train-ing corpora and the test corpus.
And in the train-ing corpora, there exist two types of corpus inthis task.
The labeled corpus is Chinese textwhich has been segmented into words while theunlabelled corpus covers two domains: literatureand computer science.
The test corpus contains 4domains, which are literature, computer science,medicine and finance.There are four evaluation metrics used inthis bake-off task: Precision, Recall, F1 measure(F1 = 2RP/(R+P)) and OOV measure, where Rand P are the recall and precision of the segmen-tation and OOV (Out-Of-Vocabulary Word) is aword which occurs in the reference corpus butdoes not occur in the labeled training corpus.Our system uses the CRF++ package Ver-sion 0.49 implemented by Taku Kudo 1  fromsourceforge.3.2 Results and DiscussionsWe test the techniques described in section 2with the given data.
Now we will show the re-sults of each operation.3.2.1  PreprocessingAs we have mentioned in section 2.1, the firststep is to cut the text into a series of sentences.1 http://crfpp.sourceforge.net/Then we should give each character in one sen-tence a label.
Before this step, it is necessary todo atomic segmentation.
And we will regard thecontinuous non-Chinese characters as a wholeand give the whole part a single label.
This ismeaningful to those corpora containing a lot ofEnglish words.
Due to the diversity of the Eng-lish words, segmenting the sentences with a lotnon-Chinese characters correctly is rather diffi-cult only through CRF learning.
We should doatomic segmentation to all training and test cor-pora.
This may achieve a higher accuracy in acertain degree.The results of word segmentation are re-ported in Table 1.
?Clouse+/-?
indicates whethertext clause has been done.Table 1: Results with clause and without clausecorpus Precision Recall FLiteratureClause+ 0.922 0.916 0.919Clause- 0.921 0.915 0.918ComputerClause+ 0.934 0.939 0.937Clause- 0.934 0.939 0.936MedicineClause+ 0.911 0.917 0.914Clause- 0.509 0.511 0.510FinanceClause+ 0.940 0.943 0.941Clause- 0.933 0.940 0.937From Table 1, we can see there is some im-provement in different degree and the effect inthe medicine corpus is the most obvious.
So wecan conclude that our preprocessing is useful tothe word segmentation.3.2.2 CRF-based labelingAfter preprocessing, we can use CRF++ packageto learn and test.The selection of feature template is also animportant factor.
For the purpose of comparison,we test two kinds of feature templates in our sys-tem.
The one is showed in Table 2 and the otherone is showed in Table 3.Table 2: Template 1# UnigramU00:%x[-1,0]U01:%x[0,0]U02:%x[1,0]U03:%x[-1,0]/%x[0,0]U04:%x[0,0]/%x[1,0]U05:%x[-1,0]/%x[1,0]# BigramBTable 3: Template 2# UnigramU00:%x[-1,0]U01:%x[0,0]U02:%x[1,0]U03:%x[-1,0]/%x[0,0]U04:%x[0,0]/%x[1,0]U05:%x[-1,0]/%x[1,0]U10:%x[-1,1]U11:%x[0,1]U12:%x[1,1]U13:%x[-1,1]/%x[0,1]U14:%x[0,1]/%x[1,1]U15:%x[-1,1]/%x[1,1]# BigramBNow we will explain the meanings of thetemplates.
Here is an example.
In table 4, weshow the format of the input file.
The first col-umn represents the word itself and the secondrepresents the feature of the word, where thereare five kinds of features: date (D), number (N),letter (L), punctuation (P) and others (C).
Themeanings of the templates are showed in table 5.Table 4: the format of the input file for CRF?
C?
D?
C?
C?
P?
C?
C?
C1 N?
C?
PTable 5: the example of the templatestemplate Expanded feature%x[0,0] ?%x[0,1] C%x[1,0] ?%x[-1,0] ?%x[-1,0]/ %x[0,0] ?/?%x[0,0]/ %x[0,1] ?/CWith two different feature templates, we con-tinue our experiments in the four different do-mains.
The segmentation performances of oursystem on test corpora using different featuretemplates are presented in Table 6.Table 6: Results with different feature templatescorpus Precision Recall FLiteratureT1 0.917 0.909 0.913T2 0.922 0.916 0.919ComputerT1 0.914 0.902 0.908T2 0.934 0.939 0.937MedicineT1 0.906 0.905 0.905T2 0.911 0.917 0.914FinanceT1 0.937 0.925 0.931T2 0.940 0.943 0.941Here T1 stands for Template 1 while T2stands for Template 2.From the Table 4 we can see the second fea-ture templates make the results of the segmenta-tion improved more significantly.At the same time we need get the outputs withconfidence measure by setting some parametersin CRF test.3.2.3 Re-labelingAs for the outputs with confidence measuregenerated by previous step, we should do somespecial processes.
Here we set a particular valueas our standard and choose the sentences withconfidence above the value.
As we know, thetest corpora are limited, the higher confidencemay cause the corpora meeting our requirementsare less.
The lower confidence may not guaran-tee the reliability.
So the setting of the confi-dence value is very significant.
In our experi-ments, we set the parameter at 0.8.Then we add the sentences whose confidenceis above 0.8 to the training corpus.
We shouldre-learn with new corpora, generate the newmodel and re-test the corpora related with 4 do-mains.
The segmentation performances after re-labeling are represented in Table 7.Table 7: Results with re-labeling and without re-labelingcorpus Precision Recall FLiteratureRe + 0.922 0.916 0.919Re - 0.921 0.916 0.918ComputerRe + 0.934 0.939 0.937Re - 0.932 0.934 0.933MedicineRe + 0.911 0.917 0.914Re - 0.912 0.918 0.915FinanceRe + 0.940 0.943 0.941Re - 0.937 0.941 0.939Here Re+/- indicates whether the re-labelingstep is to be done.From the results we know, even though the re-labeling step makes the results in the medicinecorpus a little worse, it has much better effect inthe other corpora.
Overall, the operation of re-labeling is necessary.3.3 Our results in this bakeoffIn this task, our results are showed in Table 8.Table 8: our results in this bakeoffPrecision Recall FLiterature 0.922 0.916 0.919Computer 0.934 0.939 0.937Medicine 0.911 0.917 0.914Finance 0.940 0.943 0.941From Table 6, we can see our system canachieve a high precision, especially in the do-mains of computer and finance.
This proves ourmethods are fairly effective.4 Discussion4.1 Segmentation FeaturesIn our system, we only take advantage of thefeatures of the words.
We try to add other fea-tures to our experiments such as AV feature(Feng et al, 2004a; Feng et al, 2004b; Hai Zhaoet al, 2007) with the expectation of improvingthe results.
But the results are not satisfying.
Webelieve that the feature of words frequency maybe an important factor, but how to use it is worthstudying.
So finding some meaningful and effec-tive features is the crucial point.4.2 OOVIn our system, we do not process the wordsout of vocabulary in the special way.
The recog-nition of OOV is still a problem.
In a word, thereis still much to be done to improve our system.In the present work, we make use of some sur-face features, and further study should be con-tinued to find more effective features.5 ConclusionIn this paper, we have briefly described theChinese word segmentation for out-of-domaintexts.
The CRFs model is implemented.
In orderto make the best use of the test corpora, somespecial strategies are introduced.
Further im-provement is made with these strategies.
How-ever, there is still much to do to achieve moreimprovement.
From the results, we got good ex-perience and knew the weaknesses of our system.These all help to improve the performance of oursystem in the future.AcknowledgementsThe research described in this paper was sup-ported by NSFC (Grant No.
60875033).ReferencesHai Zhao, Changning Huang, and Mu Li.
2006.
Animproved Chinese Word Segmentation Systemwith Conditional Random Field.
Proceedings ofthe Fifth SIGHAN Workshop on Chinese LanguageProcessing, Sydney, Australia.Hai Zhao and Chunyu Kit.
2007.
Incorporating globalinformation into supervised for Chinese wordsegmentation.
In PACALING-2007, Melbourne,Australia.Hai Zhao, Changning Huang, Mu Li and Bao-LiangLu.
2006b.
Effective tag set selection in Chineseword segmentation via conditional random fieldmodeling.
In PACLIC-20, Wuhan, China.John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional random fields: Proba-bilistic models for segmenting and labeling se-quence data.
In Proceeding of ICML 2001, Mor-gan Kaufmann, San Francisco, CAHai Zhao and Chunyu Kit.
2008.
Unsupervised Seg-mentation Helps Supervised Learning of CharacterTagging for Word Segmentation and Named Enti-ty Recognition.
Processing of the Sixth SIGHANWorkshop on Chinese Language Processing, Hy-derabad, India.Haodi Feng, Kang Chen, Xiaotie Deng, and WeiminZheng.
2004a.
Accessor variety criteria for Chi-nese word extraction.
Computational Linguistics.Haodi Feng, Kang Chen, Chunyu Kit, and XiaotieDeng.
2004b.
Unsupervised segmentation of Chi-nese corpus using accessor variety.
In First Inter-national Joint Conference on Natural LanguageProcessing.
Sanya, Hainan Island, China.
